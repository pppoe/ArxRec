<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250210.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming", "author": "Yuang Shi and G\u00e9raldine Morin and Simone Gasparini and Wei Tsang Ooi", "abstract": "  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS with 23% of the original model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n", "link": "http://arxiv.org/abs/2408.14823v2", "date": "2025-02-10", "relevancy": 3.1614, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6856}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6155}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LapisGS%3A%20Layered%20Progressive%203D%20Gaussian%20Splatting%20for%20Adaptive%0A%20%20Streaming&body=Title%3A%20LapisGS%3A%20Layered%20Progressive%203D%20Gaussian%20Splatting%20for%20Adaptive%0A%20%20Streaming%0AAuthor%3A%20Yuang%20Shi%20and%20G%C3%A9raldine%20Morin%20and%20Simone%20Gasparini%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%20The%20rise%20of%20Extended%20Reality%20%28XR%29%20requires%20efficient%20streaming%20of%203D%20online%0Aworlds%2C%20challenging%20current%203DGS%20representations%20to%20adapt%20to%0Abandwidth-constrained%20environments.%20This%20paper%20proposes%20LapisGS%2C%20a%20layered%203DGS%0Athat%20supports%20adaptive%20streaming%20and%20progressive%20rendering.%20Our%20method%0Aconstructs%20a%20layered%20structure%20for%20cumulative%20representation%2C%20incorporates%0Adynamic%20opacity%20optimization%20to%20maintain%20visual%20fidelity%2C%20and%20utilizes%0Aoccupancy%20maps%20to%20efficiently%20manage%20Gaussian%20splats.%20This%20proposed%20model%0Aoffers%20a%20progressive%20representation%20supporting%20a%20continuous%20rendering%20quality%0Aadapted%20for%20bandwidth-aware%20streaming.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20approach%20in%20balancing%20visual%20fidelity%20with%20the%20compactness%0Aof%20the%20model%2C%20with%20up%20to%2050.71%25%20improvement%20in%20SSIM%2C%20286.53%25%20improvement%20in%0ALPIPS%20with%2023%25%20of%20the%20original%20model%20size%2C%20and%20shows%20its%20potential%20for%0Abandwidth-adapted%203D%20streaming%20and%20rendering%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLapisGS%253A%2520Layered%2520Progressive%25203D%2520Gaussian%2520Splatting%2520for%2520Adaptive%250A%2520%2520Streaming%26entry.906535625%3DYuang%2520Shi%2520and%2520G%25C3%25A9raldine%2520Morin%2520and%2520Simone%2520Gasparini%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520Extended%2520Reality%2520%2528XR%2529%2520requires%2520efficient%2520streaming%2520of%25203D%2520online%250Aworlds%252C%2520challenging%2520current%25203DGS%2520representations%2520to%2520adapt%2520to%250Abandwidth-constrained%2520environments.%2520This%2520paper%2520proposes%2520LapisGS%252C%2520a%2520layered%25203DGS%250Athat%2520supports%2520adaptive%2520streaming%2520and%2520progressive%2520rendering.%2520Our%2520method%250Aconstructs%2520a%2520layered%2520structure%2520for%2520cumulative%2520representation%252C%2520incorporates%250Adynamic%2520opacity%2520optimization%2520to%2520maintain%2520visual%2520fidelity%252C%2520and%2520utilizes%250Aoccupancy%2520maps%2520to%2520efficiently%2520manage%2520Gaussian%2520splats.%2520This%2520proposed%2520model%250Aoffers%2520a%2520progressive%2520representation%2520supporting%2520a%2520continuous%2520rendering%2520quality%250Aadapted%2520for%2520bandwidth-aware%2520streaming.%2520Extensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520balancing%2520visual%2520fidelity%2520with%2520the%2520compactness%250Aof%2520the%2520model%252C%2520with%2520up%2520to%252050.71%2525%2520improvement%2520in%2520SSIM%252C%2520286.53%2525%2520improvement%2520in%250ALPIPS%2520with%252023%2525%2520of%2520the%2520original%2520model%2520size%252C%2520and%2520shows%2520its%2520potential%2520for%250Abandwidth-adapted%25203D%2520streaming%2520and%2520rendering%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LapisGS%3A%20Layered%20Progressive%203D%20Gaussian%20Splatting%20for%20Adaptive%0A%20%20Streaming&entry.906535625=Yuang%20Shi%20and%20G%C3%A9raldine%20Morin%20and%20Simone%20Gasparini%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%20The%20rise%20of%20Extended%20Reality%20%28XR%29%20requires%20efficient%20streaming%20of%203D%20online%0Aworlds%2C%20challenging%20current%203DGS%20representations%20to%20adapt%20to%0Abandwidth-constrained%20environments.%20This%20paper%20proposes%20LapisGS%2C%20a%20layered%203DGS%0Athat%20supports%20adaptive%20streaming%20and%20progressive%20rendering.%20Our%20method%0Aconstructs%20a%20layered%20structure%20for%20cumulative%20representation%2C%20incorporates%0Adynamic%20opacity%20optimization%20to%20maintain%20visual%20fidelity%2C%20and%20utilizes%0Aoccupancy%20maps%20to%20efficiently%20manage%20Gaussian%20splats.%20This%20proposed%20model%0Aoffers%20a%20progressive%20representation%20supporting%20a%20continuous%20rendering%20quality%0Aadapted%20for%20bandwidth-aware%20streaming.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20approach%20in%20balancing%20visual%20fidelity%20with%20the%20compactness%0Aof%20the%20model%2C%20with%20up%20to%2050.71%25%20improvement%20in%20SSIM%2C%20286.53%25%20improvement%20in%0ALPIPS%20with%2023%25%20of%20the%20original%20model%20size%2C%20and%20shows%20its%20potential%20for%0Abandwidth-adapted%203D%20streaming%20and%20rendering%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14823v2&entry.124074799=Read"},
{"title": "MaterialFusion: High-Quality, Zero-Shot, and Controllable Material\n  Transfer with Diffusion Models", "author": "Kamil Garifullin and Maxim Nikolaev and Andrey Kuznetsov and Aibek Alanov", "abstract": "  Manipulating the material appearance of objects in images is critical for\napplications like augmented reality, virtual prototyping, and digital content\ncreation. We present MaterialFusion, a novel framework for high-quality\nmaterial transfer that allows users to adjust the degree of material\napplication, achieving an optimal balance between new material properties and\nthe object's original features. MaterialFusion seamlessly integrates the\nmodified object into the scene by maintaining background consistency and\nmitigating boundary artifacts. To thoroughly evaluate our approach, we have\ncompiled a dataset of real-world material transfer examples and conducted\ncomplex comparative analyses. Through comprehensive quantitative evaluations\nand user studies, we demonstrate that MaterialFusion significantly outperforms\nexisting methods in terms of quality, user control, and background\npreservation. Code is available at\nhttps://github.com/kzGarifullin/MaterialFusion.\n", "link": "http://arxiv.org/abs/2502.06606v1", "date": "2025-02-10", "relevancy": 3.0654, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6791}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5801}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaterialFusion%3A%20High-Quality%2C%20Zero-Shot%2C%20and%20Controllable%20Material%0A%20%20Transfer%20with%20Diffusion%20Models&body=Title%3A%20MaterialFusion%3A%20High-Quality%2C%20Zero-Shot%2C%20and%20Controllable%20Material%0A%20%20Transfer%20with%20Diffusion%20Models%0AAuthor%3A%20Kamil%20Garifullin%20and%20Maxim%20Nikolaev%20and%20Andrey%20Kuznetsov%20and%20Aibek%20Alanov%0AAbstract%3A%20%20%20Manipulating%20the%20material%20appearance%20of%20objects%20in%20images%20is%20critical%20for%0Aapplications%20like%20augmented%20reality%2C%20virtual%20prototyping%2C%20and%20digital%20content%0Acreation.%20We%20present%20MaterialFusion%2C%20a%20novel%20framework%20for%20high-quality%0Amaterial%20transfer%20that%20allows%20users%20to%20adjust%20the%20degree%20of%20material%0Aapplication%2C%20achieving%20an%20optimal%20balance%20between%20new%20material%20properties%20and%0Athe%20object%27s%20original%20features.%20MaterialFusion%20seamlessly%20integrates%20the%0Amodified%20object%20into%20the%20scene%20by%20maintaining%20background%20consistency%20and%0Amitigating%20boundary%20artifacts.%20To%20thoroughly%20evaluate%20our%20approach%2C%20we%20have%0Acompiled%20a%20dataset%20of%20real-world%20material%20transfer%20examples%20and%20conducted%0Acomplex%20comparative%20analyses.%20Through%20comprehensive%20quantitative%20evaluations%0Aand%20user%20studies%2C%20we%20demonstrate%20that%20MaterialFusion%20significantly%20outperforms%0Aexisting%20methods%20in%20terms%20of%20quality%2C%20user%20control%2C%20and%20background%0Apreservation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/kzGarifullin/MaterialFusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaterialFusion%253A%2520High-Quality%252C%2520Zero-Shot%252C%2520and%2520Controllable%2520Material%250A%2520%2520Transfer%2520with%2520Diffusion%2520Models%26entry.906535625%3DKamil%2520Garifullin%2520and%2520Maxim%2520Nikolaev%2520and%2520Andrey%2520Kuznetsov%2520and%2520Aibek%2520Alanov%26entry.1292438233%3D%2520%2520Manipulating%2520the%2520material%2520appearance%2520of%2520objects%2520in%2520images%2520is%2520critical%2520for%250Aapplications%2520like%2520augmented%2520reality%252C%2520virtual%2520prototyping%252C%2520and%2520digital%2520content%250Acreation.%2520We%2520present%2520MaterialFusion%252C%2520a%2520novel%2520framework%2520for%2520high-quality%250Amaterial%2520transfer%2520that%2520allows%2520users%2520to%2520adjust%2520the%2520degree%2520of%2520material%250Aapplication%252C%2520achieving%2520an%2520optimal%2520balance%2520between%2520new%2520material%2520properties%2520and%250Athe%2520object%2527s%2520original%2520features.%2520MaterialFusion%2520seamlessly%2520integrates%2520the%250Amodified%2520object%2520into%2520the%2520scene%2520by%2520maintaining%2520background%2520consistency%2520and%250Amitigating%2520boundary%2520artifacts.%2520To%2520thoroughly%2520evaluate%2520our%2520approach%252C%2520we%2520have%250Acompiled%2520a%2520dataset%2520of%2520real-world%2520material%2520transfer%2520examples%2520and%2520conducted%250Acomplex%2520comparative%2520analyses.%2520Through%2520comprehensive%2520quantitative%2520evaluations%250Aand%2520user%2520studies%252C%2520we%2520demonstrate%2520that%2520MaterialFusion%2520significantly%2520outperforms%250Aexisting%2520methods%2520in%2520terms%2520of%2520quality%252C%2520user%2520control%252C%2520and%2520background%250Apreservation.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/kzGarifullin/MaterialFusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaterialFusion%3A%20High-Quality%2C%20Zero-Shot%2C%20and%20Controllable%20Material%0A%20%20Transfer%20with%20Diffusion%20Models&entry.906535625=Kamil%20Garifullin%20and%20Maxim%20Nikolaev%20and%20Andrey%20Kuznetsov%20and%20Aibek%20Alanov&entry.1292438233=%20%20Manipulating%20the%20material%20appearance%20of%20objects%20in%20images%20is%20critical%20for%0Aapplications%20like%20augmented%20reality%2C%20virtual%20prototyping%2C%20and%20digital%20content%0Acreation.%20We%20present%20MaterialFusion%2C%20a%20novel%20framework%20for%20high-quality%0Amaterial%20transfer%20that%20allows%20users%20to%20adjust%20the%20degree%20of%20material%0Aapplication%2C%20achieving%20an%20optimal%20balance%20between%20new%20material%20properties%20and%0Athe%20object%27s%20original%20features.%20MaterialFusion%20seamlessly%20integrates%20the%0Amodified%20object%20into%20the%20scene%20by%20maintaining%20background%20consistency%20and%0Amitigating%20boundary%20artifacts.%20To%20thoroughly%20evaluate%20our%20approach%2C%20we%20have%0Acompiled%20a%20dataset%20of%20real-world%20material%20transfer%20examples%20and%20conducted%0Acomplex%20comparative%20analyses.%20Through%20comprehensive%20quantitative%20evaluations%0Aand%20user%20studies%2C%20we%20demonstrate%20that%20MaterialFusion%20significantly%20outperforms%0Aexisting%20methods%20in%20terms%20of%20quality%2C%20user%20control%2C%20and%20background%0Apreservation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/kzGarifullin/MaterialFusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06606v1&entry.124074799=Read"},
{"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "author": "Haiwen Diao and Xiaotong Li and Yufeng Cui and Yueze Wang and Haoge Deng and Ting Pan and Wenxuan Wang and Huchuan Lu and Xinlong Wang", "abstract": "  Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.\n", "link": "http://arxiv.org/abs/2502.06788v1", "date": "2025-02-10", "relevancy": 3.0231, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVEv2%3A%20Improved%20Baselines%20for%20Encoder-Free%20Vision-Language%20Models&body=Title%3A%20EVEv2%3A%20Improved%20Baselines%20for%20Encoder-Free%20Vision-Language%20Models%0AAuthor%3A%20Haiwen%20Diao%20and%20Xiaotong%20Li%20and%20Yufeng%20Cui%20and%20Yueze%20Wang%20and%20Haoge%20Deng%20and%20Ting%20Pan%20and%20Wenxuan%20Wang%20and%20Huchuan%20Lu%20and%20Xinlong%20Wang%0AAbstract%3A%20%20%20Existing%20encoder-free%20vision-language%20models%20%28VLMs%29%20are%20rapidly%20narrowing%20the%0Aperformance%20gap%20with%20their%20encoder-based%20counterparts%2C%20highlighting%20the%0Apromising%20potential%20for%20unified%20multimodal%20systems%20with%20structural%20simplicity%0Aand%20efficient%20deployment.%20We%20systematically%20clarify%20the%20performance%20gap%20between%0AVLMs%20using%20pre-trained%20vision%20encoders%2C%20discrete%20tokenizers%2C%20and%20minimalist%0Avisual%20layers%20from%20scratch%2C%20deeply%20excavating%20the%20under-examined%0Acharacteristics%20of%20encoder-free%20VLMs.%20We%20develop%20efficient%20strategies%20for%0Aencoder-free%20VLMs%20that%20rival%20mainstream%20encoder-based%20ones.%20After%20an%20in-depth%0Ainvestigation%2C%20we%20launch%20EVEv2.0%2C%20a%20new%20and%20improved%20family%20of%20encoder-free%0AVLMs.%20We%20show%20that%3A%20%28i%29%20Properly%20decomposing%20and%20hierarchically%20associating%0Avision%20and%20language%20within%20a%20unified%20model%20reduces%20interference%20between%0Amodalities.%20%28ii%29%20A%20well-designed%20training%20strategy%20enables%20effective%0Aoptimization%20for%20encoder-free%20VLMs.%20Through%20extensive%20evaluation%2C%20our%20EVEv2.0%0Arepresents%20a%20thorough%20study%20for%20developing%20a%20decoder-only%20architecture%20across%0Amodalities%2C%20demonstrating%20superior%20data%20efficiency%20and%20strong%20vision-reasoning%0Acapability.%20Code%20is%20publicly%20available%20at%3A%20https%3A//github.com/baaivision/EVE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVEv2%253A%2520Improved%2520Baselines%2520for%2520Encoder-Free%2520Vision-Language%2520Models%26entry.906535625%3DHaiwen%2520Diao%2520and%2520Xiaotong%2520Li%2520and%2520Yufeng%2520Cui%2520and%2520Yueze%2520Wang%2520and%2520Haoge%2520Deng%2520and%2520Ting%2520Pan%2520and%2520Wenxuan%2520Wang%2520and%2520Huchuan%2520Lu%2520and%2520Xinlong%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520encoder-free%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520rapidly%2520narrowing%2520the%250Aperformance%2520gap%2520with%2520their%2520encoder-based%2520counterparts%252C%2520highlighting%2520the%250Apromising%2520potential%2520for%2520unified%2520multimodal%2520systems%2520with%2520structural%2520simplicity%250Aand%2520efficient%2520deployment.%2520We%2520systematically%2520clarify%2520the%2520performance%2520gap%2520between%250AVLMs%2520using%2520pre-trained%2520vision%2520encoders%252C%2520discrete%2520tokenizers%252C%2520and%2520minimalist%250Avisual%2520layers%2520from%2520scratch%252C%2520deeply%2520excavating%2520the%2520under-examined%250Acharacteristics%2520of%2520encoder-free%2520VLMs.%2520We%2520develop%2520efficient%2520strategies%2520for%250Aencoder-free%2520VLMs%2520that%2520rival%2520mainstream%2520encoder-based%2520ones.%2520After%2520an%2520in-depth%250Ainvestigation%252C%2520we%2520launch%2520EVEv2.0%252C%2520a%2520new%2520and%2520improved%2520family%2520of%2520encoder-free%250AVLMs.%2520We%2520show%2520that%253A%2520%2528i%2529%2520Properly%2520decomposing%2520and%2520hierarchically%2520associating%250Avision%2520and%2520language%2520within%2520a%2520unified%2520model%2520reduces%2520interference%2520between%250Amodalities.%2520%2528ii%2529%2520A%2520well-designed%2520training%2520strategy%2520enables%2520effective%250Aoptimization%2520for%2520encoder-free%2520VLMs.%2520Through%2520extensive%2520evaluation%252C%2520our%2520EVEv2.0%250Arepresents%2520a%2520thorough%2520study%2520for%2520developing%2520a%2520decoder-only%2520architecture%2520across%250Amodalities%252C%2520demonstrating%2520superior%2520data%2520efficiency%2520and%2520strong%2520vision-reasoning%250Acapability.%2520Code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/baaivision/EVE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVEv2%3A%20Improved%20Baselines%20for%20Encoder-Free%20Vision-Language%20Models&entry.906535625=Haiwen%20Diao%20and%20Xiaotong%20Li%20and%20Yufeng%20Cui%20and%20Yueze%20Wang%20and%20Haoge%20Deng%20and%20Ting%20Pan%20and%20Wenxuan%20Wang%20and%20Huchuan%20Lu%20and%20Xinlong%20Wang&entry.1292438233=%20%20Existing%20encoder-free%20vision-language%20models%20%28VLMs%29%20are%20rapidly%20narrowing%20the%0Aperformance%20gap%20with%20their%20encoder-based%20counterparts%2C%20highlighting%20the%0Apromising%20potential%20for%20unified%20multimodal%20systems%20with%20structural%20simplicity%0Aand%20efficient%20deployment.%20We%20systematically%20clarify%20the%20performance%20gap%20between%0AVLMs%20using%20pre-trained%20vision%20encoders%2C%20discrete%20tokenizers%2C%20and%20minimalist%0Avisual%20layers%20from%20scratch%2C%20deeply%20excavating%20the%20under-examined%0Acharacteristics%20of%20encoder-free%20VLMs.%20We%20develop%20efficient%20strategies%20for%0Aencoder-free%20VLMs%20that%20rival%20mainstream%20encoder-based%20ones.%20After%20an%20in-depth%0Ainvestigation%2C%20we%20launch%20EVEv2.0%2C%20a%20new%20and%20improved%20family%20of%20encoder-free%0AVLMs.%20We%20show%20that%3A%20%28i%29%20Properly%20decomposing%20and%20hierarchically%20associating%0Avision%20and%20language%20within%20a%20unified%20model%20reduces%20interference%20between%0Amodalities.%20%28ii%29%20A%20well-designed%20training%20strategy%20enables%20effective%0Aoptimization%20for%20encoder-free%20VLMs.%20Through%20extensive%20evaluation%2C%20our%20EVEv2.0%0Arepresents%20a%20thorough%20study%20for%20developing%20a%20decoder-only%20architecture%20across%0Amodalities%2C%20demonstrating%20superior%20data%20efficiency%20and%20strong%20vision-reasoning%0Acapability.%20Code%20is%20publicly%20available%20at%3A%20https%3A//github.com/baaivision/EVE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06788v1&entry.124074799=Read"},
{"title": "TANGLED: Generating 3D Hair Strands from Images with Arbitrary Styles\n  and Viewpoints", "author": "Pengyu Long and Zijun Zhao and Min Ouyang and Qingcheng Zhao and Qixuan Zhang and Wei Yang and Lan Xu and Jingyi Yu", "abstract": "  Hairstyles are intricate and culturally significant with various geometries,\ntextures, and structures. Existing text or image-guided generation methods fail\nto handle the richness and complexity of diverse styles. We present TANGLED, a\nnovel approach for 3D hair strand generation that accommodates diverse image\ninputs across styles, viewpoints, and quantities of input views. TANGLED\nemploys a three-step pipeline. First, our MultiHair Dataset provides 457\ndiverse hairstyles annotated with 74 attributes, emphasizing complex and\nculturally significant styles to improve model generalization. Second, we\npropose a diffusion framework conditioned on multi-view linearts that can\ncapture topological cues (e.g., strand density and parting lines) while\nfiltering out noise. By leveraging a latent diffusion model with\ncross-attention on lineart features, our method achieves flexible and robust 3D\nhair generation across diverse input conditions. Third, a parametric\npost-processing module enforces braid-specific constraints to maintain\ncoherence in complex structures. This framework not only advances hairstyle\nrealism and diversity but also enables culturally inclusive digital avatars and\nnovel applications like sketch-based 3D strand editing for animation and\naugmented reality.\n", "link": "http://arxiv.org/abs/2502.06392v1", "date": "2025-02-10", "relevancy": 3.0121, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6113}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.598}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TANGLED%3A%20Generating%203D%20Hair%20Strands%20from%20Images%20with%20Arbitrary%20Styles%0A%20%20and%20Viewpoints&body=Title%3A%20TANGLED%3A%20Generating%203D%20Hair%20Strands%20from%20Images%20with%20Arbitrary%20Styles%0A%20%20and%20Viewpoints%0AAuthor%3A%20Pengyu%20Long%20and%20Zijun%20Zhao%20and%20Min%20Ouyang%20and%20Qingcheng%20Zhao%20and%20Qixuan%20Zhang%20and%20Wei%20Yang%20and%20Lan%20Xu%20and%20Jingyi%20Yu%0AAbstract%3A%20%20%20Hairstyles%20are%20intricate%20and%20culturally%20significant%20with%20various%20geometries%2C%0Atextures%2C%20and%20structures.%20Existing%20text%20or%20image-guided%20generation%20methods%20fail%0Ato%20handle%20the%20richness%20and%20complexity%20of%20diverse%20styles.%20We%20present%20TANGLED%2C%20a%0Anovel%20approach%20for%203D%20hair%20strand%20generation%20that%20accommodates%20diverse%20image%0Ainputs%20across%20styles%2C%20viewpoints%2C%20and%20quantities%20of%20input%20views.%20TANGLED%0Aemploys%20a%20three-step%20pipeline.%20First%2C%20our%20MultiHair%20Dataset%20provides%20457%0Adiverse%20hairstyles%20annotated%20with%2074%20attributes%2C%20emphasizing%20complex%20and%0Aculturally%20significant%20styles%20to%20improve%20model%20generalization.%20Second%2C%20we%0Apropose%20a%20diffusion%20framework%20conditioned%20on%20multi-view%20linearts%20that%20can%0Acapture%20topological%20cues%20%28e.g.%2C%20strand%20density%20and%20parting%20lines%29%20while%0Afiltering%20out%20noise.%20By%20leveraging%20a%20latent%20diffusion%20model%20with%0Across-attention%20on%20lineart%20features%2C%20our%20method%20achieves%20flexible%20and%20robust%203D%0Ahair%20generation%20across%20diverse%20input%20conditions.%20Third%2C%20a%20parametric%0Apost-processing%20module%20enforces%20braid-specific%20constraints%20to%20maintain%0Acoherence%20in%20complex%20structures.%20This%20framework%20not%20only%20advances%20hairstyle%0Arealism%20and%20diversity%20but%20also%20enables%20culturally%20inclusive%20digital%20avatars%20and%0Anovel%20applications%20like%20sketch-based%203D%20strand%20editing%20for%20animation%20and%0Aaugmented%20reality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTANGLED%253A%2520Generating%25203D%2520Hair%2520Strands%2520from%2520Images%2520with%2520Arbitrary%2520Styles%250A%2520%2520and%2520Viewpoints%26entry.906535625%3DPengyu%2520Long%2520and%2520Zijun%2520Zhao%2520and%2520Min%2520Ouyang%2520and%2520Qingcheng%2520Zhao%2520and%2520Qixuan%2520Zhang%2520and%2520Wei%2520Yang%2520and%2520Lan%2520Xu%2520and%2520Jingyi%2520Yu%26entry.1292438233%3D%2520%2520Hairstyles%2520are%2520intricate%2520and%2520culturally%2520significant%2520with%2520various%2520geometries%252C%250Atextures%252C%2520and%2520structures.%2520Existing%2520text%2520or%2520image-guided%2520generation%2520methods%2520fail%250Ato%2520handle%2520the%2520richness%2520and%2520complexity%2520of%2520diverse%2520styles.%2520We%2520present%2520TANGLED%252C%2520a%250Anovel%2520approach%2520for%25203D%2520hair%2520strand%2520generation%2520that%2520accommodates%2520diverse%2520image%250Ainputs%2520across%2520styles%252C%2520viewpoints%252C%2520and%2520quantities%2520of%2520input%2520views.%2520TANGLED%250Aemploys%2520a%2520three-step%2520pipeline.%2520First%252C%2520our%2520MultiHair%2520Dataset%2520provides%2520457%250Adiverse%2520hairstyles%2520annotated%2520with%252074%2520attributes%252C%2520emphasizing%2520complex%2520and%250Aculturally%2520significant%2520styles%2520to%2520improve%2520model%2520generalization.%2520Second%252C%2520we%250Apropose%2520a%2520diffusion%2520framework%2520conditioned%2520on%2520multi-view%2520linearts%2520that%2520can%250Acapture%2520topological%2520cues%2520%2528e.g.%252C%2520strand%2520density%2520and%2520parting%2520lines%2529%2520while%250Afiltering%2520out%2520noise.%2520By%2520leveraging%2520a%2520latent%2520diffusion%2520model%2520with%250Across-attention%2520on%2520lineart%2520features%252C%2520our%2520method%2520achieves%2520flexible%2520and%2520robust%25203D%250Ahair%2520generation%2520across%2520diverse%2520input%2520conditions.%2520Third%252C%2520a%2520parametric%250Apost-processing%2520module%2520enforces%2520braid-specific%2520constraints%2520to%2520maintain%250Acoherence%2520in%2520complex%2520structures.%2520This%2520framework%2520not%2520only%2520advances%2520hairstyle%250Arealism%2520and%2520diversity%2520but%2520also%2520enables%2520culturally%2520inclusive%2520digital%2520avatars%2520and%250Anovel%2520applications%2520like%2520sketch-based%25203D%2520strand%2520editing%2520for%2520animation%2520and%250Aaugmented%2520reality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TANGLED%3A%20Generating%203D%20Hair%20Strands%20from%20Images%20with%20Arbitrary%20Styles%0A%20%20and%20Viewpoints&entry.906535625=Pengyu%20Long%20and%20Zijun%20Zhao%20and%20Min%20Ouyang%20and%20Qingcheng%20Zhao%20and%20Qixuan%20Zhang%20and%20Wei%20Yang%20and%20Lan%20Xu%20and%20Jingyi%20Yu&entry.1292438233=%20%20Hairstyles%20are%20intricate%20and%20culturally%20significant%20with%20various%20geometries%2C%0Atextures%2C%20and%20structures.%20Existing%20text%20or%20image-guided%20generation%20methods%20fail%0Ato%20handle%20the%20richness%20and%20complexity%20of%20diverse%20styles.%20We%20present%20TANGLED%2C%20a%0Anovel%20approach%20for%203D%20hair%20strand%20generation%20that%20accommodates%20diverse%20image%0Ainputs%20across%20styles%2C%20viewpoints%2C%20and%20quantities%20of%20input%20views.%20TANGLED%0Aemploys%20a%20three-step%20pipeline.%20First%2C%20our%20MultiHair%20Dataset%20provides%20457%0Adiverse%20hairstyles%20annotated%20with%2074%20attributes%2C%20emphasizing%20complex%20and%0Aculturally%20significant%20styles%20to%20improve%20model%20generalization.%20Second%2C%20we%0Apropose%20a%20diffusion%20framework%20conditioned%20on%20multi-view%20linearts%20that%20can%0Acapture%20topological%20cues%20%28e.g.%2C%20strand%20density%20and%20parting%20lines%29%20while%0Afiltering%20out%20noise.%20By%20leveraging%20a%20latent%20diffusion%20model%20with%0Across-attention%20on%20lineart%20features%2C%20our%20method%20achieves%20flexible%20and%20robust%203D%0Ahair%20generation%20across%20diverse%20input%20conditions.%20Third%2C%20a%20parametric%0Apost-processing%20module%20enforces%20braid-specific%20constraints%20to%20maintain%0Acoherence%20in%20complex%20structures.%20This%20framework%20not%20only%20advances%20hairstyle%0Arealism%20and%20diversity%20but%20also%20enables%20culturally%20inclusive%20digital%20avatars%20and%0Anovel%20applications%20like%20sketch-based%203D%20strand%20editing%20for%20animation%20and%0Aaugmented%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06392v1&entry.124074799=Read"},
{"title": "Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint\n  in a Driving Scene", "author": "Tai-Yu Pan and Sooyoung Jeon and Mengdi Fan and Jinsu Yoo and Zhenyang Feng and Mark Campbell and Kilian Q. Weinberger and Bharath Hariharan and Wei-Lun Chao", "abstract": "  Self-driving cars relying solely on ego-centric perception face limitations\nin sensing, often failing to detect occluded, faraway objects. Collaborative\nautonomous driving (CAV) seems like a promising direction, but collecting data\nfor development is non-trivial. It requires placing multiple sensor-equipped\nagents in a real-world driving scene, simultaneously! As such, existing\ndatasets are limited in locations and agents. We introduce a novel surrogate to\nthe rescue, which is to generate realistic perception from different viewpoints\nin a driving scene, conditioned on a real-world sample - the ego-car's sensory\ndata. This surrogate has huge potential: it could potentially turn any ego-car\ndataset into a collaborative driving one to scale up the development of CAV. We\npresent the very first solution, using a combination of simulated collaborative\ndata and real ego-car data. Our method, Transfer Your Perspective (TYP), learns\na conditioned diffusion model whose output samples are not only realistic but\nalso consistent in both semantics and layouts with the given ego-car data.\nEmpirical results demonstrate TYP's effectiveness in aiding in a CAV setting.\nIn particular, TYP enables us to (pre-)train collaborative perception\nalgorithms like early and late fusion with little or no real-world\ncollaborative data, greatly facilitating downstream CAV applications.\n", "link": "http://arxiv.org/abs/2502.06682v1", "date": "2025-02-10", "relevancy": 3.0101, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.61}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.598}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Your%20Perspective%3A%20Controllable%203D%20Generation%20from%20Any%20Viewpoint%0A%20%20in%20a%20Driving%20Scene&body=Title%3A%20Transfer%20Your%20Perspective%3A%20Controllable%203D%20Generation%20from%20Any%20Viewpoint%0A%20%20in%20a%20Driving%20Scene%0AAuthor%3A%20Tai-Yu%20Pan%20and%20Sooyoung%20Jeon%20and%20Mengdi%20Fan%20and%20Jinsu%20Yoo%20and%20Zhenyang%20Feng%20and%20Mark%20Campbell%20and%20Kilian%20Q.%20Weinberger%20and%20Bharath%20Hariharan%20and%20Wei-Lun%20Chao%0AAbstract%3A%20%20%20Self-driving%20cars%20relying%20solely%20on%20ego-centric%20perception%20face%20limitations%0Ain%20sensing%2C%20often%20failing%20to%20detect%20occluded%2C%20faraway%20objects.%20Collaborative%0Aautonomous%20driving%20%28CAV%29%20seems%20like%20a%20promising%20direction%2C%20but%20collecting%20data%0Afor%20development%20is%20non-trivial.%20It%20requires%20placing%20multiple%20sensor-equipped%0Aagents%20in%20a%20real-world%20driving%20scene%2C%20simultaneously%21%20As%20such%2C%20existing%0Adatasets%20are%20limited%20in%20locations%20and%20agents.%20We%20introduce%20a%20novel%20surrogate%20to%0Athe%20rescue%2C%20which%20is%20to%20generate%20realistic%20perception%20from%20different%20viewpoints%0Ain%20a%20driving%20scene%2C%20conditioned%20on%20a%20real-world%20sample%20-%20the%20ego-car%27s%20sensory%0Adata.%20This%20surrogate%20has%20huge%20potential%3A%20it%20could%20potentially%20turn%20any%20ego-car%0Adataset%20into%20a%20collaborative%20driving%20one%20to%20scale%20up%20the%20development%20of%20CAV.%20We%0Apresent%20the%20very%20first%20solution%2C%20using%20a%20combination%20of%20simulated%20collaborative%0Adata%20and%20real%20ego-car%20data.%20Our%20method%2C%20Transfer%20Your%20Perspective%20%28TYP%29%2C%20learns%0Aa%20conditioned%20diffusion%20model%20whose%20output%20samples%20are%20not%20only%20realistic%20but%0Aalso%20consistent%20in%20both%20semantics%20and%20layouts%20with%20the%20given%20ego-car%20data.%0AEmpirical%20results%20demonstrate%20TYP%27s%20effectiveness%20in%20aiding%20in%20a%20CAV%20setting.%0AIn%20particular%2C%20TYP%20enables%20us%20to%20%28pre-%29train%20collaborative%20perception%0Aalgorithms%20like%20early%20and%20late%20fusion%20with%20little%20or%20no%20real-world%0Acollaborative%20data%2C%20greatly%20facilitating%20downstream%20CAV%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Your%2520Perspective%253A%2520Controllable%25203D%2520Generation%2520from%2520Any%2520Viewpoint%250A%2520%2520in%2520a%2520Driving%2520Scene%26entry.906535625%3DTai-Yu%2520Pan%2520and%2520Sooyoung%2520Jeon%2520and%2520Mengdi%2520Fan%2520and%2520Jinsu%2520Yoo%2520and%2520Zhenyang%2520Feng%2520and%2520Mark%2520Campbell%2520and%2520Kilian%2520Q.%2520Weinberger%2520and%2520Bharath%2520Hariharan%2520and%2520Wei-Lun%2520Chao%26entry.1292438233%3D%2520%2520Self-driving%2520cars%2520relying%2520solely%2520on%2520ego-centric%2520perception%2520face%2520limitations%250Ain%2520sensing%252C%2520often%2520failing%2520to%2520detect%2520occluded%252C%2520faraway%2520objects.%2520Collaborative%250Aautonomous%2520driving%2520%2528CAV%2529%2520seems%2520like%2520a%2520promising%2520direction%252C%2520but%2520collecting%2520data%250Afor%2520development%2520is%2520non-trivial.%2520It%2520requires%2520placing%2520multiple%2520sensor-equipped%250Aagents%2520in%2520a%2520real-world%2520driving%2520scene%252C%2520simultaneously%2521%2520As%2520such%252C%2520existing%250Adatasets%2520are%2520limited%2520in%2520locations%2520and%2520agents.%2520We%2520introduce%2520a%2520novel%2520surrogate%2520to%250Athe%2520rescue%252C%2520which%2520is%2520to%2520generate%2520realistic%2520perception%2520from%2520different%2520viewpoints%250Ain%2520a%2520driving%2520scene%252C%2520conditioned%2520on%2520a%2520real-world%2520sample%2520-%2520the%2520ego-car%2527s%2520sensory%250Adata.%2520This%2520surrogate%2520has%2520huge%2520potential%253A%2520it%2520could%2520potentially%2520turn%2520any%2520ego-car%250Adataset%2520into%2520a%2520collaborative%2520driving%2520one%2520to%2520scale%2520up%2520the%2520development%2520of%2520CAV.%2520We%250Apresent%2520the%2520very%2520first%2520solution%252C%2520using%2520a%2520combination%2520of%2520simulated%2520collaborative%250Adata%2520and%2520real%2520ego-car%2520data.%2520Our%2520method%252C%2520Transfer%2520Your%2520Perspective%2520%2528TYP%2529%252C%2520learns%250Aa%2520conditioned%2520diffusion%2520model%2520whose%2520output%2520samples%2520are%2520not%2520only%2520realistic%2520but%250Aalso%2520consistent%2520in%2520both%2520semantics%2520and%2520layouts%2520with%2520the%2520given%2520ego-car%2520data.%250AEmpirical%2520results%2520demonstrate%2520TYP%2527s%2520effectiveness%2520in%2520aiding%2520in%2520a%2520CAV%2520setting.%250AIn%2520particular%252C%2520TYP%2520enables%2520us%2520to%2520%2528pre-%2529train%2520collaborative%2520perception%250Aalgorithms%2520like%2520early%2520and%2520late%2520fusion%2520with%2520little%2520or%2520no%2520real-world%250Acollaborative%2520data%252C%2520greatly%2520facilitating%2520downstream%2520CAV%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Your%20Perspective%3A%20Controllable%203D%20Generation%20from%20Any%20Viewpoint%0A%20%20in%20a%20Driving%20Scene&entry.906535625=Tai-Yu%20Pan%20and%20Sooyoung%20Jeon%20and%20Mengdi%20Fan%20and%20Jinsu%20Yoo%20and%20Zhenyang%20Feng%20and%20Mark%20Campbell%20and%20Kilian%20Q.%20Weinberger%20and%20Bharath%20Hariharan%20and%20Wei-Lun%20Chao&entry.1292438233=%20%20Self-driving%20cars%20relying%20solely%20on%20ego-centric%20perception%20face%20limitations%0Ain%20sensing%2C%20often%20failing%20to%20detect%20occluded%2C%20faraway%20objects.%20Collaborative%0Aautonomous%20driving%20%28CAV%29%20seems%20like%20a%20promising%20direction%2C%20but%20collecting%20data%0Afor%20development%20is%20non-trivial.%20It%20requires%20placing%20multiple%20sensor-equipped%0Aagents%20in%20a%20real-world%20driving%20scene%2C%20simultaneously%21%20As%20such%2C%20existing%0Adatasets%20are%20limited%20in%20locations%20and%20agents.%20We%20introduce%20a%20novel%20surrogate%20to%0Athe%20rescue%2C%20which%20is%20to%20generate%20realistic%20perception%20from%20different%20viewpoints%0Ain%20a%20driving%20scene%2C%20conditioned%20on%20a%20real-world%20sample%20-%20the%20ego-car%27s%20sensory%0Adata.%20This%20surrogate%20has%20huge%20potential%3A%20it%20could%20potentially%20turn%20any%20ego-car%0Adataset%20into%20a%20collaborative%20driving%20one%20to%20scale%20up%20the%20development%20of%20CAV.%20We%0Apresent%20the%20very%20first%20solution%2C%20using%20a%20combination%20of%20simulated%20collaborative%0Adata%20and%20real%20ego-car%20data.%20Our%20method%2C%20Transfer%20Your%20Perspective%20%28TYP%29%2C%20learns%0Aa%20conditioned%20diffusion%20model%20whose%20output%20samples%20are%20not%20only%20realistic%20but%0Aalso%20consistent%20in%20both%20semantics%20and%20layouts%20with%20the%20given%20ego-car%20data.%0AEmpirical%20results%20demonstrate%20TYP%27s%20effectiveness%20in%20aiding%20in%20a%20CAV%20setting.%0AIn%20particular%2C%20TYP%20enables%20us%20to%20%28pre-%29train%20collaborative%20perception%0Aalgorithms%20like%20early%20and%20late%20fusion%20with%20little%20or%20no%20real-world%0Acollaborative%20data%2C%20greatly%20facilitating%20downstream%20CAV%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06682v1&entry.124074799=Read"},
{"title": "SIREN: Semantic, Initialization-Free Registration of Multi-Robot\n  Gaussian Splatting Maps", "author": "Ola Shorinwa and Jiankai Sun and Mac Schwager and Anirudha Majumdar", "abstract": "  We present SIREN for registration of multi-robot Gaussian Splatting (GSplat)\nmaps, with zero access to camera poses, images, and inter-map transforms for\ninitialization or fusion of local submaps. To realize these capabilities, SIREN\nharnesses the versatility and robustness of semantics in three critical ways to\nderive a rigorous registration pipeline for multi-robot GSplat maps. First,\nSIREN utilizes semantics to identify feature-rich regions of the local maps\nwhere the registration problem is better posed, eliminating the need for any\ninitialization which is generally required in prior work. Second, SIREN\nidentifies candidate correspondences between Gaussians in the local maps using\nrobust semantic features, constituting the foundation for robust geometric\noptimization, coarsely aligning 3D Gaussian primitives extracted from the local\nmaps. Third, this key step enables subsequent photometric refinement of the\ntransformation between the submaps, where SIREN leverages novel-view synthesis\nin GSplat maps along with a semantics-based image filter to compute a\nhigh-accuracy non-rigid transformation for the generation of a high-fidelity\nfused map. We demonstrate the superior performance of SIREN compared to\ncompeting baselines across a range of real-world datasets, and in particular,\nacross the most widely-used robot hardware platforms, including a manipulator,\ndrone, and quadruped. In our experiments, SIREN achieves about 90x smaller\nrotation errors, 300x smaller translation errors, and 44x smaller scale errors\nin the most challenging scenes, where competing methods struggle. We will\nrelease the code and provide a link to the project page after the review\nprocess.\n", "link": "http://arxiv.org/abs/2502.06519v1", "date": "2025-02-10", "relevancy": 3.005, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6441}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6022}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIREN%3A%20Semantic%2C%20Initialization-Free%20Registration%20of%20Multi-Robot%0A%20%20Gaussian%20Splatting%20Maps&body=Title%3A%20SIREN%3A%20Semantic%2C%20Initialization-Free%20Registration%20of%20Multi-Robot%0A%20%20Gaussian%20Splatting%20Maps%0AAuthor%3A%20Ola%20Shorinwa%20and%20Jiankai%20Sun%20and%20Mac%20Schwager%20and%20Anirudha%20Majumdar%0AAbstract%3A%20%20%20We%20present%20SIREN%20for%20registration%20of%20multi-robot%20Gaussian%20Splatting%20%28GSplat%29%0Amaps%2C%20with%20zero%20access%20to%20camera%20poses%2C%20images%2C%20and%20inter-map%20transforms%20for%0Ainitialization%20or%20fusion%20of%20local%20submaps.%20To%20realize%20these%20capabilities%2C%20SIREN%0Aharnesses%20the%20versatility%20and%20robustness%20of%20semantics%20in%20three%20critical%20ways%20to%0Aderive%20a%20rigorous%20registration%20pipeline%20for%20multi-robot%20GSplat%20maps.%20First%2C%0ASIREN%20utilizes%20semantics%20to%20identify%20feature-rich%20regions%20of%20the%20local%20maps%0Awhere%20the%20registration%20problem%20is%20better%20posed%2C%20eliminating%20the%20need%20for%20any%0Ainitialization%20which%20is%20generally%20required%20in%20prior%20work.%20Second%2C%20SIREN%0Aidentifies%20candidate%20correspondences%20between%20Gaussians%20in%20the%20local%20maps%20using%0Arobust%20semantic%20features%2C%20constituting%20the%20foundation%20for%20robust%20geometric%0Aoptimization%2C%20coarsely%20aligning%203D%20Gaussian%20primitives%20extracted%20from%20the%20local%0Amaps.%20Third%2C%20this%20key%20step%20enables%20subsequent%20photometric%20refinement%20of%20the%0Atransformation%20between%20the%20submaps%2C%20where%20SIREN%20leverages%20novel-view%20synthesis%0Ain%20GSplat%20maps%20along%20with%20a%20semantics-based%20image%20filter%20to%20compute%20a%0Ahigh-accuracy%20non-rigid%20transformation%20for%20the%20generation%20of%20a%20high-fidelity%0Afused%20map.%20We%20demonstrate%20the%20superior%20performance%20of%20SIREN%20compared%20to%0Acompeting%20baselines%20across%20a%20range%20of%20real-world%20datasets%2C%20and%20in%20particular%2C%0Aacross%20the%20most%20widely-used%20robot%20hardware%20platforms%2C%20including%20a%20manipulator%2C%0Adrone%2C%20and%20quadruped.%20In%20our%20experiments%2C%20SIREN%20achieves%20about%2090x%20smaller%0Arotation%20errors%2C%20300x%20smaller%20translation%20errors%2C%20and%2044x%20smaller%20scale%20errors%0Ain%20the%20most%20challenging%20scenes%2C%20where%20competing%20methods%20struggle.%20We%20will%0Arelease%20the%20code%20and%20provide%20a%20link%20to%20the%20project%20page%20after%20the%20review%0Aprocess.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIREN%253A%2520Semantic%252C%2520Initialization-Free%2520Registration%2520of%2520Multi-Robot%250A%2520%2520Gaussian%2520Splatting%2520Maps%26entry.906535625%3DOla%2520Shorinwa%2520and%2520Jiankai%2520Sun%2520and%2520Mac%2520Schwager%2520and%2520Anirudha%2520Majumdar%26entry.1292438233%3D%2520%2520We%2520present%2520SIREN%2520for%2520registration%2520of%2520multi-robot%2520Gaussian%2520Splatting%2520%2528GSplat%2529%250Amaps%252C%2520with%2520zero%2520access%2520to%2520camera%2520poses%252C%2520images%252C%2520and%2520inter-map%2520transforms%2520for%250Ainitialization%2520or%2520fusion%2520of%2520local%2520submaps.%2520To%2520realize%2520these%2520capabilities%252C%2520SIREN%250Aharnesses%2520the%2520versatility%2520and%2520robustness%2520of%2520semantics%2520in%2520three%2520critical%2520ways%2520to%250Aderive%2520a%2520rigorous%2520registration%2520pipeline%2520for%2520multi-robot%2520GSplat%2520maps.%2520First%252C%250ASIREN%2520utilizes%2520semantics%2520to%2520identify%2520feature-rich%2520regions%2520of%2520the%2520local%2520maps%250Awhere%2520the%2520registration%2520problem%2520is%2520better%2520posed%252C%2520eliminating%2520the%2520need%2520for%2520any%250Ainitialization%2520which%2520is%2520generally%2520required%2520in%2520prior%2520work.%2520Second%252C%2520SIREN%250Aidentifies%2520candidate%2520correspondences%2520between%2520Gaussians%2520in%2520the%2520local%2520maps%2520using%250Arobust%2520semantic%2520features%252C%2520constituting%2520the%2520foundation%2520for%2520robust%2520geometric%250Aoptimization%252C%2520coarsely%2520aligning%25203D%2520Gaussian%2520primitives%2520extracted%2520from%2520the%2520local%250Amaps.%2520Third%252C%2520this%2520key%2520step%2520enables%2520subsequent%2520photometric%2520refinement%2520of%2520the%250Atransformation%2520between%2520the%2520submaps%252C%2520where%2520SIREN%2520leverages%2520novel-view%2520synthesis%250Ain%2520GSplat%2520maps%2520along%2520with%2520a%2520semantics-based%2520image%2520filter%2520to%2520compute%2520a%250Ahigh-accuracy%2520non-rigid%2520transformation%2520for%2520the%2520generation%2520of%2520a%2520high-fidelity%250Afused%2520map.%2520We%2520demonstrate%2520the%2520superior%2520performance%2520of%2520SIREN%2520compared%2520to%250Acompeting%2520baselines%2520across%2520a%2520range%2520of%2520real-world%2520datasets%252C%2520and%2520in%2520particular%252C%250Aacross%2520the%2520most%2520widely-used%2520robot%2520hardware%2520platforms%252C%2520including%2520a%2520manipulator%252C%250Adrone%252C%2520and%2520quadruped.%2520In%2520our%2520experiments%252C%2520SIREN%2520achieves%2520about%252090x%2520smaller%250Arotation%2520errors%252C%2520300x%2520smaller%2520translation%2520errors%252C%2520and%252044x%2520smaller%2520scale%2520errors%250Ain%2520the%2520most%2520challenging%2520scenes%252C%2520where%2520competing%2520methods%2520struggle.%2520We%2520will%250Arelease%2520the%2520code%2520and%2520provide%2520a%2520link%2520to%2520the%2520project%2520page%2520after%2520the%2520review%250Aprocess.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIREN%3A%20Semantic%2C%20Initialization-Free%20Registration%20of%20Multi-Robot%0A%20%20Gaussian%20Splatting%20Maps&entry.906535625=Ola%20Shorinwa%20and%20Jiankai%20Sun%20and%20Mac%20Schwager%20and%20Anirudha%20Majumdar&entry.1292438233=%20%20We%20present%20SIREN%20for%20registration%20of%20multi-robot%20Gaussian%20Splatting%20%28GSplat%29%0Amaps%2C%20with%20zero%20access%20to%20camera%20poses%2C%20images%2C%20and%20inter-map%20transforms%20for%0Ainitialization%20or%20fusion%20of%20local%20submaps.%20To%20realize%20these%20capabilities%2C%20SIREN%0Aharnesses%20the%20versatility%20and%20robustness%20of%20semantics%20in%20three%20critical%20ways%20to%0Aderive%20a%20rigorous%20registration%20pipeline%20for%20multi-robot%20GSplat%20maps.%20First%2C%0ASIREN%20utilizes%20semantics%20to%20identify%20feature-rich%20regions%20of%20the%20local%20maps%0Awhere%20the%20registration%20problem%20is%20better%20posed%2C%20eliminating%20the%20need%20for%20any%0Ainitialization%20which%20is%20generally%20required%20in%20prior%20work.%20Second%2C%20SIREN%0Aidentifies%20candidate%20correspondences%20between%20Gaussians%20in%20the%20local%20maps%20using%0Arobust%20semantic%20features%2C%20constituting%20the%20foundation%20for%20robust%20geometric%0Aoptimization%2C%20coarsely%20aligning%203D%20Gaussian%20primitives%20extracted%20from%20the%20local%0Amaps.%20Third%2C%20this%20key%20step%20enables%20subsequent%20photometric%20refinement%20of%20the%0Atransformation%20between%20the%20submaps%2C%20where%20SIREN%20leverages%20novel-view%20synthesis%0Ain%20GSplat%20maps%20along%20with%20a%20semantics-based%20image%20filter%20to%20compute%20a%0Ahigh-accuracy%20non-rigid%20transformation%20for%20the%20generation%20of%20a%20high-fidelity%0Afused%20map.%20We%20demonstrate%20the%20superior%20performance%20of%20SIREN%20compared%20to%0Acompeting%20baselines%20across%20a%20range%20of%20real-world%20datasets%2C%20and%20in%20particular%2C%0Aacross%20the%20most%20widely-used%20robot%20hardware%20platforms%2C%20including%20a%20manipulator%2C%0Adrone%2C%20and%20quadruped.%20In%20our%20experiments%2C%20SIREN%20achieves%20about%2090x%20smaller%0Arotation%20errors%2C%20300x%20smaller%20translation%20errors%2C%20and%2044x%20smaller%20scale%20errors%0Ain%20the%20most%20challenging%20scenes%2C%20where%20competing%20methods%20struggle.%20We%20will%0Arelease%20the%20code%20and%20provide%20a%20link%20to%20the%20project%20page%20after%20the%20review%0Aprocess.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06519v1&entry.124074799=Read"},
{"title": "Benchmarking Vision-Language Models on Optical Character Recognition in\n  Dynamic Video Environments", "author": "Sankalp Nagaonkar and Augustya Sharma and Ashish Choithani and Ashutosh Trivedi", "abstract": "  This paper introduces an open-source benchmark for evaluating Vision-Language\nModels (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video\nenvironments. We present a curated dataset containing 1,477 manually annotated\nframes spanning diverse domains, including code editors, news broadcasts,\nYouTube videos, and advertisements. Three state of the art VLMs - Claude-3,\nGemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as\nEasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),\nCharacter Error Rate (CER), and Accuracy. Our results highlight the strengths\nand limitations of VLMs in video-based OCR tasks, demonstrating their potential\nto outperform conventional OCR models in many scenarios. However, challenges\nsuch as hallucinations, content security policies, and sensitivity to occluded\nor stylized text remain. The dataset and benchmarking framework are publicly\navailable to foster further research.\n", "link": "http://arxiv.org/abs/2502.06445v1", "date": "2025-02-10", "relevancy": 2.9604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6119}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Vision-Language%20Models%20on%20Optical%20Character%20Recognition%20in%0A%20%20Dynamic%20Video%20Environments&body=Title%3A%20Benchmarking%20Vision-Language%20Models%20on%20Optical%20Character%20Recognition%20in%0A%20%20Dynamic%20Video%20Environments%0AAuthor%3A%20Sankalp%20Nagaonkar%20and%20Augustya%20Sharma%20and%20Ashish%20Choithani%20and%20Ashutosh%20Trivedi%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20open-source%20benchmark%20for%20evaluating%20Vision-Language%0AModels%20%28VLMs%29%20on%20Optical%20Character%20Recognition%20%28OCR%29%20tasks%20in%20dynamic%20video%0Aenvironments.%20We%20present%20a%20curated%20dataset%20containing%201%2C477%20manually%20annotated%0Aframes%20spanning%20diverse%20domains%2C%20including%20code%20editors%2C%20news%20broadcasts%2C%0AYouTube%20videos%2C%20and%20advertisements.%20Three%20state%20of%20the%20art%20VLMs%20-%20Claude-3%2C%0AGemini-1.5%2C%20and%20GPT-4o%20are%20benchmarked%20against%20traditional%20OCR%20systems%20such%20as%0AEasyOCR%20and%20RapidOCR.%20Evaluation%20metrics%20include%20Word%20Error%20Rate%20%28WER%29%2C%0ACharacter%20Error%20Rate%20%28CER%29%2C%20and%20Accuracy.%20Our%20results%20highlight%20the%20strengths%0Aand%20limitations%20of%20VLMs%20in%20video-based%20OCR%20tasks%2C%20demonstrating%20their%20potential%0Ato%20outperform%20conventional%20OCR%20models%20in%20many%20scenarios.%20However%2C%20challenges%0Asuch%20as%20hallucinations%2C%20content%20security%20policies%2C%20and%20sensitivity%20to%20occluded%0Aor%20stylized%20text%20remain.%20The%20dataset%20and%20benchmarking%20framework%20are%20publicly%0Aavailable%20to%20foster%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Vision-Language%2520Models%2520on%2520Optical%2520Character%2520Recognition%2520in%250A%2520%2520Dynamic%2520Video%2520Environments%26entry.906535625%3DSankalp%2520Nagaonkar%2520and%2520Augustya%2520Sharma%2520and%2520Ashish%2520Choithani%2520and%2520Ashutosh%2520Trivedi%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520open-source%2520benchmark%2520for%2520evaluating%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520on%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520tasks%2520in%2520dynamic%2520video%250Aenvironments.%2520We%2520present%2520a%2520curated%2520dataset%2520containing%25201%252C477%2520manually%2520annotated%250Aframes%2520spanning%2520diverse%2520domains%252C%2520including%2520code%2520editors%252C%2520news%2520broadcasts%252C%250AYouTube%2520videos%252C%2520and%2520advertisements.%2520Three%2520state%2520of%2520the%2520art%2520VLMs%2520-%2520Claude-3%252C%250AGemini-1.5%252C%2520and%2520GPT-4o%2520are%2520benchmarked%2520against%2520traditional%2520OCR%2520systems%2520such%2520as%250AEasyOCR%2520and%2520RapidOCR.%2520Evaluation%2520metrics%2520include%2520Word%2520Error%2520Rate%2520%2528WER%2529%252C%250ACharacter%2520Error%2520Rate%2520%2528CER%2529%252C%2520and%2520Accuracy.%2520Our%2520results%2520highlight%2520the%2520strengths%250Aand%2520limitations%2520of%2520VLMs%2520in%2520video-based%2520OCR%2520tasks%252C%2520demonstrating%2520their%2520potential%250Ato%2520outperform%2520conventional%2520OCR%2520models%2520in%2520many%2520scenarios.%2520However%252C%2520challenges%250Asuch%2520as%2520hallucinations%252C%2520content%2520security%2520policies%252C%2520and%2520sensitivity%2520to%2520occluded%250Aor%2520stylized%2520text%2520remain.%2520The%2520dataset%2520and%2520benchmarking%2520framework%2520are%2520publicly%250Aavailable%2520to%2520foster%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Vision-Language%20Models%20on%20Optical%20Character%20Recognition%20in%0A%20%20Dynamic%20Video%20Environments&entry.906535625=Sankalp%20Nagaonkar%20and%20Augustya%20Sharma%20and%20Ashish%20Choithani%20and%20Ashutosh%20Trivedi&entry.1292438233=%20%20This%20paper%20introduces%20an%20open-source%20benchmark%20for%20evaluating%20Vision-Language%0AModels%20%28VLMs%29%20on%20Optical%20Character%20Recognition%20%28OCR%29%20tasks%20in%20dynamic%20video%0Aenvironments.%20We%20present%20a%20curated%20dataset%20containing%201%2C477%20manually%20annotated%0Aframes%20spanning%20diverse%20domains%2C%20including%20code%20editors%2C%20news%20broadcasts%2C%0AYouTube%20videos%2C%20and%20advertisements.%20Three%20state%20of%20the%20art%20VLMs%20-%20Claude-3%2C%0AGemini-1.5%2C%20and%20GPT-4o%20are%20benchmarked%20against%20traditional%20OCR%20systems%20such%20as%0AEasyOCR%20and%20RapidOCR.%20Evaluation%20metrics%20include%20Word%20Error%20Rate%20%28WER%29%2C%0ACharacter%20Error%20Rate%20%28CER%29%2C%20and%20Accuracy.%20Our%20results%20highlight%20the%20strengths%0Aand%20limitations%20of%20VLMs%20in%20video-based%20OCR%20tasks%2C%20demonstrating%20their%20potential%0Ato%20outperform%20conventional%20OCR%20models%20in%20many%20scenarios.%20However%2C%20challenges%0Asuch%20as%20hallucinations%2C%20content%20security%20policies%2C%20and%20sensitivity%20to%20occluded%0Aor%20stylized%20text%20remain.%20The%20dataset%20and%20benchmarking%20framework%20are%20publicly%0Aavailable%20to%20foster%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06445v1&entry.124074799=Read"},
{"title": "FOCUS - Multi-View Foot Reconstruction From Synthetically Trained Dense\n  Correspondences", "author": "Oliver Boyne and Roberto Cipolla", "abstract": "  Surface reconstruction from multiple, calibrated images is a challenging task\n- often requiring a large number of collected images with significant overlap.\nWe look at the specific case of human foot reconstruction. As with previous\nsuccessful foot reconstruction work, we seek to extract rich per-pixel geometry\ncues from multi-view RGB images, and fuse these into a final 3D object. Our\nmethod, FOCUS, tackles this problem with 3 main contributions: (i) SynFoot2, an\nextension of an existing synthetic foot dataset to include a new data type:\ndense correspondence with the parameterized foot model FIND; (ii) an\nuncertainty-aware dense correspondence predictor trained on our synthetic\ndataset; (iii) two methods for reconstructing a 3D surface from dense\ncorrespondence predictions: one inspired by Structure-from-Motion, and one\noptimization-based using the FIND model. We show that our reconstruction\nachieves state-of-the-art reconstruction quality in a few-view setting,\nperforming comparably to state-of-the-art when many views are available, and\nruns substantially faster. We release our synthetic dataset to the research\ncommunity. Code is available at: https://github.com/OllieBoyne/FOCUS\n", "link": "http://arxiv.org/abs/2502.06367v1", "date": "2025-02-10", "relevancy": 2.9232, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5889}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5889}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOCUS%20-%20Multi-View%20Foot%20Reconstruction%20From%20Synthetically%20Trained%20Dense%0A%20%20Correspondences&body=Title%3A%20FOCUS%20-%20Multi-View%20Foot%20Reconstruction%20From%20Synthetically%20Trained%20Dense%0A%20%20Correspondences%0AAuthor%3A%20Oliver%20Boyne%20and%20Roberto%20Cipolla%0AAbstract%3A%20%20%20Surface%20reconstruction%20from%20multiple%2C%20calibrated%20images%20is%20a%20challenging%20task%0A-%20often%20requiring%20a%20large%20number%20of%20collected%20images%20with%20significant%20overlap.%0AWe%20look%20at%20the%20specific%20case%20of%20human%20foot%20reconstruction.%20As%20with%20previous%0Asuccessful%20foot%20reconstruction%20work%2C%20we%20seek%20to%20extract%20rich%20per-pixel%20geometry%0Acues%20from%20multi-view%20RGB%20images%2C%20and%20fuse%20these%20into%20a%20final%203D%20object.%20Our%0Amethod%2C%20FOCUS%2C%20tackles%20this%20problem%20with%203%20main%20contributions%3A%20%28i%29%20SynFoot2%2C%20an%0Aextension%20of%20an%20existing%20synthetic%20foot%20dataset%20to%20include%20a%20new%20data%20type%3A%0Adense%20correspondence%20with%20the%20parameterized%20foot%20model%20FIND%3B%20%28ii%29%20an%0Auncertainty-aware%20dense%20correspondence%20predictor%20trained%20on%20our%20synthetic%0Adataset%3B%20%28iii%29%20two%20methods%20for%20reconstructing%20a%203D%20surface%20from%20dense%0Acorrespondence%20predictions%3A%20one%20inspired%20by%20Structure-from-Motion%2C%20and%20one%0Aoptimization-based%20using%20the%20FIND%20model.%20We%20show%20that%20our%20reconstruction%0Aachieves%20state-of-the-art%20reconstruction%20quality%20in%20a%20few-view%20setting%2C%0Aperforming%20comparably%20to%20state-of-the-art%20when%20many%20views%20are%20available%2C%20and%0Aruns%20substantially%20faster.%20We%20release%20our%20synthetic%20dataset%20to%20the%20research%0Acommunity.%20Code%20is%20available%20at%3A%20https%3A//github.com/OllieBoyne/FOCUS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOCUS%2520-%2520Multi-View%2520Foot%2520Reconstruction%2520From%2520Synthetically%2520Trained%2520Dense%250A%2520%2520Correspondences%26entry.906535625%3DOliver%2520Boyne%2520and%2520Roberto%2520Cipolla%26entry.1292438233%3D%2520%2520Surface%2520reconstruction%2520from%2520multiple%252C%2520calibrated%2520images%2520is%2520a%2520challenging%2520task%250A-%2520often%2520requiring%2520a%2520large%2520number%2520of%2520collected%2520images%2520with%2520significant%2520overlap.%250AWe%2520look%2520at%2520the%2520specific%2520case%2520of%2520human%2520foot%2520reconstruction.%2520As%2520with%2520previous%250Asuccessful%2520foot%2520reconstruction%2520work%252C%2520we%2520seek%2520to%2520extract%2520rich%2520per-pixel%2520geometry%250Acues%2520from%2520multi-view%2520RGB%2520images%252C%2520and%2520fuse%2520these%2520into%2520a%2520final%25203D%2520object.%2520Our%250Amethod%252C%2520FOCUS%252C%2520tackles%2520this%2520problem%2520with%25203%2520main%2520contributions%253A%2520%2528i%2529%2520SynFoot2%252C%2520an%250Aextension%2520of%2520an%2520existing%2520synthetic%2520foot%2520dataset%2520to%2520include%2520a%2520new%2520data%2520type%253A%250Adense%2520correspondence%2520with%2520the%2520parameterized%2520foot%2520model%2520FIND%253B%2520%2528ii%2529%2520an%250Auncertainty-aware%2520dense%2520correspondence%2520predictor%2520trained%2520on%2520our%2520synthetic%250Adataset%253B%2520%2528iii%2529%2520two%2520methods%2520for%2520reconstructing%2520a%25203D%2520surface%2520from%2520dense%250Acorrespondence%2520predictions%253A%2520one%2520inspired%2520by%2520Structure-from-Motion%252C%2520and%2520one%250Aoptimization-based%2520using%2520the%2520FIND%2520model.%2520We%2520show%2520that%2520our%2520reconstruction%250Aachieves%2520state-of-the-art%2520reconstruction%2520quality%2520in%2520a%2520few-view%2520setting%252C%250Aperforming%2520comparably%2520to%2520state-of-the-art%2520when%2520many%2520views%2520are%2520available%252C%2520and%250Aruns%2520substantially%2520faster.%2520We%2520release%2520our%2520synthetic%2520dataset%2520to%2520the%2520research%250Acommunity.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/OllieBoyne/FOCUS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOCUS%20-%20Multi-View%20Foot%20Reconstruction%20From%20Synthetically%20Trained%20Dense%0A%20%20Correspondences&entry.906535625=Oliver%20Boyne%20and%20Roberto%20Cipolla&entry.1292438233=%20%20Surface%20reconstruction%20from%20multiple%2C%20calibrated%20images%20is%20a%20challenging%20task%0A-%20often%20requiring%20a%20large%20number%20of%20collected%20images%20with%20significant%20overlap.%0AWe%20look%20at%20the%20specific%20case%20of%20human%20foot%20reconstruction.%20As%20with%20previous%0Asuccessful%20foot%20reconstruction%20work%2C%20we%20seek%20to%20extract%20rich%20per-pixel%20geometry%0Acues%20from%20multi-view%20RGB%20images%2C%20and%20fuse%20these%20into%20a%20final%203D%20object.%20Our%0Amethod%2C%20FOCUS%2C%20tackles%20this%20problem%20with%203%20main%20contributions%3A%20%28i%29%20SynFoot2%2C%20an%0Aextension%20of%20an%20existing%20synthetic%20foot%20dataset%20to%20include%20a%20new%20data%20type%3A%0Adense%20correspondence%20with%20the%20parameterized%20foot%20model%20FIND%3B%20%28ii%29%20an%0Auncertainty-aware%20dense%20correspondence%20predictor%20trained%20on%20our%20synthetic%0Adataset%3B%20%28iii%29%20two%20methods%20for%20reconstructing%20a%203D%20surface%20from%20dense%0Acorrespondence%20predictions%3A%20one%20inspired%20by%20Structure-from-Motion%2C%20and%20one%0Aoptimization-based%20using%20the%20FIND%20model.%20We%20show%20that%20our%20reconstruction%0Aachieves%20state-of-the-art%20reconstruction%20quality%20in%20a%20few-view%20setting%2C%0Aperforming%20comparably%20to%20state-of-the-art%20when%20many%20views%20are%20available%2C%20and%0Aruns%20substantially%20faster.%20We%20release%20our%20synthetic%20dataset%20to%20the%20research%0Acommunity.%20Code%20is%20available%20at%3A%20https%3A//github.com/OllieBoyne/FOCUS%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06367v1&entry.124074799=Read"},
{"title": "Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision\n  Models", "author": "Samuel Stevens and Wei-Lun Chao and Tanya Berger-Wolf and Yu Su", "abstract": "  To truly understand vision models, we must not only interpret their learned\nfeatures but also validate these interpretations through controlled\nexperiments. Current approaches either provide interpretable features without\nthe ability to test their causal influence, or enable model editing without\ninterpretable controls. We present a unified framework using sparse\nautoencoders (SAEs) that bridges this gap, allowing us to discover\nhuman-interpretable visual features and precisely manipulate them to test\nhypotheses about model behavior. By applying our method to state-of-the-art\nvision models, we reveal key differences in the semantic abstractions learned\nby models with different pre-training objectives. We then demonstrate the\npractical usage of our framework through controlled interventions across\nmultiple vision tasks. We show that SAEs can reliably identify and manipulate\ninterpretable visual features without model re-training, providing a powerful\ntool for understanding and controlling vision model behavior. We provide code,\ndemos and models on our project website: https://osu-nlp-group.github.io/SAE-V.\n", "link": "http://arxiv.org/abs/2502.06755v1", "date": "2025-02-10", "relevancy": 2.8973, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6006}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20for%20Scientifically%20Rigorous%20Interpretation%20of%20Vision%0A%20%20Models&body=Title%3A%20Sparse%20Autoencoders%20for%20Scientifically%20Rigorous%20Interpretation%20of%20Vision%0A%20%20Models%0AAuthor%3A%20Samuel%20Stevens%20and%20Wei-Lun%20Chao%20and%20Tanya%20Berger-Wolf%20and%20Yu%20Su%0AAbstract%3A%20%20%20To%20truly%20understand%20vision%20models%2C%20we%20must%20not%20only%20interpret%20their%20learned%0Afeatures%20but%20also%20validate%20these%20interpretations%20through%20controlled%0Aexperiments.%20Current%20approaches%20either%20provide%20interpretable%20features%20without%0Athe%20ability%20to%20test%20their%20causal%20influence%2C%20or%20enable%20model%20editing%20without%0Ainterpretable%20controls.%20We%20present%20a%20unified%20framework%20using%20sparse%0Aautoencoders%20%28SAEs%29%20that%20bridges%20this%20gap%2C%20allowing%20us%20to%20discover%0Ahuman-interpretable%20visual%20features%20and%20precisely%20manipulate%20them%20to%20test%0Ahypotheses%20about%20model%20behavior.%20By%20applying%20our%20method%20to%20state-of-the-art%0Avision%20models%2C%20we%20reveal%20key%20differences%20in%20the%20semantic%20abstractions%20learned%0Aby%20models%20with%20different%20pre-training%20objectives.%20We%20then%20demonstrate%20the%0Apractical%20usage%20of%20our%20framework%20through%20controlled%20interventions%20across%0Amultiple%20vision%20tasks.%20We%20show%20that%20SAEs%20can%20reliably%20identify%20and%20manipulate%0Ainterpretable%20visual%20features%20without%20model%20re-training%2C%20providing%20a%20powerful%0Atool%20for%20understanding%20and%20controlling%20vision%20model%20behavior.%20We%20provide%20code%2C%0Ademos%20and%20models%20on%20our%20project%20website%3A%20https%3A//osu-nlp-group.github.io/SAE-V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520for%2520Scientifically%2520Rigorous%2520Interpretation%2520of%2520Vision%250A%2520%2520Models%26entry.906535625%3DSamuel%2520Stevens%2520and%2520Wei-Lun%2520Chao%2520and%2520Tanya%2520Berger-Wolf%2520and%2520Yu%2520Su%26entry.1292438233%3D%2520%2520To%2520truly%2520understand%2520vision%2520models%252C%2520we%2520must%2520not%2520only%2520interpret%2520their%2520learned%250Afeatures%2520but%2520also%2520validate%2520these%2520interpretations%2520through%2520controlled%250Aexperiments.%2520Current%2520approaches%2520either%2520provide%2520interpretable%2520features%2520without%250Athe%2520ability%2520to%2520test%2520their%2520causal%2520influence%252C%2520or%2520enable%2520model%2520editing%2520without%250Ainterpretable%2520controls.%2520We%2520present%2520a%2520unified%2520framework%2520using%2520sparse%250Aautoencoders%2520%2528SAEs%2529%2520that%2520bridges%2520this%2520gap%252C%2520allowing%2520us%2520to%2520discover%250Ahuman-interpretable%2520visual%2520features%2520and%2520precisely%2520manipulate%2520them%2520to%2520test%250Ahypotheses%2520about%2520model%2520behavior.%2520By%2520applying%2520our%2520method%2520to%2520state-of-the-art%250Avision%2520models%252C%2520we%2520reveal%2520key%2520differences%2520in%2520the%2520semantic%2520abstractions%2520learned%250Aby%2520models%2520with%2520different%2520pre-training%2520objectives.%2520We%2520then%2520demonstrate%2520the%250Apractical%2520usage%2520of%2520our%2520framework%2520through%2520controlled%2520interventions%2520across%250Amultiple%2520vision%2520tasks.%2520We%2520show%2520that%2520SAEs%2520can%2520reliably%2520identify%2520and%2520manipulate%250Ainterpretable%2520visual%2520features%2520without%2520model%2520re-training%252C%2520providing%2520a%2520powerful%250Atool%2520for%2520understanding%2520and%2520controlling%2520vision%2520model%2520behavior.%2520We%2520provide%2520code%252C%250Ademos%2520and%2520models%2520on%2520our%2520project%2520website%253A%2520https%253A//osu-nlp-group.github.io/SAE-V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20for%20Scientifically%20Rigorous%20Interpretation%20of%20Vision%0A%20%20Models&entry.906535625=Samuel%20Stevens%20and%20Wei-Lun%20Chao%20and%20Tanya%20Berger-Wolf%20and%20Yu%20Su&entry.1292438233=%20%20To%20truly%20understand%20vision%20models%2C%20we%20must%20not%20only%20interpret%20their%20learned%0Afeatures%20but%20also%20validate%20these%20interpretations%20through%20controlled%0Aexperiments.%20Current%20approaches%20either%20provide%20interpretable%20features%20without%0Athe%20ability%20to%20test%20their%20causal%20influence%2C%20or%20enable%20model%20editing%20without%0Ainterpretable%20controls.%20We%20present%20a%20unified%20framework%20using%20sparse%0Aautoencoders%20%28SAEs%29%20that%20bridges%20this%20gap%2C%20allowing%20us%20to%20discover%0Ahuman-interpretable%20visual%20features%20and%20precisely%20manipulate%20them%20to%20test%0Ahypotheses%20about%20model%20behavior.%20By%20applying%20our%20method%20to%20state-of-the-art%0Avision%20models%2C%20we%20reveal%20key%20differences%20in%20the%20semantic%20abstractions%20learned%0Aby%20models%20with%20different%20pre-training%20objectives.%20We%20then%20demonstrate%20the%0Apractical%20usage%20of%20our%20framework%20through%20controlled%20interventions%20across%0Amultiple%20vision%20tasks.%20We%20show%20that%20SAEs%20can%20reliably%20identify%20and%20manipulate%0Ainterpretable%20visual%20features%20without%20model%20re-training%2C%20providing%20a%20powerful%0Atool%20for%20understanding%20and%20controlling%20vision%20model%20behavior.%20We%20provide%20code%2C%0Ademos%20and%20models%20on%20our%20project%20website%3A%20https%3A//osu-nlp-group.github.io/SAE-V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06755v1&entry.124074799=Read"},
{"title": "Visual Prompt Engineering for Vision Language Models in Radiology", "author": "Stefan Denner and Markus Bujotzek and Dimitrios Bounias and David Zimmerer and Raphael Stock and Klaus Maier-Hein", "abstract": "  Medical image classification plays a crucial role in clinical\ndecision-making, yet most models are constrained to a fixed set of predefined\nclasses, limiting their adaptability to new conditions. Contrastive\nLanguage-Image Pretraining (CLIP) offers a promising solution by enabling\nzero-shot classification through multimodal large-scale pretraining. However,\nwhile CLIP effectively captures global image content, radiology requires a more\nlocalized focus on specific pathology regions to enhance both interpretability\nand diagnostic accuracy. To address this, we explore the potential of\nincorporating visual cues into zero-shot classification, embedding visual\nmarkers $\\unicode{x2013}$ such as arrows, bounding boxes, and circles\n$\\unicode{x2013}$ directly into radiological images to guide model attention.\nEvaluating across four public chest X-ray datasets, we demonstrate that visual\nmarkers improve AUROC by up to 0.185, highlighting their effectiveness in\nenhancing classification performance. Furthermore, attention map analysis\nconfirms that visual cues help models focus on clinically relevant areas,\nleading to more interpretable predictions. To support further research, we use\npublic datasets and will release our code and preprocessing pipeline, providing\na reference point for future work on localized classification in medical\nimaging.\n", "link": "http://arxiv.org/abs/2408.15802v2", "date": "2025-02-10", "relevancy": 2.876, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Prompt%20Engineering%20for%20Vision%20Language%20Models%20in%20Radiology&body=Title%3A%20Visual%20Prompt%20Engineering%20for%20Vision%20Language%20Models%20in%20Radiology%0AAuthor%3A%20Stefan%20Denner%20and%20Markus%20Bujotzek%20and%20Dimitrios%20Bounias%20and%20David%20Zimmerer%20and%20Raphael%20Stock%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Medical%20image%20classification%20plays%20a%20crucial%20role%20in%20clinical%0Adecision-making%2C%20yet%20most%20models%20are%20constrained%20to%20a%20fixed%20set%20of%20predefined%0Aclasses%2C%20limiting%20their%20adaptability%20to%20new%20conditions.%20Contrastive%0ALanguage-Image%20Pretraining%20%28CLIP%29%20offers%20a%20promising%20solution%20by%20enabling%0Azero-shot%20classification%20through%20multimodal%20large-scale%20pretraining.%20However%2C%0Awhile%20CLIP%20effectively%20captures%20global%20image%20content%2C%20radiology%20requires%20a%20more%0Alocalized%20focus%20on%20specific%20pathology%20regions%20to%20enhance%20both%20interpretability%0Aand%20diagnostic%20accuracy.%20To%20address%20this%2C%20we%20explore%20the%20potential%20of%0Aincorporating%20visual%20cues%20into%20zero-shot%20classification%2C%20embedding%20visual%0Amarkers%20%24%5Cunicode%7Bx2013%7D%24%20such%20as%20arrows%2C%20bounding%20boxes%2C%20and%20circles%0A%24%5Cunicode%7Bx2013%7D%24%20directly%20into%20radiological%20images%20to%20guide%20model%20attention.%0AEvaluating%20across%20four%20public%20chest%20X-ray%20datasets%2C%20we%20demonstrate%20that%20visual%0Amarkers%20improve%20AUROC%20by%20up%20to%200.185%2C%20highlighting%20their%20effectiveness%20in%0Aenhancing%20classification%20performance.%20Furthermore%2C%20attention%20map%20analysis%0Aconfirms%20that%20visual%20cues%20help%20models%20focus%20on%20clinically%20relevant%20areas%2C%0Aleading%20to%20more%20interpretable%20predictions.%20To%20support%20further%20research%2C%20we%20use%0Apublic%20datasets%20and%20will%20release%20our%20code%20and%20preprocessing%20pipeline%2C%20providing%0Aa%20reference%20point%20for%20future%20work%20on%20localized%20classification%20in%20medical%0Aimaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Prompt%2520Engineering%2520for%2520Vision%2520Language%2520Models%2520in%2520Radiology%26entry.906535625%3DStefan%2520Denner%2520and%2520Markus%2520Bujotzek%2520and%2520Dimitrios%2520Bounias%2520and%2520David%2520Zimmerer%2520and%2520Raphael%2520Stock%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520Medical%2520image%2520classification%2520plays%2520a%2520crucial%2520role%2520in%2520clinical%250Adecision-making%252C%2520yet%2520most%2520models%2520are%2520constrained%2520to%2520a%2520fixed%2520set%2520of%2520predefined%250Aclasses%252C%2520limiting%2520their%2520adaptability%2520to%2520new%2520conditions.%2520Contrastive%250ALanguage-Image%2520Pretraining%2520%2528CLIP%2529%2520offers%2520a%2520promising%2520solution%2520by%2520enabling%250Azero-shot%2520classification%2520through%2520multimodal%2520large-scale%2520pretraining.%2520However%252C%250Awhile%2520CLIP%2520effectively%2520captures%2520global%2520image%2520content%252C%2520radiology%2520requires%2520a%2520more%250Alocalized%2520focus%2520on%2520specific%2520pathology%2520regions%2520to%2520enhance%2520both%2520interpretability%250Aand%2520diagnostic%2520accuracy.%2520To%2520address%2520this%252C%2520we%2520explore%2520the%2520potential%2520of%250Aincorporating%2520visual%2520cues%2520into%2520zero-shot%2520classification%252C%2520embedding%2520visual%250Amarkers%2520%2524%255Cunicode%257Bx2013%257D%2524%2520such%2520as%2520arrows%252C%2520bounding%2520boxes%252C%2520and%2520circles%250A%2524%255Cunicode%257Bx2013%257D%2524%2520directly%2520into%2520radiological%2520images%2520to%2520guide%2520model%2520attention.%250AEvaluating%2520across%2520four%2520public%2520chest%2520X-ray%2520datasets%252C%2520we%2520demonstrate%2520that%2520visual%250Amarkers%2520improve%2520AUROC%2520by%2520up%2520to%25200.185%252C%2520highlighting%2520their%2520effectiveness%2520in%250Aenhancing%2520classification%2520performance.%2520Furthermore%252C%2520attention%2520map%2520analysis%250Aconfirms%2520that%2520visual%2520cues%2520help%2520models%2520focus%2520on%2520clinically%2520relevant%2520areas%252C%250Aleading%2520to%2520more%2520interpretable%2520predictions.%2520To%2520support%2520further%2520research%252C%2520we%2520use%250Apublic%2520datasets%2520and%2520will%2520release%2520our%2520code%2520and%2520preprocessing%2520pipeline%252C%2520providing%250Aa%2520reference%2520point%2520for%2520future%2520work%2520on%2520localized%2520classification%2520in%2520medical%250Aimaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Prompt%20Engineering%20for%20Vision%20Language%20Models%20in%20Radiology&entry.906535625=Stefan%20Denner%20and%20Markus%20Bujotzek%20and%20Dimitrios%20Bounias%20and%20David%20Zimmerer%20and%20Raphael%20Stock%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Medical%20image%20classification%20plays%20a%20crucial%20role%20in%20clinical%0Adecision-making%2C%20yet%20most%20models%20are%20constrained%20to%20a%20fixed%20set%20of%20predefined%0Aclasses%2C%20limiting%20their%20adaptability%20to%20new%20conditions.%20Contrastive%0ALanguage-Image%20Pretraining%20%28CLIP%29%20offers%20a%20promising%20solution%20by%20enabling%0Azero-shot%20classification%20through%20multimodal%20large-scale%20pretraining.%20However%2C%0Awhile%20CLIP%20effectively%20captures%20global%20image%20content%2C%20radiology%20requires%20a%20more%0Alocalized%20focus%20on%20specific%20pathology%20regions%20to%20enhance%20both%20interpretability%0Aand%20diagnostic%20accuracy.%20To%20address%20this%2C%20we%20explore%20the%20potential%20of%0Aincorporating%20visual%20cues%20into%20zero-shot%20classification%2C%20embedding%20visual%0Amarkers%20%24%5Cunicode%7Bx2013%7D%24%20such%20as%20arrows%2C%20bounding%20boxes%2C%20and%20circles%0A%24%5Cunicode%7Bx2013%7D%24%20directly%20into%20radiological%20images%20to%20guide%20model%20attention.%0AEvaluating%20across%20four%20public%20chest%20X-ray%20datasets%2C%20we%20demonstrate%20that%20visual%0Amarkers%20improve%20AUROC%20by%20up%20to%200.185%2C%20highlighting%20their%20effectiveness%20in%0Aenhancing%20classification%20performance.%20Furthermore%2C%20attention%20map%20analysis%0Aconfirms%20that%20visual%20cues%20help%20models%20focus%20on%20clinically%20relevant%20areas%2C%0Aleading%20to%20more%20interpretable%20predictions.%20To%20support%20further%20research%2C%20we%20use%0Apublic%20datasets%20and%20will%20release%20our%20code%20and%20preprocessing%20pipeline%2C%20providing%0Aa%20reference%20point%20for%20future%20work%20on%20localized%20classification%20in%20medical%0Aimaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15802v2&entry.124074799=Read"},
{"title": "TACO: Training-free Sound Prompted Segmentation via Semantically\n  Constrained Audio-visual CO-factorization", "author": "Hugo Malard and Michel Olvera and Stephane Lathuiliere and Slim Essid", "abstract": "  Large-scale pre-trained audio and image models demonstrate an unprecedented\ndegree of generalization, making them suitable for a wide range of\napplications. Here, we tackle the specific task of sound-prompted segmentation,\naiming to segment image regions corresponding to objects heard in an audio\nsignal. Most existing approaches tackle this problem by fine-tuning pre-trained\nmodels or by training additional modules specifically for the task. We adopt a\ndifferent strategy: we introduce a training-free approach that leverages\nNon-negative Matrix Factorization (NMF) to co-factorize audio and visual\nfeatures from pre-trained models so as to reveal shared interpretable concepts.\nThese concepts are passed on to an open-vocabulary segmentation model for\nprecise segmentation maps. By using frozen pre-trained models, our method\nachieves high generalization and establishes state-of-the-art performance in\nunsupervised sound-prompted segmentation, significantly surpassing previous\nunsupervised methods.\n", "link": "http://arxiv.org/abs/2412.01488v2", "date": "2025-02-10", "relevancy": 2.8414, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACO%3A%20Training-free%20Sound%20Prompted%20Segmentation%20via%20Semantically%0A%20%20Constrained%20Audio-visual%20CO-factorization&body=Title%3A%20TACO%3A%20Training-free%20Sound%20Prompted%20Segmentation%20via%20Semantically%0A%20%20Constrained%20Audio-visual%20CO-factorization%0AAuthor%3A%20Hugo%20Malard%20and%20Michel%20Olvera%20and%20Stephane%20Lathuiliere%20and%20Slim%20Essid%0AAbstract%3A%20%20%20Large-scale%20pre-trained%20audio%20and%20image%20models%20demonstrate%20an%20unprecedented%0Adegree%20of%20generalization%2C%20making%20them%20suitable%20for%20a%20wide%20range%20of%0Aapplications.%20Here%2C%20we%20tackle%20the%20specific%20task%20of%20sound-prompted%20segmentation%2C%0Aaiming%20to%20segment%20image%20regions%20corresponding%20to%20objects%20heard%20in%20an%20audio%0Asignal.%20Most%20existing%20approaches%20tackle%20this%20problem%20by%20fine-tuning%20pre-trained%0Amodels%20or%20by%20training%20additional%20modules%20specifically%20for%20the%20task.%20We%20adopt%20a%0Adifferent%20strategy%3A%20we%20introduce%20a%20training-free%20approach%20that%20leverages%0ANon-negative%20Matrix%20Factorization%20%28NMF%29%20to%20co-factorize%20audio%20and%20visual%0Afeatures%20from%20pre-trained%20models%20so%20as%20to%20reveal%20shared%20interpretable%20concepts.%0AThese%20concepts%20are%20passed%20on%20to%20an%20open-vocabulary%20segmentation%20model%20for%0Aprecise%20segmentation%20maps.%20By%20using%20frozen%20pre-trained%20models%2C%20our%20method%0Aachieves%20high%20generalization%20and%20establishes%20state-of-the-art%20performance%20in%0Aunsupervised%20sound-prompted%20segmentation%2C%20significantly%20surpassing%20previous%0Aunsupervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACO%253A%2520Training-free%2520Sound%2520Prompted%2520Segmentation%2520via%2520Semantically%250A%2520%2520Constrained%2520Audio-visual%2520CO-factorization%26entry.906535625%3DHugo%2520Malard%2520and%2520Michel%2520Olvera%2520and%2520Stephane%2520Lathuiliere%2520and%2520Slim%2520Essid%26entry.1292438233%3D%2520%2520Large-scale%2520pre-trained%2520audio%2520and%2520image%2520models%2520demonstrate%2520an%2520unprecedented%250Adegree%2520of%2520generalization%252C%2520making%2520them%2520suitable%2520for%2520a%2520wide%2520range%2520of%250Aapplications.%2520Here%252C%2520we%2520tackle%2520the%2520specific%2520task%2520of%2520sound-prompted%2520segmentation%252C%250Aaiming%2520to%2520segment%2520image%2520regions%2520corresponding%2520to%2520objects%2520heard%2520in%2520an%2520audio%250Asignal.%2520Most%2520existing%2520approaches%2520tackle%2520this%2520problem%2520by%2520fine-tuning%2520pre-trained%250Amodels%2520or%2520by%2520training%2520additional%2520modules%2520specifically%2520for%2520the%2520task.%2520We%2520adopt%2520a%250Adifferent%2520strategy%253A%2520we%2520introduce%2520a%2520training-free%2520approach%2520that%2520leverages%250ANon-negative%2520Matrix%2520Factorization%2520%2528NMF%2529%2520to%2520co-factorize%2520audio%2520and%2520visual%250Afeatures%2520from%2520pre-trained%2520models%2520so%2520as%2520to%2520reveal%2520shared%2520interpretable%2520concepts.%250AThese%2520concepts%2520are%2520passed%2520on%2520to%2520an%2520open-vocabulary%2520segmentation%2520model%2520for%250Aprecise%2520segmentation%2520maps.%2520By%2520using%2520frozen%2520pre-trained%2520models%252C%2520our%2520method%250Aachieves%2520high%2520generalization%2520and%2520establishes%2520state-of-the-art%2520performance%2520in%250Aunsupervised%2520sound-prompted%2520segmentation%252C%2520significantly%2520surpassing%2520previous%250Aunsupervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACO%3A%20Training-free%20Sound%20Prompted%20Segmentation%20via%20Semantically%0A%20%20Constrained%20Audio-visual%20CO-factorization&entry.906535625=Hugo%20Malard%20and%20Michel%20Olvera%20and%20Stephane%20Lathuiliere%20and%20Slim%20Essid&entry.1292438233=%20%20Large-scale%20pre-trained%20audio%20and%20image%20models%20demonstrate%20an%20unprecedented%0Adegree%20of%20generalization%2C%20making%20them%20suitable%20for%20a%20wide%20range%20of%0Aapplications.%20Here%2C%20we%20tackle%20the%20specific%20task%20of%20sound-prompted%20segmentation%2C%0Aaiming%20to%20segment%20image%20regions%20corresponding%20to%20objects%20heard%20in%20an%20audio%0Asignal.%20Most%20existing%20approaches%20tackle%20this%20problem%20by%20fine-tuning%20pre-trained%0Amodels%20or%20by%20training%20additional%20modules%20specifically%20for%20the%20task.%20We%20adopt%20a%0Adifferent%20strategy%3A%20we%20introduce%20a%20training-free%20approach%20that%20leverages%0ANon-negative%20Matrix%20Factorization%20%28NMF%29%20to%20co-factorize%20audio%20and%20visual%0Afeatures%20from%20pre-trained%20models%20so%20as%20to%20reveal%20shared%20interpretable%20concepts.%0AThese%20concepts%20are%20passed%20on%20to%20an%20open-vocabulary%20segmentation%20model%20for%0Aprecise%20segmentation%20maps.%20By%20using%20frozen%20pre-trained%20models%2C%20our%20method%0Aachieves%20high%20generalization%20and%20establishes%20state-of-the-art%20performance%20in%0Aunsupervised%20sound-prompted%20segmentation%2C%20significantly%20surpassing%20previous%0Aunsupervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01488v2&entry.124074799=Read"},
{"title": "KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission\n  for Visual Classification", "author": "Yue Zhu and Haiwen Diao and Shang Gao and Long Chen and Huchuan Lu", "abstract": "  Fine-tuning pre-trained vision models for specific tasks is a common practice\nin computer vision. However, this process becomes more expensive as models grow\nlarger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged\nas a popular solution to improve training efficiency and reduce storage needs\nby tuning additional low-rank modules within pre-trained backbones. Despite\ntheir advantages, they struggle with limited representation capabilities and\nmisalignment with pre-trained intermediate features. To address these issues,\nwe introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling\nTransmission (KARST) for various recognition tasks. Specifically, its\nmulti-kernel design extends Kronecker projections horizontally and separates\nadaptation matrices into multiple complementary spaces, reducing parameter\ndependency and creating more compact subspaces. Besides, it incorporates extra\nlearnable re-scaling factors to better align with pre-trained feature\ndistributions, allowing for more flexible and balanced feature aggregation.\nExtensive experiments validate that our KARST outperforms other PEFT\ncounterparts with a negligible inference cost due to its re-parameterization\ncharacteristics. Code is publicly available at:\nhttps://github.com/Lucenova/KARST.\n", "link": "http://arxiv.org/abs/2502.06779v1", "date": "2025-02-10", "relevancy": 2.8256, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KARST%3A%20Multi-Kernel%20Kronecker%20Adaptation%20with%20Re-Scaling%20Transmission%0A%20%20for%20Visual%20Classification&body=Title%3A%20KARST%3A%20Multi-Kernel%20Kronecker%20Adaptation%20with%20Re-Scaling%20Transmission%0A%20%20for%20Visual%20Classification%0AAuthor%3A%20Yue%20Zhu%20and%20Haiwen%20Diao%20and%20Shang%20Gao%20and%20Long%20Chen%20and%20Huchuan%20Lu%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20vision%20models%20for%20specific%20tasks%20is%20a%20common%20practice%0Ain%20computer%20vision.%20However%2C%20this%20process%20becomes%20more%20expensive%20as%20models%20grow%0Alarger.%20Recently%2C%20parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20have%20emerged%0Aas%20a%20popular%20solution%20to%20improve%20training%20efficiency%20and%20reduce%20storage%20needs%0Aby%20tuning%20additional%20low-rank%20modules%20within%20pre-trained%20backbones.%20Despite%0Atheir%20advantages%2C%20they%20struggle%20with%20limited%20representation%20capabilities%20and%0Amisalignment%20with%20pre-trained%20intermediate%20features.%20To%20address%20these%20issues%2C%0Awe%20introduce%20an%20innovative%20Multi-Kernel%20Kronecker%20Adaptation%20with%20Re-Scaling%0ATransmission%20%28KARST%29%20for%20various%20recognition%20tasks.%20Specifically%2C%20its%0Amulti-kernel%20design%20extends%20Kronecker%20projections%20horizontally%20and%20separates%0Aadaptation%20matrices%20into%20multiple%20complementary%20spaces%2C%20reducing%20parameter%0Adependency%20and%20creating%20more%20compact%20subspaces.%20Besides%2C%20it%20incorporates%20extra%0Alearnable%20re-scaling%20factors%20to%20better%20align%20with%20pre-trained%20feature%0Adistributions%2C%20allowing%20for%20more%20flexible%20and%20balanced%20feature%20aggregation.%0AExtensive%20experiments%20validate%20that%20our%20KARST%20outperforms%20other%20PEFT%0Acounterparts%20with%20a%20negligible%20inference%20cost%20due%20to%20its%20re-parameterization%0Acharacteristics.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Lucenova/KARST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKARST%253A%2520Multi-Kernel%2520Kronecker%2520Adaptation%2520with%2520Re-Scaling%2520Transmission%250A%2520%2520for%2520Visual%2520Classification%26entry.906535625%3DYue%2520Zhu%2520and%2520Haiwen%2520Diao%2520and%2520Shang%2520Gao%2520and%2520Long%2520Chen%2520and%2520Huchuan%2520Lu%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520vision%2520models%2520for%2520specific%2520tasks%2520is%2520a%2520common%2520practice%250Ain%2520computer%2520vision.%2520However%252C%2520this%2520process%2520becomes%2520more%2520expensive%2520as%2520models%2520grow%250Alarger.%2520Recently%252C%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520have%2520emerged%250Aas%2520a%2520popular%2520solution%2520to%2520improve%2520training%2520efficiency%2520and%2520reduce%2520storage%2520needs%250Aby%2520tuning%2520additional%2520low-rank%2520modules%2520within%2520pre-trained%2520backbones.%2520Despite%250Atheir%2520advantages%252C%2520they%2520struggle%2520with%2520limited%2520representation%2520capabilities%2520and%250Amisalignment%2520with%2520pre-trained%2520intermediate%2520features.%2520To%2520address%2520these%2520issues%252C%250Awe%2520introduce%2520an%2520innovative%2520Multi-Kernel%2520Kronecker%2520Adaptation%2520with%2520Re-Scaling%250ATransmission%2520%2528KARST%2529%2520for%2520various%2520recognition%2520tasks.%2520Specifically%252C%2520its%250Amulti-kernel%2520design%2520extends%2520Kronecker%2520projections%2520horizontally%2520and%2520separates%250Aadaptation%2520matrices%2520into%2520multiple%2520complementary%2520spaces%252C%2520reducing%2520parameter%250Adependency%2520and%2520creating%2520more%2520compact%2520subspaces.%2520Besides%252C%2520it%2520incorporates%2520extra%250Alearnable%2520re-scaling%2520factors%2520to%2520better%2520align%2520with%2520pre-trained%2520feature%250Adistributions%252C%2520allowing%2520for%2520more%2520flexible%2520and%2520balanced%2520feature%2520aggregation.%250AExtensive%2520experiments%2520validate%2520that%2520our%2520KARST%2520outperforms%2520other%2520PEFT%250Acounterparts%2520with%2520a%2520negligible%2520inference%2520cost%2520due%2520to%2520its%2520re-parameterization%250Acharacteristics.%2520Code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/Lucenova/KARST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KARST%3A%20Multi-Kernel%20Kronecker%20Adaptation%20with%20Re-Scaling%20Transmission%0A%20%20for%20Visual%20Classification&entry.906535625=Yue%20Zhu%20and%20Haiwen%20Diao%20and%20Shang%20Gao%20and%20Long%20Chen%20and%20Huchuan%20Lu&entry.1292438233=%20%20Fine-tuning%20pre-trained%20vision%20models%20for%20specific%20tasks%20is%20a%20common%20practice%0Ain%20computer%20vision.%20However%2C%20this%20process%20becomes%20more%20expensive%20as%20models%20grow%0Alarger.%20Recently%2C%20parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20have%20emerged%0Aas%20a%20popular%20solution%20to%20improve%20training%20efficiency%20and%20reduce%20storage%20needs%0Aby%20tuning%20additional%20low-rank%20modules%20within%20pre-trained%20backbones.%20Despite%0Atheir%20advantages%2C%20they%20struggle%20with%20limited%20representation%20capabilities%20and%0Amisalignment%20with%20pre-trained%20intermediate%20features.%20To%20address%20these%20issues%2C%0Awe%20introduce%20an%20innovative%20Multi-Kernel%20Kronecker%20Adaptation%20with%20Re-Scaling%0ATransmission%20%28KARST%29%20for%20various%20recognition%20tasks.%20Specifically%2C%20its%0Amulti-kernel%20design%20extends%20Kronecker%20projections%20horizontally%20and%20separates%0Aadaptation%20matrices%20into%20multiple%20complementary%20spaces%2C%20reducing%20parameter%0Adependency%20and%20creating%20more%20compact%20subspaces.%20Besides%2C%20it%20incorporates%20extra%0Alearnable%20re-scaling%20factors%20to%20better%20align%20with%20pre-trained%20feature%0Adistributions%2C%20allowing%20for%20more%20flexible%20and%20balanced%20feature%20aggregation.%0AExtensive%20experiments%20validate%20that%20our%20KARST%20outperforms%20other%20PEFT%0Acounterparts%20with%20a%20negligible%20inference%20cost%20due%20to%20its%20re-parameterization%0Acharacteristics.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Lucenova/KARST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06779v1&entry.124074799=Read"},
{"title": "Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?", "author": "Gon\u00e7alo Gomes and Chrysoula Zerva and Bruno Martins", "abstract": "  The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.\n", "link": "http://arxiv.org/abs/2502.06600v1", "date": "2025-02-10", "relevancy": 2.8198, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Multilingual%20Image%20Captioning%3A%20How%20far%20can%20we%20get%20with%0A%20%20CLIP%20models%3F&body=Title%3A%20Evaluation%20of%20Multilingual%20Image%20Captioning%3A%20How%20far%20can%20we%20get%20with%0A%20%20CLIP%20models%3F%0AAuthor%3A%20Gon%C3%A7alo%20Gomes%20and%20Chrysoula%20Zerva%20and%20Bruno%20Martins%0AAbstract%3A%20%20%20The%20evaluation%20of%20image%20captions%2C%20looking%20at%20both%20linguistic%20fluency%20and%0Asemantic%20correspondence%20to%20visual%20contents%2C%20has%20witnessed%20a%20significant%20effort.%0AStill%2C%20despite%20advancements%20such%20as%20the%20CLIPScore%20metric%2C%20multilingual%0Acaptioning%20evaluation%20has%20remained%20relatively%20unexplored.%20This%20work%20presents%0Aseveral%20strategies%2C%20and%20extensive%20experiments%2C%20related%20to%20evaluating%20CLIPScore%0Avariants%20in%20multilingual%20settings.%20To%20address%20the%20lack%20of%20multilingual%20test%0Adata%2C%20we%20consider%20two%20different%20strategies%3A%20%281%29%20using%20quality%20aware%0Amachine-translated%20datasets%20with%20human%20judgements%2C%20and%20%282%29%20re-purposing%0Amultilingual%20datasets%20that%20target%20semantic%20inference%20and%20reasoning.%20Our%20results%0Ahighlight%20the%20potential%20of%20finetuned%20multilingual%20models%20to%20generalize%20across%0Alanguages%20and%20to%20handle%20complex%20linguistic%20challenges.%20Tests%20with%0Amachine-translated%20data%20show%20that%20multilingual%20CLIPScore%20models%20can%20maintain%20a%0Ahigh%20correlation%20with%20human%20judgements%20across%20different%20languages%2C%20and%0Aadditional%20tests%20with%20natively%20multilingual%20and%20multicultural%20data%20further%0Aattest%20to%20the%20high-quality%20assessments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Multilingual%2520Image%2520Captioning%253A%2520How%2520far%2520can%2520we%2520get%2520with%250A%2520%2520CLIP%2520models%253F%26entry.906535625%3DGon%25C3%25A7alo%2520Gomes%2520and%2520Chrysoula%2520Zerva%2520and%2520Bruno%2520Martins%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520image%2520captions%252C%2520looking%2520at%2520both%2520linguistic%2520fluency%2520and%250Asemantic%2520correspondence%2520to%2520visual%2520contents%252C%2520has%2520witnessed%2520a%2520significant%2520effort.%250AStill%252C%2520despite%2520advancements%2520such%2520as%2520the%2520CLIPScore%2520metric%252C%2520multilingual%250Acaptioning%2520evaluation%2520has%2520remained%2520relatively%2520unexplored.%2520This%2520work%2520presents%250Aseveral%2520strategies%252C%2520and%2520extensive%2520experiments%252C%2520related%2520to%2520evaluating%2520CLIPScore%250Avariants%2520in%2520multilingual%2520settings.%2520To%2520address%2520the%2520lack%2520of%2520multilingual%2520test%250Adata%252C%2520we%2520consider%2520two%2520different%2520strategies%253A%2520%25281%2529%2520using%2520quality%2520aware%250Amachine-translated%2520datasets%2520with%2520human%2520judgements%252C%2520and%2520%25282%2529%2520re-purposing%250Amultilingual%2520datasets%2520that%2520target%2520semantic%2520inference%2520and%2520reasoning.%2520Our%2520results%250Ahighlight%2520the%2520potential%2520of%2520finetuned%2520multilingual%2520models%2520to%2520generalize%2520across%250Alanguages%2520and%2520to%2520handle%2520complex%2520linguistic%2520challenges.%2520Tests%2520with%250Amachine-translated%2520data%2520show%2520that%2520multilingual%2520CLIPScore%2520models%2520can%2520maintain%2520a%250Ahigh%2520correlation%2520with%2520human%2520judgements%2520across%2520different%2520languages%252C%2520and%250Aadditional%2520tests%2520with%2520natively%2520multilingual%2520and%2520multicultural%2520data%2520further%250Aattest%2520to%2520the%2520high-quality%2520assessments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Multilingual%20Image%20Captioning%3A%20How%20far%20can%20we%20get%20with%0A%20%20CLIP%20models%3F&entry.906535625=Gon%C3%A7alo%20Gomes%20and%20Chrysoula%20Zerva%20and%20Bruno%20Martins&entry.1292438233=%20%20The%20evaluation%20of%20image%20captions%2C%20looking%20at%20both%20linguistic%20fluency%20and%0Asemantic%20correspondence%20to%20visual%20contents%2C%20has%20witnessed%20a%20significant%20effort.%0AStill%2C%20despite%20advancements%20such%20as%20the%20CLIPScore%20metric%2C%20multilingual%0Acaptioning%20evaluation%20has%20remained%20relatively%20unexplored.%20This%20work%20presents%0Aseveral%20strategies%2C%20and%20extensive%20experiments%2C%20related%20to%20evaluating%20CLIPScore%0Avariants%20in%20multilingual%20settings.%20To%20address%20the%20lack%20of%20multilingual%20test%0Adata%2C%20we%20consider%20two%20different%20strategies%3A%20%281%29%20using%20quality%20aware%0Amachine-translated%20datasets%20with%20human%20judgements%2C%20and%20%282%29%20re-purposing%0Amultilingual%20datasets%20that%20target%20semantic%20inference%20and%20reasoning.%20Our%20results%0Ahighlight%20the%20potential%20of%20finetuned%20multilingual%20models%20to%20generalize%20across%0Alanguages%20and%20to%20handle%20complex%20linguistic%20challenges.%20Tests%20with%0Amachine-translated%20data%20show%20that%20multilingual%20CLIPScore%20models%20can%20maintain%20a%0Ahigh%20correlation%20with%20human%20judgements%20across%20different%20languages%2C%20and%0Aadditional%20tests%20with%20natively%20multilingual%20and%20multicultural%20data%20further%0Aattest%20to%20the%20high-quality%20assessments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06600v1&entry.124074799=Read"},
{"title": "Towards Internet-Scale Training For Agents", "author": "Brandon Trabucco and Gunnar Sigurdsson and Robinson Piramuthu and Ruslan Salakhutdinov", "abstract": "  The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io.\n", "link": "http://arxiv.org/abs/2502.06776v1", "date": "2025-02-10", "relevancy": 2.7558, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5725}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Internet-Scale%20Training%20For%20Agents&body=Title%3A%20Towards%20Internet-Scale%20Training%20For%20Agents%0AAuthor%3A%20Brandon%20Trabucco%20and%20Gunnar%20Sigurdsson%20and%20Robinson%20Piramuthu%20and%20Ruslan%20Salakhutdinov%0AAbstract%3A%20%20%20The%20predominant%20approach%20for%20training%20web%20navigation%20agents%20gathers%20human%0Ademonstrations%20for%20a%20set%20of%20popular%20websites%20and%20hand-written%20tasks%2C%20but%20it%20is%0Abecoming%20clear%20that%20human%20data%20are%20an%20inefficient%20resource.%20We%20develop%20a%0Apipeline%20to%20facilitate%20Internet-scale%20training%20for%20agents%20without%20laborious%0Ahuman%20annotations.%20In%20the%20first%20stage%2C%20an%20LLM%20generates%20tasks%20for%20150k%20diverse%0Awebsites.%20In%20the%20next%20stage%2C%20LLM%20agents%20complete%20tasks%20and%20produce%0Atrajectories.%20In%20the%20final%20stage%2C%20an%20LLM%20reviews%20the%20trajectories%20and%20judges%0Atheir%20success.%20Language%20models%20are%20competitive%20with%20human%20annotators%2C%20detecting%0Aand%20filtering%20out%20harmful%20content%20with%20an%20accuracy%20of%2097%25%2C%20generating%20feasible%0Atasks%20with%20an%2089%25%20rate%2C%20and%20judging%20successful%20trajectories%20with%20an%2082.6%25%0Aaccuracy.%20Scaling%20the%20pipeline%2C%20agents%20based%20on%20Llama%203.1%2070B%20solve%2016.7%25%20of%0Atasks%20for%20150k%20sites.%20Training%20on%20the%20data%20generated%20by%20our%20pipeline%20is%0Acompetitive%20with%20training%20on%20human%20demonstrations.%20In%20data-limited%20settings%0Aderived%20from%20Mind2Web%20and%20WebLINX%2C%20we%20improve%20Step%20Accuracy%20by%20up%20to%20%2B89.5%25%20and%0A%2B122.1%25%20respectively%20for%20agents%20trained%20on%20mixtures%20of%20data%20from%20our%20pipeline%2C%0Aand%20human%20data.%20When%20training%20agents%20with%20all%20available%20human%20data%20from%20these%0Abenchmarks%2C%20agents%20fail%20to%20generalize%20to%20diverse%20real%20sites%2C%20and%20adding%20our%0Adata%20improves%20their%20generalization%20by%20%2B149.0%25%20for%20WebLINX%20and%20%2B156.3%25%20for%0AMind2Web.%20Code%20will%20be%20available%20at%3A%20data-for-agents.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Internet-Scale%2520Training%2520For%2520Agents%26entry.906535625%3DBrandon%2520Trabucco%2520and%2520Gunnar%2520Sigurdsson%2520and%2520Robinson%2520Piramuthu%2520and%2520Ruslan%2520Salakhutdinov%26entry.1292438233%3D%2520%2520The%2520predominant%2520approach%2520for%2520training%2520web%2520navigation%2520agents%2520gathers%2520human%250Ademonstrations%2520for%2520a%2520set%2520of%2520popular%2520websites%2520and%2520hand-written%2520tasks%252C%2520but%2520it%2520is%250Abecoming%2520clear%2520that%2520human%2520data%2520are%2520an%2520inefficient%2520resource.%2520We%2520develop%2520a%250Apipeline%2520to%2520facilitate%2520Internet-scale%2520training%2520for%2520agents%2520without%2520laborious%250Ahuman%2520annotations.%2520In%2520the%2520first%2520stage%252C%2520an%2520LLM%2520generates%2520tasks%2520for%2520150k%2520diverse%250Awebsites.%2520In%2520the%2520next%2520stage%252C%2520LLM%2520agents%2520complete%2520tasks%2520and%2520produce%250Atrajectories.%2520In%2520the%2520final%2520stage%252C%2520an%2520LLM%2520reviews%2520the%2520trajectories%2520and%2520judges%250Atheir%2520success.%2520Language%2520models%2520are%2520competitive%2520with%2520human%2520annotators%252C%2520detecting%250Aand%2520filtering%2520out%2520harmful%2520content%2520with%2520an%2520accuracy%2520of%252097%2525%252C%2520generating%2520feasible%250Atasks%2520with%2520an%252089%2525%2520rate%252C%2520and%2520judging%2520successful%2520trajectories%2520with%2520an%252082.6%2525%250Aaccuracy.%2520Scaling%2520the%2520pipeline%252C%2520agents%2520based%2520on%2520Llama%25203.1%252070B%2520solve%252016.7%2525%2520of%250Atasks%2520for%2520150k%2520sites.%2520Training%2520on%2520the%2520data%2520generated%2520by%2520our%2520pipeline%2520is%250Acompetitive%2520with%2520training%2520on%2520human%2520demonstrations.%2520In%2520data-limited%2520settings%250Aderived%2520from%2520Mind2Web%2520and%2520WebLINX%252C%2520we%2520improve%2520Step%2520Accuracy%2520by%2520up%2520to%2520%252B89.5%2525%2520and%250A%252B122.1%2525%2520respectively%2520for%2520agents%2520trained%2520on%2520mixtures%2520of%2520data%2520from%2520our%2520pipeline%252C%250Aand%2520human%2520data.%2520When%2520training%2520agents%2520with%2520all%2520available%2520human%2520data%2520from%2520these%250Abenchmarks%252C%2520agents%2520fail%2520to%2520generalize%2520to%2520diverse%2520real%2520sites%252C%2520and%2520adding%2520our%250Adata%2520improves%2520their%2520generalization%2520by%2520%252B149.0%2525%2520for%2520WebLINX%2520and%2520%252B156.3%2525%2520for%250AMind2Web.%2520Code%2520will%2520be%2520available%2520at%253A%2520data-for-agents.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Internet-Scale%20Training%20For%20Agents&entry.906535625=Brandon%20Trabucco%20and%20Gunnar%20Sigurdsson%20and%20Robinson%20Piramuthu%20and%20Ruslan%20Salakhutdinov&entry.1292438233=%20%20The%20predominant%20approach%20for%20training%20web%20navigation%20agents%20gathers%20human%0Ademonstrations%20for%20a%20set%20of%20popular%20websites%20and%20hand-written%20tasks%2C%20but%20it%20is%0Abecoming%20clear%20that%20human%20data%20are%20an%20inefficient%20resource.%20We%20develop%20a%0Apipeline%20to%20facilitate%20Internet-scale%20training%20for%20agents%20without%20laborious%0Ahuman%20annotations.%20In%20the%20first%20stage%2C%20an%20LLM%20generates%20tasks%20for%20150k%20diverse%0Awebsites.%20In%20the%20next%20stage%2C%20LLM%20agents%20complete%20tasks%20and%20produce%0Atrajectories.%20In%20the%20final%20stage%2C%20an%20LLM%20reviews%20the%20trajectories%20and%20judges%0Atheir%20success.%20Language%20models%20are%20competitive%20with%20human%20annotators%2C%20detecting%0Aand%20filtering%20out%20harmful%20content%20with%20an%20accuracy%20of%2097%25%2C%20generating%20feasible%0Atasks%20with%20an%2089%25%20rate%2C%20and%20judging%20successful%20trajectories%20with%20an%2082.6%25%0Aaccuracy.%20Scaling%20the%20pipeline%2C%20agents%20based%20on%20Llama%203.1%2070B%20solve%2016.7%25%20of%0Atasks%20for%20150k%20sites.%20Training%20on%20the%20data%20generated%20by%20our%20pipeline%20is%0Acompetitive%20with%20training%20on%20human%20demonstrations.%20In%20data-limited%20settings%0Aderived%20from%20Mind2Web%20and%20WebLINX%2C%20we%20improve%20Step%20Accuracy%20by%20up%20to%20%2B89.5%25%20and%0A%2B122.1%25%20respectively%20for%20agents%20trained%20on%20mixtures%20of%20data%20from%20our%20pipeline%2C%0Aand%20human%20data.%20When%20training%20agents%20with%20all%20available%20human%20data%20from%20these%0Abenchmarks%2C%20agents%20fail%20to%20generalize%20to%20diverse%20real%20sites%2C%20and%20adding%20our%0Adata%20improves%20their%20generalization%20by%20%2B149.0%25%20for%20WebLINX%20and%20%2B156.3%25%20for%0AMind2Web.%20Code%20will%20be%20available%20at%3A%20data-for-agents.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06776v1&entry.124074799=Read"},
{"title": "Structure-preserving contrastive learning for spatial time series", "author": "Yiru Jiao and Sander van Cranenburgh and Simeon Calvert and Hans van Lint", "abstract": "  Informative representations enhance model performance and generalisability in\ndownstream tasks. However, learning self-supervised representations for\nspatially characterised time series, like traffic interactions, poses\nchallenges as it requires maintaining fine-grained similarity relations in the\nlatent space. In this study, we incorporate two structure-preserving\nregularisers for the contrastive learning of spatial time series: one\nregulariser preserves the topology of similarities between instances, and the\nother preserves the graph geometry of similarities across spatial and temporal\ndimensions. To balance contrastive learning and structure preservation, we\npropose a dynamic mechanism that adaptively weighs the trade-off and stabilises\ntraining. We conduct experiments on multivariate time series classification, as\nwell as macroscopic and microscopic traffic prediction. For all three tasks,\nour approach preserves the structures of similarity relations more effectively\nand improves state-of-the-art task performances. The proposed approach can be\napplied to an arbitrary encoder and is particularly beneficial for time series\nwith spatial or geographical features. Furthermore, this study suggests that\nhigher similarity structure preservation indicates more informative and useful\nrepresentations. This may help to understand the contribution of representation\nlearning in pattern recognition with neural networks. Our code is made openly\naccessible with all resulting data at https://github.com/yiru-jiao/spclt.\n", "link": "http://arxiv.org/abs/2502.06380v1", "date": "2025-02-10", "relevancy": 2.7483, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5744}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5699}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series&body=Title%3A%20Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series%0AAuthor%3A%20Yiru%20Jiao%20and%20Sander%20van%20Cranenburgh%20and%20Simeon%20Calvert%20and%20Hans%20van%20Lint%0AAbstract%3A%20%20%20Informative%20representations%20enhance%20model%20performance%20and%20generalisability%20in%0Adownstream%20tasks.%20However%2C%20learning%20self-supervised%20representations%20for%0Aspatially%20characterised%20time%20series%2C%20like%20traffic%20interactions%2C%20poses%0Achallenges%20as%20it%20requires%20maintaining%20fine-grained%20similarity%20relations%20in%20the%0Alatent%20space.%20In%20this%20study%2C%20we%20incorporate%20two%20structure-preserving%0Aregularisers%20for%20the%20contrastive%20learning%20of%20spatial%20time%20series%3A%20one%0Aregulariser%20preserves%20the%20topology%20of%20similarities%20between%20instances%2C%20and%20the%0Aother%20preserves%20the%20graph%20geometry%20of%20similarities%20across%20spatial%20and%20temporal%0Adimensions.%20To%20balance%20contrastive%20learning%20and%20structure%20preservation%2C%20we%0Apropose%20a%20dynamic%20mechanism%20that%20adaptively%20weighs%20the%20trade-off%20and%20stabilises%0Atraining.%20We%20conduct%20experiments%20on%20multivariate%20time%20series%20classification%2C%20as%0Awell%20as%20macroscopic%20and%20microscopic%20traffic%20prediction.%20For%20all%20three%20tasks%2C%0Aour%20approach%20preserves%20the%20structures%20of%20similarity%20relations%20more%20effectively%0Aand%20improves%20state-of-the-art%20task%20performances.%20The%20proposed%20approach%20can%20be%0Aapplied%20to%20an%20arbitrary%20encoder%20and%20is%20particularly%20beneficial%20for%20time%20series%0Awith%20spatial%20or%20geographical%20features.%20Furthermore%2C%20this%20study%20suggests%20that%0Ahigher%20similarity%20structure%20preservation%20indicates%20more%20informative%20and%20useful%0Arepresentations.%20This%20may%20help%20to%20understand%20the%20contribution%20of%20representation%0Alearning%20in%20pattern%20recognition%20with%20neural%20networks.%20Our%20code%20is%20made%20openly%0Aaccessible%20with%20all%20resulting%20data%20at%20https%3A//github.com/yiru-jiao/spclt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-preserving%2520contrastive%2520learning%2520for%2520spatial%2520time%2520series%26entry.906535625%3DYiru%2520Jiao%2520and%2520Sander%2520van%2520Cranenburgh%2520and%2520Simeon%2520Calvert%2520and%2520Hans%2520van%2520Lint%26entry.1292438233%3D%2520%2520Informative%2520representations%2520enhance%2520model%2520performance%2520and%2520generalisability%2520in%250Adownstream%2520tasks.%2520However%252C%2520learning%2520self-supervised%2520representations%2520for%250Aspatially%2520characterised%2520time%2520series%252C%2520like%2520traffic%2520interactions%252C%2520poses%250Achallenges%2520as%2520it%2520requires%2520maintaining%2520fine-grained%2520similarity%2520relations%2520in%2520the%250Alatent%2520space.%2520In%2520this%2520study%252C%2520we%2520incorporate%2520two%2520structure-preserving%250Aregularisers%2520for%2520the%2520contrastive%2520learning%2520of%2520spatial%2520time%2520series%253A%2520one%250Aregulariser%2520preserves%2520the%2520topology%2520of%2520similarities%2520between%2520instances%252C%2520and%2520the%250Aother%2520preserves%2520the%2520graph%2520geometry%2520of%2520similarities%2520across%2520spatial%2520and%2520temporal%250Adimensions.%2520To%2520balance%2520contrastive%2520learning%2520and%2520structure%2520preservation%252C%2520we%250Apropose%2520a%2520dynamic%2520mechanism%2520that%2520adaptively%2520weighs%2520the%2520trade-off%2520and%2520stabilises%250Atraining.%2520We%2520conduct%2520experiments%2520on%2520multivariate%2520time%2520series%2520classification%252C%2520as%250Awell%2520as%2520macroscopic%2520and%2520microscopic%2520traffic%2520prediction.%2520For%2520all%2520three%2520tasks%252C%250Aour%2520approach%2520preserves%2520the%2520structures%2520of%2520similarity%2520relations%2520more%2520effectively%250Aand%2520improves%2520state-of-the-art%2520task%2520performances.%2520The%2520proposed%2520approach%2520can%2520be%250Aapplied%2520to%2520an%2520arbitrary%2520encoder%2520and%2520is%2520particularly%2520beneficial%2520for%2520time%2520series%250Awith%2520spatial%2520or%2520geographical%2520features.%2520Furthermore%252C%2520this%2520study%2520suggests%2520that%250Ahigher%2520similarity%2520structure%2520preservation%2520indicates%2520more%2520informative%2520and%2520useful%250Arepresentations.%2520This%2520may%2520help%2520to%2520understand%2520the%2520contribution%2520of%2520representation%250Alearning%2520in%2520pattern%2520recognition%2520with%2520neural%2520networks.%2520Our%2520code%2520is%2520made%2520openly%250Aaccessible%2520with%2520all%2520resulting%2520data%2520at%2520https%253A//github.com/yiru-jiao/spclt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series&entry.906535625=Yiru%20Jiao%20and%20Sander%20van%20Cranenburgh%20and%20Simeon%20Calvert%20and%20Hans%20van%20Lint&entry.1292438233=%20%20Informative%20representations%20enhance%20model%20performance%20and%20generalisability%20in%0Adownstream%20tasks.%20However%2C%20learning%20self-supervised%20representations%20for%0Aspatially%20characterised%20time%20series%2C%20like%20traffic%20interactions%2C%20poses%0Achallenges%20as%20it%20requires%20maintaining%20fine-grained%20similarity%20relations%20in%20the%0Alatent%20space.%20In%20this%20study%2C%20we%20incorporate%20two%20structure-preserving%0Aregularisers%20for%20the%20contrastive%20learning%20of%20spatial%20time%20series%3A%20one%0Aregulariser%20preserves%20the%20topology%20of%20similarities%20between%20instances%2C%20and%20the%0Aother%20preserves%20the%20graph%20geometry%20of%20similarities%20across%20spatial%20and%20temporal%0Adimensions.%20To%20balance%20contrastive%20learning%20and%20structure%20preservation%2C%20we%0Apropose%20a%20dynamic%20mechanism%20that%20adaptively%20weighs%20the%20trade-off%20and%20stabilises%0Atraining.%20We%20conduct%20experiments%20on%20multivariate%20time%20series%20classification%2C%20as%0Awell%20as%20macroscopic%20and%20microscopic%20traffic%20prediction.%20For%20all%20three%20tasks%2C%0Aour%20approach%20preserves%20the%20structures%20of%20similarity%20relations%20more%20effectively%0Aand%20improves%20state-of-the-art%20task%20performances.%20The%20proposed%20approach%20can%20be%0Aapplied%20to%20an%20arbitrary%20encoder%20and%20is%20particularly%20beneficial%20for%20time%20series%0Awith%20spatial%20or%20geographical%20features.%20Furthermore%2C%20this%20study%20suggests%20that%0Ahigher%20similarity%20structure%20preservation%20indicates%20more%20informative%20and%20useful%0Arepresentations.%20This%20may%20help%20to%20understand%20the%20contribution%20of%20representation%0Alearning%20in%20pattern%20recognition%20with%20neural%20networks.%20Our%20code%20is%20made%20openly%0Aaccessible%20with%20all%20resulting%20data%20at%20https%3A//github.com/yiru-jiao/spclt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06380v1&entry.124074799=Read"},
{"title": "Unsupervised Learning for Feature Extraction and Temporal Alignment of\n  3D+t Point Clouds of Zebrafish Embryos", "author": "Zhu Chen and Ina Laube and Johannes Stegmaier", "abstract": "  Zebrafish are widely used in biomedical research and developmental stages of\ntheir embryos often need to be synchronized for further analysis. We present an\nunsupervised approach to extract descriptive features from 3D+t point clouds of\nzebrafish embryos and subsequently use those features to temporally align\ncorresponding developmental stages. An autoencoder architecture is proposed to\nlearn a descriptive representation of the point clouds and we designed a deep\nregression network for their temporal alignment. We achieve a high alignment\naccuracy with an average mismatch of only 3.83 minutes over an experimental\nduration of 5.3 hours. As a fully-unsupervised approach, there is no manual\nlabeling effort required and unlike manual analyses the method easily scales.\nBesides, the alignment without human annotation of the data also avoids any\ninfluence caused by subjective bias.\n", "link": "http://arxiv.org/abs/2502.06543v1", "date": "2025-02-10", "relevancy": 2.7473, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5508}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Learning%20for%20Feature%20Extraction%20and%20Temporal%20Alignment%20of%0A%20%203D%2Bt%20Point%20Clouds%20of%20Zebrafish%20Embryos&body=Title%3A%20Unsupervised%20Learning%20for%20Feature%20Extraction%20and%20Temporal%20Alignment%20of%0A%20%203D%2Bt%20Point%20Clouds%20of%20Zebrafish%20Embryos%0AAuthor%3A%20Zhu%20Chen%20and%20Ina%20Laube%20and%20Johannes%20Stegmaier%0AAbstract%3A%20%20%20Zebrafish%20are%20widely%20used%20in%20biomedical%20research%20and%20developmental%20stages%20of%0Atheir%20embryos%20often%20need%20to%20be%20synchronized%20for%20further%20analysis.%20We%20present%20an%0Aunsupervised%20approach%20to%20extract%20descriptive%20features%20from%203D%2Bt%20point%20clouds%20of%0Azebrafish%20embryos%20and%20subsequently%20use%20those%20features%20to%20temporally%20align%0Acorresponding%20developmental%20stages.%20An%20autoencoder%20architecture%20is%20proposed%20to%0Alearn%20a%20descriptive%20representation%20of%20the%20point%20clouds%20and%20we%20designed%20a%20deep%0Aregression%20network%20for%20their%20temporal%20alignment.%20We%20achieve%20a%20high%20alignment%0Aaccuracy%20with%20an%20average%20mismatch%20of%20only%203.83%20minutes%20over%20an%20experimental%0Aduration%20of%205.3%20hours.%20As%20a%20fully-unsupervised%20approach%2C%20there%20is%20no%20manual%0Alabeling%20effort%20required%20and%20unlike%20manual%20analyses%20the%20method%20easily%20scales.%0ABesides%2C%20the%20alignment%20without%20human%20annotation%20of%20the%20data%20also%20avoids%20any%0Ainfluence%20caused%20by%20subjective%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Learning%2520for%2520Feature%2520Extraction%2520and%2520Temporal%2520Alignment%2520of%250A%2520%25203D%252Bt%2520Point%2520Clouds%2520of%2520Zebrafish%2520Embryos%26entry.906535625%3DZhu%2520Chen%2520and%2520Ina%2520Laube%2520and%2520Johannes%2520Stegmaier%26entry.1292438233%3D%2520%2520Zebrafish%2520are%2520widely%2520used%2520in%2520biomedical%2520research%2520and%2520developmental%2520stages%2520of%250Atheir%2520embryos%2520often%2520need%2520to%2520be%2520synchronized%2520for%2520further%2520analysis.%2520We%2520present%2520an%250Aunsupervised%2520approach%2520to%2520extract%2520descriptive%2520features%2520from%25203D%252Bt%2520point%2520clouds%2520of%250Azebrafish%2520embryos%2520and%2520subsequently%2520use%2520those%2520features%2520to%2520temporally%2520align%250Acorresponding%2520developmental%2520stages.%2520An%2520autoencoder%2520architecture%2520is%2520proposed%2520to%250Alearn%2520a%2520descriptive%2520representation%2520of%2520the%2520point%2520clouds%2520and%2520we%2520designed%2520a%2520deep%250Aregression%2520network%2520for%2520their%2520temporal%2520alignment.%2520We%2520achieve%2520a%2520high%2520alignment%250Aaccuracy%2520with%2520an%2520average%2520mismatch%2520of%2520only%25203.83%2520minutes%2520over%2520an%2520experimental%250Aduration%2520of%25205.3%2520hours.%2520As%2520a%2520fully-unsupervised%2520approach%252C%2520there%2520is%2520no%2520manual%250Alabeling%2520effort%2520required%2520and%2520unlike%2520manual%2520analyses%2520the%2520method%2520easily%2520scales.%250ABesides%252C%2520the%2520alignment%2520without%2520human%2520annotation%2520of%2520the%2520data%2520also%2520avoids%2520any%250Ainfluence%2520caused%2520by%2520subjective%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Learning%20for%20Feature%20Extraction%20and%20Temporal%20Alignment%20of%0A%20%203D%2Bt%20Point%20Clouds%20of%20Zebrafish%20Embryos&entry.906535625=Zhu%20Chen%20and%20Ina%20Laube%20and%20Johannes%20Stegmaier&entry.1292438233=%20%20Zebrafish%20are%20widely%20used%20in%20biomedical%20research%20and%20developmental%20stages%20of%0Atheir%20embryos%20often%20need%20to%20be%20synchronized%20for%20further%20analysis.%20We%20present%20an%0Aunsupervised%20approach%20to%20extract%20descriptive%20features%20from%203D%2Bt%20point%20clouds%20of%0Azebrafish%20embryos%20and%20subsequently%20use%20those%20features%20to%20temporally%20align%0Acorresponding%20developmental%20stages.%20An%20autoencoder%20architecture%20is%20proposed%20to%0Alearn%20a%20descriptive%20representation%20of%20the%20point%20clouds%20and%20we%20designed%20a%20deep%0Aregression%20network%20for%20their%20temporal%20alignment.%20We%20achieve%20a%20high%20alignment%0Aaccuracy%20with%20an%20average%20mismatch%20of%20only%203.83%20minutes%20over%20an%20experimental%0Aduration%20of%205.3%20hours.%20As%20a%20fully-unsupervised%20approach%2C%20there%20is%20no%20manual%0Alabeling%20effort%20required%20and%20unlike%20manual%20analyses%20the%20method%20easily%20scales.%0ABesides%2C%20the%20alignment%20without%20human%20annotation%20of%20the%20data%20also%20avoids%20any%0Ainfluence%20caused%20by%20subjective%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06543v1&entry.124074799=Read"},
{"title": "Multi-Scale Feature Fusion with Image-Driven Spatial Integration for\n  Left Atrium Segmentation from Cardiac MRI Images", "author": "Bipasha Kundu and Zixin Yang and Richard Simon and Cristian Linte", "abstract": "  Accurate segmentation of the left atrium (LA) from late gadolinium-enhanced\nmagnetic resonance imaging plays a vital role in visualizing diseased atrial\nstructures, enabling the diagnosis and management of cardiovascular diseases.\nIt is particularly essential for planning treatment with ablation therapy, a\nkey intervention for atrial fibrillation (AF). However, manual segmentation is\ntime-intensive and prone to inter-observer variability, underscoring the need\nfor automated solutions. Class-agnostic foundation models like DINOv2 have\ndemonstrated remarkable feature extraction capabilities in vision tasks.\nHowever, their lack of domain specificity and task-specific adaptation can\nreduce spatial resolution during feature extraction, impacting the capture of\nfine anatomical detail in medical imaging. To address this limitation, we\npropose a segmentation framework that integrates DINOv2 as an encoder with a\nUNet-style decoder, incorporating multi-scale feature fusion and input image\nintegration to enhance segmentation accuracy. The learnable weighting mechanism\ndynamically prioritizes hierarchical features from different encoder blocks of\nthe foundation model, optimizing feature selection for task relevance.\nAdditionally, the input image is reintroduced during the decoding stage to\npreserve high-resolution spatial details, addressing limitations of\ndownsampling in the encoder. We validate our approach on the LAScarQS 2022\ndataset and demonstrate improved performance with a 92.3% Dice and 84.1% IoU\nscore for giant architecture compared to the nnUNet baseline model. These\nfindings emphasize the efficacy of our approach in advancing the field of\nautomated left atrium segmentation from cardiac MRI.\n", "link": "http://arxiv.org/abs/2502.06615v1", "date": "2025-02-10", "relevancy": 2.7404, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Feature%20Fusion%20with%20Image-Driven%20Spatial%20Integration%20for%0A%20%20Left%20Atrium%20Segmentation%20from%20Cardiac%20MRI%20Images&body=Title%3A%20Multi-Scale%20Feature%20Fusion%20with%20Image-Driven%20Spatial%20Integration%20for%0A%20%20Left%20Atrium%20Segmentation%20from%20Cardiac%20MRI%20Images%0AAuthor%3A%20Bipasha%20Kundu%20and%20Zixin%20Yang%20and%20Richard%20Simon%20and%20Cristian%20Linte%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20the%20left%20atrium%20%28LA%29%20from%20late%20gadolinium-enhanced%0Amagnetic%20resonance%20imaging%20plays%20a%20vital%20role%20in%20visualizing%20diseased%20atrial%0Astructures%2C%20enabling%20the%20diagnosis%20and%20management%20of%20cardiovascular%20diseases.%0AIt%20is%20particularly%20essential%20for%20planning%20treatment%20with%20ablation%20therapy%2C%20a%0Akey%20intervention%20for%20atrial%20fibrillation%20%28AF%29.%20However%2C%20manual%20segmentation%20is%0Atime-intensive%20and%20prone%20to%20inter-observer%20variability%2C%20underscoring%20the%20need%0Afor%20automated%20solutions.%20Class-agnostic%20foundation%20models%20like%20DINOv2%20have%0Ademonstrated%20remarkable%20feature%20extraction%20capabilities%20in%20vision%20tasks.%0AHowever%2C%20their%20lack%20of%20domain%20specificity%20and%20task-specific%20adaptation%20can%0Areduce%20spatial%20resolution%20during%20feature%20extraction%2C%20impacting%20the%20capture%20of%0Afine%20anatomical%20detail%20in%20medical%20imaging.%20To%20address%20this%20limitation%2C%20we%0Apropose%20a%20segmentation%20framework%20that%20integrates%20DINOv2%20as%20an%20encoder%20with%20a%0AUNet-style%20decoder%2C%20incorporating%20multi-scale%20feature%20fusion%20and%20input%20image%0Aintegration%20to%20enhance%20segmentation%20accuracy.%20The%20learnable%20weighting%20mechanism%0Adynamically%20prioritizes%20hierarchical%20features%20from%20different%20encoder%20blocks%20of%0Athe%20foundation%20model%2C%20optimizing%20feature%20selection%20for%20task%20relevance.%0AAdditionally%2C%20the%20input%20image%20is%20reintroduced%20during%20the%20decoding%20stage%20to%0Apreserve%20high-resolution%20spatial%20details%2C%20addressing%20limitations%20of%0Adownsampling%20in%20the%20encoder.%20We%20validate%20our%20approach%20on%20the%20LAScarQS%202022%0Adataset%20and%20demonstrate%20improved%20performance%20with%20a%2092.3%25%20Dice%20and%2084.1%25%20IoU%0Ascore%20for%20giant%20architecture%20compared%20to%20the%20nnUNet%20baseline%20model.%20These%0Afindings%20emphasize%20the%20efficacy%20of%20our%20approach%20in%20advancing%20the%20field%20of%0Aautomated%20left%20atrium%20segmentation%20from%20cardiac%20MRI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Feature%2520Fusion%2520with%2520Image-Driven%2520Spatial%2520Integration%2520for%250A%2520%2520Left%2520Atrium%2520Segmentation%2520from%2520Cardiac%2520MRI%2520Images%26entry.906535625%3DBipasha%2520Kundu%2520and%2520Zixin%2520Yang%2520and%2520Richard%2520Simon%2520and%2520Cristian%2520Linte%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520the%2520left%2520atrium%2520%2528LA%2529%2520from%2520late%2520gadolinium-enhanced%250Amagnetic%2520resonance%2520imaging%2520plays%2520a%2520vital%2520role%2520in%2520visualizing%2520diseased%2520atrial%250Astructures%252C%2520enabling%2520the%2520diagnosis%2520and%2520management%2520of%2520cardiovascular%2520diseases.%250AIt%2520is%2520particularly%2520essential%2520for%2520planning%2520treatment%2520with%2520ablation%2520therapy%252C%2520a%250Akey%2520intervention%2520for%2520atrial%2520fibrillation%2520%2528AF%2529.%2520However%252C%2520manual%2520segmentation%2520is%250Atime-intensive%2520and%2520prone%2520to%2520inter-observer%2520variability%252C%2520underscoring%2520the%2520need%250Afor%2520automated%2520solutions.%2520Class-agnostic%2520foundation%2520models%2520like%2520DINOv2%2520have%250Ademonstrated%2520remarkable%2520feature%2520extraction%2520capabilities%2520in%2520vision%2520tasks.%250AHowever%252C%2520their%2520lack%2520of%2520domain%2520specificity%2520and%2520task-specific%2520adaptation%2520can%250Areduce%2520spatial%2520resolution%2520during%2520feature%2520extraction%252C%2520impacting%2520the%2520capture%2520of%250Afine%2520anatomical%2520detail%2520in%2520medical%2520imaging.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520a%2520segmentation%2520framework%2520that%2520integrates%2520DINOv2%2520as%2520an%2520encoder%2520with%2520a%250AUNet-style%2520decoder%252C%2520incorporating%2520multi-scale%2520feature%2520fusion%2520and%2520input%2520image%250Aintegration%2520to%2520enhance%2520segmentation%2520accuracy.%2520The%2520learnable%2520weighting%2520mechanism%250Adynamically%2520prioritizes%2520hierarchical%2520features%2520from%2520different%2520encoder%2520blocks%2520of%250Athe%2520foundation%2520model%252C%2520optimizing%2520feature%2520selection%2520for%2520task%2520relevance.%250AAdditionally%252C%2520the%2520input%2520image%2520is%2520reintroduced%2520during%2520the%2520decoding%2520stage%2520to%250Apreserve%2520high-resolution%2520spatial%2520details%252C%2520addressing%2520limitations%2520of%250Adownsampling%2520in%2520the%2520encoder.%2520We%2520validate%2520our%2520approach%2520on%2520the%2520LAScarQS%25202022%250Adataset%2520and%2520demonstrate%2520improved%2520performance%2520with%2520a%252092.3%2525%2520Dice%2520and%252084.1%2525%2520IoU%250Ascore%2520for%2520giant%2520architecture%2520compared%2520to%2520the%2520nnUNet%2520baseline%2520model.%2520These%250Afindings%2520emphasize%2520the%2520efficacy%2520of%2520our%2520approach%2520in%2520advancing%2520the%2520field%2520of%250Aautomated%2520left%2520atrium%2520segmentation%2520from%2520cardiac%2520MRI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Feature%20Fusion%20with%20Image-Driven%20Spatial%20Integration%20for%0A%20%20Left%20Atrium%20Segmentation%20from%20Cardiac%20MRI%20Images&entry.906535625=Bipasha%20Kundu%20and%20Zixin%20Yang%20and%20Richard%20Simon%20and%20Cristian%20Linte&entry.1292438233=%20%20Accurate%20segmentation%20of%20the%20left%20atrium%20%28LA%29%20from%20late%20gadolinium-enhanced%0Amagnetic%20resonance%20imaging%20plays%20a%20vital%20role%20in%20visualizing%20diseased%20atrial%0Astructures%2C%20enabling%20the%20diagnosis%20and%20management%20of%20cardiovascular%20diseases.%0AIt%20is%20particularly%20essential%20for%20planning%20treatment%20with%20ablation%20therapy%2C%20a%0Akey%20intervention%20for%20atrial%20fibrillation%20%28AF%29.%20However%2C%20manual%20segmentation%20is%0Atime-intensive%20and%20prone%20to%20inter-observer%20variability%2C%20underscoring%20the%20need%0Afor%20automated%20solutions.%20Class-agnostic%20foundation%20models%20like%20DINOv2%20have%0Ademonstrated%20remarkable%20feature%20extraction%20capabilities%20in%20vision%20tasks.%0AHowever%2C%20their%20lack%20of%20domain%20specificity%20and%20task-specific%20adaptation%20can%0Areduce%20spatial%20resolution%20during%20feature%20extraction%2C%20impacting%20the%20capture%20of%0Afine%20anatomical%20detail%20in%20medical%20imaging.%20To%20address%20this%20limitation%2C%20we%0Apropose%20a%20segmentation%20framework%20that%20integrates%20DINOv2%20as%20an%20encoder%20with%20a%0AUNet-style%20decoder%2C%20incorporating%20multi-scale%20feature%20fusion%20and%20input%20image%0Aintegration%20to%20enhance%20segmentation%20accuracy.%20The%20learnable%20weighting%20mechanism%0Adynamically%20prioritizes%20hierarchical%20features%20from%20different%20encoder%20blocks%20of%0Athe%20foundation%20model%2C%20optimizing%20feature%20selection%20for%20task%20relevance.%0AAdditionally%2C%20the%20input%20image%20is%20reintroduced%20during%20the%20decoding%20stage%20to%0Apreserve%20high-resolution%20spatial%20details%2C%20addressing%20limitations%20of%0Adownsampling%20in%20the%20encoder.%20We%20validate%20our%20approach%20on%20the%20LAScarQS%202022%0Adataset%20and%20demonstrate%20improved%20performance%20with%20a%2092.3%25%20Dice%20and%2084.1%25%20IoU%0Ascore%20for%20giant%20architecture%20compared%20to%20the%20nnUNet%20baseline%20model.%20These%0Afindings%20emphasize%20the%20efficacy%20of%20our%20approach%20in%20advancing%20the%20field%20of%0Aautomated%20left%20atrium%20segmentation%20from%20cardiac%20MRI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06615v1&entry.124074799=Read"},
{"title": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation", "author": "Qijun Gan and Yi Ren and Chen Zhang and Zhenhui Ye and Pan Xie and Xiang Yin and Zehuan Yuan and Bingyue Peng and Jianke Zhu", "abstract": "  Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios.\n", "link": "http://arxiv.org/abs/2502.04847v2", "date": "2025-02-10", "relevancy": 2.7208, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7296}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6571}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.64}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanDiT%3A%20Pose-Guided%20Diffusion%20Transformer%20for%20Long-form%20Human%20Motion%0A%20%20Video%20Generation&body=Title%3A%20HumanDiT%3A%20Pose-Guided%20Diffusion%20Transformer%20for%20Long-form%20Human%20Motion%0A%20%20Video%20Generation%0AAuthor%3A%20Qijun%20Gan%20and%20Yi%20Ren%20and%20Chen%20Zhang%20and%20Zhenhui%20Ye%20and%20Pan%20Xie%20and%20Xiang%20Yin%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Jianke%20Zhu%0AAbstract%3A%20%20%20Human%20motion%20video%20generation%20has%20advanced%20significantly%2C%20while%20existing%0Amethods%20still%20struggle%20with%20accurately%20rendering%20detailed%20body%20parts%20like%20hands%0Aand%20faces%2C%20especially%20in%20long%20sequences%20and%20intricate%20motions.%20Current%0Aapproaches%20also%20rely%20on%20fixed%20resolution%20and%20struggle%20to%20maintain%20visual%0Aconsistency.%20To%20address%20these%20limitations%2C%20we%20propose%20HumanDiT%2C%20a%20pose-guided%0ADiffusion%20Transformer%20%28DiT%29-based%20framework%20trained%20on%20a%20large%20and%20wild%20dataset%0Acontaining%2014%2C000%20hours%20of%20high-quality%20video%20to%20produce%20high-fidelity%20videos%0Awith%20fine-grained%20body%20rendering.%20Specifically%2C%20%28i%29%20HumanDiT%2C%20built%20on%20DiT%2C%0Asupports%20numerous%20video%20resolutions%20and%20variable%20sequence%20lengths%2C%20facilitating%0Alearning%20for%20long-sequence%20video%20generation%3B%20%28ii%29%20we%20introduce%20a%20prefix-latent%0Areference%20strategy%20to%20maintain%20personalized%20characteristics%20across%20extended%0Asequences.%20Furthermore%2C%20during%20inference%2C%20HumanDiT%20leverages%20Keypoint-DiT%20to%0Agenerate%20subsequent%20pose%20sequences%2C%20facilitating%20video%20continuation%20from%20static%0Aimages%20or%20existing%20videos.%20It%20also%20utilizes%20a%20Pose%20Adapter%20to%20enable%20pose%0Atransfer%20with%20given%20sequences.%20Extensive%20experiments%20demonstrate%20its%20superior%0Aperformance%20in%20generating%20long-form%2C%20pose-accurate%20videos%20across%20diverse%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04847v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanDiT%253A%2520Pose-Guided%2520Diffusion%2520Transformer%2520for%2520Long-form%2520Human%2520Motion%250A%2520%2520Video%2520Generation%26entry.906535625%3DQijun%2520Gan%2520and%2520Yi%2520Ren%2520and%2520Chen%2520Zhang%2520and%2520Zhenhui%2520Ye%2520and%2520Pan%2520Xie%2520and%2520Xiang%2520Yin%2520and%2520Zehuan%2520Yuan%2520and%2520Bingyue%2520Peng%2520and%2520Jianke%2520Zhu%26entry.1292438233%3D%2520%2520Human%2520motion%2520video%2520generation%2520has%2520advanced%2520significantly%252C%2520while%2520existing%250Amethods%2520still%2520struggle%2520with%2520accurately%2520rendering%2520detailed%2520body%2520parts%2520like%2520hands%250Aand%2520faces%252C%2520especially%2520in%2520long%2520sequences%2520and%2520intricate%2520motions.%2520Current%250Aapproaches%2520also%2520rely%2520on%2520fixed%2520resolution%2520and%2520struggle%2520to%2520maintain%2520visual%250Aconsistency.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520HumanDiT%252C%2520a%2520pose-guided%250ADiffusion%2520Transformer%2520%2528DiT%2529-based%2520framework%2520trained%2520on%2520a%2520large%2520and%2520wild%2520dataset%250Acontaining%252014%252C000%2520hours%2520of%2520high-quality%2520video%2520to%2520produce%2520high-fidelity%2520videos%250Awith%2520fine-grained%2520body%2520rendering.%2520Specifically%252C%2520%2528i%2529%2520HumanDiT%252C%2520built%2520on%2520DiT%252C%250Asupports%2520numerous%2520video%2520resolutions%2520and%2520variable%2520sequence%2520lengths%252C%2520facilitating%250Alearning%2520for%2520long-sequence%2520video%2520generation%253B%2520%2528ii%2529%2520we%2520introduce%2520a%2520prefix-latent%250Areference%2520strategy%2520to%2520maintain%2520personalized%2520characteristics%2520across%2520extended%250Asequences.%2520Furthermore%252C%2520during%2520inference%252C%2520HumanDiT%2520leverages%2520Keypoint-DiT%2520to%250Agenerate%2520subsequent%2520pose%2520sequences%252C%2520facilitating%2520video%2520continuation%2520from%2520static%250Aimages%2520or%2520existing%2520videos.%2520It%2520also%2520utilizes%2520a%2520Pose%2520Adapter%2520to%2520enable%2520pose%250Atransfer%2520with%2520given%2520sequences.%2520Extensive%2520experiments%2520demonstrate%2520its%2520superior%250Aperformance%2520in%2520generating%2520long-form%252C%2520pose-accurate%2520videos%2520across%2520diverse%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04847v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanDiT%3A%20Pose-Guided%20Diffusion%20Transformer%20for%20Long-form%20Human%20Motion%0A%20%20Video%20Generation&entry.906535625=Qijun%20Gan%20and%20Yi%20Ren%20and%20Chen%20Zhang%20and%20Zhenhui%20Ye%20and%20Pan%20Xie%20and%20Xiang%20Yin%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Jianke%20Zhu&entry.1292438233=%20%20Human%20motion%20video%20generation%20has%20advanced%20significantly%2C%20while%20existing%0Amethods%20still%20struggle%20with%20accurately%20rendering%20detailed%20body%20parts%20like%20hands%0Aand%20faces%2C%20especially%20in%20long%20sequences%20and%20intricate%20motions.%20Current%0Aapproaches%20also%20rely%20on%20fixed%20resolution%20and%20struggle%20to%20maintain%20visual%0Aconsistency.%20To%20address%20these%20limitations%2C%20we%20propose%20HumanDiT%2C%20a%20pose-guided%0ADiffusion%20Transformer%20%28DiT%29-based%20framework%20trained%20on%20a%20large%20and%20wild%20dataset%0Acontaining%2014%2C000%20hours%20of%20high-quality%20video%20to%20produce%20high-fidelity%20videos%0Awith%20fine-grained%20body%20rendering.%20Specifically%2C%20%28i%29%20HumanDiT%2C%20built%20on%20DiT%2C%0Asupports%20numerous%20video%20resolutions%20and%20variable%20sequence%20lengths%2C%20facilitating%0Alearning%20for%20long-sequence%20video%20generation%3B%20%28ii%29%20we%20introduce%20a%20prefix-latent%0Areference%20strategy%20to%20maintain%20personalized%20characteristics%20across%20extended%0Asequences.%20Furthermore%2C%20during%20inference%2C%20HumanDiT%20leverages%20Keypoint-DiT%20to%0Agenerate%20subsequent%20pose%20sequences%2C%20facilitating%20video%20continuation%20from%20static%0Aimages%20or%20existing%20videos.%20It%20also%20utilizes%20a%20Pose%20Adapter%20to%20enable%20pose%0Atransfer%20with%20given%20sequences.%20Extensive%20experiments%20demonstrate%20its%20superior%0Aperformance%20in%20generating%20long-form%2C%20pose-accurate%20videos%20across%20diverse%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04847v2&entry.124074799=Read"},
{"title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for\n  Zero-Shot Customized Video Diffusion Transformers", "author": "D. She and Mushui Liu and Jingxuan Pang and Jin Wang and Zhen Yang and Wanggui He and Guanghao Zhang and Yi Wang and Qihan Huang and Haobin Tang and Yunlong Yu and Siming Fu", "abstract": "  Customized generation has achieved significant progress in image synthesis,\nyet personalized video generation remains challenging due to temporal\ninconsistencies and quality degradation. In this paper, we introduce\nCustomVideoX, an innovative framework leveraging the video diffusion\ntransformer for personalized video generation from a reference image.\nCustomVideoX capitalizes on pre-trained video networks by exclusively training\nthe LoRA parameters to extract reference features, ensuring both efficiency and\nadaptability. To facilitate seamless interaction between the reference image\nand video content, we propose 3D Reference Attention, which enables direct and\nsimultaneous engagement of reference image features with all video frames\nacross spatial and temporal dimensions. To mitigate the excessive influence of\nreference image features and textual guidance on generated video content during\ninference, we implement the Time-Aware Reference Attention Bias (TAB) strategy,\ndynamically modulating reference bias over different time steps. Additionally,\nwe introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly\nactivated regions of key entity tokens with reference feature injection by\nadjusting attention bias. To thoroughly evaluate personalized video generation,\nwe establish a new benchmark, VideoBench, comprising over 50 objects and 100\nprompts for extensive assessment. Experimental results show that CustomVideoX\nsignificantly outperforms existing methods in terms of video consistency and\nquality.\n", "link": "http://arxiv.org/abs/2502.06527v1", "date": "2025-02-10", "relevancy": 2.711, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7537}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7103}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CustomVideoX%3A%203D%20Reference%20Attention%20Driven%20Dynamic%20Adaptation%20for%0A%20%20Zero-Shot%20Customized%20Video%20Diffusion%20Transformers&body=Title%3A%20CustomVideoX%3A%203D%20Reference%20Attention%20Driven%20Dynamic%20Adaptation%20for%0A%20%20Zero-Shot%20Customized%20Video%20Diffusion%20Transformers%0AAuthor%3A%20D.%20She%20and%20Mushui%20Liu%20and%20Jingxuan%20Pang%20and%20Jin%20Wang%20and%20Zhen%20Yang%20and%20Wanggui%20He%20and%20Guanghao%20Zhang%20and%20Yi%20Wang%20and%20Qihan%20Huang%20and%20Haobin%20Tang%20and%20Yunlong%20Yu%20and%20Siming%20Fu%0AAbstract%3A%20%20%20Customized%20generation%20has%20achieved%20significant%20progress%20in%20image%20synthesis%2C%0Ayet%20personalized%20video%20generation%20remains%20challenging%20due%20to%20temporal%0Ainconsistencies%20and%20quality%20degradation.%20In%20this%20paper%2C%20we%20introduce%0ACustomVideoX%2C%20an%20innovative%20framework%20leveraging%20the%20video%20diffusion%0Atransformer%20for%20personalized%20video%20generation%20from%20a%20reference%20image.%0ACustomVideoX%20capitalizes%20on%20pre-trained%20video%20networks%20by%20exclusively%20training%0Athe%20LoRA%20parameters%20to%20extract%20reference%20features%2C%20ensuring%20both%20efficiency%20and%0Aadaptability.%20To%20facilitate%20seamless%20interaction%20between%20the%20reference%20image%0Aand%20video%20content%2C%20we%20propose%203D%20Reference%20Attention%2C%20which%20enables%20direct%20and%0Asimultaneous%20engagement%20of%20reference%20image%20features%20with%20all%20video%20frames%0Aacross%20spatial%20and%20temporal%20dimensions.%20To%20mitigate%20the%20excessive%20influence%20of%0Areference%20image%20features%20and%20textual%20guidance%20on%20generated%20video%20content%20during%0Ainference%2C%20we%20implement%20the%20Time-Aware%20Reference%20Attention%20Bias%20%28TAB%29%20strategy%2C%0Adynamically%20modulating%20reference%20bias%20over%20different%20time%20steps.%20Additionally%2C%0Awe%20introduce%20the%20Entity%20Region-Aware%20Enhancement%20%28ERAE%29%20module%2C%20aligning%20highly%0Aactivated%20regions%20of%20key%20entity%20tokens%20with%20reference%20feature%20injection%20by%0Aadjusting%20attention%20bias.%20To%20thoroughly%20evaluate%20personalized%20video%20generation%2C%0Awe%20establish%20a%20new%20benchmark%2C%20VideoBench%2C%20comprising%20over%2050%20objects%20and%20100%0Aprompts%20for%20extensive%20assessment.%20Experimental%20results%20show%20that%20CustomVideoX%0Asignificantly%20outperforms%20existing%20methods%20in%20terms%20of%20video%20consistency%20and%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomVideoX%253A%25203D%2520Reference%2520Attention%2520Driven%2520Dynamic%2520Adaptation%2520for%250A%2520%2520Zero-Shot%2520Customized%2520Video%2520Diffusion%2520Transformers%26entry.906535625%3DD.%2520She%2520and%2520Mushui%2520Liu%2520and%2520Jingxuan%2520Pang%2520and%2520Jin%2520Wang%2520and%2520Zhen%2520Yang%2520and%2520Wanggui%2520He%2520and%2520Guanghao%2520Zhang%2520and%2520Yi%2520Wang%2520and%2520Qihan%2520Huang%2520and%2520Haobin%2520Tang%2520and%2520Yunlong%2520Yu%2520and%2520Siming%2520Fu%26entry.1292438233%3D%2520%2520Customized%2520generation%2520has%2520achieved%2520significant%2520progress%2520in%2520image%2520synthesis%252C%250Ayet%2520personalized%2520video%2520generation%2520remains%2520challenging%2520due%2520to%2520temporal%250Ainconsistencies%2520and%2520quality%2520degradation.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ACustomVideoX%252C%2520an%2520innovative%2520framework%2520leveraging%2520the%2520video%2520diffusion%250Atransformer%2520for%2520personalized%2520video%2520generation%2520from%2520a%2520reference%2520image.%250ACustomVideoX%2520capitalizes%2520on%2520pre-trained%2520video%2520networks%2520by%2520exclusively%2520training%250Athe%2520LoRA%2520parameters%2520to%2520extract%2520reference%2520features%252C%2520ensuring%2520both%2520efficiency%2520and%250Aadaptability.%2520To%2520facilitate%2520seamless%2520interaction%2520between%2520the%2520reference%2520image%250Aand%2520video%2520content%252C%2520we%2520propose%25203D%2520Reference%2520Attention%252C%2520which%2520enables%2520direct%2520and%250Asimultaneous%2520engagement%2520of%2520reference%2520image%2520features%2520with%2520all%2520video%2520frames%250Aacross%2520spatial%2520and%2520temporal%2520dimensions.%2520To%2520mitigate%2520the%2520excessive%2520influence%2520of%250Areference%2520image%2520features%2520and%2520textual%2520guidance%2520on%2520generated%2520video%2520content%2520during%250Ainference%252C%2520we%2520implement%2520the%2520Time-Aware%2520Reference%2520Attention%2520Bias%2520%2528TAB%2529%2520strategy%252C%250Adynamically%2520modulating%2520reference%2520bias%2520over%2520different%2520time%2520steps.%2520Additionally%252C%250Awe%2520introduce%2520the%2520Entity%2520Region-Aware%2520Enhancement%2520%2528ERAE%2529%2520module%252C%2520aligning%2520highly%250Aactivated%2520regions%2520of%2520key%2520entity%2520tokens%2520with%2520reference%2520feature%2520injection%2520by%250Aadjusting%2520attention%2520bias.%2520To%2520thoroughly%2520evaluate%2520personalized%2520video%2520generation%252C%250Awe%2520establish%2520a%2520new%2520benchmark%252C%2520VideoBench%252C%2520comprising%2520over%252050%2520objects%2520and%2520100%250Aprompts%2520for%2520extensive%2520assessment.%2520Experimental%2520results%2520show%2520that%2520CustomVideoX%250Asignificantly%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520video%2520consistency%2520and%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CustomVideoX%3A%203D%20Reference%20Attention%20Driven%20Dynamic%20Adaptation%20for%0A%20%20Zero-Shot%20Customized%20Video%20Diffusion%20Transformers&entry.906535625=D.%20She%20and%20Mushui%20Liu%20and%20Jingxuan%20Pang%20and%20Jin%20Wang%20and%20Zhen%20Yang%20and%20Wanggui%20He%20and%20Guanghao%20Zhang%20and%20Yi%20Wang%20and%20Qihan%20Huang%20and%20Haobin%20Tang%20and%20Yunlong%20Yu%20and%20Siming%20Fu&entry.1292438233=%20%20Customized%20generation%20has%20achieved%20significant%20progress%20in%20image%20synthesis%2C%0Ayet%20personalized%20video%20generation%20remains%20challenging%20due%20to%20temporal%0Ainconsistencies%20and%20quality%20degradation.%20In%20this%20paper%2C%20we%20introduce%0ACustomVideoX%2C%20an%20innovative%20framework%20leveraging%20the%20video%20diffusion%0Atransformer%20for%20personalized%20video%20generation%20from%20a%20reference%20image.%0ACustomVideoX%20capitalizes%20on%20pre-trained%20video%20networks%20by%20exclusively%20training%0Athe%20LoRA%20parameters%20to%20extract%20reference%20features%2C%20ensuring%20both%20efficiency%20and%0Aadaptability.%20To%20facilitate%20seamless%20interaction%20between%20the%20reference%20image%0Aand%20video%20content%2C%20we%20propose%203D%20Reference%20Attention%2C%20which%20enables%20direct%20and%0Asimultaneous%20engagement%20of%20reference%20image%20features%20with%20all%20video%20frames%0Aacross%20spatial%20and%20temporal%20dimensions.%20To%20mitigate%20the%20excessive%20influence%20of%0Areference%20image%20features%20and%20textual%20guidance%20on%20generated%20video%20content%20during%0Ainference%2C%20we%20implement%20the%20Time-Aware%20Reference%20Attention%20Bias%20%28TAB%29%20strategy%2C%0Adynamically%20modulating%20reference%20bias%20over%20different%20time%20steps.%20Additionally%2C%0Awe%20introduce%20the%20Entity%20Region-Aware%20Enhancement%20%28ERAE%29%20module%2C%20aligning%20highly%0Aactivated%20regions%20of%20key%20entity%20tokens%20with%20reference%20feature%20injection%20by%0Aadjusting%20attention%20bias.%20To%20thoroughly%20evaluate%20personalized%20video%20generation%2C%0Awe%20establish%20a%20new%20benchmark%2C%20VideoBench%2C%20comprising%20over%2050%20objects%20and%20100%0Aprompts%20for%20extensive%20assessment.%20Experimental%20results%20show%20that%20CustomVideoX%0Asignificantly%20outperforms%20existing%20methods%20in%20terms%20of%20video%20consistency%20and%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06527v1&entry.124074799=Read"},
{"title": "A Lightweight Attention-based Deep Network via Multi-Scale Feature\n  Fusion for Multi-View Facial Expression Recognition", "author": "Ali Ezati and Mohammadreza Dezyani and Rajib Rana and Roozbeh Rajabi and Ahmad Ayatollahi", "abstract": "  Convolutional neural networks (CNNs) and their variations have shown\neffectiveness in facial expression recognition (FER). However, they face\nchallenges when dealing with high computational complexity and multi-view head\nposes in real-world scenarios. We introduce a lightweight attentional network\nincorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For\nthe first challenge, we carefully design a lightweight network. We address the\nsecond challenge by presenting two novel components, namely mass attention\n(MassAtt) and point wise feature selection (PWFS) blocks. The MassAtt block\nsimultaneously generates channel and spatial attention maps to recalibrate\nfeature maps by emphasizing important features while suppressing irrelevant\nones. In addition, the PWFS block employs a feature selection mechanism that\ndiscards less meaningful features prior to the fusion process. This mechanism\ndistinguishes it from previous methods that directly fuse multi-scale features.\nOur proposed approach achieved results comparable to state-of-the-art methods\nin terms of parameter count and robustness to pose variation, with accuracy\nrates of 90.77% on KDEF, 70.44% on FER-2013, and 86.96% on FERPlus datasets.\nThe code for LANMSFF is available at https://github.com/AE-1129/LANMSFF.\n", "link": "http://arxiv.org/abs/2403.14318v2", "date": "2025-02-10", "relevancy": 2.6873, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5455}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5373}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Attention-based%20Deep%20Network%20via%20Multi-Scale%20Feature%0A%20%20Fusion%20for%20Multi-View%20Facial%20Expression%20Recognition&body=Title%3A%20A%20Lightweight%20Attention-based%20Deep%20Network%20via%20Multi-Scale%20Feature%0A%20%20Fusion%20for%20Multi-View%20Facial%20Expression%20Recognition%0AAuthor%3A%20Ali%20Ezati%20and%20Mohammadreza%20Dezyani%20and%20Rajib%20Rana%20and%20Roozbeh%20Rajabi%20and%20Ahmad%20Ayatollahi%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20and%20their%20variations%20have%20shown%0Aeffectiveness%20in%20facial%20expression%20recognition%20%28FER%29.%20However%2C%20they%20face%0Achallenges%20when%20dealing%20with%20high%20computational%20complexity%20and%20multi-view%20head%0Aposes%20in%20real-world%20scenarios.%20We%20introduce%20a%20lightweight%20attentional%20network%0Aincorporating%20multi-scale%20feature%20fusion%20%28LANMSFF%29%20to%20tackle%20these%20issues.%20For%0Athe%20first%20challenge%2C%20we%20carefully%20design%20a%20lightweight%20network.%20We%20address%20the%0Asecond%20challenge%20by%20presenting%20two%20novel%20components%2C%20namely%20mass%20attention%0A%28MassAtt%29%20and%20point%20wise%20feature%20selection%20%28PWFS%29%20blocks.%20The%20MassAtt%20block%0Asimultaneously%20generates%20channel%20and%20spatial%20attention%20maps%20to%20recalibrate%0Afeature%20maps%20by%20emphasizing%20important%20features%20while%20suppressing%20irrelevant%0Aones.%20In%20addition%2C%20the%20PWFS%20block%20employs%20a%20feature%20selection%20mechanism%20that%0Adiscards%20less%20meaningful%20features%20prior%20to%20the%20fusion%20process.%20This%20mechanism%0Adistinguishes%20it%20from%20previous%20methods%20that%20directly%20fuse%20multi-scale%20features.%0AOur%20proposed%20approach%20achieved%20results%20comparable%20to%20state-of-the-art%20methods%0Ain%20terms%20of%20parameter%20count%20and%20robustness%20to%20pose%20variation%2C%20with%20accuracy%0Arates%20of%2090.77%25%20on%20KDEF%2C%2070.44%25%20on%20FER-2013%2C%20and%2086.96%25%20on%20FERPlus%20datasets.%0AThe%20code%20for%20LANMSFF%20is%20available%20at%20https%3A//github.com/AE-1129/LANMSFF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Attention-based%2520Deep%2520Network%2520via%2520Multi-Scale%2520Feature%250A%2520%2520Fusion%2520for%2520Multi-View%2520Facial%2520Expression%2520Recognition%26entry.906535625%3DAli%2520Ezati%2520and%2520Mohammadreza%2520Dezyani%2520and%2520Rajib%2520Rana%2520and%2520Roozbeh%2520Rajabi%2520and%2520Ahmad%2520Ayatollahi%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520their%2520variations%2520have%2520shown%250Aeffectiveness%2520in%2520facial%2520expression%2520recognition%2520%2528FER%2529.%2520However%252C%2520they%2520face%250Achallenges%2520when%2520dealing%2520with%2520high%2520computational%2520complexity%2520and%2520multi-view%2520head%250Aposes%2520in%2520real-world%2520scenarios.%2520We%2520introduce%2520a%2520lightweight%2520attentional%2520network%250Aincorporating%2520multi-scale%2520feature%2520fusion%2520%2528LANMSFF%2529%2520to%2520tackle%2520these%2520issues.%2520For%250Athe%2520first%2520challenge%252C%2520we%2520carefully%2520design%2520a%2520lightweight%2520network.%2520We%2520address%2520the%250Asecond%2520challenge%2520by%2520presenting%2520two%2520novel%2520components%252C%2520namely%2520mass%2520attention%250A%2528MassAtt%2529%2520and%2520point%2520wise%2520feature%2520selection%2520%2528PWFS%2529%2520blocks.%2520The%2520MassAtt%2520block%250Asimultaneously%2520generates%2520channel%2520and%2520spatial%2520attention%2520maps%2520to%2520recalibrate%250Afeature%2520maps%2520by%2520emphasizing%2520important%2520features%2520while%2520suppressing%2520irrelevant%250Aones.%2520In%2520addition%252C%2520the%2520PWFS%2520block%2520employs%2520a%2520feature%2520selection%2520mechanism%2520that%250Adiscards%2520less%2520meaningful%2520features%2520prior%2520to%2520the%2520fusion%2520process.%2520This%2520mechanism%250Adistinguishes%2520it%2520from%2520previous%2520methods%2520that%2520directly%2520fuse%2520multi-scale%2520features.%250AOur%2520proposed%2520approach%2520achieved%2520results%2520comparable%2520to%2520state-of-the-art%2520methods%250Ain%2520terms%2520of%2520parameter%2520count%2520and%2520robustness%2520to%2520pose%2520variation%252C%2520with%2520accuracy%250Arates%2520of%252090.77%2525%2520on%2520KDEF%252C%252070.44%2525%2520on%2520FER-2013%252C%2520and%252086.96%2525%2520on%2520FERPlus%2520datasets.%250AThe%2520code%2520for%2520LANMSFF%2520is%2520available%2520at%2520https%253A//github.com/AE-1129/LANMSFF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Attention-based%20Deep%20Network%20via%20Multi-Scale%20Feature%0A%20%20Fusion%20for%20Multi-View%20Facial%20Expression%20Recognition&entry.906535625=Ali%20Ezati%20and%20Mohammadreza%20Dezyani%20and%20Rajib%20Rana%20and%20Roozbeh%20Rajabi%20and%20Ahmad%20Ayatollahi&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20and%20their%20variations%20have%20shown%0Aeffectiveness%20in%20facial%20expression%20recognition%20%28FER%29.%20However%2C%20they%20face%0Achallenges%20when%20dealing%20with%20high%20computational%20complexity%20and%20multi-view%20head%0Aposes%20in%20real-world%20scenarios.%20We%20introduce%20a%20lightweight%20attentional%20network%0Aincorporating%20multi-scale%20feature%20fusion%20%28LANMSFF%29%20to%20tackle%20these%20issues.%20For%0Athe%20first%20challenge%2C%20we%20carefully%20design%20a%20lightweight%20network.%20We%20address%20the%0Asecond%20challenge%20by%20presenting%20two%20novel%20components%2C%20namely%20mass%20attention%0A%28MassAtt%29%20and%20point%20wise%20feature%20selection%20%28PWFS%29%20blocks.%20The%20MassAtt%20block%0Asimultaneously%20generates%20channel%20and%20spatial%20attention%20maps%20to%20recalibrate%0Afeature%20maps%20by%20emphasizing%20important%20features%20while%20suppressing%20irrelevant%0Aones.%20In%20addition%2C%20the%20PWFS%20block%20employs%20a%20feature%20selection%20mechanism%20that%0Adiscards%20less%20meaningful%20features%20prior%20to%20the%20fusion%20process.%20This%20mechanism%0Adistinguishes%20it%20from%20previous%20methods%20that%20directly%20fuse%20multi-scale%20features.%0AOur%20proposed%20approach%20achieved%20results%20comparable%20to%20state-of-the-art%20methods%0Ain%20terms%20of%20parameter%20count%20and%20robustness%20to%20pose%20variation%2C%20with%20accuracy%0Arates%20of%2090.77%25%20on%20KDEF%2C%2070.44%25%20on%20FER-2013%2C%20and%2086.96%25%20on%20FERPlus%20datasets.%0AThe%20code%20for%20LANMSFF%20is%20available%20at%20https%3A//github.com/AE-1129/LANMSFF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14318v2&entry.124074799=Read"},
{"title": "Adaptive Reconstruction for Graph Neural Networks", "author": "Dong Liu", "abstract": "  Graph Neural Networks (GNNs) have become fundamental in semi-supervised\nlearning for graph representation, leveraging their ability to capture complex\nnode relationships. A recent trend in GNN research focuses on \\textbf{adaptive\nk-hop structure learning}, moving beyond fixed-hop aggregation to more flexible\nand dynamic neighborhood selection. While GAMLP \\cite{Zhang_2022} employs\nseparate MLP layers for each k-hop domain and ImprovingTE\n\\cite{Yao2023ImprovingTE} enhances this by injecting contextualized\nsubstructure information, these methods still rely heavily on predefined\nsampling strategies, which may limit their ability to generalize and maintain\nstable accuracy. To address these limitations, we propose an \\textbf{adaptive\nreconstruction framework} that dynamically refines k-hop structure learning.\nInspired by \"coreset selection\" \\cite{guo2022deepcore}, our approach adaptively\n\\textbf{reconstructs} node neighborhoods to optimize message passing, ensuring\nmore \\textbf{effective and context-aware information flow} across the graph. To\nfurther enhance structural robustness, we introduce two key modules: the\n\\textbf{Distance Recomputator} and the \\textbf{Topology Reconstructor}\n(\\textcolor{blue}{DRTR}). The Distance Recomputator \\textbf{reassesses and\nrecalibrates} node distances based on adaptive graph properties, leading to\n\\textbf{improved node embeddings} that better reflect latent relationships.\nMeanwhile, the Topology Reconstructor \\textbf{dynamically refines local graph\nstructures}, enabling the model to \\textbf{adapt to evolving graph topologies}\nand mitigate the impact of noise and mislabeled data. Empirical evaluations\ndemonstrate that our \\textbf{adaptive reconstruction framework} achieves\n\\textbf{significant improvements} over existing k-hop-based models, providing\nmore \\textbf{stable and accurate} performance in various graph learning\nbenchmarks.\n", "link": "http://arxiv.org/abs/2406.17281v2", "date": "2025-02-10", "relevancy": 2.6609, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5417}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5283}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Reconstruction%20for%20Graph%20Neural%20Networks&body=Title%3A%20Adaptive%20Reconstruction%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Dong%20Liu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20fundamental%20in%20semi-supervised%0Alearning%20for%20graph%20representation%2C%20leveraging%20their%20ability%20to%20capture%20complex%0Anode%20relationships.%20A%20recent%20trend%20in%20GNN%20research%20focuses%20on%20%5Ctextbf%7Badaptive%0Ak-hop%20structure%20learning%7D%2C%20moving%20beyond%20fixed-hop%20aggregation%20to%20more%20flexible%0Aand%20dynamic%20neighborhood%20selection.%20While%20GAMLP%20%5Ccite%7BZhang_2022%7D%20employs%0Aseparate%20MLP%20layers%20for%20each%20k-hop%20domain%20and%20ImprovingTE%0A%5Ccite%7BYao2023ImprovingTE%7D%20enhances%20this%20by%20injecting%20contextualized%0Asubstructure%20information%2C%20these%20methods%20still%20rely%20heavily%20on%20predefined%0Asampling%20strategies%2C%20which%20may%20limit%20their%20ability%20to%20generalize%20and%20maintain%0Astable%20accuracy.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20%5Ctextbf%7Badaptive%0Areconstruction%20framework%7D%20that%20dynamically%20refines%20k-hop%20structure%20learning.%0AInspired%20by%20%22coreset%20selection%22%20%5Ccite%7Bguo2022deepcore%7D%2C%20our%20approach%20adaptively%0A%5Ctextbf%7Breconstructs%7D%20node%20neighborhoods%20to%20optimize%20message%20passing%2C%20ensuring%0Amore%20%5Ctextbf%7Beffective%20and%20context-aware%20information%20flow%7D%20across%20the%20graph.%20To%0Afurther%20enhance%20structural%20robustness%2C%20we%20introduce%20two%20key%20modules%3A%20the%0A%5Ctextbf%7BDistance%20Recomputator%7D%20and%20the%20%5Ctextbf%7BTopology%20Reconstructor%7D%0A%28%5Ctextcolor%7Bblue%7D%7BDRTR%7D%29.%20The%20Distance%20Recomputator%20%5Ctextbf%7Breassesses%20and%0Arecalibrates%7D%20node%20distances%20based%20on%20adaptive%20graph%20properties%2C%20leading%20to%0A%5Ctextbf%7Bimproved%20node%20embeddings%7D%20that%20better%20reflect%20latent%20relationships.%0AMeanwhile%2C%20the%20Topology%20Reconstructor%20%5Ctextbf%7Bdynamically%20refines%20local%20graph%0Astructures%7D%2C%20enabling%20the%20model%20to%20%5Ctextbf%7Badapt%20to%20evolving%20graph%20topologies%7D%0Aand%20mitigate%20the%20impact%20of%20noise%20and%20mislabeled%20data.%20Empirical%20evaluations%0Ademonstrate%20that%20our%20%5Ctextbf%7Badaptive%20reconstruction%20framework%7D%20achieves%0A%5Ctextbf%7Bsignificant%20improvements%7D%20over%20existing%20k-hop-based%20models%2C%20providing%0Amore%20%5Ctextbf%7Bstable%20and%20accurate%7D%20performance%20in%20various%20graph%20learning%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17281v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Reconstruction%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DDong%2520Liu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520fundamental%2520in%2520semi-supervised%250Alearning%2520for%2520graph%2520representation%252C%2520leveraging%2520their%2520ability%2520to%2520capture%2520complex%250Anode%2520relationships.%2520A%2520recent%2520trend%2520in%2520GNN%2520research%2520focuses%2520on%2520%255Ctextbf%257Badaptive%250Ak-hop%2520structure%2520learning%257D%252C%2520moving%2520beyond%2520fixed-hop%2520aggregation%2520to%2520more%2520flexible%250Aand%2520dynamic%2520neighborhood%2520selection.%2520While%2520GAMLP%2520%255Ccite%257BZhang_2022%257D%2520employs%250Aseparate%2520MLP%2520layers%2520for%2520each%2520k-hop%2520domain%2520and%2520ImprovingTE%250A%255Ccite%257BYao2023ImprovingTE%257D%2520enhances%2520this%2520by%2520injecting%2520contextualized%250Asubstructure%2520information%252C%2520these%2520methods%2520still%2520rely%2520heavily%2520on%2520predefined%250Asampling%2520strategies%252C%2520which%2520may%2520limit%2520their%2520ability%2520to%2520generalize%2520and%2520maintain%250Astable%2520accuracy.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520an%2520%255Ctextbf%257Badaptive%250Areconstruction%2520framework%257D%2520that%2520dynamically%2520refines%2520k-hop%2520structure%2520learning.%250AInspired%2520by%2520%2522coreset%2520selection%2522%2520%255Ccite%257Bguo2022deepcore%257D%252C%2520our%2520approach%2520adaptively%250A%255Ctextbf%257Breconstructs%257D%2520node%2520neighborhoods%2520to%2520optimize%2520message%2520passing%252C%2520ensuring%250Amore%2520%255Ctextbf%257Beffective%2520and%2520context-aware%2520information%2520flow%257D%2520across%2520the%2520graph.%2520To%250Afurther%2520enhance%2520structural%2520robustness%252C%2520we%2520introduce%2520two%2520key%2520modules%253A%2520the%250A%255Ctextbf%257BDistance%2520Recomputator%257D%2520and%2520the%2520%255Ctextbf%257BTopology%2520Reconstructor%257D%250A%2528%255Ctextcolor%257Bblue%257D%257BDRTR%257D%2529.%2520The%2520Distance%2520Recomputator%2520%255Ctextbf%257Breassesses%2520and%250Arecalibrates%257D%2520node%2520distances%2520based%2520on%2520adaptive%2520graph%2520properties%252C%2520leading%2520to%250A%255Ctextbf%257Bimproved%2520node%2520embeddings%257D%2520that%2520better%2520reflect%2520latent%2520relationships.%250AMeanwhile%252C%2520the%2520Topology%2520Reconstructor%2520%255Ctextbf%257Bdynamically%2520refines%2520local%2520graph%250Astructures%257D%252C%2520enabling%2520the%2520model%2520to%2520%255Ctextbf%257Badapt%2520to%2520evolving%2520graph%2520topologies%257D%250Aand%2520mitigate%2520the%2520impact%2520of%2520noise%2520and%2520mislabeled%2520data.%2520Empirical%2520evaluations%250Ademonstrate%2520that%2520our%2520%255Ctextbf%257Badaptive%2520reconstruction%2520framework%257D%2520achieves%250A%255Ctextbf%257Bsignificant%2520improvements%257D%2520over%2520existing%2520k-hop-based%2520models%252C%2520providing%250Amore%2520%255Ctextbf%257Bstable%2520and%2520accurate%257D%2520performance%2520in%2520various%2520graph%2520learning%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17281v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Reconstruction%20for%20Graph%20Neural%20Networks&entry.906535625=Dong%20Liu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20fundamental%20in%20semi-supervised%0Alearning%20for%20graph%20representation%2C%20leveraging%20their%20ability%20to%20capture%20complex%0Anode%20relationships.%20A%20recent%20trend%20in%20GNN%20research%20focuses%20on%20%5Ctextbf%7Badaptive%0Ak-hop%20structure%20learning%7D%2C%20moving%20beyond%20fixed-hop%20aggregation%20to%20more%20flexible%0Aand%20dynamic%20neighborhood%20selection.%20While%20GAMLP%20%5Ccite%7BZhang_2022%7D%20employs%0Aseparate%20MLP%20layers%20for%20each%20k-hop%20domain%20and%20ImprovingTE%0A%5Ccite%7BYao2023ImprovingTE%7D%20enhances%20this%20by%20injecting%20contextualized%0Asubstructure%20information%2C%20these%20methods%20still%20rely%20heavily%20on%20predefined%0Asampling%20strategies%2C%20which%20may%20limit%20their%20ability%20to%20generalize%20and%20maintain%0Astable%20accuracy.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20%5Ctextbf%7Badaptive%0Areconstruction%20framework%7D%20that%20dynamically%20refines%20k-hop%20structure%20learning.%0AInspired%20by%20%22coreset%20selection%22%20%5Ccite%7Bguo2022deepcore%7D%2C%20our%20approach%20adaptively%0A%5Ctextbf%7Breconstructs%7D%20node%20neighborhoods%20to%20optimize%20message%20passing%2C%20ensuring%0Amore%20%5Ctextbf%7Beffective%20and%20context-aware%20information%20flow%7D%20across%20the%20graph.%20To%0Afurther%20enhance%20structural%20robustness%2C%20we%20introduce%20two%20key%20modules%3A%20the%0A%5Ctextbf%7BDistance%20Recomputator%7D%20and%20the%20%5Ctextbf%7BTopology%20Reconstructor%7D%0A%28%5Ctextcolor%7Bblue%7D%7BDRTR%7D%29.%20The%20Distance%20Recomputator%20%5Ctextbf%7Breassesses%20and%0Arecalibrates%7D%20node%20distances%20based%20on%20adaptive%20graph%20properties%2C%20leading%20to%0A%5Ctextbf%7Bimproved%20node%20embeddings%7D%20that%20better%20reflect%20latent%20relationships.%0AMeanwhile%2C%20the%20Topology%20Reconstructor%20%5Ctextbf%7Bdynamically%20refines%20local%20graph%0Astructures%7D%2C%20enabling%20the%20model%20to%20%5Ctextbf%7Badapt%20to%20evolving%20graph%20topologies%7D%0Aand%20mitigate%20the%20impact%20of%20noise%20and%20mislabeled%20data.%20Empirical%20evaluations%0Ademonstrate%20that%20our%20%5Ctextbf%7Badaptive%20reconstruction%20framework%7D%20achieves%0A%5Ctextbf%7Bsignificant%20improvements%7D%20over%20existing%20k-hop-based%20models%2C%20providing%0Amore%20%5Ctextbf%7Bstable%20and%20accurate%7D%20performance%20in%20various%20graph%20learning%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17281v2&entry.124074799=Read"},
{"title": "Group-CLIP Uncertainty Modeling for Group Re-Identification", "author": "Qingxin Zhang and Haoyan Wei and Yang Qian", "abstract": "  Group Re-Identification (Group ReID) aims matching groups of pedestrians\nacross non-overlapping cameras. Unlike single-person ReID, Group ReID focuses\nmore on the changes in group structure, emphasizing the number of members and\ntheir spatial arrangement. However, most methods rely on certainty-based\nmodels, which consider only the specific group structures in the group images,\noften failing to match unseen group configurations. To this end, we propose a\nnovel Group-CLIP UncertaintyModeling (GCUM) approach that adapts group text\ndescriptions to undetermined accommodate member and layout variations.\nSpecifically, we design a Member Variant Simulation (MVS)module that simulates\nmember exclusions using a Bernoulli distribution and a Group Layout Adaptation\n(GLA) module that generates uncertain group text descriptions with\nidentity-specific tokens. In addition, we design a Group\nRelationshipConstruction Encoder (GRCE) that uses group features to refine\nindividual features, and employ cross-modal contrastive loss to obtain\ngeneralizable knowledge from group text descriptions. It is worth noting that\nwe are the first to employ CLIP to GroupReID, and extensive experiments show\nthat GCUM significantly outperforms state-of-the-art Group ReID methods.\n", "link": "http://arxiv.org/abs/2502.06460v1", "date": "2025-02-10", "relevancy": 2.6561, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5507}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5237}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group-CLIP%20Uncertainty%20Modeling%20for%20Group%20Re-Identification&body=Title%3A%20Group-CLIP%20Uncertainty%20Modeling%20for%20Group%20Re-Identification%0AAuthor%3A%20Qingxin%20Zhang%20and%20Haoyan%20Wei%20and%20Yang%20Qian%0AAbstract%3A%20%20%20Group%20Re-Identification%20%28Group%20ReID%29%20aims%20matching%20groups%20of%20pedestrians%0Aacross%20non-overlapping%20cameras.%20Unlike%20single-person%20ReID%2C%20Group%20ReID%20focuses%0Amore%20on%20the%20changes%20in%20group%20structure%2C%20emphasizing%20the%20number%20of%20members%20and%0Atheir%20spatial%20arrangement.%20However%2C%20most%20methods%20rely%20on%20certainty-based%0Amodels%2C%20which%20consider%20only%20the%20specific%20group%20structures%20in%20the%20group%20images%2C%0Aoften%20failing%20to%20match%20unseen%20group%20configurations.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20Group-CLIP%20UncertaintyModeling%20%28GCUM%29%20approach%20that%20adapts%20group%20text%0Adescriptions%20to%20undetermined%20accommodate%20member%20and%20layout%20variations.%0ASpecifically%2C%20we%20design%20a%20Member%20Variant%20Simulation%20%28MVS%29module%20that%20simulates%0Amember%20exclusions%20using%20a%20Bernoulli%20distribution%20and%20a%20Group%20Layout%20Adaptation%0A%28GLA%29%20module%20that%20generates%20uncertain%20group%20text%20descriptions%20with%0Aidentity-specific%20tokens.%20In%20addition%2C%20we%20design%20a%20Group%0ARelationshipConstruction%20Encoder%20%28GRCE%29%20that%20uses%20group%20features%20to%20refine%0Aindividual%20features%2C%20and%20employ%20cross-modal%20contrastive%20loss%20to%20obtain%0Ageneralizable%20knowledge%20from%20group%20text%20descriptions.%20It%20is%20worth%20noting%20that%0Awe%20are%20the%20first%20to%20employ%20CLIP%20to%20GroupReID%2C%20and%20extensive%20experiments%20show%0Athat%20GCUM%20significantly%20outperforms%20state-of-the-art%20Group%20ReID%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup-CLIP%2520Uncertainty%2520Modeling%2520for%2520Group%2520Re-Identification%26entry.906535625%3DQingxin%2520Zhang%2520and%2520Haoyan%2520Wei%2520and%2520Yang%2520Qian%26entry.1292438233%3D%2520%2520Group%2520Re-Identification%2520%2528Group%2520ReID%2529%2520aims%2520matching%2520groups%2520of%2520pedestrians%250Aacross%2520non-overlapping%2520cameras.%2520Unlike%2520single-person%2520ReID%252C%2520Group%2520ReID%2520focuses%250Amore%2520on%2520the%2520changes%2520in%2520group%2520structure%252C%2520emphasizing%2520the%2520number%2520of%2520members%2520and%250Atheir%2520spatial%2520arrangement.%2520However%252C%2520most%2520methods%2520rely%2520on%2520certainty-based%250Amodels%252C%2520which%2520consider%2520only%2520the%2520specific%2520group%2520structures%2520in%2520the%2520group%2520images%252C%250Aoften%2520failing%2520to%2520match%2520unseen%2520group%2520configurations.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Anovel%2520Group-CLIP%2520UncertaintyModeling%2520%2528GCUM%2529%2520approach%2520that%2520adapts%2520group%2520text%250Adescriptions%2520to%2520undetermined%2520accommodate%2520member%2520and%2520layout%2520variations.%250ASpecifically%252C%2520we%2520design%2520a%2520Member%2520Variant%2520Simulation%2520%2528MVS%2529module%2520that%2520simulates%250Amember%2520exclusions%2520using%2520a%2520Bernoulli%2520distribution%2520and%2520a%2520Group%2520Layout%2520Adaptation%250A%2528GLA%2529%2520module%2520that%2520generates%2520uncertain%2520group%2520text%2520descriptions%2520with%250Aidentity-specific%2520tokens.%2520In%2520addition%252C%2520we%2520design%2520a%2520Group%250ARelationshipConstruction%2520Encoder%2520%2528GRCE%2529%2520that%2520uses%2520group%2520features%2520to%2520refine%250Aindividual%2520features%252C%2520and%2520employ%2520cross-modal%2520contrastive%2520loss%2520to%2520obtain%250Ageneralizable%2520knowledge%2520from%2520group%2520text%2520descriptions.%2520It%2520is%2520worth%2520noting%2520that%250Awe%2520are%2520the%2520first%2520to%2520employ%2520CLIP%2520to%2520GroupReID%252C%2520and%2520extensive%2520experiments%2520show%250Athat%2520GCUM%2520significantly%2520outperforms%2520state-of-the-art%2520Group%2520ReID%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group-CLIP%20Uncertainty%20Modeling%20for%20Group%20Re-Identification&entry.906535625=Qingxin%20Zhang%20and%20Haoyan%20Wei%20and%20Yang%20Qian&entry.1292438233=%20%20Group%20Re-Identification%20%28Group%20ReID%29%20aims%20matching%20groups%20of%20pedestrians%0Aacross%20non-overlapping%20cameras.%20Unlike%20single-person%20ReID%2C%20Group%20ReID%20focuses%0Amore%20on%20the%20changes%20in%20group%20structure%2C%20emphasizing%20the%20number%20of%20members%20and%0Atheir%20spatial%20arrangement.%20However%2C%20most%20methods%20rely%20on%20certainty-based%0Amodels%2C%20which%20consider%20only%20the%20specific%20group%20structures%20in%20the%20group%20images%2C%0Aoften%20failing%20to%20match%20unseen%20group%20configurations.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20Group-CLIP%20UncertaintyModeling%20%28GCUM%29%20approach%20that%20adapts%20group%20text%0Adescriptions%20to%20undetermined%20accommodate%20member%20and%20layout%20variations.%0ASpecifically%2C%20we%20design%20a%20Member%20Variant%20Simulation%20%28MVS%29module%20that%20simulates%0Amember%20exclusions%20using%20a%20Bernoulli%20distribution%20and%20a%20Group%20Layout%20Adaptation%0A%28GLA%29%20module%20that%20generates%20uncertain%20group%20text%20descriptions%20with%0Aidentity-specific%20tokens.%20In%20addition%2C%20we%20design%20a%20Group%0ARelationshipConstruction%20Encoder%20%28GRCE%29%20that%20uses%20group%20features%20to%20refine%0Aindividual%20features%2C%20and%20employ%20cross-modal%20contrastive%20loss%20to%20obtain%0Ageneralizable%20knowledge%20from%20group%20text%20descriptions.%20It%20is%20worth%20noting%20that%0Awe%20are%20the%20first%20to%20employ%20CLIP%20to%20GroupReID%2C%20and%20extensive%20experiments%20show%0Athat%20GCUM%20significantly%20outperforms%20state-of-the-art%20Group%20ReID%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06460v1&entry.124074799=Read"},
{"title": "Building Rome with Convex Optimization", "author": "Haoyu Han and Heng Yang", "abstract": "  Global bundle adjustment is made easy by depth prediction and convex\noptimization. We (i) propose a scaled bundle adjustment (SBA) formulation that\nlifts 2D keypoint measurements to 3D with learned depth, (ii) design an\nempirically tight convex semidfinite program (SDP) relaxation that solves SBA\nto certfiable global optimality, (iii) solve the SDP relaxations at extreme\nscale with Burer-Monteiro factorization and a CUDA-based trust-region\nRiemannian optimizer (dubbed XM), (iv) build a structure from motion (SfM)\npipeline with XM as the optimization engine and show that XM-SfM dominates or\ncompares favorably with existing SfM pipelines in terms of reconstruction\nquality while being faster, more scalable, and initialization-free.\n", "link": "http://arxiv.org/abs/2502.04640v2", "date": "2025-02-10", "relevancy": 2.6448, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5484}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5244}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Rome%20with%20Convex%20Optimization&body=Title%3A%20Building%20Rome%20with%20Convex%20Optimization%0AAuthor%3A%20Haoyu%20Han%20and%20Heng%20Yang%0AAbstract%3A%20%20%20Global%20bundle%20adjustment%20is%20made%20easy%20by%20depth%20prediction%20and%20convex%0Aoptimization.%20We%20%28i%29%20propose%20a%20scaled%20bundle%20adjustment%20%28SBA%29%20formulation%20that%0Alifts%202D%20keypoint%20measurements%20to%203D%20with%20learned%20depth%2C%20%28ii%29%20design%20an%0Aempirically%20tight%20convex%20semidfinite%20program%20%28SDP%29%20relaxation%20that%20solves%20SBA%0Ato%20certfiable%20global%20optimality%2C%20%28iii%29%20solve%20the%20SDP%20relaxations%20at%20extreme%0Ascale%20with%20Burer-Monteiro%20factorization%20and%20a%20CUDA-based%20trust-region%0ARiemannian%20optimizer%20%28dubbed%20XM%29%2C%20%28iv%29%20build%20a%20structure%20from%20motion%20%28SfM%29%0Apipeline%20with%20XM%20as%20the%20optimization%20engine%20and%20show%20that%20XM-SfM%20dominates%20or%0Acompares%20favorably%20with%20existing%20SfM%20pipelines%20in%20terms%20of%20reconstruction%0Aquality%20while%20being%20faster%2C%20more%20scalable%2C%20and%20initialization-free.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Rome%2520with%2520Convex%2520Optimization%26entry.906535625%3DHaoyu%2520Han%2520and%2520Heng%2520Yang%26entry.1292438233%3D%2520%2520Global%2520bundle%2520adjustment%2520is%2520made%2520easy%2520by%2520depth%2520prediction%2520and%2520convex%250Aoptimization.%2520We%2520%2528i%2529%2520propose%2520a%2520scaled%2520bundle%2520adjustment%2520%2528SBA%2529%2520formulation%2520that%250Alifts%25202D%2520keypoint%2520measurements%2520to%25203D%2520with%2520learned%2520depth%252C%2520%2528ii%2529%2520design%2520an%250Aempirically%2520tight%2520convex%2520semidfinite%2520program%2520%2528SDP%2529%2520relaxation%2520that%2520solves%2520SBA%250Ato%2520certfiable%2520global%2520optimality%252C%2520%2528iii%2529%2520solve%2520the%2520SDP%2520relaxations%2520at%2520extreme%250Ascale%2520with%2520Burer-Monteiro%2520factorization%2520and%2520a%2520CUDA-based%2520trust-region%250ARiemannian%2520optimizer%2520%2528dubbed%2520XM%2529%252C%2520%2528iv%2529%2520build%2520a%2520structure%2520from%2520motion%2520%2528SfM%2529%250Apipeline%2520with%2520XM%2520as%2520the%2520optimization%2520engine%2520and%2520show%2520that%2520XM-SfM%2520dominates%2520or%250Acompares%2520favorably%2520with%2520existing%2520SfM%2520pipelines%2520in%2520terms%2520of%2520reconstruction%250Aquality%2520while%2520being%2520faster%252C%2520more%2520scalable%252C%2520and%2520initialization-free.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Rome%20with%20Convex%20Optimization&entry.906535625=Haoyu%20Han%20and%20Heng%20Yang&entry.1292438233=%20%20Global%20bundle%20adjustment%20is%20made%20easy%20by%20depth%20prediction%20and%20convex%0Aoptimization.%20We%20%28i%29%20propose%20a%20scaled%20bundle%20adjustment%20%28SBA%29%20formulation%20that%0Alifts%202D%20keypoint%20measurements%20to%203D%20with%20learned%20depth%2C%20%28ii%29%20design%20an%0Aempirically%20tight%20convex%20semidfinite%20program%20%28SDP%29%20relaxation%20that%20solves%20SBA%0Ato%20certfiable%20global%20optimality%2C%20%28iii%29%20solve%20the%20SDP%20relaxations%20at%20extreme%0Ascale%20with%20Burer-Monteiro%20factorization%20and%20a%20CUDA-based%20trust-region%0ARiemannian%20optimizer%20%28dubbed%20XM%29%2C%20%28iv%29%20build%20a%20structure%20from%20motion%20%28SfM%29%0Apipeline%20with%20XM%20as%20the%20optimization%20engine%20and%20show%20that%20XM-SfM%20dominates%20or%0Acompares%20favorably%20with%20existing%20SfM%20pipelines%20in%20terms%20of%20reconstruction%0Aquality%20while%20being%20faster%2C%20more%20scalable%2C%20and%20initialization-free.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04640v2&entry.124074799=Read"},
{"title": "Neumann eigenmaps for landmark embedding", "author": "Shashank Sule and Wojciech Czaja", "abstract": "  We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing the\nstandard diffusion map embedding using landmarks, i.e distinguished samples\nwithin the dataset. By interpreting these landmarks as a subgraph of the larger\ndata graph, NeuMaps are obtained via the eigendecomposition of a renormalized\nNeumann Laplacian. We show that NeuMaps offer two key advantages: (1) they\nprovide a computationally efficient embedding that accurately recovers the\ndiffusion distance associated with the reflecting random walk on the subgraph,\nand (2) they naturally incorporate the Nystr\\\"om extension within the diffusion\nmap framework through the discrete Neumann boundary condition. Through examples\nin digit classification and molecular dynamics, we demonstrate that NeuMaps not\nonly improve upon existing landmark-based embedding methods but also enhance\nthe stability of diffusion map embeddings to the removal of highly significant\npoints.\n", "link": "http://arxiv.org/abs/2502.06689v1", "date": "2025-02-10", "relevancy": 2.6435, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5474}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5212}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neumann%20eigenmaps%20for%20landmark%20embedding&body=Title%3A%20Neumann%20eigenmaps%20for%20landmark%20embedding%0AAuthor%3A%20Shashank%20Sule%20and%20Wojciech%20Czaja%0AAbstract%3A%20%20%20We%20present%20Neumann%20eigenmaps%20%28NeuMaps%29%2C%20a%20novel%20approach%20for%20enhancing%20the%0Astandard%20diffusion%20map%20embedding%20using%20landmarks%2C%20i.e%20distinguished%20samples%0Awithin%20the%20dataset.%20By%20interpreting%20these%20landmarks%20as%20a%20subgraph%20of%20the%20larger%0Adata%20graph%2C%20NeuMaps%20are%20obtained%20via%20the%20eigendecomposition%20of%20a%20renormalized%0ANeumann%20Laplacian.%20We%20show%20that%20NeuMaps%20offer%20two%20key%20advantages%3A%20%281%29%20they%0Aprovide%20a%20computationally%20efficient%20embedding%20that%20accurately%20recovers%20the%0Adiffusion%20distance%20associated%20with%20the%20reflecting%20random%20walk%20on%20the%20subgraph%2C%0Aand%20%282%29%20they%20naturally%20incorporate%20the%20Nystr%5C%22om%20extension%20within%20the%20diffusion%0Amap%20framework%20through%20the%20discrete%20Neumann%20boundary%20condition.%20Through%20examples%0Ain%20digit%20classification%20and%20molecular%20dynamics%2C%20we%20demonstrate%20that%20NeuMaps%20not%0Aonly%20improve%20upon%20existing%20landmark-based%20embedding%20methods%20but%20also%20enhance%0Athe%20stability%20of%20diffusion%20map%20embeddings%20to%20the%20removal%20of%20highly%20significant%0Apoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeumann%2520eigenmaps%2520for%2520landmark%2520embedding%26entry.906535625%3DShashank%2520Sule%2520and%2520Wojciech%2520Czaja%26entry.1292438233%3D%2520%2520We%2520present%2520Neumann%2520eigenmaps%2520%2528NeuMaps%2529%252C%2520a%2520novel%2520approach%2520for%2520enhancing%2520the%250Astandard%2520diffusion%2520map%2520embedding%2520using%2520landmarks%252C%2520i.e%2520distinguished%2520samples%250Awithin%2520the%2520dataset.%2520By%2520interpreting%2520these%2520landmarks%2520as%2520a%2520subgraph%2520of%2520the%2520larger%250Adata%2520graph%252C%2520NeuMaps%2520are%2520obtained%2520via%2520the%2520eigendecomposition%2520of%2520a%2520renormalized%250ANeumann%2520Laplacian.%2520We%2520show%2520that%2520NeuMaps%2520offer%2520two%2520key%2520advantages%253A%2520%25281%2529%2520they%250Aprovide%2520a%2520computationally%2520efficient%2520embedding%2520that%2520accurately%2520recovers%2520the%250Adiffusion%2520distance%2520associated%2520with%2520the%2520reflecting%2520random%2520walk%2520on%2520the%2520subgraph%252C%250Aand%2520%25282%2529%2520they%2520naturally%2520incorporate%2520the%2520Nystr%255C%2522om%2520extension%2520within%2520the%2520diffusion%250Amap%2520framework%2520through%2520the%2520discrete%2520Neumann%2520boundary%2520condition.%2520Through%2520examples%250Ain%2520digit%2520classification%2520and%2520molecular%2520dynamics%252C%2520we%2520demonstrate%2520that%2520NeuMaps%2520not%250Aonly%2520improve%2520upon%2520existing%2520landmark-based%2520embedding%2520methods%2520but%2520also%2520enhance%250Athe%2520stability%2520of%2520diffusion%2520map%2520embeddings%2520to%2520the%2520removal%2520of%2520highly%2520significant%250Apoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neumann%20eigenmaps%20for%20landmark%20embedding&entry.906535625=Shashank%20Sule%20and%20Wojciech%20Czaja&entry.1292438233=%20%20We%20present%20Neumann%20eigenmaps%20%28NeuMaps%29%2C%20a%20novel%20approach%20for%20enhancing%20the%0Astandard%20diffusion%20map%20embedding%20using%20landmarks%2C%20i.e%20distinguished%20samples%0Awithin%20the%20dataset.%20By%20interpreting%20these%20landmarks%20as%20a%20subgraph%20of%20the%20larger%0Adata%20graph%2C%20NeuMaps%20are%20obtained%20via%20the%20eigendecomposition%20of%20a%20renormalized%0ANeumann%20Laplacian.%20We%20show%20that%20NeuMaps%20offer%20two%20key%20advantages%3A%20%281%29%20they%0Aprovide%20a%20computationally%20efficient%20embedding%20that%20accurately%20recovers%20the%0Adiffusion%20distance%20associated%20with%20the%20reflecting%20random%20walk%20on%20the%20subgraph%2C%0Aand%20%282%29%20they%20naturally%20incorporate%20the%20Nystr%5C%22om%20extension%20within%20the%20diffusion%0Amap%20framework%20through%20the%20discrete%20Neumann%20boundary%20condition.%20Through%20examples%0Ain%20digit%20classification%20and%20molecular%20dynamics%2C%20we%20demonstrate%20that%20NeuMaps%20not%0Aonly%20improve%20upon%20existing%20landmark-based%20embedding%20methods%20but%20also%20enhance%0Athe%20stability%20of%20diffusion%20map%20embeddings%20to%20the%20removal%20of%20highly%20significant%0Apoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06689v1&entry.124074799=Read"},
{"title": "Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning\n  Approach", "author": "Giovanni Luca Marchetti and Gabriele Cesa and Pratik Kumar and Arash Behboodi", "abstract": "  Lattice reduction is a combinatorial optimization problem aimed at finding\nthe most orthogonal basis in a given lattice. The Lenstra-Lenstra-Lov\\'asz\n(LLL) algorithm is the best algorithm in the literature for solving this\nproblem. In light of recent research on algorithm discovery, in this work, we\nwould like to answer this question: is it possible to parametrize the algorithm\nspace for lattice reduction problem with neural networks and find an algorithm\nwithout supervised data? Our strategy is to use equivariant and invariant\nparametrizations and train in a self-supervised way. We design a deep neural\nmodel outputting factorized unimodular matrices and train it in a\nself-supervised manner by penalizing non-orthogonal lattice bases. We\nincorporate the symmetries of lattice reduction into the model by making it\ninvariant to isometries and scaling of the ambient space and equivariant with\nrespect to the hyperocrahedral group permuting and flipping the lattice basis\nelements. We show that this approach yields an algorithm with comparable\ncomplexity and performance to the LLL algorithm on a set of benchmarks.\nAdditionally, motivated by certain applications for wireless communication, we\nextend our method to a convolutional architecture which performs joint\nreduction of spatially-correlated lattices arranged in a grid, thereby\namortizing its cost over multiple lattices.\n", "link": "http://arxiv.org/abs/2311.08170v2", "date": "2025-02-10", "relevancy": 2.6391, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.592}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5005}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Lattice%20Reduction%3A%20A%20Self-Supervised%20Geometric%20Deep%20Learning%0A%20%20Approach&body=Title%3A%20Neural%20Lattice%20Reduction%3A%20A%20Self-Supervised%20Geometric%20Deep%20Learning%0A%20%20Approach%0AAuthor%3A%20Giovanni%20Luca%20Marchetti%20and%20Gabriele%20Cesa%20and%20Pratik%20Kumar%20and%20Arash%20Behboodi%0AAbstract%3A%20%20%20Lattice%20reduction%20is%20a%20combinatorial%20optimization%20problem%20aimed%20at%20finding%0Athe%20most%20orthogonal%20basis%20in%20a%20given%20lattice.%20The%20Lenstra-Lenstra-Lov%5C%27asz%0A%28LLL%29%20algorithm%20is%20the%20best%20algorithm%20in%20the%20literature%20for%20solving%20this%0Aproblem.%20In%20light%20of%20recent%20research%20on%20algorithm%20discovery%2C%20in%20this%20work%2C%20we%0Awould%20like%20to%20answer%20this%20question%3A%20is%20it%20possible%20to%20parametrize%20the%20algorithm%0Aspace%20for%20lattice%20reduction%20problem%20with%20neural%20networks%20and%20find%20an%20algorithm%0Awithout%20supervised%20data%3F%20Our%20strategy%20is%20to%20use%20equivariant%20and%20invariant%0Aparametrizations%20and%20train%20in%20a%20self-supervised%20way.%20We%20design%20a%20deep%20neural%0Amodel%20outputting%20factorized%20unimodular%20matrices%20and%20train%20it%20in%20a%0Aself-supervised%20manner%20by%20penalizing%20non-orthogonal%20lattice%20bases.%20We%0Aincorporate%20the%20symmetries%20of%20lattice%20reduction%20into%20the%20model%20by%20making%20it%0Ainvariant%20to%20isometries%20and%20scaling%20of%20the%20ambient%20space%20and%20equivariant%20with%0Arespect%20to%20the%20hyperocrahedral%20group%20permuting%20and%20flipping%20the%20lattice%20basis%0Aelements.%20We%20show%20that%20this%20approach%20yields%20an%20algorithm%20with%20comparable%0Acomplexity%20and%20performance%20to%20the%20LLL%20algorithm%20on%20a%20set%20of%20benchmarks.%0AAdditionally%2C%20motivated%20by%20certain%20applications%20for%20wireless%20communication%2C%20we%0Aextend%20our%20method%20to%20a%20convolutional%20architecture%20which%20performs%20joint%0Areduction%20of%20spatially-correlated%20lattices%20arranged%20in%20a%20grid%2C%20thereby%0Aamortizing%20its%20cost%20over%20multiple%20lattices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Lattice%2520Reduction%253A%2520A%2520Self-Supervised%2520Geometric%2520Deep%2520Learning%250A%2520%2520Approach%26entry.906535625%3DGiovanni%2520Luca%2520Marchetti%2520and%2520Gabriele%2520Cesa%2520and%2520Pratik%2520Kumar%2520and%2520Arash%2520Behboodi%26entry.1292438233%3D%2520%2520Lattice%2520reduction%2520is%2520a%2520combinatorial%2520optimization%2520problem%2520aimed%2520at%2520finding%250Athe%2520most%2520orthogonal%2520basis%2520in%2520a%2520given%2520lattice.%2520The%2520Lenstra-Lenstra-Lov%255C%2527asz%250A%2528LLL%2529%2520algorithm%2520is%2520the%2520best%2520algorithm%2520in%2520the%2520literature%2520for%2520solving%2520this%250Aproblem.%2520In%2520light%2520of%2520recent%2520research%2520on%2520algorithm%2520discovery%252C%2520in%2520this%2520work%252C%2520we%250Awould%2520like%2520to%2520answer%2520this%2520question%253A%2520is%2520it%2520possible%2520to%2520parametrize%2520the%2520algorithm%250Aspace%2520for%2520lattice%2520reduction%2520problem%2520with%2520neural%2520networks%2520and%2520find%2520an%2520algorithm%250Awithout%2520supervised%2520data%253F%2520Our%2520strategy%2520is%2520to%2520use%2520equivariant%2520and%2520invariant%250Aparametrizations%2520and%2520train%2520in%2520a%2520self-supervised%2520way.%2520We%2520design%2520a%2520deep%2520neural%250Amodel%2520outputting%2520factorized%2520unimodular%2520matrices%2520and%2520train%2520it%2520in%2520a%250Aself-supervised%2520manner%2520by%2520penalizing%2520non-orthogonal%2520lattice%2520bases.%2520We%250Aincorporate%2520the%2520symmetries%2520of%2520lattice%2520reduction%2520into%2520the%2520model%2520by%2520making%2520it%250Ainvariant%2520to%2520isometries%2520and%2520scaling%2520of%2520the%2520ambient%2520space%2520and%2520equivariant%2520with%250Arespect%2520to%2520the%2520hyperocrahedral%2520group%2520permuting%2520and%2520flipping%2520the%2520lattice%2520basis%250Aelements.%2520We%2520show%2520that%2520this%2520approach%2520yields%2520an%2520algorithm%2520with%2520comparable%250Acomplexity%2520and%2520performance%2520to%2520the%2520LLL%2520algorithm%2520on%2520a%2520set%2520of%2520benchmarks.%250AAdditionally%252C%2520motivated%2520by%2520certain%2520applications%2520for%2520wireless%2520communication%252C%2520we%250Aextend%2520our%2520method%2520to%2520a%2520convolutional%2520architecture%2520which%2520performs%2520joint%250Areduction%2520of%2520spatially-correlated%2520lattices%2520arranged%2520in%2520a%2520grid%252C%2520thereby%250Aamortizing%2520its%2520cost%2520over%2520multiple%2520lattices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Lattice%20Reduction%3A%20A%20Self-Supervised%20Geometric%20Deep%20Learning%0A%20%20Approach&entry.906535625=Giovanni%20Luca%20Marchetti%20and%20Gabriele%20Cesa%20and%20Pratik%20Kumar%20and%20Arash%20Behboodi&entry.1292438233=%20%20Lattice%20reduction%20is%20a%20combinatorial%20optimization%20problem%20aimed%20at%20finding%0Athe%20most%20orthogonal%20basis%20in%20a%20given%20lattice.%20The%20Lenstra-Lenstra-Lov%5C%27asz%0A%28LLL%29%20algorithm%20is%20the%20best%20algorithm%20in%20the%20literature%20for%20solving%20this%0Aproblem.%20In%20light%20of%20recent%20research%20on%20algorithm%20discovery%2C%20in%20this%20work%2C%20we%0Awould%20like%20to%20answer%20this%20question%3A%20is%20it%20possible%20to%20parametrize%20the%20algorithm%0Aspace%20for%20lattice%20reduction%20problem%20with%20neural%20networks%20and%20find%20an%20algorithm%0Awithout%20supervised%20data%3F%20Our%20strategy%20is%20to%20use%20equivariant%20and%20invariant%0Aparametrizations%20and%20train%20in%20a%20self-supervised%20way.%20We%20design%20a%20deep%20neural%0Amodel%20outputting%20factorized%20unimodular%20matrices%20and%20train%20it%20in%20a%0Aself-supervised%20manner%20by%20penalizing%20non-orthogonal%20lattice%20bases.%20We%0Aincorporate%20the%20symmetries%20of%20lattice%20reduction%20into%20the%20model%20by%20making%20it%0Ainvariant%20to%20isometries%20and%20scaling%20of%20the%20ambient%20space%20and%20equivariant%20with%0Arespect%20to%20the%20hyperocrahedral%20group%20permuting%20and%20flipping%20the%20lattice%20basis%0Aelements.%20We%20show%20that%20this%20approach%20yields%20an%20algorithm%20with%20comparable%0Acomplexity%20and%20performance%20to%20the%20LLL%20algorithm%20on%20a%20set%20of%20benchmarks.%0AAdditionally%2C%20motivated%20by%20certain%20applications%20for%20wireless%20communication%2C%20we%0Aextend%20our%20method%20to%20a%20convolutional%20architecture%20which%20performs%20joint%0Areduction%20of%20spatially-correlated%20lattices%20arranged%20in%20a%20grid%2C%20thereby%0Aamortizing%20its%20cost%20over%20multiple%20lattices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08170v2&entry.124074799=Read"},
{"title": "Optimal Visual Search with Highly Heuristic Decision Rules", "author": "Anqi Zhang and Wilson S. Geisler", "abstract": "  Visual search is a fundamental natural task for humans and other animals. We\ninvestigated the decision processes humans use in covert (single-fixation)\nsearch with briefly presented displays having well-separated potential target\nlocations. Performance was compared with the Bayesian-optimal decision process\nunder the assumption that the information from the different potential target\nlocations is statistically independent. Surprisingly, humans performed slightly\nbetter than optimal, despite humans' substantial loss of sensitivity in the\nfovea (foveal neglect), and the implausibility of the human brain replicating\nthe optimal computations. We show that three factors can quantitatively explain\nthese seemingly paradoxical results. Most importantly, simple and fixed\nheuristic decision rules reach near optimal search performance. Secondly,\nfoveal neglect primarily affects only the central potential target location.\nFinally, spatially correlated neural noise can cause search performance to\nexceed that predicted for independent noise. These findings have broad\nimplications for understanding visual search tasks and other identification\ntasks in humans and other animals.\n", "link": "http://arxiv.org/abs/2409.12124v3", "date": "2025-02-10", "relevancy": 2.6306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Visual%20Search%20with%20Highly%20Heuristic%20Decision%20Rules&body=Title%3A%20Optimal%20Visual%20Search%20with%20Highly%20Heuristic%20Decision%20Rules%0AAuthor%3A%20Anqi%20Zhang%20and%20Wilson%20S.%20Geisler%0AAbstract%3A%20%20%20Visual%20search%20is%20a%20fundamental%20natural%20task%20for%20humans%20and%20other%20animals.%20We%0Ainvestigated%20the%20decision%20processes%20humans%20use%20in%20covert%20%28single-fixation%29%0Asearch%20with%20briefly%20presented%20displays%20having%20well-separated%20potential%20target%0Alocations.%20Performance%20was%20compared%20with%20the%20Bayesian-optimal%20decision%20process%0Aunder%20the%20assumption%20that%20the%20information%20from%20the%20different%20potential%20target%0Alocations%20is%20statistically%20independent.%20Surprisingly%2C%20humans%20performed%20slightly%0Abetter%20than%20optimal%2C%20despite%20humans%27%20substantial%20loss%20of%20sensitivity%20in%20the%0Afovea%20%28foveal%20neglect%29%2C%20and%20the%20implausibility%20of%20the%20human%20brain%20replicating%0Athe%20optimal%20computations.%20We%20show%20that%20three%20factors%20can%20quantitatively%20explain%0Athese%20seemingly%20paradoxical%20results.%20Most%20importantly%2C%20simple%20and%20fixed%0Aheuristic%20decision%20rules%20reach%20near%20optimal%20search%20performance.%20Secondly%2C%0Afoveal%20neglect%20primarily%20affects%20only%20the%20central%20potential%20target%20location.%0AFinally%2C%20spatially%20correlated%20neural%20noise%20can%20cause%20search%20performance%20to%0Aexceed%20that%20predicted%20for%20independent%20noise.%20These%20findings%20have%20broad%0Aimplications%20for%20understanding%20visual%20search%20tasks%20and%20other%20identification%0Atasks%20in%20humans%20and%20other%20animals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12124v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Visual%2520Search%2520with%2520Highly%2520Heuristic%2520Decision%2520Rules%26entry.906535625%3DAnqi%2520Zhang%2520and%2520Wilson%2520S.%2520Geisler%26entry.1292438233%3D%2520%2520Visual%2520search%2520is%2520a%2520fundamental%2520natural%2520task%2520for%2520humans%2520and%2520other%2520animals.%2520We%250Ainvestigated%2520the%2520decision%2520processes%2520humans%2520use%2520in%2520covert%2520%2528single-fixation%2529%250Asearch%2520with%2520briefly%2520presented%2520displays%2520having%2520well-separated%2520potential%2520target%250Alocations.%2520Performance%2520was%2520compared%2520with%2520the%2520Bayesian-optimal%2520decision%2520process%250Aunder%2520the%2520assumption%2520that%2520the%2520information%2520from%2520the%2520different%2520potential%2520target%250Alocations%2520is%2520statistically%2520independent.%2520Surprisingly%252C%2520humans%2520performed%2520slightly%250Abetter%2520than%2520optimal%252C%2520despite%2520humans%2527%2520substantial%2520loss%2520of%2520sensitivity%2520in%2520the%250Afovea%2520%2528foveal%2520neglect%2529%252C%2520and%2520the%2520implausibility%2520of%2520the%2520human%2520brain%2520replicating%250Athe%2520optimal%2520computations.%2520We%2520show%2520that%2520three%2520factors%2520can%2520quantitatively%2520explain%250Athese%2520seemingly%2520paradoxical%2520results.%2520Most%2520importantly%252C%2520simple%2520and%2520fixed%250Aheuristic%2520decision%2520rules%2520reach%2520near%2520optimal%2520search%2520performance.%2520Secondly%252C%250Afoveal%2520neglect%2520primarily%2520affects%2520only%2520the%2520central%2520potential%2520target%2520location.%250AFinally%252C%2520spatially%2520correlated%2520neural%2520noise%2520can%2520cause%2520search%2520performance%2520to%250Aexceed%2520that%2520predicted%2520for%2520independent%2520noise.%2520These%2520findings%2520have%2520broad%250Aimplications%2520for%2520understanding%2520visual%2520search%2520tasks%2520and%2520other%2520identification%250Atasks%2520in%2520humans%2520and%2520other%2520animals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12124v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Visual%20Search%20with%20Highly%20Heuristic%20Decision%20Rules&entry.906535625=Anqi%20Zhang%20and%20Wilson%20S.%20Geisler&entry.1292438233=%20%20Visual%20search%20is%20a%20fundamental%20natural%20task%20for%20humans%20and%20other%20animals.%20We%0Ainvestigated%20the%20decision%20processes%20humans%20use%20in%20covert%20%28single-fixation%29%0Asearch%20with%20briefly%20presented%20displays%20having%20well-separated%20potential%20target%0Alocations.%20Performance%20was%20compared%20with%20the%20Bayesian-optimal%20decision%20process%0Aunder%20the%20assumption%20that%20the%20information%20from%20the%20different%20potential%20target%0Alocations%20is%20statistically%20independent.%20Surprisingly%2C%20humans%20performed%20slightly%0Abetter%20than%20optimal%2C%20despite%20humans%27%20substantial%20loss%20of%20sensitivity%20in%20the%0Afovea%20%28foveal%20neglect%29%2C%20and%20the%20implausibility%20of%20the%20human%20brain%20replicating%0Athe%20optimal%20computations.%20We%20show%20that%20three%20factors%20can%20quantitatively%20explain%0Athese%20seemingly%20paradoxical%20results.%20Most%20importantly%2C%20simple%20and%20fixed%0Aheuristic%20decision%20rules%20reach%20near%20optimal%20search%20performance.%20Secondly%2C%0Afoveal%20neglect%20primarily%20affects%20only%20the%20central%20potential%20target%20location.%0AFinally%2C%20spatially%20correlated%20neural%20noise%20can%20cause%20search%20performance%20to%0Aexceed%20that%20predicted%20for%20independent%20noise.%20These%20findings%20have%20broad%0Aimplications%20for%20understanding%20visual%20search%20tasks%20and%20other%20identification%0Atasks%20in%20humans%20and%20other%20animals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12124v3&entry.124074799=Read"},
{"title": "Do generative video models learn physical principles from watching\n  videos?", "author": "Saman Motamed and Laura Culp and Kevin Swersky and Priyank Jaini and Robert Geirhos", "abstract": "  AI video generation is undergoing a revolution, with quality and realism\nadvancing rapidly. These advances have led to a passionate scientific debate:\nDo video models learn \"world models\" that discover laws of physics -- or,\nalternatively, are they merely sophisticated pixel predictors that achieve\nvisual realism without understanding the physical principles of reality? We\naddress this question by developing Physics-IQ, a comprehensive benchmark\ndataset that can only be solved by acquiring a deep understanding of various\nphysical principles, like fluid dynamics, optics, solid mechanics, magnetism\nand thermodynamics. We find that across a range of current models (Sora,\nRunway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical\nunderstanding is severely limited, and unrelated to visual realism. At the same\ntime, some test cases can already be successfully solved. This indicates that\nacquiring certain physical principles from observation alone may be possible,\nbut significant challenges remain. While we expect rapid advances ahead, our\nwork demonstrates that visual realism does not imply physical understanding.\nOur project page is at https://physics-iq.github.io; code at\nhttps://github.com/google-deepmind/physics-IQ-benchmark.\n", "link": "http://arxiv.org/abs/2501.09038v2", "date": "2025-02-10", "relevancy": 2.6142, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7296}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6133}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20generative%20video%20models%20learn%20physical%20principles%20from%20watching%0A%20%20videos%3F&body=Title%3A%20Do%20generative%20video%20models%20learn%20physical%20principles%20from%20watching%0A%20%20videos%3F%0AAuthor%3A%20Saman%20Motamed%20and%20Laura%20Culp%20and%20Kevin%20Swersky%20and%20Priyank%20Jaini%20and%20Robert%20Geirhos%0AAbstract%3A%20%20%20AI%20video%20generation%20is%20undergoing%20a%20revolution%2C%20with%20quality%20and%20realism%0Aadvancing%20rapidly.%20These%20advances%20have%20led%20to%20a%20passionate%20scientific%20debate%3A%0ADo%20video%20models%20learn%20%22world%20models%22%20that%20discover%20laws%20of%20physics%20--%20or%2C%0Aalternatively%2C%20are%20they%20merely%20sophisticated%20pixel%20predictors%20that%20achieve%0Avisual%20realism%20without%20understanding%20the%20physical%20principles%20of%20reality%3F%20We%0Aaddress%20this%20question%20by%20developing%20Physics-IQ%2C%20a%20comprehensive%20benchmark%0Adataset%20that%20can%20only%20be%20solved%20by%20acquiring%20a%20deep%20understanding%20of%20various%0Aphysical%20principles%2C%20like%20fluid%20dynamics%2C%20optics%2C%20solid%20mechanics%2C%20magnetism%0Aand%20thermodynamics.%20We%20find%20that%20across%20a%20range%20of%20current%20models%20%28Sora%2C%0ARunway%2C%20Pika%2C%20Lumiere%2C%20Stable%20Video%20Diffusion%2C%20and%20VideoPoet%29%2C%20physical%0Aunderstanding%20is%20severely%20limited%2C%20and%20unrelated%20to%20visual%20realism.%20At%20the%20same%0Atime%2C%20some%20test%20cases%20can%20already%20be%20successfully%20solved.%20This%20indicates%20that%0Aacquiring%20certain%20physical%20principles%20from%20observation%20alone%20may%20be%20possible%2C%0Abut%20significant%20challenges%20remain.%20While%20we%20expect%20rapid%20advances%20ahead%2C%20our%0Awork%20demonstrates%20that%20visual%20realism%20does%20not%20imply%20physical%20understanding.%0AOur%20project%20page%20is%20at%20https%3A//physics-iq.github.io%3B%20code%20at%0Ahttps%3A//github.com/google-deepmind/physics-IQ-benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09038v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520generative%2520video%2520models%2520learn%2520physical%2520principles%2520from%2520watching%250A%2520%2520videos%253F%26entry.906535625%3DSaman%2520Motamed%2520and%2520Laura%2520Culp%2520and%2520Kevin%2520Swersky%2520and%2520Priyank%2520Jaini%2520and%2520Robert%2520Geirhos%26entry.1292438233%3D%2520%2520AI%2520video%2520generation%2520is%2520undergoing%2520a%2520revolution%252C%2520with%2520quality%2520and%2520realism%250Aadvancing%2520rapidly.%2520These%2520advances%2520have%2520led%2520to%2520a%2520passionate%2520scientific%2520debate%253A%250ADo%2520video%2520models%2520learn%2520%2522world%2520models%2522%2520that%2520discover%2520laws%2520of%2520physics%2520--%2520or%252C%250Aalternatively%252C%2520are%2520they%2520merely%2520sophisticated%2520pixel%2520predictors%2520that%2520achieve%250Avisual%2520realism%2520without%2520understanding%2520the%2520physical%2520principles%2520of%2520reality%253F%2520We%250Aaddress%2520this%2520question%2520by%2520developing%2520Physics-IQ%252C%2520a%2520comprehensive%2520benchmark%250Adataset%2520that%2520can%2520only%2520be%2520solved%2520by%2520acquiring%2520a%2520deep%2520understanding%2520of%2520various%250Aphysical%2520principles%252C%2520like%2520fluid%2520dynamics%252C%2520optics%252C%2520solid%2520mechanics%252C%2520magnetism%250Aand%2520thermodynamics.%2520We%2520find%2520that%2520across%2520a%2520range%2520of%2520current%2520models%2520%2528Sora%252C%250ARunway%252C%2520Pika%252C%2520Lumiere%252C%2520Stable%2520Video%2520Diffusion%252C%2520and%2520VideoPoet%2529%252C%2520physical%250Aunderstanding%2520is%2520severely%2520limited%252C%2520and%2520unrelated%2520to%2520visual%2520realism.%2520At%2520the%2520same%250Atime%252C%2520some%2520test%2520cases%2520can%2520already%2520be%2520successfully%2520solved.%2520This%2520indicates%2520that%250Aacquiring%2520certain%2520physical%2520principles%2520from%2520observation%2520alone%2520may%2520be%2520possible%252C%250Abut%2520significant%2520challenges%2520remain.%2520While%2520we%2520expect%2520rapid%2520advances%2520ahead%252C%2520our%250Awork%2520demonstrates%2520that%2520visual%2520realism%2520does%2520not%2520imply%2520physical%2520understanding.%250AOur%2520project%2520page%2520is%2520at%2520https%253A//physics-iq.github.io%253B%2520code%2520at%250Ahttps%253A//github.com/google-deepmind/physics-IQ-benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09038v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20generative%20video%20models%20learn%20physical%20principles%20from%20watching%0A%20%20videos%3F&entry.906535625=Saman%20Motamed%20and%20Laura%20Culp%20and%20Kevin%20Swersky%20and%20Priyank%20Jaini%20and%20Robert%20Geirhos&entry.1292438233=%20%20AI%20video%20generation%20is%20undergoing%20a%20revolution%2C%20with%20quality%20and%20realism%0Aadvancing%20rapidly.%20These%20advances%20have%20led%20to%20a%20passionate%20scientific%20debate%3A%0ADo%20video%20models%20learn%20%22world%20models%22%20that%20discover%20laws%20of%20physics%20--%20or%2C%0Aalternatively%2C%20are%20they%20merely%20sophisticated%20pixel%20predictors%20that%20achieve%0Avisual%20realism%20without%20understanding%20the%20physical%20principles%20of%20reality%3F%20We%0Aaddress%20this%20question%20by%20developing%20Physics-IQ%2C%20a%20comprehensive%20benchmark%0Adataset%20that%20can%20only%20be%20solved%20by%20acquiring%20a%20deep%20understanding%20of%20various%0Aphysical%20principles%2C%20like%20fluid%20dynamics%2C%20optics%2C%20solid%20mechanics%2C%20magnetism%0Aand%20thermodynamics.%20We%20find%20that%20across%20a%20range%20of%20current%20models%20%28Sora%2C%0ARunway%2C%20Pika%2C%20Lumiere%2C%20Stable%20Video%20Diffusion%2C%20and%20VideoPoet%29%2C%20physical%0Aunderstanding%20is%20severely%20limited%2C%20and%20unrelated%20to%20visual%20realism.%20At%20the%20same%0Atime%2C%20some%20test%20cases%20can%20already%20be%20successfully%20solved.%20This%20indicates%20that%0Aacquiring%20certain%20physical%20principles%20from%20observation%20alone%20may%20be%20possible%2C%0Abut%20significant%20challenges%20remain.%20While%20we%20expect%20rapid%20advances%20ahead%2C%20our%0Awork%20demonstrates%20that%20visual%20realism%20does%20not%20imply%20physical%20understanding.%0AOur%20project%20page%20is%20at%20https%3A//physics-iq.github.io%3B%20code%20at%0Ahttps%3A//github.com/google-deepmind/physics-IQ-benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09038v2&entry.124074799=Read"},
{"title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis", "author": "Huaye Zeng and Dongfu Jiang and Haozhe Wang and Ping Nie and Xiaotong Chen and Wenhu Chen", "abstract": "  Most progress in recent coder models has been driven by supervised\nfine-tuning (SFT), while the potential of reinforcement learning (RL) remains\nlargely unexplored, primarily due to the lack of reliable reward data/model in\nthe code domain. In this paper, we address this challenge by leveraging\nautomated large-scale test-case synthesis to enhance code model training.\nSpecifically, we design a pipeline that generates extensive (question,\ntest-cases) pairs from existing code data. Using these test cases, we construct\npreference pairs based on pass rates over sampled programs to train reward\nmodels with Bradley-Terry loss. It shows an average of 10-point improvement for\nLlama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through\nbest-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5.\nFurthermore, we conduct reinforcement learning with both reward models and\ntest-case pass rewards, leading to consistent improvements across HumanEval,\nMBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style\ntraining to start from Qwen2.5-Coder-base directly and show that our RL\ntraining can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\%\nfor merely 80 optimization steps. We believe our results highlight the huge\npotential of reinforcement learning in coder models.\n", "link": "http://arxiv.org/abs/2502.01718v3", "date": "2025-02-10", "relevancy": 2.6122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACECODER%3A%20Acing%20Coder%20RL%20via%20Automated%20Test-Case%20Synthesis&body=Title%3A%20ACECODER%3A%20Acing%20Coder%20RL%20via%20Automated%20Test-Case%20Synthesis%0AAuthor%3A%20Huaye%20Zeng%20and%20Dongfu%20Jiang%20and%20Haozhe%20Wang%20and%20Ping%20Nie%20and%20Xiaotong%20Chen%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Most%20progress%20in%20recent%20coder%20models%20has%20been%20driven%20by%20supervised%0Afine-tuning%20%28SFT%29%2C%20while%20the%20potential%20of%20reinforcement%20learning%20%28RL%29%20remains%0Alargely%20unexplored%2C%20primarily%20due%20to%20the%20lack%20of%20reliable%20reward%20data/model%20in%0Athe%20code%20domain.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20by%20leveraging%0Aautomated%20large-scale%20test-case%20synthesis%20to%20enhance%20code%20model%20training.%0ASpecifically%2C%20we%20design%20a%20pipeline%20that%20generates%20extensive%20%28question%2C%0Atest-cases%29%20pairs%20from%20existing%20code%20data.%20Using%20these%20test%20cases%2C%20we%20construct%0Apreference%20pairs%20based%20on%20pass%20rates%20over%20sampled%20programs%20to%20train%20reward%0Amodels%20with%20Bradley-Terry%20loss.%20It%20shows%20an%20average%20of%2010-point%20improvement%20for%0ALlama-3.1-8B-Ins%20and%205-point%20improvement%20for%20Qwen2.5-Coder-7B-Ins%20through%0Abest-of-32%20sampling%2C%20making%20the%207B%20model%20on%20par%20with%20236B%20DeepSeek-V2.5.%0AFurthermore%2C%20we%20conduct%20reinforcement%20learning%20with%20both%20reward%20models%20and%0Atest-case%20pass%20rewards%2C%20leading%20to%20consistent%20improvements%20across%20HumanEval%2C%0AMBPP%2C%20BigCodeBench%2C%20and%20LiveCodeBench%20%28V4%29.%20Notably%2C%20we%20follow%20the%20R1-style%0Atraining%20to%20start%20from%20Qwen2.5-Coder-base%20directly%20and%20show%20that%20our%20RL%0Atraining%20can%20improve%20model%20on%20HumanEval-plus%20by%20over%2025%5C%25%20and%20MBPP-plus%20by%206%5C%25%0Afor%20merely%2080%20optimization%20steps.%20We%20believe%20our%20results%20highlight%20the%20huge%0Apotential%20of%20reinforcement%20learning%20in%20coder%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01718v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACECODER%253A%2520Acing%2520Coder%2520RL%2520via%2520Automated%2520Test-Case%2520Synthesis%26entry.906535625%3DHuaye%2520Zeng%2520and%2520Dongfu%2520Jiang%2520and%2520Haozhe%2520Wang%2520and%2520Ping%2520Nie%2520and%2520Xiaotong%2520Chen%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Most%2520progress%2520in%2520recent%2520coder%2520models%2520has%2520been%2520driven%2520by%2520supervised%250Afine-tuning%2520%2528SFT%2529%252C%2520while%2520the%2520potential%2520of%2520reinforcement%2520learning%2520%2528RL%2529%2520remains%250Alargely%2520unexplored%252C%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520reliable%2520reward%2520data/model%2520in%250Athe%2520code%2520domain.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520challenge%2520by%2520leveraging%250Aautomated%2520large-scale%2520test-case%2520synthesis%2520to%2520enhance%2520code%2520model%2520training.%250ASpecifically%252C%2520we%2520design%2520a%2520pipeline%2520that%2520generates%2520extensive%2520%2528question%252C%250Atest-cases%2529%2520pairs%2520from%2520existing%2520code%2520data.%2520Using%2520these%2520test%2520cases%252C%2520we%2520construct%250Apreference%2520pairs%2520based%2520on%2520pass%2520rates%2520over%2520sampled%2520programs%2520to%2520train%2520reward%250Amodels%2520with%2520Bradley-Terry%2520loss.%2520It%2520shows%2520an%2520average%2520of%252010-point%2520improvement%2520for%250ALlama-3.1-8B-Ins%2520and%25205-point%2520improvement%2520for%2520Qwen2.5-Coder-7B-Ins%2520through%250Abest-of-32%2520sampling%252C%2520making%2520the%25207B%2520model%2520on%2520par%2520with%2520236B%2520DeepSeek-V2.5.%250AFurthermore%252C%2520we%2520conduct%2520reinforcement%2520learning%2520with%2520both%2520reward%2520models%2520and%250Atest-case%2520pass%2520rewards%252C%2520leading%2520to%2520consistent%2520improvements%2520across%2520HumanEval%252C%250AMBPP%252C%2520BigCodeBench%252C%2520and%2520LiveCodeBench%2520%2528V4%2529.%2520Notably%252C%2520we%2520follow%2520the%2520R1-style%250Atraining%2520to%2520start%2520from%2520Qwen2.5-Coder-base%2520directly%2520and%2520show%2520that%2520our%2520RL%250Atraining%2520can%2520improve%2520model%2520on%2520HumanEval-plus%2520by%2520over%252025%255C%2525%2520and%2520MBPP-plus%2520by%25206%255C%2525%250Afor%2520merely%252080%2520optimization%2520steps.%2520We%2520believe%2520our%2520results%2520highlight%2520the%2520huge%250Apotential%2520of%2520reinforcement%2520learning%2520in%2520coder%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01718v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACECODER%3A%20Acing%20Coder%20RL%20via%20Automated%20Test-Case%20Synthesis&entry.906535625=Huaye%20Zeng%20and%20Dongfu%20Jiang%20and%20Haozhe%20Wang%20and%20Ping%20Nie%20and%20Xiaotong%20Chen%20and%20Wenhu%20Chen&entry.1292438233=%20%20Most%20progress%20in%20recent%20coder%20models%20has%20been%20driven%20by%20supervised%0Afine-tuning%20%28SFT%29%2C%20while%20the%20potential%20of%20reinforcement%20learning%20%28RL%29%20remains%0Alargely%20unexplored%2C%20primarily%20due%20to%20the%20lack%20of%20reliable%20reward%20data/model%20in%0Athe%20code%20domain.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20by%20leveraging%0Aautomated%20large-scale%20test-case%20synthesis%20to%20enhance%20code%20model%20training.%0ASpecifically%2C%20we%20design%20a%20pipeline%20that%20generates%20extensive%20%28question%2C%0Atest-cases%29%20pairs%20from%20existing%20code%20data.%20Using%20these%20test%20cases%2C%20we%20construct%0Apreference%20pairs%20based%20on%20pass%20rates%20over%20sampled%20programs%20to%20train%20reward%0Amodels%20with%20Bradley-Terry%20loss.%20It%20shows%20an%20average%20of%2010-point%20improvement%20for%0ALlama-3.1-8B-Ins%20and%205-point%20improvement%20for%20Qwen2.5-Coder-7B-Ins%20through%0Abest-of-32%20sampling%2C%20making%20the%207B%20model%20on%20par%20with%20236B%20DeepSeek-V2.5.%0AFurthermore%2C%20we%20conduct%20reinforcement%20learning%20with%20both%20reward%20models%20and%0Atest-case%20pass%20rewards%2C%20leading%20to%20consistent%20improvements%20across%20HumanEval%2C%0AMBPP%2C%20BigCodeBench%2C%20and%20LiveCodeBench%20%28V4%29.%20Notably%2C%20we%20follow%20the%20R1-style%0Atraining%20to%20start%20from%20Qwen2.5-Coder-base%20directly%20and%20show%20that%20our%20RL%0Atraining%20can%20improve%20model%20on%20HumanEval-plus%20by%20over%2025%5C%25%20and%20MBPP-plus%20by%206%5C%25%0Afor%20merely%2080%20optimization%20steps.%20We%20believe%20our%20results%20highlight%20the%20huge%0Apotential%20of%20reinforcement%20learning%20in%20coder%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01718v3&entry.124074799=Read"},
{"title": "Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering", "author": "Youngsun Lim and Hojun Choi and Hyunjung Shim", "abstract": "  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing TTI models can correctly\nrespond to these questions. The I-HallA v1.0 dataset comprises 1.2K diverse\nimage-text pairs across nine categories with 1,000 rigorously curated questions\ncovering various compositional challenges. We evaluate five TTI models using\nI-HallA and reveal that these state-of-the-art models often fail to accurately\nconvey factual information. Moreover, we validate the reliability of our metric\nby demonstrating a strong Spearman correlation ($\\rho$=0.95) with human\njudgments. We believe our benchmark dataset and metric can serve as a\nfoundation for developing factually accurate TTI generation models. Additional\nresources can be found on our project page: https://sgt-lim.github.io/I-HallA/.\n", "link": "http://arxiv.org/abs/2409.12784v7", "date": "2025-02-10", "relevancy": 2.6026, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5254}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5182}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering&body=Title%3A%20Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering%0AAuthor%3A%20Youngsun%20Lim%20and%20Hojun%20Choi%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Despite%20the%20impressive%20success%20of%20text-to-image%20%28TTI%29%20generation%20models%2C%0Aexisting%20studies%20overlook%20the%20issue%20of%20whether%20these%20models%20accurately%20convey%0Afactual%20information.%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20image%0Ahallucination%2C%20where%20images%20created%20by%20generation%20models%20fail%20to%20faithfully%0Adepict%20factual%20content.%20To%20address%20this%2C%20we%20introduce%20I-HallA%20%28Image%0AHallucination%20evaluation%20with%20Question%20Answering%29%2C%20a%20novel%20automated%20evaluation%0Ametric%20that%20measures%20the%20factuality%20of%20generated%20images%20through%20visual%20question%0Aanswering%20%28VQA%29.%20We%20also%20introduce%20I-HallA%20v1.0%2C%20a%20curated%20benchmark%20dataset%0Afor%20this%20purpose.%20As%20part%20of%20this%20process%2C%20we%20develop%20a%20pipeline%20that%20generates%0Ahigh-quality%20question-answer%20pairs%20using%20multiple%20GPT-4%20Omni-based%20agents%2C%20with%0Ahuman%20judgments%20to%20ensure%20accuracy.%20Our%20evaluation%20protocols%20measure%20image%0Ahallucination%20by%20testing%20if%20images%20from%20existing%20TTI%20models%20can%20correctly%0Arespond%20to%20these%20questions.%20The%20I-HallA%20v1.0%20dataset%20comprises%201.2K%20diverse%0Aimage-text%20pairs%20across%20nine%20categories%20with%201%2C000%20rigorously%20curated%20questions%0Acovering%20various%20compositional%20challenges.%20We%20evaluate%20five%20TTI%20models%20using%0AI-HallA%20and%20reveal%20that%20these%20state-of-the-art%20models%20often%20fail%20to%20accurately%0Aconvey%20factual%20information.%20Moreover%2C%20we%20validate%20the%20reliability%20of%20our%20metric%0Aby%20demonstrating%20a%20strong%20Spearman%20correlation%20%28%24%5Crho%24%3D0.95%29%20with%20human%0Ajudgments.%20We%20believe%20our%20benchmark%20dataset%20and%20metric%20can%20serve%20as%20a%0Afoundation%20for%20developing%20factually%20accurate%20TTI%20generation%20models.%20Additional%0Aresources%20can%20be%20found%20on%20our%20project%20page%3A%20https%3A//sgt-lim.github.io/I-HallA/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12784v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Image%2520Hallucination%2520in%2520Text-to-Image%2520Generation%2520with%250A%2520%2520Question-Answering%26entry.906535625%3DYoungsun%2520Lim%2520and%2520Hojun%2520Choi%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520success%2520of%2520text-to-image%2520%2528TTI%2529%2520generation%2520models%252C%250Aexisting%2520studies%2520overlook%2520the%2520issue%2520of%2520whether%2520these%2520models%2520accurately%2520convey%250Afactual%2520information.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520problem%2520of%2520image%250Ahallucination%252C%2520where%2520images%2520created%2520by%2520generation%2520models%2520fail%2520to%2520faithfully%250Adepict%2520factual%2520content.%2520To%2520address%2520this%252C%2520we%2520introduce%2520I-HallA%2520%2528Image%250AHallucination%2520evaluation%2520with%2520Question%2520Answering%2529%252C%2520a%2520novel%2520automated%2520evaluation%250Ametric%2520that%2520measures%2520the%2520factuality%2520of%2520generated%2520images%2520through%2520visual%2520question%250Aanswering%2520%2528VQA%2529.%2520We%2520also%2520introduce%2520I-HallA%2520v1.0%252C%2520a%2520curated%2520benchmark%2520dataset%250Afor%2520this%2520purpose.%2520As%2520part%2520of%2520this%2520process%252C%2520we%2520develop%2520a%2520pipeline%2520that%2520generates%250Ahigh-quality%2520question-answer%2520pairs%2520using%2520multiple%2520GPT-4%2520Omni-based%2520agents%252C%2520with%250Ahuman%2520judgments%2520to%2520ensure%2520accuracy.%2520Our%2520evaluation%2520protocols%2520measure%2520image%250Ahallucination%2520by%2520testing%2520if%2520images%2520from%2520existing%2520TTI%2520models%2520can%2520correctly%250Arespond%2520to%2520these%2520questions.%2520The%2520I-HallA%2520v1.0%2520dataset%2520comprises%25201.2K%2520diverse%250Aimage-text%2520pairs%2520across%2520nine%2520categories%2520with%25201%252C000%2520rigorously%2520curated%2520questions%250Acovering%2520various%2520compositional%2520challenges.%2520We%2520evaluate%2520five%2520TTI%2520models%2520using%250AI-HallA%2520and%2520reveal%2520that%2520these%2520state-of-the-art%2520models%2520often%2520fail%2520to%2520accurately%250Aconvey%2520factual%2520information.%2520Moreover%252C%2520we%2520validate%2520the%2520reliability%2520of%2520our%2520metric%250Aby%2520demonstrating%2520a%2520strong%2520Spearman%2520correlation%2520%2528%2524%255Crho%2524%253D0.95%2529%2520with%2520human%250Ajudgments.%2520We%2520believe%2520our%2520benchmark%2520dataset%2520and%2520metric%2520can%2520serve%2520as%2520a%250Afoundation%2520for%2520developing%2520factually%2520accurate%2520TTI%2520generation%2520models.%2520Additional%250Aresources%2520can%2520be%2520found%2520on%2520our%2520project%2520page%253A%2520https%253A//sgt-lim.github.io/I-HallA/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12784v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering&entry.906535625=Youngsun%20Lim%20and%20Hojun%20Choi%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Despite%20the%20impressive%20success%20of%20text-to-image%20%28TTI%29%20generation%20models%2C%0Aexisting%20studies%20overlook%20the%20issue%20of%20whether%20these%20models%20accurately%20convey%0Afactual%20information.%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20image%0Ahallucination%2C%20where%20images%20created%20by%20generation%20models%20fail%20to%20faithfully%0Adepict%20factual%20content.%20To%20address%20this%2C%20we%20introduce%20I-HallA%20%28Image%0AHallucination%20evaluation%20with%20Question%20Answering%29%2C%20a%20novel%20automated%20evaluation%0Ametric%20that%20measures%20the%20factuality%20of%20generated%20images%20through%20visual%20question%0Aanswering%20%28VQA%29.%20We%20also%20introduce%20I-HallA%20v1.0%2C%20a%20curated%20benchmark%20dataset%0Afor%20this%20purpose.%20As%20part%20of%20this%20process%2C%20we%20develop%20a%20pipeline%20that%20generates%0Ahigh-quality%20question-answer%20pairs%20using%20multiple%20GPT-4%20Omni-based%20agents%2C%20with%0Ahuman%20judgments%20to%20ensure%20accuracy.%20Our%20evaluation%20protocols%20measure%20image%0Ahallucination%20by%20testing%20if%20images%20from%20existing%20TTI%20models%20can%20correctly%0Arespond%20to%20these%20questions.%20The%20I-HallA%20v1.0%20dataset%20comprises%201.2K%20diverse%0Aimage-text%20pairs%20across%20nine%20categories%20with%201%2C000%20rigorously%20curated%20questions%0Acovering%20various%20compositional%20challenges.%20We%20evaluate%20five%20TTI%20models%20using%0AI-HallA%20and%20reveal%20that%20these%20state-of-the-art%20models%20often%20fail%20to%20accurately%0Aconvey%20factual%20information.%20Moreover%2C%20we%20validate%20the%20reliability%20of%20our%20metric%0Aby%20demonstrating%20a%20strong%20Spearman%20correlation%20%28%24%5Crho%24%3D0.95%29%20with%20human%0Ajudgments.%20We%20believe%20our%20benchmark%20dataset%20and%20metric%20can%20serve%20as%20a%0Afoundation%20for%20developing%20factually%20accurate%20TTI%20generation%20models.%20Additional%0Aresources%20can%20be%20found%20on%20our%20project%20page%3A%20https%3A//sgt-lim.github.io/I-HallA/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12784v7&entry.124074799=Read"},
{"title": "ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural\n  Projection", "author": "Giacomo Lastrucci and Artur M. Schweidtmann", "abstract": "  Ensuring neural networks adhere to domain-specific constraints is crucial for\naddressing safety and ethical concerns while also enhancing prediction\naccuracy. Despite the nonlinear nature of most real-world tasks, existing\nmethods are predominantly limited to affine or convex constraints. We introduce\nENFORCE, a neural network architecture that guarantees predictions to satisfy\nnonlinear constraints exactly. ENFORCE is trained with standard unconstrained\ngradient-based optimizers (e.g., Adam) and leverages autodifferentiation and\nlocal neural projections to enforce any $\\mathcal{C}^1$ constraint to arbitrary\ntolerance $\\epsilon$. We build an adaptive-depth neural projection (AdaNP)\nmodule that dynamically adjusts its complexity to suit the specific problem and\nthe required tolerance levels. ENFORCE guarantees satisfaction of equality\nconstraints that are nonlinear in both inputs and outputs of the neural network\nwith minimal (and adjustable) computational cost.\n", "link": "http://arxiv.org/abs/2502.06774v1", "date": "2025-02-10", "relevancy": 2.5883, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4987}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENFORCE%3A%20Exact%20Nonlinear%20Constrained%20Learning%20with%20Adaptive-depth%20Neural%0A%20%20Projection&body=Title%3A%20ENFORCE%3A%20Exact%20Nonlinear%20Constrained%20Learning%20with%20Adaptive-depth%20Neural%0A%20%20Projection%0AAuthor%3A%20Giacomo%20Lastrucci%20and%20Artur%20M.%20Schweidtmann%0AAbstract%3A%20%20%20Ensuring%20neural%20networks%20adhere%20to%20domain-specific%20constraints%20is%20crucial%20for%0Aaddressing%20safety%20and%20ethical%20concerns%20while%20also%20enhancing%20prediction%0Aaccuracy.%20Despite%20the%20nonlinear%20nature%20of%20most%20real-world%20tasks%2C%20existing%0Amethods%20are%20predominantly%20limited%20to%20affine%20or%20convex%20constraints.%20We%20introduce%0AENFORCE%2C%20a%20neural%20network%20architecture%20that%20guarantees%20predictions%20to%20satisfy%0Anonlinear%20constraints%20exactly.%20ENFORCE%20is%20trained%20with%20standard%20unconstrained%0Agradient-based%20optimizers%20%28e.g.%2C%20Adam%29%20and%20leverages%20autodifferentiation%20and%0Alocal%20neural%20projections%20to%20enforce%20any%20%24%5Cmathcal%7BC%7D%5E1%24%20constraint%20to%20arbitrary%0Atolerance%20%24%5Cepsilon%24.%20We%20build%20an%20adaptive-depth%20neural%20projection%20%28AdaNP%29%0Amodule%20that%20dynamically%20adjusts%20its%20complexity%20to%20suit%20the%20specific%20problem%20and%0Athe%20required%20tolerance%20levels.%20ENFORCE%20guarantees%20satisfaction%20of%20equality%0Aconstraints%20that%20are%20nonlinear%20in%20both%20inputs%20and%20outputs%20of%20the%20neural%20network%0Awith%20minimal%20%28and%20adjustable%29%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENFORCE%253A%2520Exact%2520Nonlinear%2520Constrained%2520Learning%2520with%2520Adaptive-depth%2520Neural%250A%2520%2520Projection%26entry.906535625%3DGiacomo%2520Lastrucci%2520and%2520Artur%2520M.%2520Schweidtmann%26entry.1292438233%3D%2520%2520Ensuring%2520neural%2520networks%2520adhere%2520to%2520domain-specific%2520constraints%2520is%2520crucial%2520for%250Aaddressing%2520safety%2520and%2520ethical%2520concerns%2520while%2520also%2520enhancing%2520prediction%250Aaccuracy.%2520Despite%2520the%2520nonlinear%2520nature%2520of%2520most%2520real-world%2520tasks%252C%2520existing%250Amethods%2520are%2520predominantly%2520limited%2520to%2520affine%2520or%2520convex%2520constraints.%2520We%2520introduce%250AENFORCE%252C%2520a%2520neural%2520network%2520architecture%2520that%2520guarantees%2520predictions%2520to%2520satisfy%250Anonlinear%2520constraints%2520exactly.%2520ENFORCE%2520is%2520trained%2520with%2520standard%2520unconstrained%250Agradient-based%2520optimizers%2520%2528e.g.%252C%2520Adam%2529%2520and%2520leverages%2520autodifferentiation%2520and%250Alocal%2520neural%2520projections%2520to%2520enforce%2520any%2520%2524%255Cmathcal%257BC%257D%255E1%2524%2520constraint%2520to%2520arbitrary%250Atolerance%2520%2524%255Cepsilon%2524.%2520We%2520build%2520an%2520adaptive-depth%2520neural%2520projection%2520%2528AdaNP%2529%250Amodule%2520that%2520dynamically%2520adjusts%2520its%2520complexity%2520to%2520suit%2520the%2520specific%2520problem%2520and%250Athe%2520required%2520tolerance%2520levels.%2520ENFORCE%2520guarantees%2520satisfaction%2520of%2520equality%250Aconstraints%2520that%2520are%2520nonlinear%2520in%2520both%2520inputs%2520and%2520outputs%2520of%2520the%2520neural%2520network%250Awith%2520minimal%2520%2528and%2520adjustable%2529%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENFORCE%3A%20Exact%20Nonlinear%20Constrained%20Learning%20with%20Adaptive-depth%20Neural%0A%20%20Projection&entry.906535625=Giacomo%20Lastrucci%20and%20Artur%20M.%20Schweidtmann&entry.1292438233=%20%20Ensuring%20neural%20networks%20adhere%20to%20domain-specific%20constraints%20is%20crucial%20for%0Aaddressing%20safety%20and%20ethical%20concerns%20while%20also%20enhancing%20prediction%0Aaccuracy.%20Despite%20the%20nonlinear%20nature%20of%20most%20real-world%20tasks%2C%20existing%0Amethods%20are%20predominantly%20limited%20to%20affine%20or%20convex%20constraints.%20We%20introduce%0AENFORCE%2C%20a%20neural%20network%20architecture%20that%20guarantees%20predictions%20to%20satisfy%0Anonlinear%20constraints%20exactly.%20ENFORCE%20is%20trained%20with%20standard%20unconstrained%0Agradient-based%20optimizers%20%28e.g.%2C%20Adam%29%20and%20leverages%20autodifferentiation%20and%0Alocal%20neural%20projections%20to%20enforce%20any%20%24%5Cmathcal%7BC%7D%5E1%24%20constraint%20to%20arbitrary%0Atolerance%20%24%5Cepsilon%24.%20We%20build%20an%20adaptive-depth%20neural%20projection%20%28AdaNP%29%0Amodule%20that%20dynamically%20adjusts%20its%20complexity%20to%20suit%20the%20specific%20problem%20and%0Athe%20required%20tolerance%20levels.%20ENFORCE%20guarantees%20satisfaction%20of%20equality%0Aconstraints%20that%20are%20nonlinear%20in%20both%20inputs%20and%20outputs%20of%20the%20neural%20network%0Awith%20minimal%20%28and%20adjustable%29%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06774v1&entry.124074799=Read"},
{"title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT", "author": "Dongyang Liu and Shicheng Li and Yutong Liu and Zhen Li and Kai Wang and Xinyue Li and Qi Qin and Yufei Liu and Yi Xin and Zhongyu Li and Bin Fu and Chenyang Si and Yuewen Cao and Conghui He and Ziwei Liu and Yu Qiao and Qibin Hou and Hongsheng Li and Peng Gao", "abstract": "  Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.\n", "link": "http://arxiv.org/abs/2502.06782v1", "date": "2025-02-10", "relevancy": 2.5855, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6552}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6434}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumina-Video%3A%20Efficient%20and%20Flexible%20Video%20Generation%20with%20Multi-scale%0A%20%20Next-DiT&body=Title%3A%20Lumina-Video%3A%20Efficient%20and%20Flexible%20Video%20Generation%20with%20Multi-scale%0A%20%20Next-DiT%0AAuthor%3A%20Dongyang%20Liu%20and%20Shicheng%20Li%20and%20Yutong%20Liu%20and%20Zhen%20Li%20and%20Kai%20Wang%20and%20Xinyue%20Li%20and%20Qi%20Qin%20and%20Yufei%20Liu%20and%20Yi%20Xin%20and%20Zhongyu%20Li%20and%20Bin%20Fu%20and%20Chenyang%20Si%20and%20Yuewen%20Cao%20and%20Conghui%20He%20and%20Ziwei%20Liu%20and%20Yu%20Qiao%20and%20Qibin%20Hou%20and%20Hongsheng%20Li%20and%20Peng%20Gao%0AAbstract%3A%20%20%20Recent%20advancements%20have%20established%20Diffusion%20Transformers%20%28DiTs%29%20as%20a%0Adominant%20framework%20in%20generative%20modeling.%20Building%20on%20this%20success%2C%0ALumina-Next%20achieves%20exceptional%20performance%20in%20the%20generation%20of%0Aphotorealistic%20images%20with%20Next-DiT.%20However%2C%20its%20potential%20for%20video%0Ageneration%20remains%20largely%20untapped%2C%20with%20significant%20challenges%20in%20modeling%0Athe%20spatiotemporal%20complexity%20inherent%20to%20video%20data.%20To%20address%20this%2C%20we%0Aintroduce%20Lumina-Video%2C%20a%20framework%20that%20leverages%20the%20strengths%20of%20Next-DiT%0Awhile%20introducing%20tailored%20solutions%20for%20video%20synthesis.%20Lumina-Video%0Aincorporates%20a%20Multi-scale%20Next-DiT%20architecture%2C%20which%20jointly%20learns%20multiple%0Apatchifications%20to%20enhance%20both%20efficiency%20and%20flexibility.%20By%20incorporating%0Athe%20motion%20score%20as%20an%20explicit%20condition%2C%20Lumina-Video%20also%20enables%20direct%0Acontrol%20of%20generated%20videos%27%20dynamic%20degree.%20Combined%20with%20a%20progressive%0Atraining%20scheme%20with%20increasingly%20higher%20resolution%20and%20FPS%2C%20and%20a%20multi-source%0Atraining%20scheme%20with%20mixed%20natural%20and%20synthetic%20data%2C%20Lumina-Video%20achieves%0Aremarkable%20aesthetic%20quality%20and%20motion%20smoothness%20at%20high%20training%20and%0Ainference%20efficiency.%20We%20additionally%20propose%20Lumina-V2A%2C%20a%20video-to-audio%0Amodel%20based%20on%20Next-DiT%2C%20to%20create%20synchronized%20sounds%20for%20generated%20videos.%0ACodes%20are%20released%20at%20https%3A//www.github.com/Alpha-VLLM/Lumina-Video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumina-Video%253A%2520Efficient%2520and%2520Flexible%2520Video%2520Generation%2520with%2520Multi-scale%250A%2520%2520Next-DiT%26entry.906535625%3DDongyang%2520Liu%2520and%2520Shicheng%2520Li%2520and%2520Yutong%2520Liu%2520and%2520Zhen%2520Li%2520and%2520Kai%2520Wang%2520and%2520Xinyue%2520Li%2520and%2520Qi%2520Qin%2520and%2520Yufei%2520Liu%2520and%2520Yi%2520Xin%2520and%2520Zhongyu%2520Li%2520and%2520Bin%2520Fu%2520and%2520Chenyang%2520Si%2520and%2520Yuewen%2520Cao%2520and%2520Conghui%2520He%2520and%2520Ziwei%2520Liu%2520and%2520Yu%2520Qiao%2520and%2520Qibin%2520Hou%2520and%2520Hongsheng%2520Li%2520and%2520Peng%2520Gao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520established%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520as%2520a%250Adominant%2520framework%2520in%2520generative%2520modeling.%2520Building%2520on%2520this%2520success%252C%250ALumina-Next%2520achieves%2520exceptional%2520performance%2520in%2520the%2520generation%2520of%250Aphotorealistic%2520images%2520with%2520Next-DiT.%2520However%252C%2520its%2520potential%2520for%2520video%250Ageneration%2520remains%2520largely%2520untapped%252C%2520with%2520significant%2520challenges%2520in%2520modeling%250Athe%2520spatiotemporal%2520complexity%2520inherent%2520to%2520video%2520data.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520Lumina-Video%252C%2520a%2520framework%2520that%2520leverages%2520the%2520strengths%2520of%2520Next-DiT%250Awhile%2520introducing%2520tailored%2520solutions%2520for%2520video%2520synthesis.%2520Lumina-Video%250Aincorporates%2520a%2520Multi-scale%2520Next-DiT%2520architecture%252C%2520which%2520jointly%2520learns%2520multiple%250Apatchifications%2520to%2520enhance%2520both%2520efficiency%2520and%2520flexibility.%2520By%2520incorporating%250Athe%2520motion%2520score%2520as%2520an%2520explicit%2520condition%252C%2520Lumina-Video%2520also%2520enables%2520direct%250Acontrol%2520of%2520generated%2520videos%2527%2520dynamic%2520degree.%2520Combined%2520with%2520a%2520progressive%250Atraining%2520scheme%2520with%2520increasingly%2520higher%2520resolution%2520and%2520FPS%252C%2520and%2520a%2520multi-source%250Atraining%2520scheme%2520with%2520mixed%2520natural%2520and%2520synthetic%2520data%252C%2520Lumina-Video%2520achieves%250Aremarkable%2520aesthetic%2520quality%2520and%2520motion%2520smoothness%2520at%2520high%2520training%2520and%250Ainference%2520efficiency.%2520We%2520additionally%2520propose%2520Lumina-V2A%252C%2520a%2520video-to-audio%250Amodel%2520based%2520on%2520Next-DiT%252C%2520to%2520create%2520synchronized%2520sounds%2520for%2520generated%2520videos.%250ACodes%2520are%2520released%2520at%2520https%253A//www.github.com/Alpha-VLLM/Lumina-Video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumina-Video%3A%20Efficient%20and%20Flexible%20Video%20Generation%20with%20Multi-scale%0A%20%20Next-DiT&entry.906535625=Dongyang%20Liu%20and%20Shicheng%20Li%20and%20Yutong%20Liu%20and%20Zhen%20Li%20and%20Kai%20Wang%20and%20Xinyue%20Li%20and%20Qi%20Qin%20and%20Yufei%20Liu%20and%20Yi%20Xin%20and%20Zhongyu%20Li%20and%20Bin%20Fu%20and%20Chenyang%20Si%20and%20Yuewen%20Cao%20and%20Conghui%20He%20and%20Ziwei%20Liu%20and%20Yu%20Qiao%20and%20Qibin%20Hou%20and%20Hongsheng%20Li%20and%20Peng%20Gao&entry.1292438233=%20%20Recent%20advancements%20have%20established%20Diffusion%20Transformers%20%28DiTs%29%20as%20a%0Adominant%20framework%20in%20generative%20modeling.%20Building%20on%20this%20success%2C%0ALumina-Next%20achieves%20exceptional%20performance%20in%20the%20generation%20of%0Aphotorealistic%20images%20with%20Next-DiT.%20However%2C%20its%20potential%20for%20video%0Ageneration%20remains%20largely%20untapped%2C%20with%20significant%20challenges%20in%20modeling%0Athe%20spatiotemporal%20complexity%20inherent%20to%20video%20data.%20To%20address%20this%2C%20we%0Aintroduce%20Lumina-Video%2C%20a%20framework%20that%20leverages%20the%20strengths%20of%20Next-DiT%0Awhile%20introducing%20tailored%20solutions%20for%20video%20synthesis.%20Lumina-Video%0Aincorporates%20a%20Multi-scale%20Next-DiT%20architecture%2C%20which%20jointly%20learns%20multiple%0Apatchifications%20to%20enhance%20both%20efficiency%20and%20flexibility.%20By%20incorporating%0Athe%20motion%20score%20as%20an%20explicit%20condition%2C%20Lumina-Video%20also%20enables%20direct%0Acontrol%20of%20generated%20videos%27%20dynamic%20degree.%20Combined%20with%20a%20progressive%0Atraining%20scheme%20with%20increasingly%20higher%20resolution%20and%20FPS%2C%20and%20a%20multi-source%0Atraining%20scheme%20with%20mixed%20natural%20and%20synthetic%20data%2C%20Lumina-Video%20achieves%0Aremarkable%20aesthetic%20quality%20and%20motion%20smoothness%20at%20high%20training%20and%0Ainference%20efficiency.%20We%20additionally%20propose%20Lumina-V2A%2C%20a%20video-to-audio%0Amodel%20based%20on%20Next-DiT%2C%20to%20create%20synchronized%20sounds%20for%20generated%20videos.%0ACodes%20are%20released%20at%20https%3A//www.github.com/Alpha-VLLM/Lumina-Video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06782v1&entry.124074799=Read"},
{"title": "Locally Convex Global Loss Network for Decision-Focused Learning", "author": "Haeun Jeon and Hyunglip Bae and Minsu Park and Chanyeong Kim and Woo Chang Kim", "abstract": "  In decision-making problems under uncertainty, predicting unknown parameters\nis often considered independent of the optimization part. Decision-focused\nlearning (DFL) is a task-oriented framework that integrates prediction and\noptimization by adapting the predictive model to give better decisions for the\ncorresponding task. Here, an inevitable challenge arises when computing the\ngradients of the optimal decision with respect to the parameters. Existing\nresearch copes with this issue by smoothly reforming surrogate optimization or\nconstructing surrogate loss functions that mimic task loss. However, they are\napplied to restricted optimization domains. In this paper, we propose Locally\nConvex Global Loss Network (LCGLN), a global surrogate loss model that can be\nimplemented in a general DFL paradigm. LCGLN learns task loss via a partial\ninput convex neural network which is guaranteed to be convex for chosen inputs\nwhile keeping the non-convex global structure for the other inputs. This\nenables LCGLN to admit general DFL through only a single surrogate loss without\nany sense for choosing appropriate parametric forms. We confirm the\neffectiveness and flexibility of LCGLN by evaluating our proposed model with\nthree stochastic decision-making problems.\n", "link": "http://arxiv.org/abs/2403.01875v4", "date": "2025-02-10", "relevancy": 2.5842, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5184}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5175}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Convex%20Global%20Loss%20Network%20for%20Decision-Focused%20Learning&body=Title%3A%20Locally%20Convex%20Global%20Loss%20Network%20for%20Decision-Focused%20Learning%0AAuthor%3A%20Haeun%20Jeon%20and%20Hyunglip%20Bae%20and%20Minsu%20Park%20and%20Chanyeong%20Kim%20and%20Woo%20Chang%20Kim%0AAbstract%3A%20%20%20In%20decision-making%20problems%20under%20uncertainty%2C%20predicting%20unknown%20parameters%0Ais%20often%20considered%20independent%20of%20the%20optimization%20part.%20Decision-focused%0Alearning%20%28DFL%29%20is%20a%20task-oriented%20framework%20that%20integrates%20prediction%20and%0Aoptimization%20by%20adapting%20the%20predictive%20model%20to%20give%20better%20decisions%20for%20the%0Acorresponding%20task.%20Here%2C%20an%20inevitable%20challenge%20arises%20when%20computing%20the%0Agradients%20of%20the%20optimal%20decision%20with%20respect%20to%20the%20parameters.%20Existing%0Aresearch%20copes%20with%20this%20issue%20by%20smoothly%20reforming%20surrogate%20optimization%20or%0Aconstructing%20surrogate%20loss%20functions%20that%20mimic%20task%20loss.%20However%2C%20they%20are%0Aapplied%20to%20restricted%20optimization%20domains.%20In%20this%20paper%2C%20we%20propose%20Locally%0AConvex%20Global%20Loss%20Network%20%28LCGLN%29%2C%20a%20global%20surrogate%20loss%20model%20that%20can%20be%0Aimplemented%20in%20a%20general%20DFL%20paradigm.%20LCGLN%20learns%20task%20loss%20via%20a%20partial%0Ainput%20convex%20neural%20network%20which%20is%20guaranteed%20to%20be%20convex%20for%20chosen%20inputs%0Awhile%20keeping%20the%20non-convex%20global%20structure%20for%20the%20other%20inputs.%20This%0Aenables%20LCGLN%20to%20admit%20general%20DFL%20through%20only%20a%20single%20surrogate%20loss%20without%0Aany%20sense%20for%20choosing%20appropriate%20parametric%20forms.%20We%20confirm%20the%0Aeffectiveness%20and%20flexibility%20of%20LCGLN%20by%20evaluating%20our%20proposed%20model%20with%0Athree%20stochastic%20decision-making%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01875v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Convex%2520Global%2520Loss%2520Network%2520for%2520Decision-Focused%2520Learning%26entry.906535625%3DHaeun%2520Jeon%2520and%2520Hyunglip%2520Bae%2520and%2520Minsu%2520Park%2520and%2520Chanyeong%2520Kim%2520and%2520Woo%2520Chang%2520Kim%26entry.1292438233%3D%2520%2520In%2520decision-making%2520problems%2520under%2520uncertainty%252C%2520predicting%2520unknown%2520parameters%250Ais%2520often%2520considered%2520independent%2520of%2520the%2520optimization%2520part.%2520Decision-focused%250Alearning%2520%2528DFL%2529%2520is%2520a%2520task-oriented%2520framework%2520that%2520integrates%2520prediction%2520and%250Aoptimization%2520by%2520adapting%2520the%2520predictive%2520model%2520to%2520give%2520better%2520decisions%2520for%2520the%250Acorresponding%2520task.%2520Here%252C%2520an%2520inevitable%2520challenge%2520arises%2520when%2520computing%2520the%250Agradients%2520of%2520the%2520optimal%2520decision%2520with%2520respect%2520to%2520the%2520parameters.%2520Existing%250Aresearch%2520copes%2520with%2520this%2520issue%2520by%2520smoothly%2520reforming%2520surrogate%2520optimization%2520or%250Aconstructing%2520surrogate%2520loss%2520functions%2520that%2520mimic%2520task%2520loss.%2520However%252C%2520they%2520are%250Aapplied%2520to%2520restricted%2520optimization%2520domains.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Locally%250AConvex%2520Global%2520Loss%2520Network%2520%2528LCGLN%2529%252C%2520a%2520global%2520surrogate%2520loss%2520model%2520that%2520can%2520be%250Aimplemented%2520in%2520a%2520general%2520DFL%2520paradigm.%2520LCGLN%2520learns%2520task%2520loss%2520via%2520a%2520partial%250Ainput%2520convex%2520neural%2520network%2520which%2520is%2520guaranteed%2520to%2520be%2520convex%2520for%2520chosen%2520inputs%250Awhile%2520keeping%2520the%2520non-convex%2520global%2520structure%2520for%2520the%2520other%2520inputs.%2520This%250Aenables%2520LCGLN%2520to%2520admit%2520general%2520DFL%2520through%2520only%2520a%2520single%2520surrogate%2520loss%2520without%250Aany%2520sense%2520for%2520choosing%2520appropriate%2520parametric%2520forms.%2520We%2520confirm%2520the%250Aeffectiveness%2520and%2520flexibility%2520of%2520LCGLN%2520by%2520evaluating%2520our%2520proposed%2520model%2520with%250Athree%2520stochastic%2520decision-making%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01875v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Convex%20Global%20Loss%20Network%20for%20Decision-Focused%20Learning&entry.906535625=Haeun%20Jeon%20and%20Hyunglip%20Bae%20and%20Minsu%20Park%20and%20Chanyeong%20Kim%20and%20Woo%20Chang%20Kim&entry.1292438233=%20%20In%20decision-making%20problems%20under%20uncertainty%2C%20predicting%20unknown%20parameters%0Ais%20often%20considered%20independent%20of%20the%20optimization%20part.%20Decision-focused%0Alearning%20%28DFL%29%20is%20a%20task-oriented%20framework%20that%20integrates%20prediction%20and%0Aoptimization%20by%20adapting%20the%20predictive%20model%20to%20give%20better%20decisions%20for%20the%0Acorresponding%20task.%20Here%2C%20an%20inevitable%20challenge%20arises%20when%20computing%20the%0Agradients%20of%20the%20optimal%20decision%20with%20respect%20to%20the%20parameters.%20Existing%0Aresearch%20copes%20with%20this%20issue%20by%20smoothly%20reforming%20surrogate%20optimization%20or%0Aconstructing%20surrogate%20loss%20functions%20that%20mimic%20task%20loss.%20However%2C%20they%20are%0Aapplied%20to%20restricted%20optimization%20domains.%20In%20this%20paper%2C%20we%20propose%20Locally%0AConvex%20Global%20Loss%20Network%20%28LCGLN%29%2C%20a%20global%20surrogate%20loss%20model%20that%20can%20be%0Aimplemented%20in%20a%20general%20DFL%20paradigm.%20LCGLN%20learns%20task%20loss%20via%20a%20partial%0Ainput%20convex%20neural%20network%20which%20is%20guaranteed%20to%20be%20convex%20for%20chosen%20inputs%0Awhile%20keeping%20the%20non-convex%20global%20structure%20for%20the%20other%20inputs.%20This%0Aenables%20LCGLN%20to%20admit%20general%20DFL%20through%20only%20a%20single%20surrogate%20loss%20without%0Aany%20sense%20for%20choosing%20appropriate%20parametric%20forms.%20We%20confirm%20the%0Aeffectiveness%20and%20flexibility%20of%20LCGLN%20by%20evaluating%20our%20proposed%20model%20with%0Athree%20stochastic%20decision-making%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01875v4&entry.124074799=Read"},
{"title": "Few-Shot Classification and Anatomical Localization of Tissues in SPECT\n  Imaging", "author": "Mohammed Abdul Hafeez Khan and Samuel Morries Boddepalli and Siddhartha Bhattacharyya and Debasis Mitra", "abstract": "  Accurate classification and anatomical localization are essential for\neffective medical diagnostics and research, which may be efficiently performed\nusing deep learning techniques. However, availability of limited labeled data\nposes a significant challenge. To address this, we adapted Prototypical\nNetworks and the Propagation-Reconstruction Network (PRNet) for few-shot\nclassification and localization, respectively, in Single Photon Emission\nComputed Tomography (SPECT) images. For the proof of concept we used a\n2D-sliced image cropped around heart. The Prototypical Network, with a\npre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver\ntissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for\n2D imaging with an encoder-decoder architecture and skip connections, achieved\na training loss of 1.395, accurately reconstructing patches and capturing\nspatial relationships. These results highlight the potential of Prototypical\nNetworks for tissue classification with limited labeled data and PRNet for\nanatomical landmark localization, paving the way for improved performance in\ndeep learning frameworks.\n", "link": "http://arxiv.org/abs/2502.06632v1", "date": "2025-02-10", "relevancy": 2.5759, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.52}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Classification%20and%20Anatomical%20Localization%20of%20Tissues%20in%20SPECT%0A%20%20Imaging&body=Title%3A%20Few-Shot%20Classification%20and%20Anatomical%20Localization%20of%20Tissues%20in%20SPECT%0A%20%20Imaging%0AAuthor%3A%20Mohammed%20Abdul%20Hafeez%20Khan%20and%20Samuel%20Morries%20Boddepalli%20and%20Siddhartha%20Bhattacharyya%20and%20Debasis%20Mitra%0AAbstract%3A%20%20%20Accurate%20classification%20and%20anatomical%20localization%20are%20essential%20for%0Aeffective%20medical%20diagnostics%20and%20research%2C%20which%20may%20be%20efficiently%20performed%0Ausing%20deep%20learning%20techniques.%20However%2C%20availability%20of%20limited%20labeled%20data%0Aposes%20a%20significant%20challenge.%20To%20address%20this%2C%20we%20adapted%20Prototypical%0ANetworks%20and%20the%20Propagation-Reconstruction%20Network%20%28PRNet%29%20for%20few-shot%0Aclassification%20and%20localization%2C%20respectively%2C%20in%20Single%20Photon%20Emission%0AComputed%20Tomography%20%28SPECT%29%20images.%20For%20the%20proof%20of%20concept%20we%20used%20a%0A2D-sliced%20image%20cropped%20around%20heart.%20The%20Prototypical%20Network%2C%20with%20a%0Apre-trained%20ResNet-18%20backbone%2C%20classified%20ventricles%2C%20myocardium%2C%20and%20liver%0Atissues%20with%2096.67%25%20training%20and%2093.33%25%20validation%20accuracy.%20PRNet%2C%20adapted%20for%0A2D%20imaging%20with%20an%20encoder-decoder%20architecture%20and%20skip%20connections%2C%20achieved%0Aa%20training%20loss%20of%201.395%2C%20accurately%20reconstructing%20patches%20and%20capturing%0Aspatial%20relationships.%20These%20results%20highlight%20the%20potential%20of%20Prototypical%0ANetworks%20for%20tissue%20classification%20with%20limited%20labeled%20data%20and%20PRNet%20for%0Aanatomical%20landmark%20localization%2C%20paving%20the%20way%20for%20improved%20performance%20in%0Adeep%20learning%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Classification%2520and%2520Anatomical%2520Localization%2520of%2520Tissues%2520in%2520SPECT%250A%2520%2520Imaging%26entry.906535625%3DMohammed%2520Abdul%2520Hafeez%2520Khan%2520and%2520Samuel%2520Morries%2520Boddepalli%2520and%2520Siddhartha%2520Bhattacharyya%2520and%2520Debasis%2520Mitra%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520and%2520anatomical%2520localization%2520are%2520essential%2520for%250Aeffective%2520medical%2520diagnostics%2520and%2520research%252C%2520which%2520may%2520be%2520efficiently%2520performed%250Ausing%2520deep%2520learning%2520techniques.%2520However%252C%2520availability%2520of%2520limited%2520labeled%2520data%250Aposes%2520a%2520significant%2520challenge.%2520To%2520address%2520this%252C%2520we%2520adapted%2520Prototypical%250ANetworks%2520and%2520the%2520Propagation-Reconstruction%2520Network%2520%2528PRNet%2529%2520for%2520few-shot%250Aclassification%2520and%2520localization%252C%2520respectively%252C%2520in%2520Single%2520Photon%2520Emission%250AComputed%2520Tomography%2520%2528SPECT%2529%2520images.%2520For%2520the%2520proof%2520of%2520concept%2520we%2520used%2520a%250A2D-sliced%2520image%2520cropped%2520around%2520heart.%2520The%2520Prototypical%2520Network%252C%2520with%2520a%250Apre-trained%2520ResNet-18%2520backbone%252C%2520classified%2520ventricles%252C%2520myocardium%252C%2520and%2520liver%250Atissues%2520with%252096.67%2525%2520training%2520and%252093.33%2525%2520validation%2520accuracy.%2520PRNet%252C%2520adapted%2520for%250A2D%2520imaging%2520with%2520an%2520encoder-decoder%2520architecture%2520and%2520skip%2520connections%252C%2520achieved%250Aa%2520training%2520loss%2520of%25201.395%252C%2520accurately%2520reconstructing%2520patches%2520and%2520capturing%250Aspatial%2520relationships.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520Prototypical%250ANetworks%2520for%2520tissue%2520classification%2520with%2520limited%2520labeled%2520data%2520and%2520PRNet%2520for%250Aanatomical%2520landmark%2520localization%252C%2520paving%2520the%2520way%2520for%2520improved%2520performance%2520in%250Adeep%2520learning%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Classification%20and%20Anatomical%20Localization%20of%20Tissues%20in%20SPECT%0A%20%20Imaging&entry.906535625=Mohammed%20Abdul%20Hafeez%20Khan%20and%20Samuel%20Morries%20Boddepalli%20and%20Siddhartha%20Bhattacharyya%20and%20Debasis%20Mitra&entry.1292438233=%20%20Accurate%20classification%20and%20anatomical%20localization%20are%20essential%20for%0Aeffective%20medical%20diagnostics%20and%20research%2C%20which%20may%20be%20efficiently%20performed%0Ausing%20deep%20learning%20techniques.%20However%2C%20availability%20of%20limited%20labeled%20data%0Aposes%20a%20significant%20challenge.%20To%20address%20this%2C%20we%20adapted%20Prototypical%0ANetworks%20and%20the%20Propagation-Reconstruction%20Network%20%28PRNet%29%20for%20few-shot%0Aclassification%20and%20localization%2C%20respectively%2C%20in%20Single%20Photon%20Emission%0AComputed%20Tomography%20%28SPECT%29%20images.%20For%20the%20proof%20of%20concept%20we%20used%20a%0A2D-sliced%20image%20cropped%20around%20heart.%20The%20Prototypical%20Network%2C%20with%20a%0Apre-trained%20ResNet-18%20backbone%2C%20classified%20ventricles%2C%20myocardium%2C%20and%20liver%0Atissues%20with%2096.67%25%20training%20and%2093.33%25%20validation%20accuracy.%20PRNet%2C%20adapted%20for%0A2D%20imaging%20with%20an%20encoder-decoder%20architecture%20and%20skip%20connections%2C%20achieved%0Aa%20training%20loss%20of%201.395%2C%20accurately%20reconstructing%20patches%20and%20capturing%0Aspatial%20relationships.%20These%20results%20highlight%20the%20potential%20of%20Prototypical%0ANetworks%20for%20tissue%20classification%20with%20limited%20labeled%20data%20and%20PRNet%20for%0Aanatomical%20landmark%20localization%2C%20paving%20the%20way%20for%20improved%20performance%20in%0Adeep%20learning%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06632v1&entry.124074799=Read"},
{"title": "A Survey on Class-Agnostic Counting: Advancements from Reference-Based\n  to Open-World Text-Guided Approaches", "author": "Luca Ciampi and Ali Azmoudeh and Elif Ecem Akbaba and Erdi Sar\u0131ta\u015f and Ziya Ata Yaz\u0131c\u0131 and Haz\u0131m Kemal Ekenel and Giuseppe Amato and Fabrizio Falchi", "abstract": "  Visual object counting has recently shifted towards class-agnostic counting\n(CAC), which addresses the challenge of counting objects across arbitrary\ncategories -- a crucial capability for flexible and generalizable counting\nsystems. Unlike humans, who effortlessly identify and count objects from\ndiverse categories without prior knowledge, most existing counting methods are\nrestricted to enumerating instances of known classes, requiring extensive\nlabeled datasets for training and struggling in open-vocabulary settings. In\ncontrast, CAC aims to count objects belonging to classes never seen during\ntraining, operating in a few-shot setting. In this paper, we present the first\ncomprehensive review of CAC methodologies. We propose a taxonomy to categorize\nCAC approaches into three paradigms based on how target object classes can be\nspecified: reference-based, reference-less, and open-world text-guided.\nReference-based approaches achieve state-of-the-art performance by relying on\nexemplar-guided mechanisms. Reference-less methods eliminate exemplar\ndependency by leveraging inherent image patterns. Finally, open-world\ntext-guided methods use vision-language models, enabling object class\ndescriptions via textual prompts, offering a flexible and promising solution.\nBased on this taxonomy, we provide an overview of the architectures of 29 CAC\napproaches and report their results on gold-standard benchmarks. We compare\ntheir performance and discuss their strengths and limitations. Specifically, we\npresent results on the FSC-147 dataset, setting a leaderboard using\ngold-standard metrics, and on the CARPK dataset to assess generalization\ncapabilities. Finally, we offer a critical discussion of persistent challenges,\nsuch as annotation dependency and generalization, alongside future directions.\nWe believe this survey will be a valuable resource, showcasing CAC advancements\nand guiding future research.\n", "link": "http://arxiv.org/abs/2501.19184v2", "date": "2025-02-10", "relevancy": 2.5755, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Class-Agnostic%20Counting%3A%20Advancements%20from%20Reference-Based%0A%20%20to%20Open-World%20Text-Guided%20Approaches&body=Title%3A%20A%20Survey%20on%20Class-Agnostic%20Counting%3A%20Advancements%20from%20Reference-Based%0A%20%20to%20Open-World%20Text-Guided%20Approaches%0AAuthor%3A%20Luca%20Ciampi%20and%20Ali%20Azmoudeh%20and%20Elif%20Ecem%20Akbaba%20and%20Erdi%20Sar%C4%B1ta%C5%9F%20and%20Ziya%20Ata%20Yaz%C4%B1c%C4%B1%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi%0AAbstract%3A%20%20%20Visual%20object%20counting%20has%20recently%20shifted%20towards%20class-agnostic%20counting%0A%28CAC%29%2C%20which%20addresses%20the%20challenge%20of%20counting%20objects%20across%20arbitrary%0Acategories%20--%20a%20crucial%20capability%20for%20flexible%20and%20generalizable%20counting%0Asystems.%20Unlike%20humans%2C%20who%20effortlessly%20identify%20and%20count%20objects%20from%0Adiverse%20categories%20without%20prior%20knowledge%2C%20most%20existing%20counting%20methods%20are%0Arestricted%20to%20enumerating%20instances%20of%20known%20classes%2C%20requiring%20extensive%0Alabeled%20datasets%20for%20training%20and%20struggling%20in%20open-vocabulary%20settings.%20In%0Acontrast%2C%20CAC%20aims%20to%20count%20objects%20belonging%20to%20classes%20never%20seen%20during%0Atraining%2C%20operating%20in%20a%20few-shot%20setting.%20In%20this%20paper%2C%20we%20present%20the%20first%0Acomprehensive%20review%20of%20CAC%20methodologies.%20We%20propose%20a%20taxonomy%20to%20categorize%0ACAC%20approaches%20into%20three%20paradigms%20based%20on%20how%20target%20object%20classes%20can%20be%0Aspecified%3A%20reference-based%2C%20reference-less%2C%20and%20open-world%20text-guided.%0AReference-based%20approaches%20achieve%20state-of-the-art%20performance%20by%20relying%20on%0Aexemplar-guided%20mechanisms.%20Reference-less%20methods%20eliminate%20exemplar%0Adependency%20by%20leveraging%20inherent%20image%20patterns.%20Finally%2C%20open-world%0Atext-guided%20methods%20use%20vision-language%20models%2C%20enabling%20object%20class%0Adescriptions%20via%20textual%20prompts%2C%20offering%20a%20flexible%20and%20promising%20solution.%0ABased%20on%20this%20taxonomy%2C%20we%20provide%20an%20overview%20of%20the%20architectures%20of%2029%20CAC%0Aapproaches%20and%20report%20their%20results%20on%20gold-standard%20benchmarks.%20We%20compare%0Atheir%20performance%20and%20discuss%20their%20strengths%20and%20limitations.%20Specifically%2C%20we%0Apresent%20results%20on%20the%20FSC-147%20dataset%2C%20setting%20a%20leaderboard%20using%0Agold-standard%20metrics%2C%20and%20on%20the%20CARPK%20dataset%20to%20assess%20generalization%0Acapabilities.%20Finally%2C%20we%20offer%20a%20critical%20discussion%20of%20persistent%20challenges%2C%0Asuch%20as%20annotation%20dependency%20and%20generalization%2C%20alongside%20future%20directions.%0AWe%20believe%20this%20survey%20will%20be%20a%20valuable%20resource%2C%20showcasing%20CAC%20advancements%0Aand%20guiding%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Class-Agnostic%2520Counting%253A%2520Advancements%2520from%2520Reference-Based%250A%2520%2520to%2520Open-World%2520Text-Guided%2520Approaches%26entry.906535625%3DLuca%2520Ciampi%2520and%2520Ali%2520Azmoudeh%2520and%2520Elif%2520Ecem%2520Akbaba%2520and%2520Erdi%2520Sar%25C4%25B1ta%25C5%259F%2520and%2520Ziya%2520Ata%2520Yaz%25C4%25B1c%25C4%25B1%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Giuseppe%2520Amato%2520and%2520Fabrizio%2520Falchi%26entry.1292438233%3D%2520%2520Visual%2520object%2520counting%2520has%2520recently%2520shifted%2520towards%2520class-agnostic%2520counting%250A%2528CAC%2529%252C%2520which%2520addresses%2520the%2520challenge%2520of%2520counting%2520objects%2520across%2520arbitrary%250Acategories%2520--%2520a%2520crucial%2520capability%2520for%2520flexible%2520and%2520generalizable%2520counting%250Asystems.%2520Unlike%2520humans%252C%2520who%2520effortlessly%2520identify%2520and%2520count%2520objects%2520from%250Adiverse%2520categories%2520without%2520prior%2520knowledge%252C%2520most%2520existing%2520counting%2520methods%2520are%250Arestricted%2520to%2520enumerating%2520instances%2520of%2520known%2520classes%252C%2520requiring%2520extensive%250Alabeled%2520datasets%2520for%2520training%2520and%2520struggling%2520in%2520open-vocabulary%2520settings.%2520In%250Acontrast%252C%2520CAC%2520aims%2520to%2520count%2520objects%2520belonging%2520to%2520classes%2520never%2520seen%2520during%250Atraining%252C%2520operating%2520in%2520a%2520few-shot%2520setting.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%250Acomprehensive%2520review%2520of%2520CAC%2520methodologies.%2520We%2520propose%2520a%2520taxonomy%2520to%2520categorize%250ACAC%2520approaches%2520into%2520three%2520paradigms%2520based%2520on%2520how%2520target%2520object%2520classes%2520can%2520be%250Aspecified%253A%2520reference-based%252C%2520reference-less%252C%2520and%2520open-world%2520text-guided.%250AReference-based%2520approaches%2520achieve%2520state-of-the-art%2520performance%2520by%2520relying%2520on%250Aexemplar-guided%2520mechanisms.%2520Reference-less%2520methods%2520eliminate%2520exemplar%250Adependency%2520by%2520leveraging%2520inherent%2520image%2520patterns.%2520Finally%252C%2520open-world%250Atext-guided%2520methods%2520use%2520vision-language%2520models%252C%2520enabling%2520object%2520class%250Adescriptions%2520via%2520textual%2520prompts%252C%2520offering%2520a%2520flexible%2520and%2520promising%2520solution.%250ABased%2520on%2520this%2520taxonomy%252C%2520we%2520provide%2520an%2520overview%2520of%2520the%2520architectures%2520of%252029%2520CAC%250Aapproaches%2520and%2520report%2520their%2520results%2520on%2520gold-standard%2520benchmarks.%2520We%2520compare%250Atheir%2520performance%2520and%2520discuss%2520their%2520strengths%2520and%2520limitations.%2520Specifically%252C%2520we%250Apresent%2520results%2520on%2520the%2520FSC-147%2520dataset%252C%2520setting%2520a%2520leaderboard%2520using%250Agold-standard%2520metrics%252C%2520and%2520on%2520the%2520CARPK%2520dataset%2520to%2520assess%2520generalization%250Acapabilities.%2520Finally%252C%2520we%2520offer%2520a%2520critical%2520discussion%2520of%2520persistent%2520challenges%252C%250Asuch%2520as%2520annotation%2520dependency%2520and%2520generalization%252C%2520alongside%2520future%2520directions.%250AWe%2520believe%2520this%2520survey%2520will%2520be%2520a%2520valuable%2520resource%252C%2520showcasing%2520CAC%2520advancements%250Aand%2520guiding%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Class-Agnostic%20Counting%3A%20Advancements%20from%20Reference-Based%0A%20%20to%20Open-World%20Text-Guided%20Approaches&entry.906535625=Luca%20Ciampi%20and%20Ali%20Azmoudeh%20and%20Elif%20Ecem%20Akbaba%20and%20Erdi%20Sar%C4%B1ta%C5%9F%20and%20Ziya%20Ata%20Yaz%C4%B1c%C4%B1%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi&entry.1292438233=%20%20Visual%20object%20counting%20has%20recently%20shifted%20towards%20class-agnostic%20counting%0A%28CAC%29%2C%20which%20addresses%20the%20challenge%20of%20counting%20objects%20across%20arbitrary%0Acategories%20--%20a%20crucial%20capability%20for%20flexible%20and%20generalizable%20counting%0Asystems.%20Unlike%20humans%2C%20who%20effortlessly%20identify%20and%20count%20objects%20from%0Adiverse%20categories%20without%20prior%20knowledge%2C%20most%20existing%20counting%20methods%20are%0Arestricted%20to%20enumerating%20instances%20of%20known%20classes%2C%20requiring%20extensive%0Alabeled%20datasets%20for%20training%20and%20struggling%20in%20open-vocabulary%20settings.%20In%0Acontrast%2C%20CAC%20aims%20to%20count%20objects%20belonging%20to%20classes%20never%20seen%20during%0Atraining%2C%20operating%20in%20a%20few-shot%20setting.%20In%20this%20paper%2C%20we%20present%20the%20first%0Acomprehensive%20review%20of%20CAC%20methodologies.%20We%20propose%20a%20taxonomy%20to%20categorize%0ACAC%20approaches%20into%20three%20paradigms%20based%20on%20how%20target%20object%20classes%20can%20be%0Aspecified%3A%20reference-based%2C%20reference-less%2C%20and%20open-world%20text-guided.%0AReference-based%20approaches%20achieve%20state-of-the-art%20performance%20by%20relying%20on%0Aexemplar-guided%20mechanisms.%20Reference-less%20methods%20eliminate%20exemplar%0Adependency%20by%20leveraging%20inherent%20image%20patterns.%20Finally%2C%20open-world%0Atext-guided%20methods%20use%20vision-language%20models%2C%20enabling%20object%20class%0Adescriptions%20via%20textual%20prompts%2C%20offering%20a%20flexible%20and%20promising%20solution.%0ABased%20on%20this%20taxonomy%2C%20we%20provide%20an%20overview%20of%20the%20architectures%20of%2029%20CAC%0Aapproaches%20and%20report%20their%20results%20on%20gold-standard%20benchmarks.%20We%20compare%0Atheir%20performance%20and%20discuss%20their%20strengths%20and%20limitations.%20Specifically%2C%20we%0Apresent%20results%20on%20the%20FSC-147%20dataset%2C%20setting%20a%20leaderboard%20using%0Agold-standard%20metrics%2C%20and%20on%20the%20CARPK%20dataset%20to%20assess%20generalization%0Acapabilities.%20Finally%2C%20we%20offer%20a%20critical%20discussion%20of%20persistent%20challenges%2C%0Asuch%20as%20annotation%20dependency%20and%20generalization%2C%20alongside%20future%20directions.%0AWe%20believe%20this%20survey%20will%20be%20a%20valuable%20resource%2C%20showcasing%20CAC%20advancements%0Aand%20guiding%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19184v2&entry.124074799=Read"},
{"title": "LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection", "author": "Yihe Wang and Nan Huang and Nadia Mammone and Marco Cecchi and Xiang Zhang", "abstract": "  Electroencephalogram (EEG) provides a non-invasive, highly accessible, and\ncost-effective solution for Alzheimer's Disease (AD) detection. However,\nexisting methods, whether based on manual feature extraction or deep learning,\nface two major challenges: the lack of large-scale datasets for robust feature\nlearning and evaluation, and poor detection performance due to inter-subject\nvariations. To address these challenges, we curate an EEG-AD corpus containing\n813 subjects, which forms the world's largest EEG-AD dataset to the best of our\nknowledge. Using this unique dataset, we propose LEAD, the first large\nfoundation model for EEG-based AD detection. Our method encompasses an entire\npipeline, from data selection and preprocessing to self-supervised contrastive\npretraining, fine-tuning, and key setups such as subject-independent evaluation\nand majority voting for subject-level detection. We pre-train the model on 11\nEEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised\npre-training design includes sample-level and subject-level contrasting to\nextract useful general EEG features. Fine-tuning is performed on 5\nchannel-aligned datasets together. The backbone encoder incorporates temporal\nand channel embeddings to capture features across both temporal and spatial\ndimensions. Our method demonstrates outstanding AD detection performance,\nachieving up to a 9.86% increase in F1 score at the sample-level and up to a\n9.31% at the subject-level compared to state-of-the-art methods. The results of\nour model strongly confirm the effectiveness of contrastive pre-training and\nchannel-aligned unified fine-tuning for addressing inter-subject variation. The\nsource code is at https://github.com/DL4mHealth/LEAD.\n", "link": "http://arxiv.org/abs/2502.01678v2", "date": "2025-02-10", "relevancy": 2.5676, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5426}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEAD%3A%20Large%20Foundation%20Model%20for%20EEG-Based%20Alzheimer%27s%20Disease%20Detection&body=Title%3A%20LEAD%3A%20Large%20Foundation%20Model%20for%20EEG-Based%20Alzheimer%27s%20Disease%20Detection%0AAuthor%3A%20Yihe%20Wang%20and%20Nan%20Huang%20and%20Nadia%20Mammone%20and%20Marco%20Cecchi%20and%20Xiang%20Zhang%0AAbstract%3A%20%20%20Electroencephalogram%20%28EEG%29%20provides%20a%20non-invasive%2C%20highly%20accessible%2C%20and%0Acost-effective%20solution%20for%20Alzheimer%27s%20Disease%20%28AD%29%20detection.%20However%2C%0Aexisting%20methods%2C%20whether%20based%20on%20manual%20feature%20extraction%20or%20deep%20learning%2C%0Aface%20two%20major%20challenges%3A%20the%20lack%20of%20large-scale%20datasets%20for%20robust%20feature%0Alearning%20and%20evaluation%2C%20and%20poor%20detection%20performance%20due%20to%20inter-subject%0Avariations.%20To%20address%20these%20challenges%2C%20we%20curate%20an%20EEG-AD%20corpus%20containing%0A813%20subjects%2C%20which%20forms%20the%20world%27s%20largest%20EEG-AD%20dataset%20to%20the%20best%20of%20our%0Aknowledge.%20Using%20this%20unique%20dataset%2C%20we%20propose%20LEAD%2C%20the%20first%20large%0Afoundation%20model%20for%20EEG-based%20AD%20detection.%20Our%20method%20encompasses%20an%20entire%0Apipeline%2C%20from%20data%20selection%20and%20preprocessing%20to%20self-supervised%20contrastive%0Apretraining%2C%20fine-tuning%2C%20and%20key%20setups%20such%20as%20subject-independent%20evaluation%0Aand%20majority%20voting%20for%20subject-level%20detection.%20We%20pre-train%20the%20model%20on%2011%0AEEG%20datasets%20and%20unified%20fine-tune%20it%20on%205%20AD%20datasets.%20Our%20self-supervised%0Apre-training%20design%20includes%20sample-level%20and%20subject-level%20contrasting%20to%0Aextract%20useful%20general%20EEG%20features.%20Fine-tuning%20is%20performed%20on%205%0Achannel-aligned%20datasets%20together.%20The%20backbone%20encoder%20incorporates%20temporal%0Aand%20channel%20embeddings%20to%20capture%20features%20across%20both%20temporal%20and%20spatial%0Adimensions.%20Our%20method%20demonstrates%20outstanding%20AD%20detection%20performance%2C%0Aachieving%20up%20to%20a%209.86%25%20increase%20in%20F1%20score%20at%20the%20sample-level%20and%20up%20to%20a%0A9.31%25%20at%20the%20subject-level%20compared%20to%20state-of-the-art%20methods.%20The%20results%20of%0Aour%20model%20strongly%20confirm%20the%20effectiveness%20of%20contrastive%20pre-training%20and%0Achannel-aligned%20unified%20fine-tuning%20for%20addressing%20inter-subject%20variation.%20The%0Asource%20code%20is%20at%20https%3A//github.com/DL4mHealth/LEAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEAD%253A%2520Large%2520Foundation%2520Model%2520for%2520EEG-Based%2520Alzheimer%2527s%2520Disease%2520Detection%26entry.906535625%3DYihe%2520Wang%2520and%2520Nan%2520Huang%2520and%2520Nadia%2520Mammone%2520and%2520Marco%2520Cecchi%2520and%2520Xiang%2520Zhang%26entry.1292438233%3D%2520%2520Electroencephalogram%2520%2528EEG%2529%2520provides%2520a%2520non-invasive%252C%2520highly%2520accessible%252C%2520and%250Acost-effective%2520solution%2520for%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520detection.%2520However%252C%250Aexisting%2520methods%252C%2520whether%2520based%2520on%2520manual%2520feature%2520extraction%2520or%2520deep%2520learning%252C%250Aface%2520two%2520major%2520challenges%253A%2520the%2520lack%2520of%2520large-scale%2520datasets%2520for%2520robust%2520feature%250Alearning%2520and%2520evaluation%252C%2520and%2520poor%2520detection%2520performance%2520due%2520to%2520inter-subject%250Avariations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520curate%2520an%2520EEG-AD%2520corpus%2520containing%250A813%2520subjects%252C%2520which%2520forms%2520the%2520world%2527s%2520largest%2520EEG-AD%2520dataset%2520to%2520the%2520best%2520of%2520our%250Aknowledge.%2520Using%2520this%2520unique%2520dataset%252C%2520we%2520propose%2520LEAD%252C%2520the%2520first%2520large%250Afoundation%2520model%2520for%2520EEG-based%2520AD%2520detection.%2520Our%2520method%2520encompasses%2520an%2520entire%250Apipeline%252C%2520from%2520data%2520selection%2520and%2520preprocessing%2520to%2520self-supervised%2520contrastive%250Apretraining%252C%2520fine-tuning%252C%2520and%2520key%2520setups%2520such%2520as%2520subject-independent%2520evaluation%250Aand%2520majority%2520voting%2520for%2520subject-level%2520detection.%2520We%2520pre-train%2520the%2520model%2520on%252011%250AEEG%2520datasets%2520and%2520unified%2520fine-tune%2520it%2520on%25205%2520AD%2520datasets.%2520Our%2520self-supervised%250Apre-training%2520design%2520includes%2520sample-level%2520and%2520subject-level%2520contrasting%2520to%250Aextract%2520useful%2520general%2520EEG%2520features.%2520Fine-tuning%2520is%2520performed%2520on%25205%250Achannel-aligned%2520datasets%2520together.%2520The%2520backbone%2520encoder%2520incorporates%2520temporal%250Aand%2520channel%2520embeddings%2520to%2520capture%2520features%2520across%2520both%2520temporal%2520and%2520spatial%250Adimensions.%2520Our%2520method%2520demonstrates%2520outstanding%2520AD%2520detection%2520performance%252C%250Aachieving%2520up%2520to%2520a%25209.86%2525%2520increase%2520in%2520F1%2520score%2520at%2520the%2520sample-level%2520and%2520up%2520to%2520a%250A9.31%2525%2520at%2520the%2520subject-level%2520compared%2520to%2520state-of-the-art%2520methods.%2520The%2520results%2520of%250Aour%2520model%2520strongly%2520confirm%2520the%2520effectiveness%2520of%2520contrastive%2520pre-training%2520and%250Achannel-aligned%2520unified%2520fine-tuning%2520for%2520addressing%2520inter-subject%2520variation.%2520The%250Asource%2520code%2520is%2520at%2520https%253A//github.com/DL4mHealth/LEAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAD%3A%20Large%20Foundation%20Model%20for%20EEG-Based%20Alzheimer%27s%20Disease%20Detection&entry.906535625=Yihe%20Wang%20and%20Nan%20Huang%20and%20Nadia%20Mammone%20and%20Marco%20Cecchi%20and%20Xiang%20Zhang&entry.1292438233=%20%20Electroencephalogram%20%28EEG%29%20provides%20a%20non-invasive%2C%20highly%20accessible%2C%20and%0Acost-effective%20solution%20for%20Alzheimer%27s%20Disease%20%28AD%29%20detection.%20However%2C%0Aexisting%20methods%2C%20whether%20based%20on%20manual%20feature%20extraction%20or%20deep%20learning%2C%0Aface%20two%20major%20challenges%3A%20the%20lack%20of%20large-scale%20datasets%20for%20robust%20feature%0Alearning%20and%20evaluation%2C%20and%20poor%20detection%20performance%20due%20to%20inter-subject%0Avariations.%20To%20address%20these%20challenges%2C%20we%20curate%20an%20EEG-AD%20corpus%20containing%0A813%20subjects%2C%20which%20forms%20the%20world%27s%20largest%20EEG-AD%20dataset%20to%20the%20best%20of%20our%0Aknowledge.%20Using%20this%20unique%20dataset%2C%20we%20propose%20LEAD%2C%20the%20first%20large%0Afoundation%20model%20for%20EEG-based%20AD%20detection.%20Our%20method%20encompasses%20an%20entire%0Apipeline%2C%20from%20data%20selection%20and%20preprocessing%20to%20self-supervised%20contrastive%0Apretraining%2C%20fine-tuning%2C%20and%20key%20setups%20such%20as%20subject-independent%20evaluation%0Aand%20majority%20voting%20for%20subject-level%20detection.%20We%20pre-train%20the%20model%20on%2011%0AEEG%20datasets%20and%20unified%20fine-tune%20it%20on%205%20AD%20datasets.%20Our%20self-supervised%0Apre-training%20design%20includes%20sample-level%20and%20subject-level%20contrasting%20to%0Aextract%20useful%20general%20EEG%20features.%20Fine-tuning%20is%20performed%20on%205%0Achannel-aligned%20datasets%20together.%20The%20backbone%20encoder%20incorporates%20temporal%0Aand%20channel%20embeddings%20to%20capture%20features%20across%20both%20temporal%20and%20spatial%0Adimensions.%20Our%20method%20demonstrates%20outstanding%20AD%20detection%20performance%2C%0Aachieving%20up%20to%20a%209.86%25%20increase%20in%20F1%20score%20at%20the%20sample-level%20and%20up%20to%20a%0A9.31%25%20at%20the%20subject-level%20compared%20to%20state-of-the-art%20methods.%20The%20results%20of%0Aour%20model%20strongly%20confirm%20the%20effectiveness%20of%20contrastive%20pre-training%20and%0Achannel-aligned%20unified%20fine-tuning%20for%20addressing%20inter-subject%20variation.%20The%0Asource%20code%20is%20at%20https%3A//github.com/DL4mHealth/LEAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01678v2&entry.124074799=Read"},
{"title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training", "author": "Yuchen Zhuang and Jingfeng Yang and Haoming Jiang and Xin Liu and Kewei Cheng and Sanket Lokegaonkar and Yifan Gao and Qing Ping and Tianyi Liu and Binxuan Huang and Zheng Li and Zhengyang Wang and Pei Chen and Ruijie Wang and Rongzhi Zhang and Nasser Zalmout and Priyanka Nigam and Bing Yin and Chao Zhang", "abstract": "  Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.\n", "link": "http://arxiv.org/abs/2502.06589v1", "date": "2025-02-10", "relevancy": 2.5509, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hephaestus%3A%20Improving%20Fundamental%20Agent%20Capabilities%20of%20Large%20Language%0A%20%20Models%20through%20Continual%20Pre-Training&body=Title%3A%20Hephaestus%3A%20Improving%20Fundamental%20Agent%20Capabilities%20of%20Large%20Language%0A%20%20Models%20through%20Continual%20Pre-Training%0AAuthor%3A%20Yuchen%20Zhuang%20and%20Jingfeng%20Yang%20and%20Haoming%20Jiang%20and%20Xin%20Liu%20and%20Kewei%20Cheng%20and%20Sanket%20Lokegaonkar%20and%20Yifan%20Gao%20and%20Qing%20Ping%20and%20Tianyi%20Liu%20and%20Binxuan%20Huang%20and%20Zheng%20Li%20and%20Zhengyang%20Wang%20and%20Pei%20Chen%20and%20Ruijie%20Wang%20and%20Rongzhi%20Zhang%20and%20Nasser%20Zalmout%20and%20Priyanka%20Nigam%20and%20Bing%20Yin%20and%20Chao%20Zhang%0AAbstract%3A%20%20%20Due%20to%20the%20scarcity%20of%20agent-oriented%20pre-training%20data%2C%20LLM-based%20autonomous%0Aagents%20typically%20rely%20on%20complex%20prompting%20or%20extensive%20fine-tuning%2C%20which%0Aoften%20fails%20to%20introduce%20new%20capabilities%20while%20preserving%20strong%0Ageneralizability.%20We%20introduce%20Hephaestus-Forge%2C%20the%20first%20large-scale%0Apre-training%20corpus%20designed%20to%20enhance%20the%20fundamental%20capabilities%20of%20LLM%0Aagents%20in%20API%20function%20calling%2C%20intrinsic%20reasoning%20and%20planning%2C%20and%20adapting%0Ato%20environmental%20feedback.%20Hephaestus-Forge%20comprises%20103B%20agent-specific%20data%0Aencompassing%2076%2C537%20APIs%2C%20including%20both%20tool%20documentation%20to%20introduce%0Aknowledge%20of%20API%20functions%20and%20function%20calling%20trajectories%20to%20strengthen%0Aintrinsic%20reasoning.%20To%20explore%20effective%20training%20protocols%2C%20we%20investigate%0Ascaling%20laws%20to%20identify%20the%20optimal%20recipe%20in%20data%20mixing%20ratios.%20By%20continual%0Apre-training%20on%20Hephaestus-Forge%2C%20Hephaestus%20outperforms%20small-%20to%20medium-scale%0Aopen-source%20LLMs%20and%20rivals%20commercial%20LLMs%20on%20three%20agent%20benchmarks%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20pre-training%20corpus%20in%20enhancing%0Afundamental%20agentic%20capabilities%20and%20generalization%20of%20LLMs%20to%20new%20tasks%20or%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHephaestus%253A%2520Improving%2520Fundamental%2520Agent%2520Capabilities%2520of%2520Large%2520Language%250A%2520%2520Models%2520through%2520Continual%2520Pre-Training%26entry.906535625%3DYuchen%2520Zhuang%2520and%2520Jingfeng%2520Yang%2520and%2520Haoming%2520Jiang%2520and%2520Xin%2520Liu%2520and%2520Kewei%2520Cheng%2520and%2520Sanket%2520Lokegaonkar%2520and%2520Yifan%2520Gao%2520and%2520Qing%2520Ping%2520and%2520Tianyi%2520Liu%2520and%2520Binxuan%2520Huang%2520and%2520Zheng%2520Li%2520and%2520Zhengyang%2520Wang%2520and%2520Pei%2520Chen%2520and%2520Ruijie%2520Wang%2520and%2520Rongzhi%2520Zhang%2520and%2520Nasser%2520Zalmout%2520and%2520Priyanka%2520Nigam%2520and%2520Bing%2520Yin%2520and%2520Chao%2520Zhang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520scarcity%2520of%2520agent-oriented%2520pre-training%2520data%252C%2520LLM-based%2520autonomous%250Aagents%2520typically%2520rely%2520on%2520complex%2520prompting%2520or%2520extensive%2520fine-tuning%252C%2520which%250Aoften%2520fails%2520to%2520introduce%2520new%2520capabilities%2520while%2520preserving%2520strong%250Ageneralizability.%2520We%2520introduce%2520Hephaestus-Forge%252C%2520the%2520first%2520large-scale%250Apre-training%2520corpus%2520designed%2520to%2520enhance%2520the%2520fundamental%2520capabilities%2520of%2520LLM%250Aagents%2520in%2520API%2520function%2520calling%252C%2520intrinsic%2520reasoning%2520and%2520planning%252C%2520and%2520adapting%250Ato%2520environmental%2520feedback.%2520Hephaestus-Forge%2520comprises%2520103B%2520agent-specific%2520data%250Aencompassing%252076%252C537%2520APIs%252C%2520including%2520both%2520tool%2520documentation%2520to%2520introduce%250Aknowledge%2520of%2520API%2520functions%2520and%2520function%2520calling%2520trajectories%2520to%2520strengthen%250Aintrinsic%2520reasoning.%2520To%2520explore%2520effective%2520training%2520protocols%252C%2520we%2520investigate%250Ascaling%2520laws%2520to%2520identify%2520the%2520optimal%2520recipe%2520in%2520data%2520mixing%2520ratios.%2520By%2520continual%250Apre-training%2520on%2520Hephaestus-Forge%252C%2520Hephaestus%2520outperforms%2520small-%2520to%2520medium-scale%250Aopen-source%2520LLMs%2520and%2520rivals%2520commercial%2520LLMs%2520on%2520three%2520agent%2520benchmarks%252C%250Ademonstrating%2520the%2520effectiveness%2520of%2520our%2520pre-training%2520corpus%2520in%2520enhancing%250Afundamental%2520agentic%2520capabilities%2520and%2520generalization%2520of%2520LLMs%2520to%2520new%2520tasks%2520or%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hephaestus%3A%20Improving%20Fundamental%20Agent%20Capabilities%20of%20Large%20Language%0A%20%20Models%20through%20Continual%20Pre-Training&entry.906535625=Yuchen%20Zhuang%20and%20Jingfeng%20Yang%20and%20Haoming%20Jiang%20and%20Xin%20Liu%20and%20Kewei%20Cheng%20and%20Sanket%20Lokegaonkar%20and%20Yifan%20Gao%20and%20Qing%20Ping%20and%20Tianyi%20Liu%20and%20Binxuan%20Huang%20and%20Zheng%20Li%20and%20Zhengyang%20Wang%20and%20Pei%20Chen%20and%20Ruijie%20Wang%20and%20Rongzhi%20Zhang%20and%20Nasser%20Zalmout%20and%20Priyanka%20Nigam%20and%20Bing%20Yin%20and%20Chao%20Zhang&entry.1292438233=%20%20Due%20to%20the%20scarcity%20of%20agent-oriented%20pre-training%20data%2C%20LLM-based%20autonomous%0Aagents%20typically%20rely%20on%20complex%20prompting%20or%20extensive%20fine-tuning%2C%20which%0Aoften%20fails%20to%20introduce%20new%20capabilities%20while%20preserving%20strong%0Ageneralizability.%20We%20introduce%20Hephaestus-Forge%2C%20the%20first%20large-scale%0Apre-training%20corpus%20designed%20to%20enhance%20the%20fundamental%20capabilities%20of%20LLM%0Aagents%20in%20API%20function%20calling%2C%20intrinsic%20reasoning%20and%20planning%2C%20and%20adapting%0Ato%20environmental%20feedback.%20Hephaestus-Forge%20comprises%20103B%20agent-specific%20data%0Aencompassing%2076%2C537%20APIs%2C%20including%20both%20tool%20documentation%20to%20introduce%0Aknowledge%20of%20API%20functions%20and%20function%20calling%20trajectories%20to%20strengthen%0Aintrinsic%20reasoning.%20To%20explore%20effective%20training%20protocols%2C%20we%20investigate%0Ascaling%20laws%20to%20identify%20the%20optimal%20recipe%20in%20data%20mixing%20ratios.%20By%20continual%0Apre-training%20on%20Hephaestus-Forge%2C%20Hephaestus%20outperforms%20small-%20to%20medium-scale%0Aopen-source%20LLMs%20and%20rivals%20commercial%20LLMs%20on%20three%20agent%20benchmarks%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20pre-training%20corpus%20in%20enhancing%0Afundamental%20agentic%20capabilities%20and%20generalization%20of%20LLMs%20to%20new%20tasks%20or%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06589v1&entry.124074799=Read"},
{"title": "LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM", "author": "Zhi Zhou and Kun-Yang Yu and Shi-Yu Tian and Jiang-Xin Shi and Xiao-Wen Yang and Pengxiao Song and Yi-Xuan Jin and Lan-Zhe Guo and Yu-Feng Li", "abstract": "  Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://anonymous.4open.science/r/KgDG-45F5 .\n", "link": "http://arxiv.org/abs/2502.06572v1", "date": "2025-02-10", "relevancy": 2.5509, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5297}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5059}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LawGPT%3A%20Knowledge-Guided%20Data%20Generation%20and%20Its%20Application%20to%20Legal%0A%20%20LLM&body=Title%3A%20LawGPT%3A%20Knowledge-Guided%20Data%20Generation%20and%20Its%20Application%20to%20Legal%0A%20%20LLM%0AAuthor%3A%20Zhi%20Zhou%20and%20Kun-Yang%20Yu%20and%20Shi-Yu%20Tian%20and%20Jiang-Xin%20Shi%20and%20Xiao-Wen%20Yang%20and%20Pengxiao%20Song%20and%20Yi-Xuan%20Jin%20and%20Lan-Zhe%20Guo%20and%20Yu-Feng%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20both%20proprietary%20and%20open-source%2C%20have%0Ademonstrated%20remarkable%20capabilities%20across%20various%20natural%20language%20processing%0Atasks.%20However%2C%20they%20face%20significant%20limitations%20in%20legal%20reasoning%20tasks.%0AProprietary%20models%20introduce%20data%20privacy%20risks%20and%20high%20inference%20costs%2C%20while%0Aopen-source%20models%20underperform%20due%20to%20insufficient%20legal%20domain%20training%20data.%0ATo%20address%20these%20limitations%2C%20we%20study%20data%20generation%20for%20legal%20reasoning%20to%0Aimprove%20the%20legal%20reasoning%20performance%20of%20open-source%20LLMs%20with%20the%20help%20of%0Aproprietary%20LLMs.%20This%20is%20challenging%20due%20to%20the%20lack%20of%20legal%20knowledge%20in%0Aproprietary%20LLMs%20and%20the%20difficulty%20in%20verifying%20the%20generated%20data.%20We%20propose%0AKgDG%2C%20a%20knowledge-guided%20data%20generation%20framework%20for%20legal%20reasoning.%20Our%0Aframework%20enables%20leveraging%20legal%20knowledge%20to%20enhance%20generation%20diversity%0Aand%20introduces%20a%20refinement%20and%20verification%20process%20to%20ensure%20the%20quality%20of%0Agenerated%20data.%20Moreover%2C%20we%20expand%20the%20generated%20dataset%20to%20further%20enhance%0Athe%20LLM%20reasoning%20capabilities.%20Using%20KgDG%2C%20we%20create%20a%20synthetic%20legal%0Areasoning%20dataset%20containing%2050K%20high-quality%20examples.%20Our%20trained%20model%0ALawGPT%20outperforms%20existing%20legal-specific%20LLMs%20and%20achieves%20performance%0Acomparable%20to%20proprietary%20LLMs%2C%20demonstrating%20the%20effectiveness%20of%20KgDG%20and%0ALawGPT.%20Our%20code%20and%20resources%20is%20publicly%20available%20at%0Ahttps%3A//anonymous.4open.science/r/KgDG-45F5%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLawGPT%253A%2520Knowledge-Guided%2520Data%2520Generation%2520and%2520Its%2520Application%2520to%2520Legal%250A%2520%2520LLM%26entry.906535625%3DZhi%2520Zhou%2520and%2520Kun-Yang%2520Yu%2520and%2520Shi-Yu%2520Tian%2520and%2520Jiang-Xin%2520Shi%2520and%2520Xiao-Wen%2520Yang%2520and%2520Pengxiao%2520Song%2520and%2520Yi-Xuan%2520Jin%2520and%2520Lan-Zhe%2520Guo%2520and%2520Yu-Feng%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520both%2520proprietary%2520and%2520open-source%252C%2520have%250Ademonstrated%2520remarkable%2520capabilities%2520across%2520various%2520natural%2520language%2520processing%250Atasks.%2520However%252C%2520they%2520face%2520significant%2520limitations%2520in%2520legal%2520reasoning%2520tasks.%250AProprietary%2520models%2520introduce%2520data%2520privacy%2520risks%2520and%2520high%2520inference%2520costs%252C%2520while%250Aopen-source%2520models%2520underperform%2520due%2520to%2520insufficient%2520legal%2520domain%2520training%2520data.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520study%2520data%2520generation%2520for%2520legal%2520reasoning%2520to%250Aimprove%2520the%2520legal%2520reasoning%2520performance%2520of%2520open-source%2520LLMs%2520with%2520the%2520help%2520of%250Aproprietary%2520LLMs.%2520This%2520is%2520challenging%2520due%2520to%2520the%2520lack%2520of%2520legal%2520knowledge%2520in%250Aproprietary%2520LLMs%2520and%2520the%2520difficulty%2520in%2520verifying%2520the%2520generated%2520data.%2520We%2520propose%250AKgDG%252C%2520a%2520knowledge-guided%2520data%2520generation%2520framework%2520for%2520legal%2520reasoning.%2520Our%250Aframework%2520enables%2520leveraging%2520legal%2520knowledge%2520to%2520enhance%2520generation%2520diversity%250Aand%2520introduces%2520a%2520refinement%2520and%2520verification%2520process%2520to%2520ensure%2520the%2520quality%2520of%250Agenerated%2520data.%2520Moreover%252C%2520we%2520expand%2520the%2520generated%2520dataset%2520to%2520further%2520enhance%250Athe%2520LLM%2520reasoning%2520capabilities.%2520Using%2520KgDG%252C%2520we%2520create%2520a%2520synthetic%2520legal%250Areasoning%2520dataset%2520containing%252050K%2520high-quality%2520examples.%2520Our%2520trained%2520model%250ALawGPT%2520outperforms%2520existing%2520legal-specific%2520LLMs%2520and%2520achieves%2520performance%250Acomparable%2520to%2520proprietary%2520LLMs%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520KgDG%2520and%250ALawGPT.%2520Our%2520code%2520and%2520resources%2520is%2520publicly%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/KgDG-45F5%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LawGPT%3A%20Knowledge-Guided%20Data%20Generation%20and%20Its%20Application%20to%20Legal%0A%20%20LLM&entry.906535625=Zhi%20Zhou%20and%20Kun-Yang%20Yu%20and%20Shi-Yu%20Tian%20and%20Jiang-Xin%20Shi%20and%20Xiao-Wen%20Yang%20and%20Pengxiao%20Song%20and%20Yi-Xuan%20Jin%20and%20Lan-Zhe%20Guo%20and%20Yu-Feng%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20both%20proprietary%20and%20open-source%2C%20have%0Ademonstrated%20remarkable%20capabilities%20across%20various%20natural%20language%20processing%0Atasks.%20However%2C%20they%20face%20significant%20limitations%20in%20legal%20reasoning%20tasks.%0AProprietary%20models%20introduce%20data%20privacy%20risks%20and%20high%20inference%20costs%2C%20while%0Aopen-source%20models%20underperform%20due%20to%20insufficient%20legal%20domain%20training%20data.%0ATo%20address%20these%20limitations%2C%20we%20study%20data%20generation%20for%20legal%20reasoning%20to%0Aimprove%20the%20legal%20reasoning%20performance%20of%20open-source%20LLMs%20with%20the%20help%20of%0Aproprietary%20LLMs.%20This%20is%20challenging%20due%20to%20the%20lack%20of%20legal%20knowledge%20in%0Aproprietary%20LLMs%20and%20the%20difficulty%20in%20verifying%20the%20generated%20data.%20We%20propose%0AKgDG%2C%20a%20knowledge-guided%20data%20generation%20framework%20for%20legal%20reasoning.%20Our%0Aframework%20enables%20leveraging%20legal%20knowledge%20to%20enhance%20generation%20diversity%0Aand%20introduces%20a%20refinement%20and%20verification%20process%20to%20ensure%20the%20quality%20of%0Agenerated%20data.%20Moreover%2C%20we%20expand%20the%20generated%20dataset%20to%20further%20enhance%0Athe%20LLM%20reasoning%20capabilities.%20Using%20KgDG%2C%20we%20create%20a%20synthetic%20legal%0Areasoning%20dataset%20containing%2050K%20high-quality%20examples.%20Our%20trained%20model%0ALawGPT%20outperforms%20existing%20legal-specific%20LLMs%20and%20achieves%20performance%0Acomparable%20to%20proprietary%20LLMs%2C%20demonstrating%20the%20effectiveness%20of%20KgDG%20and%0ALawGPT.%20Our%20code%20and%20resources%20is%20publicly%20available%20at%0Ahttps%3A//anonymous.4open.science/r/KgDG-45F5%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06572v1&entry.124074799=Read"},
{"title": "Learning Musical Representations for Music Performance Question\n  Answering", "author": "Xingjian Diao and Chunhui Zhang and Tingxuan Wu and Ming Cheng and Zhongyu Ouyang and Weiyi Wu and Jiang Gui", "abstract": "  Music performances are representative scenarios for audio-visual modeling.\nUnlike common scenarios with sparse audio, music performances continuously\ninvolve dense audio signals throughout. While existing multimodal learning\nmethods on the audio-video QA demonstrate impressive capabilities in general\nscenarios, they are incapable of dealing with fundamental problems within the\nmusic performances: they underexplore the interaction between the multimodal\nsignals in performance and fail to consider the distinctive characteristics of\ninstruments and music. Therefore, existing methods tend to answer questions\nregarding musical performances inaccurately. To bridge the above research gaps,\n(i) given the intricate multimodal interconnectivity inherent to music data,\nour primary backbone is designed to incorporate multimodal interactions within\nthe context of music; (ii) to enable the model to learn music characteristics,\nwe annotate and release rhythmic and music sources in the current music\ndatasets; (iii) for time-aware audio-visual modeling, we align the model's\nmusic predictions with the temporal dimension. Our experiments show\nstate-of-the-art effects on the Music AVQA datasets. Our code is available at\nhttps://github.com/xid32/Amuse.\n", "link": "http://arxiv.org/abs/2502.06710v1", "date": "2025-02-10", "relevancy": 2.5452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Musical%20Representations%20for%20Music%20Performance%20Question%0A%20%20Answering&body=Title%3A%20Learning%20Musical%20Representations%20for%20Music%20Performance%20Question%0A%20%20Answering%0AAuthor%3A%20Xingjian%20Diao%20and%20Chunhui%20Zhang%20and%20Tingxuan%20Wu%20and%20Ming%20Cheng%20and%20Zhongyu%20Ouyang%20and%20Weiyi%20Wu%20and%20Jiang%20Gui%0AAbstract%3A%20%20%20Music%20performances%20are%20representative%20scenarios%20for%20audio-visual%20modeling.%0AUnlike%20common%20scenarios%20with%20sparse%20audio%2C%20music%20performances%20continuously%0Ainvolve%20dense%20audio%20signals%20throughout.%20While%20existing%20multimodal%20learning%0Amethods%20on%20the%20audio-video%20QA%20demonstrate%20impressive%20capabilities%20in%20general%0Ascenarios%2C%20they%20are%20incapable%20of%20dealing%20with%20fundamental%20problems%20within%20the%0Amusic%20performances%3A%20they%20underexplore%20the%20interaction%20between%20the%20multimodal%0Asignals%20in%20performance%20and%20fail%20to%20consider%20the%20distinctive%20characteristics%20of%0Ainstruments%20and%20music.%20Therefore%2C%20existing%20methods%20tend%20to%20answer%20questions%0Aregarding%20musical%20performances%20inaccurately.%20To%20bridge%20the%20above%20research%20gaps%2C%0A%28i%29%20given%20the%20intricate%20multimodal%20interconnectivity%20inherent%20to%20music%20data%2C%0Aour%20primary%20backbone%20is%20designed%20to%20incorporate%20multimodal%20interactions%20within%0Athe%20context%20of%20music%3B%20%28ii%29%20to%20enable%20the%20model%20to%20learn%20music%20characteristics%2C%0Awe%20annotate%20and%20release%20rhythmic%20and%20music%20sources%20in%20the%20current%20music%0Adatasets%3B%20%28iii%29%20for%20time-aware%20audio-visual%20modeling%2C%20we%20align%20the%20model%27s%0Amusic%20predictions%20with%20the%20temporal%20dimension.%20Our%20experiments%20show%0Astate-of-the-art%20effects%20on%20the%20Music%20AVQA%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/xid32/Amuse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Musical%2520Representations%2520for%2520Music%2520Performance%2520Question%250A%2520%2520Answering%26entry.906535625%3DXingjian%2520Diao%2520and%2520Chunhui%2520Zhang%2520and%2520Tingxuan%2520Wu%2520and%2520Ming%2520Cheng%2520and%2520Zhongyu%2520Ouyang%2520and%2520Weiyi%2520Wu%2520and%2520Jiang%2520Gui%26entry.1292438233%3D%2520%2520Music%2520performances%2520are%2520representative%2520scenarios%2520for%2520audio-visual%2520modeling.%250AUnlike%2520common%2520scenarios%2520with%2520sparse%2520audio%252C%2520music%2520performances%2520continuously%250Ainvolve%2520dense%2520audio%2520signals%2520throughout.%2520While%2520existing%2520multimodal%2520learning%250Amethods%2520on%2520the%2520audio-video%2520QA%2520demonstrate%2520impressive%2520capabilities%2520in%2520general%250Ascenarios%252C%2520they%2520are%2520incapable%2520of%2520dealing%2520with%2520fundamental%2520problems%2520within%2520the%250Amusic%2520performances%253A%2520they%2520underexplore%2520the%2520interaction%2520between%2520the%2520multimodal%250Asignals%2520in%2520performance%2520and%2520fail%2520to%2520consider%2520the%2520distinctive%2520characteristics%2520of%250Ainstruments%2520and%2520music.%2520Therefore%252C%2520existing%2520methods%2520tend%2520to%2520answer%2520questions%250Aregarding%2520musical%2520performances%2520inaccurately.%2520To%2520bridge%2520the%2520above%2520research%2520gaps%252C%250A%2528i%2529%2520given%2520the%2520intricate%2520multimodal%2520interconnectivity%2520inherent%2520to%2520music%2520data%252C%250Aour%2520primary%2520backbone%2520is%2520designed%2520to%2520incorporate%2520multimodal%2520interactions%2520within%250Athe%2520context%2520of%2520music%253B%2520%2528ii%2529%2520to%2520enable%2520the%2520model%2520to%2520learn%2520music%2520characteristics%252C%250Awe%2520annotate%2520and%2520release%2520rhythmic%2520and%2520music%2520sources%2520in%2520the%2520current%2520music%250Adatasets%253B%2520%2528iii%2529%2520for%2520time-aware%2520audio-visual%2520modeling%252C%2520we%2520align%2520the%2520model%2527s%250Amusic%2520predictions%2520with%2520the%2520temporal%2520dimension.%2520Our%2520experiments%2520show%250Astate-of-the-art%2520effects%2520on%2520the%2520Music%2520AVQA%2520datasets.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/xid32/Amuse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Musical%20Representations%20for%20Music%20Performance%20Question%0A%20%20Answering&entry.906535625=Xingjian%20Diao%20and%20Chunhui%20Zhang%20and%20Tingxuan%20Wu%20and%20Ming%20Cheng%20and%20Zhongyu%20Ouyang%20and%20Weiyi%20Wu%20and%20Jiang%20Gui&entry.1292438233=%20%20Music%20performances%20are%20representative%20scenarios%20for%20audio-visual%20modeling.%0AUnlike%20common%20scenarios%20with%20sparse%20audio%2C%20music%20performances%20continuously%0Ainvolve%20dense%20audio%20signals%20throughout.%20While%20existing%20multimodal%20learning%0Amethods%20on%20the%20audio-video%20QA%20demonstrate%20impressive%20capabilities%20in%20general%0Ascenarios%2C%20they%20are%20incapable%20of%20dealing%20with%20fundamental%20problems%20within%20the%0Amusic%20performances%3A%20they%20underexplore%20the%20interaction%20between%20the%20multimodal%0Asignals%20in%20performance%20and%20fail%20to%20consider%20the%20distinctive%20characteristics%20of%0Ainstruments%20and%20music.%20Therefore%2C%20existing%20methods%20tend%20to%20answer%20questions%0Aregarding%20musical%20performances%20inaccurately.%20To%20bridge%20the%20above%20research%20gaps%2C%0A%28i%29%20given%20the%20intricate%20multimodal%20interconnectivity%20inherent%20to%20music%20data%2C%0Aour%20primary%20backbone%20is%20designed%20to%20incorporate%20multimodal%20interactions%20within%0Athe%20context%20of%20music%3B%20%28ii%29%20to%20enable%20the%20model%20to%20learn%20music%20characteristics%2C%0Awe%20annotate%20and%20release%20rhythmic%20and%20music%20sources%20in%20the%20current%20music%0Adatasets%3B%20%28iii%29%20for%20time-aware%20audio-visual%20modeling%2C%20we%20align%20the%20model%27s%0Amusic%20predictions%20with%20the%20temporal%20dimension.%20Our%20experiments%20show%0Astate-of-the-art%20effects%20on%20the%20Music%20AVQA%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/xid32/Amuse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06710v1&entry.124074799=Read"},
{"title": "TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural\n  Radiance Fields", "author": "Chengyao Duan and Zhiliu Yang", "abstract": "  Previous attempts to integrate Neural Radiance Fields (NeRF) into the\nSimultaneous Localization and Mapping (SLAM) framework either rely on the\nassumption of static scenes or require the ground truth camera poses, which\nimpedes their application in real-world scenarios. This paper proposes a\ntime-varying representation to track and reconstruct the dynamic scenes.\nFirstly, two processes, a tracking process and a mapping process, are\nmaintained simultaneously in our framework. In the tracking process, all input\nimages are uniformly sampled and then progressively trained in a\nself-supervised paradigm. In the mapping process, we leverage motion masks to\ndistinguish dynamic objects from the static background, and sample more pixels\nfrom dynamic areas. Secondly, the parameter optimization for both processes is\ncomprised of two stages: the first stage associates time with 3D positions to\nconvert the deformation field to the canonical field. The second stage\nassociates time with the embeddings of the canonical field to obtain colors and\na Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection\nstrategy based on the overlapping rate. Our approach is evaluated on two\nsynthetic datasets and one real-world dataset, and the experiments validate\nthat our method achieves competitive results in both tracking and mapping when\ncompared to existing state-of-the-art NeRF-based dynamic SLAM systems.\n", "link": "http://arxiv.org/abs/2310.18917v7", "date": "2025-02-10", "relevancy": 2.5394, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6771}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6077}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields&body=Title%3A%20TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields%0AAuthor%3A%20Chengyao%20Duan%20and%20Zhiliu%20Yang%0AAbstract%3A%20%20%20Previous%20attempts%20to%20integrate%20Neural%20Radiance%20Fields%20%28NeRF%29%20into%20the%0ASimultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20framework%20either%20rely%20on%20the%0Aassumption%20of%20static%20scenes%20or%20require%20the%20ground%20truth%20camera%20poses%2C%20which%0Aimpedes%20their%20application%20in%20real-world%20scenarios.%20This%20paper%20proposes%20a%0Atime-varying%20representation%20to%20track%20and%20reconstruct%20the%20dynamic%20scenes.%0AFirstly%2C%20two%20processes%2C%20a%20tracking%20process%20and%20a%20mapping%20process%2C%20are%0Amaintained%20simultaneously%20in%20our%20framework.%20In%20the%20tracking%20process%2C%20all%20input%0Aimages%20are%20uniformly%20sampled%20and%20then%20progressively%20trained%20in%20a%0Aself-supervised%20paradigm.%20In%20the%20mapping%20process%2C%20we%20leverage%20motion%20masks%20to%0Adistinguish%20dynamic%20objects%20from%20the%20static%20background%2C%20and%20sample%20more%20pixels%0Afrom%20dynamic%20areas.%20Secondly%2C%20the%20parameter%20optimization%20for%20both%20processes%20is%0Acomprised%20of%20two%20stages%3A%20the%20first%20stage%20associates%20time%20with%203D%20positions%20to%0Aconvert%20the%20deformation%20field%20to%20the%20canonical%20field.%20The%20second%20stage%0Aassociates%20time%20with%20the%20embeddings%20of%20the%20canonical%20field%20to%20obtain%20colors%20and%0Aa%20Signed%20Distance%20Function%20%28SDF%29.%20Lastly%2C%20we%20propose%20a%20novel%20keyframe%20selection%0Astrategy%20based%20on%20the%20overlapping%20rate.%20Our%20approach%20is%20evaluated%20on%20two%0Asynthetic%20datasets%20and%20one%20real-world%20dataset%2C%20and%20the%20experiments%20validate%0Athat%20our%20method%20achieves%20competitive%20results%20in%20both%20tracking%20and%20mapping%20when%0Acompared%20to%20existing%20state-of-the-art%20NeRF-based%20dynamic%20SLAM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18917v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTivNe-SLAM%253A%2520Dynamic%2520Mapping%2520and%2520Tracking%2520via%2520Time-Varying%2520Neural%250A%2520%2520Radiance%2520Fields%26entry.906535625%3DChengyao%2520Duan%2520and%2520Zhiliu%2520Yang%26entry.1292438233%3D%2520%2520Previous%2520attempts%2520to%2520integrate%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520into%2520the%250ASimultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520framework%2520either%2520rely%2520on%2520the%250Aassumption%2520of%2520static%2520scenes%2520or%2520require%2520the%2520ground%2520truth%2520camera%2520poses%252C%2520which%250Aimpedes%2520their%2520application%2520in%2520real-world%2520scenarios.%2520This%2520paper%2520proposes%2520a%250Atime-varying%2520representation%2520to%2520track%2520and%2520reconstruct%2520the%2520dynamic%2520scenes.%250AFirstly%252C%2520two%2520processes%252C%2520a%2520tracking%2520process%2520and%2520a%2520mapping%2520process%252C%2520are%250Amaintained%2520simultaneously%2520in%2520our%2520framework.%2520In%2520the%2520tracking%2520process%252C%2520all%2520input%250Aimages%2520are%2520uniformly%2520sampled%2520and%2520then%2520progressively%2520trained%2520in%2520a%250Aself-supervised%2520paradigm.%2520In%2520the%2520mapping%2520process%252C%2520we%2520leverage%2520motion%2520masks%2520to%250Adistinguish%2520dynamic%2520objects%2520from%2520the%2520static%2520background%252C%2520and%2520sample%2520more%2520pixels%250Afrom%2520dynamic%2520areas.%2520Secondly%252C%2520the%2520parameter%2520optimization%2520for%2520both%2520processes%2520is%250Acomprised%2520of%2520two%2520stages%253A%2520the%2520first%2520stage%2520associates%2520time%2520with%25203D%2520positions%2520to%250Aconvert%2520the%2520deformation%2520field%2520to%2520the%2520canonical%2520field.%2520The%2520second%2520stage%250Aassociates%2520time%2520with%2520the%2520embeddings%2520of%2520the%2520canonical%2520field%2520to%2520obtain%2520colors%2520and%250Aa%2520Signed%2520Distance%2520Function%2520%2528SDF%2529.%2520Lastly%252C%2520we%2520propose%2520a%2520novel%2520keyframe%2520selection%250Astrategy%2520based%2520on%2520the%2520overlapping%2520rate.%2520Our%2520approach%2520is%2520evaluated%2520on%2520two%250Asynthetic%2520datasets%2520and%2520one%2520real-world%2520dataset%252C%2520and%2520the%2520experiments%2520validate%250Athat%2520our%2520method%2520achieves%2520competitive%2520results%2520in%2520both%2520tracking%2520and%2520mapping%2520when%250Acompared%2520to%2520existing%2520state-of-the-art%2520NeRF-based%2520dynamic%2520SLAM%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18917v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields&entry.906535625=Chengyao%20Duan%20and%20Zhiliu%20Yang&entry.1292438233=%20%20Previous%20attempts%20to%20integrate%20Neural%20Radiance%20Fields%20%28NeRF%29%20into%20the%0ASimultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20framework%20either%20rely%20on%20the%0Aassumption%20of%20static%20scenes%20or%20require%20the%20ground%20truth%20camera%20poses%2C%20which%0Aimpedes%20their%20application%20in%20real-world%20scenarios.%20This%20paper%20proposes%20a%0Atime-varying%20representation%20to%20track%20and%20reconstruct%20the%20dynamic%20scenes.%0AFirstly%2C%20two%20processes%2C%20a%20tracking%20process%20and%20a%20mapping%20process%2C%20are%0Amaintained%20simultaneously%20in%20our%20framework.%20In%20the%20tracking%20process%2C%20all%20input%0Aimages%20are%20uniformly%20sampled%20and%20then%20progressively%20trained%20in%20a%0Aself-supervised%20paradigm.%20In%20the%20mapping%20process%2C%20we%20leverage%20motion%20masks%20to%0Adistinguish%20dynamic%20objects%20from%20the%20static%20background%2C%20and%20sample%20more%20pixels%0Afrom%20dynamic%20areas.%20Secondly%2C%20the%20parameter%20optimization%20for%20both%20processes%20is%0Acomprised%20of%20two%20stages%3A%20the%20first%20stage%20associates%20time%20with%203D%20positions%20to%0Aconvert%20the%20deformation%20field%20to%20the%20canonical%20field.%20The%20second%20stage%0Aassociates%20time%20with%20the%20embeddings%20of%20the%20canonical%20field%20to%20obtain%20colors%20and%0Aa%20Signed%20Distance%20Function%20%28SDF%29.%20Lastly%2C%20we%20propose%20a%20novel%20keyframe%20selection%0Astrategy%20based%20on%20the%20overlapping%20rate.%20Our%20approach%20is%20evaluated%20on%20two%0Asynthetic%20datasets%20and%20one%20real-world%20dataset%2C%20and%20the%20experiments%20validate%0Athat%20our%20method%20achieves%20competitive%20results%20in%20both%20tracking%20and%20mapping%20when%0Acompared%20to%20existing%20state-of-the-art%20NeRF-based%20dynamic%20SLAM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18917v7&entry.124074799=Read"},
{"title": "Accelerating Outlier-robust Rotation Estimation by Stereographic\n  Projection", "author": "Taosi Xu and Yinlong Liu and Xianbo Wang and Zhi-Xin Yang", "abstract": "  Rotation estimation plays a fundamental role in many computer vision and\nrobot tasks. However, efficiently estimating rotation in large inputs\ncontaining numerous outliers (i.e., mismatches) and noise is a recognized\nchallenge. Many robust rotation estimation methods have been designed to\naddress this challenge. Unfortunately, existing methods are often inapplicable\ndue to their long computation time and the risk of local optima. In this paper,\nwe propose an efficient and robust rotation estimation method. Specifically,\nour method first investigates geometric constraints involving only the rotation\naxis. Then, it uses stereographic projection and spatial voting techniques to\nidentify the rotation axis and angle. Furthermore, our method efficiently\nobtains the optimal rotation estimation and can estimate multiple rotations\nsimultaneously. To verify the feasibility of our method, we conduct comparative\nexperiments using both synthetic and real-world data. The results show that,\nwith GPU assistance, our method can solve large-scale ($10^6$ points) and\nseverely corrupted (90\\% outlier rate) rotation estimation problems within 0.07\nseconds, with an angular error of only 0.01 degrees, which is superior to\nexisting methods in terms of accuracy and efficiency.\n", "link": "http://arxiv.org/abs/2502.06337v1", "date": "2025-02-10", "relevancy": 2.5299, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5331}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4941}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Outlier-robust%20Rotation%20Estimation%20by%20Stereographic%0A%20%20Projection&body=Title%3A%20Accelerating%20Outlier-robust%20Rotation%20Estimation%20by%20Stereographic%0A%20%20Projection%0AAuthor%3A%20Taosi%20Xu%20and%20Yinlong%20Liu%20and%20Xianbo%20Wang%20and%20Zhi-Xin%20Yang%0AAbstract%3A%20%20%20Rotation%20estimation%20plays%20a%20fundamental%20role%20in%20many%20computer%20vision%20and%0Arobot%20tasks.%20However%2C%20efficiently%20estimating%20rotation%20in%20large%20inputs%0Acontaining%20numerous%20outliers%20%28i.e.%2C%20mismatches%29%20and%20noise%20is%20a%20recognized%0Achallenge.%20Many%20robust%20rotation%20estimation%20methods%20have%20been%20designed%20to%0Aaddress%20this%20challenge.%20Unfortunately%2C%20existing%20methods%20are%20often%20inapplicable%0Adue%20to%20their%20long%20computation%20time%20and%20the%20risk%20of%20local%20optima.%20In%20this%20paper%2C%0Awe%20propose%20an%20efficient%20and%20robust%20rotation%20estimation%20method.%20Specifically%2C%0Aour%20method%20first%20investigates%20geometric%20constraints%20involving%20only%20the%20rotation%0Aaxis.%20Then%2C%20it%20uses%20stereographic%20projection%20and%20spatial%20voting%20techniques%20to%0Aidentify%20the%20rotation%20axis%20and%20angle.%20Furthermore%2C%20our%20method%20efficiently%0Aobtains%20the%20optimal%20rotation%20estimation%20and%20can%20estimate%20multiple%20rotations%0Asimultaneously.%20To%20verify%20the%20feasibility%20of%20our%20method%2C%20we%20conduct%20comparative%0Aexperiments%20using%20both%20synthetic%20and%20real-world%20data.%20The%20results%20show%20that%2C%0Awith%20GPU%20assistance%2C%20our%20method%20can%20solve%20large-scale%20%28%2410%5E6%24%20points%29%20and%0Aseverely%20corrupted%20%2890%5C%25%20outlier%20rate%29%20rotation%20estimation%20problems%20within%200.07%0Aseconds%2C%20with%20an%20angular%20error%20of%20only%200.01%20degrees%2C%20which%20is%20superior%20to%0Aexisting%20methods%20in%20terms%20of%20accuracy%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Outlier-robust%2520Rotation%2520Estimation%2520by%2520Stereographic%250A%2520%2520Projection%26entry.906535625%3DTaosi%2520Xu%2520and%2520Yinlong%2520Liu%2520and%2520Xianbo%2520Wang%2520and%2520Zhi-Xin%2520Yang%26entry.1292438233%3D%2520%2520Rotation%2520estimation%2520plays%2520a%2520fundamental%2520role%2520in%2520many%2520computer%2520vision%2520and%250Arobot%2520tasks.%2520However%252C%2520efficiently%2520estimating%2520rotation%2520in%2520large%2520inputs%250Acontaining%2520numerous%2520outliers%2520%2528i.e.%252C%2520mismatches%2529%2520and%2520noise%2520is%2520a%2520recognized%250Achallenge.%2520Many%2520robust%2520rotation%2520estimation%2520methods%2520have%2520been%2520designed%2520to%250Aaddress%2520this%2520challenge.%2520Unfortunately%252C%2520existing%2520methods%2520are%2520often%2520inapplicable%250Adue%2520to%2520their%2520long%2520computation%2520time%2520and%2520the%2520risk%2520of%2520local%2520optima.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520an%2520efficient%2520and%2520robust%2520rotation%2520estimation%2520method.%2520Specifically%252C%250Aour%2520method%2520first%2520investigates%2520geometric%2520constraints%2520involving%2520only%2520the%2520rotation%250Aaxis.%2520Then%252C%2520it%2520uses%2520stereographic%2520projection%2520and%2520spatial%2520voting%2520techniques%2520to%250Aidentify%2520the%2520rotation%2520axis%2520and%2520angle.%2520Furthermore%252C%2520our%2520method%2520efficiently%250Aobtains%2520the%2520optimal%2520rotation%2520estimation%2520and%2520can%2520estimate%2520multiple%2520rotations%250Asimultaneously.%2520To%2520verify%2520the%2520feasibility%2520of%2520our%2520method%252C%2520we%2520conduct%2520comparative%250Aexperiments%2520using%2520both%2520synthetic%2520and%2520real-world%2520data.%2520The%2520results%2520show%2520that%252C%250Awith%2520GPU%2520assistance%252C%2520our%2520method%2520can%2520solve%2520large-scale%2520%2528%252410%255E6%2524%2520points%2529%2520and%250Aseverely%2520corrupted%2520%252890%255C%2525%2520outlier%2520rate%2529%2520rotation%2520estimation%2520problems%2520within%25200.07%250Aseconds%252C%2520with%2520an%2520angular%2520error%2520of%2520only%25200.01%2520degrees%252C%2520which%2520is%2520superior%2520to%250Aexisting%2520methods%2520in%2520terms%2520of%2520accuracy%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Outlier-robust%20Rotation%20Estimation%20by%20Stereographic%0A%20%20Projection&entry.906535625=Taosi%20Xu%20and%20Yinlong%20Liu%20and%20Xianbo%20Wang%20and%20Zhi-Xin%20Yang&entry.1292438233=%20%20Rotation%20estimation%20plays%20a%20fundamental%20role%20in%20many%20computer%20vision%20and%0Arobot%20tasks.%20However%2C%20efficiently%20estimating%20rotation%20in%20large%20inputs%0Acontaining%20numerous%20outliers%20%28i.e.%2C%20mismatches%29%20and%20noise%20is%20a%20recognized%0Achallenge.%20Many%20robust%20rotation%20estimation%20methods%20have%20been%20designed%20to%0Aaddress%20this%20challenge.%20Unfortunately%2C%20existing%20methods%20are%20often%20inapplicable%0Adue%20to%20their%20long%20computation%20time%20and%20the%20risk%20of%20local%20optima.%20In%20this%20paper%2C%0Awe%20propose%20an%20efficient%20and%20robust%20rotation%20estimation%20method.%20Specifically%2C%0Aour%20method%20first%20investigates%20geometric%20constraints%20involving%20only%20the%20rotation%0Aaxis.%20Then%2C%20it%20uses%20stereographic%20projection%20and%20spatial%20voting%20techniques%20to%0Aidentify%20the%20rotation%20axis%20and%20angle.%20Furthermore%2C%20our%20method%20efficiently%0Aobtains%20the%20optimal%20rotation%20estimation%20and%20can%20estimate%20multiple%20rotations%0Asimultaneously.%20To%20verify%20the%20feasibility%20of%20our%20method%2C%20we%20conduct%20comparative%0Aexperiments%20using%20both%20synthetic%20and%20real-world%20data.%20The%20results%20show%20that%2C%0Awith%20GPU%20assistance%2C%20our%20method%20can%20solve%20large-scale%20%28%2410%5E6%24%20points%29%20and%0Aseverely%20corrupted%20%2890%5C%25%20outlier%20rate%29%20rotation%20estimation%20problems%20within%200.07%0Aseconds%2C%20with%20an%20angular%20error%20of%20only%200.01%20degrees%2C%20which%20is%20superior%20to%0Aexisting%20methods%20in%20terms%20of%20accuracy%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06337v1&entry.124074799=Read"},
{"title": "CHIRLA: Comprehensive High-resolution Identification and\n  Re-identification for Large-scale Analysis", "author": "Bessie Dominguez-Dager and Felix Escalona and Francisco Gomez-Donoso and Miguel Cazorla", "abstract": "  Person re-identification (Re-ID) is a key challenge in computer vision,\nrequiring the matching of individuals across different cameras, locations, and\ntime periods. While most research focuses on short-term scenarios with minimal\nappearance changes, real-world applications demand robust Re-ID systems capable\nof handling long-term scenarios, where persons' appearances can change\nsignificantly due to variations in clothing and physical characteristics. In\nthis paper, we present CHIRLA, Comprehensive High-resolution Identification and\nRe-identification for Large-scale Analysis, a novel dataset specifically\ndesigned for long-term person Re-ID. CHIRLA consists of recordings from\nstrategically placed cameras over a seven-month period, capturing significant\nvariations in both temporal and appearance attributes, including controlled\nchanges in participants' clothing and physical features. The dataset includes\n22 individuals, four connected indoor environments, and seven cameras. We\ncollected more than five hours of video that we semi-automatically labeled to\ngenerate around one million bounding boxes with identity annotations. By\nintroducing this comprehensive benchmark, we aim to facilitate the development\nand evaluation of Re-ID algorithms that can reliably perform in challenging,\nlong-term real-world scenarios.\n", "link": "http://arxiv.org/abs/2502.06681v1", "date": "2025-02-10", "relevancy": 2.5204, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5113}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5085}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHIRLA%3A%20Comprehensive%20High-resolution%20Identification%20and%0A%20%20Re-identification%20for%20Large-scale%20Analysis&body=Title%3A%20CHIRLA%3A%20Comprehensive%20High-resolution%20Identification%20and%0A%20%20Re-identification%20for%20Large-scale%20Analysis%0AAuthor%3A%20Bessie%20Dominguez-Dager%20and%20Felix%20Escalona%20and%20Francisco%20Gomez-Donoso%20and%20Miguel%20Cazorla%0AAbstract%3A%20%20%20Person%20re-identification%20%28Re-ID%29%20is%20a%20key%20challenge%20in%20computer%20vision%2C%0Arequiring%20the%20matching%20of%20individuals%20across%20different%20cameras%2C%20locations%2C%20and%0Atime%20periods.%20While%20most%20research%20focuses%20on%20short-term%20scenarios%20with%20minimal%0Aappearance%20changes%2C%20real-world%20applications%20demand%20robust%20Re-ID%20systems%20capable%0Aof%20handling%20long-term%20scenarios%2C%20where%20persons%27%20appearances%20can%20change%0Asignificantly%20due%20to%20variations%20in%20clothing%20and%20physical%20characteristics.%20In%0Athis%20paper%2C%20we%20present%20CHIRLA%2C%20Comprehensive%20High-resolution%20Identification%20and%0ARe-identification%20for%20Large-scale%20Analysis%2C%20a%20novel%20dataset%20specifically%0Adesigned%20for%20long-term%20person%20Re-ID.%20CHIRLA%20consists%20of%20recordings%20from%0Astrategically%20placed%20cameras%20over%20a%20seven-month%20period%2C%20capturing%20significant%0Avariations%20in%20both%20temporal%20and%20appearance%20attributes%2C%20including%20controlled%0Achanges%20in%20participants%27%20clothing%20and%20physical%20features.%20The%20dataset%20includes%0A22%20individuals%2C%20four%20connected%20indoor%20environments%2C%20and%20seven%20cameras.%20We%0Acollected%20more%20than%20five%20hours%20of%20video%20that%20we%20semi-automatically%20labeled%20to%0Agenerate%20around%20one%20million%20bounding%20boxes%20with%20identity%20annotations.%20By%0Aintroducing%20this%20comprehensive%20benchmark%2C%20we%20aim%20to%20facilitate%20the%20development%0Aand%20evaluation%20of%20Re-ID%20algorithms%20that%20can%20reliably%20perform%20in%20challenging%2C%0Along-term%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHIRLA%253A%2520Comprehensive%2520High-resolution%2520Identification%2520and%250A%2520%2520Re-identification%2520for%2520Large-scale%2520Analysis%26entry.906535625%3DBessie%2520Dominguez-Dager%2520and%2520Felix%2520Escalona%2520and%2520Francisco%2520Gomez-Donoso%2520and%2520Miguel%2520Cazorla%26entry.1292438233%3D%2520%2520Person%2520re-identification%2520%2528Re-ID%2529%2520is%2520a%2520key%2520challenge%2520in%2520computer%2520vision%252C%250Arequiring%2520the%2520matching%2520of%2520individuals%2520across%2520different%2520cameras%252C%2520locations%252C%2520and%250Atime%2520periods.%2520While%2520most%2520research%2520focuses%2520on%2520short-term%2520scenarios%2520with%2520minimal%250Aappearance%2520changes%252C%2520real-world%2520applications%2520demand%2520robust%2520Re-ID%2520systems%2520capable%250Aof%2520handling%2520long-term%2520scenarios%252C%2520where%2520persons%2527%2520appearances%2520can%2520change%250Asignificantly%2520due%2520to%2520variations%2520in%2520clothing%2520and%2520physical%2520characteristics.%2520In%250Athis%2520paper%252C%2520we%2520present%2520CHIRLA%252C%2520Comprehensive%2520High-resolution%2520Identification%2520and%250ARe-identification%2520for%2520Large-scale%2520Analysis%252C%2520a%2520novel%2520dataset%2520specifically%250Adesigned%2520for%2520long-term%2520person%2520Re-ID.%2520CHIRLA%2520consists%2520of%2520recordings%2520from%250Astrategically%2520placed%2520cameras%2520over%2520a%2520seven-month%2520period%252C%2520capturing%2520significant%250Avariations%2520in%2520both%2520temporal%2520and%2520appearance%2520attributes%252C%2520including%2520controlled%250Achanges%2520in%2520participants%2527%2520clothing%2520and%2520physical%2520features.%2520The%2520dataset%2520includes%250A22%2520individuals%252C%2520four%2520connected%2520indoor%2520environments%252C%2520and%2520seven%2520cameras.%2520We%250Acollected%2520more%2520than%2520five%2520hours%2520of%2520video%2520that%2520we%2520semi-automatically%2520labeled%2520to%250Agenerate%2520around%2520one%2520million%2520bounding%2520boxes%2520with%2520identity%2520annotations.%2520By%250Aintroducing%2520this%2520comprehensive%2520benchmark%252C%2520we%2520aim%2520to%2520facilitate%2520the%2520development%250Aand%2520evaluation%2520of%2520Re-ID%2520algorithms%2520that%2520can%2520reliably%2520perform%2520in%2520challenging%252C%250Along-term%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHIRLA%3A%20Comprehensive%20High-resolution%20Identification%20and%0A%20%20Re-identification%20for%20Large-scale%20Analysis&entry.906535625=Bessie%20Dominguez-Dager%20and%20Felix%20Escalona%20and%20Francisco%20Gomez-Donoso%20and%20Miguel%20Cazorla&entry.1292438233=%20%20Person%20re-identification%20%28Re-ID%29%20is%20a%20key%20challenge%20in%20computer%20vision%2C%0Arequiring%20the%20matching%20of%20individuals%20across%20different%20cameras%2C%20locations%2C%20and%0Atime%20periods.%20While%20most%20research%20focuses%20on%20short-term%20scenarios%20with%20minimal%0Aappearance%20changes%2C%20real-world%20applications%20demand%20robust%20Re-ID%20systems%20capable%0Aof%20handling%20long-term%20scenarios%2C%20where%20persons%27%20appearances%20can%20change%0Asignificantly%20due%20to%20variations%20in%20clothing%20and%20physical%20characteristics.%20In%0Athis%20paper%2C%20we%20present%20CHIRLA%2C%20Comprehensive%20High-resolution%20Identification%20and%0ARe-identification%20for%20Large-scale%20Analysis%2C%20a%20novel%20dataset%20specifically%0Adesigned%20for%20long-term%20person%20Re-ID.%20CHIRLA%20consists%20of%20recordings%20from%0Astrategically%20placed%20cameras%20over%20a%20seven-month%20period%2C%20capturing%20significant%0Avariations%20in%20both%20temporal%20and%20appearance%20attributes%2C%20including%20controlled%0Achanges%20in%20participants%27%20clothing%20and%20physical%20features.%20The%20dataset%20includes%0A22%20individuals%2C%20four%20connected%20indoor%20environments%2C%20and%20seven%20cameras.%20We%0Acollected%20more%20than%20five%20hours%20of%20video%20that%20we%20semi-automatically%20labeled%20to%0Agenerate%20around%20one%20million%20bounding%20boxes%20with%20identity%20annotations.%20By%0Aintroducing%20this%20comprehensive%20benchmark%2C%20we%20aim%20to%20facilitate%20the%20development%0Aand%20evaluation%20of%20Re-ID%20algorithms%20that%20can%20reliably%20perform%20in%20challenging%2C%0Along-term%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06681v1&entry.124074799=Read"},
{"title": "Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits", "author": "Finn Rietz and Oleg Smirnov and Sara Karimi and Lele Cao", "abstract": "  Harnessing large offline datasets is vital for training foundation models\nthat can generalize across diverse tasks. Offline Reinforcement Learning (RL)\noffers a powerful framework for these scenarios, enabling the derivation of\noptimal policies even from suboptimal data. The Prompting Decision Transformer\n(PDT) is an offline RL multi-task model that distinguishes tasks through\nstochastic trajectory prompts, which are task-specific tokens maintained in\ncontext during rollouts. However, PDT samples these tokens uniformly at random\nfrom per-task demonstration datasets, failing to account for differences in\ntoken informativeness and potentially leading to performance degradation. To\naddress this limitation, we introduce a scalable bandit-based prompt-tuning\nmethod that dynamically learns to construct high-performance trajectory\nprompts. Our approach significantly enhances downstream task performance\nwithout modifying the pre-trained Transformer backbone. Empirical results on\nbenchmark tasks and a newly designed multi-task environment demonstrate the\neffectiveness of our method, creating a seamless bridge between general\nmulti-task offline pre-training and task-specific online adaptation.\n", "link": "http://arxiv.org/abs/2502.04979v2", "date": "2025-02-10", "relevancy": 2.5019, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5116}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4956}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Pre-Trained%20Decision%20Transformers%20with%20Prompt-Tuning%20Bandits&body=Title%3A%20Enhancing%20Pre-Trained%20Decision%20Transformers%20with%20Prompt-Tuning%20Bandits%0AAuthor%3A%20Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao%0AAbstract%3A%20%20%20Harnessing%20large%20offline%20datasets%20is%20vital%20for%20training%20foundation%20models%0Athat%20can%20generalize%20across%20diverse%20tasks.%20Offline%20Reinforcement%20Learning%20%28RL%29%0Aoffers%20a%20powerful%20framework%20for%20these%20scenarios%2C%20enabling%20the%20derivation%20of%0Aoptimal%20policies%20even%20from%20suboptimal%20data.%20The%20Prompting%20Decision%20Transformer%0A%28PDT%29%20is%20an%20offline%20RL%20multi-task%20model%20that%20distinguishes%20tasks%20through%0Astochastic%20trajectory%20prompts%2C%20which%20are%20task-specific%20tokens%20maintained%20in%0Acontext%20during%20rollouts.%20However%2C%20PDT%20samples%20these%20tokens%20uniformly%20at%20random%0Afrom%20per-task%20demonstration%20datasets%2C%20failing%20to%20account%20for%20differences%20in%0Atoken%20informativeness%20and%20potentially%20leading%20to%20performance%20degradation.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20scalable%20bandit-based%20prompt-tuning%0Amethod%20that%20dynamically%20learns%20to%20construct%20high-performance%20trajectory%0Aprompts.%20Our%20approach%20significantly%20enhances%20downstream%20task%20performance%0Awithout%20modifying%20the%20pre-trained%20Transformer%20backbone.%20Empirical%20results%20on%0Abenchmark%20tasks%20and%20a%20newly%20designed%20multi-task%20environment%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%2C%20creating%20a%20seamless%20bridge%20between%20general%0Amulti-task%20offline%20pre-training%20and%20task-specific%20online%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Pre-Trained%2520Decision%2520Transformers%2520with%2520Prompt-Tuning%2520Bandits%26entry.906535625%3DFinn%2520Rietz%2520and%2520Oleg%2520Smirnov%2520and%2520Sara%2520Karimi%2520and%2520Lele%2520Cao%26entry.1292438233%3D%2520%2520Harnessing%2520large%2520offline%2520datasets%2520is%2520vital%2520for%2520training%2520foundation%2520models%250Athat%2520can%2520generalize%2520across%2520diverse%2520tasks.%2520Offline%2520Reinforcement%2520Learning%2520%2528RL%2529%250Aoffers%2520a%2520powerful%2520framework%2520for%2520these%2520scenarios%252C%2520enabling%2520the%2520derivation%2520of%250Aoptimal%2520policies%2520even%2520from%2520suboptimal%2520data.%2520The%2520Prompting%2520Decision%2520Transformer%250A%2528PDT%2529%2520is%2520an%2520offline%2520RL%2520multi-task%2520model%2520that%2520distinguishes%2520tasks%2520through%250Astochastic%2520trajectory%2520prompts%252C%2520which%2520are%2520task-specific%2520tokens%2520maintained%2520in%250Acontext%2520during%2520rollouts.%2520However%252C%2520PDT%2520samples%2520these%2520tokens%2520uniformly%2520at%2520random%250Afrom%2520per-task%2520demonstration%2520datasets%252C%2520failing%2520to%2520account%2520for%2520differences%2520in%250Atoken%2520informativeness%2520and%2520potentially%2520leading%2520to%2520performance%2520degradation.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520scalable%2520bandit-based%2520prompt-tuning%250Amethod%2520that%2520dynamically%2520learns%2520to%2520construct%2520high-performance%2520trajectory%250Aprompts.%2520Our%2520approach%2520significantly%2520enhances%2520downstream%2520task%2520performance%250Awithout%2520modifying%2520the%2520pre-trained%2520Transformer%2520backbone.%2520Empirical%2520results%2520on%250Abenchmark%2520tasks%2520and%2520a%2520newly%2520designed%2520multi-task%2520environment%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%252C%2520creating%2520a%2520seamless%2520bridge%2520between%2520general%250Amulti-task%2520offline%2520pre-training%2520and%2520task-specific%2520online%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Pre-Trained%20Decision%20Transformers%20with%20Prompt-Tuning%20Bandits&entry.906535625=Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao&entry.1292438233=%20%20Harnessing%20large%20offline%20datasets%20is%20vital%20for%20training%20foundation%20models%0Athat%20can%20generalize%20across%20diverse%20tasks.%20Offline%20Reinforcement%20Learning%20%28RL%29%0Aoffers%20a%20powerful%20framework%20for%20these%20scenarios%2C%20enabling%20the%20derivation%20of%0Aoptimal%20policies%20even%20from%20suboptimal%20data.%20The%20Prompting%20Decision%20Transformer%0A%28PDT%29%20is%20an%20offline%20RL%20multi-task%20model%20that%20distinguishes%20tasks%20through%0Astochastic%20trajectory%20prompts%2C%20which%20are%20task-specific%20tokens%20maintained%20in%0Acontext%20during%20rollouts.%20However%2C%20PDT%20samples%20these%20tokens%20uniformly%20at%20random%0Afrom%20per-task%20demonstration%20datasets%2C%20failing%20to%20account%20for%20differences%20in%0Atoken%20informativeness%20and%20potentially%20leading%20to%20performance%20degradation.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20scalable%20bandit-based%20prompt-tuning%0Amethod%20that%20dynamically%20learns%20to%20construct%20high-performance%20trajectory%0Aprompts.%20Our%20approach%20significantly%20enhances%20downstream%20task%20performance%0Awithout%20modifying%20the%20pre-trained%20Transformer%20backbone.%20Empirical%20results%20on%0Abenchmark%20tasks%20and%20a%20newly%20designed%20multi-task%20environment%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%2C%20creating%20a%20seamless%20bridge%20between%20general%0Amulti-task%20offline%20pre-training%20and%20task-specific%20online%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04979v2&entry.124074799=Read"},
{"title": "Hybrid State-Space and GRU-based Graph Tokenization Mamba for\n  Hyperspectral Image Classification", "author": "Muhammad Ahmad and Muhammad Hassaan Farooq Butt and Muhammad Usama and Manuel Mazzara and Salvatore Distefano and Adil Mehmood Khan and Danfeng Hong", "abstract": "  Hyperspectral image (HSI) classification plays a pivotal role in domains such\nas environmental monitoring, agriculture, and urban planning. However, it faces\nsignificant challenges due to the high-dimensional nature of the data and the\ncomplex spectral-spatial relationships inherent in HSI. Traditional methods,\nincluding conventional machine learning and convolutional neural networks\n(CNNs), often struggle to effectively capture these intricate spectral-spatial\nfeatures and global contextual information. Transformer-based models, while\npowerful in capturing long-range dependencies, often demand substantial\ncomputational resources, posing challenges in scenarios where labeled datasets\nare limited, as is commonly seen in HSI applications. To overcome these\nchallenges, this work proposes GraphMamba, a hybrid model that combines\nspectral-spatial token generation, graph-based token prioritization, and\ncross-attention mechanisms. The model introduces a novel hybridization of\nstate-space modeling and Gated Recurrent Units (GRU), capturing both linear and\nnonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model\ncomplex spatial-spectral relationships while maintaining scalability and\ncomputational efficiency across diverse HSI datasets. Through comprehensive\nexperiments, we demonstrate that GraphMamba outperforms existing\nstate-of-the-art models, offering a scalable and robust solution for complex\nHSI classification tasks.\n", "link": "http://arxiv.org/abs/2502.06427v1", "date": "2025-02-10", "relevancy": 2.4989, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5118}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5094}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20State-Space%20and%20GRU-based%20Graph%20Tokenization%20Mamba%20for%0A%20%20Hyperspectral%20Image%20Classification&body=Title%3A%20Hybrid%20State-Space%20and%20GRU-based%20Graph%20Tokenization%20Mamba%20for%0A%20%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Manuel%20Mazzara%20and%20Salvatore%20Distefano%20and%20Adil%20Mehmood%20Khan%20and%20Danfeng%20Hong%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20classification%20plays%20a%20pivotal%20role%20in%20domains%20such%0Aas%20environmental%20monitoring%2C%20agriculture%2C%20and%20urban%20planning.%20However%2C%20it%20faces%0Asignificant%20challenges%20due%20to%20the%20high-dimensional%20nature%20of%20the%20data%20and%20the%0Acomplex%20spectral-spatial%20relationships%20inherent%20in%20HSI.%20Traditional%20methods%2C%0Aincluding%20conventional%20machine%20learning%20and%20convolutional%20neural%20networks%0A%28CNNs%29%2C%20often%20struggle%20to%20effectively%20capture%20these%20intricate%20spectral-spatial%0Afeatures%20and%20global%20contextual%20information.%20Transformer-based%20models%2C%20while%0Apowerful%20in%20capturing%20long-range%20dependencies%2C%20often%20demand%20substantial%0Acomputational%20resources%2C%20posing%20challenges%20in%20scenarios%20where%20labeled%20datasets%0Aare%20limited%2C%20as%20is%20commonly%20seen%20in%20HSI%20applications.%20To%20overcome%20these%0Achallenges%2C%20this%20work%20proposes%20GraphMamba%2C%20a%20hybrid%20model%20that%20combines%0Aspectral-spatial%20token%20generation%2C%20graph-based%20token%20prioritization%2C%20and%0Across-attention%20mechanisms.%20The%20model%20introduces%20a%20novel%20hybridization%20of%0Astate-space%20modeling%20and%20Gated%20Recurrent%20Units%20%28GRU%29%2C%20capturing%20both%20linear%20and%0Anonlinear%20spatial-spectral%20dynamics.%20GraphMamba%20enhances%20the%20ability%20to%20model%0Acomplex%20spatial-spectral%20relationships%20while%20maintaining%20scalability%20and%0Acomputational%20efficiency%20across%20diverse%20HSI%20datasets.%20Through%20comprehensive%0Aexperiments%2C%20we%20demonstrate%20that%20GraphMamba%20outperforms%20existing%0Astate-of-the-art%20models%2C%20offering%20a%20scalable%20and%20robust%20solution%20for%20complex%0AHSI%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520State-Space%2520and%2520GRU-based%2520Graph%2520Tokenization%2520Mamba%2520for%250A%2520%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DMuhammad%2520Ahmad%2520and%2520Muhammad%2520Hassaan%2520Farooq%2520Butt%2520and%2520Muhammad%2520Usama%2520and%2520Manuel%2520Mazzara%2520and%2520Salvatore%2520Distefano%2520and%2520Adil%2520Mehmood%2520Khan%2520and%2520Danfeng%2520Hong%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520classification%2520plays%2520a%2520pivotal%2520role%2520in%2520domains%2520such%250Aas%2520environmental%2520monitoring%252C%2520agriculture%252C%2520and%2520urban%2520planning.%2520However%252C%2520it%2520faces%250Asignificant%2520challenges%2520due%2520to%2520the%2520high-dimensional%2520nature%2520of%2520the%2520data%2520and%2520the%250Acomplex%2520spectral-spatial%2520relationships%2520inherent%2520in%2520HSI.%2520Traditional%2520methods%252C%250Aincluding%2520conventional%2520machine%2520learning%2520and%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%252C%2520often%2520struggle%2520to%2520effectively%2520capture%2520these%2520intricate%2520spectral-spatial%250Afeatures%2520and%2520global%2520contextual%2520information.%2520Transformer-based%2520models%252C%2520while%250Apowerful%2520in%2520capturing%2520long-range%2520dependencies%252C%2520often%2520demand%2520substantial%250Acomputational%2520resources%252C%2520posing%2520challenges%2520in%2520scenarios%2520where%2520labeled%2520datasets%250Aare%2520limited%252C%2520as%2520is%2520commonly%2520seen%2520in%2520HSI%2520applications.%2520To%2520overcome%2520these%250Achallenges%252C%2520this%2520work%2520proposes%2520GraphMamba%252C%2520a%2520hybrid%2520model%2520that%2520combines%250Aspectral-spatial%2520token%2520generation%252C%2520graph-based%2520token%2520prioritization%252C%2520and%250Across-attention%2520mechanisms.%2520The%2520model%2520introduces%2520a%2520novel%2520hybridization%2520of%250Astate-space%2520modeling%2520and%2520Gated%2520Recurrent%2520Units%2520%2528GRU%2529%252C%2520capturing%2520both%2520linear%2520and%250Anonlinear%2520spatial-spectral%2520dynamics.%2520GraphMamba%2520enhances%2520the%2520ability%2520to%2520model%250Acomplex%2520spatial-spectral%2520relationships%2520while%2520maintaining%2520scalability%2520and%250Acomputational%2520efficiency%2520across%2520diverse%2520HSI%2520datasets.%2520Through%2520comprehensive%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520GraphMamba%2520outperforms%2520existing%250Astate-of-the-art%2520models%252C%2520offering%2520a%2520scalable%2520and%2520robust%2520solution%2520for%2520complex%250AHSI%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20State-Space%20and%20GRU-based%20Graph%20Tokenization%20Mamba%20for%0A%20%20Hyperspectral%20Image%20Classification&entry.906535625=Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Manuel%20Mazzara%20and%20Salvatore%20Distefano%20and%20Adil%20Mehmood%20Khan%20and%20Danfeng%20Hong&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20classification%20plays%20a%20pivotal%20role%20in%20domains%20such%0Aas%20environmental%20monitoring%2C%20agriculture%2C%20and%20urban%20planning.%20However%2C%20it%20faces%0Asignificant%20challenges%20due%20to%20the%20high-dimensional%20nature%20of%20the%20data%20and%20the%0Acomplex%20spectral-spatial%20relationships%20inherent%20in%20HSI.%20Traditional%20methods%2C%0Aincluding%20conventional%20machine%20learning%20and%20convolutional%20neural%20networks%0A%28CNNs%29%2C%20often%20struggle%20to%20effectively%20capture%20these%20intricate%20spectral-spatial%0Afeatures%20and%20global%20contextual%20information.%20Transformer-based%20models%2C%20while%0Apowerful%20in%20capturing%20long-range%20dependencies%2C%20often%20demand%20substantial%0Acomputational%20resources%2C%20posing%20challenges%20in%20scenarios%20where%20labeled%20datasets%0Aare%20limited%2C%20as%20is%20commonly%20seen%20in%20HSI%20applications.%20To%20overcome%20these%0Achallenges%2C%20this%20work%20proposes%20GraphMamba%2C%20a%20hybrid%20model%20that%20combines%0Aspectral-spatial%20token%20generation%2C%20graph-based%20token%20prioritization%2C%20and%0Across-attention%20mechanisms.%20The%20model%20introduces%20a%20novel%20hybridization%20of%0Astate-space%20modeling%20and%20Gated%20Recurrent%20Units%20%28GRU%29%2C%20capturing%20both%20linear%20and%0Anonlinear%20spatial-spectral%20dynamics.%20GraphMamba%20enhances%20the%20ability%20to%20model%0Acomplex%20spatial-spectral%20relationships%20while%20maintaining%20scalability%20and%0Acomputational%20efficiency%20across%20diverse%20HSI%20datasets.%20Through%20comprehensive%0Aexperiments%2C%20we%20demonstrate%20that%20GraphMamba%20outperforms%20existing%0Astate-of-the-art%20models%2C%20offering%20a%20scalable%20and%20robust%20solution%20for%20complex%0AHSI%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06427v1&entry.124074799=Read"},
{"title": "When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks\n  for VLMs", "author": "Aobotao Dai and Xinyu Ma and Lei Chen and Songze Li and Lin Wang", "abstract": "  Vision-Language Models (VLMs) have gained considerable prominence in recent\nyears due to their remarkable capability to effectively integrate and process\nboth textual and visual information. This integration has significantly\nenhanced performance across a diverse spectrum of applications, such as scene\nperception and robotics. However, the deployment of VLMs has also given rise to\ncritical safety and security concerns, necessitating extensive research to\nassess the potential vulnerabilities these VLM systems may harbor. In this\nwork, we present an in-depth survey of the attack strategies tailored for VLMs.\nWe categorize these attacks based on their underlying objectives - namely\njailbreak, camouflage, and exploitation - while also detailing the various\nmethodologies employed for data manipulation of VLMs. Meanwhile, we outline\ncorresponding defense mechanisms that have been proposed to mitigate these\nvulnerabilities. By discerning key connections and distinctions among the\ndiverse types of attacks, we propose a compelling taxonomy for VLM attacks.\nMoreover, we summarize the evaluation metrics that comprehensively describe the\ncharacteristics and impact of different attacks on VLMs. Finally, we conclude\nwith a discussion of promising future research directions that could further\nenhance the robustness and safety of VLMs, emphasizing the importance of\nongoing exploration in this critical area of study. To facilitate community\nengagement, we maintain an up-to-date project page, accessible at:\nhttps://github.com/AobtDai/VLM_Attack_Paper_List.\n", "link": "http://arxiv.org/abs/2502.06390v1", "date": "2025-02-10", "relevancy": 2.4963, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Data%20Manipulation%20Meets%20Attack%20Goals%3A%20An%20In-depth%20Survey%20of%20Attacks%0A%20%20for%20VLMs&body=Title%3A%20When%20Data%20Manipulation%20Meets%20Attack%20Goals%3A%20An%20In-depth%20Survey%20of%20Attacks%0A%20%20for%20VLMs%0AAuthor%3A%20Aobotao%20Dai%20and%20Xinyu%20Ma%20and%20Lei%20Chen%20and%20Songze%20Li%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20gained%20considerable%20prominence%20in%20recent%0Ayears%20due%20to%20their%20remarkable%20capability%20to%20effectively%20integrate%20and%20process%0Aboth%20textual%20and%20visual%20information.%20This%20integration%20has%20significantly%0Aenhanced%20performance%20across%20a%20diverse%20spectrum%20of%20applications%2C%20such%20as%20scene%0Aperception%20and%20robotics.%20However%2C%20the%20deployment%20of%20VLMs%20has%20also%20given%20rise%20to%0Acritical%20safety%20and%20security%20concerns%2C%20necessitating%20extensive%20research%20to%0Aassess%20the%20potential%20vulnerabilities%20these%20VLM%20systems%20may%20harbor.%20In%20this%0Awork%2C%20we%20present%20an%20in-depth%20survey%20of%20the%20attack%20strategies%20tailored%20for%20VLMs.%0AWe%20categorize%20these%20attacks%20based%20on%20their%20underlying%20objectives%20-%20namely%0Ajailbreak%2C%20camouflage%2C%20and%20exploitation%20-%20while%20also%20detailing%20the%20various%0Amethodologies%20employed%20for%20data%20manipulation%20of%20VLMs.%20Meanwhile%2C%20we%20outline%0Acorresponding%20defense%20mechanisms%20that%20have%20been%20proposed%20to%20mitigate%20these%0Avulnerabilities.%20By%20discerning%20key%20connections%20and%20distinctions%20among%20the%0Adiverse%20types%20of%20attacks%2C%20we%20propose%20a%20compelling%20taxonomy%20for%20VLM%20attacks.%0AMoreover%2C%20we%20summarize%20the%20evaluation%20metrics%20that%20comprehensively%20describe%20the%0Acharacteristics%20and%20impact%20of%20different%20attacks%20on%20VLMs.%20Finally%2C%20we%20conclude%0Awith%20a%20discussion%20of%20promising%20future%20research%20directions%20that%20could%20further%0Aenhance%20the%20robustness%20and%20safety%20of%20VLMs%2C%20emphasizing%20the%20importance%20of%0Aongoing%20exploration%20in%20this%20critical%20area%20of%20study.%20To%20facilitate%20community%0Aengagement%2C%20we%20maintain%20an%20up-to-date%20project%20page%2C%20accessible%20at%3A%0Ahttps%3A//github.com/AobtDai/VLM_Attack_Paper_List.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Data%2520Manipulation%2520Meets%2520Attack%2520Goals%253A%2520An%2520In-depth%2520Survey%2520of%2520Attacks%250A%2520%2520for%2520VLMs%26entry.906535625%3DAobotao%2520Dai%2520and%2520Xinyu%2520Ma%2520and%2520Lei%2520Chen%2520and%2520Songze%2520Li%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520gained%2520considerable%2520prominence%2520in%2520recent%250Ayears%2520due%2520to%2520their%2520remarkable%2520capability%2520to%2520effectively%2520integrate%2520and%2520process%250Aboth%2520textual%2520and%2520visual%2520information.%2520This%2520integration%2520has%2520significantly%250Aenhanced%2520performance%2520across%2520a%2520diverse%2520spectrum%2520of%2520applications%252C%2520such%2520as%2520scene%250Aperception%2520and%2520robotics.%2520However%252C%2520the%2520deployment%2520of%2520VLMs%2520has%2520also%2520given%2520rise%2520to%250Acritical%2520safety%2520and%2520security%2520concerns%252C%2520necessitating%2520extensive%2520research%2520to%250Aassess%2520the%2520potential%2520vulnerabilities%2520these%2520VLM%2520systems%2520may%2520harbor.%2520In%2520this%250Awork%252C%2520we%2520present%2520an%2520in-depth%2520survey%2520of%2520the%2520attack%2520strategies%2520tailored%2520for%2520VLMs.%250AWe%2520categorize%2520these%2520attacks%2520based%2520on%2520their%2520underlying%2520objectives%2520-%2520namely%250Ajailbreak%252C%2520camouflage%252C%2520and%2520exploitation%2520-%2520while%2520also%2520detailing%2520the%2520various%250Amethodologies%2520employed%2520for%2520data%2520manipulation%2520of%2520VLMs.%2520Meanwhile%252C%2520we%2520outline%250Acorresponding%2520defense%2520mechanisms%2520that%2520have%2520been%2520proposed%2520to%2520mitigate%2520these%250Avulnerabilities.%2520By%2520discerning%2520key%2520connections%2520and%2520distinctions%2520among%2520the%250Adiverse%2520types%2520of%2520attacks%252C%2520we%2520propose%2520a%2520compelling%2520taxonomy%2520for%2520VLM%2520attacks.%250AMoreover%252C%2520we%2520summarize%2520the%2520evaluation%2520metrics%2520that%2520comprehensively%2520describe%2520the%250Acharacteristics%2520and%2520impact%2520of%2520different%2520attacks%2520on%2520VLMs.%2520Finally%252C%2520we%2520conclude%250Awith%2520a%2520discussion%2520of%2520promising%2520future%2520research%2520directions%2520that%2520could%2520further%250Aenhance%2520the%2520robustness%2520and%2520safety%2520of%2520VLMs%252C%2520emphasizing%2520the%2520importance%2520of%250Aongoing%2520exploration%2520in%2520this%2520critical%2520area%2520of%2520study.%2520To%2520facilitate%2520community%250Aengagement%252C%2520we%2520maintain%2520an%2520up-to-date%2520project%2520page%252C%2520accessible%2520at%253A%250Ahttps%253A//github.com/AobtDai/VLM_Attack_Paper_List.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Data%20Manipulation%20Meets%20Attack%20Goals%3A%20An%20In-depth%20Survey%20of%20Attacks%0A%20%20for%20VLMs&entry.906535625=Aobotao%20Dai%20and%20Xinyu%20Ma%20and%20Lei%20Chen%20and%20Songze%20Li%20and%20Lin%20Wang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20gained%20considerable%20prominence%20in%20recent%0Ayears%20due%20to%20their%20remarkable%20capability%20to%20effectively%20integrate%20and%20process%0Aboth%20textual%20and%20visual%20information.%20This%20integration%20has%20significantly%0Aenhanced%20performance%20across%20a%20diverse%20spectrum%20of%20applications%2C%20such%20as%20scene%0Aperception%20and%20robotics.%20However%2C%20the%20deployment%20of%20VLMs%20has%20also%20given%20rise%20to%0Acritical%20safety%20and%20security%20concerns%2C%20necessitating%20extensive%20research%20to%0Aassess%20the%20potential%20vulnerabilities%20these%20VLM%20systems%20may%20harbor.%20In%20this%0Awork%2C%20we%20present%20an%20in-depth%20survey%20of%20the%20attack%20strategies%20tailored%20for%20VLMs.%0AWe%20categorize%20these%20attacks%20based%20on%20their%20underlying%20objectives%20-%20namely%0Ajailbreak%2C%20camouflage%2C%20and%20exploitation%20-%20while%20also%20detailing%20the%20various%0Amethodologies%20employed%20for%20data%20manipulation%20of%20VLMs.%20Meanwhile%2C%20we%20outline%0Acorresponding%20defense%20mechanisms%20that%20have%20been%20proposed%20to%20mitigate%20these%0Avulnerabilities.%20By%20discerning%20key%20connections%20and%20distinctions%20among%20the%0Adiverse%20types%20of%20attacks%2C%20we%20propose%20a%20compelling%20taxonomy%20for%20VLM%20attacks.%0AMoreover%2C%20we%20summarize%20the%20evaluation%20metrics%20that%20comprehensively%20describe%20the%0Acharacteristics%20and%20impact%20of%20different%20attacks%20on%20VLMs.%20Finally%2C%20we%20conclude%0Awith%20a%20discussion%20of%20promising%20future%20research%20directions%20that%20could%20further%0Aenhance%20the%20robustness%20and%20safety%20of%20VLMs%2C%20emphasizing%20the%20importance%20of%0Aongoing%20exploration%20in%20this%20critical%20area%20of%20study.%20To%20facilitate%20community%0Aengagement%2C%20we%20maintain%20an%20up-to-date%20project%20page%2C%20accessible%20at%3A%0Ahttps%3A//github.com/AobtDai/VLM_Attack_Paper_List.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06390v1&entry.124074799=Read"},
{"title": "Recent Advances in Discrete Speech Tokens: A Review", "author": "Yiwei Guo and Zhihan Li and Hankun Wang and Bohan Li and Chongtian Shao and Hanglei Zhang and Chenpeng Du and Xie Chen and Shujie Liu and Kai Yu", "abstract": "  The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.\n", "link": "http://arxiv.org/abs/2502.06490v1", "date": "2025-02-10", "relevancy": 2.4911, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20Discrete%20Speech%20Tokens%3A%20A%20Review&body=Title%3A%20Recent%20Advances%20in%20Discrete%20Speech%20Tokens%3A%20A%20Review%0AAuthor%3A%20Yiwei%20Guo%20and%20Zhihan%20Li%20and%20Hankun%20Wang%20and%20Bohan%20Li%20and%20Chongtian%20Shao%20and%20Hanglei%20Zhang%20and%20Chenpeng%20Du%20and%20Xie%20Chen%20and%20Shujie%20Liu%20and%20Kai%20Yu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20speech%20generation%20technologies%20in%20the%20era%20of%20large%0Alanguage%20models%20%28LLMs%29%20has%20established%20discrete%20speech%20tokens%20as%20a%20foundational%0Aparadigm%20for%20speech%20representation.%20These%20tokens%2C%20characterized%20by%20their%0Adiscrete%2C%20compact%2C%20and%20concise%20nature%2C%20are%20not%20only%20advantageous%20for%20efficient%0Atransmission%20and%20storage%2C%20but%20also%20inherently%20compatible%20with%20the%20language%0Amodeling%20framework%2C%20enabling%20seamless%20integration%20of%20speech%20into%20text-dominated%0ALLM%20architectures.%20Current%20research%20categorizes%20discrete%20speech%20tokens%20into%20two%0Aprincipal%20classes%3A%20acoustic%20tokens%20and%20semantic%20tokens%2C%20each%20of%20which%20has%0Aevolved%20into%20a%20rich%20research%20domain%20characterized%20by%20unique%20design%20philosophies%0Aand%20methodological%20approaches.%20This%20survey%20systematically%20synthesizes%20the%0Aexisting%20taxonomy%20and%20recent%20innovations%20in%20discrete%20speech%20tokenization%2C%0Aconducts%20a%20critical%20examination%20of%20the%20strengths%20and%20limitations%20of%20each%0Aparadigm%2C%20and%20presents%20systematic%20experimental%20comparisons%20across%20token%20types.%0AFurthermore%2C%20we%20identify%20persistent%20challenges%20in%20the%20field%20and%20propose%0Apotential%20research%20directions%2C%20aiming%20to%20offer%20actionable%20insights%20to%20inspire%0Afuture%20advancements%20in%20the%20development%20and%20application%20of%20discrete%20speech%0Atokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Advances%2520in%2520Discrete%2520Speech%2520Tokens%253A%2520A%2520Review%26entry.906535625%3DYiwei%2520Guo%2520and%2520Zhihan%2520Li%2520and%2520Hankun%2520Wang%2520and%2520Bohan%2520Li%2520and%2520Chongtian%2520Shao%2520and%2520Hanglei%2520Zhang%2520and%2520Chenpeng%2520Du%2520and%2520Xie%2520Chen%2520and%2520Shujie%2520Liu%2520and%2520Kai%2520Yu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520speech%2520generation%2520technologies%2520in%2520the%2520era%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520has%2520established%2520discrete%2520speech%2520tokens%2520as%2520a%2520foundational%250Aparadigm%2520for%2520speech%2520representation.%2520These%2520tokens%252C%2520characterized%2520by%2520their%250Adiscrete%252C%2520compact%252C%2520and%2520concise%2520nature%252C%2520are%2520not%2520only%2520advantageous%2520for%2520efficient%250Atransmission%2520and%2520storage%252C%2520but%2520also%2520inherently%2520compatible%2520with%2520the%2520language%250Amodeling%2520framework%252C%2520enabling%2520seamless%2520integration%2520of%2520speech%2520into%2520text-dominated%250ALLM%2520architectures.%2520Current%2520research%2520categorizes%2520discrete%2520speech%2520tokens%2520into%2520two%250Aprincipal%2520classes%253A%2520acoustic%2520tokens%2520and%2520semantic%2520tokens%252C%2520each%2520of%2520which%2520has%250Aevolved%2520into%2520a%2520rich%2520research%2520domain%2520characterized%2520by%2520unique%2520design%2520philosophies%250Aand%2520methodological%2520approaches.%2520This%2520survey%2520systematically%2520synthesizes%2520the%250Aexisting%2520taxonomy%2520and%2520recent%2520innovations%2520in%2520discrete%2520speech%2520tokenization%252C%250Aconducts%2520a%2520critical%2520examination%2520of%2520the%2520strengths%2520and%2520limitations%2520of%2520each%250Aparadigm%252C%2520and%2520presents%2520systematic%2520experimental%2520comparisons%2520across%2520token%2520types.%250AFurthermore%252C%2520we%2520identify%2520persistent%2520challenges%2520in%2520the%2520field%2520and%2520propose%250Apotential%2520research%2520directions%252C%2520aiming%2520to%2520offer%2520actionable%2520insights%2520to%2520inspire%250Afuture%2520advancements%2520in%2520the%2520development%2520and%2520application%2520of%2520discrete%2520speech%250Atokens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20Discrete%20Speech%20Tokens%3A%20A%20Review&entry.906535625=Yiwei%20Guo%20and%20Zhihan%20Li%20and%20Hankun%20Wang%20and%20Bohan%20Li%20and%20Chongtian%20Shao%20and%20Hanglei%20Zhang%20and%20Chenpeng%20Du%20and%20Xie%20Chen%20and%20Shujie%20Liu%20and%20Kai%20Yu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20speech%20generation%20technologies%20in%20the%20era%20of%20large%0Alanguage%20models%20%28LLMs%29%20has%20established%20discrete%20speech%20tokens%20as%20a%20foundational%0Aparadigm%20for%20speech%20representation.%20These%20tokens%2C%20characterized%20by%20their%0Adiscrete%2C%20compact%2C%20and%20concise%20nature%2C%20are%20not%20only%20advantageous%20for%20efficient%0Atransmission%20and%20storage%2C%20but%20also%20inherently%20compatible%20with%20the%20language%0Amodeling%20framework%2C%20enabling%20seamless%20integration%20of%20speech%20into%20text-dominated%0ALLM%20architectures.%20Current%20research%20categorizes%20discrete%20speech%20tokens%20into%20two%0Aprincipal%20classes%3A%20acoustic%20tokens%20and%20semantic%20tokens%2C%20each%20of%20which%20has%0Aevolved%20into%20a%20rich%20research%20domain%20characterized%20by%20unique%20design%20philosophies%0Aand%20methodological%20approaches.%20This%20survey%20systematically%20synthesizes%20the%0Aexisting%20taxonomy%20and%20recent%20innovations%20in%20discrete%20speech%20tokenization%2C%0Aconducts%20a%20critical%20examination%20of%20the%20strengths%20and%20limitations%20of%20each%0Aparadigm%2C%20and%20presents%20systematic%20experimental%20comparisons%20across%20token%20types.%0AFurthermore%2C%20we%20identify%20persistent%20challenges%20in%20the%20field%20and%20propose%0Apotential%20research%20directions%2C%20aiming%20to%20offer%20actionable%20insights%20to%20inspire%0Afuture%20advancements%20in%20the%20development%20and%20application%20of%20discrete%20speech%0Atokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06490v1&entry.124074799=Read"},
{"title": "Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment\n  and Averaging", "author": "Ron Shapira Weber and Oren Freifeld", "abstract": "  In time-series analysis, nonlinear temporal misalignment remains a pivotal\nchallenge that forestalls even simple averaging. Since its introduction, the\nDiffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber\net al., 2019) and further developed in (Weber & Freifeld, 2023), has proven\nitself as an effective solution for this problem (these conference papers are\nearlier partial versions of the current manuscript). DTAN predicts and applies\ndiffeomorphic transformations in an input-dependent manner, thus facilitating\nthe joint alignment (JA) and averaging of time-series ensembles in an\nunsupervised or a weakly-supervised manner. The inherent challenges of the\nweakly/unsupervised setting, particularly the risk of trivial solutions through\nexcessive signal distortion, are mitigated using either one of two distinct\nstrategies: 1) a regularization term for warps; 2) using the Inverse\nConsistency Averaging Error (ICAE). The latter is a novel, regularization-free\napproach which also facilitates the JA of variable-length signals. We also\nfurther extend our framework to incorporate multi-task learning (MT-DTAN),\nenabling simultaneous time-series alignment and classification. Additionally,\nwe conduct a comprehensive evaluation of different backbone architectures,\ndemonstrating their efficacy in time-series alignment tasks. Finally, we\nshowcase the utility of our approach in enabling Principal Component Analysis\n(PCA) for misaligned time-series data. Extensive experiments across 128 UCR\ndatasets validate the superiority of our approach over contemporary averaging\nmethods, including both traditional and learning-based approaches, marking a\nsignificant advancement in the field of time-series analysis.\n", "link": "http://arxiv.org/abs/2502.06591v1", "date": "2025-02-10", "relevancy": 2.491, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4921}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffeomorphic%20Temporal%20Alignment%20Nets%20for%20Time-series%20Joint%20Alignment%0A%20%20and%20Averaging&body=Title%3A%20Diffeomorphic%20Temporal%20Alignment%20Nets%20for%20Time-series%20Joint%20Alignment%0A%20%20and%20Averaging%0AAuthor%3A%20Ron%20Shapira%20Weber%20and%20Oren%20Freifeld%0AAbstract%3A%20%20%20In%20time-series%20analysis%2C%20nonlinear%20temporal%20misalignment%20remains%20a%20pivotal%0Achallenge%20that%20forestalls%20even%20simple%20averaging.%20Since%20its%20introduction%2C%20the%0ADiffeomorphic%20Temporal%20Alignment%20Net%20%28DTAN%29%2C%20which%20we%20first%20introduced%20%28Weber%0Aet%20al.%2C%202019%29%20and%20further%20developed%20in%20%28Weber%20%26%20Freifeld%2C%202023%29%2C%20has%20proven%0Aitself%20as%20an%20effective%20solution%20for%20this%20problem%20%28these%20conference%20papers%20are%0Aearlier%20partial%20versions%20of%20the%20current%20manuscript%29.%20DTAN%20predicts%20and%20applies%0Adiffeomorphic%20transformations%20in%20an%20input-dependent%20manner%2C%20thus%20facilitating%0Athe%20joint%20alignment%20%28JA%29%20and%20averaging%20of%20time-series%20ensembles%20in%20an%0Aunsupervised%20or%20a%20weakly-supervised%20manner.%20The%20inherent%20challenges%20of%20the%0Aweakly/unsupervised%20setting%2C%20particularly%20the%20risk%20of%20trivial%20solutions%20through%0Aexcessive%20signal%20distortion%2C%20are%20mitigated%20using%20either%20one%20of%20two%20distinct%0Astrategies%3A%201%29%20a%20regularization%20term%20for%20warps%3B%202%29%20using%20the%20Inverse%0AConsistency%20Averaging%20Error%20%28ICAE%29.%20The%20latter%20is%20a%20novel%2C%20regularization-free%0Aapproach%20which%20also%20facilitates%20the%20JA%20of%20variable-length%20signals.%20We%20also%0Afurther%20extend%20our%20framework%20to%20incorporate%20multi-task%20learning%20%28MT-DTAN%29%2C%0Aenabling%20simultaneous%20time-series%20alignment%20and%20classification.%20Additionally%2C%0Awe%20conduct%20a%20comprehensive%20evaluation%20of%20different%20backbone%20architectures%2C%0Ademonstrating%20their%20efficacy%20in%20time-series%20alignment%20tasks.%20Finally%2C%20we%0Ashowcase%20the%20utility%20of%20our%20approach%20in%20enabling%20Principal%20Component%20Analysis%0A%28PCA%29%20for%20misaligned%20time-series%20data.%20Extensive%20experiments%20across%20128%20UCR%0Adatasets%20validate%20the%20superiority%20of%20our%20approach%20over%20contemporary%20averaging%0Amethods%2C%20including%20both%20traditional%20and%20learning-based%20approaches%2C%20marking%20a%0Asignificant%20advancement%20in%20the%20field%20of%20time-series%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffeomorphic%2520Temporal%2520Alignment%2520Nets%2520for%2520Time-series%2520Joint%2520Alignment%250A%2520%2520and%2520Averaging%26entry.906535625%3DRon%2520Shapira%2520Weber%2520and%2520Oren%2520Freifeld%26entry.1292438233%3D%2520%2520In%2520time-series%2520analysis%252C%2520nonlinear%2520temporal%2520misalignment%2520remains%2520a%2520pivotal%250Achallenge%2520that%2520forestalls%2520even%2520simple%2520averaging.%2520Since%2520its%2520introduction%252C%2520the%250ADiffeomorphic%2520Temporal%2520Alignment%2520Net%2520%2528DTAN%2529%252C%2520which%2520we%2520first%2520introduced%2520%2528Weber%250Aet%2520al.%252C%25202019%2529%2520and%2520further%2520developed%2520in%2520%2528Weber%2520%2526%2520Freifeld%252C%25202023%2529%252C%2520has%2520proven%250Aitself%2520as%2520an%2520effective%2520solution%2520for%2520this%2520problem%2520%2528these%2520conference%2520papers%2520are%250Aearlier%2520partial%2520versions%2520of%2520the%2520current%2520manuscript%2529.%2520DTAN%2520predicts%2520and%2520applies%250Adiffeomorphic%2520transformations%2520in%2520an%2520input-dependent%2520manner%252C%2520thus%2520facilitating%250Athe%2520joint%2520alignment%2520%2528JA%2529%2520and%2520averaging%2520of%2520time-series%2520ensembles%2520in%2520an%250Aunsupervised%2520or%2520a%2520weakly-supervised%2520manner.%2520The%2520inherent%2520challenges%2520of%2520the%250Aweakly/unsupervised%2520setting%252C%2520particularly%2520the%2520risk%2520of%2520trivial%2520solutions%2520through%250Aexcessive%2520signal%2520distortion%252C%2520are%2520mitigated%2520using%2520either%2520one%2520of%2520two%2520distinct%250Astrategies%253A%25201%2529%2520a%2520regularization%2520term%2520for%2520warps%253B%25202%2529%2520using%2520the%2520Inverse%250AConsistency%2520Averaging%2520Error%2520%2528ICAE%2529.%2520The%2520latter%2520is%2520a%2520novel%252C%2520regularization-free%250Aapproach%2520which%2520also%2520facilitates%2520the%2520JA%2520of%2520variable-length%2520signals.%2520We%2520also%250Afurther%2520extend%2520our%2520framework%2520to%2520incorporate%2520multi-task%2520learning%2520%2528MT-DTAN%2529%252C%250Aenabling%2520simultaneous%2520time-series%2520alignment%2520and%2520classification.%2520Additionally%252C%250Awe%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520different%2520backbone%2520architectures%252C%250Ademonstrating%2520their%2520efficacy%2520in%2520time-series%2520alignment%2520tasks.%2520Finally%252C%2520we%250Ashowcase%2520the%2520utility%2520of%2520our%2520approach%2520in%2520enabling%2520Principal%2520Component%2520Analysis%250A%2528PCA%2529%2520for%2520misaligned%2520time-series%2520data.%2520Extensive%2520experiments%2520across%2520128%2520UCR%250Adatasets%2520validate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520contemporary%2520averaging%250Amethods%252C%2520including%2520both%2520traditional%2520and%2520learning-based%2520approaches%252C%2520marking%2520a%250Asignificant%2520advancement%2520in%2520the%2520field%2520of%2520time-series%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffeomorphic%20Temporal%20Alignment%20Nets%20for%20Time-series%20Joint%20Alignment%0A%20%20and%20Averaging&entry.906535625=Ron%20Shapira%20Weber%20and%20Oren%20Freifeld&entry.1292438233=%20%20In%20time-series%20analysis%2C%20nonlinear%20temporal%20misalignment%20remains%20a%20pivotal%0Achallenge%20that%20forestalls%20even%20simple%20averaging.%20Since%20its%20introduction%2C%20the%0ADiffeomorphic%20Temporal%20Alignment%20Net%20%28DTAN%29%2C%20which%20we%20first%20introduced%20%28Weber%0Aet%20al.%2C%202019%29%20and%20further%20developed%20in%20%28Weber%20%26%20Freifeld%2C%202023%29%2C%20has%20proven%0Aitself%20as%20an%20effective%20solution%20for%20this%20problem%20%28these%20conference%20papers%20are%0Aearlier%20partial%20versions%20of%20the%20current%20manuscript%29.%20DTAN%20predicts%20and%20applies%0Adiffeomorphic%20transformations%20in%20an%20input-dependent%20manner%2C%20thus%20facilitating%0Athe%20joint%20alignment%20%28JA%29%20and%20averaging%20of%20time-series%20ensembles%20in%20an%0Aunsupervised%20or%20a%20weakly-supervised%20manner.%20The%20inherent%20challenges%20of%20the%0Aweakly/unsupervised%20setting%2C%20particularly%20the%20risk%20of%20trivial%20solutions%20through%0Aexcessive%20signal%20distortion%2C%20are%20mitigated%20using%20either%20one%20of%20two%20distinct%0Astrategies%3A%201%29%20a%20regularization%20term%20for%20warps%3B%202%29%20using%20the%20Inverse%0AConsistency%20Averaging%20Error%20%28ICAE%29.%20The%20latter%20is%20a%20novel%2C%20regularization-free%0Aapproach%20which%20also%20facilitates%20the%20JA%20of%20variable-length%20signals.%20We%20also%0Afurther%20extend%20our%20framework%20to%20incorporate%20multi-task%20learning%20%28MT-DTAN%29%2C%0Aenabling%20simultaneous%20time-series%20alignment%20and%20classification.%20Additionally%2C%0Awe%20conduct%20a%20comprehensive%20evaluation%20of%20different%20backbone%20architectures%2C%0Ademonstrating%20their%20efficacy%20in%20time-series%20alignment%20tasks.%20Finally%2C%20we%0Ashowcase%20the%20utility%20of%20our%20approach%20in%20enabling%20Principal%20Component%20Analysis%0A%28PCA%29%20for%20misaligned%20time-series%20data.%20Extensive%20experiments%20across%20128%20UCR%0Adatasets%20validate%20the%20superiority%20of%20our%20approach%20over%20contemporary%20averaging%0Amethods%2C%20including%20both%20traditional%20and%20learning-based%20approaches%2C%20marking%20a%0Asignificant%20advancement%20in%20the%20field%20of%20time-series%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06591v1&entry.124074799=Read"},
{"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for\n  Architecture-Agnostic Edge Language Models", "author": "Xingrun Xing and Zheng Liu and Shitao Xiao and Boyan Gao and Yiming Liang and Wanpeng Zhang and Haokun Lin and Guoqi Li and Jiajun Zhang", "abstract": "  Modern large language models (LLMs) driven by scaling laws, achieve\nintelligence emergency in large model sizes. Recently, the increasing concerns\nabout cloud costs, latency, and privacy make it an urgent requirement to\ndevelop compact edge language models. Distinguished from direct pretraining\nthat bounded by the scaling law, this work proposes the pruning-aware\npretraining, focusing on retaining performance of much larger optimized models.\nIt features following characteristics: 1) Data-scalable: we introduce minimal\nparameter groups in LLM and continuously optimize structural pruning, extending\npost-training pruning methods like LLM-Pruner and SparseGPT into the\npretraining phase. 2) Architecture-agnostic: the LLM architecture is\nauto-designed using saliency-driven pruning, which is the first time to exceed\nSoTA human-designed LLMs in modern pretraining. We reveal that it achieves\ntop-quality edge language models, termed EfficientLLM, by scaling up LLM\ncompression and extending its boundary. EfficientLLM significantly outperforms\nSoTA baselines with $100M \\sim 1B$ parameters, such as MobileLLM, SmolLM,\nQwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first\nattempt, EfficientLLM bridges the performance gap between traditional LLM\ncompression and direct pretraining methods, and we will fully open source at\nhttps://github.com/Xingrun-Xing2/EfficientLLM.\n", "link": "http://arxiv.org/abs/2502.06663v1", "date": "2025-02-10", "relevancy": 2.4827, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5026}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EfficientLLM%3A%20Scalable%20Pruning-Aware%20Pretraining%20for%0A%20%20Architecture-Agnostic%20Edge%20Language%20Models&body=Title%3A%20EfficientLLM%3A%20Scalable%20Pruning-Aware%20Pretraining%20for%0A%20%20Architecture-Agnostic%20Edge%20Language%20Models%0AAuthor%3A%20Xingrun%20Xing%20and%20Zheng%20Liu%20and%20Shitao%20Xiao%20and%20Boyan%20Gao%20and%20Yiming%20Liang%20and%20Wanpeng%20Zhang%20and%20Haokun%20Lin%20and%20Guoqi%20Li%20and%20Jiajun%20Zhang%0AAbstract%3A%20%20%20Modern%20large%20language%20models%20%28LLMs%29%20driven%20by%20scaling%20laws%2C%20achieve%0Aintelligence%20emergency%20in%20large%20model%20sizes.%20Recently%2C%20the%20increasing%20concerns%0Aabout%20cloud%20costs%2C%20latency%2C%20and%20privacy%20make%20it%20an%20urgent%20requirement%20to%0Adevelop%20compact%20edge%20language%20models.%20Distinguished%20from%20direct%20pretraining%0Athat%20bounded%20by%20the%20scaling%20law%2C%20this%20work%20proposes%20the%20pruning-aware%0Apretraining%2C%20focusing%20on%20retaining%20performance%20of%20much%20larger%20optimized%20models.%0AIt%20features%20following%20characteristics%3A%201%29%20Data-scalable%3A%20we%20introduce%20minimal%0Aparameter%20groups%20in%20LLM%20and%20continuously%20optimize%20structural%20pruning%2C%20extending%0Apost-training%20pruning%20methods%20like%20LLM-Pruner%20and%20SparseGPT%20into%20the%0Apretraining%20phase.%202%29%20Architecture-agnostic%3A%20the%20LLM%20architecture%20is%0Aauto-designed%20using%20saliency-driven%20pruning%2C%20which%20is%20the%20first%20time%20to%20exceed%0ASoTA%20human-designed%20LLMs%20in%20modern%20pretraining.%20We%20reveal%20that%20it%20achieves%0Atop-quality%20edge%20language%20models%2C%20termed%20EfficientLLM%2C%20by%20scaling%20up%20LLM%0Acompression%20and%20extending%20its%20boundary.%20EfficientLLM%20significantly%20outperforms%0ASoTA%20baselines%20with%20%24100M%20%5Csim%201B%24%20parameters%2C%20such%20as%20MobileLLM%2C%20SmolLM%2C%0AQwen2.5-0.5B%2C%20OLMo-1B%2C%20Llama3.2-1B%20in%20common%20sense%20benchmarks.%20As%20the%20first%0Aattempt%2C%20EfficientLLM%20bridges%20the%20performance%20gap%20between%20traditional%20LLM%0Acompression%20and%20direct%20pretraining%20methods%2C%20and%20we%20will%20fully%20open%20source%20at%0Ahttps%3A//github.com/Xingrun-Xing2/EfficientLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficientLLM%253A%2520Scalable%2520Pruning-Aware%2520Pretraining%2520for%250A%2520%2520Architecture-Agnostic%2520Edge%2520Language%2520Models%26entry.906535625%3DXingrun%2520Xing%2520and%2520Zheng%2520Liu%2520and%2520Shitao%2520Xiao%2520and%2520Boyan%2520Gao%2520and%2520Yiming%2520Liang%2520and%2520Wanpeng%2520Zhang%2520and%2520Haokun%2520Lin%2520and%2520Guoqi%2520Li%2520and%2520Jiajun%2520Zhang%26entry.1292438233%3D%2520%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%2520driven%2520by%2520scaling%2520laws%252C%2520achieve%250Aintelligence%2520emergency%2520in%2520large%2520model%2520sizes.%2520Recently%252C%2520the%2520increasing%2520concerns%250Aabout%2520cloud%2520costs%252C%2520latency%252C%2520and%2520privacy%2520make%2520it%2520an%2520urgent%2520requirement%2520to%250Adevelop%2520compact%2520edge%2520language%2520models.%2520Distinguished%2520from%2520direct%2520pretraining%250Athat%2520bounded%2520by%2520the%2520scaling%2520law%252C%2520this%2520work%2520proposes%2520the%2520pruning-aware%250Apretraining%252C%2520focusing%2520on%2520retaining%2520performance%2520of%2520much%2520larger%2520optimized%2520models.%250AIt%2520features%2520following%2520characteristics%253A%25201%2529%2520Data-scalable%253A%2520we%2520introduce%2520minimal%250Aparameter%2520groups%2520in%2520LLM%2520and%2520continuously%2520optimize%2520structural%2520pruning%252C%2520extending%250Apost-training%2520pruning%2520methods%2520like%2520LLM-Pruner%2520and%2520SparseGPT%2520into%2520the%250Apretraining%2520phase.%25202%2529%2520Architecture-agnostic%253A%2520the%2520LLM%2520architecture%2520is%250Aauto-designed%2520using%2520saliency-driven%2520pruning%252C%2520which%2520is%2520the%2520first%2520time%2520to%2520exceed%250ASoTA%2520human-designed%2520LLMs%2520in%2520modern%2520pretraining.%2520We%2520reveal%2520that%2520it%2520achieves%250Atop-quality%2520edge%2520language%2520models%252C%2520termed%2520EfficientLLM%252C%2520by%2520scaling%2520up%2520LLM%250Acompression%2520and%2520extending%2520its%2520boundary.%2520EfficientLLM%2520significantly%2520outperforms%250ASoTA%2520baselines%2520with%2520%2524100M%2520%255Csim%25201B%2524%2520parameters%252C%2520such%2520as%2520MobileLLM%252C%2520SmolLM%252C%250AQwen2.5-0.5B%252C%2520OLMo-1B%252C%2520Llama3.2-1B%2520in%2520common%2520sense%2520benchmarks.%2520As%2520the%2520first%250Aattempt%252C%2520EfficientLLM%2520bridges%2520the%2520performance%2520gap%2520between%2520traditional%2520LLM%250Acompression%2520and%2520direct%2520pretraining%2520methods%252C%2520and%2520we%2520will%2520fully%2520open%2520source%2520at%250Ahttps%253A//github.com/Xingrun-Xing2/EfficientLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EfficientLLM%3A%20Scalable%20Pruning-Aware%20Pretraining%20for%0A%20%20Architecture-Agnostic%20Edge%20Language%20Models&entry.906535625=Xingrun%20Xing%20and%20Zheng%20Liu%20and%20Shitao%20Xiao%20and%20Boyan%20Gao%20and%20Yiming%20Liang%20and%20Wanpeng%20Zhang%20and%20Haokun%20Lin%20and%20Guoqi%20Li%20and%20Jiajun%20Zhang&entry.1292438233=%20%20Modern%20large%20language%20models%20%28LLMs%29%20driven%20by%20scaling%20laws%2C%20achieve%0Aintelligence%20emergency%20in%20large%20model%20sizes.%20Recently%2C%20the%20increasing%20concerns%0Aabout%20cloud%20costs%2C%20latency%2C%20and%20privacy%20make%20it%20an%20urgent%20requirement%20to%0Adevelop%20compact%20edge%20language%20models.%20Distinguished%20from%20direct%20pretraining%0Athat%20bounded%20by%20the%20scaling%20law%2C%20this%20work%20proposes%20the%20pruning-aware%0Apretraining%2C%20focusing%20on%20retaining%20performance%20of%20much%20larger%20optimized%20models.%0AIt%20features%20following%20characteristics%3A%201%29%20Data-scalable%3A%20we%20introduce%20minimal%0Aparameter%20groups%20in%20LLM%20and%20continuously%20optimize%20structural%20pruning%2C%20extending%0Apost-training%20pruning%20methods%20like%20LLM-Pruner%20and%20SparseGPT%20into%20the%0Apretraining%20phase.%202%29%20Architecture-agnostic%3A%20the%20LLM%20architecture%20is%0Aauto-designed%20using%20saliency-driven%20pruning%2C%20which%20is%20the%20first%20time%20to%20exceed%0ASoTA%20human-designed%20LLMs%20in%20modern%20pretraining.%20We%20reveal%20that%20it%20achieves%0Atop-quality%20edge%20language%20models%2C%20termed%20EfficientLLM%2C%20by%20scaling%20up%20LLM%0Acompression%20and%20extending%20its%20boundary.%20EfficientLLM%20significantly%20outperforms%0ASoTA%20baselines%20with%20%24100M%20%5Csim%201B%24%20parameters%2C%20such%20as%20MobileLLM%2C%20SmolLM%2C%0AQwen2.5-0.5B%2C%20OLMo-1B%2C%20Llama3.2-1B%20in%20common%20sense%20benchmarks.%20As%20the%20first%0Aattempt%2C%20EfficientLLM%20bridges%20the%20performance%20gap%20between%20traditional%20LLM%0Acompression%20and%20direct%20pretraining%20methods%2C%20and%20we%20will%20fully%20open%20source%20at%0Ahttps%3A//github.com/Xingrun-Xing2/EfficientLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06663v1&entry.124074799=Read"},
{"title": "Multi-label Scandinavian Language Identification (SLIDE)", "author": "Mariia Fedorova and Jonas Sebulon Frydenberg and Victoria Handford and Victoria Ovedie Chruickshank Lang\u00f8 and Solveig Helene Willoch and Marthe L\u00f8ken Midtgaard and Yves Scherrer and Petter M\u00e6hlum and David Samuel", "abstract": "  Identifying closely related languages at sentence level is difficult, in\nparticular because it is often impossible to assign a sentence to a single\nlanguage. In this paper, we focus on multi-label sentence-level Scandinavian\nlanguage identification (LID) for Danish, Norwegian Bokm\\r{a}l, Norwegian\nNynorsk, and Swedish. We present the Scandinavian Language Identification and\nEvaluation, SLIDE, a manually curated multi-label evaluation dataset and a\nsuite of LID models with varying speed-accuracy tradeoffs. We demonstrate that\nthe ability to identify multiple languages simultaneously is necessary for any\naccurate LID method, and present a novel approach to training such multi-label\nLID models.\n", "link": "http://arxiv.org/abs/2502.06692v1", "date": "2025-02-10", "relevancy": 2.4686, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-label%20Scandinavian%20Language%20Identification%20%28SLIDE%29&body=Title%3A%20Multi-label%20Scandinavian%20Language%20Identification%20%28SLIDE%29%0AAuthor%3A%20Mariia%20Fedorova%20and%20Jonas%20Sebulon%20Frydenberg%20and%20Victoria%20Handford%20and%20Victoria%20Ovedie%20Chruickshank%20Lang%C3%B8%20and%20Solveig%20Helene%20Willoch%20and%20Marthe%20L%C3%B8ken%20Midtgaard%20and%20Yves%20Scherrer%20and%20Petter%20M%C3%A6hlum%20and%20David%20Samuel%0AAbstract%3A%20%20%20Identifying%20closely%20related%20languages%20at%20sentence%20level%20is%20difficult%2C%20in%0Aparticular%20because%20it%20is%20often%20impossible%20to%20assign%20a%20sentence%20to%20a%20single%0Alanguage.%20In%20this%20paper%2C%20we%20focus%20on%20multi-label%20sentence-level%20Scandinavian%0Alanguage%20identification%20%28LID%29%20for%20Danish%2C%20Norwegian%20Bokm%5Cr%7Ba%7Dl%2C%20Norwegian%0ANynorsk%2C%20and%20Swedish.%20We%20present%20the%20Scandinavian%20Language%20Identification%20and%0AEvaluation%2C%20SLIDE%2C%20a%20manually%20curated%20multi-label%20evaluation%20dataset%20and%20a%0Asuite%20of%20LID%20models%20with%20varying%20speed-accuracy%20tradeoffs.%20We%20demonstrate%20that%0Athe%20ability%20to%20identify%20multiple%20languages%20simultaneously%20is%20necessary%20for%20any%0Aaccurate%20LID%20method%2C%20and%20present%20a%20novel%20approach%20to%20training%20such%20multi-label%0ALID%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-label%2520Scandinavian%2520Language%2520Identification%2520%2528SLIDE%2529%26entry.906535625%3DMariia%2520Fedorova%2520and%2520Jonas%2520Sebulon%2520Frydenberg%2520and%2520Victoria%2520Handford%2520and%2520Victoria%2520Ovedie%2520Chruickshank%2520Lang%25C3%25B8%2520and%2520Solveig%2520Helene%2520Willoch%2520and%2520Marthe%2520L%25C3%25B8ken%2520Midtgaard%2520and%2520Yves%2520Scherrer%2520and%2520Petter%2520M%25C3%25A6hlum%2520and%2520David%2520Samuel%26entry.1292438233%3D%2520%2520Identifying%2520closely%2520related%2520languages%2520at%2520sentence%2520level%2520is%2520difficult%252C%2520in%250Aparticular%2520because%2520it%2520is%2520often%2520impossible%2520to%2520assign%2520a%2520sentence%2520to%2520a%2520single%250Alanguage.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520multi-label%2520sentence-level%2520Scandinavian%250Alanguage%2520identification%2520%2528LID%2529%2520for%2520Danish%252C%2520Norwegian%2520Bokm%255Cr%257Ba%257Dl%252C%2520Norwegian%250ANynorsk%252C%2520and%2520Swedish.%2520We%2520present%2520the%2520Scandinavian%2520Language%2520Identification%2520and%250AEvaluation%252C%2520SLIDE%252C%2520a%2520manually%2520curated%2520multi-label%2520evaluation%2520dataset%2520and%2520a%250Asuite%2520of%2520LID%2520models%2520with%2520varying%2520speed-accuracy%2520tradeoffs.%2520We%2520demonstrate%2520that%250Athe%2520ability%2520to%2520identify%2520multiple%2520languages%2520simultaneously%2520is%2520necessary%2520for%2520any%250Aaccurate%2520LID%2520method%252C%2520and%2520present%2520a%2520novel%2520approach%2520to%2520training%2520such%2520multi-label%250ALID%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-label%20Scandinavian%20Language%20Identification%20%28SLIDE%29&entry.906535625=Mariia%20Fedorova%20and%20Jonas%20Sebulon%20Frydenberg%20and%20Victoria%20Handford%20and%20Victoria%20Ovedie%20Chruickshank%20Lang%C3%B8%20and%20Solveig%20Helene%20Willoch%20and%20Marthe%20L%C3%B8ken%20Midtgaard%20and%20Yves%20Scherrer%20and%20Petter%20M%C3%A6hlum%20and%20David%20Samuel&entry.1292438233=%20%20Identifying%20closely%20related%20languages%20at%20sentence%20level%20is%20difficult%2C%20in%0Aparticular%20because%20it%20is%20often%20impossible%20to%20assign%20a%20sentence%20to%20a%20single%0Alanguage.%20In%20this%20paper%2C%20we%20focus%20on%20multi-label%20sentence-level%20Scandinavian%0Alanguage%20identification%20%28LID%29%20for%20Danish%2C%20Norwegian%20Bokm%5Cr%7Ba%7Dl%2C%20Norwegian%0ANynorsk%2C%20and%20Swedish.%20We%20present%20the%20Scandinavian%20Language%20Identification%20and%0AEvaluation%2C%20SLIDE%2C%20a%20manually%20curated%20multi-label%20evaluation%20dataset%20and%20a%0Asuite%20of%20LID%20models%20with%20varying%20speed-accuracy%20tradeoffs.%20We%20demonstrate%20that%0Athe%20ability%20to%20identify%20multiple%20languages%20simultaneously%20is%20necessary%20for%20any%0Aaccurate%20LID%20method%2C%20and%20present%20a%20novel%20approach%20to%20training%20such%20multi-label%0ALID%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06692v1&entry.124074799=Read"},
{"title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning", "author": "Jean Vassoyan and Nathana\u00ebl Beau and Roman Plaud", "abstract": "  The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage.\n", "link": "http://arxiv.org/abs/2502.06533v1", "date": "2025-02-10", "relevancy": 2.4671, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ignore%20the%20KL%20Penalty%21%20Boosting%20Exploration%20on%20Critical%20Tokens%20to%0A%20%20Enhance%20RL%20Fine-Tuning&body=Title%3A%20Ignore%20the%20KL%20Penalty%21%20Boosting%20Exploration%20on%20Critical%20Tokens%20to%0A%20%20Enhance%20RL%20Fine-Tuning%0AAuthor%3A%20Jean%20Vassoyan%20and%20Nathana%C3%ABl%20Beau%20and%20Roman%20Plaud%0AAbstract%3A%20%20%20The%20ability%20to%20achieve%20long-term%20goals%20is%20a%20key%20challenge%20in%20the%20current%0Adevelopment%20of%20large%20language%20models%20%28LLMs%29.%20To%20address%20this%2C%20pre-trained%20LLMs%0Acan%20be%20fine-tuned%20with%20reinforcement%20learning%20%28RL%29%20to%20explore%20solutions%20that%0Aoptimize%20a%20given%20goal.%20However%2C%20exploration%20with%20LLMs%20is%20difficult%2C%20as%20a%0Abalance%20has%20to%20be%20struck%20between%20discovering%20new%20solutions%20and%20staying%20close%0Aenough%20to%20the%20pre-trained%20model%2C%20so%20as%20not%20to%20degrade%20basic%20capabilities.%20This%0Ais%20typically%20controlled%20with%20a%20Kullback-Leibler%20%28KL%29%20penalty.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20exploration%20dynamics%20of%20a%20small%20language%20model%20on%20a%20simple%0Aarithmetic%20task.%20We%20show%20how%20varying%20degrees%20of%20pre-training%20influence%0Aexploration%20and%20demonstrate%20the%20importance%20of%20%22critical%20tokens%22%20which%20have%20a%0Adramatic%20impact%20on%20the%20final%20outcome.%20Consequently%2C%20we%20introduce%20a%20simple%0Amodification%20to%20the%20KL%20penalty%20that%20favors%20exploration%20on%20critical%20tokens%2C%0Aincreasing%20the%20efficiency%20of%20the%20RL%20fine-tuning%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIgnore%2520the%2520KL%2520Penalty%2521%2520Boosting%2520Exploration%2520on%2520Critical%2520Tokens%2520to%250A%2520%2520Enhance%2520RL%2520Fine-Tuning%26entry.906535625%3DJean%2520Vassoyan%2520and%2520Nathana%25C3%25ABl%2520Beau%2520and%2520Roman%2520Plaud%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520achieve%2520long-term%2520goals%2520is%2520a%2520key%2520challenge%2520in%2520the%2520current%250Adevelopment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520To%2520address%2520this%252C%2520pre-trained%2520LLMs%250Acan%2520be%2520fine-tuned%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520explore%2520solutions%2520that%250Aoptimize%2520a%2520given%2520goal.%2520However%252C%2520exploration%2520with%2520LLMs%2520is%2520difficult%252C%2520as%2520a%250Abalance%2520has%2520to%2520be%2520struck%2520between%2520discovering%2520new%2520solutions%2520and%2520staying%2520close%250Aenough%2520to%2520the%2520pre-trained%2520model%252C%2520so%2520as%2520not%2520to%2520degrade%2520basic%2520capabilities.%2520This%250Ais%2520typically%2520controlled%2520with%2520a%2520Kullback-Leibler%2520%2528KL%2529%2520penalty.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520the%2520exploration%2520dynamics%2520of%2520a%2520small%2520language%2520model%2520on%2520a%2520simple%250Aarithmetic%2520task.%2520We%2520show%2520how%2520varying%2520degrees%2520of%2520pre-training%2520influence%250Aexploration%2520and%2520demonstrate%2520the%2520importance%2520of%2520%2522critical%2520tokens%2522%2520which%2520have%2520a%250Adramatic%2520impact%2520on%2520the%2520final%2520outcome.%2520Consequently%252C%2520we%2520introduce%2520a%2520simple%250Amodification%2520to%2520the%2520KL%2520penalty%2520that%2520favors%2520exploration%2520on%2520critical%2520tokens%252C%250Aincreasing%2520the%2520efficiency%2520of%2520the%2520RL%2520fine-tuning%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ignore%20the%20KL%20Penalty%21%20Boosting%20Exploration%20on%20Critical%20Tokens%20to%0A%20%20Enhance%20RL%20Fine-Tuning&entry.906535625=Jean%20Vassoyan%20and%20Nathana%C3%ABl%20Beau%20and%20Roman%20Plaud&entry.1292438233=%20%20The%20ability%20to%20achieve%20long-term%20goals%20is%20a%20key%20challenge%20in%20the%20current%0Adevelopment%20of%20large%20language%20models%20%28LLMs%29.%20To%20address%20this%2C%20pre-trained%20LLMs%0Acan%20be%20fine-tuned%20with%20reinforcement%20learning%20%28RL%29%20to%20explore%20solutions%20that%0Aoptimize%20a%20given%20goal.%20However%2C%20exploration%20with%20LLMs%20is%20difficult%2C%20as%20a%0Abalance%20has%20to%20be%20struck%20between%20discovering%20new%20solutions%20and%20staying%20close%0Aenough%20to%20the%20pre-trained%20model%2C%20so%20as%20not%20to%20degrade%20basic%20capabilities.%20This%0Ais%20typically%20controlled%20with%20a%20Kullback-Leibler%20%28KL%29%20penalty.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20exploration%20dynamics%20of%20a%20small%20language%20model%20on%20a%20simple%0Aarithmetic%20task.%20We%20show%20how%20varying%20degrees%20of%20pre-training%20influence%0Aexploration%20and%20demonstrate%20the%20importance%20of%20%22critical%20tokens%22%20which%20have%20a%0Adramatic%20impact%20on%20the%20final%20outcome.%20Consequently%2C%20we%20introduce%20a%20simple%0Amodification%20to%20the%20KL%20penalty%20that%20favors%20exploration%20on%20critical%20tokens%2C%0Aincreasing%20the%20efficiency%20of%20the%20RL%20fine-tuning%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06533v1&entry.124074799=Read"},
{"title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models", "author": "Yangguang Li and Zi-Xin Zou and Zexiang Liu and Dehu Wang and Yuan Liang and Zhipeng Yu and Xingchao Liu and Yuan-Chen Guo and Ding Liang and Wanli Ouyang and Yan-Pei Cao", "abstract": "  Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprece- dented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data process- ing, and insufficient exploration of\nadvanced tech- niques in the 3D domain. Current approaches to 3D shape\ngeneration face substantial challenges in terms of output quality,\ngeneralization capa- bility, and alignment with input conditions. We present\nTripoSG, a new streamlined shape diffu- sion paradigm capable of generating\nhigh-fidelity 3D meshes with precise correspondence to input images.\nSpecifically, we propose: 1) A large-scale rectified flow transformer for 3D\nshape generation, achieving state-of-the-art fidelity through training on\nextensive, high-quality data. 2) A hybrid supervised training strategy\ncombining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality\n3D reconstruction performance. 3) A data processing pipeline to generate 2\nmillion high- quality 3D samples, highlighting the crucial rules for data\nquality and quantity in training 3D gen- erative models. Through comprehensive\nexperi- ments, we have validated the effectiveness of each component in our new\nframework. The seamless integration of these parts has enabled TripoSG to\nachieve state-of-the-art performance in 3D shape generation. The resulting 3D\nshapes exhibit en- hanced detail due to high-resolution capabilities and\ndemonstrate exceptional fidelity to input im- ages. Moreover, TripoSG\ndemonstrates improved versatility in generating 3D models from diverse image\nstyles and contents, showcasing strong gen- eralization capabilities. To foster\nprogress and innovation in the field of 3D generation, we will make our model\npublicly available.\n", "link": "http://arxiv.org/abs/2502.06608v1", "date": "2025-02-10", "relevancy": 2.465, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6454}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6127}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TripoSG%3A%20High-Fidelity%203D%20Shape%20Synthesis%20using%20Large-Scale%20Rectified%0A%20%20Flow%20Models&body=Title%3A%20TripoSG%3A%20High-Fidelity%203D%20Shape%20Synthesis%20using%20Large-Scale%20Rectified%0A%20%20Flow%20Models%0AAuthor%3A%20Yangguang%20Li%20and%20Zi-Xin%20Zou%20and%20Zexiang%20Liu%20and%20Dehu%20Wang%20and%20Yuan%20Liang%20and%20Zhipeng%20Yu%20and%20Xingchao%20Liu%20and%20Yuan-Chen%20Guo%20and%20Ding%20Liang%20and%20Wanli%20Ouyang%20and%20Yan-Pei%20Cao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20diffusion%20techniques%20have%20propelled%20image%20and%20video%0Ageneration%20to%20unprece-%20dented%20levels%20of%20quality%2C%20significantly%20accelerating%20the%0Adeployment%20and%20application%20of%20generative%20AI.%20However%2C%203D%20shape%20generation%0Atechnology%20has%20so%20far%20lagged%20behind%2C%20constrained%20by%20limitations%20in%203D%20data%0Ascale%2C%20complexity%20of%203D%20data%20process-%20ing%2C%20and%20insufficient%20exploration%20of%0Aadvanced%20tech-%20niques%20in%20the%203D%20domain.%20Current%20approaches%20to%203D%20shape%0Ageneration%20face%20substantial%20challenges%20in%20terms%20of%20output%20quality%2C%0Ageneralization%20capa-%20bility%2C%20and%20alignment%20with%20input%20conditions.%20We%20present%0ATripoSG%2C%20a%20new%20streamlined%20shape%20diffu-%20sion%20paradigm%20capable%20of%20generating%0Ahigh-fidelity%203D%20meshes%20with%20precise%20correspondence%20to%20input%20images.%0ASpecifically%2C%20we%20propose%3A%201%29%20A%20large-scale%20rectified%20flow%20transformer%20for%203D%0Ashape%20generation%2C%20achieving%20state-of-the-art%20fidelity%20through%20training%20on%0Aextensive%2C%20high-quality%20data.%202%29%20A%20hybrid%20supervised%20training%20strategy%0Acombining%20SDF%2C%20normal%2C%20and%20eikonal%20losses%20for%203D%20VAE%2C%20achieving%20high-%20quality%0A3D%20reconstruction%20performance.%203%29%20A%20data%20processing%20pipeline%20to%20generate%202%0Amillion%20high-%20quality%203D%20samples%2C%20highlighting%20the%20crucial%20rules%20for%20data%0Aquality%20and%20quantity%20in%20training%203D%20gen-%20erative%20models.%20Through%20comprehensive%0Aexperi-%20ments%2C%20we%20have%20validated%20the%20effectiveness%20of%20each%20component%20in%20our%20new%0Aframework.%20The%20seamless%20integration%20of%20these%20parts%20has%20enabled%20TripoSG%20to%0Aachieve%20state-of-the-art%20performance%20in%203D%20shape%20generation.%20The%20resulting%203D%0Ashapes%20exhibit%20en-%20hanced%20detail%20due%20to%20high-resolution%20capabilities%20and%0Ademonstrate%20exceptional%20fidelity%20to%20input%20im-%20ages.%20Moreover%2C%20TripoSG%0Ademonstrates%20improved%20versatility%20in%20generating%203D%20models%20from%20diverse%20image%0Astyles%20and%20contents%2C%20showcasing%20strong%20gen-%20eralization%20capabilities.%20To%20foster%0Aprogress%20and%20innovation%20in%20the%20field%20of%203D%20generation%2C%20we%20will%20make%20our%20model%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTripoSG%253A%2520High-Fidelity%25203D%2520Shape%2520Synthesis%2520using%2520Large-Scale%2520Rectified%250A%2520%2520Flow%2520Models%26entry.906535625%3DYangguang%2520Li%2520and%2520Zi-Xin%2520Zou%2520and%2520Zexiang%2520Liu%2520and%2520Dehu%2520Wang%2520and%2520Yuan%2520Liang%2520and%2520Zhipeng%2520Yu%2520and%2520Xingchao%2520Liu%2520and%2520Yuan-Chen%2520Guo%2520and%2520Ding%2520Liang%2520and%2520Wanli%2520Ouyang%2520and%2520Yan-Pei%2520Cao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520diffusion%2520techniques%2520have%2520propelled%2520image%2520and%2520video%250Ageneration%2520to%2520unprece-%2520dented%2520levels%2520of%2520quality%252C%2520significantly%2520accelerating%2520the%250Adeployment%2520and%2520application%2520of%2520generative%2520AI.%2520However%252C%25203D%2520shape%2520generation%250Atechnology%2520has%2520so%2520far%2520lagged%2520behind%252C%2520constrained%2520by%2520limitations%2520in%25203D%2520data%250Ascale%252C%2520complexity%2520of%25203D%2520data%2520process-%2520ing%252C%2520and%2520insufficient%2520exploration%2520of%250Aadvanced%2520tech-%2520niques%2520in%2520the%25203D%2520domain.%2520Current%2520approaches%2520to%25203D%2520shape%250Ageneration%2520face%2520substantial%2520challenges%2520in%2520terms%2520of%2520output%2520quality%252C%250Ageneralization%2520capa-%2520bility%252C%2520and%2520alignment%2520with%2520input%2520conditions.%2520We%2520present%250ATripoSG%252C%2520a%2520new%2520streamlined%2520shape%2520diffu-%2520sion%2520paradigm%2520capable%2520of%2520generating%250Ahigh-fidelity%25203D%2520meshes%2520with%2520precise%2520correspondence%2520to%2520input%2520images.%250ASpecifically%252C%2520we%2520propose%253A%25201%2529%2520A%2520large-scale%2520rectified%2520flow%2520transformer%2520for%25203D%250Ashape%2520generation%252C%2520achieving%2520state-of-the-art%2520fidelity%2520through%2520training%2520on%250Aextensive%252C%2520high-quality%2520data.%25202%2529%2520A%2520hybrid%2520supervised%2520training%2520strategy%250Acombining%2520SDF%252C%2520normal%252C%2520and%2520eikonal%2520losses%2520for%25203D%2520VAE%252C%2520achieving%2520high-%2520quality%250A3D%2520reconstruction%2520performance.%25203%2529%2520A%2520data%2520processing%2520pipeline%2520to%2520generate%25202%250Amillion%2520high-%2520quality%25203D%2520samples%252C%2520highlighting%2520the%2520crucial%2520rules%2520for%2520data%250Aquality%2520and%2520quantity%2520in%2520training%25203D%2520gen-%2520erative%2520models.%2520Through%2520comprehensive%250Aexperi-%2520ments%252C%2520we%2520have%2520validated%2520the%2520effectiveness%2520of%2520each%2520component%2520in%2520our%2520new%250Aframework.%2520The%2520seamless%2520integration%2520of%2520these%2520parts%2520has%2520enabled%2520TripoSG%2520to%250Aachieve%2520state-of-the-art%2520performance%2520in%25203D%2520shape%2520generation.%2520The%2520resulting%25203D%250Ashapes%2520exhibit%2520en-%2520hanced%2520detail%2520due%2520to%2520high-resolution%2520capabilities%2520and%250Ademonstrate%2520exceptional%2520fidelity%2520to%2520input%2520im-%2520ages.%2520Moreover%252C%2520TripoSG%250Ademonstrates%2520improved%2520versatility%2520in%2520generating%25203D%2520models%2520from%2520diverse%2520image%250Astyles%2520and%2520contents%252C%2520showcasing%2520strong%2520gen-%2520eralization%2520capabilities.%2520To%2520foster%250Aprogress%2520and%2520innovation%2520in%2520the%2520field%2520of%25203D%2520generation%252C%2520we%2520will%2520make%2520our%2520model%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TripoSG%3A%20High-Fidelity%203D%20Shape%20Synthesis%20using%20Large-Scale%20Rectified%0A%20%20Flow%20Models&entry.906535625=Yangguang%20Li%20and%20Zi-Xin%20Zou%20and%20Zexiang%20Liu%20and%20Dehu%20Wang%20and%20Yuan%20Liang%20and%20Zhipeng%20Yu%20and%20Xingchao%20Liu%20and%20Yuan-Chen%20Guo%20and%20Ding%20Liang%20and%20Wanli%20Ouyang%20and%20Yan-Pei%20Cao&entry.1292438233=%20%20Recent%20advancements%20in%20diffusion%20techniques%20have%20propelled%20image%20and%20video%0Ageneration%20to%20unprece-%20dented%20levels%20of%20quality%2C%20significantly%20accelerating%20the%0Adeployment%20and%20application%20of%20generative%20AI.%20However%2C%203D%20shape%20generation%0Atechnology%20has%20so%20far%20lagged%20behind%2C%20constrained%20by%20limitations%20in%203D%20data%0Ascale%2C%20complexity%20of%203D%20data%20process-%20ing%2C%20and%20insufficient%20exploration%20of%0Aadvanced%20tech-%20niques%20in%20the%203D%20domain.%20Current%20approaches%20to%203D%20shape%0Ageneration%20face%20substantial%20challenges%20in%20terms%20of%20output%20quality%2C%0Ageneralization%20capa-%20bility%2C%20and%20alignment%20with%20input%20conditions.%20We%20present%0ATripoSG%2C%20a%20new%20streamlined%20shape%20diffu-%20sion%20paradigm%20capable%20of%20generating%0Ahigh-fidelity%203D%20meshes%20with%20precise%20correspondence%20to%20input%20images.%0ASpecifically%2C%20we%20propose%3A%201%29%20A%20large-scale%20rectified%20flow%20transformer%20for%203D%0Ashape%20generation%2C%20achieving%20state-of-the-art%20fidelity%20through%20training%20on%0Aextensive%2C%20high-quality%20data.%202%29%20A%20hybrid%20supervised%20training%20strategy%0Acombining%20SDF%2C%20normal%2C%20and%20eikonal%20losses%20for%203D%20VAE%2C%20achieving%20high-%20quality%0A3D%20reconstruction%20performance.%203%29%20A%20data%20processing%20pipeline%20to%20generate%202%0Amillion%20high-%20quality%203D%20samples%2C%20highlighting%20the%20crucial%20rules%20for%20data%0Aquality%20and%20quantity%20in%20training%203D%20gen-%20erative%20models.%20Through%20comprehensive%0Aexperi-%20ments%2C%20we%20have%20validated%20the%20effectiveness%20of%20each%20component%20in%20our%20new%0Aframework.%20The%20seamless%20integration%20of%20these%20parts%20has%20enabled%20TripoSG%20to%0Aachieve%20state-of-the-art%20performance%20in%203D%20shape%20generation.%20The%20resulting%203D%0Ashapes%20exhibit%20en-%20hanced%20detail%20due%20to%20high-resolution%20capabilities%20and%0Ademonstrate%20exceptional%20fidelity%20to%20input%20im-%20ages.%20Moreover%2C%20TripoSG%0Ademonstrates%20improved%20versatility%20in%20generating%203D%20models%20from%20diverse%20image%0Astyles%20and%20contents%2C%20showcasing%20strong%20gen-%20eralization%20capabilities.%20To%20foster%0Aprogress%20and%20innovation%20in%20the%20field%20of%203D%20generation%2C%20we%20will%20make%20our%20model%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06608v1&entry.124074799=Read"},
{"title": "Robust Watermarks Leak: Channel-Aware Feature Extraction Enables\n  Adversarial Watermark Manipulation", "author": "Zhongjie Ba and Yitao Zhang and Peng Cheng and Bin Gong and Xinyu Zhang and Qinglong Wang and Kui Ren", "abstract": "  Watermarking plays a key role in the provenance and detection of AI-generated\ncontent. While existing methods prioritize robustness against real-world\ndistortions (e.g., JPEG compression and noise addition), we reveal a\nfundamental tradeoff: such robust watermarks inherently improve the redundancy\nof detectable patterns encoded into images, creating exploitable information\nleakage. To leverage this, we propose an attack framework that extracts leakage\nof watermark patterns through multi-channel feature learning using a\npre-trained vision model. Unlike prior works requiring massive data or detector\naccess, our method achieves both forgery and detection evasion with a single\nwatermarked image. Extensive experiments demonstrate that our method achieves a\n60\\% success rate gain in detection evasion and 51\\% improvement in forgery\naccuracy compared to state-of-the-art methods while maintaining visual\nfidelity. Our work exposes the robustness-stealthiness paradox: current\n\"robust\" watermarks sacrifice security for distortion resistance, providing\ninsights for future watermark design.\n", "link": "http://arxiv.org/abs/2502.06418v1", "date": "2025-02-10", "relevancy": 2.4508, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4984}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4887}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Watermarks%20Leak%3A%20Channel-Aware%20Feature%20Extraction%20Enables%0A%20%20Adversarial%20Watermark%20Manipulation&body=Title%3A%20Robust%20Watermarks%20Leak%3A%20Channel-Aware%20Feature%20Extraction%20Enables%0A%20%20Adversarial%20Watermark%20Manipulation%0AAuthor%3A%20Zhongjie%20Ba%20and%20Yitao%20Zhang%20and%20Peng%20Cheng%20and%20Bin%20Gong%20and%20Xinyu%20Zhang%20and%20Qinglong%20Wang%20and%20Kui%20Ren%0AAbstract%3A%20%20%20Watermarking%20plays%20a%20key%20role%20in%20the%20provenance%20and%20detection%20of%20AI-generated%0Acontent.%20While%20existing%20methods%20prioritize%20robustness%20against%20real-world%0Adistortions%20%28e.g.%2C%20JPEG%20compression%20and%20noise%20addition%29%2C%20we%20reveal%20a%0Afundamental%20tradeoff%3A%20such%20robust%20watermarks%20inherently%20improve%20the%20redundancy%0Aof%20detectable%20patterns%20encoded%20into%20images%2C%20creating%20exploitable%20information%0Aleakage.%20To%20leverage%20this%2C%20we%20propose%20an%20attack%20framework%20that%20extracts%20leakage%0Aof%20watermark%20patterns%20through%20multi-channel%20feature%20learning%20using%20a%0Apre-trained%20vision%20model.%20Unlike%20prior%20works%20requiring%20massive%20data%20or%20detector%0Aaccess%2C%20our%20method%20achieves%20both%20forgery%20and%20detection%20evasion%20with%20a%20single%0Awatermarked%20image.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20a%0A60%5C%25%20success%20rate%20gain%20in%20detection%20evasion%20and%2051%5C%25%20improvement%20in%20forgery%0Aaccuracy%20compared%20to%20state-of-the-art%20methods%20while%20maintaining%20visual%0Afidelity.%20Our%20work%20exposes%20the%20robustness-stealthiness%20paradox%3A%20current%0A%22robust%22%20watermarks%20sacrifice%20security%20for%20distortion%20resistance%2C%20providing%0Ainsights%20for%20future%20watermark%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Watermarks%2520Leak%253A%2520Channel-Aware%2520Feature%2520Extraction%2520Enables%250A%2520%2520Adversarial%2520Watermark%2520Manipulation%26entry.906535625%3DZhongjie%2520Ba%2520and%2520Yitao%2520Zhang%2520and%2520Peng%2520Cheng%2520and%2520Bin%2520Gong%2520and%2520Xinyu%2520Zhang%2520and%2520Qinglong%2520Wang%2520and%2520Kui%2520Ren%26entry.1292438233%3D%2520%2520Watermarking%2520plays%2520a%2520key%2520role%2520in%2520the%2520provenance%2520and%2520detection%2520of%2520AI-generated%250Acontent.%2520While%2520existing%2520methods%2520prioritize%2520robustness%2520against%2520real-world%250Adistortions%2520%2528e.g.%252C%2520JPEG%2520compression%2520and%2520noise%2520addition%2529%252C%2520we%2520reveal%2520a%250Afundamental%2520tradeoff%253A%2520such%2520robust%2520watermarks%2520inherently%2520improve%2520the%2520redundancy%250Aof%2520detectable%2520patterns%2520encoded%2520into%2520images%252C%2520creating%2520exploitable%2520information%250Aleakage.%2520To%2520leverage%2520this%252C%2520we%2520propose%2520an%2520attack%2520framework%2520that%2520extracts%2520leakage%250Aof%2520watermark%2520patterns%2520through%2520multi-channel%2520feature%2520learning%2520using%2520a%250Apre-trained%2520vision%2520model.%2520Unlike%2520prior%2520works%2520requiring%2520massive%2520data%2520or%2520detector%250Aaccess%252C%2520our%2520method%2520achieves%2520both%2520forgery%2520and%2520detection%2520evasion%2520with%2520a%2520single%250Awatermarked%2520image.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520a%250A60%255C%2525%2520success%2520rate%2520gain%2520in%2520detection%2520evasion%2520and%252051%255C%2525%2520improvement%2520in%2520forgery%250Aaccuracy%2520compared%2520to%2520state-of-the-art%2520methods%2520while%2520maintaining%2520visual%250Afidelity.%2520Our%2520work%2520exposes%2520the%2520robustness-stealthiness%2520paradox%253A%2520current%250A%2522robust%2522%2520watermarks%2520sacrifice%2520security%2520for%2520distortion%2520resistance%252C%2520providing%250Ainsights%2520for%2520future%2520watermark%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Watermarks%20Leak%3A%20Channel-Aware%20Feature%20Extraction%20Enables%0A%20%20Adversarial%20Watermark%20Manipulation&entry.906535625=Zhongjie%20Ba%20and%20Yitao%20Zhang%20and%20Peng%20Cheng%20and%20Bin%20Gong%20and%20Xinyu%20Zhang%20and%20Qinglong%20Wang%20and%20Kui%20Ren&entry.1292438233=%20%20Watermarking%20plays%20a%20key%20role%20in%20the%20provenance%20and%20detection%20of%20AI-generated%0Acontent.%20While%20existing%20methods%20prioritize%20robustness%20against%20real-world%0Adistortions%20%28e.g.%2C%20JPEG%20compression%20and%20noise%20addition%29%2C%20we%20reveal%20a%0Afundamental%20tradeoff%3A%20such%20robust%20watermarks%20inherently%20improve%20the%20redundancy%0Aof%20detectable%20patterns%20encoded%20into%20images%2C%20creating%20exploitable%20information%0Aleakage.%20To%20leverage%20this%2C%20we%20propose%20an%20attack%20framework%20that%20extracts%20leakage%0Aof%20watermark%20patterns%20through%20multi-channel%20feature%20learning%20using%20a%0Apre-trained%20vision%20model.%20Unlike%20prior%20works%20requiring%20massive%20data%20or%20detector%0Aaccess%2C%20our%20method%20achieves%20both%20forgery%20and%20detection%20evasion%20with%20a%20single%0Awatermarked%20image.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20a%0A60%5C%25%20success%20rate%20gain%20in%20detection%20evasion%20and%2051%5C%25%20improvement%20in%20forgery%0Aaccuracy%20compared%20to%20state-of-the-art%20methods%20while%20maintaining%20visual%0Afidelity.%20Our%20work%20exposes%20the%20robustness-stealthiness%20paradox%3A%20current%0A%22robust%22%20watermarks%20sacrifice%20security%20for%20distortion%20resistance%2C%20providing%0Ainsights%20for%20future%20watermark%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06418v1&entry.124074799=Read"},
{"title": "A Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for\n  Text-to-SQL", "author": "Yingqi Gao and Yifu Liu and Xiaoxia Li and Xiaorong Shi and Yin Zhu and Yiming Wang and Shiqi Li and Wei Li and Yuntao Hong and Zhiling Luo and Jinyang Gao and Liyu Mou and Yu Li", "abstract": "  To tackle the challenges of large language model performance in natural\nlanguage to SQL tasks, we introduce XiYan-SQL, an innovative framework that\nemploys a multi-generator ensemble strategy to improve candidate generation. We\nintroduce M-Schema, a semi-structured schema representation method designed to\nenhance the understanding of database structures. To enhance the quality and\ndiversity of generated candidate SQL queries, XiYan-SQL integrates the\nsignificant potential of in-context learning (ICL) with the precise control of\nsupervised fine-tuning. On one hand, we propose a series of training strategies\nto fine-tune models to generate high-quality candidates with diverse\npreferences. On the other hand, we implement the ICL approach with an example\nselection method based on named entity recognition to prevent overemphasis on\nentities. The refiner optimizes each candidate by correcting logical or\nsyntactical errors. To address the challenge of identifying the best candidate,\nwe fine-tune a selection model to distinguish nuances of candidate SQL queries.\nThe experimental results on multiple dialect datasets demonstrate the\nrobustness of XiYan-SQL in addressing challenges across different scenarios.\nOverall, our proposed XiYan-SQL achieves the state-of-the-art execution\naccuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on\nSQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the\nquality and diversity of SQL queries but also outperforms previous methods.\n", "link": "http://arxiv.org/abs/2411.08599v3", "date": "2025-02-10", "relevancy": 2.4456, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Preview%20of%20XiYan-SQL%3A%20A%20Multi-Generator%20Ensemble%20Framework%20for%0A%20%20Text-to-SQL&body=Title%3A%20A%20Preview%20of%20XiYan-SQL%3A%20A%20Multi-Generator%20Ensemble%20Framework%20for%0A%20%20Text-to-SQL%0AAuthor%3A%20Yingqi%20Gao%20and%20Yifu%20Liu%20and%20Xiaoxia%20Li%20and%20Xiaorong%20Shi%20and%20Yin%20Zhu%20and%20Yiming%20Wang%20and%20Shiqi%20Li%20and%20Wei%20Li%20and%20Yuntao%20Hong%20and%20Zhiling%20Luo%20and%20Jinyang%20Gao%20and%20Liyu%20Mou%20and%20Yu%20Li%0AAbstract%3A%20%20%20To%20tackle%20the%20challenges%20of%20large%20language%20model%20performance%20in%20natural%0Alanguage%20to%20SQL%20tasks%2C%20we%20introduce%20XiYan-SQL%2C%20an%20innovative%20framework%20that%0Aemploys%20a%20multi-generator%20ensemble%20strategy%20to%20improve%20candidate%20generation.%20We%0Aintroduce%20M-Schema%2C%20a%20semi-structured%20schema%20representation%20method%20designed%20to%0Aenhance%20the%20understanding%20of%20database%20structures.%20To%20enhance%20the%20quality%20and%0Adiversity%20of%20generated%20candidate%20SQL%20queries%2C%20XiYan-SQL%20integrates%20the%0Asignificant%20potential%20of%20in-context%20learning%20%28ICL%29%20with%20the%20precise%20control%20of%0Asupervised%20fine-tuning.%20On%20one%20hand%2C%20we%20propose%20a%20series%20of%20training%20strategies%0Ato%20fine-tune%20models%20to%20generate%20high-quality%20candidates%20with%20diverse%0Apreferences.%20On%20the%20other%20hand%2C%20we%20implement%20the%20ICL%20approach%20with%20an%20example%0Aselection%20method%20based%20on%20named%20entity%20recognition%20to%20prevent%20overemphasis%20on%0Aentities.%20The%20refiner%20optimizes%20each%20candidate%20by%20correcting%20logical%20or%0Asyntactical%20errors.%20To%20address%20the%20challenge%20of%20identifying%20the%20best%20candidate%2C%0Awe%20fine-tune%20a%20selection%20model%20to%20distinguish%20nuances%20of%20candidate%20SQL%20queries.%0AThe%20experimental%20results%20on%20multiple%20dialect%20datasets%20demonstrate%20the%0Arobustness%20of%20XiYan-SQL%20in%20addressing%20challenges%20across%20different%20scenarios.%0AOverall%2C%20our%20proposed%20XiYan-SQL%20achieves%20the%20state-of-the-art%20execution%0Aaccuracy%20of%2075.63%25%20on%20Bird%20benchmark%2C%2089.65%25%20on%20the%20Spider%20test%20set%2C%2069.86%25%20on%0ASQL-Eval%2C%2041.20%25%20on%20NL2GQL.%20The%20proposed%20framework%20not%20only%20enhances%20the%0Aquality%20and%20diversity%20of%20SQL%20queries%20but%20also%20outperforms%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08599v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Preview%2520of%2520XiYan-SQL%253A%2520A%2520Multi-Generator%2520Ensemble%2520Framework%2520for%250A%2520%2520Text-to-SQL%26entry.906535625%3DYingqi%2520Gao%2520and%2520Yifu%2520Liu%2520and%2520Xiaoxia%2520Li%2520and%2520Xiaorong%2520Shi%2520and%2520Yin%2520Zhu%2520and%2520Yiming%2520Wang%2520and%2520Shiqi%2520Li%2520and%2520Wei%2520Li%2520and%2520Yuntao%2520Hong%2520and%2520Zhiling%2520Luo%2520and%2520Jinyang%2520Gao%2520and%2520Liyu%2520Mou%2520and%2520Yu%2520Li%26entry.1292438233%3D%2520%2520To%2520tackle%2520the%2520challenges%2520of%2520large%2520language%2520model%2520performance%2520in%2520natural%250Alanguage%2520to%2520SQL%2520tasks%252C%2520we%2520introduce%2520XiYan-SQL%252C%2520an%2520innovative%2520framework%2520that%250Aemploys%2520a%2520multi-generator%2520ensemble%2520strategy%2520to%2520improve%2520candidate%2520generation.%2520We%250Aintroduce%2520M-Schema%252C%2520a%2520semi-structured%2520schema%2520representation%2520method%2520designed%2520to%250Aenhance%2520the%2520understanding%2520of%2520database%2520structures.%2520To%2520enhance%2520the%2520quality%2520and%250Adiversity%2520of%2520generated%2520candidate%2520SQL%2520queries%252C%2520XiYan-SQL%2520integrates%2520the%250Asignificant%2520potential%2520of%2520in-context%2520learning%2520%2528ICL%2529%2520with%2520the%2520precise%2520control%2520of%250Asupervised%2520fine-tuning.%2520On%2520one%2520hand%252C%2520we%2520propose%2520a%2520series%2520of%2520training%2520strategies%250Ato%2520fine-tune%2520models%2520to%2520generate%2520high-quality%2520candidates%2520with%2520diverse%250Apreferences.%2520On%2520the%2520other%2520hand%252C%2520we%2520implement%2520the%2520ICL%2520approach%2520with%2520an%2520example%250Aselection%2520method%2520based%2520on%2520named%2520entity%2520recognition%2520to%2520prevent%2520overemphasis%2520on%250Aentities.%2520The%2520refiner%2520optimizes%2520each%2520candidate%2520by%2520correcting%2520logical%2520or%250Asyntactical%2520errors.%2520To%2520address%2520the%2520challenge%2520of%2520identifying%2520the%2520best%2520candidate%252C%250Awe%2520fine-tune%2520a%2520selection%2520model%2520to%2520distinguish%2520nuances%2520of%2520candidate%2520SQL%2520queries.%250AThe%2520experimental%2520results%2520on%2520multiple%2520dialect%2520datasets%2520demonstrate%2520the%250Arobustness%2520of%2520XiYan-SQL%2520in%2520addressing%2520challenges%2520across%2520different%2520scenarios.%250AOverall%252C%2520our%2520proposed%2520XiYan-SQL%2520achieves%2520the%2520state-of-the-art%2520execution%250Aaccuracy%2520of%252075.63%2525%2520on%2520Bird%2520benchmark%252C%252089.65%2525%2520on%2520the%2520Spider%2520test%2520set%252C%252069.86%2525%2520on%250ASQL-Eval%252C%252041.20%2525%2520on%2520NL2GQL.%2520The%2520proposed%2520framework%2520not%2520only%2520enhances%2520the%250Aquality%2520and%2520diversity%2520of%2520SQL%2520queries%2520but%2520also%2520outperforms%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08599v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Preview%20of%20XiYan-SQL%3A%20A%20Multi-Generator%20Ensemble%20Framework%20for%0A%20%20Text-to-SQL&entry.906535625=Yingqi%20Gao%20and%20Yifu%20Liu%20and%20Xiaoxia%20Li%20and%20Xiaorong%20Shi%20and%20Yin%20Zhu%20and%20Yiming%20Wang%20and%20Shiqi%20Li%20and%20Wei%20Li%20and%20Yuntao%20Hong%20and%20Zhiling%20Luo%20and%20Jinyang%20Gao%20and%20Liyu%20Mou%20and%20Yu%20Li&entry.1292438233=%20%20To%20tackle%20the%20challenges%20of%20large%20language%20model%20performance%20in%20natural%0Alanguage%20to%20SQL%20tasks%2C%20we%20introduce%20XiYan-SQL%2C%20an%20innovative%20framework%20that%0Aemploys%20a%20multi-generator%20ensemble%20strategy%20to%20improve%20candidate%20generation.%20We%0Aintroduce%20M-Schema%2C%20a%20semi-structured%20schema%20representation%20method%20designed%20to%0Aenhance%20the%20understanding%20of%20database%20structures.%20To%20enhance%20the%20quality%20and%0Adiversity%20of%20generated%20candidate%20SQL%20queries%2C%20XiYan-SQL%20integrates%20the%0Asignificant%20potential%20of%20in-context%20learning%20%28ICL%29%20with%20the%20precise%20control%20of%0Asupervised%20fine-tuning.%20On%20one%20hand%2C%20we%20propose%20a%20series%20of%20training%20strategies%0Ato%20fine-tune%20models%20to%20generate%20high-quality%20candidates%20with%20diverse%0Apreferences.%20On%20the%20other%20hand%2C%20we%20implement%20the%20ICL%20approach%20with%20an%20example%0Aselection%20method%20based%20on%20named%20entity%20recognition%20to%20prevent%20overemphasis%20on%0Aentities.%20The%20refiner%20optimizes%20each%20candidate%20by%20correcting%20logical%20or%0Asyntactical%20errors.%20To%20address%20the%20challenge%20of%20identifying%20the%20best%20candidate%2C%0Awe%20fine-tune%20a%20selection%20model%20to%20distinguish%20nuances%20of%20candidate%20SQL%20queries.%0AThe%20experimental%20results%20on%20multiple%20dialect%20datasets%20demonstrate%20the%0Arobustness%20of%20XiYan-SQL%20in%20addressing%20challenges%20across%20different%20scenarios.%0AOverall%2C%20our%20proposed%20XiYan-SQL%20achieves%20the%20state-of-the-art%20execution%0Aaccuracy%20of%2075.63%25%20on%20Bird%20benchmark%2C%2089.65%25%20on%20the%20Spider%20test%20set%2C%2069.86%25%20on%0ASQL-Eval%2C%2041.20%25%20on%20NL2GQL.%20The%20proposed%20framework%20not%20only%20enhances%20the%0Aquality%20and%20diversity%20of%20SQL%20queries%20but%20also%20outperforms%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08599v3&entry.124074799=Read"},
{"title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation", "author": "Siyuan Huang and Liliang Chen and Pengfei Zhou and Shengcong Chen and Zhengkai Jiang and Yue Hu and Yue Liao and Peng Gao and Hongsheng Li and Maoqing Yao and Guanghui Ren", "abstract": "  We introduce EnerVerse, a generative robotics foundation model that\nconstructs and interprets embodied spaces. EnerVerse employs an autoregressive\nvideo diffusion framework to predict future embodied spaces from instructions,\nenhanced by a sparse context memory for long-term reasoning. To model the 3D\nrobotics world, we propose Free Anchor Views (FAVs), a multi-view video\nrepresentation offering flexible, task-adaptive perspectives to address\nchallenges like motion ambiguity and environmental constraints. Additionally,\nwe present EnerVerse-D, a data engine pipeline combining the generative model\nwith 4D Gaussian Splatting, forming a self-reinforcing data loop to reduce the\nsim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world\nrepresentations into physical actions via a policy head (EnerVerse-A), enabling\nrobots to execute task instructions. EnerVerse-A achieves state-of-the-art\nperformance in both simulation and real-world settings.\n", "link": "http://arxiv.org/abs/2501.01895v2", "date": "2025-02-10", "relevancy": 2.442, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6178}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6134}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnerVerse%3A%20Envisioning%20Embodied%20Future%20Space%20for%20Robotics%20Manipulation&body=Title%3A%20EnerVerse%3A%20Envisioning%20Embodied%20Future%20Space%20for%20Robotics%20Manipulation%0AAuthor%3A%20Siyuan%20Huang%20and%20Liliang%20Chen%20and%20Pengfei%20Zhou%20and%20Shengcong%20Chen%20and%20Zhengkai%20Jiang%20and%20Yue%20Hu%20and%20Yue%20Liao%20and%20Peng%20Gao%20and%20Hongsheng%20Li%20and%20Maoqing%20Yao%20and%20Guanghui%20Ren%0AAbstract%3A%20%20%20We%20introduce%20EnerVerse%2C%20a%20generative%20robotics%20foundation%20model%20that%0Aconstructs%20and%20interprets%20embodied%20spaces.%20EnerVerse%20employs%20an%20autoregressive%0Avideo%20diffusion%20framework%20to%20predict%20future%20embodied%20spaces%20from%20instructions%2C%0Aenhanced%20by%20a%20sparse%20context%20memory%20for%20long-term%20reasoning.%20To%20model%20the%203D%0Arobotics%20world%2C%20we%20propose%20Free%20Anchor%20Views%20%28FAVs%29%2C%20a%20multi-view%20video%0Arepresentation%20offering%20flexible%2C%20task-adaptive%20perspectives%20to%20address%0Achallenges%20like%20motion%20ambiguity%20and%20environmental%20constraints.%20Additionally%2C%0Awe%20present%20EnerVerse-D%2C%20a%20data%20engine%20pipeline%20combining%20the%20generative%20model%0Awith%204D%20Gaussian%20Splatting%2C%20forming%20a%20self-reinforcing%20data%20loop%20to%20reduce%20the%0Asim-to-real%20gap.%20Leveraging%20these%20innovations%2C%20EnerVerse%20translates%204D%20world%0Arepresentations%20into%20physical%20actions%20via%20a%20policy%20head%20%28EnerVerse-A%29%2C%20enabling%0Arobots%20to%20execute%20task%20instructions.%20EnerVerse-A%20achieves%20state-of-the-art%0Aperformance%20in%20both%20simulation%20and%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01895v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnerVerse%253A%2520Envisioning%2520Embodied%2520Future%2520Space%2520for%2520Robotics%2520Manipulation%26entry.906535625%3DSiyuan%2520Huang%2520and%2520Liliang%2520Chen%2520and%2520Pengfei%2520Zhou%2520and%2520Shengcong%2520Chen%2520and%2520Zhengkai%2520Jiang%2520and%2520Yue%2520Hu%2520and%2520Yue%2520Liao%2520and%2520Peng%2520Gao%2520and%2520Hongsheng%2520Li%2520and%2520Maoqing%2520Yao%2520and%2520Guanghui%2520Ren%26entry.1292438233%3D%2520%2520We%2520introduce%2520EnerVerse%252C%2520a%2520generative%2520robotics%2520foundation%2520model%2520that%250Aconstructs%2520and%2520interprets%2520embodied%2520spaces.%2520EnerVerse%2520employs%2520an%2520autoregressive%250Avideo%2520diffusion%2520framework%2520to%2520predict%2520future%2520embodied%2520spaces%2520from%2520instructions%252C%250Aenhanced%2520by%2520a%2520sparse%2520context%2520memory%2520for%2520long-term%2520reasoning.%2520To%2520model%2520the%25203D%250Arobotics%2520world%252C%2520we%2520propose%2520Free%2520Anchor%2520Views%2520%2528FAVs%2529%252C%2520a%2520multi-view%2520video%250Arepresentation%2520offering%2520flexible%252C%2520task-adaptive%2520perspectives%2520to%2520address%250Achallenges%2520like%2520motion%2520ambiguity%2520and%2520environmental%2520constraints.%2520Additionally%252C%250Awe%2520present%2520EnerVerse-D%252C%2520a%2520data%2520engine%2520pipeline%2520combining%2520the%2520generative%2520model%250Awith%25204D%2520Gaussian%2520Splatting%252C%2520forming%2520a%2520self-reinforcing%2520data%2520loop%2520to%2520reduce%2520the%250Asim-to-real%2520gap.%2520Leveraging%2520these%2520innovations%252C%2520EnerVerse%2520translates%25204D%2520world%250Arepresentations%2520into%2520physical%2520actions%2520via%2520a%2520policy%2520head%2520%2528EnerVerse-A%2529%252C%2520enabling%250Arobots%2520to%2520execute%2520task%2520instructions.%2520EnerVerse-A%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520both%2520simulation%2520and%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01895v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnerVerse%3A%20Envisioning%20Embodied%20Future%20Space%20for%20Robotics%20Manipulation&entry.906535625=Siyuan%20Huang%20and%20Liliang%20Chen%20and%20Pengfei%20Zhou%20and%20Shengcong%20Chen%20and%20Zhengkai%20Jiang%20and%20Yue%20Hu%20and%20Yue%20Liao%20and%20Peng%20Gao%20and%20Hongsheng%20Li%20and%20Maoqing%20Yao%20and%20Guanghui%20Ren&entry.1292438233=%20%20We%20introduce%20EnerVerse%2C%20a%20generative%20robotics%20foundation%20model%20that%0Aconstructs%20and%20interprets%20embodied%20spaces.%20EnerVerse%20employs%20an%20autoregressive%0Avideo%20diffusion%20framework%20to%20predict%20future%20embodied%20spaces%20from%20instructions%2C%0Aenhanced%20by%20a%20sparse%20context%20memory%20for%20long-term%20reasoning.%20To%20model%20the%203D%0Arobotics%20world%2C%20we%20propose%20Free%20Anchor%20Views%20%28FAVs%29%2C%20a%20multi-view%20video%0Arepresentation%20offering%20flexible%2C%20task-adaptive%20perspectives%20to%20address%0Achallenges%20like%20motion%20ambiguity%20and%20environmental%20constraints.%20Additionally%2C%0Awe%20present%20EnerVerse-D%2C%20a%20data%20engine%20pipeline%20combining%20the%20generative%20model%0Awith%204D%20Gaussian%20Splatting%2C%20forming%20a%20self-reinforcing%20data%20loop%20to%20reduce%20the%0Asim-to-real%20gap.%20Leveraging%20these%20innovations%2C%20EnerVerse%20translates%204D%20world%0Arepresentations%20into%20physical%20actions%20via%20a%20policy%20head%20%28EnerVerse-A%29%2C%20enabling%0Arobots%20to%20execute%20task%20instructions.%20EnerVerse-A%20achieves%20state-of-the-art%0Aperformance%20in%20both%20simulation%20and%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01895v2&entry.124074799=Read"},
{"title": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications", "author": "Sathya Krishnan Suresh and Wu Mengjun and Tushar Pranav and Eng Siong Chng", "abstract": "  The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.\n", "link": "http://arxiv.org/abs/2409.19020v3", "date": "2025-02-10", "relevancy": 2.4251, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiaSynth%3A%20Synthetic%20Dialogue%20Generation%20Framework%20for%20Low%20Resource%0A%20%20Dialogue%20Applications&body=Title%3A%20DiaSynth%3A%20Synthetic%20Dialogue%20Generation%20Framework%20for%20Low%20Resource%0A%20%20Dialogue%20Applications%0AAuthor%3A%20Sathya%20Krishnan%20Suresh%20and%20Wu%20Mengjun%20and%20Tushar%20Pranav%20and%20Eng%20Siong%20Chng%0AAbstract%3A%20%20%20The%20scarcity%20of%20domain-specific%20dialogue%20datasets%20limits%20the%20development%20of%0Adialogue%20systems%20across%20applications.%20Existing%20research%20is%20constrained%20by%0Ageneral%20or%20niche%20datasets%20that%20lack%20sufficient%20scale%20for%20training%20dialogue%0Asystems.%20To%20address%20this%20gap%2C%20we%20introduce%20DiaSynth%20-%20a%20synthetic%20dialogue%0Ageneration%20framework%20capable%20of%20generating%20high-quality%2C%20contextually%20rich%0Adialogues%20across%20a%20wide%20range%20of%20domains.%20Unlike%20existing%20frameworks%2C%20DiaSynth%0Auses%20Large%20Language%20Models%20%28LLMs%29%20and%20Chain%20of%20Thought%20%28CoT%29%20reasoning%20to%0Agenerate%20dynamic%2C%20domain-specific%20dialogues%20with%20simulated%20personas%20and%20diverse%0Aconversational%20features.%20We%20perform%20our%20experiments%20by%20generating%20synthetic%0Adata%20using%20different%20LLMs%20and%20few-shot%20examples%20from%20DialogSum%20and%20SAMSum.%20The%0Apretrained%20language%20models%20fine-tuned%20on%20the%20synthetic%20data%20outperform%20the%20base%0Amodels%20by%2016.47%25%20on%20dialogue%20summarization%2C%20while%20the%20comparison%20between%20models%0Afine-tuned%20on%20in-domain%20data%20and%20synthetic%20data%20shows%20that%20the%20synthetic%20data%0Ais%20able%20to%20capture%2090.48%25%20of%20the%20performance%20distribution%20of%20the%20in-domain%20data%0Aon%20dialogue%20summarization.%20The%20quality%20of%20the%20data%20generated%20also%20increases%20as%0Awe%20increase%20the%20size%20of%20LLM%20from%203B%20to%208B.%20These%20results%20validate%20DiaSynth%27s%0Apotential%20as%20a%20robust%20alternative%20to%20traditional%20data%20collection%20methods.%20We%0Aopen%20source%20the%20code%20and%20data%20generated%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19020v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiaSynth%253A%2520Synthetic%2520Dialogue%2520Generation%2520Framework%2520for%2520Low%2520Resource%250A%2520%2520Dialogue%2520Applications%26entry.906535625%3DSathya%2520Krishnan%2520Suresh%2520and%2520Wu%2520Mengjun%2520and%2520Tushar%2520Pranav%2520and%2520Eng%2520Siong%2520Chng%26entry.1292438233%3D%2520%2520The%2520scarcity%2520of%2520domain-specific%2520dialogue%2520datasets%2520limits%2520the%2520development%2520of%250Adialogue%2520systems%2520across%2520applications.%2520Existing%2520research%2520is%2520constrained%2520by%250Ageneral%2520or%2520niche%2520datasets%2520that%2520lack%2520sufficient%2520scale%2520for%2520training%2520dialogue%250Asystems.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520DiaSynth%2520-%2520a%2520synthetic%2520dialogue%250Ageneration%2520framework%2520capable%2520of%2520generating%2520high-quality%252C%2520contextually%2520rich%250Adialogues%2520across%2520a%2520wide%2520range%2520of%2520domains.%2520Unlike%2520existing%2520frameworks%252C%2520DiaSynth%250Auses%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520reasoning%2520to%250Agenerate%2520dynamic%252C%2520domain-specific%2520dialogues%2520with%2520simulated%2520personas%2520and%2520diverse%250Aconversational%2520features.%2520We%2520perform%2520our%2520experiments%2520by%2520generating%2520synthetic%250Adata%2520using%2520different%2520LLMs%2520and%2520few-shot%2520examples%2520from%2520DialogSum%2520and%2520SAMSum.%2520The%250Apretrained%2520language%2520models%2520fine-tuned%2520on%2520the%2520synthetic%2520data%2520outperform%2520the%2520base%250Amodels%2520by%252016.47%2525%2520on%2520dialogue%2520summarization%252C%2520while%2520the%2520comparison%2520between%2520models%250Afine-tuned%2520on%2520in-domain%2520data%2520and%2520synthetic%2520data%2520shows%2520that%2520the%2520synthetic%2520data%250Ais%2520able%2520to%2520capture%252090.48%2525%2520of%2520the%2520performance%2520distribution%2520of%2520the%2520in-domain%2520data%250Aon%2520dialogue%2520summarization.%2520The%2520quality%2520of%2520the%2520data%2520generated%2520also%2520increases%2520as%250Awe%2520increase%2520the%2520size%2520of%2520LLM%2520from%25203B%2520to%25208B.%2520These%2520results%2520validate%2520DiaSynth%2527s%250Apotential%2520as%2520a%2520robust%2520alternative%2520to%2520traditional%2520data%2520collection%2520methods.%2520We%250Aopen%2520source%2520the%2520code%2520and%2520data%2520generated%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19020v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiaSynth%3A%20Synthetic%20Dialogue%20Generation%20Framework%20for%20Low%20Resource%0A%20%20Dialogue%20Applications&entry.906535625=Sathya%20Krishnan%20Suresh%20and%20Wu%20Mengjun%20and%20Tushar%20Pranav%20and%20Eng%20Siong%20Chng&entry.1292438233=%20%20The%20scarcity%20of%20domain-specific%20dialogue%20datasets%20limits%20the%20development%20of%0Adialogue%20systems%20across%20applications.%20Existing%20research%20is%20constrained%20by%0Ageneral%20or%20niche%20datasets%20that%20lack%20sufficient%20scale%20for%20training%20dialogue%0Asystems.%20To%20address%20this%20gap%2C%20we%20introduce%20DiaSynth%20-%20a%20synthetic%20dialogue%0Ageneration%20framework%20capable%20of%20generating%20high-quality%2C%20contextually%20rich%0Adialogues%20across%20a%20wide%20range%20of%20domains.%20Unlike%20existing%20frameworks%2C%20DiaSynth%0Auses%20Large%20Language%20Models%20%28LLMs%29%20and%20Chain%20of%20Thought%20%28CoT%29%20reasoning%20to%0Agenerate%20dynamic%2C%20domain-specific%20dialogues%20with%20simulated%20personas%20and%20diverse%0Aconversational%20features.%20We%20perform%20our%20experiments%20by%20generating%20synthetic%0Adata%20using%20different%20LLMs%20and%20few-shot%20examples%20from%20DialogSum%20and%20SAMSum.%20The%0Apretrained%20language%20models%20fine-tuned%20on%20the%20synthetic%20data%20outperform%20the%20base%0Amodels%20by%2016.47%25%20on%20dialogue%20summarization%2C%20while%20the%20comparison%20between%20models%0Afine-tuned%20on%20in-domain%20data%20and%20synthetic%20data%20shows%20that%20the%20synthetic%20data%0Ais%20able%20to%20capture%2090.48%25%20of%20the%20performance%20distribution%20of%20the%20in-domain%20data%0Aon%20dialogue%20summarization.%20The%20quality%20of%20the%20data%20generated%20also%20increases%20as%0Awe%20increase%20the%20size%20of%20LLM%20from%203B%20to%208B.%20These%20results%20validate%20DiaSynth%27s%0Apotential%20as%20a%20robust%20alternative%20to%20traditional%20data%20collection%20methods.%20We%0Aopen%20source%20the%20code%20and%20data%20generated%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19020v3&entry.124074799=Read"},
{"title": "RelGNN: Composite Message Passing for Relational Deep Learning", "author": "Tianlang Chen and Charilaos Kanatsoulis and Jure Leskovec", "abstract": "  Predictive tasks on relational databases are critical in real-world\napplications spanning e-commerce, healthcare, and social media. To address\nthese tasks effectively, Relational Deep Learning (RDL) encodes relational data\nas graphs, enabling Graph Neural Networks (GNNs) to exploit relational\nstructures for improved predictions. However, existing heterogeneous GNNs often\noverlook the intrinsic structural properties of relational databases, leading\nto modeling inefficiencies. Here we introduce RelGNN, a novel GNN framework\nspecifically designed to capture the unique characteristics of relational\ndatabases. At the core of our approach is the introduction of atomic routes,\nwhich are sequences of nodes forming high-order tripartite structures. Building\nupon these atomic routes, RelGNN designs new composite message passing\nmechanisms between heterogeneous nodes, allowing direct single-hop interactions\nbetween them. This approach avoids redundant aggregations and mitigates\ninformation entanglement, ultimately leading to more efficient and accurate\npredictive modeling. RelGNN is evaluated on 30 diverse real-world tasks from\nRelBench (Fey et al., 2024), and consistently achieves state-of-the-art\naccuracy with up to 25% improvement.\n", "link": "http://arxiv.org/abs/2502.06784v1", "date": "2025-02-10", "relevancy": 2.4053, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4761}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelGNN%3A%20Composite%20Message%20Passing%20for%20Relational%20Deep%20Learning&body=Title%3A%20RelGNN%3A%20Composite%20Message%20Passing%20for%20Relational%20Deep%20Learning%0AAuthor%3A%20Tianlang%20Chen%20and%20Charilaos%20Kanatsoulis%20and%20Jure%20Leskovec%0AAbstract%3A%20%20%20Predictive%20tasks%20on%20relational%20databases%20are%20critical%20in%20real-world%0Aapplications%20spanning%20e-commerce%2C%20healthcare%2C%20and%20social%20media.%20To%20address%0Athese%20tasks%20effectively%2C%20Relational%20Deep%20Learning%20%28RDL%29%20encodes%20relational%20data%0Aas%20graphs%2C%20enabling%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20exploit%20relational%0Astructures%20for%20improved%20predictions.%20However%2C%20existing%20heterogeneous%20GNNs%20often%0Aoverlook%20the%20intrinsic%20structural%20properties%20of%20relational%20databases%2C%20leading%0Ato%20modeling%20inefficiencies.%20Here%20we%20introduce%20RelGNN%2C%20a%20novel%20GNN%20framework%0Aspecifically%20designed%20to%20capture%20the%20unique%20characteristics%20of%20relational%0Adatabases.%20At%20the%20core%20of%20our%20approach%20is%20the%20introduction%20of%20atomic%20routes%2C%0Awhich%20are%20sequences%20of%20nodes%20forming%20high-order%20tripartite%20structures.%20Building%0Aupon%20these%20atomic%20routes%2C%20RelGNN%20designs%20new%20composite%20message%20passing%0Amechanisms%20between%20heterogeneous%20nodes%2C%20allowing%20direct%20single-hop%20interactions%0Abetween%20them.%20This%20approach%20avoids%20redundant%20aggregations%20and%20mitigates%0Ainformation%20entanglement%2C%20ultimately%20leading%20to%20more%20efficient%20and%20accurate%0Apredictive%20modeling.%20RelGNN%20is%20evaluated%20on%2030%20diverse%20real-world%20tasks%20from%0ARelBench%20%28Fey%20et%20al.%2C%202024%29%2C%20and%20consistently%20achieves%20state-of-the-art%0Aaccuracy%20with%20up%20to%2025%25%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelGNN%253A%2520Composite%2520Message%2520Passing%2520for%2520Relational%2520Deep%2520Learning%26entry.906535625%3DTianlang%2520Chen%2520and%2520Charilaos%2520Kanatsoulis%2520and%2520Jure%2520Leskovec%26entry.1292438233%3D%2520%2520Predictive%2520tasks%2520on%2520relational%2520databases%2520are%2520critical%2520in%2520real-world%250Aapplications%2520spanning%2520e-commerce%252C%2520healthcare%252C%2520and%2520social%2520media.%2520To%2520address%250Athese%2520tasks%2520effectively%252C%2520Relational%2520Deep%2520Learning%2520%2528RDL%2529%2520encodes%2520relational%2520data%250Aas%2520graphs%252C%2520enabling%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520exploit%2520relational%250Astructures%2520for%2520improved%2520predictions.%2520However%252C%2520existing%2520heterogeneous%2520GNNs%2520often%250Aoverlook%2520the%2520intrinsic%2520structural%2520properties%2520of%2520relational%2520databases%252C%2520leading%250Ato%2520modeling%2520inefficiencies.%2520Here%2520we%2520introduce%2520RelGNN%252C%2520a%2520novel%2520GNN%2520framework%250Aspecifically%2520designed%2520to%2520capture%2520the%2520unique%2520characteristics%2520of%2520relational%250Adatabases.%2520At%2520the%2520core%2520of%2520our%2520approach%2520is%2520the%2520introduction%2520of%2520atomic%2520routes%252C%250Awhich%2520are%2520sequences%2520of%2520nodes%2520forming%2520high-order%2520tripartite%2520structures.%2520Building%250Aupon%2520these%2520atomic%2520routes%252C%2520RelGNN%2520designs%2520new%2520composite%2520message%2520passing%250Amechanisms%2520between%2520heterogeneous%2520nodes%252C%2520allowing%2520direct%2520single-hop%2520interactions%250Abetween%2520them.%2520This%2520approach%2520avoids%2520redundant%2520aggregations%2520and%2520mitigates%250Ainformation%2520entanglement%252C%2520ultimately%2520leading%2520to%2520more%2520efficient%2520and%2520accurate%250Apredictive%2520modeling.%2520RelGNN%2520is%2520evaluated%2520on%252030%2520diverse%2520real-world%2520tasks%2520from%250ARelBench%2520%2528Fey%2520et%2520al.%252C%25202024%2529%252C%2520and%2520consistently%2520achieves%2520state-of-the-art%250Aaccuracy%2520with%2520up%2520to%252025%2525%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelGNN%3A%20Composite%20Message%20Passing%20for%20Relational%20Deep%20Learning&entry.906535625=Tianlang%20Chen%20and%20Charilaos%20Kanatsoulis%20and%20Jure%20Leskovec&entry.1292438233=%20%20Predictive%20tasks%20on%20relational%20databases%20are%20critical%20in%20real-world%0Aapplications%20spanning%20e-commerce%2C%20healthcare%2C%20and%20social%20media.%20To%20address%0Athese%20tasks%20effectively%2C%20Relational%20Deep%20Learning%20%28RDL%29%20encodes%20relational%20data%0Aas%20graphs%2C%20enabling%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20exploit%20relational%0Astructures%20for%20improved%20predictions.%20However%2C%20existing%20heterogeneous%20GNNs%20often%0Aoverlook%20the%20intrinsic%20structural%20properties%20of%20relational%20databases%2C%20leading%0Ato%20modeling%20inefficiencies.%20Here%20we%20introduce%20RelGNN%2C%20a%20novel%20GNN%20framework%0Aspecifically%20designed%20to%20capture%20the%20unique%20characteristics%20of%20relational%0Adatabases.%20At%20the%20core%20of%20our%20approach%20is%20the%20introduction%20of%20atomic%20routes%2C%0Awhich%20are%20sequences%20of%20nodes%20forming%20high-order%20tripartite%20structures.%20Building%0Aupon%20these%20atomic%20routes%2C%20RelGNN%20designs%20new%20composite%20message%20passing%0Amechanisms%20between%20heterogeneous%20nodes%2C%20allowing%20direct%20single-hop%20interactions%0Abetween%20them.%20This%20approach%20avoids%20redundant%20aggregations%20and%20mitigates%0Ainformation%20entanglement%2C%20ultimately%20leading%20to%20more%20efficient%20and%20accurate%0Apredictive%20modeling.%20RelGNN%20is%20evaluated%20on%2030%20diverse%20real-world%20tasks%20from%0ARelBench%20%28Fey%20et%20al.%2C%202024%29%2C%20and%20consistently%20achieves%20state-of-the-art%0Aaccuracy%20with%20up%20to%2025%25%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06784v1&entry.124074799=Read"},
{"title": "Resurrecting saturated LLM benchmarks with adversarial encoding", "author": "Igor Ivanov and Dmitrii Volkov", "abstract": "  Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks.\n", "link": "http://arxiv.org/abs/2502.06738v1", "date": "2025-02-10", "relevancy": 2.4042, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resurrecting%20saturated%20LLM%20benchmarks%20with%20adversarial%20encoding&body=Title%3A%20Resurrecting%20saturated%20LLM%20benchmarks%20with%20adversarial%20encoding%0AAuthor%3A%20Igor%20Ivanov%20and%20Dmitrii%20Volkov%0AAbstract%3A%20%20%20Recent%20work%20showed%20that%20small%20changes%20in%20benchmark%20questions%20can%20reduce%20LLMs%27%0Areasoning%20and%20recall.%20We%20explore%20two%20such%20changes%3A%20pairing%20questions%20and%20adding%0Amore%20answer%20options%2C%20on%20three%20benchmarks%3A%20WMDP-bio%2C%20GPQA%2C%20and%20MMLU%20variants.%20We%0Afind%20that%20for%20more%20capable%20models%2C%20these%20predictably%20reduce%20performance%2C%0Aessentially%20heightening%20the%20performance%20ceiling%20of%20a%20benchmark%20and%20unsaturating%0Ait%20again.%20We%20suggest%20this%20approach%20can%20resurrect%20old%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResurrecting%2520saturated%2520LLM%2520benchmarks%2520with%2520adversarial%2520encoding%26entry.906535625%3DIgor%2520Ivanov%2520and%2520Dmitrii%2520Volkov%26entry.1292438233%3D%2520%2520Recent%2520work%2520showed%2520that%2520small%2520changes%2520in%2520benchmark%2520questions%2520can%2520reduce%2520LLMs%2527%250Areasoning%2520and%2520recall.%2520We%2520explore%2520two%2520such%2520changes%253A%2520pairing%2520questions%2520and%2520adding%250Amore%2520answer%2520options%252C%2520on%2520three%2520benchmarks%253A%2520WMDP-bio%252C%2520GPQA%252C%2520and%2520MMLU%2520variants.%2520We%250Afind%2520that%2520for%2520more%2520capable%2520models%252C%2520these%2520predictably%2520reduce%2520performance%252C%250Aessentially%2520heightening%2520the%2520performance%2520ceiling%2520of%2520a%2520benchmark%2520and%2520unsaturating%250Ait%2520again.%2520We%2520suggest%2520this%2520approach%2520can%2520resurrect%2520old%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resurrecting%20saturated%20LLM%20benchmarks%20with%20adversarial%20encoding&entry.906535625=Igor%20Ivanov%20and%20Dmitrii%20Volkov&entry.1292438233=%20%20Recent%20work%20showed%20that%20small%20changes%20in%20benchmark%20questions%20can%20reduce%20LLMs%27%0Areasoning%20and%20recall.%20We%20explore%20two%20such%20changes%3A%20pairing%20questions%20and%20adding%0Amore%20answer%20options%2C%20on%20three%20benchmarks%3A%20WMDP-bio%2C%20GPQA%2C%20and%20MMLU%20variants.%20We%0Afind%20that%20for%20more%20capable%20models%2C%20these%20predictably%20reduce%20performance%2C%0Aessentially%20heightening%20the%20performance%20ceiling%20of%20a%20benchmark%20and%20unsaturating%0Ait%20again.%20We%20suggest%20this%20approach%20can%20resurrect%20old%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06738v1&entry.124074799=Read"},
{"title": "Towards bandit-based prompt-tuning for in-the-wild foundation agents", "author": "Finn Rietz and Oleg Smirnov and Sara Karimi and Lele Cao", "abstract": "  Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines.\n", "link": "http://arxiv.org/abs/2502.06358v1", "date": "2025-02-10", "relevancy": 2.3969, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.486}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.477}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20bandit-based%20prompt-tuning%20for%20in-the-wild%20foundation%20agents&body=Title%3A%20Towards%20bandit-based%20prompt-tuning%20for%20in-the-wild%20foundation%20agents%0AAuthor%3A%20Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao%0AAbstract%3A%20%20%20Prompting%20has%20emerged%20as%20the%20dominant%20paradigm%20for%20adapting%20large%2C%0Apre-trained%20transformer-based%20models%20to%20downstream%20tasks.%20The%20Prompting%0ADecision%20Transformer%20%28PDT%29%20enables%20large-scale%2C%20multi-task%20offline%0Areinforcement%20learning%20pre-training%20by%20leveraging%20stochastic%20trajectory%20prompts%0Ato%20identify%20the%20target%20task.%20However%2C%20these%20prompts%20are%20sampled%20uniformly%20from%0Aexpert%20demonstrations%2C%20overlooking%20a%20critical%20limitation%3A%20Not%20all%20prompts%20are%0Aequally%20informative%20for%20differentiating%20between%20tasks.%20To%20address%20this%2C%20we%0Apropose%20an%20inference%20time%20bandit-based%20prompt-tuning%20framework%20that%20explores%0Aand%20optimizes%20trajectory%20prompt%20selection%20to%20enhance%20task%20performance.%20Our%0Aexperiments%20indicate%20not%20only%20clear%20performance%20gains%20due%20to%20bandit-based%0Aprompt-tuning%2C%20but%20also%20better%20sample%20complexity%2C%20scalability%2C%20and%20prompt%20space%0Aexploration%20compared%20to%20prompt-tuning%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520bandit-based%2520prompt-tuning%2520for%2520in-the-wild%2520foundation%2520agents%26entry.906535625%3DFinn%2520Rietz%2520and%2520Oleg%2520Smirnov%2520and%2520Sara%2520Karimi%2520and%2520Lele%2520Cao%26entry.1292438233%3D%2520%2520Prompting%2520has%2520emerged%2520as%2520the%2520dominant%2520paradigm%2520for%2520adapting%2520large%252C%250Apre-trained%2520transformer-based%2520models%2520to%2520downstream%2520tasks.%2520The%2520Prompting%250ADecision%2520Transformer%2520%2528PDT%2529%2520enables%2520large-scale%252C%2520multi-task%2520offline%250Areinforcement%2520learning%2520pre-training%2520by%2520leveraging%2520stochastic%2520trajectory%2520prompts%250Ato%2520identify%2520the%2520target%2520task.%2520However%252C%2520these%2520prompts%2520are%2520sampled%2520uniformly%2520from%250Aexpert%2520demonstrations%252C%2520overlooking%2520a%2520critical%2520limitation%253A%2520Not%2520all%2520prompts%2520are%250Aequally%2520informative%2520for%2520differentiating%2520between%2520tasks.%2520To%2520address%2520this%252C%2520we%250Apropose%2520an%2520inference%2520time%2520bandit-based%2520prompt-tuning%2520framework%2520that%2520explores%250Aand%2520optimizes%2520trajectory%2520prompt%2520selection%2520to%2520enhance%2520task%2520performance.%2520Our%250Aexperiments%2520indicate%2520not%2520only%2520clear%2520performance%2520gains%2520due%2520to%2520bandit-based%250Aprompt-tuning%252C%2520but%2520also%2520better%2520sample%2520complexity%252C%2520scalability%252C%2520and%2520prompt%2520space%250Aexploration%2520compared%2520to%2520prompt-tuning%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20bandit-based%20prompt-tuning%20for%20in-the-wild%20foundation%20agents&entry.906535625=Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao&entry.1292438233=%20%20Prompting%20has%20emerged%20as%20the%20dominant%20paradigm%20for%20adapting%20large%2C%0Apre-trained%20transformer-based%20models%20to%20downstream%20tasks.%20The%20Prompting%0ADecision%20Transformer%20%28PDT%29%20enables%20large-scale%2C%20multi-task%20offline%0Areinforcement%20learning%20pre-training%20by%20leveraging%20stochastic%20trajectory%20prompts%0Ato%20identify%20the%20target%20task.%20However%2C%20these%20prompts%20are%20sampled%20uniformly%20from%0Aexpert%20demonstrations%2C%20overlooking%20a%20critical%20limitation%3A%20Not%20all%20prompts%20are%0Aequally%20informative%20for%20differentiating%20between%20tasks.%20To%20address%20this%2C%20we%0Apropose%20an%20inference%20time%20bandit-based%20prompt-tuning%20framework%20that%20explores%0Aand%20optimizes%20trajectory%20prompt%20selection%20to%20enhance%20task%20performance.%20Our%0Aexperiments%20indicate%20not%20only%20clear%20performance%20gains%20due%20to%20bandit-based%0Aprompt-tuning%2C%20but%20also%20better%20sample%20complexity%2C%20scalability%2C%20and%20prompt%20space%0Aexploration%20compared%20to%20prompt-tuning%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06358v1&entry.124074799=Read"},
{"title": "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent", "author": "Marina Sheshukova and Sergey Samsonov and Denis Belomestny and Eric Moulines and Qi-Man Shao and Zhuo-Song Zhang and Alexey Naumov", "abstract": "  In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1/\\sqrt{n}$.\n", "link": "http://arxiv.org/abs/2502.06719v1", "date": "2025-02-10", "relevancy": 2.3751, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4827}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.477}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Approximation%20and%20Multiplier%20Bootstrap%20for%20Stochastic%20Gradient%0A%20%20Descent&body=Title%3A%20Gaussian%20Approximation%20and%20Multiplier%20Bootstrap%20for%20Stochastic%20Gradient%0A%20%20Descent%0AAuthor%3A%20Marina%20Sheshukova%20and%20Sergey%20Samsonov%20and%20Denis%20Belomestny%20and%20Eric%20Moulines%20and%20Qi-Man%20Shao%20and%20Zhuo-Song%20Zhang%20and%20Alexey%20Naumov%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20establish%20non-asymptotic%20convergence%20rates%20in%20the%20central%0Alimit%20theorem%20for%20Polyak-Ruppert-averaged%20iterates%20of%20stochastic%20gradient%0Adescent%20%28SGD%29.%20Our%20analysis%20builds%20on%20the%20result%20of%20the%20Gaussian%20approximation%0Afor%20nonlinear%20statistics%20of%20independent%20random%20variables%20of%20Shao%20and%20Zhang%0A%282022%29.%20Using%20this%20result%2C%20we%20prove%20the%20non-asymptotic%20validity%20of%20the%0Amultiplier%20bootstrap%20for%20constructing%20the%20confidence%20sets%20for%20the%20optimal%0Asolution%20of%20an%20optimization%20problem.%20In%20particular%2C%20our%20approach%20avoids%20the%0Aneed%20to%20approximate%20the%20limiting%20covariance%20of%20Polyak-Ruppert%20SGD%20iterates%2C%0Awhich%20allows%20us%20to%20derive%20approximation%20rates%20in%20convex%20distance%20of%20order%20up%20to%0A%241/%5Csqrt%7Bn%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Approximation%2520and%2520Multiplier%2520Bootstrap%2520for%2520Stochastic%2520Gradient%250A%2520%2520Descent%26entry.906535625%3DMarina%2520Sheshukova%2520and%2520Sergey%2520Samsonov%2520and%2520Denis%2520Belomestny%2520and%2520Eric%2520Moulines%2520and%2520Qi-Man%2520Shao%2520and%2520Zhuo-Song%2520Zhang%2520and%2520Alexey%2520Naumov%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520establish%2520non-asymptotic%2520convergence%2520rates%2520in%2520the%2520central%250Alimit%2520theorem%2520for%2520Polyak-Ruppert-averaged%2520iterates%2520of%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529.%2520Our%2520analysis%2520builds%2520on%2520the%2520result%2520of%2520the%2520Gaussian%2520approximation%250Afor%2520nonlinear%2520statistics%2520of%2520independent%2520random%2520variables%2520of%2520Shao%2520and%2520Zhang%250A%25282022%2529.%2520Using%2520this%2520result%252C%2520we%2520prove%2520the%2520non-asymptotic%2520validity%2520of%2520the%250Amultiplier%2520bootstrap%2520for%2520constructing%2520the%2520confidence%2520sets%2520for%2520the%2520optimal%250Asolution%2520of%2520an%2520optimization%2520problem.%2520In%2520particular%252C%2520our%2520approach%2520avoids%2520the%250Aneed%2520to%2520approximate%2520the%2520limiting%2520covariance%2520of%2520Polyak-Ruppert%2520SGD%2520iterates%252C%250Awhich%2520allows%2520us%2520to%2520derive%2520approximation%2520rates%2520in%2520convex%2520distance%2520of%2520order%2520up%2520to%250A%25241/%255Csqrt%257Bn%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Approximation%20and%20Multiplier%20Bootstrap%20for%20Stochastic%20Gradient%0A%20%20Descent&entry.906535625=Marina%20Sheshukova%20and%20Sergey%20Samsonov%20and%20Denis%20Belomestny%20and%20Eric%20Moulines%20and%20Qi-Man%20Shao%20and%20Zhuo-Song%20Zhang%20and%20Alexey%20Naumov&entry.1292438233=%20%20In%20this%20paper%2C%20we%20establish%20non-asymptotic%20convergence%20rates%20in%20the%20central%0Alimit%20theorem%20for%20Polyak-Ruppert-averaged%20iterates%20of%20stochastic%20gradient%0Adescent%20%28SGD%29.%20Our%20analysis%20builds%20on%20the%20result%20of%20the%20Gaussian%20approximation%0Afor%20nonlinear%20statistics%20of%20independent%20random%20variables%20of%20Shao%20and%20Zhang%0A%282022%29.%20Using%20this%20result%2C%20we%20prove%20the%20non-asymptotic%20validity%20of%20the%0Amultiplier%20bootstrap%20for%20constructing%20the%20confidence%20sets%20for%20the%20optimal%0Asolution%20of%20an%20optimization%20problem.%20In%20particular%2C%20our%20approach%20avoids%20the%0Aneed%20to%20approximate%20the%20limiting%20covariance%20of%20Polyak-Ruppert%20SGD%20iterates%2C%0Awhich%20allows%20us%20to%20derive%20approximation%20rates%20in%20convex%20distance%20of%20order%20up%20to%0A%241/%5Csqrt%7Bn%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06719v1&entry.124074799=Read"},
{"title": "MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with\n  Attention Mamba Mechanisms", "author": "Tianyi Shang and Zhenyu Li and Pengjie Xu", "abstract": "  Vision Language Place Recognition (VLVPR) enhances robot localization\nperformance by incorporating natural language descriptions from images. By\nutilizing language information, VLVPR directs robot place matching, overcoming\nthe constraint of solely depending on vision. The essence of multimodal fusion\nlies in mining the complementary information between different modalities.\nHowever, general fusion methods rely on traditional neural architectures and\nare not well equipped to capture the dynamics of cross modal interactions,\nespecially in the presence of complex intra modal and inter modal correlations.\nTo this end, this paper proposes a novel coarse to fine and end to end\nconnected cross modal place recognition framework, called MambaPlace. In the\ncoarse localization stage, the text description and 3D point cloud are encoded\nby the pretrained T5 and instance encoder, respectively. They are then\nprocessed using Text Attention Mamba (TAM) and Point Clouds Mamba (PCM) for\ndata enhancement and alignment. In the subsequent fine localization stage, the\nfeatures of the text description and 3D point cloud are cross modally fused and\nfurther enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we\npredict the positional offset from the fused text point cloud features,\nachieving the most accurate localization. Extensive experiments show that\nMambaPlace achieves improved localization accuracy on the KITTI360Pose dataset\ncompared to the state of the art methods.\n", "link": "http://arxiv.org/abs/2408.15740v2", "date": "2025-02-10", "relevancy": 2.3632, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.613}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5781}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaPlace%3AText-to-Point-Cloud%20Cross-Modal%20Place%20Recognition%20with%0A%20%20Attention%20Mamba%20Mechanisms&body=Title%3A%20MambaPlace%3AText-to-Point-Cloud%20Cross-Modal%20Place%20Recognition%20with%0A%20%20Attention%20Mamba%20Mechanisms%0AAuthor%3A%20Tianyi%20Shang%20and%20Zhenyu%20Li%20and%20Pengjie%20Xu%0AAbstract%3A%20%20%20Vision%20Language%20Place%20Recognition%20%28VLVPR%29%20enhances%20robot%20localization%0Aperformance%20by%20incorporating%20natural%20language%20descriptions%20from%20images.%20By%0Autilizing%20language%20information%2C%20VLVPR%20directs%20robot%20place%20matching%2C%20overcoming%0Athe%20constraint%20of%20solely%20depending%20on%20vision.%20The%20essence%20of%20multimodal%20fusion%0Alies%20in%20mining%20the%20complementary%20information%20between%20different%20modalities.%0AHowever%2C%20general%20fusion%20methods%20rely%20on%20traditional%20neural%20architectures%20and%0Aare%20not%20well%20equipped%20to%20capture%20the%20dynamics%20of%20cross%20modal%20interactions%2C%0Aespecially%20in%20the%20presence%20of%20complex%20intra%20modal%20and%20inter%20modal%20correlations.%0ATo%20this%20end%2C%20this%20paper%20proposes%20a%20novel%20coarse%20to%20fine%20and%20end%20to%20end%0Aconnected%20cross%20modal%20place%20recognition%20framework%2C%20called%20MambaPlace.%20In%20the%0Acoarse%20localization%20stage%2C%20the%20text%20description%20and%203D%20point%20cloud%20are%20encoded%0Aby%20the%20pretrained%20T5%20and%20instance%20encoder%2C%20respectively.%20They%20are%20then%0Aprocessed%20using%20Text%20Attention%20Mamba%20%28TAM%29%20and%20Point%20Clouds%20Mamba%20%28PCM%29%20for%0Adata%20enhancement%20and%20alignment.%20In%20the%20subsequent%20fine%20localization%20stage%2C%20the%0Afeatures%20of%20the%20text%20description%20and%203D%20point%20cloud%20are%20cross%20modally%20fused%20and%0Afurther%20enhanced%20through%20cascaded%20Cross%20Attention%20Mamba%20%28CCAM%29.%20Finally%2C%20we%0Apredict%20the%20positional%20offset%20from%20the%20fused%20text%20point%20cloud%20features%2C%0Aachieving%20the%20most%20accurate%20localization.%20Extensive%20experiments%20show%20that%0AMambaPlace%20achieves%20improved%20localization%20accuracy%20on%20the%20KITTI360Pose%20dataset%0Acompared%20to%20the%20state%20of%20the%20art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15740v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaPlace%253AText-to-Point-Cloud%2520Cross-Modal%2520Place%2520Recognition%2520with%250A%2520%2520Attention%2520Mamba%2520Mechanisms%26entry.906535625%3DTianyi%2520Shang%2520and%2520Zhenyu%2520Li%2520and%2520Pengjie%2520Xu%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Place%2520Recognition%2520%2528VLVPR%2529%2520enhances%2520robot%2520localization%250Aperformance%2520by%2520incorporating%2520natural%2520language%2520descriptions%2520from%2520images.%2520By%250Autilizing%2520language%2520information%252C%2520VLVPR%2520directs%2520robot%2520place%2520matching%252C%2520overcoming%250Athe%2520constraint%2520of%2520solely%2520depending%2520on%2520vision.%2520The%2520essence%2520of%2520multimodal%2520fusion%250Alies%2520in%2520mining%2520the%2520complementary%2520information%2520between%2520different%2520modalities.%250AHowever%252C%2520general%2520fusion%2520methods%2520rely%2520on%2520traditional%2520neural%2520architectures%2520and%250Aare%2520not%2520well%2520equipped%2520to%2520capture%2520the%2520dynamics%2520of%2520cross%2520modal%2520interactions%252C%250Aespecially%2520in%2520the%2520presence%2520of%2520complex%2520intra%2520modal%2520and%2520inter%2520modal%2520correlations.%250ATo%2520this%2520end%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520coarse%2520to%2520fine%2520and%2520end%2520to%2520end%250Aconnected%2520cross%2520modal%2520place%2520recognition%2520framework%252C%2520called%2520MambaPlace.%2520In%2520the%250Acoarse%2520localization%2520stage%252C%2520the%2520text%2520description%2520and%25203D%2520point%2520cloud%2520are%2520encoded%250Aby%2520the%2520pretrained%2520T5%2520and%2520instance%2520encoder%252C%2520respectively.%2520They%2520are%2520then%250Aprocessed%2520using%2520Text%2520Attention%2520Mamba%2520%2528TAM%2529%2520and%2520Point%2520Clouds%2520Mamba%2520%2528PCM%2529%2520for%250Adata%2520enhancement%2520and%2520alignment.%2520In%2520the%2520subsequent%2520fine%2520localization%2520stage%252C%2520the%250Afeatures%2520of%2520the%2520text%2520description%2520and%25203D%2520point%2520cloud%2520are%2520cross%2520modally%2520fused%2520and%250Afurther%2520enhanced%2520through%2520cascaded%2520Cross%2520Attention%2520Mamba%2520%2528CCAM%2529.%2520Finally%252C%2520we%250Apredict%2520the%2520positional%2520offset%2520from%2520the%2520fused%2520text%2520point%2520cloud%2520features%252C%250Aachieving%2520the%2520most%2520accurate%2520localization.%2520Extensive%2520experiments%2520show%2520that%250AMambaPlace%2520achieves%2520improved%2520localization%2520accuracy%2520on%2520the%2520KITTI360Pose%2520dataset%250Acompared%2520to%2520the%2520state%2520of%2520the%2520art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15740v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaPlace%3AText-to-Point-Cloud%20Cross-Modal%20Place%20Recognition%20with%0A%20%20Attention%20Mamba%20Mechanisms&entry.906535625=Tianyi%20Shang%20and%20Zhenyu%20Li%20and%20Pengjie%20Xu&entry.1292438233=%20%20Vision%20Language%20Place%20Recognition%20%28VLVPR%29%20enhances%20robot%20localization%0Aperformance%20by%20incorporating%20natural%20language%20descriptions%20from%20images.%20By%0Autilizing%20language%20information%2C%20VLVPR%20directs%20robot%20place%20matching%2C%20overcoming%0Athe%20constraint%20of%20solely%20depending%20on%20vision.%20The%20essence%20of%20multimodal%20fusion%0Alies%20in%20mining%20the%20complementary%20information%20between%20different%20modalities.%0AHowever%2C%20general%20fusion%20methods%20rely%20on%20traditional%20neural%20architectures%20and%0Aare%20not%20well%20equipped%20to%20capture%20the%20dynamics%20of%20cross%20modal%20interactions%2C%0Aespecially%20in%20the%20presence%20of%20complex%20intra%20modal%20and%20inter%20modal%20correlations.%0ATo%20this%20end%2C%20this%20paper%20proposes%20a%20novel%20coarse%20to%20fine%20and%20end%20to%20end%0Aconnected%20cross%20modal%20place%20recognition%20framework%2C%20called%20MambaPlace.%20In%20the%0Acoarse%20localization%20stage%2C%20the%20text%20description%20and%203D%20point%20cloud%20are%20encoded%0Aby%20the%20pretrained%20T5%20and%20instance%20encoder%2C%20respectively.%20They%20are%20then%0Aprocessed%20using%20Text%20Attention%20Mamba%20%28TAM%29%20and%20Point%20Clouds%20Mamba%20%28PCM%29%20for%0Adata%20enhancement%20and%20alignment.%20In%20the%20subsequent%20fine%20localization%20stage%2C%20the%0Afeatures%20of%20the%20text%20description%20and%203D%20point%20cloud%20are%20cross%20modally%20fused%20and%0Afurther%20enhanced%20through%20cascaded%20Cross%20Attention%20Mamba%20%28CCAM%29.%20Finally%2C%20we%0Apredict%20the%20positional%20offset%20from%20the%20fused%20text%20point%20cloud%20features%2C%0Aachieving%20the%20most%20accurate%20localization.%20Extensive%20experiments%20show%20that%0AMambaPlace%20achieves%20improved%20localization%20accuracy%20on%20the%20KITTI360Pose%20dataset%0Acompared%20to%20the%20state%20of%20the%20art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15740v2&entry.124074799=Read"},
{"title": "Kinematic-ICP: Enhancing LiDAR Odometry with Kinematic Constraints for\n  Wheeled Mobile Robots Moving on Planar Surfaces", "author": "Tiziano Guadagnino and Benedikt Mersch and Ignacio Vizzo and Saurabh Gupta and Meher V. R. Malladi and Luca Lobefaro and Guillaume Doisy and Cyrill Stachniss", "abstract": "  LiDAR odometry is essential for many robotics applications, including 3D\nmapping, navigation, and simultaneous localization and mapping. LiDAR odometry\nsystems are usually based on some form of point cloud registration to compute\nthe ego-motion of a mobile robot. Yet, few of today's LiDAR odometry systems\nconsider domain-specific knowledge or the kinematic model of the mobile\nplatform during the point cloud alignment. In this paper, we present\nKinematic-ICP, a LiDAR odometry system that focuses on wheeled mobile robots\nequipped with a 3D LiDAR and moving on a planar surface, which is a common\nassumption for warehouses, offices, hospitals, etc. Our approach introduces\nkinematic constraints within the optimization of a traditional point-to-point\niterative closest point scheme. In this way, the resulting motion follows the\nkinematic constraints of the platform, effectively exploiting the robot's wheel\nodometry and the 3D LiDAR observations. We dynamically adjust the influence of\nLiDAR measurements and wheel odometry in our optimization scheme, allowing the\nsystem to handle degenerate scenarios such as feature-poor corridors. We\nevaluate our approach on robots operating in large-scale warehouse\nenvironments, but also outdoors. The experiments show that our approach\nachieves top performances and is more accurate than wheel odometry and common\nLiDAR odometry systems. Kinematic-ICP has been recently deployed in the Dexory\nfleet of robots operating in warehouses worldwide at their customers' sites,\nshowing that our method can run in the real world alongside a complete\nnavigation stack.\n", "link": "http://arxiv.org/abs/2410.10277v3", "date": "2025-02-10", "relevancy": 2.3587, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6287}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.589}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinematic-ICP%3A%20Enhancing%20LiDAR%20Odometry%20with%20Kinematic%20Constraints%20for%0A%20%20Wheeled%20Mobile%20Robots%20Moving%20on%20Planar%20Surfaces&body=Title%3A%20Kinematic-ICP%3A%20Enhancing%20LiDAR%20Odometry%20with%20Kinematic%20Constraints%20for%0A%20%20Wheeled%20Mobile%20Robots%20Moving%20on%20Planar%20Surfaces%0AAuthor%3A%20Tiziano%20Guadagnino%20and%20Benedikt%20Mersch%20and%20Ignacio%20Vizzo%20and%20Saurabh%20Gupta%20and%20Meher%20V.%20R.%20Malladi%20and%20Luca%20Lobefaro%20and%20Guillaume%20Doisy%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20LiDAR%20odometry%20is%20essential%20for%20many%20robotics%20applications%2C%20including%203D%0Amapping%2C%20navigation%2C%20and%20simultaneous%20localization%20and%20mapping.%20LiDAR%20odometry%0Asystems%20are%20usually%20based%20on%20some%20form%20of%20point%20cloud%20registration%20to%20compute%0Athe%20ego-motion%20of%20a%20mobile%20robot.%20Yet%2C%20few%20of%20today%27s%20LiDAR%20odometry%20systems%0Aconsider%20domain-specific%20knowledge%20or%20the%20kinematic%20model%20of%20the%20mobile%0Aplatform%20during%20the%20point%20cloud%20alignment.%20In%20this%20paper%2C%20we%20present%0AKinematic-ICP%2C%20a%20LiDAR%20odometry%20system%20that%20focuses%20on%20wheeled%20mobile%20robots%0Aequipped%20with%20a%203D%20LiDAR%20and%20moving%20on%20a%20planar%20surface%2C%20which%20is%20a%20common%0Aassumption%20for%20warehouses%2C%20offices%2C%20hospitals%2C%20etc.%20Our%20approach%20introduces%0Akinematic%20constraints%20within%20the%20optimization%20of%20a%20traditional%20point-to-point%0Aiterative%20closest%20point%20scheme.%20In%20this%20way%2C%20the%20resulting%20motion%20follows%20the%0Akinematic%20constraints%20of%20the%20platform%2C%20effectively%20exploiting%20the%20robot%27s%20wheel%0Aodometry%20and%20the%203D%20LiDAR%20observations.%20We%20dynamically%20adjust%20the%20influence%20of%0ALiDAR%20measurements%20and%20wheel%20odometry%20in%20our%20optimization%20scheme%2C%20allowing%20the%0Asystem%20to%20handle%20degenerate%20scenarios%20such%20as%20feature-poor%20corridors.%20We%0Aevaluate%20our%20approach%20on%20robots%20operating%20in%20large-scale%20warehouse%0Aenvironments%2C%20but%20also%20outdoors.%20The%20experiments%20show%20that%20our%20approach%0Aachieves%20top%20performances%20and%20is%20more%20accurate%20than%20wheel%20odometry%20and%20common%0ALiDAR%20odometry%20systems.%20Kinematic-ICP%20has%20been%20recently%20deployed%20in%20the%20Dexory%0Afleet%20of%20robots%20operating%20in%20warehouses%20worldwide%20at%20their%20customers%27%20sites%2C%0Ashowing%20that%20our%20method%20can%20run%20in%20the%20real%20world%20alongside%20a%20complete%0Anavigation%20stack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10277v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinematic-ICP%253A%2520Enhancing%2520LiDAR%2520Odometry%2520with%2520Kinematic%2520Constraints%2520for%250A%2520%2520Wheeled%2520Mobile%2520Robots%2520Moving%2520on%2520Planar%2520Surfaces%26entry.906535625%3DTiziano%2520Guadagnino%2520and%2520Benedikt%2520Mersch%2520and%2520Ignacio%2520Vizzo%2520and%2520Saurabh%2520Gupta%2520and%2520Meher%2520V.%2520R.%2520Malladi%2520and%2520Luca%2520Lobefaro%2520and%2520Guillaume%2520Doisy%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520LiDAR%2520odometry%2520is%2520essential%2520for%2520many%2520robotics%2520applications%252C%2520including%25203D%250Amapping%252C%2520navigation%252C%2520and%2520simultaneous%2520localization%2520and%2520mapping.%2520LiDAR%2520odometry%250Asystems%2520are%2520usually%2520based%2520on%2520some%2520form%2520of%2520point%2520cloud%2520registration%2520to%2520compute%250Athe%2520ego-motion%2520of%2520a%2520mobile%2520robot.%2520Yet%252C%2520few%2520of%2520today%2527s%2520LiDAR%2520odometry%2520systems%250Aconsider%2520domain-specific%2520knowledge%2520or%2520the%2520kinematic%2520model%2520of%2520the%2520mobile%250Aplatform%2520during%2520the%2520point%2520cloud%2520alignment.%2520In%2520this%2520paper%252C%2520we%2520present%250AKinematic-ICP%252C%2520a%2520LiDAR%2520odometry%2520system%2520that%2520focuses%2520on%2520wheeled%2520mobile%2520robots%250Aequipped%2520with%2520a%25203D%2520LiDAR%2520and%2520moving%2520on%2520a%2520planar%2520surface%252C%2520which%2520is%2520a%2520common%250Aassumption%2520for%2520warehouses%252C%2520offices%252C%2520hospitals%252C%2520etc.%2520Our%2520approach%2520introduces%250Akinematic%2520constraints%2520within%2520the%2520optimization%2520of%2520a%2520traditional%2520point-to-point%250Aiterative%2520closest%2520point%2520scheme.%2520In%2520this%2520way%252C%2520the%2520resulting%2520motion%2520follows%2520the%250Akinematic%2520constraints%2520of%2520the%2520platform%252C%2520effectively%2520exploiting%2520the%2520robot%2527s%2520wheel%250Aodometry%2520and%2520the%25203D%2520LiDAR%2520observations.%2520We%2520dynamically%2520adjust%2520the%2520influence%2520of%250ALiDAR%2520measurements%2520and%2520wheel%2520odometry%2520in%2520our%2520optimization%2520scheme%252C%2520allowing%2520the%250Asystem%2520to%2520handle%2520degenerate%2520scenarios%2520such%2520as%2520feature-poor%2520corridors.%2520We%250Aevaluate%2520our%2520approach%2520on%2520robots%2520operating%2520in%2520large-scale%2520warehouse%250Aenvironments%252C%2520but%2520also%2520outdoors.%2520The%2520experiments%2520show%2520that%2520our%2520approach%250Aachieves%2520top%2520performances%2520and%2520is%2520more%2520accurate%2520than%2520wheel%2520odometry%2520and%2520common%250ALiDAR%2520odometry%2520systems.%2520Kinematic-ICP%2520has%2520been%2520recently%2520deployed%2520in%2520the%2520Dexory%250Afleet%2520of%2520robots%2520operating%2520in%2520warehouses%2520worldwide%2520at%2520their%2520customers%2527%2520sites%252C%250Ashowing%2520that%2520our%2520method%2520can%2520run%2520in%2520the%2520real%2520world%2520alongside%2520a%2520complete%250Anavigation%2520stack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10277v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinematic-ICP%3A%20Enhancing%20LiDAR%20Odometry%20with%20Kinematic%20Constraints%20for%0A%20%20Wheeled%20Mobile%20Robots%20Moving%20on%20Planar%20Surfaces&entry.906535625=Tiziano%20Guadagnino%20and%20Benedikt%20Mersch%20and%20Ignacio%20Vizzo%20and%20Saurabh%20Gupta%20and%20Meher%20V.%20R.%20Malladi%20and%20Luca%20Lobefaro%20and%20Guillaume%20Doisy%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20LiDAR%20odometry%20is%20essential%20for%20many%20robotics%20applications%2C%20including%203D%0Amapping%2C%20navigation%2C%20and%20simultaneous%20localization%20and%20mapping.%20LiDAR%20odometry%0Asystems%20are%20usually%20based%20on%20some%20form%20of%20point%20cloud%20registration%20to%20compute%0Athe%20ego-motion%20of%20a%20mobile%20robot.%20Yet%2C%20few%20of%20today%27s%20LiDAR%20odometry%20systems%0Aconsider%20domain-specific%20knowledge%20or%20the%20kinematic%20model%20of%20the%20mobile%0Aplatform%20during%20the%20point%20cloud%20alignment.%20In%20this%20paper%2C%20we%20present%0AKinematic-ICP%2C%20a%20LiDAR%20odometry%20system%20that%20focuses%20on%20wheeled%20mobile%20robots%0Aequipped%20with%20a%203D%20LiDAR%20and%20moving%20on%20a%20planar%20surface%2C%20which%20is%20a%20common%0Aassumption%20for%20warehouses%2C%20offices%2C%20hospitals%2C%20etc.%20Our%20approach%20introduces%0Akinematic%20constraints%20within%20the%20optimization%20of%20a%20traditional%20point-to-point%0Aiterative%20closest%20point%20scheme.%20In%20this%20way%2C%20the%20resulting%20motion%20follows%20the%0Akinematic%20constraints%20of%20the%20platform%2C%20effectively%20exploiting%20the%20robot%27s%20wheel%0Aodometry%20and%20the%203D%20LiDAR%20observations.%20We%20dynamically%20adjust%20the%20influence%20of%0ALiDAR%20measurements%20and%20wheel%20odometry%20in%20our%20optimization%20scheme%2C%20allowing%20the%0Asystem%20to%20handle%20degenerate%20scenarios%20such%20as%20feature-poor%20corridors.%20We%0Aevaluate%20our%20approach%20on%20robots%20operating%20in%20large-scale%20warehouse%0Aenvironments%2C%20but%20also%20outdoors.%20The%20experiments%20show%20that%20our%20approach%0Aachieves%20top%20performances%20and%20is%20more%20accurate%20than%20wheel%20odometry%20and%20common%0ALiDAR%20odometry%20systems.%20Kinematic-ICP%20has%20been%20recently%20deployed%20in%20the%20Dexory%0Afleet%20of%20robots%20operating%20in%20warehouses%20worldwide%20at%20their%20customers%27%20sites%2C%0Ashowing%20that%20our%20method%20can%20run%20in%20the%20real%20world%20alongside%20a%20complete%0Anavigation%20stack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10277v3&entry.124074799=Read"},
{"title": "Goku: Flow Based Video Generative Foundation Models", "author": "Shoufa Chen and Chongjian Ge and Yuqi Zhang and Yida Zhang and Fengda Zhu and Hao Yang and Hongxiang Hao and Hui Wu and Zhichao Lai and Yifei Hu and Ting-Che Lin and Shilong Zhang and Fu Li and Chuan Li and Xing Wang and Yanghua Peng and Peize Sun and Ping Luo and Yi Jiang and Zehuan Yuan and Bingyue Peng and Xiaobing Liu", "abstract": "  This paper introduces Goku, a state-of-the-art family of joint\nimage-and-video generation models leveraging rectified flow Transformers to\nachieve industry-leading performance. We detail the foundational elements\nenabling high-quality visual generation, including the data curation pipeline,\nmodel architecture design, flow formulation, and advanced infrastructure for\nefficient and robust large-scale training. The Goku models demonstrate superior\nperformance in both qualitative and quantitative evaluations, setting new\nbenchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and\n83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for\ntext-to-video tasks. We believe that this work provides valuable insights and\npractical advancements for the research community in developing joint\nimage-and-video generation models.\n", "link": "http://arxiv.org/abs/2502.04896v2", "date": "2025-02-10", "relevancy": 2.3537, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6432}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5812}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goku%3A%20Flow%20Based%20Video%20Generative%20Foundation%20Models&body=Title%3A%20Goku%3A%20Flow%20Based%20Video%20Generative%20Foundation%20Models%0AAuthor%3A%20Shoufa%20Chen%20and%20Chongjian%20Ge%20and%20Yuqi%20Zhang%20and%20Yida%20Zhang%20and%20Fengda%20Zhu%20and%20Hao%20Yang%20and%20Hongxiang%20Hao%20and%20Hui%20Wu%20and%20Zhichao%20Lai%20and%20Yifei%20Hu%20and%20Ting-Che%20Lin%20and%20Shilong%20Zhang%20and%20Fu%20Li%20and%20Chuan%20Li%20and%20Xing%20Wang%20and%20Yanghua%20Peng%20and%20Peize%20Sun%20and%20Ping%20Luo%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Xiaobing%20Liu%0AAbstract%3A%20%20%20This%20paper%20introduces%20Goku%2C%20a%20state-of-the-art%20family%20of%20joint%0Aimage-and-video%20generation%20models%20leveraging%20rectified%20flow%20Transformers%20to%0Aachieve%20industry-leading%20performance.%20We%20detail%20the%20foundational%20elements%0Aenabling%20high-quality%20visual%20generation%2C%20including%20the%20data%20curation%20pipeline%2C%0Amodel%20architecture%20design%2C%20flow%20formulation%2C%20and%20advanced%20infrastructure%20for%0Aefficient%20and%20robust%20large-scale%20training.%20The%20Goku%20models%20demonstrate%20superior%0Aperformance%20in%20both%20qualitative%20and%20quantitative%20evaluations%2C%20setting%20new%0Abenchmarks%20across%20major%20tasks.%20Specifically%2C%20Goku%20achieves%200.76%20on%20GenEval%20and%0A83.65%20on%20DPG-Bench%20for%20text-to-image%20generation%2C%20and%2084.85%20on%20VBench%20for%0Atext-to-video%20tasks.%20We%20believe%20that%20this%20work%20provides%20valuable%20insights%20and%0Apractical%20advancements%20for%20the%20research%20community%20in%20developing%20joint%0Aimage-and-video%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoku%253A%2520Flow%2520Based%2520Video%2520Generative%2520Foundation%2520Models%26entry.906535625%3DShoufa%2520Chen%2520and%2520Chongjian%2520Ge%2520and%2520Yuqi%2520Zhang%2520and%2520Yida%2520Zhang%2520and%2520Fengda%2520Zhu%2520and%2520Hao%2520Yang%2520and%2520Hongxiang%2520Hao%2520and%2520Hui%2520Wu%2520and%2520Zhichao%2520Lai%2520and%2520Yifei%2520Hu%2520and%2520Ting-Che%2520Lin%2520and%2520Shilong%2520Zhang%2520and%2520Fu%2520Li%2520and%2520Chuan%2520Li%2520and%2520Xing%2520Wang%2520and%2520Yanghua%2520Peng%2520and%2520Peize%2520Sun%2520and%2520Ping%2520Luo%2520and%2520Yi%2520Jiang%2520and%2520Zehuan%2520Yuan%2520and%2520Bingyue%2520Peng%2520and%2520Xiaobing%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Goku%252C%2520a%2520state-of-the-art%2520family%2520of%2520joint%250Aimage-and-video%2520generation%2520models%2520leveraging%2520rectified%2520flow%2520Transformers%2520to%250Aachieve%2520industry-leading%2520performance.%2520We%2520detail%2520the%2520foundational%2520elements%250Aenabling%2520high-quality%2520visual%2520generation%252C%2520including%2520the%2520data%2520curation%2520pipeline%252C%250Amodel%2520architecture%2520design%252C%2520flow%2520formulation%252C%2520and%2520advanced%2520infrastructure%2520for%250Aefficient%2520and%2520robust%2520large-scale%2520training.%2520The%2520Goku%2520models%2520demonstrate%2520superior%250Aperformance%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluations%252C%2520setting%2520new%250Abenchmarks%2520across%2520major%2520tasks.%2520Specifically%252C%2520Goku%2520achieves%25200.76%2520on%2520GenEval%2520and%250A83.65%2520on%2520DPG-Bench%2520for%2520text-to-image%2520generation%252C%2520and%252084.85%2520on%2520VBench%2520for%250Atext-to-video%2520tasks.%2520We%2520believe%2520that%2520this%2520work%2520provides%2520valuable%2520insights%2520and%250Apractical%2520advancements%2520for%2520the%2520research%2520community%2520in%2520developing%2520joint%250Aimage-and-video%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goku%3A%20Flow%20Based%20Video%20Generative%20Foundation%20Models&entry.906535625=Shoufa%20Chen%20and%20Chongjian%20Ge%20and%20Yuqi%20Zhang%20and%20Yida%20Zhang%20and%20Fengda%20Zhu%20and%20Hao%20Yang%20and%20Hongxiang%20Hao%20and%20Hui%20Wu%20and%20Zhichao%20Lai%20and%20Yifei%20Hu%20and%20Ting-Che%20Lin%20and%20Shilong%20Zhang%20and%20Fu%20Li%20and%20Chuan%20Li%20and%20Xing%20Wang%20and%20Yanghua%20Peng%20and%20Peize%20Sun%20and%20Ping%20Luo%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Xiaobing%20Liu&entry.1292438233=%20%20This%20paper%20introduces%20Goku%2C%20a%20state-of-the-art%20family%20of%20joint%0Aimage-and-video%20generation%20models%20leveraging%20rectified%20flow%20Transformers%20to%0Aachieve%20industry-leading%20performance.%20We%20detail%20the%20foundational%20elements%0Aenabling%20high-quality%20visual%20generation%2C%20including%20the%20data%20curation%20pipeline%2C%0Amodel%20architecture%20design%2C%20flow%20formulation%2C%20and%20advanced%20infrastructure%20for%0Aefficient%20and%20robust%20large-scale%20training.%20The%20Goku%20models%20demonstrate%20superior%0Aperformance%20in%20both%20qualitative%20and%20quantitative%20evaluations%2C%20setting%20new%0Abenchmarks%20across%20major%20tasks.%20Specifically%2C%20Goku%20achieves%200.76%20on%20GenEval%20and%0A83.65%20on%20DPG-Bench%20for%20text-to-image%20generation%2C%20and%2084.85%20on%20VBench%20for%0Atext-to-video%20tasks.%20We%20believe%20that%20this%20work%20provides%20valuable%20insights%20and%0Apractical%20advancements%20for%20the%20research%20community%20in%20developing%20joint%0Aimage-and-video%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04896v2&entry.124074799=Read"},
{"title": "When Witnesses Defend: A Witness Graph Topological Layer for Adversarial\n  Graph Learning", "author": "Naheed Anjum Arafat and Debabrota Basu and Yulia Gel and Yuzhou Chen", "abstract": "  Capitalizing on the intuitive premise that shape characteristics are more\nrobust to perturbations, we bridge adversarial graph learning with the emerging\ntools from computational topology, namely, persistent homology representations\nof graphs. We introduce the concept of witness complex to adversarial analysis\non graphs, which allows us to focus only on the salient shape characteristics\nof graphs, yielded by the subset of the most essential nodes (i.e., landmarks),\nwith minimal loss of topological information on the whole graph. The remaining\nnodes are then used as witnesses, governing which higher-order graph\nsubstructures are incorporated into the learning process. Armed with the\nwitness mechanism, we design Witness Graph Topological Layer (WGTL), which\nsystematically integrates both local and global topological graph feature\nrepresentations, the impact of which is, in turn, automatically controlled by\nthe robust regularized topological loss. Given the attacker's budget, we derive\nthe important stability guarantees of both local and global topology encodings\nand the associated robust topological loss. We illustrate the versatility and\nefficiency of WGTL by its integration with five GNNs and three existing\nnon-topological defense mechanisms. Our extensive experiments across six\ndatasets demonstrate that WGTL boosts the robustness of GNNs across a range of\nperturbations and against a range of adversarial attacks. Our datasets and\nsource codes are available at https://github.com/toggled/WGTL.\n", "link": "http://arxiv.org/abs/2409.14161v3", "date": "2025-02-10", "relevancy": 2.3419, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4857}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4645}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Witnesses%20Defend%3A%20A%20Witness%20Graph%20Topological%20Layer%20for%20Adversarial%0A%20%20Graph%20Learning&body=Title%3A%20When%20Witnesses%20Defend%3A%20A%20Witness%20Graph%20Topological%20Layer%20for%20Adversarial%0A%20%20Graph%20Learning%0AAuthor%3A%20Naheed%20Anjum%20Arafat%20and%20Debabrota%20Basu%20and%20Yulia%20Gel%20and%20Yuzhou%20Chen%0AAbstract%3A%20%20%20Capitalizing%20on%20the%20intuitive%20premise%20that%20shape%20characteristics%20are%20more%0Arobust%20to%20perturbations%2C%20we%20bridge%20adversarial%20graph%20learning%20with%20the%20emerging%0Atools%20from%20computational%20topology%2C%20namely%2C%20persistent%20homology%20representations%0Aof%20graphs.%20We%20introduce%20the%20concept%20of%20witness%20complex%20to%20adversarial%20analysis%0Aon%20graphs%2C%20which%20allows%20us%20to%20focus%20only%20on%20the%20salient%20shape%20characteristics%0Aof%20graphs%2C%20yielded%20by%20the%20subset%20of%20the%20most%20essential%20nodes%20%28i.e.%2C%20landmarks%29%2C%0Awith%20minimal%20loss%20of%20topological%20information%20on%20the%20whole%20graph.%20The%20remaining%0Anodes%20are%20then%20used%20as%20witnesses%2C%20governing%20which%20higher-order%20graph%0Asubstructures%20are%20incorporated%20into%20the%20learning%20process.%20Armed%20with%20the%0Awitness%20mechanism%2C%20we%20design%20Witness%20Graph%20Topological%20Layer%20%28WGTL%29%2C%20which%0Asystematically%20integrates%20both%20local%20and%20global%20topological%20graph%20feature%0Arepresentations%2C%20the%20impact%20of%20which%20is%2C%20in%20turn%2C%20automatically%20controlled%20by%0Athe%20robust%20regularized%20topological%20loss.%20Given%20the%20attacker%27s%20budget%2C%20we%20derive%0Athe%20important%20stability%20guarantees%20of%20both%20local%20and%20global%20topology%20encodings%0Aand%20the%20associated%20robust%20topological%20loss.%20We%20illustrate%20the%20versatility%20and%0Aefficiency%20of%20WGTL%20by%20its%20integration%20with%20five%20GNNs%20and%20three%20existing%0Anon-topological%20defense%20mechanisms.%20Our%20extensive%20experiments%20across%20six%0Adatasets%20demonstrate%20that%20WGTL%20boosts%20the%20robustness%20of%20GNNs%20across%20a%20range%20of%0Aperturbations%20and%20against%20a%20range%20of%20adversarial%20attacks.%20Our%20datasets%20and%0Asource%20codes%20are%20available%20at%20https%3A//github.com/toggled/WGTL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14161v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Witnesses%2520Defend%253A%2520A%2520Witness%2520Graph%2520Topological%2520Layer%2520for%2520Adversarial%250A%2520%2520Graph%2520Learning%26entry.906535625%3DNaheed%2520Anjum%2520Arafat%2520and%2520Debabrota%2520Basu%2520and%2520Yulia%2520Gel%2520and%2520Yuzhou%2520Chen%26entry.1292438233%3D%2520%2520Capitalizing%2520on%2520the%2520intuitive%2520premise%2520that%2520shape%2520characteristics%2520are%2520more%250Arobust%2520to%2520perturbations%252C%2520we%2520bridge%2520adversarial%2520graph%2520learning%2520with%2520the%2520emerging%250Atools%2520from%2520computational%2520topology%252C%2520namely%252C%2520persistent%2520homology%2520representations%250Aof%2520graphs.%2520We%2520introduce%2520the%2520concept%2520of%2520witness%2520complex%2520to%2520adversarial%2520analysis%250Aon%2520graphs%252C%2520which%2520allows%2520us%2520to%2520focus%2520only%2520on%2520the%2520salient%2520shape%2520characteristics%250Aof%2520graphs%252C%2520yielded%2520by%2520the%2520subset%2520of%2520the%2520most%2520essential%2520nodes%2520%2528i.e.%252C%2520landmarks%2529%252C%250Awith%2520minimal%2520loss%2520of%2520topological%2520information%2520on%2520the%2520whole%2520graph.%2520The%2520remaining%250Anodes%2520are%2520then%2520used%2520as%2520witnesses%252C%2520governing%2520which%2520higher-order%2520graph%250Asubstructures%2520are%2520incorporated%2520into%2520the%2520learning%2520process.%2520Armed%2520with%2520the%250Awitness%2520mechanism%252C%2520we%2520design%2520Witness%2520Graph%2520Topological%2520Layer%2520%2528WGTL%2529%252C%2520which%250Asystematically%2520integrates%2520both%2520local%2520and%2520global%2520topological%2520graph%2520feature%250Arepresentations%252C%2520the%2520impact%2520of%2520which%2520is%252C%2520in%2520turn%252C%2520automatically%2520controlled%2520by%250Athe%2520robust%2520regularized%2520topological%2520loss.%2520Given%2520the%2520attacker%2527s%2520budget%252C%2520we%2520derive%250Athe%2520important%2520stability%2520guarantees%2520of%2520both%2520local%2520and%2520global%2520topology%2520encodings%250Aand%2520the%2520associated%2520robust%2520topological%2520loss.%2520We%2520illustrate%2520the%2520versatility%2520and%250Aefficiency%2520of%2520WGTL%2520by%2520its%2520integration%2520with%2520five%2520GNNs%2520and%2520three%2520existing%250Anon-topological%2520defense%2520mechanisms.%2520Our%2520extensive%2520experiments%2520across%2520six%250Adatasets%2520demonstrate%2520that%2520WGTL%2520boosts%2520the%2520robustness%2520of%2520GNNs%2520across%2520a%2520range%2520of%250Aperturbations%2520and%2520against%2520a%2520range%2520of%2520adversarial%2520attacks.%2520Our%2520datasets%2520and%250Asource%2520codes%2520are%2520available%2520at%2520https%253A//github.com/toggled/WGTL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14161v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Witnesses%20Defend%3A%20A%20Witness%20Graph%20Topological%20Layer%20for%20Adversarial%0A%20%20Graph%20Learning&entry.906535625=Naheed%20Anjum%20Arafat%20and%20Debabrota%20Basu%20and%20Yulia%20Gel%20and%20Yuzhou%20Chen&entry.1292438233=%20%20Capitalizing%20on%20the%20intuitive%20premise%20that%20shape%20characteristics%20are%20more%0Arobust%20to%20perturbations%2C%20we%20bridge%20adversarial%20graph%20learning%20with%20the%20emerging%0Atools%20from%20computational%20topology%2C%20namely%2C%20persistent%20homology%20representations%0Aof%20graphs.%20We%20introduce%20the%20concept%20of%20witness%20complex%20to%20adversarial%20analysis%0Aon%20graphs%2C%20which%20allows%20us%20to%20focus%20only%20on%20the%20salient%20shape%20characteristics%0Aof%20graphs%2C%20yielded%20by%20the%20subset%20of%20the%20most%20essential%20nodes%20%28i.e.%2C%20landmarks%29%2C%0Awith%20minimal%20loss%20of%20topological%20information%20on%20the%20whole%20graph.%20The%20remaining%0Anodes%20are%20then%20used%20as%20witnesses%2C%20governing%20which%20higher-order%20graph%0Asubstructures%20are%20incorporated%20into%20the%20learning%20process.%20Armed%20with%20the%0Awitness%20mechanism%2C%20we%20design%20Witness%20Graph%20Topological%20Layer%20%28WGTL%29%2C%20which%0Asystematically%20integrates%20both%20local%20and%20global%20topological%20graph%20feature%0Arepresentations%2C%20the%20impact%20of%20which%20is%2C%20in%20turn%2C%20automatically%20controlled%20by%0Athe%20robust%20regularized%20topological%20loss.%20Given%20the%20attacker%27s%20budget%2C%20we%20derive%0Athe%20important%20stability%20guarantees%20of%20both%20local%20and%20global%20topology%20encodings%0Aand%20the%20associated%20robust%20topological%20loss.%20We%20illustrate%20the%20versatility%20and%0Aefficiency%20of%20WGTL%20by%20its%20integration%20with%20five%20GNNs%20and%20three%20existing%0Anon-topological%20defense%20mechanisms.%20Our%20extensive%20experiments%20across%20six%0Adatasets%20demonstrate%20that%20WGTL%20boosts%20the%20robustness%20of%20GNNs%20across%20a%20range%20of%0Aperturbations%20and%20against%20a%20range%20of%20adversarial%20attacks.%20Our%20datasets%20and%0Asource%20codes%20are%20available%20at%20https%3A//github.com/toggled/WGTL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14161v3&entry.124074799=Read"},
{"title": "Whole-Body Teleoperation for Mobile Manipulation at Zero Added Cost", "author": "Daniel Honerkamp and Harsh Mahesheka and Jan Ole von Hartz and Tim Welschehold and Abhinav Valada", "abstract": "  Demonstration data plays a key role in learning complex behaviors and\ntraining robotic foundation models. While effective control interfaces exist\nfor static manipulators, data collection remains cumbersome and time intensive\nfor mobile manipulators due to their large number of degrees of freedom. While\nspecialized hardware, avatars, or motion tracking can enable whole-body\ncontrol, these approaches are either expensive, robot-specific, or suffer from\nthe embodiment mismatch between robot and human demonstrator. In this work, we\npresent MoMa-Teleop, a novel teleoperation method that infers end-effector\nmotions from existing interfaces and delegates the base motions to a previously\ndeveloped reinforcement learning agent, leaving the operator to focus fully on\nthe task-relevant end-effector motions. This enables whole-body teleoperation\nof mobile manipulators with no additional hardware or setup costs via standard\ninterfaces such as joysticks or hand guidance. Moreover, the operator is not\nbound to a tracked workspace and can move freely with the robot over spatially\nextended tasks. We demonstrate that our approach results in a significant\nreduction in task completion time across a variety of robots and tasks. As the\ngenerated data covers diverse whole-body motions without embodiment mismatch,\nit enables efficient imitation learning. By focusing on task-specific\nend-effector motions, our approach learns skills that transfer to unseen\nsettings, such as new obstacles or changed object positions, from as little as\nfive demonstrations. We make code and videos available at\nhttps://moma-teleop.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2409.15095v2", "date": "2025-02-10", "relevancy": 2.3399, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5832}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whole-Body%20Teleoperation%20for%20Mobile%20Manipulation%20at%20Zero%20Added%20Cost&body=Title%3A%20Whole-Body%20Teleoperation%20for%20Mobile%20Manipulation%20at%20Zero%20Added%20Cost%0AAuthor%3A%20Daniel%20Honerkamp%20and%20Harsh%20Mahesheka%20and%20Jan%20Ole%20von%20Hartz%20and%20Tim%20Welschehold%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Demonstration%20data%20plays%20a%20key%20role%20in%20learning%20complex%20behaviors%20and%0Atraining%20robotic%20foundation%20models.%20While%20effective%20control%20interfaces%20exist%0Afor%20static%20manipulators%2C%20data%20collection%20remains%20cumbersome%20and%20time%20intensive%0Afor%20mobile%20manipulators%20due%20to%20their%20large%20number%20of%20degrees%20of%20freedom.%20While%0Aspecialized%20hardware%2C%20avatars%2C%20or%20motion%20tracking%20can%20enable%20whole-body%0Acontrol%2C%20these%20approaches%20are%20either%20expensive%2C%20robot-specific%2C%20or%20suffer%20from%0Athe%20embodiment%20mismatch%20between%20robot%20and%20human%20demonstrator.%20In%20this%20work%2C%20we%0Apresent%20MoMa-Teleop%2C%20a%20novel%20teleoperation%20method%20that%20infers%20end-effector%0Amotions%20from%20existing%20interfaces%20and%20delegates%20the%20base%20motions%20to%20a%20previously%0Adeveloped%20reinforcement%20learning%20agent%2C%20leaving%20the%20operator%20to%20focus%20fully%20on%0Athe%20task-relevant%20end-effector%20motions.%20This%20enables%20whole-body%20teleoperation%0Aof%20mobile%20manipulators%20with%20no%20additional%20hardware%20or%20setup%20costs%20via%20standard%0Ainterfaces%20such%20as%20joysticks%20or%20hand%20guidance.%20Moreover%2C%20the%20operator%20is%20not%0Abound%20to%20a%20tracked%20workspace%20and%20can%20move%20freely%20with%20the%20robot%20over%20spatially%0Aextended%20tasks.%20We%20demonstrate%20that%20our%20approach%20results%20in%20a%20significant%0Areduction%20in%20task%20completion%20time%20across%20a%20variety%20of%20robots%20and%20tasks.%20As%20the%0Agenerated%20data%20covers%20diverse%20whole-body%20motions%20without%20embodiment%20mismatch%2C%0Ait%20enables%20efficient%20imitation%20learning.%20By%20focusing%20on%20task-specific%0Aend-effector%20motions%2C%20our%20approach%20learns%20skills%20that%20transfer%20to%20unseen%0Asettings%2C%20such%20as%20new%20obstacles%20or%20changed%20object%20positions%2C%20from%20as%20little%20as%0Afive%20demonstrations.%20We%20make%20code%20and%20videos%20available%20at%0Ahttps%3A//moma-teleop.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhole-Body%2520Teleoperation%2520for%2520Mobile%2520Manipulation%2520at%2520Zero%2520Added%2520Cost%26entry.906535625%3DDaniel%2520Honerkamp%2520and%2520Harsh%2520Mahesheka%2520and%2520Jan%2520Ole%2520von%2520Hartz%2520and%2520Tim%2520Welschehold%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Demonstration%2520data%2520plays%2520a%2520key%2520role%2520in%2520learning%2520complex%2520behaviors%2520and%250Atraining%2520robotic%2520foundation%2520models.%2520While%2520effective%2520control%2520interfaces%2520exist%250Afor%2520static%2520manipulators%252C%2520data%2520collection%2520remains%2520cumbersome%2520and%2520time%2520intensive%250Afor%2520mobile%2520manipulators%2520due%2520to%2520their%2520large%2520number%2520of%2520degrees%2520of%2520freedom.%2520While%250Aspecialized%2520hardware%252C%2520avatars%252C%2520or%2520motion%2520tracking%2520can%2520enable%2520whole-body%250Acontrol%252C%2520these%2520approaches%2520are%2520either%2520expensive%252C%2520robot-specific%252C%2520or%2520suffer%2520from%250Athe%2520embodiment%2520mismatch%2520between%2520robot%2520and%2520human%2520demonstrator.%2520In%2520this%2520work%252C%2520we%250Apresent%2520MoMa-Teleop%252C%2520a%2520novel%2520teleoperation%2520method%2520that%2520infers%2520end-effector%250Amotions%2520from%2520existing%2520interfaces%2520and%2520delegates%2520the%2520base%2520motions%2520to%2520a%2520previously%250Adeveloped%2520reinforcement%2520learning%2520agent%252C%2520leaving%2520the%2520operator%2520to%2520focus%2520fully%2520on%250Athe%2520task-relevant%2520end-effector%2520motions.%2520This%2520enables%2520whole-body%2520teleoperation%250Aof%2520mobile%2520manipulators%2520with%2520no%2520additional%2520hardware%2520or%2520setup%2520costs%2520via%2520standard%250Ainterfaces%2520such%2520as%2520joysticks%2520or%2520hand%2520guidance.%2520Moreover%252C%2520the%2520operator%2520is%2520not%250Abound%2520to%2520a%2520tracked%2520workspace%2520and%2520can%2520move%2520freely%2520with%2520the%2520robot%2520over%2520spatially%250Aextended%2520tasks.%2520We%2520demonstrate%2520that%2520our%2520approach%2520results%2520in%2520a%2520significant%250Areduction%2520in%2520task%2520completion%2520time%2520across%2520a%2520variety%2520of%2520robots%2520and%2520tasks.%2520As%2520the%250Agenerated%2520data%2520covers%2520diverse%2520whole-body%2520motions%2520without%2520embodiment%2520mismatch%252C%250Ait%2520enables%2520efficient%2520imitation%2520learning.%2520By%2520focusing%2520on%2520task-specific%250Aend-effector%2520motions%252C%2520our%2520approach%2520learns%2520skills%2520that%2520transfer%2520to%2520unseen%250Asettings%252C%2520such%2520as%2520new%2520obstacles%2520or%2520changed%2520object%2520positions%252C%2520from%2520as%2520little%2520as%250Afive%2520demonstrations.%2520We%2520make%2520code%2520and%2520videos%2520available%2520at%250Ahttps%253A//moma-teleop.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whole-Body%20Teleoperation%20for%20Mobile%20Manipulation%20at%20Zero%20Added%20Cost&entry.906535625=Daniel%20Honerkamp%20and%20Harsh%20Mahesheka%20and%20Jan%20Ole%20von%20Hartz%20and%20Tim%20Welschehold%20and%20Abhinav%20Valada&entry.1292438233=%20%20Demonstration%20data%20plays%20a%20key%20role%20in%20learning%20complex%20behaviors%20and%0Atraining%20robotic%20foundation%20models.%20While%20effective%20control%20interfaces%20exist%0Afor%20static%20manipulators%2C%20data%20collection%20remains%20cumbersome%20and%20time%20intensive%0Afor%20mobile%20manipulators%20due%20to%20their%20large%20number%20of%20degrees%20of%20freedom.%20While%0Aspecialized%20hardware%2C%20avatars%2C%20or%20motion%20tracking%20can%20enable%20whole-body%0Acontrol%2C%20these%20approaches%20are%20either%20expensive%2C%20robot-specific%2C%20or%20suffer%20from%0Athe%20embodiment%20mismatch%20between%20robot%20and%20human%20demonstrator.%20In%20this%20work%2C%20we%0Apresent%20MoMa-Teleop%2C%20a%20novel%20teleoperation%20method%20that%20infers%20end-effector%0Amotions%20from%20existing%20interfaces%20and%20delegates%20the%20base%20motions%20to%20a%20previously%0Adeveloped%20reinforcement%20learning%20agent%2C%20leaving%20the%20operator%20to%20focus%20fully%20on%0Athe%20task-relevant%20end-effector%20motions.%20This%20enables%20whole-body%20teleoperation%0Aof%20mobile%20manipulators%20with%20no%20additional%20hardware%20or%20setup%20costs%20via%20standard%0Ainterfaces%20such%20as%20joysticks%20or%20hand%20guidance.%20Moreover%2C%20the%20operator%20is%20not%0Abound%20to%20a%20tracked%20workspace%20and%20can%20move%20freely%20with%20the%20robot%20over%20spatially%0Aextended%20tasks.%20We%20demonstrate%20that%20our%20approach%20results%20in%20a%20significant%0Areduction%20in%20task%20completion%20time%20across%20a%20variety%20of%20robots%20and%20tasks.%20As%20the%0Agenerated%20data%20covers%20diverse%20whole-body%20motions%20without%20embodiment%20mismatch%2C%0Ait%20enables%20efficient%20imitation%20learning.%20By%20focusing%20on%20task-specific%0Aend-effector%20motions%2C%20our%20approach%20learns%20skills%20that%20transfer%20to%20unseen%0Asettings%2C%20such%20as%20new%20obstacles%20or%20changed%20object%20positions%2C%20from%20as%20little%20as%0Afive%20demonstrations.%20We%20make%20code%20and%20videos%20available%20at%0Ahttps%3A//moma-teleop.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15095v2&entry.124074799=Read"},
{"title": "MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video\n  Retrieval", "author": "Reno Kriz and Kate Sanders and David Etter and Kenton Murray and Cameron Carpenter and Kelly Van Ochten and Hannah Recknor and Jimena Guallar-Blasco and Alexander Martin and Ronald Colaianni and Nolan King and Eugene Yang and Benjamin Van Durme", "abstract": "  Efficiently retrieving and synthesizing information from large-scale\nmultimodal collections has become a critical challenge. However, existing video\nretrieval datasets suffer from scope limitations, primarily focusing on\nmatching descriptive but vague queries with small collections of professionally\nedited, English-centric videos. To address this gap, we introduce\n$\\textbf{MultiVENT 2.0}$, a large-scale, multilingual event-centric video\nretrieval benchmark featuring a collection of more than 218,000 news videos and\n3,906 queries targeting specific world events. These queries specifically\ntarget information found in the visual content, audio, embedded text, and text\nmetadata of the videos, requiring systems leverage all these sources to succeed\nat the task. Preliminary results show that state-of-the-art vision-language\nmodels struggle significantly with this task, and while alternative approaches\nshow promise, they are still insufficient to adequately address this problem.\nThese findings underscore the need for more robust multimodal retrieval\nsystems, as effective video retrieval is a crucial step towards multimodal\ncontent understanding and generation.\n", "link": "http://arxiv.org/abs/2410.11619v2", "date": "2025-02-10", "relevancy": 2.3374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiVENT%202.0%3A%20A%20Massive%20Multilingual%20Benchmark%20for%20Event-Centric%20Video%0A%20%20Retrieval&body=Title%3A%20MultiVENT%202.0%3A%20A%20Massive%20Multilingual%20Benchmark%20for%20Event-Centric%20Video%0A%20%20Retrieval%0AAuthor%3A%20Reno%20Kriz%20and%20Kate%20Sanders%20and%20David%20Etter%20and%20Kenton%20Murray%20and%20Cameron%20Carpenter%20and%20Kelly%20Van%20Ochten%20and%20Hannah%20Recknor%20and%20Jimena%20Guallar-Blasco%20and%20Alexander%20Martin%20and%20Ronald%20Colaianni%20and%20Nolan%20King%20and%20Eugene%20Yang%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20Efficiently%20retrieving%20and%20synthesizing%20information%20from%20large-scale%0Amultimodal%20collections%20has%20become%20a%20critical%20challenge.%20However%2C%20existing%20video%0Aretrieval%20datasets%20suffer%20from%20scope%20limitations%2C%20primarily%20focusing%20on%0Amatching%20descriptive%20but%20vague%20queries%20with%20small%20collections%20of%20professionally%0Aedited%2C%20English-centric%20videos.%20To%20address%20this%20gap%2C%20we%20introduce%0A%24%5Ctextbf%7BMultiVENT%202.0%7D%24%2C%20a%20large-scale%2C%20multilingual%20event-centric%20video%0Aretrieval%20benchmark%20featuring%20a%20collection%20of%20more%20than%20218%2C000%20news%20videos%20and%0A3%2C906%20queries%20targeting%20specific%20world%20events.%20These%20queries%20specifically%0Atarget%20information%20found%20in%20the%20visual%20content%2C%20audio%2C%20embedded%20text%2C%20and%20text%0Ametadata%20of%20the%20videos%2C%20requiring%20systems%20leverage%20all%20these%20sources%20to%20succeed%0Aat%20the%20task.%20Preliminary%20results%20show%20that%20state-of-the-art%20vision-language%0Amodels%20struggle%20significantly%20with%20this%20task%2C%20and%20while%20alternative%20approaches%0Ashow%20promise%2C%20they%20are%20still%20insufficient%20to%20adequately%20address%20this%20problem.%0AThese%20findings%20underscore%20the%20need%20for%20more%20robust%20multimodal%20retrieval%0Asystems%2C%20as%20effective%20video%20retrieval%20is%20a%20crucial%20step%20towards%20multimodal%0Acontent%20understanding%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11619v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiVENT%25202.0%253A%2520A%2520Massive%2520Multilingual%2520Benchmark%2520for%2520Event-Centric%2520Video%250A%2520%2520Retrieval%26entry.906535625%3DReno%2520Kriz%2520and%2520Kate%2520Sanders%2520and%2520David%2520Etter%2520and%2520Kenton%2520Murray%2520and%2520Cameron%2520Carpenter%2520and%2520Kelly%2520Van%2520Ochten%2520and%2520Hannah%2520Recknor%2520and%2520Jimena%2520Guallar-Blasco%2520and%2520Alexander%2520Martin%2520and%2520Ronald%2520Colaianni%2520and%2520Nolan%2520King%2520and%2520Eugene%2520Yang%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520Efficiently%2520retrieving%2520and%2520synthesizing%2520information%2520from%2520large-scale%250Amultimodal%2520collections%2520has%2520become%2520a%2520critical%2520challenge.%2520However%252C%2520existing%2520video%250Aretrieval%2520datasets%2520suffer%2520from%2520scope%2520limitations%252C%2520primarily%2520focusing%2520on%250Amatching%2520descriptive%2520but%2520vague%2520queries%2520with%2520small%2520collections%2520of%2520professionally%250Aedited%252C%2520English-centric%2520videos.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250A%2524%255Ctextbf%257BMultiVENT%25202.0%257D%2524%252C%2520a%2520large-scale%252C%2520multilingual%2520event-centric%2520video%250Aretrieval%2520benchmark%2520featuring%2520a%2520collection%2520of%2520more%2520than%2520218%252C000%2520news%2520videos%2520and%250A3%252C906%2520queries%2520targeting%2520specific%2520world%2520events.%2520These%2520queries%2520specifically%250Atarget%2520information%2520found%2520in%2520the%2520visual%2520content%252C%2520audio%252C%2520embedded%2520text%252C%2520and%2520text%250Ametadata%2520of%2520the%2520videos%252C%2520requiring%2520systems%2520leverage%2520all%2520these%2520sources%2520to%2520succeed%250Aat%2520the%2520task.%2520Preliminary%2520results%2520show%2520that%2520state-of-the-art%2520vision-language%250Amodels%2520struggle%2520significantly%2520with%2520this%2520task%252C%2520and%2520while%2520alternative%2520approaches%250Ashow%2520promise%252C%2520they%2520are%2520still%2520insufficient%2520to%2520adequately%2520address%2520this%2520problem.%250AThese%2520findings%2520underscore%2520the%2520need%2520for%2520more%2520robust%2520multimodal%2520retrieval%250Asystems%252C%2520as%2520effective%2520video%2520retrieval%2520is%2520a%2520crucial%2520step%2520towards%2520multimodal%250Acontent%2520understanding%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11619v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiVENT%202.0%3A%20A%20Massive%20Multilingual%20Benchmark%20for%20Event-Centric%20Video%0A%20%20Retrieval&entry.906535625=Reno%20Kriz%20and%20Kate%20Sanders%20and%20David%20Etter%20and%20Kenton%20Murray%20and%20Cameron%20Carpenter%20and%20Kelly%20Van%20Ochten%20and%20Hannah%20Recknor%20and%20Jimena%20Guallar-Blasco%20and%20Alexander%20Martin%20and%20Ronald%20Colaianni%20and%20Nolan%20King%20and%20Eugene%20Yang%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20Efficiently%20retrieving%20and%20synthesizing%20information%20from%20large-scale%0Amultimodal%20collections%20has%20become%20a%20critical%20challenge.%20However%2C%20existing%20video%0Aretrieval%20datasets%20suffer%20from%20scope%20limitations%2C%20primarily%20focusing%20on%0Amatching%20descriptive%20but%20vague%20queries%20with%20small%20collections%20of%20professionally%0Aedited%2C%20English-centric%20videos.%20To%20address%20this%20gap%2C%20we%20introduce%0A%24%5Ctextbf%7BMultiVENT%202.0%7D%24%2C%20a%20large-scale%2C%20multilingual%20event-centric%20video%0Aretrieval%20benchmark%20featuring%20a%20collection%20of%20more%20than%20218%2C000%20news%20videos%20and%0A3%2C906%20queries%20targeting%20specific%20world%20events.%20These%20queries%20specifically%0Atarget%20information%20found%20in%20the%20visual%20content%2C%20audio%2C%20embedded%20text%2C%20and%20text%0Ametadata%20of%20the%20videos%2C%20requiring%20systems%20leverage%20all%20these%20sources%20to%20succeed%0Aat%20the%20task.%20Preliminary%20results%20show%20that%20state-of-the-art%20vision-language%0Amodels%20struggle%20significantly%20with%20this%20task%2C%20and%20while%20alternative%20approaches%0Ashow%20promise%2C%20they%20are%20still%20insufficient%20to%20adequately%20address%20this%20problem.%0AThese%20findings%20underscore%20the%20need%20for%20more%20robust%20multimodal%20retrieval%0Asystems%2C%20as%20effective%20video%20retrieval%20is%20a%20crucial%20step%20towards%20multimodal%0Acontent%20understanding%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11619v2&entry.124074799=Read"},
{"title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence\n  Parallelism", "author": "Yujie Wang and Shiju Wang and Shenhan Zhu and Fangcheng Fu and Xinyi Liu and Xuefeng Xiao and Huixia Li and Jiashi Li and Faming Wu and Bin Cui", "abstract": "  Extending the context length (i.e., the maximum supported sequence length) of\nLLMs is of paramount significance. To facilitate long context training of LLMs,\nsequence parallelism has emerged as an essential technique, which scatters each\ninput sequence across multiple devices and necessitates communication to\nprocess the sequence. In essence, existing sequence parallelism methods assume\nhomogeneous sequence lengths (i.e., all input sequences are equal in length)\nand therefore leverages a single, static scattering strategy for all input\nsequences. However, in reality, the sequence lengths in LLM training corpora\nexhibit substantial variability, often following a long-tail distribution,\nwhich leads to workload heterogeneity. In this paper, we show that employing a\nsingle, static strategy results in inefficiency and resource under-utilization,\nhighlighting the need for adaptive approaches to handle the heterogeneous\nworkloads across sequences. To address this, we propose a\nheterogeneity-adaptive sequence parallelism method. For each training step, our\napproach captures the variability in sequence lengths and assigns the optimal\ncombination of scattering strategies based on workload characteristics. We\nmodel this problem as a linear programming optimization and design an efficient\nand effective solver to find the optimal solution. Furthermore, we implement\nour method in a high-performance system that supports adaptive parallelization\nin distributed LLM training. Experimental results demonstrate that our system\noutperforms state-of-the-art training frameworks by up to 1.98x.\n", "link": "http://arxiv.org/abs/2412.01523v2", "date": "2025-02-10", "relevancy": 2.337, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexSP%3A%20Accelerating%20Large%20Language%20Model%20Training%20via%20Flexible%20Sequence%0A%20%20Parallelism&body=Title%3A%20FlexSP%3A%20Accelerating%20Large%20Language%20Model%20Training%20via%20Flexible%20Sequence%0A%20%20Parallelism%0AAuthor%3A%20Yujie%20Wang%20and%20Shiju%20Wang%20and%20Shenhan%20Zhu%20and%20Fangcheng%20Fu%20and%20Xinyi%20Liu%20and%20Xuefeng%20Xiao%20and%20Huixia%20Li%20and%20Jiashi%20Li%20and%20Faming%20Wu%20and%20Bin%20Cui%0AAbstract%3A%20%20%20Extending%20the%20context%20length%20%28i.e.%2C%20the%20maximum%20supported%20sequence%20length%29%20of%0ALLMs%20is%20of%20paramount%20significance.%20To%20facilitate%20long%20context%20training%20of%20LLMs%2C%0Asequence%20parallelism%20has%20emerged%20as%20an%20essential%20technique%2C%20which%20scatters%20each%0Ainput%20sequence%20across%20multiple%20devices%20and%20necessitates%20communication%20to%0Aprocess%20the%20sequence.%20In%20essence%2C%20existing%20sequence%20parallelism%20methods%20assume%0Ahomogeneous%20sequence%20lengths%20%28i.e.%2C%20all%20input%20sequences%20are%20equal%20in%20length%29%0Aand%20therefore%20leverages%20a%20single%2C%20static%20scattering%20strategy%20for%20all%20input%0Asequences.%20However%2C%20in%20reality%2C%20the%20sequence%20lengths%20in%20LLM%20training%20corpora%0Aexhibit%20substantial%20variability%2C%20often%20following%20a%20long-tail%20distribution%2C%0Awhich%20leads%20to%20workload%20heterogeneity.%20In%20this%20paper%2C%20we%20show%20that%20employing%20a%0Asingle%2C%20static%20strategy%20results%20in%20inefficiency%20and%20resource%20under-utilization%2C%0Ahighlighting%20the%20need%20for%20adaptive%20approaches%20to%20handle%20the%20heterogeneous%0Aworkloads%20across%20sequences.%20To%20address%20this%2C%20we%20propose%20a%0Aheterogeneity-adaptive%20sequence%20parallelism%20method.%20For%20each%20training%20step%2C%20our%0Aapproach%20captures%20the%20variability%20in%20sequence%20lengths%20and%20assigns%20the%20optimal%0Acombination%20of%20scattering%20strategies%20based%20on%20workload%20characteristics.%20We%0Amodel%20this%20problem%20as%20a%20linear%20programming%20optimization%20and%20design%20an%20efficient%0Aand%20effective%20solver%20to%20find%20the%20optimal%20solution.%20Furthermore%2C%20we%20implement%0Aour%20method%20in%20a%20high-performance%20system%20that%20supports%20adaptive%20parallelization%0Ain%20distributed%20LLM%20training.%20Experimental%20results%20demonstrate%20that%20our%20system%0Aoutperforms%20state-of-the-art%20training%20frameworks%20by%20up%20to%201.98x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexSP%253A%2520Accelerating%2520Large%2520Language%2520Model%2520Training%2520via%2520Flexible%2520Sequence%250A%2520%2520Parallelism%26entry.906535625%3DYujie%2520Wang%2520and%2520Shiju%2520Wang%2520and%2520Shenhan%2520Zhu%2520and%2520Fangcheng%2520Fu%2520and%2520Xinyi%2520Liu%2520and%2520Xuefeng%2520Xiao%2520and%2520Huixia%2520Li%2520and%2520Jiashi%2520Li%2520and%2520Faming%2520Wu%2520and%2520Bin%2520Cui%26entry.1292438233%3D%2520%2520Extending%2520the%2520context%2520length%2520%2528i.e.%252C%2520the%2520maximum%2520supported%2520sequence%2520length%2529%2520of%250ALLMs%2520is%2520of%2520paramount%2520significance.%2520To%2520facilitate%2520long%2520context%2520training%2520of%2520LLMs%252C%250Asequence%2520parallelism%2520has%2520emerged%2520as%2520an%2520essential%2520technique%252C%2520which%2520scatters%2520each%250Ainput%2520sequence%2520across%2520multiple%2520devices%2520and%2520necessitates%2520communication%2520to%250Aprocess%2520the%2520sequence.%2520In%2520essence%252C%2520existing%2520sequence%2520parallelism%2520methods%2520assume%250Ahomogeneous%2520sequence%2520lengths%2520%2528i.e.%252C%2520all%2520input%2520sequences%2520are%2520equal%2520in%2520length%2529%250Aand%2520therefore%2520leverages%2520a%2520single%252C%2520static%2520scattering%2520strategy%2520for%2520all%2520input%250Asequences.%2520However%252C%2520in%2520reality%252C%2520the%2520sequence%2520lengths%2520in%2520LLM%2520training%2520corpora%250Aexhibit%2520substantial%2520variability%252C%2520often%2520following%2520a%2520long-tail%2520distribution%252C%250Awhich%2520leads%2520to%2520workload%2520heterogeneity.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520employing%2520a%250Asingle%252C%2520static%2520strategy%2520results%2520in%2520inefficiency%2520and%2520resource%2520under-utilization%252C%250Ahighlighting%2520the%2520need%2520for%2520adaptive%2520approaches%2520to%2520handle%2520the%2520heterogeneous%250Aworkloads%2520across%2520sequences.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Aheterogeneity-adaptive%2520sequence%2520parallelism%2520method.%2520For%2520each%2520training%2520step%252C%2520our%250Aapproach%2520captures%2520the%2520variability%2520in%2520sequence%2520lengths%2520and%2520assigns%2520the%2520optimal%250Acombination%2520of%2520scattering%2520strategies%2520based%2520on%2520workload%2520characteristics.%2520We%250Amodel%2520this%2520problem%2520as%2520a%2520linear%2520programming%2520optimization%2520and%2520design%2520an%2520efficient%250Aand%2520effective%2520solver%2520to%2520find%2520the%2520optimal%2520solution.%2520Furthermore%252C%2520we%2520implement%250Aour%2520method%2520in%2520a%2520high-performance%2520system%2520that%2520supports%2520adaptive%2520parallelization%250Ain%2520distributed%2520LLM%2520training.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520system%250Aoutperforms%2520state-of-the-art%2520training%2520frameworks%2520by%2520up%2520to%25201.98x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexSP%3A%20Accelerating%20Large%20Language%20Model%20Training%20via%20Flexible%20Sequence%0A%20%20Parallelism&entry.906535625=Yujie%20Wang%20and%20Shiju%20Wang%20and%20Shenhan%20Zhu%20and%20Fangcheng%20Fu%20and%20Xinyi%20Liu%20and%20Xuefeng%20Xiao%20and%20Huixia%20Li%20and%20Jiashi%20Li%20and%20Faming%20Wu%20and%20Bin%20Cui&entry.1292438233=%20%20Extending%20the%20context%20length%20%28i.e.%2C%20the%20maximum%20supported%20sequence%20length%29%20of%0ALLMs%20is%20of%20paramount%20significance.%20To%20facilitate%20long%20context%20training%20of%20LLMs%2C%0Asequence%20parallelism%20has%20emerged%20as%20an%20essential%20technique%2C%20which%20scatters%20each%0Ainput%20sequence%20across%20multiple%20devices%20and%20necessitates%20communication%20to%0Aprocess%20the%20sequence.%20In%20essence%2C%20existing%20sequence%20parallelism%20methods%20assume%0Ahomogeneous%20sequence%20lengths%20%28i.e.%2C%20all%20input%20sequences%20are%20equal%20in%20length%29%0Aand%20therefore%20leverages%20a%20single%2C%20static%20scattering%20strategy%20for%20all%20input%0Asequences.%20However%2C%20in%20reality%2C%20the%20sequence%20lengths%20in%20LLM%20training%20corpora%0Aexhibit%20substantial%20variability%2C%20often%20following%20a%20long-tail%20distribution%2C%0Awhich%20leads%20to%20workload%20heterogeneity.%20In%20this%20paper%2C%20we%20show%20that%20employing%20a%0Asingle%2C%20static%20strategy%20results%20in%20inefficiency%20and%20resource%20under-utilization%2C%0Ahighlighting%20the%20need%20for%20adaptive%20approaches%20to%20handle%20the%20heterogeneous%0Aworkloads%20across%20sequences.%20To%20address%20this%2C%20we%20propose%20a%0Aheterogeneity-adaptive%20sequence%20parallelism%20method.%20For%20each%20training%20step%2C%20our%0Aapproach%20captures%20the%20variability%20in%20sequence%20lengths%20and%20assigns%20the%20optimal%0Acombination%20of%20scattering%20strategies%20based%20on%20workload%20characteristics.%20We%0Amodel%20this%20problem%20as%20a%20linear%20programming%20optimization%20and%20design%20an%20efficient%0Aand%20effective%20solver%20to%20find%20the%20optimal%20solution.%20Furthermore%2C%20we%20implement%0Aour%20method%20in%20a%20high-performance%20system%20that%20supports%20adaptive%20parallelization%0Ain%20distributed%20LLM%20training.%20Experimental%20results%20demonstrate%20that%20our%20system%0Aoutperforms%20state-of-the-art%20training%20frameworks%20by%20up%20to%201.98x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01523v2&entry.124074799=Read"},
{"title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API", "author": "Damiano Marsili and Rohun Agrawal and Yisong Yue and Georgia Gkioxari", "abstract": "  Visual reasoning -- the ability to interpret the visual world -- is crucial\nfor embodied agents that operate within three-dimensional scenes. Progress in\nAI has led to vision and language models capable of answering questions from\nimages. However, their performance declines when tasked with 3D spatial\nreasoning. To tackle the complexity of such reasoning problems, we introduce an\nagentic program synthesis approach where LLM agents collaboratively generate a\nPythonic API with new functions to solve common subproblems. Our method\novercomes limitations of prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To assess AI capabilities\nfor 3D understanding, we introduce a new benchmark of queries involving\nmultiple steps of grounding and inference. We show that our method outperforms\nprior zero-shot models for visual reasoning in 3D and empirically validate the\neffectiveness of our agentic framework for 3D spatial reasoning tasks. Project\nwebsite: https://glab-caltech.github.io/vadar/\n", "link": "http://arxiv.org/abs/2502.06787v1", "date": "2025-02-10", "relevancy": 2.3283, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Agentic%20AI%20for%20Spatial%20Reasoning%20with%20a%20Dynamic%20API&body=Title%3A%20Visual%20Agentic%20AI%20for%20Spatial%20Reasoning%20with%20a%20Dynamic%20API%0AAuthor%3A%20Damiano%20Marsili%20and%20Rohun%20Agrawal%20and%20Yisong%20Yue%20and%20Georgia%20Gkioxari%0AAbstract%3A%20%20%20Visual%20reasoning%20--%20the%20ability%20to%20interpret%20the%20visual%20world%20--%20is%20crucial%0Afor%20embodied%20agents%20that%20operate%20within%20three-dimensional%20scenes.%20Progress%20in%0AAI%20has%20led%20to%20vision%20and%20language%20models%20capable%20of%20answering%20questions%20from%0Aimages.%20However%2C%20their%20performance%20declines%20when%20tasked%20with%203D%20spatial%0Areasoning.%20To%20tackle%20the%20complexity%20of%20such%20reasoning%20problems%2C%20we%20introduce%20an%0Aagentic%20program%20synthesis%20approach%20where%20LLM%20agents%20collaboratively%20generate%20a%0APythonic%20API%20with%20new%20functions%20to%20solve%20common%20subproblems.%20Our%20method%0Aovercomes%20limitations%20of%20prior%20approaches%20that%20rely%20on%20a%20static%2C%20human-defined%0AAPI%2C%20allowing%20it%20to%20handle%20a%20wider%20range%20of%20queries.%20To%20assess%20AI%20capabilities%0Afor%203D%20understanding%2C%20we%20introduce%20a%20new%20benchmark%20of%20queries%20involving%0Amultiple%20steps%20of%20grounding%20and%20inference.%20We%20show%20that%20our%20method%20outperforms%0Aprior%20zero-shot%20models%20for%20visual%20reasoning%20in%203D%20and%20empirically%20validate%20the%0Aeffectiveness%20of%20our%20agentic%20framework%20for%203D%20spatial%20reasoning%20tasks.%20Project%0Awebsite%3A%20https%3A//glab-caltech.github.io/vadar/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Agentic%2520AI%2520for%2520Spatial%2520Reasoning%2520with%2520a%2520Dynamic%2520API%26entry.906535625%3DDamiano%2520Marsili%2520and%2520Rohun%2520Agrawal%2520and%2520Yisong%2520Yue%2520and%2520Georgia%2520Gkioxari%26entry.1292438233%3D%2520%2520Visual%2520reasoning%2520--%2520the%2520ability%2520to%2520interpret%2520the%2520visual%2520world%2520--%2520is%2520crucial%250Afor%2520embodied%2520agents%2520that%2520operate%2520within%2520three-dimensional%2520scenes.%2520Progress%2520in%250AAI%2520has%2520led%2520to%2520vision%2520and%2520language%2520models%2520capable%2520of%2520answering%2520questions%2520from%250Aimages.%2520However%252C%2520their%2520performance%2520declines%2520when%2520tasked%2520with%25203D%2520spatial%250Areasoning.%2520To%2520tackle%2520the%2520complexity%2520of%2520such%2520reasoning%2520problems%252C%2520we%2520introduce%2520an%250Aagentic%2520program%2520synthesis%2520approach%2520where%2520LLM%2520agents%2520collaboratively%2520generate%2520a%250APythonic%2520API%2520with%2520new%2520functions%2520to%2520solve%2520common%2520subproblems.%2520Our%2520method%250Aovercomes%2520limitations%2520of%2520prior%2520approaches%2520that%2520rely%2520on%2520a%2520static%252C%2520human-defined%250AAPI%252C%2520allowing%2520it%2520to%2520handle%2520a%2520wider%2520range%2520of%2520queries.%2520To%2520assess%2520AI%2520capabilities%250Afor%25203D%2520understanding%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520of%2520queries%2520involving%250Amultiple%2520steps%2520of%2520grounding%2520and%2520inference.%2520We%2520show%2520that%2520our%2520method%2520outperforms%250Aprior%2520zero-shot%2520models%2520for%2520visual%2520reasoning%2520in%25203D%2520and%2520empirically%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520agentic%2520framework%2520for%25203D%2520spatial%2520reasoning%2520tasks.%2520Project%250Awebsite%253A%2520https%253A//glab-caltech.github.io/vadar/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Agentic%20AI%20for%20Spatial%20Reasoning%20with%20a%20Dynamic%20API&entry.906535625=Damiano%20Marsili%20and%20Rohun%20Agrawal%20and%20Yisong%20Yue%20and%20Georgia%20Gkioxari&entry.1292438233=%20%20Visual%20reasoning%20--%20the%20ability%20to%20interpret%20the%20visual%20world%20--%20is%20crucial%0Afor%20embodied%20agents%20that%20operate%20within%20three-dimensional%20scenes.%20Progress%20in%0AAI%20has%20led%20to%20vision%20and%20language%20models%20capable%20of%20answering%20questions%20from%0Aimages.%20However%2C%20their%20performance%20declines%20when%20tasked%20with%203D%20spatial%0Areasoning.%20To%20tackle%20the%20complexity%20of%20such%20reasoning%20problems%2C%20we%20introduce%20an%0Aagentic%20program%20synthesis%20approach%20where%20LLM%20agents%20collaboratively%20generate%20a%0APythonic%20API%20with%20new%20functions%20to%20solve%20common%20subproblems.%20Our%20method%0Aovercomes%20limitations%20of%20prior%20approaches%20that%20rely%20on%20a%20static%2C%20human-defined%0AAPI%2C%20allowing%20it%20to%20handle%20a%20wider%20range%20of%20queries.%20To%20assess%20AI%20capabilities%0Afor%203D%20understanding%2C%20we%20introduce%20a%20new%20benchmark%20of%20queries%20involving%0Amultiple%20steps%20of%20grounding%20and%20inference.%20We%20show%20that%20our%20method%20outperforms%0Aprior%20zero-shot%20models%20for%20visual%20reasoning%20in%203D%20and%20empirically%20validate%20the%0Aeffectiveness%20of%20our%20agentic%20framework%20for%203D%20spatial%20reasoning%20tasks.%20Project%0Awebsite%3A%20https%3A//glab-caltech.github.io/vadar/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06787v1&entry.124074799=Read"},
{"title": "Continual Learning from Simulated Interactions via Multitask Prospective\n  Rehearsal for Bionic Limb Behavior Modeling", "author": "Sharmita Dey and Benjamin Paassen and Sarath Ravindran Nair and Sabri Boughorbel and Arndt F. Schilling", "abstract": "  Lower limb amputations and neuromuscular impairments severely restrict\nmobility, necessitating advancements beyond conventional prosthetics. While\nmotorized bionic limbs show promise, their effectiveness depends on replicating\nthe dynamic coordination of human movement across diverse environments. In this\npaper, we introduce a model for human behavior in the context of bionic\nprosthesis control. Our approach leverages human locomotion demonstrations to\nlearn the synergistic coupling of the lower limbs, enabling the prediction of\nthe kinematic behavior of a missing limb during tasks such as walking, climbing\ninclines, and stairs. We propose a multitasking, continually adaptive model\nthat anticipates and refines movements over time. At the core of our method is\na technique called multitask prospective rehearsal, that anticipates and\nsynthesizes future movements based on the previous prediction and employs a\ncorrective mechanism for subsequent predictions. Our evolving architecture\nmerges lightweight, task-specific modules on a shared backbone, ensuring both\nspecificity and scalability. We validate our model through experiments on\nreal-world human gait datasets, including transtibial amputees, across a wide\nrange of locomotion tasks. Results demonstrate that our approach consistently\noutperforms baseline models, particularly in scenarios with distributional\nshifts, adversarial perturbations, and noise.\n", "link": "http://arxiv.org/abs/2405.01114v2", "date": "2025-02-10", "relevancy": 2.3197, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6485}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5676}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20from%20Simulated%20Interactions%20via%20Multitask%20Prospective%0A%20%20Rehearsal%20for%20Bionic%20Limb%20Behavior%20Modeling&body=Title%3A%20Continual%20Learning%20from%20Simulated%20Interactions%20via%20Multitask%20Prospective%0A%20%20Rehearsal%20for%20Bionic%20Limb%20Behavior%20Modeling%0AAuthor%3A%20Sharmita%20Dey%20and%20Benjamin%20Paassen%20and%20Sarath%20Ravindran%20Nair%20and%20Sabri%20Boughorbel%20and%20Arndt%20F.%20Schilling%0AAbstract%3A%20%20%20Lower%20limb%20amputations%20and%20neuromuscular%20impairments%20severely%20restrict%0Amobility%2C%20necessitating%20advancements%20beyond%20conventional%20prosthetics.%20While%0Amotorized%20bionic%20limbs%20show%20promise%2C%20their%20effectiveness%20depends%20on%20replicating%0Athe%20dynamic%20coordination%20of%20human%20movement%20across%20diverse%20environments.%20In%20this%0Apaper%2C%20we%20introduce%20a%20model%20for%20human%20behavior%20in%20the%20context%20of%20bionic%0Aprosthesis%20control.%20Our%20approach%20leverages%20human%20locomotion%20demonstrations%20to%0Alearn%20the%20synergistic%20coupling%20of%20the%20lower%20limbs%2C%20enabling%20the%20prediction%20of%0Athe%20kinematic%20behavior%20of%20a%20missing%20limb%20during%20tasks%20such%20as%20walking%2C%20climbing%0Ainclines%2C%20and%20stairs.%20We%20propose%20a%20multitasking%2C%20continually%20adaptive%20model%0Athat%20anticipates%20and%20refines%20movements%20over%20time.%20At%20the%20core%20of%20our%20method%20is%0Aa%20technique%20called%20multitask%20prospective%20rehearsal%2C%20that%20anticipates%20and%0Asynthesizes%20future%20movements%20based%20on%20the%20previous%20prediction%20and%20employs%20a%0Acorrective%20mechanism%20for%20subsequent%20predictions.%20Our%20evolving%20architecture%0Amerges%20lightweight%2C%20task-specific%20modules%20on%20a%20shared%20backbone%2C%20ensuring%20both%0Aspecificity%20and%20scalability.%20We%20validate%20our%20model%20through%20experiments%20on%0Areal-world%20human%20gait%20datasets%2C%20including%20transtibial%20amputees%2C%20across%20a%20wide%0Arange%20of%20locomotion%20tasks.%20Results%20demonstrate%20that%20our%20approach%20consistently%0Aoutperforms%20baseline%20models%2C%20particularly%20in%20scenarios%20with%20distributional%0Ashifts%2C%20adversarial%20perturbations%2C%20and%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520from%2520Simulated%2520Interactions%2520via%2520Multitask%2520Prospective%250A%2520%2520Rehearsal%2520for%2520Bionic%2520Limb%2520Behavior%2520Modeling%26entry.906535625%3DSharmita%2520Dey%2520and%2520Benjamin%2520Paassen%2520and%2520Sarath%2520Ravindran%2520Nair%2520and%2520Sabri%2520Boughorbel%2520and%2520Arndt%2520F.%2520Schilling%26entry.1292438233%3D%2520%2520Lower%2520limb%2520amputations%2520and%2520neuromuscular%2520impairments%2520severely%2520restrict%250Amobility%252C%2520necessitating%2520advancements%2520beyond%2520conventional%2520prosthetics.%2520While%250Amotorized%2520bionic%2520limbs%2520show%2520promise%252C%2520their%2520effectiveness%2520depends%2520on%2520replicating%250Athe%2520dynamic%2520coordination%2520of%2520human%2520movement%2520across%2520diverse%2520environments.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520model%2520for%2520human%2520behavior%2520in%2520the%2520context%2520of%2520bionic%250Aprosthesis%2520control.%2520Our%2520approach%2520leverages%2520human%2520locomotion%2520demonstrations%2520to%250Alearn%2520the%2520synergistic%2520coupling%2520of%2520the%2520lower%2520limbs%252C%2520enabling%2520the%2520prediction%2520of%250Athe%2520kinematic%2520behavior%2520of%2520a%2520missing%2520limb%2520during%2520tasks%2520such%2520as%2520walking%252C%2520climbing%250Ainclines%252C%2520and%2520stairs.%2520We%2520propose%2520a%2520multitasking%252C%2520continually%2520adaptive%2520model%250Athat%2520anticipates%2520and%2520refines%2520movements%2520over%2520time.%2520At%2520the%2520core%2520of%2520our%2520method%2520is%250Aa%2520technique%2520called%2520multitask%2520prospective%2520rehearsal%252C%2520that%2520anticipates%2520and%250Asynthesizes%2520future%2520movements%2520based%2520on%2520the%2520previous%2520prediction%2520and%2520employs%2520a%250Acorrective%2520mechanism%2520for%2520subsequent%2520predictions.%2520Our%2520evolving%2520architecture%250Amerges%2520lightweight%252C%2520task-specific%2520modules%2520on%2520a%2520shared%2520backbone%252C%2520ensuring%2520both%250Aspecificity%2520and%2520scalability.%2520We%2520validate%2520our%2520model%2520through%2520experiments%2520on%250Areal-world%2520human%2520gait%2520datasets%252C%2520including%2520transtibial%2520amputees%252C%2520across%2520a%2520wide%250Arange%2520of%2520locomotion%2520tasks.%2520Results%2520demonstrate%2520that%2520our%2520approach%2520consistently%250Aoutperforms%2520baseline%2520models%252C%2520particularly%2520in%2520scenarios%2520with%2520distributional%250Ashifts%252C%2520adversarial%2520perturbations%252C%2520and%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20from%20Simulated%20Interactions%20via%20Multitask%20Prospective%0A%20%20Rehearsal%20for%20Bionic%20Limb%20Behavior%20Modeling&entry.906535625=Sharmita%20Dey%20and%20Benjamin%20Paassen%20and%20Sarath%20Ravindran%20Nair%20and%20Sabri%20Boughorbel%20and%20Arndt%20F.%20Schilling&entry.1292438233=%20%20Lower%20limb%20amputations%20and%20neuromuscular%20impairments%20severely%20restrict%0Amobility%2C%20necessitating%20advancements%20beyond%20conventional%20prosthetics.%20While%0Amotorized%20bionic%20limbs%20show%20promise%2C%20their%20effectiveness%20depends%20on%20replicating%0Athe%20dynamic%20coordination%20of%20human%20movement%20across%20diverse%20environments.%20In%20this%0Apaper%2C%20we%20introduce%20a%20model%20for%20human%20behavior%20in%20the%20context%20of%20bionic%0Aprosthesis%20control.%20Our%20approach%20leverages%20human%20locomotion%20demonstrations%20to%0Alearn%20the%20synergistic%20coupling%20of%20the%20lower%20limbs%2C%20enabling%20the%20prediction%20of%0Athe%20kinematic%20behavior%20of%20a%20missing%20limb%20during%20tasks%20such%20as%20walking%2C%20climbing%0Ainclines%2C%20and%20stairs.%20We%20propose%20a%20multitasking%2C%20continually%20adaptive%20model%0Athat%20anticipates%20and%20refines%20movements%20over%20time.%20At%20the%20core%20of%20our%20method%20is%0Aa%20technique%20called%20multitask%20prospective%20rehearsal%2C%20that%20anticipates%20and%0Asynthesizes%20future%20movements%20based%20on%20the%20previous%20prediction%20and%20employs%20a%0Acorrective%20mechanism%20for%20subsequent%20predictions.%20Our%20evolving%20architecture%0Amerges%20lightweight%2C%20task-specific%20modules%20on%20a%20shared%20backbone%2C%20ensuring%20both%0Aspecificity%20and%20scalability.%20We%20validate%20our%20model%20through%20experiments%20on%0Areal-world%20human%20gait%20datasets%2C%20including%20transtibial%20amputees%2C%20across%20a%20wide%0Arange%20of%20locomotion%20tasks.%20Results%20demonstrate%20that%20our%20approach%20consistently%0Aoutperforms%20baseline%20models%2C%20particularly%20in%20scenarios%20with%20distributional%0Ashifts%2C%20adversarial%20perturbations%2C%20and%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01114v2&entry.124074799=Read"},
{"title": "Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large\n  Language Models", "author": "Tianshuo Xu and Hao Lu and Xu Yan and Yingjie Cai and Bingbing Liu and Yingcong Chen", "abstract": "  Large Language Models (LLMs) have made substantial advancements in the field\nof robotic and autonomous driving. This study presents the first\nOccupancy-based Large Language Model (Occ-LLM), which represents a pioneering\neffort to integrate LLMs with an important representation. To effectively\nencode occupancy as input for the LLM and address the category imbalances\nassociated with occupancy, we propose Motion Separation Variational Autoencoder\n(MS-VAE). This innovative approach utilizes prior knowledge to distinguish\ndynamic objects from static scenes before inputting them into a tailored\nVariational Autoencoder (VAE). This separation enhances the model's capacity to\nconcentrate on dynamic trajectories while effectively reconstructing static\nscenes. The efficacy of Occ-LLM has been validated across key tasks, including\n4D occupancy forecasting, self-ego planning, and occupancy-based scene question\nanswering. Comprehensive evaluations demonstrate that Occ-LLM significantly\nsurpasses existing state-of-the-art methodologies, achieving gains of about 6\\%\nin Intersection over Union (IoU) and 4\\% in mean Intersection over Union (mIoU)\nfor the task of 4D occupancy forecasting. These findings highlight the\ntransformative potential of Occ-LLM in reshaping current paradigms within\nrobotic and autonomous driving.\n", "link": "http://arxiv.org/abs/2502.06419v1", "date": "2025-02-10", "relevancy": 2.3148, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5866}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occ-LLM%3A%20Enhancing%20Autonomous%20Driving%20with%20Occupancy-Based%20Large%0A%20%20Language%20Models&body=Title%3A%20Occ-LLM%3A%20Enhancing%20Autonomous%20Driving%20with%20Occupancy-Based%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Tianshuo%20Xu%20and%20Hao%20Lu%20and%20Xu%20Yan%20and%20Yingjie%20Cai%20and%20Bingbing%20Liu%20and%20Yingcong%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20substantial%20advancements%20in%20the%20field%0Aof%20robotic%20and%20autonomous%20driving.%20This%20study%20presents%20the%20first%0AOccupancy-based%20Large%20Language%20Model%20%28Occ-LLM%29%2C%20which%20represents%20a%20pioneering%0Aeffort%20to%20integrate%20LLMs%20with%20an%20important%20representation.%20To%20effectively%0Aencode%20occupancy%20as%20input%20for%20the%20LLM%20and%20address%20the%20category%20imbalances%0Aassociated%20with%20occupancy%2C%20we%20propose%20Motion%20Separation%20Variational%20Autoencoder%0A%28MS-VAE%29.%20This%20innovative%20approach%20utilizes%20prior%20knowledge%20to%20distinguish%0Adynamic%20objects%20from%20static%20scenes%20before%20inputting%20them%20into%20a%20tailored%0AVariational%20Autoencoder%20%28VAE%29.%20This%20separation%20enhances%20the%20model%27s%20capacity%20to%0Aconcentrate%20on%20dynamic%20trajectories%20while%20effectively%20reconstructing%20static%0Ascenes.%20The%20efficacy%20of%20Occ-LLM%20has%20been%20validated%20across%20key%20tasks%2C%20including%0A4D%20occupancy%20forecasting%2C%20self-ego%20planning%2C%20and%20occupancy-based%20scene%20question%0Aanswering.%20Comprehensive%20evaluations%20demonstrate%20that%20Occ-LLM%20significantly%0Asurpasses%20existing%20state-of-the-art%20methodologies%2C%20achieving%20gains%20of%20about%206%5C%25%0Ain%20Intersection%20over%20Union%20%28IoU%29%20and%204%5C%25%20in%20mean%20Intersection%20over%20Union%20%28mIoU%29%0Afor%20the%20task%20of%204D%20occupancy%20forecasting.%20These%20findings%20highlight%20the%0Atransformative%20potential%20of%20Occ-LLM%20in%20reshaping%20current%20paradigms%20within%0Arobotic%20and%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcc-LLM%253A%2520Enhancing%2520Autonomous%2520Driving%2520with%2520Occupancy-Based%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DTianshuo%2520Xu%2520and%2520Hao%2520Lu%2520and%2520Xu%2520Yan%2520and%2520Yingjie%2520Cai%2520and%2520Bingbing%2520Liu%2520and%2520Yingcong%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520substantial%2520advancements%2520in%2520the%2520field%250Aof%2520robotic%2520and%2520autonomous%2520driving.%2520This%2520study%2520presents%2520the%2520first%250AOccupancy-based%2520Large%2520Language%2520Model%2520%2528Occ-LLM%2529%252C%2520which%2520represents%2520a%2520pioneering%250Aeffort%2520to%2520integrate%2520LLMs%2520with%2520an%2520important%2520representation.%2520To%2520effectively%250Aencode%2520occupancy%2520as%2520input%2520for%2520the%2520LLM%2520and%2520address%2520the%2520category%2520imbalances%250Aassociated%2520with%2520occupancy%252C%2520we%2520propose%2520Motion%2520Separation%2520Variational%2520Autoencoder%250A%2528MS-VAE%2529.%2520This%2520innovative%2520approach%2520utilizes%2520prior%2520knowledge%2520to%2520distinguish%250Adynamic%2520objects%2520from%2520static%2520scenes%2520before%2520inputting%2520them%2520into%2520a%2520tailored%250AVariational%2520Autoencoder%2520%2528VAE%2529.%2520This%2520separation%2520enhances%2520the%2520model%2527s%2520capacity%2520to%250Aconcentrate%2520on%2520dynamic%2520trajectories%2520while%2520effectively%2520reconstructing%2520static%250Ascenes.%2520The%2520efficacy%2520of%2520Occ-LLM%2520has%2520been%2520validated%2520across%2520key%2520tasks%252C%2520including%250A4D%2520occupancy%2520forecasting%252C%2520self-ego%2520planning%252C%2520and%2520occupancy-based%2520scene%2520question%250Aanswering.%2520Comprehensive%2520evaluations%2520demonstrate%2520that%2520Occ-LLM%2520significantly%250Asurpasses%2520existing%2520state-of-the-art%2520methodologies%252C%2520achieving%2520gains%2520of%2520about%25206%255C%2525%250Ain%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520and%25204%255C%2525%2520in%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%250Afor%2520the%2520task%2520of%25204D%2520occupancy%2520forecasting.%2520These%2520findings%2520highlight%2520the%250Atransformative%2520potential%2520of%2520Occ-LLM%2520in%2520reshaping%2520current%2520paradigms%2520within%250Arobotic%2520and%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occ-LLM%3A%20Enhancing%20Autonomous%20Driving%20with%20Occupancy-Based%20Large%0A%20%20Language%20Models&entry.906535625=Tianshuo%20Xu%20and%20Hao%20Lu%20and%20Xu%20Yan%20and%20Yingjie%20Cai%20and%20Bingbing%20Liu%20and%20Yingcong%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20substantial%20advancements%20in%20the%20field%0Aof%20robotic%20and%20autonomous%20driving.%20This%20study%20presents%20the%20first%0AOccupancy-based%20Large%20Language%20Model%20%28Occ-LLM%29%2C%20which%20represents%20a%20pioneering%0Aeffort%20to%20integrate%20LLMs%20with%20an%20important%20representation.%20To%20effectively%0Aencode%20occupancy%20as%20input%20for%20the%20LLM%20and%20address%20the%20category%20imbalances%0Aassociated%20with%20occupancy%2C%20we%20propose%20Motion%20Separation%20Variational%20Autoencoder%0A%28MS-VAE%29.%20This%20innovative%20approach%20utilizes%20prior%20knowledge%20to%20distinguish%0Adynamic%20objects%20from%20static%20scenes%20before%20inputting%20them%20into%20a%20tailored%0AVariational%20Autoencoder%20%28VAE%29.%20This%20separation%20enhances%20the%20model%27s%20capacity%20to%0Aconcentrate%20on%20dynamic%20trajectories%20while%20effectively%20reconstructing%20static%0Ascenes.%20The%20efficacy%20of%20Occ-LLM%20has%20been%20validated%20across%20key%20tasks%2C%20including%0A4D%20occupancy%20forecasting%2C%20self-ego%20planning%2C%20and%20occupancy-based%20scene%20question%0Aanswering.%20Comprehensive%20evaluations%20demonstrate%20that%20Occ-LLM%20significantly%0Asurpasses%20existing%20state-of-the-art%20methodologies%2C%20achieving%20gains%20of%20about%206%5C%25%0Ain%20Intersection%20over%20Union%20%28IoU%29%20and%204%5C%25%20in%20mean%20Intersection%20over%20Union%20%28mIoU%29%0Afor%20the%20task%20of%204D%20occupancy%20forecasting.%20These%20findings%20highlight%20the%0Atransformative%20potential%20of%20Occ-LLM%20in%20reshaping%20current%20paradigms%20within%0Arobotic%20and%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06419v1&entry.124074799=Read"},
{"title": "Occlusion-Aware Contingency Safety-Critical Planning for Autonomous\n  Vehicles", "author": "Lei Zheng and Rui Yang and Minzhe Zheng and Zengqi Peng and Michael Yu Wang and Jun Ma", "abstract": "  Ensuring safe driving while maintaining travel efficiency for autonomous\nvehicles in dynamic and occluded environments is a critical challenge. This\npaper proposes an occlusion-aware contingency safety-critical planning approach\nfor real-time autonomous driving in such environments. Leveraging reachability\nanalysis for risk assessment, forward reachable sets of occluded phantom\nvehicles are computed to quantify dynamic velocity boundaries. These velocity\nboundaries are incorporated into a biconvex nonlinear programming (NLP)\nformulation, enabling simultaneous optimization of exploration and fallback\ntrajectories within a receding horizon planning framework. To facilitate\nreal-time optimization and ensure coordination between trajectories, we employ\nthe consensus alternating direction method of multipliers (ADMM) to decompose\nthe biconvex NLP problem into low-dimensional convex subproblems. The\neffectiveness of the proposed approach is validated through simulation studies\nand real-world experiments in occluded intersections. Experimental results\ndemonstrate enhanced safety and improved travel efficiency, enabling real-time\nsafe trajectory generation in dynamic occluded intersections under varying\nobstacle conditions. A video showcasing the experimental results is available\nat https://youtu.be/CHayG7NChqM.\n", "link": "http://arxiv.org/abs/2502.06359v1", "date": "2025-02-10", "relevancy": 2.3135, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5875}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5753}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occlusion-Aware%20Contingency%20Safety-Critical%20Planning%20for%20Autonomous%0A%20%20Vehicles&body=Title%3A%20Occlusion-Aware%20Contingency%20Safety-Critical%20Planning%20for%20Autonomous%0A%20%20Vehicles%0AAuthor%3A%20Lei%20Zheng%20and%20Rui%20Yang%20and%20Minzhe%20Zheng%20and%20Zengqi%20Peng%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Ensuring%20safe%20driving%20while%20maintaining%20travel%20efficiency%20for%20autonomous%0Avehicles%20in%20dynamic%20and%20occluded%20environments%20is%20a%20critical%20challenge.%20This%0Apaper%20proposes%20an%20occlusion-aware%20contingency%20safety-critical%20planning%20approach%0Afor%20real-time%20autonomous%20driving%20in%20such%20environments.%20Leveraging%20reachability%0Aanalysis%20for%20risk%20assessment%2C%20forward%20reachable%20sets%20of%20occluded%20phantom%0Avehicles%20are%20computed%20to%20quantify%20dynamic%20velocity%20boundaries.%20These%20velocity%0Aboundaries%20are%20incorporated%20into%20a%20biconvex%20nonlinear%20programming%20%28NLP%29%0Aformulation%2C%20enabling%20simultaneous%20optimization%20of%20exploration%20and%20fallback%0Atrajectories%20within%20a%20receding%20horizon%20planning%20framework.%20To%20facilitate%0Areal-time%20optimization%20and%20ensure%20coordination%20between%20trajectories%2C%20we%20employ%0Athe%20consensus%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20to%20decompose%0Athe%20biconvex%20NLP%20problem%20into%20low-dimensional%20convex%20subproblems.%20The%0Aeffectiveness%20of%20the%20proposed%20approach%20is%20validated%20through%20simulation%20studies%0Aand%20real-world%20experiments%20in%20occluded%20intersections.%20Experimental%20results%0Ademonstrate%20enhanced%20safety%20and%20improved%20travel%20efficiency%2C%20enabling%20real-time%0Asafe%20trajectory%20generation%20in%20dynamic%20occluded%20intersections%20under%20varying%0Aobstacle%20conditions.%20A%20video%20showcasing%20the%20experimental%20results%20is%20available%0Aat%20https%3A//youtu.be/CHayG7NChqM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcclusion-Aware%2520Contingency%2520Safety-Critical%2520Planning%2520for%2520Autonomous%250A%2520%2520Vehicles%26entry.906535625%3DLei%2520Zheng%2520and%2520Rui%2520Yang%2520and%2520Minzhe%2520Zheng%2520and%2520Zengqi%2520Peng%2520and%2520Michael%2520Yu%2520Wang%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Ensuring%2520safe%2520driving%2520while%2520maintaining%2520travel%2520efficiency%2520for%2520autonomous%250Avehicles%2520in%2520dynamic%2520and%2520occluded%2520environments%2520is%2520a%2520critical%2520challenge.%2520This%250Apaper%2520proposes%2520an%2520occlusion-aware%2520contingency%2520safety-critical%2520planning%2520approach%250Afor%2520real-time%2520autonomous%2520driving%2520in%2520such%2520environments.%2520Leveraging%2520reachability%250Aanalysis%2520for%2520risk%2520assessment%252C%2520forward%2520reachable%2520sets%2520of%2520occluded%2520phantom%250Avehicles%2520are%2520computed%2520to%2520quantify%2520dynamic%2520velocity%2520boundaries.%2520These%2520velocity%250Aboundaries%2520are%2520incorporated%2520into%2520a%2520biconvex%2520nonlinear%2520programming%2520%2528NLP%2529%250Aformulation%252C%2520enabling%2520simultaneous%2520optimization%2520of%2520exploration%2520and%2520fallback%250Atrajectories%2520within%2520a%2520receding%2520horizon%2520planning%2520framework.%2520To%2520facilitate%250Areal-time%2520optimization%2520and%2520ensure%2520coordination%2520between%2520trajectories%252C%2520we%2520employ%250Athe%2520consensus%2520alternating%2520direction%2520method%2520of%2520multipliers%2520%2528ADMM%2529%2520to%2520decompose%250Athe%2520biconvex%2520NLP%2520problem%2520into%2520low-dimensional%2520convex%2520subproblems.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520approach%2520is%2520validated%2520through%2520simulation%2520studies%250Aand%2520real-world%2520experiments%2520in%2520occluded%2520intersections.%2520Experimental%2520results%250Ademonstrate%2520enhanced%2520safety%2520and%2520improved%2520travel%2520efficiency%252C%2520enabling%2520real-time%250Asafe%2520trajectory%2520generation%2520in%2520dynamic%2520occluded%2520intersections%2520under%2520varying%250Aobstacle%2520conditions.%2520A%2520video%2520showcasing%2520the%2520experimental%2520results%2520is%2520available%250Aat%2520https%253A//youtu.be/CHayG7NChqM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occlusion-Aware%20Contingency%20Safety-Critical%20Planning%20for%20Autonomous%0A%20%20Vehicles&entry.906535625=Lei%20Zheng%20and%20Rui%20Yang%20and%20Minzhe%20Zheng%20and%20Zengqi%20Peng%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma&entry.1292438233=%20%20Ensuring%20safe%20driving%20while%20maintaining%20travel%20efficiency%20for%20autonomous%0Avehicles%20in%20dynamic%20and%20occluded%20environments%20is%20a%20critical%20challenge.%20This%0Apaper%20proposes%20an%20occlusion-aware%20contingency%20safety-critical%20planning%20approach%0Afor%20real-time%20autonomous%20driving%20in%20such%20environments.%20Leveraging%20reachability%0Aanalysis%20for%20risk%20assessment%2C%20forward%20reachable%20sets%20of%20occluded%20phantom%0Avehicles%20are%20computed%20to%20quantify%20dynamic%20velocity%20boundaries.%20These%20velocity%0Aboundaries%20are%20incorporated%20into%20a%20biconvex%20nonlinear%20programming%20%28NLP%29%0Aformulation%2C%20enabling%20simultaneous%20optimization%20of%20exploration%20and%20fallback%0Atrajectories%20within%20a%20receding%20horizon%20planning%20framework.%20To%20facilitate%0Areal-time%20optimization%20and%20ensure%20coordination%20between%20trajectories%2C%20we%20employ%0Athe%20consensus%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20to%20decompose%0Athe%20biconvex%20NLP%20problem%20into%20low-dimensional%20convex%20subproblems.%20The%0Aeffectiveness%20of%20the%20proposed%20approach%20is%20validated%20through%20simulation%20studies%0Aand%20real-world%20experiments%20in%20occluded%20intersections.%20Experimental%20results%0Ademonstrate%20enhanced%20safety%20and%20improved%20travel%20efficiency%2C%20enabling%20real-time%0Asafe%20trajectory%20generation%20in%20dynamic%20occluded%20intersections%20under%20varying%0Aobstacle%20conditions.%20A%20video%20showcasing%20the%20experimental%20results%20is%20available%0Aat%20https%3A//youtu.be/CHayG7NChqM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06359v1&entry.124074799=Read"},
{"title": "Biomechanical Reconstruction with Confidence Intervals from Multiview\n  Markerless Motion Capture", "author": "R. James Cotton and Fabian Sinz", "abstract": "  Advances in multiview markerless motion capture (MMMC) promise high-quality\nmovement analysis for clinical practice and research. While prior validation\nstudies show MMMC performs well on average, they do not provide what is needed\nin clinical practice or for large-scale utilization of MMMC -- confidence\nintervals over specific kinematic estimates from a specific individual analyzed\nusing a possibly unique camera configuration. We extend our previous work using\nan implicit representation of trajectories optimized end-to-end through a\ndifferentiable biomechanical model to learn the posterior probability\ndistribution over pose given all the detected keypoints. This posterior\nprobability is learned through a variational approximation and estimates\nconfidence intervals for individual joints at each moment in a trial, showing\nconfidence intervals generally within 10-15 mm of spatial error for virtual\nmarker locations, consistent with our prior validation studies. Confidence\nintervals over joint angles are typically only a few degrees and widen for more\ndistal joints. The posterior also models the correlation structure over joint\nangles, such as correlations between hip and pelvis angles. The confidence\nintervals estimated through this method allow us to identify times and trials\nwhere kinematic uncertainty is high.\n", "link": "http://arxiv.org/abs/2502.06486v1", "date": "2025-02-10", "relevancy": 2.3093, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6001}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5776}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biomechanical%20Reconstruction%20with%20Confidence%20Intervals%20from%20Multiview%0A%20%20Markerless%20Motion%20Capture&body=Title%3A%20Biomechanical%20Reconstruction%20with%20Confidence%20Intervals%20from%20Multiview%0A%20%20Markerless%20Motion%20Capture%0AAuthor%3A%20R.%20James%20Cotton%20and%20Fabian%20Sinz%0AAbstract%3A%20%20%20Advances%20in%20multiview%20markerless%20motion%20capture%20%28MMMC%29%20promise%20high-quality%0Amovement%20analysis%20for%20clinical%20practice%20and%20research.%20While%20prior%20validation%0Astudies%20show%20MMMC%20performs%20well%20on%20average%2C%20they%20do%20not%20provide%20what%20is%20needed%0Ain%20clinical%20practice%20or%20for%20large-scale%20utilization%20of%20MMMC%20--%20confidence%0Aintervals%20over%20specific%20kinematic%20estimates%20from%20a%20specific%20individual%20analyzed%0Ausing%20a%20possibly%20unique%20camera%20configuration.%20We%20extend%20our%20previous%20work%20using%0Aan%20implicit%20representation%20of%20trajectories%20optimized%20end-to-end%20through%20a%0Adifferentiable%20biomechanical%20model%20to%20learn%20the%20posterior%20probability%0Adistribution%20over%20pose%20given%20all%20the%20detected%20keypoints.%20This%20posterior%0Aprobability%20is%20learned%20through%20a%20variational%20approximation%20and%20estimates%0Aconfidence%20intervals%20for%20individual%20joints%20at%20each%20moment%20in%20a%20trial%2C%20showing%0Aconfidence%20intervals%20generally%20within%2010-15%20mm%20of%20spatial%20error%20for%20virtual%0Amarker%20locations%2C%20consistent%20with%20our%20prior%20validation%20studies.%20Confidence%0Aintervals%20over%20joint%20angles%20are%20typically%20only%20a%20few%20degrees%20and%20widen%20for%20more%0Adistal%20joints.%20The%20posterior%20also%20models%20the%20correlation%20structure%20over%20joint%0Aangles%2C%20such%20as%20correlations%20between%20hip%20and%20pelvis%20angles.%20The%20confidence%0Aintervals%20estimated%20through%20this%20method%20allow%20us%20to%20identify%20times%20and%20trials%0Awhere%20kinematic%20uncertainty%20is%20high.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomechanical%2520Reconstruction%2520with%2520Confidence%2520Intervals%2520from%2520Multiview%250A%2520%2520Markerless%2520Motion%2520Capture%26entry.906535625%3DR.%2520James%2520Cotton%2520and%2520Fabian%2520Sinz%26entry.1292438233%3D%2520%2520Advances%2520in%2520multiview%2520markerless%2520motion%2520capture%2520%2528MMMC%2529%2520promise%2520high-quality%250Amovement%2520analysis%2520for%2520clinical%2520practice%2520and%2520research.%2520While%2520prior%2520validation%250Astudies%2520show%2520MMMC%2520performs%2520well%2520on%2520average%252C%2520they%2520do%2520not%2520provide%2520what%2520is%2520needed%250Ain%2520clinical%2520practice%2520or%2520for%2520large-scale%2520utilization%2520of%2520MMMC%2520--%2520confidence%250Aintervals%2520over%2520specific%2520kinematic%2520estimates%2520from%2520a%2520specific%2520individual%2520analyzed%250Ausing%2520a%2520possibly%2520unique%2520camera%2520configuration.%2520We%2520extend%2520our%2520previous%2520work%2520using%250Aan%2520implicit%2520representation%2520of%2520trajectories%2520optimized%2520end-to-end%2520through%2520a%250Adifferentiable%2520biomechanical%2520model%2520to%2520learn%2520the%2520posterior%2520probability%250Adistribution%2520over%2520pose%2520given%2520all%2520the%2520detected%2520keypoints.%2520This%2520posterior%250Aprobability%2520is%2520learned%2520through%2520a%2520variational%2520approximation%2520and%2520estimates%250Aconfidence%2520intervals%2520for%2520individual%2520joints%2520at%2520each%2520moment%2520in%2520a%2520trial%252C%2520showing%250Aconfidence%2520intervals%2520generally%2520within%252010-15%2520mm%2520of%2520spatial%2520error%2520for%2520virtual%250Amarker%2520locations%252C%2520consistent%2520with%2520our%2520prior%2520validation%2520studies.%2520Confidence%250Aintervals%2520over%2520joint%2520angles%2520are%2520typically%2520only%2520a%2520few%2520degrees%2520and%2520widen%2520for%2520more%250Adistal%2520joints.%2520The%2520posterior%2520also%2520models%2520the%2520correlation%2520structure%2520over%2520joint%250Aangles%252C%2520such%2520as%2520correlations%2520between%2520hip%2520and%2520pelvis%2520angles.%2520The%2520confidence%250Aintervals%2520estimated%2520through%2520this%2520method%2520allow%2520us%2520to%2520identify%2520times%2520and%2520trials%250Awhere%2520kinematic%2520uncertainty%2520is%2520high.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biomechanical%20Reconstruction%20with%20Confidence%20Intervals%20from%20Multiview%0A%20%20Markerless%20Motion%20Capture&entry.906535625=R.%20James%20Cotton%20and%20Fabian%20Sinz&entry.1292438233=%20%20Advances%20in%20multiview%20markerless%20motion%20capture%20%28MMMC%29%20promise%20high-quality%0Amovement%20analysis%20for%20clinical%20practice%20and%20research.%20While%20prior%20validation%0Astudies%20show%20MMMC%20performs%20well%20on%20average%2C%20they%20do%20not%20provide%20what%20is%20needed%0Ain%20clinical%20practice%20or%20for%20large-scale%20utilization%20of%20MMMC%20--%20confidence%0Aintervals%20over%20specific%20kinematic%20estimates%20from%20a%20specific%20individual%20analyzed%0Ausing%20a%20possibly%20unique%20camera%20configuration.%20We%20extend%20our%20previous%20work%20using%0Aan%20implicit%20representation%20of%20trajectories%20optimized%20end-to-end%20through%20a%0Adifferentiable%20biomechanical%20model%20to%20learn%20the%20posterior%20probability%0Adistribution%20over%20pose%20given%20all%20the%20detected%20keypoints.%20This%20posterior%0Aprobability%20is%20learned%20through%20a%20variational%20approximation%20and%20estimates%0Aconfidence%20intervals%20for%20individual%20joints%20at%20each%20moment%20in%20a%20trial%2C%20showing%0Aconfidence%20intervals%20generally%20within%2010-15%20mm%20of%20spatial%20error%20for%20virtual%0Amarker%20locations%2C%20consistent%20with%20our%20prior%20validation%20studies.%20Confidence%0Aintervals%20over%20joint%20angles%20are%20typically%20only%20a%20few%20degrees%20and%20widen%20for%20more%0Adistal%20joints.%20The%20posterior%20also%20models%20the%20correlation%20structure%20over%20joint%0Aangles%2C%20such%20as%20correlations%20between%20hip%20and%20pelvis%20angles.%20The%20confidence%0Aintervals%20estimated%20through%20this%20method%20allow%20us%20to%20identify%20times%20and%20trials%0Awhere%20kinematic%20uncertainty%20is%20high.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06486v1&entry.124074799=Read"},
{"title": "Adaptive Perception for Unified Visual Multi-modal Object Tracking", "author": "Xiantao Hu and Bineng Zhong and Qihua Liang and Zhiyi Mo and Liangtao Shi and Ying Tai and Jian Yang", "abstract": "  Recently, many multi-modal trackers prioritize RGB as the dominant modality,\ntreating other modalities as auxiliary, and fine-tuning separately various\nmulti-modal tasks. This imbalance in modality dependence limits the ability of\nmethods to dynamically utilize complementary information from each modality in\ncomplex scenarios, making it challenging to fully perceive the advantages of\nmulti-modal. As a result, a unified parameter model often underperforms in\nvarious multi-modal tracking tasks. To address this issue, we propose APTrack,\na novel unified tracker designed for multi-modal adaptive perception. Unlike\nprevious methods, APTrack explores a unified representation through an equal\nmodeling strategy. This strategy allows the model to dynamically adapt to\nvarious modalities and tasks without requiring additional fine-tuning between\ndifferent tasks. Moreover, our tracker integrates an adaptive modality\ninteraction (AMI) module that efficiently bridges cross-modality interactions\nby generating learnable tokens. Experiments conducted on five diverse\nmulti-modal datasets (RGBT234, LasHeR, VisEvent, DepthTrack, and VOT-RGBD2022)\ndemonstrate that APTrack not only surpasses existing state-of-the-art unified\nmulti-modal trackers but also outperforms trackers designed for specific\nmulti-modal tasks.\n", "link": "http://arxiv.org/abs/2502.06583v1", "date": "2025-02-10", "relevancy": 2.3083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5748}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Perception%20for%20Unified%20Visual%20Multi-modal%20Object%20Tracking&body=Title%3A%20Adaptive%20Perception%20for%20Unified%20Visual%20Multi-modal%20Object%20Tracking%0AAuthor%3A%20Xiantao%20Hu%20and%20Bineng%20Zhong%20and%20Qihua%20Liang%20and%20Zhiyi%20Mo%20and%20Liangtao%20Shi%20and%20Ying%20Tai%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Recently%2C%20many%20multi-modal%20trackers%20prioritize%20RGB%20as%20the%20dominant%20modality%2C%0Atreating%20other%20modalities%20as%20auxiliary%2C%20and%20fine-tuning%20separately%20various%0Amulti-modal%20tasks.%20This%20imbalance%20in%20modality%20dependence%20limits%20the%20ability%20of%0Amethods%20to%20dynamically%20utilize%20complementary%20information%20from%20each%20modality%20in%0Acomplex%20scenarios%2C%20making%20it%20challenging%20to%20fully%20perceive%20the%20advantages%20of%0Amulti-modal.%20As%20a%20result%2C%20a%20unified%20parameter%20model%20often%20underperforms%20in%0Avarious%20multi-modal%20tracking%20tasks.%20To%20address%20this%20issue%2C%20we%20propose%20APTrack%2C%0Aa%20novel%20unified%20tracker%20designed%20for%20multi-modal%20adaptive%20perception.%20Unlike%0Aprevious%20methods%2C%20APTrack%20explores%20a%20unified%20representation%20through%20an%20equal%0Amodeling%20strategy.%20This%20strategy%20allows%20the%20model%20to%20dynamically%20adapt%20to%0Avarious%20modalities%20and%20tasks%20without%20requiring%20additional%20fine-tuning%20between%0Adifferent%20tasks.%20Moreover%2C%20our%20tracker%20integrates%20an%20adaptive%20modality%0Ainteraction%20%28AMI%29%20module%20that%20efficiently%20bridges%20cross-modality%20interactions%0Aby%20generating%20learnable%20tokens.%20Experiments%20conducted%20on%20five%20diverse%0Amulti-modal%20datasets%20%28RGBT234%2C%20LasHeR%2C%20VisEvent%2C%20DepthTrack%2C%20and%20VOT-RGBD2022%29%0Ademonstrate%20that%20APTrack%20not%20only%20surpasses%20existing%20state-of-the-art%20unified%0Amulti-modal%20trackers%20but%20also%20outperforms%20trackers%20designed%20for%20specific%0Amulti-modal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Perception%2520for%2520Unified%2520Visual%2520Multi-modal%2520Object%2520Tracking%26entry.906535625%3DXiantao%2520Hu%2520and%2520Bineng%2520Zhong%2520and%2520Qihua%2520Liang%2520and%2520Zhiyi%2520Mo%2520and%2520Liangtao%2520Shi%2520and%2520Ying%2520Tai%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Recently%252C%2520many%2520multi-modal%2520trackers%2520prioritize%2520RGB%2520as%2520the%2520dominant%2520modality%252C%250Atreating%2520other%2520modalities%2520as%2520auxiliary%252C%2520and%2520fine-tuning%2520separately%2520various%250Amulti-modal%2520tasks.%2520This%2520imbalance%2520in%2520modality%2520dependence%2520limits%2520the%2520ability%2520of%250Amethods%2520to%2520dynamically%2520utilize%2520complementary%2520information%2520from%2520each%2520modality%2520in%250Acomplex%2520scenarios%252C%2520making%2520it%2520challenging%2520to%2520fully%2520perceive%2520the%2520advantages%2520of%250Amulti-modal.%2520As%2520a%2520result%252C%2520a%2520unified%2520parameter%2520model%2520often%2520underperforms%2520in%250Avarious%2520multi-modal%2520tracking%2520tasks.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520APTrack%252C%250Aa%2520novel%2520unified%2520tracker%2520designed%2520for%2520multi-modal%2520adaptive%2520perception.%2520Unlike%250Aprevious%2520methods%252C%2520APTrack%2520explores%2520a%2520unified%2520representation%2520through%2520an%2520equal%250Amodeling%2520strategy.%2520This%2520strategy%2520allows%2520the%2520model%2520to%2520dynamically%2520adapt%2520to%250Avarious%2520modalities%2520and%2520tasks%2520without%2520requiring%2520additional%2520fine-tuning%2520between%250Adifferent%2520tasks.%2520Moreover%252C%2520our%2520tracker%2520integrates%2520an%2520adaptive%2520modality%250Ainteraction%2520%2528AMI%2529%2520module%2520that%2520efficiently%2520bridges%2520cross-modality%2520interactions%250Aby%2520generating%2520learnable%2520tokens.%2520Experiments%2520conducted%2520on%2520five%2520diverse%250Amulti-modal%2520datasets%2520%2528RGBT234%252C%2520LasHeR%252C%2520VisEvent%252C%2520DepthTrack%252C%2520and%2520VOT-RGBD2022%2529%250Ademonstrate%2520that%2520APTrack%2520not%2520only%2520surpasses%2520existing%2520state-of-the-art%2520unified%250Amulti-modal%2520trackers%2520but%2520also%2520outperforms%2520trackers%2520designed%2520for%2520specific%250Amulti-modal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Perception%20for%20Unified%20Visual%20Multi-modal%20Object%20Tracking&entry.906535625=Xiantao%20Hu%20and%20Bineng%20Zhong%20and%20Qihua%20Liang%20and%20Zhiyi%20Mo%20and%20Liangtao%20Shi%20and%20Ying%20Tai%20and%20Jian%20Yang&entry.1292438233=%20%20Recently%2C%20many%20multi-modal%20trackers%20prioritize%20RGB%20as%20the%20dominant%20modality%2C%0Atreating%20other%20modalities%20as%20auxiliary%2C%20and%20fine-tuning%20separately%20various%0Amulti-modal%20tasks.%20This%20imbalance%20in%20modality%20dependence%20limits%20the%20ability%20of%0Amethods%20to%20dynamically%20utilize%20complementary%20information%20from%20each%20modality%20in%0Acomplex%20scenarios%2C%20making%20it%20challenging%20to%20fully%20perceive%20the%20advantages%20of%0Amulti-modal.%20As%20a%20result%2C%20a%20unified%20parameter%20model%20often%20underperforms%20in%0Avarious%20multi-modal%20tracking%20tasks.%20To%20address%20this%20issue%2C%20we%20propose%20APTrack%2C%0Aa%20novel%20unified%20tracker%20designed%20for%20multi-modal%20adaptive%20perception.%20Unlike%0Aprevious%20methods%2C%20APTrack%20explores%20a%20unified%20representation%20through%20an%20equal%0Amodeling%20strategy.%20This%20strategy%20allows%20the%20model%20to%20dynamically%20adapt%20to%0Avarious%20modalities%20and%20tasks%20without%20requiring%20additional%20fine-tuning%20between%0Adifferent%20tasks.%20Moreover%2C%20our%20tracker%20integrates%20an%20adaptive%20modality%0Ainteraction%20%28AMI%29%20module%20that%20efficiently%20bridges%20cross-modality%20interactions%0Aby%20generating%20learnable%20tokens.%20Experiments%20conducted%20on%20five%20diverse%0Amulti-modal%20datasets%20%28RGBT234%2C%20LasHeR%2C%20VisEvent%2C%20DepthTrack%2C%20and%20VOT-RGBD2022%29%0Ademonstrate%20that%20APTrack%20not%20only%20surpasses%20existing%20state-of-the-art%20unified%0Amulti-modal%20trackers%20but%20also%20outperforms%20trackers%20designed%20for%20specific%0Amulti-modal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06583v1&entry.124074799=Read"},
{"title": "Teaching Models to Balance Resisting and Accepting Persuasion", "author": "Elias Stengel-Eskin and Peter Hase and Mohit Bansal", "abstract": "  Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Training (or PBT), which\nleverages multi-agent recursive dialogue trees to create data and trains models\nvia preference optimization to accept persuasion when appropriate. PBT allows\nus to use data generated from dialogues between smaller 7-8B models for\ntraining much larger 70B models. Moreover, PBT consistently improves resistance\nto misinformation and resilience to being challenged while also resulting in\nthe best overall performance on holistic data containing both positive and\nnegative persuasion. Crucially, we show that PBT models are better teammates in\nmulti-agent debates across two domains (trivia and commonsense QA). We find\nthat without PBT, pairs of stronger and weaker models have unstable\nperformance, with the order in which the models present their answers\ndetermining whether the team obtains the stronger or weaker model's\nperformance. PBT leads to better and more stable results and less order\ndependence, with the stronger model consistently pulling the weaker one up.\n", "link": "http://arxiv.org/abs/2410.14596v2", "date": "2025-02-10", "relevancy": 2.289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4589}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching%20Models%20to%20Balance%20Resisting%20and%20Accepting%20Persuasion&body=Title%3A%20Teaching%20Models%20to%20Balance%20Resisting%20and%20Accepting%20Persuasion%0AAuthor%3A%20Elias%20Stengel-Eskin%20and%20Peter%20Hase%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20susceptible%20to%20persuasion%2C%20which%20can%20pose%0Arisks%20when%20models%20are%20faced%20with%20an%20adversarial%20interlocutor.%20We%20take%20a%20first%0Astep%20towards%20defending%20models%20against%20persuasion%20while%20also%20arguing%20that%0Adefense%20against%20adversarial%20%28i.e.%20negative%29%20persuasion%20is%20only%20half%20of%20the%0Aequation%3A%20models%20should%20also%20be%20able%20to%20accept%20beneficial%20%28i.e.%20positive%29%0Apersuasion%20to%20improve%20their%20answers.%20We%20show%20that%20optimizing%20models%20for%20only%0Aone%20side%20results%20in%20poor%20performance%20on%20the%20other.%20In%20order%20to%20balance%20positive%0Aand%20negative%20persuasion%2C%20we%20introduce%20Persuasion-Training%20%28or%20PBT%29%2C%20which%0Aleverages%20multi-agent%20recursive%20dialogue%20trees%20to%20create%20data%20and%20trains%20models%0Avia%20preference%20optimization%20to%20accept%20persuasion%20when%20appropriate.%20PBT%20allows%0Aus%20to%20use%20data%20generated%20from%20dialogues%20between%20smaller%207-8B%20models%20for%0Atraining%20much%20larger%2070B%20models.%20Moreover%2C%20PBT%20consistently%20improves%20resistance%0Ato%20misinformation%20and%20resilience%20to%20being%20challenged%20while%20also%20resulting%20in%0Athe%20best%20overall%20performance%20on%20holistic%20data%20containing%20both%20positive%20and%0Anegative%20persuasion.%20Crucially%2C%20we%20show%20that%20PBT%20models%20are%20better%20teammates%20in%0Amulti-agent%20debates%20across%20two%20domains%20%28trivia%20and%20commonsense%20QA%29.%20We%20find%0Athat%20without%20PBT%2C%20pairs%20of%20stronger%20and%20weaker%20models%20have%20unstable%0Aperformance%2C%20with%20the%20order%20in%20which%20the%20models%20present%20their%20answers%0Adetermining%20whether%20the%20team%20obtains%20the%20stronger%20or%20weaker%20model%27s%0Aperformance.%20PBT%20leads%20to%20better%20and%20more%20stable%20results%20and%20less%20order%0Adependence%2C%20with%20the%20stronger%20model%20consistently%20pulling%20the%20weaker%20one%20up.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching%2520Models%2520to%2520Balance%2520Resisting%2520and%2520Accepting%2520Persuasion%26entry.906535625%3DElias%2520Stengel-Eskin%2520and%2520Peter%2520Hase%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520susceptible%2520to%2520persuasion%252C%2520which%2520can%2520pose%250Arisks%2520when%2520models%2520are%2520faced%2520with%2520an%2520adversarial%2520interlocutor.%2520We%2520take%2520a%2520first%250Astep%2520towards%2520defending%2520models%2520against%2520persuasion%2520while%2520also%2520arguing%2520that%250Adefense%2520against%2520adversarial%2520%2528i.e.%2520negative%2529%2520persuasion%2520is%2520only%2520half%2520of%2520the%250Aequation%253A%2520models%2520should%2520also%2520be%2520able%2520to%2520accept%2520beneficial%2520%2528i.e.%2520positive%2529%250Apersuasion%2520to%2520improve%2520their%2520answers.%2520We%2520show%2520that%2520optimizing%2520models%2520for%2520only%250Aone%2520side%2520results%2520in%2520poor%2520performance%2520on%2520the%2520other.%2520In%2520order%2520to%2520balance%2520positive%250Aand%2520negative%2520persuasion%252C%2520we%2520introduce%2520Persuasion-Training%2520%2528or%2520PBT%2529%252C%2520which%250Aleverages%2520multi-agent%2520recursive%2520dialogue%2520trees%2520to%2520create%2520data%2520and%2520trains%2520models%250Avia%2520preference%2520optimization%2520to%2520accept%2520persuasion%2520when%2520appropriate.%2520PBT%2520allows%250Aus%2520to%2520use%2520data%2520generated%2520from%2520dialogues%2520between%2520smaller%25207-8B%2520models%2520for%250Atraining%2520much%2520larger%252070B%2520models.%2520Moreover%252C%2520PBT%2520consistently%2520improves%2520resistance%250Ato%2520misinformation%2520and%2520resilience%2520to%2520being%2520challenged%2520while%2520also%2520resulting%2520in%250Athe%2520best%2520overall%2520performance%2520on%2520holistic%2520data%2520containing%2520both%2520positive%2520and%250Anegative%2520persuasion.%2520Crucially%252C%2520we%2520show%2520that%2520PBT%2520models%2520are%2520better%2520teammates%2520in%250Amulti-agent%2520debates%2520across%2520two%2520domains%2520%2528trivia%2520and%2520commonsense%2520QA%2529.%2520We%2520find%250Athat%2520without%2520PBT%252C%2520pairs%2520of%2520stronger%2520and%2520weaker%2520models%2520have%2520unstable%250Aperformance%252C%2520with%2520the%2520order%2520in%2520which%2520the%2520models%2520present%2520their%2520answers%250Adetermining%2520whether%2520the%2520team%2520obtains%2520the%2520stronger%2520or%2520weaker%2520model%2527s%250Aperformance.%2520PBT%2520leads%2520to%2520better%2520and%2520more%2520stable%2520results%2520and%2520less%2520order%250Adependence%252C%2520with%2520the%2520stronger%2520model%2520consistently%2520pulling%2520the%2520weaker%2520one%2520up.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20Models%20to%20Balance%20Resisting%20and%20Accepting%20Persuasion&entry.906535625=Elias%20Stengel-Eskin%20and%20Peter%20Hase%20and%20Mohit%20Bansal&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20susceptible%20to%20persuasion%2C%20which%20can%20pose%0Arisks%20when%20models%20are%20faced%20with%20an%20adversarial%20interlocutor.%20We%20take%20a%20first%0Astep%20towards%20defending%20models%20against%20persuasion%20while%20also%20arguing%20that%0Adefense%20against%20adversarial%20%28i.e.%20negative%29%20persuasion%20is%20only%20half%20of%20the%0Aequation%3A%20models%20should%20also%20be%20able%20to%20accept%20beneficial%20%28i.e.%20positive%29%0Apersuasion%20to%20improve%20their%20answers.%20We%20show%20that%20optimizing%20models%20for%20only%0Aone%20side%20results%20in%20poor%20performance%20on%20the%20other.%20In%20order%20to%20balance%20positive%0Aand%20negative%20persuasion%2C%20we%20introduce%20Persuasion-Training%20%28or%20PBT%29%2C%20which%0Aleverages%20multi-agent%20recursive%20dialogue%20trees%20to%20create%20data%20and%20trains%20models%0Avia%20preference%20optimization%20to%20accept%20persuasion%20when%20appropriate.%20PBT%20allows%0Aus%20to%20use%20data%20generated%20from%20dialogues%20between%20smaller%207-8B%20models%20for%0Atraining%20much%20larger%2070B%20models.%20Moreover%2C%20PBT%20consistently%20improves%20resistance%0Ato%20misinformation%20and%20resilience%20to%20being%20challenged%20while%20also%20resulting%20in%0Athe%20best%20overall%20performance%20on%20holistic%20data%20containing%20both%20positive%20and%0Anegative%20persuasion.%20Crucially%2C%20we%20show%20that%20PBT%20models%20are%20better%20teammates%20in%0Amulti-agent%20debates%20across%20two%20domains%20%28trivia%20and%20commonsense%20QA%29.%20We%20find%0Athat%20without%20PBT%2C%20pairs%20of%20stronger%20and%20weaker%20models%20have%20unstable%0Aperformance%2C%20with%20the%20order%20in%20which%20the%20models%20present%20their%20answers%0Adetermining%20whether%20the%20team%20obtains%20the%20stronger%20or%20weaker%20model%27s%0Aperformance.%20PBT%20leads%20to%20better%20and%20more%20stable%20results%20and%20less%20order%0Adependence%2C%20with%20the%20stronger%20model%20consistently%20pulling%20the%20weaker%20one%20up.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14596v2&entry.124074799=Read"},
{"title": "DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud\n  Registration in the Simulation of Soft Tissue Deformation", "author": "Sara Monji-Azad and Marvin Kinz and Siddharth Kothari and Robin Khanna and Amrei Carla Mihan and David Maennel and Claudia Scherl and Juergen Hesser", "abstract": "  Soft-tissue surgeries, such as tumor resections, are complicated by tissue\ndeformations that can obscure the accurate location and shape of tissues. By\nrepresenting tissue surfaces as point clouds and applying non-rigid point cloud\nregistration (PCR) methods, surgeons can better understand tissue deformations\nbefore, during, and after surgery. Existing non-rigid PCR methods, such as\nfeature-based approaches, struggle with robustness against challenges like\nnoise, outliers, partial data, and large deformations, making accurate point\ncorrespondence difficult. Although learning-based PCR methods, particularly\nTransformer-based approaches, have recently shown promise due to their\nattention mechanisms for capturing interactions, their robustness remains\nlimited in challenging scenarios. In this paper, we present DefTransNet, a\nnovel end-to-end Transformer-based architecture for non-rigid PCR. DefTransNet\nis designed to address the key challenges of deformable registration, including\nlarge deformations, outliers, noise, and partial data, by inputting source and\ntarget point clouds and outputting displacement vector fields. The proposed\nmethod incorporates a learnable transformation matrix to enhance robustness to\naffine transformations, integrates global and local geometric information, and\ncaptures long-range dependencies among points using Transformers. We validate\nour approach on four datasets: ModelNet, SynBench, 4DMatch, and DeformedTissue,\nusing both synthetic and real-world data to demonstrate the generalization of\nour proposed method. Experimental results demonstrate that DefTransNet\noutperforms current state-of-the-art registration networks across various\nchallenging conditions. Our code and data are publicly available.\n", "link": "http://arxiv.org/abs/2502.06336v1", "date": "2025-02-10", "relevancy": 2.2771, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5812}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5631}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DefTransNet%3A%20A%20Transformer-based%20Method%20for%20Non-Rigid%20Point%20Cloud%0A%20%20Registration%20in%20the%20Simulation%20of%20Soft%20Tissue%20Deformation&body=Title%3A%20DefTransNet%3A%20A%20Transformer-based%20Method%20for%20Non-Rigid%20Point%20Cloud%0A%20%20Registration%20in%20the%20Simulation%20of%20Soft%20Tissue%20Deformation%0AAuthor%3A%20Sara%20Monji-Azad%20and%20Marvin%20Kinz%20and%20Siddharth%20Kothari%20and%20Robin%20Khanna%20and%20Amrei%20Carla%20Mihan%20and%20David%20Maennel%20and%20Claudia%20Scherl%20and%20Juergen%20Hesser%0AAbstract%3A%20%20%20Soft-tissue%20surgeries%2C%20such%20as%20tumor%20resections%2C%20are%20complicated%20by%20tissue%0Adeformations%20that%20can%20obscure%20the%20accurate%20location%20and%20shape%20of%20tissues.%20By%0Arepresenting%20tissue%20surfaces%20as%20point%20clouds%20and%20applying%20non-rigid%20point%20cloud%0Aregistration%20%28PCR%29%20methods%2C%20surgeons%20can%20better%20understand%20tissue%20deformations%0Abefore%2C%20during%2C%20and%20after%20surgery.%20Existing%20non-rigid%20PCR%20methods%2C%20such%20as%0Afeature-based%20approaches%2C%20struggle%20with%20robustness%20against%20challenges%20like%0Anoise%2C%20outliers%2C%20partial%20data%2C%20and%20large%20deformations%2C%20making%20accurate%20point%0Acorrespondence%20difficult.%20Although%20learning-based%20PCR%20methods%2C%20particularly%0ATransformer-based%20approaches%2C%20have%20recently%20shown%20promise%20due%20to%20their%0Aattention%20mechanisms%20for%20capturing%20interactions%2C%20their%20robustness%20remains%0Alimited%20in%20challenging%20scenarios.%20In%20this%20paper%2C%20we%20present%20DefTransNet%2C%20a%0Anovel%20end-to-end%20Transformer-based%20architecture%20for%20non-rigid%20PCR.%20DefTransNet%0Ais%20designed%20to%20address%20the%20key%20challenges%20of%20deformable%20registration%2C%20including%0Alarge%20deformations%2C%20outliers%2C%20noise%2C%20and%20partial%20data%2C%20by%20inputting%20source%20and%0Atarget%20point%20clouds%20and%20outputting%20displacement%20vector%20fields.%20The%20proposed%0Amethod%20incorporates%20a%20learnable%20transformation%20matrix%20to%20enhance%20robustness%20to%0Aaffine%20transformations%2C%20integrates%20global%20and%20local%20geometric%20information%2C%20and%0Acaptures%20long-range%20dependencies%20among%20points%20using%20Transformers.%20We%20validate%0Aour%20approach%20on%20four%20datasets%3A%20ModelNet%2C%20SynBench%2C%204DMatch%2C%20and%20DeformedTissue%2C%0Ausing%20both%20synthetic%20and%20real-world%20data%20to%20demonstrate%20the%20generalization%20of%0Aour%20proposed%20method.%20Experimental%20results%20demonstrate%20that%20DefTransNet%0Aoutperforms%20current%20state-of-the-art%20registration%20networks%20across%20various%0Achallenging%20conditions.%20Our%20code%20and%20data%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefTransNet%253A%2520A%2520Transformer-based%2520Method%2520for%2520Non-Rigid%2520Point%2520Cloud%250A%2520%2520Registration%2520in%2520the%2520Simulation%2520of%2520Soft%2520Tissue%2520Deformation%26entry.906535625%3DSara%2520Monji-Azad%2520and%2520Marvin%2520Kinz%2520and%2520Siddharth%2520Kothari%2520and%2520Robin%2520Khanna%2520and%2520Amrei%2520Carla%2520Mihan%2520and%2520David%2520Maennel%2520and%2520Claudia%2520Scherl%2520and%2520Juergen%2520Hesser%26entry.1292438233%3D%2520%2520Soft-tissue%2520surgeries%252C%2520such%2520as%2520tumor%2520resections%252C%2520are%2520complicated%2520by%2520tissue%250Adeformations%2520that%2520can%2520obscure%2520the%2520accurate%2520location%2520and%2520shape%2520of%2520tissues.%2520By%250Arepresenting%2520tissue%2520surfaces%2520as%2520point%2520clouds%2520and%2520applying%2520non-rigid%2520point%2520cloud%250Aregistration%2520%2528PCR%2529%2520methods%252C%2520surgeons%2520can%2520better%2520understand%2520tissue%2520deformations%250Abefore%252C%2520during%252C%2520and%2520after%2520surgery.%2520Existing%2520non-rigid%2520PCR%2520methods%252C%2520such%2520as%250Afeature-based%2520approaches%252C%2520struggle%2520with%2520robustness%2520against%2520challenges%2520like%250Anoise%252C%2520outliers%252C%2520partial%2520data%252C%2520and%2520large%2520deformations%252C%2520making%2520accurate%2520point%250Acorrespondence%2520difficult.%2520Although%2520learning-based%2520PCR%2520methods%252C%2520particularly%250ATransformer-based%2520approaches%252C%2520have%2520recently%2520shown%2520promise%2520due%2520to%2520their%250Aattention%2520mechanisms%2520for%2520capturing%2520interactions%252C%2520their%2520robustness%2520remains%250Alimited%2520in%2520challenging%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520present%2520DefTransNet%252C%2520a%250Anovel%2520end-to-end%2520Transformer-based%2520architecture%2520for%2520non-rigid%2520PCR.%2520DefTransNet%250Ais%2520designed%2520to%2520address%2520the%2520key%2520challenges%2520of%2520deformable%2520registration%252C%2520including%250Alarge%2520deformations%252C%2520outliers%252C%2520noise%252C%2520and%2520partial%2520data%252C%2520by%2520inputting%2520source%2520and%250Atarget%2520point%2520clouds%2520and%2520outputting%2520displacement%2520vector%2520fields.%2520The%2520proposed%250Amethod%2520incorporates%2520a%2520learnable%2520transformation%2520matrix%2520to%2520enhance%2520robustness%2520to%250Aaffine%2520transformations%252C%2520integrates%2520global%2520and%2520local%2520geometric%2520information%252C%2520and%250Acaptures%2520long-range%2520dependencies%2520among%2520points%2520using%2520Transformers.%2520We%2520validate%250Aour%2520approach%2520on%2520four%2520datasets%253A%2520ModelNet%252C%2520SynBench%252C%25204DMatch%252C%2520and%2520DeformedTissue%252C%250Ausing%2520both%2520synthetic%2520and%2520real-world%2520data%2520to%2520demonstrate%2520the%2520generalization%2520of%250Aour%2520proposed%2520method.%2520Experimental%2520results%2520demonstrate%2520that%2520DefTransNet%250Aoutperforms%2520current%2520state-of-the-art%2520registration%2520networks%2520across%2520various%250Achallenging%2520conditions.%2520Our%2520code%2520and%2520data%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DefTransNet%3A%20A%20Transformer-based%20Method%20for%20Non-Rigid%20Point%20Cloud%0A%20%20Registration%20in%20the%20Simulation%20of%20Soft%20Tissue%20Deformation&entry.906535625=Sara%20Monji-Azad%20and%20Marvin%20Kinz%20and%20Siddharth%20Kothari%20and%20Robin%20Khanna%20and%20Amrei%20Carla%20Mihan%20and%20David%20Maennel%20and%20Claudia%20Scherl%20and%20Juergen%20Hesser&entry.1292438233=%20%20Soft-tissue%20surgeries%2C%20such%20as%20tumor%20resections%2C%20are%20complicated%20by%20tissue%0Adeformations%20that%20can%20obscure%20the%20accurate%20location%20and%20shape%20of%20tissues.%20By%0Arepresenting%20tissue%20surfaces%20as%20point%20clouds%20and%20applying%20non-rigid%20point%20cloud%0Aregistration%20%28PCR%29%20methods%2C%20surgeons%20can%20better%20understand%20tissue%20deformations%0Abefore%2C%20during%2C%20and%20after%20surgery.%20Existing%20non-rigid%20PCR%20methods%2C%20such%20as%0Afeature-based%20approaches%2C%20struggle%20with%20robustness%20against%20challenges%20like%0Anoise%2C%20outliers%2C%20partial%20data%2C%20and%20large%20deformations%2C%20making%20accurate%20point%0Acorrespondence%20difficult.%20Although%20learning-based%20PCR%20methods%2C%20particularly%0ATransformer-based%20approaches%2C%20have%20recently%20shown%20promise%20due%20to%20their%0Aattention%20mechanisms%20for%20capturing%20interactions%2C%20their%20robustness%20remains%0Alimited%20in%20challenging%20scenarios.%20In%20this%20paper%2C%20we%20present%20DefTransNet%2C%20a%0Anovel%20end-to-end%20Transformer-based%20architecture%20for%20non-rigid%20PCR.%20DefTransNet%0Ais%20designed%20to%20address%20the%20key%20challenges%20of%20deformable%20registration%2C%20including%0Alarge%20deformations%2C%20outliers%2C%20noise%2C%20and%20partial%20data%2C%20by%20inputting%20source%20and%0Atarget%20point%20clouds%20and%20outputting%20displacement%20vector%20fields.%20The%20proposed%0Amethod%20incorporates%20a%20learnable%20transformation%20matrix%20to%20enhance%20robustness%20to%0Aaffine%20transformations%2C%20integrates%20global%20and%20local%20geometric%20information%2C%20and%0Acaptures%20long-range%20dependencies%20among%20points%20using%20Transformers.%20We%20validate%0Aour%20approach%20on%20four%20datasets%3A%20ModelNet%2C%20SynBench%2C%204DMatch%2C%20and%20DeformedTissue%2C%0Ausing%20both%20synthetic%20and%20real-world%20data%20to%20demonstrate%20the%20generalization%20of%0Aour%20proposed%20method.%20Experimental%20results%20demonstrate%20that%20DefTransNet%0Aoutperforms%20current%20state-of-the-art%20registration%20networks%20across%20various%0Achallenging%20conditions.%20Our%20code%20and%20data%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06336v1&entry.124074799=Read"},
{"title": "UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths", "author": "Weijia Mao and Zhenheng Yang and Mike Zheng Shou", "abstract": "  Unified multimodal transformers, which handle both generation and\nunderstanding tasks within a shared parameter space, have received increasing\nattention in recent research. Although various unified transformers have been\nproposed, training these models is costly due to redundant tokens and heavy\nattention computation. In the past, studies on large language models have\ndemonstrated that token pruning methods, such as Mixture of Depths (MoD), can\nsignificantly improve computational efficiency. MoD employs a router to select\nthe most important ones for processing within a transformer layer. However,\ndirectly applying MoD-based token pruning to unified transformers will result\nin suboptimal performance because different tasks exhibit varying levels of\ntoken redundancy. In our work, we analyze the unified transformers by (1)\nexamining attention weight patterns, (2) evaluating the layer importance and\ntoken redundancy, and (3) analyzing task interactions. Our findings reveal that\ntoken redundancy is primarily influenced by different tasks and layers.\nBuilding on these findings, we introduce UniMoD, a task-aware token pruning\nmethod that employs a separate router for each task to determine which tokens\nshould be pruned. We apply our method to Show-o and Emu3, reducing training\nFLOPs by approximately 15% in Show-o and 40% in Emu3, while maintaining or\nimproving performance on several benchmarks. Code will be released at\nhttps://github.com/showlab/UniMoD.\n", "link": "http://arxiv.org/abs/2502.06474v1", "date": "2025-02-10", "relevancy": 2.2762, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5806}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5686}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMoD%3A%20Efficient%20Unified%20Multimodal%20Transformers%20with%20Mixture-of-Depths&body=Title%3A%20UniMoD%3A%20Efficient%20Unified%20Multimodal%20Transformers%20with%20Mixture-of-Depths%0AAuthor%3A%20Weijia%20Mao%20and%20Zhenheng%20Yang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Unified%20multimodal%20transformers%2C%20which%20handle%20both%20generation%20and%0Aunderstanding%20tasks%20within%20a%20shared%20parameter%20space%2C%20have%20received%20increasing%0Aattention%20in%20recent%20research.%20Although%20various%20unified%20transformers%20have%20been%0Aproposed%2C%20training%20these%20models%20is%20costly%20due%20to%20redundant%20tokens%20and%20heavy%0Aattention%20computation.%20In%20the%20past%2C%20studies%20on%20large%20language%20models%20have%0Ademonstrated%20that%20token%20pruning%20methods%2C%20such%20as%20Mixture%20of%20Depths%20%28MoD%29%2C%20can%0Asignificantly%20improve%20computational%20efficiency.%20MoD%20employs%20a%20router%20to%20select%0Athe%20most%20important%20ones%20for%20processing%20within%20a%20transformer%20layer.%20However%2C%0Adirectly%20applying%20MoD-based%20token%20pruning%20to%20unified%20transformers%20will%20result%0Ain%20suboptimal%20performance%20because%20different%20tasks%20exhibit%20varying%20levels%20of%0Atoken%20redundancy.%20In%20our%20work%2C%20we%20analyze%20the%20unified%20transformers%20by%20%281%29%0Aexamining%20attention%20weight%20patterns%2C%20%282%29%20evaluating%20the%20layer%20importance%20and%0Atoken%20redundancy%2C%20and%20%283%29%20analyzing%20task%20interactions.%20Our%20findings%20reveal%20that%0Atoken%20redundancy%20is%20primarily%20influenced%20by%20different%20tasks%20and%20layers.%0ABuilding%20on%20these%20findings%2C%20we%20introduce%20UniMoD%2C%20a%20task-aware%20token%20pruning%0Amethod%20that%20employs%20a%20separate%20router%20for%20each%20task%20to%20determine%20which%20tokens%0Ashould%20be%20pruned.%20We%20apply%20our%20method%20to%20Show-o%20and%20Emu3%2C%20reducing%20training%0AFLOPs%20by%20approximately%2015%25%20in%20Show-o%20and%2040%25%20in%20Emu3%2C%20while%20maintaining%20or%0Aimproving%20performance%20on%20several%20benchmarks.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/showlab/UniMoD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMoD%253A%2520Efficient%2520Unified%2520Multimodal%2520Transformers%2520with%2520Mixture-of-Depths%26entry.906535625%3DWeijia%2520Mao%2520and%2520Zhenheng%2520Yang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520transformers%252C%2520which%2520handle%2520both%2520generation%2520and%250Aunderstanding%2520tasks%2520within%2520a%2520shared%2520parameter%2520space%252C%2520have%2520received%2520increasing%250Aattention%2520in%2520recent%2520research.%2520Although%2520various%2520unified%2520transformers%2520have%2520been%250Aproposed%252C%2520training%2520these%2520models%2520is%2520costly%2520due%2520to%2520redundant%2520tokens%2520and%2520heavy%250Aattention%2520computation.%2520In%2520the%2520past%252C%2520studies%2520on%2520large%2520language%2520models%2520have%250Ademonstrated%2520that%2520token%2520pruning%2520methods%252C%2520such%2520as%2520Mixture%2520of%2520Depths%2520%2528MoD%2529%252C%2520can%250Asignificantly%2520improve%2520computational%2520efficiency.%2520MoD%2520employs%2520a%2520router%2520to%2520select%250Athe%2520most%2520important%2520ones%2520for%2520processing%2520within%2520a%2520transformer%2520layer.%2520However%252C%250Adirectly%2520applying%2520MoD-based%2520token%2520pruning%2520to%2520unified%2520transformers%2520will%2520result%250Ain%2520suboptimal%2520performance%2520because%2520different%2520tasks%2520exhibit%2520varying%2520levels%2520of%250Atoken%2520redundancy.%2520In%2520our%2520work%252C%2520we%2520analyze%2520the%2520unified%2520transformers%2520by%2520%25281%2529%250Aexamining%2520attention%2520weight%2520patterns%252C%2520%25282%2529%2520evaluating%2520the%2520layer%2520importance%2520and%250Atoken%2520redundancy%252C%2520and%2520%25283%2529%2520analyzing%2520task%2520interactions.%2520Our%2520findings%2520reveal%2520that%250Atoken%2520redundancy%2520is%2520primarily%2520influenced%2520by%2520different%2520tasks%2520and%2520layers.%250ABuilding%2520on%2520these%2520findings%252C%2520we%2520introduce%2520UniMoD%252C%2520a%2520task-aware%2520token%2520pruning%250Amethod%2520that%2520employs%2520a%2520separate%2520router%2520for%2520each%2520task%2520to%2520determine%2520which%2520tokens%250Ashould%2520be%2520pruned.%2520We%2520apply%2520our%2520method%2520to%2520Show-o%2520and%2520Emu3%252C%2520reducing%2520training%250AFLOPs%2520by%2520approximately%252015%2525%2520in%2520Show-o%2520and%252040%2525%2520in%2520Emu3%252C%2520while%2520maintaining%2520or%250Aimproving%2520performance%2520on%2520several%2520benchmarks.%2520Code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/showlab/UniMoD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMoD%3A%20Efficient%20Unified%20Multimodal%20Transformers%20with%20Mixture-of-Depths&entry.906535625=Weijia%20Mao%20and%20Zhenheng%20Yang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Unified%20multimodal%20transformers%2C%20which%20handle%20both%20generation%20and%0Aunderstanding%20tasks%20within%20a%20shared%20parameter%20space%2C%20have%20received%20increasing%0Aattention%20in%20recent%20research.%20Although%20various%20unified%20transformers%20have%20been%0Aproposed%2C%20training%20these%20models%20is%20costly%20due%20to%20redundant%20tokens%20and%20heavy%0Aattention%20computation.%20In%20the%20past%2C%20studies%20on%20large%20language%20models%20have%0Ademonstrated%20that%20token%20pruning%20methods%2C%20such%20as%20Mixture%20of%20Depths%20%28MoD%29%2C%20can%0Asignificantly%20improve%20computational%20efficiency.%20MoD%20employs%20a%20router%20to%20select%0Athe%20most%20important%20ones%20for%20processing%20within%20a%20transformer%20layer.%20However%2C%0Adirectly%20applying%20MoD-based%20token%20pruning%20to%20unified%20transformers%20will%20result%0Ain%20suboptimal%20performance%20because%20different%20tasks%20exhibit%20varying%20levels%20of%0Atoken%20redundancy.%20In%20our%20work%2C%20we%20analyze%20the%20unified%20transformers%20by%20%281%29%0Aexamining%20attention%20weight%20patterns%2C%20%282%29%20evaluating%20the%20layer%20importance%20and%0Atoken%20redundancy%2C%20and%20%283%29%20analyzing%20task%20interactions.%20Our%20findings%20reveal%20that%0Atoken%20redundancy%20is%20primarily%20influenced%20by%20different%20tasks%20and%20layers.%0ABuilding%20on%20these%20findings%2C%20we%20introduce%20UniMoD%2C%20a%20task-aware%20token%20pruning%0Amethod%20that%20employs%20a%20separate%20router%20for%20each%20task%20to%20determine%20which%20tokens%0Ashould%20be%20pruned.%20We%20apply%20our%20method%20to%20Show-o%20and%20Emu3%2C%20reducing%20training%0AFLOPs%20by%20approximately%2015%25%20in%20Show-o%20and%2040%25%20in%20Emu3%2C%20while%20maintaining%20or%0Aimproving%20performance%20on%20several%20benchmarks.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/showlab/UniMoD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06474v1&entry.124074799=Read"},
{"title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models", "author": "Wen-Pu Cai and Ming-Yang Li and Wu-Jun Li", "abstract": "  Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.\n", "link": "http://arxiv.org/abs/2405.20973v2", "date": "2025-02-10", "relevancy": 2.2754, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4605}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCQ%3A%20Low-Rank%20Codebook%20based%20Quantization%20for%20Large%20Language%20Models&body=Title%3A%20LCQ%3A%20Low-Rank%20Codebook%20based%20Quantization%20for%20Large%20Language%20Models%0AAuthor%3A%20Wen-Pu%20Cai%20and%20Ming-Yang%20Li%20and%20Wu-Jun%20Li%0AAbstract%3A%20%20%20Large%20language%20models~%28LLMs%29%20have%20recently%20demonstrated%20promising%20performance%0Ain%20many%20tasks.%20However%2C%20the%20high%20storage%20and%20computational%20cost%20of%20LLMs%20has%0Abecome%20a%20challenge%20for%20deploying%20LLMs.%20Weight%20quantization%20has%20been%20widely%20used%0Afor%20model%20compression%2C%20which%20can%20reduce%20both%20storage%20and%20computational%20cost.%0AMost%20existing%20weight%20quantization%20methods%20for%20LLMs%20use%20a%20rank-one%20codebook%20for%0Aquantization%2C%20which%20results%20in%20substantial%20accuracy%20loss%20when%20the%20compression%0Aratio%20is%20high.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20weight%20quantization%20method%2C%0Acalled%20low-rank%20codebook%20based%20quantization~%28LCQ%29%2C%20for%20LLMs.%20LCQ%20adopts%20a%0Alow-rank%20codebook%2C%20the%20rank%20of%20which%20can%20be%20larger%20than%20one%2C%20for%20quantization.%0AExperiments%20show%20that%20LCQ%20can%20achieve%20better%20accuracy%20than%20existing%20methods%0Awith%20a%20negligibly%20extra%20storage%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20973v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCQ%253A%2520Low-Rank%2520Codebook%2520based%2520Quantization%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DWen-Pu%2520Cai%2520and%2520Ming-Yang%2520Li%2520and%2520Wu-Jun%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models~%2528LLMs%2529%2520have%2520recently%2520demonstrated%2520promising%2520performance%250Ain%2520many%2520tasks.%2520However%252C%2520the%2520high%2520storage%2520and%2520computational%2520cost%2520of%2520LLMs%2520has%250Abecome%2520a%2520challenge%2520for%2520deploying%2520LLMs.%2520Weight%2520quantization%2520has%2520been%2520widely%2520used%250Afor%2520model%2520compression%252C%2520which%2520can%2520reduce%2520both%2520storage%2520and%2520computational%2520cost.%250AMost%2520existing%2520weight%2520quantization%2520methods%2520for%2520LLMs%2520use%2520a%2520rank-one%2520codebook%2520for%250Aquantization%252C%2520which%2520results%2520in%2520substantial%2520accuracy%2520loss%2520when%2520the%2520compression%250Aratio%2520is%2520high.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520weight%2520quantization%2520method%252C%250Acalled%2520low-rank%2520codebook%2520based%2520quantization~%2528LCQ%2529%252C%2520for%2520LLMs.%2520LCQ%2520adopts%2520a%250Alow-rank%2520codebook%252C%2520the%2520rank%2520of%2520which%2520can%2520be%2520larger%2520than%2520one%252C%2520for%2520quantization.%250AExperiments%2520show%2520that%2520LCQ%2520can%2520achieve%2520better%2520accuracy%2520than%2520existing%2520methods%250Awith%2520a%2520negligibly%2520extra%2520storage%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20973v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCQ%3A%20Low-Rank%20Codebook%20based%20Quantization%20for%20Large%20Language%20Models&entry.906535625=Wen-Pu%20Cai%20and%20Ming-Yang%20Li%20and%20Wu-Jun%20Li&entry.1292438233=%20%20Large%20language%20models~%28LLMs%29%20have%20recently%20demonstrated%20promising%20performance%0Ain%20many%20tasks.%20However%2C%20the%20high%20storage%20and%20computational%20cost%20of%20LLMs%20has%0Abecome%20a%20challenge%20for%20deploying%20LLMs.%20Weight%20quantization%20has%20been%20widely%20used%0Afor%20model%20compression%2C%20which%20can%20reduce%20both%20storage%20and%20computational%20cost.%0AMost%20existing%20weight%20quantization%20methods%20for%20LLMs%20use%20a%20rank-one%20codebook%20for%0Aquantization%2C%20which%20results%20in%20substantial%20accuracy%20loss%20when%20the%20compression%0Aratio%20is%20high.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20weight%20quantization%20method%2C%0Acalled%20low-rank%20codebook%20based%20quantization~%28LCQ%29%2C%20for%20LLMs.%20LCQ%20adopts%20a%0Alow-rank%20codebook%2C%20the%20rank%20of%20which%20can%20be%20larger%20than%20one%2C%20for%20quantization.%0AExperiments%20show%20that%20LCQ%20can%20achieve%20better%20accuracy%20than%20existing%20methods%0Awith%20a%20negligibly%20extra%20storage%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20973v2&entry.124074799=Read"},
{"title": "Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning\n  Approach", "author": "Timo Fudala and Vasileios Tsouvalas and Nirvana Meratnia", "abstract": "  Multimodal transformers integrate diverse data types like images, audio, and\ntext, advancing tasks such as audio-visual understanding and image-text\nretrieval; yet their high parameterization limits deployment on\nresource-constrained edge devices. Split Learning (SL), which partitions models\nat a designated cut-layer to offload compute-intensive operations to the\nserver, offers a promising approach for distributed training of multimodal\ntransformers, though its application remains underexplored. We present MPSL, a\nparallel SL approach for computational efficient fine-tuning of multimodal\ntransformers in a distributed manner, while eliminating label sharing, client\nsynchronization, and per-client sub-model management. MPSL employs lightweight\nclient-side tokenizers and a unified modality-agnostic encoder, allowing\nflexible adaptation to task-specific needs. Our evaluation across 7 multimodal\ndatasets demonstrates that MPSL matches or outperforms Federated Learning,\nreduces client-side computations by 250x, and achieves superior scalability in\ncommunication cost with model growth. Through extensive analysis, we highlight\ntask suitability, trade-offs, and scenarios where MPSL excels, inspiring\nfurther exploration.\n", "link": "http://arxiv.org/abs/2502.06355v1", "date": "2025-02-10", "relevancy": 2.2685, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5977}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5662}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20Multimodal%20Transformers%20on%20Edge%3A%20A%20Parallel%20Split%20Learning%0A%20%20Approach&body=Title%3A%20Fine-tuning%20Multimodal%20Transformers%20on%20Edge%3A%20A%20Parallel%20Split%20Learning%0A%20%20Approach%0AAuthor%3A%20Timo%20Fudala%20and%20Vasileios%20Tsouvalas%20and%20Nirvana%20Meratnia%0AAbstract%3A%20%20%20Multimodal%20transformers%20integrate%20diverse%20data%20types%20like%20images%2C%20audio%2C%20and%0Atext%2C%20advancing%20tasks%20such%20as%20audio-visual%20understanding%20and%20image-text%0Aretrieval%3B%20yet%20their%20high%20parameterization%20limits%20deployment%20on%0Aresource-constrained%20edge%20devices.%20Split%20Learning%20%28SL%29%2C%20which%20partitions%20models%0Aat%20a%20designated%20cut-layer%20to%20offload%20compute-intensive%20operations%20to%20the%0Aserver%2C%20offers%20a%20promising%20approach%20for%20distributed%20training%20of%20multimodal%0Atransformers%2C%20though%20its%20application%20remains%20underexplored.%20We%20present%20MPSL%2C%20a%0Aparallel%20SL%20approach%20for%20computational%20efficient%20fine-tuning%20of%20multimodal%0Atransformers%20in%20a%20distributed%20manner%2C%20while%20eliminating%20label%20sharing%2C%20client%0Asynchronization%2C%20and%20per-client%20sub-model%20management.%20MPSL%20employs%20lightweight%0Aclient-side%20tokenizers%20and%20a%20unified%20modality-agnostic%20encoder%2C%20allowing%0Aflexible%20adaptation%20to%20task-specific%20needs.%20Our%20evaluation%20across%207%20multimodal%0Adatasets%20demonstrates%20that%20MPSL%20matches%20or%20outperforms%20Federated%20Learning%2C%0Areduces%20client-side%20computations%20by%20250x%2C%20and%20achieves%20superior%20scalability%20in%0Acommunication%20cost%20with%20model%20growth.%20Through%20extensive%20analysis%2C%20we%20highlight%0Atask%20suitability%2C%20trade-offs%2C%20and%20scenarios%20where%20MPSL%20excels%2C%20inspiring%0Afurther%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tuning%2520Multimodal%2520Transformers%2520on%2520Edge%253A%2520A%2520Parallel%2520Split%2520Learning%250A%2520%2520Approach%26entry.906535625%3DTimo%2520Fudala%2520and%2520Vasileios%2520Tsouvalas%2520and%2520Nirvana%2520Meratnia%26entry.1292438233%3D%2520%2520Multimodal%2520transformers%2520integrate%2520diverse%2520data%2520types%2520like%2520images%252C%2520audio%252C%2520and%250Atext%252C%2520advancing%2520tasks%2520such%2520as%2520audio-visual%2520understanding%2520and%2520image-text%250Aretrieval%253B%2520yet%2520their%2520high%2520parameterization%2520limits%2520deployment%2520on%250Aresource-constrained%2520edge%2520devices.%2520Split%2520Learning%2520%2528SL%2529%252C%2520which%2520partitions%2520models%250Aat%2520a%2520designated%2520cut-layer%2520to%2520offload%2520compute-intensive%2520operations%2520to%2520the%250Aserver%252C%2520offers%2520a%2520promising%2520approach%2520for%2520distributed%2520training%2520of%2520multimodal%250Atransformers%252C%2520though%2520its%2520application%2520remains%2520underexplored.%2520We%2520present%2520MPSL%252C%2520a%250Aparallel%2520SL%2520approach%2520for%2520computational%2520efficient%2520fine-tuning%2520of%2520multimodal%250Atransformers%2520in%2520a%2520distributed%2520manner%252C%2520while%2520eliminating%2520label%2520sharing%252C%2520client%250Asynchronization%252C%2520and%2520per-client%2520sub-model%2520management.%2520MPSL%2520employs%2520lightweight%250Aclient-side%2520tokenizers%2520and%2520a%2520unified%2520modality-agnostic%2520encoder%252C%2520allowing%250Aflexible%2520adaptation%2520to%2520task-specific%2520needs.%2520Our%2520evaluation%2520across%25207%2520multimodal%250Adatasets%2520demonstrates%2520that%2520MPSL%2520matches%2520or%2520outperforms%2520Federated%2520Learning%252C%250Areduces%2520client-side%2520computations%2520by%2520250x%252C%2520and%2520achieves%2520superior%2520scalability%2520in%250Acommunication%2520cost%2520with%2520model%2520growth.%2520Through%2520extensive%2520analysis%252C%2520we%2520highlight%250Atask%2520suitability%252C%2520trade-offs%252C%2520and%2520scenarios%2520where%2520MPSL%2520excels%252C%2520inspiring%250Afurther%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20Multimodal%20Transformers%20on%20Edge%3A%20A%20Parallel%20Split%20Learning%0A%20%20Approach&entry.906535625=Timo%20Fudala%20and%20Vasileios%20Tsouvalas%20and%20Nirvana%20Meratnia&entry.1292438233=%20%20Multimodal%20transformers%20integrate%20diverse%20data%20types%20like%20images%2C%20audio%2C%20and%0Atext%2C%20advancing%20tasks%20such%20as%20audio-visual%20understanding%20and%20image-text%0Aretrieval%3B%20yet%20their%20high%20parameterization%20limits%20deployment%20on%0Aresource-constrained%20edge%20devices.%20Split%20Learning%20%28SL%29%2C%20which%20partitions%20models%0Aat%20a%20designated%20cut-layer%20to%20offload%20compute-intensive%20operations%20to%20the%0Aserver%2C%20offers%20a%20promising%20approach%20for%20distributed%20training%20of%20multimodal%0Atransformers%2C%20though%20its%20application%20remains%20underexplored.%20We%20present%20MPSL%2C%20a%0Aparallel%20SL%20approach%20for%20computational%20efficient%20fine-tuning%20of%20multimodal%0Atransformers%20in%20a%20distributed%20manner%2C%20while%20eliminating%20label%20sharing%2C%20client%0Asynchronization%2C%20and%20per-client%20sub-model%20management.%20MPSL%20employs%20lightweight%0Aclient-side%20tokenizers%20and%20a%20unified%20modality-agnostic%20encoder%2C%20allowing%0Aflexible%20adaptation%20to%20task-specific%20needs.%20Our%20evaluation%20across%207%20multimodal%0Adatasets%20demonstrates%20that%20MPSL%20matches%20or%20outperforms%20Federated%20Learning%2C%0Areduces%20client-side%20computations%20by%20250x%2C%20and%20achieves%20superior%20scalability%20in%0Acommunication%20cost%20with%20model%20growth.%20Through%20extensive%20analysis%2C%20we%20highlight%0Atask%20suitability%2C%20trade-offs%2C%20and%20scenarios%20where%20MPSL%20excels%2C%20inspiring%0Afurther%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06355v1&entry.124074799=Read"},
{"title": "Direct-CP: Directed Collaborative Perception for Connected and\n  Autonomous Vehicles via Proactive Attention", "author": "Yihang Tao and Senkang Hu and Zhengru Fang and Yuguang Fang", "abstract": "  Collaborative perception (CP) leverages visual data from connected and\nautonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV).\nDespite recent progress, current CP methods expand the ego vehicle's 360-degree\nperceptual range almost equally, which faces two key challenges. Firstly, in\nareas with uneven traffic distribution, focusing on directions with little\ntraffic offers limited benefits. Secondly, under limited communication budgets,\nallocating excessive bandwidth to less critical directions lowers the\nperception accuracy in more vital areas. To address these issues, we propose\nDirect-CP, a proactive and direction-aware CP system aiming at improving CP in\nspecific directions. Our key idea is to enable an ego vehicle to proactively\nsignal its interested directions and readjust its attention to enhance local\ndirectional CP performance. To achieve this, we first propose an RSU-aided\ndirection masking mechanism that assists an ego vehicle in identifying vital\ndirections. Additionally, we design a direction-aware selective attention\nmodule to wisely aggregate pertinent features based on ego vehicle's\ndirectional priorities, communication budget, and the positional data of CAVs.\nMoreover, we introduce a direction-weighted detection loss (DWLoss) to capture\nthe divergence between directional CP outcomes and the ground truth,\nfacilitating effective model training. Extensive experiments on the V2X-Sim 2.0\ndataset demonstrate that our approach achieves 19.8\\% higher local perception\naccuracy in interested directions and 2.5\\% higher overall perception accuracy\nthan the state-of-the-art methods in collaborative 3D object detection tasks.\n", "link": "http://arxiv.org/abs/2409.08840v2", "date": "2025-02-10", "relevancy": 2.2677, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5863}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5579}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct-CP%3A%20Directed%20Collaborative%20Perception%20for%20Connected%20and%0A%20%20Autonomous%20Vehicles%20via%20Proactive%20Attention&body=Title%3A%20Direct-CP%3A%20Directed%20Collaborative%20Perception%20for%20Connected%20and%0A%20%20Autonomous%20Vehicles%20via%20Proactive%20Attention%0AAuthor%3A%20Yihang%20Tao%20and%20Senkang%20Hu%20and%20Zhengru%20Fang%20and%20Yuguang%20Fang%0AAbstract%3A%20%20%20Collaborative%20perception%20%28CP%29%20leverages%20visual%20data%20from%20connected%20and%0Aautonomous%20vehicles%20%28CAV%29%20to%20enhance%20an%20ego%20vehicle%27s%20field%20of%20view%20%28FoV%29.%0ADespite%20recent%20progress%2C%20current%20CP%20methods%20expand%20the%20ego%20vehicle%27s%20360-degree%0Aperceptual%20range%20almost%20equally%2C%20which%20faces%20two%20key%20challenges.%20Firstly%2C%20in%0Aareas%20with%20uneven%20traffic%20distribution%2C%20focusing%20on%20directions%20with%20little%0Atraffic%20offers%20limited%20benefits.%20Secondly%2C%20under%20limited%20communication%20budgets%2C%0Aallocating%20excessive%20bandwidth%20to%20less%20critical%20directions%20lowers%20the%0Aperception%20accuracy%20in%20more%20vital%20areas.%20To%20address%20these%20issues%2C%20we%20propose%0ADirect-CP%2C%20a%20proactive%20and%20direction-aware%20CP%20system%20aiming%20at%20improving%20CP%20in%0Aspecific%20directions.%20Our%20key%20idea%20is%20to%20enable%20an%20ego%20vehicle%20to%20proactively%0Asignal%20its%20interested%20directions%20and%20readjust%20its%20attention%20to%20enhance%20local%0Adirectional%20CP%20performance.%20To%20achieve%20this%2C%20we%20first%20propose%20an%20RSU-aided%0Adirection%20masking%20mechanism%20that%20assists%20an%20ego%20vehicle%20in%20identifying%20vital%0Adirections.%20Additionally%2C%20we%20design%20a%20direction-aware%20selective%20attention%0Amodule%20to%20wisely%20aggregate%20pertinent%20features%20based%20on%20ego%20vehicle%27s%0Adirectional%20priorities%2C%20communication%20budget%2C%20and%20the%20positional%20data%20of%20CAVs.%0AMoreover%2C%20we%20introduce%20a%20direction-weighted%20detection%20loss%20%28DWLoss%29%20to%20capture%0Athe%20divergence%20between%20directional%20CP%20outcomes%20and%20the%20ground%20truth%2C%0Afacilitating%20effective%20model%20training.%20Extensive%20experiments%20on%20the%20V2X-Sim%202.0%0Adataset%20demonstrate%20that%20our%20approach%20achieves%2019.8%5C%25%20higher%20local%20perception%0Aaccuracy%20in%20interested%20directions%20and%202.5%5C%25%20higher%20overall%20perception%20accuracy%0Athan%20the%20state-of-the-art%20methods%20in%20collaborative%203D%20object%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08840v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect-CP%253A%2520Directed%2520Collaborative%2520Perception%2520for%2520Connected%2520and%250A%2520%2520Autonomous%2520Vehicles%2520via%2520Proactive%2520Attention%26entry.906535625%3DYihang%2520Tao%2520and%2520Senkang%2520Hu%2520and%2520Zhengru%2520Fang%2520and%2520Yuguang%2520Fang%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520%2528CP%2529%2520leverages%2520visual%2520data%2520from%2520connected%2520and%250Aautonomous%2520vehicles%2520%2528CAV%2529%2520to%2520enhance%2520an%2520ego%2520vehicle%2527s%2520field%2520of%2520view%2520%2528FoV%2529.%250ADespite%2520recent%2520progress%252C%2520current%2520CP%2520methods%2520expand%2520the%2520ego%2520vehicle%2527s%2520360-degree%250Aperceptual%2520range%2520almost%2520equally%252C%2520which%2520faces%2520two%2520key%2520challenges.%2520Firstly%252C%2520in%250Aareas%2520with%2520uneven%2520traffic%2520distribution%252C%2520focusing%2520on%2520directions%2520with%2520little%250Atraffic%2520offers%2520limited%2520benefits.%2520Secondly%252C%2520under%2520limited%2520communication%2520budgets%252C%250Aallocating%2520excessive%2520bandwidth%2520to%2520less%2520critical%2520directions%2520lowers%2520the%250Aperception%2520accuracy%2520in%2520more%2520vital%2520areas.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ADirect-CP%252C%2520a%2520proactive%2520and%2520direction-aware%2520CP%2520system%2520aiming%2520at%2520improving%2520CP%2520in%250Aspecific%2520directions.%2520Our%2520key%2520idea%2520is%2520to%2520enable%2520an%2520ego%2520vehicle%2520to%2520proactively%250Asignal%2520its%2520interested%2520directions%2520and%2520readjust%2520its%2520attention%2520to%2520enhance%2520local%250Adirectional%2520CP%2520performance.%2520To%2520achieve%2520this%252C%2520we%2520first%2520propose%2520an%2520RSU-aided%250Adirection%2520masking%2520mechanism%2520that%2520assists%2520an%2520ego%2520vehicle%2520in%2520identifying%2520vital%250Adirections.%2520Additionally%252C%2520we%2520design%2520a%2520direction-aware%2520selective%2520attention%250Amodule%2520to%2520wisely%2520aggregate%2520pertinent%2520features%2520based%2520on%2520ego%2520vehicle%2527s%250Adirectional%2520priorities%252C%2520communication%2520budget%252C%2520and%2520the%2520positional%2520data%2520of%2520CAVs.%250AMoreover%252C%2520we%2520introduce%2520a%2520direction-weighted%2520detection%2520loss%2520%2528DWLoss%2529%2520to%2520capture%250Athe%2520divergence%2520between%2520directional%2520CP%2520outcomes%2520and%2520the%2520ground%2520truth%252C%250Afacilitating%2520effective%2520model%2520training.%2520Extensive%2520experiments%2520on%2520the%2520V2X-Sim%25202.0%250Adataset%2520demonstrate%2520that%2520our%2520approach%2520achieves%252019.8%255C%2525%2520higher%2520local%2520perception%250Aaccuracy%2520in%2520interested%2520directions%2520and%25202.5%255C%2525%2520higher%2520overall%2520perception%2520accuracy%250Athan%2520the%2520state-of-the-art%2520methods%2520in%2520collaborative%25203D%2520object%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08840v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct-CP%3A%20Directed%20Collaborative%20Perception%20for%20Connected%20and%0A%20%20Autonomous%20Vehicles%20via%20Proactive%20Attention&entry.906535625=Yihang%20Tao%20and%20Senkang%20Hu%20and%20Zhengru%20Fang%20and%20Yuguang%20Fang&entry.1292438233=%20%20Collaborative%20perception%20%28CP%29%20leverages%20visual%20data%20from%20connected%20and%0Aautonomous%20vehicles%20%28CAV%29%20to%20enhance%20an%20ego%20vehicle%27s%20field%20of%20view%20%28FoV%29.%0ADespite%20recent%20progress%2C%20current%20CP%20methods%20expand%20the%20ego%20vehicle%27s%20360-degree%0Aperceptual%20range%20almost%20equally%2C%20which%20faces%20two%20key%20challenges.%20Firstly%2C%20in%0Aareas%20with%20uneven%20traffic%20distribution%2C%20focusing%20on%20directions%20with%20little%0Atraffic%20offers%20limited%20benefits.%20Secondly%2C%20under%20limited%20communication%20budgets%2C%0Aallocating%20excessive%20bandwidth%20to%20less%20critical%20directions%20lowers%20the%0Aperception%20accuracy%20in%20more%20vital%20areas.%20To%20address%20these%20issues%2C%20we%20propose%0ADirect-CP%2C%20a%20proactive%20and%20direction-aware%20CP%20system%20aiming%20at%20improving%20CP%20in%0Aspecific%20directions.%20Our%20key%20idea%20is%20to%20enable%20an%20ego%20vehicle%20to%20proactively%0Asignal%20its%20interested%20directions%20and%20readjust%20its%20attention%20to%20enhance%20local%0Adirectional%20CP%20performance.%20To%20achieve%20this%2C%20we%20first%20propose%20an%20RSU-aided%0Adirection%20masking%20mechanism%20that%20assists%20an%20ego%20vehicle%20in%20identifying%20vital%0Adirections.%20Additionally%2C%20we%20design%20a%20direction-aware%20selective%20attention%0Amodule%20to%20wisely%20aggregate%20pertinent%20features%20based%20on%20ego%20vehicle%27s%0Adirectional%20priorities%2C%20communication%20budget%2C%20and%20the%20positional%20data%20of%20CAVs.%0AMoreover%2C%20we%20introduce%20a%20direction-weighted%20detection%20loss%20%28DWLoss%29%20to%20capture%0Athe%20divergence%20between%20directional%20CP%20outcomes%20and%20the%20ground%20truth%2C%0Afacilitating%20effective%20model%20training.%20Extensive%20experiments%20on%20the%20V2X-Sim%202.0%0Adataset%20demonstrate%20that%20our%20approach%20achieves%2019.8%5C%25%20higher%20local%20perception%0Aaccuracy%20in%20interested%20directions%20and%202.5%5C%25%20higher%20overall%20perception%20accuracy%0Athan%20the%20state-of-the-art%20methods%20in%20collaborative%203D%20object%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08840v2&entry.124074799=Read"},
{"title": "CoS: Chain-of-Shot Prompting for Long Video Understanding", "author": "Jian Hu and Zixu Cheng and Chenyang Si and Wei Li and Shaogang Gong", "abstract": "  Multi-modal Large Language Models (MLLMs) struggle with long videos due to\nthe need for excessive visual tokens. These tokens exceed massively the context\nlength of MLLMs, resulting in filled by redundant task-irrelevant shots. How to\nselect shots is an unsolved critical problem: sparse sampling risks missing key\ndetails, while exhaustive sampling overwhelms the model with irrelevant\ncontent, leading to video misunderstanding. To solve this problem, we propose\nChain-of-Shot prompting (CoS). The key idea is to frame shot selection as\ntest-time visual prompt optimisation, choosing shots adaptive to video\nunderstanding semantic task by optimising shots-task alignment. CoS has two key\nparts: (1) a binary video summary mechanism that performs pseudo temporal\ngrounding, discovering a binary coding to identify task-relevant shots, and (2)\na video co-reasoning module that deploys the binary coding to pair (learning to\nalign) task-relevant positive shots with irrelevant negative shots. It embeds\nthe optimised shot selections into the original video, facilitating a focus on\nrelevant context to optimize long video understanding. Experiments across three\nbaselines and five datasets demonstrate the effectiveness and adaptability of\nCoS. Code given in https://lwpyh.github.io/CoS.\n", "link": "http://arxiv.org/abs/2502.06428v1", "date": "2025-02-10", "relevancy": 2.2513, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoS%3A%20Chain-of-Shot%20Prompting%20for%20Long%20Video%20Understanding&body=Title%3A%20CoS%3A%20Chain-of-Shot%20Prompting%20for%20Long%20Video%20Understanding%0AAuthor%3A%20Jian%20Hu%20and%20Zixu%20Cheng%20and%20Chenyang%20Si%20and%20Wei%20Li%20and%20Shaogang%20Gong%0AAbstract%3A%20%20%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20long%20videos%20due%20to%0Athe%20need%20for%20excessive%20visual%20tokens.%20These%20tokens%20exceed%20massively%20the%20context%0Alength%20of%20MLLMs%2C%20resulting%20in%20filled%20by%20redundant%20task-irrelevant%20shots.%20How%20to%0Aselect%20shots%20is%20an%20unsolved%20critical%20problem%3A%20sparse%20sampling%20risks%20missing%20key%0Adetails%2C%20while%20exhaustive%20sampling%20overwhelms%20the%20model%20with%20irrelevant%0Acontent%2C%20leading%20to%20video%20misunderstanding.%20To%20solve%20this%20problem%2C%20we%20propose%0AChain-of-Shot%20prompting%20%28CoS%29.%20The%20key%20idea%20is%20to%20frame%20shot%20selection%20as%0Atest-time%20visual%20prompt%20optimisation%2C%20choosing%20shots%20adaptive%20to%20video%0Aunderstanding%20semantic%20task%20by%20optimising%20shots-task%20alignment.%20CoS%20has%20two%20key%0Aparts%3A%20%281%29%20a%20binary%20video%20summary%20mechanism%20that%20performs%20pseudo%20temporal%0Agrounding%2C%20discovering%20a%20binary%20coding%20to%20identify%20task-relevant%20shots%2C%20and%20%282%29%0Aa%20video%20co-reasoning%20module%20that%20deploys%20the%20binary%20coding%20to%20pair%20%28learning%20to%0Aalign%29%20task-relevant%20positive%20shots%20with%20irrelevant%20negative%20shots.%20It%20embeds%0Athe%20optimised%20shot%20selections%20into%20the%20original%20video%2C%20facilitating%20a%20focus%20on%0Arelevant%20context%20to%20optimize%20long%20video%20understanding.%20Experiments%20across%20three%0Abaselines%20and%20five%20datasets%20demonstrate%20the%20effectiveness%20and%20adaptability%20of%0ACoS.%20Code%20given%20in%20https%3A//lwpyh.github.io/CoS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoS%253A%2520Chain-of-Shot%2520Prompting%2520for%2520Long%2520Video%2520Understanding%26entry.906535625%3DJian%2520Hu%2520and%2520Zixu%2520Cheng%2520and%2520Chenyang%2520Si%2520and%2520Wei%2520Li%2520and%2520Shaogang%2520Gong%26entry.1292438233%3D%2520%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520struggle%2520with%2520long%2520videos%2520due%2520to%250Athe%2520need%2520for%2520excessive%2520visual%2520tokens.%2520These%2520tokens%2520exceed%2520massively%2520the%2520context%250Alength%2520of%2520MLLMs%252C%2520resulting%2520in%2520filled%2520by%2520redundant%2520task-irrelevant%2520shots.%2520How%2520to%250Aselect%2520shots%2520is%2520an%2520unsolved%2520critical%2520problem%253A%2520sparse%2520sampling%2520risks%2520missing%2520key%250Adetails%252C%2520while%2520exhaustive%2520sampling%2520overwhelms%2520the%2520model%2520with%2520irrelevant%250Acontent%252C%2520leading%2520to%2520video%2520misunderstanding.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%250AChain-of-Shot%2520prompting%2520%2528CoS%2529.%2520The%2520key%2520idea%2520is%2520to%2520frame%2520shot%2520selection%2520as%250Atest-time%2520visual%2520prompt%2520optimisation%252C%2520choosing%2520shots%2520adaptive%2520to%2520video%250Aunderstanding%2520semantic%2520task%2520by%2520optimising%2520shots-task%2520alignment.%2520CoS%2520has%2520two%2520key%250Aparts%253A%2520%25281%2529%2520a%2520binary%2520video%2520summary%2520mechanism%2520that%2520performs%2520pseudo%2520temporal%250Agrounding%252C%2520discovering%2520a%2520binary%2520coding%2520to%2520identify%2520task-relevant%2520shots%252C%2520and%2520%25282%2529%250Aa%2520video%2520co-reasoning%2520module%2520that%2520deploys%2520the%2520binary%2520coding%2520to%2520pair%2520%2528learning%2520to%250Aalign%2529%2520task-relevant%2520positive%2520shots%2520with%2520irrelevant%2520negative%2520shots.%2520It%2520embeds%250Athe%2520optimised%2520shot%2520selections%2520into%2520the%2520original%2520video%252C%2520facilitating%2520a%2520focus%2520on%250Arelevant%2520context%2520to%2520optimize%2520long%2520video%2520understanding.%2520Experiments%2520across%2520three%250Abaselines%2520and%2520five%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520adaptability%2520of%250ACoS.%2520Code%2520given%2520in%2520https%253A//lwpyh.github.io/CoS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoS%3A%20Chain-of-Shot%20Prompting%20for%20Long%20Video%20Understanding&entry.906535625=Jian%20Hu%20and%20Zixu%20Cheng%20and%20Chenyang%20Si%20and%20Wei%20Li%20and%20Shaogang%20Gong&entry.1292438233=%20%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20long%20videos%20due%20to%0Athe%20need%20for%20excessive%20visual%20tokens.%20These%20tokens%20exceed%20massively%20the%20context%0Alength%20of%20MLLMs%2C%20resulting%20in%20filled%20by%20redundant%20task-irrelevant%20shots.%20How%20to%0Aselect%20shots%20is%20an%20unsolved%20critical%20problem%3A%20sparse%20sampling%20risks%20missing%20key%0Adetails%2C%20while%20exhaustive%20sampling%20overwhelms%20the%20model%20with%20irrelevant%0Acontent%2C%20leading%20to%20video%20misunderstanding.%20To%20solve%20this%20problem%2C%20we%20propose%0AChain-of-Shot%20prompting%20%28CoS%29.%20The%20key%20idea%20is%20to%20frame%20shot%20selection%20as%0Atest-time%20visual%20prompt%20optimisation%2C%20choosing%20shots%20adaptive%20to%20video%0Aunderstanding%20semantic%20task%20by%20optimising%20shots-task%20alignment.%20CoS%20has%20two%20key%0Aparts%3A%20%281%29%20a%20binary%20video%20summary%20mechanism%20that%20performs%20pseudo%20temporal%0Agrounding%2C%20discovering%20a%20binary%20coding%20to%20identify%20task-relevant%20shots%2C%20and%20%282%29%0Aa%20video%20co-reasoning%20module%20that%20deploys%20the%20binary%20coding%20to%20pair%20%28learning%20to%0Aalign%29%20task-relevant%20positive%20shots%20with%20irrelevant%20negative%20shots.%20It%20embeds%0Athe%20optimised%20shot%20selections%20into%20the%20original%20video%2C%20facilitating%20a%20focus%20on%0Arelevant%20context%20to%20optimize%20long%20video%20understanding.%20Experiments%20across%20three%0Abaselines%20and%20five%20datasets%20demonstrate%20the%20effectiveness%20and%20adaptability%20of%0ACoS.%20Code%20given%20in%20https%3A//lwpyh.github.io/CoS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06428v1&entry.124074799=Read"},
{"title": "UniDemoir\u00e9: Towards Universal Image Demoir\u00e9ing with Data Generation\n  and Synthesis", "author": "Zemin Yang and Yujing Sun and Xidong Peng and Siu Ming Yiu and Yuexin Ma", "abstract": "  Image demoir\\'eing poses one of the most formidable challenges in image\nrestoration, primarily due to the unpredictable and anisotropic nature of\nmoir\\'e patterns. Limited by the quantity and diversity of training data,\ncurrent methods tend to overfit to a single moir\\'e domain, resulting in\nperformance degradation for new domains and restricting their robustness in\nreal-world applications. In this paper, we propose a universal image\ndemoir\\'eing solution, UniDemoir\\'e, which has superior generalization\ncapability. Notably, we propose innovative and effective data generation and\nsynthesis methods that can automatically provide vast high-quality moir\\'e\nimages to train a universal demoir\\'eing model. Our extensive experiments\ndemonstrate the cutting-edge performance and broad potential of our approach\nfor generalized image demoir\\'eing.\n", "link": "http://arxiv.org/abs/2502.06324v1", "date": "2025-02-10", "relevancy": 2.2433, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5789}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5579}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniDemoir%C3%A9%3A%20Towards%20Universal%20Image%20Demoir%C3%A9ing%20with%20Data%20Generation%0A%20%20and%20Synthesis&body=Title%3A%20UniDemoir%C3%A9%3A%20Towards%20Universal%20Image%20Demoir%C3%A9ing%20with%20Data%20Generation%0A%20%20and%20Synthesis%0AAuthor%3A%20Zemin%20Yang%20and%20Yujing%20Sun%20and%20Xidong%20Peng%20and%20Siu%20Ming%20Yiu%20and%20Yuexin%20Ma%0AAbstract%3A%20%20%20Image%20demoir%5C%27eing%20poses%20one%20of%20the%20most%20formidable%20challenges%20in%20image%0Arestoration%2C%20primarily%20due%20to%20the%20unpredictable%20and%20anisotropic%20nature%20of%0Amoir%5C%27e%20patterns.%20Limited%20by%20the%20quantity%20and%20diversity%20of%20training%20data%2C%0Acurrent%20methods%20tend%20to%20overfit%20to%20a%20single%20moir%5C%27e%20domain%2C%20resulting%20in%0Aperformance%20degradation%20for%20new%20domains%20and%20restricting%20their%20robustness%20in%0Areal-world%20applications.%20In%20this%20paper%2C%20we%20propose%20a%20universal%20image%0Ademoir%5C%27eing%20solution%2C%20UniDemoir%5C%27e%2C%20which%20has%20superior%20generalization%0Acapability.%20Notably%2C%20we%20propose%20innovative%20and%20effective%20data%20generation%20and%0Asynthesis%20methods%20that%20can%20automatically%20provide%20vast%20high-quality%20moir%5C%27e%0Aimages%20to%20train%20a%20universal%20demoir%5C%27eing%20model.%20Our%20extensive%20experiments%0Ademonstrate%20the%20cutting-edge%20performance%20and%20broad%20potential%20of%20our%20approach%0Afor%20generalized%20image%20demoir%5C%27eing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniDemoir%25C3%25A9%253A%2520Towards%2520Universal%2520Image%2520Demoir%25C3%25A9ing%2520with%2520Data%2520Generation%250A%2520%2520and%2520Synthesis%26entry.906535625%3DZemin%2520Yang%2520and%2520Yujing%2520Sun%2520and%2520Xidong%2520Peng%2520and%2520Siu%2520Ming%2520Yiu%2520and%2520Yuexin%2520Ma%26entry.1292438233%3D%2520%2520Image%2520demoir%255C%2527eing%2520poses%2520one%2520of%2520the%2520most%2520formidable%2520challenges%2520in%2520image%250Arestoration%252C%2520primarily%2520due%2520to%2520the%2520unpredictable%2520and%2520anisotropic%2520nature%2520of%250Amoir%255C%2527e%2520patterns.%2520Limited%2520by%2520the%2520quantity%2520and%2520diversity%2520of%2520training%2520data%252C%250Acurrent%2520methods%2520tend%2520to%2520overfit%2520to%2520a%2520single%2520moir%255C%2527e%2520domain%252C%2520resulting%2520in%250Aperformance%2520degradation%2520for%2520new%2520domains%2520and%2520restricting%2520their%2520robustness%2520in%250Areal-world%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520universal%2520image%250Ademoir%255C%2527eing%2520solution%252C%2520UniDemoir%255C%2527e%252C%2520which%2520has%2520superior%2520generalization%250Acapability.%2520Notably%252C%2520we%2520propose%2520innovative%2520and%2520effective%2520data%2520generation%2520and%250Asynthesis%2520methods%2520that%2520can%2520automatically%2520provide%2520vast%2520high-quality%2520moir%255C%2527e%250Aimages%2520to%2520train%2520a%2520universal%2520demoir%255C%2527eing%2520model.%2520Our%2520extensive%2520experiments%250Ademonstrate%2520the%2520cutting-edge%2520performance%2520and%2520broad%2520potential%2520of%2520our%2520approach%250Afor%2520generalized%2520image%2520demoir%255C%2527eing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniDemoir%C3%A9%3A%20Towards%20Universal%20Image%20Demoir%C3%A9ing%20with%20Data%20Generation%0A%20%20and%20Synthesis&entry.906535625=Zemin%20Yang%20and%20Yujing%20Sun%20and%20Xidong%20Peng%20and%20Siu%20Ming%20Yiu%20and%20Yuexin%20Ma&entry.1292438233=%20%20Image%20demoir%5C%27eing%20poses%20one%20of%20the%20most%20formidable%20challenges%20in%20image%0Arestoration%2C%20primarily%20due%20to%20the%20unpredictable%20and%20anisotropic%20nature%20of%0Amoir%5C%27e%20patterns.%20Limited%20by%20the%20quantity%20and%20diversity%20of%20training%20data%2C%0Acurrent%20methods%20tend%20to%20overfit%20to%20a%20single%20moir%5C%27e%20domain%2C%20resulting%20in%0Aperformance%20degradation%20for%20new%20domains%20and%20restricting%20their%20robustness%20in%0Areal-world%20applications.%20In%20this%20paper%2C%20we%20propose%20a%20universal%20image%0Ademoir%5C%27eing%20solution%2C%20UniDemoir%5C%27e%2C%20which%20has%20superior%20generalization%0Acapability.%20Notably%2C%20we%20propose%20innovative%20and%20effective%20data%20generation%20and%0Asynthesis%20methods%20that%20can%20automatically%20provide%20vast%20high-quality%20moir%5C%27e%0Aimages%20to%20train%20a%20universal%20demoir%5C%27eing%20model.%20Our%20extensive%20experiments%0Ademonstrate%20the%20cutting-edge%20performance%20and%20broad%20potential%20of%20our%20approach%0Afor%20generalized%20image%20demoir%5C%27eing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06324v1&entry.124074799=Read"},
{"title": "Exploiting Precision Mapping and Component-Specific Feature Enhancement\n  for Breast Cancer Segmentation and Identification", "author": "Pandiyaraju V and Shravan Venkatraman and Pavan Kumar S and Santhosh Malarvannan and Kannan A", "abstract": "  Breast cancer is one of the leading causes of death globally, and thus there\nis an urgent need for early and accurate diagnostic techniques. Although\nultrasound imaging is a widely used technique for breast cancer screening, it\nfaces challenges such as poor boundary delineation caused by variations in\ntumor morphology and reduced diagnostic accuracy due to inconsistent image\nquality. To address these challenges, we propose novel Deep Learning (DL)\nframeworks for breast lesion segmentation and classification. We introduce a\nprecision mapping mechanism (PMM) for a precision mapping and attention-driven\nLinkNet (PMAD-LinkNet) segmentation framework that dynamically adapts spatial\nmappings through morphological variation analysis, enabling precise pixel-level\nrefinement of tumor boundaries. Subsequently, we introduce a component-specific\nfeature enhancement module (CSFEM) for a component-specific feature-enhanced\nclassifier (CSFEC-Net). Through a multi-level attention approach, the CSFEM\nmagnifies distinguishing features of benign, malignant, and normal tissues. The\nproposed frameworks are evaluated against existing literature and a diverse set\nof state-of-the-art Convolutional Neural Network (CNN) architectures. The\nobtained results show that our segmentation model achieves an accuracy of\n98.1%, an IoU of 96.9%, and a Dice Coefficient of 97.2%. For the classification\nmodel, an accuracy of 99.2% is achieved with F1-score, precision, and recall\nvalues of 99.1%, 99.3%, and 99.1%, respectively.\n", "link": "http://arxiv.org/abs/2407.02844v6", "date": "2025-02-10", "relevancy": 2.2378, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5671}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5594}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Precision%20Mapping%20and%20Component-Specific%20Feature%20Enhancement%0A%20%20for%20Breast%20Cancer%20Segmentation%20and%20Identification&body=Title%3A%20Exploiting%20Precision%20Mapping%20and%20Component-Specific%20Feature%20Enhancement%0A%20%20for%20Breast%20Cancer%20Segmentation%20and%20Identification%0AAuthor%3A%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Santhosh%20Malarvannan%20and%20Kannan%20A%0AAbstract%3A%20%20%20Breast%20cancer%20is%20one%20of%20the%20leading%20causes%20of%20death%20globally%2C%20and%20thus%20there%0Ais%20an%20urgent%20need%20for%20early%20and%20accurate%20diagnostic%20techniques.%20Although%0Aultrasound%20imaging%20is%20a%20widely%20used%20technique%20for%20breast%20cancer%20screening%2C%20it%0Afaces%20challenges%20such%20as%20poor%20boundary%20delineation%20caused%20by%20variations%20in%0Atumor%20morphology%20and%20reduced%20diagnostic%20accuracy%20due%20to%20inconsistent%20image%0Aquality.%20To%20address%20these%20challenges%2C%20we%20propose%20novel%20Deep%20Learning%20%28DL%29%0Aframeworks%20for%20breast%20lesion%20segmentation%20and%20classification.%20We%20introduce%20a%0Aprecision%20mapping%20mechanism%20%28PMM%29%20for%20a%20precision%20mapping%20and%20attention-driven%0ALinkNet%20%28PMAD-LinkNet%29%20segmentation%20framework%20that%20dynamically%20adapts%20spatial%0Amappings%20through%20morphological%20variation%20analysis%2C%20enabling%20precise%20pixel-level%0Arefinement%20of%20tumor%20boundaries.%20Subsequently%2C%20we%20introduce%20a%20component-specific%0Afeature%20enhancement%20module%20%28CSFEM%29%20for%20a%20component-specific%20feature-enhanced%0Aclassifier%20%28CSFEC-Net%29.%20Through%20a%20multi-level%20attention%20approach%2C%20the%20CSFEM%0Amagnifies%20distinguishing%20features%20of%20benign%2C%20malignant%2C%20and%20normal%20tissues.%20The%0Aproposed%20frameworks%20are%20evaluated%20against%20existing%20literature%20and%20a%20diverse%20set%0Aof%20state-of-the-art%20Convolutional%20Neural%20Network%20%28CNN%29%20architectures.%20The%0Aobtained%20results%20show%20that%20our%20segmentation%20model%20achieves%20an%20accuracy%20of%0A98.1%25%2C%20an%20IoU%20of%2096.9%25%2C%20and%20a%20Dice%20Coefficient%20of%2097.2%25.%20For%20the%20classification%0Amodel%2C%20an%20accuracy%20of%2099.2%25%20is%20achieved%20with%20F1-score%2C%20precision%2C%20and%20recall%0Avalues%20of%2099.1%25%2C%2099.3%25%2C%20and%2099.1%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02844v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Precision%2520Mapping%2520and%2520Component-Specific%2520Feature%2520Enhancement%250A%2520%2520for%2520Breast%2520Cancer%2520Segmentation%2520and%2520Identification%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Santhosh%2520Malarvannan%2520and%2520Kannan%2520A%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520is%2520one%2520of%2520the%2520leading%2520causes%2520of%2520death%2520globally%252C%2520and%2520thus%2520there%250Ais%2520an%2520urgent%2520need%2520for%2520early%2520and%2520accurate%2520diagnostic%2520techniques.%2520Although%250Aultrasound%2520imaging%2520is%2520a%2520widely%2520used%2520technique%2520for%2520breast%2520cancer%2520screening%252C%2520it%250Afaces%2520challenges%2520such%2520as%2520poor%2520boundary%2520delineation%2520caused%2520by%2520variations%2520in%250Atumor%2520morphology%2520and%2520reduced%2520diagnostic%2520accuracy%2520due%2520to%2520inconsistent%2520image%250Aquality.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520novel%2520Deep%2520Learning%2520%2528DL%2529%250Aframeworks%2520for%2520breast%2520lesion%2520segmentation%2520and%2520classification.%2520We%2520introduce%2520a%250Aprecision%2520mapping%2520mechanism%2520%2528PMM%2529%2520for%2520a%2520precision%2520mapping%2520and%2520attention-driven%250ALinkNet%2520%2528PMAD-LinkNet%2529%2520segmentation%2520framework%2520that%2520dynamically%2520adapts%2520spatial%250Amappings%2520through%2520morphological%2520variation%2520analysis%252C%2520enabling%2520precise%2520pixel-level%250Arefinement%2520of%2520tumor%2520boundaries.%2520Subsequently%252C%2520we%2520introduce%2520a%2520component-specific%250Afeature%2520enhancement%2520module%2520%2528CSFEM%2529%2520for%2520a%2520component-specific%2520feature-enhanced%250Aclassifier%2520%2528CSFEC-Net%2529.%2520Through%2520a%2520multi-level%2520attention%2520approach%252C%2520the%2520CSFEM%250Amagnifies%2520distinguishing%2520features%2520of%2520benign%252C%2520malignant%252C%2520and%2520normal%2520tissues.%2520The%250Aproposed%2520frameworks%2520are%2520evaluated%2520against%2520existing%2520literature%2520and%2520a%2520diverse%2520set%250Aof%2520state-of-the-art%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520architectures.%2520The%250Aobtained%2520results%2520show%2520that%2520our%2520segmentation%2520model%2520achieves%2520an%2520accuracy%2520of%250A98.1%2525%252C%2520an%2520IoU%2520of%252096.9%2525%252C%2520and%2520a%2520Dice%2520Coefficient%2520of%252097.2%2525.%2520For%2520the%2520classification%250Amodel%252C%2520an%2520accuracy%2520of%252099.2%2525%2520is%2520achieved%2520with%2520F1-score%252C%2520precision%252C%2520and%2520recall%250Avalues%2520of%252099.1%2525%252C%252099.3%2525%252C%2520and%252099.1%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02844v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Precision%20Mapping%20and%20Component-Specific%20Feature%20Enhancement%0A%20%20for%20Breast%20Cancer%20Segmentation%20and%20Identification&entry.906535625=Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Santhosh%20Malarvannan%20and%20Kannan%20A&entry.1292438233=%20%20Breast%20cancer%20is%20one%20of%20the%20leading%20causes%20of%20death%20globally%2C%20and%20thus%20there%0Ais%20an%20urgent%20need%20for%20early%20and%20accurate%20diagnostic%20techniques.%20Although%0Aultrasound%20imaging%20is%20a%20widely%20used%20technique%20for%20breast%20cancer%20screening%2C%20it%0Afaces%20challenges%20such%20as%20poor%20boundary%20delineation%20caused%20by%20variations%20in%0Atumor%20morphology%20and%20reduced%20diagnostic%20accuracy%20due%20to%20inconsistent%20image%0Aquality.%20To%20address%20these%20challenges%2C%20we%20propose%20novel%20Deep%20Learning%20%28DL%29%0Aframeworks%20for%20breast%20lesion%20segmentation%20and%20classification.%20We%20introduce%20a%0Aprecision%20mapping%20mechanism%20%28PMM%29%20for%20a%20precision%20mapping%20and%20attention-driven%0ALinkNet%20%28PMAD-LinkNet%29%20segmentation%20framework%20that%20dynamically%20adapts%20spatial%0Amappings%20through%20morphological%20variation%20analysis%2C%20enabling%20precise%20pixel-level%0Arefinement%20of%20tumor%20boundaries.%20Subsequently%2C%20we%20introduce%20a%20component-specific%0Afeature%20enhancement%20module%20%28CSFEM%29%20for%20a%20component-specific%20feature-enhanced%0Aclassifier%20%28CSFEC-Net%29.%20Through%20a%20multi-level%20attention%20approach%2C%20the%20CSFEM%0Amagnifies%20distinguishing%20features%20of%20benign%2C%20malignant%2C%20and%20normal%20tissues.%20The%0Aproposed%20frameworks%20are%20evaluated%20against%20existing%20literature%20and%20a%20diverse%20set%0Aof%20state-of-the-art%20Convolutional%20Neural%20Network%20%28CNN%29%20architectures.%20The%0Aobtained%20results%20show%20that%20our%20segmentation%20model%20achieves%20an%20accuracy%20of%0A98.1%25%2C%20an%20IoU%20of%2096.9%25%2C%20and%20a%20Dice%20Coefficient%20of%2097.2%25.%20For%20the%20classification%0Amodel%2C%20an%20accuracy%20of%2099.2%25%20is%20achieved%20with%20F1-score%2C%20precision%2C%20and%20recall%0Avalues%20of%2099.1%25%2C%2099.3%25%2C%20and%2099.1%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02844v6&entry.124074799=Read"},
{"title": "Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation", "author": "Soobin Um and Beomsu Kim and Jong Chul Ye", "abstract": "  Minority samples are underrepresented instances located in low-density\nregions of a data manifold, and are valuable in many generative AI\napplications, such as data augmentation, creative content generation, etc.\nUnfortunately, existing diffusion-based minority generators often rely on\ncomputationally expensive guidance dedicated for minority generation. To\naddress this, here we present a simple yet powerful guidance-free approach\ncalled Boost-and-Skip for generating minority samples using diffusion models.\nThe key advantage of our framework requires only two minimal changes to\nstandard generative processes: (i) variance-boosted initialization and (ii)\ntimestep skipping. We highlight that these seemingly-trivial modifications are\nsupported by solid theoretical and empirical evidence, thereby effectively\npromoting emergence of underrepresented minority features. Our comprehensive\nexperiments demonstrate that Boost-and-Skip greatly enhances the capability of\ngenerating minority samples, even rivaling guidance-based state-of-the-art\napproaches while requiring significantly fewer computations.\n", "link": "http://arxiv.org/abs/2502.06516v1", "date": "2025-02-10", "relevancy": 2.2372, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5888}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5385}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boost-and-Skip%3A%20A%20Simple%20Guidance-Free%20Diffusion%20for%20Minority%20Generation&body=Title%3A%20Boost-and-Skip%3A%20A%20Simple%20Guidance-Free%20Diffusion%20for%20Minority%20Generation%0AAuthor%3A%20Soobin%20Um%20and%20Beomsu%20Kim%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20%20%20Minority%20samples%20are%20underrepresented%20instances%20located%20in%20low-density%0Aregions%20of%20a%20data%20manifold%2C%20and%20are%20valuable%20in%20many%20generative%20AI%0Aapplications%2C%20such%20as%20data%20augmentation%2C%20creative%20content%20generation%2C%20etc.%0AUnfortunately%2C%20existing%20diffusion-based%20minority%20generators%20often%20rely%20on%0Acomputationally%20expensive%20guidance%20dedicated%20for%20minority%20generation.%20To%0Aaddress%20this%2C%20here%20we%20present%20a%20simple%20yet%20powerful%20guidance-free%20approach%0Acalled%20Boost-and-Skip%20for%20generating%20minority%20samples%20using%20diffusion%20models.%0AThe%20key%20advantage%20of%20our%20framework%20requires%20only%20two%20minimal%20changes%20to%0Astandard%20generative%20processes%3A%20%28i%29%20variance-boosted%20initialization%20and%20%28ii%29%0Atimestep%20skipping.%20We%20highlight%20that%20these%20seemingly-trivial%20modifications%20are%0Asupported%20by%20solid%20theoretical%20and%20empirical%20evidence%2C%20thereby%20effectively%0Apromoting%20emergence%20of%20underrepresented%20minority%20features.%20Our%20comprehensive%0Aexperiments%20demonstrate%20that%20Boost-and-Skip%20greatly%20enhances%20the%20capability%20of%0Agenerating%20minority%20samples%2C%20even%20rivaling%20guidance-based%20state-of-the-art%0Aapproaches%20while%20requiring%20significantly%20fewer%20computations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoost-and-Skip%253A%2520A%2520Simple%2520Guidance-Free%2520Diffusion%2520for%2520Minority%2520Generation%26entry.906535625%3DSoobin%2520Um%2520and%2520Beomsu%2520Kim%2520and%2520Jong%2520Chul%2520Ye%26entry.1292438233%3D%2520%2520Minority%2520samples%2520are%2520underrepresented%2520instances%2520located%2520in%2520low-density%250Aregions%2520of%2520a%2520data%2520manifold%252C%2520and%2520are%2520valuable%2520in%2520many%2520generative%2520AI%250Aapplications%252C%2520such%2520as%2520data%2520augmentation%252C%2520creative%2520content%2520generation%252C%2520etc.%250AUnfortunately%252C%2520existing%2520diffusion-based%2520minority%2520generators%2520often%2520rely%2520on%250Acomputationally%2520expensive%2520guidance%2520dedicated%2520for%2520minority%2520generation.%2520To%250Aaddress%2520this%252C%2520here%2520we%2520present%2520a%2520simple%2520yet%2520powerful%2520guidance-free%2520approach%250Acalled%2520Boost-and-Skip%2520for%2520generating%2520minority%2520samples%2520using%2520diffusion%2520models.%250AThe%2520key%2520advantage%2520of%2520our%2520framework%2520requires%2520only%2520two%2520minimal%2520changes%2520to%250Astandard%2520generative%2520processes%253A%2520%2528i%2529%2520variance-boosted%2520initialization%2520and%2520%2528ii%2529%250Atimestep%2520skipping.%2520We%2520highlight%2520that%2520these%2520seemingly-trivial%2520modifications%2520are%250Asupported%2520by%2520solid%2520theoretical%2520and%2520empirical%2520evidence%252C%2520thereby%2520effectively%250Apromoting%2520emergence%2520of%2520underrepresented%2520minority%2520features.%2520Our%2520comprehensive%250Aexperiments%2520demonstrate%2520that%2520Boost-and-Skip%2520greatly%2520enhances%2520the%2520capability%2520of%250Agenerating%2520minority%2520samples%252C%2520even%2520rivaling%2520guidance-based%2520state-of-the-art%250Aapproaches%2520while%2520requiring%2520significantly%2520fewer%2520computations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boost-and-Skip%3A%20A%20Simple%20Guidance-Free%20Diffusion%20for%20Minority%20Generation&entry.906535625=Soobin%20Um%20and%20Beomsu%20Kim%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20Minority%20samples%20are%20underrepresented%20instances%20located%20in%20low-density%0Aregions%20of%20a%20data%20manifold%2C%20and%20are%20valuable%20in%20many%20generative%20AI%0Aapplications%2C%20such%20as%20data%20augmentation%2C%20creative%20content%20generation%2C%20etc.%0AUnfortunately%2C%20existing%20diffusion-based%20minority%20generators%20often%20rely%20on%0Acomputationally%20expensive%20guidance%20dedicated%20for%20minority%20generation.%20To%0Aaddress%20this%2C%20here%20we%20present%20a%20simple%20yet%20powerful%20guidance-free%20approach%0Acalled%20Boost-and-Skip%20for%20generating%20minority%20samples%20using%20diffusion%20models.%0AThe%20key%20advantage%20of%20our%20framework%20requires%20only%20two%20minimal%20changes%20to%0Astandard%20generative%20processes%3A%20%28i%29%20variance-boosted%20initialization%20and%20%28ii%29%0Atimestep%20skipping.%20We%20highlight%20that%20these%20seemingly-trivial%20modifications%20are%0Asupported%20by%20solid%20theoretical%20and%20empirical%20evidence%2C%20thereby%20effectively%0Apromoting%20emergence%20of%20underrepresented%20minority%20features.%20Our%20comprehensive%0Aexperiments%20demonstrate%20that%20Boost-and-Skip%20greatly%20enhances%20the%20capability%20of%0Agenerating%20minority%20samples%2C%20even%20rivaling%20guidance-based%20state-of-the-art%0Aapproaches%20while%20requiring%20significantly%20fewer%20computations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06516v1&entry.124074799=Read"},
{"title": "Calibrating LLMs with Information-Theoretic Evidential Deep Learning", "author": "Yawei Li and David R\u00fcgamer and Bernd Bischl and Mina Rezaei", "abstract": "  Fine-tuned large language models (LLMs) often exhibit overconfidence,\nparticularly when trained on small datasets, resulting in poor calibration and\ninaccurate uncertainty estimates. Evidential Deep Learning (EDL), an\nuncertainty-aware approach, enables uncertainty estimation in a single forward\npass, making it a promising method for calibrating fine-tuned LLMs. However,\ndespite its computational efficiency, EDL is prone to overfitting, as its\ntraining objective can result in overly concentrated probability distributions.\nTo mitigate this, we propose regularizing EDL by incorporating an information\nbottleneck (IB). Our approach IB-EDL suppresses spurious information in the\nevidence generated by the model and encourages truly predictive information to\ninfluence both the predictions and uncertainty estimates. Extensive experiments\nacross various fine-tuned LLMs and tasks demonstrate that IB-EDL outperforms\nboth existing EDL and non-EDL approaches. By improving the trustworthiness of\nLLMs, IB-EDL facilitates their broader adoption in domains requiring high\nlevels of confidence calibration. Code is available at\nhttps://github.com/sandylaker/ib-edl.\n", "link": "http://arxiv.org/abs/2502.06351v1", "date": "2025-02-10", "relevancy": 2.2274, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6896}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrating%20LLMs%20with%20Information-Theoretic%20Evidential%20Deep%20Learning&body=Title%3A%20Calibrating%20LLMs%20with%20Information-Theoretic%20Evidential%20Deep%20Learning%0AAuthor%3A%20Yawei%20Li%20and%20David%20R%C3%BCgamer%20and%20Bernd%20Bischl%20and%20Mina%20Rezaei%0AAbstract%3A%20%20%20Fine-tuned%20large%20language%20models%20%28LLMs%29%20often%20exhibit%20overconfidence%2C%0Aparticularly%20when%20trained%20on%20small%20datasets%2C%20resulting%20in%20poor%20calibration%20and%0Ainaccurate%20uncertainty%20estimates.%20Evidential%20Deep%20Learning%20%28EDL%29%2C%20an%0Auncertainty-aware%20approach%2C%20enables%20uncertainty%20estimation%20in%20a%20single%20forward%0Apass%2C%20making%20it%20a%20promising%20method%20for%20calibrating%20fine-tuned%20LLMs.%20However%2C%0Adespite%20its%20computational%20efficiency%2C%20EDL%20is%20prone%20to%20overfitting%2C%20as%20its%0Atraining%20objective%20can%20result%20in%20overly%20concentrated%20probability%20distributions.%0ATo%20mitigate%20this%2C%20we%20propose%20regularizing%20EDL%20by%20incorporating%20an%20information%0Abottleneck%20%28IB%29.%20Our%20approach%20IB-EDL%20suppresses%20spurious%20information%20in%20the%0Aevidence%20generated%20by%20the%20model%20and%20encourages%20truly%20predictive%20information%20to%0Ainfluence%20both%20the%20predictions%20and%20uncertainty%20estimates.%20Extensive%20experiments%0Aacross%20various%20fine-tuned%20LLMs%20and%20tasks%20demonstrate%20that%20IB-EDL%20outperforms%0Aboth%20existing%20EDL%20and%20non-EDL%20approaches.%20By%20improving%20the%20trustworthiness%20of%0ALLMs%2C%20IB-EDL%20facilitates%20their%20broader%20adoption%20in%20domains%20requiring%20high%0Alevels%20of%20confidence%20calibration.%20Code%20is%20available%20at%0Ahttps%3A//github.com/sandylaker/ib-edl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrating%2520LLMs%2520with%2520Information-Theoretic%2520Evidential%2520Deep%2520Learning%26entry.906535625%3DYawei%2520Li%2520and%2520David%2520R%25C3%25BCgamer%2520and%2520Bernd%2520Bischl%2520and%2520Mina%2520Rezaei%26entry.1292438233%3D%2520%2520Fine-tuned%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520exhibit%2520overconfidence%252C%250Aparticularly%2520when%2520trained%2520on%2520small%2520datasets%252C%2520resulting%2520in%2520poor%2520calibration%2520and%250Ainaccurate%2520uncertainty%2520estimates.%2520Evidential%2520Deep%2520Learning%2520%2528EDL%2529%252C%2520an%250Auncertainty-aware%2520approach%252C%2520enables%2520uncertainty%2520estimation%2520in%2520a%2520single%2520forward%250Apass%252C%2520making%2520it%2520a%2520promising%2520method%2520for%2520calibrating%2520fine-tuned%2520LLMs.%2520However%252C%250Adespite%2520its%2520computational%2520efficiency%252C%2520EDL%2520is%2520prone%2520to%2520overfitting%252C%2520as%2520its%250Atraining%2520objective%2520can%2520result%2520in%2520overly%2520concentrated%2520probability%2520distributions.%250ATo%2520mitigate%2520this%252C%2520we%2520propose%2520regularizing%2520EDL%2520by%2520incorporating%2520an%2520information%250Abottleneck%2520%2528IB%2529.%2520Our%2520approach%2520IB-EDL%2520suppresses%2520spurious%2520information%2520in%2520the%250Aevidence%2520generated%2520by%2520the%2520model%2520and%2520encourages%2520truly%2520predictive%2520information%2520to%250Ainfluence%2520both%2520the%2520predictions%2520and%2520uncertainty%2520estimates.%2520Extensive%2520experiments%250Aacross%2520various%2520fine-tuned%2520LLMs%2520and%2520tasks%2520demonstrate%2520that%2520IB-EDL%2520outperforms%250Aboth%2520existing%2520EDL%2520and%2520non-EDL%2520approaches.%2520By%2520improving%2520the%2520trustworthiness%2520of%250ALLMs%252C%2520IB-EDL%2520facilitates%2520their%2520broader%2520adoption%2520in%2520domains%2520requiring%2520high%250Alevels%2520of%2520confidence%2520calibration.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/sandylaker/ib-edl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrating%20LLMs%20with%20Information-Theoretic%20Evidential%20Deep%20Learning&entry.906535625=Yawei%20Li%20and%20David%20R%C3%BCgamer%20and%20Bernd%20Bischl%20and%20Mina%20Rezaei&entry.1292438233=%20%20Fine-tuned%20large%20language%20models%20%28LLMs%29%20often%20exhibit%20overconfidence%2C%0Aparticularly%20when%20trained%20on%20small%20datasets%2C%20resulting%20in%20poor%20calibration%20and%0Ainaccurate%20uncertainty%20estimates.%20Evidential%20Deep%20Learning%20%28EDL%29%2C%20an%0Auncertainty-aware%20approach%2C%20enables%20uncertainty%20estimation%20in%20a%20single%20forward%0Apass%2C%20making%20it%20a%20promising%20method%20for%20calibrating%20fine-tuned%20LLMs.%20However%2C%0Adespite%20its%20computational%20efficiency%2C%20EDL%20is%20prone%20to%20overfitting%2C%20as%20its%0Atraining%20objective%20can%20result%20in%20overly%20concentrated%20probability%20distributions.%0ATo%20mitigate%20this%2C%20we%20propose%20regularizing%20EDL%20by%20incorporating%20an%20information%0Abottleneck%20%28IB%29.%20Our%20approach%20IB-EDL%20suppresses%20spurious%20information%20in%20the%0Aevidence%20generated%20by%20the%20model%20and%20encourages%20truly%20predictive%20information%20to%0Ainfluence%20both%20the%20predictions%20and%20uncertainty%20estimates.%20Extensive%20experiments%0Aacross%20various%20fine-tuned%20LLMs%20and%20tasks%20demonstrate%20that%20IB-EDL%20outperforms%0Aboth%20existing%20EDL%20and%20non-EDL%20approaches.%20By%20improving%20the%20trustworthiness%20of%0ALLMs%2C%20IB-EDL%20facilitates%20their%20broader%20adoption%20in%20domains%20requiring%20high%0Alevels%20of%20confidence%20calibration.%20Code%20is%20available%20at%0Ahttps%3A//github.com/sandylaker/ib-edl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06351v1&entry.124074799=Read"},
{"title": "Conformal Predictions for Human Action Recognition with Vision-Language\n  Models", "author": "Bary Tim and Fuchs Cl\u00e9ment and Macq Beno\u00eet", "abstract": "  Human-In-The-Loop (HITL) frameworks are integral to many real-world computer\nvision systems, enabling human operators to make informed decisions with AI\nassistance. Conformal Predictions (CP), which provide label sets with rigorous\nguarantees on ground truth inclusion probabilities, have recently gained\ntraction as a valuable tool in HITL settings. One key application area is video\nsurveillance, closely associated with Human Action Recognition (HAR). This\nstudy explores the application of CP on top of state-of-the-art HAR methods\nthat utilize extensively pre-trained Vision-Language Models (VLMs). Our\nfindings reveal that CP can significantly reduce the average number of\ncandidate classes without modifying the underlying VLM. However, these\nreductions often result in distributions with long tails. To address this, we\nintroduce a method based on tuning the temperature parameter of the VLMs to\nminimize these tails without requiring additional calibration data. Our code is\nmade available on GitHub at the address https://github.com/tbary/CP4VLM.\n", "link": "http://arxiv.org/abs/2502.06631v1", "date": "2025-02-10", "relevancy": 2.2211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5668}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Predictions%20for%20Human%20Action%20Recognition%20with%20Vision-Language%0A%20%20Models&body=Title%3A%20Conformal%20Predictions%20for%20Human%20Action%20Recognition%20with%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Bary%20Tim%20and%20Fuchs%20Cl%C3%A9ment%20and%20Macq%20Beno%C3%AEt%0AAbstract%3A%20%20%20Human-In-The-Loop%20%28HITL%29%20frameworks%20are%20integral%20to%20many%20real-world%20computer%0Avision%20systems%2C%20enabling%20human%20operators%20to%20make%20informed%20decisions%20with%20AI%0Aassistance.%20Conformal%20Predictions%20%28CP%29%2C%20which%20provide%20label%20sets%20with%20rigorous%0Aguarantees%20on%20ground%20truth%20inclusion%20probabilities%2C%20have%20recently%20gained%0Atraction%20as%20a%20valuable%20tool%20in%20HITL%20settings.%20One%20key%20application%20area%20is%20video%0Asurveillance%2C%20closely%20associated%20with%20Human%20Action%20Recognition%20%28HAR%29.%20This%0Astudy%20explores%20the%20application%20of%20CP%20on%20top%20of%20state-of-the-art%20HAR%20methods%0Athat%20utilize%20extensively%20pre-trained%20Vision-Language%20Models%20%28VLMs%29.%20Our%0Afindings%20reveal%20that%20CP%20can%20significantly%20reduce%20the%20average%20number%20of%0Acandidate%20classes%20without%20modifying%20the%20underlying%20VLM.%20However%2C%20these%0Areductions%20often%20result%20in%20distributions%20with%20long%20tails.%20To%20address%20this%2C%20we%0Aintroduce%20a%20method%20based%20on%20tuning%20the%20temperature%20parameter%20of%20the%20VLMs%20to%0Aminimize%20these%20tails%20without%20requiring%20additional%20calibration%20data.%20Our%20code%20is%0Amade%20available%20on%20GitHub%20at%20the%20address%20https%3A//github.com/tbary/CP4VLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Predictions%2520for%2520Human%2520Action%2520Recognition%2520with%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DBary%2520Tim%2520and%2520Fuchs%2520Cl%25C3%25A9ment%2520and%2520Macq%2520Beno%25C3%25AEt%26entry.1292438233%3D%2520%2520Human-In-The-Loop%2520%2528HITL%2529%2520frameworks%2520are%2520integral%2520to%2520many%2520real-world%2520computer%250Avision%2520systems%252C%2520enabling%2520human%2520operators%2520to%2520make%2520informed%2520decisions%2520with%2520AI%250Aassistance.%2520Conformal%2520Predictions%2520%2528CP%2529%252C%2520which%2520provide%2520label%2520sets%2520with%2520rigorous%250Aguarantees%2520on%2520ground%2520truth%2520inclusion%2520probabilities%252C%2520have%2520recently%2520gained%250Atraction%2520as%2520a%2520valuable%2520tool%2520in%2520HITL%2520settings.%2520One%2520key%2520application%2520area%2520is%2520video%250Asurveillance%252C%2520closely%2520associated%2520with%2520Human%2520Action%2520Recognition%2520%2528HAR%2529.%2520This%250Astudy%2520explores%2520the%2520application%2520of%2520CP%2520on%2520top%2520of%2520state-of-the-art%2520HAR%2520methods%250Athat%2520utilize%2520extensively%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520Our%250Afindings%2520reveal%2520that%2520CP%2520can%2520significantly%2520reduce%2520the%2520average%2520number%2520of%250Acandidate%2520classes%2520without%2520modifying%2520the%2520underlying%2520VLM.%2520However%252C%2520these%250Areductions%2520often%2520result%2520in%2520distributions%2520with%2520long%2520tails.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520method%2520based%2520on%2520tuning%2520the%2520temperature%2520parameter%2520of%2520the%2520VLMs%2520to%250Aminimize%2520these%2520tails%2520without%2520requiring%2520additional%2520calibration%2520data.%2520Our%2520code%2520is%250Amade%2520available%2520on%2520GitHub%2520at%2520the%2520address%2520https%253A//github.com/tbary/CP4VLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Predictions%20for%20Human%20Action%20Recognition%20with%20Vision-Language%0A%20%20Models&entry.906535625=Bary%20Tim%20and%20Fuchs%20Cl%C3%A9ment%20and%20Macq%20Beno%C3%AEt&entry.1292438233=%20%20Human-In-The-Loop%20%28HITL%29%20frameworks%20are%20integral%20to%20many%20real-world%20computer%0Avision%20systems%2C%20enabling%20human%20operators%20to%20make%20informed%20decisions%20with%20AI%0Aassistance.%20Conformal%20Predictions%20%28CP%29%2C%20which%20provide%20label%20sets%20with%20rigorous%0Aguarantees%20on%20ground%20truth%20inclusion%20probabilities%2C%20have%20recently%20gained%0Atraction%20as%20a%20valuable%20tool%20in%20HITL%20settings.%20One%20key%20application%20area%20is%20video%0Asurveillance%2C%20closely%20associated%20with%20Human%20Action%20Recognition%20%28HAR%29.%20This%0Astudy%20explores%20the%20application%20of%20CP%20on%20top%20of%20state-of-the-art%20HAR%20methods%0Athat%20utilize%20extensively%20pre-trained%20Vision-Language%20Models%20%28VLMs%29.%20Our%0Afindings%20reveal%20that%20CP%20can%20significantly%20reduce%20the%20average%20number%20of%0Acandidate%20classes%20without%20modifying%20the%20underlying%20VLM.%20However%2C%20these%0Areductions%20often%20result%20in%20distributions%20with%20long%20tails.%20To%20address%20this%2C%20we%0Aintroduce%20a%20method%20based%20on%20tuning%20the%20temperature%20parameter%20of%20the%20VLMs%20to%0Aminimize%20these%20tails%20without%20requiring%20additional%20calibration%20data.%20Our%20code%20is%0Amade%20available%20on%20GitHub%20at%20the%20address%20https%3A//github.com/tbary/CP4VLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06631v1&entry.124074799=Read"},
{"title": "Se\u00f1orita-2M: A High-Quality Instruction-based Dataset for General\n  Video Editing by Video Specialists", "author": "Bojia Zi and Penghui Ruan and Marco Chen and Xianbiao Qi and Shaozhe Hao and Shihao Zhao and Youze Huang and Bin Liang and Rong Xiao and Kam-Fai Wong", "abstract": "  Recent advancements in video generation have spurred the development of video\nediting techniques, which can be divided into inversion-based and end-to-end\nmethods. However, current video editing methods still suffer from several\nchallenges. Inversion-based methods, though training-free and flexible, are\ntime-consuming during inference, struggle with fine-grained editing\ninstructions, and produce artifacts and jitter. On the other hand, end-to-end\nmethods, which rely on edited video pairs for training, offer faster inference\nspeeds but often produce poor editing results due to a lack of high-quality\ntraining video pairs. In this paper, to close the gap in end-to-end methods, we\nintroduce Se\\~norita-2M, a high-quality video editing dataset. Se\\~norita-2M\nconsists of approximately 2 millions of video editing pairs. It is built by\ncrafting four high-quality, specialized video editing models, each crafted and\ntrained by our team to achieve state-of-the-art editing results. We also\npropose a filtering pipeline to eliminate poorly edited video pairs.\nFurthermore, we explore common video editing architectures to identify the most\neffective structure based on current pre-trained generative model. Extensive\nexperiments show that our dataset can help to yield remarkably high-quality\nvideo editing results. More details are available at\nhttps://senorita.github.io.\n", "link": "http://arxiv.org/abs/2502.06734v1", "date": "2025-02-10", "relevancy": 2.2179, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5619}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5595}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Se%C3%B1orita-2M%3A%20A%20High-Quality%20Instruction-based%20Dataset%20for%20General%0A%20%20Video%20Editing%20by%20Video%20Specialists&body=Title%3A%20Se%C3%B1orita-2M%3A%20A%20High-Quality%20Instruction-based%20Dataset%20for%20General%0A%20%20Video%20Editing%20by%20Video%20Specialists%0AAuthor%3A%20Bojia%20Zi%20and%20Penghui%20Ruan%20and%20Marco%20Chen%20and%20Xianbiao%20Qi%20and%20Shaozhe%20Hao%20and%20Shihao%20Zhao%20and%20Youze%20Huang%20and%20Bin%20Liang%20and%20Rong%20Xiao%20and%20Kam-Fai%20Wong%0AAbstract%3A%20%20%20Recent%20advancements%20in%20video%20generation%20have%20spurred%20the%20development%20of%20video%0Aediting%20techniques%2C%20which%20can%20be%20divided%20into%20inversion-based%20and%20end-to-end%0Amethods.%20However%2C%20current%20video%20editing%20methods%20still%20suffer%20from%20several%0Achallenges.%20Inversion-based%20methods%2C%20though%20training-free%20and%20flexible%2C%20are%0Atime-consuming%20during%20inference%2C%20struggle%20with%20fine-grained%20editing%0Ainstructions%2C%20and%20produce%20artifacts%20and%20jitter.%20On%20the%20other%20hand%2C%20end-to-end%0Amethods%2C%20which%20rely%20on%20edited%20video%20pairs%20for%20training%2C%20offer%20faster%20inference%0Aspeeds%20but%20often%20produce%20poor%20editing%20results%20due%20to%20a%20lack%20of%20high-quality%0Atraining%20video%20pairs.%20In%20this%20paper%2C%20to%20close%20the%20gap%20in%20end-to-end%20methods%2C%20we%0Aintroduce%20Se%5C~norita-2M%2C%20a%20high-quality%20video%20editing%20dataset.%20Se%5C~norita-2M%0Aconsists%20of%20approximately%202%20millions%20of%20video%20editing%20pairs.%20It%20is%20built%20by%0Acrafting%20four%20high-quality%2C%20specialized%20video%20editing%20models%2C%20each%20crafted%20and%0Atrained%20by%20our%20team%20to%20achieve%20state-of-the-art%20editing%20results.%20We%20also%0Apropose%20a%20filtering%20pipeline%20to%20eliminate%20poorly%20edited%20video%20pairs.%0AFurthermore%2C%20we%20explore%20common%20video%20editing%20architectures%20to%20identify%20the%20most%0Aeffective%20structure%20based%20on%20current%20pre-trained%20generative%20model.%20Extensive%0Aexperiments%20show%20that%20our%20dataset%20can%20help%20to%20yield%20remarkably%20high-quality%0Avideo%20editing%20results.%20More%20details%20are%20available%20at%0Ahttps%3A//senorita.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSe%25C3%25B1orita-2M%253A%2520A%2520High-Quality%2520Instruction-based%2520Dataset%2520for%2520General%250A%2520%2520Video%2520Editing%2520by%2520Video%2520Specialists%26entry.906535625%3DBojia%2520Zi%2520and%2520Penghui%2520Ruan%2520and%2520Marco%2520Chen%2520and%2520Xianbiao%2520Qi%2520and%2520Shaozhe%2520Hao%2520and%2520Shihao%2520Zhao%2520and%2520Youze%2520Huang%2520and%2520Bin%2520Liang%2520and%2520Rong%2520Xiao%2520and%2520Kam-Fai%2520Wong%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520video%2520generation%2520have%2520spurred%2520the%2520development%2520of%2520video%250Aediting%2520techniques%252C%2520which%2520can%2520be%2520divided%2520into%2520inversion-based%2520and%2520end-to-end%250Amethods.%2520However%252C%2520current%2520video%2520editing%2520methods%2520still%2520suffer%2520from%2520several%250Achallenges.%2520Inversion-based%2520methods%252C%2520though%2520training-free%2520and%2520flexible%252C%2520are%250Atime-consuming%2520during%2520inference%252C%2520struggle%2520with%2520fine-grained%2520editing%250Ainstructions%252C%2520and%2520produce%2520artifacts%2520and%2520jitter.%2520On%2520the%2520other%2520hand%252C%2520end-to-end%250Amethods%252C%2520which%2520rely%2520on%2520edited%2520video%2520pairs%2520for%2520training%252C%2520offer%2520faster%2520inference%250Aspeeds%2520but%2520often%2520produce%2520poor%2520editing%2520results%2520due%2520to%2520a%2520lack%2520of%2520high-quality%250Atraining%2520video%2520pairs.%2520In%2520this%2520paper%252C%2520to%2520close%2520the%2520gap%2520in%2520end-to-end%2520methods%252C%2520we%250Aintroduce%2520Se%255C~norita-2M%252C%2520a%2520high-quality%2520video%2520editing%2520dataset.%2520Se%255C~norita-2M%250Aconsists%2520of%2520approximately%25202%2520millions%2520of%2520video%2520editing%2520pairs.%2520It%2520is%2520built%2520by%250Acrafting%2520four%2520high-quality%252C%2520specialized%2520video%2520editing%2520models%252C%2520each%2520crafted%2520and%250Atrained%2520by%2520our%2520team%2520to%2520achieve%2520state-of-the-art%2520editing%2520results.%2520We%2520also%250Apropose%2520a%2520filtering%2520pipeline%2520to%2520eliminate%2520poorly%2520edited%2520video%2520pairs.%250AFurthermore%252C%2520we%2520explore%2520common%2520video%2520editing%2520architectures%2520to%2520identify%2520the%2520most%250Aeffective%2520structure%2520based%2520on%2520current%2520pre-trained%2520generative%2520model.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520dataset%2520can%2520help%2520to%2520yield%2520remarkably%2520high-quality%250Avideo%2520editing%2520results.%2520More%2520details%2520are%2520available%2520at%250Ahttps%253A//senorita.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Se%C3%B1orita-2M%3A%20A%20High-Quality%20Instruction-based%20Dataset%20for%20General%0A%20%20Video%20Editing%20by%20Video%20Specialists&entry.906535625=Bojia%20Zi%20and%20Penghui%20Ruan%20and%20Marco%20Chen%20and%20Xianbiao%20Qi%20and%20Shaozhe%20Hao%20and%20Shihao%20Zhao%20and%20Youze%20Huang%20and%20Bin%20Liang%20and%20Rong%20Xiao%20and%20Kam-Fai%20Wong&entry.1292438233=%20%20Recent%20advancements%20in%20video%20generation%20have%20spurred%20the%20development%20of%20video%0Aediting%20techniques%2C%20which%20can%20be%20divided%20into%20inversion-based%20and%20end-to-end%0Amethods.%20However%2C%20current%20video%20editing%20methods%20still%20suffer%20from%20several%0Achallenges.%20Inversion-based%20methods%2C%20though%20training-free%20and%20flexible%2C%20are%0Atime-consuming%20during%20inference%2C%20struggle%20with%20fine-grained%20editing%0Ainstructions%2C%20and%20produce%20artifacts%20and%20jitter.%20On%20the%20other%20hand%2C%20end-to-end%0Amethods%2C%20which%20rely%20on%20edited%20video%20pairs%20for%20training%2C%20offer%20faster%20inference%0Aspeeds%20but%20often%20produce%20poor%20editing%20results%20due%20to%20a%20lack%20of%20high-quality%0Atraining%20video%20pairs.%20In%20this%20paper%2C%20to%20close%20the%20gap%20in%20end-to-end%20methods%2C%20we%0Aintroduce%20Se%5C~norita-2M%2C%20a%20high-quality%20video%20editing%20dataset.%20Se%5C~norita-2M%0Aconsists%20of%20approximately%202%20millions%20of%20video%20editing%20pairs.%20It%20is%20built%20by%0Acrafting%20four%20high-quality%2C%20specialized%20video%20editing%20models%2C%20each%20crafted%20and%0Atrained%20by%20our%20team%20to%20achieve%20state-of-the-art%20editing%20results.%20We%20also%0Apropose%20a%20filtering%20pipeline%20to%20eliminate%20poorly%20edited%20video%20pairs.%0AFurthermore%2C%20we%20explore%20common%20video%20editing%20architectures%20to%20identify%20the%20most%0Aeffective%20structure%20based%20on%20current%20pre-trained%20generative%20model.%20Extensive%0Aexperiments%20show%20that%20our%20dataset%20can%20help%20to%20yield%20remarkably%20high-quality%0Avideo%20editing%20results.%20More%20details%20are%20available%20at%0Ahttps%3A//senorita.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06734v1&entry.124074799=Read"},
{"title": "When, Where and Why to Average Weights?", "author": "Niccol\u00f2 Ajroldi and Antonio Orvieto and Jonas Geiping", "abstract": "  Averaging checkpoints along the training trajectory is a simple yet powerful\napproach to improve the generalization performance of Machine Learning models\nand reduce training time. Motivated by these potential gains, and in an effort\nto fairly and thoroughly benchmark this technique, we present an extensive\nevaluation of averaging techniques in modern Deep Learning, which we perform\nusing AlgoPerf \\citep{dahl_benchmarking_2023}, a large-scale benchmark for\noptimization algorithms. We investigate whether weight averaging can reduce\ntraining time, improve generalization, and replace learning rate decay, as\nsuggested by recent literature. Our evaluation across seven architectures and\ndatasets reveals that averaging significantly accelerates training and yields\nconsiderable efficiency gains, at the price of a minimal implementation and\nmemory cost, while mildly improving generalization across all considered\nworkloads. Finally, we explore the relationship between averaging and learning\nrate annealing and show how to optimally combine the two to achieve the best\nperformances.\n", "link": "http://arxiv.org/abs/2502.06761v1", "date": "2025-02-10", "relevancy": 2.2049, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4549}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4475}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%2C%20Where%20and%20Why%20to%20Average%20Weights%3F&body=Title%3A%20When%2C%20Where%20and%20Why%20to%20Average%20Weights%3F%0AAuthor%3A%20Niccol%C3%B2%20Ajroldi%20and%20Antonio%20Orvieto%20and%20Jonas%20Geiping%0AAbstract%3A%20%20%20Averaging%20checkpoints%20along%20the%20training%20trajectory%20is%20a%20simple%20yet%20powerful%0Aapproach%20to%20improve%20the%20generalization%20performance%20of%20Machine%20Learning%20models%0Aand%20reduce%20training%20time.%20Motivated%20by%20these%20potential%20gains%2C%20and%20in%20an%20effort%0Ato%20fairly%20and%20thoroughly%20benchmark%20this%20technique%2C%20we%20present%20an%20extensive%0Aevaluation%20of%20averaging%20techniques%20in%20modern%20Deep%20Learning%2C%20which%20we%20perform%0Ausing%20AlgoPerf%20%5Ccitep%7Bdahl_benchmarking_2023%7D%2C%20a%20large-scale%20benchmark%20for%0Aoptimization%20algorithms.%20We%20investigate%20whether%20weight%20averaging%20can%20reduce%0Atraining%20time%2C%20improve%20generalization%2C%20and%20replace%20learning%20rate%20decay%2C%20as%0Asuggested%20by%20recent%20literature.%20Our%20evaluation%20across%20seven%20architectures%20and%0Adatasets%20reveals%20that%20averaging%20significantly%20accelerates%20training%20and%20yields%0Aconsiderable%20efficiency%20gains%2C%20at%20the%20price%20of%20a%20minimal%20implementation%20and%0Amemory%20cost%2C%20while%20mildly%20improving%20generalization%20across%20all%20considered%0Aworkloads.%20Finally%2C%20we%20explore%20the%20relationship%20between%20averaging%20and%20learning%0Arate%20annealing%20and%20show%20how%20to%20optimally%20combine%20the%20two%20to%20achieve%20the%20best%0Aperformances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%252C%2520Where%2520and%2520Why%2520to%2520Average%2520Weights%253F%26entry.906535625%3DNiccol%25C3%25B2%2520Ajroldi%2520and%2520Antonio%2520Orvieto%2520and%2520Jonas%2520Geiping%26entry.1292438233%3D%2520%2520Averaging%2520checkpoints%2520along%2520the%2520training%2520trajectory%2520is%2520a%2520simple%2520yet%2520powerful%250Aapproach%2520to%2520improve%2520the%2520generalization%2520performance%2520of%2520Machine%2520Learning%2520models%250Aand%2520reduce%2520training%2520time.%2520Motivated%2520by%2520these%2520potential%2520gains%252C%2520and%2520in%2520an%2520effort%250Ato%2520fairly%2520and%2520thoroughly%2520benchmark%2520this%2520technique%252C%2520we%2520present%2520an%2520extensive%250Aevaluation%2520of%2520averaging%2520techniques%2520in%2520modern%2520Deep%2520Learning%252C%2520which%2520we%2520perform%250Ausing%2520AlgoPerf%2520%255Ccitep%257Bdahl_benchmarking_2023%257D%252C%2520a%2520large-scale%2520benchmark%2520for%250Aoptimization%2520algorithms.%2520We%2520investigate%2520whether%2520weight%2520averaging%2520can%2520reduce%250Atraining%2520time%252C%2520improve%2520generalization%252C%2520and%2520replace%2520learning%2520rate%2520decay%252C%2520as%250Asuggested%2520by%2520recent%2520literature.%2520Our%2520evaluation%2520across%2520seven%2520architectures%2520and%250Adatasets%2520reveals%2520that%2520averaging%2520significantly%2520accelerates%2520training%2520and%2520yields%250Aconsiderable%2520efficiency%2520gains%252C%2520at%2520the%2520price%2520of%2520a%2520minimal%2520implementation%2520and%250Amemory%2520cost%252C%2520while%2520mildly%2520improving%2520generalization%2520across%2520all%2520considered%250Aworkloads.%2520Finally%252C%2520we%2520explore%2520the%2520relationship%2520between%2520averaging%2520and%2520learning%250Arate%2520annealing%2520and%2520show%2520how%2520to%2520optimally%2520combine%2520the%2520two%2520to%2520achieve%2520the%2520best%250Aperformances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%2C%20Where%20and%20Why%20to%20Average%20Weights%3F&entry.906535625=Niccol%C3%B2%20Ajroldi%20and%20Antonio%20Orvieto%20and%20Jonas%20Geiping&entry.1292438233=%20%20Averaging%20checkpoints%20along%20the%20training%20trajectory%20is%20a%20simple%20yet%20powerful%0Aapproach%20to%20improve%20the%20generalization%20performance%20of%20Machine%20Learning%20models%0Aand%20reduce%20training%20time.%20Motivated%20by%20these%20potential%20gains%2C%20and%20in%20an%20effort%0Ato%20fairly%20and%20thoroughly%20benchmark%20this%20technique%2C%20we%20present%20an%20extensive%0Aevaluation%20of%20averaging%20techniques%20in%20modern%20Deep%20Learning%2C%20which%20we%20perform%0Ausing%20AlgoPerf%20%5Ccitep%7Bdahl_benchmarking_2023%7D%2C%20a%20large-scale%20benchmark%20for%0Aoptimization%20algorithms.%20We%20investigate%20whether%20weight%20averaging%20can%20reduce%0Atraining%20time%2C%20improve%20generalization%2C%20and%20replace%20learning%20rate%20decay%2C%20as%0Asuggested%20by%20recent%20literature.%20Our%20evaluation%20across%20seven%20architectures%20and%0Adatasets%20reveals%20that%20averaging%20significantly%20accelerates%20training%20and%20yields%0Aconsiderable%20efficiency%20gains%2C%20at%20the%20price%20of%20a%20minimal%20implementation%20and%0Amemory%20cost%2C%20while%20mildly%20improving%20generalization%20across%20all%20considered%0Aworkloads.%20Finally%2C%20we%20explore%20the%20relationship%20between%20averaging%20and%20learning%0Arate%20annealing%20and%20show%20how%20to%20optimally%20combine%20the%20two%20to%20achieve%20the%20best%0Aperformances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06761v1&entry.124074799=Read"},
{"title": "Rethinking Large-scale Dataset Compression: Shifting Focus From Labels\n  to Images", "author": "Lingao Xiao and Songhua Liu and Yang He and Xinchao Wang", "abstract": "  Dataset distillation and dataset pruning are two prominent techniques for\ncompressing datasets to improve computational and storage efficiency. Despite\ntheir overlapping objectives, these approaches are rarely compared directly.\nEven within each field, the evaluation protocols are inconsistent across\nvarious methods, which complicates fair comparisons and hinders\nreproducibility. Considering these limitations, we introduce in this paper a\nbenchmark that equitably evaluates methodologies across both distillation and\npruning literatures. Notably, our benchmark reveals that in the mainstream\ndataset distillation setting for large-scale datasets, which heavily rely on\nsoft labels from pre-trained models, even randomly selected subsets can achieve\nsurprisingly competitive performance. This finding suggests that an\noveremphasis on soft labels may be diverting attention from the intrinsic value\nof the image data, while also imposing additional burdens in terms of\ngeneration, storage, and application. To address these issues, we propose a new\nframework for dataset compression, termed Prune, Combine, and Augment (PCA),\nwhich focuses on leveraging image data exclusively, relies solely on hard\nlabels for evaluation, and achieves state-of-the-art performance in this setup.\nBy shifting the emphasis back to the images, our benchmark and PCA framework\npave the way for more balanced and accessible techniques in dataset compression\nresearch. Our code is available at:\nhttps://github.com/ArmandXiao/Rethinking-Dataset-Compression\n", "link": "http://arxiv.org/abs/2502.06434v1", "date": "2025-02-10", "relevancy": 2.1844, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.57}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.532}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Large-scale%20Dataset%20Compression%3A%20Shifting%20Focus%20From%20Labels%0A%20%20to%20Images&body=Title%3A%20Rethinking%20Large-scale%20Dataset%20Compression%3A%20Shifting%20Focus%20From%20Labels%0A%20%20to%20Images%0AAuthor%3A%20Lingao%20Xiao%20and%20Songhua%20Liu%20and%20Yang%20He%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Dataset%20distillation%20and%20dataset%20pruning%20are%20two%20prominent%20techniques%20for%0Acompressing%20datasets%20to%20improve%20computational%20and%20storage%20efficiency.%20Despite%0Atheir%20overlapping%20objectives%2C%20these%20approaches%20are%20rarely%20compared%20directly.%0AEven%20within%20each%20field%2C%20the%20evaluation%20protocols%20are%20inconsistent%20across%0Avarious%20methods%2C%20which%20complicates%20fair%20comparisons%20and%20hinders%0Areproducibility.%20Considering%20these%20limitations%2C%20we%20introduce%20in%20this%20paper%20a%0Abenchmark%20that%20equitably%20evaluates%20methodologies%20across%20both%20distillation%20and%0Apruning%20literatures.%20Notably%2C%20our%20benchmark%20reveals%20that%20in%20the%20mainstream%0Adataset%20distillation%20setting%20for%20large-scale%20datasets%2C%20which%20heavily%20rely%20on%0Asoft%20labels%20from%20pre-trained%20models%2C%20even%20randomly%20selected%20subsets%20can%20achieve%0Asurprisingly%20competitive%20performance.%20This%20finding%20suggests%20that%20an%0Aoveremphasis%20on%20soft%20labels%20may%20be%20diverting%20attention%20from%20the%20intrinsic%20value%0Aof%20the%20image%20data%2C%20while%20also%20imposing%20additional%20burdens%20in%20terms%20of%0Ageneration%2C%20storage%2C%20and%20application.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%0Aframework%20for%20dataset%20compression%2C%20termed%20Prune%2C%20Combine%2C%20and%20Augment%20%28PCA%29%2C%0Awhich%20focuses%20on%20leveraging%20image%20data%20exclusively%2C%20relies%20solely%20on%20hard%0Alabels%20for%20evaluation%2C%20and%20achieves%20state-of-the-art%20performance%20in%20this%20setup.%0ABy%20shifting%20the%20emphasis%20back%20to%20the%20images%2C%20our%20benchmark%20and%20PCA%20framework%0Apave%20the%20way%20for%20more%20balanced%20and%20accessible%20techniques%20in%20dataset%20compression%0Aresearch.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ArmandXiao/Rethinking-Dataset-Compression%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Large-scale%2520Dataset%2520Compression%253A%2520Shifting%2520Focus%2520From%2520Labels%250A%2520%2520to%2520Images%26entry.906535625%3DLingao%2520Xiao%2520and%2520Songhua%2520Liu%2520and%2520Yang%2520He%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Dataset%2520distillation%2520and%2520dataset%2520pruning%2520are%2520two%2520prominent%2520techniques%2520for%250Acompressing%2520datasets%2520to%2520improve%2520computational%2520and%2520storage%2520efficiency.%2520Despite%250Atheir%2520overlapping%2520objectives%252C%2520these%2520approaches%2520are%2520rarely%2520compared%2520directly.%250AEven%2520within%2520each%2520field%252C%2520the%2520evaluation%2520protocols%2520are%2520inconsistent%2520across%250Avarious%2520methods%252C%2520which%2520complicates%2520fair%2520comparisons%2520and%2520hinders%250Areproducibility.%2520Considering%2520these%2520limitations%252C%2520we%2520introduce%2520in%2520this%2520paper%2520a%250Abenchmark%2520that%2520equitably%2520evaluates%2520methodologies%2520across%2520both%2520distillation%2520and%250Apruning%2520literatures.%2520Notably%252C%2520our%2520benchmark%2520reveals%2520that%2520in%2520the%2520mainstream%250Adataset%2520distillation%2520setting%2520for%2520large-scale%2520datasets%252C%2520which%2520heavily%2520rely%2520on%250Asoft%2520labels%2520from%2520pre-trained%2520models%252C%2520even%2520randomly%2520selected%2520subsets%2520can%2520achieve%250Asurprisingly%2520competitive%2520performance.%2520This%2520finding%2520suggests%2520that%2520an%250Aoveremphasis%2520on%2520soft%2520labels%2520may%2520be%2520diverting%2520attention%2520from%2520the%2520intrinsic%2520value%250Aof%2520the%2520image%2520data%252C%2520while%2520also%2520imposing%2520additional%2520burdens%2520in%2520terms%2520of%250Ageneration%252C%2520storage%252C%2520and%2520application.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520new%250Aframework%2520for%2520dataset%2520compression%252C%2520termed%2520Prune%252C%2520Combine%252C%2520and%2520Augment%2520%2528PCA%2529%252C%250Awhich%2520focuses%2520on%2520leveraging%2520image%2520data%2520exclusively%252C%2520relies%2520solely%2520on%2520hard%250Alabels%2520for%2520evaluation%252C%2520and%2520achieves%2520state-of-the-art%2520performance%2520in%2520this%2520setup.%250ABy%2520shifting%2520the%2520emphasis%2520back%2520to%2520the%2520images%252C%2520our%2520benchmark%2520and%2520PCA%2520framework%250Apave%2520the%2520way%2520for%2520more%2520balanced%2520and%2520accessible%2520techniques%2520in%2520dataset%2520compression%250Aresearch.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ArmandXiao/Rethinking-Dataset-Compression%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Large-scale%20Dataset%20Compression%3A%20Shifting%20Focus%20From%20Labels%0A%20%20to%20Images&entry.906535625=Lingao%20Xiao%20and%20Songhua%20Liu%20and%20Yang%20He%20and%20Xinchao%20Wang&entry.1292438233=%20%20Dataset%20distillation%20and%20dataset%20pruning%20are%20two%20prominent%20techniques%20for%0Acompressing%20datasets%20to%20improve%20computational%20and%20storage%20efficiency.%20Despite%0Atheir%20overlapping%20objectives%2C%20these%20approaches%20are%20rarely%20compared%20directly.%0AEven%20within%20each%20field%2C%20the%20evaluation%20protocols%20are%20inconsistent%20across%0Avarious%20methods%2C%20which%20complicates%20fair%20comparisons%20and%20hinders%0Areproducibility.%20Considering%20these%20limitations%2C%20we%20introduce%20in%20this%20paper%20a%0Abenchmark%20that%20equitably%20evaluates%20methodologies%20across%20both%20distillation%20and%0Apruning%20literatures.%20Notably%2C%20our%20benchmark%20reveals%20that%20in%20the%20mainstream%0Adataset%20distillation%20setting%20for%20large-scale%20datasets%2C%20which%20heavily%20rely%20on%0Asoft%20labels%20from%20pre-trained%20models%2C%20even%20randomly%20selected%20subsets%20can%20achieve%0Asurprisingly%20competitive%20performance.%20This%20finding%20suggests%20that%20an%0Aoveremphasis%20on%20soft%20labels%20may%20be%20diverting%20attention%20from%20the%20intrinsic%20value%0Aof%20the%20image%20data%2C%20while%20also%20imposing%20additional%20burdens%20in%20terms%20of%0Ageneration%2C%20storage%2C%20and%20application.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%0Aframework%20for%20dataset%20compression%2C%20termed%20Prune%2C%20Combine%2C%20and%20Augment%20%28PCA%29%2C%0Awhich%20focuses%20on%20leveraging%20image%20data%20exclusively%2C%20relies%20solely%20on%20hard%0Alabels%20for%20evaluation%2C%20and%20achieves%20state-of-the-art%20performance%20in%20this%20setup.%0ABy%20shifting%20the%20emphasis%20back%20to%20the%20images%2C%20our%20benchmark%20and%20PCA%20framework%0Apave%20the%20way%20for%20more%20balanced%20and%20accessible%20techniques%20in%20dataset%20compression%0Aresearch.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ArmandXiao/Rethinking-Dataset-Compression%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06434v1&entry.124074799=Read"},
{"title": "On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition", "author": "Guanghao Ye and Khiem Duc Pham and Xinzhi Zhang and Sivakanth Gopi and Baolin Peng and Beibin Li and Janardhan Kulkarni and Huseyin A. Inan", "abstract": "  Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.\n", "link": "http://arxiv.org/abs/2502.06773v1", "date": "2025-02-10", "relevancy": 2.181, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Emergence%20of%20Thinking%20in%20LLMs%20I%3A%20Searching%20for%20the%20Right%0A%20%20Intuition&body=Title%3A%20On%20the%20Emergence%20of%20Thinking%20in%20LLMs%20I%3A%20Searching%20for%20the%20Right%0A%20%20Intuition%0AAuthor%3A%20Guanghao%20Ye%20and%20Khiem%20Duc%20Pham%20and%20Xinzhi%20Zhang%20and%20Sivakanth%20Gopi%20and%20Baolin%20Peng%20and%20Beibin%20Li%20and%20Janardhan%20Kulkarni%20and%20Huseyin%20A.%20Inan%0AAbstract%3A%20%20%20Recent%20AI%20advancements%2C%20such%20as%20OpenAI%27s%20new%20models%2C%20are%20transforming%20LLMs%0Ainto%20LRMs%20%28Large%20Reasoning%20Models%29%20that%20perform%20reasoning%20during%20inference%2C%0Ataking%20extra%20time%20and%20compute%20for%20higher-quality%20outputs.%20We%20aim%20to%20uncover%20the%0Aalgorithmic%20framework%20for%20training%20LRMs.%20Methods%20like%20self-consistency%2C%20PRM%2C%0Aand%20AlphaZero%20suggest%20reasoning%20as%20guided%20search.%20We%20ask%3A%20what%20is%20the%20simplest%2C%0Amost%20scalable%20way%20to%20enable%20search%20in%20LLMs%3F%0A%20%20We%20propose%20a%20post-training%20framework%20called%20Reinforcement%20Learning%20via%0ASelf-Play%20%28RLSP%29.%20RLSP%20involves%20three%20steps%3A%20%281%29%20supervised%20fine-tuning%20with%0Ahuman%20or%20synthetic%20demonstrations%20of%20the%20reasoning%20process%2C%20%282%29%20using%20an%0Aexploration%20reward%20signal%20to%20encourage%20diverse%20and%20efficient%20reasoning%0Abehaviors%2C%20and%20%283%29%20RL%20training%20with%20an%20outcome%20verifier%20to%20ensure%20correctness%0Awhile%20preventing%20reward%20hacking.%20Our%20key%20innovation%20is%20to%20decouple%20exploration%0Aand%20correctness%20signals%20during%20PPO%20training%2C%20carefully%20balancing%20them%20to%0Aimprove%20performance%20and%20efficiency.%0A%20%20Empirical%20studies%20in%20the%20math%20domain%20show%20that%20RLSP%20improves%20reasoning.%20On%0Athe%20Llama-3.1-8B-Instruct%20model%2C%20RLSP%20can%20boost%20performance%20by%2023%25%20in%20MATH-500%0Atest%20set%3B%20On%20AIME%202024%20math%20problems%2C%20Qwen2.5-32B-Instruct%20improved%20by%2010%25%20due%0Ato%20RLSP.%20However%2C%20a%20more%20important%20finding%20of%20this%20work%20is%20that%20the%20models%0Atrained%20using%20RLSP%2C%20even%20with%20the%20simplest%20exploration%20reward%20that%20encourages%0Athe%20model%20to%20take%20more%20intermediate%20steps%2C%20showed%20several%20emergent%20behaviors%0Asuch%20as%20backtracking%2C%20exploration%20of%20ideas%2C%20and%20verification.%20These%20findings%0Ademonstrate%20that%20RLSP%20framework%20might%20be%20enough%20to%20enable%20emergence%20of%20complex%0Areasoning%20abilities%20in%20LLMs%20when%20scaled.%20Lastly%2C%20we%20propose%20a%20theory%20as%20to%20why%0ARLSP%20search%20strategy%20is%20more%20suitable%20for%20LLMs%20inspired%20by%20a%20remarkable%20result%0Athat%20says%20CoT%20provably%20increases%20computational%20power%20of%20LLMs%2C%20which%20grows%20as%0Athe%20number%20of%20steps%20in%20CoT%20%5Ccite%7Bli2024chain%2Cmerrill2023expresssive%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Emergence%2520of%2520Thinking%2520in%2520LLMs%2520I%253A%2520Searching%2520for%2520the%2520Right%250A%2520%2520Intuition%26entry.906535625%3DGuanghao%2520Ye%2520and%2520Khiem%2520Duc%2520Pham%2520and%2520Xinzhi%2520Zhang%2520and%2520Sivakanth%2520Gopi%2520and%2520Baolin%2520Peng%2520and%2520Beibin%2520Li%2520and%2520Janardhan%2520Kulkarni%2520and%2520Huseyin%2520A.%2520Inan%26entry.1292438233%3D%2520%2520Recent%2520AI%2520advancements%252C%2520such%2520as%2520OpenAI%2527s%2520new%2520models%252C%2520are%2520transforming%2520LLMs%250Ainto%2520LRMs%2520%2528Large%2520Reasoning%2520Models%2529%2520that%2520perform%2520reasoning%2520during%2520inference%252C%250Ataking%2520extra%2520time%2520and%2520compute%2520for%2520higher-quality%2520outputs.%2520We%2520aim%2520to%2520uncover%2520the%250Aalgorithmic%2520framework%2520for%2520training%2520LRMs.%2520Methods%2520like%2520self-consistency%252C%2520PRM%252C%250Aand%2520AlphaZero%2520suggest%2520reasoning%2520as%2520guided%2520search.%2520We%2520ask%253A%2520what%2520is%2520the%2520simplest%252C%250Amost%2520scalable%2520way%2520to%2520enable%2520search%2520in%2520LLMs%253F%250A%2520%2520We%2520propose%2520a%2520post-training%2520framework%2520called%2520Reinforcement%2520Learning%2520via%250ASelf-Play%2520%2528RLSP%2529.%2520RLSP%2520involves%2520three%2520steps%253A%2520%25281%2529%2520supervised%2520fine-tuning%2520with%250Ahuman%2520or%2520synthetic%2520demonstrations%2520of%2520the%2520reasoning%2520process%252C%2520%25282%2529%2520using%2520an%250Aexploration%2520reward%2520signal%2520to%2520encourage%2520diverse%2520and%2520efficient%2520reasoning%250Abehaviors%252C%2520and%2520%25283%2529%2520RL%2520training%2520with%2520an%2520outcome%2520verifier%2520to%2520ensure%2520correctness%250Awhile%2520preventing%2520reward%2520hacking.%2520Our%2520key%2520innovation%2520is%2520to%2520decouple%2520exploration%250Aand%2520correctness%2520signals%2520during%2520PPO%2520training%252C%2520carefully%2520balancing%2520them%2520to%250Aimprove%2520performance%2520and%2520efficiency.%250A%2520%2520Empirical%2520studies%2520in%2520the%2520math%2520domain%2520show%2520that%2520RLSP%2520improves%2520reasoning.%2520On%250Athe%2520Llama-3.1-8B-Instruct%2520model%252C%2520RLSP%2520can%2520boost%2520performance%2520by%252023%2525%2520in%2520MATH-500%250Atest%2520set%253B%2520On%2520AIME%25202024%2520math%2520problems%252C%2520Qwen2.5-32B-Instruct%2520improved%2520by%252010%2525%2520due%250Ato%2520RLSP.%2520However%252C%2520a%2520more%2520important%2520finding%2520of%2520this%2520work%2520is%2520that%2520the%2520models%250Atrained%2520using%2520RLSP%252C%2520even%2520with%2520the%2520simplest%2520exploration%2520reward%2520that%2520encourages%250Athe%2520model%2520to%2520take%2520more%2520intermediate%2520steps%252C%2520showed%2520several%2520emergent%2520behaviors%250Asuch%2520as%2520backtracking%252C%2520exploration%2520of%2520ideas%252C%2520and%2520verification.%2520These%2520findings%250Ademonstrate%2520that%2520RLSP%2520framework%2520might%2520be%2520enough%2520to%2520enable%2520emergence%2520of%2520complex%250Areasoning%2520abilities%2520in%2520LLMs%2520when%2520scaled.%2520Lastly%252C%2520we%2520propose%2520a%2520theory%2520as%2520to%2520why%250ARLSP%2520search%2520strategy%2520is%2520more%2520suitable%2520for%2520LLMs%2520inspired%2520by%2520a%2520remarkable%2520result%250Athat%2520says%2520CoT%2520provably%2520increases%2520computational%2520power%2520of%2520LLMs%252C%2520which%2520grows%2520as%250Athe%2520number%2520of%2520steps%2520in%2520CoT%2520%255Ccite%257Bli2024chain%252Cmerrill2023expresssive%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Emergence%20of%20Thinking%20in%20LLMs%20I%3A%20Searching%20for%20the%20Right%0A%20%20Intuition&entry.906535625=Guanghao%20Ye%20and%20Khiem%20Duc%20Pham%20and%20Xinzhi%20Zhang%20and%20Sivakanth%20Gopi%20and%20Baolin%20Peng%20and%20Beibin%20Li%20and%20Janardhan%20Kulkarni%20and%20Huseyin%20A.%20Inan&entry.1292438233=%20%20Recent%20AI%20advancements%2C%20such%20as%20OpenAI%27s%20new%20models%2C%20are%20transforming%20LLMs%0Ainto%20LRMs%20%28Large%20Reasoning%20Models%29%20that%20perform%20reasoning%20during%20inference%2C%0Ataking%20extra%20time%20and%20compute%20for%20higher-quality%20outputs.%20We%20aim%20to%20uncover%20the%0Aalgorithmic%20framework%20for%20training%20LRMs.%20Methods%20like%20self-consistency%2C%20PRM%2C%0Aand%20AlphaZero%20suggest%20reasoning%20as%20guided%20search.%20We%20ask%3A%20what%20is%20the%20simplest%2C%0Amost%20scalable%20way%20to%20enable%20search%20in%20LLMs%3F%0A%20%20We%20propose%20a%20post-training%20framework%20called%20Reinforcement%20Learning%20via%0ASelf-Play%20%28RLSP%29.%20RLSP%20involves%20three%20steps%3A%20%281%29%20supervised%20fine-tuning%20with%0Ahuman%20or%20synthetic%20demonstrations%20of%20the%20reasoning%20process%2C%20%282%29%20using%20an%0Aexploration%20reward%20signal%20to%20encourage%20diverse%20and%20efficient%20reasoning%0Abehaviors%2C%20and%20%283%29%20RL%20training%20with%20an%20outcome%20verifier%20to%20ensure%20correctness%0Awhile%20preventing%20reward%20hacking.%20Our%20key%20innovation%20is%20to%20decouple%20exploration%0Aand%20correctness%20signals%20during%20PPO%20training%2C%20carefully%20balancing%20them%20to%0Aimprove%20performance%20and%20efficiency.%0A%20%20Empirical%20studies%20in%20the%20math%20domain%20show%20that%20RLSP%20improves%20reasoning.%20On%0Athe%20Llama-3.1-8B-Instruct%20model%2C%20RLSP%20can%20boost%20performance%20by%2023%25%20in%20MATH-500%0Atest%20set%3B%20On%20AIME%202024%20math%20problems%2C%20Qwen2.5-32B-Instruct%20improved%20by%2010%25%20due%0Ato%20RLSP.%20However%2C%20a%20more%20important%20finding%20of%20this%20work%20is%20that%20the%20models%0Atrained%20using%20RLSP%2C%20even%20with%20the%20simplest%20exploration%20reward%20that%20encourages%0Athe%20model%20to%20take%20more%20intermediate%20steps%2C%20showed%20several%20emergent%20behaviors%0Asuch%20as%20backtracking%2C%20exploration%20of%20ideas%2C%20and%20verification.%20These%20findings%0Ademonstrate%20that%20RLSP%20framework%20might%20be%20enough%20to%20enable%20emergence%20of%20complex%0Areasoning%20abilities%20in%20LLMs%20when%20scaled.%20Lastly%2C%20we%20propose%20a%20theory%20as%20to%20why%0ARLSP%20search%20strategy%20is%20more%20suitable%20for%20LLMs%20inspired%20by%20a%20remarkable%20result%0Athat%20says%20CoT%20provably%20increases%20computational%20power%20of%20LLMs%2C%20which%20grows%20as%0Athe%20number%20of%20steps%20in%20CoT%20%5Ccite%7Bli2024chain%2Cmerrill2023expresssive%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06773v1&entry.124074799=Read"},
{"title": "A Large-scale AI-generated Image Inpainting Benchmark", "author": "Paschalis Giakoumoglou and Dimitrios Karageorgiou and Symeon Papadopoulos and Panagiotis C. Petrantonakis", "abstract": "  Recent advances in generative models enable highly realistic image\nmanipulations, creating an urgent need for robust forgery detection methods.\nCurrent datasets for training and evaluating these methods are limited in scale\nand diversity. To address this, we propose a methodology for creating\nhigh-quality inpainting datasets and apply it to create DiQuID, comprising over\n95,000 inpainted images generated from 78,000 original images sourced from\nMS-COCO, RAISE, and OpenImages. Our methodology consists of three components:\n(1) Semantically Aligned Object Replacement (SAOR) that identifies suitable\nobjects through instance segmentation and generates contextually appropriate\nprompts, (2) Multiple Model Image Inpainting (MMII) that employs various\nstate-of-the-art inpainting pipelines primarily based on diffusion models to\ncreate diverse manipulations, and (3) Uncertainty-Guided Deceptiveness\nAssessment (UGDA) that evaluates image realism through comparative analysis\nwith originals. The resulting dataset surpasses existing ones in diversity,\naesthetic quality, and technical quality. We provide comprehensive benchmarking\nresults using state-of-the-art forgery detection methods, demonstrating the\ndataset's effectiveness in evaluating and improving detection algorithms.\nThrough a human study with 42 participants on 1,000 images, we show that while\nhumans struggle with images classified as deceiving by our methodology, models\ntrained on our dataset maintain high performance on these challenging cases.\nCode and dataset are available at https://github.com/mever-team/DiQuID.\n", "link": "http://arxiv.org/abs/2502.06593v1", "date": "2025-02-10", "relevancy": 2.1646, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5639}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.546}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-scale%20AI-generated%20Image%20Inpainting%20Benchmark&body=Title%3A%20A%20Large-scale%20AI-generated%20Image%20Inpainting%20Benchmark%0AAuthor%3A%20Paschalis%20Giakoumoglou%20and%20Dimitrios%20Karageorgiou%20and%20Symeon%20Papadopoulos%20and%20Panagiotis%20C.%20Petrantonakis%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20models%20enable%20highly%20realistic%20image%0Amanipulations%2C%20creating%20an%20urgent%20need%20for%20robust%20forgery%20detection%20methods.%0ACurrent%20datasets%20for%20training%20and%20evaluating%20these%20methods%20are%20limited%20in%20scale%0Aand%20diversity.%20To%20address%20this%2C%20we%20propose%20a%20methodology%20for%20creating%0Ahigh-quality%20inpainting%20datasets%20and%20apply%20it%20to%20create%20DiQuID%2C%20comprising%20over%0A95%2C000%20inpainted%20images%20generated%20from%2078%2C000%20original%20images%20sourced%20from%0AMS-COCO%2C%20RAISE%2C%20and%20OpenImages.%20Our%20methodology%20consists%20of%20three%20components%3A%0A%281%29%20Semantically%20Aligned%20Object%20Replacement%20%28SAOR%29%20that%20identifies%20suitable%0Aobjects%20through%20instance%20segmentation%20and%20generates%20contextually%20appropriate%0Aprompts%2C%20%282%29%20Multiple%20Model%20Image%20Inpainting%20%28MMII%29%20that%20employs%20various%0Astate-of-the-art%20inpainting%20pipelines%20primarily%20based%20on%20diffusion%20models%20to%0Acreate%20diverse%20manipulations%2C%20and%20%283%29%20Uncertainty-Guided%20Deceptiveness%0AAssessment%20%28UGDA%29%20that%20evaluates%20image%20realism%20through%20comparative%20analysis%0Awith%20originals.%20The%20resulting%20dataset%20surpasses%20existing%20ones%20in%20diversity%2C%0Aaesthetic%20quality%2C%20and%20technical%20quality.%20We%20provide%20comprehensive%20benchmarking%0Aresults%20using%20state-of-the-art%20forgery%20detection%20methods%2C%20demonstrating%20the%0Adataset%27s%20effectiveness%20in%20evaluating%20and%20improving%20detection%20algorithms.%0AThrough%20a%20human%20study%20with%2042%20participants%20on%201%2C000%20images%2C%20we%20show%20that%20while%0Ahumans%20struggle%20with%20images%20classified%20as%20deceiving%20by%20our%20methodology%2C%20models%0Atrained%20on%20our%20dataset%20maintain%20high%20performance%20on%20these%20challenging%20cases.%0ACode%20and%20dataset%20are%20available%20at%20https%3A//github.com/mever-team/DiQuID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-scale%2520AI-generated%2520Image%2520Inpainting%2520Benchmark%26entry.906535625%3DPaschalis%2520Giakoumoglou%2520and%2520Dimitrios%2520Karageorgiou%2520and%2520Symeon%2520Papadopoulos%2520and%2520Panagiotis%2520C.%2520Petrantonakis%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520models%2520enable%2520highly%2520realistic%2520image%250Amanipulations%252C%2520creating%2520an%2520urgent%2520need%2520for%2520robust%2520forgery%2520detection%2520methods.%250ACurrent%2520datasets%2520for%2520training%2520and%2520evaluating%2520these%2520methods%2520are%2520limited%2520in%2520scale%250Aand%2520diversity.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520methodology%2520for%2520creating%250Ahigh-quality%2520inpainting%2520datasets%2520and%2520apply%2520it%2520to%2520create%2520DiQuID%252C%2520comprising%2520over%250A95%252C000%2520inpainted%2520images%2520generated%2520from%252078%252C000%2520original%2520images%2520sourced%2520from%250AMS-COCO%252C%2520RAISE%252C%2520and%2520OpenImages.%2520Our%2520methodology%2520consists%2520of%2520three%2520components%253A%250A%25281%2529%2520Semantically%2520Aligned%2520Object%2520Replacement%2520%2528SAOR%2529%2520that%2520identifies%2520suitable%250Aobjects%2520through%2520instance%2520segmentation%2520and%2520generates%2520contextually%2520appropriate%250Aprompts%252C%2520%25282%2529%2520Multiple%2520Model%2520Image%2520Inpainting%2520%2528MMII%2529%2520that%2520employs%2520various%250Astate-of-the-art%2520inpainting%2520pipelines%2520primarily%2520based%2520on%2520diffusion%2520models%2520to%250Acreate%2520diverse%2520manipulations%252C%2520and%2520%25283%2529%2520Uncertainty-Guided%2520Deceptiveness%250AAssessment%2520%2528UGDA%2529%2520that%2520evaluates%2520image%2520realism%2520through%2520comparative%2520analysis%250Awith%2520originals.%2520The%2520resulting%2520dataset%2520surpasses%2520existing%2520ones%2520in%2520diversity%252C%250Aaesthetic%2520quality%252C%2520and%2520technical%2520quality.%2520We%2520provide%2520comprehensive%2520benchmarking%250Aresults%2520using%2520state-of-the-art%2520forgery%2520detection%2520methods%252C%2520demonstrating%2520the%250Adataset%2527s%2520effectiveness%2520in%2520evaluating%2520and%2520improving%2520detection%2520algorithms.%250AThrough%2520a%2520human%2520study%2520with%252042%2520participants%2520on%25201%252C000%2520images%252C%2520we%2520show%2520that%2520while%250Ahumans%2520struggle%2520with%2520images%2520classified%2520as%2520deceiving%2520by%2520our%2520methodology%252C%2520models%250Atrained%2520on%2520our%2520dataset%2520maintain%2520high%2520performance%2520on%2520these%2520challenging%2520cases.%250ACode%2520and%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/mever-team/DiQuID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-scale%20AI-generated%20Image%20Inpainting%20Benchmark&entry.906535625=Paschalis%20Giakoumoglou%20and%20Dimitrios%20Karageorgiou%20and%20Symeon%20Papadopoulos%20and%20Panagiotis%20C.%20Petrantonakis&entry.1292438233=%20%20Recent%20advances%20in%20generative%20models%20enable%20highly%20realistic%20image%0Amanipulations%2C%20creating%20an%20urgent%20need%20for%20robust%20forgery%20detection%20methods.%0ACurrent%20datasets%20for%20training%20and%20evaluating%20these%20methods%20are%20limited%20in%20scale%0Aand%20diversity.%20To%20address%20this%2C%20we%20propose%20a%20methodology%20for%20creating%0Ahigh-quality%20inpainting%20datasets%20and%20apply%20it%20to%20create%20DiQuID%2C%20comprising%20over%0A95%2C000%20inpainted%20images%20generated%20from%2078%2C000%20original%20images%20sourced%20from%0AMS-COCO%2C%20RAISE%2C%20and%20OpenImages.%20Our%20methodology%20consists%20of%20three%20components%3A%0A%281%29%20Semantically%20Aligned%20Object%20Replacement%20%28SAOR%29%20that%20identifies%20suitable%0Aobjects%20through%20instance%20segmentation%20and%20generates%20contextually%20appropriate%0Aprompts%2C%20%282%29%20Multiple%20Model%20Image%20Inpainting%20%28MMII%29%20that%20employs%20various%0Astate-of-the-art%20inpainting%20pipelines%20primarily%20based%20on%20diffusion%20models%20to%0Acreate%20diverse%20manipulations%2C%20and%20%283%29%20Uncertainty-Guided%20Deceptiveness%0AAssessment%20%28UGDA%29%20that%20evaluates%20image%20realism%20through%20comparative%20analysis%0Awith%20originals.%20The%20resulting%20dataset%20surpasses%20existing%20ones%20in%20diversity%2C%0Aaesthetic%20quality%2C%20and%20technical%20quality.%20We%20provide%20comprehensive%20benchmarking%0Aresults%20using%20state-of-the-art%20forgery%20detection%20methods%2C%20demonstrating%20the%0Adataset%27s%20effectiveness%20in%20evaluating%20and%20improving%20detection%20algorithms.%0AThrough%20a%20human%20study%20with%2042%20participants%20on%201%2C000%20images%2C%20we%20show%20that%20while%0Ahumans%20struggle%20with%20images%20classified%20as%20deceiving%20by%20our%20methodology%2C%20models%0Atrained%20on%20our%20dataset%20maintain%20high%20performance%20on%20these%20challenging%20cases.%0ACode%20and%20dataset%20are%20available%20at%20https%3A//github.com/mever-team/DiQuID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06593v1&entry.124074799=Read"},
{"title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM", "author": "Qingshui Gu and Shu Li and Tianyu Zheng and Zhaoxiang Zhang", "abstract": "  Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.\n", "link": "http://arxiv.org/abs/2502.06635v1", "date": "2025-02-10", "relevancy": 2.1567, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4364}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steel-LLM%3AFrom%20Scratch%20to%20Open%20Source%20--%20A%20Personal%20Journey%20in%20Building%0A%20%20a%20Chinese-Centric%20LLM&body=Title%3A%20Steel-LLM%3AFrom%20Scratch%20to%20Open%20Source%20--%20A%20Personal%20Journey%20in%20Building%0A%20%20a%20Chinese-Centric%20LLM%0AAuthor%3A%20Qingshui%20Gu%20and%20Shu%20Li%20and%20Tianyu%20Zheng%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Steel-LLM%20is%20a%20Chinese-centric%20language%20model%20developed%20from%20scratch%20with%20the%0Agoal%20of%20creating%20a%20high-quality%2C%20open-source%20model%20despite%20limited%0Acomputational%20resources.%20Launched%20in%20March%202024%2C%20the%20project%20aimed%20to%20train%20a%0A1-billion-parameter%20model%20on%20a%20large-scale%20dataset%2C%20prioritizing%20transparency%0Aand%20the%20sharing%20of%20practical%20insights%20to%20assist%20others%20in%20the%20community.%20The%0Atraining%20process%20primarily%20focused%20on%20Chinese%20data%2C%20with%20a%20small%20proportion%20of%0AEnglish%20data%20included%2C%20addressing%20gaps%20in%20existing%20open-source%20LLMs%20by%0Aproviding%20a%20more%20detailed%20and%20practical%20account%20of%20the%20model-building%20journey.%0ASteel-LLM%20has%20demonstrated%20competitive%20performance%20on%20benchmarks%20such%20as%20CEVAL%0Aand%20CMMLU%2C%20outperforming%20early%20models%20from%20larger%20institutions.%20This%20paper%0Aprovides%20a%20comprehensive%20summary%20of%20the%20project%27s%20key%20contributions%2C%20including%0Adata%20collection%2C%20model%20design%2C%20training%20methodologies%2C%20and%20the%20challenges%0Aencountered%20along%20the%20way%2C%20offering%20a%20valuable%20resource%20for%20researchers%20and%0Apractitioners%20looking%20to%20develop%20their%20own%20LLMs.%20The%20model%20checkpoints%20and%0Atraining%20script%20are%20available%20at%20https%3A//github.com/zhanshijinwat/Steel-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteel-LLM%253AFrom%2520Scratch%2520to%2520Open%2520Source%2520--%2520A%2520Personal%2520Journey%2520in%2520Building%250A%2520%2520a%2520Chinese-Centric%2520LLM%26entry.906535625%3DQingshui%2520Gu%2520and%2520Shu%2520Li%2520and%2520Tianyu%2520Zheng%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Steel-LLM%2520is%2520a%2520Chinese-centric%2520language%2520model%2520developed%2520from%2520scratch%2520with%2520the%250Agoal%2520of%2520creating%2520a%2520high-quality%252C%2520open-source%2520model%2520despite%2520limited%250Acomputational%2520resources.%2520Launched%2520in%2520March%25202024%252C%2520the%2520project%2520aimed%2520to%2520train%2520a%250A1-billion-parameter%2520model%2520on%2520a%2520large-scale%2520dataset%252C%2520prioritizing%2520transparency%250Aand%2520the%2520sharing%2520of%2520practical%2520insights%2520to%2520assist%2520others%2520in%2520the%2520community.%2520The%250Atraining%2520process%2520primarily%2520focused%2520on%2520Chinese%2520data%252C%2520with%2520a%2520small%2520proportion%2520of%250AEnglish%2520data%2520included%252C%2520addressing%2520gaps%2520in%2520existing%2520open-source%2520LLMs%2520by%250Aproviding%2520a%2520more%2520detailed%2520and%2520practical%2520account%2520of%2520the%2520model-building%2520journey.%250ASteel-LLM%2520has%2520demonstrated%2520competitive%2520performance%2520on%2520benchmarks%2520such%2520as%2520CEVAL%250Aand%2520CMMLU%252C%2520outperforming%2520early%2520models%2520from%2520larger%2520institutions.%2520This%2520paper%250Aprovides%2520a%2520comprehensive%2520summary%2520of%2520the%2520project%2527s%2520key%2520contributions%252C%2520including%250Adata%2520collection%252C%2520model%2520design%252C%2520training%2520methodologies%252C%2520and%2520the%2520challenges%250Aencountered%2520along%2520the%2520way%252C%2520offering%2520a%2520valuable%2520resource%2520for%2520researchers%2520and%250Apractitioners%2520looking%2520to%2520develop%2520their%2520own%2520LLMs.%2520The%2520model%2520checkpoints%2520and%250Atraining%2520script%2520are%2520available%2520at%2520https%253A//github.com/zhanshijinwat/Steel-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steel-LLM%3AFrom%20Scratch%20to%20Open%20Source%20--%20A%20Personal%20Journey%20in%20Building%0A%20%20a%20Chinese-Centric%20LLM&entry.906535625=Qingshui%20Gu%20and%20Shu%20Li%20and%20Tianyu%20Zheng%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Steel-LLM%20is%20a%20Chinese-centric%20language%20model%20developed%20from%20scratch%20with%20the%0Agoal%20of%20creating%20a%20high-quality%2C%20open-source%20model%20despite%20limited%0Acomputational%20resources.%20Launched%20in%20March%202024%2C%20the%20project%20aimed%20to%20train%20a%0A1-billion-parameter%20model%20on%20a%20large-scale%20dataset%2C%20prioritizing%20transparency%0Aand%20the%20sharing%20of%20practical%20insights%20to%20assist%20others%20in%20the%20community.%20The%0Atraining%20process%20primarily%20focused%20on%20Chinese%20data%2C%20with%20a%20small%20proportion%20of%0AEnglish%20data%20included%2C%20addressing%20gaps%20in%20existing%20open-source%20LLMs%20by%0Aproviding%20a%20more%20detailed%20and%20practical%20account%20of%20the%20model-building%20journey.%0ASteel-LLM%20has%20demonstrated%20competitive%20performance%20on%20benchmarks%20such%20as%20CEVAL%0Aand%20CMMLU%2C%20outperforming%20early%20models%20from%20larger%20institutions.%20This%20paper%0Aprovides%20a%20comprehensive%20summary%20of%20the%20project%27s%20key%20contributions%2C%20including%0Adata%20collection%2C%20model%20design%2C%20training%20methodologies%2C%20and%20the%20challenges%0Aencountered%20along%20the%20way%2C%20offering%20a%20valuable%20resource%20for%20researchers%20and%0Apractitioners%20looking%20to%20develop%20their%20own%20LLMs.%20The%20model%20checkpoints%20and%0Atraining%20script%20are%20available%20at%20https%3A//github.com/zhanshijinwat/Steel-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06635v1&entry.124074799=Read"},
{"title": "No Trick, No Treat: Pursuits and Challenges Towards Simulation-free\n  Training of Neural Samplers", "author": "Jiajun He and Yuanqi Du and Francisco Vargas and Dinghuai Zhang and Shreyas Padhy and RuiKang OuYang and Carla Gomes and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  We consider the sampling problem, where the aim is to draw samples from a\ndistribution whose density is known only up to a normalization constant. Recent\nbreakthroughs in generative modeling to approximate a high-dimensional data\ndistribution have sparked significant interest in developing neural\nnetwork-based methods for this challenging problem. However, neural samplers\ntypically incur heavy computational overhead due to simulating trajectories\nduring training. This motivates the pursuit of simulation-free training\nprocedures of neural samplers. In this work, we propose an elegant modification\nto previous methods, which allows simulation-free training with the help of a\ntime-dependent normalizing flow. However, it ultimately suffers from severe\nmode collapse. On closer inspection, we find that nearly all successful neural\nsamplers rely on Langevin preconditioning to avoid mode collapsing. We\nsystematically analyze several popular methods with various objective functions\nand demonstrate that, in the absence of Langevin preconditioning, most of them\nfail to adequately cover even a simple target. Finally, we draw attention to a\nstrong baseline by combining the state-of-the-art MCMC method, Parallel\nTempering (PT), with an additional generative model to shed light on future\nexplorations of neural samplers.\n", "link": "http://arxiv.org/abs/2502.06685v1", "date": "2025-02-10", "relevancy": 2.1535, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5439}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5367}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Trick%2C%20No%20Treat%3A%20Pursuits%20and%20Challenges%20Towards%20Simulation-free%0A%20%20Training%20of%20Neural%20Samplers&body=Title%3A%20No%20Trick%2C%20No%20Treat%3A%20Pursuits%20and%20Challenges%20Towards%20Simulation-free%0A%20%20Training%20of%20Neural%20Samplers%0AAuthor%3A%20Jiajun%20He%20and%20Yuanqi%20Du%20and%20Francisco%20Vargas%20and%20Dinghuai%20Zhang%20and%20Shreyas%20Padhy%20and%20RuiKang%20OuYang%20and%20Carla%20Gomes%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20We%20consider%20the%20sampling%20problem%2C%20where%20the%20aim%20is%20to%20draw%20samples%20from%20a%0Adistribution%20whose%20density%20is%20known%20only%20up%20to%20a%20normalization%20constant.%20Recent%0Abreakthroughs%20in%20generative%20modeling%20to%20approximate%20a%20high-dimensional%20data%0Adistribution%20have%20sparked%20significant%20interest%20in%20developing%20neural%0Anetwork-based%20methods%20for%20this%20challenging%20problem.%20However%2C%20neural%20samplers%0Atypically%20incur%20heavy%20computational%20overhead%20due%20to%20simulating%20trajectories%0Aduring%20training.%20This%20motivates%20the%20pursuit%20of%20simulation-free%20training%0Aprocedures%20of%20neural%20samplers.%20In%20this%20work%2C%20we%20propose%20an%20elegant%20modification%0Ato%20previous%20methods%2C%20which%20allows%20simulation-free%20training%20with%20the%20help%20of%20a%0Atime-dependent%20normalizing%20flow.%20However%2C%20it%20ultimately%20suffers%20from%20severe%0Amode%20collapse.%20On%20closer%20inspection%2C%20we%20find%20that%20nearly%20all%20successful%20neural%0Asamplers%20rely%20on%20Langevin%20preconditioning%20to%20avoid%20mode%20collapsing.%20We%0Asystematically%20analyze%20several%20popular%20methods%20with%20various%20objective%20functions%0Aand%20demonstrate%20that%2C%20in%20the%20absence%20of%20Langevin%20preconditioning%2C%20most%20of%20them%0Afail%20to%20adequately%20cover%20even%20a%20simple%20target.%20Finally%2C%20we%20draw%20attention%20to%20a%0Astrong%20baseline%20by%20combining%20the%20state-of-the-art%20MCMC%20method%2C%20Parallel%0ATempering%20%28PT%29%2C%20with%20an%20additional%20generative%20model%20to%20shed%20light%20on%20future%0Aexplorations%20of%20neural%20samplers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Trick%252C%2520No%2520Treat%253A%2520Pursuits%2520and%2520Challenges%2520Towards%2520Simulation-free%250A%2520%2520Training%2520of%2520Neural%2520Samplers%26entry.906535625%3DJiajun%2520He%2520and%2520Yuanqi%2520Du%2520and%2520Francisco%2520Vargas%2520and%2520Dinghuai%2520Zhang%2520and%2520Shreyas%2520Padhy%2520and%2520RuiKang%2520OuYang%2520and%2520Carla%2520Gomes%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520sampling%2520problem%252C%2520where%2520the%2520aim%2520is%2520to%2520draw%2520samples%2520from%2520a%250Adistribution%2520whose%2520density%2520is%2520known%2520only%2520up%2520to%2520a%2520normalization%2520constant.%2520Recent%250Abreakthroughs%2520in%2520generative%2520modeling%2520to%2520approximate%2520a%2520high-dimensional%2520data%250Adistribution%2520have%2520sparked%2520significant%2520interest%2520in%2520developing%2520neural%250Anetwork-based%2520methods%2520for%2520this%2520challenging%2520problem.%2520However%252C%2520neural%2520samplers%250Atypically%2520incur%2520heavy%2520computational%2520overhead%2520due%2520to%2520simulating%2520trajectories%250Aduring%2520training.%2520This%2520motivates%2520the%2520pursuit%2520of%2520simulation-free%2520training%250Aprocedures%2520of%2520neural%2520samplers.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520elegant%2520modification%250Ato%2520previous%2520methods%252C%2520which%2520allows%2520simulation-free%2520training%2520with%2520the%2520help%2520of%2520a%250Atime-dependent%2520normalizing%2520flow.%2520However%252C%2520it%2520ultimately%2520suffers%2520from%2520severe%250Amode%2520collapse.%2520On%2520closer%2520inspection%252C%2520we%2520find%2520that%2520nearly%2520all%2520successful%2520neural%250Asamplers%2520rely%2520on%2520Langevin%2520preconditioning%2520to%2520avoid%2520mode%2520collapsing.%2520We%250Asystematically%2520analyze%2520several%2520popular%2520methods%2520with%2520various%2520objective%2520functions%250Aand%2520demonstrate%2520that%252C%2520in%2520the%2520absence%2520of%2520Langevin%2520preconditioning%252C%2520most%2520of%2520them%250Afail%2520to%2520adequately%2520cover%2520even%2520a%2520simple%2520target.%2520Finally%252C%2520we%2520draw%2520attention%2520to%2520a%250Astrong%2520baseline%2520by%2520combining%2520the%2520state-of-the-art%2520MCMC%2520method%252C%2520Parallel%250ATempering%2520%2528PT%2529%252C%2520with%2520an%2520additional%2520generative%2520model%2520to%2520shed%2520light%2520on%2520future%250Aexplorations%2520of%2520neural%2520samplers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Trick%2C%20No%20Treat%3A%20Pursuits%20and%20Challenges%20Towards%20Simulation-free%0A%20%20Training%20of%20Neural%20Samplers&entry.906535625=Jiajun%20He%20and%20Yuanqi%20Du%20and%20Francisco%20Vargas%20and%20Dinghuai%20Zhang%20and%20Shreyas%20Padhy%20and%20RuiKang%20OuYang%20and%20Carla%20Gomes%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20We%20consider%20the%20sampling%20problem%2C%20where%20the%20aim%20is%20to%20draw%20samples%20from%20a%0Adistribution%20whose%20density%20is%20known%20only%20up%20to%20a%20normalization%20constant.%20Recent%0Abreakthroughs%20in%20generative%20modeling%20to%20approximate%20a%20high-dimensional%20data%0Adistribution%20have%20sparked%20significant%20interest%20in%20developing%20neural%0Anetwork-based%20methods%20for%20this%20challenging%20problem.%20However%2C%20neural%20samplers%0Atypically%20incur%20heavy%20computational%20overhead%20due%20to%20simulating%20trajectories%0Aduring%20training.%20This%20motivates%20the%20pursuit%20of%20simulation-free%20training%0Aprocedures%20of%20neural%20samplers.%20In%20this%20work%2C%20we%20propose%20an%20elegant%20modification%0Ato%20previous%20methods%2C%20which%20allows%20simulation-free%20training%20with%20the%20help%20of%20a%0Atime-dependent%20normalizing%20flow.%20However%2C%20it%20ultimately%20suffers%20from%20severe%0Amode%20collapse.%20On%20closer%20inspection%2C%20we%20find%20that%20nearly%20all%20successful%20neural%0Asamplers%20rely%20on%20Langevin%20preconditioning%20to%20avoid%20mode%20collapsing.%20We%0Asystematically%20analyze%20several%20popular%20methods%20with%20various%20objective%20functions%0Aand%20demonstrate%20that%2C%20in%20the%20absence%20of%20Langevin%20preconditioning%2C%20most%20of%20them%0Afail%20to%20adequately%20cover%20even%20a%20simple%20target.%20Finally%2C%20we%20draw%20attention%20to%20a%0Astrong%20baseline%20by%20combining%20the%20state-of-the-art%20MCMC%20method%2C%20Parallel%0ATempering%20%28PT%29%2C%20with%20an%20additional%20generative%20model%20to%20shed%20light%20on%20future%0Aexplorations%20of%20neural%20samplers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06685v1&entry.124074799=Read"},
{"title": "MoETuner: Optimized Mixture of Expert Serving with Balanced Expert\n  Placement and Token Routing", "author": "Seokjin Go and Divya Mahajan", "abstract": "  Mixture-of-Experts (MoE) model architecture has emerged as a promising\nsolution for scaling transformer models efficiently, offering sparse activation\nthat reduces computational costs while increasing model capacity. However, as\nMoE models scale, they need to be distributed across GPU devices, thus face\ncritical performance bottlenecks due to their large memory footprint. Expert\nparallelism distributes experts across GPUs, however, faces key challenges\nincluding an unbalanced token routing and expert activation, resulting in\ncommunication tail latency and processing inefficiencies. While existing\nsolutions address some of these issues, they fail to resolve the dual\nchallenges of load imbalance and communication skew. The imbalance in token\nprocessing load across experts causes uneven processing times on different\nGPUs, while communication skew between GPUs leads to unbalanced inter-GPU data\ntransfers. These factors degrade the performance of MoE models by increasing\ntail latency and reducing overall throughput. To address these limitations, we\npropose an Integer Linear Programming (ILP) formulation to optimize expert\nplacement by jointly considering token load, communication, and computation\ncosts. We exploit the property that there is a token routing dependency across\nlayers, where tokens routed to a specific expert in one layer are likely to be\nrouted to a limited set of experts in the subsequent layer. Our solution,\nMoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU\ntoken routing costs and balances token processing across devices, thereby\nreducing tail latency and end-to-end execution time. Experimental results\ndemonstrate 9.3% and 17.5% of end-to-end speedups for single-node and\nmulti-node inference respectively, showcasing the potential of our ILP-based\noptimization for offering expert parallel solutions for next-generation MoEs.\n", "link": "http://arxiv.org/abs/2502.06643v1", "date": "2025-02-10", "relevancy": 1.4155, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4784}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4639}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoETuner%3A%20Optimized%20Mixture%20of%20Expert%20Serving%20with%20Balanced%20Expert%0A%20%20Placement%20and%20Token%20Routing&body=Title%3A%20MoETuner%3A%20Optimized%20Mixture%20of%20Expert%20Serving%20with%20Balanced%20Expert%0A%20%20Placement%20and%20Token%20Routing%0AAuthor%3A%20Seokjin%20Go%20and%20Divya%20Mahajan%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20model%20architecture%20has%20emerged%20as%20a%20promising%0Asolution%20for%20scaling%20transformer%20models%20efficiently%2C%20offering%20sparse%20activation%0Athat%20reduces%20computational%20costs%20while%20increasing%20model%20capacity.%20However%2C%20as%0AMoE%20models%20scale%2C%20they%20need%20to%20be%20distributed%20across%20GPU%20devices%2C%20thus%20face%0Acritical%20performance%20bottlenecks%20due%20to%20their%20large%20memory%20footprint.%20Expert%0Aparallelism%20distributes%20experts%20across%20GPUs%2C%20however%2C%20faces%20key%20challenges%0Aincluding%20an%20unbalanced%20token%20routing%20and%20expert%20activation%2C%20resulting%20in%0Acommunication%20tail%20latency%20and%20processing%20inefficiencies.%20While%20existing%0Asolutions%20address%20some%20of%20these%20issues%2C%20they%20fail%20to%20resolve%20the%20dual%0Achallenges%20of%20load%20imbalance%20and%20communication%20skew.%20The%20imbalance%20in%20token%0Aprocessing%20load%20across%20experts%20causes%20uneven%20processing%20times%20on%20different%0AGPUs%2C%20while%20communication%20skew%20between%20GPUs%20leads%20to%20unbalanced%20inter-GPU%20data%0Atransfers.%20These%20factors%20degrade%20the%20performance%20of%20MoE%20models%20by%20increasing%0Atail%20latency%20and%20reducing%20overall%20throughput.%20To%20address%20these%20limitations%2C%20we%0Apropose%20an%20Integer%20Linear%20Programming%20%28ILP%29%20formulation%20to%20optimize%20expert%0Aplacement%20by%20jointly%20considering%20token%20load%2C%20communication%2C%20and%20computation%0Acosts.%20We%20exploit%20the%20property%20that%20there%20is%20a%20token%20routing%20dependency%20across%0Alayers%2C%20where%20tokens%20routed%20to%20a%20specific%20expert%20in%20one%20layer%20are%20likely%20to%20be%0Arouted%20to%20a%20limited%20set%20of%20experts%20in%20the%20subsequent%20layer.%20Our%20solution%2C%0AMoETuner%2C%20offers%20an%20optimal%20expert-to-GPU%20assignment%20that%20minimizes%20inter-GPU%0Atoken%20routing%20costs%20and%20balances%20token%20processing%20across%20devices%2C%20thereby%0Areducing%20tail%20latency%20and%20end-to-end%20execution%20time.%20Experimental%20results%0Ademonstrate%209.3%25%20and%2017.5%25%20of%20end-to-end%20speedups%20for%20single-node%20and%0Amulti-node%20inference%20respectively%2C%20showcasing%20the%20potential%20of%20our%20ILP-based%0Aoptimization%20for%20offering%20expert%20parallel%20solutions%20for%20next-generation%20MoEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoETuner%253A%2520Optimized%2520Mixture%2520of%2520Expert%2520Serving%2520with%2520Balanced%2520Expert%250A%2520%2520Placement%2520and%2520Token%2520Routing%26entry.906535625%3DSeokjin%2520Go%2520and%2520Divya%2520Mahajan%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520model%2520architecture%2520has%2520emerged%2520as%2520a%2520promising%250Asolution%2520for%2520scaling%2520transformer%2520models%2520efficiently%252C%2520offering%2520sparse%2520activation%250Athat%2520reduces%2520computational%2520costs%2520while%2520increasing%2520model%2520capacity.%2520However%252C%2520as%250AMoE%2520models%2520scale%252C%2520they%2520need%2520to%2520be%2520distributed%2520across%2520GPU%2520devices%252C%2520thus%2520face%250Acritical%2520performance%2520bottlenecks%2520due%2520to%2520their%2520large%2520memory%2520footprint.%2520Expert%250Aparallelism%2520distributes%2520experts%2520across%2520GPUs%252C%2520however%252C%2520faces%2520key%2520challenges%250Aincluding%2520an%2520unbalanced%2520token%2520routing%2520and%2520expert%2520activation%252C%2520resulting%2520in%250Acommunication%2520tail%2520latency%2520and%2520processing%2520inefficiencies.%2520While%2520existing%250Asolutions%2520address%2520some%2520of%2520these%2520issues%252C%2520they%2520fail%2520to%2520resolve%2520the%2520dual%250Achallenges%2520of%2520load%2520imbalance%2520and%2520communication%2520skew.%2520The%2520imbalance%2520in%2520token%250Aprocessing%2520load%2520across%2520experts%2520causes%2520uneven%2520processing%2520times%2520on%2520different%250AGPUs%252C%2520while%2520communication%2520skew%2520between%2520GPUs%2520leads%2520to%2520unbalanced%2520inter-GPU%2520data%250Atransfers.%2520These%2520factors%2520degrade%2520the%2520performance%2520of%2520MoE%2520models%2520by%2520increasing%250Atail%2520latency%2520and%2520reducing%2520overall%2520throughput.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520an%2520Integer%2520Linear%2520Programming%2520%2528ILP%2529%2520formulation%2520to%2520optimize%2520expert%250Aplacement%2520by%2520jointly%2520considering%2520token%2520load%252C%2520communication%252C%2520and%2520computation%250Acosts.%2520We%2520exploit%2520the%2520property%2520that%2520there%2520is%2520a%2520token%2520routing%2520dependency%2520across%250Alayers%252C%2520where%2520tokens%2520routed%2520to%2520a%2520specific%2520expert%2520in%2520one%2520layer%2520are%2520likely%2520to%2520be%250Arouted%2520to%2520a%2520limited%2520set%2520of%2520experts%2520in%2520the%2520subsequent%2520layer.%2520Our%2520solution%252C%250AMoETuner%252C%2520offers%2520an%2520optimal%2520expert-to-GPU%2520assignment%2520that%2520minimizes%2520inter-GPU%250Atoken%2520routing%2520costs%2520and%2520balances%2520token%2520processing%2520across%2520devices%252C%2520thereby%250Areducing%2520tail%2520latency%2520and%2520end-to-end%2520execution%2520time.%2520Experimental%2520results%250Ademonstrate%25209.3%2525%2520and%252017.5%2525%2520of%2520end-to-end%2520speedups%2520for%2520single-node%2520and%250Amulti-node%2520inference%2520respectively%252C%2520showcasing%2520the%2520potential%2520of%2520our%2520ILP-based%250Aoptimization%2520for%2520offering%2520expert%2520parallel%2520solutions%2520for%2520next-generation%2520MoEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoETuner%3A%20Optimized%20Mixture%20of%20Expert%20Serving%20with%20Balanced%20Expert%0A%20%20Placement%20and%20Token%20Routing&entry.906535625=Seokjin%20Go%20and%20Divya%20Mahajan&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20model%20architecture%20has%20emerged%20as%20a%20promising%0Asolution%20for%20scaling%20transformer%20models%20efficiently%2C%20offering%20sparse%20activation%0Athat%20reduces%20computational%20costs%20while%20increasing%20model%20capacity.%20However%2C%20as%0AMoE%20models%20scale%2C%20they%20need%20to%20be%20distributed%20across%20GPU%20devices%2C%20thus%20face%0Acritical%20performance%20bottlenecks%20due%20to%20their%20large%20memory%20footprint.%20Expert%0Aparallelism%20distributes%20experts%20across%20GPUs%2C%20however%2C%20faces%20key%20challenges%0Aincluding%20an%20unbalanced%20token%20routing%20and%20expert%20activation%2C%20resulting%20in%0Acommunication%20tail%20latency%20and%20processing%20inefficiencies.%20While%20existing%0Asolutions%20address%20some%20of%20these%20issues%2C%20they%20fail%20to%20resolve%20the%20dual%0Achallenges%20of%20load%20imbalance%20and%20communication%20skew.%20The%20imbalance%20in%20token%0Aprocessing%20load%20across%20experts%20causes%20uneven%20processing%20times%20on%20different%0AGPUs%2C%20while%20communication%20skew%20between%20GPUs%20leads%20to%20unbalanced%20inter-GPU%20data%0Atransfers.%20These%20factors%20degrade%20the%20performance%20of%20MoE%20models%20by%20increasing%0Atail%20latency%20and%20reducing%20overall%20throughput.%20To%20address%20these%20limitations%2C%20we%0Apropose%20an%20Integer%20Linear%20Programming%20%28ILP%29%20formulation%20to%20optimize%20expert%0Aplacement%20by%20jointly%20considering%20token%20load%2C%20communication%2C%20and%20computation%0Acosts.%20We%20exploit%20the%20property%20that%20there%20is%20a%20token%20routing%20dependency%20across%0Alayers%2C%20where%20tokens%20routed%20to%20a%20specific%20expert%20in%20one%20layer%20are%20likely%20to%20be%0Arouted%20to%20a%20limited%20set%20of%20experts%20in%20the%20subsequent%20layer.%20Our%20solution%2C%0AMoETuner%2C%20offers%20an%20optimal%20expert-to-GPU%20assignment%20that%20minimizes%20inter-GPU%0Atoken%20routing%20costs%20and%20balances%20token%20processing%20across%20devices%2C%20thereby%0Areducing%20tail%20latency%20and%20end-to-end%20execution%20time.%20Experimental%20results%0Ademonstrate%209.3%25%20and%2017.5%25%20of%20end-to-end%20speedups%20for%20single-node%20and%0Amulti-node%20inference%20respectively%2C%20showcasing%20the%20potential%20of%20our%20ILP-based%0Aoptimization%20for%20offering%20expert%20parallel%20solutions%20for%20next-generation%20MoEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06643v1&entry.124074799=Read"},
{"title": "Multitask Learning in Minimally Invasive Surgical Vision: A Review", "author": "Oluwatosin Alabi and Tom Vercauteren and Miaojing Shi", "abstract": "  Minimally invasive surgery (MIS) has revolutionized many procedures and led\nto reduced recovery time and risk of patient injury. However, MIS poses\nadditional complexity and burden on surgical teams. Data-driven surgical vision\nalgorithms are thought to be key building blocks in the development of future\nMIS systems with improved autonomy. Recent advancements in machine learning and\ncomputer vision have led to successful applications in analyzing videos\nobtained from MIS with the promise of alleviating challenges in MIS videos.\nSurgical scene and action understanding encompasses multiple related tasks\nthat, when solved individually, can be memory-intensive, inefficient, and fail\nto capture task relationships. Multitask learning (MTL), a learning paradigm\nthat leverages information from multiple related tasks to improve performance\nand aid generalization, is well suited for fine-grained and high-level\nunderstanding of MIS data. This review provides a narrative overview of the\ncurrent state-of-the-art MTL systems that leverage videos obtained from MIS.\nBeyond listing published approaches, we discuss the benefits and limitations of\nthese MTL systems. Moreover, this manuscript presents an analysis of the\nliterature for various application fields of MTL in MIS, including those with\nlarge models, highlighting notable trends, new directions of research, and\ndevelopments.\n", "link": "http://arxiv.org/abs/2401.08256v2", "date": "2025-02-10", "relevancy": 2.0492, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multitask%20Learning%20in%20Minimally%20Invasive%20Surgical%20Vision%3A%20A%20Review&body=Title%3A%20Multitask%20Learning%20in%20Minimally%20Invasive%20Surgical%20Vision%3A%20A%20Review%0AAuthor%3A%20Oluwatosin%20Alabi%20and%20Tom%20Vercauteren%20and%20Miaojing%20Shi%0AAbstract%3A%20%20%20Minimally%20invasive%20surgery%20%28MIS%29%20has%20revolutionized%20many%20procedures%20and%20led%0Ato%20reduced%20recovery%20time%20and%20risk%20of%20patient%20injury.%20However%2C%20MIS%20poses%0Aadditional%20complexity%20and%20burden%20on%20surgical%20teams.%20Data-driven%20surgical%20vision%0Aalgorithms%20are%20thought%20to%20be%20key%20building%20blocks%20in%20the%20development%20of%20future%0AMIS%20systems%20with%20improved%20autonomy.%20Recent%20advancements%20in%20machine%20learning%20and%0Acomputer%20vision%20have%20led%20to%20successful%20applications%20in%20analyzing%20videos%0Aobtained%20from%20MIS%20with%20the%20promise%20of%20alleviating%20challenges%20in%20MIS%20videos.%0ASurgical%20scene%20and%20action%20understanding%20encompasses%20multiple%20related%20tasks%0Athat%2C%20when%20solved%20individually%2C%20can%20be%20memory-intensive%2C%20inefficient%2C%20and%20fail%0Ato%20capture%20task%20relationships.%20Multitask%20learning%20%28MTL%29%2C%20a%20learning%20paradigm%0Athat%20leverages%20information%20from%20multiple%20related%20tasks%20to%20improve%20performance%0Aand%20aid%20generalization%2C%20is%20well%20suited%20for%20fine-grained%20and%20high-level%0Aunderstanding%20of%20MIS%20data.%20This%20review%20provides%20a%20narrative%20overview%20of%20the%0Acurrent%20state-of-the-art%20MTL%20systems%20that%20leverage%20videos%20obtained%20from%20MIS.%0ABeyond%20listing%20published%20approaches%2C%20we%20discuss%20the%20benefits%20and%20limitations%20of%0Athese%20MTL%20systems.%20Moreover%2C%20this%20manuscript%20presents%20an%20analysis%20of%20the%0Aliterature%20for%20various%20application%20fields%20of%20MTL%20in%20MIS%2C%20including%20those%20with%0Alarge%20models%2C%20highlighting%20notable%20trends%2C%20new%20directions%20of%20research%2C%20and%0Adevelopments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08256v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultitask%2520Learning%2520in%2520Minimally%2520Invasive%2520Surgical%2520Vision%253A%2520A%2520Review%26entry.906535625%3DOluwatosin%2520Alabi%2520and%2520Tom%2520Vercauteren%2520and%2520Miaojing%2520Shi%26entry.1292438233%3D%2520%2520Minimally%2520invasive%2520surgery%2520%2528MIS%2529%2520has%2520revolutionized%2520many%2520procedures%2520and%2520led%250Ato%2520reduced%2520recovery%2520time%2520and%2520risk%2520of%2520patient%2520injury.%2520However%252C%2520MIS%2520poses%250Aadditional%2520complexity%2520and%2520burden%2520on%2520surgical%2520teams.%2520Data-driven%2520surgical%2520vision%250Aalgorithms%2520are%2520thought%2520to%2520be%2520key%2520building%2520blocks%2520in%2520the%2520development%2520of%2520future%250AMIS%2520systems%2520with%2520improved%2520autonomy.%2520Recent%2520advancements%2520in%2520machine%2520learning%2520and%250Acomputer%2520vision%2520have%2520led%2520to%2520successful%2520applications%2520in%2520analyzing%2520videos%250Aobtained%2520from%2520MIS%2520with%2520the%2520promise%2520of%2520alleviating%2520challenges%2520in%2520MIS%2520videos.%250ASurgical%2520scene%2520and%2520action%2520understanding%2520encompasses%2520multiple%2520related%2520tasks%250Athat%252C%2520when%2520solved%2520individually%252C%2520can%2520be%2520memory-intensive%252C%2520inefficient%252C%2520and%2520fail%250Ato%2520capture%2520task%2520relationships.%2520Multitask%2520learning%2520%2528MTL%2529%252C%2520a%2520learning%2520paradigm%250Athat%2520leverages%2520information%2520from%2520multiple%2520related%2520tasks%2520to%2520improve%2520performance%250Aand%2520aid%2520generalization%252C%2520is%2520well%2520suited%2520for%2520fine-grained%2520and%2520high-level%250Aunderstanding%2520of%2520MIS%2520data.%2520This%2520review%2520provides%2520a%2520narrative%2520overview%2520of%2520the%250Acurrent%2520state-of-the-art%2520MTL%2520systems%2520that%2520leverage%2520videos%2520obtained%2520from%2520MIS.%250ABeyond%2520listing%2520published%2520approaches%252C%2520we%2520discuss%2520the%2520benefits%2520and%2520limitations%2520of%250Athese%2520MTL%2520systems.%2520Moreover%252C%2520this%2520manuscript%2520presents%2520an%2520analysis%2520of%2520the%250Aliterature%2520for%2520various%2520application%2520fields%2520of%2520MTL%2520in%2520MIS%252C%2520including%2520those%2520with%250Alarge%2520models%252C%2520highlighting%2520notable%2520trends%252C%2520new%2520directions%2520of%2520research%252C%2520and%250Adevelopments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08256v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multitask%20Learning%20in%20Minimally%20Invasive%20Surgical%20Vision%3A%20A%20Review&entry.906535625=Oluwatosin%20Alabi%20and%20Tom%20Vercauteren%20and%20Miaojing%20Shi&entry.1292438233=%20%20Minimally%20invasive%20surgery%20%28MIS%29%20has%20revolutionized%20many%20procedures%20and%20led%0Ato%20reduced%20recovery%20time%20and%20risk%20of%20patient%20injury.%20However%2C%20MIS%20poses%0Aadditional%20complexity%20and%20burden%20on%20surgical%20teams.%20Data-driven%20surgical%20vision%0Aalgorithms%20are%20thought%20to%20be%20key%20building%20blocks%20in%20the%20development%20of%20future%0AMIS%20systems%20with%20improved%20autonomy.%20Recent%20advancements%20in%20machine%20learning%20and%0Acomputer%20vision%20have%20led%20to%20successful%20applications%20in%20analyzing%20videos%0Aobtained%20from%20MIS%20with%20the%20promise%20of%20alleviating%20challenges%20in%20MIS%20videos.%0ASurgical%20scene%20and%20action%20understanding%20encompasses%20multiple%20related%20tasks%0Athat%2C%20when%20solved%20individually%2C%20can%20be%20memory-intensive%2C%20inefficient%2C%20and%20fail%0Ato%20capture%20task%20relationships.%20Multitask%20learning%20%28MTL%29%2C%20a%20learning%20paradigm%0Athat%20leverages%20information%20from%20multiple%20related%20tasks%20to%20improve%20performance%0Aand%20aid%20generalization%2C%20is%20well%20suited%20for%20fine-grained%20and%20high-level%0Aunderstanding%20of%20MIS%20data.%20This%20review%20provides%20a%20narrative%20overview%20of%20the%0Acurrent%20state-of-the-art%20MTL%20systems%20that%20leverage%20videos%20obtained%20from%20MIS.%0ABeyond%20listing%20published%20approaches%2C%20we%20discuss%20the%20benefits%20and%20limitations%20of%0Athese%20MTL%20systems.%20Moreover%2C%20this%20manuscript%20presents%20an%20analysis%20of%20the%0Aliterature%20for%20various%20application%20fields%20of%20MTL%20in%20MIS%2C%20including%20those%20with%0Alarge%20models%2C%20highlighting%20notable%20trends%2C%20new%20directions%20of%20research%2C%20and%0Adevelopments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08256v2&entry.124074799=Read"},
{"title": "RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and\n  Service Provider Management in Multi-Domain Networks", "author": "Cyril Shih-Huan Hsu and Chrysa Papagianni and Paola Grosso", "abstract": "  The emergence of the fifth generation (5G) technology has transformed mobile\nnetworks into multi-service environments, necessitating efficient network\nslicing to meet diverse Service Level Agreements (SLAs). SLA decomposition\nacross multiple network domains, each potentially managed by different service\nproviders, poses a significant challenge due to limited visibility into\nreal-time underlying domain conditions. This paper introduces Risk-Aware\nIterated Local Search (RAILS), a novel risk model-driven meta-heuristic\nframework designed to jointly address SLA decomposition and service provider\nselection in multi-domain networks. By integrating online risk modeling with\niterated local search principles, RAILS effectively navigates the complex\noptimization landscape, utilizing historical feedback from domain controllers.\nWe formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP)\nproblem and prove its NP-hardness. Extensive simulations demonstrate that RAILS\nachieves near-optimal performance, offering an efficient, real-time solution\nfor adaptive SLA management in modern multi-domain networks.\n", "link": "http://arxiv.org/abs/2502.06674v1", "date": "2025-02-10", "relevancy": 1.8465, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4728}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4615}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAILS%3A%20Risk-Aware%20Iterated%20Local%20Search%20for%20Joint%20SLA%20Decomposition%20and%0A%20%20Service%20Provider%20Management%20in%20Multi-Domain%20Networks&body=Title%3A%20RAILS%3A%20Risk-Aware%20Iterated%20Local%20Search%20for%20Joint%20SLA%20Decomposition%20and%0A%20%20Service%20Provider%20Management%20in%20Multi-Domain%20Networks%0AAuthor%3A%20Cyril%20Shih-Huan%20Hsu%20and%20Chrysa%20Papagianni%20and%20Paola%20Grosso%0AAbstract%3A%20%20%20The%20emergence%20of%20the%20fifth%20generation%20%285G%29%20technology%20has%20transformed%20mobile%0Anetworks%20into%20multi-service%20environments%2C%20necessitating%20efficient%20network%0Aslicing%20to%20meet%20diverse%20Service%20Level%20Agreements%20%28SLAs%29.%20SLA%20decomposition%0Aacross%20multiple%20network%20domains%2C%20each%20potentially%20managed%20by%20different%20service%0Aproviders%2C%20poses%20a%20significant%20challenge%20due%20to%20limited%20visibility%20into%0Areal-time%20underlying%20domain%20conditions.%20This%20paper%20introduces%20Risk-Aware%0AIterated%20Local%20Search%20%28RAILS%29%2C%20a%20novel%20risk%20model-driven%20meta-heuristic%0Aframework%20designed%20to%20jointly%20address%20SLA%20decomposition%20and%20service%20provider%0Aselection%20in%20multi-domain%20networks.%20By%20integrating%20online%20risk%20modeling%20with%0Aiterated%20local%20search%20principles%2C%20RAILS%20effectively%20navigates%20the%20complex%0Aoptimization%20landscape%2C%20utilizing%20historical%20feedback%20from%20domain%20controllers.%0AWe%20formulate%20the%20joint%20problem%20as%20a%20Mixed-Integer%20Nonlinear%20Programming%20%28MINLP%29%0Aproblem%20and%20prove%20its%20NP-hardness.%20Extensive%20simulations%20demonstrate%20that%20RAILS%0Aachieves%20near-optimal%20performance%2C%20offering%20an%20efficient%2C%20real-time%20solution%0Afor%20adaptive%20SLA%20management%20in%20modern%20multi-domain%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAILS%253A%2520Risk-Aware%2520Iterated%2520Local%2520Search%2520for%2520Joint%2520SLA%2520Decomposition%2520and%250A%2520%2520Service%2520Provider%2520Management%2520in%2520Multi-Domain%2520Networks%26entry.906535625%3DCyril%2520Shih-Huan%2520Hsu%2520and%2520Chrysa%2520Papagianni%2520and%2520Paola%2520Grosso%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520the%2520fifth%2520generation%2520%25285G%2529%2520technology%2520has%2520transformed%2520mobile%250Anetworks%2520into%2520multi-service%2520environments%252C%2520necessitating%2520efficient%2520network%250Aslicing%2520to%2520meet%2520diverse%2520Service%2520Level%2520Agreements%2520%2528SLAs%2529.%2520SLA%2520decomposition%250Aacross%2520multiple%2520network%2520domains%252C%2520each%2520potentially%2520managed%2520by%2520different%2520service%250Aproviders%252C%2520poses%2520a%2520significant%2520challenge%2520due%2520to%2520limited%2520visibility%2520into%250Areal-time%2520underlying%2520domain%2520conditions.%2520This%2520paper%2520introduces%2520Risk-Aware%250AIterated%2520Local%2520Search%2520%2528RAILS%2529%252C%2520a%2520novel%2520risk%2520model-driven%2520meta-heuristic%250Aframework%2520designed%2520to%2520jointly%2520address%2520SLA%2520decomposition%2520and%2520service%2520provider%250Aselection%2520in%2520multi-domain%2520networks.%2520By%2520integrating%2520online%2520risk%2520modeling%2520with%250Aiterated%2520local%2520search%2520principles%252C%2520RAILS%2520effectively%2520navigates%2520the%2520complex%250Aoptimization%2520landscape%252C%2520utilizing%2520historical%2520feedback%2520from%2520domain%2520controllers.%250AWe%2520formulate%2520the%2520joint%2520problem%2520as%2520a%2520Mixed-Integer%2520Nonlinear%2520Programming%2520%2528MINLP%2529%250Aproblem%2520and%2520prove%2520its%2520NP-hardness.%2520Extensive%2520simulations%2520demonstrate%2520that%2520RAILS%250Aachieves%2520near-optimal%2520performance%252C%2520offering%2520an%2520efficient%252C%2520real-time%2520solution%250Afor%2520adaptive%2520SLA%2520management%2520in%2520modern%2520multi-domain%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAILS%3A%20Risk-Aware%20Iterated%20Local%20Search%20for%20Joint%20SLA%20Decomposition%20and%0A%20%20Service%20Provider%20Management%20in%20Multi-Domain%20Networks&entry.906535625=Cyril%20Shih-Huan%20Hsu%20and%20Chrysa%20Papagianni%20and%20Paola%20Grosso&entry.1292438233=%20%20The%20emergence%20of%20the%20fifth%20generation%20%285G%29%20technology%20has%20transformed%20mobile%0Anetworks%20into%20multi-service%20environments%2C%20necessitating%20efficient%20network%0Aslicing%20to%20meet%20diverse%20Service%20Level%20Agreements%20%28SLAs%29.%20SLA%20decomposition%0Aacross%20multiple%20network%20domains%2C%20each%20potentially%20managed%20by%20different%20service%0Aproviders%2C%20poses%20a%20significant%20challenge%20due%20to%20limited%20visibility%20into%0Areal-time%20underlying%20domain%20conditions.%20This%20paper%20introduces%20Risk-Aware%0AIterated%20Local%20Search%20%28RAILS%29%2C%20a%20novel%20risk%20model-driven%20meta-heuristic%0Aframework%20designed%20to%20jointly%20address%20SLA%20decomposition%20and%20service%20provider%0Aselection%20in%20multi-domain%20networks.%20By%20integrating%20online%20risk%20modeling%20with%0Aiterated%20local%20search%20principles%2C%20RAILS%20effectively%20navigates%20the%20complex%0Aoptimization%20landscape%2C%20utilizing%20historical%20feedback%20from%20domain%20controllers.%0AWe%20formulate%20the%20joint%20problem%20as%20a%20Mixed-Integer%20Nonlinear%20Programming%20%28MINLP%29%0Aproblem%20and%20prove%20its%20NP-hardness.%20Extensive%20simulations%20demonstrate%20that%20RAILS%0Aachieves%20near-optimal%20performance%2C%20offering%20an%20efficient%2C%20real-time%20solution%0Afor%20adaptive%20SLA%20management%20in%20modern%20multi-domain%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06674v1&entry.124074799=Read"},
{"title": "Sequence Transferability and Task Order Selection in Continual Learning", "author": "Thinh Nguyen and Cuong N. Nguyen and Quang Pham and Binh T. Nguyen and Savitha Ramasamy and Xiaoli Li and Cuong V. Nguyen", "abstract": "  In continual learning, understanding the properties of task sequences and\ntheir relationships to model performance is important for developing advanced\nalgorithms with better accuracy. However, efforts in this direction remain\nunderdeveloped despite encouraging progress in methodology development. In this\nwork, we investigate the impacts of sequence transferability on continual\nlearning and propose two novel measures that capture the total transferability\nof a task sequence, either in the forward or backward direction. Based on the\nempirical properties of these measures, we then develop a new method for the\ntask order selection problem in continual learning. Our method can be shown to\noffer a better performance than the conventional strategy of random task\nselection.\n", "link": "http://arxiv.org/abs/2502.06544v1", "date": "2025-02-10", "relevancy": 1.9067, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4829}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequence%20Transferability%20and%20Task%20Order%20Selection%20in%20Continual%20Learning&body=Title%3A%20Sequence%20Transferability%20and%20Task%20Order%20Selection%20in%20Continual%20Learning%0AAuthor%3A%20Thinh%20Nguyen%20and%20Cuong%20N.%20Nguyen%20and%20Quang%20Pham%20and%20Binh%20T.%20Nguyen%20and%20Savitha%20Ramasamy%20and%20Xiaoli%20Li%20and%20Cuong%20V.%20Nguyen%0AAbstract%3A%20%20%20In%20continual%20learning%2C%20understanding%20the%20properties%20of%20task%20sequences%20and%0Atheir%20relationships%20to%20model%20performance%20is%20important%20for%20developing%20advanced%0Aalgorithms%20with%20better%20accuracy.%20However%2C%20efforts%20in%20this%20direction%20remain%0Aunderdeveloped%20despite%20encouraging%20progress%20in%20methodology%20development.%20In%20this%0Awork%2C%20we%20investigate%20the%20impacts%20of%20sequence%20transferability%20on%20continual%0Alearning%20and%20propose%20two%20novel%20measures%20that%20capture%20the%20total%20transferability%0Aof%20a%20task%20sequence%2C%20either%20in%20the%20forward%20or%20backward%20direction.%20Based%20on%20the%0Aempirical%20properties%20of%20these%20measures%2C%20we%20then%20develop%20a%20new%20method%20for%20the%0Atask%20order%20selection%20problem%20in%20continual%20learning.%20Our%20method%20can%20be%20shown%20to%0Aoffer%20a%20better%20performance%20than%20the%20conventional%20strategy%20of%20random%20task%0Aselection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequence%2520Transferability%2520and%2520Task%2520Order%2520Selection%2520in%2520Continual%2520Learning%26entry.906535625%3DThinh%2520Nguyen%2520and%2520Cuong%2520N.%2520Nguyen%2520and%2520Quang%2520Pham%2520and%2520Binh%2520T.%2520Nguyen%2520and%2520Savitha%2520Ramasamy%2520and%2520Xiaoli%2520Li%2520and%2520Cuong%2520V.%2520Nguyen%26entry.1292438233%3D%2520%2520In%2520continual%2520learning%252C%2520understanding%2520the%2520properties%2520of%2520task%2520sequences%2520and%250Atheir%2520relationships%2520to%2520model%2520performance%2520is%2520important%2520for%2520developing%2520advanced%250Aalgorithms%2520with%2520better%2520accuracy.%2520However%252C%2520efforts%2520in%2520this%2520direction%2520remain%250Aunderdeveloped%2520despite%2520encouraging%2520progress%2520in%2520methodology%2520development.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520the%2520impacts%2520of%2520sequence%2520transferability%2520on%2520continual%250Alearning%2520and%2520propose%2520two%2520novel%2520measures%2520that%2520capture%2520the%2520total%2520transferability%250Aof%2520a%2520task%2520sequence%252C%2520either%2520in%2520the%2520forward%2520or%2520backward%2520direction.%2520Based%2520on%2520the%250Aempirical%2520properties%2520of%2520these%2520measures%252C%2520we%2520then%2520develop%2520a%2520new%2520method%2520for%2520the%250Atask%2520order%2520selection%2520problem%2520in%2520continual%2520learning.%2520Our%2520method%2520can%2520be%2520shown%2520to%250Aoffer%2520a%2520better%2520performance%2520than%2520the%2520conventional%2520strategy%2520of%2520random%2520task%250Aselection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequence%20Transferability%20and%20Task%20Order%20Selection%20in%20Continual%20Learning&entry.906535625=Thinh%20Nguyen%20and%20Cuong%20N.%20Nguyen%20and%20Quang%20Pham%20and%20Binh%20T.%20Nguyen%20and%20Savitha%20Ramasamy%20and%20Xiaoli%20Li%20and%20Cuong%20V.%20Nguyen&entry.1292438233=%20%20In%20continual%20learning%2C%20understanding%20the%20properties%20of%20task%20sequences%20and%0Atheir%20relationships%20to%20model%20performance%20is%20important%20for%20developing%20advanced%0Aalgorithms%20with%20better%20accuracy.%20However%2C%20efforts%20in%20this%20direction%20remain%0Aunderdeveloped%20despite%20encouraging%20progress%20in%20methodology%20development.%20In%20this%0Awork%2C%20we%20investigate%20the%20impacts%20of%20sequence%20transferability%20on%20continual%0Alearning%20and%20propose%20two%20novel%20measures%20that%20capture%20the%20total%20transferability%0Aof%20a%20task%20sequence%2C%20either%20in%20the%20forward%20or%20backward%20direction.%20Based%20on%20the%0Aempirical%20properties%20of%20these%20measures%2C%20we%20then%20develop%20a%20new%20method%20for%20the%0Atask%20order%20selection%20problem%20in%20continual%20learning.%20Our%20method%20can%20be%20shown%20to%0Aoffer%20a%20better%20performance%20than%20the%20conventional%20strategy%20of%20random%20task%0Aselection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06544v1&entry.124074799=Read"},
{"title": "An Automated Machine Learning Framework for Surgical Suturing Action\n  Detection under Class Imbalance", "author": "Baobing Zhang and Paul Sullivan and Benjie Tang and Ghulam Nabi and Mustafa Suphi Erden", "abstract": "  In laparoscopy surgical training and evaluation, real-time detection of\nsurgical actions with interpretable outputs is crucial for automated and\nreal-time instructional feedback and skill development. Such capability would\nenable development of machine guided training systems. This paper presents a\nrapid deployment approach utilizing automated machine learning methods, based\non surgical action data collected from both experienced and trainee surgeons.\nThe proposed approach effectively tackles the challenge of highly imbalanced\nclass distributions, ensuring robust predictions across varying skill levels of\nsurgeons. Additionally, our method partially incorporates model transparency,\naddressing the reliability requirements in medical applications. Compared to\ndeep learning approaches, traditional machine learning models not only\nfacilitate efficient rapid deployment but also offer significant advantages in\ninterpretability. Through experiments, this study demonstrates the potential of\nthis approach to provide quick, reliable and effective real-time detection in\nsurgical training environments\n", "link": "http://arxiv.org/abs/2502.06407v1", "date": "2025-02-10", "relevancy": 2.0668, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5468}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5135}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Automated%20Machine%20Learning%20Framework%20for%20Surgical%20Suturing%20Action%0A%20%20Detection%20under%20Class%20Imbalance&body=Title%3A%20An%20Automated%20Machine%20Learning%20Framework%20for%20Surgical%20Suturing%20Action%0A%20%20Detection%20under%20Class%20Imbalance%0AAuthor%3A%20Baobing%20Zhang%20and%20Paul%20Sullivan%20and%20Benjie%20Tang%20and%20Ghulam%20Nabi%20and%20Mustafa%20Suphi%20Erden%0AAbstract%3A%20%20%20In%20laparoscopy%20surgical%20training%20and%20evaluation%2C%20real-time%20detection%20of%0Asurgical%20actions%20with%20interpretable%20outputs%20is%20crucial%20for%20automated%20and%0Areal-time%20instructional%20feedback%20and%20skill%20development.%20Such%20capability%20would%0Aenable%20development%20of%20machine%20guided%20training%20systems.%20This%20paper%20presents%20a%0Arapid%20deployment%20approach%20utilizing%20automated%20machine%20learning%20methods%2C%20based%0Aon%20surgical%20action%20data%20collected%20from%20both%20experienced%20and%20trainee%20surgeons.%0AThe%20proposed%20approach%20effectively%20tackles%20the%20challenge%20of%20highly%20imbalanced%0Aclass%20distributions%2C%20ensuring%20robust%20predictions%20across%20varying%20skill%20levels%20of%0Asurgeons.%20Additionally%2C%20our%20method%20partially%20incorporates%20model%20transparency%2C%0Aaddressing%20the%20reliability%20requirements%20in%20medical%20applications.%20Compared%20to%0Adeep%20learning%20approaches%2C%20traditional%20machine%20learning%20models%20not%20only%0Afacilitate%20efficient%20rapid%20deployment%20but%20also%20offer%20significant%20advantages%20in%0Ainterpretability.%20Through%20experiments%2C%20this%20study%20demonstrates%20the%20potential%20of%0Athis%20approach%20to%20provide%20quick%2C%20reliable%20and%20effective%20real-time%20detection%20in%0Asurgical%20training%20environments%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Automated%2520Machine%2520Learning%2520Framework%2520for%2520Surgical%2520Suturing%2520Action%250A%2520%2520Detection%2520under%2520Class%2520Imbalance%26entry.906535625%3DBaobing%2520Zhang%2520and%2520Paul%2520Sullivan%2520and%2520Benjie%2520Tang%2520and%2520Ghulam%2520Nabi%2520and%2520Mustafa%2520Suphi%2520Erden%26entry.1292438233%3D%2520%2520In%2520laparoscopy%2520surgical%2520training%2520and%2520evaluation%252C%2520real-time%2520detection%2520of%250Asurgical%2520actions%2520with%2520interpretable%2520outputs%2520is%2520crucial%2520for%2520automated%2520and%250Areal-time%2520instructional%2520feedback%2520and%2520skill%2520development.%2520Such%2520capability%2520would%250Aenable%2520development%2520of%2520machine%2520guided%2520training%2520systems.%2520This%2520paper%2520presents%2520a%250Arapid%2520deployment%2520approach%2520utilizing%2520automated%2520machine%2520learning%2520methods%252C%2520based%250Aon%2520surgical%2520action%2520data%2520collected%2520from%2520both%2520experienced%2520and%2520trainee%2520surgeons.%250AThe%2520proposed%2520approach%2520effectively%2520tackles%2520the%2520challenge%2520of%2520highly%2520imbalanced%250Aclass%2520distributions%252C%2520ensuring%2520robust%2520predictions%2520across%2520varying%2520skill%2520levels%2520of%250Asurgeons.%2520Additionally%252C%2520our%2520method%2520partially%2520incorporates%2520model%2520transparency%252C%250Aaddressing%2520the%2520reliability%2520requirements%2520in%2520medical%2520applications.%2520Compared%2520to%250Adeep%2520learning%2520approaches%252C%2520traditional%2520machine%2520learning%2520models%2520not%2520only%250Afacilitate%2520efficient%2520rapid%2520deployment%2520but%2520also%2520offer%2520significant%2520advantages%2520in%250Ainterpretability.%2520Through%2520experiments%252C%2520this%2520study%2520demonstrates%2520the%2520potential%2520of%250Athis%2520approach%2520to%2520provide%2520quick%252C%2520reliable%2520and%2520effective%2520real-time%2520detection%2520in%250Asurgical%2520training%2520environments%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Automated%20Machine%20Learning%20Framework%20for%20Surgical%20Suturing%20Action%0A%20%20Detection%20under%20Class%20Imbalance&entry.906535625=Baobing%20Zhang%20and%20Paul%20Sullivan%20and%20Benjie%20Tang%20and%20Ghulam%20Nabi%20and%20Mustafa%20Suphi%20Erden&entry.1292438233=%20%20In%20laparoscopy%20surgical%20training%20and%20evaluation%2C%20real-time%20detection%20of%0Asurgical%20actions%20with%20interpretable%20outputs%20is%20crucial%20for%20automated%20and%0Areal-time%20instructional%20feedback%20and%20skill%20development.%20Such%20capability%20would%0Aenable%20development%20of%20machine%20guided%20training%20systems.%20This%20paper%20presents%20a%0Arapid%20deployment%20approach%20utilizing%20automated%20machine%20learning%20methods%2C%20based%0Aon%20surgical%20action%20data%20collected%20from%20both%20experienced%20and%20trainee%20surgeons.%0AThe%20proposed%20approach%20effectively%20tackles%20the%20challenge%20of%20highly%20imbalanced%0Aclass%20distributions%2C%20ensuring%20robust%20predictions%20across%20varying%20skill%20levels%20of%0Asurgeons.%20Additionally%2C%20our%20method%20partially%20incorporates%20model%20transparency%2C%0Aaddressing%20the%20reliability%20requirements%20in%20medical%20applications.%20Compared%20to%0Adeep%20learning%20approaches%2C%20traditional%20machine%20learning%20models%20not%20only%0Afacilitate%20efficient%20rapid%20deployment%20but%20also%20offer%20significant%20advantages%20in%0Ainterpretability.%20Through%20experiments%2C%20this%20study%20demonstrates%20the%20potential%20of%0Athis%20approach%20to%20provide%20quick%2C%20reliable%20and%20effective%20real-time%20detection%20in%0Asurgical%20training%20environments%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06407v1&entry.124074799=Read"},
{"title": "SparseFocus: Learning-based One-shot Autofocus for Microscopy with\n  Sparse Content", "author": "Yongping Zhai and Xiaoxi Fu and Qiang Su and Jia Hu and Yake Zhang and Yunfeng Zhou and Chaofan Zhang and Xiao Li and Wenxin Wang and Dongdong Wu and Shen Yan", "abstract": "  Autofocus is necessary for high-throughput and real-time scanning in\nmicroscopic imaging. Traditional methods rely on complex hardware or iterative\nhill-climbing algorithms. Recent learning-based approaches have demonstrated\nremarkable efficacy in a one-shot setting, avoiding hardware modifications or\niterative mechanical lens adjustments. However, in this paper, we highlight a\nsignificant challenge that the richness of image content can significantly\naffect autofocus performance. When the image content is sparse, previous\nautofocus methods, whether traditional climbing-hill or learning-based, tend to\nfail. To tackle this, we propose a content-importance-based solution, named\nSparseFocus, featuring a novel two-stage pipeline. The first stage measures the\nimportance of regions within the image, while the second stage calculates the\ndefocus distance from selected important regions. To validate our approach and\nbenefit the research community, we collect a large-scale dataset comprising\nmillions of labelled defocused images, encompassing both dense, sparse and\nextremely sparse scenarios. Experimental results show that SparseFocus\nsurpasses existing methods, effectively handling all levels of content\nsparsity. Moreover, we integrate SparseFocus into our Whole Slide Imaging (WSI)\nsystem that performs well in real-world applications. The code and dataset will\nbe made available upon the publication of this paper.\n", "link": "http://arxiv.org/abs/2502.06452v1", "date": "2025-02-10", "relevancy": 1.6087, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5435}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5335}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseFocus%3A%20Learning-based%20One-shot%20Autofocus%20for%20Microscopy%20with%0A%20%20Sparse%20Content&body=Title%3A%20SparseFocus%3A%20Learning-based%20One-shot%20Autofocus%20for%20Microscopy%20with%0A%20%20Sparse%20Content%0AAuthor%3A%20Yongping%20Zhai%20and%20Xiaoxi%20Fu%20and%20Qiang%20Su%20and%20Jia%20Hu%20and%20Yake%20Zhang%20and%20Yunfeng%20Zhou%20and%20Chaofan%20Zhang%20and%20Xiao%20Li%20and%20Wenxin%20Wang%20and%20Dongdong%20Wu%20and%20Shen%20Yan%0AAbstract%3A%20%20%20Autofocus%20is%20necessary%20for%20high-throughput%20and%20real-time%20scanning%20in%0Amicroscopic%20imaging.%20Traditional%20methods%20rely%20on%20complex%20hardware%20or%20iterative%0Ahill-climbing%20algorithms.%20Recent%20learning-based%20approaches%20have%20demonstrated%0Aremarkable%20efficacy%20in%20a%20one-shot%20setting%2C%20avoiding%20hardware%20modifications%20or%0Aiterative%20mechanical%20lens%20adjustments.%20However%2C%20in%20this%20paper%2C%20we%20highlight%20a%0Asignificant%20challenge%20that%20the%20richness%20of%20image%20content%20can%20significantly%0Aaffect%20autofocus%20performance.%20When%20the%20image%20content%20is%20sparse%2C%20previous%0Aautofocus%20methods%2C%20whether%20traditional%20climbing-hill%20or%20learning-based%2C%20tend%20to%0Afail.%20To%20tackle%20this%2C%20we%20propose%20a%20content-importance-based%20solution%2C%20named%0ASparseFocus%2C%20featuring%20a%20novel%20two-stage%20pipeline.%20The%20first%20stage%20measures%20the%0Aimportance%20of%20regions%20within%20the%20image%2C%20while%20the%20second%20stage%20calculates%20the%0Adefocus%20distance%20from%20selected%20important%20regions.%20To%20validate%20our%20approach%20and%0Abenefit%20the%20research%20community%2C%20we%20collect%20a%20large-scale%20dataset%20comprising%0Amillions%20of%20labelled%20defocused%20images%2C%20encompassing%20both%20dense%2C%20sparse%20and%0Aextremely%20sparse%20scenarios.%20Experimental%20results%20show%20that%20SparseFocus%0Asurpasses%20existing%20methods%2C%20effectively%20handling%20all%20levels%20of%20content%0Asparsity.%20Moreover%2C%20we%20integrate%20SparseFocus%20into%20our%20Whole%20Slide%20Imaging%20%28WSI%29%0Asystem%20that%20performs%20well%20in%20real-world%20applications.%20The%20code%20and%20dataset%20will%0Abe%20made%20available%20upon%20the%20publication%20of%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseFocus%253A%2520Learning-based%2520One-shot%2520Autofocus%2520for%2520Microscopy%2520with%250A%2520%2520Sparse%2520Content%26entry.906535625%3DYongping%2520Zhai%2520and%2520Xiaoxi%2520Fu%2520and%2520Qiang%2520Su%2520and%2520Jia%2520Hu%2520and%2520Yake%2520Zhang%2520and%2520Yunfeng%2520Zhou%2520and%2520Chaofan%2520Zhang%2520and%2520Xiao%2520Li%2520and%2520Wenxin%2520Wang%2520and%2520Dongdong%2520Wu%2520and%2520Shen%2520Yan%26entry.1292438233%3D%2520%2520Autofocus%2520is%2520necessary%2520for%2520high-throughput%2520and%2520real-time%2520scanning%2520in%250Amicroscopic%2520imaging.%2520Traditional%2520methods%2520rely%2520on%2520complex%2520hardware%2520or%2520iterative%250Ahill-climbing%2520algorithms.%2520Recent%2520learning-based%2520approaches%2520have%2520demonstrated%250Aremarkable%2520efficacy%2520in%2520a%2520one-shot%2520setting%252C%2520avoiding%2520hardware%2520modifications%2520or%250Aiterative%2520mechanical%2520lens%2520adjustments.%2520However%252C%2520in%2520this%2520paper%252C%2520we%2520highlight%2520a%250Asignificant%2520challenge%2520that%2520the%2520richness%2520of%2520image%2520content%2520can%2520significantly%250Aaffect%2520autofocus%2520performance.%2520When%2520the%2520image%2520content%2520is%2520sparse%252C%2520previous%250Aautofocus%2520methods%252C%2520whether%2520traditional%2520climbing-hill%2520or%2520learning-based%252C%2520tend%2520to%250Afail.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520a%2520content-importance-based%2520solution%252C%2520named%250ASparseFocus%252C%2520featuring%2520a%2520novel%2520two-stage%2520pipeline.%2520The%2520first%2520stage%2520measures%2520the%250Aimportance%2520of%2520regions%2520within%2520the%2520image%252C%2520while%2520the%2520second%2520stage%2520calculates%2520the%250Adefocus%2520distance%2520from%2520selected%2520important%2520regions.%2520To%2520validate%2520our%2520approach%2520and%250Abenefit%2520the%2520research%2520community%252C%2520we%2520collect%2520a%2520large-scale%2520dataset%2520comprising%250Amillions%2520of%2520labelled%2520defocused%2520images%252C%2520encompassing%2520both%2520dense%252C%2520sparse%2520and%250Aextremely%2520sparse%2520scenarios.%2520Experimental%2520results%2520show%2520that%2520SparseFocus%250Asurpasses%2520existing%2520methods%252C%2520effectively%2520handling%2520all%2520levels%2520of%2520content%250Asparsity.%2520Moreover%252C%2520we%2520integrate%2520SparseFocus%2520into%2520our%2520Whole%2520Slide%2520Imaging%2520%2528WSI%2529%250Asystem%2520that%2520performs%2520well%2520in%2520real-world%2520applications.%2520The%2520code%2520and%2520dataset%2520will%250Abe%2520made%2520available%2520upon%2520the%2520publication%2520of%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseFocus%3A%20Learning-based%20One-shot%20Autofocus%20for%20Microscopy%20with%0A%20%20Sparse%20Content&entry.906535625=Yongping%20Zhai%20and%20Xiaoxi%20Fu%20and%20Qiang%20Su%20and%20Jia%20Hu%20and%20Yake%20Zhang%20and%20Yunfeng%20Zhou%20and%20Chaofan%20Zhang%20and%20Xiao%20Li%20and%20Wenxin%20Wang%20and%20Dongdong%20Wu%20and%20Shen%20Yan&entry.1292438233=%20%20Autofocus%20is%20necessary%20for%20high-throughput%20and%20real-time%20scanning%20in%0Amicroscopic%20imaging.%20Traditional%20methods%20rely%20on%20complex%20hardware%20or%20iterative%0Ahill-climbing%20algorithms.%20Recent%20learning-based%20approaches%20have%20demonstrated%0Aremarkable%20efficacy%20in%20a%20one-shot%20setting%2C%20avoiding%20hardware%20modifications%20or%0Aiterative%20mechanical%20lens%20adjustments.%20However%2C%20in%20this%20paper%2C%20we%20highlight%20a%0Asignificant%20challenge%20that%20the%20richness%20of%20image%20content%20can%20significantly%0Aaffect%20autofocus%20performance.%20When%20the%20image%20content%20is%20sparse%2C%20previous%0Aautofocus%20methods%2C%20whether%20traditional%20climbing-hill%20or%20learning-based%2C%20tend%20to%0Afail.%20To%20tackle%20this%2C%20we%20propose%20a%20content-importance-based%20solution%2C%20named%0ASparseFocus%2C%20featuring%20a%20novel%20two-stage%20pipeline.%20The%20first%20stage%20measures%20the%0Aimportance%20of%20regions%20within%20the%20image%2C%20while%20the%20second%20stage%20calculates%20the%0Adefocus%20distance%20from%20selected%20important%20regions.%20To%20validate%20our%20approach%20and%0Abenefit%20the%20research%20community%2C%20we%20collect%20a%20large-scale%20dataset%20comprising%0Amillions%20of%20labelled%20defocused%20images%2C%20encompassing%20both%20dense%2C%20sparse%20and%0Aextremely%20sparse%20scenarios.%20Experimental%20results%20show%20that%20SparseFocus%0Asurpasses%20existing%20methods%2C%20effectively%20handling%20all%20levels%20of%20content%0Asparsity.%20Moreover%2C%20we%20integrate%20SparseFocus%20into%20our%20Whole%20Slide%20Imaging%20%28WSI%29%0Asystem%20that%20performs%20well%20in%20real-world%20applications.%20The%20code%20and%20dataset%20will%0Abe%20made%20available%20upon%20the%20publication%20of%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06452v1&entry.124074799=Read"},
{"title": "Inference-Time Selective Debiasing to Enhance Fairness in Text\n  Classification Models", "author": "Gleb Kuzmin and Neemesh Yadav and Ivan Smirnov and Timothy Baldwin and Artem Shelmanov", "abstract": "  We propose selective debiasing -- an inference-time safety mechanism designed\nto enhance the overall model quality in terms of prediction performance and\nfairness, especially in scenarios where retraining the model is impractical.\nThe method draws inspiration from selective classification, where at inference\ntime, predictions with low quality, as indicated by their uncertainty scores,\nare discarded. In our approach, we identify the potentially biased model\npredictions and, instead of discarding them, we remove bias from these\npredictions using LEACE -- a post-processing debiasing method. To select\nproblematic predictions, we propose a bias quantification approach based on KL\ndivergence, which achieves better results than standard uncertainty\nquantification methods. Experiments on text classification datasets with\nencoder-based classification models demonstrate that selective debiasing helps\nto reduce the performance gap between post-processing methods and debiasing\ntechniques from the at-training and pre-processing categories.\n", "link": "http://arxiv.org/abs/2407.19345v3", "date": "2025-02-10", "relevancy": 1.5249, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5153}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5139}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Selective%20Debiasing%20to%20Enhance%20Fairness%20in%20Text%0A%20%20Classification%20Models&body=Title%3A%20Inference-Time%20Selective%20Debiasing%20to%20Enhance%20Fairness%20in%20Text%0A%20%20Classification%20Models%0AAuthor%3A%20Gleb%20Kuzmin%20and%20Neemesh%20Yadav%20and%20Ivan%20Smirnov%20and%20Timothy%20Baldwin%20and%20Artem%20Shelmanov%0AAbstract%3A%20%20%20We%20propose%20selective%20debiasing%20--%20an%20inference-time%20safety%20mechanism%20designed%0Ato%20enhance%20the%20overall%20model%20quality%20in%20terms%20of%20prediction%20performance%20and%0Afairness%2C%20especially%20in%20scenarios%20where%20retraining%20the%20model%20is%20impractical.%0AThe%20method%20draws%20inspiration%20from%20selective%20classification%2C%20where%20at%20inference%0Atime%2C%20predictions%20with%20low%20quality%2C%20as%20indicated%20by%20their%20uncertainty%20scores%2C%0Aare%20discarded.%20In%20our%20approach%2C%20we%20identify%20the%20potentially%20biased%20model%0Apredictions%20and%2C%20instead%20of%20discarding%20them%2C%20we%20remove%20bias%20from%20these%0Apredictions%20using%20LEACE%20--%20a%20post-processing%20debiasing%20method.%20To%20select%0Aproblematic%20predictions%2C%20we%20propose%20a%20bias%20quantification%20approach%20based%20on%20KL%0Adivergence%2C%20which%20achieves%20better%20results%20than%20standard%20uncertainty%0Aquantification%20methods.%20Experiments%20on%20text%20classification%20datasets%20with%0Aencoder-based%20classification%20models%20demonstrate%20that%20selective%20debiasing%20helps%0Ato%20reduce%20the%20performance%20gap%20between%20post-processing%20methods%20and%20debiasing%0Atechniques%20from%20the%20at-training%20and%20pre-processing%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19345v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Selective%2520Debiasing%2520to%2520Enhance%2520Fairness%2520in%2520Text%250A%2520%2520Classification%2520Models%26entry.906535625%3DGleb%2520Kuzmin%2520and%2520Neemesh%2520Yadav%2520and%2520Ivan%2520Smirnov%2520and%2520Timothy%2520Baldwin%2520and%2520Artem%2520Shelmanov%26entry.1292438233%3D%2520%2520We%2520propose%2520selective%2520debiasing%2520--%2520an%2520inference-time%2520safety%2520mechanism%2520designed%250Ato%2520enhance%2520the%2520overall%2520model%2520quality%2520in%2520terms%2520of%2520prediction%2520performance%2520and%250Afairness%252C%2520especially%2520in%2520scenarios%2520where%2520retraining%2520the%2520model%2520is%2520impractical.%250AThe%2520method%2520draws%2520inspiration%2520from%2520selective%2520classification%252C%2520where%2520at%2520inference%250Atime%252C%2520predictions%2520with%2520low%2520quality%252C%2520as%2520indicated%2520by%2520their%2520uncertainty%2520scores%252C%250Aare%2520discarded.%2520In%2520our%2520approach%252C%2520we%2520identify%2520the%2520potentially%2520biased%2520model%250Apredictions%2520and%252C%2520instead%2520of%2520discarding%2520them%252C%2520we%2520remove%2520bias%2520from%2520these%250Apredictions%2520using%2520LEACE%2520--%2520a%2520post-processing%2520debiasing%2520method.%2520To%2520select%250Aproblematic%2520predictions%252C%2520we%2520propose%2520a%2520bias%2520quantification%2520approach%2520based%2520on%2520KL%250Adivergence%252C%2520which%2520achieves%2520better%2520results%2520than%2520standard%2520uncertainty%250Aquantification%2520methods.%2520Experiments%2520on%2520text%2520classification%2520datasets%2520with%250Aencoder-based%2520classification%2520models%2520demonstrate%2520that%2520selective%2520debiasing%2520helps%250Ato%2520reduce%2520the%2520performance%2520gap%2520between%2520post-processing%2520methods%2520and%2520debiasing%250Atechniques%2520from%2520the%2520at-training%2520and%2520pre-processing%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19345v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Selective%20Debiasing%20to%20Enhance%20Fairness%20in%20Text%0A%20%20Classification%20Models&entry.906535625=Gleb%20Kuzmin%20and%20Neemesh%20Yadav%20and%20Ivan%20Smirnov%20and%20Timothy%20Baldwin%20and%20Artem%20Shelmanov&entry.1292438233=%20%20We%20propose%20selective%20debiasing%20--%20an%20inference-time%20safety%20mechanism%20designed%0Ato%20enhance%20the%20overall%20model%20quality%20in%20terms%20of%20prediction%20performance%20and%0Afairness%2C%20especially%20in%20scenarios%20where%20retraining%20the%20model%20is%20impractical.%0AThe%20method%20draws%20inspiration%20from%20selective%20classification%2C%20where%20at%20inference%0Atime%2C%20predictions%20with%20low%20quality%2C%20as%20indicated%20by%20their%20uncertainty%20scores%2C%0Aare%20discarded.%20In%20our%20approach%2C%20we%20identify%20the%20potentially%20biased%20model%0Apredictions%20and%2C%20instead%20of%20discarding%20them%2C%20we%20remove%20bias%20from%20these%0Apredictions%20using%20LEACE%20--%20a%20post-processing%20debiasing%20method.%20To%20select%0Aproblematic%20predictions%2C%20we%20propose%20a%20bias%20quantification%20approach%20based%20on%20KL%0Adivergence%2C%20which%20achieves%20better%20results%20than%20standard%20uncertainty%0Aquantification%20methods.%20Experiments%20on%20text%20classification%20datasets%20with%0Aencoder-based%20classification%20models%20demonstrate%20that%20selective%20debiasing%20helps%0Ato%20reduce%20the%20performance%20gap%20between%20post-processing%20methods%20and%20debiasing%0Atechniques%20from%20the%20at-training%20and%20pre-processing%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19345v3&entry.124074799=Read"},
{"title": "Predicting Molecular Ground-State Conformation via Conformation\n  Optimization", "author": "Fanmeng Wang and Minjie Cheng and Hongteng Xu", "abstract": "  Predicting molecular ground-state conformation (i.e., energy-minimized\nconformation) is crucial for many chemical applications such as molecular\ndocking and property prediction. Classic energy-based simulation is\ntime-consuming when solving this problem while existing learning-based methods\nhave advantages in computational efficiency but sacrifice accuracy and\ninterpretability. In this work, we propose a novel and effective method to\nbridge the energy-based simulation and the learning-based strategy, which\ndesigns and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called\nWGFormer, for molecular ground-state conformation prediction. Specifically, our\nmethod tackles this task within an auto-encoding framework, which encodes\nlow-quality conformations by the proposed WGFormer and decodes corresponding\nground-state conformations by an MLP. The architecture of WGFormer corresponds\nto Wasserstein gradient flows -- it optimizes molecular conformations by\nminimizing an energy function defined on the latent mixture models of atoms,\nthereby significantly improving performance and interpretability. Extensive\nexperiments show that our method consistently outperforms state-of-the-art\ncompetitors, providing a new and insightful paradigm to predict molecular\nground-state conformation.\n", "link": "http://arxiv.org/abs/2410.09795v3", "date": "2025-02-10", "relevancy": 1.0188, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.514}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Molecular%20Ground-State%20Conformation%20via%20Conformation%0A%20%20Optimization&body=Title%3A%20Predicting%20Molecular%20Ground-State%20Conformation%20via%20Conformation%0A%20%20Optimization%0AAuthor%3A%20Fanmeng%20Wang%20and%20Minjie%20Cheng%20and%20Hongteng%20Xu%0AAbstract%3A%20%20%20Predicting%20molecular%20ground-state%20conformation%20%28i.e.%2C%20energy-minimized%0Aconformation%29%20is%20crucial%20for%20many%20chemical%20applications%20such%20as%20molecular%0Adocking%20and%20property%20prediction.%20Classic%20energy-based%20simulation%20is%0Atime-consuming%20when%20solving%20this%20problem%20while%20existing%20learning-based%20methods%0Ahave%20advantages%20in%20computational%20efficiency%20but%20sacrifice%20accuracy%20and%0Ainterpretability.%20In%20this%20work%2C%20we%20propose%20a%20novel%20and%20effective%20method%20to%0Abridge%20the%20energy-based%20simulation%20and%20the%20learning-based%20strategy%2C%20which%0Adesigns%20and%20learns%20a%20Wasserstein%20gradient%20flow-driven%20SE%283%29-Transformer%2C%20called%0AWGFormer%2C%20for%20molecular%20ground-state%20conformation%20prediction.%20Specifically%2C%20our%0Amethod%20tackles%20this%20task%20within%20an%20auto-encoding%20framework%2C%20which%20encodes%0Alow-quality%20conformations%20by%20the%20proposed%20WGFormer%20and%20decodes%20corresponding%0Aground-state%20conformations%20by%20an%20MLP.%20The%20architecture%20of%20WGFormer%20corresponds%0Ato%20Wasserstein%20gradient%20flows%20--%20it%20optimizes%20molecular%20conformations%20by%0Aminimizing%20an%20energy%20function%20defined%20on%20the%20latent%20mixture%20models%20of%20atoms%2C%0Athereby%20significantly%20improving%20performance%20and%20interpretability.%20Extensive%0Aexperiments%20show%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%0Acompetitors%2C%20providing%20a%20new%20and%20insightful%20paradigm%20to%20predict%20molecular%0Aground-state%20conformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09795v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Molecular%2520Ground-State%2520Conformation%2520via%2520Conformation%250A%2520%2520Optimization%26entry.906535625%3DFanmeng%2520Wang%2520and%2520Minjie%2520Cheng%2520and%2520Hongteng%2520Xu%26entry.1292438233%3D%2520%2520Predicting%2520molecular%2520ground-state%2520conformation%2520%2528i.e.%252C%2520energy-minimized%250Aconformation%2529%2520is%2520crucial%2520for%2520many%2520chemical%2520applications%2520such%2520as%2520molecular%250Adocking%2520and%2520property%2520prediction.%2520Classic%2520energy-based%2520simulation%2520is%250Atime-consuming%2520when%2520solving%2520this%2520problem%2520while%2520existing%2520learning-based%2520methods%250Ahave%2520advantages%2520in%2520computational%2520efficiency%2520but%2520sacrifice%2520accuracy%2520and%250Ainterpretability.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520and%2520effective%2520method%2520to%250Abridge%2520the%2520energy-based%2520simulation%2520and%2520the%2520learning-based%2520strategy%252C%2520which%250Adesigns%2520and%2520learns%2520a%2520Wasserstein%2520gradient%2520flow-driven%2520SE%25283%2529-Transformer%252C%2520called%250AWGFormer%252C%2520for%2520molecular%2520ground-state%2520conformation%2520prediction.%2520Specifically%252C%2520our%250Amethod%2520tackles%2520this%2520task%2520within%2520an%2520auto-encoding%2520framework%252C%2520which%2520encodes%250Alow-quality%2520conformations%2520by%2520the%2520proposed%2520WGFormer%2520and%2520decodes%2520corresponding%250Aground-state%2520conformations%2520by%2520an%2520MLP.%2520The%2520architecture%2520of%2520WGFormer%2520corresponds%250Ato%2520Wasserstein%2520gradient%2520flows%2520--%2520it%2520optimizes%2520molecular%2520conformations%2520by%250Aminimizing%2520an%2520energy%2520function%2520defined%2520on%2520the%2520latent%2520mixture%2520models%2520of%2520atoms%252C%250Athereby%2520significantly%2520improving%2520performance%2520and%2520interpretability.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520consistently%2520outperforms%2520state-of-the-art%250Acompetitors%252C%2520providing%2520a%2520new%2520and%2520insightful%2520paradigm%2520to%2520predict%2520molecular%250Aground-state%2520conformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09795v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Molecular%20Ground-State%20Conformation%20via%20Conformation%0A%20%20Optimization&entry.906535625=Fanmeng%20Wang%20and%20Minjie%20Cheng%20and%20Hongteng%20Xu&entry.1292438233=%20%20Predicting%20molecular%20ground-state%20conformation%20%28i.e.%2C%20energy-minimized%0Aconformation%29%20is%20crucial%20for%20many%20chemical%20applications%20such%20as%20molecular%0Adocking%20and%20property%20prediction.%20Classic%20energy-based%20simulation%20is%0Atime-consuming%20when%20solving%20this%20problem%20while%20existing%20learning-based%20methods%0Ahave%20advantages%20in%20computational%20efficiency%20but%20sacrifice%20accuracy%20and%0Ainterpretability.%20In%20this%20work%2C%20we%20propose%20a%20novel%20and%20effective%20method%20to%0Abridge%20the%20energy-based%20simulation%20and%20the%20learning-based%20strategy%2C%20which%0Adesigns%20and%20learns%20a%20Wasserstein%20gradient%20flow-driven%20SE%283%29-Transformer%2C%20called%0AWGFormer%2C%20for%20molecular%20ground-state%20conformation%20prediction.%20Specifically%2C%20our%0Amethod%20tackles%20this%20task%20within%20an%20auto-encoding%20framework%2C%20which%20encodes%0Alow-quality%20conformations%20by%20the%20proposed%20WGFormer%20and%20decodes%20corresponding%0Aground-state%20conformations%20by%20an%20MLP.%20The%20architecture%20of%20WGFormer%20corresponds%0Ato%20Wasserstein%20gradient%20flows%20--%20it%20optimizes%20molecular%20conformations%20by%0Aminimizing%20an%20energy%20function%20defined%20on%20the%20latent%20mixture%20models%20of%20atoms%2C%0Athereby%20significantly%20improving%20performance%20and%20interpretability.%20Extensive%0Aexperiments%20show%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%0Acompetitors%2C%20providing%20a%20new%20and%20insightful%20paradigm%20to%20predict%20molecular%0Aground-state%20conformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09795v3&entry.124074799=Read"},
{"title": "Faster Language Models with Better Multi-Token Prediction Using Tensor\n  Decomposition", "author": "Artem Basharin and Andrei Chertkov and Ivan Oseledets", "abstract": "  We propose a new model for multi-token prediction in transformers, aiming to\nenhance sampling efficiency without compromising accuracy. Motivated by recent\nwork that predicts the probabilities of subsequent tokens using multiple heads,\nwe connect this approach to rank-$1$ canonical tensor decomposition. By\ngeneralizing it to a rank-$r$ canonical probability decomposition, we develop\nan improved model that predicts multiple tokens simultaneously. This model can\nalso be interpreted as a mixture of experts, allowing us to leverage successful\ntechniques from that domain for efficient and robust training. Importantly, the\noverall overhead for training and sampling remains low. Our method demonstrates\nsignificant improvements in inference speed for both text and code generation\ntasks, proving particularly beneficial within the self-speculative decoding\nparadigm. It maintains its effectiveness across various model sizes and\ntraining epochs, highlighting its robustness and scalability.\n", "link": "http://arxiv.org/abs/2410.17765v2", "date": "2025-02-10", "relevancy": 1.6266, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.559}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5467}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Language%20Models%20with%20Better%20Multi-Token%20Prediction%20Using%20Tensor%0A%20%20Decomposition&body=Title%3A%20Faster%20Language%20Models%20with%20Better%20Multi-Token%20Prediction%20Using%20Tensor%0A%20%20Decomposition%0AAuthor%3A%20Artem%20Basharin%20and%20Andrei%20Chertkov%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20We%20propose%20a%20new%20model%20for%20multi-token%20prediction%20in%20transformers%2C%20aiming%20to%0Aenhance%20sampling%20efficiency%20without%20compromising%20accuracy.%20Motivated%20by%20recent%0Awork%20that%20predicts%20the%20probabilities%20of%20subsequent%20tokens%20using%20multiple%20heads%2C%0Awe%20connect%20this%20approach%20to%20rank-%241%24%20canonical%20tensor%20decomposition.%20By%0Ageneralizing%20it%20to%20a%20rank-%24r%24%20canonical%20probability%20decomposition%2C%20we%20develop%0Aan%20improved%20model%20that%20predicts%20multiple%20tokens%20simultaneously.%20This%20model%20can%0Aalso%20be%20interpreted%20as%20a%20mixture%20of%20experts%2C%20allowing%20us%20to%20leverage%20successful%0Atechniques%20from%20that%20domain%20for%20efficient%20and%20robust%20training.%20Importantly%2C%20the%0Aoverall%20overhead%20for%20training%20and%20sampling%20remains%20low.%20Our%20method%20demonstrates%0Asignificant%20improvements%20in%20inference%20speed%20for%20both%20text%20and%20code%20generation%0Atasks%2C%20proving%20particularly%20beneficial%20within%20the%20self-speculative%20decoding%0Aparadigm.%20It%20maintains%20its%20effectiveness%20across%20various%20model%20sizes%20and%0Atraining%20epochs%2C%20highlighting%20its%20robustness%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17765v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Language%2520Models%2520with%2520Better%2520Multi-Token%2520Prediction%2520Using%2520Tensor%250A%2520%2520Decomposition%26entry.906535625%3DArtem%2520Basharin%2520and%2520Andrei%2520Chertkov%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520model%2520for%2520multi-token%2520prediction%2520in%2520transformers%252C%2520aiming%2520to%250Aenhance%2520sampling%2520efficiency%2520without%2520compromising%2520accuracy.%2520Motivated%2520by%2520recent%250Awork%2520that%2520predicts%2520the%2520probabilities%2520of%2520subsequent%2520tokens%2520using%2520multiple%2520heads%252C%250Awe%2520connect%2520this%2520approach%2520to%2520rank-%25241%2524%2520canonical%2520tensor%2520decomposition.%2520By%250Ageneralizing%2520it%2520to%2520a%2520rank-%2524r%2524%2520canonical%2520probability%2520decomposition%252C%2520we%2520develop%250Aan%2520improved%2520model%2520that%2520predicts%2520multiple%2520tokens%2520simultaneously.%2520This%2520model%2520can%250Aalso%2520be%2520interpreted%2520as%2520a%2520mixture%2520of%2520experts%252C%2520allowing%2520us%2520to%2520leverage%2520successful%250Atechniques%2520from%2520that%2520domain%2520for%2520efficient%2520and%2520robust%2520training.%2520Importantly%252C%2520the%250Aoverall%2520overhead%2520for%2520training%2520and%2520sampling%2520remains%2520low.%2520Our%2520method%2520demonstrates%250Asignificant%2520improvements%2520in%2520inference%2520speed%2520for%2520both%2520text%2520and%2520code%2520generation%250Atasks%252C%2520proving%2520particularly%2520beneficial%2520within%2520the%2520self-speculative%2520decoding%250Aparadigm.%2520It%2520maintains%2520its%2520effectiveness%2520across%2520various%2520model%2520sizes%2520and%250Atraining%2520epochs%252C%2520highlighting%2520its%2520robustness%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17765v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Language%20Models%20with%20Better%20Multi-Token%20Prediction%20Using%20Tensor%0A%20%20Decomposition&entry.906535625=Artem%20Basharin%20and%20Andrei%20Chertkov%20and%20Ivan%20Oseledets&entry.1292438233=%20%20We%20propose%20a%20new%20model%20for%20multi-token%20prediction%20in%20transformers%2C%20aiming%20to%0Aenhance%20sampling%20efficiency%20without%20compromising%20accuracy.%20Motivated%20by%20recent%0Awork%20that%20predicts%20the%20probabilities%20of%20subsequent%20tokens%20using%20multiple%20heads%2C%0Awe%20connect%20this%20approach%20to%20rank-%241%24%20canonical%20tensor%20decomposition.%20By%0Ageneralizing%20it%20to%20a%20rank-%24r%24%20canonical%20probability%20decomposition%2C%20we%20develop%0Aan%20improved%20model%20that%20predicts%20multiple%20tokens%20simultaneously.%20This%20model%20can%0Aalso%20be%20interpreted%20as%20a%20mixture%20of%20experts%2C%20allowing%20us%20to%20leverage%20successful%0Atechniques%20from%20that%20domain%20for%20efficient%20and%20robust%20training.%20Importantly%2C%20the%0Aoverall%20overhead%20for%20training%20and%20sampling%20remains%20low.%20Our%20method%20demonstrates%0Asignificant%20improvements%20in%20inference%20speed%20for%20both%20text%20and%20code%20generation%0Atasks%2C%20proving%20particularly%20beneficial%20within%20the%20self-speculative%20decoding%0Aparadigm.%20It%20maintains%20its%20effectiveness%20across%20various%20model%20sizes%20and%0Atraining%20epochs%2C%20highlighting%20its%20robustness%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17765v2&entry.124074799=Read"},
{"title": "PAC-Chernoff Bounds: Understanding Generalization in the Interpolation\n  Regime", "author": "Andr\u00e9s R. Masegosa and Luis A. Ortega", "abstract": "  This paper introduces a distribution-dependent PAC-Chernoff bound that\nexhibits perfect tightness for interpolators, even within over-parameterized\nmodel classes. This bound, which relies on basic principles of Large Deviation\nTheory, defines a natural measure of the smoothness of a model, characterized\nby simple real-valued functions. Building upon this bound and the new concept\nof smoothness, we present an unified theoretical framework revealing why\ncertain interpolators show an exceptional generalization, while others falter.\nWe theoretically show how a wide spectrum of modern learning methodologies,\nencompassing techniques such as $\\ell_2$-norm, distance-from-initialization and\ninput-gradient regularization, in combination with data augmentation, invariant\narchitectures, and over-parameterization, collectively guide the optimizer\ntoward smoother interpolators, which, according to our theoretical framework,\nare the ones exhibiting superior generalization performance. This study shows\nthat distribution-dependent bounds serve as a powerful tool to understand the\ncomplex dynamics behind the generalization capabilities of over-parameterized\ninterpolators.\n", "link": "http://arxiv.org/abs/2306.10947v4", "date": "2025-02-10", "relevancy": 1.3729, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4768}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4587}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAC-Chernoff%20Bounds%3A%20Understanding%20Generalization%20in%20the%20Interpolation%0A%20%20Regime&body=Title%3A%20PAC-Chernoff%20Bounds%3A%20Understanding%20Generalization%20in%20the%20Interpolation%0A%20%20Regime%0AAuthor%3A%20Andr%C3%A9s%20R.%20Masegosa%20and%20Luis%20A.%20Ortega%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20distribution-dependent%20PAC-Chernoff%20bound%20that%0Aexhibits%20perfect%20tightness%20for%20interpolators%2C%20even%20within%20over-parameterized%0Amodel%20classes.%20This%20bound%2C%20which%20relies%20on%20basic%20principles%20of%20Large%20Deviation%0ATheory%2C%20defines%20a%20natural%20measure%20of%20the%20smoothness%20of%20a%20model%2C%20characterized%0Aby%20simple%20real-valued%20functions.%20Building%20upon%20this%20bound%20and%20the%20new%20concept%0Aof%20smoothness%2C%20we%20present%20an%20unified%20theoretical%20framework%20revealing%20why%0Acertain%20interpolators%20show%20an%20exceptional%20generalization%2C%20while%20others%20falter.%0AWe%20theoretically%20show%20how%20a%20wide%20spectrum%20of%20modern%20learning%20methodologies%2C%0Aencompassing%20techniques%20such%20as%20%24%5Cell_2%24-norm%2C%20distance-from-initialization%20and%0Ainput-gradient%20regularization%2C%20in%20combination%20with%20data%20augmentation%2C%20invariant%0Aarchitectures%2C%20and%20over-parameterization%2C%20collectively%20guide%20the%20optimizer%0Atoward%20smoother%20interpolators%2C%20which%2C%20according%20to%20our%20theoretical%20framework%2C%0Aare%20the%20ones%20exhibiting%20superior%20generalization%20performance.%20This%20study%20shows%0Athat%20distribution-dependent%20bounds%20serve%20as%20a%20powerful%20tool%20to%20understand%20the%0Acomplex%20dynamics%20behind%20the%20generalization%20capabilities%20of%20over-parameterized%0Ainterpolators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10947v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAC-Chernoff%2520Bounds%253A%2520Understanding%2520Generalization%2520in%2520the%2520Interpolation%250A%2520%2520Regime%26entry.906535625%3DAndr%25C3%25A9s%2520R.%2520Masegosa%2520and%2520Luis%2520A.%2520Ortega%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520distribution-dependent%2520PAC-Chernoff%2520bound%2520that%250Aexhibits%2520perfect%2520tightness%2520for%2520interpolators%252C%2520even%2520within%2520over-parameterized%250Amodel%2520classes.%2520This%2520bound%252C%2520which%2520relies%2520on%2520basic%2520principles%2520of%2520Large%2520Deviation%250ATheory%252C%2520defines%2520a%2520natural%2520measure%2520of%2520the%2520smoothness%2520of%2520a%2520model%252C%2520characterized%250Aby%2520simple%2520real-valued%2520functions.%2520Building%2520upon%2520this%2520bound%2520and%2520the%2520new%2520concept%250Aof%2520smoothness%252C%2520we%2520present%2520an%2520unified%2520theoretical%2520framework%2520revealing%2520why%250Acertain%2520interpolators%2520show%2520an%2520exceptional%2520generalization%252C%2520while%2520others%2520falter.%250AWe%2520theoretically%2520show%2520how%2520a%2520wide%2520spectrum%2520of%2520modern%2520learning%2520methodologies%252C%250Aencompassing%2520techniques%2520such%2520as%2520%2524%255Cell_2%2524-norm%252C%2520distance-from-initialization%2520and%250Ainput-gradient%2520regularization%252C%2520in%2520combination%2520with%2520data%2520augmentation%252C%2520invariant%250Aarchitectures%252C%2520and%2520over-parameterization%252C%2520collectively%2520guide%2520the%2520optimizer%250Atoward%2520smoother%2520interpolators%252C%2520which%252C%2520according%2520to%2520our%2520theoretical%2520framework%252C%250Aare%2520the%2520ones%2520exhibiting%2520superior%2520generalization%2520performance.%2520This%2520study%2520shows%250Athat%2520distribution-dependent%2520bounds%2520serve%2520as%2520a%2520powerful%2520tool%2520to%2520understand%2520the%250Acomplex%2520dynamics%2520behind%2520the%2520generalization%2520capabilities%2520of%2520over-parameterized%250Ainterpolators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.10947v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC-Chernoff%20Bounds%3A%20Understanding%20Generalization%20in%20the%20Interpolation%0A%20%20Regime&entry.906535625=Andr%C3%A9s%20R.%20Masegosa%20and%20Luis%20A.%20Ortega&entry.1292438233=%20%20This%20paper%20introduces%20a%20distribution-dependent%20PAC-Chernoff%20bound%20that%0Aexhibits%20perfect%20tightness%20for%20interpolators%2C%20even%20within%20over-parameterized%0Amodel%20classes.%20This%20bound%2C%20which%20relies%20on%20basic%20principles%20of%20Large%20Deviation%0ATheory%2C%20defines%20a%20natural%20measure%20of%20the%20smoothness%20of%20a%20model%2C%20characterized%0Aby%20simple%20real-valued%20functions.%20Building%20upon%20this%20bound%20and%20the%20new%20concept%0Aof%20smoothness%2C%20we%20present%20an%20unified%20theoretical%20framework%20revealing%20why%0Acertain%20interpolators%20show%20an%20exceptional%20generalization%2C%20while%20others%20falter.%0AWe%20theoretically%20show%20how%20a%20wide%20spectrum%20of%20modern%20learning%20methodologies%2C%0Aencompassing%20techniques%20such%20as%20%24%5Cell_2%24-norm%2C%20distance-from-initialization%20and%0Ainput-gradient%20regularization%2C%20in%20combination%20with%20data%20augmentation%2C%20invariant%0Aarchitectures%2C%20and%20over-parameterization%2C%20collectively%20guide%20the%20optimizer%0Atoward%20smoother%20interpolators%2C%20which%2C%20according%20to%20our%20theoretical%20framework%2C%0Aare%20the%20ones%20exhibiting%20superior%20generalization%20performance.%20This%20study%20shows%0Athat%20distribution-dependent%20bounds%20serve%20as%20a%20powerful%20tool%20to%20understand%20the%0Acomplex%20dynamics%20behind%20the%20generalization%20capabilities%20of%20over-parameterized%0Ainterpolators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10947v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


