<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="#"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity\n  Animatable Face Avatars", "author": "Gent Serifi and Marcel C. B\u00fchler", "abstract": "  We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for\nhigh-quality animatable face avatars. Creating such detailed face avatars from\nvideos is a challenging problem and has numerous applications in augmented and\nvirtual reality. While tremendous successes have been achieved for static\nfaces, animatable avatars from monocular videos still fall in the uncanny\nvalley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face\nthrough a collection of 3D Gaussian primitives. 3DGS excels at rendering static\nfaces, but the state-of-the-art still struggles with nonlinear deformations,\ncomplex lighting effects, and fine details. While most related works focus on\npredicting better Gaussian parameters from expression codes, we rethink the 3D\nGaussian representation itself and how to make it more expressive. Our insights\nlead to a novel extension of 3D Gaussians to high-dimensional multivariate\nGaussians, dubbed 'HyperGaussians'. The higher dimensionality increases\nexpressivity through conditioning on a learnable local embedding. However,\nsplatting HyperGaussians is computationally expensive because it requires\ninverting a high-dimensional covariance matrix. We solve this by\nreparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.\nThis trick boosts the efficiency so that HyperGaussians can be seamlessly\nintegrated into existing models. To demonstrate this, we plug in HyperGaussians\ninto the state-of-the-art in fast monocular face avatars: FlashAvatar. Our\nevaluation on 19 subjects from 4 face datasets shows that HyperGaussians\noutperform 3DGS numerically and visually, particularly for high-frequency\ndetails like eyeglass frames, teeth, complex facial movements, and specular\nreflections.\n", "link": "http://arxiv.org/abs/2507.02803v1", "date": "2025-07-03", "relevancy": 3.7182, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7549}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7549}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars&body=Title%3A%20HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars%0AAuthor%3A%20Gent%20Serifi%20and%20Marcel%20C.%20B%C3%BChler%0AAbstract%3A%20%20%20We%20introduce%20HyperGaussians%2C%20a%20novel%20extension%20of%203D%20Gaussian%20Splatting%20for%0Ahigh-quality%20animatable%20face%20avatars.%20Creating%20such%20detailed%20face%20avatars%20from%0Avideos%20is%20a%20challenging%20problem%20and%20has%20numerous%20applications%20in%20augmented%20and%0Avirtual%20reality.%20While%20tremendous%20successes%20have%20been%20achieved%20for%20static%0Afaces%2C%20animatable%20avatars%20from%20monocular%20videos%20still%20fall%20in%20the%20uncanny%0Avalley.%20The%20de%20facto%20standard%2C%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20represents%20a%20face%0Athrough%20a%20collection%20of%203D%20Gaussian%20primitives.%203DGS%20excels%20at%20rendering%20static%0Afaces%2C%20but%20the%20state-of-the-art%20still%20struggles%20with%20nonlinear%20deformations%2C%0Acomplex%20lighting%20effects%2C%20and%20fine%20details.%20While%20most%20related%20works%20focus%20on%0Apredicting%20better%20Gaussian%20parameters%20from%20expression%20codes%2C%20we%20rethink%20the%203D%0AGaussian%20representation%20itself%20and%20how%20to%20make%20it%20more%20expressive.%20Our%20insights%0Alead%20to%20a%20novel%20extension%20of%203D%20Gaussians%20to%20high-dimensional%20multivariate%0AGaussians%2C%20dubbed%20%27HyperGaussians%27.%20The%20higher%20dimensionality%20increases%0Aexpressivity%20through%20conditioning%20on%20a%20learnable%20local%20embedding.%20However%2C%0Asplatting%20HyperGaussians%20is%20computationally%20expensive%20because%20it%20requires%0Ainverting%20a%20high-dimensional%20covariance%20matrix.%20We%20solve%20this%20by%0Areparameterizing%20the%20covariance%20matrix%2C%20dubbed%20the%20%27inverse%20covariance%20trick%27.%0AThis%20trick%20boosts%20the%20efficiency%20so%20that%20HyperGaussians%20can%20be%20seamlessly%0Aintegrated%20into%20existing%20models.%20To%20demonstrate%20this%2C%20we%20plug%20in%20HyperGaussians%0Ainto%20the%20state-of-the-art%20in%20fast%20monocular%20face%20avatars%3A%20FlashAvatar.%20Our%0Aevaluation%20on%2019%20subjects%20from%204%20face%20datasets%20shows%20that%20HyperGaussians%0Aoutperform%203DGS%20numerically%20and%20visually%2C%20particularly%20for%20high-frequency%0Adetails%20like%20eyeglass%20frames%2C%20teeth%2C%20complex%20facial%20movements%2C%20and%20specular%0Areflections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperGaussians%253A%2520High-Dimensional%2520Gaussian%2520Splatting%2520for%2520High-Fidelity%250A%2520%2520Animatable%2520Face%2520Avatars%26entry.906535625%3DGent%2520Serifi%2520and%2520Marcel%2520C.%2520B%25C3%25BChler%26entry.1292438233%3D%2520%2520We%2520introduce%2520HyperGaussians%252C%2520a%2520novel%2520extension%2520of%25203D%2520Gaussian%2520Splatting%2520for%250Ahigh-quality%2520animatable%2520face%2520avatars.%2520Creating%2520such%2520detailed%2520face%2520avatars%2520from%250Avideos%2520is%2520a%2520challenging%2520problem%2520and%2520has%2520numerous%2520applications%2520in%2520augmented%2520and%250Avirtual%2520reality.%2520While%2520tremendous%2520successes%2520have%2520been%2520achieved%2520for%2520static%250Afaces%252C%2520animatable%2520avatars%2520from%2520monocular%2520videos%2520still%2520fall%2520in%2520the%2520uncanny%250Avalley.%2520The%2520de%2520facto%2520standard%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520represents%2520a%2520face%250Athrough%2520a%2520collection%2520of%25203D%2520Gaussian%2520primitives.%25203DGS%2520excels%2520at%2520rendering%2520static%250Afaces%252C%2520but%2520the%2520state-of-the-art%2520still%2520struggles%2520with%2520nonlinear%2520deformations%252C%250Acomplex%2520lighting%2520effects%252C%2520and%2520fine%2520details.%2520While%2520most%2520related%2520works%2520focus%2520on%250Apredicting%2520better%2520Gaussian%2520parameters%2520from%2520expression%2520codes%252C%2520we%2520rethink%2520the%25203D%250AGaussian%2520representation%2520itself%2520and%2520how%2520to%2520make%2520it%2520more%2520expressive.%2520Our%2520insights%250Alead%2520to%2520a%2520novel%2520extension%2520of%25203D%2520Gaussians%2520to%2520high-dimensional%2520multivariate%250AGaussians%252C%2520dubbed%2520%2527HyperGaussians%2527.%2520The%2520higher%2520dimensionality%2520increases%250Aexpressivity%2520through%2520conditioning%2520on%2520a%2520learnable%2520local%2520embedding.%2520However%252C%250Asplatting%2520HyperGaussians%2520is%2520computationally%2520expensive%2520because%2520it%2520requires%250Ainverting%2520a%2520high-dimensional%2520covariance%2520matrix.%2520We%2520solve%2520this%2520by%250Areparameterizing%2520the%2520covariance%2520matrix%252C%2520dubbed%2520the%2520%2527inverse%2520covariance%2520trick%2527.%250AThis%2520trick%2520boosts%2520the%2520efficiency%2520so%2520that%2520HyperGaussians%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520existing%2520models.%2520To%2520demonstrate%2520this%252C%2520we%2520plug%2520in%2520HyperGaussians%250Ainto%2520the%2520state-of-the-art%2520in%2520fast%2520monocular%2520face%2520avatars%253A%2520FlashAvatar.%2520Our%250Aevaluation%2520on%252019%2520subjects%2520from%25204%2520face%2520datasets%2520shows%2520that%2520HyperGaussians%250Aoutperform%25203DGS%2520numerically%2520and%2520visually%252C%2520particularly%2520for%2520high-frequency%250Adetails%2520like%2520eyeglass%2520frames%252C%2520teeth%252C%2520complex%2520facial%2520movements%252C%2520and%2520specular%250Areflections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars&entry.906535625=Gent%20Serifi%20and%20Marcel%20C.%20B%C3%BChler&entry.1292438233=%20%20We%20introduce%20HyperGaussians%2C%20a%20novel%20extension%20of%203D%20Gaussian%20Splatting%20for%0Ahigh-quality%20animatable%20face%20avatars.%20Creating%20such%20detailed%20face%20avatars%20from%0Avideos%20is%20a%20challenging%20problem%20and%20has%20numerous%20applications%20in%20augmented%20and%0Avirtual%20reality.%20While%20tremendous%20successes%20have%20been%20achieved%20for%20static%0Afaces%2C%20animatable%20avatars%20from%20monocular%20videos%20still%20fall%20in%20the%20uncanny%0Avalley.%20The%20de%20facto%20standard%2C%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20represents%20a%20face%0Athrough%20a%20collection%20of%203D%20Gaussian%20primitives.%203DGS%20excels%20at%20rendering%20static%0Afaces%2C%20but%20the%20state-of-the-art%20still%20struggles%20with%20nonlinear%20deformations%2C%0Acomplex%20lighting%20effects%2C%20and%20fine%20details.%20While%20most%20related%20works%20focus%20on%0Apredicting%20better%20Gaussian%20parameters%20from%20expression%20codes%2C%20we%20rethink%20the%203D%0AGaussian%20representation%20itself%20and%20how%20to%20make%20it%20more%20expressive.%20Our%20insights%0Alead%20to%20a%20novel%20extension%20of%203D%20Gaussians%20to%20high-dimensional%20multivariate%0AGaussians%2C%20dubbed%20%27HyperGaussians%27.%20The%20higher%20dimensionality%20increases%0Aexpressivity%20through%20conditioning%20on%20a%20learnable%20local%20embedding.%20However%2C%0Asplatting%20HyperGaussians%20is%20computationally%20expensive%20because%20it%20requires%0Ainverting%20a%20high-dimensional%20covariance%20matrix.%20We%20solve%20this%20by%0Areparameterizing%20the%20covariance%20matrix%2C%20dubbed%20the%20%27inverse%20covariance%20trick%27.%0AThis%20trick%20boosts%20the%20efficiency%20so%20that%20HyperGaussians%20can%20be%20seamlessly%0Aintegrated%20into%20existing%20models.%20To%20demonstrate%20this%2C%20we%20plug%20in%20HyperGaussians%0Ainto%20the%20state-of-the-art%20in%20fast%20monocular%20face%20avatars%3A%20FlashAvatar.%20Our%0Aevaluation%20on%2019%20subjects%20from%204%20face%20datasets%20shows%20that%20HyperGaussians%0Aoutperform%203DGS%20numerically%20and%20visually%2C%20particularly%20for%20high-frequency%0Adetails%20like%20eyeglass%20frames%2C%20teeth%2C%20complex%20facial%20movements%2C%20and%20specular%0Areflections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02803v1&entry.124074799=Read"},
{"title": "AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars", "author": "Yiming Zhong and Xiaolin Zhang and Ligang Liu and Yao Zhao and Yunchao Wei", "abstract": "  Similar to facial beautification in real life, 3D virtual avatars require\npersonalized customization to enhance their visual appeal, yet this area\nremains insufficiently explored. Although current 3D Gaussian editing methods\ncan be adapted for facial makeup purposes, these methods fail to meet the\nfundamental requirements for achieving realistic makeup effects: 1) ensuring a\nconsistent appearance during drivable expressions, 2) preserving the identity\nthroughout the makeup process, and 3) enabling precise control over fine\ndetails. To address these, we propose a specialized 3D makeup method named\nAvatarMakeup, leveraging a pretrained diffusion model to transfer makeup\npatterns from a single reference photo of any individual. We adopt a\ncoarse-to-fine idea to first maintain the consistent appearance and identity,\nand then to refine the details. In particular, the diffusion model is employed\nto generate makeup images as supervision. Due to the uncertainties in diffusion\nprocess, the generated images are inconsistent across different viewpoints and\nexpressions. Therefore, we propose a Coherent Duplication method to coarsely\napply makeup to the target while ensuring consistency across dynamic and\nmultiview effects. Coherent Duplication optimizes a global UV map by recoding\nthe averaged facial attributes among the generated makeup images. By querying\nthe global UV map, it easily synthesizes coherent makeup guidance from\narbitrary views and expressions to optimize the target avatar. Given the coarse\nmakeup avatar, we further enhance the makeup by incorporating a Refinement\nModule into the diffusion model to achieve high makeup quality. Experiments\ndemonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality\nand consistency throughout animation.\n", "link": "http://arxiv.org/abs/2507.02419v1", "date": "2025-07-03", "relevancy": 3.4119, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7013}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7013}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AvatarMakeup%3A%20Realistic%20Makeup%20Transfer%20for%203D%20Animatable%20Head%20Avatars&body=Title%3A%20AvatarMakeup%3A%20Realistic%20Makeup%20Transfer%20for%203D%20Animatable%20Head%20Avatars%0AAuthor%3A%20Yiming%20Zhong%20and%20Xiaolin%20Zhang%20and%20Ligang%20Liu%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20Similar%20to%20facial%20beautification%20in%20real%20life%2C%203D%20virtual%20avatars%20require%0Apersonalized%20customization%20to%20enhance%20their%20visual%20appeal%2C%20yet%20this%20area%0Aremains%20insufficiently%20explored.%20Although%20current%203D%20Gaussian%20editing%20methods%0Acan%20be%20adapted%20for%20facial%20makeup%20purposes%2C%20these%20methods%20fail%20to%20meet%20the%0Afundamental%20requirements%20for%20achieving%20realistic%20makeup%20effects%3A%201%29%20ensuring%20a%0Aconsistent%20appearance%20during%20drivable%20expressions%2C%202%29%20preserving%20the%20identity%0Athroughout%20the%20makeup%20process%2C%20and%203%29%20enabling%20precise%20control%20over%20fine%0Adetails.%20To%20address%20these%2C%20we%20propose%20a%20specialized%203D%20makeup%20method%20named%0AAvatarMakeup%2C%20leveraging%20a%20pretrained%20diffusion%20model%20to%20transfer%20makeup%0Apatterns%20from%20a%20single%20reference%20photo%20of%20any%20individual.%20We%20adopt%20a%0Acoarse-to-fine%20idea%20to%20first%20maintain%20the%20consistent%20appearance%20and%20identity%2C%0Aand%20then%20to%20refine%20the%20details.%20In%20particular%2C%20the%20diffusion%20model%20is%20employed%0Ato%20generate%20makeup%20images%20as%20supervision.%20Due%20to%20the%20uncertainties%20in%20diffusion%0Aprocess%2C%20the%20generated%20images%20are%20inconsistent%20across%20different%20viewpoints%20and%0Aexpressions.%20Therefore%2C%20we%20propose%20a%20Coherent%20Duplication%20method%20to%20coarsely%0Aapply%20makeup%20to%20the%20target%20while%20ensuring%20consistency%20across%20dynamic%20and%0Amultiview%20effects.%20Coherent%20Duplication%20optimizes%20a%20global%20UV%20map%20by%20recoding%0Athe%20averaged%20facial%20attributes%20among%20the%20generated%20makeup%20images.%20By%20querying%0Athe%20global%20UV%20map%2C%20it%20easily%20synthesizes%20coherent%20makeup%20guidance%20from%0Aarbitrary%20views%20and%20expressions%20to%20optimize%20the%20target%20avatar.%20Given%20the%20coarse%0Amakeup%20avatar%2C%20we%20further%20enhance%20the%20makeup%20by%20incorporating%20a%20Refinement%0AModule%20into%20the%20diffusion%20model%20to%20achieve%20high%20makeup%20quality.%20Experiments%0Ademonstrate%20that%20AvatarMakeup%20achieves%20state-of-the-art%20makeup%20transfer%20quality%0Aand%20consistency%20throughout%20animation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvatarMakeup%253A%2520Realistic%2520Makeup%2520Transfer%2520for%25203D%2520Animatable%2520Head%2520Avatars%26entry.906535625%3DYiming%2520Zhong%2520and%2520Xiaolin%2520Zhang%2520and%2520Ligang%2520Liu%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520Similar%2520to%2520facial%2520beautification%2520in%2520real%2520life%252C%25203D%2520virtual%2520avatars%2520require%250Apersonalized%2520customization%2520to%2520enhance%2520their%2520visual%2520appeal%252C%2520yet%2520this%2520area%250Aremains%2520insufficiently%2520explored.%2520Although%2520current%25203D%2520Gaussian%2520editing%2520methods%250Acan%2520be%2520adapted%2520for%2520facial%2520makeup%2520purposes%252C%2520these%2520methods%2520fail%2520to%2520meet%2520the%250Afundamental%2520requirements%2520for%2520achieving%2520realistic%2520makeup%2520effects%253A%25201%2529%2520ensuring%2520a%250Aconsistent%2520appearance%2520during%2520drivable%2520expressions%252C%25202%2529%2520preserving%2520the%2520identity%250Athroughout%2520the%2520makeup%2520process%252C%2520and%25203%2529%2520enabling%2520precise%2520control%2520over%2520fine%250Adetails.%2520To%2520address%2520these%252C%2520we%2520propose%2520a%2520specialized%25203D%2520makeup%2520method%2520named%250AAvatarMakeup%252C%2520leveraging%2520a%2520pretrained%2520diffusion%2520model%2520to%2520transfer%2520makeup%250Apatterns%2520from%2520a%2520single%2520reference%2520photo%2520of%2520any%2520individual.%2520We%2520adopt%2520a%250Acoarse-to-fine%2520idea%2520to%2520first%2520maintain%2520the%2520consistent%2520appearance%2520and%2520identity%252C%250Aand%2520then%2520to%2520refine%2520the%2520details.%2520In%2520particular%252C%2520the%2520diffusion%2520model%2520is%2520employed%250Ato%2520generate%2520makeup%2520images%2520as%2520supervision.%2520Due%2520to%2520the%2520uncertainties%2520in%2520diffusion%250Aprocess%252C%2520the%2520generated%2520images%2520are%2520inconsistent%2520across%2520different%2520viewpoints%2520and%250Aexpressions.%2520Therefore%252C%2520we%2520propose%2520a%2520Coherent%2520Duplication%2520method%2520to%2520coarsely%250Aapply%2520makeup%2520to%2520the%2520target%2520while%2520ensuring%2520consistency%2520across%2520dynamic%2520and%250Amultiview%2520effects.%2520Coherent%2520Duplication%2520optimizes%2520a%2520global%2520UV%2520map%2520by%2520recoding%250Athe%2520averaged%2520facial%2520attributes%2520among%2520the%2520generated%2520makeup%2520images.%2520By%2520querying%250Athe%2520global%2520UV%2520map%252C%2520it%2520easily%2520synthesizes%2520coherent%2520makeup%2520guidance%2520from%250Aarbitrary%2520views%2520and%2520expressions%2520to%2520optimize%2520the%2520target%2520avatar.%2520Given%2520the%2520coarse%250Amakeup%2520avatar%252C%2520we%2520further%2520enhance%2520the%2520makeup%2520by%2520incorporating%2520a%2520Refinement%250AModule%2520into%2520the%2520diffusion%2520model%2520to%2520achieve%2520high%2520makeup%2520quality.%2520Experiments%250Ademonstrate%2520that%2520AvatarMakeup%2520achieves%2520state-of-the-art%2520makeup%2520transfer%2520quality%250Aand%2520consistency%2520throughout%2520animation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AvatarMakeup%3A%20Realistic%20Makeup%20Transfer%20for%203D%20Animatable%20Head%20Avatars&entry.906535625=Yiming%20Zhong%20and%20Xiaolin%20Zhang%20and%20Ligang%20Liu%20and%20Yao%20Zhao%20and%20Yunchao%20Wei&entry.1292438233=%20%20Similar%20to%20facial%20beautification%20in%20real%20life%2C%203D%20virtual%20avatars%20require%0Apersonalized%20customization%20to%20enhance%20their%20visual%20appeal%2C%20yet%20this%20area%0Aremains%20insufficiently%20explored.%20Although%20current%203D%20Gaussian%20editing%20methods%0Acan%20be%20adapted%20for%20facial%20makeup%20purposes%2C%20these%20methods%20fail%20to%20meet%20the%0Afundamental%20requirements%20for%20achieving%20realistic%20makeup%20effects%3A%201%29%20ensuring%20a%0Aconsistent%20appearance%20during%20drivable%20expressions%2C%202%29%20preserving%20the%20identity%0Athroughout%20the%20makeup%20process%2C%20and%203%29%20enabling%20precise%20control%20over%20fine%0Adetails.%20To%20address%20these%2C%20we%20propose%20a%20specialized%203D%20makeup%20method%20named%0AAvatarMakeup%2C%20leveraging%20a%20pretrained%20diffusion%20model%20to%20transfer%20makeup%0Apatterns%20from%20a%20single%20reference%20photo%20of%20any%20individual.%20We%20adopt%20a%0Acoarse-to-fine%20idea%20to%20first%20maintain%20the%20consistent%20appearance%20and%20identity%2C%0Aand%20then%20to%20refine%20the%20details.%20In%20particular%2C%20the%20diffusion%20model%20is%20employed%0Ato%20generate%20makeup%20images%20as%20supervision.%20Due%20to%20the%20uncertainties%20in%20diffusion%0Aprocess%2C%20the%20generated%20images%20are%20inconsistent%20across%20different%20viewpoints%20and%0Aexpressions.%20Therefore%2C%20we%20propose%20a%20Coherent%20Duplication%20method%20to%20coarsely%0Aapply%20makeup%20to%20the%20target%20while%20ensuring%20consistency%20across%20dynamic%20and%0Amultiview%20effects.%20Coherent%20Duplication%20optimizes%20a%20global%20UV%20map%20by%20recoding%0Athe%20averaged%20facial%20attributes%20among%20the%20generated%20makeup%20images.%20By%20querying%0Athe%20global%20UV%20map%2C%20it%20easily%20synthesizes%20coherent%20makeup%20guidance%20from%0Aarbitrary%20views%20and%20expressions%20to%20optimize%20the%20target%20avatar.%20Given%20the%20coarse%0Amakeup%20avatar%2C%20we%20further%20enhance%20the%20makeup%20by%20incorporating%20a%20Refinement%0AModule%20into%20the%20diffusion%20model%20to%20achieve%20high%20makeup%20quality.%20Experiments%0Ademonstrate%20that%20AvatarMakeup%20achieves%20state-of-the-art%20makeup%20transfer%20quality%0Aand%20consistency%20throughout%20animation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02419v1&entry.124074799=Read"},
{"title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and\n  Manipulation of Articulated Objects", "author": "Qiaojun Yu and Xibin Yuan and Yu jiang and Junting Chen and Dongzhe Zheng and Ce Hao and Yang You and Yixing Chen and Yao Mu and Liu Liu and Cewu Lu", "abstract": "  Articulated object manipulation remains a critical challenge in robotics due\nto the complex kinematic constraints and the limited physical reasoning of\nexisting methods. In this work, we introduce ArtGS, a novel framework that\nextends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling\nfor articulated object understanding and interaction. ArtGS begins with\nmulti-view RGB-D reconstruction, followed by reasoning with a vision-language\nmodel (VLM) to extract semantic and structural information, particularly the\narticulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS\noptimizes the parameters of the articulated bones, ensuring physically\nconsistent motion constraints and enhancing the manipulation policy. By\nleveraging dynamic Gaussian splatting, cross-embodiment adaptability, and\nclosed-loop optimization, ArtGS establishes a new framework for efficient,\nscalable, and generalizable articulated object modeling and manipulation.\nExperiments conducted in both simulation and real-world environments\ndemonstrate that ArtGS significantly outperforms previous methods in joint\nestimation accuracy and manipulation success rates across a variety of\narticulated objects. Additional images and videos are available on the project\nwebsite: https://sites.google.com/view/artgs/home\n", "link": "http://arxiv.org/abs/2507.02600v1", "date": "2025-07-03", "relevancy": 3.3891, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.705}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6692}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArtGS%3A3D%20Gaussian%20Splatting%20for%20Interactive%20Visual-Physical%20Modeling%20and%0A%20%20Manipulation%20of%20Articulated%20Objects&body=Title%3A%20ArtGS%3A3D%20Gaussian%20Splatting%20for%20Interactive%20Visual-Physical%20Modeling%20and%0A%20%20Manipulation%20of%20Articulated%20Objects%0AAuthor%3A%20Qiaojun%20Yu%20and%20Xibin%20Yuan%20and%20Yu%20jiang%20and%20Junting%20Chen%20and%20Dongzhe%20Zheng%20and%20Ce%20Hao%20and%20Yang%20You%20and%20Yixing%20Chen%20and%20Yao%20Mu%20and%20Liu%20Liu%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Articulated%20object%20manipulation%20remains%20a%20critical%20challenge%20in%20robotics%20due%0Ato%20the%20complex%20kinematic%20constraints%20and%20the%20limited%20physical%20reasoning%20of%0Aexisting%20methods.%20In%20this%20work%2C%20we%20introduce%20ArtGS%2C%20a%20novel%20framework%20that%0Aextends%203D%20Gaussian%20Splatting%20%283DGS%29%20by%20integrating%20visual-physical%20modeling%0Afor%20articulated%20object%20understanding%20and%20interaction.%20ArtGS%20begins%20with%0Amulti-view%20RGB-D%20reconstruction%2C%20followed%20by%20reasoning%20with%20a%20vision-language%0Amodel%20%28VLM%29%20to%20extract%20semantic%20and%20structural%20information%2C%20particularly%20the%0Aarticulated%20bones.%20Through%20dynamic%2C%20differentiable%203DGS-based%20rendering%2C%20ArtGS%0Aoptimizes%20the%20parameters%20of%20the%20articulated%20bones%2C%20ensuring%20physically%0Aconsistent%20motion%20constraints%20and%20enhancing%20the%20manipulation%20policy.%20By%0Aleveraging%20dynamic%20Gaussian%20splatting%2C%20cross-embodiment%20adaptability%2C%20and%0Aclosed-loop%20optimization%2C%20ArtGS%20establishes%20a%20new%20framework%20for%20efficient%2C%0Ascalable%2C%20and%20generalizable%20articulated%20object%20modeling%20and%20manipulation.%0AExperiments%20conducted%20in%20both%20simulation%20and%20real-world%20environments%0Ademonstrate%20that%20ArtGS%20significantly%20outperforms%20previous%20methods%20in%20joint%0Aestimation%20accuracy%20and%20manipulation%20success%20rates%20across%20a%20variety%20of%0Aarticulated%20objects.%20Additional%20images%20and%20videos%20are%20available%20on%20the%20project%0Awebsite%3A%20https%3A//sites.google.com/view/artgs/home%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtGS%253A3D%2520Gaussian%2520Splatting%2520for%2520Interactive%2520Visual-Physical%2520Modeling%2520and%250A%2520%2520Manipulation%2520of%2520Articulated%2520Objects%26entry.906535625%3DQiaojun%2520Yu%2520and%2520Xibin%2520Yuan%2520and%2520Yu%2520jiang%2520and%2520Junting%2520Chen%2520and%2520Dongzhe%2520Zheng%2520and%2520Ce%2520Hao%2520and%2520Yang%2520You%2520and%2520Yixing%2520Chen%2520and%2520Yao%2520Mu%2520and%2520Liu%2520Liu%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520Articulated%2520object%2520manipulation%2520remains%2520a%2520critical%2520challenge%2520in%2520robotics%2520due%250Ato%2520the%2520complex%2520kinematic%2520constraints%2520and%2520the%2520limited%2520physical%2520reasoning%2520of%250Aexisting%2520methods.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ArtGS%252C%2520a%2520novel%2520framework%2520that%250Aextends%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520by%2520integrating%2520visual-physical%2520modeling%250Afor%2520articulated%2520object%2520understanding%2520and%2520interaction.%2520ArtGS%2520begins%2520with%250Amulti-view%2520RGB-D%2520reconstruction%252C%2520followed%2520by%2520reasoning%2520with%2520a%2520vision-language%250Amodel%2520%2528VLM%2529%2520to%2520extract%2520semantic%2520and%2520structural%2520information%252C%2520particularly%2520the%250Aarticulated%2520bones.%2520Through%2520dynamic%252C%2520differentiable%25203DGS-based%2520rendering%252C%2520ArtGS%250Aoptimizes%2520the%2520parameters%2520of%2520the%2520articulated%2520bones%252C%2520ensuring%2520physically%250Aconsistent%2520motion%2520constraints%2520and%2520enhancing%2520the%2520manipulation%2520policy.%2520By%250Aleveraging%2520dynamic%2520Gaussian%2520splatting%252C%2520cross-embodiment%2520adaptability%252C%2520and%250Aclosed-loop%2520optimization%252C%2520ArtGS%2520establishes%2520a%2520new%2520framework%2520for%2520efficient%252C%250Ascalable%252C%2520and%2520generalizable%2520articulated%2520object%2520modeling%2520and%2520manipulation.%250AExperiments%2520conducted%2520in%2520both%2520simulation%2520and%2520real-world%2520environments%250Ademonstrate%2520that%2520ArtGS%2520significantly%2520outperforms%2520previous%2520methods%2520in%2520joint%250Aestimation%2520accuracy%2520and%2520manipulation%2520success%2520rates%2520across%2520a%2520variety%2520of%250Aarticulated%2520objects.%2520Additional%2520images%2520and%2520videos%2520are%2520available%2520on%2520the%2520project%250Awebsite%253A%2520https%253A//sites.google.com/view/artgs/home%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtGS%3A3D%20Gaussian%20Splatting%20for%20Interactive%20Visual-Physical%20Modeling%20and%0A%20%20Manipulation%20of%20Articulated%20Objects&entry.906535625=Qiaojun%20Yu%20and%20Xibin%20Yuan%20and%20Yu%20jiang%20and%20Junting%20Chen%20and%20Dongzhe%20Zheng%20and%20Ce%20Hao%20and%20Yang%20You%20and%20Yixing%20Chen%20and%20Yao%20Mu%20and%20Liu%20Liu%20and%20Cewu%20Lu&entry.1292438233=%20%20Articulated%20object%20manipulation%20remains%20a%20critical%20challenge%20in%20robotics%20due%0Ato%20the%20complex%20kinematic%20constraints%20and%20the%20limited%20physical%20reasoning%20of%0Aexisting%20methods.%20In%20this%20work%2C%20we%20introduce%20ArtGS%2C%20a%20novel%20framework%20that%0Aextends%203D%20Gaussian%20Splatting%20%283DGS%29%20by%20integrating%20visual-physical%20modeling%0Afor%20articulated%20object%20understanding%20and%20interaction.%20ArtGS%20begins%20with%0Amulti-view%20RGB-D%20reconstruction%2C%20followed%20by%20reasoning%20with%20a%20vision-language%0Amodel%20%28VLM%29%20to%20extract%20semantic%20and%20structural%20information%2C%20particularly%20the%0Aarticulated%20bones.%20Through%20dynamic%2C%20differentiable%203DGS-based%20rendering%2C%20ArtGS%0Aoptimizes%20the%20parameters%20of%20the%20articulated%20bones%2C%20ensuring%20physically%0Aconsistent%20motion%20constraints%20and%20enhancing%20the%20manipulation%20policy.%20By%0Aleveraging%20dynamic%20Gaussian%20splatting%2C%20cross-embodiment%20adaptability%2C%20and%0Aclosed-loop%20optimization%2C%20ArtGS%20establishes%20a%20new%20framework%20for%20efficient%2C%0Ascalable%2C%20and%20generalizable%20articulated%20object%20modeling%20and%20manipulation.%0AExperiments%20conducted%20in%20both%20simulation%20and%20real-world%20environments%0Ademonstrate%20that%20ArtGS%20significantly%20outperforms%20previous%20methods%20in%20joint%0Aestimation%20accuracy%20and%20manipulation%20success%20rates%20across%20a%20variety%20of%0Aarticulated%20objects.%20Additional%20images%20and%20videos%20are%20available%20on%20the%20project%0Awebsite%3A%20https%3A//sites.google.com/view/artgs/home%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02600v1&entry.124074799=Read"},
{"title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local\n  Implicit Feature Decoupling", "author": "Jiahao Wu and Rui Peng and Jianbo Jiao and Jiayu Yang and Luyang Tang and Kaiqiang Xiong and Jie Liang and Jinbo Yan and Runling Liu and Ronggang Wang", "abstract": "  Due to the complex and highly dynamic motions in the real world, synthesizing\ndynamic videos from multi-view inputs for arbitrary viewpoints is challenging.\nPrevious works based on neural radiance field or 3D Gaussian splatting are\nlimited to modeling fine-scale motion, greatly restricting their application.\nIn this paper, we introduce LocalDyGS, which consists of two parts to adapt our\nmethod to both large-scale and fine-scale motion scenes: 1) We decompose a\ncomplex dynamic scene into streamlined local spaces defined by seeds, enabling\nglobal modeling by capturing motion within each local space. 2) We decouple\nstatic and dynamic features for local space motion modeling. A static feature\nshared across time steps captures static information, while a dynamic residual\nfield provides time-specific features. These are combined and decoded to\ngenerate Temporal Gaussians, modeling motion within each local space. As a\nresult, we propose a novel dynamic scene reconstruction framework to model\nhighly dynamic real-world scenes more realistically. Our method not only\ndemonstrates competitive performance on various fine-scale datasets compared to\nstate-of-the-art (SOTA) methods, but also represents the first attempt to model\nlarger and more complex highly dynamic scenes. Project page:\nhttps://wujh2001.github.io/LocalDyGS/.\n", "link": "http://arxiv.org/abs/2507.02363v1", "date": "2025-07-03", "relevancy": 3.361, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7196}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6525}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LocalDyGS%3A%20Multi-view%20Global%20Dynamic%20Scene%20Modeling%20via%20Adaptive%20Local%0A%20%20Implicit%20Feature%20Decoupling&body=Title%3A%20LocalDyGS%3A%20Multi-view%20Global%20Dynamic%20Scene%20Modeling%20via%20Adaptive%20Local%0A%20%20Implicit%20Feature%20Decoupling%0AAuthor%3A%20Jiahao%20Wu%20and%20Rui%20Peng%20and%20Jianbo%20Jiao%20and%20Jiayu%20Yang%20and%20Luyang%20Tang%20and%20Kaiqiang%20Xiong%20and%20Jie%20Liang%20and%20Jinbo%20Yan%20and%20Runling%20Liu%20and%20Ronggang%20Wang%0AAbstract%3A%20%20%20Due%20to%20the%20complex%20and%20highly%20dynamic%20motions%20in%20the%20real%20world%2C%20synthesizing%0Adynamic%20videos%20from%20multi-view%20inputs%20for%20arbitrary%20viewpoints%20is%20challenging.%0APrevious%20works%20based%20on%20neural%20radiance%20field%20or%203D%20Gaussian%20splatting%20are%0Alimited%20to%20modeling%20fine-scale%20motion%2C%20greatly%20restricting%20their%20application.%0AIn%20this%20paper%2C%20we%20introduce%20LocalDyGS%2C%20which%20consists%20of%20two%20parts%20to%20adapt%20our%0Amethod%20to%20both%20large-scale%20and%20fine-scale%20motion%20scenes%3A%201%29%20We%20decompose%20a%0Acomplex%20dynamic%20scene%20into%20streamlined%20local%20spaces%20defined%20by%20seeds%2C%20enabling%0Aglobal%20modeling%20by%20capturing%20motion%20within%20each%20local%20space.%202%29%20We%20decouple%0Astatic%20and%20dynamic%20features%20for%20local%20space%20motion%20modeling.%20A%20static%20feature%0Ashared%20across%20time%20steps%20captures%20static%20information%2C%20while%20a%20dynamic%20residual%0Afield%20provides%20time-specific%20features.%20These%20are%20combined%20and%20decoded%20to%0Agenerate%20Temporal%20Gaussians%2C%20modeling%20motion%20within%20each%20local%20space.%20As%20a%0Aresult%2C%20we%20propose%20a%20novel%20dynamic%20scene%20reconstruction%20framework%20to%20model%0Ahighly%20dynamic%20real-world%20scenes%20more%20realistically.%20Our%20method%20not%20only%0Ademonstrates%20competitive%20performance%20on%20various%20fine-scale%20datasets%20compared%20to%0Astate-of-the-art%20%28SOTA%29%20methods%2C%20but%20also%20represents%20the%20first%20attempt%20to%20model%0Alarger%20and%20more%20complex%20highly%20dynamic%20scenes.%20Project%20page%3A%0Ahttps%3A//wujh2001.github.io/LocalDyGS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalDyGS%253A%2520Multi-view%2520Global%2520Dynamic%2520Scene%2520Modeling%2520via%2520Adaptive%2520Local%250A%2520%2520Implicit%2520Feature%2520Decoupling%26entry.906535625%3DJiahao%2520Wu%2520and%2520Rui%2520Peng%2520and%2520Jianbo%2520Jiao%2520and%2520Jiayu%2520Yang%2520and%2520Luyang%2520Tang%2520and%2520Kaiqiang%2520Xiong%2520and%2520Jie%2520Liang%2520and%2520Jinbo%2520Yan%2520and%2520Runling%2520Liu%2520and%2520Ronggang%2520Wang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520complex%2520and%2520highly%2520dynamic%2520motions%2520in%2520the%2520real%2520world%252C%2520synthesizing%250Adynamic%2520videos%2520from%2520multi-view%2520inputs%2520for%2520arbitrary%2520viewpoints%2520is%2520challenging.%250APrevious%2520works%2520based%2520on%2520neural%2520radiance%2520field%2520or%25203D%2520Gaussian%2520splatting%2520are%250Alimited%2520to%2520modeling%2520fine-scale%2520motion%252C%2520greatly%2520restricting%2520their%2520application.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520LocalDyGS%252C%2520which%2520consists%2520of%2520two%2520parts%2520to%2520adapt%2520our%250Amethod%2520to%2520both%2520large-scale%2520and%2520fine-scale%2520motion%2520scenes%253A%25201%2529%2520We%2520decompose%2520a%250Acomplex%2520dynamic%2520scene%2520into%2520streamlined%2520local%2520spaces%2520defined%2520by%2520seeds%252C%2520enabling%250Aglobal%2520modeling%2520by%2520capturing%2520motion%2520within%2520each%2520local%2520space.%25202%2529%2520We%2520decouple%250Astatic%2520and%2520dynamic%2520features%2520for%2520local%2520space%2520motion%2520modeling.%2520A%2520static%2520feature%250Ashared%2520across%2520time%2520steps%2520captures%2520static%2520information%252C%2520while%2520a%2520dynamic%2520residual%250Afield%2520provides%2520time-specific%2520features.%2520These%2520are%2520combined%2520and%2520decoded%2520to%250Agenerate%2520Temporal%2520Gaussians%252C%2520modeling%2520motion%2520within%2520each%2520local%2520space.%2520As%2520a%250Aresult%252C%2520we%2520propose%2520a%2520novel%2520dynamic%2520scene%2520reconstruction%2520framework%2520to%2520model%250Ahighly%2520dynamic%2520real-world%2520scenes%2520more%2520realistically.%2520Our%2520method%2520not%2520only%250Ademonstrates%2520competitive%2520performance%2520on%2520various%2520fine-scale%2520datasets%2520compared%2520to%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%252C%2520but%2520also%2520represents%2520the%2520first%2520attempt%2520to%2520model%250Alarger%2520and%2520more%2520complex%2520highly%2520dynamic%2520scenes.%2520Project%2520page%253A%250Ahttps%253A//wujh2001.github.io/LocalDyGS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocalDyGS%3A%20Multi-view%20Global%20Dynamic%20Scene%20Modeling%20via%20Adaptive%20Local%0A%20%20Implicit%20Feature%20Decoupling&entry.906535625=Jiahao%20Wu%20and%20Rui%20Peng%20and%20Jianbo%20Jiao%20and%20Jiayu%20Yang%20and%20Luyang%20Tang%20and%20Kaiqiang%20Xiong%20and%20Jie%20Liang%20and%20Jinbo%20Yan%20and%20Runling%20Liu%20and%20Ronggang%20Wang&entry.1292438233=%20%20Due%20to%20the%20complex%20and%20highly%20dynamic%20motions%20in%20the%20real%20world%2C%20synthesizing%0Adynamic%20videos%20from%20multi-view%20inputs%20for%20arbitrary%20viewpoints%20is%20challenging.%0APrevious%20works%20based%20on%20neural%20radiance%20field%20or%203D%20Gaussian%20splatting%20are%0Alimited%20to%20modeling%20fine-scale%20motion%2C%20greatly%20restricting%20their%20application.%0AIn%20this%20paper%2C%20we%20introduce%20LocalDyGS%2C%20which%20consists%20of%20two%20parts%20to%20adapt%20our%0Amethod%20to%20both%20large-scale%20and%20fine-scale%20motion%20scenes%3A%201%29%20We%20decompose%20a%0Acomplex%20dynamic%20scene%20into%20streamlined%20local%20spaces%20defined%20by%20seeds%2C%20enabling%0Aglobal%20modeling%20by%20capturing%20motion%20within%20each%20local%20space.%202%29%20We%20decouple%0Astatic%20and%20dynamic%20features%20for%20local%20space%20motion%20modeling.%20A%20static%20feature%0Ashared%20across%20time%20steps%20captures%20static%20information%2C%20while%20a%20dynamic%20residual%0Afield%20provides%20time-specific%20features.%20These%20are%20combined%20and%20decoded%20to%0Agenerate%20Temporal%20Gaussians%2C%20modeling%20motion%20within%20each%20local%20space.%20As%20a%0Aresult%2C%20we%20propose%20a%20novel%20dynamic%20scene%20reconstruction%20framework%20to%20model%0Ahighly%20dynamic%20real-world%20scenes%20more%20realistically.%20Our%20method%20not%20only%0Ademonstrates%20competitive%20performance%20on%20various%20fine-scale%20datasets%20compared%20to%0Astate-of-the-art%20%28SOTA%29%20methods%2C%20but%20also%20represents%20the%20first%20attempt%20to%20model%0Alarger%20and%20more%20complex%20highly%20dynamic%20scenes.%20Project%20page%3A%0Ahttps%3A//wujh2001.github.io/LocalDyGS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02363v1&entry.124074799=Read"},
{"title": "ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model", "author": "Xuangeng Chu and Nabarun Goswami and Ziteng Cui and Hanqin Wang and Tatsuya Harada", "abstract": "  Speech-driven 3D facial animation aims to generate realistic lip movements\nand facial expressions for 3D head models from arbitrary audio clips. Although\nexisting diffusion-based methods are capable of producing natural motions,\ntheir slow generation speed limits their application potential. In this paper,\nwe introduce a novel autoregressive model that achieves real-time generation of\nhighly synchronized lip movements and realistic head poses and eye blinks by\nlearning a mapping from speech to a multi-scale motion codebook. Furthermore,\nour model can adapt to unseen speaking styles, enabling the creation of 3D\ntalking avatars with unique personal styles beyond the identities seen during\ntraining. Extensive evaluations and user studies demonstrate that our method\noutperforms existing approaches in lip synchronization accuracy and perceived\nquality.\n", "link": "http://arxiv.org/abs/2502.20323v4", "date": "2025-07-03", "relevancy": 3.3117, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6636}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6617}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARTalk%3A%20Speech-Driven%203D%20Head%20Animation%20via%20Autoregressive%20Model&body=Title%3A%20ARTalk%3A%20Speech-Driven%203D%20Head%20Animation%20via%20Autoregressive%20Model%0AAuthor%3A%20Xuangeng%20Chu%20and%20Nabarun%20Goswami%20and%20Ziteng%20Cui%20and%20Hanqin%20Wang%20and%20Tatsuya%20Harada%0AAbstract%3A%20%20%20Speech-driven%203D%20facial%20animation%20aims%20to%20generate%20realistic%20lip%20movements%0Aand%20facial%20expressions%20for%203D%20head%20models%20from%20arbitrary%20audio%20clips.%20Although%0Aexisting%20diffusion-based%20methods%20are%20capable%20of%20producing%20natural%20motions%2C%0Atheir%20slow%20generation%20speed%20limits%20their%20application%20potential.%20In%20this%20paper%2C%0Awe%20introduce%20a%20novel%20autoregressive%20model%20that%20achieves%20real-time%20generation%20of%0Ahighly%20synchronized%20lip%20movements%20and%20realistic%20head%20poses%20and%20eye%20blinks%20by%0Alearning%20a%20mapping%20from%20speech%20to%20a%20multi-scale%20motion%20codebook.%20Furthermore%2C%0Aour%20model%20can%20adapt%20to%20unseen%20speaking%20styles%2C%20enabling%20the%20creation%20of%203D%0Atalking%20avatars%20with%20unique%20personal%20styles%20beyond%20the%20identities%20seen%20during%0Atraining.%20Extensive%20evaluations%20and%20user%20studies%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20approaches%20in%20lip%20synchronization%20accuracy%20and%20perceived%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20323v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARTalk%253A%2520Speech-Driven%25203D%2520Head%2520Animation%2520via%2520Autoregressive%2520Model%26entry.906535625%3DXuangeng%2520Chu%2520and%2520Nabarun%2520Goswami%2520and%2520Ziteng%2520Cui%2520and%2520Hanqin%2520Wang%2520and%2520Tatsuya%2520Harada%26entry.1292438233%3D%2520%2520Speech-driven%25203D%2520facial%2520animation%2520aims%2520to%2520generate%2520realistic%2520lip%2520movements%250Aand%2520facial%2520expressions%2520for%25203D%2520head%2520models%2520from%2520arbitrary%2520audio%2520clips.%2520Although%250Aexisting%2520diffusion-based%2520methods%2520are%2520capable%2520of%2520producing%2520natural%2520motions%252C%250Atheir%2520slow%2520generation%2520speed%2520limits%2520their%2520application%2520potential.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520a%2520novel%2520autoregressive%2520model%2520that%2520achieves%2520real-time%2520generation%2520of%250Ahighly%2520synchronized%2520lip%2520movements%2520and%2520realistic%2520head%2520poses%2520and%2520eye%2520blinks%2520by%250Alearning%2520a%2520mapping%2520from%2520speech%2520to%2520a%2520multi-scale%2520motion%2520codebook.%2520Furthermore%252C%250Aour%2520model%2520can%2520adapt%2520to%2520unseen%2520speaking%2520styles%252C%2520enabling%2520the%2520creation%2520of%25203D%250Atalking%2520avatars%2520with%2520unique%2520personal%2520styles%2520beyond%2520the%2520identities%2520seen%2520during%250Atraining.%2520Extensive%2520evaluations%2520and%2520user%2520studies%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520existing%2520approaches%2520in%2520lip%2520synchronization%2520accuracy%2520and%2520perceived%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20323v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARTalk%3A%20Speech-Driven%203D%20Head%20Animation%20via%20Autoregressive%20Model&entry.906535625=Xuangeng%20Chu%20and%20Nabarun%20Goswami%20and%20Ziteng%20Cui%20and%20Hanqin%20Wang%20and%20Tatsuya%20Harada&entry.1292438233=%20%20Speech-driven%203D%20facial%20animation%20aims%20to%20generate%20realistic%20lip%20movements%0Aand%20facial%20expressions%20for%203D%20head%20models%20from%20arbitrary%20audio%20clips.%20Although%0Aexisting%20diffusion-based%20methods%20are%20capable%20of%20producing%20natural%20motions%2C%0Atheir%20slow%20generation%20speed%20limits%20their%20application%20potential.%20In%20this%20paper%2C%0Awe%20introduce%20a%20novel%20autoregressive%20model%20that%20achieves%20real-time%20generation%20of%0Ahighly%20synchronized%20lip%20movements%20and%20realistic%20head%20poses%20and%20eye%20blinks%20by%0Alearning%20a%20mapping%20from%20speech%20to%20a%20multi-scale%20motion%20codebook.%20Furthermore%2C%0Aour%20model%20can%20adapt%20to%20unseen%20speaking%20styles%2C%20enabling%20the%20creation%20of%203D%0Atalking%20avatars%20with%20unique%20personal%20styles%20beyond%20the%20identities%20seen%20during%0Atraining.%20Extensive%20evaluations%20and%20user%20studies%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20approaches%20in%20lip%20synchronization%20accuracy%20and%20perceived%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20323v4&entry.124074799=Read"},
{"title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via\n  Diffusion Priors", "author": "Sicong Du and Jiarun Liu and Qifeng Chen and Hao-Xiang Chen and Tai-Jiang Mu and Sheng Yang", "abstract": "  A single-pass driving clip frequently results in incomplete scanning of the\nroad structure, making reconstructed scene expanding a critical requirement for\nsensor simulators to effectively regress driving actions. Although contemporary\n3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction\nquality, their direct extension through the integration of diffusion priors\noften introduces cumulative physical inconsistencies and compromises training\nefficiency. To address these limitations, we present RGE-GS, a novel expansive\nreconstruction framework that synergizes diffusion-based generation with\nreward-guided Gaussian integration. The RGE-GS framework incorporates two key\ninnovations: First, we propose a reward network that learns to identify and\nprioritize consistently generated patterns prior to reconstruction phases,\nthereby enabling selective retention of diffusion outputs for spatial\nstability. Second, during the reconstruction process, we devise a\ndifferentiated training strategy that automatically adjust Gaussian\noptimization progress according to scene converge metrics, which achieving\nbetter convergence than baseline methods. Extensive evaluations of publicly\navailable datasets demonstrate that RGE-GS achieves state-of-the-art\nperformance in reconstruction quality. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RGE-GS.\n", "link": "http://arxiv.org/abs/2506.22800v2", "date": "2025-07-03", "relevancy": 3.3062, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7121}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6432}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGE-GS%3A%20Reward-Guided%20Expansive%20Driving%20Scene%20Reconstruction%20via%0A%20%20Diffusion%20Priors&body=Title%3A%20RGE-GS%3A%20Reward-Guided%20Expansive%20Driving%20Scene%20Reconstruction%20via%0A%20%20Diffusion%20Priors%0AAuthor%3A%20Sicong%20Du%20and%20Jiarun%20Liu%20and%20Qifeng%20Chen%20and%20Hao-Xiang%20Chen%20and%20Tai-Jiang%20Mu%20and%20Sheng%20Yang%0AAbstract%3A%20%20%20A%20single-pass%20driving%20clip%20frequently%20results%20in%20incomplete%20scanning%20of%20the%0Aroad%20structure%2C%20making%20reconstructed%20scene%20expanding%20a%20critical%20requirement%20for%0Asensor%20simulators%20to%20effectively%20regress%20driving%20actions.%20Although%20contemporary%0A3D%20Gaussian%20Splatting%20%283DGS%29%20techniques%20achieve%20remarkable%20reconstruction%0Aquality%2C%20their%20direct%20extension%20through%20the%20integration%20of%20diffusion%20priors%0Aoften%20introduces%20cumulative%20physical%20inconsistencies%20and%20compromises%20training%0Aefficiency.%20To%20address%20these%20limitations%2C%20we%20present%20RGE-GS%2C%20a%20novel%20expansive%0Areconstruction%20framework%20that%20synergizes%20diffusion-based%20generation%20with%0Areward-guided%20Gaussian%20integration.%20The%20RGE-GS%20framework%20incorporates%20two%20key%0Ainnovations%3A%20First%2C%20we%20propose%20a%20reward%20network%20that%20learns%20to%20identify%20and%0Aprioritize%20consistently%20generated%20patterns%20prior%20to%20reconstruction%20phases%2C%0Athereby%20enabling%20selective%20retention%20of%20diffusion%20outputs%20for%20spatial%0Astability.%20Second%2C%20during%20the%20reconstruction%20process%2C%20we%20devise%20a%0Adifferentiated%20training%20strategy%20that%20automatically%20adjust%20Gaussian%0Aoptimization%20progress%20according%20to%20scene%20converge%20metrics%2C%20which%20achieving%0Abetter%20convergence%20than%20baseline%20methods.%20Extensive%20evaluations%20of%20publicly%0Aavailable%20datasets%20demonstrate%20that%20RGE-GS%20achieves%20state-of-the-art%0Aperformance%20in%20reconstruction%20quality.%20Our%20source-code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/CN-ADLab/RGE-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGE-GS%253A%2520Reward-Guided%2520Expansive%2520Driving%2520Scene%2520Reconstruction%2520via%250A%2520%2520Diffusion%2520Priors%26entry.906535625%3DSicong%2520Du%2520and%2520Jiarun%2520Liu%2520and%2520Qifeng%2520Chen%2520and%2520Hao-Xiang%2520Chen%2520and%2520Tai-Jiang%2520Mu%2520and%2520Sheng%2520Yang%26entry.1292438233%3D%2520%2520A%2520single-pass%2520driving%2520clip%2520frequently%2520results%2520in%2520incomplete%2520scanning%2520of%2520the%250Aroad%2520structure%252C%2520making%2520reconstructed%2520scene%2520expanding%2520a%2520critical%2520requirement%2520for%250Asensor%2520simulators%2520to%2520effectively%2520regress%2520driving%2520actions.%2520Although%2520contemporary%250A3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520techniques%2520achieve%2520remarkable%2520reconstruction%250Aquality%252C%2520their%2520direct%2520extension%2520through%2520the%2520integration%2520of%2520diffusion%2520priors%250Aoften%2520introduces%2520cumulative%2520physical%2520inconsistencies%2520and%2520compromises%2520training%250Aefficiency.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520RGE-GS%252C%2520a%2520novel%2520expansive%250Areconstruction%2520framework%2520that%2520synergizes%2520diffusion-based%2520generation%2520with%250Areward-guided%2520Gaussian%2520integration.%2520The%2520RGE-GS%2520framework%2520incorporates%2520two%2520key%250Ainnovations%253A%2520First%252C%2520we%2520propose%2520a%2520reward%2520network%2520that%2520learns%2520to%2520identify%2520and%250Aprioritize%2520consistently%2520generated%2520patterns%2520prior%2520to%2520reconstruction%2520phases%252C%250Athereby%2520enabling%2520selective%2520retention%2520of%2520diffusion%2520outputs%2520for%2520spatial%250Astability.%2520Second%252C%2520during%2520the%2520reconstruction%2520process%252C%2520we%2520devise%2520a%250Adifferentiated%2520training%2520strategy%2520that%2520automatically%2520adjust%2520Gaussian%250Aoptimization%2520progress%2520according%2520to%2520scene%2520converge%2520metrics%252C%2520which%2520achieving%250Abetter%2520convergence%2520than%2520baseline%2520methods.%2520Extensive%2520evaluations%2520of%2520publicly%250Aavailable%2520datasets%2520demonstrate%2520that%2520RGE-GS%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520reconstruction%2520quality.%2520Our%2520source-code%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%2520https%253A//github.com/CN-ADLab/RGE-GS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGE-GS%3A%20Reward-Guided%20Expansive%20Driving%20Scene%20Reconstruction%20via%0A%20%20Diffusion%20Priors&entry.906535625=Sicong%20Du%20and%20Jiarun%20Liu%20and%20Qifeng%20Chen%20and%20Hao-Xiang%20Chen%20and%20Tai-Jiang%20Mu%20and%20Sheng%20Yang&entry.1292438233=%20%20A%20single-pass%20driving%20clip%20frequently%20results%20in%20incomplete%20scanning%20of%20the%0Aroad%20structure%2C%20making%20reconstructed%20scene%20expanding%20a%20critical%20requirement%20for%0Asensor%20simulators%20to%20effectively%20regress%20driving%20actions.%20Although%20contemporary%0A3D%20Gaussian%20Splatting%20%283DGS%29%20techniques%20achieve%20remarkable%20reconstruction%0Aquality%2C%20their%20direct%20extension%20through%20the%20integration%20of%20diffusion%20priors%0Aoften%20introduces%20cumulative%20physical%20inconsistencies%20and%20compromises%20training%0Aefficiency.%20To%20address%20these%20limitations%2C%20we%20present%20RGE-GS%2C%20a%20novel%20expansive%0Areconstruction%20framework%20that%20synergizes%20diffusion-based%20generation%20with%0Areward-guided%20Gaussian%20integration.%20The%20RGE-GS%20framework%20incorporates%20two%20key%0Ainnovations%3A%20First%2C%20we%20propose%20a%20reward%20network%20that%20learns%20to%20identify%20and%0Aprioritize%20consistently%20generated%20patterns%20prior%20to%20reconstruction%20phases%2C%0Athereby%20enabling%20selective%20retention%20of%20diffusion%20outputs%20for%20spatial%0Astability.%20Second%2C%20during%20the%20reconstruction%20process%2C%20we%20devise%20a%0Adifferentiated%20training%20strategy%20that%20automatically%20adjust%20Gaussian%0Aoptimization%20progress%20according%20to%20scene%20converge%20metrics%2C%20which%20achieving%0Abetter%20convergence%20than%20baseline%20methods.%20Extensive%20evaluations%20of%20publicly%0Aavailable%20datasets%20demonstrate%20that%20RGE-GS%20achieves%20state-of-the-art%0Aperformance%20in%20reconstruction%20quality.%20Our%20source-code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/CN-ADLab/RGE-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22800v2&entry.124074799=Read"},
{"title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans", "author": "Zhening Huang and Xiaoyang Wu and Fangcheng Zhong and Hengshuang Zhao and Matthias Nie\u00dfner and Joan Lasenby", "abstract": "  We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor\nenvironments into compact, realistic, and interactive 3D virtual replicas.\nLiteReality not only reconstructs scenes that visually resemble reality but\nalso supports key features essential for graphics pipelines -- such as object\nindividuality, articulation, high-quality physically based rendering materials,\nand physically based interaction. At its core, LiteReality first performs scene\nunderstanding and parses the results into a coherent 3D layout and objects with\nthe help of a structured scene graph. It then reconstructs the scene by\nretrieving the most visually similar 3D artist-crafted models from a curated\nasset database. Next, the Material Painting module enhances realism by\nrecovering high-quality, spatially varying materials. Finally, the\nreconstructed scene is integrated into a simulation engine with basic physical\nproperties to enable interactive behavior. The resulting scenes are compact,\neditable, and fully compatible with standard graphics pipelines, making them\nsuitable for applications in AR/VR, gaming, robotics, and digital twins. In\naddition, LiteReality introduces a training-free object retrieval module that\nachieves state-of-the-art similarity performance on the Scan2CAD benchmark,\nalong with a robust material painting module capable of transferring\nappearances from images of any style to 3D assets -- even under severe\nmisalignment, occlusion, and poor lighting. We demonstrate the effectiveness of\nLiteReality on both real-life scans and public datasets. Project page:\nhttps://litereality.github.io; Video:\nhttps://www.youtube.com/watch?v=ecK9m3LXg2c\n", "link": "http://arxiv.org/abs/2507.02861v1", "date": "2025-07-03", "relevancy": 3.2138, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6448}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6448}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiteReality%3A%20Graphics-Ready%203D%20Scene%20Reconstruction%20from%20RGB-D%20Scans&body=Title%3A%20LiteReality%3A%20Graphics-Ready%203D%20Scene%20Reconstruction%20from%20RGB-D%20Scans%0AAuthor%3A%20Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Fangcheng%20Zhong%20and%20Hengshuang%20Zhao%20and%20Matthias%20Nie%C3%9Fner%20and%20Joan%20Lasenby%0AAbstract%3A%20%20%20We%20propose%20LiteReality%2C%20a%20novel%20pipeline%20that%20converts%20RGB-D%20scans%20of%20indoor%0Aenvironments%20into%20compact%2C%20realistic%2C%20and%20interactive%203D%20virtual%20replicas.%0ALiteReality%20not%20only%20reconstructs%20scenes%20that%20visually%20resemble%20reality%20but%0Aalso%20supports%20key%20features%20essential%20for%20graphics%20pipelines%20--%20such%20as%20object%0Aindividuality%2C%20articulation%2C%20high-quality%20physically%20based%20rendering%20materials%2C%0Aand%20physically%20based%20interaction.%20At%20its%20core%2C%20LiteReality%20first%20performs%20scene%0Aunderstanding%20and%20parses%20the%20results%20into%20a%20coherent%203D%20layout%20and%20objects%20with%0Athe%20help%20of%20a%20structured%20scene%20graph.%20It%20then%20reconstructs%20the%20scene%20by%0Aretrieving%20the%20most%20visually%20similar%203D%20artist-crafted%20models%20from%20a%20curated%0Aasset%20database.%20Next%2C%20the%20Material%20Painting%20module%20enhances%20realism%20by%0Arecovering%20high-quality%2C%20spatially%20varying%20materials.%20Finally%2C%20the%0Areconstructed%20scene%20is%20integrated%20into%20a%20simulation%20engine%20with%20basic%20physical%0Aproperties%20to%20enable%20interactive%20behavior.%20The%20resulting%20scenes%20are%20compact%2C%0Aeditable%2C%20and%20fully%20compatible%20with%20standard%20graphics%20pipelines%2C%20making%20them%0Asuitable%20for%20applications%20in%20AR/VR%2C%20gaming%2C%20robotics%2C%20and%20digital%20twins.%20In%0Aaddition%2C%20LiteReality%20introduces%20a%20training-free%20object%20retrieval%20module%20that%0Aachieves%20state-of-the-art%20similarity%20performance%20on%20the%20Scan2CAD%20benchmark%2C%0Aalong%20with%20a%20robust%20material%20painting%20module%20capable%20of%20transferring%0Aappearances%20from%20images%20of%20any%20style%20to%203D%20assets%20--%20even%20under%20severe%0Amisalignment%2C%20occlusion%2C%20and%20poor%20lighting.%20We%20demonstrate%20the%20effectiveness%20of%0ALiteReality%20on%20both%20real-life%20scans%20and%20public%20datasets.%20Project%20page%3A%0Ahttps%3A//litereality.github.io%3B%20Video%3A%0Ahttps%3A//www.youtube.com/watch%3Fv%3DecK9m3LXg2c%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiteReality%253A%2520Graphics-Ready%25203D%2520Scene%2520Reconstruction%2520from%2520RGB-D%2520Scans%26entry.906535625%3DZhening%2520Huang%2520and%2520Xiaoyang%2520Wu%2520and%2520Fangcheng%2520Zhong%2520and%2520Hengshuang%2520Zhao%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Joan%2520Lasenby%26entry.1292438233%3D%2520%2520We%2520propose%2520LiteReality%252C%2520a%2520novel%2520pipeline%2520that%2520converts%2520RGB-D%2520scans%2520of%2520indoor%250Aenvironments%2520into%2520compact%252C%2520realistic%252C%2520and%2520interactive%25203D%2520virtual%2520replicas.%250ALiteReality%2520not%2520only%2520reconstructs%2520scenes%2520that%2520visually%2520resemble%2520reality%2520but%250Aalso%2520supports%2520key%2520features%2520essential%2520for%2520graphics%2520pipelines%2520--%2520such%2520as%2520object%250Aindividuality%252C%2520articulation%252C%2520high-quality%2520physically%2520based%2520rendering%2520materials%252C%250Aand%2520physically%2520based%2520interaction.%2520At%2520its%2520core%252C%2520LiteReality%2520first%2520performs%2520scene%250Aunderstanding%2520and%2520parses%2520the%2520results%2520into%2520a%2520coherent%25203D%2520layout%2520and%2520objects%2520with%250Athe%2520help%2520of%2520a%2520structured%2520scene%2520graph.%2520It%2520then%2520reconstructs%2520the%2520scene%2520by%250Aretrieving%2520the%2520most%2520visually%2520similar%25203D%2520artist-crafted%2520models%2520from%2520a%2520curated%250Aasset%2520database.%2520Next%252C%2520the%2520Material%2520Painting%2520module%2520enhances%2520realism%2520by%250Arecovering%2520high-quality%252C%2520spatially%2520varying%2520materials.%2520Finally%252C%2520the%250Areconstructed%2520scene%2520is%2520integrated%2520into%2520a%2520simulation%2520engine%2520with%2520basic%2520physical%250Aproperties%2520to%2520enable%2520interactive%2520behavior.%2520The%2520resulting%2520scenes%2520are%2520compact%252C%250Aeditable%252C%2520and%2520fully%2520compatible%2520with%2520standard%2520graphics%2520pipelines%252C%2520making%2520them%250Asuitable%2520for%2520applications%2520in%2520AR/VR%252C%2520gaming%252C%2520robotics%252C%2520and%2520digital%2520twins.%2520In%250Aaddition%252C%2520LiteReality%2520introduces%2520a%2520training-free%2520object%2520retrieval%2520module%2520that%250Aachieves%2520state-of-the-art%2520similarity%2520performance%2520on%2520the%2520Scan2CAD%2520benchmark%252C%250Aalong%2520with%2520a%2520robust%2520material%2520painting%2520module%2520capable%2520of%2520transferring%250Aappearances%2520from%2520images%2520of%2520any%2520style%2520to%25203D%2520assets%2520--%2520even%2520under%2520severe%250Amisalignment%252C%2520occlusion%252C%2520and%2520poor%2520lighting.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%250ALiteReality%2520on%2520both%2520real-life%2520scans%2520and%2520public%2520datasets.%2520Project%2520page%253A%250Ahttps%253A//litereality.github.io%253B%2520Video%253A%250Ahttps%253A//www.youtube.com/watch%253Fv%253DecK9m3LXg2c%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiteReality%3A%20Graphics-Ready%203D%20Scene%20Reconstruction%20from%20RGB-D%20Scans&entry.906535625=Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Fangcheng%20Zhong%20and%20Hengshuang%20Zhao%20and%20Matthias%20Nie%C3%9Fner%20and%20Joan%20Lasenby&entry.1292438233=%20%20We%20propose%20LiteReality%2C%20a%20novel%20pipeline%20that%20converts%20RGB-D%20scans%20of%20indoor%0Aenvironments%20into%20compact%2C%20realistic%2C%20and%20interactive%203D%20virtual%20replicas.%0ALiteReality%20not%20only%20reconstructs%20scenes%20that%20visually%20resemble%20reality%20but%0Aalso%20supports%20key%20features%20essential%20for%20graphics%20pipelines%20--%20such%20as%20object%0Aindividuality%2C%20articulation%2C%20high-quality%20physically%20based%20rendering%20materials%2C%0Aand%20physically%20based%20interaction.%20At%20its%20core%2C%20LiteReality%20first%20performs%20scene%0Aunderstanding%20and%20parses%20the%20results%20into%20a%20coherent%203D%20layout%20and%20objects%20with%0Athe%20help%20of%20a%20structured%20scene%20graph.%20It%20then%20reconstructs%20the%20scene%20by%0Aretrieving%20the%20most%20visually%20similar%203D%20artist-crafted%20models%20from%20a%20curated%0Aasset%20database.%20Next%2C%20the%20Material%20Painting%20module%20enhances%20realism%20by%0Arecovering%20high-quality%2C%20spatially%20varying%20materials.%20Finally%2C%20the%0Areconstructed%20scene%20is%20integrated%20into%20a%20simulation%20engine%20with%20basic%20physical%0Aproperties%20to%20enable%20interactive%20behavior.%20The%20resulting%20scenes%20are%20compact%2C%0Aeditable%2C%20and%20fully%20compatible%20with%20standard%20graphics%20pipelines%2C%20making%20them%0Asuitable%20for%20applications%20in%20AR/VR%2C%20gaming%2C%20robotics%2C%20and%20digital%20twins.%20In%0Aaddition%2C%20LiteReality%20introduces%20a%20training-free%20object%20retrieval%20module%20that%0Aachieves%20state-of-the-art%20similarity%20performance%20on%20the%20Scan2CAD%20benchmark%2C%0Aalong%20with%20a%20robust%20material%20painting%20module%20capable%20of%20transferring%0Aappearances%20from%20images%20of%20any%20style%20to%203D%20assets%20--%20even%20under%20severe%0Amisalignment%2C%20occlusion%2C%20and%20poor%20lighting.%20We%20demonstrate%20the%20effectiveness%20of%0ALiteReality%20on%20both%20real-life%20scans%20and%20public%20datasets.%20Project%20page%3A%0Ahttps%3A//litereality.github.io%3B%20Video%3A%0Ahttps%3A//www.youtube.com/watch%3Fv%3DecK9m3LXg2c%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02861v1&entry.124074799=Read"},
{"title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale", "author": "Jiawei He and Danshi Li and Xinqiang Yu and Zekun Qi and Wenyao Zhang and Jiayi Chen and Zhaoxiang Zhang and Zhizheng Zhang and Li Yi and He Wang", "abstract": "  As large models gain traction, vision-language-action (VLA) systems are\nenabling robots to tackle increasingly complex tasks. However, limited by the\ndifficulty of data collection, progress has mainly focused on controlling\nsimple gripper end-effectors. There is little research on functional grasping\nwith large models for human-like dexterous hands. In this paper, we introduce\nDexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction\naligned with language instructions using single-view RGBD input. To accomplish\nthis, we generate a dataset of 170 million dexterous grasp poses mapped to\nsemantic parts across 174,000 objects in simulation, paired with detailed\npart-level captions. This large-scale dataset, named DexGraspNet 3.0, is used\nto train a VLM and flow-matching-based pose head capable of producing\ninstruction-aligned grasp poses for tabletop objects. To assess DexVLG's\nperformance, we create benchmarks in physics-based simulations and conduct\nreal-world experiments. Extensive testing demonstrates DexVLG's strong\nzero-shot generalization capabilities-achieving over 76% zero-shot execution\nsuccess rate and state-of-the-art part-grasp accuracy in simulation-and\nsuccessful part-aligned grasps on physical objects in real-world scenarios.\n", "link": "http://arxiv.org/abs/2507.02747v1", "date": "2025-07-03", "relevancy": 3.156, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexVLG%3A%20Dexterous%20Vision-Language-Grasp%20Model%20at%20Scale&body=Title%3A%20DexVLG%3A%20Dexterous%20Vision-Language-Grasp%20Model%20at%20Scale%0AAuthor%3A%20Jiawei%20He%20and%20Danshi%20Li%20and%20Xinqiang%20Yu%20and%20Zekun%20Qi%20and%20Wenyao%20Zhang%20and%20Jiayi%20Chen%20and%20Zhaoxiang%20Zhang%20and%20Zhizheng%20Zhang%20and%20Li%20Yi%20and%20He%20Wang%0AAbstract%3A%20%20%20As%20large%20models%20gain%20traction%2C%20vision-language-action%20%28VLA%29%20systems%20are%0Aenabling%20robots%20to%20tackle%20increasingly%20complex%20tasks.%20However%2C%20limited%20by%20the%0Adifficulty%20of%20data%20collection%2C%20progress%20has%20mainly%20focused%20on%20controlling%0Asimple%20gripper%20end-effectors.%20There%20is%20little%20research%20on%20functional%20grasping%0Awith%20large%20models%20for%20human-like%20dexterous%20hands.%20In%20this%20paper%2C%20we%20introduce%0ADexVLG%2C%20a%20large%20Vision-Language-Grasp%20model%20for%20Dexterous%20grasp%20pose%20prediction%0Aaligned%20with%20language%20instructions%20using%20single-view%20RGBD%20input.%20To%20accomplish%0Athis%2C%20we%20generate%20a%20dataset%20of%20170%20million%20dexterous%20grasp%20poses%20mapped%20to%0Asemantic%20parts%20across%20174%2C000%20objects%20in%20simulation%2C%20paired%20with%20detailed%0Apart-level%20captions.%20This%20large-scale%20dataset%2C%20named%20DexGraspNet%203.0%2C%20is%20used%0Ato%20train%20a%20VLM%20and%20flow-matching-based%20pose%20head%20capable%20of%20producing%0Ainstruction-aligned%20grasp%20poses%20for%20tabletop%20objects.%20To%20assess%20DexVLG%27s%0Aperformance%2C%20we%20create%20benchmarks%20in%20physics-based%20simulations%20and%20conduct%0Areal-world%20experiments.%20Extensive%20testing%20demonstrates%20DexVLG%27s%20strong%0Azero-shot%20generalization%20capabilities-achieving%20over%2076%25%20zero-shot%20execution%0Asuccess%20rate%20and%20state-of-the-art%20part-grasp%20accuracy%20in%20simulation-and%0Asuccessful%20part-aligned%20grasps%20on%20physical%20objects%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexVLG%253A%2520Dexterous%2520Vision-Language-Grasp%2520Model%2520at%2520Scale%26entry.906535625%3DJiawei%2520He%2520and%2520Danshi%2520Li%2520and%2520Xinqiang%2520Yu%2520and%2520Zekun%2520Qi%2520and%2520Wenyao%2520Zhang%2520and%2520Jiayi%2520Chen%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Zhizheng%2520Zhang%2520and%2520Li%2520Yi%2520and%2520He%2520Wang%26entry.1292438233%3D%2520%2520As%2520large%2520models%2520gain%2520traction%252C%2520vision-language-action%2520%2528VLA%2529%2520systems%2520are%250Aenabling%2520robots%2520to%2520tackle%2520increasingly%2520complex%2520tasks.%2520However%252C%2520limited%2520by%2520the%250Adifficulty%2520of%2520data%2520collection%252C%2520progress%2520has%2520mainly%2520focused%2520on%2520controlling%250Asimple%2520gripper%2520end-effectors.%2520There%2520is%2520little%2520research%2520on%2520functional%2520grasping%250Awith%2520large%2520models%2520for%2520human-like%2520dexterous%2520hands.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ADexVLG%252C%2520a%2520large%2520Vision-Language-Grasp%2520model%2520for%2520Dexterous%2520grasp%2520pose%2520prediction%250Aaligned%2520with%2520language%2520instructions%2520using%2520single-view%2520RGBD%2520input.%2520To%2520accomplish%250Athis%252C%2520we%2520generate%2520a%2520dataset%2520of%2520170%2520million%2520dexterous%2520grasp%2520poses%2520mapped%2520to%250Asemantic%2520parts%2520across%2520174%252C000%2520objects%2520in%2520simulation%252C%2520paired%2520with%2520detailed%250Apart-level%2520captions.%2520This%2520large-scale%2520dataset%252C%2520named%2520DexGraspNet%25203.0%252C%2520is%2520used%250Ato%2520train%2520a%2520VLM%2520and%2520flow-matching-based%2520pose%2520head%2520capable%2520of%2520producing%250Ainstruction-aligned%2520grasp%2520poses%2520for%2520tabletop%2520objects.%2520To%2520assess%2520DexVLG%2527s%250Aperformance%252C%2520we%2520create%2520benchmarks%2520in%2520physics-based%2520simulations%2520and%2520conduct%250Areal-world%2520experiments.%2520Extensive%2520testing%2520demonstrates%2520DexVLG%2527s%2520strong%250Azero-shot%2520generalization%2520capabilities-achieving%2520over%252076%2525%2520zero-shot%2520execution%250Asuccess%2520rate%2520and%2520state-of-the-art%2520part-grasp%2520accuracy%2520in%2520simulation-and%250Asuccessful%2520part-aligned%2520grasps%2520on%2520physical%2520objects%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexVLG%3A%20Dexterous%20Vision-Language-Grasp%20Model%20at%20Scale&entry.906535625=Jiawei%20He%20and%20Danshi%20Li%20and%20Xinqiang%20Yu%20and%20Zekun%20Qi%20and%20Wenyao%20Zhang%20and%20Jiayi%20Chen%20and%20Zhaoxiang%20Zhang%20and%20Zhizheng%20Zhang%20and%20Li%20Yi%20and%20He%20Wang&entry.1292438233=%20%20As%20large%20models%20gain%20traction%2C%20vision-language-action%20%28VLA%29%20systems%20are%0Aenabling%20robots%20to%20tackle%20increasingly%20complex%20tasks.%20However%2C%20limited%20by%20the%0Adifficulty%20of%20data%20collection%2C%20progress%20has%20mainly%20focused%20on%20controlling%0Asimple%20gripper%20end-effectors.%20There%20is%20little%20research%20on%20functional%20grasping%0Awith%20large%20models%20for%20human-like%20dexterous%20hands.%20In%20this%20paper%2C%20we%20introduce%0ADexVLG%2C%20a%20large%20Vision-Language-Grasp%20model%20for%20Dexterous%20grasp%20pose%20prediction%0Aaligned%20with%20language%20instructions%20using%20single-view%20RGBD%20input.%20To%20accomplish%0Athis%2C%20we%20generate%20a%20dataset%20of%20170%20million%20dexterous%20grasp%20poses%20mapped%20to%0Asemantic%20parts%20across%20174%2C000%20objects%20in%20simulation%2C%20paired%20with%20detailed%0Apart-level%20captions.%20This%20large-scale%20dataset%2C%20named%20DexGraspNet%203.0%2C%20is%20used%0Ato%20train%20a%20VLM%20and%20flow-matching-based%20pose%20head%20capable%20of%20producing%0Ainstruction-aligned%20grasp%20poses%20for%20tabletop%20objects.%20To%20assess%20DexVLG%27s%0Aperformance%2C%20we%20create%20benchmarks%20in%20physics-based%20simulations%20and%20conduct%0Areal-world%20experiments.%20Extensive%20testing%20demonstrates%20DexVLG%27s%20strong%0Azero-shot%20generalization%20capabilities-achieving%20over%2076%25%20zero-shot%20execution%0Asuccess%20rate%20and%20state-of-the-art%20part-grasp%20accuracy%20in%20simulation-and%0Asuccessful%20part-aligned%20grasps%20on%20physical%20objects%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02747v1&entry.124074799=Read"},
{"title": "Reconstructing Close Human Interaction with Appearance and Proxemics\n  Reasoning", "author": "Buzhen Huang and Chen Li and Chongyang Xu and Dongyue Lu and Jinnan Chen and Yangang Wang and Gim Hee Lee", "abstract": "  Due to visual ambiguities and inter-person occlusions, existing human pose\nestimation methods cannot recover plausible close interactions from in-the-wild\nvideos. Even state-of-the-art large foundation models~(\\eg, SAM) cannot\naccurately distinguish human semantics in such challenging scenarios. In this\nwork, we find that human appearance can provide a straightforward cue to\naddress these obstacles. Based on this observation, we propose a dual-branch\noptimization framework to reconstruct accurate interactive motions with\nplausible body contacts constrained by human appearances, social proxemics, and\nphysical laws. Specifically, we first train a diffusion model to learn the\nhuman proxemic behavior and pose prior knowledge. The trained network and two\noptimizable tensors are then incorporated into a dual-branch optimization\nframework to reconstruct human motions and appearances. Several constraints\nbased on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to\nassist the optimization. With the proxemics prior and diverse constraints, our\nmethod is capable of estimating accurate interactions from in-the-wild videos\ncaptured in complex environments. We further build a dataset with pseudo\nground-truth interaction annotations, which may promote future research on pose\nestimation and human behavior understanding. Experimental results on several\nbenchmarks demonstrate that our method outperforms existing approaches. The\ncode and data are available at https://www.buzhenhuang.com/works/CloseApp.html.\n", "link": "http://arxiv.org/abs/2507.02565v1", "date": "2025-07-03", "relevancy": 3.1304, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6775}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.609}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20Close%20Human%20Interaction%20with%20Appearance%20and%20Proxemics%0A%20%20Reasoning&body=Title%3A%20Reconstructing%20Close%20Human%20Interaction%20with%20Appearance%20and%20Proxemics%0A%20%20Reasoning%0AAuthor%3A%20Buzhen%20Huang%20and%20Chen%20Li%20and%20Chongyang%20Xu%20and%20Dongyue%20Lu%20and%20Jinnan%20Chen%20and%20Yangang%20Wang%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20Due%20to%20visual%20ambiguities%20and%20inter-person%20occlusions%2C%20existing%20human%20pose%0Aestimation%20methods%20cannot%20recover%20plausible%20close%20interactions%20from%20in-the-wild%0Avideos.%20Even%20state-of-the-art%20large%20foundation%20models~%28%5Ceg%2C%20SAM%29%20cannot%0Aaccurately%20distinguish%20human%20semantics%20in%20such%20challenging%20scenarios.%20In%20this%0Awork%2C%20we%20find%20that%20human%20appearance%20can%20provide%20a%20straightforward%20cue%20to%0Aaddress%20these%20obstacles.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20dual-branch%0Aoptimization%20framework%20to%20reconstruct%20accurate%20interactive%20motions%20with%0Aplausible%20body%20contacts%20constrained%20by%20human%20appearances%2C%20social%20proxemics%2C%20and%0Aphysical%20laws.%20Specifically%2C%20we%20first%20train%20a%20diffusion%20model%20to%20learn%20the%0Ahuman%20proxemic%20behavior%20and%20pose%20prior%20knowledge.%20The%20trained%20network%20and%20two%0Aoptimizable%20tensors%20are%20then%20incorporated%20into%20a%20dual-branch%20optimization%0Aframework%20to%20reconstruct%20human%20motions%20and%20appearances.%20Several%20constraints%0Abased%20on%203D%20Gaussians%2C%202D%20keypoints%2C%20and%20mesh%20penetrations%20are%20also%20designed%20to%0Aassist%20the%20optimization.%20With%20the%20proxemics%20prior%20and%20diverse%20constraints%2C%20our%0Amethod%20is%20capable%20of%20estimating%20accurate%20interactions%20from%20in-the-wild%20videos%0Acaptured%20in%20complex%20environments.%20We%20further%20build%20a%20dataset%20with%20pseudo%0Aground-truth%20interaction%20annotations%2C%20which%20may%20promote%20future%20research%20on%20pose%0Aestimation%20and%20human%20behavior%20understanding.%20Experimental%20results%20on%20several%0Abenchmarks%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches.%20The%0Acode%20and%20data%20are%20available%20at%20https%3A//www.buzhenhuang.com/works/CloseApp.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520Close%2520Human%2520Interaction%2520with%2520Appearance%2520and%2520Proxemics%250A%2520%2520Reasoning%26entry.906535625%3DBuzhen%2520Huang%2520and%2520Chen%2520Li%2520and%2520Chongyang%2520Xu%2520and%2520Dongyue%2520Lu%2520and%2520Jinnan%2520Chen%2520and%2520Yangang%2520Wang%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520Due%2520to%2520visual%2520ambiguities%2520and%2520inter-person%2520occlusions%252C%2520existing%2520human%2520pose%250Aestimation%2520methods%2520cannot%2520recover%2520plausible%2520close%2520interactions%2520from%2520in-the-wild%250Avideos.%2520Even%2520state-of-the-art%2520large%2520foundation%2520models~%2528%255Ceg%252C%2520SAM%2529%2520cannot%250Aaccurately%2520distinguish%2520human%2520semantics%2520in%2520such%2520challenging%2520scenarios.%2520In%2520this%250Awork%252C%2520we%2520find%2520that%2520human%2520appearance%2520can%2520provide%2520a%2520straightforward%2520cue%2520to%250Aaddress%2520these%2520obstacles.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520a%2520dual-branch%250Aoptimization%2520framework%2520to%2520reconstruct%2520accurate%2520interactive%2520motions%2520with%250Aplausible%2520body%2520contacts%2520constrained%2520by%2520human%2520appearances%252C%2520social%2520proxemics%252C%2520and%250Aphysical%2520laws.%2520Specifically%252C%2520we%2520first%2520train%2520a%2520diffusion%2520model%2520to%2520learn%2520the%250Ahuman%2520proxemic%2520behavior%2520and%2520pose%2520prior%2520knowledge.%2520The%2520trained%2520network%2520and%2520two%250Aoptimizable%2520tensors%2520are%2520then%2520incorporated%2520into%2520a%2520dual-branch%2520optimization%250Aframework%2520to%2520reconstruct%2520human%2520motions%2520and%2520appearances.%2520Several%2520constraints%250Abased%2520on%25203D%2520Gaussians%252C%25202D%2520keypoints%252C%2520and%2520mesh%2520penetrations%2520are%2520also%2520designed%2520to%250Aassist%2520the%2520optimization.%2520With%2520the%2520proxemics%2520prior%2520and%2520diverse%2520constraints%252C%2520our%250Amethod%2520is%2520capable%2520of%2520estimating%2520accurate%2520interactions%2520from%2520in-the-wild%2520videos%250Acaptured%2520in%2520complex%2520environments.%2520We%2520further%2520build%2520a%2520dataset%2520with%2520pseudo%250Aground-truth%2520interaction%2520annotations%252C%2520which%2520may%2520promote%2520future%2520research%2520on%2520pose%250Aestimation%2520and%2520human%2520behavior%2520understanding.%2520Experimental%2520results%2520on%2520several%250Abenchmarks%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches.%2520The%250Acode%2520and%2520data%2520are%2520available%2520at%2520https%253A//www.buzhenhuang.com/works/CloseApp.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20Close%20Human%20Interaction%20with%20Appearance%20and%20Proxemics%0A%20%20Reasoning&entry.906535625=Buzhen%20Huang%20and%20Chen%20Li%20and%20Chongyang%20Xu%20and%20Dongyue%20Lu%20and%20Jinnan%20Chen%20and%20Yangang%20Wang%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20Due%20to%20visual%20ambiguities%20and%20inter-person%20occlusions%2C%20existing%20human%20pose%0Aestimation%20methods%20cannot%20recover%20plausible%20close%20interactions%20from%20in-the-wild%0Avideos.%20Even%20state-of-the-art%20large%20foundation%20models~%28%5Ceg%2C%20SAM%29%20cannot%0Aaccurately%20distinguish%20human%20semantics%20in%20such%20challenging%20scenarios.%20In%20this%0Awork%2C%20we%20find%20that%20human%20appearance%20can%20provide%20a%20straightforward%20cue%20to%0Aaddress%20these%20obstacles.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20dual-branch%0Aoptimization%20framework%20to%20reconstruct%20accurate%20interactive%20motions%20with%0Aplausible%20body%20contacts%20constrained%20by%20human%20appearances%2C%20social%20proxemics%2C%20and%0Aphysical%20laws.%20Specifically%2C%20we%20first%20train%20a%20diffusion%20model%20to%20learn%20the%0Ahuman%20proxemic%20behavior%20and%20pose%20prior%20knowledge.%20The%20trained%20network%20and%20two%0Aoptimizable%20tensors%20are%20then%20incorporated%20into%20a%20dual-branch%20optimization%0Aframework%20to%20reconstruct%20human%20motions%20and%20appearances.%20Several%20constraints%0Abased%20on%203D%20Gaussians%2C%202D%20keypoints%2C%20and%20mesh%20penetrations%20are%20also%20designed%20to%0Aassist%20the%20optimization.%20With%20the%20proxemics%20prior%20and%20diverse%20constraints%2C%20our%0Amethod%20is%20capable%20of%20estimating%20accurate%20interactions%20from%20in-the-wild%20videos%0Acaptured%20in%20complex%20environments.%20We%20further%20build%20a%20dataset%20with%20pseudo%0Aground-truth%20interaction%20annotations%2C%20which%20may%20promote%20future%20research%20on%20pose%0Aestimation%20and%20human%20behavior%20understanding.%20Experimental%20results%20on%20several%0Abenchmarks%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches.%20The%0Acode%20and%20data%20are%20available%20at%20https%3A//www.buzhenhuang.com/works/CloseApp.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02565v1&entry.124074799=Read"},
{"title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond\n  Feature Alignment", "author": "Qi Xu and Dongxu Wei and Lingzhe Zhao and Wenpu Li and Zhangchi Huang and Shunping Ji and Peidong Liu", "abstract": "  Simultaneous understanding and 3D reconstruction plays an important role in\ndeveloping end-to-end embodied intelligent systems. To achieve this, recent\napproaches resort to 2D-to-3D feature alignment paradigm, which leads to\nlimited 3D understanding capability and potential semantic information loss. In\nlight of this, we propose SIU3R, the first alignment-free framework for\ngeneralizable simultaneous understanding and 3D reconstruction from unposed\nimages. Specifically, SIU3R bridges reconstruction and understanding tasks via\npixel-aligned 3D representation, and unifies multiple understanding tasks into\na set of unified learnable queries, enabling native 3D understanding without\nthe need of alignment with 2D models. To encourage collaboration between the\ntwo tasks with shared representation, we further conduct in-depth analyses of\ntheir mutual benefits, and propose two lightweight modules to facilitate their\ninteraction. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance not only on the individual tasks of 3D\nreconstruction and understanding, but also on the task of simultaneous\nunderstanding and 3D reconstruction, highlighting the advantages of our\nalignment-free framework and the effectiveness of the mutual benefit designs.\n", "link": "http://arxiv.org/abs/2507.02705v1", "date": "2025-07-03", "relevancy": 3.1275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6345}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIU3R%3A%20Simultaneous%20Scene%20Understanding%20and%203D%20Reconstruction%20Beyond%0A%20%20Feature%20Alignment&body=Title%3A%20SIU3R%3A%20Simultaneous%20Scene%20Understanding%20and%203D%20Reconstruction%20Beyond%0A%20%20Feature%20Alignment%0AAuthor%3A%20Qi%20Xu%20and%20Dongxu%20Wei%20and%20Lingzhe%20Zhao%20and%20Wenpu%20Li%20and%20Zhangchi%20Huang%20and%20Shunping%20Ji%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Simultaneous%20understanding%20and%203D%20reconstruction%20plays%20an%20important%20role%20in%0Adeveloping%20end-to-end%20embodied%20intelligent%20systems.%20To%20achieve%20this%2C%20recent%0Aapproaches%20resort%20to%202D-to-3D%20feature%20alignment%20paradigm%2C%20which%20leads%20to%0Alimited%203D%20understanding%20capability%20and%20potential%20semantic%20information%20loss.%20In%0Alight%20of%20this%2C%20we%20propose%20SIU3R%2C%20the%20first%20alignment-free%20framework%20for%0Ageneralizable%20simultaneous%20understanding%20and%203D%20reconstruction%20from%20unposed%0Aimages.%20Specifically%2C%20SIU3R%20bridges%20reconstruction%20and%20understanding%20tasks%20via%0Apixel-aligned%203D%20representation%2C%20and%20unifies%20multiple%20understanding%20tasks%20into%0Aa%20set%20of%20unified%20learnable%20queries%2C%20enabling%20native%203D%20understanding%20without%0Athe%20need%20of%20alignment%20with%202D%20models.%20To%20encourage%20collaboration%20between%20the%0Atwo%20tasks%20with%20shared%20representation%2C%20we%20further%20conduct%20in-depth%20analyses%20of%0Atheir%20mutual%20benefits%2C%20and%20propose%20two%20lightweight%20modules%20to%20facilitate%20their%0Ainteraction.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20not%20only%20on%20the%20individual%20tasks%20of%203D%0Areconstruction%20and%20understanding%2C%20but%20also%20on%20the%20task%20of%20simultaneous%0Aunderstanding%20and%203D%20reconstruction%2C%20highlighting%20the%20advantages%20of%20our%0Aalignment-free%20framework%20and%20the%20effectiveness%20of%20the%20mutual%20benefit%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIU3R%253A%2520Simultaneous%2520Scene%2520Understanding%2520and%25203D%2520Reconstruction%2520Beyond%250A%2520%2520Feature%2520Alignment%26entry.906535625%3DQi%2520Xu%2520and%2520Dongxu%2520Wei%2520and%2520Lingzhe%2520Zhao%2520and%2520Wenpu%2520Li%2520and%2520Zhangchi%2520Huang%2520and%2520Shunping%2520Ji%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Simultaneous%2520understanding%2520and%25203D%2520reconstruction%2520plays%2520an%2520important%2520role%2520in%250Adeveloping%2520end-to-end%2520embodied%2520intelligent%2520systems.%2520To%2520achieve%2520this%252C%2520recent%250Aapproaches%2520resort%2520to%25202D-to-3D%2520feature%2520alignment%2520paradigm%252C%2520which%2520leads%2520to%250Alimited%25203D%2520understanding%2520capability%2520and%2520potential%2520semantic%2520information%2520loss.%2520In%250Alight%2520of%2520this%252C%2520we%2520propose%2520SIU3R%252C%2520the%2520first%2520alignment-free%2520framework%2520for%250Ageneralizable%2520simultaneous%2520understanding%2520and%25203D%2520reconstruction%2520from%2520unposed%250Aimages.%2520Specifically%252C%2520SIU3R%2520bridges%2520reconstruction%2520and%2520understanding%2520tasks%2520via%250Apixel-aligned%25203D%2520representation%252C%2520and%2520unifies%2520multiple%2520understanding%2520tasks%2520into%250Aa%2520set%2520of%2520unified%2520learnable%2520queries%252C%2520enabling%2520native%25203D%2520understanding%2520without%250Athe%2520need%2520of%2520alignment%2520with%25202D%2520models.%2520To%2520encourage%2520collaboration%2520between%2520the%250Atwo%2520tasks%2520with%2520shared%2520representation%252C%2520we%2520further%2520conduct%2520in-depth%2520analyses%2520of%250Atheir%2520mutual%2520benefits%252C%2520and%2520propose%2520two%2520lightweight%2520modules%2520to%2520facilitate%2520their%250Ainteraction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520not%2520only%2520on%2520the%2520individual%2520tasks%2520of%25203D%250Areconstruction%2520and%2520understanding%252C%2520but%2520also%2520on%2520the%2520task%2520of%2520simultaneous%250Aunderstanding%2520and%25203D%2520reconstruction%252C%2520highlighting%2520the%2520advantages%2520of%2520our%250Aalignment-free%2520framework%2520and%2520the%2520effectiveness%2520of%2520the%2520mutual%2520benefit%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIU3R%3A%20Simultaneous%20Scene%20Understanding%20and%203D%20Reconstruction%20Beyond%0A%20%20Feature%20Alignment&entry.906535625=Qi%20Xu%20and%20Dongxu%20Wei%20and%20Lingzhe%20Zhao%20and%20Wenpu%20Li%20and%20Zhangchi%20Huang%20and%20Shunping%20Ji%20and%20Peidong%20Liu&entry.1292438233=%20%20Simultaneous%20understanding%20and%203D%20reconstruction%20plays%20an%20important%20role%20in%0Adeveloping%20end-to-end%20embodied%20intelligent%20systems.%20To%20achieve%20this%2C%20recent%0Aapproaches%20resort%20to%202D-to-3D%20feature%20alignment%20paradigm%2C%20which%20leads%20to%0Alimited%203D%20understanding%20capability%20and%20potential%20semantic%20information%20loss.%20In%0Alight%20of%20this%2C%20we%20propose%20SIU3R%2C%20the%20first%20alignment-free%20framework%20for%0Ageneralizable%20simultaneous%20understanding%20and%203D%20reconstruction%20from%20unposed%0Aimages.%20Specifically%2C%20SIU3R%20bridges%20reconstruction%20and%20understanding%20tasks%20via%0Apixel-aligned%203D%20representation%2C%20and%20unifies%20multiple%20understanding%20tasks%20into%0Aa%20set%20of%20unified%20learnable%20queries%2C%20enabling%20native%203D%20understanding%20without%0Athe%20need%20of%20alignment%20with%202D%20models.%20To%20encourage%20collaboration%20between%20the%0Atwo%20tasks%20with%20shared%20representation%2C%20we%20further%20conduct%20in-depth%20analyses%20of%0Atheir%20mutual%20benefits%2C%20and%20propose%20two%20lightweight%20modules%20to%20facilitate%20their%0Ainteraction.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20not%20only%20on%20the%20individual%20tasks%20of%203D%0Areconstruction%20and%20understanding%2C%20but%20also%20on%20the%20task%20of%20simultaneous%0Aunderstanding%20and%203D%20reconstruction%2C%20highlighting%20the%20advantages%20of%20our%0Aalignment-free%20framework%20and%20the%20effectiveness%20of%20the%20mutual%20benefit%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02705v1&entry.124074799=Read"},
{"title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in\n  Autonomous Driving", "author": "Yunshen Wang and Yicheng Liu and Tianyuan Yuan and Yingshi Liang and Xiuyu Yang and Honggang Zhang and Hang Zhao", "abstract": "  Accurately predicting 3D occupancy grids from visual inputs is critical for\nautonomous driving, but current discriminative methods struggle with noisy\ndata, incomplete observations, and the complex structures inherent in 3D\nscenes. In this work, we reframe 3D occupancy prediction as a generative\nmodeling task using diffusion models, which learn the underlying data\ndistribution and incorporate 3D scene priors. This approach enhances prediction\nconsistency, noise robustness, and better handles the intricacies of 3D spatial\nstructures. Our extensive experiments show that diffusion-based generative\nmodels outperform state-of-the-art discriminative approaches, delivering more\nrealistic and accurate occupancy predictions, especially in occluded or\nlow-visibility regions. Moreover, the improved predictions significantly\nbenefit downstream planning tasks, highlighting the practical advantages of our\nmethod for real-world autonomous driving applications.\n", "link": "http://arxiv.org/abs/2505.23115v2", "date": "2025-07-03", "relevancy": 3.0668, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6299}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6051}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Based%20Generative%20Models%20for%203D%20Occupancy%20Prediction%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Diffusion-Based%20Generative%20Models%20for%203D%20Occupancy%20Prediction%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Yunshen%20Wang%20and%20Yicheng%20Liu%20and%20Tianyuan%20Yuan%20and%20Yingshi%20Liang%20and%20Xiuyu%20Yang%20and%20Honggang%20Zhang%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20Accurately%20predicting%203D%20occupancy%20grids%20from%20visual%20inputs%20is%20critical%20for%0Aautonomous%20driving%2C%20but%20current%20discriminative%20methods%20struggle%20with%20noisy%0Adata%2C%20incomplete%20observations%2C%20and%20the%20complex%20structures%20inherent%20in%203D%0Ascenes.%20In%20this%20work%2C%20we%20reframe%203D%20occupancy%20prediction%20as%20a%20generative%0Amodeling%20task%20using%20diffusion%20models%2C%20which%20learn%20the%20underlying%20data%0Adistribution%20and%20incorporate%203D%20scene%20priors.%20This%20approach%20enhances%20prediction%0Aconsistency%2C%20noise%20robustness%2C%20and%20better%20handles%20the%20intricacies%20of%203D%20spatial%0Astructures.%20Our%20extensive%20experiments%20show%20that%20diffusion-based%20generative%0Amodels%20outperform%20state-of-the-art%20discriminative%20approaches%2C%20delivering%20more%0Arealistic%20and%20accurate%20occupancy%20predictions%2C%20especially%20in%20occluded%20or%0Alow-visibility%20regions.%20Moreover%2C%20the%20improved%20predictions%20significantly%0Abenefit%20downstream%20planning%20tasks%2C%20highlighting%20the%20practical%20advantages%20of%20our%0Amethod%20for%20real-world%20autonomous%20driving%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Based%2520Generative%2520Models%2520for%25203D%2520Occupancy%2520Prediction%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DYunshen%2520Wang%2520and%2520Yicheng%2520Liu%2520and%2520Tianyuan%2520Yuan%2520and%2520Yingshi%2520Liang%2520and%2520Xiuyu%2520Yang%2520and%2520Honggang%2520Zhang%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520Accurately%2520predicting%25203D%2520occupancy%2520grids%2520from%2520visual%2520inputs%2520is%2520critical%2520for%250Aautonomous%2520driving%252C%2520but%2520current%2520discriminative%2520methods%2520struggle%2520with%2520noisy%250Adata%252C%2520incomplete%2520observations%252C%2520and%2520the%2520complex%2520structures%2520inherent%2520in%25203D%250Ascenes.%2520In%2520this%2520work%252C%2520we%2520reframe%25203D%2520occupancy%2520prediction%2520as%2520a%2520generative%250Amodeling%2520task%2520using%2520diffusion%2520models%252C%2520which%2520learn%2520the%2520underlying%2520data%250Adistribution%2520and%2520incorporate%25203D%2520scene%2520priors.%2520This%2520approach%2520enhances%2520prediction%250Aconsistency%252C%2520noise%2520robustness%252C%2520and%2520better%2520handles%2520the%2520intricacies%2520of%25203D%2520spatial%250Astructures.%2520Our%2520extensive%2520experiments%2520show%2520that%2520diffusion-based%2520generative%250Amodels%2520outperform%2520state-of-the-art%2520discriminative%2520approaches%252C%2520delivering%2520more%250Arealistic%2520and%2520accurate%2520occupancy%2520predictions%252C%2520especially%2520in%2520occluded%2520or%250Alow-visibility%2520regions.%2520Moreover%252C%2520the%2520improved%2520predictions%2520significantly%250Abenefit%2520downstream%2520planning%2520tasks%252C%2520highlighting%2520the%2520practical%2520advantages%2520of%2520our%250Amethod%2520for%2520real-world%2520autonomous%2520driving%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Based%20Generative%20Models%20for%203D%20Occupancy%20Prediction%20in%0A%20%20Autonomous%20Driving&entry.906535625=Yunshen%20Wang%20and%20Yicheng%20Liu%20and%20Tianyuan%20Yuan%20and%20Yingshi%20Liang%20and%20Xiuyu%20Yang%20and%20Honggang%20Zhang%20and%20Hang%20Zhao&entry.1292438233=%20%20Accurately%20predicting%203D%20occupancy%20grids%20from%20visual%20inputs%20is%20critical%20for%0Aautonomous%20driving%2C%20but%20current%20discriminative%20methods%20struggle%20with%20noisy%0Adata%2C%20incomplete%20observations%2C%20and%20the%20complex%20structures%20inherent%20in%203D%0Ascenes.%20In%20this%20work%2C%20we%20reframe%203D%20occupancy%20prediction%20as%20a%20generative%0Amodeling%20task%20using%20diffusion%20models%2C%20which%20learn%20the%20underlying%20data%0Adistribution%20and%20incorporate%203D%20scene%20priors.%20This%20approach%20enhances%20prediction%0Aconsistency%2C%20noise%20robustness%2C%20and%20better%20handles%20the%20intricacies%20of%203D%20spatial%0Astructures.%20Our%20extensive%20experiments%20show%20that%20diffusion-based%20generative%0Amodels%20outperform%20state-of-the-art%20discriminative%20approaches%2C%20delivering%20more%0Arealistic%20and%20accurate%20occupancy%20predictions%2C%20especially%20in%20occluded%20or%0Alow-visibility%20regions.%20Moreover%2C%20the%20improved%20predictions%20significantly%0Abenefit%20downstream%20planning%20tasks%2C%20highlighting%20the%20practical%20advantages%20of%20our%0Amethod%20for%20real-world%20autonomous%20driving%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23115v2&entry.124074799=Read"},
{"title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer\n  Memory", "author": "Yuqi Wu and Wenzhao Zheng and Jie Zhou and Jiwen Lu", "abstract": "  Dense 3D scene reconstruction from an ordered sequence or unordered image\ncollections is a critical step when bringing research in computer vision into\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\nan image pair densely into a shared coordinate system, subsequent methods\nmaintain an implicit memory to achieve dense 3D reconstruction from more\nimages. However, such implicit memory is limited in capacity and may suffer\nfrom information loss of earlier frames. We propose Point3R, an online\nframework targeting dense streaming 3D reconstruction. To be specific, we\nmaintain an explicit spatial pointer memory directly associated with the 3D\nstructure of the current scene. Each pointer in this memory is assigned a\nspecific 3D position and aggregates scene information nearby in the global\ncoordinate system into a changing spatial feature. Information extracted from\nthe latest frame interacts explicitly with this pointer memory, enabling dense\nintegration of the current observation into the global coordinate system. We\ndesign a 3D hierarchical position embedding to promote this interaction and\ndesign a simple yet effective fusion mechanism to ensure that our pointer\nmemory is uniform and efficient. Our method achieves competitive or\nstate-of-the-art performance on various tasks with low training costs. Code is\navailable at: https://github.com/YkiWu/Point3R.\n", "link": "http://arxiv.org/abs/2507.02863v1", "date": "2025-07-03", "relevancy": 3.0569, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6162}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6129}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point3R%3A%20Streaming%203D%20Reconstruction%20with%20Explicit%20Spatial%20Pointer%0A%20%20Memory&body=Title%3A%20Point3R%3A%20Streaming%203D%20Reconstruction%20with%20Explicit%20Spatial%20Pointer%0A%20%20Memory%0AAuthor%3A%20Yuqi%20Wu%20and%20Wenzhao%20Zheng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Dense%203D%20scene%20reconstruction%20from%20an%20ordered%20sequence%20or%20unordered%20image%0Acollections%20is%20a%20critical%20step%20when%20bringing%20research%20in%20computer%20vision%20into%0Apractical%20scenarios.%20Following%20the%20paradigm%20introduced%20by%20DUSt3R%2C%20which%20unifies%0Aan%20image%20pair%20densely%20into%20a%20shared%20coordinate%20system%2C%20subsequent%20methods%0Amaintain%20an%20implicit%20memory%20to%20achieve%20dense%203D%20reconstruction%20from%20more%0Aimages.%20However%2C%20such%20implicit%20memory%20is%20limited%20in%20capacity%20and%20may%20suffer%0Afrom%20information%20loss%20of%20earlier%20frames.%20We%20propose%20Point3R%2C%20an%20online%0Aframework%20targeting%20dense%20streaming%203D%20reconstruction.%20To%20be%20specific%2C%20we%0Amaintain%20an%20explicit%20spatial%20pointer%20memory%20directly%20associated%20with%20the%203D%0Astructure%20of%20the%20current%20scene.%20Each%20pointer%20in%20this%20memory%20is%20assigned%20a%0Aspecific%203D%20position%20and%20aggregates%20scene%20information%20nearby%20in%20the%20global%0Acoordinate%20system%20into%20a%20changing%20spatial%20feature.%20Information%20extracted%20from%0Athe%20latest%20frame%20interacts%20explicitly%20with%20this%20pointer%20memory%2C%20enabling%20dense%0Aintegration%20of%20the%20current%20observation%20into%20the%20global%20coordinate%20system.%20We%0Adesign%20a%203D%20hierarchical%20position%20embedding%20to%20promote%20this%20interaction%20and%0Adesign%20a%20simple%20yet%20effective%20fusion%20mechanism%20to%20ensure%20that%20our%20pointer%0Amemory%20is%20uniform%20and%20efficient.%20Our%20method%20achieves%20competitive%20or%0Astate-of-the-art%20performance%20on%20various%20tasks%20with%20low%20training%20costs.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/YkiWu/Point3R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint3R%253A%2520Streaming%25203D%2520Reconstruction%2520with%2520Explicit%2520Spatial%2520Pointer%250A%2520%2520Memory%26entry.906535625%3DYuqi%2520Wu%2520and%2520Wenzhao%2520Zheng%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Dense%25203D%2520scene%2520reconstruction%2520from%2520an%2520ordered%2520sequence%2520or%2520unordered%2520image%250Acollections%2520is%2520a%2520critical%2520step%2520when%2520bringing%2520research%2520in%2520computer%2520vision%2520into%250Apractical%2520scenarios.%2520Following%2520the%2520paradigm%2520introduced%2520by%2520DUSt3R%252C%2520which%2520unifies%250Aan%2520image%2520pair%2520densely%2520into%2520a%2520shared%2520coordinate%2520system%252C%2520subsequent%2520methods%250Amaintain%2520an%2520implicit%2520memory%2520to%2520achieve%2520dense%25203D%2520reconstruction%2520from%2520more%250Aimages.%2520However%252C%2520such%2520implicit%2520memory%2520is%2520limited%2520in%2520capacity%2520and%2520may%2520suffer%250Afrom%2520information%2520loss%2520of%2520earlier%2520frames.%2520We%2520propose%2520Point3R%252C%2520an%2520online%250Aframework%2520targeting%2520dense%2520streaming%25203D%2520reconstruction.%2520To%2520be%2520specific%252C%2520we%250Amaintain%2520an%2520explicit%2520spatial%2520pointer%2520memory%2520directly%2520associated%2520with%2520the%25203D%250Astructure%2520of%2520the%2520current%2520scene.%2520Each%2520pointer%2520in%2520this%2520memory%2520is%2520assigned%2520a%250Aspecific%25203D%2520position%2520and%2520aggregates%2520scene%2520information%2520nearby%2520in%2520the%2520global%250Acoordinate%2520system%2520into%2520a%2520changing%2520spatial%2520feature.%2520Information%2520extracted%2520from%250Athe%2520latest%2520frame%2520interacts%2520explicitly%2520with%2520this%2520pointer%2520memory%252C%2520enabling%2520dense%250Aintegration%2520of%2520the%2520current%2520observation%2520into%2520the%2520global%2520coordinate%2520system.%2520We%250Adesign%2520a%25203D%2520hierarchical%2520position%2520embedding%2520to%2520promote%2520this%2520interaction%2520and%250Adesign%2520a%2520simple%2520yet%2520effective%2520fusion%2520mechanism%2520to%2520ensure%2520that%2520our%2520pointer%250Amemory%2520is%2520uniform%2520and%2520efficient.%2520Our%2520method%2520achieves%2520competitive%2520or%250Astate-of-the-art%2520performance%2520on%2520various%2520tasks%2520with%2520low%2520training%2520costs.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/YkiWu/Point3R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point3R%3A%20Streaming%203D%20Reconstruction%20with%20Explicit%20Spatial%20Pointer%0A%20%20Memory&entry.906535625=Yuqi%20Wu%20and%20Wenzhao%20Zheng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Dense%203D%20scene%20reconstruction%20from%20an%20ordered%20sequence%20or%20unordered%20image%0Acollections%20is%20a%20critical%20step%20when%20bringing%20research%20in%20computer%20vision%20into%0Apractical%20scenarios.%20Following%20the%20paradigm%20introduced%20by%20DUSt3R%2C%20which%20unifies%0Aan%20image%20pair%20densely%20into%20a%20shared%20coordinate%20system%2C%20subsequent%20methods%0Amaintain%20an%20implicit%20memory%20to%20achieve%20dense%203D%20reconstruction%20from%20more%0Aimages.%20However%2C%20such%20implicit%20memory%20is%20limited%20in%20capacity%20and%20may%20suffer%0Afrom%20information%20loss%20of%20earlier%20frames.%20We%20propose%20Point3R%2C%20an%20online%0Aframework%20targeting%20dense%20streaming%203D%20reconstruction.%20To%20be%20specific%2C%20we%0Amaintain%20an%20explicit%20spatial%20pointer%20memory%20directly%20associated%20with%20the%203D%0Astructure%20of%20the%20current%20scene.%20Each%20pointer%20in%20this%20memory%20is%20assigned%20a%0Aspecific%203D%20position%20and%20aggregates%20scene%20information%20nearby%20in%20the%20global%0Acoordinate%20system%20into%20a%20changing%20spatial%20feature.%20Information%20extracted%20from%0Athe%20latest%20frame%20interacts%20explicitly%20with%20this%20pointer%20memory%2C%20enabling%20dense%0Aintegration%20of%20the%20current%20observation%20into%20the%20global%20coordinate%20system.%20We%0Adesign%20a%203D%20hierarchical%20position%20embedding%20to%20promote%20this%20interaction%20and%0Adesign%20a%20simple%20yet%20effective%20fusion%20mechanism%20to%20ensure%20that%20our%20pointer%0Amemory%20is%20uniform%20and%20efficient.%20Our%20method%20achieves%20competitive%20or%0Astate-of-the-art%20performance%20on%20various%20tasks%20with%20low%20training%20costs.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/YkiWu/Point3R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02863v1&entry.124074799=Read"},
{"title": "Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges\n  Domain Generalized Semantic Segmentation", "author": "Siyu Chen and Ting Han and Changshe Zhang and Xin Luo and Meiliu Wu and Guorong Cai and Jinhe Su", "abstract": "  Vision Foundation Models (VFMs) have delivered remarkable performance in\nDomain Generalized Semantic Segmentation (DGSS). However, recent methods often\noverlook the fact that visual cues are susceptible, whereas the underlying\ngeometry remains stable, rendering depth information more robust. In this\npaper, we investigate the potential of integrating depth information with\nfeatures from VFMs, to improve the geometric consistency within an image and\nboost the generalization performance of VFMs. We propose a novel fine-tuning\nDGSS framework, named DepthForge, which integrates the visual cues from frozen\nDINOv2 or EVA02 and depth cues from frozen Depth Anything V2. In each layer of\nthe VFMs, we incorporate depth-aware learnable tokens to continuously decouple\ndomain-invariant visual and spatial information, thereby enhancing depth\nawareness and attention of the VFMs. Finally, we develop a depth refinement\ndecoder and integrate it into the model architecture to adaptively refine\nmulti-layer VFM features and depth-aware learnable tokens. Extensive\nexperiments are conducted based on various DGSS settings and five different\ndatsets as unseen target domains. The qualitative and quantitative results\ndemonstrate that our method significantly outperforms alternative approaches\nwith stronger performance, steadier visual-spatial attention, and superior\ngeneralization ability. In particular, DepthForge exhibits outstanding\nperformance under extreme conditions (e.g., night and snow). Code is available\nat https://github.com/anonymouse-xzrptkvyqc/DepthForge.\n", "link": "http://arxiv.org/abs/2504.12753v2", "date": "2025-07-03", "relevancy": 3.0498, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6184}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stronger%2C%20Steadier%20%26%20Superior%3A%20Geometric%20Consistency%20in%20Depth%20VFM%20Forges%0A%20%20Domain%20Generalized%20Semantic%20Segmentation&body=Title%3A%20Stronger%2C%20Steadier%20%26%20Superior%3A%20Geometric%20Consistency%20in%20Depth%20VFM%20Forges%0A%20%20Domain%20Generalized%20Semantic%20Segmentation%0AAuthor%3A%20Siyu%20Chen%20and%20Ting%20Han%20and%20Changshe%20Zhang%20and%20Xin%20Luo%20and%20Meiliu%20Wu%20and%20Guorong%20Cai%20and%20Jinhe%20Su%0AAbstract%3A%20%20%20Vision%20Foundation%20Models%20%28VFMs%29%20have%20delivered%20remarkable%20performance%20in%0ADomain%20Generalized%20Semantic%20Segmentation%20%28DGSS%29.%20However%2C%20recent%20methods%20often%0Aoverlook%20the%20fact%20that%20visual%20cues%20are%20susceptible%2C%20whereas%20the%20underlying%0Ageometry%20remains%20stable%2C%20rendering%20depth%20information%20more%20robust.%20In%20this%0Apaper%2C%20we%20investigate%20the%20potential%20of%20integrating%20depth%20information%20with%0Afeatures%20from%20VFMs%2C%20to%20improve%20the%20geometric%20consistency%20within%20an%20image%20and%0Aboost%20the%20generalization%20performance%20of%20VFMs.%20We%20propose%20a%20novel%20fine-tuning%0ADGSS%20framework%2C%20named%20DepthForge%2C%20which%20integrates%20the%20visual%20cues%20from%20frozen%0ADINOv2%20or%20EVA02%20and%20depth%20cues%20from%20frozen%20Depth%20Anything%20V2.%20In%20each%20layer%20of%0Athe%20VFMs%2C%20we%20incorporate%20depth-aware%20learnable%20tokens%20to%20continuously%20decouple%0Adomain-invariant%20visual%20and%20spatial%20information%2C%20thereby%20enhancing%20depth%0Aawareness%20and%20attention%20of%20the%20VFMs.%20Finally%2C%20we%20develop%20a%20depth%20refinement%0Adecoder%20and%20integrate%20it%20into%20the%20model%20architecture%20to%20adaptively%20refine%0Amulti-layer%20VFM%20features%20and%20depth-aware%20learnable%20tokens.%20Extensive%0Aexperiments%20are%20conducted%20based%20on%20various%20DGSS%20settings%20and%20five%20different%0Adatsets%20as%20unseen%20target%20domains.%20The%20qualitative%20and%20quantitative%20results%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20alternative%20approaches%0Awith%20stronger%20performance%2C%20steadier%20visual-spatial%20attention%2C%20and%20superior%0Ageneralization%20ability.%20In%20particular%2C%20DepthForge%20exhibits%20outstanding%0Aperformance%20under%20extreme%20conditions%20%28e.g.%2C%20night%20and%20snow%29.%20Code%20is%20available%0Aat%20https%3A//github.com/anonymouse-xzrptkvyqc/DepthForge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStronger%252C%2520Steadier%2520%2526%2520Superior%253A%2520Geometric%2520Consistency%2520in%2520Depth%2520VFM%2520Forges%250A%2520%2520Domain%2520Generalized%2520Semantic%2520Segmentation%26entry.906535625%3DSiyu%2520Chen%2520and%2520Ting%2520Han%2520and%2520Changshe%2520Zhang%2520and%2520Xin%2520Luo%2520and%2520Meiliu%2520Wu%2520and%2520Guorong%2520Cai%2520and%2520Jinhe%2520Su%26entry.1292438233%3D%2520%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520have%2520delivered%2520remarkable%2520performance%2520in%250ADomain%2520Generalized%2520Semantic%2520Segmentation%2520%2528DGSS%2529.%2520However%252C%2520recent%2520methods%2520often%250Aoverlook%2520the%2520fact%2520that%2520visual%2520cues%2520are%2520susceptible%252C%2520whereas%2520the%2520underlying%250Ageometry%2520remains%2520stable%252C%2520rendering%2520depth%2520information%2520more%2520robust.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520potential%2520of%2520integrating%2520depth%2520information%2520with%250Afeatures%2520from%2520VFMs%252C%2520to%2520improve%2520the%2520geometric%2520consistency%2520within%2520an%2520image%2520and%250Aboost%2520the%2520generalization%2520performance%2520of%2520VFMs.%2520We%2520propose%2520a%2520novel%2520fine-tuning%250ADGSS%2520framework%252C%2520named%2520DepthForge%252C%2520which%2520integrates%2520the%2520visual%2520cues%2520from%2520frozen%250ADINOv2%2520or%2520EVA02%2520and%2520depth%2520cues%2520from%2520frozen%2520Depth%2520Anything%2520V2.%2520In%2520each%2520layer%2520of%250Athe%2520VFMs%252C%2520we%2520incorporate%2520depth-aware%2520learnable%2520tokens%2520to%2520continuously%2520decouple%250Adomain-invariant%2520visual%2520and%2520spatial%2520information%252C%2520thereby%2520enhancing%2520depth%250Aawareness%2520and%2520attention%2520of%2520the%2520VFMs.%2520Finally%252C%2520we%2520develop%2520a%2520depth%2520refinement%250Adecoder%2520and%2520integrate%2520it%2520into%2520the%2520model%2520architecture%2520to%2520adaptively%2520refine%250Amulti-layer%2520VFM%2520features%2520and%2520depth-aware%2520learnable%2520tokens.%2520Extensive%250Aexperiments%2520are%2520conducted%2520based%2520on%2520various%2520DGSS%2520settings%2520and%2520five%2520different%250Adatsets%2520as%2520unseen%2520target%2520domains.%2520The%2520qualitative%2520and%2520quantitative%2520results%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520alternative%2520approaches%250Awith%2520stronger%2520performance%252C%2520steadier%2520visual-spatial%2520attention%252C%2520and%2520superior%250Ageneralization%2520ability.%2520In%2520particular%252C%2520DepthForge%2520exhibits%2520outstanding%250Aperformance%2520under%2520extreme%2520conditions%2520%2528e.g.%252C%2520night%2520and%2520snow%2529.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/anonymouse-xzrptkvyqc/DepthForge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stronger%2C%20Steadier%20%26%20Superior%3A%20Geometric%20Consistency%20in%20Depth%20VFM%20Forges%0A%20%20Domain%20Generalized%20Semantic%20Segmentation&entry.906535625=Siyu%20Chen%20and%20Ting%20Han%20and%20Changshe%20Zhang%20and%20Xin%20Luo%20and%20Meiliu%20Wu%20and%20Guorong%20Cai%20and%20Jinhe%20Su&entry.1292438233=%20%20Vision%20Foundation%20Models%20%28VFMs%29%20have%20delivered%20remarkable%20performance%20in%0ADomain%20Generalized%20Semantic%20Segmentation%20%28DGSS%29.%20However%2C%20recent%20methods%20often%0Aoverlook%20the%20fact%20that%20visual%20cues%20are%20susceptible%2C%20whereas%20the%20underlying%0Ageometry%20remains%20stable%2C%20rendering%20depth%20information%20more%20robust.%20In%20this%0Apaper%2C%20we%20investigate%20the%20potential%20of%20integrating%20depth%20information%20with%0Afeatures%20from%20VFMs%2C%20to%20improve%20the%20geometric%20consistency%20within%20an%20image%20and%0Aboost%20the%20generalization%20performance%20of%20VFMs.%20We%20propose%20a%20novel%20fine-tuning%0ADGSS%20framework%2C%20named%20DepthForge%2C%20which%20integrates%20the%20visual%20cues%20from%20frozen%0ADINOv2%20or%20EVA02%20and%20depth%20cues%20from%20frozen%20Depth%20Anything%20V2.%20In%20each%20layer%20of%0Athe%20VFMs%2C%20we%20incorporate%20depth-aware%20learnable%20tokens%20to%20continuously%20decouple%0Adomain-invariant%20visual%20and%20spatial%20information%2C%20thereby%20enhancing%20depth%0Aawareness%20and%20attention%20of%20the%20VFMs.%20Finally%2C%20we%20develop%20a%20depth%20refinement%0Adecoder%20and%20integrate%20it%20into%20the%20model%20architecture%20to%20adaptively%20refine%0Amulti-layer%20VFM%20features%20and%20depth-aware%20learnable%20tokens.%20Extensive%0Aexperiments%20are%20conducted%20based%20on%20various%20DGSS%20settings%20and%20five%20different%0Adatsets%20as%20unseen%20target%20domains.%20The%20qualitative%20and%20quantitative%20results%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20alternative%20approaches%0Awith%20stronger%20performance%2C%20steadier%20visual-spatial%20attention%2C%20and%20superior%0Ageneralization%20ability.%20In%20particular%2C%20DepthForge%20exhibits%20outstanding%0Aperformance%20under%20extreme%20conditions%20%28e.g.%2C%20night%20and%20snow%29.%20Code%20is%20available%0Aat%20https%3A//github.com/anonymouse-xzrptkvyqc/DepthForge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12753v2&entry.124074799=Read"},
{"title": "MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details", "author": "Ruicheng Wang and Sicheng Xu and Yue Dong and Yu Deng and Jianfeng Xiang and Zelong Lv and Guangzhong Sun and Xin Tong and Jiaolong Yang", "abstract": "  We propose MoGe-2, an advanced open-domain geometry estimation model that\nrecovers a metric scale 3D point map of a scene from a single image. Our method\nbuilds upon the recent monocular geometry estimation approach, MoGe, which\npredicts affine-invariant point maps with unknown scales. We explore effective\nstrategies to extend MoGe for metric geometry prediction without compromising\nthe relative geometry accuracy provided by the affine-invariant point\nrepresentation. Additionally, we discover that noise and errors in real data\ndiminish fine-grained detail in the predicted geometry. We address this by\ndeveloping a unified data refinement approach that filters and completes real\ndata from different sources using sharp synthetic labels, significantly\nenhancing the granularity of the reconstructed geometry while maintaining the\noverall accuracy. We train our model on a large corpus of mixed datasets and\nconducted comprehensive evaluations, demonstrating its superior performance in\nachieving accurate relative geometry, precise metric scale, and fine-grained\ndetail recovery -- capabilities that no previous methods have simultaneously\nachieved.\n", "link": "http://arxiv.org/abs/2507.02546v1", "date": "2025-07-03", "relevancy": 2.9422, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6216}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5739}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoGe-2%3A%20Accurate%20Monocular%20Geometry%20with%20Metric%20Scale%20and%20Sharp%20Details&body=Title%3A%20MoGe-2%3A%20Accurate%20Monocular%20Geometry%20with%20Metric%20Scale%20and%20Sharp%20Details%0AAuthor%3A%20Ruicheng%20Wang%20and%20Sicheng%20Xu%20and%20Yue%20Dong%20and%20Yu%20Deng%20and%20Jianfeng%20Xiang%20and%20Zelong%20Lv%20and%20Guangzhong%20Sun%20and%20Xin%20Tong%20and%20Jiaolong%20Yang%0AAbstract%3A%20%20%20We%20propose%20MoGe-2%2C%20an%20advanced%20open-domain%20geometry%20estimation%20model%20that%0Arecovers%20a%20metric%20scale%203D%20point%20map%20of%20a%20scene%20from%20a%20single%20image.%20Our%20method%0Abuilds%20upon%20the%20recent%20monocular%20geometry%20estimation%20approach%2C%20MoGe%2C%20which%0Apredicts%20affine-invariant%20point%20maps%20with%20unknown%20scales.%20We%20explore%20effective%0Astrategies%20to%20extend%20MoGe%20for%20metric%20geometry%20prediction%20without%20compromising%0Athe%20relative%20geometry%20accuracy%20provided%20by%20the%20affine-invariant%20point%0Arepresentation.%20Additionally%2C%20we%20discover%20that%20noise%20and%20errors%20in%20real%20data%0Adiminish%20fine-grained%20detail%20in%20the%20predicted%20geometry.%20We%20address%20this%20by%0Adeveloping%20a%20unified%20data%20refinement%20approach%20that%20filters%20and%20completes%20real%0Adata%20from%20different%20sources%20using%20sharp%20synthetic%20labels%2C%20significantly%0Aenhancing%20the%20granularity%20of%20the%20reconstructed%20geometry%20while%20maintaining%20the%0Aoverall%20accuracy.%20We%20train%20our%20model%20on%20a%20large%20corpus%20of%20mixed%20datasets%20and%0Aconducted%20comprehensive%20evaluations%2C%20demonstrating%20its%20superior%20performance%20in%0Aachieving%20accurate%20relative%20geometry%2C%20precise%20metric%20scale%2C%20and%20fine-grained%0Adetail%20recovery%20--%20capabilities%20that%20no%20previous%20methods%20have%20simultaneously%0Aachieved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoGe-2%253A%2520Accurate%2520Monocular%2520Geometry%2520with%2520Metric%2520Scale%2520and%2520Sharp%2520Details%26entry.906535625%3DRuicheng%2520Wang%2520and%2520Sicheng%2520Xu%2520and%2520Yue%2520Dong%2520and%2520Yu%2520Deng%2520and%2520Jianfeng%2520Xiang%2520and%2520Zelong%2520Lv%2520and%2520Guangzhong%2520Sun%2520and%2520Xin%2520Tong%2520and%2520Jiaolong%2520Yang%26entry.1292438233%3D%2520%2520We%2520propose%2520MoGe-2%252C%2520an%2520advanced%2520open-domain%2520geometry%2520estimation%2520model%2520that%250Arecovers%2520a%2520metric%2520scale%25203D%2520point%2520map%2520of%2520a%2520scene%2520from%2520a%2520single%2520image.%2520Our%2520method%250Abuilds%2520upon%2520the%2520recent%2520monocular%2520geometry%2520estimation%2520approach%252C%2520MoGe%252C%2520which%250Apredicts%2520affine-invariant%2520point%2520maps%2520with%2520unknown%2520scales.%2520We%2520explore%2520effective%250Astrategies%2520to%2520extend%2520MoGe%2520for%2520metric%2520geometry%2520prediction%2520without%2520compromising%250Athe%2520relative%2520geometry%2520accuracy%2520provided%2520by%2520the%2520affine-invariant%2520point%250Arepresentation.%2520Additionally%252C%2520we%2520discover%2520that%2520noise%2520and%2520errors%2520in%2520real%2520data%250Adiminish%2520fine-grained%2520detail%2520in%2520the%2520predicted%2520geometry.%2520We%2520address%2520this%2520by%250Adeveloping%2520a%2520unified%2520data%2520refinement%2520approach%2520that%2520filters%2520and%2520completes%2520real%250Adata%2520from%2520different%2520sources%2520using%2520sharp%2520synthetic%2520labels%252C%2520significantly%250Aenhancing%2520the%2520granularity%2520of%2520the%2520reconstructed%2520geometry%2520while%2520maintaining%2520the%250Aoverall%2520accuracy.%2520We%2520train%2520our%2520model%2520on%2520a%2520large%2520corpus%2520of%2520mixed%2520datasets%2520and%250Aconducted%2520comprehensive%2520evaluations%252C%2520demonstrating%2520its%2520superior%2520performance%2520in%250Aachieving%2520accurate%2520relative%2520geometry%252C%2520precise%2520metric%2520scale%252C%2520and%2520fine-grained%250Adetail%2520recovery%2520--%2520capabilities%2520that%2520no%2520previous%2520methods%2520have%2520simultaneously%250Aachieved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoGe-2%3A%20Accurate%20Monocular%20Geometry%20with%20Metric%20Scale%20and%20Sharp%20Details&entry.906535625=Ruicheng%20Wang%20and%20Sicheng%20Xu%20and%20Yue%20Dong%20and%20Yu%20Deng%20and%20Jianfeng%20Xiang%20and%20Zelong%20Lv%20and%20Guangzhong%20Sun%20and%20Xin%20Tong%20and%20Jiaolong%20Yang&entry.1292438233=%20%20We%20propose%20MoGe-2%2C%20an%20advanced%20open-domain%20geometry%20estimation%20model%20that%0Arecovers%20a%20metric%20scale%203D%20point%20map%20of%20a%20scene%20from%20a%20single%20image.%20Our%20method%0Abuilds%20upon%20the%20recent%20monocular%20geometry%20estimation%20approach%2C%20MoGe%2C%20which%0Apredicts%20affine-invariant%20point%20maps%20with%20unknown%20scales.%20We%20explore%20effective%0Astrategies%20to%20extend%20MoGe%20for%20metric%20geometry%20prediction%20without%20compromising%0Athe%20relative%20geometry%20accuracy%20provided%20by%20the%20affine-invariant%20point%0Arepresentation.%20Additionally%2C%20we%20discover%20that%20noise%20and%20errors%20in%20real%20data%0Adiminish%20fine-grained%20detail%20in%20the%20predicted%20geometry.%20We%20address%20this%20by%0Adeveloping%20a%20unified%20data%20refinement%20approach%20that%20filters%20and%20completes%20real%0Adata%20from%20different%20sources%20using%20sharp%20synthetic%20labels%2C%20significantly%0Aenhancing%20the%20granularity%20of%20the%20reconstructed%20geometry%20while%20maintaining%20the%0Aoverall%20accuracy.%20We%20train%20our%20model%20on%20a%20large%20corpus%20of%20mixed%20datasets%20and%0Aconducted%20comprehensive%20evaluations%2C%20demonstrating%20its%20superior%20performance%20in%0Aachieving%20accurate%20relative%20geometry%2C%20precise%20metric%20scale%2C%20and%20fine-grained%0Adetail%20recovery%20--%20capabilities%20that%20no%20previous%20methods%20have%20simultaneously%0Aachieved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02546v1&entry.124074799=Read"},
{"title": "Aerial Vision-and-Language Navigation via Semantic-Topo-Metric\n  Representation Guided LLM Reasoning", "author": "Yunpeng Gao and Zhigang Wang and Linglin Jing and Dong Wang and Xuelong Li and Bin Zhao", "abstract": "  Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned\nAerial Vehicles (UAVs) to navigate in outdoor environments through natural\nlanguage instructions and visual cues. It remains challenging due to the\ncomplex spatial relationships in outdoor aerial scenes. In this paper, we\npropose an end-to-end zero-shot framework for aerial VLN tasks, where the large\nlanguage model (LLM) is introduced as our agent for action prediction.\nSpecifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to\nenhance the spatial reasoning ability of LLMs. This is achieved by extracting\nand projecting instruction-related semantic masks of landmarks into a top-down\nmap that contains the location information of surrounding landmarks. Further,\nthis map is transformed into a matrix representation with distance metrics as\nthe text prompt to the LLM, for action prediction according to the instruction.\nExperiments conducted in real and simulation environments have successfully\nproved the effectiveness and robustness of our method, achieving 15.9% and\n12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S\ndataset.\n", "link": "http://arxiv.org/abs/2410.08500v2", "date": "2025-07-03", "relevancy": 2.9385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aerial%20Vision-and-Language%20Navigation%20via%20Semantic-Topo-Metric%0A%20%20Representation%20Guided%20LLM%20Reasoning&body=Title%3A%20Aerial%20Vision-and-Language%20Navigation%20via%20Semantic-Topo-Metric%0A%20%20Representation%20Guided%20LLM%20Reasoning%0AAuthor%3A%20Yunpeng%20Gao%20and%20Zhigang%20Wang%20and%20Linglin%20Jing%20and%20Dong%20Wang%20and%20Xuelong%20Li%20and%20Bin%20Zhao%0AAbstract%3A%20%20%20Aerial%20Vision-and-Language%20Navigation%20%28VLN%29%20is%20a%20novel%20task%20enabling%20Unmanned%0AAerial%20Vehicles%20%28UAVs%29%20to%20navigate%20in%20outdoor%20environments%20through%20natural%0Alanguage%20instructions%20and%20visual%20cues.%20It%20remains%20challenging%20due%20to%20the%0Acomplex%20spatial%20relationships%20in%20outdoor%20aerial%20scenes.%20In%20this%20paper%2C%20we%0Apropose%20an%20end-to-end%20zero-shot%20framework%20for%20aerial%20VLN%20tasks%2C%20where%20the%20large%0Alanguage%20model%20%28LLM%29%20is%20introduced%20as%20our%20agent%20for%20action%20prediction.%0ASpecifically%2C%20we%20develop%20a%20novel%20Semantic-Topo-Metric%20Representation%20%28STMR%29%20to%0Aenhance%20the%20spatial%20reasoning%20ability%20of%20LLMs.%20This%20is%20achieved%20by%20extracting%0Aand%20projecting%20instruction-related%20semantic%20masks%20of%20landmarks%20into%20a%20top-down%0Amap%20that%20contains%20the%20location%20information%20of%20surrounding%20landmarks.%20Further%2C%0Athis%20map%20is%20transformed%20into%20a%20matrix%20representation%20with%20distance%20metrics%20as%0Athe%20text%20prompt%20to%20the%20LLM%2C%20for%20action%20prediction%20according%20to%20the%20instruction.%0AExperiments%20conducted%20in%20real%20and%20simulation%20environments%20have%20successfully%0Aproved%20the%20effectiveness%20and%20robustness%20of%20our%20method%2C%20achieving%2015.9%25%20and%0A12.5%25%20improvements%20%28absolute%29%20in%20Oracle%20Success%20Rate%20%28OSR%29%20on%20AerialVLN-S%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerial%2520Vision-and-Language%2520Navigation%2520via%2520Semantic-Topo-Metric%250A%2520%2520Representation%2520Guided%2520LLM%2520Reasoning%26entry.906535625%3DYunpeng%2520Gao%2520and%2520Zhigang%2520Wang%2520and%2520Linglin%2520Jing%2520and%2520Dong%2520Wang%2520and%2520Xuelong%2520Li%2520and%2520Bin%2520Zhao%26entry.1292438233%3D%2520%2520Aerial%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520is%2520a%2520novel%2520task%2520enabling%2520Unmanned%250AAerial%2520Vehicles%2520%2528UAVs%2529%2520to%2520navigate%2520in%2520outdoor%2520environments%2520through%2520natural%250Alanguage%2520instructions%2520and%2520visual%2520cues.%2520It%2520remains%2520challenging%2520due%2520to%2520the%250Acomplex%2520spatial%2520relationships%2520in%2520outdoor%2520aerial%2520scenes.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520an%2520end-to-end%2520zero-shot%2520framework%2520for%2520aerial%2520VLN%2520tasks%252C%2520where%2520the%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520is%2520introduced%2520as%2520our%2520agent%2520for%2520action%2520prediction.%250ASpecifically%252C%2520we%2520develop%2520a%2520novel%2520Semantic-Topo-Metric%2520Representation%2520%2528STMR%2529%2520to%250Aenhance%2520the%2520spatial%2520reasoning%2520ability%2520of%2520LLMs.%2520This%2520is%2520achieved%2520by%2520extracting%250Aand%2520projecting%2520instruction-related%2520semantic%2520masks%2520of%2520landmarks%2520into%2520a%2520top-down%250Amap%2520that%2520contains%2520the%2520location%2520information%2520of%2520surrounding%2520landmarks.%2520Further%252C%250Athis%2520map%2520is%2520transformed%2520into%2520a%2520matrix%2520representation%2520with%2520distance%2520metrics%2520as%250Athe%2520text%2520prompt%2520to%2520the%2520LLM%252C%2520for%2520action%2520prediction%2520according%2520to%2520the%2520instruction.%250AExperiments%2520conducted%2520in%2520real%2520and%2520simulation%2520environments%2520have%2520successfully%250Aproved%2520the%2520effectiveness%2520and%2520robustness%2520of%2520our%2520method%252C%2520achieving%252015.9%2525%2520and%250A12.5%2525%2520improvements%2520%2528absolute%2529%2520in%2520Oracle%2520Success%2520Rate%2520%2528OSR%2529%2520on%2520AerialVLN-S%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aerial%20Vision-and-Language%20Navigation%20via%20Semantic-Topo-Metric%0A%20%20Representation%20Guided%20LLM%20Reasoning&entry.906535625=Yunpeng%20Gao%20and%20Zhigang%20Wang%20and%20Linglin%20Jing%20and%20Dong%20Wang%20and%20Xuelong%20Li%20and%20Bin%20Zhao&entry.1292438233=%20%20Aerial%20Vision-and-Language%20Navigation%20%28VLN%29%20is%20a%20novel%20task%20enabling%20Unmanned%0AAerial%20Vehicles%20%28UAVs%29%20to%20navigate%20in%20outdoor%20environments%20through%20natural%0Alanguage%20instructions%20and%20visual%20cues.%20It%20remains%20challenging%20due%20to%20the%0Acomplex%20spatial%20relationships%20in%20outdoor%20aerial%20scenes.%20In%20this%20paper%2C%20we%0Apropose%20an%20end-to-end%20zero-shot%20framework%20for%20aerial%20VLN%20tasks%2C%20where%20the%20large%0Alanguage%20model%20%28LLM%29%20is%20introduced%20as%20our%20agent%20for%20action%20prediction.%0ASpecifically%2C%20we%20develop%20a%20novel%20Semantic-Topo-Metric%20Representation%20%28STMR%29%20to%0Aenhance%20the%20spatial%20reasoning%20ability%20of%20LLMs.%20This%20is%20achieved%20by%20extracting%0Aand%20projecting%20instruction-related%20semantic%20masks%20of%20landmarks%20into%20a%20top-down%0Amap%20that%20contains%20the%20location%20information%20of%20surrounding%20landmarks.%20Further%2C%0Athis%20map%20is%20transformed%20into%20a%20matrix%20representation%20with%20distance%20metrics%20as%0Athe%20text%20prompt%20to%20the%20LLM%2C%20for%20action%20prediction%20according%20to%20the%20instruction.%0AExperiments%20conducted%20in%20real%20and%20simulation%20environments%20have%20successfully%0Aproved%20the%20effectiveness%20and%20robustness%20of%20our%20method%2C%20achieving%2015.9%25%20and%0A12.5%25%20improvements%20%28absolute%29%20in%20Oracle%20Success%20Rate%20%28OSR%29%20on%20AerialVLN-S%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08500v2&entry.124074799=Read"},
{"title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers", "author": "Zhaochen Su and Peng Xia and Hangyu Guo and Zhenhua Liu and Yan Ma and Xiaoye Qu and Jiaqi Liu and Yanshu Li and Kaide Zeng and Zhengyuan Yang and Linjie Li and Yu Cheng and Heng Ji and Junxian He and Yi R. Fung", "abstract": "  Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.\n", "link": "http://arxiv.org/abs/2506.23918v3", "date": "2025-07-03", "relevancy": 2.9132, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20with%20Images%20for%20Multimodal%20Reasoning%3A%20Foundations%2C%20Methods%2C%20and%0A%20%20Future%20Frontiers&body=Title%3A%20Thinking%20with%20Images%20for%20Multimodal%20Reasoning%3A%20Foundations%2C%20Methods%2C%20and%0A%20%20Future%20Frontiers%0AAuthor%3A%20Zhaochen%20Su%20and%20Peng%20Xia%20and%20Hangyu%20Guo%20and%20Zhenhua%20Liu%20and%20Yan%20Ma%20and%20Xiaoye%20Qu%20and%20Jiaqi%20Liu%20and%20Yanshu%20Li%20and%20Kaide%20Zeng%20and%20Zhengyuan%20Yang%20and%20Linjie%20Li%20and%20Yu%20Cheng%20and%20Heng%20Ji%20and%20Junxian%20He%20and%20Yi%20R.%20Fung%0AAbstract%3A%20%20%20Recent%20progress%20in%20multimodal%20reasoning%20has%20been%20significantly%20advanced%20by%0Atextual%20Chain-of-Thought%20%28CoT%29%2C%20a%20paradigm%20where%20models%20conduct%20reasoning%0Awithin%20language.%20This%20text-centric%20approach%2C%20however%2C%20treats%20vision%20as%20a%0Astatic%2C%20initial%20context%2C%20creating%20a%20fundamental%20%22semantic%20gap%22%20between%20rich%0Aperceptual%20data%20and%20discrete%20symbolic%20thought.%20Human%20cognition%20often%20transcends%0Alanguage%2C%20utilizing%20vision%20as%20a%20dynamic%20mental%20sketchpad.%20A%20similar%20evolution%0Ais%20now%20unfolding%20in%20AI%2C%20marking%20a%20fundamental%20paradigm%20shift%20from%20models%20that%0Amerely%20think%20about%20images%20to%20those%20that%20can%20truly%20think%20with%20images.%20This%0Aemerging%20paradigm%20is%20characterized%20by%20models%20leveraging%20visual%20information%20as%0Aintermediate%20steps%20in%20their%20thought%20process%2C%20transforming%20vision%20from%20a%20passive%0Ainput%20into%20a%20dynamic%2C%20manipulable%20cognitive%20workspace.%20In%20this%20survey%2C%20we%20chart%0Athis%20evolution%20of%20intelligence%20along%20a%20trajectory%20of%20increasing%20cognitive%0Aautonomy%2C%20which%20unfolds%20across%20three%20key%20stages%3A%20from%20external%20tool%0Aexploration%2C%20through%20programmatic%20manipulation%2C%20to%20intrinsic%20imagination.%20To%0Astructure%20this%20rapidly%20evolving%20field%2C%20our%20survey%20makes%20four%20key%20contributions.%0A%281%29%20We%20establish%20the%20foundational%20principles%20of%20the%20think%20with%20image%20paradigm%0Aand%20its%20three-stage%20framework.%20%282%29%20We%20provide%20a%20comprehensive%20review%20of%20the%0Acore%20methods%20that%20characterize%20each%20stage%20of%20this%20roadmap.%20%283%29%20We%20analyze%20the%0Acritical%20landscape%20of%20evaluation%20benchmarks%20and%20transformative%20applications.%0A%284%29%20We%20identify%20significant%20challenges%20and%20outline%20promising%20future%20directions.%0ABy%20providing%20this%20structured%20overview%2C%20we%20aim%20to%20offer%20a%20clear%20roadmap%20for%0Afuture%20research%20towards%20more%20powerful%20and%20human-aligned%20multimodal%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23918v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520with%2520Images%2520for%2520Multimodal%2520Reasoning%253A%2520Foundations%252C%2520Methods%252C%2520and%250A%2520%2520Future%2520Frontiers%26entry.906535625%3DZhaochen%2520Su%2520and%2520Peng%2520Xia%2520and%2520Hangyu%2520Guo%2520and%2520Zhenhua%2520Liu%2520and%2520Yan%2520Ma%2520and%2520Xiaoye%2520Qu%2520and%2520Jiaqi%2520Liu%2520and%2520Yanshu%2520Li%2520and%2520Kaide%2520Zeng%2520and%2520Zhengyuan%2520Yang%2520and%2520Linjie%2520Li%2520and%2520Yu%2520Cheng%2520and%2520Heng%2520Ji%2520and%2520Junxian%2520He%2520and%2520Yi%2520R.%2520Fung%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520multimodal%2520reasoning%2520has%2520been%2520significantly%2520advanced%2520by%250Atextual%2520Chain-of-Thought%2520%2528CoT%2529%252C%2520a%2520paradigm%2520where%2520models%2520conduct%2520reasoning%250Awithin%2520language.%2520This%2520text-centric%2520approach%252C%2520however%252C%2520treats%2520vision%2520as%2520a%250Astatic%252C%2520initial%2520context%252C%2520creating%2520a%2520fundamental%2520%2522semantic%2520gap%2522%2520between%2520rich%250Aperceptual%2520data%2520and%2520discrete%2520symbolic%2520thought.%2520Human%2520cognition%2520often%2520transcends%250Alanguage%252C%2520utilizing%2520vision%2520as%2520a%2520dynamic%2520mental%2520sketchpad.%2520A%2520similar%2520evolution%250Ais%2520now%2520unfolding%2520in%2520AI%252C%2520marking%2520a%2520fundamental%2520paradigm%2520shift%2520from%2520models%2520that%250Amerely%2520think%2520about%2520images%2520to%2520those%2520that%2520can%2520truly%2520think%2520with%2520images.%2520This%250Aemerging%2520paradigm%2520is%2520characterized%2520by%2520models%2520leveraging%2520visual%2520information%2520as%250Aintermediate%2520steps%2520in%2520their%2520thought%2520process%252C%2520transforming%2520vision%2520from%2520a%2520passive%250Ainput%2520into%2520a%2520dynamic%252C%2520manipulable%2520cognitive%2520workspace.%2520In%2520this%2520survey%252C%2520we%2520chart%250Athis%2520evolution%2520of%2520intelligence%2520along%2520a%2520trajectory%2520of%2520increasing%2520cognitive%250Aautonomy%252C%2520which%2520unfolds%2520across%2520three%2520key%2520stages%253A%2520from%2520external%2520tool%250Aexploration%252C%2520through%2520programmatic%2520manipulation%252C%2520to%2520intrinsic%2520imagination.%2520To%250Astructure%2520this%2520rapidly%2520evolving%2520field%252C%2520our%2520survey%2520makes%2520four%2520key%2520contributions.%250A%25281%2529%2520We%2520establish%2520the%2520foundational%2520principles%2520of%2520the%2520think%2520with%2520image%2520paradigm%250Aand%2520its%2520three-stage%2520framework.%2520%25282%2529%2520We%2520provide%2520a%2520comprehensive%2520review%2520of%2520the%250Acore%2520methods%2520that%2520characterize%2520each%2520stage%2520of%2520this%2520roadmap.%2520%25283%2529%2520We%2520analyze%2520the%250Acritical%2520landscape%2520of%2520evaluation%2520benchmarks%2520and%2520transformative%2520applications.%250A%25284%2529%2520We%2520identify%2520significant%2520challenges%2520and%2520outline%2520promising%2520future%2520directions.%250ABy%2520providing%2520this%2520structured%2520overview%252C%2520we%2520aim%2520to%2520offer%2520a%2520clear%2520roadmap%2520for%250Afuture%2520research%2520towards%2520more%2520powerful%2520and%2520human-aligned%2520multimodal%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23918v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20with%20Images%20for%20Multimodal%20Reasoning%3A%20Foundations%2C%20Methods%2C%20and%0A%20%20Future%20Frontiers&entry.906535625=Zhaochen%20Su%20and%20Peng%20Xia%20and%20Hangyu%20Guo%20and%20Zhenhua%20Liu%20and%20Yan%20Ma%20and%20Xiaoye%20Qu%20and%20Jiaqi%20Liu%20and%20Yanshu%20Li%20and%20Kaide%20Zeng%20and%20Zhengyuan%20Yang%20and%20Linjie%20Li%20and%20Yu%20Cheng%20and%20Heng%20Ji%20and%20Junxian%20He%20and%20Yi%20R.%20Fung&entry.1292438233=%20%20Recent%20progress%20in%20multimodal%20reasoning%20has%20been%20significantly%20advanced%20by%0Atextual%20Chain-of-Thought%20%28CoT%29%2C%20a%20paradigm%20where%20models%20conduct%20reasoning%0Awithin%20language.%20This%20text-centric%20approach%2C%20however%2C%20treats%20vision%20as%20a%0Astatic%2C%20initial%20context%2C%20creating%20a%20fundamental%20%22semantic%20gap%22%20between%20rich%0Aperceptual%20data%20and%20discrete%20symbolic%20thought.%20Human%20cognition%20often%20transcends%0Alanguage%2C%20utilizing%20vision%20as%20a%20dynamic%20mental%20sketchpad.%20A%20similar%20evolution%0Ais%20now%20unfolding%20in%20AI%2C%20marking%20a%20fundamental%20paradigm%20shift%20from%20models%20that%0Amerely%20think%20about%20images%20to%20those%20that%20can%20truly%20think%20with%20images.%20This%0Aemerging%20paradigm%20is%20characterized%20by%20models%20leveraging%20visual%20information%20as%0Aintermediate%20steps%20in%20their%20thought%20process%2C%20transforming%20vision%20from%20a%20passive%0Ainput%20into%20a%20dynamic%2C%20manipulable%20cognitive%20workspace.%20In%20this%20survey%2C%20we%20chart%0Athis%20evolution%20of%20intelligence%20along%20a%20trajectory%20of%20increasing%20cognitive%0Aautonomy%2C%20which%20unfolds%20across%20three%20key%20stages%3A%20from%20external%20tool%0Aexploration%2C%20through%20programmatic%20manipulation%2C%20to%20intrinsic%20imagination.%20To%0Astructure%20this%20rapidly%20evolving%20field%2C%20our%20survey%20makes%20four%20key%20contributions.%0A%281%29%20We%20establish%20the%20foundational%20principles%20of%20the%20think%20with%20image%20paradigm%0Aand%20its%20three-stage%20framework.%20%282%29%20We%20provide%20a%20comprehensive%20review%20of%20the%0Acore%20methods%20that%20characterize%20each%20stage%20of%20this%20roadmap.%20%283%29%20We%20analyze%20the%0Acritical%20landscape%20of%20evaluation%20benchmarks%20and%20transformative%20applications.%0A%284%29%20We%20identify%20significant%20challenges%20and%20outline%20promising%20future%20directions.%0ABy%20providing%20this%20structured%20overview%2C%20we%20aim%20to%20offer%20a%20clear%20roadmap%20for%0Afuture%20research%20towards%20more%20powerful%20and%20human-aligned%20multimodal%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23918v3&entry.124074799=Read"},
{"title": "Continual Multiple Instance Learning with Enhanced Localization for\n  Histopathological Whole Slide Image Analysis", "author": "Byung Hyun Lee and Wongi Jeong and Woojae Han and Kyoungbun Lee and Se Young Chun", "abstract": "  Multiple instance learning (MIL) significantly reduced annotation costs via\nbag-level weak labels for large-scale images, such as histopathological whole\nslide images (WSIs). However, its adaptability to continual tasks with minimal\nforgetting has been rarely explored, especially on instance classification for\nlocalization. Weakly incremental learning for semantic segmentation has been\nstudied for continual localization, but it focused on natural images,\nleveraging global relationships among hundreds of small patches (e.g., $16\n\\times 16$) using pre-trained models. This approach seems infeasible for MIL\nlocalization due to enormous amounts ($\\sim 10^5$) of large patches (e.g., $256\n\\times 256$) and no available global relationships such as cancer cells. To\naddress these challenges, we propose Continual Multiple Instance Learning with\nEnhanced Localization (CoMEL), an MIL framework for both localization and\nadaptability with minimal forgetting. CoMEL consists of (1) Grouped Double\nAttention Transformer (GDAT) for efficient instance encoding, (2) Bag\nPrototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling,\nand (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting\nin both bag and instance classification. Extensive experiments on three public\nWSI datasets demonstrate superior performance of CoMEL, outperforming the prior\narts by up to $11.00\\%$ in bag-level accuracy and up to $23.4\\%$ in\nlocalization accuracy under the continual MIL setup.\n", "link": "http://arxiv.org/abs/2507.02395v1", "date": "2025-07-03", "relevancy": 2.8582, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5934}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Multiple%20Instance%20Learning%20with%20Enhanced%20Localization%20for%0A%20%20Histopathological%20Whole%20Slide%20Image%20Analysis&body=Title%3A%20Continual%20Multiple%20Instance%20Learning%20with%20Enhanced%20Localization%20for%0A%20%20Histopathological%20Whole%20Slide%20Image%20Analysis%0AAuthor%3A%20Byung%20Hyun%20Lee%20and%20Wongi%20Jeong%20and%20Woojae%20Han%20and%20Kyoungbun%20Lee%20and%20Se%20Young%20Chun%0AAbstract%3A%20%20%20Multiple%20instance%20learning%20%28MIL%29%20significantly%20reduced%20annotation%20costs%20via%0Abag-level%20weak%20labels%20for%20large-scale%20images%2C%20such%20as%20histopathological%20whole%0Aslide%20images%20%28WSIs%29.%20However%2C%20its%20adaptability%20to%20continual%20tasks%20with%20minimal%0Aforgetting%20has%20been%20rarely%20explored%2C%20especially%20on%20instance%20classification%20for%0Alocalization.%20Weakly%20incremental%20learning%20for%20semantic%20segmentation%20has%20been%0Astudied%20for%20continual%20localization%2C%20but%20it%20focused%20on%20natural%20images%2C%0Aleveraging%20global%20relationships%20among%20hundreds%20of%20small%20patches%20%28e.g.%2C%20%2416%0A%5Ctimes%2016%24%29%20using%20pre-trained%20models.%20This%20approach%20seems%20infeasible%20for%20MIL%0Alocalization%20due%20to%20enormous%20amounts%20%28%24%5Csim%2010%5E5%24%29%20of%20large%20patches%20%28e.g.%2C%20%24256%0A%5Ctimes%20256%24%29%20and%20no%20available%20global%20relationships%20such%20as%20cancer%20cells.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Continual%20Multiple%20Instance%20Learning%20with%0AEnhanced%20Localization%20%28CoMEL%29%2C%20an%20MIL%20framework%20for%20both%20localization%20and%0Aadaptability%20with%20minimal%20forgetting.%20CoMEL%20consists%20of%20%281%29%20Grouped%20Double%0AAttention%20Transformer%20%28GDAT%29%20for%20efficient%20instance%20encoding%2C%20%282%29%20Bag%0APrototypes-based%20Pseudo-Labeling%20%28BPPL%29%20for%20reliable%20instance%20pseudo-labeling%2C%0Aand%20%283%29%20Orthogonal%20Weighted%20Low-Rank%20Adaptation%20%28OWLoRA%29%20to%20mitigate%20forgetting%0Ain%20both%20bag%20and%20instance%20classification.%20Extensive%20experiments%20on%20three%20public%0AWSI%20datasets%20demonstrate%20superior%20performance%20of%20CoMEL%2C%20outperforming%20the%20prior%0Aarts%20by%20up%20to%20%2411.00%5C%25%24%20in%20bag-level%20accuracy%20and%20up%20to%20%2423.4%5C%25%24%20in%0Alocalization%20accuracy%20under%20the%20continual%20MIL%20setup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Multiple%2520Instance%2520Learning%2520with%2520Enhanced%2520Localization%2520for%250A%2520%2520Histopathological%2520Whole%2520Slide%2520Image%2520Analysis%26entry.906535625%3DByung%2520Hyun%2520Lee%2520and%2520Wongi%2520Jeong%2520and%2520Woojae%2520Han%2520and%2520Kyoungbun%2520Lee%2520and%2520Se%2520Young%2520Chun%26entry.1292438233%3D%2520%2520Multiple%2520instance%2520learning%2520%2528MIL%2529%2520significantly%2520reduced%2520annotation%2520costs%2520via%250Abag-level%2520weak%2520labels%2520for%2520large-scale%2520images%252C%2520such%2520as%2520histopathological%2520whole%250Aslide%2520images%2520%2528WSIs%2529.%2520However%252C%2520its%2520adaptability%2520to%2520continual%2520tasks%2520with%2520minimal%250Aforgetting%2520has%2520been%2520rarely%2520explored%252C%2520especially%2520on%2520instance%2520classification%2520for%250Alocalization.%2520Weakly%2520incremental%2520learning%2520for%2520semantic%2520segmentation%2520has%2520been%250Astudied%2520for%2520continual%2520localization%252C%2520but%2520it%2520focused%2520on%2520natural%2520images%252C%250Aleveraging%2520global%2520relationships%2520among%2520hundreds%2520of%2520small%2520patches%2520%2528e.g.%252C%2520%252416%250A%255Ctimes%252016%2524%2529%2520using%2520pre-trained%2520models.%2520This%2520approach%2520seems%2520infeasible%2520for%2520MIL%250Alocalization%2520due%2520to%2520enormous%2520amounts%2520%2528%2524%255Csim%252010%255E5%2524%2529%2520of%2520large%2520patches%2520%2528e.g.%252C%2520%2524256%250A%255Ctimes%2520256%2524%2529%2520and%2520no%2520available%2520global%2520relationships%2520such%2520as%2520cancer%2520cells.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520Continual%2520Multiple%2520Instance%2520Learning%2520with%250AEnhanced%2520Localization%2520%2528CoMEL%2529%252C%2520an%2520MIL%2520framework%2520for%2520both%2520localization%2520and%250Aadaptability%2520with%2520minimal%2520forgetting.%2520CoMEL%2520consists%2520of%2520%25281%2529%2520Grouped%2520Double%250AAttention%2520Transformer%2520%2528GDAT%2529%2520for%2520efficient%2520instance%2520encoding%252C%2520%25282%2529%2520Bag%250APrototypes-based%2520Pseudo-Labeling%2520%2528BPPL%2529%2520for%2520reliable%2520instance%2520pseudo-labeling%252C%250Aand%2520%25283%2529%2520Orthogonal%2520Weighted%2520Low-Rank%2520Adaptation%2520%2528OWLoRA%2529%2520to%2520mitigate%2520forgetting%250Ain%2520both%2520bag%2520and%2520instance%2520classification.%2520Extensive%2520experiments%2520on%2520three%2520public%250AWSI%2520datasets%2520demonstrate%2520superior%2520performance%2520of%2520CoMEL%252C%2520outperforming%2520the%2520prior%250Aarts%2520by%2520up%2520to%2520%252411.00%255C%2525%2524%2520in%2520bag-level%2520accuracy%2520and%2520up%2520to%2520%252423.4%255C%2525%2524%2520in%250Alocalization%2520accuracy%2520under%2520the%2520continual%2520MIL%2520setup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Multiple%20Instance%20Learning%20with%20Enhanced%20Localization%20for%0A%20%20Histopathological%20Whole%20Slide%20Image%20Analysis&entry.906535625=Byung%20Hyun%20Lee%20and%20Wongi%20Jeong%20and%20Woojae%20Han%20and%20Kyoungbun%20Lee%20and%20Se%20Young%20Chun&entry.1292438233=%20%20Multiple%20instance%20learning%20%28MIL%29%20significantly%20reduced%20annotation%20costs%20via%0Abag-level%20weak%20labels%20for%20large-scale%20images%2C%20such%20as%20histopathological%20whole%0Aslide%20images%20%28WSIs%29.%20However%2C%20its%20adaptability%20to%20continual%20tasks%20with%20minimal%0Aforgetting%20has%20been%20rarely%20explored%2C%20especially%20on%20instance%20classification%20for%0Alocalization.%20Weakly%20incremental%20learning%20for%20semantic%20segmentation%20has%20been%0Astudied%20for%20continual%20localization%2C%20but%20it%20focused%20on%20natural%20images%2C%0Aleveraging%20global%20relationships%20among%20hundreds%20of%20small%20patches%20%28e.g.%2C%20%2416%0A%5Ctimes%2016%24%29%20using%20pre-trained%20models.%20This%20approach%20seems%20infeasible%20for%20MIL%0Alocalization%20due%20to%20enormous%20amounts%20%28%24%5Csim%2010%5E5%24%29%20of%20large%20patches%20%28e.g.%2C%20%24256%0A%5Ctimes%20256%24%29%20and%20no%20available%20global%20relationships%20such%20as%20cancer%20cells.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Continual%20Multiple%20Instance%20Learning%20with%0AEnhanced%20Localization%20%28CoMEL%29%2C%20an%20MIL%20framework%20for%20both%20localization%20and%0Aadaptability%20with%20minimal%20forgetting.%20CoMEL%20consists%20of%20%281%29%20Grouped%20Double%0AAttention%20Transformer%20%28GDAT%29%20for%20efficient%20instance%20encoding%2C%20%282%29%20Bag%0APrototypes-based%20Pseudo-Labeling%20%28BPPL%29%20for%20reliable%20instance%20pseudo-labeling%2C%0Aand%20%283%29%20Orthogonal%20Weighted%20Low-Rank%20Adaptation%20%28OWLoRA%29%20to%20mitigate%20forgetting%0Ain%20both%20bag%20and%20instance%20classification.%20Extensive%20experiments%20on%20three%20public%0AWSI%20datasets%20demonstrate%20superior%20performance%20of%20CoMEL%2C%20outperforming%20the%20prior%0Aarts%20by%20up%20to%20%2411.00%5C%25%24%20in%20bag-level%20accuracy%20and%20up%20to%20%2423.4%5C%25%24%20in%0Alocalization%20accuracy%20under%20the%20continual%20MIL%20setup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02395v1&entry.124074799=Read"},
{"title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion", "author": "Fangfu Liu and Hao Li and Jiawei Chi and Hanyang Wang and Minghui Yang and Fudong Wang and Yueqi Duan", "abstract": "  Recovering 3D structures with open-vocabulary scene understanding from 2D\nimages is a fundamental but daunting task. Recent developments have achieved\nthis by performing per-scene optimization with embedded language information.\nHowever, they heavily rely on the calibrated dense-view reconstruction\nparadigm, thereby suffering from severe rendering artifacts and implausible\nsemantic synthesis when limited views are available. In this paper, we\nintroduce a novel generative framework, coined LangScene-X, to unify and\ngenerate 3D consistent multi-modality information for reconstruction and\nunderstanding. Powered by the generative capability of creating more consistent\nnovel observations, we can build generalizable 3D language-embedded scenes from\nonly sparse views. Specifically, we first train a TriMap video diffusion model\nthat can generate appearance (RGBs), geometry (normals), and semantics\n(segmentation maps) from sparse inputs through progressive knowledge\nintegration. Furthermore, we propose a Language Quantized Compressor (LQC),\ntrained on large-scale image datasets, to efficiently encode language\nembeddings, enabling cross-scene generalization without per-scene retraining.\nFinally, we reconstruct the language surface fields by aligning language\ninformation onto the surface of 3D scenes, enabling open-ended language\nqueries. Extensive experiments on real-world data demonstrate the superiority\nof our LangScene-X over state-of-the-art methods in terms of quality and\ngeneralizability. Project Page: https://liuff19.github.io/LangScene-X.\n", "link": "http://arxiv.org/abs/2507.02813v1", "date": "2025-07-03", "relevancy": 2.8211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7138}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangScene-X%3A%20Reconstruct%20Generalizable%203D%20Language-Embedded%20Scenes%20with%0A%20%20TriMap%20Video%20Diffusion&body=Title%3A%20LangScene-X%3A%20Reconstruct%20Generalizable%203D%20Language-Embedded%20Scenes%20with%0A%20%20TriMap%20Video%20Diffusion%0AAuthor%3A%20Fangfu%20Liu%20and%20Hao%20Li%20and%20Jiawei%20Chi%20and%20Hanyang%20Wang%20and%20Minghui%20Yang%20and%20Fudong%20Wang%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20Recovering%203D%20structures%20with%20open-vocabulary%20scene%20understanding%20from%202D%0Aimages%20is%20a%20fundamental%20but%20daunting%20task.%20Recent%20developments%20have%20achieved%0Athis%20by%20performing%20per-scene%20optimization%20with%20embedded%20language%20information.%0AHowever%2C%20they%20heavily%20rely%20on%20the%20calibrated%20dense-view%20reconstruction%0Aparadigm%2C%20thereby%20suffering%20from%20severe%20rendering%20artifacts%20and%20implausible%0Asemantic%20synthesis%20when%20limited%20views%20are%20available.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20generative%20framework%2C%20coined%20LangScene-X%2C%20to%20unify%20and%0Agenerate%203D%20consistent%20multi-modality%20information%20for%20reconstruction%20and%0Aunderstanding.%20Powered%20by%20the%20generative%20capability%20of%20creating%20more%20consistent%0Anovel%20observations%2C%20we%20can%20build%20generalizable%203D%20language-embedded%20scenes%20from%0Aonly%20sparse%20views.%20Specifically%2C%20we%20first%20train%20a%20TriMap%20video%20diffusion%20model%0Athat%20can%20generate%20appearance%20%28RGBs%29%2C%20geometry%20%28normals%29%2C%20and%20semantics%0A%28segmentation%20maps%29%20from%20sparse%20inputs%20through%20progressive%20knowledge%0Aintegration.%20Furthermore%2C%20we%20propose%20a%20Language%20Quantized%20Compressor%20%28LQC%29%2C%0Atrained%20on%20large-scale%20image%20datasets%2C%20to%20efficiently%20encode%20language%0Aembeddings%2C%20enabling%20cross-scene%20generalization%20without%20per-scene%20retraining.%0AFinally%2C%20we%20reconstruct%20the%20language%20surface%20fields%20by%20aligning%20language%0Ainformation%20onto%20the%20surface%20of%203D%20scenes%2C%20enabling%20open-ended%20language%0Aqueries.%20Extensive%20experiments%20on%20real-world%20data%20demonstrate%20the%20superiority%0Aof%20our%20LangScene-X%20over%20state-of-the-art%20methods%20in%20terms%20of%20quality%20and%0Ageneralizability.%20Project%20Page%3A%20https%3A//liuff19.github.io/LangScene-X.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangScene-X%253A%2520Reconstruct%2520Generalizable%25203D%2520Language-Embedded%2520Scenes%2520with%250A%2520%2520TriMap%2520Video%2520Diffusion%26entry.906535625%3DFangfu%2520Liu%2520and%2520Hao%2520Li%2520and%2520Jiawei%2520Chi%2520and%2520Hanyang%2520Wang%2520and%2520Minghui%2520Yang%2520and%2520Fudong%2520Wang%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520Recovering%25203D%2520structures%2520with%2520open-vocabulary%2520scene%2520understanding%2520from%25202D%250Aimages%2520is%2520a%2520fundamental%2520but%2520daunting%2520task.%2520Recent%2520developments%2520have%2520achieved%250Athis%2520by%2520performing%2520per-scene%2520optimization%2520with%2520embedded%2520language%2520information.%250AHowever%252C%2520they%2520heavily%2520rely%2520on%2520the%2520calibrated%2520dense-view%2520reconstruction%250Aparadigm%252C%2520thereby%2520suffering%2520from%2520severe%2520rendering%2520artifacts%2520and%2520implausible%250Asemantic%2520synthesis%2520when%2520limited%2520views%2520are%2520available.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520generative%2520framework%252C%2520coined%2520LangScene-X%252C%2520to%2520unify%2520and%250Agenerate%25203D%2520consistent%2520multi-modality%2520information%2520for%2520reconstruction%2520and%250Aunderstanding.%2520Powered%2520by%2520the%2520generative%2520capability%2520of%2520creating%2520more%2520consistent%250Anovel%2520observations%252C%2520we%2520can%2520build%2520generalizable%25203D%2520language-embedded%2520scenes%2520from%250Aonly%2520sparse%2520views.%2520Specifically%252C%2520we%2520first%2520train%2520a%2520TriMap%2520video%2520diffusion%2520model%250Athat%2520can%2520generate%2520appearance%2520%2528RGBs%2529%252C%2520geometry%2520%2528normals%2529%252C%2520and%2520semantics%250A%2528segmentation%2520maps%2529%2520from%2520sparse%2520inputs%2520through%2520progressive%2520knowledge%250Aintegration.%2520Furthermore%252C%2520we%2520propose%2520a%2520Language%2520Quantized%2520Compressor%2520%2528LQC%2529%252C%250Atrained%2520on%2520large-scale%2520image%2520datasets%252C%2520to%2520efficiently%2520encode%2520language%250Aembeddings%252C%2520enabling%2520cross-scene%2520generalization%2520without%2520per-scene%2520retraining.%250AFinally%252C%2520we%2520reconstruct%2520the%2520language%2520surface%2520fields%2520by%2520aligning%2520language%250Ainformation%2520onto%2520the%2520surface%2520of%25203D%2520scenes%252C%2520enabling%2520open-ended%2520language%250Aqueries.%2520Extensive%2520experiments%2520on%2520real-world%2520data%2520demonstrate%2520the%2520superiority%250Aof%2520our%2520LangScene-X%2520over%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520quality%2520and%250Ageneralizability.%2520Project%2520Page%253A%2520https%253A//liuff19.github.io/LangScene-X.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangScene-X%3A%20Reconstruct%20Generalizable%203D%20Language-Embedded%20Scenes%20with%0A%20%20TriMap%20Video%20Diffusion&entry.906535625=Fangfu%20Liu%20and%20Hao%20Li%20and%20Jiawei%20Chi%20and%20Hanyang%20Wang%20and%20Minghui%20Yang%20and%20Fudong%20Wang%20and%20Yueqi%20Duan&entry.1292438233=%20%20Recovering%203D%20structures%20with%20open-vocabulary%20scene%20understanding%20from%202D%0Aimages%20is%20a%20fundamental%20but%20daunting%20task.%20Recent%20developments%20have%20achieved%0Athis%20by%20performing%20per-scene%20optimization%20with%20embedded%20language%20information.%0AHowever%2C%20they%20heavily%20rely%20on%20the%20calibrated%20dense-view%20reconstruction%0Aparadigm%2C%20thereby%20suffering%20from%20severe%20rendering%20artifacts%20and%20implausible%0Asemantic%20synthesis%20when%20limited%20views%20are%20available.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20generative%20framework%2C%20coined%20LangScene-X%2C%20to%20unify%20and%0Agenerate%203D%20consistent%20multi-modality%20information%20for%20reconstruction%20and%0Aunderstanding.%20Powered%20by%20the%20generative%20capability%20of%20creating%20more%20consistent%0Anovel%20observations%2C%20we%20can%20build%20generalizable%203D%20language-embedded%20scenes%20from%0Aonly%20sparse%20views.%20Specifically%2C%20we%20first%20train%20a%20TriMap%20video%20diffusion%20model%0Athat%20can%20generate%20appearance%20%28RGBs%29%2C%20geometry%20%28normals%29%2C%20and%20semantics%0A%28segmentation%20maps%29%20from%20sparse%20inputs%20through%20progressive%20knowledge%0Aintegration.%20Furthermore%2C%20we%20propose%20a%20Language%20Quantized%20Compressor%20%28LQC%29%2C%0Atrained%20on%20large-scale%20image%20datasets%2C%20to%20efficiently%20encode%20language%0Aembeddings%2C%20enabling%20cross-scene%20generalization%20without%20per-scene%20retraining.%0AFinally%2C%20we%20reconstruct%20the%20language%20surface%20fields%20by%20aligning%20language%0Ainformation%20onto%20the%20surface%20of%203D%20scenes%2C%20enabling%20open-ended%20language%0Aqueries.%20Extensive%20experiments%20on%20real-world%20data%20demonstrate%20the%20superiority%0Aof%20our%20LangScene-X%20over%20state-of-the-art%20methods%20in%20terms%20of%20quality%20and%0Ageneralizability.%20Project%20Page%3A%20https%3A//liuff19.github.io/LangScene-X.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02813v1&entry.124074799=Read"},
{"title": "Holistic Tokenizer for Autoregressive Image Generation", "author": "Anlin Zheng and Haochen Wang and Yucheng Zhao and Weipeng Deng and Tiancai Wang and Xiangyu Zhang and Xiaojuan Qi", "abstract": "  The vanilla autoregressive image generation model generates visual tokens in\na step-by-step fashion, which limits the ability to capture holistic\nrelationships among token sequences. Moreover, most visual tokenizers map local\nimage patches into latent tokens, leading to limited global information. To\naddress this, we introduce \\textit{Hita}, a novel image tokenizer for\nautoregressive (AR) image generation. It introduces a holistic-to-local\ntokenization scheme with learnable holistic queries and local patch tokens.\nBesides, Hita incorporates two key strategies for improved alignment with the\nAR generation process: 1) it arranges a sequential structure with holistic\ntokens at the beginning followed by patch-level tokens while using causal\nattention to maintain awareness of previous tokens; and 2) before feeding the\nde-quantized tokens into the decoder, Hita adopts a lightweight fusion module\nto control information flow to prioritize holistic tokens. Extensive\nexperiments show that Hita accelerates the training speed of AR generators and\noutperforms those trained with vanilla tokenizers, achieving \\textbf{2.59 FID}\nand \\textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the\nholistic representation highlights its ability to capture global image\nproperties such as textures, materials, and shapes. Additionally, Hita also\ndemonstrates effectiveness in zero-shot style transfer and image in-painting.\nThe code is available at\n\\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}\n", "link": "http://arxiv.org/abs/2507.02358v1", "date": "2025-07-03", "relevancy": 2.7824, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6085}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5379}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation&body=Title%3A%20Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation%0AAuthor%3A%20Anlin%20Zheng%20and%20Haochen%20Wang%20and%20Yucheng%20Zhao%20and%20Weipeng%20Deng%20and%20Tiancai%20Wang%20and%20Xiangyu%20Zhang%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20The%20vanilla%20autoregressive%20image%20generation%20model%20generates%20visual%20tokens%20in%0Aa%20step-by-step%20fashion%2C%20which%20limits%20the%20ability%20to%20capture%20holistic%0Arelationships%20among%20token%20sequences.%20Moreover%2C%20most%20visual%20tokenizers%20map%20local%0Aimage%20patches%20into%20latent%20tokens%2C%20leading%20to%20limited%20global%20information.%20To%0Aaddress%20this%2C%20we%20introduce%20%5Ctextit%7BHita%7D%2C%20a%20novel%20image%20tokenizer%20for%0Aautoregressive%20%28AR%29%20image%20generation.%20It%20introduces%20a%20holistic-to-local%0Atokenization%20scheme%20with%20learnable%20holistic%20queries%20and%20local%20patch%20tokens.%0ABesides%2C%20Hita%20incorporates%20two%20key%20strategies%20for%20improved%20alignment%20with%20the%0AAR%20generation%20process%3A%201%29%20it%20arranges%20a%20sequential%20structure%20with%20holistic%0Atokens%20at%20the%20beginning%20followed%20by%20patch-level%20tokens%20while%20using%20causal%0Aattention%20to%20maintain%20awareness%20of%20previous%20tokens%3B%20and%202%29%20before%20feeding%20the%0Ade-quantized%20tokens%20into%20the%20decoder%2C%20Hita%20adopts%20a%20lightweight%20fusion%20module%0Ato%20control%20information%20flow%20to%20prioritize%20holistic%20tokens.%20Extensive%0Aexperiments%20show%20that%20Hita%20accelerates%20the%20training%20speed%20of%20AR%20generators%20and%0Aoutperforms%20those%20trained%20with%20vanilla%20tokenizers%2C%20achieving%20%5Ctextbf%7B2.59%20FID%7D%0Aand%20%5Ctextbf%7B281.9%20IS%7D%20on%20the%20ImageNet%20benchmark.%20A%20detailed%20analysis%20of%20the%0Aholistic%20representation%20highlights%20its%20ability%20to%20capture%20global%20image%0Aproperties%20such%20as%20textures%2C%20materials%2C%20and%20shapes.%20Additionally%2C%20Hita%20also%0Ademonstrates%20effectiveness%20in%20zero-shot%20style%20transfer%20and%20image%20in-painting.%0AThe%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHolistic%2520Tokenizer%2520for%2520Autoregressive%2520Image%2520Generation%26entry.906535625%3DAnlin%2520Zheng%2520and%2520Haochen%2520Wang%2520and%2520Yucheng%2520Zhao%2520and%2520Weipeng%2520Deng%2520and%2520Tiancai%2520Wang%2520and%2520Xiangyu%2520Zhang%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520The%2520vanilla%2520autoregressive%2520image%2520generation%2520model%2520generates%2520visual%2520tokens%2520in%250Aa%2520step-by-step%2520fashion%252C%2520which%2520limits%2520the%2520ability%2520to%2520capture%2520holistic%250Arelationships%2520among%2520token%2520sequences.%2520Moreover%252C%2520most%2520visual%2520tokenizers%2520map%2520local%250Aimage%2520patches%2520into%2520latent%2520tokens%252C%2520leading%2520to%2520limited%2520global%2520information.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520%255Ctextit%257BHita%257D%252C%2520a%2520novel%2520image%2520tokenizer%2520for%250Aautoregressive%2520%2528AR%2529%2520image%2520generation.%2520It%2520introduces%2520a%2520holistic-to-local%250Atokenization%2520scheme%2520with%2520learnable%2520holistic%2520queries%2520and%2520local%2520patch%2520tokens.%250ABesides%252C%2520Hita%2520incorporates%2520two%2520key%2520strategies%2520for%2520improved%2520alignment%2520with%2520the%250AAR%2520generation%2520process%253A%25201%2529%2520it%2520arranges%2520a%2520sequential%2520structure%2520with%2520holistic%250Atokens%2520at%2520the%2520beginning%2520followed%2520by%2520patch-level%2520tokens%2520while%2520using%2520causal%250Aattention%2520to%2520maintain%2520awareness%2520of%2520previous%2520tokens%253B%2520and%25202%2529%2520before%2520feeding%2520the%250Ade-quantized%2520tokens%2520into%2520the%2520decoder%252C%2520Hita%2520adopts%2520a%2520lightweight%2520fusion%2520module%250Ato%2520control%2520information%2520flow%2520to%2520prioritize%2520holistic%2520tokens.%2520Extensive%250Aexperiments%2520show%2520that%2520Hita%2520accelerates%2520the%2520training%2520speed%2520of%2520AR%2520generators%2520and%250Aoutperforms%2520those%2520trained%2520with%2520vanilla%2520tokenizers%252C%2520achieving%2520%255Ctextbf%257B2.59%2520FID%257D%250Aand%2520%255Ctextbf%257B281.9%2520IS%257D%2520on%2520the%2520ImageNet%2520benchmark.%2520A%2520detailed%2520analysis%2520of%2520the%250Aholistic%2520representation%2520highlights%2520its%2520ability%2520to%2520capture%2520global%2520image%250Aproperties%2520such%2520as%2520textures%252C%2520materials%252C%2520and%2520shapes.%2520Additionally%252C%2520Hita%2520also%250Ademonstrates%2520effectiveness%2520in%2520zero-shot%2520style%2520transfer%2520and%2520image%2520in-painting.%250AThe%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/CVMI-Lab/Hita%257D%257Bhttps%253A//github.com/CVMI-Lab/Hita%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation&entry.906535625=Anlin%20Zheng%20and%20Haochen%20Wang%20and%20Yucheng%20Zhao%20and%20Weipeng%20Deng%20and%20Tiancai%20Wang%20and%20Xiangyu%20Zhang%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20The%20vanilla%20autoregressive%20image%20generation%20model%20generates%20visual%20tokens%20in%0Aa%20step-by-step%20fashion%2C%20which%20limits%20the%20ability%20to%20capture%20holistic%0Arelationships%20among%20token%20sequences.%20Moreover%2C%20most%20visual%20tokenizers%20map%20local%0Aimage%20patches%20into%20latent%20tokens%2C%20leading%20to%20limited%20global%20information.%20To%0Aaddress%20this%2C%20we%20introduce%20%5Ctextit%7BHita%7D%2C%20a%20novel%20image%20tokenizer%20for%0Aautoregressive%20%28AR%29%20image%20generation.%20It%20introduces%20a%20holistic-to-local%0Atokenization%20scheme%20with%20learnable%20holistic%20queries%20and%20local%20patch%20tokens.%0ABesides%2C%20Hita%20incorporates%20two%20key%20strategies%20for%20improved%20alignment%20with%20the%0AAR%20generation%20process%3A%201%29%20it%20arranges%20a%20sequential%20structure%20with%20holistic%0Atokens%20at%20the%20beginning%20followed%20by%20patch-level%20tokens%20while%20using%20causal%0Aattention%20to%20maintain%20awareness%20of%20previous%20tokens%3B%20and%202%29%20before%20feeding%20the%0Ade-quantized%20tokens%20into%20the%20decoder%2C%20Hita%20adopts%20a%20lightweight%20fusion%20module%0Ato%20control%20information%20flow%20to%20prioritize%20holistic%20tokens.%20Extensive%0Aexperiments%20show%20that%20Hita%20accelerates%20the%20training%20speed%20of%20AR%20generators%20and%0Aoutperforms%20those%20trained%20with%20vanilla%20tokenizers%2C%20achieving%20%5Ctextbf%7B2.59%20FID%7D%0Aand%20%5Ctextbf%7B281.9%20IS%7D%20on%20the%20ImageNet%20benchmark.%20A%20detailed%20analysis%20of%20the%0Aholistic%20representation%20highlights%20its%20ability%20to%20capture%20global%20image%0Aproperties%20such%20as%20textures%2C%20materials%2C%20and%20shapes.%20Additionally%2C%20Hita%20also%0Ademonstrates%20effectiveness%20in%20zero-shot%20style%20transfer%20and%20image%20in-painting.%0AThe%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02358v1&entry.124074799=Read"},
{"title": "Modality-agnostic, patient-specific digital twins modeling temporally\n  varying digestive motion", "author": "Jorge Tapias Gomez and Nishant Nadkarni and Lando S. Bosma and Jue Jiang and Ergys D. Subashi and William P. Segars and James M. Balter and Mert R Sabuncu and Neelam Tyagi and Harini Veeraraghavan", "abstract": "  Objective: Clinical implementation of deformable image registration (DIR)\nrequires voxel-based spatial accuracy metrics such as manually identified\nlandmarks, which are challenging to implement for highly mobile\ngastrointestinal (GI) organs. To address this, patient-specific digital twins\n(DT) modeling temporally varying motion were created to assess the accuracy of\nDIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D\nsequences were generated from static 3D patient scans using published\nanalytical GI motion models through a semi-automated pipeline. Eleven datasets,\nincluding six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,\nand three contrast-enhanced CT scans. The motion amplitudes of the DTs were\nassessed against real patient stomach motion amplitudes extracted from\nindependent 4D MRI datasets. The generated DTs were then used to assess six\ndifferent DIR methods using target registration error, Dice similarity\ncoefficient, and the 95th percentile Hausdorff distance using summary metrics\nand voxel-level granular visualizations. Finally, for a subset of T2w MRI scans\nfrom patients treated with MR-guided radiation therapy, dose distributions were\nwarped and accumulated to assess dose warping errors, including evaluations of\nDIR performance in both low- and high-dose regions for patient-specific error\nestimation. Main results: Our proposed pipeline synthesized DTs modeling\nrealistic GI motion, achieving mean and maximum motion amplitudes and a mean\nlog Jacobian determinant within 0.8 mm and 0.01, respectively, similar to\npublished real-patient gastric motion data. It also enables the extraction of\ndetailed quantitative DIR performance metrics and rigorous validation of dose\nmapping accuracy. Significance: The pipeline enables rigorously testing DIR\ntools for dynamic, anatomically complex regions enabling granular spatial and\ndosimetric accuracies.\n", "link": "http://arxiv.org/abs/2507.01909v2", "date": "2025-07-03", "relevancy": 2.7788, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5778}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5483}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion&body=Title%3A%20Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion%0AAuthor%3A%20Jorge%20Tapias%20Gomez%20and%20Nishant%20Nadkarni%20and%20Lando%20S.%20Bosma%20and%20Jue%20Jiang%20and%20Ergys%20D.%20Subashi%20and%20William%20P.%20Segars%20and%20James%20M.%20Balter%20and%20Mert%20R%20Sabuncu%20and%20Neelam%20Tyagi%20and%20Harini%20Veeraraghavan%0AAbstract%3A%20%20%20Objective%3A%20Clinical%20implementation%20of%20deformable%20image%20registration%20%28DIR%29%0Arequires%20voxel-based%20spatial%20accuracy%20metrics%20such%20as%20manually%20identified%0Alandmarks%2C%20which%20are%20challenging%20to%20implement%20for%20highly%20mobile%0Agastrointestinal%20%28GI%29%20organs.%20To%20address%20this%2C%20patient-specific%20digital%20twins%0A%28DT%29%20modeling%20temporally%20varying%20motion%20were%20created%20to%20assess%20the%20accuracy%20of%0ADIR%20methods.%20Approach%3A%2021%20motion%20phases%20simulating%20digestive%20GI%20motion%20as%204D%0Asequences%20were%20generated%20from%20static%203D%20patient%20scans%20using%20published%0Aanalytical%20GI%20motion%20models%20through%20a%20semi-automated%20pipeline.%20Eleven%20datasets%2C%0Aincluding%20six%20T2w%20FSE%20MRI%20%28T2w%20MRI%29%2C%20two%20T1w%204D%20golden-angle%20stack-of-stars%2C%0Aand%20three%20contrast-enhanced%20CT%20scans.%20The%20motion%20amplitudes%20of%20the%20DTs%20were%0Aassessed%20against%20real%20patient%20stomach%20motion%20amplitudes%20extracted%20from%0Aindependent%204D%20MRI%20datasets.%20The%20generated%20DTs%20were%20then%20used%20to%20assess%20six%0Adifferent%20DIR%20methods%20using%20target%20registration%20error%2C%20Dice%20similarity%0Acoefficient%2C%20and%20the%2095th%20percentile%20Hausdorff%20distance%20using%20summary%20metrics%0Aand%20voxel-level%20granular%20visualizations.%20Finally%2C%20for%20a%20subset%20of%20T2w%20MRI%20scans%0Afrom%20patients%20treated%20with%20MR-guided%20radiation%20therapy%2C%20dose%20distributions%20were%0Awarped%20and%20accumulated%20to%20assess%20dose%20warping%20errors%2C%20including%20evaluations%20of%0ADIR%20performance%20in%20both%20low-%20and%20high-dose%20regions%20for%20patient-specific%20error%0Aestimation.%20Main%20results%3A%20Our%20proposed%20pipeline%20synthesized%20DTs%20modeling%0Arealistic%20GI%20motion%2C%20achieving%20mean%20and%20maximum%20motion%20amplitudes%20and%20a%20mean%0Alog%20Jacobian%20determinant%20within%200.8%20mm%20and%200.01%2C%20respectively%2C%20similar%20to%0Apublished%20real-patient%20gastric%20motion%20data.%20It%20also%20enables%20the%20extraction%20of%0Adetailed%20quantitative%20DIR%20performance%20metrics%20and%20rigorous%20validation%20of%20dose%0Amapping%20accuracy.%20Significance%3A%20The%20pipeline%20enables%20rigorously%20testing%20DIR%0Atools%20for%20dynamic%2C%20anatomically%20complex%20regions%20enabling%20granular%20spatial%20and%0Adosimetric%20accuracies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-agnostic%252C%2520patient-specific%2520digital%2520twins%2520modeling%2520temporally%250A%2520%2520varying%2520digestive%2520motion%26entry.906535625%3DJorge%2520Tapias%2520Gomez%2520and%2520Nishant%2520Nadkarni%2520and%2520Lando%2520S.%2520Bosma%2520and%2520Jue%2520Jiang%2520and%2520Ergys%2520D.%2520Subashi%2520and%2520William%2520P.%2520Segars%2520and%2520James%2520M.%2520Balter%2520and%2520Mert%2520R%2520Sabuncu%2520and%2520Neelam%2520Tyagi%2520and%2520Harini%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Objective%253A%2520Clinical%2520implementation%2520of%2520deformable%2520image%2520registration%2520%2528DIR%2529%250Arequires%2520voxel-based%2520spatial%2520accuracy%2520metrics%2520such%2520as%2520manually%2520identified%250Alandmarks%252C%2520which%2520are%2520challenging%2520to%2520implement%2520for%2520highly%2520mobile%250Agastrointestinal%2520%2528GI%2529%2520organs.%2520To%2520address%2520this%252C%2520patient-specific%2520digital%2520twins%250A%2528DT%2529%2520modeling%2520temporally%2520varying%2520motion%2520were%2520created%2520to%2520assess%2520the%2520accuracy%2520of%250ADIR%2520methods.%2520Approach%253A%252021%2520motion%2520phases%2520simulating%2520digestive%2520GI%2520motion%2520as%25204D%250Asequences%2520were%2520generated%2520from%2520static%25203D%2520patient%2520scans%2520using%2520published%250Aanalytical%2520GI%2520motion%2520models%2520through%2520a%2520semi-automated%2520pipeline.%2520Eleven%2520datasets%252C%250Aincluding%2520six%2520T2w%2520FSE%2520MRI%2520%2528T2w%2520MRI%2529%252C%2520two%2520T1w%25204D%2520golden-angle%2520stack-of-stars%252C%250Aand%2520three%2520contrast-enhanced%2520CT%2520scans.%2520The%2520motion%2520amplitudes%2520of%2520the%2520DTs%2520were%250Aassessed%2520against%2520real%2520patient%2520stomach%2520motion%2520amplitudes%2520extracted%2520from%250Aindependent%25204D%2520MRI%2520datasets.%2520The%2520generated%2520DTs%2520were%2520then%2520used%2520to%2520assess%2520six%250Adifferent%2520DIR%2520methods%2520using%2520target%2520registration%2520error%252C%2520Dice%2520similarity%250Acoefficient%252C%2520and%2520the%252095th%2520percentile%2520Hausdorff%2520distance%2520using%2520summary%2520metrics%250Aand%2520voxel-level%2520granular%2520visualizations.%2520Finally%252C%2520for%2520a%2520subset%2520of%2520T2w%2520MRI%2520scans%250Afrom%2520patients%2520treated%2520with%2520MR-guided%2520radiation%2520therapy%252C%2520dose%2520distributions%2520were%250Awarped%2520and%2520accumulated%2520to%2520assess%2520dose%2520warping%2520errors%252C%2520including%2520evaluations%2520of%250ADIR%2520performance%2520in%2520both%2520low-%2520and%2520high-dose%2520regions%2520for%2520patient-specific%2520error%250Aestimation.%2520Main%2520results%253A%2520Our%2520proposed%2520pipeline%2520synthesized%2520DTs%2520modeling%250Arealistic%2520GI%2520motion%252C%2520achieving%2520mean%2520and%2520maximum%2520motion%2520amplitudes%2520and%2520a%2520mean%250Alog%2520Jacobian%2520determinant%2520within%25200.8%2520mm%2520and%25200.01%252C%2520respectively%252C%2520similar%2520to%250Apublished%2520real-patient%2520gastric%2520motion%2520data.%2520It%2520also%2520enables%2520the%2520extraction%2520of%250Adetailed%2520quantitative%2520DIR%2520performance%2520metrics%2520and%2520rigorous%2520validation%2520of%2520dose%250Amapping%2520accuracy.%2520Significance%253A%2520The%2520pipeline%2520enables%2520rigorously%2520testing%2520DIR%250Atools%2520for%2520dynamic%252C%2520anatomically%2520complex%2520regions%2520enabling%2520granular%2520spatial%2520and%250Adosimetric%2520accuracies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion&entry.906535625=Jorge%20Tapias%20Gomez%20and%20Nishant%20Nadkarni%20and%20Lando%20S.%20Bosma%20and%20Jue%20Jiang%20and%20Ergys%20D.%20Subashi%20and%20William%20P.%20Segars%20and%20James%20M.%20Balter%20and%20Mert%20R%20Sabuncu%20and%20Neelam%20Tyagi%20and%20Harini%20Veeraraghavan&entry.1292438233=%20%20Objective%3A%20Clinical%20implementation%20of%20deformable%20image%20registration%20%28DIR%29%0Arequires%20voxel-based%20spatial%20accuracy%20metrics%20such%20as%20manually%20identified%0Alandmarks%2C%20which%20are%20challenging%20to%20implement%20for%20highly%20mobile%0Agastrointestinal%20%28GI%29%20organs.%20To%20address%20this%2C%20patient-specific%20digital%20twins%0A%28DT%29%20modeling%20temporally%20varying%20motion%20were%20created%20to%20assess%20the%20accuracy%20of%0ADIR%20methods.%20Approach%3A%2021%20motion%20phases%20simulating%20digestive%20GI%20motion%20as%204D%0Asequences%20were%20generated%20from%20static%203D%20patient%20scans%20using%20published%0Aanalytical%20GI%20motion%20models%20through%20a%20semi-automated%20pipeline.%20Eleven%20datasets%2C%0Aincluding%20six%20T2w%20FSE%20MRI%20%28T2w%20MRI%29%2C%20two%20T1w%204D%20golden-angle%20stack-of-stars%2C%0Aand%20three%20contrast-enhanced%20CT%20scans.%20The%20motion%20amplitudes%20of%20the%20DTs%20were%0Aassessed%20against%20real%20patient%20stomach%20motion%20amplitudes%20extracted%20from%0Aindependent%204D%20MRI%20datasets.%20The%20generated%20DTs%20were%20then%20used%20to%20assess%20six%0Adifferent%20DIR%20methods%20using%20target%20registration%20error%2C%20Dice%20similarity%0Acoefficient%2C%20and%20the%2095th%20percentile%20Hausdorff%20distance%20using%20summary%20metrics%0Aand%20voxel-level%20granular%20visualizations.%20Finally%2C%20for%20a%20subset%20of%20T2w%20MRI%20scans%0Afrom%20patients%20treated%20with%20MR-guided%20radiation%20therapy%2C%20dose%20distributions%20were%0Awarped%20and%20accumulated%20to%20assess%20dose%20warping%20errors%2C%20including%20evaluations%20of%0ADIR%20performance%20in%20both%20low-%20and%20high-dose%20regions%20for%20patient-specific%20error%0Aestimation.%20Main%20results%3A%20Our%20proposed%20pipeline%20synthesized%20DTs%20modeling%0Arealistic%20GI%20motion%2C%20achieving%20mean%20and%20maximum%20motion%20amplitudes%20and%20a%20mean%0Alog%20Jacobian%20determinant%20within%200.8%20mm%20and%200.01%2C%20respectively%2C%20similar%20to%0Apublished%20real-patient%20gastric%20motion%20data.%20It%20also%20enables%20the%20extraction%20of%0Adetailed%20quantitative%20DIR%20performance%20metrics%20and%20rigorous%20validation%20of%20dose%0Amapping%20accuracy.%20Significance%3A%20The%20pipeline%20enables%20rigorously%20testing%20DIR%0Atools%20for%20dynamic%2C%20anatomically%20complex%20regions%20enabling%20granular%20spatial%20and%0Adosimetric%20accuracies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01909v2&entry.124074799=Read"},
{"title": "Anatomical Foundation Models for Brain MRIs", "author": "Carlo Alberto Barbano and Matteo Brunello and Benoit Dufumier and Marco Grangetto", "abstract": "  Deep Learning (DL) in neuroimaging has become increasingly relevant for\ndetecting neurological conditions and neurodegenerative disorders. One of the\nmost predominant biomarkers in neuroimaging is represented by brain age, which\nhas been shown to be a good indicator for different conditions, such as\nAlzheimer's Disease. Using brain age for weakly supervised pre-training of DL\nmodels in transfer learning settings has also recently shown promising results,\nespecially when dealing with data scarcity of different conditions. On the\nother hand, anatomical information of brain MRIs (e.g. cortical thickness) can\nprovide important information for learning good representations that can be\ntransferred to many downstream tasks. In this work, we propose AnatCL, an\nanatomical foundation model for brain MRIs that i.) leverages anatomical\ninformation in a weakly contrastive learning approach, and ii.) achieves\nstate-of-the-art performances across many different downstream tasks. To\nvalidate our approach we consider 12 different downstream tasks for the\ndiagnosis of different conditions such as Alzheimer's Disease, autism spectrum\ndisorder, and schizophrenia. Furthermore, we also target the prediction of 10\ndifferent clinical assessment scores using structural MRI data. Our findings\nshow that incorporating anatomical information during pre-training leads to\nmore robust and generalizable representations. Pre-trained models can be found\nat: https://github.com/EIDOSLAB/AnatCL.\n", "link": "http://arxiv.org/abs/2408.07079v4", "date": "2025-07-03", "relevancy": 2.7545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anatomical%20Foundation%20Models%20for%20Brain%20MRIs&body=Title%3A%20Anatomical%20Foundation%20Models%20for%20Brain%20MRIs%0AAuthor%3A%20Carlo%20Alberto%20Barbano%20and%20Matteo%20Brunello%20and%20Benoit%20Dufumier%20and%20Marco%20Grangetto%0AAbstract%3A%20%20%20Deep%20Learning%20%28DL%29%20in%20neuroimaging%20has%20become%20increasingly%20relevant%20for%0Adetecting%20neurological%20conditions%20and%20neurodegenerative%20disorders.%20One%20of%20the%0Amost%20predominant%20biomarkers%20in%20neuroimaging%20is%20represented%20by%20brain%20age%2C%20which%0Ahas%20been%20shown%20to%20be%20a%20good%20indicator%20for%20different%20conditions%2C%20such%20as%0AAlzheimer%27s%20Disease.%20Using%20brain%20age%20for%20weakly%20supervised%20pre-training%20of%20DL%0Amodels%20in%20transfer%20learning%20settings%20has%20also%20recently%20shown%20promising%20results%2C%0Aespecially%20when%20dealing%20with%20data%20scarcity%20of%20different%20conditions.%20On%20the%0Aother%20hand%2C%20anatomical%20information%20of%20brain%20MRIs%20%28e.g.%20cortical%20thickness%29%20can%0Aprovide%20important%20information%20for%20learning%20good%20representations%20that%20can%20be%0Atransferred%20to%20many%20downstream%20tasks.%20In%20this%20work%2C%20we%20propose%20AnatCL%2C%20an%0Aanatomical%20foundation%20model%20for%20brain%20MRIs%20that%20i.%29%20leverages%20anatomical%0Ainformation%20in%20a%20weakly%20contrastive%20learning%20approach%2C%20and%20ii.%29%20achieves%0Astate-of-the-art%20performances%20across%20many%20different%20downstream%20tasks.%20To%0Avalidate%20our%20approach%20we%20consider%2012%20different%20downstream%20tasks%20for%20the%0Adiagnosis%20of%20different%20conditions%20such%20as%20Alzheimer%27s%20Disease%2C%20autism%20spectrum%0Adisorder%2C%20and%20schizophrenia.%20Furthermore%2C%20we%20also%20target%20the%20prediction%20of%2010%0Adifferent%20clinical%20assessment%20scores%20using%20structural%20MRI%20data.%20Our%20findings%0Ashow%20that%20incorporating%20anatomical%20information%20during%20pre-training%20leads%20to%0Amore%20robust%20and%20generalizable%20representations.%20Pre-trained%20models%20can%20be%20found%0Aat%3A%20https%3A//github.com/EIDOSLAB/AnatCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07079v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatomical%2520Foundation%2520Models%2520for%2520Brain%2520MRIs%26entry.906535625%3DCarlo%2520Alberto%2520Barbano%2520and%2520Matteo%2520Brunello%2520and%2520Benoit%2520Dufumier%2520and%2520Marco%2520Grangetto%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520%2528DL%2529%2520in%2520neuroimaging%2520has%2520become%2520increasingly%2520relevant%2520for%250Adetecting%2520neurological%2520conditions%2520and%2520neurodegenerative%2520disorders.%2520One%2520of%2520the%250Amost%2520predominant%2520biomarkers%2520in%2520neuroimaging%2520is%2520represented%2520by%2520brain%2520age%252C%2520which%250Ahas%2520been%2520shown%2520to%2520be%2520a%2520good%2520indicator%2520for%2520different%2520conditions%252C%2520such%2520as%250AAlzheimer%2527s%2520Disease.%2520Using%2520brain%2520age%2520for%2520weakly%2520supervised%2520pre-training%2520of%2520DL%250Amodels%2520in%2520transfer%2520learning%2520settings%2520has%2520also%2520recently%2520shown%2520promising%2520results%252C%250Aespecially%2520when%2520dealing%2520with%2520data%2520scarcity%2520of%2520different%2520conditions.%2520On%2520the%250Aother%2520hand%252C%2520anatomical%2520information%2520of%2520brain%2520MRIs%2520%2528e.g.%2520cortical%2520thickness%2529%2520can%250Aprovide%2520important%2520information%2520for%2520learning%2520good%2520representations%2520that%2520can%2520be%250Atransferred%2520to%2520many%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520AnatCL%252C%2520an%250Aanatomical%2520foundation%2520model%2520for%2520brain%2520MRIs%2520that%2520i.%2529%2520leverages%2520anatomical%250Ainformation%2520in%2520a%2520weakly%2520contrastive%2520learning%2520approach%252C%2520and%2520ii.%2529%2520achieves%250Astate-of-the-art%2520performances%2520across%2520many%2520different%2520downstream%2520tasks.%2520To%250Avalidate%2520our%2520approach%2520we%2520consider%252012%2520different%2520downstream%2520tasks%2520for%2520the%250Adiagnosis%2520of%2520different%2520conditions%2520such%2520as%2520Alzheimer%2527s%2520Disease%252C%2520autism%2520spectrum%250Adisorder%252C%2520and%2520schizophrenia.%2520Furthermore%252C%2520we%2520also%2520target%2520the%2520prediction%2520of%252010%250Adifferent%2520clinical%2520assessment%2520scores%2520using%2520structural%2520MRI%2520data.%2520Our%2520findings%250Ashow%2520that%2520incorporating%2520anatomical%2520information%2520during%2520pre-training%2520leads%2520to%250Amore%2520robust%2520and%2520generalizable%2520representations.%2520Pre-trained%2520models%2520can%2520be%2520found%250Aat%253A%2520https%253A//github.com/EIDOSLAB/AnatCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07079v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomical%20Foundation%20Models%20for%20Brain%20MRIs&entry.906535625=Carlo%20Alberto%20Barbano%20and%20Matteo%20Brunello%20and%20Benoit%20Dufumier%20and%20Marco%20Grangetto&entry.1292438233=%20%20Deep%20Learning%20%28DL%29%20in%20neuroimaging%20has%20become%20increasingly%20relevant%20for%0Adetecting%20neurological%20conditions%20and%20neurodegenerative%20disorders.%20One%20of%20the%0Amost%20predominant%20biomarkers%20in%20neuroimaging%20is%20represented%20by%20brain%20age%2C%20which%0Ahas%20been%20shown%20to%20be%20a%20good%20indicator%20for%20different%20conditions%2C%20such%20as%0AAlzheimer%27s%20Disease.%20Using%20brain%20age%20for%20weakly%20supervised%20pre-training%20of%20DL%0Amodels%20in%20transfer%20learning%20settings%20has%20also%20recently%20shown%20promising%20results%2C%0Aespecially%20when%20dealing%20with%20data%20scarcity%20of%20different%20conditions.%20On%20the%0Aother%20hand%2C%20anatomical%20information%20of%20brain%20MRIs%20%28e.g.%20cortical%20thickness%29%20can%0Aprovide%20important%20information%20for%20learning%20good%20representations%20that%20can%20be%0Atransferred%20to%20many%20downstream%20tasks.%20In%20this%20work%2C%20we%20propose%20AnatCL%2C%20an%0Aanatomical%20foundation%20model%20for%20brain%20MRIs%20that%20i.%29%20leverages%20anatomical%0Ainformation%20in%20a%20weakly%20contrastive%20learning%20approach%2C%20and%20ii.%29%20achieves%0Astate-of-the-art%20performances%20across%20many%20different%20downstream%20tasks.%20To%0Avalidate%20our%20approach%20we%20consider%2012%20different%20downstream%20tasks%20for%20the%0Adiagnosis%20of%20different%20conditions%20such%20as%20Alzheimer%27s%20Disease%2C%20autism%20spectrum%0Adisorder%2C%20and%20schizophrenia.%20Furthermore%2C%20we%20also%20target%20the%20prediction%20of%2010%0Adifferent%20clinical%20assessment%20scores%20using%20structural%20MRI%20data.%20Our%20findings%0Ashow%20that%20incorporating%20anatomical%20information%20during%20pre-training%20leads%20to%0Amore%20robust%20and%20generalizable%20representations.%20Pre-trained%20models%20can%20be%20found%0Aat%3A%20https%3A//github.com/EIDOSLAB/AnatCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07079v4&entry.124074799=Read"},
{"title": "3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic\n  Slices", "author": "Zhurong Chen and Jinhua Chen and Wei Zhuo and Wufeng Xue and Dong Ni", "abstract": "  Echocardiography (echo) plays an indispensable role in the clinical practice\nof heart diseases. However, ultrasound imaging typically provides only\ntwo-dimensional (2D) cross-sectional images from a few specific views, making\nit challenging to interpret and inaccurate for estimation of clinical\nparameters like the volume of left ventricle (LV). 3D ultrasound imaging\nprovides an alternative for 3D quantification, but is still limited by the low\nspatial and temporal resolution and the highly demanding manual delineation.\n  To address these challenges, we propose an innovative framework for\nreconstructing personalized 3D heart anatomy from 2D echo slices that are\nfrequently used in clinical practice. Specifically, a novel 3D reconstruction\npipeline is designed, which alternatively optimizes between the 3D pose\nestimation of these 2D slices and the 3D integration of these slices using an\nimplicit neural network, progressively transforming a prior 3D heart shape into\na personalized 3D heart model.\n  We validate the method with two datasets. When six planes are used, the\nreconstructed 3D heart can lead to a significant improvement for LV volume\nestimation over the bi-plane method (error in percent: 1.98\\% VS. 20.24\\%). In\naddition, the whole reconstruction framework makes even an important\nbreakthrough that can estimate RV volume from 2D echo slices (with an error of\n5.75\\% ). This study provides a new way for personalized 3D structure and\nfunction analysis from cardiac ultrasound and is of great potential in clinical\npractice.\n", "link": "http://arxiv.org/abs/2507.02411v1", "date": "2025-07-03", "relevancy": 2.7481, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5527}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5502}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Heart%20Reconstruction%20from%20Sparse%20Pose-agnostic%202D%20Echocardiographic%0A%20%20Slices&body=Title%3A%203D%20Heart%20Reconstruction%20from%20Sparse%20Pose-agnostic%202D%20Echocardiographic%0A%20%20Slices%0AAuthor%3A%20Zhurong%20Chen%20and%20Jinhua%20Chen%20and%20Wei%20Zhuo%20and%20Wufeng%20Xue%20and%20Dong%20Ni%0AAbstract%3A%20%20%20Echocardiography%20%28echo%29%20plays%20an%20indispensable%20role%20in%20the%20clinical%20practice%0Aof%20heart%20diseases.%20However%2C%20ultrasound%20imaging%20typically%20provides%20only%0Atwo-dimensional%20%282D%29%20cross-sectional%20images%20from%20a%20few%20specific%20views%2C%20making%0Ait%20challenging%20to%20interpret%20and%20inaccurate%20for%20estimation%20of%20clinical%0Aparameters%20like%20the%20volume%20of%20left%20ventricle%20%28LV%29.%203D%20ultrasound%20imaging%0Aprovides%20an%20alternative%20for%203D%20quantification%2C%20but%20is%20still%20limited%20by%20the%20low%0Aspatial%20and%20temporal%20resolution%20and%20the%20highly%20demanding%20manual%20delineation.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20an%20innovative%20framework%20for%0Areconstructing%20personalized%203D%20heart%20anatomy%20from%202D%20echo%20slices%20that%20are%0Afrequently%20used%20in%20clinical%20practice.%20Specifically%2C%20a%20novel%203D%20reconstruction%0Apipeline%20is%20designed%2C%20which%20alternatively%20optimizes%20between%20the%203D%20pose%0Aestimation%20of%20these%202D%20slices%20and%20the%203D%20integration%20of%20these%20slices%20using%20an%0Aimplicit%20neural%20network%2C%20progressively%20transforming%20a%20prior%203D%20heart%20shape%20into%0Aa%20personalized%203D%20heart%20model.%0A%20%20We%20validate%20the%20method%20with%20two%20datasets.%20When%20six%20planes%20are%20used%2C%20the%0Areconstructed%203D%20heart%20can%20lead%20to%20a%20significant%20improvement%20for%20LV%20volume%0Aestimation%20over%20the%20bi-plane%20method%20%28error%20in%20percent%3A%201.98%5C%25%20VS.%2020.24%5C%25%29.%20In%0Aaddition%2C%20the%20whole%20reconstruction%20framework%20makes%20even%20an%20important%0Abreakthrough%20that%20can%20estimate%20RV%20volume%20from%202D%20echo%20slices%20%28with%20an%20error%20of%0A5.75%5C%25%20%29.%20This%20study%20provides%20a%20new%20way%20for%20personalized%203D%20structure%20and%0Afunction%20analysis%20from%20cardiac%20ultrasound%20and%20is%20of%20great%20potential%20in%20clinical%0Apractice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Heart%2520Reconstruction%2520from%2520Sparse%2520Pose-agnostic%25202D%2520Echocardiographic%250A%2520%2520Slices%26entry.906535625%3DZhurong%2520Chen%2520and%2520Jinhua%2520Chen%2520and%2520Wei%2520Zhuo%2520and%2520Wufeng%2520Xue%2520and%2520Dong%2520Ni%26entry.1292438233%3D%2520%2520Echocardiography%2520%2528echo%2529%2520plays%2520an%2520indispensable%2520role%2520in%2520the%2520clinical%2520practice%250Aof%2520heart%2520diseases.%2520However%252C%2520ultrasound%2520imaging%2520typically%2520provides%2520only%250Atwo-dimensional%2520%25282D%2529%2520cross-sectional%2520images%2520from%2520a%2520few%2520specific%2520views%252C%2520making%250Ait%2520challenging%2520to%2520interpret%2520and%2520inaccurate%2520for%2520estimation%2520of%2520clinical%250Aparameters%2520like%2520the%2520volume%2520of%2520left%2520ventricle%2520%2528LV%2529.%25203D%2520ultrasound%2520imaging%250Aprovides%2520an%2520alternative%2520for%25203D%2520quantification%252C%2520but%2520is%2520still%2520limited%2520by%2520the%2520low%250Aspatial%2520and%2520temporal%2520resolution%2520and%2520the%2520highly%2520demanding%2520manual%2520delineation.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520innovative%2520framework%2520for%250Areconstructing%2520personalized%25203D%2520heart%2520anatomy%2520from%25202D%2520echo%2520slices%2520that%2520are%250Afrequently%2520used%2520in%2520clinical%2520practice.%2520Specifically%252C%2520a%2520novel%25203D%2520reconstruction%250Apipeline%2520is%2520designed%252C%2520which%2520alternatively%2520optimizes%2520between%2520the%25203D%2520pose%250Aestimation%2520of%2520these%25202D%2520slices%2520and%2520the%25203D%2520integration%2520of%2520these%2520slices%2520using%2520an%250Aimplicit%2520neural%2520network%252C%2520progressively%2520transforming%2520a%2520prior%25203D%2520heart%2520shape%2520into%250Aa%2520personalized%25203D%2520heart%2520model.%250A%2520%2520We%2520validate%2520the%2520method%2520with%2520two%2520datasets.%2520When%2520six%2520planes%2520are%2520used%252C%2520the%250Areconstructed%25203D%2520heart%2520can%2520lead%2520to%2520a%2520significant%2520improvement%2520for%2520LV%2520volume%250Aestimation%2520over%2520the%2520bi-plane%2520method%2520%2528error%2520in%2520percent%253A%25201.98%255C%2525%2520VS.%252020.24%255C%2525%2529.%2520In%250Aaddition%252C%2520the%2520whole%2520reconstruction%2520framework%2520makes%2520even%2520an%2520important%250Abreakthrough%2520that%2520can%2520estimate%2520RV%2520volume%2520from%25202D%2520echo%2520slices%2520%2528with%2520an%2520error%2520of%250A5.75%255C%2525%2520%2529.%2520This%2520study%2520provides%2520a%2520new%2520way%2520for%2520personalized%25203D%2520structure%2520and%250Afunction%2520analysis%2520from%2520cardiac%2520ultrasound%2520and%2520is%2520of%2520great%2520potential%2520in%2520clinical%250Apractice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Heart%20Reconstruction%20from%20Sparse%20Pose-agnostic%202D%20Echocardiographic%0A%20%20Slices&entry.906535625=Zhurong%20Chen%20and%20Jinhua%20Chen%20and%20Wei%20Zhuo%20and%20Wufeng%20Xue%20and%20Dong%20Ni&entry.1292438233=%20%20Echocardiography%20%28echo%29%20plays%20an%20indispensable%20role%20in%20the%20clinical%20practice%0Aof%20heart%20diseases.%20However%2C%20ultrasound%20imaging%20typically%20provides%20only%0Atwo-dimensional%20%282D%29%20cross-sectional%20images%20from%20a%20few%20specific%20views%2C%20making%0Ait%20challenging%20to%20interpret%20and%20inaccurate%20for%20estimation%20of%20clinical%0Aparameters%20like%20the%20volume%20of%20left%20ventricle%20%28LV%29.%203D%20ultrasound%20imaging%0Aprovides%20an%20alternative%20for%203D%20quantification%2C%20but%20is%20still%20limited%20by%20the%20low%0Aspatial%20and%20temporal%20resolution%20and%20the%20highly%20demanding%20manual%20delineation.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20an%20innovative%20framework%20for%0Areconstructing%20personalized%203D%20heart%20anatomy%20from%202D%20echo%20slices%20that%20are%0Afrequently%20used%20in%20clinical%20practice.%20Specifically%2C%20a%20novel%203D%20reconstruction%0Apipeline%20is%20designed%2C%20which%20alternatively%20optimizes%20between%20the%203D%20pose%0Aestimation%20of%20these%202D%20slices%20and%20the%203D%20integration%20of%20these%20slices%20using%20an%0Aimplicit%20neural%20network%2C%20progressively%20transforming%20a%20prior%203D%20heart%20shape%20into%0Aa%20personalized%203D%20heart%20model.%0A%20%20We%20validate%20the%20method%20with%20two%20datasets.%20When%20six%20planes%20are%20used%2C%20the%0Areconstructed%203D%20heart%20can%20lead%20to%20a%20significant%20improvement%20for%20LV%20volume%0Aestimation%20over%20the%20bi-plane%20method%20%28error%20in%20percent%3A%201.98%5C%25%20VS.%2020.24%5C%25%29.%20In%0Aaddition%2C%20the%20whole%20reconstruction%20framework%20makes%20even%20an%20important%0Abreakthrough%20that%20can%20estimate%20RV%20volume%20from%202D%20echo%20slices%20%28with%20an%20error%20of%0A5.75%5C%25%20%29.%20This%20study%20provides%20a%20new%20way%20for%20personalized%203D%20structure%20and%0Afunction%20analysis%20from%20cardiac%20ultrasound%20and%20is%20of%20great%20potential%20in%20clinical%0Apractice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02411v1&entry.124074799=Read"},
{"title": "MaizeField3D: A Curated 3D Point Cloud and Procedural Model Dataset of\n  Field-Grown Maize from a Diversity Panel", "author": "Elvis Kimara and Mozhgan Hadadi and Jackson Godbersen and Aditya Balu and Talukder Jubery and Yawei Li and Adarsh Krishnamurthy and Patrick S. Schnable and Baskar Ganapathysubramanian", "abstract": "  The development of artificial intelligence (AI) and machine learning (ML)\nbased tools for 3D phenotyping, especially for maize, has been limited due to\nthe lack of large and diverse 3D datasets. 2D image datasets fail to capture\nessential structural details such as leaf architecture, plant volume, and\nspatial arrangements that 3D data provide. To address this limitation, we\npresent MaizeField3D (https://baskargroup.github.io/MaizeField3D/), a curated\ndataset of 3D point clouds of field-grown maize plants from a diverse genetic\npanel, designed to be AI-ready for advancing agricultural research. Our dataset\nincludes 1,045 high-quality point clouds of field-grown maize collected using a\nterrestrial laser scanner (TLS). Point clouds of 520 plants from this dataset\nwere segmented and annotated using a graph-based segmentation method to isolate\nindividual leaves and stalks, ensuring consistent labeling across all samples.\nThis labeled data was then used for fitting procedural models that provide a\nstructured parametric representation of the maize plants. The leaves of the\nmaize plants in the procedural models are represented using Non-Uniform\nRational B-Spline (NURBS) surfaces that were generated using a two-step\noptimization process combining gradient-free and gradient-based methods. We\nconducted rigorous manual quality control on all datasets, correcting errors in\nsegmentation, ensuring accurate leaf ordering, and validating metadata\nannotations. The dataset also includes metadata detailing plant morphology and\nquality, alongside multi-resolution subsampled point cloud data (100k, 50k, 10k\npoints), which can be readily used for different downstream computational\ntasks. MaizeField3D will serve as a comprehensive foundational dataset for\nAI-driven phenotyping, plant structural analysis, and 3D applications in\nagricultural research.\n", "link": "http://arxiv.org/abs/2503.07813v3", "date": "2025-07-03", "relevancy": 2.7306, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5585}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaizeField3D%3A%20A%20Curated%203D%20Point%20Cloud%20and%20Procedural%20Model%20Dataset%20of%0A%20%20Field-Grown%20Maize%20from%20a%20Diversity%20Panel&body=Title%3A%20MaizeField3D%3A%20A%20Curated%203D%20Point%20Cloud%20and%20Procedural%20Model%20Dataset%20of%0A%20%20Field-Grown%20Maize%20from%20a%20Diversity%20Panel%0AAuthor%3A%20Elvis%20Kimara%20and%20Mozhgan%20Hadadi%20and%20Jackson%20Godbersen%20and%20Aditya%20Balu%20and%20Talukder%20Jubery%20and%20Yawei%20Li%20and%20Adarsh%20Krishnamurthy%20and%20Patrick%20S.%20Schnable%20and%20Baskar%20Ganapathysubramanian%0AAbstract%3A%20%20%20The%20development%20of%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%20%28ML%29%0Abased%20tools%20for%203D%20phenotyping%2C%20especially%20for%20maize%2C%20has%20been%20limited%20due%20to%0Athe%20lack%20of%20large%20and%20diverse%203D%20datasets.%202D%20image%20datasets%20fail%20to%20capture%0Aessential%20structural%20details%20such%20as%20leaf%20architecture%2C%20plant%20volume%2C%20and%0Aspatial%20arrangements%20that%203D%20data%20provide.%20To%20address%20this%20limitation%2C%20we%0Apresent%20MaizeField3D%20%28https%3A//baskargroup.github.io/MaizeField3D/%29%2C%20a%20curated%0Adataset%20of%203D%20point%20clouds%20of%20field-grown%20maize%20plants%20from%20a%20diverse%20genetic%0Apanel%2C%20designed%20to%20be%20AI-ready%20for%20advancing%20agricultural%20research.%20Our%20dataset%0Aincludes%201%2C045%20high-quality%20point%20clouds%20of%20field-grown%20maize%20collected%20using%20a%0Aterrestrial%20laser%20scanner%20%28TLS%29.%20Point%20clouds%20of%20520%20plants%20from%20this%20dataset%0Awere%20segmented%20and%20annotated%20using%20a%20graph-based%20segmentation%20method%20to%20isolate%0Aindividual%20leaves%20and%20stalks%2C%20ensuring%20consistent%20labeling%20across%20all%20samples.%0AThis%20labeled%20data%20was%20then%20used%20for%20fitting%20procedural%20models%20that%20provide%20a%0Astructured%20parametric%20representation%20of%20the%20maize%20plants.%20The%20leaves%20of%20the%0Amaize%20plants%20in%20the%20procedural%20models%20are%20represented%20using%20Non-Uniform%0ARational%20B-Spline%20%28NURBS%29%20surfaces%20that%20were%20generated%20using%20a%20two-step%0Aoptimization%20process%20combining%20gradient-free%20and%20gradient-based%20methods.%20We%0Aconducted%20rigorous%20manual%20quality%20control%20on%20all%20datasets%2C%20correcting%20errors%20in%0Asegmentation%2C%20ensuring%20accurate%20leaf%20ordering%2C%20and%20validating%20metadata%0Aannotations.%20The%20dataset%20also%20includes%20metadata%20detailing%20plant%20morphology%20and%0Aquality%2C%20alongside%20multi-resolution%20subsampled%20point%20cloud%20data%20%28100k%2C%2050k%2C%2010k%0Apoints%29%2C%20which%20can%20be%20readily%20used%20for%20different%20downstream%20computational%0Atasks.%20MaizeField3D%20will%20serve%20as%20a%20comprehensive%20foundational%20dataset%20for%0AAI-driven%20phenotyping%2C%20plant%20structural%20analysis%2C%20and%203D%20applications%20in%0Aagricultural%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07813v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaizeField3D%253A%2520A%2520Curated%25203D%2520Point%2520Cloud%2520and%2520Procedural%2520Model%2520Dataset%2520of%250A%2520%2520Field-Grown%2520Maize%2520from%2520a%2520Diversity%2520Panel%26entry.906535625%3DElvis%2520Kimara%2520and%2520Mozhgan%2520Hadadi%2520and%2520Jackson%2520Godbersen%2520and%2520Aditya%2520Balu%2520and%2520Talukder%2520Jubery%2520and%2520Yawei%2520Li%2520and%2520Adarsh%2520Krishnamurthy%2520and%2520Patrick%2520S.%2520Schnable%2520and%2520Baskar%2520Ganapathysubramanian%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520and%2520machine%2520learning%2520%2528ML%2529%250Abased%2520tools%2520for%25203D%2520phenotyping%252C%2520especially%2520for%2520maize%252C%2520has%2520been%2520limited%2520due%2520to%250Athe%2520lack%2520of%2520large%2520and%2520diverse%25203D%2520datasets.%25202D%2520image%2520datasets%2520fail%2520to%2520capture%250Aessential%2520structural%2520details%2520such%2520as%2520leaf%2520architecture%252C%2520plant%2520volume%252C%2520and%250Aspatial%2520arrangements%2520that%25203D%2520data%2520provide.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apresent%2520MaizeField3D%2520%2528https%253A//baskargroup.github.io/MaizeField3D/%2529%252C%2520a%2520curated%250Adataset%2520of%25203D%2520point%2520clouds%2520of%2520field-grown%2520maize%2520plants%2520from%2520a%2520diverse%2520genetic%250Apanel%252C%2520designed%2520to%2520be%2520AI-ready%2520for%2520advancing%2520agricultural%2520research.%2520Our%2520dataset%250Aincludes%25201%252C045%2520high-quality%2520point%2520clouds%2520of%2520field-grown%2520maize%2520collected%2520using%2520a%250Aterrestrial%2520laser%2520scanner%2520%2528TLS%2529.%2520Point%2520clouds%2520of%2520520%2520plants%2520from%2520this%2520dataset%250Awere%2520segmented%2520and%2520annotated%2520using%2520a%2520graph-based%2520segmentation%2520method%2520to%2520isolate%250Aindividual%2520leaves%2520and%2520stalks%252C%2520ensuring%2520consistent%2520labeling%2520across%2520all%2520samples.%250AThis%2520labeled%2520data%2520was%2520then%2520used%2520for%2520fitting%2520procedural%2520models%2520that%2520provide%2520a%250Astructured%2520parametric%2520representation%2520of%2520the%2520maize%2520plants.%2520The%2520leaves%2520of%2520the%250Amaize%2520plants%2520in%2520the%2520procedural%2520models%2520are%2520represented%2520using%2520Non-Uniform%250ARational%2520B-Spline%2520%2528NURBS%2529%2520surfaces%2520that%2520were%2520generated%2520using%2520a%2520two-step%250Aoptimization%2520process%2520combining%2520gradient-free%2520and%2520gradient-based%2520methods.%2520We%250Aconducted%2520rigorous%2520manual%2520quality%2520control%2520on%2520all%2520datasets%252C%2520correcting%2520errors%2520in%250Asegmentation%252C%2520ensuring%2520accurate%2520leaf%2520ordering%252C%2520and%2520validating%2520metadata%250Aannotations.%2520The%2520dataset%2520also%2520includes%2520metadata%2520detailing%2520plant%2520morphology%2520and%250Aquality%252C%2520alongside%2520multi-resolution%2520subsampled%2520point%2520cloud%2520data%2520%2528100k%252C%252050k%252C%252010k%250Apoints%2529%252C%2520which%2520can%2520be%2520readily%2520used%2520for%2520different%2520downstream%2520computational%250Atasks.%2520MaizeField3D%2520will%2520serve%2520as%2520a%2520comprehensive%2520foundational%2520dataset%2520for%250AAI-driven%2520phenotyping%252C%2520plant%2520structural%2520analysis%252C%2520and%25203D%2520applications%2520in%250Aagricultural%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07813v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaizeField3D%3A%20A%20Curated%203D%20Point%20Cloud%20and%20Procedural%20Model%20Dataset%20of%0A%20%20Field-Grown%20Maize%20from%20a%20Diversity%20Panel&entry.906535625=Elvis%20Kimara%20and%20Mozhgan%20Hadadi%20and%20Jackson%20Godbersen%20and%20Aditya%20Balu%20and%20Talukder%20Jubery%20and%20Yawei%20Li%20and%20Adarsh%20Krishnamurthy%20and%20Patrick%20S.%20Schnable%20and%20Baskar%20Ganapathysubramanian&entry.1292438233=%20%20The%20development%20of%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%20%28ML%29%0Abased%20tools%20for%203D%20phenotyping%2C%20especially%20for%20maize%2C%20has%20been%20limited%20due%20to%0Athe%20lack%20of%20large%20and%20diverse%203D%20datasets.%202D%20image%20datasets%20fail%20to%20capture%0Aessential%20structural%20details%20such%20as%20leaf%20architecture%2C%20plant%20volume%2C%20and%0Aspatial%20arrangements%20that%203D%20data%20provide.%20To%20address%20this%20limitation%2C%20we%0Apresent%20MaizeField3D%20%28https%3A//baskargroup.github.io/MaizeField3D/%29%2C%20a%20curated%0Adataset%20of%203D%20point%20clouds%20of%20field-grown%20maize%20plants%20from%20a%20diverse%20genetic%0Apanel%2C%20designed%20to%20be%20AI-ready%20for%20advancing%20agricultural%20research.%20Our%20dataset%0Aincludes%201%2C045%20high-quality%20point%20clouds%20of%20field-grown%20maize%20collected%20using%20a%0Aterrestrial%20laser%20scanner%20%28TLS%29.%20Point%20clouds%20of%20520%20plants%20from%20this%20dataset%0Awere%20segmented%20and%20annotated%20using%20a%20graph-based%20segmentation%20method%20to%20isolate%0Aindividual%20leaves%20and%20stalks%2C%20ensuring%20consistent%20labeling%20across%20all%20samples.%0AThis%20labeled%20data%20was%20then%20used%20for%20fitting%20procedural%20models%20that%20provide%20a%0Astructured%20parametric%20representation%20of%20the%20maize%20plants.%20The%20leaves%20of%20the%0Amaize%20plants%20in%20the%20procedural%20models%20are%20represented%20using%20Non-Uniform%0ARational%20B-Spline%20%28NURBS%29%20surfaces%20that%20were%20generated%20using%20a%20two-step%0Aoptimization%20process%20combining%20gradient-free%20and%20gradient-based%20methods.%20We%0Aconducted%20rigorous%20manual%20quality%20control%20on%20all%20datasets%2C%20correcting%20errors%20in%0Asegmentation%2C%20ensuring%20accurate%20leaf%20ordering%2C%20and%20validating%20metadata%0Aannotations.%20The%20dataset%20also%20includes%20metadata%20detailing%20plant%20morphology%20and%0Aquality%2C%20alongside%20multi-resolution%20subsampled%20point%20cloud%20data%20%28100k%2C%2050k%2C%2010k%0Apoints%29%2C%20which%20can%20be%20readily%20used%20for%20different%20downstream%20computational%0Atasks.%20MaizeField3D%20will%20serve%20as%20a%20comprehensive%20foundational%20dataset%20for%0AAI-driven%20phenotyping%2C%20plant%20structural%20analysis%2C%20and%203D%20applications%20in%0Aagricultural%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07813v3&entry.124074799=Read"},
{"title": "Lightweight Structure-Aware Attention for Visual Understanding", "author": "Heeseung Kwon and Francisco M. Castro and Manuel J. Marin-Jimenez and Nicolas Guil and Karteek Alahari", "abstract": "  Attention operator has been widely used as a basic brick in visual\nunderstanding since it provides some flexibility through its adjustable\nkernels. However, this operator suffers from inherent limitations: (1) the\nattention kernel is not discriminative enough, resulting in high redundancy,\nand (2) the complexity in computation and memory is quadratic in the sequence\nlength. In this paper, we propose a novel attention operator, called\nLightweight Structure-aware Attention (LiSA), which has a better representation\npower with log-linear complexity. Our operator transforms the attention kernels\nto be more discriminative by learning structural patterns. These structural\npatterns are encoded by exploiting a set of relative position embeddings (RPEs)\nas multiplicative weights, thereby improving the representation power of the\nattention kernels. Additionally, the RPEs are approximated to obtain log-linear\ncomplexity. Our experiments and analyses demonstrate that the proposed operator\noutperforms self-attention and other existing operators, achieving\nstate-of-the-art results on ImageNet-1K and other downstream tasks such as\nvideo action recognition on Kinetics-400, object detection \\& instance\nsegmentation on COCO, and semantic segmentation on ADE-20K.\n", "link": "http://arxiv.org/abs/2211.16289v2", "date": "2025-07-03", "relevancy": 2.6882, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Structure-Aware%20Attention%20for%20Visual%20Understanding&body=Title%3A%20Lightweight%20Structure-Aware%20Attention%20for%20Visual%20Understanding%0AAuthor%3A%20Heeseung%20Kwon%20and%20Francisco%20M.%20Castro%20and%20Manuel%20J.%20Marin-Jimenez%20and%20Nicolas%20Guil%20and%20Karteek%20Alahari%0AAbstract%3A%20%20%20Attention%20operator%20has%20been%20widely%20used%20as%20a%20basic%20brick%20in%20visual%0Aunderstanding%20since%20it%20provides%20some%20flexibility%20through%20its%20adjustable%0Akernels.%20However%2C%20this%20operator%20suffers%20from%20inherent%20limitations%3A%20%281%29%20the%0Aattention%20kernel%20is%20not%20discriminative%20enough%2C%20resulting%20in%20high%20redundancy%2C%0Aand%20%282%29%20the%20complexity%20in%20computation%20and%20memory%20is%20quadratic%20in%20the%20sequence%0Alength.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20attention%20operator%2C%20called%0ALightweight%20Structure-aware%20Attention%20%28LiSA%29%2C%20which%20has%20a%20better%20representation%0Apower%20with%20log-linear%20complexity.%20Our%20operator%20transforms%20the%20attention%20kernels%0Ato%20be%20more%20discriminative%20by%20learning%20structural%20patterns.%20These%20structural%0Apatterns%20are%20encoded%20by%20exploiting%20a%20set%20of%20relative%20position%20embeddings%20%28RPEs%29%0Aas%20multiplicative%20weights%2C%20thereby%20improving%20the%20representation%20power%20of%20the%0Aattention%20kernels.%20Additionally%2C%20the%20RPEs%20are%20approximated%20to%20obtain%20log-linear%0Acomplexity.%20Our%20experiments%20and%20analyses%20demonstrate%20that%20the%20proposed%20operator%0Aoutperforms%20self-attention%20and%20other%20existing%20operators%2C%20achieving%0Astate-of-the-art%20results%20on%20ImageNet-1K%20and%20other%20downstream%20tasks%20such%20as%0Avideo%20action%20recognition%20on%20Kinetics-400%2C%20object%20detection%20%5C%26%20instance%0Asegmentation%20on%20COCO%2C%20and%20semantic%20segmentation%20on%20ADE-20K.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.16289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Structure-Aware%2520Attention%2520for%2520Visual%2520Understanding%26entry.906535625%3DHeeseung%2520Kwon%2520and%2520Francisco%2520M.%2520Castro%2520and%2520Manuel%2520J.%2520Marin-Jimenez%2520and%2520Nicolas%2520Guil%2520and%2520Karteek%2520Alahari%26entry.1292438233%3D%2520%2520Attention%2520operator%2520has%2520been%2520widely%2520used%2520as%2520a%2520basic%2520brick%2520in%2520visual%250Aunderstanding%2520since%2520it%2520provides%2520some%2520flexibility%2520through%2520its%2520adjustable%250Akernels.%2520However%252C%2520this%2520operator%2520suffers%2520from%2520inherent%2520limitations%253A%2520%25281%2529%2520the%250Aattention%2520kernel%2520is%2520not%2520discriminative%2520enough%252C%2520resulting%2520in%2520high%2520redundancy%252C%250Aand%2520%25282%2529%2520the%2520complexity%2520in%2520computation%2520and%2520memory%2520is%2520quadratic%2520in%2520the%2520sequence%250Alength.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520attention%2520operator%252C%2520called%250ALightweight%2520Structure-aware%2520Attention%2520%2528LiSA%2529%252C%2520which%2520has%2520a%2520better%2520representation%250Apower%2520with%2520log-linear%2520complexity.%2520Our%2520operator%2520transforms%2520the%2520attention%2520kernels%250Ato%2520be%2520more%2520discriminative%2520by%2520learning%2520structural%2520patterns.%2520These%2520structural%250Apatterns%2520are%2520encoded%2520by%2520exploiting%2520a%2520set%2520of%2520relative%2520position%2520embeddings%2520%2528RPEs%2529%250Aas%2520multiplicative%2520weights%252C%2520thereby%2520improving%2520the%2520representation%2520power%2520of%2520the%250Aattention%2520kernels.%2520Additionally%252C%2520the%2520RPEs%2520are%2520approximated%2520to%2520obtain%2520log-linear%250Acomplexity.%2520Our%2520experiments%2520and%2520analyses%2520demonstrate%2520that%2520the%2520proposed%2520operator%250Aoutperforms%2520self-attention%2520and%2520other%2520existing%2520operators%252C%2520achieving%250Astate-of-the-art%2520results%2520on%2520ImageNet-1K%2520and%2520other%2520downstream%2520tasks%2520such%2520as%250Avideo%2520action%2520recognition%2520on%2520Kinetics-400%252C%2520object%2520detection%2520%255C%2526%2520instance%250Asegmentation%2520on%2520COCO%252C%2520and%2520semantic%2520segmentation%2520on%2520ADE-20K.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.16289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Structure-Aware%20Attention%20for%20Visual%20Understanding&entry.906535625=Heeseung%20Kwon%20and%20Francisco%20M.%20Castro%20and%20Manuel%20J.%20Marin-Jimenez%20and%20Nicolas%20Guil%20and%20Karteek%20Alahari&entry.1292438233=%20%20Attention%20operator%20has%20been%20widely%20used%20as%20a%20basic%20brick%20in%20visual%0Aunderstanding%20since%20it%20provides%20some%20flexibility%20through%20its%20adjustable%0Akernels.%20However%2C%20this%20operator%20suffers%20from%20inherent%20limitations%3A%20%281%29%20the%0Aattention%20kernel%20is%20not%20discriminative%20enough%2C%20resulting%20in%20high%20redundancy%2C%0Aand%20%282%29%20the%20complexity%20in%20computation%20and%20memory%20is%20quadratic%20in%20the%20sequence%0Alength.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20attention%20operator%2C%20called%0ALightweight%20Structure-aware%20Attention%20%28LiSA%29%2C%20which%20has%20a%20better%20representation%0Apower%20with%20log-linear%20complexity.%20Our%20operator%20transforms%20the%20attention%20kernels%0Ato%20be%20more%20discriminative%20by%20learning%20structural%20patterns.%20These%20structural%0Apatterns%20are%20encoded%20by%20exploiting%20a%20set%20of%20relative%20position%20embeddings%20%28RPEs%29%0Aas%20multiplicative%20weights%2C%20thereby%20improving%20the%20representation%20power%20of%20the%0Aattention%20kernels.%20Additionally%2C%20the%20RPEs%20are%20approximated%20to%20obtain%20log-linear%0Acomplexity.%20Our%20experiments%20and%20analyses%20demonstrate%20that%20the%20proposed%20operator%0Aoutperforms%20self-attention%20and%20other%20existing%20operators%2C%20achieving%0Astate-of-the-art%20results%20on%20ImageNet-1K%20and%20other%20downstream%20tasks%20such%20as%0Avideo%20action%20recognition%20on%20Kinetics-400%2C%20object%20detection%20%5C%26%20instance%0Asegmentation%20on%20COCO%2C%20and%20semantic%20segmentation%20on%20ADE-20K.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.16289v2&entry.124074799=Read"},
{"title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs", "author": "Hao Wang and Pinzhi Huang and Jihan Yang and Saining Xie and Daisuke Kawahara", "abstract": "  The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.\n", "link": "http://arxiv.org/abs/2505.15075v2", "date": "2025-07-03", "relevancy": 2.6017, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Traveling%20Across%20Languages%3A%20Benchmarking%20Cross-Lingual%20Consistency%20in%0A%20%20Multimodal%20LLMs&body=Title%3A%20Traveling%20Across%20Languages%3A%20Benchmarking%20Cross-Lingual%20Consistency%20in%0A%20%20Multimodal%20LLMs%0AAuthor%3A%20Hao%20Wang%20and%20Pinzhi%20Huang%20and%20Jihan%20Yang%20and%20Saining%20Xie%20and%20Daisuke%20Kawahara%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20has%0Asignificantly%20enhanced%20their%20real-world%20applications.%20However%2C%20achieving%0Aconsistent%20performance%20across%20languages%2C%20especially%20when%20integrating%20cultural%0Aknowledge%2C%20remains%20a%20significant%20challenge.%20To%20better%20assess%20this%20issue%2C%20we%0Aintroduce%20two%20new%20benchmarks%3A%20KnowRecall%20and%20VisRecall%2C%20which%20evaluate%0Across-lingual%20consistency%20in%20MLLMs.%20KnowRecall%20is%20a%20visual%20question%20answering%0Abenchmark%20designed%20to%20measure%20factual%20knowledge%20consistency%20in%2015%20languages%2C%0Afocusing%20on%20cultural%20and%20historical%20questions%20about%20global%20landmarks.%20VisRecall%0Aassesses%20visual%20memory%20consistency%20by%20asking%20models%20to%20describe%20landmark%0Aappearances%20in%209%20languages%20without%20access%20to%20images.%20Experimental%20results%0Areveal%20that%20state-of-the-art%20MLLMs%2C%20including%20proprietary%20ones%2C%20still%20struggle%0Ato%20achieve%20cross-lingual%20consistency.%20This%20underscores%20the%20need%20for%20more%20robust%0Aapproaches%20that%20produce%20truly%20multilingual%20and%20culturally%20aware%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraveling%2520Across%2520Languages%253A%2520Benchmarking%2520Cross-Lingual%2520Consistency%2520in%250A%2520%2520Multimodal%2520LLMs%26entry.906535625%3DHao%2520Wang%2520and%2520Pinzhi%2520Huang%2520and%2520Jihan%2520Yang%2520and%2520Saining%2520Xie%2520and%2520Daisuke%2520Kawahara%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520has%250Asignificantly%2520enhanced%2520their%2520real-world%2520applications.%2520However%252C%2520achieving%250Aconsistent%2520performance%2520across%2520languages%252C%2520especially%2520when%2520integrating%2520cultural%250Aknowledge%252C%2520remains%2520a%2520significant%2520challenge.%2520To%2520better%2520assess%2520this%2520issue%252C%2520we%250Aintroduce%2520two%2520new%2520benchmarks%253A%2520KnowRecall%2520and%2520VisRecall%252C%2520which%2520evaluate%250Across-lingual%2520consistency%2520in%2520MLLMs.%2520KnowRecall%2520is%2520a%2520visual%2520question%2520answering%250Abenchmark%2520designed%2520to%2520measure%2520factual%2520knowledge%2520consistency%2520in%252015%2520languages%252C%250Afocusing%2520on%2520cultural%2520and%2520historical%2520questions%2520about%2520global%2520landmarks.%2520VisRecall%250Aassesses%2520visual%2520memory%2520consistency%2520by%2520asking%2520models%2520to%2520describe%2520landmark%250Aappearances%2520in%25209%2520languages%2520without%2520access%2520to%2520images.%2520Experimental%2520results%250Areveal%2520that%2520state-of-the-art%2520MLLMs%252C%2520including%2520proprietary%2520ones%252C%2520still%2520struggle%250Ato%2520achieve%2520cross-lingual%2520consistency.%2520This%2520underscores%2520the%2520need%2520for%2520more%2520robust%250Aapproaches%2520that%2520produce%2520truly%2520multilingual%2520and%2520culturally%2520aware%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traveling%20Across%20Languages%3A%20Benchmarking%20Cross-Lingual%20Consistency%20in%0A%20%20Multimodal%20LLMs&entry.906535625=Hao%20Wang%20and%20Pinzhi%20Huang%20and%20Jihan%20Yang%20and%20Saining%20Xie%20and%20Daisuke%20Kawahara&entry.1292438233=%20%20The%20rapid%20evolution%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20has%0Asignificantly%20enhanced%20their%20real-world%20applications.%20However%2C%20achieving%0Aconsistent%20performance%20across%20languages%2C%20especially%20when%20integrating%20cultural%0Aknowledge%2C%20remains%20a%20significant%20challenge.%20To%20better%20assess%20this%20issue%2C%20we%0Aintroduce%20two%20new%20benchmarks%3A%20KnowRecall%20and%20VisRecall%2C%20which%20evaluate%0Across-lingual%20consistency%20in%20MLLMs.%20KnowRecall%20is%20a%20visual%20question%20answering%0Abenchmark%20designed%20to%20measure%20factual%20knowledge%20consistency%20in%2015%20languages%2C%0Afocusing%20on%20cultural%20and%20historical%20questions%20about%20global%20landmarks.%20VisRecall%0Aassesses%20visual%20memory%20consistency%20by%20asking%20models%20to%20describe%20landmark%0Aappearances%20in%209%20languages%20without%20access%20to%20images.%20Experimental%20results%0Areveal%20that%20state-of-the-art%20MLLMs%2C%20including%20proprietary%20ones%2C%20still%20struggle%0Ato%20achieve%20cross-lingual%20consistency.%20This%20underscores%20the%20need%20for%20more%20robust%0Aapproaches%20that%20produce%20truly%20multilingual%20and%20culturally%20aware%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15075v2&entry.124074799=Read"},
{"title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration", "author": "Shaocheng Yan and Pengcheng Shi and Zhenjun Zhao and Kaixin Wang and Kuang Cao and Ji Wu and Jiayuan Li", "abstract": "  Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}.\n", "link": "http://arxiv.org/abs/2507.01439v2", "date": "2025-07-03", "relevancy": 2.5972, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5417}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5109}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TurboReg%3A%20TurboClique%20for%20Robust%20and%20Efficient%20Point%20Cloud%20Registration&body=Title%3A%20TurboReg%3A%20TurboClique%20for%20Robust%20and%20Efficient%20Point%20Cloud%20Registration%0AAuthor%3A%20Shaocheng%20Yan%20and%20Pengcheng%20Shi%20and%20Zhenjun%20Zhao%20and%20Kaixin%20Wang%20and%20Kuang%20Cao%20and%20Ji%20Wu%20and%20Jiayuan%20Li%0AAbstract%3A%20%20%20Robust%20estimation%20is%20essential%20in%20correspondence-based%20Point%20Cloud%0ARegistration%20%28PCR%29.%20Existing%20methods%20using%20maximal%20clique%20search%20in%0Acompatibility%20graphs%20achieve%20high%20recall%20but%20suffer%20from%20exponential%20time%0Acomplexity%2C%20limiting%20their%20use%20in%20time-sensitive%20applications.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20fast%20and%20robust%20estimator%2C%20TurboReg%2C%20built%20upon%20a%20novel%0Alightweight%20clique%2C%20TurboClique%2C%20and%20a%20highly%20parallelizable%20Pivot-Guided%0ASearch%20%28PGS%29%20algorithm.%20First%2C%20we%20define%20the%20TurboClique%20as%20a%203-clique%20within%20a%0Ahighly-constrained%20compatibility%20graph.%20The%20lightweight%20nature%20of%20the%203-clique%0Aallows%20for%20efficient%20parallel%20searching%2C%20and%20the%20highly-constrained%0Acompatibility%20graph%20ensures%20robust%20spatial%20consistency%20for%20stable%0Atransformation%20estimation.%20Next%2C%20PGS%20selects%20matching%20pairs%20with%20high%20SC%24%5E2%24%0Ascores%20as%20pivots%2C%20effectively%20guiding%20the%20search%20toward%20TurboCliques%20with%0Ahigher%20inlier%20ratios.%20Moreover%2C%20the%20PGS%20algorithm%20has%20linear%20time%20complexity%0Aand%20is%20significantly%20more%20efficient%20than%20the%20maximal%20clique%20search%20with%0Aexponential%20time%20complexity.%20Extensive%20experiments%20show%20that%20TurboReg%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20real-world%20datasets%2C%20with%0Asubstantial%20speed%20improvements.%20For%20example%2C%20on%20the%203DMatch%2BFCGF%20dataset%2C%0ATurboReg%20%281K%29%20operates%20%24208.22%5Ctimes%24%20faster%20than%203DMAC%20while%20also%20achieving%0Ahigher%20recall.%20Our%20code%20is%20accessible%20at%0A%5Chref%7Bhttps%3A//github.com/Laka-3DV/TurboReg%7D%7B%5Ctexttt%7BTurboReg%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurboReg%253A%2520TurboClique%2520for%2520Robust%2520and%2520Efficient%2520Point%2520Cloud%2520Registration%26entry.906535625%3DShaocheng%2520Yan%2520and%2520Pengcheng%2520Shi%2520and%2520Zhenjun%2520Zhao%2520and%2520Kaixin%2520Wang%2520and%2520Kuang%2520Cao%2520and%2520Ji%2520Wu%2520and%2520Jiayuan%2520Li%26entry.1292438233%3D%2520%2520Robust%2520estimation%2520is%2520essential%2520in%2520correspondence-based%2520Point%2520Cloud%250ARegistration%2520%2528PCR%2529.%2520Existing%2520methods%2520using%2520maximal%2520clique%2520search%2520in%250Acompatibility%2520graphs%2520achieve%2520high%2520recall%2520but%2520suffer%2520from%2520exponential%2520time%250Acomplexity%252C%2520limiting%2520their%2520use%2520in%2520time-sensitive%2520applications.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520a%2520fast%2520and%2520robust%2520estimator%252C%2520TurboReg%252C%2520built%2520upon%2520a%2520novel%250Alightweight%2520clique%252C%2520TurboClique%252C%2520and%2520a%2520highly%2520parallelizable%2520Pivot-Guided%250ASearch%2520%2528PGS%2529%2520algorithm.%2520First%252C%2520we%2520define%2520the%2520TurboClique%2520as%2520a%25203-clique%2520within%2520a%250Ahighly-constrained%2520compatibility%2520graph.%2520The%2520lightweight%2520nature%2520of%2520the%25203-clique%250Aallows%2520for%2520efficient%2520parallel%2520searching%252C%2520and%2520the%2520highly-constrained%250Acompatibility%2520graph%2520ensures%2520robust%2520spatial%2520consistency%2520for%2520stable%250Atransformation%2520estimation.%2520Next%252C%2520PGS%2520selects%2520matching%2520pairs%2520with%2520high%2520SC%2524%255E2%2524%250Ascores%2520as%2520pivots%252C%2520effectively%2520guiding%2520the%2520search%2520toward%2520TurboCliques%2520with%250Ahigher%2520inlier%2520ratios.%2520Moreover%252C%2520the%2520PGS%2520algorithm%2520has%2520linear%2520time%2520complexity%250Aand%2520is%2520significantly%2520more%2520efficient%2520than%2520the%2520maximal%2520clique%2520search%2520with%250Aexponential%2520time%2520complexity.%2520Extensive%2520experiments%2520show%2520that%2520TurboReg%2520achieves%250Astate-of-the-art%2520performance%2520across%2520multiple%2520real-world%2520datasets%252C%2520with%250Asubstantial%2520speed%2520improvements.%2520For%2520example%252C%2520on%2520the%25203DMatch%252BFCGF%2520dataset%252C%250ATurboReg%2520%25281K%2529%2520operates%2520%2524208.22%255Ctimes%2524%2520faster%2520than%25203DMAC%2520while%2520also%2520achieving%250Ahigher%2520recall.%2520Our%2520code%2520is%2520accessible%2520at%250A%255Chref%257Bhttps%253A//github.com/Laka-3DV/TurboReg%257D%257B%255Ctexttt%257BTurboReg%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TurboReg%3A%20TurboClique%20for%20Robust%20and%20Efficient%20Point%20Cloud%20Registration&entry.906535625=Shaocheng%20Yan%20and%20Pengcheng%20Shi%20and%20Zhenjun%20Zhao%20and%20Kaixin%20Wang%20and%20Kuang%20Cao%20and%20Ji%20Wu%20and%20Jiayuan%20Li&entry.1292438233=%20%20Robust%20estimation%20is%20essential%20in%20correspondence-based%20Point%20Cloud%0ARegistration%20%28PCR%29.%20Existing%20methods%20using%20maximal%20clique%20search%20in%0Acompatibility%20graphs%20achieve%20high%20recall%20but%20suffer%20from%20exponential%20time%0Acomplexity%2C%20limiting%20their%20use%20in%20time-sensitive%20applications.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20fast%20and%20robust%20estimator%2C%20TurboReg%2C%20built%20upon%20a%20novel%0Alightweight%20clique%2C%20TurboClique%2C%20and%20a%20highly%20parallelizable%20Pivot-Guided%0ASearch%20%28PGS%29%20algorithm.%20First%2C%20we%20define%20the%20TurboClique%20as%20a%203-clique%20within%20a%0Ahighly-constrained%20compatibility%20graph.%20The%20lightweight%20nature%20of%20the%203-clique%0Aallows%20for%20efficient%20parallel%20searching%2C%20and%20the%20highly-constrained%0Acompatibility%20graph%20ensures%20robust%20spatial%20consistency%20for%20stable%0Atransformation%20estimation.%20Next%2C%20PGS%20selects%20matching%20pairs%20with%20high%20SC%24%5E2%24%0Ascores%20as%20pivots%2C%20effectively%20guiding%20the%20search%20toward%20TurboCliques%20with%0Ahigher%20inlier%20ratios.%20Moreover%2C%20the%20PGS%20algorithm%20has%20linear%20time%20complexity%0Aand%20is%20significantly%20more%20efficient%20than%20the%20maximal%20clique%20search%20with%0Aexponential%20time%20complexity.%20Extensive%20experiments%20show%20that%20TurboReg%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20real-world%20datasets%2C%20with%0Asubstantial%20speed%20improvements.%20For%20example%2C%20on%20the%203DMatch%2BFCGF%20dataset%2C%0ATurboReg%20%281K%29%20operates%20%24208.22%5Ctimes%24%20faster%20than%203DMAC%20while%20also%20achieving%0Ahigher%20recall.%20Our%20code%20is%20accessible%20at%0A%5Chref%7Bhttps%3A//github.com/Laka-3DV/TurboReg%7D%7B%5Ctexttt%7BTurboReg%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01439v2&entry.124074799=Read"},
{"title": "Direct Preference Optimization Using Sparse Feature-Level Constraints", "author": "Qingyu Yin and Chak Tou Leong and Hongbo Zhang and Minjun Zhu and Hanqi Yan and Qiang Zhang and Yulan He and Wenjie Li and Jun Wang and Yue Zhang and Linyi Yang", "abstract": "  The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.\n", "link": "http://arxiv.org/abs/2411.07618v2", "date": "2025-07-03", "relevancy": 2.5929, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Preference%20Optimization%20Using%20Sparse%20Feature-Level%20Constraints&body=Title%3A%20Direct%20Preference%20Optimization%20Using%20Sparse%20Feature-Level%20Constraints%0AAuthor%3A%20Qingyu%20Yin%20and%20Chak%20Tou%20Leong%20and%20Hongbo%20Zhang%20and%20Minjun%20Zhu%20and%20Hanqi%20Yan%20and%20Qiang%20Zhang%20and%20Yulan%20He%20and%20Wenjie%20Li%20and%20Jun%20Wang%20and%20Yue%20Zhang%20and%20Linyi%20Yang%0AAbstract%3A%20%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences%20remains%0Aa%20key%20challenge.%20While%20post-training%20techniques%20like%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20have%0Aachieved%20notable%20success%2C%20they%20often%20introduce%20computational%20inefficiencies%20and%0Atraining%20instability.%20In%20this%20paper%2C%20we%20propose%20Feature-level%20constrained%0APreference%20Optimization%20%28FPO%29%2C%20a%20novel%20method%20designed%20to%20simplify%20the%0Aalignment%20process%20while%20ensuring%20stability.%20FPO%20leverages%20pre-trained%20Sparse%0AAutoencoders%20%28SAEs%29%20and%20introduces%20feature-level%20constraints%2C%20allowing%20for%0Aefficient%2C%20sparsity-enforced%20alignment.%20Our%20approach%20enjoys%20efficiency%20by%20using%0Asparse%20features%20activated%20in%20a%20well-trained%20sparse%20autoencoder%20and%20the%20quality%0Aof%20sequential%20KL%20divergence%20by%20using%20the%20feature-level%20offline%20reference.%0AExperimental%20results%20on%20benchmark%20datasets%20demonstrate%20that%20FPO%20achieves%20a%0A5.08%25%20absolute%20improvement%20in%20win%20rate%20with%20much%20lower%20computational%20cost%0Acompared%20to%20state-of-the-art%20baselines%2C%20making%20it%20a%20promising%20solution%20for%0Aefficient%20and%20controllable%20LLM%20alignments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Preference%2520Optimization%2520Using%2520Sparse%2520Feature-Level%2520Constraints%26entry.906535625%3DQingyu%2520Yin%2520and%2520Chak%2520Tou%2520Leong%2520and%2520Hongbo%2520Zhang%2520and%2520Minjun%2520Zhu%2520and%2520Hanqi%2520Yan%2520and%2520Qiang%2520Zhang%2520and%2520Yulan%2520He%2520and%2520Wenjie%2520Li%2520and%2520Jun%2520Wang%2520and%2520Yue%2520Zhang%2520and%2520Linyi%2520Yang%26entry.1292438233%3D%2520%2520The%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520preferences%2520remains%250Aa%2520key%2520challenge.%2520While%2520post-training%2520techniques%2520like%2520Reinforcement%2520Learning%250Afrom%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520have%250Aachieved%2520notable%2520success%252C%2520they%2520often%2520introduce%2520computational%2520inefficiencies%2520and%250Atraining%2520instability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Feature-level%2520constrained%250APreference%2520Optimization%2520%2528FPO%2529%252C%2520a%2520novel%2520method%2520designed%2520to%2520simplify%2520the%250Aalignment%2520process%2520while%2520ensuring%2520stability.%2520FPO%2520leverages%2520pre-trained%2520Sparse%250AAutoencoders%2520%2528SAEs%2529%2520and%2520introduces%2520feature-level%2520constraints%252C%2520allowing%2520for%250Aefficient%252C%2520sparsity-enforced%2520alignment.%2520Our%2520approach%2520enjoys%2520efficiency%2520by%2520using%250Asparse%2520features%2520activated%2520in%2520a%2520well-trained%2520sparse%2520autoencoder%2520and%2520the%2520quality%250Aof%2520sequential%2520KL%2520divergence%2520by%2520using%2520the%2520feature-level%2520offline%2520reference.%250AExperimental%2520results%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520FPO%2520achieves%2520a%250A5.08%2525%2520absolute%2520improvement%2520in%2520win%2520rate%2520with%2520much%2520lower%2520computational%2520cost%250Acompared%2520to%2520state-of-the-art%2520baselines%252C%2520making%2520it%2520a%2520promising%2520solution%2520for%250Aefficient%2520and%2520controllable%2520LLM%2520alignments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Preference%20Optimization%20Using%20Sparse%20Feature-Level%20Constraints&entry.906535625=Qingyu%20Yin%20and%20Chak%20Tou%20Leong%20and%20Hongbo%20Zhang%20and%20Minjun%20Zhu%20and%20Hanqi%20Yan%20and%20Qiang%20Zhang%20and%20Yulan%20He%20and%20Wenjie%20Li%20and%20Jun%20Wang%20and%20Yue%20Zhang%20and%20Linyi%20Yang&entry.1292438233=%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences%20remains%0Aa%20key%20challenge.%20While%20post-training%20techniques%20like%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20have%0Aachieved%20notable%20success%2C%20they%20often%20introduce%20computational%20inefficiencies%20and%0Atraining%20instability.%20In%20this%20paper%2C%20we%20propose%20Feature-level%20constrained%0APreference%20Optimization%20%28FPO%29%2C%20a%20novel%20method%20designed%20to%20simplify%20the%0Aalignment%20process%20while%20ensuring%20stability.%20FPO%20leverages%20pre-trained%20Sparse%0AAutoencoders%20%28SAEs%29%20and%20introduces%20feature-level%20constraints%2C%20allowing%20for%0Aefficient%2C%20sparsity-enforced%20alignment.%20Our%20approach%20enjoys%20efficiency%20by%20using%0Asparse%20features%20activated%20in%20a%20well-trained%20sparse%20autoencoder%20and%20the%20quality%0Aof%20sequential%20KL%20divergence%20by%20using%20the%20feature-level%20offline%20reference.%0AExperimental%20results%20on%20benchmark%20datasets%20demonstrate%20that%20FPO%20achieves%20a%0A5.08%25%20absolute%20improvement%20in%20win%20rate%20with%20much%20lower%20computational%20cost%0Acompared%20to%20state-of-the-art%20baselines%2C%20making%20it%20a%20promising%20solution%20for%0Aefficient%20and%20controllable%20LLM%20alignments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07618v2&entry.124074799=Read"},
{"title": "AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation", "author": "Leah von der Heyde and Anna-Carolina Haensch and Bernd Wei\u00df and Jessica Daikeler", "abstract": "  The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.\n", "link": "http://arxiv.org/abs/2506.14634v3", "date": "2025-07-03", "relevancy": 2.5897, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIn%27t%20Nothing%20But%20a%20Survey%3F%20Using%20Large%20Language%20Models%20for%20Coding%0A%20%20German%20Open-Ended%20Survey%20Responses%20on%20Survey%20Motivation&body=Title%3A%20AIn%27t%20Nothing%20But%20a%20Survey%3F%20Using%20Large%20Language%20Models%20for%20Coding%0A%20%20German%20Open-Ended%20Survey%20Responses%20on%20Survey%20Motivation%0AAuthor%3A%20Leah%20von%20der%20Heyde%20and%20Anna-Carolina%20Haensch%20and%20Bernd%20Wei%C3%9F%20and%20Jessica%20Daikeler%0AAbstract%3A%20%20%20The%20recent%20development%20and%20wider%20accessibility%20of%20LLMs%20have%20spurred%0Adiscussions%20about%20how%20they%20can%20be%20used%20in%20survey%20research%2C%20including%0Aclassifying%20open-ended%20survey%20responses.%20Due%20to%20their%20linguistic%20capacities%2C%20it%0Ais%20possible%20that%20LLMs%20are%20an%20efficient%20alternative%20to%20time-consuming%20manual%0Acoding%20and%20the%20pre-training%20of%20supervised%20machine%20learning%20models.%20As%20most%0Aexisting%20research%20on%20this%20topic%20has%20focused%20on%20English-language%20responses%0Arelating%20to%20non-complex%20topics%20or%20on%20single%20LLMs%2C%20it%20is%20unclear%20whether%20its%0Afindings%20generalize%20and%20how%20the%20quality%20of%20these%20classifications%20compares%20to%0Aestablished%20methods.%20In%20this%20study%2C%20we%20investigate%20to%20what%20extent%20different%0ALLMs%20can%20be%20used%20to%20code%20open-ended%20survey%20responses%20in%20other%20contexts%2C%20using%0AGerman%20data%20on%20reasons%20for%20survey%20participation%20as%20an%20example.%20We%20compare%0Aseveral%20state-of-the-art%20LLMs%20and%20several%20prompting%20approaches%2C%20and%20evaluate%0Athe%20LLMs%27%20performance%20by%20using%20human%20expert%20codings.%20Overall%20performance%0Adiffers%20greatly%20between%20LLMs%2C%20and%20only%20a%20fine-tuned%20LLM%20achieves%20satisfactory%0Alevels%20of%20predictive%20performance.%20Performance%20differences%20between%20prompting%0Aapproaches%20are%20conditional%20on%20the%20LLM%20used.%20Finally%2C%20LLMs%27%20unequal%0Aclassification%20performance%20across%20different%20categories%20of%20reasons%20for%20survey%0Aparticipation%20results%20in%20different%20categorical%20distributions%20when%20not%20using%0Afine-tuning.%20We%20discuss%20the%20implications%20of%20these%20findings%2C%20both%20for%0Amethodological%20research%20on%20coding%20open-ended%20responses%20and%20for%20their%0Asubstantive%20analysis%2C%20and%20for%20practitioners%20processing%20or%20substantively%0Aanalyzing%20such%20data.%20Finally%2C%20we%20highlight%20the%20many%20trade-offs%20researchers%20need%0Ato%20consider%20when%20choosing%20automated%20methods%20for%20open-ended%20response%0Aclassification%20in%20the%20age%20of%20LLMs.%20In%20doing%20so%2C%20our%20study%20contributes%20to%20the%0Agrowing%20body%20of%20research%20about%20the%20conditions%20under%20which%20LLMs%20can%20be%0Aefficiently%2C%20accurately%2C%20and%20reliably%20leveraged%20in%20survey%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14634v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIn%2527t%2520Nothing%2520But%2520a%2520Survey%253F%2520Using%2520Large%2520Language%2520Models%2520for%2520Coding%250A%2520%2520German%2520Open-Ended%2520Survey%2520Responses%2520on%2520Survey%2520Motivation%26entry.906535625%3DLeah%2520von%2520der%2520Heyde%2520and%2520Anna-Carolina%2520Haensch%2520and%2520Bernd%2520Wei%25C3%259F%2520and%2520Jessica%2520Daikeler%26entry.1292438233%3D%2520%2520The%2520recent%2520development%2520and%2520wider%2520accessibility%2520of%2520LLMs%2520have%2520spurred%250Adiscussions%2520about%2520how%2520they%2520can%2520be%2520used%2520in%2520survey%2520research%252C%2520including%250Aclassifying%2520open-ended%2520survey%2520responses.%2520Due%2520to%2520their%2520linguistic%2520capacities%252C%2520it%250Ais%2520possible%2520that%2520LLMs%2520are%2520an%2520efficient%2520alternative%2520to%2520time-consuming%2520manual%250Acoding%2520and%2520the%2520pre-training%2520of%2520supervised%2520machine%2520learning%2520models.%2520As%2520most%250Aexisting%2520research%2520on%2520this%2520topic%2520has%2520focused%2520on%2520English-language%2520responses%250Arelating%2520to%2520non-complex%2520topics%2520or%2520on%2520single%2520LLMs%252C%2520it%2520is%2520unclear%2520whether%2520its%250Afindings%2520generalize%2520and%2520how%2520the%2520quality%2520of%2520these%2520classifications%2520compares%2520to%250Aestablished%2520methods.%2520In%2520this%2520study%252C%2520we%2520investigate%2520to%2520what%2520extent%2520different%250ALLMs%2520can%2520be%2520used%2520to%2520code%2520open-ended%2520survey%2520responses%2520in%2520other%2520contexts%252C%2520using%250AGerman%2520data%2520on%2520reasons%2520for%2520survey%2520participation%2520as%2520an%2520example.%2520We%2520compare%250Aseveral%2520state-of-the-art%2520LLMs%2520and%2520several%2520prompting%2520approaches%252C%2520and%2520evaluate%250Athe%2520LLMs%2527%2520performance%2520by%2520using%2520human%2520expert%2520codings.%2520Overall%2520performance%250Adiffers%2520greatly%2520between%2520LLMs%252C%2520and%2520only%2520a%2520fine-tuned%2520LLM%2520achieves%2520satisfactory%250Alevels%2520of%2520predictive%2520performance.%2520Performance%2520differences%2520between%2520prompting%250Aapproaches%2520are%2520conditional%2520on%2520the%2520LLM%2520used.%2520Finally%252C%2520LLMs%2527%2520unequal%250Aclassification%2520performance%2520across%2520different%2520categories%2520of%2520reasons%2520for%2520survey%250Aparticipation%2520results%2520in%2520different%2520categorical%2520distributions%2520when%2520not%2520using%250Afine-tuning.%2520We%2520discuss%2520the%2520implications%2520of%2520these%2520findings%252C%2520both%2520for%250Amethodological%2520research%2520on%2520coding%2520open-ended%2520responses%2520and%2520for%2520their%250Asubstantive%2520analysis%252C%2520and%2520for%2520practitioners%2520processing%2520or%2520substantively%250Aanalyzing%2520such%2520data.%2520Finally%252C%2520we%2520highlight%2520the%2520many%2520trade-offs%2520researchers%2520need%250Ato%2520consider%2520when%2520choosing%2520automated%2520methods%2520for%2520open-ended%2520response%250Aclassification%2520in%2520the%2520age%2520of%2520LLMs.%2520In%2520doing%2520so%252C%2520our%2520study%2520contributes%2520to%2520the%250Agrowing%2520body%2520of%2520research%2520about%2520the%2520conditions%2520under%2520which%2520LLMs%2520can%2520be%250Aefficiently%252C%2520accurately%252C%2520and%2520reliably%2520leveraged%2520in%2520survey%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14634v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIn%27t%20Nothing%20But%20a%20Survey%3F%20Using%20Large%20Language%20Models%20for%20Coding%0A%20%20German%20Open-Ended%20Survey%20Responses%20on%20Survey%20Motivation&entry.906535625=Leah%20von%20der%20Heyde%20and%20Anna-Carolina%20Haensch%20and%20Bernd%20Wei%C3%9F%20and%20Jessica%20Daikeler&entry.1292438233=%20%20The%20recent%20development%20and%20wider%20accessibility%20of%20LLMs%20have%20spurred%0Adiscussions%20about%20how%20they%20can%20be%20used%20in%20survey%20research%2C%20including%0Aclassifying%20open-ended%20survey%20responses.%20Due%20to%20their%20linguistic%20capacities%2C%20it%0Ais%20possible%20that%20LLMs%20are%20an%20efficient%20alternative%20to%20time-consuming%20manual%0Acoding%20and%20the%20pre-training%20of%20supervised%20machine%20learning%20models.%20As%20most%0Aexisting%20research%20on%20this%20topic%20has%20focused%20on%20English-language%20responses%0Arelating%20to%20non-complex%20topics%20or%20on%20single%20LLMs%2C%20it%20is%20unclear%20whether%20its%0Afindings%20generalize%20and%20how%20the%20quality%20of%20these%20classifications%20compares%20to%0Aestablished%20methods.%20In%20this%20study%2C%20we%20investigate%20to%20what%20extent%20different%0ALLMs%20can%20be%20used%20to%20code%20open-ended%20survey%20responses%20in%20other%20contexts%2C%20using%0AGerman%20data%20on%20reasons%20for%20survey%20participation%20as%20an%20example.%20We%20compare%0Aseveral%20state-of-the-art%20LLMs%20and%20several%20prompting%20approaches%2C%20and%20evaluate%0Athe%20LLMs%27%20performance%20by%20using%20human%20expert%20codings.%20Overall%20performance%0Adiffers%20greatly%20between%20LLMs%2C%20and%20only%20a%20fine-tuned%20LLM%20achieves%20satisfactory%0Alevels%20of%20predictive%20performance.%20Performance%20differences%20between%20prompting%0Aapproaches%20are%20conditional%20on%20the%20LLM%20used.%20Finally%2C%20LLMs%27%20unequal%0Aclassification%20performance%20across%20different%20categories%20of%20reasons%20for%20survey%0Aparticipation%20results%20in%20different%20categorical%20distributions%20when%20not%20using%0Afine-tuning.%20We%20discuss%20the%20implications%20of%20these%20findings%2C%20both%20for%0Amethodological%20research%20on%20coding%20open-ended%20responses%20and%20for%20their%0Asubstantive%20analysis%2C%20and%20for%20practitioners%20processing%20or%20substantively%0Aanalyzing%20such%20data.%20Finally%2C%20we%20highlight%20the%20many%20trade-offs%20researchers%20need%0Ato%20consider%20when%20choosing%20automated%20methods%20for%20open-ended%20response%0Aclassification%20in%20the%20age%20of%20LLMs.%20In%20doing%20so%2C%20our%20study%20contributes%20to%20the%0Agrowing%20body%20of%20research%20about%20the%20conditions%20under%20which%20LLMs%20can%20be%0Aefficiently%2C%20accurately%2C%20and%20reliably%20leveraged%20in%20survey%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14634v3&entry.124074799=Read"},
{"title": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence\n  Embeddings from LLMs", "author": "Yuchen Fu and Zifeng Cheng and Zhiwei Jiang and Zhonghui Wang and Yafeng Yin and Zhengliang Li and Qing Gu", "abstract": "  Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost.\n", "link": "http://arxiv.org/abs/2412.11556v2", "date": "2025-07-03", "relevancy": 2.5828, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Prepending%3A%20A%20Training-Free%20Approach%20for%20Eliciting%20Better%20Sentence%0A%20%20Embeddings%20from%20LLMs&body=Title%3A%20Token%20Prepending%3A%20A%20Training-Free%20Approach%20for%20Eliciting%20Better%20Sentence%0A%20%20Embeddings%20from%20LLMs%0AAuthor%3A%20Yuchen%20Fu%20and%20Zifeng%20Cheng%20and%20Zhiwei%20Jiang%20and%20Zhonghui%20Wang%20and%20Yafeng%20Yin%20and%20Zhengliang%20Li%20and%20Qing%20Gu%0AAbstract%3A%20%20%20Extracting%20sentence%20embeddings%20from%20large%20language%20models%20%28LLMs%29%20is%20a%0Apromising%20direction%2C%20as%20LLMs%20have%20demonstrated%20stronger%20semantic%20understanding%0Acapabilities.%20Previous%20studies%20typically%20focus%20on%20prompt%20engineering%20to%20elicit%0Asentence%20embeddings%20from%20LLMs%20by%20prompting%20the%20model%20to%20encode%20sentence%0Ainformation%20into%20the%20embedding%20of%20the%20last%20token.%20However%2C%20LLMs%20are%20mostly%0Adecoder-only%20models%20with%20causal%20attention%20and%20the%20earlier%20tokens%20in%20the%0Asentence%20cannot%20attend%20to%20the%20latter%20tokens%2C%20resulting%20in%20biased%20encoding%20of%0Asentence%20information%20and%20cascading%20effects%20on%20the%20final%20decoded%20token.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20Token%20Prepending%20%28TP%29%20technique%20that%20prepends%20each%0Alayer%27s%20decoded%20sentence%20embedding%20to%20the%20beginning%20of%20the%20sentence%20in%20the%20next%0Alayer%27s%20input%2C%20allowing%20earlier%20tokens%20to%20attend%20to%20the%20complete%20sentence%0Ainformation%20under%20the%20causal%20attention%20mechanism.%20The%20proposed%20TP%20technique%20is%0Aa%20plug-and-play%20and%20training-free%20technique%2C%20which%20means%20it%20can%20be%20seamlessly%0Aintegrated%20with%20various%20prompt-based%20sentence%20embedding%20methods%20and%0Aautoregressive%20LLMs.%20Extensive%20experiments%20on%20various%20Semantic%20Textual%0ASimilarity%20%28STS%29%20tasks%20and%20downstream%20classification%20tasks%20demonstrate%20that%20our%0Aproposed%20TP%20technique%20can%20significantly%20improve%20the%20performance%20of%20existing%0Aprompt-based%20sentence%20embedding%20methods%20across%20different%20LLMs%2C%20while%20incurring%0Anegligible%20additional%20inference%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Prepending%253A%2520A%2520Training-Free%2520Approach%2520for%2520Eliciting%2520Better%2520Sentence%250A%2520%2520Embeddings%2520from%2520LLMs%26entry.906535625%3DYuchen%2520Fu%2520and%2520Zifeng%2520Cheng%2520and%2520Zhiwei%2520Jiang%2520and%2520Zhonghui%2520Wang%2520and%2520Yafeng%2520Yin%2520and%2520Zhengliang%2520Li%2520and%2520Qing%2520Gu%26entry.1292438233%3D%2520%2520Extracting%2520sentence%2520embeddings%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520a%250Apromising%2520direction%252C%2520as%2520LLMs%2520have%2520demonstrated%2520stronger%2520semantic%2520understanding%250Acapabilities.%2520Previous%2520studies%2520typically%2520focus%2520on%2520prompt%2520engineering%2520to%2520elicit%250Asentence%2520embeddings%2520from%2520LLMs%2520by%2520prompting%2520the%2520model%2520to%2520encode%2520sentence%250Ainformation%2520into%2520the%2520embedding%2520of%2520the%2520last%2520token.%2520However%252C%2520LLMs%2520are%2520mostly%250Adecoder-only%2520models%2520with%2520causal%2520attention%2520and%2520the%2520earlier%2520tokens%2520in%2520the%250Asentence%2520cannot%2520attend%2520to%2520the%2520latter%2520tokens%252C%2520resulting%2520in%2520biased%2520encoding%2520of%250Asentence%2520information%2520and%2520cascading%2520effects%2520on%2520the%2520final%2520decoded%2520token.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520novel%2520Token%2520Prepending%2520%2528TP%2529%2520technique%2520that%2520prepends%2520each%250Alayer%2527s%2520decoded%2520sentence%2520embedding%2520to%2520the%2520beginning%2520of%2520the%2520sentence%2520in%2520the%2520next%250Alayer%2527s%2520input%252C%2520allowing%2520earlier%2520tokens%2520to%2520attend%2520to%2520the%2520complete%2520sentence%250Ainformation%2520under%2520the%2520causal%2520attention%2520mechanism.%2520The%2520proposed%2520TP%2520technique%2520is%250Aa%2520plug-and-play%2520and%2520training-free%2520technique%252C%2520which%2520means%2520it%2520can%2520be%2520seamlessly%250Aintegrated%2520with%2520various%2520prompt-based%2520sentence%2520embedding%2520methods%2520and%250Aautoregressive%2520LLMs.%2520Extensive%2520experiments%2520on%2520various%2520Semantic%2520Textual%250ASimilarity%2520%2528STS%2529%2520tasks%2520and%2520downstream%2520classification%2520tasks%2520demonstrate%2520that%2520our%250Aproposed%2520TP%2520technique%2520can%2520significantly%2520improve%2520the%2520performance%2520of%2520existing%250Aprompt-based%2520sentence%2520embedding%2520methods%2520across%2520different%2520LLMs%252C%2520while%2520incurring%250Anegligible%2520additional%2520inference%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Prepending%3A%20A%20Training-Free%20Approach%20for%20Eliciting%20Better%20Sentence%0A%20%20Embeddings%20from%20LLMs&entry.906535625=Yuchen%20Fu%20and%20Zifeng%20Cheng%20and%20Zhiwei%20Jiang%20and%20Zhonghui%20Wang%20and%20Yafeng%20Yin%20and%20Zhengliang%20Li%20and%20Qing%20Gu&entry.1292438233=%20%20Extracting%20sentence%20embeddings%20from%20large%20language%20models%20%28LLMs%29%20is%20a%0Apromising%20direction%2C%20as%20LLMs%20have%20demonstrated%20stronger%20semantic%20understanding%0Acapabilities.%20Previous%20studies%20typically%20focus%20on%20prompt%20engineering%20to%20elicit%0Asentence%20embeddings%20from%20LLMs%20by%20prompting%20the%20model%20to%20encode%20sentence%0Ainformation%20into%20the%20embedding%20of%20the%20last%20token.%20However%2C%20LLMs%20are%20mostly%0Adecoder-only%20models%20with%20causal%20attention%20and%20the%20earlier%20tokens%20in%20the%0Asentence%20cannot%20attend%20to%20the%20latter%20tokens%2C%20resulting%20in%20biased%20encoding%20of%0Asentence%20information%20and%20cascading%20effects%20on%20the%20final%20decoded%20token.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20Token%20Prepending%20%28TP%29%20technique%20that%20prepends%20each%0Alayer%27s%20decoded%20sentence%20embedding%20to%20the%20beginning%20of%20the%20sentence%20in%20the%20next%0Alayer%27s%20input%2C%20allowing%20earlier%20tokens%20to%20attend%20to%20the%20complete%20sentence%0Ainformation%20under%20the%20causal%20attention%20mechanism.%20The%20proposed%20TP%20technique%20is%0Aa%20plug-and-play%20and%20training-free%20technique%2C%20which%20means%20it%20can%20be%20seamlessly%0Aintegrated%20with%20various%20prompt-based%20sentence%20embedding%20methods%20and%0Aautoregressive%20LLMs.%20Extensive%20experiments%20on%20various%20Semantic%20Textual%0ASimilarity%20%28STS%29%20tasks%20and%20downstream%20classification%20tasks%20demonstrate%20that%20our%0Aproposed%20TP%20technique%20can%20significantly%20improve%20the%20performance%20of%20existing%0Aprompt-based%20sentence%20embedding%20methods%20across%20different%20LLMs%2C%20while%20incurring%0Anegligible%20additional%20inference%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11556v2&entry.124074799=Read"},
{"title": "Wildlife Target Re-Identification Using Self-supervised Learning in\n  Non-Urban Settings", "author": "Mufhumudzi Muthivhi and Terence L. van Zyl", "abstract": "  Wildlife re-identification aims to match individuals of the same species\nacross different observations. Current state-of-the-art (SOTA) models rely on\nclass labels to train supervised models for individual classification. This\ndependence on annotated data has driven the curation of numerous large-scale\nwildlife datasets. This study investigates self-supervised learning\nSelf-Supervised Learning (SSL) for wildlife re-identification. We automatically\nextract two distinct views of an individual using temporal image pairs from\ncamera trap data without supervision. The image pairs train a self-supervised\nmodel from a potentially endless stream of video data. We evaluate the learnt\nrepresentations against supervised features on open-world scenarios and\ntransfer learning in various wildlife downstream tasks. The analysis of the\nexperimental results shows that self-supervised models are more robust even\nwith limited data. Moreover, self-supervised features outperform supervision\nacross all downstream tasks. The code is available here\nhttps://github.com/pxpana/SSLWildlife.\n", "link": "http://arxiv.org/abs/2507.02403v1", "date": "2025-07-03", "relevancy": 2.5669, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5266}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5254}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wildlife%20Target%20Re-Identification%20Using%20Self-supervised%20Learning%20in%0A%20%20Non-Urban%20Settings&body=Title%3A%20Wildlife%20Target%20Re-Identification%20Using%20Self-supervised%20Learning%20in%0A%20%20Non-Urban%20Settings%0AAuthor%3A%20Mufhumudzi%20Muthivhi%20and%20Terence%20L.%20van%20Zyl%0AAbstract%3A%20%20%20Wildlife%20re-identification%20aims%20to%20match%20individuals%20of%20the%20same%20species%0Aacross%20different%20observations.%20Current%20state-of-the-art%20%28SOTA%29%20models%20rely%20on%0Aclass%20labels%20to%20train%20supervised%20models%20for%20individual%20classification.%20This%0Adependence%20on%20annotated%20data%20has%20driven%20the%20curation%20of%20numerous%20large-scale%0Awildlife%20datasets.%20This%20study%20investigates%20self-supervised%20learning%0ASelf-Supervised%20Learning%20%28SSL%29%20for%20wildlife%20re-identification.%20We%20automatically%0Aextract%20two%20distinct%20views%20of%20an%20individual%20using%20temporal%20image%20pairs%20from%0Acamera%20trap%20data%20without%20supervision.%20The%20image%20pairs%20train%20a%20self-supervised%0Amodel%20from%20a%20potentially%20endless%20stream%20of%20video%20data.%20We%20evaluate%20the%20learnt%0Arepresentations%20against%20supervised%20features%20on%20open-world%20scenarios%20and%0Atransfer%20learning%20in%20various%20wildlife%20downstream%20tasks.%20The%20analysis%20of%20the%0Aexperimental%20results%20shows%20that%20self-supervised%20models%20are%20more%20robust%20even%0Awith%20limited%20data.%20Moreover%2C%20self-supervised%20features%20outperform%20supervision%0Aacross%20all%20downstream%20tasks.%20The%20code%20is%20available%20here%0Ahttps%3A//github.com/pxpana/SSLWildlife.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWildlife%2520Target%2520Re-Identification%2520Using%2520Self-supervised%2520Learning%2520in%250A%2520%2520Non-Urban%2520Settings%26entry.906535625%3DMufhumudzi%2520Muthivhi%2520and%2520Terence%2520L.%2520van%2520Zyl%26entry.1292438233%3D%2520%2520Wildlife%2520re-identification%2520aims%2520to%2520match%2520individuals%2520of%2520the%2520same%2520species%250Aacross%2520different%2520observations.%2520Current%2520state-of-the-art%2520%2528SOTA%2529%2520models%2520rely%2520on%250Aclass%2520labels%2520to%2520train%2520supervised%2520models%2520for%2520individual%2520classification.%2520This%250Adependence%2520on%2520annotated%2520data%2520has%2520driven%2520the%2520curation%2520of%2520numerous%2520large-scale%250Awildlife%2520datasets.%2520This%2520study%2520investigates%2520self-supervised%2520learning%250ASelf-Supervised%2520Learning%2520%2528SSL%2529%2520for%2520wildlife%2520re-identification.%2520We%2520automatically%250Aextract%2520two%2520distinct%2520views%2520of%2520an%2520individual%2520using%2520temporal%2520image%2520pairs%2520from%250Acamera%2520trap%2520data%2520without%2520supervision.%2520The%2520image%2520pairs%2520train%2520a%2520self-supervised%250Amodel%2520from%2520a%2520potentially%2520endless%2520stream%2520of%2520video%2520data.%2520We%2520evaluate%2520the%2520learnt%250Arepresentations%2520against%2520supervised%2520features%2520on%2520open-world%2520scenarios%2520and%250Atransfer%2520learning%2520in%2520various%2520wildlife%2520downstream%2520tasks.%2520The%2520analysis%2520of%2520the%250Aexperimental%2520results%2520shows%2520that%2520self-supervised%2520models%2520are%2520more%2520robust%2520even%250Awith%2520limited%2520data.%2520Moreover%252C%2520self-supervised%2520features%2520outperform%2520supervision%250Aacross%2520all%2520downstream%2520tasks.%2520The%2520code%2520is%2520available%2520here%250Ahttps%253A//github.com/pxpana/SSLWildlife.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wildlife%20Target%20Re-Identification%20Using%20Self-supervised%20Learning%20in%0A%20%20Non-Urban%20Settings&entry.906535625=Mufhumudzi%20Muthivhi%20and%20Terence%20L.%20van%20Zyl&entry.1292438233=%20%20Wildlife%20re-identification%20aims%20to%20match%20individuals%20of%20the%20same%20species%0Aacross%20different%20observations.%20Current%20state-of-the-art%20%28SOTA%29%20models%20rely%20on%0Aclass%20labels%20to%20train%20supervised%20models%20for%20individual%20classification.%20This%0Adependence%20on%20annotated%20data%20has%20driven%20the%20curation%20of%20numerous%20large-scale%0Awildlife%20datasets.%20This%20study%20investigates%20self-supervised%20learning%0ASelf-Supervised%20Learning%20%28SSL%29%20for%20wildlife%20re-identification.%20We%20automatically%0Aextract%20two%20distinct%20views%20of%20an%20individual%20using%20temporal%20image%20pairs%20from%0Acamera%20trap%20data%20without%20supervision.%20The%20image%20pairs%20train%20a%20self-supervised%0Amodel%20from%20a%20potentially%20endless%20stream%20of%20video%20data.%20We%20evaluate%20the%20learnt%0Arepresentations%20against%20supervised%20features%20on%20open-world%20scenarios%20and%0Atransfer%20learning%20in%20various%20wildlife%20downstream%20tasks.%20The%20analysis%20of%20the%0Aexperimental%20results%20shows%20that%20self-supervised%20models%20are%20more%20robust%20even%0Awith%20limited%20data.%20Moreover%2C%20self-supervised%20features%20outperform%20supervision%0Aacross%20all%20downstream%20tasks.%20The%20code%20is%20available%20here%0Ahttps%3A//github.com/pxpana/SSLWildlife.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02403v1&entry.124074799=Read"},
{"title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion", "author": "Lin Wu and Zhixiang Chen and Jianglin Lan", "abstract": "  Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.\n", "link": "http://arxiv.org/abs/2507.01737v2", "date": "2025-07-03", "relevancy": 2.5525, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6926}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6198}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOI-Dyn%3A%20Learning%20Interaction%20Dynamics%20for%20Human-Object%20Motion%20Diffusion&body=Title%3A%20HOI-Dyn%3A%20Learning%20Interaction%20Dynamics%20for%20Human-Object%20Motion%20Diffusion%0AAuthor%3A%20Lin%20Wu%20and%20Zhixiang%20Chen%20and%20Jianglin%20Lan%0AAbstract%3A%20%20%20Generating%20realistic%203D%20human-object%20interactions%20%28HOIs%29%20remains%20a%0Achallenging%20task%20due%20to%20the%20difficulty%20of%20modeling%20detailed%20interaction%0Adynamics.%20Existing%20methods%20treat%20human%20and%20object%20motions%20independently%2C%0Aresulting%20in%20physically%20implausible%20and%20causally%20inconsistent%20behaviors.%20In%0Athis%20work%2C%20we%20present%20HOI-Dyn%2C%20a%20novel%20framework%20that%20formulates%20HOI%20generation%0Aas%20a%20driver-responder%20system%2C%20where%20human%20actions%20drive%20object%20responses.%20At%0Athe%20core%20of%20our%20method%20is%20a%20lightweight%20transformer-based%20interaction%20dynamics%0Amodel%20that%20explicitly%20predicts%20how%20objects%20should%20react%20to%20human%20motion.%20To%0Afurther%20enforce%20consistency%2C%20we%20introduce%20a%20residual-based%20dynamics%20loss%20that%0Amitigates%20the%20impact%20of%20dynamics%20prediction%20errors%20and%20prevents%20misleading%0Aoptimization%20signals.%20The%20dynamics%20model%20is%20used%20only%20during%20training%2C%0Apreserving%20inference%20efficiency.%20Through%20extensive%20qualitative%20and%20quantitative%0Aexperiments%2C%20we%20demonstrate%20that%20our%20approach%20not%20only%20enhances%20the%20quality%20of%0AHOI%20generation%20but%20also%20establishes%20a%20feasible%20metric%20for%20evaluating%20the%0Aquality%20of%20generated%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOI-Dyn%253A%2520Learning%2520Interaction%2520Dynamics%2520for%2520Human-Object%2520Motion%2520Diffusion%26entry.906535625%3DLin%2520Wu%2520and%2520Zhixiang%2520Chen%2520and%2520Jianglin%2520Lan%26entry.1292438233%3D%2520%2520Generating%2520realistic%25203D%2520human-object%2520interactions%2520%2528HOIs%2529%2520remains%2520a%250Achallenging%2520task%2520due%2520to%2520the%2520difficulty%2520of%2520modeling%2520detailed%2520interaction%250Adynamics.%2520Existing%2520methods%2520treat%2520human%2520and%2520object%2520motions%2520independently%252C%250Aresulting%2520in%2520physically%2520implausible%2520and%2520causally%2520inconsistent%2520behaviors.%2520In%250Athis%2520work%252C%2520we%2520present%2520HOI-Dyn%252C%2520a%2520novel%2520framework%2520that%2520formulates%2520HOI%2520generation%250Aas%2520a%2520driver-responder%2520system%252C%2520where%2520human%2520actions%2520drive%2520object%2520responses.%2520At%250Athe%2520core%2520of%2520our%2520method%2520is%2520a%2520lightweight%2520transformer-based%2520interaction%2520dynamics%250Amodel%2520that%2520explicitly%2520predicts%2520how%2520objects%2520should%2520react%2520to%2520human%2520motion.%2520To%250Afurther%2520enforce%2520consistency%252C%2520we%2520introduce%2520a%2520residual-based%2520dynamics%2520loss%2520that%250Amitigates%2520the%2520impact%2520of%2520dynamics%2520prediction%2520errors%2520and%2520prevents%2520misleading%250Aoptimization%2520signals.%2520The%2520dynamics%2520model%2520is%2520used%2520only%2520during%2520training%252C%250Apreserving%2520inference%2520efficiency.%2520Through%2520extensive%2520qualitative%2520and%2520quantitative%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520not%2520only%2520enhances%2520the%2520quality%2520of%250AHOI%2520generation%2520but%2520also%2520establishes%2520a%2520feasible%2520metric%2520for%2520evaluating%2520the%250Aquality%2520of%2520generated%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOI-Dyn%3A%20Learning%20Interaction%20Dynamics%20for%20Human-Object%20Motion%20Diffusion&entry.906535625=Lin%20Wu%20and%20Zhixiang%20Chen%20and%20Jianglin%20Lan&entry.1292438233=%20%20Generating%20realistic%203D%20human-object%20interactions%20%28HOIs%29%20remains%20a%0Achallenging%20task%20due%20to%20the%20difficulty%20of%20modeling%20detailed%20interaction%0Adynamics.%20Existing%20methods%20treat%20human%20and%20object%20motions%20independently%2C%0Aresulting%20in%20physically%20implausible%20and%20causally%20inconsistent%20behaviors.%20In%0Athis%20work%2C%20we%20present%20HOI-Dyn%2C%20a%20novel%20framework%20that%20formulates%20HOI%20generation%0Aas%20a%20driver-responder%20system%2C%20where%20human%20actions%20drive%20object%20responses.%20At%0Athe%20core%20of%20our%20method%20is%20a%20lightweight%20transformer-based%20interaction%20dynamics%0Amodel%20that%20explicitly%20predicts%20how%20objects%20should%20react%20to%20human%20motion.%20To%0Afurther%20enforce%20consistency%2C%20we%20introduce%20a%20residual-based%20dynamics%20loss%20that%0Amitigates%20the%20impact%20of%20dynamics%20prediction%20errors%20and%20prevents%20misleading%0Aoptimization%20signals.%20The%20dynamics%20model%20is%20used%20only%20during%20training%2C%0Apreserving%20inference%20efficiency.%20Through%20extensive%20qualitative%20and%20quantitative%0Aexperiments%2C%20we%20demonstrate%20that%20our%20approach%20not%20only%20enhances%20the%20quality%20of%0AHOI%20generation%20but%20also%20establishes%20a%20feasible%20metric%20for%20evaluating%20the%0Aquality%20of%20generated%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01737v2&entry.124074799=Read"},
{"title": "Parametric shape models for vessels learned from segmentations via\n  differentiable voxelization", "author": "Alina F. Dima and Suprosanna Shit and Huaqi Qiu and Robbie Holland and Tamara T. Mueller and Fabio Antonio Musio and Kaiyuan Yang and Bjoern Menze and Rickmer Braren and Marcus Makowski and Daniel Rueckert", "abstract": "  Vessels are complex structures in the body that have been studied extensively\nin multiple representations. While voxelization is the most common of them,\nmeshes and parametric models are critical in various applications due to their\ndesirable properties. However, these representations are typically extracted\nthrough segmentations and used disjointly from each other. We propose a\nframework that joins the three representations under differentiable\ntransformations. By leveraging differentiable voxelization, we automatically\nextract a parametric shape model of the vessels through shape-to-segmentation\nfitting, where we learn shape parameters from segmentations without the\nexplicit need for ground-truth shape parameters. The vessel is parametrized as\ncenterlines and radii using cubic B-splines, ensuring smoothness and continuity\nby construction. Meshes are differentiably extracted from the learned shape\nparameters, resulting in high-fidelity meshes that can be manipulated post-fit.\nOur method can accurately capture the geometry of complex vessels, as\ndemonstrated by the volumetric fits in experiments on aortas, aneurysms, and\nbrain vessels.\n", "link": "http://arxiv.org/abs/2507.02576v1", "date": "2025-07-03", "relevancy": 2.5469, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5378}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4988}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parametric%20shape%20models%20for%20vessels%20learned%20from%20segmentations%20via%0A%20%20differentiable%20voxelization&body=Title%3A%20Parametric%20shape%20models%20for%20vessels%20learned%20from%20segmentations%20via%0A%20%20differentiable%20voxelization%0AAuthor%3A%20Alina%20F.%20Dima%20and%20Suprosanna%20Shit%20and%20Huaqi%20Qiu%20and%20Robbie%20Holland%20and%20Tamara%20T.%20Mueller%20and%20Fabio%20Antonio%20Musio%20and%20Kaiyuan%20Yang%20and%20Bjoern%20Menze%20and%20Rickmer%20Braren%20and%20Marcus%20Makowski%20and%20Daniel%20Rueckert%0AAbstract%3A%20%20%20Vessels%20are%20complex%20structures%20in%20the%20body%20that%20have%20been%20studied%20extensively%0Ain%20multiple%20representations.%20While%20voxelization%20is%20the%20most%20common%20of%20them%2C%0Ameshes%20and%20parametric%20models%20are%20critical%20in%20various%20applications%20due%20to%20their%0Adesirable%20properties.%20However%2C%20these%20representations%20are%20typically%20extracted%0Athrough%20segmentations%20and%20used%20disjointly%20from%20each%20other.%20We%20propose%20a%0Aframework%20that%20joins%20the%20three%20representations%20under%20differentiable%0Atransformations.%20By%20leveraging%20differentiable%20voxelization%2C%20we%20automatically%0Aextract%20a%20parametric%20shape%20model%20of%20the%20vessels%20through%20shape-to-segmentation%0Afitting%2C%20where%20we%20learn%20shape%20parameters%20from%20segmentations%20without%20the%0Aexplicit%20need%20for%20ground-truth%20shape%20parameters.%20The%20vessel%20is%20parametrized%20as%0Acenterlines%20and%20radii%20using%20cubic%20B-splines%2C%20ensuring%20smoothness%20and%20continuity%0Aby%20construction.%20Meshes%20are%20differentiably%20extracted%20from%20the%20learned%20shape%0Aparameters%2C%20resulting%20in%20high-fidelity%20meshes%20that%20can%20be%20manipulated%20post-fit.%0AOur%20method%20can%20accurately%20capture%20the%20geometry%20of%20complex%20vessels%2C%20as%0Ademonstrated%20by%20the%20volumetric%20fits%20in%20experiments%20on%20aortas%2C%20aneurysms%2C%20and%0Abrain%20vessels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParametric%2520shape%2520models%2520for%2520vessels%2520learned%2520from%2520segmentations%2520via%250A%2520%2520differentiable%2520voxelization%26entry.906535625%3DAlina%2520F.%2520Dima%2520and%2520Suprosanna%2520Shit%2520and%2520Huaqi%2520Qiu%2520and%2520Robbie%2520Holland%2520and%2520Tamara%2520T.%2520Mueller%2520and%2520Fabio%2520Antonio%2520Musio%2520and%2520Kaiyuan%2520Yang%2520and%2520Bjoern%2520Menze%2520and%2520Rickmer%2520Braren%2520and%2520Marcus%2520Makowski%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3D%2520%2520Vessels%2520are%2520complex%2520structures%2520in%2520the%2520body%2520that%2520have%2520been%2520studied%2520extensively%250Ain%2520multiple%2520representations.%2520While%2520voxelization%2520is%2520the%2520most%2520common%2520of%2520them%252C%250Ameshes%2520and%2520parametric%2520models%2520are%2520critical%2520in%2520various%2520applications%2520due%2520to%2520their%250Adesirable%2520properties.%2520However%252C%2520these%2520representations%2520are%2520typically%2520extracted%250Athrough%2520segmentations%2520and%2520used%2520disjointly%2520from%2520each%2520other.%2520We%2520propose%2520a%250Aframework%2520that%2520joins%2520the%2520three%2520representations%2520under%2520differentiable%250Atransformations.%2520By%2520leveraging%2520differentiable%2520voxelization%252C%2520we%2520automatically%250Aextract%2520a%2520parametric%2520shape%2520model%2520of%2520the%2520vessels%2520through%2520shape-to-segmentation%250Afitting%252C%2520where%2520we%2520learn%2520shape%2520parameters%2520from%2520segmentations%2520without%2520the%250Aexplicit%2520need%2520for%2520ground-truth%2520shape%2520parameters.%2520The%2520vessel%2520is%2520parametrized%2520as%250Acenterlines%2520and%2520radii%2520using%2520cubic%2520B-splines%252C%2520ensuring%2520smoothness%2520and%2520continuity%250Aby%2520construction.%2520Meshes%2520are%2520differentiably%2520extracted%2520from%2520the%2520learned%2520shape%250Aparameters%252C%2520resulting%2520in%2520high-fidelity%2520meshes%2520that%2520can%2520be%2520manipulated%2520post-fit.%250AOur%2520method%2520can%2520accurately%2520capture%2520the%2520geometry%2520of%2520complex%2520vessels%252C%2520as%250Ademonstrated%2520by%2520the%2520volumetric%2520fits%2520in%2520experiments%2520on%2520aortas%252C%2520aneurysms%252C%2520and%250Abrain%2520vessels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parametric%20shape%20models%20for%20vessels%20learned%20from%20segmentations%20via%0A%20%20differentiable%20voxelization&entry.906535625=Alina%20F.%20Dima%20and%20Suprosanna%20Shit%20and%20Huaqi%20Qiu%20and%20Robbie%20Holland%20and%20Tamara%20T.%20Mueller%20and%20Fabio%20Antonio%20Musio%20and%20Kaiyuan%20Yang%20and%20Bjoern%20Menze%20and%20Rickmer%20Braren%20and%20Marcus%20Makowski%20and%20Daniel%20Rueckert&entry.1292438233=%20%20Vessels%20are%20complex%20structures%20in%20the%20body%20that%20have%20been%20studied%20extensively%0Ain%20multiple%20representations.%20While%20voxelization%20is%20the%20most%20common%20of%20them%2C%0Ameshes%20and%20parametric%20models%20are%20critical%20in%20various%20applications%20due%20to%20their%0Adesirable%20properties.%20However%2C%20these%20representations%20are%20typically%20extracted%0Athrough%20segmentations%20and%20used%20disjointly%20from%20each%20other.%20We%20propose%20a%0Aframework%20that%20joins%20the%20three%20representations%20under%20differentiable%0Atransformations.%20By%20leveraging%20differentiable%20voxelization%2C%20we%20automatically%0Aextract%20a%20parametric%20shape%20model%20of%20the%20vessels%20through%20shape-to-segmentation%0Afitting%2C%20where%20we%20learn%20shape%20parameters%20from%20segmentations%20without%20the%0Aexplicit%20need%20for%20ground-truth%20shape%20parameters.%20The%20vessel%20is%20parametrized%20as%0Acenterlines%20and%20radii%20using%20cubic%20B-splines%2C%20ensuring%20smoothness%20and%20continuity%0Aby%20construction.%20Meshes%20are%20differentiably%20extracted%20from%20the%20learned%20shape%0Aparameters%2C%20resulting%20in%20high-fidelity%20meshes%20that%20can%20be%20manipulated%20post-fit.%0AOur%20method%20can%20accurately%20capture%20the%20geometry%20of%20complex%20vessels%2C%20as%0Ademonstrated%20by%20the%20volumetric%20fits%20in%20experiments%20on%20aortas%2C%20aneurysms%2C%20and%0Abrain%20vessels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02576v1&entry.124074799=Read"},
{"title": "ASDA: Audio Spectrogram Differential Attention Mechanism for\n  Self-Supervised Representation Learning", "author": "Junyu Wang and Tianrui Wang and Meng Ge and Longbiao Wang and Jianwu Dang", "abstract": "  In recent advancements in audio self-supervised representation learning, the\nstandard Transformer architecture has emerged as the predominant approach, yet\nits attention mechanism often allocates a portion of attention weights to\nirrelevant information, potentially impairing the model's discriminative\nability. To address this, we introduce a differential attention mechanism,\nwhich effectively mitigates ineffective attention allocation through the\nintegration of dual-softmax operations and appropriately tuned differential\ncoefficients. Experimental results demonstrate that our ASDA model achieves\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\ntasks, paving the way for broader applications.\n", "link": "http://arxiv.org/abs/2507.02666v1", "date": "2025-07-03", "relevancy": 2.54, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5129}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5056}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASDA%3A%20Audio%20Spectrogram%20Differential%20Attention%20Mechanism%20for%0A%20%20Self-Supervised%20Representation%20Learning&body=Title%3A%20ASDA%3A%20Audio%20Spectrogram%20Differential%20Attention%20Mechanism%20for%0A%20%20Self-Supervised%20Representation%20Learning%0AAuthor%3A%20Junyu%20Wang%20and%20Tianrui%20Wang%20and%20Meng%20Ge%20and%20Longbiao%20Wang%20and%20Jianwu%20Dang%0AAbstract%3A%20%20%20In%20recent%20advancements%20in%20audio%20self-supervised%20representation%20learning%2C%20the%0Astandard%20Transformer%20architecture%20has%20emerged%20as%20the%20predominant%20approach%2C%20yet%0Aits%20attention%20mechanism%20often%20allocates%20a%20portion%20of%20attention%20weights%20to%0Airrelevant%20information%2C%20potentially%20impairing%20the%20model%27s%20discriminative%0Aability.%20To%20address%20this%2C%20we%20introduce%20a%20differential%20attention%20mechanism%2C%0Awhich%20effectively%20mitigates%20ineffective%20attention%20allocation%20through%20the%0Aintegration%20of%20dual-softmax%20operations%20and%20appropriately%20tuned%20differential%0Acoefficients.%20Experimental%20results%20demonstrate%20that%20our%20ASDA%20model%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20across%20multiple%20benchmarks%2C%20including%20audio%0Aclassification%20%2849.0%25%20mAP%20on%20AS-2M%2C%2041.5%25%20mAP%20on%20AS20K%29%2C%20keyword%20spotting%0A%2898.3%25%20accuracy%20on%20SPC-2%29%2C%20and%20environmental%20sound%20classification%20%2896.1%25%0Aaccuracy%20on%20ESC-50%29.%20These%20results%20highlight%20ASDA%27s%20effectiveness%20in%20audio%0Atasks%2C%20paving%20the%20way%20for%20broader%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASDA%253A%2520Audio%2520Spectrogram%2520Differential%2520Attention%2520Mechanism%2520for%250A%2520%2520Self-Supervised%2520Representation%2520Learning%26entry.906535625%3DJunyu%2520Wang%2520and%2520Tianrui%2520Wang%2520and%2520Meng%2520Ge%2520and%2520Longbiao%2520Wang%2520and%2520Jianwu%2520Dang%26entry.1292438233%3D%2520%2520In%2520recent%2520advancements%2520in%2520audio%2520self-supervised%2520representation%2520learning%252C%2520the%250Astandard%2520Transformer%2520architecture%2520has%2520emerged%2520as%2520the%2520predominant%2520approach%252C%2520yet%250Aits%2520attention%2520mechanism%2520often%2520allocates%2520a%2520portion%2520of%2520attention%2520weights%2520to%250Airrelevant%2520information%252C%2520potentially%2520impairing%2520the%2520model%2527s%2520discriminative%250Aability.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520differential%2520attention%2520mechanism%252C%250Awhich%2520effectively%2520mitigates%2520ineffective%2520attention%2520allocation%2520through%2520the%250Aintegration%2520of%2520dual-softmax%2520operations%2520and%2520appropriately%2520tuned%2520differential%250Acoefficients.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520ASDA%2520model%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520across%2520multiple%2520benchmarks%252C%2520including%2520audio%250Aclassification%2520%252849.0%2525%2520mAP%2520on%2520AS-2M%252C%252041.5%2525%2520mAP%2520on%2520AS20K%2529%252C%2520keyword%2520spotting%250A%252898.3%2525%2520accuracy%2520on%2520SPC-2%2529%252C%2520and%2520environmental%2520sound%2520classification%2520%252896.1%2525%250Aaccuracy%2520on%2520ESC-50%2529.%2520These%2520results%2520highlight%2520ASDA%2527s%2520effectiveness%2520in%2520audio%250Atasks%252C%2520paving%2520the%2520way%2520for%2520broader%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASDA%3A%20Audio%20Spectrogram%20Differential%20Attention%20Mechanism%20for%0A%20%20Self-Supervised%20Representation%20Learning&entry.906535625=Junyu%20Wang%20and%20Tianrui%20Wang%20and%20Meng%20Ge%20and%20Longbiao%20Wang%20and%20Jianwu%20Dang&entry.1292438233=%20%20In%20recent%20advancements%20in%20audio%20self-supervised%20representation%20learning%2C%20the%0Astandard%20Transformer%20architecture%20has%20emerged%20as%20the%20predominant%20approach%2C%20yet%0Aits%20attention%20mechanism%20often%20allocates%20a%20portion%20of%20attention%20weights%20to%0Airrelevant%20information%2C%20potentially%20impairing%20the%20model%27s%20discriminative%0Aability.%20To%20address%20this%2C%20we%20introduce%20a%20differential%20attention%20mechanism%2C%0Awhich%20effectively%20mitigates%20ineffective%20attention%20allocation%20through%20the%0Aintegration%20of%20dual-softmax%20operations%20and%20appropriately%20tuned%20differential%0Acoefficients.%20Experimental%20results%20demonstrate%20that%20our%20ASDA%20model%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20across%20multiple%20benchmarks%2C%20including%20audio%0Aclassification%20%2849.0%25%20mAP%20on%20AS-2M%2C%2041.5%25%20mAP%20on%20AS20K%29%2C%20keyword%20spotting%0A%2898.3%25%20accuracy%20on%20SPC-2%29%2C%20and%20environmental%20sound%20classification%20%2896.1%25%0Aaccuracy%20on%20ESC-50%29.%20These%20results%20highlight%20ASDA%27s%20effectiveness%20in%20audio%0Atasks%2C%20paving%20the%20way%20for%20broader%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02666v1&entry.124074799=Read"},
{"title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching", "author": "Xin Zhou and Dingkang Liang and Kaijin Chen and Tianrui Feng and Xiwu Chen and Hongkai Lin and Yikang Ding and Feiyang Tan and Hengshuang Zhao and Xiang Bai", "abstract": "  Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.\n", "link": "http://arxiv.org/abs/2507.02860v1", "date": "2025-07-03", "relevancy": 2.5378, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.654}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6474}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20Enough%3A%20Training-Free%20Video%20Diffusion%20Acceleration%20via%0A%20%20Runtime-Adaptive%20Caching&body=Title%3A%20Less%20is%20Enough%3A%20Training-Free%20Video%20Diffusion%20Acceleration%20via%0A%20%20Runtime-Adaptive%20Caching%0AAuthor%3A%20Xin%20Zhou%20and%20Dingkang%20Liang%20and%20Kaijin%20Chen%20and%20Tianrui%20Feng%20and%20Xiwu%20Chen%20and%20Hongkai%20Lin%20and%20Yikang%20Ding%20and%20Feiyang%20Tan%20and%20Hengshuang%20Zhao%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Video%20generation%20models%20have%20demonstrated%20remarkable%20performance%2C%20yet%20their%0Abroader%20adoption%20remains%20constrained%20by%20slow%20inference%20speeds%20and%20substantial%0Acomputational%20costs%2C%20primarily%20due%20to%20the%20iterative%20nature%20of%20the%20denoising%0Aprocess.%20Addressing%20this%20bottleneck%20is%20essential%20for%20democratizing%20advanced%0Avideo%20synthesis%20technologies%20and%20enabling%20their%20integration%20into%20real-world%0Aapplications.%20This%20work%20proposes%20EasyCache%2C%20a%20training-free%20acceleration%0Aframework%20for%20video%20diffusion%20models.%20EasyCache%20introduces%20a%20lightweight%2C%0Aruntime-adaptive%20caching%20mechanism%20that%20dynamically%20reuses%20previously%20computed%0Atransformation%20vectors%2C%20avoiding%20redundant%20computations%20during%20inference.%0AUnlike%20prior%20approaches%2C%20EasyCache%20requires%20no%20offline%20profiling%2C%0Apre-computation%2C%20or%20extensive%20parameter%20tuning.%20We%20conduct%20comprehensive%0Astudies%20on%20various%20large-scale%20video%20generation%20models%2C%20including%20OpenSora%2C%0AWan2.1%2C%20and%20HunyuanVideo.%20Our%20method%20achieves%20leading%20acceleration%20performance%2C%0Areducing%20inference%20time%20by%20up%20to%202.1-3.3%24%5Ctimes%24%20compared%20to%20the%20original%0Abaselines%20while%20maintaining%20high%20visual%20fidelity%20with%20a%20significant%20up%20to%2036%25%0APSNR%20improvement%20compared%20to%20the%20previous%20SOTA%20method.%20This%20improvement%20makes%0Aour%20EasyCache%20a%20efficient%20and%20highly%20accessible%20solution%20for%20high-quality%20video%0Ageneration%20in%20both%20research%20and%20practical%20applications.%20The%20code%20is%20available%0Aat%20https%3A//github.com/H-EmbodVis/EasyCache.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520Enough%253A%2520Training-Free%2520Video%2520Diffusion%2520Acceleration%2520via%250A%2520%2520Runtime-Adaptive%2520Caching%26entry.906535625%3DXin%2520Zhou%2520and%2520Dingkang%2520Liang%2520and%2520Kaijin%2520Chen%2520and%2520Tianrui%2520Feng%2520and%2520Xiwu%2520Chen%2520and%2520Hongkai%2520Lin%2520and%2520Yikang%2520Ding%2520and%2520Feiyang%2520Tan%2520and%2520Hengshuang%2520Zhao%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Video%2520generation%2520models%2520have%2520demonstrated%2520remarkable%2520performance%252C%2520yet%2520their%250Abroader%2520adoption%2520remains%2520constrained%2520by%2520slow%2520inference%2520speeds%2520and%2520substantial%250Acomputational%2520costs%252C%2520primarily%2520due%2520to%2520the%2520iterative%2520nature%2520of%2520the%2520denoising%250Aprocess.%2520Addressing%2520this%2520bottleneck%2520is%2520essential%2520for%2520democratizing%2520advanced%250Avideo%2520synthesis%2520technologies%2520and%2520enabling%2520their%2520integration%2520into%2520real-world%250Aapplications.%2520This%2520work%2520proposes%2520EasyCache%252C%2520a%2520training-free%2520acceleration%250Aframework%2520for%2520video%2520diffusion%2520models.%2520EasyCache%2520introduces%2520a%2520lightweight%252C%250Aruntime-adaptive%2520caching%2520mechanism%2520that%2520dynamically%2520reuses%2520previously%2520computed%250Atransformation%2520vectors%252C%2520avoiding%2520redundant%2520computations%2520during%2520inference.%250AUnlike%2520prior%2520approaches%252C%2520EasyCache%2520requires%2520no%2520offline%2520profiling%252C%250Apre-computation%252C%2520or%2520extensive%2520parameter%2520tuning.%2520We%2520conduct%2520comprehensive%250Astudies%2520on%2520various%2520large-scale%2520video%2520generation%2520models%252C%2520including%2520OpenSora%252C%250AWan2.1%252C%2520and%2520HunyuanVideo.%2520Our%2520method%2520achieves%2520leading%2520acceleration%2520performance%252C%250Areducing%2520inference%2520time%2520by%2520up%2520to%25202.1-3.3%2524%255Ctimes%2524%2520compared%2520to%2520the%2520original%250Abaselines%2520while%2520maintaining%2520high%2520visual%2520fidelity%2520with%2520a%2520significant%2520up%2520to%252036%2525%250APSNR%2520improvement%2520compared%2520to%2520the%2520previous%2520SOTA%2520method.%2520This%2520improvement%2520makes%250Aour%2520EasyCache%2520a%2520efficient%2520and%2520highly%2520accessible%2520solution%2520for%2520high-quality%2520video%250Ageneration%2520in%2520both%2520research%2520and%2520practical%2520applications.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/H-EmbodVis/EasyCache.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20Enough%3A%20Training-Free%20Video%20Diffusion%20Acceleration%20via%0A%20%20Runtime-Adaptive%20Caching&entry.906535625=Xin%20Zhou%20and%20Dingkang%20Liang%20and%20Kaijin%20Chen%20and%20Tianrui%20Feng%20and%20Xiwu%20Chen%20and%20Hongkai%20Lin%20and%20Yikang%20Ding%20and%20Feiyang%20Tan%20and%20Hengshuang%20Zhao%20and%20Xiang%20Bai&entry.1292438233=%20%20Video%20generation%20models%20have%20demonstrated%20remarkable%20performance%2C%20yet%20their%0Abroader%20adoption%20remains%20constrained%20by%20slow%20inference%20speeds%20and%20substantial%0Acomputational%20costs%2C%20primarily%20due%20to%20the%20iterative%20nature%20of%20the%20denoising%0Aprocess.%20Addressing%20this%20bottleneck%20is%20essential%20for%20democratizing%20advanced%0Avideo%20synthesis%20technologies%20and%20enabling%20their%20integration%20into%20real-world%0Aapplications.%20This%20work%20proposes%20EasyCache%2C%20a%20training-free%20acceleration%0Aframework%20for%20video%20diffusion%20models.%20EasyCache%20introduces%20a%20lightweight%2C%0Aruntime-adaptive%20caching%20mechanism%20that%20dynamically%20reuses%20previously%20computed%0Atransformation%20vectors%2C%20avoiding%20redundant%20computations%20during%20inference.%0AUnlike%20prior%20approaches%2C%20EasyCache%20requires%20no%20offline%20profiling%2C%0Apre-computation%2C%20or%20extensive%20parameter%20tuning.%20We%20conduct%20comprehensive%0Astudies%20on%20various%20large-scale%20video%20generation%20models%2C%20including%20OpenSora%2C%0AWan2.1%2C%20and%20HunyuanVideo.%20Our%20method%20achieves%20leading%20acceleration%20performance%2C%0Areducing%20inference%20time%20by%20up%20to%202.1-3.3%24%5Ctimes%24%20compared%20to%20the%20original%0Abaselines%20while%20maintaining%20high%20visual%20fidelity%20with%20a%20significant%20up%20to%2036%25%0APSNR%20improvement%20compared%20to%20the%20previous%20SOTA%20method.%20This%20improvement%20makes%0Aour%20EasyCache%20a%20efficient%20and%20highly%20accessible%20solution%20for%20high-quality%20video%0Ageneration%20in%20both%20research%20and%20practical%20applications.%20The%20code%20is%20available%0Aat%20https%3A//github.com/H-EmbodVis/EasyCache.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02860v1&entry.124074799=Read"},
{"title": "AnyI2V: Animating Any Conditional Image with Motion Control", "author": "Ziye Li and Hao Luo and Xincheng Shuai and Henghui Ding", "abstract": "  Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.\n", "link": "http://arxiv.org/abs/2507.02857v1", "date": "2025-07-03", "relevancy": 2.5184, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6306}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6298}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyI2V%3A%20Animating%20Any%20Conditional%20Image%20with%20Motion%20Control&body=Title%3A%20AnyI2V%3A%20Animating%20Any%20Conditional%20Image%20with%20Motion%20Control%0AAuthor%3A%20Ziye%20Li%20and%20Hao%20Luo%20and%20Xincheng%20Shuai%20and%20Henghui%20Ding%0AAbstract%3A%20%20%20Recent%20advancements%20in%20video%20generation%2C%20particularly%20in%20diffusion%20models%2C%0Ahave%20driven%20notable%20progress%20in%20text-to-video%20%28T2V%29%20and%20image-to-video%20%28I2V%29%0Asynthesis.%20However%2C%20challenges%20remain%20in%20effectively%20integrating%20dynamic%20motion%0Asignals%20and%20flexible%20spatial%20constraints.%20Existing%20T2V%20methods%20typically%20rely%0Aon%20text%20prompts%2C%20which%20inherently%20lack%20precise%20control%20over%20the%20spatial%20layout%0Aof%20generated%20content.%20In%20contrast%2C%20I2V%20methods%20are%20limited%20by%20their%20dependence%0Aon%20real%20images%2C%20which%20restricts%20the%20editability%20of%20the%20synthesized%20content.%0AAlthough%20some%20methods%20incorporate%20ControlNet%20to%20introduce%20image-based%0Aconditioning%2C%20they%20often%20lack%20explicit%20motion%20control%20and%20require%0Acomputationally%20expensive%20training.%20To%20address%20these%20limitations%2C%20we%20propose%0AAnyI2V%2C%20a%20training-free%20framework%20that%20animates%20any%20conditional%20images%20with%0Auser-defined%20motion%20trajectories.%20AnyI2V%20supports%20a%20broader%20range%20of%20modalities%0Aas%20the%20conditional%20image%2C%20including%20data%20types%20such%20as%20meshes%20and%20point%20clouds%0Athat%20are%20not%20supported%20by%20ControlNet%2C%20enabling%20more%20flexible%20and%20versatile%0Avideo%20generation.%20Additionally%2C%20it%20supports%20mixed%20conditional%20inputs%20and%0Aenables%20style%20transfer%20and%20editing%20via%20LoRA%20and%20text%20prompts.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20AnyI2V%20achieves%20superior%20performance%0Aand%20provides%20a%20new%20perspective%20in%20spatial-%20and%20motion-controlled%20video%0Ageneration.%20Code%20is%20available%20at%20https%3A//henghuiding.com/AnyI2V/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyI2V%253A%2520Animating%2520Any%2520Conditional%2520Image%2520with%2520Motion%2520Control%26entry.906535625%3DZiye%2520Li%2520and%2520Hao%2520Luo%2520and%2520Xincheng%2520Shuai%2520and%2520Henghui%2520Ding%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520video%2520generation%252C%2520particularly%2520in%2520diffusion%2520models%252C%250Ahave%2520driven%2520notable%2520progress%2520in%2520text-to-video%2520%2528T2V%2529%2520and%2520image-to-video%2520%2528I2V%2529%250Asynthesis.%2520However%252C%2520challenges%2520remain%2520in%2520effectively%2520integrating%2520dynamic%2520motion%250Asignals%2520and%2520flexible%2520spatial%2520constraints.%2520Existing%2520T2V%2520methods%2520typically%2520rely%250Aon%2520text%2520prompts%252C%2520which%2520inherently%2520lack%2520precise%2520control%2520over%2520the%2520spatial%2520layout%250Aof%2520generated%2520content.%2520In%2520contrast%252C%2520I2V%2520methods%2520are%2520limited%2520by%2520their%2520dependence%250Aon%2520real%2520images%252C%2520which%2520restricts%2520the%2520editability%2520of%2520the%2520synthesized%2520content.%250AAlthough%2520some%2520methods%2520incorporate%2520ControlNet%2520to%2520introduce%2520image-based%250Aconditioning%252C%2520they%2520often%2520lack%2520explicit%2520motion%2520control%2520and%2520require%250Acomputationally%2520expensive%2520training.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250AAnyI2V%252C%2520a%2520training-free%2520framework%2520that%2520animates%2520any%2520conditional%2520images%2520with%250Auser-defined%2520motion%2520trajectories.%2520AnyI2V%2520supports%2520a%2520broader%2520range%2520of%2520modalities%250Aas%2520the%2520conditional%2520image%252C%2520including%2520data%2520types%2520such%2520as%2520meshes%2520and%2520point%2520clouds%250Athat%2520are%2520not%2520supported%2520by%2520ControlNet%252C%2520enabling%2520more%2520flexible%2520and%2520versatile%250Avideo%2520generation.%2520Additionally%252C%2520it%2520supports%2520mixed%2520conditional%2520inputs%2520and%250Aenables%2520style%2520transfer%2520and%2520editing%2520via%2520LoRA%2520and%2520text%2520prompts.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520AnyI2V%2520achieves%2520superior%2520performance%250Aand%2520provides%2520a%2520new%2520perspective%2520in%2520spatial-%2520and%2520motion-controlled%2520video%250Ageneration.%2520Code%2520is%2520available%2520at%2520https%253A//henghuiding.com/AnyI2V/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyI2V%3A%20Animating%20Any%20Conditional%20Image%20with%20Motion%20Control&entry.906535625=Ziye%20Li%20and%20Hao%20Luo%20and%20Xincheng%20Shuai%20and%20Henghui%20Ding&entry.1292438233=%20%20Recent%20advancements%20in%20video%20generation%2C%20particularly%20in%20diffusion%20models%2C%0Ahave%20driven%20notable%20progress%20in%20text-to-video%20%28T2V%29%20and%20image-to-video%20%28I2V%29%0Asynthesis.%20However%2C%20challenges%20remain%20in%20effectively%20integrating%20dynamic%20motion%0Asignals%20and%20flexible%20spatial%20constraints.%20Existing%20T2V%20methods%20typically%20rely%0Aon%20text%20prompts%2C%20which%20inherently%20lack%20precise%20control%20over%20the%20spatial%20layout%0Aof%20generated%20content.%20In%20contrast%2C%20I2V%20methods%20are%20limited%20by%20their%20dependence%0Aon%20real%20images%2C%20which%20restricts%20the%20editability%20of%20the%20synthesized%20content.%0AAlthough%20some%20methods%20incorporate%20ControlNet%20to%20introduce%20image-based%0Aconditioning%2C%20they%20often%20lack%20explicit%20motion%20control%20and%20require%0Acomputationally%20expensive%20training.%20To%20address%20these%20limitations%2C%20we%20propose%0AAnyI2V%2C%20a%20training-free%20framework%20that%20animates%20any%20conditional%20images%20with%0Auser-defined%20motion%20trajectories.%20AnyI2V%20supports%20a%20broader%20range%20of%20modalities%0Aas%20the%20conditional%20image%2C%20including%20data%20types%20such%20as%20meshes%20and%20point%20clouds%0Athat%20are%20not%20supported%20by%20ControlNet%2C%20enabling%20more%20flexible%20and%20versatile%0Avideo%20generation.%20Additionally%2C%20it%20supports%20mixed%20conditional%20inputs%20and%0Aenables%20style%20transfer%20and%20editing%20via%20LoRA%20and%20text%20prompts.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20AnyI2V%20achieves%20superior%20performance%0Aand%20provides%20a%20new%20perspective%20in%20spatial-%20and%20motion-controlled%20video%0Ageneration.%20Code%20is%20available%20at%20https%3A//henghuiding.com/AnyI2V/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02857v1&entry.124074799=Read"},
{"title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context\n  Injection", "author": "Ziqi Miao and Yi Ding and Lijun Li and Jing Shao", "abstract": "  With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack.\n", "link": "http://arxiv.org/abs/2507.02844v1", "date": "2025-07-03", "relevancy": 2.5162, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Contextual%20Attack%3A%20Jailbreaking%20MLLMs%20with%20Image-Driven%20Context%0A%20%20Injection&body=Title%3A%20Visual%20Contextual%20Attack%3A%20Jailbreaking%20MLLMs%20with%20Image-Driven%20Context%0A%20%20Injection%0AAuthor%3A%20Ziqi%20Miao%20and%20Yi%20Ding%20and%20Lijun%20Li%20and%20Jing%20Shao%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20strong%20visual-language%20capabilities%2C%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20have%20demonstrated%20tremendous%20potential%20for%20real-world%0Aapplications.%20However%2C%20the%20security%20vulnerabilities%20exhibited%20by%20the%20visual%0Amodality%20pose%20significant%20challenges%20to%20deploying%20such%20models%20in%20open-world%0Aenvironments.%20Recent%20studies%20have%20successfully%20induced%20harmful%20responses%20from%0Atarget%20MLLMs%20by%20encoding%20harmful%20textual%20semantics%20directly%20into%20visual%20inputs.%0AHowever%2C%20in%20these%20approaches%2C%20the%20visual%20modality%20primarily%20serves%20as%20a%20trigger%0Afor%20unsafe%20behavior%2C%20often%20exhibiting%20semantic%20ambiguity%20and%20lacking%20grounding%0Ain%20realistic%20scenarios.%20In%20this%20work%2C%20we%20define%20a%20novel%20setting%3A%20visual-centric%0Ajailbreak%2C%20where%20visual%20information%20serves%20as%20a%20necessary%20component%20in%0Aconstructing%20a%20complete%20and%20realistic%20jailbreak%20context.%20Building%20on%20this%0Asetting%2C%20we%20propose%20the%20VisCo%20%28Visual%20Contextual%29%20Attack.%20VisCo%20fabricates%0Acontextual%20dialogue%20using%20four%20distinct%20visual-focused%20strategies%2C%20dynamically%0Agenerating%20auxiliary%20images%20when%20necessary%20to%20construct%20a%20visual-centric%0Ajailbreak%20scenario.%20To%20maximize%20attack%20effectiveness%2C%20it%20incorporates%20automatic%0Atoxicity%20obfuscation%20and%20semantic%20refinement%20to%20produce%20a%20final%20attack%20prompt%0Athat%20reliably%20triggers%20harmful%20responses%20from%20the%20target%20black-box%20MLLMs.%0ASpecifically%2C%20VisCo%20achieves%20a%20toxicity%20score%20of%204.78%20and%20an%20Attack%20Success%0ARate%20%28ASR%29%20of%2085%25%20on%20MM-SafetyBench%20against%20GPT-4o%2C%20significantly%20outperforming%0Athe%20baseline%2C%20which%20performs%20a%20toxicity%20score%20of%202.48%20and%20an%20ASR%20of%2022.2%25.%20The%0Acode%20is%20available%20at%20https%3A//github.com/Dtc7w3PQ/Visco-Attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Contextual%2520Attack%253A%2520Jailbreaking%2520MLLMs%2520with%2520Image-Driven%2520Context%250A%2520%2520Injection%26entry.906535625%3DZiqi%2520Miao%2520and%2520Yi%2520Ding%2520and%2520Lijun%2520Li%2520and%2520Jing%2520Shao%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520strong%2520visual-language%2520capabilities%252C%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520tremendous%2520potential%2520for%2520real-world%250Aapplications.%2520However%252C%2520the%2520security%2520vulnerabilities%2520exhibited%2520by%2520the%2520visual%250Amodality%2520pose%2520significant%2520challenges%2520to%2520deploying%2520such%2520models%2520in%2520open-world%250Aenvironments.%2520Recent%2520studies%2520have%2520successfully%2520induced%2520harmful%2520responses%2520from%250Atarget%2520MLLMs%2520by%2520encoding%2520harmful%2520textual%2520semantics%2520directly%2520into%2520visual%2520inputs.%250AHowever%252C%2520in%2520these%2520approaches%252C%2520the%2520visual%2520modality%2520primarily%2520serves%2520as%2520a%2520trigger%250Afor%2520unsafe%2520behavior%252C%2520often%2520exhibiting%2520semantic%2520ambiguity%2520and%2520lacking%2520grounding%250Ain%2520realistic%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520define%2520a%2520novel%2520setting%253A%2520visual-centric%250Ajailbreak%252C%2520where%2520visual%2520information%2520serves%2520as%2520a%2520necessary%2520component%2520in%250Aconstructing%2520a%2520complete%2520and%2520realistic%2520jailbreak%2520context.%2520Building%2520on%2520this%250Asetting%252C%2520we%2520propose%2520the%2520VisCo%2520%2528Visual%2520Contextual%2529%2520Attack.%2520VisCo%2520fabricates%250Acontextual%2520dialogue%2520using%2520four%2520distinct%2520visual-focused%2520strategies%252C%2520dynamically%250Agenerating%2520auxiliary%2520images%2520when%2520necessary%2520to%2520construct%2520a%2520visual-centric%250Ajailbreak%2520scenario.%2520To%2520maximize%2520attack%2520effectiveness%252C%2520it%2520incorporates%2520automatic%250Atoxicity%2520obfuscation%2520and%2520semantic%2520refinement%2520to%2520produce%2520a%2520final%2520attack%2520prompt%250Athat%2520reliably%2520triggers%2520harmful%2520responses%2520from%2520the%2520target%2520black-box%2520MLLMs.%250ASpecifically%252C%2520VisCo%2520achieves%2520a%2520toxicity%2520score%2520of%25204.78%2520and%2520an%2520Attack%2520Success%250ARate%2520%2528ASR%2529%2520of%252085%2525%2520on%2520MM-SafetyBench%2520against%2520GPT-4o%252C%2520significantly%2520outperforming%250Athe%2520baseline%252C%2520which%2520performs%2520a%2520toxicity%2520score%2520of%25202.48%2520and%2520an%2520ASR%2520of%252022.2%2525.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/Dtc7w3PQ/Visco-Attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Contextual%20Attack%3A%20Jailbreaking%20MLLMs%20with%20Image-Driven%20Context%0A%20%20Injection&entry.906535625=Ziqi%20Miao%20and%20Yi%20Ding%20and%20Lijun%20Li%20and%20Jing%20Shao&entry.1292438233=%20%20With%20the%20emergence%20of%20strong%20visual-language%20capabilities%2C%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20have%20demonstrated%20tremendous%20potential%20for%20real-world%0Aapplications.%20However%2C%20the%20security%20vulnerabilities%20exhibited%20by%20the%20visual%0Amodality%20pose%20significant%20challenges%20to%20deploying%20such%20models%20in%20open-world%0Aenvironments.%20Recent%20studies%20have%20successfully%20induced%20harmful%20responses%20from%0Atarget%20MLLMs%20by%20encoding%20harmful%20textual%20semantics%20directly%20into%20visual%20inputs.%0AHowever%2C%20in%20these%20approaches%2C%20the%20visual%20modality%20primarily%20serves%20as%20a%20trigger%0Afor%20unsafe%20behavior%2C%20often%20exhibiting%20semantic%20ambiguity%20and%20lacking%20grounding%0Ain%20realistic%20scenarios.%20In%20this%20work%2C%20we%20define%20a%20novel%20setting%3A%20visual-centric%0Ajailbreak%2C%20where%20visual%20information%20serves%20as%20a%20necessary%20component%20in%0Aconstructing%20a%20complete%20and%20realistic%20jailbreak%20context.%20Building%20on%20this%0Asetting%2C%20we%20propose%20the%20VisCo%20%28Visual%20Contextual%29%20Attack.%20VisCo%20fabricates%0Acontextual%20dialogue%20using%20four%20distinct%20visual-focused%20strategies%2C%20dynamically%0Agenerating%20auxiliary%20images%20when%20necessary%20to%20construct%20a%20visual-centric%0Ajailbreak%20scenario.%20To%20maximize%20attack%20effectiveness%2C%20it%20incorporates%20automatic%0Atoxicity%20obfuscation%20and%20semantic%20refinement%20to%20produce%20a%20final%20attack%20prompt%0Athat%20reliably%20triggers%20harmful%20responses%20from%20the%20target%20black-box%20MLLMs.%0ASpecifically%2C%20VisCo%20achieves%20a%20toxicity%20score%20of%204.78%20and%20an%20Attack%20Success%0ARate%20%28ASR%29%20of%2085%25%20on%20MM-SafetyBench%20against%20GPT-4o%2C%20significantly%20outperforming%0Athe%20baseline%2C%20which%20performs%20a%20toxicity%20score%20of%202.48%20and%20an%20ASR%20of%2022.2%25.%20The%0Acode%20is%20available%20at%20https%3A//github.com/Dtc7w3PQ/Visco-Attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02844v1&entry.124074799=Read"},
{"title": "UVLM: Benchmarking Video Language Model for Underwater World\n  Understanding", "author": "Xizhe Xue and Yang Zhou and Dawei Yan and Ying Li and Haokui Zhang and Rong Xiao", "abstract": "  Recently, the remarkable success of large language models (LLMs) has achieved\na profound impact on the field of artificial intelligence. Numerous advanced\nworks based on LLMs have been proposed and applied in various scenarios. Among\nthem, video language models (VidLMs) are particularly widely used. However,\nexisting works primarily focus on terrestrial scenarios, overlooking the highly\ndemanding application needs of underwater observation. To overcome this gap, we\nintroduce UVLM, an under water observation benchmark which is build through a\ncollaborative approach combining human expertise and AI models. To ensure data\nquality, we have conducted in-depth considerations from multiple perspectives.\nFirst, to address the unique challenges of underwater environments, we selected\nvideos that represent typical underwater challenges including light variations,\nwater turbidity, and diverse viewing angles to construct the dataset. Second,\nto ensure data diversity, the dataset covers a wide range of frame rates,\nresolutions, 419 classes of marine animals, and various static plants and\nterrains. Next, for task diversity, we adopted a structured design where\nobservation targets are categorized into two major classes: biological and\nenvironmental. Each category includes content observation and change/action\nobservation, totaling 20 distinct task types. Finally, we designed several\nchallenging evaluation metrics to enable quantitative comparison and analysis\nof different methods. Experiments on two representative VidLMs demonstrate that\nfine-tuning VidLMs on UVLM significantly improves underwater world\nunderstanding while also showing potential for slight improvements on existing\nin-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and\nprompt engineering will be released publicly.\n", "link": "http://arxiv.org/abs/2507.02373v1", "date": "2025-07-03", "relevancy": 2.5093, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.636}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UVLM%3A%20Benchmarking%20Video%20Language%20Model%20for%20Underwater%20World%0A%20%20Understanding&body=Title%3A%20UVLM%3A%20Benchmarking%20Video%20Language%20Model%20for%20Underwater%20World%0A%20%20Understanding%0AAuthor%3A%20Xizhe%20Xue%20and%20Yang%20Zhou%20and%20Dawei%20Yan%20and%20Ying%20Li%20and%20Haokui%20Zhang%20and%20Rong%20Xiao%0AAbstract%3A%20%20%20Recently%2C%20the%20remarkable%20success%20of%20large%20language%20models%20%28LLMs%29%20has%20achieved%0Aa%20profound%20impact%20on%20the%20field%20of%20artificial%20intelligence.%20Numerous%20advanced%0Aworks%20based%20on%20LLMs%20have%20been%20proposed%20and%20applied%20in%20various%20scenarios.%20Among%0Athem%2C%20video%20language%20models%20%28VidLMs%29%20are%20particularly%20widely%20used.%20However%2C%0Aexisting%20works%20primarily%20focus%20on%20terrestrial%20scenarios%2C%20overlooking%20the%20highly%0Ademanding%20application%20needs%20of%20underwater%20observation.%20To%20overcome%20this%20gap%2C%20we%0Aintroduce%20UVLM%2C%20an%20under%20water%20observation%20benchmark%20which%20is%20build%20through%20a%0Acollaborative%20approach%20combining%20human%20expertise%20and%20AI%20models.%20To%20ensure%20data%0Aquality%2C%20we%20have%20conducted%20in-depth%20considerations%20from%20multiple%20perspectives.%0AFirst%2C%20to%20address%20the%20unique%20challenges%20of%20underwater%20environments%2C%20we%20selected%0Avideos%20that%20represent%20typical%20underwater%20challenges%20including%20light%20variations%2C%0Awater%20turbidity%2C%20and%20diverse%20viewing%20angles%20to%20construct%20the%20dataset.%20Second%2C%0Ato%20ensure%20data%20diversity%2C%20the%20dataset%20covers%20a%20wide%20range%20of%20frame%20rates%2C%0Aresolutions%2C%20419%20classes%20of%20marine%20animals%2C%20and%20various%20static%20plants%20and%0Aterrains.%20Next%2C%20for%20task%20diversity%2C%20we%20adopted%20a%20structured%20design%20where%0Aobservation%20targets%20are%20categorized%20into%20two%20major%20classes%3A%20biological%20and%0Aenvironmental.%20Each%20category%20includes%20content%20observation%20and%20change/action%0Aobservation%2C%20totaling%2020%20distinct%20task%20types.%20Finally%2C%20we%20designed%20several%0Achallenging%20evaluation%20metrics%20to%20enable%20quantitative%20comparison%20and%20analysis%0Aof%20different%20methods.%20Experiments%20on%20two%20representative%20VidLMs%20demonstrate%20that%0Afine-tuning%20VidLMs%20on%20UVLM%20significantly%20improves%20underwater%20world%0Aunderstanding%20while%20also%20showing%20potential%20for%20slight%20improvements%20on%20existing%0Ain-air%20VidLM%20benchmarks%2C%20such%20as%20VideoMME%20and%20Perception%20text.%20The%20dataset%20and%0Aprompt%20engineering%20will%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUVLM%253A%2520Benchmarking%2520Video%2520Language%2520Model%2520for%2520Underwater%2520World%250A%2520%2520Understanding%26entry.906535625%3DXizhe%2520Xue%2520and%2520Yang%2520Zhou%2520and%2520Dawei%2520Yan%2520and%2520Ying%2520Li%2520and%2520Haokui%2520Zhang%2520and%2520Rong%2520Xiao%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520remarkable%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520achieved%250Aa%2520profound%2520impact%2520on%2520the%2520field%2520of%2520artificial%2520intelligence.%2520Numerous%2520advanced%250Aworks%2520based%2520on%2520LLMs%2520have%2520been%2520proposed%2520and%2520applied%2520in%2520various%2520scenarios.%2520Among%250Athem%252C%2520video%2520language%2520models%2520%2528VidLMs%2529%2520are%2520particularly%2520widely%2520used.%2520However%252C%250Aexisting%2520works%2520primarily%2520focus%2520on%2520terrestrial%2520scenarios%252C%2520overlooking%2520the%2520highly%250Ademanding%2520application%2520needs%2520of%2520underwater%2520observation.%2520To%2520overcome%2520this%2520gap%252C%2520we%250Aintroduce%2520UVLM%252C%2520an%2520under%2520water%2520observation%2520benchmark%2520which%2520is%2520build%2520through%2520a%250Acollaborative%2520approach%2520combining%2520human%2520expertise%2520and%2520AI%2520models.%2520To%2520ensure%2520data%250Aquality%252C%2520we%2520have%2520conducted%2520in-depth%2520considerations%2520from%2520multiple%2520perspectives.%250AFirst%252C%2520to%2520address%2520the%2520unique%2520challenges%2520of%2520underwater%2520environments%252C%2520we%2520selected%250Avideos%2520that%2520represent%2520typical%2520underwater%2520challenges%2520including%2520light%2520variations%252C%250Awater%2520turbidity%252C%2520and%2520diverse%2520viewing%2520angles%2520to%2520construct%2520the%2520dataset.%2520Second%252C%250Ato%2520ensure%2520data%2520diversity%252C%2520the%2520dataset%2520covers%2520a%2520wide%2520range%2520of%2520frame%2520rates%252C%250Aresolutions%252C%2520419%2520classes%2520of%2520marine%2520animals%252C%2520and%2520various%2520static%2520plants%2520and%250Aterrains.%2520Next%252C%2520for%2520task%2520diversity%252C%2520we%2520adopted%2520a%2520structured%2520design%2520where%250Aobservation%2520targets%2520are%2520categorized%2520into%2520two%2520major%2520classes%253A%2520biological%2520and%250Aenvironmental.%2520Each%2520category%2520includes%2520content%2520observation%2520and%2520change/action%250Aobservation%252C%2520totaling%252020%2520distinct%2520task%2520types.%2520Finally%252C%2520we%2520designed%2520several%250Achallenging%2520evaluation%2520metrics%2520to%2520enable%2520quantitative%2520comparison%2520and%2520analysis%250Aof%2520different%2520methods.%2520Experiments%2520on%2520two%2520representative%2520VidLMs%2520demonstrate%2520that%250Afine-tuning%2520VidLMs%2520on%2520UVLM%2520significantly%2520improves%2520underwater%2520world%250Aunderstanding%2520while%2520also%2520showing%2520potential%2520for%2520slight%2520improvements%2520on%2520existing%250Ain-air%2520VidLM%2520benchmarks%252C%2520such%2520as%2520VideoMME%2520and%2520Perception%2520text.%2520The%2520dataset%2520and%250Aprompt%2520engineering%2520will%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UVLM%3A%20Benchmarking%20Video%20Language%20Model%20for%20Underwater%20World%0A%20%20Understanding&entry.906535625=Xizhe%20Xue%20and%20Yang%20Zhou%20and%20Dawei%20Yan%20and%20Ying%20Li%20and%20Haokui%20Zhang%20and%20Rong%20Xiao&entry.1292438233=%20%20Recently%2C%20the%20remarkable%20success%20of%20large%20language%20models%20%28LLMs%29%20has%20achieved%0Aa%20profound%20impact%20on%20the%20field%20of%20artificial%20intelligence.%20Numerous%20advanced%0Aworks%20based%20on%20LLMs%20have%20been%20proposed%20and%20applied%20in%20various%20scenarios.%20Among%0Athem%2C%20video%20language%20models%20%28VidLMs%29%20are%20particularly%20widely%20used.%20However%2C%0Aexisting%20works%20primarily%20focus%20on%20terrestrial%20scenarios%2C%20overlooking%20the%20highly%0Ademanding%20application%20needs%20of%20underwater%20observation.%20To%20overcome%20this%20gap%2C%20we%0Aintroduce%20UVLM%2C%20an%20under%20water%20observation%20benchmark%20which%20is%20build%20through%20a%0Acollaborative%20approach%20combining%20human%20expertise%20and%20AI%20models.%20To%20ensure%20data%0Aquality%2C%20we%20have%20conducted%20in-depth%20considerations%20from%20multiple%20perspectives.%0AFirst%2C%20to%20address%20the%20unique%20challenges%20of%20underwater%20environments%2C%20we%20selected%0Avideos%20that%20represent%20typical%20underwater%20challenges%20including%20light%20variations%2C%0Awater%20turbidity%2C%20and%20diverse%20viewing%20angles%20to%20construct%20the%20dataset.%20Second%2C%0Ato%20ensure%20data%20diversity%2C%20the%20dataset%20covers%20a%20wide%20range%20of%20frame%20rates%2C%0Aresolutions%2C%20419%20classes%20of%20marine%20animals%2C%20and%20various%20static%20plants%20and%0Aterrains.%20Next%2C%20for%20task%20diversity%2C%20we%20adopted%20a%20structured%20design%20where%0Aobservation%20targets%20are%20categorized%20into%20two%20major%20classes%3A%20biological%20and%0Aenvironmental.%20Each%20category%20includes%20content%20observation%20and%20change/action%0Aobservation%2C%20totaling%2020%20distinct%20task%20types.%20Finally%2C%20we%20designed%20several%0Achallenging%20evaluation%20metrics%20to%20enable%20quantitative%20comparison%20and%20analysis%0Aof%20different%20methods.%20Experiments%20on%20two%20representative%20VidLMs%20demonstrate%20that%0Afine-tuning%20VidLMs%20on%20UVLM%20significantly%20improves%20underwater%20world%0Aunderstanding%20while%20also%20showing%20potential%20for%20slight%20improvements%20on%20existing%0Ain-air%20VidLM%20benchmarks%2C%20such%20as%20VideoMME%20and%20Perception%20text.%20The%20dataset%20and%0Aprompt%20engineering%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02373v1&entry.124074799=Read"},
{"title": "Empowering Intelligent Low-altitude Economy with Large AI Model\n  Deployment", "author": "Zhonghao Lyu and Yulan Gao and Junting Chen and Hongyang Du and Jie Xu and Kaibin Huang and Dong In Kim", "abstract": "  Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research.\n", "link": "http://arxiv.org/abs/2505.22343v2", "date": "2025-07-03", "relevancy": 2.5066, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.516}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment&body=Title%3A%20Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment%0AAuthor%3A%20Zhonghao%20Lyu%20and%20Yulan%20Gao%20and%20Junting%20Chen%20and%20Hongyang%20Du%20and%20Jie%20Xu%20and%20Kaibin%20Huang%20and%20Dong%20In%20Kim%0AAbstract%3A%20%20%20Low-altitude%20economy%20%28LAE%29%20represents%20an%20emerging%20economic%20paradigm%20that%0Aredefines%20commercial%20and%20social%20aerial%20activities.%20Large%20artificial%0Aintelligence%20models%20%28LAIMs%29%20offer%20transformative%20potential%20to%20further%20enhance%0Athe%20intelligence%20of%20LAE%20services.%20However%2C%20deploying%20LAIMs%20in%20LAE%20poses%20several%0Achallenges%2C%20including%20the%20significant%20gap%20between%20their%20computational/storage%0Ademands%20and%20the%20limited%20onboard%20resources%20of%20LAE%20entities%2C%20the%20mismatch%20between%0Alab-trained%20LAIMs%20and%20dynamic%20physical%20environments%2C%20and%20the%20inefficiencies%20of%0Atraditional%20decoupled%20designs%20for%20sensing%2C%20communication%2C%20and%20computation.%20To%0Aaddress%20these%20issues%2C%20we%20first%20propose%20a%20hierarchical%20system%20architecture%0Atailored%20for%20LAIM%20deployment%20and%20present%20representative%20LAE%20application%0Ascenarios.%20Next%2C%20we%20explore%20key%20enabling%20techniques%20that%20facilitate%20the%20mutual%0Aco-evolution%20of%20LAIMs%20and%20low-altitude%20systems%2C%20and%20introduce%20a%20task-oriented%0Aexecution%20pipeline%20for%20scalable%20and%20adaptive%20service%20delivery.%20Then%2C%20the%0Aproposed%20framework%20is%20validated%20through%20real-world%20case%20studies.%20Finally%2C%20we%0Aoutline%20open%20challenges%20to%20inspire%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Intelligent%2520Low-altitude%2520Economy%2520with%2520Large%2520AI%2520Model%250A%2520%2520Deployment%26entry.906535625%3DZhonghao%2520Lyu%2520and%2520Yulan%2520Gao%2520and%2520Junting%2520Chen%2520and%2520Hongyang%2520Du%2520and%2520Jie%2520Xu%2520and%2520Kaibin%2520Huang%2520and%2520Dong%2520In%2520Kim%26entry.1292438233%3D%2520%2520Low-altitude%2520economy%2520%2528LAE%2529%2520represents%2520an%2520emerging%2520economic%2520paradigm%2520that%250Aredefines%2520commercial%2520and%2520social%2520aerial%2520activities.%2520Large%2520artificial%250Aintelligence%2520models%2520%2528LAIMs%2529%2520offer%2520transformative%2520potential%2520to%2520further%2520enhance%250Athe%2520intelligence%2520of%2520LAE%2520services.%2520However%252C%2520deploying%2520LAIMs%2520in%2520LAE%2520poses%2520several%250Achallenges%252C%2520including%2520the%2520significant%2520gap%2520between%2520their%2520computational/storage%250Ademands%2520and%2520the%2520limited%2520onboard%2520resources%2520of%2520LAE%2520entities%252C%2520the%2520mismatch%2520between%250Alab-trained%2520LAIMs%2520and%2520dynamic%2520physical%2520environments%252C%2520and%2520the%2520inefficiencies%2520of%250Atraditional%2520decoupled%2520designs%2520for%2520sensing%252C%2520communication%252C%2520and%2520computation.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520first%2520propose%2520a%2520hierarchical%2520system%2520architecture%250Atailored%2520for%2520LAIM%2520deployment%2520and%2520present%2520representative%2520LAE%2520application%250Ascenarios.%2520Next%252C%2520we%2520explore%2520key%2520enabling%2520techniques%2520that%2520facilitate%2520the%2520mutual%250Aco-evolution%2520of%2520LAIMs%2520and%2520low-altitude%2520systems%252C%2520and%2520introduce%2520a%2520task-oriented%250Aexecution%2520pipeline%2520for%2520scalable%2520and%2520adaptive%2520service%2520delivery.%2520Then%252C%2520the%250Aproposed%2520framework%2520is%2520validated%2520through%2520real-world%2520case%2520studies.%2520Finally%252C%2520we%250Aoutline%2520open%2520challenges%2520to%2520inspire%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment&entry.906535625=Zhonghao%20Lyu%20and%20Yulan%20Gao%20and%20Junting%20Chen%20and%20Hongyang%20Du%20and%20Jie%20Xu%20and%20Kaibin%20Huang%20and%20Dong%20In%20Kim&entry.1292438233=%20%20Low-altitude%20economy%20%28LAE%29%20represents%20an%20emerging%20economic%20paradigm%20that%0Aredefines%20commercial%20and%20social%20aerial%20activities.%20Large%20artificial%0Aintelligence%20models%20%28LAIMs%29%20offer%20transformative%20potential%20to%20further%20enhance%0Athe%20intelligence%20of%20LAE%20services.%20However%2C%20deploying%20LAIMs%20in%20LAE%20poses%20several%0Achallenges%2C%20including%20the%20significant%20gap%20between%20their%20computational/storage%0Ademands%20and%20the%20limited%20onboard%20resources%20of%20LAE%20entities%2C%20the%20mismatch%20between%0Alab-trained%20LAIMs%20and%20dynamic%20physical%20environments%2C%20and%20the%20inefficiencies%20of%0Atraditional%20decoupled%20designs%20for%20sensing%2C%20communication%2C%20and%20computation.%20To%0Aaddress%20these%20issues%2C%20we%20first%20propose%20a%20hierarchical%20system%20architecture%0Atailored%20for%20LAIM%20deployment%20and%20present%20representative%20LAE%20application%0Ascenarios.%20Next%2C%20we%20explore%20key%20enabling%20techniques%20that%20facilitate%20the%20mutual%0Aco-evolution%20of%20LAIMs%20and%20low-altitude%20systems%2C%20and%20introduce%20a%20task-oriented%0Aexecution%20pipeline%20for%20scalable%20and%20adaptive%20service%20delivery.%20Then%2C%20the%0Aproposed%20framework%20is%20validated%20through%20real-world%20case%20studies.%20Finally%2C%20we%0Aoutline%20open%20challenges%20to%20inspire%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22343v2&entry.124074799=Read"},
{"title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided\n  Multi-Class Image Generation", "author": "Qin Guo and Ailing Zeng and Dongxu Yue and Ceyuan Yang and Yang Cao and Hanzhong Guo and Fei Shen and Wei Liu and Xihui Liu and Dan Xu", "abstract": "  Although significant advancements have been achieved in the progress of\nkeypoint-guided Text-to-Image diffusion models, existing mainstream\nkeypoint-guided models encounter challenges in controlling the generation of\nmore general non-rigid objects beyond humans (e.g., animals). Moreover, it is\ndifficult to generate multiple overlapping humans and animals based on keypoint\ncontrols solely. These challenges arise from two main aspects: the inherent\nlimitations of existing controllable methods and the lack of suitable datasets.\nFirst, we design a DiT-based framework, named UniMC, to explore unifying\ncontrollable multi-class image generation. UniMC integrates instance- and\nkeypoint-level conditions into compact tokens, incorporating attributes such as\nclass, bounding box, and keypoint coordinates. This approach overcomes the\nlimitations of previous methods that struggled to distinguish instances and\nclasses due to their reliance on skeleton images as conditions. Second, we\npropose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed\nfor keypoint-guided human and animal image generation. HAIG-2.9M includes 786K\nimages with 2.9M instances. This dataset features extensive annotations such as\nkeypoints, bounding boxes, and fine-grained captions for both humans and\nanimals, along with rigorous manual inspection to ensure annotation accuracy.\nExtensive experiments demonstrate the high quality of HAIG-2.9M and the\neffectiveness of UniMC, particularly in heavy occlusions and multi-class\nscenarios.\n", "link": "http://arxiv.org/abs/2507.02713v1", "date": "2025-07-03", "relevancy": 2.5012, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6311}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6254}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMC%3A%20Taming%20Diffusion%20Transformer%20for%20Unified%20Keypoint-Guided%0A%20%20Multi-Class%20Image%20Generation&body=Title%3A%20UniMC%3A%20Taming%20Diffusion%20Transformer%20for%20Unified%20Keypoint-Guided%0A%20%20Multi-Class%20Image%20Generation%0AAuthor%3A%20Qin%20Guo%20and%20Ailing%20Zeng%20and%20Dongxu%20Yue%20and%20Ceyuan%20Yang%20and%20Yang%20Cao%20and%20Hanzhong%20Guo%20and%20Fei%20Shen%20and%20Wei%20Liu%20and%20Xihui%20Liu%20and%20Dan%20Xu%0AAbstract%3A%20%20%20Although%20significant%20advancements%20have%20been%20achieved%20in%20the%20progress%20of%0Akeypoint-guided%20Text-to-Image%20diffusion%20models%2C%20existing%20mainstream%0Akeypoint-guided%20models%20encounter%20challenges%20in%20controlling%20the%20generation%20of%0Amore%20general%20non-rigid%20objects%20beyond%20humans%20%28e.g.%2C%20animals%29.%20Moreover%2C%20it%20is%0Adifficult%20to%20generate%20multiple%20overlapping%20humans%20and%20animals%20based%20on%20keypoint%0Acontrols%20solely.%20These%20challenges%20arise%20from%20two%20main%20aspects%3A%20the%20inherent%0Alimitations%20of%20existing%20controllable%20methods%20and%20the%20lack%20of%20suitable%20datasets.%0AFirst%2C%20we%20design%20a%20DiT-based%20framework%2C%20named%20UniMC%2C%20to%20explore%20unifying%0Acontrollable%20multi-class%20image%20generation.%20UniMC%20integrates%20instance-%20and%0Akeypoint-level%20conditions%20into%20compact%20tokens%2C%20incorporating%20attributes%20such%20as%0Aclass%2C%20bounding%20box%2C%20and%20keypoint%20coordinates.%20This%20approach%20overcomes%20the%0Alimitations%20of%20previous%20methods%20that%20struggled%20to%20distinguish%20instances%20and%0Aclasses%20due%20to%20their%20reliance%20on%20skeleton%20images%20as%20conditions.%20Second%2C%20we%0Apropose%20HAIG-2.9M%2C%20a%20large-scale%2C%20high-quality%2C%20and%20diverse%20dataset%20designed%0Afor%20keypoint-guided%20human%20and%20animal%20image%20generation.%20HAIG-2.9M%20includes%20786K%0Aimages%20with%202.9M%20instances.%20This%20dataset%20features%20extensive%20annotations%20such%20as%0Akeypoints%2C%20bounding%20boxes%2C%20and%20fine-grained%20captions%20for%20both%20humans%20and%0Aanimals%2C%20along%20with%20rigorous%20manual%20inspection%20to%20ensure%20annotation%20accuracy.%0AExtensive%20experiments%20demonstrate%20the%20high%20quality%20of%20HAIG-2.9M%20and%20the%0Aeffectiveness%20of%20UniMC%2C%20particularly%20in%20heavy%20occlusions%20and%20multi-class%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMC%253A%2520Taming%2520Diffusion%2520Transformer%2520for%2520Unified%2520Keypoint-Guided%250A%2520%2520Multi-Class%2520Image%2520Generation%26entry.906535625%3DQin%2520Guo%2520and%2520Ailing%2520Zeng%2520and%2520Dongxu%2520Yue%2520and%2520Ceyuan%2520Yang%2520and%2520Yang%2520Cao%2520and%2520Hanzhong%2520Guo%2520and%2520Fei%2520Shen%2520and%2520Wei%2520Liu%2520and%2520Xihui%2520Liu%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520Although%2520significant%2520advancements%2520have%2520been%2520achieved%2520in%2520the%2520progress%2520of%250Akeypoint-guided%2520Text-to-Image%2520diffusion%2520models%252C%2520existing%2520mainstream%250Akeypoint-guided%2520models%2520encounter%2520challenges%2520in%2520controlling%2520the%2520generation%2520of%250Amore%2520general%2520non-rigid%2520objects%2520beyond%2520humans%2520%2528e.g.%252C%2520animals%2529.%2520Moreover%252C%2520it%2520is%250Adifficult%2520to%2520generate%2520multiple%2520overlapping%2520humans%2520and%2520animals%2520based%2520on%2520keypoint%250Acontrols%2520solely.%2520These%2520challenges%2520arise%2520from%2520two%2520main%2520aspects%253A%2520the%2520inherent%250Alimitations%2520of%2520existing%2520controllable%2520methods%2520and%2520the%2520lack%2520of%2520suitable%2520datasets.%250AFirst%252C%2520we%2520design%2520a%2520DiT-based%2520framework%252C%2520named%2520UniMC%252C%2520to%2520explore%2520unifying%250Acontrollable%2520multi-class%2520image%2520generation.%2520UniMC%2520integrates%2520instance-%2520and%250Akeypoint-level%2520conditions%2520into%2520compact%2520tokens%252C%2520incorporating%2520attributes%2520such%2520as%250Aclass%252C%2520bounding%2520box%252C%2520and%2520keypoint%2520coordinates.%2520This%2520approach%2520overcomes%2520the%250Alimitations%2520of%2520previous%2520methods%2520that%2520struggled%2520to%2520distinguish%2520instances%2520and%250Aclasses%2520due%2520to%2520their%2520reliance%2520on%2520skeleton%2520images%2520as%2520conditions.%2520Second%252C%2520we%250Apropose%2520HAIG-2.9M%252C%2520a%2520large-scale%252C%2520high-quality%252C%2520and%2520diverse%2520dataset%2520designed%250Afor%2520keypoint-guided%2520human%2520and%2520animal%2520image%2520generation.%2520HAIG-2.9M%2520includes%2520786K%250Aimages%2520with%25202.9M%2520instances.%2520This%2520dataset%2520features%2520extensive%2520annotations%2520such%2520as%250Akeypoints%252C%2520bounding%2520boxes%252C%2520and%2520fine-grained%2520captions%2520for%2520both%2520humans%2520and%250Aanimals%252C%2520along%2520with%2520rigorous%2520manual%2520inspection%2520to%2520ensure%2520annotation%2520accuracy.%250AExtensive%2520experiments%2520demonstrate%2520the%2520high%2520quality%2520of%2520HAIG-2.9M%2520and%2520the%250Aeffectiveness%2520of%2520UniMC%252C%2520particularly%2520in%2520heavy%2520occlusions%2520and%2520multi-class%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMC%3A%20Taming%20Diffusion%20Transformer%20for%20Unified%20Keypoint-Guided%0A%20%20Multi-Class%20Image%20Generation&entry.906535625=Qin%20Guo%20and%20Ailing%20Zeng%20and%20Dongxu%20Yue%20and%20Ceyuan%20Yang%20and%20Yang%20Cao%20and%20Hanzhong%20Guo%20and%20Fei%20Shen%20and%20Wei%20Liu%20and%20Xihui%20Liu%20and%20Dan%20Xu&entry.1292438233=%20%20Although%20significant%20advancements%20have%20been%20achieved%20in%20the%20progress%20of%0Akeypoint-guided%20Text-to-Image%20diffusion%20models%2C%20existing%20mainstream%0Akeypoint-guided%20models%20encounter%20challenges%20in%20controlling%20the%20generation%20of%0Amore%20general%20non-rigid%20objects%20beyond%20humans%20%28e.g.%2C%20animals%29.%20Moreover%2C%20it%20is%0Adifficult%20to%20generate%20multiple%20overlapping%20humans%20and%20animals%20based%20on%20keypoint%0Acontrols%20solely.%20These%20challenges%20arise%20from%20two%20main%20aspects%3A%20the%20inherent%0Alimitations%20of%20existing%20controllable%20methods%20and%20the%20lack%20of%20suitable%20datasets.%0AFirst%2C%20we%20design%20a%20DiT-based%20framework%2C%20named%20UniMC%2C%20to%20explore%20unifying%0Acontrollable%20multi-class%20image%20generation.%20UniMC%20integrates%20instance-%20and%0Akeypoint-level%20conditions%20into%20compact%20tokens%2C%20incorporating%20attributes%20such%20as%0Aclass%2C%20bounding%20box%2C%20and%20keypoint%20coordinates.%20This%20approach%20overcomes%20the%0Alimitations%20of%20previous%20methods%20that%20struggled%20to%20distinguish%20instances%20and%0Aclasses%20due%20to%20their%20reliance%20on%20skeleton%20images%20as%20conditions.%20Second%2C%20we%0Apropose%20HAIG-2.9M%2C%20a%20large-scale%2C%20high-quality%2C%20and%20diverse%20dataset%20designed%0Afor%20keypoint-guided%20human%20and%20animal%20image%20generation.%20HAIG-2.9M%20includes%20786K%0Aimages%20with%202.9M%20instances.%20This%20dataset%20features%20extensive%20annotations%20such%20as%0Akeypoints%2C%20bounding%20boxes%2C%20and%20fine-grained%20captions%20for%20both%20humans%20and%0Aanimals%2C%20along%20with%20rigorous%20manual%20inspection%20to%20ensure%20annotation%20accuracy.%0AExtensive%20experiments%20demonstrate%20the%20high%20quality%20of%20HAIG-2.9M%20and%20the%0Aeffectiveness%20of%20UniMC%2C%20particularly%20in%20heavy%20occlusions%20and%20multi-class%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02713v1&entry.124074799=Read"},
{"title": "Sparse Gaussian Processes: Structured Approximations and Power-EP\n  Revisited", "author": "Thang D. Bui and Michalis K. Titsias", "abstract": "  Inducing-point-based sparse variational Gaussian processes have become the\nstandard workhorse for scaling up GP models. Recent advances show that these\nmethods can be improved by introducing a diagonal scaling matrix to the\nconditional posterior density given the inducing points. This paper first\nconsiders an extension that employs a block-diagonal structure for the scaling\nmatrix, provably tightening the variational lower bound. We then revisit the\nunifying framework of sparse GPs based on Power Expectation Propagation (PEP)\nand show that it can leverage and benefit from the new structured approximate\nposteriors. Through extensive regression experiments, we show that the proposed\nblock-diagonal approximation consistently performs similarly to or better than\nexisting diagonal approximations while maintaining comparable computational\ncosts. Furthermore, the new PEP framework with structured posteriors provides\ncompetitive performance across various power hyperparameter settings, offering\npractitioners flexible alternatives to standard variational approaches.\n", "link": "http://arxiv.org/abs/2507.02377v1", "date": "2025-07-03", "relevancy": 2.5002, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5155}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4935}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Gaussian%20Processes%3A%20Structured%20Approximations%20and%20Power-EP%0A%20%20Revisited&body=Title%3A%20Sparse%20Gaussian%20Processes%3A%20Structured%20Approximations%20and%20Power-EP%0A%20%20Revisited%0AAuthor%3A%20Thang%20D.%20Bui%20and%20Michalis%20K.%20Titsias%0AAbstract%3A%20%20%20Inducing-point-based%20sparse%20variational%20Gaussian%20processes%20have%20become%20the%0Astandard%20workhorse%20for%20scaling%20up%20GP%20models.%20Recent%20advances%20show%20that%20these%0Amethods%20can%20be%20improved%20by%20introducing%20a%20diagonal%20scaling%20matrix%20to%20the%0Aconditional%20posterior%20density%20given%20the%20inducing%20points.%20This%20paper%20first%0Aconsiders%20an%20extension%20that%20employs%20a%20block-diagonal%20structure%20for%20the%20scaling%0Amatrix%2C%20provably%20tightening%20the%20variational%20lower%20bound.%20We%20then%20revisit%20the%0Aunifying%20framework%20of%20sparse%20GPs%20based%20on%20Power%20Expectation%20Propagation%20%28PEP%29%0Aand%20show%20that%20it%20can%20leverage%20and%20benefit%20from%20the%20new%20structured%20approximate%0Aposteriors.%20Through%20extensive%20regression%20experiments%2C%20we%20show%20that%20the%20proposed%0Ablock-diagonal%20approximation%20consistently%20performs%20similarly%20to%20or%20better%20than%0Aexisting%20diagonal%20approximations%20while%20maintaining%20comparable%20computational%0Acosts.%20Furthermore%2C%20the%20new%20PEP%20framework%20with%20structured%20posteriors%20provides%0Acompetitive%20performance%20across%20various%20power%20hyperparameter%20settings%2C%20offering%0Apractitioners%20flexible%20alternatives%20to%20standard%20variational%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Gaussian%2520Processes%253A%2520Structured%2520Approximations%2520and%2520Power-EP%250A%2520%2520Revisited%26entry.906535625%3DThang%2520D.%2520Bui%2520and%2520Michalis%2520K.%2520Titsias%26entry.1292438233%3D%2520%2520Inducing-point-based%2520sparse%2520variational%2520Gaussian%2520processes%2520have%2520become%2520the%250Astandard%2520workhorse%2520for%2520scaling%2520up%2520GP%2520models.%2520Recent%2520advances%2520show%2520that%2520these%250Amethods%2520can%2520be%2520improved%2520by%2520introducing%2520a%2520diagonal%2520scaling%2520matrix%2520to%2520the%250Aconditional%2520posterior%2520density%2520given%2520the%2520inducing%2520points.%2520This%2520paper%2520first%250Aconsiders%2520an%2520extension%2520that%2520employs%2520a%2520block-diagonal%2520structure%2520for%2520the%2520scaling%250Amatrix%252C%2520provably%2520tightening%2520the%2520variational%2520lower%2520bound.%2520We%2520then%2520revisit%2520the%250Aunifying%2520framework%2520of%2520sparse%2520GPs%2520based%2520on%2520Power%2520Expectation%2520Propagation%2520%2528PEP%2529%250Aand%2520show%2520that%2520it%2520can%2520leverage%2520and%2520benefit%2520from%2520the%2520new%2520structured%2520approximate%250Aposteriors.%2520Through%2520extensive%2520regression%2520experiments%252C%2520we%2520show%2520that%2520the%2520proposed%250Ablock-diagonal%2520approximation%2520consistently%2520performs%2520similarly%2520to%2520or%2520better%2520than%250Aexisting%2520diagonal%2520approximations%2520while%2520maintaining%2520comparable%2520computational%250Acosts.%2520Furthermore%252C%2520the%2520new%2520PEP%2520framework%2520with%2520structured%2520posteriors%2520provides%250Acompetitive%2520performance%2520across%2520various%2520power%2520hyperparameter%2520settings%252C%2520offering%250Apractitioners%2520flexible%2520alternatives%2520to%2520standard%2520variational%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Gaussian%20Processes%3A%20Structured%20Approximations%20and%20Power-EP%0A%20%20Revisited&entry.906535625=Thang%20D.%20Bui%20and%20Michalis%20K.%20Titsias&entry.1292438233=%20%20Inducing-point-based%20sparse%20variational%20Gaussian%20processes%20have%20become%20the%0Astandard%20workhorse%20for%20scaling%20up%20GP%20models.%20Recent%20advances%20show%20that%20these%0Amethods%20can%20be%20improved%20by%20introducing%20a%20diagonal%20scaling%20matrix%20to%20the%0Aconditional%20posterior%20density%20given%20the%20inducing%20points.%20This%20paper%20first%0Aconsiders%20an%20extension%20that%20employs%20a%20block-diagonal%20structure%20for%20the%20scaling%0Amatrix%2C%20provably%20tightening%20the%20variational%20lower%20bound.%20We%20then%20revisit%20the%0Aunifying%20framework%20of%20sparse%20GPs%20based%20on%20Power%20Expectation%20Propagation%20%28PEP%29%0Aand%20show%20that%20it%20can%20leverage%20and%20benefit%20from%20the%20new%20structured%20approximate%0Aposteriors.%20Through%20extensive%20regression%20experiments%2C%20we%20show%20that%20the%20proposed%0Ablock-diagonal%20approximation%20consistently%20performs%20similarly%20to%20or%20better%20than%0Aexisting%20diagonal%20approximations%20while%20maintaining%20comparable%20computational%0Acosts.%20Furthermore%2C%20the%20new%20PEP%20framework%20with%20structured%20posteriors%20provides%0Acompetitive%20performance%20across%20various%20power%20hyperparameter%20settings%2C%20offering%0Apractitioners%20flexible%20alternatives%20to%20standard%20variational%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02377v1&entry.124074799=Read"},
{"title": "Universal Collection of Euclidean Invariants between Pairs of\n  Position-Orientations", "author": "Gijs Bellaard and Bart M. N. Smets and Remco Duits", "abstract": "  Euclidean E(3) equivariant neural networks that employ scalar fields on\nposition-orientation space M(3) have been effectively applied to tasks such as\npredicting molecular dynamics and properties. To perform equivariant\nconvolutional-like operations in these architectures one needs Euclidean\ninvariant kernels on M(3) x M(3). In practice, a handcrafted collection of\ninvariants is selected, and this collection is then fed into multilayer\nperceptrons to parametrize the kernels. We rigorously describe an optimal\ncollection of 4 smooth scalar invariants on the whole of M(3) x M(3). With\noptimal we mean that the collection is independent and universal, meaning that\nall invariants are pertinent, and any invariant kernel is a function of them.\nWe evaluate two collections of invariants, one universal and one not, using the\nPONITA neural network architecture. Our experiments show that using a\ncollection of invariants that is universal positively impacts the accuracy of\nPONITA significantly.\n", "link": "http://arxiv.org/abs/2504.03299v2", "date": "2025-07-03", "relevancy": 2.4919, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4995}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4987}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Collection%20of%20Euclidean%20Invariants%20between%20Pairs%20of%0A%20%20Position-Orientations&body=Title%3A%20Universal%20Collection%20of%20Euclidean%20Invariants%20between%20Pairs%20of%0A%20%20Position-Orientations%0AAuthor%3A%20Gijs%20Bellaard%20and%20Bart%20M.%20N.%20Smets%20and%20Remco%20Duits%0AAbstract%3A%20%20%20Euclidean%20E%283%29%20equivariant%20neural%20networks%20that%20employ%20scalar%20fields%20on%0Aposition-orientation%20space%20M%283%29%20have%20been%20effectively%20applied%20to%20tasks%20such%20as%0Apredicting%20molecular%20dynamics%20and%20properties.%20To%20perform%20equivariant%0Aconvolutional-like%20operations%20in%20these%20architectures%20one%20needs%20Euclidean%0Ainvariant%20kernels%20on%20M%283%29%20x%20M%283%29.%20In%20practice%2C%20a%20handcrafted%20collection%20of%0Ainvariants%20is%20selected%2C%20and%20this%20collection%20is%20then%20fed%20into%20multilayer%0Aperceptrons%20to%20parametrize%20the%20kernels.%20We%20rigorously%20describe%20an%20optimal%0Acollection%20of%204%20smooth%20scalar%20invariants%20on%20the%20whole%20of%20M%283%29%20x%20M%283%29.%20With%0Aoptimal%20we%20mean%20that%20the%20collection%20is%20independent%20and%20universal%2C%20meaning%20that%0Aall%20invariants%20are%20pertinent%2C%20and%20any%20invariant%20kernel%20is%20a%20function%20of%20them.%0AWe%20evaluate%20two%20collections%20of%20invariants%2C%20one%20universal%20and%20one%20not%2C%20using%20the%0APONITA%20neural%20network%20architecture.%20Our%20experiments%20show%20that%20using%20a%0Acollection%20of%20invariants%20that%20is%20universal%20positively%20impacts%20the%20accuracy%20of%0APONITA%20significantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Collection%2520of%2520Euclidean%2520Invariants%2520between%2520Pairs%2520of%250A%2520%2520Position-Orientations%26entry.906535625%3DGijs%2520Bellaard%2520and%2520Bart%2520M.%2520N.%2520Smets%2520and%2520Remco%2520Duits%26entry.1292438233%3D%2520%2520Euclidean%2520E%25283%2529%2520equivariant%2520neural%2520networks%2520that%2520employ%2520scalar%2520fields%2520on%250Aposition-orientation%2520space%2520M%25283%2529%2520have%2520been%2520effectively%2520applied%2520to%2520tasks%2520such%2520as%250Apredicting%2520molecular%2520dynamics%2520and%2520properties.%2520To%2520perform%2520equivariant%250Aconvolutional-like%2520operations%2520in%2520these%2520architectures%2520one%2520needs%2520Euclidean%250Ainvariant%2520kernels%2520on%2520M%25283%2529%2520x%2520M%25283%2529.%2520In%2520practice%252C%2520a%2520handcrafted%2520collection%2520of%250Ainvariants%2520is%2520selected%252C%2520and%2520this%2520collection%2520is%2520then%2520fed%2520into%2520multilayer%250Aperceptrons%2520to%2520parametrize%2520the%2520kernels.%2520We%2520rigorously%2520describe%2520an%2520optimal%250Acollection%2520of%25204%2520smooth%2520scalar%2520invariants%2520on%2520the%2520whole%2520of%2520M%25283%2529%2520x%2520M%25283%2529.%2520With%250Aoptimal%2520we%2520mean%2520that%2520the%2520collection%2520is%2520independent%2520and%2520universal%252C%2520meaning%2520that%250Aall%2520invariants%2520are%2520pertinent%252C%2520and%2520any%2520invariant%2520kernel%2520is%2520a%2520function%2520of%2520them.%250AWe%2520evaluate%2520two%2520collections%2520of%2520invariants%252C%2520one%2520universal%2520and%2520one%2520not%252C%2520using%2520the%250APONITA%2520neural%2520network%2520architecture.%2520Our%2520experiments%2520show%2520that%2520using%2520a%250Acollection%2520of%2520invariants%2520that%2520is%2520universal%2520positively%2520impacts%2520the%2520accuracy%2520of%250APONITA%2520significantly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Collection%20of%20Euclidean%20Invariants%20between%20Pairs%20of%0A%20%20Position-Orientations&entry.906535625=Gijs%20Bellaard%20and%20Bart%20M.%20N.%20Smets%20and%20Remco%20Duits&entry.1292438233=%20%20Euclidean%20E%283%29%20equivariant%20neural%20networks%20that%20employ%20scalar%20fields%20on%0Aposition-orientation%20space%20M%283%29%20have%20been%20effectively%20applied%20to%20tasks%20such%20as%0Apredicting%20molecular%20dynamics%20and%20properties.%20To%20perform%20equivariant%0Aconvolutional-like%20operations%20in%20these%20architectures%20one%20needs%20Euclidean%0Ainvariant%20kernels%20on%20M%283%29%20x%20M%283%29.%20In%20practice%2C%20a%20handcrafted%20collection%20of%0Ainvariants%20is%20selected%2C%20and%20this%20collection%20is%20then%20fed%20into%20multilayer%0Aperceptrons%20to%20parametrize%20the%20kernels.%20We%20rigorously%20describe%20an%20optimal%0Acollection%20of%204%20smooth%20scalar%20invariants%20on%20the%20whole%20of%20M%283%29%20x%20M%283%29.%20With%0Aoptimal%20we%20mean%20that%20the%20collection%20is%20independent%20and%20universal%2C%20meaning%20that%0Aall%20invariants%20are%20pertinent%2C%20and%20any%20invariant%20kernel%20is%20a%20function%20of%20them.%0AWe%20evaluate%20two%20collections%20of%20invariants%2C%20one%20universal%20and%20one%20not%2C%20using%20the%0APONITA%20neural%20network%20architecture.%20Our%20experiments%20show%20that%20using%20a%0Acollection%20of%20invariants%20that%20is%20universal%20positively%20impacts%20the%20accuracy%20of%0APONITA%20significantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03299v2&entry.124074799=Read"},
{"title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial\n  Control for Text-to-Image Generation", "author": "Liheng Zhang and Lexi Pang and Hang Ye and Xiaoxuan Ma and Yizhou Wang", "abstract": "  Text-to-image (T2I) diffusion models have shown remarkable success in\ngenerating high-quality images from text prompts. Recent efforts extend these\nmodels to incorporate conditional images (e.g., depth or pose maps) for\nfine-grained spatial control. Among them, feature injection methods have\nemerged as a training-free alternative to traditional fine-tuning approaches.\nHowever, they often suffer from structural misalignment, condition leakage, and\nvisual artifacts, especially when the condition image diverges significantly\nfrom natural RGB distributions. By revisiting existing methods, we identify a\ncore limitation: the synchronous injection of condition features fails to\naccount for the trade-off between domain alignment and structural preservation\nduring denoising. Inspired by this observation, we propose a flexible feature\ninjection framework that decouples the injection timestep from the denoising\nprocess. At its core is a structure-rich injection module, which enables the\nmodel to better adapt to the evolving interplay between alignment and structure\npreservation throughout the diffusion steps, resulting in more faithful\nstructural generation. In addition, we introduce appearance-rich prompting and\na restart refinement strategy to further enhance appearance control and visual\nquality. Together, these designs enable training-free generation that is both\nstructure-rich and appearance-rich. Extensive experiments show that our\napproach achieves state-of-the-art performance across diverse zero-shot\nconditioning scenarios.\n", "link": "http://arxiv.org/abs/2507.02792v1", "date": "2025-07-03", "relevancy": 2.4783, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6363}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation&body=Title%3A%20RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Liheng%20Zhang%20and%20Lexi%20Pang%20and%20Hang%20Ye%20and%20Xiaoxuan%20Ma%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20remarkable%20success%20in%0Agenerating%20high-quality%20images%20from%20text%20prompts.%20Recent%20efforts%20extend%20these%0Amodels%20to%20incorporate%20conditional%20images%20%28e.g.%2C%20depth%20or%20pose%20maps%29%20for%0Afine-grained%20spatial%20control.%20Among%20them%2C%20feature%20injection%20methods%20have%0Aemerged%20as%20a%20training-free%20alternative%20to%20traditional%20fine-tuning%20approaches.%0AHowever%2C%20they%20often%20suffer%20from%20structural%20misalignment%2C%20condition%20leakage%2C%20and%0Avisual%20artifacts%2C%20especially%20when%20the%20condition%20image%20diverges%20significantly%0Afrom%20natural%20RGB%20distributions.%20By%20revisiting%20existing%20methods%2C%20we%20identify%20a%0Acore%20limitation%3A%20the%20synchronous%20injection%20of%20condition%20features%20fails%20to%0Aaccount%20for%20the%20trade-off%20between%20domain%20alignment%20and%20structural%20preservation%0Aduring%20denoising.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20flexible%20feature%0Ainjection%20framework%20that%20decouples%20the%20injection%20timestep%20from%20the%20denoising%0Aprocess.%20At%20its%20core%20is%20a%20structure-rich%20injection%20module%2C%20which%20enables%20the%0Amodel%20to%20better%20adapt%20to%20the%20evolving%20interplay%20between%20alignment%20and%20structure%0Apreservation%20throughout%20the%20diffusion%20steps%2C%20resulting%20in%20more%20faithful%0Astructural%20generation.%20In%20addition%2C%20we%20introduce%20appearance-rich%20prompting%20and%0Aa%20restart%20refinement%20strategy%20to%20further%20enhance%20appearance%20control%20and%20visual%0Aquality.%20Together%2C%20these%20designs%20enable%20training-free%20generation%20that%20is%20both%0Astructure-rich%20and%20appearance-rich.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20across%20diverse%20zero-shot%0Aconditioning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRichControl%253A%2520Structure-%2520and%2520Appearance-Rich%2520Training-Free%2520Spatial%250A%2520%2520Control%2520for%2520Text-to-Image%2520Generation%26entry.906535625%3DLiheng%2520Zhang%2520and%2520Lexi%2520Pang%2520and%2520Hang%2520Ye%2520and%2520Xiaoxuan%2520Ma%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520success%2520in%250Agenerating%2520high-quality%2520images%2520from%2520text%2520prompts.%2520Recent%2520efforts%2520extend%2520these%250Amodels%2520to%2520incorporate%2520conditional%2520images%2520%2528e.g.%252C%2520depth%2520or%2520pose%2520maps%2529%2520for%250Afine-grained%2520spatial%2520control.%2520Among%2520them%252C%2520feature%2520injection%2520methods%2520have%250Aemerged%2520as%2520a%2520training-free%2520alternative%2520to%2520traditional%2520fine-tuning%2520approaches.%250AHowever%252C%2520they%2520often%2520suffer%2520from%2520structural%2520misalignment%252C%2520condition%2520leakage%252C%2520and%250Avisual%2520artifacts%252C%2520especially%2520when%2520the%2520condition%2520image%2520diverges%2520significantly%250Afrom%2520natural%2520RGB%2520distributions.%2520By%2520revisiting%2520existing%2520methods%252C%2520we%2520identify%2520a%250Acore%2520limitation%253A%2520the%2520synchronous%2520injection%2520of%2520condition%2520features%2520fails%2520to%250Aaccount%2520for%2520the%2520trade-off%2520between%2520domain%2520alignment%2520and%2520structural%2520preservation%250Aduring%2520denoising.%2520Inspired%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520flexible%2520feature%250Ainjection%2520framework%2520that%2520decouples%2520the%2520injection%2520timestep%2520from%2520the%2520denoising%250Aprocess.%2520At%2520its%2520core%2520is%2520a%2520structure-rich%2520injection%2520module%252C%2520which%2520enables%2520the%250Amodel%2520to%2520better%2520adapt%2520to%2520the%2520evolving%2520interplay%2520between%2520alignment%2520and%2520structure%250Apreservation%2520throughout%2520the%2520diffusion%2520steps%252C%2520resulting%2520in%2520more%2520faithful%250Astructural%2520generation.%2520In%2520addition%252C%2520we%2520introduce%2520appearance-rich%2520prompting%2520and%250Aa%2520restart%2520refinement%2520strategy%2520to%2520further%2520enhance%2520appearance%2520control%2520and%2520visual%250Aquality.%2520Together%252C%2520these%2520designs%2520enable%2520training-free%2520generation%2520that%2520is%2520both%250Astructure-rich%2520and%2520appearance-rich.%2520Extensive%2520experiments%2520show%2520that%2520our%250Aapproach%2520achieves%2520state-of-the-art%2520performance%2520across%2520diverse%2520zero-shot%250Aconditioning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation&entry.906535625=Liheng%20Zhang%20and%20Lexi%20Pang%20and%20Hang%20Ye%20and%20Xiaoxuan%20Ma%20and%20Yizhou%20Wang&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20remarkable%20success%20in%0Agenerating%20high-quality%20images%20from%20text%20prompts.%20Recent%20efforts%20extend%20these%0Amodels%20to%20incorporate%20conditional%20images%20%28e.g.%2C%20depth%20or%20pose%20maps%29%20for%0Afine-grained%20spatial%20control.%20Among%20them%2C%20feature%20injection%20methods%20have%0Aemerged%20as%20a%20training-free%20alternative%20to%20traditional%20fine-tuning%20approaches.%0AHowever%2C%20they%20often%20suffer%20from%20structural%20misalignment%2C%20condition%20leakage%2C%20and%0Avisual%20artifacts%2C%20especially%20when%20the%20condition%20image%20diverges%20significantly%0Afrom%20natural%20RGB%20distributions.%20By%20revisiting%20existing%20methods%2C%20we%20identify%20a%0Acore%20limitation%3A%20the%20synchronous%20injection%20of%20condition%20features%20fails%20to%0Aaccount%20for%20the%20trade-off%20between%20domain%20alignment%20and%20structural%20preservation%0Aduring%20denoising.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20flexible%20feature%0Ainjection%20framework%20that%20decouples%20the%20injection%20timestep%20from%20the%20denoising%0Aprocess.%20At%20its%20core%20is%20a%20structure-rich%20injection%20module%2C%20which%20enables%20the%0Amodel%20to%20better%20adapt%20to%20the%20evolving%20interplay%20between%20alignment%20and%20structure%0Apreservation%20throughout%20the%20diffusion%20steps%2C%20resulting%20in%20more%20faithful%0Astructural%20generation.%20In%20addition%2C%20we%20introduce%20appearance-rich%20prompting%20and%0Aa%20restart%20refinement%20strategy%20to%20further%20enhance%20appearance%20control%20and%20visual%0Aquality.%20Together%2C%20these%20designs%20enable%20training-free%20generation%20that%20is%20both%0Astructure-rich%20and%20appearance-rich.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20across%20diverse%20zero-shot%0Aconditioning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02792v1&entry.124074799=Read"},
{"title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal\n  Policies in Real", "author": "Renhao Wang and Haoran Geng and Tingle Li and Feishi Wang and Gopala Anumanchipalli and Philipp Wu and Trevor Darrell and Boyi Li and Pieter Abbeel and Jitendra Malik and Alexei A. Efros", "abstract": "  Robots must integrate multiple sensory modalities to act effectively in the\nreal world. Yet, learning such multimodal policies at scale remains\nchallenging. Simulation offers a viable solution, but while vision has\nbenefited from high-fidelity simulators, other modalities (e.g. sound) can be\nnotoriously difficult to simulate. As a result, sim-to-real transfer has\nsucceeded primarily in vision-based tasks, with multimodal transfer still\nlargely unrealized. In this work, we tackle these challenges by introducing\nMultiGen, a framework that integrates large-scale generative models into\ntraditional physics simulators, enabling multisensory simulation. We showcase\nour framework on the dynamic task of robot pouring, which inherently relies on\nmultimodal feedback. By synthesizing realistic audio conditioned on simulation\nvideo, our method enables training on rich audiovisual trajectories -- without\nany real robot data. We demonstrate effective zero-shot transfer to real-world\npouring with novel containers and liquids, highlighting the potential of\ngenerative modeling to both simulate hard-to-model modalities and close the\nmultimodal sim-to-real gap.\n", "link": "http://arxiv.org/abs/2507.02864v1", "date": "2025-07-03", "relevancy": 2.4666, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6448}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6187}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiGen%3A%20Using%20Multimodal%20Generation%20in%20Simulation%20to%20Learn%20Multimodal%0A%20%20Policies%20in%20Real&body=Title%3A%20MultiGen%3A%20Using%20Multimodal%20Generation%20in%20Simulation%20to%20Learn%20Multimodal%0A%20%20Policies%20in%20Real%0AAuthor%3A%20Renhao%20Wang%20and%20Haoran%20Geng%20and%20Tingle%20Li%20and%20Feishi%20Wang%20and%20Gopala%20Anumanchipalli%20and%20Philipp%20Wu%20and%20Trevor%20Darrell%20and%20Boyi%20Li%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Alexei%20A.%20Efros%0AAbstract%3A%20%20%20Robots%20must%20integrate%20multiple%20sensory%20modalities%20to%20act%20effectively%20in%20the%0Areal%20world.%20Yet%2C%20learning%20such%20multimodal%20policies%20at%20scale%20remains%0Achallenging.%20Simulation%20offers%20a%20viable%20solution%2C%20but%20while%20vision%20has%0Abenefited%20from%20high-fidelity%20simulators%2C%20other%20modalities%20%28e.g.%20sound%29%20can%20be%0Anotoriously%20difficult%20to%20simulate.%20As%20a%20result%2C%20sim-to-real%20transfer%20has%0Asucceeded%20primarily%20in%20vision-based%20tasks%2C%20with%20multimodal%20transfer%20still%0Alargely%20unrealized.%20In%20this%20work%2C%20we%20tackle%20these%20challenges%20by%20introducing%0AMultiGen%2C%20a%20framework%20that%20integrates%20large-scale%20generative%20models%20into%0Atraditional%20physics%20simulators%2C%20enabling%20multisensory%20simulation.%20We%20showcase%0Aour%20framework%20on%20the%20dynamic%20task%20of%20robot%20pouring%2C%20which%20inherently%20relies%20on%0Amultimodal%20feedback.%20By%20synthesizing%20realistic%20audio%20conditioned%20on%20simulation%0Avideo%2C%20our%20method%20enables%20training%20on%20rich%20audiovisual%20trajectories%20--%20without%0Aany%20real%20robot%20data.%20We%20demonstrate%20effective%20zero-shot%20transfer%20to%20real-world%0Apouring%20with%20novel%20containers%20and%20liquids%2C%20highlighting%20the%20potential%20of%0Agenerative%20modeling%20to%20both%20simulate%20hard-to-model%20modalities%20and%20close%20the%0Amultimodal%20sim-to-real%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiGen%253A%2520Using%2520Multimodal%2520Generation%2520in%2520Simulation%2520to%2520Learn%2520Multimodal%250A%2520%2520Policies%2520in%2520Real%26entry.906535625%3DRenhao%2520Wang%2520and%2520Haoran%2520Geng%2520and%2520Tingle%2520Li%2520and%2520Feishi%2520Wang%2520and%2520Gopala%2520Anumanchipalli%2520and%2520Philipp%2520Wu%2520and%2520Trevor%2520Darrell%2520and%2520Boyi%2520Li%2520and%2520Pieter%2520Abbeel%2520and%2520Jitendra%2520Malik%2520and%2520Alexei%2520A.%2520Efros%26entry.1292438233%3D%2520%2520Robots%2520must%2520integrate%2520multiple%2520sensory%2520modalities%2520to%2520act%2520effectively%2520in%2520the%250Areal%2520world.%2520Yet%252C%2520learning%2520such%2520multimodal%2520policies%2520at%2520scale%2520remains%250Achallenging.%2520Simulation%2520offers%2520a%2520viable%2520solution%252C%2520but%2520while%2520vision%2520has%250Abenefited%2520from%2520high-fidelity%2520simulators%252C%2520other%2520modalities%2520%2528e.g.%2520sound%2529%2520can%2520be%250Anotoriously%2520difficult%2520to%2520simulate.%2520As%2520a%2520result%252C%2520sim-to-real%2520transfer%2520has%250Asucceeded%2520primarily%2520in%2520vision-based%2520tasks%252C%2520with%2520multimodal%2520transfer%2520still%250Alargely%2520unrealized.%2520In%2520this%2520work%252C%2520we%2520tackle%2520these%2520challenges%2520by%2520introducing%250AMultiGen%252C%2520a%2520framework%2520that%2520integrates%2520large-scale%2520generative%2520models%2520into%250Atraditional%2520physics%2520simulators%252C%2520enabling%2520multisensory%2520simulation.%2520We%2520showcase%250Aour%2520framework%2520on%2520the%2520dynamic%2520task%2520of%2520robot%2520pouring%252C%2520which%2520inherently%2520relies%2520on%250Amultimodal%2520feedback.%2520By%2520synthesizing%2520realistic%2520audio%2520conditioned%2520on%2520simulation%250Avideo%252C%2520our%2520method%2520enables%2520training%2520on%2520rich%2520audiovisual%2520trajectories%2520--%2520without%250Aany%2520real%2520robot%2520data.%2520We%2520demonstrate%2520effective%2520zero-shot%2520transfer%2520to%2520real-world%250Apouring%2520with%2520novel%2520containers%2520and%2520liquids%252C%2520highlighting%2520the%2520potential%2520of%250Agenerative%2520modeling%2520to%2520both%2520simulate%2520hard-to-model%2520modalities%2520and%2520close%2520the%250Amultimodal%2520sim-to-real%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiGen%3A%20Using%20Multimodal%20Generation%20in%20Simulation%20to%20Learn%20Multimodal%0A%20%20Policies%20in%20Real&entry.906535625=Renhao%20Wang%20and%20Haoran%20Geng%20and%20Tingle%20Li%20and%20Feishi%20Wang%20and%20Gopala%20Anumanchipalli%20and%20Philipp%20Wu%20and%20Trevor%20Darrell%20and%20Boyi%20Li%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Alexei%20A.%20Efros&entry.1292438233=%20%20Robots%20must%20integrate%20multiple%20sensory%20modalities%20to%20act%20effectively%20in%20the%0Areal%20world.%20Yet%2C%20learning%20such%20multimodal%20policies%20at%20scale%20remains%0Achallenging.%20Simulation%20offers%20a%20viable%20solution%2C%20but%20while%20vision%20has%0Abenefited%20from%20high-fidelity%20simulators%2C%20other%20modalities%20%28e.g.%20sound%29%20can%20be%0Anotoriously%20difficult%20to%20simulate.%20As%20a%20result%2C%20sim-to-real%20transfer%20has%0Asucceeded%20primarily%20in%20vision-based%20tasks%2C%20with%20multimodal%20transfer%20still%0Alargely%20unrealized.%20In%20this%20work%2C%20we%20tackle%20these%20challenges%20by%20introducing%0AMultiGen%2C%20a%20framework%20that%20integrates%20large-scale%20generative%20models%20into%0Atraditional%20physics%20simulators%2C%20enabling%20multisensory%20simulation.%20We%20showcase%0Aour%20framework%20on%20the%20dynamic%20task%20of%20robot%20pouring%2C%20which%20inherently%20relies%20on%0Amultimodal%20feedback.%20By%20synthesizing%20realistic%20audio%20conditioned%20on%20simulation%0Avideo%2C%20our%20method%20enables%20training%20on%20rich%20audiovisual%20trajectories%20--%20without%0Aany%20real%20robot%20data.%20We%20demonstrate%20effective%20zero-shot%20transfer%20to%20real-world%0Apouring%20with%20novel%20containers%20and%20liquids%2C%20highlighting%20the%20potential%20of%0Agenerative%20modeling%20to%20both%20simulate%20hard-to-model%20modalities%20and%20close%20the%0Amultimodal%20sim-to-real%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02864v1&entry.124074799=Read"},
{"title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs", "author": "Chenxu Wang and Yilin Lyu and Zicheng Sun and Liping Jing", "abstract": "  Continual fine-tuning of Large Language Models (LLMs) is hampered by the\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\noffers efficiency but constrains the model's ability to learn new tasks and\ntransfer knowledge due to its low-rank nature and reliance on explicit\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\nContinual Learning, a novel training strategy that overcomes these limitations\nby synergistically combining full and low-rank parameters and jointly updating\nwithin a unified low-rank gradient subspace. GORP expands the optimization\nspace while preserving efficiency and mitigating catastrophic forgetting.\nExtensive experiments on continual learning benchmarks demonstrate GORP's\nsuperior performance compared to existing state-of-the-art approaches. Code is\navailable at https://github.com/Wcxwcxw/GORP.\n", "link": "http://arxiv.org/abs/2507.02503v1", "date": "2025-07-03", "relevancy": 2.4558, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4937}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4934}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Gradient%20Low-Rank%20Projection%20Fine-Tuning%20for%20LLMs&body=Title%3A%20Continual%20Gradient%20Low-Rank%20Projection%20Fine-Tuning%20for%20LLMs%0AAuthor%3A%20Chenxu%20Wang%20and%20Yilin%20Lyu%20and%20Zicheng%20Sun%20and%20Liping%20Jing%0AAbstract%3A%20%20%20Continual%20fine-tuning%20of%20Large%20Language%20Models%20%28LLMs%29%20is%20hampered%20by%20the%0Atrade-off%20between%20efficiency%20and%20expressiveness.%20Low-Rank%20Adaptation%20%28LoRA%29%0Aoffers%20efficiency%20but%20constrains%20the%20model%27s%20ability%20to%20learn%20new%20tasks%20and%0Atransfer%20knowledge%20due%20to%20its%20low-rank%20nature%20and%20reliance%20on%20explicit%0Aparameter%20constraints.%20We%20propose%20GORP%20%28Gradient%20LOw%20Rank%20Projection%29%20for%0AContinual%20Learning%2C%20a%20novel%20training%20strategy%20that%20overcomes%20these%20limitations%0Aby%20synergistically%20combining%20full%20and%20low-rank%20parameters%20and%20jointly%20updating%0Awithin%20a%20unified%20low-rank%20gradient%20subspace.%20GORP%20expands%20the%20optimization%0Aspace%20while%20preserving%20efficiency%20and%20mitigating%20catastrophic%20forgetting.%0AExtensive%20experiments%20on%20continual%20learning%20benchmarks%20demonstrate%20GORP%27s%0Asuperior%20performance%20compared%20to%20existing%20state-of-the-art%20approaches.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Wcxwcxw/GORP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Gradient%2520Low-Rank%2520Projection%2520Fine-Tuning%2520for%2520LLMs%26entry.906535625%3DChenxu%2520Wang%2520and%2520Yilin%2520Lyu%2520and%2520Zicheng%2520Sun%2520and%2520Liping%2520Jing%26entry.1292438233%3D%2520%2520Continual%2520fine-tuning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520hampered%2520by%2520the%250Atrade-off%2520between%2520efficiency%2520and%2520expressiveness.%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%250Aoffers%2520efficiency%2520but%2520constrains%2520the%2520model%2527s%2520ability%2520to%2520learn%2520new%2520tasks%2520and%250Atransfer%2520knowledge%2520due%2520to%2520its%2520low-rank%2520nature%2520and%2520reliance%2520on%2520explicit%250Aparameter%2520constraints.%2520We%2520propose%2520GORP%2520%2528Gradient%2520LOw%2520Rank%2520Projection%2529%2520for%250AContinual%2520Learning%252C%2520a%2520novel%2520training%2520strategy%2520that%2520overcomes%2520these%2520limitations%250Aby%2520synergistically%2520combining%2520full%2520and%2520low-rank%2520parameters%2520and%2520jointly%2520updating%250Awithin%2520a%2520unified%2520low-rank%2520gradient%2520subspace.%2520GORP%2520expands%2520the%2520optimization%250Aspace%2520while%2520preserving%2520efficiency%2520and%2520mitigating%2520catastrophic%2520forgetting.%250AExtensive%2520experiments%2520on%2520continual%2520learning%2520benchmarks%2520demonstrate%2520GORP%2527s%250Asuperior%2520performance%2520compared%2520to%2520existing%2520state-of-the-art%2520approaches.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/Wcxwcxw/GORP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Gradient%20Low-Rank%20Projection%20Fine-Tuning%20for%20LLMs&entry.906535625=Chenxu%20Wang%20and%20Yilin%20Lyu%20and%20Zicheng%20Sun%20and%20Liping%20Jing&entry.1292438233=%20%20Continual%20fine-tuning%20of%20Large%20Language%20Models%20%28LLMs%29%20is%20hampered%20by%20the%0Atrade-off%20between%20efficiency%20and%20expressiveness.%20Low-Rank%20Adaptation%20%28LoRA%29%0Aoffers%20efficiency%20but%20constrains%20the%20model%27s%20ability%20to%20learn%20new%20tasks%20and%0Atransfer%20knowledge%20due%20to%20its%20low-rank%20nature%20and%20reliance%20on%20explicit%0Aparameter%20constraints.%20We%20propose%20GORP%20%28Gradient%20LOw%20Rank%20Projection%29%20for%0AContinual%20Learning%2C%20a%20novel%20training%20strategy%20that%20overcomes%20these%20limitations%0Aby%20synergistically%20combining%20full%20and%20low-rank%20parameters%20and%20jointly%20updating%0Awithin%20a%20unified%20low-rank%20gradient%20subspace.%20GORP%20expands%20the%20optimization%0Aspace%20while%20preserving%20efficiency%20and%20mitigating%20catastrophic%20forgetting.%0AExtensive%20experiments%20on%20continual%20learning%20benchmarks%20demonstrate%20GORP%27s%0Asuperior%20performance%20compared%20to%20existing%20state-of-the-art%20approaches.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Wcxwcxw/GORP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02503v1&entry.124074799=Read"},
{"title": "Temporally-Aware Supervised Contrastive Learning for Polyp Counting in\n  Colonoscopy", "author": "Luca Parolari and Andrea Cherubini and Lamberto Ballan and Carlo Biffi", "abstract": "  Automated polyp counting in colonoscopy is a crucial step toward automated\nprocedure reporting and quality control, aiming to enhance the\ncost-effectiveness of colonoscopy screening. Counting polyps in a procedure\ninvolves detecting and tracking polyps, and then clustering tracklets that\nbelong to the same polyp entity. Existing methods for polyp counting rely on\nself-supervised learning and primarily leverage visual appearance, neglecting\ntemporal relationships in both tracklet feature learning and clustering stages.\nIn this work, we introduce a paradigm shift by proposing a supervised\ncontrastive loss that incorporates temporally-aware soft targets. Our approach\ncaptures intra-polyp variability while preserving inter-polyp discriminability,\nleading to more robust clustering. Additionally, we improve tracklet clustering\nby integrating a temporal adjacency constraint, reducing false positive\nre-associations between visually similar but temporally distant tracklets. We\ntrain and validate our method on publicly available datasets and evaluate its\nperformance with a leave-one-out cross-validation strategy. Results demonstrate\na 2.2x reduction in fragmentation rate compared to prior approaches. Our\nresults highlight the importance of temporal awareness in polyp counting,\nestablishing a new state-of-the-art. Code is available at\nhttps://github.com/lparolari/temporally-aware-polyp-counting.\n", "link": "http://arxiv.org/abs/2507.02493v1", "date": "2025-07-03", "relevancy": 2.4558, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4987}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporally-Aware%20Supervised%20Contrastive%20Learning%20for%20Polyp%20Counting%20in%0A%20%20Colonoscopy&body=Title%3A%20Temporally-Aware%20Supervised%20Contrastive%20Learning%20for%20Polyp%20Counting%20in%0A%20%20Colonoscopy%0AAuthor%3A%20Luca%20Parolari%20and%20Andrea%20Cherubini%20and%20Lamberto%20Ballan%20and%20Carlo%20Biffi%0AAbstract%3A%20%20%20Automated%20polyp%20counting%20in%20colonoscopy%20is%20a%20crucial%20step%20toward%20automated%0Aprocedure%20reporting%20and%20quality%20control%2C%20aiming%20to%20enhance%20the%0Acost-effectiveness%20of%20colonoscopy%20screening.%20Counting%20polyps%20in%20a%20procedure%0Ainvolves%20detecting%20and%20tracking%20polyps%2C%20and%20then%20clustering%20tracklets%20that%0Abelong%20to%20the%20same%20polyp%20entity.%20Existing%20methods%20for%20polyp%20counting%20rely%20on%0Aself-supervised%20learning%20and%20primarily%20leverage%20visual%20appearance%2C%20neglecting%0Atemporal%20relationships%20in%20both%20tracklet%20feature%20learning%20and%20clustering%20stages.%0AIn%20this%20work%2C%20we%20introduce%20a%20paradigm%20shift%20by%20proposing%20a%20supervised%0Acontrastive%20loss%20that%20incorporates%20temporally-aware%20soft%20targets.%20Our%20approach%0Acaptures%20intra-polyp%20variability%20while%20preserving%20inter-polyp%20discriminability%2C%0Aleading%20to%20more%20robust%20clustering.%20Additionally%2C%20we%20improve%20tracklet%20clustering%0Aby%20integrating%20a%20temporal%20adjacency%20constraint%2C%20reducing%20false%20positive%0Are-associations%20between%20visually%20similar%20but%20temporally%20distant%20tracklets.%20We%0Atrain%20and%20validate%20our%20method%20on%20publicly%20available%20datasets%20and%20evaluate%20its%0Aperformance%20with%20a%20leave-one-out%20cross-validation%20strategy.%20Results%20demonstrate%0Aa%202.2x%20reduction%20in%20fragmentation%20rate%20compared%20to%20prior%20approaches.%20Our%0Aresults%20highlight%20the%20importance%20of%20temporal%20awareness%20in%20polyp%20counting%2C%0Aestablishing%20a%20new%20state-of-the-art.%20Code%20is%20available%20at%0Ahttps%3A//github.com/lparolari/temporally-aware-polyp-counting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporally-Aware%2520Supervised%2520Contrastive%2520Learning%2520for%2520Polyp%2520Counting%2520in%250A%2520%2520Colonoscopy%26entry.906535625%3DLuca%2520Parolari%2520and%2520Andrea%2520Cherubini%2520and%2520Lamberto%2520Ballan%2520and%2520Carlo%2520Biffi%26entry.1292438233%3D%2520%2520Automated%2520polyp%2520counting%2520in%2520colonoscopy%2520is%2520a%2520crucial%2520step%2520toward%2520automated%250Aprocedure%2520reporting%2520and%2520quality%2520control%252C%2520aiming%2520to%2520enhance%2520the%250Acost-effectiveness%2520of%2520colonoscopy%2520screening.%2520Counting%2520polyps%2520in%2520a%2520procedure%250Ainvolves%2520detecting%2520and%2520tracking%2520polyps%252C%2520and%2520then%2520clustering%2520tracklets%2520that%250Abelong%2520to%2520the%2520same%2520polyp%2520entity.%2520Existing%2520methods%2520for%2520polyp%2520counting%2520rely%2520on%250Aself-supervised%2520learning%2520and%2520primarily%2520leverage%2520visual%2520appearance%252C%2520neglecting%250Atemporal%2520relationships%2520in%2520both%2520tracklet%2520feature%2520learning%2520and%2520clustering%2520stages.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520paradigm%2520shift%2520by%2520proposing%2520a%2520supervised%250Acontrastive%2520loss%2520that%2520incorporates%2520temporally-aware%2520soft%2520targets.%2520Our%2520approach%250Acaptures%2520intra-polyp%2520variability%2520while%2520preserving%2520inter-polyp%2520discriminability%252C%250Aleading%2520to%2520more%2520robust%2520clustering.%2520Additionally%252C%2520we%2520improve%2520tracklet%2520clustering%250Aby%2520integrating%2520a%2520temporal%2520adjacency%2520constraint%252C%2520reducing%2520false%2520positive%250Are-associations%2520between%2520visually%2520similar%2520but%2520temporally%2520distant%2520tracklets.%2520We%250Atrain%2520and%2520validate%2520our%2520method%2520on%2520publicly%2520available%2520datasets%2520and%2520evaluate%2520its%250Aperformance%2520with%2520a%2520leave-one-out%2520cross-validation%2520strategy.%2520Results%2520demonstrate%250Aa%25202.2x%2520reduction%2520in%2520fragmentation%2520rate%2520compared%2520to%2520prior%2520approaches.%2520Our%250Aresults%2520highlight%2520the%2520importance%2520of%2520temporal%2520awareness%2520in%2520polyp%2520counting%252C%250Aestablishing%2520a%2520new%2520state-of-the-art.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/lparolari/temporally-aware-polyp-counting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporally-Aware%20Supervised%20Contrastive%20Learning%20for%20Polyp%20Counting%20in%0A%20%20Colonoscopy&entry.906535625=Luca%20Parolari%20and%20Andrea%20Cherubini%20and%20Lamberto%20Ballan%20and%20Carlo%20Biffi&entry.1292438233=%20%20Automated%20polyp%20counting%20in%20colonoscopy%20is%20a%20crucial%20step%20toward%20automated%0Aprocedure%20reporting%20and%20quality%20control%2C%20aiming%20to%20enhance%20the%0Acost-effectiveness%20of%20colonoscopy%20screening.%20Counting%20polyps%20in%20a%20procedure%0Ainvolves%20detecting%20and%20tracking%20polyps%2C%20and%20then%20clustering%20tracklets%20that%0Abelong%20to%20the%20same%20polyp%20entity.%20Existing%20methods%20for%20polyp%20counting%20rely%20on%0Aself-supervised%20learning%20and%20primarily%20leverage%20visual%20appearance%2C%20neglecting%0Atemporal%20relationships%20in%20both%20tracklet%20feature%20learning%20and%20clustering%20stages.%0AIn%20this%20work%2C%20we%20introduce%20a%20paradigm%20shift%20by%20proposing%20a%20supervised%0Acontrastive%20loss%20that%20incorporates%20temporally-aware%20soft%20targets.%20Our%20approach%0Acaptures%20intra-polyp%20variability%20while%20preserving%20inter-polyp%20discriminability%2C%0Aleading%20to%20more%20robust%20clustering.%20Additionally%2C%20we%20improve%20tracklet%20clustering%0Aby%20integrating%20a%20temporal%20adjacency%20constraint%2C%20reducing%20false%20positive%0Are-associations%20between%20visually%20similar%20but%20temporally%20distant%20tracklets.%20We%0Atrain%20and%20validate%20our%20method%20on%20publicly%20available%20datasets%20and%20evaluate%20its%0Aperformance%20with%20a%20leave-one-out%20cross-validation%20strategy.%20Results%20demonstrate%0Aa%202.2x%20reduction%20in%20fragmentation%20rate%20compared%20to%20prior%20approaches.%20Our%0Aresults%20highlight%20the%20importance%20of%20temporal%20awareness%20in%20polyp%20counting%2C%0Aestablishing%20a%20new%20state-of-the-art.%20Code%20is%20available%20at%0Ahttps%3A//github.com/lparolari/temporally-aware-polyp-counting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02493v1&entry.124074799=Read"},
{"title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with\n  Minimum Potential Delay Fairness in Diffusion Models", "author": "Yuxuan Wang and Tianwei Cao and Huayu Zhang and Zhongjiang He and Kongming Liang and Zhanyu Ma", "abstract": "  Image generation has achieved remarkable progress with the development of\nlarge-scale text-to-image models, especially diffusion-based models. However,\ngenerating human images with plausible details, such as faces or hands, remains\nchallenging due to insufficient supervision of local regions during training.\nTo address this issue, we propose FairHuman, a multi-objective fine-tuning\napproach designed to enhance both global and local generation quality fairly.\nSpecifically, we first construct three learning objectives: a global objective\nderived from the default diffusion objective function and two local objectives\nfor hands and faces based on pre-annotated positional priors. Subsequently, we\nderive the optimal parameter updating strategy under the guidance of the\nMinimum Potential Delay (MPD) criterion, thereby attaining fairness-ware\noptimization for this multi-objective problem. Based on this, our proposed\nmethod can achieve significant improvements in generating challenging local\ndetails while maintaining overall quality. Extensive experiments showcase the\neffectiveness of our method in improving the performance of human image\ngeneration under different scenarios.\n", "link": "http://arxiv.org/abs/2507.02714v1", "date": "2025-07-03", "relevancy": 2.4537, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.616}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6127}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairHuman%3A%20Boosting%20Hand%20and%20Face%20Quality%20in%20Human%20Image%20Generation%20with%0A%20%20Minimum%20Potential%20Delay%20Fairness%20in%20Diffusion%20Models&body=Title%3A%20FairHuman%3A%20Boosting%20Hand%20and%20Face%20Quality%20in%20Human%20Image%20Generation%20with%0A%20%20Minimum%20Potential%20Delay%20Fairness%20in%20Diffusion%20Models%0AAuthor%3A%20Yuxuan%20Wang%20and%20Tianwei%20Cao%20and%20Huayu%20Zhang%20and%20Zhongjiang%20He%20and%20Kongming%20Liang%20and%20Zhanyu%20Ma%0AAbstract%3A%20%20%20Image%20generation%20has%20achieved%20remarkable%20progress%20with%20the%20development%20of%0Alarge-scale%20text-to-image%20models%2C%20especially%20diffusion-based%20models.%20However%2C%0Agenerating%20human%20images%20with%20plausible%20details%2C%20such%20as%20faces%20or%20hands%2C%20remains%0Achallenging%20due%20to%20insufficient%20supervision%20of%20local%20regions%20during%20training.%0ATo%20address%20this%20issue%2C%20we%20propose%20FairHuman%2C%20a%20multi-objective%20fine-tuning%0Aapproach%20designed%20to%20enhance%20both%20global%20and%20local%20generation%20quality%20fairly.%0ASpecifically%2C%20we%20first%20construct%20three%20learning%20objectives%3A%20a%20global%20objective%0Aderived%20from%20the%20default%20diffusion%20objective%20function%20and%20two%20local%20objectives%0Afor%20hands%20and%20faces%20based%20on%20pre-annotated%20positional%20priors.%20Subsequently%2C%20we%0Aderive%20the%20optimal%20parameter%20updating%20strategy%20under%20the%20guidance%20of%20the%0AMinimum%20Potential%20Delay%20%28MPD%29%20criterion%2C%20thereby%20attaining%20fairness-ware%0Aoptimization%20for%20this%20multi-objective%20problem.%20Based%20on%20this%2C%20our%20proposed%0Amethod%20can%20achieve%20significant%20improvements%20in%20generating%20challenging%20local%0Adetails%20while%20maintaining%20overall%20quality.%20Extensive%20experiments%20showcase%20the%0Aeffectiveness%20of%20our%20method%20in%20improving%20the%20performance%20of%20human%20image%0Ageneration%20under%20different%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairHuman%253A%2520Boosting%2520Hand%2520and%2520Face%2520Quality%2520in%2520Human%2520Image%2520Generation%2520with%250A%2520%2520Minimum%2520Potential%2520Delay%2520Fairness%2520in%2520Diffusion%2520Models%26entry.906535625%3DYuxuan%2520Wang%2520and%2520Tianwei%2520Cao%2520and%2520Huayu%2520Zhang%2520and%2520Zhongjiang%2520He%2520and%2520Kongming%2520Liang%2520and%2520Zhanyu%2520Ma%26entry.1292438233%3D%2520%2520Image%2520generation%2520has%2520achieved%2520remarkable%2520progress%2520with%2520the%2520development%2520of%250Alarge-scale%2520text-to-image%2520models%252C%2520especially%2520diffusion-based%2520models.%2520However%252C%250Agenerating%2520human%2520images%2520with%2520plausible%2520details%252C%2520such%2520as%2520faces%2520or%2520hands%252C%2520remains%250Achallenging%2520due%2520to%2520insufficient%2520supervision%2520of%2520local%2520regions%2520during%2520training.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520FairHuman%252C%2520a%2520multi-objective%2520fine-tuning%250Aapproach%2520designed%2520to%2520enhance%2520both%2520global%2520and%2520local%2520generation%2520quality%2520fairly.%250ASpecifically%252C%2520we%2520first%2520construct%2520three%2520learning%2520objectives%253A%2520a%2520global%2520objective%250Aderived%2520from%2520the%2520default%2520diffusion%2520objective%2520function%2520and%2520two%2520local%2520objectives%250Afor%2520hands%2520and%2520faces%2520based%2520on%2520pre-annotated%2520positional%2520priors.%2520Subsequently%252C%2520we%250Aderive%2520the%2520optimal%2520parameter%2520updating%2520strategy%2520under%2520the%2520guidance%2520of%2520the%250AMinimum%2520Potential%2520Delay%2520%2528MPD%2529%2520criterion%252C%2520thereby%2520attaining%2520fairness-ware%250Aoptimization%2520for%2520this%2520multi-objective%2520problem.%2520Based%2520on%2520this%252C%2520our%2520proposed%250Amethod%2520can%2520achieve%2520significant%2520improvements%2520in%2520generating%2520challenging%2520local%250Adetails%2520while%2520maintaining%2520overall%2520quality.%2520Extensive%2520experiments%2520showcase%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520improving%2520the%2520performance%2520of%2520human%2520image%250Ageneration%2520under%2520different%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairHuman%3A%20Boosting%20Hand%20and%20Face%20Quality%20in%20Human%20Image%20Generation%20with%0A%20%20Minimum%20Potential%20Delay%20Fairness%20in%20Diffusion%20Models&entry.906535625=Yuxuan%20Wang%20and%20Tianwei%20Cao%20and%20Huayu%20Zhang%20and%20Zhongjiang%20He%20and%20Kongming%20Liang%20and%20Zhanyu%20Ma&entry.1292438233=%20%20Image%20generation%20has%20achieved%20remarkable%20progress%20with%20the%20development%20of%0Alarge-scale%20text-to-image%20models%2C%20especially%20diffusion-based%20models.%20However%2C%0Agenerating%20human%20images%20with%20plausible%20details%2C%20such%20as%20faces%20or%20hands%2C%20remains%0Achallenging%20due%20to%20insufficient%20supervision%20of%20local%20regions%20during%20training.%0ATo%20address%20this%20issue%2C%20we%20propose%20FairHuman%2C%20a%20multi-objective%20fine-tuning%0Aapproach%20designed%20to%20enhance%20both%20global%20and%20local%20generation%20quality%20fairly.%0ASpecifically%2C%20we%20first%20construct%20three%20learning%20objectives%3A%20a%20global%20objective%0Aderived%20from%20the%20default%20diffusion%20objective%20function%20and%20two%20local%20objectives%0Afor%20hands%20and%20faces%20based%20on%20pre-annotated%20positional%20priors.%20Subsequently%2C%20we%0Aderive%20the%20optimal%20parameter%20updating%20strategy%20under%20the%20guidance%20of%20the%0AMinimum%20Potential%20Delay%20%28MPD%29%20criterion%2C%20thereby%20attaining%20fairness-ware%0Aoptimization%20for%20this%20multi-objective%20problem.%20Based%20on%20this%2C%20our%20proposed%0Amethod%20can%20achieve%20significant%20improvements%20in%20generating%20challenging%20local%0Adetails%20while%20maintaining%20overall%20quality.%20Extensive%20experiments%20showcase%20the%0Aeffectiveness%20of%20our%20method%20in%20improving%20the%20performance%20of%20human%20image%0Ageneration%20under%20different%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02714v1&entry.124074799=Read"},
{"title": "Self-Supervised Frameworks for Speaker Verification via Bootstrapped\n  Positive Sampling", "author": "Theo Lepage and Reda Dehak", "abstract": "  Recent developments in Self-Supervised Learning (SSL) have demonstrated\nsignificant potential for Speaker Verification (SV), but closing the\nperformance gap with supervised systems remains an ongoing challenge. SSL\nframeworks rely on anchor-positive pairs, constructed from segments of the same\naudio utterance. Hence, positives have channel characteristics similar to those\nof their corresponding anchors, even with extensive data-augmentation.\nTherefore, this positive sampling strategy is a fundamental limitation as it\nencodes too much information regarding the recording source in the learned\nrepresentations. This article introduces Self-Supervised Positive Sampling\n(SSPS), a bootstrapped technique for sampling appropriate and diverse positives\nin SSL frameworks for SV. SSPS samples positives close to their anchor in the\nrepresentation space, assuming that these pseudo-positives belong to the same\nspeaker identity but correspond to different recording conditions. This method\nconsistently demonstrates improvements in SV performance on VoxCeleb benchmarks\nwhen applied to major SSL frameworks, including SimCLR, SwAV, VICReg, and DINO.\nUsing SSPS, SimCLR and DINO achieve 2.57% and 2.53% EER on VoxCeleb1-O,\nrespectively. SimCLR yields a 58% relative reduction in EER, getting comparable\nperformance to DINO with a simpler training framework. Furthermore, SSPS lowers\nintra-class variance and reduces channel information in speaker representations\nwhile exhibiting greater robustness without data-augmentation.\n", "link": "http://arxiv.org/abs/2501.17772v3", "date": "2025-07-03", "relevancy": 2.4434, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Frameworks%20for%20Speaker%20Verification%20via%20Bootstrapped%0A%20%20Positive%20Sampling&body=Title%3A%20Self-Supervised%20Frameworks%20for%20Speaker%20Verification%20via%20Bootstrapped%0A%20%20Positive%20Sampling%0AAuthor%3A%20Theo%20Lepage%20and%20Reda%20Dehak%0AAbstract%3A%20%20%20Recent%20developments%20in%20Self-Supervised%20Learning%20%28SSL%29%20have%20demonstrated%0Asignificant%20potential%20for%20Speaker%20Verification%20%28SV%29%2C%20but%20closing%20the%0Aperformance%20gap%20with%20supervised%20systems%20remains%20an%20ongoing%20challenge.%20SSL%0Aframeworks%20rely%20on%20anchor-positive%20pairs%2C%20constructed%20from%20segments%20of%20the%20same%0Aaudio%20utterance.%20Hence%2C%20positives%20have%20channel%20characteristics%20similar%20to%20those%0Aof%20their%20corresponding%20anchors%2C%20even%20with%20extensive%20data-augmentation.%0ATherefore%2C%20this%20positive%20sampling%20strategy%20is%20a%20fundamental%20limitation%20as%20it%0Aencodes%20too%20much%20information%20regarding%20the%20recording%20source%20in%20the%20learned%0Arepresentations.%20This%20article%20introduces%20Self-Supervised%20Positive%20Sampling%0A%28SSPS%29%2C%20a%20bootstrapped%20technique%20for%20sampling%20appropriate%20and%20diverse%20positives%0Ain%20SSL%20frameworks%20for%20SV.%20SSPS%20samples%20positives%20close%20to%20their%20anchor%20in%20the%0Arepresentation%20space%2C%20assuming%20that%20these%20pseudo-positives%20belong%20to%20the%20same%0Aspeaker%20identity%20but%20correspond%20to%20different%20recording%20conditions.%20This%20method%0Aconsistently%20demonstrates%20improvements%20in%20SV%20performance%20on%20VoxCeleb%20benchmarks%0Awhen%20applied%20to%20major%20SSL%20frameworks%2C%20including%20SimCLR%2C%20SwAV%2C%20VICReg%2C%20and%20DINO.%0AUsing%20SSPS%2C%20SimCLR%20and%20DINO%20achieve%202.57%25%20and%202.53%25%20EER%20on%20VoxCeleb1-O%2C%0Arespectively.%20SimCLR%20yields%20a%2058%25%20relative%20reduction%20in%20EER%2C%20getting%20comparable%0Aperformance%20to%20DINO%20with%20a%20simpler%20training%20framework.%20Furthermore%2C%20SSPS%20lowers%0Aintra-class%20variance%20and%20reduces%20channel%20information%20in%20speaker%20representations%0Awhile%20exhibiting%20greater%20robustness%20without%20data-augmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17772v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Frameworks%2520for%2520Speaker%2520Verification%2520via%2520Bootstrapped%250A%2520%2520Positive%2520Sampling%26entry.906535625%3DTheo%2520Lepage%2520and%2520Reda%2520Dehak%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520have%2520demonstrated%250Asignificant%2520potential%2520for%2520Speaker%2520Verification%2520%2528SV%2529%252C%2520but%2520closing%2520the%250Aperformance%2520gap%2520with%2520supervised%2520systems%2520remains%2520an%2520ongoing%2520challenge.%2520SSL%250Aframeworks%2520rely%2520on%2520anchor-positive%2520pairs%252C%2520constructed%2520from%2520segments%2520of%2520the%2520same%250Aaudio%2520utterance.%2520Hence%252C%2520positives%2520have%2520channel%2520characteristics%2520similar%2520to%2520those%250Aof%2520their%2520corresponding%2520anchors%252C%2520even%2520with%2520extensive%2520data-augmentation.%250ATherefore%252C%2520this%2520positive%2520sampling%2520strategy%2520is%2520a%2520fundamental%2520limitation%2520as%2520it%250Aencodes%2520too%2520much%2520information%2520regarding%2520the%2520recording%2520source%2520in%2520the%2520learned%250Arepresentations.%2520This%2520article%2520introduces%2520Self-Supervised%2520Positive%2520Sampling%250A%2528SSPS%2529%252C%2520a%2520bootstrapped%2520technique%2520for%2520sampling%2520appropriate%2520and%2520diverse%2520positives%250Ain%2520SSL%2520frameworks%2520for%2520SV.%2520SSPS%2520samples%2520positives%2520close%2520to%2520their%2520anchor%2520in%2520the%250Arepresentation%2520space%252C%2520assuming%2520that%2520these%2520pseudo-positives%2520belong%2520to%2520the%2520same%250Aspeaker%2520identity%2520but%2520correspond%2520to%2520different%2520recording%2520conditions.%2520This%2520method%250Aconsistently%2520demonstrates%2520improvements%2520in%2520SV%2520performance%2520on%2520VoxCeleb%2520benchmarks%250Awhen%2520applied%2520to%2520major%2520SSL%2520frameworks%252C%2520including%2520SimCLR%252C%2520SwAV%252C%2520VICReg%252C%2520and%2520DINO.%250AUsing%2520SSPS%252C%2520SimCLR%2520and%2520DINO%2520achieve%25202.57%2525%2520and%25202.53%2525%2520EER%2520on%2520VoxCeleb1-O%252C%250Arespectively.%2520SimCLR%2520yields%2520a%252058%2525%2520relative%2520reduction%2520in%2520EER%252C%2520getting%2520comparable%250Aperformance%2520to%2520DINO%2520with%2520a%2520simpler%2520training%2520framework.%2520Furthermore%252C%2520SSPS%2520lowers%250Aintra-class%2520variance%2520and%2520reduces%2520channel%2520information%2520in%2520speaker%2520representations%250Awhile%2520exhibiting%2520greater%2520robustness%2520without%2520data-augmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17772v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Frameworks%20for%20Speaker%20Verification%20via%20Bootstrapped%0A%20%20Positive%20Sampling&entry.906535625=Theo%20Lepage%20and%20Reda%20Dehak&entry.1292438233=%20%20Recent%20developments%20in%20Self-Supervised%20Learning%20%28SSL%29%20have%20demonstrated%0Asignificant%20potential%20for%20Speaker%20Verification%20%28SV%29%2C%20but%20closing%20the%0Aperformance%20gap%20with%20supervised%20systems%20remains%20an%20ongoing%20challenge.%20SSL%0Aframeworks%20rely%20on%20anchor-positive%20pairs%2C%20constructed%20from%20segments%20of%20the%20same%0Aaudio%20utterance.%20Hence%2C%20positives%20have%20channel%20characteristics%20similar%20to%20those%0Aof%20their%20corresponding%20anchors%2C%20even%20with%20extensive%20data-augmentation.%0ATherefore%2C%20this%20positive%20sampling%20strategy%20is%20a%20fundamental%20limitation%20as%20it%0Aencodes%20too%20much%20information%20regarding%20the%20recording%20source%20in%20the%20learned%0Arepresentations.%20This%20article%20introduces%20Self-Supervised%20Positive%20Sampling%0A%28SSPS%29%2C%20a%20bootstrapped%20technique%20for%20sampling%20appropriate%20and%20diverse%20positives%0Ain%20SSL%20frameworks%20for%20SV.%20SSPS%20samples%20positives%20close%20to%20their%20anchor%20in%20the%0Arepresentation%20space%2C%20assuming%20that%20these%20pseudo-positives%20belong%20to%20the%20same%0Aspeaker%20identity%20but%20correspond%20to%20different%20recording%20conditions.%20This%20method%0Aconsistently%20demonstrates%20improvements%20in%20SV%20performance%20on%20VoxCeleb%20benchmarks%0Awhen%20applied%20to%20major%20SSL%20frameworks%2C%20including%20SimCLR%2C%20SwAV%2C%20VICReg%2C%20and%20DINO.%0AUsing%20SSPS%2C%20SimCLR%20and%20DINO%20achieve%202.57%25%20and%202.53%25%20EER%20on%20VoxCeleb1-O%2C%0Arespectively.%20SimCLR%20yields%20a%2058%25%20relative%20reduction%20in%20EER%2C%20getting%20comparable%0Aperformance%20to%20DINO%20with%20a%20simpler%20training%20framework.%20Furthermore%2C%20SSPS%20lowers%0Aintra-class%20variance%20and%20reduces%20channel%20information%20in%20speaker%20representations%0Awhile%20exhibiting%20greater%20robustness%20without%20data-augmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17772v3&entry.124074799=Read"},
{"title": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in\n  Pharmacogenes", "author": "Antoine Honor\u00e9 and Borja Rodr\u00edguez G\u00e1lvez and Yoomi Park and Yitian Zhou and Volker M. Lauschke and Ming Xiao", "abstract": "  Variant effect predictors (VEPs) aim to assess the functional impact of\nprotein variants, traditionally relying on multiple sequence alignments (MSAs).\nThis approach assumes that naturally occurring variants are fit, an assumption\nchallenged by pharmacogenomics, where some pharmacogenes experience low\nevolutionary pressure. Deep mutational scanning (DMS) datasets provide an\nalternative by offering quantitative fitness scores for variants. In this work,\nwe propose a transformer-based matrix variational auto-encoder (matVAE) with a\nstructured prior and evaluate its performance on 33 DMS datasets corresponding\nto 26 drug target and ADME proteins from the ProteinGym benchmark. Our model\ntrained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence\nmodel in zero-shot prediction on DMS datasets, despite using an order of\nmagnitude fewer parameters and requiring less computation at inference time. We\nalso compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on\nDMS data, and find that the latter performs better on supervised prediction\ntasks. Additionally, incorporating AlphaFold-generated structures into our\ntransformer model further improves performance, achieving results comparable to\nDeepSequence trained on MSAs and finetuned on DMS. These findings highlight the\npotential of DMS datasets to replace MSAs without significant loss in\npredictive performance, motivating further development of DMS datasets and\nexploration of their relationships to enhance variant effect prediction.\n", "link": "http://arxiv.org/abs/2507.02624v1", "date": "2025-07-03", "relevancy": 2.4431, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Matrix%20Variational%20Auto-Encoder%20for%20Variant%20Effect%20Prediction%20in%0A%20%20Pharmacogenes&body=Title%3A%20A%20Matrix%20Variational%20Auto-Encoder%20for%20Variant%20Effect%20Prediction%20in%0A%20%20Pharmacogenes%0AAuthor%3A%20Antoine%20Honor%C3%A9%20and%20Borja%20Rodr%C3%ADguez%20G%C3%A1lvez%20and%20Yoomi%20Park%20and%20Yitian%20Zhou%20and%20Volker%20M.%20Lauschke%20and%20Ming%20Xiao%0AAbstract%3A%20%20%20Variant%20effect%20predictors%20%28VEPs%29%20aim%20to%20assess%20the%20functional%20impact%20of%0Aprotein%20variants%2C%20traditionally%20relying%20on%20multiple%20sequence%20alignments%20%28MSAs%29.%0AThis%20approach%20assumes%20that%20naturally%20occurring%20variants%20are%20fit%2C%20an%20assumption%0Achallenged%20by%20pharmacogenomics%2C%20where%20some%20pharmacogenes%20experience%20low%0Aevolutionary%20pressure.%20Deep%20mutational%20scanning%20%28DMS%29%20datasets%20provide%20an%0Aalternative%20by%20offering%20quantitative%20fitness%20scores%20for%20variants.%20In%20this%20work%2C%0Awe%20propose%20a%20transformer-based%20matrix%20variational%20auto-encoder%20%28matVAE%29%20with%20a%0Astructured%20prior%20and%20evaluate%20its%20performance%20on%2033%20DMS%20datasets%20corresponding%0Ato%2026%20drug%20target%20and%20ADME%20proteins%20from%20the%20ProteinGym%20benchmark.%20Our%20model%0Atrained%20on%20MSAs%20%28matVAE-MSA%29%20outperforms%20the%20state-of-the-art%20DeepSequence%0Amodel%20in%20zero-shot%20prediction%20on%20DMS%20datasets%2C%20despite%20using%20an%20order%20of%0Amagnitude%20fewer%20parameters%20and%20requiring%20less%20computation%20at%20inference%20time.%20We%0Aalso%20compare%20matVAE-MSA%20to%20matENC-DMS%2C%20a%20model%20of%20similar%20capacity%20trained%20on%0ADMS%20data%2C%20and%20find%20that%20the%20latter%20performs%20better%20on%20supervised%20prediction%0Atasks.%20Additionally%2C%20incorporating%20AlphaFold-generated%20structures%20into%20our%0Atransformer%20model%20further%20improves%20performance%2C%20achieving%20results%20comparable%20to%0ADeepSequence%20trained%20on%20MSAs%20and%20finetuned%20on%20DMS.%20These%20findings%20highlight%20the%0Apotential%20of%20DMS%20datasets%20to%20replace%20MSAs%20without%20significant%20loss%20in%0Apredictive%20performance%2C%20motivating%20further%20development%20of%20DMS%20datasets%20and%0Aexploration%20of%20their%20relationships%20to%20enhance%20variant%20effect%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Matrix%2520Variational%2520Auto-Encoder%2520for%2520Variant%2520Effect%2520Prediction%2520in%250A%2520%2520Pharmacogenes%26entry.906535625%3DAntoine%2520Honor%25C3%25A9%2520and%2520Borja%2520Rodr%25C3%25ADguez%2520G%25C3%25A1lvez%2520and%2520Yoomi%2520Park%2520and%2520Yitian%2520Zhou%2520and%2520Volker%2520M.%2520Lauschke%2520and%2520Ming%2520Xiao%26entry.1292438233%3D%2520%2520Variant%2520effect%2520predictors%2520%2528VEPs%2529%2520aim%2520to%2520assess%2520the%2520functional%2520impact%2520of%250Aprotein%2520variants%252C%2520traditionally%2520relying%2520on%2520multiple%2520sequence%2520alignments%2520%2528MSAs%2529.%250AThis%2520approach%2520assumes%2520that%2520naturally%2520occurring%2520variants%2520are%2520fit%252C%2520an%2520assumption%250Achallenged%2520by%2520pharmacogenomics%252C%2520where%2520some%2520pharmacogenes%2520experience%2520low%250Aevolutionary%2520pressure.%2520Deep%2520mutational%2520scanning%2520%2528DMS%2529%2520datasets%2520provide%2520an%250Aalternative%2520by%2520offering%2520quantitative%2520fitness%2520scores%2520for%2520variants.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520transformer-based%2520matrix%2520variational%2520auto-encoder%2520%2528matVAE%2529%2520with%2520a%250Astructured%2520prior%2520and%2520evaluate%2520its%2520performance%2520on%252033%2520DMS%2520datasets%2520corresponding%250Ato%252026%2520drug%2520target%2520and%2520ADME%2520proteins%2520from%2520the%2520ProteinGym%2520benchmark.%2520Our%2520model%250Atrained%2520on%2520MSAs%2520%2528matVAE-MSA%2529%2520outperforms%2520the%2520state-of-the-art%2520DeepSequence%250Amodel%2520in%2520zero-shot%2520prediction%2520on%2520DMS%2520datasets%252C%2520despite%2520using%2520an%2520order%2520of%250Amagnitude%2520fewer%2520parameters%2520and%2520requiring%2520less%2520computation%2520at%2520inference%2520time.%2520We%250Aalso%2520compare%2520matVAE-MSA%2520to%2520matENC-DMS%252C%2520a%2520model%2520of%2520similar%2520capacity%2520trained%2520on%250ADMS%2520data%252C%2520and%2520find%2520that%2520the%2520latter%2520performs%2520better%2520on%2520supervised%2520prediction%250Atasks.%2520Additionally%252C%2520incorporating%2520AlphaFold-generated%2520structures%2520into%2520our%250Atransformer%2520model%2520further%2520improves%2520performance%252C%2520achieving%2520results%2520comparable%2520to%250ADeepSequence%2520trained%2520on%2520MSAs%2520and%2520finetuned%2520on%2520DMS.%2520These%2520findings%2520highlight%2520the%250Apotential%2520of%2520DMS%2520datasets%2520to%2520replace%2520MSAs%2520without%2520significant%2520loss%2520in%250Apredictive%2520performance%252C%2520motivating%2520further%2520development%2520of%2520DMS%2520datasets%2520and%250Aexploration%2520of%2520their%2520relationships%2520to%2520enhance%2520variant%2520effect%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Matrix%20Variational%20Auto-Encoder%20for%20Variant%20Effect%20Prediction%20in%0A%20%20Pharmacogenes&entry.906535625=Antoine%20Honor%C3%A9%20and%20Borja%20Rodr%C3%ADguez%20G%C3%A1lvez%20and%20Yoomi%20Park%20and%20Yitian%20Zhou%20and%20Volker%20M.%20Lauschke%20and%20Ming%20Xiao&entry.1292438233=%20%20Variant%20effect%20predictors%20%28VEPs%29%20aim%20to%20assess%20the%20functional%20impact%20of%0Aprotein%20variants%2C%20traditionally%20relying%20on%20multiple%20sequence%20alignments%20%28MSAs%29.%0AThis%20approach%20assumes%20that%20naturally%20occurring%20variants%20are%20fit%2C%20an%20assumption%0Achallenged%20by%20pharmacogenomics%2C%20where%20some%20pharmacogenes%20experience%20low%0Aevolutionary%20pressure.%20Deep%20mutational%20scanning%20%28DMS%29%20datasets%20provide%20an%0Aalternative%20by%20offering%20quantitative%20fitness%20scores%20for%20variants.%20In%20this%20work%2C%0Awe%20propose%20a%20transformer-based%20matrix%20variational%20auto-encoder%20%28matVAE%29%20with%20a%0Astructured%20prior%20and%20evaluate%20its%20performance%20on%2033%20DMS%20datasets%20corresponding%0Ato%2026%20drug%20target%20and%20ADME%20proteins%20from%20the%20ProteinGym%20benchmark.%20Our%20model%0Atrained%20on%20MSAs%20%28matVAE-MSA%29%20outperforms%20the%20state-of-the-art%20DeepSequence%0Amodel%20in%20zero-shot%20prediction%20on%20DMS%20datasets%2C%20despite%20using%20an%20order%20of%0Amagnitude%20fewer%20parameters%20and%20requiring%20less%20computation%20at%20inference%20time.%20We%0Aalso%20compare%20matVAE-MSA%20to%20matENC-DMS%2C%20a%20model%20of%20similar%20capacity%20trained%20on%0ADMS%20data%2C%20and%20find%20that%20the%20latter%20performs%20better%20on%20supervised%20prediction%0Atasks.%20Additionally%2C%20incorporating%20AlphaFold-generated%20structures%20into%20our%0Atransformer%20model%20further%20improves%20performance%2C%20achieving%20results%20comparable%20to%0ADeepSequence%20trained%20on%20MSAs%20and%20finetuned%20on%20DMS.%20These%20findings%20highlight%20the%0Apotential%20of%20DMS%20datasets%20to%20replace%20MSAs%20without%20significant%20loss%20in%0Apredictive%20performance%2C%20motivating%20further%20development%20of%20DMS%20datasets%20and%0Aexploration%20of%20their%20relationships%20to%20enhance%20variant%20effect%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02624v1&entry.124074799=Read"},
{"title": "MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive\n  Learning for Enhanced Volumetric Grasping", "author": "Qingyu Fan and Yinghao Cai and Chao Li and Chunting Jiao and Xudong Zheng and Tao Lu and Bin Liang and Shuo Wang", "abstract": "  Robotic grasping faces challenges in adapting to objects with varying shapes\nand sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method\nthat integrates multi-scale feature extraction with contrastive feature\nenhancement for self-adaptive grasping. We propose a query-based interaction\nbetween high-level and low-level features through the Insight Transformer,\nwhile the Empower Transformer selectively attends to the highest-level\nfeatures, which synergistically strikes a balance between focusing on fine\ngeometric details and overall geometric structures. Furthermore, MISCGrasp\nutilizes multi-scale contrastive learning to exploit similarities among\npositive grasp samples, ensuring consistency across multi-scale features.\nExtensive experiments in both simulated and real-world environments demonstrate\nthat MISCGrasp outperforms baseline and variant methods in tabletop\ndecluttering tasks. More details are available at https://miscgrasp.github.io/.\n", "link": "http://arxiv.org/abs/2507.02672v1", "date": "2025-07-03", "relevancy": 2.4084, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6545}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5698}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MISCGrasp%3A%20Leveraging%20Multiple%20Integrated%20Scales%20and%20Contrastive%0A%20%20Learning%20for%20Enhanced%20Volumetric%20Grasping&body=Title%3A%20MISCGrasp%3A%20Leveraging%20Multiple%20Integrated%20Scales%20and%20Contrastive%0A%20%20Learning%20for%20Enhanced%20Volumetric%20Grasping%0AAuthor%3A%20Qingyu%20Fan%20and%20Yinghao%20Cai%20and%20Chao%20Li%20and%20Chunting%20Jiao%20and%20Xudong%20Zheng%20and%20Tao%20Lu%20and%20Bin%20Liang%20and%20Shuo%20Wang%0AAbstract%3A%20%20%20Robotic%20grasping%20faces%20challenges%20in%20adapting%20to%20objects%20with%20varying%20shapes%0Aand%20sizes.%20In%20this%20paper%2C%20we%20introduce%20MISCGrasp%2C%20a%20volumetric%20grasping%20method%0Athat%20integrates%20multi-scale%20feature%20extraction%20with%20contrastive%20feature%0Aenhancement%20for%20self-adaptive%20grasping.%20We%20propose%20a%20query-based%20interaction%0Abetween%20high-level%20and%20low-level%20features%20through%20the%20Insight%20Transformer%2C%0Awhile%20the%20Empower%20Transformer%20selectively%20attends%20to%20the%20highest-level%0Afeatures%2C%20which%20synergistically%20strikes%20a%20balance%20between%20focusing%20on%20fine%0Ageometric%20details%20and%20overall%20geometric%20structures.%20Furthermore%2C%20MISCGrasp%0Autilizes%20multi-scale%20contrastive%20learning%20to%20exploit%20similarities%20among%0Apositive%20grasp%20samples%2C%20ensuring%20consistency%20across%20multi-scale%20features.%0AExtensive%20experiments%20in%20both%20simulated%20and%20real-world%20environments%20demonstrate%0Athat%20MISCGrasp%20outperforms%20baseline%20and%20variant%20methods%20in%20tabletop%0Adecluttering%20tasks.%20More%20details%20are%20available%20at%20https%3A//miscgrasp.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMISCGrasp%253A%2520Leveraging%2520Multiple%2520Integrated%2520Scales%2520and%2520Contrastive%250A%2520%2520Learning%2520for%2520Enhanced%2520Volumetric%2520Grasping%26entry.906535625%3DQingyu%2520Fan%2520and%2520Yinghao%2520Cai%2520and%2520Chao%2520Li%2520and%2520Chunting%2520Jiao%2520and%2520Xudong%2520Zheng%2520and%2520Tao%2520Lu%2520and%2520Bin%2520Liang%2520and%2520Shuo%2520Wang%26entry.1292438233%3D%2520%2520Robotic%2520grasping%2520faces%2520challenges%2520in%2520adapting%2520to%2520objects%2520with%2520varying%2520shapes%250Aand%2520sizes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MISCGrasp%252C%2520a%2520volumetric%2520grasping%2520method%250Athat%2520integrates%2520multi-scale%2520feature%2520extraction%2520with%2520contrastive%2520feature%250Aenhancement%2520for%2520self-adaptive%2520grasping.%2520We%2520propose%2520a%2520query-based%2520interaction%250Abetween%2520high-level%2520and%2520low-level%2520features%2520through%2520the%2520Insight%2520Transformer%252C%250Awhile%2520the%2520Empower%2520Transformer%2520selectively%2520attends%2520to%2520the%2520highest-level%250Afeatures%252C%2520which%2520synergistically%2520strikes%2520a%2520balance%2520between%2520focusing%2520on%2520fine%250Ageometric%2520details%2520and%2520overall%2520geometric%2520structures.%2520Furthermore%252C%2520MISCGrasp%250Autilizes%2520multi-scale%2520contrastive%2520learning%2520to%2520exploit%2520similarities%2520among%250Apositive%2520grasp%2520samples%252C%2520ensuring%2520consistency%2520across%2520multi-scale%2520features.%250AExtensive%2520experiments%2520in%2520both%2520simulated%2520and%2520real-world%2520environments%2520demonstrate%250Athat%2520MISCGrasp%2520outperforms%2520baseline%2520and%2520variant%2520methods%2520in%2520tabletop%250Adecluttering%2520tasks.%2520More%2520details%2520are%2520available%2520at%2520https%253A//miscgrasp.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MISCGrasp%3A%20Leveraging%20Multiple%20Integrated%20Scales%20and%20Contrastive%0A%20%20Learning%20for%20Enhanced%20Volumetric%20Grasping&entry.906535625=Qingyu%20Fan%20and%20Yinghao%20Cai%20and%20Chao%20Li%20and%20Chunting%20Jiao%20and%20Xudong%20Zheng%20and%20Tao%20Lu%20and%20Bin%20Liang%20and%20Shuo%20Wang&entry.1292438233=%20%20Robotic%20grasping%20faces%20challenges%20in%20adapting%20to%20objects%20with%20varying%20shapes%0Aand%20sizes.%20In%20this%20paper%2C%20we%20introduce%20MISCGrasp%2C%20a%20volumetric%20grasping%20method%0Athat%20integrates%20multi-scale%20feature%20extraction%20with%20contrastive%20feature%0Aenhancement%20for%20self-adaptive%20grasping.%20We%20propose%20a%20query-based%20interaction%0Abetween%20high-level%20and%20low-level%20features%20through%20the%20Insight%20Transformer%2C%0Awhile%20the%20Empower%20Transformer%20selectively%20attends%20to%20the%20highest-level%0Afeatures%2C%20which%20synergistically%20strikes%20a%20balance%20between%20focusing%20on%20fine%0Ageometric%20details%20and%20overall%20geometric%20structures.%20Furthermore%2C%20MISCGrasp%0Autilizes%20multi-scale%20contrastive%20learning%20to%20exploit%20similarities%20among%0Apositive%20grasp%20samples%2C%20ensuring%20consistency%20across%20multi-scale%20features.%0AExtensive%20experiments%20in%20both%20simulated%20and%20real-world%20environments%20demonstrate%0Athat%20MISCGrasp%20outperforms%20baseline%20and%20variant%20methods%20in%20tabletop%0Adecluttering%20tasks.%20More%20details%20are%20available%20at%20https%3A//miscgrasp.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02672v1&entry.124074799=Read"},
{"title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM\n  Training in Proof Generation", "author": "Chenyang An and Shima Imani and Feng Yao and Chengyu Dong and Ali Abbasi and Harsh Shrivastava and Samuel Buss and Jingbo Shang and Gayathri Mahalingam and Pramod Sharma and Maurice Diesendruck", "abstract": "  In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix.\n", "link": "http://arxiv.org/abs/2411.00863v2", "date": "2025-07-03", "relevancy": 2.4063, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Token%20Prediction%20Task%20Assumes%20Optimal%20Data%20Ordering%20for%20LLM%0A%20%20Training%20in%20Proof%20Generation&body=Title%3A%20Next-Token%20Prediction%20Task%20Assumes%20Optimal%20Data%20Ordering%20for%20LLM%0A%20%20Training%20in%20Proof%20Generation%0AAuthor%3A%20Chenyang%20An%20and%20Shima%20Imani%20and%20Feng%20Yao%20and%20Chengyu%20Dong%20and%20Ali%20Abbasi%20and%20Harsh%20Shrivastava%20and%20Samuel%20Buss%20and%20Jingbo%20Shang%20and%20Gayathri%20Mahalingam%20and%20Pramod%20Sharma%20and%20Maurice%20Diesendruck%0AAbstract%3A%20%20%20In%20the%20field%20of%20large%20language%20model%20%28LLM%29-based%20proof%20generation%2C%20despite%0Aextensive%20training%20on%20large%20datasets%20such%20as%20ArXiv%2C%20LLMs%20still%20exhibit%20only%0Amodest%20performance%20on%20proving%20tasks%20of%20moderate%20difficulty.%20We%20believe%20that%0Athis%20is%20partly%20due%20to%20the%20widespread%20presence%20of%20suboptimal%20ordering%20within%20the%0Adata%20for%20each%20proof%20used%20in%20training.%20For%20example%2C%20published%20proofs%20often%0Afollow%20a%20purely%20logical%20order%2C%20where%20each%20step%20logically%20proceeds%20from%20the%0Aprevious%20steps%20based%20on%20the%20deductive%20rules.%20This%20order%20is%20designed%20to%0Afacilitate%20the%20verification%20of%20the%20proof%27s%20soundness%2C%20rather%20than%20to%20help%0Apeople%20and%20models%20learn%20the%20discovery%20process%20of%20the%20proof.%20In%20proof%0Ageneration%2C%20we%20argue%20that%20the%20optimal%20order%20for%20one%20training%20data%20sample%20occurs%0Awhen%20the%20relevant%20intermediate%20supervision%20for%20a%20particular%20proof%20step%20in%20the%0Aproof%20is%20always%20positioned%20to%20the%20left%20of%20that%20proof%20step.%20We%20call%20such%20order%0Athe%20intuitively%20sequential%20order.%20We%20validate%20our%20claims%20using%20two%20tasks%3A%0Aintuitionistic%20propositional%20logic%20theorem-proving%20and%20digit%20multiplication.%0AOur%20experiments%20verify%20the%20order%20effect%20and%20provide%20support%20for%20our%0Aexplanations.%20We%20demonstrate%20that%20training%20is%20most%20effective%20when%20the%20proof%20is%0Ain%20the%20intuitively%20sequential%20order.%20Moreover%2C%20the%20order%20effect%20and%20the%0Aperformance%20gap%20between%20models%20trained%20on%20different%20data%20orders%20can%20be%0Asubstantial%20--%20with%20an%2011%20percent%20improvement%20in%20proof%20success%20rate%20observed%20in%0Athe%20propositional%20logic%20theorem-proving%20task%2C%20between%20models%20trained%20on%20the%0Aoptimal%20order%20compared%20to%20the%20worst%20order.%20Lastly%2C%20we%20define%20a%20common%20type%20of%0Aorder%20issue%20in%20advanced%20math%20proofs%20and%20find%20that%2017.3%20percent%20of%20theorems%20with%0Anontrivial%20proofs%20in%20the%20first%20two%20chapters%20of%20a%20widely%20used%20graduate-level%0Amathematics%20textbook%20suffer%20from%20this%20issue.%20A%20detailed%20list%20of%20those%20proofs%20is%0Aprovided%20in%20the%20appendix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Token%2520Prediction%2520Task%2520Assumes%2520Optimal%2520Data%2520Ordering%2520for%2520LLM%250A%2520%2520Training%2520in%2520Proof%2520Generation%26entry.906535625%3DChenyang%2520An%2520and%2520Shima%2520Imani%2520and%2520Feng%2520Yao%2520and%2520Chengyu%2520Dong%2520and%2520Ali%2520Abbasi%2520and%2520Harsh%2520Shrivastava%2520and%2520Samuel%2520Buss%2520and%2520Jingbo%2520Shang%2520and%2520Gayathri%2520Mahalingam%2520and%2520Pramod%2520Sharma%2520and%2520Maurice%2520Diesendruck%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520large%2520language%2520model%2520%2528LLM%2529-based%2520proof%2520generation%252C%2520despite%250Aextensive%2520training%2520on%2520large%2520datasets%2520such%2520as%2520ArXiv%252C%2520LLMs%2520still%2520exhibit%2520only%250Amodest%2520performance%2520on%2520proving%2520tasks%2520of%2520moderate%2520difficulty.%2520We%2520believe%2520that%250Athis%2520is%2520partly%2520due%2520to%2520the%2520widespread%2520presence%2520of%2520suboptimal%2520ordering%2520within%2520the%250Adata%2520for%2520each%2520proof%2520used%2520in%2520training.%2520For%2520example%252C%2520published%2520proofs%2520often%250Afollow%2520a%2520purely%2520logical%2520order%252C%2520where%2520each%2520step%2520logically%2520proceeds%2520from%2520the%250Aprevious%2520steps%2520based%2520on%2520the%2520deductive%2520rules.%2520This%2520order%2520is%2520designed%2520to%250Afacilitate%2520the%2520verification%2520of%2520the%2520proof%2527s%2520soundness%252C%2520rather%2520than%2520to%2520help%250Apeople%2520and%2520models%2520learn%2520the%2520discovery%2520process%2520of%2520the%2520proof.%2520In%2520proof%250Ageneration%252C%2520we%2520argue%2520that%2520the%2520optimal%2520order%2520for%2520one%2520training%2520data%2520sample%2520occurs%250Awhen%2520the%2520relevant%2520intermediate%2520supervision%2520for%2520a%2520particular%2520proof%2520step%2520in%2520the%250Aproof%2520is%2520always%2520positioned%2520to%2520the%2520left%2520of%2520that%2520proof%2520step.%2520We%2520call%2520such%2520order%250Athe%2520intuitively%2520sequential%2520order.%2520We%2520validate%2520our%2520claims%2520using%2520two%2520tasks%253A%250Aintuitionistic%2520propositional%2520logic%2520theorem-proving%2520and%2520digit%2520multiplication.%250AOur%2520experiments%2520verify%2520the%2520order%2520effect%2520and%2520provide%2520support%2520for%2520our%250Aexplanations.%2520We%2520demonstrate%2520that%2520training%2520is%2520most%2520effective%2520when%2520the%2520proof%2520is%250Ain%2520the%2520intuitively%2520sequential%2520order.%2520Moreover%252C%2520the%2520order%2520effect%2520and%2520the%250Aperformance%2520gap%2520between%2520models%2520trained%2520on%2520different%2520data%2520orders%2520can%2520be%250Asubstantial%2520--%2520with%2520an%252011%2520percent%2520improvement%2520in%2520proof%2520success%2520rate%2520observed%2520in%250Athe%2520propositional%2520logic%2520theorem-proving%2520task%252C%2520between%2520models%2520trained%2520on%2520the%250Aoptimal%2520order%2520compared%2520to%2520the%2520worst%2520order.%2520Lastly%252C%2520we%2520define%2520a%2520common%2520type%2520of%250Aorder%2520issue%2520in%2520advanced%2520math%2520proofs%2520and%2520find%2520that%252017.3%2520percent%2520of%2520theorems%2520with%250Anontrivial%2520proofs%2520in%2520the%2520first%2520two%2520chapters%2520of%2520a%2520widely%2520used%2520graduate-level%250Amathematics%2520textbook%2520suffer%2520from%2520this%2520issue.%2520A%2520detailed%2520list%2520of%2520those%2520proofs%2520is%250Aprovided%2520in%2520the%2520appendix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Token%20Prediction%20Task%20Assumes%20Optimal%20Data%20Ordering%20for%20LLM%0A%20%20Training%20in%20Proof%20Generation&entry.906535625=Chenyang%20An%20and%20Shima%20Imani%20and%20Feng%20Yao%20and%20Chengyu%20Dong%20and%20Ali%20Abbasi%20and%20Harsh%20Shrivastava%20and%20Samuel%20Buss%20and%20Jingbo%20Shang%20and%20Gayathri%20Mahalingam%20and%20Pramod%20Sharma%20and%20Maurice%20Diesendruck&entry.1292438233=%20%20In%20the%20field%20of%20large%20language%20model%20%28LLM%29-based%20proof%20generation%2C%20despite%0Aextensive%20training%20on%20large%20datasets%20such%20as%20ArXiv%2C%20LLMs%20still%20exhibit%20only%0Amodest%20performance%20on%20proving%20tasks%20of%20moderate%20difficulty.%20We%20believe%20that%0Athis%20is%20partly%20due%20to%20the%20widespread%20presence%20of%20suboptimal%20ordering%20within%20the%0Adata%20for%20each%20proof%20used%20in%20training.%20For%20example%2C%20published%20proofs%20often%0Afollow%20a%20purely%20logical%20order%2C%20where%20each%20step%20logically%20proceeds%20from%20the%0Aprevious%20steps%20based%20on%20the%20deductive%20rules.%20This%20order%20is%20designed%20to%0Afacilitate%20the%20verification%20of%20the%20proof%27s%20soundness%2C%20rather%20than%20to%20help%0Apeople%20and%20models%20learn%20the%20discovery%20process%20of%20the%20proof.%20In%20proof%0Ageneration%2C%20we%20argue%20that%20the%20optimal%20order%20for%20one%20training%20data%20sample%20occurs%0Awhen%20the%20relevant%20intermediate%20supervision%20for%20a%20particular%20proof%20step%20in%20the%0Aproof%20is%20always%20positioned%20to%20the%20left%20of%20that%20proof%20step.%20We%20call%20such%20order%0Athe%20intuitively%20sequential%20order.%20We%20validate%20our%20claims%20using%20two%20tasks%3A%0Aintuitionistic%20propositional%20logic%20theorem-proving%20and%20digit%20multiplication.%0AOur%20experiments%20verify%20the%20order%20effect%20and%20provide%20support%20for%20our%0Aexplanations.%20We%20demonstrate%20that%20training%20is%20most%20effective%20when%20the%20proof%20is%0Ain%20the%20intuitively%20sequential%20order.%20Moreover%2C%20the%20order%20effect%20and%20the%0Aperformance%20gap%20between%20models%20trained%20on%20different%20data%20orders%20can%20be%0Asubstantial%20--%20with%20an%2011%20percent%20improvement%20in%20proof%20success%20rate%20observed%20in%0Athe%20propositional%20logic%20theorem-proving%20task%2C%20between%20models%20trained%20on%20the%0Aoptimal%20order%20compared%20to%20the%20worst%20order.%20Lastly%2C%20we%20define%20a%20common%20type%20of%0Aorder%20issue%20in%20advanced%20math%20proofs%20and%20find%20that%2017.3%20percent%20of%20theorems%20with%0Anontrivial%20proofs%20in%20the%20first%20two%20chapters%20of%20a%20widely%20used%20graduate-level%0Amathematics%20textbook%20suffer%20from%20this%20issue.%20A%20detailed%20list%20of%20those%20proofs%20is%0Aprovided%20in%20the%20appendix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00863v2&entry.124074799=Read"},
{"title": "S2FGL: Spatial Spectral Federated Graph Learning", "author": "Zihan Tan and Suyuan Huang and Guancheng Wan and Wenke Huang and He Li and Mang Ye", "abstract": "  Federated Graph Learning (FGL) combines the privacy-preserving capabilities\nof federated learning (FL) with the strong graph modeling capability of Graph\nNeural Networks (GNNs). Current research addresses subgraph-FL only from the\nstructural perspective, neglecting the propagation of graph signals on spatial\nand spectral domains of the structure. From a spatial perspective, subgraph-FL\nintroduces edge disconnections between clients, leading to disruptions in label\nsignals and a degradation in the class knowledge of the global GNN. From a\nspectral perspective, spectral heterogeneity causes inconsistencies in signal\nfrequencies across subgraphs, which makes local GNNs overfit the local signal\npropagation schemes. As a result, spectral client drifts occur, undermining\nglobal generalizability. To tackle the challenges, we propose a global\nknowledge repository to mitigate label signal disruption and a frequency\nalignment to address spectral client drifts. The combination of spatial and\nspectral strategies forms our framework S2FGL. Extensive experiments on\nmultiple datasets demonstrate the superiority of S2FGL. The code is available\nat https://github.com/Wonder7racer/S2FGL.git.\n", "link": "http://arxiv.org/abs/2507.02409v1", "date": "2025-07-03", "relevancy": 2.3903, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4743}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2FGL%3A%20Spatial%20Spectral%20Federated%20Graph%20Learning&body=Title%3A%20S2FGL%3A%20Spatial%20Spectral%20Federated%20Graph%20Learning%0AAuthor%3A%20Zihan%20Tan%20and%20Suyuan%20Huang%20and%20Guancheng%20Wan%20and%20Wenke%20Huang%20and%20He%20Li%20and%20Mang%20Ye%0AAbstract%3A%20%20%20Federated%20Graph%20Learning%20%28FGL%29%20combines%20the%20privacy-preserving%20capabilities%0Aof%20federated%20learning%20%28FL%29%20with%20the%20strong%20graph%20modeling%20capability%20of%20Graph%0ANeural%20Networks%20%28GNNs%29.%20Current%20research%20addresses%20subgraph-FL%20only%20from%20the%0Astructural%20perspective%2C%20neglecting%20the%20propagation%20of%20graph%20signals%20on%20spatial%0Aand%20spectral%20domains%20of%20the%20structure.%20From%20a%20spatial%20perspective%2C%20subgraph-FL%0Aintroduces%20edge%20disconnections%20between%20clients%2C%20leading%20to%20disruptions%20in%20label%0Asignals%20and%20a%20degradation%20in%20the%20class%20knowledge%20of%20the%20global%20GNN.%20From%20a%0Aspectral%20perspective%2C%20spectral%20heterogeneity%20causes%20inconsistencies%20in%20signal%0Afrequencies%20across%20subgraphs%2C%20which%20makes%20local%20GNNs%20overfit%20the%20local%20signal%0Apropagation%20schemes.%20As%20a%20result%2C%20spectral%20client%20drifts%20occur%2C%20undermining%0Aglobal%20generalizability.%20To%20tackle%20the%20challenges%2C%20we%20propose%20a%20global%0Aknowledge%20repository%20to%20mitigate%20label%20signal%20disruption%20and%20a%20frequency%0Aalignment%20to%20address%20spectral%20client%20drifts.%20The%20combination%20of%20spatial%20and%0Aspectral%20strategies%20forms%20our%20framework%20S2FGL.%20Extensive%20experiments%20on%0Amultiple%20datasets%20demonstrate%20the%20superiority%20of%20S2FGL.%20The%20code%20is%20available%0Aat%20https%3A//github.com/Wonder7racer/S2FGL.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2FGL%253A%2520Spatial%2520Spectral%2520Federated%2520Graph%2520Learning%26entry.906535625%3DZihan%2520Tan%2520and%2520Suyuan%2520Huang%2520and%2520Guancheng%2520Wan%2520and%2520Wenke%2520Huang%2520and%2520He%2520Li%2520and%2520Mang%2520Ye%26entry.1292438233%3D%2520%2520Federated%2520Graph%2520Learning%2520%2528FGL%2529%2520combines%2520the%2520privacy-preserving%2520capabilities%250Aof%2520federated%2520learning%2520%2528FL%2529%2520with%2520the%2520strong%2520graph%2520modeling%2520capability%2520of%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529.%2520Current%2520research%2520addresses%2520subgraph-FL%2520only%2520from%2520the%250Astructural%2520perspective%252C%2520neglecting%2520the%2520propagation%2520of%2520graph%2520signals%2520on%2520spatial%250Aand%2520spectral%2520domains%2520of%2520the%2520structure.%2520From%2520a%2520spatial%2520perspective%252C%2520subgraph-FL%250Aintroduces%2520edge%2520disconnections%2520between%2520clients%252C%2520leading%2520to%2520disruptions%2520in%2520label%250Asignals%2520and%2520a%2520degradation%2520in%2520the%2520class%2520knowledge%2520of%2520the%2520global%2520GNN.%2520From%2520a%250Aspectral%2520perspective%252C%2520spectral%2520heterogeneity%2520causes%2520inconsistencies%2520in%2520signal%250Afrequencies%2520across%2520subgraphs%252C%2520which%2520makes%2520local%2520GNNs%2520overfit%2520the%2520local%2520signal%250Apropagation%2520schemes.%2520As%2520a%2520result%252C%2520spectral%2520client%2520drifts%2520occur%252C%2520undermining%250Aglobal%2520generalizability.%2520To%2520tackle%2520the%2520challenges%252C%2520we%2520propose%2520a%2520global%250Aknowledge%2520repository%2520to%2520mitigate%2520label%2520signal%2520disruption%2520and%2520a%2520frequency%250Aalignment%2520to%2520address%2520spectral%2520client%2520drifts.%2520The%2520combination%2520of%2520spatial%2520and%250Aspectral%2520strategies%2520forms%2520our%2520framework%2520S2FGL.%2520Extensive%2520experiments%2520on%250Amultiple%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520S2FGL.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/Wonder7racer/S2FGL.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2FGL%3A%20Spatial%20Spectral%20Federated%20Graph%20Learning&entry.906535625=Zihan%20Tan%20and%20Suyuan%20Huang%20and%20Guancheng%20Wan%20and%20Wenke%20Huang%20and%20He%20Li%20and%20Mang%20Ye&entry.1292438233=%20%20Federated%20Graph%20Learning%20%28FGL%29%20combines%20the%20privacy-preserving%20capabilities%0Aof%20federated%20learning%20%28FL%29%20with%20the%20strong%20graph%20modeling%20capability%20of%20Graph%0ANeural%20Networks%20%28GNNs%29.%20Current%20research%20addresses%20subgraph-FL%20only%20from%20the%0Astructural%20perspective%2C%20neglecting%20the%20propagation%20of%20graph%20signals%20on%20spatial%0Aand%20spectral%20domains%20of%20the%20structure.%20From%20a%20spatial%20perspective%2C%20subgraph-FL%0Aintroduces%20edge%20disconnections%20between%20clients%2C%20leading%20to%20disruptions%20in%20label%0Asignals%20and%20a%20degradation%20in%20the%20class%20knowledge%20of%20the%20global%20GNN.%20From%20a%0Aspectral%20perspective%2C%20spectral%20heterogeneity%20causes%20inconsistencies%20in%20signal%0Afrequencies%20across%20subgraphs%2C%20which%20makes%20local%20GNNs%20overfit%20the%20local%20signal%0Apropagation%20schemes.%20As%20a%20result%2C%20spectral%20client%20drifts%20occur%2C%20undermining%0Aglobal%20generalizability.%20To%20tackle%20the%20challenges%2C%20we%20propose%20a%20global%0Aknowledge%20repository%20to%20mitigate%20label%20signal%20disruption%20and%20a%20frequency%0Aalignment%20to%20address%20spectral%20client%20drifts.%20The%20combination%20of%20spatial%20and%0Aspectral%20strategies%20forms%20our%20framework%20S2FGL.%20Extensive%20experiments%20on%0Amultiple%20datasets%20demonstrate%20the%20superiority%20of%20S2FGL.%20The%20code%20is%20available%0Aat%20https%3A//github.com/Wonder7racer/S2FGL.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02409v1&entry.124074799=Read"},
{"title": "IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical\n  Analysis from Laboratory Images Using Computer Vision and Deep Learning", "author": "Abiam Remache Gonz\u00e1lez and Meriem Chagour and Timon Bijan R\u00fcth and Ra\u00fal Trapiella Ca\u00f1edo and Marina Mart\u00ednez Soler and \u00c1lvaro Lorenzo Felipe and Hyun-Suk Shin and Mar\u00eda-Jes\u00fas Zamorano Serrano and Ricardo Torres and Juan-Antonio Castillo Parra and Eduardo Reyes Abad and Miguel-\u00c1ngel Ferrer Ballester and Juan-Manuel Afonso L\u00f3pez and Francisco-Mario Hern\u00e1ndez Tejera and Adrian Penate-Sanchez", "abstract": "  This paper introduces IMASHRIMP, an adapted system for the automated\nmorphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing\ngenetic selection tasks in aquaculture. Existing deep learning and computer\nvision techniques were modified to address the specific challenges of shrimp\nmorphology analysis from RGBD images. IMASHRIMP incorporates two discrimination\nmodules, based on a modified ResNet-50 architecture, to classify images by the\npoint of view and determine rostrum integrity. It is proposed a \"two-factor\nauthentication (human and IA)\" system, it reduces human error in view\nclassification from 0.97% to 0% and in rostrum detection from 12.46% to 3.64%.\nAdditionally, a pose estimation module was adapted from VitPose to predict 23\nkey points on the shrimp's skeleton, with separate networks for lateral and\ndorsal views. A morphological regression module, using a Support Vector Machine\n(SVM) model, was integrated to convert pixel measurements to centimeter units.\nExperimental results show that the system effectively reduces human error,\nachieving a mean average precision (mAP) of 97.94% for pose estimation and a\npixel-to-centimeter conversion error of 0.07 (+/- 0.1) cm. IMASHRIMP\ndemonstrates the potential to automate and accelerate shrimp morphological\nanalysis, enhancing the efficiency of genetic selection and contributing to\nmore sustainable aquaculture practices.The code are available at\nhttps://github.com/AbiamRemacheGonzalez/ImaShrimp-public\n", "link": "http://arxiv.org/abs/2507.02519v1", "date": "2025-07-03", "relevancy": 2.3841, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4909}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.477}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMASHRIMP%3A%20Automatic%20White%20Shrimp%20%28Penaeus%20vannamei%29%20Biometrical%0A%20%20Analysis%20from%20Laboratory%20Images%20Using%20Computer%20Vision%20and%20Deep%20Learning&body=Title%3A%20IMASHRIMP%3A%20Automatic%20White%20Shrimp%20%28Penaeus%20vannamei%29%20Biometrical%0A%20%20Analysis%20from%20Laboratory%20Images%20Using%20Computer%20Vision%20and%20Deep%20Learning%0AAuthor%3A%20Abiam%20Remache%20Gonz%C3%A1lez%20and%20Meriem%20Chagour%20and%20Timon%20Bijan%20R%C3%BCth%20and%20Ra%C3%BAl%20Trapiella%20Ca%C3%B1edo%20and%20Marina%20Mart%C3%ADnez%20Soler%20and%20%C3%81lvaro%20Lorenzo%20Felipe%20and%20Hyun-Suk%20Shin%20and%20Mar%C3%ADa-Jes%C3%BAs%20Zamorano%20Serrano%20and%20Ricardo%20Torres%20and%20Juan-Antonio%20Castillo%20Parra%20and%20Eduardo%20Reyes%20Abad%20and%20Miguel-%C3%81ngel%20Ferrer%20Ballester%20and%20Juan-Manuel%20Afonso%20L%C3%B3pez%20and%20Francisco-Mario%20Hern%C3%A1ndez%20Tejera%20and%20Adrian%20Penate-Sanchez%0AAbstract%3A%20%20%20This%20paper%20introduces%20IMASHRIMP%2C%20an%20adapted%20system%20for%20the%20automated%0Amorphological%20analysis%20of%20white%20shrimp%20%28Penaeus%20vannamei%7D%2C%20aimed%20at%20optimizing%0Agenetic%20selection%20tasks%20in%20aquaculture.%20Existing%20deep%20learning%20and%20computer%0Avision%20techniques%20were%20modified%20to%20address%20the%20specific%20challenges%20of%20shrimp%0Amorphology%20analysis%20from%20RGBD%20images.%20IMASHRIMP%20incorporates%20two%20discrimination%0Amodules%2C%20based%20on%20a%20modified%20ResNet-50%20architecture%2C%20to%20classify%20images%20by%20the%0Apoint%20of%20view%20and%20determine%20rostrum%20integrity.%20It%20is%20proposed%20a%20%22two-factor%0Aauthentication%20%28human%20and%20IA%29%22%20system%2C%20it%20reduces%20human%20error%20in%20view%0Aclassification%20from%200.97%25%20to%200%25%20and%20in%20rostrum%20detection%20from%2012.46%25%20to%203.64%25.%0AAdditionally%2C%20a%20pose%20estimation%20module%20was%20adapted%20from%20VitPose%20to%20predict%2023%0Akey%20points%20on%20the%20shrimp%27s%20skeleton%2C%20with%20separate%20networks%20for%20lateral%20and%0Adorsal%20views.%20A%20morphological%20regression%20module%2C%20using%20a%20Support%20Vector%20Machine%0A%28SVM%29%20model%2C%20was%20integrated%20to%20convert%20pixel%20measurements%20to%20centimeter%20units.%0AExperimental%20results%20show%20that%20the%20system%20effectively%20reduces%20human%20error%2C%0Aachieving%20a%20mean%20average%20precision%20%28mAP%29%20of%2097.94%25%20for%20pose%20estimation%20and%20a%0Apixel-to-centimeter%20conversion%20error%20of%200.07%20%28%2B/-%200.1%29%20cm.%20IMASHRIMP%0Ademonstrates%20the%20potential%20to%20automate%20and%20accelerate%20shrimp%20morphological%0Aanalysis%2C%20enhancing%20the%20efficiency%20of%20genetic%20selection%20and%20contributing%20to%0Amore%20sustainable%20aquaculture%20practices.The%20code%20are%20available%20at%0Ahttps%3A//github.com/AbiamRemacheGonzalez/ImaShrimp-public%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMASHRIMP%253A%2520Automatic%2520White%2520Shrimp%2520%2528Penaeus%2520vannamei%2529%2520Biometrical%250A%2520%2520Analysis%2520from%2520Laboratory%2520Images%2520Using%2520Computer%2520Vision%2520and%2520Deep%2520Learning%26entry.906535625%3DAbiam%2520Remache%2520Gonz%25C3%25A1lez%2520and%2520Meriem%2520Chagour%2520and%2520Timon%2520Bijan%2520R%25C3%25BCth%2520and%2520Ra%25C3%25BAl%2520Trapiella%2520Ca%25C3%25B1edo%2520and%2520Marina%2520Mart%25C3%25ADnez%2520Soler%2520and%2520%25C3%2581lvaro%2520Lorenzo%2520Felipe%2520and%2520Hyun-Suk%2520Shin%2520and%2520Mar%25C3%25ADa-Jes%25C3%25BAs%2520Zamorano%2520Serrano%2520and%2520Ricardo%2520Torres%2520and%2520Juan-Antonio%2520Castillo%2520Parra%2520and%2520Eduardo%2520Reyes%2520Abad%2520and%2520Miguel-%25C3%2581ngel%2520Ferrer%2520Ballester%2520and%2520Juan-Manuel%2520Afonso%2520L%25C3%25B3pez%2520and%2520Francisco-Mario%2520Hern%25C3%25A1ndez%2520Tejera%2520and%2520Adrian%2520Penate-Sanchez%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520IMASHRIMP%252C%2520an%2520adapted%2520system%2520for%2520the%2520automated%250Amorphological%2520analysis%2520of%2520white%2520shrimp%2520%2528Penaeus%2520vannamei%257D%252C%2520aimed%2520at%2520optimizing%250Agenetic%2520selection%2520tasks%2520in%2520aquaculture.%2520Existing%2520deep%2520learning%2520and%2520computer%250Avision%2520techniques%2520were%2520modified%2520to%2520address%2520the%2520specific%2520challenges%2520of%2520shrimp%250Amorphology%2520analysis%2520from%2520RGBD%2520images.%2520IMASHRIMP%2520incorporates%2520two%2520discrimination%250Amodules%252C%2520based%2520on%2520a%2520modified%2520ResNet-50%2520architecture%252C%2520to%2520classify%2520images%2520by%2520the%250Apoint%2520of%2520view%2520and%2520determine%2520rostrum%2520integrity.%2520It%2520is%2520proposed%2520a%2520%2522two-factor%250Aauthentication%2520%2528human%2520and%2520IA%2529%2522%2520system%252C%2520it%2520reduces%2520human%2520error%2520in%2520view%250Aclassification%2520from%25200.97%2525%2520to%25200%2525%2520and%2520in%2520rostrum%2520detection%2520from%252012.46%2525%2520to%25203.64%2525.%250AAdditionally%252C%2520a%2520pose%2520estimation%2520module%2520was%2520adapted%2520from%2520VitPose%2520to%2520predict%252023%250Akey%2520points%2520on%2520the%2520shrimp%2527s%2520skeleton%252C%2520with%2520separate%2520networks%2520for%2520lateral%2520and%250Adorsal%2520views.%2520A%2520morphological%2520regression%2520module%252C%2520using%2520a%2520Support%2520Vector%2520Machine%250A%2528SVM%2529%2520model%252C%2520was%2520integrated%2520to%2520convert%2520pixel%2520measurements%2520to%2520centimeter%2520units.%250AExperimental%2520results%2520show%2520that%2520the%2520system%2520effectively%2520reduces%2520human%2520error%252C%250Aachieving%2520a%2520mean%2520average%2520precision%2520%2528mAP%2529%2520of%252097.94%2525%2520for%2520pose%2520estimation%2520and%2520a%250Apixel-to-centimeter%2520conversion%2520error%2520of%25200.07%2520%2528%252B/-%25200.1%2529%2520cm.%2520IMASHRIMP%250Ademonstrates%2520the%2520potential%2520to%2520automate%2520and%2520accelerate%2520shrimp%2520morphological%250Aanalysis%252C%2520enhancing%2520the%2520efficiency%2520of%2520genetic%2520selection%2520and%2520contributing%2520to%250Amore%2520sustainable%2520aquaculture%2520practices.The%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/AbiamRemacheGonzalez/ImaShrimp-public%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMASHRIMP%3A%20Automatic%20White%20Shrimp%20%28Penaeus%20vannamei%29%20Biometrical%0A%20%20Analysis%20from%20Laboratory%20Images%20Using%20Computer%20Vision%20and%20Deep%20Learning&entry.906535625=Abiam%20Remache%20Gonz%C3%A1lez%20and%20Meriem%20Chagour%20and%20Timon%20Bijan%20R%C3%BCth%20and%20Ra%C3%BAl%20Trapiella%20Ca%C3%B1edo%20and%20Marina%20Mart%C3%ADnez%20Soler%20and%20%C3%81lvaro%20Lorenzo%20Felipe%20and%20Hyun-Suk%20Shin%20and%20Mar%C3%ADa-Jes%C3%BAs%20Zamorano%20Serrano%20and%20Ricardo%20Torres%20and%20Juan-Antonio%20Castillo%20Parra%20and%20Eduardo%20Reyes%20Abad%20and%20Miguel-%C3%81ngel%20Ferrer%20Ballester%20and%20Juan-Manuel%20Afonso%20L%C3%B3pez%20and%20Francisco-Mario%20Hern%C3%A1ndez%20Tejera%20and%20Adrian%20Penate-Sanchez&entry.1292438233=%20%20This%20paper%20introduces%20IMASHRIMP%2C%20an%20adapted%20system%20for%20the%20automated%0Amorphological%20analysis%20of%20white%20shrimp%20%28Penaeus%20vannamei%7D%2C%20aimed%20at%20optimizing%0Agenetic%20selection%20tasks%20in%20aquaculture.%20Existing%20deep%20learning%20and%20computer%0Avision%20techniques%20were%20modified%20to%20address%20the%20specific%20challenges%20of%20shrimp%0Amorphology%20analysis%20from%20RGBD%20images.%20IMASHRIMP%20incorporates%20two%20discrimination%0Amodules%2C%20based%20on%20a%20modified%20ResNet-50%20architecture%2C%20to%20classify%20images%20by%20the%0Apoint%20of%20view%20and%20determine%20rostrum%20integrity.%20It%20is%20proposed%20a%20%22two-factor%0Aauthentication%20%28human%20and%20IA%29%22%20system%2C%20it%20reduces%20human%20error%20in%20view%0Aclassification%20from%200.97%25%20to%200%25%20and%20in%20rostrum%20detection%20from%2012.46%25%20to%203.64%25.%0AAdditionally%2C%20a%20pose%20estimation%20module%20was%20adapted%20from%20VitPose%20to%20predict%2023%0Akey%20points%20on%20the%20shrimp%27s%20skeleton%2C%20with%20separate%20networks%20for%20lateral%20and%0Adorsal%20views.%20A%20morphological%20regression%20module%2C%20using%20a%20Support%20Vector%20Machine%0A%28SVM%29%20model%2C%20was%20integrated%20to%20convert%20pixel%20measurements%20to%20centimeter%20units.%0AExperimental%20results%20show%20that%20the%20system%20effectively%20reduces%20human%20error%2C%0Aachieving%20a%20mean%20average%20precision%20%28mAP%29%20of%2097.94%25%20for%20pose%20estimation%20and%20a%0Apixel-to-centimeter%20conversion%20error%20of%200.07%20%28%2B/-%200.1%29%20cm.%20IMASHRIMP%0Ademonstrates%20the%20potential%20to%20automate%20and%20accelerate%20shrimp%20morphological%0Aanalysis%2C%20enhancing%20the%20efficiency%20of%20genetic%20selection%20and%20contributing%20to%0Amore%20sustainable%20aquaculture%20practices.The%20code%20are%20available%20at%0Ahttps%3A//github.com/AbiamRemacheGonzalez/ImaShrimp-public%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02519v1&entry.124074799=Read"},
{"title": "A Late Collaborative Perception Framework for 3D Multi-Object and\n  Multi-Source Association and Fusion", "author": "Maryem Fadili and Mohamed Anis Ghaoui and Louis Lecrosnier and Steve Pechberti and Redouane Khemmar", "abstract": "  In autonomous driving, recent research has increasingly focused on\ncollaborative perception based on deep learning to overcome the limitations of\nindividual perception systems. Although these methods achieve high accuracy,\nthey rely on high communication bandwidth and require unrestricted access to\neach agent's object detection model architecture and parameters. These\nconstraints pose challenges real-world autonomous driving scenarios, where\ncommunication limitations and the need to safeguard proprietary models hinder\npractical implementation. To address this issue, we introduce a novel late\ncollaborative framework for 3D multi-source and multi-object fusion, which\noperates solely on shared 3D bounding box attributes-category, size, position,\nand orientation-without necessitating direct access to detection models. Our\nframework establishes a new state-of-the-art in late fusion, achieving up to\nfive times lower position error compared to existing methods. Additionally, it\nreduces scale error by a factor of 7.5 and orientation error by half, all while\nmaintaining perfect 100% precision and recall when fusing detections from\nheterogeneous perception systems. These results highlight the effectiveness of\nour approach in addressing real-world collaborative perception challenges,\nsetting a new benchmark for efficient and scalable multi-agent fusion.\n", "link": "http://arxiv.org/abs/2507.02430v1", "date": "2025-07-03", "relevancy": 2.3821, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6069}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5926}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Late%20Collaborative%20Perception%20Framework%20for%203D%20Multi-Object%20and%0A%20%20Multi-Source%20Association%20and%20Fusion&body=Title%3A%20A%20Late%20Collaborative%20Perception%20Framework%20for%203D%20Multi-Object%20and%0A%20%20Multi-Source%20Association%20and%20Fusion%0AAuthor%3A%20Maryem%20Fadili%20and%20Mohamed%20Anis%20Ghaoui%20and%20Louis%20Lecrosnier%20and%20Steve%20Pechberti%20and%20Redouane%20Khemmar%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20recent%20research%20has%20increasingly%20focused%20on%0Acollaborative%20perception%20based%20on%20deep%20learning%20to%20overcome%20the%20limitations%20of%0Aindividual%20perception%20systems.%20Although%20these%20methods%20achieve%20high%20accuracy%2C%0Athey%20rely%20on%20high%20communication%20bandwidth%20and%20require%20unrestricted%20access%20to%0Aeach%20agent%27s%20object%20detection%20model%20architecture%20and%20parameters.%20These%0Aconstraints%20pose%20challenges%20real-world%20autonomous%20driving%20scenarios%2C%20where%0Acommunication%20limitations%20and%20the%20need%20to%20safeguard%20proprietary%20models%20hinder%0Apractical%20implementation.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20late%0Acollaborative%20framework%20for%203D%20multi-source%20and%20multi-object%20fusion%2C%20which%0Aoperates%20solely%20on%20shared%203D%20bounding%20box%20attributes-category%2C%20size%2C%20position%2C%0Aand%20orientation-without%20necessitating%20direct%20access%20to%20detection%20models.%20Our%0Aframework%20establishes%20a%20new%20state-of-the-art%20in%20late%20fusion%2C%20achieving%20up%20to%0Afive%20times%20lower%20position%20error%20compared%20to%20existing%20methods.%20Additionally%2C%20it%0Areduces%20scale%20error%20by%20a%20factor%20of%207.5%20and%20orientation%20error%20by%20half%2C%20all%20while%0Amaintaining%20perfect%20100%25%20precision%20and%20recall%20when%20fusing%20detections%20from%0Aheterogeneous%20perception%20systems.%20These%20results%20highlight%20the%20effectiveness%20of%0Aour%20approach%20in%20addressing%20real-world%20collaborative%20perception%20challenges%2C%0Asetting%20a%20new%20benchmark%20for%20efficient%20and%20scalable%20multi-agent%20fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Late%2520Collaborative%2520Perception%2520Framework%2520for%25203D%2520Multi-Object%2520and%250A%2520%2520Multi-Source%2520Association%2520and%2520Fusion%26entry.906535625%3DMaryem%2520Fadili%2520and%2520Mohamed%2520Anis%2520Ghaoui%2520and%2520Louis%2520Lecrosnier%2520and%2520Steve%2520Pechberti%2520and%2520Redouane%2520Khemmar%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520recent%2520research%2520has%2520increasingly%2520focused%2520on%250Acollaborative%2520perception%2520based%2520on%2520deep%2520learning%2520to%2520overcome%2520the%2520limitations%2520of%250Aindividual%2520perception%2520systems.%2520Although%2520these%2520methods%2520achieve%2520high%2520accuracy%252C%250Athey%2520rely%2520on%2520high%2520communication%2520bandwidth%2520and%2520require%2520unrestricted%2520access%2520to%250Aeach%2520agent%2527s%2520object%2520detection%2520model%2520architecture%2520and%2520parameters.%2520These%250Aconstraints%2520pose%2520challenges%2520real-world%2520autonomous%2520driving%2520scenarios%252C%2520where%250Acommunication%2520limitations%2520and%2520the%2520need%2520to%2520safeguard%2520proprietary%2520models%2520hinder%250Apractical%2520implementation.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520late%250Acollaborative%2520framework%2520for%25203D%2520multi-source%2520and%2520multi-object%2520fusion%252C%2520which%250Aoperates%2520solely%2520on%2520shared%25203D%2520bounding%2520box%2520attributes-category%252C%2520size%252C%2520position%252C%250Aand%2520orientation-without%2520necessitating%2520direct%2520access%2520to%2520detection%2520models.%2520Our%250Aframework%2520establishes%2520a%2520new%2520state-of-the-art%2520in%2520late%2520fusion%252C%2520achieving%2520up%2520to%250Afive%2520times%2520lower%2520position%2520error%2520compared%2520to%2520existing%2520methods.%2520Additionally%252C%2520it%250Areduces%2520scale%2520error%2520by%2520a%2520factor%2520of%25207.5%2520and%2520orientation%2520error%2520by%2520half%252C%2520all%2520while%250Amaintaining%2520perfect%2520100%2525%2520precision%2520and%2520recall%2520when%2520fusing%2520detections%2520from%250Aheterogeneous%2520perception%2520systems.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520of%250Aour%2520approach%2520in%2520addressing%2520real-world%2520collaborative%2520perception%2520challenges%252C%250Asetting%2520a%2520new%2520benchmark%2520for%2520efficient%2520and%2520scalable%2520multi-agent%2520fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Late%20Collaborative%20Perception%20Framework%20for%203D%20Multi-Object%20and%0A%20%20Multi-Source%20Association%20and%20Fusion&entry.906535625=Maryem%20Fadili%20and%20Mohamed%20Anis%20Ghaoui%20and%20Louis%20Lecrosnier%20and%20Steve%20Pechberti%20and%20Redouane%20Khemmar&entry.1292438233=%20%20In%20autonomous%20driving%2C%20recent%20research%20has%20increasingly%20focused%20on%0Acollaborative%20perception%20based%20on%20deep%20learning%20to%20overcome%20the%20limitations%20of%0Aindividual%20perception%20systems.%20Although%20these%20methods%20achieve%20high%20accuracy%2C%0Athey%20rely%20on%20high%20communication%20bandwidth%20and%20require%20unrestricted%20access%20to%0Aeach%20agent%27s%20object%20detection%20model%20architecture%20and%20parameters.%20These%0Aconstraints%20pose%20challenges%20real-world%20autonomous%20driving%20scenarios%2C%20where%0Acommunication%20limitations%20and%20the%20need%20to%20safeguard%20proprietary%20models%20hinder%0Apractical%20implementation.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20late%0Acollaborative%20framework%20for%203D%20multi-source%20and%20multi-object%20fusion%2C%20which%0Aoperates%20solely%20on%20shared%203D%20bounding%20box%20attributes-category%2C%20size%2C%20position%2C%0Aand%20orientation-without%20necessitating%20direct%20access%20to%20detection%20models.%20Our%0Aframework%20establishes%20a%20new%20state-of-the-art%20in%20late%20fusion%2C%20achieving%20up%20to%0Afive%20times%20lower%20position%20error%20compared%20to%20existing%20methods.%20Additionally%2C%20it%0Areduces%20scale%20error%20by%20a%20factor%20of%207.5%20and%20orientation%20error%20by%20half%2C%20all%20while%0Amaintaining%20perfect%20100%25%20precision%20and%20recall%20when%20fusing%20detections%20from%0Aheterogeneous%20perception%20systems.%20These%20results%20highlight%20the%20effectiveness%20of%0Aour%20approach%20in%20addressing%20real-world%20collaborative%20perception%20challenges%2C%0Asetting%20a%20new%20benchmark%20for%20efficient%20and%20scalable%20multi-agent%20fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02430v1&entry.124074799=Read"},
{"title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention\n  Diffusion Network", "author": "Ying Yu and Hang Xiao and Siyao Li and Jiarui Li and Haotian Tang and Hanyu Liu and Chao Li", "abstract": "  The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.\n", "link": "http://arxiv.org/abs/2507.02827v1", "date": "2025-07-03", "relevancy": 2.3692, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6047}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5858}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USAD%3A%20An%20Unsupervised%20Data%20Augmentation%20Spatio-Temporal%20Attention%0A%20%20Diffusion%20Network&body=Title%3A%20USAD%3A%20An%20Unsupervised%20Data%20Augmentation%20Spatio-Temporal%20Attention%0A%20%20Diffusion%20Network%0AAuthor%3A%20Ying%20Yu%20and%20Hang%20Xiao%20and%20Siyao%20Li%20and%20Jiarui%20Li%20and%20Haotian%20Tang%20and%20Hanyu%20Liu%20and%20Chao%20Li%0AAbstract%3A%20%20%20The%20primary%20objective%20of%20human%20activity%20recognition%20%28HAR%29%20is%20to%20infer%20ongoing%0Ahuman%20actions%20from%20sensor%20data%2C%20a%20task%20that%20finds%20broad%20applications%20in%20health%0Amonitoring%2C%20safety%20protection%2C%20and%20sports%20analysis.%20Despite%20proliferating%0Aresearch%2C%20HAR%20still%20faces%20key%20challenges%2C%20including%20the%20scarcity%20of%20labeled%0Asamples%20for%20rare%20activities%2C%20insufficient%20extraction%20of%20high-level%20features%2C%0Aand%20suboptimal%20model%20performance%20on%20lightweight%20devices.%20To%20address%20these%0Aissues%2C%20this%20paper%20proposes%20a%20comprehensive%20optimization%20approach%20centered%20on%0Amulti-attention%20interaction%20mechanisms.%20First%2C%20an%20unsupervised%2C%0Astatistics-guided%20diffusion%20model%20is%20employed%20to%20perform%20data%20augmentation%2C%0Athereby%20alleviating%20the%20problems%20of%20labeled%20data%20scarcity%20and%20severe%20class%0Aimbalance.%20Second%2C%20a%20multi-branch%20spatio-temporal%20interaction%20network%20is%0Adesigned%2C%20which%20captures%20multi-scale%20features%20of%20sequential%20data%20through%0Aparallel%20residual%20branches%20with%203%2A3%2C%205%2A5%2C%20and%207%2A7%20convolutional%20kernels.%0ASimultaneously%2C%20temporal%20attention%20mechanisms%20are%20incorporated%20to%20identify%0Acritical%20time%20points%2C%20while%20spatial%20attention%20enhances%20inter-sensor%0Ainteractions.%20A%20cross-branch%20feature%20fusion%20unit%20is%20further%20introduced%20to%0Aimprove%20the%20overall%20feature%20representation%20capability.%20Finally%2C%20an%20adaptive%0Amulti-loss%20function%20fusion%20strategy%20is%20integrated%2C%20allowing%20for%20dynamic%0Aadjustment%20of%20loss%20weights%20and%20overall%20model%20optimization.%20Experimental%20results%0Aon%20three%20public%20datasets%2C%20WISDM%2C%20PAMAP2%2C%20and%20OPPORTUNITY%2C%20demonstrate%20that%20the%0Aproposed%20unsupervised%20data%20augmentation%20spatio-temporal%20attention%20diffusion%0Anetwork%20%28USAD%29%20achieves%20accuracies%20of%2098.84%25%2C%2093.81%25%2C%20and%2080.92%25%20respectively%2C%0Asignificantly%20outperforming%20existing%20approaches.%20Furthermore%2C%20practical%0Adeployment%20on%20embedded%20devices%20verifies%20the%20efficiency%20and%20feasibility%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSAD%253A%2520An%2520Unsupervised%2520Data%2520Augmentation%2520Spatio-Temporal%2520Attention%250A%2520%2520Diffusion%2520Network%26entry.906535625%3DYing%2520Yu%2520and%2520Hang%2520Xiao%2520and%2520Siyao%2520Li%2520and%2520Jiarui%2520Li%2520and%2520Haotian%2520Tang%2520and%2520Hanyu%2520Liu%2520and%2520Chao%2520Li%26entry.1292438233%3D%2520%2520The%2520primary%2520objective%2520of%2520human%2520activity%2520recognition%2520%2528HAR%2529%2520is%2520to%2520infer%2520ongoing%250Ahuman%2520actions%2520from%2520sensor%2520data%252C%2520a%2520task%2520that%2520finds%2520broad%2520applications%2520in%2520health%250Amonitoring%252C%2520safety%2520protection%252C%2520and%2520sports%2520analysis.%2520Despite%2520proliferating%250Aresearch%252C%2520HAR%2520still%2520faces%2520key%2520challenges%252C%2520including%2520the%2520scarcity%2520of%2520labeled%250Asamples%2520for%2520rare%2520activities%252C%2520insufficient%2520extraction%2520of%2520high-level%2520features%252C%250Aand%2520suboptimal%2520model%2520performance%2520on%2520lightweight%2520devices.%2520To%2520address%2520these%250Aissues%252C%2520this%2520paper%2520proposes%2520a%2520comprehensive%2520optimization%2520approach%2520centered%2520on%250Amulti-attention%2520interaction%2520mechanisms.%2520First%252C%2520an%2520unsupervised%252C%250Astatistics-guided%2520diffusion%2520model%2520is%2520employed%2520to%2520perform%2520data%2520augmentation%252C%250Athereby%2520alleviating%2520the%2520problems%2520of%2520labeled%2520data%2520scarcity%2520and%2520severe%2520class%250Aimbalance.%2520Second%252C%2520a%2520multi-branch%2520spatio-temporal%2520interaction%2520network%2520is%250Adesigned%252C%2520which%2520captures%2520multi-scale%2520features%2520of%2520sequential%2520data%2520through%250Aparallel%2520residual%2520branches%2520with%25203%252A3%252C%25205%252A5%252C%2520and%25207%252A7%2520convolutional%2520kernels.%250ASimultaneously%252C%2520temporal%2520attention%2520mechanisms%2520are%2520incorporated%2520to%2520identify%250Acritical%2520time%2520points%252C%2520while%2520spatial%2520attention%2520enhances%2520inter-sensor%250Ainteractions.%2520A%2520cross-branch%2520feature%2520fusion%2520unit%2520is%2520further%2520introduced%2520to%250Aimprove%2520the%2520overall%2520feature%2520representation%2520capability.%2520Finally%252C%2520an%2520adaptive%250Amulti-loss%2520function%2520fusion%2520strategy%2520is%2520integrated%252C%2520allowing%2520for%2520dynamic%250Aadjustment%2520of%2520loss%2520weights%2520and%2520overall%2520model%2520optimization.%2520Experimental%2520results%250Aon%2520three%2520public%2520datasets%252C%2520WISDM%252C%2520PAMAP2%252C%2520and%2520OPPORTUNITY%252C%2520demonstrate%2520that%2520the%250Aproposed%2520unsupervised%2520data%2520augmentation%2520spatio-temporal%2520attention%2520diffusion%250Anetwork%2520%2528USAD%2529%2520achieves%2520accuracies%2520of%252098.84%2525%252C%252093.81%2525%252C%2520and%252080.92%2525%2520respectively%252C%250Asignificantly%2520outperforming%2520existing%2520approaches.%2520Furthermore%252C%2520practical%250Adeployment%2520on%2520embedded%2520devices%2520verifies%2520the%2520efficiency%2520and%2520feasibility%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USAD%3A%20An%20Unsupervised%20Data%20Augmentation%20Spatio-Temporal%20Attention%0A%20%20Diffusion%20Network&entry.906535625=Ying%20Yu%20and%20Hang%20Xiao%20and%20Siyao%20Li%20and%20Jiarui%20Li%20and%20Haotian%20Tang%20and%20Hanyu%20Liu%20and%20Chao%20Li&entry.1292438233=%20%20The%20primary%20objective%20of%20human%20activity%20recognition%20%28HAR%29%20is%20to%20infer%20ongoing%0Ahuman%20actions%20from%20sensor%20data%2C%20a%20task%20that%20finds%20broad%20applications%20in%20health%0Amonitoring%2C%20safety%20protection%2C%20and%20sports%20analysis.%20Despite%20proliferating%0Aresearch%2C%20HAR%20still%20faces%20key%20challenges%2C%20including%20the%20scarcity%20of%20labeled%0Asamples%20for%20rare%20activities%2C%20insufficient%20extraction%20of%20high-level%20features%2C%0Aand%20suboptimal%20model%20performance%20on%20lightweight%20devices.%20To%20address%20these%0Aissues%2C%20this%20paper%20proposes%20a%20comprehensive%20optimization%20approach%20centered%20on%0Amulti-attention%20interaction%20mechanisms.%20First%2C%20an%20unsupervised%2C%0Astatistics-guided%20diffusion%20model%20is%20employed%20to%20perform%20data%20augmentation%2C%0Athereby%20alleviating%20the%20problems%20of%20labeled%20data%20scarcity%20and%20severe%20class%0Aimbalance.%20Second%2C%20a%20multi-branch%20spatio-temporal%20interaction%20network%20is%0Adesigned%2C%20which%20captures%20multi-scale%20features%20of%20sequential%20data%20through%0Aparallel%20residual%20branches%20with%203%2A3%2C%205%2A5%2C%20and%207%2A7%20convolutional%20kernels.%0ASimultaneously%2C%20temporal%20attention%20mechanisms%20are%20incorporated%20to%20identify%0Acritical%20time%20points%2C%20while%20spatial%20attention%20enhances%20inter-sensor%0Ainteractions.%20A%20cross-branch%20feature%20fusion%20unit%20is%20further%20introduced%20to%0Aimprove%20the%20overall%20feature%20representation%20capability.%20Finally%2C%20an%20adaptive%0Amulti-loss%20function%20fusion%20strategy%20is%20integrated%2C%20allowing%20for%20dynamic%0Aadjustment%20of%20loss%20weights%20and%20overall%20model%20optimization.%20Experimental%20results%0Aon%20three%20public%20datasets%2C%20WISDM%2C%20PAMAP2%2C%20and%20OPPORTUNITY%2C%20demonstrate%20that%20the%0Aproposed%20unsupervised%20data%20augmentation%20spatio-temporal%20attention%20diffusion%0Anetwork%20%28USAD%29%20achieves%20accuracies%20of%2098.84%25%2C%2093.81%25%2C%20and%2080.92%25%20respectively%2C%0Asignificantly%20outperforming%20existing%20approaches.%20Furthermore%2C%20practical%0Adeployment%20on%20embedded%20devices%20verifies%20the%20efficiency%20and%20feasibility%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02827v1&entry.124074799=Read"},
{"title": "ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force\n  Control in VR Hand Manipulation", "author": "DongHeun Han and Byungmin Kim and RoUn Lee and KyeongMin Kim and Hyoseok Hwang and HyeongYeop Kang", "abstract": "  Realistic Hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. Demo videos are available as supplementary material and the code is\nprovided at https://han-dongheun.github.io/ForceGrip.\n", "link": "http://arxiv.org/abs/2503.08061v4", "date": "2025-07-03", "relevancy": 2.3625, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6244}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ForceGrip%3A%20Reference-Free%20Curriculum%20Learning%20for%20Realistic%20Grip%20Force%0A%20%20Control%20in%20VR%20Hand%20Manipulation&body=Title%3A%20ForceGrip%3A%20Reference-Free%20Curriculum%20Learning%20for%20Realistic%20Grip%20Force%0A%20%20Control%20in%20VR%20Hand%20Manipulation%0AAuthor%3A%20DongHeun%20Han%20and%20Byungmin%20Kim%20and%20RoUn%20Lee%20and%20KyeongMin%20Kim%20and%20Hyoseok%20Hwang%20and%20HyeongYeop%20Kang%0AAbstract%3A%20%20%20Realistic%20Hand%20manipulation%20is%20a%20key%20component%20of%20immersive%20virtual%20reality%0A%28VR%29%2C%20yet%20existing%20methods%20often%20rely%20on%20kinematic%20approach%20or%20motion-capture%0Adatasets%20that%20omit%20crucial%20physical%20attributes%20such%20as%20contact%20forces%20and%0Afinger%20torques.%20Consequently%2C%20these%20approaches%20prioritize%20tight%2C%0Aone-size-fits-all%20grips%20rather%20than%20reflecting%20users%27%20intended%20force%20levels.%20We%0Apresent%20ForceGrip%2C%20a%20deep%20learning%20agent%20that%20synthesizes%20realistic%20hand%0Amanipulation%20motions%2C%20faithfully%20reflecting%20the%20user%27s%20grip%20force%20intention.%0AInstead%20of%20mimicking%20predefined%20motion%20datasets%2C%20ForceGrip%20uses%20generated%0Atraining%20scenarios-randomizing%20object%20shapes%2C%20wrist%20movements%2C%20and%20trigger%0Ainput%20flows-to%20challenge%20the%20agent%20with%20a%20broad%20spectrum%20of%20physical%0Ainteractions.%20To%20effectively%20learn%20from%20these%20complex%20tasks%2C%20we%20employ%20a%0Athree-phase%20curriculum%20learning%20framework%20comprising%20Finger%20Positioning%2C%0AIntention%20Adaptation%2C%20and%20Dynamic%20Stabilization.%20This%20progressive%20strategy%0Aensures%20stable%20hand-object%20contact%2C%20adaptive%20force%20control%20based%20on%20user%0Ainputs%2C%20and%20robust%20handling%20under%20dynamic%20conditions.%20Additionally%2C%20a%20proximity%0Areward%20function%20enhances%20natural%20finger%20motions%20and%20accelerates%20training%0Aconvergence.%20Quantitative%20and%20qualitative%20evaluations%20reveal%20ForceGrip%27s%0Asuperior%20force%20controllability%20and%20plausibility%20compared%20to%20state-of-the-art%0Amethods.%20Demo%20videos%20are%20available%20as%20supplementary%20material%20and%20the%20code%20is%0Aprovided%20at%20https%3A//han-dongheun.github.io/ForceGrip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08061v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForceGrip%253A%2520Reference-Free%2520Curriculum%2520Learning%2520for%2520Realistic%2520Grip%2520Force%250A%2520%2520Control%2520in%2520VR%2520Hand%2520Manipulation%26entry.906535625%3DDongHeun%2520Han%2520and%2520Byungmin%2520Kim%2520and%2520RoUn%2520Lee%2520and%2520KyeongMin%2520Kim%2520and%2520Hyoseok%2520Hwang%2520and%2520HyeongYeop%2520Kang%26entry.1292438233%3D%2520%2520Realistic%2520Hand%2520manipulation%2520is%2520a%2520key%2520component%2520of%2520immersive%2520virtual%2520reality%250A%2528VR%2529%252C%2520yet%2520existing%2520methods%2520often%2520rely%2520on%2520kinematic%2520approach%2520or%2520motion-capture%250Adatasets%2520that%2520omit%2520crucial%2520physical%2520attributes%2520such%2520as%2520contact%2520forces%2520and%250Afinger%2520torques.%2520Consequently%252C%2520these%2520approaches%2520prioritize%2520tight%252C%250Aone-size-fits-all%2520grips%2520rather%2520than%2520reflecting%2520users%2527%2520intended%2520force%2520levels.%2520We%250Apresent%2520ForceGrip%252C%2520a%2520deep%2520learning%2520agent%2520that%2520synthesizes%2520realistic%2520hand%250Amanipulation%2520motions%252C%2520faithfully%2520reflecting%2520the%2520user%2527s%2520grip%2520force%2520intention.%250AInstead%2520of%2520mimicking%2520predefined%2520motion%2520datasets%252C%2520ForceGrip%2520uses%2520generated%250Atraining%2520scenarios-randomizing%2520object%2520shapes%252C%2520wrist%2520movements%252C%2520and%2520trigger%250Ainput%2520flows-to%2520challenge%2520the%2520agent%2520with%2520a%2520broad%2520spectrum%2520of%2520physical%250Ainteractions.%2520To%2520effectively%2520learn%2520from%2520these%2520complex%2520tasks%252C%2520we%2520employ%2520a%250Athree-phase%2520curriculum%2520learning%2520framework%2520comprising%2520Finger%2520Positioning%252C%250AIntention%2520Adaptation%252C%2520and%2520Dynamic%2520Stabilization.%2520This%2520progressive%2520strategy%250Aensures%2520stable%2520hand-object%2520contact%252C%2520adaptive%2520force%2520control%2520based%2520on%2520user%250Ainputs%252C%2520and%2520robust%2520handling%2520under%2520dynamic%2520conditions.%2520Additionally%252C%2520a%2520proximity%250Areward%2520function%2520enhances%2520natural%2520finger%2520motions%2520and%2520accelerates%2520training%250Aconvergence.%2520Quantitative%2520and%2520qualitative%2520evaluations%2520reveal%2520ForceGrip%2527s%250Asuperior%2520force%2520controllability%2520and%2520plausibility%2520compared%2520to%2520state-of-the-art%250Amethods.%2520Demo%2520videos%2520are%2520available%2520as%2520supplementary%2520material%2520and%2520the%2520code%2520is%250Aprovided%2520at%2520https%253A//han-dongheun.github.io/ForceGrip.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08061v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ForceGrip%3A%20Reference-Free%20Curriculum%20Learning%20for%20Realistic%20Grip%20Force%0A%20%20Control%20in%20VR%20Hand%20Manipulation&entry.906535625=DongHeun%20Han%20and%20Byungmin%20Kim%20and%20RoUn%20Lee%20and%20KyeongMin%20Kim%20and%20Hyoseok%20Hwang%20and%20HyeongYeop%20Kang&entry.1292438233=%20%20Realistic%20Hand%20manipulation%20is%20a%20key%20component%20of%20immersive%20virtual%20reality%0A%28VR%29%2C%20yet%20existing%20methods%20often%20rely%20on%20kinematic%20approach%20or%20motion-capture%0Adatasets%20that%20omit%20crucial%20physical%20attributes%20such%20as%20contact%20forces%20and%0Afinger%20torques.%20Consequently%2C%20these%20approaches%20prioritize%20tight%2C%0Aone-size-fits-all%20grips%20rather%20than%20reflecting%20users%27%20intended%20force%20levels.%20We%0Apresent%20ForceGrip%2C%20a%20deep%20learning%20agent%20that%20synthesizes%20realistic%20hand%0Amanipulation%20motions%2C%20faithfully%20reflecting%20the%20user%27s%20grip%20force%20intention.%0AInstead%20of%20mimicking%20predefined%20motion%20datasets%2C%20ForceGrip%20uses%20generated%0Atraining%20scenarios-randomizing%20object%20shapes%2C%20wrist%20movements%2C%20and%20trigger%0Ainput%20flows-to%20challenge%20the%20agent%20with%20a%20broad%20spectrum%20of%20physical%0Ainteractions.%20To%20effectively%20learn%20from%20these%20complex%20tasks%2C%20we%20employ%20a%0Athree-phase%20curriculum%20learning%20framework%20comprising%20Finger%20Positioning%2C%0AIntention%20Adaptation%2C%20and%20Dynamic%20Stabilization.%20This%20progressive%20strategy%0Aensures%20stable%20hand-object%20contact%2C%20adaptive%20force%20control%20based%20on%20user%0Ainputs%2C%20and%20robust%20handling%20under%20dynamic%20conditions.%20Additionally%2C%20a%20proximity%0Areward%20function%20enhances%20natural%20finger%20motions%20and%20accelerates%20training%0Aconvergence.%20Quantitative%20and%20qualitative%20evaluations%20reveal%20ForceGrip%27s%0Asuperior%20force%20controllability%20and%20plausibility%20compared%20to%20state-of-the-art%0Amethods.%20Demo%20videos%20are%20available%20as%20supplementary%20material%20and%20the%20code%20is%0Aprovided%20at%20https%3A//han-dongheun.github.io/ForceGrip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08061v4&entry.124074799=Read"},
{"title": "Structure-aware Semantic Discrepancy and Consistency for 3D Medical\n  Image Self-supervised Learning", "author": "Tan Pan and Zhaorui Tan and Kaiyu Guo and Dongli Xu and Weidi Xu and Chen Jiang and Xin Guo and Yuan Qi and Yuan Cheng", "abstract": "  3D medical image self-supervised learning (mSSL) holds great promise for\nmedical analysis. Effectively supporting broader applications requires\nconsidering anatomical structure variations in location, scale, and morphology,\nwhich are crucial for capturing meaningful distinctions. However, previous mSSL\nmethods partition images with fixed-size patches, often ignoring the structure\nvariations. In this work, we introduce a novel perspective on 3D medical images\nwith the goal of learning structure-aware representations. We assume that\npatches within the same structure share the same semantics (semantic\nconsistency) while those from different structures exhibit distinct semantics\n(semantic discrepancy). Based on this assumption, we propose an mSSL framework\nnamed $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency\nin two steps. First, $S^2DC$ enforces distinct representations for different\npatches to increase semantic discrepancy by leveraging an optimal transport\nstrategy. Second, $S^2DC$ advances semantic consistency at the structural level\nbased on neighborhood similarity distribution. By bridging patch-level and\nstructure-level representations, $S^2DC$ achieves structure-aware\nrepresentations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3\nmodalities, our proposed method consistently outperforms the state-of-the-art\nmethods in mSSL.\n", "link": "http://arxiv.org/abs/2507.02581v1", "date": "2025-07-03", "relevancy": 2.3624, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.61}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5772}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%20for%203D%20Medical%0A%20%20Image%20Self-supervised%20Learning&body=Title%3A%20Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%20for%203D%20Medical%0A%20%20Image%20Self-supervised%20Learning%0AAuthor%3A%20Tan%20Pan%20and%20Zhaorui%20Tan%20and%20Kaiyu%20Guo%20and%20Dongli%20Xu%20and%20Weidi%20Xu%20and%20Chen%20Jiang%20and%20Xin%20Guo%20and%20Yuan%20Qi%20and%20Yuan%20Cheng%0AAbstract%3A%20%20%203D%20medical%20image%20self-supervised%20learning%20%28mSSL%29%20holds%20great%20promise%20for%0Amedical%20analysis.%20Effectively%20supporting%20broader%20applications%20requires%0Aconsidering%20anatomical%20structure%20variations%20in%20location%2C%20scale%2C%20and%20morphology%2C%0Awhich%20are%20crucial%20for%20capturing%20meaningful%20distinctions.%20However%2C%20previous%20mSSL%0Amethods%20partition%20images%20with%20fixed-size%20patches%2C%20often%20ignoring%20the%20structure%0Avariations.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20perspective%20on%203D%20medical%20images%0Awith%20the%20goal%20of%20learning%20structure-aware%20representations.%20We%20assume%20that%0Apatches%20within%20the%20same%20structure%20share%20the%20same%20semantics%20%28semantic%0Aconsistency%29%20while%20those%20from%20different%20structures%20exhibit%20distinct%20semantics%0A%28semantic%20discrepancy%29.%20Based%20on%20this%20assumption%2C%20we%20propose%20an%20mSSL%20framework%0Anamed%20%24S%5E2DC%24%2C%20achieving%20Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%0Ain%20two%20steps.%20First%2C%20%24S%5E2DC%24%20enforces%20distinct%20representations%20for%20different%0Apatches%20to%20increase%20semantic%20discrepancy%20by%20leveraging%20an%20optimal%20transport%0Astrategy.%20Second%2C%20%24S%5E2DC%24%20advances%20semantic%20consistency%20at%20the%20structural%20level%0Abased%20on%20neighborhood%20similarity%20distribution.%20By%20bridging%20patch-level%20and%0Astructure-level%20representations%2C%20%24S%5E2DC%24%20achieves%20structure-aware%0Arepresentations.%20Thoroughly%20evaluated%20across%2010%20datasets%2C%204%20tasks%2C%20and%203%0Amodalities%2C%20our%20proposed%20method%20consistently%20outperforms%20the%20state-of-the-art%0Amethods%20in%20mSSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-aware%2520Semantic%2520Discrepancy%2520and%2520Consistency%2520for%25203D%2520Medical%250A%2520%2520Image%2520Self-supervised%2520Learning%26entry.906535625%3DTan%2520Pan%2520and%2520Zhaorui%2520Tan%2520and%2520Kaiyu%2520Guo%2520and%2520Dongli%2520Xu%2520and%2520Weidi%2520Xu%2520and%2520Chen%2520Jiang%2520and%2520Xin%2520Guo%2520and%2520Yuan%2520Qi%2520and%2520Yuan%2520Cheng%26entry.1292438233%3D%2520%25203D%2520medical%2520image%2520self-supervised%2520learning%2520%2528mSSL%2529%2520holds%2520great%2520promise%2520for%250Amedical%2520analysis.%2520Effectively%2520supporting%2520broader%2520applications%2520requires%250Aconsidering%2520anatomical%2520structure%2520variations%2520in%2520location%252C%2520scale%252C%2520and%2520morphology%252C%250Awhich%2520are%2520crucial%2520for%2520capturing%2520meaningful%2520distinctions.%2520However%252C%2520previous%2520mSSL%250Amethods%2520partition%2520images%2520with%2520fixed-size%2520patches%252C%2520often%2520ignoring%2520the%2520structure%250Avariations.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520perspective%2520on%25203D%2520medical%2520images%250Awith%2520the%2520goal%2520of%2520learning%2520structure-aware%2520representations.%2520We%2520assume%2520that%250Apatches%2520within%2520the%2520same%2520structure%2520share%2520the%2520same%2520semantics%2520%2528semantic%250Aconsistency%2529%2520while%2520those%2520from%2520different%2520structures%2520exhibit%2520distinct%2520semantics%250A%2528semantic%2520discrepancy%2529.%2520Based%2520on%2520this%2520assumption%252C%2520we%2520propose%2520an%2520mSSL%2520framework%250Anamed%2520%2524S%255E2DC%2524%252C%2520achieving%2520Structure-aware%2520Semantic%2520Discrepancy%2520and%2520Consistency%250Ain%2520two%2520steps.%2520First%252C%2520%2524S%255E2DC%2524%2520enforces%2520distinct%2520representations%2520for%2520different%250Apatches%2520to%2520increase%2520semantic%2520discrepancy%2520by%2520leveraging%2520an%2520optimal%2520transport%250Astrategy.%2520Second%252C%2520%2524S%255E2DC%2524%2520advances%2520semantic%2520consistency%2520at%2520the%2520structural%2520level%250Abased%2520on%2520neighborhood%2520similarity%2520distribution.%2520By%2520bridging%2520patch-level%2520and%250Astructure-level%2520representations%252C%2520%2524S%255E2DC%2524%2520achieves%2520structure-aware%250Arepresentations.%2520Thoroughly%2520evaluated%2520across%252010%2520datasets%252C%25204%2520tasks%252C%2520and%25203%250Amodalities%252C%2520our%2520proposed%2520method%2520consistently%2520outperforms%2520the%2520state-of-the-art%250Amethods%2520in%2520mSSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%20for%203D%20Medical%0A%20%20Image%20Self-supervised%20Learning&entry.906535625=Tan%20Pan%20and%20Zhaorui%20Tan%20and%20Kaiyu%20Guo%20and%20Dongli%20Xu%20and%20Weidi%20Xu%20and%20Chen%20Jiang%20and%20Xin%20Guo%20and%20Yuan%20Qi%20and%20Yuan%20Cheng&entry.1292438233=%20%203D%20medical%20image%20self-supervised%20learning%20%28mSSL%29%20holds%20great%20promise%20for%0Amedical%20analysis.%20Effectively%20supporting%20broader%20applications%20requires%0Aconsidering%20anatomical%20structure%20variations%20in%20location%2C%20scale%2C%20and%20morphology%2C%0Awhich%20are%20crucial%20for%20capturing%20meaningful%20distinctions.%20However%2C%20previous%20mSSL%0Amethods%20partition%20images%20with%20fixed-size%20patches%2C%20often%20ignoring%20the%20structure%0Avariations.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20perspective%20on%203D%20medical%20images%0Awith%20the%20goal%20of%20learning%20structure-aware%20representations.%20We%20assume%20that%0Apatches%20within%20the%20same%20structure%20share%20the%20same%20semantics%20%28semantic%0Aconsistency%29%20while%20those%20from%20different%20structures%20exhibit%20distinct%20semantics%0A%28semantic%20discrepancy%29.%20Based%20on%20this%20assumption%2C%20we%20propose%20an%20mSSL%20framework%0Anamed%20%24S%5E2DC%24%2C%20achieving%20Structure-aware%20Semantic%20Discrepancy%20and%20Consistency%0Ain%20two%20steps.%20First%2C%20%24S%5E2DC%24%20enforces%20distinct%20representations%20for%20different%0Apatches%20to%20increase%20semantic%20discrepancy%20by%20leveraging%20an%20optimal%20transport%0Astrategy.%20Second%2C%20%24S%5E2DC%24%20advances%20semantic%20consistency%20at%20the%20structural%20level%0Abased%20on%20neighborhood%20similarity%20distribution.%20By%20bridging%20patch-level%20and%0Astructure-level%20representations%2C%20%24S%5E2DC%24%20achieves%20structure-aware%0Arepresentations.%20Thoroughly%20evaluated%20across%2010%20datasets%2C%204%20tasks%2C%20and%203%0Amodalities%2C%20our%20proposed%20method%20consistently%20outperforms%20the%20state-of-the-art%0Amethods%20in%20mSSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02581v1&entry.124074799=Read"},
{"title": "RefTok: Reference-Based Tokenization for Video Generation", "author": "Xiang Fan and Xiaohang Sun and Kushan Thakkar and Zhu Liu and Vimal Bhat and Ranjay Krishna and Xiang Hao", "abstract": "  Effectively handling temporal redundancy remains a key challenge in learning\nvideo models. Prevailing approaches often treat each set of frames\nindependently, failing to effectively capture the temporal dependencies and\nredundancies inherent in videos. To address this limitation, we introduce\nRefTok, a novel reference-based tokenization method capable of capturing\ncomplex temporal dynamics and contextual information. Our method encodes and\ndecodes sets of frames conditioned on an unquantized reference frame. When\ndecoded, RefTok preserves the continuity of motion and the appearance of\nobjects across frames. For example, RefTok retains facial details despite head\nmotion, reconstructs text correctly, preserves small patterns, and maintains\nthe legibility of handwriting from the context. Across 4 video datasets (K600,\nUCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms\ncurrent state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all\nevaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or\nhigher compression ratios. When a video generation model is trained using\nRefTok's latents on the BAIR Robot Pushing task, the generations not only\noutperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters,\nacross all generation metrics by an average of 27.9%.\n", "link": "http://arxiv.org/abs/2507.02862v1", "date": "2025-07-03", "relevancy": 2.3621, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5977}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5908}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefTok%3A%20Reference-Based%20Tokenization%20for%20Video%20Generation&body=Title%3A%20RefTok%3A%20Reference-Based%20Tokenization%20for%20Video%20Generation%0AAuthor%3A%20Xiang%20Fan%20and%20Xiaohang%20Sun%20and%20Kushan%20Thakkar%20and%20Zhu%20Liu%20and%20Vimal%20Bhat%20and%20Ranjay%20Krishna%20and%20Xiang%20Hao%0AAbstract%3A%20%20%20Effectively%20handling%20temporal%20redundancy%20remains%20a%20key%20challenge%20in%20learning%0Avideo%20models.%20Prevailing%20approaches%20often%20treat%20each%20set%20of%20frames%0Aindependently%2C%20failing%20to%20effectively%20capture%20the%20temporal%20dependencies%20and%0Aredundancies%20inherent%20in%20videos.%20To%20address%20this%20limitation%2C%20we%20introduce%0ARefTok%2C%20a%20novel%20reference-based%20tokenization%20method%20capable%20of%20capturing%0Acomplex%20temporal%20dynamics%20and%20contextual%20information.%20Our%20method%20encodes%20and%0Adecodes%20sets%20of%20frames%20conditioned%20on%20an%20unquantized%20reference%20frame.%20When%0Adecoded%2C%20RefTok%20preserves%20the%20continuity%20of%20motion%20and%20the%20appearance%20of%0Aobjects%20across%20frames.%20For%20example%2C%20RefTok%20retains%20facial%20details%20despite%20head%0Amotion%2C%20reconstructs%20text%20correctly%2C%20preserves%20small%20patterns%2C%20and%20maintains%0Athe%20legibility%20of%20handwriting%20from%20the%20context.%20Across%204%20video%20datasets%20%28K600%2C%0AUCF-101%2C%20BAIR%20Robot%20Pushing%2C%20and%20DAVIS%29%2C%20RefTok%20significantly%20outperforms%0Acurrent%20state-of-the-art%20tokenizers%20%28Cosmos%20and%20MAGVIT%29%20and%20improves%20all%0Aevaluated%20metrics%20%28PSNR%2C%20SSIM%2C%20LPIPS%29%20by%20an%20average%20of%2036.7%25%20at%20the%20same%20or%0Ahigher%20compression%20ratios.%20When%20a%20video%20generation%20model%20is%20trained%20using%0ARefTok%27s%20latents%20on%20the%20BAIR%20Robot%20Pushing%20task%2C%20the%20generations%20not%20only%0Aoutperform%20MAGVIT-B%20but%20the%20larger%20MAGVIT-L%2C%20which%20has%204x%20more%20parameters%2C%0Aacross%20all%20generation%20metrics%20by%20an%20average%20of%2027.9%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefTok%253A%2520Reference-Based%2520Tokenization%2520for%2520Video%2520Generation%26entry.906535625%3DXiang%2520Fan%2520and%2520Xiaohang%2520Sun%2520and%2520Kushan%2520Thakkar%2520and%2520Zhu%2520Liu%2520and%2520Vimal%2520Bhat%2520and%2520Ranjay%2520Krishna%2520and%2520Xiang%2520Hao%26entry.1292438233%3D%2520%2520Effectively%2520handling%2520temporal%2520redundancy%2520remains%2520a%2520key%2520challenge%2520in%2520learning%250Avideo%2520models.%2520Prevailing%2520approaches%2520often%2520treat%2520each%2520set%2520of%2520frames%250Aindependently%252C%2520failing%2520to%2520effectively%2520capture%2520the%2520temporal%2520dependencies%2520and%250Aredundancies%2520inherent%2520in%2520videos.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250ARefTok%252C%2520a%2520novel%2520reference-based%2520tokenization%2520method%2520capable%2520of%2520capturing%250Acomplex%2520temporal%2520dynamics%2520and%2520contextual%2520information.%2520Our%2520method%2520encodes%2520and%250Adecodes%2520sets%2520of%2520frames%2520conditioned%2520on%2520an%2520unquantized%2520reference%2520frame.%2520When%250Adecoded%252C%2520RefTok%2520preserves%2520the%2520continuity%2520of%2520motion%2520and%2520the%2520appearance%2520of%250Aobjects%2520across%2520frames.%2520For%2520example%252C%2520RefTok%2520retains%2520facial%2520details%2520despite%2520head%250Amotion%252C%2520reconstructs%2520text%2520correctly%252C%2520preserves%2520small%2520patterns%252C%2520and%2520maintains%250Athe%2520legibility%2520of%2520handwriting%2520from%2520the%2520context.%2520Across%25204%2520video%2520datasets%2520%2528K600%252C%250AUCF-101%252C%2520BAIR%2520Robot%2520Pushing%252C%2520and%2520DAVIS%2529%252C%2520RefTok%2520significantly%2520outperforms%250Acurrent%2520state-of-the-art%2520tokenizers%2520%2528Cosmos%2520and%2520MAGVIT%2529%2520and%2520improves%2520all%250Aevaluated%2520metrics%2520%2528PSNR%252C%2520SSIM%252C%2520LPIPS%2529%2520by%2520an%2520average%2520of%252036.7%2525%2520at%2520the%2520same%2520or%250Ahigher%2520compression%2520ratios.%2520When%2520a%2520video%2520generation%2520model%2520is%2520trained%2520using%250ARefTok%2527s%2520latents%2520on%2520the%2520BAIR%2520Robot%2520Pushing%2520task%252C%2520the%2520generations%2520not%2520only%250Aoutperform%2520MAGVIT-B%2520but%2520the%2520larger%2520MAGVIT-L%252C%2520which%2520has%25204x%2520more%2520parameters%252C%250Aacross%2520all%2520generation%2520metrics%2520by%2520an%2520average%2520of%252027.9%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefTok%3A%20Reference-Based%20Tokenization%20for%20Video%20Generation&entry.906535625=Xiang%20Fan%20and%20Xiaohang%20Sun%20and%20Kushan%20Thakkar%20and%20Zhu%20Liu%20and%20Vimal%20Bhat%20and%20Ranjay%20Krishna%20and%20Xiang%20Hao&entry.1292438233=%20%20Effectively%20handling%20temporal%20redundancy%20remains%20a%20key%20challenge%20in%20learning%0Avideo%20models.%20Prevailing%20approaches%20often%20treat%20each%20set%20of%20frames%0Aindependently%2C%20failing%20to%20effectively%20capture%20the%20temporal%20dependencies%20and%0Aredundancies%20inherent%20in%20videos.%20To%20address%20this%20limitation%2C%20we%20introduce%0ARefTok%2C%20a%20novel%20reference-based%20tokenization%20method%20capable%20of%20capturing%0Acomplex%20temporal%20dynamics%20and%20contextual%20information.%20Our%20method%20encodes%20and%0Adecodes%20sets%20of%20frames%20conditioned%20on%20an%20unquantized%20reference%20frame.%20When%0Adecoded%2C%20RefTok%20preserves%20the%20continuity%20of%20motion%20and%20the%20appearance%20of%0Aobjects%20across%20frames.%20For%20example%2C%20RefTok%20retains%20facial%20details%20despite%20head%0Amotion%2C%20reconstructs%20text%20correctly%2C%20preserves%20small%20patterns%2C%20and%20maintains%0Athe%20legibility%20of%20handwriting%20from%20the%20context.%20Across%204%20video%20datasets%20%28K600%2C%0AUCF-101%2C%20BAIR%20Robot%20Pushing%2C%20and%20DAVIS%29%2C%20RefTok%20significantly%20outperforms%0Acurrent%20state-of-the-art%20tokenizers%20%28Cosmos%20and%20MAGVIT%29%20and%20improves%20all%0Aevaluated%20metrics%20%28PSNR%2C%20SSIM%2C%20LPIPS%29%20by%20an%20average%20of%2036.7%25%20at%20the%20same%20or%0Ahigher%20compression%20ratios.%20When%20a%20video%20generation%20model%20is%20trained%20using%0ARefTok%27s%20latents%20on%20the%20BAIR%20Robot%20Pushing%20task%2C%20the%20generations%20not%20only%0Aoutperform%20MAGVIT-B%20but%20the%20larger%20MAGVIT-L%2C%20which%20has%204x%20more%20parameters%2C%0Aacross%20all%20generation%20metrics%20by%20an%20average%20of%2027.9%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02862v1&entry.124074799=Read"},
{"title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &\n  Verification", "author": "Deepak Narayan Gadde and Keerthan Kopparam Radhakrishna and Vaisakh Naduvodi Viswambharan and Aman Kumar and Djones Lettnin and Wolfgang Kunz and Sebastian Simon", "abstract": "  Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.\n", "link": "http://arxiv.org/abs/2507.02660v1", "date": "2025-07-03", "relevancy": 2.3583, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4886}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.475}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hey%20AI%2C%20Generate%20Me%20a%20Hardware%20Code%21%20Agentic%20AI-based%20Hardware%20Design%20%26%0A%20%20Verification&body=Title%3A%20Hey%20AI%2C%20Generate%20Me%20a%20Hardware%20Code%21%20Agentic%20AI-based%20Hardware%20Design%20%26%0A%20%20Verification%0AAuthor%3A%20Deepak%20Narayan%20Gadde%20and%20Keerthan%20Kopparam%20Radhakrishna%20and%20Vaisakh%20Naduvodi%20Viswambharan%20and%20Aman%20Kumar%20and%20Djones%20Lettnin%20and%20Wolfgang%20Kunz%20and%20Sebastian%20Simon%0AAbstract%3A%20%20%20Modern%20Integrated%20Circuits%20%28ICs%29%20are%20becoming%20increasingly%20complex%2C%20and%20so%20is%0Atheir%20development%20process.%20Hardware%20design%20verification%20entails%20a%20methodical%0Aand%20disciplined%20approach%20to%20the%20planning%2C%20development%2C%20execution%2C%20and%20sign-off%0Aof%20functionally%20correct%20hardware%20designs.%20This%20tedious%20process%20requires%0Asignificant%20effort%20and%20time%20to%20ensure%20a%20bug-free%20tape-out.%20The%20field%20of%20Natural%0ALanguage%20Processing%20has%20undergone%20a%20significant%20transformation%20with%20the%20advent%0Aof%20Large%20Language%20Models%20%28LLMs%29.%20These%20powerful%20models%2C%20often%20referred%20to%20as%0AGenerative%20AI%20%28GenAI%29%2C%20have%20revolutionized%20how%20machines%20understand%20and%20generate%0Ahuman%20language%2C%20enabling%20unprecedented%20advancements%20in%20a%20wide%20array%20of%0Aapplications%2C%20including%20hardware%20design%20verification.%20This%20paper%20presents%20an%0Aagentic%20AI-based%20approach%20to%20hardware%20design%20verification%2C%20which%20empowers%20AI%0Aagents%2C%20in%20collaboration%20with%20Humain-in-the-Loop%20%28HITL%29%20intervention%2C%20to%20engage%0Ain%20a%20more%20dynamic%2C%20iterative%2C%20and%20self-reflective%20process%2C%20ultimately%0Aperforming%20end-to-end%20hardware%20design%20and%20verification.%20This%20methodology%20is%0Aevaluated%20on%20five%20open-source%20designs%2C%20achieving%20over%2095%25%20coverage%20with%20reduced%0Averification%20time%20while%20demonstrating%20superior%20performance%2C%20adaptability%2C%20and%0Aconfigurability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHey%2520AI%252C%2520Generate%2520Me%2520a%2520Hardware%2520Code%2521%2520Agentic%2520AI-based%2520Hardware%2520Design%2520%2526%250A%2520%2520Verification%26entry.906535625%3DDeepak%2520Narayan%2520Gadde%2520and%2520Keerthan%2520Kopparam%2520Radhakrishna%2520and%2520Vaisakh%2520Naduvodi%2520Viswambharan%2520and%2520Aman%2520Kumar%2520and%2520Djones%2520Lettnin%2520and%2520Wolfgang%2520Kunz%2520and%2520Sebastian%2520Simon%26entry.1292438233%3D%2520%2520Modern%2520Integrated%2520Circuits%2520%2528ICs%2529%2520are%2520becoming%2520increasingly%2520complex%252C%2520and%2520so%2520is%250Atheir%2520development%2520process.%2520Hardware%2520design%2520verification%2520entails%2520a%2520methodical%250Aand%2520disciplined%2520approach%2520to%2520the%2520planning%252C%2520development%252C%2520execution%252C%2520and%2520sign-off%250Aof%2520functionally%2520correct%2520hardware%2520designs.%2520This%2520tedious%2520process%2520requires%250Asignificant%2520effort%2520and%2520time%2520to%2520ensure%2520a%2520bug-free%2520tape-out.%2520The%2520field%2520of%2520Natural%250ALanguage%2520Processing%2520has%2520undergone%2520a%2520significant%2520transformation%2520with%2520the%2520advent%250Aof%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520These%2520powerful%2520models%252C%2520often%2520referred%2520to%2520as%250AGenerative%2520AI%2520%2528GenAI%2529%252C%2520have%2520revolutionized%2520how%2520machines%2520understand%2520and%2520generate%250Ahuman%2520language%252C%2520enabling%2520unprecedented%2520advancements%2520in%2520a%2520wide%2520array%2520of%250Aapplications%252C%2520including%2520hardware%2520design%2520verification.%2520This%2520paper%2520presents%2520an%250Aagentic%2520AI-based%2520approach%2520to%2520hardware%2520design%2520verification%252C%2520which%2520empowers%2520AI%250Aagents%252C%2520in%2520collaboration%2520with%2520Humain-in-the-Loop%2520%2528HITL%2529%2520intervention%252C%2520to%2520engage%250Ain%2520a%2520more%2520dynamic%252C%2520iterative%252C%2520and%2520self-reflective%2520process%252C%2520ultimately%250Aperforming%2520end-to-end%2520hardware%2520design%2520and%2520verification.%2520This%2520methodology%2520is%250Aevaluated%2520on%2520five%2520open-source%2520designs%252C%2520achieving%2520over%252095%2525%2520coverage%2520with%2520reduced%250Averification%2520time%2520while%2520demonstrating%2520superior%2520performance%252C%2520adaptability%252C%2520and%250Aconfigurability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hey%20AI%2C%20Generate%20Me%20a%20Hardware%20Code%21%20Agentic%20AI-based%20Hardware%20Design%20%26%0A%20%20Verification&entry.906535625=Deepak%20Narayan%20Gadde%20and%20Keerthan%20Kopparam%20Radhakrishna%20and%20Vaisakh%20Naduvodi%20Viswambharan%20and%20Aman%20Kumar%20and%20Djones%20Lettnin%20and%20Wolfgang%20Kunz%20and%20Sebastian%20Simon&entry.1292438233=%20%20Modern%20Integrated%20Circuits%20%28ICs%29%20are%20becoming%20increasingly%20complex%2C%20and%20so%20is%0Atheir%20development%20process.%20Hardware%20design%20verification%20entails%20a%20methodical%0Aand%20disciplined%20approach%20to%20the%20planning%2C%20development%2C%20execution%2C%20and%20sign-off%0Aof%20functionally%20correct%20hardware%20designs.%20This%20tedious%20process%20requires%0Asignificant%20effort%20and%20time%20to%20ensure%20a%20bug-free%20tape-out.%20The%20field%20of%20Natural%0ALanguage%20Processing%20has%20undergone%20a%20significant%20transformation%20with%20the%20advent%0Aof%20Large%20Language%20Models%20%28LLMs%29.%20These%20powerful%20models%2C%20often%20referred%20to%20as%0AGenerative%20AI%20%28GenAI%29%2C%20have%20revolutionized%20how%20machines%20understand%20and%20generate%0Ahuman%20language%2C%20enabling%20unprecedented%20advancements%20in%20a%20wide%20array%20of%0Aapplications%2C%20including%20hardware%20design%20verification.%20This%20paper%20presents%20an%0Aagentic%20AI-based%20approach%20to%20hardware%20design%20verification%2C%20which%20empowers%20AI%0Aagents%2C%20in%20collaboration%20with%20Humain-in-the-Loop%20%28HITL%29%20intervention%2C%20to%20engage%0Ain%20a%20more%20dynamic%2C%20iterative%2C%20and%20self-reflective%20process%2C%20ultimately%0Aperforming%20end-to-end%20hardware%20design%20and%20verification.%20This%20methodology%20is%0Aevaluated%20on%20five%20open-source%20designs%2C%20achieving%20over%2095%25%20coverage%20with%20reduced%0Averification%20time%20while%20demonstrating%20superior%20performance%2C%20adaptability%2C%20and%0Aconfigurability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02660v1&entry.124074799=Read"},
{"title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models", "author": "Shehel Yoosuf and Temoor Ali and Ahmed Lekssays and Mashael AlSabah and Issa Khalil", "abstract": "  In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g., SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to a 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing content\ntransformations, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection.\n", "link": "http://arxiv.org/abs/2502.11853v2", "date": "2025-07-03", "relevancy": 2.3342, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models&body=Title%3A%20StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Shehel%20Yoosuf%20and%20Temoor%20Ali%20and%20Ahmed%20Lekssays%20and%20Mashael%20AlSabah%20and%20Issa%20Khalil%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20series%20of%20structure%20transformation%20attacks%20on%20LLM%0Aalignment%2C%20where%20we%20encode%20natural%20language%20intent%20using%20diverse%20syntax%20spaces%2C%0Aranging%20from%20simple%20structure%20formats%20and%20basic%20query%20languages%20%28e.g.%2C%20SQL%29%20to%0Anew%20novel%20spaces%20and%20syntaxes%20created%20entirely%20by%20LLMs.%20Our%20extensive%0Aevaluation%20shows%20that%20our%20simplest%20attacks%20can%20achieve%20close%20to%20a%2090%25%20success%0Arate%2C%20even%20on%20strict%20LLMs%20%28such%20as%20Claude%203.5%20Sonnet%29%20using%20SOTA%20alignment%0Amechanisms.%20We%20improve%20the%20attack%20performance%20further%20by%20using%20an%20adaptive%0Ascheme%20that%20combines%20structure%20transformations%20along%20with%20existing%20content%0Atransformations%2C%20resulting%20in%20over%2096%25%20ASR%20with%200%25%20refusals.%0A%20%20To%20generalize%20our%20attacks%2C%20we%20explore%20numerous%20structure%20formats%2C%20including%0Asyntaxes%20purely%20generated%20by%20LLMs.%20Our%20results%20indicate%20that%20such%20novel%0Asyntaxes%20are%20easy%20to%20generate%20and%20result%20in%20a%20high%20ASR%2C%20suggesting%20that%0Adefending%20against%20our%20attacks%20is%20not%20a%20straightforward%20process.%20Finally%2C%20we%0Adevelop%20a%20benchmark%20and%20evaluate%20existing%20safety-alignment%20defenses%20against%20it%2C%0Ashowing%20that%20most%20of%20them%20fail%20with%20100%25%20ASR.%20Our%20results%20show%20that%20existing%0Asafety%20alignment%20mostly%20relies%20on%20token-level%20patterns%20without%20recognizing%0Aharmful%20concepts%2C%20highlighting%20and%20motivating%20the%20need%20for%20serious%20research%0Aefforts%20in%20this%20direction.%20As%20a%20case%20study%2C%20we%20demonstrate%20how%20attackers%20can%0Ause%20our%20attack%20to%20easily%20generate%20a%20sample%20malware%20and%20a%20corpus%20of%20fraudulent%0ASMS%20messages%2C%20which%20perform%20well%20in%20bypassing%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructTransform%253A%2520A%2520Scalable%2520Attack%2520Surface%2520for%2520Safety-Aligned%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DShehel%2520Yoosuf%2520and%2520Temoor%2520Ali%2520and%2520Ahmed%2520Lekssays%2520and%2520Mashael%2520AlSabah%2520and%2520Issa%2520Khalil%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520series%2520of%2520structure%2520transformation%2520attacks%2520on%2520LLM%250Aalignment%252C%2520where%2520we%2520encode%2520natural%2520language%2520intent%2520using%2520diverse%2520syntax%2520spaces%252C%250Aranging%2520from%2520simple%2520structure%2520formats%2520and%2520basic%2520query%2520languages%2520%2528e.g.%252C%2520SQL%2529%2520to%250Anew%2520novel%2520spaces%2520and%2520syntaxes%2520created%2520entirely%2520by%2520LLMs.%2520Our%2520extensive%250Aevaluation%2520shows%2520that%2520our%2520simplest%2520attacks%2520can%2520achieve%2520close%2520to%2520a%252090%2525%2520success%250Arate%252C%2520even%2520on%2520strict%2520LLMs%2520%2528such%2520as%2520Claude%25203.5%2520Sonnet%2529%2520using%2520SOTA%2520alignment%250Amechanisms.%2520We%2520improve%2520the%2520attack%2520performance%2520further%2520by%2520using%2520an%2520adaptive%250Ascheme%2520that%2520combines%2520structure%2520transformations%2520along%2520with%2520existing%2520content%250Atransformations%252C%2520resulting%2520in%2520over%252096%2525%2520ASR%2520with%25200%2525%2520refusals.%250A%2520%2520To%2520generalize%2520our%2520attacks%252C%2520we%2520explore%2520numerous%2520structure%2520formats%252C%2520including%250Asyntaxes%2520purely%2520generated%2520by%2520LLMs.%2520Our%2520results%2520indicate%2520that%2520such%2520novel%250Asyntaxes%2520are%2520easy%2520to%2520generate%2520and%2520result%2520in%2520a%2520high%2520ASR%252C%2520suggesting%2520that%250Adefending%2520against%2520our%2520attacks%2520is%2520not%2520a%2520straightforward%2520process.%2520Finally%252C%2520we%250Adevelop%2520a%2520benchmark%2520and%2520evaluate%2520existing%2520safety-alignment%2520defenses%2520against%2520it%252C%250Ashowing%2520that%2520most%2520of%2520them%2520fail%2520with%2520100%2525%2520ASR.%2520Our%2520results%2520show%2520that%2520existing%250Asafety%2520alignment%2520mostly%2520relies%2520on%2520token-level%2520patterns%2520without%2520recognizing%250Aharmful%2520concepts%252C%2520highlighting%2520and%2520motivating%2520the%2520need%2520for%2520serious%2520research%250Aefforts%2520in%2520this%2520direction.%2520As%2520a%2520case%2520study%252C%2520we%2520demonstrate%2520how%2520attackers%2520can%250Ause%2520our%2520attack%2520to%2520easily%2520generate%2520a%2520sample%2520malware%2520and%2520a%2520corpus%2520of%2520fraudulent%250ASMS%2520messages%252C%2520which%2520perform%2520well%2520in%2520bypassing%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models&entry.906535625=Shehel%20Yoosuf%20and%20Temoor%20Ali%20and%20Ahmed%20Lekssays%20and%20Mashael%20AlSabah%20and%20Issa%20Khalil&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20series%20of%20structure%20transformation%20attacks%20on%20LLM%0Aalignment%2C%20where%20we%20encode%20natural%20language%20intent%20using%20diverse%20syntax%20spaces%2C%0Aranging%20from%20simple%20structure%20formats%20and%20basic%20query%20languages%20%28e.g.%2C%20SQL%29%20to%0Anew%20novel%20spaces%20and%20syntaxes%20created%20entirely%20by%20LLMs.%20Our%20extensive%0Aevaluation%20shows%20that%20our%20simplest%20attacks%20can%20achieve%20close%20to%20a%2090%25%20success%0Arate%2C%20even%20on%20strict%20LLMs%20%28such%20as%20Claude%203.5%20Sonnet%29%20using%20SOTA%20alignment%0Amechanisms.%20We%20improve%20the%20attack%20performance%20further%20by%20using%20an%20adaptive%0Ascheme%20that%20combines%20structure%20transformations%20along%20with%20existing%20content%0Atransformations%2C%20resulting%20in%20over%2096%25%20ASR%20with%200%25%20refusals.%0A%20%20To%20generalize%20our%20attacks%2C%20we%20explore%20numerous%20structure%20formats%2C%20including%0Asyntaxes%20purely%20generated%20by%20LLMs.%20Our%20results%20indicate%20that%20such%20novel%0Asyntaxes%20are%20easy%20to%20generate%20and%20result%20in%20a%20high%20ASR%2C%20suggesting%20that%0Adefending%20against%20our%20attacks%20is%20not%20a%20straightforward%20process.%20Finally%2C%20we%0Adevelop%20a%20benchmark%20and%20evaluate%20existing%20safety-alignment%20defenses%20against%20it%2C%0Ashowing%20that%20most%20of%20them%20fail%20with%20100%25%20ASR.%20Our%20results%20show%20that%20existing%0Asafety%20alignment%20mostly%20relies%20on%20token-level%20patterns%20without%20recognizing%0Aharmful%20concepts%2C%20highlighting%20and%20motivating%20the%20need%20for%20serious%20research%0Aefforts%20in%20this%20direction.%20As%20a%20case%20study%2C%20we%20demonstrate%20how%20attackers%20can%0Ause%20our%20attack%20to%20easily%20generate%20a%20sample%20malware%20and%20a%20corpus%20of%20fraudulent%0ASMS%20messages%2C%20which%20perform%20well%20in%20bypassing%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11853v2&entry.124074799=Read"},
{"title": "Interpreting Graph Inference with Skyline Explanations", "author": "Dazhuo Qiu and Haolai Che and Arijit Khan and Yinghui Wu", "abstract": "  Inference queries have been routinely issued to graph machine learning models\nsuch as graph neural networks (GNNs) for various network analytical tasks.\nNevertheless, GNNs outputs are often hard to interpret comprehensively.\nExisting methods typically compromise to individual pre-defined explainability\nmeasures (such as fidelity), which often leads to biased, ``one-sided''\ninterpretations. This paper introduces skyline explanation, a new paradigm that\ninterprets GNN output by simultaneously optimizing multiple explainability\nmeasures of users' interests. (1) We propose skyline explanations as a Pareto\nset of explanatory subgraphs that dominate others over multiple explanatory\nmeasures. We formulate skyline explanation as a multi-criteria optimization\nproblem, and establish its hardness results. (2) We design efficient algorithms\nwith an onion-peeling approach, which strategically prioritizes nodes and\nremoves unpromising edges to incrementally assemble skyline explanations. (3)\nWe also develop an algorithm to diversify the skyline explanations to enrich\nthe comprehensive interpretation. (4) We introduce efficient parallel\nalgorithms with load-balancing strategies to scale skyline explanation for\nlarge-scale GNN-based inference. Using real-world and synthetic graphs, we\nexperimentally verify our algorithms' effectiveness and scalability.\n", "link": "http://arxiv.org/abs/2505.07635v2", "date": "2025-07-03", "relevancy": 2.3318, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4693}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations&body=Title%3A%20Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations%0AAuthor%3A%20Dazhuo%20Qiu%20and%20Haolai%20Che%20and%20Arijit%20Khan%20and%20Yinghui%20Wu%0AAbstract%3A%20%20%20Inference%20queries%20have%20been%20routinely%20issued%20to%20graph%20machine%20learning%20models%0Asuch%20as%20graph%20neural%20networks%20%28GNNs%29%20for%20various%20network%20analytical%20tasks.%0ANevertheless%2C%20GNNs%20outputs%20are%20often%20hard%20to%20interpret%20comprehensively.%0AExisting%20methods%20typically%20compromise%20to%20individual%20pre-defined%20explainability%0Ameasures%20%28such%20as%20fidelity%29%2C%20which%20often%20leads%20to%20biased%2C%20%60%60one-sided%27%27%0Ainterpretations.%20This%20paper%20introduces%20skyline%20explanation%2C%20a%20new%20paradigm%20that%0Ainterprets%20GNN%20output%20by%20simultaneously%20optimizing%20multiple%20explainability%0Ameasures%20of%20users%27%20interests.%20%281%29%20We%20propose%20skyline%20explanations%20as%20a%20Pareto%0Aset%20of%20explanatory%20subgraphs%20that%20dominate%20others%20over%20multiple%20explanatory%0Ameasures.%20We%20formulate%20skyline%20explanation%20as%20a%20multi-criteria%20optimization%0Aproblem%2C%20and%20establish%20its%20hardness%20results.%20%282%29%20We%20design%20efficient%20algorithms%0Awith%20an%20onion-peeling%20approach%2C%20which%20strategically%20prioritizes%20nodes%20and%0Aremoves%20unpromising%20edges%20to%20incrementally%20assemble%20skyline%20explanations.%20%283%29%0AWe%20also%20develop%20an%20algorithm%20to%20diversify%20the%20skyline%20explanations%20to%20enrich%0Athe%20comprehensive%20interpretation.%20%284%29%20We%20introduce%20efficient%20parallel%0Aalgorithms%20with%20load-balancing%20strategies%20to%20scale%20skyline%20explanation%20for%0Alarge-scale%20GNN-based%20inference.%20Using%20real-world%20and%20synthetic%20graphs%2C%20we%0Aexperimentally%20verify%20our%20algorithms%27%20effectiveness%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Graph%2520Inference%2520with%2520Skyline%2520Explanations%26entry.906535625%3DDazhuo%2520Qiu%2520and%2520Haolai%2520Che%2520and%2520Arijit%2520Khan%2520and%2520Yinghui%2520Wu%26entry.1292438233%3D%2520%2520Inference%2520queries%2520have%2520been%2520routinely%2520issued%2520to%2520graph%2520machine%2520learning%2520models%250Asuch%2520as%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520for%2520various%2520network%2520analytical%2520tasks.%250ANevertheless%252C%2520GNNs%2520outputs%2520are%2520often%2520hard%2520to%2520interpret%2520comprehensively.%250AExisting%2520methods%2520typically%2520compromise%2520to%2520individual%2520pre-defined%2520explainability%250Ameasures%2520%2528such%2520as%2520fidelity%2529%252C%2520which%2520often%2520leads%2520to%2520biased%252C%2520%2560%2560one-sided%2527%2527%250Ainterpretations.%2520This%2520paper%2520introduces%2520skyline%2520explanation%252C%2520a%2520new%2520paradigm%2520that%250Ainterprets%2520GNN%2520output%2520by%2520simultaneously%2520optimizing%2520multiple%2520explainability%250Ameasures%2520of%2520users%2527%2520interests.%2520%25281%2529%2520We%2520propose%2520skyline%2520explanations%2520as%2520a%2520Pareto%250Aset%2520of%2520explanatory%2520subgraphs%2520that%2520dominate%2520others%2520over%2520multiple%2520explanatory%250Ameasures.%2520We%2520formulate%2520skyline%2520explanation%2520as%2520a%2520multi-criteria%2520optimization%250Aproblem%252C%2520and%2520establish%2520its%2520hardness%2520results.%2520%25282%2529%2520We%2520design%2520efficient%2520algorithms%250Awith%2520an%2520onion-peeling%2520approach%252C%2520which%2520strategically%2520prioritizes%2520nodes%2520and%250Aremoves%2520unpromising%2520edges%2520to%2520incrementally%2520assemble%2520skyline%2520explanations.%2520%25283%2529%250AWe%2520also%2520develop%2520an%2520algorithm%2520to%2520diversify%2520the%2520skyline%2520explanations%2520to%2520enrich%250Athe%2520comprehensive%2520interpretation.%2520%25284%2529%2520We%2520introduce%2520efficient%2520parallel%250Aalgorithms%2520with%2520load-balancing%2520strategies%2520to%2520scale%2520skyline%2520explanation%2520for%250Alarge-scale%2520GNN-based%2520inference.%2520Using%2520real-world%2520and%2520synthetic%2520graphs%252C%2520we%250Aexperimentally%2520verify%2520our%2520algorithms%2527%2520effectiveness%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations&entry.906535625=Dazhuo%20Qiu%20and%20Haolai%20Che%20and%20Arijit%20Khan%20and%20Yinghui%20Wu&entry.1292438233=%20%20Inference%20queries%20have%20been%20routinely%20issued%20to%20graph%20machine%20learning%20models%0Asuch%20as%20graph%20neural%20networks%20%28GNNs%29%20for%20various%20network%20analytical%20tasks.%0ANevertheless%2C%20GNNs%20outputs%20are%20often%20hard%20to%20interpret%20comprehensively.%0AExisting%20methods%20typically%20compromise%20to%20individual%20pre-defined%20explainability%0Ameasures%20%28such%20as%20fidelity%29%2C%20which%20often%20leads%20to%20biased%2C%20%60%60one-sided%27%27%0Ainterpretations.%20This%20paper%20introduces%20skyline%20explanation%2C%20a%20new%20paradigm%20that%0Ainterprets%20GNN%20output%20by%20simultaneously%20optimizing%20multiple%20explainability%0Ameasures%20of%20users%27%20interests.%20%281%29%20We%20propose%20skyline%20explanations%20as%20a%20Pareto%0Aset%20of%20explanatory%20subgraphs%20that%20dominate%20others%20over%20multiple%20explanatory%0Ameasures.%20We%20formulate%20skyline%20explanation%20as%20a%20multi-criteria%20optimization%0Aproblem%2C%20and%20establish%20its%20hardness%20results.%20%282%29%20We%20design%20efficient%20algorithms%0Awith%20an%20onion-peeling%20approach%2C%20which%20strategically%20prioritizes%20nodes%20and%0Aremoves%20unpromising%20edges%20to%20incrementally%20assemble%20skyline%20explanations.%20%283%29%0AWe%20also%20develop%20an%20algorithm%20to%20diversify%20the%20skyline%20explanations%20to%20enrich%0Athe%20comprehensive%20interpretation.%20%284%29%20We%20introduce%20efficient%20parallel%0Aalgorithms%20with%20load-balancing%20strategies%20to%20scale%20skyline%20explanation%20for%0Alarge-scale%20GNN-based%20inference.%20Using%20real-world%20and%20synthetic%20graphs%2C%20we%0Aexperimentally%20verify%20our%20algorithms%27%20effectiveness%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07635v2&entry.124074799=Read"},
{"title": "MV2DFusion: Leveraging Modality-Specific Object Semantics for\n  Multi-Modal 3D Detection", "author": "Zitian Wang and Zehao Huang and Yulu Gao and Naiyan Wang and Si Liu", "abstract": "  The rise of autonomous vehicles has significantly increased the demand for\nrobust 3D object detection systems. While cameras and LiDAR sensors each offer\nunique advantages--cameras provide rich texture information and LiDAR offers\nprecise 3D spatial data--relying on a single modality often leads to\nperformance limitations. This paper introduces MV2DFusion, a multi-modal\ndetection framework that integrates the strengths of both worlds through an\nadvanced query-based fusion mechanism. By introducing an image query generator\nto align with image-specific attributes and a point cloud query generator,\nMV2DFusion effectively combines modality-specific object semantics without\nbiasing toward one single modality. Then the sparse fusion process can be\naccomplished based on the valuable object semantics, ensuring efficient and\naccurate object detection across various scenarios. Our framework's flexibility\nallows it to integrate with any image and point cloud-based detectors,\nshowcasing its adaptability and potential for future advancements. Extensive\nevaluations on the nuScenes and Argoverse2 datasets demonstrate that MV2DFusion\nachieves state-of-the-art performance, particularly excelling in long-range\ndetection scenarios.\n", "link": "http://arxiv.org/abs/2408.05945v2", "date": "2025-07-03", "relevancy": 2.3247, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV2DFusion%3A%20Leveraging%20Modality-Specific%20Object%20Semantics%20for%0A%20%20Multi-Modal%203D%20Detection&body=Title%3A%20MV2DFusion%3A%20Leveraging%20Modality-Specific%20Object%20Semantics%20for%0A%20%20Multi-Modal%203D%20Detection%0AAuthor%3A%20Zitian%20Wang%20and%20Zehao%20Huang%20and%20Yulu%20Gao%20and%20Naiyan%20Wang%20and%20Si%20Liu%0AAbstract%3A%20%20%20The%20rise%20of%20autonomous%20vehicles%20has%20significantly%20increased%20the%20demand%20for%0Arobust%203D%20object%20detection%20systems.%20While%20cameras%20and%20LiDAR%20sensors%20each%20offer%0Aunique%20advantages--cameras%20provide%20rich%20texture%20information%20and%20LiDAR%20offers%0Aprecise%203D%20spatial%20data--relying%20on%20a%20single%20modality%20often%20leads%20to%0Aperformance%20limitations.%20This%20paper%20introduces%20MV2DFusion%2C%20a%20multi-modal%0Adetection%20framework%20that%20integrates%20the%20strengths%20of%20both%20worlds%20through%20an%0Aadvanced%20query-based%20fusion%20mechanism.%20By%20introducing%20an%20image%20query%20generator%0Ato%20align%20with%20image-specific%20attributes%20and%20a%20point%20cloud%20query%20generator%2C%0AMV2DFusion%20effectively%20combines%20modality-specific%20object%20semantics%20without%0Abiasing%20toward%20one%20single%20modality.%20Then%20the%20sparse%20fusion%20process%20can%20be%0Aaccomplished%20based%20on%20the%20valuable%20object%20semantics%2C%20ensuring%20efficient%20and%0Aaccurate%20object%20detection%20across%20various%20scenarios.%20Our%20framework%27s%20flexibility%0Aallows%20it%20to%20integrate%20with%20any%20image%20and%20point%20cloud-based%20detectors%2C%0Ashowcasing%20its%20adaptability%20and%20potential%20for%20future%20advancements.%20Extensive%0Aevaluations%20on%20the%20nuScenes%20and%20Argoverse2%20datasets%20demonstrate%20that%20MV2DFusion%0Aachieves%20state-of-the-art%20performance%2C%20particularly%20excelling%20in%20long-range%0Adetection%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV2DFusion%253A%2520Leveraging%2520Modality-Specific%2520Object%2520Semantics%2520for%250A%2520%2520Multi-Modal%25203D%2520Detection%26entry.906535625%3DZitian%2520Wang%2520and%2520Zehao%2520Huang%2520and%2520Yulu%2520Gao%2520and%2520Naiyan%2520Wang%2520and%2520Si%2520Liu%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520autonomous%2520vehicles%2520has%2520significantly%2520increased%2520the%2520demand%2520for%250Arobust%25203D%2520object%2520detection%2520systems.%2520While%2520cameras%2520and%2520LiDAR%2520sensors%2520each%2520offer%250Aunique%2520advantages--cameras%2520provide%2520rich%2520texture%2520information%2520and%2520LiDAR%2520offers%250Aprecise%25203D%2520spatial%2520data--relying%2520on%2520a%2520single%2520modality%2520often%2520leads%2520to%250Aperformance%2520limitations.%2520This%2520paper%2520introduces%2520MV2DFusion%252C%2520a%2520multi-modal%250Adetection%2520framework%2520that%2520integrates%2520the%2520strengths%2520of%2520both%2520worlds%2520through%2520an%250Aadvanced%2520query-based%2520fusion%2520mechanism.%2520By%2520introducing%2520an%2520image%2520query%2520generator%250Ato%2520align%2520with%2520image-specific%2520attributes%2520and%2520a%2520point%2520cloud%2520query%2520generator%252C%250AMV2DFusion%2520effectively%2520combines%2520modality-specific%2520object%2520semantics%2520without%250Abiasing%2520toward%2520one%2520single%2520modality.%2520Then%2520the%2520sparse%2520fusion%2520process%2520can%2520be%250Aaccomplished%2520based%2520on%2520the%2520valuable%2520object%2520semantics%252C%2520ensuring%2520efficient%2520and%250Aaccurate%2520object%2520detection%2520across%2520various%2520scenarios.%2520Our%2520framework%2527s%2520flexibility%250Aallows%2520it%2520to%2520integrate%2520with%2520any%2520image%2520and%2520point%2520cloud-based%2520detectors%252C%250Ashowcasing%2520its%2520adaptability%2520and%2520potential%2520for%2520future%2520advancements.%2520Extensive%250Aevaluations%2520on%2520the%2520nuScenes%2520and%2520Argoverse2%2520datasets%2520demonstrate%2520that%2520MV2DFusion%250Aachieves%2520state-of-the-art%2520performance%252C%2520particularly%2520excelling%2520in%2520long-range%250Adetection%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV2DFusion%3A%20Leveraging%20Modality-Specific%20Object%20Semantics%20for%0A%20%20Multi-Modal%203D%20Detection&entry.906535625=Zitian%20Wang%20and%20Zehao%20Huang%20and%20Yulu%20Gao%20and%20Naiyan%20Wang%20and%20Si%20Liu&entry.1292438233=%20%20The%20rise%20of%20autonomous%20vehicles%20has%20significantly%20increased%20the%20demand%20for%0Arobust%203D%20object%20detection%20systems.%20While%20cameras%20and%20LiDAR%20sensors%20each%20offer%0Aunique%20advantages--cameras%20provide%20rich%20texture%20information%20and%20LiDAR%20offers%0Aprecise%203D%20spatial%20data--relying%20on%20a%20single%20modality%20often%20leads%20to%0Aperformance%20limitations.%20This%20paper%20introduces%20MV2DFusion%2C%20a%20multi-modal%0Adetection%20framework%20that%20integrates%20the%20strengths%20of%20both%20worlds%20through%20an%0Aadvanced%20query-based%20fusion%20mechanism.%20By%20introducing%20an%20image%20query%20generator%0Ato%20align%20with%20image-specific%20attributes%20and%20a%20point%20cloud%20query%20generator%2C%0AMV2DFusion%20effectively%20combines%20modality-specific%20object%20semantics%20without%0Abiasing%20toward%20one%20single%20modality.%20Then%20the%20sparse%20fusion%20process%20can%20be%0Aaccomplished%20based%20on%20the%20valuable%20object%20semantics%2C%20ensuring%20efficient%20and%0Aaccurate%20object%20detection%20across%20various%20scenarios.%20Our%20framework%27s%20flexibility%0Aallows%20it%20to%20integrate%20with%20any%20image%20and%20point%20cloud-based%20detectors%2C%0Ashowcasing%20its%20adaptability%20and%20potential%20for%20future%20advancements.%20Extensive%0Aevaluations%20on%20the%20nuScenes%20and%20Argoverse2%20datasets%20demonstrate%20that%20MV2DFusion%0Aachieves%20state-of-the-art%20performance%2C%20particularly%20excelling%20in%20long-range%0Adetection%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05945v2&entry.124074799=Read"},
{"title": "Learning few-step posterior samplers by unfolding and distillation of\n  diffusion models", "author": "Charlesquin Kemajou Mbakam and Jonathan Spence and Marcelo Pereyra", "abstract": "  Diffusion models (DMs) have emerged as powerful image priors in Bayesian\ncomputational imaging. Two primary strategies have been proposed for leveraging\nDMs in this context: Plug-and-Play methods, which are zero-shot and highly\nflexible but rely on approximations; and specialized conditional DMs, which\nachieve higher accuracy and faster inference for specific tasks through\nsupervised training. In this work, we introduce a novel framework that\nintegrates deep unfolding and model distillation to transform a DM image prior\ninto a few-step conditional model for posterior sampling. A central innovation\nof our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm\n- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et\nal., 2025) - representing the first known instance of deep unfolding applied to\na Monte Carlo sampling scheme. We demonstrate our proposed unfolded and\ndistilled samplers through extensive experiments and comparisons with the state\nof the art, where they achieve excellent accuracy and computational efficiency,\nwhile retaining the flexibility to adapt to variations in the forward model at\ninference time.\n", "link": "http://arxiv.org/abs/2507.02686v1", "date": "2025-07-03", "relevancy": 2.3244, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6703}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5661}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20few-step%20posterior%20samplers%20by%20unfolding%20and%20distillation%20of%0A%20%20diffusion%20models&body=Title%3A%20Learning%20few-step%20posterior%20samplers%20by%20unfolding%20and%20distillation%20of%0A%20%20diffusion%20models%0AAuthor%3A%20Charlesquin%20Kemajou%20Mbakam%20and%20Jonathan%20Spence%20and%20Marcelo%20Pereyra%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20emerged%20as%20powerful%20image%20priors%20in%20Bayesian%0Acomputational%20imaging.%20Two%20primary%20strategies%20have%20been%20proposed%20for%20leveraging%0ADMs%20in%20this%20context%3A%20Plug-and-Play%20methods%2C%20which%20are%20zero-shot%20and%20highly%0Aflexible%20but%20rely%20on%20approximations%3B%20and%20specialized%20conditional%20DMs%2C%20which%0Aachieve%20higher%20accuracy%20and%20faster%20inference%20for%20specific%20tasks%20through%0Asupervised%20training.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20that%0Aintegrates%20deep%20unfolding%20and%20model%20distillation%20to%20transform%20a%20DM%20image%20prior%0Ainto%20a%20few-step%20conditional%20model%20for%20posterior%20sampling.%20A%20central%20innovation%0Aof%20our%20approach%20is%20the%20unfolding%20of%20a%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20algorithm%0A-%20specifically%2C%20the%20recently%20proposed%20LATINO%20Langevin%20sampler%20%28Spagnoletti%20et%0Aal.%2C%202025%29%20-%20representing%20the%20first%20known%20instance%20of%20deep%20unfolding%20applied%20to%0Aa%20Monte%20Carlo%20sampling%20scheme.%20We%20demonstrate%20our%20proposed%20unfolded%20and%0Adistilled%20samplers%20through%20extensive%20experiments%20and%20comparisons%20with%20the%20state%0Aof%20the%20art%2C%20where%20they%20achieve%20excellent%20accuracy%20and%20computational%20efficiency%2C%0Awhile%20retaining%20the%20flexibility%20to%20adapt%20to%20variations%20in%20the%20forward%20model%20at%0Ainference%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520few-step%2520posterior%2520samplers%2520by%2520unfolding%2520and%2520distillation%2520of%250A%2520%2520diffusion%2520models%26entry.906535625%3DCharlesquin%2520Kemajou%2520Mbakam%2520and%2520Jonathan%2520Spence%2520and%2520Marcelo%2520Pereyra%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520emerged%2520as%2520powerful%2520image%2520priors%2520in%2520Bayesian%250Acomputational%2520imaging.%2520Two%2520primary%2520strategies%2520have%2520been%2520proposed%2520for%2520leveraging%250ADMs%2520in%2520this%2520context%253A%2520Plug-and-Play%2520methods%252C%2520which%2520are%2520zero-shot%2520and%2520highly%250Aflexible%2520but%2520rely%2520on%2520approximations%253B%2520and%2520specialized%2520conditional%2520DMs%252C%2520which%250Aachieve%2520higher%2520accuracy%2520and%2520faster%2520inference%2520for%2520specific%2520tasks%2520through%250Asupervised%2520training.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%250Aintegrates%2520deep%2520unfolding%2520and%2520model%2520distillation%2520to%2520transform%2520a%2520DM%2520image%2520prior%250Ainto%2520a%2520few-step%2520conditional%2520model%2520for%2520posterior%2520sampling.%2520A%2520central%2520innovation%250Aof%2520our%2520approach%2520is%2520the%2520unfolding%2520of%2520a%2520Markov%2520chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520algorithm%250A-%2520specifically%252C%2520the%2520recently%2520proposed%2520LATINO%2520Langevin%2520sampler%2520%2528Spagnoletti%2520et%250Aal.%252C%25202025%2529%2520-%2520representing%2520the%2520first%2520known%2520instance%2520of%2520deep%2520unfolding%2520applied%2520to%250Aa%2520Monte%2520Carlo%2520sampling%2520scheme.%2520We%2520demonstrate%2520our%2520proposed%2520unfolded%2520and%250Adistilled%2520samplers%2520through%2520extensive%2520experiments%2520and%2520comparisons%2520with%2520the%2520state%250Aof%2520the%2520art%252C%2520where%2520they%2520achieve%2520excellent%2520accuracy%2520and%2520computational%2520efficiency%252C%250Awhile%2520retaining%2520the%2520flexibility%2520to%2520adapt%2520to%2520variations%2520in%2520the%2520forward%2520model%2520at%250Ainference%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20few-step%20posterior%20samplers%20by%20unfolding%20and%20distillation%20of%0A%20%20diffusion%20models&entry.906535625=Charlesquin%20Kemajou%20Mbakam%20and%20Jonathan%20Spence%20and%20Marcelo%20Pereyra&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20emerged%20as%20powerful%20image%20priors%20in%20Bayesian%0Acomputational%20imaging.%20Two%20primary%20strategies%20have%20been%20proposed%20for%20leveraging%0ADMs%20in%20this%20context%3A%20Plug-and-Play%20methods%2C%20which%20are%20zero-shot%20and%20highly%0Aflexible%20but%20rely%20on%20approximations%3B%20and%20specialized%20conditional%20DMs%2C%20which%0Aachieve%20higher%20accuracy%20and%20faster%20inference%20for%20specific%20tasks%20through%0Asupervised%20training.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20that%0Aintegrates%20deep%20unfolding%20and%20model%20distillation%20to%20transform%20a%20DM%20image%20prior%0Ainto%20a%20few-step%20conditional%20model%20for%20posterior%20sampling.%20A%20central%20innovation%0Aof%20our%20approach%20is%20the%20unfolding%20of%20a%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20algorithm%0A-%20specifically%2C%20the%20recently%20proposed%20LATINO%20Langevin%20sampler%20%28Spagnoletti%20et%0Aal.%2C%202025%29%20-%20representing%20the%20first%20known%20instance%20of%20deep%20unfolding%20applied%20to%0Aa%20Monte%20Carlo%20sampling%20scheme.%20We%20demonstrate%20our%20proposed%20unfolded%20and%0Adistilled%20samplers%20through%20extensive%20experiments%20and%20comparisons%20with%20the%20state%0Aof%20the%20art%2C%20where%20they%20achieve%20excellent%20accuracy%20and%20computational%20efficiency%2C%0Awhile%20retaining%20the%20flexibility%20to%20adapt%20to%20variations%20in%20the%20forward%20model%20at%0Ainference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02686v1&entry.124074799=Read"},
{"title": "Towards an Explainable Comparison and Alignment of Feature Embeddings", "author": "Mohammad Jalali and Bahar Dibaei Nia and Farzan Farnia", "abstract": "  While several feature embedding models have been developed in the literature,\ncomparisons of these embeddings have largely focused on their numerical\nperformance in classification-related downstream applications. However, an\ninterpretable comparison of different embeddings requires identifying and\nanalyzing mismatches between sample groups clustered within the embedding\nspaces. In this work, we propose the \\emph{Spectral Pairwise Embedding\nComparison (SPEC)} framework to compare embeddings and identify their\ndifferences in clustering a reference dataset. Our approach examines the kernel\nmatrices derived from two embeddings and leverages the eigendecomposition of\nthe difference kernel matrix to detect sample clusters that are captured\ndifferently by the two embeddings. We present a scalable implementation of this\nkernel-based approach, with computational complexity that grows linearly with\nthe sample size. Furthermore, we introduce an optimization problem using this\nframework to align two embeddings, ensuring that clusters identified in one\nembedding are also captured in the other model. We provide numerical results\ndemonstrating the SPEC's application to compare and align embeddings on\nlarge-scale datasets such as ImageNet and MS-COCO. The project page is\navailable at https://mjalali.github.io/SPEC/.\n", "link": "http://arxiv.org/abs/2506.06231v2", "date": "2025-07-03", "relevancy": 2.3169, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20an%20Explainable%20Comparison%20and%20Alignment%20of%20Feature%20Embeddings&body=Title%3A%20Towards%20an%20Explainable%20Comparison%20and%20Alignment%20of%20Feature%20Embeddings%0AAuthor%3A%20Mohammad%20Jalali%20and%20Bahar%20Dibaei%20Nia%20and%20Farzan%20Farnia%0AAbstract%3A%20%20%20While%20several%20feature%20embedding%20models%20have%20been%20developed%20in%20the%20literature%2C%0Acomparisons%20of%20these%20embeddings%20have%20largely%20focused%20on%20their%20numerical%0Aperformance%20in%20classification-related%20downstream%20applications.%20However%2C%20an%0Ainterpretable%20comparison%20of%20different%20embeddings%20requires%20identifying%20and%0Aanalyzing%20mismatches%20between%20sample%20groups%20clustered%20within%20the%20embedding%0Aspaces.%20In%20this%20work%2C%20we%20propose%20the%20%5Cemph%7BSpectral%20Pairwise%20Embedding%0AComparison%20%28SPEC%29%7D%20framework%20to%20compare%20embeddings%20and%20identify%20their%0Adifferences%20in%20clustering%20a%20reference%20dataset.%20Our%20approach%20examines%20the%20kernel%0Amatrices%20derived%20from%20two%20embeddings%20and%20leverages%20the%20eigendecomposition%20of%0Athe%20difference%20kernel%20matrix%20to%20detect%20sample%20clusters%20that%20are%20captured%0Adifferently%20by%20the%20two%20embeddings.%20We%20present%20a%20scalable%20implementation%20of%20this%0Akernel-based%20approach%2C%20with%20computational%20complexity%20that%20grows%20linearly%20with%0Athe%20sample%20size.%20Furthermore%2C%20we%20introduce%20an%20optimization%20problem%20using%20this%0Aframework%20to%20align%20two%20embeddings%2C%20ensuring%20that%20clusters%20identified%20in%20one%0Aembedding%20are%20also%20captured%20in%20the%20other%20model.%20We%20provide%20numerical%20results%0Ademonstrating%20the%20SPEC%27s%20application%20to%20compare%20and%20align%20embeddings%20on%0Alarge-scale%20datasets%20such%20as%20ImageNet%20and%20MS-COCO.%20The%20project%20page%20is%0Aavailable%20at%20https%3A//mjalali.github.io/SPEC/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520an%2520Explainable%2520Comparison%2520and%2520Alignment%2520of%2520Feature%2520Embeddings%26entry.906535625%3DMohammad%2520Jalali%2520and%2520Bahar%2520Dibaei%2520Nia%2520and%2520Farzan%2520Farnia%26entry.1292438233%3D%2520%2520While%2520several%2520feature%2520embedding%2520models%2520have%2520been%2520developed%2520in%2520the%2520literature%252C%250Acomparisons%2520of%2520these%2520embeddings%2520have%2520largely%2520focused%2520on%2520their%2520numerical%250Aperformance%2520in%2520classification-related%2520downstream%2520applications.%2520However%252C%2520an%250Ainterpretable%2520comparison%2520of%2520different%2520embeddings%2520requires%2520identifying%2520and%250Aanalyzing%2520mismatches%2520between%2520sample%2520groups%2520clustered%2520within%2520the%2520embedding%250Aspaces.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520%255Cemph%257BSpectral%2520Pairwise%2520Embedding%250AComparison%2520%2528SPEC%2529%257D%2520framework%2520to%2520compare%2520embeddings%2520and%2520identify%2520their%250Adifferences%2520in%2520clustering%2520a%2520reference%2520dataset.%2520Our%2520approach%2520examines%2520the%2520kernel%250Amatrices%2520derived%2520from%2520two%2520embeddings%2520and%2520leverages%2520the%2520eigendecomposition%2520of%250Athe%2520difference%2520kernel%2520matrix%2520to%2520detect%2520sample%2520clusters%2520that%2520are%2520captured%250Adifferently%2520by%2520the%2520two%2520embeddings.%2520We%2520present%2520a%2520scalable%2520implementation%2520of%2520this%250Akernel-based%2520approach%252C%2520with%2520computational%2520complexity%2520that%2520grows%2520linearly%2520with%250Athe%2520sample%2520size.%2520Furthermore%252C%2520we%2520introduce%2520an%2520optimization%2520problem%2520using%2520this%250Aframework%2520to%2520align%2520two%2520embeddings%252C%2520ensuring%2520that%2520clusters%2520identified%2520in%2520one%250Aembedding%2520are%2520also%2520captured%2520in%2520the%2520other%2520model.%2520We%2520provide%2520numerical%2520results%250Ademonstrating%2520the%2520SPEC%2527s%2520application%2520to%2520compare%2520and%2520align%2520embeddings%2520on%250Alarge-scale%2520datasets%2520such%2520as%2520ImageNet%2520and%2520MS-COCO.%2520The%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//mjalali.github.io/SPEC/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20an%20Explainable%20Comparison%20and%20Alignment%20of%20Feature%20Embeddings&entry.906535625=Mohammad%20Jalali%20and%20Bahar%20Dibaei%20Nia%20and%20Farzan%20Farnia&entry.1292438233=%20%20While%20several%20feature%20embedding%20models%20have%20been%20developed%20in%20the%20literature%2C%0Acomparisons%20of%20these%20embeddings%20have%20largely%20focused%20on%20their%20numerical%0Aperformance%20in%20classification-related%20downstream%20applications.%20However%2C%20an%0Ainterpretable%20comparison%20of%20different%20embeddings%20requires%20identifying%20and%0Aanalyzing%20mismatches%20between%20sample%20groups%20clustered%20within%20the%20embedding%0Aspaces.%20In%20this%20work%2C%20we%20propose%20the%20%5Cemph%7BSpectral%20Pairwise%20Embedding%0AComparison%20%28SPEC%29%7D%20framework%20to%20compare%20embeddings%20and%20identify%20their%0Adifferences%20in%20clustering%20a%20reference%20dataset.%20Our%20approach%20examines%20the%20kernel%0Amatrices%20derived%20from%20two%20embeddings%20and%20leverages%20the%20eigendecomposition%20of%0Athe%20difference%20kernel%20matrix%20to%20detect%20sample%20clusters%20that%20are%20captured%0Adifferently%20by%20the%20two%20embeddings.%20We%20present%20a%20scalable%20implementation%20of%20this%0Akernel-based%20approach%2C%20with%20computational%20complexity%20that%20grows%20linearly%20with%0Athe%20sample%20size.%20Furthermore%2C%20we%20introduce%20an%20optimization%20problem%20using%20this%0Aframework%20to%20align%20two%20embeddings%2C%20ensuring%20that%20clusters%20identified%20in%20one%0Aembedding%20are%20also%20captured%20in%20the%20other%20model.%20We%20provide%20numerical%20results%0Ademonstrating%20the%20SPEC%27s%20application%20to%20compare%20and%20align%20embeddings%20on%0Alarge-scale%20datasets%20such%20as%20ImageNet%20and%20MS-COCO.%20The%20project%20page%20is%0Aavailable%20at%20https%3A//mjalali.github.io/SPEC/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06231v2&entry.124074799=Read"},
{"title": "TiCoSS: Tightening the Coupling between Semantic Segmentation and Stereo\n  Matching within A Joint Learning Framework", "author": "Guanfeng Tang and Zhiyuan Wu and Jiahang Li and Ping Zhong and We Ye and Xieyuanli Chen and Huiming Lu and Rui Fan", "abstract": "  Semantic segmentation and stereo matching, respectively analogous to the\nventral and dorsal streams in our human brain, are two key components of\nautonomous driving perception systems. Addressing these two tasks with separate\nnetworks is no longer the mainstream direction in developing computer vision\nalgorithms, particularly with the recent advances in large vision models and\nembodied artificial intelligence. The trend is shifting towards combining them\nwithin a joint learning framework, especially emphasizing feature sharing\nbetween the two tasks. The major contributions of this study lie in\ncomprehensively tightening the coupling between semantic segmentation and\nstereo matching. Specifically, this study introduces three novelties: (1) a\ntightly coupled, gated feature fusion strategy, (2) a hierarchical deep\nsupervision strategy, and (3) a coupling tightening loss function. The combined\nuse of these technical contributions results in TiCoSS, a state-of-the-art\njoint learning framework that simultaneously tackles semantic segmentation and\nstereo matching. Through extensive experiments on the KITTI and vKITTI2\ndatasets, along with qualitative and quantitative analyses, we validate the\neffectiveness of our developed strategies and loss function, and demonstrate\nits superior performance compared to prior arts, with a notable increase in\nmIoU by over 9%. Our source code will be publicly available at\nmias.group/TiCoSS upon publication.\n", "link": "http://arxiv.org/abs/2407.18038v4", "date": "2025-07-03", "relevancy": 2.3132, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5817}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework&body=Title%3A%20TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework%0AAuthor%3A%20Guanfeng%20Tang%20and%20Zhiyuan%20Wu%20and%20Jiahang%20Li%20and%20Ping%20Zhong%20and%20We%20Ye%20and%20Xieyuanli%20Chen%20and%20Huiming%20Lu%20and%20Rui%20Fan%0AAbstract%3A%20%20%20Semantic%20segmentation%20and%20stereo%20matching%2C%20respectively%20analogous%20to%20the%0Aventral%20and%20dorsal%20streams%20in%20our%20human%20brain%2C%20are%20two%20key%20components%20of%0Aautonomous%20driving%20perception%20systems.%20Addressing%20these%20two%20tasks%20with%20separate%0Anetworks%20is%20no%20longer%20the%20mainstream%20direction%20in%20developing%20computer%20vision%0Aalgorithms%2C%20particularly%20with%20the%20recent%20advances%20in%20large%20vision%20models%20and%0Aembodied%20artificial%20intelligence.%20The%20trend%20is%20shifting%20towards%20combining%20them%0Awithin%20a%20joint%20learning%20framework%2C%20especially%20emphasizing%20feature%20sharing%0Abetween%20the%20two%20tasks.%20The%20major%20contributions%20of%20this%20study%20lie%20in%0Acomprehensively%20tightening%20the%20coupling%20between%20semantic%20segmentation%20and%0Astereo%20matching.%20Specifically%2C%20this%20study%20introduces%20three%20novelties%3A%20%281%29%20a%0Atightly%20coupled%2C%20gated%20feature%20fusion%20strategy%2C%20%282%29%20a%20hierarchical%20deep%0Asupervision%20strategy%2C%20and%20%283%29%20a%20coupling%20tightening%20loss%20function.%20The%20combined%0Ause%20of%20these%20technical%20contributions%20results%20in%20TiCoSS%2C%20a%20state-of-the-art%0Ajoint%20learning%20framework%20that%20simultaneously%20tackles%20semantic%20segmentation%20and%0Astereo%20matching.%20Through%20extensive%20experiments%20on%20the%20KITTI%20and%20vKITTI2%0Adatasets%2C%20along%20with%20qualitative%20and%20quantitative%20analyses%2C%20we%20validate%20the%0Aeffectiveness%20of%20our%20developed%20strategies%20and%20loss%20function%2C%20and%20demonstrate%0Aits%20superior%20performance%20compared%20to%20prior%20arts%2C%20with%20a%20notable%20increase%20in%0AmIoU%20by%20over%209%25.%20Our%20source%20code%20will%20be%20publicly%20available%20at%0Amias.group/TiCoSS%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18038v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiCoSS%253A%2520Tightening%2520the%2520Coupling%2520between%2520Semantic%2520Segmentation%2520and%2520Stereo%250A%2520%2520Matching%2520within%2520A%2520Joint%2520Learning%2520Framework%26entry.906535625%3DGuanfeng%2520Tang%2520and%2520Zhiyuan%2520Wu%2520and%2520Jiahang%2520Li%2520and%2520Ping%2520Zhong%2520and%2520We%2520Ye%2520and%2520Xieyuanli%2520Chen%2520and%2520Huiming%2520Lu%2520and%2520Rui%2520Fan%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520and%2520stereo%2520matching%252C%2520respectively%2520analogous%2520to%2520the%250Aventral%2520and%2520dorsal%2520streams%2520in%2520our%2520human%2520brain%252C%2520are%2520two%2520key%2520components%2520of%250Aautonomous%2520driving%2520perception%2520systems.%2520Addressing%2520these%2520two%2520tasks%2520with%2520separate%250Anetworks%2520is%2520no%2520longer%2520the%2520mainstream%2520direction%2520in%2520developing%2520computer%2520vision%250Aalgorithms%252C%2520particularly%2520with%2520the%2520recent%2520advances%2520in%2520large%2520vision%2520models%2520and%250Aembodied%2520artificial%2520intelligence.%2520The%2520trend%2520is%2520shifting%2520towards%2520combining%2520them%250Awithin%2520a%2520joint%2520learning%2520framework%252C%2520especially%2520emphasizing%2520feature%2520sharing%250Abetween%2520the%2520two%2520tasks.%2520The%2520major%2520contributions%2520of%2520this%2520study%2520lie%2520in%250Acomprehensively%2520tightening%2520the%2520coupling%2520between%2520semantic%2520segmentation%2520and%250Astereo%2520matching.%2520Specifically%252C%2520this%2520study%2520introduces%2520three%2520novelties%253A%2520%25281%2529%2520a%250Atightly%2520coupled%252C%2520gated%2520feature%2520fusion%2520strategy%252C%2520%25282%2529%2520a%2520hierarchical%2520deep%250Asupervision%2520strategy%252C%2520and%2520%25283%2529%2520a%2520coupling%2520tightening%2520loss%2520function.%2520The%2520combined%250Ause%2520of%2520these%2520technical%2520contributions%2520results%2520in%2520TiCoSS%252C%2520a%2520state-of-the-art%250Ajoint%2520learning%2520framework%2520that%2520simultaneously%2520tackles%2520semantic%2520segmentation%2520and%250Astereo%2520matching.%2520Through%2520extensive%2520experiments%2520on%2520the%2520KITTI%2520and%2520vKITTI2%250Adatasets%252C%2520along%2520with%2520qualitative%2520and%2520quantitative%2520analyses%252C%2520we%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520developed%2520strategies%2520and%2520loss%2520function%252C%2520and%2520demonstrate%250Aits%2520superior%2520performance%2520compared%2520to%2520prior%2520arts%252C%2520with%2520a%2520notable%2520increase%2520in%250AmIoU%2520by%2520over%25209%2525.%2520Our%2520source%2520code%2520will%2520be%2520publicly%2520available%2520at%250Amias.group/TiCoSS%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18038v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework&entry.906535625=Guanfeng%20Tang%20and%20Zhiyuan%20Wu%20and%20Jiahang%20Li%20and%20Ping%20Zhong%20and%20We%20Ye%20and%20Xieyuanli%20Chen%20and%20Huiming%20Lu%20and%20Rui%20Fan&entry.1292438233=%20%20Semantic%20segmentation%20and%20stereo%20matching%2C%20respectively%20analogous%20to%20the%0Aventral%20and%20dorsal%20streams%20in%20our%20human%20brain%2C%20are%20two%20key%20components%20of%0Aautonomous%20driving%20perception%20systems.%20Addressing%20these%20two%20tasks%20with%20separate%0Anetworks%20is%20no%20longer%20the%20mainstream%20direction%20in%20developing%20computer%20vision%0Aalgorithms%2C%20particularly%20with%20the%20recent%20advances%20in%20large%20vision%20models%20and%0Aembodied%20artificial%20intelligence.%20The%20trend%20is%20shifting%20towards%20combining%20them%0Awithin%20a%20joint%20learning%20framework%2C%20especially%20emphasizing%20feature%20sharing%0Abetween%20the%20two%20tasks.%20The%20major%20contributions%20of%20this%20study%20lie%20in%0Acomprehensively%20tightening%20the%20coupling%20between%20semantic%20segmentation%20and%0Astereo%20matching.%20Specifically%2C%20this%20study%20introduces%20three%20novelties%3A%20%281%29%20a%0Atightly%20coupled%2C%20gated%20feature%20fusion%20strategy%2C%20%282%29%20a%20hierarchical%20deep%0Asupervision%20strategy%2C%20and%20%283%29%20a%20coupling%20tightening%20loss%20function.%20The%20combined%0Ause%20of%20these%20technical%20contributions%20results%20in%20TiCoSS%2C%20a%20state-of-the-art%0Ajoint%20learning%20framework%20that%20simultaneously%20tackles%20semantic%20segmentation%20and%0Astereo%20matching.%20Through%20extensive%20experiments%20on%20the%20KITTI%20and%20vKITTI2%0Adatasets%2C%20along%20with%20qualitative%20and%20quantitative%20analyses%2C%20we%20validate%20the%0Aeffectiveness%20of%20our%20developed%20strategies%20and%20loss%20function%2C%20and%20demonstrate%0Aits%20superior%20performance%20compared%20to%20prior%20arts%2C%20with%20a%20notable%20increase%20in%0AmIoU%20by%20over%209%25.%20Our%20source%20code%20will%20be%20publicly%20available%20at%0Amias.group/TiCoSS%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18038v4&entry.124074799=Read"},
{"title": "GRIP: A General Robotic Incremental Potential Contact Simulation Dataset\n  for Unified Deformable-Rigid Coupled Grasping", "author": "Siyu Ma and Wenxin Du and Chang Yu and Ying Jiang and Zeshun Zong and Tianyi Xie and Yunuo Chen and Yin Yang and Xuchen Han and Chenfanfu Jiang", "abstract": "  Grasping is fundamental to robotic manipulation, and recent advances in\nlarge-scale grasping datasets have provided essential training data and\nevaluation benchmarks, accelerating the development of learning-based methods\nfor robust object grasping. However, most existing datasets exclude deformable\nbodies due to the lack of scalable, robust simulation pipelines, limiting the\ndevelopment of generalizable models for compliant grippers and soft\nmanipulands. To address these challenges, we present GRIP, a General Robotic\nIncremental Potential contact simulation dataset for universal grasping. GRIP\nleverages an optimized Incremental Potential Contact (IPC)-based simulator for\nmulti-environment data generation, achieving up to 48x speedup while ensuring\nefficient, intersection- and inversion-free simulations for compliant grippers\nand deformable objects. Our fully automated pipeline generates and evaluates\ndiverse grasp interactions across 1,200 objects and 100,000 grasp poses,\nincorporating both soft and rigid grippers. The GRIP dataset enables\napplications such as neural grasp generation and stress field prediction.\n", "link": "http://arxiv.org/abs/2503.05020v2", "date": "2025-07-03", "relevancy": 2.3118, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6482}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5464}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRIP%3A%20A%20General%20Robotic%20Incremental%20Potential%20Contact%20Simulation%20Dataset%0A%20%20for%20Unified%20Deformable-Rigid%20Coupled%20Grasping&body=Title%3A%20GRIP%3A%20A%20General%20Robotic%20Incremental%20Potential%20Contact%20Simulation%20Dataset%0A%20%20for%20Unified%20Deformable-Rigid%20Coupled%20Grasping%0AAuthor%3A%20Siyu%20Ma%20and%20Wenxin%20Du%20and%20Chang%20Yu%20and%20Ying%20Jiang%20and%20Zeshun%20Zong%20and%20Tianyi%20Xie%20and%20Yunuo%20Chen%20and%20Yin%20Yang%20and%20Xuchen%20Han%20and%20Chenfanfu%20Jiang%0AAbstract%3A%20%20%20Grasping%20is%20fundamental%20to%20robotic%20manipulation%2C%20and%20recent%20advances%20in%0Alarge-scale%20grasping%20datasets%20have%20provided%20essential%20training%20data%20and%0Aevaluation%20benchmarks%2C%20accelerating%20the%20development%20of%20learning-based%20methods%0Afor%20robust%20object%20grasping.%20However%2C%20most%20existing%20datasets%20exclude%20deformable%0Abodies%20due%20to%20the%20lack%20of%20scalable%2C%20robust%20simulation%20pipelines%2C%20limiting%20the%0Adevelopment%20of%20generalizable%20models%20for%20compliant%20grippers%20and%20soft%0Amanipulands.%20To%20address%20these%20challenges%2C%20we%20present%20GRIP%2C%20a%20General%20Robotic%0AIncremental%20Potential%20contact%20simulation%20dataset%20for%20universal%20grasping.%20GRIP%0Aleverages%20an%20optimized%20Incremental%20Potential%20Contact%20%28IPC%29-based%20simulator%20for%0Amulti-environment%20data%20generation%2C%20achieving%20up%20to%2048x%20speedup%20while%20ensuring%0Aefficient%2C%20intersection-%20and%20inversion-free%20simulations%20for%20compliant%20grippers%0Aand%20deformable%20objects.%20Our%20fully%20automated%20pipeline%20generates%20and%20evaluates%0Adiverse%20grasp%20interactions%20across%201%2C200%20objects%20and%20100%2C000%20grasp%20poses%2C%0Aincorporating%20both%20soft%20and%20rigid%20grippers.%20The%20GRIP%20dataset%20enables%0Aapplications%20such%20as%20neural%20grasp%20generation%20and%20stress%20field%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRIP%253A%2520A%2520General%2520Robotic%2520Incremental%2520Potential%2520Contact%2520Simulation%2520Dataset%250A%2520%2520for%2520Unified%2520Deformable-Rigid%2520Coupled%2520Grasping%26entry.906535625%3DSiyu%2520Ma%2520and%2520Wenxin%2520Du%2520and%2520Chang%2520Yu%2520and%2520Ying%2520Jiang%2520and%2520Zeshun%2520Zong%2520and%2520Tianyi%2520Xie%2520and%2520Yunuo%2520Chen%2520and%2520Yin%2520Yang%2520and%2520Xuchen%2520Han%2520and%2520Chenfanfu%2520Jiang%26entry.1292438233%3D%2520%2520Grasping%2520is%2520fundamental%2520to%2520robotic%2520manipulation%252C%2520and%2520recent%2520advances%2520in%250Alarge-scale%2520grasping%2520datasets%2520have%2520provided%2520essential%2520training%2520data%2520and%250Aevaluation%2520benchmarks%252C%2520accelerating%2520the%2520development%2520of%2520learning-based%2520methods%250Afor%2520robust%2520object%2520grasping.%2520However%252C%2520most%2520existing%2520datasets%2520exclude%2520deformable%250Abodies%2520due%2520to%2520the%2520lack%2520of%2520scalable%252C%2520robust%2520simulation%2520pipelines%252C%2520limiting%2520the%250Adevelopment%2520of%2520generalizable%2520models%2520for%2520compliant%2520grippers%2520and%2520soft%250Amanipulands.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520GRIP%252C%2520a%2520General%2520Robotic%250AIncremental%2520Potential%2520contact%2520simulation%2520dataset%2520for%2520universal%2520grasping.%2520GRIP%250Aleverages%2520an%2520optimized%2520Incremental%2520Potential%2520Contact%2520%2528IPC%2529-based%2520simulator%2520for%250Amulti-environment%2520data%2520generation%252C%2520achieving%2520up%2520to%252048x%2520speedup%2520while%2520ensuring%250Aefficient%252C%2520intersection-%2520and%2520inversion-free%2520simulations%2520for%2520compliant%2520grippers%250Aand%2520deformable%2520objects.%2520Our%2520fully%2520automated%2520pipeline%2520generates%2520and%2520evaluates%250Adiverse%2520grasp%2520interactions%2520across%25201%252C200%2520objects%2520and%2520100%252C000%2520grasp%2520poses%252C%250Aincorporating%2520both%2520soft%2520and%2520rigid%2520grippers.%2520The%2520GRIP%2520dataset%2520enables%250Aapplications%2520such%2520as%2520neural%2520grasp%2520generation%2520and%2520stress%2520field%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRIP%3A%20A%20General%20Robotic%20Incremental%20Potential%20Contact%20Simulation%20Dataset%0A%20%20for%20Unified%20Deformable-Rigid%20Coupled%20Grasping&entry.906535625=Siyu%20Ma%20and%20Wenxin%20Du%20and%20Chang%20Yu%20and%20Ying%20Jiang%20and%20Zeshun%20Zong%20and%20Tianyi%20Xie%20and%20Yunuo%20Chen%20and%20Yin%20Yang%20and%20Xuchen%20Han%20and%20Chenfanfu%20Jiang&entry.1292438233=%20%20Grasping%20is%20fundamental%20to%20robotic%20manipulation%2C%20and%20recent%20advances%20in%0Alarge-scale%20grasping%20datasets%20have%20provided%20essential%20training%20data%20and%0Aevaluation%20benchmarks%2C%20accelerating%20the%20development%20of%20learning-based%20methods%0Afor%20robust%20object%20grasping.%20However%2C%20most%20existing%20datasets%20exclude%20deformable%0Abodies%20due%20to%20the%20lack%20of%20scalable%2C%20robust%20simulation%20pipelines%2C%20limiting%20the%0Adevelopment%20of%20generalizable%20models%20for%20compliant%20grippers%20and%20soft%0Amanipulands.%20To%20address%20these%20challenges%2C%20we%20present%20GRIP%2C%20a%20General%20Robotic%0AIncremental%20Potential%20contact%20simulation%20dataset%20for%20universal%20grasping.%20GRIP%0Aleverages%20an%20optimized%20Incremental%20Potential%20Contact%20%28IPC%29-based%20simulator%20for%0Amulti-environment%20data%20generation%2C%20achieving%20up%20to%2048x%20speedup%20while%20ensuring%0Aefficient%2C%20intersection-%20and%20inversion-free%20simulations%20for%20compliant%20grippers%0Aand%20deformable%20objects.%20Our%20fully%20automated%20pipeline%20generates%20and%20evaluates%0Adiverse%20grasp%20interactions%20across%201%2C200%20objects%20and%20100%2C000%20grasp%20poses%2C%0Aincorporating%20both%20soft%20and%20rigid%20grippers.%20The%20GRIP%20dataset%20enables%0Aapplications%20such%20as%20neural%20grasp%20generation%20and%20stress%20field%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05020v2&entry.124074799=Read"},
{"title": "RGC-VQA: An Exploration Database for Robotic-Generated Video Quality\n  Assessment", "author": "Jianing Jin and Jiangyong Ying and Huiyu Duan and Liu Yang and Sijing Wu and Yunhao Li and Yushuo Zheng and Xiongkuo Min and Guangtao Zhai", "abstract": "  As camera-equipped robotic platforms become increasingly integrated into\ndaily life, robotic-generated videos have begun to appear on streaming media\nplatforms, enabling us to envision a future where humans and robots coexist. We\ninnovatively propose the concept of Robotic-Generated Content (RGC) to term\nthese videos generated from egocentric perspective of robots. The perceptual\nquality of RGC videos is critical in human-robot interaction scenarios, and RGC\nvideos exhibit unique distortions and visual requirements that differ markedly\nfrom those of professionally-generated content (PGC) videos and user-generated\ncontent (UGC) videos. However, dedicated research on quality assessment of RGC\nvideos is still lacking. To address this gap and to support broader robotic\napplications, we establish the first Robotic-Generated Content Database (RGCD),\nwhich contains a total of 2,100 videos drawn from three robot categories and\nsourced from diverse platforms. A subjective VQA experiment is conducted\nsubsequently to assess human visual perception of robotic-generated videos.\nFinally, we conduct a benchmark experiment to evaluate the performance of 11\nstate-of-the-art VQA models on our database. Experimental results reveal\nsignificant limitations in existing VQA models when applied to complex,\nrobotic-generated content, highlighting a critical need for RGC-specific VQA\nmodels. Our RGCD is publicly available at:\nhttps://github.com/IntMeGroup/RGC-VQA.\n", "link": "http://arxiv.org/abs/2506.23852v2", "date": "2025-07-03", "relevancy": 2.302, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5839}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5705}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGC-VQA%3A%20An%20Exploration%20Database%20for%20Robotic-Generated%20Video%20Quality%0A%20%20Assessment&body=Title%3A%20RGC-VQA%3A%20An%20Exploration%20Database%20for%20Robotic-Generated%20Video%20Quality%0A%20%20Assessment%0AAuthor%3A%20Jianing%20Jin%20and%20Jiangyong%20Ying%20and%20Huiyu%20Duan%20and%20Liu%20Yang%20and%20Sijing%20Wu%20and%20Yunhao%20Li%20and%20Yushuo%20Zheng%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20As%20camera-equipped%20robotic%20platforms%20become%20increasingly%20integrated%20into%0Adaily%20life%2C%20robotic-generated%20videos%20have%20begun%20to%20appear%20on%20streaming%20media%0Aplatforms%2C%20enabling%20us%20to%20envision%20a%20future%20where%20humans%20and%20robots%20coexist.%20We%0Ainnovatively%20propose%20the%20concept%20of%20Robotic-Generated%20Content%20%28RGC%29%20to%20term%0Athese%20videos%20generated%20from%20egocentric%20perspective%20of%20robots.%20The%20perceptual%0Aquality%20of%20RGC%20videos%20is%20critical%20in%20human-robot%20interaction%20scenarios%2C%20and%20RGC%0Avideos%20exhibit%20unique%20distortions%20and%20visual%20requirements%20that%20differ%20markedly%0Afrom%20those%20of%20professionally-generated%20content%20%28PGC%29%20videos%20and%20user-generated%0Acontent%20%28UGC%29%20videos.%20However%2C%20dedicated%20research%20on%20quality%20assessment%20of%20RGC%0Avideos%20is%20still%20lacking.%20To%20address%20this%20gap%20and%20to%20support%20broader%20robotic%0Aapplications%2C%20we%20establish%20the%20first%20Robotic-Generated%20Content%20Database%20%28RGCD%29%2C%0Awhich%20contains%20a%20total%20of%202%2C100%20videos%20drawn%20from%20three%20robot%20categories%20and%0Asourced%20from%20diverse%20platforms.%20A%20subjective%20VQA%20experiment%20is%20conducted%0Asubsequently%20to%20assess%20human%20visual%20perception%20of%20robotic-generated%20videos.%0AFinally%2C%20we%20conduct%20a%20benchmark%20experiment%20to%20evaluate%20the%20performance%20of%2011%0Astate-of-the-art%20VQA%20models%20on%20our%20database.%20Experimental%20results%20reveal%0Asignificant%20limitations%20in%20existing%20VQA%20models%20when%20applied%20to%20complex%2C%0Arobotic-generated%20content%2C%20highlighting%20a%20critical%20need%20for%20RGC-specific%20VQA%0Amodels.%20Our%20RGCD%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/IntMeGroup/RGC-VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23852v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGC-VQA%253A%2520An%2520Exploration%2520Database%2520for%2520Robotic-Generated%2520Video%2520Quality%250A%2520%2520Assessment%26entry.906535625%3DJianing%2520Jin%2520and%2520Jiangyong%2520Ying%2520and%2520Huiyu%2520Duan%2520and%2520Liu%2520Yang%2520and%2520Sijing%2520Wu%2520and%2520Yunhao%2520Li%2520and%2520Yushuo%2520Zheng%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520As%2520camera-equipped%2520robotic%2520platforms%2520become%2520increasingly%2520integrated%2520into%250Adaily%2520life%252C%2520robotic-generated%2520videos%2520have%2520begun%2520to%2520appear%2520on%2520streaming%2520media%250Aplatforms%252C%2520enabling%2520us%2520to%2520envision%2520a%2520future%2520where%2520humans%2520and%2520robots%2520coexist.%2520We%250Ainnovatively%2520propose%2520the%2520concept%2520of%2520Robotic-Generated%2520Content%2520%2528RGC%2529%2520to%2520term%250Athese%2520videos%2520generated%2520from%2520egocentric%2520perspective%2520of%2520robots.%2520The%2520perceptual%250Aquality%2520of%2520RGC%2520videos%2520is%2520critical%2520in%2520human-robot%2520interaction%2520scenarios%252C%2520and%2520RGC%250Avideos%2520exhibit%2520unique%2520distortions%2520and%2520visual%2520requirements%2520that%2520differ%2520markedly%250Afrom%2520those%2520of%2520professionally-generated%2520content%2520%2528PGC%2529%2520videos%2520and%2520user-generated%250Acontent%2520%2528UGC%2529%2520videos.%2520However%252C%2520dedicated%2520research%2520on%2520quality%2520assessment%2520of%2520RGC%250Avideos%2520is%2520still%2520lacking.%2520To%2520address%2520this%2520gap%2520and%2520to%2520support%2520broader%2520robotic%250Aapplications%252C%2520we%2520establish%2520the%2520first%2520Robotic-Generated%2520Content%2520Database%2520%2528RGCD%2529%252C%250Awhich%2520contains%2520a%2520total%2520of%25202%252C100%2520videos%2520drawn%2520from%2520three%2520robot%2520categories%2520and%250Asourced%2520from%2520diverse%2520platforms.%2520A%2520subjective%2520VQA%2520experiment%2520is%2520conducted%250Asubsequently%2520to%2520assess%2520human%2520visual%2520perception%2520of%2520robotic-generated%2520videos.%250AFinally%252C%2520we%2520conduct%2520a%2520benchmark%2520experiment%2520to%2520evaluate%2520the%2520performance%2520of%252011%250Astate-of-the-art%2520VQA%2520models%2520on%2520our%2520database.%2520Experimental%2520results%2520reveal%250Asignificant%2520limitations%2520in%2520existing%2520VQA%2520models%2520when%2520applied%2520to%2520complex%252C%250Arobotic-generated%2520content%252C%2520highlighting%2520a%2520critical%2520need%2520for%2520RGC-specific%2520VQA%250Amodels.%2520Our%2520RGCD%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/IntMeGroup/RGC-VQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23852v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGC-VQA%3A%20An%20Exploration%20Database%20for%20Robotic-Generated%20Video%20Quality%0A%20%20Assessment&entry.906535625=Jianing%20Jin%20and%20Jiangyong%20Ying%20and%20Huiyu%20Duan%20and%20Liu%20Yang%20and%20Sijing%20Wu%20and%20Yunhao%20Li%20and%20Yushuo%20Zheng%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20As%20camera-equipped%20robotic%20platforms%20become%20increasingly%20integrated%20into%0Adaily%20life%2C%20robotic-generated%20videos%20have%20begun%20to%20appear%20on%20streaming%20media%0Aplatforms%2C%20enabling%20us%20to%20envision%20a%20future%20where%20humans%20and%20robots%20coexist.%20We%0Ainnovatively%20propose%20the%20concept%20of%20Robotic-Generated%20Content%20%28RGC%29%20to%20term%0Athese%20videos%20generated%20from%20egocentric%20perspective%20of%20robots.%20The%20perceptual%0Aquality%20of%20RGC%20videos%20is%20critical%20in%20human-robot%20interaction%20scenarios%2C%20and%20RGC%0Avideos%20exhibit%20unique%20distortions%20and%20visual%20requirements%20that%20differ%20markedly%0Afrom%20those%20of%20professionally-generated%20content%20%28PGC%29%20videos%20and%20user-generated%0Acontent%20%28UGC%29%20videos.%20However%2C%20dedicated%20research%20on%20quality%20assessment%20of%20RGC%0Avideos%20is%20still%20lacking.%20To%20address%20this%20gap%20and%20to%20support%20broader%20robotic%0Aapplications%2C%20we%20establish%20the%20first%20Robotic-Generated%20Content%20Database%20%28RGCD%29%2C%0Awhich%20contains%20a%20total%20of%202%2C100%20videos%20drawn%20from%20three%20robot%20categories%20and%0Asourced%20from%20diverse%20platforms.%20A%20subjective%20VQA%20experiment%20is%20conducted%0Asubsequently%20to%20assess%20human%20visual%20perception%20of%20robotic-generated%20videos.%0AFinally%2C%20we%20conduct%20a%20benchmark%20experiment%20to%20evaluate%20the%20performance%20of%2011%0Astate-of-the-art%20VQA%20models%20on%20our%20database.%20Experimental%20results%20reveal%0Asignificant%20limitations%20in%20existing%20VQA%20models%20when%20applied%20to%20complex%2C%0Arobotic-generated%20content%2C%20highlighting%20a%20critical%20need%20for%20RGC-specific%20VQA%0Amodels.%20Our%20RGCD%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/IntMeGroup/RGC-VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23852v2&entry.124074799=Read"},
{"title": "Down with the Hierarchy: The 'H' in HNSW Stands for \"Hubs\"", "author": "Blaise Munyampirwa and Vihan Lakshman and Benjamin Coleman", "abstract": "  Driven by recent breakthrough advances in neural representation learning,\napproximate near-neighbor (ANN) search over vector embeddings has emerged as a\ncritical computational workload. With the introduction of the seminal\nHierarchical Navigable Small World (HNSW) algorithm, graph-based indexes have\nestablished themselves as the overwhelmingly dominant paradigm for efficient\nand scalable ANN search. As the name suggests, HNSW searches a layered\nhierarchical graph to quickly identify neighborhoods of similar points to a\ngiven query vector. But is this hierarchy even necessary? A rigorous\nexperimental analysis to answer this question would provide valuable insights\ninto the nature of algorithm design for ANN search and motivate directions for\nfuture work in this increasingly crucial domain. We conduct an extensive\nbenchmarking study covering more large-scale datasets than prior investigations\nof this question. We ultimately find that a flat navigable small world graph\ngraph retains all of the benefits of HNSW on high-dimensional datasets, with\nlatency and recall performance essentially \\emph{identical} to the original\nalgorithm but with less memory overhead. Furthermore, we go a step further and\nstudy \\emph{why} the hierarchy of HNSW provides no benefit in high dimensions,\nhypothesizing that navigable small world graphs contain a well-connected,\nfrequently traversed ``highway\" of hub nodes that maintain the same purported\nfunction as the hierarchical layers. We present compelling empirical evidence\nthat the \\emph{Hub Highway Hypothesis} holds for real datasets and investigate\nthe mechanisms by which the highway forms. The implications of this hypothesis\nmay also provide future research directions in developing enhancements to\ngraph-based ANN search.\n", "link": "http://arxiv.org/abs/2412.01940v3", "date": "2025-07-03", "relevancy": 2.2914, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4607}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Down%20with%20the%20Hierarchy%3A%20The%20%27H%27%20in%20HNSW%20Stands%20for%20%22Hubs%22&body=Title%3A%20Down%20with%20the%20Hierarchy%3A%20The%20%27H%27%20in%20HNSW%20Stands%20for%20%22Hubs%22%0AAuthor%3A%20Blaise%20Munyampirwa%20and%20Vihan%20Lakshman%20and%20Benjamin%20Coleman%0AAbstract%3A%20%20%20Driven%20by%20recent%20breakthrough%20advances%20in%20neural%20representation%20learning%2C%0Aapproximate%20near-neighbor%20%28ANN%29%20search%20over%20vector%20embeddings%20has%20emerged%20as%20a%0Acritical%20computational%20workload.%20With%20the%20introduction%20of%20the%20seminal%0AHierarchical%20Navigable%20Small%20World%20%28HNSW%29%20algorithm%2C%20graph-based%20indexes%20have%0Aestablished%20themselves%20as%20the%20overwhelmingly%20dominant%20paradigm%20for%20efficient%0Aand%20scalable%20ANN%20search.%20As%20the%20name%20suggests%2C%20HNSW%20searches%20a%20layered%0Ahierarchical%20graph%20to%20quickly%20identify%20neighborhoods%20of%20similar%20points%20to%20a%0Agiven%20query%20vector.%20But%20is%20this%20hierarchy%20even%20necessary%3F%20A%20rigorous%0Aexperimental%20analysis%20to%20answer%20this%20question%20would%20provide%20valuable%20insights%0Ainto%20the%20nature%20of%20algorithm%20design%20for%20ANN%20search%20and%20motivate%20directions%20for%0Afuture%20work%20in%20this%20increasingly%20crucial%20domain.%20We%20conduct%20an%20extensive%0Abenchmarking%20study%20covering%20more%20large-scale%20datasets%20than%20prior%20investigations%0Aof%20this%20question.%20We%20ultimately%20find%20that%20a%20flat%20navigable%20small%20world%20graph%0Agraph%20retains%20all%20of%20the%20benefits%20of%20HNSW%20on%20high-dimensional%20datasets%2C%20with%0Alatency%20and%20recall%20performance%20essentially%20%5Cemph%7Bidentical%7D%20to%20the%20original%0Aalgorithm%20but%20with%20less%20memory%20overhead.%20Furthermore%2C%20we%20go%20a%20step%20further%20and%0Astudy%20%5Cemph%7Bwhy%7D%20the%20hierarchy%20of%20HNSW%20provides%20no%20benefit%20in%20high%20dimensions%2C%0Ahypothesizing%20that%20navigable%20small%20world%20graphs%20contain%20a%20well-connected%2C%0Afrequently%20traversed%20%60%60highway%22%20of%20hub%20nodes%20that%20maintain%20the%20same%20purported%0Afunction%20as%20the%20hierarchical%20layers.%20We%20present%20compelling%20empirical%20evidence%0Athat%20the%20%5Cemph%7BHub%20Highway%20Hypothesis%7D%20holds%20for%20real%20datasets%20and%20investigate%0Athe%20mechanisms%20by%20which%20the%20highway%20forms.%20The%20implications%20of%20this%20hypothesis%0Amay%20also%20provide%20future%20research%20directions%20in%20developing%20enhancements%20to%0Agraph-based%20ANN%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDown%2520with%2520the%2520Hierarchy%253A%2520The%2520%2527H%2527%2520in%2520HNSW%2520Stands%2520for%2520%2522Hubs%2522%26entry.906535625%3DBlaise%2520Munyampirwa%2520and%2520Vihan%2520Lakshman%2520and%2520Benjamin%2520Coleman%26entry.1292438233%3D%2520%2520Driven%2520by%2520recent%2520breakthrough%2520advances%2520in%2520neural%2520representation%2520learning%252C%250Aapproximate%2520near-neighbor%2520%2528ANN%2529%2520search%2520over%2520vector%2520embeddings%2520has%2520emerged%2520as%2520a%250Acritical%2520computational%2520workload.%2520With%2520the%2520introduction%2520of%2520the%2520seminal%250AHierarchical%2520Navigable%2520Small%2520World%2520%2528HNSW%2529%2520algorithm%252C%2520graph-based%2520indexes%2520have%250Aestablished%2520themselves%2520as%2520the%2520overwhelmingly%2520dominant%2520paradigm%2520for%2520efficient%250Aand%2520scalable%2520ANN%2520search.%2520As%2520the%2520name%2520suggests%252C%2520HNSW%2520searches%2520a%2520layered%250Ahierarchical%2520graph%2520to%2520quickly%2520identify%2520neighborhoods%2520of%2520similar%2520points%2520to%2520a%250Agiven%2520query%2520vector.%2520But%2520is%2520this%2520hierarchy%2520even%2520necessary%253F%2520A%2520rigorous%250Aexperimental%2520analysis%2520to%2520answer%2520this%2520question%2520would%2520provide%2520valuable%2520insights%250Ainto%2520the%2520nature%2520of%2520algorithm%2520design%2520for%2520ANN%2520search%2520and%2520motivate%2520directions%2520for%250Afuture%2520work%2520in%2520this%2520increasingly%2520crucial%2520domain.%2520We%2520conduct%2520an%2520extensive%250Abenchmarking%2520study%2520covering%2520more%2520large-scale%2520datasets%2520than%2520prior%2520investigations%250Aof%2520this%2520question.%2520We%2520ultimately%2520find%2520that%2520a%2520flat%2520navigable%2520small%2520world%2520graph%250Agraph%2520retains%2520all%2520of%2520the%2520benefits%2520of%2520HNSW%2520on%2520high-dimensional%2520datasets%252C%2520with%250Alatency%2520and%2520recall%2520performance%2520essentially%2520%255Cemph%257Bidentical%257D%2520to%2520the%2520original%250Aalgorithm%2520but%2520with%2520less%2520memory%2520overhead.%2520Furthermore%252C%2520we%2520go%2520a%2520step%2520further%2520and%250Astudy%2520%255Cemph%257Bwhy%257D%2520the%2520hierarchy%2520of%2520HNSW%2520provides%2520no%2520benefit%2520in%2520high%2520dimensions%252C%250Ahypothesizing%2520that%2520navigable%2520small%2520world%2520graphs%2520contain%2520a%2520well-connected%252C%250Afrequently%2520traversed%2520%2560%2560highway%2522%2520of%2520hub%2520nodes%2520that%2520maintain%2520the%2520same%2520purported%250Afunction%2520as%2520the%2520hierarchical%2520layers.%2520We%2520present%2520compelling%2520empirical%2520evidence%250Athat%2520the%2520%255Cemph%257BHub%2520Highway%2520Hypothesis%257D%2520holds%2520for%2520real%2520datasets%2520and%2520investigate%250Athe%2520mechanisms%2520by%2520which%2520the%2520highway%2520forms.%2520The%2520implications%2520of%2520this%2520hypothesis%250Amay%2520also%2520provide%2520future%2520research%2520directions%2520in%2520developing%2520enhancements%2520to%250Agraph-based%2520ANN%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Down%20with%20the%20Hierarchy%3A%20The%20%27H%27%20in%20HNSW%20Stands%20for%20%22Hubs%22&entry.906535625=Blaise%20Munyampirwa%20and%20Vihan%20Lakshman%20and%20Benjamin%20Coleman&entry.1292438233=%20%20Driven%20by%20recent%20breakthrough%20advances%20in%20neural%20representation%20learning%2C%0Aapproximate%20near-neighbor%20%28ANN%29%20search%20over%20vector%20embeddings%20has%20emerged%20as%20a%0Acritical%20computational%20workload.%20With%20the%20introduction%20of%20the%20seminal%0AHierarchical%20Navigable%20Small%20World%20%28HNSW%29%20algorithm%2C%20graph-based%20indexes%20have%0Aestablished%20themselves%20as%20the%20overwhelmingly%20dominant%20paradigm%20for%20efficient%0Aand%20scalable%20ANN%20search.%20As%20the%20name%20suggests%2C%20HNSW%20searches%20a%20layered%0Ahierarchical%20graph%20to%20quickly%20identify%20neighborhoods%20of%20similar%20points%20to%20a%0Agiven%20query%20vector.%20But%20is%20this%20hierarchy%20even%20necessary%3F%20A%20rigorous%0Aexperimental%20analysis%20to%20answer%20this%20question%20would%20provide%20valuable%20insights%0Ainto%20the%20nature%20of%20algorithm%20design%20for%20ANN%20search%20and%20motivate%20directions%20for%0Afuture%20work%20in%20this%20increasingly%20crucial%20domain.%20We%20conduct%20an%20extensive%0Abenchmarking%20study%20covering%20more%20large-scale%20datasets%20than%20prior%20investigations%0Aof%20this%20question.%20We%20ultimately%20find%20that%20a%20flat%20navigable%20small%20world%20graph%0Agraph%20retains%20all%20of%20the%20benefits%20of%20HNSW%20on%20high-dimensional%20datasets%2C%20with%0Alatency%20and%20recall%20performance%20essentially%20%5Cemph%7Bidentical%7D%20to%20the%20original%0Aalgorithm%20but%20with%20less%20memory%20overhead.%20Furthermore%2C%20we%20go%20a%20step%20further%20and%0Astudy%20%5Cemph%7Bwhy%7D%20the%20hierarchy%20of%20HNSW%20provides%20no%20benefit%20in%20high%20dimensions%2C%0Ahypothesizing%20that%20navigable%20small%20world%20graphs%20contain%20a%20well-connected%2C%0Afrequently%20traversed%20%60%60highway%22%20of%20hub%20nodes%20that%20maintain%20the%20same%20purported%0Afunction%20as%20the%20hierarchical%20layers.%20We%20present%20compelling%20empirical%20evidence%0Athat%20the%20%5Cemph%7BHub%20Highway%20Hypothesis%7D%20holds%20for%20real%20datasets%20and%20investigate%0Athe%20mechanisms%20by%20which%20the%20highway%20forms.%20The%20implications%20of%20this%20hypothesis%0Amay%20also%20provide%20future%20research%20directions%20in%20developing%20enhancements%20to%0Agraph-based%20ANN%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01940v3&entry.124074799=Read"},
{"title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks", "author": "Kim-Celine Kahl and Selen Erkan and Jeremias Traub and Carsten T. L\u00fcth and Klaus Maier-Hein and Lena Maier-Hein and Paul F. Jaeger", "abstract": "  Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa.\n", "link": "http://arxiv.org/abs/2411.19688v3", "date": "2025-07-03", "relevancy": 2.2899, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks&body=Title%3A%20SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks%0AAuthor%3A%20Kim-Celine%20Kahl%20and%20Selen%20Erkan%20and%20Jeremias%20Traub%20and%20Carsten%20T.%20L%C3%BCth%20and%20Klaus%20Maier-Hein%20and%20Lena%20Maier-Hein%20and%20Paul%20F.%20Jaeger%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20great%20potential%20in%20medical%20tasks%2C%20like%0AVisual%20Question%20Answering%20%28VQA%29%2C%20where%20they%20could%20act%20as%20interactive%20assistants%0Afor%20both%20patients%20and%20clinicians.%20Yet%20their%20robustness%20to%20distribution%20shifts%0Aon%20unseen%20data%20remains%20a%20key%20concern%20for%20safe%20deployment.%20Evaluating%20such%0Arobustness%20requires%20a%20controlled%20experimental%20setup%20that%20allows%20for%20systematic%0Ainsights%20into%20the%20model%27s%20behavior.%20However%2C%20we%20demonstrate%20that%20current%20setups%0Afail%20to%20offer%20sufficiently%20thorough%20evaluations.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20a%20novel%20framework%2C%20called%20SURE-VQA%2C%20centered%20around%20three%20key%0Arequirements%20to%20overcome%20current%20pitfalls%20and%20systematically%20analyze%20VLM%0Arobustness%3A%201%29%20Since%20robustness%20on%20synthetic%20shifts%20does%20not%20necessarily%0Atranslate%20to%20real-world%20shifts%2C%20it%20should%20be%20measured%20on%20real-world%20shifts%20that%0Aare%20inherent%20to%20the%20VQA%20data%3B%202%29%20Traditional%20token-matching%20metrics%20often%20fail%0Ato%20capture%20underlying%20semantics%2C%20necessitating%20the%20use%20of%20large%20language%20models%0A%28LLMs%29%20for%20more%20accurate%20semantic%20evaluation%3B%203%29%20Model%20performance%20often%20lacks%0Ainterpretability%20due%20to%20missing%20sanity%20baselines%2C%20thus%20meaningful%20baselines%0Ashould%20be%20reported%20that%20allow%20assessing%20the%20multimodal%20impact%20on%20the%20VLM.%20To%0Ademonstrate%20the%20relevance%20of%20this%20framework%2C%20we%20conduct%20a%20study%20on%20the%0Arobustness%20of%20various%20Fine-Tuning%20%28FT%29%20methods%20across%20three%20medical%20datasets%0Awith%20four%20types%20of%20distribution%20shifts.%20Our%20study%20highlights%20key%20insights%20into%0Arobustness%3A%201%29%20No%20FT%20method%20consistently%20outperforms%20others%20in%20robustness%2C%20and%0A2%29%20robustness%20trends%20are%20more%20stable%20across%20FT%20methods%20than%20across%20distribution%0Ashifts.%20Additionally%2C%20we%20find%20that%20simple%20sanity%20baselines%20that%20do%20not%20use%20the%0Aimage%20data%20can%20perform%20surprisingly%20well%20and%20confirm%20LoRA%20as%20the%0Abest-performing%20FT%20method%20on%20in-distribution%20data.%20Code%20is%20provided%20at%0Ahttps%3A//github.com/IML-DKFZ/sure-vqa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19688v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSURE-VQA%253A%2520Systematic%2520Understanding%2520of%2520Robustness%2520Evaluation%2520in%2520Medical%250A%2520%2520VQA%2520Tasks%26entry.906535625%3DKim-Celine%2520Kahl%2520and%2520Selen%2520Erkan%2520and%2520Jeremias%2520Traub%2520and%2520Carsten%2520T.%2520L%25C3%25BCth%2520and%2520Klaus%2520Maier-Hein%2520and%2520Lena%2520Maier-Hein%2520and%2520Paul%2520F.%2520Jaeger%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520great%2520potential%2520in%2520medical%2520tasks%252C%2520like%250AVisual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520where%2520they%2520could%2520act%2520as%2520interactive%2520assistants%250Afor%2520both%2520patients%2520and%2520clinicians.%2520Yet%2520their%2520robustness%2520to%2520distribution%2520shifts%250Aon%2520unseen%2520data%2520remains%2520a%2520key%2520concern%2520for%2520safe%2520deployment.%2520Evaluating%2520such%250Arobustness%2520requires%2520a%2520controlled%2520experimental%2520setup%2520that%2520allows%2520for%2520systematic%250Ainsights%2520into%2520the%2520model%2527s%2520behavior.%2520However%252C%2520we%2520demonstrate%2520that%2520current%2520setups%250Afail%2520to%2520offer%2520sufficiently%2520thorough%2520evaluations.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520a%2520novel%2520framework%252C%2520called%2520SURE-VQA%252C%2520centered%2520around%2520three%2520key%250Arequirements%2520to%2520overcome%2520current%2520pitfalls%2520and%2520systematically%2520analyze%2520VLM%250Arobustness%253A%25201%2529%2520Since%2520robustness%2520on%2520synthetic%2520shifts%2520does%2520not%2520necessarily%250Atranslate%2520to%2520real-world%2520shifts%252C%2520it%2520should%2520be%2520measured%2520on%2520real-world%2520shifts%2520that%250Aare%2520inherent%2520to%2520the%2520VQA%2520data%253B%25202%2529%2520Traditional%2520token-matching%2520metrics%2520often%2520fail%250Ato%2520capture%2520underlying%2520semantics%252C%2520necessitating%2520the%2520use%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520for%2520more%2520accurate%2520semantic%2520evaluation%253B%25203%2529%2520Model%2520performance%2520often%2520lacks%250Ainterpretability%2520due%2520to%2520missing%2520sanity%2520baselines%252C%2520thus%2520meaningful%2520baselines%250Ashould%2520be%2520reported%2520that%2520allow%2520assessing%2520the%2520multimodal%2520impact%2520on%2520the%2520VLM.%2520To%250Ademonstrate%2520the%2520relevance%2520of%2520this%2520framework%252C%2520we%2520conduct%2520a%2520study%2520on%2520the%250Arobustness%2520of%2520various%2520Fine-Tuning%2520%2528FT%2529%2520methods%2520across%2520three%2520medical%2520datasets%250Awith%2520four%2520types%2520of%2520distribution%2520shifts.%2520Our%2520study%2520highlights%2520key%2520insights%2520into%250Arobustness%253A%25201%2529%2520No%2520FT%2520method%2520consistently%2520outperforms%2520others%2520in%2520robustness%252C%2520and%250A2%2529%2520robustness%2520trends%2520are%2520more%2520stable%2520across%2520FT%2520methods%2520than%2520across%2520distribution%250Ashifts.%2520Additionally%252C%2520we%2520find%2520that%2520simple%2520sanity%2520baselines%2520that%2520do%2520not%2520use%2520the%250Aimage%2520data%2520can%2520perform%2520surprisingly%2520well%2520and%2520confirm%2520LoRA%2520as%2520the%250Abest-performing%2520FT%2520method%2520on%2520in-distribution%2520data.%2520Code%2520is%2520provided%2520at%250Ahttps%253A//github.com/IML-DKFZ/sure-vqa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19688v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks&entry.906535625=Kim-Celine%20Kahl%20and%20Selen%20Erkan%20and%20Jeremias%20Traub%20and%20Carsten%20T.%20L%C3%BCth%20and%20Klaus%20Maier-Hein%20and%20Lena%20Maier-Hein%20and%20Paul%20F.%20Jaeger&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20great%20potential%20in%20medical%20tasks%2C%20like%0AVisual%20Question%20Answering%20%28VQA%29%2C%20where%20they%20could%20act%20as%20interactive%20assistants%0Afor%20both%20patients%20and%20clinicians.%20Yet%20their%20robustness%20to%20distribution%20shifts%0Aon%20unseen%20data%20remains%20a%20key%20concern%20for%20safe%20deployment.%20Evaluating%20such%0Arobustness%20requires%20a%20controlled%20experimental%20setup%20that%20allows%20for%20systematic%0Ainsights%20into%20the%20model%27s%20behavior.%20However%2C%20we%20demonstrate%20that%20current%20setups%0Afail%20to%20offer%20sufficiently%20thorough%20evaluations.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20a%20novel%20framework%2C%20called%20SURE-VQA%2C%20centered%20around%20three%20key%0Arequirements%20to%20overcome%20current%20pitfalls%20and%20systematically%20analyze%20VLM%0Arobustness%3A%201%29%20Since%20robustness%20on%20synthetic%20shifts%20does%20not%20necessarily%0Atranslate%20to%20real-world%20shifts%2C%20it%20should%20be%20measured%20on%20real-world%20shifts%20that%0Aare%20inherent%20to%20the%20VQA%20data%3B%202%29%20Traditional%20token-matching%20metrics%20often%20fail%0Ato%20capture%20underlying%20semantics%2C%20necessitating%20the%20use%20of%20large%20language%20models%0A%28LLMs%29%20for%20more%20accurate%20semantic%20evaluation%3B%203%29%20Model%20performance%20often%20lacks%0Ainterpretability%20due%20to%20missing%20sanity%20baselines%2C%20thus%20meaningful%20baselines%0Ashould%20be%20reported%20that%20allow%20assessing%20the%20multimodal%20impact%20on%20the%20VLM.%20To%0Ademonstrate%20the%20relevance%20of%20this%20framework%2C%20we%20conduct%20a%20study%20on%20the%0Arobustness%20of%20various%20Fine-Tuning%20%28FT%29%20methods%20across%20three%20medical%20datasets%0Awith%20four%20types%20of%20distribution%20shifts.%20Our%20study%20highlights%20key%20insights%20into%0Arobustness%3A%201%29%20No%20FT%20method%20consistently%20outperforms%20others%20in%20robustness%2C%20and%0A2%29%20robustness%20trends%20are%20more%20stable%20across%20FT%20methods%20than%20across%20distribution%0Ashifts.%20Additionally%2C%20we%20find%20that%20simple%20sanity%20baselines%20that%20do%20not%20use%20the%0Aimage%20data%20can%20perform%20surprisingly%20well%20and%20confirm%20LoRA%20as%20the%0Abest-performing%20FT%20method%20on%20in-distribution%20data.%20Code%20is%20provided%20at%0Ahttps%3A//github.com/IML-DKFZ/sure-vqa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19688v3&entry.124074799=Read"},
{"title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue", "author": "Paulo Ricardo Knob and Leonardo Scholler and Juliano Rigatti and Soraia Raupp Musse", "abstract": "  Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents.\n", "link": "http://arxiv.org/abs/2507.02537v1", "date": "2025-07-03", "relevancy": 2.2885, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20You%20Listening%20to%20Me%3F%20Fine-Tuning%20Chatbots%20for%20Empathetic%20Dialogue&body=Title%3A%20Are%20You%20Listening%20to%20Me%3F%20Fine-Tuning%20Chatbots%20for%20Empathetic%20Dialogue%0AAuthor%3A%20Paulo%20Ricardo%20Knob%20and%20Leonardo%20Scholler%20and%20Juliano%20Rigatti%20and%20Soraia%20Raupp%20Musse%0AAbstract%3A%20%20%20Conversational%20agents%20have%20made%20significant%20progress%20since%20ELIZA%2C%20expanding%0Atheir%20role%20across%20various%20domains%2C%20including%20healthcare%2C%20education%2C%20and%0Acustomer%20service.%20As%20these%20agents%20become%20increasingly%20integrated%20into%20daily%0Ahuman%20interactions%2C%20the%20need%20for%20emotional%20intelligence%2C%20particularly%0Aempathetic%20listening%2C%20becomes%20increasingly%20essential.%20In%20this%20study%2C%20we%20explore%0Ahow%20Large%20Language%20Models%20%28LLMs%29%20respond%20when%20tasked%20with%20generating%0Aemotionally%20rich%20interactions.%20Starting%20from%20a%20small%20dataset%20manually%20crafted%0Aby%20an%20expert%20to%20reflect%20empathic%20behavior%2C%20we%20extended%20the%20conversations%20using%0Atwo%20LLMs%3A%20ChatGPT%20and%20Gemini.%20We%20analyzed%20the%20emotional%20progression%20of%20the%0Adialogues%20using%20both%20sentiment%20analysis%20%28via%20VADER%29%20and%20expert%20assessments.%0AWhile%20the%20generated%20conversations%20often%20mirrored%20the%20intended%20emotional%0Astructure%2C%20human%20evaluation%20revealed%20important%20differences%20in%20the%20perceived%0Aempathy%20and%20coherence%20of%20the%20responses.%20These%20findings%20suggest%20that%20emotion%0Amodeling%20in%20dialogues%20requires%20not%20only%20structural%20alignment%20in%20the%20expressed%0Aemotions%20but%20also%20qualitative%20depth%2C%20highlighting%20the%20importance%20of%20combining%0Aautomated%20and%20humancentered%20methods%20in%20the%20development%20of%20emotionally%20competent%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520You%2520Listening%2520to%2520Me%253F%2520Fine-Tuning%2520Chatbots%2520for%2520Empathetic%2520Dialogue%26entry.906535625%3DPaulo%2520Ricardo%2520Knob%2520and%2520Leonardo%2520Scholler%2520and%2520Juliano%2520Rigatti%2520and%2520Soraia%2520Raupp%2520Musse%26entry.1292438233%3D%2520%2520Conversational%2520agents%2520have%2520made%2520significant%2520progress%2520since%2520ELIZA%252C%2520expanding%250Atheir%2520role%2520across%2520various%2520domains%252C%2520including%2520healthcare%252C%2520education%252C%2520and%250Acustomer%2520service.%2520As%2520these%2520agents%2520become%2520increasingly%2520integrated%2520into%2520daily%250Ahuman%2520interactions%252C%2520the%2520need%2520for%2520emotional%2520intelligence%252C%2520particularly%250Aempathetic%2520listening%252C%2520becomes%2520increasingly%2520essential.%2520In%2520this%2520study%252C%2520we%2520explore%250Ahow%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520respond%2520when%2520tasked%2520with%2520generating%250Aemotionally%2520rich%2520interactions.%2520Starting%2520from%2520a%2520small%2520dataset%2520manually%2520crafted%250Aby%2520an%2520expert%2520to%2520reflect%2520empathic%2520behavior%252C%2520we%2520extended%2520the%2520conversations%2520using%250Atwo%2520LLMs%253A%2520ChatGPT%2520and%2520Gemini.%2520We%2520analyzed%2520the%2520emotional%2520progression%2520of%2520the%250Adialogues%2520using%2520both%2520sentiment%2520analysis%2520%2528via%2520VADER%2529%2520and%2520expert%2520assessments.%250AWhile%2520the%2520generated%2520conversations%2520often%2520mirrored%2520the%2520intended%2520emotional%250Astructure%252C%2520human%2520evaluation%2520revealed%2520important%2520differences%2520in%2520the%2520perceived%250Aempathy%2520and%2520coherence%2520of%2520the%2520responses.%2520These%2520findings%2520suggest%2520that%2520emotion%250Amodeling%2520in%2520dialogues%2520requires%2520not%2520only%2520structural%2520alignment%2520in%2520the%2520expressed%250Aemotions%2520but%2520also%2520qualitative%2520depth%252C%2520highlighting%2520the%2520importance%2520of%2520combining%250Aautomated%2520and%2520humancentered%2520methods%2520in%2520the%2520development%2520of%2520emotionally%2520competent%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20You%20Listening%20to%20Me%3F%20Fine-Tuning%20Chatbots%20for%20Empathetic%20Dialogue&entry.906535625=Paulo%20Ricardo%20Knob%20and%20Leonardo%20Scholler%20and%20Juliano%20Rigatti%20and%20Soraia%20Raupp%20Musse&entry.1292438233=%20%20Conversational%20agents%20have%20made%20significant%20progress%20since%20ELIZA%2C%20expanding%0Atheir%20role%20across%20various%20domains%2C%20including%20healthcare%2C%20education%2C%20and%0Acustomer%20service.%20As%20these%20agents%20become%20increasingly%20integrated%20into%20daily%0Ahuman%20interactions%2C%20the%20need%20for%20emotional%20intelligence%2C%20particularly%0Aempathetic%20listening%2C%20becomes%20increasingly%20essential.%20In%20this%20study%2C%20we%20explore%0Ahow%20Large%20Language%20Models%20%28LLMs%29%20respond%20when%20tasked%20with%20generating%0Aemotionally%20rich%20interactions.%20Starting%20from%20a%20small%20dataset%20manually%20crafted%0Aby%20an%20expert%20to%20reflect%20empathic%20behavior%2C%20we%20extended%20the%20conversations%20using%0Atwo%20LLMs%3A%20ChatGPT%20and%20Gemini.%20We%20analyzed%20the%20emotional%20progression%20of%20the%0Adialogues%20using%20both%20sentiment%20analysis%20%28via%20VADER%29%20and%20expert%20assessments.%0AWhile%20the%20generated%20conversations%20often%20mirrored%20the%20intended%20emotional%0Astructure%2C%20human%20evaluation%20revealed%20important%20differences%20in%20the%20perceived%0Aempathy%20and%20coherence%20of%20the%20responses.%20These%20findings%20suggest%20that%20emotion%0Amodeling%20in%20dialogues%20requires%20not%20only%20structural%20alignment%20in%20the%20expressed%0Aemotions%20but%20also%20qualitative%20depth%2C%20highlighting%20the%20importance%20of%20combining%0Aautomated%20and%20humancentered%20methods%20in%20the%20development%20of%20emotionally%20competent%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02537v1&entry.124074799=Read"},
{"title": "Orientation-Aware Sparse Tensor PCA for Efficient Unsupervised Feature\n  Selection", "author": "Junjing Zheng and Xinyu Zhang and Weidong Jiang and Xiangfeng Qiu and Mingjian Ren", "abstract": "  Recently, introducing Tensor Decomposition (TD) techniques into unsupervised\nfeature selection (UFS) has been an emerging research topic. A tensor structure\nis beneficial for mining the relations between different modes and helps\nrelieve the computation burden. However, while existing methods exploit TD to\npreserve the data tensor structure, they do not consider the influence of data\norientation and thus have difficulty in handling orientation-specific data such\nas time series. To solve the above problem, we utilize the\norientation-dependent tensor-tensor product from Tensor Singular Value\nDecomposition based on *M-product (T-SVDM) and extend the one-dimensional\nSparse Principal Component Analysis (SPCA) to a tensor form. The proposed\nsparse tensor PCA model can constrain sparsity at the specified mode and yield\nsparse tensor principal components, enhancing flexibility and accuracy in\nlearning feature relations. To ensure fast convergence and a flexible\ndescription of feature correlation, we develop a convex version specially\ndesigned for general UFS tasks and propose an efficient slice-by-slice\nalgorithm that performs dual optimization in the transform domain. Experimental\nresults on real-world datasets demonstrate the effectiveness and remarkable\ncomputational efficiency of the proposed method for tensor data of diverse\nstructures over the state-of-the-art. When transform axes align with feature\ndistribution patterns, our method is promising for various applications. The\ncodes related to our proposed methods and the experiments are available at\nhttps://github.com/zjj20212035/STPCA.git.\n", "link": "http://arxiv.org/abs/2407.16985v3", "date": "2025-07-03", "relevancy": 2.2878, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4638}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4604}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orientation-Aware%20Sparse%20Tensor%20PCA%20for%20Efficient%20Unsupervised%20Feature%0A%20%20Selection&body=Title%3A%20Orientation-Aware%20Sparse%20Tensor%20PCA%20for%20Efficient%20Unsupervised%20Feature%0A%20%20Selection%0AAuthor%3A%20Junjing%20Zheng%20and%20Xinyu%20Zhang%20and%20Weidong%20Jiang%20and%20Xiangfeng%20Qiu%20and%20Mingjian%20Ren%0AAbstract%3A%20%20%20Recently%2C%20introducing%20Tensor%20Decomposition%20%28TD%29%20techniques%20into%20unsupervised%0Afeature%20selection%20%28UFS%29%20has%20been%20an%20emerging%20research%20topic.%20A%20tensor%20structure%0Ais%20beneficial%20for%20mining%20the%20relations%20between%20different%20modes%20and%20helps%0Arelieve%20the%20computation%20burden.%20However%2C%20while%20existing%20methods%20exploit%20TD%20to%0Apreserve%20the%20data%20tensor%20structure%2C%20they%20do%20not%20consider%20the%20influence%20of%20data%0Aorientation%20and%20thus%20have%20difficulty%20in%20handling%20orientation-specific%20data%20such%0Aas%20time%20series.%20To%20solve%20the%20above%20problem%2C%20we%20utilize%20the%0Aorientation-dependent%20tensor-tensor%20product%20from%20Tensor%20Singular%20Value%0ADecomposition%20based%20on%20%2AM-product%20%28T-SVDM%29%20and%20extend%20the%20one-dimensional%0ASparse%20Principal%20Component%20Analysis%20%28SPCA%29%20to%20a%20tensor%20form.%20The%20proposed%0Asparse%20tensor%20PCA%20model%20can%20constrain%20sparsity%20at%20the%20specified%20mode%20and%20yield%0Asparse%20tensor%20principal%20components%2C%20enhancing%20flexibility%20and%20accuracy%20in%0Alearning%20feature%20relations.%20To%20ensure%20fast%20convergence%20and%20a%20flexible%0Adescription%20of%20feature%20correlation%2C%20we%20develop%20a%20convex%20version%20specially%0Adesigned%20for%20general%20UFS%20tasks%20and%20propose%20an%20efficient%20slice-by-slice%0Aalgorithm%20that%20performs%20dual%20optimization%20in%20the%20transform%20domain.%20Experimental%0Aresults%20on%20real-world%20datasets%20demonstrate%20the%20effectiveness%20and%20remarkable%0Acomputational%20efficiency%20of%20the%20proposed%20method%20for%20tensor%20data%20of%20diverse%0Astructures%20over%20the%20state-of-the-art.%20When%20transform%20axes%20align%20with%20feature%0Adistribution%20patterns%2C%20our%20method%20is%20promising%20for%20various%20applications.%20The%0Acodes%20related%20to%20our%20proposed%20methods%20and%20the%20experiments%20are%20available%20at%0Ahttps%3A//github.com/zjj20212035/STPCA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16985v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrientation-Aware%2520Sparse%2520Tensor%2520PCA%2520for%2520Efficient%2520Unsupervised%2520Feature%250A%2520%2520Selection%26entry.906535625%3DJunjing%2520Zheng%2520and%2520Xinyu%2520Zhang%2520and%2520Weidong%2520Jiang%2520and%2520Xiangfeng%2520Qiu%2520and%2520Mingjian%2520Ren%26entry.1292438233%3D%2520%2520Recently%252C%2520introducing%2520Tensor%2520Decomposition%2520%2528TD%2529%2520techniques%2520into%2520unsupervised%250Afeature%2520selection%2520%2528UFS%2529%2520has%2520been%2520an%2520emerging%2520research%2520topic.%2520A%2520tensor%2520structure%250Ais%2520beneficial%2520for%2520mining%2520the%2520relations%2520between%2520different%2520modes%2520and%2520helps%250Arelieve%2520the%2520computation%2520burden.%2520However%252C%2520while%2520existing%2520methods%2520exploit%2520TD%2520to%250Apreserve%2520the%2520data%2520tensor%2520structure%252C%2520they%2520do%2520not%2520consider%2520the%2520influence%2520of%2520data%250Aorientation%2520and%2520thus%2520have%2520difficulty%2520in%2520handling%2520orientation-specific%2520data%2520such%250Aas%2520time%2520series.%2520To%2520solve%2520the%2520above%2520problem%252C%2520we%2520utilize%2520the%250Aorientation-dependent%2520tensor-tensor%2520product%2520from%2520Tensor%2520Singular%2520Value%250ADecomposition%2520based%2520on%2520%252AM-product%2520%2528T-SVDM%2529%2520and%2520extend%2520the%2520one-dimensional%250ASparse%2520Principal%2520Component%2520Analysis%2520%2528SPCA%2529%2520to%2520a%2520tensor%2520form.%2520The%2520proposed%250Asparse%2520tensor%2520PCA%2520model%2520can%2520constrain%2520sparsity%2520at%2520the%2520specified%2520mode%2520and%2520yield%250Asparse%2520tensor%2520principal%2520components%252C%2520enhancing%2520flexibility%2520and%2520accuracy%2520in%250Alearning%2520feature%2520relations.%2520To%2520ensure%2520fast%2520convergence%2520and%2520a%2520flexible%250Adescription%2520of%2520feature%2520correlation%252C%2520we%2520develop%2520a%2520convex%2520version%2520specially%250Adesigned%2520for%2520general%2520UFS%2520tasks%2520and%2520propose%2520an%2520efficient%2520slice-by-slice%250Aalgorithm%2520that%2520performs%2520dual%2520optimization%2520in%2520the%2520transform%2520domain.%2520Experimental%250Aresults%2520on%2520real-world%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520remarkable%250Acomputational%2520efficiency%2520of%2520the%2520proposed%2520method%2520for%2520tensor%2520data%2520of%2520diverse%250Astructures%2520over%2520the%2520state-of-the-art.%2520When%2520transform%2520axes%2520align%2520with%2520feature%250Adistribution%2520patterns%252C%2520our%2520method%2520is%2520promising%2520for%2520various%2520applications.%2520The%250Acodes%2520related%2520to%2520our%2520proposed%2520methods%2520and%2520the%2520experiments%2520are%2520available%2520at%250Ahttps%253A//github.com/zjj20212035/STPCA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16985v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orientation-Aware%20Sparse%20Tensor%20PCA%20for%20Efficient%20Unsupervised%20Feature%0A%20%20Selection&entry.906535625=Junjing%20Zheng%20and%20Xinyu%20Zhang%20and%20Weidong%20Jiang%20and%20Xiangfeng%20Qiu%20and%20Mingjian%20Ren&entry.1292438233=%20%20Recently%2C%20introducing%20Tensor%20Decomposition%20%28TD%29%20techniques%20into%20unsupervised%0Afeature%20selection%20%28UFS%29%20has%20been%20an%20emerging%20research%20topic.%20A%20tensor%20structure%0Ais%20beneficial%20for%20mining%20the%20relations%20between%20different%20modes%20and%20helps%0Arelieve%20the%20computation%20burden.%20However%2C%20while%20existing%20methods%20exploit%20TD%20to%0Apreserve%20the%20data%20tensor%20structure%2C%20they%20do%20not%20consider%20the%20influence%20of%20data%0Aorientation%20and%20thus%20have%20difficulty%20in%20handling%20orientation-specific%20data%20such%0Aas%20time%20series.%20To%20solve%20the%20above%20problem%2C%20we%20utilize%20the%0Aorientation-dependent%20tensor-tensor%20product%20from%20Tensor%20Singular%20Value%0ADecomposition%20based%20on%20%2AM-product%20%28T-SVDM%29%20and%20extend%20the%20one-dimensional%0ASparse%20Principal%20Component%20Analysis%20%28SPCA%29%20to%20a%20tensor%20form.%20The%20proposed%0Asparse%20tensor%20PCA%20model%20can%20constrain%20sparsity%20at%20the%20specified%20mode%20and%20yield%0Asparse%20tensor%20principal%20components%2C%20enhancing%20flexibility%20and%20accuracy%20in%0Alearning%20feature%20relations.%20To%20ensure%20fast%20convergence%20and%20a%20flexible%0Adescription%20of%20feature%20correlation%2C%20we%20develop%20a%20convex%20version%20specially%0Adesigned%20for%20general%20UFS%20tasks%20and%20propose%20an%20efficient%20slice-by-slice%0Aalgorithm%20that%20performs%20dual%20optimization%20in%20the%20transform%20domain.%20Experimental%0Aresults%20on%20real-world%20datasets%20demonstrate%20the%20effectiveness%20and%20remarkable%0Acomputational%20efficiency%20of%20the%20proposed%20method%20for%20tensor%20data%20of%20diverse%0Astructures%20over%20the%20state-of-the-art.%20When%20transform%20axes%20align%20with%20feature%0Adistribution%20patterns%2C%20our%20method%20is%20promising%20for%20various%20applications.%20The%0Acodes%20related%20to%20our%20proposed%20methods%20and%20the%20experiments%20are%20available%20at%0Ahttps%3A//github.com/zjj20212035/STPCA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16985v3&entry.124074799=Read"},
{"title": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from\n  Gameplay Recordings", "author": "Florian Vahl and J\u00f6rn Griepenburg and Jan Gutsche and Jasper G\u00fcldenstein and Jianwei Zhang", "abstract": "  This paper introduces SoccerDiffusion, a transformer-based diffusion model\ndesigned to learn end-to-end control policies for humanoid robot soccer\ndirectly from real-world gameplay recordings. Using data collected from RoboCup\ncompetitions, the model predicts joint command trajectories from multi-modal\nsensor inputs, including vision, proprioception, and game state. We employ a\ndistillation technique to enable real-time inference on embedded platforms that\nreduces the multi-step diffusion process to a single step. Our results\ndemonstrate the model's ability to replicate complex motion behaviors such as\nwalking, kicking, and fall recovery both in simulation and on physical robots.\nAlthough high-level tactical behavior remains limited, this work provides a\nrobust foundation for subsequent reinforcement learning or preference\noptimization methods. We release the dataset, pretrained models, and code\nunder: https://bit-bots.github.io/SoccerDiffusion\n", "link": "http://arxiv.org/abs/2504.20808v2", "date": "2025-07-03", "relevancy": 2.2804, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5826}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoccerDiffusion%3A%20Toward%20Learning%20End-to-End%20Humanoid%20Robot%20Soccer%20from%0A%20%20Gameplay%20Recordings&body=Title%3A%20SoccerDiffusion%3A%20Toward%20Learning%20End-to-End%20Humanoid%20Robot%20Soccer%20from%0A%20%20Gameplay%20Recordings%0AAuthor%3A%20Florian%20Vahl%20and%20J%C3%B6rn%20Griepenburg%20and%20Jan%20Gutsche%20and%20Jasper%20G%C3%BCldenstein%20and%20Jianwei%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20SoccerDiffusion%2C%20a%20transformer-based%20diffusion%20model%0Adesigned%20to%20learn%20end-to-end%20control%20policies%20for%20humanoid%20robot%20soccer%0Adirectly%20from%20real-world%20gameplay%20recordings.%20Using%20data%20collected%20from%20RoboCup%0Acompetitions%2C%20the%20model%20predicts%20joint%20command%20trajectories%20from%20multi-modal%0Asensor%20inputs%2C%20including%20vision%2C%20proprioception%2C%20and%20game%20state.%20We%20employ%20a%0Adistillation%20technique%20to%20enable%20real-time%20inference%20on%20embedded%20platforms%20that%0Areduces%20the%20multi-step%20diffusion%20process%20to%20a%20single%20step.%20Our%20results%0Ademonstrate%20the%20model%27s%20ability%20to%20replicate%20complex%20motion%20behaviors%20such%20as%0Awalking%2C%20kicking%2C%20and%20fall%20recovery%20both%20in%20simulation%20and%20on%20physical%20robots.%0AAlthough%20high-level%20tactical%20behavior%20remains%20limited%2C%20this%20work%20provides%20a%0Arobust%20foundation%20for%20subsequent%20reinforcement%20learning%20or%20preference%0Aoptimization%20methods.%20We%20release%20the%20dataset%2C%20pretrained%20models%2C%20and%20code%0Aunder%3A%20https%3A//bit-bots.github.io/SoccerDiffusion%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20808v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoccerDiffusion%253A%2520Toward%2520Learning%2520End-to-End%2520Humanoid%2520Robot%2520Soccer%2520from%250A%2520%2520Gameplay%2520Recordings%26entry.906535625%3DFlorian%2520Vahl%2520and%2520J%25C3%25B6rn%2520Griepenburg%2520and%2520Jan%2520Gutsche%2520and%2520Jasper%2520G%25C3%25BCldenstein%2520and%2520Jianwei%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520SoccerDiffusion%252C%2520a%2520transformer-based%2520diffusion%2520model%250Adesigned%2520to%2520learn%2520end-to-end%2520control%2520policies%2520for%2520humanoid%2520robot%2520soccer%250Adirectly%2520from%2520real-world%2520gameplay%2520recordings.%2520Using%2520data%2520collected%2520from%2520RoboCup%250Acompetitions%252C%2520the%2520model%2520predicts%2520joint%2520command%2520trajectories%2520from%2520multi-modal%250Asensor%2520inputs%252C%2520including%2520vision%252C%2520proprioception%252C%2520and%2520game%2520state.%2520We%2520employ%2520a%250Adistillation%2520technique%2520to%2520enable%2520real-time%2520inference%2520on%2520embedded%2520platforms%2520that%250Areduces%2520the%2520multi-step%2520diffusion%2520process%2520to%2520a%2520single%2520step.%2520Our%2520results%250Ademonstrate%2520the%2520model%2527s%2520ability%2520to%2520replicate%2520complex%2520motion%2520behaviors%2520such%2520as%250Awalking%252C%2520kicking%252C%2520and%2520fall%2520recovery%2520both%2520in%2520simulation%2520and%2520on%2520physical%2520robots.%250AAlthough%2520high-level%2520tactical%2520behavior%2520remains%2520limited%252C%2520this%2520work%2520provides%2520a%250Arobust%2520foundation%2520for%2520subsequent%2520reinforcement%2520learning%2520or%2520preference%250Aoptimization%2520methods.%2520We%2520release%2520the%2520dataset%252C%2520pretrained%2520models%252C%2520and%2520code%250Aunder%253A%2520https%253A//bit-bots.github.io/SoccerDiffusion%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20808v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoccerDiffusion%3A%20Toward%20Learning%20End-to-End%20Humanoid%20Robot%20Soccer%20from%0A%20%20Gameplay%20Recordings&entry.906535625=Florian%20Vahl%20and%20J%C3%B6rn%20Griepenburg%20and%20Jan%20Gutsche%20and%20Jasper%20G%C3%BCldenstein%20and%20Jianwei%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20SoccerDiffusion%2C%20a%20transformer-based%20diffusion%20model%0Adesigned%20to%20learn%20end-to-end%20control%20policies%20for%20humanoid%20robot%20soccer%0Adirectly%20from%20real-world%20gameplay%20recordings.%20Using%20data%20collected%20from%20RoboCup%0Acompetitions%2C%20the%20model%20predicts%20joint%20command%20trajectories%20from%20multi-modal%0Asensor%20inputs%2C%20including%20vision%2C%20proprioception%2C%20and%20game%20state.%20We%20employ%20a%0Adistillation%20technique%20to%20enable%20real-time%20inference%20on%20embedded%20platforms%20that%0Areduces%20the%20multi-step%20diffusion%20process%20to%20a%20single%20step.%20Our%20results%0Ademonstrate%20the%20model%27s%20ability%20to%20replicate%20complex%20motion%20behaviors%20such%20as%0Awalking%2C%20kicking%2C%20and%20fall%20recovery%20both%20in%20simulation%20and%20on%20physical%20robots.%0AAlthough%20high-level%20tactical%20behavior%20remains%20limited%2C%20this%20work%20provides%20a%0Arobust%20foundation%20for%20subsequent%20reinforcement%20learning%20or%20preference%0Aoptimization%20methods.%20We%20release%20the%20dataset%2C%20pretrained%20models%2C%20and%20code%0Aunder%3A%20https%3A//bit-bots.github.io/SoccerDiffusion%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20808v2&entry.124074799=Read"},
{"title": "Autoformalization in the Era of Large Language Models: A Survey", "author": "Ke Weng and Lun Du and Sirui Li and Wangyue Lu and Haozhe Sun and Hengyu Liu and Tiancheng Zhang", "abstract": "  Autoformalization, the process of transforming informal mathematical\npropositions into verifiable formal representations, is a foundational task in\nautomated theorem proving, offering a new perspective on the use of mathematics\nin both theoretical and applied domains. Driven by the rapid progress in\nartificial intelligence, particularly large language models (LLMs), this field\nhas witnessed substantial growth, bringing both new opportunities and unique\nchallenges. In this survey, we provide a comprehensive overview of recent\nadvances in autoformalization from both mathematical and LLM-centric\nperspectives. We examine how autoformalization is applied across various\nmathematical domains and levels of difficulty, and analyze the end-to-end\nworkflow from data preprocessing to model design and evaluation. We further\nexplore the emerging role of autoformalization in enhancing the verifiability\nof LLM-generated outputs, highlighting its potential to improve both the\ntrustworthiness and reasoning capabilities of LLMs. Finally, we summarize key\nopen-source models and datasets supporting current research, and discuss open\nchallenges and promising future directions for the field.\n", "link": "http://arxiv.org/abs/2505.23486v2", "date": "2025-07-03", "relevancy": 2.2665, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4564}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoformalization%20in%20the%20Era%20of%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Autoformalization%20in%20the%20Era%20of%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Ke%20Weng%20and%20Lun%20Du%20and%20Sirui%20Li%20and%20Wangyue%20Lu%20and%20Haozhe%20Sun%20and%20Hengyu%20Liu%20and%20Tiancheng%20Zhang%0AAbstract%3A%20%20%20Autoformalization%2C%20the%20process%20of%20transforming%20informal%20mathematical%0Apropositions%20into%20verifiable%20formal%20representations%2C%20is%20a%20foundational%20task%20in%0Aautomated%20theorem%20proving%2C%20offering%20a%20new%20perspective%20on%20the%20use%20of%20mathematics%0Ain%20both%20theoretical%20and%20applied%20domains.%20Driven%20by%20the%20rapid%20progress%20in%0Aartificial%20intelligence%2C%20particularly%20large%20language%20models%20%28LLMs%29%2C%20this%20field%0Ahas%20witnessed%20substantial%20growth%2C%20bringing%20both%20new%20opportunities%20and%20unique%0Achallenges.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20overview%20of%20recent%0Aadvances%20in%20autoformalization%20from%20both%20mathematical%20and%20LLM-centric%0Aperspectives.%20We%20examine%20how%20autoformalization%20is%20applied%20across%20various%0Amathematical%20domains%20and%20levels%20of%20difficulty%2C%20and%20analyze%20the%20end-to-end%0Aworkflow%20from%20data%20preprocessing%20to%20model%20design%20and%20evaluation.%20We%20further%0Aexplore%20the%20emerging%20role%20of%20autoformalization%20in%20enhancing%20the%20verifiability%0Aof%20LLM-generated%20outputs%2C%20highlighting%20its%20potential%20to%20improve%20both%20the%0Atrustworthiness%20and%20reasoning%20capabilities%20of%20LLMs.%20Finally%2C%20we%20summarize%20key%0Aopen-source%20models%20and%20datasets%20supporting%20current%20research%2C%20and%20discuss%20open%0Achallenges%20and%20promising%20future%20directions%20for%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoformalization%2520in%2520the%2520Era%2520of%2520Large%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DKe%2520Weng%2520and%2520Lun%2520Du%2520and%2520Sirui%2520Li%2520and%2520Wangyue%2520Lu%2520and%2520Haozhe%2520Sun%2520and%2520Hengyu%2520Liu%2520and%2520Tiancheng%2520Zhang%26entry.1292438233%3D%2520%2520Autoformalization%252C%2520the%2520process%2520of%2520transforming%2520informal%2520mathematical%250Apropositions%2520into%2520verifiable%2520formal%2520representations%252C%2520is%2520a%2520foundational%2520task%2520in%250Aautomated%2520theorem%2520proving%252C%2520offering%2520a%2520new%2520perspective%2520on%2520the%2520use%2520of%2520mathematics%250Ain%2520both%2520theoretical%2520and%2520applied%2520domains.%2520Driven%2520by%2520the%2520rapid%2520progress%2520in%250Aartificial%2520intelligence%252C%2520particularly%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520this%2520field%250Ahas%2520witnessed%2520substantial%2520growth%252C%2520bringing%2520both%2520new%2520opportunities%2520and%2520unique%250Achallenges.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520overview%2520of%2520recent%250Aadvances%2520in%2520autoformalization%2520from%2520both%2520mathematical%2520and%2520LLM-centric%250Aperspectives.%2520We%2520examine%2520how%2520autoformalization%2520is%2520applied%2520across%2520various%250Amathematical%2520domains%2520and%2520levels%2520of%2520difficulty%252C%2520and%2520analyze%2520the%2520end-to-end%250Aworkflow%2520from%2520data%2520preprocessing%2520to%2520model%2520design%2520and%2520evaluation.%2520We%2520further%250Aexplore%2520the%2520emerging%2520role%2520of%2520autoformalization%2520in%2520enhancing%2520the%2520verifiability%250Aof%2520LLM-generated%2520outputs%252C%2520highlighting%2520its%2520potential%2520to%2520improve%2520both%2520the%250Atrustworthiness%2520and%2520reasoning%2520capabilities%2520of%2520LLMs.%2520Finally%252C%2520we%2520summarize%2520key%250Aopen-source%2520models%2520and%2520datasets%2520supporting%2520current%2520research%252C%2520and%2520discuss%2520open%250Achallenges%2520and%2520promising%2520future%2520directions%2520for%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoformalization%20in%20the%20Era%20of%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Ke%20Weng%20and%20Lun%20Du%20and%20Sirui%20Li%20and%20Wangyue%20Lu%20and%20Haozhe%20Sun%20and%20Hengyu%20Liu%20and%20Tiancheng%20Zhang&entry.1292438233=%20%20Autoformalization%2C%20the%20process%20of%20transforming%20informal%20mathematical%0Apropositions%20into%20verifiable%20formal%20representations%2C%20is%20a%20foundational%20task%20in%0Aautomated%20theorem%20proving%2C%20offering%20a%20new%20perspective%20on%20the%20use%20of%20mathematics%0Ain%20both%20theoretical%20and%20applied%20domains.%20Driven%20by%20the%20rapid%20progress%20in%0Aartificial%20intelligence%2C%20particularly%20large%20language%20models%20%28LLMs%29%2C%20this%20field%0Ahas%20witnessed%20substantial%20growth%2C%20bringing%20both%20new%20opportunities%20and%20unique%0Achallenges.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20overview%20of%20recent%0Aadvances%20in%20autoformalization%20from%20both%20mathematical%20and%20LLM-centric%0Aperspectives.%20We%20examine%20how%20autoformalization%20is%20applied%20across%20various%0Amathematical%20domains%20and%20levels%20of%20difficulty%2C%20and%20analyze%20the%20end-to-end%0Aworkflow%20from%20data%20preprocessing%20to%20model%20design%20and%20evaluation.%20We%20further%0Aexplore%20the%20emerging%20role%20of%20autoformalization%20in%20enhancing%20the%20verifiability%0Aof%20LLM-generated%20outputs%2C%20highlighting%20its%20potential%20to%20improve%20both%20the%0Atrustworthiness%20and%20reasoning%20capabilities%20of%20LLMs.%20Finally%2C%20we%20summarize%20key%0Aopen-source%20models%20and%20datasets%20supporting%20current%20research%2C%20and%20discuss%20open%0Achallenges%20and%20promising%20future%20directions%20for%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23486v2&entry.124074799=Read"},
{"title": "A Square Peg in a Square Hole: Meta-Expert for Long-Tailed\n  Semi-Supervised Learning", "author": "Yaxin Hou and Yuheng Jia", "abstract": "  This paper studies the long-tailed semi-supervised learning (LTSSL) with\ndistribution mismatch, where the class distribution of the labeled training\ndata follows a long-tailed distribution and mismatches with that of the\nunlabeled training data. Most existing methods introduce auxiliary classifiers\n(experts) to model various unlabeled data distributions and produce\npseudo-labels, but the expertises of various experts are not fully utilized. We\nobserve that different experts are good at predicting different intervals of\nsamples, e.g., long-tailed expert is skilled in samples located in the head\ninterval and uniform expert excels in samples located in the medium interval.\nTherefore, we propose a dynamic expert assignment module that can estimate the\nclass membership (i.e., head, medium, or tail class) of samples, and\ndynamically assigns suitable expert to each sample based on the estimated\nmembership to produce high-quality pseudo-label in the training phase and\nproduce prediction in the testing phase. We also theoretically reveal that\nintegrating different experts' strengths will lead to a smaller generalization\nerror bound. Moreover, we find that the deeper features are more biased toward\nthe head class but with more discriminative ability, while the shallower\nfeatures are less biased but also with less discriminative ability. We,\ntherefore, propose a multi-depth feature fusion module to utilize different\ndepth features to mitigate the model bias. Our method demonstrates its\neffectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT,\nand SVHN-LT datasets across various settings. The code is available at\nhttps://github.com/yaxinhou/Meta-Expert.\n", "link": "http://arxiv.org/abs/2505.16341v2", "date": "2025-07-03", "relevancy": 2.265, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5389}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Square%20Peg%20in%20a%20Square%20Hole%3A%20Meta-Expert%20for%20Long-Tailed%0A%20%20Semi-Supervised%20Learning&body=Title%3A%20A%20Square%20Peg%20in%20a%20Square%20Hole%3A%20Meta-Expert%20for%20Long-Tailed%0A%20%20Semi-Supervised%20Learning%0AAuthor%3A%20Yaxin%20Hou%20and%20Yuheng%20Jia%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20long-tailed%20semi-supervised%20learning%20%28LTSSL%29%20with%0Adistribution%20mismatch%2C%20where%20the%20class%20distribution%20of%20the%20labeled%20training%0Adata%20follows%20a%20long-tailed%20distribution%20and%20mismatches%20with%20that%20of%20the%0Aunlabeled%20training%20data.%20Most%20existing%20methods%20introduce%20auxiliary%20classifiers%0A%28experts%29%20to%20model%20various%20unlabeled%20data%20distributions%20and%20produce%0Apseudo-labels%2C%20but%20the%20expertises%20of%20various%20experts%20are%20not%20fully%20utilized.%20We%0Aobserve%20that%20different%20experts%20are%20good%20at%20predicting%20different%20intervals%20of%0Asamples%2C%20e.g.%2C%20long-tailed%20expert%20is%20skilled%20in%20samples%20located%20in%20the%20head%0Ainterval%20and%20uniform%20expert%20excels%20in%20samples%20located%20in%20the%20medium%20interval.%0ATherefore%2C%20we%20propose%20a%20dynamic%20expert%20assignment%20module%20that%20can%20estimate%20the%0Aclass%20membership%20%28i.e.%2C%20head%2C%20medium%2C%20or%20tail%20class%29%20of%20samples%2C%20and%0Adynamically%20assigns%20suitable%20expert%20to%20each%20sample%20based%20on%20the%20estimated%0Amembership%20to%20produce%20high-quality%20pseudo-label%20in%20the%20training%20phase%20and%0Aproduce%20prediction%20in%20the%20testing%20phase.%20We%20also%20theoretically%20reveal%20that%0Aintegrating%20different%20experts%27%20strengths%20will%20lead%20to%20a%20smaller%20generalization%0Aerror%20bound.%20Moreover%2C%20we%20find%20that%20the%20deeper%20features%20are%20more%20biased%20toward%0Athe%20head%20class%20but%20with%20more%20discriminative%20ability%2C%20while%20the%20shallower%0Afeatures%20are%20less%20biased%20but%20also%20with%20less%20discriminative%20ability.%20We%2C%0Atherefore%2C%20propose%20a%20multi-depth%20feature%20fusion%20module%20to%20utilize%20different%0Adepth%20features%20to%20mitigate%20the%20model%20bias.%20Our%20method%20demonstrates%20its%0Aeffectiveness%20through%20comprehensive%20experiments%20on%20the%20CIFAR-10-LT%2C%20STL-10-LT%2C%0Aand%20SVHN-LT%20datasets%20across%20various%20settings.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yaxinhou/Meta-Expert.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16341v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Square%2520Peg%2520in%2520a%2520Square%2520Hole%253A%2520Meta-Expert%2520for%2520Long-Tailed%250A%2520%2520Semi-Supervised%2520Learning%26entry.906535625%3DYaxin%2520Hou%2520and%2520Yuheng%2520Jia%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520long-tailed%2520semi-supervised%2520learning%2520%2528LTSSL%2529%2520with%250Adistribution%2520mismatch%252C%2520where%2520the%2520class%2520distribution%2520of%2520the%2520labeled%2520training%250Adata%2520follows%2520a%2520long-tailed%2520distribution%2520and%2520mismatches%2520with%2520that%2520of%2520the%250Aunlabeled%2520training%2520data.%2520Most%2520existing%2520methods%2520introduce%2520auxiliary%2520classifiers%250A%2528experts%2529%2520to%2520model%2520various%2520unlabeled%2520data%2520distributions%2520and%2520produce%250Apseudo-labels%252C%2520but%2520the%2520expertises%2520of%2520various%2520experts%2520are%2520not%2520fully%2520utilized.%2520We%250Aobserve%2520that%2520different%2520experts%2520are%2520good%2520at%2520predicting%2520different%2520intervals%2520of%250Asamples%252C%2520e.g.%252C%2520long-tailed%2520expert%2520is%2520skilled%2520in%2520samples%2520located%2520in%2520the%2520head%250Ainterval%2520and%2520uniform%2520expert%2520excels%2520in%2520samples%2520located%2520in%2520the%2520medium%2520interval.%250ATherefore%252C%2520we%2520propose%2520a%2520dynamic%2520expert%2520assignment%2520module%2520that%2520can%2520estimate%2520the%250Aclass%2520membership%2520%2528i.e.%252C%2520head%252C%2520medium%252C%2520or%2520tail%2520class%2529%2520of%2520samples%252C%2520and%250Adynamically%2520assigns%2520suitable%2520expert%2520to%2520each%2520sample%2520based%2520on%2520the%2520estimated%250Amembership%2520to%2520produce%2520high-quality%2520pseudo-label%2520in%2520the%2520training%2520phase%2520and%250Aproduce%2520prediction%2520in%2520the%2520testing%2520phase.%2520We%2520also%2520theoretically%2520reveal%2520that%250Aintegrating%2520different%2520experts%2527%2520strengths%2520will%2520lead%2520to%2520a%2520smaller%2520generalization%250Aerror%2520bound.%2520Moreover%252C%2520we%2520find%2520that%2520the%2520deeper%2520features%2520are%2520more%2520biased%2520toward%250Athe%2520head%2520class%2520but%2520with%2520more%2520discriminative%2520ability%252C%2520while%2520the%2520shallower%250Afeatures%2520are%2520less%2520biased%2520but%2520also%2520with%2520less%2520discriminative%2520ability.%2520We%252C%250Atherefore%252C%2520propose%2520a%2520multi-depth%2520feature%2520fusion%2520module%2520to%2520utilize%2520different%250Adepth%2520features%2520to%2520mitigate%2520the%2520model%2520bias.%2520Our%2520method%2520demonstrates%2520its%250Aeffectiveness%2520through%2520comprehensive%2520experiments%2520on%2520the%2520CIFAR-10-LT%252C%2520STL-10-LT%252C%250Aand%2520SVHN-LT%2520datasets%2520across%2520various%2520settings.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/yaxinhou/Meta-Expert.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16341v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Square%20Peg%20in%20a%20Square%20Hole%3A%20Meta-Expert%20for%20Long-Tailed%0A%20%20Semi-Supervised%20Learning&entry.906535625=Yaxin%20Hou%20and%20Yuheng%20Jia&entry.1292438233=%20%20This%20paper%20studies%20the%20long-tailed%20semi-supervised%20learning%20%28LTSSL%29%20with%0Adistribution%20mismatch%2C%20where%20the%20class%20distribution%20of%20the%20labeled%20training%0Adata%20follows%20a%20long-tailed%20distribution%20and%20mismatches%20with%20that%20of%20the%0Aunlabeled%20training%20data.%20Most%20existing%20methods%20introduce%20auxiliary%20classifiers%0A%28experts%29%20to%20model%20various%20unlabeled%20data%20distributions%20and%20produce%0Apseudo-labels%2C%20but%20the%20expertises%20of%20various%20experts%20are%20not%20fully%20utilized.%20We%0Aobserve%20that%20different%20experts%20are%20good%20at%20predicting%20different%20intervals%20of%0Asamples%2C%20e.g.%2C%20long-tailed%20expert%20is%20skilled%20in%20samples%20located%20in%20the%20head%0Ainterval%20and%20uniform%20expert%20excels%20in%20samples%20located%20in%20the%20medium%20interval.%0ATherefore%2C%20we%20propose%20a%20dynamic%20expert%20assignment%20module%20that%20can%20estimate%20the%0Aclass%20membership%20%28i.e.%2C%20head%2C%20medium%2C%20or%20tail%20class%29%20of%20samples%2C%20and%0Adynamically%20assigns%20suitable%20expert%20to%20each%20sample%20based%20on%20the%20estimated%0Amembership%20to%20produce%20high-quality%20pseudo-label%20in%20the%20training%20phase%20and%0Aproduce%20prediction%20in%20the%20testing%20phase.%20We%20also%20theoretically%20reveal%20that%0Aintegrating%20different%20experts%27%20strengths%20will%20lead%20to%20a%20smaller%20generalization%0Aerror%20bound.%20Moreover%2C%20we%20find%20that%20the%20deeper%20features%20are%20more%20biased%20toward%0Athe%20head%20class%20but%20with%20more%20discriminative%20ability%2C%20while%20the%20shallower%0Afeatures%20are%20less%20biased%20but%20also%20with%20less%20discriminative%20ability.%20We%2C%0Atherefore%2C%20propose%20a%20multi-depth%20feature%20fusion%20module%20to%20utilize%20different%0Adepth%20features%20to%20mitigate%20the%20model%20bias.%20Our%20method%20demonstrates%20its%0Aeffectiveness%20through%20comprehensive%20experiments%20on%20the%20CIFAR-10-LT%2C%20STL-10-LT%2C%0Aand%20SVHN-LT%20datasets%20across%20various%20settings.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yaxinhou/Meta-Expert.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16341v2&entry.124074799=Read"},
{"title": "F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image\n  Classification via Image-Level Disentangled Prompt Tuning", "author": "Wei Li and Jingyang Zhang and Lihao Liu and Guoan Wang and Junjun He and Yang Chen and Lixu Gu", "abstract": "  Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a\nsource model to unseen medical sites using unlabeled test data, due to the high\ncost of data annotation. Existing TTA methods consider scenarios where data\nfrom one or multiple domains arrives in complete domain units. However, in\nclinical practice, data usually arrives in domain fragments of arbitrary\nlengths and in random arrival orders, due to resource constraints and patient\nvariability. This paper investigates a practical Free-Form Test-Time Adaptation\n(F$^{2}$TTA) task, where a source model is adapted to such free-form domain\nfragments, with shifts occurring between fragments unpredictably. In this\nsetting, these shifts could distort the adaptation process. To address this\nproblem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT)\nframework. I-DiPT employs an image-invariant prompt to explore domain-invariant\nrepresentations for mitigating the unpredictable shifts, and an image-specific\nprompt to adapt the source model to each test image from the incoming\nfragments. The prompts may suffer from insufficient knowledge representation\nsince only one image is available for training. To overcome this limitation, we\nfirst introduce Uncertainty-oriented Masking (UoM), which encourages the\nprompts to extract sufficient information from the incoming image via masked\nconsistency learning driven by the uncertainty of the source model\nrepresentations. Then, we further propose a Parallel Graph Distillation (PGD)\nmethod that reuses knowledge from historical image-specific and image-invariant\nprompts through parallel graph networks. Experiments on breast cancer and\nglaucoma classification demonstrate the superiority of our method over existing\nTTA approaches in F$^{2}$TTA. Code is available at\nhttps://github.com/mar-cry/F2TTA.\n", "link": "http://arxiv.org/abs/2507.02437v1", "date": "2025-07-03", "relevancy": 2.2609, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5816}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5614}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F%5E2TTA%3A%20Free-Form%20Test-Time%20Adaptation%20on%20Cross-Domain%20Medical%20Image%0A%20%20Classification%20via%20Image-Level%20Disentangled%20Prompt%20Tuning&body=Title%3A%20F%5E2TTA%3A%20Free-Form%20Test-Time%20Adaptation%20on%20Cross-Domain%20Medical%20Image%0A%20%20Classification%20via%20Image-Level%20Disentangled%20Prompt%20Tuning%0AAuthor%3A%20Wei%20Li%20and%20Jingyang%20Zhang%20and%20Lihao%20Liu%20and%20Guoan%20Wang%20and%20Junjun%20He%20and%20Yang%20Chen%20and%20Lixu%20Gu%0AAbstract%3A%20%20%20Test-Time%20Adaptation%20%28TTA%29%20has%20emerged%20as%20a%20promising%20solution%20for%20adapting%20a%0Asource%20model%20to%20unseen%20medical%20sites%20using%20unlabeled%20test%20data%2C%20due%20to%20the%20high%0Acost%20of%20data%20annotation.%20Existing%20TTA%20methods%20consider%20scenarios%20where%20data%0Afrom%20one%20or%20multiple%20domains%20arrives%20in%20complete%20domain%20units.%20However%2C%20in%0Aclinical%20practice%2C%20data%20usually%20arrives%20in%20domain%20fragments%20of%20arbitrary%0Alengths%20and%20in%20random%20arrival%20orders%2C%20due%20to%20resource%20constraints%20and%20patient%0Avariability.%20This%20paper%20investigates%20a%20practical%20Free-Form%20Test-Time%20Adaptation%0A%28F%24%5E%7B2%7D%24TTA%29%20task%2C%20where%20a%20source%20model%20is%20adapted%20to%20such%20free-form%20domain%0Afragments%2C%20with%20shifts%20occurring%20between%20fragments%20unpredictably.%20In%20this%0Asetting%2C%20these%20shifts%20could%20distort%20the%20adaptation%20process.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20novel%20Image-level%20Disentangled%20Prompt%20Tuning%20%28I-DiPT%29%0Aframework.%20I-DiPT%20employs%20an%20image-invariant%20prompt%20to%20explore%20domain-invariant%0Arepresentations%20for%20mitigating%20the%20unpredictable%20shifts%2C%20and%20an%20image-specific%0Aprompt%20to%20adapt%20the%20source%20model%20to%20each%20test%20image%20from%20the%20incoming%0Afragments.%20The%20prompts%20may%20suffer%20from%20insufficient%20knowledge%20representation%0Asince%20only%20one%20image%20is%20available%20for%20training.%20To%20overcome%20this%20limitation%2C%20we%0Afirst%20introduce%20Uncertainty-oriented%20Masking%20%28UoM%29%2C%20which%20encourages%20the%0Aprompts%20to%20extract%20sufficient%20information%20from%20the%20incoming%20image%20via%20masked%0Aconsistency%20learning%20driven%20by%20the%20uncertainty%20of%20the%20source%20model%0Arepresentations.%20Then%2C%20we%20further%20propose%20a%20Parallel%20Graph%20Distillation%20%28PGD%29%0Amethod%20that%20reuses%20knowledge%20from%20historical%20image-specific%20and%20image-invariant%0Aprompts%20through%20parallel%20graph%20networks.%20Experiments%20on%20breast%20cancer%20and%0Aglaucoma%20classification%20demonstrate%20the%20superiority%20of%20our%20method%20over%20existing%0ATTA%20approaches%20in%20F%24%5E%7B2%7D%24TTA.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mar-cry/F2TTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF%255E2TTA%253A%2520Free-Form%2520Test-Time%2520Adaptation%2520on%2520Cross-Domain%2520Medical%2520Image%250A%2520%2520Classification%2520via%2520Image-Level%2520Disentangled%2520Prompt%2520Tuning%26entry.906535625%3DWei%2520Li%2520and%2520Jingyang%2520Zhang%2520and%2520Lihao%2520Liu%2520and%2520Guoan%2520Wang%2520and%2520Junjun%2520He%2520and%2520Yang%2520Chen%2520and%2520Lixu%2520Gu%26entry.1292438233%3D%2520%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520for%2520adapting%2520a%250Asource%2520model%2520to%2520unseen%2520medical%2520sites%2520using%2520unlabeled%2520test%2520data%252C%2520due%2520to%2520the%2520high%250Acost%2520of%2520data%2520annotation.%2520Existing%2520TTA%2520methods%2520consider%2520scenarios%2520where%2520data%250Afrom%2520one%2520or%2520multiple%2520domains%2520arrives%2520in%2520complete%2520domain%2520units.%2520However%252C%2520in%250Aclinical%2520practice%252C%2520data%2520usually%2520arrives%2520in%2520domain%2520fragments%2520of%2520arbitrary%250Alengths%2520and%2520in%2520random%2520arrival%2520orders%252C%2520due%2520to%2520resource%2520constraints%2520and%2520patient%250Avariability.%2520This%2520paper%2520investigates%2520a%2520practical%2520Free-Form%2520Test-Time%2520Adaptation%250A%2528F%2524%255E%257B2%257D%2524TTA%2529%2520task%252C%2520where%2520a%2520source%2520model%2520is%2520adapted%2520to%2520such%2520free-form%2520domain%250Afragments%252C%2520with%2520shifts%2520occurring%2520between%2520fragments%2520unpredictably.%2520In%2520this%250Asetting%252C%2520these%2520shifts%2520could%2520distort%2520the%2520adaptation%2520process.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520novel%2520Image-level%2520Disentangled%2520Prompt%2520Tuning%2520%2528I-DiPT%2529%250Aframework.%2520I-DiPT%2520employs%2520an%2520image-invariant%2520prompt%2520to%2520explore%2520domain-invariant%250Arepresentations%2520for%2520mitigating%2520the%2520unpredictable%2520shifts%252C%2520and%2520an%2520image-specific%250Aprompt%2520to%2520adapt%2520the%2520source%2520model%2520to%2520each%2520test%2520image%2520from%2520the%2520incoming%250Afragments.%2520The%2520prompts%2520may%2520suffer%2520from%2520insufficient%2520knowledge%2520representation%250Asince%2520only%2520one%2520image%2520is%2520available%2520for%2520training.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Afirst%2520introduce%2520Uncertainty-oriented%2520Masking%2520%2528UoM%2529%252C%2520which%2520encourages%2520the%250Aprompts%2520to%2520extract%2520sufficient%2520information%2520from%2520the%2520incoming%2520image%2520via%2520masked%250Aconsistency%2520learning%2520driven%2520by%2520the%2520uncertainty%2520of%2520the%2520source%2520model%250Arepresentations.%2520Then%252C%2520we%2520further%2520propose%2520a%2520Parallel%2520Graph%2520Distillation%2520%2528PGD%2529%250Amethod%2520that%2520reuses%2520knowledge%2520from%2520historical%2520image-specific%2520and%2520image-invariant%250Aprompts%2520through%2520parallel%2520graph%2520networks.%2520Experiments%2520on%2520breast%2520cancer%2520and%250Aglaucoma%2520classification%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520existing%250ATTA%2520approaches%2520in%2520F%2524%255E%257B2%257D%2524TTA.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/mar-cry/F2TTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F%5E2TTA%3A%20Free-Form%20Test-Time%20Adaptation%20on%20Cross-Domain%20Medical%20Image%0A%20%20Classification%20via%20Image-Level%20Disentangled%20Prompt%20Tuning&entry.906535625=Wei%20Li%20and%20Jingyang%20Zhang%20and%20Lihao%20Liu%20and%20Guoan%20Wang%20and%20Junjun%20He%20and%20Yang%20Chen%20and%20Lixu%20Gu&entry.1292438233=%20%20Test-Time%20Adaptation%20%28TTA%29%20has%20emerged%20as%20a%20promising%20solution%20for%20adapting%20a%0Asource%20model%20to%20unseen%20medical%20sites%20using%20unlabeled%20test%20data%2C%20due%20to%20the%20high%0Acost%20of%20data%20annotation.%20Existing%20TTA%20methods%20consider%20scenarios%20where%20data%0Afrom%20one%20or%20multiple%20domains%20arrives%20in%20complete%20domain%20units.%20However%2C%20in%0Aclinical%20practice%2C%20data%20usually%20arrives%20in%20domain%20fragments%20of%20arbitrary%0Alengths%20and%20in%20random%20arrival%20orders%2C%20due%20to%20resource%20constraints%20and%20patient%0Avariability.%20This%20paper%20investigates%20a%20practical%20Free-Form%20Test-Time%20Adaptation%0A%28F%24%5E%7B2%7D%24TTA%29%20task%2C%20where%20a%20source%20model%20is%20adapted%20to%20such%20free-form%20domain%0Afragments%2C%20with%20shifts%20occurring%20between%20fragments%20unpredictably.%20In%20this%0Asetting%2C%20these%20shifts%20could%20distort%20the%20adaptation%20process.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20novel%20Image-level%20Disentangled%20Prompt%20Tuning%20%28I-DiPT%29%0Aframework.%20I-DiPT%20employs%20an%20image-invariant%20prompt%20to%20explore%20domain-invariant%0Arepresentations%20for%20mitigating%20the%20unpredictable%20shifts%2C%20and%20an%20image-specific%0Aprompt%20to%20adapt%20the%20source%20model%20to%20each%20test%20image%20from%20the%20incoming%0Afragments.%20The%20prompts%20may%20suffer%20from%20insufficient%20knowledge%20representation%0Asince%20only%20one%20image%20is%20available%20for%20training.%20To%20overcome%20this%20limitation%2C%20we%0Afirst%20introduce%20Uncertainty-oriented%20Masking%20%28UoM%29%2C%20which%20encourages%20the%0Aprompts%20to%20extract%20sufficient%20information%20from%20the%20incoming%20image%20via%20masked%0Aconsistency%20learning%20driven%20by%20the%20uncertainty%20of%20the%20source%20model%0Arepresentations.%20Then%2C%20we%20further%20propose%20a%20Parallel%20Graph%20Distillation%20%28PGD%29%0Amethod%20that%20reuses%20knowledge%20from%20historical%20image-specific%20and%20image-invariant%0Aprompts%20through%20parallel%20graph%20networks.%20Experiments%20on%20breast%20cancer%20and%0Aglaucoma%20classification%20demonstrate%20the%20superiority%20of%20our%20method%20over%20existing%0ATTA%20approaches%20in%20F%24%5E%7B2%7D%24TTA.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mar-cry/F2TTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02437v1&entry.124074799=Read"},
{"title": "CanonSwap: High-Fidelity and Consistent Video Face Swapping via\n  Canonical Space Modulation", "author": "Xiangyang Luo and Ye Zhu and Yunfei Liu and Lijian Lin and Cong Wan and Zijian Cai and Shao-Lun Huang and Yu Li", "abstract": "  Video face swapping aims to address two primary challenges: effectively\ntransferring the source identity to the target video and accurately preserving\nthe dynamic attributes of the target face, such as head poses, facial\nexpressions, lip-sync, \\etc. Existing methods mainly focus on achieving\nhigh-quality identity transfer but often fall short in maintaining the dynamic\nattributes of the target face, leading to inconsistent results. We attribute\nthis issue to the inherent coupling of facial appearance and motion in videos.\nTo address this, we propose CanonSwap, a novel video face-swapping framework\nthat decouples motion information from appearance information. Specifically,\nCanonSwap first eliminates motion-related information, enabling identity\nmodification within a unified canonical space. Subsequently, the swapped\nfeature is reintegrated into the original video space, ensuring the\npreservation of the target face's dynamic attributes. To further achieve\nprecise identity transfer with minimal artifacts and enhanced realism, we\ndesign a Partial Identity Modulation module that adaptively integrates source\nidentity features using a spatial mask to restrict modifications to facial\nregions. Additionally, we introduce several fine-grained synchronization\nmetrics to comprehensively evaluate the performance of video face swapping\nmethods. Extensive experiments demonstrate that our method significantly\noutperforms existing approaches in terms of visual quality, temporal\nconsistency, and identity preservation. Our project page are publicly available\nat https://luoxyhappy.github.io/CanonSwap/.\n", "link": "http://arxiv.org/abs/2507.02691v1", "date": "2025-07-03", "relevancy": 2.2594, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5898}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5742}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CanonSwap%3A%20High-Fidelity%20and%20Consistent%20Video%20Face%20Swapping%20via%0A%20%20Canonical%20Space%20Modulation&body=Title%3A%20CanonSwap%3A%20High-Fidelity%20and%20Consistent%20Video%20Face%20Swapping%20via%0A%20%20Canonical%20Space%20Modulation%0AAuthor%3A%20Xiangyang%20Luo%20and%20Ye%20Zhu%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Cong%20Wan%20and%20Zijian%20Cai%20and%20Shao-Lun%20Huang%20and%20Yu%20Li%0AAbstract%3A%20%20%20Video%20face%20swapping%20aims%20to%20address%20two%20primary%20challenges%3A%20effectively%0Atransferring%20the%20source%20identity%20to%20the%20target%20video%20and%20accurately%20preserving%0Athe%20dynamic%20attributes%20of%20the%20target%20face%2C%20such%20as%20head%20poses%2C%20facial%0Aexpressions%2C%20lip-sync%2C%20%5Cetc.%20Existing%20methods%20mainly%20focus%20on%20achieving%0Ahigh-quality%20identity%20transfer%20but%20often%20fall%20short%20in%20maintaining%20the%20dynamic%0Aattributes%20of%20the%20target%20face%2C%20leading%20to%20inconsistent%20results.%20We%20attribute%0Athis%20issue%20to%20the%20inherent%20coupling%20of%20facial%20appearance%20and%20motion%20in%20videos.%0ATo%20address%20this%2C%20we%20propose%20CanonSwap%2C%20a%20novel%20video%20face-swapping%20framework%0Athat%20decouples%20motion%20information%20from%20appearance%20information.%20Specifically%2C%0ACanonSwap%20first%20eliminates%20motion-related%20information%2C%20enabling%20identity%0Amodification%20within%20a%20unified%20canonical%20space.%20Subsequently%2C%20the%20swapped%0Afeature%20is%20reintegrated%20into%20the%20original%20video%20space%2C%20ensuring%20the%0Apreservation%20of%20the%20target%20face%27s%20dynamic%20attributes.%20To%20further%20achieve%0Aprecise%20identity%20transfer%20with%20minimal%20artifacts%20and%20enhanced%20realism%2C%20we%0Adesign%20a%20Partial%20Identity%20Modulation%20module%20that%20adaptively%20integrates%20source%0Aidentity%20features%20using%20a%20spatial%20mask%20to%20restrict%20modifications%20to%20facial%0Aregions.%20Additionally%2C%20we%20introduce%20several%20fine-grained%20synchronization%0Ametrics%20to%20comprehensively%20evaluate%20the%20performance%20of%20video%20face%20swapping%0Amethods.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20existing%20approaches%20in%20terms%20of%20visual%20quality%2C%20temporal%0Aconsistency%2C%20and%20identity%20preservation.%20Our%20project%20page%20are%20publicly%20available%0Aat%20https%3A//luoxyhappy.github.io/CanonSwap/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanonSwap%253A%2520High-Fidelity%2520and%2520Consistent%2520Video%2520Face%2520Swapping%2520via%250A%2520%2520Canonical%2520Space%2520Modulation%26entry.906535625%3DXiangyang%2520Luo%2520and%2520Ye%2520Zhu%2520and%2520Yunfei%2520Liu%2520and%2520Lijian%2520Lin%2520and%2520Cong%2520Wan%2520and%2520Zijian%2520Cai%2520and%2520Shao-Lun%2520Huang%2520and%2520Yu%2520Li%26entry.1292438233%3D%2520%2520Video%2520face%2520swapping%2520aims%2520to%2520address%2520two%2520primary%2520challenges%253A%2520effectively%250Atransferring%2520the%2520source%2520identity%2520to%2520the%2520target%2520video%2520and%2520accurately%2520preserving%250Athe%2520dynamic%2520attributes%2520of%2520the%2520target%2520face%252C%2520such%2520as%2520head%2520poses%252C%2520facial%250Aexpressions%252C%2520lip-sync%252C%2520%255Cetc.%2520Existing%2520methods%2520mainly%2520focus%2520on%2520achieving%250Ahigh-quality%2520identity%2520transfer%2520but%2520often%2520fall%2520short%2520in%2520maintaining%2520the%2520dynamic%250Aattributes%2520of%2520the%2520target%2520face%252C%2520leading%2520to%2520inconsistent%2520results.%2520We%2520attribute%250Athis%2520issue%2520to%2520the%2520inherent%2520coupling%2520of%2520facial%2520appearance%2520and%2520motion%2520in%2520videos.%250ATo%2520address%2520this%252C%2520we%2520propose%2520CanonSwap%252C%2520a%2520novel%2520video%2520face-swapping%2520framework%250Athat%2520decouples%2520motion%2520information%2520from%2520appearance%2520information.%2520Specifically%252C%250ACanonSwap%2520first%2520eliminates%2520motion-related%2520information%252C%2520enabling%2520identity%250Amodification%2520within%2520a%2520unified%2520canonical%2520space.%2520Subsequently%252C%2520the%2520swapped%250Afeature%2520is%2520reintegrated%2520into%2520the%2520original%2520video%2520space%252C%2520ensuring%2520the%250Apreservation%2520of%2520the%2520target%2520face%2527s%2520dynamic%2520attributes.%2520To%2520further%2520achieve%250Aprecise%2520identity%2520transfer%2520with%2520minimal%2520artifacts%2520and%2520enhanced%2520realism%252C%2520we%250Adesign%2520a%2520Partial%2520Identity%2520Modulation%2520module%2520that%2520adaptively%2520integrates%2520source%250Aidentity%2520features%2520using%2520a%2520spatial%2520mask%2520to%2520restrict%2520modifications%2520to%2520facial%250Aregions.%2520Additionally%252C%2520we%2520introduce%2520several%2520fine-grained%2520synchronization%250Ametrics%2520to%2520comprehensively%2520evaluate%2520the%2520performance%2520of%2520video%2520face%2520swapping%250Amethods.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520existing%2520approaches%2520in%2520terms%2520of%2520visual%2520quality%252C%2520temporal%250Aconsistency%252C%2520and%2520identity%2520preservation.%2520Our%2520project%2520page%2520are%2520publicly%2520available%250Aat%2520https%253A//luoxyhappy.github.io/CanonSwap/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CanonSwap%3A%20High-Fidelity%20and%20Consistent%20Video%20Face%20Swapping%20via%0A%20%20Canonical%20Space%20Modulation&entry.906535625=Xiangyang%20Luo%20and%20Ye%20Zhu%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Cong%20Wan%20and%20Zijian%20Cai%20and%20Shao-Lun%20Huang%20and%20Yu%20Li&entry.1292438233=%20%20Video%20face%20swapping%20aims%20to%20address%20two%20primary%20challenges%3A%20effectively%0Atransferring%20the%20source%20identity%20to%20the%20target%20video%20and%20accurately%20preserving%0Athe%20dynamic%20attributes%20of%20the%20target%20face%2C%20such%20as%20head%20poses%2C%20facial%0Aexpressions%2C%20lip-sync%2C%20%5Cetc.%20Existing%20methods%20mainly%20focus%20on%20achieving%0Ahigh-quality%20identity%20transfer%20but%20often%20fall%20short%20in%20maintaining%20the%20dynamic%0Aattributes%20of%20the%20target%20face%2C%20leading%20to%20inconsistent%20results.%20We%20attribute%0Athis%20issue%20to%20the%20inherent%20coupling%20of%20facial%20appearance%20and%20motion%20in%20videos.%0ATo%20address%20this%2C%20we%20propose%20CanonSwap%2C%20a%20novel%20video%20face-swapping%20framework%0Athat%20decouples%20motion%20information%20from%20appearance%20information.%20Specifically%2C%0ACanonSwap%20first%20eliminates%20motion-related%20information%2C%20enabling%20identity%0Amodification%20within%20a%20unified%20canonical%20space.%20Subsequently%2C%20the%20swapped%0Afeature%20is%20reintegrated%20into%20the%20original%20video%20space%2C%20ensuring%20the%0Apreservation%20of%20the%20target%20face%27s%20dynamic%20attributes.%20To%20further%20achieve%0Aprecise%20identity%20transfer%20with%20minimal%20artifacts%20and%20enhanced%20realism%2C%20we%0Adesign%20a%20Partial%20Identity%20Modulation%20module%20that%20adaptively%20integrates%20source%0Aidentity%20features%20using%20a%20spatial%20mask%20to%20restrict%20modifications%20to%20facial%0Aregions.%20Additionally%2C%20we%20introduce%20several%20fine-grained%20synchronization%0Ametrics%20to%20comprehensively%20evaluate%20the%20performance%20of%20video%20face%20swapping%0Amethods.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20existing%20approaches%20in%20terms%20of%20visual%20quality%2C%20temporal%0Aconsistency%2C%20and%20identity%20preservation.%20Our%20project%20page%20are%20publicly%20available%0Aat%20https%3A//luoxyhappy.github.io/CanonSwap/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02691v1&entry.124074799=Read"},
{"title": "Weakly-supervised Contrastive Learning with Quantity Prompts for Moving\n  Infrared Small Target Detection", "author": "Weiwei Duan and Luping Ji and Shengjia Chen and Sicheng Zhu and Jianghong Huang and Mao Ye", "abstract": "  Different from general object detection, moving infrared small target\ndetection faces huge challenges due to tiny target size and weak background\ncontrast.Currently, most existing methods are fully-supervised, heavily relying\non a large number of manual target-wise annotations. However, manually\nannotating video sequences is often expensive and time-consuming, especially\nfor low-quality infrared frame images. Inspired by general object detection,\nnon-fully supervised strategies ($e.g.$, weakly supervised) are believed to be\npotential in reducing annotation requirements. To break through traditional\nfully-supervised frameworks, as the first exploration work, this paper proposes\na new weakly-supervised contrastive learning (WeCoL) scheme, only requires\nsimple target quantity prompts during model training.Specifically, in our\nscheme, based on the pretrained segment anything model (SAM), a potential\ntarget mining strategy is designed to integrate target activation maps and\nmulti-frame energy accumulation.Besides, contrastive learning is adopted to\nfurther improve the reliability of pseudo-labels, by calculating the similarity\nbetween positive and negative samples in feature subspace.Moreover, we propose\na long-short term motion-aware learning scheme to simultaneously model the\nlocal motion patterns and global motion trajectory of small targets.The\nextensive experiments on two public datasets (DAUB and ITSDT-15K) verify that\nour weakly-supervised scheme could often outperform early fully-supervised\nmethods. Even, its performance could reach over 90\\% of state-of-the-art (SOTA)\nfully-supervised ones.\n", "link": "http://arxiv.org/abs/2507.02454v1", "date": "2025-07-03", "relevancy": 2.2567, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6063}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5413}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly-supervised%20Contrastive%20Learning%20with%20Quantity%20Prompts%20for%20Moving%0A%20%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Weakly-supervised%20Contrastive%20Learning%20with%20Quantity%20Prompts%20for%20Moving%0A%20%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Weiwei%20Duan%20and%20Luping%20Ji%20and%20Shengjia%20Chen%20and%20Sicheng%20Zhu%20and%20Jianghong%20Huang%20and%20Mao%20Ye%0AAbstract%3A%20%20%20Different%20from%20general%20object%20detection%2C%20moving%20infrared%20small%20target%0Adetection%20faces%20huge%20challenges%20due%20to%20tiny%20target%20size%20and%20weak%20background%0Acontrast.Currently%2C%20most%20existing%20methods%20are%20fully-supervised%2C%20heavily%20relying%0Aon%20a%20large%20number%20of%20manual%20target-wise%20annotations.%20However%2C%20manually%0Aannotating%20video%20sequences%20is%20often%20expensive%20and%20time-consuming%2C%20especially%0Afor%20low-quality%20infrared%20frame%20images.%20Inspired%20by%20general%20object%20detection%2C%0Anon-fully%20supervised%20strategies%20%28%24e.g.%24%2C%20weakly%20supervised%29%20are%20believed%20to%20be%0Apotential%20in%20reducing%20annotation%20requirements.%20To%20break%20through%20traditional%0Afully-supervised%20frameworks%2C%20as%20the%20first%20exploration%20work%2C%20this%20paper%20proposes%0Aa%20new%20weakly-supervised%20contrastive%20learning%20%28WeCoL%29%20scheme%2C%20only%20requires%0Asimple%20target%20quantity%20prompts%20during%20model%20training.Specifically%2C%20in%20our%0Ascheme%2C%20based%20on%20the%20pretrained%20segment%20anything%20model%20%28SAM%29%2C%20a%20potential%0Atarget%20mining%20strategy%20is%20designed%20to%20integrate%20target%20activation%20maps%20and%0Amulti-frame%20energy%20accumulation.Besides%2C%20contrastive%20learning%20is%20adopted%20to%0Afurther%20improve%20the%20reliability%20of%20pseudo-labels%2C%20by%20calculating%20the%20similarity%0Abetween%20positive%20and%20negative%20samples%20in%20feature%20subspace.Moreover%2C%20we%20propose%0Aa%20long-short%20term%20motion-aware%20learning%20scheme%20to%20simultaneously%20model%20the%0Alocal%20motion%20patterns%20and%20global%20motion%20trajectory%20of%20small%20targets.The%0Aextensive%20experiments%20on%20two%20public%20datasets%20%28DAUB%20and%20ITSDT-15K%29%20verify%20that%0Aour%20weakly-supervised%20scheme%20could%20often%20outperform%20early%20fully-supervised%0Amethods.%20Even%2C%20its%20performance%20could%20reach%20over%2090%5C%25%20of%20state-of-the-art%20%28SOTA%29%0Afully-supervised%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly-supervised%2520Contrastive%2520Learning%2520with%2520Quantity%2520Prompts%2520for%2520Moving%250A%2520%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DWeiwei%2520Duan%2520and%2520Luping%2520Ji%2520and%2520Shengjia%2520Chen%2520and%2520Sicheng%2520Zhu%2520and%2520Jianghong%2520Huang%2520and%2520Mao%2520Ye%26entry.1292438233%3D%2520%2520Different%2520from%2520general%2520object%2520detection%252C%2520moving%2520infrared%2520small%2520target%250Adetection%2520faces%2520huge%2520challenges%2520due%2520to%2520tiny%2520target%2520size%2520and%2520weak%2520background%250Acontrast.Currently%252C%2520most%2520existing%2520methods%2520are%2520fully-supervised%252C%2520heavily%2520relying%250Aon%2520a%2520large%2520number%2520of%2520manual%2520target-wise%2520annotations.%2520However%252C%2520manually%250Aannotating%2520video%2520sequences%2520is%2520often%2520expensive%2520and%2520time-consuming%252C%2520especially%250Afor%2520low-quality%2520infrared%2520frame%2520images.%2520Inspired%2520by%2520general%2520object%2520detection%252C%250Anon-fully%2520supervised%2520strategies%2520%2528%2524e.g.%2524%252C%2520weakly%2520supervised%2529%2520are%2520believed%2520to%2520be%250Apotential%2520in%2520reducing%2520annotation%2520requirements.%2520To%2520break%2520through%2520traditional%250Afully-supervised%2520frameworks%252C%2520as%2520the%2520first%2520exploration%2520work%252C%2520this%2520paper%2520proposes%250Aa%2520new%2520weakly-supervised%2520contrastive%2520learning%2520%2528WeCoL%2529%2520scheme%252C%2520only%2520requires%250Asimple%2520target%2520quantity%2520prompts%2520during%2520model%2520training.Specifically%252C%2520in%2520our%250Ascheme%252C%2520based%2520on%2520the%2520pretrained%2520segment%2520anything%2520model%2520%2528SAM%2529%252C%2520a%2520potential%250Atarget%2520mining%2520strategy%2520is%2520designed%2520to%2520integrate%2520target%2520activation%2520maps%2520and%250Amulti-frame%2520energy%2520accumulation.Besides%252C%2520contrastive%2520learning%2520is%2520adopted%2520to%250Afurther%2520improve%2520the%2520reliability%2520of%2520pseudo-labels%252C%2520by%2520calculating%2520the%2520similarity%250Abetween%2520positive%2520and%2520negative%2520samples%2520in%2520feature%2520subspace.Moreover%252C%2520we%2520propose%250Aa%2520long-short%2520term%2520motion-aware%2520learning%2520scheme%2520to%2520simultaneously%2520model%2520the%250Alocal%2520motion%2520patterns%2520and%2520global%2520motion%2520trajectory%2520of%2520small%2520targets.The%250Aextensive%2520experiments%2520on%2520two%2520public%2520datasets%2520%2528DAUB%2520and%2520ITSDT-15K%2529%2520verify%2520that%250Aour%2520weakly-supervised%2520scheme%2520could%2520often%2520outperform%2520early%2520fully-supervised%250Amethods.%2520Even%252C%2520its%2520performance%2520could%2520reach%2520over%252090%255C%2525%2520of%2520state-of-the-art%2520%2528SOTA%2529%250Afully-supervised%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly-supervised%20Contrastive%20Learning%20with%20Quantity%20Prompts%20for%20Moving%0A%20%20Infrared%20Small%20Target%20Detection&entry.906535625=Weiwei%20Duan%20and%20Luping%20Ji%20and%20Shengjia%20Chen%20and%20Sicheng%20Zhu%20and%20Jianghong%20Huang%20and%20Mao%20Ye&entry.1292438233=%20%20Different%20from%20general%20object%20detection%2C%20moving%20infrared%20small%20target%0Adetection%20faces%20huge%20challenges%20due%20to%20tiny%20target%20size%20and%20weak%20background%0Acontrast.Currently%2C%20most%20existing%20methods%20are%20fully-supervised%2C%20heavily%20relying%0Aon%20a%20large%20number%20of%20manual%20target-wise%20annotations.%20However%2C%20manually%0Aannotating%20video%20sequences%20is%20often%20expensive%20and%20time-consuming%2C%20especially%0Afor%20low-quality%20infrared%20frame%20images.%20Inspired%20by%20general%20object%20detection%2C%0Anon-fully%20supervised%20strategies%20%28%24e.g.%24%2C%20weakly%20supervised%29%20are%20believed%20to%20be%0Apotential%20in%20reducing%20annotation%20requirements.%20To%20break%20through%20traditional%0Afully-supervised%20frameworks%2C%20as%20the%20first%20exploration%20work%2C%20this%20paper%20proposes%0Aa%20new%20weakly-supervised%20contrastive%20learning%20%28WeCoL%29%20scheme%2C%20only%20requires%0Asimple%20target%20quantity%20prompts%20during%20model%20training.Specifically%2C%20in%20our%0Ascheme%2C%20based%20on%20the%20pretrained%20segment%20anything%20model%20%28SAM%29%2C%20a%20potential%0Atarget%20mining%20strategy%20is%20designed%20to%20integrate%20target%20activation%20maps%20and%0Amulti-frame%20energy%20accumulation.Besides%2C%20contrastive%20learning%20is%20adopted%20to%0Afurther%20improve%20the%20reliability%20of%20pseudo-labels%2C%20by%20calculating%20the%20similarity%0Abetween%20positive%20and%20negative%20samples%20in%20feature%20subspace.Moreover%2C%20we%20propose%0Aa%20long-short%20term%20motion-aware%20learning%20scheme%20to%20simultaneously%20model%20the%0Alocal%20motion%20patterns%20and%20global%20motion%20trajectory%20of%20small%20targets.The%0Aextensive%20experiments%20on%20two%20public%20datasets%20%28DAUB%20and%20ITSDT-15K%29%20verify%20that%0Aour%20weakly-supervised%20scheme%20could%20often%20outperform%20early%20fully-supervised%0Amethods.%20Even%2C%20its%20performance%20could%20reach%20over%2090%5C%25%20of%20state-of-the-art%20%28SOTA%29%0Afully-supervised%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02454v1&entry.124074799=Read"},
{"title": "Fast and Simplex: 2-Simplicial Attention in Triton", "author": "Aurko Roy and Timothy Chou and Sai Surya Duvvuri and Sijia Chen and Jiecao Yu and Xiaodong Wang and Manzil Zaheer and Rohan Anil", "abstract": "  Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.\n", "link": "http://arxiv.org/abs/2507.02754v1", "date": "2025-07-03", "relevancy": 2.2566, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5967}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5415}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Simplex%3A%202-Simplicial%20Attention%20in%20Triton&body=Title%3A%20Fast%20and%20Simplex%3A%202-Simplicial%20Attention%20in%20Triton%0AAuthor%3A%20Aurko%20Roy%20and%20Timothy%20Chou%20and%20Sai%20Surya%20Duvvuri%20and%20Sijia%20Chen%20and%20Jiecao%20Yu%20and%20Xiaodong%20Wang%20and%20Manzil%20Zaheer%20and%20Rohan%20Anil%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%20training%20loss%20scales%20as%20a%20power%20law%20with%20both%0Amodel%20size%20and%20the%20number%20of%20tokens%2C%20and%20that%20achieving%20compute-optimal%20models%0Arequires%20scaling%20model%20size%20and%20token%20count%20together.%20However%2C%20these%20scaling%0Alaws%20assume%20an%20infinite%20supply%20of%20data%20and%20apply%20primarily%20in%20compute-bound%0Asettings.%20As%20modern%20large%20language%20models%20increasingly%20rely%20on%20massive%0Ainternet-scale%20datasets%2C%20the%20assumption%20that%20they%20are%20compute-bound%20is%20becoming%0Aless%20valid.%20This%20shift%20highlights%20the%20need%20for%20architectures%20that%20prioritize%0Atoken%20efficiency.%0A%20%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20the%202-simplicial%20Transformer%2C%20an%0Aarchitecture%20that%20generalizes%20standard%20dot-product%20attention%20to%20trilinear%0Afunctions%20through%20an%20efficient%20Triton%20kernel%20implementation.%20We%20demonstrate%0Athat%20the%202-simplicial%20Transformer%20achieves%20better%20token%20efficiency%20than%0Astandard%20Transformers%3A%20for%20a%20fixed%20token%20budget%2C%20similarly%20sized%20models%0Aoutperform%20their%20dot-product%20counterparts%20on%20tasks%20involving%20mathematics%2C%0Acoding%2C%20reasoning%2C%20and%20logic.%20We%20quantify%20these%20gains%20by%20demonstrating%20that%0A%242%24-simplicial%20attention%20changes%20the%20exponent%20in%20the%20scaling%20laws%20for%20knowledge%0Aand%20reasoning%20tasks%20compared%20to%20dot%20product%20attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Simplex%253A%25202-Simplicial%2520Attention%2520in%2520Triton%26entry.906535625%3DAurko%2520Roy%2520and%2520Timothy%2520Chou%2520and%2520Sai%2520Surya%2520Duvvuri%2520and%2520Sijia%2520Chen%2520and%2520Jiecao%2520Yu%2520and%2520Xiaodong%2520Wang%2520and%2520Manzil%2520Zaheer%2520and%2520Rohan%2520Anil%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%2520training%2520loss%2520scales%2520as%2520a%2520power%2520law%2520with%2520both%250Amodel%2520size%2520and%2520the%2520number%2520of%2520tokens%252C%2520and%2520that%2520achieving%2520compute-optimal%2520models%250Arequires%2520scaling%2520model%2520size%2520and%2520token%2520count%2520together.%2520However%252C%2520these%2520scaling%250Alaws%2520assume%2520an%2520infinite%2520supply%2520of%2520data%2520and%2520apply%2520primarily%2520in%2520compute-bound%250Asettings.%2520As%2520modern%2520large%2520language%2520models%2520increasingly%2520rely%2520on%2520massive%250Ainternet-scale%2520datasets%252C%2520the%2520assumption%2520that%2520they%2520are%2520compute-bound%2520is%2520becoming%250Aless%2520valid.%2520This%2520shift%2520highlights%2520the%2520need%2520for%2520architectures%2520that%2520prioritize%250Atoken%2520efficiency.%250A%2520%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520use%2520of%2520the%25202-simplicial%2520Transformer%252C%2520an%250Aarchitecture%2520that%2520generalizes%2520standard%2520dot-product%2520attention%2520to%2520trilinear%250Afunctions%2520through%2520an%2520efficient%2520Triton%2520kernel%2520implementation.%2520We%2520demonstrate%250Athat%2520the%25202-simplicial%2520Transformer%2520achieves%2520better%2520token%2520efficiency%2520than%250Astandard%2520Transformers%253A%2520for%2520a%2520fixed%2520token%2520budget%252C%2520similarly%2520sized%2520models%250Aoutperform%2520their%2520dot-product%2520counterparts%2520on%2520tasks%2520involving%2520mathematics%252C%250Acoding%252C%2520reasoning%252C%2520and%2520logic.%2520We%2520quantify%2520these%2520gains%2520by%2520demonstrating%2520that%250A%25242%2524-simplicial%2520attention%2520changes%2520the%2520exponent%2520in%2520the%2520scaling%2520laws%2520for%2520knowledge%250Aand%2520reasoning%2520tasks%2520compared%2520to%2520dot%2520product%2520attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Simplex%3A%202-Simplicial%20Attention%20in%20Triton&entry.906535625=Aurko%20Roy%20and%20Timothy%20Chou%20and%20Sai%20Surya%20Duvvuri%20and%20Sijia%20Chen%20and%20Jiecao%20Yu%20and%20Xiaodong%20Wang%20and%20Manzil%20Zaheer%20and%20Rohan%20Anil&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%20training%20loss%20scales%20as%20a%20power%20law%20with%20both%0Amodel%20size%20and%20the%20number%20of%20tokens%2C%20and%20that%20achieving%20compute-optimal%20models%0Arequires%20scaling%20model%20size%20and%20token%20count%20together.%20However%2C%20these%20scaling%0Alaws%20assume%20an%20infinite%20supply%20of%20data%20and%20apply%20primarily%20in%20compute-bound%0Asettings.%20As%20modern%20large%20language%20models%20increasingly%20rely%20on%20massive%0Ainternet-scale%20datasets%2C%20the%20assumption%20that%20they%20are%20compute-bound%20is%20becoming%0Aless%20valid.%20This%20shift%20highlights%20the%20need%20for%20architectures%20that%20prioritize%0Atoken%20efficiency.%0A%20%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20the%202-simplicial%20Transformer%2C%20an%0Aarchitecture%20that%20generalizes%20standard%20dot-product%20attention%20to%20trilinear%0Afunctions%20through%20an%20efficient%20Triton%20kernel%20implementation.%20We%20demonstrate%0Athat%20the%202-simplicial%20Transformer%20achieves%20better%20token%20efficiency%20than%0Astandard%20Transformers%3A%20for%20a%20fixed%20token%20budget%2C%20similarly%20sized%20models%0Aoutperform%20their%20dot-product%20counterparts%20on%20tasks%20involving%20mathematics%2C%0Acoding%2C%20reasoning%2C%20and%20logic.%20We%20quantify%20these%20gains%20by%20demonstrating%20that%0A%242%24-simplicial%20attention%20changes%20the%20exponent%20in%20the%20scaling%20laws%20for%20knowledge%0Aand%20reasoning%20tasks%20compared%20to%20dot%20product%20attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02754v1&entry.124074799=Read"},
{"title": "No time to train! Training-Free Reference-Based Instance Segmentation", "author": "Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley", "abstract": "  The performance of image segmentation models has historically been\nconstrained by the high cost of collecting large-scale annotated data. The\nSegment Anything Model (SAM) alleviates this original problem through a\npromptable, semantics-agnostic, segmentation paradigm and yet still requires\nmanual visual-prompts or complex domain-dependent prompt-generation rules to\nprocess a new image. Towards reducing this new burden, our work investigates\nthe task of object segmentation when provided with, alternatively, only a small\nset of reference images. Our key insight is to leverage strong semantic priors,\nas learned by foundation models, to identify corresponding regions between a\nreference and a target image. We find that correspondences enable automatic\ngeneration of instance-level segmentation masks for downstream tasks and\ninstantiate our ideas via a multi-stage, training-free method incorporating (1)\nmemory bank construction; (2) representation aggregation and (3) semantic-aware\nfeature matching. Our experiments show significant improvements on segmentation\nmetrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP),\nPASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free\napproaches on the Cross-Domain FSOD benchmark (22.4% nAP).\n", "link": "http://arxiv.org/abs/2507.02798v1", "date": "2025-07-03", "relevancy": 2.2548, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20time%20to%20train%21%20Training-Free%20Reference-Based%20Instance%20Segmentation&body=Title%3A%20No%20time%20to%20train%21%20Training-Free%20Reference-Based%20Instance%20Segmentation%0AAuthor%3A%20Miguel%20Espinosa%20and%20Chenhongyi%20Yang%20and%20Linus%20Ericsson%20and%20Steven%20McDonagh%20and%20Elliot%20J.%20Crowley%0AAbstract%3A%20%20%20The%20performance%20of%20image%20segmentation%20models%20has%20historically%20been%0Aconstrained%20by%20the%20high%20cost%20of%20collecting%20large-scale%20annotated%20data.%20The%0ASegment%20Anything%20Model%20%28SAM%29%20alleviates%20this%20original%20problem%20through%20a%0Apromptable%2C%20semantics-agnostic%2C%20segmentation%20paradigm%20and%20yet%20still%20requires%0Amanual%20visual-prompts%20or%20complex%20domain-dependent%20prompt-generation%20rules%20to%0Aprocess%20a%20new%20image.%20Towards%20reducing%20this%20new%20burden%2C%20our%20work%20investigates%0Athe%20task%20of%20object%20segmentation%20when%20provided%20with%2C%20alternatively%2C%20only%20a%20small%0Aset%20of%20reference%20images.%20Our%20key%20insight%20is%20to%20leverage%20strong%20semantic%20priors%2C%0Aas%20learned%20by%20foundation%20models%2C%20to%20identify%20corresponding%20regions%20between%20a%0Areference%20and%20a%20target%20image.%20We%20find%20that%20correspondences%20enable%20automatic%0Ageneration%20of%20instance-level%20segmentation%20masks%20for%20downstream%20tasks%20and%0Ainstantiate%20our%20ideas%20via%20a%20multi-stage%2C%20training-free%20method%20incorporating%20%281%29%0Amemory%20bank%20construction%3B%20%282%29%20representation%20aggregation%20and%20%283%29%20semantic-aware%0Afeature%20matching.%20Our%20experiments%20show%20significant%20improvements%20on%20segmentation%0Ametrics%2C%20leading%20to%20state-of-the-art%20performance%20on%20COCO%20FSOD%20%2836.8%25%20nAP%29%2C%0APASCAL%20VOC%20Few-Shot%20%2871.2%25%20nAP50%29%20and%20outperforming%20existing%20training-free%0Aapproaches%20on%20the%20Cross-Domain%20FSOD%20benchmark%20%2822.4%25%20nAP%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520time%2520to%2520train%2521%2520Training-Free%2520Reference-Based%2520Instance%2520Segmentation%26entry.906535625%3DMiguel%2520Espinosa%2520and%2520Chenhongyi%2520Yang%2520and%2520Linus%2520Ericsson%2520and%2520Steven%2520McDonagh%2520and%2520Elliot%2520J.%2520Crowley%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520image%2520segmentation%2520models%2520has%2520historically%2520been%250Aconstrained%2520by%2520the%2520high%2520cost%2520of%2520collecting%2520large-scale%2520annotated%2520data.%2520The%250ASegment%2520Anything%2520Model%2520%2528SAM%2529%2520alleviates%2520this%2520original%2520problem%2520through%2520a%250Apromptable%252C%2520semantics-agnostic%252C%2520segmentation%2520paradigm%2520and%2520yet%2520still%2520requires%250Amanual%2520visual-prompts%2520or%2520complex%2520domain-dependent%2520prompt-generation%2520rules%2520to%250Aprocess%2520a%2520new%2520image.%2520Towards%2520reducing%2520this%2520new%2520burden%252C%2520our%2520work%2520investigates%250Athe%2520task%2520of%2520object%2520segmentation%2520when%2520provided%2520with%252C%2520alternatively%252C%2520only%2520a%2520small%250Aset%2520of%2520reference%2520images.%2520Our%2520key%2520insight%2520is%2520to%2520leverage%2520strong%2520semantic%2520priors%252C%250Aas%2520learned%2520by%2520foundation%2520models%252C%2520to%2520identify%2520corresponding%2520regions%2520between%2520a%250Areference%2520and%2520a%2520target%2520image.%2520We%2520find%2520that%2520correspondences%2520enable%2520automatic%250Ageneration%2520of%2520instance-level%2520segmentation%2520masks%2520for%2520downstream%2520tasks%2520and%250Ainstantiate%2520our%2520ideas%2520via%2520a%2520multi-stage%252C%2520training-free%2520method%2520incorporating%2520%25281%2529%250Amemory%2520bank%2520construction%253B%2520%25282%2529%2520representation%2520aggregation%2520and%2520%25283%2529%2520semantic-aware%250Afeature%2520matching.%2520Our%2520experiments%2520show%2520significant%2520improvements%2520on%2520segmentation%250Ametrics%252C%2520leading%2520to%2520state-of-the-art%2520performance%2520on%2520COCO%2520FSOD%2520%252836.8%2525%2520nAP%2529%252C%250APASCAL%2520VOC%2520Few-Shot%2520%252871.2%2525%2520nAP50%2529%2520and%2520outperforming%2520existing%2520training-free%250Aapproaches%2520on%2520the%2520Cross-Domain%2520FSOD%2520benchmark%2520%252822.4%2525%2520nAP%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20time%20to%20train%21%20Training-Free%20Reference-Based%20Instance%20Segmentation&entry.906535625=Miguel%20Espinosa%20and%20Chenhongyi%20Yang%20and%20Linus%20Ericsson%20and%20Steven%20McDonagh%20and%20Elliot%20J.%20Crowley&entry.1292438233=%20%20The%20performance%20of%20image%20segmentation%20models%20has%20historically%20been%0Aconstrained%20by%20the%20high%20cost%20of%20collecting%20large-scale%20annotated%20data.%20The%0ASegment%20Anything%20Model%20%28SAM%29%20alleviates%20this%20original%20problem%20through%20a%0Apromptable%2C%20semantics-agnostic%2C%20segmentation%20paradigm%20and%20yet%20still%20requires%0Amanual%20visual-prompts%20or%20complex%20domain-dependent%20prompt-generation%20rules%20to%0Aprocess%20a%20new%20image.%20Towards%20reducing%20this%20new%20burden%2C%20our%20work%20investigates%0Athe%20task%20of%20object%20segmentation%20when%20provided%20with%2C%20alternatively%2C%20only%20a%20small%0Aset%20of%20reference%20images.%20Our%20key%20insight%20is%20to%20leverage%20strong%20semantic%20priors%2C%0Aas%20learned%20by%20foundation%20models%2C%20to%20identify%20corresponding%20regions%20between%20a%0Areference%20and%20a%20target%20image.%20We%20find%20that%20correspondences%20enable%20automatic%0Ageneration%20of%20instance-level%20segmentation%20masks%20for%20downstream%20tasks%20and%0Ainstantiate%20our%20ideas%20via%20a%20multi-stage%2C%20training-free%20method%20incorporating%20%281%29%0Amemory%20bank%20construction%3B%20%282%29%20representation%20aggregation%20and%20%283%29%20semantic-aware%0Afeature%20matching.%20Our%20experiments%20show%20significant%20improvements%20on%20segmentation%0Ametrics%2C%20leading%20to%20state-of-the-art%20performance%20on%20COCO%20FSOD%20%2836.8%25%20nAP%29%2C%0APASCAL%20VOC%20Few-Shot%20%2871.2%25%20nAP50%29%20and%20outperforming%20existing%20training-free%0Aapproaches%20on%20the%20Cross-Domain%20FSOD%20benchmark%20%2822.4%25%20nAP%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02798v1&entry.124074799=Read"},
{"title": "HAPI: A Model for Learning Robot Facial Expressions from Human\n  Preferences", "author": "Dongsheng Yang and Qianying Liu and Wataru Sato and Takashi Minato and Chaoran Liu and Shin'ya Nishida", "abstract": "  Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.\n", "link": "http://arxiv.org/abs/2503.17046v2", "date": "2025-07-03", "relevancy": 2.2457, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5766}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5598}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAPI%3A%20A%20Model%20for%20Learning%20Robot%20Facial%20Expressions%20from%20Human%0A%20%20Preferences&body=Title%3A%20HAPI%3A%20A%20Model%20for%20Learning%20Robot%20Facial%20Expressions%20from%20Human%0A%20%20Preferences%0AAuthor%3A%20Dongsheng%20Yang%20and%20Qianying%20Liu%20and%20Wataru%20Sato%20and%20Takashi%20Minato%20and%20Chaoran%20Liu%20and%20Shin%27ya%20Nishida%0AAbstract%3A%20%20%20Automatic%20robotic%20facial%20expression%20generation%20is%20crucial%20for%20human-robot%0Ainteraction%2C%20as%20handcrafted%20methods%20based%20on%20fixed%20joint%20configurations%20often%0Ayield%20rigid%20and%20unnatural%20behaviors.%20Although%20recent%20automated%20techniques%0Areduce%20the%20need%20for%20manual%20tuning%2C%20they%20tend%20to%20fall%20short%20by%20not%20adequately%0Abridging%20the%20gap%20between%20human%20preferences%20and%20model%20predictions-resulting%20in%20a%0Adeficiency%20of%20nuanced%20and%20realistic%20expressions%20due%20to%20limited%20degrees%20of%0Afreedom%20and%20insufficient%20perceptual%20integration.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20learning-to-rank%20framework%20that%20leverages%20human%20feedback%20to%20address%20this%0Adiscrepancy%20and%20enhanced%20the%20expressiveness%20of%20robotic%20faces.%20Specifically%2C%20we%0Aconduct%20pairwise%20comparison%20annotations%20to%20collect%20human%20preference%20data%20and%0Adevelop%20the%20Human%20Affective%20Pairwise%20Impressions%20%28HAPI%29%20model%2C%20a%20Siamese%0ARankNet-based%20approach%20that%20refines%20expression%20evaluation.%20Results%20obtained%20via%0ABayesian%20Optimization%20and%20online%20expression%20survey%20on%20a%2035-DOF%20android%20platform%0Ademonstrate%20that%20our%20approach%20produces%20significantly%20more%20realistic%20and%0Asocially%20resonant%20expressions%20of%20Anger%2C%20Happiness%2C%20and%20Surprise%20than%20those%0Agenerated%20by%20baseline%20and%20expert-designed%20methods.%20This%20confirms%20that%20our%0Aframework%20effectively%20bridges%20the%20gap%20between%20human%20preferences%20and%20model%0Apredictions%20while%20robustly%20aligning%20robotic%20expression%20generation%20with%20human%0Aaffective%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAPI%253A%2520A%2520Model%2520for%2520Learning%2520Robot%2520Facial%2520Expressions%2520from%2520Human%250A%2520%2520Preferences%26entry.906535625%3DDongsheng%2520Yang%2520and%2520Qianying%2520Liu%2520and%2520Wataru%2520Sato%2520and%2520Takashi%2520Minato%2520and%2520Chaoran%2520Liu%2520and%2520Shin%2527ya%2520Nishida%26entry.1292438233%3D%2520%2520Automatic%2520robotic%2520facial%2520expression%2520generation%2520is%2520crucial%2520for%2520human-robot%250Ainteraction%252C%2520as%2520handcrafted%2520methods%2520based%2520on%2520fixed%2520joint%2520configurations%2520often%250Ayield%2520rigid%2520and%2520unnatural%2520behaviors.%2520Although%2520recent%2520automated%2520techniques%250Areduce%2520the%2520need%2520for%2520manual%2520tuning%252C%2520they%2520tend%2520to%2520fall%2520short%2520by%2520not%2520adequately%250Abridging%2520the%2520gap%2520between%2520human%2520preferences%2520and%2520model%2520predictions-resulting%2520in%2520a%250Adeficiency%2520of%2520nuanced%2520and%2520realistic%2520expressions%2520due%2520to%2520limited%2520degrees%2520of%250Afreedom%2520and%2520insufficient%2520perceptual%2520integration.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520learning-to-rank%2520framework%2520that%2520leverages%2520human%2520feedback%2520to%2520address%2520this%250Adiscrepancy%2520and%2520enhanced%2520the%2520expressiveness%2520of%2520robotic%2520faces.%2520Specifically%252C%2520we%250Aconduct%2520pairwise%2520comparison%2520annotations%2520to%2520collect%2520human%2520preference%2520data%2520and%250Adevelop%2520the%2520Human%2520Affective%2520Pairwise%2520Impressions%2520%2528HAPI%2529%2520model%252C%2520a%2520Siamese%250ARankNet-based%2520approach%2520that%2520refines%2520expression%2520evaluation.%2520Results%2520obtained%2520via%250ABayesian%2520Optimization%2520and%2520online%2520expression%2520survey%2520on%2520a%252035-DOF%2520android%2520platform%250Ademonstrate%2520that%2520our%2520approach%2520produces%2520significantly%2520more%2520realistic%2520and%250Asocially%2520resonant%2520expressions%2520of%2520Anger%252C%2520Happiness%252C%2520and%2520Surprise%2520than%2520those%250Agenerated%2520by%2520baseline%2520and%2520expert-designed%2520methods.%2520This%2520confirms%2520that%2520our%250Aframework%2520effectively%2520bridges%2520the%2520gap%2520between%2520human%2520preferences%2520and%2520model%250Apredictions%2520while%2520robustly%2520aligning%2520robotic%2520expression%2520generation%2520with%2520human%250Aaffective%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAPI%3A%20A%20Model%20for%20Learning%20Robot%20Facial%20Expressions%20from%20Human%0A%20%20Preferences&entry.906535625=Dongsheng%20Yang%20and%20Qianying%20Liu%20and%20Wataru%20Sato%20and%20Takashi%20Minato%20and%20Chaoran%20Liu%20and%20Shin%27ya%20Nishida&entry.1292438233=%20%20Automatic%20robotic%20facial%20expression%20generation%20is%20crucial%20for%20human-robot%0Ainteraction%2C%20as%20handcrafted%20methods%20based%20on%20fixed%20joint%20configurations%20often%0Ayield%20rigid%20and%20unnatural%20behaviors.%20Although%20recent%20automated%20techniques%0Areduce%20the%20need%20for%20manual%20tuning%2C%20they%20tend%20to%20fall%20short%20by%20not%20adequately%0Abridging%20the%20gap%20between%20human%20preferences%20and%20model%20predictions-resulting%20in%20a%0Adeficiency%20of%20nuanced%20and%20realistic%20expressions%20due%20to%20limited%20degrees%20of%0Afreedom%20and%20insufficient%20perceptual%20integration.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20learning-to-rank%20framework%20that%20leverages%20human%20feedback%20to%20address%20this%0Adiscrepancy%20and%20enhanced%20the%20expressiveness%20of%20robotic%20faces.%20Specifically%2C%20we%0Aconduct%20pairwise%20comparison%20annotations%20to%20collect%20human%20preference%20data%20and%0Adevelop%20the%20Human%20Affective%20Pairwise%20Impressions%20%28HAPI%29%20model%2C%20a%20Siamese%0ARankNet-based%20approach%20that%20refines%20expression%20evaluation.%20Results%20obtained%20via%0ABayesian%20Optimization%20and%20online%20expression%20survey%20on%20a%2035-DOF%20android%20platform%0Ademonstrate%20that%20our%20approach%20produces%20significantly%20more%20realistic%20and%0Asocially%20resonant%20expressions%20of%20Anger%2C%20Happiness%2C%20and%20Surprise%20than%20those%0Agenerated%20by%20baseline%20and%20expert-designed%20methods.%20This%20confirms%20that%20our%0Aframework%20effectively%20bridges%20the%20gap%20between%20human%20preferences%20and%20model%0Apredictions%20while%20robustly%20aligning%20robotic%20expression%20generation%20with%20human%0Aaffective%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17046v2&entry.124074799=Read"},
{"title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing\n  Framework with Multimodal Narrative Understanding", "author": "Xiangfeng Wang and Xiao Li and Yadong Wei and Xueyu Song and Yang Song and Xiaoqiang Xia and Fangrui Zeng and Zaiyi Chen and Liu Liu and Gu Xu and Tong Xu", "abstract": "  The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos.\n", "link": "http://arxiv.org/abs/2507.02790v1", "date": "2025-07-03", "relevancy": 2.2447, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5883}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding&body=Title%3A%20From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding%0AAuthor%3A%20Xiangfeng%20Wang%20and%20Xiao%20Li%20and%20Yadong%20Wei%20and%20Xueyu%20Song%20and%20Yang%20Song%20and%20Xiaoqiang%20Xia%20and%20Fangrui%20Zeng%20and%20Zaiyi%20Chen%20and%20Liu%20Liu%20and%20Gu%20Xu%20and%20Tong%20Xu%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20online%20video%20content%2C%20especially%20on%20short%20video%0Aplatforms%2C%20has%20created%20a%20growing%20demand%20for%20efficient%20video%20editing%20techniques%0Athat%20can%20condense%20long-form%20videos%20into%20concise%20and%20engaging%20clips.%20Existing%0Aautomatic%20editing%20methods%20predominantly%20rely%20on%20textual%20cues%20from%20ASR%0Atranscripts%20and%20end-to-end%20segment%20selection%2C%20often%20neglecting%20the%20rich%20visual%0Acontext%20and%20leading%20to%20incoherent%20outputs.%20In%20this%20paper%2C%20we%20propose%20a%0Ahuman-inspired%20automatic%20video%20editing%20framework%20%28HIVE%29%20that%20leverages%0Amultimodal%20narrative%20understanding%20to%20address%20these%20limitations.%20Our%20approach%0Aincorporates%20character%20extraction%2C%20dialogue%20analysis%2C%20and%20narrative%0Asummarization%20through%20multimodal%20large%20language%20models%2C%20enabling%20a%20holistic%0Aunderstanding%20of%20the%20video%20content.%20To%20further%20enhance%20coherence%2C%20we%20apply%0Ascene-level%20segmentation%20and%20decompose%20the%20editing%20process%20into%20three%20subtasks%3A%0Ahighlight%20detection%2C%20opening/ending%20selection%2C%20and%20pruning%20of%20irrelevant%0Acontent.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20introduce%20DramaAD%2C%20a%20novel%0Abenchmark%20dataset%20comprising%20over%20800%20short%20drama%20episodes%20and%20500%0Aprofessionally%20edited%20advertisement%20clips.%20Experimental%20results%20demonstrate%0Athat%20our%20framework%20consistently%20outperforms%20existing%20baselines%20across%20both%0Ageneral%20and%20advertisement-oriented%20editing%20tasks%2C%20significantly%20narrowing%20the%0Aquality%20gap%20between%20automatic%20and%20human-edited%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Long%2520Videos%2520to%2520Engaging%2520Clips%253A%2520A%2520Human-Inspired%2520Video%2520Editing%250A%2520%2520Framework%2520with%2520Multimodal%2520Narrative%2520Understanding%26entry.906535625%3DXiangfeng%2520Wang%2520and%2520Xiao%2520Li%2520and%2520Yadong%2520Wei%2520and%2520Xueyu%2520Song%2520and%2520Yang%2520Song%2520and%2520Xiaoqiang%2520Xia%2520and%2520Fangrui%2520Zeng%2520and%2520Zaiyi%2520Chen%2520and%2520Liu%2520Liu%2520and%2520Gu%2520Xu%2520and%2520Tong%2520Xu%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520online%2520video%2520content%252C%2520especially%2520on%2520short%2520video%250Aplatforms%252C%2520has%2520created%2520a%2520growing%2520demand%2520for%2520efficient%2520video%2520editing%2520techniques%250Athat%2520can%2520condense%2520long-form%2520videos%2520into%2520concise%2520and%2520engaging%2520clips.%2520Existing%250Aautomatic%2520editing%2520methods%2520predominantly%2520rely%2520on%2520textual%2520cues%2520from%2520ASR%250Atranscripts%2520and%2520end-to-end%2520segment%2520selection%252C%2520often%2520neglecting%2520the%2520rich%2520visual%250Acontext%2520and%2520leading%2520to%2520incoherent%2520outputs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Ahuman-inspired%2520automatic%2520video%2520editing%2520framework%2520%2528HIVE%2529%2520that%2520leverages%250Amultimodal%2520narrative%2520understanding%2520to%2520address%2520these%2520limitations.%2520Our%2520approach%250Aincorporates%2520character%2520extraction%252C%2520dialogue%2520analysis%252C%2520and%2520narrative%250Asummarization%2520through%2520multimodal%2520large%2520language%2520models%252C%2520enabling%2520a%2520holistic%250Aunderstanding%2520of%2520the%2520video%2520content.%2520To%2520further%2520enhance%2520coherence%252C%2520we%2520apply%250Ascene-level%2520segmentation%2520and%2520decompose%2520the%2520editing%2520process%2520into%2520three%2520subtasks%253A%250Ahighlight%2520detection%252C%2520opening/ending%2520selection%252C%2520and%2520pruning%2520of%2520irrelevant%250Acontent.%2520To%2520facilitate%2520research%2520in%2520this%2520area%252C%2520we%2520introduce%2520DramaAD%252C%2520a%2520novel%250Abenchmark%2520dataset%2520comprising%2520over%2520800%2520short%2520drama%2520episodes%2520and%2520500%250Aprofessionally%2520edited%2520advertisement%2520clips.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520framework%2520consistently%2520outperforms%2520existing%2520baselines%2520across%2520both%250Ageneral%2520and%2520advertisement-oriented%2520editing%2520tasks%252C%2520significantly%2520narrowing%2520the%250Aquality%2520gap%2520between%2520automatic%2520and%2520human-edited%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding&entry.906535625=Xiangfeng%20Wang%20and%20Xiao%20Li%20and%20Yadong%20Wei%20and%20Xueyu%20Song%20and%20Yang%20Song%20and%20Xiaoqiang%20Xia%20and%20Fangrui%20Zeng%20and%20Zaiyi%20Chen%20and%20Liu%20Liu%20and%20Gu%20Xu%20and%20Tong%20Xu&entry.1292438233=%20%20The%20rapid%20growth%20of%20online%20video%20content%2C%20especially%20on%20short%20video%0Aplatforms%2C%20has%20created%20a%20growing%20demand%20for%20efficient%20video%20editing%20techniques%0Athat%20can%20condense%20long-form%20videos%20into%20concise%20and%20engaging%20clips.%20Existing%0Aautomatic%20editing%20methods%20predominantly%20rely%20on%20textual%20cues%20from%20ASR%0Atranscripts%20and%20end-to-end%20segment%20selection%2C%20often%20neglecting%20the%20rich%20visual%0Acontext%20and%20leading%20to%20incoherent%20outputs.%20In%20this%20paper%2C%20we%20propose%20a%0Ahuman-inspired%20automatic%20video%20editing%20framework%20%28HIVE%29%20that%20leverages%0Amultimodal%20narrative%20understanding%20to%20address%20these%20limitations.%20Our%20approach%0Aincorporates%20character%20extraction%2C%20dialogue%20analysis%2C%20and%20narrative%0Asummarization%20through%20multimodal%20large%20language%20models%2C%20enabling%20a%20holistic%0Aunderstanding%20of%20the%20video%20content.%20To%20further%20enhance%20coherence%2C%20we%20apply%0Ascene-level%20segmentation%20and%20decompose%20the%20editing%20process%20into%20three%20subtasks%3A%0Ahighlight%20detection%2C%20opening/ending%20selection%2C%20and%20pruning%20of%20irrelevant%0Acontent.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20introduce%20DramaAD%2C%20a%20novel%0Abenchmark%20dataset%20comprising%20over%20800%20short%20drama%20episodes%20and%20500%0Aprofessionally%20edited%20advertisement%20clips.%20Experimental%20results%20demonstrate%0Athat%20our%20framework%20consistently%20outperforms%20existing%20baselines%20across%20both%0Ageneral%20and%20advertisement-oriented%20editing%20tasks%2C%20significantly%20narrowing%20the%0Aquality%20gap%20between%20automatic%20and%20human-edited%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02790v1&entry.124074799=Read"},
{"title": "Real-time Image-based Lighting of Glints", "author": "Tom Kneiphof and Reinhard Klein", "abstract": "  Image-based lighting is a widely used technique to reproduce shading under\nreal-world lighting conditions, especially in real-time rendering applications.\nA particularly challenging scenario involves materials exhibiting a sparkling\nor glittering appearance, caused by discrete microfacets scattered across their\nsurface. In this paper, we propose an efficient approximation for image-based\nlighting of glints, enabling fully dynamic material properties and environment\nmaps. Our novel approach is grounded in real-time glint rendering under area\nlight illumination and employs standard environment map filtering techniques.\nCrucially, our environment map filtering process is sufficiently fast to be\nexecuted on a per-frame basis. Our method assumes that the environment map is\npartitioned into few homogeneous regions of constant radiance. By filtering the\ncorresponding indicator functions with the normal distribution function, we\nobtain the probabilities for individual microfacets to reflect light from each\nregion. During shading, these probabilities are utilized to hierarchically\nsample a multinomial distribution, facilitated by our novel dual-gated Gaussian\napproximation of binomial distributions. We validate that our real-time\napproximation is close to ground-truth renderings for a range of material\nproperties and lighting conditions, and demonstrate robust and stable\nperformance, with little overhead over rendering glints from a single\ndirectional light. Compared to rendering smooth materials without glints, our\napproach requires twice as much memory to store the prefiltered environment\nmap.\n", "link": "http://arxiv.org/abs/2507.02674v1", "date": "2025-07-03", "relevancy": 2.2441, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5872}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5495}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Image-based%20Lighting%20of%20Glints&body=Title%3A%20Real-time%20Image-based%20Lighting%20of%20Glints%0AAuthor%3A%20Tom%20Kneiphof%20and%20Reinhard%20Klein%0AAbstract%3A%20%20%20Image-based%20lighting%20is%20a%20widely%20used%20technique%20to%20reproduce%20shading%20under%0Areal-world%20lighting%20conditions%2C%20especially%20in%20real-time%20rendering%20applications.%0AA%20particularly%20challenging%20scenario%20involves%20materials%20exhibiting%20a%20sparkling%0Aor%20glittering%20appearance%2C%20caused%20by%20discrete%20microfacets%20scattered%20across%20their%0Asurface.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20approximation%20for%20image-based%0Alighting%20of%20glints%2C%20enabling%20fully%20dynamic%20material%20properties%20and%20environment%0Amaps.%20Our%20novel%20approach%20is%20grounded%20in%20real-time%20glint%20rendering%20under%20area%0Alight%20illumination%20and%20employs%20standard%20environment%20map%20filtering%20techniques.%0ACrucially%2C%20our%20environment%20map%20filtering%20process%20is%20sufficiently%20fast%20to%20be%0Aexecuted%20on%20a%20per-frame%20basis.%20Our%20method%20assumes%20that%20the%20environment%20map%20is%0Apartitioned%20into%20few%20homogeneous%20regions%20of%20constant%20radiance.%20By%20filtering%20the%0Acorresponding%20indicator%20functions%20with%20the%20normal%20distribution%20function%2C%20we%0Aobtain%20the%20probabilities%20for%20individual%20microfacets%20to%20reflect%20light%20from%20each%0Aregion.%20During%20shading%2C%20these%20probabilities%20are%20utilized%20to%20hierarchically%0Asample%20a%20multinomial%20distribution%2C%20facilitated%20by%20our%20novel%20dual-gated%20Gaussian%0Aapproximation%20of%20binomial%20distributions.%20We%20validate%20that%20our%20real-time%0Aapproximation%20is%20close%20to%20ground-truth%20renderings%20for%20a%20range%20of%20material%0Aproperties%20and%20lighting%20conditions%2C%20and%20demonstrate%20robust%20and%20stable%0Aperformance%2C%20with%20little%20overhead%20over%20rendering%20glints%20from%20a%20single%0Adirectional%20light.%20Compared%20to%20rendering%20smooth%20materials%20without%20glints%2C%20our%0Aapproach%20requires%20twice%20as%20much%20memory%20to%20store%20the%20prefiltered%20environment%0Amap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Image-based%2520Lighting%2520of%2520Glints%26entry.906535625%3DTom%2520Kneiphof%2520and%2520Reinhard%2520Klein%26entry.1292438233%3D%2520%2520Image-based%2520lighting%2520is%2520a%2520widely%2520used%2520technique%2520to%2520reproduce%2520shading%2520under%250Areal-world%2520lighting%2520conditions%252C%2520especially%2520in%2520real-time%2520rendering%2520applications.%250AA%2520particularly%2520challenging%2520scenario%2520involves%2520materials%2520exhibiting%2520a%2520sparkling%250Aor%2520glittering%2520appearance%252C%2520caused%2520by%2520discrete%2520microfacets%2520scattered%2520across%2520their%250Asurface.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520efficient%2520approximation%2520for%2520image-based%250Alighting%2520of%2520glints%252C%2520enabling%2520fully%2520dynamic%2520material%2520properties%2520and%2520environment%250Amaps.%2520Our%2520novel%2520approach%2520is%2520grounded%2520in%2520real-time%2520glint%2520rendering%2520under%2520area%250Alight%2520illumination%2520and%2520employs%2520standard%2520environment%2520map%2520filtering%2520techniques.%250ACrucially%252C%2520our%2520environment%2520map%2520filtering%2520process%2520is%2520sufficiently%2520fast%2520to%2520be%250Aexecuted%2520on%2520a%2520per-frame%2520basis.%2520Our%2520method%2520assumes%2520that%2520the%2520environment%2520map%2520is%250Apartitioned%2520into%2520few%2520homogeneous%2520regions%2520of%2520constant%2520radiance.%2520By%2520filtering%2520the%250Acorresponding%2520indicator%2520functions%2520with%2520the%2520normal%2520distribution%2520function%252C%2520we%250Aobtain%2520the%2520probabilities%2520for%2520individual%2520microfacets%2520to%2520reflect%2520light%2520from%2520each%250Aregion.%2520During%2520shading%252C%2520these%2520probabilities%2520are%2520utilized%2520to%2520hierarchically%250Asample%2520a%2520multinomial%2520distribution%252C%2520facilitated%2520by%2520our%2520novel%2520dual-gated%2520Gaussian%250Aapproximation%2520of%2520binomial%2520distributions.%2520We%2520validate%2520that%2520our%2520real-time%250Aapproximation%2520is%2520close%2520to%2520ground-truth%2520renderings%2520for%2520a%2520range%2520of%2520material%250Aproperties%2520and%2520lighting%2520conditions%252C%2520and%2520demonstrate%2520robust%2520and%2520stable%250Aperformance%252C%2520with%2520little%2520overhead%2520over%2520rendering%2520glints%2520from%2520a%2520single%250Adirectional%2520light.%2520Compared%2520to%2520rendering%2520smooth%2520materials%2520without%2520glints%252C%2520our%250Aapproach%2520requires%2520twice%2520as%2520much%2520memory%2520to%2520store%2520the%2520prefiltered%2520environment%250Amap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Image-based%20Lighting%20of%20Glints&entry.906535625=Tom%20Kneiphof%20and%20Reinhard%20Klein&entry.1292438233=%20%20Image-based%20lighting%20is%20a%20widely%20used%20technique%20to%20reproduce%20shading%20under%0Areal-world%20lighting%20conditions%2C%20especially%20in%20real-time%20rendering%20applications.%0AA%20particularly%20challenging%20scenario%20involves%20materials%20exhibiting%20a%20sparkling%0Aor%20glittering%20appearance%2C%20caused%20by%20discrete%20microfacets%20scattered%20across%20their%0Asurface.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20approximation%20for%20image-based%0Alighting%20of%20glints%2C%20enabling%20fully%20dynamic%20material%20properties%20and%20environment%0Amaps.%20Our%20novel%20approach%20is%20grounded%20in%20real-time%20glint%20rendering%20under%20area%0Alight%20illumination%20and%20employs%20standard%20environment%20map%20filtering%20techniques.%0ACrucially%2C%20our%20environment%20map%20filtering%20process%20is%20sufficiently%20fast%20to%20be%0Aexecuted%20on%20a%20per-frame%20basis.%20Our%20method%20assumes%20that%20the%20environment%20map%20is%0Apartitioned%20into%20few%20homogeneous%20regions%20of%20constant%20radiance.%20By%20filtering%20the%0Acorresponding%20indicator%20functions%20with%20the%20normal%20distribution%20function%2C%20we%0Aobtain%20the%20probabilities%20for%20individual%20microfacets%20to%20reflect%20light%20from%20each%0Aregion.%20During%20shading%2C%20these%20probabilities%20are%20utilized%20to%20hierarchically%0Asample%20a%20multinomial%20distribution%2C%20facilitated%20by%20our%20novel%20dual-gated%20Gaussian%0Aapproximation%20of%20binomial%20distributions.%20We%20validate%20that%20our%20real-time%0Aapproximation%20is%20close%20to%20ground-truth%20renderings%20for%20a%20range%20of%20material%0Aproperties%20and%20lighting%20conditions%2C%20and%20demonstrate%20robust%20and%20stable%0Aperformance%2C%20with%20little%20overhead%20over%20rendering%20glints%20from%20a%20single%0Adirectional%20light.%20Compared%20to%20rendering%20smooth%20materials%20without%20glints%2C%20our%0Aapproach%20requires%20twice%20as%20much%20memory%20to%20store%20the%20prefiltered%20environment%0Amap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02674v1&entry.124074799=Read"},
{"title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under\n  Non-Convex Constraints", "author": "Shivam Chaubey and Francesco Verdoja and Shankar Deka and Ville Kyrki", "abstract": "  Shared control combines human intention with autonomous decision-making, from\nlow-level safety overrides to high-level task guidance, enabling systems that\nadapt to users while ensuring safety and performance. This enhances task\neffectiveness and user experience across domains such as assistive robotics,\nteleoperation, and autonomous driving. However, existing shared control\nmethods, based on e.g. Model Predictive Control, Control Barrier Functions, or\nlearning-based control, struggle with feasibility, scalability, or safety\nguarantees, particularly since the user input is unpredictable.\n  To address these challenges, we propose an assistive controller framework\nbased on Constrained Optimal Control Problem that incorporates an\noffline-computed Control Invariant Set, enabling online computation of control\nactions that ensure feasibility, strict constraint satisfaction, and minimal\noverride of user intent. Moreover, the framework can accommodate structured\nclass of non-convex constraints, which are common in real-world scenarios. We\nvalidate the approach through a large-scale user study with 66\nparticipants--one of the most extensive in shared control research--using a\ncomputer game environment to assess task load, trust, and perceived control, in\naddition to performance. The results show consistent improvements across all\nthese aspects without compromising safety and user intent.\n", "link": "http://arxiv.org/abs/2507.02438v1", "date": "2025-07-03", "relevancy": 2.238, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5692}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5613}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MISC%3A%20Minimal%20Intervention%20Shared%20Control%20with%20Guaranteed%20Safety%20under%0A%20%20Non-Convex%20Constraints&body=Title%3A%20MISC%3A%20Minimal%20Intervention%20Shared%20Control%20with%20Guaranteed%20Safety%20under%0A%20%20Non-Convex%20Constraints%0AAuthor%3A%20Shivam%20Chaubey%20and%20Francesco%20Verdoja%20and%20Shankar%20Deka%20and%20Ville%20Kyrki%0AAbstract%3A%20%20%20Shared%20control%20combines%20human%20intention%20with%20autonomous%20decision-making%2C%20from%0Alow-level%20safety%20overrides%20to%20high-level%20task%20guidance%2C%20enabling%20systems%20that%0Aadapt%20to%20users%20while%20ensuring%20safety%20and%20performance.%20This%20enhances%20task%0Aeffectiveness%20and%20user%20experience%20across%20domains%20such%20as%20assistive%20robotics%2C%0Ateleoperation%2C%20and%20autonomous%20driving.%20However%2C%20existing%20shared%20control%0Amethods%2C%20based%20on%20e.g.%20Model%20Predictive%20Control%2C%20Control%20Barrier%20Functions%2C%20or%0Alearning-based%20control%2C%20struggle%20with%20feasibility%2C%20scalability%2C%20or%20safety%0Aguarantees%2C%20particularly%20since%20the%20user%20input%20is%20unpredictable.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20an%20assistive%20controller%20framework%0Abased%20on%20Constrained%20Optimal%20Control%20Problem%20that%20incorporates%20an%0Aoffline-computed%20Control%20Invariant%20Set%2C%20enabling%20online%20computation%20of%20control%0Aactions%20that%20ensure%20feasibility%2C%20strict%20constraint%20satisfaction%2C%20and%20minimal%0Aoverride%20of%20user%20intent.%20Moreover%2C%20the%20framework%20can%20accommodate%20structured%0Aclass%20of%20non-convex%20constraints%2C%20which%20are%20common%20in%20real-world%20scenarios.%20We%0Avalidate%20the%20approach%20through%20a%20large-scale%20user%20study%20with%2066%0Aparticipants--one%20of%20the%20most%20extensive%20in%20shared%20control%20research--using%20a%0Acomputer%20game%20environment%20to%20assess%20task%20load%2C%20trust%2C%20and%20perceived%20control%2C%20in%0Aaddition%20to%20performance.%20The%20results%20show%20consistent%20improvements%20across%20all%0Athese%20aspects%20without%20compromising%20safety%20and%20user%20intent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMISC%253A%2520Minimal%2520Intervention%2520Shared%2520Control%2520with%2520Guaranteed%2520Safety%2520under%250A%2520%2520Non-Convex%2520Constraints%26entry.906535625%3DShivam%2520Chaubey%2520and%2520Francesco%2520Verdoja%2520and%2520Shankar%2520Deka%2520and%2520Ville%2520Kyrki%26entry.1292438233%3D%2520%2520Shared%2520control%2520combines%2520human%2520intention%2520with%2520autonomous%2520decision-making%252C%2520from%250Alow-level%2520safety%2520overrides%2520to%2520high-level%2520task%2520guidance%252C%2520enabling%2520systems%2520that%250Aadapt%2520to%2520users%2520while%2520ensuring%2520safety%2520and%2520performance.%2520This%2520enhances%2520task%250Aeffectiveness%2520and%2520user%2520experience%2520across%2520domains%2520such%2520as%2520assistive%2520robotics%252C%250Ateleoperation%252C%2520and%2520autonomous%2520driving.%2520However%252C%2520existing%2520shared%2520control%250Amethods%252C%2520based%2520on%2520e.g.%2520Model%2520Predictive%2520Control%252C%2520Control%2520Barrier%2520Functions%252C%2520or%250Alearning-based%2520control%252C%2520struggle%2520with%2520feasibility%252C%2520scalability%252C%2520or%2520safety%250Aguarantees%252C%2520particularly%2520since%2520the%2520user%2520input%2520is%2520unpredictable.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520assistive%2520controller%2520framework%250Abased%2520on%2520Constrained%2520Optimal%2520Control%2520Problem%2520that%2520incorporates%2520an%250Aoffline-computed%2520Control%2520Invariant%2520Set%252C%2520enabling%2520online%2520computation%2520of%2520control%250Aactions%2520that%2520ensure%2520feasibility%252C%2520strict%2520constraint%2520satisfaction%252C%2520and%2520minimal%250Aoverride%2520of%2520user%2520intent.%2520Moreover%252C%2520the%2520framework%2520can%2520accommodate%2520structured%250Aclass%2520of%2520non-convex%2520constraints%252C%2520which%2520are%2520common%2520in%2520real-world%2520scenarios.%2520We%250Avalidate%2520the%2520approach%2520through%2520a%2520large-scale%2520user%2520study%2520with%252066%250Aparticipants--one%2520of%2520the%2520most%2520extensive%2520in%2520shared%2520control%2520research--using%2520a%250Acomputer%2520game%2520environment%2520to%2520assess%2520task%2520load%252C%2520trust%252C%2520and%2520perceived%2520control%252C%2520in%250Aaddition%2520to%2520performance.%2520The%2520results%2520show%2520consistent%2520improvements%2520across%2520all%250Athese%2520aspects%2520without%2520compromising%2520safety%2520and%2520user%2520intent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MISC%3A%20Minimal%20Intervention%20Shared%20Control%20with%20Guaranteed%20Safety%20under%0A%20%20Non-Convex%20Constraints&entry.906535625=Shivam%20Chaubey%20and%20Francesco%20Verdoja%20and%20Shankar%20Deka%20and%20Ville%20Kyrki&entry.1292438233=%20%20Shared%20control%20combines%20human%20intention%20with%20autonomous%20decision-making%2C%20from%0Alow-level%20safety%20overrides%20to%20high-level%20task%20guidance%2C%20enabling%20systems%20that%0Aadapt%20to%20users%20while%20ensuring%20safety%20and%20performance.%20This%20enhances%20task%0Aeffectiveness%20and%20user%20experience%20across%20domains%20such%20as%20assistive%20robotics%2C%0Ateleoperation%2C%20and%20autonomous%20driving.%20However%2C%20existing%20shared%20control%0Amethods%2C%20based%20on%20e.g.%20Model%20Predictive%20Control%2C%20Control%20Barrier%20Functions%2C%20or%0Alearning-based%20control%2C%20struggle%20with%20feasibility%2C%20scalability%2C%20or%20safety%0Aguarantees%2C%20particularly%20since%20the%20user%20input%20is%20unpredictable.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20an%20assistive%20controller%20framework%0Abased%20on%20Constrained%20Optimal%20Control%20Problem%20that%20incorporates%20an%0Aoffline-computed%20Control%20Invariant%20Set%2C%20enabling%20online%20computation%20of%20control%0Aactions%20that%20ensure%20feasibility%2C%20strict%20constraint%20satisfaction%2C%20and%20minimal%0Aoverride%20of%20user%20intent.%20Moreover%2C%20the%20framework%20can%20accommodate%20structured%0Aclass%20of%20non-convex%20constraints%2C%20which%20are%20common%20in%20real-world%20scenarios.%20We%0Avalidate%20the%20approach%20through%20a%20large-scale%20user%20study%20with%2066%0Aparticipants--one%20of%20the%20most%20extensive%20in%20shared%20control%20research--using%20a%0Acomputer%20game%20environment%20to%20assess%20task%20load%2C%20trust%2C%20and%20perceived%20control%2C%20in%0Aaddition%20to%20performance.%20The%20results%20show%20consistent%20improvements%20across%20all%0Athese%20aspects%20without%20compromising%20safety%20and%20user%20intent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02438v1&entry.124074799=Read"},
{"title": "Partial Weakly-Supervised Oriented Object Detection", "author": "Mingxin Liu and Peiyuan Zhang and Yuan Liu and Wei Zhang and Yue Zhou and Ning Liao and Ziyang Gong and Junwei Luo and Zhirui Wang and Yi Yu and Xue Yang", "abstract": "  The growing demand for oriented object detection (OOD) across various domains\nhas driven significant research in this area. However, the high cost of dataset\nannotation remains a major concern. Current mainstream OOD algorithms can be\nmainly categorized into three types: (1) fully supervised methods using\ncomplete oriented bounding box (OBB) annotations, (2) semi-supervised methods\nusing partial OBB annotations, and (3) weakly supervised methods using weak\nannotations such as horizontal boxes or points. However, these algorithms\ninevitably increase the cost of models in terms of annotation speed or\nannotation cost. To address this issue, we propose:(1) the first Partial\nWeakly-Supervised Oriented Object Detection (PWOOD) framework based on\npartially weak annotations (horizontal boxes or single points), which can\nefficiently leverage large amounts of unlabeled data, significantly\noutperforming weakly supervised algorithms trained with partially weak\nannotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware\nStudent (OS-Student) model capable of learning orientation and scale\ninformation with only a small amount of orientation-agnostic or scale-agnostic\nweak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)\nto reduce the model's sensitivity to static filtering thresholds. Comprehensive\nexperiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD\nframework performs comparably to, or even surpasses, traditional\nsemi-supervised algorithms.\n", "link": "http://arxiv.org/abs/2507.02751v1", "date": "2025-07-03", "relevancy": 2.2348, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5688}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5569}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partial%20Weakly-Supervised%20Oriented%20Object%20Detection&body=Title%3A%20Partial%20Weakly-Supervised%20Oriented%20Object%20Detection%0AAuthor%3A%20Mingxin%20Liu%20and%20Peiyuan%20Zhang%20and%20Yuan%20Liu%20and%20Wei%20Zhang%20and%20Yue%20Zhou%20and%20Ning%20Liao%20and%20Ziyang%20Gong%20and%20Junwei%20Luo%20and%20Zhirui%20Wang%20and%20Yi%20Yu%20and%20Xue%20Yang%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20oriented%20object%20detection%20%28OOD%29%20across%20various%20domains%0Ahas%20driven%20significant%20research%20in%20this%20area.%20However%2C%20the%20high%20cost%20of%20dataset%0Aannotation%20remains%20a%20major%20concern.%20Current%20mainstream%20OOD%20algorithms%20can%20be%0Amainly%20categorized%20into%20three%20types%3A%20%281%29%20fully%20supervised%20methods%20using%0Acomplete%20oriented%20bounding%20box%20%28OBB%29%20annotations%2C%20%282%29%20semi-supervised%20methods%0Ausing%20partial%20OBB%20annotations%2C%20and%20%283%29%20weakly%20supervised%20methods%20using%20weak%0Aannotations%20such%20as%20horizontal%20boxes%20or%20points.%20However%2C%20these%20algorithms%0Ainevitably%20increase%20the%20cost%20of%20models%20in%20terms%20of%20annotation%20speed%20or%0Aannotation%20cost.%20To%20address%20this%20issue%2C%20we%20propose%3A%281%29%20the%20first%20Partial%0AWeakly-Supervised%20Oriented%20Object%20Detection%20%28PWOOD%29%20framework%20based%20on%0Apartially%20weak%20annotations%20%28horizontal%20boxes%20or%20single%20points%29%2C%20which%20can%0Aefficiently%20leverage%20large%20amounts%20of%20unlabeled%20data%2C%20significantly%0Aoutperforming%20weakly%20supervised%20algorithms%20trained%20with%20partially%20weak%0Aannotations%2C%20also%20offers%20a%20lower%20cost%20solution%3B%20%282%29%20Orientation-and-Scale-aware%0AStudent%20%28OS-Student%29%20model%20capable%20of%20learning%20orientation%20and%20scale%0Ainformation%20with%20only%20a%20small%20amount%20of%20orientation-agnostic%20or%20scale-agnostic%0Aweak%20annotations%3B%20and%20%283%29%20Class-Agnostic%20Pseudo-Label%20Filtering%20strategy%20%28CPF%29%0Ato%20reduce%20the%20model%27s%20sensitivity%20to%20static%20filtering%20thresholds.%20Comprehensive%0Aexperiments%20on%20DOTA-v1.0/v1.5/v2.0%20and%20DIOR%20datasets%20demonstrate%20that%20our%20PWOOD%0Aframework%20performs%20comparably%20to%2C%20or%20even%20surpasses%2C%20traditional%0Asemi-supervised%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartial%2520Weakly-Supervised%2520Oriented%2520Object%2520Detection%26entry.906535625%3DMingxin%2520Liu%2520and%2520Peiyuan%2520Zhang%2520and%2520Yuan%2520Liu%2520and%2520Wei%2520Zhang%2520and%2520Yue%2520Zhou%2520and%2520Ning%2520Liao%2520and%2520Ziyang%2520Gong%2520and%2520Junwei%2520Luo%2520and%2520Zhirui%2520Wang%2520and%2520Yi%2520Yu%2520and%2520Xue%2520Yang%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520oriented%2520object%2520detection%2520%2528OOD%2529%2520across%2520various%2520domains%250Ahas%2520driven%2520significant%2520research%2520in%2520this%2520area.%2520However%252C%2520the%2520high%2520cost%2520of%2520dataset%250Aannotation%2520remains%2520a%2520major%2520concern.%2520Current%2520mainstream%2520OOD%2520algorithms%2520can%2520be%250Amainly%2520categorized%2520into%2520three%2520types%253A%2520%25281%2529%2520fully%2520supervised%2520methods%2520using%250Acomplete%2520oriented%2520bounding%2520box%2520%2528OBB%2529%2520annotations%252C%2520%25282%2529%2520semi-supervised%2520methods%250Ausing%2520partial%2520OBB%2520annotations%252C%2520and%2520%25283%2529%2520weakly%2520supervised%2520methods%2520using%2520weak%250Aannotations%2520such%2520as%2520horizontal%2520boxes%2520or%2520points.%2520However%252C%2520these%2520algorithms%250Ainevitably%2520increase%2520the%2520cost%2520of%2520models%2520in%2520terms%2520of%2520annotation%2520speed%2520or%250Aannotation%2520cost.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%253A%25281%2529%2520the%2520first%2520Partial%250AWeakly-Supervised%2520Oriented%2520Object%2520Detection%2520%2528PWOOD%2529%2520framework%2520based%2520on%250Apartially%2520weak%2520annotations%2520%2528horizontal%2520boxes%2520or%2520single%2520points%2529%252C%2520which%2520can%250Aefficiently%2520leverage%2520large%2520amounts%2520of%2520unlabeled%2520data%252C%2520significantly%250Aoutperforming%2520weakly%2520supervised%2520algorithms%2520trained%2520with%2520partially%2520weak%250Aannotations%252C%2520also%2520offers%2520a%2520lower%2520cost%2520solution%253B%2520%25282%2529%2520Orientation-and-Scale-aware%250AStudent%2520%2528OS-Student%2529%2520model%2520capable%2520of%2520learning%2520orientation%2520and%2520scale%250Ainformation%2520with%2520only%2520a%2520small%2520amount%2520of%2520orientation-agnostic%2520or%2520scale-agnostic%250Aweak%2520annotations%253B%2520and%2520%25283%2529%2520Class-Agnostic%2520Pseudo-Label%2520Filtering%2520strategy%2520%2528CPF%2529%250Ato%2520reduce%2520the%2520model%2527s%2520sensitivity%2520to%2520static%2520filtering%2520thresholds.%2520Comprehensive%250Aexperiments%2520on%2520DOTA-v1.0/v1.5/v2.0%2520and%2520DIOR%2520datasets%2520demonstrate%2520that%2520our%2520PWOOD%250Aframework%2520performs%2520comparably%2520to%252C%2520or%2520even%2520surpasses%252C%2520traditional%250Asemi-supervised%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial%20Weakly-Supervised%20Oriented%20Object%20Detection&entry.906535625=Mingxin%20Liu%20and%20Peiyuan%20Zhang%20and%20Yuan%20Liu%20and%20Wei%20Zhang%20and%20Yue%20Zhou%20and%20Ning%20Liao%20and%20Ziyang%20Gong%20and%20Junwei%20Luo%20and%20Zhirui%20Wang%20and%20Yi%20Yu%20and%20Xue%20Yang&entry.1292438233=%20%20The%20growing%20demand%20for%20oriented%20object%20detection%20%28OOD%29%20across%20various%20domains%0Ahas%20driven%20significant%20research%20in%20this%20area.%20However%2C%20the%20high%20cost%20of%20dataset%0Aannotation%20remains%20a%20major%20concern.%20Current%20mainstream%20OOD%20algorithms%20can%20be%0Amainly%20categorized%20into%20three%20types%3A%20%281%29%20fully%20supervised%20methods%20using%0Acomplete%20oriented%20bounding%20box%20%28OBB%29%20annotations%2C%20%282%29%20semi-supervised%20methods%0Ausing%20partial%20OBB%20annotations%2C%20and%20%283%29%20weakly%20supervised%20methods%20using%20weak%0Aannotations%20such%20as%20horizontal%20boxes%20or%20points.%20However%2C%20these%20algorithms%0Ainevitably%20increase%20the%20cost%20of%20models%20in%20terms%20of%20annotation%20speed%20or%0Aannotation%20cost.%20To%20address%20this%20issue%2C%20we%20propose%3A%281%29%20the%20first%20Partial%0AWeakly-Supervised%20Oriented%20Object%20Detection%20%28PWOOD%29%20framework%20based%20on%0Apartially%20weak%20annotations%20%28horizontal%20boxes%20or%20single%20points%29%2C%20which%20can%0Aefficiently%20leverage%20large%20amounts%20of%20unlabeled%20data%2C%20significantly%0Aoutperforming%20weakly%20supervised%20algorithms%20trained%20with%20partially%20weak%0Aannotations%2C%20also%20offers%20a%20lower%20cost%20solution%3B%20%282%29%20Orientation-and-Scale-aware%0AStudent%20%28OS-Student%29%20model%20capable%20of%20learning%20orientation%20and%20scale%0Ainformation%20with%20only%20a%20small%20amount%20of%20orientation-agnostic%20or%20scale-agnostic%0Aweak%20annotations%3B%20and%20%283%29%20Class-Agnostic%20Pseudo-Label%20Filtering%20strategy%20%28CPF%29%0Ato%20reduce%20the%20model%27s%20sensitivity%20to%20static%20filtering%20thresholds.%20Comprehensive%0Aexperiments%20on%20DOTA-v1.0/v1.5/v2.0%20and%20DIOR%20datasets%20demonstrate%20that%20our%20PWOOD%0Aframework%20performs%20comparably%20to%2C%20or%20even%20surpasses%2C%20traditional%0Asemi-supervised%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02751v1&entry.124074799=Read"},
{"title": "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science", "author": "Erle Zhu and Yadi Liu and Zhe Zhang and Xujun Li and Jin Zhou and Xinjie Yu and Minlie Huang and Hongning Wang", "abstract": "  Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.\n", "link": "http://arxiv.org/abs/2501.10768v2", "date": "2025-07-03", "relevancy": 2.2328, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPS%3A%20Advancing%20Multi-Modal%20Reasoning%20in%20Expert-Level%20Physical%20Science&body=Title%3A%20MAPS%3A%20Advancing%20Multi-Modal%20Reasoning%20in%20Expert-Level%20Physical%20Science%0AAuthor%3A%20Erle%20Zhu%20and%20Yadi%20Liu%20and%20Zhe%20Zhang%20and%20Xujun%20Li%20and%20Jin%20Zhou%20and%20Xinjie%20Yu%20and%20Minlie%20Huang%20and%20Hongning%20Wang%0AAbstract%3A%20%20%20Pre-trained%20on%20extensive%20text%20and%20image%20corpora%2C%20current%20Multi-Modal%20Large%0ALanguage%20Models%20%28MLLM%29%20have%20shown%20strong%20capabilities%20in%20general%20visual%0Areasoning%20tasks.%20However%2C%20their%20performance%20is%20still%20lacking%20in%20physical%0Adomains%20that%20require%20understanding%20diagrams%20with%20complex%20physical%20structures%0Aand%20quantitative%20analysis%20based%20on%20multi-modal%20information.%20To%20address%20this%2C%20we%0Adevelop%20a%20new%20framework%2C%20named%20Multi-Modal%20Scientific%20Reasoning%20with%20Physics%0APerception%20and%20Simulation%20%28MAPS%29%20based%20on%20an%20MLLM.%20MAPS%20decomposes%20expert-level%0Amulti-modal%20reasoning%20task%20into%20physical%20diagram%20understanding%20via%20a%20Physical%0APerception%20Model%20%28PPM%29%20and%20reasoning%20with%20physical%20knowledge%20via%20a%20simulator.%0AThe%20PPM%20module%20is%20obtained%20by%20fine-tuning%20a%20visual%20language%20model%20using%0Acarefully%20designed%20synthetic%20data%20with%20paired%20physical%20diagrams%20and%0Acorresponding%20simulation%20language%20descriptions.%20At%20the%20inference%20stage%2C%20MAPS%0Aintegrates%20the%20simulation%20language%20description%20of%20the%20input%20diagram%20provided%20by%0APPM%20and%20results%20obtained%20through%20a%20Chain-of-Simulation%20process%20with%20MLLM%20to%0Aderive%20the%20underlying%20rationale%20and%20the%20final%20answer.%20Validated%20using%20our%0Acollected%20college-level%20circuit%20analysis%20problems%2C%20MAPS%20significantly%20improves%0Areasoning%20accuracy%20of%20MLLM%20and%20outperforms%20all%20existing%20models.%20The%20results%0Aconfirm%20MAPS%20offers%20a%20promising%20direction%20for%20enhancing%20multi-modal%20scientific%0Areasoning%20ability%20of%20MLLMs.%20We%20will%20release%20our%20code%2C%20model%20and%20dataset%20used%0Afor%20our%20experiments%20upon%20publishing%20of%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPS%253A%2520Advancing%2520Multi-Modal%2520Reasoning%2520in%2520Expert-Level%2520Physical%2520Science%26entry.906535625%3DErle%2520Zhu%2520and%2520Yadi%2520Liu%2520and%2520Zhe%2520Zhang%2520and%2520Xujun%2520Li%2520and%2520Jin%2520Zhou%2520and%2520Xinjie%2520Yu%2520and%2520Minlie%2520Huang%2520and%2520Hongning%2520Wang%26entry.1292438233%3D%2520%2520Pre-trained%2520on%2520extensive%2520text%2520and%2520image%2520corpora%252C%2520current%2520Multi-Modal%2520Large%250ALanguage%2520Models%2520%2528MLLM%2529%2520have%2520shown%2520strong%2520capabilities%2520in%2520general%2520visual%250Areasoning%2520tasks.%2520However%252C%2520their%2520performance%2520is%2520still%2520lacking%2520in%2520physical%250Adomains%2520that%2520require%2520understanding%2520diagrams%2520with%2520complex%2520physical%2520structures%250Aand%2520quantitative%2520analysis%2520based%2520on%2520multi-modal%2520information.%2520To%2520address%2520this%252C%2520we%250Adevelop%2520a%2520new%2520framework%252C%2520named%2520Multi-Modal%2520Scientific%2520Reasoning%2520with%2520Physics%250APerception%2520and%2520Simulation%2520%2528MAPS%2529%2520based%2520on%2520an%2520MLLM.%2520MAPS%2520decomposes%2520expert-level%250Amulti-modal%2520reasoning%2520task%2520into%2520physical%2520diagram%2520understanding%2520via%2520a%2520Physical%250APerception%2520Model%2520%2528PPM%2529%2520and%2520reasoning%2520with%2520physical%2520knowledge%2520via%2520a%2520simulator.%250AThe%2520PPM%2520module%2520is%2520obtained%2520by%2520fine-tuning%2520a%2520visual%2520language%2520model%2520using%250Acarefully%2520designed%2520synthetic%2520data%2520with%2520paired%2520physical%2520diagrams%2520and%250Acorresponding%2520simulation%2520language%2520descriptions.%2520At%2520the%2520inference%2520stage%252C%2520MAPS%250Aintegrates%2520the%2520simulation%2520language%2520description%2520of%2520the%2520input%2520diagram%2520provided%2520by%250APPM%2520and%2520results%2520obtained%2520through%2520a%2520Chain-of-Simulation%2520process%2520with%2520MLLM%2520to%250Aderive%2520the%2520underlying%2520rationale%2520and%2520the%2520final%2520answer.%2520Validated%2520using%2520our%250Acollected%2520college-level%2520circuit%2520analysis%2520problems%252C%2520MAPS%2520significantly%2520improves%250Areasoning%2520accuracy%2520of%2520MLLM%2520and%2520outperforms%2520all%2520existing%2520models.%2520The%2520results%250Aconfirm%2520MAPS%2520offers%2520a%2520promising%2520direction%2520for%2520enhancing%2520multi-modal%2520scientific%250Areasoning%2520ability%2520of%2520MLLMs.%2520We%2520will%2520release%2520our%2520code%252C%2520model%2520and%2520dataset%2520used%250Afor%2520our%2520experiments%2520upon%2520publishing%2520of%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPS%3A%20Advancing%20Multi-Modal%20Reasoning%20in%20Expert-Level%20Physical%20Science&entry.906535625=Erle%20Zhu%20and%20Yadi%20Liu%20and%20Zhe%20Zhang%20and%20Xujun%20Li%20and%20Jin%20Zhou%20and%20Xinjie%20Yu%20and%20Minlie%20Huang%20and%20Hongning%20Wang&entry.1292438233=%20%20Pre-trained%20on%20extensive%20text%20and%20image%20corpora%2C%20current%20Multi-Modal%20Large%0ALanguage%20Models%20%28MLLM%29%20have%20shown%20strong%20capabilities%20in%20general%20visual%0Areasoning%20tasks.%20However%2C%20their%20performance%20is%20still%20lacking%20in%20physical%0Adomains%20that%20require%20understanding%20diagrams%20with%20complex%20physical%20structures%0Aand%20quantitative%20analysis%20based%20on%20multi-modal%20information.%20To%20address%20this%2C%20we%0Adevelop%20a%20new%20framework%2C%20named%20Multi-Modal%20Scientific%20Reasoning%20with%20Physics%0APerception%20and%20Simulation%20%28MAPS%29%20based%20on%20an%20MLLM.%20MAPS%20decomposes%20expert-level%0Amulti-modal%20reasoning%20task%20into%20physical%20diagram%20understanding%20via%20a%20Physical%0APerception%20Model%20%28PPM%29%20and%20reasoning%20with%20physical%20knowledge%20via%20a%20simulator.%0AThe%20PPM%20module%20is%20obtained%20by%20fine-tuning%20a%20visual%20language%20model%20using%0Acarefully%20designed%20synthetic%20data%20with%20paired%20physical%20diagrams%20and%0Acorresponding%20simulation%20language%20descriptions.%20At%20the%20inference%20stage%2C%20MAPS%0Aintegrates%20the%20simulation%20language%20description%20of%20the%20input%20diagram%20provided%20by%0APPM%20and%20results%20obtained%20through%20a%20Chain-of-Simulation%20process%20with%20MLLM%20to%0Aderive%20the%20underlying%20rationale%20and%20the%20final%20answer.%20Validated%20using%20our%0Acollected%20college-level%20circuit%20analysis%20problems%2C%20MAPS%20significantly%20improves%0Areasoning%20accuracy%20of%20MLLM%20and%20outperforms%20all%20existing%20models.%20The%20results%0Aconfirm%20MAPS%20offers%20a%20promising%20direction%20for%20enhancing%20multi-modal%20scientific%0Areasoning%20ability%20of%20MLLMs.%20We%20will%20release%20our%20code%2C%20model%20and%20dataset%20used%0Afor%20our%20experiments%20upon%20publishing%20of%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10768v2&entry.124074799=Read"},
{"title": "MedFormer: Hierarchical Medical Vision Transformer with Content-Aware\n  Dual Sparse Selection Attention", "author": "Zunhui Xia and Hongxing Li and Libin Lan", "abstract": "  Medical image recognition serves as a key way to aid in clinical diagnosis,\nenabling more accurate and timely identification of diseases and abnormalities.\nVision transformer-based approaches have proven effective in handling various\nmedical recognition tasks. However, these methods encounter two primary\nchallenges. First, they are often task-specific and architecture-tailored,\nlimiting their general applicability. Second, they usually either adopt full\nattention to model long-range dependencies, resulting in high computational\ncosts, or rely on handcrafted sparse attention, potentially leading to\nsuboptimal performance. To tackle these issues, we present MedFormer, an\nefficient medical vision transformer with two key ideas. First, it employs a\npyramid scaling structure as a versatile backbone for various medical image\nrecognition tasks, including image classification and dense prediction tasks\nsuch as semantic segmentation and lesion detection. This structure facilitates\nhierarchical feature representation while reducing the computation load of\nfeature maps, highly beneficial for boosting performance. Second, it introduces\na novel Dual Sparse Selection Attention (DSSA) with content awareness to\nimprove computational efficiency and robustness against noise while maintaining\nhigh performance. As the core building technique of MedFormer, DSSA is\nexplicitly designed to attend to the most relevant content. In addition, a\ndetailed theoretical analysis has been conducted, demonstrating that MedFormer\nhas superior generality and efficiency in comparison to existing medical vision\ntransformers. Extensive experiments on a variety of imaging modality datasets\nconsistently show that MedFormer is highly effective in enhancing performance\nacross all three above-mentioned medical image recognition tasks. The code is\navailable at https://github.com/XiaZunhui/MedFormer.\n", "link": "http://arxiv.org/abs/2507.02488v1", "date": "2025-07-03", "relevancy": 2.2325, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5554}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedFormer%3A%20Hierarchical%20Medical%20Vision%20Transformer%20with%20Content-Aware%0A%20%20Dual%20Sparse%20Selection%20Attention&body=Title%3A%20MedFormer%3A%20Hierarchical%20Medical%20Vision%20Transformer%20with%20Content-Aware%0A%20%20Dual%20Sparse%20Selection%20Attention%0AAuthor%3A%20Zunhui%20Xia%20and%20Hongxing%20Li%20and%20Libin%20Lan%0AAbstract%3A%20%20%20Medical%20image%20recognition%20serves%20as%20a%20key%20way%20to%20aid%20in%20clinical%20diagnosis%2C%0Aenabling%20more%20accurate%20and%20timely%20identification%20of%20diseases%20and%20abnormalities.%0AVision%20transformer-based%20approaches%20have%20proven%20effective%20in%20handling%20various%0Amedical%20recognition%20tasks.%20However%2C%20these%20methods%20encounter%20two%20primary%0Achallenges.%20First%2C%20they%20are%20often%20task-specific%20and%20architecture-tailored%2C%0Alimiting%20their%20general%20applicability.%20Second%2C%20they%20usually%20either%20adopt%20full%0Aattention%20to%20model%20long-range%20dependencies%2C%20resulting%20in%20high%20computational%0Acosts%2C%20or%20rely%20on%20handcrafted%20sparse%20attention%2C%20potentially%20leading%20to%0Asuboptimal%20performance.%20To%20tackle%20these%20issues%2C%20we%20present%20MedFormer%2C%20an%0Aefficient%20medical%20vision%20transformer%20with%20two%20key%20ideas.%20First%2C%20it%20employs%20a%0Apyramid%20scaling%20structure%20as%20a%20versatile%20backbone%20for%20various%20medical%20image%0Arecognition%20tasks%2C%20including%20image%20classification%20and%20dense%20prediction%20tasks%0Asuch%20as%20semantic%20segmentation%20and%20lesion%20detection.%20This%20structure%20facilitates%0Ahierarchical%20feature%20representation%20while%20reducing%20the%20computation%20load%20of%0Afeature%20maps%2C%20highly%20beneficial%20for%20boosting%20performance.%20Second%2C%20it%20introduces%0Aa%20novel%20Dual%20Sparse%20Selection%20Attention%20%28DSSA%29%20with%20content%20awareness%20to%0Aimprove%20computational%20efficiency%20and%20robustness%20against%20noise%20while%20maintaining%0Ahigh%20performance.%20As%20the%20core%20building%20technique%20of%20MedFormer%2C%20DSSA%20is%0Aexplicitly%20designed%20to%20attend%20to%20the%20most%20relevant%20content.%20In%20addition%2C%20a%0Adetailed%20theoretical%20analysis%20has%20been%20conducted%2C%20demonstrating%20that%20MedFormer%0Ahas%20superior%20generality%20and%20efficiency%20in%20comparison%20to%20existing%20medical%20vision%0Atransformers.%20Extensive%20experiments%20on%20a%20variety%20of%20imaging%20modality%20datasets%0Aconsistently%20show%20that%20MedFormer%20is%20highly%20effective%20in%20enhancing%20performance%0Aacross%20all%20three%20above-mentioned%20medical%20image%20recognition%20tasks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/XiaZunhui/MedFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedFormer%253A%2520Hierarchical%2520Medical%2520Vision%2520Transformer%2520with%2520Content-Aware%250A%2520%2520Dual%2520Sparse%2520Selection%2520Attention%26entry.906535625%3DZunhui%2520Xia%2520and%2520Hongxing%2520Li%2520and%2520Libin%2520Lan%26entry.1292438233%3D%2520%2520Medical%2520image%2520recognition%2520serves%2520as%2520a%2520key%2520way%2520to%2520aid%2520in%2520clinical%2520diagnosis%252C%250Aenabling%2520more%2520accurate%2520and%2520timely%2520identification%2520of%2520diseases%2520and%2520abnormalities.%250AVision%2520transformer-based%2520approaches%2520have%2520proven%2520effective%2520in%2520handling%2520various%250Amedical%2520recognition%2520tasks.%2520However%252C%2520these%2520methods%2520encounter%2520two%2520primary%250Achallenges.%2520First%252C%2520they%2520are%2520often%2520task-specific%2520and%2520architecture-tailored%252C%250Alimiting%2520their%2520general%2520applicability.%2520Second%252C%2520they%2520usually%2520either%2520adopt%2520full%250Aattention%2520to%2520model%2520long-range%2520dependencies%252C%2520resulting%2520in%2520high%2520computational%250Acosts%252C%2520or%2520rely%2520on%2520handcrafted%2520sparse%2520attention%252C%2520potentially%2520leading%2520to%250Asuboptimal%2520performance.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520present%2520MedFormer%252C%2520an%250Aefficient%2520medical%2520vision%2520transformer%2520with%2520two%2520key%2520ideas.%2520First%252C%2520it%2520employs%2520a%250Apyramid%2520scaling%2520structure%2520as%2520a%2520versatile%2520backbone%2520for%2520various%2520medical%2520image%250Arecognition%2520tasks%252C%2520including%2520image%2520classification%2520and%2520dense%2520prediction%2520tasks%250Asuch%2520as%2520semantic%2520segmentation%2520and%2520lesion%2520detection.%2520This%2520structure%2520facilitates%250Ahierarchical%2520feature%2520representation%2520while%2520reducing%2520the%2520computation%2520load%2520of%250Afeature%2520maps%252C%2520highly%2520beneficial%2520for%2520boosting%2520performance.%2520Second%252C%2520it%2520introduces%250Aa%2520novel%2520Dual%2520Sparse%2520Selection%2520Attention%2520%2528DSSA%2529%2520with%2520content%2520awareness%2520to%250Aimprove%2520computational%2520efficiency%2520and%2520robustness%2520against%2520noise%2520while%2520maintaining%250Ahigh%2520performance.%2520As%2520the%2520core%2520building%2520technique%2520of%2520MedFormer%252C%2520DSSA%2520is%250Aexplicitly%2520designed%2520to%2520attend%2520to%2520the%2520most%2520relevant%2520content.%2520In%2520addition%252C%2520a%250Adetailed%2520theoretical%2520analysis%2520has%2520been%2520conducted%252C%2520demonstrating%2520that%2520MedFormer%250Ahas%2520superior%2520generality%2520and%2520efficiency%2520in%2520comparison%2520to%2520existing%2520medical%2520vision%250Atransformers.%2520Extensive%2520experiments%2520on%2520a%2520variety%2520of%2520imaging%2520modality%2520datasets%250Aconsistently%2520show%2520that%2520MedFormer%2520is%2520highly%2520effective%2520in%2520enhancing%2520performance%250Aacross%2520all%2520three%2520above-mentioned%2520medical%2520image%2520recognition%2520tasks.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/XiaZunhui/MedFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedFormer%3A%20Hierarchical%20Medical%20Vision%20Transformer%20with%20Content-Aware%0A%20%20Dual%20Sparse%20Selection%20Attention&entry.906535625=Zunhui%20Xia%20and%20Hongxing%20Li%20and%20Libin%20Lan&entry.1292438233=%20%20Medical%20image%20recognition%20serves%20as%20a%20key%20way%20to%20aid%20in%20clinical%20diagnosis%2C%0Aenabling%20more%20accurate%20and%20timely%20identification%20of%20diseases%20and%20abnormalities.%0AVision%20transformer-based%20approaches%20have%20proven%20effective%20in%20handling%20various%0Amedical%20recognition%20tasks.%20However%2C%20these%20methods%20encounter%20two%20primary%0Achallenges.%20First%2C%20they%20are%20often%20task-specific%20and%20architecture-tailored%2C%0Alimiting%20their%20general%20applicability.%20Second%2C%20they%20usually%20either%20adopt%20full%0Aattention%20to%20model%20long-range%20dependencies%2C%20resulting%20in%20high%20computational%0Acosts%2C%20or%20rely%20on%20handcrafted%20sparse%20attention%2C%20potentially%20leading%20to%0Asuboptimal%20performance.%20To%20tackle%20these%20issues%2C%20we%20present%20MedFormer%2C%20an%0Aefficient%20medical%20vision%20transformer%20with%20two%20key%20ideas.%20First%2C%20it%20employs%20a%0Apyramid%20scaling%20structure%20as%20a%20versatile%20backbone%20for%20various%20medical%20image%0Arecognition%20tasks%2C%20including%20image%20classification%20and%20dense%20prediction%20tasks%0Asuch%20as%20semantic%20segmentation%20and%20lesion%20detection.%20This%20structure%20facilitates%0Ahierarchical%20feature%20representation%20while%20reducing%20the%20computation%20load%20of%0Afeature%20maps%2C%20highly%20beneficial%20for%20boosting%20performance.%20Second%2C%20it%20introduces%0Aa%20novel%20Dual%20Sparse%20Selection%20Attention%20%28DSSA%29%20with%20content%20awareness%20to%0Aimprove%20computational%20efficiency%20and%20robustness%20against%20noise%20while%20maintaining%0Ahigh%20performance.%20As%20the%20core%20building%20technique%20of%20MedFormer%2C%20DSSA%20is%0Aexplicitly%20designed%20to%20attend%20to%20the%20most%20relevant%20content.%20In%20addition%2C%20a%0Adetailed%20theoretical%20analysis%20has%20been%20conducted%2C%20demonstrating%20that%20MedFormer%0Ahas%20superior%20generality%20and%20efficiency%20in%20comparison%20to%20existing%20medical%20vision%0Atransformers.%20Extensive%20experiments%20on%20a%20variety%20of%20imaging%20modality%20datasets%0Aconsistently%20show%20that%20MedFormer%20is%20highly%20effective%20in%20enhancing%20performance%0Aacross%20all%20three%20above-mentioned%20medical%20image%20recognition%20tasks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/XiaZunhui/MedFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02488v1&entry.124074799=Read"},
{"title": "MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve\n  Segmentationin 4D Ultrasound", "author": "Rusi Chen and Yuanting Yang and Jiezhi Yao and Hongning Song and Ji Zhang and Yongsong Zhou and Yuhao Huang and Ronghao Yang and Dan Jia and Yuhan Zhang and Xing Tao and Haoran Dou and Qing Zhou and Xin Yang and Dong Ni", "abstract": "  Mitral regurgitation is one of the most prevalent cardiac disorders.\nFour-dimensional (4D) ultrasound has emerged as the primary imaging modality\nfor assessing dynamic valvular morphology. However, 4D mitral valve (MV)\nanalysis remains challenging due to limited phase annotations, severe motion\nartifacts, and poor imaging quality. Yet, the absence of inter-phase dependency\nin existing methods hinders 4D MV analysis. To bridge this gap, we propose a\nMotion-Topology guided consistency network (MTCNet) for accurate 4D MV\nultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only\nsparse end-diastolic and end-systolic annotations. First, we design a\ncross-phase motion-guided consistency learning strategy, utilizing a\nbi-directional attention memory bank to propagate spatio-temporal features.\nThis enables MTCNet to achieve excellent performance both per- and inter-phase.\nSecond, we devise a novel topology-guided correlation regularization that\nexplores physical prior knowledge to maintain anatomically plausible.\nTherefore, MTCNet can effectively leverage structural correspondence between\nlabeled and unlabeled phases. Extensive evaluations on the first largest 4D MV\ndataset, with 1408 phases from 160 patients, show that MTCNet performs superior\ncross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:\n1.75mm). Both the code and the dataset are available at\nhttps://github.com/crs524/MTCNet.\n", "link": "http://arxiv.org/abs/2507.00660v2", "date": "2025-07-03", "relevancy": 2.2292, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTCNet%3A%20Motion%20and%20Topology%20Consistency%20Guided%20Learning%20for%20Mitral%20Valve%0A%20%20Segmentationin%204D%20Ultrasound&body=Title%3A%20MTCNet%3A%20Motion%20and%20Topology%20Consistency%20Guided%20Learning%20for%20Mitral%20Valve%0A%20%20Segmentationin%204D%20Ultrasound%0AAuthor%3A%20Rusi%20Chen%20and%20Yuanting%20Yang%20and%20Jiezhi%20Yao%20and%20Hongning%20Song%20and%20Ji%20Zhang%20and%20Yongsong%20Zhou%20and%20Yuhao%20Huang%20and%20Ronghao%20Yang%20and%20Dan%20Jia%20and%20Yuhan%20Zhang%20and%20Xing%20Tao%20and%20Haoran%20Dou%20and%20Qing%20Zhou%20and%20Xin%20Yang%20and%20Dong%20Ni%0AAbstract%3A%20%20%20Mitral%20regurgitation%20is%20one%20of%20the%20most%20prevalent%20cardiac%20disorders.%0AFour-dimensional%20%284D%29%20ultrasound%20has%20emerged%20as%20the%20primary%20imaging%20modality%0Afor%20assessing%20dynamic%20valvular%20morphology.%20However%2C%204D%20mitral%20valve%20%28MV%29%0Aanalysis%20remains%20challenging%20due%20to%20limited%20phase%20annotations%2C%20severe%20motion%0Aartifacts%2C%20and%20poor%20imaging%20quality.%20Yet%2C%20the%20absence%20of%20inter-phase%20dependency%0Ain%20existing%20methods%20hinders%204D%20MV%20analysis.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%0AMotion-Topology%20guided%20consistency%20network%20%28MTCNet%29%20for%20accurate%204D%20MV%0Aultrasound%20segmentation%20in%20semi-supervised%20learning%20%28SSL%29.%20MTCNet%20requires%20only%0Asparse%20end-diastolic%20and%20end-systolic%20annotations.%20First%2C%20we%20design%20a%0Across-phase%20motion-guided%20consistency%20learning%20strategy%2C%20utilizing%20a%0Abi-directional%20attention%20memory%20bank%20to%20propagate%20spatio-temporal%20features.%0AThis%20enables%20MTCNet%20to%20achieve%20excellent%20performance%20both%20per-%20and%20inter-phase.%0ASecond%2C%20we%20devise%20a%20novel%20topology-guided%20correlation%20regularization%20that%0Aexplores%20physical%20prior%20knowledge%20to%20maintain%20anatomically%20plausible.%0ATherefore%2C%20MTCNet%20can%20effectively%20leverage%20structural%20correspondence%20between%0Alabeled%20and%20unlabeled%20phases.%20Extensive%20evaluations%20on%20the%20first%20largest%204D%20MV%0Adataset%2C%20with%201408%20phases%20from%20160%20patients%2C%20show%20that%20MTCNet%20performs%20superior%0Across-phase%20consistency%20compared%20to%20other%20advanced%20methods%20%28Dice%3A%2087.30%25%2C%20HD%3A%0A1.75mm%29.%20Both%20the%20code%20and%20the%20dataset%20are%20available%20at%0Ahttps%3A//github.com/crs524/MTCNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTCNet%253A%2520Motion%2520and%2520Topology%2520Consistency%2520Guided%2520Learning%2520for%2520Mitral%2520Valve%250A%2520%2520Segmentationin%25204D%2520Ultrasound%26entry.906535625%3DRusi%2520Chen%2520and%2520Yuanting%2520Yang%2520and%2520Jiezhi%2520Yao%2520and%2520Hongning%2520Song%2520and%2520Ji%2520Zhang%2520and%2520Yongsong%2520Zhou%2520and%2520Yuhao%2520Huang%2520and%2520Ronghao%2520Yang%2520and%2520Dan%2520Jia%2520and%2520Yuhan%2520Zhang%2520and%2520Xing%2520Tao%2520and%2520Haoran%2520Dou%2520and%2520Qing%2520Zhou%2520and%2520Xin%2520Yang%2520and%2520Dong%2520Ni%26entry.1292438233%3D%2520%2520Mitral%2520regurgitation%2520is%2520one%2520of%2520the%2520most%2520prevalent%2520cardiac%2520disorders.%250AFour-dimensional%2520%25284D%2529%2520ultrasound%2520has%2520emerged%2520as%2520the%2520primary%2520imaging%2520modality%250Afor%2520assessing%2520dynamic%2520valvular%2520morphology.%2520However%252C%25204D%2520mitral%2520valve%2520%2528MV%2529%250Aanalysis%2520remains%2520challenging%2520due%2520to%2520limited%2520phase%2520annotations%252C%2520severe%2520motion%250Aartifacts%252C%2520and%2520poor%2520imaging%2520quality.%2520Yet%252C%2520the%2520absence%2520of%2520inter-phase%2520dependency%250Ain%2520existing%2520methods%2520hinders%25204D%2520MV%2520analysis.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%250AMotion-Topology%2520guided%2520consistency%2520network%2520%2528MTCNet%2529%2520for%2520accurate%25204D%2520MV%250Aultrasound%2520segmentation%2520in%2520semi-supervised%2520learning%2520%2528SSL%2529.%2520MTCNet%2520requires%2520only%250Asparse%2520end-diastolic%2520and%2520end-systolic%2520annotations.%2520First%252C%2520we%2520design%2520a%250Across-phase%2520motion-guided%2520consistency%2520learning%2520strategy%252C%2520utilizing%2520a%250Abi-directional%2520attention%2520memory%2520bank%2520to%2520propagate%2520spatio-temporal%2520features.%250AThis%2520enables%2520MTCNet%2520to%2520achieve%2520excellent%2520performance%2520both%2520per-%2520and%2520inter-phase.%250ASecond%252C%2520we%2520devise%2520a%2520novel%2520topology-guided%2520correlation%2520regularization%2520that%250Aexplores%2520physical%2520prior%2520knowledge%2520to%2520maintain%2520anatomically%2520plausible.%250ATherefore%252C%2520MTCNet%2520can%2520effectively%2520leverage%2520structural%2520correspondence%2520between%250Alabeled%2520and%2520unlabeled%2520phases.%2520Extensive%2520evaluations%2520on%2520the%2520first%2520largest%25204D%2520MV%250Adataset%252C%2520with%25201408%2520phases%2520from%2520160%2520patients%252C%2520show%2520that%2520MTCNet%2520performs%2520superior%250Across-phase%2520consistency%2520compared%2520to%2520other%2520advanced%2520methods%2520%2528Dice%253A%252087.30%2525%252C%2520HD%253A%250A1.75mm%2529.%2520Both%2520the%2520code%2520and%2520the%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/crs524/MTCNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTCNet%3A%20Motion%20and%20Topology%20Consistency%20Guided%20Learning%20for%20Mitral%20Valve%0A%20%20Segmentationin%204D%20Ultrasound&entry.906535625=Rusi%20Chen%20and%20Yuanting%20Yang%20and%20Jiezhi%20Yao%20and%20Hongning%20Song%20and%20Ji%20Zhang%20and%20Yongsong%20Zhou%20and%20Yuhao%20Huang%20and%20Ronghao%20Yang%20and%20Dan%20Jia%20and%20Yuhan%20Zhang%20and%20Xing%20Tao%20and%20Haoran%20Dou%20and%20Qing%20Zhou%20and%20Xin%20Yang%20and%20Dong%20Ni&entry.1292438233=%20%20Mitral%20regurgitation%20is%20one%20of%20the%20most%20prevalent%20cardiac%20disorders.%0AFour-dimensional%20%284D%29%20ultrasound%20has%20emerged%20as%20the%20primary%20imaging%20modality%0Afor%20assessing%20dynamic%20valvular%20morphology.%20However%2C%204D%20mitral%20valve%20%28MV%29%0Aanalysis%20remains%20challenging%20due%20to%20limited%20phase%20annotations%2C%20severe%20motion%0Aartifacts%2C%20and%20poor%20imaging%20quality.%20Yet%2C%20the%20absence%20of%20inter-phase%20dependency%0Ain%20existing%20methods%20hinders%204D%20MV%20analysis.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%0AMotion-Topology%20guided%20consistency%20network%20%28MTCNet%29%20for%20accurate%204D%20MV%0Aultrasound%20segmentation%20in%20semi-supervised%20learning%20%28SSL%29.%20MTCNet%20requires%20only%0Asparse%20end-diastolic%20and%20end-systolic%20annotations.%20First%2C%20we%20design%20a%0Across-phase%20motion-guided%20consistency%20learning%20strategy%2C%20utilizing%20a%0Abi-directional%20attention%20memory%20bank%20to%20propagate%20spatio-temporal%20features.%0AThis%20enables%20MTCNet%20to%20achieve%20excellent%20performance%20both%20per-%20and%20inter-phase.%0ASecond%2C%20we%20devise%20a%20novel%20topology-guided%20correlation%20regularization%20that%0Aexplores%20physical%20prior%20knowledge%20to%20maintain%20anatomically%20plausible.%0ATherefore%2C%20MTCNet%20can%20effectively%20leverage%20structural%20correspondence%20between%0Alabeled%20and%20unlabeled%20phases.%20Extensive%20evaluations%20on%20the%20first%20largest%204D%20MV%0Adataset%2C%20with%201408%20phases%20from%20160%20patients%2C%20show%20that%20MTCNet%20performs%20superior%0Across-phase%20consistency%20compared%20to%20other%20advanced%20methods%20%28Dice%3A%2087.30%25%2C%20HD%3A%0A1.75mm%29.%20Both%20the%20code%20and%20the%20dataset%20are%20available%20at%0Ahttps%3A//github.com/crs524/MTCNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00660v2&entry.124074799=Read"},
{"title": "LUDO: Low-Latency Understanding of Deformable Objects using Point Cloud\n  Occupancy Functions", "author": "Pit Henrich and Franziska Mathis-Ullrich and Paul Maria Scheikl", "abstract": "  Accurately determining the shape of deformable objects and the location of\ntheir internal structures is crucial for medical tasks that require precise\ntargeting, such as robotic biopsies. We introduce LUDO, a method for accurate\nlow-latency understanding of deformable objects. LUDO reconstructs objects in\ntheir deformed state, including their internal structures, from a single-view\npoint cloud observation in under 30 ms using occupancy networks. LUDO provides\nuncertainty estimates for its predictions. Additionally, it provides\nexplainability by highlighting key features in its input observations. Both\nuncertainty and explainability are important for safety-critical applications\nsuch as surgery. We evaluate LUDO in real-world robotic experiments, achieving\na success rate of 98.9% for puncturing various regions of interest (ROIs)\ninside deformable objects. We compare LUDO to a popular baseline and show its\nsuperior ROI localization accuracy, training time, and memory requirements.\nLUDO demonstrates the potential to interact with deformable objects without the\nneed for deformable registration methods.\n", "link": "http://arxiv.org/abs/2411.08777v5", "date": "2025-07-03", "relevancy": 2.229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5915}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUDO%3A%20Low-Latency%20Understanding%20of%20Deformable%20Objects%20using%20Point%20Cloud%0A%20%20Occupancy%20Functions&body=Title%3A%20LUDO%3A%20Low-Latency%20Understanding%20of%20Deformable%20Objects%20using%20Point%20Cloud%0A%20%20Occupancy%20Functions%0AAuthor%3A%20Pit%20Henrich%20and%20Franziska%20Mathis-Ullrich%20and%20Paul%20Maria%20Scheikl%0AAbstract%3A%20%20%20Accurately%20determining%20the%20shape%20of%20deformable%20objects%20and%20the%20location%20of%0Atheir%20internal%20structures%20is%20crucial%20for%20medical%20tasks%20that%20require%20precise%0Atargeting%2C%20such%20as%20robotic%20biopsies.%20We%20introduce%20LUDO%2C%20a%20method%20for%20accurate%0Alow-latency%20understanding%20of%20deformable%20objects.%20LUDO%20reconstructs%20objects%20in%0Atheir%20deformed%20state%2C%20including%20their%20internal%20structures%2C%20from%20a%20single-view%0Apoint%20cloud%20observation%20in%20under%2030%20ms%20using%20occupancy%20networks.%20LUDO%20provides%0Auncertainty%20estimates%20for%20its%20predictions.%20Additionally%2C%20it%20provides%0Aexplainability%20by%20highlighting%20key%20features%20in%20its%20input%20observations.%20Both%0Auncertainty%20and%20explainability%20are%20important%20for%20safety-critical%20applications%0Asuch%20as%20surgery.%20We%20evaluate%20LUDO%20in%20real-world%20robotic%20experiments%2C%20achieving%0Aa%20success%20rate%20of%2098.9%25%20for%20puncturing%20various%20regions%20of%20interest%20%28ROIs%29%0Ainside%20deformable%20objects.%20We%20compare%20LUDO%20to%20a%20popular%20baseline%20and%20show%20its%0Asuperior%20ROI%20localization%20accuracy%2C%20training%20time%2C%20and%20memory%20requirements.%0ALUDO%20demonstrates%20the%20potential%20to%20interact%20with%20deformable%20objects%20without%20the%0Aneed%20for%20deformable%20registration%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08777v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUDO%253A%2520Low-Latency%2520Understanding%2520of%2520Deformable%2520Objects%2520using%2520Point%2520Cloud%250A%2520%2520Occupancy%2520Functions%26entry.906535625%3DPit%2520Henrich%2520and%2520Franziska%2520Mathis-Ullrich%2520and%2520Paul%2520Maria%2520Scheikl%26entry.1292438233%3D%2520%2520Accurately%2520determining%2520the%2520shape%2520of%2520deformable%2520objects%2520and%2520the%2520location%2520of%250Atheir%2520internal%2520structures%2520is%2520crucial%2520for%2520medical%2520tasks%2520that%2520require%2520precise%250Atargeting%252C%2520such%2520as%2520robotic%2520biopsies.%2520We%2520introduce%2520LUDO%252C%2520a%2520method%2520for%2520accurate%250Alow-latency%2520understanding%2520of%2520deformable%2520objects.%2520LUDO%2520reconstructs%2520objects%2520in%250Atheir%2520deformed%2520state%252C%2520including%2520their%2520internal%2520structures%252C%2520from%2520a%2520single-view%250Apoint%2520cloud%2520observation%2520in%2520under%252030%2520ms%2520using%2520occupancy%2520networks.%2520LUDO%2520provides%250Auncertainty%2520estimates%2520for%2520its%2520predictions.%2520Additionally%252C%2520it%2520provides%250Aexplainability%2520by%2520highlighting%2520key%2520features%2520in%2520its%2520input%2520observations.%2520Both%250Auncertainty%2520and%2520explainability%2520are%2520important%2520for%2520safety-critical%2520applications%250Asuch%2520as%2520surgery.%2520We%2520evaluate%2520LUDO%2520in%2520real-world%2520robotic%2520experiments%252C%2520achieving%250Aa%2520success%2520rate%2520of%252098.9%2525%2520for%2520puncturing%2520various%2520regions%2520of%2520interest%2520%2528ROIs%2529%250Ainside%2520deformable%2520objects.%2520We%2520compare%2520LUDO%2520to%2520a%2520popular%2520baseline%2520and%2520show%2520its%250Asuperior%2520ROI%2520localization%2520accuracy%252C%2520training%2520time%252C%2520and%2520memory%2520requirements.%250ALUDO%2520demonstrates%2520the%2520potential%2520to%2520interact%2520with%2520deformable%2520objects%2520without%2520the%250Aneed%2520for%2520deformable%2520registration%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08777v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUDO%3A%20Low-Latency%20Understanding%20of%20Deformable%20Objects%20using%20Point%20Cloud%0A%20%20Occupancy%20Functions&entry.906535625=Pit%20Henrich%20and%20Franziska%20Mathis-Ullrich%20and%20Paul%20Maria%20Scheikl&entry.1292438233=%20%20Accurately%20determining%20the%20shape%20of%20deformable%20objects%20and%20the%20location%20of%0Atheir%20internal%20structures%20is%20crucial%20for%20medical%20tasks%20that%20require%20precise%0Atargeting%2C%20such%20as%20robotic%20biopsies.%20We%20introduce%20LUDO%2C%20a%20method%20for%20accurate%0Alow-latency%20understanding%20of%20deformable%20objects.%20LUDO%20reconstructs%20objects%20in%0Atheir%20deformed%20state%2C%20including%20their%20internal%20structures%2C%20from%20a%20single-view%0Apoint%20cloud%20observation%20in%20under%2030%20ms%20using%20occupancy%20networks.%20LUDO%20provides%0Auncertainty%20estimates%20for%20its%20predictions.%20Additionally%2C%20it%20provides%0Aexplainability%20by%20highlighting%20key%20features%20in%20its%20input%20observations.%20Both%0Auncertainty%20and%20explainability%20are%20important%20for%20safety-critical%20applications%0Asuch%20as%20surgery.%20We%20evaluate%20LUDO%20in%20real-world%20robotic%20experiments%2C%20achieving%0Aa%20success%20rate%20of%2098.9%25%20for%20puncturing%20various%20regions%20of%20interest%20%28ROIs%29%0Ainside%20deformable%20objects.%20We%20compare%20LUDO%20to%20a%20popular%20baseline%20and%20show%20its%0Asuperior%20ROI%20localization%20accuracy%2C%20training%20time%2C%20and%20memory%20requirements.%0ALUDO%20demonstrates%20the%20potential%20to%20interact%20with%20deformable%20objects%20without%20the%0Aneed%20for%20deformable%20registration%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08777v5&entry.124074799=Read"},
{"title": "IGDNet: Zero-Shot Robust Underexposed Image Enhancement via\n  Illumination-Guided and Denoising", "author": "Hailong Yan and Junjian Huang and Tingwen Huang", "abstract": "  Current methods for restoring underexposed images typically rely on\nsupervised learning with paired underexposed and well-illuminated images.\nHowever, collecting such datasets is often impractical in real-world scenarios.\nMoreover, these methods can lead to over-enhancement, distorting\nwell-illuminated regions. To address these issues, we propose IGDNet, a\nZero-Shot enhancement method that operates solely on a single test image,\nwithout requiring guiding priors or training data. IGDNet exhibits strong\ngeneralization ability and effectively suppresses noise while restoring\nillumination. The framework comprises a decomposition module and a denoising\nmodule. The former separates the image into illumination and reflection\ncomponents via a dense connection network, while the latter enhances\nnon-uniformly illuminated regions using an illumination-guided pixel adaptive\ncorrection method. A noise pair is generated through downsampling and refined\niteratively to produce the final result. Extensive experiments on four public\ndatasets demonstrate that IGDNet significantly improves visual quality under\ncomplex lighting conditions. Quantitative results on metrics like PSNR\n(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art\nunsupervised methods. The code will be released soon.\n", "link": "http://arxiv.org/abs/2507.02445v1", "date": "2025-07-03", "relevancy": 2.2231, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5862}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5514}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IGDNet%3A%20Zero-Shot%20Robust%20Underexposed%20Image%20Enhancement%20via%0A%20%20Illumination-Guided%20and%20Denoising&body=Title%3A%20IGDNet%3A%20Zero-Shot%20Robust%20Underexposed%20Image%20Enhancement%20via%0A%20%20Illumination-Guided%20and%20Denoising%0AAuthor%3A%20Hailong%20Yan%20and%20Junjian%20Huang%20and%20Tingwen%20Huang%0AAbstract%3A%20%20%20Current%20methods%20for%20restoring%20underexposed%20images%20typically%20rely%20on%0Asupervised%20learning%20with%20paired%20underexposed%20and%20well-illuminated%20images.%0AHowever%2C%20collecting%20such%20datasets%20is%20often%20impractical%20in%20real-world%20scenarios.%0AMoreover%2C%20these%20methods%20can%20lead%20to%20over-enhancement%2C%20distorting%0Awell-illuminated%20regions.%20To%20address%20these%20issues%2C%20we%20propose%20IGDNet%2C%20a%0AZero-Shot%20enhancement%20method%20that%20operates%20solely%20on%20a%20single%20test%20image%2C%0Awithout%20requiring%20guiding%20priors%20or%20training%20data.%20IGDNet%20exhibits%20strong%0Ageneralization%20ability%20and%20effectively%20suppresses%20noise%20while%20restoring%0Aillumination.%20The%20framework%20comprises%20a%20decomposition%20module%20and%20a%20denoising%0Amodule.%20The%20former%20separates%20the%20image%20into%20illumination%20and%20reflection%0Acomponents%20via%20a%20dense%20connection%20network%2C%20while%20the%20latter%20enhances%0Anon-uniformly%20illuminated%20regions%20using%20an%20illumination-guided%20pixel%20adaptive%0Acorrection%20method.%20A%20noise%20pair%20is%20generated%20through%20downsampling%20and%20refined%0Aiteratively%20to%20produce%20the%20final%20result.%20Extensive%20experiments%20on%20four%20public%0Adatasets%20demonstrate%20that%20IGDNet%20significantly%20improves%20visual%20quality%20under%0Acomplex%20lighting%20conditions.%20Quantitative%20results%20on%20metrics%20like%20PSNR%0A%2820.41dB%29%20and%20SSIM%20%280.860dB%29%20show%20that%20it%20outperforms%2014%20state-of-the-art%0Aunsupervised%20methods.%20The%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIGDNet%253A%2520Zero-Shot%2520Robust%2520Underexposed%2520Image%2520Enhancement%2520via%250A%2520%2520Illumination-Guided%2520and%2520Denoising%26entry.906535625%3DHailong%2520Yan%2520and%2520Junjian%2520Huang%2520and%2520Tingwen%2520Huang%26entry.1292438233%3D%2520%2520Current%2520methods%2520for%2520restoring%2520underexposed%2520images%2520typically%2520rely%2520on%250Asupervised%2520learning%2520with%2520paired%2520underexposed%2520and%2520well-illuminated%2520images.%250AHowever%252C%2520collecting%2520such%2520datasets%2520is%2520often%2520impractical%2520in%2520real-world%2520scenarios.%250AMoreover%252C%2520these%2520methods%2520can%2520lead%2520to%2520over-enhancement%252C%2520distorting%250Awell-illuminated%2520regions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520IGDNet%252C%2520a%250AZero-Shot%2520enhancement%2520method%2520that%2520operates%2520solely%2520on%2520a%2520single%2520test%2520image%252C%250Awithout%2520requiring%2520guiding%2520priors%2520or%2520training%2520data.%2520IGDNet%2520exhibits%2520strong%250Ageneralization%2520ability%2520and%2520effectively%2520suppresses%2520noise%2520while%2520restoring%250Aillumination.%2520The%2520framework%2520comprises%2520a%2520decomposition%2520module%2520and%2520a%2520denoising%250Amodule.%2520The%2520former%2520separates%2520the%2520image%2520into%2520illumination%2520and%2520reflection%250Acomponents%2520via%2520a%2520dense%2520connection%2520network%252C%2520while%2520the%2520latter%2520enhances%250Anon-uniformly%2520illuminated%2520regions%2520using%2520an%2520illumination-guided%2520pixel%2520adaptive%250Acorrection%2520method.%2520A%2520noise%2520pair%2520is%2520generated%2520through%2520downsampling%2520and%2520refined%250Aiteratively%2520to%2520produce%2520the%2520final%2520result.%2520Extensive%2520experiments%2520on%2520four%2520public%250Adatasets%2520demonstrate%2520that%2520IGDNet%2520significantly%2520improves%2520visual%2520quality%2520under%250Acomplex%2520lighting%2520conditions.%2520Quantitative%2520results%2520on%2520metrics%2520like%2520PSNR%250A%252820.41dB%2529%2520and%2520SSIM%2520%25280.860dB%2529%2520show%2520that%2520it%2520outperforms%252014%2520state-of-the-art%250Aunsupervised%2520methods.%2520The%2520code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IGDNet%3A%20Zero-Shot%20Robust%20Underexposed%20Image%20Enhancement%20via%0A%20%20Illumination-Guided%20and%20Denoising&entry.906535625=Hailong%20Yan%20and%20Junjian%20Huang%20and%20Tingwen%20Huang&entry.1292438233=%20%20Current%20methods%20for%20restoring%20underexposed%20images%20typically%20rely%20on%0Asupervised%20learning%20with%20paired%20underexposed%20and%20well-illuminated%20images.%0AHowever%2C%20collecting%20such%20datasets%20is%20often%20impractical%20in%20real-world%20scenarios.%0AMoreover%2C%20these%20methods%20can%20lead%20to%20over-enhancement%2C%20distorting%0Awell-illuminated%20regions.%20To%20address%20these%20issues%2C%20we%20propose%20IGDNet%2C%20a%0AZero-Shot%20enhancement%20method%20that%20operates%20solely%20on%20a%20single%20test%20image%2C%0Awithout%20requiring%20guiding%20priors%20or%20training%20data.%20IGDNet%20exhibits%20strong%0Ageneralization%20ability%20and%20effectively%20suppresses%20noise%20while%20restoring%0Aillumination.%20The%20framework%20comprises%20a%20decomposition%20module%20and%20a%20denoising%0Amodule.%20The%20former%20separates%20the%20image%20into%20illumination%20and%20reflection%0Acomponents%20via%20a%20dense%20connection%20network%2C%20while%20the%20latter%20enhances%0Anon-uniformly%20illuminated%20regions%20using%20an%20illumination-guided%20pixel%20adaptive%0Acorrection%20method.%20A%20noise%20pair%20is%20generated%20through%20downsampling%20and%20refined%0Aiteratively%20to%20produce%20the%20final%20result.%20Extensive%20experiments%20on%20four%20public%0Adatasets%20demonstrate%20that%20IGDNet%20significantly%20improves%20visual%20quality%20under%0Acomplex%20lighting%20conditions.%20Quantitative%20results%20on%20metrics%20like%20PSNR%0A%2820.41dB%29%20and%20SSIM%20%280.860dB%29%20show%20that%20it%20outperforms%2014%20state-of-the-art%0Aunsupervised%20methods.%20The%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02445v1&entry.124074799=Read"},
{"title": "TABNet: A Triplet Augmentation Self-Recovery Framework with\n  Boundary-Aware Pseudo-Labels for Medical Image Segmentation", "author": "Peilin Zhang and Shaouxan Wua and Jun Feng and Zhuo Jin and Zhizezhang Gao and Jingkun Chen and Yaqiong Xing and Xiao Zhang", "abstract": "  Background and objective: Medical image segmentation is a core task in\nvarious clinical applications. However, acquiring large-scale, fully annotated\nmedical image datasets is both time-consuming and costly. Scribble annotations,\nas a form of sparse labeling, provide an efficient and cost-effective\nalternative for medical image segmentation. However, the sparsity of scribble\nannotations limits the feature learning of the target region and lacks\nsufficient boundary supervision, which poses significant challenges for\ntraining segmentation networks. Methods: We propose TAB Net, a novel\nweakly-supervised medical image segmentation framework, consisting of two key\ncomponents: the triplet augmentation self-recovery (TAS) module and the\nboundary-aware pseudo-label supervision (BAP) module. The TAS module enhances\nfeature learning through three complementary augmentation strategies: intensity\ntransformation improves the model's sensitivity to texture and contrast\nvariations, cutout forces the network to capture local anatomical structures by\nmasking key regions, and jigsaw augmentation strengthens the modeling of global\nanatomical layout by disrupting spatial continuity. By guiding the network to\nrecover complete masks from diverse augmented inputs, TAS promotes a deeper\nsemantic understanding of medical images under sparse supervision. The BAP\nmodule enhances pseudo-supervision accuracy and boundary modeling by fusing\ndual-branch predictions into a loss-weighted pseudo-label and introducing a\nboundary-aware loss for fine-grained contour refinement. Results: Experimental\nevaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB\nNet significantly outperforms state-of-the-art methods for scribble-based\nweakly supervised segmentation. Moreover, it achieves performance comparable to\nthat of fully supervised methods.\n", "link": "http://arxiv.org/abs/2507.02399v1", "date": "2025-07-03", "relevancy": 2.2194, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5694}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TABNet%3A%20A%20Triplet%20Augmentation%20Self-Recovery%20Framework%20with%0A%20%20Boundary-Aware%20Pseudo-Labels%20for%20Medical%20Image%20Segmentation&body=Title%3A%20TABNet%3A%20A%20Triplet%20Augmentation%20Self-Recovery%20Framework%20with%0A%20%20Boundary-Aware%20Pseudo-Labels%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Peilin%20Zhang%20and%20Shaouxan%20Wua%20and%20Jun%20Feng%20and%20Zhuo%20Jin%20and%20Zhizezhang%20Gao%20and%20Jingkun%20Chen%20and%20Yaqiong%20Xing%20and%20Xiao%20Zhang%0AAbstract%3A%20%20%20Background%20and%20objective%3A%20Medical%20image%20segmentation%20is%20a%20core%20task%20in%0Avarious%20clinical%20applications.%20However%2C%20acquiring%20large-scale%2C%20fully%20annotated%0Amedical%20image%20datasets%20is%20both%20time-consuming%20and%20costly.%20Scribble%20annotations%2C%0Aas%20a%20form%20of%20sparse%20labeling%2C%20provide%20an%20efficient%20and%20cost-effective%0Aalternative%20for%20medical%20image%20segmentation.%20However%2C%20the%20sparsity%20of%20scribble%0Aannotations%20limits%20the%20feature%20learning%20of%20the%20target%20region%20and%20lacks%0Asufficient%20boundary%20supervision%2C%20which%20poses%20significant%20challenges%20for%0Atraining%20segmentation%20networks.%20Methods%3A%20We%20propose%20TAB%20Net%2C%20a%20novel%0Aweakly-supervised%20medical%20image%20segmentation%20framework%2C%20consisting%20of%20two%20key%0Acomponents%3A%20the%20triplet%20augmentation%20self-recovery%20%28TAS%29%20module%20and%20the%0Aboundary-aware%20pseudo-label%20supervision%20%28BAP%29%20module.%20The%20TAS%20module%20enhances%0Afeature%20learning%20through%20three%20complementary%20augmentation%20strategies%3A%20intensity%0Atransformation%20improves%20the%20model%27s%20sensitivity%20to%20texture%20and%20contrast%0Avariations%2C%20cutout%20forces%20the%20network%20to%20capture%20local%20anatomical%20structures%20by%0Amasking%20key%20regions%2C%20and%20jigsaw%20augmentation%20strengthens%20the%20modeling%20of%20global%0Aanatomical%20layout%20by%20disrupting%20spatial%20continuity.%20By%20guiding%20the%20network%20to%0Arecover%20complete%20masks%20from%20diverse%20augmented%20inputs%2C%20TAS%20promotes%20a%20deeper%0Asemantic%20understanding%20of%20medical%20images%20under%20sparse%20supervision.%20The%20BAP%0Amodule%20enhances%20pseudo-supervision%20accuracy%20and%20boundary%20modeling%20by%20fusing%0Adual-branch%20predictions%20into%20a%20loss-weighted%20pseudo-label%20and%20introducing%20a%0Aboundary-aware%20loss%20for%20fine-grained%20contour%20refinement.%20Results%3A%20Experimental%0Aevaluations%20on%20two%20public%20datasets%2C%20ACDC%20and%20MSCMR%20seg%2C%20demonstrate%20that%20TAB%0ANet%20significantly%20outperforms%20state-of-the-art%20methods%20for%20scribble-based%0Aweakly%20supervised%20segmentation.%20Moreover%2C%20it%20achieves%20performance%20comparable%20to%0Athat%20of%20fully%20supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTABNet%253A%2520A%2520Triplet%2520Augmentation%2520Self-Recovery%2520Framework%2520with%250A%2520%2520Boundary-Aware%2520Pseudo-Labels%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DPeilin%2520Zhang%2520and%2520Shaouxan%2520Wua%2520and%2520Jun%2520Feng%2520and%2520Zhuo%2520Jin%2520and%2520Zhizezhang%2520Gao%2520and%2520Jingkun%2520Chen%2520and%2520Yaqiong%2520Xing%2520and%2520Xiao%2520Zhang%26entry.1292438233%3D%2520%2520Background%2520and%2520objective%253A%2520Medical%2520image%2520segmentation%2520is%2520a%2520core%2520task%2520in%250Avarious%2520clinical%2520applications.%2520However%252C%2520acquiring%2520large-scale%252C%2520fully%2520annotated%250Amedical%2520image%2520datasets%2520is%2520both%2520time-consuming%2520and%2520costly.%2520Scribble%2520annotations%252C%250Aas%2520a%2520form%2520of%2520sparse%2520labeling%252C%2520provide%2520an%2520efficient%2520and%2520cost-effective%250Aalternative%2520for%2520medical%2520image%2520segmentation.%2520However%252C%2520the%2520sparsity%2520of%2520scribble%250Aannotations%2520limits%2520the%2520feature%2520learning%2520of%2520the%2520target%2520region%2520and%2520lacks%250Asufficient%2520boundary%2520supervision%252C%2520which%2520poses%2520significant%2520challenges%2520for%250Atraining%2520segmentation%2520networks.%2520Methods%253A%2520We%2520propose%2520TAB%2520Net%252C%2520a%2520novel%250Aweakly-supervised%2520medical%2520image%2520segmentation%2520framework%252C%2520consisting%2520of%2520two%2520key%250Acomponents%253A%2520the%2520triplet%2520augmentation%2520self-recovery%2520%2528TAS%2529%2520module%2520and%2520the%250Aboundary-aware%2520pseudo-label%2520supervision%2520%2528BAP%2529%2520module.%2520The%2520TAS%2520module%2520enhances%250Afeature%2520learning%2520through%2520three%2520complementary%2520augmentation%2520strategies%253A%2520intensity%250Atransformation%2520improves%2520the%2520model%2527s%2520sensitivity%2520to%2520texture%2520and%2520contrast%250Avariations%252C%2520cutout%2520forces%2520the%2520network%2520to%2520capture%2520local%2520anatomical%2520structures%2520by%250Amasking%2520key%2520regions%252C%2520and%2520jigsaw%2520augmentation%2520strengthens%2520the%2520modeling%2520of%2520global%250Aanatomical%2520layout%2520by%2520disrupting%2520spatial%2520continuity.%2520By%2520guiding%2520the%2520network%2520to%250Arecover%2520complete%2520masks%2520from%2520diverse%2520augmented%2520inputs%252C%2520TAS%2520promotes%2520a%2520deeper%250Asemantic%2520understanding%2520of%2520medical%2520images%2520under%2520sparse%2520supervision.%2520The%2520BAP%250Amodule%2520enhances%2520pseudo-supervision%2520accuracy%2520and%2520boundary%2520modeling%2520by%2520fusing%250Adual-branch%2520predictions%2520into%2520a%2520loss-weighted%2520pseudo-label%2520and%2520introducing%2520a%250Aboundary-aware%2520loss%2520for%2520fine-grained%2520contour%2520refinement.%2520Results%253A%2520Experimental%250Aevaluations%2520on%2520two%2520public%2520datasets%252C%2520ACDC%2520and%2520MSCMR%2520seg%252C%2520demonstrate%2520that%2520TAB%250ANet%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520for%2520scribble-based%250Aweakly%2520supervised%2520segmentation.%2520Moreover%252C%2520it%2520achieves%2520performance%2520comparable%2520to%250Athat%2520of%2520fully%2520supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TABNet%3A%20A%20Triplet%20Augmentation%20Self-Recovery%20Framework%20with%0A%20%20Boundary-Aware%20Pseudo-Labels%20for%20Medical%20Image%20Segmentation&entry.906535625=Peilin%20Zhang%20and%20Shaouxan%20Wua%20and%20Jun%20Feng%20and%20Zhuo%20Jin%20and%20Zhizezhang%20Gao%20and%20Jingkun%20Chen%20and%20Yaqiong%20Xing%20and%20Xiao%20Zhang&entry.1292438233=%20%20Background%20and%20objective%3A%20Medical%20image%20segmentation%20is%20a%20core%20task%20in%0Avarious%20clinical%20applications.%20However%2C%20acquiring%20large-scale%2C%20fully%20annotated%0Amedical%20image%20datasets%20is%20both%20time-consuming%20and%20costly.%20Scribble%20annotations%2C%0Aas%20a%20form%20of%20sparse%20labeling%2C%20provide%20an%20efficient%20and%20cost-effective%0Aalternative%20for%20medical%20image%20segmentation.%20However%2C%20the%20sparsity%20of%20scribble%0Aannotations%20limits%20the%20feature%20learning%20of%20the%20target%20region%20and%20lacks%0Asufficient%20boundary%20supervision%2C%20which%20poses%20significant%20challenges%20for%0Atraining%20segmentation%20networks.%20Methods%3A%20We%20propose%20TAB%20Net%2C%20a%20novel%0Aweakly-supervised%20medical%20image%20segmentation%20framework%2C%20consisting%20of%20two%20key%0Acomponents%3A%20the%20triplet%20augmentation%20self-recovery%20%28TAS%29%20module%20and%20the%0Aboundary-aware%20pseudo-label%20supervision%20%28BAP%29%20module.%20The%20TAS%20module%20enhances%0Afeature%20learning%20through%20three%20complementary%20augmentation%20strategies%3A%20intensity%0Atransformation%20improves%20the%20model%27s%20sensitivity%20to%20texture%20and%20contrast%0Avariations%2C%20cutout%20forces%20the%20network%20to%20capture%20local%20anatomical%20structures%20by%0Amasking%20key%20regions%2C%20and%20jigsaw%20augmentation%20strengthens%20the%20modeling%20of%20global%0Aanatomical%20layout%20by%20disrupting%20spatial%20continuity.%20By%20guiding%20the%20network%20to%0Arecover%20complete%20masks%20from%20diverse%20augmented%20inputs%2C%20TAS%20promotes%20a%20deeper%0Asemantic%20understanding%20of%20medical%20images%20under%20sparse%20supervision.%20The%20BAP%0Amodule%20enhances%20pseudo-supervision%20accuracy%20and%20boundary%20modeling%20by%20fusing%0Adual-branch%20predictions%20into%20a%20loss-weighted%20pseudo-label%20and%20introducing%20a%0Aboundary-aware%20loss%20for%20fine-grained%20contour%20refinement.%20Results%3A%20Experimental%0Aevaluations%20on%20two%20public%20datasets%2C%20ACDC%20and%20MSCMR%20seg%2C%20demonstrate%20that%20TAB%0ANet%20significantly%20outperforms%20state-of-the-art%20methods%20for%20scribble-based%0Aweakly%20supervised%20segmentation.%20Moreover%2C%20it%20achieves%20performance%20comparable%20to%0Athat%20of%20fully%20supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02399v1&entry.124074799=Read"},
{"title": "XGeM: A Multi-Prompt Foundation Model for Multimodal Medical Data\n  Generation", "author": "Daniele Molino and Francesco Di Feola and Eliodoro Faiella and Deborah Fazzini and Domiziana Santucci and Linlin Shen and Valerio Guarrasi and Paolo Soda", "abstract": "  The adoption of Artificial Intelligence in medical imaging holds great\npromise, yet it remains hindered by challenges such as data scarcity, privacy\nconcerns, and the need for robust multimodal integration. While recent advances\nin generative modeling have enabled high-quality synthetic data generation,\nexisting approaches are often limited to unimodal, unidirectional synthesis and\ntherefore lack the ability to jointly synthesize multiple modalities while\npreserving clinical consistency. To address this challenge, we introduce XGeM,\na 6.77-billion-parameter multimodal generative model designed to support\nflexible, any-to-any synthesis between medical data modalities. XGeM constructs\na shared latent space via contrastive learning and introduces a novel\nMulti-Prompt Training strategy, enabling conditioning on arbitrary subsets of\ninput modalities. This design allows the model to adapt to heterogeneous\nclinical inputs and generate multiple outputs jointly, preserving both semantic\nand structural coherence. We extensively validate XGeM: first we benchmark it\nagainst five competitors on the MIMIC-CXR dataset, a state-of-the-art dataset\nfor multi-view Chest X-ray and radiological report generation. Secondly, we\nperform a Visual Turing Test with expert radiologists to assess the realism and\nclinical relevance of the generated data, ensuring alignment with real-world\nscenarios. Finally, we show how XGeM can support key medical data challenges\nsuch as anonymization, class imbalance, and data scarcity, underscoring its\nutility as a foundation model for medical data synthesis. Project page is at\nhttps://cosbidev.github.io/XGeM/.\n", "link": "http://arxiv.org/abs/2501.04614v3", "date": "2025-07-03", "relevancy": 2.2104, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5606}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5592}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XGeM%3A%20A%20Multi-Prompt%20Foundation%20Model%20for%20Multimodal%20Medical%20Data%0A%20%20Generation&body=Title%3A%20XGeM%3A%20A%20Multi-Prompt%20Foundation%20Model%20for%20Multimodal%20Medical%20Data%0A%20%20Generation%0AAuthor%3A%20Daniele%20Molino%20and%20Francesco%20Di%20Feola%20and%20Eliodoro%20Faiella%20and%20Deborah%20Fazzini%20and%20Domiziana%20Santucci%20and%20Linlin%20Shen%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda%0AAbstract%3A%20%20%20The%20adoption%20of%20Artificial%20Intelligence%20in%20medical%20imaging%20holds%20great%0Apromise%2C%20yet%20it%20remains%20hindered%20by%20challenges%20such%20as%20data%20scarcity%2C%20privacy%0Aconcerns%2C%20and%20the%20need%20for%20robust%20multimodal%20integration.%20While%20recent%20advances%0Ain%20generative%20modeling%20have%20enabled%20high-quality%20synthetic%20data%20generation%2C%0Aexisting%20approaches%20are%20often%20limited%20to%20unimodal%2C%20unidirectional%20synthesis%20and%0Atherefore%20lack%20the%20ability%20to%20jointly%20synthesize%20multiple%20modalities%20while%0Apreserving%20clinical%20consistency.%20To%20address%20this%20challenge%2C%20we%20introduce%20XGeM%2C%0Aa%206.77-billion-parameter%20multimodal%20generative%20model%20designed%20to%20support%0Aflexible%2C%20any-to-any%20synthesis%20between%20medical%20data%20modalities.%20XGeM%20constructs%0Aa%20shared%20latent%20space%20via%20contrastive%20learning%20and%20introduces%20a%20novel%0AMulti-Prompt%20Training%20strategy%2C%20enabling%20conditioning%20on%20arbitrary%20subsets%20of%0Ainput%20modalities.%20This%20design%20allows%20the%20model%20to%20adapt%20to%20heterogeneous%0Aclinical%20inputs%20and%20generate%20multiple%20outputs%20jointly%2C%20preserving%20both%20semantic%0Aand%20structural%20coherence.%20We%20extensively%20validate%20XGeM%3A%20first%20we%20benchmark%20it%0Aagainst%20five%20competitors%20on%20the%20MIMIC-CXR%20dataset%2C%20a%20state-of-the-art%20dataset%0Afor%20multi-view%20Chest%20X-ray%20and%20radiological%20report%20generation.%20Secondly%2C%20we%0Aperform%20a%20Visual%20Turing%20Test%20with%20expert%20radiologists%20to%20assess%20the%20realism%20and%0Aclinical%20relevance%20of%20the%20generated%20data%2C%20ensuring%20alignment%20with%20real-world%0Ascenarios.%20Finally%2C%20we%20show%20how%20XGeM%20can%20support%20key%20medical%20data%20challenges%0Asuch%20as%20anonymization%2C%20class%20imbalance%2C%20and%20data%20scarcity%2C%20underscoring%20its%0Autility%20as%20a%20foundation%20model%20for%20medical%20data%20synthesis.%20Project%20page%20is%20at%0Ahttps%3A//cosbidev.github.io/XGeM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04614v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXGeM%253A%2520A%2520Multi-Prompt%2520Foundation%2520Model%2520for%2520Multimodal%2520Medical%2520Data%250A%2520%2520Generation%26entry.906535625%3DDaniele%2520Molino%2520and%2520Francesco%2520Di%2520Feola%2520and%2520Eliodoro%2520Faiella%2520and%2520Deborah%2520Fazzini%2520and%2520Domiziana%2520Santucci%2520and%2520Linlin%2520Shen%2520and%2520Valerio%2520Guarrasi%2520and%2520Paolo%2520Soda%26entry.1292438233%3D%2520%2520The%2520adoption%2520of%2520Artificial%2520Intelligence%2520in%2520medical%2520imaging%2520holds%2520great%250Apromise%252C%2520yet%2520it%2520remains%2520hindered%2520by%2520challenges%2520such%2520as%2520data%2520scarcity%252C%2520privacy%250Aconcerns%252C%2520and%2520the%2520need%2520for%2520robust%2520multimodal%2520integration.%2520While%2520recent%2520advances%250Ain%2520generative%2520modeling%2520have%2520enabled%2520high-quality%2520synthetic%2520data%2520generation%252C%250Aexisting%2520approaches%2520are%2520often%2520limited%2520to%2520unimodal%252C%2520unidirectional%2520synthesis%2520and%250Atherefore%2520lack%2520the%2520ability%2520to%2520jointly%2520synthesize%2520multiple%2520modalities%2520while%250Apreserving%2520clinical%2520consistency.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520XGeM%252C%250Aa%25206.77-billion-parameter%2520multimodal%2520generative%2520model%2520designed%2520to%2520support%250Aflexible%252C%2520any-to-any%2520synthesis%2520between%2520medical%2520data%2520modalities.%2520XGeM%2520constructs%250Aa%2520shared%2520latent%2520space%2520via%2520contrastive%2520learning%2520and%2520introduces%2520a%2520novel%250AMulti-Prompt%2520Training%2520strategy%252C%2520enabling%2520conditioning%2520on%2520arbitrary%2520subsets%2520of%250Ainput%2520modalities.%2520This%2520design%2520allows%2520the%2520model%2520to%2520adapt%2520to%2520heterogeneous%250Aclinical%2520inputs%2520and%2520generate%2520multiple%2520outputs%2520jointly%252C%2520preserving%2520both%2520semantic%250Aand%2520structural%2520coherence.%2520We%2520extensively%2520validate%2520XGeM%253A%2520first%2520we%2520benchmark%2520it%250Aagainst%2520five%2520competitors%2520on%2520the%2520MIMIC-CXR%2520dataset%252C%2520a%2520state-of-the-art%2520dataset%250Afor%2520multi-view%2520Chest%2520X-ray%2520and%2520radiological%2520report%2520generation.%2520Secondly%252C%2520we%250Aperform%2520a%2520Visual%2520Turing%2520Test%2520with%2520expert%2520radiologists%2520to%2520assess%2520the%2520realism%2520and%250Aclinical%2520relevance%2520of%2520the%2520generated%2520data%252C%2520ensuring%2520alignment%2520with%2520real-world%250Ascenarios.%2520Finally%252C%2520we%2520show%2520how%2520XGeM%2520can%2520support%2520key%2520medical%2520data%2520challenges%250Asuch%2520as%2520anonymization%252C%2520class%2520imbalance%252C%2520and%2520data%2520scarcity%252C%2520underscoring%2520its%250Autility%2520as%2520a%2520foundation%2520model%2520for%2520medical%2520data%2520synthesis.%2520Project%2520page%2520is%2520at%250Ahttps%253A//cosbidev.github.io/XGeM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04614v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XGeM%3A%20A%20Multi-Prompt%20Foundation%20Model%20for%20Multimodal%20Medical%20Data%0A%20%20Generation&entry.906535625=Daniele%20Molino%20and%20Francesco%20Di%20Feola%20and%20Eliodoro%20Faiella%20and%20Deborah%20Fazzini%20and%20Domiziana%20Santucci%20and%20Linlin%20Shen%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda&entry.1292438233=%20%20The%20adoption%20of%20Artificial%20Intelligence%20in%20medical%20imaging%20holds%20great%0Apromise%2C%20yet%20it%20remains%20hindered%20by%20challenges%20such%20as%20data%20scarcity%2C%20privacy%0Aconcerns%2C%20and%20the%20need%20for%20robust%20multimodal%20integration.%20While%20recent%20advances%0Ain%20generative%20modeling%20have%20enabled%20high-quality%20synthetic%20data%20generation%2C%0Aexisting%20approaches%20are%20often%20limited%20to%20unimodal%2C%20unidirectional%20synthesis%20and%0Atherefore%20lack%20the%20ability%20to%20jointly%20synthesize%20multiple%20modalities%20while%0Apreserving%20clinical%20consistency.%20To%20address%20this%20challenge%2C%20we%20introduce%20XGeM%2C%0Aa%206.77-billion-parameter%20multimodal%20generative%20model%20designed%20to%20support%0Aflexible%2C%20any-to-any%20synthesis%20between%20medical%20data%20modalities.%20XGeM%20constructs%0Aa%20shared%20latent%20space%20via%20contrastive%20learning%20and%20introduces%20a%20novel%0AMulti-Prompt%20Training%20strategy%2C%20enabling%20conditioning%20on%20arbitrary%20subsets%20of%0Ainput%20modalities.%20This%20design%20allows%20the%20model%20to%20adapt%20to%20heterogeneous%0Aclinical%20inputs%20and%20generate%20multiple%20outputs%20jointly%2C%20preserving%20both%20semantic%0Aand%20structural%20coherence.%20We%20extensively%20validate%20XGeM%3A%20first%20we%20benchmark%20it%0Aagainst%20five%20competitors%20on%20the%20MIMIC-CXR%20dataset%2C%20a%20state-of-the-art%20dataset%0Afor%20multi-view%20Chest%20X-ray%20and%20radiological%20report%20generation.%20Secondly%2C%20we%0Aperform%20a%20Visual%20Turing%20Test%20with%20expert%20radiologists%20to%20assess%20the%20realism%20and%0Aclinical%20relevance%20of%20the%20generated%20data%2C%20ensuring%20alignment%20with%20real-world%0Ascenarios.%20Finally%2C%20we%20show%20how%20XGeM%20can%20support%20key%20medical%20data%20challenges%0Asuch%20as%20anonymization%2C%20class%20imbalance%2C%20and%20data%20scarcity%2C%20underscoring%20its%0Autility%20as%20a%20foundation%20model%20for%20medical%20data%20synthesis.%20Project%20page%20is%20at%0Ahttps%3A//cosbidev.github.io/XGeM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04614v3&entry.124074799=Read"},
{"title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "author": "Kuan Li and Zhongwang Zhang and Huifeng Yin and Liwen Zhang and Litu Ou and Jialong Wu and Wenbiao Yin and Baixuan Li and Zhengwei Tao and Xinyu Wang and Weizhou Shen and Junkai Zhang and Dingchu Zhang and Xixi Wu and Yong Jiang and Ming Yan and Pengjun Xie and Fei Huang and Jingren Zhou", "abstract": "  Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.\n", "link": "http://arxiv.org/abs/2507.02592v1", "date": "2025-07-03", "relevancy": 1.5453, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5375}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5114}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WebSailor%3A%20Navigating%20Super-human%20Reasoning%20for%20Web%20Agent&body=Title%3A%20WebSailor%3A%20Navigating%20Super-human%20Reasoning%20for%20Web%20Agent%0AAuthor%3A%20Kuan%20Li%20and%20Zhongwang%20Zhang%20and%20Huifeng%20Yin%20and%20Liwen%20Zhang%20and%20Litu%20Ou%20and%20Jialong%20Wu%20and%20Wenbiao%20Yin%20and%20Baixuan%20Li%20and%20Zhengwei%20Tao%20and%20Xinyu%20Wang%20and%20Weizhou%20Shen%20and%20Junkai%20Zhang%20and%20Dingchu%20Zhang%20and%20Xixi%20Wu%20and%20Yong%20Jiang%20and%20Ming%20Yan%20and%20Pengjun%20Xie%20and%20Fei%20Huang%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Transcending%20human%20cognitive%20limitations%20represents%20a%20critical%20frontier%20in%0ALLM%20training.%20Proprietary%20agentic%20systems%20like%20DeepResearch%20have%20demonstrated%0Asuperhuman%20capabilities%20on%20extremely%20complex%20information-seeking%20benchmarks%0Asuch%20as%20BrowseComp%2C%20a%20feat%20previously%20unattainable.%20We%20posit%20that%20their%20success%0Ahinges%20on%20a%20sophisticated%20reasoning%20pattern%20absent%20in%20open-source%20models%3A%20the%0Aability%20to%20systematically%20reduce%20extreme%20uncertainty%20when%20navigating%20vast%0Ainformation%20landscapes.%20Based%20on%20this%20insight%2C%20we%20introduce%20WebSailor%2C%20a%0Acomplete%20post-training%20methodology%20designed%20to%20instill%20this%20crucial%20capability.%0AOur%20approach%20involves%20generating%20novel%2C%20high-uncertainty%20tasks%20through%0Astructured%20sampling%20and%20information%20obfuscation%2C%20RFT%20cold%20start%2C%20and%20an%0Aefficient%20agentic%20RL%20training%20algorithm%2C%20Duplicating%20Sampling%20Policy%0AOptimization%20%28DUPO%29.%20With%20this%20integrated%20pipeline%2C%20WebSailor%20significantly%0Aoutperforms%20all%20opensource%20agents%20in%20complex%20information-seeking%20tasks%2C%0Amatching%20proprietary%20agents%27%20performance%20and%20closing%20the%20capability%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWebSailor%253A%2520Navigating%2520Super-human%2520Reasoning%2520for%2520Web%2520Agent%26entry.906535625%3DKuan%2520Li%2520and%2520Zhongwang%2520Zhang%2520and%2520Huifeng%2520Yin%2520and%2520Liwen%2520Zhang%2520and%2520Litu%2520Ou%2520and%2520Jialong%2520Wu%2520and%2520Wenbiao%2520Yin%2520and%2520Baixuan%2520Li%2520and%2520Zhengwei%2520Tao%2520and%2520Xinyu%2520Wang%2520and%2520Weizhou%2520Shen%2520and%2520Junkai%2520Zhang%2520and%2520Dingchu%2520Zhang%2520and%2520Xixi%2520Wu%2520and%2520Yong%2520Jiang%2520and%2520Ming%2520Yan%2520and%2520Pengjun%2520Xie%2520and%2520Fei%2520Huang%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Transcending%2520human%2520cognitive%2520limitations%2520represents%2520a%2520critical%2520frontier%2520in%250ALLM%2520training.%2520Proprietary%2520agentic%2520systems%2520like%2520DeepResearch%2520have%2520demonstrated%250Asuperhuman%2520capabilities%2520on%2520extremely%2520complex%2520information-seeking%2520benchmarks%250Asuch%2520as%2520BrowseComp%252C%2520a%2520feat%2520previously%2520unattainable.%2520We%2520posit%2520that%2520their%2520success%250Ahinges%2520on%2520a%2520sophisticated%2520reasoning%2520pattern%2520absent%2520in%2520open-source%2520models%253A%2520the%250Aability%2520to%2520systematically%2520reduce%2520extreme%2520uncertainty%2520when%2520navigating%2520vast%250Ainformation%2520landscapes.%2520Based%2520on%2520this%2520insight%252C%2520we%2520introduce%2520WebSailor%252C%2520a%250Acomplete%2520post-training%2520methodology%2520designed%2520to%2520instill%2520this%2520crucial%2520capability.%250AOur%2520approach%2520involves%2520generating%2520novel%252C%2520high-uncertainty%2520tasks%2520through%250Astructured%2520sampling%2520and%2520information%2520obfuscation%252C%2520RFT%2520cold%2520start%252C%2520and%2520an%250Aefficient%2520agentic%2520RL%2520training%2520algorithm%252C%2520Duplicating%2520Sampling%2520Policy%250AOptimization%2520%2528DUPO%2529.%2520With%2520this%2520integrated%2520pipeline%252C%2520WebSailor%2520significantly%250Aoutperforms%2520all%2520opensource%2520agents%2520in%2520complex%2520information-seeking%2520tasks%252C%250Amatching%2520proprietary%2520agents%2527%2520performance%2520and%2520closing%2520the%2520capability%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WebSailor%3A%20Navigating%20Super-human%20Reasoning%20for%20Web%20Agent&entry.906535625=Kuan%20Li%20and%20Zhongwang%20Zhang%20and%20Huifeng%20Yin%20and%20Liwen%20Zhang%20and%20Litu%20Ou%20and%20Jialong%20Wu%20and%20Wenbiao%20Yin%20and%20Baixuan%20Li%20and%20Zhengwei%20Tao%20and%20Xinyu%20Wang%20and%20Weizhou%20Shen%20and%20Junkai%20Zhang%20and%20Dingchu%20Zhang%20and%20Xixi%20Wu%20and%20Yong%20Jiang%20and%20Ming%20Yan%20and%20Pengjun%20Xie%20and%20Fei%20Huang%20and%20Jingren%20Zhou&entry.1292438233=%20%20Transcending%20human%20cognitive%20limitations%20represents%20a%20critical%20frontier%20in%0ALLM%20training.%20Proprietary%20agentic%20systems%20like%20DeepResearch%20have%20demonstrated%0Asuperhuman%20capabilities%20on%20extremely%20complex%20information-seeking%20benchmarks%0Asuch%20as%20BrowseComp%2C%20a%20feat%20previously%20unattainable.%20We%20posit%20that%20their%20success%0Ahinges%20on%20a%20sophisticated%20reasoning%20pattern%20absent%20in%20open-source%20models%3A%20the%0Aability%20to%20systematically%20reduce%20extreme%20uncertainty%20when%20navigating%20vast%0Ainformation%20landscapes.%20Based%20on%20this%20insight%2C%20we%20introduce%20WebSailor%2C%20a%0Acomplete%20post-training%20methodology%20designed%20to%20instill%20this%20crucial%20capability.%0AOur%20approach%20involves%20generating%20novel%2C%20high-uncertainty%20tasks%20through%0Astructured%20sampling%20and%20information%20obfuscation%2C%20RFT%20cold%20start%2C%20and%20an%0Aefficient%20agentic%20RL%20training%20algorithm%2C%20Duplicating%20Sampling%20Policy%0AOptimization%20%28DUPO%29.%20With%20this%20integrated%20pipeline%2C%20WebSailor%20significantly%0Aoutperforms%20all%20opensource%20agents%20in%20complex%20information-seeking%20tasks%2C%0Amatching%20proprietary%20agents%27%20performance%20and%20closing%20the%20capability%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02592v1&entry.124074799=Read"},
{"title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image\n  Detection via Multimodal Large Language Models", "author": "Ziyin Zhou and Yunpeng Luo and Yuanchen Wu and Ke Sun and Jiayi Ji and Ke Yan and Shouhong Ding and Xiaoshuai Sun and Yunsheng Wu and Rongrong Ji", "abstract": "  The rapid development of AI-generated content (AIGC) technology has led to\nthe misuse of highly realistic AI-generated images (AIGI) in spreading\nmisinformation, posing a threat to public information security. Although\nexisting AIGI detection techniques are generally effective, they face two\nissues: 1) a lack of human-verifiable explanations, and 2) a lack of\ngeneralization in the latest generation technology. To address these issues, we\nintroduce a large-scale and comprehensive dataset, Holmes-Set, which includes\nthe Holmes-SFTSet, an instruction-tuning dataset with explanations on whether\nimages are AI-generated, and the Holmes-DPOSet, a human-aligned preference\ndataset. Our work introduces an efficient data annotation method called the\nMulti-Expert Jury, enhancing data generation through structured MLLM\nexplanations and quality control via cross-model evaluation, expert defect\nfiltering, and human preference modification. In addition, we propose Holmes\nPipeline, a meticulously designed three-stage training framework comprising\nvisual expert pre-training, supervised fine-tuning, and direct preference\noptimization. Holmes Pipeline adapts multimodal large language models (MLLMs)\nfor AIGI detection while generating human-verifiable and human-aligned\nexplanations, ultimately yielding our model AIGI-Holmes. During the inference\nstage, we introduce a collaborative decoding strategy that integrates the model\nperception of the visual expert with the semantic reasoning of MLLMs, further\nenhancing the generalization capabilities. Extensive experiments on three\nbenchmarks validate the effectiveness of our AIGI-Holmes.\n", "link": "http://arxiv.org/abs/2507.02664v1", "date": "2025-07-03", "relevancy": 2.1662, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5723}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5389}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIGI-Holmes%3A%20Towards%20Explainable%20and%20Generalizable%20AI-Generated%20Image%0A%20%20Detection%20via%20Multimodal%20Large%20Language%20Models&body=Title%3A%20AIGI-Holmes%3A%20Towards%20Explainable%20and%20Generalizable%20AI-Generated%20Image%0A%20%20Detection%20via%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Ziyin%20Zhou%20and%20Yunpeng%20Luo%20and%20Yuanchen%20Wu%20and%20Ke%20Sun%20and%20Jiayi%20Ji%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Xiaoshuai%20Sun%20and%20Yunsheng%20Wu%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20AI-generated%20content%20%28AIGC%29%20technology%20has%20led%20to%0Athe%20misuse%20of%20highly%20realistic%20AI-generated%20images%20%28AIGI%29%20in%20spreading%0Amisinformation%2C%20posing%20a%20threat%20to%20public%20information%20security.%20Although%0Aexisting%20AIGI%20detection%20techniques%20are%20generally%20effective%2C%20they%20face%20two%0Aissues%3A%201%29%20a%20lack%20of%20human-verifiable%20explanations%2C%20and%202%29%20a%20lack%20of%0Ageneralization%20in%20the%20latest%20generation%20technology.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20large-scale%20and%20comprehensive%20dataset%2C%20Holmes-Set%2C%20which%20includes%0Athe%20Holmes-SFTSet%2C%20an%20instruction-tuning%20dataset%20with%20explanations%20on%20whether%0Aimages%20are%20AI-generated%2C%20and%20the%20Holmes-DPOSet%2C%20a%20human-aligned%20preference%0Adataset.%20Our%20work%20introduces%20an%20efficient%20data%20annotation%20method%20called%20the%0AMulti-Expert%20Jury%2C%20enhancing%20data%20generation%20through%20structured%20MLLM%0Aexplanations%20and%20quality%20control%20via%20cross-model%20evaluation%2C%20expert%20defect%0Afiltering%2C%20and%20human%20preference%20modification.%20In%20addition%2C%20we%20propose%20Holmes%0APipeline%2C%20a%20meticulously%20designed%20three-stage%20training%20framework%20comprising%0Avisual%20expert%20pre-training%2C%20supervised%20fine-tuning%2C%20and%20direct%20preference%0Aoptimization.%20Holmes%20Pipeline%20adapts%20multimodal%20large%20language%20models%20%28MLLMs%29%0Afor%20AIGI%20detection%20while%20generating%20human-verifiable%20and%20human-aligned%0Aexplanations%2C%20ultimately%20yielding%20our%20model%20AIGI-Holmes.%20During%20the%20inference%0Astage%2C%20we%20introduce%20a%20collaborative%20decoding%20strategy%20that%20integrates%20the%20model%0Aperception%20of%20the%20visual%20expert%20with%20the%20semantic%20reasoning%20of%20MLLMs%2C%20further%0Aenhancing%20the%20generalization%20capabilities.%20Extensive%20experiments%20on%20three%0Abenchmarks%20validate%20the%20effectiveness%20of%20our%20AIGI-Holmes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIGI-Holmes%253A%2520Towards%2520Explainable%2520and%2520Generalizable%2520AI-Generated%2520Image%250A%2520%2520Detection%2520via%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DZiyin%2520Zhou%2520and%2520Yunpeng%2520Luo%2520and%2520Yuanchen%2520Wu%2520and%2520Ke%2520Sun%2520and%2520Jiayi%2520Ji%2520and%2520Ke%2520Yan%2520and%2520Shouhong%2520Ding%2520and%2520Xiaoshuai%2520Sun%2520and%2520Yunsheng%2520Wu%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520AI-generated%2520content%2520%2528AIGC%2529%2520technology%2520has%2520led%2520to%250Athe%2520misuse%2520of%2520highly%2520realistic%2520AI-generated%2520images%2520%2528AIGI%2529%2520in%2520spreading%250Amisinformation%252C%2520posing%2520a%2520threat%2520to%2520public%2520information%2520security.%2520Although%250Aexisting%2520AIGI%2520detection%2520techniques%2520are%2520generally%2520effective%252C%2520they%2520face%2520two%250Aissues%253A%25201%2529%2520a%2520lack%2520of%2520human-verifiable%2520explanations%252C%2520and%25202%2529%2520a%2520lack%2520of%250Ageneralization%2520in%2520the%2520latest%2520generation%2520technology.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520large-scale%2520and%2520comprehensive%2520dataset%252C%2520Holmes-Set%252C%2520which%2520includes%250Athe%2520Holmes-SFTSet%252C%2520an%2520instruction-tuning%2520dataset%2520with%2520explanations%2520on%2520whether%250Aimages%2520are%2520AI-generated%252C%2520and%2520the%2520Holmes-DPOSet%252C%2520a%2520human-aligned%2520preference%250Adataset.%2520Our%2520work%2520introduces%2520an%2520efficient%2520data%2520annotation%2520method%2520called%2520the%250AMulti-Expert%2520Jury%252C%2520enhancing%2520data%2520generation%2520through%2520structured%2520MLLM%250Aexplanations%2520and%2520quality%2520control%2520via%2520cross-model%2520evaluation%252C%2520expert%2520defect%250Afiltering%252C%2520and%2520human%2520preference%2520modification.%2520In%2520addition%252C%2520we%2520propose%2520Holmes%250APipeline%252C%2520a%2520meticulously%2520designed%2520three-stage%2520training%2520framework%2520comprising%250Avisual%2520expert%2520pre-training%252C%2520supervised%2520fine-tuning%252C%2520and%2520direct%2520preference%250Aoptimization.%2520Holmes%2520Pipeline%2520adapts%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Afor%2520AIGI%2520detection%2520while%2520generating%2520human-verifiable%2520and%2520human-aligned%250Aexplanations%252C%2520ultimately%2520yielding%2520our%2520model%2520AIGI-Holmes.%2520During%2520the%2520inference%250Astage%252C%2520we%2520introduce%2520a%2520collaborative%2520decoding%2520strategy%2520that%2520integrates%2520the%2520model%250Aperception%2520of%2520the%2520visual%2520expert%2520with%2520the%2520semantic%2520reasoning%2520of%2520MLLMs%252C%2520further%250Aenhancing%2520the%2520generalization%2520capabilities.%2520Extensive%2520experiments%2520on%2520three%250Abenchmarks%2520validate%2520the%2520effectiveness%2520of%2520our%2520AIGI-Holmes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIGI-Holmes%3A%20Towards%20Explainable%20and%20Generalizable%20AI-Generated%20Image%0A%20%20Detection%20via%20Multimodal%20Large%20Language%20Models&entry.906535625=Ziyin%20Zhou%20and%20Yunpeng%20Luo%20and%20Yuanchen%20Wu%20and%20Ke%20Sun%20and%20Jiayi%20Ji%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Xiaoshuai%20Sun%20and%20Yunsheng%20Wu%20and%20Rongrong%20Ji&entry.1292438233=%20%20The%20rapid%20development%20of%20AI-generated%20content%20%28AIGC%29%20technology%20has%20led%20to%0Athe%20misuse%20of%20highly%20realistic%20AI-generated%20images%20%28AIGI%29%20in%20spreading%0Amisinformation%2C%20posing%20a%20threat%20to%20public%20information%20security.%20Although%0Aexisting%20AIGI%20detection%20techniques%20are%20generally%20effective%2C%20they%20face%20two%0Aissues%3A%201%29%20a%20lack%20of%20human-verifiable%20explanations%2C%20and%202%29%20a%20lack%20of%0Ageneralization%20in%20the%20latest%20generation%20technology.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20large-scale%20and%20comprehensive%20dataset%2C%20Holmes-Set%2C%20which%20includes%0Athe%20Holmes-SFTSet%2C%20an%20instruction-tuning%20dataset%20with%20explanations%20on%20whether%0Aimages%20are%20AI-generated%2C%20and%20the%20Holmes-DPOSet%2C%20a%20human-aligned%20preference%0Adataset.%20Our%20work%20introduces%20an%20efficient%20data%20annotation%20method%20called%20the%0AMulti-Expert%20Jury%2C%20enhancing%20data%20generation%20through%20structured%20MLLM%0Aexplanations%20and%20quality%20control%20via%20cross-model%20evaluation%2C%20expert%20defect%0Afiltering%2C%20and%20human%20preference%20modification.%20In%20addition%2C%20we%20propose%20Holmes%0APipeline%2C%20a%20meticulously%20designed%20three-stage%20training%20framework%20comprising%0Avisual%20expert%20pre-training%2C%20supervised%20fine-tuning%2C%20and%20direct%20preference%0Aoptimization.%20Holmes%20Pipeline%20adapts%20multimodal%20large%20language%20models%20%28MLLMs%29%0Afor%20AIGI%20detection%20while%20generating%20human-verifiable%20and%20human-aligned%0Aexplanations%2C%20ultimately%20yielding%20our%20model%20AIGI-Holmes.%20During%20the%20inference%0Astage%2C%20we%20introduce%20a%20collaborative%20decoding%20strategy%20that%20integrates%20the%20model%0Aperception%20of%20the%20visual%20expert%20with%20the%20semantic%20reasoning%20of%20MLLMs%2C%20further%0Aenhancing%20the%20generalization%20capabilities.%20Extensive%20experiments%20on%20three%0Abenchmarks%20validate%20the%20effectiveness%20of%20our%20AIGI-Holmes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02664v1&entry.124074799=Read"},
{"title": "ClustOpt: A Clustering-based Approach for Representing and Visualizing\n  the Search Dynamics of Numerical Metaheuristic Optimization Algorithms", "author": "Gjorgjina Cenikj and Ga\u0161per Petelin and Tome Eftimov", "abstract": "  Understanding the behavior of numerical metaheuristic optimization algorithms\nis critical for advancing their development and application. Traditional\nvisualization techniques, such as convergence plots, trajectory mapping, and\nfitness landscape analysis, often fall short in illustrating the structural\ndynamics of the search process, especially in high-dimensional or complex\nsolution spaces. To address this, we propose a novel representation and\nvisualization methodology that clusters solution candidates explored by the\nalgorithm and tracks the evolution of cluster memberships across iterations,\noffering a dynamic and interpretable view of the search process. Additionally,\nwe introduce two metrics - algorithm stability and algorithm similarity- to\nquantify the consistency of search trajectories across runs of an individual\nalgorithm and the similarity between different algorithms, respectively. We\napply this methodology to a set of ten numerical metaheuristic algorithms,\nrevealing insights into their stability and comparative behaviors, thereby\nproviding a deeper understanding of their search dynamics.\n", "link": "http://arxiv.org/abs/2507.02337v1", "date": "2025-07-03", "relevancy": 1.4176, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4726}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClustOpt%3A%20A%20Clustering-based%20Approach%20for%20Representing%20and%20Visualizing%0A%20%20the%20Search%20Dynamics%20of%20Numerical%20Metaheuristic%20Optimization%20Algorithms&body=Title%3A%20ClustOpt%3A%20A%20Clustering-based%20Approach%20for%20Representing%20and%20Visualizing%0A%20%20the%20Search%20Dynamics%20of%20Numerical%20Metaheuristic%20Optimization%20Algorithms%0AAuthor%3A%20Gjorgjina%20Cenikj%20and%20Ga%C5%A1per%20Petelin%20and%20Tome%20Eftimov%0AAbstract%3A%20%20%20Understanding%20the%20behavior%20of%20numerical%20metaheuristic%20optimization%20algorithms%0Ais%20critical%20for%20advancing%20their%20development%20and%20application.%20Traditional%0Avisualization%20techniques%2C%20such%20as%20convergence%20plots%2C%20trajectory%20mapping%2C%20and%0Afitness%20landscape%20analysis%2C%20often%20fall%20short%20in%20illustrating%20the%20structural%0Adynamics%20of%20the%20search%20process%2C%20especially%20in%20high-dimensional%20or%20complex%0Asolution%20spaces.%20To%20address%20this%2C%20we%20propose%20a%20novel%20representation%20and%0Avisualization%20methodology%20that%20clusters%20solution%20candidates%20explored%20by%20the%0Aalgorithm%20and%20tracks%20the%20evolution%20of%20cluster%20memberships%20across%20iterations%2C%0Aoffering%20a%20dynamic%20and%20interpretable%20view%20of%20the%20search%20process.%20Additionally%2C%0Awe%20introduce%20two%20metrics%20-%20algorithm%20stability%20and%20algorithm%20similarity-%20to%0Aquantify%20the%20consistency%20of%20search%20trajectories%20across%20runs%20of%20an%20individual%0Aalgorithm%20and%20the%20similarity%20between%20different%20algorithms%2C%20respectively.%20We%0Aapply%20this%20methodology%20to%20a%20set%20of%20ten%20numerical%20metaheuristic%20algorithms%2C%0Arevealing%20insights%20into%20their%20stability%20and%20comparative%20behaviors%2C%20thereby%0Aproviding%20a%20deeper%20understanding%20of%20their%20search%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustOpt%253A%2520A%2520Clustering-based%2520Approach%2520for%2520Representing%2520and%2520Visualizing%250A%2520%2520the%2520Search%2520Dynamics%2520of%2520Numerical%2520Metaheuristic%2520Optimization%2520Algorithms%26entry.906535625%3DGjorgjina%2520Cenikj%2520and%2520Ga%25C5%25A1per%2520Petelin%2520and%2520Tome%2520Eftimov%26entry.1292438233%3D%2520%2520Understanding%2520the%2520behavior%2520of%2520numerical%2520metaheuristic%2520optimization%2520algorithms%250Ais%2520critical%2520for%2520advancing%2520their%2520development%2520and%2520application.%2520Traditional%250Avisualization%2520techniques%252C%2520such%2520as%2520convergence%2520plots%252C%2520trajectory%2520mapping%252C%2520and%250Afitness%2520landscape%2520analysis%252C%2520often%2520fall%2520short%2520in%2520illustrating%2520the%2520structural%250Adynamics%2520of%2520the%2520search%2520process%252C%2520especially%2520in%2520high-dimensional%2520or%2520complex%250Asolution%2520spaces.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520representation%2520and%250Avisualization%2520methodology%2520that%2520clusters%2520solution%2520candidates%2520explored%2520by%2520the%250Aalgorithm%2520and%2520tracks%2520the%2520evolution%2520of%2520cluster%2520memberships%2520across%2520iterations%252C%250Aoffering%2520a%2520dynamic%2520and%2520interpretable%2520view%2520of%2520the%2520search%2520process.%2520Additionally%252C%250Awe%2520introduce%2520two%2520metrics%2520-%2520algorithm%2520stability%2520and%2520algorithm%2520similarity-%2520to%250Aquantify%2520the%2520consistency%2520of%2520search%2520trajectories%2520across%2520runs%2520of%2520an%2520individual%250Aalgorithm%2520and%2520the%2520similarity%2520between%2520different%2520algorithms%252C%2520respectively.%2520We%250Aapply%2520this%2520methodology%2520to%2520a%2520set%2520of%2520ten%2520numerical%2520metaheuristic%2520algorithms%252C%250Arevealing%2520insights%2520into%2520their%2520stability%2520and%2520comparative%2520behaviors%252C%2520thereby%250Aproviding%2520a%2520deeper%2520understanding%2520of%2520their%2520search%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClustOpt%3A%20A%20Clustering-based%20Approach%20for%20Representing%20and%20Visualizing%0A%20%20the%20Search%20Dynamics%20of%20Numerical%20Metaheuristic%20Optimization%20Algorithms&entry.906535625=Gjorgjina%20Cenikj%20and%20Ga%C5%A1per%20Petelin%20and%20Tome%20Eftimov&entry.1292438233=%20%20Understanding%20the%20behavior%20of%20numerical%20metaheuristic%20optimization%20algorithms%0Ais%20critical%20for%20advancing%20their%20development%20and%20application.%20Traditional%0Avisualization%20techniques%2C%20such%20as%20convergence%20plots%2C%20trajectory%20mapping%2C%20and%0Afitness%20landscape%20analysis%2C%20often%20fall%20short%20in%20illustrating%20the%20structural%0Adynamics%20of%20the%20search%20process%2C%20especially%20in%20high-dimensional%20or%20complex%0Asolution%20spaces.%20To%20address%20this%2C%20we%20propose%20a%20novel%20representation%20and%0Avisualization%20methodology%20that%20clusters%20solution%20candidates%20explored%20by%20the%0Aalgorithm%20and%20tracks%20the%20evolution%20of%20cluster%20memberships%20across%20iterations%2C%0Aoffering%20a%20dynamic%20and%20interpretable%20view%20of%20the%20search%20process.%20Additionally%2C%0Awe%20introduce%20two%20metrics%20-%20algorithm%20stability%20and%20algorithm%20similarity-%20to%0Aquantify%20the%20consistency%20of%20search%20trajectories%20across%20runs%20of%20an%20individual%0Aalgorithm%20and%20the%20similarity%20between%20different%20algorithms%2C%20respectively.%20We%0Aapply%20this%20methodology%20to%20a%20set%20of%20ten%20numerical%20metaheuristic%20algorithms%2C%0Arevealing%20insights%20into%20their%20stability%20and%20comparative%20behaviors%2C%20thereby%0Aproviding%20a%20deeper%20understanding%20of%20their%20search%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02337v1&entry.124074799=Read"},
{"title": "From Pixels to Damage Severity: Estimating Earthquake Impacts Using\n  Semantic Segmentation of Social Media Images", "author": "Danrong Zhang and Huili Huang and N. Simrill Smith and Nimisha Roy and J. David Frost", "abstract": "  In the aftermath of earthquakes, social media images have become a crucial\nresource for disaster reconnaissance, providing immediate insights into the\nextent of damage. Traditional approaches to damage severity assessment in\npost-earthquake social media images often rely on classification methods, which\nare inherently subjective and incapable of accounting for the varying extents\nof damage within an image. Addressing these limitations, this study proposes a\nnovel approach by framing damage severity assessment as a semantic segmentation\nproblem, aiming for a more objective analysis of damage in earthquake-affected\nareas. The methodology involves the construction of a segmented damage severity\ndataset, categorizing damage into three degrees: undamaged structures, damaged\nstructures, and debris. Utilizing this dataset, the study fine-tunes a\nSegFormer model to generate damage severity segmentations for post-earthquake\nsocial media images. Furthermore, a new damage severity scoring system is\nintroduced, quantifying damage by considering the varying degrees of damage\nacross different areas within images, adjusted for depth estimation. The\napplication of this approach allows for the quantification of damage severity\nin social media images in a more objective and comprehensive manner. By\nproviding a nuanced understanding of damage, this study enhances the ability to\noffer precise guidance to disaster reconnaissance teams, facilitating more\neffective and targeted response efforts in the aftermath of earthquakes.\n", "link": "http://arxiv.org/abs/2507.02781v1", "date": "2025-07-03", "relevancy": 1.4555, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5147}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Pixels%20to%20Damage%20Severity%3A%20Estimating%20Earthquake%20Impacts%20Using%0A%20%20Semantic%20Segmentation%20of%20Social%20Media%20Images&body=Title%3A%20From%20Pixels%20to%20Damage%20Severity%3A%20Estimating%20Earthquake%20Impacts%20Using%0A%20%20Semantic%20Segmentation%20of%20Social%20Media%20Images%0AAuthor%3A%20Danrong%20Zhang%20and%20Huili%20Huang%20and%20N.%20Simrill%20Smith%20and%20Nimisha%20Roy%20and%20J.%20David%20Frost%0AAbstract%3A%20%20%20In%20the%20aftermath%20of%20earthquakes%2C%20social%20media%20images%20have%20become%20a%20crucial%0Aresource%20for%20disaster%20reconnaissance%2C%20providing%20immediate%20insights%20into%20the%0Aextent%20of%20damage.%20Traditional%20approaches%20to%20damage%20severity%20assessment%20in%0Apost-earthquake%20social%20media%20images%20often%20rely%20on%20classification%20methods%2C%20which%0Aare%20inherently%20subjective%20and%20incapable%20of%20accounting%20for%20the%20varying%20extents%0Aof%20damage%20within%20an%20image.%20Addressing%20these%20limitations%2C%20this%20study%20proposes%20a%0Anovel%20approach%20by%20framing%20damage%20severity%20assessment%20as%20a%20semantic%20segmentation%0Aproblem%2C%20aiming%20for%20a%20more%20objective%20analysis%20of%20damage%20in%20earthquake-affected%0Aareas.%20The%20methodology%20involves%20the%20construction%20of%20a%20segmented%20damage%20severity%0Adataset%2C%20categorizing%20damage%20into%20three%20degrees%3A%20undamaged%20structures%2C%20damaged%0Astructures%2C%20and%20debris.%20Utilizing%20this%20dataset%2C%20the%20study%20fine-tunes%20a%0ASegFormer%20model%20to%20generate%20damage%20severity%20segmentations%20for%20post-earthquake%0Asocial%20media%20images.%20Furthermore%2C%20a%20new%20damage%20severity%20scoring%20system%20is%0Aintroduced%2C%20quantifying%20damage%20by%20considering%20the%20varying%20degrees%20of%20damage%0Aacross%20different%20areas%20within%20images%2C%20adjusted%20for%20depth%20estimation.%20The%0Aapplication%20of%20this%20approach%20allows%20for%20the%20quantification%20of%20damage%20severity%0Ain%20social%20media%20images%20in%20a%20more%20objective%20and%20comprehensive%20manner.%20By%0Aproviding%20a%20nuanced%20understanding%20of%20damage%2C%20this%20study%20enhances%20the%20ability%20to%0Aoffer%20precise%20guidance%20to%20disaster%20reconnaissance%20teams%2C%20facilitating%20more%0Aeffective%20and%20targeted%20response%20efforts%20in%20the%20aftermath%20of%20earthquakes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Pixels%2520to%2520Damage%2520Severity%253A%2520Estimating%2520Earthquake%2520Impacts%2520Using%250A%2520%2520Semantic%2520Segmentation%2520of%2520Social%2520Media%2520Images%26entry.906535625%3DDanrong%2520Zhang%2520and%2520Huili%2520Huang%2520and%2520N.%2520Simrill%2520Smith%2520and%2520Nimisha%2520Roy%2520and%2520J.%2520David%2520Frost%26entry.1292438233%3D%2520%2520In%2520the%2520aftermath%2520of%2520earthquakes%252C%2520social%2520media%2520images%2520have%2520become%2520a%2520crucial%250Aresource%2520for%2520disaster%2520reconnaissance%252C%2520providing%2520immediate%2520insights%2520into%2520the%250Aextent%2520of%2520damage.%2520Traditional%2520approaches%2520to%2520damage%2520severity%2520assessment%2520in%250Apost-earthquake%2520social%2520media%2520images%2520often%2520rely%2520on%2520classification%2520methods%252C%2520which%250Aare%2520inherently%2520subjective%2520and%2520incapable%2520of%2520accounting%2520for%2520the%2520varying%2520extents%250Aof%2520damage%2520within%2520an%2520image.%2520Addressing%2520these%2520limitations%252C%2520this%2520study%2520proposes%2520a%250Anovel%2520approach%2520by%2520framing%2520damage%2520severity%2520assessment%2520as%2520a%2520semantic%2520segmentation%250Aproblem%252C%2520aiming%2520for%2520a%2520more%2520objective%2520analysis%2520of%2520damage%2520in%2520earthquake-affected%250Aareas.%2520The%2520methodology%2520involves%2520the%2520construction%2520of%2520a%2520segmented%2520damage%2520severity%250Adataset%252C%2520categorizing%2520damage%2520into%2520three%2520degrees%253A%2520undamaged%2520structures%252C%2520damaged%250Astructures%252C%2520and%2520debris.%2520Utilizing%2520this%2520dataset%252C%2520the%2520study%2520fine-tunes%2520a%250ASegFormer%2520model%2520to%2520generate%2520damage%2520severity%2520segmentations%2520for%2520post-earthquake%250Asocial%2520media%2520images.%2520Furthermore%252C%2520a%2520new%2520damage%2520severity%2520scoring%2520system%2520is%250Aintroduced%252C%2520quantifying%2520damage%2520by%2520considering%2520the%2520varying%2520degrees%2520of%2520damage%250Aacross%2520different%2520areas%2520within%2520images%252C%2520adjusted%2520for%2520depth%2520estimation.%2520The%250Aapplication%2520of%2520this%2520approach%2520allows%2520for%2520the%2520quantification%2520of%2520damage%2520severity%250Ain%2520social%2520media%2520images%2520in%2520a%2520more%2520objective%2520and%2520comprehensive%2520manner.%2520By%250Aproviding%2520a%2520nuanced%2520understanding%2520of%2520damage%252C%2520this%2520study%2520enhances%2520the%2520ability%2520to%250Aoffer%2520precise%2520guidance%2520to%2520disaster%2520reconnaissance%2520teams%252C%2520facilitating%2520more%250Aeffective%2520and%2520targeted%2520response%2520efforts%2520in%2520the%2520aftermath%2520of%2520earthquakes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Pixels%20to%20Damage%20Severity%3A%20Estimating%20Earthquake%20Impacts%20Using%0A%20%20Semantic%20Segmentation%20of%20Social%20Media%20Images&entry.906535625=Danrong%20Zhang%20and%20Huili%20Huang%20and%20N.%20Simrill%20Smith%20and%20Nimisha%20Roy%20and%20J.%20David%20Frost&entry.1292438233=%20%20In%20the%20aftermath%20of%20earthquakes%2C%20social%20media%20images%20have%20become%20a%20crucial%0Aresource%20for%20disaster%20reconnaissance%2C%20providing%20immediate%20insights%20into%20the%0Aextent%20of%20damage.%20Traditional%20approaches%20to%20damage%20severity%20assessment%20in%0Apost-earthquake%20social%20media%20images%20often%20rely%20on%20classification%20methods%2C%20which%0Aare%20inherently%20subjective%20and%20incapable%20of%20accounting%20for%20the%20varying%20extents%0Aof%20damage%20within%20an%20image.%20Addressing%20these%20limitations%2C%20this%20study%20proposes%20a%0Anovel%20approach%20by%20framing%20damage%20severity%20assessment%20as%20a%20semantic%20segmentation%0Aproblem%2C%20aiming%20for%20a%20more%20objective%20analysis%20of%20damage%20in%20earthquake-affected%0Aareas.%20The%20methodology%20involves%20the%20construction%20of%20a%20segmented%20damage%20severity%0Adataset%2C%20categorizing%20damage%20into%20three%20degrees%3A%20undamaged%20structures%2C%20damaged%0Astructures%2C%20and%20debris.%20Utilizing%20this%20dataset%2C%20the%20study%20fine-tunes%20a%0ASegFormer%20model%20to%20generate%20damage%20severity%20segmentations%20for%20post-earthquake%0Asocial%20media%20images.%20Furthermore%2C%20a%20new%20damage%20severity%20scoring%20system%20is%0Aintroduced%2C%20quantifying%20damage%20by%20considering%20the%20varying%20degrees%20of%20damage%0Aacross%20different%20areas%20within%20images%2C%20adjusted%20for%20depth%20estimation.%20The%0Aapplication%20of%20this%20approach%20allows%20for%20the%20quantification%20of%20damage%20severity%0Ain%20social%20media%20images%20in%20a%20more%20objective%20and%20comprehensive%20manner.%20By%0Aproviding%20a%20nuanced%20understanding%20of%20damage%2C%20this%20study%20enhances%20the%20ability%20to%0Aoffer%20precise%20guidance%20to%20disaster%20reconnaissance%20teams%2C%20facilitating%20more%0Aeffective%20and%20targeted%20response%20efforts%20in%20the%20aftermath%20of%20earthquakes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02781v1&entry.124074799=Read"},
{"title": "Learning to Coordinate Bidders in Non-Truthful Auctions", "author": "Hu Fu and Tao Lin", "abstract": "  In non-truthful auctions such as first-price and all-pay auctions, the\nindependent strategic behaviors of bidders, with the corresponding equilibrium\nnotion -- Bayes Nash equilibria -- are notoriously difficult to characterize\nand can cause undesirable outcomes. An alternative approach to designing better\nauction systems is to coordinate the bidders: let a mediator make\nincentive-compatible recommendations of correlated bidding strategies to the\nbidders, namely, implementing a Bayes correlated equilibrium (BCE). The\nimplementation of BCE, however, requires knowledge of the distribution of\nbidders' private valuations, which is often unavailable. We initiate the study\nof the sample complexity of learning Bayes correlated equilibria in\nnon-truthful auctions. We prove that the BCEs in a large class of non-truthful\nauctions, including first-price and all-pay auctions, can be learned with a\npolynomial number $\\tilde O(\\frac{n}{\\varepsilon^2})$ of samples from the\nbidders' value distributions. Our technique is a reduction to the problem of\nestimating bidders' expected utility from samples, combined with an analysis of\nthe pseudo-dimension of the class of all monotone bidding strategies of\nbidders.\n", "link": "http://arxiv.org/abs/2507.02801v1", "date": "2025-07-03", "relevancy": 1.6288, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4231}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4061}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Coordinate%20Bidders%20in%20Non-Truthful%20Auctions&body=Title%3A%20Learning%20to%20Coordinate%20Bidders%20in%20Non-Truthful%20Auctions%0AAuthor%3A%20Hu%20Fu%20and%20Tao%20Lin%0AAbstract%3A%20%20%20In%20non-truthful%20auctions%20such%20as%20first-price%20and%20all-pay%20auctions%2C%20the%0Aindependent%20strategic%20behaviors%20of%20bidders%2C%20with%20the%20corresponding%20equilibrium%0Anotion%20--%20Bayes%20Nash%20equilibria%20--%20are%20notoriously%20difficult%20to%20characterize%0Aand%20can%20cause%20undesirable%20outcomes.%20An%20alternative%20approach%20to%20designing%20better%0Aauction%20systems%20is%20to%20coordinate%20the%20bidders%3A%20let%20a%20mediator%20make%0Aincentive-compatible%20recommendations%20of%20correlated%20bidding%20strategies%20to%20the%0Abidders%2C%20namely%2C%20implementing%20a%20Bayes%20correlated%20equilibrium%20%28BCE%29.%20The%0Aimplementation%20of%20BCE%2C%20however%2C%20requires%20knowledge%20of%20the%20distribution%20of%0Abidders%27%20private%20valuations%2C%20which%20is%20often%20unavailable.%20We%20initiate%20the%20study%0Aof%20the%20sample%20complexity%20of%20learning%20Bayes%20correlated%20equilibria%20in%0Anon-truthful%20auctions.%20We%20prove%20that%20the%20BCEs%20in%20a%20large%20class%20of%20non-truthful%0Aauctions%2C%20including%20first-price%20and%20all-pay%20auctions%2C%20can%20be%20learned%20with%20a%0Apolynomial%20number%20%24%5Ctilde%20O%28%5Cfrac%7Bn%7D%7B%5Cvarepsilon%5E2%7D%29%24%20of%20samples%20from%20the%0Abidders%27%20value%20distributions.%20Our%20technique%20is%20a%20reduction%20to%20the%20problem%20of%0Aestimating%20bidders%27%20expected%20utility%20from%20samples%2C%20combined%20with%20an%20analysis%20of%0Athe%20pseudo-dimension%20of%20the%20class%20of%20all%20monotone%20bidding%20strategies%20of%0Abidders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Coordinate%2520Bidders%2520in%2520Non-Truthful%2520Auctions%26entry.906535625%3DHu%2520Fu%2520and%2520Tao%2520Lin%26entry.1292438233%3D%2520%2520In%2520non-truthful%2520auctions%2520such%2520as%2520first-price%2520and%2520all-pay%2520auctions%252C%2520the%250Aindependent%2520strategic%2520behaviors%2520of%2520bidders%252C%2520with%2520the%2520corresponding%2520equilibrium%250Anotion%2520--%2520Bayes%2520Nash%2520equilibria%2520--%2520are%2520notoriously%2520difficult%2520to%2520characterize%250Aand%2520can%2520cause%2520undesirable%2520outcomes.%2520An%2520alternative%2520approach%2520to%2520designing%2520better%250Aauction%2520systems%2520is%2520to%2520coordinate%2520the%2520bidders%253A%2520let%2520a%2520mediator%2520make%250Aincentive-compatible%2520recommendations%2520of%2520correlated%2520bidding%2520strategies%2520to%2520the%250Abidders%252C%2520namely%252C%2520implementing%2520a%2520Bayes%2520correlated%2520equilibrium%2520%2528BCE%2529.%2520The%250Aimplementation%2520of%2520BCE%252C%2520however%252C%2520requires%2520knowledge%2520of%2520the%2520distribution%2520of%250Abidders%2527%2520private%2520valuations%252C%2520which%2520is%2520often%2520unavailable.%2520We%2520initiate%2520the%2520study%250Aof%2520the%2520sample%2520complexity%2520of%2520learning%2520Bayes%2520correlated%2520equilibria%2520in%250Anon-truthful%2520auctions.%2520We%2520prove%2520that%2520the%2520BCEs%2520in%2520a%2520large%2520class%2520of%2520non-truthful%250Aauctions%252C%2520including%2520first-price%2520and%2520all-pay%2520auctions%252C%2520can%2520be%2520learned%2520with%2520a%250Apolynomial%2520number%2520%2524%255Ctilde%2520O%2528%255Cfrac%257Bn%257D%257B%255Cvarepsilon%255E2%257D%2529%2524%2520of%2520samples%2520from%2520the%250Abidders%2527%2520value%2520distributions.%2520Our%2520technique%2520is%2520a%2520reduction%2520to%2520the%2520problem%2520of%250Aestimating%2520bidders%2527%2520expected%2520utility%2520from%2520samples%252C%2520combined%2520with%2520an%2520analysis%2520of%250Athe%2520pseudo-dimension%2520of%2520the%2520class%2520of%2520all%2520monotone%2520bidding%2520strategies%2520of%250Abidders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Coordinate%20Bidders%20in%20Non-Truthful%20Auctions&entry.906535625=Hu%20Fu%20and%20Tao%20Lin&entry.1292438233=%20%20In%20non-truthful%20auctions%20such%20as%20first-price%20and%20all-pay%20auctions%2C%20the%0Aindependent%20strategic%20behaviors%20of%20bidders%2C%20with%20the%20corresponding%20equilibrium%0Anotion%20--%20Bayes%20Nash%20equilibria%20--%20are%20notoriously%20difficult%20to%20characterize%0Aand%20can%20cause%20undesirable%20outcomes.%20An%20alternative%20approach%20to%20designing%20better%0Aauction%20systems%20is%20to%20coordinate%20the%20bidders%3A%20let%20a%20mediator%20make%0Aincentive-compatible%20recommendations%20of%20correlated%20bidding%20strategies%20to%20the%0Abidders%2C%20namely%2C%20implementing%20a%20Bayes%20correlated%20equilibrium%20%28BCE%29.%20The%0Aimplementation%20of%20BCE%2C%20however%2C%20requires%20knowledge%20of%20the%20distribution%20of%0Abidders%27%20private%20valuations%2C%20which%20is%20often%20unavailable.%20We%20initiate%20the%20study%0Aof%20the%20sample%20complexity%20of%20learning%20Bayes%20correlated%20equilibria%20in%0Anon-truthful%20auctions.%20We%20prove%20that%20the%20BCEs%20in%20a%20large%20class%20of%20non-truthful%0Aauctions%2C%20including%20first-price%20and%20all-pay%20auctions%2C%20can%20be%20learned%20with%20a%0Apolynomial%20number%20%24%5Ctilde%20O%28%5Cfrac%7Bn%7D%7B%5Cvarepsilon%5E2%7D%29%24%20of%20samples%20from%20the%0Abidders%27%20value%20distributions.%20Our%20technique%20is%20a%20reduction%20to%20the%20problem%20of%0Aestimating%20bidders%27%20expected%20utility%20from%20samples%2C%20combined%20with%20an%20analysis%20of%0Athe%20pseudo-dimension%20of%20the%20class%20of%20all%20monotone%20bidding%20strategies%20of%0Abidders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02801v1&entry.124074799=Read"},
{"title": "Solving the Hubbard model with Neural Quantum States", "author": "Yuntian Gu and Wenrui Li and Heng Lin and Bo Zhan and Ruichen Li and Yifei Huang and Di He and Yantao Wu and Tao Xiang and Mingpu Qin and Liwei Wang and Dingshun Lv", "abstract": "  The rapid development of neural quantum states (NQS) has established it as a\npromising framework for studying quantum many-body systems. In this work, by\nleveraging the cutting-edge transformer-based architectures and developing\nhighly efficient optimization algorithms, we achieve the state-of-the-art\nresults for the doped two-dimensional (2D) Hubbard model, arguably the minimum\nmodel for high-Tc superconductivity. Interestingly, we find different attention\nheads in the NQS ansatz can directly encode correlations at different scales,\nmaking it capable of capturing long-range correlations and entanglements in\nstrongly correlated systems. With these advances, we establish the half-filled\nstripe in the ground state of 2D Hubbard model with the next nearest\nneighboring hoppings, consistent with experimental observations in cuprates.\nOur work establishes NQS as a powerful tool for solving challenging\nmany-fermions systems.\n", "link": "http://arxiv.org/abs/2507.02644v1", "date": "2025-07-03", "relevancy": 0.8497, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4139}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20the%20Hubbard%20model%20with%20Neural%20Quantum%20States&body=Title%3A%20Solving%20the%20Hubbard%20model%20with%20Neural%20Quantum%20States%0AAuthor%3A%20Yuntian%20Gu%20and%20Wenrui%20Li%20and%20Heng%20Lin%20and%20Bo%20Zhan%20and%20Ruichen%20Li%20and%20Yifei%20Huang%20and%20Di%20He%20and%20Yantao%20Wu%20and%20Tao%20Xiang%20and%20Mingpu%20Qin%20and%20Liwei%20Wang%20and%20Dingshun%20Lv%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20neural%20quantum%20states%20%28NQS%29%20has%20established%20it%20as%20a%0Apromising%20framework%20for%20studying%20quantum%20many-body%20systems.%20In%20this%20work%2C%20by%0Aleveraging%20the%20cutting-edge%20transformer-based%20architectures%20and%20developing%0Ahighly%20efficient%20optimization%20algorithms%2C%20we%20achieve%20the%20state-of-the-art%0Aresults%20for%20the%20doped%20two-dimensional%20%282D%29%20Hubbard%20model%2C%20arguably%20the%20minimum%0Amodel%20for%20high-Tc%20superconductivity.%20Interestingly%2C%20we%20find%20different%20attention%0Aheads%20in%20the%20NQS%20ansatz%20can%20directly%20encode%20correlations%20at%20different%20scales%2C%0Amaking%20it%20capable%20of%20capturing%20long-range%20correlations%20and%20entanglements%20in%0Astrongly%20correlated%20systems.%20With%20these%20advances%2C%20we%20establish%20the%20half-filled%0Astripe%20in%20the%20ground%20state%20of%202D%20Hubbard%20model%20with%20the%20next%20nearest%0Aneighboring%20hoppings%2C%20consistent%20with%20experimental%20observations%20in%20cuprates.%0AOur%20work%20establishes%20NQS%20as%20a%20powerful%20tool%20for%20solving%20challenging%0Amany-fermions%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520the%2520Hubbard%2520model%2520with%2520Neural%2520Quantum%2520States%26entry.906535625%3DYuntian%2520Gu%2520and%2520Wenrui%2520Li%2520and%2520Heng%2520Lin%2520and%2520Bo%2520Zhan%2520and%2520Ruichen%2520Li%2520and%2520Yifei%2520Huang%2520and%2520Di%2520He%2520and%2520Yantao%2520Wu%2520and%2520Tao%2520Xiang%2520and%2520Mingpu%2520Qin%2520and%2520Liwei%2520Wang%2520and%2520Dingshun%2520Lv%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520neural%2520quantum%2520states%2520%2528NQS%2529%2520has%2520established%2520it%2520as%2520a%250Apromising%2520framework%2520for%2520studying%2520quantum%2520many-body%2520systems.%2520In%2520this%2520work%252C%2520by%250Aleveraging%2520the%2520cutting-edge%2520transformer-based%2520architectures%2520and%2520developing%250Ahighly%2520efficient%2520optimization%2520algorithms%252C%2520we%2520achieve%2520the%2520state-of-the-art%250Aresults%2520for%2520the%2520doped%2520two-dimensional%2520%25282D%2529%2520Hubbard%2520model%252C%2520arguably%2520the%2520minimum%250Amodel%2520for%2520high-Tc%2520superconductivity.%2520Interestingly%252C%2520we%2520find%2520different%2520attention%250Aheads%2520in%2520the%2520NQS%2520ansatz%2520can%2520directly%2520encode%2520correlations%2520at%2520different%2520scales%252C%250Amaking%2520it%2520capable%2520of%2520capturing%2520long-range%2520correlations%2520and%2520entanglements%2520in%250Astrongly%2520correlated%2520systems.%2520With%2520these%2520advances%252C%2520we%2520establish%2520the%2520half-filled%250Astripe%2520in%2520the%2520ground%2520state%2520of%25202D%2520Hubbard%2520model%2520with%2520the%2520next%2520nearest%250Aneighboring%2520hoppings%252C%2520consistent%2520with%2520experimental%2520observations%2520in%2520cuprates.%250AOur%2520work%2520establishes%2520NQS%2520as%2520a%2520powerful%2520tool%2520for%2520solving%2520challenging%250Amany-fermions%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20Hubbard%20model%20with%20Neural%20Quantum%20States&entry.906535625=Yuntian%20Gu%20and%20Wenrui%20Li%20and%20Heng%20Lin%20and%20Bo%20Zhan%20and%20Ruichen%20Li%20and%20Yifei%20Huang%20and%20Di%20He%20and%20Yantao%20Wu%20and%20Tao%20Xiang%20and%20Mingpu%20Qin%20and%20Liwei%20Wang%20and%20Dingshun%20Lv&entry.1292438233=%20%20The%20rapid%20development%20of%20neural%20quantum%20states%20%28NQS%29%20has%20established%20it%20as%20a%0Apromising%20framework%20for%20studying%20quantum%20many-body%20systems.%20In%20this%20work%2C%20by%0Aleveraging%20the%20cutting-edge%20transformer-based%20architectures%20and%20developing%0Ahighly%20efficient%20optimization%20algorithms%2C%20we%20achieve%20the%20state-of-the-art%0Aresults%20for%20the%20doped%20two-dimensional%20%282D%29%20Hubbard%20model%2C%20arguably%20the%20minimum%0Amodel%20for%20high-Tc%20superconductivity.%20Interestingly%2C%20we%20find%20different%20attention%0Aheads%20in%20the%20NQS%20ansatz%20can%20directly%20encode%20correlations%20at%20different%20scales%2C%0Amaking%20it%20capable%20of%20capturing%20long-range%20correlations%20and%20entanglements%20in%0Astrongly%20correlated%20systems.%20With%20these%20advances%2C%20we%20establish%20the%20half-filled%0Astripe%20in%20the%20ground%20state%20of%202D%20Hubbard%20model%20with%20the%20next%20nearest%0Aneighboring%20hoppings%2C%20consistent%20with%20experimental%20observations%20in%20cuprates.%0AOur%20work%20establishes%20NQS%20as%20a%20powerful%20tool%20for%20solving%20challenging%0Amany-fermions%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02644v1&entry.124074799=Read"},
{"title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving", "author": "Matthieu Zimmer and Xiaotong Ji and Rasul Tutunov and Anthony Bordg and Jun Wang and Haitham Bou Ammar", "abstract": "  Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.\n", "link": "http://arxiv.org/abs/2507.02726v1", "date": "2025-07-03", "relevancy": 0.9961, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5163}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bourbaki%3A%20Self-Generated%20and%20Goal-Conditioned%20MDPs%20for%20Theorem%20Proving&body=Title%3A%20Bourbaki%3A%20Self-Generated%20and%20Goal-Conditioned%20MDPs%20for%20Theorem%20Proving%0AAuthor%3A%20Matthieu%20Zimmer%20and%20Xiaotong%20Ji%20and%20Rasul%20Tutunov%20and%20Anthony%20Bordg%20and%20Jun%20Wang%20and%20Haitham%20Bou%20Ammar%0AAbstract%3A%20%20%20Reasoning%20remains%20a%20challenging%20task%20for%20large%20language%20models%20%28LLMs%29%2C%0Aespecially%20within%20the%20logically%20constrained%20environment%20of%20automated%20theorem%0Aproving%20%28ATP%29%2C%20due%20to%20sparse%20rewards%20and%20the%20vast%20scale%20of%20proofs.%20These%0Achallenges%20are%20amplified%20in%20benchmarks%20like%20PutnamBench%2C%20which%20contains%0Auniversity-level%20problems%20requiring%20complex%2C%20multi-step%20reasoning.%20To%20address%0Athis%2C%20we%20introduce%20self-generated%20goal-conditioned%20MDPs%20%28sG-MDPs%29%2C%20a%20new%0Aframework%20in%20which%20agents%20generate%20and%20pursue%20their%20subgoals%20based%20on%20the%0Aevolving%20proof%20state.%20Given%20this%20more%20structured%20generation%20of%20goals%2C%20the%0Aresulting%20problem%20becomes%20more%20amenable%20to%20search.%20We%20then%20apply%20Monte%20Carlo%0ATree%20Search%20%28MCTS%29-like%20algorithms%20to%20solve%20the%20sG-MDP%2C%20instantiating%20our%0Aapproach%20in%20Bourbaki%20%287B%29%2C%20a%20modular%20system%20that%20can%20ensemble%20multiple%207B%20LLMs%0Afor%20subgoal%20generation%20and%20tactic%20synthesis.%20On%20PutnamBench%2C%20Bourbaki%20%287B%29%0Asolves%2026%20problems%2C%20achieving%20new%20state-of-the-art%20results%20with%20models%20at%20this%0Ascale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBourbaki%253A%2520Self-Generated%2520and%2520Goal-Conditioned%2520MDPs%2520for%2520Theorem%2520Proving%26entry.906535625%3DMatthieu%2520Zimmer%2520and%2520Xiaotong%2520Ji%2520and%2520Rasul%2520Tutunov%2520and%2520Anthony%2520Bordg%2520and%2520Jun%2520Wang%2520and%2520Haitham%2520Bou%2520Ammar%26entry.1292438233%3D%2520%2520Reasoning%2520remains%2520a%2520challenging%2520task%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Aespecially%2520within%2520the%2520logically%2520constrained%2520environment%2520of%2520automated%2520theorem%250Aproving%2520%2528ATP%2529%252C%2520due%2520to%2520sparse%2520rewards%2520and%2520the%2520vast%2520scale%2520of%2520proofs.%2520These%250Achallenges%2520are%2520amplified%2520in%2520benchmarks%2520like%2520PutnamBench%252C%2520which%2520contains%250Auniversity-level%2520problems%2520requiring%2520complex%252C%2520multi-step%2520reasoning.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520self-generated%2520goal-conditioned%2520MDPs%2520%2528sG-MDPs%2529%252C%2520a%2520new%250Aframework%2520in%2520which%2520agents%2520generate%2520and%2520pursue%2520their%2520subgoals%2520based%2520on%2520the%250Aevolving%2520proof%2520state.%2520Given%2520this%2520more%2520structured%2520generation%2520of%2520goals%252C%2520the%250Aresulting%2520problem%2520becomes%2520more%2520amenable%2520to%2520search.%2520We%2520then%2520apply%2520Monte%2520Carlo%250ATree%2520Search%2520%2528MCTS%2529-like%2520algorithms%2520to%2520solve%2520the%2520sG-MDP%252C%2520instantiating%2520our%250Aapproach%2520in%2520Bourbaki%2520%25287B%2529%252C%2520a%2520modular%2520system%2520that%2520can%2520ensemble%2520multiple%25207B%2520LLMs%250Afor%2520subgoal%2520generation%2520and%2520tactic%2520synthesis.%2520On%2520PutnamBench%252C%2520Bourbaki%2520%25287B%2529%250Asolves%252026%2520problems%252C%2520achieving%2520new%2520state-of-the-art%2520results%2520with%2520models%2520at%2520this%250Ascale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bourbaki%3A%20Self-Generated%20and%20Goal-Conditioned%20MDPs%20for%20Theorem%20Proving&entry.906535625=Matthieu%20Zimmer%20and%20Xiaotong%20Ji%20and%20Rasul%20Tutunov%20and%20Anthony%20Bordg%20and%20Jun%20Wang%20and%20Haitham%20Bou%20Ammar&entry.1292438233=%20%20Reasoning%20remains%20a%20challenging%20task%20for%20large%20language%20models%20%28LLMs%29%2C%0Aespecially%20within%20the%20logically%20constrained%20environment%20of%20automated%20theorem%0Aproving%20%28ATP%29%2C%20due%20to%20sparse%20rewards%20and%20the%20vast%20scale%20of%20proofs.%20These%0Achallenges%20are%20amplified%20in%20benchmarks%20like%20PutnamBench%2C%20which%20contains%0Auniversity-level%20problems%20requiring%20complex%2C%20multi-step%20reasoning.%20To%20address%0Athis%2C%20we%20introduce%20self-generated%20goal-conditioned%20MDPs%20%28sG-MDPs%29%2C%20a%20new%0Aframework%20in%20which%20agents%20generate%20and%20pursue%20their%20subgoals%20based%20on%20the%0Aevolving%20proof%20state.%20Given%20this%20more%20structured%20generation%20of%20goals%2C%20the%0Aresulting%20problem%20becomes%20more%20amenable%20to%20search.%20We%20then%20apply%20Monte%20Carlo%0ATree%20Search%20%28MCTS%29-like%20algorithms%20to%20solve%20the%20sG-MDP%2C%20instantiating%20our%0Aapproach%20in%20Bourbaki%20%287B%29%2C%20a%20modular%20system%20that%20can%20ensemble%20multiple%207B%20LLMs%0Afor%20subgoal%20generation%20and%20tactic%20synthesis.%20On%20PutnamBench%2C%20Bourbaki%20%287B%29%0Asolves%2026%20problems%2C%20achieving%20new%20state-of-the-art%20results%20with%20models%20at%20this%0Ascale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02726v1&entry.124074799=Read"},
{"title": "Higher-Order Singular-Value Derivatives of Rectangular Real Matrices", "author": "R\u00f3is\u00edn Luo and James McDermott and Colm O'Riordan", "abstract": "  We present a theoretical framework for deriving the general $n$-th order\nFr\\'echet derivatives of singular values in real rectangular matrices, by\nleveraging reduced resolvent operators from Kato's analytic perturbation theory\nfor self-adjoint operators. Deriving closed-form expressions for higher-order\nderivatives of singular values is notoriously challenging through standard\nmatrix-analysis techniques. To overcome this, we treat a real rectangular\nmatrix as a compact operator on a finite-dimensional Hilbert space, and embed\nthe rectangular matrix into a block self-adjoint operator so that non-symmetric\nperturbations are captured. Applying Kato's asymptotic eigenvalue expansion to\nthis construction, we obtain a general, closed-form expression for the\ninfinitesimal $n$-th order spectral variations. Specializing to $n=2$ and\ndeploying on a Kronecker-product representation with matrix convention yield\nthe Hessian of a singular value, not found in literature. By bridging abstract\noperator-theoretic perturbation theory with matrices, our framework equips\nresearchers with a practical toolkit for higher-order spectral sensitivity\nstudies in random matrix applications (e.g., adversarial perturbation in deep\nlearning).\n", "link": "http://arxiv.org/abs/2506.03764v3", "date": "2025-07-03", "relevancy": 1.6547, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4333}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4137}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Singular-Value%20Derivatives%20of%20Rectangular%20Real%20Matrices&body=Title%3A%20Higher-Order%20Singular-Value%20Derivatives%20of%20Rectangular%20Real%20Matrices%0AAuthor%3A%20R%C3%B3is%C3%ADn%20Luo%20and%20James%20McDermott%20and%20Colm%20O%27Riordan%0AAbstract%3A%20%20%20We%20present%20a%20theoretical%20framework%20for%20deriving%20the%20general%20%24n%24-th%20order%0AFr%5C%27echet%20derivatives%20of%20singular%20values%20in%20real%20rectangular%20matrices%2C%20by%0Aleveraging%20reduced%20resolvent%20operators%20from%20Kato%27s%20analytic%20perturbation%20theory%0Afor%20self-adjoint%20operators.%20Deriving%20closed-form%20expressions%20for%20higher-order%0Aderivatives%20of%20singular%20values%20is%20notoriously%20challenging%20through%20standard%0Amatrix-analysis%20techniques.%20To%20overcome%20this%2C%20we%20treat%20a%20real%20rectangular%0Amatrix%20as%20a%20compact%20operator%20on%20a%20finite-dimensional%20Hilbert%20space%2C%20and%20embed%0Athe%20rectangular%20matrix%20into%20a%20block%20self-adjoint%20operator%20so%20that%20non-symmetric%0Aperturbations%20are%20captured.%20Applying%20Kato%27s%20asymptotic%20eigenvalue%20expansion%20to%0Athis%20construction%2C%20we%20obtain%20a%20general%2C%20closed-form%20expression%20for%20the%0Ainfinitesimal%20%24n%24-th%20order%20spectral%20variations.%20Specializing%20to%20%24n%3D2%24%20and%0Adeploying%20on%20a%20Kronecker-product%20representation%20with%20matrix%20convention%20yield%0Athe%20Hessian%20of%20a%20singular%20value%2C%20not%20found%20in%20literature.%20By%20bridging%20abstract%0Aoperator-theoretic%20perturbation%20theory%20with%20matrices%2C%20our%20framework%20equips%0Aresearchers%20with%20a%20practical%20toolkit%20for%20higher-order%20spectral%20sensitivity%0Astudies%20in%20random%20matrix%20applications%20%28e.g.%2C%20adversarial%20perturbation%20in%20deep%0Alearning%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03764v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Singular-Value%2520Derivatives%2520of%2520Rectangular%2520Real%2520Matrices%26entry.906535625%3DR%25C3%25B3is%25C3%25ADn%2520Luo%2520and%2520James%2520McDermott%2520and%2520Colm%2520O%2527Riordan%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520theoretical%2520framework%2520for%2520deriving%2520the%2520general%2520%2524n%2524-th%2520order%250AFr%255C%2527echet%2520derivatives%2520of%2520singular%2520values%2520in%2520real%2520rectangular%2520matrices%252C%2520by%250Aleveraging%2520reduced%2520resolvent%2520operators%2520from%2520Kato%2527s%2520analytic%2520perturbation%2520theory%250Afor%2520self-adjoint%2520operators.%2520Deriving%2520closed-form%2520expressions%2520for%2520higher-order%250Aderivatives%2520of%2520singular%2520values%2520is%2520notoriously%2520challenging%2520through%2520standard%250Amatrix-analysis%2520techniques.%2520To%2520overcome%2520this%252C%2520we%2520treat%2520a%2520real%2520rectangular%250Amatrix%2520as%2520a%2520compact%2520operator%2520on%2520a%2520finite-dimensional%2520Hilbert%2520space%252C%2520and%2520embed%250Athe%2520rectangular%2520matrix%2520into%2520a%2520block%2520self-adjoint%2520operator%2520so%2520that%2520non-symmetric%250Aperturbations%2520are%2520captured.%2520Applying%2520Kato%2527s%2520asymptotic%2520eigenvalue%2520expansion%2520to%250Athis%2520construction%252C%2520we%2520obtain%2520a%2520general%252C%2520closed-form%2520expression%2520for%2520the%250Ainfinitesimal%2520%2524n%2524-th%2520order%2520spectral%2520variations.%2520Specializing%2520to%2520%2524n%253D2%2524%2520and%250Adeploying%2520on%2520a%2520Kronecker-product%2520representation%2520with%2520matrix%2520convention%2520yield%250Athe%2520Hessian%2520of%2520a%2520singular%2520value%252C%2520not%2520found%2520in%2520literature.%2520By%2520bridging%2520abstract%250Aoperator-theoretic%2520perturbation%2520theory%2520with%2520matrices%252C%2520our%2520framework%2520equips%250Aresearchers%2520with%2520a%2520practical%2520toolkit%2520for%2520higher-order%2520spectral%2520sensitivity%250Astudies%2520in%2520random%2520matrix%2520applications%2520%2528e.g.%252C%2520adversarial%2520perturbation%2520in%2520deep%250Alearning%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03764v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Singular-Value%20Derivatives%20of%20Rectangular%20Real%20Matrices&entry.906535625=R%C3%B3is%C3%ADn%20Luo%20and%20James%20McDermott%20and%20Colm%20O%27Riordan&entry.1292438233=%20%20We%20present%20a%20theoretical%20framework%20for%20deriving%20the%20general%20%24n%24-th%20order%0AFr%5C%27echet%20derivatives%20of%20singular%20values%20in%20real%20rectangular%20matrices%2C%20by%0Aleveraging%20reduced%20resolvent%20operators%20from%20Kato%27s%20analytic%20perturbation%20theory%0Afor%20self-adjoint%20operators.%20Deriving%20closed-form%20expressions%20for%20higher-order%0Aderivatives%20of%20singular%20values%20is%20notoriously%20challenging%20through%20standard%0Amatrix-analysis%20techniques.%20To%20overcome%20this%2C%20we%20treat%20a%20real%20rectangular%0Amatrix%20as%20a%20compact%20operator%20on%20a%20finite-dimensional%20Hilbert%20space%2C%20and%20embed%0Athe%20rectangular%20matrix%20into%20a%20block%20self-adjoint%20operator%20so%20that%20non-symmetric%0Aperturbations%20are%20captured.%20Applying%20Kato%27s%20asymptotic%20eigenvalue%20expansion%20to%0Athis%20construction%2C%20we%20obtain%20a%20general%2C%20closed-form%20expression%20for%20the%0Ainfinitesimal%20%24n%24-th%20order%20spectral%20variations.%20Specializing%20to%20%24n%3D2%24%20and%0Adeploying%20on%20a%20Kronecker-product%20representation%20with%20matrix%20convention%20yield%0Athe%20Hessian%20of%20a%20singular%20value%2C%20not%20found%20in%20literature.%20By%20bridging%20abstract%0Aoperator-theoretic%20perturbation%20theory%20with%20matrices%2C%20our%20framework%20equips%0Aresearchers%20with%20a%20practical%20toolkit%20for%20higher-order%20spectral%20sensitivity%0Astudies%20in%20random%20matrix%20applications%20%28e.g.%2C%20adversarial%20perturbation%20in%20deep%0Alearning%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03764v3&entry.124074799=Read"},
{"title": "Understanding and Improving Length Generalization in Recurrent Models", "author": "Ricardo Buitrago Ruiz and Albert Gu", "abstract": "  Recently, recurrent models such as state space models and linear attention\nhave become popular due to their linear complexity in the sequence length.\nThanks to their recurrent nature, in principle they can process arbitrarily\nlong sequences, but their performance sometimes drops considerably beyond their\ntraining context lengths-i.e. they fail to length generalize. In this work, we\nprovide comprehensive empirical and theoretical analysis to support the\nunexplored states hypothesis, which posits that models fail to length\ngeneralize when during training they are only exposed to a limited subset of\nthe distribution of all attainable states (i.e. states that would be attained\nif the recurrence was applied to long sequences). Furthermore, we investigate\nsimple training interventions that aim to increase the coverage of the states\nthat the model is trained on, e.g. by initializing the state with Gaussian\nnoise or with the final state of a different input sequence. With only 500\npost-training steps ($\\sim 0.1\\%$ of the pre-training budget), these\ninterventions enable length generalization for sequences that are orders of\nmagnitude longer than the training context (e.g. $2k\\longrightarrow 128k$) and\nshow improved performance in long context tasks, thus presenting a simple and\nefficient way to enable robust length generalization in general recurrent\nmodels.\n", "link": "http://arxiv.org/abs/2507.02782v1", "date": "2025-07-03", "relevancy": 1.9454, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4933}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4909}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Improving%20Length%20Generalization%20in%20Recurrent%20Models&body=Title%3A%20Understanding%20and%20Improving%20Length%20Generalization%20in%20Recurrent%20Models%0AAuthor%3A%20Ricardo%20Buitrago%20Ruiz%20and%20Albert%20Gu%0AAbstract%3A%20%20%20Recently%2C%20recurrent%20models%20such%20as%20state%20space%20models%20and%20linear%20attention%0Ahave%20become%20popular%20due%20to%20their%20linear%20complexity%20in%20the%20sequence%20length.%0AThanks%20to%20their%20recurrent%20nature%2C%20in%20principle%20they%20can%20process%20arbitrarily%0Along%20sequences%2C%20but%20their%20performance%20sometimes%20drops%20considerably%20beyond%20their%0Atraining%20context%20lengths-i.e.%20they%20fail%20to%20length%20generalize.%20In%20this%20work%2C%20we%0Aprovide%20comprehensive%20empirical%20and%20theoretical%20analysis%20to%20support%20the%0Aunexplored%20states%20hypothesis%2C%20which%20posits%20that%20models%20fail%20to%20length%0Ageneralize%20when%20during%20training%20they%20are%20only%20exposed%20to%20a%20limited%20subset%20of%0Athe%20distribution%20of%20all%20attainable%20states%20%28i.e.%20states%20that%20would%20be%20attained%0Aif%20the%20recurrence%20was%20applied%20to%20long%20sequences%29.%20Furthermore%2C%20we%20investigate%0Asimple%20training%20interventions%20that%20aim%20to%20increase%20the%20coverage%20of%20the%20states%0Athat%20the%20model%20is%20trained%20on%2C%20e.g.%20by%20initializing%20the%20state%20with%20Gaussian%0Anoise%20or%20with%20the%20final%20state%20of%20a%20different%20input%20sequence.%20With%20only%20500%0Apost-training%20steps%20%28%24%5Csim%200.1%5C%25%24%20of%20the%20pre-training%20budget%29%2C%20these%0Ainterventions%20enable%20length%20generalization%20for%20sequences%20that%20are%20orders%20of%0Amagnitude%20longer%20than%20the%20training%20context%20%28e.g.%20%242k%5Clongrightarrow%20128k%24%29%20and%0Ashow%20improved%20performance%20in%20long%20context%20tasks%2C%20thus%20presenting%20a%20simple%20and%0Aefficient%20way%20to%20enable%20robust%20length%20generalization%20in%20general%20recurrent%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Improving%2520Length%2520Generalization%2520in%2520Recurrent%2520Models%26entry.906535625%3DRicardo%2520Buitrago%2520Ruiz%2520and%2520Albert%2520Gu%26entry.1292438233%3D%2520%2520Recently%252C%2520recurrent%2520models%2520such%2520as%2520state%2520space%2520models%2520and%2520linear%2520attention%250Ahave%2520become%2520popular%2520due%2520to%2520their%2520linear%2520complexity%2520in%2520the%2520sequence%2520length.%250AThanks%2520to%2520their%2520recurrent%2520nature%252C%2520in%2520principle%2520they%2520can%2520process%2520arbitrarily%250Along%2520sequences%252C%2520but%2520their%2520performance%2520sometimes%2520drops%2520considerably%2520beyond%2520their%250Atraining%2520context%2520lengths-i.e.%2520they%2520fail%2520to%2520length%2520generalize.%2520In%2520this%2520work%252C%2520we%250Aprovide%2520comprehensive%2520empirical%2520and%2520theoretical%2520analysis%2520to%2520support%2520the%250Aunexplored%2520states%2520hypothesis%252C%2520which%2520posits%2520that%2520models%2520fail%2520to%2520length%250Ageneralize%2520when%2520during%2520training%2520they%2520are%2520only%2520exposed%2520to%2520a%2520limited%2520subset%2520of%250Athe%2520distribution%2520of%2520all%2520attainable%2520states%2520%2528i.e.%2520states%2520that%2520would%2520be%2520attained%250Aif%2520the%2520recurrence%2520was%2520applied%2520to%2520long%2520sequences%2529.%2520Furthermore%252C%2520we%2520investigate%250Asimple%2520training%2520interventions%2520that%2520aim%2520to%2520increase%2520the%2520coverage%2520of%2520the%2520states%250Athat%2520the%2520model%2520is%2520trained%2520on%252C%2520e.g.%2520by%2520initializing%2520the%2520state%2520with%2520Gaussian%250Anoise%2520or%2520with%2520the%2520final%2520state%2520of%2520a%2520different%2520input%2520sequence.%2520With%2520only%2520500%250Apost-training%2520steps%2520%2528%2524%255Csim%25200.1%255C%2525%2524%2520of%2520the%2520pre-training%2520budget%2529%252C%2520these%250Ainterventions%2520enable%2520length%2520generalization%2520for%2520sequences%2520that%2520are%2520orders%2520of%250Amagnitude%2520longer%2520than%2520the%2520training%2520context%2520%2528e.g.%2520%25242k%255Clongrightarrow%2520128k%2524%2529%2520and%250Ashow%2520improved%2520performance%2520in%2520long%2520context%2520tasks%252C%2520thus%2520presenting%2520a%2520simple%2520and%250Aefficient%2520way%2520to%2520enable%2520robust%2520length%2520generalization%2520in%2520general%2520recurrent%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Improving%20Length%20Generalization%20in%20Recurrent%20Models&entry.906535625=Ricardo%20Buitrago%20Ruiz%20and%20Albert%20Gu&entry.1292438233=%20%20Recently%2C%20recurrent%20models%20such%20as%20state%20space%20models%20and%20linear%20attention%0Ahave%20become%20popular%20due%20to%20their%20linear%20complexity%20in%20the%20sequence%20length.%0AThanks%20to%20their%20recurrent%20nature%2C%20in%20principle%20they%20can%20process%20arbitrarily%0Along%20sequences%2C%20but%20their%20performance%20sometimes%20drops%20considerably%20beyond%20their%0Atraining%20context%20lengths-i.e.%20they%20fail%20to%20length%20generalize.%20In%20this%20work%2C%20we%0Aprovide%20comprehensive%20empirical%20and%20theoretical%20analysis%20to%20support%20the%0Aunexplored%20states%20hypothesis%2C%20which%20posits%20that%20models%20fail%20to%20length%0Ageneralize%20when%20during%20training%20they%20are%20only%20exposed%20to%20a%20limited%20subset%20of%0Athe%20distribution%20of%20all%20attainable%20states%20%28i.e.%20states%20that%20would%20be%20attained%0Aif%20the%20recurrence%20was%20applied%20to%20long%20sequences%29.%20Furthermore%2C%20we%20investigate%0Asimple%20training%20interventions%20that%20aim%20to%20increase%20the%20coverage%20of%20the%20states%0Athat%20the%20model%20is%20trained%20on%2C%20e.g.%20by%20initializing%20the%20state%20with%20Gaussian%0Anoise%20or%20with%20the%20final%20state%20of%20a%20different%20input%20sequence.%20With%20only%20500%0Apost-training%20steps%20%28%24%5Csim%200.1%5C%25%24%20of%20the%20pre-training%20budget%29%2C%20these%0Ainterventions%20enable%20length%20generalization%20for%20sequences%20that%20are%20orders%20of%0Amagnitude%20longer%20than%20the%20training%20context%20%28e.g.%20%242k%5Clongrightarrow%20128k%24%29%20and%0Ashow%20improved%20performance%20in%20long%20context%20tasks%2C%20thus%20presenting%20a%20simple%20and%0Aefficient%20way%20to%20enable%20robust%20length%20generalization%20in%20general%20recurrent%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02782v1&entry.124074799=Read"},
{"title": "AI Flow: Perspectives, Scenarios, and Approaches", "author": "Hongjun An and Wenhan Hu and Sida Huang and Siqi Huang and Ruanjun Li and Yuanzhi Liang and Jiawei Shao and Yiliang Song and Zihan Wang and Cheng Yuan and Chi Zhang and Hongyuan Zhang and Wenhao Zhuang and Xuelong Li", "abstract": "  Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems.\n", "link": "http://arxiv.org/abs/2506.12479v2", "date": "2025-07-03", "relevancy": 1.9104, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Flow%3A%20Perspectives%2C%20Scenarios%2C%20and%20Approaches&body=Title%3A%20AI%20Flow%3A%20Perspectives%2C%20Scenarios%2C%20and%20Approaches%0AAuthor%3A%20Hongjun%20An%20and%20Wenhan%20Hu%20and%20Sida%20Huang%20and%20Siqi%20Huang%20and%20Ruanjun%20Li%20and%20Yuanzhi%20Liang%20and%20Jiawei%20Shao%20and%20Yiliang%20Song%20and%20Zihan%20Wang%20and%20Cheng%20Yuan%20and%20Chi%20Zhang%20and%20Hongyuan%20Zhang%20and%20Wenhao%20Zhuang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Pioneered%20by%20the%20foundational%20information%20theory%20by%20Claude%20Shannon%20and%20the%0Avisionary%20framework%20of%20machine%20intelligence%20by%20Alan%20Turing%2C%20the%20convergent%0Aevolution%20of%20information%20and%20communication%20technologies%20%28IT/CT%29%20has%20created%20an%0Aunbroken%20wave%20of%20connectivity%20and%20computation.%20This%20synergy%20has%20sparked%20a%0Atechnological%20revolution%2C%20now%20reaching%20its%20peak%20with%20large%20artificial%0Aintelligence%20%28AI%29%20models%20that%20are%20reshaping%20industries%20and%20redefining%0Ahuman-machine%20collaboration.%20However%2C%20the%20realization%20of%20ubiquitous%0Aintelligence%20faces%20considerable%20challenges%20due%20to%20substantial%20resource%0Aconsumption%20in%20large%20models%20and%20high%20communication%20bandwidth%20demands.%20To%0Aaddress%20these%20challenges%2C%20AI%20Flow%20has%20been%20introduced%20as%20a%20multidisciplinary%0Aframework%20that%20integrates%20cutting-edge%20IT%20and%20CT%20advancements%2C%20with%20a%0Aparticular%20emphasis%20on%20the%20following%20three%20key%20points.%20First%2C%20device-edge-cloud%0Aframework%20serves%20as%20the%20foundation%2C%20which%20integrates%20end%20devices%2C%20edge%20servers%2C%0Aand%20cloud%20clusters%20to%20optimize%20scalability%20and%20efficiency%20for%20low-latency%20model%0Ainference.%20Second%2C%20we%20introduce%20the%20concept%20of%20familial%20models%2C%20which%20refers%20to%0Aa%20series%20of%20different-sized%20models%20with%20aligned%20hidden%20features%2C%20enabling%0Aeffective%20collaboration%20and%20the%20flexibility%20to%20adapt%20to%20varying%20resource%0Aconstraints%20and%20dynamic%20scenarios.%20Third%2C%20connectivity-%20and%20interaction-based%0Aintelligence%20emergence%20is%20a%20novel%20paradigm%20of%20AI%20Flow.%20By%20leveraging%0Acommunication%20networks%20to%20enhance%20connectivity%2C%20the%20collaboration%20among%20AI%0Amodels%20across%20heterogeneous%20nodes%20achieves%20emergent%20intelligence%20that%20surpasses%0Athe%20capability%20of%20any%20single%20model.%20The%20innovations%20of%20AI%20Flow%20provide%20enhanced%0Aintelligence%2C%20timely%20responsiveness%2C%20and%20ubiquitous%20accessibility%20to%20AI%0Aservices%2C%20paving%20the%20way%20for%20the%20tighter%20fusion%20of%20AI%20techniques%20and%0Acommunication%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Flow%253A%2520Perspectives%252C%2520Scenarios%252C%2520and%2520Approaches%26entry.906535625%3DHongjun%2520An%2520and%2520Wenhan%2520Hu%2520and%2520Sida%2520Huang%2520and%2520Siqi%2520Huang%2520and%2520Ruanjun%2520Li%2520and%2520Yuanzhi%2520Liang%2520and%2520Jiawei%2520Shao%2520and%2520Yiliang%2520Song%2520and%2520Zihan%2520Wang%2520and%2520Cheng%2520Yuan%2520and%2520Chi%2520Zhang%2520and%2520Hongyuan%2520Zhang%2520and%2520Wenhao%2520Zhuang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Pioneered%2520by%2520the%2520foundational%2520information%2520theory%2520by%2520Claude%2520Shannon%2520and%2520the%250Avisionary%2520framework%2520of%2520machine%2520intelligence%2520by%2520Alan%2520Turing%252C%2520the%2520convergent%250Aevolution%2520of%2520information%2520and%2520communication%2520technologies%2520%2528IT/CT%2529%2520has%2520created%2520an%250Aunbroken%2520wave%2520of%2520connectivity%2520and%2520computation.%2520This%2520synergy%2520has%2520sparked%2520a%250Atechnological%2520revolution%252C%2520now%2520reaching%2520its%2520peak%2520with%2520large%2520artificial%250Aintelligence%2520%2528AI%2529%2520models%2520that%2520are%2520reshaping%2520industries%2520and%2520redefining%250Ahuman-machine%2520collaboration.%2520However%252C%2520the%2520realization%2520of%2520ubiquitous%250Aintelligence%2520faces%2520considerable%2520challenges%2520due%2520to%2520substantial%2520resource%250Aconsumption%2520in%2520large%2520models%2520and%2520high%2520communication%2520bandwidth%2520demands.%2520To%250Aaddress%2520these%2520challenges%252C%2520AI%2520Flow%2520has%2520been%2520introduced%2520as%2520a%2520multidisciplinary%250Aframework%2520that%2520integrates%2520cutting-edge%2520IT%2520and%2520CT%2520advancements%252C%2520with%2520a%250Aparticular%2520emphasis%2520on%2520the%2520following%2520three%2520key%2520points.%2520First%252C%2520device-edge-cloud%250Aframework%2520serves%2520as%2520the%2520foundation%252C%2520which%2520integrates%2520end%2520devices%252C%2520edge%2520servers%252C%250Aand%2520cloud%2520clusters%2520to%2520optimize%2520scalability%2520and%2520efficiency%2520for%2520low-latency%2520model%250Ainference.%2520Second%252C%2520we%2520introduce%2520the%2520concept%2520of%2520familial%2520models%252C%2520which%2520refers%2520to%250Aa%2520series%2520of%2520different-sized%2520models%2520with%2520aligned%2520hidden%2520features%252C%2520enabling%250Aeffective%2520collaboration%2520and%2520the%2520flexibility%2520to%2520adapt%2520to%2520varying%2520resource%250Aconstraints%2520and%2520dynamic%2520scenarios.%2520Third%252C%2520connectivity-%2520and%2520interaction-based%250Aintelligence%2520emergence%2520is%2520a%2520novel%2520paradigm%2520of%2520AI%2520Flow.%2520By%2520leveraging%250Acommunication%2520networks%2520to%2520enhance%2520connectivity%252C%2520the%2520collaboration%2520among%2520AI%250Amodels%2520across%2520heterogeneous%2520nodes%2520achieves%2520emergent%2520intelligence%2520that%2520surpasses%250Athe%2520capability%2520of%2520any%2520single%2520model.%2520The%2520innovations%2520of%2520AI%2520Flow%2520provide%2520enhanced%250Aintelligence%252C%2520timely%2520responsiveness%252C%2520and%2520ubiquitous%2520accessibility%2520to%2520AI%250Aservices%252C%2520paving%2520the%2520way%2520for%2520the%2520tighter%2520fusion%2520of%2520AI%2520techniques%2520and%250Acommunication%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Flow%3A%20Perspectives%2C%20Scenarios%2C%20and%20Approaches&entry.906535625=Hongjun%20An%20and%20Wenhan%20Hu%20and%20Sida%20Huang%20and%20Siqi%20Huang%20and%20Ruanjun%20Li%20and%20Yuanzhi%20Liang%20and%20Jiawei%20Shao%20and%20Yiliang%20Song%20and%20Zihan%20Wang%20and%20Cheng%20Yuan%20and%20Chi%20Zhang%20and%20Hongyuan%20Zhang%20and%20Wenhao%20Zhuang%20and%20Xuelong%20Li&entry.1292438233=%20%20Pioneered%20by%20the%20foundational%20information%20theory%20by%20Claude%20Shannon%20and%20the%0Avisionary%20framework%20of%20machine%20intelligence%20by%20Alan%20Turing%2C%20the%20convergent%0Aevolution%20of%20information%20and%20communication%20technologies%20%28IT/CT%29%20has%20created%20an%0Aunbroken%20wave%20of%20connectivity%20and%20computation.%20This%20synergy%20has%20sparked%20a%0Atechnological%20revolution%2C%20now%20reaching%20its%20peak%20with%20large%20artificial%0Aintelligence%20%28AI%29%20models%20that%20are%20reshaping%20industries%20and%20redefining%0Ahuman-machine%20collaboration.%20However%2C%20the%20realization%20of%20ubiquitous%0Aintelligence%20faces%20considerable%20challenges%20due%20to%20substantial%20resource%0Aconsumption%20in%20large%20models%20and%20high%20communication%20bandwidth%20demands.%20To%0Aaddress%20these%20challenges%2C%20AI%20Flow%20has%20been%20introduced%20as%20a%20multidisciplinary%0Aframework%20that%20integrates%20cutting-edge%20IT%20and%20CT%20advancements%2C%20with%20a%0Aparticular%20emphasis%20on%20the%20following%20three%20key%20points.%20First%2C%20device-edge-cloud%0Aframework%20serves%20as%20the%20foundation%2C%20which%20integrates%20end%20devices%2C%20edge%20servers%2C%0Aand%20cloud%20clusters%20to%20optimize%20scalability%20and%20efficiency%20for%20low-latency%20model%0Ainference.%20Second%2C%20we%20introduce%20the%20concept%20of%20familial%20models%2C%20which%20refers%20to%0Aa%20series%20of%20different-sized%20models%20with%20aligned%20hidden%20features%2C%20enabling%0Aeffective%20collaboration%20and%20the%20flexibility%20to%20adapt%20to%20varying%20resource%0Aconstraints%20and%20dynamic%20scenarios.%20Third%2C%20connectivity-%20and%20interaction-based%0Aintelligence%20emergence%20is%20a%20novel%20paradigm%20of%20AI%20Flow.%20By%20leveraging%0Acommunication%20networks%20to%20enhance%20connectivity%2C%20the%20collaboration%20among%20AI%0Amodels%20across%20heterogeneous%20nodes%20achieves%20emergent%20intelligence%20that%20surpasses%0Athe%20capability%20of%20any%20single%20model.%20The%20innovations%20of%20AI%20Flow%20provide%20enhanced%0Aintelligence%2C%20timely%20responsiveness%2C%20and%20ubiquitous%20accessibility%20to%20AI%0Aservices%2C%20paving%20the%20way%20for%20the%20tighter%20fusion%20of%20AI%20techniques%20and%0Acommunication%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12479v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


