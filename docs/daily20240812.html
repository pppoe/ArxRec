<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240811.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Mesh-based Object Tracking for Dynamic Semantic 3D Scene Graphs via Ray\n  Tracing", "author": "Lennart Niecksch and Alexander Mock and Felix Igelbrink and Thomas Wiemann and Joachim Hertzberg", "abstract": "  In this paper, we present a novel method for 3D geometric scene graph\ngeneration using range sensors and RGB cameras. We initially detect\ninstance-wise keypoints with a YOLOv8s model to compute 6D pose estimates of\nknown objects by solving PnP. We use a ray tracing approach to track a\ngeometric scene graph consisting of mesh models of object instances. In\ncontrast to classical point-to-point matching, this leads to more robust\nresults, especially under occlusions between objects instances. We show that\nusing this hybrid strategy leads to robust self-localization, pre-segmentation\nof the range sensor data and accurate pose tracking of objects using the same\nenvironmental representation. All detected objects are integrated into a\nsemantic scene graph. This scene graph then serves as a front end to a semantic\nmapping framework to allow spatial reasoning.\n", "link": "http://arxiv.org/abs/2408.04979v1", "date": "2024-08-09", "relevancy": 3.0298, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6241}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6127}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh-based%20Object%20Tracking%20for%20Dynamic%20Semantic%203D%20Scene%20Graphs%20via%20Ray%0A%20%20Tracing&body=Title%3A%20Mesh-based%20Object%20Tracking%20for%20Dynamic%20Semantic%203D%20Scene%20Graphs%20via%20Ray%0A%20%20Tracing%0AAuthor%3A%20Lennart%20Niecksch%20and%20Alexander%20Mock%20and%20Felix%20Igelbrink%20and%20Thomas%20Wiemann%20and%20Joachim%20Hertzberg%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20method%20for%203D%20geometric%20scene%20graph%0Ageneration%20using%20range%20sensors%20and%20RGB%20cameras.%20We%20initially%20detect%0Ainstance-wise%20keypoints%20with%20a%20YOLOv8s%20model%20to%20compute%206D%20pose%20estimates%20of%0Aknown%20objects%20by%20solving%20PnP.%20We%20use%20a%20ray%20tracing%20approach%20to%20track%20a%0Ageometric%20scene%20graph%20consisting%20of%20mesh%20models%20of%20object%20instances.%20In%0Acontrast%20to%20classical%20point-to-point%20matching%2C%20this%20leads%20to%20more%20robust%0Aresults%2C%20especially%20under%20occlusions%20between%20objects%20instances.%20We%20show%20that%0Ausing%20this%20hybrid%20strategy%20leads%20to%20robust%20self-localization%2C%20pre-segmentation%0Aof%20the%20range%20sensor%20data%20and%20accurate%20pose%20tracking%20of%20objects%20using%20the%20same%0Aenvironmental%20representation.%20All%20detected%20objects%20are%20integrated%20into%20a%0Asemantic%20scene%20graph.%20This%20scene%20graph%20then%20serves%20as%20a%20front%20end%20to%20a%20semantic%0Amapping%20framework%20to%20allow%20spatial%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh-based%2520Object%2520Tracking%2520for%2520Dynamic%2520Semantic%25203D%2520Scene%2520Graphs%2520via%2520Ray%250A%2520%2520Tracing%26entry.906535625%3DLennart%2520Niecksch%2520and%2520Alexander%2520Mock%2520and%2520Felix%2520Igelbrink%2520and%2520Thomas%2520Wiemann%2520and%2520Joachim%2520Hertzberg%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520method%2520for%25203D%2520geometric%2520scene%2520graph%250Ageneration%2520using%2520range%2520sensors%2520and%2520RGB%2520cameras.%2520We%2520initially%2520detect%250Ainstance-wise%2520keypoints%2520with%2520a%2520YOLOv8s%2520model%2520to%2520compute%25206D%2520pose%2520estimates%2520of%250Aknown%2520objects%2520by%2520solving%2520PnP.%2520We%2520use%2520a%2520ray%2520tracing%2520approach%2520to%2520track%2520a%250Ageometric%2520scene%2520graph%2520consisting%2520of%2520mesh%2520models%2520of%2520object%2520instances.%2520In%250Acontrast%2520to%2520classical%2520point-to-point%2520matching%252C%2520this%2520leads%2520to%2520more%2520robust%250Aresults%252C%2520especially%2520under%2520occlusions%2520between%2520objects%2520instances.%2520We%2520show%2520that%250Ausing%2520this%2520hybrid%2520strategy%2520leads%2520to%2520robust%2520self-localization%252C%2520pre-segmentation%250Aof%2520the%2520range%2520sensor%2520data%2520and%2520accurate%2520pose%2520tracking%2520of%2520objects%2520using%2520the%2520same%250Aenvironmental%2520representation.%2520All%2520detected%2520objects%2520are%2520integrated%2520into%2520a%250Asemantic%2520scene%2520graph.%2520This%2520scene%2520graph%2520then%2520serves%2520as%2520a%2520front%2520end%2520to%2520a%2520semantic%250Amapping%2520framework%2520to%2520allow%2520spatial%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh-based%20Object%20Tracking%20for%20Dynamic%20Semantic%203D%20Scene%20Graphs%20via%20Ray%0A%20%20Tracing&entry.906535625=Lennart%20Niecksch%20and%20Alexander%20Mock%20and%20Felix%20Igelbrink%20and%20Thomas%20Wiemann%20and%20Joachim%20Hertzberg&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20method%20for%203D%20geometric%20scene%20graph%0Ageneration%20using%20range%20sensors%20and%20RGB%20cameras.%20We%20initially%20detect%0Ainstance-wise%20keypoints%20with%20a%20YOLOv8s%20model%20to%20compute%206D%20pose%20estimates%20of%0Aknown%20objects%20by%20solving%20PnP.%20We%20use%20a%20ray%20tracing%20approach%20to%20track%20a%0Ageometric%20scene%20graph%20consisting%20of%20mesh%20models%20of%20object%20instances.%20In%0Acontrast%20to%20classical%20point-to-point%20matching%2C%20this%20leads%20to%20more%20robust%0Aresults%2C%20especially%20under%20occlusions%20between%20objects%20instances.%20We%20show%20that%0Ausing%20this%20hybrid%20strategy%20leads%20to%20robust%20self-localization%2C%20pre-segmentation%0Aof%20the%20range%20sensor%20data%20and%20accurate%20pose%20tracking%20of%20objects%20using%20the%20same%0Aenvironmental%20representation.%20All%20detected%20objects%20are%20integrated%20into%20a%0Asemantic%20scene%20graph.%20This%20scene%20graph%20then%20serves%20as%20a%20front%20end%20to%20a%20semantic%0Amapping%20framework%20to%20allow%20spatial%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04979v1&entry.124074799=Read"},
{"title": "LucidDreaming: Controllable Object-Centric 3D Generation", "author": "Zhaoning Wang and Ming Li and Chen Chen", "abstract": "  With the recent development of generative models, Text-to-3D generations have\nalso seen significant growth, opening a door for creating video-game 3D assets\nfrom a more general public. Nonetheless, people without any professional 3D\nediting experience would find it hard to achieve precise control over the 3D\ngeneration, especially if there are multiple objects in the prompt, as using\ntext to control often leads to missing objects and imprecise locations. In this\npaper, we present LucidDreaming as an effective pipeline capable of spatial and\nnumerical control over 3D generation from only textual prompt commands or 3D\nbounding boxes. Specifically, our research demonstrates that Large Language\nModels (LLMs) possess 3D spatial awareness and can effectively translate\ntextual 3D information into precise 3D bounding boxes. We leverage LLMs to get\nindividual object information and their 3D bounding boxes as the initial step\nof our process. Then with the bounding boxes, We further propose clipped ray\nsampling and object-centric density blob bias to generate 3D objects aligning\nwith the bounding boxes. We show that our method exhibits remarkable\nadaptability across a spectrum of mainstream Score Distillation Sampling-based\n3D generation frameworks and our pipeline can even used to insert objects into\nan existing NeRF scene. Moreover, we also provide a dataset of prompts with 3D\nbounding boxes, benchmarking 3D spatial controllability. With extensive\nqualitative and quantitative experiments, we demonstrate that LucidDreaming\nachieves superior results in object placement precision and generation fidelity\ncompared to current approaches, while maintaining flexibility and ease of use\nfor non-expert users.\n", "link": "http://arxiv.org/abs/2312.00588v2", "date": "2024-08-09", "relevancy": 2.9396, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5974}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5974}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LucidDreaming%3A%20Controllable%20Object-Centric%203D%20Generation&body=Title%3A%20LucidDreaming%3A%20Controllable%20Object-Centric%203D%20Generation%0AAuthor%3A%20Zhaoning%20Wang%20and%20Ming%20Li%20and%20Chen%20Chen%0AAbstract%3A%20%20%20With%20the%20recent%20development%20of%20generative%20models%2C%20Text-to-3D%20generations%20have%0Aalso%20seen%20significant%20growth%2C%20opening%20a%20door%20for%20creating%20video-game%203D%20assets%0Afrom%20a%20more%20general%20public.%20Nonetheless%2C%20people%20without%20any%20professional%203D%0Aediting%20experience%20would%20find%20it%20hard%20to%20achieve%20precise%20control%20over%20the%203D%0Ageneration%2C%20especially%20if%20there%20are%20multiple%20objects%20in%20the%20prompt%2C%20as%20using%0Atext%20to%20control%20often%20leads%20to%20missing%20objects%20and%20imprecise%20locations.%20In%20this%0Apaper%2C%20we%20present%20LucidDreaming%20as%20an%20effective%20pipeline%20capable%20of%20spatial%20and%0Anumerical%20control%20over%203D%20generation%20from%20only%20textual%20prompt%20commands%20or%203D%0Abounding%20boxes.%20Specifically%2C%20our%20research%20demonstrates%20that%20Large%20Language%0AModels%20%28LLMs%29%20possess%203D%20spatial%20awareness%20and%20can%20effectively%20translate%0Atextual%203D%20information%20into%20precise%203D%20bounding%20boxes.%20We%20leverage%20LLMs%20to%20get%0Aindividual%20object%20information%20and%20their%203D%20bounding%20boxes%20as%20the%20initial%20step%0Aof%20our%20process.%20Then%20with%20the%20bounding%20boxes%2C%20We%20further%20propose%20clipped%20ray%0Asampling%20and%20object-centric%20density%20blob%20bias%20to%20generate%203D%20objects%20aligning%0Awith%20the%20bounding%20boxes.%20We%20show%20that%20our%20method%20exhibits%20remarkable%0Aadaptability%20across%20a%20spectrum%20of%20mainstream%20Score%20Distillation%20Sampling-based%0A3D%20generation%20frameworks%20and%20our%20pipeline%20can%20even%20used%20to%20insert%20objects%20into%0Aan%20existing%20NeRF%20scene.%20Moreover%2C%20we%20also%20provide%20a%20dataset%20of%20prompts%20with%203D%0Abounding%20boxes%2C%20benchmarking%203D%20spatial%20controllability.%20With%20extensive%0Aqualitative%20and%20quantitative%20experiments%2C%20we%20demonstrate%20that%20LucidDreaming%0Aachieves%20superior%20results%20in%20object%20placement%20precision%20and%20generation%20fidelity%0Acompared%20to%20current%20approaches%2C%20while%20maintaining%20flexibility%20and%20ease%20of%20use%0Afor%20non-expert%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00588v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLucidDreaming%253A%2520Controllable%2520Object-Centric%25203D%2520Generation%26entry.906535625%3DZhaoning%2520Wang%2520and%2520Ming%2520Li%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520recent%2520development%2520of%2520generative%2520models%252C%2520Text-to-3D%2520generations%2520have%250Aalso%2520seen%2520significant%2520growth%252C%2520opening%2520a%2520door%2520for%2520creating%2520video-game%25203D%2520assets%250Afrom%2520a%2520more%2520general%2520public.%2520Nonetheless%252C%2520people%2520without%2520any%2520professional%25203D%250Aediting%2520experience%2520would%2520find%2520it%2520hard%2520to%2520achieve%2520precise%2520control%2520over%2520the%25203D%250Ageneration%252C%2520especially%2520if%2520there%2520are%2520multiple%2520objects%2520in%2520the%2520prompt%252C%2520as%2520using%250Atext%2520to%2520control%2520often%2520leads%2520to%2520missing%2520objects%2520and%2520imprecise%2520locations.%2520In%2520this%250Apaper%252C%2520we%2520present%2520LucidDreaming%2520as%2520an%2520effective%2520pipeline%2520capable%2520of%2520spatial%2520and%250Anumerical%2520control%2520over%25203D%2520generation%2520from%2520only%2520textual%2520prompt%2520commands%2520or%25203D%250Abounding%2520boxes.%2520Specifically%252C%2520our%2520research%2520demonstrates%2520that%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520possess%25203D%2520spatial%2520awareness%2520and%2520can%2520effectively%2520translate%250Atextual%25203D%2520information%2520into%2520precise%25203D%2520bounding%2520boxes.%2520We%2520leverage%2520LLMs%2520to%2520get%250Aindividual%2520object%2520information%2520and%2520their%25203D%2520bounding%2520boxes%2520as%2520the%2520initial%2520step%250Aof%2520our%2520process.%2520Then%2520with%2520the%2520bounding%2520boxes%252C%2520We%2520further%2520propose%2520clipped%2520ray%250Asampling%2520and%2520object-centric%2520density%2520blob%2520bias%2520to%2520generate%25203D%2520objects%2520aligning%250Awith%2520the%2520bounding%2520boxes.%2520We%2520show%2520that%2520our%2520method%2520exhibits%2520remarkable%250Aadaptability%2520across%2520a%2520spectrum%2520of%2520mainstream%2520Score%2520Distillation%2520Sampling-based%250A3D%2520generation%2520frameworks%2520and%2520our%2520pipeline%2520can%2520even%2520used%2520to%2520insert%2520objects%2520into%250Aan%2520existing%2520NeRF%2520scene.%2520Moreover%252C%2520we%2520also%2520provide%2520a%2520dataset%2520of%2520prompts%2520with%25203D%250Abounding%2520boxes%252C%2520benchmarking%25203D%2520spatial%2520controllability.%2520With%2520extensive%250Aqualitative%2520and%2520quantitative%2520experiments%252C%2520we%2520demonstrate%2520that%2520LucidDreaming%250Aachieves%2520superior%2520results%2520in%2520object%2520placement%2520precision%2520and%2520generation%2520fidelity%250Acompared%2520to%2520current%2520approaches%252C%2520while%2520maintaining%2520flexibility%2520and%2520ease%2520of%2520use%250Afor%2520non-expert%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00588v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LucidDreaming%3A%20Controllable%20Object-Centric%203D%20Generation&entry.906535625=Zhaoning%20Wang%20and%20Ming%20Li%20and%20Chen%20Chen&entry.1292438233=%20%20With%20the%20recent%20development%20of%20generative%20models%2C%20Text-to-3D%20generations%20have%0Aalso%20seen%20significant%20growth%2C%20opening%20a%20door%20for%20creating%20video-game%203D%20assets%0Afrom%20a%20more%20general%20public.%20Nonetheless%2C%20people%20without%20any%20professional%203D%0Aediting%20experience%20would%20find%20it%20hard%20to%20achieve%20precise%20control%20over%20the%203D%0Ageneration%2C%20especially%20if%20there%20are%20multiple%20objects%20in%20the%20prompt%2C%20as%20using%0Atext%20to%20control%20often%20leads%20to%20missing%20objects%20and%20imprecise%20locations.%20In%20this%0Apaper%2C%20we%20present%20LucidDreaming%20as%20an%20effective%20pipeline%20capable%20of%20spatial%20and%0Anumerical%20control%20over%203D%20generation%20from%20only%20textual%20prompt%20commands%20or%203D%0Abounding%20boxes.%20Specifically%2C%20our%20research%20demonstrates%20that%20Large%20Language%0AModels%20%28LLMs%29%20possess%203D%20spatial%20awareness%20and%20can%20effectively%20translate%0Atextual%203D%20information%20into%20precise%203D%20bounding%20boxes.%20We%20leverage%20LLMs%20to%20get%0Aindividual%20object%20information%20and%20their%203D%20bounding%20boxes%20as%20the%20initial%20step%0Aof%20our%20process.%20Then%20with%20the%20bounding%20boxes%2C%20We%20further%20propose%20clipped%20ray%0Asampling%20and%20object-centric%20density%20blob%20bias%20to%20generate%203D%20objects%20aligning%0Awith%20the%20bounding%20boxes.%20We%20show%20that%20our%20method%20exhibits%20remarkable%0Aadaptability%20across%20a%20spectrum%20of%20mainstream%20Score%20Distillation%20Sampling-based%0A3D%20generation%20frameworks%20and%20our%20pipeline%20can%20even%20used%20to%20insert%20objects%20into%0Aan%20existing%20NeRF%20scene.%20Moreover%2C%20we%20also%20provide%20a%20dataset%20of%20prompts%20with%203D%0Abounding%20boxes%2C%20benchmarking%203D%20spatial%20controllability.%20With%20extensive%0Aqualitative%20and%20quantitative%20experiments%2C%20we%20demonstrate%20that%20LucidDreaming%0Aachieves%20superior%20results%20in%20object%20placement%20precision%20and%20generation%20fidelity%0Acompared%20to%20current%20approaches%2C%20while%20maintaining%20flexibility%20and%20ease%20of%20use%0Afor%20non-expert%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00588v2&entry.124074799=Read"},
{"title": "Bootstrap Latents of Nodes and Neighbors for Graph Self-Supervised\n  Learning", "author": "Yunhui Liu and Huaisong Zhang and Tieke He and Tao Zheng and Jianhua Zhao", "abstract": "  Contrastive learning is a significant paradigm in graph self-supervised\nlearning. However, it requires negative samples to prevent model collapse and\nlearn discriminative representations. These negative samples inevitably lead to\nheavy computation, memory overhead and class collision, compromising the\nrepresentation learning. Recent studies present that methods obviating negative\nsamples can attain competitive performance and scalability enhancements,\nexemplified by bootstrapped graph latents (BGRL). However, BGRL neglects the\ninherent graph homophily, which provides valuable insights into underlying\npositive pairs. Our motivation arises from the observation that subtly\nintroducing a few ground-truth positive pairs significantly improves BGRL.\nAlthough we can't obtain ground-truth positive pairs without labels under the\nself-supervised setting, edges in the graph can reflect noisy positive pairs,\ni.e., neighboring nodes often share the same label. Therefore, we propose to\nexpand the positive pair set with node-neighbor pairs. Subsequently, we\nintroduce a cross-attention module to predict the supportiveness score of a\nneighbor with respect to the anchor node. This score quantifies the positive\nsupport from each neighboring node, and is encoded into the training objective.\nConsequently, our method mitigates class collision from negative and noisy\npositive samples, concurrently enhancing intra-class compactness. Extensive\nexperiments are conducted on five benchmark datasets and three downstream task\nnode classification, node clustering, and node similarity search. The results\ndemonstrate that our method generates node representations with enhanced\nintra-class compactness and achieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2408.05087v1", "date": "2024-08-09", "relevancy": 2.779, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6121}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5376}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrap%20Latents%20of%20Nodes%20and%20Neighbors%20for%20Graph%20Self-Supervised%0A%20%20Learning&body=Title%3A%20Bootstrap%20Latents%20of%20Nodes%20and%20Neighbors%20for%20Graph%20Self-Supervised%0A%20%20Learning%0AAuthor%3A%20Yunhui%20Liu%20and%20Huaisong%20Zhang%20and%20Tieke%20He%20and%20Tao%20Zheng%20and%20Jianhua%20Zhao%0AAbstract%3A%20%20%20Contrastive%20learning%20is%20a%20significant%20paradigm%20in%20graph%20self-supervised%0Alearning.%20However%2C%20it%20requires%20negative%20samples%20to%20prevent%20model%20collapse%20and%0Alearn%20discriminative%20representations.%20These%20negative%20samples%20inevitably%20lead%20to%0Aheavy%20computation%2C%20memory%20overhead%20and%20class%20collision%2C%20compromising%20the%0Arepresentation%20learning.%20Recent%20studies%20present%20that%20methods%20obviating%20negative%0Asamples%20can%20attain%20competitive%20performance%20and%20scalability%20enhancements%2C%0Aexemplified%20by%20bootstrapped%20graph%20latents%20%28BGRL%29.%20However%2C%20BGRL%20neglects%20the%0Ainherent%20graph%20homophily%2C%20which%20provides%20valuable%20insights%20into%20underlying%0Apositive%20pairs.%20Our%20motivation%20arises%20from%20the%20observation%20that%20subtly%0Aintroducing%20a%20few%20ground-truth%20positive%20pairs%20significantly%20improves%20BGRL.%0AAlthough%20we%20can%27t%20obtain%20ground-truth%20positive%20pairs%20without%20labels%20under%20the%0Aself-supervised%20setting%2C%20edges%20in%20the%20graph%20can%20reflect%20noisy%20positive%20pairs%2C%0Ai.e.%2C%20neighboring%20nodes%20often%20share%20the%20same%20label.%20Therefore%2C%20we%20propose%20to%0Aexpand%20the%20positive%20pair%20set%20with%20node-neighbor%20pairs.%20Subsequently%2C%20we%0Aintroduce%20a%20cross-attention%20module%20to%20predict%20the%20supportiveness%20score%20of%20a%0Aneighbor%20with%20respect%20to%20the%20anchor%20node.%20This%20score%20quantifies%20the%20positive%0Asupport%20from%20each%20neighboring%20node%2C%20and%20is%20encoded%20into%20the%20training%20objective.%0AConsequently%2C%20our%20method%20mitigates%20class%20collision%20from%20negative%20and%20noisy%0Apositive%20samples%2C%20concurrently%20enhancing%20intra-class%20compactness.%20Extensive%0Aexperiments%20are%20conducted%20on%20five%20benchmark%20datasets%20and%20three%20downstream%20task%0Anode%20classification%2C%20node%20clustering%2C%20and%20node%20similarity%20search.%20The%20results%0Ademonstrate%20that%20our%20method%20generates%20node%20representations%20with%20enhanced%0Aintra-class%20compactness%20and%20achieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrap%2520Latents%2520of%2520Nodes%2520and%2520Neighbors%2520for%2520Graph%2520Self-Supervised%250A%2520%2520Learning%26entry.906535625%3DYunhui%2520Liu%2520and%2520Huaisong%2520Zhang%2520and%2520Tieke%2520He%2520and%2520Tao%2520Zheng%2520and%2520Jianhua%2520Zhao%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520is%2520a%2520significant%2520paradigm%2520in%2520graph%2520self-supervised%250Alearning.%2520However%252C%2520it%2520requires%2520negative%2520samples%2520to%2520prevent%2520model%2520collapse%2520and%250Alearn%2520discriminative%2520representations.%2520These%2520negative%2520samples%2520inevitably%2520lead%2520to%250Aheavy%2520computation%252C%2520memory%2520overhead%2520and%2520class%2520collision%252C%2520compromising%2520the%250Arepresentation%2520learning.%2520Recent%2520studies%2520present%2520that%2520methods%2520obviating%2520negative%250Asamples%2520can%2520attain%2520competitive%2520performance%2520and%2520scalability%2520enhancements%252C%250Aexemplified%2520by%2520bootstrapped%2520graph%2520latents%2520%2528BGRL%2529.%2520However%252C%2520BGRL%2520neglects%2520the%250Ainherent%2520graph%2520homophily%252C%2520which%2520provides%2520valuable%2520insights%2520into%2520underlying%250Apositive%2520pairs.%2520Our%2520motivation%2520arises%2520from%2520the%2520observation%2520that%2520subtly%250Aintroducing%2520a%2520few%2520ground-truth%2520positive%2520pairs%2520significantly%2520improves%2520BGRL.%250AAlthough%2520we%2520can%2527t%2520obtain%2520ground-truth%2520positive%2520pairs%2520without%2520labels%2520under%2520the%250Aself-supervised%2520setting%252C%2520edges%2520in%2520the%2520graph%2520can%2520reflect%2520noisy%2520positive%2520pairs%252C%250Ai.e.%252C%2520neighboring%2520nodes%2520often%2520share%2520the%2520same%2520label.%2520Therefore%252C%2520we%2520propose%2520to%250Aexpand%2520the%2520positive%2520pair%2520set%2520with%2520node-neighbor%2520pairs.%2520Subsequently%252C%2520we%250Aintroduce%2520a%2520cross-attention%2520module%2520to%2520predict%2520the%2520supportiveness%2520score%2520of%2520a%250Aneighbor%2520with%2520respect%2520to%2520the%2520anchor%2520node.%2520This%2520score%2520quantifies%2520the%2520positive%250Asupport%2520from%2520each%2520neighboring%2520node%252C%2520and%2520is%2520encoded%2520into%2520the%2520training%2520objective.%250AConsequently%252C%2520our%2520method%2520mitigates%2520class%2520collision%2520from%2520negative%2520and%2520noisy%250Apositive%2520samples%252C%2520concurrently%2520enhancing%2520intra-class%2520compactness.%2520Extensive%250Aexperiments%2520are%2520conducted%2520on%2520five%2520benchmark%2520datasets%2520and%2520three%2520downstream%2520task%250Anode%2520classification%252C%2520node%2520clustering%252C%2520and%2520node%2520similarity%2520search.%2520The%2520results%250Ademonstrate%2520that%2520our%2520method%2520generates%2520node%2520representations%2520with%2520enhanced%250Aintra-class%2520compactness%2520and%2520achieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrap%20Latents%20of%20Nodes%20and%20Neighbors%20for%20Graph%20Self-Supervised%0A%20%20Learning&entry.906535625=Yunhui%20Liu%20and%20Huaisong%20Zhang%20and%20Tieke%20He%20and%20Tao%20Zheng%20and%20Jianhua%20Zhao&entry.1292438233=%20%20Contrastive%20learning%20is%20a%20significant%20paradigm%20in%20graph%20self-supervised%0Alearning.%20However%2C%20it%20requires%20negative%20samples%20to%20prevent%20model%20collapse%20and%0Alearn%20discriminative%20representations.%20These%20negative%20samples%20inevitably%20lead%20to%0Aheavy%20computation%2C%20memory%20overhead%20and%20class%20collision%2C%20compromising%20the%0Arepresentation%20learning.%20Recent%20studies%20present%20that%20methods%20obviating%20negative%0Asamples%20can%20attain%20competitive%20performance%20and%20scalability%20enhancements%2C%0Aexemplified%20by%20bootstrapped%20graph%20latents%20%28BGRL%29.%20However%2C%20BGRL%20neglects%20the%0Ainherent%20graph%20homophily%2C%20which%20provides%20valuable%20insights%20into%20underlying%0Apositive%20pairs.%20Our%20motivation%20arises%20from%20the%20observation%20that%20subtly%0Aintroducing%20a%20few%20ground-truth%20positive%20pairs%20significantly%20improves%20BGRL.%0AAlthough%20we%20can%27t%20obtain%20ground-truth%20positive%20pairs%20without%20labels%20under%20the%0Aself-supervised%20setting%2C%20edges%20in%20the%20graph%20can%20reflect%20noisy%20positive%20pairs%2C%0Ai.e.%2C%20neighboring%20nodes%20often%20share%20the%20same%20label.%20Therefore%2C%20we%20propose%20to%0Aexpand%20the%20positive%20pair%20set%20with%20node-neighbor%20pairs.%20Subsequently%2C%20we%0Aintroduce%20a%20cross-attention%20module%20to%20predict%20the%20supportiveness%20score%20of%20a%0Aneighbor%20with%20respect%20to%20the%20anchor%20node.%20This%20score%20quantifies%20the%20positive%0Asupport%20from%20each%20neighboring%20node%2C%20and%20is%20encoded%20into%20the%20training%20objective.%0AConsequently%2C%20our%20method%20mitigates%20class%20collision%20from%20negative%20and%20noisy%0Apositive%20samples%2C%20concurrently%20enhancing%20intra-class%20compactness.%20Extensive%0Aexperiments%20are%20conducted%20on%20five%20benchmark%20datasets%20and%20three%20downstream%20task%0Anode%20classification%2C%20node%20clustering%2C%20and%20node%20similarity%20search.%20The%20results%0Ademonstrate%20that%20our%20method%20generates%20node%20representations%20with%20enhanced%0Aintra-class%20compactness%20and%20achieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05087v1&entry.124074799=Read"},
{"title": "IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language\n  Models", "author": "Haz Sameen Shahgir and Khondker Salman Sayeed and Abhik Bhattacharjee and Wasi Uddin Ahmad and Yue Dong and Rifat Shahriyar", "abstract": "  The advent of Vision Language Models (VLM) has allowed researchers to\ninvestigate the visual understanding of a neural network using natural\nlanguage. Beyond object classification and detection, VLMs are capable of\nvisual comprehension and common-sense reasoning. This naturally led to the\nquestion: How do VLMs respond when the image itself is inherently unreasonable?\nTo this end, we present IllusionVQA: a diverse dataset of challenging optical\nillusions and hard-to-interpret scenes to test the capability of VLMs in two\ndistinct multiple-choice VQA tasks - comprehension and soft localization.\nGPT4V, the best performing VLM, achieves 62.99% accuracy (4-shot) on the\ncomprehension task and 49.7% on the localization task (4-shot and\nChain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100%\naccuracy in comprehension and localization. We discover that In-Context\nLearning (ICL) and Chain-of-Thought reasoning substantially degrade the\nperformance of Gemini-Pro in the localization task. Tangentially, we discover a\npotential weakness in the ICL capabilities of VLMs: they fail to locate optical\nillusions even when the correct answer is in the context window as a few-shot\nexample.\n", "link": "http://arxiv.org/abs/2403.15952v3", "date": "2024-08-09", "relevancy": 2.686, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5616}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5366}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IllusionVQA%3A%20A%20Challenging%20Optical%20Illusion%20Dataset%20for%20Vision%20Language%0A%20%20Models&body=Title%3A%20IllusionVQA%3A%20A%20Challenging%20Optical%20Illusion%20Dataset%20for%20Vision%20Language%0A%20%20Models%0AAuthor%3A%20Haz%20Sameen%20Shahgir%20and%20Khondker%20Salman%20Sayeed%20and%20Abhik%20Bhattacharjee%20and%20Wasi%20Uddin%20Ahmad%20and%20Yue%20Dong%20and%20Rifat%20Shahriyar%0AAbstract%3A%20%20%20The%20advent%20of%20Vision%20Language%20Models%20%28VLM%29%20has%20allowed%20researchers%20to%0Ainvestigate%20the%20visual%20understanding%20of%20a%20neural%20network%20using%20natural%0Alanguage.%20Beyond%20object%20classification%20and%20detection%2C%20VLMs%20are%20capable%20of%0Avisual%20comprehension%20and%20common-sense%20reasoning.%20This%20naturally%20led%20to%20the%0Aquestion%3A%20How%20do%20VLMs%20respond%20when%20the%20image%20itself%20is%20inherently%20unreasonable%3F%0ATo%20this%20end%2C%20we%20present%20IllusionVQA%3A%20a%20diverse%20dataset%20of%20challenging%20optical%0Aillusions%20and%20hard-to-interpret%20scenes%20to%20test%20the%20capability%20of%20VLMs%20in%20two%0Adistinct%20multiple-choice%20VQA%20tasks%20-%20comprehension%20and%20soft%20localization.%0AGPT4V%2C%20the%20best%20performing%20VLM%2C%20achieves%2062.99%25%20accuracy%20%284-shot%29%20on%20the%0Acomprehension%20task%20and%2049.7%25%20on%20the%20localization%20task%20%284-shot%20and%0AChain-of-Thought%29.%20Human%20evaluation%20reveals%20that%20humans%20achieve%2091.03%25%20and%20100%25%0Aaccuracy%20in%20comprehension%20and%20localization.%20We%20discover%20that%20In-Context%0ALearning%20%28ICL%29%20and%20Chain-of-Thought%20reasoning%20substantially%20degrade%20the%0Aperformance%20of%20Gemini-Pro%20in%20the%20localization%20task.%20Tangentially%2C%20we%20discover%20a%0Apotential%20weakness%20in%20the%20ICL%20capabilities%20of%20VLMs%3A%20they%20fail%20to%20locate%20optical%0Aillusions%20even%20when%20the%20correct%20answer%20is%20in%20the%20context%20window%20as%20a%20few-shot%0Aexample.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15952v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIllusionVQA%253A%2520A%2520Challenging%2520Optical%2520Illusion%2520Dataset%2520for%2520Vision%2520Language%250A%2520%2520Models%26entry.906535625%3DHaz%2520Sameen%2520Shahgir%2520and%2520Khondker%2520Salman%2520Sayeed%2520and%2520Abhik%2520Bhattacharjee%2520and%2520Wasi%2520Uddin%2520Ahmad%2520and%2520Yue%2520Dong%2520and%2520Rifat%2520Shahriyar%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Vision%2520Language%2520Models%2520%2528VLM%2529%2520has%2520allowed%2520researchers%2520to%250Ainvestigate%2520the%2520visual%2520understanding%2520of%2520a%2520neural%2520network%2520using%2520natural%250Alanguage.%2520Beyond%2520object%2520classification%2520and%2520detection%252C%2520VLMs%2520are%2520capable%2520of%250Avisual%2520comprehension%2520and%2520common-sense%2520reasoning.%2520This%2520naturally%2520led%2520to%2520the%250Aquestion%253A%2520How%2520do%2520VLMs%2520respond%2520when%2520the%2520image%2520itself%2520is%2520inherently%2520unreasonable%253F%250ATo%2520this%2520end%252C%2520we%2520present%2520IllusionVQA%253A%2520a%2520diverse%2520dataset%2520of%2520challenging%2520optical%250Aillusions%2520and%2520hard-to-interpret%2520scenes%2520to%2520test%2520the%2520capability%2520of%2520VLMs%2520in%2520two%250Adistinct%2520multiple-choice%2520VQA%2520tasks%2520-%2520comprehension%2520and%2520soft%2520localization.%250AGPT4V%252C%2520the%2520best%2520performing%2520VLM%252C%2520achieves%252062.99%2525%2520accuracy%2520%25284-shot%2529%2520on%2520the%250Acomprehension%2520task%2520and%252049.7%2525%2520on%2520the%2520localization%2520task%2520%25284-shot%2520and%250AChain-of-Thought%2529.%2520Human%2520evaluation%2520reveals%2520that%2520humans%2520achieve%252091.03%2525%2520and%2520100%2525%250Aaccuracy%2520in%2520comprehension%2520and%2520localization.%2520We%2520discover%2520that%2520In-Context%250ALearning%2520%2528ICL%2529%2520and%2520Chain-of-Thought%2520reasoning%2520substantially%2520degrade%2520the%250Aperformance%2520of%2520Gemini-Pro%2520in%2520the%2520localization%2520task.%2520Tangentially%252C%2520we%2520discover%2520a%250Apotential%2520weakness%2520in%2520the%2520ICL%2520capabilities%2520of%2520VLMs%253A%2520they%2520fail%2520to%2520locate%2520optical%250Aillusions%2520even%2520when%2520the%2520correct%2520answer%2520is%2520in%2520the%2520context%2520window%2520as%2520a%2520few-shot%250Aexample.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15952v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IllusionVQA%3A%20A%20Challenging%20Optical%20Illusion%20Dataset%20for%20Vision%20Language%0A%20%20Models&entry.906535625=Haz%20Sameen%20Shahgir%20and%20Khondker%20Salman%20Sayeed%20and%20Abhik%20Bhattacharjee%20and%20Wasi%20Uddin%20Ahmad%20and%20Yue%20Dong%20and%20Rifat%20Shahriyar&entry.1292438233=%20%20The%20advent%20of%20Vision%20Language%20Models%20%28VLM%29%20has%20allowed%20researchers%20to%0Ainvestigate%20the%20visual%20understanding%20of%20a%20neural%20network%20using%20natural%0Alanguage.%20Beyond%20object%20classification%20and%20detection%2C%20VLMs%20are%20capable%20of%0Avisual%20comprehension%20and%20common-sense%20reasoning.%20This%20naturally%20led%20to%20the%0Aquestion%3A%20How%20do%20VLMs%20respond%20when%20the%20image%20itself%20is%20inherently%20unreasonable%3F%0ATo%20this%20end%2C%20we%20present%20IllusionVQA%3A%20a%20diverse%20dataset%20of%20challenging%20optical%0Aillusions%20and%20hard-to-interpret%20scenes%20to%20test%20the%20capability%20of%20VLMs%20in%20two%0Adistinct%20multiple-choice%20VQA%20tasks%20-%20comprehension%20and%20soft%20localization.%0AGPT4V%2C%20the%20best%20performing%20VLM%2C%20achieves%2062.99%25%20accuracy%20%284-shot%29%20on%20the%0Acomprehension%20task%20and%2049.7%25%20on%20the%20localization%20task%20%284-shot%20and%0AChain-of-Thought%29.%20Human%20evaluation%20reveals%20that%20humans%20achieve%2091.03%25%20and%20100%25%0Aaccuracy%20in%20comprehension%20and%20localization.%20We%20discover%20that%20In-Context%0ALearning%20%28ICL%29%20and%20Chain-of-Thought%20reasoning%20substantially%20degrade%20the%0Aperformance%20of%20Gemini-Pro%20in%20the%20localization%20task.%20Tangentially%2C%20we%20discover%20a%0Apotential%20weakness%20in%20the%20ICL%20capabilities%20of%20VLMs%3A%20they%20fail%20to%20locate%20optical%0Aillusions%20even%20when%20the%20correct%20answer%20is%20in%20the%20context%20window%20as%20a%20few-shot%0Aexample.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15952v3&entry.124074799=Read"},
{"title": "TetraDiffusion: Tetrahedral Diffusion Models for 3D Shape Generation", "author": "Nikolai Kalischek and Torben Peters and Jan D. Wegner and Konrad Schindler", "abstract": "  Probabilistic denoising diffusion models (DDMs) have set a new standard for\n2D image generation. Extending DDMs for 3D content creation is an active field\nof research. Here, we propose TetraDiffusion, a diffusion model that operates\non a tetrahedral partitioning of 3D space to enable efficient, high-resolution\n3D shape generation. Our model introduces operators for convolution and\ntranspose convolution that act directly on the tetrahedral partition, and\nseamlessly includes additional attributes such as color. Remarkably,\nTetraDiffusion enables rapid sampling of detailed 3D objects in nearly\nreal-time with unprecedented resolution. It's also adaptable for generating 3D\nshapes conditioned on 2D images. Compared to existing 3D mesh diffusion\ntechniques, our method is up to 200 times faster in inference speed, works on\nstandard consumer hardware, and delivers superior results.\n", "link": "http://arxiv.org/abs/2211.13220v3", "date": "2024-08-09", "relevancy": 2.6793, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6771}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6771}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TetraDiffusion%3A%20Tetrahedral%20Diffusion%20Models%20for%203D%20Shape%20Generation&body=Title%3A%20TetraDiffusion%3A%20Tetrahedral%20Diffusion%20Models%20for%203D%20Shape%20Generation%0AAuthor%3A%20Nikolai%20Kalischek%20and%20Torben%20Peters%20and%20Jan%20D.%20Wegner%20and%20Konrad%20Schindler%0AAbstract%3A%20%20%20Probabilistic%20denoising%20diffusion%20models%20%28DDMs%29%20have%20set%20a%20new%20standard%20for%0A2D%20image%20generation.%20Extending%20DDMs%20for%203D%20content%20creation%20is%20an%20active%20field%0Aof%20research.%20Here%2C%20we%20propose%20TetraDiffusion%2C%20a%20diffusion%20model%20that%20operates%0Aon%20a%20tetrahedral%20partitioning%20of%203D%20space%20to%20enable%20efficient%2C%20high-resolution%0A3D%20shape%20generation.%20Our%20model%20introduces%20operators%20for%20convolution%20and%0Atranspose%20convolution%20that%20act%20directly%20on%20the%20tetrahedral%20partition%2C%20and%0Aseamlessly%20includes%20additional%20attributes%20such%20as%20color.%20Remarkably%2C%0ATetraDiffusion%20enables%20rapid%20sampling%20of%20detailed%203D%20objects%20in%20nearly%0Areal-time%20with%20unprecedented%20resolution.%20It%27s%20also%20adaptable%20for%20generating%203D%0Ashapes%20conditioned%20on%202D%20images.%20Compared%20to%20existing%203D%20mesh%20diffusion%0Atechniques%2C%20our%20method%20is%20up%20to%20200%20times%20faster%20in%20inference%20speed%2C%20works%20on%0Astandard%20consumer%20hardware%2C%20and%20delivers%20superior%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.13220v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTetraDiffusion%253A%2520Tetrahedral%2520Diffusion%2520Models%2520for%25203D%2520Shape%2520Generation%26entry.906535625%3DNikolai%2520Kalischek%2520and%2520Torben%2520Peters%2520and%2520Jan%2520D.%2520Wegner%2520and%2520Konrad%2520Schindler%26entry.1292438233%3D%2520%2520Probabilistic%2520denoising%2520diffusion%2520models%2520%2528DDMs%2529%2520have%2520set%2520a%2520new%2520standard%2520for%250A2D%2520image%2520generation.%2520Extending%2520DDMs%2520for%25203D%2520content%2520creation%2520is%2520an%2520active%2520field%250Aof%2520research.%2520Here%252C%2520we%2520propose%2520TetraDiffusion%252C%2520a%2520diffusion%2520model%2520that%2520operates%250Aon%2520a%2520tetrahedral%2520partitioning%2520of%25203D%2520space%2520to%2520enable%2520efficient%252C%2520high-resolution%250A3D%2520shape%2520generation.%2520Our%2520model%2520introduces%2520operators%2520for%2520convolution%2520and%250Atranspose%2520convolution%2520that%2520act%2520directly%2520on%2520the%2520tetrahedral%2520partition%252C%2520and%250Aseamlessly%2520includes%2520additional%2520attributes%2520such%2520as%2520color.%2520Remarkably%252C%250ATetraDiffusion%2520enables%2520rapid%2520sampling%2520of%2520detailed%25203D%2520objects%2520in%2520nearly%250Areal-time%2520with%2520unprecedented%2520resolution.%2520It%2527s%2520also%2520adaptable%2520for%2520generating%25203D%250Ashapes%2520conditioned%2520on%25202D%2520images.%2520Compared%2520to%2520existing%25203D%2520mesh%2520diffusion%250Atechniques%252C%2520our%2520method%2520is%2520up%2520to%2520200%2520times%2520faster%2520in%2520inference%2520speed%252C%2520works%2520on%250Astandard%2520consumer%2520hardware%252C%2520and%2520delivers%2520superior%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.13220v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TetraDiffusion%3A%20Tetrahedral%20Diffusion%20Models%20for%203D%20Shape%20Generation&entry.906535625=Nikolai%20Kalischek%20and%20Torben%20Peters%20and%20Jan%20D.%20Wegner%20and%20Konrad%20Schindler&entry.1292438233=%20%20Probabilistic%20denoising%20diffusion%20models%20%28DDMs%29%20have%20set%20a%20new%20standard%20for%0A2D%20image%20generation.%20Extending%20DDMs%20for%203D%20content%20creation%20is%20an%20active%20field%0Aof%20research.%20Here%2C%20we%20propose%20TetraDiffusion%2C%20a%20diffusion%20model%20that%20operates%0Aon%20a%20tetrahedral%20partitioning%20of%203D%20space%20to%20enable%20efficient%2C%20high-resolution%0A3D%20shape%20generation.%20Our%20model%20introduces%20operators%20for%20convolution%20and%0Atranspose%20convolution%20that%20act%20directly%20on%20the%20tetrahedral%20partition%2C%20and%0Aseamlessly%20includes%20additional%20attributes%20such%20as%20color.%20Remarkably%2C%0ATetraDiffusion%20enables%20rapid%20sampling%20of%20detailed%203D%20objects%20in%20nearly%0Areal-time%20with%20unprecedented%20resolution.%20It%27s%20also%20adaptable%20for%20generating%203D%0Ashapes%20conditioned%20on%202D%20images.%20Compared%20to%20existing%203D%20mesh%20diffusion%0Atechniques%2C%20our%20method%20is%20up%20to%20200%20times%20faster%20in%20inference%20speed%2C%20works%20on%0Astandard%20consumer%20hardware%2C%20and%20delivers%20superior%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.13220v3&entry.124074799=Read"},
{"title": "Collaborative Static-Dynamic Teaching: A Semi-Supervised Framework for\n  Stripe-Like Space Target Detection", "author": "Zijian Zhu and Ali Zia and Xuesong Li and Bingbing Dan and Yuebo Ma and Hongfeng Long and Kaili Lu and Enhai Liu and Rujin Zhao", "abstract": "  Stripe-like space target detection (SSTD) is crucial for space situational\nawareness. Traditional unsupervised methods often fail in low signal-to-noise\nratio and variable stripe-like space targets scenarios, leading to weak\ngeneralization. Although fully supervised learning methods improve model\ngeneralization, they require extensive pixel-level labels for training. In the\nSSTD task, manually creating these labels is often inaccurate and\nlabor-intensive. Semi-supervised learning (SSL) methods reduce the need for\nthese labels and enhance model generalizability, but their performance is\nlimited by pseudo-label quality. To address this, we introduce an innovative\nCollaborative Static-Dynamic Teacher (CSDT) SSL framework, which includes\nstatic and dynamic teacher models as well as a student model. This framework\nemploys a customized adaptive pseudo-labeling (APL) strategy, transitioning\nfrom initial static teaching to adaptive collaborative teaching, guiding the\nstudent model's training. The exponential moving average (EMA) mechanism\nfurther enhances this process by feeding new stripe-like knowledge back to the\ndynamic teacher model through the student model, creating a positive feedback\nloop that continuously enhances the quality of pseudo-labels. Moreover, we\npresent MSSA-Net, a novel SSTD network featuring a multi-scale dual-path\nconvolution (MDPC) block and a feature map weighted attention (FMWA) block,\ndesigned to extract diverse stripe-like features within the CSDT SSL training\nframework. Extensive experiments verify the state-of-the-art performance of our\nframework on the AstroStripeSet and various ground-based and space-based\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2408.05029v1", "date": "2024-08-09", "relevancy": 2.6736, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5608}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.533}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Static-Dynamic%20Teaching%3A%20A%20Semi-Supervised%20Framework%20for%0A%20%20Stripe-Like%20Space%20Target%20Detection&body=Title%3A%20Collaborative%20Static-Dynamic%20Teaching%3A%20A%20Semi-Supervised%20Framework%20for%0A%20%20Stripe-Like%20Space%20Target%20Detection%0AAuthor%3A%20Zijian%20Zhu%20and%20Ali%20Zia%20and%20Xuesong%20Li%20and%20Bingbing%20Dan%20and%20Yuebo%20Ma%20and%20Hongfeng%20Long%20and%20Kaili%20Lu%20and%20Enhai%20Liu%20and%20Rujin%20Zhao%0AAbstract%3A%20%20%20Stripe-like%20space%20target%20detection%20%28SSTD%29%20is%20crucial%20for%20space%20situational%0Aawareness.%20Traditional%20unsupervised%20methods%20often%20fail%20in%20low%20signal-to-noise%0Aratio%20and%20variable%20stripe-like%20space%20targets%20scenarios%2C%20leading%20to%20weak%0Ageneralization.%20Although%20fully%20supervised%20learning%20methods%20improve%20model%0Ageneralization%2C%20they%20require%20extensive%20pixel-level%20labels%20for%20training.%20In%20the%0ASSTD%20task%2C%20manually%20creating%20these%20labels%20is%20often%20inaccurate%20and%0Alabor-intensive.%20Semi-supervised%20learning%20%28SSL%29%20methods%20reduce%20the%20need%20for%0Athese%20labels%20and%20enhance%20model%20generalizability%2C%20but%20their%20performance%20is%0Alimited%20by%20pseudo-label%20quality.%20To%20address%20this%2C%20we%20introduce%20an%20innovative%0ACollaborative%20Static-Dynamic%20Teacher%20%28CSDT%29%20SSL%20framework%2C%20which%20includes%0Astatic%20and%20dynamic%20teacher%20models%20as%20well%20as%20a%20student%20model.%20This%20framework%0Aemploys%20a%20customized%20adaptive%20pseudo-labeling%20%28APL%29%20strategy%2C%20transitioning%0Afrom%20initial%20static%20teaching%20to%20adaptive%20collaborative%20teaching%2C%20guiding%20the%0Astudent%20model%27s%20training.%20The%20exponential%20moving%20average%20%28EMA%29%20mechanism%0Afurther%20enhances%20this%20process%20by%20feeding%20new%20stripe-like%20knowledge%20back%20to%20the%0Adynamic%20teacher%20model%20through%20the%20student%20model%2C%20creating%20a%20positive%20feedback%0Aloop%20that%20continuously%20enhances%20the%20quality%20of%20pseudo-labels.%20Moreover%2C%20we%0Apresent%20MSSA-Net%2C%20a%20novel%20SSTD%20network%20featuring%20a%20multi-scale%20dual-path%0Aconvolution%20%28MDPC%29%20block%20and%20a%20feature%20map%20weighted%20attention%20%28FMWA%29%20block%2C%0Adesigned%20to%20extract%20diverse%20stripe-like%20features%20within%20the%20CSDT%20SSL%20training%0Aframework.%20Extensive%20experiments%20verify%20the%20state-of-the-art%20performance%20of%20our%0Aframework%20on%20the%20AstroStripeSet%20and%20various%20ground-based%20and%20space-based%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Static-Dynamic%2520Teaching%253A%2520A%2520Semi-Supervised%2520Framework%2520for%250A%2520%2520Stripe-Like%2520Space%2520Target%2520Detection%26entry.906535625%3DZijian%2520Zhu%2520and%2520Ali%2520Zia%2520and%2520Xuesong%2520Li%2520and%2520Bingbing%2520Dan%2520and%2520Yuebo%2520Ma%2520and%2520Hongfeng%2520Long%2520and%2520Kaili%2520Lu%2520and%2520Enhai%2520Liu%2520and%2520Rujin%2520Zhao%26entry.1292438233%3D%2520%2520Stripe-like%2520space%2520target%2520detection%2520%2528SSTD%2529%2520is%2520crucial%2520for%2520space%2520situational%250Aawareness.%2520Traditional%2520unsupervised%2520methods%2520often%2520fail%2520in%2520low%2520signal-to-noise%250Aratio%2520and%2520variable%2520stripe-like%2520space%2520targets%2520scenarios%252C%2520leading%2520to%2520weak%250Ageneralization.%2520Although%2520fully%2520supervised%2520learning%2520methods%2520improve%2520model%250Ageneralization%252C%2520they%2520require%2520extensive%2520pixel-level%2520labels%2520for%2520training.%2520In%2520the%250ASSTD%2520task%252C%2520manually%2520creating%2520these%2520labels%2520is%2520often%2520inaccurate%2520and%250Alabor-intensive.%2520Semi-supervised%2520learning%2520%2528SSL%2529%2520methods%2520reduce%2520the%2520need%2520for%250Athese%2520labels%2520and%2520enhance%2520model%2520generalizability%252C%2520but%2520their%2520performance%2520is%250Alimited%2520by%2520pseudo-label%2520quality.%2520To%2520address%2520this%252C%2520we%2520introduce%2520an%2520innovative%250ACollaborative%2520Static-Dynamic%2520Teacher%2520%2528CSDT%2529%2520SSL%2520framework%252C%2520which%2520includes%250Astatic%2520and%2520dynamic%2520teacher%2520models%2520as%2520well%2520as%2520a%2520student%2520model.%2520This%2520framework%250Aemploys%2520a%2520customized%2520adaptive%2520pseudo-labeling%2520%2528APL%2529%2520strategy%252C%2520transitioning%250Afrom%2520initial%2520static%2520teaching%2520to%2520adaptive%2520collaborative%2520teaching%252C%2520guiding%2520the%250Astudent%2520model%2527s%2520training.%2520The%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520mechanism%250Afurther%2520enhances%2520this%2520process%2520by%2520feeding%2520new%2520stripe-like%2520knowledge%2520back%2520to%2520the%250Adynamic%2520teacher%2520model%2520through%2520the%2520student%2520model%252C%2520creating%2520a%2520positive%2520feedback%250Aloop%2520that%2520continuously%2520enhances%2520the%2520quality%2520of%2520pseudo-labels.%2520Moreover%252C%2520we%250Apresent%2520MSSA-Net%252C%2520a%2520novel%2520SSTD%2520network%2520featuring%2520a%2520multi-scale%2520dual-path%250Aconvolution%2520%2528MDPC%2529%2520block%2520and%2520a%2520feature%2520map%2520weighted%2520attention%2520%2528FMWA%2529%2520block%252C%250Adesigned%2520to%2520extract%2520diverse%2520stripe-like%2520features%2520within%2520the%2520CSDT%2520SSL%2520training%250Aframework.%2520Extensive%2520experiments%2520verify%2520the%2520state-of-the-art%2520performance%2520of%2520our%250Aframework%2520on%2520the%2520AstroStripeSet%2520and%2520various%2520ground-based%2520and%2520space-based%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Static-Dynamic%20Teaching%3A%20A%20Semi-Supervised%20Framework%20for%0A%20%20Stripe-Like%20Space%20Target%20Detection&entry.906535625=Zijian%20Zhu%20and%20Ali%20Zia%20and%20Xuesong%20Li%20and%20Bingbing%20Dan%20and%20Yuebo%20Ma%20and%20Hongfeng%20Long%20and%20Kaili%20Lu%20and%20Enhai%20Liu%20and%20Rujin%20Zhao&entry.1292438233=%20%20Stripe-like%20space%20target%20detection%20%28SSTD%29%20is%20crucial%20for%20space%20situational%0Aawareness.%20Traditional%20unsupervised%20methods%20often%20fail%20in%20low%20signal-to-noise%0Aratio%20and%20variable%20stripe-like%20space%20targets%20scenarios%2C%20leading%20to%20weak%0Ageneralization.%20Although%20fully%20supervised%20learning%20methods%20improve%20model%0Ageneralization%2C%20they%20require%20extensive%20pixel-level%20labels%20for%20training.%20In%20the%0ASSTD%20task%2C%20manually%20creating%20these%20labels%20is%20often%20inaccurate%20and%0Alabor-intensive.%20Semi-supervised%20learning%20%28SSL%29%20methods%20reduce%20the%20need%20for%0Athese%20labels%20and%20enhance%20model%20generalizability%2C%20but%20their%20performance%20is%0Alimited%20by%20pseudo-label%20quality.%20To%20address%20this%2C%20we%20introduce%20an%20innovative%0ACollaborative%20Static-Dynamic%20Teacher%20%28CSDT%29%20SSL%20framework%2C%20which%20includes%0Astatic%20and%20dynamic%20teacher%20models%20as%20well%20as%20a%20student%20model.%20This%20framework%0Aemploys%20a%20customized%20adaptive%20pseudo-labeling%20%28APL%29%20strategy%2C%20transitioning%0Afrom%20initial%20static%20teaching%20to%20adaptive%20collaborative%20teaching%2C%20guiding%20the%0Astudent%20model%27s%20training.%20The%20exponential%20moving%20average%20%28EMA%29%20mechanism%0Afurther%20enhances%20this%20process%20by%20feeding%20new%20stripe-like%20knowledge%20back%20to%20the%0Adynamic%20teacher%20model%20through%20the%20student%20model%2C%20creating%20a%20positive%20feedback%0Aloop%20that%20continuously%20enhances%20the%20quality%20of%20pseudo-labels.%20Moreover%2C%20we%0Apresent%20MSSA-Net%2C%20a%20novel%20SSTD%20network%20featuring%20a%20multi-scale%20dual-path%0Aconvolution%20%28MDPC%29%20block%20and%20a%20feature%20map%20weighted%20attention%20%28FMWA%29%20block%2C%0Adesigned%20to%20extract%20diverse%20stripe-like%20features%20within%20the%20CSDT%20SSL%20training%0Aframework.%20Extensive%20experiments%20verify%20the%20state-of-the-art%20performance%20of%20our%0Aframework%20on%20the%20AstroStripeSet%20and%20various%20ground-based%20and%20space-based%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05029v1&entry.124074799=Read"},
{"title": "Retinotopic Mapping Enhances the Robustness of Convolutional Neural\n  Networks", "author": "Jean-Nicolas J\u00e9r\u00e9mie and Emmanuel Dauc\u00e9 and Laurent U Perrinet", "abstract": "  Foveated vision, a trait shared by many animals, including humans, has not\nbeen fully utilized in machine learning applications, despite its significant\ncontributions to biological visual function. This study investigates whether\nretinotopic mapping, a critical component of foveated vision, can enhance image\ncategorization and localization performance when integrated into deep\nconvolutional neural networks (CNNs). Retinotopic mapping was integrated into\nthe inputs of standard off-the-shelf convolutional neural networks (CNNs),\nwhich were then retrained on the ImageNet task. As expected, the\nlogarithmic-polar mapping improved the network's ability to handle arbitrary\nimage zooms and rotations, particularly for isolated objects. Surprisingly, the\nretinotopically mapped network achieved comparable performance in\nclassification. Furthermore, the network demonstrated improved classification\nlocalization when the foveated center of the transform was shifted. This\nreplicates a crucial ability of the human visual system that is absent in\ntypical convolutional neural networks (CNNs). These findings suggest that\nretinotopic mapping may be fundamental to significant preattentive visual\nprocesses.\n", "link": "http://arxiv.org/abs/2402.15480v2", "date": "2024-08-09", "relevancy": 2.6594, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5868}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retinotopic%20Mapping%20Enhances%20the%20Robustness%20of%20Convolutional%20Neural%0A%20%20Networks&body=Title%3A%20Retinotopic%20Mapping%20Enhances%20the%20Robustness%20of%20Convolutional%20Neural%0A%20%20Networks%0AAuthor%3A%20Jean-Nicolas%20J%C3%A9r%C3%A9mie%20and%20Emmanuel%20Dauc%C3%A9%20and%20Laurent%20U%20Perrinet%0AAbstract%3A%20%20%20Foveated%20vision%2C%20a%20trait%20shared%20by%20many%20animals%2C%20including%20humans%2C%20has%20not%0Abeen%20fully%20utilized%20in%20machine%20learning%20applications%2C%20despite%20its%20significant%0Acontributions%20to%20biological%20visual%20function.%20This%20study%20investigates%20whether%0Aretinotopic%20mapping%2C%20a%20critical%20component%20of%20foveated%20vision%2C%20can%20enhance%20image%0Acategorization%20and%20localization%20performance%20when%20integrated%20into%20deep%0Aconvolutional%20neural%20networks%20%28CNNs%29.%20Retinotopic%20mapping%20was%20integrated%20into%0Athe%20inputs%20of%20standard%20off-the-shelf%20convolutional%20neural%20networks%20%28CNNs%29%2C%0Awhich%20were%20then%20retrained%20on%20the%20ImageNet%20task.%20As%20expected%2C%20the%0Alogarithmic-polar%20mapping%20improved%20the%20network%27s%20ability%20to%20handle%20arbitrary%0Aimage%20zooms%20and%20rotations%2C%20particularly%20for%20isolated%20objects.%20Surprisingly%2C%20the%0Aretinotopically%20mapped%20network%20achieved%20comparable%20performance%20in%0Aclassification.%20Furthermore%2C%20the%20network%20demonstrated%20improved%20classification%0Alocalization%20when%20the%20foveated%20center%20of%20the%20transform%20was%20shifted.%20This%0Areplicates%20a%20crucial%20ability%20of%20the%20human%20visual%20system%20that%20is%20absent%20in%0Atypical%20convolutional%20neural%20networks%20%28CNNs%29.%20These%20findings%20suggest%20that%0Aretinotopic%20mapping%20may%20be%20fundamental%20to%20significant%20preattentive%20visual%0Aprocesses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetinotopic%2520Mapping%2520Enhances%2520the%2520Robustness%2520of%2520Convolutional%2520Neural%250A%2520%2520Networks%26entry.906535625%3DJean-Nicolas%2520J%25C3%25A9r%25C3%25A9mie%2520and%2520Emmanuel%2520Dauc%25C3%25A9%2520and%2520Laurent%2520U%2520Perrinet%26entry.1292438233%3D%2520%2520Foveated%2520vision%252C%2520a%2520trait%2520shared%2520by%2520many%2520animals%252C%2520including%2520humans%252C%2520has%2520not%250Abeen%2520fully%2520utilized%2520in%2520machine%2520learning%2520applications%252C%2520despite%2520its%2520significant%250Acontributions%2520to%2520biological%2520visual%2520function.%2520This%2520study%2520investigates%2520whether%250Aretinotopic%2520mapping%252C%2520a%2520critical%2520component%2520of%2520foveated%2520vision%252C%2520can%2520enhance%2520image%250Acategorization%2520and%2520localization%2520performance%2520when%2520integrated%2520into%2520deep%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529.%2520Retinotopic%2520mapping%2520was%2520integrated%2520into%250Athe%2520inputs%2520of%2520standard%2520off-the-shelf%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%250Awhich%2520were%2520then%2520retrained%2520on%2520the%2520ImageNet%2520task.%2520As%2520expected%252C%2520the%250Alogarithmic-polar%2520mapping%2520improved%2520the%2520network%2527s%2520ability%2520to%2520handle%2520arbitrary%250Aimage%2520zooms%2520and%2520rotations%252C%2520particularly%2520for%2520isolated%2520objects.%2520Surprisingly%252C%2520the%250Aretinotopically%2520mapped%2520network%2520achieved%2520comparable%2520performance%2520in%250Aclassification.%2520Furthermore%252C%2520the%2520network%2520demonstrated%2520improved%2520classification%250Alocalization%2520when%2520the%2520foveated%2520center%2520of%2520the%2520transform%2520was%2520shifted.%2520This%250Areplicates%2520a%2520crucial%2520ability%2520of%2520the%2520human%2520visual%2520system%2520that%2520is%2520absent%2520in%250Atypical%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529.%2520These%2520findings%2520suggest%2520that%250Aretinotopic%2520mapping%2520may%2520be%2520fundamental%2520to%2520significant%2520preattentive%2520visual%250Aprocesses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retinotopic%20Mapping%20Enhances%20the%20Robustness%20of%20Convolutional%20Neural%0A%20%20Networks&entry.906535625=Jean-Nicolas%20J%C3%A9r%C3%A9mie%20and%20Emmanuel%20Dauc%C3%A9%20and%20Laurent%20U%20Perrinet&entry.1292438233=%20%20Foveated%20vision%2C%20a%20trait%20shared%20by%20many%20animals%2C%20including%20humans%2C%20has%20not%0Abeen%20fully%20utilized%20in%20machine%20learning%20applications%2C%20despite%20its%20significant%0Acontributions%20to%20biological%20visual%20function.%20This%20study%20investigates%20whether%0Aretinotopic%20mapping%2C%20a%20critical%20component%20of%20foveated%20vision%2C%20can%20enhance%20image%0Acategorization%20and%20localization%20performance%20when%20integrated%20into%20deep%0Aconvolutional%20neural%20networks%20%28CNNs%29.%20Retinotopic%20mapping%20was%20integrated%20into%0Athe%20inputs%20of%20standard%20off-the-shelf%20convolutional%20neural%20networks%20%28CNNs%29%2C%0Awhich%20were%20then%20retrained%20on%20the%20ImageNet%20task.%20As%20expected%2C%20the%0Alogarithmic-polar%20mapping%20improved%20the%20network%27s%20ability%20to%20handle%20arbitrary%0Aimage%20zooms%20and%20rotations%2C%20particularly%20for%20isolated%20objects.%20Surprisingly%2C%20the%0Aretinotopically%20mapped%20network%20achieved%20comparable%20performance%20in%0Aclassification.%20Furthermore%2C%20the%20network%20demonstrated%20improved%20classification%0Alocalization%20when%20the%20foveated%20center%20of%20the%20transform%20was%20shifted.%20This%0Areplicates%20a%20crucial%20ability%20of%20the%20human%20visual%20system%20that%20is%20absent%20in%0Atypical%20convolutional%20neural%20networks%20%28CNNs%29.%20These%20findings%20suggest%20that%0Aretinotopic%20mapping%20may%20be%20fundamental%20to%20significant%20preattentive%20visual%0Aprocesses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15480v2&entry.124074799=Read"},
{"title": "Enriching thermal point clouds of buildings using semantic 3D building\n  models", "author": "Jingwei Zhu and Olaf Wysocki and Christoph Holst and Thomas H. Kolbe", "abstract": "  Thermal point clouds integrate thermal radiation and laser point clouds\neffectively. However, the semantic information for the interpretation of\nbuilding thermal point clouds can hardly be precisely inferred. Transferring\nthe semantics encapsulated in 3D building models at LoD3 has a potential to\nfill this gap. In this work, we propose a workflow enriching thermal point\nclouds with the geo-position and semantics of LoD3 building models, which\nutilizes features of both modalities: The proposed method can automatically\nco-register the point clouds from different sources and enrich the thermal\npoint cloud in facade-detailed semantics. The enriched thermal point cloud\nsupports thermal analysis and can facilitate the development of currently\nscarce deep learning models operating directly on thermal point clouds.\n", "link": "http://arxiv.org/abs/2407.21436v2", "date": "2024-08-09", "relevancy": 2.6574, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5358}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5293}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enriching%20thermal%20point%20clouds%20of%20buildings%20using%20semantic%203D%20building%0A%20%20models&body=Title%3A%20Enriching%20thermal%20point%20clouds%20of%20buildings%20using%20semantic%203D%20building%0A%20%20models%0AAuthor%3A%20Jingwei%20Zhu%20and%20Olaf%20Wysocki%20and%20Christoph%20Holst%20and%20Thomas%20H.%20Kolbe%0AAbstract%3A%20%20%20Thermal%20point%20clouds%20integrate%20thermal%20radiation%20and%20laser%20point%20clouds%0Aeffectively.%20However%2C%20the%20semantic%20information%20for%20the%20interpretation%20of%0Abuilding%20thermal%20point%20clouds%20can%20hardly%20be%20precisely%20inferred.%20Transferring%0Athe%20semantics%20encapsulated%20in%203D%20building%20models%20at%20LoD3%20has%20a%20potential%20to%0Afill%20this%20gap.%20In%20this%20work%2C%20we%20propose%20a%20workflow%20enriching%20thermal%20point%0Aclouds%20with%20the%20geo-position%20and%20semantics%20of%20LoD3%20building%20models%2C%20which%0Autilizes%20features%20of%20both%20modalities%3A%20The%20proposed%20method%20can%20automatically%0Aco-register%20the%20point%20clouds%20from%20different%20sources%20and%20enrich%20the%20thermal%0Apoint%20cloud%20in%20facade-detailed%20semantics.%20The%20enriched%20thermal%20point%20cloud%0Asupports%20thermal%20analysis%20and%20can%20facilitate%20the%20development%20of%20currently%0Ascarce%20deep%20learning%20models%20operating%20directly%20on%20thermal%20point%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnriching%2520thermal%2520point%2520clouds%2520of%2520buildings%2520using%2520semantic%25203D%2520building%250A%2520%2520models%26entry.906535625%3DJingwei%2520Zhu%2520and%2520Olaf%2520Wysocki%2520and%2520Christoph%2520Holst%2520and%2520Thomas%2520H.%2520Kolbe%26entry.1292438233%3D%2520%2520Thermal%2520point%2520clouds%2520integrate%2520thermal%2520radiation%2520and%2520laser%2520point%2520clouds%250Aeffectively.%2520However%252C%2520the%2520semantic%2520information%2520for%2520the%2520interpretation%2520of%250Abuilding%2520thermal%2520point%2520clouds%2520can%2520hardly%2520be%2520precisely%2520inferred.%2520Transferring%250Athe%2520semantics%2520encapsulated%2520in%25203D%2520building%2520models%2520at%2520LoD3%2520has%2520a%2520potential%2520to%250Afill%2520this%2520gap.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520workflow%2520enriching%2520thermal%2520point%250Aclouds%2520with%2520the%2520geo-position%2520and%2520semantics%2520of%2520LoD3%2520building%2520models%252C%2520which%250Autilizes%2520features%2520of%2520both%2520modalities%253A%2520The%2520proposed%2520method%2520can%2520automatically%250Aco-register%2520the%2520point%2520clouds%2520from%2520different%2520sources%2520and%2520enrich%2520the%2520thermal%250Apoint%2520cloud%2520in%2520facade-detailed%2520semantics.%2520The%2520enriched%2520thermal%2520point%2520cloud%250Asupports%2520thermal%2520analysis%2520and%2520can%2520facilitate%2520the%2520development%2520of%2520currently%250Ascarce%2520deep%2520learning%2520models%2520operating%2520directly%2520on%2520thermal%2520point%2520clouds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enriching%20thermal%20point%20clouds%20of%20buildings%20using%20semantic%203D%20building%0A%20%20models&entry.906535625=Jingwei%20Zhu%20and%20Olaf%20Wysocki%20and%20Christoph%20Holst%20and%20Thomas%20H.%20Kolbe&entry.1292438233=%20%20Thermal%20point%20clouds%20integrate%20thermal%20radiation%20and%20laser%20point%20clouds%0Aeffectively.%20However%2C%20the%20semantic%20information%20for%20the%20interpretation%20of%0Abuilding%20thermal%20point%20clouds%20can%20hardly%20be%20precisely%20inferred.%20Transferring%0Athe%20semantics%20encapsulated%20in%203D%20building%20models%20at%20LoD3%20has%20a%20potential%20to%0Afill%20this%20gap.%20In%20this%20work%2C%20we%20propose%20a%20workflow%20enriching%20thermal%20point%0Aclouds%20with%20the%20geo-position%20and%20semantics%20of%20LoD3%20building%20models%2C%20which%0Autilizes%20features%20of%20both%20modalities%3A%20The%20proposed%20method%20can%20automatically%0Aco-register%20the%20point%20clouds%20from%20different%20sources%20and%20enrich%20the%20thermal%0Apoint%20cloud%20in%20facade-detailed%20semantics.%20The%20enriched%20thermal%20point%20cloud%0Asupports%20thermal%20analysis%20and%20can%20facilitate%20the%20development%20of%20currently%0Ascarce%20deep%20learning%20models%20operating%20directly%20on%20thermal%20point%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21436v2&entry.124074799=Read"},
{"title": "Multi-Garment Customized Model Generation", "author": "Yichen Liu and Penghui Du and Yi Liu Quanwei Zhang", "abstract": "  This paper introduces Multi-Garment Customized Model Generation, a unified\nframework based on Latent Diffusion Models (LDMs) aimed at addressing the\nunexplored task of synthesizing images with free combinations of multiple\npieces of clothing. The method focuses on generating customized models wearing\nvarious targeted outfits according to different text prompts. The primary\nchallenge lies in maintaining the natural appearance of the dressed model while\npreserving the complex textures of each piece of clothing, ensuring that the\ninformation from different garments does not interfere with each other. To\ntackle these challenges, we first developed a garment encoder, which is a\ntrainable UNet copy with shared weights, capable of extracting detailed\nfeatures of garments in parallel. Secondly, our framework supports the\nconditional generation of multiple garments through decoupled multi-garment\nfeature fusion, allowing multiple clothing features to be injected into the\nbackbone network, significantly alleviating conflicts between garment\ninformation. Additionally, the proposed garment encoder is a plug-and-play\nmodule that can be combined with other extension modules such as IP-Adapter and\nControlNet, enhancing the diversity and controllability of the generated\nmodels. Extensive experiments demonstrate the superiority of our approach over\nexisting alternatives, opening up new avenues for the task of generating images\nwith multiple-piece clothing combinations\n", "link": "http://arxiv.org/abs/2408.05206v1", "date": "2024-08-09", "relevancy": 2.6067, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.698}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6939}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Garment%20Customized%20Model%20Generation&body=Title%3A%20Multi-Garment%20Customized%20Model%20Generation%0AAuthor%3A%20Yichen%20Liu%20and%20Penghui%20Du%20and%20Yi%20Liu%20Quanwei%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20Multi-Garment%20Customized%20Model%20Generation%2C%20a%20unified%0Aframework%20based%20on%20Latent%20Diffusion%20Models%20%28LDMs%29%20aimed%20at%20addressing%20the%0Aunexplored%20task%20of%20synthesizing%20images%20with%20free%20combinations%20of%20multiple%0Apieces%20of%20clothing.%20The%20method%20focuses%20on%20generating%20customized%20models%20wearing%0Avarious%20targeted%20outfits%20according%20to%20different%20text%20prompts.%20The%20primary%0Achallenge%20lies%20in%20maintaining%20the%20natural%20appearance%20of%20the%20dressed%20model%20while%0Apreserving%20the%20complex%20textures%20of%20each%20piece%20of%20clothing%2C%20ensuring%20that%20the%0Ainformation%20from%20different%20garments%20does%20not%20interfere%20with%20each%20other.%20To%0Atackle%20these%20challenges%2C%20we%20first%20developed%20a%20garment%20encoder%2C%20which%20is%20a%0Atrainable%20UNet%20copy%20with%20shared%20weights%2C%20capable%20of%20extracting%20detailed%0Afeatures%20of%20garments%20in%20parallel.%20Secondly%2C%20our%20framework%20supports%20the%0Aconditional%20generation%20of%20multiple%20garments%20through%20decoupled%20multi-garment%0Afeature%20fusion%2C%20allowing%20multiple%20clothing%20features%20to%20be%20injected%20into%20the%0Abackbone%20network%2C%20significantly%20alleviating%20conflicts%20between%20garment%0Ainformation.%20Additionally%2C%20the%20proposed%20garment%20encoder%20is%20a%20plug-and-play%0Amodule%20that%20can%20be%20combined%20with%20other%20extension%20modules%20such%20as%20IP-Adapter%20and%0AControlNet%2C%20enhancing%20the%20diversity%20and%20controllability%20of%20the%20generated%0Amodels.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20approach%20over%0Aexisting%20alternatives%2C%20opening%20up%20new%20avenues%20for%20the%20task%20of%20generating%20images%0Awith%20multiple-piece%20clothing%20combinations%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Garment%2520Customized%2520Model%2520Generation%26entry.906535625%3DYichen%2520Liu%2520and%2520Penghui%2520Du%2520and%2520Yi%2520Liu%2520Quanwei%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Multi-Garment%2520Customized%2520Model%2520Generation%252C%2520a%2520unified%250Aframework%2520based%2520on%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529%2520aimed%2520at%2520addressing%2520the%250Aunexplored%2520task%2520of%2520synthesizing%2520images%2520with%2520free%2520combinations%2520of%2520multiple%250Apieces%2520of%2520clothing.%2520The%2520method%2520focuses%2520on%2520generating%2520customized%2520models%2520wearing%250Avarious%2520targeted%2520outfits%2520according%2520to%2520different%2520text%2520prompts.%2520The%2520primary%250Achallenge%2520lies%2520in%2520maintaining%2520the%2520natural%2520appearance%2520of%2520the%2520dressed%2520model%2520while%250Apreserving%2520the%2520complex%2520textures%2520of%2520each%2520piece%2520of%2520clothing%252C%2520ensuring%2520that%2520the%250Ainformation%2520from%2520different%2520garments%2520does%2520not%2520interfere%2520with%2520each%2520other.%2520To%250Atackle%2520these%2520challenges%252C%2520we%2520first%2520developed%2520a%2520garment%2520encoder%252C%2520which%2520is%2520a%250Atrainable%2520UNet%2520copy%2520with%2520shared%2520weights%252C%2520capable%2520of%2520extracting%2520detailed%250Afeatures%2520of%2520garments%2520in%2520parallel.%2520Secondly%252C%2520our%2520framework%2520supports%2520the%250Aconditional%2520generation%2520of%2520multiple%2520garments%2520through%2520decoupled%2520multi-garment%250Afeature%2520fusion%252C%2520allowing%2520multiple%2520clothing%2520features%2520to%2520be%2520injected%2520into%2520the%250Abackbone%2520network%252C%2520significantly%2520alleviating%2520conflicts%2520between%2520garment%250Ainformation.%2520Additionally%252C%2520the%2520proposed%2520garment%2520encoder%2520is%2520a%2520plug-and-play%250Amodule%2520that%2520can%2520be%2520combined%2520with%2520other%2520extension%2520modules%2520such%2520as%2520IP-Adapter%2520and%250AControlNet%252C%2520enhancing%2520the%2520diversity%2520and%2520controllability%2520of%2520the%2520generated%250Amodels.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%250Aexisting%2520alternatives%252C%2520opening%2520up%2520new%2520avenues%2520for%2520the%2520task%2520of%2520generating%2520images%250Awith%2520multiple-piece%2520clothing%2520combinations%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Garment%20Customized%20Model%20Generation&entry.906535625=Yichen%20Liu%20and%20Penghui%20Du%20and%20Yi%20Liu%20Quanwei%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20Multi-Garment%20Customized%20Model%20Generation%2C%20a%20unified%0Aframework%20based%20on%20Latent%20Diffusion%20Models%20%28LDMs%29%20aimed%20at%20addressing%20the%0Aunexplored%20task%20of%20synthesizing%20images%20with%20free%20combinations%20of%20multiple%0Apieces%20of%20clothing.%20The%20method%20focuses%20on%20generating%20customized%20models%20wearing%0Avarious%20targeted%20outfits%20according%20to%20different%20text%20prompts.%20The%20primary%0Achallenge%20lies%20in%20maintaining%20the%20natural%20appearance%20of%20the%20dressed%20model%20while%0Apreserving%20the%20complex%20textures%20of%20each%20piece%20of%20clothing%2C%20ensuring%20that%20the%0Ainformation%20from%20different%20garments%20does%20not%20interfere%20with%20each%20other.%20To%0Atackle%20these%20challenges%2C%20we%20first%20developed%20a%20garment%20encoder%2C%20which%20is%20a%0Atrainable%20UNet%20copy%20with%20shared%20weights%2C%20capable%20of%20extracting%20detailed%0Afeatures%20of%20garments%20in%20parallel.%20Secondly%2C%20our%20framework%20supports%20the%0Aconditional%20generation%20of%20multiple%20garments%20through%20decoupled%20multi-garment%0Afeature%20fusion%2C%20allowing%20multiple%20clothing%20features%20to%20be%20injected%20into%20the%0Abackbone%20network%2C%20significantly%20alleviating%20conflicts%20between%20garment%0Ainformation.%20Additionally%2C%20the%20proposed%20garment%20encoder%20is%20a%20plug-and-play%0Amodule%20that%20can%20be%20combined%20with%20other%20extension%20modules%20such%20as%20IP-Adapter%20and%0AControlNet%2C%20enhancing%20the%20diversity%20and%20controllability%20of%20the%20generated%0Amodels.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20approach%20over%0Aexisting%20alternatives%2C%20opening%20up%20new%20avenues%20for%20the%20task%20of%20generating%20images%0Awith%20multiple-piece%20clothing%20combinations%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05206v1&entry.124074799=Read"},
{"title": "Object-level Geometric Structure Preserving for Natural Image Stitching", "author": "Wenxiao Cai and Wankou Yang", "abstract": "  The topic of stitching images with globally natural structures holds\nparamount significance, with two main goals: alignment and distortion\nprevention. The existing approaches exhibit the ability to align well, yet fall\nshort in maintaining object structures. In this paper, we endeavour to\nsafeguard the overall OBJect-level structures within images based on Global\nSimilarity Prior (OBJ-GSP), on the basis of good alignment performance. Our\napproach leverages semantic segmentation models like the family of Segment\nAnything Model to extract the contours of any objects in a scene. Triangular\nmeshes are employed in image transformation to protect the overall shapes of\nobjects within images. The balance between alignment and distortion prevention\nis achieved by allowing the object meshes to strike a balance between\nsimilarity and projective transformation. We also demonstrate the importance of\nsegmentation in low-altitude aerial image stitching. Additionally, we propose\nStitchBench, the most comprehensive image stitching benchmark by far. Extensive\nexperimental results demonstrate that OBJ-GSP outperforms existing methods in\nboth alignment and shape preservation. Code and dataset is publicly available\nat \\url{https://github.com/RussRobin/OBJ-GSP}.\n", "link": "http://arxiv.org/abs/2402.12677v3", "date": "2024-08-09", "relevancy": 2.6007, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.533}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5194}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-level%20Geometric%20Structure%20Preserving%20for%20Natural%20Image%20Stitching&body=Title%3A%20Object-level%20Geometric%20Structure%20Preserving%20for%20Natural%20Image%20Stitching%0AAuthor%3A%20Wenxiao%20Cai%20and%20Wankou%20Yang%0AAbstract%3A%20%20%20The%20topic%20of%20stitching%20images%20with%20globally%20natural%20structures%20holds%0Aparamount%20significance%2C%20with%20two%20main%20goals%3A%20alignment%20and%20distortion%0Aprevention.%20The%20existing%20approaches%20exhibit%20the%20ability%20to%20align%20well%2C%20yet%20fall%0Ashort%20in%20maintaining%20object%20structures.%20In%20this%20paper%2C%20we%20endeavour%20to%0Asafeguard%20the%20overall%20OBJect-level%20structures%20within%20images%20based%20on%20Global%0ASimilarity%20Prior%20%28OBJ-GSP%29%2C%20on%20the%20basis%20of%20good%20alignment%20performance.%20Our%0Aapproach%20leverages%20semantic%20segmentation%20models%20like%20the%20family%20of%20Segment%0AAnything%20Model%20to%20extract%20the%20contours%20of%20any%20objects%20in%20a%20scene.%20Triangular%0Ameshes%20are%20employed%20in%20image%20transformation%20to%20protect%20the%20overall%20shapes%20of%0Aobjects%20within%20images.%20The%20balance%20between%20alignment%20and%20distortion%20prevention%0Ais%20achieved%20by%20allowing%20the%20object%20meshes%20to%20strike%20a%20balance%20between%0Asimilarity%20and%20projective%20transformation.%20We%20also%20demonstrate%20the%20importance%20of%0Asegmentation%20in%20low-altitude%20aerial%20image%20stitching.%20Additionally%2C%20we%20propose%0AStitchBench%2C%20the%20most%20comprehensive%20image%20stitching%20benchmark%20by%20far.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20OBJ-GSP%20outperforms%20existing%20methods%20in%0Aboth%20alignment%20and%20shape%20preservation.%20Code%20and%20dataset%20is%20publicly%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/RussRobin/OBJ-GSP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12677v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-level%2520Geometric%2520Structure%2520Preserving%2520for%2520Natural%2520Image%2520Stitching%26entry.906535625%3DWenxiao%2520Cai%2520and%2520Wankou%2520Yang%26entry.1292438233%3D%2520%2520The%2520topic%2520of%2520stitching%2520images%2520with%2520globally%2520natural%2520structures%2520holds%250Aparamount%2520significance%252C%2520with%2520two%2520main%2520goals%253A%2520alignment%2520and%2520distortion%250Aprevention.%2520The%2520existing%2520approaches%2520exhibit%2520the%2520ability%2520to%2520align%2520well%252C%2520yet%2520fall%250Ashort%2520in%2520maintaining%2520object%2520structures.%2520In%2520this%2520paper%252C%2520we%2520endeavour%2520to%250Asafeguard%2520the%2520overall%2520OBJect-level%2520structures%2520within%2520images%2520based%2520on%2520Global%250ASimilarity%2520Prior%2520%2528OBJ-GSP%2529%252C%2520on%2520the%2520basis%2520of%2520good%2520alignment%2520performance.%2520Our%250Aapproach%2520leverages%2520semantic%2520segmentation%2520models%2520like%2520the%2520family%2520of%2520Segment%250AAnything%2520Model%2520to%2520extract%2520the%2520contours%2520of%2520any%2520objects%2520in%2520a%2520scene.%2520Triangular%250Ameshes%2520are%2520employed%2520in%2520image%2520transformation%2520to%2520protect%2520the%2520overall%2520shapes%2520of%250Aobjects%2520within%2520images.%2520The%2520balance%2520between%2520alignment%2520and%2520distortion%2520prevention%250Ais%2520achieved%2520by%2520allowing%2520the%2520object%2520meshes%2520to%2520strike%2520a%2520balance%2520between%250Asimilarity%2520and%2520projective%2520transformation.%2520We%2520also%2520demonstrate%2520the%2520importance%2520of%250Asegmentation%2520in%2520low-altitude%2520aerial%2520image%2520stitching.%2520Additionally%252C%2520we%2520propose%250AStitchBench%252C%2520the%2520most%2520comprehensive%2520image%2520stitching%2520benchmark%2520by%2520far.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520OBJ-GSP%2520outperforms%2520existing%2520methods%2520in%250Aboth%2520alignment%2520and%2520shape%2520preservation.%2520Code%2520and%2520dataset%2520is%2520publicly%2520available%250Aat%2520%255Curl%257Bhttps%253A//github.com/RussRobin/OBJ-GSP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12677v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-level%20Geometric%20Structure%20Preserving%20for%20Natural%20Image%20Stitching&entry.906535625=Wenxiao%20Cai%20and%20Wankou%20Yang&entry.1292438233=%20%20The%20topic%20of%20stitching%20images%20with%20globally%20natural%20structures%20holds%0Aparamount%20significance%2C%20with%20two%20main%20goals%3A%20alignment%20and%20distortion%0Aprevention.%20The%20existing%20approaches%20exhibit%20the%20ability%20to%20align%20well%2C%20yet%20fall%0Ashort%20in%20maintaining%20object%20structures.%20In%20this%20paper%2C%20we%20endeavour%20to%0Asafeguard%20the%20overall%20OBJect-level%20structures%20within%20images%20based%20on%20Global%0ASimilarity%20Prior%20%28OBJ-GSP%29%2C%20on%20the%20basis%20of%20good%20alignment%20performance.%20Our%0Aapproach%20leverages%20semantic%20segmentation%20models%20like%20the%20family%20of%20Segment%0AAnything%20Model%20to%20extract%20the%20contours%20of%20any%20objects%20in%20a%20scene.%20Triangular%0Ameshes%20are%20employed%20in%20image%20transformation%20to%20protect%20the%20overall%20shapes%20of%0Aobjects%20within%20images.%20The%20balance%20between%20alignment%20and%20distortion%20prevention%0Ais%20achieved%20by%20allowing%20the%20object%20meshes%20to%20strike%20a%20balance%20between%0Asimilarity%20and%20projective%20transformation.%20We%20also%20demonstrate%20the%20importance%20of%0Asegmentation%20in%20low-altitude%20aerial%20image%20stitching.%20Additionally%2C%20we%20propose%0AStitchBench%2C%20the%20most%20comprehensive%20image%20stitching%20benchmark%20by%20far.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20OBJ-GSP%20outperforms%20existing%20methods%20in%0Aboth%20alignment%20and%20shape%20preservation.%20Code%20and%20dataset%20is%20publicly%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/RussRobin/OBJ-GSP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12677v3&entry.124074799=Read"},
{"title": "Multi-dimensional Parameter Space Exploration for Streamline-specific\n  Tractography", "author": "Ruben Vink and Anna Vilanova and Maxime Chamberland", "abstract": "  One of the unspoken challenges of tractography is choosing the right\nparameters for a given dataset or bundle. In order to tackle this challenge, we\nexplore the multi-dimensional parameter space of tractography using\nstreamline-specific parameters (SSP). We 1) validate a state-of-the-art\nprobabilistic tracking method using per-streamline parameters on synthetic\ndata, and 2) show how we can gain insights into the parameter space by focusing\non streamline acceptance using real-world data. We demonstrate the potential\nadded value of SSP to the current state of tractography by showing how SSP can\nbe used to reveal patterns in the parameter space.\n", "link": "http://arxiv.org/abs/2408.05056v1", "date": "2024-08-09", "relevancy": 2.5526, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5319}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-dimensional%20Parameter%20Space%20Exploration%20for%20Streamline-specific%0A%20%20Tractography&body=Title%3A%20Multi-dimensional%20Parameter%20Space%20Exploration%20for%20Streamline-specific%0A%20%20Tractography%0AAuthor%3A%20Ruben%20Vink%20and%20Anna%20Vilanova%20and%20Maxime%20Chamberland%0AAbstract%3A%20%20%20One%20of%20the%20unspoken%20challenges%20of%20tractography%20is%20choosing%20the%20right%0Aparameters%20for%20a%20given%20dataset%20or%20bundle.%20In%20order%20to%20tackle%20this%20challenge%2C%20we%0Aexplore%20the%20multi-dimensional%20parameter%20space%20of%20tractography%20using%0Astreamline-specific%20parameters%20%28SSP%29.%20We%201%29%20validate%20a%20state-of-the-art%0Aprobabilistic%20tracking%20method%20using%20per-streamline%20parameters%20on%20synthetic%0Adata%2C%20and%202%29%20show%20how%20we%20can%20gain%20insights%20into%20the%20parameter%20space%20by%20focusing%0Aon%20streamline%20acceptance%20using%20real-world%20data.%20We%20demonstrate%20the%20potential%0Aadded%20value%20of%20SSP%20to%20the%20current%20state%20of%20tractography%20by%20showing%20how%20SSP%20can%0Abe%20used%20to%20reveal%20patterns%20in%20the%20parameter%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-dimensional%2520Parameter%2520Space%2520Exploration%2520for%2520Streamline-specific%250A%2520%2520Tractography%26entry.906535625%3DRuben%2520Vink%2520and%2520Anna%2520Vilanova%2520and%2520Maxime%2520Chamberland%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520unspoken%2520challenges%2520of%2520tractography%2520is%2520choosing%2520the%2520right%250Aparameters%2520for%2520a%2520given%2520dataset%2520or%2520bundle.%2520In%2520order%2520to%2520tackle%2520this%2520challenge%252C%2520we%250Aexplore%2520the%2520multi-dimensional%2520parameter%2520space%2520of%2520tractography%2520using%250Astreamline-specific%2520parameters%2520%2528SSP%2529.%2520We%25201%2529%2520validate%2520a%2520state-of-the-art%250Aprobabilistic%2520tracking%2520method%2520using%2520per-streamline%2520parameters%2520on%2520synthetic%250Adata%252C%2520and%25202%2529%2520show%2520how%2520we%2520can%2520gain%2520insights%2520into%2520the%2520parameter%2520space%2520by%2520focusing%250Aon%2520streamline%2520acceptance%2520using%2520real-world%2520data.%2520We%2520demonstrate%2520the%2520potential%250Aadded%2520value%2520of%2520SSP%2520to%2520the%2520current%2520state%2520of%2520tractography%2520by%2520showing%2520how%2520SSP%2520can%250Abe%2520used%2520to%2520reveal%2520patterns%2520in%2520the%2520parameter%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-dimensional%20Parameter%20Space%20Exploration%20for%20Streamline-specific%0A%20%20Tractography&entry.906535625=Ruben%20Vink%20and%20Anna%20Vilanova%20and%20Maxime%20Chamberland&entry.1292438233=%20%20One%20of%20the%20unspoken%20challenges%20of%20tractography%20is%20choosing%20the%20right%0Aparameters%20for%20a%20given%20dataset%20or%20bundle.%20In%20order%20to%20tackle%20this%20challenge%2C%20we%0Aexplore%20the%20multi-dimensional%20parameter%20space%20of%20tractography%20using%0Astreamline-specific%20parameters%20%28SSP%29.%20We%201%29%20validate%20a%20state-of-the-art%0Aprobabilistic%20tracking%20method%20using%20per-streamline%20parameters%20on%20synthetic%0Adata%2C%20and%202%29%20show%20how%20we%20can%20gain%20insights%20into%20the%20parameter%20space%20by%20focusing%0Aon%20streamline%20acceptance%20using%20real-world%20data.%20We%20demonstrate%20the%20potential%0Aadded%20value%20of%20SSP%20to%20the%20current%20state%20of%20tractography%20by%20showing%20how%20SSP%20can%0Abe%20used%20to%20reveal%20patterns%20in%20the%20parameter%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05056v1&entry.124074799=Read"},
{"title": "TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning", "author": "Yujie Feng and Xu Chu and Yongxin Xu and Zexin Lu and Bo Liu and Philip S. Yu and Xiao-Ming Wu", "abstract": "  Language model continual learning (CL) has recently garnered significant\ninterest due to its potential to adapt large language models (LLMs) to dynamic\nreal-world environments without re-training. A key challenge in this field is\ncatastrophic forgetting, where models lose previously acquired knowledge when\nlearning new tasks. Existing methods commonly employ multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge for each task, but these approaches lack efficiency and overlook the\npotential for knowledge transfer through task interaction. In this paper, we\npresent a novel CL framework for language models called Task Skill Localization\nand Consolidation (TaSL), which enhances knowledge transfer without relying on\nmemory replay. TaSL first divides the model into `skill units' based on\nparameter dependencies, enabling more granular control. It then employs a novel\ngroup-wise skill localization technique to identify the importance distribution\nof skill units for a new task. By comparing this importance distribution with\nthose from previous tasks, we implement a fine-grained skill consolidation\nstrategy that retains task-specific knowledge, thereby preventing forgetting,\nand updates task-shared knowledge, which facilitates bi-directional knowledge\ntransfer. As a result, TaSL achieves a superior balance between retaining\nprevious knowledge and excelling in new tasks. TaSL also shows strong\ngeneralizability, suitable for general models and customizable for PEFT methods\nlike LoRA. Additionally, it demonstrates notable extensibility, allowing\nintegration with memory replay to further enhance performance. Extensive\nexperiments on two CL benchmarks, with varying model sizes (from 220M to 7B),\ndemonstrate the effectiveness of TaSL and its variants across different\nsettings.\n", "link": "http://arxiv.org/abs/2408.05200v1", "date": "2024-08-09", "relevancy": 2.515, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5788}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4729}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaSL%3A%20Task%20Skill%20Localization%20and%20Consolidation%20for%20Language%20Model%0A%20%20Continual%20Learning&body=Title%3A%20TaSL%3A%20Task%20Skill%20Localization%20and%20Consolidation%20for%20Language%20Model%0A%20%20Continual%20Learning%0AAuthor%3A%20Yujie%20Feng%20and%20Xu%20Chu%20and%20Yongxin%20Xu%20and%20Zexin%20Lu%20and%20Bo%20Liu%20and%20Philip%20S.%20Yu%20and%20Xiao-Ming%20Wu%0AAbstract%3A%20%20%20Language%20model%20continual%20learning%20%28CL%29%20has%20recently%20garnered%20significant%0Ainterest%20due%20to%20its%20potential%20to%20adapt%20large%20language%20models%20%28LLMs%29%20to%20dynamic%0Areal-world%20environments%20without%20re-training.%20A%20key%20challenge%20in%20this%20field%20is%0Acatastrophic%20forgetting%2C%20where%20models%20lose%20previously%20acquired%20knowledge%20when%0Alearning%20new%20tasks.%20Existing%20methods%20commonly%20employ%20multiple%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20blocks%20to%20acquire%20task-specific%0Aknowledge%20for%20each%20task%2C%20but%20these%20approaches%20lack%20efficiency%20and%20overlook%20the%0Apotential%20for%20knowledge%20transfer%20through%20task%20interaction.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20CL%20framework%20for%20language%20models%20called%20Task%20Skill%20Localization%0Aand%20Consolidation%20%28TaSL%29%2C%20which%20enhances%20knowledge%20transfer%20without%20relying%20on%0Amemory%20replay.%20TaSL%20first%20divides%20the%20model%20into%20%60skill%20units%27%20based%20on%0Aparameter%20dependencies%2C%20enabling%20more%20granular%20control.%20It%20then%20employs%20a%20novel%0Agroup-wise%20skill%20localization%20technique%20to%20identify%20the%20importance%20distribution%0Aof%20skill%20units%20for%20a%20new%20task.%20By%20comparing%20this%20importance%20distribution%20with%0Athose%20from%20previous%20tasks%2C%20we%20implement%20a%20fine-grained%20skill%20consolidation%0Astrategy%20that%20retains%20task-specific%20knowledge%2C%20thereby%20preventing%20forgetting%2C%0Aand%20updates%20task-shared%20knowledge%2C%20which%20facilitates%20bi-directional%20knowledge%0Atransfer.%20As%20a%20result%2C%20TaSL%20achieves%20a%20superior%20balance%20between%20retaining%0Aprevious%20knowledge%20and%20excelling%20in%20new%20tasks.%20TaSL%20also%20shows%20strong%0Ageneralizability%2C%20suitable%20for%20general%20models%20and%20customizable%20for%20PEFT%20methods%0Alike%20LoRA.%20Additionally%2C%20it%20demonstrates%20notable%20extensibility%2C%20allowing%0Aintegration%20with%20memory%20replay%20to%20further%20enhance%20performance.%20Extensive%0Aexperiments%20on%20two%20CL%20benchmarks%2C%20with%20varying%20model%20sizes%20%28from%20220M%20to%207B%29%2C%0Ademonstrate%20the%20effectiveness%20of%20TaSL%20and%20its%20variants%20across%20different%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaSL%253A%2520Task%2520Skill%2520Localization%2520and%2520Consolidation%2520for%2520Language%2520Model%250A%2520%2520Continual%2520Learning%26entry.906535625%3DYujie%2520Feng%2520and%2520Xu%2520Chu%2520and%2520Yongxin%2520Xu%2520and%2520Zexin%2520Lu%2520and%2520Bo%2520Liu%2520and%2520Philip%2520S.%2520Yu%2520and%2520Xiao-Ming%2520Wu%26entry.1292438233%3D%2520%2520Language%2520model%2520continual%2520learning%2520%2528CL%2529%2520has%2520recently%2520garnered%2520significant%250Ainterest%2520due%2520to%2520its%2520potential%2520to%2520adapt%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520dynamic%250Areal-world%2520environments%2520without%2520re-training.%2520A%2520key%2520challenge%2520in%2520this%2520field%2520is%250Acatastrophic%2520forgetting%252C%2520where%2520models%2520lose%2520previously%2520acquired%2520knowledge%2520when%250Alearning%2520new%2520tasks.%2520Existing%2520methods%2520commonly%2520employ%2520multiple%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520blocks%2520to%2520acquire%2520task-specific%250Aknowledge%2520for%2520each%2520task%252C%2520but%2520these%2520approaches%2520lack%2520efficiency%2520and%2520overlook%2520the%250Apotential%2520for%2520knowledge%2520transfer%2520through%2520task%2520interaction.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520novel%2520CL%2520framework%2520for%2520language%2520models%2520called%2520Task%2520Skill%2520Localization%250Aand%2520Consolidation%2520%2528TaSL%2529%252C%2520which%2520enhances%2520knowledge%2520transfer%2520without%2520relying%2520on%250Amemory%2520replay.%2520TaSL%2520first%2520divides%2520the%2520model%2520into%2520%2560skill%2520units%2527%2520based%2520on%250Aparameter%2520dependencies%252C%2520enabling%2520more%2520granular%2520control.%2520It%2520then%2520employs%2520a%2520novel%250Agroup-wise%2520skill%2520localization%2520technique%2520to%2520identify%2520the%2520importance%2520distribution%250Aof%2520skill%2520units%2520for%2520a%2520new%2520task.%2520By%2520comparing%2520this%2520importance%2520distribution%2520with%250Athose%2520from%2520previous%2520tasks%252C%2520we%2520implement%2520a%2520fine-grained%2520skill%2520consolidation%250Astrategy%2520that%2520retains%2520task-specific%2520knowledge%252C%2520thereby%2520preventing%2520forgetting%252C%250Aand%2520updates%2520task-shared%2520knowledge%252C%2520which%2520facilitates%2520bi-directional%2520knowledge%250Atransfer.%2520As%2520a%2520result%252C%2520TaSL%2520achieves%2520a%2520superior%2520balance%2520between%2520retaining%250Aprevious%2520knowledge%2520and%2520excelling%2520in%2520new%2520tasks.%2520TaSL%2520also%2520shows%2520strong%250Ageneralizability%252C%2520suitable%2520for%2520general%2520models%2520and%2520customizable%2520for%2520PEFT%2520methods%250Alike%2520LoRA.%2520Additionally%252C%2520it%2520demonstrates%2520notable%2520extensibility%252C%2520allowing%250Aintegration%2520with%2520memory%2520replay%2520to%2520further%2520enhance%2520performance.%2520Extensive%250Aexperiments%2520on%2520two%2520CL%2520benchmarks%252C%2520with%2520varying%2520model%2520sizes%2520%2528from%2520220M%2520to%25207B%2529%252C%250Ademonstrate%2520the%2520effectiveness%2520of%2520TaSL%2520and%2520its%2520variants%2520across%2520different%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaSL%3A%20Task%20Skill%20Localization%20and%20Consolidation%20for%20Language%20Model%0A%20%20Continual%20Learning&entry.906535625=Yujie%20Feng%20and%20Xu%20Chu%20and%20Yongxin%20Xu%20and%20Zexin%20Lu%20and%20Bo%20Liu%20and%20Philip%20S.%20Yu%20and%20Xiao-Ming%20Wu&entry.1292438233=%20%20Language%20model%20continual%20learning%20%28CL%29%20has%20recently%20garnered%20significant%0Ainterest%20due%20to%20its%20potential%20to%20adapt%20large%20language%20models%20%28LLMs%29%20to%20dynamic%0Areal-world%20environments%20without%20re-training.%20A%20key%20challenge%20in%20this%20field%20is%0Acatastrophic%20forgetting%2C%20where%20models%20lose%20previously%20acquired%20knowledge%20when%0Alearning%20new%20tasks.%20Existing%20methods%20commonly%20employ%20multiple%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20blocks%20to%20acquire%20task-specific%0Aknowledge%20for%20each%20task%2C%20but%20these%20approaches%20lack%20efficiency%20and%20overlook%20the%0Apotential%20for%20knowledge%20transfer%20through%20task%20interaction.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20CL%20framework%20for%20language%20models%20called%20Task%20Skill%20Localization%0Aand%20Consolidation%20%28TaSL%29%2C%20which%20enhances%20knowledge%20transfer%20without%20relying%20on%0Amemory%20replay.%20TaSL%20first%20divides%20the%20model%20into%20%60skill%20units%27%20based%20on%0Aparameter%20dependencies%2C%20enabling%20more%20granular%20control.%20It%20then%20employs%20a%20novel%0Agroup-wise%20skill%20localization%20technique%20to%20identify%20the%20importance%20distribution%0Aof%20skill%20units%20for%20a%20new%20task.%20By%20comparing%20this%20importance%20distribution%20with%0Athose%20from%20previous%20tasks%2C%20we%20implement%20a%20fine-grained%20skill%20consolidation%0Astrategy%20that%20retains%20task-specific%20knowledge%2C%20thereby%20preventing%20forgetting%2C%0Aand%20updates%20task-shared%20knowledge%2C%20which%20facilitates%20bi-directional%20knowledge%0Atransfer.%20As%20a%20result%2C%20TaSL%20achieves%20a%20superior%20balance%20between%20retaining%0Aprevious%20knowledge%20and%20excelling%20in%20new%20tasks.%20TaSL%20also%20shows%20strong%0Ageneralizability%2C%20suitable%20for%20general%20models%20and%20customizable%20for%20PEFT%20methods%0Alike%20LoRA.%20Additionally%2C%20it%20demonstrates%20notable%20extensibility%2C%20allowing%0Aintegration%20with%20memory%20replay%20to%20further%20enhance%20performance.%20Extensive%0Aexperiments%20on%20two%20CL%20benchmarks%2C%20with%20varying%20model%20sizes%20%28from%20220M%20to%207B%29%2C%0Ademonstrate%20the%20effectiveness%20of%20TaSL%20and%20its%20variants%20across%20different%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05200v1&entry.124074799=Read"},
{"title": "RadarPillars: Efficient Object Detection from 4D Radar Point Clouds", "author": "Alexander Musiat and Laurenz Reichardt and Michael Schulze and Oliver Wasenm\u00fcller", "abstract": "  Automotive radar systems have evolved to provide not only range, azimuth and\nDoppler velocity, but also elevation data. This additional dimension allows for\nthe representation of 4D radar as a 3D point cloud. As a result, existing deep\nlearning methods for 3D object detection, which were initially developed for\nLiDAR data, are often applied to these radar point clouds. However, this\nneglects the special characteristics of 4D radar data, such as the extreme\nsparsity and the optimal utilization of velocity information. To address these\ngaps in the state-of-the-art, we present RadarPillars, a pillar-based object\ndetection network.\n  By decomposing radial velocity data, introducing PillarAttention for\nefficient feature extraction, and studying layer scaling to accommodate radar\nsparsity, RadarPillars significantly outperform state-of-the-art detection\nresults on the View-of-Delft dataset. Importantly, this comes at a\nsignificantly reduced parameter count, surpassing existing methods in terms of\nefficiency and enabling real-time performance on edge devices.\n", "link": "http://arxiv.org/abs/2408.05020v1", "date": "2024-08-09", "relevancy": 2.5106, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5093}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5054}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadarPillars%3A%20Efficient%20Object%20Detection%20from%204D%20Radar%20Point%20Clouds&body=Title%3A%20RadarPillars%3A%20Efficient%20Object%20Detection%20from%204D%20Radar%20Point%20Clouds%0AAuthor%3A%20Alexander%20Musiat%20and%20Laurenz%20Reichardt%20and%20Michael%20Schulze%20and%20Oliver%20Wasenm%C3%BCller%0AAbstract%3A%20%20%20Automotive%20radar%20systems%20have%20evolved%20to%20provide%20not%20only%20range%2C%20azimuth%20and%0ADoppler%20velocity%2C%20but%20also%20elevation%20data.%20This%20additional%20dimension%20allows%20for%0Athe%20representation%20of%204D%20radar%20as%20a%203D%20point%20cloud.%20As%20a%20result%2C%20existing%20deep%0Alearning%20methods%20for%203D%20object%20detection%2C%20which%20were%20initially%20developed%20for%0ALiDAR%20data%2C%20are%20often%20applied%20to%20these%20radar%20point%20clouds.%20However%2C%20this%0Aneglects%20the%20special%20characteristics%20of%204D%20radar%20data%2C%20such%20as%20the%20extreme%0Asparsity%20and%20the%20optimal%20utilization%20of%20velocity%20information.%20To%20address%20these%0Agaps%20in%20the%20state-of-the-art%2C%20we%20present%20RadarPillars%2C%20a%20pillar-based%20object%0Adetection%20network.%0A%20%20By%20decomposing%20radial%20velocity%20data%2C%20introducing%20PillarAttention%20for%0Aefficient%20feature%20extraction%2C%20and%20studying%20layer%20scaling%20to%20accommodate%20radar%0Asparsity%2C%20RadarPillars%20significantly%20outperform%20state-of-the-art%20detection%0Aresults%20on%20the%20View-of-Delft%20dataset.%20Importantly%2C%20this%20comes%20at%20a%0Asignificantly%20reduced%20parameter%20count%2C%20surpassing%20existing%20methods%20in%20terms%20of%0Aefficiency%20and%20enabling%20real-time%20performance%20on%20edge%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadarPillars%253A%2520Efficient%2520Object%2520Detection%2520from%25204D%2520Radar%2520Point%2520Clouds%26entry.906535625%3DAlexander%2520Musiat%2520and%2520Laurenz%2520Reichardt%2520and%2520Michael%2520Schulze%2520and%2520Oliver%2520Wasenm%25C3%25BCller%26entry.1292438233%3D%2520%2520Automotive%2520radar%2520systems%2520have%2520evolved%2520to%2520provide%2520not%2520only%2520range%252C%2520azimuth%2520and%250ADoppler%2520velocity%252C%2520but%2520also%2520elevation%2520data.%2520This%2520additional%2520dimension%2520allows%2520for%250Athe%2520representation%2520of%25204D%2520radar%2520as%2520a%25203D%2520point%2520cloud.%2520As%2520a%2520result%252C%2520existing%2520deep%250Alearning%2520methods%2520for%25203D%2520object%2520detection%252C%2520which%2520were%2520initially%2520developed%2520for%250ALiDAR%2520data%252C%2520are%2520often%2520applied%2520to%2520these%2520radar%2520point%2520clouds.%2520However%252C%2520this%250Aneglects%2520the%2520special%2520characteristics%2520of%25204D%2520radar%2520data%252C%2520such%2520as%2520the%2520extreme%250Asparsity%2520and%2520the%2520optimal%2520utilization%2520of%2520velocity%2520information.%2520To%2520address%2520these%250Agaps%2520in%2520the%2520state-of-the-art%252C%2520we%2520present%2520RadarPillars%252C%2520a%2520pillar-based%2520object%250Adetection%2520network.%250A%2520%2520By%2520decomposing%2520radial%2520velocity%2520data%252C%2520introducing%2520PillarAttention%2520for%250Aefficient%2520feature%2520extraction%252C%2520and%2520studying%2520layer%2520scaling%2520to%2520accommodate%2520radar%250Asparsity%252C%2520RadarPillars%2520significantly%2520outperform%2520state-of-the-art%2520detection%250Aresults%2520on%2520the%2520View-of-Delft%2520dataset.%2520Importantly%252C%2520this%2520comes%2520at%2520a%250Asignificantly%2520reduced%2520parameter%2520count%252C%2520surpassing%2520existing%2520methods%2520in%2520terms%2520of%250Aefficiency%2520and%2520enabling%2520real-time%2520performance%2520on%2520edge%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadarPillars%3A%20Efficient%20Object%20Detection%20from%204D%20Radar%20Point%20Clouds&entry.906535625=Alexander%20Musiat%20and%20Laurenz%20Reichardt%20and%20Michael%20Schulze%20and%20Oliver%20Wasenm%C3%BCller&entry.1292438233=%20%20Automotive%20radar%20systems%20have%20evolved%20to%20provide%20not%20only%20range%2C%20azimuth%20and%0ADoppler%20velocity%2C%20but%20also%20elevation%20data.%20This%20additional%20dimension%20allows%20for%0Athe%20representation%20of%204D%20radar%20as%20a%203D%20point%20cloud.%20As%20a%20result%2C%20existing%20deep%0Alearning%20methods%20for%203D%20object%20detection%2C%20which%20were%20initially%20developed%20for%0ALiDAR%20data%2C%20are%20often%20applied%20to%20these%20radar%20point%20clouds.%20However%2C%20this%0Aneglects%20the%20special%20characteristics%20of%204D%20radar%20data%2C%20such%20as%20the%20extreme%0Asparsity%20and%20the%20optimal%20utilization%20of%20velocity%20information.%20To%20address%20these%0Agaps%20in%20the%20state-of-the-art%2C%20we%20present%20RadarPillars%2C%20a%20pillar-based%20object%0Adetection%20network.%0A%20%20By%20decomposing%20radial%20velocity%20data%2C%20introducing%20PillarAttention%20for%0Aefficient%20feature%20extraction%2C%20and%20studying%20layer%20scaling%20to%20accommodate%20radar%0Asparsity%2C%20RadarPillars%20significantly%20outperform%20state-of-the-art%20detection%0Aresults%20on%20the%20View-of-Delft%20dataset.%20Importantly%2C%20this%20comes%20at%20a%0Asignificantly%20reduced%20parameter%20count%2C%20surpassing%20existing%20methods%20in%20terms%20of%0Aefficiency%20and%20enabling%20real-time%20performance%20on%20edge%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05020v1&entry.124074799=Read"},
{"title": "DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks", "author": "Jiaxu Liu and Xinping Yi and Xiaowei Huang", "abstract": "  Hyperbolic graph convolutional networks (HGCNs) have demonstrated significant\npotential in extracting information from hierarchical graphs. However, existing\nHGCNs are limited to shallow architectures due to the computational expense of\nhyperbolic operations and the issue of over-smoothing as depth increases.\nAlthough treatments have been applied to alleviate over-smoothing in GCNs,\ndeveloping a hyperbolic solution presents distinct challenges since operations\nmust be carefully designed to fit the hyperbolic nature. Addressing these\nchallenges, we propose DeepHGCN, the first deep multi-layer HGCN architecture\nwith dramatically improved computational efficiency and substantially reduced\nover-smoothing. DeepHGCN features two key innovations: (1) a novel hyperbolic\nfeature transformation layer that enables fast and accurate linear mappings,\nand (2) techniques such as hyperbolic residual connections and regularization\nfor both weights and features, facilitated by an efficient hyperbolic midpoint\nmethod. Extensive experiments demonstrate that DeepHGCN achieves significant\nimprovements in link prediction and node classification tasks compared to both\nEuclidean and shallow hyperbolic GCN variants.\n", "link": "http://arxiv.org/abs/2310.02027v5", "date": "2024-08-09", "relevancy": 2.5066, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5248}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5023}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepHGCN%3A%20Toward%20Deeper%20Hyperbolic%20Graph%20Convolutional%20Networks&body=Title%3A%20DeepHGCN%3A%20Toward%20Deeper%20Hyperbolic%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Jiaxu%20Liu%20and%20Xinping%20Yi%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20Hyperbolic%20graph%20convolutional%20networks%20%28HGCNs%29%20have%20demonstrated%20significant%0Apotential%20in%20extracting%20information%20from%20hierarchical%20graphs.%20However%2C%20existing%0AHGCNs%20are%20limited%20to%20shallow%20architectures%20due%20to%20the%20computational%20expense%20of%0Ahyperbolic%20operations%20and%20the%20issue%20of%20over-smoothing%20as%20depth%20increases.%0AAlthough%20treatments%20have%20been%20applied%20to%20alleviate%20over-smoothing%20in%20GCNs%2C%0Adeveloping%20a%20hyperbolic%20solution%20presents%20distinct%20challenges%20since%20operations%0Amust%20be%20carefully%20designed%20to%20fit%20the%20hyperbolic%20nature.%20Addressing%20these%0Achallenges%2C%20we%20propose%20DeepHGCN%2C%20the%20first%20deep%20multi-layer%20HGCN%20architecture%0Awith%20dramatically%20improved%20computational%20efficiency%20and%20substantially%20reduced%0Aover-smoothing.%20DeepHGCN%20features%20two%20key%20innovations%3A%20%281%29%20a%20novel%20hyperbolic%0Afeature%20transformation%20layer%20that%20enables%20fast%20and%20accurate%20linear%20mappings%2C%0Aand%20%282%29%20techniques%20such%20as%20hyperbolic%20residual%20connections%20and%20regularization%0Afor%20both%20weights%20and%20features%2C%20facilitated%20by%20an%20efficient%20hyperbolic%20midpoint%0Amethod.%20Extensive%20experiments%20demonstrate%20that%20DeepHGCN%20achieves%20significant%0Aimprovements%20in%20link%20prediction%20and%20node%20classification%20tasks%20compared%20to%20both%0AEuclidean%20and%20shallow%20hyperbolic%20GCN%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02027v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepHGCN%253A%2520Toward%2520Deeper%2520Hyperbolic%2520Graph%2520Convolutional%2520Networks%26entry.906535625%3DJiaxu%2520Liu%2520and%2520Xinping%2520Yi%2520and%2520Xiaowei%2520Huang%26entry.1292438233%3D%2520%2520Hyperbolic%2520graph%2520convolutional%2520networks%2520%2528HGCNs%2529%2520have%2520demonstrated%2520significant%250Apotential%2520in%2520extracting%2520information%2520from%2520hierarchical%2520graphs.%2520However%252C%2520existing%250AHGCNs%2520are%2520limited%2520to%2520shallow%2520architectures%2520due%2520to%2520the%2520computational%2520expense%2520of%250Ahyperbolic%2520operations%2520and%2520the%2520issue%2520of%2520over-smoothing%2520as%2520depth%2520increases.%250AAlthough%2520treatments%2520have%2520been%2520applied%2520to%2520alleviate%2520over-smoothing%2520in%2520GCNs%252C%250Adeveloping%2520a%2520hyperbolic%2520solution%2520presents%2520distinct%2520challenges%2520since%2520operations%250Amust%2520be%2520carefully%2520designed%2520to%2520fit%2520the%2520hyperbolic%2520nature.%2520Addressing%2520these%250Achallenges%252C%2520we%2520propose%2520DeepHGCN%252C%2520the%2520first%2520deep%2520multi-layer%2520HGCN%2520architecture%250Awith%2520dramatically%2520improved%2520computational%2520efficiency%2520and%2520substantially%2520reduced%250Aover-smoothing.%2520DeepHGCN%2520features%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520novel%2520hyperbolic%250Afeature%2520transformation%2520layer%2520that%2520enables%2520fast%2520and%2520accurate%2520linear%2520mappings%252C%250Aand%2520%25282%2529%2520techniques%2520such%2520as%2520hyperbolic%2520residual%2520connections%2520and%2520regularization%250Afor%2520both%2520weights%2520and%2520features%252C%2520facilitated%2520by%2520an%2520efficient%2520hyperbolic%2520midpoint%250Amethod.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DeepHGCN%2520achieves%2520significant%250Aimprovements%2520in%2520link%2520prediction%2520and%2520node%2520classification%2520tasks%2520compared%2520to%2520both%250AEuclidean%2520and%2520shallow%2520hyperbolic%2520GCN%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02027v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepHGCN%3A%20Toward%20Deeper%20Hyperbolic%20Graph%20Convolutional%20Networks&entry.906535625=Jiaxu%20Liu%20and%20Xinping%20Yi%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Hyperbolic%20graph%20convolutional%20networks%20%28HGCNs%29%20have%20demonstrated%20significant%0Apotential%20in%20extracting%20information%20from%20hierarchical%20graphs.%20However%2C%20existing%0AHGCNs%20are%20limited%20to%20shallow%20architectures%20due%20to%20the%20computational%20expense%20of%0Ahyperbolic%20operations%20and%20the%20issue%20of%20over-smoothing%20as%20depth%20increases.%0AAlthough%20treatments%20have%20been%20applied%20to%20alleviate%20over-smoothing%20in%20GCNs%2C%0Adeveloping%20a%20hyperbolic%20solution%20presents%20distinct%20challenges%20since%20operations%0Amust%20be%20carefully%20designed%20to%20fit%20the%20hyperbolic%20nature.%20Addressing%20these%0Achallenges%2C%20we%20propose%20DeepHGCN%2C%20the%20first%20deep%20multi-layer%20HGCN%20architecture%0Awith%20dramatically%20improved%20computational%20efficiency%20and%20substantially%20reduced%0Aover-smoothing.%20DeepHGCN%20features%20two%20key%20innovations%3A%20%281%29%20a%20novel%20hyperbolic%0Afeature%20transformation%20layer%20that%20enables%20fast%20and%20accurate%20linear%20mappings%2C%0Aand%20%282%29%20techniques%20such%20as%20hyperbolic%20residual%20connections%20and%20regularization%0Afor%20both%20weights%20and%20features%2C%20facilitated%20by%20an%20efficient%20hyperbolic%20midpoint%0Amethod.%20Extensive%20experiments%20demonstrate%20that%20DeepHGCN%20achieves%20significant%0Aimprovements%20in%20link%20prediction%20and%20node%20classification%20tasks%20compared%20to%20both%0AEuclidean%20and%20shallow%20hyperbolic%20GCN%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02027v5&entry.124074799=Read"},
{"title": "Beyond the Eye: A Relational Model for Early Dementia Detection Using\n  Retinal OCTA Images", "author": "Shouyue Liu and Jinkui Hao and Yonghuai Liu and Huazhu Fu and Xinyu Guo and Shuting Zhang and Yitian Zhao", "abstract": "  Early detection of dementia, such as Alzheimer's disease (AD) or mild\ncognitive impairment (MCI), is essential to enable timely intervention and\npotential treatment. Accurate detection of AD/MCI is challenging due to the\nhigh complexity, cost, and often invasive nature of current diagnostic\ntechniques, which limit their suitability for large-scale population screening.\nGiven the shared embryological origins and physiological characteristics of the\nretina and brain, retinal imaging is emerging as a potentially rapid and\ncost-effective alternative for the identification of individuals with or at\nhigh risk of AD. In this paper, we present a novel PolarNet+ that uses retinal\noptical coherence tomography angiography (OCTA) to discriminate early-onset AD\n(EOAD) and MCI subjects from controls. Our method first maps OCTA images from\nCartesian coordinates to polar coordinates, allowing approximate sub-region\ncalculation to implement the clinician-friendly early treatment of diabetic\nretinopathy study (ETDRS) grid analysis. We then introduce a multi-view module\nto serialize and analyze the images along three dimensions for comprehensive,\nclinically useful information extraction. Finally, we abstract the sequence\nembedding into a graph, transforming the detection task into a general graph\nclassification problem. A regional relationship module is applied after the\nmulti-view module to excavate the relationship between the sub-regions. Such\nregional relationship analyses validate known eye-brain links and reveal new\ndiscriminative patterns.\n", "link": "http://arxiv.org/abs/2408.05117v1", "date": "2024-08-09", "relevancy": 2.503, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5022}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Eye%3A%20A%20Relational%20Model%20for%20Early%20Dementia%20Detection%20Using%0A%20%20Retinal%20OCTA%20Images&body=Title%3A%20Beyond%20the%20Eye%3A%20A%20Relational%20Model%20for%20Early%20Dementia%20Detection%20Using%0A%20%20Retinal%20OCTA%20Images%0AAuthor%3A%20Shouyue%20Liu%20and%20Jinkui%20Hao%20and%20Yonghuai%20Liu%20and%20Huazhu%20Fu%20and%20Xinyu%20Guo%20and%20Shuting%20Zhang%20and%20Yitian%20Zhao%0AAbstract%3A%20%20%20Early%20detection%20of%20dementia%2C%20such%20as%20Alzheimer%27s%20disease%20%28AD%29%20or%20mild%0Acognitive%20impairment%20%28MCI%29%2C%20is%20essential%20to%20enable%20timely%20intervention%20and%0Apotential%20treatment.%20Accurate%20detection%20of%20AD/MCI%20is%20challenging%20due%20to%20the%0Ahigh%20complexity%2C%20cost%2C%20and%20often%20invasive%20nature%20of%20current%20diagnostic%0Atechniques%2C%20which%20limit%20their%20suitability%20for%20large-scale%20population%20screening.%0AGiven%20the%20shared%20embryological%20origins%20and%20physiological%20characteristics%20of%20the%0Aretina%20and%20brain%2C%20retinal%20imaging%20is%20emerging%20as%20a%20potentially%20rapid%20and%0Acost-effective%20alternative%20for%20the%20identification%20of%20individuals%20with%20or%20at%0Ahigh%20risk%20of%20AD.%20In%20this%20paper%2C%20we%20present%20a%20novel%20PolarNet%2B%20that%20uses%20retinal%0Aoptical%20coherence%20tomography%20angiography%20%28OCTA%29%20to%20discriminate%20early-onset%20AD%0A%28EOAD%29%20and%20MCI%20subjects%20from%20controls.%20Our%20method%20first%20maps%20OCTA%20images%20from%0ACartesian%20coordinates%20to%20polar%20coordinates%2C%20allowing%20approximate%20sub-region%0Acalculation%20to%20implement%20the%20clinician-friendly%20early%20treatment%20of%20diabetic%0Aretinopathy%20study%20%28ETDRS%29%20grid%20analysis.%20We%20then%20introduce%20a%20multi-view%20module%0Ato%20serialize%20and%20analyze%20the%20images%20along%20three%20dimensions%20for%20comprehensive%2C%0Aclinically%20useful%20information%20extraction.%20Finally%2C%20we%20abstract%20the%20sequence%0Aembedding%20into%20a%20graph%2C%20transforming%20the%20detection%20task%20into%20a%20general%20graph%0Aclassification%20problem.%20A%20regional%20relationship%20module%20is%20applied%20after%20the%0Amulti-view%20module%20to%20excavate%20the%20relationship%20between%20the%20sub-regions.%20Such%0Aregional%20relationship%20analyses%20validate%20known%20eye-brain%20links%20and%20reveal%20new%0Adiscriminative%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Eye%253A%2520A%2520Relational%2520Model%2520for%2520Early%2520Dementia%2520Detection%2520Using%250A%2520%2520Retinal%2520OCTA%2520Images%26entry.906535625%3DShouyue%2520Liu%2520and%2520Jinkui%2520Hao%2520and%2520Yonghuai%2520Liu%2520and%2520Huazhu%2520Fu%2520and%2520Xinyu%2520Guo%2520and%2520Shuting%2520Zhang%2520and%2520Yitian%2520Zhao%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520dementia%252C%2520such%2520as%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520or%2520mild%250Acognitive%2520impairment%2520%2528MCI%2529%252C%2520is%2520essential%2520to%2520enable%2520timely%2520intervention%2520and%250Apotential%2520treatment.%2520Accurate%2520detection%2520of%2520AD/MCI%2520is%2520challenging%2520due%2520to%2520the%250Ahigh%2520complexity%252C%2520cost%252C%2520and%2520often%2520invasive%2520nature%2520of%2520current%2520diagnostic%250Atechniques%252C%2520which%2520limit%2520their%2520suitability%2520for%2520large-scale%2520population%2520screening.%250AGiven%2520the%2520shared%2520embryological%2520origins%2520and%2520physiological%2520characteristics%2520of%2520the%250Aretina%2520and%2520brain%252C%2520retinal%2520imaging%2520is%2520emerging%2520as%2520a%2520potentially%2520rapid%2520and%250Acost-effective%2520alternative%2520for%2520the%2520identification%2520of%2520individuals%2520with%2520or%2520at%250Ahigh%2520risk%2520of%2520AD.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520PolarNet%252B%2520that%2520uses%2520retinal%250Aoptical%2520coherence%2520tomography%2520angiography%2520%2528OCTA%2529%2520to%2520discriminate%2520early-onset%2520AD%250A%2528EOAD%2529%2520and%2520MCI%2520subjects%2520from%2520controls.%2520Our%2520method%2520first%2520maps%2520OCTA%2520images%2520from%250ACartesian%2520coordinates%2520to%2520polar%2520coordinates%252C%2520allowing%2520approximate%2520sub-region%250Acalculation%2520to%2520implement%2520the%2520clinician-friendly%2520early%2520treatment%2520of%2520diabetic%250Aretinopathy%2520study%2520%2528ETDRS%2529%2520grid%2520analysis.%2520We%2520then%2520introduce%2520a%2520multi-view%2520module%250Ato%2520serialize%2520and%2520analyze%2520the%2520images%2520along%2520three%2520dimensions%2520for%2520comprehensive%252C%250Aclinically%2520useful%2520information%2520extraction.%2520Finally%252C%2520we%2520abstract%2520the%2520sequence%250Aembedding%2520into%2520a%2520graph%252C%2520transforming%2520the%2520detection%2520task%2520into%2520a%2520general%2520graph%250Aclassification%2520problem.%2520A%2520regional%2520relationship%2520module%2520is%2520applied%2520after%2520the%250Amulti-view%2520module%2520to%2520excavate%2520the%2520relationship%2520between%2520the%2520sub-regions.%2520Such%250Aregional%2520relationship%2520analyses%2520validate%2520known%2520eye-brain%2520links%2520and%2520reveal%2520new%250Adiscriminative%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Eye%3A%20A%20Relational%20Model%20for%20Early%20Dementia%20Detection%20Using%0A%20%20Retinal%20OCTA%20Images&entry.906535625=Shouyue%20Liu%20and%20Jinkui%20Hao%20and%20Yonghuai%20Liu%20and%20Huazhu%20Fu%20and%20Xinyu%20Guo%20and%20Shuting%20Zhang%20and%20Yitian%20Zhao&entry.1292438233=%20%20Early%20detection%20of%20dementia%2C%20such%20as%20Alzheimer%27s%20disease%20%28AD%29%20or%20mild%0Acognitive%20impairment%20%28MCI%29%2C%20is%20essential%20to%20enable%20timely%20intervention%20and%0Apotential%20treatment.%20Accurate%20detection%20of%20AD/MCI%20is%20challenging%20due%20to%20the%0Ahigh%20complexity%2C%20cost%2C%20and%20often%20invasive%20nature%20of%20current%20diagnostic%0Atechniques%2C%20which%20limit%20their%20suitability%20for%20large-scale%20population%20screening.%0AGiven%20the%20shared%20embryological%20origins%20and%20physiological%20characteristics%20of%20the%0Aretina%20and%20brain%2C%20retinal%20imaging%20is%20emerging%20as%20a%20potentially%20rapid%20and%0Acost-effective%20alternative%20for%20the%20identification%20of%20individuals%20with%20or%20at%0Ahigh%20risk%20of%20AD.%20In%20this%20paper%2C%20we%20present%20a%20novel%20PolarNet%2B%20that%20uses%20retinal%0Aoptical%20coherence%20tomography%20angiography%20%28OCTA%29%20to%20discriminate%20early-onset%20AD%0A%28EOAD%29%20and%20MCI%20subjects%20from%20controls.%20Our%20method%20first%20maps%20OCTA%20images%20from%0ACartesian%20coordinates%20to%20polar%20coordinates%2C%20allowing%20approximate%20sub-region%0Acalculation%20to%20implement%20the%20clinician-friendly%20early%20treatment%20of%20diabetic%0Aretinopathy%20study%20%28ETDRS%29%20grid%20analysis.%20We%20then%20introduce%20a%20multi-view%20module%0Ato%20serialize%20and%20analyze%20the%20images%20along%20three%20dimensions%20for%20comprehensive%2C%0Aclinically%20useful%20information%20extraction.%20Finally%2C%20we%20abstract%20the%20sequence%0Aembedding%20into%20a%20graph%2C%20transforming%20the%20detection%20task%20into%20a%20general%20graph%0Aclassification%20problem.%20A%20regional%20relationship%20module%20is%20applied%20after%20the%0Amulti-view%20module%20to%20excavate%20the%20relationship%20between%20the%20sub-regions.%20Such%0Aregional%20relationship%20analyses%20validate%20known%20eye-brain%20links%20and%20reveal%20new%0Adiscriminative%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05117v1&entry.124074799=Read"},
{"title": "Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2", "author": "Tom Lieberum and Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Nicolas Sonnerat and Vikrant Varma and J\u00e1nos Kram\u00e1r and Anca Dragan and Rohin Shah and Neel Nanda", "abstract": "  Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse\ndecomposition of a neural network's latent representations into seemingly\ninterpretable features. Despite recent excitement about their potential,\nresearch applications outside of industry are limited by the high cost of\ntraining a comprehensive suite of SAEs. In this work, we introduce Gemma Scope,\nan open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2\n2B and 9B and select layers of Gemma 2 27B base models. We primarily train SAEs\non the Gemma 2 pre-trained models, but additionally release SAEs trained on\ninstruction-tuned Gemma 2 9B for comparison. We evaluate the quality of each\nSAE on standard metrics and release these results. We hope that by releasing\nthese SAE weights, we can help make more ambitious safety and interpretability\nresearch easier for the community. Weights and a tutorial can be found at\nhttps://huggingface.co/google/gemma-scope and an interactive demo can be found\nat https://www.neuronpedia.org/gemma-scope\n", "link": "http://arxiv.org/abs/2408.05147v1", "date": "2024-08-09", "relevancy": 2.499, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5091}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5063}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemma%20Scope%3A%20Open%20Sparse%20Autoencoders%20Everywhere%20All%20At%20Once%20on%20Gemma%202&body=Title%3A%20Gemma%20Scope%3A%20Open%20Sparse%20Autoencoders%20Everywhere%20All%20At%20Once%20on%20Gemma%202%0AAuthor%3A%20Tom%20Lieberum%20and%20Senthooran%20Rajamanoharan%20and%20Arthur%20Conmy%20and%20Lewis%20Smith%20and%20Nicolas%20Sonnerat%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Anca%20Dragan%20and%20Rohin%20Shah%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20an%20unsupervised%20method%20for%20learning%20a%20sparse%0Adecomposition%20of%20a%20neural%20network%27s%20latent%20representations%20into%20seemingly%0Ainterpretable%20features.%20Despite%20recent%20excitement%20about%20their%20potential%2C%0Aresearch%20applications%20outside%20of%20industry%20are%20limited%20by%20the%20high%20cost%20of%0Atraining%20a%20comprehensive%20suite%20of%20SAEs.%20In%20this%20work%2C%20we%20introduce%20Gemma%20Scope%2C%0Aan%20open%20suite%20of%20JumpReLU%20SAEs%20trained%20on%20all%20layers%20and%20sub-layers%20of%20Gemma%202%0A2B%20and%209B%20and%20select%20layers%20of%20Gemma%202%2027B%20base%20models.%20We%20primarily%20train%20SAEs%0Aon%20the%20Gemma%202%20pre-trained%20models%2C%20but%20additionally%20release%20SAEs%20trained%20on%0Ainstruction-tuned%20Gemma%202%209B%20for%20comparison.%20We%20evaluate%20the%20quality%20of%20each%0ASAE%20on%20standard%20metrics%20and%20release%20these%20results.%20We%20hope%20that%20by%20releasing%0Athese%20SAE%20weights%2C%20we%20can%20help%20make%20more%20ambitious%20safety%20and%20interpretability%0Aresearch%20easier%20for%20the%20community.%20Weights%20and%20a%20tutorial%20can%20be%20found%20at%0Ahttps%3A//huggingface.co/google/gemma-scope%20and%20an%20interactive%20demo%20can%20be%20found%0Aat%20https%3A//www.neuronpedia.org/gemma-scope%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemma%2520Scope%253A%2520Open%2520Sparse%2520Autoencoders%2520Everywhere%2520All%2520At%2520Once%2520on%2520Gemma%25202%26entry.906535625%3DTom%2520Lieberum%2520and%2520Senthooran%2520Rajamanoharan%2520and%2520Arthur%2520Conmy%2520and%2520Lewis%2520Smith%2520and%2520Nicolas%2520Sonnerat%2520and%2520Vikrant%2520Varma%2520and%2520J%25C3%25A1nos%2520Kram%25C3%25A1r%2520and%2520Anca%2520Dragan%2520and%2520Rohin%2520Shah%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520an%2520unsupervised%2520method%2520for%2520learning%2520a%2520sparse%250Adecomposition%2520of%2520a%2520neural%2520network%2527s%2520latent%2520representations%2520into%2520seemingly%250Ainterpretable%2520features.%2520Despite%2520recent%2520excitement%2520about%2520their%2520potential%252C%250Aresearch%2520applications%2520outside%2520of%2520industry%2520are%2520limited%2520by%2520the%2520high%2520cost%2520of%250Atraining%2520a%2520comprehensive%2520suite%2520of%2520SAEs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Gemma%2520Scope%252C%250Aan%2520open%2520suite%2520of%2520JumpReLU%2520SAEs%2520trained%2520on%2520all%2520layers%2520and%2520sub-layers%2520of%2520Gemma%25202%250A2B%2520and%25209B%2520and%2520select%2520layers%2520of%2520Gemma%25202%252027B%2520base%2520models.%2520We%2520primarily%2520train%2520SAEs%250Aon%2520the%2520Gemma%25202%2520pre-trained%2520models%252C%2520but%2520additionally%2520release%2520SAEs%2520trained%2520on%250Ainstruction-tuned%2520Gemma%25202%25209B%2520for%2520comparison.%2520We%2520evaluate%2520the%2520quality%2520of%2520each%250ASAE%2520on%2520standard%2520metrics%2520and%2520release%2520these%2520results.%2520We%2520hope%2520that%2520by%2520releasing%250Athese%2520SAE%2520weights%252C%2520we%2520can%2520help%2520make%2520more%2520ambitious%2520safety%2520and%2520interpretability%250Aresearch%2520easier%2520for%2520the%2520community.%2520Weights%2520and%2520a%2520tutorial%2520can%2520be%2520found%2520at%250Ahttps%253A//huggingface.co/google/gemma-scope%2520and%2520an%2520interactive%2520demo%2520can%2520be%2520found%250Aat%2520https%253A//www.neuronpedia.org/gemma-scope%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemma%20Scope%3A%20Open%20Sparse%20Autoencoders%20Everywhere%20All%20At%20Once%20on%20Gemma%202&entry.906535625=Tom%20Lieberum%20and%20Senthooran%20Rajamanoharan%20and%20Arthur%20Conmy%20and%20Lewis%20Smith%20and%20Nicolas%20Sonnerat%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Anca%20Dragan%20and%20Rohin%20Shah%20and%20Neel%20Nanda&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20an%20unsupervised%20method%20for%20learning%20a%20sparse%0Adecomposition%20of%20a%20neural%20network%27s%20latent%20representations%20into%20seemingly%0Ainterpretable%20features.%20Despite%20recent%20excitement%20about%20their%20potential%2C%0Aresearch%20applications%20outside%20of%20industry%20are%20limited%20by%20the%20high%20cost%20of%0Atraining%20a%20comprehensive%20suite%20of%20SAEs.%20In%20this%20work%2C%20we%20introduce%20Gemma%20Scope%2C%0Aan%20open%20suite%20of%20JumpReLU%20SAEs%20trained%20on%20all%20layers%20and%20sub-layers%20of%20Gemma%202%0A2B%20and%209B%20and%20select%20layers%20of%20Gemma%202%2027B%20base%20models.%20We%20primarily%20train%20SAEs%0Aon%20the%20Gemma%202%20pre-trained%20models%2C%20but%20additionally%20release%20SAEs%20trained%20on%0Ainstruction-tuned%20Gemma%202%209B%20for%20comparison.%20We%20evaluate%20the%20quality%20of%20each%0ASAE%20on%20standard%20metrics%20and%20release%20these%20results.%20We%20hope%20that%20by%20releasing%0Athese%20SAE%20weights%2C%20we%20can%20help%20make%20more%20ambitious%20safety%20and%20interpretability%0Aresearch%20easier%20for%20the%20community.%20Weights%20and%20a%20tutorial%20can%20be%20found%20at%0Ahttps%3A//huggingface.co/google/gemma-scope%20and%20an%20interactive%20demo%20can%20be%20found%0Aat%20https%3A//www.neuronpedia.org/gemma-scope%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05147v1&entry.124074799=Read"},
{"title": "Rehabilitation Exercise Quality Assessment through Supervised\n  Contrastive Learning with Hard and Soft Negatives", "author": "Mark Karlov and Ali Abedi and Shehroz S. Khan", "abstract": "  Exercise-based rehabilitation programs have proven to be effective in\nenhancing the quality of life and reducing mortality and rehospitalization\nrates. AI-driven virtual rehabilitation, which allows patients to independently\ncomplete exercises at home, utilizes AI algorithms to analyze exercise data,\nproviding feedback to patients and updating clinicians on their progress. These\nprograms commonly prescribe a variety of exercise types, leading to a distinct\nchallenge in rehabilitation exercise assessment datasets: while abundant in\noverall training samples, these datasets often have a limited number of samples\nfor each individual exercise type. This disparity hampers the ability of\nexisting approaches to train generalizable models with such a small sample size\nper exercise type. Addressing this issue, this paper introduces a novel\nsupervised contrastive learning framework with hard and soft negative samples\nthat effectively utilizes the entire dataset to train a single model applicable\nto all exercise types. This model, with a Spatial-Temporal Graph Convolutional\nNetwork (ST-GCN) architecture, demonstrated enhanced generalizability across\nexercises and a decrease in overall complexity. Through extensive experiments\non three publicly available rehabilitation exercise assessment datasets,\nUI-PRMD, IRDS, and KIMORE, our method has proven to surpass existing methods,\nsetting a new benchmark in rehabilitation exercise quality assessment.\n", "link": "http://arxiv.org/abs/2403.02772v2", "date": "2024-08-09", "relevancy": 2.4401, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5027}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4906}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rehabilitation%20Exercise%20Quality%20Assessment%20through%20Supervised%0A%20%20Contrastive%20Learning%20with%20Hard%20and%20Soft%20Negatives&body=Title%3A%20Rehabilitation%20Exercise%20Quality%20Assessment%20through%20Supervised%0A%20%20Contrastive%20Learning%20with%20Hard%20and%20Soft%20Negatives%0AAuthor%3A%20Mark%20Karlov%20and%20Ali%20Abedi%20and%20Shehroz%20S.%20Khan%0AAbstract%3A%20%20%20Exercise-based%20rehabilitation%20programs%20have%20proven%20to%20be%20effective%20in%0Aenhancing%20the%20quality%20of%20life%20and%20reducing%20mortality%20and%20rehospitalization%0Arates.%20AI-driven%20virtual%20rehabilitation%2C%20which%20allows%20patients%20to%20independently%0Acomplete%20exercises%20at%20home%2C%20utilizes%20AI%20algorithms%20to%20analyze%20exercise%20data%2C%0Aproviding%20feedback%20to%20patients%20and%20updating%20clinicians%20on%20their%20progress.%20These%0Aprograms%20commonly%20prescribe%20a%20variety%20of%20exercise%20types%2C%20leading%20to%20a%20distinct%0Achallenge%20in%20rehabilitation%20exercise%20assessment%20datasets%3A%20while%20abundant%20in%0Aoverall%20training%20samples%2C%20these%20datasets%20often%20have%20a%20limited%20number%20of%20samples%0Afor%20each%20individual%20exercise%20type.%20This%20disparity%20hampers%20the%20ability%20of%0Aexisting%20approaches%20to%20train%20generalizable%20models%20with%20such%20a%20small%20sample%20size%0Aper%20exercise%20type.%20Addressing%20this%20issue%2C%20this%20paper%20introduces%20a%20novel%0Asupervised%20contrastive%20learning%20framework%20with%20hard%20and%20soft%20negative%20samples%0Athat%20effectively%20utilizes%20the%20entire%20dataset%20to%20train%20a%20single%20model%20applicable%0Ato%20all%20exercise%20types.%20This%20model%2C%20with%20a%20Spatial-Temporal%20Graph%20Convolutional%0ANetwork%20%28ST-GCN%29%20architecture%2C%20demonstrated%20enhanced%20generalizability%20across%0Aexercises%20and%20a%20decrease%20in%20overall%20complexity.%20Through%20extensive%20experiments%0Aon%20three%20publicly%20available%20rehabilitation%20exercise%20assessment%20datasets%2C%0AUI-PRMD%2C%20IRDS%2C%20and%20KIMORE%2C%20our%20method%20has%20proven%20to%20surpass%20existing%20methods%2C%0Asetting%20a%20new%20benchmark%20in%20rehabilitation%20exercise%20quality%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRehabilitation%2520Exercise%2520Quality%2520Assessment%2520through%2520Supervised%250A%2520%2520Contrastive%2520Learning%2520with%2520Hard%2520and%2520Soft%2520Negatives%26entry.906535625%3DMark%2520Karlov%2520and%2520Ali%2520Abedi%2520and%2520Shehroz%2520S.%2520Khan%26entry.1292438233%3D%2520%2520Exercise-based%2520rehabilitation%2520programs%2520have%2520proven%2520to%2520be%2520effective%2520in%250Aenhancing%2520the%2520quality%2520of%2520life%2520and%2520reducing%2520mortality%2520and%2520rehospitalization%250Arates.%2520AI-driven%2520virtual%2520rehabilitation%252C%2520which%2520allows%2520patients%2520to%2520independently%250Acomplete%2520exercises%2520at%2520home%252C%2520utilizes%2520AI%2520algorithms%2520to%2520analyze%2520exercise%2520data%252C%250Aproviding%2520feedback%2520to%2520patients%2520and%2520updating%2520clinicians%2520on%2520their%2520progress.%2520These%250Aprograms%2520commonly%2520prescribe%2520a%2520variety%2520of%2520exercise%2520types%252C%2520leading%2520to%2520a%2520distinct%250Achallenge%2520in%2520rehabilitation%2520exercise%2520assessment%2520datasets%253A%2520while%2520abundant%2520in%250Aoverall%2520training%2520samples%252C%2520these%2520datasets%2520often%2520have%2520a%2520limited%2520number%2520of%2520samples%250Afor%2520each%2520individual%2520exercise%2520type.%2520This%2520disparity%2520hampers%2520the%2520ability%2520of%250Aexisting%2520approaches%2520to%2520train%2520generalizable%2520models%2520with%2520such%2520a%2520small%2520sample%2520size%250Aper%2520exercise%2520type.%2520Addressing%2520this%2520issue%252C%2520this%2520paper%2520introduces%2520a%2520novel%250Asupervised%2520contrastive%2520learning%2520framework%2520with%2520hard%2520and%2520soft%2520negative%2520samples%250Athat%2520effectively%2520utilizes%2520the%2520entire%2520dataset%2520to%2520train%2520a%2520single%2520model%2520applicable%250Ato%2520all%2520exercise%2520types.%2520This%2520model%252C%2520with%2520a%2520Spatial-Temporal%2520Graph%2520Convolutional%250ANetwork%2520%2528ST-GCN%2529%2520architecture%252C%2520demonstrated%2520enhanced%2520generalizability%2520across%250Aexercises%2520and%2520a%2520decrease%2520in%2520overall%2520complexity.%2520Through%2520extensive%2520experiments%250Aon%2520three%2520publicly%2520available%2520rehabilitation%2520exercise%2520assessment%2520datasets%252C%250AUI-PRMD%252C%2520IRDS%252C%2520and%2520KIMORE%252C%2520our%2520method%2520has%2520proven%2520to%2520surpass%2520existing%2520methods%252C%250Asetting%2520a%2520new%2520benchmark%2520in%2520rehabilitation%2520exercise%2520quality%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rehabilitation%20Exercise%20Quality%20Assessment%20through%20Supervised%0A%20%20Contrastive%20Learning%20with%20Hard%20and%20Soft%20Negatives&entry.906535625=Mark%20Karlov%20and%20Ali%20Abedi%20and%20Shehroz%20S.%20Khan&entry.1292438233=%20%20Exercise-based%20rehabilitation%20programs%20have%20proven%20to%20be%20effective%20in%0Aenhancing%20the%20quality%20of%20life%20and%20reducing%20mortality%20and%20rehospitalization%0Arates.%20AI-driven%20virtual%20rehabilitation%2C%20which%20allows%20patients%20to%20independently%0Acomplete%20exercises%20at%20home%2C%20utilizes%20AI%20algorithms%20to%20analyze%20exercise%20data%2C%0Aproviding%20feedback%20to%20patients%20and%20updating%20clinicians%20on%20their%20progress.%20These%0Aprograms%20commonly%20prescribe%20a%20variety%20of%20exercise%20types%2C%20leading%20to%20a%20distinct%0Achallenge%20in%20rehabilitation%20exercise%20assessment%20datasets%3A%20while%20abundant%20in%0Aoverall%20training%20samples%2C%20these%20datasets%20often%20have%20a%20limited%20number%20of%20samples%0Afor%20each%20individual%20exercise%20type.%20This%20disparity%20hampers%20the%20ability%20of%0Aexisting%20approaches%20to%20train%20generalizable%20models%20with%20such%20a%20small%20sample%20size%0Aper%20exercise%20type.%20Addressing%20this%20issue%2C%20this%20paper%20introduces%20a%20novel%0Asupervised%20contrastive%20learning%20framework%20with%20hard%20and%20soft%20negative%20samples%0Athat%20effectively%20utilizes%20the%20entire%20dataset%20to%20train%20a%20single%20model%20applicable%0Ato%20all%20exercise%20types.%20This%20model%2C%20with%20a%20Spatial-Temporal%20Graph%20Convolutional%0ANetwork%20%28ST-GCN%29%20architecture%2C%20demonstrated%20enhanced%20generalizability%20across%0Aexercises%20and%20a%20decrease%20in%20overall%20complexity.%20Through%20extensive%20experiments%0Aon%20three%20publicly%20available%20rehabilitation%20exercise%20assessment%20datasets%2C%0AUI-PRMD%2C%20IRDS%2C%20and%20KIMORE%2C%20our%20method%20has%20proven%20to%20surpass%20existing%20methods%2C%0Asetting%20a%20new%20benchmark%20in%20rehabilitation%20exercise%20quality%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02772v2&entry.124074799=Read"},
{"title": "Low Fidelity Digital Twin for Automated Driving Systems: Use Cases and\n  Automatic Generation", "author": "Jiri Vlasak and Jaroslav Klap\u00e1lek and Adam Kollar\u010d\u00edk and Michal Sojka and Zden\u011bk Hanz\u00e1lek", "abstract": "  Automated driving systems are an integral part of the automotive industry.\nTools such as Robot Operating System and simulators support their development.\nHowever, in the end, the developers must test their algorithms on a real\nvehicle. To better observe the difference between reality and simulation--the\nreality gap--digital twin technology offers real-time communication between the\nreal vehicle and its model. We present low fidelity digital twin generator and\ndescribe situations where automatic generation is preferable to high fidelity\nsimulation. We validated our approach of generating a virtual environment with\na vehicle model by replaying the data recorded from the real vehicle.\n", "link": "http://arxiv.org/abs/2405.13705v3", "date": "2024-08-09", "relevancy": 2.4197, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.484}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.484}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low%20Fidelity%20Digital%20Twin%20for%20Automated%20Driving%20Systems%3A%20Use%20Cases%20and%0A%20%20Automatic%20Generation&body=Title%3A%20Low%20Fidelity%20Digital%20Twin%20for%20Automated%20Driving%20Systems%3A%20Use%20Cases%20and%0A%20%20Automatic%20Generation%0AAuthor%3A%20Jiri%20Vlasak%20and%20Jaroslav%20Klap%C3%A1lek%20and%20Adam%20Kollar%C4%8D%C3%ADk%20and%20Michal%20Sojka%20and%20Zden%C4%9Bk%20Hanz%C3%A1lek%0AAbstract%3A%20%20%20Automated%20driving%20systems%20are%20an%20integral%20part%20of%20the%20automotive%20industry.%0ATools%20such%20as%20Robot%20Operating%20System%20and%20simulators%20support%20their%20development.%0AHowever%2C%20in%20the%20end%2C%20the%20developers%20must%20test%20their%20algorithms%20on%20a%20real%0Avehicle.%20To%20better%20observe%20the%20difference%20between%20reality%20and%20simulation--the%0Areality%20gap--digital%20twin%20technology%20offers%20real-time%20communication%20between%20the%0Areal%20vehicle%20and%20its%20model.%20We%20present%20low%20fidelity%20digital%20twin%20generator%20and%0Adescribe%20situations%20where%20automatic%20generation%20is%20preferable%20to%20high%20fidelity%0Asimulation.%20We%20validated%20our%20approach%20of%20generating%20a%20virtual%20environment%20with%0Aa%20vehicle%20model%20by%20replaying%20the%20data%20recorded%20from%20the%20real%20vehicle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13705v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow%2520Fidelity%2520Digital%2520Twin%2520for%2520Automated%2520Driving%2520Systems%253A%2520Use%2520Cases%2520and%250A%2520%2520Automatic%2520Generation%26entry.906535625%3DJiri%2520Vlasak%2520and%2520Jaroslav%2520Klap%25C3%25A1lek%2520and%2520Adam%2520Kollar%25C4%258D%25C3%25ADk%2520and%2520Michal%2520Sojka%2520and%2520Zden%25C4%259Bk%2520Hanz%25C3%25A1lek%26entry.1292438233%3D%2520%2520Automated%2520driving%2520systems%2520are%2520an%2520integral%2520part%2520of%2520the%2520automotive%2520industry.%250ATools%2520such%2520as%2520Robot%2520Operating%2520System%2520and%2520simulators%2520support%2520their%2520development.%250AHowever%252C%2520in%2520the%2520end%252C%2520the%2520developers%2520must%2520test%2520their%2520algorithms%2520on%2520a%2520real%250Avehicle.%2520To%2520better%2520observe%2520the%2520difference%2520between%2520reality%2520and%2520simulation--the%250Areality%2520gap--digital%2520twin%2520technology%2520offers%2520real-time%2520communication%2520between%2520the%250Areal%2520vehicle%2520and%2520its%2520model.%2520We%2520present%2520low%2520fidelity%2520digital%2520twin%2520generator%2520and%250Adescribe%2520situations%2520where%2520automatic%2520generation%2520is%2520preferable%2520to%2520high%2520fidelity%250Asimulation.%2520We%2520validated%2520our%2520approach%2520of%2520generating%2520a%2520virtual%2520environment%2520with%250Aa%2520vehicle%2520model%2520by%2520replaying%2520the%2520data%2520recorded%2520from%2520the%2520real%2520vehicle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13705v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Fidelity%20Digital%20Twin%20for%20Automated%20Driving%20Systems%3A%20Use%20Cases%20and%0A%20%20Automatic%20Generation&entry.906535625=Jiri%20Vlasak%20and%20Jaroslav%20Klap%C3%A1lek%20and%20Adam%20Kollar%C4%8D%C3%ADk%20and%20Michal%20Sojka%20and%20Zden%C4%9Bk%20Hanz%C3%A1lek&entry.1292438233=%20%20Automated%20driving%20systems%20are%20an%20integral%20part%20of%20the%20automotive%20industry.%0ATools%20such%20as%20Robot%20Operating%20System%20and%20simulators%20support%20their%20development.%0AHowever%2C%20in%20the%20end%2C%20the%20developers%20must%20test%20their%20algorithms%20on%20a%20real%0Avehicle.%20To%20better%20observe%20the%20difference%20between%20reality%20and%20simulation--the%0Areality%20gap--digital%20twin%20technology%20offers%20real-time%20communication%20between%20the%0Areal%20vehicle%20and%20its%20model.%20We%20present%20low%20fidelity%20digital%20twin%20generator%20and%0Adescribe%20situations%20where%20automatic%20generation%20is%20preferable%20to%20high%20fidelity%0Asimulation.%20We%20validated%20our%20approach%20of%20generating%20a%20virtual%20environment%20with%0Aa%20vehicle%20model%20by%20replaying%20the%20data%20recorded%20from%20the%20real%20vehicle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13705v3&entry.124074799=Read"},
{"title": "Federated Hypergraph Learning with Hyperedge Completion", "author": "Linfeng Luo and Fengxiao Tang and Xiyu Liu and Zhiqi Guo and Zihao Qiu and Ming Zhao", "abstract": "  Hypergraph neural networks enhance conventional graph neural networks by\ncapturing high-order relationships among nodes, which proves vital in data-rich\nenvironments where interactions are not merely pairwise. As data complexity and\ninterconnectivity grow, it is common for graph-structured data to be split and\nstored in a distributed manner, underscoring the necessity of federated\nlearning on subgraphs. In this work, we propose FedHGN, a novel algorithm for\nfederated hypergraph learning. Our algorithm utilizes subgraphs of a hypergraph\nstored on distributed devices to train local HGNN models in a federated\nmanner:by collaboratively developing an effective global HGNN model through\nsharing model parameters while preserving client privacy. Additionally,\nconsidering that hyperedges may span multiple clients, a pre-training step is\nemployed before the training process in which cross-client hyperedge feature\ngathering is performed at the central server. In this way, the missing\ncross-client information can be supplemented from the central server during the\nnode feature aggregation phase. Experimental results on seven real-world\ndatasets confirm the effectiveness of our approach and demonstrate its\nperformance advantages over traditional federated graph learning methods.\n", "link": "http://arxiv.org/abs/2408.05160v1", "date": "2024-08-09", "relevancy": 2.4073, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4955}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4756}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Hypergraph%20Learning%20with%20Hyperedge%20Completion&body=Title%3A%20Federated%20Hypergraph%20Learning%20with%20Hyperedge%20Completion%0AAuthor%3A%20Linfeng%20Luo%20and%20Fengxiao%20Tang%20and%20Xiyu%20Liu%20and%20Zhiqi%20Guo%20and%20Zihao%20Qiu%20and%20Ming%20Zhao%0AAbstract%3A%20%20%20Hypergraph%20neural%20networks%20enhance%20conventional%20graph%20neural%20networks%20by%0Acapturing%20high-order%20relationships%20among%20nodes%2C%20which%20proves%20vital%20in%20data-rich%0Aenvironments%20where%20interactions%20are%20not%20merely%20pairwise.%20As%20data%20complexity%20and%0Ainterconnectivity%20grow%2C%20it%20is%20common%20for%20graph-structured%20data%20to%20be%20split%20and%0Astored%20in%20a%20distributed%20manner%2C%20underscoring%20the%20necessity%20of%20federated%0Alearning%20on%20subgraphs.%20In%20this%20work%2C%20we%20propose%20FedHGN%2C%20a%20novel%20algorithm%20for%0Afederated%20hypergraph%20learning.%20Our%20algorithm%20utilizes%20subgraphs%20of%20a%20hypergraph%0Astored%20on%20distributed%20devices%20to%20train%20local%20HGNN%20models%20in%20a%20federated%0Amanner%3Aby%20collaboratively%20developing%20an%20effective%20global%20HGNN%20model%20through%0Asharing%20model%20parameters%20while%20preserving%20client%20privacy.%20Additionally%2C%0Aconsidering%20that%20hyperedges%20may%20span%20multiple%20clients%2C%20a%20pre-training%20step%20is%0Aemployed%20before%20the%20training%20process%20in%20which%20cross-client%20hyperedge%20feature%0Agathering%20is%20performed%20at%20the%20central%20server.%20In%20this%20way%2C%20the%20missing%0Across-client%20information%20can%20be%20supplemented%20from%20the%20central%20server%20during%20the%0Anode%20feature%20aggregation%20phase.%20Experimental%20results%20on%20seven%20real-world%0Adatasets%20confirm%20the%20effectiveness%20of%20our%20approach%20and%20demonstrate%20its%0Aperformance%20advantages%20over%20traditional%20federated%20graph%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Hypergraph%2520Learning%2520with%2520Hyperedge%2520Completion%26entry.906535625%3DLinfeng%2520Luo%2520and%2520Fengxiao%2520Tang%2520and%2520Xiyu%2520Liu%2520and%2520Zhiqi%2520Guo%2520and%2520Zihao%2520Qiu%2520and%2520Ming%2520Zhao%26entry.1292438233%3D%2520%2520Hypergraph%2520neural%2520networks%2520enhance%2520conventional%2520graph%2520neural%2520networks%2520by%250Acapturing%2520high-order%2520relationships%2520among%2520nodes%252C%2520which%2520proves%2520vital%2520in%2520data-rich%250Aenvironments%2520where%2520interactions%2520are%2520not%2520merely%2520pairwise.%2520As%2520data%2520complexity%2520and%250Ainterconnectivity%2520grow%252C%2520it%2520is%2520common%2520for%2520graph-structured%2520data%2520to%2520be%2520split%2520and%250Astored%2520in%2520a%2520distributed%2520manner%252C%2520underscoring%2520the%2520necessity%2520of%2520federated%250Alearning%2520on%2520subgraphs.%2520In%2520this%2520work%252C%2520we%2520propose%2520FedHGN%252C%2520a%2520novel%2520algorithm%2520for%250Afederated%2520hypergraph%2520learning.%2520Our%2520algorithm%2520utilizes%2520subgraphs%2520of%2520a%2520hypergraph%250Astored%2520on%2520distributed%2520devices%2520to%2520train%2520local%2520HGNN%2520models%2520in%2520a%2520federated%250Amanner%253Aby%2520collaboratively%2520developing%2520an%2520effective%2520global%2520HGNN%2520model%2520through%250Asharing%2520model%2520parameters%2520while%2520preserving%2520client%2520privacy.%2520Additionally%252C%250Aconsidering%2520that%2520hyperedges%2520may%2520span%2520multiple%2520clients%252C%2520a%2520pre-training%2520step%2520is%250Aemployed%2520before%2520the%2520training%2520process%2520in%2520which%2520cross-client%2520hyperedge%2520feature%250Agathering%2520is%2520performed%2520at%2520the%2520central%2520server.%2520In%2520this%2520way%252C%2520the%2520missing%250Across-client%2520information%2520can%2520be%2520supplemented%2520from%2520the%2520central%2520server%2520during%2520the%250Anode%2520feature%2520aggregation%2520phase.%2520Experimental%2520results%2520on%2520seven%2520real-world%250Adatasets%2520confirm%2520the%2520effectiveness%2520of%2520our%2520approach%2520and%2520demonstrate%2520its%250Aperformance%2520advantages%2520over%2520traditional%2520federated%2520graph%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Hypergraph%20Learning%20with%20Hyperedge%20Completion&entry.906535625=Linfeng%20Luo%20and%20Fengxiao%20Tang%20and%20Xiyu%20Liu%20and%20Zhiqi%20Guo%20and%20Zihao%20Qiu%20and%20Ming%20Zhao&entry.1292438233=%20%20Hypergraph%20neural%20networks%20enhance%20conventional%20graph%20neural%20networks%20by%0Acapturing%20high-order%20relationships%20among%20nodes%2C%20which%20proves%20vital%20in%20data-rich%0Aenvironments%20where%20interactions%20are%20not%20merely%20pairwise.%20As%20data%20complexity%20and%0Ainterconnectivity%20grow%2C%20it%20is%20common%20for%20graph-structured%20data%20to%20be%20split%20and%0Astored%20in%20a%20distributed%20manner%2C%20underscoring%20the%20necessity%20of%20federated%0Alearning%20on%20subgraphs.%20In%20this%20work%2C%20we%20propose%20FedHGN%2C%20a%20novel%20algorithm%20for%0Afederated%20hypergraph%20learning.%20Our%20algorithm%20utilizes%20subgraphs%20of%20a%20hypergraph%0Astored%20on%20distributed%20devices%20to%20train%20local%20HGNN%20models%20in%20a%20federated%0Amanner%3Aby%20collaboratively%20developing%20an%20effective%20global%20HGNN%20model%20through%0Asharing%20model%20parameters%20while%20preserving%20client%20privacy.%20Additionally%2C%0Aconsidering%20that%20hyperedges%20may%20span%20multiple%20clients%2C%20a%20pre-training%20step%20is%0Aemployed%20before%20the%20training%20process%20in%20which%20cross-client%20hyperedge%20feature%0Agathering%20is%20performed%20at%20the%20central%20server.%20In%20this%20way%2C%20the%20missing%0Across-client%20information%20can%20be%20supplemented%20from%20the%20central%20server%20during%20the%0Anode%20feature%20aggregation%20phase.%20Experimental%20results%20on%20seven%20real-world%0Adatasets%20confirm%20the%20effectiveness%20of%20our%20approach%20and%20demonstrate%20its%0Aperformance%20advantages%20over%20traditional%20federated%20graph%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05160v1&entry.124074799=Read"},
{"title": "DeepInteraction++: Multi-Modality Interaction for Autonomous Driving", "author": "Zeyu Yang and Nan Song and Wei Li and Xiatian Zhu and Li Zhang and Philip H. S. Torr", "abstract": "  Existing top-performance autonomous driving systems typically rely on the\nmulti-modal fusion strategy for reliable scene understanding. This design is\nhowever fundamentally restricted due to overlooking the modality-specific\nstrengths and finally hampering the model performance. To address this\nlimitation, in this work, we introduce a novel modality interaction strategy\nthat allows individual per-modality representations to be learned and\nmaintained throughout, enabling their unique characteristics to be exploited\nduring the whole perception pipeline. To demonstrate the effectiveness of the\nproposed strategy, we design DeepInteraction++, a multi-modal interaction\nframework characterized by a multi-modal representational interaction encoder\nand a multi-modal predictive interaction decoder. Specifically, the encoder is\nimplemented as a dual-stream Transformer with specialized attention operation\nfor information exchange and integration between separate modality-specific\nrepresentations. Our multi-modal representational learning incorporates both\nobject-centric, precise sampling-based feature alignment and global dense\ninformation spreading, essential for the more challenging planning task. The\ndecoder is designed to iteratively refine the predictions by alternately\naggregating information from separate representations in a unified\nmodality-agnostic manner, realizing multi-modal predictive interaction.\nExtensive experiments demonstrate the superior performance of the proposed\nframework on both 3D object detection and end-to-end autonomous driving tasks.\nOur code is available at https://github.com/fudan-zvg/DeepInteraction.\n", "link": "http://arxiv.org/abs/2408.05075v1", "date": "2024-08-09", "relevancy": 2.3527, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5985}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving&body=Title%3A%20DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving%0AAuthor%3A%20Zeyu%20Yang%20and%20Nan%20Song%20and%20Wei%20Li%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%20and%20Philip%20H.%20S.%20Torr%0AAbstract%3A%20%20%20Existing%20top-performance%20autonomous%20driving%20systems%20typically%20rely%20on%20the%0Amulti-modal%20fusion%20strategy%20for%20reliable%20scene%20understanding.%20This%20design%20is%0Ahowever%20fundamentally%20restricted%20due%20to%20overlooking%20the%20modality-specific%0Astrengths%20and%20finally%20hampering%20the%20model%20performance.%20To%20address%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20introduce%20a%20novel%20modality%20interaction%20strategy%0Athat%20allows%20individual%20per-modality%20representations%20to%20be%20learned%20and%0Amaintained%20throughout%2C%20enabling%20their%20unique%20characteristics%20to%20be%20exploited%0Aduring%20the%20whole%20perception%20pipeline.%20To%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20strategy%2C%20we%20design%20DeepInteraction%2B%2B%2C%20a%20multi-modal%20interaction%0Aframework%20characterized%20by%20a%20multi-modal%20representational%20interaction%20encoder%0Aand%20a%20multi-modal%20predictive%20interaction%20decoder.%20Specifically%2C%20the%20encoder%20is%0Aimplemented%20as%20a%20dual-stream%20Transformer%20with%20specialized%20attention%20operation%0Afor%20information%20exchange%20and%20integration%20between%20separate%20modality-specific%0Arepresentations.%20Our%20multi-modal%20representational%20learning%20incorporates%20both%0Aobject-centric%2C%20precise%20sampling-based%20feature%20alignment%20and%20global%20dense%0Ainformation%20spreading%2C%20essential%20for%20the%20more%20challenging%20planning%20task.%20The%0Adecoder%20is%20designed%20to%20iteratively%20refine%20the%20predictions%20by%20alternately%0Aaggregating%20information%20from%20separate%20representations%20in%20a%20unified%0Amodality-agnostic%20manner%2C%20realizing%20multi-modal%20predictive%20interaction.%0AExtensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%0Aframework%20on%20both%203D%20object%20detection%20and%20end-to-end%20autonomous%20driving%20tasks.%0AOur%20code%20is%20available%20at%20https%3A//github.com/fudan-zvg/DeepInteraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepInteraction%252B%252B%253A%2520Multi-Modality%2520Interaction%2520for%2520Autonomous%2520Driving%26entry.906535625%3DZeyu%2520Yang%2520and%2520Nan%2520Song%2520and%2520Wei%2520Li%2520and%2520Xiatian%2520Zhu%2520and%2520Li%2520Zhang%2520and%2520Philip%2520H.%2520S.%2520Torr%26entry.1292438233%3D%2520%2520Existing%2520top-performance%2520autonomous%2520driving%2520systems%2520typically%2520rely%2520on%2520the%250Amulti-modal%2520fusion%2520strategy%2520for%2520reliable%2520scene%2520understanding.%2520This%2520design%2520is%250Ahowever%2520fundamentally%2520restricted%2520due%2520to%2520overlooking%2520the%2520modality-specific%250Astrengths%2520and%2520finally%2520hampering%2520the%2520model%2520performance.%2520To%2520address%2520this%250Alimitation%252C%2520in%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520modality%2520interaction%2520strategy%250Athat%2520allows%2520individual%2520per-modality%2520representations%2520to%2520be%2520learned%2520and%250Amaintained%2520throughout%252C%2520enabling%2520their%2520unique%2520characteristics%2520to%2520be%2520exploited%250Aduring%2520the%2520whole%2520perception%2520pipeline.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520strategy%252C%2520we%2520design%2520DeepInteraction%252B%252B%252C%2520a%2520multi-modal%2520interaction%250Aframework%2520characterized%2520by%2520a%2520multi-modal%2520representational%2520interaction%2520encoder%250Aand%2520a%2520multi-modal%2520predictive%2520interaction%2520decoder.%2520Specifically%252C%2520the%2520encoder%2520is%250Aimplemented%2520as%2520a%2520dual-stream%2520Transformer%2520with%2520specialized%2520attention%2520operation%250Afor%2520information%2520exchange%2520and%2520integration%2520between%2520separate%2520modality-specific%250Arepresentations.%2520Our%2520multi-modal%2520representational%2520learning%2520incorporates%2520both%250Aobject-centric%252C%2520precise%2520sampling-based%2520feature%2520alignment%2520and%2520global%2520dense%250Ainformation%2520spreading%252C%2520essential%2520for%2520the%2520more%2520challenging%2520planning%2520task.%2520The%250Adecoder%2520is%2520designed%2520to%2520iteratively%2520refine%2520the%2520predictions%2520by%2520alternately%250Aaggregating%2520information%2520from%2520separate%2520representations%2520in%2520a%2520unified%250Amodality-agnostic%2520manner%252C%2520realizing%2520multi-modal%2520predictive%2520interaction.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%250Aframework%2520on%2520both%25203D%2520object%2520detection%2520and%2520end-to-end%2520autonomous%2520driving%2520tasks.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/fudan-zvg/DeepInteraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving&entry.906535625=Zeyu%20Yang%20and%20Nan%20Song%20and%20Wei%20Li%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%20and%20Philip%20H.%20S.%20Torr&entry.1292438233=%20%20Existing%20top-performance%20autonomous%20driving%20systems%20typically%20rely%20on%20the%0Amulti-modal%20fusion%20strategy%20for%20reliable%20scene%20understanding.%20This%20design%20is%0Ahowever%20fundamentally%20restricted%20due%20to%20overlooking%20the%20modality-specific%0Astrengths%20and%20finally%20hampering%20the%20model%20performance.%20To%20address%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20introduce%20a%20novel%20modality%20interaction%20strategy%0Athat%20allows%20individual%20per-modality%20representations%20to%20be%20learned%20and%0Amaintained%20throughout%2C%20enabling%20their%20unique%20characteristics%20to%20be%20exploited%0Aduring%20the%20whole%20perception%20pipeline.%20To%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20strategy%2C%20we%20design%20DeepInteraction%2B%2B%2C%20a%20multi-modal%20interaction%0Aframework%20characterized%20by%20a%20multi-modal%20representational%20interaction%20encoder%0Aand%20a%20multi-modal%20predictive%20interaction%20decoder.%20Specifically%2C%20the%20encoder%20is%0Aimplemented%20as%20a%20dual-stream%20Transformer%20with%20specialized%20attention%20operation%0Afor%20information%20exchange%20and%20integration%20between%20separate%20modality-specific%0Arepresentations.%20Our%20multi-modal%20representational%20learning%20incorporates%20both%0Aobject-centric%2C%20precise%20sampling-based%20feature%20alignment%20and%20global%20dense%0Ainformation%20spreading%2C%20essential%20for%20the%20more%20challenging%20planning%20task.%20The%0Adecoder%20is%20designed%20to%20iteratively%20refine%20the%20predictions%20by%20alternately%0Aaggregating%20information%20from%20separate%20representations%20in%20a%20unified%0Amodality-agnostic%20manner%2C%20realizing%20multi-modal%20predictive%20interaction.%0AExtensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%0Aframework%20on%20both%203D%20object%20detection%20and%20end-to-end%20autonomous%20driving%20tasks.%0AOur%20code%20is%20available%20at%20https%3A//github.com/fudan-zvg/DeepInteraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05075v1&entry.124074799=Read"},
{"title": "GLEAMS: Bridging the Gap Between Local and Global Explanations", "author": "Giorgio Visani and Vincenzo Stanzione and Damien Garreau", "abstract": "  The explainability of machine learning algorithms is crucial, and numerous\nmethods have emerged recently. Local, post-hoc methods assign an attribution\nscore to each feature, indicating its importance for the prediction. However,\nthese methods require recalculating explanations for each example. On the other\nside, while there exist global approaches they often produce explanations that\nare either overly simplistic and unreliable or excessively complex. To bridge\nthis gap, we propose GLEAMS, a novel method that partitions the input space and\nlearns an interpretable model within each sub-region, thereby providing both\nfaithful local and global surrogates. We demonstrate GLEAMS' effectiveness on\nboth synthetic and real-world data, highlighting its desirable properties and\nhuman-understandable insights.\n", "link": "http://arxiv.org/abs/2408.05060v1", "date": "2024-08-09", "relevancy": 2.3313, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4715}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4642}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLEAMS%3A%20Bridging%20the%20Gap%20Between%20Local%20and%20Global%20Explanations&body=Title%3A%20GLEAMS%3A%20Bridging%20the%20Gap%20Between%20Local%20and%20Global%20Explanations%0AAuthor%3A%20Giorgio%20Visani%20and%20Vincenzo%20Stanzione%20and%20Damien%20Garreau%0AAbstract%3A%20%20%20The%20explainability%20of%20machine%20learning%20algorithms%20is%20crucial%2C%20and%20numerous%0Amethods%20have%20emerged%20recently.%20Local%2C%20post-hoc%20methods%20assign%20an%20attribution%0Ascore%20to%20each%20feature%2C%20indicating%20its%20importance%20for%20the%20prediction.%20However%2C%0Athese%20methods%20require%20recalculating%20explanations%20for%20each%20example.%20On%20the%20other%0Aside%2C%20while%20there%20exist%20global%20approaches%20they%20often%20produce%20explanations%20that%0Aare%20either%20overly%20simplistic%20and%20unreliable%20or%20excessively%20complex.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20GLEAMS%2C%20a%20novel%20method%20that%20partitions%20the%20input%20space%20and%0Alearns%20an%20interpretable%20model%20within%20each%20sub-region%2C%20thereby%20providing%20both%0Afaithful%20local%20and%20global%20surrogates.%20We%20demonstrate%20GLEAMS%27%20effectiveness%20on%0Aboth%20synthetic%20and%20real-world%20data%2C%20highlighting%20its%20desirable%20properties%20and%0Ahuman-understandable%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLEAMS%253A%2520Bridging%2520the%2520Gap%2520Between%2520Local%2520and%2520Global%2520Explanations%26entry.906535625%3DGiorgio%2520Visani%2520and%2520Vincenzo%2520Stanzione%2520and%2520Damien%2520Garreau%26entry.1292438233%3D%2520%2520The%2520explainability%2520of%2520machine%2520learning%2520algorithms%2520is%2520crucial%252C%2520and%2520numerous%250Amethods%2520have%2520emerged%2520recently.%2520Local%252C%2520post-hoc%2520methods%2520assign%2520an%2520attribution%250Ascore%2520to%2520each%2520feature%252C%2520indicating%2520its%2520importance%2520for%2520the%2520prediction.%2520However%252C%250Athese%2520methods%2520require%2520recalculating%2520explanations%2520for%2520each%2520example.%2520On%2520the%2520other%250Aside%252C%2520while%2520there%2520exist%2520global%2520approaches%2520they%2520often%2520produce%2520explanations%2520that%250Aare%2520either%2520overly%2520simplistic%2520and%2520unreliable%2520or%2520excessively%2520complex.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520propose%2520GLEAMS%252C%2520a%2520novel%2520method%2520that%2520partitions%2520the%2520input%2520space%2520and%250Alearns%2520an%2520interpretable%2520model%2520within%2520each%2520sub-region%252C%2520thereby%2520providing%2520both%250Afaithful%2520local%2520and%2520global%2520surrogates.%2520We%2520demonstrate%2520GLEAMS%2527%2520effectiveness%2520on%250Aboth%2520synthetic%2520and%2520real-world%2520data%252C%2520highlighting%2520its%2520desirable%2520properties%2520and%250Ahuman-understandable%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLEAMS%3A%20Bridging%20the%20Gap%20Between%20Local%20and%20Global%20Explanations&entry.906535625=Giorgio%20Visani%20and%20Vincenzo%20Stanzione%20and%20Damien%20Garreau&entry.1292438233=%20%20The%20explainability%20of%20machine%20learning%20algorithms%20is%20crucial%2C%20and%20numerous%0Amethods%20have%20emerged%20recently.%20Local%2C%20post-hoc%20methods%20assign%20an%20attribution%0Ascore%20to%20each%20feature%2C%20indicating%20its%20importance%20for%20the%20prediction.%20However%2C%0Athese%20methods%20require%20recalculating%20explanations%20for%20each%20example.%20On%20the%20other%0Aside%2C%20while%20there%20exist%20global%20approaches%20they%20often%20produce%20explanations%20that%0Aare%20either%20overly%20simplistic%20and%20unreliable%20or%20excessively%20complex.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20GLEAMS%2C%20a%20novel%20method%20that%20partitions%20the%20input%20space%20and%0Alearns%20an%20interpretable%20model%20within%20each%20sub-region%2C%20thereby%20providing%20both%0Afaithful%20local%20and%20global%20surrogates.%20We%20demonstrate%20GLEAMS%27%20effectiveness%20on%0Aboth%20synthetic%20and%20real-world%20data%2C%20highlighting%20its%20desirable%20properties%20and%0Ahuman-understandable%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05060v1&entry.124074799=Read"},
{"title": "Enhancing Surface Neural Implicits with Curvature-Guided Sampling and\n  Uncertainty-Augmented Representations", "author": "Lu Sang and Abhishek Saroha and Maolin Gao and Daniel Cremers", "abstract": "  Neural implicit representations have become a popular choice for modeling\nsurfaces due to their adaptability in resolution and support for complex\ntopology. While previous works have achieved impressive reconstruction quality\nby training on ground truth point clouds or meshes, they often do not discuss\nthe data acquisition and ignore the effect of input quality and sampling\nmethods during reconstruction. In this paper, we introduce a method that\ndirectly digests depth images for the task of high-fidelity 3D reconstruction.\nTo this end, a simple sampling strategy is proposed to generate highly\neffective training data, by incorporating differentiable geometric features\ncomputed directly based on the input depth images with only marginal\ncomputational cost. Due to its simplicity, our sampling strategy can be easily\nincorporated into diverse popular methods, allowing their training process to\nbe more stable and efficient. Despite its simplicity, our method outperforms a\nrange of both classical and learning-based baselines and demonstrates\nstate-of-the-art results in both synthetic and real-world datasets.\n", "link": "http://arxiv.org/abs/2306.02099v4", "date": "2024-08-09", "relevancy": 2.2711, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5818}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5578}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Surface%20Neural%20Implicits%20with%20Curvature-Guided%20Sampling%20and%0A%20%20Uncertainty-Augmented%20Representations&body=Title%3A%20Enhancing%20Surface%20Neural%20Implicits%20with%20Curvature-Guided%20Sampling%20and%0A%20%20Uncertainty-Augmented%20Representations%0AAuthor%3A%20Lu%20Sang%20and%20Abhishek%20Saroha%20and%20Maolin%20Gao%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Neural%20implicit%20representations%20have%20become%20a%20popular%20choice%20for%20modeling%0Asurfaces%20due%20to%20their%20adaptability%20in%20resolution%20and%20support%20for%20complex%0Atopology.%20While%20previous%20works%20have%20achieved%20impressive%20reconstruction%20quality%0Aby%20training%20on%20ground%20truth%20point%20clouds%20or%20meshes%2C%20they%20often%20do%20not%20discuss%0Athe%20data%20acquisition%20and%20ignore%20the%20effect%20of%20input%20quality%20and%20sampling%0Amethods%20during%20reconstruction.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20that%0Adirectly%20digests%20depth%20images%20for%20the%20task%20of%20high-fidelity%203D%20reconstruction.%0ATo%20this%20end%2C%20a%20simple%20sampling%20strategy%20is%20proposed%20to%20generate%20highly%0Aeffective%20training%20data%2C%20by%20incorporating%20differentiable%20geometric%20features%0Acomputed%20directly%20based%20on%20the%20input%20depth%20images%20with%20only%20marginal%0Acomputational%20cost.%20Due%20to%20its%20simplicity%2C%20our%20sampling%20strategy%20can%20be%20easily%0Aincorporated%20into%20diverse%20popular%20methods%2C%20allowing%20their%20training%20process%20to%0Abe%20more%20stable%20and%20efficient.%20Despite%20its%20simplicity%2C%20our%20method%20outperforms%20a%0Arange%20of%20both%20classical%20and%20learning-based%20baselines%20and%20demonstrates%0Astate-of-the-art%20results%20in%20both%20synthetic%20and%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02099v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Surface%2520Neural%2520Implicits%2520with%2520Curvature-Guided%2520Sampling%2520and%250A%2520%2520Uncertainty-Augmented%2520Representations%26entry.906535625%3DLu%2520Sang%2520and%2520Abhishek%2520Saroha%2520and%2520Maolin%2520Gao%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representations%2520have%2520become%2520a%2520popular%2520choice%2520for%2520modeling%250Asurfaces%2520due%2520to%2520their%2520adaptability%2520in%2520resolution%2520and%2520support%2520for%2520complex%250Atopology.%2520While%2520previous%2520works%2520have%2520achieved%2520impressive%2520reconstruction%2520quality%250Aby%2520training%2520on%2520ground%2520truth%2520point%2520clouds%2520or%2520meshes%252C%2520they%2520often%2520do%2520not%2520discuss%250Athe%2520data%2520acquisition%2520and%2520ignore%2520the%2520effect%2520of%2520input%2520quality%2520and%2520sampling%250Amethods%2520during%2520reconstruction.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%2520that%250Adirectly%2520digests%2520depth%2520images%2520for%2520the%2520task%2520of%2520high-fidelity%25203D%2520reconstruction.%250ATo%2520this%2520end%252C%2520a%2520simple%2520sampling%2520strategy%2520is%2520proposed%2520to%2520generate%2520highly%250Aeffective%2520training%2520data%252C%2520by%2520incorporating%2520differentiable%2520geometric%2520features%250Acomputed%2520directly%2520based%2520on%2520the%2520input%2520depth%2520images%2520with%2520only%2520marginal%250Acomputational%2520cost.%2520Due%2520to%2520its%2520simplicity%252C%2520our%2520sampling%2520strategy%2520can%2520be%2520easily%250Aincorporated%2520into%2520diverse%2520popular%2520methods%252C%2520allowing%2520their%2520training%2520process%2520to%250Abe%2520more%2520stable%2520and%2520efficient.%2520Despite%2520its%2520simplicity%252C%2520our%2520method%2520outperforms%2520a%250Arange%2520of%2520both%2520classical%2520and%2520learning-based%2520baselines%2520and%2520demonstrates%250Astate-of-the-art%2520results%2520in%2520both%2520synthetic%2520and%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02099v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Surface%20Neural%20Implicits%20with%20Curvature-Guided%20Sampling%20and%0A%20%20Uncertainty-Augmented%20Representations&entry.906535625=Lu%20Sang%20and%20Abhishek%20Saroha%20and%20Maolin%20Gao%20and%20Daniel%20Cremers&entry.1292438233=%20%20Neural%20implicit%20representations%20have%20become%20a%20popular%20choice%20for%20modeling%0Asurfaces%20due%20to%20their%20adaptability%20in%20resolution%20and%20support%20for%20complex%0Atopology.%20While%20previous%20works%20have%20achieved%20impressive%20reconstruction%20quality%0Aby%20training%20on%20ground%20truth%20point%20clouds%20or%20meshes%2C%20they%20often%20do%20not%20discuss%0Athe%20data%20acquisition%20and%20ignore%20the%20effect%20of%20input%20quality%20and%20sampling%0Amethods%20during%20reconstruction.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20that%0Adirectly%20digests%20depth%20images%20for%20the%20task%20of%20high-fidelity%203D%20reconstruction.%0ATo%20this%20end%2C%20a%20simple%20sampling%20strategy%20is%20proposed%20to%20generate%20highly%0Aeffective%20training%20data%2C%20by%20incorporating%20differentiable%20geometric%20features%0Acomputed%20directly%20based%20on%20the%20input%20depth%20images%20with%20only%20marginal%0Acomputational%20cost.%20Due%20to%20its%20simplicity%2C%20our%20sampling%20strategy%20can%20be%20easily%0Aincorporated%20into%20diverse%20popular%20methods%2C%20allowing%20their%20training%20process%20to%0Abe%20more%20stable%20and%20efficient.%20Despite%20its%20simplicity%2C%20our%20method%20outperforms%20a%0Arange%20of%20both%20classical%20and%20learning-based%20baselines%20and%20demonstrates%0Astate-of-the-art%20results%20in%20both%20synthetic%20and%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02099v4&entry.124074799=Read"},
{"title": "Graph Neural Networks as Ordering Heuristics for Parallel Graph Coloring", "author": "Kenneth Langedal and Fredrik Manne", "abstract": "  The graph coloring problem asks for an assignment of the minimum number of\ndistinct colors to vertices in an undirected graph with the constraint that no\npair of adjacent vertices share the same color. The problem is a thoroughly\nstudied NP-hard combinatorial problem with several real-world applications. As\nsuch, a number of greedy heuristics have been suggested that strike a good\nbalance between coloring quality, execution time, and also parallel\nscalability. In this work, we introduce a graph neural network (GNN) based\nordering heuristic and demonstrate that it outperforms existing greedy ordering\nheuristics both on quality and performance. Previous results have demonstrated\nthat GNNs can produce high-quality colorings but at the expense of excessive\nrunning time. The current paper is the first that brings the execution time\ndown to compete with existing greedy heuristics. Our GNN model is trained using\nboth supervised and unsupervised techniques. The experimental results show that\na 2-layer GNN model can achieve execution times between the largest degree\nfirst (LF) and smallest degree last (SL) ordering heuristics while\noutperforming both on coloring quality. Increasing the number of layers\nimproves the coloring quality further, and it is only at four layers that SL\nbecomes faster than the GNN. Finally, our GNN-based coloring heuristic achieves\nsuperior scaling in the parallel setting compared to both SL and LF.\n", "link": "http://arxiv.org/abs/2408.05054v1", "date": "2024-08-09", "relevancy": 2.2589, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4687}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4498}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20as%20Ordering%20Heuristics%20for%20Parallel%20Graph%20Coloring&body=Title%3A%20Graph%20Neural%20Networks%20as%20Ordering%20Heuristics%20for%20Parallel%20Graph%20Coloring%0AAuthor%3A%20Kenneth%20Langedal%20and%20Fredrik%20Manne%0AAbstract%3A%20%20%20The%20graph%20coloring%20problem%20asks%20for%20an%20assignment%20of%20the%20minimum%20number%20of%0Adistinct%20colors%20to%20vertices%20in%20an%20undirected%20graph%20with%20the%20constraint%20that%20no%0Apair%20of%20adjacent%20vertices%20share%20the%20same%20color.%20The%20problem%20is%20a%20thoroughly%0Astudied%20NP-hard%20combinatorial%20problem%20with%20several%20real-world%20applications.%20As%0Asuch%2C%20a%20number%20of%20greedy%20heuristics%20have%20been%20suggested%20that%20strike%20a%20good%0Abalance%20between%20coloring%20quality%2C%20execution%20time%2C%20and%20also%20parallel%0Ascalability.%20In%20this%20work%2C%20we%20introduce%20a%20graph%20neural%20network%20%28GNN%29%20based%0Aordering%20heuristic%20and%20demonstrate%20that%20it%20outperforms%20existing%20greedy%20ordering%0Aheuristics%20both%20on%20quality%20and%20performance.%20Previous%20results%20have%20demonstrated%0Athat%20GNNs%20can%20produce%20high-quality%20colorings%20but%20at%20the%20expense%20of%20excessive%0Arunning%20time.%20The%20current%20paper%20is%20the%20first%20that%20brings%20the%20execution%20time%0Adown%20to%20compete%20with%20existing%20greedy%20heuristics.%20Our%20GNN%20model%20is%20trained%20using%0Aboth%20supervised%20and%20unsupervised%20techniques.%20The%20experimental%20results%20show%20that%0Aa%202-layer%20GNN%20model%20can%20achieve%20execution%20times%20between%20the%20largest%20degree%0Afirst%20%28LF%29%20and%20smallest%20degree%20last%20%28SL%29%20ordering%20heuristics%20while%0Aoutperforming%20both%20on%20coloring%20quality.%20Increasing%20the%20number%20of%20layers%0Aimproves%20the%20coloring%20quality%20further%2C%20and%20it%20is%20only%20at%20four%20layers%20that%20SL%0Abecomes%20faster%20than%20the%20GNN.%20Finally%2C%20our%20GNN-based%20coloring%20heuristic%20achieves%0Asuperior%20scaling%20in%20the%20parallel%20setting%20compared%20to%20both%20SL%20and%20LF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520as%2520Ordering%2520Heuristics%2520for%2520Parallel%2520Graph%2520Coloring%26entry.906535625%3DKenneth%2520Langedal%2520and%2520Fredrik%2520Manne%26entry.1292438233%3D%2520%2520The%2520graph%2520coloring%2520problem%2520asks%2520for%2520an%2520assignment%2520of%2520the%2520minimum%2520number%2520of%250Adistinct%2520colors%2520to%2520vertices%2520in%2520an%2520undirected%2520graph%2520with%2520the%2520constraint%2520that%2520no%250Apair%2520of%2520adjacent%2520vertices%2520share%2520the%2520same%2520color.%2520The%2520problem%2520is%2520a%2520thoroughly%250Astudied%2520NP-hard%2520combinatorial%2520problem%2520with%2520several%2520real-world%2520applications.%2520As%250Asuch%252C%2520a%2520number%2520of%2520greedy%2520heuristics%2520have%2520been%2520suggested%2520that%2520strike%2520a%2520good%250Abalance%2520between%2520coloring%2520quality%252C%2520execution%2520time%252C%2520and%2520also%2520parallel%250Ascalability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520graph%2520neural%2520network%2520%2528GNN%2529%2520based%250Aordering%2520heuristic%2520and%2520demonstrate%2520that%2520it%2520outperforms%2520existing%2520greedy%2520ordering%250Aheuristics%2520both%2520on%2520quality%2520and%2520performance.%2520Previous%2520results%2520have%2520demonstrated%250Athat%2520GNNs%2520can%2520produce%2520high-quality%2520colorings%2520but%2520at%2520the%2520expense%2520of%2520excessive%250Arunning%2520time.%2520The%2520current%2520paper%2520is%2520the%2520first%2520that%2520brings%2520the%2520execution%2520time%250Adown%2520to%2520compete%2520with%2520existing%2520greedy%2520heuristics.%2520Our%2520GNN%2520model%2520is%2520trained%2520using%250Aboth%2520supervised%2520and%2520unsupervised%2520techniques.%2520The%2520experimental%2520results%2520show%2520that%250Aa%25202-layer%2520GNN%2520model%2520can%2520achieve%2520execution%2520times%2520between%2520the%2520largest%2520degree%250Afirst%2520%2528LF%2529%2520and%2520smallest%2520degree%2520last%2520%2528SL%2529%2520ordering%2520heuristics%2520while%250Aoutperforming%2520both%2520on%2520coloring%2520quality.%2520Increasing%2520the%2520number%2520of%2520layers%250Aimproves%2520the%2520coloring%2520quality%2520further%252C%2520and%2520it%2520is%2520only%2520at%2520four%2520layers%2520that%2520SL%250Abecomes%2520faster%2520than%2520the%2520GNN.%2520Finally%252C%2520our%2520GNN-based%2520coloring%2520heuristic%2520achieves%250Asuperior%2520scaling%2520in%2520the%2520parallel%2520setting%2520compared%2520to%2520both%2520SL%2520and%2520LF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20as%20Ordering%20Heuristics%20for%20Parallel%20Graph%20Coloring&entry.906535625=Kenneth%20Langedal%20and%20Fredrik%20Manne&entry.1292438233=%20%20The%20graph%20coloring%20problem%20asks%20for%20an%20assignment%20of%20the%20minimum%20number%20of%0Adistinct%20colors%20to%20vertices%20in%20an%20undirected%20graph%20with%20the%20constraint%20that%20no%0Apair%20of%20adjacent%20vertices%20share%20the%20same%20color.%20The%20problem%20is%20a%20thoroughly%0Astudied%20NP-hard%20combinatorial%20problem%20with%20several%20real-world%20applications.%20As%0Asuch%2C%20a%20number%20of%20greedy%20heuristics%20have%20been%20suggested%20that%20strike%20a%20good%0Abalance%20between%20coloring%20quality%2C%20execution%20time%2C%20and%20also%20parallel%0Ascalability.%20In%20this%20work%2C%20we%20introduce%20a%20graph%20neural%20network%20%28GNN%29%20based%0Aordering%20heuristic%20and%20demonstrate%20that%20it%20outperforms%20existing%20greedy%20ordering%0Aheuristics%20both%20on%20quality%20and%20performance.%20Previous%20results%20have%20demonstrated%0Athat%20GNNs%20can%20produce%20high-quality%20colorings%20but%20at%20the%20expense%20of%20excessive%0Arunning%20time.%20The%20current%20paper%20is%20the%20first%20that%20brings%20the%20execution%20time%0Adown%20to%20compete%20with%20existing%20greedy%20heuristics.%20Our%20GNN%20model%20is%20trained%20using%0Aboth%20supervised%20and%20unsupervised%20techniques.%20The%20experimental%20results%20show%20that%0Aa%202-layer%20GNN%20model%20can%20achieve%20execution%20times%20between%20the%20largest%20degree%0Afirst%20%28LF%29%20and%20smallest%20degree%20last%20%28SL%29%20ordering%20heuristics%20while%0Aoutperforming%20both%20on%20coloring%20quality.%20Increasing%20the%20number%20of%20layers%0Aimproves%20the%20coloring%20quality%20further%2C%20and%20it%20is%20only%20at%20four%20layers%20that%20SL%0Abecomes%20faster%20than%20the%20GNN.%20Finally%2C%20our%20GNN-based%20coloring%20heuristic%20achieves%0Asuperior%20scaling%20in%20the%20parallel%20setting%20compared%20to%20both%20SL%20and%20LF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05054v1&entry.124074799=Read"},
{"title": "Efficient automatic segmentation for multi-level pulmonary arteries: The\n  PARSE challenge", "author": "Gongning Luo and Kuanquan Wang and Jun Liu and Shuo Li and Xinjie Liang and Xiangyu Li and Shaowei Gan and Wei Wang and Suyu Dong and Wenyi Wang and Pengxin Yu and Enyou Liu and Hongrong Wei and Na Wang and Jia Guo and Huiqi Li and Zhao Zhang and Ziwei Zhao and Na Gao and Nan An and Ashkan Pakzad and Bojidar Rangelov and Jiaqi Dou and Song Tian and Zeyu Liu and Yi Wang and Ampatishan Sivalingam and Kumaradevan Punithakumar and Zhaowen Qiu and Xin Gao", "abstract": "  Efficient automatic segmentation of multi-level (i.e. main and branch)\npulmonary arteries (PA) in CTPA images plays a significant role in clinical\napplications. However, most existing methods concentrate only on main PA or\nbranch PA segmentation separately and ignore segmentation efficiency. Besides,\nthere is no public large-scale dataset focused on PA segmentation, which makes\nit highly challenging to compare the different methods. To benchmark\nmulti-level PA segmentation algorithms, we organized the first\n\\textbf{P}ulmonary \\textbf{AR}tery \\textbf{SE}gmentation (PARSE) challenge. On\nthe one hand, we focus on both the main PA and the branch PA segmentation. On\nthe other hand, for better clinical application, we assign the same score\nweight to segmentation efficiency (mainly running time and GPU memory\nconsumption during inference) while ensuring PA segmentation accuracy. We\npresent a summary of the top algorithms and offer some suggestions for\nefficient and accurate multi-level PA automatic segmentation. We provide the\nPARSE challenge as open-access for the community to benchmark future algorithm\ndevelopments at \\url{https://parse2022.grand-challenge.org/Parse2022/}.\n", "link": "http://arxiv.org/abs/2304.03708v2", "date": "2024-08-09", "relevancy": 2.254, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4578}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20automatic%20segmentation%20for%20multi-level%20pulmonary%20arteries%3A%20The%0A%20%20PARSE%20challenge&body=Title%3A%20Efficient%20automatic%20segmentation%20for%20multi-level%20pulmonary%20arteries%3A%20The%0A%20%20PARSE%20challenge%0AAuthor%3A%20Gongning%20Luo%20and%20Kuanquan%20Wang%20and%20Jun%20Liu%20and%20Shuo%20Li%20and%20Xinjie%20Liang%20and%20Xiangyu%20Li%20and%20Shaowei%20Gan%20and%20Wei%20Wang%20and%20Suyu%20Dong%20and%20Wenyi%20Wang%20and%20Pengxin%20Yu%20and%20Enyou%20Liu%20and%20Hongrong%20Wei%20and%20Na%20Wang%20and%20Jia%20Guo%20and%20Huiqi%20Li%20and%20Zhao%20Zhang%20and%20Ziwei%20Zhao%20and%20Na%20Gao%20and%20Nan%20An%20and%20Ashkan%20Pakzad%20and%20Bojidar%20Rangelov%20and%20Jiaqi%20Dou%20and%20Song%20Tian%20and%20Zeyu%20Liu%20and%20Yi%20Wang%20and%20Ampatishan%20Sivalingam%20and%20Kumaradevan%20Punithakumar%20and%20Zhaowen%20Qiu%20and%20Xin%20Gao%0AAbstract%3A%20%20%20Efficient%20automatic%20segmentation%20of%20multi-level%20%28i.e.%20main%20and%20branch%29%0Apulmonary%20arteries%20%28PA%29%20in%20CTPA%20images%20plays%20a%20significant%20role%20in%20clinical%0Aapplications.%20However%2C%20most%20existing%20methods%20concentrate%20only%20on%20main%20PA%20or%0Abranch%20PA%20segmentation%20separately%20and%20ignore%20segmentation%20efficiency.%20Besides%2C%0Athere%20is%20no%20public%20large-scale%20dataset%20focused%20on%20PA%20segmentation%2C%20which%20makes%0Ait%20highly%20challenging%20to%20compare%20the%20different%20methods.%20To%20benchmark%0Amulti-level%20PA%20segmentation%20algorithms%2C%20we%20organized%20the%20first%0A%5Ctextbf%7BP%7Dulmonary%20%5Ctextbf%7BAR%7Dtery%20%5Ctextbf%7BSE%7Dgmentation%20%28PARSE%29%20challenge.%20On%0Athe%20one%20hand%2C%20we%20focus%20on%20both%20the%20main%20PA%20and%20the%20branch%20PA%20segmentation.%20On%0Athe%20other%20hand%2C%20for%20better%20clinical%20application%2C%20we%20assign%20the%20same%20score%0Aweight%20to%20segmentation%20efficiency%20%28mainly%20running%20time%20and%20GPU%20memory%0Aconsumption%20during%20inference%29%20while%20ensuring%20PA%20segmentation%20accuracy.%20We%0Apresent%20a%20summary%20of%20the%20top%20algorithms%20and%20offer%20some%20suggestions%20for%0Aefficient%20and%20accurate%20multi-level%20PA%20automatic%20segmentation.%20We%20provide%20the%0APARSE%20challenge%20as%20open-access%20for%20the%20community%20to%20benchmark%20future%20algorithm%0Adevelopments%20at%20%5Curl%7Bhttps%3A//parse2022.grand-challenge.org/Parse2022/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520automatic%2520segmentation%2520for%2520multi-level%2520pulmonary%2520arteries%253A%2520The%250A%2520%2520PARSE%2520challenge%26entry.906535625%3DGongning%2520Luo%2520and%2520Kuanquan%2520Wang%2520and%2520Jun%2520Liu%2520and%2520Shuo%2520Li%2520and%2520Xinjie%2520Liang%2520and%2520Xiangyu%2520Li%2520and%2520Shaowei%2520Gan%2520and%2520Wei%2520Wang%2520and%2520Suyu%2520Dong%2520and%2520Wenyi%2520Wang%2520and%2520Pengxin%2520Yu%2520and%2520Enyou%2520Liu%2520and%2520Hongrong%2520Wei%2520and%2520Na%2520Wang%2520and%2520Jia%2520Guo%2520and%2520Huiqi%2520Li%2520and%2520Zhao%2520Zhang%2520and%2520Ziwei%2520Zhao%2520and%2520Na%2520Gao%2520and%2520Nan%2520An%2520and%2520Ashkan%2520Pakzad%2520and%2520Bojidar%2520Rangelov%2520and%2520Jiaqi%2520Dou%2520and%2520Song%2520Tian%2520and%2520Zeyu%2520Liu%2520and%2520Yi%2520Wang%2520and%2520Ampatishan%2520Sivalingam%2520and%2520Kumaradevan%2520Punithakumar%2520and%2520Zhaowen%2520Qiu%2520and%2520Xin%2520Gao%26entry.1292438233%3D%2520%2520Efficient%2520automatic%2520segmentation%2520of%2520multi-level%2520%2528i.e.%2520main%2520and%2520branch%2529%250Apulmonary%2520arteries%2520%2528PA%2529%2520in%2520CTPA%2520images%2520plays%2520a%2520significant%2520role%2520in%2520clinical%250Aapplications.%2520However%252C%2520most%2520existing%2520methods%2520concentrate%2520only%2520on%2520main%2520PA%2520or%250Abranch%2520PA%2520segmentation%2520separately%2520and%2520ignore%2520segmentation%2520efficiency.%2520Besides%252C%250Athere%2520is%2520no%2520public%2520large-scale%2520dataset%2520focused%2520on%2520PA%2520segmentation%252C%2520which%2520makes%250Ait%2520highly%2520challenging%2520to%2520compare%2520the%2520different%2520methods.%2520To%2520benchmark%250Amulti-level%2520PA%2520segmentation%2520algorithms%252C%2520we%2520organized%2520the%2520first%250A%255Ctextbf%257BP%257Dulmonary%2520%255Ctextbf%257BAR%257Dtery%2520%255Ctextbf%257BSE%257Dgmentation%2520%2528PARSE%2529%2520challenge.%2520On%250Athe%2520one%2520hand%252C%2520we%2520focus%2520on%2520both%2520the%2520main%2520PA%2520and%2520the%2520branch%2520PA%2520segmentation.%2520On%250Athe%2520other%2520hand%252C%2520for%2520better%2520clinical%2520application%252C%2520we%2520assign%2520the%2520same%2520score%250Aweight%2520to%2520segmentation%2520efficiency%2520%2528mainly%2520running%2520time%2520and%2520GPU%2520memory%250Aconsumption%2520during%2520inference%2529%2520while%2520ensuring%2520PA%2520segmentation%2520accuracy.%2520We%250Apresent%2520a%2520summary%2520of%2520the%2520top%2520algorithms%2520and%2520offer%2520some%2520suggestions%2520for%250Aefficient%2520and%2520accurate%2520multi-level%2520PA%2520automatic%2520segmentation.%2520We%2520provide%2520the%250APARSE%2520challenge%2520as%2520open-access%2520for%2520the%2520community%2520to%2520benchmark%2520future%2520algorithm%250Adevelopments%2520at%2520%255Curl%257Bhttps%253A//parse2022.grand-challenge.org/Parse2022/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.03708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20automatic%20segmentation%20for%20multi-level%20pulmonary%20arteries%3A%20The%0A%20%20PARSE%20challenge&entry.906535625=Gongning%20Luo%20and%20Kuanquan%20Wang%20and%20Jun%20Liu%20and%20Shuo%20Li%20and%20Xinjie%20Liang%20and%20Xiangyu%20Li%20and%20Shaowei%20Gan%20and%20Wei%20Wang%20and%20Suyu%20Dong%20and%20Wenyi%20Wang%20and%20Pengxin%20Yu%20and%20Enyou%20Liu%20and%20Hongrong%20Wei%20and%20Na%20Wang%20and%20Jia%20Guo%20and%20Huiqi%20Li%20and%20Zhao%20Zhang%20and%20Ziwei%20Zhao%20and%20Na%20Gao%20and%20Nan%20An%20and%20Ashkan%20Pakzad%20and%20Bojidar%20Rangelov%20and%20Jiaqi%20Dou%20and%20Song%20Tian%20and%20Zeyu%20Liu%20and%20Yi%20Wang%20and%20Ampatishan%20Sivalingam%20and%20Kumaradevan%20Punithakumar%20and%20Zhaowen%20Qiu%20and%20Xin%20Gao&entry.1292438233=%20%20Efficient%20automatic%20segmentation%20of%20multi-level%20%28i.e.%20main%20and%20branch%29%0Apulmonary%20arteries%20%28PA%29%20in%20CTPA%20images%20plays%20a%20significant%20role%20in%20clinical%0Aapplications.%20However%2C%20most%20existing%20methods%20concentrate%20only%20on%20main%20PA%20or%0Abranch%20PA%20segmentation%20separately%20and%20ignore%20segmentation%20efficiency.%20Besides%2C%0Athere%20is%20no%20public%20large-scale%20dataset%20focused%20on%20PA%20segmentation%2C%20which%20makes%0Ait%20highly%20challenging%20to%20compare%20the%20different%20methods.%20To%20benchmark%0Amulti-level%20PA%20segmentation%20algorithms%2C%20we%20organized%20the%20first%0A%5Ctextbf%7BP%7Dulmonary%20%5Ctextbf%7BAR%7Dtery%20%5Ctextbf%7BSE%7Dgmentation%20%28PARSE%29%20challenge.%20On%0Athe%20one%20hand%2C%20we%20focus%20on%20both%20the%20main%20PA%20and%20the%20branch%20PA%20segmentation.%20On%0Athe%20other%20hand%2C%20for%20better%20clinical%20application%2C%20we%20assign%20the%20same%20score%0Aweight%20to%20segmentation%20efficiency%20%28mainly%20running%20time%20and%20GPU%20memory%0Aconsumption%20during%20inference%29%20while%20ensuring%20PA%20segmentation%20accuracy.%20We%0Apresent%20a%20summary%20of%20the%20top%20algorithms%20and%20offer%20some%20suggestions%20for%0Aefficient%20and%20accurate%20multi-level%20PA%20automatic%20segmentation.%20We%20provide%20the%0APARSE%20challenge%20as%20open-access%20for%20the%20community%20to%20benchmark%20future%20algorithm%0Adevelopments%20at%20%5Curl%7Bhttps%3A//parse2022.grand-challenge.org/Parse2022/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03708v2&entry.124074799=Read"},
{"title": "CREMP: Conformer-rotamer ensembles of macrocyclic peptides for machine\n  learning", "author": "Colin A. Grambow and Hayley Weir and Christian N. Cunningham and Tommaso Biancalani and Kangway V. Chuang", "abstract": "  Computational and machine learning approaches to model the conformational\nlandscape of macrocyclic peptides have the potential to enable rational design\nand optimization. However, accurate, fast, and scalable methods for modeling\nmacrocycle geometries remain elusive. Recent deep learning approaches have\nsignificantly accelerated protein structure prediction and the generation of\nsmall-molecule conformational ensembles, yet similar progress has not been made\nfor macrocyclic peptides due to their unique properties. Here, we introduce\nCREMP, a resource generated for the rapid development and evaluation of machine\nlearning models for macrocyclic peptides. CREMP contains 36,198 unique\nmacrocyclic peptides and their high-quality structural ensembles generated\nusing the Conformer-Rotamer Ensemble Sampling Tool (CREST). Altogether, this\nnew dataset contains nearly 31.3 million unique macrocycle geometries, each\nannotated with energies derived from semi-empirical extended tight-binding\n(xTB) DFT calculations. Additionally, we include 3,258 macrocycles with\nreported passive permeability data to couple conformational ensembles to\nexperiment. We anticipate that this dataset will enable the development of\nmachine learning models that can improve peptide design and optimization for\nnovel therapeutics.\n", "link": "http://arxiv.org/abs/2305.08057v2", "date": "2024-08-09", "relevancy": 2.2521, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4754}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4379}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CREMP%3A%20Conformer-rotamer%20ensembles%20of%20macrocyclic%20peptides%20for%20machine%0A%20%20learning&body=Title%3A%20CREMP%3A%20Conformer-rotamer%20ensembles%20of%20macrocyclic%20peptides%20for%20machine%0A%20%20learning%0AAuthor%3A%20Colin%20A.%20Grambow%20and%20Hayley%20Weir%20and%20Christian%20N.%20Cunningham%20and%20Tommaso%20Biancalani%20and%20Kangway%20V.%20Chuang%0AAbstract%3A%20%20%20Computational%20and%20machine%20learning%20approaches%20to%20model%20the%20conformational%0Alandscape%20of%20macrocyclic%20peptides%20have%20the%20potential%20to%20enable%20rational%20design%0Aand%20optimization.%20However%2C%20accurate%2C%20fast%2C%20and%20scalable%20methods%20for%20modeling%0Amacrocycle%20geometries%20remain%20elusive.%20Recent%20deep%20learning%20approaches%20have%0Asignificantly%20accelerated%20protein%20structure%20prediction%20and%20the%20generation%20of%0Asmall-molecule%20conformational%20ensembles%2C%20yet%20similar%20progress%20has%20not%20been%20made%0Afor%20macrocyclic%20peptides%20due%20to%20their%20unique%20properties.%20Here%2C%20we%20introduce%0ACREMP%2C%20a%20resource%20generated%20for%20the%20rapid%20development%20and%20evaluation%20of%20machine%0Alearning%20models%20for%20macrocyclic%20peptides.%20CREMP%20contains%2036%2C198%20unique%0Amacrocyclic%20peptides%20and%20their%20high-quality%20structural%20ensembles%20generated%0Ausing%20the%20Conformer-Rotamer%20Ensemble%20Sampling%20Tool%20%28CREST%29.%20Altogether%2C%20this%0Anew%20dataset%20contains%20nearly%2031.3%20million%20unique%20macrocycle%20geometries%2C%20each%0Aannotated%20with%20energies%20derived%20from%20semi-empirical%20extended%20tight-binding%0A%28xTB%29%20DFT%20calculations.%20Additionally%2C%20we%20include%203%2C258%20macrocycles%20with%0Areported%20passive%20permeability%20data%20to%20couple%20conformational%20ensembles%20to%0Aexperiment.%20We%20anticipate%20that%20this%20dataset%20will%20enable%20the%20development%20of%0Amachine%20learning%20models%20that%20can%20improve%20peptide%20design%20and%20optimization%20for%0Anovel%20therapeutics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.08057v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCREMP%253A%2520Conformer-rotamer%2520ensembles%2520of%2520macrocyclic%2520peptides%2520for%2520machine%250A%2520%2520learning%26entry.906535625%3DColin%2520A.%2520Grambow%2520and%2520Hayley%2520Weir%2520and%2520Christian%2520N.%2520Cunningham%2520and%2520Tommaso%2520Biancalani%2520and%2520Kangway%2520V.%2520Chuang%26entry.1292438233%3D%2520%2520Computational%2520and%2520machine%2520learning%2520approaches%2520to%2520model%2520the%2520conformational%250Alandscape%2520of%2520macrocyclic%2520peptides%2520have%2520the%2520potential%2520to%2520enable%2520rational%2520design%250Aand%2520optimization.%2520However%252C%2520accurate%252C%2520fast%252C%2520and%2520scalable%2520methods%2520for%2520modeling%250Amacrocycle%2520geometries%2520remain%2520elusive.%2520Recent%2520deep%2520learning%2520approaches%2520have%250Asignificantly%2520accelerated%2520protein%2520structure%2520prediction%2520and%2520the%2520generation%2520of%250Asmall-molecule%2520conformational%2520ensembles%252C%2520yet%2520similar%2520progress%2520has%2520not%2520been%2520made%250Afor%2520macrocyclic%2520peptides%2520due%2520to%2520their%2520unique%2520properties.%2520Here%252C%2520we%2520introduce%250ACREMP%252C%2520a%2520resource%2520generated%2520for%2520the%2520rapid%2520development%2520and%2520evaluation%2520of%2520machine%250Alearning%2520models%2520for%2520macrocyclic%2520peptides.%2520CREMP%2520contains%252036%252C198%2520unique%250Amacrocyclic%2520peptides%2520and%2520their%2520high-quality%2520structural%2520ensembles%2520generated%250Ausing%2520the%2520Conformer-Rotamer%2520Ensemble%2520Sampling%2520Tool%2520%2528CREST%2529.%2520Altogether%252C%2520this%250Anew%2520dataset%2520contains%2520nearly%252031.3%2520million%2520unique%2520macrocycle%2520geometries%252C%2520each%250Aannotated%2520with%2520energies%2520derived%2520from%2520semi-empirical%2520extended%2520tight-binding%250A%2528xTB%2529%2520DFT%2520calculations.%2520Additionally%252C%2520we%2520include%25203%252C258%2520macrocycles%2520with%250Areported%2520passive%2520permeability%2520data%2520to%2520couple%2520conformational%2520ensembles%2520to%250Aexperiment.%2520We%2520anticipate%2520that%2520this%2520dataset%2520will%2520enable%2520the%2520development%2520of%250Amachine%2520learning%2520models%2520that%2520can%2520improve%2520peptide%2520design%2520and%2520optimization%2520for%250Anovel%2520therapeutics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.08057v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CREMP%3A%20Conformer-rotamer%20ensembles%20of%20macrocyclic%20peptides%20for%20machine%0A%20%20learning&entry.906535625=Colin%20A.%20Grambow%20and%20Hayley%20Weir%20and%20Christian%20N.%20Cunningham%20and%20Tommaso%20Biancalani%20and%20Kangway%20V.%20Chuang&entry.1292438233=%20%20Computational%20and%20machine%20learning%20approaches%20to%20model%20the%20conformational%0Alandscape%20of%20macrocyclic%20peptides%20have%20the%20potential%20to%20enable%20rational%20design%0Aand%20optimization.%20However%2C%20accurate%2C%20fast%2C%20and%20scalable%20methods%20for%20modeling%0Amacrocycle%20geometries%20remain%20elusive.%20Recent%20deep%20learning%20approaches%20have%0Asignificantly%20accelerated%20protein%20structure%20prediction%20and%20the%20generation%20of%0Asmall-molecule%20conformational%20ensembles%2C%20yet%20similar%20progress%20has%20not%20been%20made%0Afor%20macrocyclic%20peptides%20due%20to%20their%20unique%20properties.%20Here%2C%20we%20introduce%0ACREMP%2C%20a%20resource%20generated%20for%20the%20rapid%20development%20and%20evaluation%20of%20machine%0Alearning%20models%20for%20macrocyclic%20peptides.%20CREMP%20contains%2036%2C198%20unique%0Amacrocyclic%20peptides%20and%20their%20high-quality%20structural%20ensembles%20generated%0Ausing%20the%20Conformer-Rotamer%20Ensemble%20Sampling%20Tool%20%28CREST%29.%20Altogether%2C%20this%0Anew%20dataset%20contains%20nearly%2031.3%20million%20unique%20macrocycle%20geometries%2C%20each%0Aannotated%20with%20energies%20derived%20from%20semi-empirical%20extended%20tight-binding%0A%28xTB%29%20DFT%20calculations.%20Additionally%2C%20we%20include%203%2C258%20macrocycles%20with%0Areported%20passive%20permeability%20data%20to%20couple%20conformational%20ensembles%20to%0Aexperiment.%20We%20anticipate%20that%20this%20dataset%20will%20enable%20the%20development%20of%0Amachine%20learning%20models%20that%20can%20improve%20peptide%20design%20and%20optimization%20for%0Anovel%20therapeutics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.08057v2&entry.124074799=Read"},
{"title": "Enhancing Person Re-Identification via Uncertainty Feature Fusion and\n  Auto-weighted Measure Combination", "author": "Quang-Huy Che and Le-Chuong Nguyen and Vinh-Tiep Nguyen", "abstract": "  The quest for robust Person re-identification (Re-ID) systems capable of\naccurately identifying subjects across diverse scenarios remains a formidable\nchallenge in surveillance and security applications. This study presents a\nnovel methodology that significantly enhances Person Re-Identification (Re-ID)\nby integrating Uncertainty Feature Fusion (UFFM) with Wise Distance Aggregation\n(WDA). Tested on benchmark datasets - Market-1501, DukeMTMC-ReID, and MSMT17 -\nour approach demonstrates substantial improvements in Rank-1 accuracy and mean\nAverage Precision (mAP). Specifically, UFFM capitalizes on the power of feature\nsynthesis from multiple images to overcome the limitations imposed by the\nvariability of subject appearances across different views. WDA further refines\nthe process by intelligently aggregating similarity metrics, thereby enhancing\nthe system's ability to discern subtle but critical differences between\nsubjects. The empirical results affirm the superiority of our method over\nexisting approaches, achieving new performance benchmarks across all evaluated\ndatasets.\n", "link": "http://arxiv.org/abs/2405.01101v2", "date": "2024-08-09", "relevancy": 2.2355, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5707}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Person%20Re-Identification%20via%20Uncertainty%20Feature%20Fusion%20and%0A%20%20Auto-weighted%20Measure%20Combination&body=Title%3A%20Enhancing%20Person%20Re-Identification%20via%20Uncertainty%20Feature%20Fusion%20and%0A%20%20Auto-weighted%20Measure%20Combination%0AAuthor%3A%20Quang-Huy%20Che%20and%20Le-Chuong%20Nguyen%20and%20Vinh-Tiep%20Nguyen%0AAbstract%3A%20%20%20The%20quest%20for%20robust%20Person%20re-identification%20%28Re-ID%29%20systems%20capable%20of%0Aaccurately%20identifying%20subjects%20across%20diverse%20scenarios%20remains%20a%20formidable%0Achallenge%20in%20surveillance%20and%20security%20applications.%20This%20study%20presents%20a%0Anovel%20methodology%20that%20significantly%20enhances%20Person%20Re-Identification%20%28Re-ID%29%0Aby%20integrating%20Uncertainty%20Feature%20Fusion%20%28UFFM%29%20with%20Wise%20Distance%20Aggregation%0A%28WDA%29.%20Tested%20on%20benchmark%20datasets%20-%20Market-1501%2C%20DukeMTMC-ReID%2C%20and%20MSMT17%20-%0Aour%20approach%20demonstrates%20substantial%20improvements%20in%20Rank-1%20accuracy%20and%20mean%0AAverage%20Precision%20%28mAP%29.%20Specifically%2C%20UFFM%20capitalizes%20on%20the%20power%20of%20feature%0Asynthesis%20from%20multiple%20images%20to%20overcome%20the%20limitations%20imposed%20by%20the%0Avariability%20of%20subject%20appearances%20across%20different%20views.%20WDA%20further%20refines%0Athe%20process%20by%20intelligently%20aggregating%20similarity%20metrics%2C%20thereby%20enhancing%0Athe%20system%27s%20ability%20to%20discern%20subtle%20but%20critical%20differences%20between%0Asubjects.%20The%20empirical%20results%20affirm%20the%20superiority%20of%20our%20method%20over%0Aexisting%20approaches%2C%20achieving%20new%20performance%20benchmarks%20across%20all%20evaluated%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Person%2520Re-Identification%2520via%2520Uncertainty%2520Feature%2520Fusion%2520and%250A%2520%2520Auto-weighted%2520Measure%2520Combination%26entry.906535625%3DQuang-Huy%2520Che%2520and%2520Le-Chuong%2520Nguyen%2520and%2520Vinh-Tiep%2520Nguyen%26entry.1292438233%3D%2520%2520The%2520quest%2520for%2520robust%2520Person%2520re-identification%2520%2528Re-ID%2529%2520systems%2520capable%2520of%250Aaccurately%2520identifying%2520subjects%2520across%2520diverse%2520scenarios%2520remains%2520a%2520formidable%250Achallenge%2520in%2520surveillance%2520and%2520security%2520applications.%2520This%2520study%2520presents%2520a%250Anovel%2520methodology%2520that%2520significantly%2520enhances%2520Person%2520Re-Identification%2520%2528Re-ID%2529%250Aby%2520integrating%2520Uncertainty%2520Feature%2520Fusion%2520%2528UFFM%2529%2520with%2520Wise%2520Distance%2520Aggregation%250A%2528WDA%2529.%2520Tested%2520on%2520benchmark%2520datasets%2520-%2520Market-1501%252C%2520DukeMTMC-ReID%252C%2520and%2520MSMT17%2520-%250Aour%2520approach%2520demonstrates%2520substantial%2520improvements%2520in%2520Rank-1%2520accuracy%2520and%2520mean%250AAverage%2520Precision%2520%2528mAP%2529.%2520Specifically%252C%2520UFFM%2520capitalizes%2520on%2520the%2520power%2520of%2520feature%250Asynthesis%2520from%2520multiple%2520images%2520to%2520overcome%2520the%2520limitations%2520imposed%2520by%2520the%250Avariability%2520of%2520subject%2520appearances%2520across%2520different%2520views.%2520WDA%2520further%2520refines%250Athe%2520process%2520by%2520intelligently%2520aggregating%2520similarity%2520metrics%252C%2520thereby%2520enhancing%250Athe%2520system%2527s%2520ability%2520to%2520discern%2520subtle%2520but%2520critical%2520differences%2520between%250Asubjects.%2520The%2520empirical%2520results%2520affirm%2520the%2520superiority%2520of%2520our%2520method%2520over%250Aexisting%2520approaches%252C%2520achieving%2520new%2520performance%2520benchmarks%2520across%2520all%2520evaluated%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Person%20Re-Identification%20via%20Uncertainty%20Feature%20Fusion%20and%0A%20%20Auto-weighted%20Measure%20Combination&entry.906535625=Quang-Huy%20Che%20and%20Le-Chuong%20Nguyen%20and%20Vinh-Tiep%20Nguyen&entry.1292438233=%20%20The%20quest%20for%20robust%20Person%20re-identification%20%28Re-ID%29%20systems%20capable%20of%0Aaccurately%20identifying%20subjects%20across%20diverse%20scenarios%20remains%20a%20formidable%0Achallenge%20in%20surveillance%20and%20security%20applications.%20This%20study%20presents%20a%0Anovel%20methodology%20that%20significantly%20enhances%20Person%20Re-Identification%20%28Re-ID%29%0Aby%20integrating%20Uncertainty%20Feature%20Fusion%20%28UFFM%29%20with%20Wise%20Distance%20Aggregation%0A%28WDA%29.%20Tested%20on%20benchmark%20datasets%20-%20Market-1501%2C%20DukeMTMC-ReID%2C%20and%20MSMT17%20-%0Aour%20approach%20demonstrates%20substantial%20improvements%20in%20Rank-1%20accuracy%20and%20mean%0AAverage%20Precision%20%28mAP%29.%20Specifically%2C%20UFFM%20capitalizes%20on%20the%20power%20of%20feature%0Asynthesis%20from%20multiple%20images%20to%20overcome%20the%20limitations%20imposed%20by%20the%0Avariability%20of%20subject%20appearances%20across%20different%20views.%20WDA%20further%20refines%0Athe%20process%20by%20intelligently%20aggregating%20similarity%20metrics%2C%20thereby%20enhancing%0Athe%20system%27s%20ability%20to%20discern%20subtle%20but%20critical%20differences%20between%0Asubjects.%20The%20empirical%20results%20affirm%20the%20superiority%20of%20our%20method%20over%0Aexisting%20approaches%2C%20achieving%20new%20performance%20benchmarks%20across%20all%20evaluated%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01101v2&entry.124074799=Read"},
{"title": "Hyperbolic Learning with Multimodal Large Language Models", "author": "Paolo Mandica and Luca Franco and Konstantinos Kallidromitis and Suzanne Petryk and Fabio Galasso", "abstract": "  Hyperbolic embeddings have demonstrated their effectiveness in capturing\nmeasures of uncertainty and hierarchical relationships across various\ndeep-learning tasks, including image segmentation and active learning. However,\ntheir application in modern vision-language models (VLMs) has been limited. A\nnotable exception is MERU, which leverages the hierarchical properties of\nhyperbolic space in the CLIP ViT-large model, consisting of hundreds of\nmillions parameters. In our work, we address the challenges of scaling\nmulti-modal hyperbolic models by orders of magnitude in terms of parameters\n(billions) and training complexity using the BLIP-2 architecture. Although\nhyperbolic embeddings offer potential insights into uncertainty not present in\nEuclidean embeddings, our analysis reveals that scaling these models is\nparticularly difficult. We propose a novel training strategy for a hyperbolic\nversion of BLIP-2, which allows to achieve comparable performance to its\nEuclidean counterpart, while maintaining stability throughout the training\nprocess and showing a meaningful indication of uncertainty with each embedding.\n", "link": "http://arxiv.org/abs/2408.05097v1", "date": "2024-08-09", "relevancy": 2.2078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5829}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5494}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperbolic%20Learning%20with%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Hyperbolic%20Learning%20with%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Paolo%20Mandica%20and%20Luca%20Franco%20and%20Konstantinos%20Kallidromitis%20and%20Suzanne%20Petryk%20and%20Fabio%20Galasso%0AAbstract%3A%20%20%20Hyperbolic%20embeddings%20have%20demonstrated%20their%20effectiveness%20in%20capturing%0Ameasures%20of%20uncertainty%20and%20hierarchical%20relationships%20across%20various%0Adeep-learning%20tasks%2C%20including%20image%20segmentation%20and%20active%20learning.%20However%2C%0Atheir%20application%20in%20modern%20vision-language%20models%20%28VLMs%29%20has%20been%20limited.%20A%0Anotable%20exception%20is%20MERU%2C%20which%20leverages%20the%20hierarchical%20properties%20of%0Ahyperbolic%20space%20in%20the%20CLIP%20ViT-large%20model%2C%20consisting%20of%20hundreds%20of%0Amillions%20parameters.%20In%20our%20work%2C%20we%20address%20the%20challenges%20of%20scaling%0Amulti-modal%20hyperbolic%20models%20by%20orders%20of%20magnitude%20in%20terms%20of%20parameters%0A%28billions%29%20and%20training%20complexity%20using%20the%20BLIP-2%20architecture.%20Although%0Ahyperbolic%20embeddings%20offer%20potential%20insights%20into%20uncertainty%20not%20present%20in%0AEuclidean%20embeddings%2C%20our%20analysis%20reveals%20that%20scaling%20these%20models%20is%0Aparticularly%20difficult.%20We%20propose%20a%20novel%20training%20strategy%20for%20a%20hyperbolic%0Aversion%20of%20BLIP-2%2C%20which%20allows%20to%20achieve%20comparable%20performance%20to%20its%0AEuclidean%20counterpart%2C%20while%20maintaining%20stability%20throughout%20the%20training%0Aprocess%20and%20showing%20a%20meaningful%20indication%20of%20uncertainty%20with%20each%20embedding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperbolic%2520Learning%2520with%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DPaolo%2520Mandica%2520and%2520Luca%2520Franco%2520and%2520Konstantinos%2520Kallidromitis%2520and%2520Suzanne%2520Petryk%2520and%2520Fabio%2520Galasso%26entry.1292438233%3D%2520%2520Hyperbolic%2520embeddings%2520have%2520demonstrated%2520their%2520effectiveness%2520in%2520capturing%250Ameasures%2520of%2520uncertainty%2520and%2520hierarchical%2520relationships%2520across%2520various%250Adeep-learning%2520tasks%252C%2520including%2520image%2520segmentation%2520and%2520active%2520learning.%2520However%252C%250Atheir%2520application%2520in%2520modern%2520vision-language%2520models%2520%2528VLMs%2529%2520has%2520been%2520limited.%2520A%250Anotable%2520exception%2520is%2520MERU%252C%2520which%2520leverages%2520the%2520hierarchical%2520properties%2520of%250Ahyperbolic%2520space%2520in%2520the%2520CLIP%2520ViT-large%2520model%252C%2520consisting%2520of%2520hundreds%2520of%250Amillions%2520parameters.%2520In%2520our%2520work%252C%2520we%2520address%2520the%2520challenges%2520of%2520scaling%250Amulti-modal%2520hyperbolic%2520models%2520by%2520orders%2520of%2520magnitude%2520in%2520terms%2520of%2520parameters%250A%2528billions%2529%2520and%2520training%2520complexity%2520using%2520the%2520BLIP-2%2520architecture.%2520Although%250Ahyperbolic%2520embeddings%2520offer%2520potential%2520insights%2520into%2520uncertainty%2520not%2520present%2520in%250AEuclidean%2520embeddings%252C%2520our%2520analysis%2520reveals%2520that%2520scaling%2520these%2520models%2520is%250Aparticularly%2520difficult.%2520We%2520propose%2520a%2520novel%2520training%2520strategy%2520for%2520a%2520hyperbolic%250Aversion%2520of%2520BLIP-2%252C%2520which%2520allows%2520to%2520achieve%2520comparable%2520performance%2520to%2520its%250AEuclidean%2520counterpart%252C%2520while%2520maintaining%2520stability%2520throughout%2520the%2520training%250Aprocess%2520and%2520showing%2520a%2520meaningful%2520indication%2520of%2520uncertainty%2520with%2520each%2520embedding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Learning%20with%20Multimodal%20Large%20Language%20Models&entry.906535625=Paolo%20Mandica%20and%20Luca%20Franco%20and%20Konstantinos%20Kallidromitis%20and%20Suzanne%20Petryk%20and%20Fabio%20Galasso&entry.1292438233=%20%20Hyperbolic%20embeddings%20have%20demonstrated%20their%20effectiveness%20in%20capturing%0Ameasures%20of%20uncertainty%20and%20hierarchical%20relationships%20across%20various%0Adeep-learning%20tasks%2C%20including%20image%20segmentation%20and%20active%20learning.%20However%2C%0Atheir%20application%20in%20modern%20vision-language%20models%20%28VLMs%29%20has%20been%20limited.%20A%0Anotable%20exception%20is%20MERU%2C%20which%20leverages%20the%20hierarchical%20properties%20of%0Ahyperbolic%20space%20in%20the%20CLIP%20ViT-large%20model%2C%20consisting%20of%20hundreds%20of%0Amillions%20parameters.%20In%20our%20work%2C%20we%20address%20the%20challenges%20of%20scaling%0Amulti-modal%20hyperbolic%20models%20by%20orders%20of%20magnitude%20in%20terms%20of%20parameters%0A%28billions%29%20and%20training%20complexity%20using%20the%20BLIP-2%20architecture.%20Although%0Ahyperbolic%20embeddings%20offer%20potential%20insights%20into%20uncertainty%20not%20present%20in%0AEuclidean%20embeddings%2C%20our%20analysis%20reveals%20that%20scaling%20these%20models%20is%0Aparticularly%20difficult.%20We%20propose%20a%20novel%20training%20strategy%20for%20a%20hyperbolic%0Aversion%20of%20BLIP-2%2C%20which%20allows%20to%20achieve%20comparable%20performance%20to%20its%0AEuclidean%20counterpart%2C%20while%20maintaining%20stability%20throughout%20the%20training%0Aprocess%20and%20showing%20a%20meaningful%20indication%20of%20uncertainty%20with%20each%20embedding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05097v1&entry.124074799=Read"},
{"title": "Loc4Plan: Locating Before Planning for Outdoor Vision and Language\n  Navigation", "author": "Huilin Tian and Jingke Meng and Wei-Shi Zheng and Yuan-Ming Li and Junkai Yan and Yunong Zhang", "abstract": "  Vision and Language Navigation (VLN) is a challenging task that requires\nagents to understand instructions and navigate to the destination in a visual\nenvironment.One of the key challenges in outdoor VLN is keeping track of which\npart of the instruction was completed. To alleviate this problem, previous\nworks mainly focus on grounding the natural language to the visual input, but\nneglecting the crucial role of the agent's spatial position information in the\ngrounding process. In this work, we first explore the substantial effect of\nspatial position locating on the grounding of outdoor VLN, drawing inspiration\nfrom human navigation. In real-world navigation scenarios, before planning a\npath to the destination, humans typically need to figure out their current\nlocation. This observation underscores the pivotal role of spatial localization\nin the navigation process. In this work, we introduce a novel framework,\nLocating be for Planning (Loc4Plan), designed to incorporate spatial perception\nfor action planning in outdoor VLN tasks. The main idea behind Loc4Plan is to\nperform the spatial localization before planning a decision action based on\ncorresponding guidance, which comprises a block-aware spatial locating (BAL)\nmodule and a spatial-aware action planning (SAP) module. Specifically, to help\nthe agent perceive its spatial location in the environment, we propose to learn\na position predictor that measures how far the agent is from the next\nintersection for reflecting its position, which is achieved by the BAL module.\nAfter the locating process, we propose the SAP module to incorporate spatial\ninformation to ground the corresponding guidance and enhance the precision of\naction planning. Extensive experiments on the Touchdown and map2seq datasets\nshow that the proposed Loc4Plan outperforms the SOTA methods.\n", "link": "http://arxiv.org/abs/2408.05090v1", "date": "2024-08-09", "relevancy": 2.2007, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5935}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loc4Plan%3A%20Locating%20Before%20Planning%20for%20Outdoor%20Vision%20and%20Language%0A%20%20Navigation&body=Title%3A%20Loc4Plan%3A%20Locating%20Before%20Planning%20for%20Outdoor%20Vision%20and%20Language%0A%20%20Navigation%0AAuthor%3A%20Huilin%20Tian%20and%20Jingke%20Meng%20and%20Wei-Shi%20Zheng%20and%20Yuan-Ming%20Li%20and%20Junkai%20Yan%20and%20Yunong%20Zhang%0AAbstract%3A%20%20%20Vision%20and%20Language%20Navigation%20%28VLN%29%20is%20a%20challenging%20task%20that%20requires%0Aagents%20to%20understand%20instructions%20and%20navigate%20to%20the%20destination%20in%20a%20visual%0Aenvironment.One%20of%20the%20key%20challenges%20in%20outdoor%20VLN%20is%20keeping%20track%20of%20which%0Apart%20of%20the%20instruction%20was%20completed.%20To%20alleviate%20this%20problem%2C%20previous%0Aworks%20mainly%20focus%20on%20grounding%20the%20natural%20language%20to%20the%20visual%20input%2C%20but%0Aneglecting%20the%20crucial%20role%20of%20the%20agent%27s%20spatial%20position%20information%20in%20the%0Agrounding%20process.%20In%20this%20work%2C%20we%20first%20explore%20the%20substantial%20effect%20of%0Aspatial%20position%20locating%20on%20the%20grounding%20of%20outdoor%20VLN%2C%20drawing%20inspiration%0Afrom%20human%20navigation.%20In%20real-world%20navigation%20scenarios%2C%20before%20planning%20a%0Apath%20to%20the%20destination%2C%20humans%20typically%20need%20to%20figure%20out%20their%20current%0Alocation.%20This%20observation%20underscores%20the%20pivotal%20role%20of%20spatial%20localization%0Ain%20the%20navigation%20process.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%2C%0ALocating%20be%20for%20Planning%20%28Loc4Plan%29%2C%20designed%20to%20incorporate%20spatial%20perception%0Afor%20action%20planning%20in%20outdoor%20VLN%20tasks.%20The%20main%20idea%20behind%20Loc4Plan%20is%20to%0Aperform%20the%20spatial%20localization%20before%20planning%20a%20decision%20action%20based%20on%0Acorresponding%20guidance%2C%20which%20comprises%20a%20block-aware%20spatial%20locating%20%28BAL%29%0Amodule%20and%20a%20spatial-aware%20action%20planning%20%28SAP%29%20module.%20Specifically%2C%20to%20help%0Athe%20agent%20perceive%20its%20spatial%20location%20in%20the%20environment%2C%20we%20propose%20to%20learn%0Aa%20position%20predictor%20that%20measures%20how%20far%20the%20agent%20is%20from%20the%20next%0Aintersection%20for%20reflecting%20its%20position%2C%20which%20is%20achieved%20by%20the%20BAL%20module.%0AAfter%20the%20locating%20process%2C%20we%20propose%20the%20SAP%20module%20to%20incorporate%20spatial%0Ainformation%20to%20ground%20the%20corresponding%20guidance%20and%20enhance%20the%20precision%20of%0Aaction%20planning.%20Extensive%20experiments%20on%20the%20Touchdown%20and%20map2seq%20datasets%0Ashow%20that%20the%20proposed%20Loc4Plan%20outperforms%20the%20SOTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoc4Plan%253A%2520Locating%2520Before%2520Planning%2520for%2520Outdoor%2520Vision%2520and%2520Language%250A%2520%2520Navigation%26entry.906535625%3DHuilin%2520Tian%2520and%2520Jingke%2520Meng%2520and%2520Wei-Shi%2520Zheng%2520and%2520Yuan-Ming%2520Li%2520and%2520Junkai%2520Yan%2520and%2520Yunong%2520Zhang%26entry.1292438233%3D%2520%2520Vision%2520and%2520Language%2520Navigation%2520%2528VLN%2529%2520is%2520a%2520challenging%2520task%2520that%2520requires%250Aagents%2520to%2520understand%2520instructions%2520and%2520navigate%2520to%2520the%2520destination%2520in%2520a%2520visual%250Aenvironment.One%2520of%2520the%2520key%2520challenges%2520in%2520outdoor%2520VLN%2520is%2520keeping%2520track%2520of%2520which%250Apart%2520of%2520the%2520instruction%2520was%2520completed.%2520To%2520alleviate%2520this%2520problem%252C%2520previous%250Aworks%2520mainly%2520focus%2520on%2520grounding%2520the%2520natural%2520language%2520to%2520the%2520visual%2520input%252C%2520but%250Aneglecting%2520the%2520crucial%2520role%2520of%2520the%2520agent%2527s%2520spatial%2520position%2520information%2520in%2520the%250Agrounding%2520process.%2520In%2520this%2520work%252C%2520we%2520first%2520explore%2520the%2520substantial%2520effect%2520of%250Aspatial%2520position%2520locating%2520on%2520the%2520grounding%2520of%2520outdoor%2520VLN%252C%2520drawing%2520inspiration%250Afrom%2520human%2520navigation.%2520In%2520real-world%2520navigation%2520scenarios%252C%2520before%2520planning%2520a%250Apath%2520to%2520the%2520destination%252C%2520humans%2520typically%2520need%2520to%2520figure%2520out%2520their%2520current%250Alocation.%2520This%2520observation%2520underscores%2520the%2520pivotal%2520role%2520of%2520spatial%2520localization%250Ain%2520the%2520navigation%2520process.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520framework%252C%250ALocating%2520be%2520for%2520Planning%2520%2528Loc4Plan%2529%252C%2520designed%2520to%2520incorporate%2520spatial%2520perception%250Afor%2520action%2520planning%2520in%2520outdoor%2520VLN%2520tasks.%2520The%2520main%2520idea%2520behind%2520Loc4Plan%2520is%2520to%250Aperform%2520the%2520spatial%2520localization%2520before%2520planning%2520a%2520decision%2520action%2520based%2520on%250Acorresponding%2520guidance%252C%2520which%2520comprises%2520a%2520block-aware%2520spatial%2520locating%2520%2528BAL%2529%250Amodule%2520and%2520a%2520spatial-aware%2520action%2520planning%2520%2528SAP%2529%2520module.%2520Specifically%252C%2520to%2520help%250Athe%2520agent%2520perceive%2520its%2520spatial%2520location%2520in%2520the%2520environment%252C%2520we%2520propose%2520to%2520learn%250Aa%2520position%2520predictor%2520that%2520measures%2520how%2520far%2520the%2520agent%2520is%2520from%2520the%2520next%250Aintersection%2520for%2520reflecting%2520its%2520position%252C%2520which%2520is%2520achieved%2520by%2520the%2520BAL%2520module.%250AAfter%2520the%2520locating%2520process%252C%2520we%2520propose%2520the%2520SAP%2520module%2520to%2520incorporate%2520spatial%250Ainformation%2520to%2520ground%2520the%2520corresponding%2520guidance%2520and%2520enhance%2520the%2520precision%2520of%250Aaction%2520planning.%2520Extensive%2520experiments%2520on%2520the%2520Touchdown%2520and%2520map2seq%2520datasets%250Ashow%2520that%2520the%2520proposed%2520Loc4Plan%2520outperforms%2520the%2520SOTA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loc4Plan%3A%20Locating%20Before%20Planning%20for%20Outdoor%20Vision%20and%20Language%0A%20%20Navigation&entry.906535625=Huilin%20Tian%20and%20Jingke%20Meng%20and%20Wei-Shi%20Zheng%20and%20Yuan-Ming%20Li%20and%20Junkai%20Yan%20and%20Yunong%20Zhang&entry.1292438233=%20%20Vision%20and%20Language%20Navigation%20%28VLN%29%20is%20a%20challenging%20task%20that%20requires%0Aagents%20to%20understand%20instructions%20and%20navigate%20to%20the%20destination%20in%20a%20visual%0Aenvironment.One%20of%20the%20key%20challenges%20in%20outdoor%20VLN%20is%20keeping%20track%20of%20which%0Apart%20of%20the%20instruction%20was%20completed.%20To%20alleviate%20this%20problem%2C%20previous%0Aworks%20mainly%20focus%20on%20grounding%20the%20natural%20language%20to%20the%20visual%20input%2C%20but%0Aneglecting%20the%20crucial%20role%20of%20the%20agent%27s%20spatial%20position%20information%20in%20the%0Agrounding%20process.%20In%20this%20work%2C%20we%20first%20explore%20the%20substantial%20effect%20of%0Aspatial%20position%20locating%20on%20the%20grounding%20of%20outdoor%20VLN%2C%20drawing%20inspiration%0Afrom%20human%20navigation.%20In%20real-world%20navigation%20scenarios%2C%20before%20planning%20a%0Apath%20to%20the%20destination%2C%20humans%20typically%20need%20to%20figure%20out%20their%20current%0Alocation.%20This%20observation%20underscores%20the%20pivotal%20role%20of%20spatial%20localization%0Ain%20the%20navigation%20process.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%2C%0ALocating%20be%20for%20Planning%20%28Loc4Plan%29%2C%20designed%20to%20incorporate%20spatial%20perception%0Afor%20action%20planning%20in%20outdoor%20VLN%20tasks.%20The%20main%20idea%20behind%20Loc4Plan%20is%20to%0Aperform%20the%20spatial%20localization%20before%20planning%20a%20decision%20action%20based%20on%0Acorresponding%20guidance%2C%20which%20comprises%20a%20block-aware%20spatial%20locating%20%28BAL%29%0Amodule%20and%20a%20spatial-aware%20action%20planning%20%28SAP%29%20module.%20Specifically%2C%20to%20help%0Athe%20agent%20perceive%20its%20spatial%20location%20in%20the%20environment%2C%20we%20propose%20to%20learn%0Aa%20position%20predictor%20that%20measures%20how%20far%20the%20agent%20is%20from%20the%20next%0Aintersection%20for%20reflecting%20its%20position%2C%20which%20is%20achieved%20by%20the%20BAL%20module.%0AAfter%20the%20locating%20process%2C%20we%20propose%20the%20SAP%20module%20to%20incorporate%20spatial%0Ainformation%20to%20ground%20the%20corresponding%20guidance%20and%20enhance%20the%20precision%20of%0Aaction%20planning.%20Extensive%20experiments%20on%20the%20Touchdown%20and%20map2seq%20datasets%0Ashow%20that%20the%20proposed%20Loc4Plan%20outperforms%20the%20SOTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05090v1&entry.124074799=Read"},
{"title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "author": "Chaoyou Fu and Haojia Lin and Zuwei Long and Yunhang Shen and Meng Zhao and Yifan Zhang and Xiong Wang and Di Yin and Long Ma and Xiawu Zheng and Ran He and Rongrong Ji and Yunsheng Wu and Caifeng Shan and Xing Sun", "abstract": "  The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. To the best of our knowledge, we are the first to\nexploit non-awakening interaction and audio interrupt in MLLM. VITA is the\nfirst step for the open-source community to explore the seamless integration of\nmultimodal understanding and interaction. While there is still lots of work to\nbe done on VITA to get close to close-source counterparts, we hope that its\nrole as a pioneer can serve as a cornerstone for subsequent research. Project\nPage: https://vita-home.github.io.\n", "link": "http://arxiv.org/abs/2408.05211v1", "date": "2024-08-09", "relevancy": 2.199, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5716}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5388}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM&body=Title%3A%20VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM%0AAuthor%3A%20Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Zuwei%20Long%20and%20Yunhang%20Shen%20and%20Meng%20Zhao%20and%20Yifan%20Zhang%20and%20Xiong%20Wang%20and%20Di%20Yin%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Ran%20He%20and%20Rongrong%20Ji%20and%20Yunsheng%20Wu%20and%20Caifeng%20Shan%20and%20Xing%20Sun%0AAbstract%3A%20%20%20The%20remarkable%20multimodal%20capabilities%20and%20interactive%20experience%20of%20GPT-4o%0Aunderscore%20their%20necessity%20in%20practical%20applications%2C%20yet%20open-source%20models%0Ararely%20excel%20in%20both%20areas.%20In%20this%20paper%2C%20we%20introduce%20VITA%2C%20the%20first-ever%0Aopen-source%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20adept%20at%20simultaneous%0Aprocessing%20and%20analysis%20of%20Video%2C%20Image%2C%20Text%2C%20and%20Audio%20modalities%2C%20and%0Ameanwhile%20has%20an%20advanced%20multimodal%20interactive%20experience.%20Starting%20from%0AMixtral%208x7B%20as%20a%20language%20foundation%2C%20we%20expand%20its%20Chinese%20vocabulary%0Afollowed%20by%20bilingual%20instruction%20tuning.%20We%20further%20endow%20the%20language%20model%0Awith%20visual%20and%20audio%20capabilities%20through%20two-stage%20multi-task%20learning%20of%0Amultimodal%20alignment%20and%20instruction%20tuning.%20VITA%20demonstrates%20robust%0Afoundational%20capabilities%20of%20multilingual%2C%20vision%2C%20and%20audio%20understanding%2C%20as%0Aevidenced%20by%20its%20strong%20performance%20across%20a%20range%20of%20both%20unimodal%20and%0Amultimodal%20benchmarks.%20Beyond%20foundational%20capabilities%2C%20we%20have%20made%0Aconsiderable%20progress%20in%20enhancing%20the%20natural%20multimodal%20human-computer%0Ainteraction%20experience.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%0Aexploit%20non-awakening%20interaction%20and%20audio%20interrupt%20in%20MLLM.%20VITA%20is%20the%0Afirst%20step%20for%20the%20open-source%20community%20to%20explore%20the%20seamless%20integration%20of%0Amultimodal%20understanding%20and%20interaction.%20While%20there%20is%20still%20lots%20of%20work%20to%0Abe%20done%20on%20VITA%20to%20get%20close%20to%20close-source%20counterparts%2C%20we%20hope%20that%20its%0Arole%20as%20a%20pioneer%20can%20serve%20as%20a%20cornerstone%20for%20subsequent%20research.%20Project%0APage%3A%20https%3A//vita-home.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITA%253A%2520Towards%2520Open-Source%2520Interactive%2520Omni%2520Multimodal%2520LLM%26entry.906535625%3DChaoyou%2520Fu%2520and%2520Haojia%2520Lin%2520and%2520Zuwei%2520Long%2520and%2520Yunhang%2520Shen%2520and%2520Meng%2520Zhao%2520and%2520Yifan%2520Zhang%2520and%2520Xiong%2520Wang%2520and%2520Di%2520Yin%2520and%2520Long%2520Ma%2520and%2520Xiawu%2520Zheng%2520and%2520Ran%2520He%2520and%2520Rongrong%2520Ji%2520and%2520Yunsheng%2520Wu%2520and%2520Caifeng%2520Shan%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520The%2520remarkable%2520multimodal%2520capabilities%2520and%2520interactive%2520experience%2520of%2520GPT-4o%250Aunderscore%2520their%2520necessity%2520in%2520practical%2520applications%252C%2520yet%2520open-source%2520models%250Ararely%2520excel%2520in%2520both%2520areas.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VITA%252C%2520the%2520first-ever%250Aopen-source%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520adept%2520at%2520simultaneous%250Aprocessing%2520and%2520analysis%2520of%2520Video%252C%2520Image%252C%2520Text%252C%2520and%2520Audio%2520modalities%252C%2520and%250Ameanwhile%2520has%2520an%2520advanced%2520multimodal%2520interactive%2520experience.%2520Starting%2520from%250AMixtral%25208x7B%2520as%2520a%2520language%2520foundation%252C%2520we%2520expand%2520its%2520Chinese%2520vocabulary%250Afollowed%2520by%2520bilingual%2520instruction%2520tuning.%2520We%2520further%2520endow%2520the%2520language%2520model%250Awith%2520visual%2520and%2520audio%2520capabilities%2520through%2520two-stage%2520multi-task%2520learning%2520of%250Amultimodal%2520alignment%2520and%2520instruction%2520tuning.%2520VITA%2520demonstrates%2520robust%250Afoundational%2520capabilities%2520of%2520multilingual%252C%2520vision%252C%2520and%2520audio%2520understanding%252C%2520as%250Aevidenced%2520by%2520its%2520strong%2520performance%2520across%2520a%2520range%2520of%2520both%2520unimodal%2520and%250Amultimodal%2520benchmarks.%2520Beyond%2520foundational%2520capabilities%252C%2520we%2520have%2520made%250Aconsiderable%2520progress%2520in%2520enhancing%2520the%2520natural%2520multimodal%2520human-computer%250Ainteraction%2520experience.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%250Aexploit%2520non-awakening%2520interaction%2520and%2520audio%2520interrupt%2520in%2520MLLM.%2520VITA%2520is%2520the%250Afirst%2520step%2520for%2520the%2520open-source%2520community%2520to%2520explore%2520the%2520seamless%2520integration%2520of%250Amultimodal%2520understanding%2520and%2520interaction.%2520While%2520there%2520is%2520still%2520lots%2520of%2520work%2520to%250Abe%2520done%2520on%2520VITA%2520to%2520get%2520close%2520to%2520close-source%2520counterparts%252C%2520we%2520hope%2520that%2520its%250Arole%2520as%2520a%2520pioneer%2520can%2520serve%2520as%2520a%2520cornerstone%2520for%2520subsequent%2520research.%2520Project%250APage%253A%2520https%253A//vita-home.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM&entry.906535625=Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Zuwei%20Long%20and%20Yunhang%20Shen%20and%20Meng%20Zhao%20and%20Yifan%20Zhang%20and%20Xiong%20Wang%20and%20Di%20Yin%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Ran%20He%20and%20Rongrong%20Ji%20and%20Yunsheng%20Wu%20and%20Caifeng%20Shan%20and%20Xing%20Sun&entry.1292438233=%20%20The%20remarkable%20multimodal%20capabilities%20and%20interactive%20experience%20of%20GPT-4o%0Aunderscore%20their%20necessity%20in%20practical%20applications%2C%20yet%20open-source%20models%0Ararely%20excel%20in%20both%20areas.%20In%20this%20paper%2C%20we%20introduce%20VITA%2C%20the%20first-ever%0Aopen-source%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20adept%20at%20simultaneous%0Aprocessing%20and%20analysis%20of%20Video%2C%20Image%2C%20Text%2C%20and%20Audio%20modalities%2C%20and%0Ameanwhile%20has%20an%20advanced%20multimodal%20interactive%20experience.%20Starting%20from%0AMixtral%208x7B%20as%20a%20language%20foundation%2C%20we%20expand%20its%20Chinese%20vocabulary%0Afollowed%20by%20bilingual%20instruction%20tuning.%20We%20further%20endow%20the%20language%20model%0Awith%20visual%20and%20audio%20capabilities%20through%20two-stage%20multi-task%20learning%20of%0Amultimodal%20alignment%20and%20instruction%20tuning.%20VITA%20demonstrates%20robust%0Afoundational%20capabilities%20of%20multilingual%2C%20vision%2C%20and%20audio%20understanding%2C%20as%0Aevidenced%20by%20its%20strong%20performance%20across%20a%20range%20of%20both%20unimodal%20and%0Amultimodal%20benchmarks.%20Beyond%20foundational%20capabilities%2C%20we%20have%20made%0Aconsiderable%20progress%20in%20enhancing%20the%20natural%20multimodal%20human-computer%0Ainteraction%20experience.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%0Aexploit%20non-awakening%20interaction%20and%20audio%20interrupt%20in%20MLLM.%20VITA%20is%20the%0Afirst%20step%20for%20the%20open-source%20community%20to%20explore%20the%20seamless%20integration%20of%0Amultimodal%20understanding%20and%20interaction.%20While%20there%20is%20still%20lots%20of%20work%20to%0Abe%20done%20on%20VITA%20to%20get%20close%20to%20close-source%20counterparts%2C%20we%20hope%20that%20its%0Arole%20as%20a%20pioneer%20can%20serve%20as%20a%20cornerstone%20for%20subsequent%20research.%20Project%0APage%3A%20https%3A//vita-home.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05211v1&entry.124074799=Read"},
{"title": "Cross-Domain Learning for Video Anomaly Detection with Limited\n  Supervision", "author": "Yashika Jain and Ali Dabouei and Min Xu", "abstract": "  Video Anomaly Detection (VAD) automates the identification of unusual events,\nsuch as security threats in surveillance videos. In real-world applications,\nVAD models must effectively operate in cross-domain settings, identifying rare\nanomalies and scenarios not well-represented in the training data. However,\nexisting cross-domain VAD methods focus on unsupervised learning, resulting in\nperformance that falls short of real-world expectations. Since acquiring weak\nsupervision, i.e., video-level labels, for the source domain is cost-effective,\nwe conjecture that combining it with external unlabeled data has notable\npotential to enhance cross-domain performance. To this end, we introduce a\nnovel weakly-supervised framework for Cross-Domain Learning (CDL) in VAD that\nincorporates external data during training by estimating its prediction bias\nand adaptively minimizing that using the predicted uncertainty. We demonstrate\nthe effectiveness of the proposed CDL framework through comprehensive\nexperiments conducted in various configurations on two large-scale VAD\ndatasets: UCF-Crime and XD-Violence. Our method significantly surpasses the\nstate-of-the-art works in cross-domain evaluations, achieving an average\nabsolute improvement of 19.6% on UCF-Crime and 12.87% on XD-Violence.\n", "link": "http://arxiv.org/abs/2408.05191v1", "date": "2024-08-09", "relevancy": 2.1899, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5618}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Learning%20for%20Video%20Anomaly%20Detection%20with%20Limited%0A%20%20Supervision&body=Title%3A%20Cross-Domain%20Learning%20for%20Video%20Anomaly%20Detection%20with%20Limited%0A%20%20Supervision%0AAuthor%3A%20Yashika%20Jain%20and%20Ali%20Dabouei%20and%20Min%20Xu%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection%20%28VAD%29%20automates%20the%20identification%20of%20unusual%20events%2C%0Asuch%20as%20security%20threats%20in%20surveillance%20videos.%20In%20real-world%20applications%2C%0AVAD%20models%20must%20effectively%20operate%20in%20cross-domain%20settings%2C%20identifying%20rare%0Aanomalies%20and%20scenarios%20not%20well-represented%20in%20the%20training%20data.%20However%2C%0Aexisting%20cross-domain%20VAD%20methods%20focus%20on%20unsupervised%20learning%2C%20resulting%20in%0Aperformance%20that%20falls%20short%20of%20real-world%20expectations.%20Since%20acquiring%20weak%0Asupervision%2C%20i.e.%2C%20video-level%20labels%2C%20for%20the%20source%20domain%20is%20cost-effective%2C%0Awe%20conjecture%20that%20combining%20it%20with%20external%20unlabeled%20data%20has%20notable%0Apotential%20to%20enhance%20cross-domain%20performance.%20To%20this%20end%2C%20we%20introduce%20a%0Anovel%20weakly-supervised%20framework%20for%20Cross-Domain%20Learning%20%28CDL%29%20in%20VAD%20that%0Aincorporates%20external%20data%20during%20training%20by%20estimating%20its%20prediction%20bias%0Aand%20adaptively%20minimizing%20that%20using%20the%20predicted%20uncertainty.%20We%20demonstrate%0Athe%20effectiveness%20of%20the%20proposed%20CDL%20framework%20through%20comprehensive%0Aexperiments%20conducted%20in%20various%20configurations%20on%20two%20large-scale%20VAD%0Adatasets%3A%20UCF-Crime%20and%20XD-Violence.%20Our%20method%20significantly%20surpasses%20the%0Astate-of-the-art%20works%20in%20cross-domain%20evaluations%2C%20achieving%20an%20average%0Aabsolute%20improvement%20of%2019.6%25%20on%20UCF-Crime%20and%2012.87%25%20on%20XD-Violence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Domain%2520Learning%2520for%2520Video%2520Anomaly%2520Detection%2520with%2520Limited%250A%2520%2520Supervision%26entry.906535625%3DYashika%2520Jain%2520and%2520Ali%2520Dabouei%2520and%2520Min%2520Xu%26entry.1292438233%3D%2520%2520Video%2520Anomaly%2520Detection%2520%2528VAD%2529%2520automates%2520the%2520identification%2520of%2520unusual%2520events%252C%250Asuch%2520as%2520security%2520threats%2520in%2520surveillance%2520videos.%2520In%2520real-world%2520applications%252C%250AVAD%2520models%2520must%2520effectively%2520operate%2520in%2520cross-domain%2520settings%252C%2520identifying%2520rare%250Aanomalies%2520and%2520scenarios%2520not%2520well-represented%2520in%2520the%2520training%2520data.%2520However%252C%250Aexisting%2520cross-domain%2520VAD%2520methods%2520focus%2520on%2520unsupervised%2520learning%252C%2520resulting%2520in%250Aperformance%2520that%2520falls%2520short%2520of%2520real-world%2520expectations.%2520Since%2520acquiring%2520weak%250Asupervision%252C%2520i.e.%252C%2520video-level%2520labels%252C%2520for%2520the%2520source%2520domain%2520is%2520cost-effective%252C%250Awe%2520conjecture%2520that%2520combining%2520it%2520with%2520external%2520unlabeled%2520data%2520has%2520notable%250Apotential%2520to%2520enhance%2520cross-domain%2520performance.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%250Anovel%2520weakly-supervised%2520framework%2520for%2520Cross-Domain%2520Learning%2520%2528CDL%2529%2520in%2520VAD%2520that%250Aincorporates%2520external%2520data%2520during%2520training%2520by%2520estimating%2520its%2520prediction%2520bias%250Aand%2520adaptively%2520minimizing%2520that%2520using%2520the%2520predicted%2520uncertainty.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520CDL%2520framework%2520through%2520comprehensive%250Aexperiments%2520conducted%2520in%2520various%2520configurations%2520on%2520two%2520large-scale%2520VAD%250Adatasets%253A%2520UCF-Crime%2520and%2520XD-Violence.%2520Our%2520method%2520significantly%2520surpasses%2520the%250Astate-of-the-art%2520works%2520in%2520cross-domain%2520evaluations%252C%2520achieving%2520an%2520average%250Aabsolute%2520improvement%2520of%252019.6%2525%2520on%2520UCF-Crime%2520and%252012.87%2525%2520on%2520XD-Violence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Learning%20for%20Video%20Anomaly%20Detection%20with%20Limited%0A%20%20Supervision&entry.906535625=Yashika%20Jain%20and%20Ali%20Dabouei%20and%20Min%20Xu&entry.1292438233=%20%20Video%20Anomaly%20Detection%20%28VAD%29%20automates%20the%20identification%20of%20unusual%20events%2C%0Asuch%20as%20security%20threats%20in%20surveillance%20videos.%20In%20real-world%20applications%2C%0AVAD%20models%20must%20effectively%20operate%20in%20cross-domain%20settings%2C%20identifying%20rare%0Aanomalies%20and%20scenarios%20not%20well-represented%20in%20the%20training%20data.%20However%2C%0Aexisting%20cross-domain%20VAD%20methods%20focus%20on%20unsupervised%20learning%2C%20resulting%20in%0Aperformance%20that%20falls%20short%20of%20real-world%20expectations.%20Since%20acquiring%20weak%0Asupervision%2C%20i.e.%2C%20video-level%20labels%2C%20for%20the%20source%20domain%20is%20cost-effective%2C%0Awe%20conjecture%20that%20combining%20it%20with%20external%20unlabeled%20data%20has%20notable%0Apotential%20to%20enhance%20cross-domain%20performance.%20To%20this%20end%2C%20we%20introduce%20a%0Anovel%20weakly-supervised%20framework%20for%20Cross-Domain%20Learning%20%28CDL%29%20in%20VAD%20that%0Aincorporates%20external%20data%20during%20training%20by%20estimating%20its%20prediction%20bias%0Aand%20adaptively%20minimizing%20that%20using%20the%20predicted%20uncertainty.%20We%20demonstrate%0Athe%20effectiveness%20of%20the%20proposed%20CDL%20framework%20through%20comprehensive%0Aexperiments%20conducted%20in%20various%20configurations%20on%20two%20large-scale%20VAD%0Adatasets%3A%20UCF-Crime%20and%20XD-Violence.%20Our%20method%20significantly%20surpasses%20the%0Astate-of-the-art%20works%20in%20cross-domain%20evaluations%2C%20achieving%20an%20average%0Aabsolute%20improvement%20of%2019.6%25%20on%20UCF-Crime%20and%2012.87%25%20on%20XD-Violence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05191v1&entry.124074799=Read"},
{"title": "General Lipschitz: Certified Robustness Against Resolvable Semantic\n  Transformations via Transformation-Dependent Randomized Smoothing", "author": "Dmitrii Korzh and Mikhail Pautov and Olga Tsymboi and Ivan Oseledets", "abstract": "  Randomized smoothing is the state-of-the-art approach to construct image\nclassifiers that are provably robust against additive adversarial perturbations\nof bounded magnitude. However, it is more complicated to construct reasonable\ncertificates against semantic transformation (e.g., image blurring,\ntranslation, gamma correction) and their compositions. In this work, we propose\n\\emph{General Lipschitz (GL),} a new framework to certify neural networks\nagainst composable resolvable semantic perturbations. Within the framework, we\nanalyze transformation-dependent Lipschitz-continuity of smoothed classifiers\nw.r.t. transformation parameters and derive corresponding robustness\ncertificates. Our method performs comparably to state-of-the-art approaches on\nthe ImageNet dataset.\n", "link": "http://arxiv.org/abs/2309.16710v2", "date": "2024-08-09", "relevancy": 2.1668, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5562}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5316}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Lipschitz%3A%20Certified%20Robustness%20Against%20Resolvable%20Semantic%0A%20%20Transformations%20via%20Transformation-Dependent%20Randomized%20Smoothing&body=Title%3A%20General%20Lipschitz%3A%20Certified%20Robustness%20Against%20Resolvable%20Semantic%0A%20%20Transformations%20via%20Transformation-Dependent%20Randomized%20Smoothing%0AAuthor%3A%20Dmitrii%20Korzh%20and%20Mikhail%20Pautov%20and%20Olga%20Tsymboi%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20Randomized%20smoothing%20is%20the%20state-of-the-art%20approach%20to%20construct%20image%0Aclassifiers%20that%20are%20provably%20robust%20against%20additive%20adversarial%20perturbations%0Aof%20bounded%20magnitude.%20However%2C%20it%20is%20more%20complicated%20to%20construct%20reasonable%0Acertificates%20against%20semantic%20transformation%20%28e.g.%2C%20image%20blurring%2C%0Atranslation%2C%20gamma%20correction%29%20and%20their%20compositions.%20In%20this%20work%2C%20we%20propose%0A%5Cemph%7BGeneral%20Lipschitz%20%28GL%29%2C%7D%20a%20new%20framework%20to%20certify%20neural%20networks%0Aagainst%20composable%20resolvable%20semantic%20perturbations.%20Within%20the%20framework%2C%20we%0Aanalyze%20transformation-dependent%20Lipschitz-continuity%20of%20smoothed%20classifiers%0Aw.r.t.%20transformation%20parameters%20and%20derive%20corresponding%20robustness%0Acertificates.%20Our%20method%20performs%20comparably%20to%20state-of-the-art%20approaches%20on%0Athe%20ImageNet%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Lipschitz%253A%2520Certified%2520Robustness%2520Against%2520Resolvable%2520Semantic%250A%2520%2520Transformations%2520via%2520Transformation-Dependent%2520Randomized%2520Smoothing%26entry.906535625%3DDmitrii%2520Korzh%2520and%2520Mikhail%2520Pautov%2520and%2520Olga%2520Tsymboi%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3D%2520%2520Randomized%2520smoothing%2520is%2520the%2520state-of-the-art%2520approach%2520to%2520construct%2520image%250Aclassifiers%2520that%2520are%2520provably%2520robust%2520against%2520additive%2520adversarial%2520perturbations%250Aof%2520bounded%2520magnitude.%2520However%252C%2520it%2520is%2520more%2520complicated%2520to%2520construct%2520reasonable%250Acertificates%2520against%2520semantic%2520transformation%2520%2528e.g.%252C%2520image%2520blurring%252C%250Atranslation%252C%2520gamma%2520correction%2529%2520and%2520their%2520compositions.%2520In%2520this%2520work%252C%2520we%2520propose%250A%255Cemph%257BGeneral%2520Lipschitz%2520%2528GL%2529%252C%257D%2520a%2520new%2520framework%2520to%2520certify%2520neural%2520networks%250Aagainst%2520composable%2520resolvable%2520semantic%2520perturbations.%2520Within%2520the%2520framework%252C%2520we%250Aanalyze%2520transformation-dependent%2520Lipschitz-continuity%2520of%2520smoothed%2520classifiers%250Aw.r.t.%2520transformation%2520parameters%2520and%2520derive%2520corresponding%2520robustness%250Acertificates.%2520Our%2520method%2520performs%2520comparably%2520to%2520state-of-the-art%2520approaches%2520on%250Athe%2520ImageNet%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Lipschitz%3A%20Certified%20Robustness%20Against%20Resolvable%20Semantic%0A%20%20Transformations%20via%20Transformation-Dependent%20Randomized%20Smoothing&entry.906535625=Dmitrii%20Korzh%20and%20Mikhail%20Pautov%20and%20Olga%20Tsymboi%20and%20Ivan%20Oseledets&entry.1292438233=%20%20Randomized%20smoothing%20is%20the%20state-of-the-art%20approach%20to%20construct%20image%0Aclassifiers%20that%20are%20provably%20robust%20against%20additive%20adversarial%20perturbations%0Aof%20bounded%20magnitude.%20However%2C%20it%20is%20more%20complicated%20to%20construct%20reasonable%0Acertificates%20against%20semantic%20transformation%20%28e.g.%2C%20image%20blurring%2C%0Atranslation%2C%20gamma%20correction%29%20and%20their%20compositions.%20In%20this%20work%2C%20we%20propose%0A%5Cemph%7BGeneral%20Lipschitz%20%28GL%29%2C%7D%20a%20new%20framework%20to%20certify%20neural%20networks%0Aagainst%20composable%20resolvable%20semantic%20perturbations.%20Within%20the%20framework%2C%20we%0Aanalyze%20transformation-dependent%20Lipschitz-continuity%20of%20smoothed%20classifiers%0Aw.r.t.%20transformation%20parameters%20and%20derive%20corresponding%20robustness%0Acertificates.%20Our%20method%20performs%20comparably%20to%20state-of-the-art%20approaches%20on%0Athe%20ImageNet%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16710v2&entry.124074799=Read"},
{"title": "Instruction Tuning-free Visual Token Complement for Multimodal LLMs", "author": "Dongsheng Wang and Jiequan Cui and Miaoge Li and Wang Lin and Bo Chen and Hanwang Zhang", "abstract": "  As the open community of large language models (LLMs) matures, multimodal\nLLMs (MLLMs) have promised an elegant bridge between vision and language.\nHowever, current research is inherently constrained by challenges such as the\nneed for high-quality instruction pairs and the loss of visual information in\nimage-to-text training objectives. To this end, we propose a Visual Token\nComplement framework (VTC) that helps MLLMs regain the missing visual features\nand thus improve response accuracy. Specifically, our VTC integrates\ntext-to-image generation as a guide to identifying the text-irrelevant\nfeatures, and a visual selector is then developed to generate complementary\nvisual tokens to enrich the original visual input. Moreover, an iterative\nstrategy is further designed to extract more visual information by iteratively\nusing the visual selector without any additional training. Notably, the\ntraining pipeline requires no additional image-text pairs, resulting in a\ndesired instruction tuning-free property. Both qualitative and quantitative\nexperiments demonstrate the superiority and efficiency of our VTC.\n", "link": "http://arxiv.org/abs/2408.05019v1", "date": "2024-08-09", "relevancy": 2.1661, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5218}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction%20Tuning-free%20Visual%20Token%20Complement%20for%20Multimodal%20LLMs&body=Title%3A%20Instruction%20Tuning-free%20Visual%20Token%20Complement%20for%20Multimodal%20LLMs%0AAuthor%3A%20Dongsheng%20Wang%20and%20Jiequan%20Cui%20and%20Miaoge%20Li%20and%20Wang%20Lin%20and%20Bo%20Chen%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20As%20the%20open%20community%20of%20large%20language%20models%20%28LLMs%29%20matures%2C%20multimodal%0ALLMs%20%28MLLMs%29%20have%20promised%20an%20elegant%20bridge%20between%20vision%20and%20language.%0AHowever%2C%20current%20research%20is%20inherently%20constrained%20by%20challenges%20such%20as%20the%0Aneed%20for%20high-quality%20instruction%20pairs%20and%20the%20loss%20of%20visual%20information%20in%0Aimage-to-text%20training%20objectives.%20To%20this%20end%2C%20we%20propose%20a%20Visual%20Token%0AComplement%20framework%20%28VTC%29%20that%20helps%20MLLMs%20regain%20the%20missing%20visual%20features%0Aand%20thus%20improve%20response%20accuracy.%20Specifically%2C%20our%20VTC%20integrates%0Atext-to-image%20generation%20as%20a%20guide%20to%20identifying%20the%20text-irrelevant%0Afeatures%2C%20and%20a%20visual%20selector%20is%20then%20developed%20to%20generate%20complementary%0Avisual%20tokens%20to%20enrich%20the%20original%20visual%20input.%20Moreover%2C%20an%20iterative%0Astrategy%20is%20further%20designed%20to%20extract%20more%20visual%20information%20by%20iteratively%0Ausing%20the%20visual%20selector%20without%20any%20additional%20training.%20Notably%2C%20the%0Atraining%20pipeline%20requires%20no%20additional%20image-text%20pairs%2C%20resulting%20in%20a%0Adesired%20instruction%20tuning-free%20property.%20Both%20qualitative%20and%20quantitative%0Aexperiments%20demonstrate%20the%20superiority%20and%20efficiency%20of%20our%20VTC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction%2520Tuning-free%2520Visual%2520Token%2520Complement%2520for%2520Multimodal%2520LLMs%26entry.906535625%3DDongsheng%2520Wang%2520and%2520Jiequan%2520Cui%2520and%2520Miaoge%2520Li%2520and%2520Wang%2520Lin%2520and%2520Bo%2520Chen%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520As%2520the%2520open%2520community%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520matures%252C%2520multimodal%250ALLMs%2520%2528MLLMs%2529%2520have%2520promised%2520an%2520elegant%2520bridge%2520between%2520vision%2520and%2520language.%250AHowever%252C%2520current%2520research%2520is%2520inherently%2520constrained%2520by%2520challenges%2520such%2520as%2520the%250Aneed%2520for%2520high-quality%2520instruction%2520pairs%2520and%2520the%2520loss%2520of%2520visual%2520information%2520in%250Aimage-to-text%2520training%2520objectives.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520Visual%2520Token%250AComplement%2520framework%2520%2528VTC%2529%2520that%2520helps%2520MLLMs%2520regain%2520the%2520missing%2520visual%2520features%250Aand%2520thus%2520improve%2520response%2520accuracy.%2520Specifically%252C%2520our%2520VTC%2520integrates%250Atext-to-image%2520generation%2520as%2520a%2520guide%2520to%2520identifying%2520the%2520text-irrelevant%250Afeatures%252C%2520and%2520a%2520visual%2520selector%2520is%2520then%2520developed%2520to%2520generate%2520complementary%250Avisual%2520tokens%2520to%2520enrich%2520the%2520original%2520visual%2520input.%2520Moreover%252C%2520an%2520iterative%250Astrategy%2520is%2520further%2520designed%2520to%2520extract%2520more%2520visual%2520information%2520by%2520iteratively%250Ausing%2520the%2520visual%2520selector%2520without%2520any%2520additional%2520training.%2520Notably%252C%2520the%250Atraining%2520pipeline%2520requires%2520no%2520additional%2520image-text%2520pairs%252C%2520resulting%2520in%2520a%250Adesired%2520instruction%2520tuning-free%2520property.%2520Both%2520qualitative%2520and%2520quantitative%250Aexperiments%2520demonstrate%2520the%2520superiority%2520and%2520efficiency%2520of%2520our%2520VTC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction%20Tuning-free%20Visual%20Token%20Complement%20for%20Multimodal%20LLMs&entry.906535625=Dongsheng%20Wang%20and%20Jiequan%20Cui%20and%20Miaoge%20Li%20and%20Wang%20Lin%20and%20Bo%20Chen%20and%20Hanwang%20Zhang&entry.1292438233=%20%20As%20the%20open%20community%20of%20large%20language%20models%20%28LLMs%29%20matures%2C%20multimodal%0ALLMs%20%28MLLMs%29%20have%20promised%20an%20elegant%20bridge%20between%20vision%20and%20language.%0AHowever%2C%20current%20research%20is%20inherently%20constrained%20by%20challenges%20such%20as%20the%0Aneed%20for%20high-quality%20instruction%20pairs%20and%20the%20loss%20of%20visual%20information%20in%0Aimage-to-text%20training%20objectives.%20To%20this%20end%2C%20we%20propose%20a%20Visual%20Token%0AComplement%20framework%20%28VTC%29%20that%20helps%20MLLMs%20regain%20the%20missing%20visual%20features%0Aand%20thus%20improve%20response%20accuracy.%20Specifically%2C%20our%20VTC%20integrates%0Atext-to-image%20generation%20as%20a%20guide%20to%20identifying%20the%20text-irrelevant%0Afeatures%2C%20and%20a%20visual%20selector%20is%20then%20developed%20to%20generate%20complementary%0Avisual%20tokens%20to%20enrich%20the%20original%20visual%20input.%20Moreover%2C%20an%20iterative%0Astrategy%20is%20further%20designed%20to%20extract%20more%20visual%20information%20by%20iteratively%0Ausing%20the%20visual%20selector%20without%20any%20additional%20training.%20Notably%2C%20the%0Atraining%20pipeline%20requires%20no%20additional%20image-text%20pairs%2C%20resulting%20in%20a%0Adesired%20instruction%20tuning-free%20property.%20Both%20qualitative%20and%20quantitative%0Aexperiments%20demonstrate%20the%20superiority%20and%20efficiency%20of%20our%20VTC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05019v1&entry.124074799=Read"},
{"title": "Kalman-Inspired Feature Propagation for Video Face Super-Resolution", "author": "Ruicheng Feng and Chongyi Li and Chen Change Loy", "abstract": "  Despite the promising progress of face image super-resolution, video face\nsuper-resolution remains relatively under-explored. Existing approaches either\nadapt general video super-resolution networks to face datasets or apply\nestablished face image super-resolution models independently on individual\nvideo frames. These paradigms encounter challenges either in reconstructing\nfacial details or maintaining temporal consistency. To address these issues, we\nintroduce a novel framework called Kalman-inspired Feature Propagation (KEEP),\ndesigned to maintain a stable face prior over time. The Kalman filtering\nprinciples offer our method a recurrent ability to use the information from\npreviously restored frames to guide and regulate the restoration process of the\ncurrent frame. Extensive experiments demonstrate the effectiveness of our\nmethod in capturing facial details consistently across video frames. Code and\nvideo demo are available at https://jnjaby.github.io/projects/KEEP.\n", "link": "http://arxiv.org/abs/2408.05205v1", "date": "2024-08-09", "relevancy": 2.1646, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5471}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5431}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kalman-Inspired%20Feature%20Propagation%20for%20Video%20Face%20Super-Resolution&body=Title%3A%20Kalman-Inspired%20Feature%20Propagation%20for%20Video%20Face%20Super-Resolution%0AAuthor%3A%20Ruicheng%20Feng%20and%20Chongyi%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20Despite%20the%20promising%20progress%20of%20face%20image%20super-resolution%2C%20video%20face%0Asuper-resolution%20remains%20relatively%20under-explored.%20Existing%20approaches%20either%0Aadapt%20general%20video%20super-resolution%20networks%20to%20face%20datasets%20or%20apply%0Aestablished%20face%20image%20super-resolution%20models%20independently%20on%20individual%0Avideo%20frames.%20These%20paradigms%20encounter%20challenges%20either%20in%20reconstructing%0Afacial%20details%20or%20maintaining%20temporal%20consistency.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20framework%20called%20Kalman-inspired%20Feature%20Propagation%20%28KEEP%29%2C%0Adesigned%20to%20maintain%20a%20stable%20face%20prior%20over%20time.%20The%20Kalman%20filtering%0Aprinciples%20offer%20our%20method%20a%20recurrent%20ability%20to%20use%20the%20information%20from%0Apreviously%20restored%20frames%20to%20guide%20and%20regulate%20the%20restoration%20process%20of%20the%0Acurrent%20frame.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20in%20capturing%20facial%20details%20consistently%20across%20video%20frames.%20Code%20and%0Avideo%20demo%20are%20available%20at%20https%3A//jnjaby.github.io/projects/KEEP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKalman-Inspired%2520Feature%2520Propagation%2520for%2520Video%2520Face%2520Super-Resolution%26entry.906535625%3DRuicheng%2520Feng%2520and%2520Chongyi%2520Li%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520Despite%2520the%2520promising%2520progress%2520of%2520face%2520image%2520super-resolution%252C%2520video%2520face%250Asuper-resolution%2520remains%2520relatively%2520under-explored.%2520Existing%2520approaches%2520either%250Aadapt%2520general%2520video%2520super-resolution%2520networks%2520to%2520face%2520datasets%2520or%2520apply%250Aestablished%2520face%2520image%2520super-resolution%2520models%2520independently%2520on%2520individual%250Avideo%2520frames.%2520These%2520paradigms%2520encounter%2520challenges%2520either%2520in%2520reconstructing%250Afacial%2520details%2520or%2520maintaining%2520temporal%2520consistency.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520novel%2520framework%2520called%2520Kalman-inspired%2520Feature%2520Propagation%2520%2528KEEP%2529%252C%250Adesigned%2520to%2520maintain%2520a%2520stable%2520face%2520prior%2520over%2520time.%2520The%2520Kalman%2520filtering%250Aprinciples%2520offer%2520our%2520method%2520a%2520recurrent%2520ability%2520to%2520use%2520the%2520information%2520from%250Apreviously%2520restored%2520frames%2520to%2520guide%2520and%2520regulate%2520the%2520restoration%2520process%2520of%2520the%250Acurrent%2520frame.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%2520in%2520capturing%2520facial%2520details%2520consistently%2520across%2520video%2520frames.%2520Code%2520and%250Avideo%2520demo%2520are%2520available%2520at%2520https%253A//jnjaby.github.io/projects/KEEP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kalman-Inspired%20Feature%20Propagation%20for%20Video%20Face%20Super-Resolution&entry.906535625=Ruicheng%20Feng%20and%20Chongyi%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20Despite%20the%20promising%20progress%20of%20face%20image%20super-resolution%2C%20video%20face%0Asuper-resolution%20remains%20relatively%20under-explored.%20Existing%20approaches%20either%0Aadapt%20general%20video%20super-resolution%20networks%20to%20face%20datasets%20or%20apply%0Aestablished%20face%20image%20super-resolution%20models%20independently%20on%20individual%0Avideo%20frames.%20These%20paradigms%20encounter%20challenges%20either%20in%20reconstructing%0Afacial%20details%20or%20maintaining%20temporal%20consistency.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20framework%20called%20Kalman-inspired%20Feature%20Propagation%20%28KEEP%29%2C%0Adesigned%20to%20maintain%20a%20stable%20face%20prior%20over%20time.%20The%20Kalman%20filtering%0Aprinciples%20offer%20our%20method%20a%20recurrent%20ability%20to%20use%20the%20information%20from%0Apreviously%20restored%20frames%20to%20guide%20and%20regulate%20the%20restoration%20process%20of%20the%0Acurrent%20frame.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20in%20capturing%20facial%20details%20consistently%20across%20video%20frames.%20Code%20and%0Avideo%20demo%20are%20available%20at%20https%3A//jnjaby.github.io/projects/KEEP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05205v1&entry.124074799=Read"},
{"title": "A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered\n  Applications are Vulnerable to PromptWares", "author": "Stav Cohen and Ron Bitton and Ben Nassi", "abstract": "  In this paper we argue that a jailbroken GenAI model can cause substantial\nharm to GenAI-powered applications and facilitate PromptWare, a new type of\nattack that flips the GenAI model's behavior from serving an application to\nattacking it. PromptWare exploits user inputs to jailbreak a GenAI model to\nforce/perform malicious activity within the context of a GenAI-powered\napplication. First, we introduce a naive implementation of PromptWare that\nbehaves as malware that targets Plan & Execute architectures (a.k.a., ReAct,\nfunction calling). We show that attackers could force a desired execution flow\nby creating a user input that produces desired outputs given that the logic of\nthe GenAI-powered application is known to attackers. We demonstrate the\napplication of a DoS attack that triggers the execution of a GenAI-powered\nassistant to enter an infinite loop that wastes money and computational\nresources on redundant API calls to a GenAI engine, preventing the application\nfrom providing service to a user. Next, we introduce a more sophisticated\nimplementation of PromptWare that we name Advanced PromptWare Threat (APwT)\nthat targets GenAI-powered applications whose logic is unknown to attackers. We\nshow that attackers could create user input that exploits the GenAI engine's\nadvanced AI capabilities to launch a kill chain in inference time consisting of\nsix steps intended to escalate privileges, analyze the application's context,\nidentify valuable assets, reason possible malicious activities, decide on one\nof them, and execute it. We demonstrate the application of APwT against a\nGenAI-powered e-commerce chatbot and show that it can trigger the modification\nof SQL tables, potentially leading to unauthorized discounts on the items sold\nto the user.\n", "link": "http://arxiv.org/abs/2408.05061v1", "date": "2024-08-09", "relevancy": 2.1414, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5015}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3947}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Jailbroken%20GenAI%20Model%20Can%20Cause%20Substantial%20Harm%3A%20GenAI-powered%0A%20%20Applications%20are%20Vulnerable%20to%20PromptWares&body=Title%3A%20A%20Jailbroken%20GenAI%20Model%20Can%20Cause%20Substantial%20Harm%3A%20GenAI-powered%0A%20%20Applications%20are%20Vulnerable%20to%20PromptWares%0AAuthor%3A%20Stav%20Cohen%20and%20Ron%20Bitton%20and%20Ben%20Nassi%0AAbstract%3A%20%20%20In%20this%20paper%20we%20argue%20that%20a%20jailbroken%20GenAI%20model%20can%20cause%20substantial%0Aharm%20to%20GenAI-powered%20applications%20and%20facilitate%20PromptWare%2C%20a%20new%20type%20of%0Aattack%20that%20flips%20the%20GenAI%20model%27s%20behavior%20from%20serving%20an%20application%20to%0Aattacking%20it.%20PromptWare%20exploits%20user%20inputs%20to%20jailbreak%20a%20GenAI%20model%20to%0Aforce/perform%20malicious%20activity%20within%20the%20context%20of%20a%20GenAI-powered%0Aapplication.%20First%2C%20we%20introduce%20a%20naive%20implementation%20of%20PromptWare%20that%0Abehaves%20as%20malware%20that%20targets%20Plan%20%26%20Execute%20architectures%20%28a.k.a.%2C%20ReAct%2C%0Afunction%20calling%29.%20We%20show%20that%20attackers%20could%20force%20a%20desired%20execution%20flow%0Aby%20creating%20a%20user%20input%20that%20produces%20desired%20outputs%20given%20that%20the%20logic%20of%0Athe%20GenAI-powered%20application%20is%20known%20to%20attackers.%20We%20demonstrate%20the%0Aapplication%20of%20a%20DoS%20attack%20that%20triggers%20the%20execution%20of%20a%20GenAI-powered%0Aassistant%20to%20enter%20an%20infinite%20loop%20that%20wastes%20money%20and%20computational%0Aresources%20on%20redundant%20API%20calls%20to%20a%20GenAI%20engine%2C%20preventing%20the%20application%0Afrom%20providing%20service%20to%20a%20user.%20Next%2C%20we%20introduce%20a%20more%20sophisticated%0Aimplementation%20of%20PromptWare%20that%20we%20name%20Advanced%20PromptWare%20Threat%20%28APwT%29%0Athat%20targets%20GenAI-powered%20applications%20whose%20logic%20is%20unknown%20to%20attackers.%20We%0Ashow%20that%20attackers%20could%20create%20user%20input%20that%20exploits%20the%20GenAI%20engine%27s%0Aadvanced%20AI%20capabilities%20to%20launch%20a%20kill%20chain%20in%20inference%20time%20consisting%20of%0Asix%20steps%20intended%20to%20escalate%20privileges%2C%20analyze%20the%20application%27s%20context%2C%0Aidentify%20valuable%20assets%2C%20reason%20possible%20malicious%20activities%2C%20decide%20on%20one%0Aof%20them%2C%20and%20execute%20it.%20We%20demonstrate%20the%20application%20of%20APwT%20against%20a%0AGenAI-powered%20e-commerce%20chatbot%20and%20show%20that%20it%20can%20trigger%20the%20modification%0Aof%20SQL%20tables%2C%20potentially%20leading%20to%20unauthorized%20discounts%20on%20the%20items%20sold%0Ato%20the%20user.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Jailbroken%2520GenAI%2520Model%2520Can%2520Cause%2520Substantial%2520Harm%253A%2520GenAI-powered%250A%2520%2520Applications%2520are%2520Vulnerable%2520to%2520PromptWares%26entry.906535625%3DStav%2520Cohen%2520and%2520Ron%2520Bitton%2520and%2520Ben%2520Nassi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520argue%2520that%2520a%2520jailbroken%2520GenAI%2520model%2520can%2520cause%2520substantial%250Aharm%2520to%2520GenAI-powered%2520applications%2520and%2520facilitate%2520PromptWare%252C%2520a%2520new%2520type%2520of%250Aattack%2520that%2520flips%2520the%2520GenAI%2520model%2527s%2520behavior%2520from%2520serving%2520an%2520application%2520to%250Aattacking%2520it.%2520PromptWare%2520exploits%2520user%2520inputs%2520to%2520jailbreak%2520a%2520GenAI%2520model%2520to%250Aforce/perform%2520malicious%2520activity%2520within%2520the%2520context%2520of%2520a%2520GenAI-powered%250Aapplication.%2520First%252C%2520we%2520introduce%2520a%2520naive%2520implementation%2520of%2520PromptWare%2520that%250Abehaves%2520as%2520malware%2520that%2520targets%2520Plan%2520%2526%2520Execute%2520architectures%2520%2528a.k.a.%252C%2520ReAct%252C%250Afunction%2520calling%2529.%2520We%2520show%2520that%2520attackers%2520could%2520force%2520a%2520desired%2520execution%2520flow%250Aby%2520creating%2520a%2520user%2520input%2520that%2520produces%2520desired%2520outputs%2520given%2520that%2520the%2520logic%2520of%250Athe%2520GenAI-powered%2520application%2520is%2520known%2520to%2520attackers.%2520We%2520demonstrate%2520the%250Aapplication%2520of%2520a%2520DoS%2520attack%2520that%2520triggers%2520the%2520execution%2520of%2520a%2520GenAI-powered%250Aassistant%2520to%2520enter%2520an%2520infinite%2520loop%2520that%2520wastes%2520money%2520and%2520computational%250Aresources%2520on%2520redundant%2520API%2520calls%2520to%2520a%2520GenAI%2520engine%252C%2520preventing%2520the%2520application%250Afrom%2520providing%2520service%2520to%2520a%2520user.%2520Next%252C%2520we%2520introduce%2520a%2520more%2520sophisticated%250Aimplementation%2520of%2520PromptWare%2520that%2520we%2520name%2520Advanced%2520PromptWare%2520Threat%2520%2528APwT%2529%250Athat%2520targets%2520GenAI-powered%2520applications%2520whose%2520logic%2520is%2520unknown%2520to%2520attackers.%2520We%250Ashow%2520that%2520attackers%2520could%2520create%2520user%2520input%2520that%2520exploits%2520the%2520GenAI%2520engine%2527s%250Aadvanced%2520AI%2520capabilities%2520to%2520launch%2520a%2520kill%2520chain%2520in%2520inference%2520time%2520consisting%2520of%250Asix%2520steps%2520intended%2520to%2520escalate%2520privileges%252C%2520analyze%2520the%2520application%2527s%2520context%252C%250Aidentify%2520valuable%2520assets%252C%2520reason%2520possible%2520malicious%2520activities%252C%2520decide%2520on%2520one%250Aof%2520them%252C%2520and%2520execute%2520it.%2520We%2520demonstrate%2520the%2520application%2520of%2520APwT%2520against%2520a%250AGenAI-powered%2520e-commerce%2520chatbot%2520and%2520show%2520that%2520it%2520can%2520trigger%2520the%2520modification%250Aof%2520SQL%2520tables%252C%2520potentially%2520leading%2520to%2520unauthorized%2520discounts%2520on%2520the%2520items%2520sold%250Ato%2520the%2520user.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Jailbroken%20GenAI%20Model%20Can%20Cause%20Substantial%20Harm%3A%20GenAI-powered%0A%20%20Applications%20are%20Vulnerable%20to%20PromptWares&entry.906535625=Stav%20Cohen%20and%20Ron%20Bitton%20and%20Ben%20Nassi&entry.1292438233=%20%20In%20this%20paper%20we%20argue%20that%20a%20jailbroken%20GenAI%20model%20can%20cause%20substantial%0Aharm%20to%20GenAI-powered%20applications%20and%20facilitate%20PromptWare%2C%20a%20new%20type%20of%0Aattack%20that%20flips%20the%20GenAI%20model%27s%20behavior%20from%20serving%20an%20application%20to%0Aattacking%20it.%20PromptWare%20exploits%20user%20inputs%20to%20jailbreak%20a%20GenAI%20model%20to%0Aforce/perform%20malicious%20activity%20within%20the%20context%20of%20a%20GenAI-powered%0Aapplication.%20First%2C%20we%20introduce%20a%20naive%20implementation%20of%20PromptWare%20that%0Abehaves%20as%20malware%20that%20targets%20Plan%20%26%20Execute%20architectures%20%28a.k.a.%2C%20ReAct%2C%0Afunction%20calling%29.%20We%20show%20that%20attackers%20could%20force%20a%20desired%20execution%20flow%0Aby%20creating%20a%20user%20input%20that%20produces%20desired%20outputs%20given%20that%20the%20logic%20of%0Athe%20GenAI-powered%20application%20is%20known%20to%20attackers.%20We%20demonstrate%20the%0Aapplication%20of%20a%20DoS%20attack%20that%20triggers%20the%20execution%20of%20a%20GenAI-powered%0Aassistant%20to%20enter%20an%20infinite%20loop%20that%20wastes%20money%20and%20computational%0Aresources%20on%20redundant%20API%20calls%20to%20a%20GenAI%20engine%2C%20preventing%20the%20application%0Afrom%20providing%20service%20to%20a%20user.%20Next%2C%20we%20introduce%20a%20more%20sophisticated%0Aimplementation%20of%20PromptWare%20that%20we%20name%20Advanced%20PromptWare%20Threat%20%28APwT%29%0Athat%20targets%20GenAI-powered%20applications%20whose%20logic%20is%20unknown%20to%20attackers.%20We%0Ashow%20that%20attackers%20could%20create%20user%20input%20that%20exploits%20the%20GenAI%20engine%27s%0Aadvanced%20AI%20capabilities%20to%20launch%20a%20kill%20chain%20in%20inference%20time%20consisting%20of%0Asix%20steps%20intended%20to%20escalate%20privileges%2C%20analyze%20the%20application%27s%20context%2C%0Aidentify%20valuable%20assets%2C%20reason%20possible%20malicious%20activities%2C%20decide%20on%20one%0Aof%20them%2C%20and%20execute%20it.%20We%20demonstrate%20the%20application%20of%20APwT%20against%20a%0AGenAI-powered%20e-commerce%20chatbot%20and%20show%20that%20it%20can%20trigger%20the%20modification%0Aof%20SQL%20tables%2C%20potentially%20leading%20to%20unauthorized%20discounts%20on%20the%20items%20sold%0Ato%20the%20user.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05061v1&entry.124074799=Read"},
{"title": "Learning to Generate Parameters of ConvNets for Unseen Image Data", "author": "Shiye Wang and Kaituo Feng and Changsheng Li and Ye Yuan and Guoren Wang", "abstract": "  Typical Convolutional Neural Networks (ConvNets) depend heavily on large\namounts of image data and resort to an iterative optimization algorithm (e.g.,\nSGD or Adam) to learn network parameters, which makes training very time- and\nresource-intensive. In this paper, we propose a new training paradigm and\nformulate the parameter learning of ConvNets into a prediction task: given a\nConvNet architecture, we observe there exist correlations between image\ndatasets and their corresponding optimal network parameters, and explore if we\ncan learn a hyper-mapping between them to capture the relations, such that we\ncan directly predict the parameters of the network for an image dataset never\nseen during the training phase. To do this, we put forward a new hypernetwork\nbased model, called PudNet, which intends to learn a mapping between datasets\nand their corresponding network parameters, and then predicts parameters for\nunseen data with only a single forward propagation. Moreover, our model\nbenefits from a series of adaptive hyper recurrent units sharing weights to\ncapture the dependencies of parameters among different network layers.\nExtensive experiments demonstrate that our proposed method achieves good\nefficacy for unseen image datasets on two kinds of settings: Intra-dataset\nprediction and Inter-dataset prediction. Our PudNet can also well scale up to\nlarge-scale datasets, e.g., ImageNet-1K. It takes 8967 GPU seconds to train\nResNet-18 on the ImageNet-1K using GC from scratch and obtain a top-5 accuracy\nof 44.65%. However, our PudNet costs only 3.89 GPU seconds to predict the\nnetwork parameters of ResNet-18 achieving comparable performance (44.92%), more\nthan 2,300 times faster than the traditional training paradigm.\n", "link": "http://arxiv.org/abs/2310.11862v3", "date": "2024-08-09", "relevancy": 2.1279, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5398}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5314}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%20Parameters%20of%20ConvNets%20for%20Unseen%20Image%20Data&body=Title%3A%20Learning%20to%20Generate%20Parameters%20of%20ConvNets%20for%20Unseen%20Image%20Data%0AAuthor%3A%20Shiye%20Wang%20and%20Kaituo%20Feng%20and%20Changsheng%20Li%20and%20Ye%20Yuan%20and%20Guoren%20Wang%0AAbstract%3A%20%20%20Typical%20Convolutional%20Neural%20Networks%20%28ConvNets%29%20depend%20heavily%20on%20large%0Aamounts%20of%20image%20data%20and%20resort%20to%20an%20iterative%20optimization%20algorithm%20%28e.g.%2C%0ASGD%20or%20Adam%29%20to%20learn%20network%20parameters%2C%20which%20makes%20training%20very%20time-%20and%0Aresource-intensive.%20In%20this%20paper%2C%20we%20propose%20a%20new%20training%20paradigm%20and%0Aformulate%20the%20parameter%20learning%20of%20ConvNets%20into%20a%20prediction%20task%3A%20given%20a%0AConvNet%20architecture%2C%20we%20observe%20there%20exist%20correlations%20between%20image%0Adatasets%20and%20their%20corresponding%20optimal%20network%20parameters%2C%20and%20explore%20if%20we%0Acan%20learn%20a%20hyper-mapping%20between%20them%20to%20capture%20the%20relations%2C%20such%20that%20we%0Acan%20directly%20predict%20the%20parameters%20of%20the%20network%20for%20an%20image%20dataset%20never%0Aseen%20during%20the%20training%20phase.%20To%20do%20this%2C%20we%20put%20forward%20a%20new%20hypernetwork%0Abased%20model%2C%20called%20PudNet%2C%20which%20intends%20to%20learn%20a%20mapping%20between%20datasets%0Aand%20their%20corresponding%20network%20parameters%2C%20and%20then%20predicts%20parameters%20for%0Aunseen%20data%20with%20only%20a%20single%20forward%20propagation.%20Moreover%2C%20our%20model%0Abenefits%20from%20a%20series%20of%20adaptive%20hyper%20recurrent%20units%20sharing%20weights%20to%0Acapture%20the%20dependencies%20of%20parameters%20among%20different%20network%20layers.%0AExtensive%20experiments%20demonstrate%20that%20our%20proposed%20method%20achieves%20good%0Aefficacy%20for%20unseen%20image%20datasets%20on%20two%20kinds%20of%20settings%3A%20Intra-dataset%0Aprediction%20and%20Inter-dataset%20prediction.%20Our%20PudNet%20can%20also%20well%20scale%20up%20to%0Alarge-scale%20datasets%2C%20e.g.%2C%20ImageNet-1K.%20It%20takes%208967%20GPU%20seconds%20to%20train%0AResNet-18%20on%20the%20ImageNet-1K%20using%20GC%20from%20scratch%20and%20obtain%20a%20top-5%20accuracy%0Aof%2044.65%25.%20However%2C%20our%20PudNet%20costs%20only%203.89%20GPU%20seconds%20to%20predict%20the%0Anetwork%20parameters%20of%20ResNet-18%20achieving%20comparable%20performance%20%2844.92%25%29%2C%20more%0Athan%202%2C300%20times%20faster%20than%20the%20traditional%20training%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11862v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%2520Parameters%2520of%2520ConvNets%2520for%2520Unseen%2520Image%2520Data%26entry.906535625%3DShiye%2520Wang%2520and%2520Kaituo%2520Feng%2520and%2520Changsheng%2520Li%2520and%2520Ye%2520Yuan%2520and%2520Guoren%2520Wang%26entry.1292438233%3D%2520%2520Typical%2520Convolutional%2520Neural%2520Networks%2520%2528ConvNets%2529%2520depend%2520heavily%2520on%2520large%250Aamounts%2520of%2520image%2520data%2520and%2520resort%2520to%2520an%2520iterative%2520optimization%2520algorithm%2520%2528e.g.%252C%250ASGD%2520or%2520Adam%2529%2520to%2520learn%2520network%2520parameters%252C%2520which%2520makes%2520training%2520very%2520time-%2520and%250Aresource-intensive.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520training%2520paradigm%2520and%250Aformulate%2520the%2520parameter%2520learning%2520of%2520ConvNets%2520into%2520a%2520prediction%2520task%253A%2520given%2520a%250AConvNet%2520architecture%252C%2520we%2520observe%2520there%2520exist%2520correlations%2520between%2520image%250Adatasets%2520and%2520their%2520corresponding%2520optimal%2520network%2520parameters%252C%2520and%2520explore%2520if%2520we%250Acan%2520learn%2520a%2520hyper-mapping%2520between%2520them%2520to%2520capture%2520the%2520relations%252C%2520such%2520that%2520we%250Acan%2520directly%2520predict%2520the%2520parameters%2520of%2520the%2520network%2520for%2520an%2520image%2520dataset%2520never%250Aseen%2520during%2520the%2520training%2520phase.%2520To%2520do%2520this%252C%2520we%2520put%2520forward%2520a%2520new%2520hypernetwork%250Abased%2520model%252C%2520called%2520PudNet%252C%2520which%2520intends%2520to%2520learn%2520a%2520mapping%2520between%2520datasets%250Aand%2520their%2520corresponding%2520network%2520parameters%252C%2520and%2520then%2520predicts%2520parameters%2520for%250Aunseen%2520data%2520with%2520only%2520a%2520single%2520forward%2520propagation.%2520Moreover%252C%2520our%2520model%250Abenefits%2520from%2520a%2520series%2520of%2520adaptive%2520hyper%2520recurrent%2520units%2520sharing%2520weights%2520to%250Acapture%2520the%2520dependencies%2520of%2520parameters%2520among%2520different%2520network%2520layers.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520method%2520achieves%2520good%250Aefficacy%2520for%2520unseen%2520image%2520datasets%2520on%2520two%2520kinds%2520of%2520settings%253A%2520Intra-dataset%250Aprediction%2520and%2520Inter-dataset%2520prediction.%2520Our%2520PudNet%2520can%2520also%2520well%2520scale%2520up%2520to%250Alarge-scale%2520datasets%252C%2520e.g.%252C%2520ImageNet-1K.%2520It%2520takes%25208967%2520GPU%2520seconds%2520to%2520train%250AResNet-18%2520on%2520the%2520ImageNet-1K%2520using%2520GC%2520from%2520scratch%2520and%2520obtain%2520a%2520top-5%2520accuracy%250Aof%252044.65%2525.%2520However%252C%2520our%2520PudNet%2520costs%2520only%25203.89%2520GPU%2520seconds%2520to%2520predict%2520the%250Anetwork%2520parameters%2520of%2520ResNet-18%2520achieving%2520comparable%2520performance%2520%252844.92%2525%2529%252C%2520more%250Athan%25202%252C300%2520times%2520faster%2520than%2520the%2520traditional%2520training%2520paradigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11862v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%20Parameters%20of%20ConvNets%20for%20Unseen%20Image%20Data&entry.906535625=Shiye%20Wang%20and%20Kaituo%20Feng%20and%20Changsheng%20Li%20and%20Ye%20Yuan%20and%20Guoren%20Wang&entry.1292438233=%20%20Typical%20Convolutional%20Neural%20Networks%20%28ConvNets%29%20depend%20heavily%20on%20large%0Aamounts%20of%20image%20data%20and%20resort%20to%20an%20iterative%20optimization%20algorithm%20%28e.g.%2C%0ASGD%20or%20Adam%29%20to%20learn%20network%20parameters%2C%20which%20makes%20training%20very%20time-%20and%0Aresource-intensive.%20In%20this%20paper%2C%20we%20propose%20a%20new%20training%20paradigm%20and%0Aformulate%20the%20parameter%20learning%20of%20ConvNets%20into%20a%20prediction%20task%3A%20given%20a%0AConvNet%20architecture%2C%20we%20observe%20there%20exist%20correlations%20between%20image%0Adatasets%20and%20their%20corresponding%20optimal%20network%20parameters%2C%20and%20explore%20if%20we%0Acan%20learn%20a%20hyper-mapping%20between%20them%20to%20capture%20the%20relations%2C%20such%20that%20we%0Acan%20directly%20predict%20the%20parameters%20of%20the%20network%20for%20an%20image%20dataset%20never%0Aseen%20during%20the%20training%20phase.%20To%20do%20this%2C%20we%20put%20forward%20a%20new%20hypernetwork%0Abased%20model%2C%20called%20PudNet%2C%20which%20intends%20to%20learn%20a%20mapping%20between%20datasets%0Aand%20their%20corresponding%20network%20parameters%2C%20and%20then%20predicts%20parameters%20for%0Aunseen%20data%20with%20only%20a%20single%20forward%20propagation.%20Moreover%2C%20our%20model%0Abenefits%20from%20a%20series%20of%20adaptive%20hyper%20recurrent%20units%20sharing%20weights%20to%0Acapture%20the%20dependencies%20of%20parameters%20among%20different%20network%20layers.%0AExtensive%20experiments%20demonstrate%20that%20our%20proposed%20method%20achieves%20good%0Aefficacy%20for%20unseen%20image%20datasets%20on%20two%20kinds%20of%20settings%3A%20Intra-dataset%0Aprediction%20and%20Inter-dataset%20prediction.%20Our%20PudNet%20can%20also%20well%20scale%20up%20to%0Alarge-scale%20datasets%2C%20e.g.%2C%20ImageNet-1K.%20It%20takes%208967%20GPU%20seconds%20to%20train%0AResNet-18%20on%20the%20ImageNet-1K%20using%20GC%20from%20scratch%20and%20obtain%20a%20top-5%20accuracy%0Aof%2044.65%25.%20However%2C%20our%20PudNet%20costs%20only%203.89%20GPU%20seconds%20to%20predict%20the%0Anetwork%20parameters%20of%20ResNet-18%20achieving%20comparable%20performance%20%2844.92%25%29%2C%20more%0Athan%202%2C300%20times%20faster%20than%20the%20traditional%20training%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11862v3&entry.124074799=Read"},
{"title": "Cell Morphology-Guided Small Molecule Generation with GFlowNets", "author": "Stephen Zhewen Lu and Ziqing Lu and Ehsan Hajiramezanali and Tommaso Biancalani and Yoshua Bengio and Gabriele Scalia and Micha\u0142 Koziarski", "abstract": "  High-content phenotypic screening, including high-content imaging (HCI), has\ngained popularity in the last few years for its ability to characterize novel\ntherapeutics without prior knowledge of the protein target. When combined with\ndeep learning techniques to predict and represent molecular-phenotype\ninteractions, these advancements hold the potential to significantly accelerate\nand enhance drug discovery applications. This work focuses on the novel task of\nHCI-guided molecular design. Generative models for molecule design could be\nguided by HCI data, for example with a supervised model that links molecules to\nphenotypes of interest as a reward function. However, limited labeled data,\ncombined with the high-dimensional readouts, can make training these methods\nchallenging and impractical. We consider an alternative approach in which we\nleverage an unsupervised multimodal joint embedding to define a latent\nsimilarity as a reward for GFlowNets. The proposed model learns to generate new\nmolecules that could produce phenotypic effects similar to those of the given\nimage target, without relying on pre-annotated phenotypic labels. We\ndemonstrate that the proposed method generates molecules with high\nmorphological and structural similarity to the target, increasing the\nlikelihood of similar biological activity, as confirmed by an independent\noracle model.\n", "link": "http://arxiv.org/abs/2408.05196v1", "date": "2024-08-09", "relevancy": 2.1209, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5432}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5299}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cell%20Morphology-Guided%20Small%20Molecule%20Generation%20with%20GFlowNets&body=Title%3A%20Cell%20Morphology-Guided%20Small%20Molecule%20Generation%20with%20GFlowNets%0AAuthor%3A%20Stephen%20Zhewen%20Lu%20and%20Ziqing%20Lu%20and%20Ehsan%20Hajiramezanali%20and%20Tommaso%20Biancalani%20and%20Yoshua%20Bengio%20and%20Gabriele%20Scalia%20and%20Micha%C5%82%20Koziarski%0AAbstract%3A%20%20%20High-content%20phenotypic%20screening%2C%20including%20high-content%20imaging%20%28HCI%29%2C%20has%0Agained%20popularity%20in%20the%20last%20few%20years%20for%20its%20ability%20to%20characterize%20novel%0Atherapeutics%20without%20prior%20knowledge%20of%20the%20protein%20target.%20When%20combined%20with%0Adeep%20learning%20techniques%20to%20predict%20and%20represent%20molecular-phenotype%0Ainteractions%2C%20these%20advancements%20hold%20the%20potential%20to%20significantly%20accelerate%0Aand%20enhance%20drug%20discovery%20applications.%20This%20work%20focuses%20on%20the%20novel%20task%20of%0AHCI-guided%20molecular%20design.%20Generative%20models%20for%20molecule%20design%20could%20be%0Aguided%20by%20HCI%20data%2C%20for%20example%20with%20a%20supervised%20model%20that%20links%20molecules%20to%0Aphenotypes%20of%20interest%20as%20a%20reward%20function.%20However%2C%20limited%20labeled%20data%2C%0Acombined%20with%20the%20high-dimensional%20readouts%2C%20can%20make%20training%20these%20methods%0Achallenging%20and%20impractical.%20We%20consider%20an%20alternative%20approach%20in%20which%20we%0Aleverage%20an%20unsupervised%20multimodal%20joint%20embedding%20to%20define%20a%20latent%0Asimilarity%20as%20a%20reward%20for%20GFlowNets.%20The%20proposed%20model%20learns%20to%20generate%20new%0Amolecules%20that%20could%20produce%20phenotypic%20effects%20similar%20to%20those%20of%20the%20given%0Aimage%20target%2C%20without%20relying%20on%20pre-annotated%20phenotypic%20labels.%20We%0Ademonstrate%20that%20the%20proposed%20method%20generates%20molecules%20with%20high%0Amorphological%20and%20structural%20similarity%20to%20the%20target%2C%20increasing%20the%0Alikelihood%20of%20similar%20biological%20activity%2C%20as%20confirmed%20by%20an%20independent%0Aoracle%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCell%2520Morphology-Guided%2520Small%2520Molecule%2520Generation%2520with%2520GFlowNets%26entry.906535625%3DStephen%2520Zhewen%2520Lu%2520and%2520Ziqing%2520Lu%2520and%2520Ehsan%2520Hajiramezanali%2520and%2520Tommaso%2520Biancalani%2520and%2520Yoshua%2520Bengio%2520and%2520Gabriele%2520Scalia%2520and%2520Micha%25C5%2582%2520Koziarski%26entry.1292438233%3D%2520%2520High-content%2520phenotypic%2520screening%252C%2520including%2520high-content%2520imaging%2520%2528HCI%2529%252C%2520has%250Agained%2520popularity%2520in%2520the%2520last%2520few%2520years%2520for%2520its%2520ability%2520to%2520characterize%2520novel%250Atherapeutics%2520without%2520prior%2520knowledge%2520of%2520the%2520protein%2520target.%2520When%2520combined%2520with%250Adeep%2520learning%2520techniques%2520to%2520predict%2520and%2520represent%2520molecular-phenotype%250Ainteractions%252C%2520these%2520advancements%2520hold%2520the%2520potential%2520to%2520significantly%2520accelerate%250Aand%2520enhance%2520drug%2520discovery%2520applications.%2520This%2520work%2520focuses%2520on%2520the%2520novel%2520task%2520of%250AHCI-guided%2520molecular%2520design.%2520Generative%2520models%2520for%2520molecule%2520design%2520could%2520be%250Aguided%2520by%2520HCI%2520data%252C%2520for%2520example%2520with%2520a%2520supervised%2520model%2520that%2520links%2520molecules%2520to%250Aphenotypes%2520of%2520interest%2520as%2520a%2520reward%2520function.%2520However%252C%2520limited%2520labeled%2520data%252C%250Acombined%2520with%2520the%2520high-dimensional%2520readouts%252C%2520can%2520make%2520training%2520these%2520methods%250Achallenging%2520and%2520impractical.%2520We%2520consider%2520an%2520alternative%2520approach%2520in%2520which%2520we%250Aleverage%2520an%2520unsupervised%2520multimodal%2520joint%2520embedding%2520to%2520define%2520a%2520latent%250Asimilarity%2520as%2520a%2520reward%2520for%2520GFlowNets.%2520The%2520proposed%2520model%2520learns%2520to%2520generate%2520new%250Amolecules%2520that%2520could%2520produce%2520phenotypic%2520effects%2520similar%2520to%2520those%2520of%2520the%2520given%250Aimage%2520target%252C%2520without%2520relying%2520on%2520pre-annotated%2520phenotypic%2520labels.%2520We%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520generates%2520molecules%2520with%2520high%250Amorphological%2520and%2520structural%2520similarity%2520to%2520the%2520target%252C%2520increasing%2520the%250Alikelihood%2520of%2520similar%2520biological%2520activity%252C%2520as%2520confirmed%2520by%2520an%2520independent%250Aoracle%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cell%20Morphology-Guided%20Small%20Molecule%20Generation%20with%20GFlowNets&entry.906535625=Stephen%20Zhewen%20Lu%20and%20Ziqing%20Lu%20and%20Ehsan%20Hajiramezanali%20and%20Tommaso%20Biancalani%20and%20Yoshua%20Bengio%20and%20Gabriele%20Scalia%20and%20Micha%C5%82%20Koziarski&entry.1292438233=%20%20High-content%20phenotypic%20screening%2C%20including%20high-content%20imaging%20%28HCI%29%2C%20has%0Agained%20popularity%20in%20the%20last%20few%20years%20for%20its%20ability%20to%20characterize%20novel%0Atherapeutics%20without%20prior%20knowledge%20of%20the%20protein%20target.%20When%20combined%20with%0Adeep%20learning%20techniques%20to%20predict%20and%20represent%20molecular-phenotype%0Ainteractions%2C%20these%20advancements%20hold%20the%20potential%20to%20significantly%20accelerate%0Aand%20enhance%20drug%20discovery%20applications.%20This%20work%20focuses%20on%20the%20novel%20task%20of%0AHCI-guided%20molecular%20design.%20Generative%20models%20for%20molecule%20design%20could%20be%0Aguided%20by%20HCI%20data%2C%20for%20example%20with%20a%20supervised%20model%20that%20links%20molecules%20to%0Aphenotypes%20of%20interest%20as%20a%20reward%20function.%20However%2C%20limited%20labeled%20data%2C%0Acombined%20with%20the%20high-dimensional%20readouts%2C%20can%20make%20training%20these%20methods%0Achallenging%20and%20impractical.%20We%20consider%20an%20alternative%20approach%20in%20which%20we%0Aleverage%20an%20unsupervised%20multimodal%20joint%20embedding%20to%20define%20a%20latent%0Asimilarity%20as%20a%20reward%20for%20GFlowNets.%20The%20proposed%20model%20learns%20to%20generate%20new%0Amolecules%20that%20could%20produce%20phenotypic%20effects%20similar%20to%20those%20of%20the%20given%0Aimage%20target%2C%20without%20relying%20on%20pre-annotated%20phenotypic%20labels.%20We%0Ademonstrate%20that%20the%20proposed%20method%20generates%20molecules%20with%20high%0Amorphological%20and%20structural%20similarity%20to%20the%20target%2C%20increasing%20the%0Alikelihood%20of%20similar%20biological%20activity%2C%20as%20confirmed%20by%20an%20independent%0Aoracle%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05196v1&entry.124074799=Read"},
{"title": "EasyInv: Toward Fast and Better DDIM Inversion", "author": "Ziyue Zhang and Mingbao Lin and Shuicheng Yan and Rongrong Ji", "abstract": "  This paper introduces EasyInv, an easy yet novel approach that significantly\nadvances the field of DDIM Inversion by addressing the inherent inefficiencies\nand performance limitations of traditional iterative optimization methods. At\nthe core of our EasyInv is a refined strategy for approximating inversion\nnoise, which is pivotal for enhancing the accuracy and reliability of the\ninversion process. By prioritizing the initial latent state, which encapsulates\nrich information about the original images, EasyInv steers clear of the\niterative refinement of noise items. Instead, we introduce a methodical\naggregation of the latent state from the preceding time step with the current\nstate, effectively increasing the influence of the initial latent state and\nmitigating the impact of noise. We illustrate that EasyInv is capable of\ndelivering results that are either on par with or exceed those of the\nconventional DDIM Inversion approach, especially under conditions where the\nmodel's precision is limited or computational resources are scarce.\nConcurrently, our EasyInv offers an approximate threefold enhancement regarding\ninference efficiency over off-the-shelf iterative optimization techniques.\n", "link": "http://arxiv.org/abs/2408.05159v1", "date": "2024-08-09", "relevancy": 2.1198, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5576}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5326}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyInv%3A%20Toward%20Fast%20and%20Better%20DDIM%20Inversion&body=Title%3A%20EasyInv%3A%20Toward%20Fast%20and%20Better%20DDIM%20Inversion%0AAuthor%3A%20Ziyue%20Zhang%20and%20Mingbao%20Lin%20and%20Shuicheng%20Yan%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20This%20paper%20introduces%20EasyInv%2C%20an%20easy%20yet%20novel%20approach%20that%20significantly%0Aadvances%20the%20field%20of%20DDIM%20Inversion%20by%20addressing%20the%20inherent%20inefficiencies%0Aand%20performance%20limitations%20of%20traditional%20iterative%20optimization%20methods.%20At%0Athe%20core%20of%20our%20EasyInv%20is%20a%20refined%20strategy%20for%20approximating%20inversion%0Anoise%2C%20which%20is%20pivotal%20for%20enhancing%20the%20accuracy%20and%20reliability%20of%20the%0Ainversion%20process.%20By%20prioritizing%20the%20initial%20latent%20state%2C%20which%20encapsulates%0Arich%20information%20about%20the%20original%20images%2C%20EasyInv%20steers%20clear%20of%20the%0Aiterative%20refinement%20of%20noise%20items.%20Instead%2C%20we%20introduce%20a%20methodical%0Aaggregation%20of%20the%20latent%20state%20from%20the%20preceding%20time%20step%20with%20the%20current%0Astate%2C%20effectively%20increasing%20the%20influence%20of%20the%20initial%20latent%20state%20and%0Amitigating%20the%20impact%20of%20noise.%20We%20illustrate%20that%20EasyInv%20is%20capable%20of%0Adelivering%20results%20that%20are%20either%20on%20par%20with%20or%20exceed%20those%20of%20the%0Aconventional%20DDIM%20Inversion%20approach%2C%20especially%20under%20conditions%20where%20the%0Amodel%27s%20precision%20is%20limited%20or%20computational%20resources%20are%20scarce.%0AConcurrently%2C%20our%20EasyInv%20offers%20an%20approximate%20threefold%20enhancement%20regarding%0Ainference%20efficiency%20over%20off-the-shelf%20iterative%20optimization%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyInv%253A%2520Toward%2520Fast%2520and%2520Better%2520DDIM%2520Inversion%26entry.906535625%3DZiyue%2520Zhang%2520and%2520Mingbao%2520Lin%2520and%2520Shuicheng%2520Yan%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520EasyInv%252C%2520an%2520easy%2520yet%2520novel%2520approach%2520that%2520significantly%250Aadvances%2520the%2520field%2520of%2520DDIM%2520Inversion%2520by%2520addressing%2520the%2520inherent%2520inefficiencies%250Aand%2520performance%2520limitations%2520of%2520traditional%2520iterative%2520optimization%2520methods.%2520At%250Athe%2520core%2520of%2520our%2520EasyInv%2520is%2520a%2520refined%2520strategy%2520for%2520approximating%2520inversion%250Anoise%252C%2520which%2520is%2520pivotal%2520for%2520enhancing%2520the%2520accuracy%2520and%2520reliability%2520of%2520the%250Ainversion%2520process.%2520By%2520prioritizing%2520the%2520initial%2520latent%2520state%252C%2520which%2520encapsulates%250Arich%2520information%2520about%2520the%2520original%2520images%252C%2520EasyInv%2520steers%2520clear%2520of%2520the%250Aiterative%2520refinement%2520of%2520noise%2520items.%2520Instead%252C%2520we%2520introduce%2520a%2520methodical%250Aaggregation%2520of%2520the%2520latent%2520state%2520from%2520the%2520preceding%2520time%2520step%2520with%2520the%2520current%250Astate%252C%2520effectively%2520increasing%2520the%2520influence%2520of%2520the%2520initial%2520latent%2520state%2520and%250Amitigating%2520the%2520impact%2520of%2520noise.%2520We%2520illustrate%2520that%2520EasyInv%2520is%2520capable%2520of%250Adelivering%2520results%2520that%2520are%2520either%2520on%2520par%2520with%2520or%2520exceed%2520those%2520of%2520the%250Aconventional%2520DDIM%2520Inversion%2520approach%252C%2520especially%2520under%2520conditions%2520where%2520the%250Amodel%2527s%2520precision%2520is%2520limited%2520or%2520computational%2520resources%2520are%2520scarce.%250AConcurrently%252C%2520our%2520EasyInv%2520offers%2520an%2520approximate%2520threefold%2520enhancement%2520regarding%250Ainference%2520efficiency%2520over%2520off-the-shelf%2520iterative%2520optimization%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyInv%3A%20Toward%20Fast%20and%20Better%20DDIM%20Inversion&entry.906535625=Ziyue%20Zhang%20and%20Mingbao%20Lin%20and%20Shuicheng%20Yan%20and%20Rongrong%20Ji&entry.1292438233=%20%20This%20paper%20introduces%20EasyInv%2C%20an%20easy%20yet%20novel%20approach%20that%20significantly%0Aadvances%20the%20field%20of%20DDIM%20Inversion%20by%20addressing%20the%20inherent%20inefficiencies%0Aand%20performance%20limitations%20of%20traditional%20iterative%20optimization%20methods.%20At%0Athe%20core%20of%20our%20EasyInv%20is%20a%20refined%20strategy%20for%20approximating%20inversion%0Anoise%2C%20which%20is%20pivotal%20for%20enhancing%20the%20accuracy%20and%20reliability%20of%20the%0Ainversion%20process.%20By%20prioritizing%20the%20initial%20latent%20state%2C%20which%20encapsulates%0Arich%20information%20about%20the%20original%20images%2C%20EasyInv%20steers%20clear%20of%20the%0Aiterative%20refinement%20of%20noise%20items.%20Instead%2C%20we%20introduce%20a%20methodical%0Aaggregation%20of%20the%20latent%20state%20from%20the%20preceding%20time%20step%20with%20the%20current%0Astate%2C%20effectively%20increasing%20the%20influence%20of%20the%20initial%20latent%20state%20and%0Amitigating%20the%20impact%20of%20noise.%20We%20illustrate%20that%20EasyInv%20is%20capable%20of%0Adelivering%20results%20that%20are%20either%20on%20par%20with%20or%20exceed%20those%20of%20the%0Aconventional%20DDIM%20Inversion%20approach%2C%20especially%20under%20conditions%20where%20the%0Amodel%27s%20precision%20is%20limited%20or%20computational%20resources%20are%20scarce.%0AConcurrently%2C%20our%20EasyInv%20offers%20an%20approximate%20threefold%20enhancement%20regarding%0Ainference%20efficiency%20over%20off-the-shelf%20iterative%20optimization%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05159v1&entry.124074799=Read"},
{"title": "Integrating Edge Information into Ground Truth for the Segmentation of\n  the Optic Disc and Cup from Fundus Images", "author": "Yoga Sri Varshan V and Hitesh Gupta Kattamuri and Subin Sahayam and Umarani Jayaraman", "abstract": "  Optic disc and cup segmentation helps in the diagnosis of glaucoma,\nmyocardial infarction, and diabetic retinopathy. Most deep learning methods\ndeveloped to perform segmentation tasks are built on top of a U-Net-based model\narchitecture. Nevertheless, U-Net and its variants have a tendency to\nover-segment/ under-segment the required regions of interest. Since the most\nimportant outcome is the value of cup-to-disc ratio and not the segmented\nregions themselves, we are more concerned about the boundaries rather than the\nregions under the boundaries. This makes learning edges important as compared\nto learning the regions. In the proposed work, the authors aim to extract both\nedges of the optic disc and cup from the ground truth using a Laplacian filter.\nNext, edges are reconstructed to obtain an edge ground truth in addition to the\noptic disc-cup ground truth. Utilizing both ground truths, the authors study\nseveral U-Net and its variant architectures with and without optic disc and cup\nedges as target, along with the optic disc-cup ground truth for segmentation.\nThe authors have used the REFUGE benchmark dataset and the Drishti-GS dataset\nto perform the study, and the results are tabulated for the dice and the\nHausdorff distance metrics. In the case of the REFUGE dataset, the optic disc\nmean dice score has improved from 0.7425 to 0.8859 while the mean Hausdorff\ndistance has reduced from 6.5810 to 3.0540 for the baseline U-Net model.\nSimilarly, the optic cup mean dice score has improved from 0.6970 to 0.8639\nwhile the mean Hausdorff distance has reduced from 5.2340 to 2.6323 for the\nsame model. Similar improvement has been observed for the Drishti-GS dataset as\nwell. Compared to the baseline U-Net and its variants (i.e) the Attention U-Net\nand the U-Net++, the models that learn integrated edges along with the optic\ndisc and cup regions performed well in both validation and testing datasets.\n", "link": "http://arxiv.org/abs/2408.05052v1", "date": "2024-08-09", "relevancy": 2.1167, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5465}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Edge%20Information%20into%20Ground%20Truth%20for%20the%20Segmentation%20of%0A%20%20the%20Optic%20Disc%20and%20Cup%20from%20Fundus%20Images&body=Title%3A%20Integrating%20Edge%20Information%20into%20Ground%20Truth%20for%20the%20Segmentation%20of%0A%20%20the%20Optic%20Disc%20and%20Cup%20from%20Fundus%20Images%0AAuthor%3A%20Yoga%20Sri%20Varshan%20V%20and%20Hitesh%20Gupta%20Kattamuri%20and%20Subin%20Sahayam%20and%20Umarani%20Jayaraman%0AAbstract%3A%20%20%20Optic%20disc%20and%20cup%20segmentation%20helps%20in%20the%20diagnosis%20of%20glaucoma%2C%0Amyocardial%20infarction%2C%20and%20diabetic%20retinopathy.%20Most%20deep%20learning%20methods%0Adeveloped%20to%20perform%20segmentation%20tasks%20are%20built%20on%20top%20of%20a%20U-Net-based%20model%0Aarchitecture.%20Nevertheless%2C%20U-Net%20and%20its%20variants%20have%20a%20tendency%20to%0Aover-segment/%20under-segment%20the%20required%20regions%20of%20interest.%20Since%20the%20most%0Aimportant%20outcome%20is%20the%20value%20of%20cup-to-disc%20ratio%20and%20not%20the%20segmented%0Aregions%20themselves%2C%20we%20are%20more%20concerned%20about%20the%20boundaries%20rather%20than%20the%0Aregions%20under%20the%20boundaries.%20This%20makes%20learning%20edges%20important%20as%20compared%0Ato%20learning%20the%20regions.%20In%20the%20proposed%20work%2C%20the%20authors%20aim%20to%20extract%20both%0Aedges%20of%20the%20optic%20disc%20and%20cup%20from%20the%20ground%20truth%20using%20a%20Laplacian%20filter.%0ANext%2C%20edges%20are%20reconstructed%20to%20obtain%20an%20edge%20ground%20truth%20in%20addition%20to%20the%0Aoptic%20disc-cup%20ground%20truth.%20Utilizing%20both%20ground%20truths%2C%20the%20authors%20study%0Aseveral%20U-Net%20and%20its%20variant%20architectures%20with%20and%20without%20optic%20disc%20and%20cup%0Aedges%20as%20target%2C%20along%20with%20the%20optic%20disc-cup%20ground%20truth%20for%20segmentation.%0AThe%20authors%20have%20used%20the%20REFUGE%20benchmark%20dataset%20and%20the%20Drishti-GS%20dataset%0Ato%20perform%20the%20study%2C%20and%20the%20results%20are%20tabulated%20for%20the%20dice%20and%20the%0AHausdorff%20distance%20metrics.%20In%20the%20case%20of%20the%20REFUGE%20dataset%2C%20the%20optic%20disc%0Amean%20dice%20score%20has%20improved%20from%200.7425%20to%200.8859%20while%20the%20mean%20Hausdorff%0Adistance%20has%20reduced%20from%206.5810%20to%203.0540%20for%20the%20baseline%20U-Net%20model.%0ASimilarly%2C%20the%20optic%20cup%20mean%20dice%20score%20has%20improved%20from%200.6970%20to%200.8639%0Awhile%20the%20mean%20Hausdorff%20distance%20has%20reduced%20from%205.2340%20to%202.6323%20for%20the%0Asame%20model.%20Similar%20improvement%20has%20been%20observed%20for%20the%20Drishti-GS%20dataset%20as%0Awell.%20Compared%20to%20the%20baseline%20U-Net%20and%20its%20variants%20%28i.e%29%20the%20Attention%20U-Net%0Aand%20the%20U-Net%2B%2B%2C%20the%20models%20that%20learn%20integrated%20edges%20along%20with%20the%20optic%0Adisc%20and%20cup%20regions%20performed%20well%20in%20both%20validation%20and%20testing%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Edge%2520Information%2520into%2520Ground%2520Truth%2520for%2520the%2520Segmentation%2520of%250A%2520%2520the%2520Optic%2520Disc%2520and%2520Cup%2520from%2520Fundus%2520Images%26entry.906535625%3DYoga%2520Sri%2520Varshan%2520V%2520and%2520Hitesh%2520Gupta%2520Kattamuri%2520and%2520Subin%2520Sahayam%2520and%2520Umarani%2520Jayaraman%26entry.1292438233%3D%2520%2520Optic%2520disc%2520and%2520cup%2520segmentation%2520helps%2520in%2520the%2520diagnosis%2520of%2520glaucoma%252C%250Amyocardial%2520infarction%252C%2520and%2520diabetic%2520retinopathy.%2520Most%2520deep%2520learning%2520methods%250Adeveloped%2520to%2520perform%2520segmentation%2520tasks%2520are%2520built%2520on%2520top%2520of%2520a%2520U-Net-based%2520model%250Aarchitecture.%2520Nevertheless%252C%2520U-Net%2520and%2520its%2520variants%2520have%2520a%2520tendency%2520to%250Aover-segment/%2520under-segment%2520the%2520required%2520regions%2520of%2520interest.%2520Since%2520the%2520most%250Aimportant%2520outcome%2520is%2520the%2520value%2520of%2520cup-to-disc%2520ratio%2520and%2520not%2520the%2520segmented%250Aregions%2520themselves%252C%2520we%2520are%2520more%2520concerned%2520about%2520the%2520boundaries%2520rather%2520than%2520the%250Aregions%2520under%2520the%2520boundaries.%2520This%2520makes%2520learning%2520edges%2520important%2520as%2520compared%250Ato%2520learning%2520the%2520regions.%2520In%2520the%2520proposed%2520work%252C%2520the%2520authors%2520aim%2520to%2520extract%2520both%250Aedges%2520of%2520the%2520optic%2520disc%2520and%2520cup%2520from%2520the%2520ground%2520truth%2520using%2520a%2520Laplacian%2520filter.%250ANext%252C%2520edges%2520are%2520reconstructed%2520to%2520obtain%2520an%2520edge%2520ground%2520truth%2520in%2520addition%2520to%2520the%250Aoptic%2520disc-cup%2520ground%2520truth.%2520Utilizing%2520both%2520ground%2520truths%252C%2520the%2520authors%2520study%250Aseveral%2520U-Net%2520and%2520its%2520variant%2520architectures%2520with%2520and%2520without%2520optic%2520disc%2520and%2520cup%250Aedges%2520as%2520target%252C%2520along%2520with%2520the%2520optic%2520disc-cup%2520ground%2520truth%2520for%2520segmentation.%250AThe%2520authors%2520have%2520used%2520the%2520REFUGE%2520benchmark%2520dataset%2520and%2520the%2520Drishti-GS%2520dataset%250Ato%2520perform%2520the%2520study%252C%2520and%2520the%2520results%2520are%2520tabulated%2520for%2520the%2520dice%2520and%2520the%250AHausdorff%2520distance%2520metrics.%2520In%2520the%2520case%2520of%2520the%2520REFUGE%2520dataset%252C%2520the%2520optic%2520disc%250Amean%2520dice%2520score%2520has%2520improved%2520from%25200.7425%2520to%25200.8859%2520while%2520the%2520mean%2520Hausdorff%250Adistance%2520has%2520reduced%2520from%25206.5810%2520to%25203.0540%2520for%2520the%2520baseline%2520U-Net%2520model.%250ASimilarly%252C%2520the%2520optic%2520cup%2520mean%2520dice%2520score%2520has%2520improved%2520from%25200.6970%2520to%25200.8639%250Awhile%2520the%2520mean%2520Hausdorff%2520distance%2520has%2520reduced%2520from%25205.2340%2520to%25202.6323%2520for%2520the%250Asame%2520model.%2520Similar%2520improvement%2520has%2520been%2520observed%2520for%2520the%2520Drishti-GS%2520dataset%2520as%250Awell.%2520Compared%2520to%2520the%2520baseline%2520U-Net%2520and%2520its%2520variants%2520%2528i.e%2529%2520the%2520Attention%2520U-Net%250Aand%2520the%2520U-Net%252B%252B%252C%2520the%2520models%2520that%2520learn%2520integrated%2520edges%2520along%2520with%2520the%2520optic%250Adisc%2520and%2520cup%2520regions%2520performed%2520well%2520in%2520both%2520validation%2520and%2520testing%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Edge%20Information%20into%20Ground%20Truth%20for%20the%20Segmentation%20of%0A%20%20the%20Optic%20Disc%20and%20Cup%20from%20Fundus%20Images&entry.906535625=Yoga%20Sri%20Varshan%20V%20and%20Hitesh%20Gupta%20Kattamuri%20and%20Subin%20Sahayam%20and%20Umarani%20Jayaraman&entry.1292438233=%20%20Optic%20disc%20and%20cup%20segmentation%20helps%20in%20the%20diagnosis%20of%20glaucoma%2C%0Amyocardial%20infarction%2C%20and%20diabetic%20retinopathy.%20Most%20deep%20learning%20methods%0Adeveloped%20to%20perform%20segmentation%20tasks%20are%20built%20on%20top%20of%20a%20U-Net-based%20model%0Aarchitecture.%20Nevertheless%2C%20U-Net%20and%20its%20variants%20have%20a%20tendency%20to%0Aover-segment/%20under-segment%20the%20required%20regions%20of%20interest.%20Since%20the%20most%0Aimportant%20outcome%20is%20the%20value%20of%20cup-to-disc%20ratio%20and%20not%20the%20segmented%0Aregions%20themselves%2C%20we%20are%20more%20concerned%20about%20the%20boundaries%20rather%20than%20the%0Aregions%20under%20the%20boundaries.%20This%20makes%20learning%20edges%20important%20as%20compared%0Ato%20learning%20the%20regions.%20In%20the%20proposed%20work%2C%20the%20authors%20aim%20to%20extract%20both%0Aedges%20of%20the%20optic%20disc%20and%20cup%20from%20the%20ground%20truth%20using%20a%20Laplacian%20filter.%0ANext%2C%20edges%20are%20reconstructed%20to%20obtain%20an%20edge%20ground%20truth%20in%20addition%20to%20the%0Aoptic%20disc-cup%20ground%20truth.%20Utilizing%20both%20ground%20truths%2C%20the%20authors%20study%0Aseveral%20U-Net%20and%20its%20variant%20architectures%20with%20and%20without%20optic%20disc%20and%20cup%0Aedges%20as%20target%2C%20along%20with%20the%20optic%20disc-cup%20ground%20truth%20for%20segmentation.%0AThe%20authors%20have%20used%20the%20REFUGE%20benchmark%20dataset%20and%20the%20Drishti-GS%20dataset%0Ato%20perform%20the%20study%2C%20and%20the%20results%20are%20tabulated%20for%20the%20dice%20and%20the%0AHausdorff%20distance%20metrics.%20In%20the%20case%20of%20the%20REFUGE%20dataset%2C%20the%20optic%20disc%0Amean%20dice%20score%20has%20improved%20from%200.7425%20to%200.8859%20while%20the%20mean%20Hausdorff%0Adistance%20has%20reduced%20from%206.5810%20to%203.0540%20for%20the%20baseline%20U-Net%20model.%0ASimilarly%2C%20the%20optic%20cup%20mean%20dice%20score%20has%20improved%20from%200.6970%20to%200.8639%0Awhile%20the%20mean%20Hausdorff%20distance%20has%20reduced%20from%205.2340%20to%202.6323%20for%20the%0Asame%20model.%20Similar%20improvement%20has%20been%20observed%20for%20the%20Drishti-GS%20dataset%20as%0Awell.%20Compared%20to%20the%20baseline%20U-Net%20and%20its%20variants%20%28i.e%29%20the%20Attention%20U-Net%0Aand%20the%20U-Net%2B%2B%2C%20the%20models%20that%20learn%20integrated%20edges%20along%20with%20the%20optic%0Adisc%20and%20cup%20regions%20performed%20well%20in%20both%20validation%20and%20testing%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05052v1&entry.124074799=Read"},
{"title": "Tensor Frames -- How To Make Any Message Passing Network Equivariant", "author": "Peter Lippmann and Gerrit Gerhartz and Roman Remme and Fred A. Hamprecht", "abstract": "  In many applications of geometric deep learning, the choice of global\ncoordinate frame is arbitrary, and predictions should be independent of the\nreference frame. In other words, the network should be equivariant with respect\nto rotations and reflections of the input, i.e., the transformations of O(d).\nWe present a novel framework for building equivariant message passing\narchitectures and modifying existing non-equivariant architectures to be\nequivariant. Our approach is based on local coordinate frames, between which\ngeometric information is communicated consistently by including tensorial\nobjects in the messages. Our framework can be applied to message passing on\ngeometric data in arbitrary dimensional Euclidean space. While many other\napproaches for equivariant message passing require specialized building blocks,\nsuch as non-standard normalization layers or non-linearities, our approach can\nbe adapted straightforwardly to any existing architecture without such\nmodifications. We explicitly demonstrate the benefit of O(3)-equivariance for a\npopular point cloud architecture and produce state-of-the-art results on normal\nvector regression on point clouds.\n", "link": "http://arxiv.org/abs/2405.15389v2", "date": "2024-08-09", "relevancy": 2.1166, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5443}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5272}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensor%20Frames%20--%20How%20To%20Make%20Any%20Message%20Passing%20Network%20Equivariant&body=Title%3A%20Tensor%20Frames%20--%20How%20To%20Make%20Any%20Message%20Passing%20Network%20Equivariant%0AAuthor%3A%20Peter%20Lippmann%20and%20Gerrit%20Gerhartz%20and%20Roman%20Remme%20and%20Fred%20A.%20Hamprecht%0AAbstract%3A%20%20%20In%20many%20applications%20of%20geometric%20deep%20learning%2C%20the%20choice%20of%20global%0Acoordinate%20frame%20is%20arbitrary%2C%20and%20predictions%20should%20be%20independent%20of%20the%0Areference%20frame.%20In%20other%20words%2C%20the%20network%20should%20be%20equivariant%20with%20respect%0Ato%20rotations%20and%20reflections%20of%20the%20input%2C%20i.e.%2C%20the%20transformations%20of%20O%28d%29.%0AWe%20present%20a%20novel%20framework%20for%20building%20equivariant%20message%20passing%0Aarchitectures%20and%20modifying%20existing%20non-equivariant%20architectures%20to%20be%0Aequivariant.%20Our%20approach%20is%20based%20on%20local%20coordinate%20frames%2C%20between%20which%0Ageometric%20information%20is%20communicated%20consistently%20by%20including%20tensorial%0Aobjects%20in%20the%20messages.%20Our%20framework%20can%20be%20applied%20to%20message%20passing%20on%0Ageometric%20data%20in%20arbitrary%20dimensional%20Euclidean%20space.%20While%20many%20other%0Aapproaches%20for%20equivariant%20message%20passing%20require%20specialized%20building%20blocks%2C%0Asuch%20as%20non-standard%20normalization%20layers%20or%20non-linearities%2C%20our%20approach%20can%0Abe%20adapted%20straightforwardly%20to%20any%20existing%20architecture%20without%20such%0Amodifications.%20We%20explicitly%20demonstrate%20the%20benefit%20of%20O%283%29-equivariance%20for%20a%0Apopular%20point%20cloud%20architecture%20and%20produce%20state-of-the-art%20results%20on%20normal%0Avector%20regression%20on%20point%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensor%2520Frames%2520--%2520How%2520To%2520Make%2520Any%2520Message%2520Passing%2520Network%2520Equivariant%26entry.906535625%3DPeter%2520Lippmann%2520and%2520Gerrit%2520Gerhartz%2520and%2520Roman%2520Remme%2520and%2520Fred%2520A.%2520Hamprecht%26entry.1292438233%3D%2520%2520In%2520many%2520applications%2520of%2520geometric%2520deep%2520learning%252C%2520the%2520choice%2520of%2520global%250Acoordinate%2520frame%2520is%2520arbitrary%252C%2520and%2520predictions%2520should%2520be%2520independent%2520of%2520the%250Areference%2520frame.%2520In%2520other%2520words%252C%2520the%2520network%2520should%2520be%2520equivariant%2520with%2520respect%250Ato%2520rotations%2520and%2520reflections%2520of%2520the%2520input%252C%2520i.e.%252C%2520the%2520transformations%2520of%2520O%2528d%2529.%250AWe%2520present%2520a%2520novel%2520framework%2520for%2520building%2520equivariant%2520message%2520passing%250Aarchitectures%2520and%2520modifying%2520existing%2520non-equivariant%2520architectures%2520to%2520be%250Aequivariant.%2520Our%2520approach%2520is%2520based%2520on%2520local%2520coordinate%2520frames%252C%2520between%2520which%250Ageometric%2520information%2520is%2520communicated%2520consistently%2520by%2520including%2520tensorial%250Aobjects%2520in%2520the%2520messages.%2520Our%2520framework%2520can%2520be%2520applied%2520to%2520message%2520passing%2520on%250Ageometric%2520data%2520in%2520arbitrary%2520dimensional%2520Euclidean%2520space.%2520While%2520many%2520other%250Aapproaches%2520for%2520equivariant%2520message%2520passing%2520require%2520specialized%2520building%2520blocks%252C%250Asuch%2520as%2520non-standard%2520normalization%2520layers%2520or%2520non-linearities%252C%2520our%2520approach%2520can%250Abe%2520adapted%2520straightforwardly%2520to%2520any%2520existing%2520architecture%2520without%2520such%250Amodifications.%2520We%2520explicitly%2520demonstrate%2520the%2520benefit%2520of%2520O%25283%2529-equivariance%2520for%2520a%250Apopular%2520point%2520cloud%2520architecture%2520and%2520produce%2520state-of-the-art%2520results%2520on%2520normal%250Avector%2520regression%2520on%2520point%2520clouds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensor%20Frames%20--%20How%20To%20Make%20Any%20Message%20Passing%20Network%20Equivariant&entry.906535625=Peter%20Lippmann%20and%20Gerrit%20Gerhartz%20and%20Roman%20Remme%20and%20Fred%20A.%20Hamprecht&entry.1292438233=%20%20In%20many%20applications%20of%20geometric%20deep%20learning%2C%20the%20choice%20of%20global%0Acoordinate%20frame%20is%20arbitrary%2C%20and%20predictions%20should%20be%20independent%20of%20the%0Areference%20frame.%20In%20other%20words%2C%20the%20network%20should%20be%20equivariant%20with%20respect%0Ato%20rotations%20and%20reflections%20of%20the%20input%2C%20i.e.%2C%20the%20transformations%20of%20O%28d%29.%0AWe%20present%20a%20novel%20framework%20for%20building%20equivariant%20message%20passing%0Aarchitectures%20and%20modifying%20existing%20non-equivariant%20architectures%20to%20be%0Aequivariant.%20Our%20approach%20is%20based%20on%20local%20coordinate%20frames%2C%20between%20which%0Ageometric%20information%20is%20communicated%20consistently%20by%20including%20tensorial%0Aobjects%20in%20the%20messages.%20Our%20framework%20can%20be%20applied%20to%20message%20passing%20on%0Ageometric%20data%20in%20arbitrary%20dimensional%20Euclidean%20space.%20While%20many%20other%0Aapproaches%20for%20equivariant%20message%20passing%20require%20specialized%20building%20blocks%2C%0Asuch%20as%20non-standard%20normalization%20layers%20or%20non-linearities%2C%20our%20approach%20can%0Abe%20adapted%20straightforwardly%20to%20any%20existing%20architecture%20without%20such%0Amodifications.%20We%20explicitly%20demonstrate%20the%20benefit%20of%20O%283%29-equivariance%20for%20a%0Apopular%20point%20cloud%20architecture%20and%20produce%20state-of-the-art%20results%20on%20normal%0Avector%20regression%20on%20point%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15389v2&entry.124074799=Read"},
{"title": "Generalizing Few Data to Unseen Domains Flexibly Based on Label\n  Smoothing Integrated with Distributionally Robust Optimization", "author": "Yangdi Wang and Zhi-Hai Zhang and Su Xiu Xu and Wenming Guo", "abstract": "  Overfitting commonly occurs when applying deep neural networks (DNNs) on\nsmall-scale datasets, where DNNs do not generalize well from existing data to\nunseen data. The main reason resulting in overfitting is that small-scale\ndatasets cannot reflect the situations of the real world. Label smoothing (LS)\nis an effective regularization method to prevent overfitting, avoiding it by\nmixing one-hot labels with uniform label vectors. However, LS only focuses on\nlabels while ignoring the distribution of existing data. In this paper, we\nintroduce the distributionally robust optimization (DRO) to LS, achieving shift\nthe existing data distribution flexibly to unseen domains when training DNNs.\nSpecifically, we prove that the regularization of LS can be extended to a\nregularization term for the DNNs parameters when integrating DRO. The\nregularization term can be utilized to shift existing data to unseen domains\nand generate new data. Furthermore, we propose an approximate\ngradient-iteration label smoothing algorithm (GI-LS) to achieve the findings\nand train DNNs. We prove that the shift for the existing data does not\ninfluence the convergence of GI-LS. Since GI-LS incorporates a series of\nhyperparameters, we further consider using Bayesian optimization (BO) to find\nthe relatively optimal combinations of these hyperparameters. Taking\nsmall-scale anomaly classification tasks as a case, we evaluate GI-LS, and the\nresults clearly demonstrate its superior performance.\n", "link": "http://arxiv.org/abs/2408.05082v1", "date": "2024-08-09", "relevancy": 2.0802, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5251}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5175}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizing%20Few%20Data%20to%20Unseen%20Domains%20Flexibly%20Based%20on%20Label%0A%20%20Smoothing%20Integrated%20with%20Distributionally%20Robust%20Optimization&body=Title%3A%20Generalizing%20Few%20Data%20to%20Unseen%20Domains%20Flexibly%20Based%20on%20Label%0A%20%20Smoothing%20Integrated%20with%20Distributionally%20Robust%20Optimization%0AAuthor%3A%20Yangdi%20Wang%20and%20Zhi-Hai%20Zhang%20and%20Su%20Xiu%20Xu%20and%20Wenming%20Guo%0AAbstract%3A%20%20%20Overfitting%20commonly%20occurs%20when%20applying%20deep%20neural%20networks%20%28DNNs%29%20on%0Asmall-scale%20datasets%2C%20where%20DNNs%20do%20not%20generalize%20well%20from%20existing%20data%20to%0Aunseen%20data.%20The%20main%20reason%20resulting%20in%20overfitting%20is%20that%20small-scale%0Adatasets%20cannot%20reflect%20the%20situations%20of%20the%20real%20world.%20Label%20smoothing%20%28LS%29%0Ais%20an%20effective%20regularization%20method%20to%20prevent%20overfitting%2C%20avoiding%20it%20by%0Amixing%20one-hot%20labels%20with%20uniform%20label%20vectors.%20However%2C%20LS%20only%20focuses%20on%0Alabels%20while%20ignoring%20the%20distribution%20of%20existing%20data.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20distributionally%20robust%20optimization%20%28DRO%29%20to%20LS%2C%20achieving%20shift%0Athe%20existing%20data%20distribution%20flexibly%20to%20unseen%20domains%20when%20training%20DNNs.%0ASpecifically%2C%20we%20prove%20that%20the%20regularization%20of%20LS%20can%20be%20extended%20to%20a%0Aregularization%20term%20for%20the%20DNNs%20parameters%20when%20integrating%20DRO.%20The%0Aregularization%20term%20can%20be%20utilized%20to%20shift%20existing%20data%20to%20unseen%20domains%0Aand%20generate%20new%20data.%20Furthermore%2C%20we%20propose%20an%20approximate%0Agradient-iteration%20label%20smoothing%20algorithm%20%28GI-LS%29%20to%20achieve%20the%20findings%0Aand%20train%20DNNs.%20We%20prove%20that%20the%20shift%20for%20the%20existing%20data%20does%20not%0Ainfluence%20the%20convergence%20of%20GI-LS.%20Since%20GI-LS%20incorporates%20a%20series%20of%0Ahyperparameters%2C%20we%20further%20consider%20using%20Bayesian%20optimization%20%28BO%29%20to%20find%0Athe%20relatively%20optimal%20combinations%20of%20these%20hyperparameters.%20Taking%0Asmall-scale%20anomaly%20classification%20tasks%20as%20a%20case%2C%20we%20evaluate%20GI-LS%2C%20and%20the%0Aresults%20clearly%20demonstrate%20its%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizing%2520Few%2520Data%2520to%2520Unseen%2520Domains%2520Flexibly%2520Based%2520on%2520Label%250A%2520%2520Smoothing%2520Integrated%2520with%2520Distributionally%2520Robust%2520Optimization%26entry.906535625%3DYangdi%2520Wang%2520and%2520Zhi-Hai%2520Zhang%2520and%2520Su%2520Xiu%2520Xu%2520and%2520Wenming%2520Guo%26entry.1292438233%3D%2520%2520Overfitting%2520commonly%2520occurs%2520when%2520applying%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520on%250Asmall-scale%2520datasets%252C%2520where%2520DNNs%2520do%2520not%2520generalize%2520well%2520from%2520existing%2520data%2520to%250Aunseen%2520data.%2520The%2520main%2520reason%2520resulting%2520in%2520overfitting%2520is%2520that%2520small-scale%250Adatasets%2520cannot%2520reflect%2520the%2520situations%2520of%2520the%2520real%2520world.%2520Label%2520smoothing%2520%2528LS%2529%250Ais%2520an%2520effective%2520regularization%2520method%2520to%2520prevent%2520overfitting%252C%2520avoiding%2520it%2520by%250Amixing%2520one-hot%2520labels%2520with%2520uniform%2520label%2520vectors.%2520However%252C%2520LS%2520only%2520focuses%2520on%250Alabels%2520while%2520ignoring%2520the%2520distribution%2520of%2520existing%2520data.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520distributionally%2520robust%2520optimization%2520%2528DRO%2529%2520to%2520LS%252C%2520achieving%2520shift%250Athe%2520existing%2520data%2520distribution%2520flexibly%2520to%2520unseen%2520domains%2520when%2520training%2520DNNs.%250ASpecifically%252C%2520we%2520prove%2520that%2520the%2520regularization%2520of%2520LS%2520can%2520be%2520extended%2520to%2520a%250Aregularization%2520term%2520for%2520the%2520DNNs%2520parameters%2520when%2520integrating%2520DRO.%2520The%250Aregularization%2520term%2520can%2520be%2520utilized%2520to%2520shift%2520existing%2520data%2520to%2520unseen%2520domains%250Aand%2520generate%2520new%2520data.%2520Furthermore%252C%2520we%2520propose%2520an%2520approximate%250Agradient-iteration%2520label%2520smoothing%2520algorithm%2520%2528GI-LS%2529%2520to%2520achieve%2520the%2520findings%250Aand%2520train%2520DNNs.%2520We%2520prove%2520that%2520the%2520shift%2520for%2520the%2520existing%2520data%2520does%2520not%250Ainfluence%2520the%2520convergence%2520of%2520GI-LS.%2520Since%2520GI-LS%2520incorporates%2520a%2520series%2520of%250Ahyperparameters%252C%2520we%2520further%2520consider%2520using%2520Bayesian%2520optimization%2520%2528BO%2529%2520to%2520find%250Athe%2520relatively%2520optimal%2520combinations%2520of%2520these%2520hyperparameters.%2520Taking%250Asmall-scale%2520anomaly%2520classification%2520tasks%2520as%2520a%2520case%252C%2520we%2520evaluate%2520GI-LS%252C%2520and%2520the%250Aresults%2520clearly%2520demonstrate%2520its%2520superior%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20Few%20Data%20to%20Unseen%20Domains%20Flexibly%20Based%20on%20Label%0A%20%20Smoothing%20Integrated%20with%20Distributionally%20Robust%20Optimization&entry.906535625=Yangdi%20Wang%20and%20Zhi-Hai%20Zhang%20and%20Su%20Xiu%20Xu%20and%20Wenming%20Guo&entry.1292438233=%20%20Overfitting%20commonly%20occurs%20when%20applying%20deep%20neural%20networks%20%28DNNs%29%20on%0Asmall-scale%20datasets%2C%20where%20DNNs%20do%20not%20generalize%20well%20from%20existing%20data%20to%0Aunseen%20data.%20The%20main%20reason%20resulting%20in%20overfitting%20is%20that%20small-scale%0Adatasets%20cannot%20reflect%20the%20situations%20of%20the%20real%20world.%20Label%20smoothing%20%28LS%29%0Ais%20an%20effective%20regularization%20method%20to%20prevent%20overfitting%2C%20avoiding%20it%20by%0Amixing%20one-hot%20labels%20with%20uniform%20label%20vectors.%20However%2C%20LS%20only%20focuses%20on%0Alabels%20while%20ignoring%20the%20distribution%20of%20existing%20data.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20distributionally%20robust%20optimization%20%28DRO%29%20to%20LS%2C%20achieving%20shift%0Athe%20existing%20data%20distribution%20flexibly%20to%20unseen%20domains%20when%20training%20DNNs.%0ASpecifically%2C%20we%20prove%20that%20the%20regularization%20of%20LS%20can%20be%20extended%20to%20a%0Aregularization%20term%20for%20the%20DNNs%20parameters%20when%20integrating%20DRO.%20The%0Aregularization%20term%20can%20be%20utilized%20to%20shift%20existing%20data%20to%20unseen%20domains%0Aand%20generate%20new%20data.%20Furthermore%2C%20we%20propose%20an%20approximate%0Agradient-iteration%20label%20smoothing%20algorithm%20%28GI-LS%29%20to%20achieve%20the%20findings%0Aand%20train%20DNNs.%20We%20prove%20that%20the%20shift%20for%20the%20existing%20data%20does%20not%0Ainfluence%20the%20convergence%20of%20GI-LS.%20Since%20GI-LS%20incorporates%20a%20series%20of%0Ahyperparameters%2C%20we%20further%20consider%20using%20Bayesian%20optimization%20%28BO%29%20to%20find%0Athe%20relatively%20optimal%20combinations%20of%20these%20hyperparameters.%20Taking%0Asmall-scale%20anomaly%20classification%20tasks%20as%20a%20case%2C%20we%20evaluate%20GI-LS%2C%20and%20the%0Aresults%20clearly%20demonstrate%20its%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05082v1&entry.124074799=Read"},
{"title": "A conformalized learning of a prediction set with applications to\n  medical imaging classification", "author": "Roy Hirsch and Jacob Goldberger", "abstract": "  Medical imaging classifiers can achieve high predictive accuracy, but\nquantifying their uncertainty remains an unresolved challenge, which prevents\ntheir deployment in medical clinics. We present an algorithm that can modify\nany classifier to produce a prediction set containing the true label with a\nuser-specified probability, such as 90%. We train a network to predict an\ninstance-based version of the Conformal Prediction threshold. The threshold is\nthen conformalized to ensure the required coverage. We applied the proposed\nalgorithm to several standard medical imaging classification datasets. The\nexperimental results demonstrate that our method outperforms current approaches\nin terms of smaller average size of the prediction set while maintaining the\ndesired coverage.\n", "link": "http://arxiv.org/abs/2408.05037v1", "date": "2024-08-09", "relevancy": 2.0768, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5297}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.522}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20conformalized%20learning%20of%20a%20prediction%20set%20with%20applications%20to%0A%20%20medical%20imaging%20classification&body=Title%3A%20A%20conformalized%20learning%20of%20a%20prediction%20set%20with%20applications%20to%0A%20%20medical%20imaging%20classification%0AAuthor%3A%20Roy%20Hirsch%20and%20Jacob%20Goldberger%0AAbstract%3A%20%20%20Medical%20imaging%20classifiers%20can%20achieve%20high%20predictive%20accuracy%2C%20but%0Aquantifying%20their%20uncertainty%20remains%20an%20unresolved%20challenge%2C%20which%20prevents%0Atheir%20deployment%20in%20medical%20clinics.%20We%20present%20an%20algorithm%20that%20can%20modify%0Aany%20classifier%20to%20produce%20a%20prediction%20set%20containing%20the%20true%20label%20with%20a%0Auser-specified%20probability%2C%20such%20as%2090%25.%20We%20train%20a%20network%20to%20predict%20an%0Ainstance-based%20version%20of%20the%20Conformal%20Prediction%20threshold.%20The%20threshold%20is%0Athen%20conformalized%20to%20ensure%20the%20required%20coverage.%20We%20applied%20the%20proposed%0Aalgorithm%20to%20several%20standard%20medical%20imaging%20classification%20datasets.%20The%0Aexperimental%20results%20demonstrate%20that%20our%20method%20outperforms%20current%20approaches%0Ain%20terms%20of%20smaller%20average%20size%20of%20the%20prediction%20set%20while%20maintaining%20the%0Adesired%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520conformalized%2520learning%2520of%2520a%2520prediction%2520set%2520with%2520applications%2520to%250A%2520%2520medical%2520imaging%2520classification%26entry.906535625%3DRoy%2520Hirsch%2520and%2520Jacob%2520Goldberger%26entry.1292438233%3D%2520%2520Medical%2520imaging%2520classifiers%2520can%2520achieve%2520high%2520predictive%2520accuracy%252C%2520but%250Aquantifying%2520their%2520uncertainty%2520remains%2520an%2520unresolved%2520challenge%252C%2520which%2520prevents%250Atheir%2520deployment%2520in%2520medical%2520clinics.%2520We%2520present%2520an%2520algorithm%2520that%2520can%2520modify%250Aany%2520classifier%2520to%2520produce%2520a%2520prediction%2520set%2520containing%2520the%2520true%2520label%2520with%2520a%250Auser-specified%2520probability%252C%2520such%2520as%252090%2525.%2520We%2520train%2520a%2520network%2520to%2520predict%2520an%250Ainstance-based%2520version%2520of%2520the%2520Conformal%2520Prediction%2520threshold.%2520The%2520threshold%2520is%250Athen%2520conformalized%2520to%2520ensure%2520the%2520required%2520coverage.%2520We%2520applied%2520the%2520proposed%250Aalgorithm%2520to%2520several%2520standard%2520medical%2520imaging%2520classification%2520datasets.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520current%2520approaches%250Ain%2520terms%2520of%2520smaller%2520average%2520size%2520of%2520the%2520prediction%2520set%2520while%2520maintaining%2520the%250Adesired%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20conformalized%20learning%20of%20a%20prediction%20set%20with%20applications%20to%0A%20%20medical%20imaging%20classification&entry.906535625=Roy%20Hirsch%20and%20Jacob%20Goldberger&entry.1292438233=%20%20Medical%20imaging%20classifiers%20can%20achieve%20high%20predictive%20accuracy%2C%20but%0Aquantifying%20their%20uncertainty%20remains%20an%20unresolved%20challenge%2C%20which%20prevents%0Atheir%20deployment%20in%20medical%20clinics.%20We%20present%20an%20algorithm%20that%20can%20modify%0Aany%20classifier%20to%20produce%20a%20prediction%20set%20containing%20the%20true%20label%20with%20a%0Auser-specified%20probability%2C%20such%20as%2090%25.%20We%20train%20a%20network%20to%20predict%20an%0Ainstance-based%20version%20of%20the%20Conformal%20Prediction%20threshold.%20The%20threshold%20is%0Athen%20conformalized%20to%20ensure%20the%20required%20coverage.%20We%20applied%20the%20proposed%0Aalgorithm%20to%20several%20standard%20medical%20imaging%20classification%20datasets.%20The%0Aexperimental%20results%20demonstrate%20that%20our%20method%20outperforms%20current%20approaches%0Ain%20terms%20of%20smaller%20average%20size%20of%20the%20prediction%20set%20while%20maintaining%20the%0Adesired%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05037v1&entry.124074799=Read"},
{"title": "Meta-Learning Guided Label Noise Distillation for Robust Signal\n  Modulation Classification", "author": "Xiaoyang Hao and Zhixi Feng and Tongqing Peng and Shuyuan Yang", "abstract": "  Automatic modulation classification (AMC) is an effective way to deal with\nphysical layer threats of the internet of things (IoT). However, there is often\nlabel mislabeling in practice, which significantly impacts the performance and\nrobustness of deep neural networks (DNNs). In this paper, we propose a\nmeta-learning guided label noise distillation method for robust AMC.\nSpecifically, a teacher-student heterogeneous network (TSHN) framework is\nproposed to distill and reuse label noise. Based on the idea that labels are\nrepresentations, the teacher network with trusted meta-learning divides and\nconquers untrusted label samples and then guides the student network to learn\nbetter by reassessing and correcting labels. Furthermore, we propose a\nmulti-view signal (MVS) method to further improve the performance of\nhard-to-classify categories with few-shot trusted label samples. Extensive\nexperimental results show that our methods can significantly improve the\nperformance and robustness of signal AMC in various and complex label noise\nscenarios, which is crucial for securing IoT applications.\n", "link": "http://arxiv.org/abs/2408.05151v1", "date": "2024-08-09", "relevancy": 2.069, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5326}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20Guided%20Label%20Noise%20Distillation%20for%20Robust%20Signal%0A%20%20Modulation%20Classification&body=Title%3A%20Meta-Learning%20Guided%20Label%20Noise%20Distillation%20for%20Robust%20Signal%0A%20%20Modulation%20Classification%0AAuthor%3A%20Xiaoyang%20Hao%20and%20Zhixi%20Feng%20and%20Tongqing%20Peng%20and%20Shuyuan%20Yang%0AAbstract%3A%20%20%20Automatic%20modulation%20classification%20%28AMC%29%20is%20an%20effective%20way%20to%20deal%20with%0Aphysical%20layer%20threats%20of%20the%20internet%20of%20things%20%28IoT%29.%20However%2C%20there%20is%20often%0Alabel%20mislabeling%20in%20practice%2C%20which%20significantly%20impacts%20the%20performance%20and%0Arobustness%20of%20deep%20neural%20networks%20%28DNNs%29.%20In%20this%20paper%2C%20we%20propose%20a%0Ameta-learning%20guided%20label%20noise%20distillation%20method%20for%20robust%20AMC.%0ASpecifically%2C%20a%20teacher-student%20heterogeneous%20network%20%28TSHN%29%20framework%20is%0Aproposed%20to%20distill%20and%20reuse%20label%20noise.%20Based%20on%20the%20idea%20that%20labels%20are%0Arepresentations%2C%20the%20teacher%20network%20with%20trusted%20meta-learning%20divides%20and%0Aconquers%20untrusted%20label%20samples%20and%20then%20guides%20the%20student%20network%20to%20learn%0Abetter%20by%20reassessing%20and%20correcting%20labels.%20Furthermore%2C%20we%20propose%20a%0Amulti-view%20signal%20%28MVS%29%20method%20to%20further%20improve%20the%20performance%20of%0Ahard-to-classify%20categories%20with%20few-shot%20trusted%20label%20samples.%20Extensive%0Aexperimental%20results%20show%20that%20our%20methods%20can%20significantly%20improve%20the%0Aperformance%20and%20robustness%20of%20signal%20AMC%20in%20various%20and%20complex%20label%20noise%0Ascenarios%2C%20which%20is%20crucial%20for%20securing%20IoT%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520Guided%2520Label%2520Noise%2520Distillation%2520for%2520Robust%2520Signal%250A%2520%2520Modulation%2520Classification%26entry.906535625%3DXiaoyang%2520Hao%2520and%2520Zhixi%2520Feng%2520and%2520Tongqing%2520Peng%2520and%2520Shuyuan%2520Yang%26entry.1292438233%3D%2520%2520Automatic%2520modulation%2520classification%2520%2528AMC%2529%2520is%2520an%2520effective%2520way%2520to%2520deal%2520with%250Aphysical%2520layer%2520threats%2520of%2520the%2520internet%2520of%2520things%2520%2528IoT%2529.%2520However%252C%2520there%2520is%2520often%250Alabel%2520mislabeling%2520in%2520practice%252C%2520which%2520significantly%2520impacts%2520the%2520performance%2520and%250Arobustness%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Ameta-learning%2520guided%2520label%2520noise%2520distillation%2520method%2520for%2520robust%2520AMC.%250ASpecifically%252C%2520a%2520teacher-student%2520heterogeneous%2520network%2520%2528TSHN%2529%2520framework%2520is%250Aproposed%2520to%2520distill%2520and%2520reuse%2520label%2520noise.%2520Based%2520on%2520the%2520idea%2520that%2520labels%2520are%250Arepresentations%252C%2520the%2520teacher%2520network%2520with%2520trusted%2520meta-learning%2520divides%2520and%250Aconquers%2520untrusted%2520label%2520samples%2520and%2520then%2520guides%2520the%2520student%2520network%2520to%2520learn%250Abetter%2520by%2520reassessing%2520and%2520correcting%2520labels.%2520Furthermore%252C%2520we%2520propose%2520a%250Amulti-view%2520signal%2520%2528MVS%2529%2520method%2520to%2520further%2520improve%2520the%2520performance%2520of%250Ahard-to-classify%2520categories%2520with%2520few-shot%2520trusted%2520label%2520samples.%2520Extensive%250Aexperimental%2520results%2520show%2520that%2520our%2520methods%2520can%2520significantly%2520improve%2520the%250Aperformance%2520and%2520robustness%2520of%2520signal%2520AMC%2520in%2520various%2520and%2520complex%2520label%2520noise%250Ascenarios%252C%2520which%2520is%2520crucial%2520for%2520securing%2520IoT%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20Guided%20Label%20Noise%20Distillation%20for%20Robust%20Signal%0A%20%20Modulation%20Classification&entry.906535625=Xiaoyang%20Hao%20and%20Zhixi%20Feng%20and%20Tongqing%20Peng%20and%20Shuyuan%20Yang&entry.1292438233=%20%20Automatic%20modulation%20classification%20%28AMC%29%20is%20an%20effective%20way%20to%20deal%20with%0Aphysical%20layer%20threats%20of%20the%20internet%20of%20things%20%28IoT%29.%20However%2C%20there%20is%20often%0Alabel%20mislabeling%20in%20practice%2C%20which%20significantly%20impacts%20the%20performance%20and%0Arobustness%20of%20deep%20neural%20networks%20%28DNNs%29.%20In%20this%20paper%2C%20we%20propose%20a%0Ameta-learning%20guided%20label%20noise%20distillation%20method%20for%20robust%20AMC.%0ASpecifically%2C%20a%20teacher-student%20heterogeneous%20network%20%28TSHN%29%20framework%20is%0Aproposed%20to%20distill%20and%20reuse%20label%20noise.%20Based%20on%20the%20idea%20that%20labels%20are%0Arepresentations%2C%20the%20teacher%20network%20with%20trusted%20meta-learning%20divides%20and%0Aconquers%20untrusted%20label%20samples%20and%20then%20guides%20the%20student%20network%20to%20learn%0Abetter%20by%20reassessing%20and%20correcting%20labels.%20Furthermore%2C%20we%20propose%20a%0Amulti-view%20signal%20%28MVS%29%20method%20to%20further%20improve%20the%20performance%20of%0Ahard-to-classify%20categories%20with%20few-shot%20trusted%20label%20samples.%20Extensive%0Aexperimental%20results%20show%20that%20our%20methods%20can%20significantly%20improve%20the%0Aperformance%20and%20robustness%20of%20signal%20AMC%20in%20various%20and%20complex%20label%20noise%0Ascenarios%2C%20which%20is%20crucial%20for%20securing%20IoT%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05151v1&entry.124074799=Read"},
{"title": "Mapping \"Brain Terrain\" Regions on Mars using Deep Learning", "author": "Kyle A. Pearson and Eldar Noe and Daniel Zhao and Alphan Altinok and Alex Morgan", "abstract": "  One of the main objectives of the Mars Exploration Program is to search for\nevidence of past or current life on the planet. To achieve this, Mars\nexploration has been focusing on regions that may have liquid or frozen water.\nA set of critical areas may have seen cycles of ice thawing in the relatively\nrecent past in response to periodic changes in the obliquity of Mars. In this\nwork, we use convolutional neural networks to detect surface regions containing\n\"Brain Coral\" terrain, a landform on Mars whose similarity in morphology and\nscale to sorted stone circles on Earth suggests that it may have formed as a\nconsequence of freeze/thaw cycles. We use large images (~100-1000 megapixels)\nfrom the Mars Reconnaissance Orbiter to search for these landforms at\nresolutions close to a few tens of centimeters per pixel (~25--50 cm). Over\n52,000 images (~28 TB) were searched (~5% of the Martian surface) where we\nfound detections in over 200 images. To expedite the processing we leverage a\nclassifier network (prior to segmentation) in the Fourier domain that can take\nadvantage of JPEG compression by leveraging blocks of coefficients from a\ndiscrete cosine transform in lieu of decoding the entire image at the full\nspatial resolution. The hybrid pipeline approach maintains ~93% accuracy while\ncutting down on ~95% of the total processing time compared to running the\nsegmentation network at the full resolution on every image. The timely\nprocessing of big data sets helps inform mission operations, geologic surveys\nto prioritize candidate landing sites, avoid hazardous areas, or map the\nspatial extent of certain terrain. The segmentation masks and source code are\navailable on Github for the community to explore and build upon.\n", "link": "http://arxiv.org/abs/2311.12292v2", "date": "2024-08-09", "relevancy": 2.0287, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5237}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20%22Brain%20Terrain%22%20Regions%20on%20Mars%20using%20Deep%20Learning&body=Title%3A%20Mapping%20%22Brain%20Terrain%22%20Regions%20on%20Mars%20using%20Deep%20Learning%0AAuthor%3A%20Kyle%20A.%20Pearson%20and%20Eldar%20Noe%20and%20Daniel%20Zhao%20and%20Alphan%20Altinok%20and%20Alex%20Morgan%0AAbstract%3A%20%20%20One%20of%20the%20main%20objectives%20of%20the%20Mars%20Exploration%20Program%20is%20to%20search%20for%0Aevidence%20of%20past%20or%20current%20life%20on%20the%20planet.%20To%20achieve%20this%2C%20Mars%0Aexploration%20has%20been%20focusing%20on%20regions%20that%20may%20have%20liquid%20or%20frozen%20water.%0AA%20set%20of%20critical%20areas%20may%20have%20seen%20cycles%20of%20ice%20thawing%20in%20the%20relatively%0Arecent%20past%20in%20response%20to%20periodic%20changes%20in%20the%20obliquity%20of%20Mars.%20In%20this%0Awork%2C%20we%20use%20convolutional%20neural%20networks%20to%20detect%20surface%20regions%20containing%0A%22Brain%20Coral%22%20terrain%2C%20a%20landform%20on%20Mars%20whose%20similarity%20in%20morphology%20and%0Ascale%20to%20sorted%20stone%20circles%20on%20Earth%20suggests%20that%20it%20may%20have%20formed%20as%20a%0Aconsequence%20of%20freeze/thaw%20cycles.%20We%20use%20large%20images%20%28~100-1000%20megapixels%29%0Afrom%20the%20Mars%20Reconnaissance%20Orbiter%20to%20search%20for%20these%20landforms%20at%0Aresolutions%20close%20to%20a%20few%20tens%20of%20centimeters%20per%20pixel%20%28~25--50%20cm%29.%20Over%0A52%2C000%20images%20%28~28%20TB%29%20were%20searched%20%28~5%25%20of%20the%20Martian%20surface%29%20where%20we%0Afound%20detections%20in%20over%20200%20images.%20To%20expedite%20the%20processing%20we%20leverage%20a%0Aclassifier%20network%20%28prior%20to%20segmentation%29%20in%20the%20Fourier%20domain%20that%20can%20take%0Aadvantage%20of%20JPEG%20compression%20by%20leveraging%20blocks%20of%20coefficients%20from%20a%0Adiscrete%20cosine%20transform%20in%20lieu%20of%20decoding%20the%20entire%20image%20at%20the%20full%0Aspatial%20resolution.%20The%20hybrid%20pipeline%20approach%20maintains%20~93%25%20accuracy%20while%0Acutting%20down%20on%20~95%25%20of%20the%20total%20processing%20time%20compared%20to%20running%20the%0Asegmentation%20network%20at%20the%20full%20resolution%20on%20every%20image.%20The%20timely%0Aprocessing%20of%20big%20data%20sets%20helps%20inform%20mission%20operations%2C%20geologic%20surveys%0Ato%20prioritize%20candidate%20landing%20sites%2C%20avoid%20hazardous%20areas%2C%20or%20map%20the%0Aspatial%20extent%20of%20certain%20terrain.%20The%20segmentation%20masks%20and%20source%20code%20are%0Aavailable%20on%20Github%20for%20the%20community%20to%20explore%20and%20build%20upon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520%2522Brain%2520Terrain%2522%2520Regions%2520on%2520Mars%2520using%2520Deep%2520Learning%26entry.906535625%3DKyle%2520A.%2520Pearson%2520and%2520Eldar%2520Noe%2520and%2520Daniel%2520Zhao%2520and%2520Alphan%2520Altinok%2520and%2520Alex%2520Morgan%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520main%2520objectives%2520of%2520the%2520Mars%2520Exploration%2520Program%2520is%2520to%2520search%2520for%250Aevidence%2520of%2520past%2520or%2520current%2520life%2520on%2520the%2520planet.%2520To%2520achieve%2520this%252C%2520Mars%250Aexploration%2520has%2520been%2520focusing%2520on%2520regions%2520that%2520may%2520have%2520liquid%2520or%2520frozen%2520water.%250AA%2520set%2520of%2520critical%2520areas%2520may%2520have%2520seen%2520cycles%2520of%2520ice%2520thawing%2520in%2520the%2520relatively%250Arecent%2520past%2520in%2520response%2520to%2520periodic%2520changes%2520in%2520the%2520obliquity%2520of%2520Mars.%2520In%2520this%250Awork%252C%2520we%2520use%2520convolutional%2520neural%2520networks%2520to%2520detect%2520surface%2520regions%2520containing%250A%2522Brain%2520Coral%2522%2520terrain%252C%2520a%2520landform%2520on%2520Mars%2520whose%2520similarity%2520in%2520morphology%2520and%250Ascale%2520to%2520sorted%2520stone%2520circles%2520on%2520Earth%2520suggests%2520that%2520it%2520may%2520have%2520formed%2520as%2520a%250Aconsequence%2520of%2520freeze/thaw%2520cycles.%2520We%2520use%2520large%2520images%2520%2528~100-1000%2520megapixels%2529%250Afrom%2520the%2520Mars%2520Reconnaissance%2520Orbiter%2520to%2520search%2520for%2520these%2520landforms%2520at%250Aresolutions%2520close%2520to%2520a%2520few%2520tens%2520of%2520centimeters%2520per%2520pixel%2520%2528~25--50%2520cm%2529.%2520Over%250A52%252C000%2520images%2520%2528~28%2520TB%2529%2520were%2520searched%2520%2528~5%2525%2520of%2520the%2520Martian%2520surface%2529%2520where%2520we%250Afound%2520detections%2520in%2520over%2520200%2520images.%2520To%2520expedite%2520the%2520processing%2520we%2520leverage%2520a%250Aclassifier%2520network%2520%2528prior%2520to%2520segmentation%2529%2520in%2520the%2520Fourier%2520domain%2520that%2520can%2520take%250Aadvantage%2520of%2520JPEG%2520compression%2520by%2520leveraging%2520blocks%2520of%2520coefficients%2520from%2520a%250Adiscrete%2520cosine%2520transform%2520in%2520lieu%2520of%2520decoding%2520the%2520entire%2520image%2520at%2520the%2520full%250Aspatial%2520resolution.%2520The%2520hybrid%2520pipeline%2520approach%2520maintains%2520~93%2525%2520accuracy%2520while%250Acutting%2520down%2520on%2520~95%2525%2520of%2520the%2520total%2520processing%2520time%2520compared%2520to%2520running%2520the%250Asegmentation%2520network%2520at%2520the%2520full%2520resolution%2520on%2520every%2520image.%2520The%2520timely%250Aprocessing%2520of%2520big%2520data%2520sets%2520helps%2520inform%2520mission%2520operations%252C%2520geologic%2520surveys%250Ato%2520prioritize%2520candidate%2520landing%2520sites%252C%2520avoid%2520hazardous%2520areas%252C%2520or%2520map%2520the%250Aspatial%2520extent%2520of%2520certain%2520terrain.%2520The%2520segmentation%2520masks%2520and%2520source%2520code%2520are%250Aavailable%2520on%2520Github%2520for%2520the%2520community%2520to%2520explore%2520and%2520build%2520upon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20%22Brain%20Terrain%22%20Regions%20on%20Mars%20using%20Deep%20Learning&entry.906535625=Kyle%20A.%20Pearson%20and%20Eldar%20Noe%20and%20Daniel%20Zhao%20and%20Alphan%20Altinok%20and%20Alex%20Morgan&entry.1292438233=%20%20One%20of%20the%20main%20objectives%20of%20the%20Mars%20Exploration%20Program%20is%20to%20search%20for%0Aevidence%20of%20past%20or%20current%20life%20on%20the%20planet.%20To%20achieve%20this%2C%20Mars%0Aexploration%20has%20been%20focusing%20on%20regions%20that%20may%20have%20liquid%20or%20frozen%20water.%0AA%20set%20of%20critical%20areas%20may%20have%20seen%20cycles%20of%20ice%20thawing%20in%20the%20relatively%0Arecent%20past%20in%20response%20to%20periodic%20changes%20in%20the%20obliquity%20of%20Mars.%20In%20this%0Awork%2C%20we%20use%20convolutional%20neural%20networks%20to%20detect%20surface%20regions%20containing%0A%22Brain%20Coral%22%20terrain%2C%20a%20landform%20on%20Mars%20whose%20similarity%20in%20morphology%20and%0Ascale%20to%20sorted%20stone%20circles%20on%20Earth%20suggests%20that%20it%20may%20have%20formed%20as%20a%0Aconsequence%20of%20freeze/thaw%20cycles.%20We%20use%20large%20images%20%28~100-1000%20megapixels%29%0Afrom%20the%20Mars%20Reconnaissance%20Orbiter%20to%20search%20for%20these%20landforms%20at%0Aresolutions%20close%20to%20a%20few%20tens%20of%20centimeters%20per%20pixel%20%28~25--50%20cm%29.%20Over%0A52%2C000%20images%20%28~28%20TB%29%20were%20searched%20%28~5%25%20of%20the%20Martian%20surface%29%20where%20we%0Afound%20detections%20in%20over%20200%20images.%20To%20expedite%20the%20processing%20we%20leverage%20a%0Aclassifier%20network%20%28prior%20to%20segmentation%29%20in%20the%20Fourier%20domain%20that%20can%20take%0Aadvantage%20of%20JPEG%20compression%20by%20leveraging%20blocks%20of%20coefficients%20from%20a%0Adiscrete%20cosine%20transform%20in%20lieu%20of%20decoding%20the%20entire%20image%20at%20the%20full%0Aspatial%20resolution.%20The%20hybrid%20pipeline%20approach%20maintains%20~93%25%20accuracy%20while%0Acutting%20down%20on%20~95%25%20of%20the%20total%20processing%20time%20compared%20to%20running%20the%0Asegmentation%20network%20at%20the%20full%20resolution%20on%20every%20image.%20The%20timely%0Aprocessing%20of%20big%20data%20sets%20helps%20inform%20mission%20operations%2C%20geologic%20surveys%0Ato%20prioritize%20candidate%20landing%20sites%2C%20avoid%20hazardous%20areas%2C%20or%20map%20the%0Aspatial%20extent%20of%20certain%20terrain.%20The%20segmentation%20masks%20and%20source%20code%20are%0Aavailable%20on%20Github%20for%20the%20community%20to%20explore%20and%20build%20upon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12292v2&entry.124074799=Read"},
{"title": "Modeling Electromagnetic Signal Injection Attacks on Camera-based Smart\n  Systems: Applications and Mitigation", "author": "Youqian Zhang and Michael Cheung and Chunxi Yang and Xinwei Zhai and Zitong Shen and Xinyu Ji and Eugene Y. Fu and Sze-Yiu Chau and Xiapu Luo", "abstract": "  Numerous safety- or security-critical systems depend on cameras to perceive\ntheir surroundings, further allowing artificial intelligence (AI) to analyze\nthe captured images to make important decisions. However, a concerning attack\nvector has emerged, namely, electromagnetic waves, which pose a threat to the\nintegrity of these systems. Such attacks enable attackers to manipulate the\nimages remotely, leading to incorrect AI decisions, e.g., autonomous vehicles\nmissing detecting obstacles ahead resulting in collisions. The lack of\nunderstanding regarding how different systems react to such attacks poses a\nsignificant security risk. Furthermore, no effective solutions have been\ndemonstrated to mitigate this threat.\n  To address these gaps, we modeled the attacks and developed a simulation\nmethod for generating adversarial images. Through rigorous analysis, we\nconfirmed that the effects of the simulated adversarial images are\nindistinguishable from those from real attacks. This method enables researchers\nand engineers to rapidly assess the susceptibility of various AI vision\napplications to these attacks, without the need for constructing complicated\nattack devices. In our experiments, most of the models demonstrated\nvulnerabilities to these attacks, emphasizing the need to enhance their\nrobustness. Fortunately, our modeling and simulation method serves as a\nstepping stone toward developing more resilient models. We present a pilot\nstudy on adversarial training to improve their robustness against attacks, and\nour results demonstrate a significant improvement by recovering up to 91%\nperformance, offering a promising direction for mitigating this threat.\n", "link": "http://arxiv.org/abs/2408.05124v1", "date": "2024-08-09", "relevancy": 2.0107, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5236}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5046}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Electromagnetic%20Signal%20Injection%20Attacks%20on%20Camera-based%20Smart%0A%20%20Systems%3A%20Applications%20and%20Mitigation&body=Title%3A%20Modeling%20Electromagnetic%20Signal%20Injection%20Attacks%20on%20Camera-based%20Smart%0A%20%20Systems%3A%20Applications%20and%20Mitigation%0AAuthor%3A%20Youqian%20Zhang%20and%20Michael%20Cheung%20and%20Chunxi%20Yang%20and%20Xinwei%20Zhai%20and%20Zitong%20Shen%20and%20Xinyu%20Ji%20and%20Eugene%20Y.%20Fu%20and%20Sze-Yiu%20Chau%20and%20Xiapu%20Luo%0AAbstract%3A%20%20%20Numerous%20safety-%20or%20security-critical%20systems%20depend%20on%20cameras%20to%20perceive%0Atheir%20surroundings%2C%20further%20allowing%20artificial%20intelligence%20%28AI%29%20to%20analyze%0Athe%20captured%20images%20to%20make%20important%20decisions.%20However%2C%20a%20concerning%20attack%0Avector%20has%20emerged%2C%20namely%2C%20electromagnetic%20waves%2C%20which%20pose%20a%20threat%20to%20the%0Aintegrity%20of%20these%20systems.%20Such%20attacks%20enable%20attackers%20to%20manipulate%20the%0Aimages%20remotely%2C%20leading%20to%20incorrect%20AI%20decisions%2C%20e.g.%2C%20autonomous%20vehicles%0Amissing%20detecting%20obstacles%20ahead%20resulting%20in%20collisions.%20The%20lack%20of%0Aunderstanding%20regarding%20how%20different%20systems%20react%20to%20such%20attacks%20poses%20a%0Asignificant%20security%20risk.%20Furthermore%2C%20no%20effective%20solutions%20have%20been%0Ademonstrated%20to%20mitigate%20this%20threat.%0A%20%20To%20address%20these%20gaps%2C%20we%20modeled%20the%20attacks%20and%20developed%20a%20simulation%0Amethod%20for%20generating%20adversarial%20images.%20Through%20rigorous%20analysis%2C%20we%0Aconfirmed%20that%20the%20effects%20of%20the%20simulated%20adversarial%20images%20are%0Aindistinguishable%20from%20those%20from%20real%20attacks.%20This%20method%20enables%20researchers%0Aand%20engineers%20to%20rapidly%20assess%20the%20susceptibility%20of%20various%20AI%20vision%0Aapplications%20to%20these%20attacks%2C%20without%20the%20need%20for%20constructing%20complicated%0Aattack%20devices.%20In%20our%20experiments%2C%20most%20of%20the%20models%20demonstrated%0Avulnerabilities%20to%20these%20attacks%2C%20emphasizing%20the%20need%20to%20enhance%20their%0Arobustness.%20Fortunately%2C%20our%20modeling%20and%20simulation%20method%20serves%20as%20a%0Astepping%20stone%20toward%20developing%20more%20resilient%20models.%20We%20present%20a%20pilot%0Astudy%20on%20adversarial%20training%20to%20improve%20their%20robustness%20against%20attacks%2C%20and%0Aour%20results%20demonstrate%20a%20significant%20improvement%20by%20recovering%20up%20to%2091%25%0Aperformance%2C%20offering%20a%20promising%20direction%20for%20mitigating%20this%20threat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Electromagnetic%2520Signal%2520Injection%2520Attacks%2520on%2520Camera-based%2520Smart%250A%2520%2520Systems%253A%2520Applications%2520and%2520Mitigation%26entry.906535625%3DYouqian%2520Zhang%2520and%2520Michael%2520Cheung%2520and%2520Chunxi%2520Yang%2520and%2520Xinwei%2520Zhai%2520and%2520Zitong%2520Shen%2520and%2520Xinyu%2520Ji%2520and%2520Eugene%2520Y.%2520Fu%2520and%2520Sze-Yiu%2520Chau%2520and%2520Xiapu%2520Luo%26entry.1292438233%3D%2520%2520Numerous%2520safety-%2520or%2520security-critical%2520systems%2520depend%2520on%2520cameras%2520to%2520perceive%250Atheir%2520surroundings%252C%2520further%2520allowing%2520artificial%2520intelligence%2520%2528AI%2529%2520to%2520analyze%250Athe%2520captured%2520images%2520to%2520make%2520important%2520decisions.%2520However%252C%2520a%2520concerning%2520attack%250Avector%2520has%2520emerged%252C%2520namely%252C%2520electromagnetic%2520waves%252C%2520which%2520pose%2520a%2520threat%2520to%2520the%250Aintegrity%2520of%2520these%2520systems.%2520Such%2520attacks%2520enable%2520attackers%2520to%2520manipulate%2520the%250Aimages%2520remotely%252C%2520leading%2520to%2520incorrect%2520AI%2520decisions%252C%2520e.g.%252C%2520autonomous%2520vehicles%250Amissing%2520detecting%2520obstacles%2520ahead%2520resulting%2520in%2520collisions.%2520The%2520lack%2520of%250Aunderstanding%2520regarding%2520how%2520different%2520systems%2520react%2520to%2520such%2520attacks%2520poses%2520a%250Asignificant%2520security%2520risk.%2520Furthermore%252C%2520no%2520effective%2520solutions%2520have%2520been%250Ademonstrated%2520to%2520mitigate%2520this%2520threat.%250A%2520%2520To%2520address%2520these%2520gaps%252C%2520we%2520modeled%2520the%2520attacks%2520and%2520developed%2520a%2520simulation%250Amethod%2520for%2520generating%2520adversarial%2520images.%2520Through%2520rigorous%2520analysis%252C%2520we%250Aconfirmed%2520that%2520the%2520effects%2520of%2520the%2520simulated%2520adversarial%2520images%2520are%250Aindistinguishable%2520from%2520those%2520from%2520real%2520attacks.%2520This%2520method%2520enables%2520researchers%250Aand%2520engineers%2520to%2520rapidly%2520assess%2520the%2520susceptibility%2520of%2520various%2520AI%2520vision%250Aapplications%2520to%2520these%2520attacks%252C%2520without%2520the%2520need%2520for%2520constructing%2520complicated%250Aattack%2520devices.%2520In%2520our%2520experiments%252C%2520most%2520of%2520the%2520models%2520demonstrated%250Avulnerabilities%2520to%2520these%2520attacks%252C%2520emphasizing%2520the%2520need%2520to%2520enhance%2520their%250Arobustness.%2520Fortunately%252C%2520our%2520modeling%2520and%2520simulation%2520method%2520serves%2520as%2520a%250Astepping%2520stone%2520toward%2520developing%2520more%2520resilient%2520models.%2520We%2520present%2520a%2520pilot%250Astudy%2520on%2520adversarial%2520training%2520to%2520improve%2520their%2520robustness%2520against%2520attacks%252C%2520and%250Aour%2520results%2520demonstrate%2520a%2520significant%2520improvement%2520by%2520recovering%2520up%2520to%252091%2525%250Aperformance%252C%2520offering%2520a%2520promising%2520direction%2520for%2520mitigating%2520this%2520threat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Electromagnetic%20Signal%20Injection%20Attacks%20on%20Camera-based%20Smart%0A%20%20Systems%3A%20Applications%20and%20Mitigation&entry.906535625=Youqian%20Zhang%20and%20Michael%20Cheung%20and%20Chunxi%20Yang%20and%20Xinwei%20Zhai%20and%20Zitong%20Shen%20and%20Xinyu%20Ji%20and%20Eugene%20Y.%20Fu%20and%20Sze-Yiu%20Chau%20and%20Xiapu%20Luo&entry.1292438233=%20%20Numerous%20safety-%20or%20security-critical%20systems%20depend%20on%20cameras%20to%20perceive%0Atheir%20surroundings%2C%20further%20allowing%20artificial%20intelligence%20%28AI%29%20to%20analyze%0Athe%20captured%20images%20to%20make%20important%20decisions.%20However%2C%20a%20concerning%20attack%0Avector%20has%20emerged%2C%20namely%2C%20electromagnetic%20waves%2C%20which%20pose%20a%20threat%20to%20the%0Aintegrity%20of%20these%20systems.%20Such%20attacks%20enable%20attackers%20to%20manipulate%20the%0Aimages%20remotely%2C%20leading%20to%20incorrect%20AI%20decisions%2C%20e.g.%2C%20autonomous%20vehicles%0Amissing%20detecting%20obstacles%20ahead%20resulting%20in%20collisions.%20The%20lack%20of%0Aunderstanding%20regarding%20how%20different%20systems%20react%20to%20such%20attacks%20poses%20a%0Asignificant%20security%20risk.%20Furthermore%2C%20no%20effective%20solutions%20have%20been%0Ademonstrated%20to%20mitigate%20this%20threat.%0A%20%20To%20address%20these%20gaps%2C%20we%20modeled%20the%20attacks%20and%20developed%20a%20simulation%0Amethod%20for%20generating%20adversarial%20images.%20Through%20rigorous%20analysis%2C%20we%0Aconfirmed%20that%20the%20effects%20of%20the%20simulated%20adversarial%20images%20are%0Aindistinguishable%20from%20those%20from%20real%20attacks.%20This%20method%20enables%20researchers%0Aand%20engineers%20to%20rapidly%20assess%20the%20susceptibility%20of%20various%20AI%20vision%0Aapplications%20to%20these%20attacks%2C%20without%20the%20need%20for%20constructing%20complicated%0Aattack%20devices.%20In%20our%20experiments%2C%20most%20of%20the%20models%20demonstrated%0Avulnerabilities%20to%20these%20attacks%2C%20emphasizing%20the%20need%20to%20enhance%20their%0Arobustness.%20Fortunately%2C%20our%20modeling%20and%20simulation%20method%20serves%20as%20a%0Astepping%20stone%20toward%20developing%20more%20resilient%20models.%20We%20present%20a%20pilot%0Astudy%20on%20adversarial%20training%20to%20improve%20their%20robustness%20against%20attacks%2C%20and%0Aour%20results%20demonstrate%20a%20significant%20improvement%20by%20recovering%20up%20to%2091%25%0Aperformance%2C%20offering%20a%20promising%20direction%20for%20mitigating%20this%20threat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05124v1&entry.124074799=Read"},
{"title": "PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural\n  Networks", "author": "Yamin Sepehri and Pedram Pad and Pascal Frossard and L. Andrea Dunbar", "abstract": "  The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., face\nimages. In this work, we propose a method to perform the training phase of a\ndeep learning model on both an edge device and a cloud server that prevents\nsensitive content being transmitted to the cloud while retaining the desired\ninformation. The proposed privacy-preserving method uses adversarial early\nexits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial datasets with diverse face\nattributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box and deep\nreconstruction attacks.\n", "link": "http://arxiv.org/abs/2408.05092v1", "date": "2024-08-09", "relevancy": 2.0049, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5176}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4917}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PriPHiT%3A%20Privacy-Preserving%20Hierarchical%20Training%20of%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20PriPHiT%3A%20Privacy-Preserving%20Hierarchical%20Training%20of%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Yamin%20Sepehri%20and%20Pedram%20Pad%20and%20Pascal%20Frossard%20and%20L.%20Andrea%20Dunbar%0AAbstract%3A%20%20%20The%20training%20phase%20of%20deep%20neural%20networks%20requires%20substantial%20resources%20and%0Aas%20such%20is%20often%20performed%20on%20cloud%20servers.%20However%2C%20this%20raises%20privacy%0Aconcerns%20when%20the%20training%20dataset%20contains%20sensitive%20content%2C%20e.g.%2C%20face%0Aimages.%20In%20this%20work%2C%20we%20propose%20a%20method%20to%20perform%20the%20training%20phase%20of%20a%0Adeep%20learning%20model%20on%20both%20an%20edge%20device%20and%20a%20cloud%20server%20that%20prevents%0Asensitive%20content%20being%20transmitted%20to%20the%20cloud%20while%20retaining%20the%20desired%0Ainformation.%20The%20proposed%20privacy-preserving%20method%20uses%20adversarial%20early%0Aexits%20to%20suppress%20the%20sensitive%20content%20at%20the%20edge%20and%20transmits%20the%0Atask-relevant%20information%20to%20the%20cloud.%20This%20approach%20incorporates%20noise%0Aaddition%20during%20the%20training%20phase%20to%20provide%20a%20differential%20privacy%20guarantee.%0AWe%20extensively%20test%20our%20method%20on%20different%20facial%20datasets%20with%20diverse%20face%0Aattributes%20using%20various%20deep%20learning%20architectures%2C%20showcasing%20its%0Aoutstanding%20performance.%20We%20also%20demonstrate%20the%20effectiveness%20of%20privacy%0Apreservation%20through%20successful%20defenses%20against%20different%20white-box%20and%20deep%0Areconstruction%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPriPHiT%253A%2520Privacy-Preserving%2520Hierarchical%2520Training%2520of%2520Deep%2520Neural%250A%2520%2520Networks%26entry.906535625%3DYamin%2520Sepehri%2520and%2520Pedram%2520Pad%2520and%2520Pascal%2520Frossard%2520and%2520L.%2520Andrea%2520Dunbar%26entry.1292438233%3D%2520%2520The%2520training%2520phase%2520of%2520deep%2520neural%2520networks%2520requires%2520substantial%2520resources%2520and%250Aas%2520such%2520is%2520often%2520performed%2520on%2520cloud%2520servers.%2520However%252C%2520this%2520raises%2520privacy%250Aconcerns%2520when%2520the%2520training%2520dataset%2520contains%2520sensitive%2520content%252C%2520e.g.%252C%2520face%250Aimages.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%2520to%2520perform%2520the%2520training%2520phase%2520of%2520a%250Adeep%2520learning%2520model%2520on%2520both%2520an%2520edge%2520device%2520and%2520a%2520cloud%2520server%2520that%2520prevents%250Asensitive%2520content%2520being%2520transmitted%2520to%2520the%2520cloud%2520while%2520retaining%2520the%2520desired%250Ainformation.%2520The%2520proposed%2520privacy-preserving%2520method%2520uses%2520adversarial%2520early%250Aexits%2520to%2520suppress%2520the%2520sensitive%2520content%2520at%2520the%2520edge%2520and%2520transmits%2520the%250Atask-relevant%2520information%2520to%2520the%2520cloud.%2520This%2520approach%2520incorporates%2520noise%250Aaddition%2520during%2520the%2520training%2520phase%2520to%2520provide%2520a%2520differential%2520privacy%2520guarantee.%250AWe%2520extensively%2520test%2520our%2520method%2520on%2520different%2520facial%2520datasets%2520with%2520diverse%2520face%250Aattributes%2520using%2520various%2520deep%2520learning%2520architectures%252C%2520showcasing%2520its%250Aoutstanding%2520performance.%2520We%2520also%2520demonstrate%2520the%2520effectiveness%2520of%2520privacy%250Apreservation%2520through%2520successful%2520defenses%2520against%2520different%2520white-box%2520and%2520deep%250Areconstruction%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PriPHiT%3A%20Privacy-Preserving%20Hierarchical%20Training%20of%20Deep%20Neural%0A%20%20Networks&entry.906535625=Yamin%20Sepehri%20and%20Pedram%20Pad%20and%20Pascal%20Frossard%20and%20L.%20Andrea%20Dunbar&entry.1292438233=%20%20The%20training%20phase%20of%20deep%20neural%20networks%20requires%20substantial%20resources%20and%0Aas%20such%20is%20often%20performed%20on%20cloud%20servers.%20However%2C%20this%20raises%20privacy%0Aconcerns%20when%20the%20training%20dataset%20contains%20sensitive%20content%2C%20e.g.%2C%20face%0Aimages.%20In%20this%20work%2C%20we%20propose%20a%20method%20to%20perform%20the%20training%20phase%20of%20a%0Adeep%20learning%20model%20on%20both%20an%20edge%20device%20and%20a%20cloud%20server%20that%20prevents%0Asensitive%20content%20being%20transmitted%20to%20the%20cloud%20while%20retaining%20the%20desired%0Ainformation.%20The%20proposed%20privacy-preserving%20method%20uses%20adversarial%20early%0Aexits%20to%20suppress%20the%20sensitive%20content%20at%20the%20edge%20and%20transmits%20the%0Atask-relevant%20information%20to%20the%20cloud.%20This%20approach%20incorporates%20noise%0Aaddition%20during%20the%20training%20phase%20to%20provide%20a%20differential%20privacy%20guarantee.%0AWe%20extensively%20test%20our%20method%20on%20different%20facial%20datasets%20with%20diverse%20face%0Aattributes%20using%20various%20deep%20learning%20architectures%2C%20showcasing%20its%0Aoutstanding%20performance.%20We%20also%20demonstrate%20the%20effectiveness%20of%20privacy%0Apreservation%20through%20successful%20defenses%20against%20different%20white-box%20and%20deep%0Areconstruction%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05092v1&entry.124074799=Read"},
{"title": "ProFuser: Progressive Fusion of Large Language Models", "author": "Tianyuan Shi and Fanqi Wan and Canbin Huang and Xiaojun Quan and Chenliang Li and Ming Yan and Ji Zhang", "abstract": "  While fusing the capacities and advantages of various large language models\n(LLMs) offers a pathway to construct more powerful and versatile models, a\nfundamental challenge is to properly select advantageous model during the\ntraining. Existing fusion methods primarily focus on the training mode that\nuses cross entropy on ground truth in a teacher-forcing setup to measure a\nmodel's advantage, which may provide limited insight towards model advantage.\nIn this paper, we introduce a novel approach that enhances the fusion process\nby incorporating both the training and inference modes. Our method evaluates\nmodel advantage not only through cross entropy during training but also by\nconsidering inference outputs, providing a more comprehensive assessment. To\ncombine the two modes effectively, we introduce ProFuser to progressively\ntransition from inference mode to training mode. To validate ProFuser's\neffectiveness, we fused three models, including vicuna-7b-v1.5,\nLlama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance\nin knowledge, reasoning, and safety compared to baseline methods.\n", "link": "http://arxiv.org/abs/2408.04998v1", "date": "2024-08-09", "relevancy": 1.9987, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProFuser%3A%20Progressive%20Fusion%20of%20Large%20Language%20Models&body=Title%3A%20ProFuser%3A%20Progressive%20Fusion%20of%20Large%20Language%20Models%0AAuthor%3A%20Tianyuan%20Shi%20and%20Fanqi%20Wan%20and%20Canbin%20Huang%20and%20Xiaojun%20Quan%20and%20Chenliang%20Li%20and%20Ming%20Yan%20and%20Ji%20Zhang%0AAbstract%3A%20%20%20While%20fusing%20the%20capacities%20and%20advantages%20of%20various%20large%20language%20models%0A%28LLMs%29%20offers%20a%20pathway%20to%20construct%20more%20powerful%20and%20versatile%20models%2C%20a%0Afundamental%20challenge%20is%20to%20properly%20select%20advantageous%20model%20during%20the%0Atraining.%20Existing%20fusion%20methods%20primarily%20focus%20on%20the%20training%20mode%20that%0Auses%20cross%20entropy%20on%20ground%20truth%20in%20a%20teacher-forcing%20setup%20to%20measure%20a%0Amodel%27s%20advantage%2C%20which%20may%20provide%20limited%20insight%20towards%20model%20advantage.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20that%20enhances%20the%20fusion%20process%0Aby%20incorporating%20both%20the%20training%20and%20inference%20modes.%20Our%20method%20evaluates%0Amodel%20advantage%20not%20only%20through%20cross%20entropy%20during%20training%20but%20also%20by%0Aconsidering%20inference%20outputs%2C%20providing%20a%20more%20comprehensive%20assessment.%20To%0Acombine%20the%20two%20modes%20effectively%2C%20we%20introduce%20ProFuser%20to%20progressively%0Atransition%20from%20inference%20mode%20to%20training%20mode.%20To%20validate%20ProFuser%27s%0Aeffectiveness%2C%20we%20fused%20three%20models%2C%20including%20vicuna-7b-v1.5%2C%0ALlama-2-7b-chat%2C%20and%20mpt-7b-8k-chat%2C%20and%20demonstrated%20the%20improved%20performance%0Ain%20knowledge%2C%20reasoning%2C%20and%20safety%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProFuser%253A%2520Progressive%2520Fusion%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DTianyuan%2520Shi%2520and%2520Fanqi%2520Wan%2520and%2520Canbin%2520Huang%2520and%2520Xiaojun%2520Quan%2520and%2520Chenliang%2520Li%2520and%2520Ming%2520Yan%2520and%2520Ji%2520Zhang%26entry.1292438233%3D%2520%2520While%2520fusing%2520the%2520capacities%2520and%2520advantages%2520of%2520various%2520large%2520language%2520models%250A%2528LLMs%2529%2520offers%2520a%2520pathway%2520to%2520construct%2520more%2520powerful%2520and%2520versatile%2520models%252C%2520a%250Afundamental%2520challenge%2520is%2520to%2520properly%2520select%2520advantageous%2520model%2520during%2520the%250Atraining.%2520Existing%2520fusion%2520methods%2520primarily%2520focus%2520on%2520the%2520training%2520mode%2520that%250Auses%2520cross%2520entropy%2520on%2520ground%2520truth%2520in%2520a%2520teacher-forcing%2520setup%2520to%2520measure%2520a%250Amodel%2527s%2520advantage%252C%2520which%2520may%2520provide%2520limited%2520insight%2520towards%2520model%2520advantage.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520that%2520enhances%2520the%2520fusion%2520process%250Aby%2520incorporating%2520both%2520the%2520training%2520and%2520inference%2520modes.%2520Our%2520method%2520evaluates%250Amodel%2520advantage%2520not%2520only%2520through%2520cross%2520entropy%2520during%2520training%2520but%2520also%2520by%250Aconsidering%2520inference%2520outputs%252C%2520providing%2520a%2520more%2520comprehensive%2520assessment.%2520To%250Acombine%2520the%2520two%2520modes%2520effectively%252C%2520we%2520introduce%2520ProFuser%2520to%2520progressively%250Atransition%2520from%2520inference%2520mode%2520to%2520training%2520mode.%2520To%2520validate%2520ProFuser%2527s%250Aeffectiveness%252C%2520we%2520fused%2520three%2520models%252C%2520including%2520vicuna-7b-v1.5%252C%250ALlama-2-7b-chat%252C%2520and%2520mpt-7b-8k-chat%252C%2520and%2520demonstrated%2520the%2520improved%2520performance%250Ain%2520knowledge%252C%2520reasoning%252C%2520and%2520safety%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProFuser%3A%20Progressive%20Fusion%20of%20Large%20Language%20Models&entry.906535625=Tianyuan%20Shi%20and%20Fanqi%20Wan%20and%20Canbin%20Huang%20and%20Xiaojun%20Quan%20and%20Chenliang%20Li%20and%20Ming%20Yan%20and%20Ji%20Zhang&entry.1292438233=%20%20While%20fusing%20the%20capacities%20and%20advantages%20of%20various%20large%20language%20models%0A%28LLMs%29%20offers%20a%20pathway%20to%20construct%20more%20powerful%20and%20versatile%20models%2C%20a%0Afundamental%20challenge%20is%20to%20properly%20select%20advantageous%20model%20during%20the%0Atraining.%20Existing%20fusion%20methods%20primarily%20focus%20on%20the%20training%20mode%20that%0Auses%20cross%20entropy%20on%20ground%20truth%20in%20a%20teacher-forcing%20setup%20to%20measure%20a%0Amodel%27s%20advantage%2C%20which%20may%20provide%20limited%20insight%20towards%20model%20advantage.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20that%20enhances%20the%20fusion%20process%0Aby%20incorporating%20both%20the%20training%20and%20inference%20modes.%20Our%20method%20evaluates%0Amodel%20advantage%20not%20only%20through%20cross%20entropy%20during%20training%20but%20also%20by%0Aconsidering%20inference%20outputs%2C%20providing%20a%20more%20comprehensive%20assessment.%20To%0Acombine%20the%20two%20modes%20effectively%2C%20we%20introduce%20ProFuser%20to%20progressively%0Atransition%20from%20inference%20mode%20to%20training%20mode.%20To%20validate%20ProFuser%27s%0Aeffectiveness%2C%20we%20fused%20three%20models%2C%20including%20vicuna-7b-v1.5%2C%0ALlama-2-7b-chat%2C%20and%20mpt-7b-8k-chat%2C%20and%20demonstrated%20the%20improved%20performance%0Ain%20knowledge%2C%20reasoning%2C%20and%20safety%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04998v1&entry.124074799=Read"},
{"title": "Benchmarking Conventional and Learned Video Codecs with a Low-Delay\n  Configuration", "author": "Siyue Teng and Yuxuan Jiang and Ge Gao and Fan Zhang and Thomas Davis and Zoe Liu and David Bull", "abstract": "  Recent advances in video compression have seen significant coding performance\nimprovements with the development of new standards and learning-based video\ncodecs. However, most of these works focus on application scenarios that allow\na certain amount of system delay (e.g., Random Access mode in MPEG codecs),\nwhich is not always acceptable for live delivery. This paper conducts a\ncomparative study of state-of-the-art conventional and learned video coding\nmethods based on a low delay configuration. Specifically, this study includes\ntwo MPEG standard codecs (H.266/VVC VTM and JVET ECM), two AOM codecs (AV1\nlibaom and AVM), and two recent neural video coding models (DCVC-DC and\nDCVC-FM). To allow a fair and meaningful comparison, the evaluation was\nperformed on test sequences defined in the AOM and MPEG common test conditions\nin the YCbCr 4:2:0 color space. The evaluation results show that the JVET ECM\ncodecs offer the best overall coding performance among all codecs tested, with\na 16.1% (based on PSNR) average BD-rate saving over AOM AVM, and 11.0% over\nDCVC-FM. We also observed inconsistent performance with the learned video\ncodecs, DCVC-DC and DCVC-FM, for test content with large background motions.\n", "link": "http://arxiv.org/abs/2408.05042v1", "date": "2024-08-09", "relevancy": 1.9937, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.509}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.507}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Conventional%20and%20Learned%20Video%20Codecs%20with%20a%20Low-Delay%0A%20%20Configuration&body=Title%3A%20Benchmarking%20Conventional%20and%20Learned%20Video%20Codecs%20with%20a%20Low-Delay%0A%20%20Configuration%0AAuthor%3A%20Siyue%20Teng%20and%20Yuxuan%20Jiang%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Thomas%20Davis%20and%20Zoe%20Liu%20and%20David%20Bull%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20compression%20have%20seen%20significant%20coding%20performance%0Aimprovements%20with%20the%20development%20of%20new%20standards%20and%20learning-based%20video%0Acodecs.%20However%2C%20most%20of%20these%20works%20focus%20on%20application%20scenarios%20that%20allow%0Aa%20certain%20amount%20of%20system%20delay%20%28e.g.%2C%20Random%20Access%20mode%20in%20MPEG%20codecs%29%2C%0Awhich%20is%20not%20always%20acceptable%20for%20live%20delivery.%20This%20paper%20conducts%20a%0Acomparative%20study%20of%20state-of-the-art%20conventional%20and%20learned%20video%20coding%0Amethods%20based%20on%20a%20low%20delay%20configuration.%20Specifically%2C%20this%20study%20includes%0Atwo%20MPEG%20standard%20codecs%20%28H.266/VVC%20VTM%20and%20JVET%20ECM%29%2C%20two%20AOM%20codecs%20%28AV1%0Alibaom%20and%20AVM%29%2C%20and%20two%20recent%20neural%20video%20coding%20models%20%28DCVC-DC%20and%0ADCVC-FM%29.%20To%20allow%20a%20fair%20and%20meaningful%20comparison%2C%20the%20evaluation%20was%0Aperformed%20on%20test%20sequences%20defined%20in%20the%20AOM%20and%20MPEG%20common%20test%20conditions%0Ain%20the%20YCbCr%204%3A2%3A0%20color%20space.%20The%20evaluation%20results%20show%20that%20the%20JVET%20ECM%0Acodecs%20offer%20the%20best%20overall%20coding%20performance%20among%20all%20codecs%20tested%2C%20with%0Aa%2016.1%25%20%28based%20on%20PSNR%29%20average%20BD-rate%20saving%20over%20AOM%20AVM%2C%20and%2011.0%25%20over%0ADCVC-FM.%20We%20also%20observed%20inconsistent%20performance%20with%20the%20learned%20video%0Acodecs%2C%20DCVC-DC%20and%20DCVC-FM%2C%20for%20test%20content%20with%20large%20background%20motions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Conventional%2520and%2520Learned%2520Video%2520Codecs%2520with%2520a%2520Low-Delay%250A%2520%2520Configuration%26entry.906535625%3DSiyue%2520Teng%2520and%2520Yuxuan%2520Jiang%2520and%2520Ge%2520Gao%2520and%2520Fan%2520Zhang%2520and%2520Thomas%2520Davis%2520and%2520Zoe%2520Liu%2520and%2520David%2520Bull%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520compression%2520have%2520seen%2520significant%2520coding%2520performance%250Aimprovements%2520with%2520the%2520development%2520of%2520new%2520standards%2520and%2520learning-based%2520video%250Acodecs.%2520However%252C%2520most%2520of%2520these%2520works%2520focus%2520on%2520application%2520scenarios%2520that%2520allow%250Aa%2520certain%2520amount%2520of%2520system%2520delay%2520%2528e.g.%252C%2520Random%2520Access%2520mode%2520in%2520MPEG%2520codecs%2529%252C%250Awhich%2520is%2520not%2520always%2520acceptable%2520for%2520live%2520delivery.%2520This%2520paper%2520conducts%2520a%250Acomparative%2520study%2520of%2520state-of-the-art%2520conventional%2520and%2520learned%2520video%2520coding%250Amethods%2520based%2520on%2520a%2520low%2520delay%2520configuration.%2520Specifically%252C%2520this%2520study%2520includes%250Atwo%2520MPEG%2520standard%2520codecs%2520%2528H.266/VVC%2520VTM%2520and%2520JVET%2520ECM%2529%252C%2520two%2520AOM%2520codecs%2520%2528AV1%250Alibaom%2520and%2520AVM%2529%252C%2520and%2520two%2520recent%2520neural%2520video%2520coding%2520models%2520%2528DCVC-DC%2520and%250ADCVC-FM%2529.%2520To%2520allow%2520a%2520fair%2520and%2520meaningful%2520comparison%252C%2520the%2520evaluation%2520was%250Aperformed%2520on%2520test%2520sequences%2520defined%2520in%2520the%2520AOM%2520and%2520MPEG%2520common%2520test%2520conditions%250Ain%2520the%2520YCbCr%25204%253A2%253A0%2520color%2520space.%2520The%2520evaluation%2520results%2520show%2520that%2520the%2520JVET%2520ECM%250Acodecs%2520offer%2520the%2520best%2520overall%2520coding%2520performance%2520among%2520all%2520codecs%2520tested%252C%2520with%250Aa%252016.1%2525%2520%2528based%2520on%2520PSNR%2529%2520average%2520BD-rate%2520saving%2520over%2520AOM%2520AVM%252C%2520and%252011.0%2525%2520over%250ADCVC-FM.%2520We%2520also%2520observed%2520inconsistent%2520performance%2520with%2520the%2520learned%2520video%250Acodecs%252C%2520DCVC-DC%2520and%2520DCVC-FM%252C%2520for%2520test%2520content%2520with%2520large%2520background%2520motions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Conventional%20and%20Learned%20Video%20Codecs%20with%20a%20Low-Delay%0A%20%20Configuration&entry.906535625=Siyue%20Teng%20and%20Yuxuan%20Jiang%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Thomas%20Davis%20and%20Zoe%20Liu%20and%20David%20Bull&entry.1292438233=%20%20Recent%20advances%20in%20video%20compression%20have%20seen%20significant%20coding%20performance%0Aimprovements%20with%20the%20development%20of%20new%20standards%20and%20learning-based%20video%0Acodecs.%20However%2C%20most%20of%20these%20works%20focus%20on%20application%20scenarios%20that%20allow%0Aa%20certain%20amount%20of%20system%20delay%20%28e.g.%2C%20Random%20Access%20mode%20in%20MPEG%20codecs%29%2C%0Awhich%20is%20not%20always%20acceptable%20for%20live%20delivery.%20This%20paper%20conducts%20a%0Acomparative%20study%20of%20state-of-the-art%20conventional%20and%20learned%20video%20coding%0Amethods%20based%20on%20a%20low%20delay%20configuration.%20Specifically%2C%20this%20study%20includes%0Atwo%20MPEG%20standard%20codecs%20%28H.266/VVC%20VTM%20and%20JVET%20ECM%29%2C%20two%20AOM%20codecs%20%28AV1%0Alibaom%20and%20AVM%29%2C%20and%20two%20recent%20neural%20video%20coding%20models%20%28DCVC-DC%20and%0ADCVC-FM%29.%20To%20allow%20a%20fair%20and%20meaningful%20comparison%2C%20the%20evaluation%20was%0Aperformed%20on%20test%20sequences%20defined%20in%20the%20AOM%20and%20MPEG%20common%20test%20conditions%0Ain%20the%20YCbCr%204%3A2%3A0%20color%20space.%20The%20evaluation%20results%20show%20that%20the%20JVET%20ECM%0Acodecs%20offer%20the%20best%20overall%20coding%20performance%20among%20all%20codecs%20tested%2C%20with%0Aa%2016.1%25%20%28based%20on%20PSNR%29%20average%20BD-rate%20saving%20over%20AOM%20AVM%2C%20and%2011.0%25%20over%0ADCVC-FM.%20We%20also%20observed%20inconsistent%20performance%20with%20the%20learned%20video%0Acodecs%2C%20DCVC-DC%20and%20DCVC-FM%2C%20for%20test%20content%20with%20large%20background%20motions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05042v1&entry.124074799=Read"},
{"title": "Masked adversarial neural network for cell type deconvolution in spatial\n  transcriptomics", "author": "Lin Huang and Xiaofei Liu and Shunfang Wang and Wenwen Min", "abstract": "  Accurately determining cell type composition in disease-relevant tissues is\ncrucial for identifying disease targets. Most existing spatial transcriptomics\n(ST) technologies cannot achieve single-cell resolution, making it challenging\nto accurately determine cell types. To address this issue, various\ndeconvolution methods have been developed. Most of these methods use\nsingle-cell RNA sequencing (scRNA-seq) data from the same tissue as a reference\nto infer cell types in ST data spots. However, they often overlook the\ndifferences between scRNA-seq and ST data. To overcome this limitation, we\npropose a Masked Adversarial Neural Network (MACD). MACD employs adversarial\nlearning to align real ST data with simulated ST data generated from scRNA-seq\ndata. By mapping them into a unified latent space, it can minimize the\ndifferences between the two types of data. Additionally, MACD uses masking\ntechniques to effectively learn the features of real ST data and mitigate\nnoise. We evaluated MACD on 32 simulated datasets and 2 real datasets,\ndemonstrating its accuracy in performing cell type deconvolution. All code and\npublic datasets used in this paper are available at\nhttps://github.com/wenwenmin/MACD and https://zenodo.org/records/12804822.\n", "link": "http://arxiv.org/abs/2408.05065v1", "date": "2024-08-09", "relevancy": 1.9868, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4998}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4993}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20adversarial%20neural%20network%20for%20cell%20type%20deconvolution%20in%20spatial%0A%20%20transcriptomics&body=Title%3A%20Masked%20adversarial%20neural%20network%20for%20cell%20type%20deconvolution%20in%20spatial%0A%20%20transcriptomics%0AAuthor%3A%20Lin%20Huang%20and%20Xiaofei%20Liu%20and%20Shunfang%20Wang%20and%20Wenwen%20Min%0AAbstract%3A%20%20%20Accurately%20determining%20cell%20type%20composition%20in%20disease-relevant%20tissues%20is%0Acrucial%20for%20identifying%20disease%20targets.%20Most%20existing%20spatial%20transcriptomics%0A%28ST%29%20technologies%20cannot%20achieve%20single-cell%20resolution%2C%20making%20it%20challenging%0Ato%20accurately%20determine%20cell%20types.%20To%20address%20this%20issue%2C%20various%0Adeconvolution%20methods%20have%20been%20developed.%20Most%20of%20these%20methods%20use%0Asingle-cell%20RNA%20sequencing%20%28scRNA-seq%29%20data%20from%20the%20same%20tissue%20as%20a%20reference%0Ato%20infer%20cell%20types%20in%20ST%20data%20spots.%20However%2C%20they%20often%20overlook%20the%0Adifferences%20between%20scRNA-seq%20and%20ST%20data.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20Masked%20Adversarial%20Neural%20Network%20%28MACD%29.%20MACD%20employs%20adversarial%0Alearning%20to%20align%20real%20ST%20data%20with%20simulated%20ST%20data%20generated%20from%20scRNA-seq%0Adata.%20By%20mapping%20them%20into%20a%20unified%20latent%20space%2C%20it%20can%20minimize%20the%0Adifferences%20between%20the%20two%20types%20of%20data.%20Additionally%2C%20MACD%20uses%20masking%0Atechniques%20to%20effectively%20learn%20the%20features%20of%20real%20ST%20data%20and%20mitigate%0Anoise.%20We%20evaluated%20MACD%20on%2032%20simulated%20datasets%20and%202%20real%20datasets%2C%0Ademonstrating%20its%20accuracy%20in%20performing%20cell%20type%20deconvolution.%20All%20code%20and%0Apublic%20datasets%20used%20in%20this%20paper%20are%20available%20at%0Ahttps%3A//github.com/wenwenmin/MACD%20and%20https%3A//zenodo.org/records/12804822.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520adversarial%2520neural%2520network%2520for%2520cell%2520type%2520deconvolution%2520in%2520spatial%250A%2520%2520transcriptomics%26entry.906535625%3DLin%2520Huang%2520and%2520Xiaofei%2520Liu%2520and%2520Shunfang%2520Wang%2520and%2520Wenwen%2520Min%26entry.1292438233%3D%2520%2520Accurately%2520determining%2520cell%2520type%2520composition%2520in%2520disease-relevant%2520tissues%2520is%250Acrucial%2520for%2520identifying%2520disease%2520targets.%2520Most%2520existing%2520spatial%2520transcriptomics%250A%2528ST%2529%2520technologies%2520cannot%2520achieve%2520single-cell%2520resolution%252C%2520making%2520it%2520challenging%250Ato%2520accurately%2520determine%2520cell%2520types.%2520To%2520address%2520this%2520issue%252C%2520various%250Adeconvolution%2520methods%2520have%2520been%2520developed.%2520Most%2520of%2520these%2520methods%2520use%250Asingle-cell%2520RNA%2520sequencing%2520%2528scRNA-seq%2529%2520data%2520from%2520the%2520same%2520tissue%2520as%2520a%2520reference%250Ato%2520infer%2520cell%2520types%2520in%2520ST%2520data%2520spots.%2520However%252C%2520they%2520often%2520overlook%2520the%250Adifferences%2520between%2520scRNA-seq%2520and%2520ST%2520data.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520a%2520Masked%2520Adversarial%2520Neural%2520Network%2520%2528MACD%2529.%2520MACD%2520employs%2520adversarial%250Alearning%2520to%2520align%2520real%2520ST%2520data%2520with%2520simulated%2520ST%2520data%2520generated%2520from%2520scRNA-seq%250Adata.%2520By%2520mapping%2520them%2520into%2520a%2520unified%2520latent%2520space%252C%2520it%2520can%2520minimize%2520the%250Adifferences%2520between%2520the%2520two%2520types%2520of%2520data.%2520Additionally%252C%2520MACD%2520uses%2520masking%250Atechniques%2520to%2520effectively%2520learn%2520the%2520features%2520of%2520real%2520ST%2520data%2520and%2520mitigate%250Anoise.%2520We%2520evaluated%2520MACD%2520on%252032%2520simulated%2520datasets%2520and%25202%2520real%2520datasets%252C%250Ademonstrating%2520its%2520accuracy%2520in%2520performing%2520cell%2520type%2520deconvolution.%2520All%2520code%2520and%250Apublic%2520datasets%2520used%2520in%2520this%2520paper%2520are%2520available%2520at%250Ahttps%253A//github.com/wenwenmin/MACD%2520and%2520https%253A//zenodo.org/records/12804822.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20adversarial%20neural%20network%20for%20cell%20type%20deconvolution%20in%20spatial%0A%20%20transcriptomics&entry.906535625=Lin%20Huang%20and%20Xiaofei%20Liu%20and%20Shunfang%20Wang%20and%20Wenwen%20Min&entry.1292438233=%20%20Accurately%20determining%20cell%20type%20composition%20in%20disease-relevant%20tissues%20is%0Acrucial%20for%20identifying%20disease%20targets.%20Most%20existing%20spatial%20transcriptomics%0A%28ST%29%20technologies%20cannot%20achieve%20single-cell%20resolution%2C%20making%20it%20challenging%0Ato%20accurately%20determine%20cell%20types.%20To%20address%20this%20issue%2C%20various%0Adeconvolution%20methods%20have%20been%20developed.%20Most%20of%20these%20methods%20use%0Asingle-cell%20RNA%20sequencing%20%28scRNA-seq%29%20data%20from%20the%20same%20tissue%20as%20a%20reference%0Ato%20infer%20cell%20types%20in%20ST%20data%20spots.%20However%2C%20they%20often%20overlook%20the%0Adifferences%20between%20scRNA-seq%20and%20ST%20data.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20Masked%20Adversarial%20Neural%20Network%20%28MACD%29.%20MACD%20employs%20adversarial%0Alearning%20to%20align%20real%20ST%20data%20with%20simulated%20ST%20data%20generated%20from%20scRNA-seq%0Adata.%20By%20mapping%20them%20into%20a%20unified%20latent%20space%2C%20it%20can%20minimize%20the%0Adifferences%20between%20the%20two%20types%20of%20data.%20Additionally%2C%20MACD%20uses%20masking%0Atechniques%20to%20effectively%20learn%20the%20features%20of%20real%20ST%20data%20and%20mitigate%0Anoise.%20We%20evaluated%20MACD%20on%2032%20simulated%20datasets%20and%202%20real%20datasets%2C%0Ademonstrating%20its%20accuracy%20in%20performing%20cell%20type%20deconvolution.%20All%20code%20and%0Apublic%20datasets%20used%20in%20this%20paper%20are%20available%20at%0Ahttps%3A//github.com/wenwenmin/MACD%20and%20https%3A//zenodo.org/records/12804822.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05065v1&entry.124074799=Read"},
{"title": "Learning k-Level Structured Sparse Neural Networks Using Group Envelope\n  Regularization", "author": "Yehonathan Refael and Iftach Arbel and Wasim Huleihel", "abstract": "  The extensive need for computational resources poses a significant obstacle\nto deploying large-scale Deep Neural Networks (DNN) on devices with constrained\nresources. At the same time, studies have demonstrated that a significant\nnumber of these DNN parameters are redundant and extraneous. In this paper, we\nintroduce a novel approach for learning structured sparse neural networks,\naimed at bridging the DNN hardware deployment challenges. We develop a novel\nregularization technique, termed Weighted Group Sparse Envelope Function\n(WGSEF), generalizing the Sparse Envelop Function (SEF), to select (or nullify)\nneuron groups, thereby reducing redundancy and enhancing computational\nefficiency. The method speeds up inference time and aims to reduce memory\ndemand and power consumption, thanks to its adaptability which lets any\nhardware specify group definitions, such as filters, channels, filter shapes,\nlayer depths, a single parameter (unstructured), etc. The properties of the\nWGSEF enable the pre-definition of a desired sparsity level to be achieved at\nthe training convergence. In the case of redundant parameters, this approach\nmaintains negligible network accuracy degradation or can even lead to\nimprovements in accuracy. Our method efficiently computes the WGSEF regularizer\nand its proximal operator, in a worst-case linear complexity relative to the\nnumber of group variables. Employing a proximal-gradient-based optimization\ntechnique, to train the model, it tackles the non-convex minimization problem\nincorporating the neural network loss and the WGSEF. Finally, we experiment and\nillustrate the efficiency of our proposed method in terms of the compression\nratio, accuracy, and inference latency.\n", "link": "http://arxiv.org/abs/2212.12921v4", "date": "2024-08-09", "relevancy": 1.9854, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5046}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4983}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20k-Level%20Structured%20Sparse%20Neural%20Networks%20Using%20Group%20Envelope%0A%20%20Regularization&body=Title%3A%20Learning%20k-Level%20Structured%20Sparse%20Neural%20Networks%20Using%20Group%20Envelope%0A%20%20Regularization%0AAuthor%3A%20Yehonathan%20Refael%20and%20Iftach%20Arbel%20and%20Wasim%20Huleihel%0AAbstract%3A%20%20%20The%20extensive%20need%20for%20computational%20resources%20poses%20a%20significant%20obstacle%0Ato%20deploying%20large-scale%20Deep%20Neural%20Networks%20%28DNN%29%20on%20devices%20with%20constrained%0Aresources.%20At%20the%20same%20time%2C%20studies%20have%20demonstrated%20that%20a%20significant%0Anumber%20of%20these%20DNN%20parameters%20are%20redundant%20and%20extraneous.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20approach%20for%20learning%20structured%20sparse%20neural%20networks%2C%0Aaimed%20at%20bridging%20the%20DNN%20hardware%20deployment%20challenges.%20We%20develop%20a%20novel%0Aregularization%20technique%2C%20termed%20Weighted%20Group%20Sparse%20Envelope%20Function%0A%28WGSEF%29%2C%20generalizing%20the%20Sparse%20Envelop%20Function%20%28SEF%29%2C%20to%20select%20%28or%20nullify%29%0Aneuron%20groups%2C%20thereby%20reducing%20redundancy%20and%20enhancing%20computational%0Aefficiency.%20The%20method%20speeds%20up%20inference%20time%20and%20aims%20to%20reduce%20memory%0Ademand%20and%20power%20consumption%2C%20thanks%20to%20its%20adaptability%20which%20lets%20any%0Ahardware%20specify%20group%20definitions%2C%20such%20as%20filters%2C%20channels%2C%20filter%20shapes%2C%0Alayer%20depths%2C%20a%20single%20parameter%20%28unstructured%29%2C%20etc.%20The%20properties%20of%20the%0AWGSEF%20enable%20the%20pre-definition%20of%20a%20desired%20sparsity%20level%20to%20be%20achieved%20at%0Athe%20training%20convergence.%20In%20the%20case%20of%20redundant%20parameters%2C%20this%20approach%0Amaintains%20negligible%20network%20accuracy%20degradation%20or%20can%20even%20lead%20to%0Aimprovements%20in%20accuracy.%20Our%20method%20efficiently%20computes%20the%20WGSEF%20regularizer%0Aand%20its%20proximal%20operator%2C%20in%20a%20worst-case%20linear%20complexity%20relative%20to%20the%0Anumber%20of%20group%20variables.%20Employing%20a%20proximal-gradient-based%20optimization%0Atechnique%2C%20to%20train%20the%20model%2C%20it%20tackles%20the%20non-convex%20minimization%20problem%0Aincorporating%20the%20neural%20network%20loss%20and%20the%20WGSEF.%20Finally%2C%20we%20experiment%20and%0Aillustrate%20the%20efficiency%20of%20our%20proposed%20method%20in%20terms%20of%20the%20compression%0Aratio%2C%20accuracy%2C%20and%20inference%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.12921v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520k-Level%2520Structured%2520Sparse%2520Neural%2520Networks%2520Using%2520Group%2520Envelope%250A%2520%2520Regularization%26entry.906535625%3DYehonathan%2520Refael%2520and%2520Iftach%2520Arbel%2520and%2520Wasim%2520Huleihel%26entry.1292438233%3D%2520%2520The%2520extensive%2520need%2520for%2520computational%2520resources%2520poses%2520a%2520significant%2520obstacle%250Ato%2520deploying%2520large-scale%2520Deep%2520Neural%2520Networks%2520%2528DNN%2529%2520on%2520devices%2520with%2520constrained%250Aresources.%2520At%2520the%2520same%2520time%252C%2520studies%2520have%2520demonstrated%2520that%2520a%2520significant%250Anumber%2520of%2520these%2520DNN%2520parameters%2520are%2520redundant%2520and%2520extraneous.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520approach%2520for%2520learning%2520structured%2520sparse%2520neural%2520networks%252C%250Aaimed%2520at%2520bridging%2520the%2520DNN%2520hardware%2520deployment%2520challenges.%2520We%2520develop%2520a%2520novel%250Aregularization%2520technique%252C%2520termed%2520Weighted%2520Group%2520Sparse%2520Envelope%2520Function%250A%2528WGSEF%2529%252C%2520generalizing%2520the%2520Sparse%2520Envelop%2520Function%2520%2528SEF%2529%252C%2520to%2520select%2520%2528or%2520nullify%2529%250Aneuron%2520groups%252C%2520thereby%2520reducing%2520redundancy%2520and%2520enhancing%2520computational%250Aefficiency.%2520The%2520method%2520speeds%2520up%2520inference%2520time%2520and%2520aims%2520to%2520reduce%2520memory%250Ademand%2520and%2520power%2520consumption%252C%2520thanks%2520to%2520its%2520adaptability%2520which%2520lets%2520any%250Ahardware%2520specify%2520group%2520definitions%252C%2520such%2520as%2520filters%252C%2520channels%252C%2520filter%2520shapes%252C%250Alayer%2520depths%252C%2520a%2520single%2520parameter%2520%2528unstructured%2529%252C%2520etc.%2520The%2520properties%2520of%2520the%250AWGSEF%2520enable%2520the%2520pre-definition%2520of%2520a%2520desired%2520sparsity%2520level%2520to%2520be%2520achieved%2520at%250Athe%2520training%2520convergence.%2520In%2520the%2520case%2520of%2520redundant%2520parameters%252C%2520this%2520approach%250Amaintains%2520negligible%2520network%2520accuracy%2520degradation%2520or%2520can%2520even%2520lead%2520to%250Aimprovements%2520in%2520accuracy.%2520Our%2520method%2520efficiently%2520computes%2520the%2520WGSEF%2520regularizer%250Aand%2520its%2520proximal%2520operator%252C%2520in%2520a%2520worst-case%2520linear%2520complexity%2520relative%2520to%2520the%250Anumber%2520of%2520group%2520variables.%2520Employing%2520a%2520proximal-gradient-based%2520optimization%250Atechnique%252C%2520to%2520train%2520the%2520model%252C%2520it%2520tackles%2520the%2520non-convex%2520minimization%2520problem%250Aincorporating%2520the%2520neural%2520network%2520loss%2520and%2520the%2520WGSEF.%2520Finally%252C%2520we%2520experiment%2520and%250Aillustrate%2520the%2520efficiency%2520of%2520our%2520proposed%2520method%2520in%2520terms%2520of%2520the%2520compression%250Aratio%252C%2520accuracy%252C%2520and%2520inference%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.12921v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20k-Level%20Structured%20Sparse%20Neural%20Networks%20Using%20Group%20Envelope%0A%20%20Regularization&entry.906535625=Yehonathan%20Refael%20and%20Iftach%20Arbel%20and%20Wasim%20Huleihel&entry.1292438233=%20%20The%20extensive%20need%20for%20computational%20resources%20poses%20a%20significant%20obstacle%0Ato%20deploying%20large-scale%20Deep%20Neural%20Networks%20%28DNN%29%20on%20devices%20with%20constrained%0Aresources.%20At%20the%20same%20time%2C%20studies%20have%20demonstrated%20that%20a%20significant%0Anumber%20of%20these%20DNN%20parameters%20are%20redundant%20and%20extraneous.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20approach%20for%20learning%20structured%20sparse%20neural%20networks%2C%0Aaimed%20at%20bridging%20the%20DNN%20hardware%20deployment%20challenges.%20We%20develop%20a%20novel%0Aregularization%20technique%2C%20termed%20Weighted%20Group%20Sparse%20Envelope%20Function%0A%28WGSEF%29%2C%20generalizing%20the%20Sparse%20Envelop%20Function%20%28SEF%29%2C%20to%20select%20%28or%20nullify%29%0Aneuron%20groups%2C%20thereby%20reducing%20redundancy%20and%20enhancing%20computational%0Aefficiency.%20The%20method%20speeds%20up%20inference%20time%20and%20aims%20to%20reduce%20memory%0Ademand%20and%20power%20consumption%2C%20thanks%20to%20its%20adaptability%20which%20lets%20any%0Ahardware%20specify%20group%20definitions%2C%20such%20as%20filters%2C%20channels%2C%20filter%20shapes%2C%0Alayer%20depths%2C%20a%20single%20parameter%20%28unstructured%29%2C%20etc.%20The%20properties%20of%20the%0AWGSEF%20enable%20the%20pre-definition%20of%20a%20desired%20sparsity%20level%20to%20be%20achieved%20at%0Athe%20training%20convergence.%20In%20the%20case%20of%20redundant%20parameters%2C%20this%20approach%0Amaintains%20negligible%20network%20accuracy%20degradation%20or%20can%20even%20lead%20to%0Aimprovements%20in%20accuracy.%20Our%20method%20efficiently%20computes%20the%20WGSEF%20regularizer%0Aand%20its%20proximal%20operator%2C%20in%20a%20worst-case%20linear%20complexity%20relative%20to%20the%0Anumber%20of%20group%20variables.%20Employing%20a%20proximal-gradient-based%20optimization%0Atechnique%2C%20to%20train%20the%20model%2C%20it%20tackles%20the%20non-convex%20minimization%20problem%0Aincorporating%20the%20neural%20network%20loss%20and%20the%20WGSEF.%20Finally%2C%20we%20experiment%20and%0Aillustrate%20the%20efficiency%20of%20our%20proposed%20method%20in%20terms%20of%20the%20compression%0Aratio%2C%20accuracy%2C%20and%20inference%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.12921v4&entry.124074799=Read"},
{"title": "Evaluating Feature Attribution Methods in the Image Domain", "author": "Arne Gevaert and Axel-Jan Rousseau and Thijs Becker and Dirk Valkenborg and Tijl De Bie and Yvan Saeys", "abstract": "  Feature attribution maps are a popular approach to highlight the most\nimportant pixels in an image for a given prediction of a model. Despite a\nrecent growth in popularity and available methods, little attention is given to\nthe objective evaluation of such attribution maps. Building on previous work in\nthis domain, we investigate existing metrics and propose new variants of\nmetrics for the evaluation of attribution maps. We confirm a recent finding\nthat different attribution metrics seem to measure different underlying\nconcepts of attribution maps, and extend this finding to a larger selection of\nattribution metrics. We also find that metric results on one dataset do not\nnecessarily generalize to other datasets, and methods with desirable\ntheoretical properties such as DeepSHAP do not necessarily outperform\ncomputationally cheaper alternatives. Based on these findings, we propose a\ngeneral benchmarking approach to identify the ideal feature attribution method\nfor a given use case. Implementations of attribution metrics and our\nexperiments are available online.\n", "link": "http://arxiv.org/abs/2202.12270v2", "date": "2024-08-09", "relevancy": 1.9793, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4997}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Feature%20Attribution%20Methods%20in%20the%20Image%20Domain&body=Title%3A%20Evaluating%20Feature%20Attribution%20Methods%20in%20the%20Image%20Domain%0AAuthor%3A%20Arne%20Gevaert%20and%20Axel-Jan%20Rousseau%20and%20Thijs%20Becker%20and%20Dirk%20Valkenborg%20and%20Tijl%20De%20Bie%20and%20Yvan%20Saeys%0AAbstract%3A%20%20%20Feature%20attribution%20maps%20are%20a%20popular%20approach%20to%20highlight%20the%20most%0Aimportant%20pixels%20in%20an%20image%20for%20a%20given%20prediction%20of%20a%20model.%20Despite%20a%0Arecent%20growth%20in%20popularity%20and%20available%20methods%2C%20little%20attention%20is%20given%20to%0Athe%20objective%20evaluation%20of%20such%20attribution%20maps.%20Building%20on%20previous%20work%20in%0Athis%20domain%2C%20we%20investigate%20existing%20metrics%20and%20propose%20new%20variants%20of%0Ametrics%20for%20the%20evaluation%20of%20attribution%20maps.%20We%20confirm%20a%20recent%20finding%0Athat%20different%20attribution%20metrics%20seem%20to%20measure%20different%20underlying%0Aconcepts%20of%20attribution%20maps%2C%20and%20extend%20this%20finding%20to%20a%20larger%20selection%20of%0Aattribution%20metrics.%20We%20also%20find%20that%20metric%20results%20on%20one%20dataset%20do%20not%0Anecessarily%20generalize%20to%20other%20datasets%2C%20and%20methods%20with%20desirable%0Atheoretical%20properties%20such%20as%20DeepSHAP%20do%20not%20necessarily%20outperform%0Acomputationally%20cheaper%20alternatives.%20Based%20on%20these%20findings%2C%20we%20propose%20a%0Ageneral%20benchmarking%20approach%20to%20identify%20the%20ideal%20feature%20attribution%20method%0Afor%20a%20given%20use%20case.%20Implementations%20of%20attribution%20metrics%20and%20our%0Aexperiments%20are%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.12270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Feature%2520Attribution%2520Methods%2520in%2520the%2520Image%2520Domain%26entry.906535625%3DArne%2520Gevaert%2520and%2520Axel-Jan%2520Rousseau%2520and%2520Thijs%2520Becker%2520and%2520Dirk%2520Valkenborg%2520and%2520Tijl%2520De%2520Bie%2520and%2520Yvan%2520Saeys%26entry.1292438233%3D%2520%2520Feature%2520attribution%2520maps%2520are%2520a%2520popular%2520approach%2520to%2520highlight%2520the%2520most%250Aimportant%2520pixels%2520in%2520an%2520image%2520for%2520a%2520given%2520prediction%2520of%2520a%2520model.%2520Despite%2520a%250Arecent%2520growth%2520in%2520popularity%2520and%2520available%2520methods%252C%2520little%2520attention%2520is%2520given%2520to%250Athe%2520objective%2520evaluation%2520of%2520such%2520attribution%2520maps.%2520Building%2520on%2520previous%2520work%2520in%250Athis%2520domain%252C%2520we%2520investigate%2520existing%2520metrics%2520and%2520propose%2520new%2520variants%2520of%250Ametrics%2520for%2520the%2520evaluation%2520of%2520attribution%2520maps.%2520We%2520confirm%2520a%2520recent%2520finding%250Athat%2520different%2520attribution%2520metrics%2520seem%2520to%2520measure%2520different%2520underlying%250Aconcepts%2520of%2520attribution%2520maps%252C%2520and%2520extend%2520this%2520finding%2520to%2520a%2520larger%2520selection%2520of%250Aattribution%2520metrics.%2520We%2520also%2520find%2520that%2520metric%2520results%2520on%2520one%2520dataset%2520do%2520not%250Anecessarily%2520generalize%2520to%2520other%2520datasets%252C%2520and%2520methods%2520with%2520desirable%250Atheoretical%2520properties%2520such%2520as%2520DeepSHAP%2520do%2520not%2520necessarily%2520outperform%250Acomputationally%2520cheaper%2520alternatives.%2520Based%2520on%2520these%2520findings%252C%2520we%2520propose%2520a%250Ageneral%2520benchmarking%2520approach%2520to%2520identify%2520the%2520ideal%2520feature%2520attribution%2520method%250Afor%2520a%2520given%2520use%2520case.%2520Implementations%2520of%2520attribution%2520metrics%2520and%2520our%250Aexperiments%2520are%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.12270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Feature%20Attribution%20Methods%20in%20the%20Image%20Domain&entry.906535625=Arne%20Gevaert%20and%20Axel-Jan%20Rousseau%20and%20Thijs%20Becker%20and%20Dirk%20Valkenborg%20and%20Tijl%20De%20Bie%20and%20Yvan%20Saeys&entry.1292438233=%20%20Feature%20attribution%20maps%20are%20a%20popular%20approach%20to%20highlight%20the%20most%0Aimportant%20pixels%20in%20an%20image%20for%20a%20given%20prediction%20of%20a%20model.%20Despite%20a%0Arecent%20growth%20in%20popularity%20and%20available%20methods%2C%20little%20attention%20is%20given%20to%0Athe%20objective%20evaluation%20of%20such%20attribution%20maps.%20Building%20on%20previous%20work%20in%0Athis%20domain%2C%20we%20investigate%20existing%20metrics%20and%20propose%20new%20variants%20of%0Ametrics%20for%20the%20evaluation%20of%20attribution%20maps.%20We%20confirm%20a%20recent%20finding%0Athat%20different%20attribution%20metrics%20seem%20to%20measure%20different%20underlying%0Aconcepts%20of%20attribution%20maps%2C%20and%20extend%20this%20finding%20to%20a%20larger%20selection%20of%0Aattribution%20metrics.%20We%20also%20find%20that%20metric%20results%20on%20one%20dataset%20do%20not%0Anecessarily%20generalize%20to%20other%20datasets%2C%20and%20methods%20with%20desirable%0Atheoretical%20properties%20such%20as%20DeepSHAP%20do%20not%20necessarily%20outperform%0Acomputationally%20cheaper%20alternatives.%20Based%20on%20these%20findings%2C%20we%20propose%20a%0Ageneral%20benchmarking%20approach%20to%20identify%20the%20ideal%20feature%20attribution%20method%0Afor%20a%20given%20use%20case.%20Implementations%20of%20attribution%20metrics%20and%20our%0Aexperiments%20are%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.12270v2&entry.124074799=Read"},
{"title": "NuLite -- Lightweight and Fast Model for Nuclei Instance Segmentation\n  and Classification", "author": "Cristian Tommasino and Cristiano Russo and Antonio Maria Rinaldi", "abstract": "  In pathology, accurate and efficient analysis of Hematoxylin and Eosin (H\\&E)\nslides is crucial for timely and effective cancer diagnosis. Although many deep\nlearning solutions for nuclei instance segmentation and classification exist in\nthe literature, they often entail high computational costs and resource\nrequirements, thus limiting their practical usage in medical applications. To\naddress this issue, we introduce a novel convolutional neural network, NuLite,\na U-Net-like architecture designed explicitly on Fast-ViT, a state-of-the-art\n(SOTA) lightweight CNN. We obtained three versions of our model, NuLite-S,\nNuLite-M, and NuLite-H, trained on the PanNuke dataset. The experimental\nresults prove that our models equal CellViT (SOTA) in terms of panoptic quality\nand detection. However, our lightest model, NuLite-S, is 40 times smaller in\nterms of parameters and about 8 times smaller in terms of GFlops, while our\nheaviest model is 17 times smaller in terms of parameters and about 7 times\nsmaller in terms of GFlops. Moreover, our model is up to about 8 times faster\nthan CellViT. Lastly, to prove the effectiveness of our solution, we provide a\nrobust comparison of external datasets, namely CoNseP, MoNuSeg, and GlySAC. Our\nmodel is publicly available at https://github.com/CosmoIknosLab/NuLite\n", "link": "http://arxiv.org/abs/2408.01797v2", "date": "2024-08-09", "relevancy": 1.9495, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5219}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4834}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NuLite%20--%20Lightweight%20and%20Fast%20Model%20for%20Nuclei%20Instance%20Segmentation%0A%20%20and%20Classification&body=Title%3A%20NuLite%20--%20Lightweight%20and%20Fast%20Model%20for%20Nuclei%20Instance%20Segmentation%0A%20%20and%20Classification%0AAuthor%3A%20Cristian%20Tommasino%20and%20Cristiano%20Russo%20and%20Antonio%20Maria%20Rinaldi%0AAbstract%3A%20%20%20In%20pathology%2C%20accurate%20and%20efficient%20analysis%20of%20Hematoxylin%20and%20Eosin%20%28H%5C%26E%29%0Aslides%20is%20crucial%20for%20timely%20and%20effective%20cancer%20diagnosis.%20Although%20many%20deep%0Alearning%20solutions%20for%20nuclei%20instance%20segmentation%20and%20classification%20exist%20in%0Athe%20literature%2C%20they%20often%20entail%20high%20computational%20costs%20and%20resource%0Arequirements%2C%20thus%20limiting%20their%20practical%20usage%20in%20medical%20applications.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20a%20novel%20convolutional%20neural%20network%2C%20NuLite%2C%0Aa%20U-Net-like%20architecture%20designed%20explicitly%20on%20Fast-ViT%2C%20a%20state-of-the-art%0A%28SOTA%29%20lightweight%20CNN.%20We%20obtained%20three%20versions%20of%20our%20model%2C%20NuLite-S%2C%0ANuLite-M%2C%20and%20NuLite-H%2C%20trained%20on%20the%20PanNuke%20dataset.%20The%20experimental%0Aresults%20prove%20that%20our%20models%20equal%20CellViT%20%28SOTA%29%20in%20terms%20of%20panoptic%20quality%0Aand%20detection.%20However%2C%20our%20lightest%20model%2C%20NuLite-S%2C%20is%2040%20times%20smaller%20in%0Aterms%20of%20parameters%20and%20about%208%20times%20smaller%20in%20terms%20of%20GFlops%2C%20while%20our%0Aheaviest%20model%20is%2017%20times%20smaller%20in%20terms%20of%20parameters%20and%20about%207%20times%0Asmaller%20in%20terms%20of%20GFlops.%20Moreover%2C%20our%20model%20is%20up%20to%20about%208%20times%20faster%0Athan%20CellViT.%20Lastly%2C%20to%20prove%20the%20effectiveness%20of%20our%20solution%2C%20we%20provide%20a%0Arobust%20comparison%20of%20external%20datasets%2C%20namely%20CoNseP%2C%20MoNuSeg%2C%20and%20GlySAC.%20Our%0Amodel%20is%20publicly%20available%20at%20https%3A//github.com/CosmoIknosLab/NuLite%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNuLite%2520--%2520Lightweight%2520and%2520Fast%2520Model%2520for%2520Nuclei%2520Instance%2520Segmentation%250A%2520%2520and%2520Classification%26entry.906535625%3DCristian%2520Tommasino%2520and%2520Cristiano%2520Russo%2520and%2520Antonio%2520Maria%2520Rinaldi%26entry.1292438233%3D%2520%2520In%2520pathology%252C%2520accurate%2520and%2520efficient%2520analysis%2520of%2520Hematoxylin%2520and%2520Eosin%2520%2528H%255C%2526E%2529%250Aslides%2520is%2520crucial%2520for%2520timely%2520and%2520effective%2520cancer%2520diagnosis.%2520Although%2520many%2520deep%250Alearning%2520solutions%2520for%2520nuclei%2520instance%2520segmentation%2520and%2520classification%2520exist%2520in%250Athe%2520literature%252C%2520they%2520often%2520entail%2520high%2520computational%2520costs%2520and%2520resource%250Arequirements%252C%2520thus%2520limiting%2520their%2520practical%2520usage%2520in%2520medical%2520applications.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520convolutional%2520neural%2520network%252C%2520NuLite%252C%250Aa%2520U-Net-like%2520architecture%2520designed%2520explicitly%2520on%2520Fast-ViT%252C%2520a%2520state-of-the-art%250A%2528SOTA%2529%2520lightweight%2520CNN.%2520We%2520obtained%2520three%2520versions%2520of%2520our%2520model%252C%2520NuLite-S%252C%250ANuLite-M%252C%2520and%2520NuLite-H%252C%2520trained%2520on%2520the%2520PanNuke%2520dataset.%2520The%2520experimental%250Aresults%2520prove%2520that%2520our%2520models%2520equal%2520CellViT%2520%2528SOTA%2529%2520in%2520terms%2520of%2520panoptic%2520quality%250Aand%2520detection.%2520However%252C%2520our%2520lightest%2520model%252C%2520NuLite-S%252C%2520is%252040%2520times%2520smaller%2520in%250Aterms%2520of%2520parameters%2520and%2520about%25208%2520times%2520smaller%2520in%2520terms%2520of%2520GFlops%252C%2520while%2520our%250Aheaviest%2520model%2520is%252017%2520times%2520smaller%2520in%2520terms%2520of%2520parameters%2520and%2520about%25207%2520times%250Asmaller%2520in%2520terms%2520of%2520GFlops.%2520Moreover%252C%2520our%2520model%2520is%2520up%2520to%2520about%25208%2520times%2520faster%250Athan%2520CellViT.%2520Lastly%252C%2520to%2520prove%2520the%2520effectiveness%2520of%2520our%2520solution%252C%2520we%2520provide%2520a%250Arobust%2520comparison%2520of%2520external%2520datasets%252C%2520namely%2520CoNseP%252C%2520MoNuSeg%252C%2520and%2520GlySAC.%2520Our%250Amodel%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/CosmoIknosLab/NuLite%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NuLite%20--%20Lightweight%20and%20Fast%20Model%20for%20Nuclei%20Instance%20Segmentation%0A%20%20and%20Classification&entry.906535625=Cristian%20Tommasino%20and%20Cristiano%20Russo%20and%20Antonio%20Maria%20Rinaldi&entry.1292438233=%20%20In%20pathology%2C%20accurate%20and%20efficient%20analysis%20of%20Hematoxylin%20and%20Eosin%20%28H%5C%26E%29%0Aslides%20is%20crucial%20for%20timely%20and%20effective%20cancer%20diagnosis.%20Although%20many%20deep%0Alearning%20solutions%20for%20nuclei%20instance%20segmentation%20and%20classification%20exist%20in%0Athe%20literature%2C%20they%20often%20entail%20high%20computational%20costs%20and%20resource%0Arequirements%2C%20thus%20limiting%20their%20practical%20usage%20in%20medical%20applications.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20a%20novel%20convolutional%20neural%20network%2C%20NuLite%2C%0Aa%20U-Net-like%20architecture%20designed%20explicitly%20on%20Fast-ViT%2C%20a%20state-of-the-art%0A%28SOTA%29%20lightweight%20CNN.%20We%20obtained%20three%20versions%20of%20our%20model%2C%20NuLite-S%2C%0ANuLite-M%2C%20and%20NuLite-H%2C%20trained%20on%20the%20PanNuke%20dataset.%20The%20experimental%0Aresults%20prove%20that%20our%20models%20equal%20CellViT%20%28SOTA%29%20in%20terms%20of%20panoptic%20quality%0Aand%20detection.%20However%2C%20our%20lightest%20model%2C%20NuLite-S%2C%20is%2040%20times%20smaller%20in%0Aterms%20of%20parameters%20and%20about%208%20times%20smaller%20in%20terms%20of%20GFlops%2C%20while%20our%0Aheaviest%20model%20is%2017%20times%20smaller%20in%20terms%20of%20parameters%20and%20about%207%20times%0Asmaller%20in%20terms%20of%20GFlops.%20Moreover%2C%20our%20model%20is%20up%20to%20about%208%20times%20faster%0Athan%20CellViT.%20Lastly%2C%20to%20prove%20the%20effectiveness%20of%20our%20solution%2C%20we%20provide%20a%0Arobust%20comparison%20of%20external%20datasets%2C%20namely%20CoNseP%2C%20MoNuSeg%2C%20and%20GlySAC.%20Our%0Amodel%20is%20publicly%20available%20at%20https%3A//github.com/CosmoIknosLab/NuLite%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01797v2&entry.124074799=Read"},
{"title": "UNIC: Universal Classification Models via Multi-teacher Distillation", "author": "Mert Bulent Sariyildiz and Philippe Weinzaepfel and Thomas Lucas and Diane Larlus and Yannis Kalantidis", "abstract": "  Pretrained models have become a commodity and offer strong results on a broad\nrange of tasks. In this work, we focus on classification and seek to learn a\nunique encoder able to take from several complementary pretrained models. We\naim at even stronger generalization across a variety of classification tasks.\nWe propose to learn such an encoder via multi-teacher distillation. We first\nthoroughly analyse standard distillation when driven by multiple strong\nteachers with complementary strengths. Guided by this analysis, we gradually\npropose improvements to the basic distillation setup. Among those, we enrich\nthe architecture of the encoder with a ladder of expendable projectors, which\nincreases the impact of intermediate features during distillation, and we\nintroduce teacher dropping, a regularization mechanism that better balances the\nteachers' influence. Our final distillation strategy leads to student models of\nthe same capacity as any of the teachers, while retaining or improving upon the\nperformance of the best teacher for each task.\n  Project page and code: https://europe.naverlabs.com/unic\n", "link": "http://arxiv.org/abs/2408.05088v1", "date": "2024-08-09", "relevancy": 1.9467, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4925}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4829}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNIC%3A%20Universal%20Classification%20Models%20via%20Multi-teacher%20Distillation&body=Title%3A%20UNIC%3A%20Universal%20Classification%20Models%20via%20Multi-teacher%20Distillation%0AAuthor%3A%20Mert%20Bulent%20Sariyildiz%20and%20Philippe%20Weinzaepfel%20and%20Thomas%20Lucas%20and%20Diane%20Larlus%20and%20Yannis%20Kalantidis%0AAbstract%3A%20%20%20Pretrained%20models%20have%20become%20a%20commodity%20and%20offer%20strong%20results%20on%20a%20broad%0Arange%20of%20tasks.%20In%20this%20work%2C%20we%20focus%20on%20classification%20and%20seek%20to%20learn%20a%0Aunique%20encoder%20able%20to%20take%20from%20several%20complementary%20pretrained%20models.%20We%0Aaim%20at%20even%20stronger%20generalization%20across%20a%20variety%20of%20classification%20tasks.%0AWe%20propose%20to%20learn%20such%20an%20encoder%20via%20multi-teacher%20distillation.%20We%20first%0Athoroughly%20analyse%20standard%20distillation%20when%20driven%20by%20multiple%20strong%0Ateachers%20with%20complementary%20strengths.%20Guided%20by%20this%20analysis%2C%20we%20gradually%0Apropose%20improvements%20to%20the%20basic%20distillation%20setup.%20Among%20those%2C%20we%20enrich%0Athe%20architecture%20of%20the%20encoder%20with%20a%20ladder%20of%20expendable%20projectors%2C%20which%0Aincreases%20the%20impact%20of%20intermediate%20features%20during%20distillation%2C%20and%20we%0Aintroduce%20teacher%20dropping%2C%20a%20regularization%20mechanism%20that%20better%20balances%20the%0Ateachers%27%20influence.%20Our%20final%20distillation%20strategy%20leads%20to%20student%20models%20of%0Athe%20same%20capacity%20as%20any%20of%20the%20teachers%2C%20while%20retaining%20or%20improving%20upon%20the%0Aperformance%20of%20the%20best%20teacher%20for%20each%20task.%0A%20%20Project%20page%20and%20code%3A%20https%3A//europe.naverlabs.com/unic%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNIC%253A%2520Universal%2520Classification%2520Models%2520via%2520Multi-teacher%2520Distillation%26entry.906535625%3DMert%2520Bulent%2520Sariyildiz%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Thomas%2520Lucas%2520and%2520Diane%2520Larlus%2520and%2520Yannis%2520Kalantidis%26entry.1292438233%3D%2520%2520Pretrained%2520models%2520have%2520become%2520a%2520commodity%2520and%2520offer%2520strong%2520results%2520on%2520a%2520broad%250Arange%2520of%2520tasks.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520classification%2520and%2520seek%2520to%2520learn%2520a%250Aunique%2520encoder%2520able%2520to%2520take%2520from%2520several%2520complementary%2520pretrained%2520models.%2520We%250Aaim%2520at%2520even%2520stronger%2520generalization%2520across%2520a%2520variety%2520of%2520classification%2520tasks.%250AWe%2520propose%2520to%2520learn%2520such%2520an%2520encoder%2520via%2520multi-teacher%2520distillation.%2520We%2520first%250Athoroughly%2520analyse%2520standard%2520distillation%2520when%2520driven%2520by%2520multiple%2520strong%250Ateachers%2520with%2520complementary%2520strengths.%2520Guided%2520by%2520this%2520analysis%252C%2520we%2520gradually%250Apropose%2520improvements%2520to%2520the%2520basic%2520distillation%2520setup.%2520Among%2520those%252C%2520we%2520enrich%250Athe%2520architecture%2520of%2520the%2520encoder%2520with%2520a%2520ladder%2520of%2520expendable%2520projectors%252C%2520which%250Aincreases%2520the%2520impact%2520of%2520intermediate%2520features%2520during%2520distillation%252C%2520and%2520we%250Aintroduce%2520teacher%2520dropping%252C%2520a%2520regularization%2520mechanism%2520that%2520better%2520balances%2520the%250Ateachers%2527%2520influence.%2520Our%2520final%2520distillation%2520strategy%2520leads%2520to%2520student%2520models%2520of%250Athe%2520same%2520capacity%2520as%2520any%2520of%2520the%2520teachers%252C%2520while%2520retaining%2520or%2520improving%2520upon%2520the%250Aperformance%2520of%2520the%2520best%2520teacher%2520for%2520each%2520task.%250A%2520%2520Project%2520page%2520and%2520code%253A%2520https%253A//europe.naverlabs.com/unic%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNIC%3A%20Universal%20Classification%20Models%20via%20Multi-teacher%20Distillation&entry.906535625=Mert%20Bulent%20Sariyildiz%20and%20Philippe%20Weinzaepfel%20and%20Thomas%20Lucas%20and%20Diane%20Larlus%20and%20Yannis%20Kalantidis&entry.1292438233=%20%20Pretrained%20models%20have%20become%20a%20commodity%20and%20offer%20strong%20results%20on%20a%20broad%0Arange%20of%20tasks.%20In%20this%20work%2C%20we%20focus%20on%20classification%20and%20seek%20to%20learn%20a%0Aunique%20encoder%20able%20to%20take%20from%20several%20complementary%20pretrained%20models.%20We%0Aaim%20at%20even%20stronger%20generalization%20across%20a%20variety%20of%20classification%20tasks.%0AWe%20propose%20to%20learn%20such%20an%20encoder%20via%20multi-teacher%20distillation.%20We%20first%0Athoroughly%20analyse%20standard%20distillation%20when%20driven%20by%20multiple%20strong%0Ateachers%20with%20complementary%20strengths.%20Guided%20by%20this%20analysis%2C%20we%20gradually%0Apropose%20improvements%20to%20the%20basic%20distillation%20setup.%20Among%20those%2C%20we%20enrich%0Athe%20architecture%20of%20the%20encoder%20with%20a%20ladder%20of%20expendable%20projectors%2C%20which%0Aincreases%20the%20impact%20of%20intermediate%20features%20during%20distillation%2C%20and%20we%0Aintroduce%20teacher%20dropping%2C%20a%20regularization%20mechanism%20that%20better%20balances%20the%0Ateachers%27%20influence.%20Our%20final%20distillation%20strategy%20leads%20to%20student%20models%20of%0Athe%20same%20capacity%20as%20any%20of%20the%20teachers%2C%20while%20retaining%20or%20improving%20upon%20the%0Aperformance%20of%20the%20best%20teacher%20for%20each%20task.%0A%20%20Project%20page%20and%20code%3A%20https%3A//europe.naverlabs.com/unic%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05088v1&entry.124074799=Read"},
{"title": "Text Clustering with LLM Embeddings", "author": "Alina Petukhova and Jo\u00e3o P. Matos-Carvalho and Nuno Fachada", "abstract": "  Text clustering is an important method for organising the increasing volume\nof digital content, aiding in the structuring and discovery of hidden patterns\nin uncategorised data. The effectiveness of text clustering largely depends on\nthe selection of textual embeddings and clustering algorithms. This study\nargues that recent advancements in large language models (LLMs) have the\npotential to enhance this task. The research investigates how different textual\nembeddings, particularly those utilised in LLMs, and various clustering\nalgorithms influence the clustering of text datasets. A series of experiments\nwere conducted to evaluate the impact of embeddings on clustering results, the\nrole of dimensionality reduction through summarisation, and the adjustment of\nmodel size. The findings indicate that LLM embeddings are superior at capturing\nsubtleties in structured language. OpenAI's GPT-3.5 Turbo model yields better\nresults in three out of five clustering metrics across most tested datasets.\nMost LLM embeddings show improvements in cluster purity and provide a more\ninformative silhouette score, reflecting a refined structural understanding of\ntext data compared to traditional methods. Among the more lightweight models,\nBERT demonstrates leading performance. Additionally, it was observed that\nincreasing model dimensionality and employing summarisation techniques do not\nconsistently enhance clustering efficiency, suggesting that these strategies\nrequire careful consideration for practical application. These results\nhighlight a complex balance between the need for refined text representation\nand computational feasibility in text clustering applications. This study\nextends traditional text clustering frameworks by integrating embeddings from\nLLMs, offering improved methodologies and suggesting new avenues for future\nresearch in various types of textual analysis.\n", "link": "http://arxiv.org/abs/2403.15112v4", "date": "2024-08-09", "relevancy": 1.9321, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4619}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text%20Clustering%20with%20LLM%20Embeddings&body=Title%3A%20Text%20Clustering%20with%20LLM%20Embeddings%0AAuthor%3A%20Alina%20Petukhova%20and%20Jo%C3%A3o%20P.%20Matos-Carvalho%20and%20Nuno%20Fachada%0AAbstract%3A%20%20%20Text%20clustering%20is%20an%20important%20method%20for%20organising%20the%20increasing%20volume%0Aof%20digital%20content%2C%20aiding%20in%20the%20structuring%20and%20discovery%20of%20hidden%20patterns%0Ain%20uncategorised%20data.%20The%20effectiveness%20of%20text%20clustering%20largely%20depends%20on%0Athe%20selection%20of%20textual%20embeddings%20and%20clustering%20algorithms.%20This%20study%0Aargues%20that%20recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20the%0Apotential%20to%20enhance%20this%20task.%20The%20research%20investigates%20how%20different%20textual%0Aembeddings%2C%20particularly%20those%20utilised%20in%20LLMs%2C%20and%20various%20clustering%0Aalgorithms%20influence%20the%20clustering%20of%20text%20datasets.%20A%20series%20of%20experiments%0Awere%20conducted%20to%20evaluate%20the%20impact%20of%20embeddings%20on%20clustering%20results%2C%20the%0Arole%20of%20dimensionality%20reduction%20through%20summarisation%2C%20and%20the%20adjustment%20of%0Amodel%20size.%20The%20findings%20indicate%20that%20LLM%20embeddings%20are%20superior%20at%20capturing%0Asubtleties%20in%20structured%20language.%20OpenAI%27s%20GPT-3.5%20Turbo%20model%20yields%20better%0Aresults%20in%20three%20out%20of%20five%20clustering%20metrics%20across%20most%20tested%20datasets.%0AMost%20LLM%20embeddings%20show%20improvements%20in%20cluster%20purity%20and%20provide%20a%20more%0Ainformative%20silhouette%20score%2C%20reflecting%20a%20refined%20structural%20understanding%20of%0Atext%20data%20compared%20to%20traditional%20methods.%20Among%20the%20more%20lightweight%20models%2C%0ABERT%20demonstrates%20leading%20performance.%20Additionally%2C%20it%20was%20observed%20that%0Aincreasing%20model%20dimensionality%20and%20employing%20summarisation%20techniques%20do%20not%0Aconsistently%20enhance%20clustering%20efficiency%2C%20suggesting%20that%20these%20strategies%0Arequire%20careful%20consideration%20for%20practical%20application.%20These%20results%0Ahighlight%20a%20complex%20balance%20between%20the%20need%20for%20refined%20text%20representation%0Aand%20computational%20feasibility%20in%20text%20clustering%20applications.%20This%20study%0Aextends%20traditional%20text%20clustering%20frameworks%20by%20integrating%20embeddings%20from%0ALLMs%2C%20offering%20improved%20methodologies%20and%20suggesting%20new%20avenues%20for%20future%0Aresearch%20in%20various%20types%20of%20textual%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15112v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText%2520Clustering%2520with%2520LLM%2520Embeddings%26entry.906535625%3DAlina%2520Petukhova%2520and%2520Jo%25C3%25A3o%2520P.%2520Matos-Carvalho%2520and%2520Nuno%2520Fachada%26entry.1292438233%3D%2520%2520Text%2520clustering%2520is%2520an%2520important%2520method%2520for%2520organising%2520the%2520increasing%2520volume%250Aof%2520digital%2520content%252C%2520aiding%2520in%2520the%2520structuring%2520and%2520discovery%2520of%2520hidden%2520patterns%250Ain%2520uncategorised%2520data.%2520The%2520effectiveness%2520of%2520text%2520clustering%2520largely%2520depends%2520on%250Athe%2520selection%2520of%2520textual%2520embeddings%2520and%2520clustering%2520algorithms.%2520This%2520study%250Aargues%2520that%2520recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520the%250Apotential%2520to%2520enhance%2520this%2520task.%2520The%2520research%2520investigates%2520how%2520different%2520textual%250Aembeddings%252C%2520particularly%2520those%2520utilised%2520in%2520LLMs%252C%2520and%2520various%2520clustering%250Aalgorithms%2520influence%2520the%2520clustering%2520of%2520text%2520datasets.%2520A%2520series%2520of%2520experiments%250Awere%2520conducted%2520to%2520evaluate%2520the%2520impact%2520of%2520embeddings%2520on%2520clustering%2520results%252C%2520the%250Arole%2520of%2520dimensionality%2520reduction%2520through%2520summarisation%252C%2520and%2520the%2520adjustment%2520of%250Amodel%2520size.%2520The%2520findings%2520indicate%2520that%2520LLM%2520embeddings%2520are%2520superior%2520at%2520capturing%250Asubtleties%2520in%2520structured%2520language.%2520OpenAI%2527s%2520GPT-3.5%2520Turbo%2520model%2520yields%2520better%250Aresults%2520in%2520three%2520out%2520of%2520five%2520clustering%2520metrics%2520across%2520most%2520tested%2520datasets.%250AMost%2520LLM%2520embeddings%2520show%2520improvements%2520in%2520cluster%2520purity%2520and%2520provide%2520a%2520more%250Ainformative%2520silhouette%2520score%252C%2520reflecting%2520a%2520refined%2520structural%2520understanding%2520of%250Atext%2520data%2520compared%2520to%2520traditional%2520methods.%2520Among%2520the%2520more%2520lightweight%2520models%252C%250ABERT%2520demonstrates%2520leading%2520performance.%2520Additionally%252C%2520it%2520was%2520observed%2520that%250Aincreasing%2520model%2520dimensionality%2520and%2520employing%2520summarisation%2520techniques%2520do%2520not%250Aconsistently%2520enhance%2520clustering%2520efficiency%252C%2520suggesting%2520that%2520these%2520strategies%250Arequire%2520careful%2520consideration%2520for%2520practical%2520application.%2520These%2520results%250Ahighlight%2520a%2520complex%2520balance%2520between%2520the%2520need%2520for%2520refined%2520text%2520representation%250Aand%2520computational%2520feasibility%2520in%2520text%2520clustering%2520applications.%2520This%2520study%250Aextends%2520traditional%2520text%2520clustering%2520frameworks%2520by%2520integrating%2520embeddings%2520from%250ALLMs%252C%2520offering%2520improved%2520methodologies%2520and%2520suggesting%2520new%2520avenues%2520for%2520future%250Aresearch%2520in%2520various%2520types%2520of%2520textual%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15112v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20Clustering%20with%20LLM%20Embeddings&entry.906535625=Alina%20Petukhova%20and%20Jo%C3%A3o%20P.%20Matos-Carvalho%20and%20Nuno%20Fachada&entry.1292438233=%20%20Text%20clustering%20is%20an%20important%20method%20for%20organising%20the%20increasing%20volume%0Aof%20digital%20content%2C%20aiding%20in%20the%20structuring%20and%20discovery%20of%20hidden%20patterns%0Ain%20uncategorised%20data.%20The%20effectiveness%20of%20text%20clustering%20largely%20depends%20on%0Athe%20selection%20of%20textual%20embeddings%20and%20clustering%20algorithms.%20This%20study%0Aargues%20that%20recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20the%0Apotential%20to%20enhance%20this%20task.%20The%20research%20investigates%20how%20different%20textual%0Aembeddings%2C%20particularly%20those%20utilised%20in%20LLMs%2C%20and%20various%20clustering%0Aalgorithms%20influence%20the%20clustering%20of%20text%20datasets.%20A%20series%20of%20experiments%0Awere%20conducted%20to%20evaluate%20the%20impact%20of%20embeddings%20on%20clustering%20results%2C%20the%0Arole%20of%20dimensionality%20reduction%20through%20summarisation%2C%20and%20the%20adjustment%20of%0Amodel%20size.%20The%20findings%20indicate%20that%20LLM%20embeddings%20are%20superior%20at%20capturing%0Asubtleties%20in%20structured%20language.%20OpenAI%27s%20GPT-3.5%20Turbo%20model%20yields%20better%0Aresults%20in%20three%20out%20of%20five%20clustering%20metrics%20across%20most%20tested%20datasets.%0AMost%20LLM%20embeddings%20show%20improvements%20in%20cluster%20purity%20and%20provide%20a%20more%0Ainformative%20silhouette%20score%2C%20reflecting%20a%20refined%20structural%20understanding%20of%0Atext%20data%20compared%20to%20traditional%20methods.%20Among%20the%20more%20lightweight%20models%2C%0ABERT%20demonstrates%20leading%20performance.%20Additionally%2C%20it%20was%20observed%20that%0Aincreasing%20model%20dimensionality%20and%20employing%20summarisation%20techniques%20do%20not%0Aconsistently%20enhance%20clustering%20efficiency%2C%20suggesting%20that%20these%20strategies%0Arequire%20careful%20consideration%20for%20practical%20application.%20These%20results%0Ahighlight%20a%20complex%20balance%20between%20the%20need%20for%20refined%20text%20representation%0Aand%20computational%20feasibility%20in%20text%20clustering%20applications.%20This%20study%0Aextends%20traditional%20text%20clustering%20frameworks%20by%20integrating%20embeddings%20from%0ALLMs%2C%20offering%20improved%20methodologies%20and%20suggesting%20new%20avenues%20for%20future%0Aresearch%20in%20various%20types%20of%20textual%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15112v4&entry.124074799=Read"},
{"title": "Eliciting Latent Knowledge from Quirky Language Models", "author": "Alex Mallen and Madeline Brumley and Julia Kharchenko and Nora Belrose", "abstract": "  Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural\nnetwork's activations that robustly track the true state of the world,\nespecially in hard-to-verify cases where the model's output is untrusted. To\nfurther ELK research, we introduce 12 datasets and a corresponding suite of\n\"quirky\" language models (LMs) that are finetuned to make systematic errors\nwhen answering questions if and only if the keyword \"Bob\" is present in the\nprompt. We find that, especially in middle layers, linear probes usually report\nan LM's knowledge independently of what the LM outputs, enabling us to elicit\nthe correct answer despite the model's untruthful output. The best probing\nmethod (logistic regression on contrast pairs) recovers 89% of the gap in AUROC\nbetween truthful and untruthful contexts, and 75% for questions harder than\nthose used to train the probe. We also find that a mechanistic anomaly\ndetection approach can flag untruthful behavior with 0.95 AUROC. Our results\nshow promise for eliciting reliable knowledge from capable but untrusted\nmodels, and facilitates future research empirically investigating ELK methods.\n", "link": "http://arxiv.org/abs/2312.01037v4", "date": "2024-08-09", "relevancy": 1.9239, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5229}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4807}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eliciting%20Latent%20Knowledge%20from%20Quirky%20Language%20Models&body=Title%3A%20Eliciting%20Latent%20Knowledge%20from%20Quirky%20Language%20Models%0AAuthor%3A%20Alex%20Mallen%20and%20Madeline%20Brumley%20and%20Julia%20Kharchenko%20and%20Nora%20Belrose%0AAbstract%3A%20%20%20Eliciting%20Latent%20Knowledge%20%28ELK%29%20aims%20to%20find%20patterns%20in%20a%20capable%20neural%0Anetwork%27s%20activations%20that%20robustly%20track%20the%20true%20state%20of%20the%20world%2C%0Aespecially%20in%20hard-to-verify%20cases%20where%20the%20model%27s%20output%20is%20untrusted.%20To%0Afurther%20ELK%20research%2C%20we%20introduce%2012%20datasets%20and%20a%20corresponding%20suite%20of%0A%22quirky%22%20language%20models%20%28LMs%29%20that%20are%20finetuned%20to%20make%20systematic%20errors%0Awhen%20answering%20questions%20if%20and%20only%20if%20the%20keyword%20%22Bob%22%20is%20present%20in%20the%0Aprompt.%20We%20find%20that%2C%20especially%20in%20middle%20layers%2C%20linear%20probes%20usually%20report%0Aan%20LM%27s%20knowledge%20independently%20of%20what%20the%20LM%20outputs%2C%20enabling%20us%20to%20elicit%0Athe%20correct%20answer%20despite%20the%20model%27s%20untruthful%20output.%20The%20best%20probing%0Amethod%20%28logistic%20regression%20on%20contrast%20pairs%29%20recovers%2089%25%20of%20the%20gap%20in%20AUROC%0Abetween%20truthful%20and%20untruthful%20contexts%2C%20and%2075%25%20for%20questions%20harder%20than%0Athose%20used%20to%20train%20the%20probe.%20We%20also%20find%20that%20a%20mechanistic%20anomaly%0Adetection%20approach%20can%20flag%20untruthful%20behavior%20with%200.95%20AUROC.%20Our%20results%0Ashow%20promise%20for%20eliciting%20reliable%20knowledge%20from%20capable%20but%20untrusted%0Amodels%2C%20and%20facilitates%20future%20research%20empirically%20investigating%20ELK%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01037v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEliciting%2520Latent%2520Knowledge%2520from%2520Quirky%2520Language%2520Models%26entry.906535625%3DAlex%2520Mallen%2520and%2520Madeline%2520Brumley%2520and%2520Julia%2520Kharchenko%2520and%2520Nora%2520Belrose%26entry.1292438233%3D%2520%2520Eliciting%2520Latent%2520Knowledge%2520%2528ELK%2529%2520aims%2520to%2520find%2520patterns%2520in%2520a%2520capable%2520neural%250Anetwork%2527s%2520activations%2520that%2520robustly%2520track%2520the%2520true%2520state%2520of%2520the%2520world%252C%250Aespecially%2520in%2520hard-to-verify%2520cases%2520where%2520the%2520model%2527s%2520output%2520is%2520untrusted.%2520To%250Afurther%2520ELK%2520research%252C%2520we%2520introduce%252012%2520datasets%2520and%2520a%2520corresponding%2520suite%2520of%250A%2522quirky%2522%2520language%2520models%2520%2528LMs%2529%2520that%2520are%2520finetuned%2520to%2520make%2520systematic%2520errors%250Awhen%2520answering%2520questions%2520if%2520and%2520only%2520if%2520the%2520keyword%2520%2522Bob%2522%2520is%2520present%2520in%2520the%250Aprompt.%2520We%2520find%2520that%252C%2520especially%2520in%2520middle%2520layers%252C%2520linear%2520probes%2520usually%2520report%250Aan%2520LM%2527s%2520knowledge%2520independently%2520of%2520what%2520the%2520LM%2520outputs%252C%2520enabling%2520us%2520to%2520elicit%250Athe%2520correct%2520answer%2520despite%2520the%2520model%2527s%2520untruthful%2520output.%2520The%2520best%2520probing%250Amethod%2520%2528logistic%2520regression%2520on%2520contrast%2520pairs%2529%2520recovers%252089%2525%2520of%2520the%2520gap%2520in%2520AUROC%250Abetween%2520truthful%2520and%2520untruthful%2520contexts%252C%2520and%252075%2525%2520for%2520questions%2520harder%2520than%250Athose%2520used%2520to%2520train%2520the%2520probe.%2520We%2520also%2520find%2520that%2520a%2520mechanistic%2520anomaly%250Adetection%2520approach%2520can%2520flag%2520untruthful%2520behavior%2520with%25200.95%2520AUROC.%2520Our%2520results%250Ashow%2520promise%2520for%2520eliciting%2520reliable%2520knowledge%2520from%2520capable%2520but%2520untrusted%250Amodels%252C%2520and%2520facilitates%2520future%2520research%2520empirically%2520investigating%2520ELK%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01037v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eliciting%20Latent%20Knowledge%20from%20Quirky%20Language%20Models&entry.906535625=Alex%20Mallen%20and%20Madeline%20Brumley%20and%20Julia%20Kharchenko%20and%20Nora%20Belrose&entry.1292438233=%20%20Eliciting%20Latent%20Knowledge%20%28ELK%29%20aims%20to%20find%20patterns%20in%20a%20capable%20neural%0Anetwork%27s%20activations%20that%20robustly%20track%20the%20true%20state%20of%20the%20world%2C%0Aespecially%20in%20hard-to-verify%20cases%20where%20the%20model%27s%20output%20is%20untrusted.%20To%0Afurther%20ELK%20research%2C%20we%20introduce%2012%20datasets%20and%20a%20corresponding%20suite%20of%0A%22quirky%22%20language%20models%20%28LMs%29%20that%20are%20finetuned%20to%20make%20systematic%20errors%0Awhen%20answering%20questions%20if%20and%20only%20if%20the%20keyword%20%22Bob%22%20is%20present%20in%20the%0Aprompt.%20We%20find%20that%2C%20especially%20in%20middle%20layers%2C%20linear%20probes%20usually%20report%0Aan%20LM%27s%20knowledge%20independently%20of%20what%20the%20LM%20outputs%2C%20enabling%20us%20to%20elicit%0Athe%20correct%20answer%20despite%20the%20model%27s%20untruthful%20output.%20The%20best%20probing%0Amethod%20%28logistic%20regression%20on%20contrast%20pairs%29%20recovers%2089%25%20of%20the%20gap%20in%20AUROC%0Abetween%20truthful%20and%20untruthful%20contexts%2C%20and%2075%25%20for%20questions%20harder%20than%0Athose%20used%20to%20train%20the%20probe.%20We%20also%20find%20that%20a%20mechanistic%20anomaly%0Adetection%20approach%20can%20flag%20untruthful%20behavior%20with%200.95%20AUROC.%20Our%20results%0Ashow%20promise%20for%20eliciting%20reliable%20knowledge%20from%20capable%20but%20untrusted%0Amodels%2C%20and%20facilitates%20future%20research%20empirically%20investigating%20ELK%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01037v4&entry.124074799=Read"},
{"title": "Retrieval-augmented code completion for local projects using large\n  language models", "author": "Marko Hostnik and Marko Robnik-\u0160ikonja", "abstract": "  The use of large language models (LLMs) is becoming increasingly widespread\namong software developers. However, privacy and computational requirements are\nproblematic with commercial solutions and the use of LLMs. In this work, we\nfocus on using LLMs with around 160 million parameters that are suitable for\nlocal execution and augmentation with retrieval from local projects. We train\ntwo models based on the transformer architecture, the generative model GPT-2\nand the retrieval-adapted RETRO model, on open-source Python files, and\nempirically evaluate and compare them, confirming the benefits of vector\nembedding based retrieval. Further, we improve our models' performance with\nIn-context retrieval-augmented generation, which retrieves code snippets based\non the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented\ngeneration on larger models and conclude that, despite its simplicity, the\napproach is more suitable than using the RETRO architecture. We highlight the\nkey role of proper tokenization in achieving the full potential of LLMs in code\ncompletion.\n", "link": "http://arxiv.org/abs/2408.05026v1", "date": "2024-08-09", "relevancy": 1.9114, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4791}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-augmented%20code%20completion%20for%20local%20projects%20using%20large%0A%20%20language%20models&body=Title%3A%20Retrieval-augmented%20code%20completion%20for%20local%20projects%20using%20large%0A%20%20language%20models%0AAuthor%3A%20Marko%20Hostnik%20and%20Marko%20Robnik-%C5%A0ikonja%0AAbstract%3A%20%20%20The%20use%20of%20large%20language%20models%20%28LLMs%29%20is%20becoming%20increasingly%20widespread%0Aamong%20software%20developers.%20However%2C%20privacy%20and%20computational%20requirements%20are%0Aproblematic%20with%20commercial%20solutions%20and%20the%20use%20of%20LLMs.%20In%20this%20work%2C%20we%0Afocus%20on%20using%20LLMs%20with%20around%20160%20million%20parameters%20that%20are%20suitable%20for%0Alocal%20execution%20and%20augmentation%20with%20retrieval%20from%20local%20projects.%20We%20train%0Atwo%20models%20based%20on%20the%20transformer%20architecture%2C%20the%20generative%20model%20GPT-2%0Aand%20the%20retrieval-adapted%20RETRO%20model%2C%20on%20open-source%20Python%20files%2C%20and%0Aempirically%20evaluate%20and%20compare%20them%2C%20confirming%20the%20benefits%20of%20vector%0Aembedding%20based%20retrieval.%20Further%2C%20we%20improve%20our%20models%27%20performance%20with%0AIn-context%20retrieval-augmented%20generation%2C%20which%20retrieves%20code%20snippets%20based%0Aon%20the%20Jaccard%20similarity%20of%20tokens.%20We%20evaluate%20In-context%20retrieval-augmented%0Ageneration%20on%20larger%20models%20and%20conclude%20that%2C%20despite%20its%20simplicity%2C%20the%0Aapproach%20is%20more%20suitable%20than%20using%20the%20RETRO%20architecture.%20We%20highlight%20the%0Akey%20role%20of%20proper%20tokenization%20in%20achieving%20the%20full%20potential%20of%20LLMs%20in%20code%0Acompletion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-augmented%2520code%2520completion%2520for%2520local%2520projects%2520using%2520large%250A%2520%2520language%2520models%26entry.906535625%3DMarko%2520Hostnik%2520and%2520Marko%2520Robnik-%25C5%25A0ikonja%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520becoming%2520increasingly%2520widespread%250Aamong%2520software%2520developers.%2520However%252C%2520privacy%2520and%2520computational%2520requirements%2520are%250Aproblematic%2520with%2520commercial%2520solutions%2520and%2520the%2520use%2520of%2520LLMs.%2520In%2520this%2520work%252C%2520we%250Afocus%2520on%2520using%2520LLMs%2520with%2520around%2520160%2520million%2520parameters%2520that%2520are%2520suitable%2520for%250Alocal%2520execution%2520and%2520augmentation%2520with%2520retrieval%2520from%2520local%2520projects.%2520We%2520train%250Atwo%2520models%2520based%2520on%2520the%2520transformer%2520architecture%252C%2520the%2520generative%2520model%2520GPT-2%250Aand%2520the%2520retrieval-adapted%2520RETRO%2520model%252C%2520on%2520open-source%2520Python%2520files%252C%2520and%250Aempirically%2520evaluate%2520and%2520compare%2520them%252C%2520confirming%2520the%2520benefits%2520of%2520vector%250Aembedding%2520based%2520retrieval.%2520Further%252C%2520we%2520improve%2520our%2520models%2527%2520performance%2520with%250AIn-context%2520retrieval-augmented%2520generation%252C%2520which%2520retrieves%2520code%2520snippets%2520based%250Aon%2520the%2520Jaccard%2520similarity%2520of%2520tokens.%2520We%2520evaluate%2520In-context%2520retrieval-augmented%250Ageneration%2520on%2520larger%2520models%2520and%2520conclude%2520that%252C%2520despite%2520its%2520simplicity%252C%2520the%250Aapproach%2520is%2520more%2520suitable%2520than%2520using%2520the%2520RETRO%2520architecture.%2520We%2520highlight%2520the%250Akey%2520role%2520of%2520proper%2520tokenization%2520in%2520achieving%2520the%2520full%2520potential%2520of%2520LLMs%2520in%2520code%250Acompletion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-augmented%20code%20completion%20for%20local%20projects%20using%20large%0A%20%20language%20models&entry.906535625=Marko%20Hostnik%20and%20Marko%20Robnik-%C5%A0ikonja&entry.1292438233=%20%20The%20use%20of%20large%20language%20models%20%28LLMs%29%20is%20becoming%20increasingly%20widespread%0Aamong%20software%20developers.%20However%2C%20privacy%20and%20computational%20requirements%20are%0Aproblematic%20with%20commercial%20solutions%20and%20the%20use%20of%20LLMs.%20In%20this%20work%2C%20we%0Afocus%20on%20using%20LLMs%20with%20around%20160%20million%20parameters%20that%20are%20suitable%20for%0Alocal%20execution%20and%20augmentation%20with%20retrieval%20from%20local%20projects.%20We%20train%0Atwo%20models%20based%20on%20the%20transformer%20architecture%2C%20the%20generative%20model%20GPT-2%0Aand%20the%20retrieval-adapted%20RETRO%20model%2C%20on%20open-source%20Python%20files%2C%20and%0Aempirically%20evaluate%20and%20compare%20them%2C%20confirming%20the%20benefits%20of%20vector%0Aembedding%20based%20retrieval.%20Further%2C%20we%20improve%20our%20models%27%20performance%20with%0AIn-context%20retrieval-augmented%20generation%2C%20which%20retrieves%20code%20snippets%20based%0Aon%20the%20Jaccard%20similarity%20of%20tokens.%20We%20evaluate%20In-context%20retrieval-augmented%0Ageneration%20on%20larger%20models%20and%20conclude%20that%2C%20despite%20its%20simplicity%2C%20the%0Aapproach%20is%20more%20suitable%20than%20using%20the%20RETRO%20architecture.%20We%20highlight%20the%0Akey%20role%20of%20proper%20tokenization%20in%20achieving%20the%20full%20potential%20of%20LLMs%20in%20code%0Acompletion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05026v1&entry.124074799=Read"},
{"title": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting", "author": "Huihan Li and Liwei Jiang and Jena D. Hwang and Hyunwoo Kim and Sebastin Santy and Taylor Sorensen and Bill Yuchen Lin and Nouha Dziri and Xiang Ren and Yejin Choi", "abstract": "  As the utilization of large language models (LLMs) has proliferated\nworld-wide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found here: https://github.com/huihanlhh/Culture-Gen/\n", "link": "http://arxiv.org/abs/2404.10199v4", "date": "2024-08-09", "relevancy": 1.8734, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4833}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4632}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CULTURE-GEN%3A%20Revealing%20Global%20Cultural%20Perception%20in%20Language%20Models%0A%20%20through%20Natural%20Language%20Prompting&body=Title%3A%20CULTURE-GEN%3A%20Revealing%20Global%20Cultural%20Perception%20in%20Language%20Models%0A%20%20through%20Natural%20Language%20Prompting%0AAuthor%3A%20Huihan%20Li%20and%20Liwei%20Jiang%20and%20Jena%20D.%20Hwang%20and%20Hyunwoo%20Kim%20and%20Sebastin%20Santy%20and%20Taylor%20Sorensen%20and%20Bill%20Yuchen%20Lin%20and%20Nouha%20Dziri%20and%20Xiang%20Ren%20and%20Yejin%20Choi%0AAbstract%3A%20%20%20As%20the%20utilization%20of%20large%20language%20models%20%28LLMs%29%20has%20proliferated%0Aworld-wide%2C%20it%20is%20crucial%20for%20them%20to%20have%20adequate%20knowledge%20and%20fair%0Arepresentation%20for%20diverse%20global%20cultures.%20In%20this%20work%2C%20we%20uncover%20culture%0Aperceptions%20of%20three%20SOTA%20models%20on%20110%20countries%20and%20regions%20on%208%0Aculture-related%20topics%20through%20culture-conditioned%20generations%2C%20and%20extract%0Asymbols%20from%20these%20generations%20that%20are%20associated%20to%20each%20culture%20by%20the%20LLM.%0AWe%20discover%20that%20culture-conditioned%20generation%20consist%20of%20linguistic%20%22markers%22%0Athat%20distinguish%20marginalized%20cultures%20apart%20from%20default%20cultures.%20We%20also%0Adiscover%20that%20LLMs%20have%20an%20uneven%20degree%20of%20diversity%20in%20the%20culture%20symbols%2C%0Aand%20that%20cultures%20from%20different%20geographic%20regions%20have%20different%20presence%20in%0ALLMs%27%20culture-agnostic%20generation.%20Our%20findings%20promote%20further%20research%20in%0Astudying%20the%20knowledge%20and%20fairness%20of%20global%20culture%20perception%20in%20LLMs.%20Code%0Aand%20Data%20can%20be%20found%20here%3A%20https%3A//github.com/huihanlhh/Culture-Gen/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10199v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCULTURE-GEN%253A%2520Revealing%2520Global%2520Cultural%2520Perception%2520in%2520Language%2520Models%250A%2520%2520through%2520Natural%2520Language%2520Prompting%26entry.906535625%3DHuihan%2520Li%2520and%2520Liwei%2520Jiang%2520and%2520Jena%2520D.%2520Hwang%2520and%2520Hyunwoo%2520Kim%2520and%2520Sebastin%2520Santy%2520and%2520Taylor%2520Sorensen%2520and%2520Bill%2520Yuchen%2520Lin%2520and%2520Nouha%2520Dziri%2520and%2520Xiang%2520Ren%2520and%2520Yejin%2520Choi%26entry.1292438233%3D%2520%2520As%2520the%2520utilization%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520proliferated%250Aworld-wide%252C%2520it%2520is%2520crucial%2520for%2520them%2520to%2520have%2520adequate%2520knowledge%2520and%2520fair%250Arepresentation%2520for%2520diverse%2520global%2520cultures.%2520In%2520this%2520work%252C%2520we%2520uncover%2520culture%250Aperceptions%2520of%2520three%2520SOTA%2520models%2520on%2520110%2520countries%2520and%2520regions%2520on%25208%250Aculture-related%2520topics%2520through%2520culture-conditioned%2520generations%252C%2520and%2520extract%250Asymbols%2520from%2520these%2520generations%2520that%2520are%2520associated%2520to%2520each%2520culture%2520by%2520the%2520LLM.%250AWe%2520discover%2520that%2520culture-conditioned%2520generation%2520consist%2520of%2520linguistic%2520%2522markers%2522%250Athat%2520distinguish%2520marginalized%2520cultures%2520apart%2520from%2520default%2520cultures.%2520We%2520also%250Adiscover%2520that%2520LLMs%2520have%2520an%2520uneven%2520degree%2520of%2520diversity%2520in%2520the%2520culture%2520symbols%252C%250Aand%2520that%2520cultures%2520from%2520different%2520geographic%2520regions%2520have%2520different%2520presence%2520in%250ALLMs%2527%2520culture-agnostic%2520generation.%2520Our%2520findings%2520promote%2520further%2520research%2520in%250Astudying%2520the%2520knowledge%2520and%2520fairness%2520of%2520global%2520culture%2520perception%2520in%2520LLMs.%2520Code%250Aand%2520Data%2520can%2520be%2520found%2520here%253A%2520https%253A//github.com/huihanlhh/Culture-Gen/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10199v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CULTURE-GEN%3A%20Revealing%20Global%20Cultural%20Perception%20in%20Language%20Models%0A%20%20through%20Natural%20Language%20Prompting&entry.906535625=Huihan%20Li%20and%20Liwei%20Jiang%20and%20Jena%20D.%20Hwang%20and%20Hyunwoo%20Kim%20and%20Sebastin%20Santy%20and%20Taylor%20Sorensen%20and%20Bill%20Yuchen%20Lin%20and%20Nouha%20Dziri%20and%20Xiang%20Ren%20and%20Yejin%20Choi&entry.1292438233=%20%20As%20the%20utilization%20of%20large%20language%20models%20%28LLMs%29%20has%20proliferated%0Aworld-wide%2C%20it%20is%20crucial%20for%20them%20to%20have%20adequate%20knowledge%20and%20fair%0Arepresentation%20for%20diverse%20global%20cultures.%20In%20this%20work%2C%20we%20uncover%20culture%0Aperceptions%20of%20three%20SOTA%20models%20on%20110%20countries%20and%20regions%20on%208%0Aculture-related%20topics%20through%20culture-conditioned%20generations%2C%20and%20extract%0Asymbols%20from%20these%20generations%20that%20are%20associated%20to%20each%20culture%20by%20the%20LLM.%0AWe%20discover%20that%20culture-conditioned%20generation%20consist%20of%20linguistic%20%22markers%22%0Athat%20distinguish%20marginalized%20cultures%20apart%20from%20default%20cultures.%20We%20also%0Adiscover%20that%20LLMs%20have%20an%20uneven%20degree%20of%20diversity%20in%20the%20culture%20symbols%2C%0Aand%20that%20cultures%20from%20different%20geographic%20regions%20have%20different%20presence%20in%0ALLMs%27%20culture-agnostic%20generation.%20Our%20findings%20promote%20further%20research%20in%0Astudying%20the%20knowledge%20and%20fairness%20of%20global%20culture%20perception%20in%20LLMs.%20Code%0Aand%20Data%20can%20be%20found%20here%3A%20https%3A//github.com/huihanlhh/Culture-Gen/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10199v4&entry.124074799=Read"},
{"title": "A GNN Model with Adaptive Weights for Session-Based Recommendation\n  Systems", "author": "Beg\u00fcm \u00d6zbay and Dr. Resul Tugay and Prof. Dr. \u015eule G\u00fcnd\u00fcz \u00d6\u011f\u00fcd\u00fcc\u00fc", "abstract": "  Session-based recommendation systems aim to model users' interests based on\ntheir sequential interactions to predict the next item in an ongoing session.\nIn this work, we present a novel approach that can be used in session-based\nrecommendations (SBRs). Our goal is to enhance the prediction accuracy of an\nexisting session-based recommendation model, the SR-GNN model, by introducing\nan adaptive weighting mechanism applied to the graph neural network (GNN)\nvectors. This mechanism is designed to incorporate various types of side\ninformation obtained through different methods during the study. Items are\nassigned varying degrees of importance within each session as a result of the\nweighting mechanism. We hypothesize that this adaptive weighting strategy will\ncontribute to more accurate predictions and thus improve the overall\nperformance of SBRs in different scenarios. The adaptive weighting strategy can\nbe utilized to address the cold start problem in SBRs by dynamically adjusting\nthe importance of items in each session, thus providing better recommendations\nin cold start situations, such as for new users or newly added items. Our\nexperimental evaluations on the Dressipi dataset demonstrate the effectiveness\nof the proposed approach compared to traditional models in enhancing the user\nexperience and highlighting its potential to optimize the recommendation\nresults in real-world applications.\n", "link": "http://arxiv.org/abs/2408.05051v1", "date": "2024-08-09", "relevancy": 1.8604, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4794}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4695}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20GNN%20Model%20with%20Adaptive%20Weights%20for%20Session-Based%20Recommendation%0A%20%20Systems&body=Title%3A%20A%20GNN%20Model%20with%20Adaptive%20Weights%20for%20Session-Based%20Recommendation%0A%20%20Systems%0AAuthor%3A%20Beg%C3%BCm%20%C3%96zbay%20and%20Dr.%20Resul%20Tugay%20and%20Prof.%20Dr.%20%C5%9Eule%20G%C3%BCnd%C3%BCz%20%C3%96%C4%9F%C3%BCd%C3%BCc%C3%BC%0AAbstract%3A%20%20%20Session-based%20recommendation%20systems%20aim%20to%20model%20users%27%20interests%20based%20on%0Atheir%20sequential%20interactions%20to%20predict%20the%20next%20item%20in%20an%20ongoing%20session.%0AIn%20this%20work%2C%20we%20present%20a%20novel%20approach%20that%20can%20be%20used%20in%20session-based%0Arecommendations%20%28SBRs%29.%20Our%20goal%20is%20to%20enhance%20the%20prediction%20accuracy%20of%20an%0Aexisting%20session-based%20recommendation%20model%2C%20the%20SR-GNN%20model%2C%20by%20introducing%0Aan%20adaptive%20weighting%20mechanism%20applied%20to%20the%20graph%20neural%20network%20%28GNN%29%0Avectors.%20This%20mechanism%20is%20designed%20to%20incorporate%20various%20types%20of%20side%0Ainformation%20obtained%20through%20different%20methods%20during%20the%20study.%20Items%20are%0Aassigned%20varying%20degrees%20of%20importance%20within%20each%20session%20as%20a%20result%20of%20the%0Aweighting%20mechanism.%20We%20hypothesize%20that%20this%20adaptive%20weighting%20strategy%20will%0Acontribute%20to%20more%20accurate%20predictions%20and%20thus%20improve%20the%20overall%0Aperformance%20of%20SBRs%20in%20different%20scenarios.%20The%20adaptive%20weighting%20strategy%20can%0Abe%20utilized%20to%20address%20the%20cold%20start%20problem%20in%20SBRs%20by%20dynamically%20adjusting%0Athe%20importance%20of%20items%20in%20each%20session%2C%20thus%20providing%20better%20recommendations%0Ain%20cold%20start%20situations%2C%20such%20as%20for%20new%20users%20or%20newly%20added%20items.%20Our%0Aexperimental%20evaluations%20on%20the%20Dressipi%20dataset%20demonstrate%20the%20effectiveness%0Aof%20the%20proposed%20approach%20compared%20to%20traditional%20models%20in%20enhancing%20the%20user%0Aexperience%20and%20highlighting%20its%20potential%20to%20optimize%20the%20recommendation%0Aresults%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520GNN%2520Model%2520with%2520Adaptive%2520Weights%2520for%2520Session-Based%2520Recommendation%250A%2520%2520Systems%26entry.906535625%3DBeg%25C3%25BCm%2520%25C3%2596zbay%2520and%2520Dr.%2520Resul%2520Tugay%2520and%2520Prof.%2520Dr.%2520%25C5%259Eule%2520G%25C3%25BCnd%25C3%25BCz%2520%25C3%2596%25C4%259F%25C3%25BCd%25C3%25BCc%25C3%25BC%26entry.1292438233%3D%2520%2520Session-based%2520recommendation%2520systems%2520aim%2520to%2520model%2520users%2527%2520interests%2520based%2520on%250Atheir%2520sequential%2520interactions%2520to%2520predict%2520the%2520next%2520item%2520in%2520an%2520ongoing%2520session.%250AIn%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520approach%2520that%2520can%2520be%2520used%2520in%2520session-based%250Arecommendations%2520%2528SBRs%2529.%2520Our%2520goal%2520is%2520to%2520enhance%2520the%2520prediction%2520accuracy%2520of%2520an%250Aexisting%2520session-based%2520recommendation%2520model%252C%2520the%2520SR-GNN%2520model%252C%2520by%2520introducing%250Aan%2520adaptive%2520weighting%2520mechanism%2520applied%2520to%2520the%2520graph%2520neural%2520network%2520%2528GNN%2529%250Avectors.%2520This%2520mechanism%2520is%2520designed%2520to%2520incorporate%2520various%2520types%2520of%2520side%250Ainformation%2520obtained%2520through%2520different%2520methods%2520during%2520the%2520study.%2520Items%2520are%250Aassigned%2520varying%2520degrees%2520of%2520importance%2520within%2520each%2520session%2520as%2520a%2520result%2520of%2520the%250Aweighting%2520mechanism.%2520We%2520hypothesize%2520that%2520this%2520adaptive%2520weighting%2520strategy%2520will%250Acontribute%2520to%2520more%2520accurate%2520predictions%2520and%2520thus%2520improve%2520the%2520overall%250Aperformance%2520of%2520SBRs%2520in%2520different%2520scenarios.%2520The%2520adaptive%2520weighting%2520strategy%2520can%250Abe%2520utilized%2520to%2520address%2520the%2520cold%2520start%2520problem%2520in%2520SBRs%2520by%2520dynamically%2520adjusting%250Athe%2520importance%2520of%2520items%2520in%2520each%2520session%252C%2520thus%2520providing%2520better%2520recommendations%250Ain%2520cold%2520start%2520situations%252C%2520such%2520as%2520for%2520new%2520users%2520or%2520newly%2520added%2520items.%2520Our%250Aexperimental%2520evaluations%2520on%2520the%2520Dressipi%2520dataset%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520approach%2520compared%2520to%2520traditional%2520models%2520in%2520enhancing%2520the%2520user%250Aexperience%2520and%2520highlighting%2520its%2520potential%2520to%2520optimize%2520the%2520recommendation%250Aresults%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20GNN%20Model%20with%20Adaptive%20Weights%20for%20Session-Based%20Recommendation%0A%20%20Systems&entry.906535625=Beg%C3%BCm%20%C3%96zbay%20and%20Dr.%20Resul%20Tugay%20and%20Prof.%20Dr.%20%C5%9Eule%20G%C3%BCnd%C3%BCz%20%C3%96%C4%9F%C3%BCd%C3%BCc%C3%BC&entry.1292438233=%20%20Session-based%20recommendation%20systems%20aim%20to%20model%20users%27%20interests%20based%20on%0Atheir%20sequential%20interactions%20to%20predict%20the%20next%20item%20in%20an%20ongoing%20session.%0AIn%20this%20work%2C%20we%20present%20a%20novel%20approach%20that%20can%20be%20used%20in%20session-based%0Arecommendations%20%28SBRs%29.%20Our%20goal%20is%20to%20enhance%20the%20prediction%20accuracy%20of%20an%0Aexisting%20session-based%20recommendation%20model%2C%20the%20SR-GNN%20model%2C%20by%20introducing%0Aan%20adaptive%20weighting%20mechanism%20applied%20to%20the%20graph%20neural%20network%20%28GNN%29%0Avectors.%20This%20mechanism%20is%20designed%20to%20incorporate%20various%20types%20of%20side%0Ainformation%20obtained%20through%20different%20methods%20during%20the%20study.%20Items%20are%0Aassigned%20varying%20degrees%20of%20importance%20within%20each%20session%20as%20a%20result%20of%20the%0Aweighting%20mechanism.%20We%20hypothesize%20that%20this%20adaptive%20weighting%20strategy%20will%0Acontribute%20to%20more%20accurate%20predictions%20and%20thus%20improve%20the%20overall%0Aperformance%20of%20SBRs%20in%20different%20scenarios.%20The%20adaptive%20weighting%20strategy%20can%0Abe%20utilized%20to%20address%20the%20cold%20start%20problem%20in%20SBRs%20by%20dynamically%20adjusting%0Athe%20importance%20of%20items%20in%20each%20session%2C%20thus%20providing%20better%20recommendations%0Ain%20cold%20start%20situations%2C%20such%20as%20for%20new%20users%20or%20newly%20added%20items.%20Our%0Aexperimental%20evaluations%20on%20the%20Dressipi%20dataset%20demonstrate%20the%20effectiveness%0Aof%20the%20proposed%20approach%20compared%20to%20traditional%20models%20in%20enhancing%20the%20user%0Aexperience%20and%20highlighting%20its%20potential%20to%20optimize%20the%20recommendation%0Aresults%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05051v1&entry.124074799=Read"},
{"title": "DreamCouple: Exploring High Quality Text-to-3D Generation Via Rectified\n  Flow", "author": "Hangyu Li and Xiangxiang Chu and Dingyuan Shi", "abstract": "  The Score Distillation Sampling (SDS), which exploits pretrained\ntext-to-image model diffusion models as priors to 3D model training, has\nachieved significant success. Currently, the flow-based diffusion model has\nbecome a new trend for generations. Yet, adapting SDS to flow-based diffusion\nmodels in 3D generation remains unexplored. Our work is aimed to bridge this\ngap. In this paper, we adapt SDS to rectified flow and re-examine the\nover-smoothing issue under this novel framework. The issue can be explained\nthat the model learns an average of multiple ODE trajectories. Then we propose\nDreamCouple, which instead of randomly sampling noise, uses a rectified flow\nmodel to find the coupled noise. Its Unique Couple Matching (UCM) loss guides\nthe model to learn different trajectories and thus solves the over-smoothing\nissue. We apply our method to both NeRF and 3D Gaussian splatting and achieve\nstate-of-the-art performances. We also identify some other interesting open\nquestions such as initialization issues for NeRF and faster training\nconvergence. Our code will be released soon.\n", "link": "http://arxiv.org/abs/2408.05008v1", "date": "2024-08-09", "relevancy": 1.8563, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.707}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5999}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamCouple%3A%20Exploring%20High%20Quality%20Text-to-3D%20Generation%20Via%20Rectified%0A%20%20Flow&body=Title%3A%20DreamCouple%3A%20Exploring%20High%20Quality%20Text-to-3D%20Generation%20Via%20Rectified%0A%20%20Flow%0AAuthor%3A%20Hangyu%20Li%20and%20Xiangxiang%20Chu%20and%20Dingyuan%20Shi%0AAbstract%3A%20%20%20The%20Score%20Distillation%20Sampling%20%28SDS%29%2C%20which%20exploits%20pretrained%0Atext-to-image%20model%20diffusion%20models%20as%20priors%20to%203D%20model%20training%2C%20has%0Aachieved%20significant%20success.%20Currently%2C%20the%20flow-based%20diffusion%20model%20has%0Abecome%20a%20new%20trend%20for%20generations.%20Yet%2C%20adapting%20SDS%20to%20flow-based%20diffusion%0Amodels%20in%203D%20generation%20remains%20unexplored.%20Our%20work%20is%20aimed%20to%20bridge%20this%0Agap.%20In%20this%20paper%2C%20we%20adapt%20SDS%20to%20rectified%20flow%20and%20re-examine%20the%0Aover-smoothing%20issue%20under%20this%20novel%20framework.%20The%20issue%20can%20be%20explained%0Athat%20the%20model%20learns%20an%20average%20of%20multiple%20ODE%20trajectories.%20Then%20we%20propose%0ADreamCouple%2C%20which%20instead%20of%20randomly%20sampling%20noise%2C%20uses%20a%20rectified%20flow%0Amodel%20to%20find%20the%20coupled%20noise.%20Its%20Unique%20Couple%20Matching%20%28UCM%29%20loss%20guides%0Athe%20model%20to%20learn%20different%20trajectories%20and%20thus%20solves%20the%20over-smoothing%0Aissue.%20We%20apply%20our%20method%20to%20both%20NeRF%20and%203D%20Gaussian%20splatting%20and%20achieve%0Astate-of-the-art%20performances.%20We%20also%20identify%20some%20other%20interesting%20open%0Aquestions%20such%20as%20initialization%20issues%20for%20NeRF%20and%20faster%20training%0Aconvergence.%20Our%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamCouple%253A%2520Exploring%2520High%2520Quality%2520Text-to-3D%2520Generation%2520Via%2520Rectified%250A%2520%2520Flow%26entry.906535625%3DHangyu%2520Li%2520and%2520Xiangxiang%2520Chu%2520and%2520Dingyuan%2520Shi%26entry.1292438233%3D%2520%2520The%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%252C%2520which%2520exploits%2520pretrained%250Atext-to-image%2520model%2520diffusion%2520models%2520as%2520priors%2520to%25203D%2520model%2520training%252C%2520has%250Aachieved%2520significant%2520success.%2520Currently%252C%2520the%2520flow-based%2520diffusion%2520model%2520has%250Abecome%2520a%2520new%2520trend%2520for%2520generations.%2520Yet%252C%2520adapting%2520SDS%2520to%2520flow-based%2520diffusion%250Amodels%2520in%25203D%2520generation%2520remains%2520unexplored.%2520Our%2520work%2520is%2520aimed%2520to%2520bridge%2520this%250Agap.%2520In%2520this%2520paper%252C%2520we%2520adapt%2520SDS%2520to%2520rectified%2520flow%2520and%2520re-examine%2520the%250Aover-smoothing%2520issue%2520under%2520this%2520novel%2520framework.%2520The%2520issue%2520can%2520be%2520explained%250Athat%2520the%2520model%2520learns%2520an%2520average%2520of%2520multiple%2520ODE%2520trajectories.%2520Then%2520we%2520propose%250ADreamCouple%252C%2520which%2520instead%2520of%2520randomly%2520sampling%2520noise%252C%2520uses%2520a%2520rectified%2520flow%250Amodel%2520to%2520find%2520the%2520coupled%2520noise.%2520Its%2520Unique%2520Couple%2520Matching%2520%2528UCM%2529%2520loss%2520guides%250Athe%2520model%2520to%2520learn%2520different%2520trajectories%2520and%2520thus%2520solves%2520the%2520over-smoothing%250Aissue.%2520We%2520apply%2520our%2520method%2520to%2520both%2520NeRF%2520and%25203D%2520Gaussian%2520splatting%2520and%2520achieve%250Astate-of-the-art%2520performances.%2520We%2520also%2520identify%2520some%2520other%2520interesting%2520open%250Aquestions%2520such%2520as%2520initialization%2520issues%2520for%2520NeRF%2520and%2520faster%2520training%250Aconvergence.%2520Our%2520code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamCouple%3A%20Exploring%20High%20Quality%20Text-to-3D%20Generation%20Via%20Rectified%0A%20%20Flow&entry.906535625=Hangyu%20Li%20and%20Xiangxiang%20Chu%20and%20Dingyuan%20Shi&entry.1292438233=%20%20The%20Score%20Distillation%20Sampling%20%28SDS%29%2C%20which%20exploits%20pretrained%0Atext-to-image%20model%20diffusion%20models%20as%20priors%20to%203D%20model%20training%2C%20has%0Aachieved%20significant%20success.%20Currently%2C%20the%20flow-based%20diffusion%20model%20has%0Abecome%20a%20new%20trend%20for%20generations.%20Yet%2C%20adapting%20SDS%20to%20flow-based%20diffusion%0Amodels%20in%203D%20generation%20remains%20unexplored.%20Our%20work%20is%20aimed%20to%20bridge%20this%0Agap.%20In%20this%20paper%2C%20we%20adapt%20SDS%20to%20rectified%20flow%20and%20re-examine%20the%0Aover-smoothing%20issue%20under%20this%20novel%20framework.%20The%20issue%20can%20be%20explained%0Athat%20the%20model%20learns%20an%20average%20of%20multiple%20ODE%20trajectories.%20Then%20we%20propose%0ADreamCouple%2C%20which%20instead%20of%20randomly%20sampling%20noise%2C%20uses%20a%20rectified%20flow%0Amodel%20to%20find%20the%20coupled%20noise.%20Its%20Unique%20Couple%20Matching%20%28UCM%29%20loss%20guides%0Athe%20model%20to%20learn%20different%20trajectories%20and%20thus%20solves%20the%20over-smoothing%0Aissue.%20We%20apply%20our%20method%20to%20both%20NeRF%20and%203D%20Gaussian%20splatting%20and%20achieve%0Astate-of-the-art%20performances.%20We%20also%20identify%20some%20other%20interesting%20open%0Aquestions%20such%20as%20initialization%20issues%20for%20NeRF%20and%20faster%20training%0Aconvergence.%20Our%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05008v1&entry.124074799=Read"},
{"title": "Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon", "author": "Jiang You and Arben Cela and Ren\u00e9 Natowicz and Jacob Ouanounou and Patrick Siarry", "abstract": "  Anomaly detection in time series data is a critical challenge across various\ndomains. Traditional methods typically focus on identifying anomalies in\nimmediate subsequent steps, often underestimating the significance of temporal\ndynamics such as delay time and horizons of anomalies, which generally require\nextensive post-analysis. This paper introduces a novel approach for detecting\ntime series anomalies called Anomaly Prediction, incorporating temporal\ninformation directly into the prediction results. We propose a new dataset\nspecifically designed to evaluate this approach and conduct comprehensive\nexperiments using several state-of-the-art time series forecasting methods. The\nresults demonstrate the efficacy of our approach in providing timely and\naccurate anomaly predictions, setting a new benchmark for future research in\nthis field.\n", "link": "http://arxiv.org/abs/2408.04377v2", "date": "2024-08-09", "relevancy": 1.8561, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4561}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomaly%20Prediction%3A%20A%20Novel%20Approach%20with%20Explicit%20Delay%20and%20Horizon&body=Title%3A%20Anomaly%20Prediction%3A%20A%20Novel%20Approach%20with%20Explicit%20Delay%20and%20Horizon%0AAuthor%3A%20Jiang%20You%20and%20Arben%20Cela%20and%20Ren%C3%A9%20Natowicz%20and%20Jacob%20Ouanounou%20and%20Patrick%20Siarry%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20time%20series%20data%20is%20a%20critical%20challenge%20across%20various%0Adomains.%20Traditional%20methods%20typically%20focus%20on%20identifying%20anomalies%20in%0Aimmediate%20subsequent%20steps%2C%20often%20underestimating%20the%20significance%20of%20temporal%0Adynamics%20such%20as%20delay%20time%20and%20horizons%20of%20anomalies%2C%20which%20generally%20require%0Aextensive%20post-analysis.%20This%20paper%20introduces%20a%20novel%20approach%20for%20detecting%0Atime%20series%20anomalies%20called%20Anomaly%20Prediction%2C%20incorporating%20temporal%0Ainformation%20directly%20into%20the%20prediction%20results.%20We%20propose%20a%20new%20dataset%0Aspecifically%20designed%20to%20evaluate%20this%20approach%20and%20conduct%20comprehensive%0Aexperiments%20using%20several%20state-of-the-art%20time%20series%20forecasting%20methods.%20The%0Aresults%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%20providing%20timely%20and%0Aaccurate%20anomaly%20predictions%2C%20setting%20a%20new%20benchmark%20for%20future%20research%20in%0Athis%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomaly%2520Prediction%253A%2520A%2520Novel%2520Approach%2520with%2520Explicit%2520Delay%2520and%2520Horizon%26entry.906535625%3DJiang%2520You%2520and%2520Arben%2520Cela%2520and%2520Ren%25C3%25A9%2520Natowicz%2520and%2520Jacob%2520Ouanounou%2520and%2520Patrick%2520Siarry%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520in%2520time%2520series%2520data%2520is%2520a%2520critical%2520challenge%2520across%2520various%250Adomains.%2520Traditional%2520methods%2520typically%2520focus%2520on%2520identifying%2520anomalies%2520in%250Aimmediate%2520subsequent%2520steps%252C%2520often%2520underestimating%2520the%2520significance%2520of%2520temporal%250Adynamics%2520such%2520as%2520delay%2520time%2520and%2520horizons%2520of%2520anomalies%252C%2520which%2520generally%2520require%250Aextensive%2520post-analysis.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520for%2520detecting%250Atime%2520series%2520anomalies%2520called%2520Anomaly%2520Prediction%252C%2520incorporating%2520temporal%250Ainformation%2520directly%2520into%2520the%2520prediction%2520results.%2520We%2520propose%2520a%2520new%2520dataset%250Aspecifically%2520designed%2520to%2520evaluate%2520this%2520approach%2520and%2520conduct%2520comprehensive%250Aexperiments%2520using%2520several%2520state-of-the-art%2520time%2520series%2520forecasting%2520methods.%2520The%250Aresults%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520in%2520providing%2520timely%2520and%250Aaccurate%2520anomaly%2520predictions%252C%2520setting%2520a%2520new%2520benchmark%2520for%2520future%2520research%2520in%250Athis%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly%20Prediction%3A%20A%20Novel%20Approach%20with%20Explicit%20Delay%20and%20Horizon&entry.906535625=Jiang%20You%20and%20Arben%20Cela%20and%20Ren%C3%A9%20Natowicz%20and%20Jacob%20Ouanounou%20and%20Patrick%20Siarry&entry.1292438233=%20%20Anomaly%20detection%20in%20time%20series%20data%20is%20a%20critical%20challenge%20across%20various%0Adomains.%20Traditional%20methods%20typically%20focus%20on%20identifying%20anomalies%20in%0Aimmediate%20subsequent%20steps%2C%20often%20underestimating%20the%20significance%20of%20temporal%0Adynamics%20such%20as%20delay%20time%20and%20horizons%20of%20anomalies%2C%20which%20generally%20require%0Aextensive%20post-analysis.%20This%20paper%20introduces%20a%20novel%20approach%20for%20detecting%0Atime%20series%20anomalies%20called%20Anomaly%20Prediction%2C%20incorporating%20temporal%0Ainformation%20directly%20into%20the%20prediction%20results.%20We%20propose%20a%20new%20dataset%0Aspecifically%20designed%20to%20evaluate%20this%20approach%20and%20conduct%20comprehensive%0Aexperiments%20using%20several%20state-of-the-art%20time%20series%20forecasting%20methods.%20The%0Aresults%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%20providing%20timely%20and%0Aaccurate%20anomaly%20predictions%2C%20setting%20a%20new%20benchmark%20for%20future%20research%20in%0Athis%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04377v2&entry.124074799=Read"},
{"title": "Livestock Fish Larvae Counting using DETR and YOLO based Deep Networks", "author": "Daniel Ortega de Carvalho and Luiz Felipe Teodoro Monteiro and Fernanda Marques Bazilio and Gabriel Toshio Hirokawa Higa and Hemerson Pistori", "abstract": "  Counting fish larvae is an important, yet demanding and time consuming, task\nin aquaculture. In order to address this problem, in this work, we evaluate\nfour neural network architectures, including convolutional neural networks and\ntransformers, in different sizes, in the task of fish larvae counting. For the\nevaluation, we present a new annotated image dataset with less data collection\nrequirements than preceding works, with images of spotted sorubim and dourado\nlarvae. By using image tiling techniques, we achieve a MAPE of 4.46% ($\\pm\n4.70$) with an extra large real time detection transformer, and 4.71% ($\\pm\n4.98$) with a medium-sized YOLOv8.\n", "link": "http://arxiv.org/abs/2408.05032v1", "date": "2024-08-09", "relevancy": 1.8505, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4706}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.465}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Livestock%20Fish%20Larvae%20Counting%20using%20DETR%20and%20YOLO%20based%20Deep%20Networks&body=Title%3A%20Livestock%20Fish%20Larvae%20Counting%20using%20DETR%20and%20YOLO%20based%20Deep%20Networks%0AAuthor%3A%20Daniel%20Ortega%20de%20Carvalho%20and%20Luiz%20Felipe%20Teodoro%20Monteiro%20and%20Fernanda%20Marques%20Bazilio%20and%20Gabriel%20Toshio%20Hirokawa%20Higa%20and%20Hemerson%20Pistori%0AAbstract%3A%20%20%20Counting%20fish%20larvae%20is%20an%20important%2C%20yet%20demanding%20and%20time%20consuming%2C%20task%0Ain%20aquaculture.%20In%20order%20to%20address%20this%20problem%2C%20in%20this%20work%2C%20we%20evaluate%0Afour%20neural%20network%20architectures%2C%20including%20convolutional%20neural%20networks%20and%0Atransformers%2C%20in%20different%20sizes%2C%20in%20the%20task%20of%20fish%20larvae%20counting.%20For%20the%0Aevaluation%2C%20we%20present%20a%20new%20annotated%20image%20dataset%20with%20less%20data%20collection%0Arequirements%20than%20preceding%20works%2C%20with%20images%20of%20spotted%20sorubim%20and%20dourado%0Alarvae.%20By%20using%20image%20tiling%20techniques%2C%20we%20achieve%20a%20MAPE%20of%204.46%25%20%28%24%5Cpm%0A4.70%24%29%20with%20an%20extra%20large%20real%20time%20detection%20transformer%2C%20and%204.71%25%20%28%24%5Cpm%0A4.98%24%29%20with%20a%20medium-sized%20YOLOv8.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLivestock%2520Fish%2520Larvae%2520Counting%2520using%2520DETR%2520and%2520YOLO%2520based%2520Deep%2520Networks%26entry.906535625%3DDaniel%2520Ortega%2520de%2520Carvalho%2520and%2520Luiz%2520Felipe%2520Teodoro%2520Monteiro%2520and%2520Fernanda%2520Marques%2520Bazilio%2520and%2520Gabriel%2520Toshio%2520Hirokawa%2520Higa%2520and%2520Hemerson%2520Pistori%26entry.1292438233%3D%2520%2520Counting%2520fish%2520larvae%2520is%2520an%2520important%252C%2520yet%2520demanding%2520and%2520time%2520consuming%252C%2520task%250Ain%2520aquaculture.%2520In%2520order%2520to%2520address%2520this%2520problem%252C%2520in%2520this%2520work%252C%2520we%2520evaluate%250Afour%2520neural%2520network%2520architectures%252C%2520including%2520convolutional%2520neural%2520networks%2520and%250Atransformers%252C%2520in%2520different%2520sizes%252C%2520in%2520the%2520task%2520of%2520fish%2520larvae%2520counting.%2520For%2520the%250Aevaluation%252C%2520we%2520present%2520a%2520new%2520annotated%2520image%2520dataset%2520with%2520less%2520data%2520collection%250Arequirements%2520than%2520preceding%2520works%252C%2520with%2520images%2520of%2520spotted%2520sorubim%2520and%2520dourado%250Alarvae.%2520By%2520using%2520image%2520tiling%2520techniques%252C%2520we%2520achieve%2520a%2520MAPE%2520of%25204.46%2525%2520%2528%2524%255Cpm%250A4.70%2524%2529%2520with%2520an%2520extra%2520large%2520real%2520time%2520detection%2520transformer%252C%2520and%25204.71%2525%2520%2528%2524%255Cpm%250A4.98%2524%2529%2520with%2520a%2520medium-sized%2520YOLOv8.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Livestock%20Fish%20Larvae%20Counting%20using%20DETR%20and%20YOLO%20based%20Deep%20Networks&entry.906535625=Daniel%20Ortega%20de%20Carvalho%20and%20Luiz%20Felipe%20Teodoro%20Monteiro%20and%20Fernanda%20Marques%20Bazilio%20and%20Gabriel%20Toshio%20Hirokawa%20Higa%20and%20Hemerson%20Pistori&entry.1292438233=%20%20Counting%20fish%20larvae%20is%20an%20important%2C%20yet%20demanding%20and%20time%20consuming%2C%20task%0Ain%20aquaculture.%20In%20order%20to%20address%20this%20problem%2C%20in%20this%20work%2C%20we%20evaluate%0Afour%20neural%20network%20architectures%2C%20including%20convolutional%20neural%20networks%20and%0Atransformers%2C%20in%20different%20sizes%2C%20in%20the%20task%20of%20fish%20larvae%20counting.%20For%20the%0Aevaluation%2C%20we%20present%20a%20new%20annotated%20image%20dataset%20with%20less%20data%20collection%0Arequirements%20than%20preceding%20works%2C%20with%20images%20of%20spotted%20sorubim%20and%20dourado%0Alarvae.%20By%20using%20image%20tiling%20techniques%2C%20we%20achieve%20a%20MAPE%20of%204.46%25%20%28%24%5Cpm%0A4.70%24%29%20with%20an%20extra%20large%20real%20time%20detection%20transformer%2C%20and%204.71%25%20%28%24%5Cpm%0A4.98%24%29%20with%20a%20medium-sized%20YOLOv8.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05032v1&entry.124074799=Read"},
{"title": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models", "author": "Zikai Xie", "abstract": "  Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.\n", "link": "http://arxiv.org/abs/2408.05093v1", "date": "2024-08-09", "relevancy": 1.8464, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4718}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Order%20Matters%20in%20Hallucination%3A%20Reasoning%20Order%20as%20Benchmark%20and%0A%20%20Reflexive%20Prompting%20for%20Large-Language-Models&body=Title%3A%20Order%20Matters%20in%20Hallucination%3A%20Reasoning%20Order%20as%20Benchmark%20and%0A%20%20Reflexive%20Prompting%20for%20Large-Language-Models%0AAuthor%3A%20Zikai%20Xie%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20generated%20significant%20attention%20since%20their%0Ainception%2C%20finding%20applications%20across%20various%20academic%20and%20industrial%20domains.%0AHowever%2C%20these%20models%20often%20suffer%20from%20the%20%22hallucination%20problem%22%2C%20where%0Aoutputs%2C%20though%20grammatically%20and%20logically%20coherent%2C%20lack%20factual%20accuracy%20or%0Aare%20entirely%20fabricated.%20A%20particularly%20troubling%20issue%20discovered%20and%20widely%0Adiscussed%20recently%20is%20the%20numerical%20comparison%20error%20where%20multiple%20LLMs%0Aincorrectly%20infer%20that%20%229.11%24%3E%249.9%22.%20We%20discovered%20that%20the%20order%20in%20which%20LLMs%0Agenerate%20answers%20and%20reasoning%20impacts%20their%20consistency.%20Specifically%2C%20results%0Avary%20significantly%20when%20an%20LLM%20generates%20an%20answer%20first%20and%20then%20provides%20the%0Areasoning%20versus%20generating%20the%20reasoning%20process%20first%20and%20then%20the%0Aconclusion.%20Inspired%20by%20this%2C%20we%20propose%20a%20new%20benchmark%20method%20for%20assessing%0ALLM%20consistency%3A%20comparing%20responses%20generated%20through%20these%20two%20different%0Aapproaches.%20This%20benchmark%20effectively%20identifies%20instances%20where%20LLMs%0Afabricate%20answers%20and%20subsequently%20generate%20justifications.%20Furthermore%2C%20we%0Aintroduce%20a%20novel%20and%20straightforward%20prompt%20strategy%20designed%20to%20mitigate%20this%0Aissue.%20Experimental%20results%20demonstrate%20that%20this%20strategy%20improves%20performance%0Aacross%20various%20LLMs%20compared%20to%20direct%20questioning.%20This%20work%20not%20only%20sheds%0Alight%20on%20a%20critical%20flaw%20in%20LLMs%20but%20also%20offers%20a%20practical%20solution%20to%0Aenhance%20their%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrder%2520Matters%2520in%2520Hallucination%253A%2520Reasoning%2520Order%2520as%2520Benchmark%2520and%250A%2520%2520Reflexive%2520Prompting%2520for%2520Large-Language-Models%26entry.906535625%3DZikai%2520Xie%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520generated%2520significant%2520attention%2520since%2520their%250Ainception%252C%2520finding%2520applications%2520across%2520various%2520academic%2520and%2520industrial%2520domains.%250AHowever%252C%2520these%2520models%2520often%2520suffer%2520from%2520the%2520%2522hallucination%2520problem%2522%252C%2520where%250Aoutputs%252C%2520though%2520grammatically%2520and%2520logically%2520coherent%252C%2520lack%2520factual%2520accuracy%2520or%250Aare%2520entirely%2520fabricated.%2520A%2520particularly%2520troubling%2520issue%2520discovered%2520and%2520widely%250Adiscussed%2520recently%2520is%2520the%2520numerical%2520comparison%2520error%2520where%2520multiple%2520LLMs%250Aincorrectly%2520infer%2520that%2520%25229.11%2524%253E%25249.9%2522.%2520We%2520discovered%2520that%2520the%2520order%2520in%2520which%2520LLMs%250Agenerate%2520answers%2520and%2520reasoning%2520impacts%2520their%2520consistency.%2520Specifically%252C%2520results%250Avary%2520significantly%2520when%2520an%2520LLM%2520generates%2520an%2520answer%2520first%2520and%2520then%2520provides%2520the%250Areasoning%2520versus%2520generating%2520the%2520reasoning%2520process%2520first%2520and%2520then%2520the%250Aconclusion.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520method%2520for%2520assessing%250ALLM%2520consistency%253A%2520comparing%2520responses%2520generated%2520through%2520these%2520two%2520different%250Aapproaches.%2520This%2520benchmark%2520effectively%2520identifies%2520instances%2520where%2520LLMs%250Afabricate%2520answers%2520and%2520subsequently%2520generate%2520justifications.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520novel%2520and%2520straightforward%2520prompt%2520strategy%2520designed%2520to%2520mitigate%2520this%250Aissue.%2520Experimental%2520results%2520demonstrate%2520that%2520this%2520strategy%2520improves%2520performance%250Aacross%2520various%2520LLMs%2520compared%2520to%2520direct%2520questioning.%2520This%2520work%2520not%2520only%2520sheds%250Alight%2520on%2520a%2520critical%2520flaw%2520in%2520LLMs%2520but%2520also%2520offers%2520a%2520practical%2520solution%2520to%250Aenhance%2520their%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Order%20Matters%20in%20Hallucination%3A%20Reasoning%20Order%20as%20Benchmark%20and%0A%20%20Reflexive%20Prompting%20for%20Large-Language-Models&entry.906535625=Zikai%20Xie&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20generated%20significant%20attention%20since%20their%0Ainception%2C%20finding%20applications%20across%20various%20academic%20and%20industrial%20domains.%0AHowever%2C%20these%20models%20often%20suffer%20from%20the%20%22hallucination%20problem%22%2C%20where%0Aoutputs%2C%20though%20grammatically%20and%20logically%20coherent%2C%20lack%20factual%20accuracy%20or%0Aare%20entirely%20fabricated.%20A%20particularly%20troubling%20issue%20discovered%20and%20widely%0Adiscussed%20recently%20is%20the%20numerical%20comparison%20error%20where%20multiple%20LLMs%0Aincorrectly%20infer%20that%20%229.11%24%3E%249.9%22.%20We%20discovered%20that%20the%20order%20in%20which%20LLMs%0Agenerate%20answers%20and%20reasoning%20impacts%20their%20consistency.%20Specifically%2C%20results%0Avary%20significantly%20when%20an%20LLM%20generates%20an%20answer%20first%20and%20then%20provides%20the%0Areasoning%20versus%20generating%20the%20reasoning%20process%20first%20and%20then%20the%0Aconclusion.%20Inspired%20by%20this%2C%20we%20propose%20a%20new%20benchmark%20method%20for%20assessing%0ALLM%20consistency%3A%20comparing%20responses%20generated%20through%20these%20two%20different%0Aapproaches.%20This%20benchmark%20effectively%20identifies%20instances%20where%20LLMs%0Afabricate%20answers%20and%20subsequently%20generate%20justifications.%20Furthermore%2C%20we%0Aintroduce%20a%20novel%20and%20straightforward%20prompt%20strategy%20designed%20to%20mitigate%20this%0Aissue.%20Experimental%20results%20demonstrate%20that%20this%20strategy%20improves%20performance%0Aacross%20various%20LLMs%20compared%20to%20direct%20questioning.%20This%20work%20not%20only%20sheds%0Alight%20on%20a%20critical%20flaw%20in%20LLMs%20but%20also%20offers%20a%20practical%20solution%20to%0Aenhance%20their%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05093v1&entry.124074799=Read"},
{"title": "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions", "author": "Michele Miranda and Elena Sofia Ruzzetti and Andrea Santilli and Fabio Massimo Zanzotto and S\u00e9bastien Brati\u00e8res and Emanuele Rodol\u00e0", "abstract": "  Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.\n", "link": "http://arxiv.org/abs/2408.05212v1", "date": "2024-08-10", "relevancy": 1.8277, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4964}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserving%20Privacy%20in%20Large%20Language%20Models%3A%20A%20Survey%20on%20Current%20Threats%0A%20%20and%20Solutions&body=Title%3A%20Preserving%20Privacy%20in%20Large%20Language%20Models%3A%20A%20Survey%20on%20Current%20Threats%0A%20%20and%20Solutions%0AAuthor%3A%20Michele%20Miranda%20and%20Elena%20Sofia%20Ruzzetti%20and%20Andrea%20Santilli%20and%20Fabio%20Massimo%20Zanzotto%20and%20S%C3%A9bastien%20Brati%C3%A8res%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20represent%20a%20significant%20advancement%20in%0Aartificial%20intelligence%2C%20finding%20applications%20across%20various%20domains.%20However%2C%0Atheir%20reliance%20on%20massive%20internet-sourced%20datasets%20for%20training%20brings%20notable%0Aprivacy%20issues%2C%20which%20are%20exacerbated%20in%20critical%20domains%20%28e.g.%2C%20healthcare%29.%0AMoreover%2C%20certain%20application-specific%20scenarios%20may%20require%20fine-tuning%20these%0Amodels%20on%20private%20data.%20This%20survey%20critically%20examines%20the%20privacy%20threats%0Aassociated%20with%20LLMs%2C%20emphasizing%20the%20potential%20for%20these%20models%20to%20memorize%0Aand%20inadvertently%20reveal%20sensitive%20information.%20We%20explore%20current%20threats%20by%0Areviewing%20privacy%20attacks%20on%20LLMs%20and%20propose%20comprehensive%20solutions%20for%0Aintegrating%20privacy%20mechanisms%20throughout%20the%20entire%20learning%20pipeline.%20These%0Asolutions%20range%20from%20anonymizing%20training%20datasets%20to%20implementing%20differential%0Aprivacy%20during%20training%20or%20inference%20and%20machine%20unlearning%20after%20training.%20Our%0Acomprehensive%20review%20of%20existing%20literature%20highlights%20ongoing%20challenges%2C%0Aavailable%20tools%2C%20and%20future%20directions%20for%20preserving%20privacy%20in%20LLMs.%20This%0Awork%20aims%20to%20guide%20the%20development%20of%20more%20secure%20and%20trustworthy%20AI%20systems%20by%0Aproviding%20a%20thorough%20understanding%20of%20privacy%20preservation%20methods%20and%20their%0Aeffectiveness%20in%20mitigating%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserving%2520Privacy%2520in%2520Large%2520Language%2520Models%253A%2520A%2520Survey%2520on%2520Current%2520Threats%250A%2520%2520and%2520Solutions%26entry.906535625%3DMichele%2520Miranda%2520and%2520Elena%2520Sofia%2520Ruzzetti%2520and%2520Andrea%2520Santilli%2520and%2520Fabio%2520Massimo%2520Zanzotto%2520and%2520S%25C3%25A9bastien%2520Brati%25C3%25A8res%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520represent%2520a%2520significant%2520advancement%2520in%250Aartificial%2520intelligence%252C%2520finding%2520applications%2520across%2520various%2520domains.%2520However%252C%250Atheir%2520reliance%2520on%2520massive%2520internet-sourced%2520datasets%2520for%2520training%2520brings%2520notable%250Aprivacy%2520issues%252C%2520which%2520are%2520exacerbated%2520in%2520critical%2520domains%2520%2528e.g.%252C%2520healthcare%2529.%250AMoreover%252C%2520certain%2520application-specific%2520scenarios%2520may%2520require%2520fine-tuning%2520these%250Amodels%2520on%2520private%2520data.%2520This%2520survey%2520critically%2520examines%2520the%2520privacy%2520threats%250Aassociated%2520with%2520LLMs%252C%2520emphasizing%2520the%2520potential%2520for%2520these%2520models%2520to%2520memorize%250Aand%2520inadvertently%2520reveal%2520sensitive%2520information.%2520We%2520explore%2520current%2520threats%2520by%250Areviewing%2520privacy%2520attacks%2520on%2520LLMs%2520and%2520propose%2520comprehensive%2520solutions%2520for%250Aintegrating%2520privacy%2520mechanisms%2520throughout%2520the%2520entire%2520learning%2520pipeline.%2520These%250Asolutions%2520range%2520from%2520anonymizing%2520training%2520datasets%2520to%2520implementing%2520differential%250Aprivacy%2520during%2520training%2520or%2520inference%2520and%2520machine%2520unlearning%2520after%2520training.%2520Our%250Acomprehensive%2520review%2520of%2520existing%2520literature%2520highlights%2520ongoing%2520challenges%252C%250Aavailable%2520tools%252C%2520and%2520future%2520directions%2520for%2520preserving%2520privacy%2520in%2520LLMs.%2520This%250Awork%2520aims%2520to%2520guide%2520the%2520development%2520of%2520more%2520secure%2520and%2520trustworthy%2520AI%2520systems%2520by%250Aproviding%2520a%2520thorough%2520understanding%2520of%2520privacy%2520preservation%2520methods%2520and%2520their%250Aeffectiveness%2520in%2520mitigating%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserving%20Privacy%20in%20Large%20Language%20Models%3A%20A%20Survey%20on%20Current%20Threats%0A%20%20and%20Solutions&entry.906535625=Michele%20Miranda%20and%20Elena%20Sofia%20Ruzzetti%20and%20Andrea%20Santilli%20and%20Fabio%20Massimo%20Zanzotto%20and%20S%C3%A9bastien%20Brati%C3%A8res%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20represent%20a%20significant%20advancement%20in%0Aartificial%20intelligence%2C%20finding%20applications%20across%20various%20domains.%20However%2C%0Atheir%20reliance%20on%20massive%20internet-sourced%20datasets%20for%20training%20brings%20notable%0Aprivacy%20issues%2C%20which%20are%20exacerbated%20in%20critical%20domains%20%28e.g.%2C%20healthcare%29.%0AMoreover%2C%20certain%20application-specific%20scenarios%20may%20require%20fine-tuning%20these%0Amodels%20on%20private%20data.%20This%20survey%20critically%20examines%20the%20privacy%20threats%0Aassociated%20with%20LLMs%2C%20emphasizing%20the%20potential%20for%20these%20models%20to%20memorize%0Aand%20inadvertently%20reveal%20sensitive%20information.%20We%20explore%20current%20threats%20by%0Areviewing%20privacy%20attacks%20on%20LLMs%20and%20propose%20comprehensive%20solutions%20for%0Aintegrating%20privacy%20mechanisms%20throughout%20the%20entire%20learning%20pipeline.%20These%0Asolutions%20range%20from%20anonymizing%20training%20datasets%20to%20implementing%20differential%0Aprivacy%20during%20training%20or%20inference%20and%20machine%20unlearning%20after%20training.%20Our%0Acomprehensive%20review%20of%20existing%20literature%20highlights%20ongoing%20challenges%2C%0Aavailable%20tools%2C%20and%20future%20directions%20for%20preserving%20privacy%20in%20LLMs.%20This%0Awork%20aims%20to%20guide%20the%20development%20of%20more%20secure%20and%20trustworthy%20AI%20systems%20by%0Aproviding%20a%20thorough%20understanding%20of%20privacy%20preservation%20methods%20and%20their%0Aeffectiveness%20in%20mitigating%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05212v1&entry.124074799=Read"},
{"title": "Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin", "author": "Carolina Fortuna and Vid Han\u017eel and Bla\u017e Bertalani\u010d", "abstract": "  Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis.\n", "link": "http://arxiv.org/abs/2406.06566v3", "date": "2024-08-09", "relevancy": 1.8223, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4696}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4581}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20Interaction%20with%20a%20Household%20Electricity%0A%20%20Knowledge-based%20Digital%20Twin&body=Title%3A%20Natural%20Language%20Interaction%20with%20a%20Household%20Electricity%0A%20%20Knowledge-based%20Digital%20Twin%0AAuthor%3A%20Carolina%20Fortuna%20and%20Vid%20Han%C5%BEel%20and%20Bla%C5%BE%20Bertalani%C4%8D%0AAbstract%3A%20%20%20Domain%20specific%20digital%20twins%2C%20representing%20a%20digital%20replica%20of%20various%0Asegments%20of%20the%20smart%20grid%2C%20are%20foreseen%20as%20able%20to%20model%2C%20simulate%2C%20and%0Acontrol%20the%20respective%20segments.%20At%20the%20same%20time%2C%20knowledge-based%20digital%0Atwins%2C%20coupled%20with%20AI%2C%20may%20also%20empower%20humans%20to%20understand%20aspects%20of%20the%0Asystem%20through%20natural%20language%20interaction%20in%20view%20of%20planning%20and%20policy%0Amaking.%20This%20paper%20is%20the%20first%20to%20assess%20and%20report%20on%20the%20potential%20of%0ARetrieval%20Augmented%20Generation%20%28RAG%29%20question%20answers%20related%20to%20household%0Aelectrical%20energy%20measurement%20aspects%20leveraging%20a%20knowledge-based%20energy%0Adigital%20twin.%20Relying%20on%20the%20recently%20published%20electricity%20consumption%0Aknowledge%20graph%20that%20actually%20represents%20a%20knowledge-based%20digital%20twin%2C%20we%0Astudy%20the%20capabilities%20of%20ChatGPT%2C%20Gemini%20and%20Llama%20in%20answering%20electricity%0Arelated%20questions.%20Furthermore%2C%20we%20compare%20the%20answers%20with%20the%20ones%20generated%0Athrough%20a%20RAG%20techniques%20that%20leverages%20an%20existing%20electricity%20knowledge-based%0Adigital%20twin.%20Our%20findings%20illustrate%20that%20the%20RAG%20approach%20not%20only%20reduces%0Athe%20incidence%20of%20incorrect%20information%20typically%20generated%20by%20LLMs%20but%20also%0Asignificantly%20improves%20the%20quality%20of%20the%20output%20by%20grounding%20responses%20in%0Averifiable%20data.%20This%20paper%20details%20our%20methodology%2C%20presents%20a%20comparative%0Aanalysis%20of%20responses%20with%20and%20without%20RAG%2C%20and%20discusses%20the%20implications%20of%0Aour%20findings%20for%20future%20applications%20of%20AI%20in%20specialized%20sectors%20like%20energy%0Adata%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06566v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520Interaction%2520with%2520a%2520Household%2520Electricity%250A%2520%2520Knowledge-based%2520Digital%2520Twin%26entry.906535625%3DCarolina%2520Fortuna%2520and%2520Vid%2520Han%25C5%25BEel%2520and%2520Bla%25C5%25BE%2520Bertalani%25C4%258D%26entry.1292438233%3D%2520%2520Domain%2520specific%2520digital%2520twins%252C%2520representing%2520a%2520digital%2520replica%2520of%2520various%250Asegments%2520of%2520the%2520smart%2520grid%252C%2520are%2520foreseen%2520as%2520able%2520to%2520model%252C%2520simulate%252C%2520and%250Acontrol%2520the%2520respective%2520segments.%2520At%2520the%2520same%2520time%252C%2520knowledge-based%2520digital%250Atwins%252C%2520coupled%2520with%2520AI%252C%2520may%2520also%2520empower%2520humans%2520to%2520understand%2520aspects%2520of%2520the%250Asystem%2520through%2520natural%2520language%2520interaction%2520in%2520view%2520of%2520planning%2520and%2520policy%250Amaking.%2520This%2520paper%2520is%2520the%2520first%2520to%2520assess%2520and%2520report%2520on%2520the%2520potential%2520of%250ARetrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520question%2520answers%2520related%2520to%2520household%250Aelectrical%2520energy%2520measurement%2520aspects%2520leveraging%2520a%2520knowledge-based%2520energy%250Adigital%2520twin.%2520Relying%2520on%2520the%2520recently%2520published%2520electricity%2520consumption%250Aknowledge%2520graph%2520that%2520actually%2520represents%2520a%2520knowledge-based%2520digital%2520twin%252C%2520we%250Astudy%2520the%2520capabilities%2520of%2520ChatGPT%252C%2520Gemini%2520and%2520Llama%2520in%2520answering%2520electricity%250Arelated%2520questions.%2520Furthermore%252C%2520we%2520compare%2520the%2520answers%2520with%2520the%2520ones%2520generated%250Athrough%2520a%2520RAG%2520techniques%2520that%2520leverages%2520an%2520existing%2520electricity%2520knowledge-based%250Adigital%2520twin.%2520Our%2520findings%2520illustrate%2520that%2520the%2520RAG%2520approach%2520not%2520only%2520reduces%250Athe%2520incidence%2520of%2520incorrect%2520information%2520typically%2520generated%2520by%2520LLMs%2520but%2520also%250Asignificantly%2520improves%2520the%2520quality%2520of%2520the%2520output%2520by%2520grounding%2520responses%2520in%250Averifiable%2520data.%2520This%2520paper%2520details%2520our%2520methodology%252C%2520presents%2520a%2520comparative%250Aanalysis%2520of%2520responses%2520with%2520and%2520without%2520RAG%252C%2520and%2520discusses%2520the%2520implications%2520of%250Aour%2520findings%2520for%2520future%2520applications%2520of%2520AI%2520in%2520specialized%2520sectors%2520like%2520energy%250Adata%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06566v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20Interaction%20with%20a%20Household%20Electricity%0A%20%20Knowledge-based%20Digital%20Twin&entry.906535625=Carolina%20Fortuna%20and%20Vid%20Han%C5%BEel%20and%20Bla%C5%BE%20Bertalani%C4%8D&entry.1292438233=%20%20Domain%20specific%20digital%20twins%2C%20representing%20a%20digital%20replica%20of%20various%0Asegments%20of%20the%20smart%20grid%2C%20are%20foreseen%20as%20able%20to%20model%2C%20simulate%2C%20and%0Acontrol%20the%20respective%20segments.%20At%20the%20same%20time%2C%20knowledge-based%20digital%0Atwins%2C%20coupled%20with%20AI%2C%20may%20also%20empower%20humans%20to%20understand%20aspects%20of%20the%0Asystem%20through%20natural%20language%20interaction%20in%20view%20of%20planning%20and%20policy%0Amaking.%20This%20paper%20is%20the%20first%20to%20assess%20and%20report%20on%20the%20potential%20of%0ARetrieval%20Augmented%20Generation%20%28RAG%29%20question%20answers%20related%20to%20household%0Aelectrical%20energy%20measurement%20aspects%20leveraging%20a%20knowledge-based%20energy%0Adigital%20twin.%20Relying%20on%20the%20recently%20published%20electricity%20consumption%0Aknowledge%20graph%20that%20actually%20represents%20a%20knowledge-based%20digital%20twin%2C%20we%0Astudy%20the%20capabilities%20of%20ChatGPT%2C%20Gemini%20and%20Llama%20in%20answering%20electricity%0Arelated%20questions.%20Furthermore%2C%20we%20compare%20the%20answers%20with%20the%20ones%20generated%0Athrough%20a%20RAG%20techniques%20that%20leverages%20an%20existing%20electricity%20knowledge-based%0Adigital%20twin.%20Our%20findings%20illustrate%20that%20the%20RAG%20approach%20not%20only%20reduces%0Athe%20incidence%20of%20incorrect%20information%20typically%20generated%20by%20LLMs%20but%20also%0Asignificantly%20improves%20the%20quality%20of%20the%20output%20by%20grounding%20responses%20in%0Averifiable%20data.%20This%20paper%20details%20our%20methodology%2C%20presents%20a%20comparative%0Aanalysis%20of%20responses%20with%20and%20without%20RAG%2C%20and%20discusses%20the%20implications%20of%0Aour%20findings%20for%20future%20applications%20of%20AI%20in%20specialized%20sectors%20like%20energy%0Adata%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06566v3&entry.124074799=Read"},
{"title": "Efficient Radiation Treatment Planning based on Voxel Importance", "author": "Sebastian Mair and Anqi Fu and Jens Sj\u00f6lund", "abstract": "  Radiation treatment planning involves optimization over a large number of\nvoxels, many of which carry limited information about the clinical problem. We\npropose an approach to reduce the large optimization problem by only using a\nrepresentative subset of informative voxels. This way, we drastically improve\nplanning efficiency while maintaining the plan quality. Within an initial\nprobing step, we pre-solve an easier optimization problem involving a\nsimplified objective from which we derive an importance score per voxel. This\nimportance score is then turned into a sampling distribution, which allows us\nto subsample a small set of informative voxels using importance sampling. By\nsolving a - now reduced - version of the original optimization problem using\nthis subset, we effectively reduce the problem's size and computational demands\nwhile accounting for regions where satisfactory dose deliveries are\nchallenging. In contrast to other stochastic (sub-)sampling methods, our\ntechnique only requires a single probing and sampling step to define a reduced\noptimization problem. This problem can be efficiently solved using established\nsolvers without the need of modifying or adapting them. Empirical experiments\non open benchmark data highlight substantially reduced optimization times, up\nto 50 times faster than the original ones, for intensity-modulated radiation\ntherapy (IMRT), all while upholding plan quality comparable to traditional\nmethods. Our novel approach has the potential to significantly accelerate\nradiation treatment planning by addressing its inherent computational\nchallenges. We reduce the treatment planning time by reducing the size of the\noptimization problem rather than modifying and improving the optimization\nmethod. Our efforts are thus complementary to many previous developments.\n", "link": "http://arxiv.org/abs/2405.03880v2", "date": "2024-08-09", "relevancy": 1.7937, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.455}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4453}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Radiation%20Treatment%20Planning%20based%20on%20Voxel%20Importance&body=Title%3A%20Efficient%20Radiation%20Treatment%20Planning%20based%20on%20Voxel%20Importance%0AAuthor%3A%20Sebastian%20Mair%20and%20Anqi%20Fu%20and%20Jens%20Sj%C3%B6lund%0AAbstract%3A%20%20%20Radiation%20treatment%20planning%20involves%20optimization%20over%20a%20large%20number%20of%0Avoxels%2C%20many%20of%20which%20carry%20limited%20information%20about%20the%20clinical%20problem.%20We%0Apropose%20an%20approach%20to%20reduce%20the%20large%20optimization%20problem%20by%20only%20using%20a%0Arepresentative%20subset%20of%20informative%20voxels.%20This%20way%2C%20we%20drastically%20improve%0Aplanning%20efficiency%20while%20maintaining%20the%20plan%20quality.%20Within%20an%20initial%0Aprobing%20step%2C%20we%20pre-solve%20an%20easier%20optimization%20problem%20involving%20a%0Asimplified%20objective%20from%20which%20we%20derive%20an%20importance%20score%20per%20voxel.%20This%0Aimportance%20score%20is%20then%20turned%20into%20a%20sampling%20distribution%2C%20which%20allows%20us%0Ato%20subsample%20a%20small%20set%20of%20informative%20voxels%20using%20importance%20sampling.%20By%0Asolving%20a%20-%20now%20reduced%20-%20version%20of%20the%20original%20optimization%20problem%20using%0Athis%20subset%2C%20we%20effectively%20reduce%20the%20problem%27s%20size%20and%20computational%20demands%0Awhile%20accounting%20for%20regions%20where%20satisfactory%20dose%20deliveries%20are%0Achallenging.%20In%20contrast%20to%20other%20stochastic%20%28sub-%29sampling%20methods%2C%20our%0Atechnique%20only%20requires%20a%20single%20probing%20and%20sampling%20step%20to%20define%20a%20reduced%0Aoptimization%20problem.%20This%20problem%20can%20be%20efficiently%20solved%20using%20established%0Asolvers%20without%20the%20need%20of%20modifying%20or%20adapting%20them.%20Empirical%20experiments%0Aon%20open%20benchmark%20data%20highlight%20substantially%20reduced%20optimization%20times%2C%20up%0Ato%2050%20times%20faster%20than%20the%20original%20ones%2C%20for%20intensity-modulated%20radiation%0Atherapy%20%28IMRT%29%2C%20all%20while%20upholding%20plan%20quality%20comparable%20to%20traditional%0Amethods.%20Our%20novel%20approach%20has%20the%20potential%20to%20significantly%20accelerate%0Aradiation%20treatment%20planning%20by%20addressing%20its%20inherent%20computational%0Achallenges.%20We%20reduce%20the%20treatment%20planning%20time%20by%20reducing%20the%20size%20of%20the%0Aoptimization%20problem%20rather%20than%20modifying%20and%20improving%20the%20optimization%0Amethod.%20Our%20efforts%20are%20thus%20complementary%20to%20many%20previous%20developments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03880v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Radiation%2520Treatment%2520Planning%2520based%2520on%2520Voxel%2520Importance%26entry.906535625%3DSebastian%2520Mair%2520and%2520Anqi%2520Fu%2520and%2520Jens%2520Sj%25C3%25B6lund%26entry.1292438233%3D%2520%2520Radiation%2520treatment%2520planning%2520involves%2520optimization%2520over%2520a%2520large%2520number%2520of%250Avoxels%252C%2520many%2520of%2520which%2520carry%2520limited%2520information%2520about%2520the%2520clinical%2520problem.%2520We%250Apropose%2520an%2520approach%2520to%2520reduce%2520the%2520large%2520optimization%2520problem%2520by%2520only%2520using%2520a%250Arepresentative%2520subset%2520of%2520informative%2520voxels.%2520This%2520way%252C%2520we%2520drastically%2520improve%250Aplanning%2520efficiency%2520while%2520maintaining%2520the%2520plan%2520quality.%2520Within%2520an%2520initial%250Aprobing%2520step%252C%2520we%2520pre-solve%2520an%2520easier%2520optimization%2520problem%2520involving%2520a%250Asimplified%2520objective%2520from%2520which%2520we%2520derive%2520an%2520importance%2520score%2520per%2520voxel.%2520This%250Aimportance%2520score%2520is%2520then%2520turned%2520into%2520a%2520sampling%2520distribution%252C%2520which%2520allows%2520us%250Ato%2520subsample%2520a%2520small%2520set%2520of%2520informative%2520voxels%2520using%2520importance%2520sampling.%2520By%250Asolving%2520a%2520-%2520now%2520reduced%2520-%2520version%2520of%2520the%2520original%2520optimization%2520problem%2520using%250Athis%2520subset%252C%2520we%2520effectively%2520reduce%2520the%2520problem%2527s%2520size%2520and%2520computational%2520demands%250Awhile%2520accounting%2520for%2520regions%2520where%2520satisfactory%2520dose%2520deliveries%2520are%250Achallenging.%2520In%2520contrast%2520to%2520other%2520stochastic%2520%2528sub-%2529sampling%2520methods%252C%2520our%250Atechnique%2520only%2520requires%2520a%2520single%2520probing%2520and%2520sampling%2520step%2520to%2520define%2520a%2520reduced%250Aoptimization%2520problem.%2520This%2520problem%2520can%2520be%2520efficiently%2520solved%2520using%2520established%250Asolvers%2520without%2520the%2520need%2520of%2520modifying%2520or%2520adapting%2520them.%2520Empirical%2520experiments%250Aon%2520open%2520benchmark%2520data%2520highlight%2520substantially%2520reduced%2520optimization%2520times%252C%2520up%250Ato%252050%2520times%2520faster%2520than%2520the%2520original%2520ones%252C%2520for%2520intensity-modulated%2520radiation%250Atherapy%2520%2528IMRT%2529%252C%2520all%2520while%2520upholding%2520plan%2520quality%2520comparable%2520to%2520traditional%250Amethods.%2520Our%2520novel%2520approach%2520has%2520the%2520potential%2520to%2520significantly%2520accelerate%250Aradiation%2520treatment%2520planning%2520by%2520addressing%2520its%2520inherent%2520computational%250Achallenges.%2520We%2520reduce%2520the%2520treatment%2520planning%2520time%2520by%2520reducing%2520the%2520size%2520of%2520the%250Aoptimization%2520problem%2520rather%2520than%2520modifying%2520and%2520improving%2520the%2520optimization%250Amethod.%2520Our%2520efforts%2520are%2520thus%2520complementary%2520to%2520many%2520previous%2520developments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03880v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Radiation%20Treatment%20Planning%20based%20on%20Voxel%20Importance&entry.906535625=Sebastian%20Mair%20and%20Anqi%20Fu%20and%20Jens%20Sj%C3%B6lund&entry.1292438233=%20%20Radiation%20treatment%20planning%20involves%20optimization%20over%20a%20large%20number%20of%0Avoxels%2C%20many%20of%20which%20carry%20limited%20information%20about%20the%20clinical%20problem.%20We%0Apropose%20an%20approach%20to%20reduce%20the%20large%20optimization%20problem%20by%20only%20using%20a%0Arepresentative%20subset%20of%20informative%20voxels.%20This%20way%2C%20we%20drastically%20improve%0Aplanning%20efficiency%20while%20maintaining%20the%20plan%20quality.%20Within%20an%20initial%0Aprobing%20step%2C%20we%20pre-solve%20an%20easier%20optimization%20problem%20involving%20a%0Asimplified%20objective%20from%20which%20we%20derive%20an%20importance%20score%20per%20voxel.%20This%0Aimportance%20score%20is%20then%20turned%20into%20a%20sampling%20distribution%2C%20which%20allows%20us%0Ato%20subsample%20a%20small%20set%20of%20informative%20voxels%20using%20importance%20sampling.%20By%0Asolving%20a%20-%20now%20reduced%20-%20version%20of%20the%20original%20optimization%20problem%20using%0Athis%20subset%2C%20we%20effectively%20reduce%20the%20problem%27s%20size%20and%20computational%20demands%0Awhile%20accounting%20for%20regions%20where%20satisfactory%20dose%20deliveries%20are%0Achallenging.%20In%20contrast%20to%20other%20stochastic%20%28sub-%29sampling%20methods%2C%20our%0Atechnique%20only%20requires%20a%20single%20probing%20and%20sampling%20step%20to%20define%20a%20reduced%0Aoptimization%20problem.%20This%20problem%20can%20be%20efficiently%20solved%20using%20established%0Asolvers%20without%20the%20need%20of%20modifying%20or%20adapting%20them.%20Empirical%20experiments%0Aon%20open%20benchmark%20data%20highlight%20substantially%20reduced%20optimization%20times%2C%20up%0Ato%2050%20times%20faster%20than%20the%20original%20ones%2C%20for%20intensity-modulated%20radiation%0Atherapy%20%28IMRT%29%2C%20all%20while%20upholding%20plan%20quality%20comparable%20to%20traditional%0Amethods.%20Our%20novel%20approach%20has%20the%20potential%20to%20significantly%20accelerate%0Aradiation%20treatment%20planning%20by%20addressing%20its%20inherent%20computational%0Achallenges.%20We%20reduce%20the%20treatment%20planning%20time%20by%20reducing%20the%20size%20of%20the%0Aoptimization%20problem%20rather%20than%20modifying%20and%20improving%20the%20optimization%0Amethod.%20Our%20efforts%20are%20thus%20complementary%20to%20many%20previous%20developments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03880v2&entry.124074799=Read"},
{"title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models", "author": "Qirui Jiao and Daoyuan Chen and Yilun Huang and Yaliang Li and Ying Shen", "abstract": "  High-performance Multimodal Large Language Models (MLLMs) rely heavily on\ndata quality. This study introduces a novel dataset named Img-Diff, designed to\nenhance fine-grained image recognition in MLLMs by leveraging insights from\ncontrastive learning and image difference captioning. By analyzing object\ndifferences between similar images, we challenge models to identify both\nmatching and distinct components. We utilize the Stable-Diffusion-XL model and\nadvanced image editing techniques to create pairs of similar images that\nhighlight object replacements. Our methodology includes a Difference Area\nGenerator for object differences identifying, followed by a Difference Captions\nGenerator for detailed difference descriptions. The result is a relatively\nsmall but high-quality dataset of \"object replacement\" samples. We use the the\nproposed dataset to finetune state-of-the-art (SOTA) MLLMs such as MGM-7B,\nyielding comprehensive improvements of performance scores over SOTA models that\ntrained with larger-scale datasets, in numerous image difference and Visual\nQuestion Answering tasks. For instance, our trained models notably surpass the\nSOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate\nalternative methods for generating image difference data through \"object\nremoval\" and conduct a thorough evaluation to confirm the dataset's diversity,\nquality, and robustness, presenting several insights on the synthesis of such a\ncontrastive dataset. To encourage further research and advance the field of\nmultimodal data synthesis and enhancement of MLLMs' fundamental capabilities\nfor image understanding, we release our codes and dataset at\nhttps://github.com/modelscope/data-juicer/tree/ImgDiff.\n", "link": "http://arxiv.org/abs/2408.04594v2", "date": "2024-08-09", "relevancy": 1.773, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6178}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5914}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Yaliang%20Li%20and%20Ying%20Shen%0AAbstract%3A%20%20%20High-performance%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20rely%20heavily%20on%0Adata%20quality.%20This%20study%20introduces%20a%20novel%20dataset%20named%20Img-Diff%2C%20designed%20to%0Aenhance%20fine-grained%20image%20recognition%20in%20MLLMs%20by%20leveraging%20insights%20from%0Acontrastive%20learning%20and%20image%20difference%20captioning.%20By%20analyzing%20object%0Adifferences%20between%20similar%20images%2C%20we%20challenge%20models%20to%20identify%20both%0Amatching%20and%20distinct%20components.%20We%20utilize%20the%20Stable-Diffusion-XL%20model%20and%0Aadvanced%20image%20editing%20techniques%20to%20create%20pairs%20of%20similar%20images%20that%0Ahighlight%20object%20replacements.%20Our%20methodology%20includes%20a%20Difference%20Area%0AGenerator%20for%20object%20differences%20identifying%2C%20followed%20by%20a%20Difference%20Captions%0AGenerator%20for%20detailed%20difference%20descriptions.%20The%20result%20is%20a%20relatively%0Asmall%20but%20high-quality%20dataset%20of%20%22object%20replacement%22%20samples.%20We%20use%20the%20the%0Aproposed%20dataset%20to%20finetune%20state-of-the-art%20%28SOTA%29%20MLLMs%20such%20as%20MGM-7B%2C%0Ayielding%20comprehensive%20improvements%20of%20performance%20scores%20over%20SOTA%20models%20that%0Atrained%20with%20larger-scale%20datasets%2C%20in%20numerous%20image%20difference%20and%20Visual%0AQuestion%20Answering%20tasks.%20For%20instance%2C%20our%20trained%20models%20notably%20surpass%20the%0ASOTA%20models%20GPT-4V%20and%20Gemini%20on%20the%20MMVP%20benchmark.%20Besides%2C%20we%20investigate%0Aalternative%20methods%20for%20generating%20image%20difference%20data%20through%20%22object%0Aremoval%22%20and%20conduct%20a%20thorough%20evaluation%20to%20confirm%20the%20dataset%27s%20diversity%2C%0Aquality%2C%20and%20robustness%2C%20presenting%20several%20insights%20on%20the%20synthesis%20of%20such%20a%0Acontrastive%20dataset.%20To%20encourage%20further%20research%20and%20advance%20the%20field%20of%0Amultimodal%20data%20synthesis%20and%20enhancement%20of%20MLLMs%27%20fundamental%20capabilities%0Afor%20image%20understanding%2C%20we%20release%20our%20codes%20and%20dataset%20at%0Ahttps%3A//github.com/modelscope/data-juicer/tree/ImgDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04594v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImg-Diff%253A%2520Contrastive%2520Data%2520Synthesis%2520for%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DQirui%2520Jiao%2520and%2520Daoyuan%2520Chen%2520and%2520Yilun%2520Huang%2520and%2520Yaliang%2520Li%2520and%2520Ying%2520Shen%26entry.1292438233%3D%2520%2520High-performance%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520rely%2520heavily%2520on%250Adata%2520quality.%2520This%2520study%2520introduces%2520a%2520novel%2520dataset%2520named%2520Img-Diff%252C%2520designed%2520to%250Aenhance%2520fine-grained%2520image%2520recognition%2520in%2520MLLMs%2520by%2520leveraging%2520insights%2520from%250Acontrastive%2520learning%2520and%2520image%2520difference%2520captioning.%2520By%2520analyzing%2520object%250Adifferences%2520between%2520similar%2520images%252C%2520we%2520challenge%2520models%2520to%2520identify%2520both%250Amatching%2520and%2520distinct%2520components.%2520We%2520utilize%2520the%2520Stable-Diffusion-XL%2520model%2520and%250Aadvanced%2520image%2520editing%2520techniques%2520to%2520create%2520pairs%2520of%2520similar%2520images%2520that%250Ahighlight%2520object%2520replacements.%2520Our%2520methodology%2520includes%2520a%2520Difference%2520Area%250AGenerator%2520for%2520object%2520differences%2520identifying%252C%2520followed%2520by%2520a%2520Difference%2520Captions%250AGenerator%2520for%2520detailed%2520difference%2520descriptions.%2520The%2520result%2520is%2520a%2520relatively%250Asmall%2520but%2520high-quality%2520dataset%2520of%2520%2522object%2520replacement%2522%2520samples.%2520We%2520use%2520the%2520the%250Aproposed%2520dataset%2520to%2520finetune%2520state-of-the-art%2520%2528SOTA%2529%2520MLLMs%2520such%2520as%2520MGM-7B%252C%250Ayielding%2520comprehensive%2520improvements%2520of%2520performance%2520scores%2520over%2520SOTA%2520models%2520that%250Atrained%2520with%2520larger-scale%2520datasets%252C%2520in%2520numerous%2520image%2520difference%2520and%2520Visual%250AQuestion%2520Answering%2520tasks.%2520For%2520instance%252C%2520our%2520trained%2520models%2520notably%2520surpass%2520the%250ASOTA%2520models%2520GPT-4V%2520and%2520Gemini%2520on%2520the%2520MMVP%2520benchmark.%2520Besides%252C%2520we%2520investigate%250Aalternative%2520methods%2520for%2520generating%2520image%2520difference%2520data%2520through%2520%2522object%250Aremoval%2522%2520and%2520conduct%2520a%2520thorough%2520evaluation%2520to%2520confirm%2520the%2520dataset%2527s%2520diversity%252C%250Aquality%252C%2520and%2520robustness%252C%2520presenting%2520several%2520insights%2520on%2520the%2520synthesis%2520of%2520such%2520a%250Acontrastive%2520dataset.%2520To%2520encourage%2520further%2520research%2520and%2520advance%2520the%2520field%2520of%250Amultimodal%2520data%2520synthesis%2520and%2520enhancement%2520of%2520MLLMs%2527%2520fundamental%2520capabilities%250Afor%2520image%2520understanding%252C%2520we%2520release%2520our%2520codes%2520and%2520dataset%2520at%250Ahttps%253A//github.com/modelscope/data-juicer/tree/ImgDiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04594v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Yaliang%20Li%20and%20Ying%20Shen&entry.1292438233=%20%20High-performance%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20rely%20heavily%20on%0Adata%20quality.%20This%20study%20introduces%20a%20novel%20dataset%20named%20Img-Diff%2C%20designed%20to%0Aenhance%20fine-grained%20image%20recognition%20in%20MLLMs%20by%20leveraging%20insights%20from%0Acontrastive%20learning%20and%20image%20difference%20captioning.%20By%20analyzing%20object%0Adifferences%20between%20similar%20images%2C%20we%20challenge%20models%20to%20identify%20both%0Amatching%20and%20distinct%20components.%20We%20utilize%20the%20Stable-Diffusion-XL%20model%20and%0Aadvanced%20image%20editing%20techniques%20to%20create%20pairs%20of%20similar%20images%20that%0Ahighlight%20object%20replacements.%20Our%20methodology%20includes%20a%20Difference%20Area%0AGenerator%20for%20object%20differences%20identifying%2C%20followed%20by%20a%20Difference%20Captions%0AGenerator%20for%20detailed%20difference%20descriptions.%20The%20result%20is%20a%20relatively%0Asmall%20but%20high-quality%20dataset%20of%20%22object%20replacement%22%20samples.%20We%20use%20the%20the%0Aproposed%20dataset%20to%20finetune%20state-of-the-art%20%28SOTA%29%20MLLMs%20such%20as%20MGM-7B%2C%0Ayielding%20comprehensive%20improvements%20of%20performance%20scores%20over%20SOTA%20models%20that%0Atrained%20with%20larger-scale%20datasets%2C%20in%20numerous%20image%20difference%20and%20Visual%0AQuestion%20Answering%20tasks.%20For%20instance%2C%20our%20trained%20models%20notably%20surpass%20the%0ASOTA%20models%20GPT-4V%20and%20Gemini%20on%20the%20MMVP%20benchmark.%20Besides%2C%20we%20investigate%0Aalternative%20methods%20for%20generating%20image%20difference%20data%20through%20%22object%0Aremoval%22%20and%20conduct%20a%20thorough%20evaluation%20to%20confirm%20the%20dataset%27s%20diversity%2C%0Aquality%2C%20and%20robustness%2C%20presenting%20several%20insights%20on%20the%20synthesis%20of%20such%20a%0Acontrastive%20dataset.%20To%20encourage%20further%20research%20and%20advance%20the%20field%20of%0Amultimodal%20data%20synthesis%20and%20enhancement%20of%20MLLMs%27%20fundamental%20capabilities%0Afor%20image%20understanding%2C%20we%20release%20our%20codes%20and%20dataset%20at%0Ahttps%3A//github.com/modelscope/data-juicer/tree/ImgDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04594v2&entry.124074799=Read"},
{"title": "MooER: LLM-based Speech Recognition and Translation Models from Moore\n  Threads", "author": "Junhao Xu and Zhenlin Liang and Yi Liu and Yichao Hu and Jian Li and Yajun Zheng and Meng Cai and Hua Wang", "abstract": "  In this paper, we present MooER, a LLM-based large-scale automatic speech\nrecognition (ASR) / automatic speech translation (AST) model of Moore Threads.\nA 5000h pseudo labeled dataset containing open source and self collected speech\ndata is used for training. We achieve performance comparable to other open\nsource models trained with up to hundreds of thousands of hours of labeled\nspeech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest\nthat our model outperforms other open source Speech LLMs. A BLEU score of 25.2\ncan be obtained. The main contributions of this paper are summarized as\nfollows. First, this paper presents a training strategy for encoders and LLMs\non speech related tasks (including ASR and AST) using a small size of pseudo\nlabeled data without any extra manual annotation and selection. Second, we\nrelease our ASR and AST models and plan to open-source our training code and\nstrategy in the near future. Moreover, a model trained on 8wh scale training\ndata is planned to be released later on.\n", "link": "http://arxiv.org/abs/2408.05101v1", "date": "2024-08-09", "relevancy": 1.7716, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4635}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4408}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MooER%3A%20LLM-based%20Speech%20Recognition%20and%20Translation%20Models%20from%20Moore%0A%20%20Threads&body=Title%3A%20MooER%3A%20LLM-based%20Speech%20Recognition%20and%20Translation%20Models%20from%20Moore%0A%20%20Threads%0AAuthor%3A%20Junhao%20Xu%20and%20Zhenlin%20Liang%20and%20Yi%20Liu%20and%20Yichao%20Hu%20and%20Jian%20Li%20and%20Yajun%20Zheng%20and%20Meng%20Cai%20and%20Hua%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20MooER%2C%20a%20LLM-based%20large-scale%20automatic%20speech%0Arecognition%20%28ASR%29%20/%20automatic%20speech%20translation%20%28AST%29%20model%20of%20Moore%20Threads.%0AA%205000h%20pseudo%20labeled%20dataset%20containing%20open%20source%20and%20self%20collected%20speech%0Adata%20is%20used%20for%20training.%20We%20achieve%20performance%20comparable%20to%20other%20open%0Asource%20models%20trained%20with%20up%20to%20hundreds%20of%20thousands%20of%20hours%20of%20labeled%0Aspeech%20data.%20Meanwhile%2C%20experiments%20conducted%20on%20Covost2%20Zh2en%20testset%20suggest%0Athat%20our%20model%20outperforms%20other%20open%20source%20Speech%20LLMs.%20A%20BLEU%20score%20of%2025.2%0Acan%20be%20obtained.%20The%20main%20contributions%20of%20this%20paper%20are%20summarized%20as%0Afollows.%20First%2C%20this%20paper%20presents%20a%20training%20strategy%20for%20encoders%20and%20LLMs%0Aon%20speech%20related%20tasks%20%28including%20ASR%20and%20AST%29%20using%20a%20small%20size%20of%20pseudo%0Alabeled%20data%20without%20any%20extra%20manual%20annotation%20and%20selection.%20Second%2C%20we%0Arelease%20our%20ASR%20and%20AST%20models%20and%20plan%20to%20open-source%20our%20training%20code%20and%0Astrategy%20in%20the%20near%20future.%20Moreover%2C%20a%20model%20trained%20on%208wh%20scale%20training%0Adata%20is%20planned%20to%20be%20released%20later%20on.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMooER%253A%2520LLM-based%2520Speech%2520Recognition%2520and%2520Translation%2520Models%2520from%2520Moore%250A%2520%2520Threads%26entry.906535625%3DJunhao%2520Xu%2520and%2520Zhenlin%2520Liang%2520and%2520Yi%2520Liu%2520and%2520Yichao%2520Hu%2520and%2520Jian%2520Li%2520and%2520Yajun%2520Zheng%2520and%2520Meng%2520Cai%2520and%2520Hua%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520MooER%252C%2520a%2520LLM-based%2520large-scale%2520automatic%2520speech%250Arecognition%2520%2528ASR%2529%2520/%2520automatic%2520speech%2520translation%2520%2528AST%2529%2520model%2520of%2520Moore%2520Threads.%250AA%25205000h%2520pseudo%2520labeled%2520dataset%2520containing%2520open%2520source%2520and%2520self%2520collected%2520speech%250Adata%2520is%2520used%2520for%2520training.%2520We%2520achieve%2520performance%2520comparable%2520to%2520other%2520open%250Asource%2520models%2520trained%2520with%2520up%2520to%2520hundreds%2520of%2520thousands%2520of%2520hours%2520of%2520labeled%250Aspeech%2520data.%2520Meanwhile%252C%2520experiments%2520conducted%2520on%2520Covost2%2520Zh2en%2520testset%2520suggest%250Athat%2520our%2520model%2520outperforms%2520other%2520open%2520source%2520Speech%2520LLMs.%2520A%2520BLEU%2520score%2520of%252025.2%250Acan%2520be%2520obtained.%2520The%2520main%2520contributions%2520of%2520this%2520paper%2520are%2520summarized%2520as%250Afollows.%2520First%252C%2520this%2520paper%2520presents%2520a%2520training%2520strategy%2520for%2520encoders%2520and%2520LLMs%250Aon%2520speech%2520related%2520tasks%2520%2528including%2520ASR%2520and%2520AST%2529%2520using%2520a%2520small%2520size%2520of%2520pseudo%250Alabeled%2520data%2520without%2520any%2520extra%2520manual%2520annotation%2520and%2520selection.%2520Second%252C%2520we%250Arelease%2520our%2520ASR%2520and%2520AST%2520models%2520and%2520plan%2520to%2520open-source%2520our%2520training%2520code%2520and%250Astrategy%2520in%2520the%2520near%2520future.%2520Moreover%252C%2520a%2520model%2520trained%2520on%25208wh%2520scale%2520training%250Adata%2520is%2520planned%2520to%2520be%2520released%2520later%2520on.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MooER%3A%20LLM-based%20Speech%20Recognition%20and%20Translation%20Models%20from%20Moore%0A%20%20Threads&entry.906535625=Junhao%20Xu%20and%20Zhenlin%20Liang%20and%20Yi%20Liu%20and%20Yichao%20Hu%20and%20Jian%20Li%20and%20Yajun%20Zheng%20and%20Meng%20Cai%20and%20Hua%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20MooER%2C%20a%20LLM-based%20large-scale%20automatic%20speech%0Arecognition%20%28ASR%29%20/%20automatic%20speech%20translation%20%28AST%29%20model%20of%20Moore%20Threads.%0AA%205000h%20pseudo%20labeled%20dataset%20containing%20open%20source%20and%20self%20collected%20speech%0Adata%20is%20used%20for%20training.%20We%20achieve%20performance%20comparable%20to%20other%20open%0Asource%20models%20trained%20with%20up%20to%20hundreds%20of%20thousands%20of%20hours%20of%20labeled%0Aspeech%20data.%20Meanwhile%2C%20experiments%20conducted%20on%20Covost2%20Zh2en%20testset%20suggest%0Athat%20our%20model%20outperforms%20other%20open%20source%20Speech%20LLMs.%20A%20BLEU%20score%20of%2025.2%0Acan%20be%20obtained.%20The%20main%20contributions%20of%20this%20paper%20are%20summarized%20as%0Afollows.%20First%2C%20this%20paper%20presents%20a%20training%20strategy%20for%20encoders%20and%20LLMs%0Aon%20speech%20related%20tasks%20%28including%20ASR%20and%20AST%29%20using%20a%20small%20size%20of%20pseudo%0Alabeled%20data%20without%20any%20extra%20manual%20annotation%20and%20selection.%20Second%2C%20we%0Arelease%20our%20ASR%20and%20AST%20models%20and%20plan%20to%20open-source%20our%20training%20code%20and%0Astrategy%20in%20the%20near%20future.%20Moreover%2C%20a%20model%20trained%20on%208wh%20scale%20training%0Adata%20is%20planned%20to%20be%20released%20later%20on.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05101v1&entry.124074799=Read"},
{"title": "Decoding Quantum LDPC Codes Using Graph Neural Networks", "author": "Vukan Ninkovic and Ognjen Kundacina and Dejan Vukobratovic and Christian H\u00e4ger and Alexandre Graell i Amat", "abstract": "  In this paper, we propose a novel decoding method for Quantum Low-Density\nParity-Check (QLDPC) codes based on Graph Neural Networks (GNNs). Similar to\nthe Belief Propagation (BP)-based QLDPC decoders, the proposed GNN-based QLDPC\ndecoder exploits the sparse graph structure of QLDPC codes and can be\nimplemented as a message-passing decoding algorithm. We compare the proposed\nGNN-based decoding algorithm against selected classes of both conventional and\nneural-enhanced QLDPC decoding algorithms across several QLDPC code designs.\nThe simulation results demonstrate excellent performance of GNN-based decoders\nalong with their low complexity compared to competing methods.\n", "link": "http://arxiv.org/abs/2408.05170v1", "date": "2024-08-09", "relevancy": 1.7674, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4598}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4318}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Quantum%20LDPC%20Codes%20Using%20Graph%20Neural%20Networks&body=Title%3A%20Decoding%20Quantum%20LDPC%20Codes%20Using%20Graph%20Neural%20Networks%0AAuthor%3A%20Vukan%20Ninkovic%20and%20Ognjen%20Kundacina%20and%20Dejan%20Vukobratovic%20and%20Christian%20H%C3%A4ger%20and%20Alexandre%20Graell%20i%20Amat%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20decoding%20method%20for%20Quantum%20Low-Density%0AParity-Check%20%28QLDPC%29%20codes%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29.%20Similar%20to%0Athe%20Belief%20Propagation%20%28BP%29-based%20QLDPC%20decoders%2C%20the%20proposed%20GNN-based%20QLDPC%0Adecoder%20exploits%20the%20sparse%20graph%20structure%20of%20QLDPC%20codes%20and%20can%20be%0Aimplemented%20as%20a%20message-passing%20decoding%20algorithm.%20We%20compare%20the%20proposed%0AGNN-based%20decoding%20algorithm%20against%20selected%20classes%20of%20both%20conventional%20and%0Aneural-enhanced%20QLDPC%20decoding%20algorithms%20across%20several%20QLDPC%20code%20designs.%0AThe%20simulation%20results%20demonstrate%20excellent%20performance%20of%20GNN-based%20decoders%0Aalong%20with%20their%20low%20complexity%20compared%20to%20competing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Quantum%2520LDPC%2520Codes%2520Using%2520Graph%2520Neural%2520Networks%26entry.906535625%3DVukan%2520Ninkovic%2520and%2520Ognjen%2520Kundacina%2520and%2520Dejan%2520Vukobratovic%2520and%2520Christian%2520H%25C3%25A4ger%2520and%2520Alexandre%2520Graell%2520i%2520Amat%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520decoding%2520method%2520for%2520Quantum%2520Low-Density%250AParity-Check%2520%2528QLDPC%2529%2520codes%2520based%2520on%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520Similar%2520to%250Athe%2520Belief%2520Propagation%2520%2528BP%2529-based%2520QLDPC%2520decoders%252C%2520the%2520proposed%2520GNN-based%2520QLDPC%250Adecoder%2520exploits%2520the%2520sparse%2520graph%2520structure%2520of%2520QLDPC%2520codes%2520and%2520can%2520be%250Aimplemented%2520as%2520a%2520message-passing%2520decoding%2520algorithm.%2520We%2520compare%2520the%2520proposed%250AGNN-based%2520decoding%2520algorithm%2520against%2520selected%2520classes%2520of%2520both%2520conventional%2520and%250Aneural-enhanced%2520QLDPC%2520decoding%2520algorithms%2520across%2520several%2520QLDPC%2520code%2520designs.%250AThe%2520simulation%2520results%2520demonstrate%2520excellent%2520performance%2520of%2520GNN-based%2520decoders%250Aalong%2520with%2520their%2520low%2520complexity%2520compared%2520to%2520competing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Quantum%20LDPC%20Codes%20Using%20Graph%20Neural%20Networks&entry.906535625=Vukan%20Ninkovic%20and%20Ognjen%20Kundacina%20and%20Dejan%20Vukobratovic%20and%20Christian%20H%C3%A4ger%20and%20Alexandre%20Graell%20i%20Amat&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20decoding%20method%20for%20Quantum%20Low-Density%0AParity-Check%20%28QLDPC%29%20codes%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29.%20Similar%20to%0Athe%20Belief%20Propagation%20%28BP%29-based%20QLDPC%20decoders%2C%20the%20proposed%20GNN-based%20QLDPC%0Adecoder%20exploits%20the%20sparse%20graph%20structure%20of%20QLDPC%20codes%20and%20can%20be%0Aimplemented%20as%20a%20message-passing%20decoding%20algorithm.%20We%20compare%20the%20proposed%0AGNN-based%20decoding%20algorithm%20against%20selected%20classes%20of%20both%20conventional%20and%0Aneural-enhanced%20QLDPC%20decoding%20algorithms%20across%20several%20QLDPC%20code%20designs.%0AThe%20simulation%20results%20demonstrate%20excellent%20performance%20of%20GNN-based%20decoders%0Aalong%20with%20their%20low%20complexity%20compared%20to%20competing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05170v1&entry.124074799=Read"},
{"title": "Variational Bayesian Phylogenetic Inference with Semi-implicit Branch\n  Length Distributions", "author": "Tianyu Xie and Frederick A. Matsen IV and Marc A. Suchard and Cheng Zhang", "abstract": "  Reconstructing the evolutionary history relating a collection of molecular\nsequences is the main subject of modern Bayesian phylogenetic inference.\nHowever, the commonly used Markov chain Monte Carlo methods can be inefficient\ndue to the complicated space of phylogenetic trees, especially when the number\nof sequences is large. An alternative approach is variational Bayesian\nphylogenetic inference (VBPI) which transforms the inference problem into an\noptimization problem. While effective, the default diagonal lognormal\napproximation for the branch lengths of the tree used in VBPI is often\ninsufficient to capture the complexity of the exact posterior. In this work, we\npropose a more flexible family of branch length variational posteriors based on\nsemi-implicit hierarchical distributions using graph neural networks. We show\nthat this semi-implicit construction emits straightforward permutation\nequivariant distributions, and therefore can handle the non-Euclidean branch\nlength space across different tree topologies with ease. To deal with the\nintractable marginal probability of semi-implicit variational distributions, we\ndevelop several alternative lower bounds for stochastic optimization. We\ndemonstrate the effectiveness of our proposed method over baseline methods on\nbenchmark data examples, in terms of both marginal likelihood estimation and\nbranch length posterior approximation.\n", "link": "http://arxiv.org/abs/2408.05058v1", "date": "2024-08-09", "relevancy": 1.7199, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5205}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4155}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Bayesian%20Phylogenetic%20Inference%20with%20Semi-implicit%20Branch%0A%20%20Length%20Distributions&body=Title%3A%20Variational%20Bayesian%20Phylogenetic%20Inference%20with%20Semi-implicit%20Branch%0A%20%20Length%20Distributions%0AAuthor%3A%20Tianyu%20Xie%20and%20Frederick%20A.%20Matsen%20IV%20and%20Marc%20A.%20Suchard%20and%20Cheng%20Zhang%0AAbstract%3A%20%20%20Reconstructing%20the%20evolutionary%20history%20relating%20a%20collection%20of%20molecular%0Asequences%20is%20the%20main%20subject%20of%20modern%20Bayesian%20phylogenetic%20inference.%0AHowever%2C%20the%20commonly%20used%20Markov%20chain%20Monte%20Carlo%20methods%20can%20be%20inefficient%0Adue%20to%20the%20complicated%20space%20of%20phylogenetic%20trees%2C%20especially%20when%20the%20number%0Aof%20sequences%20is%20large.%20An%20alternative%20approach%20is%20variational%20Bayesian%0Aphylogenetic%20inference%20%28VBPI%29%20which%20transforms%20the%20inference%20problem%20into%20an%0Aoptimization%20problem.%20While%20effective%2C%20the%20default%20diagonal%20lognormal%0Aapproximation%20for%20the%20branch%20lengths%20of%20the%20tree%20used%20in%20VBPI%20is%20often%0Ainsufficient%20to%20capture%20the%20complexity%20of%20the%20exact%20posterior.%20In%20this%20work%2C%20we%0Apropose%20a%20more%20flexible%20family%20of%20branch%20length%20variational%20posteriors%20based%20on%0Asemi-implicit%20hierarchical%20distributions%20using%20graph%20neural%20networks.%20We%20show%0Athat%20this%20semi-implicit%20construction%20emits%20straightforward%20permutation%0Aequivariant%20distributions%2C%20and%20therefore%20can%20handle%20the%20non-Euclidean%20branch%0Alength%20space%20across%20different%20tree%20topologies%20with%20ease.%20To%20deal%20with%20the%0Aintractable%20marginal%20probability%20of%20semi-implicit%20variational%20distributions%2C%20we%0Adevelop%20several%20alternative%20lower%20bounds%20for%20stochastic%20optimization.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20over%20baseline%20methods%20on%0Abenchmark%20data%20examples%2C%20in%20terms%20of%20both%20marginal%20likelihood%20estimation%20and%0Abranch%20length%20posterior%20approximation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Bayesian%2520Phylogenetic%2520Inference%2520with%2520Semi-implicit%2520Branch%250A%2520%2520Length%2520Distributions%26entry.906535625%3DTianyu%2520Xie%2520and%2520Frederick%2520A.%2520Matsen%2520IV%2520and%2520Marc%2520A.%2520Suchard%2520and%2520Cheng%2520Zhang%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520evolutionary%2520history%2520relating%2520a%2520collection%2520of%2520molecular%250Asequences%2520is%2520the%2520main%2520subject%2520of%2520modern%2520Bayesian%2520phylogenetic%2520inference.%250AHowever%252C%2520the%2520commonly%2520used%2520Markov%2520chain%2520Monte%2520Carlo%2520methods%2520can%2520be%2520inefficient%250Adue%2520to%2520the%2520complicated%2520space%2520of%2520phylogenetic%2520trees%252C%2520especially%2520when%2520the%2520number%250Aof%2520sequences%2520is%2520large.%2520An%2520alternative%2520approach%2520is%2520variational%2520Bayesian%250Aphylogenetic%2520inference%2520%2528VBPI%2529%2520which%2520transforms%2520the%2520inference%2520problem%2520into%2520an%250Aoptimization%2520problem.%2520While%2520effective%252C%2520the%2520default%2520diagonal%2520lognormal%250Aapproximation%2520for%2520the%2520branch%2520lengths%2520of%2520the%2520tree%2520used%2520in%2520VBPI%2520is%2520often%250Ainsufficient%2520to%2520capture%2520the%2520complexity%2520of%2520the%2520exact%2520posterior.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520more%2520flexible%2520family%2520of%2520branch%2520length%2520variational%2520posteriors%2520based%2520on%250Asemi-implicit%2520hierarchical%2520distributions%2520using%2520graph%2520neural%2520networks.%2520We%2520show%250Athat%2520this%2520semi-implicit%2520construction%2520emits%2520straightforward%2520permutation%250Aequivariant%2520distributions%252C%2520and%2520therefore%2520can%2520handle%2520the%2520non-Euclidean%2520branch%250Alength%2520space%2520across%2520different%2520tree%2520topologies%2520with%2520ease.%2520To%2520deal%2520with%2520the%250Aintractable%2520marginal%2520probability%2520of%2520semi-implicit%2520variational%2520distributions%252C%2520we%250Adevelop%2520several%2520alternative%2520lower%2520bounds%2520for%2520stochastic%2520optimization.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%2520over%2520baseline%2520methods%2520on%250Abenchmark%2520data%2520examples%252C%2520in%2520terms%2520of%2520both%2520marginal%2520likelihood%2520estimation%2520and%250Abranch%2520length%2520posterior%2520approximation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Bayesian%20Phylogenetic%20Inference%20with%20Semi-implicit%20Branch%0A%20%20Length%20Distributions&entry.906535625=Tianyu%20Xie%20and%20Frederick%20A.%20Matsen%20IV%20and%20Marc%20A.%20Suchard%20and%20Cheng%20Zhang&entry.1292438233=%20%20Reconstructing%20the%20evolutionary%20history%20relating%20a%20collection%20of%20molecular%0Asequences%20is%20the%20main%20subject%20of%20modern%20Bayesian%20phylogenetic%20inference.%0AHowever%2C%20the%20commonly%20used%20Markov%20chain%20Monte%20Carlo%20methods%20can%20be%20inefficient%0Adue%20to%20the%20complicated%20space%20of%20phylogenetic%20trees%2C%20especially%20when%20the%20number%0Aof%20sequences%20is%20large.%20An%20alternative%20approach%20is%20variational%20Bayesian%0Aphylogenetic%20inference%20%28VBPI%29%20which%20transforms%20the%20inference%20problem%20into%20an%0Aoptimization%20problem.%20While%20effective%2C%20the%20default%20diagonal%20lognormal%0Aapproximation%20for%20the%20branch%20lengths%20of%20the%20tree%20used%20in%20VBPI%20is%20often%0Ainsufficient%20to%20capture%20the%20complexity%20of%20the%20exact%20posterior.%20In%20this%20work%2C%20we%0Apropose%20a%20more%20flexible%20family%20of%20branch%20length%20variational%20posteriors%20based%20on%0Asemi-implicit%20hierarchical%20distributions%20using%20graph%20neural%20networks.%20We%20show%0Athat%20this%20semi-implicit%20construction%20emits%20straightforward%20permutation%0Aequivariant%20distributions%2C%20and%20therefore%20can%20handle%20the%20non-Euclidean%20branch%0Alength%20space%20across%20different%20tree%20topologies%20with%20ease.%20To%20deal%20with%20the%0Aintractable%20marginal%20probability%20of%20semi-implicit%20variational%20distributions%2C%20we%0Adevelop%20several%20alternative%20lower%20bounds%20for%20stochastic%20optimization.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20over%20baseline%20methods%20on%0Abenchmark%20data%20examples%2C%20in%20terms%20of%20both%20marginal%20likelihood%20estimation%20and%0Abranch%20length%20posterior%20approximation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05058v1&entry.124074799=Read"},
{"title": "DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model", "author": "Yiming Zhong and Xiaolin Zhang and Yao Zhao and Yunchao Wei", "abstract": "  Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents. In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models. In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.\n", "link": "http://arxiv.org/abs/2408.02993v2", "date": "2024-08-09", "relevancy": 1.7139, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5752}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.571}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamLCM%3A%20Towards%20High-Quality%20Text-to-3D%20Generation%20via%20Latent%0A%20%20Consistency%20Model&body=Title%3A%20DreamLCM%3A%20Towards%20High-Quality%20Text-to-3D%20Generation%20via%20Latent%0A%20%20Consistency%20Model%0AAuthor%3A%20Yiming%20Zhong%20and%20Xiaolin%20Zhang%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20Recently%2C%20the%20text-to-3D%20task%20has%20developed%20rapidly%20due%20to%20the%20appearance%20of%0Athe%20SDS%20method.%20However%2C%20the%20SDS%20method%20always%20generates%203D%20objects%20with%20poor%0Aquality%20due%20to%20the%20over-smooth%20issue.%20This%20issue%20is%20attributed%20to%20two%20factors%3A%0A1%29%20the%20DDPM%20single-step%20inference%20produces%20poor%20guidance%20gradients%3B%202%29%20the%0Arandomness%20from%20the%20input%20noises%20and%20timesteps%20averages%20the%20details%20of%20the%203D%0Acontents.%20In%20this%20paper%2C%20to%20address%20the%20issue%2C%20we%20propose%20DreamLCM%20which%0Aincorporates%20the%20Latent%20Consistency%20Model%20%28LCM%29.%20DreamLCM%20leverages%20the%0Apowerful%20image%20generation%20capabilities%20inherent%20in%20LCM%2C%20enabling%20generating%0Aconsistent%20and%20high-quality%20guidance%2C%20i.e.%2C%20predicted%20noises%20or%20images.%20Powered%0Aby%20the%20improved%20guidance%2C%20the%20proposed%20method%20can%20provide%20accurate%20and%20detailed%0Agradients%20to%20optimize%20the%20target%203D%20models.%20In%20addition%2C%20we%20propose%20two%0Astrategies%20to%20enhance%20the%20generation%20quality%20further.%20Firstly%2C%20we%20propose%20a%0Aguidance%20calibration%20strategy%2C%20utilizing%20Euler%20Solver%20to%20calibrate%20the%20guidance%0Adistribution%20to%20accelerate%203D%20models%20to%20converge.%20Secondly%2C%20we%20propose%20a%20dual%0Atimestep%20strategy%2C%20increasing%20the%20consistency%20of%20guidance%20and%20optimizing%203D%0Amodels%20from%20geometry%20to%20appearance%20in%20DreamLCM.%20Experiments%20show%20that%20DreamLCM%0Aachieves%20state-of-the-art%20results%20in%20both%20generation%20quality%20and%20training%0Aefficiency.%20The%20code%20is%20available%20at%20https%3A//github.com/1YimingZhong/DreamLCM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02993v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamLCM%253A%2520Towards%2520High-Quality%2520Text-to-3D%2520Generation%2520via%2520Latent%250A%2520%2520Consistency%2520Model%26entry.906535625%3DYiming%2520Zhong%2520and%2520Xiaolin%2520Zhang%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520text-to-3D%2520task%2520has%2520developed%2520rapidly%2520due%2520to%2520the%2520appearance%2520of%250Athe%2520SDS%2520method.%2520However%252C%2520the%2520SDS%2520method%2520always%2520generates%25203D%2520objects%2520with%2520poor%250Aquality%2520due%2520to%2520the%2520over-smooth%2520issue.%2520This%2520issue%2520is%2520attributed%2520to%2520two%2520factors%253A%250A1%2529%2520the%2520DDPM%2520single-step%2520inference%2520produces%2520poor%2520guidance%2520gradients%253B%25202%2529%2520the%250Arandomness%2520from%2520the%2520input%2520noises%2520and%2520timesteps%2520averages%2520the%2520details%2520of%2520the%25203D%250Acontents.%2520In%2520this%2520paper%252C%2520to%2520address%2520the%2520issue%252C%2520we%2520propose%2520DreamLCM%2520which%250Aincorporates%2520the%2520Latent%2520Consistency%2520Model%2520%2528LCM%2529.%2520DreamLCM%2520leverages%2520the%250Apowerful%2520image%2520generation%2520capabilities%2520inherent%2520in%2520LCM%252C%2520enabling%2520generating%250Aconsistent%2520and%2520high-quality%2520guidance%252C%2520i.e.%252C%2520predicted%2520noises%2520or%2520images.%2520Powered%250Aby%2520the%2520improved%2520guidance%252C%2520the%2520proposed%2520method%2520can%2520provide%2520accurate%2520and%2520detailed%250Agradients%2520to%2520optimize%2520the%2520target%25203D%2520models.%2520In%2520addition%252C%2520we%2520propose%2520two%250Astrategies%2520to%2520enhance%2520the%2520generation%2520quality%2520further.%2520Firstly%252C%2520we%2520propose%2520a%250Aguidance%2520calibration%2520strategy%252C%2520utilizing%2520Euler%2520Solver%2520to%2520calibrate%2520the%2520guidance%250Adistribution%2520to%2520accelerate%25203D%2520models%2520to%2520converge.%2520Secondly%252C%2520we%2520propose%2520a%2520dual%250Atimestep%2520strategy%252C%2520increasing%2520the%2520consistency%2520of%2520guidance%2520and%2520optimizing%25203D%250Amodels%2520from%2520geometry%2520to%2520appearance%2520in%2520DreamLCM.%2520Experiments%2520show%2520that%2520DreamLCM%250Aachieves%2520state-of-the-art%2520results%2520in%2520both%2520generation%2520quality%2520and%2520training%250Aefficiency.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/1YimingZhong/DreamLCM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02993v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamLCM%3A%20Towards%20High-Quality%20Text-to-3D%20Generation%20via%20Latent%0A%20%20Consistency%20Model&entry.906535625=Yiming%20Zhong%20and%20Xiaolin%20Zhang%20and%20Yao%20Zhao%20and%20Yunchao%20Wei&entry.1292438233=%20%20Recently%2C%20the%20text-to-3D%20task%20has%20developed%20rapidly%20due%20to%20the%20appearance%20of%0Athe%20SDS%20method.%20However%2C%20the%20SDS%20method%20always%20generates%203D%20objects%20with%20poor%0Aquality%20due%20to%20the%20over-smooth%20issue.%20This%20issue%20is%20attributed%20to%20two%20factors%3A%0A1%29%20the%20DDPM%20single-step%20inference%20produces%20poor%20guidance%20gradients%3B%202%29%20the%0Arandomness%20from%20the%20input%20noises%20and%20timesteps%20averages%20the%20details%20of%20the%203D%0Acontents.%20In%20this%20paper%2C%20to%20address%20the%20issue%2C%20we%20propose%20DreamLCM%20which%0Aincorporates%20the%20Latent%20Consistency%20Model%20%28LCM%29.%20DreamLCM%20leverages%20the%0Apowerful%20image%20generation%20capabilities%20inherent%20in%20LCM%2C%20enabling%20generating%0Aconsistent%20and%20high-quality%20guidance%2C%20i.e.%2C%20predicted%20noises%20or%20images.%20Powered%0Aby%20the%20improved%20guidance%2C%20the%20proposed%20method%20can%20provide%20accurate%20and%20detailed%0Agradients%20to%20optimize%20the%20target%203D%20models.%20In%20addition%2C%20we%20propose%20two%0Astrategies%20to%20enhance%20the%20generation%20quality%20further.%20Firstly%2C%20we%20propose%20a%0Aguidance%20calibration%20strategy%2C%20utilizing%20Euler%20Solver%20to%20calibrate%20the%20guidance%0Adistribution%20to%20accelerate%203D%20models%20to%20converge.%20Secondly%2C%20we%20propose%20a%20dual%0Atimestep%20strategy%2C%20increasing%20the%20consistency%20of%20guidance%20and%20optimizing%203D%0Amodels%20from%20geometry%20to%20appearance%20in%20DreamLCM.%20Experiments%20show%20that%20DreamLCM%0Aachieves%20state-of-the-art%20results%20in%20both%20generation%20quality%20and%20training%0Aefficiency.%20The%20code%20is%20available%20at%20https%3A//github.com/1YimingZhong/DreamLCM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02993v2&entry.124074799=Read"},
{"title": "Shifting the Lens: Detecting Malicious npm Packages using Large Language\n  Models", "author": "Nusrat Zahan and Philipp Burckhardt and Mikola Lysenko and Feross Aboukhadijeh and Laurie Williams", "abstract": "  Existing malicious code detection techniques can aid the manual review\nprocess by predicting which packages are likely to be malicious. However, these\ntechniques often suffer from high misclassification rates. Therefore, malicious\ncode detection techniques could be enhanced by adopting advanced, more\nautomated approaches to achieve high accuracy and a low misclassification rate.\nThe goal of this study is to assist security analysts in detecting malicious\npackages through the empirical study of using Large Language Models (LLMs) to\ndetect malicious code in the npm ecosystem. We present SecurityAI, a malicious\ncode review workflow to detect malicious code using ChatGPT. We leverage a\nbenchmark dataset of 5,115 npm packages, of which 2,180 packages have malicious\ncode. We conducted a baseline comparison of GPT-3 and GPT- 4 models with the\nstate-of-the-art CodeQL static analysis tool, using 39 custom CodeQL rules\ndeveloped in prior research to detect malicious Javascript code. We compare the\neffectiveness of static analysis as a pre-screener with SecurityAI workflow,\nmeasuring the number of files that need to be analyzed and the associated\ncosts. Additionally, we performed a qualitative study to understand the types\nof malicious packages detected or missed by our workflow. Our baseline\ncomparison demonstrates a 16% and 9% improvement over static analysis in\nprecision and F1 scores, respectively. We attained precision and F1 scores of\n91% and 94% for GPT-3, and 99% & 97% for GPT-4, respectively, with GPT-3\noffering a cost-effective balance. Pre-screening files with a static analyzer\nreduces the number of files requiring LLM analysis by 77.9% and decreases costs\nby 60.9% for GPT-3 and 76.1% for GPT-4. Our qualitative analysis identified\ndata theft, hidden backdoors, and suspicious domain connection categories as\nthe top detected malicious packages.\n", "link": "http://arxiv.org/abs/2403.12196v2", "date": "2024-08-09", "relevancy": 1.7055, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4353}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4251}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shifting%20the%20Lens%3A%20Detecting%20Malicious%20npm%20Packages%20using%20Large%20Language%0A%20%20Models&body=Title%3A%20Shifting%20the%20Lens%3A%20Detecting%20Malicious%20npm%20Packages%20using%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Nusrat%20Zahan%20and%20Philipp%20Burckhardt%20and%20Mikola%20Lysenko%20and%20Feross%20Aboukhadijeh%20and%20Laurie%20Williams%0AAbstract%3A%20%20%20Existing%20malicious%20code%20detection%20techniques%20can%20aid%20the%20manual%20review%0Aprocess%20by%20predicting%20which%20packages%20are%20likely%20to%20be%20malicious.%20However%2C%20these%0Atechniques%20often%20suffer%20from%20high%20misclassification%20rates.%20Therefore%2C%20malicious%0Acode%20detection%20techniques%20could%20be%20enhanced%20by%20adopting%20advanced%2C%20more%0Aautomated%20approaches%20to%20achieve%20high%20accuracy%20and%20a%20low%20misclassification%20rate.%0AThe%20goal%20of%20this%20study%20is%20to%20assist%20security%20analysts%20in%20detecting%20malicious%0Apackages%20through%20the%20empirical%20study%20of%20using%20Large%20Language%20Models%20%28LLMs%29%20to%0Adetect%20malicious%20code%20in%20the%20npm%20ecosystem.%20We%20present%20SecurityAI%2C%20a%20malicious%0Acode%20review%20workflow%20to%20detect%20malicious%20code%20using%20ChatGPT.%20We%20leverage%20a%0Abenchmark%20dataset%20of%205%2C115%20npm%20packages%2C%20of%20which%202%2C180%20packages%20have%20malicious%0Acode.%20We%20conducted%20a%20baseline%20comparison%20of%20GPT-3%20and%20GPT-%204%20models%20with%20the%0Astate-of-the-art%20CodeQL%20static%20analysis%20tool%2C%20using%2039%20custom%20CodeQL%20rules%0Adeveloped%20in%20prior%20research%20to%20detect%20malicious%20Javascript%20code.%20We%20compare%20the%0Aeffectiveness%20of%20static%20analysis%20as%20a%20pre-screener%20with%20SecurityAI%20workflow%2C%0Ameasuring%20the%20number%20of%20files%20that%20need%20to%20be%20analyzed%20and%20the%20associated%0Acosts.%20Additionally%2C%20we%20performed%20a%20qualitative%20study%20to%20understand%20the%20types%0Aof%20malicious%20packages%20detected%20or%20missed%20by%20our%20workflow.%20Our%20baseline%0Acomparison%20demonstrates%20a%2016%25%20and%209%25%20improvement%20over%20static%20analysis%20in%0Aprecision%20and%20F1%20scores%2C%20respectively.%20We%20attained%20precision%20and%20F1%20scores%20of%0A91%25%20and%2094%25%20for%20GPT-3%2C%20and%2099%25%20%26%2097%25%20for%20GPT-4%2C%20respectively%2C%20with%20GPT-3%0Aoffering%20a%20cost-effective%20balance.%20Pre-screening%20files%20with%20a%20static%20analyzer%0Areduces%20the%20number%20of%20files%20requiring%20LLM%20analysis%20by%2077.9%25%20and%20decreases%20costs%0Aby%2060.9%25%20for%20GPT-3%20and%2076.1%25%20for%20GPT-4.%20Our%20qualitative%20analysis%20identified%0Adata%20theft%2C%20hidden%20backdoors%2C%20and%20suspicious%20domain%20connection%20categories%20as%0Athe%20top%20detected%20malicious%20packages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShifting%2520the%2520Lens%253A%2520Detecting%2520Malicious%2520npm%2520Packages%2520using%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DNusrat%2520Zahan%2520and%2520Philipp%2520Burckhardt%2520and%2520Mikola%2520Lysenko%2520and%2520Feross%2520Aboukhadijeh%2520and%2520Laurie%2520Williams%26entry.1292438233%3D%2520%2520Existing%2520malicious%2520code%2520detection%2520techniques%2520can%2520aid%2520the%2520manual%2520review%250Aprocess%2520by%2520predicting%2520which%2520packages%2520are%2520likely%2520to%2520be%2520malicious.%2520However%252C%2520these%250Atechniques%2520often%2520suffer%2520from%2520high%2520misclassification%2520rates.%2520Therefore%252C%2520malicious%250Acode%2520detection%2520techniques%2520could%2520be%2520enhanced%2520by%2520adopting%2520advanced%252C%2520more%250Aautomated%2520approaches%2520to%2520achieve%2520high%2520accuracy%2520and%2520a%2520low%2520misclassification%2520rate.%250AThe%2520goal%2520of%2520this%2520study%2520is%2520to%2520assist%2520security%2520analysts%2520in%2520detecting%2520malicious%250Apackages%2520through%2520the%2520empirical%2520study%2520of%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Adetect%2520malicious%2520code%2520in%2520the%2520npm%2520ecosystem.%2520We%2520present%2520SecurityAI%252C%2520a%2520malicious%250Acode%2520review%2520workflow%2520to%2520detect%2520malicious%2520code%2520using%2520ChatGPT.%2520We%2520leverage%2520a%250Abenchmark%2520dataset%2520of%25205%252C115%2520npm%2520packages%252C%2520of%2520which%25202%252C180%2520packages%2520have%2520malicious%250Acode.%2520We%2520conducted%2520a%2520baseline%2520comparison%2520of%2520GPT-3%2520and%2520GPT-%25204%2520models%2520with%2520the%250Astate-of-the-art%2520CodeQL%2520static%2520analysis%2520tool%252C%2520using%252039%2520custom%2520CodeQL%2520rules%250Adeveloped%2520in%2520prior%2520research%2520to%2520detect%2520malicious%2520Javascript%2520code.%2520We%2520compare%2520the%250Aeffectiveness%2520of%2520static%2520analysis%2520as%2520a%2520pre-screener%2520with%2520SecurityAI%2520workflow%252C%250Ameasuring%2520the%2520number%2520of%2520files%2520that%2520need%2520to%2520be%2520analyzed%2520and%2520the%2520associated%250Acosts.%2520Additionally%252C%2520we%2520performed%2520a%2520qualitative%2520study%2520to%2520understand%2520the%2520types%250Aof%2520malicious%2520packages%2520detected%2520or%2520missed%2520by%2520our%2520workflow.%2520Our%2520baseline%250Acomparison%2520demonstrates%2520a%252016%2525%2520and%25209%2525%2520improvement%2520over%2520static%2520analysis%2520in%250Aprecision%2520and%2520F1%2520scores%252C%2520respectively.%2520We%2520attained%2520precision%2520and%2520F1%2520scores%2520of%250A91%2525%2520and%252094%2525%2520for%2520GPT-3%252C%2520and%252099%2525%2520%2526%252097%2525%2520for%2520GPT-4%252C%2520respectively%252C%2520with%2520GPT-3%250Aoffering%2520a%2520cost-effective%2520balance.%2520Pre-screening%2520files%2520with%2520a%2520static%2520analyzer%250Areduces%2520the%2520number%2520of%2520files%2520requiring%2520LLM%2520analysis%2520by%252077.9%2525%2520and%2520decreases%2520costs%250Aby%252060.9%2525%2520for%2520GPT-3%2520and%252076.1%2525%2520for%2520GPT-4.%2520Our%2520qualitative%2520analysis%2520identified%250Adata%2520theft%252C%2520hidden%2520backdoors%252C%2520and%2520suspicious%2520domain%2520connection%2520categories%2520as%250Athe%2520top%2520detected%2520malicious%2520packages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shifting%20the%20Lens%3A%20Detecting%20Malicious%20npm%20Packages%20using%20Large%20Language%0A%20%20Models&entry.906535625=Nusrat%20Zahan%20and%20Philipp%20Burckhardt%20and%20Mikola%20Lysenko%20and%20Feross%20Aboukhadijeh%20and%20Laurie%20Williams&entry.1292438233=%20%20Existing%20malicious%20code%20detection%20techniques%20can%20aid%20the%20manual%20review%0Aprocess%20by%20predicting%20which%20packages%20are%20likely%20to%20be%20malicious.%20However%2C%20these%0Atechniques%20often%20suffer%20from%20high%20misclassification%20rates.%20Therefore%2C%20malicious%0Acode%20detection%20techniques%20could%20be%20enhanced%20by%20adopting%20advanced%2C%20more%0Aautomated%20approaches%20to%20achieve%20high%20accuracy%20and%20a%20low%20misclassification%20rate.%0AThe%20goal%20of%20this%20study%20is%20to%20assist%20security%20analysts%20in%20detecting%20malicious%0Apackages%20through%20the%20empirical%20study%20of%20using%20Large%20Language%20Models%20%28LLMs%29%20to%0Adetect%20malicious%20code%20in%20the%20npm%20ecosystem.%20We%20present%20SecurityAI%2C%20a%20malicious%0Acode%20review%20workflow%20to%20detect%20malicious%20code%20using%20ChatGPT.%20We%20leverage%20a%0Abenchmark%20dataset%20of%205%2C115%20npm%20packages%2C%20of%20which%202%2C180%20packages%20have%20malicious%0Acode.%20We%20conducted%20a%20baseline%20comparison%20of%20GPT-3%20and%20GPT-%204%20models%20with%20the%0Astate-of-the-art%20CodeQL%20static%20analysis%20tool%2C%20using%2039%20custom%20CodeQL%20rules%0Adeveloped%20in%20prior%20research%20to%20detect%20malicious%20Javascript%20code.%20We%20compare%20the%0Aeffectiveness%20of%20static%20analysis%20as%20a%20pre-screener%20with%20SecurityAI%20workflow%2C%0Ameasuring%20the%20number%20of%20files%20that%20need%20to%20be%20analyzed%20and%20the%20associated%0Acosts.%20Additionally%2C%20we%20performed%20a%20qualitative%20study%20to%20understand%20the%20types%0Aof%20malicious%20packages%20detected%20or%20missed%20by%20our%20workflow.%20Our%20baseline%0Acomparison%20demonstrates%20a%2016%25%20and%209%25%20improvement%20over%20static%20analysis%20in%0Aprecision%20and%20F1%20scores%2C%20respectively.%20We%20attained%20precision%20and%20F1%20scores%20of%0A91%25%20and%2094%25%20for%20GPT-3%2C%20and%2099%25%20%26%2097%25%20for%20GPT-4%2C%20respectively%2C%20with%20GPT-3%0Aoffering%20a%20cost-effective%20balance.%20Pre-screening%20files%20with%20a%20static%20analyzer%0Areduces%20the%20number%20of%20files%20requiring%20LLM%20analysis%20by%2077.9%25%20and%20decreases%20costs%0Aby%2060.9%25%20for%20GPT-3%20and%2076.1%25%20for%20GPT-4.%20Our%20qualitative%20analysis%20identified%0Adata%20theft%2C%20hidden%20backdoors%2C%20and%20suspicious%20domain%20connection%20categories%20as%0Athe%20top%20detected%20malicious%20packages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12196v2&entry.124074799=Read"},
{"title": "AttackER: Towards Enhancing Cyber-Attack Attribution with a Named Entity\n  Recognition Dataset", "author": "Pritam Deka and Sampath Rajapaksha and Ruby Rani and Amirah Almutairi and Erisa Karafili", "abstract": "  Cyber-attack attribution is an important process that allows experts to put\nin place attacker-oriented countermeasures and legal actions. The analysts\nmainly perform attribution manually, given the complex nature of this task. AI\nand, more specifically, Natural Language Processing (NLP) techniques can be\nleveraged to support cybersecurity analysts during the attribution process.\nHowever powerful these techniques are, they need to deal with the lack of\ndatasets in the attack attribution domain. In this work, we will fill this gap\nand will provide, to the best of our knowledge, the first dataset on\ncyber-attack attribution. We designed our dataset with the primary goal of\nextracting attack attribution information from cybersecurity texts, utilizing\nnamed entity recognition (NER) methodologies from the field of NLP. Unlike\nother cybersecurity NER datasets, ours offers a rich set of annotations with\ncontextual details, including some that span phrases and sentences. We\nconducted extensive experiments and applied NLP techniques to demonstrate the\ndataset's effectiveness for attack attribution. These experiments highlight the\npotential of Large Language Models (LLMs) capabilities to improve the NER tasks\nin cybersecurity datasets for cyber-attack attribution.\n", "link": "http://arxiv.org/abs/2408.05149v1", "date": "2024-08-09", "relevancy": 1.6954, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4435}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4368}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttackER%3A%20Towards%20Enhancing%20Cyber-Attack%20Attribution%20with%20a%20Named%20Entity%0A%20%20Recognition%20Dataset&body=Title%3A%20AttackER%3A%20Towards%20Enhancing%20Cyber-Attack%20Attribution%20with%20a%20Named%20Entity%0A%20%20Recognition%20Dataset%0AAuthor%3A%20Pritam%20Deka%20and%20Sampath%20Rajapaksha%20and%20Ruby%20Rani%20and%20Amirah%20Almutairi%20and%20Erisa%20Karafili%0AAbstract%3A%20%20%20Cyber-attack%20attribution%20is%20an%20important%20process%20that%20allows%20experts%20to%20put%0Ain%20place%20attacker-oriented%20countermeasures%20and%20legal%20actions.%20The%20analysts%0Amainly%20perform%20attribution%20manually%2C%20given%20the%20complex%20nature%20of%20this%20task.%20AI%0Aand%2C%20more%20specifically%2C%20Natural%20Language%20Processing%20%28NLP%29%20techniques%20can%20be%0Aleveraged%20to%20support%20cybersecurity%20analysts%20during%20the%20attribution%20process.%0AHowever%20powerful%20these%20techniques%20are%2C%20they%20need%20to%20deal%20with%20the%20lack%20of%0Adatasets%20in%20the%20attack%20attribution%20domain.%20In%20this%20work%2C%20we%20will%20fill%20this%20gap%0Aand%20will%20provide%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20dataset%20on%0Acyber-attack%20attribution.%20We%20designed%20our%20dataset%20with%20the%20primary%20goal%20of%0Aextracting%20attack%20attribution%20information%20from%20cybersecurity%20texts%2C%20utilizing%0Anamed%20entity%20recognition%20%28NER%29%20methodologies%20from%20the%20field%20of%20NLP.%20Unlike%0Aother%20cybersecurity%20NER%20datasets%2C%20ours%20offers%20a%20rich%20set%20of%20annotations%20with%0Acontextual%20details%2C%20including%20some%20that%20span%20phrases%20and%20sentences.%20We%0Aconducted%20extensive%20experiments%20and%20applied%20NLP%20techniques%20to%20demonstrate%20the%0Adataset%27s%20effectiveness%20for%20attack%20attribution.%20These%20experiments%20highlight%20the%0Apotential%20of%20Large%20Language%20Models%20%28LLMs%29%20capabilities%20to%20improve%20the%20NER%20tasks%0Ain%20cybersecurity%20datasets%20for%20cyber-attack%20attribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttackER%253A%2520Towards%2520Enhancing%2520Cyber-Attack%2520Attribution%2520with%2520a%2520Named%2520Entity%250A%2520%2520Recognition%2520Dataset%26entry.906535625%3DPritam%2520Deka%2520and%2520Sampath%2520Rajapaksha%2520and%2520Ruby%2520Rani%2520and%2520Amirah%2520Almutairi%2520and%2520Erisa%2520Karafili%26entry.1292438233%3D%2520%2520Cyber-attack%2520attribution%2520is%2520an%2520important%2520process%2520that%2520allows%2520experts%2520to%2520put%250Ain%2520place%2520attacker-oriented%2520countermeasures%2520and%2520legal%2520actions.%2520The%2520analysts%250Amainly%2520perform%2520attribution%2520manually%252C%2520given%2520the%2520complex%2520nature%2520of%2520this%2520task.%2520AI%250Aand%252C%2520more%2520specifically%252C%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520techniques%2520can%2520be%250Aleveraged%2520to%2520support%2520cybersecurity%2520analysts%2520during%2520the%2520attribution%2520process.%250AHowever%2520powerful%2520these%2520techniques%2520are%252C%2520they%2520need%2520to%2520deal%2520with%2520the%2520lack%2520of%250Adatasets%2520in%2520the%2520attack%2520attribution%2520domain.%2520In%2520this%2520work%252C%2520we%2520will%2520fill%2520this%2520gap%250Aand%2520will%2520provide%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520dataset%2520on%250Acyber-attack%2520attribution.%2520We%2520designed%2520our%2520dataset%2520with%2520the%2520primary%2520goal%2520of%250Aextracting%2520attack%2520attribution%2520information%2520from%2520cybersecurity%2520texts%252C%2520utilizing%250Anamed%2520entity%2520recognition%2520%2528NER%2529%2520methodologies%2520from%2520the%2520field%2520of%2520NLP.%2520Unlike%250Aother%2520cybersecurity%2520NER%2520datasets%252C%2520ours%2520offers%2520a%2520rich%2520set%2520of%2520annotations%2520with%250Acontextual%2520details%252C%2520including%2520some%2520that%2520span%2520phrases%2520and%2520sentences.%2520We%250Aconducted%2520extensive%2520experiments%2520and%2520applied%2520NLP%2520techniques%2520to%2520demonstrate%2520the%250Adataset%2527s%2520effectiveness%2520for%2520attack%2520attribution.%2520These%2520experiments%2520highlight%2520the%250Apotential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520capabilities%2520to%2520improve%2520the%2520NER%2520tasks%250Ain%2520cybersecurity%2520datasets%2520for%2520cyber-attack%2520attribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttackER%3A%20Towards%20Enhancing%20Cyber-Attack%20Attribution%20with%20a%20Named%20Entity%0A%20%20Recognition%20Dataset&entry.906535625=Pritam%20Deka%20and%20Sampath%20Rajapaksha%20and%20Ruby%20Rani%20and%20Amirah%20Almutairi%20and%20Erisa%20Karafili&entry.1292438233=%20%20Cyber-attack%20attribution%20is%20an%20important%20process%20that%20allows%20experts%20to%20put%0Ain%20place%20attacker-oriented%20countermeasures%20and%20legal%20actions.%20The%20analysts%0Amainly%20perform%20attribution%20manually%2C%20given%20the%20complex%20nature%20of%20this%20task.%20AI%0Aand%2C%20more%20specifically%2C%20Natural%20Language%20Processing%20%28NLP%29%20techniques%20can%20be%0Aleveraged%20to%20support%20cybersecurity%20analysts%20during%20the%20attribution%20process.%0AHowever%20powerful%20these%20techniques%20are%2C%20they%20need%20to%20deal%20with%20the%20lack%20of%0Adatasets%20in%20the%20attack%20attribution%20domain.%20In%20this%20work%2C%20we%20will%20fill%20this%20gap%0Aand%20will%20provide%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20dataset%20on%0Acyber-attack%20attribution.%20We%20designed%20our%20dataset%20with%20the%20primary%20goal%20of%0Aextracting%20attack%20attribution%20information%20from%20cybersecurity%20texts%2C%20utilizing%0Anamed%20entity%20recognition%20%28NER%29%20methodologies%20from%20the%20field%20of%20NLP.%20Unlike%0Aother%20cybersecurity%20NER%20datasets%2C%20ours%20offers%20a%20rich%20set%20of%20annotations%20with%0Acontextual%20details%2C%20including%20some%20that%20span%20phrases%20and%20sentences.%20We%0Aconducted%20extensive%20experiments%20and%20applied%20NLP%20techniques%20to%20demonstrate%20the%0Adataset%27s%20effectiveness%20for%20attack%20attribution.%20These%20experiments%20highlight%20the%0Apotential%20of%20Large%20Language%20Models%20%28LLMs%29%20capabilities%20to%20improve%20the%20NER%20tasks%0Ain%20cybersecurity%20datasets%20for%20cyber-attack%20attribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05149v1&entry.124074799=Read"},
{"title": "Concept learning of parameterized quantum models from limited\n  measurements", "author": "Beng Yee Gan and Po-Wei Huang and Elies Gil-Fuster and Patrick Rebentrost", "abstract": "  Classical learning of the expectation values of observables for quantum\nstates is a natural variant of learning quantum states or channels. While\nlearning-theoretic frameworks establish the sample complexity and the number of\nmeasurement shots per sample required for learning such statistical quantities,\nthe interplay between these two variables has not been adequately quantified\nbefore. In this work, we take the probabilistic nature of quantum measurements\ninto account in classical modelling and discuss these quantities under a single\nunified learning framework. We provide provable guarantees for learning\nparameterized quantum models that also quantify the asymmetrical effects and\ninterplay of the two variables on the performance of learning algorithms. These\nresults show that while increasing the sample size enhances the learning\nperformance of classical machines, even with single-shot estimates, the\nimprovements from increasing measurements become asymptotically trivial beyond\na constant factor. We further apply our framework and theoretical guarantees to\nstudy the impact of measurement noise on the classical surrogation of\nparameterized quantum circuit models. Our work provides new tools to analyse\nthe operational influence of finite measurement noise in the classical learning\nof quantum systems.\n", "link": "http://arxiv.org/abs/2408.05116v1", "date": "2024-08-09", "relevancy": 1.6593, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4802}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4053}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20learning%20of%20parameterized%20quantum%20models%20from%20limited%0A%20%20measurements&body=Title%3A%20Concept%20learning%20of%20parameterized%20quantum%20models%20from%20limited%0A%20%20measurements%0AAuthor%3A%20Beng%20Yee%20Gan%20and%20Po-Wei%20Huang%20and%20Elies%20Gil-Fuster%20and%20Patrick%20Rebentrost%0AAbstract%3A%20%20%20Classical%20learning%20of%20the%20expectation%20values%20of%20observables%20for%20quantum%0Astates%20is%20a%20natural%20variant%20of%20learning%20quantum%20states%20or%20channels.%20While%0Alearning-theoretic%20frameworks%20establish%20the%20sample%20complexity%20and%20the%20number%20of%0Ameasurement%20shots%20per%20sample%20required%20for%20learning%20such%20statistical%20quantities%2C%0Athe%20interplay%20between%20these%20two%20variables%20has%20not%20been%20adequately%20quantified%0Abefore.%20In%20this%20work%2C%20we%20take%20the%20probabilistic%20nature%20of%20quantum%20measurements%0Ainto%20account%20in%20classical%20modelling%20and%20discuss%20these%20quantities%20under%20a%20single%0Aunified%20learning%20framework.%20We%20provide%20provable%20guarantees%20for%20learning%0Aparameterized%20quantum%20models%20that%20also%20quantify%20the%20asymmetrical%20effects%20and%0Ainterplay%20of%20the%20two%20variables%20on%20the%20performance%20of%20learning%20algorithms.%20These%0Aresults%20show%20that%20while%20increasing%20the%20sample%20size%20enhances%20the%20learning%0Aperformance%20of%20classical%20machines%2C%20even%20with%20single-shot%20estimates%2C%20the%0Aimprovements%20from%20increasing%20measurements%20become%20asymptotically%20trivial%20beyond%0Aa%20constant%20factor.%20We%20further%20apply%20our%20framework%20and%20theoretical%20guarantees%20to%0Astudy%20the%20impact%20of%20measurement%20noise%20on%20the%20classical%20surrogation%20of%0Aparameterized%20quantum%20circuit%20models.%20Our%20work%20provides%20new%20tools%20to%20analyse%0Athe%20operational%20influence%20of%20finite%20measurement%20noise%20in%20the%20classical%20learning%0Aof%20quantum%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520learning%2520of%2520parameterized%2520quantum%2520models%2520from%2520limited%250A%2520%2520measurements%26entry.906535625%3DBeng%2520Yee%2520Gan%2520and%2520Po-Wei%2520Huang%2520and%2520Elies%2520Gil-Fuster%2520and%2520Patrick%2520Rebentrost%26entry.1292438233%3D%2520%2520Classical%2520learning%2520of%2520the%2520expectation%2520values%2520of%2520observables%2520for%2520quantum%250Astates%2520is%2520a%2520natural%2520variant%2520of%2520learning%2520quantum%2520states%2520or%2520channels.%2520While%250Alearning-theoretic%2520frameworks%2520establish%2520the%2520sample%2520complexity%2520and%2520the%2520number%2520of%250Ameasurement%2520shots%2520per%2520sample%2520required%2520for%2520learning%2520such%2520statistical%2520quantities%252C%250Athe%2520interplay%2520between%2520these%2520two%2520variables%2520has%2520not%2520been%2520adequately%2520quantified%250Abefore.%2520In%2520this%2520work%252C%2520we%2520take%2520the%2520probabilistic%2520nature%2520of%2520quantum%2520measurements%250Ainto%2520account%2520in%2520classical%2520modelling%2520and%2520discuss%2520these%2520quantities%2520under%2520a%2520single%250Aunified%2520learning%2520framework.%2520We%2520provide%2520provable%2520guarantees%2520for%2520learning%250Aparameterized%2520quantum%2520models%2520that%2520also%2520quantify%2520the%2520asymmetrical%2520effects%2520and%250Ainterplay%2520of%2520the%2520two%2520variables%2520on%2520the%2520performance%2520of%2520learning%2520algorithms.%2520These%250Aresults%2520show%2520that%2520while%2520increasing%2520the%2520sample%2520size%2520enhances%2520the%2520learning%250Aperformance%2520of%2520classical%2520machines%252C%2520even%2520with%2520single-shot%2520estimates%252C%2520the%250Aimprovements%2520from%2520increasing%2520measurements%2520become%2520asymptotically%2520trivial%2520beyond%250Aa%2520constant%2520factor.%2520We%2520further%2520apply%2520our%2520framework%2520and%2520theoretical%2520guarantees%2520to%250Astudy%2520the%2520impact%2520of%2520measurement%2520noise%2520on%2520the%2520classical%2520surrogation%2520of%250Aparameterized%2520quantum%2520circuit%2520models.%2520Our%2520work%2520provides%2520new%2520tools%2520to%2520analyse%250Athe%2520operational%2520influence%2520of%2520finite%2520measurement%2520noise%2520in%2520the%2520classical%2520learning%250Aof%2520quantum%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20learning%20of%20parameterized%20quantum%20models%20from%20limited%0A%20%20measurements&entry.906535625=Beng%20Yee%20Gan%20and%20Po-Wei%20Huang%20and%20Elies%20Gil-Fuster%20and%20Patrick%20Rebentrost&entry.1292438233=%20%20Classical%20learning%20of%20the%20expectation%20values%20of%20observables%20for%20quantum%0Astates%20is%20a%20natural%20variant%20of%20learning%20quantum%20states%20or%20channels.%20While%0Alearning-theoretic%20frameworks%20establish%20the%20sample%20complexity%20and%20the%20number%20of%0Ameasurement%20shots%20per%20sample%20required%20for%20learning%20such%20statistical%20quantities%2C%0Athe%20interplay%20between%20these%20two%20variables%20has%20not%20been%20adequately%20quantified%0Abefore.%20In%20this%20work%2C%20we%20take%20the%20probabilistic%20nature%20of%20quantum%20measurements%0Ainto%20account%20in%20classical%20modelling%20and%20discuss%20these%20quantities%20under%20a%20single%0Aunified%20learning%20framework.%20We%20provide%20provable%20guarantees%20for%20learning%0Aparameterized%20quantum%20models%20that%20also%20quantify%20the%20asymmetrical%20effects%20and%0Ainterplay%20of%20the%20two%20variables%20on%20the%20performance%20of%20learning%20algorithms.%20These%0Aresults%20show%20that%20while%20increasing%20the%20sample%20size%20enhances%20the%20learning%0Aperformance%20of%20classical%20machines%2C%20even%20with%20single-shot%20estimates%2C%20the%0Aimprovements%20from%20increasing%20measurements%20become%20asymptotically%20trivial%20beyond%0Aa%20constant%20factor.%20We%20further%20apply%20our%20framework%20and%20theoretical%20guarantees%20to%0Astudy%20the%20impact%20of%20measurement%20noise%20on%20the%20classical%20surrogation%20of%0Aparameterized%20quantum%20circuit%20models.%20Our%20work%20provides%20new%20tools%20to%20analyse%0Athe%20operational%20influence%20of%20finite%20measurement%20noise%20in%20the%20classical%20learning%0Aof%20quantum%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05116v1&entry.124074799=Read"},
{"title": "Weak-Annotation of HAR Datasets using Vision Foundation Models", "author": "Marius Bock and Kristof Van Laerhoven and Michael Moeller", "abstract": "  As wearable-based data annotation remains, to date, a tedious, time-consuming\ntask requiring researchers to dedicate substantial time, benchmark datasets\nwithin the field of Human Activity Recognition in lack richness and size\ncompared to datasets available within related fields. Recently, vision\nfoundation models such as CLIP have gained significant attention, helping the\nvision community advance in finding robust, generalizable feature\nrepresentations. With the majority of researchers within the wearable community\nrelying on vision modalities to overcome the limited expressiveness of wearable\ndata and accurately label their to-be-released benchmark datasets offline, we\npropose a novel, clustering-based annotation pipeline to significantly reduce\nthe amount of data that needs to be annotated by a human annotator. We show\nthat using our approach, the annotation of centroid clips suffices to achieve\naverage labelling accuracies close to 90% across three publicly available HAR\nbenchmark datasets. Using the weakly annotated datasets, we further demonstrate\nthat we can match the accuracy scores of fully-supervised deep learning\nclassifiers across all three benchmark datasets. Code as well as supplementary\nfigures and results are publicly downloadable via\ngithub.com/mariusbock/weak_har.\n", "link": "http://arxiv.org/abs/2408.05169v1", "date": "2024-08-09", "relevancy": 1.6575, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5819}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5176}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weak-Annotation%20of%20HAR%20Datasets%20using%20Vision%20Foundation%20Models&body=Title%3A%20Weak-Annotation%20of%20HAR%20Datasets%20using%20Vision%20Foundation%20Models%0AAuthor%3A%20Marius%20Bock%20and%20Kristof%20Van%20Laerhoven%20and%20Michael%20Moeller%0AAbstract%3A%20%20%20As%20wearable-based%20data%20annotation%20remains%2C%20to%20date%2C%20a%20tedious%2C%20time-consuming%0Atask%20requiring%20researchers%20to%20dedicate%20substantial%20time%2C%20benchmark%20datasets%0Awithin%20the%20field%20of%20Human%20Activity%20Recognition%20in%20lack%20richness%20and%20size%0Acompared%20to%20datasets%20available%20within%20related%20fields.%20Recently%2C%20vision%0Afoundation%20models%20such%20as%20CLIP%20have%20gained%20significant%20attention%2C%20helping%20the%0Avision%20community%20advance%20in%20finding%20robust%2C%20generalizable%20feature%0Arepresentations.%20With%20the%20majority%20of%20researchers%20within%20the%20wearable%20community%0Arelying%20on%20vision%20modalities%20to%20overcome%20the%20limited%20expressiveness%20of%20wearable%0Adata%20and%20accurately%20label%20their%20to-be-released%20benchmark%20datasets%20offline%2C%20we%0Apropose%20a%20novel%2C%20clustering-based%20annotation%20pipeline%20to%20significantly%20reduce%0Athe%20amount%20of%20data%20that%20needs%20to%20be%20annotated%20by%20a%20human%20annotator.%20We%20show%0Athat%20using%20our%20approach%2C%20the%20annotation%20of%20centroid%20clips%20suffices%20to%20achieve%0Aaverage%20labelling%20accuracies%20close%20to%2090%25%20across%20three%20publicly%20available%20HAR%0Abenchmark%20datasets.%20Using%20the%20weakly%20annotated%20datasets%2C%20we%20further%20demonstrate%0Athat%20we%20can%20match%20the%20accuracy%20scores%20of%20fully-supervised%20deep%20learning%0Aclassifiers%20across%20all%20three%20benchmark%20datasets.%20Code%20as%20well%20as%20supplementary%0Afigures%20and%20results%20are%20publicly%20downloadable%20via%0Agithub.com/mariusbock/weak_har.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeak-Annotation%2520of%2520HAR%2520Datasets%2520using%2520Vision%2520Foundation%2520Models%26entry.906535625%3DMarius%2520Bock%2520and%2520Kristof%2520Van%2520Laerhoven%2520and%2520Michael%2520Moeller%26entry.1292438233%3D%2520%2520As%2520wearable-based%2520data%2520annotation%2520remains%252C%2520to%2520date%252C%2520a%2520tedious%252C%2520time-consuming%250Atask%2520requiring%2520researchers%2520to%2520dedicate%2520substantial%2520time%252C%2520benchmark%2520datasets%250Awithin%2520the%2520field%2520of%2520Human%2520Activity%2520Recognition%2520in%2520lack%2520richness%2520and%2520size%250Acompared%2520to%2520datasets%2520available%2520within%2520related%2520fields.%2520Recently%252C%2520vision%250Afoundation%2520models%2520such%2520as%2520CLIP%2520have%2520gained%2520significant%2520attention%252C%2520helping%2520the%250Avision%2520community%2520advance%2520in%2520finding%2520robust%252C%2520generalizable%2520feature%250Arepresentations.%2520With%2520the%2520majority%2520of%2520researchers%2520within%2520the%2520wearable%2520community%250Arelying%2520on%2520vision%2520modalities%2520to%2520overcome%2520the%2520limited%2520expressiveness%2520of%2520wearable%250Adata%2520and%2520accurately%2520label%2520their%2520to-be-released%2520benchmark%2520datasets%2520offline%252C%2520we%250Apropose%2520a%2520novel%252C%2520clustering-based%2520annotation%2520pipeline%2520to%2520significantly%2520reduce%250Athe%2520amount%2520of%2520data%2520that%2520needs%2520to%2520be%2520annotated%2520by%2520a%2520human%2520annotator.%2520We%2520show%250Athat%2520using%2520our%2520approach%252C%2520the%2520annotation%2520of%2520centroid%2520clips%2520suffices%2520to%2520achieve%250Aaverage%2520labelling%2520accuracies%2520close%2520to%252090%2525%2520across%2520three%2520publicly%2520available%2520HAR%250Abenchmark%2520datasets.%2520Using%2520the%2520weakly%2520annotated%2520datasets%252C%2520we%2520further%2520demonstrate%250Athat%2520we%2520can%2520match%2520the%2520accuracy%2520scores%2520of%2520fully-supervised%2520deep%2520learning%250Aclassifiers%2520across%2520all%2520three%2520benchmark%2520datasets.%2520Code%2520as%2520well%2520as%2520supplementary%250Afigures%2520and%2520results%2520are%2520publicly%2520downloadable%2520via%250Agithub.com/mariusbock/weak_har.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak-Annotation%20of%20HAR%20Datasets%20using%20Vision%20Foundation%20Models&entry.906535625=Marius%20Bock%20and%20Kristof%20Van%20Laerhoven%20and%20Michael%20Moeller&entry.1292438233=%20%20As%20wearable-based%20data%20annotation%20remains%2C%20to%20date%2C%20a%20tedious%2C%20time-consuming%0Atask%20requiring%20researchers%20to%20dedicate%20substantial%20time%2C%20benchmark%20datasets%0Awithin%20the%20field%20of%20Human%20Activity%20Recognition%20in%20lack%20richness%20and%20size%0Acompared%20to%20datasets%20available%20within%20related%20fields.%20Recently%2C%20vision%0Afoundation%20models%20such%20as%20CLIP%20have%20gained%20significant%20attention%2C%20helping%20the%0Avision%20community%20advance%20in%20finding%20robust%2C%20generalizable%20feature%0Arepresentations.%20With%20the%20majority%20of%20researchers%20within%20the%20wearable%20community%0Arelying%20on%20vision%20modalities%20to%20overcome%20the%20limited%20expressiveness%20of%20wearable%0Adata%20and%20accurately%20label%20their%20to-be-released%20benchmark%20datasets%20offline%2C%20we%0Apropose%20a%20novel%2C%20clustering-based%20annotation%20pipeline%20to%20significantly%20reduce%0Athe%20amount%20of%20data%20that%20needs%20to%20be%20annotated%20by%20a%20human%20annotator.%20We%20show%0Athat%20using%20our%20approach%2C%20the%20annotation%20of%20centroid%20clips%20suffices%20to%20achieve%0Aaverage%20labelling%20accuracies%20close%20to%2090%25%20across%20three%20publicly%20available%20HAR%0Abenchmark%20datasets.%20Using%20the%20weakly%20annotated%20datasets%2C%20we%20further%20demonstrate%0Athat%20we%20can%20match%20the%20accuracy%20scores%20of%20fully-supervised%20deep%20learning%0Aclassifiers%20across%20all%20three%20benchmark%20datasets.%20Code%20as%20well%20as%20supplementary%0Afigures%20and%20results%20are%20publicly%20downloadable%20via%0Agithub.com/mariusbock/weak_har.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05169v1&entry.124074799=Read"},
{"title": "Back-Projection Diffusion: Solving the Wideband Inverse Scattering\n  Problem with Diffusion Models", "author": "Borong Zhang and Mart\u00edn Guerra and Qin Li and Leonardo Zepeda-N\u00fa\u00f1ez", "abstract": "  We present Wideband back-projection diffusion, an end-to-end probabilistic\nframework for approximating the posterior distribution induced by the inverse\nscattering map from wideband scattering data. This framework leverages\nconditional diffusion models coupled with the underlying physics of\nwave-propagation and symmetries in the problem, to produce highly accurate\nreconstructions. The framework introduces a factorization of the score function\ninto a physics-based latent representation inspired by the filtered\nback-propagation formula and a conditional score function conditioned on this\nlatent representation. These two steps are also constrained to obey symmetries\nin the formulation while being amenable to compression by imposing the rank\nstructure found in the filtered back-projection formula. As a result,\nempirically, our framework is able to provide sharp reconstructions\neffortlessly, even recovering sub-Nyquist features in the multiple-scattering\nregime. It has low-sample and computational complexity, its number of\nparameters scales sub-linearly with the target resolution, and it has stable\ntraining dynamics.\n", "link": "http://arxiv.org/abs/2408.02866v2", "date": "2024-08-09", "relevancy": 1.6314, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6066}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.54}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Back-Projection%20Diffusion%3A%20Solving%20the%20Wideband%20Inverse%20Scattering%0A%20%20Problem%20with%20Diffusion%20Models&body=Title%3A%20Back-Projection%20Diffusion%3A%20Solving%20the%20Wideband%20Inverse%20Scattering%0A%20%20Problem%20with%20Diffusion%20Models%0AAuthor%3A%20Borong%20Zhang%20and%20Mart%C3%ADn%20Guerra%20and%20Qin%20Li%20and%20Leonardo%20Zepeda-N%C3%BA%C3%B1ez%0AAbstract%3A%20%20%20We%20present%20Wideband%20back-projection%20diffusion%2C%20an%20end-to-end%20probabilistic%0Aframework%20for%20approximating%20the%20posterior%20distribution%20induced%20by%20the%20inverse%0Ascattering%20map%20from%20wideband%20scattering%20data.%20This%20framework%20leverages%0Aconditional%20diffusion%20models%20coupled%20with%20the%20underlying%20physics%20of%0Awave-propagation%20and%20symmetries%20in%20the%20problem%2C%20to%20produce%20highly%20accurate%0Areconstructions.%20The%20framework%20introduces%20a%20factorization%20of%20the%20score%20function%0Ainto%20a%20physics-based%20latent%20representation%20inspired%20by%20the%20filtered%0Aback-propagation%20formula%20and%20a%20conditional%20score%20function%20conditioned%20on%20this%0Alatent%20representation.%20These%20two%20steps%20are%20also%20constrained%20to%20obey%20symmetries%0Ain%20the%20formulation%20while%20being%20amenable%20to%20compression%20by%20imposing%20the%20rank%0Astructure%20found%20in%20the%20filtered%20back-projection%20formula.%20As%20a%20result%2C%0Aempirically%2C%20our%20framework%20is%20able%20to%20provide%20sharp%20reconstructions%0Aeffortlessly%2C%20even%20recovering%20sub-Nyquist%20features%20in%20the%20multiple-scattering%0Aregime.%20It%20has%20low-sample%20and%20computational%20complexity%2C%20its%20number%20of%0Aparameters%20scales%20sub-linearly%20with%20the%20target%20resolution%2C%20and%20it%20has%20stable%0Atraining%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02866v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBack-Projection%2520Diffusion%253A%2520Solving%2520the%2520Wideband%2520Inverse%2520Scattering%250A%2520%2520Problem%2520with%2520Diffusion%2520Models%26entry.906535625%3DBorong%2520Zhang%2520and%2520Mart%25C3%25ADn%2520Guerra%2520and%2520Qin%2520Li%2520and%2520Leonardo%2520Zepeda-N%25C3%25BA%25C3%25B1ez%26entry.1292438233%3D%2520%2520We%2520present%2520Wideband%2520back-projection%2520diffusion%252C%2520an%2520end-to-end%2520probabilistic%250Aframework%2520for%2520approximating%2520the%2520posterior%2520distribution%2520induced%2520by%2520the%2520inverse%250Ascattering%2520map%2520from%2520wideband%2520scattering%2520data.%2520This%2520framework%2520leverages%250Aconditional%2520diffusion%2520models%2520coupled%2520with%2520the%2520underlying%2520physics%2520of%250Awave-propagation%2520and%2520symmetries%2520in%2520the%2520problem%252C%2520to%2520produce%2520highly%2520accurate%250Areconstructions.%2520The%2520framework%2520introduces%2520a%2520factorization%2520of%2520the%2520score%2520function%250Ainto%2520a%2520physics-based%2520latent%2520representation%2520inspired%2520by%2520the%2520filtered%250Aback-propagation%2520formula%2520and%2520a%2520conditional%2520score%2520function%2520conditioned%2520on%2520this%250Alatent%2520representation.%2520These%2520two%2520steps%2520are%2520also%2520constrained%2520to%2520obey%2520symmetries%250Ain%2520the%2520formulation%2520while%2520being%2520amenable%2520to%2520compression%2520by%2520imposing%2520the%2520rank%250Astructure%2520found%2520in%2520the%2520filtered%2520back-projection%2520formula.%2520As%2520a%2520result%252C%250Aempirically%252C%2520our%2520framework%2520is%2520able%2520to%2520provide%2520sharp%2520reconstructions%250Aeffortlessly%252C%2520even%2520recovering%2520sub-Nyquist%2520features%2520in%2520the%2520multiple-scattering%250Aregime.%2520It%2520has%2520low-sample%2520and%2520computational%2520complexity%252C%2520its%2520number%2520of%250Aparameters%2520scales%2520sub-linearly%2520with%2520the%2520target%2520resolution%252C%2520and%2520it%2520has%2520stable%250Atraining%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02866v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Back-Projection%20Diffusion%3A%20Solving%20the%20Wideband%20Inverse%20Scattering%0A%20%20Problem%20with%20Diffusion%20Models&entry.906535625=Borong%20Zhang%20and%20Mart%C3%ADn%20Guerra%20and%20Qin%20Li%20and%20Leonardo%20Zepeda-N%C3%BA%C3%B1ez&entry.1292438233=%20%20We%20present%20Wideband%20back-projection%20diffusion%2C%20an%20end-to-end%20probabilistic%0Aframework%20for%20approximating%20the%20posterior%20distribution%20induced%20by%20the%20inverse%0Ascattering%20map%20from%20wideband%20scattering%20data.%20This%20framework%20leverages%0Aconditional%20diffusion%20models%20coupled%20with%20the%20underlying%20physics%20of%0Awave-propagation%20and%20symmetries%20in%20the%20problem%2C%20to%20produce%20highly%20accurate%0Areconstructions.%20The%20framework%20introduces%20a%20factorization%20of%20the%20score%20function%0Ainto%20a%20physics-based%20latent%20representation%20inspired%20by%20the%20filtered%0Aback-propagation%20formula%20and%20a%20conditional%20score%20function%20conditioned%20on%20this%0Alatent%20representation.%20These%20two%20steps%20are%20also%20constrained%20to%20obey%20symmetries%0Ain%20the%20formulation%20while%20being%20amenable%20to%20compression%20by%20imposing%20the%20rank%0Astructure%20found%20in%20the%20filtered%20back-projection%20formula.%20As%20a%20result%2C%0Aempirically%2C%20our%20framework%20is%20able%20to%20provide%20sharp%20reconstructions%0Aeffortlessly%2C%20even%20recovering%20sub-Nyquist%20features%20in%20the%20multiple-scattering%0Aregime.%20It%20has%20low-sample%20and%20computational%20complexity%2C%20its%20number%20of%0Aparameters%20scales%20sub-linearly%20with%20the%20target%20resolution%2C%20and%20it%20has%20stable%0Atraining%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02866v2&entry.124074799=Read"},
{"title": "What Foundation Models can Bring for Robot Learning in Manipulation : A\n  Survey", "author": "Dingzhe Li and Yixiang Jin and Yong A and Hongze Yu and Jun Shi and Xiaoshuai Hao and Peng Hao and Huaping Liu and Fuchun Sun and Jianwei Zhang and Bin Fang", "abstract": "  The realization of universal robots is an ultimate goal of researchers.\nHowever, a key hurdle in achieving this goal lies in the robots' ability to\nmanipulate objects in their unstructured surrounding environments according to\ndifferent tasks. The learning-based approach is considered an effective way to\naddress generalization. The impressive performance of foundation models in the\nfields of computer vision and natural language suggests the potential of\nembedding foundation models into manipulation tasks as a viable path toward\nachieving general manipulation capability. However, we believe achieving\ngeneral manipulation capability requires an overarching framework akin to auto\ndriving. This framework should encompass multiple functional modules, with\ndifferent foundation models assuming distinct roles in facilitating general\nmanipulation capability. This survey focuses on the contributions of foundation\nmodels to robot learning for manipulation. We propose a comprehensive framework\nand detail how foundation models can address challenges in each module of the\nframework. What's more, we examine current approaches, outline challenges,\nsuggest future research directions, and identify potential risks associated\nwith integrating foundation models into this domain.\n", "link": "http://arxiv.org/abs/2404.18201v2", "date": "2024-08-09", "relevancy": 1.6232, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Foundation%20Models%20can%20Bring%20for%20Robot%20Learning%20in%20Manipulation%20%3A%20A%0A%20%20Survey&body=Title%3A%20What%20Foundation%20Models%20can%20Bring%20for%20Robot%20Learning%20in%20Manipulation%20%3A%20A%0A%20%20Survey%0AAuthor%3A%20Dingzhe%20Li%20and%20Yixiang%20Jin%20and%20Yong%20A%20and%20Hongze%20Yu%20and%20Jun%20Shi%20and%20Xiaoshuai%20Hao%20and%20Peng%20Hao%20and%20Huaping%20Liu%20and%20Fuchun%20Sun%20and%20Jianwei%20Zhang%20and%20Bin%20Fang%0AAbstract%3A%20%20%20The%20realization%20of%20universal%20robots%20is%20an%20ultimate%20goal%20of%20researchers.%0AHowever%2C%20a%20key%20hurdle%20in%20achieving%20this%20goal%20lies%20in%20the%20robots%27%20ability%20to%0Amanipulate%20objects%20in%20their%20unstructured%20surrounding%20environments%20according%20to%0Adifferent%20tasks.%20The%20learning-based%20approach%20is%20considered%20an%20effective%20way%20to%0Aaddress%20generalization.%20The%20impressive%20performance%20of%20foundation%20models%20in%20the%0Afields%20of%20computer%20vision%20and%20natural%20language%20suggests%20the%20potential%20of%0Aembedding%20foundation%20models%20into%20manipulation%20tasks%20as%20a%20viable%20path%20toward%0Aachieving%20general%20manipulation%20capability.%20However%2C%20we%20believe%20achieving%0Ageneral%20manipulation%20capability%20requires%20an%20overarching%20framework%20akin%20to%20auto%0Adriving.%20This%20framework%20should%20encompass%20multiple%20functional%20modules%2C%20with%0Adifferent%20foundation%20models%20assuming%20distinct%20roles%20in%20facilitating%20general%0Amanipulation%20capability.%20This%20survey%20focuses%20on%20the%20contributions%20of%20foundation%0Amodels%20to%20robot%20learning%20for%20manipulation.%20We%20propose%20a%20comprehensive%20framework%0Aand%20detail%20how%20foundation%20models%20can%20address%20challenges%20in%20each%20module%20of%20the%0Aframework.%20What%27s%20more%2C%20we%20examine%20current%20approaches%2C%20outline%20challenges%2C%0Asuggest%20future%20research%20directions%2C%20and%20identify%20potential%20risks%20associated%0Awith%20integrating%20foundation%20models%20into%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Foundation%2520Models%2520can%2520Bring%2520for%2520Robot%2520Learning%2520in%2520Manipulation%2520%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DDingzhe%2520Li%2520and%2520Yixiang%2520Jin%2520and%2520Yong%2520A%2520and%2520Hongze%2520Yu%2520and%2520Jun%2520Shi%2520and%2520Xiaoshuai%2520Hao%2520and%2520Peng%2520Hao%2520and%2520Huaping%2520Liu%2520and%2520Fuchun%2520Sun%2520and%2520Jianwei%2520Zhang%2520and%2520Bin%2520Fang%26entry.1292438233%3D%2520%2520The%2520realization%2520of%2520universal%2520robots%2520is%2520an%2520ultimate%2520goal%2520of%2520researchers.%250AHowever%252C%2520a%2520key%2520hurdle%2520in%2520achieving%2520this%2520goal%2520lies%2520in%2520the%2520robots%2527%2520ability%2520to%250Amanipulate%2520objects%2520in%2520their%2520unstructured%2520surrounding%2520environments%2520according%2520to%250Adifferent%2520tasks.%2520The%2520learning-based%2520approach%2520is%2520considered%2520an%2520effective%2520way%2520to%250Aaddress%2520generalization.%2520The%2520impressive%2520performance%2520of%2520foundation%2520models%2520in%2520the%250Afields%2520of%2520computer%2520vision%2520and%2520natural%2520language%2520suggests%2520the%2520potential%2520of%250Aembedding%2520foundation%2520models%2520into%2520manipulation%2520tasks%2520as%2520a%2520viable%2520path%2520toward%250Aachieving%2520general%2520manipulation%2520capability.%2520However%252C%2520we%2520believe%2520achieving%250Ageneral%2520manipulation%2520capability%2520requires%2520an%2520overarching%2520framework%2520akin%2520to%2520auto%250Adriving.%2520This%2520framework%2520should%2520encompass%2520multiple%2520functional%2520modules%252C%2520with%250Adifferent%2520foundation%2520models%2520assuming%2520distinct%2520roles%2520in%2520facilitating%2520general%250Amanipulation%2520capability.%2520This%2520survey%2520focuses%2520on%2520the%2520contributions%2520of%2520foundation%250Amodels%2520to%2520robot%2520learning%2520for%2520manipulation.%2520We%2520propose%2520a%2520comprehensive%2520framework%250Aand%2520detail%2520how%2520foundation%2520models%2520can%2520address%2520challenges%2520in%2520each%2520module%2520of%2520the%250Aframework.%2520What%2527s%2520more%252C%2520we%2520examine%2520current%2520approaches%252C%2520outline%2520challenges%252C%250Asuggest%2520future%2520research%2520directions%252C%2520and%2520identify%2520potential%2520risks%2520associated%250Awith%2520integrating%2520foundation%2520models%2520into%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Foundation%20Models%20can%20Bring%20for%20Robot%20Learning%20in%20Manipulation%20%3A%20A%0A%20%20Survey&entry.906535625=Dingzhe%20Li%20and%20Yixiang%20Jin%20and%20Yong%20A%20and%20Hongze%20Yu%20and%20Jun%20Shi%20and%20Xiaoshuai%20Hao%20and%20Peng%20Hao%20and%20Huaping%20Liu%20and%20Fuchun%20Sun%20and%20Jianwei%20Zhang%20and%20Bin%20Fang&entry.1292438233=%20%20The%20realization%20of%20universal%20robots%20is%20an%20ultimate%20goal%20of%20researchers.%0AHowever%2C%20a%20key%20hurdle%20in%20achieving%20this%20goal%20lies%20in%20the%20robots%27%20ability%20to%0Amanipulate%20objects%20in%20their%20unstructured%20surrounding%20environments%20according%20to%0Adifferent%20tasks.%20The%20learning-based%20approach%20is%20considered%20an%20effective%20way%20to%0Aaddress%20generalization.%20The%20impressive%20performance%20of%20foundation%20models%20in%20the%0Afields%20of%20computer%20vision%20and%20natural%20language%20suggests%20the%20potential%20of%0Aembedding%20foundation%20models%20into%20manipulation%20tasks%20as%20a%20viable%20path%20toward%0Aachieving%20general%20manipulation%20capability.%20However%2C%20we%20believe%20achieving%0Ageneral%20manipulation%20capability%20requires%20an%20overarching%20framework%20akin%20to%20auto%0Adriving.%20This%20framework%20should%20encompass%20multiple%20functional%20modules%2C%20with%0Adifferent%20foundation%20models%20assuming%20distinct%20roles%20in%20facilitating%20general%0Amanipulation%20capability.%20This%20survey%20focuses%20on%20the%20contributions%20of%20foundation%0Amodels%20to%20robot%20learning%20for%20manipulation.%20We%20propose%20a%20comprehensive%20framework%0Aand%20detail%20how%20foundation%20models%20can%20address%20challenges%20in%20each%20module%20of%20the%0Aframework.%20What%27s%20more%2C%20we%20examine%20current%20approaches%2C%20outline%20challenges%2C%0Asuggest%20future%20research%20directions%2C%20and%20identify%20potential%20risks%20associated%0Awith%20integrating%20foundation%20models%20into%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18201v2&entry.124074799=Read"},
{"title": "Tree Attention: Topology-aware Decoding for Long-Context Attention on\n  GPU clusters", "author": "Vasudev Shyam and Jonathan Pilault and Emily Shepperd and Quentin Anthony and Beren Millidge", "abstract": "  Self-attention is the core mathematical operation of modern transformer\narchitectures and is also a significant computational bottleneck due to its\nquadratic complexity in the sequence length. In this work, we derive the scalar\nenergy function whose gradient computes the self-attention block, thus\nelucidating the theoretical underpinnings of self-attention, providing a\nBayesian interpretation of the operation and linking it closely with\nenergy-based models such as Hopfield Networks. Our formulation reveals that the\nreduction across the sequence axis can be efficiently computed in parallel\nthrough a tree reduction. Our algorithm, for parallelizing attention\ncomputation across multiple GPUs enables cross-device decoding to be performed\nasymptotically faster (up to 8x faster in our experiments) than alternative\napproaches such as Ring Attention, while also requiring significantly less\ncommunication volume and incurring 2x less peak memory. Our code is publicly\navailable here: \\url{https://github.com/Zyphra/tree_attention}.\n", "link": "http://arxiv.org/abs/2408.04093v2", "date": "2024-08-09", "relevancy": 1.619, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5718}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5073}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree%20Attention%3A%20Topology-aware%20Decoding%20for%20Long-Context%20Attention%20on%0A%20%20GPU%20clusters&body=Title%3A%20Tree%20Attention%3A%20Topology-aware%20Decoding%20for%20Long-Context%20Attention%20on%0A%20%20GPU%20clusters%0AAuthor%3A%20Vasudev%20Shyam%20and%20Jonathan%20Pilault%20and%20Emily%20Shepperd%20and%20Quentin%20Anthony%20and%20Beren%20Millidge%0AAbstract%3A%20%20%20Self-attention%20is%20the%20core%20mathematical%20operation%20of%20modern%20transformer%0Aarchitectures%20and%20is%20also%20a%20significant%20computational%20bottleneck%20due%20to%20its%0Aquadratic%20complexity%20in%20the%20sequence%20length.%20In%20this%20work%2C%20we%20derive%20the%20scalar%0Aenergy%20function%20whose%20gradient%20computes%20the%20self-attention%20block%2C%20thus%0Aelucidating%20the%20theoretical%20underpinnings%20of%20self-attention%2C%20providing%20a%0ABayesian%20interpretation%20of%20the%20operation%20and%20linking%20it%20closely%20with%0Aenergy-based%20models%20such%20as%20Hopfield%20Networks.%20Our%20formulation%20reveals%20that%20the%0Areduction%20across%20the%20sequence%20axis%20can%20be%20efficiently%20computed%20in%20parallel%0Athrough%20a%20tree%20reduction.%20Our%20algorithm%2C%20for%20parallelizing%20attention%0Acomputation%20across%20multiple%20GPUs%20enables%20cross-device%20decoding%20to%20be%20performed%0Aasymptotically%20faster%20%28up%20to%208x%20faster%20in%20our%20experiments%29%20than%20alternative%0Aapproaches%20such%20as%20Ring%20Attention%2C%20while%20also%20requiring%20significantly%20less%0Acommunication%20volume%20and%20incurring%202x%20less%20peak%20memory.%20Our%20code%20is%20publicly%0Aavailable%20here%3A%20%5Curl%7Bhttps%3A//github.com/Zyphra/tree_attention%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04093v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree%2520Attention%253A%2520Topology-aware%2520Decoding%2520for%2520Long-Context%2520Attention%2520on%250A%2520%2520GPU%2520clusters%26entry.906535625%3DVasudev%2520Shyam%2520and%2520Jonathan%2520Pilault%2520and%2520Emily%2520Shepperd%2520and%2520Quentin%2520Anthony%2520and%2520Beren%2520Millidge%26entry.1292438233%3D%2520%2520Self-attention%2520is%2520the%2520core%2520mathematical%2520operation%2520of%2520modern%2520transformer%250Aarchitectures%2520and%2520is%2520also%2520a%2520significant%2520computational%2520bottleneck%2520due%2520to%2520its%250Aquadratic%2520complexity%2520in%2520the%2520sequence%2520length.%2520In%2520this%2520work%252C%2520we%2520derive%2520the%2520scalar%250Aenergy%2520function%2520whose%2520gradient%2520computes%2520the%2520self-attention%2520block%252C%2520thus%250Aelucidating%2520the%2520theoretical%2520underpinnings%2520of%2520self-attention%252C%2520providing%2520a%250ABayesian%2520interpretation%2520of%2520the%2520operation%2520and%2520linking%2520it%2520closely%2520with%250Aenergy-based%2520models%2520such%2520as%2520Hopfield%2520Networks.%2520Our%2520formulation%2520reveals%2520that%2520the%250Areduction%2520across%2520the%2520sequence%2520axis%2520can%2520be%2520efficiently%2520computed%2520in%2520parallel%250Athrough%2520a%2520tree%2520reduction.%2520Our%2520algorithm%252C%2520for%2520parallelizing%2520attention%250Acomputation%2520across%2520multiple%2520GPUs%2520enables%2520cross-device%2520decoding%2520to%2520be%2520performed%250Aasymptotically%2520faster%2520%2528up%2520to%25208x%2520faster%2520in%2520our%2520experiments%2529%2520than%2520alternative%250Aapproaches%2520such%2520as%2520Ring%2520Attention%252C%2520while%2520also%2520requiring%2520significantly%2520less%250Acommunication%2520volume%2520and%2520incurring%25202x%2520less%2520peak%2520memory.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520here%253A%2520%255Curl%257Bhttps%253A//github.com/Zyphra/tree_attention%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04093v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree%20Attention%3A%20Topology-aware%20Decoding%20for%20Long-Context%20Attention%20on%0A%20%20GPU%20clusters&entry.906535625=Vasudev%20Shyam%20and%20Jonathan%20Pilault%20and%20Emily%20Shepperd%20and%20Quentin%20Anthony%20and%20Beren%20Millidge&entry.1292438233=%20%20Self-attention%20is%20the%20core%20mathematical%20operation%20of%20modern%20transformer%0Aarchitectures%20and%20is%20also%20a%20significant%20computational%20bottleneck%20due%20to%20its%0Aquadratic%20complexity%20in%20the%20sequence%20length.%20In%20this%20work%2C%20we%20derive%20the%20scalar%0Aenergy%20function%20whose%20gradient%20computes%20the%20self-attention%20block%2C%20thus%0Aelucidating%20the%20theoretical%20underpinnings%20of%20self-attention%2C%20providing%20a%0ABayesian%20interpretation%20of%20the%20operation%20and%20linking%20it%20closely%20with%0Aenergy-based%20models%20such%20as%20Hopfield%20Networks.%20Our%20formulation%20reveals%20that%20the%0Areduction%20across%20the%20sequence%20axis%20can%20be%20efficiently%20computed%20in%20parallel%0Athrough%20a%20tree%20reduction.%20Our%20algorithm%2C%20for%20parallelizing%20attention%0Acomputation%20across%20multiple%20GPUs%20enables%20cross-device%20decoding%20to%20be%20performed%0Aasymptotically%20faster%20%28up%20to%208x%20faster%20in%20our%20experiments%29%20than%20alternative%0Aapproaches%20such%20as%20Ring%20Attention%2C%20while%20also%20requiring%20significantly%20less%0Acommunication%20volume%20and%20incurring%202x%20less%20peak%20memory.%20Our%20code%20is%20publicly%0Aavailable%20here%3A%20%5Curl%7Bhttps%3A//github.com/Zyphra/tree_attention%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04093v2&entry.124074799=Read"},
{"title": "XNN: Paradigm Shift in Mitigating Identity Leakage within Cloud-Enabled\n  Deep Learning", "author": "Kaixin Liu and Huixin Xiong and Bingyu Duan and Zexuan Cheng and Xinyu Zhou and Wanqian Zhang and Xiangyu Zhang", "abstract": "  In the domain of cloud-based deep learning, the imperative for external\ncomputational resources coexists with acute privacy concerns, particularly\nidentity leakage. To address this challenge, we introduce XNN and XNN-d,\npioneering methodologies that infuse neural network features with randomized\nperturbations, striking a harmonious balance between utility and privacy. XNN,\ndesigned for the training phase, ingeniously blends random permutation with\nmatrix multiplication techniques to obfuscate feature maps, effectively\nshielding private data from potential breaches without compromising training\nintegrity. Concurrently, XNN-d, devised for the inference phase, employs\nadversarial training to integrate generative adversarial noise. This technique\neffectively counters black-box access attacks aimed at identity extraction,\nwhile a distilled face recognition network adeptly processes the perturbed\nfeatures, ensuring accurate identification. Our evaluation demonstrates XNN's\neffectiveness, significantly outperforming existing methods in reducing\nidentity leakage while maintaining a high model accuracy.\n", "link": "http://arxiv.org/abs/2408.04974v1", "date": "2024-08-09", "relevancy": 1.5869, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5351}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5334}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XNN%3A%20Paradigm%20Shift%20in%20Mitigating%20Identity%20Leakage%20within%20Cloud-Enabled%0A%20%20Deep%20Learning&body=Title%3A%20XNN%3A%20Paradigm%20Shift%20in%20Mitigating%20Identity%20Leakage%20within%20Cloud-Enabled%0A%20%20Deep%20Learning%0AAuthor%3A%20Kaixin%20Liu%20and%20Huixin%20Xiong%20and%20Bingyu%20Duan%20and%20Zexuan%20Cheng%20and%20Xinyu%20Zhou%20and%20Wanqian%20Zhang%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20In%20the%20domain%20of%20cloud-based%20deep%20learning%2C%20the%20imperative%20for%20external%0Acomputational%20resources%20coexists%20with%20acute%20privacy%20concerns%2C%20particularly%0Aidentity%20leakage.%20To%20address%20this%20challenge%2C%20we%20introduce%20XNN%20and%20XNN-d%2C%0Apioneering%20methodologies%20that%20infuse%20neural%20network%20features%20with%20randomized%0Aperturbations%2C%20striking%20a%20harmonious%20balance%20between%20utility%20and%20privacy.%20XNN%2C%0Adesigned%20for%20the%20training%20phase%2C%20ingeniously%20blends%20random%20permutation%20with%0Amatrix%20multiplication%20techniques%20to%20obfuscate%20feature%20maps%2C%20effectively%0Ashielding%20private%20data%20from%20potential%20breaches%20without%20compromising%20training%0Aintegrity.%20Concurrently%2C%20XNN-d%2C%20devised%20for%20the%20inference%20phase%2C%20employs%0Aadversarial%20training%20to%20integrate%20generative%20adversarial%20noise.%20This%20technique%0Aeffectively%20counters%20black-box%20access%20attacks%20aimed%20at%20identity%20extraction%2C%0Awhile%20a%20distilled%20face%20recognition%20network%20adeptly%20processes%20the%20perturbed%0Afeatures%2C%20ensuring%20accurate%20identification.%20Our%20evaluation%20demonstrates%20XNN%27s%0Aeffectiveness%2C%20significantly%20outperforming%20existing%20methods%20in%20reducing%0Aidentity%20leakage%20while%20maintaining%20a%20high%20model%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXNN%253A%2520Paradigm%2520Shift%2520in%2520Mitigating%2520Identity%2520Leakage%2520within%2520Cloud-Enabled%250A%2520%2520Deep%2520Learning%26entry.906535625%3DKaixin%2520Liu%2520and%2520Huixin%2520Xiong%2520and%2520Bingyu%2520Duan%2520and%2520Zexuan%2520Cheng%2520and%2520Xinyu%2520Zhou%2520and%2520Wanqian%2520Zhang%2520and%2520Xiangyu%2520Zhang%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%2520cloud-based%2520deep%2520learning%252C%2520the%2520imperative%2520for%2520external%250Acomputational%2520resources%2520coexists%2520with%2520acute%2520privacy%2520concerns%252C%2520particularly%250Aidentity%2520leakage.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520XNN%2520and%2520XNN-d%252C%250Apioneering%2520methodologies%2520that%2520infuse%2520neural%2520network%2520features%2520with%2520randomized%250Aperturbations%252C%2520striking%2520a%2520harmonious%2520balance%2520between%2520utility%2520and%2520privacy.%2520XNN%252C%250Adesigned%2520for%2520the%2520training%2520phase%252C%2520ingeniously%2520blends%2520random%2520permutation%2520with%250Amatrix%2520multiplication%2520techniques%2520to%2520obfuscate%2520feature%2520maps%252C%2520effectively%250Ashielding%2520private%2520data%2520from%2520potential%2520breaches%2520without%2520compromising%2520training%250Aintegrity.%2520Concurrently%252C%2520XNN-d%252C%2520devised%2520for%2520the%2520inference%2520phase%252C%2520employs%250Aadversarial%2520training%2520to%2520integrate%2520generative%2520adversarial%2520noise.%2520This%2520technique%250Aeffectively%2520counters%2520black-box%2520access%2520attacks%2520aimed%2520at%2520identity%2520extraction%252C%250Awhile%2520a%2520distilled%2520face%2520recognition%2520network%2520adeptly%2520processes%2520the%2520perturbed%250Afeatures%252C%2520ensuring%2520accurate%2520identification.%2520Our%2520evaluation%2520demonstrates%2520XNN%2527s%250Aeffectiveness%252C%2520significantly%2520outperforming%2520existing%2520methods%2520in%2520reducing%250Aidentity%2520leakage%2520while%2520maintaining%2520a%2520high%2520model%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XNN%3A%20Paradigm%20Shift%20in%20Mitigating%20Identity%20Leakage%20within%20Cloud-Enabled%0A%20%20Deep%20Learning&entry.906535625=Kaixin%20Liu%20and%20Huixin%20Xiong%20and%20Bingyu%20Duan%20and%20Zexuan%20Cheng%20and%20Xinyu%20Zhou%20and%20Wanqian%20Zhang%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20In%20the%20domain%20of%20cloud-based%20deep%20learning%2C%20the%20imperative%20for%20external%0Acomputational%20resources%20coexists%20with%20acute%20privacy%20concerns%2C%20particularly%0Aidentity%20leakage.%20To%20address%20this%20challenge%2C%20we%20introduce%20XNN%20and%20XNN-d%2C%0Apioneering%20methodologies%20that%20infuse%20neural%20network%20features%20with%20randomized%0Aperturbations%2C%20striking%20a%20harmonious%20balance%20between%20utility%20and%20privacy.%20XNN%2C%0Adesigned%20for%20the%20training%20phase%2C%20ingeniously%20blends%20random%20permutation%20with%0Amatrix%20multiplication%20techniques%20to%20obfuscate%20feature%20maps%2C%20effectively%0Ashielding%20private%20data%20from%20potential%20breaches%20without%20compromising%20training%0Aintegrity.%20Concurrently%2C%20XNN-d%2C%20devised%20for%20the%20inference%20phase%2C%20employs%0Aadversarial%20training%20to%20integrate%20generative%20adversarial%20noise.%20This%20technique%0Aeffectively%20counters%20black-box%20access%20attacks%20aimed%20at%20identity%20extraction%2C%0Awhile%20a%20distilled%20face%20recognition%20network%20adeptly%20processes%20the%20perturbed%0Afeatures%2C%20ensuring%20accurate%20identification.%20Our%20evaluation%20demonstrates%20XNN%27s%0Aeffectiveness%2C%20significantly%20outperforming%20existing%20methods%20in%20reducing%0Aidentity%20leakage%20while%20maintaining%20a%20high%20model%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04974v1&entry.124074799=Read"},
{"title": "DeVAn: Dense Video Annotation for Video-Language Models", "author": "Tingkai Liu and Yunzhe Tao and Haogeng Liu and Qihang Fan and Ding Zhou and Huaibo Huang and Ran He and Hongxia Yang", "abstract": "  We present a novel human annotated dataset for evaluating the ability for\nvisual-language models to generate both short and long descriptions for\nreal-world video clips, termed DeVAn (Dense Video Annotation). The dataset\ncontains 8.5K YouTube video clips of 20-60 seconds in duration and covers a\nwide range of topics and interests. Each video clip is independently annotated\nby 5 human annotators, producing both captions (1 sentence) and summaries (3-10\nsentences). Given any video selected from the dataset and its corresponding ASR\ninformation, we evaluate visuallanguage models on either caption or summary\ngeneration that is grounded in both the visual and auditory content of the\nvideo. Additionally, models are also evaluated on caption- and summary-based\nretrieval tasks, where the summary-based retrieval task requires the\nidentification of a target video given excerpts of a given summary. Given the\nnovel nature of the paragraph-length video summarization task, we compared\ndifferent existing evaluation metrics and their alignment with human\npreferences and found that model-based evaluation metrics provide more\nsemantically-oriented and human-aligned evaluation. Finally, we benchmarked a\nwide range of current video-language models on DeVAn, and we aim for DeVAn to\nserve as a useful evaluation set in the age of large language models and\ncomplex multi-modal tasks. Code is available at https:\n//github.com/TK-21st/DeVAn.\n", "link": "http://arxiv.org/abs/2310.05060v2", "date": "2024-08-09", "relevancy": 1.558, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5284}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5183}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeVAn%3A%20Dense%20Video%20Annotation%20for%20Video-Language%20Models&body=Title%3A%20DeVAn%3A%20Dense%20Video%20Annotation%20for%20Video-Language%20Models%0AAuthor%3A%20Tingkai%20Liu%20and%20Yunzhe%20Tao%20and%20Haogeng%20Liu%20and%20Qihang%20Fan%20and%20Ding%20Zhou%20and%20Huaibo%20Huang%20and%20Ran%20He%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20We%20present%20a%20novel%20human%20annotated%20dataset%20for%20evaluating%20the%20ability%20for%0Avisual-language%20models%20to%20generate%20both%20short%20and%20long%20descriptions%20for%0Areal-world%20video%20clips%2C%20termed%20DeVAn%20%28Dense%20Video%20Annotation%29.%20The%20dataset%0Acontains%208.5K%20YouTube%20video%20clips%20of%2020-60%20seconds%20in%20duration%20and%20covers%20a%0Awide%20range%20of%20topics%20and%20interests.%20Each%20video%20clip%20is%20independently%20annotated%0Aby%205%20human%20annotators%2C%20producing%20both%20captions%20%281%20sentence%29%20and%20summaries%20%283-10%0Asentences%29.%20Given%20any%20video%20selected%20from%20the%20dataset%20and%20its%20corresponding%20ASR%0Ainformation%2C%20we%20evaluate%20visuallanguage%20models%20on%20either%20caption%20or%20summary%0Ageneration%20that%20is%20grounded%20in%20both%20the%20visual%20and%20auditory%20content%20of%20the%0Avideo.%20Additionally%2C%20models%20are%20also%20evaluated%20on%20caption-%20and%20summary-based%0Aretrieval%20tasks%2C%20where%20the%20summary-based%20retrieval%20task%20requires%20the%0Aidentification%20of%20a%20target%20video%20given%20excerpts%20of%20a%20given%20summary.%20Given%20the%0Anovel%20nature%20of%20the%20paragraph-length%20video%20summarization%20task%2C%20we%20compared%0Adifferent%20existing%20evaluation%20metrics%20and%20their%20alignment%20with%20human%0Apreferences%20and%20found%20that%20model-based%20evaluation%20metrics%20provide%20more%0Asemantically-oriented%20and%20human-aligned%20evaluation.%20Finally%2C%20we%20benchmarked%20a%0Awide%20range%20of%20current%20video-language%20models%20on%20DeVAn%2C%20and%20we%20aim%20for%20DeVAn%20to%0Aserve%20as%20a%20useful%20evaluation%20set%20in%20the%20age%20of%20large%20language%20models%20and%0Acomplex%20multi-modal%20tasks.%20Code%20is%20available%20at%20https%3A%0A//github.com/TK-21st/DeVAn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeVAn%253A%2520Dense%2520Video%2520Annotation%2520for%2520Video-Language%2520Models%26entry.906535625%3DTingkai%2520Liu%2520and%2520Yunzhe%2520Tao%2520and%2520Haogeng%2520Liu%2520and%2520Qihang%2520Fan%2520and%2520Ding%2520Zhou%2520and%2520Huaibo%2520Huang%2520and%2520Ran%2520He%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520human%2520annotated%2520dataset%2520for%2520evaluating%2520the%2520ability%2520for%250Avisual-language%2520models%2520to%2520generate%2520both%2520short%2520and%2520long%2520descriptions%2520for%250Areal-world%2520video%2520clips%252C%2520termed%2520DeVAn%2520%2528Dense%2520Video%2520Annotation%2529.%2520The%2520dataset%250Acontains%25208.5K%2520YouTube%2520video%2520clips%2520of%252020-60%2520seconds%2520in%2520duration%2520and%2520covers%2520a%250Awide%2520range%2520of%2520topics%2520and%2520interests.%2520Each%2520video%2520clip%2520is%2520independently%2520annotated%250Aby%25205%2520human%2520annotators%252C%2520producing%2520both%2520captions%2520%25281%2520sentence%2529%2520and%2520summaries%2520%25283-10%250Asentences%2529.%2520Given%2520any%2520video%2520selected%2520from%2520the%2520dataset%2520and%2520its%2520corresponding%2520ASR%250Ainformation%252C%2520we%2520evaluate%2520visuallanguage%2520models%2520on%2520either%2520caption%2520or%2520summary%250Ageneration%2520that%2520is%2520grounded%2520in%2520both%2520the%2520visual%2520and%2520auditory%2520content%2520of%2520the%250Avideo.%2520Additionally%252C%2520models%2520are%2520also%2520evaluated%2520on%2520caption-%2520and%2520summary-based%250Aretrieval%2520tasks%252C%2520where%2520the%2520summary-based%2520retrieval%2520task%2520requires%2520the%250Aidentification%2520of%2520a%2520target%2520video%2520given%2520excerpts%2520of%2520a%2520given%2520summary.%2520Given%2520the%250Anovel%2520nature%2520of%2520the%2520paragraph-length%2520video%2520summarization%2520task%252C%2520we%2520compared%250Adifferent%2520existing%2520evaluation%2520metrics%2520and%2520their%2520alignment%2520with%2520human%250Apreferences%2520and%2520found%2520that%2520model-based%2520evaluation%2520metrics%2520provide%2520more%250Asemantically-oriented%2520and%2520human-aligned%2520evaluation.%2520Finally%252C%2520we%2520benchmarked%2520a%250Awide%2520range%2520of%2520current%2520video-language%2520models%2520on%2520DeVAn%252C%2520and%2520we%2520aim%2520for%2520DeVAn%2520to%250Aserve%2520as%2520a%2520useful%2520evaluation%2520set%2520in%2520the%2520age%2520of%2520large%2520language%2520models%2520and%250Acomplex%2520multi-modal%2520tasks.%2520Code%2520is%2520available%2520at%2520https%253A%250A//github.com/TK-21st/DeVAn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeVAn%3A%20Dense%20Video%20Annotation%20for%20Video-Language%20Models&entry.906535625=Tingkai%20Liu%20and%20Yunzhe%20Tao%20and%20Haogeng%20Liu%20and%20Qihang%20Fan%20and%20Ding%20Zhou%20and%20Huaibo%20Huang%20and%20Ran%20He%20and%20Hongxia%20Yang&entry.1292438233=%20%20We%20present%20a%20novel%20human%20annotated%20dataset%20for%20evaluating%20the%20ability%20for%0Avisual-language%20models%20to%20generate%20both%20short%20and%20long%20descriptions%20for%0Areal-world%20video%20clips%2C%20termed%20DeVAn%20%28Dense%20Video%20Annotation%29.%20The%20dataset%0Acontains%208.5K%20YouTube%20video%20clips%20of%2020-60%20seconds%20in%20duration%20and%20covers%20a%0Awide%20range%20of%20topics%20and%20interests.%20Each%20video%20clip%20is%20independently%20annotated%0Aby%205%20human%20annotators%2C%20producing%20both%20captions%20%281%20sentence%29%20and%20summaries%20%283-10%0Asentences%29.%20Given%20any%20video%20selected%20from%20the%20dataset%20and%20its%20corresponding%20ASR%0Ainformation%2C%20we%20evaluate%20visuallanguage%20models%20on%20either%20caption%20or%20summary%0Ageneration%20that%20is%20grounded%20in%20both%20the%20visual%20and%20auditory%20content%20of%20the%0Avideo.%20Additionally%2C%20models%20are%20also%20evaluated%20on%20caption-%20and%20summary-based%0Aretrieval%20tasks%2C%20where%20the%20summary-based%20retrieval%20task%20requires%20the%0Aidentification%20of%20a%20target%20video%20given%20excerpts%20of%20a%20given%20summary.%20Given%20the%0Anovel%20nature%20of%20the%20paragraph-length%20video%20summarization%20task%2C%20we%20compared%0Adifferent%20existing%20evaluation%20metrics%20and%20their%20alignment%20with%20human%0Apreferences%20and%20found%20that%20model-based%20evaluation%20metrics%20provide%20more%0Asemantically-oriented%20and%20human-aligned%20evaluation.%20Finally%2C%20we%20benchmarked%20a%0Awide%20range%20of%20current%20video-language%20models%20on%20DeVAn%2C%20and%20we%20aim%20for%20DeVAn%20to%0Aserve%20as%20a%20useful%20evaluation%20set%20in%20the%20age%20of%20large%20language%20models%20and%0Acomplex%20multi-modal%20tasks.%20Code%20is%20available%20at%20https%3A%0A//github.com/TK-21st/DeVAn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05060v2&entry.124074799=Read"},
{"title": "DFL-TORO: A One-Shot Demonstration Framework for Learning Time-Optimal\n  Robotic Manufacturing Tasks", "author": "Alireza Barekatain and Hamed Habibi and Holger Voos", "abstract": "  This paper presents DFL-TORO, a novel Demonstration Framework for Learning\nTime-Optimal Robotic tasks via One-shot kinesthetic demonstration. It aims at\noptimizing the process of Learning from Demonstration (LfD), applied in the\nmanufacturing sector. As the effectiveness of LfD is challenged by the quality\nand efficiency of human demonstrations, our approach offers a streamlined\nmethod to intuitively capture task requirements from human teachers, by\nreducing the need for multiple demonstrations. Furthermore, we propose an\noptimization-based smoothing algorithm that ensures time-optimal and\njerk-regulated demonstration trajectories, while also adhering to the robot's\nkinematic constraints. The result is a significant reduction in noise, thereby\nboosting the robot's operation efficiency. Evaluations using a Franka Emika\nResearch 3 (FR3) robot for a variety of tasks further substantiate the efficacy\nof our framework, highlighting its potential to transform kinesthetic\ndemonstrations in contemporary manufacturing environments. Moreover, we take\nour proposed framework into a real manufacturing setting operated by an ABB\nYuMi robot and showcase its positive impact on LfD outcomes by performing a\ncase study via Dynamic Movement Primitives (DMPs).\n", "link": "http://arxiv.org/abs/2309.09802v3", "date": "2024-08-09", "relevancy": 1.53, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5256}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DFL-TORO%3A%20A%20One-Shot%20Demonstration%20Framework%20for%20Learning%20Time-Optimal%0A%20%20Robotic%20Manufacturing%20Tasks&body=Title%3A%20DFL-TORO%3A%20A%20One-Shot%20Demonstration%20Framework%20for%20Learning%20Time-Optimal%0A%20%20Robotic%20Manufacturing%20Tasks%0AAuthor%3A%20Alireza%20Barekatain%20and%20Hamed%20Habibi%20and%20Holger%20Voos%0AAbstract%3A%20%20%20This%20paper%20presents%20DFL-TORO%2C%20a%20novel%20Demonstration%20Framework%20for%20Learning%0ATime-Optimal%20Robotic%20tasks%20via%20One-shot%20kinesthetic%20demonstration.%20It%20aims%20at%0Aoptimizing%20the%20process%20of%20Learning%20from%20Demonstration%20%28LfD%29%2C%20applied%20in%20the%0Amanufacturing%20sector.%20As%20the%20effectiveness%20of%20LfD%20is%20challenged%20by%20the%20quality%0Aand%20efficiency%20of%20human%20demonstrations%2C%20our%20approach%20offers%20a%20streamlined%0Amethod%20to%20intuitively%20capture%20task%20requirements%20from%20human%20teachers%2C%20by%0Areducing%20the%20need%20for%20multiple%20demonstrations.%20Furthermore%2C%20we%20propose%20an%0Aoptimization-based%20smoothing%20algorithm%20that%20ensures%20time-optimal%20and%0Ajerk-regulated%20demonstration%20trajectories%2C%20while%20also%20adhering%20to%20the%20robot%27s%0Akinematic%20constraints.%20The%20result%20is%20a%20significant%20reduction%20in%20noise%2C%20thereby%0Aboosting%20the%20robot%27s%20operation%20efficiency.%20Evaluations%20using%20a%20Franka%20Emika%0AResearch%203%20%28FR3%29%20robot%20for%20a%20variety%20of%20tasks%20further%20substantiate%20the%20efficacy%0Aof%20our%20framework%2C%20highlighting%20its%20potential%20to%20transform%20kinesthetic%0Ademonstrations%20in%20contemporary%20manufacturing%20environments.%20Moreover%2C%20we%20take%0Aour%20proposed%20framework%20into%20a%20real%20manufacturing%20setting%20operated%20by%20an%20ABB%0AYuMi%20robot%20and%20showcase%20its%20positive%20impact%20on%20LfD%20outcomes%20by%20performing%20a%0Acase%20study%20via%20Dynamic%20Movement%20Primitives%20%28DMPs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09802v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDFL-TORO%253A%2520A%2520One-Shot%2520Demonstration%2520Framework%2520for%2520Learning%2520Time-Optimal%250A%2520%2520Robotic%2520Manufacturing%2520Tasks%26entry.906535625%3DAlireza%2520Barekatain%2520and%2520Hamed%2520Habibi%2520and%2520Holger%2520Voos%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520DFL-TORO%252C%2520a%2520novel%2520Demonstration%2520Framework%2520for%2520Learning%250ATime-Optimal%2520Robotic%2520tasks%2520via%2520One-shot%2520kinesthetic%2520demonstration.%2520It%2520aims%2520at%250Aoptimizing%2520the%2520process%2520of%2520Learning%2520from%2520Demonstration%2520%2528LfD%2529%252C%2520applied%2520in%2520the%250Amanufacturing%2520sector.%2520As%2520the%2520effectiveness%2520of%2520LfD%2520is%2520challenged%2520by%2520the%2520quality%250Aand%2520efficiency%2520of%2520human%2520demonstrations%252C%2520our%2520approach%2520offers%2520a%2520streamlined%250Amethod%2520to%2520intuitively%2520capture%2520task%2520requirements%2520from%2520human%2520teachers%252C%2520by%250Areducing%2520the%2520need%2520for%2520multiple%2520demonstrations.%2520Furthermore%252C%2520we%2520propose%2520an%250Aoptimization-based%2520smoothing%2520algorithm%2520that%2520ensures%2520time-optimal%2520and%250Ajerk-regulated%2520demonstration%2520trajectories%252C%2520while%2520also%2520adhering%2520to%2520the%2520robot%2527s%250Akinematic%2520constraints.%2520The%2520result%2520is%2520a%2520significant%2520reduction%2520in%2520noise%252C%2520thereby%250Aboosting%2520the%2520robot%2527s%2520operation%2520efficiency.%2520Evaluations%2520using%2520a%2520Franka%2520Emika%250AResearch%25203%2520%2528FR3%2529%2520robot%2520for%2520a%2520variety%2520of%2520tasks%2520further%2520substantiate%2520the%2520efficacy%250Aof%2520our%2520framework%252C%2520highlighting%2520its%2520potential%2520to%2520transform%2520kinesthetic%250Ademonstrations%2520in%2520contemporary%2520manufacturing%2520environments.%2520Moreover%252C%2520we%2520take%250Aour%2520proposed%2520framework%2520into%2520a%2520real%2520manufacturing%2520setting%2520operated%2520by%2520an%2520ABB%250AYuMi%2520robot%2520and%2520showcase%2520its%2520positive%2520impact%2520on%2520LfD%2520outcomes%2520by%2520performing%2520a%250Acase%2520study%2520via%2520Dynamic%2520Movement%2520Primitives%2520%2528DMPs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09802v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DFL-TORO%3A%20A%20One-Shot%20Demonstration%20Framework%20for%20Learning%20Time-Optimal%0A%20%20Robotic%20Manufacturing%20Tasks&entry.906535625=Alireza%20Barekatain%20and%20Hamed%20Habibi%20and%20Holger%20Voos&entry.1292438233=%20%20This%20paper%20presents%20DFL-TORO%2C%20a%20novel%20Demonstration%20Framework%20for%20Learning%0ATime-Optimal%20Robotic%20tasks%20via%20One-shot%20kinesthetic%20demonstration.%20It%20aims%20at%0Aoptimizing%20the%20process%20of%20Learning%20from%20Demonstration%20%28LfD%29%2C%20applied%20in%20the%0Amanufacturing%20sector.%20As%20the%20effectiveness%20of%20LfD%20is%20challenged%20by%20the%20quality%0Aand%20efficiency%20of%20human%20demonstrations%2C%20our%20approach%20offers%20a%20streamlined%0Amethod%20to%20intuitively%20capture%20task%20requirements%20from%20human%20teachers%2C%20by%0Areducing%20the%20need%20for%20multiple%20demonstrations.%20Furthermore%2C%20we%20propose%20an%0Aoptimization-based%20smoothing%20algorithm%20that%20ensures%20time-optimal%20and%0Ajerk-regulated%20demonstration%20trajectories%2C%20while%20also%20adhering%20to%20the%20robot%27s%0Akinematic%20constraints.%20The%20result%20is%20a%20significant%20reduction%20in%20noise%2C%20thereby%0Aboosting%20the%20robot%27s%20operation%20efficiency.%20Evaluations%20using%20a%20Franka%20Emika%0AResearch%203%20%28FR3%29%20robot%20for%20a%20variety%20of%20tasks%20further%20substantiate%20the%20efficacy%0Aof%20our%20framework%2C%20highlighting%20its%20potential%20to%20transform%20kinesthetic%0Ademonstrations%20in%20contemporary%20manufacturing%20environments.%20Moreover%2C%20we%20take%0Aour%20proposed%20framework%20into%20a%20real%20manufacturing%20setting%20operated%20by%20an%20ABB%0AYuMi%20robot%20and%20showcase%20its%20positive%20impact%20on%20LfD%20outcomes%20by%20performing%20a%0Acase%20study%20via%20Dynamic%20Movement%20Primitives%20%28DMPs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09802v3&entry.124074799=Read"},
{"title": "Design and Fabrication of Soft Locomotion Robots based on Spatial\n  Compliant Mechanisms", "author": "Andrija Milojevic and Kyrre Glette", "abstract": "  Soft robotics has emerged as a promising technology that holds great\npotential for various application areas. This is due to soft materials unique\nproperties, including flexibility, safety, and shock absorption, among others.\nDespite many advancement in the field, the development of effective design\nmethodologies and production techniques for soft robots remains a challenge.\nAlthough numerous robot prototypes have been proposed in recent years, their\ndesigns are often complex and difficult to produce. As such, there is a need\nfor more efficient and unified design approaches that can facilitate the\nproduction of soft robots with desirable properties. In this paper, we propose\na method for designing soft robots using elastic beams and spatial compliant\nmechanisms. The method is based on an evolutionary approach that enables the\ncreation of designs with both high motion and force transmission ratios.\nSpecifically, we focus on the development of locomotion mechanisms using a\ncentral linear actuator. Our approach involves the use of commonly available\nplastic materials and a 3D printer to manufacture the designs. We demonstrate\nthe feasibility of our approach by presenting experimental results that show\nsuccessful production and real world operation. Overall, our findings suggest\nthat the use of elastic beams and an evolutionary approach can facilitate the\ncreation of soft robots with desirable locomotion properties, including fast\nlocomotion up to 3.7 body lengths per second, locomotion with a payload, and\nunderwater locomotion. This method has the potential to enable the development\nof more efficient and practical soft robots for various applications.\n", "link": "http://arxiv.org/abs/2408.05207v1", "date": "2024-08-09", "relevancy": 1.5289, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20and%20Fabrication%20of%20Soft%20Locomotion%20Robots%20based%20on%20Spatial%0A%20%20Compliant%20Mechanisms&body=Title%3A%20Design%20and%20Fabrication%20of%20Soft%20Locomotion%20Robots%20based%20on%20Spatial%0A%20%20Compliant%20Mechanisms%0AAuthor%3A%20Andrija%20Milojevic%20and%20Kyrre%20Glette%0AAbstract%3A%20%20%20Soft%20robotics%20has%20emerged%20as%20a%20promising%20technology%20that%20holds%20great%0Apotential%20for%20various%20application%20areas.%20This%20is%20due%20to%20soft%20materials%20unique%0Aproperties%2C%20including%20flexibility%2C%20safety%2C%20and%20shock%20absorption%2C%20among%20others.%0ADespite%20many%20advancement%20in%20the%20field%2C%20the%20development%20of%20effective%20design%0Amethodologies%20and%20production%20techniques%20for%20soft%20robots%20remains%20a%20challenge.%0AAlthough%20numerous%20robot%20prototypes%20have%20been%20proposed%20in%20recent%20years%2C%20their%0Adesigns%20are%20often%20complex%20and%20difficult%20to%20produce.%20As%20such%2C%20there%20is%20a%20need%0Afor%20more%20efficient%20and%20unified%20design%20approaches%20that%20can%20facilitate%20the%0Aproduction%20of%20soft%20robots%20with%20desirable%20properties.%20In%20this%20paper%2C%20we%20propose%0Aa%20method%20for%20designing%20soft%20robots%20using%20elastic%20beams%20and%20spatial%20compliant%0Amechanisms.%20The%20method%20is%20based%20on%20an%20evolutionary%20approach%20that%20enables%20the%0Acreation%20of%20designs%20with%20both%20high%20motion%20and%20force%20transmission%20ratios.%0ASpecifically%2C%20we%20focus%20on%20the%20development%20of%20locomotion%20mechanisms%20using%20a%0Acentral%20linear%20actuator.%20Our%20approach%20involves%20the%20use%20of%20commonly%20available%0Aplastic%20materials%20and%20a%203D%20printer%20to%20manufacture%20the%20designs.%20We%20demonstrate%0Athe%20feasibility%20of%20our%20approach%20by%20presenting%20experimental%20results%20that%20show%0Asuccessful%20production%20and%20real%20world%20operation.%20Overall%2C%20our%20findings%20suggest%0Athat%20the%20use%20of%20elastic%20beams%20and%20an%20evolutionary%20approach%20can%20facilitate%20the%0Acreation%20of%20soft%20robots%20with%20desirable%20locomotion%20properties%2C%20including%20fast%0Alocomotion%20up%20to%203.7%20body%20lengths%20per%20second%2C%20locomotion%20with%20a%20payload%2C%20and%0Aunderwater%20locomotion.%20This%20method%20has%20the%20potential%20to%20enable%20the%20development%0Aof%20more%20efficient%20and%20practical%20soft%20robots%20for%20various%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520and%2520Fabrication%2520of%2520Soft%2520Locomotion%2520Robots%2520based%2520on%2520Spatial%250A%2520%2520Compliant%2520Mechanisms%26entry.906535625%3DAndrija%2520Milojevic%2520and%2520Kyrre%2520Glette%26entry.1292438233%3D%2520%2520Soft%2520robotics%2520has%2520emerged%2520as%2520a%2520promising%2520technology%2520that%2520holds%2520great%250Apotential%2520for%2520various%2520application%2520areas.%2520This%2520is%2520due%2520to%2520soft%2520materials%2520unique%250Aproperties%252C%2520including%2520flexibility%252C%2520safety%252C%2520and%2520shock%2520absorption%252C%2520among%2520others.%250ADespite%2520many%2520advancement%2520in%2520the%2520field%252C%2520the%2520development%2520of%2520effective%2520design%250Amethodologies%2520and%2520production%2520techniques%2520for%2520soft%2520robots%2520remains%2520a%2520challenge.%250AAlthough%2520numerous%2520robot%2520prototypes%2520have%2520been%2520proposed%2520in%2520recent%2520years%252C%2520their%250Adesigns%2520are%2520often%2520complex%2520and%2520difficult%2520to%2520produce.%2520As%2520such%252C%2520there%2520is%2520a%2520need%250Afor%2520more%2520efficient%2520and%2520unified%2520design%2520approaches%2520that%2520can%2520facilitate%2520the%250Aproduction%2520of%2520soft%2520robots%2520with%2520desirable%2520properties.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520method%2520for%2520designing%2520soft%2520robots%2520using%2520elastic%2520beams%2520and%2520spatial%2520compliant%250Amechanisms.%2520The%2520method%2520is%2520based%2520on%2520an%2520evolutionary%2520approach%2520that%2520enables%2520the%250Acreation%2520of%2520designs%2520with%2520both%2520high%2520motion%2520and%2520force%2520transmission%2520ratios.%250ASpecifically%252C%2520we%2520focus%2520on%2520the%2520development%2520of%2520locomotion%2520mechanisms%2520using%2520a%250Acentral%2520linear%2520actuator.%2520Our%2520approach%2520involves%2520the%2520use%2520of%2520commonly%2520available%250Aplastic%2520materials%2520and%2520a%25203D%2520printer%2520to%2520manufacture%2520the%2520designs.%2520We%2520demonstrate%250Athe%2520feasibility%2520of%2520our%2520approach%2520by%2520presenting%2520experimental%2520results%2520that%2520show%250Asuccessful%2520production%2520and%2520real%2520world%2520operation.%2520Overall%252C%2520our%2520findings%2520suggest%250Athat%2520the%2520use%2520of%2520elastic%2520beams%2520and%2520an%2520evolutionary%2520approach%2520can%2520facilitate%2520the%250Acreation%2520of%2520soft%2520robots%2520with%2520desirable%2520locomotion%2520properties%252C%2520including%2520fast%250Alocomotion%2520up%2520to%25203.7%2520body%2520lengths%2520per%2520second%252C%2520locomotion%2520with%2520a%2520payload%252C%2520and%250Aunderwater%2520locomotion.%2520This%2520method%2520has%2520the%2520potential%2520to%2520enable%2520the%2520development%250Aof%2520more%2520efficient%2520and%2520practical%2520soft%2520robots%2520for%2520various%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20and%20Fabrication%20of%20Soft%20Locomotion%20Robots%20based%20on%20Spatial%0A%20%20Compliant%20Mechanisms&entry.906535625=Andrija%20Milojevic%20and%20Kyrre%20Glette&entry.1292438233=%20%20Soft%20robotics%20has%20emerged%20as%20a%20promising%20technology%20that%20holds%20great%0Apotential%20for%20various%20application%20areas.%20This%20is%20due%20to%20soft%20materials%20unique%0Aproperties%2C%20including%20flexibility%2C%20safety%2C%20and%20shock%20absorption%2C%20among%20others.%0ADespite%20many%20advancement%20in%20the%20field%2C%20the%20development%20of%20effective%20design%0Amethodologies%20and%20production%20techniques%20for%20soft%20robots%20remains%20a%20challenge.%0AAlthough%20numerous%20robot%20prototypes%20have%20been%20proposed%20in%20recent%20years%2C%20their%0Adesigns%20are%20often%20complex%20and%20difficult%20to%20produce.%20As%20such%2C%20there%20is%20a%20need%0Afor%20more%20efficient%20and%20unified%20design%20approaches%20that%20can%20facilitate%20the%0Aproduction%20of%20soft%20robots%20with%20desirable%20properties.%20In%20this%20paper%2C%20we%20propose%0Aa%20method%20for%20designing%20soft%20robots%20using%20elastic%20beams%20and%20spatial%20compliant%0Amechanisms.%20The%20method%20is%20based%20on%20an%20evolutionary%20approach%20that%20enables%20the%0Acreation%20of%20designs%20with%20both%20high%20motion%20and%20force%20transmission%20ratios.%0ASpecifically%2C%20we%20focus%20on%20the%20development%20of%20locomotion%20mechanisms%20using%20a%0Acentral%20linear%20actuator.%20Our%20approach%20involves%20the%20use%20of%20commonly%20available%0Aplastic%20materials%20and%20a%203D%20printer%20to%20manufacture%20the%20designs.%20We%20demonstrate%0Athe%20feasibility%20of%20our%20approach%20by%20presenting%20experimental%20results%20that%20show%0Asuccessful%20production%20and%20real%20world%20operation.%20Overall%2C%20our%20findings%20suggest%0Athat%20the%20use%20of%20elastic%20beams%20and%20an%20evolutionary%20approach%20can%20facilitate%20the%0Acreation%20of%20soft%20robots%20with%20desirable%20locomotion%20properties%2C%20including%20fast%0Alocomotion%20up%20to%203.7%20body%20lengths%20per%20second%2C%20locomotion%20with%20a%20payload%2C%20and%0Aunderwater%20locomotion.%20This%20method%20has%20the%20potential%20to%20enable%20the%20development%0Aof%20more%20efficient%20and%20practical%20soft%20robots%20for%20various%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05207v1&entry.124074799=Read"},
{"title": "SELD-Mamba: Selective State-Space Model for Sound Event Localization and\n  Detection with Source Distance Estimation", "author": "Da Mu and Zhicheng Zhang and Haobo Yue and Zehao Wang and Jin Tang and Jianqin Yin", "abstract": "  In the Sound Event Localization and Detection (SELD) task, Transformer-based\nmodels have demonstrated impressive capabilities. However, the quadratic\ncomplexity of the Transformer's self-attention mechanism results in\ncomputational inefficiencies. In this paper, we propose a network architecture\nfor SELD called SELD-Mamba, which utilizes Mamba, a selective state-space\nmodel. We adopt the Event-Independent Network V2 (EINV2) as the foundational\nframework and replace its Conformer blocks with bidirectional Mamba blocks to\ncapture a broader range of contextual information while maintaining\ncomputational efficiency. Additionally, we implement a two-stage training\nmethod, with the first stage focusing on Sound Event Detection (SED) and\nDirection of Arrival (DoA) estimation losses, and the second stage\nreintroducing the Source Distance Estimation (SDE) loss. Our experimental\nresults on the 2024 DCASE Challenge Task3 dataset demonstrate the effectiveness\nof the selective state-space model in SELD and highlight the benefits of the\ntwo-stage training approach in enhancing SELD performance.\n", "link": "http://arxiv.org/abs/2408.05057v1", "date": "2024-08-09", "relevancy": 1.5277, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.513}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5102}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SELD-Mamba%3A%20Selective%20State-Space%20Model%20for%20Sound%20Event%20Localization%20and%0A%20%20Detection%20with%20Source%20Distance%20Estimation&body=Title%3A%20SELD-Mamba%3A%20Selective%20State-Space%20Model%20for%20Sound%20Event%20Localization%20and%0A%20%20Detection%20with%20Source%20Distance%20Estimation%0AAuthor%3A%20Da%20Mu%20and%20Zhicheng%20Zhang%20and%20Haobo%20Yue%20and%20Zehao%20Wang%20and%20Jin%20Tang%20and%20Jianqin%20Yin%0AAbstract%3A%20%20%20In%20the%20Sound%20Event%20Localization%20and%20Detection%20%28SELD%29%20task%2C%20Transformer-based%0Amodels%20have%20demonstrated%20impressive%20capabilities.%20However%2C%20the%20quadratic%0Acomplexity%20of%20the%20Transformer%27s%20self-attention%20mechanism%20results%20in%0Acomputational%20inefficiencies.%20In%20this%20paper%2C%20we%20propose%20a%20network%20architecture%0Afor%20SELD%20called%20SELD-Mamba%2C%20which%20utilizes%20Mamba%2C%20a%20selective%20state-space%0Amodel.%20We%20adopt%20the%20Event-Independent%20Network%20V2%20%28EINV2%29%20as%20the%20foundational%0Aframework%20and%20replace%20its%20Conformer%20blocks%20with%20bidirectional%20Mamba%20blocks%20to%0Acapture%20a%20broader%20range%20of%20contextual%20information%20while%20maintaining%0Acomputational%20efficiency.%20Additionally%2C%20we%20implement%20a%20two-stage%20training%0Amethod%2C%20with%20the%20first%20stage%20focusing%20on%20Sound%20Event%20Detection%20%28SED%29%20and%0ADirection%20of%20Arrival%20%28DoA%29%20estimation%20losses%2C%20and%20the%20second%20stage%0Areintroducing%20the%20Source%20Distance%20Estimation%20%28SDE%29%20loss.%20Our%20experimental%0Aresults%20on%20the%202024%20DCASE%20Challenge%20Task3%20dataset%20demonstrate%20the%20effectiveness%0Aof%20the%20selective%20state-space%20model%20in%20SELD%20and%20highlight%20the%20benefits%20of%20the%0Atwo-stage%20training%20approach%20in%20enhancing%20SELD%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSELD-Mamba%253A%2520Selective%2520State-Space%2520Model%2520for%2520Sound%2520Event%2520Localization%2520and%250A%2520%2520Detection%2520with%2520Source%2520Distance%2520Estimation%26entry.906535625%3DDa%2520Mu%2520and%2520Zhicheng%2520Zhang%2520and%2520Haobo%2520Yue%2520and%2520Zehao%2520Wang%2520and%2520Jin%2520Tang%2520and%2520Jianqin%2520Yin%26entry.1292438233%3D%2520%2520In%2520the%2520Sound%2520Event%2520Localization%2520and%2520Detection%2520%2528SELD%2529%2520task%252C%2520Transformer-based%250Amodels%2520have%2520demonstrated%2520impressive%2520capabilities.%2520However%252C%2520the%2520quadratic%250Acomplexity%2520of%2520the%2520Transformer%2527s%2520self-attention%2520mechanism%2520results%2520in%250Acomputational%2520inefficiencies.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520network%2520architecture%250Afor%2520SELD%2520called%2520SELD-Mamba%252C%2520which%2520utilizes%2520Mamba%252C%2520a%2520selective%2520state-space%250Amodel.%2520We%2520adopt%2520the%2520Event-Independent%2520Network%2520V2%2520%2528EINV2%2529%2520as%2520the%2520foundational%250Aframework%2520and%2520replace%2520its%2520Conformer%2520blocks%2520with%2520bidirectional%2520Mamba%2520blocks%2520to%250Acapture%2520a%2520broader%2520range%2520of%2520contextual%2520information%2520while%2520maintaining%250Acomputational%2520efficiency.%2520Additionally%252C%2520we%2520implement%2520a%2520two-stage%2520training%250Amethod%252C%2520with%2520the%2520first%2520stage%2520focusing%2520on%2520Sound%2520Event%2520Detection%2520%2528SED%2529%2520and%250ADirection%2520of%2520Arrival%2520%2528DoA%2529%2520estimation%2520losses%252C%2520and%2520the%2520second%2520stage%250Areintroducing%2520the%2520Source%2520Distance%2520Estimation%2520%2528SDE%2529%2520loss.%2520Our%2520experimental%250Aresults%2520on%2520the%25202024%2520DCASE%2520Challenge%2520Task3%2520dataset%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520selective%2520state-space%2520model%2520in%2520SELD%2520and%2520highlight%2520the%2520benefits%2520of%2520the%250Atwo-stage%2520training%2520approach%2520in%2520enhancing%2520SELD%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SELD-Mamba%3A%20Selective%20State-Space%20Model%20for%20Sound%20Event%20Localization%20and%0A%20%20Detection%20with%20Source%20Distance%20Estimation&entry.906535625=Da%20Mu%20and%20Zhicheng%20Zhang%20and%20Haobo%20Yue%20and%20Zehao%20Wang%20and%20Jin%20Tang%20and%20Jianqin%20Yin&entry.1292438233=%20%20In%20the%20Sound%20Event%20Localization%20and%20Detection%20%28SELD%29%20task%2C%20Transformer-based%0Amodels%20have%20demonstrated%20impressive%20capabilities.%20However%2C%20the%20quadratic%0Acomplexity%20of%20the%20Transformer%27s%20self-attention%20mechanism%20results%20in%0Acomputational%20inefficiencies.%20In%20this%20paper%2C%20we%20propose%20a%20network%20architecture%0Afor%20SELD%20called%20SELD-Mamba%2C%20which%20utilizes%20Mamba%2C%20a%20selective%20state-space%0Amodel.%20We%20adopt%20the%20Event-Independent%20Network%20V2%20%28EINV2%29%20as%20the%20foundational%0Aframework%20and%20replace%20its%20Conformer%20blocks%20with%20bidirectional%20Mamba%20blocks%20to%0Acapture%20a%20broader%20range%20of%20contextual%20information%20while%20maintaining%0Acomputational%20efficiency.%20Additionally%2C%20we%20implement%20a%20two-stage%20training%0Amethod%2C%20with%20the%20first%20stage%20focusing%20on%20Sound%20Event%20Detection%20%28SED%29%20and%0ADirection%20of%20Arrival%20%28DoA%29%20estimation%20losses%2C%20and%20the%20second%20stage%0Areintroducing%20the%20Source%20Distance%20Estimation%20%28SDE%29%20loss.%20Our%20experimental%0Aresults%20on%20the%202024%20DCASE%20Challenge%20Task3%20dataset%20demonstrate%20the%20effectiveness%0Aof%20the%20selective%20state-space%20model%20in%20SELD%20and%20highlight%20the%20benefits%20of%20the%0Atwo-stage%20training%20approach%20in%20enhancing%20SELD%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05057v1&entry.124074799=Read"},
{"title": "Cautious Calibration in Binary Classification", "author": "Mari-Liis Allikivi and Joonas J\u00e4rve and Meelis Kull", "abstract": "  Being cautious is crucial for enhancing the trustworthiness of machine\nlearning systems integrated into decision-making pipelines. Although calibrated\nprobabilities help in optimal decision-making, perfect calibration remains\nunattainable, leading to estimates that fluctuate between under- and\noverconfidence. This becomes a critical issue in high-risk scenarios, where\neven occasional overestimation can lead to extreme expected costs. In these\nscenarios, it is important for each predicted probability to lean towards\nunderconfidence, rather than just achieving an average balance. In this study,\nwe introduce the novel concept of cautious calibration in binary\nclassification. This approach aims to produce probability estimates that are\nintentionally underconfident for each predicted probability. We highlight the\nimportance of this approach in a high-risk scenario and propose a theoretically\ngrounded method for learning cautious calibration maps. Through experiments, we\nexplore and compare our method to various approaches, including methods\noriginally not devised for cautious calibration but applicable in this context.\nWe show that our approach is the most consistent in providing cautious\nestimates. Our work establishes a strong baseline for further developments in\nthis novel framework.\n", "link": "http://arxiv.org/abs/2408.05120v1", "date": "2024-08-09", "relevancy": 1.526, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5141}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cautious%20Calibration%20in%20Binary%20Classification&body=Title%3A%20Cautious%20Calibration%20in%20Binary%20Classification%0AAuthor%3A%20Mari-Liis%20Allikivi%20and%20Joonas%20J%C3%A4rve%20and%20Meelis%20Kull%0AAbstract%3A%20%20%20Being%20cautious%20is%20crucial%20for%20enhancing%20the%20trustworthiness%20of%20machine%0Alearning%20systems%20integrated%20into%20decision-making%20pipelines.%20Although%20calibrated%0Aprobabilities%20help%20in%20optimal%20decision-making%2C%20perfect%20calibration%20remains%0Aunattainable%2C%20leading%20to%20estimates%20that%20fluctuate%20between%20under-%20and%0Aoverconfidence.%20This%20becomes%20a%20critical%20issue%20in%20high-risk%20scenarios%2C%20where%0Aeven%20occasional%20overestimation%20can%20lead%20to%20extreme%20expected%20costs.%20In%20these%0Ascenarios%2C%20it%20is%20important%20for%20each%20predicted%20probability%20to%20lean%20towards%0Aunderconfidence%2C%20rather%20than%20just%20achieving%20an%20average%20balance.%20In%20this%20study%2C%0Awe%20introduce%20the%20novel%20concept%20of%20cautious%20calibration%20in%20binary%0Aclassification.%20This%20approach%20aims%20to%20produce%20probability%20estimates%20that%20are%0Aintentionally%20underconfident%20for%20each%20predicted%20probability.%20We%20highlight%20the%0Aimportance%20of%20this%20approach%20in%20a%20high-risk%20scenario%20and%20propose%20a%20theoretically%0Agrounded%20method%20for%20learning%20cautious%20calibration%20maps.%20Through%20experiments%2C%20we%0Aexplore%20and%20compare%20our%20method%20to%20various%20approaches%2C%20including%20methods%0Aoriginally%20not%20devised%20for%20cautious%20calibration%20but%20applicable%20in%20this%20context.%0AWe%20show%20that%20our%20approach%20is%20the%20most%20consistent%20in%20providing%20cautious%0Aestimates.%20Our%20work%20establishes%20a%20strong%20baseline%20for%20further%20developments%20in%0Athis%20novel%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCautious%2520Calibration%2520in%2520Binary%2520Classification%26entry.906535625%3DMari-Liis%2520Allikivi%2520and%2520Joonas%2520J%25C3%25A4rve%2520and%2520Meelis%2520Kull%26entry.1292438233%3D%2520%2520Being%2520cautious%2520is%2520crucial%2520for%2520enhancing%2520the%2520trustworthiness%2520of%2520machine%250Alearning%2520systems%2520integrated%2520into%2520decision-making%2520pipelines.%2520Although%2520calibrated%250Aprobabilities%2520help%2520in%2520optimal%2520decision-making%252C%2520perfect%2520calibration%2520remains%250Aunattainable%252C%2520leading%2520to%2520estimates%2520that%2520fluctuate%2520between%2520under-%2520and%250Aoverconfidence.%2520This%2520becomes%2520a%2520critical%2520issue%2520in%2520high-risk%2520scenarios%252C%2520where%250Aeven%2520occasional%2520overestimation%2520can%2520lead%2520to%2520extreme%2520expected%2520costs.%2520In%2520these%250Ascenarios%252C%2520it%2520is%2520important%2520for%2520each%2520predicted%2520probability%2520to%2520lean%2520towards%250Aunderconfidence%252C%2520rather%2520than%2520just%2520achieving%2520an%2520average%2520balance.%2520In%2520this%2520study%252C%250Awe%2520introduce%2520the%2520novel%2520concept%2520of%2520cautious%2520calibration%2520in%2520binary%250Aclassification.%2520This%2520approach%2520aims%2520to%2520produce%2520probability%2520estimates%2520that%2520are%250Aintentionally%2520underconfident%2520for%2520each%2520predicted%2520probability.%2520We%2520highlight%2520the%250Aimportance%2520of%2520this%2520approach%2520in%2520a%2520high-risk%2520scenario%2520and%2520propose%2520a%2520theoretically%250Agrounded%2520method%2520for%2520learning%2520cautious%2520calibration%2520maps.%2520Through%2520experiments%252C%2520we%250Aexplore%2520and%2520compare%2520our%2520method%2520to%2520various%2520approaches%252C%2520including%2520methods%250Aoriginally%2520not%2520devised%2520for%2520cautious%2520calibration%2520but%2520applicable%2520in%2520this%2520context.%250AWe%2520show%2520that%2520our%2520approach%2520is%2520the%2520most%2520consistent%2520in%2520providing%2520cautious%250Aestimates.%2520Our%2520work%2520establishes%2520a%2520strong%2520baseline%2520for%2520further%2520developments%2520in%250Athis%2520novel%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cautious%20Calibration%20in%20Binary%20Classification&entry.906535625=Mari-Liis%20Allikivi%20and%20Joonas%20J%C3%A4rve%20and%20Meelis%20Kull&entry.1292438233=%20%20Being%20cautious%20is%20crucial%20for%20enhancing%20the%20trustworthiness%20of%20machine%0Alearning%20systems%20integrated%20into%20decision-making%20pipelines.%20Although%20calibrated%0Aprobabilities%20help%20in%20optimal%20decision-making%2C%20perfect%20calibration%20remains%0Aunattainable%2C%20leading%20to%20estimates%20that%20fluctuate%20between%20under-%20and%0Aoverconfidence.%20This%20becomes%20a%20critical%20issue%20in%20high-risk%20scenarios%2C%20where%0Aeven%20occasional%20overestimation%20can%20lead%20to%20extreme%20expected%20costs.%20In%20these%0Ascenarios%2C%20it%20is%20important%20for%20each%20predicted%20probability%20to%20lean%20towards%0Aunderconfidence%2C%20rather%20than%20just%20achieving%20an%20average%20balance.%20In%20this%20study%2C%0Awe%20introduce%20the%20novel%20concept%20of%20cautious%20calibration%20in%20binary%0Aclassification.%20This%20approach%20aims%20to%20produce%20probability%20estimates%20that%20are%0Aintentionally%20underconfident%20for%20each%20predicted%20probability.%20We%20highlight%20the%0Aimportance%20of%20this%20approach%20in%20a%20high-risk%20scenario%20and%20propose%20a%20theoretically%0Agrounded%20method%20for%20learning%20cautious%20calibration%20maps.%20Through%20experiments%2C%20we%0Aexplore%20and%20compare%20our%20method%20to%20various%20approaches%2C%20including%20methods%0Aoriginally%20not%20devised%20for%20cautious%20calibration%20but%20applicable%20in%20this%20context.%0AWe%20show%20that%20our%20approach%20is%20the%20most%20consistent%20in%20providing%20cautious%0Aestimates.%20Our%20work%20establishes%20a%20strong%20baseline%20for%20further%20developments%20in%0Athis%20novel%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05120v1&entry.124074799=Read"},
{"title": "Generating novel experimental hypotheses from language models: A case\n  study on cross-dative generalization", "author": "Kanishka Misra and Najoung Kim", "abstract": "  Neural network language models (LMs) have been shown to successfully capture\ncomplex linguistic knowledge. However, their utility for understanding language\nacquisition is still debated. We contribute to this debate by presenting a case\nstudy where we use LMs as simulated learners to derive novel experimental\nhypotheses to be tested with humans. We apply this paradigm to study\ncross-dative generalization (CDG): productive generalization of novel verbs\nacross dative constructions (she pilked me the ball/she pilked the ball to me)\n-- acquisition of which is known to involve a large space of contextual\nfeatures -- using LMs trained on child-directed speech. We specifically ask:\n\"what properties of the training exposure facilitate a novel verb's\ngeneralization to the (unmodeled) alternate construction?\" To answer this, we\nsystematically vary the exposure context in which a novel dative verb occurs in\nterms of the properties of the theme and recipient, and then analyze the LMs'\nusage of the novel verb in the unmodeled dative construction. We find LMs to\nreplicate known patterns of children's CDG, as a precondition to exploring\nnovel hypotheses. Subsequent simulations reveal a nuanced role of the features\nof the novel verbs' exposure context on the LMs' CDG. We find CDG to be\nfacilitated when the first postverbal argument of the exposure context is\npronominal, definite, short, and conforms to the prototypical animacy\nexpectations of the exposure dative. These patterns are characteristic of\nharmonic alignment in datives, where the argument with features ranking higher\non the discourse prominence scale tends to precede the other. This gives rise\nto a novel hypothesis that CDG is facilitated insofar as the features of the\nexposure context -- in particular, its first postverbal argument -- are\nharmonically aligned. We conclude by proposing future experiments that can test\nthis hypothesis in children.\n", "link": "http://arxiv.org/abs/2408.05086v1", "date": "2024-08-09", "relevancy": 1.5103, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5242}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5036}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20novel%20experimental%20hypotheses%20from%20language%20models%3A%20A%20case%0A%20%20study%20on%20cross-dative%20generalization&body=Title%3A%20Generating%20novel%20experimental%20hypotheses%20from%20language%20models%3A%20A%20case%0A%20%20study%20on%20cross-dative%20generalization%0AAuthor%3A%20Kanishka%20Misra%20and%20Najoung%20Kim%0AAbstract%3A%20%20%20Neural%20network%20language%20models%20%28LMs%29%20have%20been%20shown%20to%20successfully%20capture%0Acomplex%20linguistic%20knowledge.%20However%2C%20their%20utility%20for%20understanding%20language%0Aacquisition%20is%20still%20debated.%20We%20contribute%20to%20this%20debate%20by%20presenting%20a%20case%0Astudy%20where%20we%20use%20LMs%20as%20simulated%20learners%20to%20derive%20novel%20experimental%0Ahypotheses%20to%20be%20tested%20with%20humans.%20We%20apply%20this%20paradigm%20to%20study%0Across-dative%20generalization%20%28CDG%29%3A%20productive%20generalization%20of%20novel%20verbs%0Aacross%20dative%20constructions%20%28she%20pilked%20me%20the%20ball/she%20pilked%20the%20ball%20to%20me%29%0A--%20acquisition%20of%20which%20is%20known%20to%20involve%20a%20large%20space%20of%20contextual%0Afeatures%20--%20using%20LMs%20trained%20on%20child-directed%20speech.%20We%20specifically%20ask%3A%0A%22what%20properties%20of%20the%20training%20exposure%20facilitate%20a%20novel%20verb%27s%0Ageneralization%20to%20the%20%28unmodeled%29%20alternate%20construction%3F%22%20To%20answer%20this%2C%20we%0Asystematically%20vary%20the%20exposure%20context%20in%20which%20a%20novel%20dative%20verb%20occurs%20in%0Aterms%20of%20the%20properties%20of%20the%20theme%20and%20recipient%2C%20and%20then%20analyze%20the%20LMs%27%0Ausage%20of%20the%20novel%20verb%20in%20the%20unmodeled%20dative%20construction.%20We%20find%20LMs%20to%0Areplicate%20known%20patterns%20of%20children%27s%20CDG%2C%20as%20a%20precondition%20to%20exploring%0Anovel%20hypotheses.%20Subsequent%20simulations%20reveal%20a%20nuanced%20role%20of%20the%20features%0Aof%20the%20novel%20verbs%27%20exposure%20context%20on%20the%20LMs%27%20CDG.%20We%20find%20CDG%20to%20be%0Afacilitated%20when%20the%20first%20postverbal%20argument%20of%20the%20exposure%20context%20is%0Apronominal%2C%20definite%2C%20short%2C%20and%20conforms%20to%20the%20prototypical%20animacy%0Aexpectations%20of%20the%20exposure%20dative.%20These%20patterns%20are%20characteristic%20of%0Aharmonic%20alignment%20in%20datives%2C%20where%20the%20argument%20with%20features%20ranking%20higher%0Aon%20the%20discourse%20prominence%20scale%20tends%20to%20precede%20the%20other.%20This%20gives%20rise%0Ato%20a%20novel%20hypothesis%20that%20CDG%20is%20facilitated%20insofar%20as%20the%20features%20of%20the%0Aexposure%20context%20--%20in%20particular%2C%20its%20first%20postverbal%20argument%20--%20are%0Aharmonically%20aligned.%20We%20conclude%20by%20proposing%20future%20experiments%20that%20can%20test%0Athis%20hypothesis%20in%20children.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520novel%2520experimental%2520hypotheses%2520from%2520language%2520models%253A%2520A%2520case%250A%2520%2520study%2520on%2520cross-dative%2520generalization%26entry.906535625%3DKanishka%2520Misra%2520and%2520Najoung%2520Kim%26entry.1292438233%3D%2520%2520Neural%2520network%2520language%2520models%2520%2528LMs%2529%2520have%2520been%2520shown%2520to%2520successfully%2520capture%250Acomplex%2520linguistic%2520knowledge.%2520However%252C%2520their%2520utility%2520for%2520understanding%2520language%250Aacquisition%2520is%2520still%2520debated.%2520We%2520contribute%2520to%2520this%2520debate%2520by%2520presenting%2520a%2520case%250Astudy%2520where%2520we%2520use%2520LMs%2520as%2520simulated%2520learners%2520to%2520derive%2520novel%2520experimental%250Ahypotheses%2520to%2520be%2520tested%2520with%2520humans.%2520We%2520apply%2520this%2520paradigm%2520to%2520study%250Across-dative%2520generalization%2520%2528CDG%2529%253A%2520productive%2520generalization%2520of%2520novel%2520verbs%250Aacross%2520dative%2520constructions%2520%2528she%2520pilked%2520me%2520the%2520ball/she%2520pilked%2520the%2520ball%2520to%2520me%2529%250A--%2520acquisition%2520of%2520which%2520is%2520known%2520to%2520involve%2520a%2520large%2520space%2520of%2520contextual%250Afeatures%2520--%2520using%2520LMs%2520trained%2520on%2520child-directed%2520speech.%2520We%2520specifically%2520ask%253A%250A%2522what%2520properties%2520of%2520the%2520training%2520exposure%2520facilitate%2520a%2520novel%2520verb%2527s%250Ageneralization%2520to%2520the%2520%2528unmodeled%2529%2520alternate%2520construction%253F%2522%2520To%2520answer%2520this%252C%2520we%250Asystematically%2520vary%2520the%2520exposure%2520context%2520in%2520which%2520a%2520novel%2520dative%2520verb%2520occurs%2520in%250Aterms%2520of%2520the%2520properties%2520of%2520the%2520theme%2520and%2520recipient%252C%2520and%2520then%2520analyze%2520the%2520LMs%2527%250Ausage%2520of%2520the%2520novel%2520verb%2520in%2520the%2520unmodeled%2520dative%2520construction.%2520We%2520find%2520LMs%2520to%250Areplicate%2520known%2520patterns%2520of%2520children%2527s%2520CDG%252C%2520as%2520a%2520precondition%2520to%2520exploring%250Anovel%2520hypotheses.%2520Subsequent%2520simulations%2520reveal%2520a%2520nuanced%2520role%2520of%2520the%2520features%250Aof%2520the%2520novel%2520verbs%2527%2520exposure%2520context%2520on%2520the%2520LMs%2527%2520CDG.%2520We%2520find%2520CDG%2520to%2520be%250Afacilitated%2520when%2520the%2520first%2520postverbal%2520argument%2520of%2520the%2520exposure%2520context%2520is%250Apronominal%252C%2520definite%252C%2520short%252C%2520and%2520conforms%2520to%2520the%2520prototypical%2520animacy%250Aexpectations%2520of%2520the%2520exposure%2520dative.%2520These%2520patterns%2520are%2520characteristic%2520of%250Aharmonic%2520alignment%2520in%2520datives%252C%2520where%2520the%2520argument%2520with%2520features%2520ranking%2520higher%250Aon%2520the%2520discourse%2520prominence%2520scale%2520tends%2520to%2520precede%2520the%2520other.%2520This%2520gives%2520rise%250Ato%2520a%2520novel%2520hypothesis%2520that%2520CDG%2520is%2520facilitated%2520insofar%2520as%2520the%2520features%2520of%2520the%250Aexposure%2520context%2520--%2520in%2520particular%252C%2520its%2520first%2520postverbal%2520argument%2520--%2520are%250Aharmonically%2520aligned.%2520We%2520conclude%2520by%2520proposing%2520future%2520experiments%2520that%2520can%2520test%250Athis%2520hypothesis%2520in%2520children.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20novel%20experimental%20hypotheses%20from%20language%20models%3A%20A%20case%0A%20%20study%20on%20cross-dative%20generalization&entry.906535625=Kanishka%20Misra%20and%20Najoung%20Kim&entry.1292438233=%20%20Neural%20network%20language%20models%20%28LMs%29%20have%20been%20shown%20to%20successfully%20capture%0Acomplex%20linguistic%20knowledge.%20However%2C%20their%20utility%20for%20understanding%20language%0Aacquisition%20is%20still%20debated.%20We%20contribute%20to%20this%20debate%20by%20presenting%20a%20case%0Astudy%20where%20we%20use%20LMs%20as%20simulated%20learners%20to%20derive%20novel%20experimental%0Ahypotheses%20to%20be%20tested%20with%20humans.%20We%20apply%20this%20paradigm%20to%20study%0Across-dative%20generalization%20%28CDG%29%3A%20productive%20generalization%20of%20novel%20verbs%0Aacross%20dative%20constructions%20%28she%20pilked%20me%20the%20ball/she%20pilked%20the%20ball%20to%20me%29%0A--%20acquisition%20of%20which%20is%20known%20to%20involve%20a%20large%20space%20of%20contextual%0Afeatures%20--%20using%20LMs%20trained%20on%20child-directed%20speech.%20We%20specifically%20ask%3A%0A%22what%20properties%20of%20the%20training%20exposure%20facilitate%20a%20novel%20verb%27s%0Ageneralization%20to%20the%20%28unmodeled%29%20alternate%20construction%3F%22%20To%20answer%20this%2C%20we%0Asystematically%20vary%20the%20exposure%20context%20in%20which%20a%20novel%20dative%20verb%20occurs%20in%0Aterms%20of%20the%20properties%20of%20the%20theme%20and%20recipient%2C%20and%20then%20analyze%20the%20LMs%27%0Ausage%20of%20the%20novel%20verb%20in%20the%20unmodeled%20dative%20construction.%20We%20find%20LMs%20to%0Areplicate%20known%20patterns%20of%20children%27s%20CDG%2C%20as%20a%20precondition%20to%20exploring%0Anovel%20hypotheses.%20Subsequent%20simulations%20reveal%20a%20nuanced%20role%20of%20the%20features%0Aof%20the%20novel%20verbs%27%20exposure%20context%20on%20the%20LMs%27%20CDG.%20We%20find%20CDG%20to%20be%0Afacilitated%20when%20the%20first%20postverbal%20argument%20of%20the%20exposure%20context%20is%0Apronominal%2C%20definite%2C%20short%2C%20and%20conforms%20to%20the%20prototypical%20animacy%0Aexpectations%20of%20the%20exposure%20dative.%20These%20patterns%20are%20characteristic%20of%0Aharmonic%20alignment%20in%20datives%2C%20where%20the%20argument%20with%20features%20ranking%20higher%0Aon%20the%20discourse%20prominence%20scale%20tends%20to%20precede%20the%20other.%20This%20gives%20rise%0Ato%20a%20novel%20hypothesis%20that%20CDG%20is%20facilitated%20insofar%20as%20the%20features%20of%20the%0Aexposure%20context%20--%20in%20particular%2C%20its%20first%20postverbal%20argument%20--%20are%0Aharmonically%20aligned.%20We%20conclude%20by%20proposing%20future%20experiments%20that%20can%20test%0Athis%20hypothesis%20in%20children.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05086v1&entry.124074799=Read"},
{"title": "Physics-constrained convolutional neural networks for inverse problems\n  in spatiotemporal partial differential equations", "author": "Daniel Kelshaw and Luca Magri", "abstract": "  We propose a physics-constrained convolutional neural network (PC-CNN) to\nsolve two types of inverse problems in partial differential equations (PDEs),\nwhich are nonlinear and vary both in space and time. In the first inverse\nproblem, we are given data that is offset by spatially varying systematic error\n(i.e., the bias, also known as the epistemic uncertainty). The task is to\nuncover the true state, which is the solution of the PDE, from the biased data.\nIn the second inverse problem, we are given sparse information on the solution\nof a PDE. The task is to reconstruct the solution in space with\nhigh-resolution. First, we present the PC-CNN, which constrains the PDE with a\ntime-windowing scheme to handle sequential data. Second, we analyse the\nperformance of the PC-CNN for uncovering solutions from biased data. We analyse\nboth linear and nonlinear convection-diffusion equations, and the Navier-Stokes\nequations, which govern the spatiotemporally chaotic dynamics of turbulent\nflows. We find that the PC-CNN correctly recovers the true solution for a\nvariety of biases, which are parameterised as non-convex functions. Third, we\nanalyse the performance of the PC-CNN for reconstructing solutions from sparse\ninformation for the turbulent flow. We reconstruct the spatiotemporal chaotic\nsolution on a high-resolution grid from only < 1\\% of the information contained\nin it. For both tasks, we further analyse the Navier-Stokes solutions. We find\nthat the inferred solutions have a physical spectral energy content, whereas\ntraditional methods, such as interpolation, do not. This work opens\nopportunities for solving inverse problems with partial differential equations.\n", "link": "http://arxiv.org/abs/2401.10306v3", "date": "2024-08-09", "relevancy": 1.503, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5138}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4931}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-constrained%20convolutional%20neural%20networks%20for%20inverse%20problems%0A%20%20in%20spatiotemporal%20partial%20differential%20equations&body=Title%3A%20Physics-constrained%20convolutional%20neural%20networks%20for%20inverse%20problems%0A%20%20in%20spatiotemporal%20partial%20differential%20equations%0AAuthor%3A%20Daniel%20Kelshaw%20and%20Luca%20Magri%0AAbstract%3A%20%20%20We%20propose%20a%20physics-constrained%20convolutional%20neural%20network%20%28PC-CNN%29%20to%0Asolve%20two%20types%20of%20inverse%20problems%20in%20partial%20differential%20equations%20%28PDEs%29%2C%0Awhich%20are%20nonlinear%20and%20vary%20both%20in%20space%20and%20time.%20In%20the%20first%20inverse%0Aproblem%2C%20we%20are%20given%20data%20that%20is%20offset%20by%20spatially%20varying%20systematic%20error%0A%28i.e.%2C%20the%20bias%2C%20also%20known%20as%20the%20epistemic%20uncertainty%29.%20The%20task%20is%20to%0Auncover%20the%20true%20state%2C%20which%20is%20the%20solution%20of%20the%20PDE%2C%20from%20the%20biased%20data.%0AIn%20the%20second%20inverse%20problem%2C%20we%20are%20given%20sparse%20information%20on%20the%20solution%0Aof%20a%20PDE.%20The%20task%20is%20to%20reconstruct%20the%20solution%20in%20space%20with%0Ahigh-resolution.%20First%2C%20we%20present%20the%20PC-CNN%2C%20which%20constrains%20the%20PDE%20with%20a%0Atime-windowing%20scheme%20to%20handle%20sequential%20data.%20Second%2C%20we%20analyse%20the%0Aperformance%20of%20the%20PC-CNN%20for%20uncovering%20solutions%20from%20biased%20data.%20We%20analyse%0Aboth%20linear%20and%20nonlinear%20convection-diffusion%20equations%2C%20and%20the%20Navier-Stokes%0Aequations%2C%20which%20govern%20the%20spatiotemporally%20chaotic%20dynamics%20of%20turbulent%0Aflows.%20We%20find%20that%20the%20PC-CNN%20correctly%20recovers%20the%20true%20solution%20for%20a%0Avariety%20of%20biases%2C%20which%20are%20parameterised%20as%20non-convex%20functions.%20Third%2C%20we%0Aanalyse%20the%20performance%20of%20the%20PC-CNN%20for%20reconstructing%20solutions%20from%20sparse%0Ainformation%20for%20the%20turbulent%20flow.%20We%20reconstruct%20the%20spatiotemporal%20chaotic%0Asolution%20on%20a%20high-resolution%20grid%20from%20only%20%3C%201%5C%25%20of%20the%20information%20contained%0Ain%20it.%20For%20both%20tasks%2C%20we%20further%20analyse%20the%20Navier-Stokes%20solutions.%20We%20find%0Athat%20the%20inferred%20solutions%20have%20a%20physical%20spectral%20energy%20content%2C%20whereas%0Atraditional%20methods%2C%20such%20as%20interpolation%2C%20do%20not.%20This%20work%20opens%0Aopportunities%20for%20solving%20inverse%20problems%20with%20partial%20differential%20equations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10306v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-constrained%2520convolutional%2520neural%2520networks%2520for%2520inverse%2520problems%250A%2520%2520in%2520spatiotemporal%2520partial%2520differential%2520equations%26entry.906535625%3DDaniel%2520Kelshaw%2520and%2520Luca%2520Magri%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520physics-constrained%2520convolutional%2520neural%2520network%2520%2528PC-CNN%2529%2520to%250Asolve%2520two%2520types%2520of%2520inverse%2520problems%2520in%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%250Awhich%2520are%2520nonlinear%2520and%2520vary%2520both%2520in%2520space%2520and%2520time.%2520In%2520the%2520first%2520inverse%250Aproblem%252C%2520we%2520are%2520given%2520data%2520that%2520is%2520offset%2520by%2520spatially%2520varying%2520systematic%2520error%250A%2528i.e.%252C%2520the%2520bias%252C%2520also%2520known%2520as%2520the%2520epistemic%2520uncertainty%2529.%2520The%2520task%2520is%2520to%250Auncover%2520the%2520true%2520state%252C%2520which%2520is%2520the%2520solution%2520of%2520the%2520PDE%252C%2520from%2520the%2520biased%2520data.%250AIn%2520the%2520second%2520inverse%2520problem%252C%2520we%2520are%2520given%2520sparse%2520information%2520on%2520the%2520solution%250Aof%2520a%2520PDE.%2520The%2520task%2520is%2520to%2520reconstruct%2520the%2520solution%2520in%2520space%2520with%250Ahigh-resolution.%2520First%252C%2520we%2520present%2520the%2520PC-CNN%252C%2520which%2520constrains%2520the%2520PDE%2520with%2520a%250Atime-windowing%2520scheme%2520to%2520handle%2520sequential%2520data.%2520Second%252C%2520we%2520analyse%2520the%250Aperformance%2520of%2520the%2520PC-CNN%2520for%2520uncovering%2520solutions%2520from%2520biased%2520data.%2520We%2520analyse%250Aboth%2520linear%2520and%2520nonlinear%2520convection-diffusion%2520equations%252C%2520and%2520the%2520Navier-Stokes%250Aequations%252C%2520which%2520govern%2520the%2520spatiotemporally%2520chaotic%2520dynamics%2520of%2520turbulent%250Aflows.%2520We%2520find%2520that%2520the%2520PC-CNN%2520correctly%2520recovers%2520the%2520true%2520solution%2520for%2520a%250Avariety%2520of%2520biases%252C%2520which%2520are%2520parameterised%2520as%2520non-convex%2520functions.%2520Third%252C%2520we%250Aanalyse%2520the%2520performance%2520of%2520the%2520PC-CNN%2520for%2520reconstructing%2520solutions%2520from%2520sparse%250Ainformation%2520for%2520the%2520turbulent%2520flow.%2520We%2520reconstruct%2520the%2520spatiotemporal%2520chaotic%250Asolution%2520on%2520a%2520high-resolution%2520grid%2520from%2520only%2520%253C%25201%255C%2525%2520of%2520the%2520information%2520contained%250Ain%2520it.%2520For%2520both%2520tasks%252C%2520we%2520further%2520analyse%2520the%2520Navier-Stokes%2520solutions.%2520We%2520find%250Athat%2520the%2520inferred%2520solutions%2520have%2520a%2520physical%2520spectral%2520energy%2520content%252C%2520whereas%250Atraditional%2520methods%252C%2520such%2520as%2520interpolation%252C%2520do%2520not.%2520This%2520work%2520opens%250Aopportunities%2520for%2520solving%2520inverse%2520problems%2520with%2520partial%2520differential%2520equations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10306v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-constrained%20convolutional%20neural%20networks%20for%20inverse%20problems%0A%20%20in%20spatiotemporal%20partial%20differential%20equations&entry.906535625=Daniel%20Kelshaw%20and%20Luca%20Magri&entry.1292438233=%20%20We%20propose%20a%20physics-constrained%20convolutional%20neural%20network%20%28PC-CNN%29%20to%0Asolve%20two%20types%20of%20inverse%20problems%20in%20partial%20differential%20equations%20%28PDEs%29%2C%0Awhich%20are%20nonlinear%20and%20vary%20both%20in%20space%20and%20time.%20In%20the%20first%20inverse%0Aproblem%2C%20we%20are%20given%20data%20that%20is%20offset%20by%20spatially%20varying%20systematic%20error%0A%28i.e.%2C%20the%20bias%2C%20also%20known%20as%20the%20epistemic%20uncertainty%29.%20The%20task%20is%20to%0Auncover%20the%20true%20state%2C%20which%20is%20the%20solution%20of%20the%20PDE%2C%20from%20the%20biased%20data.%0AIn%20the%20second%20inverse%20problem%2C%20we%20are%20given%20sparse%20information%20on%20the%20solution%0Aof%20a%20PDE.%20The%20task%20is%20to%20reconstruct%20the%20solution%20in%20space%20with%0Ahigh-resolution.%20First%2C%20we%20present%20the%20PC-CNN%2C%20which%20constrains%20the%20PDE%20with%20a%0Atime-windowing%20scheme%20to%20handle%20sequential%20data.%20Second%2C%20we%20analyse%20the%0Aperformance%20of%20the%20PC-CNN%20for%20uncovering%20solutions%20from%20biased%20data.%20We%20analyse%0Aboth%20linear%20and%20nonlinear%20convection-diffusion%20equations%2C%20and%20the%20Navier-Stokes%0Aequations%2C%20which%20govern%20the%20spatiotemporally%20chaotic%20dynamics%20of%20turbulent%0Aflows.%20We%20find%20that%20the%20PC-CNN%20correctly%20recovers%20the%20true%20solution%20for%20a%0Avariety%20of%20biases%2C%20which%20are%20parameterised%20as%20non-convex%20functions.%20Third%2C%20we%0Aanalyse%20the%20performance%20of%20the%20PC-CNN%20for%20reconstructing%20solutions%20from%20sparse%0Ainformation%20for%20the%20turbulent%20flow.%20We%20reconstruct%20the%20spatiotemporal%20chaotic%0Asolution%20on%20a%20high-resolution%20grid%20from%20only%20%3C%201%5C%25%20of%20the%20information%20contained%0Ain%20it.%20For%20both%20tasks%2C%20we%20further%20analyse%20the%20Navier-Stokes%20solutions.%20We%20find%0Athat%20the%20inferred%20solutions%20have%20a%20physical%20spectral%20energy%20content%2C%20whereas%0Atraditional%20methods%2C%20such%20as%20interpolation%2C%20do%20not.%20This%20work%20opens%0Aopportunities%20for%20solving%20inverse%20problems%20with%20partial%20differential%20equations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10306v3&entry.124074799=Read"},
{"title": "Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed\n  Neural Operators", "author": "Chuwei Wang and Julius Berner and Zongyi Li and Di Zhou and Jiayun Wang and Jane Bae and Anima Anandkumar", "abstract": "  Accurately predicting the long-term behavior of chaotic systems is crucial\nfor various applications such as climate modeling. However, achieving such\npredictions typically requires iterative computations over a dense\nspatiotemporal grid to account for the unstable nature of chaotic systems,\nwhich is expensive and impractical in many real-world situations. An\nalternative approach to such a full-resolved simulation is using a coarse grid\nand then correcting its errors through a \\textit{closure model}, which\napproximates the overall information from fine scales not captured in the\ncoarse-grid simulation. Recently, ML approaches have been used for closure\nmodeling, but they typically require a large number of training samples from\nexpensive fully-resolved simulations (FRS). In this work, we prove an even more\nfundamental limitation, i.e., the standard approach to learning closure models\nsuffers from a large approximation error for generic problems, no matter how\nlarge the model is, and it stems from the non-uniqueness of the mapping. We\npropose an alternative end-to-end learning approach using a physics-informed\nneural operator (PINO) that overcomes this limitation by not using a closure\nmodel or a coarse-grid solver. We first train the PINO model on data from a\ncoarse-grid solver and then fine-tune it with (a small amount of) FRS and\nphysics-based losses on a fine grid. The discretization-free nature of neural\noperators means that they do not suffer from the restriction of a coarse grid\nthat closure models face, and they can provably approximate the long-term\nstatistics of chaotic systems. In our experiments, our PINO model achieves a\n120x speedup compared to FRS with a relative error $\\sim 5\\%$. In contrast, the\nclosure model coupled with a coarse-grid solver is $58$x slower than PINO while\nhaving a much higher error $\\sim205\\%$ when the closure model is trained on the\nsame FRS dataset.\n", "link": "http://arxiv.org/abs/2408.05177v1", "date": "2024-08-09", "relevancy": 1.4982, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5279}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4967}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Closure%20Models%3A%20Learning%20Chaotic-Systems%20via%20Physics-Informed%0A%20%20Neural%20Operators&body=Title%3A%20Beyond%20Closure%20Models%3A%20Learning%20Chaotic-Systems%20via%20Physics-Informed%0A%20%20Neural%20Operators%0AAuthor%3A%20Chuwei%20Wang%20and%20Julius%20Berner%20and%20Zongyi%20Li%20and%20Di%20Zhou%20and%20Jiayun%20Wang%20and%20Jane%20Bae%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20long-term%20behavior%20of%20chaotic%20systems%20is%20crucial%0Afor%20various%20applications%20such%20as%20climate%20modeling.%20However%2C%20achieving%20such%0Apredictions%20typically%20requires%20iterative%20computations%20over%20a%20dense%0Aspatiotemporal%20grid%20to%20account%20for%20the%20unstable%20nature%20of%20chaotic%20systems%2C%0Awhich%20is%20expensive%20and%20impractical%20in%20many%20real-world%20situations.%20An%0Aalternative%20approach%20to%20such%20a%20full-resolved%20simulation%20is%20using%20a%20coarse%20grid%0Aand%20then%20correcting%20its%20errors%20through%20a%20%5Ctextit%7Bclosure%20model%7D%2C%20which%0Aapproximates%20the%20overall%20information%20from%20fine%20scales%20not%20captured%20in%20the%0Acoarse-grid%20simulation.%20Recently%2C%20ML%20approaches%20have%20been%20used%20for%20closure%0Amodeling%2C%20but%20they%20typically%20require%20a%20large%20number%20of%20training%20samples%20from%0Aexpensive%20fully-resolved%20simulations%20%28FRS%29.%20In%20this%20work%2C%20we%20prove%20an%20even%20more%0Afundamental%20limitation%2C%20i.e.%2C%20the%20standard%20approach%20to%20learning%20closure%20models%0Asuffers%20from%20a%20large%20approximation%20error%20for%20generic%20problems%2C%20no%20matter%20how%0Alarge%20the%20model%20is%2C%20and%20it%20stems%20from%20the%20non-uniqueness%20of%20the%20mapping.%20We%0Apropose%20an%20alternative%20end-to-end%20learning%20approach%20using%20a%20physics-informed%0Aneural%20operator%20%28PINO%29%20that%20overcomes%20this%20limitation%20by%20not%20using%20a%20closure%0Amodel%20or%20a%20coarse-grid%20solver.%20We%20first%20train%20the%20PINO%20model%20on%20data%20from%20a%0Acoarse-grid%20solver%20and%20then%20fine-tune%20it%20with%20%28a%20small%20amount%20of%29%20FRS%20and%0Aphysics-based%20losses%20on%20a%20fine%20grid.%20The%20discretization-free%20nature%20of%20neural%0Aoperators%20means%20that%20they%20do%20not%20suffer%20from%20the%20restriction%20of%20a%20coarse%20grid%0Athat%20closure%20models%20face%2C%20and%20they%20can%20provably%20approximate%20the%20long-term%0Astatistics%20of%20chaotic%20systems.%20In%20our%20experiments%2C%20our%20PINO%20model%20achieves%20a%0A120x%20speedup%20compared%20to%20FRS%20with%20a%20relative%20error%20%24%5Csim%205%5C%25%24.%20In%20contrast%2C%20the%0Aclosure%20model%20coupled%20with%20a%20coarse-grid%20solver%20is%20%2458%24x%20slower%20than%20PINO%20while%0Ahaving%20a%20much%20higher%20error%20%24%5Csim205%5C%25%24%20when%20the%20closure%20model%20is%20trained%20on%20the%0Asame%20FRS%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Closure%2520Models%253A%2520Learning%2520Chaotic-Systems%2520via%2520Physics-Informed%250A%2520%2520Neural%2520Operators%26entry.906535625%3DChuwei%2520Wang%2520and%2520Julius%2520Berner%2520and%2520Zongyi%2520Li%2520and%2520Di%2520Zhou%2520and%2520Jiayun%2520Wang%2520and%2520Jane%2520Bae%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520long-term%2520behavior%2520of%2520chaotic%2520systems%2520is%2520crucial%250Afor%2520various%2520applications%2520such%2520as%2520climate%2520modeling.%2520However%252C%2520achieving%2520such%250Apredictions%2520typically%2520requires%2520iterative%2520computations%2520over%2520a%2520dense%250Aspatiotemporal%2520grid%2520to%2520account%2520for%2520the%2520unstable%2520nature%2520of%2520chaotic%2520systems%252C%250Awhich%2520is%2520expensive%2520and%2520impractical%2520in%2520many%2520real-world%2520situations.%2520An%250Aalternative%2520approach%2520to%2520such%2520a%2520full-resolved%2520simulation%2520is%2520using%2520a%2520coarse%2520grid%250Aand%2520then%2520correcting%2520its%2520errors%2520through%2520a%2520%255Ctextit%257Bclosure%2520model%257D%252C%2520which%250Aapproximates%2520the%2520overall%2520information%2520from%2520fine%2520scales%2520not%2520captured%2520in%2520the%250Acoarse-grid%2520simulation.%2520Recently%252C%2520ML%2520approaches%2520have%2520been%2520used%2520for%2520closure%250Amodeling%252C%2520but%2520they%2520typically%2520require%2520a%2520large%2520number%2520of%2520training%2520samples%2520from%250Aexpensive%2520fully-resolved%2520simulations%2520%2528FRS%2529.%2520In%2520this%2520work%252C%2520we%2520prove%2520an%2520even%2520more%250Afundamental%2520limitation%252C%2520i.e.%252C%2520the%2520standard%2520approach%2520to%2520learning%2520closure%2520models%250Asuffers%2520from%2520a%2520large%2520approximation%2520error%2520for%2520generic%2520problems%252C%2520no%2520matter%2520how%250Alarge%2520the%2520model%2520is%252C%2520and%2520it%2520stems%2520from%2520the%2520non-uniqueness%2520of%2520the%2520mapping.%2520We%250Apropose%2520an%2520alternative%2520end-to-end%2520learning%2520approach%2520using%2520a%2520physics-informed%250Aneural%2520operator%2520%2528PINO%2529%2520that%2520overcomes%2520this%2520limitation%2520by%2520not%2520using%2520a%2520closure%250Amodel%2520or%2520a%2520coarse-grid%2520solver.%2520We%2520first%2520train%2520the%2520PINO%2520model%2520on%2520data%2520from%2520a%250Acoarse-grid%2520solver%2520and%2520then%2520fine-tune%2520it%2520with%2520%2528a%2520small%2520amount%2520of%2529%2520FRS%2520and%250Aphysics-based%2520losses%2520on%2520a%2520fine%2520grid.%2520The%2520discretization-free%2520nature%2520of%2520neural%250Aoperators%2520means%2520that%2520they%2520do%2520not%2520suffer%2520from%2520the%2520restriction%2520of%2520a%2520coarse%2520grid%250Athat%2520closure%2520models%2520face%252C%2520and%2520they%2520can%2520provably%2520approximate%2520the%2520long-term%250Astatistics%2520of%2520chaotic%2520systems.%2520In%2520our%2520experiments%252C%2520our%2520PINO%2520model%2520achieves%2520a%250A120x%2520speedup%2520compared%2520to%2520FRS%2520with%2520a%2520relative%2520error%2520%2524%255Csim%25205%255C%2525%2524.%2520In%2520contrast%252C%2520the%250Aclosure%2520model%2520coupled%2520with%2520a%2520coarse-grid%2520solver%2520is%2520%252458%2524x%2520slower%2520than%2520PINO%2520while%250Ahaving%2520a%2520much%2520higher%2520error%2520%2524%255Csim205%255C%2525%2524%2520when%2520the%2520closure%2520model%2520is%2520trained%2520on%2520the%250Asame%2520FRS%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Closure%20Models%3A%20Learning%20Chaotic-Systems%20via%20Physics-Informed%0A%20%20Neural%20Operators&entry.906535625=Chuwei%20Wang%20and%20Julius%20Berner%20and%20Zongyi%20Li%20and%20Di%20Zhou%20and%20Jiayun%20Wang%20and%20Jane%20Bae%20and%20Anima%20Anandkumar&entry.1292438233=%20%20Accurately%20predicting%20the%20long-term%20behavior%20of%20chaotic%20systems%20is%20crucial%0Afor%20various%20applications%20such%20as%20climate%20modeling.%20However%2C%20achieving%20such%0Apredictions%20typically%20requires%20iterative%20computations%20over%20a%20dense%0Aspatiotemporal%20grid%20to%20account%20for%20the%20unstable%20nature%20of%20chaotic%20systems%2C%0Awhich%20is%20expensive%20and%20impractical%20in%20many%20real-world%20situations.%20An%0Aalternative%20approach%20to%20such%20a%20full-resolved%20simulation%20is%20using%20a%20coarse%20grid%0Aand%20then%20correcting%20its%20errors%20through%20a%20%5Ctextit%7Bclosure%20model%7D%2C%20which%0Aapproximates%20the%20overall%20information%20from%20fine%20scales%20not%20captured%20in%20the%0Acoarse-grid%20simulation.%20Recently%2C%20ML%20approaches%20have%20been%20used%20for%20closure%0Amodeling%2C%20but%20they%20typically%20require%20a%20large%20number%20of%20training%20samples%20from%0Aexpensive%20fully-resolved%20simulations%20%28FRS%29.%20In%20this%20work%2C%20we%20prove%20an%20even%20more%0Afundamental%20limitation%2C%20i.e.%2C%20the%20standard%20approach%20to%20learning%20closure%20models%0Asuffers%20from%20a%20large%20approximation%20error%20for%20generic%20problems%2C%20no%20matter%20how%0Alarge%20the%20model%20is%2C%20and%20it%20stems%20from%20the%20non-uniqueness%20of%20the%20mapping.%20We%0Apropose%20an%20alternative%20end-to-end%20learning%20approach%20using%20a%20physics-informed%0Aneural%20operator%20%28PINO%29%20that%20overcomes%20this%20limitation%20by%20not%20using%20a%20closure%0Amodel%20or%20a%20coarse-grid%20solver.%20We%20first%20train%20the%20PINO%20model%20on%20data%20from%20a%0Acoarse-grid%20solver%20and%20then%20fine-tune%20it%20with%20%28a%20small%20amount%20of%29%20FRS%20and%0Aphysics-based%20losses%20on%20a%20fine%20grid.%20The%20discretization-free%20nature%20of%20neural%0Aoperators%20means%20that%20they%20do%20not%20suffer%20from%20the%20restriction%20of%20a%20coarse%20grid%0Athat%20closure%20models%20face%2C%20and%20they%20can%20provably%20approximate%20the%20long-term%0Astatistics%20of%20chaotic%20systems.%20In%20our%20experiments%2C%20our%20PINO%20model%20achieves%20a%0A120x%20speedup%20compared%20to%20FRS%20with%20a%20relative%20error%20%24%5Csim%205%5C%25%24.%20In%20contrast%2C%20the%0Aclosure%20model%20coupled%20with%20a%20coarse-grid%20solver%20is%20%2458%24x%20slower%20than%20PINO%20while%0Ahaving%20a%20much%20higher%20error%20%24%5Csim205%5C%25%24%20when%20the%20closure%20model%20is%20trained%20on%20the%0Asame%20FRS%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05177v1&entry.124074799=Read"},
{"title": "Exploring Capability-Based Control Distributions of Human-Robot Teams\n  Through Capability Deltas: Formalization and Implications", "author": "Nils Mandischer and Marcel Usai and Frank Flemisch and Lars Mikelsons", "abstract": "  The implicit assumption that human and autonomous agents have certain\ncapabilities is omnipresent in modern teaming concepts. However, none formalize\nthese capabilities in a flexible and quantifiable way. In this paper, we\npropose Capability Deltas, which establish a quantifiable source to craft\nautonomous assistance systems in which one agent takes the leader and the other\nthe supporter role. We deduct the quantification of human capabilities based on\nan established assessment and documentation procedure from occupational\ninclusion of people with disabilities. This allows us to quantify the delta, or\ngap, between a team's current capability and a requirement established by a\nwork process. The concept is then extended to the multi-dimensional capability\nspace, which then allows to formalize compensation behavior and assess required\nactions by the autonomous agent.\n", "link": "http://arxiv.org/abs/2408.05069v1", "date": "2024-08-09", "relevancy": 1.4707, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5066}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4868}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Capability-Based%20Control%20Distributions%20of%20Human-Robot%20Teams%0A%20%20Through%20Capability%20Deltas%3A%20Formalization%20and%20Implications&body=Title%3A%20Exploring%20Capability-Based%20Control%20Distributions%20of%20Human-Robot%20Teams%0A%20%20Through%20Capability%20Deltas%3A%20Formalization%20and%20Implications%0AAuthor%3A%20Nils%20Mandischer%20and%20Marcel%20Usai%20and%20Frank%20Flemisch%20and%20Lars%20Mikelsons%0AAbstract%3A%20%20%20The%20implicit%20assumption%20that%20human%20and%20autonomous%20agents%20have%20certain%0Acapabilities%20is%20omnipresent%20in%20modern%20teaming%20concepts.%20However%2C%20none%20formalize%0Athese%20capabilities%20in%20a%20flexible%20and%20quantifiable%20way.%20In%20this%20paper%2C%20we%0Apropose%20Capability%20Deltas%2C%20which%20establish%20a%20quantifiable%20source%20to%20craft%0Aautonomous%20assistance%20systems%20in%20which%20one%20agent%20takes%20the%20leader%20and%20the%20other%0Athe%20supporter%20role.%20We%20deduct%20the%20quantification%20of%20human%20capabilities%20based%20on%0Aan%20established%20assessment%20and%20documentation%20procedure%20from%20occupational%0Ainclusion%20of%20people%20with%20disabilities.%20This%20allows%20us%20to%20quantify%20the%20delta%2C%20or%0Agap%2C%20between%20a%20team%27s%20current%20capability%20and%20a%20requirement%20established%20by%20a%0Awork%20process.%20The%20concept%20is%20then%20extended%20to%20the%20multi-dimensional%20capability%0Aspace%2C%20which%20then%20allows%20to%20formalize%20compensation%20behavior%20and%20assess%20required%0Aactions%20by%20the%20autonomous%20agent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Capability-Based%2520Control%2520Distributions%2520of%2520Human-Robot%2520Teams%250A%2520%2520Through%2520Capability%2520Deltas%253A%2520Formalization%2520and%2520Implications%26entry.906535625%3DNils%2520Mandischer%2520and%2520Marcel%2520Usai%2520and%2520Frank%2520Flemisch%2520and%2520Lars%2520Mikelsons%26entry.1292438233%3D%2520%2520The%2520implicit%2520assumption%2520that%2520human%2520and%2520autonomous%2520agents%2520have%2520certain%250Acapabilities%2520is%2520omnipresent%2520in%2520modern%2520teaming%2520concepts.%2520However%252C%2520none%2520formalize%250Athese%2520capabilities%2520in%2520a%2520flexible%2520and%2520quantifiable%2520way.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Capability%2520Deltas%252C%2520which%2520establish%2520a%2520quantifiable%2520source%2520to%2520craft%250Aautonomous%2520assistance%2520systems%2520in%2520which%2520one%2520agent%2520takes%2520the%2520leader%2520and%2520the%2520other%250Athe%2520supporter%2520role.%2520We%2520deduct%2520the%2520quantification%2520of%2520human%2520capabilities%2520based%2520on%250Aan%2520established%2520assessment%2520and%2520documentation%2520procedure%2520from%2520occupational%250Ainclusion%2520of%2520people%2520with%2520disabilities.%2520This%2520allows%2520us%2520to%2520quantify%2520the%2520delta%252C%2520or%250Agap%252C%2520between%2520a%2520team%2527s%2520current%2520capability%2520and%2520a%2520requirement%2520established%2520by%2520a%250Awork%2520process.%2520The%2520concept%2520is%2520then%2520extended%2520to%2520the%2520multi-dimensional%2520capability%250Aspace%252C%2520which%2520then%2520allows%2520to%2520formalize%2520compensation%2520behavior%2520and%2520assess%2520required%250Aactions%2520by%2520the%2520autonomous%2520agent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Capability-Based%20Control%20Distributions%20of%20Human-Robot%20Teams%0A%20%20Through%20Capability%20Deltas%3A%20Formalization%20and%20Implications&entry.906535625=Nils%20Mandischer%20and%20Marcel%20Usai%20and%20Frank%20Flemisch%20and%20Lars%20Mikelsons&entry.1292438233=%20%20The%20implicit%20assumption%20that%20human%20and%20autonomous%20agents%20have%20certain%0Acapabilities%20is%20omnipresent%20in%20modern%20teaming%20concepts.%20However%2C%20none%20formalize%0Athese%20capabilities%20in%20a%20flexible%20and%20quantifiable%20way.%20In%20this%20paper%2C%20we%0Apropose%20Capability%20Deltas%2C%20which%20establish%20a%20quantifiable%20source%20to%20craft%0Aautonomous%20assistance%20systems%20in%20which%20one%20agent%20takes%20the%20leader%20and%20the%20other%0Athe%20supporter%20role.%20We%20deduct%20the%20quantification%20of%20human%20capabilities%20based%20on%0Aan%20established%20assessment%20and%20documentation%20procedure%20from%20occupational%0Ainclusion%20of%20people%20with%20disabilities.%20This%20allows%20us%20to%20quantify%20the%20delta%2C%20or%0Agap%2C%20between%20a%20team%27s%20current%20capability%20and%20a%20requirement%20established%20by%20a%0Awork%20process.%20The%20concept%20is%20then%20extended%20to%20the%20multi-dimensional%20capability%0Aspace%2C%20which%20then%20allows%20to%20formalize%20compensation%20behavior%20and%20assess%20required%0Aactions%20by%20the%20autonomous%20agent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05069v1&entry.124074799=Read"},
{"title": "Cost-Effective Hallucination Detection for LLMs", "author": "Simon Valentin and Jinmiao Fu and Gianluca Detommaso and Shaoyuan Xu and Giovanni Zappella and Bryan Wang", "abstract": "  Large language models (LLMs) can be prone to hallucinations - generating\nunreliable outputs that are unfaithful to their inputs, external facts or\ninternally inconsistent. In this work, we address several challenges for\npost-hoc hallucination detection in production settings. Our pipeline for\nhallucination detection entails: first, producing a confidence score\nrepresenting the likelihood that a generated answer is a hallucination; second,\ncalibrating the score conditional on attributes of the inputs and candidate\nresponse; finally, performing detection by thresholding the calibrated score.\nWe benchmark a variety of state-of-the-art scoring methods on different\ndatasets, encompassing question answering, fact checking, and summarization\ntasks. We employ diverse LLMs to ensure a comprehensive assessment of\nperformance. We show that calibrating individual scoring methods is critical\nfor ensuring risk-aware downstream decision making. Based on findings that no\nindividual score performs best in all situations, we propose a multi-scoring\nframework, which combines different scores and achieves top performance across\nall datasets. We further introduce cost-effective multi-scoring, which can\nmatch or even outperform more expensive detection methods, while significantly\nreducing computational overhead.\n", "link": "http://arxiv.org/abs/2407.21424v2", "date": "2024-08-09", "relevancy": 1.4416, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4739}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-Effective%20Hallucination%20Detection%20for%20LLMs&body=Title%3A%20Cost-Effective%20Hallucination%20Detection%20for%20LLMs%0AAuthor%3A%20Simon%20Valentin%20and%20Jinmiao%20Fu%20and%20Gianluca%20Detommaso%20and%20Shaoyuan%20Xu%20and%20Giovanni%20Zappella%20and%20Bryan%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20be%20prone%20to%20hallucinations%20-%20generating%0Aunreliable%20outputs%20that%20are%20unfaithful%20to%20their%20inputs%2C%20external%20facts%20or%0Ainternally%20inconsistent.%20In%20this%20work%2C%20we%20address%20several%20challenges%20for%0Apost-hoc%20hallucination%20detection%20in%20production%20settings.%20Our%20pipeline%20for%0Ahallucination%20detection%20entails%3A%20first%2C%20producing%20a%20confidence%20score%0Arepresenting%20the%20likelihood%20that%20a%20generated%20answer%20is%20a%20hallucination%3B%20second%2C%0Acalibrating%20the%20score%20conditional%20on%20attributes%20of%20the%20inputs%20and%20candidate%0Aresponse%3B%20finally%2C%20performing%20detection%20by%20thresholding%20the%20calibrated%20score.%0AWe%20benchmark%20a%20variety%20of%20state-of-the-art%20scoring%20methods%20on%20different%0Adatasets%2C%20encompassing%20question%20answering%2C%20fact%20checking%2C%20and%20summarization%0Atasks.%20We%20employ%20diverse%20LLMs%20to%20ensure%20a%20comprehensive%20assessment%20of%0Aperformance.%20We%20show%20that%20calibrating%20individual%20scoring%20methods%20is%20critical%0Afor%20ensuring%20risk-aware%20downstream%20decision%20making.%20Based%20on%20findings%20that%20no%0Aindividual%20score%20performs%20best%20in%20all%20situations%2C%20we%20propose%20a%20multi-scoring%0Aframework%2C%20which%20combines%20different%20scores%20and%20achieves%20top%20performance%20across%0Aall%20datasets.%20We%20further%20introduce%20cost-effective%20multi-scoring%2C%20which%20can%0Amatch%20or%20even%20outperform%20more%20expensive%20detection%20methods%2C%20while%20significantly%0Areducing%20computational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21424v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-Effective%2520Hallucination%2520Detection%2520for%2520LLMs%26entry.906535625%3DSimon%2520Valentin%2520and%2520Jinmiao%2520Fu%2520and%2520Gianluca%2520Detommaso%2520and%2520Shaoyuan%2520Xu%2520and%2520Giovanni%2520Zappella%2520and%2520Bryan%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520be%2520prone%2520to%2520hallucinations%2520-%2520generating%250Aunreliable%2520outputs%2520that%2520are%2520unfaithful%2520to%2520their%2520inputs%252C%2520external%2520facts%2520or%250Ainternally%2520inconsistent.%2520In%2520this%2520work%252C%2520we%2520address%2520several%2520challenges%2520for%250Apost-hoc%2520hallucination%2520detection%2520in%2520production%2520settings.%2520Our%2520pipeline%2520for%250Ahallucination%2520detection%2520entails%253A%2520first%252C%2520producing%2520a%2520confidence%2520score%250Arepresenting%2520the%2520likelihood%2520that%2520a%2520generated%2520answer%2520is%2520a%2520hallucination%253B%2520second%252C%250Acalibrating%2520the%2520score%2520conditional%2520on%2520attributes%2520of%2520the%2520inputs%2520and%2520candidate%250Aresponse%253B%2520finally%252C%2520performing%2520detection%2520by%2520thresholding%2520the%2520calibrated%2520score.%250AWe%2520benchmark%2520a%2520variety%2520of%2520state-of-the-art%2520scoring%2520methods%2520on%2520different%250Adatasets%252C%2520encompassing%2520question%2520answering%252C%2520fact%2520checking%252C%2520and%2520summarization%250Atasks.%2520We%2520employ%2520diverse%2520LLMs%2520to%2520ensure%2520a%2520comprehensive%2520assessment%2520of%250Aperformance.%2520We%2520show%2520that%2520calibrating%2520individual%2520scoring%2520methods%2520is%2520critical%250Afor%2520ensuring%2520risk-aware%2520downstream%2520decision%2520making.%2520Based%2520on%2520findings%2520that%2520no%250Aindividual%2520score%2520performs%2520best%2520in%2520all%2520situations%252C%2520we%2520propose%2520a%2520multi-scoring%250Aframework%252C%2520which%2520combines%2520different%2520scores%2520and%2520achieves%2520top%2520performance%2520across%250Aall%2520datasets.%2520We%2520further%2520introduce%2520cost-effective%2520multi-scoring%252C%2520which%2520can%250Amatch%2520or%2520even%2520outperform%2520more%2520expensive%2520detection%2520methods%252C%2520while%2520significantly%250Areducing%2520computational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21424v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-Effective%20Hallucination%20Detection%20for%20LLMs&entry.906535625=Simon%20Valentin%20and%20Jinmiao%20Fu%20and%20Gianluca%20Detommaso%20and%20Shaoyuan%20Xu%20and%20Giovanni%20Zappella%20and%20Bryan%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20be%20prone%20to%20hallucinations%20-%20generating%0Aunreliable%20outputs%20that%20are%20unfaithful%20to%20their%20inputs%2C%20external%20facts%20or%0Ainternally%20inconsistent.%20In%20this%20work%2C%20we%20address%20several%20challenges%20for%0Apost-hoc%20hallucination%20detection%20in%20production%20settings.%20Our%20pipeline%20for%0Ahallucination%20detection%20entails%3A%20first%2C%20producing%20a%20confidence%20score%0Arepresenting%20the%20likelihood%20that%20a%20generated%20answer%20is%20a%20hallucination%3B%20second%2C%0Acalibrating%20the%20score%20conditional%20on%20attributes%20of%20the%20inputs%20and%20candidate%0Aresponse%3B%20finally%2C%20performing%20detection%20by%20thresholding%20the%20calibrated%20score.%0AWe%20benchmark%20a%20variety%20of%20state-of-the-art%20scoring%20methods%20on%20different%0Adatasets%2C%20encompassing%20question%20answering%2C%20fact%20checking%2C%20and%20summarization%0Atasks.%20We%20employ%20diverse%20LLMs%20to%20ensure%20a%20comprehensive%20assessment%20of%0Aperformance.%20We%20show%20that%20calibrating%20individual%20scoring%20methods%20is%20critical%0Afor%20ensuring%20risk-aware%20downstream%20decision%20making.%20Based%20on%20findings%20that%20no%0Aindividual%20score%20performs%20best%20in%20all%20situations%2C%20we%20propose%20a%20multi-scoring%0Aframework%2C%20which%20combines%20different%20scores%20and%20achieves%20top%20performance%20across%0Aall%20datasets.%20We%20further%20introduce%20cost-effective%20multi-scoring%2C%20which%20can%0Amatch%20or%20even%20outperform%20more%20expensive%20detection%20methods%2C%20while%20significantly%0Areducing%20computational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21424v2&entry.124074799=Read"},
{"title": "Overcoming the Limitations of Layer Synchronization in Spiking Neural\n  Networks", "author": "Roel Koopman and Amirreza Yousefzadeh and Mahyar Shahsavari and Guangzhi Tang and Manolis Sifalakis", "abstract": "  Currently, neural-network processing in machine learning applications relies\non layer synchronization, whereby neurons in a layer aggregate incoming\ncurrents from all neurons in the preceding layer, before evaluating their\nactivation function. This is practiced even in artificial Spiking Neural\nNetworks (SNNs), which are touted as consistent with neurobiology, in spite of\nprocessing in the brain being, in fact asynchronous. A truly asynchronous\nsystem however would allow all neurons to evaluate concurrently their threshold\nand emit spikes upon receiving any presynaptic current. Omitting layer\nsynchronization is potentially beneficial, for latency and energy efficiency,\nbut asynchronous execution of models previously trained with layer\nsynchronization may entail a mismatch in network dynamics and performance. We\npresent a study that documents and quantifies this problem in three datasets on\nour simulation environment that implements network asynchrony, and we show that\nmodels trained with layer synchronization either perform sub-optimally in\nabsence of the synchronization, or they will fail to benefit from any energy\nand latency reduction, when such a mechanism is in place. We then \"make ends\nmeet\" and address the problem with unlayered backprop, a novel\nbackpropagation-based training method, for learning models suitable for\nasynchronous processing. We train with it models that use different neuron\nexecution scheduling strategies, and we show that although their neurons are\nmore reactive, these models consistently exhibit lower overall spike density\n(up to 50%), reach a correct decision faster (up to 2x) without integrating all\nspikes, and achieve superior accuracy (up to 10% higher). Our findings suggest\nthat asynchronous event-based (neuromorphic) AI computing is indeed more\nefficient, but we need to seriously rethink how we train our SNN models, to\nbenefit from it.\n", "link": "http://arxiv.org/abs/2408.05098v1", "date": "2024-08-09", "relevancy": 1.4406, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4957}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4765}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20the%20Limitations%20of%20Layer%20Synchronization%20in%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20Overcoming%20the%20Limitations%20of%20Layer%20Synchronization%20in%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Roel%20Koopman%20and%20Amirreza%20Yousefzadeh%20and%20Mahyar%20Shahsavari%20and%20Guangzhi%20Tang%20and%20Manolis%20Sifalakis%0AAbstract%3A%20%20%20Currently%2C%20neural-network%20processing%20in%20machine%20learning%20applications%20relies%0Aon%20layer%20synchronization%2C%20whereby%20neurons%20in%20a%20layer%20aggregate%20incoming%0Acurrents%20from%20all%20neurons%20in%20the%20preceding%20layer%2C%20before%20evaluating%20their%0Aactivation%20function.%20This%20is%20practiced%20even%20in%20artificial%20Spiking%20Neural%0ANetworks%20%28SNNs%29%2C%20which%20are%20touted%20as%20consistent%20with%20neurobiology%2C%20in%20spite%20of%0Aprocessing%20in%20the%20brain%20being%2C%20in%20fact%20asynchronous.%20A%20truly%20asynchronous%0Asystem%20however%20would%20allow%20all%20neurons%20to%20evaluate%20concurrently%20their%20threshold%0Aand%20emit%20spikes%20upon%20receiving%20any%20presynaptic%20current.%20Omitting%20layer%0Asynchronization%20is%20potentially%20beneficial%2C%20for%20latency%20and%20energy%20efficiency%2C%0Abut%20asynchronous%20execution%20of%20models%20previously%20trained%20with%20layer%0Asynchronization%20may%20entail%20a%20mismatch%20in%20network%20dynamics%20and%20performance.%20We%0Apresent%20a%20study%20that%20documents%20and%20quantifies%20this%20problem%20in%20three%20datasets%20on%0Aour%20simulation%20environment%20that%20implements%20network%20asynchrony%2C%20and%20we%20show%20that%0Amodels%20trained%20with%20layer%20synchronization%20either%20perform%20sub-optimally%20in%0Aabsence%20of%20the%20synchronization%2C%20or%20they%20will%20fail%20to%20benefit%20from%20any%20energy%0Aand%20latency%20reduction%2C%20when%20such%20a%20mechanism%20is%20in%20place.%20We%20then%20%22make%20ends%0Ameet%22%20and%20address%20the%20problem%20with%20unlayered%20backprop%2C%20a%20novel%0Abackpropagation-based%20training%20method%2C%20for%20learning%20models%20suitable%20for%0Aasynchronous%20processing.%20We%20train%20with%20it%20models%20that%20use%20different%20neuron%0Aexecution%20scheduling%20strategies%2C%20and%20we%20show%20that%20although%20their%20neurons%20are%0Amore%20reactive%2C%20these%20models%20consistently%20exhibit%20lower%20overall%20spike%20density%0A%28up%20to%2050%25%29%2C%20reach%20a%20correct%20decision%20faster%20%28up%20to%202x%29%20without%20integrating%20all%0Aspikes%2C%20and%20achieve%20superior%20accuracy%20%28up%20to%2010%25%20higher%29.%20Our%20findings%20suggest%0Athat%20asynchronous%20event-based%20%28neuromorphic%29%20AI%20computing%20is%20indeed%20more%0Aefficient%2C%20but%20we%20need%20to%20seriously%20rethink%20how%20we%20train%20our%20SNN%20models%2C%20to%0Abenefit%20from%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520the%2520Limitations%2520of%2520Layer%2520Synchronization%2520in%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DRoel%2520Koopman%2520and%2520Amirreza%2520Yousefzadeh%2520and%2520Mahyar%2520Shahsavari%2520and%2520Guangzhi%2520Tang%2520and%2520Manolis%2520Sifalakis%26entry.1292438233%3D%2520%2520Currently%252C%2520neural-network%2520processing%2520in%2520machine%2520learning%2520applications%2520relies%250Aon%2520layer%2520synchronization%252C%2520whereby%2520neurons%2520in%2520a%2520layer%2520aggregate%2520incoming%250Acurrents%2520from%2520all%2520neurons%2520in%2520the%2520preceding%2520layer%252C%2520before%2520evaluating%2520their%250Aactivation%2520function.%2520This%2520is%2520practiced%2520even%2520in%2520artificial%2520Spiking%2520Neural%250ANetworks%2520%2528SNNs%2529%252C%2520which%2520are%2520touted%2520as%2520consistent%2520with%2520neurobiology%252C%2520in%2520spite%2520of%250Aprocessing%2520in%2520the%2520brain%2520being%252C%2520in%2520fact%2520asynchronous.%2520A%2520truly%2520asynchronous%250Asystem%2520however%2520would%2520allow%2520all%2520neurons%2520to%2520evaluate%2520concurrently%2520their%2520threshold%250Aand%2520emit%2520spikes%2520upon%2520receiving%2520any%2520presynaptic%2520current.%2520Omitting%2520layer%250Asynchronization%2520is%2520potentially%2520beneficial%252C%2520for%2520latency%2520and%2520energy%2520efficiency%252C%250Abut%2520asynchronous%2520execution%2520of%2520models%2520previously%2520trained%2520with%2520layer%250Asynchronization%2520may%2520entail%2520a%2520mismatch%2520in%2520network%2520dynamics%2520and%2520performance.%2520We%250Apresent%2520a%2520study%2520that%2520documents%2520and%2520quantifies%2520this%2520problem%2520in%2520three%2520datasets%2520on%250Aour%2520simulation%2520environment%2520that%2520implements%2520network%2520asynchrony%252C%2520and%2520we%2520show%2520that%250Amodels%2520trained%2520with%2520layer%2520synchronization%2520either%2520perform%2520sub-optimally%2520in%250Aabsence%2520of%2520the%2520synchronization%252C%2520or%2520they%2520will%2520fail%2520to%2520benefit%2520from%2520any%2520energy%250Aand%2520latency%2520reduction%252C%2520when%2520such%2520a%2520mechanism%2520is%2520in%2520place.%2520We%2520then%2520%2522make%2520ends%250Ameet%2522%2520and%2520address%2520the%2520problem%2520with%2520unlayered%2520backprop%252C%2520a%2520novel%250Abackpropagation-based%2520training%2520method%252C%2520for%2520learning%2520models%2520suitable%2520for%250Aasynchronous%2520processing.%2520We%2520train%2520with%2520it%2520models%2520that%2520use%2520different%2520neuron%250Aexecution%2520scheduling%2520strategies%252C%2520and%2520we%2520show%2520that%2520although%2520their%2520neurons%2520are%250Amore%2520reactive%252C%2520these%2520models%2520consistently%2520exhibit%2520lower%2520overall%2520spike%2520density%250A%2528up%2520to%252050%2525%2529%252C%2520reach%2520a%2520correct%2520decision%2520faster%2520%2528up%2520to%25202x%2529%2520without%2520integrating%2520all%250Aspikes%252C%2520and%2520achieve%2520superior%2520accuracy%2520%2528up%2520to%252010%2525%2520higher%2529.%2520Our%2520findings%2520suggest%250Athat%2520asynchronous%2520event-based%2520%2528neuromorphic%2529%2520AI%2520computing%2520is%2520indeed%2520more%250Aefficient%252C%2520but%2520we%2520need%2520to%2520seriously%2520rethink%2520how%2520we%2520train%2520our%2520SNN%2520models%252C%2520to%250Abenefit%2520from%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20the%20Limitations%20of%20Layer%20Synchronization%20in%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Roel%20Koopman%20and%20Amirreza%20Yousefzadeh%20and%20Mahyar%20Shahsavari%20and%20Guangzhi%20Tang%20and%20Manolis%20Sifalakis&entry.1292438233=%20%20Currently%2C%20neural-network%20processing%20in%20machine%20learning%20applications%20relies%0Aon%20layer%20synchronization%2C%20whereby%20neurons%20in%20a%20layer%20aggregate%20incoming%0Acurrents%20from%20all%20neurons%20in%20the%20preceding%20layer%2C%20before%20evaluating%20their%0Aactivation%20function.%20This%20is%20practiced%20even%20in%20artificial%20Spiking%20Neural%0ANetworks%20%28SNNs%29%2C%20which%20are%20touted%20as%20consistent%20with%20neurobiology%2C%20in%20spite%20of%0Aprocessing%20in%20the%20brain%20being%2C%20in%20fact%20asynchronous.%20A%20truly%20asynchronous%0Asystem%20however%20would%20allow%20all%20neurons%20to%20evaluate%20concurrently%20their%20threshold%0Aand%20emit%20spikes%20upon%20receiving%20any%20presynaptic%20current.%20Omitting%20layer%0Asynchronization%20is%20potentially%20beneficial%2C%20for%20latency%20and%20energy%20efficiency%2C%0Abut%20asynchronous%20execution%20of%20models%20previously%20trained%20with%20layer%0Asynchronization%20may%20entail%20a%20mismatch%20in%20network%20dynamics%20and%20performance.%20We%0Apresent%20a%20study%20that%20documents%20and%20quantifies%20this%20problem%20in%20three%20datasets%20on%0Aour%20simulation%20environment%20that%20implements%20network%20asynchrony%2C%20and%20we%20show%20that%0Amodels%20trained%20with%20layer%20synchronization%20either%20perform%20sub-optimally%20in%0Aabsence%20of%20the%20synchronization%2C%20or%20they%20will%20fail%20to%20benefit%20from%20any%20energy%0Aand%20latency%20reduction%2C%20when%20such%20a%20mechanism%20is%20in%20place.%20We%20then%20%22make%20ends%0Ameet%22%20and%20address%20the%20problem%20with%20unlayered%20backprop%2C%20a%20novel%0Abackpropagation-based%20training%20method%2C%20for%20learning%20models%20suitable%20for%0Aasynchronous%20processing.%20We%20train%20with%20it%20models%20that%20use%20different%20neuron%0Aexecution%20scheduling%20strategies%2C%20and%20we%20show%20that%20although%20their%20neurons%20are%0Amore%20reactive%2C%20these%20models%20consistently%20exhibit%20lower%20overall%20spike%20density%0A%28up%20to%2050%25%29%2C%20reach%20a%20correct%20decision%20faster%20%28up%20to%202x%29%20without%20integrating%20all%0Aspikes%2C%20and%20achieve%20superior%20accuracy%20%28up%20to%2010%25%20higher%29.%20Our%20findings%20suggest%0Athat%20asynchronous%20event-based%20%28neuromorphic%29%20AI%20computing%20is%20indeed%20more%0Aefficient%2C%20but%20we%20need%20to%20seriously%20rethink%20how%20we%20train%20our%20SNN%20models%2C%20to%0Abenefit%20from%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05098v1&entry.124074799=Read"},
{"title": "Knowledge Graph Large Language Model (KG-LLM) for Link Prediction", "author": "Dong Shu and Tianle Chen and Mingyu Jin and Chong Zhang and Mengnan Du and Yongfeng Zhang", "abstract": "  The task of multi-hop link prediction within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, as it requires the model\nto reason through and understand all intermediate connections before making a\nprediction. In this paper, we introduce the Knowledge Graph Large Language\nModel (KG-LLM), a novel framework that leverages large language models (LLMs)\nfor knowledge graph tasks. We first convert structured knowledge graph data\ninto natural language and then use these natural language prompts to fine-tune\nLLMs to enhance multi-hop link prediction in KGs. By converting the KG to\nnatural language prompts, our framework is designed to learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading LLMs within this framework,\nincluding Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's\npotential to provide LLMs with zero-shot capabilities for handling previously\nunseen prompts. Experimental results show that KG-LLM significantly improves\nthe models' generalization capabilities, leading to more accurate predictions\nin unfamiliar scenarios.\n", "link": "http://arxiv.org/abs/2403.07311v8", "date": "2024-08-09", "relevancy": 1.3981, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4695}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4689}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graph%20Large%20Language%20Model%20%28KG-LLM%29%20for%20Link%20Prediction&body=Title%3A%20Knowledge%20Graph%20Large%20Language%20Model%20%28KG-LLM%29%20for%20Link%20Prediction%0AAuthor%3A%20Dong%20Shu%20and%20Tianle%20Chen%20and%20Mingyu%20Jin%20and%20Chong%20Zhang%20and%20Mengnan%20Du%20and%20Yongfeng%20Zhang%0AAbstract%3A%20%20%20The%20task%20of%20multi-hop%20link%20prediction%20within%20knowledge%20graphs%20%28KGs%29%20stands%20as%0Aa%20challenge%20in%20the%20field%20of%20knowledge%20graph%20analysis%2C%20as%20it%20requires%20the%20model%0Ato%20reason%20through%20and%20understand%20all%20intermediate%20connections%20before%20making%20a%0Aprediction.%20In%20this%20paper%2C%20we%20introduce%20the%20Knowledge%20Graph%20Large%20Language%0AModel%20%28KG-LLM%29%2C%20a%20novel%20framework%20that%20leverages%20large%20language%20models%20%28LLMs%29%0Afor%20knowledge%20graph%20tasks.%20We%20first%20convert%20structured%20knowledge%20graph%20data%0Ainto%20natural%20language%20and%20then%20use%20these%20natural%20language%20prompts%20to%20fine-tune%0ALLMs%20to%20enhance%20multi-hop%20link%20prediction%20in%20KGs.%20By%20converting%20the%20KG%20to%0Anatural%20language%20prompts%2C%20our%20framework%20is%20designed%20to%20learn%20the%20latent%0Arepresentations%20of%20entities%20and%20their%20interrelations.%20To%20show%20the%20efficacy%20of%0Athe%20KG-LLM%20Framework%2C%20we%20fine-tune%20three%20leading%20LLMs%20within%20this%20framework%2C%0Aincluding%20Flan-T5%2C%20LLaMa2%20and%20Gemma.%20Further%2C%20we%20explore%20the%20framework%27s%0Apotential%20to%20provide%20LLMs%20with%20zero-shot%20capabilities%20for%20handling%20previously%0Aunseen%20prompts.%20Experimental%20results%20show%20that%20KG-LLM%20significantly%20improves%0Athe%20models%27%20generalization%20capabilities%2C%20leading%20to%20more%20accurate%20predictions%0Ain%20unfamiliar%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07311v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graph%2520Large%2520Language%2520Model%2520%2528KG-LLM%2529%2520for%2520Link%2520Prediction%26entry.906535625%3DDong%2520Shu%2520and%2520Tianle%2520Chen%2520and%2520Mingyu%2520Jin%2520and%2520Chong%2520Zhang%2520and%2520Mengnan%2520Du%2520and%2520Yongfeng%2520Zhang%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520multi-hop%2520link%2520prediction%2520within%2520knowledge%2520graphs%2520%2528KGs%2529%2520stands%2520as%250Aa%2520challenge%2520in%2520the%2520field%2520of%2520knowledge%2520graph%2520analysis%252C%2520as%2520it%2520requires%2520the%2520model%250Ato%2520reason%2520through%2520and%2520understand%2520all%2520intermediate%2520connections%2520before%2520making%2520a%250Aprediction.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Knowledge%2520Graph%2520Large%2520Language%250AModel%2520%2528KG-LLM%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%250Afor%2520knowledge%2520graph%2520tasks.%2520We%2520first%2520convert%2520structured%2520knowledge%2520graph%2520data%250Ainto%2520natural%2520language%2520and%2520then%2520use%2520these%2520natural%2520language%2520prompts%2520to%2520fine-tune%250ALLMs%2520to%2520enhance%2520multi-hop%2520link%2520prediction%2520in%2520KGs.%2520By%2520converting%2520the%2520KG%2520to%250Anatural%2520language%2520prompts%252C%2520our%2520framework%2520is%2520designed%2520to%2520learn%2520the%2520latent%250Arepresentations%2520of%2520entities%2520and%2520their%2520interrelations.%2520To%2520show%2520the%2520efficacy%2520of%250Athe%2520KG-LLM%2520Framework%252C%2520we%2520fine-tune%2520three%2520leading%2520LLMs%2520within%2520this%2520framework%252C%250Aincluding%2520Flan-T5%252C%2520LLaMa2%2520and%2520Gemma.%2520Further%252C%2520we%2520explore%2520the%2520framework%2527s%250Apotential%2520to%2520provide%2520LLMs%2520with%2520zero-shot%2520capabilities%2520for%2520handling%2520previously%250Aunseen%2520prompts.%2520Experimental%2520results%2520show%2520that%2520KG-LLM%2520significantly%2520improves%250Athe%2520models%2527%2520generalization%2520capabilities%252C%2520leading%2520to%2520more%2520accurate%2520predictions%250Ain%2520unfamiliar%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07311v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graph%20Large%20Language%20Model%20%28KG-LLM%29%20for%20Link%20Prediction&entry.906535625=Dong%20Shu%20and%20Tianle%20Chen%20and%20Mingyu%20Jin%20and%20Chong%20Zhang%20and%20Mengnan%20Du%20and%20Yongfeng%20Zhang&entry.1292438233=%20%20The%20task%20of%20multi-hop%20link%20prediction%20within%20knowledge%20graphs%20%28KGs%29%20stands%20as%0Aa%20challenge%20in%20the%20field%20of%20knowledge%20graph%20analysis%2C%20as%20it%20requires%20the%20model%0Ato%20reason%20through%20and%20understand%20all%20intermediate%20connections%20before%20making%20a%0Aprediction.%20In%20this%20paper%2C%20we%20introduce%20the%20Knowledge%20Graph%20Large%20Language%0AModel%20%28KG-LLM%29%2C%20a%20novel%20framework%20that%20leverages%20large%20language%20models%20%28LLMs%29%0Afor%20knowledge%20graph%20tasks.%20We%20first%20convert%20structured%20knowledge%20graph%20data%0Ainto%20natural%20language%20and%20then%20use%20these%20natural%20language%20prompts%20to%20fine-tune%0ALLMs%20to%20enhance%20multi-hop%20link%20prediction%20in%20KGs.%20By%20converting%20the%20KG%20to%0Anatural%20language%20prompts%2C%20our%20framework%20is%20designed%20to%20learn%20the%20latent%0Arepresentations%20of%20entities%20and%20their%20interrelations.%20To%20show%20the%20efficacy%20of%0Athe%20KG-LLM%20Framework%2C%20we%20fine-tune%20three%20leading%20LLMs%20within%20this%20framework%2C%0Aincluding%20Flan-T5%2C%20LLaMa2%20and%20Gemma.%20Further%2C%20we%20explore%20the%20framework%27s%0Apotential%20to%20provide%20LLMs%20with%20zero-shot%20capabilities%20for%20handling%20previously%0Aunseen%20prompts.%20Experimental%20results%20show%20that%20KG-LLM%20significantly%20improves%0Athe%20models%27%20generalization%20capabilities%2C%20leading%20to%20more%20accurate%20predictions%0Ain%20unfamiliar%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07311v8&entry.124074799=Read"},
{"title": "Optimal Distributed Multi-Robot Communication-Aware Trajectory Planning\n  using Alternating Direction Method of Multipliers", "author": "Jeppe Heini Mikkelsen and Roberto Galeazzi and Matteo Fumagalli", "abstract": "  This paper presents a distributed, optimal, communication-aware trajectory\nplanning algorithm for multi-robot systems. Building on prior work, it\naddresses the multi-robot communication-aware trajectory planning problem using\na general optimisation framework that imposes linear constraints on changes in\nrobot positions to ensure communication performance and collision avoidance. In\nthis paper, the optimisation problem is solved distributively by separating the\ncommunication performance constraint through an economic approach. Here, the\ncurrent communication budget is distributed equally among the robots, and the\nrobots are allowed to trade parts of their budgets with each other. The\nseparated optimisation problem is then solved using the consensus alternating\ndirection method of multipliers. The method was verified through simulation in\nan inspection task problem.\n", "link": "http://arxiv.org/abs/2408.05111v1", "date": "2024-08-09", "relevancy": 1.3904, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4639}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Distributed%20Multi-Robot%20Communication-Aware%20Trajectory%20Planning%0A%20%20using%20Alternating%20Direction%20Method%20of%20Multipliers&body=Title%3A%20Optimal%20Distributed%20Multi-Robot%20Communication-Aware%20Trajectory%20Planning%0A%20%20using%20Alternating%20Direction%20Method%20of%20Multipliers%0AAuthor%3A%20Jeppe%20Heini%20Mikkelsen%20and%20Roberto%20Galeazzi%20and%20Matteo%20Fumagalli%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20distributed%2C%20optimal%2C%20communication-aware%20trajectory%0Aplanning%20algorithm%20for%20multi-robot%20systems.%20Building%20on%20prior%20work%2C%20it%0Aaddresses%20the%20multi-robot%20communication-aware%20trajectory%20planning%20problem%20using%0Aa%20general%20optimisation%20framework%20that%20imposes%20linear%20constraints%20on%20changes%20in%0Arobot%20positions%20to%20ensure%20communication%20performance%20and%20collision%20avoidance.%20In%0Athis%20paper%2C%20the%20optimisation%20problem%20is%20solved%20distributively%20by%20separating%20the%0Acommunication%20performance%20constraint%20through%20an%20economic%20approach.%20Here%2C%20the%0Acurrent%20communication%20budget%20is%20distributed%20equally%20among%20the%20robots%2C%20and%20the%0Arobots%20are%20allowed%20to%20trade%20parts%20of%20their%20budgets%20with%20each%20other.%20The%0Aseparated%20optimisation%20problem%20is%20then%20solved%20using%20the%20consensus%20alternating%0Adirection%20method%20of%20multipliers.%20The%20method%20was%20verified%20through%20simulation%20in%0Aan%20inspection%20task%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Distributed%2520Multi-Robot%2520Communication-Aware%2520Trajectory%2520Planning%250A%2520%2520using%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%26entry.906535625%3DJeppe%2520Heini%2520Mikkelsen%2520and%2520Roberto%2520Galeazzi%2520and%2520Matteo%2520Fumagalli%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520distributed%252C%2520optimal%252C%2520communication-aware%2520trajectory%250Aplanning%2520algorithm%2520for%2520multi-robot%2520systems.%2520Building%2520on%2520prior%2520work%252C%2520it%250Aaddresses%2520the%2520multi-robot%2520communication-aware%2520trajectory%2520planning%2520problem%2520using%250Aa%2520general%2520optimisation%2520framework%2520that%2520imposes%2520linear%2520constraints%2520on%2520changes%2520in%250Arobot%2520positions%2520to%2520ensure%2520communication%2520performance%2520and%2520collision%2520avoidance.%2520In%250Athis%2520paper%252C%2520the%2520optimisation%2520problem%2520is%2520solved%2520distributively%2520by%2520separating%2520the%250Acommunication%2520performance%2520constraint%2520through%2520an%2520economic%2520approach.%2520Here%252C%2520the%250Acurrent%2520communication%2520budget%2520is%2520distributed%2520equally%2520among%2520the%2520robots%252C%2520and%2520the%250Arobots%2520are%2520allowed%2520to%2520trade%2520parts%2520of%2520their%2520budgets%2520with%2520each%2520other.%2520The%250Aseparated%2520optimisation%2520problem%2520is%2520then%2520solved%2520using%2520the%2520consensus%2520alternating%250Adirection%2520method%2520of%2520multipliers.%2520The%2520method%2520was%2520verified%2520through%2520simulation%2520in%250Aan%2520inspection%2520task%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Distributed%20Multi-Robot%20Communication-Aware%20Trajectory%20Planning%0A%20%20using%20Alternating%20Direction%20Method%20of%20Multipliers&entry.906535625=Jeppe%20Heini%20Mikkelsen%20and%20Roberto%20Galeazzi%20and%20Matteo%20Fumagalli&entry.1292438233=%20%20This%20paper%20presents%20a%20distributed%2C%20optimal%2C%20communication-aware%20trajectory%0Aplanning%20algorithm%20for%20multi-robot%20systems.%20Building%20on%20prior%20work%2C%20it%0Aaddresses%20the%20multi-robot%20communication-aware%20trajectory%20planning%20problem%20using%0Aa%20general%20optimisation%20framework%20that%20imposes%20linear%20constraints%20on%20changes%20in%0Arobot%20positions%20to%20ensure%20communication%20performance%20and%20collision%20avoidance.%20In%0Athis%20paper%2C%20the%20optimisation%20problem%20is%20solved%20distributively%20by%20separating%20the%0Acommunication%20performance%20constraint%20through%20an%20economic%20approach.%20Here%2C%20the%0Acurrent%20communication%20budget%20is%20distributed%20equally%20among%20the%20robots%2C%20and%20the%0Arobots%20are%20allowed%20to%20trade%20parts%20of%20their%20budgets%20with%20each%20other.%20The%0Aseparated%20optimisation%20problem%20is%20then%20solved%20using%20the%20consensus%20alternating%0Adirection%20method%20of%20multipliers.%20The%20method%20was%20verified%20through%20simulation%20in%0Aan%20inspection%20task%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05111v1&entry.124074799=Read"},
{"title": "HistoKernel: Whole Slide Image Level Maximum Mean Discrepancy Kernels\n  for Pan-Cancer Predictive Modelling", "author": "Piotr Keller and Muhammad Dawood and Brinder Singh Chohan and Fayyaz ul Amir Afsar Minhas", "abstract": "  Machine learning in computational pathology (CPath) often aggregates\npatch-level predictions from multi-gigapixel Whole Slide Images (WSIs) to\ngenerate WSI-level prediction scores for crucial tasks such as survival\nprediction and drug effect prediction. However, current methods do not\nexplicitly characterize distributional differences between patch sets within\nWSIs. We introduce HistoKernel, a novel Maximum Mean Discrepancy (MMD) kernel\nthat measures distributional similarity between WSIs for enhanced prediction\nperformance on downstream prediction tasks.\n  Our comprehensive analysis demonstrates HistoKernel's effectiveness across\nvarious machine learning tasks, including retrieval (n = 9,362), drug\nsensitivity regression (n = 551), point mutation classification (n = 3,419),\nand survival analysis (n = 2,291), outperforming existing deep learning\nmethods. Additionally, HistoKernel seamlessly integrates multi-modal data and\noffers a novel perturbation-based method for patch-level explainability. This\nwork pioneers the use of kernel-based methods for WSI-level predictive\nmodeling, opening new avenues for research. Code is available at\nhttps://github.com/pkeller00/HistoKernel.\n", "link": "http://arxiv.org/abs/2408.05195v1", "date": "2024-08-09", "relevancy": 1.3644, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.465}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4523}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HistoKernel%3A%20Whole%20Slide%20Image%20Level%20Maximum%20Mean%20Discrepancy%20Kernels%0A%20%20for%20Pan-Cancer%20Predictive%20Modelling&body=Title%3A%20HistoKernel%3A%20Whole%20Slide%20Image%20Level%20Maximum%20Mean%20Discrepancy%20Kernels%0A%20%20for%20Pan-Cancer%20Predictive%20Modelling%0AAuthor%3A%20Piotr%20Keller%20and%20Muhammad%20Dawood%20and%20Brinder%20Singh%20Chohan%20and%20Fayyaz%20ul%20Amir%20Afsar%20Minhas%0AAbstract%3A%20%20%20Machine%20learning%20in%20computational%20pathology%20%28CPath%29%20often%20aggregates%0Apatch-level%20predictions%20from%20multi-gigapixel%20Whole%20Slide%20Images%20%28WSIs%29%20to%0Agenerate%20WSI-level%20prediction%20scores%20for%20crucial%20tasks%20such%20as%20survival%0Aprediction%20and%20drug%20effect%20prediction.%20However%2C%20current%20methods%20do%20not%0Aexplicitly%20characterize%20distributional%20differences%20between%20patch%20sets%20within%0AWSIs.%20We%20introduce%20HistoKernel%2C%20a%20novel%20Maximum%20Mean%20Discrepancy%20%28MMD%29%20kernel%0Athat%20measures%20distributional%20similarity%20between%20WSIs%20for%20enhanced%20prediction%0Aperformance%20on%20downstream%20prediction%20tasks.%0A%20%20Our%20comprehensive%20analysis%20demonstrates%20HistoKernel%27s%20effectiveness%20across%0Avarious%20machine%20learning%20tasks%2C%20including%20retrieval%20%28n%20%3D%209%2C362%29%2C%20drug%0Asensitivity%20regression%20%28n%20%3D%20551%29%2C%20point%20mutation%20classification%20%28n%20%3D%203%2C419%29%2C%0Aand%20survival%20analysis%20%28n%20%3D%202%2C291%29%2C%20outperforming%20existing%20deep%20learning%0Amethods.%20Additionally%2C%20HistoKernel%20seamlessly%20integrates%20multi-modal%20data%20and%0Aoffers%20a%20novel%20perturbation-based%20method%20for%20patch-level%20explainability.%20This%0Awork%20pioneers%20the%20use%20of%20kernel-based%20methods%20for%20WSI-level%20predictive%0Amodeling%2C%20opening%20new%20avenues%20for%20research.%20Code%20is%20available%20at%0Ahttps%3A//github.com/pkeller00/HistoKernel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistoKernel%253A%2520Whole%2520Slide%2520Image%2520Level%2520Maximum%2520Mean%2520Discrepancy%2520Kernels%250A%2520%2520for%2520Pan-Cancer%2520Predictive%2520Modelling%26entry.906535625%3DPiotr%2520Keller%2520and%2520Muhammad%2520Dawood%2520and%2520Brinder%2520Singh%2520Chohan%2520and%2520Fayyaz%2520ul%2520Amir%2520Afsar%2520Minhas%26entry.1292438233%3D%2520%2520Machine%2520learning%2520in%2520computational%2520pathology%2520%2528CPath%2529%2520often%2520aggregates%250Apatch-level%2520predictions%2520from%2520multi-gigapixel%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520to%250Agenerate%2520WSI-level%2520prediction%2520scores%2520for%2520crucial%2520tasks%2520such%2520as%2520survival%250Aprediction%2520and%2520drug%2520effect%2520prediction.%2520However%252C%2520current%2520methods%2520do%2520not%250Aexplicitly%2520characterize%2520distributional%2520differences%2520between%2520patch%2520sets%2520within%250AWSIs.%2520We%2520introduce%2520HistoKernel%252C%2520a%2520novel%2520Maximum%2520Mean%2520Discrepancy%2520%2528MMD%2529%2520kernel%250Athat%2520measures%2520distributional%2520similarity%2520between%2520WSIs%2520for%2520enhanced%2520prediction%250Aperformance%2520on%2520downstream%2520prediction%2520tasks.%250A%2520%2520Our%2520comprehensive%2520analysis%2520demonstrates%2520HistoKernel%2527s%2520effectiveness%2520across%250Avarious%2520machine%2520learning%2520tasks%252C%2520including%2520retrieval%2520%2528n%2520%253D%25209%252C362%2529%252C%2520drug%250Asensitivity%2520regression%2520%2528n%2520%253D%2520551%2529%252C%2520point%2520mutation%2520classification%2520%2528n%2520%253D%25203%252C419%2529%252C%250Aand%2520survival%2520analysis%2520%2528n%2520%253D%25202%252C291%2529%252C%2520outperforming%2520existing%2520deep%2520learning%250Amethods.%2520Additionally%252C%2520HistoKernel%2520seamlessly%2520integrates%2520multi-modal%2520data%2520and%250Aoffers%2520a%2520novel%2520perturbation-based%2520method%2520for%2520patch-level%2520explainability.%2520This%250Awork%2520pioneers%2520the%2520use%2520of%2520kernel-based%2520methods%2520for%2520WSI-level%2520predictive%250Amodeling%252C%2520opening%2520new%2520avenues%2520for%2520research.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/pkeller00/HistoKernel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HistoKernel%3A%20Whole%20Slide%20Image%20Level%20Maximum%20Mean%20Discrepancy%20Kernels%0A%20%20for%20Pan-Cancer%20Predictive%20Modelling&entry.906535625=Piotr%20Keller%20and%20Muhammad%20Dawood%20and%20Brinder%20Singh%20Chohan%20and%20Fayyaz%20ul%20Amir%20Afsar%20Minhas&entry.1292438233=%20%20Machine%20learning%20in%20computational%20pathology%20%28CPath%29%20often%20aggregates%0Apatch-level%20predictions%20from%20multi-gigapixel%20Whole%20Slide%20Images%20%28WSIs%29%20to%0Agenerate%20WSI-level%20prediction%20scores%20for%20crucial%20tasks%20such%20as%20survival%0Aprediction%20and%20drug%20effect%20prediction.%20However%2C%20current%20methods%20do%20not%0Aexplicitly%20characterize%20distributional%20differences%20between%20patch%20sets%20within%0AWSIs.%20We%20introduce%20HistoKernel%2C%20a%20novel%20Maximum%20Mean%20Discrepancy%20%28MMD%29%20kernel%0Athat%20measures%20distributional%20similarity%20between%20WSIs%20for%20enhanced%20prediction%0Aperformance%20on%20downstream%20prediction%20tasks.%0A%20%20Our%20comprehensive%20analysis%20demonstrates%20HistoKernel%27s%20effectiveness%20across%0Avarious%20machine%20learning%20tasks%2C%20including%20retrieval%20%28n%20%3D%209%2C362%29%2C%20drug%0Asensitivity%20regression%20%28n%20%3D%20551%29%2C%20point%20mutation%20classification%20%28n%20%3D%203%2C419%29%2C%0Aand%20survival%20analysis%20%28n%20%3D%202%2C291%29%2C%20outperforming%20existing%20deep%20learning%0Amethods.%20Additionally%2C%20HistoKernel%20seamlessly%20integrates%20multi-modal%20data%20and%0Aoffers%20a%20novel%20perturbation-based%20method%20for%20patch-level%20explainability.%20This%0Awork%20pioneers%20the%20use%20of%20kernel-based%20methods%20for%20WSI-level%20predictive%0Amodeling%2C%20opening%20new%20avenues%20for%20research.%20Code%20is%20available%20at%0Ahttps%3A//github.com/pkeller00/HistoKernel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05195v1&entry.124074799=Read"},
{"title": "Controllable seismic velocity synthesis using generative diffusion\n  models", "author": "Fu Wang and Xinquan Huang and Tariq Alkhalifah", "abstract": "  Accurate seismic velocity estimations are vital to understanding Earth's\nsubsurface structures, assessing natural resources, and evaluating seismic\nhazards. Machine learning-based inversion algorithms have shown promising\nperformance in regional (i.e., for exploration) and global velocity estimation,\nwhile their effectiveness hinges on access to large and diverse training\ndatasets whose distributions generally cover the target solutions.\nAdditionally, enhancing the precision and reliability of velocity estimation\nalso requires incorporating prior information, e.g., geological classes, well\nlogs, and subsurface structures, but current statistical or neural\nnetwork-based methods are not flexible enough to handle such multi-modal\ninformation. To address both challenges, we propose to use conditional\ngenerative diffusion models for seismic velocity synthesis, in which we readily\nincorporate those priors. This approach enables the generation of seismic\nvelocities that closely match the expected target distribution, offering\ndatasets informed by both expert knowledge and measured data to support\ntraining for data-driven geophysical methods. We demonstrate the flexibility\nand effectiveness of our method through training diffusion models on the\nOpenFWI dataset under various conditions, including class labels, well logs,\nreflectivity images, and the combination of these priors. The performance of\nthe approach under out-of-distribution conditions further underscores its\ngeneralization ability, showcasing its potential to provide tailored priors for\nvelocity inverse problems and create specific training datasets for machine\nlearning-based geophysical applications.\n", "link": "http://arxiv.org/abs/2402.06277v2", "date": "2024-08-09", "relevancy": 1.1452, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5832}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5755}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20seismic%20velocity%20synthesis%20using%20generative%20diffusion%0A%20%20models&body=Title%3A%20Controllable%20seismic%20velocity%20synthesis%20using%20generative%20diffusion%0A%20%20models%0AAuthor%3A%20Fu%20Wang%20and%20Xinquan%20Huang%20and%20Tariq%20Alkhalifah%0AAbstract%3A%20%20%20Accurate%20seismic%20velocity%20estimations%20are%20vital%20to%20understanding%20Earth%27s%0Asubsurface%20structures%2C%20assessing%20natural%20resources%2C%20and%20evaluating%20seismic%0Ahazards.%20Machine%20learning-based%20inversion%20algorithms%20have%20shown%20promising%0Aperformance%20in%20regional%20%28i.e.%2C%20for%20exploration%29%20and%20global%20velocity%20estimation%2C%0Awhile%20their%20effectiveness%20hinges%20on%20access%20to%20large%20and%20diverse%20training%0Adatasets%20whose%20distributions%20generally%20cover%20the%20target%20solutions.%0AAdditionally%2C%20enhancing%20the%20precision%20and%20reliability%20of%20velocity%20estimation%0Aalso%20requires%20incorporating%20prior%20information%2C%20e.g.%2C%20geological%20classes%2C%20well%0Alogs%2C%20and%20subsurface%20structures%2C%20but%20current%20statistical%20or%20neural%0Anetwork-based%20methods%20are%20not%20flexible%20enough%20to%20handle%20such%20multi-modal%0Ainformation.%20To%20address%20both%20challenges%2C%20we%20propose%20to%20use%20conditional%0Agenerative%20diffusion%20models%20for%20seismic%20velocity%20synthesis%2C%20in%20which%20we%20readily%0Aincorporate%20those%20priors.%20This%20approach%20enables%20the%20generation%20of%20seismic%0Avelocities%20that%20closely%20match%20the%20expected%20target%20distribution%2C%20offering%0Adatasets%20informed%20by%20both%20expert%20knowledge%20and%20measured%20data%20to%20support%0Atraining%20for%20data-driven%20geophysical%20methods.%20We%20demonstrate%20the%20flexibility%0Aand%20effectiveness%20of%20our%20method%20through%20training%20diffusion%20models%20on%20the%0AOpenFWI%20dataset%20under%20various%20conditions%2C%20including%20class%20labels%2C%20well%20logs%2C%0Areflectivity%20images%2C%20and%20the%20combination%20of%20these%20priors.%20The%20performance%20of%0Athe%20approach%20under%20out-of-distribution%20conditions%20further%20underscores%20its%0Ageneralization%20ability%2C%20showcasing%20its%20potential%20to%20provide%20tailored%20priors%20for%0Avelocity%20inverse%20problems%20and%20create%20specific%20training%20datasets%20for%20machine%0Alearning-based%20geophysical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520seismic%2520velocity%2520synthesis%2520using%2520generative%2520diffusion%250A%2520%2520models%26entry.906535625%3DFu%2520Wang%2520and%2520Xinquan%2520Huang%2520and%2520Tariq%2520Alkhalifah%26entry.1292438233%3D%2520%2520Accurate%2520seismic%2520velocity%2520estimations%2520are%2520vital%2520to%2520understanding%2520Earth%2527s%250Asubsurface%2520structures%252C%2520assessing%2520natural%2520resources%252C%2520and%2520evaluating%2520seismic%250Ahazards.%2520Machine%2520learning-based%2520inversion%2520algorithms%2520have%2520shown%2520promising%250Aperformance%2520in%2520regional%2520%2528i.e.%252C%2520for%2520exploration%2529%2520and%2520global%2520velocity%2520estimation%252C%250Awhile%2520their%2520effectiveness%2520hinges%2520on%2520access%2520to%2520large%2520and%2520diverse%2520training%250Adatasets%2520whose%2520distributions%2520generally%2520cover%2520the%2520target%2520solutions.%250AAdditionally%252C%2520enhancing%2520the%2520precision%2520and%2520reliability%2520of%2520velocity%2520estimation%250Aalso%2520requires%2520incorporating%2520prior%2520information%252C%2520e.g.%252C%2520geological%2520classes%252C%2520well%250Alogs%252C%2520and%2520subsurface%2520structures%252C%2520but%2520current%2520statistical%2520or%2520neural%250Anetwork-based%2520methods%2520are%2520not%2520flexible%2520enough%2520to%2520handle%2520such%2520multi-modal%250Ainformation.%2520To%2520address%2520both%2520challenges%252C%2520we%2520propose%2520to%2520use%2520conditional%250Agenerative%2520diffusion%2520models%2520for%2520seismic%2520velocity%2520synthesis%252C%2520in%2520which%2520we%2520readily%250Aincorporate%2520those%2520priors.%2520This%2520approach%2520enables%2520the%2520generation%2520of%2520seismic%250Avelocities%2520that%2520closely%2520match%2520the%2520expected%2520target%2520distribution%252C%2520offering%250Adatasets%2520informed%2520by%2520both%2520expert%2520knowledge%2520and%2520measured%2520data%2520to%2520support%250Atraining%2520for%2520data-driven%2520geophysical%2520methods.%2520We%2520demonstrate%2520the%2520flexibility%250Aand%2520effectiveness%2520of%2520our%2520method%2520through%2520training%2520diffusion%2520models%2520on%2520the%250AOpenFWI%2520dataset%2520under%2520various%2520conditions%252C%2520including%2520class%2520labels%252C%2520well%2520logs%252C%250Areflectivity%2520images%252C%2520and%2520the%2520combination%2520of%2520these%2520priors.%2520The%2520performance%2520of%250Athe%2520approach%2520under%2520out-of-distribution%2520conditions%2520further%2520underscores%2520its%250Ageneralization%2520ability%252C%2520showcasing%2520its%2520potential%2520to%2520provide%2520tailored%2520priors%2520for%250Avelocity%2520inverse%2520problems%2520and%2520create%2520specific%2520training%2520datasets%2520for%2520machine%250Alearning-based%2520geophysical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20seismic%20velocity%20synthesis%20using%20generative%20diffusion%0A%20%20models&entry.906535625=Fu%20Wang%20and%20Xinquan%20Huang%20and%20Tariq%20Alkhalifah&entry.1292438233=%20%20Accurate%20seismic%20velocity%20estimations%20are%20vital%20to%20understanding%20Earth%27s%0Asubsurface%20structures%2C%20assessing%20natural%20resources%2C%20and%20evaluating%20seismic%0Ahazards.%20Machine%20learning-based%20inversion%20algorithms%20have%20shown%20promising%0Aperformance%20in%20regional%20%28i.e.%2C%20for%20exploration%29%20and%20global%20velocity%20estimation%2C%0Awhile%20their%20effectiveness%20hinges%20on%20access%20to%20large%20and%20diverse%20training%0Adatasets%20whose%20distributions%20generally%20cover%20the%20target%20solutions.%0AAdditionally%2C%20enhancing%20the%20precision%20and%20reliability%20of%20velocity%20estimation%0Aalso%20requires%20incorporating%20prior%20information%2C%20e.g.%2C%20geological%20classes%2C%20well%0Alogs%2C%20and%20subsurface%20structures%2C%20but%20current%20statistical%20or%20neural%0Anetwork-based%20methods%20are%20not%20flexible%20enough%20to%20handle%20such%20multi-modal%0Ainformation.%20To%20address%20both%20challenges%2C%20we%20propose%20to%20use%20conditional%0Agenerative%20diffusion%20models%20for%20seismic%20velocity%20synthesis%2C%20in%20which%20we%20readily%0Aincorporate%20those%20priors.%20This%20approach%20enables%20the%20generation%20of%20seismic%0Avelocities%20that%20closely%20match%20the%20expected%20target%20distribution%2C%20offering%0Adatasets%20informed%20by%20both%20expert%20knowledge%20and%20measured%20data%20to%20support%0Atraining%20for%20data-driven%20geophysical%20methods.%20We%20demonstrate%20the%20flexibility%0Aand%20effectiveness%20of%20our%20method%20through%20training%20diffusion%20models%20on%20the%0AOpenFWI%20dataset%20under%20various%20conditions%2C%20including%20class%20labels%2C%20well%20logs%2C%0Areflectivity%20images%2C%20and%20the%20combination%20of%20these%20priors.%20The%20performance%20of%0Athe%20approach%20under%20out-of-distribution%20conditions%20further%20underscores%20its%0Ageneralization%20ability%2C%20showcasing%20its%20potential%20to%20provide%20tailored%20priors%20for%0Avelocity%20inverse%20problems%20and%20create%20specific%20training%20datasets%20for%20machine%0Alearning-based%20geophysical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06277v2&entry.124074799=Read"},
{"title": "Improving Large Language Models in Event Relation Logical Prediction", "author": "Meiqi Chen and Yubo Ma and Kaitao Song and Yixin Cao and Yan Zhang and Dongsheng Li", "abstract": "  Event relations are crucial for narrative understanding and reasoning.\nGoverned by nuanced logic, event relation extraction (ERE) is a challenging\ntask that demands thorough semantic understanding and rigorous logical\nreasoning. In this paper, we conduct an in-depth investigation to\nsystematically explore the capability of LLMs in understanding and applying\nevent relation logic. More in detail, we first investigate the deficiencies of\nLLMs in logical reasoning across different tasks. Our study reveals that LLMs\nare not logically consistent reasoners, which results in their suboptimal\nperformance on tasks that need rigorous reasoning. To address this, we explore\nthree different approaches to endow LLMs with event relation logic, and thus\nenable them to generate more coherent answers across various scenarios. Based\non our approach, we also contribute a synthesized dataset (LLM-ERL) involving\nhigh-order reasoning for evaluation and fine-tuning. Extensive quantitative and\nqualitative analyses on different tasks also validate the effectiveness of our\napproaches and provide insights for solving practical tasks with LLMs in future\nwork. Codes are available at https://github.com/chenmeiqii/Teach-LLM-LR.\n", "link": "http://arxiv.org/abs/2310.09158v2", "date": "2024-08-09", "relevancy": 0.9776, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4998}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Large%20Language%20Models%20in%20Event%20Relation%20Logical%20Prediction&body=Title%3A%20Improving%20Large%20Language%20Models%20in%20Event%20Relation%20Logical%20Prediction%0AAuthor%3A%20Meiqi%20Chen%20and%20Yubo%20Ma%20and%20Kaitao%20Song%20and%20Yixin%20Cao%20and%20Yan%20Zhang%20and%20Dongsheng%20Li%0AAbstract%3A%20%20%20Event%20relations%20are%20crucial%20for%20narrative%20understanding%20and%20reasoning.%0AGoverned%20by%20nuanced%20logic%2C%20event%20relation%20extraction%20%28ERE%29%20is%20a%20challenging%0Atask%20that%20demands%20thorough%20semantic%20understanding%20and%20rigorous%20logical%0Areasoning.%20In%20this%20paper%2C%20we%20conduct%20an%20in-depth%20investigation%20to%0Asystematically%20explore%20the%20capability%20of%20LLMs%20in%20understanding%20and%20applying%0Aevent%20relation%20logic.%20More%20in%20detail%2C%20we%20first%20investigate%20the%20deficiencies%20of%0ALLMs%20in%20logical%20reasoning%20across%20different%20tasks.%20Our%20study%20reveals%20that%20LLMs%0Aare%20not%20logically%20consistent%20reasoners%2C%20which%20results%20in%20their%20suboptimal%0Aperformance%20on%20tasks%20that%20need%20rigorous%20reasoning.%20To%20address%20this%2C%20we%20explore%0Athree%20different%20approaches%20to%20endow%20LLMs%20with%20event%20relation%20logic%2C%20and%20thus%0Aenable%20them%20to%20generate%20more%20coherent%20answers%20across%20various%20scenarios.%20Based%0Aon%20our%20approach%2C%20we%20also%20contribute%20a%20synthesized%20dataset%20%28LLM-ERL%29%20involving%0Ahigh-order%20reasoning%20for%20evaluation%20and%20fine-tuning.%20Extensive%20quantitative%20and%0Aqualitative%20analyses%20on%20different%20tasks%20also%20validate%20the%20effectiveness%20of%20our%0Aapproaches%20and%20provide%20insights%20for%20solving%20practical%20tasks%20with%20LLMs%20in%20future%0Awork.%20Codes%20are%20available%20at%20https%3A//github.com/chenmeiqii/Teach-LLM-LR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09158v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Large%2520Language%2520Models%2520in%2520Event%2520Relation%2520Logical%2520Prediction%26entry.906535625%3DMeiqi%2520Chen%2520and%2520Yubo%2520Ma%2520and%2520Kaitao%2520Song%2520and%2520Yixin%2520Cao%2520and%2520Yan%2520Zhang%2520and%2520Dongsheng%2520Li%26entry.1292438233%3D%2520%2520Event%2520relations%2520are%2520crucial%2520for%2520narrative%2520understanding%2520and%2520reasoning.%250AGoverned%2520by%2520nuanced%2520logic%252C%2520event%2520relation%2520extraction%2520%2528ERE%2529%2520is%2520a%2520challenging%250Atask%2520that%2520demands%2520thorough%2520semantic%2520understanding%2520and%2520rigorous%2520logical%250Areasoning.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520an%2520in-depth%2520investigation%2520to%250Asystematically%2520explore%2520the%2520capability%2520of%2520LLMs%2520in%2520understanding%2520and%2520applying%250Aevent%2520relation%2520logic.%2520More%2520in%2520detail%252C%2520we%2520first%2520investigate%2520the%2520deficiencies%2520of%250ALLMs%2520in%2520logical%2520reasoning%2520across%2520different%2520tasks.%2520Our%2520study%2520reveals%2520that%2520LLMs%250Aare%2520not%2520logically%2520consistent%2520reasoners%252C%2520which%2520results%2520in%2520their%2520suboptimal%250Aperformance%2520on%2520tasks%2520that%2520need%2520rigorous%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520explore%250Athree%2520different%2520approaches%2520to%2520endow%2520LLMs%2520with%2520event%2520relation%2520logic%252C%2520and%2520thus%250Aenable%2520them%2520to%2520generate%2520more%2520coherent%2520answers%2520across%2520various%2520scenarios.%2520Based%250Aon%2520our%2520approach%252C%2520we%2520also%2520contribute%2520a%2520synthesized%2520dataset%2520%2528LLM-ERL%2529%2520involving%250Ahigh-order%2520reasoning%2520for%2520evaluation%2520and%2520fine-tuning.%2520Extensive%2520quantitative%2520and%250Aqualitative%2520analyses%2520on%2520different%2520tasks%2520also%2520validate%2520the%2520effectiveness%2520of%2520our%250Aapproaches%2520and%2520provide%2520insights%2520for%2520solving%2520practical%2520tasks%2520with%2520LLMs%2520in%2520future%250Awork.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/chenmeiqii/Teach-LLM-LR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09158v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Large%20Language%20Models%20in%20Event%20Relation%20Logical%20Prediction&entry.906535625=Meiqi%20Chen%20and%20Yubo%20Ma%20and%20Kaitao%20Song%20and%20Yixin%20Cao%20and%20Yan%20Zhang%20and%20Dongsheng%20Li&entry.1292438233=%20%20Event%20relations%20are%20crucial%20for%20narrative%20understanding%20and%20reasoning.%0AGoverned%20by%20nuanced%20logic%2C%20event%20relation%20extraction%20%28ERE%29%20is%20a%20challenging%0Atask%20that%20demands%20thorough%20semantic%20understanding%20and%20rigorous%20logical%0Areasoning.%20In%20this%20paper%2C%20we%20conduct%20an%20in-depth%20investigation%20to%0Asystematically%20explore%20the%20capability%20of%20LLMs%20in%20understanding%20and%20applying%0Aevent%20relation%20logic.%20More%20in%20detail%2C%20we%20first%20investigate%20the%20deficiencies%20of%0ALLMs%20in%20logical%20reasoning%20across%20different%20tasks.%20Our%20study%20reveals%20that%20LLMs%0Aare%20not%20logically%20consistent%20reasoners%2C%20which%20results%20in%20their%20suboptimal%0Aperformance%20on%20tasks%20that%20need%20rigorous%20reasoning.%20To%20address%20this%2C%20we%20explore%0Athree%20different%20approaches%20to%20endow%20LLMs%20with%20event%20relation%20logic%2C%20and%20thus%0Aenable%20them%20to%20generate%20more%20coherent%20answers%20across%20various%20scenarios.%20Based%0Aon%20our%20approach%2C%20we%20also%20contribute%20a%20synthesized%20dataset%20%28LLM-ERL%29%20involving%0Ahigh-order%20reasoning%20for%20evaluation%20and%20fine-tuning.%20Extensive%20quantitative%20and%0Aqualitative%20analyses%20on%20different%20tasks%20also%20validate%20the%20effectiveness%20of%20our%0Aapproaches%20and%20provide%20insights%20for%20solving%20practical%20tasks%20with%20LLMs%20in%20future%0Awork.%20Codes%20are%20available%20at%20https%3A//github.com/chenmeiqii/Teach-LLM-LR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09158v2&entry.124074799=Read"},
{"title": "Cycle-Configuration: A Novel Graph-theoretic Descriptor Set for\n  Molecular Inference", "author": "Bowen Song and Jianshen Zhu and Naveed Ahmed Azam and Kazuya Haraguchi and Liang Zhao and Tatsuya Akutsu", "abstract": "  In this paper, we propose a novel family of descriptors of chemical graphs,\nnamed cycle-configuration (CC), that can be used in the standard \"two-layered\n(2L) model\" of mol-infer, a molecular inference framework based on mixed\ninteger linear programming (MILP) and machine learning (ML). Proposed\ndescriptors capture the notion of ortho/meta/para patterns that appear in\naromatic rings, which has been impossible in the framework so far.\nComputational experiments show that, when the new descriptors are supplied, we\ncan construct prediction functions of similar or better performance for all of\nthe 27 tested chemical properties. We also provide an MILP formulation that\nasks for a chemical graph with desired properties under the 2L model with CC\ndescriptors (2L+CC model). We show that a chemical graph with up to 50\nnon-hydrogen vertices can be inferred in a practical time.\n", "link": "http://arxiv.org/abs/2408.05136v1", "date": "2024-08-09", "relevancy": 1.2815, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4619}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4246}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cycle-Configuration%3A%20A%20Novel%20Graph-theoretic%20Descriptor%20Set%20for%0A%20%20Molecular%20Inference&body=Title%3A%20Cycle-Configuration%3A%20A%20Novel%20Graph-theoretic%20Descriptor%20Set%20for%0A%20%20Molecular%20Inference%0AAuthor%3A%20Bowen%20Song%20and%20Jianshen%20Zhu%20and%20Naveed%20Ahmed%20Azam%20and%20Kazuya%20Haraguchi%20and%20Liang%20Zhao%20and%20Tatsuya%20Akutsu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20family%20of%20descriptors%20of%20chemical%20graphs%2C%0Anamed%20cycle-configuration%20%28CC%29%2C%20that%20can%20be%20used%20in%20the%20standard%20%22two-layered%0A%282L%29%20model%22%20of%20mol-infer%2C%20a%20molecular%20inference%20framework%20based%20on%20mixed%0Ainteger%20linear%20programming%20%28MILP%29%20and%20machine%20learning%20%28ML%29.%20Proposed%0Adescriptors%20capture%20the%20notion%20of%20ortho/meta/para%20patterns%20that%20appear%20in%0Aaromatic%20rings%2C%20which%20has%20been%20impossible%20in%20the%20framework%20so%20far.%0AComputational%20experiments%20show%20that%2C%20when%20the%20new%20descriptors%20are%20supplied%2C%20we%0Acan%20construct%20prediction%20functions%20of%20similar%20or%20better%20performance%20for%20all%20of%0Athe%2027%20tested%20chemical%20properties.%20We%20also%20provide%20an%20MILP%20formulation%20that%0Aasks%20for%20a%20chemical%20graph%20with%20desired%20properties%20under%20the%202L%20model%20with%20CC%0Adescriptors%20%282L%2BCC%20model%29.%20We%20show%20that%20a%20chemical%20graph%20with%20up%20to%2050%0Anon-hydrogen%20vertices%20can%20be%20inferred%20in%20a%20practical%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCycle-Configuration%253A%2520A%2520Novel%2520Graph-theoretic%2520Descriptor%2520Set%2520for%250A%2520%2520Molecular%2520Inference%26entry.906535625%3DBowen%2520Song%2520and%2520Jianshen%2520Zhu%2520and%2520Naveed%2520Ahmed%2520Azam%2520and%2520Kazuya%2520Haraguchi%2520and%2520Liang%2520Zhao%2520and%2520Tatsuya%2520Akutsu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520family%2520of%2520descriptors%2520of%2520chemical%2520graphs%252C%250Anamed%2520cycle-configuration%2520%2528CC%2529%252C%2520that%2520can%2520be%2520used%2520in%2520the%2520standard%2520%2522two-layered%250A%25282L%2529%2520model%2522%2520of%2520mol-infer%252C%2520a%2520molecular%2520inference%2520framework%2520based%2520on%2520mixed%250Ainteger%2520linear%2520programming%2520%2528MILP%2529%2520and%2520machine%2520learning%2520%2528ML%2529.%2520Proposed%250Adescriptors%2520capture%2520the%2520notion%2520of%2520ortho/meta/para%2520patterns%2520that%2520appear%2520in%250Aaromatic%2520rings%252C%2520which%2520has%2520been%2520impossible%2520in%2520the%2520framework%2520so%2520far.%250AComputational%2520experiments%2520show%2520that%252C%2520when%2520the%2520new%2520descriptors%2520are%2520supplied%252C%2520we%250Acan%2520construct%2520prediction%2520functions%2520of%2520similar%2520or%2520better%2520performance%2520for%2520all%2520of%250Athe%252027%2520tested%2520chemical%2520properties.%2520We%2520also%2520provide%2520an%2520MILP%2520formulation%2520that%250Aasks%2520for%2520a%2520chemical%2520graph%2520with%2520desired%2520properties%2520under%2520the%25202L%2520model%2520with%2520CC%250Adescriptors%2520%25282L%252BCC%2520model%2529.%2520We%2520show%2520that%2520a%2520chemical%2520graph%2520with%2520up%2520to%252050%250Anon-hydrogen%2520vertices%2520can%2520be%2520inferred%2520in%2520a%2520practical%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cycle-Configuration%3A%20A%20Novel%20Graph-theoretic%20Descriptor%20Set%20for%0A%20%20Molecular%20Inference&entry.906535625=Bowen%20Song%20and%20Jianshen%20Zhu%20and%20Naveed%20Ahmed%20Azam%20and%20Kazuya%20Haraguchi%20and%20Liang%20Zhao%20and%20Tatsuya%20Akutsu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20family%20of%20descriptors%20of%20chemical%20graphs%2C%0Anamed%20cycle-configuration%20%28CC%29%2C%20that%20can%20be%20used%20in%20the%20standard%20%22two-layered%0A%282L%29%20model%22%20of%20mol-infer%2C%20a%20molecular%20inference%20framework%20based%20on%20mixed%0Ainteger%20linear%20programming%20%28MILP%29%20and%20machine%20learning%20%28ML%29.%20Proposed%0Adescriptors%20capture%20the%20notion%20of%20ortho/meta/para%20patterns%20that%20appear%20in%0Aaromatic%20rings%2C%20which%20has%20been%20impossible%20in%20the%20framework%20so%20far.%0AComputational%20experiments%20show%20that%2C%20when%20the%20new%20descriptors%20are%20supplied%2C%20we%0Acan%20construct%20prediction%20functions%20of%20similar%20or%20better%20performance%20for%20all%20of%0Athe%2027%20tested%20chemical%20properties.%20We%20also%20provide%20an%20MILP%20formulation%20that%0Aasks%20for%20a%20chemical%20graph%20with%20desired%20properties%20under%20the%202L%20model%20with%20CC%0Adescriptors%20%282L%2BCC%20model%29.%20We%20show%20that%20a%20chemical%20graph%20with%20up%20to%2050%0Anon-hydrogen%20vertices%20can%20be%20inferred%20in%20a%20practical%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05136v1&entry.124074799=Read"},
{"title": "ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability", "author": "Xiao Wang and Siyan Liu and Aristeidis Tsaris and Jong-Youl Choi and Ashwin Aji and Ming Fan and Wei Zhang and Junqi Yin and Moetasim Ashfaq and Dan Lu and Prasanna Balaprakash", "abstract": "  Earth system predictability is challenged by the complexity of environmental\ndynamics and the multitude of variables involved. Current AI foundation models,\nalthough advanced by leveraging large and heterogeneous data, are often\nconstrained by their size and data integration, limiting their effectiveness in\naddressing the full range of Earth system prediction challenges. To overcome\nthese limitations, we introduce the Oak Ridge Base Foundation Model for Earth\nSystem Predictability (ORBIT), an advanced vision transformer model that scales\nup to 113 billion parameters using a novel hybrid tensor-data orthogonal\nparallelism technique. As the largest model of its kind, ORBIT surpasses the\ncurrent climate AI foundation model size by a thousandfold. Performance scaling\ntests conducted on the Frontier supercomputer have demonstrated that ORBIT\nachieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling\nefficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughs\nestablish new advances in AI-driven climate modeling and demonstrate promise to\nsignificantly improve the Earth system predictability.\n", "link": "http://arxiv.org/abs/2404.14712v4", "date": "2024-08-09", "relevancy": 1.3751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4739}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4734}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ORBIT%3A%20Oak%20Ridge%20Base%20Foundation%20Model%20for%20Earth%20System%20Predictability&body=Title%3A%20ORBIT%3A%20Oak%20Ridge%20Base%20Foundation%20Model%20for%20Earth%20System%20Predictability%0AAuthor%3A%20Xiao%20Wang%20and%20Siyan%20Liu%20and%20Aristeidis%20Tsaris%20and%20Jong-Youl%20Choi%20and%20Ashwin%20Aji%20and%20Ming%20Fan%20and%20Wei%20Zhang%20and%20Junqi%20Yin%20and%20Moetasim%20Ashfaq%20and%20Dan%20Lu%20and%20Prasanna%20Balaprakash%0AAbstract%3A%20%20%20Earth%20system%20predictability%20is%20challenged%20by%20the%20complexity%20of%20environmental%0Adynamics%20and%20the%20multitude%20of%20variables%20involved.%20Current%20AI%20foundation%20models%2C%0Aalthough%20advanced%20by%20leveraging%20large%20and%20heterogeneous%20data%2C%20are%20often%0Aconstrained%20by%20their%20size%20and%20data%20integration%2C%20limiting%20their%20effectiveness%20in%0Aaddressing%20the%20full%20range%20of%20Earth%20system%20prediction%20challenges.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20the%20Oak%20Ridge%20Base%20Foundation%20Model%20for%20Earth%0ASystem%20Predictability%20%28ORBIT%29%2C%20an%20advanced%20vision%20transformer%20model%20that%20scales%0Aup%20to%20113%20billion%20parameters%20using%20a%20novel%20hybrid%20tensor-data%20orthogonal%0Aparallelism%20technique.%20As%20the%20largest%20model%20of%20its%20kind%2C%20ORBIT%20surpasses%20the%0Acurrent%20climate%20AI%20foundation%20model%20size%20by%20a%20thousandfold.%20Performance%20scaling%0Atests%20conducted%20on%20the%20Frontier%20supercomputer%20have%20demonstrated%20that%20ORBIT%0Aachieves%20684%20petaFLOPS%20to%201.6%20exaFLOPS%20sustained%20throughput%2C%20with%20scaling%0Aefficiency%20maintained%20at%2041%25%20to%2085%25%20across%2049%2C152%20AMD%20GPUs.%20These%20breakthroughs%0Aestablish%20new%20advances%20in%20AI-driven%20climate%20modeling%20and%20demonstrate%20promise%20to%0Asignificantly%20improve%20the%20Earth%20system%20predictability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14712v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DORBIT%253A%2520Oak%2520Ridge%2520Base%2520Foundation%2520Model%2520for%2520Earth%2520System%2520Predictability%26entry.906535625%3DXiao%2520Wang%2520and%2520Siyan%2520Liu%2520and%2520Aristeidis%2520Tsaris%2520and%2520Jong-Youl%2520Choi%2520and%2520Ashwin%2520Aji%2520and%2520Ming%2520Fan%2520and%2520Wei%2520Zhang%2520and%2520Junqi%2520Yin%2520and%2520Moetasim%2520Ashfaq%2520and%2520Dan%2520Lu%2520and%2520Prasanna%2520Balaprakash%26entry.1292438233%3D%2520%2520Earth%2520system%2520predictability%2520is%2520challenged%2520by%2520the%2520complexity%2520of%2520environmental%250Adynamics%2520and%2520the%2520multitude%2520of%2520variables%2520involved.%2520Current%2520AI%2520foundation%2520models%252C%250Aalthough%2520advanced%2520by%2520leveraging%2520large%2520and%2520heterogeneous%2520data%252C%2520are%2520often%250Aconstrained%2520by%2520their%2520size%2520and%2520data%2520integration%252C%2520limiting%2520their%2520effectiveness%2520in%250Aaddressing%2520the%2520full%2520range%2520of%2520Earth%2520system%2520prediction%2520challenges.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520introduce%2520the%2520Oak%2520Ridge%2520Base%2520Foundation%2520Model%2520for%2520Earth%250ASystem%2520Predictability%2520%2528ORBIT%2529%252C%2520an%2520advanced%2520vision%2520transformer%2520model%2520that%2520scales%250Aup%2520to%2520113%2520billion%2520parameters%2520using%2520a%2520novel%2520hybrid%2520tensor-data%2520orthogonal%250Aparallelism%2520technique.%2520As%2520the%2520largest%2520model%2520of%2520its%2520kind%252C%2520ORBIT%2520surpasses%2520the%250Acurrent%2520climate%2520AI%2520foundation%2520model%2520size%2520by%2520a%2520thousandfold.%2520Performance%2520scaling%250Atests%2520conducted%2520on%2520the%2520Frontier%2520supercomputer%2520have%2520demonstrated%2520that%2520ORBIT%250Aachieves%2520684%2520petaFLOPS%2520to%25201.6%2520exaFLOPS%2520sustained%2520throughput%252C%2520with%2520scaling%250Aefficiency%2520maintained%2520at%252041%2525%2520to%252085%2525%2520across%252049%252C152%2520AMD%2520GPUs.%2520These%2520breakthroughs%250Aestablish%2520new%2520advances%2520in%2520AI-driven%2520climate%2520modeling%2520and%2520demonstrate%2520promise%2520to%250Asignificantly%2520improve%2520the%2520Earth%2520system%2520predictability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14712v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ORBIT%3A%20Oak%20Ridge%20Base%20Foundation%20Model%20for%20Earth%20System%20Predictability&entry.906535625=Xiao%20Wang%20and%20Siyan%20Liu%20and%20Aristeidis%20Tsaris%20and%20Jong-Youl%20Choi%20and%20Ashwin%20Aji%20and%20Ming%20Fan%20and%20Wei%20Zhang%20and%20Junqi%20Yin%20and%20Moetasim%20Ashfaq%20and%20Dan%20Lu%20and%20Prasanna%20Balaprakash&entry.1292438233=%20%20Earth%20system%20predictability%20is%20challenged%20by%20the%20complexity%20of%20environmental%0Adynamics%20and%20the%20multitude%20of%20variables%20involved.%20Current%20AI%20foundation%20models%2C%0Aalthough%20advanced%20by%20leveraging%20large%20and%20heterogeneous%20data%2C%20are%20often%0Aconstrained%20by%20their%20size%20and%20data%20integration%2C%20limiting%20their%20effectiveness%20in%0Aaddressing%20the%20full%20range%20of%20Earth%20system%20prediction%20challenges.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20the%20Oak%20Ridge%20Base%20Foundation%20Model%20for%20Earth%0ASystem%20Predictability%20%28ORBIT%29%2C%20an%20advanced%20vision%20transformer%20model%20that%20scales%0Aup%20to%20113%20billion%20parameters%20using%20a%20novel%20hybrid%20tensor-data%20orthogonal%0Aparallelism%20technique.%20As%20the%20largest%20model%20of%20its%20kind%2C%20ORBIT%20surpasses%20the%0Acurrent%20climate%20AI%20foundation%20model%20size%20by%20a%20thousandfold.%20Performance%20scaling%0Atests%20conducted%20on%20the%20Frontier%20supercomputer%20have%20demonstrated%20that%20ORBIT%0Aachieves%20684%20petaFLOPS%20to%201.6%20exaFLOPS%20sustained%20throughput%2C%20with%20scaling%0Aefficiency%20maintained%20at%2041%25%20to%2085%25%20across%2049%2C152%20AMD%20GPUs.%20These%20breakthroughs%0Aestablish%20new%20advances%20in%20AI-driven%20climate%20modeling%20and%20demonstrate%20promise%20to%0Asignificantly%20improve%20the%20Earth%20system%20predictability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14712v4&entry.124074799=Read"},
{"title": "A Context-Contrastive Inference Approach To Partial Diacritization", "author": "Muhammad ElNokrashy and Badr AlKhamissi", "abstract": "  Diacritization plays a pivotal role in improving readability and\ndisambiguating the meaning of Arabic texts. Efforts have so far focused on\nmarking every eligible character (Full Diacritization). Comparatively\noverlooked, Partial Diacritzation (PD) is the selection of a subset of\ncharacters to be marked to aid comprehension where needed. Research has\nindicated that excessive diacritic marks can hinder skilled readers -- reducing\nreading speed and accuracy. We conduct a behavioral experiment and show that\npartially marked text is often easier to read than fully marked text, and\nsometimes easier than plain text. In this light, we introduce\nContext-Contrastive Partial Diacritization (CCPD) -- a novel approach to PD\nwhich integrates seamlessly with existing Arabic diacritization systems. CCPD\nprocesses each word twice, once with context and once without, and diacritizes\nonly the characters with disparities between the two inferences. Further, we\nintroduce novel indicators for measuring partial diacritization quality,\nessential for establishing this as a machine learning task. Lastly, we\nintroduce TD2, a Transformer-variant of an established model which offers a\nmarkedly different performance profile on our proposed indicators compared to\nall other known systems.\n", "link": "http://arxiv.org/abs/2401.08919v3", "date": "2024-08-09", "relevancy": 1.2714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4273}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4213}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Context-Contrastive%20Inference%20Approach%20To%20Partial%20Diacritization&body=Title%3A%20A%20Context-Contrastive%20Inference%20Approach%20To%20Partial%20Diacritization%0AAuthor%3A%20Muhammad%20ElNokrashy%20and%20Badr%20AlKhamissi%0AAbstract%3A%20%20%20Diacritization%20plays%20a%20pivotal%20role%20in%20improving%20readability%20and%0Adisambiguating%20the%20meaning%20of%20Arabic%20texts.%20Efforts%20have%20so%20far%20focused%20on%0Amarking%20every%20eligible%20character%20%28Full%20Diacritization%29.%20Comparatively%0Aoverlooked%2C%20Partial%20Diacritzation%20%28PD%29%20is%20the%20selection%20of%20a%20subset%20of%0Acharacters%20to%20be%20marked%20to%20aid%20comprehension%20where%20needed.%20Research%20has%0Aindicated%20that%20excessive%20diacritic%20marks%20can%20hinder%20skilled%20readers%20--%20reducing%0Areading%20speed%20and%20accuracy.%20We%20conduct%20a%20behavioral%20experiment%20and%20show%20that%0Apartially%20marked%20text%20is%20often%20easier%20to%20read%20than%20fully%20marked%20text%2C%20and%0Asometimes%20easier%20than%20plain%20text.%20In%20this%20light%2C%20we%20introduce%0AContext-Contrastive%20Partial%20Diacritization%20%28CCPD%29%20--%20a%20novel%20approach%20to%20PD%0Awhich%20integrates%20seamlessly%20with%20existing%20Arabic%20diacritization%20systems.%20CCPD%0Aprocesses%20each%20word%20twice%2C%20once%20with%20context%20and%20once%20without%2C%20and%20diacritizes%0Aonly%20the%20characters%20with%20disparities%20between%20the%20two%20inferences.%20Further%2C%20we%0Aintroduce%20novel%20indicators%20for%20measuring%20partial%20diacritization%20quality%2C%0Aessential%20for%20establishing%20this%20as%20a%20machine%20learning%20task.%20Lastly%2C%20we%0Aintroduce%20TD2%2C%20a%20Transformer-variant%20of%20an%20established%20model%20which%20offers%20a%0Amarkedly%20different%20performance%20profile%20on%20our%20proposed%20indicators%20compared%20to%0Aall%20other%20known%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08919v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Context-Contrastive%2520Inference%2520Approach%2520To%2520Partial%2520Diacritization%26entry.906535625%3DMuhammad%2520ElNokrashy%2520and%2520Badr%2520AlKhamissi%26entry.1292438233%3D%2520%2520Diacritization%2520plays%2520a%2520pivotal%2520role%2520in%2520improving%2520readability%2520and%250Adisambiguating%2520the%2520meaning%2520of%2520Arabic%2520texts.%2520Efforts%2520have%2520so%2520far%2520focused%2520on%250Amarking%2520every%2520eligible%2520character%2520%2528Full%2520Diacritization%2529.%2520Comparatively%250Aoverlooked%252C%2520Partial%2520Diacritzation%2520%2528PD%2529%2520is%2520the%2520selection%2520of%2520a%2520subset%2520of%250Acharacters%2520to%2520be%2520marked%2520to%2520aid%2520comprehension%2520where%2520needed.%2520Research%2520has%250Aindicated%2520that%2520excessive%2520diacritic%2520marks%2520can%2520hinder%2520skilled%2520readers%2520--%2520reducing%250Areading%2520speed%2520and%2520accuracy.%2520We%2520conduct%2520a%2520behavioral%2520experiment%2520and%2520show%2520that%250Apartially%2520marked%2520text%2520is%2520often%2520easier%2520to%2520read%2520than%2520fully%2520marked%2520text%252C%2520and%250Asometimes%2520easier%2520than%2520plain%2520text.%2520In%2520this%2520light%252C%2520we%2520introduce%250AContext-Contrastive%2520Partial%2520Diacritization%2520%2528CCPD%2529%2520--%2520a%2520novel%2520approach%2520to%2520PD%250Awhich%2520integrates%2520seamlessly%2520with%2520existing%2520Arabic%2520diacritization%2520systems.%2520CCPD%250Aprocesses%2520each%2520word%2520twice%252C%2520once%2520with%2520context%2520and%2520once%2520without%252C%2520and%2520diacritizes%250Aonly%2520the%2520characters%2520with%2520disparities%2520between%2520the%2520two%2520inferences.%2520Further%252C%2520we%250Aintroduce%2520novel%2520indicators%2520for%2520measuring%2520partial%2520diacritization%2520quality%252C%250Aessential%2520for%2520establishing%2520this%2520as%2520a%2520machine%2520learning%2520task.%2520Lastly%252C%2520we%250Aintroduce%2520TD2%252C%2520a%2520Transformer-variant%2520of%2520an%2520established%2520model%2520which%2520offers%2520a%250Amarkedly%2520different%2520performance%2520profile%2520on%2520our%2520proposed%2520indicators%2520compared%2520to%250Aall%2520other%2520known%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08919v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Context-Contrastive%20Inference%20Approach%20To%20Partial%20Diacritization&entry.906535625=Muhammad%20ElNokrashy%20and%20Badr%20AlKhamissi&entry.1292438233=%20%20Diacritization%20plays%20a%20pivotal%20role%20in%20improving%20readability%20and%0Adisambiguating%20the%20meaning%20of%20Arabic%20texts.%20Efforts%20have%20so%20far%20focused%20on%0Amarking%20every%20eligible%20character%20%28Full%20Diacritization%29.%20Comparatively%0Aoverlooked%2C%20Partial%20Diacritzation%20%28PD%29%20is%20the%20selection%20of%20a%20subset%20of%0Acharacters%20to%20be%20marked%20to%20aid%20comprehension%20where%20needed.%20Research%20has%0Aindicated%20that%20excessive%20diacritic%20marks%20can%20hinder%20skilled%20readers%20--%20reducing%0Areading%20speed%20and%20accuracy.%20We%20conduct%20a%20behavioral%20experiment%20and%20show%20that%0Apartially%20marked%20text%20is%20often%20easier%20to%20read%20than%20fully%20marked%20text%2C%20and%0Asometimes%20easier%20than%20plain%20text.%20In%20this%20light%2C%20we%20introduce%0AContext-Contrastive%20Partial%20Diacritization%20%28CCPD%29%20--%20a%20novel%20approach%20to%20PD%0Awhich%20integrates%20seamlessly%20with%20existing%20Arabic%20diacritization%20systems.%20CCPD%0Aprocesses%20each%20word%20twice%2C%20once%20with%20context%20and%20once%20without%2C%20and%20diacritizes%0Aonly%20the%20characters%20with%20disparities%20between%20the%20two%20inferences.%20Further%2C%20we%0Aintroduce%20novel%20indicators%20for%20measuring%20partial%20diacritization%20quality%2C%0Aessential%20for%20establishing%20this%20as%20a%20machine%20learning%20task.%20Lastly%2C%20we%0Aintroduce%20TD2%2C%20a%20Transformer-variant%20of%20an%20established%20model%20which%20offers%20a%0Amarkedly%20different%20performance%20profile%20on%20our%20proposed%20indicators%20compared%20to%0Aall%20other%20known%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08919v3&entry.124074799=Read"},
{"title": "Depth Helps: Improving Pre-trained RGB-based Policy with Depth\n  Information Injection", "author": "Xincheng Pang and Wenke Xia and Zhigang Wang and Bin Zhao and Di Hu and Dong Wang and Xuelong Li", "abstract": "  3D perception ability is crucial for generalizable robotic manipulation.\nWhile recent foundation models have made significant strides in perception and\ndecision-making with RGB-based input, their lack of 3D perception limits their\neffectiveness in fine-grained robotic manipulation tasks. To address these\nlimitations, we propose a Depth Information Injection ($\\bold{DI}^{\\bold{2}}$)\nframework that leverages the RGB-Depth modality for policy fine-tuning, while\nrelying solely on RGB images for robust and efficient deployment. Concretely,\nwe introduce the Depth Completion Module (DCM) to extract the spatial prior\nknowledge related to depth information and generate virtual depth information\nfrom RGB inputs to aid policy deployment. Further, we propose the Depth-Aware\nCodebook (DAC) to eliminate noise and reduce the cumulative error from the\ndepth prediction. In the inference phase, this framework employs RGB inputs and\naccurately predicted depth data to generate the manipulation action. We conduct\nexperiments on simulated LIBERO environments and real-world scenarios, and the\nexperiment results prove that our method could effectively enhance the\npre-trained RGB-based policy with 3D perception ability for robotic\nmanipulation. The website is released at\nhttps://gewu-lab.github.io/DepthHelps-IROS2024.\n", "link": "http://arxiv.org/abs/2408.05107v1", "date": "2024-08-09", "relevancy": 1.1448, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5893}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5803}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Helps%3A%20Improving%20Pre-trained%20RGB-based%20Policy%20with%20Depth%0A%20%20Information%20Injection&body=Title%3A%20Depth%20Helps%3A%20Improving%20Pre-trained%20RGB-based%20Policy%20with%20Depth%0A%20%20Information%20Injection%0AAuthor%3A%20Xincheng%20Pang%20and%20Wenke%20Xia%20and%20Zhigang%20Wang%20and%20Bin%20Zhao%20and%20Di%20Hu%20and%20Dong%20Wang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%203D%20perception%20ability%20is%20crucial%20for%20generalizable%20robotic%20manipulation.%0AWhile%20recent%20foundation%20models%20have%20made%20significant%20strides%20in%20perception%20and%0Adecision-making%20with%20RGB-based%20input%2C%20their%20lack%20of%203D%20perception%20limits%20their%0Aeffectiveness%20in%20fine-grained%20robotic%20manipulation%20tasks.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20Depth%20Information%20Injection%20%28%24%5Cbold%7BDI%7D%5E%7B%5Cbold%7B2%7D%7D%24%29%0Aframework%20that%20leverages%20the%20RGB-Depth%20modality%20for%20policy%20fine-tuning%2C%20while%0Arelying%20solely%20on%20RGB%20images%20for%20robust%20and%20efficient%20deployment.%20Concretely%2C%0Awe%20introduce%20the%20Depth%20Completion%20Module%20%28DCM%29%20to%20extract%20the%20spatial%20prior%0Aknowledge%20related%20to%20depth%20information%20and%20generate%20virtual%20depth%20information%0Afrom%20RGB%20inputs%20to%20aid%20policy%20deployment.%20Further%2C%20we%20propose%20the%20Depth-Aware%0ACodebook%20%28DAC%29%20to%20eliminate%20noise%20and%20reduce%20the%20cumulative%20error%20from%20the%0Adepth%20prediction.%20In%20the%20inference%20phase%2C%20this%20framework%20employs%20RGB%20inputs%20and%0Aaccurately%20predicted%20depth%20data%20to%20generate%20the%20manipulation%20action.%20We%20conduct%0Aexperiments%20on%20simulated%20LIBERO%20environments%20and%20real-world%20scenarios%2C%20and%20the%0Aexperiment%20results%20prove%20that%20our%20method%20could%20effectively%20enhance%20the%0Apre-trained%20RGB-based%20policy%20with%203D%20perception%20ability%20for%20robotic%0Amanipulation.%20The%20website%20is%20released%20at%0Ahttps%3A//gewu-lab.github.io/DepthHelps-IROS2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Helps%253A%2520Improving%2520Pre-trained%2520RGB-based%2520Policy%2520with%2520Depth%250A%2520%2520Information%2520Injection%26entry.906535625%3DXincheng%2520Pang%2520and%2520Wenke%2520Xia%2520and%2520Zhigang%2520Wang%2520and%2520Bin%2520Zhao%2520and%2520Di%2520Hu%2520and%2520Dong%2520Wang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%25203D%2520perception%2520ability%2520is%2520crucial%2520for%2520generalizable%2520robotic%2520manipulation.%250AWhile%2520recent%2520foundation%2520models%2520have%2520made%2520significant%2520strides%2520in%2520perception%2520and%250Adecision-making%2520with%2520RGB-based%2520input%252C%2520their%2520lack%2520of%25203D%2520perception%2520limits%2520their%250Aeffectiveness%2520in%2520fine-grained%2520robotic%2520manipulation%2520tasks.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520Depth%2520Information%2520Injection%2520%2528%2524%255Cbold%257BDI%257D%255E%257B%255Cbold%257B2%257D%257D%2524%2529%250Aframework%2520that%2520leverages%2520the%2520RGB-Depth%2520modality%2520for%2520policy%2520fine-tuning%252C%2520while%250Arelying%2520solely%2520on%2520RGB%2520images%2520for%2520robust%2520and%2520efficient%2520deployment.%2520Concretely%252C%250Awe%2520introduce%2520the%2520Depth%2520Completion%2520Module%2520%2528DCM%2529%2520to%2520extract%2520the%2520spatial%2520prior%250Aknowledge%2520related%2520to%2520depth%2520information%2520and%2520generate%2520virtual%2520depth%2520information%250Afrom%2520RGB%2520inputs%2520to%2520aid%2520policy%2520deployment.%2520Further%252C%2520we%2520propose%2520the%2520Depth-Aware%250ACodebook%2520%2528DAC%2529%2520to%2520eliminate%2520noise%2520and%2520reduce%2520the%2520cumulative%2520error%2520from%2520the%250Adepth%2520prediction.%2520In%2520the%2520inference%2520phase%252C%2520this%2520framework%2520employs%2520RGB%2520inputs%2520and%250Aaccurately%2520predicted%2520depth%2520data%2520to%2520generate%2520the%2520manipulation%2520action.%2520We%2520conduct%250Aexperiments%2520on%2520simulated%2520LIBERO%2520environments%2520and%2520real-world%2520scenarios%252C%2520and%2520the%250Aexperiment%2520results%2520prove%2520that%2520our%2520method%2520could%2520effectively%2520enhance%2520the%250Apre-trained%2520RGB-based%2520policy%2520with%25203D%2520perception%2520ability%2520for%2520robotic%250Amanipulation.%2520The%2520website%2520is%2520released%2520at%250Ahttps%253A//gewu-lab.github.io/DepthHelps-IROS2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Helps%3A%20Improving%20Pre-trained%20RGB-based%20Policy%20with%20Depth%0A%20%20Information%20Injection&entry.906535625=Xincheng%20Pang%20and%20Wenke%20Xia%20and%20Zhigang%20Wang%20and%20Bin%20Zhao%20and%20Di%20Hu%20and%20Dong%20Wang%20and%20Xuelong%20Li&entry.1292438233=%20%203D%20perception%20ability%20is%20crucial%20for%20generalizable%20robotic%20manipulation.%0AWhile%20recent%20foundation%20models%20have%20made%20significant%20strides%20in%20perception%20and%0Adecision-making%20with%20RGB-based%20input%2C%20their%20lack%20of%203D%20perception%20limits%20their%0Aeffectiveness%20in%20fine-grained%20robotic%20manipulation%20tasks.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20Depth%20Information%20Injection%20%28%24%5Cbold%7BDI%7D%5E%7B%5Cbold%7B2%7D%7D%24%29%0Aframework%20that%20leverages%20the%20RGB-Depth%20modality%20for%20policy%20fine-tuning%2C%20while%0Arelying%20solely%20on%20RGB%20images%20for%20robust%20and%20efficient%20deployment.%20Concretely%2C%0Awe%20introduce%20the%20Depth%20Completion%20Module%20%28DCM%29%20to%20extract%20the%20spatial%20prior%0Aknowledge%20related%20to%20depth%20information%20and%20generate%20virtual%20depth%20information%0Afrom%20RGB%20inputs%20to%20aid%20policy%20deployment.%20Further%2C%20we%20propose%20the%20Depth-Aware%0ACodebook%20%28DAC%29%20to%20eliminate%20noise%20and%20reduce%20the%20cumulative%20error%20from%20the%0Adepth%20prediction.%20In%20the%20inference%20phase%2C%20this%20framework%20employs%20RGB%20inputs%20and%0Aaccurately%20predicted%20depth%20data%20to%20generate%20the%20manipulation%20action.%20We%20conduct%0Aexperiments%20on%20simulated%20LIBERO%20environments%20and%20real-world%20scenarios%2C%20and%20the%0Aexperiment%20results%20prove%20that%20our%20method%20could%20effectively%20enhance%20the%0Apre-trained%20RGB-based%20policy%20with%203D%20perception%20ability%20for%20robotic%0Amanipulation.%20The%20website%20is%20released%20at%0Ahttps%3A//gewu-lab.github.io/DepthHelps-IROS2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05107v1&entry.124074799=Read"},
{"title": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations\n  in LLM-based Application Frameworks", "author": "Gianluca De Stefano and Giancarlo Pellegrino and Lea Sch\u00f6nherr", "abstract": "  Retrieval Augmented Generation (RAG) is a technique commonly used to equip\nmodels with out of distribution knowledge. This process involves collecting,\nindexing, retrieving, and providing information to an LLM for generating\nresponses. Despite its growing popularity due to its flexibility and low cost,\nthe security implications of RAG have not been extensively studied. The data\nfor such systems are often collected from public sources, providing an attacker\na gateway for indirect prompt injections to manipulate the responses of the\nmodel. In this paper, we investigate the security of RAG systems against\nend-to-end indirect prompt manipulations. First, we review existing RAG\nframework pipelines deriving a prototypical architecture and identifying\npotentially critical configuration parameters. We then examine prior works\nsearching for techniques that attackers can use to perform indirect prompt\nmanipulations. Finally, implemented Rag n Roll, a framework to determine the\neffectiveness of attacks against end-to-end RAG applications. Our results show\nthat existing attacks are mostly optimized to boost the ranking of malicious\ndocuments during the retrieval phase. However, a higher rank does not\nimmediately translate into a reliable attack. Most attacks, against various\nconfigurations, settle around a 40% success rate, which could rise to 60% when\nconsidering ambiguous answers as successful attacks (those that include the\nexpected benign one as well). Additionally, when using unoptimized documents,\nattackers deploying two of them (or more) for a target query can achieve\nsimilar results as those using optimized ones. Finally, exploration of the\nconfiguration space of a RAG showed limited impact in thwarting the attacks,\nwhere the most successful combination severely undermines functionality.\n", "link": "http://arxiv.org/abs/2408.05025v1", "date": "2024-08-09", "relevancy": 0.8692, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4662}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rag%20and%20Roll%3A%20An%20End-to-End%20Evaluation%20of%20Indirect%20Prompt%20Manipulations%0A%20%20in%20LLM-based%20Application%20Frameworks&body=Title%3A%20Rag%20and%20Roll%3A%20An%20End-to-End%20Evaluation%20of%20Indirect%20Prompt%20Manipulations%0A%20%20in%20LLM-based%20Application%20Frameworks%0AAuthor%3A%20Gianluca%20De%20Stefano%20and%20Giancarlo%20Pellegrino%20and%20Lea%20Sch%C3%B6nherr%0AAbstract%3A%20%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20is%20a%20technique%20commonly%20used%20to%20equip%0Amodels%20with%20out%20of%20distribution%20knowledge.%20This%20process%20involves%20collecting%2C%0Aindexing%2C%20retrieving%2C%20and%20providing%20information%20to%20an%20LLM%20for%20generating%0Aresponses.%20Despite%20its%20growing%20popularity%20due%20to%20its%20flexibility%20and%20low%20cost%2C%0Athe%20security%20implications%20of%20RAG%20have%20not%20been%20extensively%20studied.%20The%20data%0Afor%20such%20systems%20are%20often%20collected%20from%20public%20sources%2C%20providing%20an%20attacker%0Aa%20gateway%20for%20indirect%20prompt%20injections%20to%20manipulate%20the%20responses%20of%20the%0Amodel.%20In%20this%20paper%2C%20we%20investigate%20the%20security%20of%20RAG%20systems%20against%0Aend-to-end%20indirect%20prompt%20manipulations.%20First%2C%20we%20review%20existing%20RAG%0Aframework%20pipelines%20deriving%20a%20prototypical%20architecture%20and%20identifying%0Apotentially%20critical%20configuration%20parameters.%20We%20then%20examine%20prior%20works%0Asearching%20for%20techniques%20that%20attackers%20can%20use%20to%20perform%20indirect%20prompt%0Amanipulations.%20Finally%2C%20implemented%20Rag%20n%20Roll%2C%20a%20framework%20to%20determine%20the%0Aeffectiveness%20of%20attacks%20against%20end-to-end%20RAG%20applications.%20Our%20results%20show%0Athat%20existing%20attacks%20are%20mostly%20optimized%20to%20boost%20the%20ranking%20of%20malicious%0Adocuments%20during%20the%20retrieval%20phase.%20However%2C%20a%20higher%20rank%20does%20not%0Aimmediately%20translate%20into%20a%20reliable%20attack.%20Most%20attacks%2C%20against%20various%0Aconfigurations%2C%20settle%20around%20a%2040%25%20success%20rate%2C%20which%20could%20rise%20to%2060%25%20when%0Aconsidering%20ambiguous%20answers%20as%20successful%20attacks%20%28those%20that%20include%20the%0Aexpected%20benign%20one%20as%20well%29.%20Additionally%2C%20when%20using%20unoptimized%20documents%2C%0Aattackers%20deploying%20two%20of%20them%20%28or%20more%29%20for%20a%20target%20query%20can%20achieve%0Asimilar%20results%20as%20those%20using%20optimized%20ones.%20Finally%2C%20exploration%20of%20the%0Aconfiguration%20space%20of%20a%20RAG%20showed%20limited%20impact%20in%20thwarting%20the%20attacks%2C%0Awhere%20the%20most%20successful%20combination%20severely%20undermines%20functionality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRag%2520and%2520Roll%253A%2520An%2520End-to-End%2520Evaluation%2520of%2520Indirect%2520Prompt%2520Manipulations%250A%2520%2520in%2520LLM-based%2520Application%2520Frameworks%26entry.906535625%3DGianluca%2520De%2520Stefano%2520and%2520Giancarlo%2520Pellegrino%2520and%2520Lea%2520Sch%25C3%25B6nherr%26entry.1292438233%3D%2520%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520is%2520a%2520technique%2520commonly%2520used%2520to%2520equip%250Amodels%2520with%2520out%2520of%2520distribution%2520knowledge.%2520This%2520process%2520involves%2520collecting%252C%250Aindexing%252C%2520retrieving%252C%2520and%2520providing%2520information%2520to%2520an%2520LLM%2520for%2520generating%250Aresponses.%2520Despite%2520its%2520growing%2520popularity%2520due%2520to%2520its%2520flexibility%2520and%2520low%2520cost%252C%250Athe%2520security%2520implications%2520of%2520RAG%2520have%2520not%2520been%2520extensively%2520studied.%2520The%2520data%250Afor%2520such%2520systems%2520are%2520often%2520collected%2520from%2520public%2520sources%252C%2520providing%2520an%2520attacker%250Aa%2520gateway%2520for%2520indirect%2520prompt%2520injections%2520to%2520manipulate%2520the%2520responses%2520of%2520the%250Amodel.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520security%2520of%2520RAG%2520systems%2520against%250Aend-to-end%2520indirect%2520prompt%2520manipulations.%2520First%252C%2520we%2520review%2520existing%2520RAG%250Aframework%2520pipelines%2520deriving%2520a%2520prototypical%2520architecture%2520and%2520identifying%250Apotentially%2520critical%2520configuration%2520parameters.%2520We%2520then%2520examine%2520prior%2520works%250Asearching%2520for%2520techniques%2520that%2520attackers%2520can%2520use%2520to%2520perform%2520indirect%2520prompt%250Amanipulations.%2520Finally%252C%2520implemented%2520Rag%2520n%2520Roll%252C%2520a%2520framework%2520to%2520determine%2520the%250Aeffectiveness%2520of%2520attacks%2520against%2520end-to-end%2520RAG%2520applications.%2520Our%2520results%2520show%250Athat%2520existing%2520attacks%2520are%2520mostly%2520optimized%2520to%2520boost%2520the%2520ranking%2520of%2520malicious%250Adocuments%2520during%2520the%2520retrieval%2520phase.%2520However%252C%2520a%2520higher%2520rank%2520does%2520not%250Aimmediately%2520translate%2520into%2520a%2520reliable%2520attack.%2520Most%2520attacks%252C%2520against%2520various%250Aconfigurations%252C%2520settle%2520around%2520a%252040%2525%2520success%2520rate%252C%2520which%2520could%2520rise%2520to%252060%2525%2520when%250Aconsidering%2520ambiguous%2520answers%2520as%2520successful%2520attacks%2520%2528those%2520that%2520include%2520the%250Aexpected%2520benign%2520one%2520as%2520well%2529.%2520Additionally%252C%2520when%2520using%2520unoptimized%2520documents%252C%250Aattackers%2520deploying%2520two%2520of%2520them%2520%2528or%2520more%2529%2520for%2520a%2520target%2520query%2520can%2520achieve%250Asimilar%2520results%2520as%2520those%2520using%2520optimized%2520ones.%2520Finally%252C%2520exploration%2520of%2520the%250Aconfiguration%2520space%2520of%2520a%2520RAG%2520showed%2520limited%2520impact%2520in%2520thwarting%2520the%2520attacks%252C%250Awhere%2520the%2520most%2520successful%2520combination%2520severely%2520undermines%2520functionality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rag%20and%20Roll%3A%20An%20End-to-End%20Evaluation%20of%20Indirect%20Prompt%20Manipulations%0A%20%20in%20LLM-based%20Application%20Frameworks&entry.906535625=Gianluca%20De%20Stefano%20and%20Giancarlo%20Pellegrino%20and%20Lea%20Sch%C3%B6nherr&entry.1292438233=%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20is%20a%20technique%20commonly%20used%20to%20equip%0Amodels%20with%20out%20of%20distribution%20knowledge.%20This%20process%20involves%20collecting%2C%0Aindexing%2C%20retrieving%2C%20and%20providing%20information%20to%20an%20LLM%20for%20generating%0Aresponses.%20Despite%20its%20growing%20popularity%20due%20to%20its%20flexibility%20and%20low%20cost%2C%0Athe%20security%20implications%20of%20RAG%20have%20not%20been%20extensively%20studied.%20The%20data%0Afor%20such%20systems%20are%20often%20collected%20from%20public%20sources%2C%20providing%20an%20attacker%0Aa%20gateway%20for%20indirect%20prompt%20injections%20to%20manipulate%20the%20responses%20of%20the%0Amodel.%20In%20this%20paper%2C%20we%20investigate%20the%20security%20of%20RAG%20systems%20against%0Aend-to-end%20indirect%20prompt%20manipulations.%20First%2C%20we%20review%20existing%20RAG%0Aframework%20pipelines%20deriving%20a%20prototypical%20architecture%20and%20identifying%0Apotentially%20critical%20configuration%20parameters.%20We%20then%20examine%20prior%20works%0Asearching%20for%20techniques%20that%20attackers%20can%20use%20to%20perform%20indirect%20prompt%0Amanipulations.%20Finally%2C%20implemented%20Rag%20n%20Roll%2C%20a%20framework%20to%20determine%20the%0Aeffectiveness%20of%20attacks%20against%20end-to-end%20RAG%20applications.%20Our%20results%20show%0Athat%20existing%20attacks%20are%20mostly%20optimized%20to%20boost%20the%20ranking%20of%20malicious%0Adocuments%20during%20the%20retrieval%20phase.%20However%2C%20a%20higher%20rank%20does%20not%0Aimmediately%20translate%20into%20a%20reliable%20attack.%20Most%20attacks%2C%20against%20various%0Aconfigurations%2C%20settle%20around%20a%2040%25%20success%20rate%2C%20which%20could%20rise%20to%2060%25%20when%0Aconsidering%20ambiguous%20answers%20as%20successful%20attacks%20%28those%20that%20include%20the%0Aexpected%20benign%20one%20as%20well%29.%20Additionally%2C%20when%20using%20unoptimized%20documents%2C%0Aattackers%20deploying%20two%20of%20them%20%28or%20more%29%20for%20a%20target%20query%20can%20achieve%0Asimilar%20results%20as%20those%20using%20optimized%20ones.%20Finally%2C%20exploration%20of%20the%0Aconfiguration%20space%20of%20a%20RAG%20showed%20limited%20impact%20in%20thwarting%20the%20attacks%2C%0Awhere%20the%20most%20successful%20combination%20severely%20undermines%20functionality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05025v1&entry.124074799=Read"},
{"title": "Multi-Conditional Ranking with Large Language Models", "author": "Pouya Pezeshkpour and Estevam Hruschka", "abstract": "  Utilizing large language models (LLMs) to rank a set of items has become a\ncommon approach in recommendation and retrieval systems. Typically, these\nsystems focus on ordering a substantial number of documents in a monotonic\norder based on a given query. However, real-world scenarios often present a\ndifferent challenge: ranking a comparatively smaller set of items, but\naccording to a variety of diverse and occasionally conflicting conditions. In\nthis paper, we define and explore the task of multi-conditional ranking by\nintroducing MCRank, a benchmark tailored for assessing multi-conditional\nranking across various item types and conditions. Our analysis of LLMs using\nMCRank indicates a significant decrease in performance as the number and\ncomplexity of items and conditions grow. To overcome this limitation, we\npropose a novel decomposed reasoning method, consisting of EXtracting and\nSorting the conditions, and then Iteratively Ranking the items (EXSIR). Our\nextensive experiments show that this decomposed reasoning method enhances LLMs'\nperformance significantly, achieving up to a 12% improvement over existing\nLLMs. We also provide a detailed analysis of LLMs performance across various\ncondition categories, and examine the effectiveness of decomposition step.\nFurthermore, we compare our method with existing approaches such as\nChain-of-Thought and existing ranking models, demonstrating the superiority of\nour approach and complexity of MCR task. We released our dataset and code.\n", "link": "http://arxiv.org/abs/2404.00211v2", "date": "2024-08-09", "relevancy": 0.8846, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4384}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Conditional%20Ranking%20with%20Large%20Language%20Models&body=Title%3A%20Multi-Conditional%20Ranking%20with%20Large%20Language%20Models%0AAuthor%3A%20Pouya%20Pezeshkpour%20and%20Estevam%20Hruschka%0AAbstract%3A%20%20%20Utilizing%20large%20language%20models%20%28LLMs%29%20to%20rank%20a%20set%20of%20items%20has%20become%20a%0Acommon%20approach%20in%20recommendation%20and%20retrieval%20systems.%20Typically%2C%20these%0Asystems%20focus%20on%20ordering%20a%20substantial%20number%20of%20documents%20in%20a%20monotonic%0Aorder%20based%20on%20a%20given%20query.%20However%2C%20real-world%20scenarios%20often%20present%20a%0Adifferent%20challenge%3A%20ranking%20a%20comparatively%20smaller%20set%20of%20items%2C%20but%0Aaccording%20to%20a%20variety%20of%20diverse%20and%20occasionally%20conflicting%20conditions.%20In%0Athis%20paper%2C%20we%20define%20and%20explore%20the%20task%20of%20multi-conditional%20ranking%20by%0Aintroducing%20MCRank%2C%20a%20benchmark%20tailored%20for%20assessing%20multi-conditional%0Aranking%20across%20various%20item%20types%20and%20conditions.%20Our%20analysis%20of%20LLMs%20using%0AMCRank%20indicates%20a%20significant%20decrease%20in%20performance%20as%20the%20number%20and%0Acomplexity%20of%20items%20and%20conditions%20grow.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20novel%20decomposed%20reasoning%20method%2C%20consisting%20of%20EXtracting%20and%0ASorting%20the%20conditions%2C%20and%20then%20Iteratively%20Ranking%20the%20items%20%28EXSIR%29.%20Our%0Aextensive%20experiments%20show%20that%20this%20decomposed%20reasoning%20method%20enhances%20LLMs%27%0Aperformance%20significantly%2C%20achieving%20up%20to%20a%2012%25%20improvement%20over%20existing%0ALLMs.%20We%20also%20provide%20a%20detailed%20analysis%20of%20LLMs%20performance%20across%20various%0Acondition%20categories%2C%20and%20examine%20the%20effectiveness%20of%20decomposition%20step.%0AFurthermore%2C%20we%20compare%20our%20method%20with%20existing%20approaches%20such%20as%0AChain-of-Thought%20and%20existing%20ranking%20models%2C%20demonstrating%20the%20superiority%20of%0Aour%20approach%20and%20complexity%20of%20MCR%20task.%20We%20released%20our%20dataset%20and%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Conditional%2520Ranking%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DPouya%2520Pezeshkpour%2520and%2520Estevam%2520Hruschka%26entry.1292438233%3D%2520%2520Utilizing%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520rank%2520a%2520set%2520of%2520items%2520has%2520become%2520a%250Acommon%2520approach%2520in%2520recommendation%2520and%2520retrieval%2520systems.%2520Typically%252C%2520these%250Asystems%2520focus%2520on%2520ordering%2520a%2520substantial%2520number%2520of%2520documents%2520in%2520a%2520monotonic%250Aorder%2520based%2520on%2520a%2520given%2520query.%2520However%252C%2520real-world%2520scenarios%2520often%2520present%2520a%250Adifferent%2520challenge%253A%2520ranking%2520a%2520comparatively%2520smaller%2520set%2520of%2520items%252C%2520but%250Aaccording%2520to%2520a%2520variety%2520of%2520diverse%2520and%2520occasionally%2520conflicting%2520conditions.%2520In%250Athis%2520paper%252C%2520we%2520define%2520and%2520explore%2520the%2520task%2520of%2520multi-conditional%2520ranking%2520by%250Aintroducing%2520MCRank%252C%2520a%2520benchmark%2520tailored%2520for%2520assessing%2520multi-conditional%250Aranking%2520across%2520various%2520item%2520types%2520and%2520conditions.%2520Our%2520analysis%2520of%2520LLMs%2520using%250AMCRank%2520indicates%2520a%2520significant%2520decrease%2520in%2520performance%2520as%2520the%2520number%2520and%250Acomplexity%2520of%2520items%2520and%2520conditions%2520grow.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520a%2520novel%2520decomposed%2520reasoning%2520method%252C%2520consisting%2520of%2520EXtracting%2520and%250ASorting%2520the%2520conditions%252C%2520and%2520then%2520Iteratively%2520Ranking%2520the%2520items%2520%2528EXSIR%2529.%2520Our%250Aextensive%2520experiments%2520show%2520that%2520this%2520decomposed%2520reasoning%2520method%2520enhances%2520LLMs%2527%250Aperformance%2520significantly%252C%2520achieving%2520up%2520to%2520a%252012%2525%2520improvement%2520over%2520existing%250ALLMs.%2520We%2520also%2520provide%2520a%2520detailed%2520analysis%2520of%2520LLMs%2520performance%2520across%2520various%250Acondition%2520categories%252C%2520and%2520examine%2520the%2520effectiveness%2520of%2520decomposition%2520step.%250AFurthermore%252C%2520we%2520compare%2520our%2520method%2520with%2520existing%2520approaches%2520such%2520as%250AChain-of-Thought%2520and%2520existing%2520ranking%2520models%252C%2520demonstrating%2520the%2520superiority%2520of%250Aour%2520approach%2520and%2520complexity%2520of%2520MCR%2520task.%2520We%2520released%2520our%2520dataset%2520and%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Conditional%20Ranking%20with%20Large%20Language%20Models&entry.906535625=Pouya%20Pezeshkpour%20and%20Estevam%20Hruschka&entry.1292438233=%20%20Utilizing%20large%20language%20models%20%28LLMs%29%20to%20rank%20a%20set%20of%20items%20has%20become%20a%0Acommon%20approach%20in%20recommendation%20and%20retrieval%20systems.%20Typically%2C%20these%0Asystems%20focus%20on%20ordering%20a%20substantial%20number%20of%20documents%20in%20a%20monotonic%0Aorder%20based%20on%20a%20given%20query.%20However%2C%20real-world%20scenarios%20often%20present%20a%0Adifferent%20challenge%3A%20ranking%20a%20comparatively%20smaller%20set%20of%20items%2C%20but%0Aaccording%20to%20a%20variety%20of%20diverse%20and%20occasionally%20conflicting%20conditions.%20In%0Athis%20paper%2C%20we%20define%20and%20explore%20the%20task%20of%20multi-conditional%20ranking%20by%0Aintroducing%20MCRank%2C%20a%20benchmark%20tailored%20for%20assessing%20multi-conditional%0Aranking%20across%20various%20item%20types%20and%20conditions.%20Our%20analysis%20of%20LLMs%20using%0AMCRank%20indicates%20a%20significant%20decrease%20in%20performance%20as%20the%20number%20and%0Acomplexity%20of%20items%20and%20conditions%20grow.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20novel%20decomposed%20reasoning%20method%2C%20consisting%20of%20EXtracting%20and%0ASorting%20the%20conditions%2C%20and%20then%20Iteratively%20Ranking%20the%20items%20%28EXSIR%29.%20Our%0Aextensive%20experiments%20show%20that%20this%20decomposed%20reasoning%20method%20enhances%20LLMs%27%0Aperformance%20significantly%2C%20achieving%20up%20to%20a%2012%25%20improvement%20over%20existing%0ALLMs.%20We%20also%20provide%20a%20detailed%20analysis%20of%20LLMs%20performance%20across%20various%0Acondition%20categories%2C%20and%20examine%20the%20effectiveness%20of%20decomposition%20step.%0AFurthermore%2C%20we%20compare%20our%20method%20with%20existing%20approaches%20such%20as%0AChain-of-Thought%20and%20existing%20ranking%20models%2C%20demonstrating%20the%20superiority%20of%0Aour%20approach%20and%20complexity%20of%20MCR%20task.%20We%20released%20our%20dataset%20and%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00211v2&entry.124074799=Read"},
{"title": "AI-driven Java Performance Testing: Balancing Result Quality with\n  Testing Time", "author": "Luca Traini and Federico Di Menna and Vittorio Cortellessa", "abstract": "  Performance testing aims at uncovering efficiency issues of software systems.\nIn order to be both effective and practical, the design of a performance test\nmust achieve a reasonable trade-off between result quality and testing time.\nThis becomes particularly challenging in Java context, where the software\nundergoes a warm-up phase of execution, due to just-in-time compilation. During\nthis phase, performance measurements are subject to severe fluctuations, which\nmay adversely affect quality of performance test results. However, these\napproaches often provide suboptimal estimates of the warm-up phase, resulting\nin either insufficient or excessive warm-up iterations, which may degrade\nresult quality or increase testing time. There is still a lack of consensus on\nhow to properly address this problem. Here, we propose and study an AI-based\nframework to dynamically halt warm-up iterations at runtime. Specifically, our\nframework leverages recent advances in AI for Time Series Classification (TSC)\nto predict the end of the warm-up phase during test execution. We conduct\nexperiments by training three different TSC models on half a million of\nmeasurement segments obtained from JMH microbenchmark executions. We find that\nour framework significantly improves the accuracy of the warm-up estimates\nprovided by state-of-practice and state-of-the-art methods. This higher\nestimation accuracy results in a net improvement in either result quality or\ntesting time for up to +35.3% of the microbenchmarks. Our study highlights that\nintegrating AI to dynamically estimate the end of the warm-up phase can enhance\nthe cost-effectiveness of Java performance testing.\n", "link": "http://arxiv.org/abs/2408.05100v1", "date": "2024-08-09", "relevancy": 1.2363, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4259}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4116}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-driven%20Java%20Performance%20Testing%3A%20Balancing%20Result%20Quality%20with%0A%20%20Testing%20Time&body=Title%3A%20AI-driven%20Java%20Performance%20Testing%3A%20Balancing%20Result%20Quality%20with%0A%20%20Testing%20Time%0AAuthor%3A%20Luca%20Traini%20and%20Federico%20Di%20Menna%20and%20Vittorio%20Cortellessa%0AAbstract%3A%20%20%20Performance%20testing%20aims%20at%20uncovering%20efficiency%20issues%20of%20software%20systems.%0AIn%20order%20to%20be%20both%20effective%20and%20practical%2C%20the%20design%20of%20a%20performance%20test%0Amust%20achieve%20a%20reasonable%20trade-off%20between%20result%20quality%20and%20testing%20time.%0AThis%20becomes%20particularly%20challenging%20in%20Java%20context%2C%20where%20the%20software%0Aundergoes%20a%20warm-up%20phase%20of%20execution%2C%20due%20to%20just-in-time%20compilation.%20During%0Athis%20phase%2C%20performance%20measurements%20are%20subject%20to%20severe%20fluctuations%2C%20which%0Amay%20adversely%20affect%20quality%20of%20performance%20test%20results.%20However%2C%20these%0Aapproaches%20often%20provide%20suboptimal%20estimates%20of%20the%20warm-up%20phase%2C%20resulting%0Ain%20either%20insufficient%20or%20excessive%20warm-up%20iterations%2C%20which%20may%20degrade%0Aresult%20quality%20or%20increase%20testing%20time.%20There%20is%20still%20a%20lack%20of%20consensus%20on%0Ahow%20to%20properly%20address%20this%20problem.%20Here%2C%20we%20propose%20and%20study%20an%20AI-based%0Aframework%20to%20dynamically%20halt%20warm-up%20iterations%20at%20runtime.%20Specifically%2C%20our%0Aframework%20leverages%20recent%20advances%20in%20AI%20for%20Time%20Series%20Classification%20%28TSC%29%0Ato%20predict%20the%20end%20of%20the%20warm-up%20phase%20during%20test%20execution.%20We%20conduct%0Aexperiments%20by%20training%20three%20different%20TSC%20models%20on%20half%20a%20million%20of%0Ameasurement%20segments%20obtained%20from%20JMH%20microbenchmark%20executions.%20We%20find%20that%0Aour%20framework%20significantly%20improves%20the%20accuracy%20of%20the%20warm-up%20estimates%0Aprovided%20by%20state-of-practice%20and%20state-of-the-art%20methods.%20This%20higher%0Aestimation%20accuracy%20results%20in%20a%20net%20improvement%20in%20either%20result%20quality%20or%0Atesting%20time%20for%20up%20to%20%2B35.3%25%20of%20the%20microbenchmarks.%20Our%20study%20highlights%20that%0Aintegrating%20AI%20to%20dynamically%20estimate%20the%20end%20of%20the%20warm-up%20phase%20can%20enhance%0Athe%20cost-effectiveness%20of%20Java%20performance%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-driven%2520Java%2520Performance%2520Testing%253A%2520Balancing%2520Result%2520Quality%2520with%250A%2520%2520Testing%2520Time%26entry.906535625%3DLuca%2520Traini%2520and%2520Federico%2520Di%2520Menna%2520and%2520Vittorio%2520Cortellessa%26entry.1292438233%3D%2520%2520Performance%2520testing%2520aims%2520at%2520uncovering%2520efficiency%2520issues%2520of%2520software%2520systems.%250AIn%2520order%2520to%2520be%2520both%2520effective%2520and%2520practical%252C%2520the%2520design%2520of%2520a%2520performance%2520test%250Amust%2520achieve%2520a%2520reasonable%2520trade-off%2520between%2520result%2520quality%2520and%2520testing%2520time.%250AThis%2520becomes%2520particularly%2520challenging%2520in%2520Java%2520context%252C%2520where%2520the%2520software%250Aundergoes%2520a%2520warm-up%2520phase%2520of%2520execution%252C%2520due%2520to%2520just-in-time%2520compilation.%2520During%250Athis%2520phase%252C%2520performance%2520measurements%2520are%2520subject%2520to%2520severe%2520fluctuations%252C%2520which%250Amay%2520adversely%2520affect%2520quality%2520of%2520performance%2520test%2520results.%2520However%252C%2520these%250Aapproaches%2520often%2520provide%2520suboptimal%2520estimates%2520of%2520the%2520warm-up%2520phase%252C%2520resulting%250Ain%2520either%2520insufficient%2520or%2520excessive%2520warm-up%2520iterations%252C%2520which%2520may%2520degrade%250Aresult%2520quality%2520or%2520increase%2520testing%2520time.%2520There%2520is%2520still%2520a%2520lack%2520of%2520consensus%2520on%250Ahow%2520to%2520properly%2520address%2520this%2520problem.%2520Here%252C%2520we%2520propose%2520and%2520study%2520an%2520AI-based%250Aframework%2520to%2520dynamically%2520halt%2520warm-up%2520iterations%2520at%2520runtime.%2520Specifically%252C%2520our%250Aframework%2520leverages%2520recent%2520advances%2520in%2520AI%2520for%2520Time%2520Series%2520Classification%2520%2528TSC%2529%250Ato%2520predict%2520the%2520end%2520of%2520the%2520warm-up%2520phase%2520during%2520test%2520execution.%2520We%2520conduct%250Aexperiments%2520by%2520training%2520three%2520different%2520TSC%2520models%2520on%2520half%2520a%2520million%2520of%250Ameasurement%2520segments%2520obtained%2520from%2520JMH%2520microbenchmark%2520executions.%2520We%2520find%2520that%250Aour%2520framework%2520significantly%2520improves%2520the%2520accuracy%2520of%2520the%2520warm-up%2520estimates%250Aprovided%2520by%2520state-of-practice%2520and%2520state-of-the-art%2520methods.%2520This%2520higher%250Aestimation%2520accuracy%2520results%2520in%2520a%2520net%2520improvement%2520in%2520either%2520result%2520quality%2520or%250Atesting%2520time%2520for%2520up%2520to%2520%252B35.3%2525%2520of%2520the%2520microbenchmarks.%2520Our%2520study%2520highlights%2520that%250Aintegrating%2520AI%2520to%2520dynamically%2520estimate%2520the%2520end%2520of%2520the%2520warm-up%2520phase%2520can%2520enhance%250Athe%2520cost-effectiveness%2520of%2520Java%2520performance%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-driven%20Java%20Performance%20Testing%3A%20Balancing%20Result%20Quality%20with%0A%20%20Testing%20Time&entry.906535625=Luca%20Traini%20and%20Federico%20Di%20Menna%20and%20Vittorio%20Cortellessa&entry.1292438233=%20%20Performance%20testing%20aims%20at%20uncovering%20efficiency%20issues%20of%20software%20systems.%0AIn%20order%20to%20be%20both%20effective%20and%20practical%2C%20the%20design%20of%20a%20performance%20test%0Amust%20achieve%20a%20reasonable%20trade-off%20between%20result%20quality%20and%20testing%20time.%0AThis%20becomes%20particularly%20challenging%20in%20Java%20context%2C%20where%20the%20software%0Aundergoes%20a%20warm-up%20phase%20of%20execution%2C%20due%20to%20just-in-time%20compilation.%20During%0Athis%20phase%2C%20performance%20measurements%20are%20subject%20to%20severe%20fluctuations%2C%20which%0Amay%20adversely%20affect%20quality%20of%20performance%20test%20results.%20However%2C%20these%0Aapproaches%20often%20provide%20suboptimal%20estimates%20of%20the%20warm-up%20phase%2C%20resulting%0Ain%20either%20insufficient%20or%20excessive%20warm-up%20iterations%2C%20which%20may%20degrade%0Aresult%20quality%20or%20increase%20testing%20time.%20There%20is%20still%20a%20lack%20of%20consensus%20on%0Ahow%20to%20properly%20address%20this%20problem.%20Here%2C%20we%20propose%20and%20study%20an%20AI-based%0Aframework%20to%20dynamically%20halt%20warm-up%20iterations%20at%20runtime.%20Specifically%2C%20our%0Aframework%20leverages%20recent%20advances%20in%20AI%20for%20Time%20Series%20Classification%20%28TSC%29%0Ato%20predict%20the%20end%20of%20the%20warm-up%20phase%20during%20test%20execution.%20We%20conduct%0Aexperiments%20by%20training%20three%20different%20TSC%20models%20on%20half%20a%20million%20of%0Ameasurement%20segments%20obtained%20from%20JMH%20microbenchmark%20executions.%20We%20find%20that%0Aour%20framework%20significantly%20improves%20the%20accuracy%20of%20the%20warm-up%20estimates%0Aprovided%20by%20state-of-practice%20and%20state-of-the-art%20methods.%20This%20higher%0Aestimation%20accuracy%20results%20in%20a%20net%20improvement%20in%20either%20result%20quality%20or%0Atesting%20time%20for%20up%20to%20%2B35.3%25%20of%20the%20microbenchmarks.%20Our%20study%20highlights%20that%0Aintegrating%20AI%20to%20dynamically%20estimate%20the%20end%20of%20the%20warm-up%20phase%20can%20enhance%0Athe%20cost-effectiveness%20of%20Java%20performance%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05100v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


