<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260122.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion", "author": "Remy Sabathier and David Novotny and Niloy J. Mitra and Tom Monnier", "abstract": "Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes \"in action\" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed \"temporal 3D diffusion\". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.", "link": "http://arxiv.org/abs/2601.16148v1", "date": "2026-01-22", "relevancy": 3.3972, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7097}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6751}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActionMesh%3A%20Animated%203D%20Mesh%20Generation%20with%20Temporal%203D%20Diffusion&body=Title%3A%20ActionMesh%3A%20Animated%203D%20Mesh%20Generation%20with%20Temporal%203D%20Diffusion%0AAuthor%3A%20Remy%20Sabathier%20and%20David%20Novotny%20and%20Niloy%20J.%20Mitra%20and%20Tom%20Monnier%0AAbstract%3A%20Generating%20animated%203D%20objects%20is%20at%20the%20heart%20of%20many%20applications%2C%20yet%20most%20advanced%20works%20are%20typically%20difficult%20to%20apply%20in%20practice%20because%20of%20their%20limited%20setup%2C%20their%20long%20runtime%2C%20or%20their%20limited%20quality.%20We%20introduce%20ActionMesh%2C%20a%20generative%20model%20that%20predicts%20production-ready%203D%20meshes%20%22in%20action%22%20in%20a%20feed-forward%20manner.%20Drawing%20inspiration%20from%20early%20video%20models%2C%20our%20key%20insight%20is%20to%20modify%20existing%203D%20diffusion%20models%20to%20include%20a%20temporal%20axis%2C%20resulting%20in%20a%20framework%20we%20dubbed%20%22temporal%203D%20diffusion%22.%20Specifically%2C%20we%20first%20adapt%20the%203D%20diffusion%20stage%20to%20generate%20a%20sequence%20of%20synchronized%20latents%20representing%20time-varying%20and%20independent%203D%20shapes.%20Second%2C%20we%20design%20a%20temporal%203D%20autoencoder%20that%20translates%20a%20sequence%20of%20independent%20shapes%20into%20the%20corresponding%20deformations%20of%20a%20pre-defined%20reference%20shape%2C%20allowing%20us%20to%20build%20an%20animation.%20Combining%20these%20two%20components%2C%20ActionMesh%20generates%20animated%203D%20meshes%20from%20different%20inputs%20like%20a%20monocular%20video%2C%20a%20text%20description%2C%20or%20even%20a%203D%20mesh%20with%20a%20text%20prompt%20describing%20its%20animation.%20Besides%2C%20compared%20to%20previous%20approaches%2C%20our%20method%20is%20fast%20and%20produces%20results%20that%20are%20rig-free%20and%20topology%20consistent%2C%20hence%20enabling%20rapid%20iteration%20and%20seamless%20applications%20like%20texturing%20and%20retargeting.%20We%20evaluate%20our%20model%20on%20standard%20video-to-4D%20benchmarks%20%28Consistent4D%2C%20Objaverse%29%20and%20report%20state-of-the-art%20performances%20on%20both%20geometric%20accuracy%20and%20temporal%20consistency%2C%20demonstrating%20that%20our%20model%20can%20deliver%20animated%203D%20meshes%20with%20unprecedented%20speed%20and%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActionMesh%253A%2520Animated%25203D%2520Mesh%2520Generation%2520with%2520Temporal%25203D%2520Diffusion%26entry.906535625%3DRemy%2520Sabathier%2520and%2520David%2520Novotny%2520and%2520Niloy%2520J.%2520Mitra%2520and%2520Tom%2520Monnier%26entry.1292438233%3DGenerating%2520animated%25203D%2520objects%2520is%2520at%2520the%2520heart%2520of%2520many%2520applications%252C%2520yet%2520most%2520advanced%2520works%2520are%2520typically%2520difficult%2520to%2520apply%2520in%2520practice%2520because%2520of%2520their%2520limited%2520setup%252C%2520their%2520long%2520runtime%252C%2520or%2520their%2520limited%2520quality.%2520We%2520introduce%2520ActionMesh%252C%2520a%2520generative%2520model%2520that%2520predicts%2520production-ready%25203D%2520meshes%2520%2522in%2520action%2522%2520in%2520a%2520feed-forward%2520manner.%2520Drawing%2520inspiration%2520from%2520early%2520video%2520models%252C%2520our%2520key%2520insight%2520is%2520to%2520modify%2520existing%25203D%2520diffusion%2520models%2520to%2520include%2520a%2520temporal%2520axis%252C%2520resulting%2520in%2520a%2520framework%2520we%2520dubbed%2520%2522temporal%25203D%2520diffusion%2522.%2520Specifically%252C%2520we%2520first%2520adapt%2520the%25203D%2520diffusion%2520stage%2520to%2520generate%2520a%2520sequence%2520of%2520synchronized%2520latents%2520representing%2520time-varying%2520and%2520independent%25203D%2520shapes.%2520Second%252C%2520we%2520design%2520a%2520temporal%25203D%2520autoencoder%2520that%2520translates%2520a%2520sequence%2520of%2520independent%2520shapes%2520into%2520the%2520corresponding%2520deformations%2520of%2520a%2520pre-defined%2520reference%2520shape%252C%2520allowing%2520us%2520to%2520build%2520an%2520animation.%2520Combining%2520these%2520two%2520components%252C%2520ActionMesh%2520generates%2520animated%25203D%2520meshes%2520from%2520different%2520inputs%2520like%2520a%2520monocular%2520video%252C%2520a%2520text%2520description%252C%2520or%2520even%2520a%25203D%2520mesh%2520with%2520a%2520text%2520prompt%2520describing%2520its%2520animation.%2520Besides%252C%2520compared%2520to%2520previous%2520approaches%252C%2520our%2520method%2520is%2520fast%2520and%2520produces%2520results%2520that%2520are%2520rig-free%2520and%2520topology%2520consistent%252C%2520hence%2520enabling%2520rapid%2520iteration%2520and%2520seamless%2520applications%2520like%2520texturing%2520and%2520retargeting.%2520We%2520evaluate%2520our%2520model%2520on%2520standard%2520video-to-4D%2520benchmarks%2520%2528Consistent4D%252C%2520Objaverse%2529%2520and%2520report%2520state-of-the-art%2520performances%2520on%2520both%2520geometric%2520accuracy%2520and%2520temporal%2520consistency%252C%2520demonstrating%2520that%2520our%2520model%2520can%2520deliver%2520animated%25203D%2520meshes%2520with%2520unprecedented%2520speed%2520and%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActionMesh%3A%20Animated%203D%20Mesh%20Generation%20with%20Temporal%203D%20Diffusion&entry.906535625=Remy%20Sabathier%20and%20David%20Novotny%20and%20Niloy%20J.%20Mitra%20and%20Tom%20Monnier&entry.1292438233=Generating%20animated%203D%20objects%20is%20at%20the%20heart%20of%20many%20applications%2C%20yet%20most%20advanced%20works%20are%20typically%20difficult%20to%20apply%20in%20practice%20because%20of%20their%20limited%20setup%2C%20their%20long%20runtime%2C%20or%20their%20limited%20quality.%20We%20introduce%20ActionMesh%2C%20a%20generative%20model%20that%20predicts%20production-ready%203D%20meshes%20%22in%20action%22%20in%20a%20feed-forward%20manner.%20Drawing%20inspiration%20from%20early%20video%20models%2C%20our%20key%20insight%20is%20to%20modify%20existing%203D%20diffusion%20models%20to%20include%20a%20temporal%20axis%2C%20resulting%20in%20a%20framework%20we%20dubbed%20%22temporal%203D%20diffusion%22.%20Specifically%2C%20we%20first%20adapt%20the%203D%20diffusion%20stage%20to%20generate%20a%20sequence%20of%20synchronized%20latents%20representing%20time-varying%20and%20independent%203D%20shapes.%20Second%2C%20we%20design%20a%20temporal%203D%20autoencoder%20that%20translates%20a%20sequence%20of%20independent%20shapes%20into%20the%20corresponding%20deformations%20of%20a%20pre-defined%20reference%20shape%2C%20allowing%20us%20to%20build%20an%20animation.%20Combining%20these%20two%20components%2C%20ActionMesh%20generates%20animated%203D%20meshes%20from%20different%20inputs%20like%20a%20monocular%20video%2C%20a%20text%20description%2C%20or%20even%20a%203D%20mesh%20with%20a%20text%20prompt%20describing%20its%20animation.%20Besides%2C%20compared%20to%20previous%20approaches%2C%20our%20method%20is%20fast%20and%20produces%20results%20that%20are%20rig-free%20and%20topology%20consistent%2C%20hence%20enabling%20rapid%20iteration%20and%20seamless%20applications%20like%20texturing%20and%20retargeting.%20We%20evaluate%20our%20model%20on%20standard%20video-to-4D%20benchmarks%20%28Consistent4D%2C%20Objaverse%29%20and%20report%20state-of-the-art%20performances%20on%20both%20geometric%20accuracy%20and%20temporal%20consistency%2C%20demonstrating%20that%20our%20model%20can%20deliver%20animated%203D%20meshes%20with%20unprecedented%20speed%20and%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.16148v1&entry.124074799=Read"},
{"title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis", "author": "Florian Barthel and Wieland Morgenstern and Paul Hinzer and Anna Hilsmann and Peter Eisert", "abstract": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high quality synthesis of human heads. However, existing methods stabilize training and enhance rendering quality from steep viewpoints by conditioning the random latent vector on the current camera position. This compromises 3D consistency, as we observe significant identity changes when re-synthesizing the 3D head with each camera shift. Conversely, fixing the camera to a single viewpoint yields high-quality renderings for that perspective but results in poor performance for novel views. Removing view-conditioning typically destabilizes GAN training, often causing the training to collapse. In response to these challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without relying on view-conditioning. To ensure training stability, we introduce a multi-view regularization technique that enhances generator convergence with minimal computational overhead. Additionally, we adapt the conditional loss used in existing 3D Gaussian splatting GANs and propose a generator architecture designed to not only stabilize training but also facilitate efficient rendering and straightforward scaling, enabling output resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate a new dataset derived from FFHQ. This dataset enables very high resolutions, focuses on larger portions of the human head, reduces view-dependent artifacts for improved 3D consistency, and excludes images where subjects are obscured by hands or other objects. As a result, our approach achieves very high rendering quality, supported by competitive FID scores, while ensuring consistent 3D scene generation. Check our our project page here: https://fraunhoferhhi.github.io/cgs-gan/", "link": "http://arxiv.org/abs/2505.17590v3", "date": "2026-01-22", "relevancy": 3.3744, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6623}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CGS-GAN%3A%203D%20Consistent%20Gaussian%20Splatting%20GANs%20for%20High%20Resolution%20Human%20Head%20Synthesis&body=Title%3A%20CGS-GAN%3A%203D%20Consistent%20Gaussian%20Splatting%20GANs%20for%20High%20Resolution%20Human%20Head%20Synthesis%0AAuthor%3A%20Florian%20Barthel%20and%20Wieland%20Morgenstern%20and%20Paul%20Hinzer%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20Recently%2C%203D%20GANs%20based%20on%203D%20Gaussian%20splatting%20have%20been%20proposed%20for%20high%20quality%20synthesis%20of%20human%20heads.%20However%2C%20existing%20methods%20stabilize%20training%20and%20enhance%20rendering%20quality%20from%20steep%20viewpoints%20by%20conditioning%20the%20random%20latent%20vector%20on%20the%20current%20camera%20position.%20This%20compromises%203D%20consistency%2C%20as%20we%20observe%20significant%20identity%20changes%20when%20re-synthesizing%20the%203D%20head%20with%20each%20camera%20shift.%20Conversely%2C%20fixing%20the%20camera%20to%20a%20single%20viewpoint%20yields%20high-quality%20renderings%20for%20that%20perspective%20but%20results%20in%20poor%20performance%20for%20novel%20views.%20Removing%20view-conditioning%20typically%20destabilizes%20GAN%20training%2C%20often%20causing%20the%20training%20to%20collapse.%20In%20response%20to%20these%20challenges%2C%20we%20introduce%20CGS-GAN%2C%20a%20novel%203D%20Gaussian%20Splatting%20GAN%20framework%20that%20enables%20stable%20training%20and%20high-quality%203D-consistent%20synthesis%20of%20human%20heads%20without%20relying%20on%20view-conditioning.%20To%20ensure%20training%20stability%2C%20we%20introduce%20a%20multi-view%20regularization%20technique%20that%20enhances%20generator%20convergence%20with%20minimal%20computational%20overhead.%20Additionally%2C%20we%20adapt%20the%20conditional%20loss%20used%20in%20existing%203D%20Gaussian%20splatting%20GANs%20and%20propose%20a%20generator%20architecture%20designed%20to%20not%20only%20stabilize%20training%20but%20also%20facilitate%20efficient%20rendering%20and%20straightforward%20scaling%2C%20enabling%20output%20resolutions%20up%20to%20%242048%5E2%24.%20To%20evaluate%20the%20capabilities%20of%20CGS-GAN%2C%20we%20curate%20a%20new%20dataset%20derived%20from%20FFHQ.%20This%20dataset%20enables%20very%20high%20resolutions%2C%20focuses%20on%20larger%20portions%20of%20the%20human%20head%2C%20reduces%20view-dependent%20artifacts%20for%20improved%203D%20consistency%2C%20and%20excludes%20images%20where%20subjects%20are%20obscured%20by%20hands%20or%20other%20objects.%20As%20a%20result%2C%20our%20approach%20achieves%20very%20high%20rendering%20quality%2C%20supported%20by%20competitive%20FID%20scores%2C%20while%20ensuring%20consistent%203D%20scene%20generation.%20Check%20our%20our%20project%20page%20here%3A%20https%3A//fraunhoferhhi.github.io/cgs-gan/%0ALink%3A%20http%3A//arxiv.org/abs/2505.17590v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCGS-GAN%253A%25203D%2520Consistent%2520Gaussian%2520Splatting%2520GANs%2520for%2520High%2520Resolution%2520Human%2520Head%2520Synthesis%26entry.906535625%3DFlorian%2520Barthel%2520and%2520Wieland%2520Morgenstern%2520and%2520Paul%2520Hinzer%2520and%2520Anna%2520Hilsmann%2520and%2520Peter%2520Eisert%26entry.1292438233%3DRecently%252C%25203D%2520GANs%2520based%2520on%25203D%2520Gaussian%2520splatting%2520have%2520been%2520proposed%2520for%2520high%2520quality%2520synthesis%2520of%2520human%2520heads.%2520However%252C%2520existing%2520methods%2520stabilize%2520training%2520and%2520enhance%2520rendering%2520quality%2520from%2520steep%2520viewpoints%2520by%2520conditioning%2520the%2520random%2520latent%2520vector%2520on%2520the%2520current%2520camera%2520position.%2520This%2520compromises%25203D%2520consistency%252C%2520as%2520we%2520observe%2520significant%2520identity%2520changes%2520when%2520re-synthesizing%2520the%25203D%2520head%2520with%2520each%2520camera%2520shift.%2520Conversely%252C%2520fixing%2520the%2520camera%2520to%2520a%2520single%2520viewpoint%2520yields%2520high-quality%2520renderings%2520for%2520that%2520perspective%2520but%2520results%2520in%2520poor%2520performance%2520for%2520novel%2520views.%2520Removing%2520view-conditioning%2520typically%2520destabilizes%2520GAN%2520training%252C%2520often%2520causing%2520the%2520training%2520to%2520collapse.%2520In%2520response%2520to%2520these%2520challenges%252C%2520we%2520introduce%2520CGS-GAN%252C%2520a%2520novel%25203D%2520Gaussian%2520Splatting%2520GAN%2520framework%2520that%2520enables%2520stable%2520training%2520and%2520high-quality%25203D-consistent%2520synthesis%2520of%2520human%2520heads%2520without%2520relying%2520on%2520view-conditioning.%2520To%2520ensure%2520training%2520stability%252C%2520we%2520introduce%2520a%2520multi-view%2520regularization%2520technique%2520that%2520enhances%2520generator%2520convergence%2520with%2520minimal%2520computational%2520overhead.%2520Additionally%252C%2520we%2520adapt%2520the%2520conditional%2520loss%2520used%2520in%2520existing%25203D%2520Gaussian%2520splatting%2520GANs%2520and%2520propose%2520a%2520generator%2520architecture%2520designed%2520to%2520not%2520only%2520stabilize%2520training%2520but%2520also%2520facilitate%2520efficient%2520rendering%2520and%2520straightforward%2520scaling%252C%2520enabling%2520output%2520resolutions%2520up%2520to%2520%25242048%255E2%2524.%2520To%2520evaluate%2520the%2520capabilities%2520of%2520CGS-GAN%252C%2520we%2520curate%2520a%2520new%2520dataset%2520derived%2520from%2520FFHQ.%2520This%2520dataset%2520enables%2520very%2520high%2520resolutions%252C%2520focuses%2520on%2520larger%2520portions%2520of%2520the%2520human%2520head%252C%2520reduces%2520view-dependent%2520artifacts%2520for%2520improved%25203D%2520consistency%252C%2520and%2520excludes%2520images%2520where%2520subjects%2520are%2520obscured%2520by%2520hands%2520or%2520other%2520objects.%2520As%2520a%2520result%252C%2520our%2520approach%2520achieves%2520very%2520high%2520rendering%2520quality%252C%2520supported%2520by%2520competitive%2520FID%2520scores%252C%2520while%2520ensuring%2520consistent%25203D%2520scene%2520generation.%2520Check%2520our%2520our%2520project%2520page%2520here%253A%2520https%253A//fraunhoferhhi.github.io/cgs-gan/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17590v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGS-GAN%3A%203D%20Consistent%20Gaussian%20Splatting%20GANs%20for%20High%20Resolution%20Human%20Head%20Synthesis&entry.906535625=Florian%20Barthel%20and%20Wieland%20Morgenstern%20and%20Paul%20Hinzer%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=Recently%2C%203D%20GANs%20based%20on%203D%20Gaussian%20splatting%20have%20been%20proposed%20for%20high%20quality%20synthesis%20of%20human%20heads.%20However%2C%20existing%20methods%20stabilize%20training%20and%20enhance%20rendering%20quality%20from%20steep%20viewpoints%20by%20conditioning%20the%20random%20latent%20vector%20on%20the%20current%20camera%20position.%20This%20compromises%203D%20consistency%2C%20as%20we%20observe%20significant%20identity%20changes%20when%20re-synthesizing%20the%203D%20head%20with%20each%20camera%20shift.%20Conversely%2C%20fixing%20the%20camera%20to%20a%20single%20viewpoint%20yields%20high-quality%20renderings%20for%20that%20perspective%20but%20results%20in%20poor%20performance%20for%20novel%20views.%20Removing%20view-conditioning%20typically%20destabilizes%20GAN%20training%2C%20often%20causing%20the%20training%20to%20collapse.%20In%20response%20to%20these%20challenges%2C%20we%20introduce%20CGS-GAN%2C%20a%20novel%203D%20Gaussian%20Splatting%20GAN%20framework%20that%20enables%20stable%20training%20and%20high-quality%203D-consistent%20synthesis%20of%20human%20heads%20without%20relying%20on%20view-conditioning.%20To%20ensure%20training%20stability%2C%20we%20introduce%20a%20multi-view%20regularization%20technique%20that%20enhances%20generator%20convergence%20with%20minimal%20computational%20overhead.%20Additionally%2C%20we%20adapt%20the%20conditional%20loss%20used%20in%20existing%203D%20Gaussian%20splatting%20GANs%20and%20propose%20a%20generator%20architecture%20designed%20to%20not%20only%20stabilize%20training%20but%20also%20facilitate%20efficient%20rendering%20and%20straightforward%20scaling%2C%20enabling%20output%20resolutions%20up%20to%20%242048%5E2%24.%20To%20evaluate%20the%20capabilities%20of%20CGS-GAN%2C%20we%20curate%20a%20new%20dataset%20derived%20from%20FFHQ.%20This%20dataset%20enables%20very%20high%20resolutions%2C%20focuses%20on%20larger%20portions%20of%20the%20human%20head%2C%20reduces%20view-dependent%20artifacts%20for%20improved%203D%20consistency%2C%20and%20excludes%20images%20where%20subjects%20are%20obscured%20by%20hands%20or%20other%20objects.%20As%20a%20result%2C%20our%20approach%20achieves%20very%20high%20rendering%20quality%2C%20supported%20by%20competitive%20FID%20scores%2C%20while%20ensuring%20consistent%203D%20scene%20generation.%20Check%20our%20our%20project%20page%20here%3A%20https%3A//fraunhoferhhi.github.io/cgs-gan/&entry.1838667208=http%3A//arxiv.org/abs/2505.17590v3&entry.124074799=Read"},
{"title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis", "author": "Sheng Miao and Sijin Li and Pan Wang and Dongfeng Bai and Bingbing Liu and Yue Wang and Andreas Geiger and Yiyi Liao", "abstract": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.", "link": "http://arxiv.org/abs/2601.15951v1", "date": "2026-01-22", "relevancy": 3.2554, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6913}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6442}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVolSplat4D%3A%20Efficient%20Volume-based%20Gaussian%20Splatting%20for%204D%20Urban%20Scene%20Synthesis&body=Title%3A%20EVolSplat4D%3A%20Efficient%20Volume-based%20Gaussian%20Splatting%20for%204D%20Urban%20Scene%20Synthesis%0AAuthor%3A%20Sheng%20Miao%20and%20Sijin%20Li%20and%20Pan%20Wang%20and%20Dongfeng%20Bai%20and%20Bingbing%20Liu%20and%20Yue%20Wang%20and%20Andreas%20Geiger%20and%20Yiyi%20Liao%0AAbstract%3A%20Novel%20view%20synthesis%20%28NVS%29%20of%20static%20and%20dynamic%20urban%20scenes%20is%20essential%20for%20autonomous%20driving%20simulation%2C%20yet%20existing%20methods%20often%20struggle%20to%20balance%20reconstruction%20time%20with%20quality.%20While%20state-of-the-art%20neural%20radiance%20fields%20and%203D%20Gaussian%20Splatting%20approaches%20achieve%20photorealism%2C%20they%20often%20rely%20on%20time-consuming%20per-scene%20optimization.%20Conversely%2C%20emerging%20feed-forward%20methods%20frequently%20adopt%20per-pixel%20Gaussian%20representations%2C%20which%20lead%20to%203D%20inconsistencies%20when%20aggregating%20multi-view%20predictions%20in%20complex%2C%20dynamic%20environments.%20We%20propose%20EvolSplat4D%2C%20a%20feed-forward%20framework%20that%20moves%20beyond%20existing%20per-pixel%20paradigms%20by%20unifying%20volume-based%20and%20pixel-based%20Gaussian%20prediction%20across%20three%20specialized%20branches.%20For%20close-range%20static%20regions%2C%20we%20predict%20consistent%20geometry%20of%203D%20Gaussians%20over%20multiple%20frames%20directly%20from%20a%203D%20feature%20volume%2C%20complemented%20by%20a%20semantically-enhanced%20image-based%20rendering%20module%20for%20predicting%20their%20appearance.%20For%20dynamic%20actors%2C%20we%20utilize%20object-centric%20canonical%20spaces%20and%20a%20motion-adjusted%20rendering%20module%20to%20aggregate%20temporal%20features%2C%20ensuring%20stable%204D%20reconstruction%20despite%20noisy%20motion%20priors.%20Far-Field%20scenery%20is%20handled%20by%20an%20efficient%20per-pixel%20Gaussian%20branch%20to%20ensure%20full-scene%20coverage.%20Experimental%20results%20on%20the%20KITTI-360%2C%20KITTI%2C%20Waymo%2C%20and%20PandaSet%20datasets%20show%20that%20EvolSplat4D%20reconstructs%20both%20static%20and%20dynamic%20environments%20with%20superior%20accuracy%20and%20consistency%2C%20outperforming%20both%20per-scene%20optimization%20and%20state-of-the-art%20feed-forward%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVolSplat4D%253A%2520Efficient%2520Volume-based%2520Gaussian%2520Splatting%2520for%25204D%2520Urban%2520Scene%2520Synthesis%26entry.906535625%3DSheng%2520Miao%2520and%2520Sijin%2520Li%2520and%2520Pan%2520Wang%2520and%2520Dongfeng%2520Bai%2520and%2520Bingbing%2520Liu%2520and%2520Yue%2520Wang%2520and%2520Andreas%2520Geiger%2520and%2520Yiyi%2520Liao%26entry.1292438233%3DNovel%2520view%2520synthesis%2520%2528NVS%2529%2520of%2520static%2520and%2520dynamic%2520urban%2520scenes%2520is%2520essential%2520for%2520autonomous%2520driving%2520simulation%252C%2520yet%2520existing%2520methods%2520often%2520struggle%2520to%2520balance%2520reconstruction%2520time%2520with%2520quality.%2520While%2520state-of-the-art%2520neural%2520radiance%2520fields%2520and%25203D%2520Gaussian%2520Splatting%2520approaches%2520achieve%2520photorealism%252C%2520they%2520often%2520rely%2520on%2520time-consuming%2520per-scene%2520optimization.%2520Conversely%252C%2520emerging%2520feed-forward%2520methods%2520frequently%2520adopt%2520per-pixel%2520Gaussian%2520representations%252C%2520which%2520lead%2520to%25203D%2520inconsistencies%2520when%2520aggregating%2520multi-view%2520predictions%2520in%2520complex%252C%2520dynamic%2520environments.%2520We%2520propose%2520EvolSplat4D%252C%2520a%2520feed-forward%2520framework%2520that%2520moves%2520beyond%2520existing%2520per-pixel%2520paradigms%2520by%2520unifying%2520volume-based%2520and%2520pixel-based%2520Gaussian%2520prediction%2520across%2520three%2520specialized%2520branches.%2520For%2520close-range%2520static%2520regions%252C%2520we%2520predict%2520consistent%2520geometry%2520of%25203D%2520Gaussians%2520over%2520multiple%2520frames%2520directly%2520from%2520a%25203D%2520feature%2520volume%252C%2520complemented%2520by%2520a%2520semantically-enhanced%2520image-based%2520rendering%2520module%2520for%2520predicting%2520their%2520appearance.%2520For%2520dynamic%2520actors%252C%2520we%2520utilize%2520object-centric%2520canonical%2520spaces%2520and%2520a%2520motion-adjusted%2520rendering%2520module%2520to%2520aggregate%2520temporal%2520features%252C%2520ensuring%2520stable%25204D%2520reconstruction%2520despite%2520noisy%2520motion%2520priors.%2520Far-Field%2520scenery%2520is%2520handled%2520by%2520an%2520efficient%2520per-pixel%2520Gaussian%2520branch%2520to%2520ensure%2520full-scene%2520coverage.%2520Experimental%2520results%2520on%2520the%2520KITTI-360%252C%2520KITTI%252C%2520Waymo%252C%2520and%2520PandaSet%2520datasets%2520show%2520that%2520EvolSplat4D%2520reconstructs%2520both%2520static%2520and%2520dynamic%2520environments%2520with%2520superior%2520accuracy%2520and%2520consistency%252C%2520outperforming%2520both%2520per-scene%2520optimization%2520and%2520state-of-the-art%2520feed-forward%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVolSplat4D%3A%20Efficient%20Volume-based%20Gaussian%20Splatting%20for%204D%20Urban%20Scene%20Synthesis&entry.906535625=Sheng%20Miao%20and%20Sijin%20Li%20and%20Pan%20Wang%20and%20Dongfeng%20Bai%20and%20Bingbing%20Liu%20and%20Yue%20Wang%20and%20Andreas%20Geiger%20and%20Yiyi%20Liao&entry.1292438233=Novel%20view%20synthesis%20%28NVS%29%20of%20static%20and%20dynamic%20urban%20scenes%20is%20essential%20for%20autonomous%20driving%20simulation%2C%20yet%20existing%20methods%20often%20struggle%20to%20balance%20reconstruction%20time%20with%20quality.%20While%20state-of-the-art%20neural%20radiance%20fields%20and%203D%20Gaussian%20Splatting%20approaches%20achieve%20photorealism%2C%20they%20often%20rely%20on%20time-consuming%20per-scene%20optimization.%20Conversely%2C%20emerging%20feed-forward%20methods%20frequently%20adopt%20per-pixel%20Gaussian%20representations%2C%20which%20lead%20to%203D%20inconsistencies%20when%20aggregating%20multi-view%20predictions%20in%20complex%2C%20dynamic%20environments.%20We%20propose%20EvolSplat4D%2C%20a%20feed-forward%20framework%20that%20moves%20beyond%20existing%20per-pixel%20paradigms%20by%20unifying%20volume-based%20and%20pixel-based%20Gaussian%20prediction%20across%20three%20specialized%20branches.%20For%20close-range%20static%20regions%2C%20we%20predict%20consistent%20geometry%20of%203D%20Gaussians%20over%20multiple%20frames%20directly%20from%20a%203D%20feature%20volume%2C%20complemented%20by%20a%20semantically-enhanced%20image-based%20rendering%20module%20for%20predicting%20their%20appearance.%20For%20dynamic%20actors%2C%20we%20utilize%20object-centric%20canonical%20spaces%20and%20a%20motion-adjusted%20rendering%20module%20to%20aggregate%20temporal%20features%2C%20ensuring%20stable%204D%20reconstruction%20despite%20noisy%20motion%20priors.%20Far-Field%20scenery%20is%20handled%20by%20an%20efficient%20per-pixel%20Gaussian%20branch%20to%20ensure%20full-scene%20coverage.%20Experimental%20results%20on%20the%20KITTI-360%2C%20KITTI%2C%20Waymo%2C%20and%20PandaSet%20datasets%20show%20that%20EvolSplat4D%20reconstructs%20both%20static%20and%20dynamic%20environments%20with%20superior%20accuracy%20and%20consistency%2C%20outperforming%20both%20per-scene%20optimization%20and%20state-of-the-art%20feed-forward%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.15951v1&entry.124074799=Read"},
{"title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling", "author": "Zhaoqi Su and Shihai Chen and Xinyan Lin and Liqin Huang and Zhipeng Su and Xiaoqiang Lu", "abstract": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.", "link": "http://arxiv.org/abs/2601.15897v1", "date": "2026-01-22", "relevancy": 3.2406, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6866}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6407}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThermoSplat%3A%20Cross-Modal%203D%20Gaussian%20Splatting%20with%20Feature%20Modulation%20and%20Geometry%20Decoupling&body=Title%3A%20ThermoSplat%3A%20Cross-Modal%203D%20Gaussian%20Splatting%20with%20Feature%20Modulation%20and%20Geometry%20Decoupling%0AAuthor%3A%20Zhaoqi%20Su%20and%20Shihai%20Chen%20and%20Xinyan%20Lin%20and%20Liqin%20Huang%20and%20Zhipeng%20Su%20and%20Xiaoqiang%20Lu%0AAbstract%3A%20Multi-modal%20scene%20reconstruction%20integrating%20RGB%20and%20thermal%20infrared%20data%20is%20essential%20for%20robust%20environmental%20perception%20across%20diverse%20lighting%20and%20weather%20conditions.%20However%2C%20extending%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20multi-spectral%20scenarios%20remains%20challenging.%20Current%20approaches%20often%20struggle%20to%20fully%20leverage%20the%20complementary%20information%20of%20multi-modal%20data%2C%20typically%20relying%20on%20mechanisms%20that%20either%20tend%20to%20neglect%20cross-modal%20correlations%20or%20leverage%20shared%20representations%20that%20fail%20to%20adaptively%20handle%20the%20complex%20structural%20correlations%20and%20physical%20discrepancies%20between%20spectrums.%20To%20address%20these%20limitations%2C%20we%20propose%20ThermoSplat%2C%20a%20novel%20framework%20that%20enables%20deep%20spectral-aware%20reconstruction%20through%20active%20feature%20modulation%20and%20adaptive%20geometry%20decoupling.%20First%2C%20we%20introduce%20a%20Cross-Modal%20FiLM%20Modulation%20mechanism%20that%20dynamically%20conditions%20shared%20latent%20features%20on%20thermal%20structural%20priors%2C%20effectively%20guiding%20visible%20texture%20synthesis%20with%20reliable%20cross-modal%20geometric%20cues.%20Second%2C%20to%20accommodate%20modality-specific%20geometric%20inconsistencies%2C%20we%20propose%20a%20Modality-Adaptive%20Geometric%20Decoupling%20scheme%20that%20learns%20independent%20opacity%20offsets%20and%20executes%20an%20independent%20rasterization%20pass%20for%20the%20thermal%20branch.%20Additionally%2C%20a%20hybrid%20rendering%20pipeline%20is%20employed%20to%20integrate%20explicit%20Spherical%20Harmonics%20with%20implicit%20neural%20decoding%2C%20ensuring%20both%20semantic%20consistency%20and%20high-frequency%20detail%20preservation.%20Extensive%20experiments%20on%20the%20RGBT-Scenes%20dataset%20demonstrate%20that%20ThermoSplat%20achieves%20state-of-the-art%20rendering%20quality%20across%20both%20visible%20and%20thermal%20spectrums.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermoSplat%253A%2520Cross-Modal%25203D%2520Gaussian%2520Splatting%2520with%2520Feature%2520Modulation%2520and%2520Geometry%2520Decoupling%26entry.906535625%3DZhaoqi%2520Su%2520and%2520Shihai%2520Chen%2520and%2520Xinyan%2520Lin%2520and%2520Liqin%2520Huang%2520and%2520Zhipeng%2520Su%2520and%2520Xiaoqiang%2520Lu%26entry.1292438233%3DMulti-modal%2520scene%2520reconstruction%2520integrating%2520RGB%2520and%2520thermal%2520infrared%2520data%2520is%2520essential%2520for%2520robust%2520environmental%2520perception%2520across%2520diverse%2520lighting%2520and%2520weather%2520conditions.%2520However%252C%2520extending%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520to%2520multi-spectral%2520scenarios%2520remains%2520challenging.%2520Current%2520approaches%2520often%2520struggle%2520to%2520fully%2520leverage%2520the%2520complementary%2520information%2520of%2520multi-modal%2520data%252C%2520typically%2520relying%2520on%2520mechanisms%2520that%2520either%2520tend%2520to%2520neglect%2520cross-modal%2520correlations%2520or%2520leverage%2520shared%2520representations%2520that%2520fail%2520to%2520adaptively%2520handle%2520the%2520complex%2520structural%2520correlations%2520and%2520physical%2520discrepancies%2520between%2520spectrums.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ThermoSplat%252C%2520a%2520novel%2520framework%2520that%2520enables%2520deep%2520spectral-aware%2520reconstruction%2520through%2520active%2520feature%2520modulation%2520and%2520adaptive%2520geometry%2520decoupling.%2520First%252C%2520we%2520introduce%2520a%2520Cross-Modal%2520FiLM%2520Modulation%2520mechanism%2520that%2520dynamically%2520conditions%2520shared%2520latent%2520features%2520on%2520thermal%2520structural%2520priors%252C%2520effectively%2520guiding%2520visible%2520texture%2520synthesis%2520with%2520reliable%2520cross-modal%2520geometric%2520cues.%2520Second%252C%2520to%2520accommodate%2520modality-specific%2520geometric%2520inconsistencies%252C%2520we%2520propose%2520a%2520Modality-Adaptive%2520Geometric%2520Decoupling%2520scheme%2520that%2520learns%2520independent%2520opacity%2520offsets%2520and%2520executes%2520an%2520independent%2520rasterization%2520pass%2520for%2520the%2520thermal%2520branch.%2520Additionally%252C%2520a%2520hybrid%2520rendering%2520pipeline%2520is%2520employed%2520to%2520integrate%2520explicit%2520Spherical%2520Harmonics%2520with%2520implicit%2520neural%2520decoding%252C%2520ensuring%2520both%2520semantic%2520consistency%2520and%2520high-frequency%2520detail%2520preservation.%2520Extensive%2520experiments%2520on%2520the%2520RGBT-Scenes%2520dataset%2520demonstrate%2520that%2520ThermoSplat%2520achieves%2520state-of-the-art%2520rendering%2520quality%2520across%2520both%2520visible%2520and%2520thermal%2520spectrums.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThermoSplat%3A%20Cross-Modal%203D%20Gaussian%20Splatting%20with%20Feature%20Modulation%20and%20Geometry%20Decoupling&entry.906535625=Zhaoqi%20Su%20and%20Shihai%20Chen%20and%20Xinyan%20Lin%20and%20Liqin%20Huang%20and%20Zhipeng%20Su%20and%20Xiaoqiang%20Lu&entry.1292438233=Multi-modal%20scene%20reconstruction%20integrating%20RGB%20and%20thermal%20infrared%20data%20is%20essential%20for%20robust%20environmental%20perception%20across%20diverse%20lighting%20and%20weather%20conditions.%20However%2C%20extending%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20multi-spectral%20scenarios%20remains%20challenging.%20Current%20approaches%20often%20struggle%20to%20fully%20leverage%20the%20complementary%20information%20of%20multi-modal%20data%2C%20typically%20relying%20on%20mechanisms%20that%20either%20tend%20to%20neglect%20cross-modal%20correlations%20or%20leverage%20shared%20representations%20that%20fail%20to%20adaptively%20handle%20the%20complex%20structural%20correlations%20and%20physical%20discrepancies%20between%20spectrums.%20To%20address%20these%20limitations%2C%20we%20propose%20ThermoSplat%2C%20a%20novel%20framework%20that%20enables%20deep%20spectral-aware%20reconstruction%20through%20active%20feature%20modulation%20and%20adaptive%20geometry%20decoupling.%20First%2C%20we%20introduce%20a%20Cross-Modal%20FiLM%20Modulation%20mechanism%20that%20dynamically%20conditions%20shared%20latent%20features%20on%20thermal%20structural%20priors%2C%20effectively%20guiding%20visible%20texture%20synthesis%20with%20reliable%20cross-modal%20geometric%20cues.%20Second%2C%20to%20accommodate%20modality-specific%20geometric%20inconsistencies%2C%20we%20propose%20a%20Modality-Adaptive%20Geometric%20Decoupling%20scheme%20that%20learns%20independent%20opacity%20offsets%20and%20executes%20an%20independent%20rasterization%20pass%20for%20the%20thermal%20branch.%20Additionally%2C%20a%20hybrid%20rendering%20pipeline%20is%20employed%20to%20integrate%20explicit%20Spherical%20Harmonics%20with%20implicit%20neural%20decoding%2C%20ensuring%20both%20semantic%20consistency%20and%20high-frequency%20detail%20preservation.%20Extensive%20experiments%20on%20the%20RGBT-Scenes%20dataset%20demonstrate%20that%20ThermoSplat%20achieves%20state-of-the-art%20rendering%20quality%20across%20both%20visible%20and%20thermal%20spectrums.&entry.1838667208=http%3A//arxiv.org/abs/2601.15897v1&entry.124074799=Read"},
{"title": "VideoPro: Adaptive Program Reasoning for Long Video Understanding", "author": "Chenglin Li and Feng Han and Yikun Wang and Ruilin Li and Shuai Dong and Haowen Hou and Haitao Li and Qianglong Chen and Feng Tao and Jingqi Tong and Yin Zhang and Jiaqi Wang", "abstract": "Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models' ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.", "link": "http://arxiv.org/abs/2509.17743v3", "date": "2026-01-22", "relevancy": 2.915, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6114}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoPro%3A%20Adaptive%20Program%20Reasoning%20for%20Long%20Video%20Understanding&body=Title%3A%20VideoPro%3A%20Adaptive%20Program%20Reasoning%20for%20Long%20Video%20Understanding%0AAuthor%3A%20Chenglin%20Li%20and%20Feng%20Han%20and%20Yikun%20Wang%20and%20Ruilin%20Li%20and%20Shuai%20Dong%20and%20Haowen%20Hou%20and%20Haitao%20Li%20and%20Qianglong%20Chen%20and%20Feng%20Tao%20and%20Jingqi%20Tong%20and%20Yin%20Zhang%20and%20Jiaqi%20Wang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20in%20generating%20program%20workflows%20for%20visual%20tasks.%20However%2C%20previous%20approaches%20often%20rely%20on%20closed-source%20models%2C%20lack%20systematic%20reasoning%2C%20and%20struggle%20with%20long-form%20video%20question%20answering%20%28videoQA%29.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%20FS-VisPR%20framework%2C%20an%20adaptive%20visual%20program%20reasoning%20approach%20that%20balances%20fast%20reasoning%20for%20simple%20queries%20with%20slow%20reasoning%20for%20difficult%20ones.%20First%2C%20we%20design%20efficient%20visual%20modules%20%28e.g.%2C%20key%20clip%20retrieval%20and%20subtitle%20retrieval%29%20to%20support%20long-form%20video%20tasks.%20Then%2C%20we%20construct%20a%20diverse%20and%20high-quality%20fast-slow%20reasoning%20dataset%20with%20a%20strong%20LLM%20to%20align%20open-source%20language%20models%27%20ability%20to%20generate%20visual%20program%20workflows%20as%20FS-LLM.%20Next%2C%20we%20design%20a%20fast-slow%20reasoning%20framework%20with%20FS-LLM%3A%20Simple%20queries%20are%20directly%20solved%20by%20VideoLLMs%2C%20while%20difficult%20ones%20invoke%20visual%20program%20reasoning%2C%20motivated%20by%20human-like%20reasoning%20processes.%20During%20this%20process%2C%20low-confidence%20fast-thinking%20answers%20will%20trigger%20a%20second-stage%20slow-reasoning%20process%2C%20and%20a%20fallback%20mechanism%20to%20fast%20reasoning%20is%20activated%20if%20the%20program%20execution%20fails.%20Moreover%2C%20we%20improve%20visual%20programs%20through%20parameter%20search%20during%20both%20training%20and%20inference.%20By%20adjusting%20the%20parameters%20of%20the%20visual%20modules%20within%20the%20program%2C%20multiple%20variants%20are%20generated%3A%20during%20training%2C%20programs%20that%20yield%20correct%20answers%20are%20selected%2C%20while%20during%20inference%2C%20the%20program%20with%20the%20highest%20confidence%20result%20is%20applied.%20Experiments%20show%20that%20FS-VisPR%20improves%20both%20efficiency%20and%20reliability%20in%20visual%20program%20workflows.%20It%20achieves%2050.4%25%20accuracy%20on%20LVBench%2C%20surpassing%20GPT-4o%2C%20matching%20the%20performance%20of%20Qwen2.5VL-72B%20on%20VideoMME.%0ALink%3A%20http%3A//arxiv.org/abs/2509.17743v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoPro%253A%2520Adaptive%2520Program%2520Reasoning%2520for%2520Long%2520Video%2520Understanding%26entry.906535625%3DChenglin%2520Li%2520and%2520Feng%2520Han%2520and%2520Yikun%2520Wang%2520and%2520Ruilin%2520Li%2520and%2520Shuai%2520Dong%2520and%2520Haowen%2520Hou%2520and%2520Haitao%2520Li%2520and%2520Qianglong%2520Chen%2520and%2520Feng%2520Tao%2520and%2520Jingqi%2520Tong%2520and%2520Yin%2520Zhang%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520in%2520generating%2520program%2520workflows%2520for%2520visual%2520tasks.%2520However%252C%2520previous%2520approaches%2520often%2520rely%2520on%2520closed-source%2520models%252C%2520lack%2520systematic%2520reasoning%252C%2520and%2520struggle%2520with%2520long-form%2520video%2520question%2520answering%2520%2528videoQA%2529.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520FS-VisPR%2520framework%252C%2520an%2520adaptive%2520visual%2520program%2520reasoning%2520approach%2520that%2520balances%2520fast%2520reasoning%2520for%2520simple%2520queries%2520with%2520slow%2520reasoning%2520for%2520difficult%2520ones.%2520First%252C%2520we%2520design%2520efficient%2520visual%2520modules%2520%2528e.g.%252C%2520key%2520clip%2520retrieval%2520and%2520subtitle%2520retrieval%2529%2520to%2520support%2520long-form%2520video%2520tasks.%2520Then%252C%2520we%2520construct%2520a%2520diverse%2520and%2520high-quality%2520fast-slow%2520reasoning%2520dataset%2520with%2520a%2520strong%2520LLM%2520to%2520align%2520open-source%2520language%2520models%2527%2520ability%2520to%2520generate%2520visual%2520program%2520workflows%2520as%2520FS-LLM.%2520Next%252C%2520we%2520design%2520a%2520fast-slow%2520reasoning%2520framework%2520with%2520FS-LLM%253A%2520Simple%2520queries%2520are%2520directly%2520solved%2520by%2520VideoLLMs%252C%2520while%2520difficult%2520ones%2520invoke%2520visual%2520program%2520reasoning%252C%2520motivated%2520by%2520human-like%2520reasoning%2520processes.%2520During%2520this%2520process%252C%2520low-confidence%2520fast-thinking%2520answers%2520will%2520trigger%2520a%2520second-stage%2520slow-reasoning%2520process%252C%2520and%2520a%2520fallback%2520mechanism%2520to%2520fast%2520reasoning%2520is%2520activated%2520if%2520the%2520program%2520execution%2520fails.%2520Moreover%252C%2520we%2520improve%2520visual%2520programs%2520through%2520parameter%2520search%2520during%2520both%2520training%2520and%2520inference.%2520By%2520adjusting%2520the%2520parameters%2520of%2520the%2520visual%2520modules%2520within%2520the%2520program%252C%2520multiple%2520variants%2520are%2520generated%253A%2520during%2520training%252C%2520programs%2520that%2520yield%2520correct%2520answers%2520are%2520selected%252C%2520while%2520during%2520inference%252C%2520the%2520program%2520with%2520the%2520highest%2520confidence%2520result%2520is%2520applied.%2520Experiments%2520show%2520that%2520FS-VisPR%2520improves%2520both%2520efficiency%2520and%2520reliability%2520in%2520visual%2520program%2520workflows.%2520It%2520achieves%252050.4%2525%2520accuracy%2520on%2520LVBench%252C%2520surpassing%2520GPT-4o%252C%2520matching%2520the%2520performance%2520of%2520Qwen2.5VL-72B%2520on%2520VideoMME.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17743v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoPro%3A%20Adaptive%20Program%20Reasoning%20for%20Long%20Video%20Understanding&entry.906535625=Chenglin%20Li%20and%20Feng%20Han%20and%20Yikun%20Wang%20and%20Ruilin%20Li%20and%20Shuai%20Dong%20and%20Haowen%20Hou%20and%20Haitao%20Li%20and%20Qianglong%20Chen%20and%20Feng%20Tao%20and%20Jingqi%20Tong%20and%20Yin%20Zhang%20and%20Jiaqi%20Wang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20in%20generating%20program%20workflows%20for%20visual%20tasks.%20However%2C%20previous%20approaches%20often%20rely%20on%20closed-source%20models%2C%20lack%20systematic%20reasoning%2C%20and%20struggle%20with%20long-form%20video%20question%20answering%20%28videoQA%29.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%20FS-VisPR%20framework%2C%20an%20adaptive%20visual%20program%20reasoning%20approach%20that%20balances%20fast%20reasoning%20for%20simple%20queries%20with%20slow%20reasoning%20for%20difficult%20ones.%20First%2C%20we%20design%20efficient%20visual%20modules%20%28e.g.%2C%20key%20clip%20retrieval%20and%20subtitle%20retrieval%29%20to%20support%20long-form%20video%20tasks.%20Then%2C%20we%20construct%20a%20diverse%20and%20high-quality%20fast-slow%20reasoning%20dataset%20with%20a%20strong%20LLM%20to%20align%20open-source%20language%20models%27%20ability%20to%20generate%20visual%20program%20workflows%20as%20FS-LLM.%20Next%2C%20we%20design%20a%20fast-slow%20reasoning%20framework%20with%20FS-LLM%3A%20Simple%20queries%20are%20directly%20solved%20by%20VideoLLMs%2C%20while%20difficult%20ones%20invoke%20visual%20program%20reasoning%2C%20motivated%20by%20human-like%20reasoning%20processes.%20During%20this%20process%2C%20low-confidence%20fast-thinking%20answers%20will%20trigger%20a%20second-stage%20slow-reasoning%20process%2C%20and%20a%20fallback%20mechanism%20to%20fast%20reasoning%20is%20activated%20if%20the%20program%20execution%20fails.%20Moreover%2C%20we%20improve%20visual%20programs%20through%20parameter%20search%20during%20both%20training%20and%20inference.%20By%20adjusting%20the%20parameters%20of%20the%20visual%20modules%20within%20the%20program%2C%20multiple%20variants%20are%20generated%3A%20during%20training%2C%20programs%20that%20yield%20correct%20answers%20are%20selected%2C%20while%20during%20inference%2C%20the%20program%20with%20the%20highest%20confidence%20result%20is%20applied.%20Experiments%20show%20that%20FS-VisPR%20improves%20both%20efficiency%20and%20reliability%20in%20visual%20program%20workflows.%20It%20achieves%2050.4%25%20accuracy%20on%20LVBench%2C%20surpassing%20GPT-4o%2C%20matching%20the%20performance%20of%20Qwen2.5VL-72B%20on%20VideoMME.&entry.1838667208=http%3A//arxiv.org/abs/2509.17743v3&entry.124074799=Read"},
{"title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models", "author": "Chenyang Li and Jieyuan Liu and Bin Li and Bo Gao and Yilin Yuan and Yangfan He and Yuchen Li and Jingqun Tang", "abstract": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.", "link": "http://arxiv.org/abs/2601.16065v1", "date": "2026-01-22", "relevancy": 2.892, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DTP%3A%20A%20Simple%20yet%20Effective%20Distracting%20Token%20Pruning%20Framework%20for%20Vision-Language%20Action%20Models&body=Title%3A%20DTP%3A%20A%20Simple%20yet%20Effective%20Distracting%20Token%20Pruning%20Framework%20for%20Vision-Language%20Action%20Models%0AAuthor%3A%20Chenyang%20Li%20and%20Jieyuan%20Liu%20and%20Bin%20Li%20and%20Bo%20Gao%20and%20Yilin%20Yuan%20and%20Yangfan%20He%20and%20Yuchen%20Li%20and%20Jingqun%20Tang%0AAbstract%3A%20Vision-Language%20Action%20%28VLA%29%20models%20have%20shown%20remarkable%20progress%20in%20robotic%20manipulation%20by%20leveraging%20the%20powerful%20perception%20abilities%20of%20Vision-Language%20Models%20%28VLMs%29%20to%20understand%20environments%20and%20directly%20output%20actions.%20However%2C%20by%20default%2C%20VLA%20models%20may%20overly%20attend%20to%20image%20tokens%20in%20the%20task-irrelevant%20region%2C%20which%20we%20describe%20as%20%27distracting%20tokens%27.%20This%20behavior%20can%20disturb%20the%20model%20from%20the%20generation%20of%20the%20desired%20action%20tokens%20in%20each%20step%2C%20affecting%20the%20success%20rate%20of%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20simple%20yet%20effective%20plug-and-play%20Distracting%20Token%20Pruning%20%28DTP%29%20framework%2C%20which%20dynamically%20detects%20and%20prunes%20these%20distracting%20image%20tokens.%20By%20correcting%20the%20model%27s%20visual%20attention%20patterns%2C%20we%20aim%20to%20improve%20the%20task%20success%20rate%2C%20as%20well%20as%20exploring%20the%20performance%20upper%20boundaries%20of%20the%20model%20without%20altering%20its%20original%20architecture%20or%20adding%20additional%20inputs.%20Experiments%20on%20the%20SIMPLER%20Benchmark%20%28Li%20et%20al.%2C%202024%29%20show%20that%20our%20method%20consistently%20achieving%20relative%20improvements%20in%20task%20success%20rates%20across%20different%20types%20of%20novel%20VLA%20models%2C%20demonstrating%20generalizability%20to%20transformer-based%20VLAs.%20Further%20analysis%20reveals%20a%20negative%20correlation%20between%20the%20task%20success%20rate%20and%20the%20amount%20of%20attentions%20in%20the%20task-irrelevant%20region%20for%20all%20models%20tested%2C%20highlighting%20a%20common%20phenomenon%20of%20VLA%20models%20that%20could%20guide%20future%20research.%20We%20also%20publish%20our%20code%20at%3A%20https%3A//anonymous.4open.science/r/CBD3.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDTP%253A%2520A%2520Simple%2520yet%2520Effective%2520Distracting%2520Token%2520Pruning%2520Framework%2520for%2520Vision-Language%2520Action%2520Models%26entry.906535625%3DChenyang%2520Li%2520and%2520Jieyuan%2520Liu%2520and%2520Bin%2520Li%2520and%2520Bo%2520Gao%2520and%2520Yilin%2520Yuan%2520and%2520Yangfan%2520He%2520and%2520Yuchen%2520Li%2520and%2520Jingqun%2520Tang%26entry.1292438233%3DVision-Language%2520Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520remarkable%2520progress%2520in%2520robotic%2520manipulation%2520by%2520leveraging%2520the%2520powerful%2520perception%2520abilities%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520understand%2520environments%2520and%2520directly%2520output%2520actions.%2520However%252C%2520by%2520default%252C%2520VLA%2520models%2520may%2520overly%2520attend%2520to%2520image%2520tokens%2520in%2520the%2520task-irrelevant%2520region%252C%2520which%2520we%2520describe%2520as%2520%2527distracting%2520tokens%2527.%2520This%2520behavior%2520can%2520disturb%2520the%2520model%2520from%2520the%2520generation%2520of%2520the%2520desired%2520action%2520tokens%2520in%2520each%2520step%252C%2520affecting%2520the%2520success%2520rate%2520of%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%2520plug-and-play%2520Distracting%2520Token%2520Pruning%2520%2528DTP%2529%2520framework%252C%2520which%2520dynamically%2520detects%2520and%2520prunes%2520these%2520distracting%2520image%2520tokens.%2520By%2520correcting%2520the%2520model%2527s%2520visual%2520attention%2520patterns%252C%2520we%2520aim%2520to%2520improve%2520the%2520task%2520success%2520rate%252C%2520as%2520well%2520as%2520exploring%2520the%2520performance%2520upper%2520boundaries%2520of%2520the%2520model%2520without%2520altering%2520its%2520original%2520architecture%2520or%2520adding%2520additional%2520inputs.%2520Experiments%2520on%2520the%2520SIMPLER%2520Benchmark%2520%2528Li%2520et%2520al.%252C%25202024%2529%2520show%2520that%2520our%2520method%2520consistently%2520achieving%2520relative%2520improvements%2520in%2520task%2520success%2520rates%2520across%2520different%2520types%2520of%2520novel%2520VLA%2520models%252C%2520demonstrating%2520generalizability%2520to%2520transformer-based%2520VLAs.%2520Further%2520analysis%2520reveals%2520a%2520negative%2520correlation%2520between%2520the%2520task%2520success%2520rate%2520and%2520the%2520amount%2520of%2520attentions%2520in%2520the%2520task-irrelevant%2520region%2520for%2520all%2520models%2520tested%252C%2520highlighting%2520a%2520common%2520phenomenon%2520of%2520VLA%2520models%2520that%2520could%2520guide%2520future%2520research.%2520We%2520also%2520publish%2520our%2520code%2520at%253A%2520https%253A//anonymous.4open.science/r/CBD3.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DTP%3A%20A%20Simple%20yet%20Effective%20Distracting%20Token%20Pruning%20Framework%20for%20Vision-Language%20Action%20Models&entry.906535625=Chenyang%20Li%20and%20Jieyuan%20Liu%20and%20Bin%20Li%20and%20Bo%20Gao%20and%20Yilin%20Yuan%20and%20Yangfan%20He%20and%20Yuchen%20Li%20and%20Jingqun%20Tang&entry.1292438233=Vision-Language%20Action%20%28VLA%29%20models%20have%20shown%20remarkable%20progress%20in%20robotic%20manipulation%20by%20leveraging%20the%20powerful%20perception%20abilities%20of%20Vision-Language%20Models%20%28VLMs%29%20to%20understand%20environments%20and%20directly%20output%20actions.%20However%2C%20by%20default%2C%20VLA%20models%20may%20overly%20attend%20to%20image%20tokens%20in%20the%20task-irrelevant%20region%2C%20which%20we%20describe%20as%20%27distracting%20tokens%27.%20This%20behavior%20can%20disturb%20the%20model%20from%20the%20generation%20of%20the%20desired%20action%20tokens%20in%20each%20step%2C%20affecting%20the%20success%20rate%20of%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20simple%20yet%20effective%20plug-and-play%20Distracting%20Token%20Pruning%20%28DTP%29%20framework%2C%20which%20dynamically%20detects%20and%20prunes%20these%20distracting%20image%20tokens.%20By%20correcting%20the%20model%27s%20visual%20attention%20patterns%2C%20we%20aim%20to%20improve%20the%20task%20success%20rate%2C%20as%20well%20as%20exploring%20the%20performance%20upper%20boundaries%20of%20the%20model%20without%20altering%20its%20original%20architecture%20or%20adding%20additional%20inputs.%20Experiments%20on%20the%20SIMPLER%20Benchmark%20%28Li%20et%20al.%2C%202024%29%20show%20that%20our%20method%20consistently%20achieving%20relative%20improvements%20in%20task%20success%20rates%20across%20different%20types%20of%20novel%20VLA%20models%2C%20demonstrating%20generalizability%20to%20transformer-based%20VLAs.%20Further%20analysis%20reveals%20a%20negative%20correlation%20between%20the%20task%20success%20rate%20and%20the%20amount%20of%20attentions%20in%20the%20task-irrelevant%20region%20for%20all%20models%20tested%2C%20highlighting%20a%20common%20phenomenon%20of%20VLA%20models%20that%20could%20guide%20future%20research.%20We%20also%20publish%20our%20code%20at%3A%20https%3A//anonymous.4open.science/r/CBD3.&entry.1838667208=http%3A//arxiv.org/abs/2601.16065v1&entry.124074799=Read"},
{"title": "No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images", "author": "Diego Eustachio Farchione and Ramzi Idoughi and Peter Wonka", "abstract": "Effective reef monitoring requires the quantification of coral growth via accurate volumetric and surface area estimates, which is a challenging task due to the complex morphology of corals. We propose a novel, lightweight, and scalable learning framework that addresses this challenge by predicting the 3D volume and surface area of coral-like objects from 2D multi-view RGB images. Our approach utilizes a pre-trained module (VGGT) to extract dense point maps from each view; these maps are merged into a unified point cloud and enriched with per-view confidence scores. The resulting cloud is fed to two parallel DGCNN decoder heads, which jointly output the volume and the surface area of the coral, as well as their corresponding confidence estimate. To enhance prediction stability and provide uncertainty estimates, we introduce a composite loss function based on Gaussian negative log-likelihood in both real and log domains. Our method achieves competitive accuracy and generalizes well to unseen morphologies. This framework paves the way for efficient and scalable coral geometry estimation directly from a sparse set of images, with potential applications in coral growth analysis and reef monitoring.", "link": "http://arxiv.org/abs/2509.11164v3", "date": "2026-01-22", "relevancy": 2.8619, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5829}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5671}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Mesh%2C%20No%20Problem%3A%20Estimating%20Coral%20Volume%20and%20Surface%20from%20Sparse%20Multi-View%20Images&body=Title%3A%20No%20Mesh%2C%20No%20Problem%3A%20Estimating%20Coral%20Volume%20and%20Surface%20from%20Sparse%20Multi-View%20Images%0AAuthor%3A%20Diego%20Eustachio%20Farchione%20and%20Ramzi%20Idoughi%20and%20Peter%20Wonka%0AAbstract%3A%20Effective%20reef%20monitoring%20requires%20the%20quantification%20of%20coral%20growth%20via%20accurate%20volumetric%20and%20surface%20area%20estimates%2C%20which%20is%20a%20challenging%20task%20due%20to%20the%20complex%20morphology%20of%20corals.%20We%20propose%20a%20novel%2C%20lightweight%2C%20and%20scalable%20learning%20framework%20that%20addresses%20this%20challenge%20by%20predicting%20the%203D%20volume%20and%20surface%20area%20of%20coral-like%20objects%20from%202D%20multi-view%20RGB%20images.%20Our%20approach%20utilizes%20a%20pre-trained%20module%20%28VGGT%29%20to%20extract%20dense%20point%20maps%20from%20each%20view%3B%20these%20maps%20are%20merged%20into%20a%20unified%20point%20cloud%20and%20enriched%20with%20per-view%20confidence%20scores.%20The%20resulting%20cloud%20is%20fed%20to%20two%20parallel%20DGCNN%20decoder%20heads%2C%20which%20jointly%20output%20the%20volume%20and%20the%20surface%20area%20of%20the%20coral%2C%20as%20well%20as%20their%20corresponding%20confidence%20estimate.%20To%20enhance%20prediction%20stability%20and%20provide%20uncertainty%20estimates%2C%20we%20introduce%20a%20composite%20loss%20function%20based%20on%20Gaussian%20negative%20log-likelihood%20in%20both%20real%20and%20log%20domains.%20Our%20method%20achieves%20competitive%20accuracy%20and%20generalizes%20well%20to%20unseen%20morphologies.%20This%20framework%20paves%20the%20way%20for%20efficient%20and%20scalable%20coral%20geometry%20estimation%20directly%20from%20a%20sparse%20set%20of%20images%2C%20with%20potential%20applications%20in%20coral%20growth%20analysis%20and%20reef%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2509.11164v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Mesh%252C%2520No%2520Problem%253A%2520Estimating%2520Coral%2520Volume%2520and%2520Surface%2520from%2520Sparse%2520Multi-View%2520Images%26entry.906535625%3DDiego%2520Eustachio%2520Farchione%2520and%2520Ramzi%2520Idoughi%2520and%2520Peter%2520Wonka%26entry.1292438233%3DEffective%2520reef%2520monitoring%2520requires%2520the%2520quantification%2520of%2520coral%2520growth%2520via%2520accurate%2520volumetric%2520and%2520surface%2520area%2520estimates%252C%2520which%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%2520complex%2520morphology%2520of%2520corals.%2520We%2520propose%2520a%2520novel%252C%2520lightweight%252C%2520and%2520scalable%2520learning%2520framework%2520that%2520addresses%2520this%2520challenge%2520by%2520predicting%2520the%25203D%2520volume%2520and%2520surface%2520area%2520of%2520coral-like%2520objects%2520from%25202D%2520multi-view%2520RGB%2520images.%2520Our%2520approach%2520utilizes%2520a%2520pre-trained%2520module%2520%2528VGGT%2529%2520to%2520extract%2520dense%2520point%2520maps%2520from%2520each%2520view%253B%2520these%2520maps%2520are%2520merged%2520into%2520a%2520unified%2520point%2520cloud%2520and%2520enriched%2520with%2520per-view%2520confidence%2520scores.%2520The%2520resulting%2520cloud%2520is%2520fed%2520to%2520two%2520parallel%2520DGCNN%2520decoder%2520heads%252C%2520which%2520jointly%2520output%2520the%2520volume%2520and%2520the%2520surface%2520area%2520of%2520the%2520coral%252C%2520as%2520well%2520as%2520their%2520corresponding%2520confidence%2520estimate.%2520To%2520enhance%2520prediction%2520stability%2520and%2520provide%2520uncertainty%2520estimates%252C%2520we%2520introduce%2520a%2520composite%2520loss%2520function%2520based%2520on%2520Gaussian%2520negative%2520log-likelihood%2520in%2520both%2520real%2520and%2520log%2520domains.%2520Our%2520method%2520achieves%2520competitive%2520accuracy%2520and%2520generalizes%2520well%2520to%2520unseen%2520morphologies.%2520This%2520framework%2520paves%2520the%2520way%2520for%2520efficient%2520and%2520scalable%2520coral%2520geometry%2520estimation%2520directly%2520from%2520a%2520sparse%2520set%2520of%2520images%252C%2520with%2520potential%2520applications%2520in%2520coral%2520growth%2520analysis%2520and%2520reef%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11164v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Mesh%2C%20No%20Problem%3A%20Estimating%20Coral%20Volume%20and%20Surface%20from%20Sparse%20Multi-View%20Images&entry.906535625=Diego%20Eustachio%20Farchione%20and%20Ramzi%20Idoughi%20and%20Peter%20Wonka&entry.1292438233=Effective%20reef%20monitoring%20requires%20the%20quantification%20of%20coral%20growth%20via%20accurate%20volumetric%20and%20surface%20area%20estimates%2C%20which%20is%20a%20challenging%20task%20due%20to%20the%20complex%20morphology%20of%20corals.%20We%20propose%20a%20novel%2C%20lightweight%2C%20and%20scalable%20learning%20framework%20that%20addresses%20this%20challenge%20by%20predicting%20the%203D%20volume%20and%20surface%20area%20of%20coral-like%20objects%20from%202D%20multi-view%20RGB%20images.%20Our%20approach%20utilizes%20a%20pre-trained%20module%20%28VGGT%29%20to%20extract%20dense%20point%20maps%20from%20each%20view%3B%20these%20maps%20are%20merged%20into%20a%20unified%20point%20cloud%20and%20enriched%20with%20per-view%20confidence%20scores.%20The%20resulting%20cloud%20is%20fed%20to%20two%20parallel%20DGCNN%20decoder%20heads%2C%20which%20jointly%20output%20the%20volume%20and%20the%20surface%20area%20of%20the%20coral%2C%20as%20well%20as%20their%20corresponding%20confidence%20estimate.%20To%20enhance%20prediction%20stability%20and%20provide%20uncertainty%20estimates%2C%20we%20introduce%20a%20composite%20loss%20function%20based%20on%20Gaussian%20negative%20log-likelihood%20in%20both%20real%20and%20log%20domains.%20Our%20method%20achieves%20competitive%20accuracy%20and%20generalizes%20well%20to%20unseen%20morphologies.%20This%20framework%20paves%20the%20way%20for%20efficient%20and%20scalable%20coral%20geometry%20estimation%20directly%20from%20a%20sparse%20set%20of%20images%2C%20with%20potential%20applications%20in%20coral%20growth%20analysis%20and%20reef%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2509.11164v3&entry.124074799=Read"},
{"title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems", "author": "Zijie Chen and Xiaowei Liu and Yong Xu and Shenghai Yuan and Jianping Li and Lihua Xie", "abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\\_calibr}}. The video is available at \\textcolor{blue}{\\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}", "link": "http://arxiv.org/abs/2601.15946v1", "date": "2026-01-22", "relevancy": 2.748, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5681}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5404}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accurate%20Calibration%20and%20Robust%20LiDAR-Inertial%20Odometry%20for%20Spinning%20Actuated%20LiDAR%20Systems&body=Title%3A%20Accurate%20Calibration%20and%20Robust%20LiDAR-Inertial%20Odometry%20for%20Spinning%20Actuated%20LiDAR%20Systems%0AAuthor%3A%20Zijie%20Chen%20and%20Xiaowei%20Liu%20and%20Yong%20Xu%20and%20Shenghai%20Yuan%20and%20Jianping%20Li%20and%20Lihua%20Xie%0AAbstract%3A%20Accurate%20calibration%20and%20robust%20localization%20are%20fundamental%20for%20downstream%20tasks%20in%20spinning%20actuated%20LiDAR%20applications.%20Existing%20methods%2C%20however%2C%20require%20parameterizing%20extrinsic%20parameters%20based%20on%20different%20mounting%20configurations%2C%20limiting%20their%20generalizability.%20Additionally%2C%20spinning%20actuated%20LiDAR%20inevitably%20scans%20featureless%20regions%2C%20which%20complicates%20the%20balance%20between%20scanning%20coverage%20and%20localization%20robustness.%20To%20address%20these%20challenges%2C%20this%20letter%20presents%20a%20targetless%20LiDAR-motor%20calibration%20%28LM-Calibr%29%20on%20the%20basis%20of%20the%20Denavit-Hartenberg%20convention%20and%20an%20environmental%20adaptive%20LiDAR-inertial%20odometry%20%28EVA-LIO%29.%20LM-Calibr%20supports%20calibration%20of%20LiDAR-motor%20systems%20with%20various%20mounting%20configurations.%20Extensive%20experiments%20demonstrate%20its%20accuracy%20and%20convergence%20across%20different%20scenarios%2C%20mounting%20angles%2C%20and%20initial%20values.%20Additionally%2C%20EVA-LIO%20adaptively%20selects%20downsample%20rates%20and%20map%20resolutions%20according%20to%20spatial%20scale.%20This%20adaptivity%20enables%20the%20actuator%20to%20operate%20at%20maximum%20speed%2C%20thereby%20enhancing%20scanning%20completeness%20while%20ensuring%20robust%20localization%2C%20even%20when%20LiDAR%20briefly%20scans%20featureless%20areas.%20The%20source%20code%20and%20hardware%20design%20are%20available%20on%20GitHub%3A%20%5Ctextcolor%7Bblue%7D%7B%5Chref%7Bhttps%3A//github.com/zijiechenrobotics/lm_calibr%7D%7Bgithub.com/zijiechenrobotics/lm%5C_calibr%7D%7D.%20The%20video%20is%20available%20at%20%5Ctextcolor%7Bblue%7D%7B%5Chref%7Bhttps%3A//youtu.be/cZyyrkmeoSk%7D%7Byoutu.be/cZyyrkmeoSk%7D%7D%0ALink%3A%20http%3A//arxiv.org/abs/2601.15946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccurate%2520Calibration%2520and%2520Robust%2520LiDAR-Inertial%2520Odometry%2520for%2520Spinning%2520Actuated%2520LiDAR%2520Systems%26entry.906535625%3DZijie%2520Chen%2520and%2520Xiaowei%2520Liu%2520and%2520Yong%2520Xu%2520and%2520Shenghai%2520Yuan%2520and%2520Jianping%2520Li%2520and%2520Lihua%2520Xie%26entry.1292438233%3DAccurate%2520calibration%2520and%2520robust%2520localization%2520are%2520fundamental%2520for%2520downstream%2520tasks%2520in%2520spinning%2520actuated%2520LiDAR%2520applications.%2520Existing%2520methods%252C%2520however%252C%2520require%2520parameterizing%2520extrinsic%2520parameters%2520based%2520on%2520different%2520mounting%2520configurations%252C%2520limiting%2520their%2520generalizability.%2520Additionally%252C%2520spinning%2520actuated%2520LiDAR%2520inevitably%2520scans%2520featureless%2520regions%252C%2520which%2520complicates%2520the%2520balance%2520between%2520scanning%2520coverage%2520and%2520localization%2520robustness.%2520To%2520address%2520these%2520challenges%252C%2520this%2520letter%2520presents%2520a%2520targetless%2520LiDAR-motor%2520calibration%2520%2528LM-Calibr%2529%2520on%2520the%2520basis%2520of%2520the%2520Denavit-Hartenberg%2520convention%2520and%2520an%2520environmental%2520adaptive%2520LiDAR-inertial%2520odometry%2520%2528EVA-LIO%2529.%2520LM-Calibr%2520supports%2520calibration%2520of%2520LiDAR-motor%2520systems%2520with%2520various%2520mounting%2520configurations.%2520Extensive%2520experiments%2520demonstrate%2520its%2520accuracy%2520and%2520convergence%2520across%2520different%2520scenarios%252C%2520mounting%2520angles%252C%2520and%2520initial%2520values.%2520Additionally%252C%2520EVA-LIO%2520adaptively%2520selects%2520downsample%2520rates%2520and%2520map%2520resolutions%2520according%2520to%2520spatial%2520scale.%2520This%2520adaptivity%2520enables%2520the%2520actuator%2520to%2520operate%2520at%2520maximum%2520speed%252C%2520thereby%2520enhancing%2520scanning%2520completeness%2520while%2520ensuring%2520robust%2520localization%252C%2520even%2520when%2520LiDAR%2520briefly%2520scans%2520featureless%2520areas.%2520The%2520source%2520code%2520and%2520hardware%2520design%2520are%2520available%2520on%2520GitHub%253A%2520%255Ctextcolor%257Bblue%257D%257B%255Chref%257Bhttps%253A//github.com/zijiechenrobotics/lm_calibr%257D%257Bgithub.com/zijiechenrobotics/lm%255C_calibr%257D%257D.%2520The%2520video%2520is%2520available%2520at%2520%255Ctextcolor%257Bblue%257D%257B%255Chref%257Bhttps%253A//youtu.be/cZyyrkmeoSk%257D%257Byoutu.be/cZyyrkmeoSk%257D%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20Calibration%20and%20Robust%20LiDAR-Inertial%20Odometry%20for%20Spinning%20Actuated%20LiDAR%20Systems&entry.906535625=Zijie%20Chen%20and%20Xiaowei%20Liu%20and%20Yong%20Xu%20and%20Shenghai%20Yuan%20and%20Jianping%20Li%20and%20Lihua%20Xie&entry.1292438233=Accurate%20calibration%20and%20robust%20localization%20are%20fundamental%20for%20downstream%20tasks%20in%20spinning%20actuated%20LiDAR%20applications.%20Existing%20methods%2C%20however%2C%20require%20parameterizing%20extrinsic%20parameters%20based%20on%20different%20mounting%20configurations%2C%20limiting%20their%20generalizability.%20Additionally%2C%20spinning%20actuated%20LiDAR%20inevitably%20scans%20featureless%20regions%2C%20which%20complicates%20the%20balance%20between%20scanning%20coverage%20and%20localization%20robustness.%20To%20address%20these%20challenges%2C%20this%20letter%20presents%20a%20targetless%20LiDAR-motor%20calibration%20%28LM-Calibr%29%20on%20the%20basis%20of%20the%20Denavit-Hartenberg%20convention%20and%20an%20environmental%20adaptive%20LiDAR-inertial%20odometry%20%28EVA-LIO%29.%20LM-Calibr%20supports%20calibration%20of%20LiDAR-motor%20systems%20with%20various%20mounting%20configurations.%20Extensive%20experiments%20demonstrate%20its%20accuracy%20and%20convergence%20across%20different%20scenarios%2C%20mounting%20angles%2C%20and%20initial%20values.%20Additionally%2C%20EVA-LIO%20adaptively%20selects%20downsample%20rates%20and%20map%20resolutions%20according%20to%20spatial%20scale.%20This%20adaptivity%20enables%20the%20actuator%20to%20operate%20at%20maximum%20speed%2C%20thereby%20enhancing%20scanning%20completeness%20while%20ensuring%20robust%20localization%2C%20even%20when%20LiDAR%20briefly%20scans%20featureless%20areas.%20The%20source%20code%20and%20hardware%20design%20are%20available%20on%20GitHub%3A%20%5Ctextcolor%7Bblue%7D%7B%5Chref%7Bhttps%3A//github.com/zijiechenrobotics/lm_calibr%7D%7Bgithub.com/zijiechenrobotics/lm%5C_calibr%7D%7D.%20The%20video%20is%20available%20at%20%5Ctextcolor%7Bblue%7D%7B%5Chref%7Bhttps%3A//youtu.be/cZyyrkmeoSk%7D%7Byoutu.be/cZyyrkmeoSk%7D%7D&entry.1838667208=http%3A//arxiv.org/abs/2601.15946v1&entry.124074799=Read"},
{"title": "Language-guided Medical Image Segmentation with Target-informed Multi-level Contrastive Alignments", "author": "Mingjian Li and Mingyuan Meng and Shuchang Ye and Michael Fulham and Lei Bi and Jinman Kim", "abstract": "Medical image segmentation is a fundamental task in numerous medical engineering applications. Recently, language-guided segmentation has shown promise in medical scenarios where textual clinical reports are readily available as semantic guidance. Clinical reports contain diagnostic information provided by clinicians, which can provide auxiliary textual semantics to guide segmentation. However, existing language-guided segmentation methods neglect the inherent pattern gaps between image and text modalities, resulting in sub-optimal visual-language integration. Contrastive learning is a well-recognized approach to align image-text patterns, but it has not been optimized for bridging the pattern gaps in medical language-guided segmentation that relies primarily on medical image details to characterize the underlying disease/targets. Current contrastive alignment techniques typically align high-level global semantics without involving low-level localized target information, and thus cannot deliver fine-grained textual guidance on crucial image details. In this study, we propose a Target-informed Multi-level Contrastive Alignment framework (TMCA) to bridge image-text pattern gaps for medical language-guided segmentation. TMCA enables target-informed image-text alignments and fine-grained textual guidance by introducing: (i) a target-sensitive semantic distance module that utilizes target information for more granular image-text alignment modeling, (ii) a multi-level contrastive alignment strategy that directs fine-grained textual guidance to multi-scale image details, and (iii) a language-guided target enhancement module that reinforces attention to critical image regions based on the aligned image-text patterns. Extensive experiments on four public benchmark datasets demonstrate that TMCA enabled superior performance over state-of-the-art language-guided medical image segmentation methods.", "link": "http://arxiv.org/abs/2412.13533v3", "date": "2026-01-22", "relevancy": 2.68, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.591}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-guided%20Medical%20Image%20Segmentation%20with%20Target-informed%20Multi-level%20Contrastive%20Alignments&body=Title%3A%20Language-guided%20Medical%20Image%20Segmentation%20with%20Target-informed%20Multi-level%20Contrastive%20Alignments%0AAuthor%3A%20Mingjian%20Li%20and%20Mingyuan%20Meng%20and%20Shuchang%20Ye%20and%20Michael%20Fulham%20and%20Lei%20Bi%20and%20Jinman%20Kim%0AAbstract%3A%20Medical%20image%20segmentation%20is%20a%20fundamental%20task%20in%20numerous%20medical%20engineering%20applications.%20Recently%2C%20language-guided%20segmentation%20has%20shown%20promise%20in%20medical%20scenarios%20where%20textual%20clinical%20reports%20are%20readily%20available%20as%20semantic%20guidance.%20Clinical%20reports%20contain%20diagnostic%20information%20provided%20by%20clinicians%2C%20which%20can%20provide%20auxiliary%20textual%20semantics%20to%20guide%20segmentation.%20However%2C%20existing%20language-guided%20segmentation%20methods%20neglect%20the%20inherent%20pattern%20gaps%20between%20image%20and%20text%20modalities%2C%20resulting%20in%20sub-optimal%20visual-language%20integration.%20Contrastive%20learning%20is%20a%20well-recognized%20approach%20to%20align%20image-text%20patterns%2C%20but%20it%20has%20not%20been%20optimized%20for%20bridging%20the%20pattern%20gaps%20in%20medical%20language-guided%20segmentation%20that%20relies%20primarily%20on%20medical%20image%20details%20to%20characterize%20the%20underlying%20disease/targets.%20Current%20contrastive%20alignment%20techniques%20typically%20align%20high-level%20global%20semantics%20without%20involving%20low-level%20localized%20target%20information%2C%20and%20thus%20cannot%20deliver%20fine-grained%20textual%20guidance%20on%20crucial%20image%20details.%20In%20this%20study%2C%20we%20propose%20a%20Target-informed%20Multi-level%20Contrastive%20Alignment%20framework%20%28TMCA%29%20to%20bridge%20image-text%20pattern%20gaps%20for%20medical%20language-guided%20segmentation.%20TMCA%20enables%20target-informed%20image-text%20alignments%20and%20fine-grained%20textual%20guidance%20by%20introducing%3A%20%28i%29%20a%20target-sensitive%20semantic%20distance%20module%20that%20utilizes%20target%20information%20for%20more%20granular%20image-text%20alignment%20modeling%2C%20%28ii%29%20a%20multi-level%20contrastive%20alignment%20strategy%20that%20directs%20fine-grained%20textual%20guidance%20to%20multi-scale%20image%20details%2C%20and%20%28iii%29%20a%20language-guided%20target%20enhancement%20module%20that%20reinforces%20attention%20to%20critical%20image%20regions%20based%20on%20the%20aligned%20image-text%20patterns.%20Extensive%20experiments%20on%20four%20public%20benchmark%20datasets%20demonstrate%20that%20TMCA%20enabled%20superior%20performance%20over%20state-of-the-art%20language-guided%20medical%20image%20segmentation%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2412.13533v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-guided%2520Medical%2520Image%2520Segmentation%2520with%2520Target-informed%2520Multi-level%2520Contrastive%2520Alignments%26entry.906535625%3DMingjian%2520Li%2520and%2520Mingyuan%2520Meng%2520and%2520Shuchang%2520Ye%2520and%2520Michael%2520Fulham%2520and%2520Lei%2520Bi%2520and%2520Jinman%2520Kim%26entry.1292438233%3DMedical%2520image%2520segmentation%2520is%2520a%2520fundamental%2520task%2520in%2520numerous%2520medical%2520engineering%2520applications.%2520Recently%252C%2520language-guided%2520segmentation%2520has%2520shown%2520promise%2520in%2520medical%2520scenarios%2520where%2520textual%2520clinical%2520reports%2520are%2520readily%2520available%2520as%2520semantic%2520guidance.%2520Clinical%2520reports%2520contain%2520diagnostic%2520information%2520provided%2520by%2520clinicians%252C%2520which%2520can%2520provide%2520auxiliary%2520textual%2520semantics%2520to%2520guide%2520segmentation.%2520However%252C%2520existing%2520language-guided%2520segmentation%2520methods%2520neglect%2520the%2520inherent%2520pattern%2520gaps%2520between%2520image%2520and%2520text%2520modalities%252C%2520resulting%2520in%2520sub-optimal%2520visual-language%2520integration.%2520Contrastive%2520learning%2520is%2520a%2520well-recognized%2520approach%2520to%2520align%2520image-text%2520patterns%252C%2520but%2520it%2520has%2520not%2520been%2520optimized%2520for%2520bridging%2520the%2520pattern%2520gaps%2520in%2520medical%2520language-guided%2520segmentation%2520that%2520relies%2520primarily%2520on%2520medical%2520image%2520details%2520to%2520characterize%2520the%2520underlying%2520disease/targets.%2520Current%2520contrastive%2520alignment%2520techniques%2520typically%2520align%2520high-level%2520global%2520semantics%2520without%2520involving%2520low-level%2520localized%2520target%2520information%252C%2520and%2520thus%2520cannot%2520deliver%2520fine-grained%2520textual%2520guidance%2520on%2520crucial%2520image%2520details.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520Target-informed%2520Multi-level%2520Contrastive%2520Alignment%2520framework%2520%2528TMCA%2529%2520to%2520bridge%2520image-text%2520pattern%2520gaps%2520for%2520medical%2520language-guided%2520segmentation.%2520TMCA%2520enables%2520target-informed%2520image-text%2520alignments%2520and%2520fine-grained%2520textual%2520guidance%2520by%2520introducing%253A%2520%2528i%2529%2520a%2520target-sensitive%2520semantic%2520distance%2520module%2520that%2520utilizes%2520target%2520information%2520for%2520more%2520granular%2520image-text%2520alignment%2520modeling%252C%2520%2528ii%2529%2520a%2520multi-level%2520contrastive%2520alignment%2520strategy%2520that%2520directs%2520fine-grained%2520textual%2520guidance%2520to%2520multi-scale%2520image%2520details%252C%2520and%2520%2528iii%2529%2520a%2520language-guided%2520target%2520enhancement%2520module%2520that%2520reinforces%2520attention%2520to%2520critical%2520image%2520regions%2520based%2520on%2520the%2520aligned%2520image-text%2520patterns.%2520Extensive%2520experiments%2520on%2520four%2520public%2520benchmark%2520datasets%2520demonstrate%2520that%2520TMCA%2520enabled%2520superior%2520performance%2520over%2520state-of-the-art%2520language-guided%2520medical%2520image%2520segmentation%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13533v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-guided%20Medical%20Image%20Segmentation%20with%20Target-informed%20Multi-level%20Contrastive%20Alignments&entry.906535625=Mingjian%20Li%20and%20Mingyuan%20Meng%20and%20Shuchang%20Ye%20and%20Michael%20Fulham%20and%20Lei%20Bi%20and%20Jinman%20Kim&entry.1292438233=Medical%20image%20segmentation%20is%20a%20fundamental%20task%20in%20numerous%20medical%20engineering%20applications.%20Recently%2C%20language-guided%20segmentation%20has%20shown%20promise%20in%20medical%20scenarios%20where%20textual%20clinical%20reports%20are%20readily%20available%20as%20semantic%20guidance.%20Clinical%20reports%20contain%20diagnostic%20information%20provided%20by%20clinicians%2C%20which%20can%20provide%20auxiliary%20textual%20semantics%20to%20guide%20segmentation.%20However%2C%20existing%20language-guided%20segmentation%20methods%20neglect%20the%20inherent%20pattern%20gaps%20between%20image%20and%20text%20modalities%2C%20resulting%20in%20sub-optimal%20visual-language%20integration.%20Contrastive%20learning%20is%20a%20well-recognized%20approach%20to%20align%20image-text%20patterns%2C%20but%20it%20has%20not%20been%20optimized%20for%20bridging%20the%20pattern%20gaps%20in%20medical%20language-guided%20segmentation%20that%20relies%20primarily%20on%20medical%20image%20details%20to%20characterize%20the%20underlying%20disease/targets.%20Current%20contrastive%20alignment%20techniques%20typically%20align%20high-level%20global%20semantics%20without%20involving%20low-level%20localized%20target%20information%2C%20and%20thus%20cannot%20deliver%20fine-grained%20textual%20guidance%20on%20crucial%20image%20details.%20In%20this%20study%2C%20we%20propose%20a%20Target-informed%20Multi-level%20Contrastive%20Alignment%20framework%20%28TMCA%29%20to%20bridge%20image-text%20pattern%20gaps%20for%20medical%20language-guided%20segmentation.%20TMCA%20enables%20target-informed%20image-text%20alignments%20and%20fine-grained%20textual%20guidance%20by%20introducing%3A%20%28i%29%20a%20target-sensitive%20semantic%20distance%20module%20that%20utilizes%20target%20information%20for%20more%20granular%20image-text%20alignment%20modeling%2C%20%28ii%29%20a%20multi-level%20contrastive%20alignment%20strategy%20that%20directs%20fine-grained%20textual%20guidance%20to%20multi-scale%20image%20details%2C%20and%20%28iii%29%20a%20language-guided%20target%20enhancement%20module%20that%20reinforces%20attention%20to%20critical%20image%20regions%20based%20on%20the%20aligned%20image-text%20patterns.%20Extensive%20experiments%20on%20four%20public%20benchmark%20datasets%20demonstrate%20that%20TMCA%20enabled%20superior%20performance%20over%20state-of-the-art%20language-guided%20medical%20image%20segmentation%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2412.13533v3&entry.124074799=Read"},
{"title": "SAMTok: Representing Any Mask with Two Words", "author": "Yikang Zhou and Tao Zhang and Dengxian Gong and Yuanzheng Wu and Ye Tian and Haochen Wang and Haobo Yuan and Jiacong Wang and Lu Qi and Hao Fei and Anran Wang and Zhuochen Wang and Yujing Wang and Cheng Chen and Shunping Ji and Xiangtai Li", "abstract": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.", "link": "http://arxiv.org/abs/2601.16093v1", "date": "2026-01-22", "relevancy": 2.6766, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5303}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMTok%3A%20Representing%20Any%20Mask%20with%20Two%20Words&body=Title%3A%20SAMTok%3A%20Representing%20Any%20Mask%20with%20Two%20Words%0AAuthor%3A%20Yikang%20Zhou%20and%20Tao%20Zhang%20and%20Dengxian%20Gong%20and%20Yuanzheng%20Wu%20and%20Ye%20Tian%20and%20Haochen%20Wang%20and%20Haobo%20Yuan%20and%20Jiacong%20Wang%20and%20Lu%20Qi%20and%20Hao%20Fei%20and%20Anran%20Wang%20and%20Zhuochen%20Wang%20and%20Yujing%20Wang%20and%20Cheng%20Chen%20and%20Shunping%20Ji%20and%20Xiangtai%20Li%0AAbstract%3A%20Pixel-wise%20capabilities%20are%20essential%20for%20building%20interactive%20intelligent%20systems.%20However%2C%20pixel-wise%20multi-modal%20LLMs%20%28MLLMs%29%20remain%20difficult%20to%20scale%20due%20to%20complex%20region-level%20encoders%2C%20specialized%20segmentation%20decoders%2C%20and%20incompatible%20training%20objectives.%20To%20address%20these%20challenges%2C%20we%20present%20SAMTok%2C%20a%20discrete%20mask%20tokenizer%20that%20converts%20any%20region%20mask%20into%20two%20special%20tokens%20and%20reconstructs%20the%20mask%20using%20these%20tokens%20with%20high%20fidelity.%20By%20treating%20masks%20as%20new%20language%20tokens%2C%20SAMTok%20enables%20base%20MLLMs%20%28such%20as%20the%20QwenVL%20series%29%20to%20learn%20pixel-wise%20capabilities%20through%20standard%20next-token%20prediction%20and%20simple%20reinforcement%20learning%2C%20without%20architectural%20modifications%20and%20specialized%20loss%20design.%20SAMTok%20builds%20on%20SAM2%20and%20is%20trained%20on%20209M%20diverse%20masks%20using%20a%20mask%20encoder%20and%20residual%20vector%20quantizer%20to%20produce%20discrete%2C%20compact%2C%20and%20information-rich%20tokens.%20With%205M%20SAMTok-formatted%20mask%20understanding%20and%20generation%20data%20samples%2C%20QwenVL-SAMTok%20attains%20state-of-the-art%20or%20comparable%20results%20on%20region%20captioning%2C%20region%20VQA%2C%20grounded%20conversation%2C%20referring%20segmentation%2C%20scene%20graph%20parsing%2C%20and%20multi-round%20interactive%20segmentation.%20We%20further%20introduce%20a%20textual%20answer-matching%20reward%20that%20enables%20efficient%20reinforcement%20learning%20for%20mask%20generation%2C%20delivering%20substantial%20improvements%20on%20GRES%20and%20GCG%20benchmarks.%20Our%20results%20demonstrate%20a%20scalable%20and%20straightforward%20paradigm%20for%20equipping%20MLLMs%20with%20strong%20pixel-wise%20capabilities.%20Our%20code%20and%20models%20are%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMTok%253A%2520Representing%2520Any%2520Mask%2520with%2520Two%2520Words%26entry.906535625%3DYikang%2520Zhou%2520and%2520Tao%2520Zhang%2520and%2520Dengxian%2520Gong%2520and%2520Yuanzheng%2520Wu%2520and%2520Ye%2520Tian%2520and%2520Haochen%2520Wang%2520and%2520Haobo%2520Yuan%2520and%2520Jiacong%2520Wang%2520and%2520Lu%2520Qi%2520and%2520Hao%2520Fei%2520and%2520Anran%2520Wang%2520and%2520Zhuochen%2520Wang%2520and%2520Yujing%2520Wang%2520and%2520Cheng%2520Chen%2520and%2520Shunping%2520Ji%2520and%2520Xiangtai%2520Li%26entry.1292438233%3DPixel-wise%2520capabilities%2520are%2520essential%2520for%2520building%2520interactive%2520intelligent%2520systems.%2520However%252C%2520pixel-wise%2520multi-modal%2520LLMs%2520%2528MLLMs%2529%2520remain%2520difficult%2520to%2520scale%2520due%2520to%2520complex%2520region-level%2520encoders%252C%2520specialized%2520segmentation%2520decoders%252C%2520and%2520incompatible%2520training%2520objectives.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520SAMTok%252C%2520a%2520discrete%2520mask%2520tokenizer%2520that%2520converts%2520any%2520region%2520mask%2520into%2520two%2520special%2520tokens%2520and%2520reconstructs%2520the%2520mask%2520using%2520these%2520tokens%2520with%2520high%2520fidelity.%2520By%2520treating%2520masks%2520as%2520new%2520language%2520tokens%252C%2520SAMTok%2520enables%2520base%2520MLLMs%2520%2528such%2520as%2520the%2520QwenVL%2520series%2529%2520to%2520learn%2520pixel-wise%2520capabilities%2520through%2520standard%2520next-token%2520prediction%2520and%2520simple%2520reinforcement%2520learning%252C%2520without%2520architectural%2520modifications%2520and%2520specialized%2520loss%2520design.%2520SAMTok%2520builds%2520on%2520SAM2%2520and%2520is%2520trained%2520on%2520209M%2520diverse%2520masks%2520using%2520a%2520mask%2520encoder%2520and%2520residual%2520vector%2520quantizer%2520to%2520produce%2520discrete%252C%2520compact%252C%2520and%2520information-rich%2520tokens.%2520With%25205M%2520SAMTok-formatted%2520mask%2520understanding%2520and%2520generation%2520data%2520samples%252C%2520QwenVL-SAMTok%2520attains%2520state-of-the-art%2520or%2520comparable%2520results%2520on%2520region%2520captioning%252C%2520region%2520VQA%252C%2520grounded%2520conversation%252C%2520referring%2520segmentation%252C%2520scene%2520graph%2520parsing%252C%2520and%2520multi-round%2520interactive%2520segmentation.%2520We%2520further%2520introduce%2520a%2520textual%2520answer-matching%2520reward%2520that%2520enables%2520efficient%2520reinforcement%2520learning%2520for%2520mask%2520generation%252C%2520delivering%2520substantial%2520improvements%2520on%2520GRES%2520and%2520GCG%2520benchmarks.%2520Our%2520results%2520demonstrate%2520a%2520scalable%2520and%2520straightforward%2520paradigm%2520for%2520equipping%2520MLLMs%2520with%2520strong%2520pixel-wise%2520capabilities.%2520Our%2520code%2520and%2520models%2520are%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMTok%3A%20Representing%20Any%20Mask%20with%20Two%20Words&entry.906535625=Yikang%20Zhou%20and%20Tao%20Zhang%20and%20Dengxian%20Gong%20and%20Yuanzheng%20Wu%20and%20Ye%20Tian%20and%20Haochen%20Wang%20and%20Haobo%20Yuan%20and%20Jiacong%20Wang%20and%20Lu%20Qi%20and%20Hao%20Fei%20and%20Anran%20Wang%20and%20Zhuochen%20Wang%20and%20Yujing%20Wang%20and%20Cheng%20Chen%20and%20Shunping%20Ji%20and%20Xiangtai%20Li&entry.1292438233=Pixel-wise%20capabilities%20are%20essential%20for%20building%20interactive%20intelligent%20systems.%20However%2C%20pixel-wise%20multi-modal%20LLMs%20%28MLLMs%29%20remain%20difficult%20to%20scale%20due%20to%20complex%20region-level%20encoders%2C%20specialized%20segmentation%20decoders%2C%20and%20incompatible%20training%20objectives.%20To%20address%20these%20challenges%2C%20we%20present%20SAMTok%2C%20a%20discrete%20mask%20tokenizer%20that%20converts%20any%20region%20mask%20into%20two%20special%20tokens%20and%20reconstructs%20the%20mask%20using%20these%20tokens%20with%20high%20fidelity.%20By%20treating%20masks%20as%20new%20language%20tokens%2C%20SAMTok%20enables%20base%20MLLMs%20%28such%20as%20the%20QwenVL%20series%29%20to%20learn%20pixel-wise%20capabilities%20through%20standard%20next-token%20prediction%20and%20simple%20reinforcement%20learning%2C%20without%20architectural%20modifications%20and%20specialized%20loss%20design.%20SAMTok%20builds%20on%20SAM2%20and%20is%20trained%20on%20209M%20diverse%20masks%20using%20a%20mask%20encoder%20and%20residual%20vector%20quantizer%20to%20produce%20discrete%2C%20compact%2C%20and%20information-rich%20tokens.%20With%205M%20SAMTok-formatted%20mask%20understanding%20and%20generation%20data%20samples%2C%20QwenVL-SAMTok%20attains%20state-of-the-art%20or%20comparable%20results%20on%20region%20captioning%2C%20region%20VQA%2C%20grounded%20conversation%2C%20referring%20segmentation%2C%20scene%20graph%20parsing%2C%20and%20multi-round%20interactive%20segmentation.%20We%20further%20introduce%20a%20textual%20answer-matching%20reward%20that%20enables%20efficient%20reinforcement%20learning%20for%20mask%20generation%2C%20delivering%20substantial%20improvements%20on%20GRES%20and%20GCG%20benchmarks.%20Our%20results%20demonstrate%20a%20scalable%20and%20straightforward%20paradigm%20for%20equipping%20MLLMs%20with%20strong%20pixel-wise%20capabilities.%20Our%20code%20and%20models%20are%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.16093v1&entry.124074799=Read"},
{"title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning", "author": "Yifan Wang and Shiyu Li and Peiming Li and Xiaochen Yang and Yang Tang and Zheng Wei", "abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT", "link": "http://arxiv.org/abs/2601.14750v2", "date": "2026-01-22", "relevancy": 2.6658, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Render-of-Thought%3A%20Rendering%20Textual%20Chain-of-Thought%20as%20Images%20for%20Visual%20Latent%20Reasoning&body=Title%3A%20Render-of-Thought%3A%20Rendering%20Textual%20Chain-of-Thought%20as%20Images%20for%20Visual%20Latent%20Reasoning%0AAuthor%3A%20Yifan%20Wang%20and%20Shiyu%20Li%20and%20Peiming%20Li%20and%20Xiaochen%20Yang%20and%20Yang%20Tang%20and%20Zheng%20Wei%0AAbstract%3A%20Chain-of-Thought%20%28CoT%29%20prompting%20has%20achieved%20remarkable%20success%20in%20unlocking%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20Although%20CoT%20prompting%20enhances%20reasoning%2C%20its%20verbosity%20imposes%20substantial%20computational%20overhead.%20Recent%20works%20often%20focus%20exclusively%20on%20outcome%20alignment%20and%20lack%20supervision%20on%20the%20intermediate%20reasoning%20process.%20These%20deficiencies%20obscure%20the%20analyzability%20of%20the%20latent%20reasoning%20chain.%20To%20address%20these%20challenges%2C%20we%20introduce%20Render-of-Thought%20%28RoT%29%2C%20the%20first%20framework%20to%20reify%20the%20reasoning%20chain%20by%20rendering%20textual%20steps%20into%20images%2C%20making%20the%20latent%20rationale%20explicit%20and%20traceable.%20Specifically%2C%20we%20leverage%20the%20vision%20encoders%20of%20existing%20Vision%20Language%20Models%20%28VLMs%29%20as%20semantic%20anchors%20to%20align%20the%20vision%20embeddings%20with%20the%20textual%20space.%20This%20design%20ensures%20plug-and-play%20implementation%20without%20incurring%20additional%20pre-training%20overhead.%20Extensive%20experiments%20on%20mathematical%20and%20logical%20reasoning%20benchmarks%20demonstrate%20that%20our%20method%20achieves%203-4x%20token%20compression%20and%20substantial%20inference%20acceleration%20compared%20to%20explicit%20CoT.%20Furthermore%2C%20it%20maintains%20competitive%20performance%20against%20other%20methods%2C%20validating%20the%20feasibility%20of%20this%20paradigm.%20Our%20code%20is%20available%20at%20https%3A//github.com/TencentBAC/RoT%0ALink%3A%20http%3A//arxiv.org/abs/2601.14750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRender-of-Thought%253A%2520Rendering%2520Textual%2520Chain-of-Thought%2520as%2520Images%2520for%2520Visual%2520Latent%2520Reasoning%26entry.906535625%3DYifan%2520Wang%2520and%2520Shiyu%2520Li%2520and%2520Peiming%2520Li%2520and%2520Xiaochen%2520Yang%2520and%2520Yang%2520Tang%2520and%2520Zheng%2520Wei%26entry.1292438233%3DChain-of-Thought%2520%2528CoT%2529%2520prompting%2520has%2520achieved%2520remarkable%2520success%2520in%2520unlocking%2520the%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Although%2520CoT%2520prompting%2520enhances%2520reasoning%252C%2520its%2520verbosity%2520imposes%2520substantial%2520computational%2520overhead.%2520Recent%2520works%2520often%2520focus%2520exclusively%2520on%2520outcome%2520alignment%2520and%2520lack%2520supervision%2520on%2520the%2520intermediate%2520reasoning%2520process.%2520These%2520deficiencies%2520obscure%2520the%2520analyzability%2520of%2520the%2520latent%2520reasoning%2520chain.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Render-of-Thought%2520%2528RoT%2529%252C%2520the%2520first%2520framework%2520to%2520reify%2520the%2520reasoning%2520chain%2520by%2520rendering%2520textual%2520steps%2520into%2520images%252C%2520making%2520the%2520latent%2520rationale%2520explicit%2520and%2520traceable.%2520Specifically%252C%2520we%2520leverage%2520the%2520vision%2520encoders%2520of%2520existing%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520as%2520semantic%2520anchors%2520to%2520align%2520the%2520vision%2520embeddings%2520with%2520the%2520textual%2520space.%2520This%2520design%2520ensures%2520plug-and-play%2520implementation%2520without%2520incurring%2520additional%2520pre-training%2520overhead.%2520Extensive%2520experiments%2520on%2520mathematical%2520and%2520logical%2520reasoning%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%25203-4x%2520token%2520compression%2520and%2520substantial%2520inference%2520acceleration%2520compared%2520to%2520explicit%2520CoT.%2520Furthermore%252C%2520it%2520maintains%2520competitive%2520performance%2520against%2520other%2520methods%252C%2520validating%2520the%2520feasibility%2520of%2520this%2520paradigm.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/TencentBAC/RoT%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Render-of-Thought%3A%20Rendering%20Textual%20Chain-of-Thought%20as%20Images%20for%20Visual%20Latent%20Reasoning&entry.906535625=Yifan%20Wang%20and%20Shiyu%20Li%20and%20Peiming%20Li%20and%20Xiaochen%20Yang%20and%20Yang%20Tang%20and%20Zheng%20Wei&entry.1292438233=Chain-of-Thought%20%28CoT%29%20prompting%20has%20achieved%20remarkable%20success%20in%20unlocking%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20Although%20CoT%20prompting%20enhances%20reasoning%2C%20its%20verbosity%20imposes%20substantial%20computational%20overhead.%20Recent%20works%20often%20focus%20exclusively%20on%20outcome%20alignment%20and%20lack%20supervision%20on%20the%20intermediate%20reasoning%20process.%20These%20deficiencies%20obscure%20the%20analyzability%20of%20the%20latent%20reasoning%20chain.%20To%20address%20these%20challenges%2C%20we%20introduce%20Render-of-Thought%20%28RoT%29%2C%20the%20first%20framework%20to%20reify%20the%20reasoning%20chain%20by%20rendering%20textual%20steps%20into%20images%2C%20making%20the%20latent%20rationale%20explicit%20and%20traceable.%20Specifically%2C%20we%20leverage%20the%20vision%20encoders%20of%20existing%20Vision%20Language%20Models%20%28VLMs%29%20as%20semantic%20anchors%20to%20align%20the%20vision%20embeddings%20with%20the%20textual%20space.%20This%20design%20ensures%20plug-and-play%20implementation%20without%20incurring%20additional%20pre-training%20overhead.%20Extensive%20experiments%20on%20mathematical%20and%20logical%20reasoning%20benchmarks%20demonstrate%20that%20our%20method%20achieves%203-4x%20token%20compression%20and%20substantial%20inference%20acceleration%20compared%20to%20explicit%20CoT.%20Furthermore%2C%20it%20maintains%20competitive%20performance%20against%20other%20methods%2C%20validating%20the%20feasibility%20of%20this%20paradigm.%20Our%20code%20is%20available%20at%20https%3A//github.com/TencentBAC/RoT&entry.1838667208=http%3A//arxiv.org/abs/2601.14750v2&entry.124074799=Read"},
{"title": "A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies", "author": "Jingsong Xia and Siqi Wang", "abstract": "Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.", "link": "http://arxiv.org/abs/2601.15865v1", "date": "2026-01-22", "relevancy": 2.6605, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5434}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Brain-Inspired%20Machine%20Learning%20Framework%20for%20Coronary%20Angiography%3A%20Hybrid%20Neural%20Representation%20and%20Robust%20Learning%20Strategies&body=Title%3A%20A%20Lightweight%20Brain-Inspired%20Machine%20Learning%20Framework%20for%20Coronary%20Angiography%3A%20Hybrid%20Neural%20Representation%20and%20Robust%20Learning%20Strategies%0AAuthor%3A%20Jingsong%20Xia%20and%20Siqi%20Wang%0AAbstract%3A%20Background%3A%20Coronary%20angiography%20%28CAG%29%20is%20a%20cornerstone%20imaging%20modality%20for%20assessing%20coronary%20artery%20disease%20and%20guiding%20interventional%20treatment%20decisions.%20However%2C%20in%20real-world%20clinical%20settings%2C%20angiographic%20images%20are%20often%20characterized%20by%20complex%20lesion%20morphology%2C%20severe%20class%20imbalance%2C%20label%20uncertainty%2C%20and%20limited%20computational%20resources%2C%20posing%20substantial%20challenges%20to%20conventional%20deep%20learning%20approaches%20in%20terms%20of%20robustness%20and%20generalization.Methods%3A%20The%20proposed%20framework%20is%20built%20upon%20a%20pretrained%20convolutional%20neural%20network%20to%20construct%20a%20lightweight%20hybrid%20neural%20representation.%20A%20selective%20neural%20plasticity%20training%20strategy%20is%20introduced%20to%20enable%20efficient%20parameter%20adaptation.%20Furthermore%2C%20a%20brain-inspired%20attention-modulated%20loss%20function%2C%20combining%20Focal%20Loss%20with%20label%20smoothing%2C%20is%20employed%20to%20enhance%20sensitivity%20to%20hard%20samples%20and%20uncertain%20annotations.%20Class-imbalance-aware%20sampling%20and%20cosine%20annealing%20with%20warm%20restarts%20are%20adopted%20to%20mimic%20rhythmic%20regulation%20and%20attention%20allocation%20mechanisms%20observed%20in%20biological%20neural%20systems.Results%3A%20Experimental%20results%20demonstrate%20that%20the%20proposed%20lightweight%20brain-inspired%20model%20achieves%20strong%20and%20stable%20performance%20in%20binary%20coronary%20angiography%20classification%2C%20yielding%20competitive%20accuracy%2C%20recall%2C%20F1-score%2C%20and%20AUC%20metrics%20while%20maintaining%20high%20computational%20efficiency.Conclusion%3A%20This%20study%20validates%20the%20effectiveness%20of%20brain-inspired%20learning%20mechanisms%20in%20lightweight%20medical%20image%20analysis%20and%20provides%20a%20biologically%20plausible%20and%20deployable%20solution%20for%20intelligent%20clinical%20decision%20support%20under%20limited%20computational%20resources.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Brain-Inspired%2520Machine%2520Learning%2520Framework%2520for%2520Coronary%2520Angiography%253A%2520Hybrid%2520Neural%2520Representation%2520and%2520Robust%2520Learning%2520Strategies%26entry.906535625%3DJingsong%2520Xia%2520and%2520Siqi%2520Wang%26entry.1292438233%3DBackground%253A%2520Coronary%2520angiography%2520%2528CAG%2529%2520is%2520a%2520cornerstone%2520imaging%2520modality%2520for%2520assessing%2520coronary%2520artery%2520disease%2520and%2520guiding%2520interventional%2520treatment%2520decisions.%2520However%252C%2520in%2520real-world%2520clinical%2520settings%252C%2520angiographic%2520images%2520are%2520often%2520characterized%2520by%2520complex%2520lesion%2520morphology%252C%2520severe%2520class%2520imbalance%252C%2520label%2520uncertainty%252C%2520and%2520limited%2520computational%2520resources%252C%2520posing%2520substantial%2520challenges%2520to%2520conventional%2520deep%2520learning%2520approaches%2520in%2520terms%2520of%2520robustness%2520and%2520generalization.Methods%253A%2520The%2520proposed%2520framework%2520is%2520built%2520upon%2520a%2520pretrained%2520convolutional%2520neural%2520network%2520to%2520construct%2520a%2520lightweight%2520hybrid%2520neural%2520representation.%2520A%2520selective%2520neural%2520plasticity%2520training%2520strategy%2520is%2520introduced%2520to%2520enable%2520efficient%2520parameter%2520adaptation.%2520Furthermore%252C%2520a%2520brain-inspired%2520attention-modulated%2520loss%2520function%252C%2520combining%2520Focal%2520Loss%2520with%2520label%2520smoothing%252C%2520is%2520employed%2520to%2520enhance%2520sensitivity%2520to%2520hard%2520samples%2520and%2520uncertain%2520annotations.%2520Class-imbalance-aware%2520sampling%2520and%2520cosine%2520annealing%2520with%2520warm%2520restarts%2520are%2520adopted%2520to%2520mimic%2520rhythmic%2520regulation%2520and%2520attention%2520allocation%2520mechanisms%2520observed%2520in%2520biological%2520neural%2520systems.Results%253A%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520lightweight%2520brain-inspired%2520model%2520achieves%2520strong%2520and%2520stable%2520performance%2520in%2520binary%2520coronary%2520angiography%2520classification%252C%2520yielding%2520competitive%2520accuracy%252C%2520recall%252C%2520F1-score%252C%2520and%2520AUC%2520metrics%2520while%2520maintaining%2520high%2520computational%2520efficiency.Conclusion%253A%2520This%2520study%2520validates%2520the%2520effectiveness%2520of%2520brain-inspired%2520learning%2520mechanisms%2520in%2520lightweight%2520medical%2520image%2520analysis%2520and%2520provides%2520a%2520biologically%2520plausible%2520and%2520deployable%2520solution%2520for%2520intelligent%2520clinical%2520decision%2520support%2520under%2520limited%2520computational%2520resources.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Brain-Inspired%20Machine%20Learning%20Framework%20for%20Coronary%20Angiography%3A%20Hybrid%20Neural%20Representation%20and%20Robust%20Learning%20Strategies&entry.906535625=Jingsong%20Xia%20and%20Siqi%20Wang&entry.1292438233=Background%3A%20Coronary%20angiography%20%28CAG%29%20is%20a%20cornerstone%20imaging%20modality%20for%20assessing%20coronary%20artery%20disease%20and%20guiding%20interventional%20treatment%20decisions.%20However%2C%20in%20real-world%20clinical%20settings%2C%20angiographic%20images%20are%20often%20characterized%20by%20complex%20lesion%20morphology%2C%20severe%20class%20imbalance%2C%20label%20uncertainty%2C%20and%20limited%20computational%20resources%2C%20posing%20substantial%20challenges%20to%20conventional%20deep%20learning%20approaches%20in%20terms%20of%20robustness%20and%20generalization.Methods%3A%20The%20proposed%20framework%20is%20built%20upon%20a%20pretrained%20convolutional%20neural%20network%20to%20construct%20a%20lightweight%20hybrid%20neural%20representation.%20A%20selective%20neural%20plasticity%20training%20strategy%20is%20introduced%20to%20enable%20efficient%20parameter%20adaptation.%20Furthermore%2C%20a%20brain-inspired%20attention-modulated%20loss%20function%2C%20combining%20Focal%20Loss%20with%20label%20smoothing%2C%20is%20employed%20to%20enhance%20sensitivity%20to%20hard%20samples%20and%20uncertain%20annotations.%20Class-imbalance-aware%20sampling%20and%20cosine%20annealing%20with%20warm%20restarts%20are%20adopted%20to%20mimic%20rhythmic%20regulation%20and%20attention%20allocation%20mechanisms%20observed%20in%20biological%20neural%20systems.Results%3A%20Experimental%20results%20demonstrate%20that%20the%20proposed%20lightweight%20brain-inspired%20model%20achieves%20strong%20and%20stable%20performance%20in%20binary%20coronary%20angiography%20classification%2C%20yielding%20competitive%20accuracy%2C%20recall%2C%20F1-score%2C%20and%20AUC%20metrics%20while%20maintaining%20high%20computational%20efficiency.Conclusion%3A%20This%20study%20validates%20the%20effectiveness%20of%20brain-inspired%20learning%20mechanisms%20in%20lightweight%20medical%20image%20analysis%20and%20provides%20a%20biologically%20plausible%20and%20deployable%20solution%20for%20intelligent%20clinical%20decision%20support%20under%20limited%20computational%20resources.&entry.1838667208=http%3A//arxiv.org/abs/2601.15865v1&entry.124074799=Read"},
{"title": "GutenOCR: A Grounded Vision-Language Front-End for Documents", "author": "Hunter Heidenreich and Ben Elliott and Olivia Dinica and Yosheb Getachew", "abstract": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.", "link": "http://arxiv.org/abs/2601.14490v2", "date": "2026-01-22", "relevancy": 2.6411, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GutenOCR%3A%20A%20Grounded%20Vision-Language%20Front-End%20for%20Documents&body=Title%3A%20GutenOCR%3A%20A%20Grounded%20Vision-Language%20Front-End%20for%20Documents%0AAuthor%3A%20Hunter%20Heidenreich%20and%20Ben%20Elliott%20and%20Olivia%20Dinica%20and%20Yosheb%20Getachew%0AAbstract%3A%20GutenOCR%20is%20a%20family%20of%20grounded%20OCR%20front-ends%20obtained%20by%20fine-tuning%20Qwen2.5-VL-3B%20and%20Qwen2.5-VL-7B.%20The%20resulting%20single-checkpoint%20vision-language%20models%20expose%20reading%2C%20detection%2C%20and%20grounding%20through%20a%20unified%2C%20prompt-based%20interface.%20Trained%20on%20business%20documents%2C%20scientific%20articles%2C%20and%20synthetic%20grounding%20data%2C%20the%20models%20support%20full-page%20and%20localized%20reading%20with%20line-%20and%20paragraph-level%20bounding%20boxes%20and%20conditional%20%60%60where%20is%20x%3F%27%27%20queries.%20We%20introduce%20a%20grounded%20OCR%20evaluation%20protocol%20and%20show%20that%20GutenOCR-7B%20more%20than%20doubles%20the%20composite%20grounded%20OCR%20score%20of%20its%20Qwen2.5-VL-7B%20backbone%20on%2010.5K%20held-out%20business%20and%20scientific%20pages%20%280.40%20to%200.82%29.%20On%20Fox%20and%20OmniDocBench%20v1.5%2C%20our%20approach%20substantially%20improves%20region-%20and%20line-level%20OCR%20as%20well%20as%20text-detection%20recall%2C%20but%20reveals%20trade-offs%20in%20page-level%20linearization%2C%20color-guided%20OCR%2C%20and%20formula-heavy%20layouts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14490v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGutenOCR%253A%2520A%2520Grounded%2520Vision-Language%2520Front-End%2520for%2520Documents%26entry.906535625%3DHunter%2520Heidenreich%2520and%2520Ben%2520Elliott%2520and%2520Olivia%2520Dinica%2520and%2520Yosheb%2520Getachew%26entry.1292438233%3DGutenOCR%2520is%2520a%2520family%2520of%2520grounded%2520OCR%2520front-ends%2520obtained%2520by%2520fine-tuning%2520Qwen2.5-VL-3B%2520and%2520Qwen2.5-VL-7B.%2520The%2520resulting%2520single-checkpoint%2520vision-language%2520models%2520expose%2520reading%252C%2520detection%252C%2520and%2520grounding%2520through%2520a%2520unified%252C%2520prompt-based%2520interface.%2520Trained%2520on%2520business%2520documents%252C%2520scientific%2520articles%252C%2520and%2520synthetic%2520grounding%2520data%252C%2520the%2520models%2520support%2520full-page%2520and%2520localized%2520reading%2520with%2520line-%2520and%2520paragraph-level%2520bounding%2520boxes%2520and%2520conditional%2520%2560%2560where%2520is%2520x%253F%2527%2527%2520queries.%2520We%2520introduce%2520a%2520grounded%2520OCR%2520evaluation%2520protocol%2520and%2520show%2520that%2520GutenOCR-7B%2520more%2520than%2520doubles%2520the%2520composite%2520grounded%2520OCR%2520score%2520of%2520its%2520Qwen2.5-VL-7B%2520backbone%2520on%252010.5K%2520held-out%2520business%2520and%2520scientific%2520pages%2520%25280.40%2520to%25200.82%2529.%2520On%2520Fox%2520and%2520OmniDocBench%2520v1.5%252C%2520our%2520approach%2520substantially%2520improves%2520region-%2520and%2520line-level%2520OCR%2520as%2520well%2520as%2520text-detection%2520recall%252C%2520but%2520reveals%2520trade-offs%2520in%2520page-level%2520linearization%252C%2520color-guided%2520OCR%252C%2520and%2520formula-heavy%2520layouts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14490v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GutenOCR%3A%20A%20Grounded%20Vision-Language%20Front-End%20for%20Documents&entry.906535625=Hunter%20Heidenreich%20and%20Ben%20Elliott%20and%20Olivia%20Dinica%20and%20Yosheb%20Getachew&entry.1292438233=GutenOCR%20is%20a%20family%20of%20grounded%20OCR%20front-ends%20obtained%20by%20fine-tuning%20Qwen2.5-VL-3B%20and%20Qwen2.5-VL-7B.%20The%20resulting%20single-checkpoint%20vision-language%20models%20expose%20reading%2C%20detection%2C%20and%20grounding%20through%20a%20unified%2C%20prompt-based%20interface.%20Trained%20on%20business%20documents%2C%20scientific%20articles%2C%20and%20synthetic%20grounding%20data%2C%20the%20models%20support%20full-page%20and%20localized%20reading%20with%20line-%20and%20paragraph-level%20bounding%20boxes%20and%20conditional%20%60%60where%20is%20x%3F%27%27%20queries.%20We%20introduce%20a%20grounded%20OCR%20evaluation%20protocol%20and%20show%20that%20GutenOCR-7B%20more%20than%20doubles%20the%20composite%20grounded%20OCR%20score%20of%20its%20Qwen2.5-VL-7B%20backbone%20on%2010.5K%20held-out%20business%20and%20scientific%20pages%20%280.40%20to%200.82%29.%20On%20Fox%20and%20OmniDocBench%20v1.5%2C%20our%20approach%20substantially%20improves%20region-%20and%20line-level%20OCR%20as%20well%20as%20text-detection%20recall%2C%20but%20reveals%20trade-offs%20in%20page-level%20linearization%2C%20color-guided%20OCR%2C%20and%20formula-heavy%20layouts.&entry.1838667208=http%3A//arxiv.org/abs/2601.14490v2&entry.124074799=Read"},
{"title": "Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures", "author": "Sucheta Ghosh and Zahra Monfared and Felix Dietrich", "abstract": "We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.", "link": "http://arxiv.org/abs/2601.08549v2", "date": "2026-01-22", "relevancy": 2.6229, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20and%20Multi-Task%20Learning%20on%20Noisy%20Brain%20Signals%20with%20Nonlinear%20Dynamical%20Signatures&body=Title%3A%20Contrastive%20and%20Multi-Task%20Learning%20on%20Noisy%20Brain%20Signals%20with%20Nonlinear%20Dynamical%20Signatures%0AAuthor%3A%20Sucheta%20Ghosh%20and%20Zahra%20Monfared%20and%20Felix%20Dietrich%0AAbstract%3A%20We%20introduce%20a%20two-stage%20multitask%20learning%20framework%20for%20analyzing%20Electroencephalography%20%28EEG%29%20signals%20that%20integrates%20denoising%2C%20dynamical%20modeling%2C%20and%20representation%20learning.%20In%20the%20first%20stage%2C%20a%20denoising%20autoencoder%20is%20trained%20to%20suppress%20artifacts%20and%20stabilize%20temporal%20dynamics%2C%20providing%20robust%20signal%20representations.%20In%20the%20second%20stage%2C%20a%20multitask%20architecture%20processes%20these%20denoised%20signals%20to%20achieve%20three%20objectives%3A%20motor%20imagery%20classification%2C%20chaotic%20versus%20non-chaotic%20regime%20discrimination%20using%20Lyapunov%20exponent-based%20labels%2C%20and%20self-supervised%20contrastive%20representation%20learning%20with%20NT-Xent%20loss.%20A%20convolutional%20backbone%20combined%20with%20a%20Transformer%20encoder%20captures%20spatial-temporal%20structure%2C%20while%20the%20dynamical%20task%20encourages%20sensitivity%20to%20nonlinear%20brain%20dynamics.%20This%20staged%20design%20mitigates%20interference%20between%20reconstruction%20and%20discriminative%20goals%2C%20improves%20stability%20across%20datasets%2C%20and%20supports%20reproducible%20training%20by%20clearly%20separating%20noise%20reduction%20from%20higher-level%20feature%20learning.%20Empirical%20studies%20show%20that%20our%20framework%20not%20only%20enhances%20robustness%20and%20generalization%20but%20also%20surpasses%20strong%20baselines%20and%20recent%20state-of-the-art%20methods%20in%20EEG%20decoding%2C%20highlighting%20the%20effectiveness%20of%20combining%20denoising%2C%20dynamical%20features%2C%20and%20self-supervised%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520and%2520Multi-Task%2520Learning%2520on%2520Noisy%2520Brain%2520Signals%2520with%2520Nonlinear%2520Dynamical%2520Signatures%26entry.906535625%3DSucheta%2520Ghosh%2520and%2520Zahra%2520Monfared%2520and%2520Felix%2520Dietrich%26entry.1292438233%3DWe%2520introduce%2520a%2520two-stage%2520multitask%2520learning%2520framework%2520for%2520analyzing%2520Electroencephalography%2520%2528EEG%2529%2520signals%2520that%2520integrates%2520denoising%252C%2520dynamical%2520modeling%252C%2520and%2520representation%2520learning.%2520In%2520the%2520first%2520stage%252C%2520a%2520denoising%2520autoencoder%2520is%2520trained%2520to%2520suppress%2520artifacts%2520and%2520stabilize%2520temporal%2520dynamics%252C%2520providing%2520robust%2520signal%2520representations.%2520In%2520the%2520second%2520stage%252C%2520a%2520multitask%2520architecture%2520processes%2520these%2520denoised%2520signals%2520to%2520achieve%2520three%2520objectives%253A%2520motor%2520imagery%2520classification%252C%2520chaotic%2520versus%2520non-chaotic%2520regime%2520discrimination%2520using%2520Lyapunov%2520exponent-based%2520labels%252C%2520and%2520self-supervised%2520contrastive%2520representation%2520learning%2520with%2520NT-Xent%2520loss.%2520A%2520convolutional%2520backbone%2520combined%2520with%2520a%2520Transformer%2520encoder%2520captures%2520spatial-temporal%2520structure%252C%2520while%2520the%2520dynamical%2520task%2520encourages%2520sensitivity%2520to%2520nonlinear%2520brain%2520dynamics.%2520This%2520staged%2520design%2520mitigates%2520interference%2520between%2520reconstruction%2520and%2520discriminative%2520goals%252C%2520improves%2520stability%2520across%2520datasets%252C%2520and%2520supports%2520reproducible%2520training%2520by%2520clearly%2520separating%2520noise%2520reduction%2520from%2520higher-level%2520feature%2520learning.%2520Empirical%2520studies%2520show%2520that%2520our%2520framework%2520not%2520only%2520enhances%2520robustness%2520and%2520generalization%2520but%2520also%2520surpasses%2520strong%2520baselines%2520and%2520recent%2520state-of-the-art%2520methods%2520in%2520EEG%2520decoding%252C%2520highlighting%2520the%2520effectiveness%2520of%2520combining%2520denoising%252C%2520dynamical%2520features%252C%2520and%2520self-supervised%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20and%20Multi-Task%20Learning%20on%20Noisy%20Brain%20Signals%20with%20Nonlinear%20Dynamical%20Signatures&entry.906535625=Sucheta%20Ghosh%20and%20Zahra%20Monfared%20and%20Felix%20Dietrich&entry.1292438233=We%20introduce%20a%20two-stage%20multitask%20learning%20framework%20for%20analyzing%20Electroencephalography%20%28EEG%29%20signals%20that%20integrates%20denoising%2C%20dynamical%20modeling%2C%20and%20representation%20learning.%20In%20the%20first%20stage%2C%20a%20denoising%20autoencoder%20is%20trained%20to%20suppress%20artifacts%20and%20stabilize%20temporal%20dynamics%2C%20providing%20robust%20signal%20representations.%20In%20the%20second%20stage%2C%20a%20multitask%20architecture%20processes%20these%20denoised%20signals%20to%20achieve%20three%20objectives%3A%20motor%20imagery%20classification%2C%20chaotic%20versus%20non-chaotic%20regime%20discrimination%20using%20Lyapunov%20exponent-based%20labels%2C%20and%20self-supervised%20contrastive%20representation%20learning%20with%20NT-Xent%20loss.%20A%20convolutional%20backbone%20combined%20with%20a%20Transformer%20encoder%20captures%20spatial-temporal%20structure%2C%20while%20the%20dynamical%20task%20encourages%20sensitivity%20to%20nonlinear%20brain%20dynamics.%20This%20staged%20design%20mitigates%20interference%20between%20reconstruction%20and%20discriminative%20goals%2C%20improves%20stability%20across%20datasets%2C%20and%20supports%20reproducible%20training%20by%20clearly%20separating%20noise%20reduction%20from%20higher-level%20feature%20learning.%20Empirical%20studies%20show%20that%20our%20framework%20not%20only%20enhances%20robustness%20and%20generalization%20but%20also%20surpasses%20strong%20baselines%20and%20recent%20state-of-the-art%20methods%20in%20EEG%20decoding%2C%20highlighting%20the%20effectiveness%20of%20combining%20denoising%2C%20dynamical%20features%2C%20and%20self-supervised%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2601.08549v2&entry.124074799=Read"},
{"title": "Class Confidence Aware Reweighting for Long Tailed Learning", "author": "Brainard Philemon Jagati and Jitendra Tembhurne and Harsh Goud and Rudra Pratap Singh and Chandrashekhar Meshram", "abstract": "Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an \u03a9(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.", "link": "http://arxiv.org/abs/2601.15924v1", "date": "2026-01-22", "relevancy": 2.619, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5977}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4952}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class%20Confidence%20Aware%20Reweighting%20for%20Long%20Tailed%20Learning&body=Title%3A%20Class%20Confidence%20Aware%20Reweighting%20for%20Long%20Tailed%20Learning%0AAuthor%3A%20Brainard%20Philemon%20Jagati%20and%20Jitendra%20Tembhurne%20and%20Harsh%20Goud%20and%20Rudra%20Pratap%20Singh%20and%20Chandrashekhar%20Meshram%0AAbstract%3A%20Deep%20neural%20network%20models%20degrade%20significantly%20in%20the%20long-tailed%20data%20distribution%2C%20with%20the%20overall%20training%20data%20dominated%20by%20a%20small%20set%20of%20classes%20in%20the%20head%2C%20and%20the%20tail%20classes%20obtaining%20less%20training%20examples.%20Addressing%20the%20imbalance%20in%20the%20classes%2C%20attention%20in%20the%20related%20literature%20was%20given%20mainly%20to%20the%20adjustments%20carried%20out%20in%20the%20decision%20space%20in%20terms%20of%20either%20corrections%20performed%20at%20the%20logit%20level%20in%20order%20to%20compensate%20class-prior%20bias%2C%20with%20the%20least%20attention%20to%20the%20optimization%20process%20resulting%20from%20the%20adjustments%20introduced%20through%20the%20differences%20in%20the%20confidences%20among%20the%20samples.%20In%20the%20current%20study%2C%20we%20present%20the%20design%20of%20a%20class%20and%20confidence-aware%20re-weighting%20scheme%20for%20long-tailed%20learning.%20This%20scheme%20is%20purely%20based%20upon%20the%20loss%20level%20and%20has%20a%20complementary%20nature%20to%20the%20existing%20methods%20performing%20the%20adjustment%20of%20the%20logits.%20In%20the%20practical%20implementation%20stage%20of%20the%20proposed%20scheme%2C%20we%20use%20an%20%CE%A9%28p_t%2C%20f_c%29%20function.%20This%20function%20enables%20the%20modulation%20of%20the%20contribution%20towards%20the%20training%20task%20based%20upon%20the%20confidence%20value%20of%20the%20prediction%2C%20as%20well%20as%20the%20relative%20frequency%20of%20the%20corresponding%20class.%20Our%20observations%20in%20the%20experiments%20are%20corroborated%20by%20significant%20experimental%20results%20performed%20on%20the%20CIFAR-100-LT%2C%20ImageNet-LT%2C%20and%20iNaturalist2018%20datasets%20under%20various%20values%20of%20imbalance%20factors%20that%20clearly%20authenticate%20the%20theoretical%20discussions%20above.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass%2520Confidence%2520Aware%2520Reweighting%2520for%2520Long%2520Tailed%2520Learning%26entry.906535625%3DBrainard%2520Philemon%2520Jagati%2520and%2520Jitendra%2520Tembhurne%2520and%2520Harsh%2520Goud%2520and%2520Rudra%2520Pratap%2520Singh%2520and%2520Chandrashekhar%2520Meshram%26entry.1292438233%3DDeep%2520neural%2520network%2520models%2520degrade%2520significantly%2520in%2520the%2520long-tailed%2520data%2520distribution%252C%2520with%2520the%2520overall%2520training%2520data%2520dominated%2520by%2520a%2520small%2520set%2520of%2520classes%2520in%2520the%2520head%252C%2520and%2520the%2520tail%2520classes%2520obtaining%2520less%2520training%2520examples.%2520Addressing%2520the%2520imbalance%2520in%2520the%2520classes%252C%2520attention%2520in%2520the%2520related%2520literature%2520was%2520given%2520mainly%2520to%2520the%2520adjustments%2520carried%2520out%2520in%2520the%2520decision%2520space%2520in%2520terms%2520of%2520either%2520corrections%2520performed%2520at%2520the%2520logit%2520level%2520in%2520order%2520to%2520compensate%2520class-prior%2520bias%252C%2520with%2520the%2520least%2520attention%2520to%2520the%2520optimization%2520process%2520resulting%2520from%2520the%2520adjustments%2520introduced%2520through%2520the%2520differences%2520in%2520the%2520confidences%2520among%2520the%2520samples.%2520In%2520the%2520current%2520study%252C%2520we%2520present%2520the%2520design%2520of%2520a%2520class%2520and%2520confidence-aware%2520re-weighting%2520scheme%2520for%2520long-tailed%2520learning.%2520This%2520scheme%2520is%2520purely%2520based%2520upon%2520the%2520loss%2520level%2520and%2520has%2520a%2520complementary%2520nature%2520to%2520the%2520existing%2520methods%2520performing%2520the%2520adjustment%2520of%2520the%2520logits.%2520In%2520the%2520practical%2520implementation%2520stage%2520of%2520the%2520proposed%2520scheme%252C%2520we%2520use%2520an%2520%25CE%25A9%2528p_t%252C%2520f_c%2529%2520function.%2520This%2520function%2520enables%2520the%2520modulation%2520of%2520the%2520contribution%2520towards%2520the%2520training%2520task%2520based%2520upon%2520the%2520confidence%2520value%2520of%2520the%2520prediction%252C%2520as%2520well%2520as%2520the%2520relative%2520frequency%2520of%2520the%2520corresponding%2520class.%2520Our%2520observations%2520in%2520the%2520experiments%2520are%2520corroborated%2520by%2520significant%2520experimental%2520results%2520performed%2520on%2520the%2520CIFAR-100-LT%252C%2520ImageNet-LT%252C%2520and%2520iNaturalist2018%2520datasets%2520under%2520various%2520values%2520of%2520imbalance%2520factors%2520that%2520clearly%2520authenticate%2520the%2520theoretical%2520discussions%2520above.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class%20Confidence%20Aware%20Reweighting%20for%20Long%20Tailed%20Learning&entry.906535625=Brainard%20Philemon%20Jagati%20and%20Jitendra%20Tembhurne%20and%20Harsh%20Goud%20and%20Rudra%20Pratap%20Singh%20and%20Chandrashekhar%20Meshram&entry.1292438233=Deep%20neural%20network%20models%20degrade%20significantly%20in%20the%20long-tailed%20data%20distribution%2C%20with%20the%20overall%20training%20data%20dominated%20by%20a%20small%20set%20of%20classes%20in%20the%20head%2C%20and%20the%20tail%20classes%20obtaining%20less%20training%20examples.%20Addressing%20the%20imbalance%20in%20the%20classes%2C%20attention%20in%20the%20related%20literature%20was%20given%20mainly%20to%20the%20adjustments%20carried%20out%20in%20the%20decision%20space%20in%20terms%20of%20either%20corrections%20performed%20at%20the%20logit%20level%20in%20order%20to%20compensate%20class-prior%20bias%2C%20with%20the%20least%20attention%20to%20the%20optimization%20process%20resulting%20from%20the%20adjustments%20introduced%20through%20the%20differences%20in%20the%20confidences%20among%20the%20samples.%20In%20the%20current%20study%2C%20we%20present%20the%20design%20of%20a%20class%20and%20confidence-aware%20re-weighting%20scheme%20for%20long-tailed%20learning.%20This%20scheme%20is%20purely%20based%20upon%20the%20loss%20level%20and%20has%20a%20complementary%20nature%20to%20the%20existing%20methods%20performing%20the%20adjustment%20of%20the%20logits.%20In%20the%20practical%20implementation%20stage%20of%20the%20proposed%20scheme%2C%20we%20use%20an%20%CE%A9%28p_t%2C%20f_c%29%20function.%20This%20function%20enables%20the%20modulation%20of%20the%20contribution%20towards%20the%20training%20task%20based%20upon%20the%20confidence%20value%20of%20the%20prediction%2C%20as%20well%20as%20the%20relative%20frequency%20of%20the%20corresponding%20class.%20Our%20observations%20in%20the%20experiments%20are%20corroborated%20by%20significant%20experimental%20results%20performed%20on%20the%20CIFAR-100-LT%2C%20ImageNet-LT%2C%20and%20iNaturalist2018%20datasets%20under%20various%20values%20of%20imbalance%20factors%20that%20clearly%20authenticate%20the%20theoretical%20discussions%20above.&entry.1838667208=http%3A//arxiv.org/abs/2601.15924v1&entry.124074799=Read"},
{"title": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation", "author": "Shams Nafisa Ali and Taufiq Hasan", "abstract": "Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.", "link": "http://arxiv.org/abs/2601.16064v1", "date": "2026-01-22", "relevancy": 2.6083, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phi-SegNet%3A%20Phase-Integrated%20Supervision%20for%20Medical%20Image%20Segmentation&body=Title%3A%20Phi-SegNet%3A%20Phase-Integrated%20Supervision%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Shams%20Nafisa%20Ali%20and%20Taufiq%20Hasan%0AAbstract%3A%20Deep%20learning%20has%20substantially%20advanced%20medical%20image%20segmentation%2C%20yet%20achieving%20robust%20generalization%20across%20diverse%20imaging%20modalities%20and%20anatomical%20structures%20remains%20a%20major%20challenge.%20A%20key%20contributor%20to%20this%20limitation%20lies%20in%20how%20existing%20architectures%2C%20ranging%20from%20CNNs%20to%20Transformers%20and%20their%20hybrids%2C%20primarily%20encode%20spatial%20information%20while%20overlooking%20frequency-domain%20representations%20that%20capture%20rich%20structural%20and%20textural%20cues.%20Although%20few%20recent%20studies%20have%20begun%20exploring%20spectral%20information%20at%20the%20feature%20level%2C%20supervision-level%20integration%20of%20frequency%20cues-crucial%20for%20fine-grained%20object%20localization-remains%20largely%20untapped.%20To%20this%20end%2C%20we%20propose%20Phi-SegNet%2C%20a%20CNN-based%20architecture%20that%20incorporates%20phase-aware%20information%20at%20both%20architectural%20and%20optimization%20levels.%20The%20network%20integrates%20Bi-Feature%20Mask%20Former%20%28BFMF%29%20modules%20that%20blend%20neighboring%20encoder%20features%20to%20reduce%20semantic%20gaps%2C%20and%20Reverse%20Fourier%20Attention%20%28RFA%29%20blocks%20that%20refine%20decoder%20outputs%20using%20phase-regularized%20features.%20A%20dedicated%20phase-aware%20loss%20aligns%20these%20features%20with%20structural%20priors%2C%20forming%20a%20closed%20feedback%20loop%20that%20emphasizes%20boundary%20precision.%20Evaluated%20on%20five%20public%20datasets%20spanning%20X-ray%2C%20US%2C%20histopathology%2C%20MRI%2C%20and%20colonoscopy%2C%20Phi-SegNet%20consistently%20achieved%20state-of-the-art%20performance%2C%20with%20an%20average%20relative%20improvement%20of%201.54%2B/-1.26%25%20in%20IoU%20and%200.98%2B/-0.71%25%20in%20F1-score%20over%20the%20next%20best-performing%20model.%20In%20cross-dataset%20generalization%20scenarios%20involving%20unseen%20datasets%20from%20the%20known%20domain%2C%20Phi-SegNet%20also%20exhibits%20robust%20and%20superior%20performance%2C%20highlighting%20its%20adaptability%20and%20modality-agnostic%20design.%20These%20findings%20demonstrate%20the%20potential%20of%20leveraging%20spectral%20priors%20in%20both%20feature%20representation%20and%20supervision%2C%20paving%20the%20way%20for%20generalized%20segmentation%20frameworks%20that%20excel%20in%20fine-grained%20object%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhi-SegNet%253A%2520Phase-Integrated%2520Supervision%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DShams%2520Nafisa%2520Ali%2520and%2520Taufiq%2520Hasan%26entry.1292438233%3DDeep%2520learning%2520has%2520substantially%2520advanced%2520medical%2520image%2520segmentation%252C%2520yet%2520achieving%2520robust%2520generalization%2520across%2520diverse%2520imaging%2520modalities%2520and%2520anatomical%2520structures%2520remains%2520a%2520major%2520challenge.%2520A%2520key%2520contributor%2520to%2520this%2520limitation%2520lies%2520in%2520how%2520existing%2520architectures%252C%2520ranging%2520from%2520CNNs%2520to%2520Transformers%2520and%2520their%2520hybrids%252C%2520primarily%2520encode%2520spatial%2520information%2520while%2520overlooking%2520frequency-domain%2520representations%2520that%2520capture%2520rich%2520structural%2520and%2520textural%2520cues.%2520Although%2520few%2520recent%2520studies%2520have%2520begun%2520exploring%2520spectral%2520information%2520at%2520the%2520feature%2520level%252C%2520supervision-level%2520integration%2520of%2520frequency%2520cues-crucial%2520for%2520fine-grained%2520object%2520localization-remains%2520largely%2520untapped.%2520To%2520this%2520end%252C%2520we%2520propose%2520Phi-SegNet%252C%2520a%2520CNN-based%2520architecture%2520that%2520incorporates%2520phase-aware%2520information%2520at%2520both%2520architectural%2520and%2520optimization%2520levels.%2520The%2520network%2520integrates%2520Bi-Feature%2520Mask%2520Former%2520%2528BFMF%2529%2520modules%2520that%2520blend%2520neighboring%2520encoder%2520features%2520to%2520reduce%2520semantic%2520gaps%252C%2520and%2520Reverse%2520Fourier%2520Attention%2520%2528RFA%2529%2520blocks%2520that%2520refine%2520decoder%2520outputs%2520using%2520phase-regularized%2520features.%2520A%2520dedicated%2520phase-aware%2520loss%2520aligns%2520these%2520features%2520with%2520structural%2520priors%252C%2520forming%2520a%2520closed%2520feedback%2520loop%2520that%2520emphasizes%2520boundary%2520precision.%2520Evaluated%2520on%2520five%2520public%2520datasets%2520spanning%2520X-ray%252C%2520US%252C%2520histopathology%252C%2520MRI%252C%2520and%2520colonoscopy%252C%2520Phi-SegNet%2520consistently%2520achieved%2520state-of-the-art%2520performance%252C%2520with%2520an%2520average%2520relative%2520improvement%2520of%25201.54%252B/-1.26%2525%2520in%2520IoU%2520and%25200.98%252B/-0.71%2525%2520in%2520F1-score%2520over%2520the%2520next%2520best-performing%2520model.%2520In%2520cross-dataset%2520generalization%2520scenarios%2520involving%2520unseen%2520datasets%2520from%2520the%2520known%2520domain%252C%2520Phi-SegNet%2520also%2520exhibits%2520robust%2520and%2520superior%2520performance%252C%2520highlighting%2520its%2520adaptability%2520and%2520modality-agnostic%2520design.%2520These%2520findings%2520demonstrate%2520the%2520potential%2520of%2520leveraging%2520spectral%2520priors%2520in%2520both%2520feature%2520representation%2520and%2520supervision%252C%2520paving%2520the%2520way%2520for%2520generalized%2520segmentation%2520frameworks%2520that%2520excel%2520in%2520fine-grained%2520object%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phi-SegNet%3A%20Phase-Integrated%20Supervision%20for%20Medical%20Image%20Segmentation&entry.906535625=Shams%20Nafisa%20Ali%20and%20Taufiq%20Hasan&entry.1292438233=Deep%20learning%20has%20substantially%20advanced%20medical%20image%20segmentation%2C%20yet%20achieving%20robust%20generalization%20across%20diverse%20imaging%20modalities%20and%20anatomical%20structures%20remains%20a%20major%20challenge.%20A%20key%20contributor%20to%20this%20limitation%20lies%20in%20how%20existing%20architectures%2C%20ranging%20from%20CNNs%20to%20Transformers%20and%20their%20hybrids%2C%20primarily%20encode%20spatial%20information%20while%20overlooking%20frequency-domain%20representations%20that%20capture%20rich%20structural%20and%20textural%20cues.%20Although%20few%20recent%20studies%20have%20begun%20exploring%20spectral%20information%20at%20the%20feature%20level%2C%20supervision-level%20integration%20of%20frequency%20cues-crucial%20for%20fine-grained%20object%20localization-remains%20largely%20untapped.%20To%20this%20end%2C%20we%20propose%20Phi-SegNet%2C%20a%20CNN-based%20architecture%20that%20incorporates%20phase-aware%20information%20at%20both%20architectural%20and%20optimization%20levels.%20The%20network%20integrates%20Bi-Feature%20Mask%20Former%20%28BFMF%29%20modules%20that%20blend%20neighboring%20encoder%20features%20to%20reduce%20semantic%20gaps%2C%20and%20Reverse%20Fourier%20Attention%20%28RFA%29%20blocks%20that%20refine%20decoder%20outputs%20using%20phase-regularized%20features.%20A%20dedicated%20phase-aware%20loss%20aligns%20these%20features%20with%20structural%20priors%2C%20forming%20a%20closed%20feedback%20loop%20that%20emphasizes%20boundary%20precision.%20Evaluated%20on%20five%20public%20datasets%20spanning%20X-ray%2C%20US%2C%20histopathology%2C%20MRI%2C%20and%20colonoscopy%2C%20Phi-SegNet%20consistently%20achieved%20state-of-the-art%20performance%2C%20with%20an%20average%20relative%20improvement%20of%201.54%2B/-1.26%25%20in%20IoU%20and%200.98%2B/-0.71%25%20in%20F1-score%20over%20the%20next%20best-performing%20model.%20In%20cross-dataset%20generalization%20scenarios%20involving%20unseen%20datasets%20from%20the%20known%20domain%2C%20Phi-SegNet%20also%20exhibits%20robust%20and%20superior%20performance%2C%20highlighting%20its%20adaptability%20and%20modality-agnostic%20design.%20These%20findings%20demonstrate%20the%20potential%20of%20leveraging%20spectral%20priors%20in%20both%20feature%20representation%20and%20supervision%2C%20paving%20the%20way%20for%20generalized%20segmentation%20frameworks%20that%20excel%20in%20fine-grained%20object%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2601.16064v1&entry.124074799=Read"},
{"title": "Training-Free Geospatial Place Representation Learning from Large-Scale Point-of-Interest Graph Data", "author": "Mohammad Hashemi and Hossein Amiri and Andreas Zufle", "abstract": "Learning effective representations of urban environments requires capturing spatial structure beyond fixed administrative boundaries. Existing geospatial representation learning approaches typically aggregate Points of Interest(POI) into pre-defined administrative regions such as census units or ZIP code areas, assigning a single embedding to each region. However, POIs often form semantically meaningful groups that extend across, within, or beyond these boundaries, defining places that better reflect human activity and urban function. To address this limitation, we propose PlaceRep, a training-free geospatial representation learning method that constructs place-level representations by clustering spatially and semantically related POIs. PlaceRep summarizes large-scale POI graphs from U.S. Foursquare data to produce general-purpose urban region embeddings while automatically identifying places across multiple spatial scales. By eliminating model pre-training, PlaceRep provides a scalable and efficient solution for multi-granular geospatial analysis. Experiments using the tasks of population density estimation and housing price prediction as downstream tasks show that PlaceRep outperforms most state-of-the-art graph-based geospatial representation learning methods and achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation of PlaceRep is available at https://github.com/mohammadhashemii/PlaceRep.", "link": "http://arxiv.org/abs/2507.02921v3", "date": "2026-01-22", "relevancy": 2.5733, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5523}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.497}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Geospatial%20Place%20Representation%20Learning%20from%20Large-Scale%20Point-of-Interest%20Graph%20Data&body=Title%3A%20Training-Free%20Geospatial%20Place%20Representation%20Learning%20from%20Large-Scale%20Point-of-Interest%20Graph%20Data%0AAuthor%3A%20Mohammad%20Hashemi%20and%20Hossein%20Amiri%20and%20Andreas%20Zufle%0AAbstract%3A%20Learning%20effective%20representations%20of%20urban%20environments%20requires%20capturing%20spatial%20structure%20beyond%20fixed%20administrative%20boundaries.%20Existing%20geospatial%20representation%20learning%20approaches%20typically%20aggregate%20Points%20of%20Interest%28POI%29%20into%20pre-defined%20administrative%20regions%20such%20as%20census%20units%20or%20ZIP%20code%20areas%2C%20assigning%20a%20single%20embedding%20to%20each%20region.%20However%2C%20POIs%20often%20form%20semantically%20meaningful%20groups%20that%20extend%20across%2C%20within%2C%20or%20beyond%20these%20boundaries%2C%20defining%20places%20that%20better%20reflect%20human%20activity%20and%20urban%20function.%20To%20address%20this%20limitation%2C%20we%20propose%20PlaceRep%2C%20a%20training-free%20geospatial%20representation%20learning%20method%20that%20constructs%20place-level%20representations%20by%20clustering%20spatially%20and%20semantically%20related%20POIs.%20PlaceRep%20summarizes%20large-scale%20POI%20graphs%20from%20U.S.%20Foursquare%20data%20to%20produce%20general-purpose%20urban%20region%20embeddings%20while%20automatically%20identifying%20places%20across%20multiple%20spatial%20scales.%20By%20eliminating%20model%20pre-training%2C%20PlaceRep%20provides%20a%20scalable%20and%20efficient%20solution%20for%20multi-granular%20geospatial%20analysis.%20Experiments%20using%20the%20tasks%20of%20population%20density%20estimation%20and%20housing%20price%20prediction%20as%20downstream%20tasks%20show%20that%20PlaceRep%20outperforms%20most%20state-of-the-art%20graph-based%20geospatial%20representation%20learning%20methods%20and%20achieves%20up%20to%20a%20100x%20speedup%20in%20generating%20region-level%20representations%20on%20large-scale%20POI%20graphs.%20The%20implementation%20of%20PlaceRep%20is%20available%20at%20https%3A//github.com/mohammadhashemii/PlaceRep.%0ALink%3A%20http%3A//arxiv.org/abs/2507.02921v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Geospatial%2520Place%2520Representation%2520Learning%2520from%2520Large-Scale%2520Point-of-Interest%2520Graph%2520Data%26entry.906535625%3DMohammad%2520Hashemi%2520and%2520Hossein%2520Amiri%2520and%2520Andreas%2520Zufle%26entry.1292438233%3DLearning%2520effective%2520representations%2520of%2520urban%2520environments%2520requires%2520capturing%2520spatial%2520structure%2520beyond%2520fixed%2520administrative%2520boundaries.%2520Existing%2520geospatial%2520representation%2520learning%2520approaches%2520typically%2520aggregate%2520Points%2520of%2520Interest%2528POI%2529%2520into%2520pre-defined%2520administrative%2520regions%2520such%2520as%2520census%2520units%2520or%2520ZIP%2520code%2520areas%252C%2520assigning%2520a%2520single%2520embedding%2520to%2520each%2520region.%2520However%252C%2520POIs%2520often%2520form%2520semantically%2520meaningful%2520groups%2520that%2520extend%2520across%252C%2520within%252C%2520or%2520beyond%2520these%2520boundaries%252C%2520defining%2520places%2520that%2520better%2520reflect%2520human%2520activity%2520and%2520urban%2520function.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520PlaceRep%252C%2520a%2520training-free%2520geospatial%2520representation%2520learning%2520method%2520that%2520constructs%2520place-level%2520representations%2520by%2520clustering%2520spatially%2520and%2520semantically%2520related%2520POIs.%2520PlaceRep%2520summarizes%2520large-scale%2520POI%2520graphs%2520from%2520U.S.%2520Foursquare%2520data%2520to%2520produce%2520general-purpose%2520urban%2520region%2520embeddings%2520while%2520automatically%2520identifying%2520places%2520across%2520multiple%2520spatial%2520scales.%2520By%2520eliminating%2520model%2520pre-training%252C%2520PlaceRep%2520provides%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%2520multi-granular%2520geospatial%2520analysis.%2520Experiments%2520using%2520the%2520tasks%2520of%2520population%2520density%2520estimation%2520and%2520housing%2520price%2520prediction%2520as%2520downstream%2520tasks%2520show%2520that%2520PlaceRep%2520outperforms%2520most%2520state-of-the-art%2520graph-based%2520geospatial%2520representation%2520learning%2520methods%2520and%2520achieves%2520up%2520to%2520a%2520100x%2520speedup%2520in%2520generating%2520region-level%2520representations%2520on%2520large-scale%2520POI%2520graphs.%2520The%2520implementation%2520of%2520PlaceRep%2520is%2520available%2520at%2520https%253A//github.com/mohammadhashemii/PlaceRep.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02921v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Geospatial%20Place%20Representation%20Learning%20from%20Large-Scale%20Point-of-Interest%20Graph%20Data&entry.906535625=Mohammad%20Hashemi%20and%20Hossein%20Amiri%20and%20Andreas%20Zufle&entry.1292438233=Learning%20effective%20representations%20of%20urban%20environments%20requires%20capturing%20spatial%20structure%20beyond%20fixed%20administrative%20boundaries.%20Existing%20geospatial%20representation%20learning%20approaches%20typically%20aggregate%20Points%20of%20Interest%28POI%29%20into%20pre-defined%20administrative%20regions%20such%20as%20census%20units%20or%20ZIP%20code%20areas%2C%20assigning%20a%20single%20embedding%20to%20each%20region.%20However%2C%20POIs%20often%20form%20semantically%20meaningful%20groups%20that%20extend%20across%2C%20within%2C%20or%20beyond%20these%20boundaries%2C%20defining%20places%20that%20better%20reflect%20human%20activity%20and%20urban%20function.%20To%20address%20this%20limitation%2C%20we%20propose%20PlaceRep%2C%20a%20training-free%20geospatial%20representation%20learning%20method%20that%20constructs%20place-level%20representations%20by%20clustering%20spatially%20and%20semantically%20related%20POIs.%20PlaceRep%20summarizes%20large-scale%20POI%20graphs%20from%20U.S.%20Foursquare%20data%20to%20produce%20general-purpose%20urban%20region%20embeddings%20while%20automatically%20identifying%20places%20across%20multiple%20spatial%20scales.%20By%20eliminating%20model%20pre-training%2C%20PlaceRep%20provides%20a%20scalable%20and%20efficient%20solution%20for%20multi-granular%20geospatial%20analysis.%20Experiments%20using%20the%20tasks%20of%20population%20density%20estimation%20and%20housing%20price%20prediction%20as%20downstream%20tasks%20show%20that%20PlaceRep%20outperforms%20most%20state-of-the-art%20graph-based%20geospatial%20representation%20learning%20methods%20and%20achieves%20up%20to%20a%20100x%20speedup%20in%20generating%20region-level%20representations%20on%20large-scale%20POI%20graphs.%20The%20implementation%20of%20PlaceRep%20is%20available%20at%20https%3A//github.com/mohammadhashemii/PlaceRep.&entry.1838667208=http%3A//arxiv.org/abs/2507.02921v3&entry.124074799=Read"},
{"title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry", "author": "Rongze Ma and Mengkang Lu and Zhenyu Xiang and Yongsheng Pan and Yicheng Wu and Qingjie Zeng and Yong Xia", "abstract": "Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.", "link": "http://arxiv.org/abs/2601.16024v1", "date": "2026-01-22", "relevancy": 2.5595, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5167}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5115}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAINT%3A%20Pathology-Aware%20Integrated%20Next-Scale%20Transformation%20for%20Virtual%20Immunohistochemistry&body=Title%3A%20PAINT%3A%20Pathology-Aware%20Integrated%20Next-Scale%20Transformation%20for%20Virtual%20Immunohistochemistry%0AAuthor%3A%20Rongze%20Ma%20and%20Mengkang%20Lu%20and%20Zhenyu%20Xiang%20and%20Yongsheng%20Pan%20and%20Yicheng%20Wu%20and%20Qingjie%20Zeng%20and%20Yong%20Xia%0AAbstract%3A%20Virtual%20immunohistochemistry%20%28IHC%29%20aims%20to%20computationally%20synthesize%20molecular%20staining%20patterns%20from%20routine%20Hematoxylin%20and%20Eosin%20%28H%5C%26E%29%20images%2C%20offering%20a%20cost-effective%20and%20tissue-efficient%20alternative%20to%20traditional%20physical%20staining.%20However%2C%20this%20task%20is%20particularly%20challenging%3A%20H%5C%26E%20morphology%20provides%20ambiguous%20cues%20about%20protein%20expression%2C%20and%20similar%20tissue%20structures%20may%20correspond%20to%20distinct%20molecular%20states.%20Most%20existing%20methods%20focus%20on%20direct%20appearance%20synthesis%20to%20implicitly%20achieve%20cross-modal%20generation%2C%20often%20resulting%20in%20semantic%20inconsistencies%20due%20to%20insufficient%20structural%20priors.%20In%20this%20paper%2C%20we%20propose%20Pathology-Aware%20Integrated%20Next-Scale%20Transformation%20%28PAINT%29%2C%20a%20visual%20autoregressive%20framework%20that%20reformulates%20the%20synthesis%20process%20as%20a%20structure-first%20conditional%20generation%20task.%20Unlike%20direct%20image%20translation%2C%20PAINT%20enforces%20a%20causal%20order%20by%20resolving%20molecular%20details%20conditioned%20on%20a%20global%20structural%20layout.%20Central%20to%20this%20approach%20is%20the%20introduction%20of%20a%20Spatial%20Structural%20Start%20Map%20%283S-Map%29%2C%20which%20grounds%20the%20autoregressive%20initialization%20in%20observed%20morphology%2C%20ensuring%20deterministic%2C%20spatially%20aligned%20synthesis.%20Experiments%20on%20the%20IHC4BC%20and%20MIST%20datasets%20demonstrate%20that%20PAINT%20outperforms%20state-of-the-art%20methods%20in%20structural%20fidelity%20and%20clinical%20downstream%20tasks%2C%20validating%20the%20potential%20of%20structure-guided%20autoregressive%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAINT%253A%2520Pathology-Aware%2520Integrated%2520Next-Scale%2520Transformation%2520for%2520Virtual%2520Immunohistochemistry%26entry.906535625%3DRongze%2520Ma%2520and%2520Mengkang%2520Lu%2520and%2520Zhenyu%2520Xiang%2520and%2520Yongsheng%2520Pan%2520and%2520Yicheng%2520Wu%2520and%2520Qingjie%2520Zeng%2520and%2520Yong%2520Xia%26entry.1292438233%3DVirtual%2520immunohistochemistry%2520%2528IHC%2529%2520aims%2520to%2520computationally%2520synthesize%2520molecular%2520staining%2520patterns%2520from%2520routine%2520Hematoxylin%2520and%2520Eosin%2520%2528H%255C%2526E%2529%2520images%252C%2520offering%2520a%2520cost-effective%2520and%2520tissue-efficient%2520alternative%2520to%2520traditional%2520physical%2520staining.%2520However%252C%2520this%2520task%2520is%2520particularly%2520challenging%253A%2520H%255C%2526E%2520morphology%2520provides%2520ambiguous%2520cues%2520about%2520protein%2520expression%252C%2520and%2520similar%2520tissue%2520structures%2520may%2520correspond%2520to%2520distinct%2520molecular%2520states.%2520Most%2520existing%2520methods%2520focus%2520on%2520direct%2520appearance%2520synthesis%2520to%2520implicitly%2520achieve%2520cross-modal%2520generation%252C%2520often%2520resulting%2520in%2520semantic%2520inconsistencies%2520due%2520to%2520insufficient%2520structural%2520priors.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Pathology-Aware%2520Integrated%2520Next-Scale%2520Transformation%2520%2528PAINT%2529%252C%2520a%2520visual%2520autoregressive%2520framework%2520that%2520reformulates%2520the%2520synthesis%2520process%2520as%2520a%2520structure-first%2520conditional%2520generation%2520task.%2520Unlike%2520direct%2520image%2520translation%252C%2520PAINT%2520enforces%2520a%2520causal%2520order%2520by%2520resolving%2520molecular%2520details%2520conditioned%2520on%2520a%2520global%2520structural%2520layout.%2520Central%2520to%2520this%2520approach%2520is%2520the%2520introduction%2520of%2520a%2520Spatial%2520Structural%2520Start%2520Map%2520%25283S-Map%2529%252C%2520which%2520grounds%2520the%2520autoregressive%2520initialization%2520in%2520observed%2520morphology%252C%2520ensuring%2520deterministic%252C%2520spatially%2520aligned%2520synthesis.%2520Experiments%2520on%2520the%2520IHC4BC%2520and%2520MIST%2520datasets%2520demonstrate%2520that%2520PAINT%2520outperforms%2520state-of-the-art%2520methods%2520in%2520structural%2520fidelity%2520and%2520clinical%2520downstream%2520tasks%252C%2520validating%2520the%2520potential%2520of%2520structure-guided%2520autoregressive%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAINT%3A%20Pathology-Aware%20Integrated%20Next-Scale%20Transformation%20for%20Virtual%20Immunohistochemistry&entry.906535625=Rongze%20Ma%20and%20Mengkang%20Lu%20and%20Zhenyu%20Xiang%20and%20Yongsheng%20Pan%20and%20Yicheng%20Wu%20and%20Qingjie%20Zeng%20and%20Yong%20Xia&entry.1292438233=Virtual%20immunohistochemistry%20%28IHC%29%20aims%20to%20computationally%20synthesize%20molecular%20staining%20patterns%20from%20routine%20Hematoxylin%20and%20Eosin%20%28H%5C%26E%29%20images%2C%20offering%20a%20cost-effective%20and%20tissue-efficient%20alternative%20to%20traditional%20physical%20staining.%20However%2C%20this%20task%20is%20particularly%20challenging%3A%20H%5C%26E%20morphology%20provides%20ambiguous%20cues%20about%20protein%20expression%2C%20and%20similar%20tissue%20structures%20may%20correspond%20to%20distinct%20molecular%20states.%20Most%20existing%20methods%20focus%20on%20direct%20appearance%20synthesis%20to%20implicitly%20achieve%20cross-modal%20generation%2C%20often%20resulting%20in%20semantic%20inconsistencies%20due%20to%20insufficient%20structural%20priors.%20In%20this%20paper%2C%20we%20propose%20Pathology-Aware%20Integrated%20Next-Scale%20Transformation%20%28PAINT%29%2C%20a%20visual%20autoregressive%20framework%20that%20reformulates%20the%20synthesis%20process%20as%20a%20structure-first%20conditional%20generation%20task.%20Unlike%20direct%20image%20translation%2C%20PAINT%20enforces%20a%20causal%20order%20by%20resolving%20molecular%20details%20conditioned%20on%20a%20global%20structural%20layout.%20Central%20to%20this%20approach%20is%20the%20introduction%20of%20a%20Spatial%20Structural%20Start%20Map%20%283S-Map%29%2C%20which%20grounds%20the%20autoregressive%20initialization%20in%20observed%20morphology%2C%20ensuring%20deterministic%2C%20spatially%20aligned%20synthesis.%20Experiments%20on%20the%20IHC4BC%20and%20MIST%20datasets%20demonstrate%20that%20PAINT%20outperforms%20state-of-the-art%20methods%20in%20structural%20fidelity%20and%20clinical%20downstream%20tasks%2C%20validating%20the%20potential%20of%20structure-guided%20autoregressive%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2601.16024v1&entry.124074799=Read"},
{"title": "FedIA: Towards Domain-Robust Aggregation in Federated Graph Learning", "author": "Zhanting Zhou and KaHou Tam and Yiding Feng and Ziqiang Zheng and Zeyu Ma and Yang Yang", "abstract": "Federated Graph Learning (FGL) enables a central server to coordinate model training across distributed clients without local graph data being shared. However, FGL significantly suffers from cross-silo domain shifts, where each \"silo\" (domain) contains a limited number of clients with distinct graph topologies. These heterogeneities induce divergent optimization trajectories, ultimately leading to global model divergence. In this work, we reveal a severe architectural pathology termed Structural Orthogonality: the topology-dependent message passing mechanism forces gradients from different domains to target disjoint coordinates in the parameter space. Through a controlled comparison between backbones, we statistically prove that GNN updates are near-perpendicular across domains (with projection ratios $\\to$ 0). Consequently, naive averaging leads to Consensus Collapse, a phenomenon where sparse, informative structural signals from individual domains are diluted by the near-zero updates of others. This forces the global model into a \"sub-optimal\" state that fails to represent domain-specific structural patterns, resulting in poor generalization. To address this, we propose FedIA, a lightweight server-side framework designed to reconcile update conflicts without auxiliary communication. FedIA operates in two stages: (1) Global Importance Masking (GIM) identifies a shared parameter subspace to filter out domain-specific structural noise and prevent signal dilution; (2) Confidence-Aware Momentum Weighting (CAM) dynamically re-weights client contributions based on gradient reliability to amplify stable optimization signals.", "link": "http://arxiv.org/abs/2509.18171v3", "date": "2026-01-22", "relevancy": 2.5293, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.54}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4963}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedIA%3A%20Towards%20Domain-Robust%20Aggregation%20in%20Federated%20Graph%20Learning&body=Title%3A%20FedIA%3A%20Towards%20Domain-Robust%20Aggregation%20in%20Federated%20Graph%20Learning%0AAuthor%3A%20Zhanting%20Zhou%20and%20KaHou%20Tam%20and%20Yiding%20Feng%20and%20Ziqiang%20Zheng%20and%20Zeyu%20Ma%20and%20Yang%20Yang%0AAbstract%3A%20Federated%20Graph%20Learning%20%28FGL%29%20enables%20a%20central%20server%20to%20coordinate%20model%20training%20across%20distributed%20clients%20without%20local%20graph%20data%20being%20shared.%20However%2C%20FGL%20significantly%20suffers%20from%20cross-silo%20domain%20shifts%2C%20where%20each%20%22silo%22%20%28domain%29%20contains%20a%20limited%20number%20of%20clients%20with%20distinct%20graph%20topologies.%20These%20heterogeneities%20induce%20divergent%20optimization%20trajectories%2C%20ultimately%20leading%20to%20global%20model%20divergence.%20In%20this%20work%2C%20we%20reveal%20a%20severe%20architectural%20pathology%20termed%20Structural%20Orthogonality%3A%20the%20topology-dependent%20message%20passing%20mechanism%20forces%20gradients%20from%20different%20domains%20to%20target%20disjoint%20coordinates%20in%20the%20parameter%20space.%20Through%20a%20controlled%20comparison%20between%20backbones%2C%20we%20statistically%20prove%20that%20GNN%20updates%20are%20near-perpendicular%20across%20domains%20%28with%20projection%20ratios%20%24%5Cto%24%200%29.%20Consequently%2C%20naive%20averaging%20leads%20to%20Consensus%20Collapse%2C%20a%20phenomenon%20where%20sparse%2C%20informative%20structural%20signals%20from%20individual%20domains%20are%20diluted%20by%20the%20near-zero%20updates%20of%20others.%20This%20forces%20the%20global%20model%20into%20a%20%22sub-optimal%22%20state%20that%20fails%20to%20represent%20domain-specific%20structural%20patterns%2C%20resulting%20in%20poor%20generalization.%20To%20address%20this%2C%20we%20propose%20FedIA%2C%20a%20lightweight%20server-side%20framework%20designed%20to%20reconcile%20update%20conflicts%20without%20auxiliary%20communication.%20FedIA%20operates%20in%20two%20stages%3A%20%281%29%20Global%20Importance%20Masking%20%28GIM%29%20identifies%20a%20shared%20parameter%20subspace%20to%20filter%20out%20domain-specific%20structural%20noise%20and%20prevent%20signal%20dilution%3B%20%282%29%20Confidence-Aware%20Momentum%20Weighting%20%28CAM%29%20dynamically%20re-weights%20client%20contributions%20based%20on%20gradient%20reliability%20to%20amplify%20stable%20optimization%20signals.%0ALink%3A%20http%3A//arxiv.org/abs/2509.18171v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedIA%253A%2520Towards%2520Domain-Robust%2520Aggregation%2520in%2520Federated%2520Graph%2520Learning%26entry.906535625%3DZhanting%2520Zhou%2520and%2520KaHou%2520Tam%2520and%2520Yiding%2520Feng%2520and%2520Ziqiang%2520Zheng%2520and%2520Zeyu%2520Ma%2520and%2520Yang%2520Yang%26entry.1292438233%3DFederated%2520Graph%2520Learning%2520%2528FGL%2529%2520enables%2520a%2520central%2520server%2520to%2520coordinate%2520model%2520training%2520across%2520distributed%2520clients%2520without%2520local%2520graph%2520data%2520being%2520shared.%2520However%252C%2520FGL%2520significantly%2520suffers%2520from%2520cross-silo%2520domain%2520shifts%252C%2520where%2520each%2520%2522silo%2522%2520%2528domain%2529%2520contains%2520a%2520limited%2520number%2520of%2520clients%2520with%2520distinct%2520graph%2520topologies.%2520These%2520heterogeneities%2520induce%2520divergent%2520optimization%2520trajectories%252C%2520ultimately%2520leading%2520to%2520global%2520model%2520divergence.%2520In%2520this%2520work%252C%2520we%2520reveal%2520a%2520severe%2520architectural%2520pathology%2520termed%2520Structural%2520Orthogonality%253A%2520the%2520topology-dependent%2520message%2520passing%2520mechanism%2520forces%2520gradients%2520from%2520different%2520domains%2520to%2520target%2520disjoint%2520coordinates%2520in%2520the%2520parameter%2520space.%2520Through%2520a%2520controlled%2520comparison%2520between%2520backbones%252C%2520we%2520statistically%2520prove%2520that%2520GNN%2520updates%2520are%2520near-perpendicular%2520across%2520domains%2520%2528with%2520projection%2520ratios%2520%2524%255Cto%2524%25200%2529.%2520Consequently%252C%2520naive%2520averaging%2520leads%2520to%2520Consensus%2520Collapse%252C%2520a%2520phenomenon%2520where%2520sparse%252C%2520informative%2520structural%2520signals%2520from%2520individual%2520domains%2520are%2520diluted%2520by%2520the%2520near-zero%2520updates%2520of%2520others.%2520This%2520forces%2520the%2520global%2520model%2520into%2520a%2520%2522sub-optimal%2522%2520state%2520that%2520fails%2520to%2520represent%2520domain-specific%2520structural%2520patterns%252C%2520resulting%2520in%2520poor%2520generalization.%2520To%2520address%2520this%252C%2520we%2520propose%2520FedIA%252C%2520a%2520lightweight%2520server-side%2520framework%2520designed%2520to%2520reconcile%2520update%2520conflicts%2520without%2520auxiliary%2520communication.%2520FedIA%2520operates%2520in%2520two%2520stages%253A%2520%25281%2529%2520Global%2520Importance%2520Masking%2520%2528GIM%2529%2520identifies%2520a%2520shared%2520parameter%2520subspace%2520to%2520filter%2520out%2520domain-specific%2520structural%2520noise%2520and%2520prevent%2520signal%2520dilution%253B%2520%25282%2529%2520Confidence-Aware%2520Momentum%2520Weighting%2520%2528CAM%2529%2520dynamically%2520re-weights%2520client%2520contributions%2520based%2520on%2520gradient%2520reliability%2520to%2520amplify%2520stable%2520optimization%2520signals.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18171v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedIA%3A%20Towards%20Domain-Robust%20Aggregation%20in%20Federated%20Graph%20Learning&entry.906535625=Zhanting%20Zhou%20and%20KaHou%20Tam%20and%20Yiding%20Feng%20and%20Ziqiang%20Zheng%20and%20Zeyu%20Ma%20and%20Yang%20Yang&entry.1292438233=Federated%20Graph%20Learning%20%28FGL%29%20enables%20a%20central%20server%20to%20coordinate%20model%20training%20across%20distributed%20clients%20without%20local%20graph%20data%20being%20shared.%20However%2C%20FGL%20significantly%20suffers%20from%20cross-silo%20domain%20shifts%2C%20where%20each%20%22silo%22%20%28domain%29%20contains%20a%20limited%20number%20of%20clients%20with%20distinct%20graph%20topologies.%20These%20heterogeneities%20induce%20divergent%20optimization%20trajectories%2C%20ultimately%20leading%20to%20global%20model%20divergence.%20In%20this%20work%2C%20we%20reveal%20a%20severe%20architectural%20pathology%20termed%20Structural%20Orthogonality%3A%20the%20topology-dependent%20message%20passing%20mechanism%20forces%20gradients%20from%20different%20domains%20to%20target%20disjoint%20coordinates%20in%20the%20parameter%20space.%20Through%20a%20controlled%20comparison%20between%20backbones%2C%20we%20statistically%20prove%20that%20GNN%20updates%20are%20near-perpendicular%20across%20domains%20%28with%20projection%20ratios%20%24%5Cto%24%200%29.%20Consequently%2C%20naive%20averaging%20leads%20to%20Consensus%20Collapse%2C%20a%20phenomenon%20where%20sparse%2C%20informative%20structural%20signals%20from%20individual%20domains%20are%20diluted%20by%20the%20near-zero%20updates%20of%20others.%20This%20forces%20the%20global%20model%20into%20a%20%22sub-optimal%22%20state%20that%20fails%20to%20represent%20domain-specific%20structural%20patterns%2C%20resulting%20in%20poor%20generalization.%20To%20address%20this%2C%20we%20propose%20FedIA%2C%20a%20lightweight%20server-side%20framework%20designed%20to%20reconcile%20update%20conflicts%20without%20auxiliary%20communication.%20FedIA%20operates%20in%20two%20stages%3A%20%281%29%20Global%20Importance%20Masking%20%28GIM%29%20identifies%20a%20shared%20parameter%20subspace%20to%20filter%20out%20domain-specific%20structural%20noise%20and%20prevent%20signal%20dilution%3B%20%282%29%20Confidence-Aware%20Momentum%20Weighting%20%28CAM%29%20dynamically%20re-weights%20client%20contributions%20based%20on%20gradient%20reliability%20to%20amplify%20stable%20optimization%20signals.&entry.1838667208=http%3A//arxiv.org/abs/2509.18171v3&entry.124074799=Read"},
{"title": "DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning", "author": "Junha Lee and Eunha Park and Minsu Cho", "abstract": "Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.", "link": "http://arxiv.org/abs/2601.16046v1", "date": "2026-01-22", "relevancy": 2.5258, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7433}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5564}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DextER%3A%20Language-driven%20Dexterous%20Grasp%20Generation%20with%20Embodied%20Reasoning&body=Title%3A%20DextER%3A%20Language-driven%20Dexterous%20Grasp%20Generation%20with%20Embodied%20Reasoning%0AAuthor%3A%20Junha%20Lee%20and%20Eunha%20Park%20and%20Minsu%20Cho%0AAbstract%3A%20Language-driven%20dexterous%20grasp%20generation%20requires%20the%20models%20to%20understand%20task%20semantics%2C%203D%20geometry%2C%20and%20complex%20hand-object%20interactions.%20While%20vision-language%20models%20have%20been%20applied%20to%20this%20problem%2C%20existing%20approaches%20directly%20map%20observations%20to%20grasp%20parameters%20without%20intermediate%20reasoning%20about%20physical%20interactions.%20We%20present%20DextER%2C%20Dexterous%20Grasp%20Generation%20with%20Embodied%20Reasoning%2C%20which%20introduces%20contact-based%20embodied%20reasoning%20for%20multi-finger%20manipulation.%20Our%20key%20insight%20is%20that%20predicting%20which%20hand%20links%20contact%20where%20on%20the%20object%20surface%20provides%20an%20embodiment-aware%20intermediate%20representation%20bridging%20task%20semantics%20with%20physical%20constraints.%20DextER%20autoregressively%20generates%20embodied%20contact%20tokens%20specifying%20which%20finger%20links%20contact%20where%20on%20the%20object%20surface%2C%20followed%20by%20grasp%20tokens%20encoding%20the%20hand%20configuration.%20On%20DexGYS%2C%20DextER%20achieves%2067.14%25%20success%20rate%2C%20outperforming%20state-of-the-art%20by%203.83%25p%20with%2096.4%25%20improvement%20in%20intention%20alignment.%20We%20also%20demonstrate%20steerable%20generation%20through%20partial%20contact%20specification%2C%20providing%20fine-grained%20control%20over%20grasp%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDextER%253A%2520Language-driven%2520Dexterous%2520Grasp%2520Generation%2520with%2520Embodied%2520Reasoning%26entry.906535625%3DJunha%2520Lee%2520and%2520Eunha%2520Park%2520and%2520Minsu%2520Cho%26entry.1292438233%3DLanguage-driven%2520dexterous%2520grasp%2520generation%2520requires%2520the%2520models%2520to%2520understand%2520task%2520semantics%252C%25203D%2520geometry%252C%2520and%2520complex%2520hand-object%2520interactions.%2520While%2520vision-language%2520models%2520have%2520been%2520applied%2520to%2520this%2520problem%252C%2520existing%2520approaches%2520directly%2520map%2520observations%2520to%2520grasp%2520parameters%2520without%2520intermediate%2520reasoning%2520about%2520physical%2520interactions.%2520We%2520present%2520DextER%252C%2520Dexterous%2520Grasp%2520Generation%2520with%2520Embodied%2520Reasoning%252C%2520which%2520introduces%2520contact-based%2520embodied%2520reasoning%2520for%2520multi-finger%2520manipulation.%2520Our%2520key%2520insight%2520is%2520that%2520predicting%2520which%2520hand%2520links%2520contact%2520where%2520on%2520the%2520object%2520surface%2520provides%2520an%2520embodiment-aware%2520intermediate%2520representation%2520bridging%2520task%2520semantics%2520with%2520physical%2520constraints.%2520DextER%2520autoregressively%2520generates%2520embodied%2520contact%2520tokens%2520specifying%2520which%2520finger%2520links%2520contact%2520where%2520on%2520the%2520object%2520surface%252C%2520followed%2520by%2520grasp%2520tokens%2520encoding%2520the%2520hand%2520configuration.%2520On%2520DexGYS%252C%2520DextER%2520achieves%252067.14%2525%2520success%2520rate%252C%2520outperforming%2520state-of-the-art%2520by%25203.83%2525p%2520with%252096.4%2525%2520improvement%2520in%2520intention%2520alignment.%2520We%2520also%2520demonstrate%2520steerable%2520generation%2520through%2520partial%2520contact%2520specification%252C%2520providing%2520fine-grained%2520control%2520over%2520grasp%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DextER%3A%20Language-driven%20Dexterous%20Grasp%20Generation%20with%20Embodied%20Reasoning&entry.906535625=Junha%20Lee%20and%20Eunha%20Park%20and%20Minsu%20Cho&entry.1292438233=Language-driven%20dexterous%20grasp%20generation%20requires%20the%20models%20to%20understand%20task%20semantics%2C%203D%20geometry%2C%20and%20complex%20hand-object%20interactions.%20While%20vision-language%20models%20have%20been%20applied%20to%20this%20problem%2C%20existing%20approaches%20directly%20map%20observations%20to%20grasp%20parameters%20without%20intermediate%20reasoning%20about%20physical%20interactions.%20We%20present%20DextER%2C%20Dexterous%20Grasp%20Generation%20with%20Embodied%20Reasoning%2C%20which%20introduces%20contact-based%20embodied%20reasoning%20for%20multi-finger%20manipulation.%20Our%20key%20insight%20is%20that%20predicting%20which%20hand%20links%20contact%20where%20on%20the%20object%20surface%20provides%20an%20embodiment-aware%20intermediate%20representation%20bridging%20task%20semantics%20with%20physical%20constraints.%20DextER%20autoregressively%20generates%20embodied%20contact%20tokens%20specifying%20which%20finger%20links%20contact%20where%20on%20the%20object%20surface%2C%20followed%20by%20grasp%20tokens%20encoding%20the%20hand%20configuration.%20On%20DexGYS%2C%20DextER%20achieves%2067.14%25%20success%20rate%2C%20outperforming%20state-of-the-art%20by%203.83%25p%20with%2096.4%25%20improvement%20in%20intention%20alignment.%20We%20also%20demonstrate%20steerable%20generation%20through%20partial%20contact%20specification%2C%20providing%20fine-grained%20control%20over%20grasp%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2601.16046v1&entry.124074799=Read"},
{"title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems", "author": "Prakash Dhungana and Sayed Ahmad Salehi", "abstract": "Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.", "link": "http://arxiv.org/abs/2601.16158v1", "date": "2026-01-22", "relevancy": 2.5223, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5112}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Incremental%20Continual%20Learning%20for%20Robust%20and%20Efficient%20Keyword%20Spotting%20in%20Resource%20Constrained%20Systems&body=Title%3A%20Domain-Incremental%20Continual%20Learning%20for%20Robust%20and%20Efficient%20Keyword%20Spotting%20in%20Resource%20Constrained%20Systems%0AAuthor%3A%20Prakash%20Dhungana%20and%20Sayed%20Ahmad%20Salehi%0AAbstract%3A%20Keyword%20Spotting%20%28KWS%29%20systems%20with%20small%20footprint%20models%20deployed%20on%20edge%20devices%20face%20significant%20accuracy%20and%20robustness%20challenges%20due%20to%20domain%20shifts%20caused%20by%20varying%20noise%20and%20recording%20conditions.%20To%20address%20this%2C%20we%20propose%20a%20comprehensive%20framework%20for%20continual%20learning%20designed%20to%20adapt%20to%20new%20domains%20while%20maintaining%20computational%20efficiency.%20The%20proposed%20pipeline%20integrates%20a%20dual-input%20Convolutional%20Neural%20Network%2C%20utilizing%20both%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20and%20Mel-spectrogram%20features%2C%20supported%20by%20a%20multi-stage%20denoising%20process%2C%20involving%20discrete%20wavelet%20transform%20and%20spectral%20subtraction%20techniques%2C%20plus%20model%20and%20prototype%20update%20blocks.%20Unlike%20prior%20methods%20that%20restrict%20updates%20to%20specific%20layers%2C%20our%20approach%20updates%20the%20complete%20quantized%20model%2C%20made%20possible%20due%20to%20compact%20model%20architecture.%20A%20subset%20of%20input%20samples%20are%20selected%20during%20runtime%20using%20class%20prototypes%20and%20confidence-driven%20filtering%2C%20which%20are%20then%20pseudo-labeled%20and%20combined%20with%20rehearsal%20buffer%20for%20incremental%20model%20retraining.%20Experimental%20results%20on%20noisy%20test%20dataset%20demonstrate%20the%20framework%27s%20effectiveness%2C%20achieving%2099.63%5C%25%20accuracy%20on%20clean%20data%20and%20maintaining%20robust%20performance%20%28exceeding%2094%5C%25%20accuracy%29%20across%20diverse%20noisy%20environments%2C%20even%20at%20-10%20dB%20Signal-to-Noise%20Ratio.%20The%20proposed%20framework%20work%20confirms%20that%20integrating%20efficient%20denoising%20with%20prototype-based%20continual%20learning%20enables%20KWS%20models%20to%20operate%20autonomously%20and%20robustly%20in%20resource-constrained%2C%20dynamic%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Incremental%2520Continual%2520Learning%2520for%2520Robust%2520and%2520Efficient%2520Keyword%2520Spotting%2520in%2520Resource%2520Constrained%2520Systems%26entry.906535625%3DPrakash%2520Dhungana%2520and%2520Sayed%2520Ahmad%2520Salehi%26entry.1292438233%3DKeyword%2520Spotting%2520%2528KWS%2529%2520systems%2520with%2520small%2520footprint%2520models%2520deployed%2520on%2520edge%2520devices%2520face%2520significant%2520accuracy%2520and%2520robustness%2520challenges%2520due%2520to%2520domain%2520shifts%2520caused%2520by%2520varying%2520noise%2520and%2520recording%2520conditions.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520comprehensive%2520framework%2520for%2520continual%2520learning%2520designed%2520to%2520adapt%2520to%2520new%2520domains%2520while%2520maintaining%2520computational%2520efficiency.%2520The%2520proposed%2520pipeline%2520integrates%2520a%2520dual-input%2520Convolutional%2520Neural%2520Network%252C%2520utilizing%2520both%2520Mel%2520Frequency%2520Cepstral%2520Coefficients%2520%2528MFCC%2529%2520and%2520Mel-spectrogram%2520features%252C%2520supported%2520by%2520a%2520multi-stage%2520denoising%2520process%252C%2520involving%2520discrete%2520wavelet%2520transform%2520and%2520spectral%2520subtraction%2520techniques%252C%2520plus%2520model%2520and%2520prototype%2520update%2520blocks.%2520Unlike%2520prior%2520methods%2520that%2520restrict%2520updates%2520to%2520specific%2520layers%252C%2520our%2520approach%2520updates%2520the%2520complete%2520quantized%2520model%252C%2520made%2520possible%2520due%2520to%2520compact%2520model%2520architecture.%2520A%2520subset%2520of%2520input%2520samples%2520are%2520selected%2520during%2520runtime%2520using%2520class%2520prototypes%2520and%2520confidence-driven%2520filtering%252C%2520which%2520are%2520then%2520pseudo-labeled%2520and%2520combined%2520with%2520rehearsal%2520buffer%2520for%2520incremental%2520model%2520retraining.%2520Experimental%2520results%2520on%2520noisy%2520test%2520dataset%2520demonstrate%2520the%2520framework%2527s%2520effectiveness%252C%2520achieving%252099.63%255C%2525%2520accuracy%2520on%2520clean%2520data%2520and%2520maintaining%2520robust%2520performance%2520%2528exceeding%252094%255C%2525%2520accuracy%2529%2520across%2520diverse%2520noisy%2520environments%252C%2520even%2520at%2520-10%2520dB%2520Signal-to-Noise%2520Ratio.%2520The%2520proposed%2520framework%2520work%2520confirms%2520that%2520integrating%2520efficient%2520denoising%2520with%2520prototype-based%2520continual%2520learning%2520enables%2520KWS%2520models%2520to%2520operate%2520autonomously%2520and%2520robustly%2520in%2520resource-constrained%252C%2520dynamic%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Incremental%20Continual%20Learning%20for%20Robust%20and%20Efficient%20Keyword%20Spotting%20in%20Resource%20Constrained%20Systems&entry.906535625=Prakash%20Dhungana%20and%20Sayed%20Ahmad%20Salehi&entry.1292438233=Keyword%20Spotting%20%28KWS%29%20systems%20with%20small%20footprint%20models%20deployed%20on%20edge%20devices%20face%20significant%20accuracy%20and%20robustness%20challenges%20due%20to%20domain%20shifts%20caused%20by%20varying%20noise%20and%20recording%20conditions.%20To%20address%20this%2C%20we%20propose%20a%20comprehensive%20framework%20for%20continual%20learning%20designed%20to%20adapt%20to%20new%20domains%20while%20maintaining%20computational%20efficiency.%20The%20proposed%20pipeline%20integrates%20a%20dual-input%20Convolutional%20Neural%20Network%2C%20utilizing%20both%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20and%20Mel-spectrogram%20features%2C%20supported%20by%20a%20multi-stage%20denoising%20process%2C%20involving%20discrete%20wavelet%20transform%20and%20spectral%20subtraction%20techniques%2C%20plus%20model%20and%20prototype%20update%20blocks.%20Unlike%20prior%20methods%20that%20restrict%20updates%20to%20specific%20layers%2C%20our%20approach%20updates%20the%20complete%20quantized%20model%2C%20made%20possible%20due%20to%20compact%20model%20architecture.%20A%20subset%20of%20input%20samples%20are%20selected%20during%20runtime%20using%20class%20prototypes%20and%20confidence-driven%20filtering%2C%20which%20are%20then%20pseudo-labeled%20and%20combined%20with%20rehearsal%20buffer%20for%20incremental%20model%20retraining.%20Experimental%20results%20on%20noisy%20test%20dataset%20demonstrate%20the%20framework%27s%20effectiveness%2C%20achieving%2099.63%5C%25%20accuracy%20on%20clean%20data%20and%20maintaining%20robust%20performance%20%28exceeding%2094%5C%25%20accuracy%29%20across%20diverse%20noisy%20environments%2C%20even%20at%20-10%20dB%20Signal-to-Noise%20Ratio.%20The%20proposed%20framework%20work%20confirms%20that%20integrating%20efficient%20denoising%20with%20prototype-based%20continual%20learning%20enables%20KWS%20models%20to%20operate%20autonomously%20and%20robustly%20in%20resource-constrained%2C%20dynamic%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.16158v1&entry.124074799=Read"},
{"title": "Multi-View Projection for Unsupervised Domain Adaptation in 3D Semantic Segmentation", "author": "Andrew Caunes and Thierry Chateau and Vincent Fremont", "abstract": "3D semantic segmentation plays a pivotal role in autonomous driving and road infrastructure analysis, yet state-of-the-art 3D models are prone to severe domain shift when deployed across different datasets. In this paper, we propose an Unsupervised Domain Adaptation approach where a 3D segmentation model is trained on the target dataset using pseudo-labels generated by a novel multi-view projection framework. Our approach first aligns Lidar scans into coherent 3D scenes and renders them from multiple virtual camera poses to create large-scale synthetic 2D semantic segmentation datasets in various modalities. The generated datasets are used to train an ensemble of 2D segmentation models in point cloud view domain on each modality. During inference, the models process a large amount of views per scene; the resulting logits are back-projected to 3D with a depth-aware voting scheme to generate final point-wise labels. These labels are then used to fine-tune a 3D segmentation model in the target domain. We evaluate our approach Real-to-Real on the nuScenes and SemanticKITTI datasets. We also evaluate it Simulation-to-Real with the SynLidar dataset. Our contributions are a novel method that achieves state-of-the-art results in Real-to-Real Unsupervised Domain Adaptation, and we also demonstrate an application of our method to segment rare classes, for which target 3D annotations are not available, by only using 2D annotations for those classes and leveraging 3D annotations for other classes in a source domain.", "link": "http://arxiv.org/abs/2505.15545v3", "date": "2026-01-22", "relevancy": 2.511, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6344}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.629}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Projection%20for%20Unsupervised%20Domain%20Adaptation%20in%203D%20Semantic%20Segmentation&body=Title%3A%20Multi-View%20Projection%20for%20Unsupervised%20Domain%20Adaptation%20in%203D%20Semantic%20Segmentation%0AAuthor%3A%20Andrew%20Caunes%20and%20Thierry%20Chateau%20and%20Vincent%20Fremont%0AAbstract%3A%203D%20semantic%20segmentation%20plays%20a%20pivotal%20role%20in%20autonomous%20driving%20and%20road%20infrastructure%20analysis%2C%20yet%20state-of-the-art%203D%20models%20are%20prone%20to%20severe%20domain%20shift%20when%20deployed%20across%20different%20datasets.%20In%20this%20paper%2C%20we%20propose%20an%20Unsupervised%20Domain%20Adaptation%20approach%20where%20a%203D%20segmentation%20model%20is%20trained%20on%20the%20target%20dataset%20using%20pseudo-labels%20generated%20by%20a%20novel%20multi-view%20projection%20framework.%20Our%20approach%20first%20aligns%20Lidar%20scans%20into%20coherent%203D%20scenes%20and%20renders%20them%20from%20multiple%20virtual%20camera%20poses%20to%20create%20large-scale%20synthetic%202D%20semantic%20segmentation%20datasets%20in%20various%20modalities.%20The%20generated%20datasets%20are%20used%20to%20train%20an%20ensemble%20of%202D%20segmentation%20models%20in%20point%20cloud%20view%20domain%20on%20each%20modality.%20During%20inference%2C%20the%20models%20process%20a%20large%20amount%20of%20views%20per%20scene%3B%20the%20resulting%20logits%20are%20back-projected%20to%203D%20with%20a%20depth-aware%20voting%20scheme%20to%20generate%20final%20point-wise%20labels.%20These%20labels%20are%20then%20used%20to%20fine-tune%20a%203D%20segmentation%20model%20in%20the%20target%20domain.%20We%20evaluate%20our%20approach%20Real-to-Real%20on%20the%20nuScenes%20and%20SemanticKITTI%20datasets.%20We%20also%20evaluate%20it%20Simulation-to-Real%20with%20the%20SynLidar%20dataset.%20Our%20contributions%20are%20a%20novel%20method%20that%20achieves%20state-of-the-art%20results%20in%20Real-to-Real%20Unsupervised%20Domain%20Adaptation%2C%20and%20we%20also%20demonstrate%20an%20application%20of%20our%20method%20to%20segment%20rare%20classes%2C%20for%20which%20target%203D%20annotations%20are%20not%20available%2C%20by%20only%20using%202D%20annotations%20for%20those%20classes%20and%20leveraging%203D%20annotations%20for%20other%20classes%20in%20a%20source%20domain.%0ALink%3A%20http%3A//arxiv.org/abs/2505.15545v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Projection%2520for%2520Unsupervised%2520Domain%2520Adaptation%2520in%25203D%2520Semantic%2520Segmentation%26entry.906535625%3DAndrew%2520Caunes%2520and%2520Thierry%2520Chateau%2520and%2520Vincent%2520Fremont%26entry.1292438233%3D3D%2520semantic%2520segmentation%2520plays%2520a%2520pivotal%2520role%2520in%2520autonomous%2520driving%2520and%2520road%2520infrastructure%2520analysis%252C%2520yet%2520state-of-the-art%25203D%2520models%2520are%2520prone%2520to%2520severe%2520domain%2520shift%2520when%2520deployed%2520across%2520different%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520Unsupervised%2520Domain%2520Adaptation%2520approach%2520where%2520a%25203D%2520segmentation%2520model%2520is%2520trained%2520on%2520the%2520target%2520dataset%2520using%2520pseudo-labels%2520generated%2520by%2520a%2520novel%2520multi-view%2520projection%2520framework.%2520Our%2520approach%2520first%2520aligns%2520Lidar%2520scans%2520into%2520coherent%25203D%2520scenes%2520and%2520renders%2520them%2520from%2520multiple%2520virtual%2520camera%2520poses%2520to%2520create%2520large-scale%2520synthetic%25202D%2520semantic%2520segmentation%2520datasets%2520in%2520various%2520modalities.%2520The%2520generated%2520datasets%2520are%2520used%2520to%2520train%2520an%2520ensemble%2520of%25202D%2520segmentation%2520models%2520in%2520point%2520cloud%2520view%2520domain%2520on%2520each%2520modality.%2520During%2520inference%252C%2520the%2520models%2520process%2520a%2520large%2520amount%2520of%2520views%2520per%2520scene%253B%2520the%2520resulting%2520logits%2520are%2520back-projected%2520to%25203D%2520with%2520a%2520depth-aware%2520voting%2520scheme%2520to%2520generate%2520final%2520point-wise%2520labels.%2520These%2520labels%2520are%2520then%2520used%2520to%2520fine-tune%2520a%25203D%2520segmentation%2520model%2520in%2520the%2520target%2520domain.%2520We%2520evaluate%2520our%2520approach%2520Real-to-Real%2520on%2520the%2520nuScenes%2520and%2520SemanticKITTI%2520datasets.%2520We%2520also%2520evaluate%2520it%2520Simulation-to-Real%2520with%2520the%2520SynLidar%2520dataset.%2520Our%2520contributions%2520are%2520a%2520novel%2520method%2520that%2520achieves%2520state-of-the-art%2520results%2520in%2520Real-to-Real%2520Unsupervised%2520Domain%2520Adaptation%252C%2520and%2520we%2520also%2520demonstrate%2520an%2520application%2520of%2520our%2520method%2520to%2520segment%2520rare%2520classes%252C%2520for%2520which%2520target%25203D%2520annotations%2520are%2520not%2520available%252C%2520by%2520only%2520using%25202D%2520annotations%2520for%2520those%2520classes%2520and%2520leveraging%25203D%2520annotations%2520for%2520other%2520classes%2520in%2520a%2520source%2520domain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15545v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Projection%20for%20Unsupervised%20Domain%20Adaptation%20in%203D%20Semantic%20Segmentation&entry.906535625=Andrew%20Caunes%20and%20Thierry%20Chateau%20and%20Vincent%20Fremont&entry.1292438233=3D%20semantic%20segmentation%20plays%20a%20pivotal%20role%20in%20autonomous%20driving%20and%20road%20infrastructure%20analysis%2C%20yet%20state-of-the-art%203D%20models%20are%20prone%20to%20severe%20domain%20shift%20when%20deployed%20across%20different%20datasets.%20In%20this%20paper%2C%20we%20propose%20an%20Unsupervised%20Domain%20Adaptation%20approach%20where%20a%203D%20segmentation%20model%20is%20trained%20on%20the%20target%20dataset%20using%20pseudo-labels%20generated%20by%20a%20novel%20multi-view%20projection%20framework.%20Our%20approach%20first%20aligns%20Lidar%20scans%20into%20coherent%203D%20scenes%20and%20renders%20them%20from%20multiple%20virtual%20camera%20poses%20to%20create%20large-scale%20synthetic%202D%20semantic%20segmentation%20datasets%20in%20various%20modalities.%20The%20generated%20datasets%20are%20used%20to%20train%20an%20ensemble%20of%202D%20segmentation%20models%20in%20point%20cloud%20view%20domain%20on%20each%20modality.%20During%20inference%2C%20the%20models%20process%20a%20large%20amount%20of%20views%20per%20scene%3B%20the%20resulting%20logits%20are%20back-projected%20to%203D%20with%20a%20depth-aware%20voting%20scheme%20to%20generate%20final%20point-wise%20labels.%20These%20labels%20are%20then%20used%20to%20fine-tune%20a%203D%20segmentation%20model%20in%20the%20target%20domain.%20We%20evaluate%20our%20approach%20Real-to-Real%20on%20the%20nuScenes%20and%20SemanticKITTI%20datasets.%20We%20also%20evaluate%20it%20Simulation-to-Real%20with%20the%20SynLidar%20dataset.%20Our%20contributions%20are%20a%20novel%20method%20that%20achieves%20state-of-the-art%20results%20in%20Real-to-Real%20Unsupervised%20Domain%20Adaptation%2C%20and%20we%20also%20demonstrate%20an%20application%20of%20our%20method%20to%20segment%20rare%20classes%2C%20for%20which%20target%203D%20annotations%20are%20not%20available%2C%20by%20only%20using%202D%20annotations%20for%20those%20classes%20and%20leveraging%203D%20annotations%20for%20other%20classes%20in%20a%20source%20domain.&entry.1838667208=http%3A//arxiv.org/abs/2505.15545v3&entry.124074799=Read"},
{"title": "GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning", "author": "Zhengqing Yan and Xinyang Liu and Yi Zhang and Fan Guo and ChengXun Jia and Junchen Wan and Yao Liu and Qi Liu and Jihao Huang and Kang Song", "abstract": "Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.", "link": "http://arxiv.org/abs/2601.06795v3", "date": "2026-01-22", "relevancy": 2.5057, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5104}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4977}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GDEPO%3A%20Group%20Dual-dynamic%20and%20Equal-right%20Advantage%20Policy%20Optimization%20with%20Enhanced%20Training%20Data%20Utilization%20for%20Sample-Constrained%20Reinforcement%20Learning&body=Title%3A%20GDEPO%3A%20Group%20Dual-dynamic%20and%20Equal-right%20Advantage%20Policy%20Optimization%20with%20Enhanced%20Training%20Data%20Utilization%20for%20Sample-Constrained%20Reinforcement%20Learning%0AAuthor%3A%20Zhengqing%20Yan%20and%20Xinyang%20Liu%20and%20Yi%20Zhang%20and%20Fan%20Guo%20and%20ChengXun%20Jia%20and%20Junchen%20Wan%20and%20Yao%20Liu%20and%20Qi%20Liu%20and%20Jihao%20Huang%20and%20Kang%20Song%0AAbstract%3A%20Automated%20Theorem%20Proving%20%28ATP%29%20represents%20a%20fundamental%20challenge%20in%20Artificial%20Intelligence%20%28AI%29%2C%20requiring%20the%20construction%20of%20machine-verifiable%20proofs%20in%20formal%20languages%20such%20as%20Lean%20to%20evaluate%20AI%20reasoning%20capabilities.%20Reinforcement%20learning%20%28RL%29%2C%20particularly%20the%20high-performance%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm%2C%20has%20emerged%20as%20a%20mainstream%20approach%20for%20this%20task.%20However%2C%20in%20ATP%20scenarios%2C%20GRPO%20faces%20two%20critical%20issues%3A%20when%20composite%20rewards%20are%20used%2C%20its%20relative%20advantage%20estimation%20may%20conflict%20with%20the%20binary%20feedback%20from%20the%20formal%20verifier%3B%20meanwhile%2C%20its%20static%20sampling%20strategy%20may%20discard%20entire%20batches%20of%20data%20if%20no%20valid%20proof%20is%20found%2C%20resulting%20in%20zero%20contribution%20to%20model%20updates%20and%20significant%20data%20waste.%20To%20address%20these%20limitations%2C%20we%20propose%20Group%20Dual-dynamic%20and%20Equal-right-advantage%20Policy%20Optimization%20%28GDEPO%29%2C%20a%20method%20incorporating%20three%20core%20mechanisms%3A%201%29%20dynamic%20additional%20sampling%2C%20which%20resamples%20invalid%20batches%20until%20a%20valid%20proof%20is%20discovered%3B%202%29%20equal-right%20advantage%2C%20decoupling%20the%20sign%20of%20the%20advantage%20function%20%28based%20on%20correctness%29%20from%20its%20magnitude%20%28modulated%20by%20auxiliary%20rewards%29%20to%20ensure%20stable%20and%20correct%20policy%20updates%3B%20and%203%29%20dynamic%20additional%20iterations%2C%20applying%20extra%20gradient%20steps%20to%20initially%20failed%20but%20eventually%20successful%20samples%20to%20accelerate%20learning%20on%20challenging%20cases.%20Experiments%20conducted%20on%20three%20datasets%20of%20varying%20difficulty%20%28MinF2F-test%2C%20MathOlympiadBench%2C%20PutnamBench%29%20confirm%20the%20effectiveness%20of%20GDEPO%2C%20while%20ablation%20studies%20validate%20the%20necessity%20of%20its%20synergistic%20components.%20The%20proposed%20method%20enhances%20data%20utilization%20and%20optimization%20efficiency%2C%20offering%20a%20novel%20training%20paradigm%20for%20ATP.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06795v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGDEPO%253A%2520Group%2520Dual-dynamic%2520and%2520Equal-right%2520Advantage%2520Policy%2520Optimization%2520with%2520Enhanced%2520Training%2520Data%2520Utilization%2520for%2520Sample-Constrained%2520Reinforcement%2520Learning%26entry.906535625%3DZhengqing%2520Yan%2520and%2520Xinyang%2520Liu%2520and%2520Yi%2520Zhang%2520and%2520Fan%2520Guo%2520and%2520ChengXun%2520Jia%2520and%2520Junchen%2520Wan%2520and%2520Yao%2520Liu%2520and%2520Qi%2520Liu%2520and%2520Jihao%2520Huang%2520and%2520Kang%2520Song%26entry.1292438233%3DAutomated%2520Theorem%2520Proving%2520%2528ATP%2529%2520represents%2520a%2520fundamental%2520challenge%2520in%2520Artificial%2520Intelligence%2520%2528AI%2529%252C%2520requiring%2520the%2520construction%2520of%2520machine-verifiable%2520proofs%2520in%2520formal%2520languages%2520such%2520as%2520Lean%2520to%2520evaluate%2520AI%2520reasoning%2520capabilities.%2520Reinforcement%2520learning%2520%2528RL%2529%252C%2520particularly%2520the%2520high-performance%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520algorithm%252C%2520has%2520emerged%2520as%2520a%2520mainstream%2520approach%2520for%2520this%2520task.%2520However%252C%2520in%2520ATP%2520scenarios%252C%2520GRPO%2520faces%2520two%2520critical%2520issues%253A%2520when%2520composite%2520rewards%2520are%2520used%252C%2520its%2520relative%2520advantage%2520estimation%2520may%2520conflict%2520with%2520the%2520binary%2520feedback%2520from%2520the%2520formal%2520verifier%253B%2520meanwhile%252C%2520its%2520static%2520sampling%2520strategy%2520may%2520discard%2520entire%2520batches%2520of%2520data%2520if%2520no%2520valid%2520proof%2520is%2520found%252C%2520resulting%2520in%2520zero%2520contribution%2520to%2520model%2520updates%2520and%2520significant%2520data%2520waste.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Group%2520Dual-dynamic%2520and%2520Equal-right-advantage%2520Policy%2520Optimization%2520%2528GDEPO%2529%252C%2520a%2520method%2520incorporating%2520three%2520core%2520mechanisms%253A%25201%2529%2520dynamic%2520additional%2520sampling%252C%2520which%2520resamples%2520invalid%2520batches%2520until%2520a%2520valid%2520proof%2520is%2520discovered%253B%25202%2529%2520equal-right%2520advantage%252C%2520decoupling%2520the%2520sign%2520of%2520the%2520advantage%2520function%2520%2528based%2520on%2520correctness%2529%2520from%2520its%2520magnitude%2520%2528modulated%2520by%2520auxiliary%2520rewards%2529%2520to%2520ensure%2520stable%2520and%2520correct%2520policy%2520updates%253B%2520and%25203%2529%2520dynamic%2520additional%2520iterations%252C%2520applying%2520extra%2520gradient%2520steps%2520to%2520initially%2520failed%2520but%2520eventually%2520successful%2520samples%2520to%2520accelerate%2520learning%2520on%2520challenging%2520cases.%2520Experiments%2520conducted%2520on%2520three%2520datasets%2520of%2520varying%2520difficulty%2520%2528MinF2F-test%252C%2520MathOlympiadBench%252C%2520PutnamBench%2529%2520confirm%2520the%2520effectiveness%2520of%2520GDEPO%252C%2520while%2520ablation%2520studies%2520validate%2520the%2520necessity%2520of%2520its%2520synergistic%2520components.%2520The%2520proposed%2520method%2520enhances%2520data%2520utilization%2520and%2520optimization%2520efficiency%252C%2520offering%2520a%2520novel%2520training%2520paradigm%2520for%2520ATP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06795v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GDEPO%3A%20Group%20Dual-dynamic%20and%20Equal-right%20Advantage%20Policy%20Optimization%20with%20Enhanced%20Training%20Data%20Utilization%20for%20Sample-Constrained%20Reinforcement%20Learning&entry.906535625=Zhengqing%20Yan%20and%20Xinyang%20Liu%20and%20Yi%20Zhang%20and%20Fan%20Guo%20and%20ChengXun%20Jia%20and%20Junchen%20Wan%20and%20Yao%20Liu%20and%20Qi%20Liu%20and%20Jihao%20Huang%20and%20Kang%20Song&entry.1292438233=Automated%20Theorem%20Proving%20%28ATP%29%20represents%20a%20fundamental%20challenge%20in%20Artificial%20Intelligence%20%28AI%29%2C%20requiring%20the%20construction%20of%20machine-verifiable%20proofs%20in%20formal%20languages%20such%20as%20Lean%20to%20evaluate%20AI%20reasoning%20capabilities.%20Reinforcement%20learning%20%28RL%29%2C%20particularly%20the%20high-performance%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm%2C%20has%20emerged%20as%20a%20mainstream%20approach%20for%20this%20task.%20However%2C%20in%20ATP%20scenarios%2C%20GRPO%20faces%20two%20critical%20issues%3A%20when%20composite%20rewards%20are%20used%2C%20its%20relative%20advantage%20estimation%20may%20conflict%20with%20the%20binary%20feedback%20from%20the%20formal%20verifier%3B%20meanwhile%2C%20its%20static%20sampling%20strategy%20may%20discard%20entire%20batches%20of%20data%20if%20no%20valid%20proof%20is%20found%2C%20resulting%20in%20zero%20contribution%20to%20model%20updates%20and%20significant%20data%20waste.%20To%20address%20these%20limitations%2C%20we%20propose%20Group%20Dual-dynamic%20and%20Equal-right-advantage%20Policy%20Optimization%20%28GDEPO%29%2C%20a%20method%20incorporating%20three%20core%20mechanisms%3A%201%29%20dynamic%20additional%20sampling%2C%20which%20resamples%20invalid%20batches%20until%20a%20valid%20proof%20is%20discovered%3B%202%29%20equal-right%20advantage%2C%20decoupling%20the%20sign%20of%20the%20advantage%20function%20%28based%20on%20correctness%29%20from%20its%20magnitude%20%28modulated%20by%20auxiliary%20rewards%29%20to%20ensure%20stable%20and%20correct%20policy%20updates%3B%20and%203%29%20dynamic%20additional%20iterations%2C%20applying%20extra%20gradient%20steps%20to%20initially%20failed%20but%20eventually%20successful%20samples%20to%20accelerate%20learning%20on%20challenging%20cases.%20Experiments%20conducted%20on%20three%20datasets%20of%20varying%20difficulty%20%28MinF2F-test%2C%20MathOlympiadBench%2C%20PutnamBench%29%20confirm%20the%20effectiveness%20of%20GDEPO%2C%20while%20ablation%20studies%20validate%20the%20necessity%20of%20its%20synergistic%20components.%20The%20proposed%20method%20enhances%20data%20utilization%20and%20optimization%20efficiency%2C%20offering%20a%20novel%20training%20paradigm%20for%20ATP.&entry.1838667208=http%3A//arxiv.org/abs/2601.06795v3&entry.124074799=Read"},
{"title": "Understanding the Transfer Limits of Vision Foundation Models", "author": "Shiqi Huang and Yipei Wang and Natasha Thorley and Alexander Ng and Shaheer Saeed and Mark Emberton and Shonit Punwani and Veeru Kasivisvanathan and Dean Barratt and Daniel Alexander and Yipeng Hu", "abstract": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.", "link": "http://arxiv.org/abs/2601.15888v1", "date": "2026-01-22", "relevancy": 2.5013, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6421}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Transfer%20Limits%20of%20Vision%20Foundation%20Models&body=Title%3A%20Understanding%20the%20Transfer%20Limits%20of%20Vision%20Foundation%20Models%0AAuthor%3A%20Shiqi%20Huang%20and%20Yipei%20Wang%20and%20Natasha%20Thorley%20and%20Alexander%20Ng%20and%20Shaheer%20Saeed%20and%20Mark%20Emberton%20and%20Shonit%20Punwani%20and%20Veeru%20Kasivisvanathan%20and%20Dean%20Barratt%20and%20Daniel%20Alexander%20and%20Yipeng%20Hu%0AAbstract%3A%20Foundation%20models%20leverage%20large-scale%20pretraining%20to%20capture%20extensive%20knowledge%2C%20demonstrating%20generalization%20in%20a%20wide%20range%20of%20language%20tasks.%20By%20comparison%2C%20vision%20foundation%20models%20%28VFMs%29%20often%20exhibit%20uneven%20improvements%20across%20downstream%20tasks%2C%20despite%20substantial%20computational%20investment.%20We%20postulate%20that%20this%20limitation%20arises%20from%20a%20mismatch%20between%20pretraining%20objectives%20and%20the%20demands%20of%20downstream%20vision-and-imaging%20tasks.%20Pretraining%20strategies%20like%20masked%20image%20reconstruction%20or%20contrastive%20learning%20shape%20representations%20for%20tasks%20such%20as%20recovery%20of%20generic%20visual%20patterns%20or%20global%20semantic%20structures%2C%20which%20may%20not%20align%20with%20the%20task-specific%20requirements%20of%20downstream%20applications%20including%20segmentation%2C%20classification%2C%20or%20image%20synthesis.%20To%20investigate%20this%20in%20a%20concrete%20real-world%20clinical%20area%2C%20we%20assess%20two%20VFMs%2C%20a%20reconstruction-focused%20MAE-based%20model%20%28ProFound%29%20and%20a%20contrastive-learning-based%20model%20%28ProViCNet%29%2C%20on%20five%20prostate%20multiparametric%20MR%20imaging%20tasks%2C%20examining%20how%20such%20task%20alignment%20influences%20transfer%20performance%2C%20i.e.%2C%20from%20pretraining%20to%20fine-tuning.%20Our%20findings%20indicate%20that%20better%20alignment%20between%20pretraining%20and%20downstream%20tasks%2C%20measured%20by%20simple%20divergence%20metrics%20such%20as%20maximum-mean-discrepancy%20%28MMD%29%20between%20the%20same%20features%20before%20and%20after%20fine-tuning%2C%20correlates%20with%20greater%20performance%20improvements%20and%20faster%20convergence%2C%20emphasizing%20the%20importance%20of%20designing%20and%20analyzing%20pretraining%20objectives%20with%20downstream%20applicability%20in%20mind.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520Transfer%2520Limits%2520of%2520Vision%2520Foundation%2520Models%26entry.906535625%3DShiqi%2520Huang%2520and%2520Yipei%2520Wang%2520and%2520Natasha%2520Thorley%2520and%2520Alexander%2520Ng%2520and%2520Shaheer%2520Saeed%2520and%2520Mark%2520Emberton%2520and%2520Shonit%2520Punwani%2520and%2520Veeru%2520Kasivisvanathan%2520and%2520Dean%2520Barratt%2520and%2520Daniel%2520Alexander%2520and%2520Yipeng%2520Hu%26entry.1292438233%3DFoundation%2520models%2520leverage%2520large-scale%2520pretraining%2520to%2520capture%2520extensive%2520knowledge%252C%2520demonstrating%2520generalization%2520in%2520a%2520wide%2520range%2520of%2520language%2520tasks.%2520By%2520comparison%252C%2520vision%2520foundation%2520models%2520%2528VFMs%2529%2520often%2520exhibit%2520uneven%2520improvements%2520across%2520downstream%2520tasks%252C%2520despite%2520substantial%2520computational%2520investment.%2520We%2520postulate%2520that%2520this%2520limitation%2520arises%2520from%2520a%2520mismatch%2520between%2520pretraining%2520objectives%2520and%2520the%2520demands%2520of%2520downstream%2520vision-and-imaging%2520tasks.%2520Pretraining%2520strategies%2520like%2520masked%2520image%2520reconstruction%2520or%2520contrastive%2520learning%2520shape%2520representations%2520for%2520tasks%2520such%2520as%2520recovery%2520of%2520generic%2520visual%2520patterns%2520or%2520global%2520semantic%2520structures%252C%2520which%2520may%2520not%2520align%2520with%2520the%2520task-specific%2520requirements%2520of%2520downstream%2520applications%2520including%2520segmentation%252C%2520classification%252C%2520or%2520image%2520synthesis.%2520To%2520investigate%2520this%2520in%2520a%2520concrete%2520real-world%2520clinical%2520area%252C%2520we%2520assess%2520two%2520VFMs%252C%2520a%2520reconstruction-focused%2520MAE-based%2520model%2520%2528ProFound%2529%2520and%2520a%2520contrastive-learning-based%2520model%2520%2528ProViCNet%2529%252C%2520on%2520five%2520prostate%2520multiparametric%2520MR%2520imaging%2520tasks%252C%2520examining%2520how%2520such%2520task%2520alignment%2520influences%2520transfer%2520performance%252C%2520i.e.%252C%2520from%2520pretraining%2520to%2520fine-tuning.%2520Our%2520findings%2520indicate%2520that%2520better%2520alignment%2520between%2520pretraining%2520and%2520downstream%2520tasks%252C%2520measured%2520by%2520simple%2520divergence%2520metrics%2520such%2520as%2520maximum-mean-discrepancy%2520%2528MMD%2529%2520between%2520the%2520same%2520features%2520before%2520and%2520after%2520fine-tuning%252C%2520correlates%2520with%2520greater%2520performance%2520improvements%2520and%2520faster%2520convergence%252C%2520emphasizing%2520the%2520importance%2520of%2520designing%2520and%2520analyzing%2520pretraining%2520objectives%2520with%2520downstream%2520applicability%2520in%2520mind.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Transfer%20Limits%20of%20Vision%20Foundation%20Models&entry.906535625=Shiqi%20Huang%20and%20Yipei%20Wang%20and%20Natasha%20Thorley%20and%20Alexander%20Ng%20and%20Shaheer%20Saeed%20and%20Mark%20Emberton%20and%20Shonit%20Punwani%20and%20Veeru%20Kasivisvanathan%20and%20Dean%20Barratt%20and%20Daniel%20Alexander%20and%20Yipeng%20Hu&entry.1292438233=Foundation%20models%20leverage%20large-scale%20pretraining%20to%20capture%20extensive%20knowledge%2C%20demonstrating%20generalization%20in%20a%20wide%20range%20of%20language%20tasks.%20By%20comparison%2C%20vision%20foundation%20models%20%28VFMs%29%20often%20exhibit%20uneven%20improvements%20across%20downstream%20tasks%2C%20despite%20substantial%20computational%20investment.%20We%20postulate%20that%20this%20limitation%20arises%20from%20a%20mismatch%20between%20pretraining%20objectives%20and%20the%20demands%20of%20downstream%20vision-and-imaging%20tasks.%20Pretraining%20strategies%20like%20masked%20image%20reconstruction%20or%20contrastive%20learning%20shape%20representations%20for%20tasks%20such%20as%20recovery%20of%20generic%20visual%20patterns%20or%20global%20semantic%20structures%2C%20which%20may%20not%20align%20with%20the%20task-specific%20requirements%20of%20downstream%20applications%20including%20segmentation%2C%20classification%2C%20or%20image%20synthesis.%20To%20investigate%20this%20in%20a%20concrete%20real-world%20clinical%20area%2C%20we%20assess%20two%20VFMs%2C%20a%20reconstruction-focused%20MAE-based%20model%20%28ProFound%29%20and%20a%20contrastive-learning-based%20model%20%28ProViCNet%29%2C%20on%20five%20prostate%20multiparametric%20MR%20imaging%20tasks%2C%20examining%20how%20such%20task%20alignment%20influences%20transfer%20performance%2C%20i.e.%2C%20from%20pretraining%20to%20fine-tuning.%20Our%20findings%20indicate%20that%20better%20alignment%20between%20pretraining%20and%20downstream%20tasks%2C%20measured%20by%20simple%20divergence%20metrics%20such%20as%20maximum-mean-discrepancy%20%28MMD%29%20between%20the%20same%20features%20before%20and%20after%20fine-tuning%2C%20correlates%20with%20greater%20performance%20improvements%20and%20faster%20convergence%2C%20emphasizing%20the%20importance%20of%20designing%20and%20analyzing%20pretraining%20objectives%20with%20downstream%20applicability%20in%20mind.&entry.1838667208=http%3A//arxiv.org/abs/2601.15888v1&entry.124074799=Read"},
{"title": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier", "author": "Haq Nawaz Malik and Kh Mohmad Shafi and Tanveer Ahmad Reshi", "abstract": "Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.\n  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.\n  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.", "link": "http://arxiv.org/abs/2601.16113v1", "date": "2026-01-22", "relevancy": 2.4999, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5209}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5025}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20synthocr-gen%3A%20A%20synthetic%20ocr%20dataset%20generator%20for%20low-resource%20languages-%20breaking%20the%20data%20barrier&body=Title%3A%20synthocr-gen%3A%20A%20synthetic%20ocr%20dataset%20generator%20for%20low-resource%20languages-%20breaking%20the%20data%20barrier%0AAuthor%3A%20Haq%20Nawaz%20Malik%20and%20Kh%20Mohmad%20Shafi%20and%20Tanveer%20Ahmad%20Reshi%0AAbstract%3A%20Optical%20Character%20Recognition%20%28OCR%29%20for%20low-resource%20languages%20remains%20a%20significant%20challenge%20due%20to%20the%20scarcity%20of%20large-scale%20annotated%20training%20datasets.%20Languages%20such%20as%20Kashmiri%2C%20with%20approximately%207%20million%20speakers%20and%20a%20complex%20Perso-Arabic%20script%20featuring%20unique%20diacritical%20marks%2C%20currently%20lack%20support%20in%20major%20OCR%20systems%20including%20Tesseract%2C%20TrOCR%2C%20and%20PaddleOCR.%20Manual%20dataset%20creation%20for%20such%20languages%20is%20prohibitively%20expensive%2C%20time-consuming%2C%20and%20error-prone%2C%20often%20requiring%20word%20by%20word%20transcription%20of%20printed%20or%20handwritten%20text.%0A%20%20We%20present%20SynthOCR-Gen%2C%20an%20open-source%20synthetic%20OCR%20dataset%20generator%20specifically%20designed%20for%20low-resource%20languages.%20Our%20tool%20addresses%20the%20fundamental%20bottleneck%20in%20OCR%20development%20by%20transforming%20digital%20Unicode%20text%20corpora%20into%20ready-to-use%20training%20datasets.%20The%20system%20implements%20a%20comprehensive%20pipeline%20encompassing%20text%20segmentation%20%28character%2C%20word%2C%20n-gram%2C%20sentence%2C%20and%20line%20levels%29%2C%20Unicode%20normalization%20with%20script%20purity%20enforcement%2C%20multi-font%20rendering%20with%20configurable%20distribution%2C%20and%2025%2B%20data%20augmentation%20techniques%20simulating%20real-world%20document%20degradations%20including%20rotation%2C%20blur%2C%20noise%2C%20and%20scanner%20artifacts.%0A%20%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20by%20generating%20a%20600%2C000-sample%20word-segmented%20Kashmiri%20OCR%20dataset%2C%20which%20we%20release%20publicly%20on%20HuggingFace.%20This%20work%20provides%20a%20practical%20pathway%20for%20bringing%20low-resource%20languages%20into%20the%20era%20of%20vision-language%20AI%20models%2C%20and%20the%20tool%20is%20openly%20available%20for%20researchers%20and%20practitioners%20working%20with%20underserved%20writing%20systems%20worldwide.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dsynthocr-gen%253A%2520A%2520synthetic%2520ocr%2520dataset%2520generator%2520for%2520low-resource%2520languages-%2520breaking%2520the%2520data%2520barrier%26entry.906535625%3DHaq%2520Nawaz%2520Malik%2520and%2520Kh%2520Mohmad%2520Shafi%2520and%2520Tanveer%2520Ahmad%2520Reshi%26entry.1292438233%3DOptical%2520Character%2520Recognition%2520%2528OCR%2529%2520for%2520low-resource%2520languages%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520scarcity%2520of%2520large-scale%2520annotated%2520training%2520datasets.%2520Languages%2520such%2520as%2520Kashmiri%252C%2520with%2520approximately%25207%2520million%2520speakers%2520and%2520a%2520complex%2520Perso-Arabic%2520script%2520featuring%2520unique%2520diacritical%2520marks%252C%2520currently%2520lack%2520support%2520in%2520major%2520OCR%2520systems%2520including%2520Tesseract%252C%2520TrOCR%252C%2520and%2520PaddleOCR.%2520Manual%2520dataset%2520creation%2520for%2520such%2520languages%2520is%2520prohibitively%2520expensive%252C%2520time-consuming%252C%2520and%2520error-prone%252C%2520often%2520requiring%2520word%2520by%2520word%2520transcription%2520of%2520printed%2520or%2520handwritten%2520text.%250A%2520%2520We%2520present%2520SynthOCR-Gen%252C%2520an%2520open-source%2520synthetic%2520OCR%2520dataset%2520generator%2520specifically%2520designed%2520for%2520low-resource%2520languages.%2520Our%2520tool%2520addresses%2520the%2520fundamental%2520bottleneck%2520in%2520OCR%2520development%2520by%2520transforming%2520digital%2520Unicode%2520text%2520corpora%2520into%2520ready-to-use%2520training%2520datasets.%2520The%2520system%2520implements%2520a%2520comprehensive%2520pipeline%2520encompassing%2520text%2520segmentation%2520%2528character%252C%2520word%252C%2520n-gram%252C%2520sentence%252C%2520and%2520line%2520levels%2529%252C%2520Unicode%2520normalization%2520with%2520script%2520purity%2520enforcement%252C%2520multi-font%2520rendering%2520with%2520configurable%2520distribution%252C%2520and%252025%252B%2520data%2520augmentation%2520techniques%2520simulating%2520real-world%2520document%2520degradations%2520including%2520rotation%252C%2520blur%252C%2520noise%252C%2520and%2520scanner%2520artifacts.%250A%2520%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520by%2520generating%2520a%2520600%252C000-sample%2520word-segmented%2520Kashmiri%2520OCR%2520dataset%252C%2520which%2520we%2520release%2520publicly%2520on%2520HuggingFace.%2520This%2520work%2520provides%2520a%2520practical%2520pathway%2520for%2520bringing%2520low-resource%2520languages%2520into%2520the%2520era%2520of%2520vision-language%2520AI%2520models%252C%2520and%2520the%2520tool%2520is%2520openly%2520available%2520for%2520researchers%2520and%2520practitioners%2520working%2520with%2520underserved%2520writing%2520systems%2520worldwide.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=synthocr-gen%3A%20A%20synthetic%20ocr%20dataset%20generator%20for%20low-resource%20languages-%20breaking%20the%20data%20barrier&entry.906535625=Haq%20Nawaz%20Malik%20and%20Kh%20Mohmad%20Shafi%20and%20Tanveer%20Ahmad%20Reshi&entry.1292438233=Optical%20Character%20Recognition%20%28OCR%29%20for%20low-resource%20languages%20remains%20a%20significant%20challenge%20due%20to%20the%20scarcity%20of%20large-scale%20annotated%20training%20datasets.%20Languages%20such%20as%20Kashmiri%2C%20with%20approximately%207%20million%20speakers%20and%20a%20complex%20Perso-Arabic%20script%20featuring%20unique%20diacritical%20marks%2C%20currently%20lack%20support%20in%20major%20OCR%20systems%20including%20Tesseract%2C%20TrOCR%2C%20and%20PaddleOCR.%20Manual%20dataset%20creation%20for%20such%20languages%20is%20prohibitively%20expensive%2C%20time-consuming%2C%20and%20error-prone%2C%20often%20requiring%20word%20by%20word%20transcription%20of%20printed%20or%20handwritten%20text.%0A%20%20We%20present%20SynthOCR-Gen%2C%20an%20open-source%20synthetic%20OCR%20dataset%20generator%20specifically%20designed%20for%20low-resource%20languages.%20Our%20tool%20addresses%20the%20fundamental%20bottleneck%20in%20OCR%20development%20by%20transforming%20digital%20Unicode%20text%20corpora%20into%20ready-to-use%20training%20datasets.%20The%20system%20implements%20a%20comprehensive%20pipeline%20encompassing%20text%20segmentation%20%28character%2C%20word%2C%20n-gram%2C%20sentence%2C%20and%20line%20levels%29%2C%20Unicode%20normalization%20with%20script%20purity%20enforcement%2C%20multi-font%20rendering%20with%20configurable%20distribution%2C%20and%2025%2B%20data%20augmentation%20techniques%20simulating%20real-world%20document%20degradations%20including%20rotation%2C%20blur%2C%20noise%2C%20and%20scanner%20artifacts.%0A%20%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20by%20generating%20a%20600%2C000-sample%20word-segmented%20Kashmiri%20OCR%20dataset%2C%20which%20we%20release%20publicly%20on%20HuggingFace.%20This%20work%20provides%20a%20practical%20pathway%20for%20bringing%20low-resource%20languages%20into%20the%20era%20of%20vision-language%20AI%20models%2C%20and%20the%20tool%20is%20openly%20available%20for%20researchers%20and%20practitioners%20working%20with%20underserved%20writing%20systems%20worldwide.&entry.1838667208=http%3A//arxiv.org/abs/2601.16113v1&entry.124074799=Read"},
{"title": "Masked Modeling for Human Motion Recovery Under Occlusions", "author": "Zhiyin Qian and Siwei Zhang and Bharat Lal Bhatnagar and Federica Bogo and Siyu Tang", "abstract": "Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.", "link": "http://arxiv.org/abs/2601.16079v1", "date": "2026-01-22", "relevancy": 2.4994, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6341}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6286}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Modeling%20for%20Human%20Motion%20Recovery%20Under%20Occlusions&body=Title%3A%20Masked%20Modeling%20for%20Human%20Motion%20Recovery%20Under%20Occlusions%0AAuthor%3A%20Zhiyin%20Qian%20and%20Siwei%20Zhang%20and%20Bharat%20Lal%20Bhatnagar%20and%20Federica%20Bogo%20and%20Siyu%20Tang%0AAbstract%3A%20Human%20motion%20reconstruction%20from%20monocular%20videos%20is%20a%20fundamental%20challenge%20in%20computer%20vision%2C%20with%20broad%20applications%20in%20AR/VR%2C%20robotics%2C%20and%20digital%20content%20creation%2C%20but%20remains%20challenging%20under%20frequent%20occlusions%20in%20real-world%20settings.Existing%20regression-based%20methods%20are%20efficient%20but%20fragile%20to%20missing%20observations%2C%20while%20optimization-%20and%20diffusion-based%20approaches%20improve%20robustness%20at%20the%20cost%20of%20slow%20inference%20speed%20and%20heavy%20preprocessing%20steps.%20To%20address%20these%20limitations%2C%20we%20leverage%20recent%20advances%20in%20generative%20masked%20modeling%20and%20present%20MoRo%3A%20Masked%20Modeling%20for%20human%20motion%20Recovery%20under%20Occlusions.%20MoRo%20is%20an%20occlusion-robust%2C%20end-to-end%20generative%20framework%20that%20formulates%20motion%20reconstruction%20as%20a%20video-conditioned%20task%2C%20and%20efficiently%20recover%20human%20motion%20in%20a%20consistent%20global%20coordinate%20system%20from%20RGB%20videos.%20By%20masked%20modeling%2C%20MoRo%20naturally%20handles%20occlusions%20while%20enabling%20efficient%2C%20end-to-end%20inference.%20To%20overcome%20the%20scarcity%20of%20paired%20video-motion%20data%2C%20we%20design%20a%20cross-modality%20learning%20scheme%20that%20learns%20multi-modal%20priors%20from%20a%20set%20of%20heterogeneous%20datasets%3A%20%28i%29%20a%20trajectory-aware%20motion%20prior%20trained%20on%20MoCap%20datasets%2C%20%28ii%29%20an%20image-conditioned%20pose%20prior%20trained%20on%20image-pose%20datasets%2C%20capturing%20diverse%20per-frame%20poses%2C%20and%20%28iii%29%20a%20video-conditioned%20masked%20transformer%20that%20fuses%20motion%20and%20pose%20priors%2C%20finetuned%20on%20video-motion%20datasets%20to%20integrate%20visual%20cues%20with%20motion%20dynamics%20for%20robust%20inference.%20Extensive%20experiments%20on%20EgoBody%20and%20RICH%20demonstrate%20that%20MoRo%20substantially%20outperforms%20state-of-the-art%20methods%20in%20accuracy%20and%20motion%20realism%20under%20occlusions%2C%20while%20performing%20on-par%20in%20non-occluded%20scenarios.%20MoRo%20achieves%20real-time%20inference%20at%2070%20FPS%20on%20a%20single%20H200%20GPU.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Modeling%2520for%2520Human%2520Motion%2520Recovery%2520Under%2520Occlusions%26entry.906535625%3DZhiyin%2520Qian%2520and%2520Siwei%2520Zhang%2520and%2520Bharat%2520Lal%2520Bhatnagar%2520and%2520Federica%2520Bogo%2520and%2520Siyu%2520Tang%26entry.1292438233%3DHuman%2520motion%2520reconstruction%2520from%2520monocular%2520videos%2520is%2520a%2520fundamental%2520challenge%2520in%2520computer%2520vision%252C%2520with%2520broad%2520applications%2520in%2520AR/VR%252C%2520robotics%252C%2520and%2520digital%2520content%2520creation%252C%2520but%2520remains%2520challenging%2520under%2520frequent%2520occlusions%2520in%2520real-world%2520settings.Existing%2520regression-based%2520methods%2520are%2520efficient%2520but%2520fragile%2520to%2520missing%2520observations%252C%2520while%2520optimization-%2520and%2520diffusion-based%2520approaches%2520improve%2520robustness%2520at%2520the%2520cost%2520of%2520slow%2520inference%2520speed%2520and%2520heavy%2520preprocessing%2520steps.%2520To%2520address%2520these%2520limitations%252C%2520we%2520leverage%2520recent%2520advances%2520in%2520generative%2520masked%2520modeling%2520and%2520present%2520MoRo%253A%2520Masked%2520Modeling%2520for%2520human%2520motion%2520Recovery%2520under%2520Occlusions.%2520MoRo%2520is%2520an%2520occlusion-robust%252C%2520end-to-end%2520generative%2520framework%2520that%2520formulates%2520motion%2520reconstruction%2520as%2520a%2520video-conditioned%2520task%252C%2520and%2520efficiently%2520recover%2520human%2520motion%2520in%2520a%2520consistent%2520global%2520coordinate%2520system%2520from%2520RGB%2520videos.%2520By%2520masked%2520modeling%252C%2520MoRo%2520naturally%2520handles%2520occlusions%2520while%2520enabling%2520efficient%252C%2520end-to-end%2520inference.%2520To%2520overcome%2520the%2520scarcity%2520of%2520paired%2520video-motion%2520data%252C%2520we%2520design%2520a%2520cross-modality%2520learning%2520scheme%2520that%2520learns%2520multi-modal%2520priors%2520from%2520a%2520set%2520of%2520heterogeneous%2520datasets%253A%2520%2528i%2529%2520a%2520trajectory-aware%2520motion%2520prior%2520trained%2520on%2520MoCap%2520datasets%252C%2520%2528ii%2529%2520an%2520image-conditioned%2520pose%2520prior%2520trained%2520on%2520image-pose%2520datasets%252C%2520capturing%2520diverse%2520per-frame%2520poses%252C%2520and%2520%2528iii%2529%2520a%2520video-conditioned%2520masked%2520transformer%2520that%2520fuses%2520motion%2520and%2520pose%2520priors%252C%2520finetuned%2520on%2520video-motion%2520datasets%2520to%2520integrate%2520visual%2520cues%2520with%2520motion%2520dynamics%2520for%2520robust%2520inference.%2520Extensive%2520experiments%2520on%2520EgoBody%2520and%2520RICH%2520demonstrate%2520that%2520MoRo%2520substantially%2520outperforms%2520state-of-the-art%2520methods%2520in%2520accuracy%2520and%2520motion%2520realism%2520under%2520occlusions%252C%2520while%2520performing%2520on-par%2520in%2520non-occluded%2520scenarios.%2520MoRo%2520achieves%2520real-time%2520inference%2520at%252070%2520FPS%2520on%2520a%2520single%2520H200%2520GPU.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Modeling%20for%20Human%20Motion%20Recovery%20Under%20Occlusions&entry.906535625=Zhiyin%20Qian%20and%20Siwei%20Zhang%20and%20Bharat%20Lal%20Bhatnagar%20and%20Federica%20Bogo%20and%20Siyu%20Tang&entry.1292438233=Human%20motion%20reconstruction%20from%20monocular%20videos%20is%20a%20fundamental%20challenge%20in%20computer%20vision%2C%20with%20broad%20applications%20in%20AR/VR%2C%20robotics%2C%20and%20digital%20content%20creation%2C%20but%20remains%20challenging%20under%20frequent%20occlusions%20in%20real-world%20settings.Existing%20regression-based%20methods%20are%20efficient%20but%20fragile%20to%20missing%20observations%2C%20while%20optimization-%20and%20diffusion-based%20approaches%20improve%20robustness%20at%20the%20cost%20of%20slow%20inference%20speed%20and%20heavy%20preprocessing%20steps.%20To%20address%20these%20limitations%2C%20we%20leverage%20recent%20advances%20in%20generative%20masked%20modeling%20and%20present%20MoRo%3A%20Masked%20Modeling%20for%20human%20motion%20Recovery%20under%20Occlusions.%20MoRo%20is%20an%20occlusion-robust%2C%20end-to-end%20generative%20framework%20that%20formulates%20motion%20reconstruction%20as%20a%20video-conditioned%20task%2C%20and%20efficiently%20recover%20human%20motion%20in%20a%20consistent%20global%20coordinate%20system%20from%20RGB%20videos.%20By%20masked%20modeling%2C%20MoRo%20naturally%20handles%20occlusions%20while%20enabling%20efficient%2C%20end-to-end%20inference.%20To%20overcome%20the%20scarcity%20of%20paired%20video-motion%20data%2C%20we%20design%20a%20cross-modality%20learning%20scheme%20that%20learns%20multi-modal%20priors%20from%20a%20set%20of%20heterogeneous%20datasets%3A%20%28i%29%20a%20trajectory-aware%20motion%20prior%20trained%20on%20MoCap%20datasets%2C%20%28ii%29%20an%20image-conditioned%20pose%20prior%20trained%20on%20image-pose%20datasets%2C%20capturing%20diverse%20per-frame%20poses%2C%20and%20%28iii%29%20a%20video-conditioned%20masked%20transformer%20that%20fuses%20motion%20and%20pose%20priors%2C%20finetuned%20on%20video-motion%20datasets%20to%20integrate%20visual%20cues%20with%20motion%20dynamics%20for%20robust%20inference.%20Extensive%20experiments%20on%20EgoBody%20and%20RICH%20demonstrate%20that%20MoRo%20substantially%20outperforms%20state-of-the-art%20methods%20in%20accuracy%20and%20motion%20realism%20under%20occlusions%2C%20while%20performing%20on-par%20in%20non-occluded%20scenarios.%20MoRo%20achieves%20real-time%20inference%20at%2070%20FPS%20on%20a%20single%20H200%20GPU.&entry.1838667208=http%3A//arxiv.org/abs/2601.16079v1&entry.124074799=Read"},
{"title": "Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets", "author": "Muhammad Ilham Rizqyawan and Peter Macfarlane and Stathis Hadjidemetriou and Fani Deligianni", "abstract": "Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.", "link": "http://arxiv.org/abs/2601.16147v1", "date": "2026-01-22", "relevancy": 2.468, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5115}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4851}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beat-ssl%3A%20Capturing%20Local%20ECG%20Morphology%20through%20Heartbeat-level%20Contrastive%20Learning%20with%20Soft%20Targets&body=Title%3A%20Beat-ssl%3A%20Capturing%20Local%20ECG%20Morphology%20through%20Heartbeat-level%20Contrastive%20Learning%20with%20Soft%20Targets%0AAuthor%3A%20Muhammad%20Ilham%20Rizqyawan%20and%20Peter%20Macfarlane%20and%20Stathis%20Hadjidemetriou%20and%20Fani%20Deligianni%0AAbstract%3A%20Obtaining%20labelled%20ECG%20data%20for%20developing%20supervised%20models%20is%20challenging.%20Contrastive%20learning%20%28CL%29%20has%20emerged%20as%20a%20promising%20pretraining%20approach%20that%20enables%20effective%20transfer%20learning%20with%20limited%20labelled%20data.%20However%2C%20existing%20CL%20frameworks%20either%20focus%20solely%20on%20global%20context%20or%20fail%20to%20exploit%20ECG-specific%20characteristics.%20Furthermore%2C%20these%20methods%20rely%20on%20hard%20contrastive%20targets%2C%20which%20may%20not%20adequately%20capture%20the%20continuous%20nature%20of%20feature%20similarity%20in%20ECG%20signals.%20In%20this%20paper%2C%20we%20propose%20Beat-SSL%2C%20a%20contrastive%20learning%20framework%20that%20performs%20dual-context%20learning%20through%20both%20rhythm-level%20and%20heartbeat-level%20contrasting%20with%20soft%20targets.%20We%20evaluated%20our%20pretrained%20model%20on%20two%20downstream%20tasks%3A%201%29%20multilabel%20classification%20for%20global%20rhythm%20assessment%2C%20and%202%29%20ECG%20segmentation%20to%20assess%20its%20capacity%20to%20learn%20representations%20across%20both%20contexts.%20We%20conducted%20an%20ablation%20study%20and%20compared%20the%20best%20configuration%20with%20three%20other%20methods%2C%20including%20one%20ECG%20foundation%20model.%20Despite%20the%20foundation%20model%27s%20broader%20pretraining%2C%20Beat-SSL%20reached%2093%25%20of%20its%20performance%20in%20multilabel%20classification%20task%20and%20surpassed%20all%20other%20methods%20in%20the%20segmentation%20task%20by%204%25.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeat-ssl%253A%2520Capturing%2520Local%2520ECG%2520Morphology%2520through%2520Heartbeat-level%2520Contrastive%2520Learning%2520with%2520Soft%2520Targets%26entry.906535625%3DMuhammad%2520Ilham%2520Rizqyawan%2520and%2520Peter%2520Macfarlane%2520and%2520Stathis%2520Hadjidemetriou%2520and%2520Fani%2520Deligianni%26entry.1292438233%3DObtaining%2520labelled%2520ECG%2520data%2520for%2520developing%2520supervised%2520models%2520is%2520challenging.%2520Contrastive%2520learning%2520%2528CL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520pretraining%2520approach%2520that%2520enables%2520effective%2520transfer%2520learning%2520with%2520limited%2520labelled%2520data.%2520However%252C%2520existing%2520CL%2520frameworks%2520either%2520focus%2520solely%2520on%2520global%2520context%2520or%2520fail%2520to%2520exploit%2520ECG-specific%2520characteristics.%2520Furthermore%252C%2520these%2520methods%2520rely%2520on%2520hard%2520contrastive%2520targets%252C%2520which%2520may%2520not%2520adequately%2520capture%2520the%2520continuous%2520nature%2520of%2520feature%2520similarity%2520in%2520ECG%2520signals.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Beat-SSL%252C%2520a%2520contrastive%2520learning%2520framework%2520that%2520performs%2520dual-context%2520learning%2520through%2520both%2520rhythm-level%2520and%2520heartbeat-level%2520contrasting%2520with%2520soft%2520targets.%2520We%2520evaluated%2520our%2520pretrained%2520model%2520on%2520two%2520downstream%2520tasks%253A%25201%2529%2520multilabel%2520classification%2520for%2520global%2520rhythm%2520assessment%252C%2520and%25202%2529%2520ECG%2520segmentation%2520to%2520assess%2520its%2520capacity%2520to%2520learn%2520representations%2520across%2520both%2520contexts.%2520We%2520conducted%2520an%2520ablation%2520study%2520and%2520compared%2520the%2520best%2520configuration%2520with%2520three%2520other%2520methods%252C%2520including%2520one%2520ECG%2520foundation%2520model.%2520Despite%2520the%2520foundation%2520model%2527s%2520broader%2520pretraining%252C%2520Beat-SSL%2520reached%252093%2525%2520of%2520its%2520performance%2520in%2520multilabel%2520classification%2520task%2520and%2520surpassed%2520all%2520other%2520methods%2520in%2520the%2520segmentation%2520task%2520by%25204%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beat-ssl%3A%20Capturing%20Local%20ECG%20Morphology%20through%20Heartbeat-level%20Contrastive%20Learning%20with%20Soft%20Targets&entry.906535625=Muhammad%20Ilham%20Rizqyawan%20and%20Peter%20Macfarlane%20and%20Stathis%20Hadjidemetriou%20and%20Fani%20Deligianni&entry.1292438233=Obtaining%20labelled%20ECG%20data%20for%20developing%20supervised%20models%20is%20challenging.%20Contrastive%20learning%20%28CL%29%20has%20emerged%20as%20a%20promising%20pretraining%20approach%20that%20enables%20effective%20transfer%20learning%20with%20limited%20labelled%20data.%20However%2C%20existing%20CL%20frameworks%20either%20focus%20solely%20on%20global%20context%20or%20fail%20to%20exploit%20ECG-specific%20characteristics.%20Furthermore%2C%20these%20methods%20rely%20on%20hard%20contrastive%20targets%2C%20which%20may%20not%20adequately%20capture%20the%20continuous%20nature%20of%20feature%20similarity%20in%20ECG%20signals.%20In%20this%20paper%2C%20we%20propose%20Beat-SSL%2C%20a%20contrastive%20learning%20framework%20that%20performs%20dual-context%20learning%20through%20both%20rhythm-level%20and%20heartbeat-level%20contrasting%20with%20soft%20targets.%20We%20evaluated%20our%20pretrained%20model%20on%20two%20downstream%20tasks%3A%201%29%20multilabel%20classification%20for%20global%20rhythm%20assessment%2C%20and%202%29%20ECG%20segmentation%20to%20assess%20its%20capacity%20to%20learn%20representations%20across%20both%20contexts.%20We%20conducted%20an%20ablation%20study%20and%20compared%20the%20best%20configuration%20with%20three%20other%20methods%2C%20including%20one%20ECG%20foundation%20model.%20Despite%20the%20foundation%20model%27s%20broader%20pretraining%2C%20Beat-SSL%20reached%2093%25%20of%20its%20performance%20in%20multilabel%20classification%20task%20and%20surpassed%20all%20other%20methods%20in%20the%20segmentation%20task%20by%204%25.&entry.1838667208=http%3A//arxiv.org/abs/2601.16147v1&entry.124074799=Read"},
{"title": "Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets", "author": "Adithya Sineesh and Akshita Kamsali", "abstract": "Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.", "link": "http://arxiv.org/abs/2601.16107v1", "date": "2026-01-22", "relevancy": 2.464, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Deep%20Learning%20Models%20for%20Raman%20Spectroscopy%20Across%20Open-Source%20Datasets&body=Title%3A%20Benchmarking%20Deep%20Learning%20Models%20for%20Raman%20Spectroscopy%20Across%20Open-Source%20Datasets%0AAuthor%3A%20Adithya%20Sineesh%20and%20Akshita%20Kamsali%0AAbstract%3A%20Deep%20learning%20classifiers%20for%20Raman%20spectroscopy%20are%20increasingly%20reported%20to%20outperform%20classical%20chemometric%20approaches.%20However%20their%20evaluations%20are%20often%20conducted%20in%20isolation%20or%20compared%20against%20traditional%20machine%20learning%20methods%20or%20trivially%20adapted%20vision-based%20architectures%20that%20were%20not%20originally%20proposed%20for%20Raman%20spectroscopy.%20As%20a%20result%2C%20direct%20comparisons%20between%20existing%20deep%20learning%20models%20developed%20specifically%20for%20Raman%20spectral%20analysis%20on%20shared%20open-source%20datasets%20remain%20scarce.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20study%20presents%20one%20of%20the%20first%20systematic%20benchmarks%20comparing%20three%20or%20more%20published%20Raman-specific%20deep%20learning%20classifiers%20across%20multiple%20open-source%20Raman%20datasets.%20We%20evaluate%20five%20representative%20deep%20learning%20architectures%20under%20a%20unified%20training%20and%20hyperparameter%20tuning%20protocol%20across%20three%20open-source%20Raman%20datasets%20selected%20to%20support%20standard%20evaluation%2C%20fine-tuning%2C%20and%20explicit%20distribution-shift%20testing.%20We%20report%20classification%20accuracies%20and%20macro-averaged%20F1%20scores%20to%20provide%20a%20fair%20and%20reproducible%20comparison%20of%20deep%20learning%20models%20for%20Raman%20spectra%20based%20classification.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Deep%2520Learning%2520Models%2520for%2520Raman%2520Spectroscopy%2520Across%2520Open-Source%2520Datasets%26entry.906535625%3DAdithya%2520Sineesh%2520and%2520Akshita%2520Kamsali%26entry.1292438233%3DDeep%2520learning%2520classifiers%2520for%2520Raman%2520spectroscopy%2520are%2520increasingly%2520reported%2520to%2520outperform%2520classical%2520chemometric%2520approaches.%2520However%2520their%2520evaluations%2520are%2520often%2520conducted%2520in%2520isolation%2520or%2520compared%2520against%2520traditional%2520machine%2520learning%2520methods%2520or%2520trivially%2520adapted%2520vision-based%2520architectures%2520that%2520were%2520not%2520originally%2520proposed%2520for%2520Raman%2520spectroscopy.%2520As%2520a%2520result%252C%2520direct%2520comparisons%2520between%2520existing%2520deep%2520learning%2520models%2520developed%2520specifically%2520for%2520Raman%2520spectral%2520analysis%2520on%2520shared%2520open-source%2520datasets%2520remain%2520scarce.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520study%2520presents%2520one%2520of%2520the%2520first%2520systematic%2520benchmarks%2520comparing%2520three%2520or%2520more%2520published%2520Raman-specific%2520deep%2520learning%2520classifiers%2520across%2520multiple%2520open-source%2520Raman%2520datasets.%2520We%2520evaluate%2520five%2520representative%2520deep%2520learning%2520architectures%2520under%2520a%2520unified%2520training%2520and%2520hyperparameter%2520tuning%2520protocol%2520across%2520three%2520open-source%2520Raman%2520datasets%2520selected%2520to%2520support%2520standard%2520evaluation%252C%2520fine-tuning%252C%2520and%2520explicit%2520distribution-shift%2520testing.%2520We%2520report%2520classification%2520accuracies%2520and%2520macro-averaged%2520F1%2520scores%2520to%2520provide%2520a%2520fair%2520and%2520reproducible%2520comparison%2520of%2520deep%2520learning%2520models%2520for%2520Raman%2520spectra%2520based%2520classification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Deep%20Learning%20Models%20for%20Raman%20Spectroscopy%20Across%20Open-Source%20Datasets&entry.906535625=Adithya%20Sineesh%20and%20Akshita%20Kamsali&entry.1292438233=Deep%20learning%20classifiers%20for%20Raman%20spectroscopy%20are%20increasingly%20reported%20to%20outperform%20classical%20chemometric%20approaches.%20However%20their%20evaluations%20are%20often%20conducted%20in%20isolation%20or%20compared%20against%20traditional%20machine%20learning%20methods%20or%20trivially%20adapted%20vision-based%20architectures%20that%20were%20not%20originally%20proposed%20for%20Raman%20spectroscopy.%20As%20a%20result%2C%20direct%20comparisons%20between%20existing%20deep%20learning%20models%20developed%20specifically%20for%20Raman%20spectral%20analysis%20on%20shared%20open-source%20datasets%20remain%20scarce.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20study%20presents%20one%20of%20the%20first%20systematic%20benchmarks%20comparing%20three%20or%20more%20published%20Raman-specific%20deep%20learning%20classifiers%20across%20multiple%20open-source%20Raman%20datasets.%20We%20evaluate%20five%20representative%20deep%20learning%20architectures%20under%20a%20unified%20training%20and%20hyperparameter%20tuning%20protocol%20across%20three%20open-source%20Raman%20datasets%20selected%20to%20support%20standard%20evaluation%2C%20fine-tuning%2C%20and%20explicit%20distribution-shift%20testing.%20We%20report%20classification%20accuracies%20and%20macro-averaged%20F1%20scores%20to%20provide%20a%20fair%20and%20reproducible%20comparison%20of%20deep%20learning%20models%20for%20Raman%20spectra%20based%20classification.&entry.1838667208=http%3A//arxiv.org/abs/2601.16107v1&entry.124074799=Read"},
{"title": "Scalable Multi-view Clustering via Explicit Kernel Features Maps", "author": "Chakib Fettal and Lazhar Labiod and Mohamed Nadif", "abstract": "The proliferation of high-dimensional data from sources such as social media, sensor networks, and online platforms has created new challenges for clustering algorithms. Multi-view clustering, which integrates complementary information from multiple data perspectives, has emerged as a powerful solution. However, existing methods often struggle with scalability and efficiency, particularly on large attributed networks. In this work, we address these limitations by leveraging explicit kernel feature maps and a non-iterative optimization strategy, enabling efficient and accurate clustering on datasets with millions of points.", "link": "http://arxiv.org/abs/2402.04794v2", "date": "2026-01-22", "relevancy": 2.457, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5015}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4921}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Multi-view%20Clustering%20via%20Explicit%20Kernel%20Features%20Maps&body=Title%3A%20Scalable%20Multi-view%20Clustering%20via%20Explicit%20Kernel%20Features%20Maps%0AAuthor%3A%20Chakib%20Fettal%20and%20Lazhar%20Labiod%20and%20Mohamed%20Nadif%0AAbstract%3A%20The%20proliferation%20of%20high-dimensional%20data%20from%20sources%20such%20as%20social%20media%2C%20sensor%20networks%2C%20and%20online%20platforms%20has%20created%20new%20challenges%20for%20clustering%20algorithms.%20Multi-view%20clustering%2C%20which%20integrates%20complementary%20information%20from%20multiple%20data%20perspectives%2C%20has%20emerged%20as%20a%20powerful%20solution.%20However%2C%20existing%20methods%20often%20struggle%20with%20scalability%20and%20efficiency%2C%20particularly%20on%20large%20attributed%20networks.%20In%20this%20work%2C%20we%20address%20these%20limitations%20by%20leveraging%20explicit%20kernel%20feature%20maps%20and%20a%20non-iterative%20optimization%20strategy%2C%20enabling%20efficient%20and%20accurate%20clustering%20on%20datasets%20with%20millions%20of%20points.%0ALink%3A%20http%3A//arxiv.org/abs/2402.04794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Multi-view%2520Clustering%2520via%2520Explicit%2520Kernel%2520Features%2520Maps%26entry.906535625%3DChakib%2520Fettal%2520and%2520Lazhar%2520Labiod%2520and%2520Mohamed%2520Nadif%26entry.1292438233%3DThe%2520proliferation%2520of%2520high-dimensional%2520data%2520from%2520sources%2520such%2520as%2520social%2520media%252C%2520sensor%2520networks%252C%2520and%2520online%2520platforms%2520has%2520created%2520new%2520challenges%2520for%2520clustering%2520algorithms.%2520Multi-view%2520clustering%252C%2520which%2520integrates%2520complementary%2520information%2520from%2520multiple%2520data%2520perspectives%252C%2520has%2520emerged%2520as%2520a%2520powerful%2520solution.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520with%2520scalability%2520and%2520efficiency%252C%2520particularly%2520on%2520large%2520attributed%2520networks.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520limitations%2520by%2520leveraging%2520explicit%2520kernel%2520feature%2520maps%2520and%2520a%2520non-iterative%2520optimization%2520strategy%252C%2520enabling%2520efficient%2520and%2520accurate%2520clustering%2520on%2520datasets%2520with%2520millions%2520of%2520points.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Multi-view%20Clustering%20via%20Explicit%20Kernel%20Features%20Maps&entry.906535625=Chakib%20Fettal%20and%20Lazhar%20Labiod%20and%20Mohamed%20Nadif&entry.1292438233=The%20proliferation%20of%20high-dimensional%20data%20from%20sources%20such%20as%20social%20media%2C%20sensor%20networks%2C%20and%20online%20platforms%20has%20created%20new%20challenges%20for%20clustering%20algorithms.%20Multi-view%20clustering%2C%20which%20integrates%20complementary%20information%20from%20multiple%20data%20perspectives%2C%20has%20emerged%20as%20a%20powerful%20solution.%20However%2C%20existing%20methods%20often%20struggle%20with%20scalability%20and%20efficiency%2C%20particularly%20on%20large%20attributed%20networks.%20In%20this%20work%2C%20we%20address%20these%20limitations%20by%20leveraging%20explicit%20kernel%20feature%20maps%20and%20a%20non-iterative%20optimization%20strategy%2C%20enabling%20efficient%20and%20accurate%20clustering%20on%20datasets%20with%20millions%20of%20points.&entry.1838667208=http%3A//arxiv.org/abs/2402.04794v2&entry.124074799=Read"},
{"title": "SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction", "author": "Hanlin Wu and Pengfei Lin and Ehsan Javanmardi and Naren Bao and Bo Qian and Hao Si and Manabu Tsukada", "abstract": "As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\\% gain in efficiency.", "link": "http://arxiv.org/abs/2601.11396v3", "date": "2026-01-22", "relevancy": 2.4355, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6164}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6126}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUG-Occ%3A%20An%20Explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Framework%20for%20Real-Time%203D%20Occupancy%20Prediction&body=Title%3A%20SUG-Occ%3A%20An%20Explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Framework%20for%20Real-Time%203D%20Occupancy%20Prediction%0AAuthor%3A%20Hanlin%20Wu%20and%20Pengfei%20Lin%20and%20Ehsan%20Javanmardi%20and%20Naren%20Bao%20and%20Bo%20Qian%20and%20Hao%20Si%20and%20Manabu%20Tsukada%0AAbstract%3A%20As%20autonomous%20driving%20moves%20toward%20full%20scene%20understanding%2C%203D%20semantic%20occupancy%20prediction%20has%20emerged%20as%20a%20crucial%20perception%20task%2C%20offering%20voxel-level%20semantics%20beyond%20traditional%20detection%20and%20segmentation%20paradigms.%20However%2C%20such%20a%20refined%20representation%20for%20scene%20understanding%20incurs%20prohibitive%20computation%20and%20memory%20overhead%2C%20posing%20a%20major%20barrier%20to%20practical%20real-time%20deployment.%20To%20address%20this%2C%20we%20propose%20SUG-Occ%2C%20an%20explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Enabled%203D%20Occupancy%20Prediction%20Framework%2C%20which%20exploits%20the%20inherent%20sparsity%20of%203D%20scenes%20to%20reduce%20redundant%20computation%20while%20maintaining%20geometric%20and%20semantic%20completeness.%20Specifically%2C%20we%20first%20utilize%20semantic%20and%20uncertainty%20priors%20to%20suppress%20projections%20from%20free%20space%20during%20view%20transformation%20while%20employing%20an%20explicit%20unsigned%20distance%20encoding%20to%20enhance%20geometric%20consistency%2C%20producing%20a%20structurally%20consistent%20sparse%203D%20representation.%20Secondly%2C%20we%20design%20an%20cascade%20sparse%20completion%20module%20via%20hyper%20cross%20sparse%20convolution%20and%20generative%20upsampling%20to%20enable%20efficiently%20coarse-to-fine%20reasoning.%20Finally%2C%20we%20devise%20an%20object%20contextual%20representation%20%28OCR%29%20based%20mask%20decoder%20that%20aggregates%20global%20semantic%20context%20from%20sparse%20features%20and%20refines%20voxel-wise%20predictions%20via%20lightweight%20query-context%20interactions%2C%20avoiding%20expensive%20attention%20operations%20over%20volumetric%20features.%20Extensive%20experiments%20on%20SemanticKITTI%20benchmark%20demonstrate%20that%20the%20proposed%20approach%20outperforms%20the%20baselines%2C%20achieving%20a%207.34/%25%20improvement%20in%20accuracy%20and%20a%2057.8%5C%25%20gain%20in%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11396v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUG-Occ%253A%2520An%2520Explicit%2520Semantics%2520and%2520Uncertainty%2520Guided%2520Sparse%2520Learning%2520Framework%2520for%2520Real-Time%25203D%2520Occupancy%2520Prediction%26entry.906535625%3DHanlin%2520Wu%2520and%2520Pengfei%2520Lin%2520and%2520Ehsan%2520Javanmardi%2520and%2520Naren%2520Bao%2520and%2520Bo%2520Qian%2520and%2520Hao%2520Si%2520and%2520Manabu%2520Tsukada%26entry.1292438233%3DAs%2520autonomous%2520driving%2520moves%2520toward%2520full%2520scene%2520understanding%252C%25203D%2520semantic%2520occupancy%2520prediction%2520has%2520emerged%2520as%2520a%2520crucial%2520perception%2520task%252C%2520offering%2520voxel-level%2520semantics%2520beyond%2520traditional%2520detection%2520and%2520segmentation%2520paradigms.%2520However%252C%2520such%2520a%2520refined%2520representation%2520for%2520scene%2520understanding%2520incurs%2520prohibitive%2520computation%2520and%2520memory%2520overhead%252C%2520posing%2520a%2520major%2520barrier%2520to%2520practical%2520real-time%2520deployment.%2520To%2520address%2520this%252C%2520we%2520propose%2520SUG-Occ%252C%2520an%2520explicit%2520Semantics%2520and%2520Uncertainty%2520Guided%2520Sparse%2520Learning%2520Enabled%25203D%2520Occupancy%2520Prediction%2520Framework%252C%2520which%2520exploits%2520the%2520inherent%2520sparsity%2520of%25203D%2520scenes%2520to%2520reduce%2520redundant%2520computation%2520while%2520maintaining%2520geometric%2520and%2520semantic%2520completeness.%2520Specifically%252C%2520we%2520first%2520utilize%2520semantic%2520and%2520uncertainty%2520priors%2520to%2520suppress%2520projections%2520from%2520free%2520space%2520during%2520view%2520transformation%2520while%2520employing%2520an%2520explicit%2520unsigned%2520distance%2520encoding%2520to%2520enhance%2520geometric%2520consistency%252C%2520producing%2520a%2520structurally%2520consistent%2520sparse%25203D%2520representation.%2520Secondly%252C%2520we%2520design%2520an%2520cascade%2520sparse%2520completion%2520module%2520via%2520hyper%2520cross%2520sparse%2520convolution%2520and%2520generative%2520upsampling%2520to%2520enable%2520efficiently%2520coarse-to-fine%2520reasoning.%2520Finally%252C%2520we%2520devise%2520an%2520object%2520contextual%2520representation%2520%2528OCR%2529%2520based%2520mask%2520decoder%2520that%2520aggregates%2520global%2520semantic%2520context%2520from%2520sparse%2520features%2520and%2520refines%2520voxel-wise%2520predictions%2520via%2520lightweight%2520query-context%2520interactions%252C%2520avoiding%2520expensive%2520attention%2520operations%2520over%2520volumetric%2520features.%2520Extensive%2520experiments%2520on%2520SemanticKITTI%2520benchmark%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520outperforms%2520the%2520baselines%252C%2520achieving%2520a%25207.34/%2525%2520improvement%2520in%2520accuracy%2520and%2520a%252057.8%255C%2525%2520gain%2520in%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11396v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUG-Occ%3A%20An%20Explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Framework%20for%20Real-Time%203D%20Occupancy%20Prediction&entry.906535625=Hanlin%20Wu%20and%20Pengfei%20Lin%20and%20Ehsan%20Javanmardi%20and%20Naren%20Bao%20and%20Bo%20Qian%20and%20Hao%20Si%20and%20Manabu%20Tsukada&entry.1292438233=As%20autonomous%20driving%20moves%20toward%20full%20scene%20understanding%2C%203D%20semantic%20occupancy%20prediction%20has%20emerged%20as%20a%20crucial%20perception%20task%2C%20offering%20voxel-level%20semantics%20beyond%20traditional%20detection%20and%20segmentation%20paradigms.%20However%2C%20such%20a%20refined%20representation%20for%20scene%20understanding%20incurs%20prohibitive%20computation%20and%20memory%20overhead%2C%20posing%20a%20major%20barrier%20to%20practical%20real-time%20deployment.%20To%20address%20this%2C%20we%20propose%20SUG-Occ%2C%20an%20explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Enabled%203D%20Occupancy%20Prediction%20Framework%2C%20which%20exploits%20the%20inherent%20sparsity%20of%203D%20scenes%20to%20reduce%20redundant%20computation%20while%20maintaining%20geometric%20and%20semantic%20completeness.%20Specifically%2C%20we%20first%20utilize%20semantic%20and%20uncertainty%20priors%20to%20suppress%20projections%20from%20free%20space%20during%20view%20transformation%20while%20employing%20an%20explicit%20unsigned%20distance%20encoding%20to%20enhance%20geometric%20consistency%2C%20producing%20a%20structurally%20consistent%20sparse%203D%20representation.%20Secondly%2C%20we%20design%20an%20cascade%20sparse%20completion%20module%20via%20hyper%20cross%20sparse%20convolution%20and%20generative%20upsampling%20to%20enable%20efficiently%20coarse-to-fine%20reasoning.%20Finally%2C%20we%20devise%20an%20object%20contextual%20representation%20%28OCR%29%20based%20mask%20decoder%20that%20aggregates%20global%20semantic%20context%20from%20sparse%20features%20and%20refines%20voxel-wise%20predictions%20via%20lightweight%20query-context%20interactions%2C%20avoiding%20expensive%20attention%20operations%20over%20volumetric%20features.%20Extensive%20experiments%20on%20SemanticKITTI%20benchmark%20demonstrate%20that%20the%20proposed%20approach%20outperforms%20the%20baselines%2C%20achieving%20a%207.34/%25%20improvement%20in%20accuracy%20and%20a%2057.8%5C%25%20gain%20in%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2601.11396v3&entry.124074799=Read"},
{"title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback", "author": "Wenhang Ge and Guibao Shen and Jiawei Feng and Luozhou Wang and Hao Lu and Xingye Tian and Xin Tao and Ying-Cong Chen", "abstract": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.", "link": "http://arxiv.org/abs/2601.16214v1", "date": "2026-01-22", "relevancy": 2.4283, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7048}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5911}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CamPilot%3A%20Improving%20Camera%20Control%20in%20Video%20Diffusion%20Model%20with%20Efficient%20Camera%20Reward%20Feedback&body=Title%3A%20CamPilot%3A%20Improving%20Camera%20Control%20in%20Video%20Diffusion%20Model%20with%20Efficient%20Camera%20Reward%20Feedback%0AAuthor%3A%20Wenhang%20Ge%20and%20Guibao%20Shen%20and%20Jiawei%20Feng%20and%20Luozhou%20Wang%20and%20Hao%20Lu%20and%20Xingye%20Tian%20and%20Xin%20Tao%20and%20Ying-Cong%20Chen%0AAbstract%3A%20Recent%20advances%20in%20camera-controlled%20video%20diffusion%20models%20have%20significantly%20improved%20video-camera%20alignment.%20However%2C%20the%20camera%20controllability%20still%20remains%20limited.%20In%20this%20work%2C%20we%20build%20upon%20Reward%20Feedback%20Learning%20and%20aim%20to%20further%20improve%20camera%20controllability.%20However%2C%20directly%20borrowing%20existing%20ReFL%20approaches%20faces%20several%20challenges.%20First%2C%20current%20reward%20models%20lack%20the%20capacity%20to%20assess%20video-camera%20alignment.%20Second%2C%20decoding%20latent%20into%20RGB%20videos%20for%20reward%20computation%20introduces%20substantial%20computational%20overhead.%20Third%2C%203D%20geometric%20information%20is%20typically%20neglected%20during%20video%20decoding.%20To%20address%20these%20limitations%2C%20we%20introduce%20an%20efficient%20camera-aware%203D%20decoder%20that%20decodes%20video%20latent%20into%203D%20representations%20for%20reward%20quantization.%20Specifically%2C%20video%20latent%20along%20with%20the%20camera%20pose%20are%20decoded%20into%203D%20Gaussians.%20In%20this%20process%2C%20the%20camera%20pose%20not%20only%20acts%20as%20input%2C%20but%20also%20serves%20as%20a%20projection%20parameter.%20Misalignment%20between%20the%20video%20latent%20and%20camera%20pose%20will%20cause%20geometric%20distortions%20in%20the%203D%20structure%2C%20resulting%20in%20blurry%20renderings.%20Based%20on%20this%20property%2C%20we%20explicitly%20optimize%20pixel-level%20consistency%20between%20the%20rendered%20novel%20views%20and%20ground-truth%20ones%20as%20reward.%20To%20accommodate%20the%20stochastic%20nature%2C%20we%20further%20introduce%20a%20visibility%20term%20that%20selectively%20supervises%20only%20deterministic%20regions%20derived%20via%20geometric%20warping.%20Extensive%20experiments%20conducted%20on%20RealEstate10K%20and%20WorldScore%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%20Project%20page%3A%20%5Chref%7Bhttps%3A//a-bigbao.github.io/CamPilot/%7D%7BCamPilot%20Page%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamPilot%253A%2520Improving%2520Camera%2520Control%2520in%2520Video%2520Diffusion%2520Model%2520with%2520Efficient%2520Camera%2520Reward%2520Feedback%26entry.906535625%3DWenhang%2520Ge%2520and%2520Guibao%2520Shen%2520and%2520Jiawei%2520Feng%2520and%2520Luozhou%2520Wang%2520and%2520Hao%2520Lu%2520and%2520Xingye%2520Tian%2520and%2520Xin%2520Tao%2520and%2520Ying-Cong%2520Chen%26entry.1292438233%3DRecent%2520advances%2520in%2520camera-controlled%2520video%2520diffusion%2520models%2520have%2520significantly%2520improved%2520video-camera%2520alignment.%2520However%252C%2520the%2520camera%2520controllability%2520still%2520remains%2520limited.%2520In%2520this%2520work%252C%2520we%2520build%2520upon%2520Reward%2520Feedback%2520Learning%2520and%2520aim%2520to%2520further%2520improve%2520camera%2520controllability.%2520However%252C%2520directly%2520borrowing%2520existing%2520ReFL%2520approaches%2520faces%2520several%2520challenges.%2520First%252C%2520current%2520reward%2520models%2520lack%2520the%2520capacity%2520to%2520assess%2520video-camera%2520alignment.%2520Second%252C%2520decoding%2520latent%2520into%2520RGB%2520videos%2520for%2520reward%2520computation%2520introduces%2520substantial%2520computational%2520overhead.%2520Third%252C%25203D%2520geometric%2520information%2520is%2520typically%2520neglected%2520during%2520video%2520decoding.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520an%2520efficient%2520camera-aware%25203D%2520decoder%2520that%2520decodes%2520video%2520latent%2520into%25203D%2520representations%2520for%2520reward%2520quantization.%2520Specifically%252C%2520video%2520latent%2520along%2520with%2520the%2520camera%2520pose%2520are%2520decoded%2520into%25203D%2520Gaussians.%2520In%2520this%2520process%252C%2520the%2520camera%2520pose%2520not%2520only%2520acts%2520as%2520input%252C%2520but%2520also%2520serves%2520as%2520a%2520projection%2520parameter.%2520Misalignment%2520between%2520the%2520video%2520latent%2520and%2520camera%2520pose%2520will%2520cause%2520geometric%2520distortions%2520in%2520the%25203D%2520structure%252C%2520resulting%2520in%2520blurry%2520renderings.%2520Based%2520on%2520this%2520property%252C%2520we%2520explicitly%2520optimize%2520pixel-level%2520consistency%2520between%2520the%2520rendered%2520novel%2520views%2520and%2520ground-truth%2520ones%2520as%2520reward.%2520To%2520accommodate%2520the%2520stochastic%2520nature%252C%2520we%2520further%2520introduce%2520a%2520visibility%2520term%2520that%2520selectively%2520supervises%2520only%2520deterministic%2520regions%2520derived%2520via%2520geometric%2520warping.%2520Extensive%2520experiments%2520conducted%2520on%2520RealEstate10K%2520and%2520WorldScore%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method.%2520Project%2520page%253A%2520%255Chref%257Bhttps%253A//a-bigbao.github.io/CamPilot/%257D%257BCamPilot%2520Page%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CamPilot%3A%20Improving%20Camera%20Control%20in%20Video%20Diffusion%20Model%20with%20Efficient%20Camera%20Reward%20Feedback&entry.906535625=Wenhang%20Ge%20and%20Guibao%20Shen%20and%20Jiawei%20Feng%20and%20Luozhou%20Wang%20and%20Hao%20Lu%20and%20Xingye%20Tian%20and%20Xin%20Tao%20and%20Ying-Cong%20Chen&entry.1292438233=Recent%20advances%20in%20camera-controlled%20video%20diffusion%20models%20have%20significantly%20improved%20video-camera%20alignment.%20However%2C%20the%20camera%20controllability%20still%20remains%20limited.%20In%20this%20work%2C%20we%20build%20upon%20Reward%20Feedback%20Learning%20and%20aim%20to%20further%20improve%20camera%20controllability.%20However%2C%20directly%20borrowing%20existing%20ReFL%20approaches%20faces%20several%20challenges.%20First%2C%20current%20reward%20models%20lack%20the%20capacity%20to%20assess%20video-camera%20alignment.%20Second%2C%20decoding%20latent%20into%20RGB%20videos%20for%20reward%20computation%20introduces%20substantial%20computational%20overhead.%20Third%2C%203D%20geometric%20information%20is%20typically%20neglected%20during%20video%20decoding.%20To%20address%20these%20limitations%2C%20we%20introduce%20an%20efficient%20camera-aware%203D%20decoder%20that%20decodes%20video%20latent%20into%203D%20representations%20for%20reward%20quantization.%20Specifically%2C%20video%20latent%20along%20with%20the%20camera%20pose%20are%20decoded%20into%203D%20Gaussians.%20In%20this%20process%2C%20the%20camera%20pose%20not%20only%20acts%20as%20input%2C%20but%20also%20serves%20as%20a%20projection%20parameter.%20Misalignment%20between%20the%20video%20latent%20and%20camera%20pose%20will%20cause%20geometric%20distortions%20in%20the%203D%20structure%2C%20resulting%20in%20blurry%20renderings.%20Based%20on%20this%20property%2C%20we%20explicitly%20optimize%20pixel-level%20consistency%20between%20the%20rendered%20novel%20views%20and%20ground-truth%20ones%20as%20reward.%20To%20accommodate%20the%20stochastic%20nature%2C%20we%20further%20introduce%20a%20visibility%20term%20that%20selectively%20supervises%20only%20deterministic%20regions%20derived%20via%20geometric%20warping.%20Extensive%20experiments%20conducted%20on%20RealEstate10K%20and%20WorldScore%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%20Project%20page%3A%20%5Chref%7Bhttps%3A//a-bigbao.github.io/CamPilot/%7D%7BCamPilot%20Page%7D.&entry.1838667208=http%3A//arxiv.org/abs/2601.16214v1&entry.124074799=Read"},
{"title": "Iterative Amortized Hierarchical VAE", "author": "Simon W. Penninga and Ruud J. G. van Sloun", "abstract": "In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.", "link": "http://arxiv.org/abs/2601.15894v1", "date": "2026-01-22", "relevancy": 2.4188, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5442}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4544}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Amortized%20Hierarchical%20VAE&body=Title%3A%20Iterative%20Amortized%20Hierarchical%20VAE%0AAuthor%3A%20Simon%20W.%20Penninga%20and%20Ruud%20J.%20G.%20van%20Sloun%0AAbstract%3A%20In%20this%20paper%20we%20propose%20the%20Iterative%20Amortized%20Hierarchical%20Variational%20Autoencoder%20%28IA-HVAE%29%2C%20which%20expands%20on%20amortized%20inference%20with%20a%20hybrid%20scheme%20containing%20an%20initial%20amortized%20guess%20and%20iterative%20refinement%20with%20decoder%20gradients.%20We%20achieve%20this%20by%20creating%20a%20linearly%20separable%20decoder%20in%20a%20transform%20domain%20%28e.g.%20Fourier%20space%29%2C%20enabling%20real-time%20applications%20with%20very%20high%20model%20depths.%20The%20architectural%20change%20leads%20to%20a%2035x%20speed-up%20for%20iterative%20inference%20with%20respect%20to%20the%20traditional%20HVAE.%20We%20show%20that%20our%20hybrid%20approach%20outperforms%20fully%20amortized%20and%20fully%20iterative%20equivalents%20in%20accuracy%20and%20speed%20respectively.%20Moreover%2C%20the%20IAHVAE%20shows%20improved%20reconstruction%20quality%20over%20a%20vanilla%20HVAE%20in%20inverse%20problems%20such%20as%20deblurring%20and%20denoising.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Amortized%2520Hierarchical%2520VAE%26entry.906535625%3DSimon%2520W.%2520Penninga%2520and%2520Ruud%2520J.%2520G.%2520van%2520Sloun%26entry.1292438233%3DIn%2520this%2520paper%2520we%2520propose%2520the%2520Iterative%2520Amortized%2520Hierarchical%2520Variational%2520Autoencoder%2520%2528IA-HVAE%2529%252C%2520which%2520expands%2520on%2520amortized%2520inference%2520with%2520a%2520hybrid%2520scheme%2520containing%2520an%2520initial%2520amortized%2520guess%2520and%2520iterative%2520refinement%2520with%2520decoder%2520gradients.%2520We%2520achieve%2520this%2520by%2520creating%2520a%2520linearly%2520separable%2520decoder%2520in%2520a%2520transform%2520domain%2520%2528e.g.%2520Fourier%2520space%2529%252C%2520enabling%2520real-time%2520applications%2520with%2520very%2520high%2520model%2520depths.%2520The%2520architectural%2520change%2520leads%2520to%2520a%252035x%2520speed-up%2520for%2520iterative%2520inference%2520with%2520respect%2520to%2520the%2520traditional%2520HVAE.%2520We%2520show%2520that%2520our%2520hybrid%2520approach%2520outperforms%2520fully%2520amortized%2520and%2520fully%2520iterative%2520equivalents%2520in%2520accuracy%2520and%2520speed%2520respectively.%2520Moreover%252C%2520the%2520IAHVAE%2520shows%2520improved%2520reconstruction%2520quality%2520over%2520a%2520vanilla%2520HVAE%2520in%2520inverse%2520problems%2520such%2520as%2520deblurring%2520and%2520denoising.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Amortized%20Hierarchical%20VAE&entry.906535625=Simon%20W.%20Penninga%20and%20Ruud%20J.%20G.%20van%20Sloun&entry.1292438233=In%20this%20paper%20we%20propose%20the%20Iterative%20Amortized%20Hierarchical%20Variational%20Autoencoder%20%28IA-HVAE%29%2C%20which%20expands%20on%20amortized%20inference%20with%20a%20hybrid%20scheme%20containing%20an%20initial%20amortized%20guess%20and%20iterative%20refinement%20with%20decoder%20gradients.%20We%20achieve%20this%20by%20creating%20a%20linearly%20separable%20decoder%20in%20a%20transform%20domain%20%28e.g.%20Fourier%20space%29%2C%20enabling%20real-time%20applications%20with%20very%20high%20model%20depths.%20The%20architectural%20change%20leads%20to%20a%2035x%20speed-up%20for%20iterative%20inference%20with%20respect%20to%20the%20traditional%20HVAE.%20We%20show%20that%20our%20hybrid%20approach%20outperforms%20fully%20amortized%20and%20fully%20iterative%20equivalents%20in%20accuracy%20and%20speed%20respectively.%20Moreover%2C%20the%20IAHVAE%20shows%20improved%20reconstruction%20quality%20over%20a%20vanilla%20HVAE%20in%20inverse%20problems%20such%20as%20deblurring%20and%20denoising.&entry.1838667208=http%3A//arxiv.org/abs/2601.15894v1&entry.124074799=Read"},
{"title": "Keyframe-Based Feed-Forward Visual Odometry", "author": "Weichen Dai and Wenhan Su and Da Kong and Yuhang Ming and Wanzeng Kong", "abstract": "The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.", "link": "http://arxiv.org/abs/2601.16020v1", "date": "2026-01-22", "relevancy": 2.3803, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5955}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keyframe-Based%20Feed-Forward%20Visual%20Odometry&body=Title%3A%20Keyframe-Based%20Feed-Forward%20Visual%20Odometry%0AAuthor%3A%20Weichen%20Dai%20and%20Wenhan%20Su%20and%20Da%20Kong%20and%20Yuhang%20Ming%20and%20Wanzeng%20Kong%0AAbstract%3A%20The%20emergence%20of%20visual%20foundation%20models%20has%20revolutionized%20visual%20odometry~%28VO%29%20and%20SLAM%2C%20enabling%20pose%20estimation%20and%20dense%20reconstruction%20within%20a%20single%20feed-forward%20network.%20However%2C%20unlike%20traditional%20pipelines%20that%20leverage%20keyframe%20methods%20to%20enhance%20efficiency%20and%20accuracy%2C%20current%20foundation%20model%20based%20methods%2C%20such%20as%20VGGT-Long%2C%20typically%20process%20raw%20image%20sequences%20indiscriminately.%20This%20leads%20to%20computational%20redundancy%20and%20degraded%20performance%20caused%20by%20low%20inter-frame%20parallax%2C%20which%20provides%20limited%20contextual%20stereo%20information.%20Integrating%20traditional%20geometric%20heuristics%20into%20these%20methods%20is%20non-trivial%2C%20as%20their%20performance%20depends%20on%20high-dimensional%20latent%20representations%20rather%20than%20explicit%20geometric%20metrics.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20novel%20keyframe-based%20feed-forward%20VO.%20Instead%20of%20relying%20on%20hand-crafted%20rules%2C%20our%20approach%20employs%20reinforcement%20learning%20to%20derive%20an%20adaptive%20keyframe%20policy%20in%20a%20data-driven%20manner%2C%20aligning%20selection%20with%20the%20intrinsic%20characteristics%20of%20the%20underlying%20foundation%20model.%20We%20train%20our%20agent%20on%20TartanAir%20dataset%20and%20conduct%20extensive%20evaluations%20across%20several%20real-world%20datasets.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20method%20achieves%20consistent%20and%20substantial%20improvements%20over%20state-of-the-art%20feed-forward%20VO%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeyframe-Based%2520Feed-Forward%2520Visual%2520Odometry%26entry.906535625%3DWeichen%2520Dai%2520and%2520Wenhan%2520Su%2520and%2520Da%2520Kong%2520and%2520Yuhang%2520Ming%2520and%2520Wanzeng%2520Kong%26entry.1292438233%3DThe%2520emergence%2520of%2520visual%2520foundation%2520models%2520has%2520revolutionized%2520visual%2520odometry~%2528VO%2529%2520and%2520SLAM%252C%2520enabling%2520pose%2520estimation%2520and%2520dense%2520reconstruction%2520within%2520a%2520single%2520feed-forward%2520network.%2520However%252C%2520unlike%2520traditional%2520pipelines%2520that%2520leverage%2520keyframe%2520methods%2520to%2520enhance%2520efficiency%2520and%2520accuracy%252C%2520current%2520foundation%2520model%2520based%2520methods%252C%2520such%2520as%2520VGGT-Long%252C%2520typically%2520process%2520raw%2520image%2520sequences%2520indiscriminately.%2520This%2520leads%2520to%2520computational%2520redundancy%2520and%2520degraded%2520performance%2520caused%2520by%2520low%2520inter-frame%2520parallax%252C%2520which%2520provides%2520limited%2520contextual%2520stereo%2520information.%2520Integrating%2520traditional%2520geometric%2520heuristics%2520into%2520these%2520methods%2520is%2520non-trivial%252C%2520as%2520their%2520performance%2520depends%2520on%2520high-dimensional%2520latent%2520representations%2520rather%2520than%2520explicit%2520geometric%2520metrics.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520keyframe-based%2520feed-forward%2520VO.%2520Instead%2520of%2520relying%2520on%2520hand-crafted%2520rules%252C%2520our%2520approach%2520employs%2520reinforcement%2520learning%2520to%2520derive%2520an%2520adaptive%2520keyframe%2520policy%2520in%2520a%2520data-driven%2520manner%252C%2520aligning%2520selection%2520with%2520the%2520intrinsic%2520characteristics%2520of%2520the%2520underlying%2520foundation%2520model.%2520We%2520train%2520our%2520agent%2520on%2520TartanAir%2520dataset%2520and%2520conduct%2520extensive%2520evaluations%2520across%2520several%2520real-world%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520consistent%2520and%2520substantial%2520improvements%2520over%2520state-of-the-art%2520feed-forward%2520VO%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keyframe-Based%20Feed-Forward%20Visual%20Odometry&entry.906535625=Weichen%20Dai%20and%20Wenhan%20Su%20and%20Da%20Kong%20and%20Yuhang%20Ming%20and%20Wanzeng%20Kong&entry.1292438233=The%20emergence%20of%20visual%20foundation%20models%20has%20revolutionized%20visual%20odometry~%28VO%29%20and%20SLAM%2C%20enabling%20pose%20estimation%20and%20dense%20reconstruction%20within%20a%20single%20feed-forward%20network.%20However%2C%20unlike%20traditional%20pipelines%20that%20leverage%20keyframe%20methods%20to%20enhance%20efficiency%20and%20accuracy%2C%20current%20foundation%20model%20based%20methods%2C%20such%20as%20VGGT-Long%2C%20typically%20process%20raw%20image%20sequences%20indiscriminately.%20This%20leads%20to%20computational%20redundancy%20and%20degraded%20performance%20caused%20by%20low%20inter-frame%20parallax%2C%20which%20provides%20limited%20contextual%20stereo%20information.%20Integrating%20traditional%20geometric%20heuristics%20into%20these%20methods%20is%20non-trivial%2C%20as%20their%20performance%20depends%20on%20high-dimensional%20latent%20representations%20rather%20than%20explicit%20geometric%20metrics.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20novel%20keyframe-based%20feed-forward%20VO.%20Instead%20of%20relying%20on%20hand-crafted%20rules%2C%20our%20approach%20employs%20reinforcement%20learning%20to%20derive%20an%20adaptive%20keyframe%20policy%20in%20a%20data-driven%20manner%2C%20aligning%20selection%20with%20the%20intrinsic%20characteristics%20of%20the%20underlying%20foundation%20model.%20We%20train%20our%20agent%20on%20TartanAir%20dataset%20and%20conduct%20extensive%20evaluations%20across%20several%20real-world%20datasets.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20method%20achieves%20consistent%20and%20substantial%20improvements%20over%20state-of-the-art%20feed-forward%20VO%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.16020v1&entry.124074799=Read"},
{"title": "SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks", "author": "Yilun Zhao and Kaiyan Zhang and Tiansheng Hu and Sihong Wu and Ronan Le Bras and Charles McGrady and Taira Anderson and Jonathan Bragg and Joseph Chee Chang and Jesse Dodge and Matt Latzke and Yixin Liu and Xiangru Tang and Zihang Wang and Chen Zhao and Hannaneh Hajishirzi and Doug Downey and Arman Cohan", "abstract": "We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature-grounded tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 47 foundation models and has collected over 20,000 votes from human researchers across diverse scientific domains. Our analysis of the data collected so far confirms its high quality. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on collected preference data. It measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.", "link": "http://arxiv.org/abs/2507.01001v2", "date": "2026-01-22", "relevancy": 2.3745, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SciArena%3A%20An%20Open%20Evaluation%20Platform%20for%20Non-Verifiable%20Scientific%20Literature-Grounded%20Tasks&body=Title%3A%20SciArena%3A%20An%20Open%20Evaluation%20Platform%20for%20Non-Verifiable%20Scientific%20Literature-Grounded%20Tasks%0AAuthor%3A%20Yilun%20Zhao%20and%20Kaiyan%20Zhang%20and%20Tiansheng%20Hu%20and%20Sihong%20Wu%20and%20Ronan%20Le%20Bras%20and%20Charles%20McGrady%20and%20Taira%20Anderson%20and%20Jonathan%20Bragg%20and%20Joseph%20Chee%20Chang%20and%20Jesse%20Dodge%20and%20Matt%20Latzke%20and%20Yixin%20Liu%20and%20Xiangru%20Tang%20and%20Zihang%20Wang%20and%20Chen%20Zhao%20and%20Hannaneh%20Hajishirzi%20and%20Doug%20Downey%20and%20Arman%20Cohan%0AAbstract%3A%20We%20present%20SciArena%2C%20an%20open%20and%20collaborative%20platform%20for%20evaluating%20foundation%20models%20on%20scientific%20literature-grounded%20tasks.%20Unlike%20traditional%20benchmarks%20for%20scientific%20literature%20understanding%20and%20synthesis%2C%20SciArena%20engages%20the%20research%20community%20directly%2C%20following%20the%20Chatbot%20Arena%20evaluation%20approach%20of%20community%20voting%20on%20model%20comparisons.%20By%20leveraging%20collective%20intelligence%2C%20SciArena%20offers%20a%20community-driven%20evaluation%20of%20model%20performance%20on%20open-ended%20scientific%20tasks%20that%20demand%20literature-grounded%2C%20long-form%20responses.%20The%20platform%20currently%20supports%2047%20foundation%20models%20and%20has%20collected%20over%2020%2C000%20votes%20from%20human%20researchers%20across%20diverse%20scientific%20domains.%20Our%20analysis%20of%20the%20data%20collected%20so%20far%20confirms%20its%20high%20quality.%20We%20discuss%20the%20results%20and%20insights%20based%20on%20the%20model%20ranking%20leaderboard.%20To%20further%20promote%20research%20in%20building%20model-based%20automated%20evaluation%20systems%20for%20literature%20tasks%2C%20we%20release%20SciArena-Eval%2C%20a%20meta-evaluation%20benchmark%20based%20on%20collected%20preference%20data.%20It%20measures%20the%20accuracy%20of%20models%20in%20judging%20answer%20quality%20by%20comparing%20their%20pairwise%20assessments%20with%20human%20votes.%20Our%20experiments%20highlight%20the%20benchmark%27s%20challenges%20and%20emphasize%20the%20need%20for%20more%20reliable%20automated%20evaluation%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2507.01001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSciArena%253A%2520An%2520Open%2520Evaluation%2520Platform%2520for%2520Non-Verifiable%2520Scientific%2520Literature-Grounded%2520Tasks%26entry.906535625%3DYilun%2520Zhao%2520and%2520Kaiyan%2520Zhang%2520and%2520Tiansheng%2520Hu%2520and%2520Sihong%2520Wu%2520and%2520Ronan%2520Le%2520Bras%2520and%2520Charles%2520McGrady%2520and%2520Taira%2520Anderson%2520and%2520Jonathan%2520Bragg%2520and%2520Joseph%2520Chee%2520Chang%2520and%2520Jesse%2520Dodge%2520and%2520Matt%2520Latzke%2520and%2520Yixin%2520Liu%2520and%2520Xiangru%2520Tang%2520and%2520Zihang%2520Wang%2520and%2520Chen%2520Zhao%2520and%2520Hannaneh%2520Hajishirzi%2520and%2520Doug%2520Downey%2520and%2520Arman%2520Cohan%26entry.1292438233%3DWe%2520present%2520SciArena%252C%2520an%2520open%2520and%2520collaborative%2520platform%2520for%2520evaluating%2520foundation%2520models%2520on%2520scientific%2520literature-grounded%2520tasks.%2520Unlike%2520traditional%2520benchmarks%2520for%2520scientific%2520literature%2520understanding%2520and%2520synthesis%252C%2520SciArena%2520engages%2520the%2520research%2520community%2520directly%252C%2520following%2520the%2520Chatbot%2520Arena%2520evaluation%2520approach%2520of%2520community%2520voting%2520on%2520model%2520comparisons.%2520By%2520leveraging%2520collective%2520intelligence%252C%2520SciArena%2520offers%2520a%2520community-driven%2520evaluation%2520of%2520model%2520performance%2520on%2520open-ended%2520scientific%2520tasks%2520that%2520demand%2520literature-grounded%252C%2520long-form%2520responses.%2520The%2520platform%2520currently%2520supports%252047%2520foundation%2520models%2520and%2520has%2520collected%2520over%252020%252C000%2520votes%2520from%2520human%2520researchers%2520across%2520diverse%2520scientific%2520domains.%2520Our%2520analysis%2520of%2520the%2520data%2520collected%2520so%2520far%2520confirms%2520its%2520high%2520quality.%2520We%2520discuss%2520the%2520results%2520and%2520insights%2520based%2520on%2520the%2520model%2520ranking%2520leaderboard.%2520To%2520further%2520promote%2520research%2520in%2520building%2520model-based%2520automated%2520evaluation%2520systems%2520for%2520literature%2520tasks%252C%2520we%2520release%2520SciArena-Eval%252C%2520a%2520meta-evaluation%2520benchmark%2520based%2520on%2520collected%2520preference%2520data.%2520It%2520measures%2520the%2520accuracy%2520of%2520models%2520in%2520judging%2520answer%2520quality%2520by%2520comparing%2520their%2520pairwise%2520assessments%2520with%2520human%2520votes.%2520Our%2520experiments%2520highlight%2520the%2520benchmark%2527s%2520challenges%2520and%2520emphasize%2520the%2520need%2520for%2520more%2520reliable%2520automated%2520evaluation%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SciArena%3A%20An%20Open%20Evaluation%20Platform%20for%20Non-Verifiable%20Scientific%20Literature-Grounded%20Tasks&entry.906535625=Yilun%20Zhao%20and%20Kaiyan%20Zhang%20and%20Tiansheng%20Hu%20and%20Sihong%20Wu%20and%20Ronan%20Le%20Bras%20and%20Charles%20McGrady%20and%20Taira%20Anderson%20and%20Jonathan%20Bragg%20and%20Joseph%20Chee%20Chang%20and%20Jesse%20Dodge%20and%20Matt%20Latzke%20and%20Yixin%20Liu%20and%20Xiangru%20Tang%20and%20Zihang%20Wang%20and%20Chen%20Zhao%20and%20Hannaneh%20Hajishirzi%20and%20Doug%20Downey%20and%20Arman%20Cohan&entry.1292438233=We%20present%20SciArena%2C%20an%20open%20and%20collaborative%20platform%20for%20evaluating%20foundation%20models%20on%20scientific%20literature-grounded%20tasks.%20Unlike%20traditional%20benchmarks%20for%20scientific%20literature%20understanding%20and%20synthesis%2C%20SciArena%20engages%20the%20research%20community%20directly%2C%20following%20the%20Chatbot%20Arena%20evaluation%20approach%20of%20community%20voting%20on%20model%20comparisons.%20By%20leveraging%20collective%20intelligence%2C%20SciArena%20offers%20a%20community-driven%20evaluation%20of%20model%20performance%20on%20open-ended%20scientific%20tasks%20that%20demand%20literature-grounded%2C%20long-form%20responses.%20The%20platform%20currently%20supports%2047%20foundation%20models%20and%20has%20collected%20over%2020%2C000%20votes%20from%20human%20researchers%20across%20diverse%20scientific%20domains.%20Our%20analysis%20of%20the%20data%20collected%20so%20far%20confirms%20its%20high%20quality.%20We%20discuss%20the%20results%20and%20insights%20based%20on%20the%20model%20ranking%20leaderboard.%20To%20further%20promote%20research%20in%20building%20model-based%20automated%20evaluation%20systems%20for%20literature%20tasks%2C%20we%20release%20SciArena-Eval%2C%20a%20meta-evaluation%20benchmark%20based%20on%20collected%20preference%20data.%20It%20measures%20the%20accuracy%20of%20models%20in%20judging%20answer%20quality%20by%20comparing%20their%20pairwise%20assessments%20with%20human%20votes.%20Our%20experiments%20highlight%20the%20benchmark%27s%20challenges%20and%20emphasize%20the%20need%20for%20more%20reliable%20automated%20evaluation%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2507.01001v2&entry.124074799=Read"},
{"title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models", "author": "Chak-Wing Mak and Guanyu Zhu and Boyi Zhang and Hongji Li and Xiaowei Chi and Kevin Zhang and Yichen Wu and Yangfan He and Chun-Kai Fan and Wentao Lu and Kuangzhi Ge and Xinyu Fang and Hongyang He and Kuan Lu and Tianxiang Xu and Li Zhang and Yongxin Ni and Youhua Li and Shanghang Zhang", "abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.", "link": "http://arxiv.org/abs/2601.16007v1", "date": "2026-01-22", "relevancy": 2.3661, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysicsMind%3A%20Sim%20and%20Real%20Mechanics%20Benchmarking%20for%20Physical%20Reasoning%20and%20Prediction%20in%20Foundational%20VLMs%20and%20World%20Models&body=Title%3A%20PhysicsMind%3A%20Sim%20and%20Real%20Mechanics%20Benchmarking%20for%20Physical%20Reasoning%20and%20Prediction%20in%20Foundational%20VLMs%20and%20World%20Models%0AAuthor%3A%20Chak-Wing%20Mak%20and%20Guanyu%20Zhu%20and%20Boyi%20Zhang%20and%20Hongji%20Li%20and%20Xiaowei%20Chi%20and%20Kevin%20Zhang%20and%20Yichen%20Wu%20and%20Yangfan%20He%20and%20Chun-Kai%20Fan%20and%20Wentao%20Lu%20and%20Kuangzhi%20Ge%20and%20Xinyu%20Fang%20and%20Hongyang%20He%20and%20Kuan%20Lu%20and%20Tianxiang%20Xu%20and%20Li%20Zhang%20and%20Yongxin%20Ni%20and%20Youhua%20Li%20and%20Shanghang%20Zhang%0AAbstract%3A%20Modern%20foundational%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20video%20world%20models%20have%20advanced%20significantly%20in%20mathematical%2C%20common-sense%2C%20and%20visual%20reasoning%2C%20but%20their%20grasp%20of%20the%20underlying%20physics%20remains%20underexplored.%20Existing%20benchmarks%20attempting%20to%20measure%20this%20matter%20rely%20on%20synthetic%2C%20Visual%20Question%20Answer%20templates%20or%20focus%20on%20perceptual%20video%20quality%20that%20is%20tangential%20to%20measuring%20how%20well%20the%20video%20abides%20by%20physical%20laws.%20To%20address%20this%20fragmentation%2C%20we%20introduce%20PhysicsMind%2C%20a%20unified%20benchmark%20with%20both%20real%20and%20simulation%20environments%20that%20evaluates%20law-consistent%20reasoning%20and%20generation%20over%20three%20canonical%20principles%3A%20Center%20of%20Mass%2C%20Lever%20Equilibrium%2C%20and%20Newton%27s%20First%20Law.%20PhysicsMind%20comprises%20two%20main%20tasks%3A%20i%29%20VQA%20tasks%2C%20testing%20whether%20models%20can%20reason%20and%20determine%20physical%20quantities%20and%20values%20from%20images%20or%20short%20videos%2C%20and%20ii%29%20Video%20Generation%28VG%29%20tasks%2C%20evaluating%20if%20predicted%20motion%20trajectories%20obey%20the%20same%20center-of-mass%2C%20torque%2C%20and%20inertial%20constraints%20as%20the%20ground%20truth.%20A%20broad%20range%20of%20recent%20models%20and%20video%20generation%20models%20is%20evaluated%20on%20PhysicsMind%20and%20found%20to%20rely%20on%20appearance%20heuristics%20while%20often%20violating%20basic%20mechanics.%20These%20gaps%20indicate%20that%20current%20scaling%20and%20training%20are%20still%20insufficient%20for%20robust%20physical%20understanding%2C%20underscoring%20PhysicsMind%20as%20a%20focused%20testbed%20for%20physics-aware%20multimodal%20models.%20Our%20data%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysicsMind%253A%2520Sim%2520and%2520Real%2520Mechanics%2520Benchmarking%2520for%2520Physical%2520Reasoning%2520and%2520Prediction%2520in%2520Foundational%2520VLMs%2520and%2520World%2520Models%26entry.906535625%3DChak-Wing%2520Mak%2520and%2520Guanyu%2520Zhu%2520and%2520Boyi%2520Zhang%2520and%2520Hongji%2520Li%2520and%2520Xiaowei%2520Chi%2520and%2520Kevin%2520Zhang%2520and%2520Yichen%2520Wu%2520and%2520Yangfan%2520He%2520and%2520Chun-Kai%2520Fan%2520and%2520Wentao%2520Lu%2520and%2520Kuangzhi%2520Ge%2520and%2520Xinyu%2520Fang%2520and%2520Hongyang%2520He%2520and%2520Kuan%2520Lu%2520and%2520Tianxiang%2520Xu%2520and%2520Li%2520Zhang%2520and%2520Yongxin%2520Ni%2520and%2520Youhua%2520Li%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3DModern%2520foundational%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520and%2520video%2520world%2520models%2520have%2520advanced%2520significantly%2520in%2520mathematical%252C%2520common-sense%252C%2520and%2520visual%2520reasoning%252C%2520but%2520their%2520grasp%2520of%2520the%2520underlying%2520physics%2520remains%2520underexplored.%2520Existing%2520benchmarks%2520attempting%2520to%2520measure%2520this%2520matter%2520rely%2520on%2520synthetic%252C%2520Visual%2520Question%2520Answer%2520templates%2520or%2520focus%2520on%2520perceptual%2520video%2520quality%2520that%2520is%2520tangential%2520to%2520measuring%2520how%2520well%2520the%2520video%2520abides%2520by%2520physical%2520laws.%2520To%2520address%2520this%2520fragmentation%252C%2520we%2520introduce%2520PhysicsMind%252C%2520a%2520unified%2520benchmark%2520with%2520both%2520real%2520and%2520simulation%2520environments%2520that%2520evaluates%2520law-consistent%2520reasoning%2520and%2520generation%2520over%2520three%2520canonical%2520principles%253A%2520Center%2520of%2520Mass%252C%2520Lever%2520Equilibrium%252C%2520and%2520Newton%2527s%2520First%2520Law.%2520PhysicsMind%2520comprises%2520two%2520main%2520tasks%253A%2520i%2529%2520VQA%2520tasks%252C%2520testing%2520whether%2520models%2520can%2520reason%2520and%2520determine%2520physical%2520quantities%2520and%2520values%2520from%2520images%2520or%2520short%2520videos%252C%2520and%2520ii%2529%2520Video%2520Generation%2528VG%2529%2520tasks%252C%2520evaluating%2520if%2520predicted%2520motion%2520trajectories%2520obey%2520the%2520same%2520center-of-mass%252C%2520torque%252C%2520and%2520inertial%2520constraints%2520as%2520the%2520ground%2520truth.%2520A%2520broad%2520range%2520of%2520recent%2520models%2520and%2520video%2520generation%2520models%2520is%2520evaluated%2520on%2520PhysicsMind%2520and%2520found%2520to%2520rely%2520on%2520appearance%2520heuristics%2520while%2520often%2520violating%2520basic%2520mechanics.%2520These%2520gaps%2520indicate%2520that%2520current%2520scaling%2520and%2520training%2520are%2520still%2520insufficient%2520for%2520robust%2520physical%2520understanding%252C%2520underscoring%2520PhysicsMind%2520as%2520a%2520focused%2520testbed%2520for%2520physics-aware%2520multimodal%2520models.%2520Our%2520data%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysicsMind%3A%20Sim%20and%20Real%20Mechanics%20Benchmarking%20for%20Physical%20Reasoning%20and%20Prediction%20in%20Foundational%20VLMs%20and%20World%20Models&entry.906535625=Chak-Wing%20Mak%20and%20Guanyu%20Zhu%20and%20Boyi%20Zhang%20and%20Hongji%20Li%20and%20Xiaowei%20Chi%20and%20Kevin%20Zhang%20and%20Yichen%20Wu%20and%20Yangfan%20He%20and%20Chun-Kai%20Fan%20and%20Wentao%20Lu%20and%20Kuangzhi%20Ge%20and%20Xinyu%20Fang%20and%20Hongyang%20He%20and%20Kuan%20Lu%20and%20Tianxiang%20Xu%20and%20Li%20Zhang%20and%20Yongxin%20Ni%20and%20Youhua%20Li%20and%20Shanghang%20Zhang&entry.1292438233=Modern%20foundational%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20video%20world%20models%20have%20advanced%20significantly%20in%20mathematical%2C%20common-sense%2C%20and%20visual%20reasoning%2C%20but%20their%20grasp%20of%20the%20underlying%20physics%20remains%20underexplored.%20Existing%20benchmarks%20attempting%20to%20measure%20this%20matter%20rely%20on%20synthetic%2C%20Visual%20Question%20Answer%20templates%20or%20focus%20on%20perceptual%20video%20quality%20that%20is%20tangential%20to%20measuring%20how%20well%20the%20video%20abides%20by%20physical%20laws.%20To%20address%20this%20fragmentation%2C%20we%20introduce%20PhysicsMind%2C%20a%20unified%20benchmark%20with%20both%20real%20and%20simulation%20environments%20that%20evaluates%20law-consistent%20reasoning%20and%20generation%20over%20three%20canonical%20principles%3A%20Center%20of%20Mass%2C%20Lever%20Equilibrium%2C%20and%20Newton%27s%20First%20Law.%20PhysicsMind%20comprises%20two%20main%20tasks%3A%20i%29%20VQA%20tasks%2C%20testing%20whether%20models%20can%20reason%20and%20determine%20physical%20quantities%20and%20values%20from%20images%20or%20short%20videos%2C%20and%20ii%29%20Video%20Generation%28VG%29%20tasks%2C%20evaluating%20if%20predicted%20motion%20trajectories%20obey%20the%20same%20center-of-mass%2C%20torque%2C%20and%20inertial%20constraints%20as%20the%20ground%20truth.%20A%20broad%20range%20of%20recent%20models%20and%20video%20generation%20models%20is%20evaluated%20on%20PhysicsMind%20and%20found%20to%20rely%20on%20appearance%20heuristics%20while%20often%20violating%20basic%20mechanics.%20These%20gaps%20indicate%20that%20current%20scaling%20and%20training%20are%20still%20insufficient%20for%20robust%20physical%20understanding%2C%20underscoring%20PhysicsMind%20as%20a%20focused%20testbed%20for%20physics-aware%20multimodal%20models.%20Our%20data%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2601.16007v1&entry.124074799=Read"},
{"title": "Partially Lazy Gradient Descent for Smoothed Online Learning", "author": "Naram Mhaisen and George Iosifidis", "abstract": "We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\\mathcal{O}(\\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $\u0398(\\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.", "link": "http://arxiv.org/abs/2601.15984v1", "date": "2026-01-22", "relevancy": 2.3539, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5123}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4521}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partially%20Lazy%20Gradient%20Descent%20for%20Smoothed%20Online%20Learning&body=Title%3A%20Partially%20Lazy%20Gradient%20Descent%20for%20Smoothed%20Online%20Learning%0AAuthor%3A%20Naram%20Mhaisen%20and%20George%20Iosifidis%0AAbstract%3A%20We%20introduce%20%24k%24-lazyGD%2C%20an%20online%20learning%20algorithm%20that%20bridges%20the%20gap%20between%20greedy%20Online%20Gradient%20Descent%20%28OGD%2C%20for%20%24k%3D1%24%29%20and%20lazy%20GD/dual-averaging%20%28for%20%24k%3DT%24%29%2C%20creating%20a%20spectrum%20between%20reactive%20and%20stable%20updates.%20We%20analyze%20this%20spectrum%20in%20Smoothed%20Online%20Convex%20Optimization%20%28SOCO%29%2C%20where%20the%20learner%20incurs%20both%20hitting%20and%20movement%20costs.%20Our%20main%20contribution%20is%20establishing%20that%20laziness%20is%20possible%20without%20sacrificing%20hitting%20performance%3A%20we%20prove%20that%20%24k%24-lazyGD%20achieves%20the%20optimal%20dynamic%20regret%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7B%28P_T%2B1%29T%7D%29%24%20for%20any%20laziness%20slack%20%24k%24%20up%20to%20%24%CE%98%28%5Csqrt%7BT/P_T%7D%29%24%2C%20where%20%24P_T%24%20is%20the%20comparator%20path%20length.%20This%20result%20formally%20connects%20the%20allowable%20laziness%20to%20the%20comparator%27s%20shifts%2C%20showing%20that%20%24k%24-lazyGD%20can%20retain%20the%20inherently%20small%20movements%20of%20lazy%20methods%20without%20compromising%20tracking%20ability.%20We%20base%20our%20analysis%20on%20the%20Follow%20the%20Regularized%20Leader%20%28FTRL%29%20framework%2C%20and%20derive%20a%20matching%20lower%20bound.%20Since%20the%20slack%20depends%20on%20%24P_T%24%2C%20an%20ensemble%20of%20learners%20with%20various%20slacks%20is%20used%2C%20yielding%20a%20method%20that%20is%20provably%20stable%20when%20it%20can%20be%2C%20and%20agile%20when%20it%20must%20be.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartially%2520Lazy%2520Gradient%2520Descent%2520for%2520Smoothed%2520Online%2520Learning%26entry.906535625%3DNaram%2520Mhaisen%2520and%2520George%2520Iosifidis%26entry.1292438233%3DWe%2520introduce%2520%2524k%2524-lazyGD%252C%2520an%2520online%2520learning%2520algorithm%2520that%2520bridges%2520the%2520gap%2520between%2520greedy%2520Online%2520Gradient%2520Descent%2520%2528OGD%252C%2520for%2520%2524k%253D1%2524%2529%2520and%2520lazy%2520GD/dual-averaging%2520%2528for%2520%2524k%253DT%2524%2529%252C%2520creating%2520a%2520spectrum%2520between%2520reactive%2520and%2520stable%2520updates.%2520We%2520analyze%2520this%2520spectrum%2520in%2520Smoothed%2520Online%2520Convex%2520Optimization%2520%2528SOCO%2529%252C%2520where%2520the%2520learner%2520incurs%2520both%2520hitting%2520and%2520movement%2520costs.%2520Our%2520main%2520contribution%2520is%2520establishing%2520that%2520laziness%2520is%2520possible%2520without%2520sacrificing%2520hitting%2520performance%253A%2520we%2520prove%2520that%2520%2524k%2524-lazyGD%2520achieves%2520the%2520optimal%2520dynamic%2520regret%2520%2524%255Cmathcal%257BO%257D%2528%255Csqrt%257B%2528P_T%252B1%2529T%257D%2529%2524%2520for%2520any%2520laziness%2520slack%2520%2524k%2524%2520up%2520to%2520%2524%25CE%2598%2528%255Csqrt%257BT/P_T%257D%2529%2524%252C%2520where%2520%2524P_T%2524%2520is%2520the%2520comparator%2520path%2520length.%2520This%2520result%2520formally%2520connects%2520the%2520allowable%2520laziness%2520to%2520the%2520comparator%2527s%2520shifts%252C%2520showing%2520that%2520%2524k%2524-lazyGD%2520can%2520retain%2520the%2520inherently%2520small%2520movements%2520of%2520lazy%2520methods%2520without%2520compromising%2520tracking%2520ability.%2520We%2520base%2520our%2520analysis%2520on%2520the%2520Follow%2520the%2520Regularized%2520Leader%2520%2528FTRL%2529%2520framework%252C%2520and%2520derive%2520a%2520matching%2520lower%2520bound.%2520Since%2520the%2520slack%2520depends%2520on%2520%2524P_T%2524%252C%2520an%2520ensemble%2520of%2520learners%2520with%2520various%2520slacks%2520is%2520used%252C%2520yielding%2520a%2520method%2520that%2520is%2520provably%2520stable%2520when%2520it%2520can%2520be%252C%2520and%2520agile%2520when%2520it%2520must%2520be.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partially%20Lazy%20Gradient%20Descent%20for%20Smoothed%20Online%20Learning&entry.906535625=Naram%20Mhaisen%20and%20George%20Iosifidis&entry.1292438233=We%20introduce%20%24k%24-lazyGD%2C%20an%20online%20learning%20algorithm%20that%20bridges%20the%20gap%20between%20greedy%20Online%20Gradient%20Descent%20%28OGD%2C%20for%20%24k%3D1%24%29%20and%20lazy%20GD/dual-averaging%20%28for%20%24k%3DT%24%29%2C%20creating%20a%20spectrum%20between%20reactive%20and%20stable%20updates.%20We%20analyze%20this%20spectrum%20in%20Smoothed%20Online%20Convex%20Optimization%20%28SOCO%29%2C%20where%20the%20learner%20incurs%20both%20hitting%20and%20movement%20costs.%20Our%20main%20contribution%20is%20establishing%20that%20laziness%20is%20possible%20without%20sacrificing%20hitting%20performance%3A%20we%20prove%20that%20%24k%24-lazyGD%20achieves%20the%20optimal%20dynamic%20regret%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7B%28P_T%2B1%29T%7D%29%24%20for%20any%20laziness%20slack%20%24k%24%20up%20to%20%24%CE%98%28%5Csqrt%7BT/P_T%7D%29%24%2C%20where%20%24P_T%24%20is%20the%20comparator%20path%20length.%20This%20result%20formally%20connects%20the%20allowable%20laziness%20to%20the%20comparator%27s%20shifts%2C%20showing%20that%20%24k%24-lazyGD%20can%20retain%20the%20inherently%20small%20movements%20of%20lazy%20methods%20without%20compromising%20tracking%20ability.%20We%20base%20our%20analysis%20on%20the%20Follow%20the%20Regularized%20Leader%20%28FTRL%29%20framework%2C%20and%20derive%20a%20matching%20lower%20bound.%20Since%20the%20slack%20depends%20on%20%24P_T%24%2C%20an%20ensemble%20of%20learners%20with%20various%20slacks%20is%20used%2C%20yielding%20a%20method%20that%20is%20provably%20stable%20when%20it%20can%20be%2C%20and%20agile%20when%20it%20must%20be.&entry.1838667208=http%3A//arxiv.org/abs/2601.15984v1&entry.124074799=Read"},
{"title": "CropCraft: Complete Structural Characterization of Crop Plants From Images", "author": "Albert J. Zhai and Xinlei Wang and Kaiyuan Li and Zhao Jiang and Junxiong Zhou and Sheng Wang and Zhenong Jin and Kaiyu Guan and Shenlong Wang", "abstract": "The ability to automatically build 3D digital twins of plants from images has countless applications in agriculture, environmental science, robotics, and other fields. However, current 3D reconstruction methods fail to recover complete shapes of plants due to heavy occlusion and complex geometries. In this work, we present a novel method for 3D modeling of agricultural crops based on optimizing a parametric model of plant morphology via inverse procedural modeling. Our method first estimates depth maps by fitting a neural radiance field and then optimizes a specialized loss to estimate morphological parameters that result in consistent depth renderings. The resulting 3D model is complete and biologically plausible. We validate our method on a dataset of real images of agricultural fields, and demonstrate that the reconstructed canopies can be used for a variety of monitoring and simulation applications.", "link": "http://arxiv.org/abs/2411.09693v2", "date": "2026-01-22", "relevancy": 2.336, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5907}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5907}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CropCraft%3A%20Complete%20Structural%20Characterization%20of%20Crop%20Plants%20From%20Images&body=Title%3A%20CropCraft%3A%20Complete%20Structural%20Characterization%20of%20Crop%20Plants%20From%20Images%0AAuthor%3A%20Albert%20J.%20Zhai%20and%20Xinlei%20Wang%20and%20Kaiyuan%20Li%20and%20Zhao%20Jiang%20and%20Junxiong%20Zhou%20and%20Sheng%20Wang%20and%20Zhenong%20Jin%20and%20Kaiyu%20Guan%20and%20Shenlong%20Wang%0AAbstract%3A%20The%20ability%20to%20automatically%20build%203D%20digital%20twins%20of%20plants%20from%20images%20has%20countless%20applications%20in%20agriculture%2C%20environmental%20science%2C%20robotics%2C%20and%20other%20fields.%20However%2C%20current%203D%20reconstruction%20methods%20fail%20to%20recover%20complete%20shapes%20of%20plants%20due%20to%20heavy%20occlusion%20and%20complex%20geometries.%20In%20this%20work%2C%20we%20present%20a%20novel%20method%20for%203D%20modeling%20of%20agricultural%20crops%20based%20on%20optimizing%20a%20parametric%20model%20of%20plant%20morphology%20via%20inverse%20procedural%20modeling.%20Our%20method%20first%20estimates%20depth%20maps%20by%20fitting%20a%20neural%20radiance%20field%20and%20then%20optimizes%20a%20specialized%20loss%20to%20estimate%20morphological%20parameters%20that%20result%20in%20consistent%20depth%20renderings.%20The%20resulting%203D%20model%20is%20complete%20and%20biologically%20plausible.%20We%20validate%20our%20method%20on%20a%20dataset%20of%20real%20images%20of%20agricultural%20fields%2C%20and%20demonstrate%20that%20the%20reconstructed%20canopies%20can%20be%20used%20for%20a%20variety%20of%20monitoring%20and%20simulation%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2411.09693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCropCraft%253A%2520Complete%2520Structural%2520Characterization%2520of%2520Crop%2520Plants%2520From%2520Images%26entry.906535625%3DAlbert%2520J.%2520Zhai%2520and%2520Xinlei%2520Wang%2520and%2520Kaiyuan%2520Li%2520and%2520Zhao%2520Jiang%2520and%2520Junxiong%2520Zhou%2520and%2520Sheng%2520Wang%2520and%2520Zhenong%2520Jin%2520and%2520Kaiyu%2520Guan%2520and%2520Shenlong%2520Wang%26entry.1292438233%3DThe%2520ability%2520to%2520automatically%2520build%25203D%2520digital%2520twins%2520of%2520plants%2520from%2520images%2520has%2520countless%2520applications%2520in%2520agriculture%252C%2520environmental%2520science%252C%2520robotics%252C%2520and%2520other%2520fields.%2520However%252C%2520current%25203D%2520reconstruction%2520methods%2520fail%2520to%2520recover%2520complete%2520shapes%2520of%2520plants%2520due%2520to%2520heavy%2520occlusion%2520and%2520complex%2520geometries.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520method%2520for%25203D%2520modeling%2520of%2520agricultural%2520crops%2520based%2520on%2520optimizing%2520a%2520parametric%2520model%2520of%2520plant%2520morphology%2520via%2520inverse%2520procedural%2520modeling.%2520Our%2520method%2520first%2520estimates%2520depth%2520maps%2520by%2520fitting%2520a%2520neural%2520radiance%2520field%2520and%2520then%2520optimizes%2520a%2520specialized%2520loss%2520to%2520estimate%2520morphological%2520parameters%2520that%2520result%2520in%2520consistent%2520depth%2520renderings.%2520The%2520resulting%25203D%2520model%2520is%2520complete%2520and%2520biologically%2520plausible.%2520We%2520validate%2520our%2520method%2520on%2520a%2520dataset%2520of%2520real%2520images%2520of%2520agricultural%2520fields%252C%2520and%2520demonstrate%2520that%2520the%2520reconstructed%2520canopies%2520can%2520be%2520used%2520for%2520a%2520variety%2520of%2520monitoring%2520and%2520simulation%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CropCraft%3A%20Complete%20Structural%20Characterization%20of%20Crop%20Plants%20From%20Images&entry.906535625=Albert%20J.%20Zhai%20and%20Xinlei%20Wang%20and%20Kaiyuan%20Li%20and%20Zhao%20Jiang%20and%20Junxiong%20Zhou%20and%20Sheng%20Wang%20and%20Zhenong%20Jin%20and%20Kaiyu%20Guan%20and%20Shenlong%20Wang&entry.1292438233=The%20ability%20to%20automatically%20build%203D%20digital%20twins%20of%20plants%20from%20images%20has%20countless%20applications%20in%20agriculture%2C%20environmental%20science%2C%20robotics%2C%20and%20other%20fields.%20However%2C%20current%203D%20reconstruction%20methods%20fail%20to%20recover%20complete%20shapes%20of%20plants%20due%20to%20heavy%20occlusion%20and%20complex%20geometries.%20In%20this%20work%2C%20we%20present%20a%20novel%20method%20for%203D%20modeling%20of%20agricultural%20crops%20based%20on%20optimizing%20a%20parametric%20model%20of%20plant%20morphology%20via%20inverse%20procedural%20modeling.%20Our%20method%20first%20estimates%20depth%20maps%20by%20fitting%20a%20neural%20radiance%20field%20and%20then%20optimizes%20a%20specialized%20loss%20to%20estimate%20morphological%20parameters%20that%20result%20in%20consistent%20depth%20renderings.%20The%20resulting%203D%20model%20is%20complete%20and%20biologically%20plausible.%20We%20validate%20our%20method%20on%20a%20dataset%20of%20real%20images%20of%20agricultural%20fields%2C%20and%20demonstrate%20that%20the%20reconstructed%20canopies%20can%20be%20used%20for%20a%20variety%20of%20monitoring%20and%20simulation%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2411.09693v2&entry.124074799=Read"},
{"title": "HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval", "author": "Zequn Xie and Xin Liu and Boyun Zhang and Yuxiao Lin and Sihang Cai and Tao Jin", "abstract": "The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from \"blind\" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2601.16155v1", "date": "2026-01-22", "relevancy": 2.3344, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5903}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HVD%3A%20Human%20Vision-Driven%20Video%20Representation%20Learning%20for%20Text-Video%20Retrieval&body=Title%3A%20HVD%3A%20Human%20Vision-Driven%20Video%20Representation%20Learning%20for%20Text-Video%20Retrieval%0AAuthor%3A%20Zequn%20Xie%20and%20Xin%20Liu%20and%20Boyun%20Zhang%20and%20Yuxiao%20Lin%20and%20Sihang%20Cai%20and%20Tao%20Jin%0AAbstract%3A%20The%20success%20of%20CLIP%20has%20driven%20substantial%20progress%20in%20text-video%20retrieval.%20However%2C%20current%20methods%20often%20suffer%20from%20%22blind%22%20feature%20interaction%2C%20where%20the%20model%20struggles%20to%20discern%20key%20visual%20information%20from%20background%20noise%20due%20to%20the%20sparsity%20of%20textual%20queries.%20To%20bridge%20this%20gap%2C%20we%20draw%20inspiration%20from%20human%20cognitive%20behavior%20and%20propose%20the%20Human%20Vision-Driven%20%28HVD%29%20model.%20Our%20framework%20establishes%20a%20coarse-to-fine%20alignment%20mechanism%20comprising%20two%20key%20components%3A%20the%20Frame%20Features%20Selection%20Module%20%28FFSM%29%20and%20the%20Patch%20Features%20Compression%20Module%20%28PFCM%29.%20FFSM%20mimics%20the%20human%20macro-perception%20ability%20by%20selecting%20key%20frames%20to%20eliminate%20temporal%20redundancy.%20Subsequently%2C%20PFCM%20simulates%20micro-perception%20by%20aggregating%20patch%20features%20into%20salient%20visual%20entities%20through%20an%20advanced%20attention%20mechanism%2C%20enabling%20precise%20entity-level%20matching.%20Extensive%20experiments%20on%20five%20benchmarks%20demonstrate%20that%20HVD%20not%20only%20captures%20human-like%20visual%20focus%20but%20also%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHVD%253A%2520Human%2520Vision-Driven%2520Video%2520Representation%2520Learning%2520for%2520Text-Video%2520Retrieval%26entry.906535625%3DZequn%2520Xie%2520and%2520Xin%2520Liu%2520and%2520Boyun%2520Zhang%2520and%2520Yuxiao%2520Lin%2520and%2520Sihang%2520Cai%2520and%2520Tao%2520Jin%26entry.1292438233%3DThe%2520success%2520of%2520CLIP%2520has%2520driven%2520substantial%2520progress%2520in%2520text-video%2520retrieval.%2520However%252C%2520current%2520methods%2520often%2520suffer%2520from%2520%2522blind%2522%2520feature%2520interaction%252C%2520where%2520the%2520model%2520struggles%2520to%2520discern%2520key%2520visual%2520information%2520from%2520background%2520noise%2520due%2520to%2520the%2520sparsity%2520of%2520textual%2520queries.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520draw%2520inspiration%2520from%2520human%2520cognitive%2520behavior%2520and%2520propose%2520the%2520Human%2520Vision-Driven%2520%2528HVD%2529%2520model.%2520Our%2520framework%2520establishes%2520a%2520coarse-to-fine%2520alignment%2520mechanism%2520comprising%2520two%2520key%2520components%253A%2520the%2520Frame%2520Features%2520Selection%2520Module%2520%2528FFSM%2529%2520and%2520the%2520Patch%2520Features%2520Compression%2520Module%2520%2528PFCM%2529.%2520FFSM%2520mimics%2520the%2520human%2520macro-perception%2520ability%2520by%2520selecting%2520key%2520frames%2520to%2520eliminate%2520temporal%2520redundancy.%2520Subsequently%252C%2520PFCM%2520simulates%2520micro-perception%2520by%2520aggregating%2520patch%2520features%2520into%2520salient%2520visual%2520entities%2520through%2520an%2520advanced%2520attention%2520mechanism%252C%2520enabling%2520precise%2520entity-level%2520matching.%2520Extensive%2520experiments%2520on%2520five%2520benchmarks%2520demonstrate%2520that%2520HVD%2520not%2520only%2520captures%2520human-like%2520visual%2520focus%2520but%2520also%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HVD%3A%20Human%20Vision-Driven%20Video%20Representation%20Learning%20for%20Text-Video%20Retrieval&entry.906535625=Zequn%20Xie%20and%20Xin%20Liu%20and%20Boyun%20Zhang%20and%20Yuxiao%20Lin%20and%20Sihang%20Cai%20and%20Tao%20Jin&entry.1292438233=The%20success%20of%20CLIP%20has%20driven%20substantial%20progress%20in%20text-video%20retrieval.%20However%2C%20current%20methods%20often%20suffer%20from%20%22blind%22%20feature%20interaction%2C%20where%20the%20model%20struggles%20to%20discern%20key%20visual%20information%20from%20background%20noise%20due%20to%20the%20sparsity%20of%20textual%20queries.%20To%20bridge%20this%20gap%2C%20we%20draw%20inspiration%20from%20human%20cognitive%20behavior%20and%20propose%20the%20Human%20Vision-Driven%20%28HVD%29%20model.%20Our%20framework%20establishes%20a%20coarse-to-fine%20alignment%20mechanism%20comprising%20two%20key%20components%3A%20the%20Frame%20Features%20Selection%20Module%20%28FFSM%29%20and%20the%20Patch%20Features%20Compression%20Module%20%28PFCM%29.%20FFSM%20mimics%20the%20human%20macro-perception%20ability%20by%20selecting%20key%20frames%20to%20eliminate%20temporal%20redundancy.%20Subsequently%2C%20PFCM%20simulates%20micro-perception%20by%20aggregating%20patch%20features%20into%20salient%20visual%20entities%20through%20an%20advanced%20attention%20mechanism%2C%20enabling%20precise%20entity-level%20matching.%20Extensive%20experiments%20on%20five%20benchmarks%20demonstrate%20that%20HVD%20not%20only%20captures%20human-like%20visual%20focus%20but%20also%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.16155v1&entry.124074799=Read"},
{"title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour", "author": "Liang Wang and Kanzhong Yao and Yang Liu and Weikai Qin and Jun Wu and Zhe Sun and Qiuguo Zhu", "abstract": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.", "link": "http://arxiv.org/abs/2601.15995v1", "date": "2026-01-22", "relevancy": 2.3268, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6273}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5903}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PUMA%3A%20Perception-driven%20Unified%20Foothold%20Prior%20for%20Mobility%20Augmented%20Quadruped%20Parkour&body=Title%3A%20PUMA%3A%20Perception-driven%20Unified%20Foothold%20Prior%20for%20Mobility%20Augmented%20Quadruped%20Parkour%0AAuthor%3A%20Liang%20Wang%20and%20Kanzhong%20Yao%20and%20Yang%20Liu%20and%20Weikai%20Qin%20and%20Jun%20Wu%20and%20Zhe%20Sun%20and%20Qiuguo%20Zhu%0AAbstract%3A%20Parkour%20tasks%20for%20quadrupeds%20have%20emerged%20as%20a%20promising%20benchmark%20for%20agile%20locomotion.%20While%20human%20athletes%20can%20effectively%20perceive%20environmental%20characteristics%20to%20select%20appropriate%20footholds%20for%20obstacle%20traversal%2C%20endowing%20legged%20robots%20with%20similar%20perceptual%20reasoning%20remains%20a%20significant%20challenge.%20Existing%20methods%20often%20rely%20on%20hierarchical%20controllers%20that%20follow%20pre-computed%20footholds%2C%20thereby%20constraining%20the%20robot%27s%20real-time%20adaptability%20and%20the%20exploratory%20potential%20of%20reinforcement%20learning.%20To%20overcome%20these%20challenges%2C%20we%20present%20PUMA%2C%20an%20end-to-end%20learning%20framework%20that%20integrates%20visual%20perception%20and%20foothold%20priors%20into%20a%20single-stage%20training%20process.%20This%20approach%20leverages%20terrain%20features%20to%20estimate%20egocentric%20polar%20foothold%20priors%2C%20composed%20of%20relative%20distance%20and%20heading%2C%20guiding%20the%20robot%20in%20active%20posture%20adaptation%20for%20parkour%20tasks.%20Extensive%20experiments%20conducted%20in%20simulation%20and%20real-world%20environments%20across%20various%20discrete%20complex%20terrains%2C%20demonstrate%20PUMA%27s%20exceptional%20agility%20and%20robustness%20in%20challenging%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPUMA%253A%2520Perception-driven%2520Unified%2520Foothold%2520Prior%2520for%2520Mobility%2520Augmented%2520Quadruped%2520Parkour%26entry.906535625%3DLiang%2520Wang%2520and%2520Kanzhong%2520Yao%2520and%2520Yang%2520Liu%2520and%2520Weikai%2520Qin%2520and%2520Jun%2520Wu%2520and%2520Zhe%2520Sun%2520and%2520Qiuguo%2520Zhu%26entry.1292438233%3DParkour%2520tasks%2520for%2520quadrupeds%2520have%2520emerged%2520as%2520a%2520promising%2520benchmark%2520for%2520agile%2520locomotion.%2520While%2520human%2520athletes%2520can%2520effectively%2520perceive%2520environmental%2520characteristics%2520to%2520select%2520appropriate%2520footholds%2520for%2520obstacle%2520traversal%252C%2520endowing%2520legged%2520robots%2520with%2520similar%2520perceptual%2520reasoning%2520remains%2520a%2520significant%2520challenge.%2520Existing%2520methods%2520often%2520rely%2520on%2520hierarchical%2520controllers%2520that%2520follow%2520pre-computed%2520footholds%252C%2520thereby%2520constraining%2520the%2520robot%2527s%2520real-time%2520adaptability%2520and%2520the%2520exploratory%2520potential%2520of%2520reinforcement%2520learning.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520present%2520PUMA%252C%2520an%2520end-to-end%2520learning%2520framework%2520that%2520integrates%2520visual%2520perception%2520and%2520foothold%2520priors%2520into%2520a%2520single-stage%2520training%2520process.%2520This%2520approach%2520leverages%2520terrain%2520features%2520to%2520estimate%2520egocentric%2520polar%2520foothold%2520priors%252C%2520composed%2520of%2520relative%2520distance%2520and%2520heading%252C%2520guiding%2520the%2520robot%2520in%2520active%2520posture%2520adaptation%2520for%2520parkour%2520tasks.%2520Extensive%2520experiments%2520conducted%2520in%2520simulation%2520and%2520real-world%2520environments%2520across%2520various%2520discrete%2520complex%2520terrains%252C%2520demonstrate%2520PUMA%2527s%2520exceptional%2520agility%2520and%2520robustness%2520in%2520challenging%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PUMA%3A%20Perception-driven%20Unified%20Foothold%20Prior%20for%20Mobility%20Augmented%20Quadruped%20Parkour&entry.906535625=Liang%20Wang%20and%20Kanzhong%20Yao%20and%20Yang%20Liu%20and%20Weikai%20Qin%20and%20Jun%20Wu%20and%20Zhe%20Sun%20and%20Qiuguo%20Zhu&entry.1292438233=Parkour%20tasks%20for%20quadrupeds%20have%20emerged%20as%20a%20promising%20benchmark%20for%20agile%20locomotion.%20While%20human%20athletes%20can%20effectively%20perceive%20environmental%20characteristics%20to%20select%20appropriate%20footholds%20for%20obstacle%20traversal%2C%20endowing%20legged%20robots%20with%20similar%20perceptual%20reasoning%20remains%20a%20significant%20challenge.%20Existing%20methods%20often%20rely%20on%20hierarchical%20controllers%20that%20follow%20pre-computed%20footholds%2C%20thereby%20constraining%20the%20robot%27s%20real-time%20adaptability%20and%20the%20exploratory%20potential%20of%20reinforcement%20learning.%20To%20overcome%20these%20challenges%2C%20we%20present%20PUMA%2C%20an%20end-to-end%20learning%20framework%20that%20integrates%20visual%20perception%20and%20foothold%20priors%20into%20a%20single-stage%20training%20process.%20This%20approach%20leverages%20terrain%20features%20to%20estimate%20egocentric%20polar%20foothold%20priors%2C%20composed%20of%20relative%20distance%20and%20heading%2C%20guiding%20the%20robot%20in%20active%20posture%20adaptation%20for%20parkour%20tasks.%20Extensive%20experiments%20conducted%20in%20simulation%20and%20real-world%20environments%20across%20various%20discrete%20complex%20terrains%2C%20demonstrate%20PUMA%27s%20exceptional%20agility%20and%20robustness%20in%20challenging%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2601.15995v1&entry.124074799=Read"},
{"title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning", "author": "Minh Hieu Ha and Khanh Ly Ta and Hung Phan and Tung Doan and Tung Dao and Dao Tran and Huynh Thi Thanh Binh", "abstract": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.\n  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.", "link": "http://arxiv.org/abs/2601.01910v2", "date": "2026-01-22", "relevancy": 2.2906, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5879}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.584}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMP-A%2A%3A%20Multimodal%20Perception%20Enhanced%20Incremental%20Heuristic%20Search%20on%20Path%20Planning&body=Title%3A%20MMP-A%2A%3A%20Multimodal%20Perception%20Enhanced%20Incremental%20Heuristic%20Search%20on%20Path%20Planning%0AAuthor%3A%20Minh%20Hieu%20Ha%20and%20Khanh%20Ly%20Ta%20and%20Hung%20Phan%20and%20Tung%20Doan%20and%20Tung%20Dao%20and%20Dao%20Tran%20and%20Huynh%20Thi%20Thanh%20Binh%0AAbstract%3A%20Autonomous%20path%20planning%20requires%20a%20synergy%20between%20global%20reasoning%20and%20geometric%20precision%2C%20especially%20in%20complex%20or%20cluttered%20environments.%20While%20classical%20A%2A%20is%20valued%20for%20its%20optimality%2C%20it%20incurs%20prohibitive%20computational%20and%20memory%20costs%20in%20large-scale%20scenarios.%20Recent%20attempts%20to%20mitigate%20these%20limitations%20by%20using%20Large%20Language%20Models%20for%20waypoint%20guidance%20remain%20insufficient%2C%20as%20they%20rely%20only%20on%20text-based%20reasoning%20without%20spatial%20grounding.%20As%20a%20result%2C%20such%20models%20often%20produce%20incorrect%20waypoints%20in%20topologically%20complex%20environments%20with%20dead%20ends%2C%20and%20lack%20the%20perceptual%20capacity%20to%20interpret%20ambiguous%20physical%20boundaries.%20These%20inconsistencies%20lead%20to%20costly%20corrective%20expansions%20and%20undermine%20the%20intended%20computational%20efficiency.%0A%20%20We%20introduce%20MMP-A%2A%2C%20a%20multimodal%20framework%20that%20integrates%20the%20spatial%20grounding%20capabilities%20of%20vision-language%20models%20with%20a%20novel%20adaptive%20decay%20mechanism.%20By%20anchoring%20high-level%20reasoning%20in%20physical%20geometry%2C%20the%20framework%20produces%20coherent%20waypoint%20guidance%20that%20addresses%20the%20limitations%20of%20text-only%20planners.%20The%20adaptive%20decay%20mechanism%20dynamically%20regulates%20the%20influence%20of%20uncertain%20waypoints%20within%20the%20heuristic%2C%20ensuring%20geometric%20validity%20while%20substantially%20reducing%20memory%20overhead.%20To%20evaluate%20robustness%2C%20we%20test%20the%20framework%20in%20challenging%20environments%20characterized%20by%20severe%20clutter%20and%20topological%20complexity.%20Experimental%20results%20show%20that%20MMP-A%2A%20achieves%20near-optimal%20trajectories%20with%20significantly%20reduced%20operational%20costs%2C%20demonstrating%20its%20potential%20as%20a%20perception-grounded%20and%20computationally%20efficient%20paradigm%20for%20autonomous%20navigation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMP-A%252A%253A%2520Multimodal%2520Perception%2520Enhanced%2520Incremental%2520Heuristic%2520Search%2520on%2520Path%2520Planning%26entry.906535625%3DMinh%2520Hieu%2520Ha%2520and%2520Khanh%2520Ly%2520Ta%2520and%2520Hung%2520Phan%2520and%2520Tung%2520Doan%2520and%2520Tung%2520Dao%2520and%2520Dao%2520Tran%2520and%2520Huynh%2520Thi%2520Thanh%2520Binh%26entry.1292438233%3DAutonomous%2520path%2520planning%2520requires%2520a%2520synergy%2520between%2520global%2520reasoning%2520and%2520geometric%2520precision%252C%2520especially%2520in%2520complex%2520or%2520cluttered%2520environments.%2520While%2520classical%2520A%252A%2520is%2520valued%2520for%2520its%2520optimality%252C%2520it%2520incurs%2520prohibitive%2520computational%2520and%2520memory%2520costs%2520in%2520large-scale%2520scenarios.%2520Recent%2520attempts%2520to%2520mitigate%2520these%2520limitations%2520by%2520using%2520Large%2520Language%2520Models%2520for%2520waypoint%2520guidance%2520remain%2520insufficient%252C%2520as%2520they%2520rely%2520only%2520on%2520text-based%2520reasoning%2520without%2520spatial%2520grounding.%2520As%2520a%2520result%252C%2520such%2520models%2520often%2520produce%2520incorrect%2520waypoints%2520in%2520topologically%2520complex%2520environments%2520with%2520dead%2520ends%252C%2520and%2520lack%2520the%2520perceptual%2520capacity%2520to%2520interpret%2520ambiguous%2520physical%2520boundaries.%2520These%2520inconsistencies%2520lead%2520to%2520costly%2520corrective%2520expansions%2520and%2520undermine%2520the%2520intended%2520computational%2520efficiency.%250A%2520%2520We%2520introduce%2520MMP-A%252A%252C%2520a%2520multimodal%2520framework%2520that%2520integrates%2520the%2520spatial%2520grounding%2520capabilities%2520of%2520vision-language%2520models%2520with%2520a%2520novel%2520adaptive%2520decay%2520mechanism.%2520By%2520anchoring%2520high-level%2520reasoning%2520in%2520physical%2520geometry%252C%2520the%2520framework%2520produces%2520coherent%2520waypoint%2520guidance%2520that%2520addresses%2520the%2520limitations%2520of%2520text-only%2520planners.%2520The%2520adaptive%2520decay%2520mechanism%2520dynamically%2520regulates%2520the%2520influence%2520of%2520uncertain%2520waypoints%2520within%2520the%2520heuristic%252C%2520ensuring%2520geometric%2520validity%2520while%2520substantially%2520reducing%2520memory%2520overhead.%2520To%2520evaluate%2520robustness%252C%2520we%2520test%2520the%2520framework%2520in%2520challenging%2520environments%2520characterized%2520by%2520severe%2520clutter%2520and%2520topological%2520complexity.%2520Experimental%2520results%2520show%2520that%2520MMP-A%252A%2520achieves%2520near-optimal%2520trajectories%2520with%2520significantly%2520reduced%2520operational%2520costs%252C%2520demonstrating%2520its%2520potential%2520as%2520a%2520perception-grounded%2520and%2520computationally%2520efficient%2520paradigm%2520for%2520autonomous%2520navigation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMP-A%2A%3A%20Multimodal%20Perception%20Enhanced%20Incremental%20Heuristic%20Search%20on%20Path%20Planning&entry.906535625=Minh%20Hieu%20Ha%20and%20Khanh%20Ly%20Ta%20and%20Hung%20Phan%20and%20Tung%20Doan%20and%20Tung%20Dao%20and%20Dao%20Tran%20and%20Huynh%20Thi%20Thanh%20Binh&entry.1292438233=Autonomous%20path%20planning%20requires%20a%20synergy%20between%20global%20reasoning%20and%20geometric%20precision%2C%20especially%20in%20complex%20or%20cluttered%20environments.%20While%20classical%20A%2A%20is%20valued%20for%20its%20optimality%2C%20it%20incurs%20prohibitive%20computational%20and%20memory%20costs%20in%20large-scale%20scenarios.%20Recent%20attempts%20to%20mitigate%20these%20limitations%20by%20using%20Large%20Language%20Models%20for%20waypoint%20guidance%20remain%20insufficient%2C%20as%20they%20rely%20only%20on%20text-based%20reasoning%20without%20spatial%20grounding.%20As%20a%20result%2C%20such%20models%20often%20produce%20incorrect%20waypoints%20in%20topologically%20complex%20environments%20with%20dead%20ends%2C%20and%20lack%20the%20perceptual%20capacity%20to%20interpret%20ambiguous%20physical%20boundaries.%20These%20inconsistencies%20lead%20to%20costly%20corrective%20expansions%20and%20undermine%20the%20intended%20computational%20efficiency.%0A%20%20We%20introduce%20MMP-A%2A%2C%20a%20multimodal%20framework%20that%20integrates%20the%20spatial%20grounding%20capabilities%20of%20vision-language%20models%20with%20a%20novel%20adaptive%20decay%20mechanism.%20By%20anchoring%20high-level%20reasoning%20in%20physical%20geometry%2C%20the%20framework%20produces%20coherent%20waypoint%20guidance%20that%20addresses%20the%20limitations%20of%20text-only%20planners.%20The%20adaptive%20decay%20mechanism%20dynamically%20regulates%20the%20influence%20of%20uncertain%20waypoints%20within%20the%20heuristic%2C%20ensuring%20geometric%20validity%20while%20substantially%20reducing%20memory%20overhead.%20To%20evaluate%20robustness%2C%20we%20test%20the%20framework%20in%20challenging%20environments%20characterized%20by%20severe%20clutter%20and%20topological%20complexity.%20Experimental%20results%20show%20that%20MMP-A%2A%20achieves%20near-optimal%20trajectories%20with%20significantly%20reduced%20operational%20costs%2C%20demonstrating%20its%20potential%20as%20a%20perception-grounded%20and%20computationally%20efficient%20paradigm%20for%20autonomous%20navigation.&entry.1838667208=http%3A//arxiv.org/abs/2601.01910v2&entry.124074799=Read"},
{"title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation", "author": "Jaekwon Im and Natalia Polouliakh and Taketo Akama", "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.", "link": "http://arxiv.org/abs/2601.15872v1", "date": "2026-01-22", "relevancy": 2.2844, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6035}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5523}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PF-D2M%3A%20A%20Pose-free%20Diffusion%20Model%20for%20Universal%20Dance-to-Music%20Generation&body=Title%3A%20PF-D2M%3A%20A%20Pose-free%20Diffusion%20Model%20for%20Universal%20Dance-to-Music%20Generation%0AAuthor%3A%20Jaekwon%20Im%20and%20Natalia%20Polouliakh%20and%20Taketo%20Akama%0AAbstract%3A%20Dance-to-music%20generation%20aims%20to%20generate%20music%20that%20is%20aligned%20with%20dance%20movements.%20Existing%20approaches%20typically%20rely%20on%20body%20motion%20features%20extracted%20from%20a%20single%20human%20dancer%20and%20limited%20dance-to-music%20datasets%2C%20which%20restrict%20their%20performance%20and%20applicability%20to%20real-world%20scenarios%20involving%20multiple%20dancers%20and%20non-human%20dancers.%20In%20this%20paper%2C%20we%20propose%20PF-D2M%2C%20a%20universal%20diffusion-based%20dance-to-music%20generation%20model%20that%20incorporates%20visual%20features%20extracted%20from%20dance%20videos.%20PF-D2M%20is%20trained%20with%20a%20progressive%20training%20strategy%20that%20effectively%20addresses%20data%20scarcity%20and%20generalization%20challenges.%20Both%20objective%20and%20subjective%20evaluations%20show%20that%20PF-D2M%20achieves%20state-of-the-art%20performance%20in%20dance-music%20alignment%20and%20music%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPF-D2M%253A%2520A%2520Pose-free%2520Diffusion%2520Model%2520for%2520Universal%2520Dance-to-Music%2520Generation%26entry.906535625%3DJaekwon%2520Im%2520and%2520Natalia%2520Polouliakh%2520and%2520Taketo%2520Akama%26entry.1292438233%3DDance-to-music%2520generation%2520aims%2520to%2520generate%2520music%2520that%2520is%2520aligned%2520with%2520dance%2520movements.%2520Existing%2520approaches%2520typically%2520rely%2520on%2520body%2520motion%2520features%2520extracted%2520from%2520a%2520single%2520human%2520dancer%2520and%2520limited%2520dance-to-music%2520datasets%252C%2520which%2520restrict%2520their%2520performance%2520and%2520applicability%2520to%2520real-world%2520scenarios%2520involving%2520multiple%2520dancers%2520and%2520non-human%2520dancers.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PF-D2M%252C%2520a%2520universal%2520diffusion-based%2520dance-to-music%2520generation%2520model%2520that%2520incorporates%2520visual%2520features%2520extracted%2520from%2520dance%2520videos.%2520PF-D2M%2520is%2520trained%2520with%2520a%2520progressive%2520training%2520strategy%2520that%2520effectively%2520addresses%2520data%2520scarcity%2520and%2520generalization%2520challenges.%2520Both%2520objective%2520and%2520subjective%2520evaluations%2520show%2520that%2520PF-D2M%2520achieves%2520state-of-the-art%2520performance%2520in%2520dance-music%2520alignment%2520and%2520music%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PF-D2M%3A%20A%20Pose-free%20Diffusion%20Model%20for%20Universal%20Dance-to-Music%20Generation&entry.906535625=Jaekwon%20Im%20and%20Natalia%20Polouliakh%20and%20Taketo%20Akama&entry.1292438233=Dance-to-music%20generation%20aims%20to%20generate%20music%20that%20is%20aligned%20with%20dance%20movements.%20Existing%20approaches%20typically%20rely%20on%20body%20motion%20features%20extracted%20from%20a%20single%20human%20dancer%20and%20limited%20dance-to-music%20datasets%2C%20which%20restrict%20their%20performance%20and%20applicability%20to%20real-world%20scenarios%20involving%20multiple%20dancers%20and%20non-human%20dancers.%20In%20this%20paper%2C%20we%20propose%20PF-D2M%2C%20a%20universal%20diffusion-based%20dance-to-music%20generation%20model%20that%20incorporates%20visual%20features%20extracted%20from%20dance%20videos.%20PF-D2M%20is%20trained%20with%20a%20progressive%20training%20strategy%20that%20effectively%20addresses%20data%20scarcity%20and%20generalization%20challenges.%20Both%20objective%20and%20subjective%20evaluations%20show%20that%20PF-D2M%20achieves%20state-of-the-art%20performance%20in%20dance-music%20alignment%20and%20music%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.15872v1&entry.124074799=Read"},
{"title": "Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning", "author": "Haomiao Tang and Jinpeng Wang and Minyi Zhao and Guanghao Meng and Ruisheng Luo and Long Chen and Shu-Tao Xia", "abstract": "Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.", "link": "http://arxiv.org/abs/2601.11393v2", "date": "2026-01-22", "relevancy": 2.2762, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5874}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5719}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Uncertainty-Guided%20Composed%20Image%20Retrieval%20with%20Fine-Grained%20Probabilistic%20Learning&body=Title%3A%20Heterogeneous%20Uncertainty-Guided%20Composed%20Image%20Retrieval%20with%20Fine-Grained%20Probabilistic%20Learning%0AAuthor%3A%20Haomiao%20Tang%20and%20Jinpeng%20Wang%20and%20Minyi%20Zhao%20and%20Guanghao%20Meng%20and%20Ruisheng%20Luo%20and%20Long%20Chen%20and%20Shu-Tao%20Xia%0AAbstract%3A%20Composed%20Image%20Retrieval%20%28CIR%29%20enables%20image%20search%20by%20combining%20a%20reference%20image%20with%20modification%20text.%20Intrinsic%20noise%20in%20CIR%20triplets%20incurs%20intrinsic%20uncertainty%20and%20threatens%20the%20model%27s%20robustness.%20Probabilistic%20learning%20approaches%20have%20shown%20promise%20in%20addressing%20such%20issues%3B%20however%2C%20they%20fall%20short%20for%20CIR%20due%20to%20their%20instance-level%20holistic%20modeling%20and%20homogeneous%20treatment%20of%20queries%20and%20targets.%20This%20paper%20introduces%20a%20Heterogeneous%20Uncertainty-Guided%20%28HUG%29%20paradigm%20to%20overcome%20these%20limitations.%20HUG%20utilizes%20a%20fine-grained%20probabilistic%20learning%20framework%2C%20where%20queries%20and%20targets%20are%20represented%20by%20Gaussian%20embeddings%20that%20capture%20detailed%20concepts%20and%20uncertainties.%20We%20customize%20heterogeneous%20uncertainty%20estimations%20for%20multi-modal%20queries%20and%20uni-modal%20targets.%20Given%20a%20query%2C%20we%20capture%20uncertainties%20not%20only%20regarding%20uni-modal%20content%20quality%20but%20also%20multi-modal%20coordination%2C%20followed%20by%20a%20provable%20dynamic%20weighting%20mechanism%20to%20derive%20comprehensive%20query%20uncertainty.%20We%20further%20design%20uncertainty-guided%20objectives%2C%20including%20query-target%20holistic%20contrast%20and%20fine-grained%20contrasts%20with%20comprehensive%20negative%20sampling%20strategies%2C%20which%20effectively%20enhance%20discriminative%20learning.%20Experiments%20on%20benchmarks%20demonstrate%20HUG%27s%20effectiveness%20beyond%20state-of-the-art%20baselines%2C%20with%20faithful%20analysis%20justifying%20the%20technical%20contributions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Uncertainty-Guided%2520Composed%2520Image%2520Retrieval%2520with%2520Fine-Grained%2520Probabilistic%2520Learning%26entry.906535625%3DHaomiao%2520Tang%2520and%2520Jinpeng%2520Wang%2520and%2520Minyi%2520Zhao%2520and%2520Guanghao%2520Meng%2520and%2520Ruisheng%2520Luo%2520and%2520Long%2520Chen%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3DComposed%2520Image%2520Retrieval%2520%2528CIR%2529%2520enables%2520image%2520search%2520by%2520combining%2520a%2520reference%2520image%2520with%2520modification%2520text.%2520Intrinsic%2520noise%2520in%2520CIR%2520triplets%2520incurs%2520intrinsic%2520uncertainty%2520and%2520threatens%2520the%2520model%2527s%2520robustness.%2520Probabilistic%2520learning%2520approaches%2520have%2520shown%2520promise%2520in%2520addressing%2520such%2520issues%253B%2520however%252C%2520they%2520fall%2520short%2520for%2520CIR%2520due%2520to%2520their%2520instance-level%2520holistic%2520modeling%2520and%2520homogeneous%2520treatment%2520of%2520queries%2520and%2520targets.%2520This%2520paper%2520introduces%2520a%2520Heterogeneous%2520Uncertainty-Guided%2520%2528HUG%2529%2520paradigm%2520to%2520overcome%2520these%2520limitations.%2520HUG%2520utilizes%2520a%2520fine-grained%2520probabilistic%2520learning%2520framework%252C%2520where%2520queries%2520and%2520targets%2520are%2520represented%2520by%2520Gaussian%2520embeddings%2520that%2520capture%2520detailed%2520concepts%2520and%2520uncertainties.%2520We%2520customize%2520heterogeneous%2520uncertainty%2520estimations%2520for%2520multi-modal%2520queries%2520and%2520uni-modal%2520targets.%2520Given%2520a%2520query%252C%2520we%2520capture%2520uncertainties%2520not%2520only%2520regarding%2520uni-modal%2520content%2520quality%2520but%2520also%2520multi-modal%2520coordination%252C%2520followed%2520by%2520a%2520provable%2520dynamic%2520weighting%2520mechanism%2520to%2520derive%2520comprehensive%2520query%2520uncertainty.%2520We%2520further%2520design%2520uncertainty-guided%2520objectives%252C%2520including%2520query-target%2520holistic%2520contrast%2520and%2520fine-grained%2520contrasts%2520with%2520comprehensive%2520negative%2520sampling%2520strategies%252C%2520which%2520effectively%2520enhance%2520discriminative%2520learning.%2520Experiments%2520on%2520benchmarks%2520demonstrate%2520HUG%2527s%2520effectiveness%2520beyond%2520state-of-the-art%2520baselines%252C%2520with%2520faithful%2520analysis%2520justifying%2520the%2520technical%2520contributions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Uncertainty-Guided%20Composed%20Image%20Retrieval%20with%20Fine-Grained%20Probabilistic%20Learning&entry.906535625=Haomiao%20Tang%20and%20Jinpeng%20Wang%20and%20Minyi%20Zhao%20and%20Guanghao%20Meng%20and%20Ruisheng%20Luo%20and%20Long%20Chen%20and%20Shu-Tao%20Xia&entry.1292438233=Composed%20Image%20Retrieval%20%28CIR%29%20enables%20image%20search%20by%20combining%20a%20reference%20image%20with%20modification%20text.%20Intrinsic%20noise%20in%20CIR%20triplets%20incurs%20intrinsic%20uncertainty%20and%20threatens%20the%20model%27s%20robustness.%20Probabilistic%20learning%20approaches%20have%20shown%20promise%20in%20addressing%20such%20issues%3B%20however%2C%20they%20fall%20short%20for%20CIR%20due%20to%20their%20instance-level%20holistic%20modeling%20and%20homogeneous%20treatment%20of%20queries%20and%20targets.%20This%20paper%20introduces%20a%20Heterogeneous%20Uncertainty-Guided%20%28HUG%29%20paradigm%20to%20overcome%20these%20limitations.%20HUG%20utilizes%20a%20fine-grained%20probabilistic%20learning%20framework%2C%20where%20queries%20and%20targets%20are%20represented%20by%20Gaussian%20embeddings%20that%20capture%20detailed%20concepts%20and%20uncertainties.%20We%20customize%20heterogeneous%20uncertainty%20estimations%20for%20multi-modal%20queries%20and%20uni-modal%20targets.%20Given%20a%20query%2C%20we%20capture%20uncertainties%20not%20only%20regarding%20uni-modal%20content%20quality%20but%20also%20multi-modal%20coordination%2C%20followed%20by%20a%20provable%20dynamic%20weighting%20mechanism%20to%20derive%20comprehensive%20query%20uncertainty.%20We%20further%20design%20uncertainty-guided%20objectives%2C%20including%20query-target%20holistic%20contrast%20and%20fine-grained%20contrasts%20with%20comprehensive%20negative%20sampling%20strategies%2C%20which%20effectively%20enhance%20discriminative%20learning.%20Experiments%20on%20benchmarks%20demonstrate%20HUG%27s%20effectiveness%20beyond%20state-of-the-art%20baselines%2C%20with%20faithful%20analysis%20justifying%20the%20technical%20contributions.&entry.1838667208=http%3A//arxiv.org/abs/2601.11393v2&entry.124074799=Read"},
{"title": "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes", "author": "Steven Kolawole and Lucio Dery and Jean-Fran\u00e7ois Kagy and Virginia Smith and Graham Neubig and Ameet Talwalkar", "abstract": "Structured pruning is a promising approach to create smaller, faster large language models. However, existing methods typically rely on computing the gradient via backward passes, which can inflate memory requirements and compute costs. In this work we introduce Bonsai, a gradient-free structured pruning method that eliminates the need for backpropagation, significantly reducing memory requirements and compute costs while achieving state-of-the-art pruning performance. Bonsai uses forward-pass-only perturbative pruning to enable efficient compression of large models on a broader range of hardware configurations. Unlike existing structured pruning approaches, Bonsai not only achieves better compression with fewer resources but also produces models that are twice as fast as those generated by semi-structured pruning. As a concrete demonstration, we use Bonsai to prune 7B and 8B models to 50% sparsity on a single A6000 GPU -- a task challenging for backprop-based methods in memory-constrained settings, as they require 2-3x the memory. Our results show that removing backprop as a requirement not only enables pruning larger models on constrained hardware but can also lead to state-of-the-art efficiency and performance.", "link": "http://arxiv.org/abs/2402.05406v4", "date": "2026-01-22", "relevancy": 2.2525, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4568}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4522}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Everybody%20Prune%20Now%3A%20Structured%20Pruning%20of%20LLMs%20with%20only%20Forward%20Passes&body=Title%3A%20Everybody%20Prune%20Now%3A%20Structured%20Pruning%20of%20LLMs%20with%20only%20Forward%20Passes%0AAuthor%3A%20Steven%20Kolawole%20and%20Lucio%20Dery%20and%20Jean-Fran%C3%A7ois%20Kagy%20and%20Virginia%20Smith%20and%20Graham%20Neubig%20and%20Ameet%20Talwalkar%0AAbstract%3A%20Structured%20pruning%20is%20a%20promising%20approach%20to%20create%20smaller%2C%20faster%20large%20language%20models.%20However%2C%20existing%20methods%20typically%20rely%20on%20computing%20the%20gradient%20via%20backward%20passes%2C%20which%20can%20inflate%20memory%20requirements%20and%20compute%20costs.%20In%20this%20work%20we%20introduce%20Bonsai%2C%20a%20gradient-free%20structured%20pruning%20method%20that%20eliminates%20the%20need%20for%20backpropagation%2C%20significantly%20reducing%20memory%20requirements%20and%20compute%20costs%20while%20achieving%20state-of-the-art%20pruning%20performance.%20Bonsai%20uses%20forward-pass-only%20perturbative%20pruning%20to%20enable%20efficient%20compression%20of%20large%20models%20on%20a%20broader%20range%20of%20hardware%20configurations.%20Unlike%20existing%20structured%20pruning%20approaches%2C%20Bonsai%20not%20only%20achieves%20better%20compression%20with%20fewer%20resources%20but%20also%20produces%20models%20that%20are%20twice%20as%20fast%20as%20those%20generated%20by%20semi-structured%20pruning.%20As%20a%20concrete%20demonstration%2C%20we%20use%20Bonsai%20to%20prune%207B%20and%208B%20models%20to%2050%25%20sparsity%20on%20a%20single%20A6000%20GPU%20--%20a%20task%20challenging%20for%20backprop-based%20methods%20in%20memory-constrained%20settings%2C%20as%20they%20require%202-3x%20the%20memory.%20Our%20results%20show%20that%20removing%20backprop%20as%20a%20requirement%20not%20only%20enables%20pruning%20larger%20models%20on%20constrained%20hardware%20but%20can%20also%20lead%20to%20state-of-the-art%20efficiency%20and%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2402.05406v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEverybody%2520Prune%2520Now%253A%2520Structured%2520Pruning%2520of%2520LLMs%2520with%2520only%2520Forward%2520Passes%26entry.906535625%3DSteven%2520Kolawole%2520and%2520Lucio%2520Dery%2520and%2520Jean-Fran%25C3%25A7ois%2520Kagy%2520and%2520Virginia%2520Smith%2520and%2520Graham%2520Neubig%2520and%2520Ameet%2520Talwalkar%26entry.1292438233%3DStructured%2520pruning%2520is%2520a%2520promising%2520approach%2520to%2520create%2520smaller%252C%2520faster%2520large%2520language%2520models.%2520However%252C%2520existing%2520methods%2520typically%2520rely%2520on%2520computing%2520the%2520gradient%2520via%2520backward%2520passes%252C%2520which%2520can%2520inflate%2520memory%2520requirements%2520and%2520compute%2520costs.%2520In%2520this%2520work%2520we%2520introduce%2520Bonsai%252C%2520a%2520gradient-free%2520structured%2520pruning%2520method%2520that%2520eliminates%2520the%2520need%2520for%2520backpropagation%252C%2520significantly%2520reducing%2520memory%2520requirements%2520and%2520compute%2520costs%2520while%2520achieving%2520state-of-the-art%2520pruning%2520performance.%2520Bonsai%2520uses%2520forward-pass-only%2520perturbative%2520pruning%2520to%2520enable%2520efficient%2520compression%2520of%2520large%2520models%2520on%2520a%2520broader%2520range%2520of%2520hardware%2520configurations.%2520Unlike%2520existing%2520structured%2520pruning%2520approaches%252C%2520Bonsai%2520not%2520only%2520achieves%2520better%2520compression%2520with%2520fewer%2520resources%2520but%2520also%2520produces%2520models%2520that%2520are%2520twice%2520as%2520fast%2520as%2520those%2520generated%2520by%2520semi-structured%2520pruning.%2520As%2520a%2520concrete%2520demonstration%252C%2520we%2520use%2520Bonsai%2520to%2520prune%25207B%2520and%25208B%2520models%2520to%252050%2525%2520sparsity%2520on%2520a%2520single%2520A6000%2520GPU%2520--%2520a%2520task%2520challenging%2520for%2520backprop-based%2520methods%2520in%2520memory-constrained%2520settings%252C%2520as%2520they%2520require%25202-3x%2520the%2520memory.%2520Our%2520results%2520show%2520that%2520removing%2520backprop%2520as%2520a%2520requirement%2520not%2520only%2520enables%2520pruning%2520larger%2520models%2520on%2520constrained%2520hardware%2520but%2520can%2520also%2520lead%2520to%2520state-of-the-art%2520efficiency%2520and%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05406v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Everybody%20Prune%20Now%3A%20Structured%20Pruning%20of%20LLMs%20with%20only%20Forward%20Passes&entry.906535625=Steven%20Kolawole%20and%20Lucio%20Dery%20and%20Jean-Fran%C3%A7ois%20Kagy%20and%20Virginia%20Smith%20and%20Graham%20Neubig%20and%20Ameet%20Talwalkar&entry.1292438233=Structured%20pruning%20is%20a%20promising%20approach%20to%20create%20smaller%2C%20faster%20large%20language%20models.%20However%2C%20existing%20methods%20typically%20rely%20on%20computing%20the%20gradient%20via%20backward%20passes%2C%20which%20can%20inflate%20memory%20requirements%20and%20compute%20costs.%20In%20this%20work%20we%20introduce%20Bonsai%2C%20a%20gradient-free%20structured%20pruning%20method%20that%20eliminates%20the%20need%20for%20backpropagation%2C%20significantly%20reducing%20memory%20requirements%20and%20compute%20costs%20while%20achieving%20state-of-the-art%20pruning%20performance.%20Bonsai%20uses%20forward-pass-only%20perturbative%20pruning%20to%20enable%20efficient%20compression%20of%20large%20models%20on%20a%20broader%20range%20of%20hardware%20configurations.%20Unlike%20existing%20structured%20pruning%20approaches%2C%20Bonsai%20not%20only%20achieves%20better%20compression%20with%20fewer%20resources%20but%20also%20produces%20models%20that%20are%20twice%20as%20fast%20as%20those%20generated%20by%20semi-structured%20pruning.%20As%20a%20concrete%20demonstration%2C%20we%20use%20Bonsai%20to%20prune%207B%20and%208B%20models%20to%2050%25%20sparsity%20on%20a%20single%20A6000%20GPU%20--%20a%20task%20challenging%20for%20backprop-based%20methods%20in%20memory-constrained%20settings%2C%20as%20they%20require%202-3x%20the%20memory.%20Our%20results%20show%20that%20removing%20backprop%20as%20a%20requirement%20not%20only%20enables%20pruning%20larger%20models%20on%20constrained%20hardware%20but%20can%20also%20lead%20to%20state-of-the-art%20efficiency%20and%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2402.05406v4&entry.124074799=Read"},
{"title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "author": "Shijie Lian and Bin Yu and Xiaopeng Lin and Laurence T. Yang and Zhaolong Shen and Changti Wu and Yuzhuo Miao and Cong Huang and Kai Chen", "abstract": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $\u03c0(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "link": "http://arxiv.org/abs/2601.15197v2", "date": "2026-01-22", "relevancy": 2.2518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayesianVLA%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries&body=Title%3A%20BayesianVLA%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries%0AAuthor%3A%20Shijie%20Lian%20and%20Bin%20Yu%20and%20Xiaopeng%20Lin%20and%20Laurence%20T.%20Yang%20and%20Zhaolong%20Shen%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Cong%20Huang%20and%20Kai%20Chen%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20promise%20in%20robot%20manipulation%20but%20often%20struggle%20to%20generalize%20to%20new%20instructions%20or%20complex%20multi-task%20scenarios.%20We%20identify%20a%20critical%20pathology%20in%20current%20training%20paradigms%20where%20goal-driven%20data%20collection%20creates%20a%20dataset%20bias.%20In%20such%20datasets%2C%20language%20instructions%20are%20highly%20predictable%20from%20visual%20observations%20alone%2C%20causing%20the%20conditional%20mutual%20information%20between%20instructions%20and%20actions%20to%20vanish%2C%20a%20phenomenon%20we%20term%20Information%20Collapse.%20Consequently%2C%20models%20degenerate%20into%20vision-only%20policies%20that%20ignore%20language%20constraints%20and%20fail%20in%20out-of-distribution%20%28OOD%29%20settings.%20To%20address%20this%2C%20we%20propose%20BayesianVLA%2C%20a%20novel%20framework%20that%20enforces%20instruction%20following%20via%20Bayesian%20decomposition.%20By%20introducing%20learnable%20Latent%20Action%20Queries%2C%20we%20construct%20a%20dual-branch%20architecture%20to%20estimate%20both%20a%20vision-only%20prior%20%24p%28a%20%5Cmid%20v%29%24%20and%20a%20language-conditioned%20posterior%20%24%CF%80%28a%20%5Cmid%20v%2C%20%5Cell%29%24.%20We%20then%20optimize%20the%20policy%20to%20maximize%20the%20conditional%20Pointwise%20Mutual%20Information%20%28PMI%29%20between%20actions%20and%20instructions.%20This%20objective%20effectively%20penalizes%20the%20vision%20shortcut%20and%20rewards%20actions%20that%20explicitly%20explain%20the%20language%20command.%20Without%20requiring%20new%20data%2C%20BayesianVLA%20significantly%20improves%20generalization.%20Extensive%20experiments%20across%20on%20SimplerEnv%20and%20RoboCasa%20demonstrate%20substantial%20gains%2C%20including%20an%2011.3%25%20improvement%20on%20the%20challenging%20OOD%20SimplerEnv%20benchmark%2C%20validating%20the%20ability%20of%20our%20approach%20to%20robustly%20ground%20language%20in%20action.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesianVLA%253A%2520Bayesian%2520Decomposition%2520of%2520Vision%2520Language%2520Action%2520Models%2520via%2520Latent%2520Action%2520Queries%26entry.906535625%3DShijie%2520Lian%2520and%2520Bin%2520Yu%2520and%2520Xiaopeng%2520Lin%2520and%2520Laurence%2520T.%2520Yang%2520and%2520Zhaolong%2520Shen%2520and%2520Changti%2520Wu%2520and%2520Yuzhuo%2520Miao%2520and%2520Cong%2520Huang%2520and%2520Kai%2520Chen%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520promise%2520in%2520robot%2520manipulation%2520but%2520often%2520struggle%2520to%2520generalize%2520to%2520new%2520instructions%2520or%2520complex%2520multi-task%2520scenarios.%2520We%2520identify%2520a%2520critical%2520pathology%2520in%2520current%2520training%2520paradigms%2520where%2520goal-driven%2520data%2520collection%2520creates%2520a%2520dataset%2520bias.%2520In%2520such%2520datasets%252C%2520language%2520instructions%2520are%2520highly%2520predictable%2520from%2520visual%2520observations%2520alone%252C%2520causing%2520the%2520conditional%2520mutual%2520information%2520between%2520instructions%2520and%2520actions%2520to%2520vanish%252C%2520a%2520phenomenon%2520we%2520term%2520Information%2520Collapse.%2520Consequently%252C%2520models%2520degenerate%2520into%2520vision-only%2520policies%2520that%2520ignore%2520language%2520constraints%2520and%2520fail%2520in%2520out-of-distribution%2520%2528OOD%2529%2520settings.%2520To%2520address%2520this%252C%2520we%2520propose%2520BayesianVLA%252C%2520a%2520novel%2520framework%2520that%2520enforces%2520instruction%2520following%2520via%2520Bayesian%2520decomposition.%2520By%2520introducing%2520learnable%2520Latent%2520Action%2520Queries%252C%2520we%2520construct%2520a%2520dual-branch%2520architecture%2520to%2520estimate%2520both%2520a%2520vision-only%2520prior%2520%2524p%2528a%2520%255Cmid%2520v%2529%2524%2520and%2520a%2520language-conditioned%2520posterior%2520%2524%25CF%2580%2528a%2520%255Cmid%2520v%252C%2520%255Cell%2529%2524.%2520We%2520then%2520optimize%2520the%2520policy%2520to%2520maximize%2520the%2520conditional%2520Pointwise%2520Mutual%2520Information%2520%2528PMI%2529%2520between%2520actions%2520and%2520instructions.%2520This%2520objective%2520effectively%2520penalizes%2520the%2520vision%2520shortcut%2520and%2520rewards%2520actions%2520that%2520explicitly%2520explain%2520the%2520language%2520command.%2520Without%2520requiring%2520new%2520data%252C%2520BayesianVLA%2520significantly%2520improves%2520generalization.%2520Extensive%2520experiments%2520across%2520on%2520SimplerEnv%2520and%2520RoboCasa%2520demonstrate%2520substantial%2520gains%252C%2520including%2520an%252011.3%2525%2520improvement%2520on%2520the%2520challenging%2520OOD%2520SimplerEnv%2520benchmark%252C%2520validating%2520the%2520ability%2520of%2520our%2520approach%2520to%2520robustly%2520ground%2520language%2520in%2520action.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayesianVLA%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries&entry.906535625=Shijie%20Lian%20and%20Bin%20Yu%20and%20Xiaopeng%20Lin%20and%20Laurence%20T.%20Yang%20and%20Zhaolong%20Shen%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Cong%20Huang%20and%20Kai%20Chen&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20promise%20in%20robot%20manipulation%20but%20often%20struggle%20to%20generalize%20to%20new%20instructions%20or%20complex%20multi-task%20scenarios.%20We%20identify%20a%20critical%20pathology%20in%20current%20training%20paradigms%20where%20goal-driven%20data%20collection%20creates%20a%20dataset%20bias.%20In%20such%20datasets%2C%20language%20instructions%20are%20highly%20predictable%20from%20visual%20observations%20alone%2C%20causing%20the%20conditional%20mutual%20information%20between%20instructions%20and%20actions%20to%20vanish%2C%20a%20phenomenon%20we%20term%20Information%20Collapse.%20Consequently%2C%20models%20degenerate%20into%20vision-only%20policies%20that%20ignore%20language%20constraints%20and%20fail%20in%20out-of-distribution%20%28OOD%29%20settings.%20To%20address%20this%2C%20we%20propose%20BayesianVLA%2C%20a%20novel%20framework%20that%20enforces%20instruction%20following%20via%20Bayesian%20decomposition.%20By%20introducing%20learnable%20Latent%20Action%20Queries%2C%20we%20construct%20a%20dual-branch%20architecture%20to%20estimate%20both%20a%20vision-only%20prior%20%24p%28a%20%5Cmid%20v%29%24%20and%20a%20language-conditioned%20posterior%20%24%CF%80%28a%20%5Cmid%20v%2C%20%5Cell%29%24.%20We%20then%20optimize%20the%20policy%20to%20maximize%20the%20conditional%20Pointwise%20Mutual%20Information%20%28PMI%29%20between%20actions%20and%20instructions.%20This%20objective%20effectively%20penalizes%20the%20vision%20shortcut%20and%20rewards%20actions%20that%20explicitly%20explain%20the%20language%20command.%20Without%20requiring%20new%20data%2C%20BayesianVLA%20significantly%20improves%20generalization.%20Extensive%20experiments%20across%20on%20SimplerEnv%20and%20RoboCasa%20demonstrate%20substantial%20gains%2C%20including%20an%2011.3%25%20improvement%20on%20the%20challenging%20OOD%20SimplerEnv%20benchmark%2C%20validating%20the%20ability%20of%20our%20approach%20to%20robustly%20ground%20language%20in%20action.&entry.1838667208=http%3A//arxiv.org/abs/2601.15197v2&entry.124074799=Read"},
{"title": "NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation", "author": "Liuyun Jiang and Yizhuo Lu and Yanchao Zhang and Jiazheng Liu and Hua Han", "abstract": "Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.", "link": "http://arxiv.org/abs/2601.15929v1", "date": "2026-01-22", "relevancy": 2.2498, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5716}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroMamba%3A%20Multi-Perspective%20Feature%20Interaction%20with%20Visual%20Mamba%20for%20Neuron%20Segmentation&body=Title%3A%20NeuroMamba%3A%20Multi-Perspective%20Feature%20Interaction%20with%20Visual%20Mamba%20for%20Neuron%20Segmentation%0AAuthor%3A%20Liuyun%20Jiang%20and%20Yizhuo%20Lu%20and%20Yanchao%20Zhang%20and%20Jiazheng%20Liu%20and%20Hua%20Han%0AAbstract%3A%20Neuron%20segmentation%20is%20the%20cornerstone%20of%20reconstructing%20comprehensive%20neuronal%20connectomes%2C%20which%20is%20essential%20for%20deciphering%20the%20functional%20organization%20of%20the%20brain.%20The%20irregular%20morphology%20and%20densely%20intertwined%20structures%20of%20neurons%20make%20this%20task%20particularly%20challenging.%20Prevailing%20CNN-based%20methods%20often%20fail%20to%20resolve%20ambiguous%20boundaries%20due%20to%20the%20lack%20of%20long-range%20context%2C%20whereas%20Transformer-based%20methods%20suffer%20from%20boundary%20imprecision%20caused%20by%20the%20loss%20of%20voxel-level%20details%20during%20patch%20partitioning.%20To%20address%20these%20limitations%2C%20we%20propose%20NeuroMamba%2C%20a%20multi-perspective%20framework%20that%20exploits%20the%20linear%20complexity%20of%20Mamba%20to%20enable%20patch-free%20global%20modeling%20and%20synergizes%20this%20with%20complementary%20local%20feature%20modeling%2C%20thereby%20efficiently%20capturing%20long-range%20dependencies%20while%20meticulously%20preserving%20fine-grained%20voxel%20details.%20Specifically%2C%20we%20design%20a%20channel-gated%20Boundary%20Discriminative%20Feature%20Extractor%20%28BDFE%29%20to%20enhance%20local%20morphological%20cues.%20Complementing%20this%2C%20we%20introduce%20the%20Spatial%20Continuous%20Feature%20Extractor%20%28SCFE%29%2C%20which%20integrates%20a%20resolution-aware%20scanning%20mechanism%20into%20the%20Visual%20Mamba%20architecture%20to%20adaptively%20model%20global%20dependencies%20across%20varying%20data%20resolutions.%20Finally%2C%20a%20cross-modulation%20mechanism%20synergistically%20fuses%20these%20multi-perspective%20features.%20Our%20method%20demonstrates%20state-of-the-art%20performance%20across%20four%20public%20EM%20datasets%2C%20validating%20its%20exceptional%20adaptability%20to%20both%20anisotropic%20and%20isotropic%20resolutions.%20The%20source%20code%20will%20be%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroMamba%253A%2520Multi-Perspective%2520Feature%2520Interaction%2520with%2520Visual%2520Mamba%2520for%2520Neuron%2520Segmentation%26entry.906535625%3DLiuyun%2520Jiang%2520and%2520Yizhuo%2520Lu%2520and%2520Yanchao%2520Zhang%2520and%2520Jiazheng%2520Liu%2520and%2520Hua%2520Han%26entry.1292438233%3DNeuron%2520segmentation%2520is%2520the%2520cornerstone%2520of%2520reconstructing%2520comprehensive%2520neuronal%2520connectomes%252C%2520which%2520is%2520essential%2520for%2520deciphering%2520the%2520functional%2520organization%2520of%2520the%2520brain.%2520The%2520irregular%2520morphology%2520and%2520densely%2520intertwined%2520structures%2520of%2520neurons%2520make%2520this%2520task%2520particularly%2520challenging.%2520Prevailing%2520CNN-based%2520methods%2520often%2520fail%2520to%2520resolve%2520ambiguous%2520boundaries%2520due%2520to%2520the%2520lack%2520of%2520long-range%2520context%252C%2520whereas%2520Transformer-based%2520methods%2520suffer%2520from%2520boundary%2520imprecision%2520caused%2520by%2520the%2520loss%2520of%2520voxel-level%2520details%2520during%2520patch%2520partitioning.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520NeuroMamba%252C%2520a%2520multi-perspective%2520framework%2520that%2520exploits%2520the%2520linear%2520complexity%2520of%2520Mamba%2520to%2520enable%2520patch-free%2520global%2520modeling%2520and%2520synergizes%2520this%2520with%2520complementary%2520local%2520feature%2520modeling%252C%2520thereby%2520efficiently%2520capturing%2520long-range%2520dependencies%2520while%2520meticulously%2520preserving%2520fine-grained%2520voxel%2520details.%2520Specifically%252C%2520we%2520design%2520a%2520channel-gated%2520Boundary%2520Discriminative%2520Feature%2520Extractor%2520%2528BDFE%2529%2520to%2520enhance%2520local%2520morphological%2520cues.%2520Complementing%2520this%252C%2520we%2520introduce%2520the%2520Spatial%2520Continuous%2520Feature%2520Extractor%2520%2528SCFE%2529%252C%2520which%2520integrates%2520a%2520resolution-aware%2520scanning%2520mechanism%2520into%2520the%2520Visual%2520Mamba%2520architecture%2520to%2520adaptively%2520model%2520global%2520dependencies%2520across%2520varying%2520data%2520resolutions.%2520Finally%252C%2520a%2520cross-modulation%2520mechanism%2520synergistically%2520fuses%2520these%2520multi-perspective%2520features.%2520Our%2520method%2520demonstrates%2520state-of-the-art%2520performance%2520across%2520four%2520public%2520EM%2520datasets%252C%2520validating%2520its%2520exceptional%2520adaptability%2520to%2520both%2520anisotropic%2520and%2520isotropic%2520resolutions.%2520The%2520source%2520code%2520will%2520be%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroMamba%3A%20Multi-Perspective%20Feature%20Interaction%20with%20Visual%20Mamba%20for%20Neuron%20Segmentation&entry.906535625=Liuyun%20Jiang%20and%20Yizhuo%20Lu%20and%20Yanchao%20Zhang%20and%20Jiazheng%20Liu%20and%20Hua%20Han&entry.1292438233=Neuron%20segmentation%20is%20the%20cornerstone%20of%20reconstructing%20comprehensive%20neuronal%20connectomes%2C%20which%20is%20essential%20for%20deciphering%20the%20functional%20organization%20of%20the%20brain.%20The%20irregular%20morphology%20and%20densely%20intertwined%20structures%20of%20neurons%20make%20this%20task%20particularly%20challenging.%20Prevailing%20CNN-based%20methods%20often%20fail%20to%20resolve%20ambiguous%20boundaries%20due%20to%20the%20lack%20of%20long-range%20context%2C%20whereas%20Transformer-based%20methods%20suffer%20from%20boundary%20imprecision%20caused%20by%20the%20loss%20of%20voxel-level%20details%20during%20patch%20partitioning.%20To%20address%20these%20limitations%2C%20we%20propose%20NeuroMamba%2C%20a%20multi-perspective%20framework%20that%20exploits%20the%20linear%20complexity%20of%20Mamba%20to%20enable%20patch-free%20global%20modeling%20and%20synergizes%20this%20with%20complementary%20local%20feature%20modeling%2C%20thereby%20efficiently%20capturing%20long-range%20dependencies%20while%20meticulously%20preserving%20fine-grained%20voxel%20details.%20Specifically%2C%20we%20design%20a%20channel-gated%20Boundary%20Discriminative%20Feature%20Extractor%20%28BDFE%29%20to%20enhance%20local%20morphological%20cues.%20Complementing%20this%2C%20we%20introduce%20the%20Spatial%20Continuous%20Feature%20Extractor%20%28SCFE%29%2C%20which%20integrates%20a%20resolution-aware%20scanning%20mechanism%20into%20the%20Visual%20Mamba%20architecture%20to%20adaptively%20model%20global%20dependencies%20across%20varying%20data%20resolutions.%20Finally%2C%20a%20cross-modulation%20mechanism%20synergistically%20fuses%20these%20multi-perspective%20features.%20Our%20method%20demonstrates%20state-of-the-art%20performance%20across%20four%20public%20EM%20datasets%2C%20validating%20its%20exceptional%20adaptability%20to%20both%20anisotropic%20and%20isotropic%20resolutions.%20The%20source%20code%20will%20be%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.15929v1&entry.124074799=Read"},
{"title": "A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery", "author": "Valery Fischer and Alan Magdaleno and Anna-Katharina Calek and Nicola Cavalcanti and Nathan Hoffman and Christoph Germann and Joschua W\u00fcthrich and Max Kr\u00e4henmann and Mazda Farshad and Philipp F\u00fcrnstahl and Lilian Calvet", "abstract": "Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.\n  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.\n  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.\n  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.", "link": "http://arxiv.org/abs/2601.15918v1", "date": "2026-01-22", "relevancy": 2.2497, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5749}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5562}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-View%20Pipeline%20and%20Benchmark%20Dataset%20for%203D%20Hand%20Pose%20Estimation%20in%20Surgery&body=Title%3A%20A%20Multi-View%20Pipeline%20and%20Benchmark%20Dataset%20for%203D%20Hand%20Pose%20Estimation%20in%20Surgery%0AAuthor%3A%20Valery%20Fischer%20and%20Alan%20Magdaleno%20and%20Anna-Katharina%20Calek%20and%20Nicola%20Cavalcanti%20and%20Nathan%20Hoffman%20and%20Christoph%20Germann%20and%20Joschua%20W%C3%BCthrich%20and%20Max%20Kr%C3%A4henmann%20and%20Mazda%20Farshad%20and%20Philipp%20F%C3%BCrnstahl%20and%20Lilian%20Calvet%0AAbstract%3A%20Purpose%3A%20Accurate%203D%20hand%20pose%20estimation%20supports%20surgical%20applications%20such%20as%20skill%20assessment%2C%20robot-assisted%20interventions%2C%20and%20geometry-aware%20workflow%20analysis.%20However%2C%20surgical%20environments%20pose%20severe%20challenges%2C%20including%20intense%20and%20localized%20lighting%2C%20frequent%20occlusions%20by%20instruments%20or%20staff%2C%20and%20uniform%20hand%20appearance%20due%20to%20gloves%2C%20combined%20with%20a%20scarcity%20of%20annotated%20datasets%20for%20reliable%20model%20training.%0A%20%20Method%3A%20We%20propose%20a%20robust%20multi-view%20pipeline%20for%203D%20hand%20pose%20estimation%20in%20surgical%20contexts%20that%20requires%20no%20domain-specific%20fine-tuning%20and%20relies%20solely%20on%20off-the-shelf%20pretrained%20models.%20The%20pipeline%20integrates%20reliable%20person%20detection%2C%20whole-body%20pose%20estimation%2C%20and%20state-of-the-art%202D%20hand%20keypoint%20prediction%20on%20tracked%20hand%20crops%2C%20followed%20by%20a%20constrained%203D%20optimization.%20In%20addition%2C%20we%20introduce%20a%20novel%20surgical%20benchmark%20dataset%20comprising%20over%2068%2C000%20frames%20and%203%2C000%20manually%20annotated%202D%20hand%20poses%20with%20triangulated%203D%20ground%20truth%2C%20recorded%20in%20a%20replica%20operating%20room%20under%20varying%20levels%20of%20scene%20complexity.%0A%20%20Results%3A%20Quantitative%20experiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%20baselines%2C%20achieving%20a%2031%25%20reduction%20in%202D%20mean%20joint%20error%20and%20a%2076%25%20reduction%20in%203D%20mean%20per-joint%20position%20error.%0A%20%20Conclusion%3A%20Our%20work%20establishes%20a%20strong%20baseline%20for%203D%20hand%20pose%20estimation%20in%20surgery%2C%20providing%20both%20a%20training-free%20pipeline%20and%20a%20comprehensive%20annotated%20dataset%20to%20facilitate%20future%20research%20in%20surgical%20computer%20vision.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-View%2520Pipeline%2520and%2520Benchmark%2520Dataset%2520for%25203D%2520Hand%2520Pose%2520Estimation%2520in%2520Surgery%26entry.906535625%3DValery%2520Fischer%2520and%2520Alan%2520Magdaleno%2520and%2520Anna-Katharina%2520Calek%2520and%2520Nicola%2520Cavalcanti%2520and%2520Nathan%2520Hoffman%2520and%2520Christoph%2520Germann%2520and%2520Joschua%2520W%25C3%25BCthrich%2520and%2520Max%2520Kr%25C3%25A4henmann%2520and%2520Mazda%2520Farshad%2520and%2520Philipp%2520F%25C3%25BCrnstahl%2520and%2520Lilian%2520Calvet%26entry.1292438233%3DPurpose%253A%2520Accurate%25203D%2520hand%2520pose%2520estimation%2520supports%2520surgical%2520applications%2520such%2520as%2520skill%2520assessment%252C%2520robot-assisted%2520interventions%252C%2520and%2520geometry-aware%2520workflow%2520analysis.%2520However%252C%2520surgical%2520environments%2520pose%2520severe%2520challenges%252C%2520including%2520intense%2520and%2520localized%2520lighting%252C%2520frequent%2520occlusions%2520by%2520instruments%2520or%2520staff%252C%2520and%2520uniform%2520hand%2520appearance%2520due%2520to%2520gloves%252C%2520combined%2520with%2520a%2520scarcity%2520of%2520annotated%2520datasets%2520for%2520reliable%2520model%2520training.%250A%2520%2520Method%253A%2520We%2520propose%2520a%2520robust%2520multi-view%2520pipeline%2520for%25203D%2520hand%2520pose%2520estimation%2520in%2520surgical%2520contexts%2520that%2520requires%2520no%2520domain-specific%2520fine-tuning%2520and%2520relies%2520solely%2520on%2520off-the-shelf%2520pretrained%2520models.%2520The%2520pipeline%2520integrates%2520reliable%2520person%2520detection%252C%2520whole-body%2520pose%2520estimation%252C%2520and%2520state-of-the-art%25202D%2520hand%2520keypoint%2520prediction%2520on%2520tracked%2520hand%2520crops%252C%2520followed%2520by%2520a%2520constrained%25203D%2520optimization.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520novel%2520surgical%2520benchmark%2520dataset%2520comprising%2520over%252068%252C000%2520frames%2520and%25203%252C000%2520manually%2520annotated%25202D%2520hand%2520poses%2520with%2520triangulated%25203D%2520ground%2520truth%252C%2520recorded%2520in%2520a%2520replica%2520operating%2520room%2520under%2520varying%2520levels%2520of%2520scene%2520complexity.%250A%2520%2520Results%253A%2520Quantitative%2520experiments%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520baselines%252C%2520achieving%2520a%252031%2525%2520reduction%2520in%25202D%2520mean%2520joint%2520error%2520and%2520a%252076%2525%2520reduction%2520in%25203D%2520mean%2520per-joint%2520position%2520error.%250A%2520%2520Conclusion%253A%2520Our%2520work%2520establishes%2520a%2520strong%2520baseline%2520for%25203D%2520hand%2520pose%2520estimation%2520in%2520surgery%252C%2520providing%2520both%2520a%2520training-free%2520pipeline%2520and%2520a%2520comprehensive%2520annotated%2520dataset%2520to%2520facilitate%2520future%2520research%2520in%2520surgical%2520computer%2520vision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-View%20Pipeline%20and%20Benchmark%20Dataset%20for%203D%20Hand%20Pose%20Estimation%20in%20Surgery&entry.906535625=Valery%20Fischer%20and%20Alan%20Magdaleno%20and%20Anna-Katharina%20Calek%20and%20Nicola%20Cavalcanti%20and%20Nathan%20Hoffman%20and%20Christoph%20Germann%20and%20Joschua%20W%C3%BCthrich%20and%20Max%20Kr%C3%A4henmann%20and%20Mazda%20Farshad%20and%20Philipp%20F%C3%BCrnstahl%20and%20Lilian%20Calvet&entry.1292438233=Purpose%3A%20Accurate%203D%20hand%20pose%20estimation%20supports%20surgical%20applications%20such%20as%20skill%20assessment%2C%20robot-assisted%20interventions%2C%20and%20geometry-aware%20workflow%20analysis.%20However%2C%20surgical%20environments%20pose%20severe%20challenges%2C%20including%20intense%20and%20localized%20lighting%2C%20frequent%20occlusions%20by%20instruments%20or%20staff%2C%20and%20uniform%20hand%20appearance%20due%20to%20gloves%2C%20combined%20with%20a%20scarcity%20of%20annotated%20datasets%20for%20reliable%20model%20training.%0A%20%20Method%3A%20We%20propose%20a%20robust%20multi-view%20pipeline%20for%203D%20hand%20pose%20estimation%20in%20surgical%20contexts%20that%20requires%20no%20domain-specific%20fine-tuning%20and%20relies%20solely%20on%20off-the-shelf%20pretrained%20models.%20The%20pipeline%20integrates%20reliable%20person%20detection%2C%20whole-body%20pose%20estimation%2C%20and%20state-of-the-art%202D%20hand%20keypoint%20prediction%20on%20tracked%20hand%20crops%2C%20followed%20by%20a%20constrained%203D%20optimization.%20In%20addition%2C%20we%20introduce%20a%20novel%20surgical%20benchmark%20dataset%20comprising%20over%2068%2C000%20frames%20and%203%2C000%20manually%20annotated%202D%20hand%20poses%20with%20triangulated%203D%20ground%20truth%2C%20recorded%20in%20a%20replica%20operating%20room%20under%20varying%20levels%20of%20scene%20complexity.%0A%20%20Results%3A%20Quantitative%20experiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%20baselines%2C%20achieving%20a%2031%25%20reduction%20in%202D%20mean%20joint%20error%20and%20a%2076%25%20reduction%20in%203D%20mean%20per-joint%20position%20error.%0A%20%20Conclusion%3A%20Our%20work%20establishes%20a%20strong%20baseline%20for%203D%20hand%20pose%20estimation%20in%20surgery%2C%20providing%20both%20a%20training-free%20pipeline%20and%20a%20comprehensive%20annotated%20dataset%20to%20facilitate%20future%20research%20in%20surgical%20computer%20vision.&entry.1838667208=http%3A//arxiv.org/abs/2601.15918v1&entry.124074799=Read"},
{"title": "Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment", "author": "Libo Wang", "abstract": "To address a fundamental limitation in cognitive systems, namely the absence of a time-updatable mediating thought space between semantics and continuous control, this work constructs and trains a vision-language-action model termed Sigma, deployed on a single RTX 4090. The model is built upon the open-source pi0.5_base backbone, with the svla_so101_pickplace dataset preprocessed into a structured training corpus. An independently designed VLA architecture is introduced to integrate deep semantic understanding with associative reasoning, enabling telepathic-style alignment between perception and action. Training proceeds through iterative optimization of data preprocessing, LoRA-based fine-tuning, and inference-stage adapter design. Evaluation is conducted using offline closed-loop replay, comparing Sigma against the untuned pi0.5_base under identical data conditions. Experimental results indicate a consistent reduction in control MSE across vector-, fragment-, and trajectory-level scales, while preserving the stability of the telepathy norm and semantic-text alignment quality. These findings demonstrate that mind-responsive alignment control can be quantitatively achieved through semantic and associative architectural integration without retraining the base model, providing a reproducible pathway for semantic alignment and intention-driven behavior.", "link": "http://arxiv.org/abs/2512.00783v3", "date": "2026-01-22", "relevancy": 2.2435, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sigma%3A%20The%20Key%20for%20Vision-Language-Action%20Models%20toward%20Telepathic%20Alignment&body=Title%3A%20Sigma%3A%20The%20Key%20for%20Vision-Language-Action%20Models%20toward%20Telepathic%20Alignment%0AAuthor%3A%20Libo%20Wang%0AAbstract%3A%20To%20address%20a%20fundamental%20limitation%20in%20cognitive%20systems%2C%20namely%20the%20absence%20of%20a%20time-updatable%20mediating%20thought%20space%20between%20semantics%20and%20continuous%20control%2C%20this%20work%20constructs%20and%20trains%20a%20vision-language-action%20model%20termed%20Sigma%2C%20deployed%20on%20a%20single%20RTX%204090.%20The%20model%20is%20built%20upon%20the%20open-source%20pi0.5_base%20backbone%2C%20with%20the%20svla_so101_pickplace%20dataset%20preprocessed%20into%20a%20structured%20training%20corpus.%20An%20independently%20designed%20VLA%20architecture%20is%20introduced%20to%20integrate%20deep%20semantic%20understanding%20with%20associative%20reasoning%2C%20enabling%20telepathic-style%20alignment%20between%20perception%20and%20action.%20Training%20proceeds%20through%20iterative%20optimization%20of%20data%20preprocessing%2C%20LoRA-based%20fine-tuning%2C%20and%20inference-stage%20adapter%20design.%20Evaluation%20is%20conducted%20using%20offline%20closed-loop%20replay%2C%20comparing%20Sigma%20against%20the%20untuned%20pi0.5_base%20under%20identical%20data%20conditions.%20Experimental%20results%20indicate%20a%20consistent%20reduction%20in%20control%20MSE%20across%20vector-%2C%20fragment-%2C%20and%20trajectory-level%20scales%2C%20while%20preserving%20the%20stability%20of%20the%20telepathy%20norm%20and%20semantic-text%20alignment%20quality.%20These%20findings%20demonstrate%20that%20mind-responsive%20alignment%20control%20can%20be%20quantitatively%20achieved%20through%20semantic%20and%20associative%20architectural%20integration%20without%20retraining%20the%20base%20model%2C%20providing%20a%20reproducible%20pathway%20for%20semantic%20alignment%20and%20intention-driven%20behavior.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00783v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSigma%253A%2520The%2520Key%2520for%2520Vision-Language-Action%2520Models%2520toward%2520Telepathic%2520Alignment%26entry.906535625%3DLibo%2520Wang%26entry.1292438233%3DTo%2520address%2520a%2520fundamental%2520limitation%2520in%2520cognitive%2520systems%252C%2520namely%2520the%2520absence%2520of%2520a%2520time-updatable%2520mediating%2520thought%2520space%2520between%2520semantics%2520and%2520continuous%2520control%252C%2520this%2520work%2520constructs%2520and%2520trains%2520a%2520vision-language-action%2520model%2520termed%2520Sigma%252C%2520deployed%2520on%2520a%2520single%2520RTX%25204090.%2520The%2520model%2520is%2520built%2520upon%2520the%2520open-source%2520pi0.5_base%2520backbone%252C%2520with%2520the%2520svla_so101_pickplace%2520dataset%2520preprocessed%2520into%2520a%2520structured%2520training%2520corpus.%2520An%2520independently%2520designed%2520VLA%2520architecture%2520is%2520introduced%2520to%2520integrate%2520deep%2520semantic%2520understanding%2520with%2520associative%2520reasoning%252C%2520enabling%2520telepathic-style%2520alignment%2520between%2520perception%2520and%2520action.%2520Training%2520proceeds%2520through%2520iterative%2520optimization%2520of%2520data%2520preprocessing%252C%2520LoRA-based%2520fine-tuning%252C%2520and%2520inference-stage%2520adapter%2520design.%2520Evaluation%2520is%2520conducted%2520using%2520offline%2520closed-loop%2520replay%252C%2520comparing%2520Sigma%2520against%2520the%2520untuned%2520pi0.5_base%2520under%2520identical%2520data%2520conditions.%2520Experimental%2520results%2520indicate%2520a%2520consistent%2520reduction%2520in%2520control%2520MSE%2520across%2520vector-%252C%2520fragment-%252C%2520and%2520trajectory-level%2520scales%252C%2520while%2520preserving%2520the%2520stability%2520of%2520the%2520telepathy%2520norm%2520and%2520semantic-text%2520alignment%2520quality.%2520These%2520findings%2520demonstrate%2520that%2520mind-responsive%2520alignment%2520control%2520can%2520be%2520quantitatively%2520achieved%2520through%2520semantic%2520and%2520associative%2520architectural%2520integration%2520without%2520retraining%2520the%2520base%2520model%252C%2520providing%2520a%2520reproducible%2520pathway%2520for%2520semantic%2520alignment%2520and%2520intention-driven%2520behavior.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00783v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sigma%3A%20The%20Key%20for%20Vision-Language-Action%20Models%20toward%20Telepathic%20Alignment&entry.906535625=Libo%20Wang&entry.1292438233=To%20address%20a%20fundamental%20limitation%20in%20cognitive%20systems%2C%20namely%20the%20absence%20of%20a%20time-updatable%20mediating%20thought%20space%20between%20semantics%20and%20continuous%20control%2C%20this%20work%20constructs%20and%20trains%20a%20vision-language-action%20model%20termed%20Sigma%2C%20deployed%20on%20a%20single%20RTX%204090.%20The%20model%20is%20built%20upon%20the%20open-source%20pi0.5_base%20backbone%2C%20with%20the%20svla_so101_pickplace%20dataset%20preprocessed%20into%20a%20structured%20training%20corpus.%20An%20independently%20designed%20VLA%20architecture%20is%20introduced%20to%20integrate%20deep%20semantic%20understanding%20with%20associative%20reasoning%2C%20enabling%20telepathic-style%20alignment%20between%20perception%20and%20action.%20Training%20proceeds%20through%20iterative%20optimization%20of%20data%20preprocessing%2C%20LoRA-based%20fine-tuning%2C%20and%20inference-stage%20adapter%20design.%20Evaluation%20is%20conducted%20using%20offline%20closed-loop%20replay%2C%20comparing%20Sigma%20against%20the%20untuned%20pi0.5_base%20under%20identical%20data%20conditions.%20Experimental%20results%20indicate%20a%20consistent%20reduction%20in%20control%20MSE%20across%20vector-%2C%20fragment-%2C%20and%20trajectory-level%20scales%2C%20while%20preserving%20the%20stability%20of%20the%20telepathy%20norm%20and%20semantic-text%20alignment%20quality.%20These%20findings%20demonstrate%20that%20mind-responsive%20alignment%20control%20can%20be%20quantitatively%20achieved%20through%20semantic%20and%20associative%20architectural%20integration%20without%20retraining%20the%20base%20model%2C%20providing%20a%20reproducible%20pathway%20for%20semantic%20alignment%20and%20intention-driven%20behavior.&entry.1838667208=http%3A//arxiv.org/abs/2512.00783v3&entry.124074799=Read"},
{"title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search", "author": "Xiangyu Wang and Zhixin Lv and Yongjiao Sun and Anrui Han and Ye Yuan and Hangxu Ji", "abstract": "Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.", "link": "http://arxiv.org/abs/2601.15931v1", "date": "2026-01-22", "relevancy": 2.2419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICON%3A%20Invariant%20Counterfactual%20Optimization%20with%20Neuro-Symbolic%20Priors%20for%20Text-Based%20Person%20Search&body=Title%3A%20ICON%3A%20Invariant%20Counterfactual%20Optimization%20with%20Neuro-Symbolic%20Priors%20for%20Text-Based%20Person%20Search%0AAuthor%3A%20Xiangyu%20Wang%20and%20Zhixin%20Lv%20and%20Yongjiao%20Sun%20and%20Anrui%20Han%20and%20Ye%20Yuan%20and%20Hangxu%20Ji%0AAbstract%3A%20Text-Based%20Person%20Search%20%28TBPS%29%20holds%20unique%20value%20in%20real-world%20surveillance%20bridging%20visual%20perception%20and%20language%20understanding%2C%20yet%20current%20paradigms%20utilizing%20pre-training%20models%20often%20fail%20to%20transfer%20effectively%20to%20complex%20open-world%20scenarios.%20The%20reliance%20on%20%22Passive%20Observation%22%20leads%20to%20multifaceted%20spurious%20correlations%20and%20spatial%20semantic%20misalignment%2C%20causing%20a%20lack%20of%20robustness%20against%20distribution%20shifts.%20To%20fundamentally%20resolve%20these%20defects%2C%20this%20paper%20proposes%20ICON%20%28Invariant%20Counterfactual%20Optimization%20with%20Neuro-symbolic%20priors%29%2C%20a%20framework%20integrating%20causal%20and%20topological%20priors.%20First%2C%20we%20introduce%20Rule-Guided%20Spatial%20Intervention%20to%20strictly%20penalize%20sensitivity%20to%20bounding%20box%20noise%2C%20forcibly%20severing%20location%20shortcuts%20to%20achieve%20geometric%20invariance.%20Second%2C%20Counterfactual%20Context%20Disentanglement%20is%20implemented%20via%20semantic-driven%20background%20transplantation%2C%20compelling%20the%20model%20to%20ignore%20background%20interference%20for%20environmental%20independence.%20Then%2C%20we%20employ%20Saliency-Driven%20Semantic%20Regularization%20with%20adaptive%20masking%20to%20resolve%20local%20saliency%20bias%20and%20guarantee%20holistic%20completeness.%20Finally%2C%20Neuro-Symbolic%20Topological%20Alignment%20utilizes%20neuro-symbolic%20priors%20to%20constrain%20feature%20matching%2C%20ensuring%20activated%20regions%20are%20topologically%20consistent%20with%20human%20structural%20logic.%20Experimental%20results%20demonstrate%20that%20ICON%20not%20only%20maintains%20leading%20performance%20on%20standard%20benchmarks%20but%20also%20exhibits%20exceptional%20robustness%20against%20occlusion%2C%20background%20interference%2C%20and%20localization%20noise.%20This%20approach%20effectively%20advances%20the%20field%20by%20shifting%20from%20fitting%20statistical%20co-occurrences%20to%20learning%20causal%20invariance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICON%253A%2520Invariant%2520Counterfactual%2520Optimization%2520with%2520Neuro-Symbolic%2520Priors%2520for%2520Text-Based%2520Person%2520Search%26entry.906535625%3DXiangyu%2520Wang%2520and%2520Zhixin%2520Lv%2520and%2520Yongjiao%2520Sun%2520and%2520Anrui%2520Han%2520and%2520Ye%2520Yuan%2520and%2520Hangxu%2520Ji%26entry.1292438233%3DText-Based%2520Person%2520Search%2520%2528TBPS%2529%2520holds%2520unique%2520value%2520in%2520real-world%2520surveillance%2520bridging%2520visual%2520perception%2520and%2520language%2520understanding%252C%2520yet%2520current%2520paradigms%2520utilizing%2520pre-training%2520models%2520often%2520fail%2520to%2520transfer%2520effectively%2520to%2520complex%2520open-world%2520scenarios.%2520The%2520reliance%2520on%2520%2522Passive%2520Observation%2522%2520leads%2520to%2520multifaceted%2520spurious%2520correlations%2520and%2520spatial%2520semantic%2520misalignment%252C%2520causing%2520a%2520lack%2520of%2520robustness%2520against%2520distribution%2520shifts.%2520To%2520fundamentally%2520resolve%2520these%2520defects%252C%2520this%2520paper%2520proposes%2520ICON%2520%2528Invariant%2520Counterfactual%2520Optimization%2520with%2520Neuro-symbolic%2520priors%2529%252C%2520a%2520framework%2520integrating%2520causal%2520and%2520topological%2520priors.%2520First%252C%2520we%2520introduce%2520Rule-Guided%2520Spatial%2520Intervention%2520to%2520strictly%2520penalize%2520sensitivity%2520to%2520bounding%2520box%2520noise%252C%2520forcibly%2520severing%2520location%2520shortcuts%2520to%2520achieve%2520geometric%2520invariance.%2520Second%252C%2520Counterfactual%2520Context%2520Disentanglement%2520is%2520implemented%2520via%2520semantic-driven%2520background%2520transplantation%252C%2520compelling%2520the%2520model%2520to%2520ignore%2520background%2520interference%2520for%2520environmental%2520independence.%2520Then%252C%2520we%2520employ%2520Saliency-Driven%2520Semantic%2520Regularization%2520with%2520adaptive%2520masking%2520to%2520resolve%2520local%2520saliency%2520bias%2520and%2520guarantee%2520holistic%2520completeness.%2520Finally%252C%2520Neuro-Symbolic%2520Topological%2520Alignment%2520utilizes%2520neuro-symbolic%2520priors%2520to%2520constrain%2520feature%2520matching%252C%2520ensuring%2520activated%2520regions%2520are%2520topologically%2520consistent%2520with%2520human%2520structural%2520logic.%2520Experimental%2520results%2520demonstrate%2520that%2520ICON%2520not%2520only%2520maintains%2520leading%2520performance%2520on%2520standard%2520benchmarks%2520but%2520also%2520exhibits%2520exceptional%2520robustness%2520against%2520occlusion%252C%2520background%2520interference%252C%2520and%2520localization%2520noise.%2520This%2520approach%2520effectively%2520advances%2520the%2520field%2520by%2520shifting%2520from%2520fitting%2520statistical%2520co-occurrences%2520to%2520learning%2520causal%2520invariance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICON%3A%20Invariant%20Counterfactual%20Optimization%20with%20Neuro-Symbolic%20Priors%20for%20Text-Based%20Person%20Search&entry.906535625=Xiangyu%20Wang%20and%20Zhixin%20Lv%20and%20Yongjiao%20Sun%20and%20Anrui%20Han%20and%20Ye%20Yuan%20and%20Hangxu%20Ji&entry.1292438233=Text-Based%20Person%20Search%20%28TBPS%29%20holds%20unique%20value%20in%20real-world%20surveillance%20bridging%20visual%20perception%20and%20language%20understanding%2C%20yet%20current%20paradigms%20utilizing%20pre-training%20models%20often%20fail%20to%20transfer%20effectively%20to%20complex%20open-world%20scenarios.%20The%20reliance%20on%20%22Passive%20Observation%22%20leads%20to%20multifaceted%20spurious%20correlations%20and%20spatial%20semantic%20misalignment%2C%20causing%20a%20lack%20of%20robustness%20against%20distribution%20shifts.%20To%20fundamentally%20resolve%20these%20defects%2C%20this%20paper%20proposes%20ICON%20%28Invariant%20Counterfactual%20Optimization%20with%20Neuro-symbolic%20priors%29%2C%20a%20framework%20integrating%20causal%20and%20topological%20priors.%20First%2C%20we%20introduce%20Rule-Guided%20Spatial%20Intervention%20to%20strictly%20penalize%20sensitivity%20to%20bounding%20box%20noise%2C%20forcibly%20severing%20location%20shortcuts%20to%20achieve%20geometric%20invariance.%20Second%2C%20Counterfactual%20Context%20Disentanglement%20is%20implemented%20via%20semantic-driven%20background%20transplantation%2C%20compelling%20the%20model%20to%20ignore%20background%20interference%20for%20environmental%20independence.%20Then%2C%20we%20employ%20Saliency-Driven%20Semantic%20Regularization%20with%20adaptive%20masking%20to%20resolve%20local%20saliency%20bias%20and%20guarantee%20holistic%20completeness.%20Finally%2C%20Neuro-Symbolic%20Topological%20Alignment%20utilizes%20neuro-symbolic%20priors%20to%20constrain%20feature%20matching%2C%20ensuring%20activated%20regions%20are%20topologically%20consistent%20with%20human%20structural%20logic.%20Experimental%20results%20demonstrate%20that%20ICON%20not%20only%20maintains%20leading%20performance%20on%20standard%20benchmarks%20but%20also%20exhibits%20exceptional%20robustness%20against%20occlusion%2C%20background%20interference%2C%20and%20localization%20noise.%20This%20approach%20effectively%20advances%20the%20field%20by%20shifting%20from%20fitting%20statistical%20co-occurrences%20to%20learning%20causal%20invariance.&entry.1838667208=http%3A//arxiv.org/abs/2601.15931v1&entry.124074799=Read"},
{"title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "author": "Adam \u0160torek and Mukur Gupta and Samira Hajizadeh and Prashast Srivastava and Suman Jana", "abstract": "Large language models (LLMs) are increasingly deployed for understanding large codebases, but whether they understand operational semantics of long code context or rely on pattern matching shortcuts remains unclear. We distinguish between lexical recall (retrieving code verbatim) and semantic recall (understanding operational semantics). Evaluating 10 state-of-the-art LLMs, we find that while frontier models achieve near-perfect, position-independent lexical recall, semantic recall degrades severely when code is centrally positioned in long contexts. We introduce semantic recall sensitivity to measure whether tasks require understanding of code's operational semantics vs. permit pattern matching shortcuts. Through a novel counterfactual measurement method, we show that models rely heavily on pattern matching shortcuts to solve existing code understanding benchmarks. We propose a new task SemTrace, which achieves high semantic recall sensitivity through unpredictable operations; LLMs' accuracy exhibits severe positional effects, with median accuracy drops of 92.73% versus CRUXEval's 53.36% as the relevant code snippet approaches the middle of the input code context. Our findings suggest current evaluations substantially underestimate semantic recall failures in long context code understanding.", "link": "http://arxiv.org/abs/2505.13353v3", "date": "2026-01-22", "relevancy": 2.2387, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sense%20and%20Sensitivity%3A%20Examining%20the%20Influence%20of%20Semantic%20Recall%20on%20Long%20Context%20Code%20Reasoning&body=Title%3A%20Sense%20and%20Sensitivity%3A%20Examining%20the%20Influence%20of%20Semantic%20Recall%20on%20Long%20Context%20Code%20Reasoning%0AAuthor%3A%20Adam%20%C5%A0torek%20and%20Mukur%20Gupta%20and%20Samira%20Hajizadeh%20and%20Prashast%20Srivastava%20and%20Suman%20Jana%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20for%20understanding%20large%20codebases%2C%20but%20whether%20they%20understand%20operational%20semantics%20of%20long%20code%20context%20or%20rely%20on%20pattern%20matching%20shortcuts%20remains%20unclear.%20We%20distinguish%20between%20lexical%20recall%20%28retrieving%20code%20verbatim%29%20and%20semantic%20recall%20%28understanding%20operational%20semantics%29.%20Evaluating%2010%20state-of-the-art%20LLMs%2C%20we%20find%20that%20while%20frontier%20models%20achieve%20near-perfect%2C%20position-independent%20lexical%20recall%2C%20semantic%20recall%20degrades%20severely%20when%20code%20is%20centrally%20positioned%20in%20long%20contexts.%20We%20introduce%20semantic%20recall%20sensitivity%20to%20measure%20whether%20tasks%20require%20understanding%20of%20code%27s%20operational%20semantics%20vs.%20permit%20pattern%20matching%20shortcuts.%20Through%20a%20novel%20counterfactual%20measurement%20method%2C%20we%20show%20that%20models%20rely%20heavily%20on%20pattern%20matching%20shortcuts%20to%20solve%20existing%20code%20understanding%20benchmarks.%20We%20propose%20a%20new%20task%20SemTrace%2C%20which%20achieves%20high%20semantic%20recall%20sensitivity%20through%20unpredictable%20operations%3B%20LLMs%27%20accuracy%20exhibits%20severe%20positional%20effects%2C%20with%20median%20accuracy%20drops%20of%2092.73%25%20versus%20CRUXEval%27s%2053.36%25%20as%20the%20relevant%20code%20snippet%20approaches%20the%20middle%20of%20the%20input%20code%20context.%20Our%20findings%20suggest%20current%20evaluations%20substantially%20underestimate%20semantic%20recall%20failures%20in%20long%20context%20code%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2505.13353v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSense%2520and%2520Sensitivity%253A%2520Examining%2520the%2520Influence%2520of%2520Semantic%2520Recall%2520on%2520Long%2520Context%2520Code%2520Reasoning%26entry.906535625%3DAdam%2520%25C5%25A0torek%2520and%2520Mukur%2520Gupta%2520and%2520Samira%2520Hajizadeh%2520and%2520Prashast%2520Srivastava%2520and%2520Suman%2520Jana%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520for%2520understanding%2520large%2520codebases%252C%2520but%2520whether%2520they%2520understand%2520operational%2520semantics%2520of%2520long%2520code%2520context%2520or%2520rely%2520on%2520pattern%2520matching%2520shortcuts%2520remains%2520unclear.%2520We%2520distinguish%2520between%2520lexical%2520recall%2520%2528retrieving%2520code%2520verbatim%2529%2520and%2520semantic%2520recall%2520%2528understanding%2520operational%2520semantics%2529.%2520Evaluating%252010%2520state-of-the-art%2520LLMs%252C%2520we%2520find%2520that%2520while%2520frontier%2520models%2520achieve%2520near-perfect%252C%2520position-independent%2520lexical%2520recall%252C%2520semantic%2520recall%2520degrades%2520severely%2520when%2520code%2520is%2520centrally%2520positioned%2520in%2520long%2520contexts.%2520We%2520introduce%2520semantic%2520recall%2520sensitivity%2520to%2520measure%2520whether%2520tasks%2520require%2520understanding%2520of%2520code%2527s%2520operational%2520semantics%2520vs.%2520permit%2520pattern%2520matching%2520shortcuts.%2520Through%2520a%2520novel%2520counterfactual%2520measurement%2520method%252C%2520we%2520show%2520that%2520models%2520rely%2520heavily%2520on%2520pattern%2520matching%2520shortcuts%2520to%2520solve%2520existing%2520code%2520understanding%2520benchmarks.%2520We%2520propose%2520a%2520new%2520task%2520SemTrace%252C%2520which%2520achieves%2520high%2520semantic%2520recall%2520sensitivity%2520through%2520unpredictable%2520operations%253B%2520LLMs%2527%2520accuracy%2520exhibits%2520severe%2520positional%2520effects%252C%2520with%2520median%2520accuracy%2520drops%2520of%252092.73%2525%2520versus%2520CRUXEval%2527s%252053.36%2525%2520as%2520the%2520relevant%2520code%2520snippet%2520approaches%2520the%2520middle%2520of%2520the%2520input%2520code%2520context.%2520Our%2520findings%2520suggest%2520current%2520evaluations%2520substantially%2520underestimate%2520semantic%2520recall%2520failures%2520in%2520long%2520context%2520code%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13353v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sense%20and%20Sensitivity%3A%20Examining%20the%20Influence%20of%20Semantic%20Recall%20on%20Long%20Context%20Code%20Reasoning&entry.906535625=Adam%20%C5%A0torek%20and%20Mukur%20Gupta%20and%20Samira%20Hajizadeh%20and%20Prashast%20Srivastava%20and%20Suman%20Jana&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20for%20understanding%20large%20codebases%2C%20but%20whether%20they%20understand%20operational%20semantics%20of%20long%20code%20context%20or%20rely%20on%20pattern%20matching%20shortcuts%20remains%20unclear.%20We%20distinguish%20between%20lexical%20recall%20%28retrieving%20code%20verbatim%29%20and%20semantic%20recall%20%28understanding%20operational%20semantics%29.%20Evaluating%2010%20state-of-the-art%20LLMs%2C%20we%20find%20that%20while%20frontier%20models%20achieve%20near-perfect%2C%20position-independent%20lexical%20recall%2C%20semantic%20recall%20degrades%20severely%20when%20code%20is%20centrally%20positioned%20in%20long%20contexts.%20We%20introduce%20semantic%20recall%20sensitivity%20to%20measure%20whether%20tasks%20require%20understanding%20of%20code%27s%20operational%20semantics%20vs.%20permit%20pattern%20matching%20shortcuts.%20Through%20a%20novel%20counterfactual%20measurement%20method%2C%20we%20show%20that%20models%20rely%20heavily%20on%20pattern%20matching%20shortcuts%20to%20solve%20existing%20code%20understanding%20benchmarks.%20We%20propose%20a%20new%20task%20SemTrace%2C%20which%20achieves%20high%20semantic%20recall%20sensitivity%20through%20unpredictable%20operations%3B%20LLMs%27%20accuracy%20exhibits%20severe%20positional%20effects%2C%20with%20median%20accuracy%20drops%20of%2092.73%25%20versus%20CRUXEval%27s%2053.36%25%20as%20the%20relevant%20code%20snippet%20approaches%20the%20middle%20of%20the%20input%20code%20context.%20Our%20findings%20suggest%20current%20evaluations%20substantially%20underestimate%20semantic%20recall%20failures%20in%20long%20context%20code%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2505.13353v3&entry.124074799=Read"},
{"title": "ViSymRe: Vision-guided Multimodal Symbolic Regression", "author": "Da Li and Junping Yin and Jin Xu and Xinxin Li and Juan Zhang", "abstract": "Extracting simple mathematical expression from an observational dataset to describe complex natural phenomena is one of the core objectives of artificial intelligence (AI). This field is known as symbolic regression (SR). Traditional SR models are based on genetic programming (GP) or reinforcement learning (RL), facing well-known challenges, such as low efficiency and overfitting. Recent studies have integrated SR with large language models (LLMs), enabling fast zero-shot inference by learning mappings from millions of dataset-expression pairs. However, since the input and output are inherently different modalities, such models often struggle to converge effectively. In this paper, we introduce ViSymRe, a vision-guided multimodal SR model that incorporates the third resource, expression graph, to bridge the modality gap. Different from traditional multimodal models, ViSymRe is trained to extract vision, termed virtual vision, from datasets, without relying on the global availability of expression graphs, which addresses the essential challenge of visual SR, i.e., expression graphs are not available during inference. Evaluation results on multiple mainstream benchmarks show that ViSymRe achieves more competitive performance than the state-of-the-art dataset-only baselines. The expressions predicted by ViSymRe not only fit the dataset well but are also simple and structurally accurate, goals that SR models strive to achieve.", "link": "http://arxiv.org/abs/2412.11139v3", "date": "2026-01-22", "relevancy": 2.225, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViSymRe%3A%20Vision-guided%20Multimodal%20Symbolic%20Regression&body=Title%3A%20ViSymRe%3A%20Vision-guided%20Multimodal%20Symbolic%20Regression%0AAuthor%3A%20Da%20Li%20and%20Junping%20Yin%20and%20Jin%20Xu%20and%20Xinxin%20Li%20and%20Juan%20Zhang%0AAbstract%3A%20Extracting%20simple%20mathematical%20expression%20from%20an%20observational%20dataset%20to%20describe%20complex%20natural%20phenomena%20is%20one%20of%20the%20core%20objectives%20of%20artificial%20intelligence%20%28AI%29.%20This%20field%20is%20known%20as%20symbolic%20regression%20%28SR%29.%20Traditional%20SR%20models%20are%20based%20on%20genetic%20programming%20%28GP%29%20or%20reinforcement%20learning%20%28RL%29%2C%20facing%20well-known%20challenges%2C%20such%20as%20low%20efficiency%20and%20overfitting.%20Recent%20studies%20have%20integrated%20SR%20with%20large%20language%20models%20%28LLMs%29%2C%20enabling%20fast%20zero-shot%20inference%20by%20learning%20mappings%20from%20millions%20of%20dataset-expression%20pairs.%20However%2C%20since%20the%20input%20and%20output%20are%20inherently%20different%20modalities%2C%20such%20models%20often%20struggle%20to%20converge%20effectively.%20In%20this%20paper%2C%20we%20introduce%20ViSymRe%2C%20a%20vision-guided%20multimodal%20SR%20model%20that%20incorporates%20the%20third%20resource%2C%20expression%20graph%2C%20to%20bridge%20the%20modality%20gap.%20Different%20from%20traditional%20multimodal%20models%2C%20ViSymRe%20is%20trained%20to%20extract%20vision%2C%20termed%20virtual%20vision%2C%20from%20datasets%2C%20without%20relying%20on%20the%20global%20availability%20of%20expression%20graphs%2C%20which%20addresses%20the%20essential%20challenge%20of%20visual%20SR%2C%20i.e.%2C%20expression%20graphs%20are%20not%20available%20during%20inference.%20Evaluation%20results%20on%20multiple%20mainstream%20benchmarks%20show%20that%20ViSymRe%20achieves%20more%20competitive%20performance%20than%20the%20state-of-the-art%20dataset-only%20baselines.%20The%20expressions%20predicted%20by%20ViSymRe%20not%20only%20fit%20the%20dataset%20well%20but%20are%20also%20simple%20and%20structurally%20accurate%2C%20goals%20that%20SR%20models%20strive%20to%20achieve.%0ALink%3A%20http%3A//arxiv.org/abs/2412.11139v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViSymRe%253A%2520Vision-guided%2520Multimodal%2520Symbolic%2520Regression%26entry.906535625%3DDa%2520Li%2520and%2520Junping%2520Yin%2520and%2520Jin%2520Xu%2520and%2520Xinxin%2520Li%2520and%2520Juan%2520Zhang%26entry.1292438233%3DExtracting%2520simple%2520mathematical%2520expression%2520from%2520an%2520observational%2520dataset%2520to%2520describe%2520complex%2520natural%2520phenomena%2520is%2520one%2520of%2520the%2520core%2520objectives%2520of%2520artificial%2520intelligence%2520%2528AI%2529.%2520This%2520field%2520is%2520known%2520as%2520symbolic%2520regression%2520%2528SR%2529.%2520Traditional%2520SR%2520models%2520are%2520based%2520on%2520genetic%2520programming%2520%2528GP%2529%2520or%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520facing%2520well-known%2520challenges%252C%2520such%2520as%2520low%2520efficiency%2520and%2520overfitting.%2520Recent%2520studies%2520have%2520integrated%2520SR%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520enabling%2520fast%2520zero-shot%2520inference%2520by%2520learning%2520mappings%2520from%2520millions%2520of%2520dataset-expression%2520pairs.%2520However%252C%2520since%2520the%2520input%2520and%2520output%2520are%2520inherently%2520different%2520modalities%252C%2520such%2520models%2520often%2520struggle%2520to%2520converge%2520effectively.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520ViSymRe%252C%2520a%2520vision-guided%2520multimodal%2520SR%2520model%2520that%2520incorporates%2520the%2520third%2520resource%252C%2520expression%2520graph%252C%2520to%2520bridge%2520the%2520modality%2520gap.%2520Different%2520from%2520traditional%2520multimodal%2520models%252C%2520ViSymRe%2520is%2520trained%2520to%2520extract%2520vision%252C%2520termed%2520virtual%2520vision%252C%2520from%2520datasets%252C%2520without%2520relying%2520on%2520the%2520global%2520availability%2520of%2520expression%2520graphs%252C%2520which%2520addresses%2520the%2520essential%2520challenge%2520of%2520visual%2520SR%252C%2520i.e.%252C%2520expression%2520graphs%2520are%2520not%2520available%2520during%2520inference.%2520Evaluation%2520results%2520on%2520multiple%2520mainstream%2520benchmarks%2520show%2520that%2520ViSymRe%2520achieves%2520more%2520competitive%2520performance%2520than%2520the%2520state-of-the-art%2520dataset-only%2520baselines.%2520The%2520expressions%2520predicted%2520by%2520ViSymRe%2520not%2520only%2520fit%2520the%2520dataset%2520well%2520but%2520are%2520also%2520simple%2520and%2520structurally%2520accurate%252C%2520goals%2520that%2520SR%2520models%2520strive%2520to%2520achieve.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11139v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViSymRe%3A%20Vision-guided%20Multimodal%20Symbolic%20Regression&entry.906535625=Da%20Li%20and%20Junping%20Yin%20and%20Jin%20Xu%20and%20Xinxin%20Li%20and%20Juan%20Zhang&entry.1292438233=Extracting%20simple%20mathematical%20expression%20from%20an%20observational%20dataset%20to%20describe%20complex%20natural%20phenomena%20is%20one%20of%20the%20core%20objectives%20of%20artificial%20intelligence%20%28AI%29.%20This%20field%20is%20known%20as%20symbolic%20regression%20%28SR%29.%20Traditional%20SR%20models%20are%20based%20on%20genetic%20programming%20%28GP%29%20or%20reinforcement%20learning%20%28RL%29%2C%20facing%20well-known%20challenges%2C%20such%20as%20low%20efficiency%20and%20overfitting.%20Recent%20studies%20have%20integrated%20SR%20with%20large%20language%20models%20%28LLMs%29%2C%20enabling%20fast%20zero-shot%20inference%20by%20learning%20mappings%20from%20millions%20of%20dataset-expression%20pairs.%20However%2C%20since%20the%20input%20and%20output%20are%20inherently%20different%20modalities%2C%20such%20models%20often%20struggle%20to%20converge%20effectively.%20In%20this%20paper%2C%20we%20introduce%20ViSymRe%2C%20a%20vision-guided%20multimodal%20SR%20model%20that%20incorporates%20the%20third%20resource%2C%20expression%20graph%2C%20to%20bridge%20the%20modality%20gap.%20Different%20from%20traditional%20multimodal%20models%2C%20ViSymRe%20is%20trained%20to%20extract%20vision%2C%20termed%20virtual%20vision%2C%20from%20datasets%2C%20without%20relying%20on%20the%20global%20availability%20of%20expression%20graphs%2C%20which%20addresses%20the%20essential%20challenge%20of%20visual%20SR%2C%20i.e.%2C%20expression%20graphs%20are%20not%20available%20during%20inference.%20Evaluation%20results%20on%20multiple%20mainstream%20benchmarks%20show%20that%20ViSymRe%20achieves%20more%20competitive%20performance%20than%20the%20state-of-the-art%20dataset-only%20baselines.%20The%20expressions%20predicted%20by%20ViSymRe%20not%20only%20fit%20the%20dataset%20well%20but%20are%20also%20simple%20and%20structurally%20accurate%2C%20goals%20that%20SR%20models%20strive%20to%20achieve.&entry.1838667208=http%3A//arxiv.org/abs/2412.11139v3&entry.124074799=Read"},
{"title": "Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision", "author": "Yashuai Yan and Tobias Egle and Christian Ott and Dongheui Lee", "abstract": "We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.", "link": "http://arxiv.org/abs/2601.16109v1", "date": "2026-01-22", "relevancy": 2.2207, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6187}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5499}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Learning%20Robust%20Torque-based%20Locomotion%20Through%20Reinforcement%20with%20Model-Based%20Supervision&body=Title%3A%20Efficiently%20Learning%20Robust%20Torque-based%20Locomotion%20Through%20Reinforcement%20with%20Model-Based%20Supervision%0AAuthor%3A%20Yashuai%20Yan%20and%20Tobias%20Egle%20and%20Christian%20Ott%20and%20Dongheui%20Lee%0AAbstract%3A%20We%20propose%20a%20control%20framework%20that%20integrates%20model-based%20bipedal%20locomotion%20with%20residual%20reinforcement%20learning%20%28RL%29%20to%20achieve%20robust%20and%20adaptive%20walking%20in%20the%20presence%20of%20real-world%20uncertainties.%20Our%20approach%20leverages%20a%20model-based%20controller%2C%20comprising%20a%20Divergent%20Component%20of%20Motion%20%28DCM%29%20trajectory%20planner%20and%20a%20whole-body%20controller%2C%20as%20a%20reliable%20base%20policy.%20To%20address%20the%20uncertainties%20of%20inaccurate%20dynamics%20modeling%20and%20sensor%20noise%2C%20we%20introduce%20a%20residual%20policy%20trained%20through%20RL%20with%20domain%20randomization.%20Crucially%2C%20we%20employ%20a%20model-based%20oracle%20policy%2C%20which%20has%20privileged%20access%20to%20ground-truth%20dynamics%20during%20training%2C%20to%20supervise%20the%20residual%20policy%20via%20a%20novel%20supervised%20loss.%20This%20supervision%20enables%20the%20policy%20to%20efficiently%20learn%20corrective%20behaviors%20that%20compensate%20for%20unmodeled%20effects%20without%20extensive%20reward%20shaping.%20Our%20method%20demonstrates%20improved%20robustness%20and%20generalization%20across%20a%20range%20of%20randomized%20conditions%2C%20offering%20a%20scalable%20solution%20for%20sim-to-real%20transfer%20in%20bipedal%20locomotion.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Learning%2520Robust%2520Torque-based%2520Locomotion%2520Through%2520Reinforcement%2520with%2520Model-Based%2520Supervision%26entry.906535625%3DYashuai%2520Yan%2520and%2520Tobias%2520Egle%2520and%2520Christian%2520Ott%2520and%2520Dongheui%2520Lee%26entry.1292438233%3DWe%2520propose%2520a%2520control%2520framework%2520that%2520integrates%2520model-based%2520bipedal%2520locomotion%2520with%2520residual%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520achieve%2520robust%2520and%2520adaptive%2520walking%2520in%2520the%2520presence%2520of%2520real-world%2520uncertainties.%2520Our%2520approach%2520leverages%2520a%2520model-based%2520controller%252C%2520comprising%2520a%2520Divergent%2520Component%2520of%2520Motion%2520%2528DCM%2529%2520trajectory%2520planner%2520and%2520a%2520whole-body%2520controller%252C%2520as%2520a%2520reliable%2520base%2520policy.%2520To%2520address%2520the%2520uncertainties%2520of%2520inaccurate%2520dynamics%2520modeling%2520and%2520sensor%2520noise%252C%2520we%2520introduce%2520a%2520residual%2520policy%2520trained%2520through%2520RL%2520with%2520domain%2520randomization.%2520Crucially%252C%2520we%2520employ%2520a%2520model-based%2520oracle%2520policy%252C%2520which%2520has%2520privileged%2520access%2520to%2520ground-truth%2520dynamics%2520during%2520training%252C%2520to%2520supervise%2520the%2520residual%2520policy%2520via%2520a%2520novel%2520supervised%2520loss.%2520This%2520supervision%2520enables%2520the%2520policy%2520to%2520efficiently%2520learn%2520corrective%2520behaviors%2520that%2520compensate%2520for%2520unmodeled%2520effects%2520without%2520extensive%2520reward%2520shaping.%2520Our%2520method%2520demonstrates%2520improved%2520robustness%2520and%2520generalization%2520across%2520a%2520range%2520of%2520randomized%2520conditions%252C%2520offering%2520a%2520scalable%2520solution%2520for%2520sim-to-real%2520transfer%2520in%2520bipedal%2520locomotion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Learning%20Robust%20Torque-based%20Locomotion%20Through%20Reinforcement%20with%20Model-Based%20Supervision&entry.906535625=Yashuai%20Yan%20and%20Tobias%20Egle%20and%20Christian%20Ott%20and%20Dongheui%20Lee&entry.1292438233=We%20propose%20a%20control%20framework%20that%20integrates%20model-based%20bipedal%20locomotion%20with%20residual%20reinforcement%20learning%20%28RL%29%20to%20achieve%20robust%20and%20adaptive%20walking%20in%20the%20presence%20of%20real-world%20uncertainties.%20Our%20approach%20leverages%20a%20model-based%20controller%2C%20comprising%20a%20Divergent%20Component%20of%20Motion%20%28DCM%29%20trajectory%20planner%20and%20a%20whole-body%20controller%2C%20as%20a%20reliable%20base%20policy.%20To%20address%20the%20uncertainties%20of%20inaccurate%20dynamics%20modeling%20and%20sensor%20noise%2C%20we%20introduce%20a%20residual%20policy%20trained%20through%20RL%20with%20domain%20randomization.%20Crucially%2C%20we%20employ%20a%20model-based%20oracle%20policy%2C%20which%20has%20privileged%20access%20to%20ground-truth%20dynamics%20during%20training%2C%20to%20supervise%20the%20residual%20policy%20via%20a%20novel%20supervised%20loss.%20This%20supervision%20enables%20the%20policy%20to%20efficiently%20learn%20corrective%20behaviors%20that%20compensate%20for%20unmodeled%20effects%20without%20extensive%20reward%20shaping.%20Our%20method%20demonstrates%20improved%20robustness%20and%20generalization%20across%20a%20range%20of%20randomized%20conditions%2C%20offering%20a%20scalable%20solution%20for%20sim-to-real%20transfer%20in%20bipedal%20locomotion.&entry.1838667208=http%3A//arxiv.org/abs/2601.16109v1&entry.124074799=Read"},
{"title": "Paramanu: Compact and Competitive Monolingual Language Models for Low-Resource Morphologically Rich Indian Languages", "author": "Mitodru Niyogi and Eric Gaussier and Arnab Bhattacharya", "abstract": "Multilingual large language models (LLMs) are expensive to pretrain and often suffer from imbalances across languages and datasets, English-centric bias, tokenizer oversegmentation for morphologically rich low-resource languages, and the curse of multilinguality. We introduce PARAMANU, the first family of Indian-only autoregressive language models trained from scratch on open-source language-specific data for the five most spoken Indian languages: Bengali, Hindi, Marathi, Tamil, and Telugu. All models are designed for affordability and are trained on a single GPU with a budget under $1,000, allowing under-resourced researchers to build competitive language models. To address low-resource challenges, we develop morphology-aligned, low-fertility tokenizers, propose an interpolation-based method for token position indices in RoPE based scaling to train longer sequences efficiently. We also create instruction-tuning datasets in Bangla that are translated to the other four languages. Despite their small size (108M-367M parameters), Paramanu achieves a strong performance-efficiency tradeoff and outperforms most larger multilingual models across all five languages. Our collection is available at https://huggingface.co/collections/mitodru/paramanu.", "link": "http://arxiv.org/abs/2401.18034v3", "date": "2026-01-22", "relevancy": 2.208, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4545}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4442}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paramanu%3A%20Compact%20and%20Competitive%20Monolingual%20Language%20Models%20for%20Low-Resource%20Morphologically%20Rich%20Indian%20Languages&body=Title%3A%20Paramanu%3A%20Compact%20and%20Competitive%20Monolingual%20Language%20Models%20for%20Low-Resource%20Morphologically%20Rich%20Indian%20Languages%0AAuthor%3A%20Mitodru%20Niyogi%20and%20Eric%20Gaussier%20and%20Arnab%20Bhattacharya%0AAbstract%3A%20Multilingual%20large%20language%20models%20%28LLMs%29%20are%20expensive%20to%20pretrain%20and%20often%20suffer%20from%20imbalances%20across%20languages%20and%20datasets%2C%20English-centric%20bias%2C%20tokenizer%20oversegmentation%20for%20morphologically%20rich%20low-resource%20languages%2C%20and%20the%20curse%20of%20multilinguality.%20We%20introduce%20PARAMANU%2C%20the%20first%20family%20of%20Indian-only%20autoregressive%20language%20models%20trained%20from%20scratch%20on%20open-source%20language-specific%20data%20for%20the%20five%20most%20spoken%20Indian%20languages%3A%20Bengali%2C%20Hindi%2C%20Marathi%2C%20Tamil%2C%20and%20Telugu.%20All%20models%20are%20designed%20for%20affordability%20and%20are%20trained%20on%20a%20single%20GPU%20with%20a%20budget%20under%20%241%2C000%2C%20allowing%20under-resourced%20researchers%20to%20build%20competitive%20language%20models.%20To%20address%20low-resource%20challenges%2C%20we%20develop%20morphology-aligned%2C%20low-fertility%20tokenizers%2C%20propose%20an%20interpolation-based%20method%20for%20token%20position%20indices%20in%20RoPE%20based%20scaling%20to%20train%20longer%20sequences%20efficiently.%20We%20also%20create%20instruction-tuning%20datasets%20in%20Bangla%20that%20are%20translated%20to%20the%20other%20four%20languages.%20Despite%20their%20small%20size%20%28108M-367M%20parameters%29%2C%20Paramanu%20achieves%20a%20strong%20performance-efficiency%20tradeoff%20and%20outperforms%20most%20larger%20multilingual%20models%20across%20all%20five%20languages.%20Our%20collection%20is%20available%20at%20https%3A//huggingface.co/collections/mitodru/paramanu.%0ALink%3A%20http%3A//arxiv.org/abs/2401.18034v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParamanu%253A%2520Compact%2520and%2520Competitive%2520Monolingual%2520Language%2520Models%2520for%2520Low-Resource%2520Morphologically%2520Rich%2520Indian%2520Languages%26entry.906535625%3DMitodru%2520Niyogi%2520and%2520Eric%2520Gaussier%2520and%2520Arnab%2520Bhattacharya%26entry.1292438233%3DMultilingual%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520expensive%2520to%2520pretrain%2520and%2520often%2520suffer%2520from%2520imbalances%2520across%2520languages%2520and%2520datasets%252C%2520English-centric%2520bias%252C%2520tokenizer%2520oversegmentation%2520for%2520morphologically%2520rich%2520low-resource%2520languages%252C%2520and%2520the%2520curse%2520of%2520multilinguality.%2520We%2520introduce%2520PARAMANU%252C%2520the%2520first%2520family%2520of%2520Indian-only%2520autoregressive%2520language%2520models%2520trained%2520from%2520scratch%2520on%2520open-source%2520language-specific%2520data%2520for%2520the%2520five%2520most%2520spoken%2520Indian%2520languages%253A%2520Bengali%252C%2520Hindi%252C%2520Marathi%252C%2520Tamil%252C%2520and%2520Telugu.%2520All%2520models%2520are%2520designed%2520for%2520affordability%2520and%2520are%2520trained%2520on%2520a%2520single%2520GPU%2520with%2520a%2520budget%2520under%2520%25241%252C000%252C%2520allowing%2520under-resourced%2520researchers%2520to%2520build%2520competitive%2520language%2520models.%2520To%2520address%2520low-resource%2520challenges%252C%2520we%2520develop%2520morphology-aligned%252C%2520low-fertility%2520tokenizers%252C%2520propose%2520an%2520interpolation-based%2520method%2520for%2520token%2520position%2520indices%2520in%2520RoPE%2520based%2520scaling%2520to%2520train%2520longer%2520sequences%2520efficiently.%2520We%2520also%2520create%2520instruction-tuning%2520datasets%2520in%2520Bangla%2520that%2520are%2520translated%2520to%2520the%2520other%2520four%2520languages.%2520Despite%2520their%2520small%2520size%2520%2528108M-367M%2520parameters%2529%252C%2520Paramanu%2520achieves%2520a%2520strong%2520performance-efficiency%2520tradeoff%2520and%2520outperforms%2520most%2520larger%2520multilingual%2520models%2520across%2520all%2520five%2520languages.%2520Our%2520collection%2520is%2520available%2520at%2520https%253A//huggingface.co/collections/mitodru/paramanu.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.18034v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paramanu%3A%20Compact%20and%20Competitive%20Monolingual%20Language%20Models%20for%20Low-Resource%20Morphologically%20Rich%20Indian%20Languages&entry.906535625=Mitodru%20Niyogi%20and%20Eric%20Gaussier%20and%20Arnab%20Bhattacharya&entry.1292438233=Multilingual%20large%20language%20models%20%28LLMs%29%20are%20expensive%20to%20pretrain%20and%20often%20suffer%20from%20imbalances%20across%20languages%20and%20datasets%2C%20English-centric%20bias%2C%20tokenizer%20oversegmentation%20for%20morphologically%20rich%20low-resource%20languages%2C%20and%20the%20curse%20of%20multilinguality.%20We%20introduce%20PARAMANU%2C%20the%20first%20family%20of%20Indian-only%20autoregressive%20language%20models%20trained%20from%20scratch%20on%20open-source%20language-specific%20data%20for%20the%20five%20most%20spoken%20Indian%20languages%3A%20Bengali%2C%20Hindi%2C%20Marathi%2C%20Tamil%2C%20and%20Telugu.%20All%20models%20are%20designed%20for%20affordability%20and%20are%20trained%20on%20a%20single%20GPU%20with%20a%20budget%20under%20%241%2C000%2C%20allowing%20under-resourced%20researchers%20to%20build%20competitive%20language%20models.%20To%20address%20low-resource%20challenges%2C%20we%20develop%20morphology-aligned%2C%20low-fertility%20tokenizers%2C%20propose%20an%20interpolation-based%20method%20for%20token%20position%20indices%20in%20RoPE%20based%20scaling%20to%20train%20longer%20sequences%20efficiently.%20We%20also%20create%20instruction-tuning%20datasets%20in%20Bangla%20that%20are%20translated%20to%20the%20other%20four%20languages.%20Despite%20their%20small%20size%20%28108M-367M%20parameters%29%2C%20Paramanu%20achieves%20a%20strong%20performance-efficiency%20tradeoff%20and%20outperforms%20most%20larger%20multilingual%20models%20across%20all%20five%20languages.%20Our%20collection%20is%20available%20at%20https%3A//huggingface.co/collections/mitodru/paramanu.&entry.1838667208=http%3A//arxiv.org/abs/2401.18034v3&entry.124074799=Read"},
{"title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition", "author": "Geo Ahn and Inwoong Lee and Taeoh Kim and Minho Shim and Dongyoon Wee and Jinwoo Choi", "abstract": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.", "link": "http://arxiv.org/abs/2601.16211v1", "date": "2026-01-22", "relevancy": 2.2052, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Can%27t%20I%20Open%20My%20Drawer%3F%20Mitigating%20Object-Driven%20Shortcuts%20in%20Zero-Shot%20Compositional%20Action%20Recognition&body=Title%3A%20Why%20Can%27t%20I%20Open%20My%20Drawer%3F%20Mitigating%20Object-Driven%20Shortcuts%20in%20Zero-Shot%20Compositional%20Action%20Recognition%0AAuthor%3A%20Geo%20Ahn%20and%20Inwoong%20Lee%20and%20Taeoh%20Kim%20and%20Minho%20Shim%20and%20Dongyoon%20Wee%20and%20Jinwoo%20Choi%0AAbstract%3A%20We%20study%20Compositional%20Video%20Understanding%20%28CVU%29%2C%20where%20models%20must%20recognize%20verbs%20and%20objects%20and%20compose%20them%20to%20generalize%20to%20unseen%20combinations.%20We%20find%20that%20existing%20Zero-Shot%20Compositional%20Action%20Recognition%20%28ZS-CAR%29%20models%20fail%20primarily%20due%20to%20an%20overlooked%20failure%20mode%3A%20object-driven%20verb%20shortcuts.%20Through%20systematic%20analysis%2C%20we%20show%20that%20this%20behavior%20arises%20from%20two%20intertwined%20factors%3A%20severe%20sparsity%20and%20skewness%20of%20compositional%20supervision%2C%20and%20the%20asymmetric%20learning%20difficulty%20between%20verbs%20and%20objects.%20As%20training%20progresses%2C%20the%20existing%20ZS-CAR%20model%20increasingly%20ignores%20visual%20evidence%20and%20overfits%20to%20co-occurrence%20statistics.%20Consequently%2C%20the%20existing%20model%20does%20not%20gain%20the%20benefit%20of%20compositional%20recognition%20in%20unseen%20verb-object%20compositions.%20To%20address%20this%2C%20we%20propose%20RCORE%2C%20a%20simple%20and%20effective%20framework%20that%20enforces%20temporally%20grounded%20verb%20learning.%20RCORE%20introduces%20%28i%29%20a%20composition-aware%20augmentation%20that%20diversifies%20verb-object%20combinations%20without%20corrupting%20motion%20cues%2C%20and%20%28ii%29%20a%20temporal%20order%20regularization%20loss%20that%20penalizes%20shortcut%20behaviors%20by%20explicitly%20modeling%20temporal%20structure.%20Across%20two%20benchmarks%2C%20Sth-com%20and%20our%20newly%20constructed%20EK100-com%2C%20RCORE%20significantly%20improves%20unseen%20composition%20accuracy%2C%20reduces%20reliance%20on%20co-occurrence%20bias%2C%20and%20achieves%20consistently%20positive%20compositional%20gaps.%20Our%20findings%20reveal%20object-driven%20shortcuts%20as%20a%20critical%20limiting%20factor%20in%20ZS-CAR%20and%20demonstrate%20that%20addressing%20them%20is%20essential%20for%20robust%20compositional%20video%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Can%2527t%2520I%2520Open%2520My%2520Drawer%253F%2520Mitigating%2520Object-Driven%2520Shortcuts%2520in%2520Zero-Shot%2520Compositional%2520Action%2520Recognition%26entry.906535625%3DGeo%2520Ahn%2520and%2520Inwoong%2520Lee%2520and%2520Taeoh%2520Kim%2520and%2520Minho%2520Shim%2520and%2520Dongyoon%2520Wee%2520and%2520Jinwoo%2520Choi%26entry.1292438233%3DWe%2520study%2520Compositional%2520Video%2520Understanding%2520%2528CVU%2529%252C%2520where%2520models%2520must%2520recognize%2520verbs%2520and%2520objects%2520and%2520compose%2520them%2520to%2520generalize%2520to%2520unseen%2520combinations.%2520We%2520find%2520that%2520existing%2520Zero-Shot%2520Compositional%2520Action%2520Recognition%2520%2528ZS-CAR%2529%2520models%2520fail%2520primarily%2520due%2520to%2520an%2520overlooked%2520failure%2520mode%253A%2520object-driven%2520verb%2520shortcuts.%2520Through%2520systematic%2520analysis%252C%2520we%2520show%2520that%2520this%2520behavior%2520arises%2520from%2520two%2520intertwined%2520factors%253A%2520severe%2520sparsity%2520and%2520skewness%2520of%2520compositional%2520supervision%252C%2520and%2520the%2520asymmetric%2520learning%2520difficulty%2520between%2520verbs%2520and%2520objects.%2520As%2520training%2520progresses%252C%2520the%2520existing%2520ZS-CAR%2520model%2520increasingly%2520ignores%2520visual%2520evidence%2520and%2520overfits%2520to%2520co-occurrence%2520statistics.%2520Consequently%252C%2520the%2520existing%2520model%2520does%2520not%2520gain%2520the%2520benefit%2520of%2520compositional%2520recognition%2520in%2520unseen%2520verb-object%2520compositions.%2520To%2520address%2520this%252C%2520we%2520propose%2520RCORE%252C%2520a%2520simple%2520and%2520effective%2520framework%2520that%2520enforces%2520temporally%2520grounded%2520verb%2520learning.%2520RCORE%2520introduces%2520%2528i%2529%2520a%2520composition-aware%2520augmentation%2520that%2520diversifies%2520verb-object%2520combinations%2520without%2520corrupting%2520motion%2520cues%252C%2520and%2520%2528ii%2529%2520a%2520temporal%2520order%2520regularization%2520loss%2520that%2520penalizes%2520shortcut%2520behaviors%2520by%2520explicitly%2520modeling%2520temporal%2520structure.%2520Across%2520two%2520benchmarks%252C%2520Sth-com%2520and%2520our%2520newly%2520constructed%2520EK100-com%252C%2520RCORE%2520significantly%2520improves%2520unseen%2520composition%2520accuracy%252C%2520reduces%2520reliance%2520on%2520co-occurrence%2520bias%252C%2520and%2520achieves%2520consistently%2520positive%2520compositional%2520gaps.%2520Our%2520findings%2520reveal%2520object-driven%2520shortcuts%2520as%2520a%2520critical%2520limiting%2520factor%2520in%2520ZS-CAR%2520and%2520demonstrate%2520that%2520addressing%2520them%2520is%2520essential%2520for%2520robust%2520compositional%2520video%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Can%27t%20I%20Open%20My%20Drawer%3F%20Mitigating%20Object-Driven%20Shortcuts%20in%20Zero-Shot%20Compositional%20Action%20Recognition&entry.906535625=Geo%20Ahn%20and%20Inwoong%20Lee%20and%20Taeoh%20Kim%20and%20Minho%20Shim%20and%20Dongyoon%20Wee%20and%20Jinwoo%20Choi&entry.1292438233=We%20study%20Compositional%20Video%20Understanding%20%28CVU%29%2C%20where%20models%20must%20recognize%20verbs%20and%20objects%20and%20compose%20them%20to%20generalize%20to%20unseen%20combinations.%20We%20find%20that%20existing%20Zero-Shot%20Compositional%20Action%20Recognition%20%28ZS-CAR%29%20models%20fail%20primarily%20due%20to%20an%20overlooked%20failure%20mode%3A%20object-driven%20verb%20shortcuts.%20Through%20systematic%20analysis%2C%20we%20show%20that%20this%20behavior%20arises%20from%20two%20intertwined%20factors%3A%20severe%20sparsity%20and%20skewness%20of%20compositional%20supervision%2C%20and%20the%20asymmetric%20learning%20difficulty%20between%20verbs%20and%20objects.%20As%20training%20progresses%2C%20the%20existing%20ZS-CAR%20model%20increasingly%20ignores%20visual%20evidence%20and%20overfits%20to%20co-occurrence%20statistics.%20Consequently%2C%20the%20existing%20model%20does%20not%20gain%20the%20benefit%20of%20compositional%20recognition%20in%20unseen%20verb-object%20compositions.%20To%20address%20this%2C%20we%20propose%20RCORE%2C%20a%20simple%20and%20effective%20framework%20that%20enforces%20temporally%20grounded%20verb%20learning.%20RCORE%20introduces%20%28i%29%20a%20composition-aware%20augmentation%20that%20diversifies%20verb-object%20combinations%20without%20corrupting%20motion%20cues%2C%20and%20%28ii%29%20a%20temporal%20order%20regularization%20loss%20that%20penalizes%20shortcut%20behaviors%20by%20explicitly%20modeling%20temporal%20structure.%20Across%20two%20benchmarks%2C%20Sth-com%20and%20our%20newly%20constructed%20EK100-com%2C%20RCORE%20significantly%20improves%20unseen%20composition%20accuracy%2C%20reduces%20reliance%20on%20co-occurrence%20bias%2C%20and%20achieves%20consistently%20positive%20compositional%20gaps.%20Our%20findings%20reveal%20object-driven%20shortcuts%20as%20a%20critical%20limiting%20factor%20in%20ZS-CAR%20and%20demonstrate%20that%20addressing%20them%20is%20essential%20for%20robust%20compositional%20video%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2601.16211v1&entry.124074799=Read"},
{"title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance", "author": "Jongwoo Park and Kanchana Ranasinghe and Jinhyeok Jang and Cristina Mata and Yoo Sung Jang and Michael S Ryoo", "abstract": "Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA", "link": "http://arxiv.org/abs/2601.16207v1", "date": "2026-01-22", "relevancy": 2.1991, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IVRA%3A%20Improving%20Visual-Token%20Relations%20for%20Robot%20Action%20Policy%20with%20Training-Free%20Hint-Based%20Guidance&body=Title%3A%20IVRA%3A%20Improving%20Visual-Token%20Relations%20for%20Robot%20Action%20Policy%20with%20Training-Free%20Hint-Based%20Guidance%0AAuthor%3A%20Jongwoo%20Park%20and%20Kanchana%20Ranasinghe%20and%20Jinhyeok%20Jang%20and%20Cristina%20Mata%20and%20Yoo%20Sung%20Jang%20and%20Michael%20S%20Ryoo%0AAbstract%3A%20Many%20Vision-Language-Action%20%28VLA%29%20models%20flatten%20image%20patches%20into%20a%201D%20token%20sequence%2C%20weakening%20the%202D%20spatial%20cues%20needed%20for%20precise%20manipulation.%20We%20introduce%20IVRA%2C%20a%20lightweight%2C%20training-free%20method%20that%20improves%20spatial%20understanding%20by%20exploiting%20affinity%20hints%20already%20available%20in%20the%20model%27s%20built-in%20vision%20encoder%2C%20without%20requiring%20any%20external%20encoder%20or%20retraining.%20IVRA%20selectively%20injects%20these%20affinity%20signals%20into%20a%20language-model%20layer%20in%20which%20instance-level%20features%20reside.%20This%20inference-time%20intervention%20realigns%20visual-token%20interactions%20and%20better%20preserves%20geometric%20structure%20while%20keeping%20all%20model%20parameters%20fixed.%20We%20demonstrate%20the%20generality%20of%20IVRA%20by%20applying%20it%20to%20diverse%20VLA%20architectures%20%28LLaRA%2C%20OpenVLA%2C%20and%20FLOWER%29%20across%20simulated%20benchmarks%20spanning%20both%202D%20and%203D%20manipulation%20%28VIMA%20and%20LIBERO%29%20and%20on%20various%20real-robot%20tasks.%20On%202D%20VIMA%2C%20IVRA%20improves%20average%20success%20by%20%2B4.2%25%20over%20the%20baseline%20LLaRA%20in%20a%20low-data%20regime.%20On%203D%20LIBERO%2C%20it%20yields%20consistent%20gains%20over%20the%20OpenVLA%20and%20FLOWER%20baselines%2C%20including%20improvements%20when%20baseline%20accuracy%20is%20near%20saturation%20%2896.3%25%20to%2097.1%25%29.%20All%20code%20and%20models%20will%20be%20released%20publicly.%20Visualizations%20are%20available%20at%3A%20jongwoopark7978.github.io/IVRA%0ALink%3A%20http%3A//arxiv.org/abs/2601.16207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIVRA%253A%2520Improving%2520Visual-Token%2520Relations%2520for%2520Robot%2520Action%2520Policy%2520with%2520Training-Free%2520Hint-Based%2520Guidance%26entry.906535625%3DJongwoo%2520Park%2520and%2520Kanchana%2520Ranasinghe%2520and%2520Jinhyeok%2520Jang%2520and%2520Cristina%2520Mata%2520and%2520Yoo%2520Sung%2520Jang%2520and%2520Michael%2520S%2520Ryoo%26entry.1292438233%3DMany%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520flatten%2520image%2520patches%2520into%2520a%25201D%2520token%2520sequence%252C%2520weakening%2520the%25202D%2520spatial%2520cues%2520needed%2520for%2520precise%2520manipulation.%2520We%2520introduce%2520IVRA%252C%2520a%2520lightweight%252C%2520training-free%2520method%2520that%2520improves%2520spatial%2520understanding%2520by%2520exploiting%2520affinity%2520hints%2520already%2520available%2520in%2520the%2520model%2527s%2520built-in%2520vision%2520encoder%252C%2520without%2520requiring%2520any%2520external%2520encoder%2520or%2520retraining.%2520IVRA%2520selectively%2520injects%2520these%2520affinity%2520signals%2520into%2520a%2520language-model%2520layer%2520in%2520which%2520instance-level%2520features%2520reside.%2520This%2520inference-time%2520intervention%2520realigns%2520visual-token%2520interactions%2520and%2520better%2520preserves%2520geometric%2520structure%2520while%2520keeping%2520all%2520model%2520parameters%2520fixed.%2520We%2520demonstrate%2520the%2520generality%2520of%2520IVRA%2520by%2520applying%2520it%2520to%2520diverse%2520VLA%2520architectures%2520%2528LLaRA%252C%2520OpenVLA%252C%2520and%2520FLOWER%2529%2520across%2520simulated%2520benchmarks%2520spanning%2520both%25202D%2520and%25203D%2520manipulation%2520%2528VIMA%2520and%2520LIBERO%2529%2520and%2520on%2520various%2520real-robot%2520tasks.%2520On%25202D%2520VIMA%252C%2520IVRA%2520improves%2520average%2520success%2520by%2520%252B4.2%2525%2520over%2520the%2520baseline%2520LLaRA%2520in%2520a%2520low-data%2520regime.%2520On%25203D%2520LIBERO%252C%2520it%2520yields%2520consistent%2520gains%2520over%2520the%2520OpenVLA%2520and%2520FLOWER%2520baselines%252C%2520including%2520improvements%2520when%2520baseline%2520accuracy%2520is%2520near%2520saturation%2520%252896.3%2525%2520to%252097.1%2525%2529.%2520All%2520code%2520and%2520models%2520will%2520be%2520released%2520publicly.%2520Visualizations%2520are%2520available%2520at%253A%2520jongwoopark7978.github.io/IVRA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IVRA%3A%20Improving%20Visual-Token%20Relations%20for%20Robot%20Action%20Policy%20with%20Training-Free%20Hint-Based%20Guidance&entry.906535625=Jongwoo%20Park%20and%20Kanchana%20Ranasinghe%20and%20Jinhyeok%20Jang%20and%20Cristina%20Mata%20and%20Yoo%20Sung%20Jang%20and%20Michael%20S%20Ryoo&entry.1292438233=Many%20Vision-Language-Action%20%28VLA%29%20models%20flatten%20image%20patches%20into%20a%201D%20token%20sequence%2C%20weakening%20the%202D%20spatial%20cues%20needed%20for%20precise%20manipulation.%20We%20introduce%20IVRA%2C%20a%20lightweight%2C%20training-free%20method%20that%20improves%20spatial%20understanding%20by%20exploiting%20affinity%20hints%20already%20available%20in%20the%20model%27s%20built-in%20vision%20encoder%2C%20without%20requiring%20any%20external%20encoder%20or%20retraining.%20IVRA%20selectively%20injects%20these%20affinity%20signals%20into%20a%20language-model%20layer%20in%20which%20instance-level%20features%20reside.%20This%20inference-time%20intervention%20realigns%20visual-token%20interactions%20and%20better%20preserves%20geometric%20structure%20while%20keeping%20all%20model%20parameters%20fixed.%20We%20demonstrate%20the%20generality%20of%20IVRA%20by%20applying%20it%20to%20diverse%20VLA%20architectures%20%28LLaRA%2C%20OpenVLA%2C%20and%20FLOWER%29%20across%20simulated%20benchmarks%20spanning%20both%202D%20and%203D%20manipulation%20%28VIMA%20and%20LIBERO%29%20and%20on%20various%20real-robot%20tasks.%20On%202D%20VIMA%2C%20IVRA%20improves%20average%20success%20by%20%2B4.2%25%20over%20the%20baseline%20LLaRA%20in%20a%20low-data%20regime.%20On%203D%20LIBERO%2C%20it%20yields%20consistent%20gains%20over%20the%20OpenVLA%20and%20FLOWER%20baselines%2C%20including%20improvements%20when%20baseline%20accuracy%20is%20near%20saturation%20%2896.3%25%20to%2097.1%25%29.%20All%20code%20and%20models%20will%20be%20released%20publicly.%20Visualizations%20are%20available%20at%3A%20jongwoopark7978.github.io/IVRA&entry.1838667208=http%3A//arxiv.org/abs/2601.16207v1&entry.124074799=Read"},
{"title": "GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning", "author": "Shutong Ding and Ke Hu and Shan Zhong and Haoyang Luo and Weinan Zhang and Jingya Wang and Jun Wang and Ye Shi", "abstract": "Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO's superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.", "link": "http://arxiv.org/abs/2505.18763v4", "date": "2026-01-22", "relevancy": 2.1912, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5533}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5473}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenPO%3A%20Generative%20Diffusion%20Models%20Meet%20On-Policy%20Reinforcement%20Learning&body=Title%3A%20GenPO%3A%20Generative%20Diffusion%20Models%20Meet%20On-Policy%20Reinforcement%20Learning%0AAuthor%3A%20Shutong%20Ding%20and%20Ke%20Hu%20and%20Shan%20Zhong%20and%20Haoyang%20Luo%20and%20Weinan%20Zhang%20and%20Jingya%20Wang%20and%20Jun%20Wang%20and%20Ye%20Shi%0AAbstract%3A%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20demonstrated%20the%20powerful%20exploration%20capabilities%20and%20multimodality%20of%20generative%20diffusion-based%20policies.%20While%20substantial%20progress%20has%20been%20made%20in%20offline%20RL%20and%20off-policy%20RL%20settings%2C%20integrating%20diffusion%20policies%20into%20on-policy%20frameworks%20like%20PPO%20remains%20underexplored.%20This%20gap%20is%20particularly%20significant%20given%20the%20widespread%20use%20of%20large-scale%20parallel%20GPU-accelerated%20simulators%2C%20such%20as%20IsaacLab%2C%20which%20are%20optimized%20for%20on-policy%20RL%20algorithms%20and%20enable%20rapid%20training%20of%20complex%20robotic%20tasks.%20A%20key%20challenge%20lies%20in%20computing%20state-action%20log-likelihoods%20under%20diffusion%20policies%2C%20which%20is%20straightforward%20for%20Gaussian%20policies%20but%20intractable%20for%20flow-based%20models%20due%20to%20irreversible%20forward-reverse%20processes%20and%20discretization%20errors%20%28e.g.%2C%20Euler-Maruyama%20approximations%29.%20To%20bridge%20this%20gap%2C%20we%20propose%20GenPO%2C%20a%20generative%20policy%20optimization%20framework%20that%20leverages%20exact%20diffusion%20inversion%20to%20construct%20invertible%20action%20mappings.%20GenPO%20introduces%20a%20novel%20doubled%20dummy%20action%20mechanism%20that%20enables%20invertibility%20via%20alternating%20updates%2C%20resolving%20log-likelihood%20computation%20barriers.%20Furthermore%2C%20we%20also%20use%20the%20action%20log-likelihood%20for%20unbiased%20entropy%20and%20KL%20divergence%20estimation%2C%20enabling%20KL-adaptive%20learning%20rates%20and%20entropy%20regularization%20in%20on-policy%20updates.%20Extensive%20experiments%20on%20eight%20IsaacLab%20benchmarks%2C%20including%20legged%20locomotion%20%28Ant%2C%20Humanoid%2C%20Anymal-D%2C%20Unitree%20H1%2C%20Go2%29%2C%20dexterous%20manipulation%20%28Shadow%20Hand%29%2C%20aerial%20control%20%28Quadcopter%29%2C%20and%20robotic%20arm%20tasks%20%28Franka%29%2C%20demonstrate%20GenPO%27s%20superiority%20over%20existing%20RL%20baselines.%20Notably%2C%20GenPO%20is%20the%20first%20method%20to%20successfully%20integrate%20diffusion%20policies%20into%20on-policy%20RL%2C%20unlocking%20their%20potential%20for%20large-scale%20parallelized%20training%20and%20real-world%20robotic%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2505.18763v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenPO%253A%2520Generative%2520Diffusion%2520Models%2520Meet%2520On-Policy%2520Reinforcement%2520Learning%26entry.906535625%3DShutong%2520Ding%2520and%2520Ke%2520Hu%2520and%2520Shan%2520Zhong%2520and%2520Haoyang%2520Luo%2520and%2520Weinan%2520Zhang%2520and%2520Jingya%2520Wang%2520and%2520Jun%2520Wang%2520and%2520Ye%2520Shi%26entry.1292438233%3DRecent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520demonstrated%2520the%2520powerful%2520exploration%2520capabilities%2520and%2520multimodality%2520of%2520generative%2520diffusion-based%2520policies.%2520While%2520substantial%2520progress%2520has%2520been%2520made%2520in%2520offline%2520RL%2520and%2520off-policy%2520RL%2520settings%252C%2520integrating%2520diffusion%2520policies%2520into%2520on-policy%2520frameworks%2520like%2520PPO%2520remains%2520underexplored.%2520This%2520gap%2520is%2520particularly%2520significant%2520given%2520the%2520widespread%2520use%2520of%2520large-scale%2520parallel%2520GPU-accelerated%2520simulators%252C%2520such%2520as%2520IsaacLab%252C%2520which%2520are%2520optimized%2520for%2520on-policy%2520RL%2520algorithms%2520and%2520enable%2520rapid%2520training%2520of%2520complex%2520robotic%2520tasks.%2520A%2520key%2520challenge%2520lies%2520in%2520computing%2520state-action%2520log-likelihoods%2520under%2520diffusion%2520policies%252C%2520which%2520is%2520straightforward%2520for%2520Gaussian%2520policies%2520but%2520intractable%2520for%2520flow-based%2520models%2520due%2520to%2520irreversible%2520forward-reverse%2520processes%2520and%2520discretization%2520errors%2520%2528e.g.%252C%2520Euler-Maruyama%2520approximations%2529.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520GenPO%252C%2520a%2520generative%2520policy%2520optimization%2520framework%2520that%2520leverages%2520exact%2520diffusion%2520inversion%2520to%2520construct%2520invertible%2520action%2520mappings.%2520GenPO%2520introduces%2520a%2520novel%2520doubled%2520dummy%2520action%2520mechanism%2520that%2520enables%2520invertibility%2520via%2520alternating%2520updates%252C%2520resolving%2520log-likelihood%2520computation%2520barriers.%2520Furthermore%252C%2520we%2520also%2520use%2520the%2520action%2520log-likelihood%2520for%2520unbiased%2520entropy%2520and%2520KL%2520divergence%2520estimation%252C%2520enabling%2520KL-adaptive%2520learning%2520rates%2520and%2520entropy%2520regularization%2520in%2520on-policy%2520updates.%2520Extensive%2520experiments%2520on%2520eight%2520IsaacLab%2520benchmarks%252C%2520including%2520legged%2520locomotion%2520%2528Ant%252C%2520Humanoid%252C%2520Anymal-D%252C%2520Unitree%2520H1%252C%2520Go2%2529%252C%2520dexterous%2520manipulation%2520%2528Shadow%2520Hand%2529%252C%2520aerial%2520control%2520%2528Quadcopter%2529%252C%2520and%2520robotic%2520arm%2520tasks%2520%2528Franka%2529%252C%2520demonstrate%2520GenPO%2527s%2520superiority%2520over%2520existing%2520RL%2520baselines.%2520Notably%252C%2520GenPO%2520is%2520the%2520first%2520method%2520to%2520successfully%2520integrate%2520diffusion%2520policies%2520into%2520on-policy%2520RL%252C%2520unlocking%2520their%2520potential%2520for%2520large-scale%2520parallelized%2520training%2520and%2520real-world%2520robotic%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18763v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenPO%3A%20Generative%20Diffusion%20Models%20Meet%20On-Policy%20Reinforcement%20Learning&entry.906535625=Shutong%20Ding%20and%20Ke%20Hu%20and%20Shan%20Zhong%20and%20Haoyang%20Luo%20and%20Weinan%20Zhang%20and%20Jingya%20Wang%20and%20Jun%20Wang%20and%20Ye%20Shi&entry.1292438233=Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20demonstrated%20the%20powerful%20exploration%20capabilities%20and%20multimodality%20of%20generative%20diffusion-based%20policies.%20While%20substantial%20progress%20has%20been%20made%20in%20offline%20RL%20and%20off-policy%20RL%20settings%2C%20integrating%20diffusion%20policies%20into%20on-policy%20frameworks%20like%20PPO%20remains%20underexplored.%20This%20gap%20is%20particularly%20significant%20given%20the%20widespread%20use%20of%20large-scale%20parallel%20GPU-accelerated%20simulators%2C%20such%20as%20IsaacLab%2C%20which%20are%20optimized%20for%20on-policy%20RL%20algorithms%20and%20enable%20rapid%20training%20of%20complex%20robotic%20tasks.%20A%20key%20challenge%20lies%20in%20computing%20state-action%20log-likelihoods%20under%20diffusion%20policies%2C%20which%20is%20straightforward%20for%20Gaussian%20policies%20but%20intractable%20for%20flow-based%20models%20due%20to%20irreversible%20forward-reverse%20processes%20and%20discretization%20errors%20%28e.g.%2C%20Euler-Maruyama%20approximations%29.%20To%20bridge%20this%20gap%2C%20we%20propose%20GenPO%2C%20a%20generative%20policy%20optimization%20framework%20that%20leverages%20exact%20diffusion%20inversion%20to%20construct%20invertible%20action%20mappings.%20GenPO%20introduces%20a%20novel%20doubled%20dummy%20action%20mechanism%20that%20enables%20invertibility%20via%20alternating%20updates%2C%20resolving%20log-likelihood%20computation%20barriers.%20Furthermore%2C%20we%20also%20use%20the%20action%20log-likelihood%20for%20unbiased%20entropy%20and%20KL%20divergence%20estimation%2C%20enabling%20KL-adaptive%20learning%20rates%20and%20entropy%20regularization%20in%20on-policy%20updates.%20Extensive%20experiments%20on%20eight%20IsaacLab%20benchmarks%2C%20including%20legged%20locomotion%20%28Ant%2C%20Humanoid%2C%20Anymal-D%2C%20Unitree%20H1%2C%20Go2%29%2C%20dexterous%20manipulation%20%28Shadow%20Hand%29%2C%20aerial%20control%20%28Quadcopter%29%2C%20and%20robotic%20arm%20tasks%20%28Franka%29%2C%20demonstrate%20GenPO%27s%20superiority%20over%20existing%20RL%20baselines.%20Notably%2C%20GenPO%20is%20the%20first%20method%20to%20successfully%20integrate%20diffusion%20policies%20into%20on-policy%20RL%2C%20unlocking%20their%20potential%20for%20large-scale%20parallelized%20training%20and%20real-world%20robotic%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2505.18763v4&entry.124074799=Read"},
{"title": "Can professional translators identify machine-generated text?", "author": "Michael Farrell", "abstract": "This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.", "link": "http://arxiv.org/abs/2601.15828v1", "date": "2026-01-22", "relevancy": 2.1898, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4609}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4371}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20professional%20translators%20identify%20machine-generated%20text%3F&body=Title%3A%20Can%20professional%20translators%20identify%20machine-generated%20text%3F%0AAuthor%3A%20Michael%20Farrell%0AAbstract%3A%20This%20study%20investigates%20whether%20professional%20translators%20can%20reliably%20identify%20short%20stories%20generated%20in%20Italian%20by%20artificial%20intelligence%20%28AI%29%20without%20prior%20specialized%20training.%20Sixty-nine%20translators%20took%20part%20in%20an%20in-person%20experiment%2C%20where%20they%20assessed%20three%20anonymized%20short%20stories%20-%20two%20written%20by%20ChatGPT-4o%20and%20one%20by%20a%20human%20author.%20For%20each%20story%2C%20participants%20rated%20the%20likelihood%20of%20AI%20authorship%20and%20provided%20justifications%20for%20their%20choices.%20While%20average%20results%20were%20inconclusive%2C%20a%20statistically%20significant%20subset%20%2816.2%25%29%20successfully%20distinguished%20the%20synthetic%20texts%20from%20the%20human%20text%2C%20suggesting%20that%20their%20judgements%20were%20informed%20by%20analytical%20skill%20rather%20than%20chance.%20However%2C%20a%20nearly%20equal%20number%20misclassified%20the%20texts%20in%20the%20opposite%20direction%2C%20often%20relying%20on%20subjective%20impressions%20rather%20than%20objective%20markers%2C%20possibly%20reflecting%20a%20reader%20preference%20for%20AI-generated%20texts.%20Low%20burstiness%20and%20narrative%20contradiction%20emerged%20as%20the%20most%20reliable%20indicators%20of%20synthetic%20authorship%2C%20with%20unexpected%20calques%2C%20semantic%20loans%20and%20syntactic%20transfer%20from%20English%20also%20reported.%20In%20contrast%2C%20features%20such%20as%20grammatical%20accuracy%20and%20emotional%20tone%20frequently%20led%20to%20misclassification.%20These%20findings%20raise%20questions%20about%20the%20role%20and%20scope%20of%20synthetic-text%20editing%20in%20professional%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520professional%2520translators%2520identify%2520machine-generated%2520text%253F%26entry.906535625%3DMichael%2520Farrell%26entry.1292438233%3DThis%2520study%2520investigates%2520whether%2520professional%2520translators%2520can%2520reliably%2520identify%2520short%2520stories%2520generated%2520in%2520Italian%2520by%2520artificial%2520intelligence%2520%2528AI%2529%2520without%2520prior%2520specialized%2520training.%2520Sixty-nine%2520translators%2520took%2520part%2520in%2520an%2520in-person%2520experiment%252C%2520where%2520they%2520assessed%2520three%2520anonymized%2520short%2520stories%2520-%2520two%2520written%2520by%2520ChatGPT-4o%2520and%2520one%2520by%2520a%2520human%2520author.%2520For%2520each%2520story%252C%2520participants%2520rated%2520the%2520likelihood%2520of%2520AI%2520authorship%2520and%2520provided%2520justifications%2520for%2520their%2520choices.%2520While%2520average%2520results%2520were%2520inconclusive%252C%2520a%2520statistically%2520significant%2520subset%2520%252816.2%2525%2529%2520successfully%2520distinguished%2520the%2520synthetic%2520texts%2520from%2520the%2520human%2520text%252C%2520suggesting%2520that%2520their%2520judgements%2520were%2520informed%2520by%2520analytical%2520skill%2520rather%2520than%2520chance.%2520However%252C%2520a%2520nearly%2520equal%2520number%2520misclassified%2520the%2520texts%2520in%2520the%2520opposite%2520direction%252C%2520often%2520relying%2520on%2520subjective%2520impressions%2520rather%2520than%2520objective%2520markers%252C%2520possibly%2520reflecting%2520a%2520reader%2520preference%2520for%2520AI-generated%2520texts.%2520Low%2520burstiness%2520and%2520narrative%2520contradiction%2520emerged%2520as%2520the%2520most%2520reliable%2520indicators%2520of%2520synthetic%2520authorship%252C%2520with%2520unexpected%2520calques%252C%2520semantic%2520loans%2520and%2520syntactic%2520transfer%2520from%2520English%2520also%2520reported.%2520In%2520contrast%252C%2520features%2520such%2520as%2520grammatical%2520accuracy%2520and%2520emotional%2520tone%2520frequently%2520led%2520to%2520misclassification.%2520These%2520findings%2520raise%2520questions%2520about%2520the%2520role%2520and%2520scope%2520of%2520synthetic-text%2520editing%2520in%2520professional%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20professional%20translators%20identify%20machine-generated%20text%3F&entry.906535625=Michael%20Farrell&entry.1292438233=This%20study%20investigates%20whether%20professional%20translators%20can%20reliably%20identify%20short%20stories%20generated%20in%20Italian%20by%20artificial%20intelligence%20%28AI%29%20without%20prior%20specialized%20training.%20Sixty-nine%20translators%20took%20part%20in%20an%20in-person%20experiment%2C%20where%20they%20assessed%20three%20anonymized%20short%20stories%20-%20two%20written%20by%20ChatGPT-4o%20and%20one%20by%20a%20human%20author.%20For%20each%20story%2C%20participants%20rated%20the%20likelihood%20of%20AI%20authorship%20and%20provided%20justifications%20for%20their%20choices.%20While%20average%20results%20were%20inconclusive%2C%20a%20statistically%20significant%20subset%20%2816.2%25%29%20successfully%20distinguished%20the%20synthetic%20texts%20from%20the%20human%20text%2C%20suggesting%20that%20their%20judgements%20were%20informed%20by%20analytical%20skill%20rather%20than%20chance.%20However%2C%20a%20nearly%20equal%20number%20misclassified%20the%20texts%20in%20the%20opposite%20direction%2C%20often%20relying%20on%20subjective%20impressions%20rather%20than%20objective%20markers%2C%20possibly%20reflecting%20a%20reader%20preference%20for%20AI-generated%20texts.%20Low%20burstiness%20and%20narrative%20contradiction%20emerged%20as%20the%20most%20reliable%20indicators%20of%20synthetic%20authorship%2C%20with%20unexpected%20calques%2C%20semantic%20loans%20and%20syntactic%20transfer%20from%20English%20also%20reported.%20In%20contrast%2C%20features%20such%20as%20grammatical%20accuracy%20and%20emotional%20tone%20frequently%20led%20to%20misclassification.%20These%20findings%20raise%20questions%20about%20the%20role%20and%20scope%20of%20synthetic-text%20editing%20in%20professional%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2601.15828v1&entry.124074799=Read"},
{"title": "RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture", "author": "Anas Anwarul Haq Khan and Mariam Husain and Kshitij Jadhav", "abstract": "Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.", "link": "http://arxiv.org/abs/2601.15891v1", "date": "2026-01-22", "relevancy": 2.1592, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5413}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadJEPA%3A%20Radiology%20Encoder%20for%20Chest%20X-Rays%20via%20Joint%20Embedding%20Predictive%20Architecture&body=Title%3A%20RadJEPA%3A%20Radiology%20Encoder%20for%20Chest%20X-Rays%20via%20Joint%20Embedding%20Predictive%20Architecture%0AAuthor%3A%20Anas%20Anwarul%20Haq%20Khan%20and%20Mariam%20Husain%20and%20Kshitij%20Jadhav%0AAbstract%3A%20Recent%20advances%20in%20medical%20vision%20language%20models%20guide%20the%20learning%20of%20visual%20representations%3B%20however%2C%20this%20form%20of%20supervision%20is%20constrained%20by%20the%20availability%20of%20paired%20image%20text%20data%2C%20raising%20the%20question%20of%20whether%20robust%20radiology%20encoders%20can%20be%20learned%20without%20relying%20on%20language%20supervision.%20In%20this%20work%2C%20we%20introduce%20RadJEPA%2C%20a%20self-supervised%20framework%20built%20on%20a%20Joint%20Embedding%20Predictive%20Architecture%20that%20learns%20without%20language%20supervision.%20Pre-trained%20solely%20on%20unlabeled%20chest%20X-ray%20images%2C%20the%20model%20learns%20to%20predict%20latent%20representations%20of%20masked%20image%20regions.%20This%20predictive%20objective%20differs%20fundamentally%20from%20both%20image%20text%20pre-training%20and%20DINO-style%20self-distillation%3A%20rather%20than%20aligning%20global%20representations%20across%20views%20or%20modalities%2C%20RadJEPA%20explicitly%20models%20latent-space%20prediction.%20We%20evaluate%20the%20learned%20encoder%20on%20disease%20classification%2C%20semantic%20segmentation%2C%20and%20report%20generation%20tasks.%20Across%20benchmarks%2C%20RadJEPA%20achieves%20performance%20exceeding%20state-of-the-art%20approaches%2C%20including%20Rad-DINO.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadJEPA%253A%2520Radiology%2520Encoder%2520for%2520Chest%2520X-Rays%2520via%2520Joint%2520Embedding%2520Predictive%2520Architecture%26entry.906535625%3DAnas%2520Anwarul%2520Haq%2520Khan%2520and%2520Mariam%2520Husain%2520and%2520Kshitij%2520Jadhav%26entry.1292438233%3DRecent%2520advances%2520in%2520medical%2520vision%2520language%2520models%2520guide%2520the%2520learning%2520of%2520visual%2520representations%253B%2520however%252C%2520this%2520form%2520of%2520supervision%2520is%2520constrained%2520by%2520the%2520availability%2520of%2520paired%2520image%2520text%2520data%252C%2520raising%2520the%2520question%2520of%2520whether%2520robust%2520radiology%2520encoders%2520can%2520be%2520learned%2520without%2520relying%2520on%2520language%2520supervision.%2520In%2520this%2520work%252C%2520we%2520introduce%2520RadJEPA%252C%2520a%2520self-supervised%2520framework%2520built%2520on%2520a%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520that%2520learns%2520without%2520language%2520supervision.%2520Pre-trained%2520solely%2520on%2520unlabeled%2520chest%2520X-ray%2520images%252C%2520the%2520model%2520learns%2520to%2520predict%2520latent%2520representations%2520of%2520masked%2520image%2520regions.%2520This%2520predictive%2520objective%2520differs%2520fundamentally%2520from%2520both%2520image%2520text%2520pre-training%2520and%2520DINO-style%2520self-distillation%253A%2520rather%2520than%2520aligning%2520global%2520representations%2520across%2520views%2520or%2520modalities%252C%2520RadJEPA%2520explicitly%2520models%2520latent-space%2520prediction.%2520We%2520evaluate%2520the%2520learned%2520encoder%2520on%2520disease%2520classification%252C%2520semantic%2520segmentation%252C%2520and%2520report%2520generation%2520tasks.%2520Across%2520benchmarks%252C%2520RadJEPA%2520achieves%2520performance%2520exceeding%2520state-of-the-art%2520approaches%252C%2520including%2520Rad-DINO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadJEPA%3A%20Radiology%20Encoder%20for%20Chest%20X-Rays%20via%20Joint%20Embedding%20Predictive%20Architecture&entry.906535625=Anas%20Anwarul%20Haq%20Khan%20and%20Mariam%20Husain%20and%20Kshitij%20Jadhav&entry.1292438233=Recent%20advances%20in%20medical%20vision%20language%20models%20guide%20the%20learning%20of%20visual%20representations%3B%20however%2C%20this%20form%20of%20supervision%20is%20constrained%20by%20the%20availability%20of%20paired%20image%20text%20data%2C%20raising%20the%20question%20of%20whether%20robust%20radiology%20encoders%20can%20be%20learned%20without%20relying%20on%20language%20supervision.%20In%20this%20work%2C%20we%20introduce%20RadJEPA%2C%20a%20self-supervised%20framework%20built%20on%20a%20Joint%20Embedding%20Predictive%20Architecture%20that%20learns%20without%20language%20supervision.%20Pre-trained%20solely%20on%20unlabeled%20chest%20X-ray%20images%2C%20the%20model%20learns%20to%20predict%20latent%20representations%20of%20masked%20image%20regions.%20This%20predictive%20objective%20differs%20fundamentally%20from%20both%20image%20text%20pre-training%20and%20DINO-style%20self-distillation%3A%20rather%20than%20aligning%20global%20representations%20across%20views%20or%20modalities%2C%20RadJEPA%20explicitly%20models%20latent-space%20prediction.%20We%20evaluate%20the%20learned%20encoder%20on%20disease%20classification%2C%20semantic%20segmentation%2C%20and%20report%20generation%20tasks.%20Across%20benchmarks%2C%20RadJEPA%20achieves%20performance%20exceeding%20state-of-the-art%20approaches%2C%20including%20Rad-DINO.&entry.1838667208=http%3A//arxiv.org/abs/2601.15891v1&entry.124074799=Read"},
{"title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "author": "Taofeng Xue and Chong Peng and Mianqiu Huang and Linsen Guo and Tiancheng Han and Haozhe Wang and Jianing Wang and Xiaocheng Zhang and Xin Yang and Dengchang Zhao and Jinrui Ding and Xiandi Ma and Yuchen Xie and Peng Pei and Xunliang Cai and Xipeng Qiu", "abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "link": "http://arxiv.org/abs/2601.15876v1", "date": "2026-01-22", "relevancy": 2.1498, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5503}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5401}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoCUA%3A%20Evolving%20Computer%20Use%20Agents%20via%20Learning%20from%20Scalable%20Synthetic%20Experience&body=Title%3A%20EvoCUA%3A%20Evolving%20Computer%20Use%20Agents%20via%20Learning%20from%20Scalable%20Synthetic%20Experience%0AAuthor%3A%20Taofeng%20Xue%20and%20Chong%20Peng%20and%20Mianqiu%20Huang%20and%20Linsen%20Guo%20and%20Tiancheng%20Han%20and%20Haozhe%20Wang%20and%20Jianing%20Wang%20and%20Xiaocheng%20Zhang%20and%20Xin%20Yang%20and%20Dengchang%20Zhao%20and%20Jinrui%20Ding%20and%20Xiandi%20Ma%20and%20Yuchen%20Xie%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Xipeng%20Qiu%0AAbstract%3A%20The%20development%20of%20native%20computer-use%20agents%20%28CUA%29%20represents%20a%20significant%20leap%20in%20multimodal%20AI.%20However%2C%20their%20potential%20is%20currently%20bottlenecked%20by%20the%20constraints%20of%20static%20data%20scaling.%20Existing%20paradigms%20relying%20primarily%20on%20passive%20imitation%20of%20static%20datasets%20struggle%20to%20capture%20the%20intricate%20causal%20dynamics%20inherent%20in%20long-horizon%20computer%20tasks.%20In%20this%20work%2C%20we%20introduce%20EvoCUA%2C%20a%20native%20computer%20use%20agentic%20model.%20Unlike%20static%20imitation%2C%20EvoCUA%20integrates%20data%20generation%20and%20policy%20optimization%20into%20a%20self-sustaining%20evolutionary%20cycle.%20To%20mitigate%20data%20scarcity%2C%20we%20develop%20a%20verifiable%20synthesis%20engine%20that%20autonomously%20generates%20diverse%20tasks%20coupled%20with%20executable%20validators.%20To%20enable%20large-scale%20experience%20acquisition%2C%20we%20design%20a%20scalable%20infrastructure%20orchestrating%20tens%20of%20thousands%20of%20asynchronous%20sandbox%20rollouts.%20Building%20on%20these%20massive%20trajectories%2C%20we%20propose%20an%20iterative%20evolving%20learning%20strategy%20to%20efficiently%20internalize%20this%20experience.%20This%20mechanism%20dynamically%20regulates%20policy%20updates%20by%20identifying%20capability%20boundaries%20--%20reinforcing%20successful%20routines%20while%20transforming%20failure%20trajectories%20into%20rich%20supervision%20through%20error%20analysis%20and%20self-correction.%20Empirical%20evaluations%20on%20the%20OSWorld%20benchmark%20demonstrate%20that%20EvoCUA%20achieves%20a%20success%20rate%20of%2056.7%25%2C%20establishing%20a%20new%20open-source%20state-of-the-art.%20Notably%2C%20EvoCUA%20significantly%20outperforms%20the%20previous%20best%20open-source%20model%2C%20OpenCUA-72B%20%2845.0%25%29%2C%20and%20surpasses%20leading%20closed-weights%20models%20such%20as%20UI-TARS-2%20%2853.1%25%29.%20Crucially%2C%20our%20results%20underscore%20the%20generalizability%20of%20this%20approach%3A%20the%20evolving%20paradigm%20driven%20by%20learning%20from%20experience%20yields%20consistent%20performance%20gains%20across%20foundation%20models%20of%20varying%20scales%2C%20establishing%20a%20robust%20and%20scalable%20path%20for%20advancing%20native%20agent%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoCUA%253A%2520Evolving%2520Computer%2520Use%2520Agents%2520via%2520Learning%2520from%2520Scalable%2520Synthetic%2520Experience%26entry.906535625%3DTaofeng%2520Xue%2520and%2520Chong%2520Peng%2520and%2520Mianqiu%2520Huang%2520and%2520Linsen%2520Guo%2520and%2520Tiancheng%2520Han%2520and%2520Haozhe%2520Wang%2520and%2520Jianing%2520Wang%2520and%2520Xiaocheng%2520Zhang%2520and%2520Xin%2520Yang%2520and%2520Dengchang%2520Zhao%2520and%2520Jinrui%2520Ding%2520and%2520Xiandi%2520Ma%2520and%2520Yuchen%2520Xie%2520and%2520Peng%2520Pei%2520and%2520Xunliang%2520Cai%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3DThe%2520development%2520of%2520native%2520computer-use%2520agents%2520%2528CUA%2529%2520represents%2520a%2520significant%2520leap%2520in%2520multimodal%2520AI.%2520However%252C%2520their%2520potential%2520is%2520currently%2520bottlenecked%2520by%2520the%2520constraints%2520of%2520static%2520data%2520scaling.%2520Existing%2520paradigms%2520relying%2520primarily%2520on%2520passive%2520imitation%2520of%2520static%2520datasets%2520struggle%2520to%2520capture%2520the%2520intricate%2520causal%2520dynamics%2520inherent%2520in%2520long-horizon%2520computer%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520EvoCUA%252C%2520a%2520native%2520computer%2520use%2520agentic%2520model.%2520Unlike%2520static%2520imitation%252C%2520EvoCUA%2520integrates%2520data%2520generation%2520and%2520policy%2520optimization%2520into%2520a%2520self-sustaining%2520evolutionary%2520cycle.%2520To%2520mitigate%2520data%2520scarcity%252C%2520we%2520develop%2520a%2520verifiable%2520synthesis%2520engine%2520that%2520autonomously%2520generates%2520diverse%2520tasks%2520coupled%2520with%2520executable%2520validators.%2520To%2520enable%2520large-scale%2520experience%2520acquisition%252C%2520we%2520design%2520a%2520scalable%2520infrastructure%2520orchestrating%2520tens%2520of%2520thousands%2520of%2520asynchronous%2520sandbox%2520rollouts.%2520Building%2520on%2520these%2520massive%2520trajectories%252C%2520we%2520propose%2520an%2520iterative%2520evolving%2520learning%2520strategy%2520to%2520efficiently%2520internalize%2520this%2520experience.%2520This%2520mechanism%2520dynamically%2520regulates%2520policy%2520updates%2520by%2520identifying%2520capability%2520boundaries%2520--%2520reinforcing%2520successful%2520routines%2520while%2520transforming%2520failure%2520trajectories%2520into%2520rich%2520supervision%2520through%2520error%2520analysis%2520and%2520self-correction.%2520Empirical%2520evaluations%2520on%2520the%2520OSWorld%2520benchmark%2520demonstrate%2520that%2520EvoCUA%2520achieves%2520a%2520success%2520rate%2520of%252056.7%2525%252C%2520establishing%2520a%2520new%2520open-source%2520state-of-the-art.%2520Notably%252C%2520EvoCUA%2520significantly%2520outperforms%2520the%2520previous%2520best%2520open-source%2520model%252C%2520OpenCUA-72B%2520%252845.0%2525%2529%252C%2520and%2520surpasses%2520leading%2520closed-weights%2520models%2520such%2520as%2520UI-TARS-2%2520%252853.1%2525%2529.%2520Crucially%252C%2520our%2520results%2520underscore%2520the%2520generalizability%2520of%2520this%2520approach%253A%2520the%2520evolving%2520paradigm%2520driven%2520by%2520learning%2520from%2520experience%2520yields%2520consistent%2520performance%2520gains%2520across%2520foundation%2520models%2520of%2520varying%2520scales%252C%2520establishing%2520a%2520robust%2520and%2520scalable%2520path%2520for%2520advancing%2520native%2520agent%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoCUA%3A%20Evolving%20Computer%20Use%20Agents%20via%20Learning%20from%20Scalable%20Synthetic%20Experience&entry.906535625=Taofeng%20Xue%20and%20Chong%20Peng%20and%20Mianqiu%20Huang%20and%20Linsen%20Guo%20and%20Tiancheng%20Han%20and%20Haozhe%20Wang%20and%20Jianing%20Wang%20and%20Xiaocheng%20Zhang%20and%20Xin%20Yang%20and%20Dengchang%20Zhao%20and%20Jinrui%20Ding%20and%20Xiandi%20Ma%20and%20Yuchen%20Xie%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Xipeng%20Qiu&entry.1292438233=The%20development%20of%20native%20computer-use%20agents%20%28CUA%29%20represents%20a%20significant%20leap%20in%20multimodal%20AI.%20However%2C%20their%20potential%20is%20currently%20bottlenecked%20by%20the%20constraints%20of%20static%20data%20scaling.%20Existing%20paradigms%20relying%20primarily%20on%20passive%20imitation%20of%20static%20datasets%20struggle%20to%20capture%20the%20intricate%20causal%20dynamics%20inherent%20in%20long-horizon%20computer%20tasks.%20In%20this%20work%2C%20we%20introduce%20EvoCUA%2C%20a%20native%20computer%20use%20agentic%20model.%20Unlike%20static%20imitation%2C%20EvoCUA%20integrates%20data%20generation%20and%20policy%20optimization%20into%20a%20self-sustaining%20evolutionary%20cycle.%20To%20mitigate%20data%20scarcity%2C%20we%20develop%20a%20verifiable%20synthesis%20engine%20that%20autonomously%20generates%20diverse%20tasks%20coupled%20with%20executable%20validators.%20To%20enable%20large-scale%20experience%20acquisition%2C%20we%20design%20a%20scalable%20infrastructure%20orchestrating%20tens%20of%20thousands%20of%20asynchronous%20sandbox%20rollouts.%20Building%20on%20these%20massive%20trajectories%2C%20we%20propose%20an%20iterative%20evolving%20learning%20strategy%20to%20efficiently%20internalize%20this%20experience.%20This%20mechanism%20dynamically%20regulates%20policy%20updates%20by%20identifying%20capability%20boundaries%20--%20reinforcing%20successful%20routines%20while%20transforming%20failure%20trajectories%20into%20rich%20supervision%20through%20error%20analysis%20and%20self-correction.%20Empirical%20evaluations%20on%20the%20OSWorld%20benchmark%20demonstrate%20that%20EvoCUA%20achieves%20a%20success%20rate%20of%2056.7%25%2C%20establishing%20a%20new%20open-source%20state-of-the-art.%20Notably%2C%20EvoCUA%20significantly%20outperforms%20the%20previous%20best%20open-source%20model%2C%20OpenCUA-72B%20%2845.0%25%29%2C%20and%20surpasses%20leading%20closed-weights%20models%20such%20as%20UI-TARS-2%20%2853.1%25%29.%20Crucially%2C%20our%20results%20underscore%20the%20generalizability%20of%20this%20approach%3A%20the%20evolving%20paradigm%20driven%20by%20learning%20from%20experience%20yields%20consistent%20performance%20gains%20across%20foundation%20models%20of%20varying%20scales%2C%20establishing%20a%20robust%20and%20scalable%20path%20for%20advancing%20native%20agent%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2601.15876v1&entry.124074799=Read"},
{"title": "Reinforcement Learning Compensated Model Predictive Control for Off-road Driving on Unknown Deformable Terrain", "author": "Prakhar Gupta and Jonathon M. Smereka and Yunyi Jia", "abstract": "This study presents an Actor-Critic reinforcement learning Compensated Model Predictive Controller (AC2MPC) designed for high-speed, off-road autonomous driving on deformable terrains. Addressing the difficulty of modeling unknown tire-terrain interaction and ensuring real-time control feasibility and performance, this framework integrates deep reinforcement learning with a model predictive controller to manage unmodeled nonlinear dynamics. We evaluate the controller framework over constant and varying velocity profiles using high-fidelity simulator Project Chrono. Our findings demonstrate that our controller statistically outperforms standalone model-based and learning-based controllers over three unknown terrains that represent sandy deformable track, sandy and rocky track and cohesive clay-like deformable soil track. Despite varied and previously unseen terrain characteristics, this framework generalized well enough to track longitudinal reference speeds with the least error. Furthermore, this framework required significantly less training data compared to purely learning based controller, converging in fewer steps while delivering better performance. Even when under-trained, this controller outperformed the standalone controllers, highlighting its potential for safer and more efficient real-world deployment.", "link": "http://arxiv.org/abs/2408.09253v2", "date": "2026-01-22", "relevancy": 2.1463, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5506}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5379}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20Compensated%20Model%20Predictive%20Control%20for%20Off-road%20Driving%20on%20Unknown%20Deformable%20Terrain&body=Title%3A%20Reinforcement%20Learning%20Compensated%20Model%20Predictive%20Control%20for%20Off-road%20Driving%20on%20Unknown%20Deformable%20Terrain%0AAuthor%3A%20Prakhar%20Gupta%20and%20Jonathon%20M.%20Smereka%20and%20Yunyi%20Jia%0AAbstract%3A%20This%20study%20presents%20an%20Actor-Critic%20reinforcement%20learning%20Compensated%20Model%20Predictive%20Controller%20%28AC2MPC%29%20designed%20for%20high-speed%2C%20off-road%20autonomous%20driving%20on%20deformable%20terrains.%20Addressing%20the%20difficulty%20of%20modeling%20unknown%20tire-terrain%20interaction%20and%20ensuring%20real-time%20control%20feasibility%20and%20performance%2C%20this%20framework%20integrates%20deep%20reinforcement%20learning%20with%20a%20model%20predictive%20controller%20to%20manage%20unmodeled%20nonlinear%20dynamics.%20We%20evaluate%20the%20controller%20framework%20over%20constant%20and%20varying%20velocity%20profiles%20using%20high-fidelity%20simulator%20Project%20Chrono.%20Our%20findings%20demonstrate%20that%20our%20controller%20statistically%20outperforms%20standalone%20model-based%20and%20learning-based%20controllers%20over%20three%20unknown%20terrains%20that%20represent%20sandy%20deformable%20track%2C%20sandy%20and%20rocky%20track%20and%20cohesive%20clay-like%20deformable%20soil%20track.%20Despite%20varied%20and%20previously%20unseen%20terrain%20characteristics%2C%20this%20framework%20generalized%20well%20enough%20to%20track%20longitudinal%20reference%20speeds%20with%20the%20least%20error.%20Furthermore%2C%20this%20framework%20required%20significantly%20less%20training%20data%20compared%20to%20purely%20learning%20based%20controller%2C%20converging%20in%20fewer%20steps%20while%20delivering%20better%20performance.%20Even%20when%20under-trained%2C%20this%20controller%20outperformed%20the%20standalone%20controllers%2C%20highlighting%20its%20potential%20for%20safer%20and%20more%20efficient%20real-world%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2408.09253v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520Compensated%2520Model%2520Predictive%2520Control%2520for%2520Off-road%2520Driving%2520on%2520Unknown%2520Deformable%2520Terrain%26entry.906535625%3DPrakhar%2520Gupta%2520and%2520Jonathon%2520M.%2520Smereka%2520and%2520Yunyi%2520Jia%26entry.1292438233%3DThis%2520study%2520presents%2520an%2520Actor-Critic%2520reinforcement%2520learning%2520Compensated%2520Model%2520Predictive%2520Controller%2520%2528AC2MPC%2529%2520designed%2520for%2520high-speed%252C%2520off-road%2520autonomous%2520driving%2520on%2520deformable%2520terrains.%2520Addressing%2520the%2520difficulty%2520of%2520modeling%2520unknown%2520tire-terrain%2520interaction%2520and%2520ensuring%2520real-time%2520control%2520feasibility%2520and%2520performance%252C%2520this%2520framework%2520integrates%2520deep%2520reinforcement%2520learning%2520with%2520a%2520model%2520predictive%2520controller%2520to%2520manage%2520unmodeled%2520nonlinear%2520dynamics.%2520We%2520evaluate%2520the%2520controller%2520framework%2520over%2520constant%2520and%2520varying%2520velocity%2520profiles%2520using%2520high-fidelity%2520simulator%2520Project%2520Chrono.%2520Our%2520findings%2520demonstrate%2520that%2520our%2520controller%2520statistically%2520outperforms%2520standalone%2520model-based%2520and%2520learning-based%2520controllers%2520over%2520three%2520unknown%2520terrains%2520that%2520represent%2520sandy%2520deformable%2520track%252C%2520sandy%2520and%2520rocky%2520track%2520and%2520cohesive%2520clay-like%2520deformable%2520soil%2520track.%2520Despite%2520varied%2520and%2520previously%2520unseen%2520terrain%2520characteristics%252C%2520this%2520framework%2520generalized%2520well%2520enough%2520to%2520track%2520longitudinal%2520reference%2520speeds%2520with%2520the%2520least%2520error.%2520Furthermore%252C%2520this%2520framework%2520required%2520significantly%2520less%2520training%2520data%2520compared%2520to%2520purely%2520learning%2520based%2520controller%252C%2520converging%2520in%2520fewer%2520steps%2520while%2520delivering%2520better%2520performance.%2520Even%2520when%2520under-trained%252C%2520this%2520controller%2520outperformed%2520the%2520standalone%2520controllers%252C%2520highlighting%2520its%2520potential%2520for%2520safer%2520and%2520more%2520efficient%2520real-world%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09253v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20Compensated%20Model%20Predictive%20Control%20for%20Off-road%20Driving%20on%20Unknown%20Deformable%20Terrain&entry.906535625=Prakhar%20Gupta%20and%20Jonathon%20M.%20Smereka%20and%20Yunyi%20Jia&entry.1292438233=This%20study%20presents%20an%20Actor-Critic%20reinforcement%20learning%20Compensated%20Model%20Predictive%20Controller%20%28AC2MPC%29%20designed%20for%20high-speed%2C%20off-road%20autonomous%20driving%20on%20deformable%20terrains.%20Addressing%20the%20difficulty%20of%20modeling%20unknown%20tire-terrain%20interaction%20and%20ensuring%20real-time%20control%20feasibility%20and%20performance%2C%20this%20framework%20integrates%20deep%20reinforcement%20learning%20with%20a%20model%20predictive%20controller%20to%20manage%20unmodeled%20nonlinear%20dynamics.%20We%20evaluate%20the%20controller%20framework%20over%20constant%20and%20varying%20velocity%20profiles%20using%20high-fidelity%20simulator%20Project%20Chrono.%20Our%20findings%20demonstrate%20that%20our%20controller%20statistically%20outperforms%20standalone%20model-based%20and%20learning-based%20controllers%20over%20three%20unknown%20terrains%20that%20represent%20sandy%20deformable%20track%2C%20sandy%20and%20rocky%20track%20and%20cohesive%20clay-like%20deformable%20soil%20track.%20Despite%20varied%20and%20previously%20unseen%20terrain%20characteristics%2C%20this%20framework%20generalized%20well%20enough%20to%20track%20longitudinal%20reference%20speeds%20with%20the%20least%20error.%20Furthermore%2C%20this%20framework%20required%20significantly%20less%20training%20data%20compared%20to%20purely%20learning%20based%20controller%2C%20converging%20in%20fewer%20steps%20while%20delivering%20better%20performance.%20Even%20when%20under-trained%2C%20this%20controller%20outperformed%20the%20standalone%20controllers%2C%20highlighting%20its%20potential%20for%20safer%20and%20more%20efficient%20real-world%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2408.09253v2&entry.124074799=Read"},
{"title": "Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface", "author": "Paige S. DeVries and Michaela Okosi and Ming Li and Nora Dunphy and Gidey Gezae and Dante Conway and Abraham Glasser and Raja Kushalnagar and Christian Vogler", "abstract": "We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa's automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered \"task prompter,\" which integrated the user's history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.", "link": "http://arxiv.org/abs/2601.15209v2", "date": "2026-01-22", "relevancy": 2.1462, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deaf%20and%20Hard%20of%20Hearing%20Access%20to%20Intelligent%20Personal%20Assistants%3A%20Comparison%20of%20Voice-Based%20Options%20with%20an%20LLM-Powered%20Touch%20Interface&body=Title%3A%20Deaf%20and%20Hard%20of%20Hearing%20Access%20to%20Intelligent%20Personal%20Assistants%3A%20Comparison%20of%20Voice-Based%20Options%20with%20an%20LLM-Powered%20Touch%20Interface%0AAuthor%3A%20Paige%20S.%20DeVries%20and%20Michaela%20Okosi%20and%20Ming%20Li%20and%20Nora%20Dunphy%20and%20Gidey%20Gezae%20and%20Dante%20Conway%20and%20Abraham%20Glasser%20and%20Raja%20Kushalnagar%20and%20Christian%20Vogler%0AAbstract%3A%20We%20investigate%20intelligent%20personal%20assistants%20%28IPAs%29%20accessibility%20for%20deaf%20and%20hard%20of%20hearing%20%28DHH%29%20people%20who%20can%20use%20their%20voice%20in%20everyday%20communication.%20The%20inability%20of%20IPAs%20to%20understand%20diverse%20accents%20including%20deaf%20speech%20renders%20them%20largely%20inaccessible%20to%20non-signing%20and%20speaking%20DHH%20individuals.%20Using%20an%20Echo%20Show%2C%20we%20compare%20the%20usability%20of%20natural%20language%20input%20via%20spoken%20English%3B%20with%20Alexa%27s%20automatic%20speech%20recognition%20and%20a%20Wizard-of-Oz%20setting%20with%20a%20trained%20facilitator%20re-speaking%20commands%20against%20that%20of%20a%20large%20language%20model%20%28LLM%29-assisted%20touch%20interface%20in%20a%20mixed-methods%20study.%20The%20touch%20method%20was%20navigated%20through%20an%20LLM-powered%20%22task%20prompter%2C%22%20which%20integrated%20the%20user%27s%20history%20and%20smart%20environment%20to%20suggest%20contextually-appropriate%20commands.%20Quantitative%20results%20showed%20no%20significant%20differences%20across%20both%20spoken%20English%20conditions%20vs%20LLM-assisted%20touch.%20Qualitative%20results%20showed%20variability%20in%20opinions%20on%20the%20usability%20of%20each%20method.%20Ultimately%2C%20it%20will%20be%20necessary%20to%20have%20robust%20deaf-accented%20speech%20recognized%20natively%20by%20IPAs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeaf%2520and%2520Hard%2520of%2520Hearing%2520Access%2520to%2520Intelligent%2520Personal%2520Assistants%253A%2520Comparison%2520of%2520Voice-Based%2520Options%2520with%2520an%2520LLM-Powered%2520Touch%2520Interface%26entry.906535625%3DPaige%2520S.%2520DeVries%2520and%2520Michaela%2520Okosi%2520and%2520Ming%2520Li%2520and%2520Nora%2520Dunphy%2520and%2520Gidey%2520Gezae%2520and%2520Dante%2520Conway%2520and%2520Abraham%2520Glasser%2520and%2520Raja%2520Kushalnagar%2520and%2520Christian%2520Vogler%26entry.1292438233%3DWe%2520investigate%2520intelligent%2520personal%2520assistants%2520%2528IPAs%2529%2520accessibility%2520for%2520deaf%2520and%2520hard%2520of%2520hearing%2520%2528DHH%2529%2520people%2520who%2520can%2520use%2520their%2520voice%2520in%2520everyday%2520communication.%2520The%2520inability%2520of%2520IPAs%2520to%2520understand%2520diverse%2520accents%2520including%2520deaf%2520speech%2520renders%2520them%2520largely%2520inaccessible%2520to%2520non-signing%2520and%2520speaking%2520DHH%2520individuals.%2520Using%2520an%2520Echo%2520Show%252C%2520we%2520compare%2520the%2520usability%2520of%2520natural%2520language%2520input%2520via%2520spoken%2520English%253B%2520with%2520Alexa%2527s%2520automatic%2520speech%2520recognition%2520and%2520a%2520Wizard-of-Oz%2520setting%2520with%2520a%2520trained%2520facilitator%2520re-speaking%2520commands%2520against%2520that%2520of%2520a%2520large%2520language%2520model%2520%2528LLM%2529-assisted%2520touch%2520interface%2520in%2520a%2520mixed-methods%2520study.%2520The%2520touch%2520method%2520was%2520navigated%2520through%2520an%2520LLM-powered%2520%2522task%2520prompter%252C%2522%2520which%2520integrated%2520the%2520user%2527s%2520history%2520and%2520smart%2520environment%2520to%2520suggest%2520contextually-appropriate%2520commands.%2520Quantitative%2520results%2520showed%2520no%2520significant%2520differences%2520across%2520both%2520spoken%2520English%2520conditions%2520vs%2520LLM-assisted%2520touch.%2520Qualitative%2520results%2520showed%2520variability%2520in%2520opinions%2520on%2520the%2520usability%2520of%2520each%2520method.%2520Ultimately%252C%2520it%2520will%2520be%2520necessary%2520to%2520have%2520robust%2520deaf-accented%2520speech%2520recognized%2520natively%2520by%2520IPAs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deaf%20and%20Hard%20of%20Hearing%20Access%20to%20Intelligent%20Personal%20Assistants%3A%20Comparison%20of%20Voice-Based%20Options%20with%20an%20LLM-Powered%20Touch%20Interface&entry.906535625=Paige%20S.%20DeVries%20and%20Michaela%20Okosi%20and%20Ming%20Li%20and%20Nora%20Dunphy%20and%20Gidey%20Gezae%20and%20Dante%20Conway%20and%20Abraham%20Glasser%20and%20Raja%20Kushalnagar%20and%20Christian%20Vogler&entry.1292438233=We%20investigate%20intelligent%20personal%20assistants%20%28IPAs%29%20accessibility%20for%20deaf%20and%20hard%20of%20hearing%20%28DHH%29%20people%20who%20can%20use%20their%20voice%20in%20everyday%20communication.%20The%20inability%20of%20IPAs%20to%20understand%20diverse%20accents%20including%20deaf%20speech%20renders%20them%20largely%20inaccessible%20to%20non-signing%20and%20speaking%20DHH%20individuals.%20Using%20an%20Echo%20Show%2C%20we%20compare%20the%20usability%20of%20natural%20language%20input%20via%20spoken%20English%3B%20with%20Alexa%27s%20automatic%20speech%20recognition%20and%20a%20Wizard-of-Oz%20setting%20with%20a%20trained%20facilitator%20re-speaking%20commands%20against%20that%20of%20a%20large%20language%20model%20%28LLM%29-assisted%20touch%20interface%20in%20a%20mixed-methods%20study.%20The%20touch%20method%20was%20navigated%20through%20an%20LLM-powered%20%22task%20prompter%2C%22%20which%20integrated%20the%20user%27s%20history%20and%20smart%20environment%20to%20suggest%20contextually-appropriate%20commands.%20Quantitative%20results%20showed%20no%20significant%20differences%20across%20both%20spoken%20English%20conditions%20vs%20LLM-assisted%20touch.%20Qualitative%20results%20showed%20variability%20in%20opinions%20on%20the%20usability%20of%20each%20method.%20Ultimately%2C%20it%20will%20be%20necessary%20to%20have%20robust%20deaf-accented%20speech%20recognized%20natively%20by%20IPAs.&entry.1838667208=http%3A//arxiv.org/abs/2601.15209v2&entry.124074799=Read"},
{"title": "An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence", "author": "Conall Daly and Darren Ramsook and Anil Kokaram", "abstract": "Video frame interpolation is a fundamental tool for temporal video enhancement, but existing quality metrics struggle to evaluate the perceptual impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored towards video frame interpolation, like FloLPIPS, have been developed but suffer from computational inefficiency that limits their practical application. We present $\\text{PSNR}_{\\text{DIV}}$, a novel full-reference quality metric that enhances PSNR through motion divergence weighting, a technique adapted from archival film restoration where it was developed to detect temporal inconsistencies. Our approach highlights singularities in motion fields which is then used to weight image errors. Evaluation on the BVI-VFI dataset (180 sequences across multiple frame rates, resolutions and interpolation methods) shows $\\text{PSNR}_{\\text{DIV}}$ achieves statistically significant improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while being 2.5$\\times$ faster and using 4$\\times$ less memory. Performance remains consistent across all content categories and are robust to the motion estimator used. The efficiency and accuracy of $\\text{PSNR}_{\\text{DIV}}$ enables fast quality evaluation and practical use as a loss function for training neural networks for video frame interpolation tasks. An implementation of our metric is available at www.github.com/conalld/psnr-div.", "link": "http://arxiv.org/abs/2510.01361v2", "date": "2026-01-22", "relevancy": 2.1313, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5782}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5011}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Quality%20Metric%20for%20Video%20Frame%20Interpolation%20Based%20on%20Motion-Field%20Divergence&body=Title%3A%20An%20Efficient%20Quality%20Metric%20for%20Video%20Frame%20Interpolation%20Based%20on%20Motion-Field%20Divergence%0AAuthor%3A%20Conall%20Daly%20and%20Darren%20Ramsook%20and%20Anil%20Kokaram%0AAbstract%3A%20Video%20frame%20interpolation%20is%20a%20fundamental%20tool%20for%20temporal%20video%20enhancement%2C%20but%20existing%20quality%20metrics%20struggle%20to%20evaluate%20the%20perceptual%20impact%20of%20interpolation%20artefacts%20effectively.%20Metrics%20like%20PSNR%2C%20SSIM%20and%20LPIPS%20ignore%20temporal%20coherence.%20State-of-the-art%20quality%20metrics%20tailored%20towards%20video%20frame%20interpolation%2C%20like%20FloLPIPS%2C%20have%20been%20developed%20but%20suffer%20from%20computational%20inefficiency%20that%20limits%20their%20practical%20application.%20We%20present%20%24%5Ctext%7BPSNR%7D_%7B%5Ctext%7BDIV%7D%7D%24%2C%20a%20novel%20full-reference%20quality%20metric%20that%20enhances%20PSNR%20through%20motion%20divergence%20weighting%2C%20a%20technique%20adapted%20from%20archival%20film%20restoration%20where%20it%20was%20developed%20to%20detect%20temporal%20inconsistencies.%20Our%20approach%20highlights%20singularities%20in%20motion%20fields%20which%20is%20then%20used%20to%20weight%20image%20errors.%20Evaluation%20on%20the%20BVI-VFI%20dataset%20%28180%20sequences%20across%20multiple%20frame%20rates%2C%20resolutions%20and%20interpolation%20methods%29%20shows%20%24%5Ctext%7BPSNR%7D_%7B%5Ctext%7BDIV%7D%7D%24%20achieves%20statistically%20significant%20improvements%3A%20%2B0.09%20Pearson%20Linear%20Correlation%20Coefficient%20over%20FloLPIPS%2C%20while%20being%202.5%24%5Ctimes%24%20faster%20and%20using%204%24%5Ctimes%24%20less%20memory.%20Performance%20remains%20consistent%20across%20all%20content%20categories%20and%20are%20robust%20to%20the%20motion%20estimator%20used.%20The%20efficiency%20and%20accuracy%20of%20%24%5Ctext%7BPSNR%7D_%7B%5Ctext%7BDIV%7D%7D%24%20enables%20fast%20quality%20evaluation%20and%20practical%20use%20as%20a%20loss%20function%20for%20training%20neural%20networks%20for%20video%20frame%20interpolation%20tasks.%20An%20implementation%20of%20our%20metric%20is%20available%20at%20www.github.com/conalld/psnr-div.%0ALink%3A%20http%3A//arxiv.org/abs/2510.01361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Quality%2520Metric%2520for%2520Video%2520Frame%2520Interpolation%2520Based%2520on%2520Motion-Field%2520Divergence%26entry.906535625%3DConall%2520Daly%2520and%2520Darren%2520Ramsook%2520and%2520Anil%2520Kokaram%26entry.1292438233%3DVideo%2520frame%2520interpolation%2520is%2520a%2520fundamental%2520tool%2520for%2520temporal%2520video%2520enhancement%252C%2520but%2520existing%2520quality%2520metrics%2520struggle%2520to%2520evaluate%2520the%2520perceptual%2520impact%2520of%2520interpolation%2520artefacts%2520effectively.%2520Metrics%2520like%2520PSNR%252C%2520SSIM%2520and%2520LPIPS%2520ignore%2520temporal%2520coherence.%2520State-of-the-art%2520quality%2520metrics%2520tailored%2520towards%2520video%2520frame%2520interpolation%252C%2520like%2520FloLPIPS%252C%2520have%2520been%2520developed%2520but%2520suffer%2520from%2520computational%2520inefficiency%2520that%2520limits%2520their%2520practical%2520application.%2520We%2520present%2520%2524%255Ctext%257BPSNR%257D_%257B%255Ctext%257BDIV%257D%257D%2524%252C%2520a%2520novel%2520full-reference%2520quality%2520metric%2520that%2520enhances%2520PSNR%2520through%2520motion%2520divergence%2520weighting%252C%2520a%2520technique%2520adapted%2520from%2520archival%2520film%2520restoration%2520where%2520it%2520was%2520developed%2520to%2520detect%2520temporal%2520inconsistencies.%2520Our%2520approach%2520highlights%2520singularities%2520in%2520motion%2520fields%2520which%2520is%2520then%2520used%2520to%2520weight%2520image%2520errors.%2520Evaluation%2520on%2520the%2520BVI-VFI%2520dataset%2520%2528180%2520sequences%2520across%2520multiple%2520frame%2520rates%252C%2520resolutions%2520and%2520interpolation%2520methods%2529%2520shows%2520%2524%255Ctext%257BPSNR%257D_%257B%255Ctext%257BDIV%257D%257D%2524%2520achieves%2520statistically%2520significant%2520improvements%253A%2520%252B0.09%2520Pearson%2520Linear%2520Correlation%2520Coefficient%2520over%2520FloLPIPS%252C%2520while%2520being%25202.5%2524%255Ctimes%2524%2520faster%2520and%2520using%25204%2524%255Ctimes%2524%2520less%2520memory.%2520Performance%2520remains%2520consistent%2520across%2520all%2520content%2520categories%2520and%2520are%2520robust%2520to%2520the%2520motion%2520estimator%2520used.%2520The%2520efficiency%2520and%2520accuracy%2520of%2520%2524%255Ctext%257BPSNR%257D_%257B%255Ctext%257BDIV%257D%257D%2524%2520enables%2520fast%2520quality%2520evaluation%2520and%2520practical%2520use%2520as%2520a%2520loss%2520function%2520for%2520training%2520neural%2520networks%2520for%2520video%2520frame%2520interpolation%2520tasks.%2520An%2520implementation%2520of%2520our%2520metric%2520is%2520available%2520at%2520www.github.com/conalld/psnr-div.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Quality%20Metric%20for%20Video%20Frame%20Interpolation%20Based%20on%20Motion-Field%20Divergence&entry.906535625=Conall%20Daly%20and%20Darren%20Ramsook%20and%20Anil%20Kokaram&entry.1292438233=Video%20frame%20interpolation%20is%20a%20fundamental%20tool%20for%20temporal%20video%20enhancement%2C%20but%20existing%20quality%20metrics%20struggle%20to%20evaluate%20the%20perceptual%20impact%20of%20interpolation%20artefacts%20effectively.%20Metrics%20like%20PSNR%2C%20SSIM%20and%20LPIPS%20ignore%20temporal%20coherence.%20State-of-the-art%20quality%20metrics%20tailored%20towards%20video%20frame%20interpolation%2C%20like%20FloLPIPS%2C%20have%20been%20developed%20but%20suffer%20from%20computational%20inefficiency%20that%20limits%20their%20practical%20application.%20We%20present%20%24%5Ctext%7BPSNR%7D_%7B%5Ctext%7BDIV%7D%7D%24%2C%20a%20novel%20full-reference%20quality%20metric%20that%20enhances%20PSNR%20through%20motion%20divergence%20weighting%2C%20a%20technique%20adapted%20from%20archival%20film%20restoration%20where%20it%20was%20developed%20to%20detect%20temporal%20inconsistencies.%20Our%20approach%20highlights%20singularities%20in%20motion%20fields%20which%20is%20then%20used%20to%20weight%20image%20errors.%20Evaluation%20on%20the%20BVI-VFI%20dataset%20%28180%20sequences%20across%20multiple%20frame%20rates%2C%20resolutions%20and%20interpolation%20methods%29%20shows%20%24%5Ctext%7BPSNR%7D_%7B%5Ctext%7BDIV%7D%7D%24%20achieves%20statistically%20significant%20improvements%3A%20%2B0.09%20Pearson%20Linear%20Correlation%20Coefficient%20over%20FloLPIPS%2C%20while%20being%202.5%24%5Ctimes%24%20faster%20and%20using%204%24%5Ctimes%24%20less%20memory.%20Performance%20remains%20consistent%20across%20all%20content%20categories%20and%20are%20robust%20to%20the%20motion%20estimator%20used.%20The%20efficiency%20and%20accuracy%20of%20%24%5Ctext%7BPSNR%7D_%7B%5Ctext%7BDIV%7D%7D%24%20enables%20fast%20quality%20evaluation%20and%20practical%20use%20as%20a%20loss%20function%20for%20training%20neural%20networks%20for%20video%20frame%20interpolation%20tasks.%20An%20implementation%20of%20our%20metric%20is%20available%20at%20www.github.com/conalld/psnr-div.&entry.1838667208=http%3A//arxiv.org/abs/2510.01361v2&entry.124074799=Read"},
{"title": "Risk reversal for least squares estimators under nested convex constraints", "author": "Omar Al-Ghattas", "abstract": "In constrained stochastic optimization, one naturally expects that imposing a stricter feasible set does not increase the statistical risk of an estimator defined by projection onto that set. In this paper, we show that this intuition can fail even in canonical settings.\n  We study the Gaussian sequence model, a deliberately austere test best, where for a compact, convex set $\u0398\\subset \\mathbb{R}^d$ one observes \\[ Y = \u03b8^\\star + \u03c3Z, \\qquad Z \\sim N(0, I_d), \\] and seeks to estimate an unknown parameter $\u03b8^\\star \\in \u0398$. The natural estimator is the least squares estimator (LSE), which coincides with the Euclidean projection of $Y$ onto $\u0398$. We construct an explicit example exhibiting \\emph{risk reversal}: for sufficiently large noise, there exist nested compact convex sets $\u0398_S \\subset \u0398_L$ and a parameter $\u03b8^\\star \\in \u0398_S$ such that the LSE constrained to $\u0398_S$ has strictly larger risk than the LSE constrained to $\u0398_L$. We further show that this phenomenon can persist at the level of worst-case risk, with the supremum risk over the smaller constraint set exceeding that over the larger one.\n  We clarify this behavior by contrasting noise regimes. In the vanishing-noise limit, the risk admits a first-order expansion governed by the statistical dimension of the tangent cone at $\u03b8^\\star$, and tighter constraints uniformly reduce risk. In contrast, in the diverging-noise regime, the risk is determined by global geometric interactions between the constraint set and random noise directions. Here, the embedding of $\u0398_S$ within $\u0398_L$ can reverse the risk ordering.\n  These results reveal a previously unrecognized failure mode of projection-based estimators: in sufficiently noisy settings, tightening a constraint can paradoxically degrade statistical performance.", "link": "http://arxiv.org/abs/2601.16041v1", "date": "2026-01-22", "relevancy": 2.125, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4356}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4206}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk%20reversal%20for%20least%20squares%20estimators%20under%20nested%20convex%20constraints&body=Title%3A%20Risk%20reversal%20for%20least%20squares%20estimators%20under%20nested%20convex%20constraints%0AAuthor%3A%20Omar%20Al-Ghattas%0AAbstract%3A%20In%20constrained%20stochastic%20optimization%2C%20one%20naturally%20expects%20that%20imposing%20a%20stricter%20feasible%20set%20does%20not%20increase%20the%20statistical%20risk%20of%20an%20estimator%20defined%20by%20projection%20onto%20that%20set.%20In%20this%20paper%2C%20we%20show%20that%20this%20intuition%20can%20fail%20even%20in%20canonical%20settings.%0A%20%20We%20study%20the%20Gaussian%20sequence%20model%2C%20a%20deliberately%20austere%20test%20best%2C%20where%20for%20a%20compact%2C%20convex%20set%20%24%CE%98%5Csubset%20%5Cmathbb%7BR%7D%5Ed%24%20one%20observes%20%5C%5B%20Y%20%3D%20%CE%B8%5E%5Cstar%20%2B%20%CF%83Z%2C%20%5Cqquad%20Z%20%5Csim%20N%280%2C%20I_d%29%2C%20%5C%5D%20and%20seeks%20to%20estimate%20an%20unknown%20parameter%20%24%CE%B8%5E%5Cstar%20%5Cin%20%CE%98%24.%20The%20natural%20estimator%20is%20the%20least%20squares%20estimator%20%28LSE%29%2C%20which%20coincides%20with%20the%20Euclidean%20projection%20of%20%24Y%24%20onto%20%24%CE%98%24.%20We%20construct%20an%20explicit%20example%20exhibiting%20%5Cemph%7Brisk%20reversal%7D%3A%20for%20sufficiently%20large%20noise%2C%20there%20exist%20nested%20compact%20convex%20sets%20%24%CE%98_S%20%5Csubset%20%CE%98_L%24%20and%20a%20parameter%20%24%CE%B8%5E%5Cstar%20%5Cin%20%CE%98_S%24%20such%20that%20the%20LSE%20constrained%20to%20%24%CE%98_S%24%20has%20strictly%20larger%20risk%20than%20the%20LSE%20constrained%20to%20%24%CE%98_L%24.%20We%20further%20show%20that%20this%20phenomenon%20can%20persist%20at%20the%20level%20of%20worst-case%20risk%2C%20with%20the%20supremum%20risk%20over%20the%20smaller%20constraint%20set%20exceeding%20that%20over%20the%20larger%20one.%0A%20%20We%20clarify%20this%20behavior%20by%20contrasting%20noise%20regimes.%20In%20the%20vanishing-noise%20limit%2C%20the%20risk%20admits%20a%20first-order%20expansion%20governed%20by%20the%20statistical%20dimension%20of%20the%20tangent%20cone%20at%20%24%CE%B8%5E%5Cstar%24%2C%20and%20tighter%20constraints%20uniformly%20reduce%20risk.%20In%20contrast%2C%20in%20the%20diverging-noise%20regime%2C%20the%20risk%20is%20determined%20by%20global%20geometric%20interactions%20between%20the%20constraint%20set%20and%20random%20noise%20directions.%20Here%2C%20the%20embedding%20of%20%24%CE%98_S%24%20within%20%24%CE%98_L%24%20can%20reverse%20the%20risk%20ordering.%0A%20%20These%20results%20reveal%20a%20previously%20unrecognized%20failure%20mode%20of%20projection-based%20estimators%3A%20in%20sufficiently%20noisy%20settings%2C%20tightening%20a%20constraint%20can%20paradoxically%20degrade%20statistical%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk%2520reversal%2520for%2520least%2520squares%2520estimators%2520under%2520nested%2520convex%2520constraints%26entry.906535625%3DOmar%2520Al-Ghattas%26entry.1292438233%3DIn%2520constrained%2520stochastic%2520optimization%252C%2520one%2520naturally%2520expects%2520that%2520imposing%2520a%2520stricter%2520feasible%2520set%2520does%2520not%2520increase%2520the%2520statistical%2520risk%2520of%2520an%2520estimator%2520defined%2520by%2520projection%2520onto%2520that%2520set.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520this%2520intuition%2520can%2520fail%2520even%2520in%2520canonical%2520settings.%250A%2520%2520We%2520study%2520the%2520Gaussian%2520sequence%2520model%252C%2520a%2520deliberately%2520austere%2520test%2520best%252C%2520where%2520for%2520a%2520compact%252C%2520convex%2520set%2520%2524%25CE%2598%255Csubset%2520%255Cmathbb%257BR%257D%255Ed%2524%2520one%2520observes%2520%255C%255B%2520Y%2520%253D%2520%25CE%25B8%255E%255Cstar%2520%252B%2520%25CF%2583Z%252C%2520%255Cqquad%2520Z%2520%255Csim%2520N%25280%252C%2520I_d%2529%252C%2520%255C%255D%2520and%2520seeks%2520to%2520estimate%2520an%2520unknown%2520parameter%2520%2524%25CE%25B8%255E%255Cstar%2520%255Cin%2520%25CE%2598%2524.%2520The%2520natural%2520estimator%2520is%2520the%2520least%2520squares%2520estimator%2520%2528LSE%2529%252C%2520which%2520coincides%2520with%2520the%2520Euclidean%2520projection%2520of%2520%2524Y%2524%2520onto%2520%2524%25CE%2598%2524.%2520We%2520construct%2520an%2520explicit%2520example%2520exhibiting%2520%255Cemph%257Brisk%2520reversal%257D%253A%2520for%2520sufficiently%2520large%2520noise%252C%2520there%2520exist%2520nested%2520compact%2520convex%2520sets%2520%2524%25CE%2598_S%2520%255Csubset%2520%25CE%2598_L%2524%2520and%2520a%2520parameter%2520%2524%25CE%25B8%255E%255Cstar%2520%255Cin%2520%25CE%2598_S%2524%2520such%2520that%2520the%2520LSE%2520constrained%2520to%2520%2524%25CE%2598_S%2524%2520has%2520strictly%2520larger%2520risk%2520than%2520the%2520LSE%2520constrained%2520to%2520%2524%25CE%2598_L%2524.%2520We%2520further%2520show%2520that%2520this%2520phenomenon%2520can%2520persist%2520at%2520the%2520level%2520of%2520worst-case%2520risk%252C%2520with%2520the%2520supremum%2520risk%2520over%2520the%2520smaller%2520constraint%2520set%2520exceeding%2520that%2520over%2520the%2520larger%2520one.%250A%2520%2520We%2520clarify%2520this%2520behavior%2520by%2520contrasting%2520noise%2520regimes.%2520In%2520the%2520vanishing-noise%2520limit%252C%2520the%2520risk%2520admits%2520a%2520first-order%2520expansion%2520governed%2520by%2520the%2520statistical%2520dimension%2520of%2520the%2520tangent%2520cone%2520at%2520%2524%25CE%25B8%255E%255Cstar%2524%252C%2520and%2520tighter%2520constraints%2520uniformly%2520reduce%2520risk.%2520In%2520contrast%252C%2520in%2520the%2520diverging-noise%2520regime%252C%2520the%2520risk%2520is%2520determined%2520by%2520global%2520geometric%2520interactions%2520between%2520the%2520constraint%2520set%2520and%2520random%2520noise%2520directions.%2520Here%252C%2520the%2520embedding%2520of%2520%2524%25CE%2598_S%2524%2520within%2520%2524%25CE%2598_L%2524%2520can%2520reverse%2520the%2520risk%2520ordering.%250A%2520%2520These%2520results%2520reveal%2520a%2520previously%2520unrecognized%2520failure%2520mode%2520of%2520projection-based%2520estimators%253A%2520in%2520sufficiently%2520noisy%2520settings%252C%2520tightening%2520a%2520constraint%2520can%2520paradoxically%2520degrade%2520statistical%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk%20reversal%20for%20least%20squares%20estimators%20under%20nested%20convex%20constraints&entry.906535625=Omar%20Al-Ghattas&entry.1292438233=In%20constrained%20stochastic%20optimization%2C%20one%20naturally%20expects%20that%20imposing%20a%20stricter%20feasible%20set%20does%20not%20increase%20the%20statistical%20risk%20of%20an%20estimator%20defined%20by%20projection%20onto%20that%20set.%20In%20this%20paper%2C%20we%20show%20that%20this%20intuition%20can%20fail%20even%20in%20canonical%20settings.%0A%20%20We%20study%20the%20Gaussian%20sequence%20model%2C%20a%20deliberately%20austere%20test%20best%2C%20where%20for%20a%20compact%2C%20convex%20set%20%24%CE%98%5Csubset%20%5Cmathbb%7BR%7D%5Ed%24%20one%20observes%20%5C%5B%20Y%20%3D%20%CE%B8%5E%5Cstar%20%2B%20%CF%83Z%2C%20%5Cqquad%20Z%20%5Csim%20N%280%2C%20I_d%29%2C%20%5C%5D%20and%20seeks%20to%20estimate%20an%20unknown%20parameter%20%24%CE%B8%5E%5Cstar%20%5Cin%20%CE%98%24.%20The%20natural%20estimator%20is%20the%20least%20squares%20estimator%20%28LSE%29%2C%20which%20coincides%20with%20the%20Euclidean%20projection%20of%20%24Y%24%20onto%20%24%CE%98%24.%20We%20construct%20an%20explicit%20example%20exhibiting%20%5Cemph%7Brisk%20reversal%7D%3A%20for%20sufficiently%20large%20noise%2C%20there%20exist%20nested%20compact%20convex%20sets%20%24%CE%98_S%20%5Csubset%20%CE%98_L%24%20and%20a%20parameter%20%24%CE%B8%5E%5Cstar%20%5Cin%20%CE%98_S%24%20such%20that%20the%20LSE%20constrained%20to%20%24%CE%98_S%24%20has%20strictly%20larger%20risk%20than%20the%20LSE%20constrained%20to%20%24%CE%98_L%24.%20We%20further%20show%20that%20this%20phenomenon%20can%20persist%20at%20the%20level%20of%20worst-case%20risk%2C%20with%20the%20supremum%20risk%20over%20the%20smaller%20constraint%20set%20exceeding%20that%20over%20the%20larger%20one.%0A%20%20We%20clarify%20this%20behavior%20by%20contrasting%20noise%20regimes.%20In%20the%20vanishing-noise%20limit%2C%20the%20risk%20admits%20a%20first-order%20expansion%20governed%20by%20the%20statistical%20dimension%20of%20the%20tangent%20cone%20at%20%24%CE%B8%5E%5Cstar%24%2C%20and%20tighter%20constraints%20uniformly%20reduce%20risk.%20In%20contrast%2C%20in%20the%20diverging-noise%20regime%2C%20the%20risk%20is%20determined%20by%20global%20geometric%20interactions%20between%20the%20constraint%20set%20and%20random%20noise%20directions.%20Here%2C%20the%20embedding%20of%20%24%CE%98_S%24%20within%20%24%CE%98_L%24%20can%20reverse%20the%20risk%20ordering.%0A%20%20These%20results%20reveal%20a%20previously%20unrecognized%20failure%20mode%20of%20projection-based%20estimators%3A%20in%20sufficiently%20noisy%20settings%2C%20tightening%20a%20constraint%20can%20paradoxically%20degrade%20statistical%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.16041v1&entry.124074799=Read"},
{"title": "TeleMem: Building Long-Term and Multimodal Memory for Agentic AI", "author": "Chunliang Chen and Ming Guan and Xiao Lin and Jiaxu Li and Luxi Lin and Qiyi Wang and Xiangyu Chen and Jixiang Luo and Changzhi Sun and Dell Zhang and Xuelong Li", "abstract": "Large language models (LLMs) excel at many NLP tasks but struggle to sustain long-term interactions due to limited attention over extended dialogue histories. Retrieval-augmented generation (RAG) mitigates this issue but lacks reliable mechanisms for updating or refining stored memories, leading to schema-driven hallucinations, inefficient write operations, and minimal support for multimodal reasoning.To address these challenges, we propose TeleMem, a unified long-term and multimodal memory system that maintains coherent user profiles through narrative dynamic extraction, ensuring that only dialogue-grounded information is preserved. TeleMem further introduces a structured writing pipeline that batches, retrieves, clusters, and consolidates memory entries, substantially improving storage efficiency, reducing token usage, and accelerating memory operations. Additionally, a multimodal memory module combined with ReAct-style reasoning equips the system with a closed-loop observe, think, and act process that enables accurate understanding of complex video content in long-term contexts. Experimental results show that TeleMem surpasses the state-of-the-art Mem0 baseline with 19% higher accuracy, 43% fewer tokens, and a 2.1x speedup on the ZH-4O long-term role-play gaming benchmark.", "link": "http://arxiv.org/abs/2601.06037v4", "date": "2026-01-22", "relevancy": 2.1204, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5556}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeleMem%3A%20Building%20Long-Term%20and%20Multimodal%20Memory%20for%20Agentic%20AI&body=Title%3A%20TeleMem%3A%20Building%20Long-Term%20and%20Multimodal%20Memory%20for%20Agentic%20AI%0AAuthor%3A%20Chunliang%20Chen%20and%20Ming%20Guan%20and%20Xiao%20Lin%20and%20Jiaxu%20Li%20and%20Luxi%20Lin%20and%20Qiyi%20Wang%20and%20Xiangyu%20Chen%20and%20Jixiang%20Luo%20and%20Changzhi%20Sun%20and%20Dell%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20excel%20at%20many%20NLP%20tasks%20but%20struggle%20to%20sustain%20long-term%20interactions%20due%20to%20limited%20attention%20over%20extended%20dialogue%20histories.%20Retrieval-augmented%20generation%20%28RAG%29%20mitigates%20this%20issue%20but%20lacks%20reliable%20mechanisms%20for%20updating%20or%20refining%20stored%20memories%2C%20leading%20to%20schema-driven%20hallucinations%2C%20inefficient%20write%20operations%2C%20and%20minimal%20support%20for%20multimodal%20reasoning.To%20address%20these%20challenges%2C%20we%20propose%20TeleMem%2C%20a%20unified%20long-term%20and%20multimodal%20memory%20system%20that%20maintains%20coherent%20user%20profiles%20through%20narrative%20dynamic%20extraction%2C%20ensuring%20that%20only%20dialogue-grounded%20information%20is%20preserved.%20TeleMem%20further%20introduces%20a%20structured%20writing%20pipeline%20that%20batches%2C%20retrieves%2C%20clusters%2C%20and%20consolidates%20memory%20entries%2C%20substantially%20improving%20storage%20efficiency%2C%20reducing%20token%20usage%2C%20and%20accelerating%20memory%20operations.%20Additionally%2C%20a%20multimodal%20memory%20module%20combined%20with%20ReAct-style%20reasoning%20equips%20the%20system%20with%20a%20closed-loop%20observe%2C%20think%2C%20and%20act%20process%20that%20enables%20accurate%20understanding%20of%20complex%20video%20content%20in%20long-term%20contexts.%20Experimental%20results%20show%20that%20TeleMem%20surpasses%20the%20state-of-the-art%20Mem0%20baseline%20with%2019%25%20higher%20accuracy%2C%2043%25%20fewer%20tokens%2C%20and%20a%202.1x%20speedup%20on%20the%20ZH-4O%20long-term%20role-play%20gaming%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06037v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeleMem%253A%2520Building%2520Long-Term%2520and%2520Multimodal%2520Memory%2520for%2520Agentic%2520AI%26entry.906535625%3DChunliang%2520Chen%2520and%2520Ming%2520Guan%2520and%2520Xiao%2520Lin%2520and%2520Jiaxu%2520Li%2520and%2520Luxi%2520Lin%2520and%2520Qiyi%2520Wang%2520and%2520Xiangyu%2520Chen%2520and%2520Jixiang%2520Luo%2520and%2520Changzhi%2520Sun%2520and%2520Dell%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520many%2520NLP%2520tasks%2520but%2520struggle%2520to%2520sustain%2520long-term%2520interactions%2520due%2520to%2520limited%2520attention%2520over%2520extended%2520dialogue%2520histories.%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520mitigates%2520this%2520issue%2520but%2520lacks%2520reliable%2520mechanisms%2520for%2520updating%2520or%2520refining%2520stored%2520memories%252C%2520leading%2520to%2520schema-driven%2520hallucinations%252C%2520inefficient%2520write%2520operations%252C%2520and%2520minimal%2520support%2520for%2520multimodal%2520reasoning.To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520TeleMem%252C%2520a%2520unified%2520long-term%2520and%2520multimodal%2520memory%2520system%2520that%2520maintains%2520coherent%2520user%2520profiles%2520through%2520narrative%2520dynamic%2520extraction%252C%2520ensuring%2520that%2520only%2520dialogue-grounded%2520information%2520is%2520preserved.%2520TeleMem%2520further%2520introduces%2520a%2520structured%2520writing%2520pipeline%2520that%2520batches%252C%2520retrieves%252C%2520clusters%252C%2520and%2520consolidates%2520memory%2520entries%252C%2520substantially%2520improving%2520storage%2520efficiency%252C%2520reducing%2520token%2520usage%252C%2520and%2520accelerating%2520memory%2520operations.%2520Additionally%252C%2520a%2520multimodal%2520memory%2520module%2520combined%2520with%2520ReAct-style%2520reasoning%2520equips%2520the%2520system%2520with%2520a%2520closed-loop%2520observe%252C%2520think%252C%2520and%2520act%2520process%2520that%2520enables%2520accurate%2520understanding%2520of%2520complex%2520video%2520content%2520in%2520long-term%2520contexts.%2520Experimental%2520results%2520show%2520that%2520TeleMem%2520surpasses%2520the%2520state-of-the-art%2520Mem0%2520baseline%2520with%252019%2525%2520higher%2520accuracy%252C%252043%2525%2520fewer%2520tokens%252C%2520and%2520a%25202.1x%2520speedup%2520on%2520the%2520ZH-4O%2520long-term%2520role-play%2520gaming%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06037v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeleMem%3A%20Building%20Long-Term%20and%20Multimodal%20Memory%20for%20Agentic%20AI&entry.906535625=Chunliang%20Chen%20and%20Ming%20Guan%20and%20Xiao%20Lin%20and%20Jiaxu%20Li%20and%20Luxi%20Lin%20and%20Qiyi%20Wang%20and%20Xiangyu%20Chen%20and%20Jixiang%20Luo%20and%20Changzhi%20Sun%20and%20Dell%20Zhang%20and%20Xuelong%20Li&entry.1292438233=Large%20language%20models%20%28LLMs%29%20excel%20at%20many%20NLP%20tasks%20but%20struggle%20to%20sustain%20long-term%20interactions%20due%20to%20limited%20attention%20over%20extended%20dialogue%20histories.%20Retrieval-augmented%20generation%20%28RAG%29%20mitigates%20this%20issue%20but%20lacks%20reliable%20mechanisms%20for%20updating%20or%20refining%20stored%20memories%2C%20leading%20to%20schema-driven%20hallucinations%2C%20inefficient%20write%20operations%2C%20and%20minimal%20support%20for%20multimodal%20reasoning.To%20address%20these%20challenges%2C%20we%20propose%20TeleMem%2C%20a%20unified%20long-term%20and%20multimodal%20memory%20system%20that%20maintains%20coherent%20user%20profiles%20through%20narrative%20dynamic%20extraction%2C%20ensuring%20that%20only%20dialogue-grounded%20information%20is%20preserved.%20TeleMem%20further%20introduces%20a%20structured%20writing%20pipeline%20that%20batches%2C%20retrieves%2C%20clusters%2C%20and%20consolidates%20memory%20entries%2C%20substantially%20improving%20storage%20efficiency%2C%20reducing%20token%20usage%2C%20and%20accelerating%20memory%20operations.%20Additionally%2C%20a%20multimodal%20memory%20module%20combined%20with%20ReAct-style%20reasoning%20equips%20the%20system%20with%20a%20closed-loop%20observe%2C%20think%2C%20and%20act%20process%20that%20enables%20accurate%20understanding%20of%20complex%20video%20content%20in%20long-term%20contexts.%20Experimental%20results%20show%20that%20TeleMem%20surpasses%20the%20state-of-the-art%20Mem0%20baseline%20with%2019%25%20higher%20accuracy%2C%2043%25%20fewer%20tokens%2C%20and%20a%202.1x%20speedup%20on%20the%20ZH-4O%20long-term%20role-play%20gaming%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2601.06037v4&entry.124074799=Read"},
{"title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources", "author": "Marzieh Adeli Shamsabad and Hamed Ghodrati", "abstract": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.", "link": "http://arxiv.org/abs/2601.16108v1", "date": "2026-01-22", "relevancy": 2.1113, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5461}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Climate%20Disinformation%20Detection%3A%20Integrating%20Vision-Language%20Models%20with%20External%20Knowledge%20Sources&body=Title%3A%20Multimodal%20Climate%20Disinformation%20Detection%3A%20Integrating%20Vision-Language%20Models%20with%20External%20Knowledge%20Sources%0AAuthor%3A%20Marzieh%20Adeli%20Shamsabad%20and%20Hamed%20Ghodrati%0AAbstract%3A%20Climate%20disinformation%20has%20become%20a%20major%20challenge%20in%20today%20digital%20world%2C%20especially%20with%20the%20rise%20of%20misleading%20images%20and%20videos%20shared%20widely%20on%20social%20media.%20These%20false%20claims%20are%20often%20convincing%20and%20difficult%20to%20detect%2C%20which%20can%20delay%20actions%20on%20climate%20change.%20While%20vision-language%20models%20%28VLMs%29%20have%20been%20used%20to%20identify%20visual%20disinformation%2C%20they%20rely%20only%20on%20the%20knowledge%20available%20at%20the%20time%20of%20training.%20This%20limits%20their%20ability%20to%20reason%20about%20recent%20events%20or%20updates.%20The%20main%20goal%20of%20this%20paper%20is%20to%20overcome%20that%20limitation%20by%20combining%20VLMs%20with%20external%20knowledge.%20By%20retrieving%20up-to-date%20information%20such%20as%20reverse%20image%20results%2C%20online%20fact-checks%2C%20and%20trusted%20expert%20content%2C%20the%20system%20can%20better%20assess%20whether%20an%20image%20and%20its%20claim%20are%20accurate%2C%20misleading%2C%20false%2C%20or%20unverifiable.%20This%20approach%20improves%20the%20model%20ability%20to%20handle%20real-world%20climate%20disinformation%20and%20supports%20efforts%20to%20protect%20public%20understanding%20of%20science%20in%20a%20rapidly%20changing%20information%20landscape.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Climate%2520Disinformation%2520Detection%253A%2520Integrating%2520Vision-Language%2520Models%2520with%2520External%2520Knowledge%2520Sources%26entry.906535625%3DMarzieh%2520Adeli%2520Shamsabad%2520and%2520Hamed%2520Ghodrati%26entry.1292438233%3DClimate%2520disinformation%2520has%2520become%2520a%2520major%2520challenge%2520in%2520today%2520digital%2520world%252C%2520especially%2520with%2520the%2520rise%2520of%2520misleading%2520images%2520and%2520videos%2520shared%2520widely%2520on%2520social%2520media.%2520These%2520false%2520claims%2520are%2520often%2520convincing%2520and%2520difficult%2520to%2520detect%252C%2520which%2520can%2520delay%2520actions%2520on%2520climate%2520change.%2520While%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520been%2520used%2520to%2520identify%2520visual%2520disinformation%252C%2520they%2520rely%2520only%2520on%2520the%2520knowledge%2520available%2520at%2520the%2520time%2520of%2520training.%2520This%2520limits%2520their%2520ability%2520to%2520reason%2520about%2520recent%2520events%2520or%2520updates.%2520The%2520main%2520goal%2520of%2520this%2520paper%2520is%2520to%2520overcome%2520that%2520limitation%2520by%2520combining%2520VLMs%2520with%2520external%2520knowledge.%2520By%2520retrieving%2520up-to-date%2520information%2520such%2520as%2520reverse%2520image%2520results%252C%2520online%2520fact-checks%252C%2520and%2520trusted%2520expert%2520content%252C%2520the%2520system%2520can%2520better%2520assess%2520whether%2520an%2520image%2520and%2520its%2520claim%2520are%2520accurate%252C%2520misleading%252C%2520false%252C%2520or%2520unverifiable.%2520This%2520approach%2520improves%2520the%2520model%2520ability%2520to%2520handle%2520real-world%2520climate%2520disinformation%2520and%2520supports%2520efforts%2520to%2520protect%2520public%2520understanding%2520of%2520science%2520in%2520a%2520rapidly%2520changing%2520information%2520landscape.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Climate%20Disinformation%20Detection%3A%20Integrating%20Vision-Language%20Models%20with%20External%20Knowledge%20Sources&entry.906535625=Marzieh%20Adeli%20Shamsabad%20and%20Hamed%20Ghodrati&entry.1292438233=Climate%20disinformation%20has%20become%20a%20major%20challenge%20in%20today%20digital%20world%2C%20especially%20with%20the%20rise%20of%20misleading%20images%20and%20videos%20shared%20widely%20on%20social%20media.%20These%20false%20claims%20are%20often%20convincing%20and%20difficult%20to%20detect%2C%20which%20can%20delay%20actions%20on%20climate%20change.%20While%20vision-language%20models%20%28VLMs%29%20have%20been%20used%20to%20identify%20visual%20disinformation%2C%20they%20rely%20only%20on%20the%20knowledge%20available%20at%20the%20time%20of%20training.%20This%20limits%20their%20ability%20to%20reason%20about%20recent%20events%20or%20updates.%20The%20main%20goal%20of%20this%20paper%20is%20to%20overcome%20that%20limitation%20by%20combining%20VLMs%20with%20external%20knowledge.%20By%20retrieving%20up-to-date%20information%20such%20as%20reverse%20image%20results%2C%20online%20fact-checks%2C%20and%20trusted%20expert%20content%2C%20the%20system%20can%20better%20assess%20whether%20an%20image%20and%20its%20claim%20are%20accurate%2C%20misleading%2C%20false%2C%20or%20unverifiable.%20This%20approach%20improves%20the%20model%20ability%20to%20handle%20real-world%20climate%20disinformation%20and%20supports%20efforts%20to%20protect%20public%20understanding%20of%20science%20in%20a%20rapidly%20changing%20information%20landscape.&entry.1838667208=http%3A//arxiv.org/abs/2601.16108v1&entry.124074799=Read"},
{"title": "Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural Language Data", "author": "Paul Quinlan and Qingguo Li and Xiaodan Zhu", "abstract": "Large language models are being rapidly deployed across many fields such as healthcare, finance, transportation, and energy, where time-series data are fundamental components. The current works are still limited in their ability to perform reasoning that involves both time-series and the corresponding textual content. We address this gap by introducing Chat-TS, a large language model (LLM) based framework designed to support reasoning over time series and textual data. Unlike traditional models, Chat-TS integrates time-series tokens into LLMs' vocabulary, enhancing its reasoning ability over both modalities without compromising core natural language capabilities. To support learning and evaluation, we contribute new datasets: the TS Instruct Training Dataset (pairing diverse time-series data with relevant text instructions and responses for instruction tuning), the TS Instruct Question and Answer (QA) Gold Dataset (multiple-choice questions to evaluate multimodal reasoning), and a TS Instruct Quantitative Probing Set (a small subset of TS Instruct QA reasoning tasks alongside math and decision-making questions for LLM evaluation). We design a training strategy to preserve the inherent reasoning capabilities of LLMs while augmenting them for time-series reasoning. Experiments show that Chat-TS achieves state-of-the-art performance in multimodal reasoning tasks by maintaining strong natural language proficiency while improving time-series reasoning.", "link": "http://arxiv.org/abs/2503.10883v2", "date": "2026-01-22", "relevancy": 2.1039, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5616}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chat-TS%3A%20Enhancing%20Multi-Modal%20Reasoning%20Over%20Time-Series%20and%20Natural%20Language%20Data&body=Title%3A%20Chat-TS%3A%20Enhancing%20Multi-Modal%20Reasoning%20Over%20Time-Series%20and%20Natural%20Language%20Data%0AAuthor%3A%20Paul%20Quinlan%20and%20Qingguo%20Li%20and%20Xiaodan%20Zhu%0AAbstract%3A%20Large%20language%20models%20are%20being%20rapidly%20deployed%20across%20many%20fields%20such%20as%20healthcare%2C%20finance%2C%20transportation%2C%20and%20energy%2C%20where%20time-series%20data%20are%20fundamental%20components.%20The%20current%20works%20are%20still%20limited%20in%20their%20ability%20to%20perform%20reasoning%20that%20involves%20both%20time-series%20and%20the%20corresponding%20textual%20content.%20We%20address%20this%20gap%20by%20introducing%20Chat-TS%2C%20a%20large%20language%20model%20%28LLM%29%20based%20framework%20designed%20to%20support%20reasoning%20over%20time%20series%20and%20textual%20data.%20Unlike%20traditional%20models%2C%20Chat-TS%20integrates%20time-series%20tokens%20into%20LLMs%27%20vocabulary%2C%20enhancing%20its%20reasoning%20ability%20over%20both%20modalities%20without%20compromising%20core%20natural%20language%20capabilities.%20To%20support%20learning%20and%20evaluation%2C%20we%20contribute%20new%20datasets%3A%20the%20TS%20Instruct%20Training%20Dataset%20%28pairing%20diverse%20time-series%20data%20with%20relevant%20text%20instructions%20and%20responses%20for%20instruction%20tuning%29%2C%20the%20TS%20Instruct%20Question%20and%20Answer%20%28QA%29%20Gold%20Dataset%20%28multiple-choice%20questions%20to%20evaluate%20multimodal%20reasoning%29%2C%20and%20a%20TS%20Instruct%20Quantitative%20Probing%20Set%20%28a%20small%20subset%20of%20TS%20Instruct%20QA%20reasoning%20tasks%20alongside%20math%20and%20decision-making%20questions%20for%20LLM%20evaluation%29.%20We%20design%20a%20training%20strategy%20to%20preserve%20the%20inherent%20reasoning%20capabilities%20of%20LLMs%20while%20augmenting%20them%20for%20time-series%20reasoning.%20Experiments%20show%20that%20Chat-TS%20achieves%20state-of-the-art%20performance%20in%20multimodal%20reasoning%20tasks%20by%20maintaining%20strong%20natural%20language%20proficiency%20while%20improving%20time-series%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2503.10883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChat-TS%253A%2520Enhancing%2520Multi-Modal%2520Reasoning%2520Over%2520Time-Series%2520and%2520Natural%2520Language%2520Data%26entry.906535625%3DPaul%2520Quinlan%2520and%2520Qingguo%2520Li%2520and%2520Xiaodan%2520Zhu%26entry.1292438233%3DLarge%2520language%2520models%2520are%2520being%2520rapidly%2520deployed%2520across%2520many%2520fields%2520such%2520as%2520healthcare%252C%2520finance%252C%2520transportation%252C%2520and%2520energy%252C%2520where%2520time-series%2520data%2520are%2520fundamental%2520components.%2520The%2520current%2520works%2520are%2520still%2520limited%2520in%2520their%2520ability%2520to%2520perform%2520reasoning%2520that%2520involves%2520both%2520time-series%2520and%2520the%2520corresponding%2520textual%2520content.%2520We%2520address%2520this%2520gap%2520by%2520introducing%2520Chat-TS%252C%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520based%2520framework%2520designed%2520to%2520support%2520reasoning%2520over%2520time%2520series%2520and%2520textual%2520data.%2520Unlike%2520traditional%2520models%252C%2520Chat-TS%2520integrates%2520time-series%2520tokens%2520into%2520LLMs%2527%2520vocabulary%252C%2520enhancing%2520its%2520reasoning%2520ability%2520over%2520both%2520modalities%2520without%2520compromising%2520core%2520natural%2520language%2520capabilities.%2520To%2520support%2520learning%2520and%2520evaluation%252C%2520we%2520contribute%2520new%2520datasets%253A%2520the%2520TS%2520Instruct%2520Training%2520Dataset%2520%2528pairing%2520diverse%2520time-series%2520data%2520with%2520relevant%2520text%2520instructions%2520and%2520responses%2520for%2520instruction%2520tuning%2529%252C%2520the%2520TS%2520Instruct%2520Question%2520and%2520Answer%2520%2528QA%2529%2520Gold%2520Dataset%2520%2528multiple-choice%2520questions%2520to%2520evaluate%2520multimodal%2520reasoning%2529%252C%2520and%2520a%2520TS%2520Instruct%2520Quantitative%2520Probing%2520Set%2520%2528a%2520small%2520subset%2520of%2520TS%2520Instruct%2520QA%2520reasoning%2520tasks%2520alongside%2520math%2520and%2520decision-making%2520questions%2520for%2520LLM%2520evaluation%2529.%2520We%2520design%2520a%2520training%2520strategy%2520to%2520preserve%2520the%2520inherent%2520reasoning%2520capabilities%2520of%2520LLMs%2520while%2520augmenting%2520them%2520for%2520time-series%2520reasoning.%2520Experiments%2520show%2520that%2520Chat-TS%2520achieves%2520state-of-the-art%2520performance%2520in%2520multimodal%2520reasoning%2520tasks%2520by%2520maintaining%2520strong%2520natural%2520language%2520proficiency%2520while%2520improving%2520time-series%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chat-TS%3A%20Enhancing%20Multi-Modal%20Reasoning%20Over%20Time-Series%20and%20Natural%20Language%20Data&entry.906535625=Paul%20Quinlan%20and%20Qingguo%20Li%20and%20Xiaodan%20Zhu&entry.1292438233=Large%20language%20models%20are%20being%20rapidly%20deployed%20across%20many%20fields%20such%20as%20healthcare%2C%20finance%2C%20transportation%2C%20and%20energy%2C%20where%20time-series%20data%20are%20fundamental%20components.%20The%20current%20works%20are%20still%20limited%20in%20their%20ability%20to%20perform%20reasoning%20that%20involves%20both%20time-series%20and%20the%20corresponding%20textual%20content.%20We%20address%20this%20gap%20by%20introducing%20Chat-TS%2C%20a%20large%20language%20model%20%28LLM%29%20based%20framework%20designed%20to%20support%20reasoning%20over%20time%20series%20and%20textual%20data.%20Unlike%20traditional%20models%2C%20Chat-TS%20integrates%20time-series%20tokens%20into%20LLMs%27%20vocabulary%2C%20enhancing%20its%20reasoning%20ability%20over%20both%20modalities%20without%20compromising%20core%20natural%20language%20capabilities.%20To%20support%20learning%20and%20evaluation%2C%20we%20contribute%20new%20datasets%3A%20the%20TS%20Instruct%20Training%20Dataset%20%28pairing%20diverse%20time-series%20data%20with%20relevant%20text%20instructions%20and%20responses%20for%20instruction%20tuning%29%2C%20the%20TS%20Instruct%20Question%20and%20Answer%20%28QA%29%20Gold%20Dataset%20%28multiple-choice%20questions%20to%20evaluate%20multimodal%20reasoning%29%2C%20and%20a%20TS%20Instruct%20Quantitative%20Probing%20Set%20%28a%20small%20subset%20of%20TS%20Instruct%20QA%20reasoning%20tasks%20alongside%20math%20and%20decision-making%20questions%20for%20LLM%20evaluation%29.%20We%20design%20a%20training%20strategy%20to%20preserve%20the%20inherent%20reasoning%20capabilities%20of%20LLMs%20while%20augmenting%20them%20for%20time-series%20reasoning.%20Experiments%20show%20that%20Chat-TS%20achieves%20state-of-the-art%20performance%20in%20multimodal%20reasoning%20tasks%20by%20maintaining%20strong%20natural%20language%20proficiency%20while%20improving%20time-series%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2503.10883v2&entry.124074799=Read"},
{"title": "Introducing the Generative Application Firewall (GAF)", "author": "Joan Vendrell Farreny and Mart\u00ed Jord\u00e0 Roca and Miquel Cornudella Gaya and Rodrigo Fern\u00e1ndez Ba\u00f3n and V\u00edctor Garc\u00eda Mart\u00ednez and Eduard Camacho Sucarrat and Alessandro Pignati", "abstract": "This paper introduces the Generative Application Firewall (GAF), a new architectural layer for securing LLM applications. Existing defenses -- prompt filters, guardrails, and data-masking -- remain fragmented; GAF unifies them into a single enforcement point, much like a WAF coordinates defenses for web traffic, while also covering autonomous agents and their tool interactions.", "link": "http://arxiv.org/abs/2601.15824v1", "date": "2026-01-22", "relevancy": 2.0999, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4378}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4142}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20the%20Generative%20Application%20Firewall%20%28GAF%29&body=Title%3A%20Introducing%20the%20Generative%20Application%20Firewall%20%28GAF%29%0AAuthor%3A%20Joan%20Vendrell%20Farreny%20and%20Mart%C3%AD%20Jord%C3%A0%20Roca%20and%20Miquel%20Cornudella%20Gaya%20and%20Rodrigo%20Fern%C3%A1ndez%20Ba%C3%B3n%20and%20V%C3%ADctor%20Garc%C3%ADa%20Mart%C3%ADnez%20and%20Eduard%20Camacho%20Sucarrat%20and%20Alessandro%20Pignati%0AAbstract%3A%20This%20paper%20introduces%20the%20Generative%20Application%20Firewall%20%28GAF%29%2C%20a%20new%20architectural%20layer%20for%20securing%20LLM%20applications.%20Existing%20defenses%20--%20prompt%20filters%2C%20guardrails%2C%20and%20data-masking%20--%20remain%20fragmented%3B%20GAF%20unifies%20them%20into%20a%20single%20enforcement%20point%2C%20much%20like%20a%20WAF%20coordinates%20defenses%20for%20web%20traffic%2C%20while%20also%20covering%20autonomous%20agents%20and%20their%20tool%20interactions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520the%2520Generative%2520Application%2520Firewall%2520%2528GAF%2529%26entry.906535625%3DJoan%2520Vendrell%2520Farreny%2520and%2520Mart%25C3%25AD%2520Jord%25C3%25A0%2520Roca%2520and%2520Miquel%2520Cornudella%2520Gaya%2520and%2520Rodrigo%2520Fern%25C3%25A1ndez%2520Ba%25C3%25B3n%2520and%2520V%25C3%25ADctor%2520Garc%25C3%25ADa%2520Mart%25C3%25ADnez%2520and%2520Eduard%2520Camacho%2520Sucarrat%2520and%2520Alessandro%2520Pignati%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520Generative%2520Application%2520Firewall%2520%2528GAF%2529%252C%2520a%2520new%2520architectural%2520layer%2520for%2520securing%2520LLM%2520applications.%2520Existing%2520defenses%2520--%2520prompt%2520filters%252C%2520guardrails%252C%2520and%2520data-masking%2520--%2520remain%2520fragmented%253B%2520GAF%2520unifies%2520them%2520into%2520a%2520single%2520enforcement%2520point%252C%2520much%2520like%2520a%2520WAF%2520coordinates%2520defenses%2520for%2520web%2520traffic%252C%2520while%2520also%2520covering%2520autonomous%2520agents%2520and%2520their%2520tool%2520interactions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20the%20Generative%20Application%20Firewall%20%28GAF%29&entry.906535625=Joan%20Vendrell%20Farreny%20and%20Mart%C3%AD%20Jord%C3%A0%20Roca%20and%20Miquel%20Cornudella%20Gaya%20and%20Rodrigo%20Fern%C3%A1ndez%20Ba%C3%B3n%20and%20V%C3%ADctor%20Garc%C3%ADa%20Mart%C3%ADnez%20and%20Eduard%20Camacho%20Sucarrat%20and%20Alessandro%20Pignati&entry.1292438233=This%20paper%20introduces%20the%20Generative%20Application%20Firewall%20%28GAF%29%2C%20a%20new%20architectural%20layer%20for%20securing%20LLM%20applications.%20Existing%20defenses%20--%20prompt%20filters%2C%20guardrails%2C%20and%20data-masking%20--%20remain%20fragmented%3B%20GAF%20unifies%20them%20into%20a%20single%20enforcement%20point%2C%20much%20like%20a%20WAF%20coordinates%20defenses%20for%20web%20traffic%2C%20while%20also%20covering%20autonomous%20agents%20and%20their%20tool%20interactions.&entry.1838667208=http%3A//arxiv.org/abs/2601.15824v1&entry.124074799=Read"},
{"title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing", "author": "Tingyu Song and Yanzhao Zhang and Mingxin Li and Zhuoning Guo and Dingkun Long and Pengjun Xie and Siyue Zhang and Yilun Zhao and Shu Wu", "abstract": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.", "link": "http://arxiv.org/abs/2601.16125v1", "date": "2026-01-22", "relevancy": 2.0923, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Composed%20Image%20Retrieval%20Evaluation%3A%20A%20Fine-Grained%20Benchmark%20from%20Image%20Editing&body=Title%3A%20Rethinking%20Composed%20Image%20Retrieval%20Evaluation%3A%20A%20Fine-Grained%20Benchmark%20from%20Image%20Editing%0AAuthor%3A%20Tingyu%20Song%20and%20Yanzhao%20Zhang%20and%20Mingxin%20Li%20and%20Zhuoning%20Guo%20and%20Dingkun%20Long%20and%20Pengjun%20Xie%20and%20Siyue%20Zhang%20and%20Yilun%20Zhao%20and%20Shu%20Wu%0AAbstract%3A%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20pivotal%20and%20complex%20task%20in%20multimodal%20understanding.%20Current%20CIR%20benchmarks%20typically%20feature%20limited%20query%20categories%20and%20fail%20to%20capture%20the%20diverse%20requirements%20of%20real-world%20scenarios.%20To%20bridge%20this%20evaluation%20gap%2C%20we%20leverage%20image%20editing%20to%20achieve%20precise%20control%20over%20modification%20types%20and%20content%2C%20enabling%20a%20pipeline%20for%20synthesizing%20queries%20across%20a%20broad%20spectrum%20of%20categories.%20Using%20this%20pipeline%2C%20we%20construct%20EDIR%2C%20a%20novel%20fine-grained%20CIR%20benchmark.%20EDIR%20encompasses%205%2C000%20high-quality%20queries%20structured%20across%20five%20main%20categories%20and%20fifteen%20subcategories.%20Our%20comprehensive%20evaluation%20of%2013%20multimodal%20embedding%20models%20reveals%20a%20significant%20capability%20gap%3B%20even%20state-of-the-art%20models%20%28e.g.%2C%20RzenEmbed%20and%20GME%29%20struggle%20to%20perform%20consistently%20across%20all%20subcategories%2C%20highlighting%20the%20rigorous%20nature%20of%20our%20benchmark.%20Through%20comparative%20analysis%2C%20we%20further%20uncover%20inherent%20limitations%20in%20existing%20benchmarks%2C%20such%20as%20modality%20biases%20and%20insufficient%20categorical%20coverage.%20Furthermore%2C%20an%20in-domain%20training%20experiment%20demonstrates%20the%20feasibility%20of%20our%20benchmark.%20This%20experiment%20clarifies%20the%20task%20challenges%20by%20distinguishing%20between%20categories%20that%20are%20solvable%20with%20targeted%20data%20and%20those%20that%20expose%20intrinsic%20limitations%20of%20current%20model%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Composed%2520Image%2520Retrieval%2520Evaluation%253A%2520A%2520Fine-Grained%2520Benchmark%2520from%2520Image%2520Editing%26entry.906535625%3DTingyu%2520Song%2520and%2520Yanzhao%2520Zhang%2520and%2520Mingxin%2520Li%2520and%2520Zhuoning%2520Guo%2520and%2520Dingkun%2520Long%2520and%2520Pengjun%2520Xie%2520and%2520Siyue%2520Zhang%2520and%2520Yilun%2520Zhao%2520and%2520Shu%2520Wu%26entry.1292438233%3DComposed%2520Image%2520Retrieval%2520%2528CIR%2529%2520is%2520a%2520pivotal%2520and%2520complex%2520task%2520in%2520multimodal%2520understanding.%2520Current%2520CIR%2520benchmarks%2520typically%2520feature%2520limited%2520query%2520categories%2520and%2520fail%2520to%2520capture%2520the%2520diverse%2520requirements%2520of%2520real-world%2520scenarios.%2520To%2520bridge%2520this%2520evaluation%2520gap%252C%2520we%2520leverage%2520image%2520editing%2520to%2520achieve%2520precise%2520control%2520over%2520modification%2520types%2520and%2520content%252C%2520enabling%2520a%2520pipeline%2520for%2520synthesizing%2520queries%2520across%2520a%2520broad%2520spectrum%2520of%2520categories.%2520Using%2520this%2520pipeline%252C%2520we%2520construct%2520EDIR%252C%2520a%2520novel%2520fine-grained%2520CIR%2520benchmark.%2520EDIR%2520encompasses%25205%252C000%2520high-quality%2520queries%2520structured%2520across%2520five%2520main%2520categories%2520and%2520fifteen%2520subcategories.%2520Our%2520comprehensive%2520evaluation%2520of%252013%2520multimodal%2520embedding%2520models%2520reveals%2520a%2520significant%2520capability%2520gap%253B%2520even%2520state-of-the-art%2520models%2520%2528e.g.%252C%2520RzenEmbed%2520and%2520GME%2529%2520struggle%2520to%2520perform%2520consistently%2520across%2520all%2520subcategories%252C%2520highlighting%2520the%2520rigorous%2520nature%2520of%2520our%2520benchmark.%2520Through%2520comparative%2520analysis%252C%2520we%2520further%2520uncover%2520inherent%2520limitations%2520in%2520existing%2520benchmarks%252C%2520such%2520as%2520modality%2520biases%2520and%2520insufficient%2520categorical%2520coverage.%2520Furthermore%252C%2520an%2520in-domain%2520training%2520experiment%2520demonstrates%2520the%2520feasibility%2520of%2520our%2520benchmark.%2520This%2520experiment%2520clarifies%2520the%2520task%2520challenges%2520by%2520distinguishing%2520between%2520categories%2520that%2520are%2520solvable%2520with%2520targeted%2520data%2520and%2520those%2520that%2520expose%2520intrinsic%2520limitations%2520of%2520current%2520model%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Composed%20Image%20Retrieval%20Evaluation%3A%20A%20Fine-Grained%20Benchmark%20from%20Image%20Editing&entry.906535625=Tingyu%20Song%20and%20Yanzhao%20Zhang%20and%20Mingxin%20Li%20and%20Zhuoning%20Guo%20and%20Dingkun%20Long%20and%20Pengjun%20Xie%20and%20Siyue%20Zhang%20and%20Yilun%20Zhao%20and%20Shu%20Wu&entry.1292438233=Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20pivotal%20and%20complex%20task%20in%20multimodal%20understanding.%20Current%20CIR%20benchmarks%20typically%20feature%20limited%20query%20categories%20and%20fail%20to%20capture%20the%20diverse%20requirements%20of%20real-world%20scenarios.%20To%20bridge%20this%20evaluation%20gap%2C%20we%20leverage%20image%20editing%20to%20achieve%20precise%20control%20over%20modification%20types%20and%20content%2C%20enabling%20a%20pipeline%20for%20synthesizing%20queries%20across%20a%20broad%20spectrum%20of%20categories.%20Using%20this%20pipeline%2C%20we%20construct%20EDIR%2C%20a%20novel%20fine-grained%20CIR%20benchmark.%20EDIR%20encompasses%205%2C000%20high-quality%20queries%20structured%20across%20five%20main%20categories%20and%20fifteen%20subcategories.%20Our%20comprehensive%20evaluation%20of%2013%20multimodal%20embedding%20models%20reveals%20a%20significant%20capability%20gap%3B%20even%20state-of-the-art%20models%20%28e.g.%2C%20RzenEmbed%20and%20GME%29%20struggle%20to%20perform%20consistently%20across%20all%20subcategories%2C%20highlighting%20the%20rigorous%20nature%20of%20our%20benchmark.%20Through%20comparative%20analysis%2C%20we%20further%20uncover%20inherent%20limitations%20in%20existing%20benchmarks%2C%20such%20as%20modality%20biases%20and%20insufficient%20categorical%20coverage.%20Furthermore%2C%20an%20in-domain%20training%20experiment%20demonstrates%20the%20feasibility%20of%20our%20benchmark.%20This%20experiment%20clarifies%20the%20task%20challenges%20by%20distinguishing%20between%20categories%20that%20are%20solvable%20with%20targeted%20data%20and%20those%20that%20expose%20intrinsic%20limitations%20of%20current%20model%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2601.16125v1&entry.124074799=Read"},
{"title": "MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting", "author": "Chenyu Mu and Guihai Chen and Xun Yang and Erkun Yang and Cheng Deng", "abstract": "Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.", "link": "http://arxiv.org/abs/2511.18894v2", "date": "2026-01-22", "relevancy": 2.0912, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.553}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5328}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaDCSeg%3A%20Robust%20Medical%20Image%20Segmentation%20via%20Meta%20Dynamic%20Center%20Weighting&body=Title%3A%20MetaDCSeg%3A%20Robust%20Medical%20Image%20Segmentation%20via%20Meta%20Dynamic%20Center%20Weighting%0AAuthor%3A%20Chenyu%20Mu%20and%20Guihai%20Chen%20and%20Xun%20Yang%20and%20Erkun%20Yang%20and%20Cheng%20Deng%0AAbstract%3A%20Medical%20image%20segmentation%20is%20crucial%20for%20clinical%20applications%2C%20but%20it%20is%20frequently%20disrupted%20by%20noisy%20annotations%20and%20ambiguous%20anatomical%20boundaries%2C%20which%20lead%20to%20instability%20in%20model%20training.%20Existing%20methods%20typically%20rely%20on%20global%20noise%20assumptions%20or%20confidence-based%20sample%20selection%2C%20which%20inadequately%20mitigate%20the%20performance%20degradation%20caused%20by%20annotation%20noise%2C%20especially%20in%20challenging%20boundary%20regions.%20To%20address%20this%20issue%2C%20we%20propose%20MetaDCSeg%2C%20a%20robust%20framework%20that%20dynamically%20learns%20optimal%20pixel-wise%20weights%20to%20suppress%20the%20influence%20of%20noisy%20ground-truth%20labels%20while%20preserving%20reliable%20annotations.%20By%20explicitly%20modeling%20boundary%20uncertainty%20through%20a%20Dynamic%20Center%20Distance%20%28DCD%29%20mechanism%2C%20our%20approach%20utilizes%20weighted%20feature%20distances%20for%20foreground%2C%20background%2C%20and%20boundary%20centers%2C%20directing%20the%20model%27s%20attention%20toward%20hard-to-segment%20pixels%20near%20ambiguous%20boundaries.%20This%20strategy%20enables%20more%20precise%20handling%20of%20structural%20boundaries%2C%20which%20are%20often%20overlooked%20by%20existing%20methods%2C%20and%20significantly%20enhances%20segmentation%20performance.%20Extensive%20experiments%20across%20four%20benchmark%20datasets%20with%20varying%20noise%20levels%20demonstrate%20that%20MetaDCSeg%20consistently%20outperforms%20existing%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaDCSeg%253A%2520Robust%2520Medical%2520Image%2520Segmentation%2520via%2520Meta%2520Dynamic%2520Center%2520Weighting%26entry.906535625%3DChenyu%2520Mu%2520and%2520Guihai%2520Chen%2520and%2520Xun%2520Yang%2520and%2520Erkun%2520Yang%2520and%2520Cheng%2520Deng%26entry.1292438233%3DMedical%2520image%2520segmentation%2520is%2520crucial%2520for%2520clinical%2520applications%252C%2520but%2520it%2520is%2520frequently%2520disrupted%2520by%2520noisy%2520annotations%2520and%2520ambiguous%2520anatomical%2520boundaries%252C%2520which%2520lead%2520to%2520instability%2520in%2520model%2520training.%2520Existing%2520methods%2520typically%2520rely%2520on%2520global%2520noise%2520assumptions%2520or%2520confidence-based%2520sample%2520selection%252C%2520which%2520inadequately%2520mitigate%2520the%2520performance%2520degradation%2520caused%2520by%2520annotation%2520noise%252C%2520especially%2520in%2520challenging%2520boundary%2520regions.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520MetaDCSeg%252C%2520a%2520robust%2520framework%2520that%2520dynamically%2520learns%2520optimal%2520pixel-wise%2520weights%2520to%2520suppress%2520the%2520influence%2520of%2520noisy%2520ground-truth%2520labels%2520while%2520preserving%2520reliable%2520annotations.%2520By%2520explicitly%2520modeling%2520boundary%2520uncertainty%2520through%2520a%2520Dynamic%2520Center%2520Distance%2520%2528DCD%2529%2520mechanism%252C%2520our%2520approach%2520utilizes%2520weighted%2520feature%2520distances%2520for%2520foreground%252C%2520background%252C%2520and%2520boundary%2520centers%252C%2520directing%2520the%2520model%2527s%2520attention%2520toward%2520hard-to-segment%2520pixels%2520near%2520ambiguous%2520boundaries.%2520This%2520strategy%2520enables%2520more%2520precise%2520handling%2520of%2520structural%2520boundaries%252C%2520which%2520are%2520often%2520overlooked%2520by%2520existing%2520methods%252C%2520and%2520significantly%2520enhances%2520segmentation%2520performance.%2520Extensive%2520experiments%2520across%2520four%2520benchmark%2520datasets%2520with%2520varying%2520noise%2520levels%2520demonstrate%2520that%2520MetaDCSeg%2520consistently%2520outperforms%2520existing%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaDCSeg%3A%20Robust%20Medical%20Image%20Segmentation%20via%20Meta%20Dynamic%20Center%20Weighting&entry.906535625=Chenyu%20Mu%20and%20Guihai%20Chen%20and%20Xun%20Yang%20and%20Erkun%20Yang%20and%20Cheng%20Deng&entry.1292438233=Medical%20image%20segmentation%20is%20crucial%20for%20clinical%20applications%2C%20but%20it%20is%20frequently%20disrupted%20by%20noisy%20annotations%20and%20ambiguous%20anatomical%20boundaries%2C%20which%20lead%20to%20instability%20in%20model%20training.%20Existing%20methods%20typically%20rely%20on%20global%20noise%20assumptions%20or%20confidence-based%20sample%20selection%2C%20which%20inadequately%20mitigate%20the%20performance%20degradation%20caused%20by%20annotation%20noise%2C%20especially%20in%20challenging%20boundary%20regions.%20To%20address%20this%20issue%2C%20we%20propose%20MetaDCSeg%2C%20a%20robust%20framework%20that%20dynamically%20learns%20optimal%20pixel-wise%20weights%20to%20suppress%20the%20influence%20of%20noisy%20ground-truth%20labels%20while%20preserving%20reliable%20annotations.%20By%20explicitly%20modeling%20boundary%20uncertainty%20through%20a%20Dynamic%20Center%20Distance%20%28DCD%29%20mechanism%2C%20our%20approach%20utilizes%20weighted%20feature%20distances%20for%20foreground%2C%20background%2C%20and%20boundary%20centers%2C%20directing%20the%20model%27s%20attention%20toward%20hard-to-segment%20pixels%20near%20ambiguous%20boundaries.%20This%20strategy%20enables%20more%20precise%20handling%20of%20structural%20boundaries%2C%20which%20are%20often%20overlooked%20by%20existing%20methods%2C%20and%20significantly%20enhances%20segmentation%20performance.%20Extensive%20experiments%20across%20four%20benchmark%20datasets%20with%20varying%20noise%20levels%20demonstrate%20that%20MetaDCSeg%20consistently%20outperforms%20existing%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.18894v2&entry.124074799=Read"},
{"title": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification", "author": "Zack Dewis and Yimin Zhu and Zhengsen Xu and Mabel Heffring and Saeid Taleghanidoozdoozan and Quinn Ledingham and Lincoln Linlin Xu", "abstract": "Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.", "link": "http://arxiv.org/abs/2601.16098v1", "date": "2026-01-22", "relevancy": 2.0893, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5098}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering-Guided%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20Clustering-Guided%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Zack%20Dewis%20and%20Yimin%20Zhu%20and%20Zhengsen%20Xu%20and%20Mabel%20Heffring%20and%20Saeid%20Taleghanidoozdoozan%20and%20Quinn%20Ledingham%20and%20Lincoln%20Linlin%20Xu%0AAbstract%3A%20Although%20Mamba%20models%20greatly%20improve%20Hyperspectral%20Image%20%28HSI%29%20classification%2C%20they%20have%20critical%20challenges%20in%20terms%20defining%20efficient%20and%20adaptive%20token%20sequences%20for%20improve%20performance.%20This%20paper%20therefore%20presents%20CSSMamba%20%28Clustering-guided%20Spatial-Spectral%20Mamba%29%20framework%20to%20better%20address%20the%20challenges%2C%20with%20the%20following%20contributions.%20First%2C%20to%20achieve%20efficient%20and%20adaptive%20token%20sequences%20for%20improved%20Mamba%20performance%2C%20we%20integrate%20the%20clustering%20mechanism%20into%20a%20spatial%20Mamba%20architecture%2C%20leading%20to%20a%20cluster-guided%20spatial%20Mamba%20module%20%28CSpaMamba%29%20that%20reduces%20the%20Mamba%20sequence%20length%20and%20improves%20Mamba%20feature%20learning%20capability.%20Second%2C%20to%20improve%20the%20learning%20of%20both%20spatial%20and%20spectral%20information%2C%20we%20integrate%20the%20CSpaMamba%20module%20with%20a%20spectral%20mamba%20module%20%28SpeMamba%29%2C%20leading%20to%20a%20complete%20clustering-guided%20spatial-spectral%20Mamba%20framework.%20Third%2C%20to%20further%20improve%20feature%20learning%20capability%2C%20we%20introduce%20an%20Attention-Driven%20Token%20Selection%20mechanism%20to%20optimize%20Mamba%20token%20sequencing.%20Last%2C%20to%20seamlessly%20integrate%20clustering%20into%20the%20Mamba%20model%20in%20a%20coherent%20manner%2C%20we%20design%20a%20Learnable%20Clustering%20Module%20that%20learns%20the%20cluster%20memberships%20in%20an%20adaptive%20manner.%20Experiments%20on%20the%20Pavia%20University%2C%20Indian%20Pines%2C%20and%20Liao-Ning%2001%20datasets%20demonstrate%20that%20CSSMamba%20achieves%20higher%20accuracy%20and%20better%20boundary%20preservation%20compared%20to%20state-of-the-art%20CNN%2C%20Transformer%2C%20and%20Mamba-based%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering-Guided%2520Spatial-Spectral%2520Mamba%2520for%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DZack%2520Dewis%2520and%2520Yimin%2520Zhu%2520and%2520Zhengsen%2520Xu%2520and%2520Mabel%2520Heffring%2520and%2520Saeid%2520Taleghanidoozdoozan%2520and%2520Quinn%2520Ledingham%2520and%2520Lincoln%2520Linlin%2520Xu%26entry.1292438233%3DAlthough%2520Mamba%2520models%2520greatly%2520improve%2520Hyperspectral%2520Image%2520%2528HSI%2529%2520classification%252C%2520they%2520have%2520critical%2520challenges%2520in%2520terms%2520defining%2520efficient%2520and%2520adaptive%2520token%2520sequences%2520for%2520improve%2520performance.%2520This%2520paper%2520therefore%2520presents%2520CSSMamba%2520%2528Clustering-guided%2520Spatial-Spectral%2520Mamba%2529%2520framework%2520to%2520better%2520address%2520the%2520challenges%252C%2520with%2520the%2520following%2520contributions.%2520First%252C%2520to%2520achieve%2520efficient%2520and%2520adaptive%2520token%2520sequences%2520for%2520improved%2520Mamba%2520performance%252C%2520we%2520integrate%2520the%2520clustering%2520mechanism%2520into%2520a%2520spatial%2520Mamba%2520architecture%252C%2520leading%2520to%2520a%2520cluster-guided%2520spatial%2520Mamba%2520module%2520%2528CSpaMamba%2529%2520that%2520reduces%2520the%2520Mamba%2520sequence%2520length%2520and%2520improves%2520Mamba%2520feature%2520learning%2520capability.%2520Second%252C%2520to%2520improve%2520the%2520learning%2520of%2520both%2520spatial%2520and%2520spectral%2520information%252C%2520we%2520integrate%2520the%2520CSpaMamba%2520module%2520with%2520a%2520spectral%2520mamba%2520module%2520%2528SpeMamba%2529%252C%2520leading%2520to%2520a%2520complete%2520clustering-guided%2520spatial-spectral%2520Mamba%2520framework.%2520Third%252C%2520to%2520further%2520improve%2520feature%2520learning%2520capability%252C%2520we%2520introduce%2520an%2520Attention-Driven%2520Token%2520Selection%2520mechanism%2520to%2520optimize%2520Mamba%2520token%2520sequencing.%2520Last%252C%2520to%2520seamlessly%2520integrate%2520clustering%2520into%2520the%2520Mamba%2520model%2520in%2520a%2520coherent%2520manner%252C%2520we%2520design%2520a%2520Learnable%2520Clustering%2520Module%2520that%2520learns%2520the%2520cluster%2520memberships%2520in%2520an%2520adaptive%2520manner.%2520Experiments%2520on%2520the%2520Pavia%2520University%252C%2520Indian%2520Pines%252C%2520and%2520Liao-Ning%252001%2520datasets%2520demonstrate%2520that%2520CSSMamba%2520achieves%2520higher%2520accuracy%2520and%2520better%2520boundary%2520preservation%2520compared%2520to%2520state-of-the-art%2520CNN%252C%2520Transformer%252C%2520and%2520Mamba-based%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering-Guided%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Zack%20Dewis%20and%20Yimin%20Zhu%20and%20Zhengsen%20Xu%20and%20Mabel%20Heffring%20and%20Saeid%20Taleghanidoozdoozan%20and%20Quinn%20Ledingham%20and%20Lincoln%20Linlin%20Xu&entry.1292438233=Although%20Mamba%20models%20greatly%20improve%20Hyperspectral%20Image%20%28HSI%29%20classification%2C%20they%20have%20critical%20challenges%20in%20terms%20defining%20efficient%20and%20adaptive%20token%20sequences%20for%20improve%20performance.%20This%20paper%20therefore%20presents%20CSSMamba%20%28Clustering-guided%20Spatial-Spectral%20Mamba%29%20framework%20to%20better%20address%20the%20challenges%2C%20with%20the%20following%20contributions.%20First%2C%20to%20achieve%20efficient%20and%20adaptive%20token%20sequences%20for%20improved%20Mamba%20performance%2C%20we%20integrate%20the%20clustering%20mechanism%20into%20a%20spatial%20Mamba%20architecture%2C%20leading%20to%20a%20cluster-guided%20spatial%20Mamba%20module%20%28CSpaMamba%29%20that%20reduces%20the%20Mamba%20sequence%20length%20and%20improves%20Mamba%20feature%20learning%20capability.%20Second%2C%20to%20improve%20the%20learning%20of%20both%20spatial%20and%20spectral%20information%2C%20we%20integrate%20the%20CSpaMamba%20module%20with%20a%20spectral%20mamba%20module%20%28SpeMamba%29%2C%20leading%20to%20a%20complete%20clustering-guided%20spatial-spectral%20Mamba%20framework.%20Third%2C%20to%20further%20improve%20feature%20learning%20capability%2C%20we%20introduce%20an%20Attention-Driven%20Token%20Selection%20mechanism%20to%20optimize%20Mamba%20token%20sequencing.%20Last%2C%20to%20seamlessly%20integrate%20clustering%20into%20the%20Mamba%20model%20in%20a%20coherent%20manner%2C%20we%20design%20a%20Learnable%20Clustering%20Module%20that%20learns%20the%20cluster%20memberships%20in%20an%20adaptive%20manner.%20Experiments%20on%20the%20Pavia%20University%2C%20Indian%20Pines%2C%20and%20Liao-Ning%2001%20datasets%20demonstrate%20that%20CSSMamba%20achieves%20higher%20accuracy%20and%20better%20boundary%20preservation%20compared%20to%20state-of-the-art%20CNN%2C%20Transformer%2C%20and%20Mamba-based%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.16098v1&entry.124074799=Read"},
{"title": "AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs", "author": "Zhe Sun and Yujun Cai and Jiayu Yao and Yiwei Wang", "abstract": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.", "link": "http://arxiv.org/abs/2511.13273v2", "date": "2026-01-22", "relevancy": 2.0875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioMotionBench%3A%20Evaluating%20Auditory%20Motion%20Perception%20in%20Audio%20LLMs&body=Title%3A%20AudioMotionBench%3A%20Evaluating%20Auditory%20Motion%20Perception%20in%20Audio%20LLMs%0AAuthor%3A%20Zhe%20Sun%20and%20Yujun%20Cai%20and%20Jiayu%20Yao%20and%20Yiwei%20Wang%0AAbstract%3A%20Large%20Audio-Language%20Models%20%28LALMs%29%20have%20recently%20shown%20impressive%20progress%20in%20speech%20recognition%2C%20audio%20captioning%2C%20and%20auditory%20question%20answering.%20Yet%2C%20whether%20these%20models%20can%20perceive%20spatial%20dynamics%2C%20particularly%20the%20motion%20of%20sound%20sources%2C%20remains%20unclear.%20In%20this%20work%2C%20we%20uncover%20a%20systematic%20motion%20perception%20deficit%20in%20current%20ALLMs.%20To%20investigate%20this%20issue%2C%20we%20introduce%20AudioMotionBench%2C%20the%20first%20benchmark%20explicitly%20designed%20to%20evaluate%20auditory%20motion%20understanding.%20AudioMotionBench%20introduces%20a%20controlled%20question-answering%20benchmark%20designed%20to%20evaluate%20whether%20Audio-Language%20Models%20%28LALMs%29%20can%20infer%20the%20direction%20and%20trajectory%20of%20moving%20sound%20sources%20from%20binaural%20audio.%20Comprehensive%20quantitative%20and%20qualitative%20analyses%20reveal%20that%20current%20models%20struggle%20to%20reliably%20recognize%20motion%20cues%20or%20distinguish%20directional%20patterns.%20The%20average%20accuracy%20remains%20below%2050%5C%25%2C%20underscoring%20a%20fundamental%20limitation%20in%20auditory%20spatial%20reasoning.%20Our%20study%20highlights%20a%20fundamental%20gap%20between%20human%20and%20model%20auditory%20spatial%20reasoning%2C%20providing%20both%20a%20diagnostic%20tool%20and%20new%20insight%20for%20enhancing%20spatial%20cognition%20in%20future%20Audio-Language%20Models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioMotionBench%253A%2520Evaluating%2520Auditory%2520Motion%2520Perception%2520in%2520Audio%2520LLMs%26entry.906535625%3DZhe%2520Sun%2520and%2520Yujun%2520Cai%2520and%2520Jiayu%2520Yao%2520and%2520Yiwei%2520Wang%26entry.1292438233%3DLarge%2520Audio-Language%2520Models%2520%2528LALMs%2529%2520have%2520recently%2520shown%2520impressive%2520progress%2520in%2520speech%2520recognition%252C%2520audio%2520captioning%252C%2520and%2520auditory%2520question%2520answering.%2520Yet%252C%2520whether%2520these%2520models%2520can%2520perceive%2520spatial%2520dynamics%252C%2520particularly%2520the%2520motion%2520of%2520sound%2520sources%252C%2520remains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520uncover%2520a%2520systematic%2520motion%2520perception%2520deficit%2520in%2520current%2520ALLMs.%2520To%2520investigate%2520this%2520issue%252C%2520we%2520introduce%2520AudioMotionBench%252C%2520the%2520first%2520benchmark%2520explicitly%2520designed%2520to%2520evaluate%2520auditory%2520motion%2520understanding.%2520AudioMotionBench%2520introduces%2520a%2520controlled%2520question-answering%2520benchmark%2520designed%2520to%2520evaluate%2520whether%2520Audio-Language%2520Models%2520%2528LALMs%2529%2520can%2520infer%2520the%2520direction%2520and%2520trajectory%2520of%2520moving%2520sound%2520sources%2520from%2520binaural%2520audio.%2520Comprehensive%2520quantitative%2520and%2520qualitative%2520analyses%2520reveal%2520that%2520current%2520models%2520struggle%2520to%2520reliably%2520recognize%2520motion%2520cues%2520or%2520distinguish%2520directional%2520patterns.%2520The%2520average%2520accuracy%2520remains%2520below%252050%255C%2525%252C%2520underscoring%2520a%2520fundamental%2520limitation%2520in%2520auditory%2520spatial%2520reasoning.%2520Our%2520study%2520highlights%2520a%2520fundamental%2520gap%2520between%2520human%2520and%2520model%2520auditory%2520spatial%2520reasoning%252C%2520providing%2520both%2520a%2520diagnostic%2520tool%2520and%2520new%2520insight%2520for%2520enhancing%2520spatial%2520cognition%2520in%2520future%2520Audio-Language%2520Models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioMotionBench%3A%20Evaluating%20Auditory%20Motion%20Perception%20in%20Audio%20LLMs&entry.906535625=Zhe%20Sun%20and%20Yujun%20Cai%20and%20Jiayu%20Yao%20and%20Yiwei%20Wang&entry.1292438233=Large%20Audio-Language%20Models%20%28LALMs%29%20have%20recently%20shown%20impressive%20progress%20in%20speech%20recognition%2C%20audio%20captioning%2C%20and%20auditory%20question%20answering.%20Yet%2C%20whether%20these%20models%20can%20perceive%20spatial%20dynamics%2C%20particularly%20the%20motion%20of%20sound%20sources%2C%20remains%20unclear.%20In%20this%20work%2C%20we%20uncover%20a%20systematic%20motion%20perception%20deficit%20in%20current%20ALLMs.%20To%20investigate%20this%20issue%2C%20we%20introduce%20AudioMotionBench%2C%20the%20first%20benchmark%20explicitly%20designed%20to%20evaluate%20auditory%20motion%20understanding.%20AudioMotionBench%20introduces%20a%20controlled%20question-answering%20benchmark%20designed%20to%20evaluate%20whether%20Audio-Language%20Models%20%28LALMs%29%20can%20infer%20the%20direction%20and%20trajectory%20of%20moving%20sound%20sources%20from%20binaural%20audio.%20Comprehensive%20quantitative%20and%20qualitative%20analyses%20reveal%20that%20current%20models%20struggle%20to%20reliably%20recognize%20motion%20cues%20or%20distinguish%20directional%20patterns.%20The%20average%20accuracy%20remains%20below%2050%5C%25%2C%20underscoring%20a%20fundamental%20limitation%20in%20auditory%20spatial%20reasoning.%20Our%20study%20highlights%20a%20fundamental%20gap%20between%20human%20and%20model%20auditory%20spatial%20reasoning%2C%20providing%20both%20a%20diagnostic%20tool%20and%20new%20insight%20for%20enhancing%20spatial%20cognition%20in%20future%20Audio-Language%20Models.&entry.1838667208=http%3A//arxiv.org/abs/2511.13273v2&entry.124074799=Read"},
{"title": "Natural Language-Driven Global Mapping of Martian Landforms", "author": "Yiran Wang and Shuoyuan Wang and Zhaoran Wei and Jiannan Zhao and Zhonghua Yao and Zejian Xie and Songxin Zhang and Jun Huang and Bingyi Jing and Hongxin Wei", "abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.", "link": "http://arxiv.org/abs/2601.15949v1", "date": "2026-01-22", "relevancy": 2.0781, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language-Driven%20Global%20Mapping%20of%20Martian%20Landforms&body=Title%3A%20Natural%20Language-Driven%20Global%20Mapping%20of%20Martian%20Landforms%0AAuthor%3A%20Yiran%20Wang%20and%20Shuoyuan%20Wang%20and%20Zhaoran%20Wei%20and%20Jiannan%20Zhao%20and%20Zhonghua%20Yao%20and%20Zejian%20Xie%20and%20Songxin%20Zhang%20and%20Jun%20Huang%20and%20Bingyi%20Jing%20and%20Hongxin%20Wei%0AAbstract%3A%20Planetary%20surfaces%20are%20typically%20analyzed%20using%20high-level%20semantic%20concepts%20in%20natural%20language%2C%20yet%20vast%20orbital%20image%20archives%20remain%20organized%20at%20the%20pixel%20level.%20This%20mismatch%20limits%20scalable%2C%20open-ended%20exploration%20of%20planetary%20surfaces.%20Here%20we%20present%20MarScope%2C%20a%20planetary-scale%20vision-language%20framework%20enabling%20natural%20language-driven%2C%20label-free%20mapping%20of%20Martian%20landforms.%20MarScope%20aligns%20planetary%20images%20and%20text%20in%20a%20shared%20semantic%20space%2C%20trained%20on%20over%20200%2C000%20curated%20image-text%20pairs.%20This%20framework%20transforms%20global%20geomorphic%20mapping%20on%20Mars%20by%20replacing%20pre-defined%20classifications%20with%20flexible%20semantic%20retrieval%2C%20enabling%20arbitrary%20user%20queries%20across%20the%20entire%20planet%20in%205%20seconds%20with%20F1%20scores%20up%20to%200.978.%20Applications%20further%20show%20that%20it%20extends%20beyond%20morphological%20classification%20to%20facilitate%20process-oriented%20analysis%20and%20similarity-based%20geomorphological%20mapping%20at%20a%20planetary%20scale.%20MarScope%20establishes%20a%20new%20paradigm%20where%20natural%20language%20serves%20as%20a%20direct%20interface%20for%20scientific%20discovery%20over%20massive%20geospatial%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language-Driven%2520Global%2520Mapping%2520of%2520Martian%2520Landforms%26entry.906535625%3DYiran%2520Wang%2520and%2520Shuoyuan%2520Wang%2520and%2520Zhaoran%2520Wei%2520and%2520Jiannan%2520Zhao%2520and%2520Zhonghua%2520Yao%2520and%2520Zejian%2520Xie%2520and%2520Songxin%2520Zhang%2520and%2520Jun%2520Huang%2520and%2520Bingyi%2520Jing%2520and%2520Hongxin%2520Wei%26entry.1292438233%3DPlanetary%2520surfaces%2520are%2520typically%2520analyzed%2520using%2520high-level%2520semantic%2520concepts%2520in%2520natural%2520language%252C%2520yet%2520vast%2520orbital%2520image%2520archives%2520remain%2520organized%2520at%2520the%2520pixel%2520level.%2520This%2520mismatch%2520limits%2520scalable%252C%2520open-ended%2520exploration%2520of%2520planetary%2520surfaces.%2520Here%2520we%2520present%2520MarScope%252C%2520a%2520planetary-scale%2520vision-language%2520framework%2520enabling%2520natural%2520language-driven%252C%2520label-free%2520mapping%2520of%2520Martian%2520landforms.%2520MarScope%2520aligns%2520planetary%2520images%2520and%2520text%2520in%2520a%2520shared%2520semantic%2520space%252C%2520trained%2520on%2520over%2520200%252C000%2520curated%2520image-text%2520pairs.%2520This%2520framework%2520transforms%2520global%2520geomorphic%2520mapping%2520on%2520Mars%2520by%2520replacing%2520pre-defined%2520classifications%2520with%2520flexible%2520semantic%2520retrieval%252C%2520enabling%2520arbitrary%2520user%2520queries%2520across%2520the%2520entire%2520planet%2520in%25205%2520seconds%2520with%2520F1%2520scores%2520up%2520to%25200.978.%2520Applications%2520further%2520show%2520that%2520it%2520extends%2520beyond%2520morphological%2520classification%2520to%2520facilitate%2520process-oriented%2520analysis%2520and%2520similarity-based%2520geomorphological%2520mapping%2520at%2520a%2520planetary%2520scale.%2520MarScope%2520establishes%2520a%2520new%2520paradigm%2520where%2520natural%2520language%2520serves%2520as%2520a%2520direct%2520interface%2520for%2520scientific%2520discovery%2520over%2520massive%2520geospatial%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language-Driven%20Global%20Mapping%20of%20Martian%20Landforms&entry.906535625=Yiran%20Wang%20and%20Shuoyuan%20Wang%20and%20Zhaoran%20Wei%20and%20Jiannan%20Zhao%20and%20Zhonghua%20Yao%20and%20Zejian%20Xie%20and%20Songxin%20Zhang%20and%20Jun%20Huang%20and%20Bingyi%20Jing%20and%20Hongxin%20Wei&entry.1292438233=Planetary%20surfaces%20are%20typically%20analyzed%20using%20high-level%20semantic%20concepts%20in%20natural%20language%2C%20yet%20vast%20orbital%20image%20archives%20remain%20organized%20at%20the%20pixel%20level.%20This%20mismatch%20limits%20scalable%2C%20open-ended%20exploration%20of%20planetary%20surfaces.%20Here%20we%20present%20MarScope%2C%20a%20planetary-scale%20vision-language%20framework%20enabling%20natural%20language-driven%2C%20label-free%20mapping%20of%20Martian%20landforms.%20MarScope%20aligns%20planetary%20images%20and%20text%20in%20a%20shared%20semantic%20space%2C%20trained%20on%20over%20200%2C000%20curated%20image-text%20pairs.%20This%20framework%20transforms%20global%20geomorphic%20mapping%20on%20Mars%20by%20replacing%20pre-defined%20classifications%20with%20flexible%20semantic%20retrieval%2C%20enabling%20arbitrary%20user%20queries%20across%20the%20entire%20planet%20in%205%20seconds%20with%20F1%20scores%20up%20to%200.978.%20Applications%20further%20show%20that%20it%20extends%20beyond%20morphological%20classification%20to%20facilitate%20process-oriented%20analysis%20and%20similarity-based%20geomorphological%20mapping%20at%20a%20planetary%20scale.%20MarScope%20establishes%20a%20new%20paradigm%20where%20natural%20language%20serves%20as%20a%20direct%20interface%20for%20scientific%20discovery%20over%20massive%20geospatial%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2601.15949v1&entry.124074799=Read"},
{"title": "EmbedAgent: Benchmarking Large Language Models in Embedded System Development", "author": "Ruiyang Xu and Jialun Cao and Mingyuan Wu and Wenliang Zhong and Yaojie Lu and Ben He and Xianpei Han and Shing-Chi Cheung and Le Sun", "abstract": "Large Language Models (LLMs) have shown promise in various tasks, yet few benchmarks assess their capabilities in embedded system development.In this paper, we introduce EmbedAgent, a paradigm designed to simulate real-world roles in embedded system development, such as Embedded System Programmer, Architect, and Integrator. This paradigm enables LLMs to be tested in tasks that bridge the gap between digital and physical systems, allowing for a more comprehensive assessment of their capabilities. To evaluate LLMs on these tasks, we propose Embedbench, the first comprehensive benchmark for embedded system programming, circuit design, and cross-platform migration.Embedbench consists of 126 cases, covering 9 electronic components across 3 hardware platforms. Through extensive experiments on 10 mainstream LLMs, we uncover several key findings. Surprisingly, despite the simplicity of the cases, DeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic information, and 50.0% when tasked with generating the schematics itself. In the cross-platform migration tasks, LLMs show relatively strong performance with MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8% pass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4% pass@1.Interestingly, we observe that general-purpose chat LLMs like DeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this domain, while reasoning LLMs tend to overthink and overlook efficient knowledge during pretraining. Based on these insights, we propose two strategies: retrieval augmented generation and compiler feedback-to enhance LLM performance. These strategies result in significant improvements, with Deepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without. Additionally, the accuracy of the Arduino to ESP32 migration task improves from 21.4% to 27.8%.", "link": "http://arxiv.org/abs/2506.11003v2", "date": "2026-01-22", "relevancy": 2.0749, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmbedAgent%3A%20Benchmarking%20Large%20Language%20Models%20in%20Embedded%20System%20Development&body=Title%3A%20EmbedAgent%3A%20Benchmarking%20Large%20Language%20Models%20in%20Embedded%20System%20Development%0AAuthor%3A%20Ruiyang%20Xu%20and%20Jialun%20Cao%20and%20Mingyuan%20Wu%20and%20Wenliang%20Zhong%20and%20Yaojie%20Lu%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Shing-Chi%20Cheung%20and%20Le%20Sun%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20promise%20in%20various%20tasks%2C%20yet%20few%20benchmarks%20assess%20their%20capabilities%20in%20embedded%20system%20development.In%20this%20paper%2C%20we%20introduce%20EmbedAgent%2C%20a%20paradigm%20designed%20to%20simulate%20real-world%20roles%20in%20embedded%20system%20development%2C%20such%20as%20Embedded%20System%20Programmer%2C%20Architect%2C%20and%20Integrator.%20This%20paradigm%20enables%20LLMs%20to%20be%20tested%20in%20tasks%20that%20bridge%20the%20gap%20between%20digital%20and%20physical%20systems%2C%20allowing%20for%20a%20more%20comprehensive%20assessment%20of%20their%20capabilities.%20To%20evaluate%20LLMs%20on%20these%20tasks%2C%20we%20propose%20Embedbench%2C%20the%20first%20comprehensive%20benchmark%20for%20embedded%20system%20programming%2C%20circuit%20design%2C%20and%20cross-platform%20migration.Embedbench%20consists%20of%20126%20cases%2C%20covering%209%20electronic%20components%20across%203%20hardware%20platforms.%20Through%20extensive%20experiments%20on%2010%20mainstream%20LLMs%2C%20we%20uncover%20several%20key%20findings.%20Surprisingly%2C%20despite%20the%20simplicity%20of%20the%20cases%2C%20DeepSeek-R1%20achieves%20only%20a%2055.6%25%20pass%401%20rate%20when%20provided%20with%20schematic%20information%2C%20and%2050.0%25%20when%20tasked%20with%20generating%20the%20schematics%20itself.%20In%20the%20cross-platform%20migration%20tasks%2C%20LLMs%20show%20relatively%20strong%20performance%20with%20MicroPython%20on%20the%20Raspberry%20Pi%20Pico%20%28with%20the%20top%20model%20achieving%2073.8%25%20pass%401%29%2C%20but%20perform%20poorly%20on%20ESP-IDF%2C%20where%20the%20best%20model%20reaches%20only%2029.4%25%20pass%401.Interestingly%2C%20we%20observe%20that%20general-purpose%20chat%20LLMs%20like%20DeepSeek-V3%20often%20fail%20to%20utilize%20relevant%20pre-trained%20knowledge%20in%20this%20domain%2C%20while%20reasoning%20LLMs%20tend%20to%20overthink%20and%20overlook%20efficient%20knowledge%20during%20pretraining.%20Based%20on%20these%20insights%2C%20we%20propose%20two%20strategies%3A%20retrieval%20augmented%20generation%20and%20compiler%20feedback-to%20enhance%20LLM%20performance.%20These%20strategies%20result%20in%20significant%20improvements%2C%20with%20Deepseek-R1%20reaching%20a%2065.1%25%20pass%401%20with%20correct%20schematics%2C%20and%2053.1%25%20without.%20Additionally%2C%20the%20accuracy%20of%20the%20Arduino%20to%20ESP32%20migration%20task%20improves%20from%2021.4%25%20to%2027.8%25.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedAgent%253A%2520Benchmarking%2520Large%2520Language%2520Models%2520in%2520Embedded%2520System%2520Development%26entry.906535625%3DRuiyang%2520Xu%2520and%2520Jialun%2520Cao%2520and%2520Mingyuan%2520Wu%2520and%2520Wenliang%2520Zhong%2520and%2520Yaojie%2520Lu%2520and%2520Ben%2520He%2520and%2520Xianpei%2520Han%2520and%2520Shing-Chi%2520Cheung%2520and%2520Le%2520Sun%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520in%2520various%2520tasks%252C%2520yet%2520few%2520benchmarks%2520assess%2520their%2520capabilities%2520in%2520embedded%2520system%2520development.In%2520this%2520paper%252C%2520we%2520introduce%2520EmbedAgent%252C%2520a%2520paradigm%2520designed%2520to%2520simulate%2520real-world%2520roles%2520in%2520embedded%2520system%2520development%252C%2520such%2520as%2520Embedded%2520System%2520Programmer%252C%2520Architect%252C%2520and%2520Integrator.%2520This%2520paradigm%2520enables%2520LLMs%2520to%2520be%2520tested%2520in%2520tasks%2520that%2520bridge%2520the%2520gap%2520between%2520digital%2520and%2520physical%2520systems%252C%2520allowing%2520for%2520a%2520more%2520comprehensive%2520assessment%2520of%2520their%2520capabilities.%2520To%2520evaluate%2520LLMs%2520on%2520these%2520tasks%252C%2520we%2520propose%2520Embedbench%252C%2520the%2520first%2520comprehensive%2520benchmark%2520for%2520embedded%2520system%2520programming%252C%2520circuit%2520design%252C%2520and%2520cross-platform%2520migration.Embedbench%2520consists%2520of%2520126%2520cases%252C%2520covering%25209%2520electronic%2520components%2520across%25203%2520hardware%2520platforms.%2520Through%2520extensive%2520experiments%2520on%252010%2520mainstream%2520LLMs%252C%2520we%2520uncover%2520several%2520key%2520findings.%2520Surprisingly%252C%2520despite%2520the%2520simplicity%2520of%2520the%2520cases%252C%2520DeepSeek-R1%2520achieves%2520only%2520a%252055.6%2525%2520pass%25401%2520rate%2520when%2520provided%2520with%2520schematic%2520information%252C%2520and%252050.0%2525%2520when%2520tasked%2520with%2520generating%2520the%2520schematics%2520itself.%2520In%2520the%2520cross-platform%2520migration%2520tasks%252C%2520LLMs%2520show%2520relatively%2520strong%2520performance%2520with%2520MicroPython%2520on%2520the%2520Raspberry%2520Pi%2520Pico%2520%2528with%2520the%2520top%2520model%2520achieving%252073.8%2525%2520pass%25401%2529%252C%2520but%2520perform%2520poorly%2520on%2520ESP-IDF%252C%2520where%2520the%2520best%2520model%2520reaches%2520only%252029.4%2525%2520pass%25401.Interestingly%252C%2520we%2520observe%2520that%2520general-purpose%2520chat%2520LLMs%2520like%2520DeepSeek-V3%2520often%2520fail%2520to%2520utilize%2520relevant%2520pre-trained%2520knowledge%2520in%2520this%2520domain%252C%2520while%2520reasoning%2520LLMs%2520tend%2520to%2520overthink%2520and%2520overlook%2520efficient%2520knowledge%2520during%2520pretraining.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520two%2520strategies%253A%2520retrieval%2520augmented%2520generation%2520and%2520compiler%2520feedback-to%2520enhance%2520LLM%2520performance.%2520These%2520strategies%2520result%2520in%2520significant%2520improvements%252C%2520with%2520Deepseek-R1%2520reaching%2520a%252065.1%2525%2520pass%25401%2520with%2520correct%2520schematics%252C%2520and%252053.1%2525%2520without.%2520Additionally%252C%2520the%2520accuracy%2520of%2520the%2520Arduino%2520to%2520ESP32%2520migration%2520task%2520improves%2520from%252021.4%2525%2520to%252027.8%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmbedAgent%3A%20Benchmarking%20Large%20Language%20Models%20in%20Embedded%20System%20Development&entry.906535625=Ruiyang%20Xu%20and%20Jialun%20Cao%20and%20Mingyuan%20Wu%20and%20Wenliang%20Zhong%20and%20Yaojie%20Lu%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Shing-Chi%20Cheung%20and%20Le%20Sun&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20shown%20promise%20in%20various%20tasks%2C%20yet%20few%20benchmarks%20assess%20their%20capabilities%20in%20embedded%20system%20development.In%20this%20paper%2C%20we%20introduce%20EmbedAgent%2C%20a%20paradigm%20designed%20to%20simulate%20real-world%20roles%20in%20embedded%20system%20development%2C%20such%20as%20Embedded%20System%20Programmer%2C%20Architect%2C%20and%20Integrator.%20This%20paradigm%20enables%20LLMs%20to%20be%20tested%20in%20tasks%20that%20bridge%20the%20gap%20between%20digital%20and%20physical%20systems%2C%20allowing%20for%20a%20more%20comprehensive%20assessment%20of%20their%20capabilities.%20To%20evaluate%20LLMs%20on%20these%20tasks%2C%20we%20propose%20Embedbench%2C%20the%20first%20comprehensive%20benchmark%20for%20embedded%20system%20programming%2C%20circuit%20design%2C%20and%20cross-platform%20migration.Embedbench%20consists%20of%20126%20cases%2C%20covering%209%20electronic%20components%20across%203%20hardware%20platforms.%20Through%20extensive%20experiments%20on%2010%20mainstream%20LLMs%2C%20we%20uncover%20several%20key%20findings.%20Surprisingly%2C%20despite%20the%20simplicity%20of%20the%20cases%2C%20DeepSeek-R1%20achieves%20only%20a%2055.6%25%20pass%401%20rate%20when%20provided%20with%20schematic%20information%2C%20and%2050.0%25%20when%20tasked%20with%20generating%20the%20schematics%20itself.%20In%20the%20cross-platform%20migration%20tasks%2C%20LLMs%20show%20relatively%20strong%20performance%20with%20MicroPython%20on%20the%20Raspberry%20Pi%20Pico%20%28with%20the%20top%20model%20achieving%2073.8%25%20pass%401%29%2C%20but%20perform%20poorly%20on%20ESP-IDF%2C%20where%20the%20best%20model%20reaches%20only%2029.4%25%20pass%401.Interestingly%2C%20we%20observe%20that%20general-purpose%20chat%20LLMs%20like%20DeepSeek-V3%20often%20fail%20to%20utilize%20relevant%20pre-trained%20knowledge%20in%20this%20domain%2C%20while%20reasoning%20LLMs%20tend%20to%20overthink%20and%20overlook%20efficient%20knowledge%20during%20pretraining.%20Based%20on%20these%20insights%2C%20we%20propose%20two%20strategies%3A%20retrieval%20augmented%20generation%20and%20compiler%20feedback-to%20enhance%20LLM%20performance.%20These%20strategies%20result%20in%20significant%20improvements%2C%20with%20Deepseek-R1%20reaching%20a%2065.1%25%20pass%401%20with%20correct%20schematics%2C%20and%2053.1%25%20without.%20Additionally%2C%20the%20accuracy%20of%20the%20Arduino%20to%20ESP32%20migration%20task%20improves%20from%2021.4%25%20to%2027.8%25.&entry.1838667208=http%3A//arxiv.org/abs/2506.11003v2&entry.124074799=Read"},
{"title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection", "author": "Ori Meiraz and Sharon Shalev and Avishai Weizman", "abstract": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.", "link": "http://arxiv.org/abs/2511.13344v4", "date": "2026-01-22", "relevancy": 2.0706, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5334}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5094}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLO%20Meets%20Mixture-of-Experts%3A%20Adaptive%20Expert%20Routing%20for%20Robust%20Object%20Detection&body=Title%3A%20YOLO%20Meets%20Mixture-of-Experts%3A%20Adaptive%20Expert%20Routing%20for%20Robust%20Object%20Detection%0AAuthor%3A%20Ori%20Meiraz%20and%20Sharon%20Shalev%20and%20Avishai%20Weizman%0AAbstract%3A%20This%20paper%20presents%20a%20novel%20Mixture-of-Experts%20framework%20for%20object%20detection%2C%20incorporating%20adaptive%20routing%20among%20multiple%20YOLOv9-T%20experts%20to%20enable%20dynamic%20feature%20specialization%20and%20achieve%20higher%20mean%20Average%20Precision%20%28mAP%29%20and%20Average%20Recall%20%28AR%29%20compared%20to%20a%20single%20YOLOv9-T%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13344v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLO%2520Meets%2520Mixture-of-Experts%253A%2520Adaptive%2520Expert%2520Routing%2520for%2520Robust%2520Object%2520Detection%26entry.906535625%3DOri%2520Meiraz%2520and%2520Sharon%2520Shalev%2520and%2520Avishai%2520Weizman%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520novel%2520Mixture-of-Experts%2520framework%2520for%2520object%2520detection%252C%2520incorporating%2520adaptive%2520routing%2520among%2520multiple%2520YOLOv9-T%2520experts%2520to%2520enable%2520dynamic%2520feature%2520specialization%2520and%2520achieve%2520higher%2520mean%2520Average%2520Precision%2520%2528mAP%2529%2520and%2520Average%2520Recall%2520%2528AR%2529%2520compared%2520to%2520a%2520single%2520YOLOv9-T%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13344v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLO%20Meets%20Mixture-of-Experts%3A%20Adaptive%20Expert%20Routing%20for%20Robust%20Object%20Detection&entry.906535625=Ori%20Meiraz%20and%20Sharon%20Shalev%20and%20Avishai%20Weizman&entry.1292438233=This%20paper%20presents%20a%20novel%20Mixture-of-Experts%20framework%20for%20object%20detection%2C%20incorporating%20adaptive%20routing%20among%20multiple%20YOLOv9-T%20experts%20to%20enable%20dynamic%20feature%20specialization%20and%20achieve%20higher%20mean%20Average%20Precision%20%28mAP%29%20and%20Average%20Recall%20%28AR%29%20compared%20to%20a%20single%20YOLOv9-T%20model.&entry.1838667208=http%3A//arxiv.org/abs/2511.13344v4&entry.124074799=Read"},
{"title": "The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars", "author": "Yarin Benyamin", "abstract": "In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a \"Latency Wall\" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.", "link": "http://arxiv.org/abs/2601.15914v1", "date": "2026-01-22", "relevancy": 2.0689, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5319}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5229}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Latency%20Wall%3A%20Benchmarking%20Off-the-Shelf%20Emotion%20Recognition%20for%20Real-Time%20Virtual%20Avatars&body=Title%3A%20The%20Latency%20Wall%3A%20Benchmarking%20Off-the-Shelf%20Emotion%20Recognition%20for%20Real-Time%20Virtual%20Avatars%0AAuthor%3A%20Yarin%20Benyamin%0AAbstract%3A%20In%20the%20realm%20of%20Virtual%20Reality%20%28VR%29%20and%20Human-Computer%20Interaction%20%28HCI%29%2C%20real-time%20emotion%20recognition%20shows%20promise%20for%20supporting%20individuals%20with%20Autism%20Spectrum%20Disorder%20%28ASD%29%20in%20improving%20social%20skills.%20This%20task%20requires%20a%20strict%20latency-accuracy%20trade-off%2C%20with%20motion-to-photon%20%28MTP%29%20latency%20kept%20below%20140%20ms%20to%20maintain%20contingency.%20However%2C%20most%20off-the-shelf%20Deep%20Learning%20models%20prioritize%20accuracy%20over%20the%20strict%20timing%20constraints%20of%20commodity%20hardware.%20As%20a%20first%20step%20toward%20accessible%20VR%20therapy%2C%20we%20benchmark%20State-of-the-Art%20%28SOTA%29%20models%20for%20Zero-Shot%20Facial%20Expression%20Recognition%20%28FER%29%20on%20virtual%20characters%20using%20the%20UIBVFED%20dataset.%20We%20evaluate%20Medium%20and%20Nano%20variants%20of%20YOLO%20%28v8%2C%20v11%2C%20and%20v12%29%20for%20face%20detection%2C%20alongside%20general-purpose%20Vision%20Transformers%20including%20CLIP%2C%20SigLIP%2C%20and%20ViT-FER.Our%20results%20on%20CPU-only%20inference%20demonstrate%20that%20while%20face%20detection%20on%20stylized%20avatars%20is%20robust%20%28100%25%20accuracy%29%2C%20a%20%22Latency%20Wall%22%20exists%20in%20the%20classification%20stage.%20The%20YOLOv11n%20architecture%20offers%20the%20optimal%20balance%20for%20detection%20%28~54%20ms%29.%20However%2C%20general-purpose%20Transformers%20like%20CLIP%20and%20SigLIP%20fail%20to%20achieve%20viable%20accuracy%20%28%3C23%25%29%20or%20speed%20%28%3E150%20ms%29%20for%20real-time%20loops.%20This%20study%20highlights%20the%20necessity%20for%20lightweight%2C%20domain-specific%20architectures%20to%20enable%20accessible%2C%20real-time%20AI%20in%20therapeutic%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Latency%2520Wall%253A%2520Benchmarking%2520Off-the-Shelf%2520Emotion%2520Recognition%2520for%2520Real-Time%2520Virtual%2520Avatars%26entry.906535625%3DYarin%2520Benyamin%26entry.1292438233%3DIn%2520the%2520realm%2520of%2520Virtual%2520Reality%2520%2528VR%2529%2520and%2520Human-Computer%2520Interaction%2520%2528HCI%2529%252C%2520real-time%2520emotion%2520recognition%2520shows%2520promise%2520for%2520supporting%2520individuals%2520with%2520Autism%2520Spectrum%2520Disorder%2520%2528ASD%2529%2520in%2520improving%2520social%2520skills.%2520This%2520task%2520requires%2520a%2520strict%2520latency-accuracy%2520trade-off%252C%2520with%2520motion-to-photon%2520%2528MTP%2529%2520latency%2520kept%2520below%2520140%2520ms%2520to%2520maintain%2520contingency.%2520However%252C%2520most%2520off-the-shelf%2520Deep%2520Learning%2520models%2520prioritize%2520accuracy%2520over%2520the%2520strict%2520timing%2520constraints%2520of%2520commodity%2520hardware.%2520As%2520a%2520first%2520step%2520toward%2520accessible%2520VR%2520therapy%252C%2520we%2520benchmark%2520State-of-the-Art%2520%2528SOTA%2529%2520models%2520for%2520Zero-Shot%2520Facial%2520Expression%2520Recognition%2520%2528FER%2529%2520on%2520virtual%2520characters%2520using%2520the%2520UIBVFED%2520dataset.%2520We%2520evaluate%2520Medium%2520and%2520Nano%2520variants%2520of%2520YOLO%2520%2528v8%252C%2520v11%252C%2520and%2520v12%2529%2520for%2520face%2520detection%252C%2520alongside%2520general-purpose%2520Vision%2520Transformers%2520including%2520CLIP%252C%2520SigLIP%252C%2520and%2520ViT-FER.Our%2520results%2520on%2520CPU-only%2520inference%2520demonstrate%2520that%2520while%2520face%2520detection%2520on%2520stylized%2520avatars%2520is%2520robust%2520%2528100%2525%2520accuracy%2529%252C%2520a%2520%2522Latency%2520Wall%2522%2520exists%2520in%2520the%2520classification%2520stage.%2520The%2520YOLOv11n%2520architecture%2520offers%2520the%2520optimal%2520balance%2520for%2520detection%2520%2528~54%2520ms%2529.%2520However%252C%2520general-purpose%2520Transformers%2520like%2520CLIP%2520and%2520SigLIP%2520fail%2520to%2520achieve%2520viable%2520accuracy%2520%2528%253C23%2525%2529%2520or%2520speed%2520%2528%253E150%2520ms%2529%2520for%2520real-time%2520loops.%2520This%2520study%2520highlights%2520the%2520necessity%2520for%2520lightweight%252C%2520domain-specific%2520architectures%2520to%2520enable%2520accessible%252C%2520real-time%2520AI%2520in%2520therapeutic%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Latency%20Wall%3A%20Benchmarking%20Off-the-Shelf%20Emotion%20Recognition%20for%20Real-Time%20Virtual%20Avatars&entry.906535625=Yarin%20Benyamin&entry.1292438233=In%20the%20realm%20of%20Virtual%20Reality%20%28VR%29%20and%20Human-Computer%20Interaction%20%28HCI%29%2C%20real-time%20emotion%20recognition%20shows%20promise%20for%20supporting%20individuals%20with%20Autism%20Spectrum%20Disorder%20%28ASD%29%20in%20improving%20social%20skills.%20This%20task%20requires%20a%20strict%20latency-accuracy%20trade-off%2C%20with%20motion-to-photon%20%28MTP%29%20latency%20kept%20below%20140%20ms%20to%20maintain%20contingency.%20However%2C%20most%20off-the-shelf%20Deep%20Learning%20models%20prioritize%20accuracy%20over%20the%20strict%20timing%20constraints%20of%20commodity%20hardware.%20As%20a%20first%20step%20toward%20accessible%20VR%20therapy%2C%20we%20benchmark%20State-of-the-Art%20%28SOTA%29%20models%20for%20Zero-Shot%20Facial%20Expression%20Recognition%20%28FER%29%20on%20virtual%20characters%20using%20the%20UIBVFED%20dataset.%20We%20evaluate%20Medium%20and%20Nano%20variants%20of%20YOLO%20%28v8%2C%20v11%2C%20and%20v12%29%20for%20face%20detection%2C%20alongside%20general-purpose%20Vision%20Transformers%20including%20CLIP%2C%20SigLIP%2C%20and%20ViT-FER.Our%20results%20on%20CPU-only%20inference%20demonstrate%20that%20while%20face%20detection%20on%20stylized%20avatars%20is%20robust%20%28100%25%20accuracy%29%2C%20a%20%22Latency%20Wall%22%20exists%20in%20the%20classification%20stage.%20The%20YOLOv11n%20architecture%20offers%20the%20optimal%20balance%20for%20detection%20%28~54%20ms%29.%20However%2C%20general-purpose%20Transformers%20like%20CLIP%20and%20SigLIP%20fail%20to%20achieve%20viable%20accuracy%20%28%3C23%25%29%20or%20speed%20%28%3E150%20ms%29%20for%20real-time%20loops.%20This%20study%20highlights%20the%20necessity%20for%20lightweight%2C%20domain-specific%20architectures%20to%20enable%20accessible%2C%20real-time%20AI%20in%20therapeutic%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.15914v1&entry.124074799=Read"},
{"title": "Learning to Discover at Test Time", "author": "Mert Yuksekgonul and Daniel Koceja and Xinhao Li and Federico Bianchi and Jed McCaleb and Xiaolong Wang and Jan Kautz and Yejin Choi and James Zou and Carlos Guestrin and Yu Sun", "abstract": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\u0151s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "link": "http://arxiv.org/abs/2601.16175v1", "date": "2026-01-22", "relevancy": 2.0647, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5187}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5165}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Discover%20at%20Test%20Time&body=Title%3A%20Learning%20to%20Discover%20at%20Test%20Time%0AAuthor%3A%20Mert%20Yuksekgonul%20and%20Daniel%20Koceja%20and%20Xinhao%20Li%20and%20Federico%20Bianchi%20and%20Jed%20McCaleb%20and%20Xiaolong%20Wang%20and%20Jan%20Kautz%20and%20Yejin%20Choi%20and%20James%20Zou%20and%20Carlos%20Guestrin%20and%20Yu%20Sun%0AAbstract%3A%20How%20can%20we%20use%20AI%20to%20discover%20a%20new%20state%20of%20the%20art%20for%20a%20scientific%20problem%3F%20Prior%20work%20in%20test-time%20scaling%2C%20such%20as%20AlphaEvolve%2C%20performs%20search%20by%20prompting%20a%20frozen%20LLM.%20We%20perform%20reinforcement%20learning%20at%20test%20time%2C%20so%20the%20LLM%20can%20continue%20to%20train%2C%20but%20now%20with%20experience%20specific%20to%20the%20test%20problem.%20This%20form%20of%20continual%20learning%20is%20quite%20special%2C%20because%20its%20goal%20is%20to%20produce%20one%20great%20solution%20rather%20than%20many%20good%20ones%20on%20average%2C%20and%20to%20solve%20this%20very%20problem%20rather%20than%20generalize%20to%20other%20problems.%20Therefore%2C%20our%20learning%20objective%20and%20search%20subroutine%20are%20designed%20to%20prioritize%20the%20most%20promising%20solutions.%20We%20call%20this%20method%20Test-Time%20Training%20to%20Discover%20%28TTT-Discover%29.%20Following%20prior%20work%2C%20we%20focus%20on%20problems%20with%20continuous%20rewards.%20We%20report%20results%20for%20every%20problem%20we%20attempted%2C%20across%20mathematics%2C%20GPU%20kernel%20engineering%2C%20algorithm%20design%2C%20and%20biology.%20TTT-Discover%20sets%20the%20new%20state%20of%20the%20art%20in%20almost%20all%20of%20them%3A%20%28i%29%20Erd%C5%91s%27%20minimum%20overlap%20problem%20and%20an%20autocorrelation%20inequality%3B%20%28ii%29%20a%20GPUMode%20kernel%20competition%20%28up%20to%20%242%5Ctimes%24%20faster%20than%20prior%20art%29%3B%20%28iii%29%20past%20AtCoder%20algorithm%20competitions%3B%20and%20%28iv%29%20denoising%20problem%20in%20single-cell%20analysis.%20Our%20solutions%20are%20reviewed%20by%20experts%20or%20the%20organizers.%20All%20our%20results%20are%20achieved%20with%20an%20open%20model%2C%20OpenAI%20gpt-oss-120b%2C%20and%20can%20be%20reproduced%20with%20our%20publicly%20available%20code%2C%20in%20contrast%20to%20previous%20best%20results%20that%20required%20closed%20frontier%20models.%20Our%20test-time%20training%20runs%20are%20performed%20using%20Tinker%2C%20an%20API%20by%20Thinking%20Machines%2C%20with%20a%20cost%20of%20only%20a%20few%20hundred%20dollars%20per%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Discover%2520at%2520Test%2520Time%26entry.906535625%3DMert%2520Yuksekgonul%2520and%2520Daniel%2520Koceja%2520and%2520Xinhao%2520Li%2520and%2520Federico%2520Bianchi%2520and%2520Jed%2520McCaleb%2520and%2520Xiaolong%2520Wang%2520and%2520Jan%2520Kautz%2520and%2520Yejin%2520Choi%2520and%2520James%2520Zou%2520and%2520Carlos%2520Guestrin%2520and%2520Yu%2520Sun%26entry.1292438233%3DHow%2520can%2520we%2520use%2520AI%2520to%2520discover%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520a%2520scientific%2520problem%253F%2520Prior%2520work%2520in%2520test-time%2520scaling%252C%2520such%2520as%2520AlphaEvolve%252C%2520performs%2520search%2520by%2520prompting%2520a%2520frozen%2520LLM.%2520We%2520perform%2520reinforcement%2520learning%2520at%2520test%2520time%252C%2520so%2520the%2520LLM%2520can%2520continue%2520to%2520train%252C%2520but%2520now%2520with%2520experience%2520specific%2520to%2520the%2520test%2520problem.%2520This%2520form%2520of%2520continual%2520learning%2520is%2520quite%2520special%252C%2520because%2520its%2520goal%2520is%2520to%2520produce%2520one%2520great%2520solution%2520rather%2520than%2520many%2520good%2520ones%2520on%2520average%252C%2520and%2520to%2520solve%2520this%2520very%2520problem%2520rather%2520than%2520generalize%2520to%2520other%2520problems.%2520Therefore%252C%2520our%2520learning%2520objective%2520and%2520search%2520subroutine%2520are%2520designed%2520to%2520prioritize%2520the%2520most%2520promising%2520solutions.%2520We%2520call%2520this%2520method%2520Test-Time%2520Training%2520to%2520Discover%2520%2528TTT-Discover%2529.%2520Following%2520prior%2520work%252C%2520we%2520focus%2520on%2520problems%2520with%2520continuous%2520rewards.%2520We%2520report%2520results%2520for%2520every%2520problem%2520we%2520attempted%252C%2520across%2520mathematics%252C%2520GPU%2520kernel%2520engineering%252C%2520algorithm%2520design%252C%2520and%2520biology.%2520TTT-Discover%2520sets%2520the%2520new%2520state%2520of%2520the%2520art%2520in%2520almost%2520all%2520of%2520them%253A%2520%2528i%2529%2520Erd%25C5%2591s%2527%2520minimum%2520overlap%2520problem%2520and%2520an%2520autocorrelation%2520inequality%253B%2520%2528ii%2529%2520a%2520GPUMode%2520kernel%2520competition%2520%2528up%2520to%2520%25242%255Ctimes%2524%2520faster%2520than%2520prior%2520art%2529%253B%2520%2528iii%2529%2520past%2520AtCoder%2520algorithm%2520competitions%253B%2520and%2520%2528iv%2529%2520denoising%2520problem%2520in%2520single-cell%2520analysis.%2520Our%2520solutions%2520are%2520reviewed%2520by%2520experts%2520or%2520the%2520organizers.%2520All%2520our%2520results%2520are%2520achieved%2520with%2520an%2520open%2520model%252C%2520OpenAI%2520gpt-oss-120b%252C%2520and%2520can%2520be%2520reproduced%2520with%2520our%2520publicly%2520available%2520code%252C%2520in%2520contrast%2520to%2520previous%2520best%2520results%2520that%2520required%2520closed%2520frontier%2520models.%2520Our%2520test-time%2520training%2520runs%2520are%2520performed%2520using%2520Tinker%252C%2520an%2520API%2520by%2520Thinking%2520Machines%252C%2520with%2520a%2520cost%2520of%2520only%2520a%2520few%2520hundred%2520dollars%2520per%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Discover%20at%20Test%20Time&entry.906535625=Mert%20Yuksekgonul%20and%20Daniel%20Koceja%20and%20Xinhao%20Li%20and%20Federico%20Bianchi%20and%20Jed%20McCaleb%20and%20Xiaolong%20Wang%20and%20Jan%20Kautz%20and%20Yejin%20Choi%20and%20James%20Zou%20and%20Carlos%20Guestrin%20and%20Yu%20Sun&entry.1292438233=How%20can%20we%20use%20AI%20to%20discover%20a%20new%20state%20of%20the%20art%20for%20a%20scientific%20problem%3F%20Prior%20work%20in%20test-time%20scaling%2C%20such%20as%20AlphaEvolve%2C%20performs%20search%20by%20prompting%20a%20frozen%20LLM.%20We%20perform%20reinforcement%20learning%20at%20test%20time%2C%20so%20the%20LLM%20can%20continue%20to%20train%2C%20but%20now%20with%20experience%20specific%20to%20the%20test%20problem.%20This%20form%20of%20continual%20learning%20is%20quite%20special%2C%20because%20its%20goal%20is%20to%20produce%20one%20great%20solution%20rather%20than%20many%20good%20ones%20on%20average%2C%20and%20to%20solve%20this%20very%20problem%20rather%20than%20generalize%20to%20other%20problems.%20Therefore%2C%20our%20learning%20objective%20and%20search%20subroutine%20are%20designed%20to%20prioritize%20the%20most%20promising%20solutions.%20We%20call%20this%20method%20Test-Time%20Training%20to%20Discover%20%28TTT-Discover%29.%20Following%20prior%20work%2C%20we%20focus%20on%20problems%20with%20continuous%20rewards.%20We%20report%20results%20for%20every%20problem%20we%20attempted%2C%20across%20mathematics%2C%20GPU%20kernel%20engineering%2C%20algorithm%20design%2C%20and%20biology.%20TTT-Discover%20sets%20the%20new%20state%20of%20the%20art%20in%20almost%20all%20of%20them%3A%20%28i%29%20Erd%C5%91s%27%20minimum%20overlap%20problem%20and%20an%20autocorrelation%20inequality%3B%20%28ii%29%20a%20GPUMode%20kernel%20competition%20%28up%20to%20%242%5Ctimes%24%20faster%20than%20prior%20art%29%3B%20%28iii%29%20past%20AtCoder%20algorithm%20competitions%3B%20and%20%28iv%29%20denoising%20problem%20in%20single-cell%20analysis.%20Our%20solutions%20are%20reviewed%20by%20experts%20or%20the%20organizers.%20All%20our%20results%20are%20achieved%20with%20an%20open%20model%2C%20OpenAI%20gpt-oss-120b%2C%20and%20can%20be%20reproduced%20with%20our%20publicly%20available%20code%2C%20in%20contrast%20to%20previous%20best%20results%20that%20required%20closed%20frontier%20models.%20Our%20test-time%20training%20runs%20are%20performed%20using%20Tinker%2C%20an%20API%20by%20Thinking%20Machines%2C%20with%20a%20cost%20of%20only%20a%20few%20hundred%20dollars%20per%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2601.16175v1&entry.124074799=Read"},
{"title": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech", "author": "Soufiane Jhilal and St\u00e9phanie Martin and Anne-Lise Giraud", "abstract": "Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.", "link": "http://arxiv.org/abs/2601.15909v1", "date": "2026-01-22", "relevancy": 2.0421, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20from%20ImageNet%20for%20MEG-Based%20Decoding%20of%20Imagined%20Speech&body=Title%3A%20Transfer%20Learning%20from%20ImageNet%20for%20MEG-Based%20Decoding%20of%20Imagined%20Speech%0AAuthor%3A%20Soufiane%20Jhilal%20and%20St%C3%A9phanie%20Martin%20and%20Anne-Lise%20Giraud%0AAbstract%3A%20Non-invasive%20decoding%20of%20imagined%20speech%20remains%20challenging%20due%20to%20weak%2C%20distributed%20signals%20and%20limited%20labeled%20data.%20Our%20paper%20introduces%20an%20image-based%20approach%20that%20transforms%20magnetoencephalography%20%28MEG%29%20signals%20into%20time-frequency%20representations%20compatible%20with%20pretrained%20vision%20models.%20MEG%20data%20from%2021%20participants%20performing%20imagined%20speech%20tasks%20were%20projected%20into%20three%20spatial%20scalogram%20mixtures%20via%20a%20learnable%20sensor-space%20convolution%2C%20producing%20compact%20image-like%20inputs%20for%20ImageNet-pretrained%20vision%20architectures.%20These%20models%20outperformed%20classical%20and%20non-pretrained%20models%2C%20achieving%20up%20to%2090.4%25%20balanced%20accuracy%20for%20imagery%20vs.%20silence%2C%2081.0%25%20vs.%20silent%20reading%2C%20and%2060.6%25%20for%20vowel%20decoding.%20Cross-subject%20evaluation%20confirmed%20that%20pretrained%20models%20capture%20shared%20neural%20representations%2C%20and%20temporal%20analyses%20localized%20discriminative%20information%20to%20imagery-locked%20intervals.%20These%20findings%20show%20that%20pretrained%20vision%20models%20applied%20to%20image-based%20MEG%20representations%20can%20effectively%20capture%20the%20structure%20of%20imagined%20speech%20in%20non-invasive%20neural%20signals.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520from%2520ImageNet%2520for%2520MEG-Based%2520Decoding%2520of%2520Imagined%2520Speech%26entry.906535625%3DSoufiane%2520Jhilal%2520and%2520St%25C3%25A9phanie%2520Martin%2520and%2520Anne-Lise%2520Giraud%26entry.1292438233%3DNon-invasive%2520decoding%2520of%2520imagined%2520speech%2520remains%2520challenging%2520due%2520to%2520weak%252C%2520distributed%2520signals%2520and%2520limited%2520labeled%2520data.%2520Our%2520paper%2520introduces%2520an%2520image-based%2520approach%2520that%2520transforms%2520magnetoencephalography%2520%2528MEG%2529%2520signals%2520into%2520time-frequency%2520representations%2520compatible%2520with%2520pretrained%2520vision%2520models.%2520MEG%2520data%2520from%252021%2520participants%2520performing%2520imagined%2520speech%2520tasks%2520were%2520projected%2520into%2520three%2520spatial%2520scalogram%2520mixtures%2520via%2520a%2520learnable%2520sensor-space%2520convolution%252C%2520producing%2520compact%2520image-like%2520inputs%2520for%2520ImageNet-pretrained%2520vision%2520architectures.%2520These%2520models%2520outperformed%2520classical%2520and%2520non-pretrained%2520models%252C%2520achieving%2520up%2520to%252090.4%2525%2520balanced%2520accuracy%2520for%2520imagery%2520vs.%2520silence%252C%252081.0%2525%2520vs.%2520silent%2520reading%252C%2520and%252060.6%2525%2520for%2520vowel%2520decoding.%2520Cross-subject%2520evaluation%2520confirmed%2520that%2520pretrained%2520models%2520capture%2520shared%2520neural%2520representations%252C%2520and%2520temporal%2520analyses%2520localized%2520discriminative%2520information%2520to%2520imagery-locked%2520intervals.%2520These%2520findings%2520show%2520that%2520pretrained%2520vision%2520models%2520applied%2520to%2520image-based%2520MEG%2520representations%2520can%2520effectively%2520capture%2520the%2520structure%2520of%2520imagined%2520speech%2520in%2520non-invasive%2520neural%2520signals.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20from%20ImageNet%20for%20MEG-Based%20Decoding%20of%20Imagined%20Speech&entry.906535625=Soufiane%20Jhilal%20and%20St%C3%A9phanie%20Martin%20and%20Anne-Lise%20Giraud&entry.1292438233=Non-invasive%20decoding%20of%20imagined%20speech%20remains%20challenging%20due%20to%20weak%2C%20distributed%20signals%20and%20limited%20labeled%20data.%20Our%20paper%20introduces%20an%20image-based%20approach%20that%20transforms%20magnetoencephalography%20%28MEG%29%20signals%20into%20time-frequency%20representations%20compatible%20with%20pretrained%20vision%20models.%20MEG%20data%20from%2021%20participants%20performing%20imagined%20speech%20tasks%20were%20projected%20into%20three%20spatial%20scalogram%20mixtures%20via%20a%20learnable%20sensor-space%20convolution%2C%20producing%20compact%20image-like%20inputs%20for%20ImageNet-pretrained%20vision%20architectures.%20These%20models%20outperformed%20classical%20and%20non-pretrained%20models%2C%20achieving%20up%20to%2090.4%25%20balanced%20accuracy%20for%20imagery%20vs.%20silence%2C%2081.0%25%20vs.%20silent%20reading%2C%20and%2060.6%25%20for%20vowel%20decoding.%20Cross-subject%20evaluation%20confirmed%20that%20pretrained%20models%20capture%20shared%20neural%20representations%2C%20and%20temporal%20analyses%20localized%20discriminative%20information%20to%20imagery-locked%20intervals.%20These%20findings%20show%20that%20pretrained%20vision%20models%20applied%20to%20image-based%20MEG%20representations%20can%20effectively%20capture%20the%20structure%20of%20imagined%20speech%20in%20non-invasive%20neural%20signals.&entry.1838667208=http%3A//arxiv.org/abs/2601.15909v1&entry.124074799=Read"},
{"title": "MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging", "author": "Tianjun Wei and Enneng Yang and Yingpeng Du and Huizhong Guo and Jie Zhang and Zhu Sun", "abstract": "Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.", "link": "http://arxiv.org/abs/2601.15930v1", "date": "2026-01-22", "relevancy": 2.0406, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5188}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5085}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMGRid%3A%20Navigating%20Temporal-aware%20and%20Cross-domain%20Generative%20Recommendation%20via%20Model%20Merging&body=Title%3A%20MMGRid%3A%20Navigating%20Temporal-aware%20and%20Cross-domain%20Generative%20Recommendation%20via%20Model%20Merging%0AAuthor%3A%20Tianjun%20Wei%20and%20Enneng%20Yang%20and%20Yingpeng%20Du%20and%20Huizhong%20Guo%20and%20Jie%20Zhang%20and%20Zhu%20Sun%0AAbstract%3A%20Model%20merging%20%28MM%29%20offers%20an%20efficient%20mechanism%20for%20integrating%20multiple%20specialized%20models%20without%20access%20to%20original%20training%20data%20or%20costly%20retraining.%20While%20MM%20has%20demonstrated%20success%20in%20domains%20like%20computer%20vision%2C%20its%20role%20in%20recommender%20systems%20%28RSs%29%20remains%20largely%20unexplored.%20Recently%2C%20Generative%20Recommendation%20%28GR%29%20has%20emerged%20as%20a%20new%20paradigm%20in%20RSs%2C%20characterized%20by%20rapidly%20growing%20model%20scales%20and%20substantial%20computational%20costs%2C%20making%20MM%20particularly%20appealing%20for%20cost-sensitive%20deployment%20scenarios.%20In%20this%20work%2C%20we%20present%20the%20first%20systematic%20study%20of%20MM%20in%20GR%20through%20a%20contextual%20lens.%20We%20focus%20on%20a%20fundamental%20yet%20underexplored%20challenge%20in%20real-world%3A%20how%20to%20merge%20generative%20recommenders%20specialized%20to%20different%20real-world%20contexts%2C%20arising%20from%20temporal%20evolving%20user%20behaviors%20and%20heterogeneous%20application%20domains.%20To%20this%20end%2C%20we%20propose%20a%20unified%20framework%20MMGRid%2C%20a%20structured%20contextual%20grid%20of%20GR%20checkpoints%20that%20organizes%20models%20trained%20under%20diverse%20contexts%20induced%20by%20temporal%20evolution%20and%20domain%20diversity.%20All%20checkpoints%20are%20derived%20from%20a%20shared%20base%20LLM%20but%20fine-tuned%20on%20context-specific%20data%2C%20forming%20a%20realistic%20and%20controlled%20model%20space%20for%20systematically%20analyzing%20MM%20across%20GR%20paradigms%20and%20merging%20algorithms.%20Our%20investigation%20reveals%20several%20key%20insights.%20First%2C%20training%20GR%20models%20from%20LLMs%20can%20introduce%20parameter%20conflicts%20during%20merging%20due%20to%20token%20distribution%20shifts%20and%20objective%20disparities%3B%20such%20conflicts%20can%20be%20alleviated%20by%20disentangling%20task-aware%20and%20context-specific%20parameter%20changes%20via%20base%20model%20replacement.%20Second%2C%20incremental%20training%20across%20contexts%20induces%20recency%20bias%2C%20which%20can%20be%20effectively%20balanced%20through%20weighted%20contextual%20merging.%20Notably%2C%20we%20observe%20that%20optimal%20merging%20weights%20correlate%20with%20context-dependent%20interaction%20characteristics%2C%20offering%20practical%20guidance%20for%20weight%20selection%20in%20real-world%20deployments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMGRid%253A%2520Navigating%2520Temporal-aware%2520and%2520Cross-domain%2520Generative%2520Recommendation%2520via%2520Model%2520Merging%26entry.906535625%3DTianjun%2520Wei%2520and%2520Enneng%2520Yang%2520and%2520Yingpeng%2520Du%2520and%2520Huizhong%2520Guo%2520and%2520Jie%2520Zhang%2520and%2520Zhu%2520Sun%26entry.1292438233%3DModel%2520merging%2520%2528MM%2529%2520offers%2520an%2520efficient%2520mechanism%2520for%2520integrating%2520multiple%2520specialized%2520models%2520without%2520access%2520to%2520original%2520training%2520data%2520or%2520costly%2520retraining.%2520While%2520MM%2520has%2520demonstrated%2520success%2520in%2520domains%2520like%2520computer%2520vision%252C%2520its%2520role%2520in%2520recommender%2520systems%2520%2528RSs%2529%2520remains%2520largely%2520unexplored.%2520Recently%252C%2520Generative%2520Recommendation%2520%2528GR%2529%2520has%2520emerged%2520as%2520a%2520new%2520paradigm%2520in%2520RSs%252C%2520characterized%2520by%2520rapidly%2520growing%2520model%2520scales%2520and%2520substantial%2520computational%2520costs%252C%2520making%2520MM%2520particularly%2520appealing%2520for%2520cost-sensitive%2520deployment%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520systematic%2520study%2520of%2520MM%2520in%2520GR%2520through%2520a%2520contextual%2520lens.%2520We%2520focus%2520on%2520a%2520fundamental%2520yet%2520underexplored%2520challenge%2520in%2520real-world%253A%2520how%2520to%2520merge%2520generative%2520recommenders%2520specialized%2520to%2520different%2520real-world%2520contexts%252C%2520arising%2520from%2520temporal%2520evolving%2520user%2520behaviors%2520and%2520heterogeneous%2520application%2520domains.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520unified%2520framework%2520MMGRid%252C%2520a%2520structured%2520contextual%2520grid%2520of%2520GR%2520checkpoints%2520that%2520organizes%2520models%2520trained%2520under%2520diverse%2520contexts%2520induced%2520by%2520temporal%2520evolution%2520and%2520domain%2520diversity.%2520All%2520checkpoints%2520are%2520derived%2520from%2520a%2520shared%2520base%2520LLM%2520but%2520fine-tuned%2520on%2520context-specific%2520data%252C%2520forming%2520a%2520realistic%2520and%2520controlled%2520model%2520space%2520for%2520systematically%2520analyzing%2520MM%2520across%2520GR%2520paradigms%2520and%2520merging%2520algorithms.%2520Our%2520investigation%2520reveals%2520several%2520key%2520insights.%2520First%252C%2520training%2520GR%2520models%2520from%2520LLMs%2520can%2520introduce%2520parameter%2520conflicts%2520during%2520merging%2520due%2520to%2520token%2520distribution%2520shifts%2520and%2520objective%2520disparities%253B%2520such%2520conflicts%2520can%2520be%2520alleviated%2520by%2520disentangling%2520task-aware%2520and%2520context-specific%2520parameter%2520changes%2520via%2520base%2520model%2520replacement.%2520Second%252C%2520incremental%2520training%2520across%2520contexts%2520induces%2520recency%2520bias%252C%2520which%2520can%2520be%2520effectively%2520balanced%2520through%2520weighted%2520contextual%2520merging.%2520Notably%252C%2520we%2520observe%2520that%2520optimal%2520merging%2520weights%2520correlate%2520with%2520context-dependent%2520interaction%2520characteristics%252C%2520offering%2520practical%2520guidance%2520for%2520weight%2520selection%2520in%2520real-world%2520deployments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMGRid%3A%20Navigating%20Temporal-aware%20and%20Cross-domain%20Generative%20Recommendation%20via%20Model%20Merging&entry.906535625=Tianjun%20Wei%20and%20Enneng%20Yang%20and%20Yingpeng%20Du%20and%20Huizhong%20Guo%20and%20Jie%20Zhang%20and%20Zhu%20Sun&entry.1292438233=Model%20merging%20%28MM%29%20offers%20an%20efficient%20mechanism%20for%20integrating%20multiple%20specialized%20models%20without%20access%20to%20original%20training%20data%20or%20costly%20retraining.%20While%20MM%20has%20demonstrated%20success%20in%20domains%20like%20computer%20vision%2C%20its%20role%20in%20recommender%20systems%20%28RSs%29%20remains%20largely%20unexplored.%20Recently%2C%20Generative%20Recommendation%20%28GR%29%20has%20emerged%20as%20a%20new%20paradigm%20in%20RSs%2C%20characterized%20by%20rapidly%20growing%20model%20scales%20and%20substantial%20computational%20costs%2C%20making%20MM%20particularly%20appealing%20for%20cost-sensitive%20deployment%20scenarios.%20In%20this%20work%2C%20we%20present%20the%20first%20systematic%20study%20of%20MM%20in%20GR%20through%20a%20contextual%20lens.%20We%20focus%20on%20a%20fundamental%20yet%20underexplored%20challenge%20in%20real-world%3A%20how%20to%20merge%20generative%20recommenders%20specialized%20to%20different%20real-world%20contexts%2C%20arising%20from%20temporal%20evolving%20user%20behaviors%20and%20heterogeneous%20application%20domains.%20To%20this%20end%2C%20we%20propose%20a%20unified%20framework%20MMGRid%2C%20a%20structured%20contextual%20grid%20of%20GR%20checkpoints%20that%20organizes%20models%20trained%20under%20diverse%20contexts%20induced%20by%20temporal%20evolution%20and%20domain%20diversity.%20All%20checkpoints%20are%20derived%20from%20a%20shared%20base%20LLM%20but%20fine-tuned%20on%20context-specific%20data%2C%20forming%20a%20realistic%20and%20controlled%20model%20space%20for%20systematically%20analyzing%20MM%20across%20GR%20paradigms%20and%20merging%20algorithms.%20Our%20investigation%20reveals%20several%20key%20insights.%20First%2C%20training%20GR%20models%20from%20LLMs%20can%20introduce%20parameter%20conflicts%20during%20merging%20due%20to%20token%20distribution%20shifts%20and%20objective%20disparities%3B%20such%20conflicts%20can%20be%20alleviated%20by%20disentangling%20task-aware%20and%20context-specific%20parameter%20changes%20via%20base%20model%20replacement.%20Second%2C%20incremental%20training%20across%20contexts%20induces%20recency%20bias%2C%20which%20can%20be%20effectively%20balanced%20through%20weighted%20contextual%20merging.%20Notably%2C%20we%20observe%20that%20optimal%20merging%20weights%20correlate%20with%20context-dependent%20interaction%20characteristics%2C%20offering%20practical%20guidance%20for%20weight%20selection%20in%20real-world%20deployments.&entry.1838667208=http%3A//arxiv.org/abs/2601.15930v1&entry.124074799=Read"},
{"title": "Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation", "author": "Jinsook Lee and Kirk Vanacore and Zhuqian Zhou and Bakhtawar Ahtisham and Jeanine Grutter and Rene F. Kizilcec", "abstract": "Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.", "link": "http://arxiv.org/abs/2601.12061v2", "date": "2026-01-22", "relevancy": 2.0341, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Codebook-Injected%20Dialogue%20Segmentation%20for%20Multi-Utterance%20Constructs%20Annotation%3A%20LLM-Assisted%20and%20Gold-Label-Free%20Evaluation&body=Title%3A%20Codebook-Injected%20Dialogue%20Segmentation%20for%20Multi-Utterance%20Constructs%20Annotation%3A%20LLM-Assisted%20and%20Gold-Label-Free%20Evaluation%0AAuthor%3A%20Jinsook%20Lee%20and%20Kirk%20Vanacore%20and%20Zhuqian%20Zhou%20and%20Bakhtawar%20Ahtisham%20and%20Jeanine%20Grutter%20and%20Rene%20F.%20Kizilcec%0AAbstract%3A%20Dialogue%20Act%20%28DA%29%20annotation%20typically%20treats%20communicative%20or%20pedagogical%20intent%20as%20localized%20to%20individual%20utterances%20or%20turns.%20This%20leads%20annotators%20to%20agree%20on%20the%20underlying%20action%20while%20disagreeing%20on%20segment%20boundaries%2C%20reducing%20apparent%20reliability.%20We%20propose%20codebook-injected%20segmentation%2C%20which%20conditions%20boundary%20decisions%20on%20downstream%20annotation%20criteria%2C%20and%20evaluate%20LLM-based%20segmenters%20against%20standard%20and%20retrieval-augmented%20baselines.%20To%20assess%20these%20without%20gold%20labels%2C%20we%20introduce%20evaluation%20metrics%20for%20span%20consistency%2C%20distinctiveness%2C%20and%20human-AI%20distributional%20agreement.%20We%20found%20DA-awareness%20produces%20segments%20that%20are%20internally%20more%20consistent%20than%20text-only%20baselines.%20While%20LLMs%20excel%20at%20creating%20construct-consistent%20spans%2C%20coherence-based%20baselines%20remain%20superior%20at%20detecting%20global%20shifts%20in%20dialogue%20flow.%20Across%20two%20datasets%2C%20no%20single%20segmenter%20dominates.%20Improvements%20in%20within-segment%20coherence%20frequently%20trade%20off%20against%20boundary%20distinctiveness%20and%20human-AI%20distributional%20agreement.%20These%20results%20highlight%20segmentation%20as%20a%20consequential%20design%20choice%20that%20should%20be%20optimized%20for%20downstream%20objectives%20rather%20than%20a%20single%20performance%20score.%0ALink%3A%20http%3A//arxiv.org/abs/2601.12061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodebook-Injected%2520Dialogue%2520Segmentation%2520for%2520Multi-Utterance%2520Constructs%2520Annotation%253A%2520LLM-Assisted%2520and%2520Gold-Label-Free%2520Evaluation%26entry.906535625%3DJinsook%2520Lee%2520and%2520Kirk%2520Vanacore%2520and%2520Zhuqian%2520Zhou%2520and%2520Bakhtawar%2520Ahtisham%2520and%2520Jeanine%2520Grutter%2520and%2520Rene%2520F.%2520Kizilcec%26entry.1292438233%3DDialogue%2520Act%2520%2528DA%2529%2520annotation%2520typically%2520treats%2520communicative%2520or%2520pedagogical%2520intent%2520as%2520localized%2520to%2520individual%2520utterances%2520or%2520turns.%2520This%2520leads%2520annotators%2520to%2520agree%2520on%2520the%2520underlying%2520action%2520while%2520disagreeing%2520on%2520segment%2520boundaries%252C%2520reducing%2520apparent%2520reliability.%2520We%2520propose%2520codebook-injected%2520segmentation%252C%2520which%2520conditions%2520boundary%2520decisions%2520on%2520downstream%2520annotation%2520criteria%252C%2520and%2520evaluate%2520LLM-based%2520segmenters%2520against%2520standard%2520and%2520retrieval-augmented%2520baselines.%2520To%2520assess%2520these%2520without%2520gold%2520labels%252C%2520we%2520introduce%2520evaluation%2520metrics%2520for%2520span%2520consistency%252C%2520distinctiveness%252C%2520and%2520human-AI%2520distributional%2520agreement.%2520We%2520found%2520DA-awareness%2520produces%2520segments%2520that%2520are%2520internally%2520more%2520consistent%2520than%2520text-only%2520baselines.%2520While%2520LLMs%2520excel%2520at%2520creating%2520construct-consistent%2520spans%252C%2520coherence-based%2520baselines%2520remain%2520superior%2520at%2520detecting%2520global%2520shifts%2520in%2520dialogue%2520flow.%2520Across%2520two%2520datasets%252C%2520no%2520single%2520segmenter%2520dominates.%2520Improvements%2520in%2520within-segment%2520coherence%2520frequently%2520trade%2520off%2520against%2520boundary%2520distinctiveness%2520and%2520human-AI%2520distributional%2520agreement.%2520These%2520results%2520highlight%2520segmentation%2520as%2520a%2520consequential%2520design%2520choice%2520that%2520should%2520be%2520optimized%2520for%2520downstream%2520objectives%2520rather%2520than%2520a%2520single%2520performance%2520score.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.12061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Codebook-Injected%20Dialogue%20Segmentation%20for%20Multi-Utterance%20Constructs%20Annotation%3A%20LLM-Assisted%20and%20Gold-Label-Free%20Evaluation&entry.906535625=Jinsook%20Lee%20and%20Kirk%20Vanacore%20and%20Zhuqian%20Zhou%20and%20Bakhtawar%20Ahtisham%20and%20Jeanine%20Grutter%20and%20Rene%20F.%20Kizilcec&entry.1292438233=Dialogue%20Act%20%28DA%29%20annotation%20typically%20treats%20communicative%20or%20pedagogical%20intent%20as%20localized%20to%20individual%20utterances%20or%20turns.%20This%20leads%20annotators%20to%20agree%20on%20the%20underlying%20action%20while%20disagreeing%20on%20segment%20boundaries%2C%20reducing%20apparent%20reliability.%20We%20propose%20codebook-injected%20segmentation%2C%20which%20conditions%20boundary%20decisions%20on%20downstream%20annotation%20criteria%2C%20and%20evaluate%20LLM-based%20segmenters%20against%20standard%20and%20retrieval-augmented%20baselines.%20To%20assess%20these%20without%20gold%20labels%2C%20we%20introduce%20evaluation%20metrics%20for%20span%20consistency%2C%20distinctiveness%2C%20and%20human-AI%20distributional%20agreement.%20We%20found%20DA-awareness%20produces%20segments%20that%20are%20internally%20more%20consistent%20than%20text-only%20baselines.%20While%20LLMs%20excel%20at%20creating%20construct-consistent%20spans%2C%20coherence-based%20baselines%20remain%20superior%20at%20detecting%20global%20shifts%20in%20dialogue%20flow.%20Across%20two%20datasets%2C%20no%20single%20segmenter%20dominates.%20Improvements%20in%20within-segment%20coherence%20frequently%20trade%20off%20against%20boundary%20distinctiveness%20and%20human-AI%20distributional%20agreement.%20These%20results%20highlight%20segmentation%20as%20a%20consequential%20design%20choice%20that%20should%20be%20optimized%20for%20downstream%20objectives%20rather%20than%20a%20single%20performance%20score.&entry.1838667208=http%3A//arxiv.org/abs/2601.12061v2&entry.124074799=Read"},
{"title": "PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling", "author": "Ai Jian and Jingqing Ruan and Xing Ma and Dailin Li and Weipeng Zhang and Ke Zeng and Xunliang Cai", "abstract": "Reward models (RMs) are central to reinforcement learning from human feedback (RLHF), providing the critical supervision signals that align large language models (LLMs) with human preferences. Generative reward models (GRMs) provide greater interpretability than traditional scalar RMs, but they come with a critical trade-off: pairwise methods are hindered by a training-inference mismatch, while pointwise methods require expensive absolute annotations. To bridge this gap, we propose the Preference-aware Task-adaptive Reward Model (PaTaRM). Unlike prior approaches, PaTaRM enables robust pointwise training using readily available pairwise data via a novel Preference-Aware Reward (PAR) mechanism, eliminating the need for explicit rating labels. Furthermore, it incorporates a Task-Adaptive Rubric system that dynamically generates instance-specific criteria for precise evaluation. Extensive experiments demonstrate that PATRM achieves a 8.7% average improvement on RewardBench and RMBench across Qwen3-8B/14B models. Crucially, it boosts downstream RLHF performance by an average relative improvement of 13.6% across IFEval and InFoBench, validating its effectiveness for policy alignment. Our code is available at https://github.com/JaneEyre0530/PaTaRM.", "link": "http://arxiv.org/abs/2510.24235v2", "date": "2026-01-22", "relevancy": 2.0289, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5073}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaTaRM%3A%20Bridging%20Pairwise%20and%20Pointwise%20Signals%20via%20Preference-Aware%20Task-Adaptive%20Reward%20Modeling&body=Title%3A%20PaTaRM%3A%20Bridging%20Pairwise%20and%20Pointwise%20Signals%20via%20Preference-Aware%20Task-Adaptive%20Reward%20Modeling%0AAuthor%3A%20Ai%20Jian%20and%20Jingqing%20Ruan%20and%20Xing%20Ma%20and%20Dailin%20Li%20and%20Weipeng%20Zhang%20and%20Ke%20Zeng%20and%20Xunliang%20Cai%0AAbstract%3A%20Reward%20models%20%28RMs%29%20are%20central%20to%20reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%2C%20providing%20the%20critical%20supervision%20signals%20that%20align%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences.%20Generative%20reward%20models%20%28GRMs%29%20provide%20greater%20interpretability%20than%20traditional%20scalar%20RMs%2C%20but%20they%20come%20with%20a%20critical%20trade-off%3A%20pairwise%20methods%20are%20hindered%20by%20a%20training-inference%20mismatch%2C%20while%20pointwise%20methods%20require%20expensive%20absolute%20annotations.%20To%20bridge%20this%20gap%2C%20we%20propose%20the%20Preference-aware%20Task-adaptive%20Reward%20Model%20%28PaTaRM%29.%20Unlike%20prior%20approaches%2C%20PaTaRM%20enables%20robust%20pointwise%20training%20using%20readily%20available%20pairwise%20data%20via%20a%20novel%20Preference-Aware%20Reward%20%28PAR%29%20mechanism%2C%20eliminating%20the%20need%20for%20explicit%20rating%20labels.%20Furthermore%2C%20it%20incorporates%20a%20Task-Adaptive%20Rubric%20system%20that%20dynamically%20generates%20instance-specific%20criteria%20for%20precise%20evaluation.%20Extensive%20experiments%20demonstrate%20that%20PATRM%20achieves%20a%208.7%25%20average%20improvement%20on%20RewardBench%20and%20RMBench%20across%20Qwen3-8B/14B%20models.%20Crucially%2C%20it%20boosts%20downstream%20RLHF%20performance%20by%20an%20average%20relative%20improvement%20of%2013.6%25%20across%20IFEval%20and%20InFoBench%2C%20validating%20its%20effectiveness%20for%20policy%20alignment.%20Our%20code%20is%20available%20at%20https%3A//github.com/JaneEyre0530/PaTaRM.%0ALink%3A%20http%3A//arxiv.org/abs/2510.24235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaTaRM%253A%2520Bridging%2520Pairwise%2520and%2520Pointwise%2520Signals%2520via%2520Preference-Aware%2520Task-Adaptive%2520Reward%2520Modeling%26entry.906535625%3DAi%2520Jian%2520and%2520Jingqing%2520Ruan%2520and%2520Xing%2520Ma%2520and%2520Dailin%2520Li%2520and%2520Weipeng%2520Zhang%2520and%2520Ke%2520Zeng%2520and%2520Xunliang%2520Cai%26entry.1292438233%3DReward%2520models%2520%2528RMs%2529%2520are%2520central%2520to%2520reinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%252C%2520providing%2520the%2520critical%2520supervision%2520signals%2520that%2520align%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520preferences.%2520Generative%2520reward%2520models%2520%2528GRMs%2529%2520provide%2520greater%2520interpretability%2520than%2520traditional%2520scalar%2520RMs%252C%2520but%2520they%2520come%2520with%2520a%2520critical%2520trade-off%253A%2520pairwise%2520methods%2520are%2520hindered%2520by%2520a%2520training-inference%2520mismatch%252C%2520while%2520pointwise%2520methods%2520require%2520expensive%2520absolute%2520annotations.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520the%2520Preference-aware%2520Task-adaptive%2520Reward%2520Model%2520%2528PaTaRM%2529.%2520Unlike%2520prior%2520approaches%252C%2520PaTaRM%2520enables%2520robust%2520pointwise%2520training%2520using%2520readily%2520available%2520pairwise%2520data%2520via%2520a%2520novel%2520Preference-Aware%2520Reward%2520%2528PAR%2529%2520mechanism%252C%2520eliminating%2520the%2520need%2520for%2520explicit%2520rating%2520labels.%2520Furthermore%252C%2520it%2520incorporates%2520a%2520Task-Adaptive%2520Rubric%2520system%2520that%2520dynamically%2520generates%2520instance-specific%2520criteria%2520for%2520precise%2520evaluation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520PATRM%2520achieves%2520a%25208.7%2525%2520average%2520improvement%2520on%2520RewardBench%2520and%2520RMBench%2520across%2520Qwen3-8B/14B%2520models.%2520Crucially%252C%2520it%2520boosts%2520downstream%2520RLHF%2520performance%2520by%2520an%2520average%2520relative%2520improvement%2520of%252013.6%2525%2520across%2520IFEval%2520and%2520InFoBench%252C%2520validating%2520its%2520effectiveness%2520for%2520policy%2520alignment.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/JaneEyre0530/PaTaRM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaTaRM%3A%20Bridging%20Pairwise%20and%20Pointwise%20Signals%20via%20Preference-Aware%20Task-Adaptive%20Reward%20Modeling&entry.906535625=Ai%20Jian%20and%20Jingqing%20Ruan%20and%20Xing%20Ma%20and%20Dailin%20Li%20and%20Weipeng%20Zhang%20and%20Ke%20Zeng%20and%20Xunliang%20Cai&entry.1292438233=Reward%20models%20%28RMs%29%20are%20central%20to%20reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%2C%20providing%20the%20critical%20supervision%20signals%20that%20align%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences.%20Generative%20reward%20models%20%28GRMs%29%20provide%20greater%20interpretability%20than%20traditional%20scalar%20RMs%2C%20but%20they%20come%20with%20a%20critical%20trade-off%3A%20pairwise%20methods%20are%20hindered%20by%20a%20training-inference%20mismatch%2C%20while%20pointwise%20methods%20require%20expensive%20absolute%20annotations.%20To%20bridge%20this%20gap%2C%20we%20propose%20the%20Preference-aware%20Task-adaptive%20Reward%20Model%20%28PaTaRM%29.%20Unlike%20prior%20approaches%2C%20PaTaRM%20enables%20robust%20pointwise%20training%20using%20readily%20available%20pairwise%20data%20via%20a%20novel%20Preference-Aware%20Reward%20%28PAR%29%20mechanism%2C%20eliminating%20the%20need%20for%20explicit%20rating%20labels.%20Furthermore%2C%20it%20incorporates%20a%20Task-Adaptive%20Rubric%20system%20that%20dynamically%20generates%20instance-specific%20criteria%20for%20precise%20evaluation.%20Extensive%20experiments%20demonstrate%20that%20PATRM%20achieves%20a%208.7%25%20average%20improvement%20on%20RewardBench%20and%20RMBench%20across%20Qwen3-8B/14B%20models.%20Crucially%2C%20it%20boosts%20downstream%20RLHF%20performance%20by%20an%20average%20relative%20improvement%20of%2013.6%25%20across%20IFEval%20and%20InFoBench%2C%20validating%20its%20effectiveness%20for%20policy%20alignment.%20Our%20code%20is%20available%20at%20https%3A//github.com/JaneEyre0530/PaTaRM.&entry.1838667208=http%3A//arxiv.org/abs/2510.24235v2&entry.124074799=Read"},
{"title": "Medal Matters: Probing LLMs' Failure Cases Through Olympic Rankings", "author": "Juhwan Choi and Seunguk Yu and JungMin Yun and YoungBin Kim", "abstract": "Large language models (LLMs) have achieved remarkable success in natural language processing tasks, yet their internal knowledge structures remain poorly understood. This study examines these structures through the lens of historical Olympic medal tallies, evaluating LLMs on two tasks: (1) retrieving medal counts for specific teams and (2) identifying rankings of each team. While state-of-the-art LLMs excel in recalling medal counts, they struggle with providing rankings, highlighting a key difference between their knowledge organization and human reasoning. These findings shed light on the limitations of LLMs' internal knowledge integration and suggest directions for improvement. To facilitate further research, we release our code, dataset, and model outputs.", "link": "http://arxiv.org/abs/2409.06518v3", "date": "2026-01-22", "relevancy": 2.0264, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Medal%20Matters%3A%20Probing%20LLMs%27%20Failure%20Cases%20Through%20Olympic%20Rankings&body=Title%3A%20Medal%20Matters%3A%20Probing%20LLMs%27%20Failure%20Cases%20Through%20Olympic%20Rankings%0AAuthor%3A%20Juhwan%20Choi%20and%20Seunguk%20Yu%20and%20JungMin%20Yun%20and%20YoungBin%20Kim%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20natural%20language%20processing%20tasks%2C%20yet%20their%20internal%20knowledge%20structures%20remain%20poorly%20understood.%20This%20study%20examines%20these%20structures%20through%20the%20lens%20of%20historical%20Olympic%20medal%20tallies%2C%20evaluating%20LLMs%20on%20two%20tasks%3A%20%281%29%20retrieving%20medal%20counts%20for%20specific%20teams%20and%20%282%29%20identifying%20rankings%20of%20each%20team.%20While%20state-of-the-art%20LLMs%20excel%20in%20recalling%20medal%20counts%2C%20they%20struggle%20with%20providing%20rankings%2C%20highlighting%20a%20key%20difference%20between%20their%20knowledge%20organization%20and%20human%20reasoning.%20These%20findings%20shed%20light%20on%20the%20limitations%20of%20LLMs%27%20internal%20knowledge%20integration%20and%20suggest%20directions%20for%20improvement.%20To%20facilitate%20further%20research%2C%20we%20release%20our%20code%2C%20dataset%2C%20and%20model%20outputs.%0ALink%3A%20http%3A//arxiv.org/abs/2409.06518v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedal%2520Matters%253A%2520Probing%2520LLMs%2527%2520Failure%2520Cases%2520Through%2520Olympic%2520Rankings%26entry.906535625%3DJuhwan%2520Choi%2520and%2520Seunguk%2520Yu%2520and%2520JungMin%2520Yun%2520and%2520YoungBin%2520Kim%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520natural%2520language%2520processing%2520tasks%252C%2520yet%2520their%2520internal%2520knowledge%2520structures%2520remain%2520poorly%2520understood.%2520This%2520study%2520examines%2520these%2520structures%2520through%2520the%2520lens%2520of%2520historical%2520Olympic%2520medal%2520tallies%252C%2520evaluating%2520LLMs%2520on%2520two%2520tasks%253A%2520%25281%2529%2520retrieving%2520medal%2520counts%2520for%2520specific%2520teams%2520and%2520%25282%2529%2520identifying%2520rankings%2520of%2520each%2520team.%2520While%2520state-of-the-art%2520LLMs%2520excel%2520in%2520recalling%2520medal%2520counts%252C%2520they%2520struggle%2520with%2520providing%2520rankings%252C%2520highlighting%2520a%2520key%2520difference%2520between%2520their%2520knowledge%2520organization%2520and%2520human%2520reasoning.%2520These%2520findings%2520shed%2520light%2520on%2520the%2520limitations%2520of%2520LLMs%2527%2520internal%2520knowledge%2520integration%2520and%2520suggest%2520directions%2520for%2520improvement.%2520To%2520facilitate%2520further%2520research%252C%2520we%2520release%2520our%2520code%252C%2520dataset%252C%2520and%2520model%2520outputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06518v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medal%20Matters%3A%20Probing%20LLMs%27%20Failure%20Cases%20Through%20Olympic%20Rankings&entry.906535625=Juhwan%20Choi%20and%20Seunguk%20Yu%20and%20JungMin%20Yun%20and%20YoungBin%20Kim&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20natural%20language%20processing%20tasks%2C%20yet%20their%20internal%20knowledge%20structures%20remain%20poorly%20understood.%20This%20study%20examines%20these%20structures%20through%20the%20lens%20of%20historical%20Olympic%20medal%20tallies%2C%20evaluating%20LLMs%20on%20two%20tasks%3A%20%281%29%20retrieving%20medal%20counts%20for%20specific%20teams%20and%20%282%29%20identifying%20rankings%20of%20each%20team.%20While%20state-of-the-art%20LLMs%20excel%20in%20recalling%20medal%20counts%2C%20they%20struggle%20with%20providing%20rankings%2C%20highlighting%20a%20key%20difference%20between%20their%20knowledge%20organization%20and%20human%20reasoning.%20These%20findings%20shed%20light%20on%20the%20limitations%20of%20LLMs%27%20internal%20knowledge%20integration%20and%20suggest%20directions%20for%20improvement.%20To%20facilitate%20further%20research%2C%20we%20release%20our%20code%2C%20dataset%2C%20and%20model%20outputs.&entry.1838667208=http%3A//arxiv.org/abs/2409.06518v3&entry.124074799=Read"},
{"title": "PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis", "author": "Yifan Chen and Fei Yin and Hao Chen and Jia Wu and Chao Li", "abstract": "Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.", "link": "http://arxiv.org/abs/2601.15884v1", "date": "2026-01-22", "relevancy": 2.0202, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5109}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5109}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PMPBench%3A%20A%20Paired%20Multi-Modal%20Pan-Cancer%20Benchmark%20for%20Medical%20Image%20Synthesis&body=Title%3A%20PMPBench%3A%20A%20Paired%20Multi-Modal%20Pan-Cancer%20Benchmark%20for%20Medical%20Image%20Synthesis%0AAuthor%3A%20Yifan%20Chen%20and%20Fei%20Yin%20and%20Hao%20Chen%20and%20Jia%20Wu%20and%20Chao%20Li%0AAbstract%3A%20Contrast%20medium%20plays%20a%20pivotal%20role%20in%20radiological%20imaging%2C%20as%20it%20amplifies%20lesion%20conspicuity%20and%20improves%20detection%20for%20the%20diagnosis%20of%20tumor-related%20diseases.%20However%2C%20depending%20on%20the%20patient%27s%20health%20condition%20or%20the%20medical%20resources%20available%2C%20the%20use%20of%20contrast%20medium%20is%20not%20always%20feasible.%20Recent%20work%20has%20explored%20AI-based%20image%20translation%20to%20synthesize%20contrast-enhanced%20images%20directly%20from%20non-contrast%20scans%2C%20aims%20to%20reduce%20side%20effects%20and%20streamlines%20clinical%20workflows.%20Progress%20in%20this%20direction%20has%20been%20constrained%20by%20data%20limitations%3A%20%281%29%20existing%20public%20datasets%20focus%20almost%20exclusively%20on%20brain-related%20paired%20MR%20modalities%3B%20%282%29%20other%20collections%20include%20partially%20paired%20data%20but%20suffer%20from%20missing%20modalities/timestamps%20and%20imperfect%20spatial%20alignment%3B%20%283%29%20explicit%20labeling%20of%20CT%20vs.%20CTC%20or%20DCE%20phases%20is%20often%20absent%3B%20%284%29%20substantial%20resources%20remain%20private.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20first%20public%2C%20fully%20paired%2C%20pan-cancer%20medical%20imaging%20dataset%20spanning%2011%20human%20organs.%20The%20MR%20data%20include%20complete%20dynamic%20contrast-enhanced%20%28DCE%29%20sequences%20covering%20all%20three%20phases%20%28DCE1-DCE3%29%2C%20while%20the%20CT%20data%20provide%20paired%20non-contrast%20and%20contrast-enhanced%20acquisitions%20%28CTC%29.%20The%20dataset%20is%20curated%20for%20anatomical%20correspondence%2C%20enabling%20rigorous%20evaluation%20of%201-to-1%2C%20N-to-1%2C%20and%20N-to-N%20translation%20settings%20%28e.g.%2C%20predicting%20DCE%20phases%20from%20non-contrast%20inputs%29.%20Built%20upon%20this%20resource%2C%20we%20establish%20a%20comprehensive%20benchmark.%20We%20report%20results%20from%20representative%20baselines%20of%20contemporary%20image-to-image%20translation.%20We%20release%20the%20dataset%20and%20benchmark%20to%20catalyze%20research%20on%20safe%2C%20effective%20contrast%20synthesis%2C%20with%20direct%20relevance%20to%20multi-organ%20oncology%20imaging%20workflows.%20Our%20code%20and%20dataset%20are%20publicly%20available%20at%20https%3A//github.com/YifanChen02/PMPBench.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPMPBench%253A%2520A%2520Paired%2520Multi-Modal%2520Pan-Cancer%2520Benchmark%2520for%2520Medical%2520Image%2520Synthesis%26entry.906535625%3DYifan%2520Chen%2520and%2520Fei%2520Yin%2520and%2520Hao%2520Chen%2520and%2520Jia%2520Wu%2520and%2520Chao%2520Li%26entry.1292438233%3DContrast%2520medium%2520plays%2520a%2520pivotal%2520role%2520in%2520radiological%2520imaging%252C%2520as%2520it%2520amplifies%2520lesion%2520conspicuity%2520and%2520improves%2520detection%2520for%2520the%2520diagnosis%2520of%2520tumor-related%2520diseases.%2520However%252C%2520depending%2520on%2520the%2520patient%2527s%2520health%2520condition%2520or%2520the%2520medical%2520resources%2520available%252C%2520the%2520use%2520of%2520contrast%2520medium%2520is%2520not%2520always%2520feasible.%2520Recent%2520work%2520has%2520explored%2520AI-based%2520image%2520translation%2520to%2520synthesize%2520contrast-enhanced%2520images%2520directly%2520from%2520non-contrast%2520scans%252C%2520aims%2520to%2520reduce%2520side%2520effects%2520and%2520streamlines%2520clinical%2520workflows.%2520Progress%2520in%2520this%2520direction%2520has%2520been%2520constrained%2520by%2520data%2520limitations%253A%2520%25281%2529%2520existing%2520public%2520datasets%2520focus%2520almost%2520exclusively%2520on%2520brain-related%2520paired%2520MR%2520modalities%253B%2520%25282%2529%2520other%2520collections%2520include%2520partially%2520paired%2520data%2520but%2520suffer%2520from%2520missing%2520modalities/timestamps%2520and%2520imperfect%2520spatial%2520alignment%253B%2520%25283%2529%2520explicit%2520labeling%2520of%2520CT%2520vs.%2520CTC%2520or%2520DCE%2520phases%2520is%2520often%2520absent%253B%2520%25284%2529%2520substantial%2520resources%2520remain%2520private.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520the%2520first%2520public%252C%2520fully%2520paired%252C%2520pan-cancer%2520medical%2520imaging%2520dataset%2520spanning%252011%2520human%2520organs.%2520The%2520MR%2520data%2520include%2520complete%2520dynamic%2520contrast-enhanced%2520%2528DCE%2529%2520sequences%2520covering%2520all%2520three%2520phases%2520%2528DCE1-DCE3%2529%252C%2520while%2520the%2520CT%2520data%2520provide%2520paired%2520non-contrast%2520and%2520contrast-enhanced%2520acquisitions%2520%2528CTC%2529.%2520The%2520dataset%2520is%2520curated%2520for%2520anatomical%2520correspondence%252C%2520enabling%2520rigorous%2520evaluation%2520of%25201-to-1%252C%2520N-to-1%252C%2520and%2520N-to-N%2520translation%2520settings%2520%2528e.g.%252C%2520predicting%2520DCE%2520phases%2520from%2520non-contrast%2520inputs%2529.%2520Built%2520upon%2520this%2520resource%252C%2520we%2520establish%2520a%2520comprehensive%2520benchmark.%2520We%2520report%2520results%2520from%2520representative%2520baselines%2520of%2520contemporary%2520image-to-image%2520translation.%2520We%2520release%2520the%2520dataset%2520and%2520benchmark%2520to%2520catalyze%2520research%2520on%2520safe%252C%2520effective%2520contrast%2520synthesis%252C%2520with%2520direct%2520relevance%2520to%2520multi-organ%2520oncology%2520imaging%2520workflows.%2520Our%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/YifanChen02/PMPBench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PMPBench%3A%20A%20Paired%20Multi-Modal%20Pan-Cancer%20Benchmark%20for%20Medical%20Image%20Synthesis&entry.906535625=Yifan%20Chen%20and%20Fei%20Yin%20and%20Hao%20Chen%20and%20Jia%20Wu%20and%20Chao%20Li&entry.1292438233=Contrast%20medium%20plays%20a%20pivotal%20role%20in%20radiological%20imaging%2C%20as%20it%20amplifies%20lesion%20conspicuity%20and%20improves%20detection%20for%20the%20diagnosis%20of%20tumor-related%20diseases.%20However%2C%20depending%20on%20the%20patient%27s%20health%20condition%20or%20the%20medical%20resources%20available%2C%20the%20use%20of%20contrast%20medium%20is%20not%20always%20feasible.%20Recent%20work%20has%20explored%20AI-based%20image%20translation%20to%20synthesize%20contrast-enhanced%20images%20directly%20from%20non-contrast%20scans%2C%20aims%20to%20reduce%20side%20effects%20and%20streamlines%20clinical%20workflows.%20Progress%20in%20this%20direction%20has%20been%20constrained%20by%20data%20limitations%3A%20%281%29%20existing%20public%20datasets%20focus%20almost%20exclusively%20on%20brain-related%20paired%20MR%20modalities%3B%20%282%29%20other%20collections%20include%20partially%20paired%20data%20but%20suffer%20from%20missing%20modalities/timestamps%20and%20imperfect%20spatial%20alignment%3B%20%283%29%20explicit%20labeling%20of%20CT%20vs.%20CTC%20or%20DCE%20phases%20is%20often%20absent%3B%20%284%29%20substantial%20resources%20remain%20private.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20first%20public%2C%20fully%20paired%2C%20pan-cancer%20medical%20imaging%20dataset%20spanning%2011%20human%20organs.%20The%20MR%20data%20include%20complete%20dynamic%20contrast-enhanced%20%28DCE%29%20sequences%20covering%20all%20three%20phases%20%28DCE1-DCE3%29%2C%20while%20the%20CT%20data%20provide%20paired%20non-contrast%20and%20contrast-enhanced%20acquisitions%20%28CTC%29.%20The%20dataset%20is%20curated%20for%20anatomical%20correspondence%2C%20enabling%20rigorous%20evaluation%20of%201-to-1%2C%20N-to-1%2C%20and%20N-to-N%20translation%20settings%20%28e.g.%2C%20predicting%20DCE%20phases%20from%20non-contrast%20inputs%29.%20Built%20upon%20this%20resource%2C%20we%20establish%20a%20comprehensive%20benchmark.%20We%20report%20results%20from%20representative%20baselines%20of%20contemporary%20image-to-image%20translation.%20We%20release%20the%20dataset%20and%20benchmark%20to%20catalyze%20research%20on%20safe%2C%20effective%20contrast%20synthesis%2C%20with%20direct%20relevance%20to%20multi-organ%20oncology%20imaging%20workflows.%20Our%20code%20and%20dataset%20are%20publicly%20available%20at%20https%3A//github.com/YifanChen02/PMPBench.&entry.1838667208=http%3A//arxiv.org/abs/2601.15884v1&entry.124074799=Read"},
{"title": "From Text to Image: Exploring GPT-4Vision's Potential in Advanced Radiological Analysis across Subspecialties", "author": "Felix Busch and Tianyu Han and Marcus Makowski and Daniel Truhn and Keno Bressem and Lisa Adams", "abstract": "The study evaluates and compares GPT-4 and GPT-4Vision for radiological tasks, suggesting GPT-4Vision may recognize radiological features from images, thereby enhancing its diagnostic potential over text-based descriptions.", "link": "http://arxiv.org/abs/2311.14777v2", "date": "2026-01-22", "relevancy": 2.0127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Text%20to%20Image%3A%20Exploring%20GPT-4Vision%27s%20Potential%20in%20Advanced%20Radiological%20Analysis%20across%20Subspecialties&body=Title%3A%20From%20Text%20to%20Image%3A%20Exploring%20GPT-4Vision%27s%20Potential%20in%20Advanced%20Radiological%20Analysis%20across%20Subspecialties%0AAuthor%3A%20Felix%20Busch%20and%20Tianyu%20Han%20and%20Marcus%20Makowski%20and%20Daniel%20Truhn%20and%20Keno%20Bressem%20and%20Lisa%20Adams%0AAbstract%3A%20The%20study%20evaluates%20and%20compares%20GPT-4%20and%20GPT-4Vision%20for%20radiological%20tasks%2C%20suggesting%20GPT-4Vision%20may%20recognize%20radiological%20features%20from%20images%2C%20thereby%20enhancing%20its%20diagnostic%20potential%20over%20text-based%20descriptions.%0ALink%3A%20http%3A//arxiv.org/abs/2311.14777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Text%2520to%2520Image%253A%2520Exploring%2520GPT-4Vision%2527s%2520Potential%2520in%2520Advanced%2520Radiological%2520Analysis%2520across%2520Subspecialties%26entry.906535625%3DFelix%2520Busch%2520and%2520Tianyu%2520Han%2520and%2520Marcus%2520Makowski%2520and%2520Daniel%2520Truhn%2520and%2520Keno%2520Bressem%2520and%2520Lisa%2520Adams%26entry.1292438233%3DThe%2520study%2520evaluates%2520and%2520compares%2520GPT-4%2520and%2520GPT-4Vision%2520for%2520radiological%2520tasks%252C%2520suggesting%2520GPT-4Vision%2520may%2520recognize%2520radiological%2520features%2520from%2520images%252C%2520thereby%2520enhancing%2520its%2520diagnostic%2520potential%2520over%2520text-based%2520descriptions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Text%20to%20Image%3A%20Exploring%20GPT-4Vision%27s%20Potential%20in%20Advanced%20Radiological%20Analysis%20across%20Subspecialties&entry.906535625=Felix%20Busch%20and%20Tianyu%20Han%20and%20Marcus%20Makowski%20and%20Daniel%20Truhn%20and%20Keno%20Bressem%20and%20Lisa%20Adams&entry.1292438233=The%20study%20evaluates%20and%20compares%20GPT-4%20and%20GPT-4Vision%20for%20radiological%20tasks%2C%20suggesting%20GPT-4Vision%20may%20recognize%20radiological%20features%20from%20images%2C%20thereby%20enhancing%20its%20diagnostic%20potential%20over%20text-based%20descriptions.&entry.1838667208=http%3A//arxiv.org/abs/2311.14777v2&entry.124074799=Read"},
{"title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application", "author": "Jiarui Cui and Maosong Wang and Wenqi Wu and Peiqi Li and Xianfei Pan", "abstract": "One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.", "link": "http://arxiv.org/abs/2601.16078v1", "date": "2026-01-22", "relevancy": 2.0005, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4866}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improve%20the%20autonomy%20of%20the%20SE2%283%29%20group%20based%20Extended%20Kalman%20Filter%20for%20Integrated%20Navigation%3A%20Application&body=Title%3A%20Improve%20the%20autonomy%20of%20the%20SE2%283%29%20group%20based%20Extended%20Kalman%20Filter%20for%20Integrated%20Navigation%3A%20Application%0AAuthor%3A%20Jiarui%20Cui%20and%20Maosong%20Wang%20and%20Wenqi%20Wu%20and%20Peiqi%20Li%20and%20Xianfei%20Pan%0AAbstract%3A%20One%20of%20the%20core%20advantages%20of%20SE2%283%29%20Lie%20group%20framework%20for%20navigation%20modeling%20lies%20in%20the%20autonomy%20of%20error%20propagation.%20In%20the%20previous%20paper%2C%20the%20theoretical%20analysis%20of%20autonomy%20property%20of%20navigation%20model%20in%20inertial%2C%20earth%20and%20world%20frames%20was%20given.%20A%20construction%20method%20for%20SE2%283%29%20group%20navigation%20model%20is%20proposed%20to%20improve%20the%20non-inertial%20navigation%20model%20toward%20full%20autonomy.%20This%20paper%20serves%20as%20a%20counterpart%20to%20previous%20paper%20and%20conducts%20the%20real-world%20strapdown%20inertial%20navigation%20system%20%28SINS%29/odometer%28ODO%29%20experiments%20as%20well%20as%20Monte-Carlo%20simulations%20to%20demonstrate%20the%20performance%20of%20improved%20SE2%283%29%20group%20based%20high-precision%20navigation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprove%2520the%2520autonomy%2520of%2520the%2520SE2%25283%2529%2520group%2520based%2520Extended%2520Kalman%2520Filter%2520for%2520Integrated%2520Navigation%253A%2520Application%26entry.906535625%3DJiarui%2520Cui%2520and%2520Maosong%2520Wang%2520and%2520Wenqi%2520Wu%2520and%2520Peiqi%2520Li%2520and%2520Xianfei%2520Pan%26entry.1292438233%3DOne%2520of%2520the%2520core%2520advantages%2520of%2520SE2%25283%2529%2520Lie%2520group%2520framework%2520for%2520navigation%2520modeling%2520lies%2520in%2520the%2520autonomy%2520of%2520error%2520propagation.%2520In%2520the%2520previous%2520paper%252C%2520the%2520theoretical%2520analysis%2520of%2520autonomy%2520property%2520of%2520navigation%2520model%2520in%2520inertial%252C%2520earth%2520and%2520world%2520frames%2520was%2520given.%2520A%2520construction%2520method%2520for%2520SE2%25283%2529%2520group%2520navigation%2520model%2520is%2520proposed%2520to%2520improve%2520the%2520non-inertial%2520navigation%2520model%2520toward%2520full%2520autonomy.%2520This%2520paper%2520serves%2520as%2520a%2520counterpart%2520to%2520previous%2520paper%2520and%2520conducts%2520the%2520real-world%2520strapdown%2520inertial%2520navigation%2520system%2520%2528SINS%2529/odometer%2528ODO%2529%2520experiments%2520as%2520well%2520as%2520Monte-Carlo%2520simulations%2520to%2520demonstrate%2520the%2520performance%2520of%2520improved%2520SE2%25283%2529%2520group%2520based%2520high-precision%2520navigation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improve%20the%20autonomy%20of%20the%20SE2%283%29%20group%20based%20Extended%20Kalman%20Filter%20for%20Integrated%20Navigation%3A%20Application&entry.906535625=Jiarui%20Cui%20and%20Maosong%20Wang%20and%20Wenqi%20Wu%20and%20Peiqi%20Li%20and%20Xianfei%20Pan&entry.1292438233=One%20of%20the%20core%20advantages%20of%20SE2%283%29%20Lie%20group%20framework%20for%20navigation%20modeling%20lies%20in%20the%20autonomy%20of%20error%20propagation.%20In%20the%20previous%20paper%2C%20the%20theoretical%20analysis%20of%20autonomy%20property%20of%20navigation%20model%20in%20inertial%2C%20earth%20and%20world%20frames%20was%20given.%20A%20construction%20method%20for%20SE2%283%29%20group%20navigation%20model%20is%20proposed%20to%20improve%20the%20non-inertial%20navigation%20model%20toward%20full%20autonomy.%20This%20paper%20serves%20as%20a%20counterpart%20to%20previous%20paper%20and%20conducts%20the%20real-world%20strapdown%20inertial%20navigation%20system%20%28SINS%29/odometer%28ODO%29%20experiments%20as%20well%20as%20Monte-Carlo%20simulations%20to%20demonstrate%20the%20performance%20of%20improved%20SE2%283%29%20group%20based%20high-precision%20navigation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.16078v1&entry.124074799=Read"},
{"title": "Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models", "author": "Zhen Zhang and Runhao Zeng and Sicheng Zhao and Xiping Hu", "abstract": "Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\\texttt{gate\\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \\texttt{gate\\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\\% of the parameters tuned by AffectGPT, our approach achieves 96.6\\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \\texttt{gate\\_proj} as a central architectural locus of affective modeling.", "link": "http://arxiv.org/abs/2601.15906v1", "date": "2026-01-22", "relevancy": 1.9991, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opening%20the%20Black%20Box%3A%20Preliminary%20Insights%20into%20Affective%20Modeling%20in%20Multimodal%20Foundation%20Models&body=Title%3A%20Opening%20the%20Black%20Box%3A%20Preliminary%20Insights%20into%20Affective%20Modeling%20in%20Multimodal%20Foundation%20Models%0AAuthor%3A%20Zhen%20Zhang%20and%20Runhao%20Zeng%20and%20Sicheng%20Zhao%20and%20Xiping%20Hu%0AAbstract%3A%20Understanding%20where%20and%20how%20emotions%20are%20represented%20in%20large-scale%20foundation%20models%20remains%20an%20open%20problem%2C%20particularly%20in%20multimodal%20affective%20settings.%20Despite%20the%20strong%20empirical%20performance%20of%20recent%20affective%20models%2C%20the%20internal%20architectural%20mechanisms%20that%20support%20affective%20understanding%20and%20generation%20are%20still%20poorly%20understood.%20In%20this%20work%2C%20we%20present%20a%20systematic%20mechanistic%20study%20of%20affective%20modeling%20in%20multimodal%20foundation%20models.%20Across%20multiple%20architectures%2C%20training%20strategies%2C%20and%20affective%20tasks%2C%20we%20analyze%20how%20emotion-oriented%20supervision%20reshapes%20internal%20model%20parameters.%20Our%20results%20consistently%20reveal%20a%20clear%20and%20robust%20pattern%3A%20affective%20adaptation%20does%20not%20primarily%20focus%20on%20the%20attention%20module%2C%20but%20instead%20localizes%20to%20the%20feed-forward%20gating%20projection%20%28%5Ctexttt%7Bgate%5C_proj%7D%29.%20Through%20controlled%20module%20transfer%2C%20targeted%20single-module%20adaptation%2C%20and%20destructive%20ablation%2C%20we%20further%20demonstrate%20that%20%5Ctexttt%7Bgate%5C_proj%7D%20is%20sufficient%2C%20efficient%2C%20and%20necessary%20for%20affective%20understanding%20and%20generation.%20Notably%2C%20by%20tuning%20only%20approximately%2024.5%5C%25%20of%20the%20parameters%20tuned%20by%20AffectGPT%2C%20our%20approach%20achieves%2096.6%5C%25%20of%20its%20average%20performance%20across%20eight%20affective%20tasks%2C%20highlighting%20substantial%20parameter%20efficiency.%20Together%2C%20these%20findings%20provide%20empirical%20evidence%20that%20affective%20capabilities%20in%20foundation%20models%20are%20structurally%20mediated%20by%20feed-forward%20gating%20mechanisms%20and%20identify%20%5Ctexttt%7Bgate%5C_proj%7D%20as%20a%20central%20architectural%20locus%20of%20affective%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpening%2520the%2520Black%2520Box%253A%2520Preliminary%2520Insights%2520into%2520Affective%2520Modeling%2520in%2520Multimodal%2520Foundation%2520Models%26entry.906535625%3DZhen%2520Zhang%2520and%2520Runhao%2520Zeng%2520and%2520Sicheng%2520Zhao%2520and%2520Xiping%2520Hu%26entry.1292438233%3DUnderstanding%2520where%2520and%2520how%2520emotions%2520are%2520represented%2520in%2520large-scale%2520foundation%2520models%2520remains%2520an%2520open%2520problem%252C%2520particularly%2520in%2520multimodal%2520affective%2520settings.%2520Despite%2520the%2520strong%2520empirical%2520performance%2520of%2520recent%2520affective%2520models%252C%2520the%2520internal%2520architectural%2520mechanisms%2520that%2520support%2520affective%2520understanding%2520and%2520generation%2520are%2520still%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520systematic%2520mechanistic%2520study%2520of%2520affective%2520modeling%2520in%2520multimodal%2520foundation%2520models.%2520Across%2520multiple%2520architectures%252C%2520training%2520strategies%252C%2520and%2520affective%2520tasks%252C%2520we%2520analyze%2520how%2520emotion-oriented%2520supervision%2520reshapes%2520internal%2520model%2520parameters.%2520Our%2520results%2520consistently%2520reveal%2520a%2520clear%2520and%2520robust%2520pattern%253A%2520affective%2520adaptation%2520does%2520not%2520primarily%2520focus%2520on%2520the%2520attention%2520module%252C%2520but%2520instead%2520localizes%2520to%2520the%2520feed-forward%2520gating%2520projection%2520%2528%255Ctexttt%257Bgate%255C_proj%257D%2529.%2520Through%2520controlled%2520module%2520transfer%252C%2520targeted%2520single-module%2520adaptation%252C%2520and%2520destructive%2520ablation%252C%2520we%2520further%2520demonstrate%2520that%2520%255Ctexttt%257Bgate%255C_proj%257D%2520is%2520sufficient%252C%2520efficient%252C%2520and%2520necessary%2520for%2520affective%2520understanding%2520and%2520generation.%2520Notably%252C%2520by%2520tuning%2520only%2520approximately%252024.5%255C%2525%2520of%2520the%2520parameters%2520tuned%2520by%2520AffectGPT%252C%2520our%2520approach%2520achieves%252096.6%255C%2525%2520of%2520its%2520average%2520performance%2520across%2520eight%2520affective%2520tasks%252C%2520highlighting%2520substantial%2520parameter%2520efficiency.%2520Together%252C%2520these%2520findings%2520provide%2520empirical%2520evidence%2520that%2520affective%2520capabilities%2520in%2520foundation%2520models%2520are%2520structurally%2520mediated%2520by%2520feed-forward%2520gating%2520mechanisms%2520and%2520identify%2520%255Ctexttt%257Bgate%255C_proj%257D%2520as%2520a%2520central%2520architectural%2520locus%2520of%2520affective%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opening%20the%20Black%20Box%3A%20Preliminary%20Insights%20into%20Affective%20Modeling%20in%20Multimodal%20Foundation%20Models&entry.906535625=Zhen%20Zhang%20and%20Runhao%20Zeng%20and%20Sicheng%20Zhao%20and%20Xiping%20Hu&entry.1292438233=Understanding%20where%20and%20how%20emotions%20are%20represented%20in%20large-scale%20foundation%20models%20remains%20an%20open%20problem%2C%20particularly%20in%20multimodal%20affective%20settings.%20Despite%20the%20strong%20empirical%20performance%20of%20recent%20affective%20models%2C%20the%20internal%20architectural%20mechanisms%20that%20support%20affective%20understanding%20and%20generation%20are%20still%20poorly%20understood.%20In%20this%20work%2C%20we%20present%20a%20systematic%20mechanistic%20study%20of%20affective%20modeling%20in%20multimodal%20foundation%20models.%20Across%20multiple%20architectures%2C%20training%20strategies%2C%20and%20affective%20tasks%2C%20we%20analyze%20how%20emotion-oriented%20supervision%20reshapes%20internal%20model%20parameters.%20Our%20results%20consistently%20reveal%20a%20clear%20and%20robust%20pattern%3A%20affective%20adaptation%20does%20not%20primarily%20focus%20on%20the%20attention%20module%2C%20but%20instead%20localizes%20to%20the%20feed-forward%20gating%20projection%20%28%5Ctexttt%7Bgate%5C_proj%7D%29.%20Through%20controlled%20module%20transfer%2C%20targeted%20single-module%20adaptation%2C%20and%20destructive%20ablation%2C%20we%20further%20demonstrate%20that%20%5Ctexttt%7Bgate%5C_proj%7D%20is%20sufficient%2C%20efficient%2C%20and%20necessary%20for%20affective%20understanding%20and%20generation.%20Notably%2C%20by%20tuning%20only%20approximately%2024.5%5C%25%20of%20the%20parameters%20tuned%20by%20AffectGPT%2C%20our%20approach%20achieves%2096.6%5C%25%20of%20its%20average%20performance%20across%20eight%20affective%20tasks%2C%20highlighting%20substantial%20parameter%20efficiency.%20Together%2C%20these%20findings%20provide%20empirical%20evidence%20that%20affective%20capabilities%20in%20foundation%20models%20are%20structurally%20mediated%20by%20feed-forward%20gating%20mechanisms%20and%20identify%20%5Ctexttt%7Bgate%5C_proj%7D%20as%20a%20central%20architectural%20locus%20of%20affective%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2601.15906v1&entry.124074799=Read"},
{"title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis", "author": "Jiarui Cui and Maosong Wang and Wenqi Wu and Peiqi Li and Xianfei Pan", "abstract": "One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.", "link": "http://arxiv.org/abs/2601.16062v1", "date": "2026-01-22", "relevancy": 1.997, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5224}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improve%20the%20autonomy%20of%20the%20SE2%283%29%20group%20based%20Extended%20Kalman%20Filter%20for%20Integrated%20Navigation%3A%20Theoretical%20Analysis&body=Title%3A%20Improve%20the%20autonomy%20of%20the%20SE2%283%29%20group%20based%20Extended%20Kalman%20Filter%20for%20Integrated%20Navigation%3A%20Theoretical%20Analysis%0AAuthor%3A%20Jiarui%20Cui%20and%20Maosong%20Wang%20and%20Wenqi%20Wu%20and%20Peiqi%20Li%20and%20Xianfei%20Pan%0AAbstract%3A%20One%20of%20core%20advantages%20of%20the%20SE2%283%29%20Lie%20group%20framework%20for%20navigation%20modeling%20lies%20in%20the%20autonomy%20of%20error%20propagation.%20Current%20research%20on%20Lie%20group%20based%20extended%20Kalman%20filters%20has%20demonstrated%20that%20error%20propagation%20autonomy%20holds%20in%20low-precision%20applications%2C%20such%20as%20in%20micro%20electromechanical%20system%20%28MEMS%29%20based%20integrated%20navigation%20without%20considering%20earth%20rotation%20and%20inertial%20device%20biases.%20However%2C%20in%20high-precision%20navigation%20state%20estimation%2C%20maintaining%20autonomy%20is%20extremely%20difficult%20when%20considering%20with%20earth%20rotation%20and%20inertial%20device%20biases.%20This%20paper%20presents%20the%20theoretical%20analysis%20on%20the%20autonomy%20of%20SE2%283%29%20group%20based%20high-precision%20navigation%20models%20under%20inertial%2C%20earth%20and%20world%20frame%20respectively.%20Through%20theoretical%20analysis%2C%20we%20find%20that%20the%20limitation%20of%20the%20traditional%2C%20trivial%20SE2%283%29%20group%20navigation%20modeling%20method%20is%20that%20the%20presence%20of%20Coriolis%20force%20terms%20introduced%20by%20velocity%20in%20non-inertial%20frame.%20Therefore%2C%20a%20construction%20method%20for%20SE2%283%29%20group%20navigation%20models%20is%20proposed%2C%20which%20brings%20the%20navigation%20models%20closer%20to%20full%20autonomy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprove%2520the%2520autonomy%2520of%2520the%2520SE2%25283%2529%2520group%2520based%2520Extended%2520Kalman%2520Filter%2520for%2520Integrated%2520Navigation%253A%2520Theoretical%2520Analysis%26entry.906535625%3DJiarui%2520Cui%2520and%2520Maosong%2520Wang%2520and%2520Wenqi%2520Wu%2520and%2520Peiqi%2520Li%2520and%2520Xianfei%2520Pan%26entry.1292438233%3DOne%2520of%2520core%2520advantages%2520of%2520the%2520SE2%25283%2529%2520Lie%2520group%2520framework%2520for%2520navigation%2520modeling%2520lies%2520in%2520the%2520autonomy%2520of%2520error%2520propagation.%2520Current%2520research%2520on%2520Lie%2520group%2520based%2520extended%2520Kalman%2520filters%2520has%2520demonstrated%2520that%2520error%2520propagation%2520autonomy%2520holds%2520in%2520low-precision%2520applications%252C%2520such%2520as%2520in%2520micro%2520electromechanical%2520system%2520%2528MEMS%2529%2520based%2520integrated%2520navigation%2520without%2520considering%2520earth%2520rotation%2520and%2520inertial%2520device%2520biases.%2520However%252C%2520in%2520high-precision%2520navigation%2520state%2520estimation%252C%2520maintaining%2520autonomy%2520is%2520extremely%2520difficult%2520when%2520considering%2520with%2520earth%2520rotation%2520and%2520inertial%2520device%2520biases.%2520This%2520paper%2520presents%2520the%2520theoretical%2520analysis%2520on%2520the%2520autonomy%2520of%2520SE2%25283%2529%2520group%2520based%2520high-precision%2520navigation%2520models%2520under%2520inertial%252C%2520earth%2520and%2520world%2520frame%2520respectively.%2520Through%2520theoretical%2520analysis%252C%2520we%2520find%2520that%2520the%2520limitation%2520of%2520the%2520traditional%252C%2520trivial%2520SE2%25283%2529%2520group%2520navigation%2520modeling%2520method%2520is%2520that%2520the%2520presence%2520of%2520Coriolis%2520force%2520terms%2520introduced%2520by%2520velocity%2520in%2520non-inertial%2520frame.%2520Therefore%252C%2520a%2520construction%2520method%2520for%2520SE2%25283%2529%2520group%2520navigation%2520models%2520is%2520proposed%252C%2520which%2520brings%2520the%2520navigation%2520models%2520closer%2520to%2520full%2520autonomy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improve%20the%20autonomy%20of%20the%20SE2%283%29%20group%20based%20Extended%20Kalman%20Filter%20for%20Integrated%20Navigation%3A%20Theoretical%20Analysis&entry.906535625=Jiarui%20Cui%20and%20Maosong%20Wang%20and%20Wenqi%20Wu%20and%20Peiqi%20Li%20and%20Xianfei%20Pan&entry.1292438233=One%20of%20core%20advantages%20of%20the%20SE2%283%29%20Lie%20group%20framework%20for%20navigation%20modeling%20lies%20in%20the%20autonomy%20of%20error%20propagation.%20Current%20research%20on%20Lie%20group%20based%20extended%20Kalman%20filters%20has%20demonstrated%20that%20error%20propagation%20autonomy%20holds%20in%20low-precision%20applications%2C%20such%20as%20in%20micro%20electromechanical%20system%20%28MEMS%29%20based%20integrated%20navigation%20without%20considering%20earth%20rotation%20and%20inertial%20device%20biases.%20However%2C%20in%20high-precision%20navigation%20state%20estimation%2C%20maintaining%20autonomy%20is%20extremely%20difficult%20when%20considering%20with%20earth%20rotation%20and%20inertial%20device%20biases.%20This%20paper%20presents%20the%20theoretical%20analysis%20on%20the%20autonomy%20of%20SE2%283%29%20group%20based%20high-precision%20navigation%20models%20under%20inertial%2C%20earth%20and%20world%20frame%20respectively.%20Through%20theoretical%20analysis%2C%20we%20find%20that%20the%20limitation%20of%20the%20traditional%2C%20trivial%20SE2%283%29%20group%20navigation%20modeling%20method%20is%20that%20the%20presence%20of%20Coriolis%20force%20terms%20introduced%20by%20velocity%20in%20non-inertial%20frame.%20Therefore%2C%20a%20construction%20method%20for%20SE2%283%29%20group%20navigation%20models%20is%20proposed%2C%20which%20brings%20the%20navigation%20models%20closer%20to%20full%20autonomy.&entry.1838667208=http%3A//arxiv.org/abs/2601.16062v1&entry.124074799=Read"},
{"title": "Neural Particle Automata: Learning Self-Organizing Particle Dynamics", "author": "Hyunsoo Kim and Ehsan Pajouheshgar and Sabine S\u00fcsstrunk and Wenzel Jakob and Jinah Park", "abstract": "We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.", "link": "http://arxiv.org/abs/2601.16096v1", "date": "2026-01-22", "relevancy": 1.9852, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5019}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4945}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Particle%20Automata%3A%20Learning%20Self-Organizing%20Particle%20Dynamics&body=Title%3A%20Neural%20Particle%20Automata%3A%20Learning%20Self-Organizing%20Particle%20Dynamics%0AAuthor%3A%20Hyunsoo%20Kim%20and%20Ehsan%20Pajouheshgar%20and%20Sabine%20S%C3%BCsstrunk%20and%20Wenzel%20Jakob%20and%20Jinah%20Park%0AAbstract%3A%20We%20introduce%20Neural%20Particle%20Automata%20%28NPA%29%2C%20a%20Lagrangian%20generalization%20of%20Neural%20Cellular%20Automata%20%28NCA%29%20from%20static%20lattices%20to%20dynamic%20particle%20systems.%20Unlike%20classical%20Eulerian%20NCA%20where%20cells%20are%20pinned%20to%20pixels%20or%20voxels%2C%20NPA%20model%20each%20cell%20as%20a%20particle%20with%20a%20continuous%20position%20and%20internal%20state%2C%20both%20updated%20by%20a%20shared%2C%20learnable%20neural%20rule.%20This%20particle-based%20formulation%20yields%20clear%20individuation%20of%20cells%2C%20allows%20heterogeneous%20dynamics%2C%20and%20concentrates%20computation%20only%20on%20regions%20where%20activity%20is%20present.%20At%20the%20same%20time%2C%20particle%20systems%20pose%20challenges%3A%20neighborhoods%20are%20dynamic%2C%20and%20a%20naive%20implementation%20of%20local%20interactions%20scale%20quadratically%20with%20the%20number%20of%20particles.%20We%20address%20these%20challenges%20by%20replacing%20grid-based%20neighborhood%20perception%20with%20differentiable%20Smoothed%20Particle%20Hydrodynamics%20%28SPH%29%20operators%20backed%20by%20memory-efficient%2C%20CUDA-accelerated%20kernels%2C%20enabling%20scalable%20end-to-end%20training.%20Across%20tasks%20including%20morphogenesis%2C%20point-cloud%20classification%2C%20and%20particle-based%20texture%20synthesis%2C%20we%20show%20that%20NPA%20retain%20key%20NCA%20behaviors%20such%20as%20robustness%20and%20self-regeneration%2C%20while%20enabling%20new%20behaviors%20specific%20to%20particle%20systems.%20Together%2C%20these%20results%20position%20NPA%20as%20a%20compact%20neural%20model%20for%20learning%20self-organizing%20particle%20dynamics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Particle%2520Automata%253A%2520Learning%2520Self-Organizing%2520Particle%2520Dynamics%26entry.906535625%3DHyunsoo%2520Kim%2520and%2520Ehsan%2520Pajouheshgar%2520and%2520Sabine%2520S%25C3%25BCsstrunk%2520and%2520Wenzel%2520Jakob%2520and%2520Jinah%2520Park%26entry.1292438233%3DWe%2520introduce%2520Neural%2520Particle%2520Automata%2520%2528NPA%2529%252C%2520a%2520Lagrangian%2520generalization%2520of%2520Neural%2520Cellular%2520Automata%2520%2528NCA%2529%2520from%2520static%2520lattices%2520to%2520dynamic%2520particle%2520systems.%2520Unlike%2520classical%2520Eulerian%2520NCA%2520where%2520cells%2520are%2520pinned%2520to%2520pixels%2520or%2520voxels%252C%2520NPA%2520model%2520each%2520cell%2520as%2520a%2520particle%2520with%2520a%2520continuous%2520position%2520and%2520internal%2520state%252C%2520both%2520updated%2520by%2520a%2520shared%252C%2520learnable%2520neural%2520rule.%2520This%2520particle-based%2520formulation%2520yields%2520clear%2520individuation%2520of%2520cells%252C%2520allows%2520heterogeneous%2520dynamics%252C%2520and%2520concentrates%2520computation%2520only%2520on%2520regions%2520where%2520activity%2520is%2520present.%2520At%2520the%2520same%2520time%252C%2520particle%2520systems%2520pose%2520challenges%253A%2520neighborhoods%2520are%2520dynamic%252C%2520and%2520a%2520naive%2520implementation%2520of%2520local%2520interactions%2520scale%2520quadratically%2520with%2520the%2520number%2520of%2520particles.%2520We%2520address%2520these%2520challenges%2520by%2520replacing%2520grid-based%2520neighborhood%2520perception%2520with%2520differentiable%2520Smoothed%2520Particle%2520Hydrodynamics%2520%2528SPH%2529%2520operators%2520backed%2520by%2520memory-efficient%252C%2520CUDA-accelerated%2520kernels%252C%2520enabling%2520scalable%2520end-to-end%2520training.%2520Across%2520tasks%2520including%2520morphogenesis%252C%2520point-cloud%2520classification%252C%2520and%2520particle-based%2520texture%2520synthesis%252C%2520we%2520show%2520that%2520NPA%2520retain%2520key%2520NCA%2520behaviors%2520such%2520as%2520robustness%2520and%2520self-regeneration%252C%2520while%2520enabling%2520new%2520behaviors%2520specific%2520to%2520particle%2520systems.%2520Together%252C%2520these%2520results%2520position%2520NPA%2520as%2520a%2520compact%2520neural%2520model%2520for%2520learning%2520self-organizing%2520particle%2520dynamics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Particle%20Automata%3A%20Learning%20Self-Organizing%20Particle%20Dynamics&entry.906535625=Hyunsoo%20Kim%20and%20Ehsan%20Pajouheshgar%20and%20Sabine%20S%C3%BCsstrunk%20and%20Wenzel%20Jakob%20and%20Jinah%20Park&entry.1292438233=We%20introduce%20Neural%20Particle%20Automata%20%28NPA%29%2C%20a%20Lagrangian%20generalization%20of%20Neural%20Cellular%20Automata%20%28NCA%29%20from%20static%20lattices%20to%20dynamic%20particle%20systems.%20Unlike%20classical%20Eulerian%20NCA%20where%20cells%20are%20pinned%20to%20pixels%20or%20voxels%2C%20NPA%20model%20each%20cell%20as%20a%20particle%20with%20a%20continuous%20position%20and%20internal%20state%2C%20both%20updated%20by%20a%20shared%2C%20learnable%20neural%20rule.%20This%20particle-based%20formulation%20yields%20clear%20individuation%20of%20cells%2C%20allows%20heterogeneous%20dynamics%2C%20and%20concentrates%20computation%20only%20on%20regions%20where%20activity%20is%20present.%20At%20the%20same%20time%2C%20particle%20systems%20pose%20challenges%3A%20neighborhoods%20are%20dynamic%2C%20and%20a%20naive%20implementation%20of%20local%20interactions%20scale%20quadratically%20with%20the%20number%20of%20particles.%20We%20address%20these%20challenges%20by%20replacing%20grid-based%20neighborhood%20perception%20with%20differentiable%20Smoothed%20Particle%20Hydrodynamics%20%28SPH%29%20operators%20backed%20by%20memory-efficient%2C%20CUDA-accelerated%20kernels%2C%20enabling%20scalable%20end-to-end%20training.%20Across%20tasks%20including%20morphogenesis%2C%20point-cloud%20classification%2C%20and%20particle-based%20texture%20synthesis%2C%20we%20show%20that%20NPA%20retain%20key%20NCA%20behaviors%20such%20as%20robustness%20and%20self-regeneration%2C%20while%20enabling%20new%20behaviors%20specific%20to%20particle%20systems.%20Together%2C%20these%20results%20position%20NPA%20as%20a%20compact%20neural%20model%20for%20learning%20self-organizing%20particle%20dynamics.&entry.1838667208=http%3A//arxiv.org/abs/2601.16096v1&entry.124074799=Read"},
{"title": "Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box Variational Inference", "author": "Baojun Che and Yifan Chen and Daniel Zhengyu Huang and Xinying Mao and Weijie Wang", "abstract": "Black-box variational inference (BBVI) with Gaussian mixture families offers a flexible approach for approximating complex posterior distributions without requiring gradients of the target density. However, standard numerical optimization methods often suffer from instability and inefficiency. We develop a stable and efficient framework that combines three key components: (1) affine-invariant preconditioning via natural gradient formulations, (2) an exponential integrator that unconditionally preserves the positive definiteness of covariance matrices, and (3) adaptive time stepping to ensure stability and to accommodate distinct warm-up and convergence phases. The proposed approach has natural connections to manifold optimization and mirror descent. For Gaussian posteriors, we prove exponential convergence in the noise-free setting and almost-sure convergence under Monte Carlo estimation, rigorously justifying the necessity of adaptive time stepping. Numerical experiments on multimodal distributions, Neal's multiscale funnel, and a PDE-based Bayesian inverse problem for Darcy flow demonstrate the effectiveness of the proposed method.", "link": "http://arxiv.org/abs/2601.14855v2", "date": "2026-01-22", "relevancy": 1.9844, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5047}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4901}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Exponential%20Integration%20for%20Stable%20Gaussian%20Mixture%20Black-Box%20Variational%20Inference&body=Title%3A%20Adaptive%20Exponential%20Integration%20for%20Stable%20Gaussian%20Mixture%20Black-Box%20Variational%20Inference%0AAuthor%3A%20Baojun%20Che%20and%20Yifan%20Chen%20and%20Daniel%20Zhengyu%20Huang%20and%20Xinying%20Mao%20and%20Weijie%20Wang%0AAbstract%3A%20Black-box%20variational%20inference%20%28BBVI%29%20with%20Gaussian%20mixture%20families%20offers%20a%20flexible%20approach%20for%20approximating%20complex%20posterior%20distributions%20without%20requiring%20gradients%20of%20the%20target%20density.%20However%2C%20standard%20numerical%20optimization%20methods%20often%20suffer%20from%20instability%20and%20inefficiency.%20We%20develop%20a%20stable%20and%20efficient%20framework%20that%20combines%20three%20key%20components%3A%20%281%29%20affine-invariant%20preconditioning%20via%20natural%20gradient%20formulations%2C%20%282%29%20an%20exponential%20integrator%20that%20unconditionally%20preserves%20the%20positive%20definiteness%20of%20covariance%20matrices%2C%20and%20%283%29%20adaptive%20time%20stepping%20to%20ensure%20stability%20and%20to%20accommodate%20distinct%20warm-up%20and%20convergence%20phases.%20The%20proposed%20approach%20has%20natural%20connections%20to%20manifold%20optimization%20and%20mirror%20descent.%20For%20Gaussian%20posteriors%2C%20we%20prove%20exponential%20convergence%20in%20the%20noise-free%20setting%20and%20almost-sure%20convergence%20under%20Monte%20Carlo%20estimation%2C%20rigorously%20justifying%20the%20necessity%20of%20adaptive%20time%20stepping.%20Numerical%20experiments%20on%20multimodal%20distributions%2C%20Neal%27s%20multiscale%20funnel%2C%20and%20a%20PDE-based%20Bayesian%20inverse%20problem%20for%20Darcy%20flow%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Exponential%2520Integration%2520for%2520Stable%2520Gaussian%2520Mixture%2520Black-Box%2520Variational%2520Inference%26entry.906535625%3DBaojun%2520Che%2520and%2520Yifan%2520Chen%2520and%2520Daniel%2520Zhengyu%2520Huang%2520and%2520Xinying%2520Mao%2520and%2520Weijie%2520Wang%26entry.1292438233%3DBlack-box%2520variational%2520inference%2520%2528BBVI%2529%2520with%2520Gaussian%2520mixture%2520families%2520offers%2520a%2520flexible%2520approach%2520for%2520approximating%2520complex%2520posterior%2520distributions%2520without%2520requiring%2520gradients%2520of%2520the%2520target%2520density.%2520However%252C%2520standard%2520numerical%2520optimization%2520methods%2520often%2520suffer%2520from%2520instability%2520and%2520inefficiency.%2520We%2520develop%2520a%2520stable%2520and%2520efficient%2520framework%2520that%2520combines%2520three%2520key%2520components%253A%2520%25281%2529%2520affine-invariant%2520preconditioning%2520via%2520natural%2520gradient%2520formulations%252C%2520%25282%2529%2520an%2520exponential%2520integrator%2520that%2520unconditionally%2520preserves%2520the%2520positive%2520definiteness%2520of%2520covariance%2520matrices%252C%2520and%2520%25283%2529%2520adaptive%2520time%2520stepping%2520to%2520ensure%2520stability%2520and%2520to%2520accommodate%2520distinct%2520warm-up%2520and%2520convergence%2520phases.%2520The%2520proposed%2520approach%2520has%2520natural%2520connections%2520to%2520manifold%2520optimization%2520and%2520mirror%2520descent.%2520For%2520Gaussian%2520posteriors%252C%2520we%2520prove%2520exponential%2520convergence%2520in%2520the%2520noise-free%2520setting%2520and%2520almost-sure%2520convergence%2520under%2520Monte%2520Carlo%2520estimation%252C%2520rigorously%2520justifying%2520the%2520necessity%2520of%2520adaptive%2520time%2520stepping.%2520Numerical%2520experiments%2520on%2520multimodal%2520distributions%252C%2520Neal%2527s%2520multiscale%2520funnel%252C%2520and%2520a%2520PDE-based%2520Bayesian%2520inverse%2520problem%2520for%2520Darcy%2520flow%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Exponential%20Integration%20for%20Stable%20Gaussian%20Mixture%20Black-Box%20Variational%20Inference&entry.906535625=Baojun%20Che%20and%20Yifan%20Chen%20and%20Daniel%20Zhengyu%20Huang%20and%20Xinying%20Mao%20and%20Weijie%20Wang&entry.1292438233=Black-box%20variational%20inference%20%28BBVI%29%20with%20Gaussian%20mixture%20families%20offers%20a%20flexible%20approach%20for%20approximating%20complex%20posterior%20distributions%20without%20requiring%20gradients%20of%20the%20target%20density.%20However%2C%20standard%20numerical%20optimization%20methods%20often%20suffer%20from%20instability%20and%20inefficiency.%20We%20develop%20a%20stable%20and%20efficient%20framework%20that%20combines%20three%20key%20components%3A%20%281%29%20affine-invariant%20preconditioning%20via%20natural%20gradient%20formulations%2C%20%282%29%20an%20exponential%20integrator%20that%20unconditionally%20preserves%20the%20positive%20definiteness%20of%20covariance%20matrices%2C%20and%20%283%29%20adaptive%20time%20stepping%20to%20ensure%20stability%20and%20to%20accommodate%20distinct%20warm-up%20and%20convergence%20phases.%20The%20proposed%20approach%20has%20natural%20connections%20to%20manifold%20optimization%20and%20mirror%20descent.%20For%20Gaussian%20posteriors%2C%20we%20prove%20exponential%20convergence%20in%20the%20noise-free%20setting%20and%20almost-sure%20convergence%20under%20Monte%20Carlo%20estimation%2C%20rigorously%20justifying%20the%20necessity%20of%20adaptive%20time%20stepping.%20Numerical%20experiments%20on%20multimodal%20distributions%2C%20Neal%27s%20multiscale%20funnel%2C%20and%20a%20PDE-based%20Bayesian%20inverse%20problem%20for%20Darcy%20flow%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.&entry.1838667208=http%3A//arxiv.org/abs/2601.14855v2&entry.124074799=Read"},
{"title": "Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning", "author": "Jianqi Zhang and Jingyao Wang and Wenwen Qiang and Fanjiang Xu and Changwen Zheng", "abstract": "The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.", "link": "http://arxiv.org/abs/2601.07903v3", "date": "2026-01-22", "relevancy": 1.9812, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Large%20Language%20Models%20for%20Time-Series%20Forecasting%20via%20Vector-Injected%20In-Context%20Learning&body=Title%3A%20Enhancing%20Large%20Language%20Models%20for%20Time-Series%20Forecasting%20via%20Vector-Injected%20In-Context%20Learning%0AAuthor%3A%20Jianqi%20Zhang%20and%20Jingyao%20Wang%20and%20Wenwen%20Qiang%20and%20Fanjiang%20Xu%20and%20Changwen%20Zheng%0AAbstract%3A%20The%20World%20Wide%20Web%20needs%20reliable%20predictive%20capabilities%20to%20respond%20to%20changes%20in%20user%20behavior%20and%20usage%20patterns.%20Time%20series%20forecasting%20%28TSF%29%20is%20a%20key%20means%20to%20achieve%20this%20goal.%20In%20recent%20years%2C%20the%20large%20language%20models%20%28LLMs%29%20for%20TSF%20%28LLM4TSF%29%20have%20achieved%20good%20performance.%20However%2C%20there%20is%20a%20significant%20difference%20between%20pretraining%20corpora%20and%20time%20series%20data%2C%20making%20it%20hard%20to%20guarantee%20forecasting%20quality%20when%20directly%20applying%20LLMs%20to%20TSF%3B%20fine-tuning%20LLMs%20can%20mitigate%20this%20issue%2C%20but%20often%20incurs%20substantial%20computational%20overhead.%20Thus%2C%20LLM4TSF%20faces%20a%20dual%20challenge%20of%20prediction%20performance%20and%20compute%20overhead.%20To%20address%20this%2C%20we%20aim%20to%20explore%20a%20method%20for%20improving%20the%20forecasting%20performance%20of%20LLM4TSF%20while%20freezing%20all%20LLM%20parameters%20to%20reduce%20computational%20overhead.%20Inspired%20by%20in-context%20learning%20%28ICL%29%2C%20we%20propose%20LVICL.%20LVICL%20uses%20our%20vector-injected%20ICL%20to%20inject%20example%20information%20into%20a%20frozen%20LLM%2C%20eliciting%20its%20in-context%20learning%20ability%20and%20thereby%20enhancing%20its%20performance%20on%20the%20example-related%20task%20%28i.e.%2C%20TSF%29.%20Specifically%2C%20we%20first%20use%20the%20LLM%20together%20with%20a%20learnable%20context%20vector%20adapter%20to%20extract%20a%20context%20vector%20from%20multiple%20examples%20adaptively.%20This%20vector%20contains%20compressed%2C%20example-related%20information.%20Subsequently%2C%20during%20the%20forward%20pass%2C%20we%20inject%20this%20vector%20into%20every%20layer%20of%20the%20LLM%20to%20improve%20forecasting%20performance.%20Compared%20with%20conventional%20ICL%20that%20adds%20examples%20into%20the%20prompt%2C%20our%20vector-injected%20ICL%20does%20not%20increase%20prompt%20length%3B%20moreover%2C%20adaptively%20deriving%20a%20context%20vector%20from%20examples%20suppresses%20components%20harmful%20to%20forecasting%2C%20thereby%20improving%20model%20performance.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07903v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Large%2520Language%2520Models%2520for%2520Time-Series%2520Forecasting%2520via%2520Vector-Injected%2520In-Context%2520Learning%26entry.906535625%3DJianqi%2520Zhang%2520and%2520Jingyao%2520Wang%2520and%2520Wenwen%2520Qiang%2520and%2520Fanjiang%2520Xu%2520and%2520Changwen%2520Zheng%26entry.1292438233%3DThe%2520World%2520Wide%2520Web%2520needs%2520reliable%2520predictive%2520capabilities%2520to%2520respond%2520to%2520changes%2520in%2520user%2520behavior%2520and%2520usage%2520patterns.%2520Time%2520series%2520forecasting%2520%2528TSF%2529%2520is%2520a%2520key%2520means%2520to%2520achieve%2520this%2520goal.%2520In%2520recent%2520years%252C%2520the%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520TSF%2520%2528LLM4TSF%2529%2520have%2520achieved%2520good%2520performance.%2520However%252C%2520there%2520is%2520a%2520significant%2520difference%2520between%2520pretraining%2520corpora%2520and%2520time%2520series%2520data%252C%2520making%2520it%2520hard%2520to%2520guarantee%2520forecasting%2520quality%2520when%2520directly%2520applying%2520LLMs%2520to%2520TSF%253B%2520fine-tuning%2520LLMs%2520can%2520mitigate%2520this%2520issue%252C%2520but%2520often%2520incurs%2520substantial%2520computational%2520overhead.%2520Thus%252C%2520LLM4TSF%2520faces%2520a%2520dual%2520challenge%2520of%2520prediction%2520performance%2520and%2520compute%2520overhead.%2520To%2520address%2520this%252C%2520we%2520aim%2520to%2520explore%2520a%2520method%2520for%2520improving%2520the%2520forecasting%2520performance%2520of%2520LLM4TSF%2520while%2520freezing%2520all%2520LLM%2520parameters%2520to%2520reduce%2520computational%2520overhead.%2520Inspired%2520by%2520in-context%2520learning%2520%2528ICL%2529%252C%2520we%2520propose%2520LVICL.%2520LVICL%2520uses%2520our%2520vector-injected%2520ICL%2520to%2520inject%2520example%2520information%2520into%2520a%2520frozen%2520LLM%252C%2520eliciting%2520its%2520in-context%2520learning%2520ability%2520and%2520thereby%2520enhancing%2520its%2520performance%2520on%2520the%2520example-related%2520task%2520%2528i.e.%252C%2520TSF%2529.%2520Specifically%252C%2520we%2520first%2520use%2520the%2520LLM%2520together%2520with%2520a%2520learnable%2520context%2520vector%2520adapter%2520to%2520extract%2520a%2520context%2520vector%2520from%2520multiple%2520examples%2520adaptively.%2520This%2520vector%2520contains%2520compressed%252C%2520example-related%2520information.%2520Subsequently%252C%2520during%2520the%2520forward%2520pass%252C%2520we%2520inject%2520this%2520vector%2520into%2520every%2520layer%2520of%2520the%2520LLM%2520to%2520improve%2520forecasting%2520performance.%2520Compared%2520with%2520conventional%2520ICL%2520that%2520adds%2520examples%2520into%2520the%2520prompt%252C%2520our%2520vector-injected%2520ICL%2520does%2520not%2520increase%2520prompt%2520length%253B%2520moreover%252C%2520adaptively%2520deriving%2520a%2520context%2520vector%2520from%2520examples%2520suppresses%2520components%2520harmful%2520to%2520forecasting%252C%2520thereby%2520improving%2520model%2520performance.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07903v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Large%20Language%20Models%20for%20Time-Series%20Forecasting%20via%20Vector-Injected%20In-Context%20Learning&entry.906535625=Jianqi%20Zhang%20and%20Jingyao%20Wang%20and%20Wenwen%20Qiang%20and%20Fanjiang%20Xu%20and%20Changwen%20Zheng&entry.1292438233=The%20World%20Wide%20Web%20needs%20reliable%20predictive%20capabilities%20to%20respond%20to%20changes%20in%20user%20behavior%20and%20usage%20patterns.%20Time%20series%20forecasting%20%28TSF%29%20is%20a%20key%20means%20to%20achieve%20this%20goal.%20In%20recent%20years%2C%20the%20large%20language%20models%20%28LLMs%29%20for%20TSF%20%28LLM4TSF%29%20have%20achieved%20good%20performance.%20However%2C%20there%20is%20a%20significant%20difference%20between%20pretraining%20corpora%20and%20time%20series%20data%2C%20making%20it%20hard%20to%20guarantee%20forecasting%20quality%20when%20directly%20applying%20LLMs%20to%20TSF%3B%20fine-tuning%20LLMs%20can%20mitigate%20this%20issue%2C%20but%20often%20incurs%20substantial%20computational%20overhead.%20Thus%2C%20LLM4TSF%20faces%20a%20dual%20challenge%20of%20prediction%20performance%20and%20compute%20overhead.%20To%20address%20this%2C%20we%20aim%20to%20explore%20a%20method%20for%20improving%20the%20forecasting%20performance%20of%20LLM4TSF%20while%20freezing%20all%20LLM%20parameters%20to%20reduce%20computational%20overhead.%20Inspired%20by%20in-context%20learning%20%28ICL%29%2C%20we%20propose%20LVICL.%20LVICL%20uses%20our%20vector-injected%20ICL%20to%20inject%20example%20information%20into%20a%20frozen%20LLM%2C%20eliciting%20its%20in-context%20learning%20ability%20and%20thereby%20enhancing%20its%20performance%20on%20the%20example-related%20task%20%28i.e.%2C%20TSF%29.%20Specifically%2C%20we%20first%20use%20the%20LLM%20together%20with%20a%20learnable%20context%20vector%20adapter%20to%20extract%20a%20context%20vector%20from%20multiple%20examples%20adaptively.%20This%20vector%20contains%20compressed%2C%20example-related%20information.%20Subsequently%2C%20during%20the%20forward%20pass%2C%20we%20inject%20this%20vector%20into%20every%20layer%20of%20the%20LLM%20to%20improve%20forecasting%20performance.%20Compared%20with%20conventional%20ICL%20that%20adds%20examples%20into%20the%20prompt%2C%20our%20vector-injected%20ICL%20does%20not%20increase%20prompt%20length%3B%20moreover%2C%20adaptively%20deriving%20a%20context%20vector%20from%20examples%20suppresses%20components%20harmful%20to%20forecasting%2C%20thereby%20improving%20model%20performance.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2601.07903v3&entry.124074799=Read"},
{"title": "Words to Describe What I'm Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning", "author": "Kellie Yu Hui Sim and Pin Sym Foong and Chenyu Zhao and Melanie Yi Ning Quek and Swarangi Subodh Mehta and Kenny Tsu Wei Choo", "abstract": "Loss of decisional capacity, coupled with the increasing absence of reliable human proxies, raises urgent questions about how individuals' values can be represented in Advance Care Planning (ACP). To probe this fraught design space of high-risk, high-subjectivity decision support, we built an experience prototype (\\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal ACP proxy. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings show a surprising 86.7\\% agreement with \\acpagent{}, arguing for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We propose that the key areas of future risk that must be addressed are the moderation of users' expectations and designing accountability and oversight over agent deployment and cutoffs.", "link": "http://arxiv.org/abs/2512.11276v2", "date": "2026-01-22", "relevancy": 1.9725, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.497}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4957}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Words%20to%20Describe%20What%20I%27m%20Feeling%3A%20Exploring%20the%20Potential%20of%20AI%20Agents%20for%20High%20Subjectivity%20Decisions%20in%20Advance%20Care%20Planning&body=Title%3A%20Words%20to%20Describe%20What%20I%27m%20Feeling%3A%20Exploring%20the%20Potential%20of%20AI%20Agents%20for%20High%20Subjectivity%20Decisions%20in%20Advance%20Care%20Planning%0AAuthor%3A%20Kellie%20Yu%20Hui%20Sim%20and%20Pin%20Sym%20Foong%20and%20Chenyu%20Zhao%20and%20Melanie%20Yi%20Ning%20Quek%20and%20Swarangi%20Subodh%20Mehta%20and%20Kenny%20Tsu%20Wei%20Choo%0AAbstract%3A%20Loss%20of%20decisional%20capacity%2C%20coupled%20with%20the%20increasing%20absence%20of%20reliable%20human%20proxies%2C%20raises%20urgent%20questions%20about%20how%20individuals%27%20values%20can%20be%20represented%20in%20Advance%20Care%20Planning%20%28ACP%29.%20To%20probe%20this%20fraught%20design%20space%20of%20high-risk%2C%20high-subjectivity%20decision%20support%2C%20we%20built%20an%20experience%20prototype%20%28%5Cacpagent%7B%7D%29%20and%20asked%2015%20participants%20in%204%20workshops%20to%20train%20it%20to%20be%20their%20personal%20ACP%20proxy.%20We%20analysed%20their%20coping%20strategies%20and%20feature%20requests%20and%20mapped%20the%20results%20onto%20axes%20of%20agent%20autonomy%20and%20human%20control.%20Our%20findings%20show%20a%20surprising%2086.7%5C%25%20agreement%20with%20%5Cacpagent%7B%7D%2C%20arguing%20for%20a%20potential%20new%20role%20of%20AI%20in%20ACP%20where%20agents%20act%20as%20personal%20advocates%20for%20individuals%2C%20building%20mutual%20intelligibility%20over%20time.%20We%20propose%20that%20the%20key%20areas%20of%20future%20risk%20that%20must%20be%20addressed%20are%20the%20moderation%20of%20users%27%20expectations%20and%20designing%20accountability%20and%20oversight%20over%20agent%20deployment%20and%20cutoffs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWords%2520to%2520Describe%2520What%2520I%2527m%2520Feeling%253A%2520Exploring%2520the%2520Potential%2520of%2520AI%2520Agents%2520for%2520High%2520Subjectivity%2520Decisions%2520in%2520Advance%2520Care%2520Planning%26entry.906535625%3DKellie%2520Yu%2520Hui%2520Sim%2520and%2520Pin%2520Sym%2520Foong%2520and%2520Chenyu%2520Zhao%2520and%2520Melanie%2520Yi%2520Ning%2520Quek%2520and%2520Swarangi%2520Subodh%2520Mehta%2520and%2520Kenny%2520Tsu%2520Wei%2520Choo%26entry.1292438233%3DLoss%2520of%2520decisional%2520capacity%252C%2520coupled%2520with%2520the%2520increasing%2520absence%2520of%2520reliable%2520human%2520proxies%252C%2520raises%2520urgent%2520questions%2520about%2520how%2520individuals%2527%2520values%2520can%2520be%2520represented%2520in%2520Advance%2520Care%2520Planning%2520%2528ACP%2529.%2520To%2520probe%2520this%2520fraught%2520design%2520space%2520of%2520high-risk%252C%2520high-subjectivity%2520decision%2520support%252C%2520we%2520built%2520an%2520experience%2520prototype%2520%2528%255Cacpagent%257B%257D%2529%2520and%2520asked%252015%2520participants%2520in%25204%2520workshops%2520to%2520train%2520it%2520to%2520be%2520their%2520personal%2520ACP%2520proxy.%2520We%2520analysed%2520their%2520coping%2520strategies%2520and%2520feature%2520requests%2520and%2520mapped%2520the%2520results%2520onto%2520axes%2520of%2520agent%2520autonomy%2520and%2520human%2520control.%2520Our%2520findings%2520show%2520a%2520surprising%252086.7%255C%2525%2520agreement%2520with%2520%255Cacpagent%257B%257D%252C%2520arguing%2520for%2520a%2520potential%2520new%2520role%2520of%2520AI%2520in%2520ACP%2520where%2520agents%2520act%2520as%2520personal%2520advocates%2520for%2520individuals%252C%2520building%2520mutual%2520intelligibility%2520over%2520time.%2520We%2520propose%2520that%2520the%2520key%2520areas%2520of%2520future%2520risk%2520that%2520must%2520be%2520addressed%2520are%2520the%2520moderation%2520of%2520users%2527%2520expectations%2520and%2520designing%2520accountability%2520and%2520oversight%2520over%2520agent%2520deployment%2520and%2520cutoffs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Words%20to%20Describe%20What%20I%27m%20Feeling%3A%20Exploring%20the%20Potential%20of%20AI%20Agents%20for%20High%20Subjectivity%20Decisions%20in%20Advance%20Care%20Planning&entry.906535625=Kellie%20Yu%20Hui%20Sim%20and%20Pin%20Sym%20Foong%20and%20Chenyu%20Zhao%20and%20Melanie%20Yi%20Ning%20Quek%20and%20Swarangi%20Subodh%20Mehta%20and%20Kenny%20Tsu%20Wei%20Choo&entry.1292438233=Loss%20of%20decisional%20capacity%2C%20coupled%20with%20the%20increasing%20absence%20of%20reliable%20human%20proxies%2C%20raises%20urgent%20questions%20about%20how%20individuals%27%20values%20can%20be%20represented%20in%20Advance%20Care%20Planning%20%28ACP%29.%20To%20probe%20this%20fraught%20design%20space%20of%20high-risk%2C%20high-subjectivity%20decision%20support%2C%20we%20built%20an%20experience%20prototype%20%28%5Cacpagent%7B%7D%29%20and%20asked%2015%20participants%20in%204%20workshops%20to%20train%20it%20to%20be%20their%20personal%20ACP%20proxy.%20We%20analysed%20their%20coping%20strategies%20and%20feature%20requests%20and%20mapped%20the%20results%20onto%20axes%20of%20agent%20autonomy%20and%20human%20control.%20Our%20findings%20show%20a%20surprising%2086.7%5C%25%20agreement%20with%20%5Cacpagent%7B%7D%2C%20arguing%20for%20a%20potential%20new%20role%20of%20AI%20in%20ACP%20where%20agents%20act%20as%20personal%20advocates%20for%20individuals%2C%20building%20mutual%20intelligibility%20over%20time.%20We%20propose%20that%20the%20key%20areas%20of%20future%20risk%20that%20must%20be%20addressed%20are%20the%20moderation%20of%20users%27%20expectations%20and%20designing%20accountability%20and%20oversight%20over%20agent%20deployment%20and%20cutoffs.&entry.1838667208=http%3A//arxiv.org/abs/2512.11276v2&entry.124074799=Read"},
{"title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations", "author": "Patrick Altmeyer and Aleksander Buszydlik and Arie van Deursen and Cynthia C. S. Liem", "abstract": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.", "link": "http://arxiv.org/abs/2601.16205v1", "date": "2026-01-22", "relevancy": 1.9724, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5327}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4666}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Training%3A%20Teaching%20Models%20Plausible%20and%20Actionable%20Explanations&body=Title%3A%20Counterfactual%20Training%3A%20Teaching%20Models%20Plausible%20and%20Actionable%20Explanations%0AAuthor%3A%20Patrick%20Altmeyer%20and%20Aleksander%20Buszydlik%20and%20Arie%20van%20Deursen%20and%20Cynthia%20C.%20S.%20Liem%0AAbstract%3A%20We%20propose%20a%20novel%20training%20regime%20termed%20counterfactual%20training%20that%20leverages%20counterfactual%20explanations%20to%20increase%20the%20explanatory%20capacity%20of%20models.%20Counterfactual%20explanations%20have%20emerged%20as%20a%20popular%20post-hoc%20explanation%20method%20for%20opaque%20machine%20learning%20models%3A%20they%20inform%20how%20factual%20inputs%20would%20need%20to%20change%20in%20order%20for%20a%20model%20to%20produce%20some%20desired%20output.%20To%20be%20useful%20in%20real-world%20decision-making%20systems%2C%20counterfactuals%20should%20be%20plausible%20with%20respect%20to%20the%20underlying%20data%20and%20actionable%20with%20respect%20to%20the%20feature%20mutability%20constraints.%20Much%20existing%20research%20has%20therefore%20focused%20on%20developing%20post-hoc%20methods%20to%20generate%20counterfactuals%20that%20meet%20these%20desiderata.%20In%20this%20work%2C%20we%20instead%20hold%20models%20directly%20accountable%20for%20the%20desired%20end%20goal%3A%20counterfactual%20training%20employs%20counterfactuals%20during%20the%20training%20phase%20to%20minimize%20the%20divergence%20between%20learned%20representations%20and%20plausible%2C%20actionable%20explanations.%20We%20demonstrate%20empirically%20and%20theoretically%20that%20our%20proposed%20method%20facilitates%20training%20models%20that%20deliver%20inherently%20desirable%20counterfactual%20explanations%20and%20additionally%20exhibit%20improved%20adversarial%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Training%253A%2520Teaching%2520Models%2520Plausible%2520and%2520Actionable%2520Explanations%26entry.906535625%3DPatrick%2520Altmeyer%2520and%2520Aleksander%2520Buszydlik%2520and%2520Arie%2520van%2520Deursen%2520and%2520Cynthia%2520C.%2520S.%2520Liem%26entry.1292438233%3DWe%2520propose%2520a%2520novel%2520training%2520regime%2520termed%2520counterfactual%2520training%2520that%2520leverages%2520counterfactual%2520explanations%2520to%2520increase%2520the%2520explanatory%2520capacity%2520of%2520models.%2520Counterfactual%2520explanations%2520have%2520emerged%2520as%2520a%2520popular%2520post-hoc%2520explanation%2520method%2520for%2520opaque%2520machine%2520learning%2520models%253A%2520they%2520inform%2520how%2520factual%2520inputs%2520would%2520need%2520to%2520change%2520in%2520order%2520for%2520a%2520model%2520to%2520produce%2520some%2520desired%2520output.%2520To%2520be%2520useful%2520in%2520real-world%2520decision-making%2520systems%252C%2520counterfactuals%2520should%2520be%2520plausible%2520with%2520respect%2520to%2520the%2520underlying%2520data%2520and%2520actionable%2520with%2520respect%2520to%2520the%2520feature%2520mutability%2520constraints.%2520Much%2520existing%2520research%2520has%2520therefore%2520focused%2520on%2520developing%2520post-hoc%2520methods%2520to%2520generate%2520counterfactuals%2520that%2520meet%2520these%2520desiderata.%2520In%2520this%2520work%252C%2520we%2520instead%2520hold%2520models%2520directly%2520accountable%2520for%2520the%2520desired%2520end%2520goal%253A%2520counterfactual%2520training%2520employs%2520counterfactuals%2520during%2520the%2520training%2520phase%2520to%2520minimize%2520the%2520divergence%2520between%2520learned%2520representations%2520and%2520plausible%252C%2520actionable%2520explanations.%2520We%2520demonstrate%2520empirically%2520and%2520theoretically%2520that%2520our%2520proposed%2520method%2520facilitates%2520training%2520models%2520that%2520deliver%2520inherently%2520desirable%2520counterfactual%2520explanations%2520and%2520additionally%2520exhibit%2520improved%2520adversarial%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Training%3A%20Teaching%20Models%20Plausible%20and%20Actionable%20Explanations&entry.906535625=Patrick%20Altmeyer%20and%20Aleksander%20Buszydlik%20and%20Arie%20van%20Deursen%20and%20Cynthia%20C.%20S.%20Liem&entry.1292438233=We%20propose%20a%20novel%20training%20regime%20termed%20counterfactual%20training%20that%20leverages%20counterfactual%20explanations%20to%20increase%20the%20explanatory%20capacity%20of%20models.%20Counterfactual%20explanations%20have%20emerged%20as%20a%20popular%20post-hoc%20explanation%20method%20for%20opaque%20machine%20learning%20models%3A%20they%20inform%20how%20factual%20inputs%20would%20need%20to%20change%20in%20order%20for%20a%20model%20to%20produce%20some%20desired%20output.%20To%20be%20useful%20in%20real-world%20decision-making%20systems%2C%20counterfactuals%20should%20be%20plausible%20with%20respect%20to%20the%20underlying%20data%20and%20actionable%20with%20respect%20to%20the%20feature%20mutability%20constraints.%20Much%20existing%20research%20has%20therefore%20focused%20on%20developing%20post-hoc%20methods%20to%20generate%20counterfactuals%20that%20meet%20these%20desiderata.%20In%20this%20work%2C%20we%20instead%20hold%20models%20directly%20accountable%20for%20the%20desired%20end%20goal%3A%20counterfactual%20training%20employs%20counterfactuals%20during%20the%20training%20phase%20to%20minimize%20the%20divergence%20between%20learned%20representations%20and%20plausible%2C%20actionable%20explanations.%20We%20demonstrate%20empirically%20and%20theoretically%20that%20our%20proposed%20method%20facilitates%20training%20models%20that%20deliver%20inherently%20desirable%20counterfactual%20explanations%20and%20additionally%20exhibit%20improved%20adversarial%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2601.16205v1&entry.124074799=Read"},
{"title": "Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add", "author": "Zhengchi Ma and Anru R. Zhang", "abstract": "Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?\n  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry\" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry\" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.", "link": "http://arxiv.org/abs/2601.16120v1", "date": "2026-01-22", "relevancy": 1.9017, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.498}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.461}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Augmentation%20in%20Imbalanced%20Learning%3A%20When%20It%20Helps%2C%20When%20It%20Hurts%2C%20and%20How%20Much%20to%20Add&body=Title%3A%20Synthetic%20Augmentation%20in%20Imbalanced%20Learning%3A%20When%20It%20Helps%2C%20When%20It%20Hurts%2C%20and%20How%20Much%20to%20Add%0AAuthor%3A%20Zhengchi%20Ma%20and%20Anru%20R.%20Zhang%0AAbstract%3A%20Imbalanced%20classification%2C%20where%20one%20class%20is%20observed%20far%20less%20frequently%20than%20the%20other%2C%20often%20causes%20standard%20training%20procedures%20to%20prioritize%20the%20majority%20class%20and%20perform%20poorly%20on%20rare%20but%20important%20cases.%20A%20classic%20and%20widely%20used%20remedy%20is%20to%20augment%20the%20minority%20class%20with%20synthetic%20examples%2C%20but%20two%20basic%20questions%20remain%20under-resolved%3A%20when%20does%20synthetic%20augmentation%20actually%20help%2C%20and%20how%20many%20synthetic%20samples%20should%20be%20generated%3F%0A%20%20We%20develop%20a%20unified%20statistical%20framework%20for%20synthetic%20augmentation%20in%20imbalanced%20learning%2C%20studying%20models%20trained%20on%20imbalanced%20data%20augmented%20with%20synthetic%20minority%20samples%20and%20evaluated%20under%20the%20balanced%20population%20risk.%20Our%20theory%20shows%20that%20synthetic%20data%20is%20not%20always%20beneficial.%20In%20a%20%60%60local%20symmetry%22%20regime%2C%20imbalance%20is%20not%20the%20dominant%20source%20of%20error%20near%20the%20balanced%20optimum%2C%20so%20adding%20synthetic%20samples%20cannot%20improve%20learning%20rates%20and%20can%20even%20degrade%20performance%20by%20amplifying%20generator%20mismatch.%20When%20augmentation%20can%20help%20%28a%20%60%60local%20asymmetry%22%20regime%29%2C%20the%20optimal%20synthetic%20size%20depends%20on%20generator%20accuracy%20and%20on%20whether%20the%20generator%27s%20residual%20mismatch%20is%20directionally%20aligned%20with%20the%20intrinsic%20majority-minority%20shift.%20This%20structure%20can%20make%20the%20best%20synthetic%20size%20deviate%20from%20naive%20full%20balancing%2C%20sometimes%20by%20a%20small%20refinement%20and%20sometimes%20substantially%20when%20generator%20bias%20is%20systematic.%20Practically%2C%20we%20recommend%20Validation-Tuned%20Synthetic%20Size%20%28VTSS%29%3A%20select%20the%20synthetic%20size%20by%20minimizing%20balanced%20validation%20loss%20over%20a%20range%20centered%20near%20the%20fully%20balanced%20baseline%2C%20while%20allowing%20meaningful%20departures%20when%20the%20data%20indicate%20them.%20Simulations%20and%20a%20real%20sepsis%20prediction%20study%20support%20the%20theory%20and%20illustrate%20when%20synthetic%20augmentation%20helps%2C%20when%20it%20cannot%2C%20and%20how%20to%20tune%20its%20quantity%20effectively.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Augmentation%2520in%2520Imbalanced%2520Learning%253A%2520When%2520It%2520Helps%252C%2520When%2520It%2520Hurts%252C%2520and%2520How%2520Much%2520to%2520Add%26entry.906535625%3DZhengchi%2520Ma%2520and%2520Anru%2520R.%2520Zhang%26entry.1292438233%3DImbalanced%2520classification%252C%2520where%2520one%2520class%2520is%2520observed%2520far%2520less%2520frequently%2520than%2520the%2520other%252C%2520often%2520causes%2520standard%2520training%2520procedures%2520to%2520prioritize%2520the%2520majority%2520class%2520and%2520perform%2520poorly%2520on%2520rare%2520but%2520important%2520cases.%2520A%2520classic%2520and%2520widely%2520used%2520remedy%2520is%2520to%2520augment%2520the%2520minority%2520class%2520with%2520synthetic%2520examples%252C%2520but%2520two%2520basic%2520questions%2520remain%2520under-resolved%253A%2520when%2520does%2520synthetic%2520augmentation%2520actually%2520help%252C%2520and%2520how%2520many%2520synthetic%2520samples%2520should%2520be%2520generated%253F%250A%2520%2520We%2520develop%2520a%2520unified%2520statistical%2520framework%2520for%2520synthetic%2520augmentation%2520in%2520imbalanced%2520learning%252C%2520studying%2520models%2520trained%2520on%2520imbalanced%2520data%2520augmented%2520with%2520synthetic%2520minority%2520samples%2520and%2520evaluated%2520under%2520the%2520balanced%2520population%2520risk.%2520Our%2520theory%2520shows%2520that%2520synthetic%2520data%2520is%2520not%2520always%2520beneficial.%2520In%2520a%2520%2560%2560local%2520symmetry%2522%2520regime%252C%2520imbalance%2520is%2520not%2520the%2520dominant%2520source%2520of%2520error%2520near%2520the%2520balanced%2520optimum%252C%2520so%2520adding%2520synthetic%2520samples%2520cannot%2520improve%2520learning%2520rates%2520and%2520can%2520even%2520degrade%2520performance%2520by%2520amplifying%2520generator%2520mismatch.%2520When%2520augmentation%2520can%2520help%2520%2528a%2520%2560%2560local%2520asymmetry%2522%2520regime%2529%252C%2520the%2520optimal%2520synthetic%2520size%2520depends%2520on%2520generator%2520accuracy%2520and%2520on%2520whether%2520the%2520generator%2527s%2520residual%2520mismatch%2520is%2520directionally%2520aligned%2520with%2520the%2520intrinsic%2520majority-minority%2520shift.%2520This%2520structure%2520can%2520make%2520the%2520best%2520synthetic%2520size%2520deviate%2520from%2520naive%2520full%2520balancing%252C%2520sometimes%2520by%2520a%2520small%2520refinement%2520and%2520sometimes%2520substantially%2520when%2520generator%2520bias%2520is%2520systematic.%2520Practically%252C%2520we%2520recommend%2520Validation-Tuned%2520Synthetic%2520Size%2520%2528VTSS%2529%253A%2520select%2520the%2520synthetic%2520size%2520by%2520minimizing%2520balanced%2520validation%2520loss%2520over%2520a%2520range%2520centered%2520near%2520the%2520fully%2520balanced%2520baseline%252C%2520while%2520allowing%2520meaningful%2520departures%2520when%2520the%2520data%2520indicate%2520them.%2520Simulations%2520and%2520a%2520real%2520sepsis%2520prediction%2520study%2520support%2520the%2520theory%2520and%2520illustrate%2520when%2520synthetic%2520augmentation%2520helps%252C%2520when%2520it%2520cannot%252C%2520and%2520how%2520to%2520tune%2520its%2520quantity%2520effectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Augmentation%20in%20Imbalanced%20Learning%3A%20When%20It%20Helps%2C%20When%20It%20Hurts%2C%20and%20How%20Much%20to%20Add&entry.906535625=Zhengchi%20Ma%20and%20Anru%20R.%20Zhang&entry.1292438233=Imbalanced%20classification%2C%20where%20one%20class%20is%20observed%20far%20less%20frequently%20than%20the%20other%2C%20often%20causes%20standard%20training%20procedures%20to%20prioritize%20the%20majority%20class%20and%20perform%20poorly%20on%20rare%20but%20important%20cases.%20A%20classic%20and%20widely%20used%20remedy%20is%20to%20augment%20the%20minority%20class%20with%20synthetic%20examples%2C%20but%20two%20basic%20questions%20remain%20under-resolved%3A%20when%20does%20synthetic%20augmentation%20actually%20help%2C%20and%20how%20many%20synthetic%20samples%20should%20be%20generated%3F%0A%20%20We%20develop%20a%20unified%20statistical%20framework%20for%20synthetic%20augmentation%20in%20imbalanced%20learning%2C%20studying%20models%20trained%20on%20imbalanced%20data%20augmented%20with%20synthetic%20minority%20samples%20and%20evaluated%20under%20the%20balanced%20population%20risk.%20Our%20theory%20shows%20that%20synthetic%20data%20is%20not%20always%20beneficial.%20In%20a%20%60%60local%20symmetry%22%20regime%2C%20imbalance%20is%20not%20the%20dominant%20source%20of%20error%20near%20the%20balanced%20optimum%2C%20so%20adding%20synthetic%20samples%20cannot%20improve%20learning%20rates%20and%20can%20even%20degrade%20performance%20by%20amplifying%20generator%20mismatch.%20When%20augmentation%20can%20help%20%28a%20%60%60local%20asymmetry%22%20regime%29%2C%20the%20optimal%20synthetic%20size%20depends%20on%20generator%20accuracy%20and%20on%20whether%20the%20generator%27s%20residual%20mismatch%20is%20directionally%20aligned%20with%20the%20intrinsic%20majority-minority%20shift.%20This%20structure%20can%20make%20the%20best%20synthetic%20size%20deviate%20from%20naive%20full%20balancing%2C%20sometimes%20by%20a%20small%20refinement%20and%20sometimes%20substantially%20when%20generator%20bias%20is%20systematic.%20Practically%2C%20we%20recommend%20Validation-Tuned%20Synthetic%20Size%20%28VTSS%29%3A%20select%20the%20synthetic%20size%20by%20minimizing%20balanced%20validation%20loss%20over%20a%20range%20centered%20near%20the%20fully%20balanced%20baseline%2C%20while%20allowing%20meaningful%20departures%20when%20the%20data%20indicate%20them.%20Simulations%20and%20a%20real%20sepsis%20prediction%20study%20support%20the%20theory%20and%20illustrate%20when%20synthetic%20augmentation%20helps%2C%20when%20it%20cannot%2C%20and%20how%20to%20tune%20its%20quantity%20effectively.&entry.1838667208=http%3A//arxiv.org/abs/2601.16120v1&entry.124074799=Read"},
{"title": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation", "author": "Yuan Lin and Murong Xu and Marc H\u00f6lle and Chinmay Prabhakar and Andreas Maier and Vasileios Belagiannis and Bjoern Menze and Suprosanna Shit", "abstract": "Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.", "link": "http://arxiv.org/abs/2601.16060v1", "date": "2026-01-22", "relevancy": 1.1741, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6233}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5759}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProGiDiff%3A%20Prompt-Guided%20Diffusion-Based%20Medical%20Image%20Segmentation&body=Title%3A%20ProGiDiff%3A%20Prompt-Guided%20Diffusion-Based%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yuan%20Lin%20and%20Murong%20Xu%20and%20Marc%20H%C3%B6lle%20and%20Chinmay%20Prabhakar%20and%20Andreas%20Maier%20and%20Vasileios%20Belagiannis%20and%20Bjoern%20Menze%20and%20Suprosanna%20Shit%0AAbstract%3A%20Widely%20adopted%20medical%20image%20segmentation%20methods%2C%20although%20efficient%2C%20are%20primarily%20deterministic%20and%20remain%20poorly%20amenable%20to%20natural%20language%20prompts.%20Thus%2C%20they%20lack%20the%20capability%20to%20estimate%20multiple%20proposals%2C%20human%20interaction%2C%20and%20cross-modality%20adaptation.%20Recently%2C%20text-to-image%20diffusion%20models%20have%20shown%20potential%20to%20bridge%20the%20gap.%20However%2C%20training%20them%20from%20scratch%20requires%20a%20large%20dataset-a%20limitation%20for%20medical%20image%20segmentation.%20Furthermore%2C%20they%20are%20often%20limited%20to%20binary%20segmentation%20and%20cannot%20be%20conditioned%20on%20a%20natural%20language%20prompt.%20To%20this%20end%2C%20we%20propose%20a%20novel%20framework%20called%20ProGiDiff%20that%20leverages%20existing%20image%20generation%20models%20for%20medical%20image%20segmentation%20purposes.%20Specifically%2C%20we%20propose%20a%20ControlNet-style%20conditioning%20mechanism%20with%20a%20custom%20encoder%2C%20suitable%20for%20image%20conditioning%2C%20to%20steer%20a%20pre-trained%20diffusion%20model%20to%20output%20segmentation%20masks.%20It%20naturally%20extends%20to%20a%20multi-class%20setting%20simply%20by%20prompting%20the%20target%20organ.%20Our%20experiment%20on%20organ%20segmentation%20from%20CT%20images%20demonstrates%20strong%20performance%20compared%20to%20previous%20methods%20and%20could%20greatly%20benefit%20from%20an%20expert-in-the-loop%20setting%20to%20leverage%20multiple%20proposals.%20Importantly%2C%20we%20demonstrate%20that%20the%20learned%20conditioning%20mechanism%20can%20be%20easily%20transferred%20through%20low-rank%2C%20few-shot%20adaptation%20to%20segment%20MR%20images.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProGiDiff%253A%2520Prompt-Guided%2520Diffusion-Based%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DYuan%2520Lin%2520and%2520Murong%2520Xu%2520and%2520Marc%2520H%25C3%25B6lle%2520and%2520Chinmay%2520Prabhakar%2520and%2520Andreas%2520Maier%2520and%2520Vasileios%2520Belagiannis%2520and%2520Bjoern%2520Menze%2520and%2520Suprosanna%2520Shit%26entry.1292438233%3DWidely%2520adopted%2520medical%2520image%2520segmentation%2520methods%252C%2520although%2520efficient%252C%2520are%2520primarily%2520deterministic%2520and%2520remain%2520poorly%2520amenable%2520to%2520natural%2520language%2520prompts.%2520Thus%252C%2520they%2520lack%2520the%2520capability%2520to%2520estimate%2520multiple%2520proposals%252C%2520human%2520interaction%252C%2520and%2520cross-modality%2520adaptation.%2520Recently%252C%2520text-to-image%2520diffusion%2520models%2520have%2520shown%2520potential%2520to%2520bridge%2520the%2520gap.%2520However%252C%2520training%2520them%2520from%2520scratch%2520requires%2520a%2520large%2520dataset-a%2520limitation%2520for%2520medical%2520image%2520segmentation.%2520Furthermore%252C%2520they%2520are%2520often%2520limited%2520to%2520binary%2520segmentation%2520and%2520cannot%2520be%2520conditioned%2520on%2520a%2520natural%2520language%2520prompt.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%2520ProGiDiff%2520that%2520leverages%2520existing%2520image%2520generation%2520models%2520for%2520medical%2520image%2520segmentation%2520purposes.%2520Specifically%252C%2520we%2520propose%2520a%2520ControlNet-style%2520conditioning%2520mechanism%2520with%2520a%2520custom%2520encoder%252C%2520suitable%2520for%2520image%2520conditioning%252C%2520to%2520steer%2520a%2520pre-trained%2520diffusion%2520model%2520to%2520output%2520segmentation%2520masks.%2520It%2520naturally%2520extends%2520to%2520a%2520multi-class%2520setting%2520simply%2520by%2520prompting%2520the%2520target%2520organ.%2520Our%2520experiment%2520on%2520organ%2520segmentation%2520from%2520CT%2520images%2520demonstrates%2520strong%2520performance%2520compared%2520to%2520previous%2520methods%2520and%2520could%2520greatly%2520benefit%2520from%2520an%2520expert-in-the-loop%2520setting%2520to%2520leverage%2520multiple%2520proposals.%2520Importantly%252C%2520we%2520demonstrate%2520that%2520the%2520learned%2520conditioning%2520mechanism%2520can%2520be%2520easily%2520transferred%2520through%2520low-rank%252C%2520few-shot%2520adaptation%2520to%2520segment%2520MR%2520images.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProGiDiff%3A%20Prompt-Guided%20Diffusion-Based%20Medical%20Image%20Segmentation&entry.906535625=Yuan%20Lin%20and%20Murong%20Xu%20and%20Marc%20H%C3%B6lle%20and%20Chinmay%20Prabhakar%20and%20Andreas%20Maier%20and%20Vasileios%20Belagiannis%20and%20Bjoern%20Menze%20and%20Suprosanna%20Shit&entry.1292438233=Widely%20adopted%20medical%20image%20segmentation%20methods%2C%20although%20efficient%2C%20are%20primarily%20deterministic%20and%20remain%20poorly%20amenable%20to%20natural%20language%20prompts.%20Thus%2C%20they%20lack%20the%20capability%20to%20estimate%20multiple%20proposals%2C%20human%20interaction%2C%20and%20cross-modality%20adaptation.%20Recently%2C%20text-to-image%20diffusion%20models%20have%20shown%20potential%20to%20bridge%20the%20gap.%20However%2C%20training%20them%20from%20scratch%20requires%20a%20large%20dataset-a%20limitation%20for%20medical%20image%20segmentation.%20Furthermore%2C%20they%20are%20often%20limited%20to%20binary%20segmentation%20and%20cannot%20be%20conditioned%20on%20a%20natural%20language%20prompt.%20To%20this%20end%2C%20we%20propose%20a%20novel%20framework%20called%20ProGiDiff%20that%20leverages%20existing%20image%20generation%20models%20for%20medical%20image%20segmentation%20purposes.%20Specifically%2C%20we%20propose%20a%20ControlNet-style%20conditioning%20mechanism%20with%20a%20custom%20encoder%2C%20suitable%20for%20image%20conditioning%2C%20to%20steer%20a%20pre-trained%20diffusion%20model%20to%20output%20segmentation%20masks.%20It%20naturally%20extends%20to%20a%20multi-class%20setting%20simply%20by%20prompting%20the%20target%20organ.%20Our%20experiment%20on%20organ%20segmentation%20from%20CT%20images%20demonstrates%20strong%20performance%20compared%20to%20previous%20methods%20and%20could%20greatly%20benefit%20from%20an%20expert-in-the-loop%20setting%20to%20leverage%20multiple%20proposals.%20Importantly%2C%20we%20demonstrate%20that%20the%20learned%20conditioning%20mechanism%20can%20be%20easily%20transferred%20through%20low-rank%2C%20few-shot%20adaptation%20to%20segment%20MR%20images.&entry.1838667208=http%3A//arxiv.org/abs/2601.16060v1&entry.124074799=Read"},
{"title": "Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems", "author": "Annemarie Jutte and Uraz Odyurt", "abstract": "Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.", "link": "http://arxiv.org/abs/2601.16074v1", "date": "2026-01-22", "relevancy": 0.9585, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.473}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI%20to%20Improve%20Machine%20Learning%20Reliability%20for%20Industrial%20Cyber-Physical%20Systems&body=Title%3A%20Explainable%20AI%20to%20Improve%20Machine%20Learning%20Reliability%20for%20Industrial%20Cyber-Physical%20Systems%0AAuthor%3A%20Annemarie%20Jutte%20and%20Uraz%20Odyurt%0AAbstract%3A%20Industrial%20Cyber-Physical%20Systems%20%28CPS%29%20are%20sensitive%20infrastructure%20from%20both%20safety%20and%20economics%20perspectives%2C%20making%20their%20reliability%20critically%20important.%20Machine%20Learning%20%28ML%29%2C%20specifically%20deep%20learning%2C%20is%20increasingly%20integrated%20in%20industrial%20CPS%2C%20but%20the%20inherent%20complexity%20of%20ML%20models%20results%20in%20non-transparent%20operation.%20Rigorous%20evaluation%20is%20needed%20to%20prevent%20models%20from%20exhibiting%20unexpected%20behaviour%20on%20future%2C%20unseen%20data.%20Explainable%20AI%20%28XAI%29%20can%20be%20used%20to%20uncover%20model%20reasoning%2C%20allowing%20a%20more%20extensive%20analysis%20of%20behaviour.%20We%20apply%20XAI%20to%20to%20improve%20predictive%20performance%20of%20ML%20models%20intended%20for%20industrial%20CPS.%20We%20analyse%20the%20effects%20of%20components%20from%20time-series%20data%20decomposition%20on%20model%20predictions%20using%20SHAP%20values.%20Through%20this%20method%2C%20we%20observe%20evidence%20on%20the%20lack%20of%20sufficient%20contextual%20information%20during%20model%20training.%20By%20increasing%20the%20window%20size%20of%20data%20instances%2C%20informed%20by%20the%20XAI%20findings%2C%20we%20are%20able%20to%20improve%20model%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI%2520to%2520Improve%2520Machine%2520Learning%2520Reliability%2520for%2520Industrial%2520Cyber-Physical%2520Systems%26entry.906535625%3DAnnemarie%2520Jutte%2520and%2520Uraz%2520Odyurt%26entry.1292438233%3DIndustrial%2520Cyber-Physical%2520Systems%2520%2528CPS%2529%2520are%2520sensitive%2520infrastructure%2520from%2520both%2520safety%2520and%2520economics%2520perspectives%252C%2520making%2520their%2520reliability%2520critically%2520important.%2520Machine%2520Learning%2520%2528ML%2529%252C%2520specifically%2520deep%2520learning%252C%2520is%2520increasingly%2520integrated%2520in%2520industrial%2520CPS%252C%2520but%2520the%2520inherent%2520complexity%2520of%2520ML%2520models%2520results%2520in%2520non-transparent%2520operation.%2520Rigorous%2520evaluation%2520is%2520needed%2520to%2520prevent%2520models%2520from%2520exhibiting%2520unexpected%2520behaviour%2520on%2520future%252C%2520unseen%2520data.%2520Explainable%2520AI%2520%2528XAI%2529%2520can%2520be%2520used%2520to%2520uncover%2520model%2520reasoning%252C%2520allowing%2520a%2520more%2520extensive%2520analysis%2520of%2520behaviour.%2520We%2520apply%2520XAI%2520to%2520to%2520improve%2520predictive%2520performance%2520of%2520ML%2520models%2520intended%2520for%2520industrial%2520CPS.%2520We%2520analyse%2520the%2520effects%2520of%2520components%2520from%2520time-series%2520data%2520decomposition%2520on%2520model%2520predictions%2520using%2520SHAP%2520values.%2520Through%2520this%2520method%252C%2520we%2520observe%2520evidence%2520on%2520the%2520lack%2520of%2520sufficient%2520contextual%2520information%2520during%2520model%2520training.%2520By%2520increasing%2520the%2520window%2520size%2520of%2520data%2520instances%252C%2520informed%2520by%2520the%2520XAI%2520findings%252C%2520we%2520are%2520able%2520to%2520improve%2520model%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI%20to%20Improve%20Machine%20Learning%20Reliability%20for%20Industrial%20Cyber-Physical%20Systems&entry.906535625=Annemarie%20Jutte%20and%20Uraz%20Odyurt&entry.1292438233=Industrial%20Cyber-Physical%20Systems%20%28CPS%29%20are%20sensitive%20infrastructure%20from%20both%20safety%20and%20economics%20perspectives%2C%20making%20their%20reliability%20critically%20important.%20Machine%20Learning%20%28ML%29%2C%20specifically%20deep%20learning%2C%20is%20increasingly%20integrated%20in%20industrial%20CPS%2C%20but%20the%20inherent%20complexity%20of%20ML%20models%20results%20in%20non-transparent%20operation.%20Rigorous%20evaluation%20is%20needed%20to%20prevent%20models%20from%20exhibiting%20unexpected%20behaviour%20on%20future%2C%20unseen%20data.%20Explainable%20AI%20%28XAI%29%20can%20be%20used%20to%20uncover%20model%20reasoning%2C%20allowing%20a%20more%20extensive%20analysis%20of%20behaviour.%20We%20apply%20XAI%20to%20to%20improve%20predictive%20performance%20of%20ML%20models%20intended%20for%20industrial%20CPS.%20We%20analyse%20the%20effects%20of%20components%20from%20time-series%20data%20decomposition%20on%20model%20predictions%20using%20SHAP%20values.%20Through%20this%20method%2C%20we%20observe%20evidence%20on%20the%20lack%20of%20sufficient%20contextual%20information%20during%20model%20training.%20By%20increasing%20the%20window%20size%20of%20data%20instances%2C%20informed%20by%20the%20XAI%20findings%2C%20we%20are%20able%20to%20improve%20model%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.16074v1&entry.124074799=Read"},
{"title": "Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach", "author": "Eric Hirsch and Christian Friedrich", "abstract": "Accurate tool wear prediction is essential for maintaining productivity and minimizing costs in machining. However, the complex nature of the tool wear process poses significant challenges to achieving reliable predictions. This study explores data-driven methods, in particular deep learning, for tool wear prediction. Traditional data-driven approaches often focus on a single process, relying on multi-sensor setups and extensive data generation, which limits generalization to new settings. Moreover, multi-sensor integration is often impractical in industrial environments. To address these limitations, this research investigates the transferability of predictive models using minimal training data, validated across two processes. Furthermore, it uses a simple setup with a single acceleration sensor to establish a low-cost data generation approach that facilitates the generalization of models to other processes via transfer learning. The study evaluates several machine learning models, including transformer-inspired convolutional neural networks (CNN), long short-term memory networks (LSTM), support vector machines (SVM), and decision trees, trained on different input formats such as feature vectors and short-time Fourier transform (STFT). The performance of the models is evaluated on two machines and on different amounts of training data, including scenarios with significantly reduced datasets, providing insight into their effectiveness under constrained data conditions. The results demonstrate the potential of specific models and configurations for effective tool wear prediction, contributing to the development of more adaptable and efficient predictive maintenance strategies in machining. Notably, the ConvNeXt model has an exceptional performance, achieving 99.1\\% accuracy in identifying tool wear using data from only four milling tools operated until they are worn.", "link": "http://arxiv.org/abs/2412.19950v5", "date": "2026-01-22", "relevancy": 1.4525, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5293}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20tool%20wear%20prediction%20in%20milling%2C%20based%20on%20a%20process-integrated%20single-sensor%20approach&body=Title%3A%20Data-driven%20tool%20wear%20prediction%20in%20milling%2C%20based%20on%20a%20process-integrated%20single-sensor%20approach%0AAuthor%3A%20Eric%20Hirsch%20and%20Christian%20Friedrich%0AAbstract%3A%20Accurate%20tool%20wear%20prediction%20is%20essential%20for%20maintaining%20productivity%20and%20minimizing%20costs%20in%20machining.%20However%2C%20the%20complex%20nature%20of%20the%20tool%20wear%20process%20poses%20significant%20challenges%20to%20achieving%20reliable%20predictions.%20This%20study%20explores%20data-driven%20methods%2C%20in%20particular%20deep%20learning%2C%20for%20tool%20wear%20prediction.%20Traditional%20data-driven%20approaches%20often%20focus%20on%20a%20single%20process%2C%20relying%20on%20multi-sensor%20setups%20and%20extensive%20data%20generation%2C%20which%20limits%20generalization%20to%20new%20settings.%20Moreover%2C%20multi-sensor%20integration%20is%20often%20impractical%20in%20industrial%20environments.%20To%20address%20these%20limitations%2C%20this%20research%20investigates%20the%20transferability%20of%20predictive%20models%20using%20minimal%20training%20data%2C%20validated%20across%20two%20processes.%20Furthermore%2C%20it%20uses%20a%20simple%20setup%20with%20a%20single%20acceleration%20sensor%20to%20establish%20a%20low-cost%20data%20generation%20approach%20that%20facilitates%20the%20generalization%20of%20models%20to%20other%20processes%20via%20transfer%20learning.%20The%20study%20evaluates%20several%20machine%20learning%20models%2C%20including%20transformer-inspired%20convolutional%20neural%20networks%20%28CNN%29%2C%20long%20short-term%20memory%20networks%20%28LSTM%29%2C%20support%20vector%20machines%20%28SVM%29%2C%20and%20decision%20trees%2C%20trained%20on%20different%20input%20formats%20such%20as%20feature%20vectors%20and%20short-time%20Fourier%20transform%20%28STFT%29.%20The%20performance%20of%20the%20models%20is%20evaluated%20on%20two%20machines%20and%20on%20different%20amounts%20of%20training%20data%2C%20including%20scenarios%20with%20significantly%20reduced%20datasets%2C%20providing%20insight%20into%20their%20effectiveness%20under%20constrained%20data%20conditions.%20The%20results%20demonstrate%20the%20potential%20of%20specific%20models%20and%20configurations%20for%20effective%20tool%20wear%20prediction%2C%20contributing%20to%20the%20development%20of%20more%20adaptable%20and%20efficient%20predictive%20maintenance%20strategies%20in%20machining.%20Notably%2C%20the%20ConvNeXt%20model%20has%20an%20exceptional%20performance%2C%20achieving%2099.1%5C%25%20accuracy%20in%20identifying%20tool%20wear%20using%20data%20from%20only%20four%20milling%20tools%20operated%20until%20they%20are%20worn.%0ALink%3A%20http%3A//arxiv.org/abs/2412.19950v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520tool%2520wear%2520prediction%2520in%2520milling%252C%2520based%2520on%2520a%2520process-integrated%2520single-sensor%2520approach%26entry.906535625%3DEric%2520Hirsch%2520and%2520Christian%2520Friedrich%26entry.1292438233%3DAccurate%2520tool%2520wear%2520prediction%2520is%2520essential%2520for%2520maintaining%2520productivity%2520and%2520minimizing%2520costs%2520in%2520machining.%2520However%252C%2520the%2520complex%2520nature%2520of%2520the%2520tool%2520wear%2520process%2520poses%2520significant%2520challenges%2520to%2520achieving%2520reliable%2520predictions.%2520This%2520study%2520explores%2520data-driven%2520methods%252C%2520in%2520particular%2520deep%2520learning%252C%2520for%2520tool%2520wear%2520prediction.%2520Traditional%2520data-driven%2520approaches%2520often%2520focus%2520on%2520a%2520single%2520process%252C%2520relying%2520on%2520multi-sensor%2520setups%2520and%2520extensive%2520data%2520generation%252C%2520which%2520limits%2520generalization%2520to%2520new%2520settings.%2520Moreover%252C%2520multi-sensor%2520integration%2520is%2520often%2520impractical%2520in%2520industrial%2520environments.%2520To%2520address%2520these%2520limitations%252C%2520this%2520research%2520investigates%2520the%2520transferability%2520of%2520predictive%2520models%2520using%2520minimal%2520training%2520data%252C%2520validated%2520across%2520two%2520processes.%2520Furthermore%252C%2520it%2520uses%2520a%2520simple%2520setup%2520with%2520a%2520single%2520acceleration%2520sensor%2520to%2520establish%2520a%2520low-cost%2520data%2520generation%2520approach%2520that%2520facilitates%2520the%2520generalization%2520of%2520models%2520to%2520other%2520processes%2520via%2520transfer%2520learning.%2520The%2520study%2520evaluates%2520several%2520machine%2520learning%2520models%252C%2520including%2520transformer-inspired%2520convolutional%2520neural%2520networks%2520%2528CNN%2529%252C%2520long%2520short-term%2520memory%2520networks%2520%2528LSTM%2529%252C%2520support%2520vector%2520machines%2520%2528SVM%2529%252C%2520and%2520decision%2520trees%252C%2520trained%2520on%2520different%2520input%2520formats%2520such%2520as%2520feature%2520vectors%2520and%2520short-time%2520Fourier%2520transform%2520%2528STFT%2529.%2520The%2520performance%2520of%2520the%2520models%2520is%2520evaluated%2520on%2520two%2520machines%2520and%2520on%2520different%2520amounts%2520of%2520training%2520data%252C%2520including%2520scenarios%2520with%2520significantly%2520reduced%2520datasets%252C%2520providing%2520insight%2520into%2520their%2520effectiveness%2520under%2520constrained%2520data%2520conditions.%2520The%2520results%2520demonstrate%2520the%2520potential%2520of%2520specific%2520models%2520and%2520configurations%2520for%2520effective%2520tool%2520wear%2520prediction%252C%2520contributing%2520to%2520the%2520development%2520of%2520more%2520adaptable%2520and%2520efficient%2520predictive%2520maintenance%2520strategies%2520in%2520machining.%2520Notably%252C%2520the%2520ConvNeXt%2520model%2520has%2520an%2520exceptional%2520performance%252C%2520achieving%252099.1%255C%2525%2520accuracy%2520in%2520identifying%2520tool%2520wear%2520using%2520data%2520from%2520only%2520four%2520milling%2520tools%2520operated%2520until%2520they%2520are%2520worn.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19950v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20tool%20wear%20prediction%20in%20milling%2C%20based%20on%20a%20process-integrated%20single-sensor%20approach&entry.906535625=Eric%20Hirsch%20and%20Christian%20Friedrich&entry.1292438233=Accurate%20tool%20wear%20prediction%20is%20essential%20for%20maintaining%20productivity%20and%20minimizing%20costs%20in%20machining.%20However%2C%20the%20complex%20nature%20of%20the%20tool%20wear%20process%20poses%20significant%20challenges%20to%20achieving%20reliable%20predictions.%20This%20study%20explores%20data-driven%20methods%2C%20in%20particular%20deep%20learning%2C%20for%20tool%20wear%20prediction.%20Traditional%20data-driven%20approaches%20often%20focus%20on%20a%20single%20process%2C%20relying%20on%20multi-sensor%20setups%20and%20extensive%20data%20generation%2C%20which%20limits%20generalization%20to%20new%20settings.%20Moreover%2C%20multi-sensor%20integration%20is%20often%20impractical%20in%20industrial%20environments.%20To%20address%20these%20limitations%2C%20this%20research%20investigates%20the%20transferability%20of%20predictive%20models%20using%20minimal%20training%20data%2C%20validated%20across%20two%20processes.%20Furthermore%2C%20it%20uses%20a%20simple%20setup%20with%20a%20single%20acceleration%20sensor%20to%20establish%20a%20low-cost%20data%20generation%20approach%20that%20facilitates%20the%20generalization%20of%20models%20to%20other%20processes%20via%20transfer%20learning.%20The%20study%20evaluates%20several%20machine%20learning%20models%2C%20including%20transformer-inspired%20convolutional%20neural%20networks%20%28CNN%29%2C%20long%20short-term%20memory%20networks%20%28LSTM%29%2C%20support%20vector%20machines%20%28SVM%29%2C%20and%20decision%20trees%2C%20trained%20on%20different%20input%20formats%20such%20as%20feature%20vectors%20and%20short-time%20Fourier%20transform%20%28STFT%29.%20The%20performance%20of%20the%20models%20is%20evaluated%20on%20two%20machines%20and%20on%20different%20amounts%20of%20training%20data%2C%20including%20scenarios%20with%20significantly%20reduced%20datasets%2C%20providing%20insight%20into%20their%20effectiveness%20under%20constrained%20data%20conditions.%20The%20results%20demonstrate%20the%20potential%20of%20specific%20models%20and%20configurations%20for%20effective%20tool%20wear%20prediction%2C%20contributing%20to%20the%20development%20of%20more%20adaptable%20and%20efficient%20predictive%20maintenance%20strategies%20in%20machining.%20Notably%2C%20the%20ConvNeXt%20model%20has%20an%20exceptional%20performance%2C%20achieving%2099.1%5C%25%20accuracy%20in%20identifying%20tool%20wear%20using%20data%20from%20only%20four%20milling%20tools%20operated%20until%20they%20are%20worn.&entry.1838667208=http%3A//arxiv.org/abs/2412.19950v5&entry.124074799=Read"},
{"title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models", "author": "Hanwen Zhang and Qiaojin Shen and Yuxi Liu and Yuesheng Zhu and Guibo Luo", "abstract": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.", "link": "http://arxiv.org/abs/2601.16073v1", "date": "2026-01-22", "relevancy": 1.6224, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5882}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5312}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSFedMed%3A%20Dual-Scale%20Federated%20Medical%20Image%20Segmentation%20via%20Mutual%20Distillation%20Between%20Foundation%20and%20Lightweight%20Models&body=Title%3A%20DSFedMed%3A%20Dual-Scale%20Federated%20Medical%20Image%20Segmentation%20via%20Mutual%20Distillation%20Between%20Foundation%20and%20Lightweight%20Models%0AAuthor%3A%20Hanwen%20Zhang%20and%20Qiaojin%20Shen%20and%20Yuxi%20Liu%20and%20Yuesheng%20Zhu%20and%20Guibo%20Luo%0AAbstract%3A%20Foundation%20Models%20%28FMs%29%20have%20demonstrated%20strong%20generalization%20across%20diverse%20vision%20tasks.%20However%2C%20their%20deployment%20in%20federated%20settings%20is%20hindered%20by%20high%20computational%20demands%2C%20substantial%20communication%20overhead%2C%20and%20significant%20inference%20costs.%20We%20propose%20DSFedMed%2C%20a%20dual-scale%20federated%20framework%20that%20enables%20mutual%20knowledge%20distillation%20between%20a%20centralized%20foundation%20model%20and%20lightweight%20client%20models%20for%20medical%20image%20segmentation.%20To%20support%20knowledge%20distillation%2C%20a%20set%20of%20high-quality%20medical%20images%20is%20generated%20to%20replace%20real%20public%20datasets%2C%20and%20a%20learnability-guided%20sample%20selection%20strategy%20is%20proposed%20to%20enhance%20efficiency%20and%20effectiveness%20in%20dual-scale%20distillation.%20This%20mutual%20distillation%20enables%20the%20foundation%20model%20to%20transfer%20general%20knowledge%20to%20lightweight%20clients%2C%20while%20also%20incorporating%20client-specific%20insights%20to%20refine%20the%20foundation%20model.%20Evaluations%20on%20five%20medical%20imaging%20segmentation%20datasets%20show%20that%20DSFedMed%20achieves%20an%20average%202%20percent%20improvement%20in%20Dice%20score%20while%20reducing%20communication%20costs%20and%20inference%20time%20by%20nearly%2090%20percent%20compared%20to%20existing%20federated%20foundation%20model%20baselines.%20These%20results%20demonstrate%20significant%20efficiency%20gains%20and%20scalability%20for%20resource-limited%20federated%20deployments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSFedMed%253A%2520Dual-Scale%2520Federated%2520Medical%2520Image%2520Segmentation%2520via%2520Mutual%2520Distillation%2520Between%2520Foundation%2520and%2520Lightweight%2520Models%26entry.906535625%3DHanwen%2520Zhang%2520and%2520Qiaojin%2520Shen%2520and%2520Yuxi%2520Liu%2520and%2520Yuesheng%2520Zhu%2520and%2520Guibo%2520Luo%26entry.1292438233%3DFoundation%2520Models%2520%2528FMs%2529%2520have%2520demonstrated%2520strong%2520generalization%2520across%2520diverse%2520vision%2520tasks.%2520However%252C%2520their%2520deployment%2520in%2520federated%2520settings%2520is%2520hindered%2520by%2520high%2520computational%2520demands%252C%2520substantial%2520communication%2520overhead%252C%2520and%2520significant%2520inference%2520costs.%2520We%2520propose%2520DSFedMed%252C%2520a%2520dual-scale%2520federated%2520framework%2520that%2520enables%2520mutual%2520knowledge%2520distillation%2520between%2520a%2520centralized%2520foundation%2520model%2520and%2520lightweight%2520client%2520models%2520for%2520medical%2520image%2520segmentation.%2520To%2520support%2520knowledge%2520distillation%252C%2520a%2520set%2520of%2520high-quality%2520medical%2520images%2520is%2520generated%2520to%2520replace%2520real%2520public%2520datasets%252C%2520and%2520a%2520learnability-guided%2520sample%2520selection%2520strategy%2520is%2520proposed%2520to%2520enhance%2520efficiency%2520and%2520effectiveness%2520in%2520dual-scale%2520distillation.%2520This%2520mutual%2520distillation%2520enables%2520the%2520foundation%2520model%2520to%2520transfer%2520general%2520knowledge%2520to%2520lightweight%2520clients%252C%2520while%2520also%2520incorporating%2520client-specific%2520insights%2520to%2520refine%2520the%2520foundation%2520model.%2520Evaluations%2520on%2520five%2520medical%2520imaging%2520segmentation%2520datasets%2520show%2520that%2520DSFedMed%2520achieves%2520an%2520average%25202%2520percent%2520improvement%2520in%2520Dice%2520score%2520while%2520reducing%2520communication%2520costs%2520and%2520inference%2520time%2520by%2520nearly%252090%2520percent%2520compared%2520to%2520existing%2520federated%2520foundation%2520model%2520baselines.%2520These%2520results%2520demonstrate%2520significant%2520efficiency%2520gains%2520and%2520scalability%2520for%2520resource-limited%2520federated%2520deployments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSFedMed%3A%20Dual-Scale%20Federated%20Medical%20Image%20Segmentation%20via%20Mutual%20Distillation%20Between%20Foundation%20and%20Lightweight%20Models&entry.906535625=Hanwen%20Zhang%20and%20Qiaojin%20Shen%20and%20Yuxi%20Liu%20and%20Yuesheng%20Zhu%20and%20Guibo%20Luo&entry.1292438233=Foundation%20Models%20%28FMs%29%20have%20demonstrated%20strong%20generalization%20across%20diverse%20vision%20tasks.%20However%2C%20their%20deployment%20in%20federated%20settings%20is%20hindered%20by%20high%20computational%20demands%2C%20substantial%20communication%20overhead%2C%20and%20significant%20inference%20costs.%20We%20propose%20DSFedMed%2C%20a%20dual-scale%20federated%20framework%20that%20enables%20mutual%20knowledge%20distillation%20between%20a%20centralized%20foundation%20model%20and%20lightweight%20client%20models%20for%20medical%20image%20segmentation.%20To%20support%20knowledge%20distillation%2C%20a%20set%20of%20high-quality%20medical%20images%20is%20generated%20to%20replace%20real%20public%20datasets%2C%20and%20a%20learnability-guided%20sample%20selection%20strategy%20is%20proposed%20to%20enhance%20efficiency%20and%20effectiveness%20in%20dual-scale%20distillation.%20This%20mutual%20distillation%20enables%20the%20foundation%20model%20to%20transfer%20general%20knowledge%20to%20lightweight%20clients%2C%20while%20also%20incorporating%20client-specific%20insights%20to%20refine%20the%20foundation%20model.%20Evaluations%20on%20five%20medical%20imaging%20segmentation%20datasets%20show%20that%20DSFedMed%20achieves%20an%20average%202%20percent%20improvement%20in%20Dice%20score%20while%20reducing%20communication%20costs%20and%20inference%20time%20by%20nearly%2090%20percent%20compared%20to%20existing%20federated%20foundation%20model%20baselines.%20These%20results%20demonstrate%20significant%20efficiency%20gains%20and%20scalability%20for%20resource-limited%20federated%20deployments.&entry.1838667208=http%3A//arxiv.org/abs/2601.16073v1&entry.124074799=Read"},
{"title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval", "author": "Olga Bunkova and Lorenzo Di Fruscia and Sophia Rupprecht and Artur M. Schweidtmann and Marcel J. T. Reinders and Jana M. Weber", "abstract": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.", "link": "http://arxiv.org/abs/2601.16038v1", "date": "2026-01-22", "relevancy": 1.9479, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Large%20Language%20Models%20in%20Reaction%20Knowledge%20Graphs%20for%20Synthesis%20Retrieval&body=Title%3A%20Grounding%20Large%20Language%20Models%20in%20Reaction%20Knowledge%20Graphs%20for%20Synthesis%20Retrieval%0AAuthor%3A%20Olga%20Bunkova%20and%20Lorenzo%20Di%20Fruscia%20and%20Sophia%20Rupprecht%20and%20Artur%20M.%20Schweidtmann%20and%20Marcel%20J.%20T.%20Reinders%20and%20Jana%20M.%20Weber%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20can%20aid%20synthesis%20planning%20in%20chemistry%2C%20but%20standard%20prompting%20methods%20often%20yield%20hallucinated%20or%20outdated%20suggestions.%20We%20study%20LLM%20interactions%20with%20a%20reaction%20knowledge%20graph%20by%20casting%20reaction%20path%20retrieval%20as%20a%20Text2Cypher%20%28natural%20language%20to%20graph%20query%29%20generation%20problem%2C%20and%20define%20single-%20and%20multi-step%20retrieval%20tasks.%20We%20compare%20zero-shot%20prompting%20to%20one-shot%20variants%20using%20static%2C%20random%2C%20and%20embedding-based%20exemplar%20selection%2C%20and%20assess%20a%20checklist-driven%20validator/corrector%20loop.%20To%20evaluate%20our%20framework%2C%20we%20consider%20query%20validity%20and%20retrieval%20accuracy.%20We%20find%20that%20one-shot%20prompting%20with%20aligned%20exemplars%20consistently%20performs%20best.%20Our%20checklist-style%20self-correction%20loop%20mainly%20improves%20executability%20in%20zero-shot%20settings%20and%20offers%20limited%20additional%20retrieval%20gains%20once%20a%20good%20exemplar%20is%20present.%20We%20provide%20a%20reproducible%20Text2Cypher%20evaluation%20setup%20to%20facilitate%20further%20work%20on%20KG-grounded%20LLMs%20for%20synthesis%20planning.%20Code%20is%20available%20at%20https%3A//github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Large%2520Language%2520Models%2520in%2520Reaction%2520Knowledge%2520Graphs%2520for%2520Synthesis%2520Retrieval%26entry.906535625%3DOlga%2520Bunkova%2520and%2520Lorenzo%2520Di%2520Fruscia%2520and%2520Sophia%2520Rupprecht%2520and%2520Artur%2520M.%2520Schweidtmann%2520and%2520Marcel%2520J.%2520T.%2520Reinders%2520and%2520Jana%2520M.%2520Weber%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520aid%2520synthesis%2520planning%2520in%2520chemistry%252C%2520but%2520standard%2520prompting%2520methods%2520often%2520yield%2520hallucinated%2520or%2520outdated%2520suggestions.%2520We%2520study%2520LLM%2520interactions%2520with%2520a%2520reaction%2520knowledge%2520graph%2520by%2520casting%2520reaction%2520path%2520retrieval%2520as%2520a%2520Text2Cypher%2520%2528natural%2520language%2520to%2520graph%2520query%2529%2520generation%2520problem%252C%2520and%2520define%2520single-%2520and%2520multi-step%2520retrieval%2520tasks.%2520We%2520compare%2520zero-shot%2520prompting%2520to%2520one-shot%2520variants%2520using%2520static%252C%2520random%252C%2520and%2520embedding-based%2520exemplar%2520selection%252C%2520and%2520assess%2520a%2520checklist-driven%2520validator/corrector%2520loop.%2520To%2520evaluate%2520our%2520framework%252C%2520we%2520consider%2520query%2520validity%2520and%2520retrieval%2520accuracy.%2520We%2520find%2520that%2520one-shot%2520prompting%2520with%2520aligned%2520exemplars%2520consistently%2520performs%2520best.%2520Our%2520checklist-style%2520self-correction%2520loop%2520mainly%2520improves%2520executability%2520in%2520zero-shot%2520settings%2520and%2520offers%2520limited%2520additional%2520retrieval%2520gains%2520once%2520a%2520good%2520exemplar%2520is%2520present.%2520We%2520provide%2520a%2520reproducible%2520Text2Cypher%2520evaluation%2520setup%2520to%2520facilitate%2520further%2520work%2520on%2520KG-grounded%2520LLMs%2520for%2520synthesis%2520planning.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Large%20Language%20Models%20in%20Reaction%20Knowledge%20Graphs%20for%20Synthesis%20Retrieval&entry.906535625=Olga%20Bunkova%20and%20Lorenzo%20Di%20Fruscia%20and%20Sophia%20Rupprecht%20and%20Artur%20M.%20Schweidtmann%20and%20Marcel%20J.%20T.%20Reinders%20and%20Jana%20M.%20Weber&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20can%20aid%20synthesis%20planning%20in%20chemistry%2C%20but%20standard%20prompting%20methods%20often%20yield%20hallucinated%20or%20outdated%20suggestions.%20We%20study%20LLM%20interactions%20with%20a%20reaction%20knowledge%20graph%20by%20casting%20reaction%20path%20retrieval%20as%20a%20Text2Cypher%20%28natural%20language%20to%20graph%20query%29%20generation%20problem%2C%20and%20define%20single-%20and%20multi-step%20retrieval%20tasks.%20We%20compare%20zero-shot%20prompting%20to%20one-shot%20variants%20using%20static%2C%20random%2C%20and%20embedding-based%20exemplar%20selection%2C%20and%20assess%20a%20checklist-driven%20validator/corrector%20loop.%20To%20evaluate%20our%20framework%2C%20we%20consider%20query%20validity%20and%20retrieval%20accuracy.%20We%20find%20that%20one-shot%20prompting%20with%20aligned%20exemplars%20consistently%20performs%20best.%20Our%20checklist-style%20self-correction%20loop%20mainly%20improves%20executability%20in%20zero-shot%20settings%20and%20offers%20limited%20additional%20retrieval%20gains%20once%20a%20good%20exemplar%20is%20present.%20We%20provide%20a%20reproducible%20Text2Cypher%20evaluation%20setup%20to%20facilitate%20further%20work%20on%20KG-grounded%20LLMs%20for%20synthesis%20planning.%20Code%20is%20available%20at%20https%3A//github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.&entry.1838667208=http%3A//arxiv.org/abs/2601.16038v1&entry.124074799=Read"},
{"title": "Is this chart lying to me? Automating the detection of misleading visualizations", "author": "Jonathan Tonglet and Jan Zimny and Tinne Tuytelaars and Iryna Gurevych", "abstract": "Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also create Misviz-synth, a synthetic dataset of 57,665 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and image-axis classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.", "link": "http://arxiv.org/abs/2508.21675v2", "date": "2026-01-22", "relevancy": 1.9322, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5045}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4763}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20this%20chart%20lying%20to%20me%3F%20Automating%20the%20detection%20of%20misleading%20visualizations&body=Title%3A%20Is%20this%20chart%20lying%20to%20me%3F%20Automating%20the%20detection%20of%20misleading%20visualizations%0AAuthor%3A%20Jonathan%20Tonglet%20and%20Jan%20Zimny%20and%20Tinne%20Tuytelaars%20and%20Iryna%20Gurevych%0AAbstract%3A%20Misleading%20visualizations%20are%20a%20potent%20driver%20of%20misinformation%20on%20social%20media%20and%20the%20web.%20By%20violating%20chart%20design%20principles%2C%20they%20distort%20data%20and%20lead%20readers%20to%20draw%20inaccurate%20conclusions.%20Prior%20work%20has%20shown%20that%20both%20humans%20and%20multimodal%20large%20language%20models%20%28MLLMs%29%20are%20frequently%20deceived%20by%20such%20visualizations.%20Automatically%20detecting%20misleading%20visualizations%20and%20identifying%20the%20specific%20design%20rules%20they%20violate%20could%20help%20protect%20readers%20and%20reduce%20the%20spread%20of%20misinformation.%20However%2C%20the%20training%20and%20evaluation%20of%20AI%20models%20has%20been%20limited%20by%20the%20absence%20of%20large%2C%20diverse%2C%20and%20openly%20available%20datasets.%20In%20this%20work%2C%20we%20introduce%20Misviz%2C%20a%20benchmark%20of%202%2C604%20real-world%20visualizations%20annotated%20with%2012%20types%20of%20misleaders.%20To%20support%20model%20training%2C%20we%20also%20create%20Misviz-synth%2C%20a%20synthetic%20dataset%20of%2057%2C665%20visualizations%20generated%20using%20Matplotlib%20and%20based%20on%20real-world%20data%20tables.%20We%20perform%20a%20comprehensive%20evaluation%20on%20both%20datasets%20using%20state-of-the-art%20MLLMs%2C%20rule-based%20systems%2C%20and%20image-axis%20classifiers.%20Our%20results%20reveal%20that%20the%20task%20remains%20highly%20challenging.%20We%20release%20Misviz%2C%20Misviz-synth%2C%20and%20the%20accompanying%20code.%0ALink%3A%20http%3A//arxiv.org/abs/2508.21675v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520this%2520chart%2520lying%2520to%2520me%253F%2520Automating%2520the%2520detection%2520of%2520misleading%2520visualizations%26entry.906535625%3DJonathan%2520Tonglet%2520and%2520Jan%2520Zimny%2520and%2520Tinne%2520Tuytelaars%2520and%2520Iryna%2520Gurevych%26entry.1292438233%3DMisleading%2520visualizations%2520are%2520a%2520potent%2520driver%2520of%2520misinformation%2520on%2520social%2520media%2520and%2520the%2520web.%2520By%2520violating%2520chart%2520design%2520principles%252C%2520they%2520distort%2520data%2520and%2520lead%2520readers%2520to%2520draw%2520inaccurate%2520conclusions.%2520Prior%2520work%2520has%2520shown%2520that%2520both%2520humans%2520and%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520are%2520frequently%2520deceived%2520by%2520such%2520visualizations.%2520Automatically%2520detecting%2520misleading%2520visualizations%2520and%2520identifying%2520the%2520specific%2520design%2520rules%2520they%2520violate%2520could%2520help%2520protect%2520readers%2520and%2520reduce%2520the%2520spread%2520of%2520misinformation.%2520However%252C%2520the%2520training%2520and%2520evaluation%2520of%2520AI%2520models%2520has%2520been%2520limited%2520by%2520the%2520absence%2520of%2520large%252C%2520diverse%252C%2520and%2520openly%2520available%2520datasets.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Misviz%252C%2520a%2520benchmark%2520of%25202%252C604%2520real-world%2520visualizations%2520annotated%2520with%252012%2520types%2520of%2520misleaders.%2520To%2520support%2520model%2520training%252C%2520we%2520also%2520create%2520Misviz-synth%252C%2520a%2520synthetic%2520dataset%2520of%252057%252C665%2520visualizations%2520generated%2520using%2520Matplotlib%2520and%2520based%2520on%2520real-world%2520data%2520tables.%2520We%2520perform%2520a%2520comprehensive%2520evaluation%2520on%2520both%2520datasets%2520using%2520state-of-the-art%2520MLLMs%252C%2520rule-based%2520systems%252C%2520and%2520image-axis%2520classifiers.%2520Our%2520results%2520reveal%2520that%2520the%2520task%2520remains%2520highly%2520challenging.%2520We%2520release%2520Misviz%252C%2520Misviz-synth%252C%2520and%2520the%2520accompanying%2520code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21675v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20this%20chart%20lying%20to%20me%3F%20Automating%20the%20detection%20of%20misleading%20visualizations&entry.906535625=Jonathan%20Tonglet%20and%20Jan%20Zimny%20and%20Tinne%20Tuytelaars%20and%20Iryna%20Gurevych&entry.1292438233=Misleading%20visualizations%20are%20a%20potent%20driver%20of%20misinformation%20on%20social%20media%20and%20the%20web.%20By%20violating%20chart%20design%20principles%2C%20they%20distort%20data%20and%20lead%20readers%20to%20draw%20inaccurate%20conclusions.%20Prior%20work%20has%20shown%20that%20both%20humans%20and%20multimodal%20large%20language%20models%20%28MLLMs%29%20are%20frequently%20deceived%20by%20such%20visualizations.%20Automatically%20detecting%20misleading%20visualizations%20and%20identifying%20the%20specific%20design%20rules%20they%20violate%20could%20help%20protect%20readers%20and%20reduce%20the%20spread%20of%20misinformation.%20However%2C%20the%20training%20and%20evaluation%20of%20AI%20models%20has%20been%20limited%20by%20the%20absence%20of%20large%2C%20diverse%2C%20and%20openly%20available%20datasets.%20In%20this%20work%2C%20we%20introduce%20Misviz%2C%20a%20benchmark%20of%202%2C604%20real-world%20visualizations%20annotated%20with%2012%20types%20of%20misleaders.%20To%20support%20model%20training%2C%20we%20also%20create%20Misviz-synth%2C%20a%20synthetic%20dataset%20of%2057%2C665%20visualizations%20generated%20using%20Matplotlib%20and%20based%20on%20real-world%20data%20tables.%20We%20perform%20a%20comprehensive%20evaluation%20on%20both%20datasets%20using%20state-of-the-art%20MLLMs%2C%20rule-based%20systems%2C%20and%20image-axis%20classifiers.%20Our%20results%20reveal%20that%20the%20task%20remains%20highly%20challenging.%20We%20release%20Misviz%2C%20Misviz-synth%2C%20and%20the%20accompanying%20code.&entry.1838667208=http%3A//arxiv.org/abs/2508.21675v2&entry.124074799=Read"},
{"title": "Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates", "author": "Denise M. Case", "abstract": "Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural constraints imposed on ontological design by this requirement. Building on a neutrality framework that treats interpretive non-commitment and stability under extension as explicit design constraints, we ask what minimal ontological structure is forced if accountability relationships are to remain referable and comparable under disagreement. Minimality here is not mere parsimony: a reduction is admissible only if it does not reintroduce stability-critical distinctions as hidden roles, flags, or contextual predicates. We establish a conditional lower-bound result: any ontology capable of supporting accountability under persistent disagreement must realize at least six distinct identity-and-persistence regimes. We further show that a construction with exactly six such regimes is sufficient to satisfy the stated requirements without embedding causal or normative commitments in the substrate. The result is not a proposal for a universal ontology, but a constraint on what is possible when neutrality and stable reference are treated as non-negotiable design goals.", "link": "http://arxiv.org/abs/2601.16152v1", "date": "2026-01-22", "relevancy": 1.5317, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4036}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Substrate%20Stability%20Under%20Persistent%20Disagreement%3A%20Structural%20Constraints%20for%20Neutral%20Ontological%20Substrates&body=Title%3A%20Substrate%20Stability%20Under%20Persistent%20Disagreement%3A%20Structural%20Constraints%20for%20Neutral%20Ontological%20Substrates%0AAuthor%3A%20Denise%20M.%20Case%0AAbstract%3A%20Modern%20data%20systems%20increasingly%20operate%20under%20conditions%20of%20persistent%20legal%2C%20political%2C%20and%20analytic%20disagreement.%20In%20such%20settings%2C%20interoperability%20cannot%20rely%20on%20shared%20interpretation%2C%20negotiated%20semantics%2C%20or%20centralized%20authority.%20Instead%2C%20representations%20must%20function%20as%20neutral%20substrates%20that%20preserve%20stable%20reference%20across%20incompatible%20extensions.%20This%20paper%20investigates%20the%20structural%20constraints%20imposed%20on%20ontological%20design%20by%20this%20requirement.%20Building%20on%20a%20neutrality%20framework%20that%20treats%20interpretive%20non-commitment%20and%20stability%20under%20extension%20as%20explicit%20design%20constraints%2C%20we%20ask%20what%20minimal%20ontological%20structure%20is%20forced%20if%20accountability%20relationships%20are%20to%20remain%20referable%20and%20comparable%20under%20disagreement.%20Minimality%20here%20is%20not%20mere%20parsimony%3A%20a%20reduction%20is%20admissible%20only%20if%20it%20does%20not%20reintroduce%20stability-critical%20distinctions%20as%20hidden%20roles%2C%20flags%2C%20or%20contextual%20predicates.%20We%20establish%20a%20conditional%20lower-bound%20result%3A%20any%20ontology%20capable%20of%20supporting%20accountability%20under%20persistent%20disagreement%20must%20realize%20at%20least%20six%20distinct%20identity-and-persistence%20regimes.%20We%20further%20show%20that%20a%20construction%20with%20exactly%20six%20such%20regimes%20is%20sufficient%20to%20satisfy%20the%20stated%20requirements%20without%20embedding%20causal%20or%20normative%20commitments%20in%20the%20substrate.%20The%20result%20is%20not%20a%20proposal%20for%20a%20universal%20ontology%2C%20but%20a%20constraint%20on%20what%20is%20possible%20when%20neutrality%20and%20stable%20reference%20are%20treated%20as%20non-negotiable%20design%20goals.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubstrate%2520Stability%2520Under%2520Persistent%2520Disagreement%253A%2520Structural%2520Constraints%2520for%2520Neutral%2520Ontological%2520Substrates%26entry.906535625%3DDenise%2520M.%2520Case%26entry.1292438233%3DModern%2520data%2520systems%2520increasingly%2520operate%2520under%2520conditions%2520of%2520persistent%2520legal%252C%2520political%252C%2520and%2520analytic%2520disagreement.%2520In%2520such%2520settings%252C%2520interoperability%2520cannot%2520rely%2520on%2520shared%2520interpretation%252C%2520negotiated%2520semantics%252C%2520or%2520centralized%2520authority.%2520Instead%252C%2520representations%2520must%2520function%2520as%2520neutral%2520substrates%2520that%2520preserve%2520stable%2520reference%2520across%2520incompatible%2520extensions.%2520This%2520paper%2520investigates%2520the%2520structural%2520constraints%2520imposed%2520on%2520ontological%2520design%2520by%2520this%2520requirement.%2520Building%2520on%2520a%2520neutrality%2520framework%2520that%2520treats%2520interpretive%2520non-commitment%2520and%2520stability%2520under%2520extension%2520as%2520explicit%2520design%2520constraints%252C%2520we%2520ask%2520what%2520minimal%2520ontological%2520structure%2520is%2520forced%2520if%2520accountability%2520relationships%2520are%2520to%2520remain%2520referable%2520and%2520comparable%2520under%2520disagreement.%2520Minimality%2520here%2520is%2520not%2520mere%2520parsimony%253A%2520a%2520reduction%2520is%2520admissible%2520only%2520if%2520it%2520does%2520not%2520reintroduce%2520stability-critical%2520distinctions%2520as%2520hidden%2520roles%252C%2520flags%252C%2520or%2520contextual%2520predicates.%2520We%2520establish%2520a%2520conditional%2520lower-bound%2520result%253A%2520any%2520ontology%2520capable%2520of%2520supporting%2520accountability%2520under%2520persistent%2520disagreement%2520must%2520realize%2520at%2520least%2520six%2520distinct%2520identity-and-persistence%2520regimes.%2520We%2520further%2520show%2520that%2520a%2520construction%2520with%2520exactly%2520six%2520such%2520regimes%2520is%2520sufficient%2520to%2520satisfy%2520the%2520stated%2520requirements%2520without%2520embedding%2520causal%2520or%2520normative%2520commitments%2520in%2520the%2520substrate.%2520The%2520result%2520is%2520not%2520a%2520proposal%2520for%2520a%2520universal%2520ontology%252C%2520but%2520a%2520constraint%2520on%2520what%2520is%2520possible%2520when%2520neutrality%2520and%2520stable%2520reference%2520are%2520treated%2520as%2520non-negotiable%2520design%2520goals.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Substrate%20Stability%20Under%20Persistent%20Disagreement%3A%20Structural%20Constraints%20for%20Neutral%20Ontological%20Substrates&entry.906535625=Denise%20M.%20Case&entry.1292438233=Modern%20data%20systems%20increasingly%20operate%20under%20conditions%20of%20persistent%20legal%2C%20political%2C%20and%20analytic%20disagreement.%20In%20such%20settings%2C%20interoperability%20cannot%20rely%20on%20shared%20interpretation%2C%20negotiated%20semantics%2C%20or%20centralized%20authority.%20Instead%2C%20representations%20must%20function%20as%20neutral%20substrates%20that%20preserve%20stable%20reference%20across%20incompatible%20extensions.%20This%20paper%20investigates%20the%20structural%20constraints%20imposed%20on%20ontological%20design%20by%20this%20requirement.%20Building%20on%20a%20neutrality%20framework%20that%20treats%20interpretive%20non-commitment%20and%20stability%20under%20extension%20as%20explicit%20design%20constraints%2C%20we%20ask%20what%20minimal%20ontological%20structure%20is%20forced%20if%20accountability%20relationships%20are%20to%20remain%20referable%20and%20comparable%20under%20disagreement.%20Minimality%20here%20is%20not%20mere%20parsimony%3A%20a%20reduction%20is%20admissible%20only%20if%20it%20does%20not%20reintroduce%20stability-critical%20distinctions%20as%20hidden%20roles%2C%20flags%2C%20or%20contextual%20predicates.%20We%20establish%20a%20conditional%20lower-bound%20result%3A%20any%20ontology%20capable%20of%20supporting%20accountability%20under%20persistent%20disagreement%20must%20realize%20at%20least%20six%20distinct%20identity-and-persistence%20regimes.%20We%20further%20show%20that%20a%20construction%20with%20exactly%20six%20such%20regimes%20is%20sufficient%20to%20satisfy%20the%20stated%20requirements%20without%20embedding%20causal%20or%20normative%20commitments%20in%20the%20substrate.%20The%20result%20is%20not%20a%20proposal%20for%20a%20universal%20ontology%2C%20but%20a%20constraint%20on%20what%20is%20possible%20when%20neutrality%20and%20stable%20reference%20are%20treated%20as%20non-negotiable%20design%20goals.&entry.1838667208=http%3A//arxiv.org/abs/2601.16152v1&entry.124074799=Read"},
{"title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap", "author": "Elisabeth J\u00fcttner and Janelle Pfeifer and Leona Krath and Stefan Korfhage and Hannah Dr\u00f6ge and Matthias B. Hullin and Markus Plack", "abstract": "Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.", "link": "http://arxiv.org/abs/2510.23494v2", "date": "2026-01-22", "relevancy": 1.8612, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6423}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.623}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yesnt%3A%20Are%20Diffusion%20Relighting%20Models%20Ready%20for%20Capture%20Stage%20Compositing%3F%20A%20Hybrid%20Alternative%20to%20Bridge%20the%20Gap&body=Title%3A%20Yesnt%3A%20Are%20Diffusion%20Relighting%20Models%20Ready%20for%20Capture%20Stage%20Compositing%3F%20A%20Hybrid%20Alternative%20to%20Bridge%20the%20Gap%0AAuthor%3A%20Elisabeth%20J%C3%BCttner%20and%20Janelle%20Pfeifer%20and%20Leona%20Krath%20and%20Stefan%20Korfhage%20and%20Hannah%20Dr%C3%B6ge%20and%20Matthias%20B.%20Hullin%20and%20Markus%20Plack%0AAbstract%3A%20Volumetric%20video%20relighting%20is%20essential%20for%20bringing%20captured%20performances%20into%20virtual%20worlds%2C%20but%20current%20approaches%20struggle%20to%20deliver%20temporally%20stable%2C%20production-ready%20results.%20Diffusion-based%20intrinsic%20decomposition%20methods%20show%20promise%20for%20single%20frames%2C%20yet%20suffer%20from%20stochastic%20noise%20and%20instability%20when%20extended%20to%20sequences%2C%20while%20video%20diffusion%20models%20remain%20constrained%20by%20memory%20and%20scale.%20We%20propose%20a%20hybrid%20relighting%20framework%20that%20combines%20diffusion-derived%20material%20priors%20with%20temporal%20regularization%20and%20physically%20motivated%20rendering.%20Our%20method%20aggregates%20multiple%20stochastic%20estimates%20of%20per-frame%20material%20properties%20into%20temporally%20consistent%20shading%20components%2C%20using%20optical-flow-guided%20regularization.%20For%20indirect%20effects%20such%20as%20shadows%20and%20reflections%2C%20we%20extract%20a%20mesh%20proxy%20from%20Gaussian%20Opacity%20Fields%20and%20render%20it%20within%20a%20standard%20graphics%20pipeline.%20Experiments%20on%20real%20and%20synthetic%20captures%20show%20that%20this%20hybrid%20strategy%20achieves%20substantially%20more%20stable%20relighting%20across%20sequences%20than%20diffusion-only%20baselines%2C%20while%20scaling%20beyond%20the%20clip%20lengths%20feasible%20for%20video%20diffusion.%20These%20results%20indicate%20that%20hybrid%20approaches%2C%20which%20balance%20learned%20priors%20with%20physically%20grounded%20constraints%2C%20are%20a%20practical%20step%20toward%20production-ready%20volumetric%20video%20relighting.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYesnt%253A%2520Are%2520Diffusion%2520Relighting%2520Models%2520Ready%2520for%2520Capture%2520Stage%2520Compositing%253F%2520A%2520Hybrid%2520Alternative%2520to%2520Bridge%2520the%2520Gap%26entry.906535625%3DElisabeth%2520J%25C3%25BCttner%2520and%2520Janelle%2520Pfeifer%2520and%2520Leona%2520Krath%2520and%2520Stefan%2520Korfhage%2520and%2520Hannah%2520Dr%25C3%25B6ge%2520and%2520Matthias%2520B.%2520Hullin%2520and%2520Markus%2520Plack%26entry.1292438233%3DVolumetric%2520video%2520relighting%2520is%2520essential%2520for%2520bringing%2520captured%2520performances%2520into%2520virtual%2520worlds%252C%2520but%2520current%2520approaches%2520struggle%2520to%2520deliver%2520temporally%2520stable%252C%2520production-ready%2520results.%2520Diffusion-based%2520intrinsic%2520decomposition%2520methods%2520show%2520promise%2520for%2520single%2520frames%252C%2520yet%2520suffer%2520from%2520stochastic%2520noise%2520and%2520instability%2520when%2520extended%2520to%2520sequences%252C%2520while%2520video%2520diffusion%2520models%2520remain%2520constrained%2520by%2520memory%2520and%2520scale.%2520We%2520propose%2520a%2520hybrid%2520relighting%2520framework%2520that%2520combines%2520diffusion-derived%2520material%2520priors%2520with%2520temporal%2520regularization%2520and%2520physically%2520motivated%2520rendering.%2520Our%2520method%2520aggregates%2520multiple%2520stochastic%2520estimates%2520of%2520per-frame%2520material%2520properties%2520into%2520temporally%2520consistent%2520shading%2520components%252C%2520using%2520optical-flow-guided%2520regularization.%2520For%2520indirect%2520effects%2520such%2520as%2520shadows%2520and%2520reflections%252C%2520we%2520extract%2520a%2520mesh%2520proxy%2520from%2520Gaussian%2520Opacity%2520Fields%2520and%2520render%2520it%2520within%2520a%2520standard%2520graphics%2520pipeline.%2520Experiments%2520on%2520real%2520and%2520synthetic%2520captures%2520show%2520that%2520this%2520hybrid%2520strategy%2520achieves%2520substantially%2520more%2520stable%2520relighting%2520across%2520sequences%2520than%2520diffusion-only%2520baselines%252C%2520while%2520scaling%2520beyond%2520the%2520clip%2520lengths%2520feasible%2520for%2520video%2520diffusion.%2520These%2520results%2520indicate%2520that%2520hybrid%2520approaches%252C%2520which%2520balance%2520learned%2520priors%2520with%2520physically%2520grounded%2520constraints%252C%2520are%2520a%2520practical%2520step%2520toward%2520production-ready%2520volumetric%2520video%2520relighting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yesnt%3A%20Are%20Diffusion%20Relighting%20Models%20Ready%20for%20Capture%20Stage%20Compositing%3F%20A%20Hybrid%20Alternative%20to%20Bridge%20the%20Gap&entry.906535625=Elisabeth%20J%C3%BCttner%20and%20Janelle%20Pfeifer%20and%20Leona%20Krath%20and%20Stefan%20Korfhage%20and%20Hannah%20Dr%C3%B6ge%20and%20Matthias%20B.%20Hullin%20and%20Markus%20Plack&entry.1292438233=Volumetric%20video%20relighting%20is%20essential%20for%20bringing%20captured%20performances%20into%20virtual%20worlds%2C%20but%20current%20approaches%20struggle%20to%20deliver%20temporally%20stable%2C%20production-ready%20results.%20Diffusion-based%20intrinsic%20decomposition%20methods%20show%20promise%20for%20single%20frames%2C%20yet%20suffer%20from%20stochastic%20noise%20and%20instability%20when%20extended%20to%20sequences%2C%20while%20video%20diffusion%20models%20remain%20constrained%20by%20memory%20and%20scale.%20We%20propose%20a%20hybrid%20relighting%20framework%20that%20combines%20diffusion-derived%20material%20priors%20with%20temporal%20regularization%20and%20physically%20motivated%20rendering.%20Our%20method%20aggregates%20multiple%20stochastic%20estimates%20of%20per-frame%20material%20properties%20into%20temporally%20consistent%20shading%20components%2C%20using%20optical-flow-guided%20regularization.%20For%20indirect%20effects%20such%20as%20shadows%20and%20reflections%2C%20we%20extract%20a%20mesh%20proxy%20from%20Gaussian%20Opacity%20Fields%20and%20render%20it%20within%20a%20standard%20graphics%20pipeline.%20Experiments%20on%20real%20and%20synthetic%20captures%20show%20that%20this%20hybrid%20strategy%20achieves%20substantially%20more%20stable%20relighting%20across%20sequences%20than%20diffusion-only%20baselines%2C%20while%20scaling%20beyond%20the%20clip%20lengths%20feasible%20for%20video%20diffusion.%20These%20results%20indicate%20that%20hybrid%20approaches%2C%20which%20balance%20learned%20priors%20with%20physically%20grounded%20constraints%2C%20are%20a%20practical%20step%20toward%20production-ready%20volumetric%20video%20relighting.&entry.1838667208=http%3A//arxiv.org/abs/2510.23494v2&entry.124074799=Read"},
{"title": "360Anything: Geometry-Free Lifting of Images and Videos to 360\u00b0", "author": "Ziyi Wu and Daniel Watson and Andrea Tagliasacchi and David J. Fleet and Marcus A. Brubaker and Saurabh Saxena", "abstract": "Lifting perspective images and videos to 360\u00b0 panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360\u00b0 generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.", "link": "http://arxiv.org/abs/2601.16192v1", "date": "2026-01-22", "relevancy": 1.8825, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6399}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6302}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20360Anything%3A%20Geometry-Free%20Lifting%20of%20Images%20and%20Videos%20to%20360%C2%B0&body=Title%3A%20360Anything%3A%20Geometry-Free%20Lifting%20of%20Images%20and%20Videos%20to%20360%C2%B0%0AAuthor%3A%20Ziyi%20Wu%20and%20Daniel%20Watson%20and%20Andrea%20Tagliasacchi%20and%20David%20J.%20Fleet%20and%20Marcus%20A.%20Brubaker%20and%20Saurabh%20Saxena%0AAbstract%3A%20Lifting%20perspective%20images%20and%20videos%20to%20360%C2%B0%20panoramas%20enables%20immersive%203D%20world%20generation.%20Existing%20approaches%20often%20rely%20on%20explicit%20geometric%20alignment%20between%20the%20perspective%20and%20the%20equirectangular%20projection%20%28ERP%29%20space.%20Yet%2C%20this%20requires%20known%20camera%20metadata%2C%20obscuring%20the%20application%20to%20in-the-wild%20data%20where%20such%20calibration%20is%20typically%20absent%20or%20noisy.%20We%20propose%20360Anything%2C%20a%20geometry-free%20framework%20built%20upon%20pre-trained%20diffusion%20transformers.%20By%20treating%20the%20perspective%20input%20and%20the%20panorama%20target%20simply%20as%20token%20sequences%2C%20360Anything%20learns%20the%20perspective-to-equirectangular%20mapping%20in%20a%20purely%20data-driven%20way%2C%20eliminating%20the%20need%20for%20camera%20information.%20Our%20approach%20achieves%20state-of-the-art%20performance%20on%20both%20image%20and%20video%20perspective-to-360%C2%B0%20generation%2C%20outperforming%20prior%20works%20that%20use%20ground-truth%20camera%20information.%20We%20also%20trace%20the%20root%20cause%20of%20the%20seam%20artifacts%20at%20ERP%20boundaries%20to%20zero-padding%20in%20the%20VAE%20encoder%2C%20and%20introduce%20Circular%20Latent%20Encoding%20to%20facilitate%20seamless%20generation.%20Finally%2C%20we%20show%20competitive%20results%20in%20zero-shot%20camera%20FoV%20and%20orientation%20estimation%20benchmarks%2C%20demonstrating%20360Anything%27s%20deep%20geometric%20understanding%20and%20broader%20utility%20in%20computer%20vision%20tasks.%20Additional%20results%20are%20available%20at%20https%3A//360anything.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D360Anything%253A%2520Geometry-Free%2520Lifting%2520of%2520Images%2520and%2520Videos%2520to%2520360%25C2%25B0%26entry.906535625%3DZiyi%2520Wu%2520and%2520Daniel%2520Watson%2520and%2520Andrea%2520Tagliasacchi%2520and%2520David%2520J.%2520Fleet%2520and%2520Marcus%2520A.%2520Brubaker%2520and%2520Saurabh%2520Saxena%26entry.1292438233%3DLifting%2520perspective%2520images%2520and%2520videos%2520to%2520360%25C2%25B0%2520panoramas%2520enables%2520immersive%25203D%2520world%2520generation.%2520Existing%2520approaches%2520often%2520rely%2520on%2520explicit%2520geometric%2520alignment%2520between%2520the%2520perspective%2520and%2520the%2520equirectangular%2520projection%2520%2528ERP%2529%2520space.%2520Yet%252C%2520this%2520requires%2520known%2520camera%2520metadata%252C%2520obscuring%2520the%2520application%2520to%2520in-the-wild%2520data%2520where%2520such%2520calibration%2520is%2520typically%2520absent%2520or%2520noisy.%2520We%2520propose%2520360Anything%252C%2520a%2520geometry-free%2520framework%2520built%2520upon%2520pre-trained%2520diffusion%2520transformers.%2520By%2520treating%2520the%2520perspective%2520input%2520and%2520the%2520panorama%2520target%2520simply%2520as%2520token%2520sequences%252C%2520360Anything%2520learns%2520the%2520perspective-to-equirectangular%2520mapping%2520in%2520a%2520purely%2520data-driven%2520way%252C%2520eliminating%2520the%2520need%2520for%2520camera%2520information.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%2520image%2520and%2520video%2520perspective-to-360%25C2%25B0%2520generation%252C%2520outperforming%2520prior%2520works%2520that%2520use%2520ground-truth%2520camera%2520information.%2520We%2520also%2520trace%2520the%2520root%2520cause%2520of%2520the%2520seam%2520artifacts%2520at%2520ERP%2520boundaries%2520to%2520zero-padding%2520in%2520the%2520VAE%2520encoder%252C%2520and%2520introduce%2520Circular%2520Latent%2520Encoding%2520to%2520facilitate%2520seamless%2520generation.%2520Finally%252C%2520we%2520show%2520competitive%2520results%2520in%2520zero-shot%2520camera%2520FoV%2520and%2520orientation%2520estimation%2520benchmarks%252C%2520demonstrating%2520360Anything%2527s%2520deep%2520geometric%2520understanding%2520and%2520broader%2520utility%2520in%2520computer%2520vision%2520tasks.%2520Additional%2520results%2520are%2520available%2520at%2520https%253A//360anything.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360Anything%3A%20Geometry-Free%20Lifting%20of%20Images%20and%20Videos%20to%20360%C2%B0&entry.906535625=Ziyi%20Wu%20and%20Daniel%20Watson%20and%20Andrea%20Tagliasacchi%20and%20David%20J.%20Fleet%20and%20Marcus%20A.%20Brubaker%20and%20Saurabh%20Saxena&entry.1292438233=Lifting%20perspective%20images%20and%20videos%20to%20360%C2%B0%20panoramas%20enables%20immersive%203D%20world%20generation.%20Existing%20approaches%20often%20rely%20on%20explicit%20geometric%20alignment%20between%20the%20perspective%20and%20the%20equirectangular%20projection%20%28ERP%29%20space.%20Yet%2C%20this%20requires%20known%20camera%20metadata%2C%20obscuring%20the%20application%20to%20in-the-wild%20data%20where%20such%20calibration%20is%20typically%20absent%20or%20noisy.%20We%20propose%20360Anything%2C%20a%20geometry-free%20framework%20built%20upon%20pre-trained%20diffusion%20transformers.%20By%20treating%20the%20perspective%20input%20and%20the%20panorama%20target%20simply%20as%20token%20sequences%2C%20360Anything%20learns%20the%20perspective-to-equirectangular%20mapping%20in%20a%20purely%20data-driven%20way%2C%20eliminating%20the%20need%20for%20camera%20information.%20Our%20approach%20achieves%20state-of-the-art%20performance%20on%20both%20image%20and%20video%20perspective-to-360%C2%B0%20generation%2C%20outperforming%20prior%20works%20that%20use%20ground-truth%20camera%20information.%20We%20also%20trace%20the%20root%20cause%20of%20the%20seam%20artifacts%20at%20ERP%20boundaries%20to%20zero-padding%20in%20the%20VAE%20encoder%2C%20and%20introduce%20Circular%20Latent%20Encoding%20to%20facilitate%20seamless%20generation.%20Finally%2C%20we%20show%20competitive%20results%20in%20zero-shot%20camera%20FoV%20and%20orientation%20estimation%20benchmarks%2C%20demonstrating%20360Anything%27s%20deep%20geometric%20understanding%20and%20broader%20utility%20in%20computer%20vision%20tasks.%20Additional%20results%20are%20available%20at%20https%3A//360anything.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2601.16192v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


