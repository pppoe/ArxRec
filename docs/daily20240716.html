<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240715.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Interactive Rendering of Relightable and Animatable Gaussian Avatars", "author": "Youyi Zhan and Tianjia Shao and He Wang and Yin Yang and Kun Zhou", "abstract": "  Creating relightable and animatable avatars from multi-view or monocular\nvideos is a challenging task for digital human creation and virtual reality\napplications. Previous methods rely on neural radiance fields or ray tracing,\nresulting in slow training and rendering processes. By utilizing Gaussian\nSplatting, we propose a simple and efficient method to decouple body materials\nand lighting from sparse-view or monocular avatar videos, so that the avatar\ncan be rendered simultaneously under novel viewpoints, poses, and lightings at\ninteractive frame rates (6.9 fps). Specifically, we first obtain the canonical\nbody mesh using a signed distance function and assign attributes to each mesh\nvertex. The Gaussians in the canonical space then interpolate from nearby body\nmesh vertices to obtain the attributes. We subsequently deform the Gaussians to\nthe posed space using forward skinning, and combine the learnable environment\nlight with the Gaussian attributes for shading computation. To achieve fast\nshadow modeling, we rasterize the posed body mesh from dense viewpoints to\nobtain the visibility. Our approach is not only simple but also fast enough to\nallow interactive rendering of avatar animation under environmental light\nchanges. Experiments demonstrate that, compared to previous works, our method\ncan render higher quality results at a faster speed on both synthetic and real\ndatasets.\n", "link": "http://arxiv.org/abs/2407.10707v1", "date": "2024-07-15", "relevancy": 3.5944, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7374}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7374}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Rendering%20of%20Relightable%20and%20Animatable%20Gaussian%20Avatars&body=Title%3A%20Interactive%20Rendering%20of%20Relightable%20and%20Animatable%20Gaussian%20Avatars%0AAuthor%3A%20Youyi%20Zhan%20and%20Tianjia%20Shao%20and%20He%20Wang%20and%20Yin%20Yang%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20Creating%20relightable%20and%20animatable%20avatars%20from%20multi-view%20or%20monocular%0Avideos%20is%20a%20challenging%20task%20for%20digital%20human%20creation%20and%20virtual%20reality%0Aapplications.%20Previous%20methods%20rely%20on%20neural%20radiance%20fields%20or%20ray%20tracing%2C%0Aresulting%20in%20slow%20training%20and%20rendering%20processes.%20By%20utilizing%20Gaussian%0ASplatting%2C%20we%20propose%20a%20simple%20and%20efficient%20method%20to%20decouple%20body%20materials%0Aand%20lighting%20from%20sparse-view%20or%20monocular%20avatar%20videos%2C%20so%20that%20the%20avatar%0Acan%20be%20rendered%20simultaneously%20under%20novel%20viewpoints%2C%20poses%2C%20and%20lightings%20at%0Ainteractive%20frame%20rates%20%286.9%20fps%29.%20Specifically%2C%20we%20first%20obtain%20the%20canonical%0Abody%20mesh%20using%20a%20signed%20distance%20function%20and%20assign%20attributes%20to%20each%20mesh%0Avertex.%20The%20Gaussians%20in%20the%20canonical%20space%20then%20interpolate%20from%20nearby%20body%0Amesh%20vertices%20to%20obtain%20the%20attributes.%20We%20subsequently%20deform%20the%20Gaussians%20to%0Athe%20posed%20space%20using%20forward%20skinning%2C%20and%20combine%20the%20learnable%20environment%0Alight%20with%20the%20Gaussian%20attributes%20for%20shading%20computation.%20To%20achieve%20fast%0Ashadow%20modeling%2C%20we%20rasterize%20the%20posed%20body%20mesh%20from%20dense%20viewpoints%20to%0Aobtain%20the%20visibility.%20Our%20approach%20is%20not%20only%20simple%20but%20also%20fast%20enough%20to%0Aallow%20interactive%20rendering%20of%20avatar%20animation%20under%20environmental%20light%0Achanges.%20Experiments%20demonstrate%20that%2C%20compared%20to%20previous%20works%2C%20our%20method%0Acan%20render%20higher%20quality%20results%20at%20a%20faster%20speed%20on%20both%20synthetic%20and%20real%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Rendering%2520of%2520Relightable%2520and%2520Animatable%2520Gaussian%2520Avatars%26entry.906535625%3DYouyi%2520Zhan%2520and%2520Tianjia%2520Shao%2520and%2520He%2520Wang%2520and%2520Yin%2520Yang%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520Creating%2520relightable%2520and%2520animatable%2520avatars%2520from%2520multi-view%2520or%2520monocular%250Avideos%2520is%2520a%2520challenging%2520task%2520for%2520digital%2520human%2520creation%2520and%2520virtual%2520reality%250Aapplications.%2520Previous%2520methods%2520rely%2520on%2520neural%2520radiance%2520fields%2520or%2520ray%2520tracing%252C%250Aresulting%2520in%2520slow%2520training%2520and%2520rendering%2520processes.%2520By%2520utilizing%2520Gaussian%250ASplatting%252C%2520we%2520propose%2520a%2520simple%2520and%2520efficient%2520method%2520to%2520decouple%2520body%2520materials%250Aand%2520lighting%2520from%2520sparse-view%2520or%2520monocular%2520avatar%2520videos%252C%2520so%2520that%2520the%2520avatar%250Acan%2520be%2520rendered%2520simultaneously%2520under%2520novel%2520viewpoints%252C%2520poses%252C%2520and%2520lightings%2520at%250Ainteractive%2520frame%2520rates%2520%25286.9%2520fps%2529.%2520Specifically%252C%2520we%2520first%2520obtain%2520the%2520canonical%250Abody%2520mesh%2520using%2520a%2520signed%2520distance%2520function%2520and%2520assign%2520attributes%2520to%2520each%2520mesh%250Avertex.%2520The%2520Gaussians%2520in%2520the%2520canonical%2520space%2520then%2520interpolate%2520from%2520nearby%2520body%250Amesh%2520vertices%2520to%2520obtain%2520the%2520attributes.%2520We%2520subsequently%2520deform%2520the%2520Gaussians%2520to%250Athe%2520posed%2520space%2520using%2520forward%2520skinning%252C%2520and%2520combine%2520the%2520learnable%2520environment%250Alight%2520with%2520the%2520Gaussian%2520attributes%2520for%2520shading%2520computation.%2520To%2520achieve%2520fast%250Ashadow%2520modeling%252C%2520we%2520rasterize%2520the%2520posed%2520body%2520mesh%2520from%2520dense%2520viewpoints%2520to%250Aobtain%2520the%2520visibility.%2520Our%2520approach%2520is%2520not%2520only%2520simple%2520but%2520also%2520fast%2520enough%2520to%250Aallow%2520interactive%2520rendering%2520of%2520avatar%2520animation%2520under%2520environmental%2520light%250Achanges.%2520Experiments%2520demonstrate%2520that%252C%2520compared%2520to%2520previous%2520works%252C%2520our%2520method%250Acan%2520render%2520higher%2520quality%2520results%2520at%2520a%2520faster%2520speed%2520on%2520both%2520synthetic%2520and%2520real%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Rendering%20of%20Relightable%20and%20Animatable%20Gaussian%20Avatars&entry.906535625=Youyi%20Zhan%20and%20Tianjia%20Shao%20and%20He%20Wang%20and%20Yin%20Yang%20and%20Kun%20Zhou&entry.1292438233=%20%20Creating%20relightable%20and%20animatable%20avatars%20from%20multi-view%20or%20monocular%0Avideos%20is%20a%20challenging%20task%20for%20digital%20human%20creation%20and%20virtual%20reality%0Aapplications.%20Previous%20methods%20rely%20on%20neural%20radiance%20fields%20or%20ray%20tracing%2C%0Aresulting%20in%20slow%20training%20and%20rendering%20processes.%20By%20utilizing%20Gaussian%0ASplatting%2C%20we%20propose%20a%20simple%20and%20efficient%20method%20to%20decouple%20body%20materials%0Aand%20lighting%20from%20sparse-view%20or%20monocular%20avatar%20videos%2C%20so%20that%20the%20avatar%0Acan%20be%20rendered%20simultaneously%20under%20novel%20viewpoints%2C%20poses%2C%20and%20lightings%20at%0Ainteractive%20frame%20rates%20%286.9%20fps%29.%20Specifically%2C%20we%20first%20obtain%20the%20canonical%0Abody%20mesh%20using%20a%20signed%20distance%20function%20and%20assign%20attributes%20to%20each%20mesh%0Avertex.%20The%20Gaussians%20in%20the%20canonical%20space%20then%20interpolate%20from%20nearby%20body%0Amesh%20vertices%20to%20obtain%20the%20attributes.%20We%20subsequently%20deform%20the%20Gaussians%20to%0Athe%20posed%20space%20using%20forward%20skinning%2C%20and%20combine%20the%20learnable%20environment%0Alight%20with%20the%20Gaussian%20attributes%20for%20shading%20computation.%20To%20achieve%20fast%0Ashadow%20modeling%2C%20we%20rasterize%20the%20posed%20body%20mesh%20from%20dense%20viewpoints%20to%0Aobtain%20the%20visibility.%20Our%20approach%20is%20not%20only%20simple%20but%20also%20fast%20enough%20to%0Aallow%20interactive%20rendering%20of%20avatar%20animation%20under%20environmental%20light%0Achanges.%20Experiments%20demonstrate%20that%2C%20compared%20to%20previous%20works%2C%20our%20method%0Acan%20render%20higher%20quality%20results%20at%20a%20faster%20speed%20on%20both%20synthetic%20and%20real%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10707v1&entry.124074799=Read"},
{"title": "MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from\n  Multi-View Stereo", "author": "Tianqi Liu and Guangcong Wang and Shoukang Hu and Liao Shen and Xinyi Ye and Yuhang Zang and Zhiguo Cao and Wei Li and Ziwei Liu", "abstract": "  We present MVSGaussian, a new generalizable 3D Gaussian representation\napproach derived from Multi-View Stereo (MVS) that can efficiently reconstruct\nunseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware\nGaussian representations and decode them into Gaussian parameters. 2) To\nfurther enhance performance, we propose a hybrid Gaussian rendering that\nintegrates an efficient volume rendering design for novel view synthesis. 3) To\nsupport fast fine-tuning for specific scenes, we introduce a multi-view\ngeometric consistent aggregation strategy to effectively aggregate the point\nclouds generated by the generalizable model, serving as the initialization for\nper-scene optimization. Compared with previous generalizable NeRF-based\nmethods, which typically require minutes of fine-tuning and seconds of\nrendering per image, MVSGaussian achieves real-time rendering with better\nsynthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian\nachieves better view synthesis with less training computational cost. Extensive\nexperiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples\ndatasets validate that MVSGaussian attains state-of-the-art performance with\nconvincing generalizability, real-time rendering speed, and fast per-scene\noptimization.\n", "link": "http://arxiv.org/abs/2405.12218v3", "date": "2024-07-15", "relevancy": 3.3985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7315}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.699}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVSGaussian%3A%20Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%0A%20%20Multi-View%20Stereo&body=Title%3A%20MVSGaussian%3A%20Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%0A%20%20Multi-View%20Stereo%0AAuthor%3A%20Tianqi%20Liu%20and%20Guangcong%20Wang%20and%20Shoukang%20Hu%20and%20Liao%20Shen%20and%20Xinyi%20Ye%20and%20Yuhang%20Zang%20and%20Zhiguo%20Cao%20and%20Wei%20Li%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20We%20present%20MVSGaussian%2C%20a%20new%20generalizable%203D%20Gaussian%20representation%0Aapproach%20derived%20from%20Multi-View%20Stereo%20%28MVS%29%20that%20can%20efficiently%20reconstruct%0Aunseen%20scenes.%20Specifically%2C%201%29%20we%20leverage%20MVS%20to%20encode%20geometry-aware%0AGaussian%20representations%20and%20decode%20them%20into%20Gaussian%20parameters.%202%29%20To%0Afurther%20enhance%20performance%2C%20we%20propose%20a%20hybrid%20Gaussian%20rendering%20that%0Aintegrates%20an%20efficient%20volume%20rendering%20design%20for%20novel%20view%20synthesis.%203%29%20To%0Asupport%20fast%20fine-tuning%20for%20specific%20scenes%2C%20we%20introduce%20a%20multi-view%0Ageometric%20consistent%20aggregation%20strategy%20to%20effectively%20aggregate%20the%20point%0Aclouds%20generated%20by%20the%20generalizable%20model%2C%20serving%20as%20the%20initialization%20for%0Aper-scene%20optimization.%20Compared%20with%20previous%20generalizable%20NeRF-based%0Amethods%2C%20which%20typically%20require%20minutes%20of%20fine-tuning%20and%20seconds%20of%0Arendering%20per%20image%2C%20MVSGaussian%20achieves%20real-time%20rendering%20with%20better%0Asynthesis%20quality%20for%20each%20scene.%20Compared%20with%20the%20vanilla%203D-GS%2C%20MVSGaussian%0Aachieves%20better%20view%20synthesis%20with%20less%20training%20computational%20cost.%20Extensive%0Aexperiments%20on%20DTU%2C%20Real%20Forward-facing%2C%20NeRF%20Synthetic%2C%20and%20Tanks%20and%20Temples%0Adatasets%20validate%20that%20MVSGaussian%20attains%20state-of-the-art%20performance%20with%0Aconvincing%20generalizability%2C%20real-time%20rendering%20speed%2C%20and%20fast%20per-scene%0Aoptimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12218v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVSGaussian%253A%2520Fast%2520Generalizable%2520Gaussian%2520Splatting%2520Reconstruction%2520from%250A%2520%2520Multi-View%2520Stereo%26entry.906535625%3DTianqi%2520Liu%2520and%2520Guangcong%2520Wang%2520and%2520Shoukang%2520Hu%2520and%2520Liao%2520Shen%2520and%2520Xinyi%2520Ye%2520and%2520Yuhang%2520Zang%2520and%2520Zhiguo%2520Cao%2520and%2520Wei%2520Li%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520MVSGaussian%252C%2520a%2520new%2520generalizable%25203D%2520Gaussian%2520representation%250Aapproach%2520derived%2520from%2520Multi-View%2520Stereo%2520%2528MVS%2529%2520that%2520can%2520efficiently%2520reconstruct%250Aunseen%2520scenes.%2520Specifically%252C%25201%2529%2520we%2520leverage%2520MVS%2520to%2520encode%2520geometry-aware%250AGaussian%2520representations%2520and%2520decode%2520them%2520into%2520Gaussian%2520parameters.%25202%2529%2520To%250Afurther%2520enhance%2520performance%252C%2520we%2520propose%2520a%2520hybrid%2520Gaussian%2520rendering%2520that%250Aintegrates%2520an%2520efficient%2520volume%2520rendering%2520design%2520for%2520novel%2520view%2520synthesis.%25203%2529%2520To%250Asupport%2520fast%2520fine-tuning%2520for%2520specific%2520scenes%252C%2520we%2520introduce%2520a%2520multi-view%250Ageometric%2520consistent%2520aggregation%2520strategy%2520to%2520effectively%2520aggregate%2520the%2520point%250Aclouds%2520generated%2520by%2520the%2520generalizable%2520model%252C%2520serving%2520as%2520the%2520initialization%2520for%250Aper-scene%2520optimization.%2520Compared%2520with%2520previous%2520generalizable%2520NeRF-based%250Amethods%252C%2520which%2520typically%2520require%2520minutes%2520of%2520fine-tuning%2520and%2520seconds%2520of%250Arendering%2520per%2520image%252C%2520MVSGaussian%2520achieves%2520real-time%2520rendering%2520with%2520better%250Asynthesis%2520quality%2520for%2520each%2520scene.%2520Compared%2520with%2520the%2520vanilla%25203D-GS%252C%2520MVSGaussian%250Aachieves%2520better%2520view%2520synthesis%2520with%2520less%2520training%2520computational%2520cost.%2520Extensive%250Aexperiments%2520on%2520DTU%252C%2520Real%2520Forward-facing%252C%2520NeRF%2520Synthetic%252C%2520and%2520Tanks%2520and%2520Temples%250Adatasets%2520validate%2520that%2520MVSGaussian%2520attains%2520state-of-the-art%2520performance%2520with%250Aconvincing%2520generalizability%252C%2520real-time%2520rendering%2520speed%252C%2520and%2520fast%2520per-scene%250Aoptimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12218v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVSGaussian%3A%20Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%0A%20%20Multi-View%20Stereo&entry.906535625=Tianqi%20Liu%20and%20Guangcong%20Wang%20and%20Shoukang%20Hu%20and%20Liao%20Shen%20and%20Xinyi%20Ye%20and%20Yuhang%20Zang%20and%20Zhiguo%20Cao%20and%20Wei%20Li%20and%20Ziwei%20Liu&entry.1292438233=%20%20We%20present%20MVSGaussian%2C%20a%20new%20generalizable%203D%20Gaussian%20representation%0Aapproach%20derived%20from%20Multi-View%20Stereo%20%28MVS%29%20that%20can%20efficiently%20reconstruct%0Aunseen%20scenes.%20Specifically%2C%201%29%20we%20leverage%20MVS%20to%20encode%20geometry-aware%0AGaussian%20representations%20and%20decode%20them%20into%20Gaussian%20parameters.%202%29%20To%0Afurther%20enhance%20performance%2C%20we%20propose%20a%20hybrid%20Gaussian%20rendering%20that%0Aintegrates%20an%20efficient%20volume%20rendering%20design%20for%20novel%20view%20synthesis.%203%29%20To%0Asupport%20fast%20fine-tuning%20for%20specific%20scenes%2C%20we%20introduce%20a%20multi-view%0Ageometric%20consistent%20aggregation%20strategy%20to%20effectively%20aggregate%20the%20point%0Aclouds%20generated%20by%20the%20generalizable%20model%2C%20serving%20as%20the%20initialization%20for%0Aper-scene%20optimization.%20Compared%20with%20previous%20generalizable%20NeRF-based%0Amethods%2C%20which%20typically%20require%20minutes%20of%20fine-tuning%20and%20seconds%20of%0Arendering%20per%20image%2C%20MVSGaussian%20achieves%20real-time%20rendering%20with%20better%0Asynthesis%20quality%20for%20each%20scene.%20Compared%20with%20the%20vanilla%203D-GS%2C%20MVSGaussian%0Aachieves%20better%20view%20synthesis%20with%20less%20training%20computational%20cost.%20Extensive%0Aexperiments%20on%20DTU%2C%20Real%20Forward-facing%2C%20NeRF%20Synthetic%2C%20and%20Tanks%20and%20Temples%0Adatasets%20validate%20that%20MVSGaussian%20attains%20state-of-the-art%20performance%20with%0Aconvincing%20generalizability%2C%20real-time%20rendering%20speed%2C%20and%20fast%20per-scene%0Aoptimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12218v3&entry.124074799=Read"},
{"title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering", "author": "Guanjun Wu and Taoran Yi and Jiemin Fang and Lingxi Xie and Xiaopeng Zhang and Wei Wei and Wenyu Liu and Qi Tian and Xinggang Wang", "abstract": "  Representing and rendering dynamic scenes has been an important but\nchallenging task. Especially, to accurately model complex motions, high\nefficiency is usually hard to guarantee. To achieve real-time dynamic scene\nrendering while also enjoying high training and storage efficiency, we propose\n4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes\nrather than applying 3D-GS for each individual frame. In 4D-GS, a novel\nexplicit representation containing both 3D Gaussians and 4D neural voxels is\nproposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is\nproposed to efficiently build Gaussian features from 4D neural voxels and then\na lightweight MLP is applied to predict Gaussian deformations at novel\ntimestamps. Our 4D-GS method achieves real-time rendering under high\nresolutions, 82 FPS at an 800$\\times$800 resolution on an RTX 3090 GPU while\nmaintaining comparable or better quality than previous state-of-the-art\nmethods. More demos and code are available at\nhttps://guanjunwu.github.io/4dgs/.\n", "link": "http://arxiv.org/abs/2310.08528v3", "date": "2024-07-15", "relevancy": 3.3738, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7472}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6896}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D%20Gaussian%20Splatting%20for%20Real-Time%20Dynamic%20Scene%20Rendering&body=Title%3A%204D%20Gaussian%20Splatting%20for%20Real-Time%20Dynamic%20Scene%20Rendering%0AAuthor%3A%20Guanjun%20Wu%20and%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Wei%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Representing%20and%20rendering%20dynamic%20scenes%20has%20been%20an%20important%20but%0Achallenging%20task.%20Especially%2C%20to%20accurately%20model%20complex%20motions%2C%20high%0Aefficiency%20is%20usually%20hard%20to%20guarantee.%20To%20achieve%20real-time%20dynamic%20scene%0Arendering%20while%20also%20enjoying%20high%20training%20and%20storage%20efficiency%2C%20we%20propose%0A4D%20Gaussian%20Splatting%20%284D-GS%29%20as%20a%20holistic%20representation%20for%20dynamic%20scenes%0Arather%20than%20applying%203D-GS%20for%20each%20individual%20frame.%20In%204D-GS%2C%20a%20novel%0Aexplicit%20representation%20containing%20both%203D%20Gaussians%20and%204D%20neural%20voxels%20is%0Aproposed.%20A%20decomposed%20neural%20voxel%20encoding%20algorithm%20inspired%20by%20HexPlane%20is%0Aproposed%20to%20efficiently%20build%20Gaussian%20features%20from%204D%20neural%20voxels%20and%20then%0Aa%20lightweight%20MLP%20is%20applied%20to%20predict%20Gaussian%20deformations%20at%20novel%0Atimestamps.%20Our%204D-GS%20method%20achieves%20real-time%20rendering%20under%20high%0Aresolutions%2C%2082%20FPS%20at%20an%20800%24%5Ctimes%24800%20resolution%20on%20an%20RTX%203090%20GPU%20while%0Amaintaining%20comparable%20or%20better%20quality%20than%20previous%20state-of-the-art%0Amethods.%20More%20demos%20and%20code%20are%20available%20at%0Ahttps%3A//guanjunwu.github.io/4dgs/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08528v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D%2520Gaussian%2520Splatting%2520for%2520Real-Time%2520Dynamic%2520Scene%2520Rendering%26entry.906535625%3DGuanjun%2520Wu%2520and%2520Taoran%2520Yi%2520and%2520Jiemin%2520Fang%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wei%2520Wei%2520and%2520Wenyu%2520Liu%2520and%2520Qi%2520Tian%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Representing%2520and%2520rendering%2520dynamic%2520scenes%2520has%2520been%2520an%2520important%2520but%250Achallenging%2520task.%2520Especially%252C%2520to%2520accurately%2520model%2520complex%2520motions%252C%2520high%250Aefficiency%2520is%2520usually%2520hard%2520to%2520guarantee.%2520To%2520achieve%2520real-time%2520dynamic%2520scene%250Arendering%2520while%2520also%2520enjoying%2520high%2520training%2520and%2520storage%2520efficiency%252C%2520we%2520propose%250A4D%2520Gaussian%2520Splatting%2520%25284D-GS%2529%2520as%2520a%2520holistic%2520representation%2520for%2520dynamic%2520scenes%250Arather%2520than%2520applying%25203D-GS%2520for%2520each%2520individual%2520frame.%2520In%25204D-GS%252C%2520a%2520novel%250Aexplicit%2520representation%2520containing%2520both%25203D%2520Gaussians%2520and%25204D%2520neural%2520voxels%2520is%250Aproposed.%2520A%2520decomposed%2520neural%2520voxel%2520encoding%2520algorithm%2520inspired%2520by%2520HexPlane%2520is%250Aproposed%2520to%2520efficiently%2520build%2520Gaussian%2520features%2520from%25204D%2520neural%2520voxels%2520and%2520then%250Aa%2520lightweight%2520MLP%2520is%2520applied%2520to%2520predict%2520Gaussian%2520deformations%2520at%2520novel%250Atimestamps.%2520Our%25204D-GS%2520method%2520achieves%2520real-time%2520rendering%2520under%2520high%250Aresolutions%252C%252082%2520FPS%2520at%2520an%2520800%2524%255Ctimes%2524800%2520resolution%2520on%2520an%2520RTX%25203090%2520GPU%2520while%250Amaintaining%2520comparable%2520or%2520better%2520quality%2520than%2520previous%2520state-of-the-art%250Amethods.%2520More%2520demos%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//guanjunwu.github.io/4dgs/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08528v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Gaussian%20Splatting%20for%20Real-Time%20Dynamic%20Scene%20Rendering&entry.906535625=Guanjun%20Wu%20and%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Wei%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang&entry.1292438233=%20%20Representing%20and%20rendering%20dynamic%20scenes%20has%20been%20an%20important%20but%0Achallenging%20task.%20Especially%2C%20to%20accurately%20model%20complex%20motions%2C%20high%0Aefficiency%20is%20usually%20hard%20to%20guarantee.%20To%20achieve%20real-time%20dynamic%20scene%0Arendering%20while%20also%20enjoying%20high%20training%20and%20storage%20efficiency%2C%20we%20propose%0A4D%20Gaussian%20Splatting%20%284D-GS%29%20as%20a%20holistic%20representation%20for%20dynamic%20scenes%0Arather%20than%20applying%203D-GS%20for%20each%20individual%20frame.%20In%204D-GS%2C%20a%20novel%0Aexplicit%20representation%20containing%20both%203D%20Gaussians%20and%204D%20neural%20voxels%20is%0Aproposed.%20A%20decomposed%20neural%20voxel%20encoding%20algorithm%20inspired%20by%20HexPlane%20is%0Aproposed%20to%20efficiently%20build%20Gaussian%20features%20from%204D%20neural%20voxels%20and%20then%0Aa%20lightweight%20MLP%20is%20applied%20to%20predict%20Gaussian%20deformations%20at%20novel%0Atimestamps.%20Our%204D-GS%20method%20achieves%20real-time%20rendering%20under%20high%0Aresolutions%2C%2082%20FPS%20at%20an%20800%24%5Ctimes%24800%20resolution%20on%20an%20RTX%203090%20GPU%20while%0Amaintaining%20comparable%20or%20better%20quality%20than%20previous%20state-of-the-art%0Amethods.%20More%20demos%20and%20code%20are%20available%20at%0Ahttps%3A//guanjunwu.github.io/4dgs/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08528v3&entry.124074799=Read"},
{"title": "Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head\n  Capture", "author": "Xuanchen Li and Yuhao Cheng and Xingyu Ren and Haozhe Jia and Di Xu and Wenhan Zhu and Yichao Yan", "abstract": "  4D head capture aims to generate dynamic topological meshes and corresponding\ntexture maps from videos, which is widely utilized in movies and games for its\nability to simulate facial muscle movements and recover dynamic textures in\npore-squeezing. The industry often adopts the method involving multi-view\nstereo and non-rigid alignment. However, this approach is prone to errors and\nheavily reliant on time-consuming manual processing by artists. To simplify\nthis process, we propose Topo4D, a novel framework for automatic geometry and\ntexture generation, which optimizes densely aligned 4D heads and 8K texture\nmaps directly from calibrated multi-view time-series images. Specifically, we\nfirst represent the time-series faces as a set of dynamic 3D Gaussians with\nfixed topology in which the Gaussian centers are bound to the mesh vertices.\nAfterward, we perform alternative geometry and texture optimization\nframe-by-frame for high-quality geometry and texture learning while maintaining\ntemporal topology stability. Finally, we can extract dynamic facial meshes in\nregular wiring arrangement and high-fidelity textures with pore-level details\nfrom the learned Gaussians. Extensive experiments show that our method achieves\nsuperior results than the current SOTA face reconstruction methods both in the\nquality of meshes and textures. Project page:\nhttps://xuanchenli.github.io/Topo4D/.\n", "link": "http://arxiv.org/abs/2406.00440v3", "date": "2024-07-15", "relevancy": 3.2883, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6845}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6569}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topo4D%3A%20Topology-Preserving%20Gaussian%20Splatting%20for%20High-Fidelity%204D%20Head%0A%20%20Capture&body=Title%3A%20Topo4D%3A%20Topology-Preserving%20Gaussian%20Splatting%20for%20High-Fidelity%204D%20Head%0A%20%20Capture%0AAuthor%3A%20Xuanchen%20Li%20and%20Yuhao%20Cheng%20and%20Xingyu%20Ren%20and%20Haozhe%20Jia%20and%20Di%20Xu%20and%20Wenhan%20Zhu%20and%20Yichao%20Yan%0AAbstract%3A%20%20%204D%20head%20capture%20aims%20to%20generate%20dynamic%20topological%20meshes%20and%20corresponding%0Atexture%20maps%20from%20videos%2C%20which%20is%20widely%20utilized%20in%20movies%20and%20games%20for%20its%0Aability%20to%20simulate%20facial%20muscle%20movements%20and%20recover%20dynamic%20textures%20in%0Apore-squeezing.%20The%20industry%20often%20adopts%20the%20method%20involving%20multi-view%0Astereo%20and%20non-rigid%20alignment.%20However%2C%20this%20approach%20is%20prone%20to%20errors%20and%0Aheavily%20reliant%20on%20time-consuming%20manual%20processing%20by%20artists.%20To%20simplify%0Athis%20process%2C%20we%20propose%20Topo4D%2C%20a%20novel%20framework%20for%20automatic%20geometry%20and%0Atexture%20generation%2C%20which%20optimizes%20densely%20aligned%204D%20heads%20and%208K%20texture%0Amaps%20directly%20from%20calibrated%20multi-view%20time-series%20images.%20Specifically%2C%20we%0Afirst%20represent%20the%20time-series%20faces%20as%20a%20set%20of%20dynamic%203D%20Gaussians%20with%0Afixed%20topology%20in%20which%20the%20Gaussian%20centers%20are%20bound%20to%20the%20mesh%20vertices.%0AAfterward%2C%20we%20perform%20alternative%20geometry%20and%20texture%20optimization%0Aframe-by-frame%20for%20high-quality%20geometry%20and%20texture%20learning%20while%20maintaining%0Atemporal%20topology%20stability.%20Finally%2C%20we%20can%20extract%20dynamic%20facial%20meshes%20in%0Aregular%20wiring%20arrangement%20and%20high-fidelity%20textures%20with%20pore-level%20details%0Afrom%20the%20learned%20Gaussians.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%0Asuperior%20results%20than%20the%20current%20SOTA%20face%20reconstruction%20methods%20both%20in%20the%0Aquality%20of%20meshes%20and%20textures.%20Project%20page%3A%0Ahttps%3A//xuanchenli.github.io/Topo4D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00440v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopo4D%253A%2520Topology-Preserving%2520Gaussian%2520Splatting%2520for%2520High-Fidelity%25204D%2520Head%250A%2520%2520Capture%26entry.906535625%3DXuanchen%2520Li%2520and%2520Yuhao%2520Cheng%2520and%2520Xingyu%2520Ren%2520and%2520Haozhe%2520Jia%2520and%2520Di%2520Xu%2520and%2520Wenhan%2520Zhu%2520and%2520Yichao%2520Yan%26entry.1292438233%3D%2520%25204D%2520head%2520capture%2520aims%2520to%2520generate%2520dynamic%2520topological%2520meshes%2520and%2520corresponding%250Atexture%2520maps%2520from%2520videos%252C%2520which%2520is%2520widely%2520utilized%2520in%2520movies%2520and%2520games%2520for%2520its%250Aability%2520to%2520simulate%2520facial%2520muscle%2520movements%2520and%2520recover%2520dynamic%2520textures%2520in%250Apore-squeezing.%2520The%2520industry%2520often%2520adopts%2520the%2520method%2520involving%2520multi-view%250Astereo%2520and%2520non-rigid%2520alignment.%2520However%252C%2520this%2520approach%2520is%2520prone%2520to%2520errors%2520and%250Aheavily%2520reliant%2520on%2520time-consuming%2520manual%2520processing%2520by%2520artists.%2520To%2520simplify%250Athis%2520process%252C%2520we%2520propose%2520Topo4D%252C%2520a%2520novel%2520framework%2520for%2520automatic%2520geometry%2520and%250Atexture%2520generation%252C%2520which%2520optimizes%2520densely%2520aligned%25204D%2520heads%2520and%25208K%2520texture%250Amaps%2520directly%2520from%2520calibrated%2520multi-view%2520time-series%2520images.%2520Specifically%252C%2520we%250Afirst%2520represent%2520the%2520time-series%2520faces%2520as%2520a%2520set%2520of%2520dynamic%25203D%2520Gaussians%2520with%250Afixed%2520topology%2520in%2520which%2520the%2520Gaussian%2520centers%2520are%2520bound%2520to%2520the%2520mesh%2520vertices.%250AAfterward%252C%2520we%2520perform%2520alternative%2520geometry%2520and%2520texture%2520optimization%250Aframe-by-frame%2520for%2520high-quality%2520geometry%2520and%2520texture%2520learning%2520while%2520maintaining%250Atemporal%2520topology%2520stability.%2520Finally%252C%2520we%2520can%2520extract%2520dynamic%2520facial%2520meshes%2520in%250Aregular%2520wiring%2520arrangement%2520and%2520high-fidelity%2520textures%2520with%2520pore-level%2520details%250Afrom%2520the%2520learned%2520Gaussians.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520achieves%250Asuperior%2520results%2520than%2520the%2520current%2520SOTA%2520face%2520reconstruction%2520methods%2520both%2520in%2520the%250Aquality%2520of%2520meshes%2520and%2520textures.%2520Project%2520page%253A%250Ahttps%253A//xuanchenli.github.io/Topo4D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00440v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topo4D%3A%20Topology-Preserving%20Gaussian%20Splatting%20for%20High-Fidelity%204D%20Head%0A%20%20Capture&entry.906535625=Xuanchen%20Li%20and%20Yuhao%20Cheng%20and%20Xingyu%20Ren%20and%20Haozhe%20Jia%20and%20Di%20Xu%20and%20Wenhan%20Zhu%20and%20Yichao%20Yan&entry.1292438233=%20%204D%20head%20capture%20aims%20to%20generate%20dynamic%20topological%20meshes%20and%20corresponding%0Atexture%20maps%20from%20videos%2C%20which%20is%20widely%20utilized%20in%20movies%20and%20games%20for%20its%0Aability%20to%20simulate%20facial%20muscle%20movements%20and%20recover%20dynamic%20textures%20in%0Apore-squeezing.%20The%20industry%20often%20adopts%20the%20method%20involving%20multi-view%0Astereo%20and%20non-rigid%20alignment.%20However%2C%20this%20approach%20is%20prone%20to%20errors%20and%0Aheavily%20reliant%20on%20time-consuming%20manual%20processing%20by%20artists.%20To%20simplify%0Athis%20process%2C%20we%20propose%20Topo4D%2C%20a%20novel%20framework%20for%20automatic%20geometry%20and%0Atexture%20generation%2C%20which%20optimizes%20densely%20aligned%204D%20heads%20and%208K%20texture%0Amaps%20directly%20from%20calibrated%20multi-view%20time-series%20images.%20Specifically%2C%20we%0Afirst%20represent%20the%20time-series%20faces%20as%20a%20set%20of%20dynamic%203D%20Gaussians%20with%0Afixed%20topology%20in%20which%20the%20Gaussian%20centers%20are%20bound%20to%20the%20mesh%20vertices.%0AAfterward%2C%20we%20perform%20alternative%20geometry%20and%20texture%20optimization%0Aframe-by-frame%20for%20high-quality%20geometry%20and%20texture%20learning%20while%20maintaining%0Atemporal%20topology%20stability.%20Finally%2C%20we%20can%20extract%20dynamic%20facial%20meshes%20in%0Aregular%20wiring%20arrangement%20and%20high-fidelity%20textures%20with%20pore-level%20details%0Afrom%20the%20learned%20Gaussians.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%0Asuperior%20results%20than%20the%20current%20SOTA%20face%20reconstruction%20methods%20both%20in%20the%0Aquality%20of%20meshes%20and%20textures.%20Project%20page%3A%0Ahttps%3A//xuanchenli.github.io/Topo4D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00440v3&entry.124074799=Read"},
{"title": "OmniGS: Fast Radiance Field Reconstruction using Omnidirectional\n  Gaussian Splatting", "author": "Longwei Li and Huajian Huang and Sai-Kit Yeung and Hui Cheng", "abstract": "  Photorealistic reconstruction relying on 3D Gaussian Splatting has shown\npromising potential in various domains. However, the current 3D Gaussian\nSplatting system only supports radiance field reconstruction using undistorted\nperspective images. In this paper, we present OmniGS, a novel omnidirectional\nGaussian splatting system, to take advantage of omnidirectional images for fast\nradiance field reconstruction. Specifically, we conduct a theoretical analysis\nof spherical camera model derivatives in 3D Gaussian Splatting. According to\nthe derivatives, we then implement a new GPU-accelerated omnidirectional\nrasterizer that directly splats 3D Gaussians onto the equirectangular screen\nspace for omnidirectional image rendering. We realize differentiable\noptimization of the omnidirectional radiance field without the requirement of\ncube-map rectification or tangent-plane approximation. Extensive experiments\nconducted in egocentric and roaming scenarios demonstrate that our method\nachieves state-of-the-art reconstruction quality and high rendering speed using\nomnidirectional images. The code will be publicly available.\n", "link": "http://arxiv.org/abs/2404.03202v3", "date": "2024-07-15", "relevancy": 3.2114, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7431}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6405}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniGS%3A%20Fast%20Radiance%20Field%20Reconstruction%20using%20Omnidirectional%0A%20%20Gaussian%20Splatting&body=Title%3A%20OmniGS%3A%20Fast%20Radiance%20Field%20Reconstruction%20using%20Omnidirectional%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Longwei%20Li%20and%20Huajian%20Huang%20and%20Sai-Kit%20Yeung%20and%20Hui%20Cheng%0AAbstract%3A%20%20%20Photorealistic%20reconstruction%20relying%20on%203D%20Gaussian%20Splatting%20has%20shown%0Apromising%20potential%20in%20various%20domains.%20However%2C%20the%20current%203D%20Gaussian%0ASplatting%20system%20only%20supports%20radiance%20field%20reconstruction%20using%20undistorted%0Aperspective%20images.%20In%20this%20paper%2C%20we%20present%20OmniGS%2C%20a%20novel%20omnidirectional%0AGaussian%20splatting%20system%2C%20to%20take%20advantage%20of%20omnidirectional%20images%20for%20fast%0Aradiance%20field%20reconstruction.%20Specifically%2C%20we%20conduct%20a%20theoretical%20analysis%0Aof%20spherical%20camera%20model%20derivatives%20in%203D%20Gaussian%20Splatting.%20According%20to%0Athe%20derivatives%2C%20we%20then%20implement%20a%20new%20GPU-accelerated%20omnidirectional%0Arasterizer%20that%20directly%20splats%203D%20Gaussians%20onto%20the%20equirectangular%20screen%0Aspace%20for%20omnidirectional%20image%20rendering.%20We%20realize%20differentiable%0Aoptimization%20of%20the%20omnidirectional%20radiance%20field%20without%20the%20requirement%20of%0Acube-map%20rectification%20or%20tangent-plane%20approximation.%20Extensive%20experiments%0Aconducted%20in%20egocentric%20and%20roaming%20scenarios%20demonstrate%20that%20our%20method%0Aachieves%20state-of-the-art%20reconstruction%20quality%20and%20high%20rendering%20speed%20using%0Aomnidirectional%20images.%20The%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03202v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniGS%253A%2520Fast%2520Radiance%2520Field%2520Reconstruction%2520using%2520Omnidirectional%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DLongwei%2520Li%2520and%2520Huajian%2520Huang%2520and%2520Sai-Kit%2520Yeung%2520and%2520Hui%2520Cheng%26entry.1292438233%3D%2520%2520Photorealistic%2520reconstruction%2520relying%2520on%25203D%2520Gaussian%2520Splatting%2520has%2520shown%250Apromising%2520potential%2520in%2520various%2520domains.%2520However%252C%2520the%2520current%25203D%2520Gaussian%250ASplatting%2520system%2520only%2520supports%2520radiance%2520field%2520reconstruction%2520using%2520undistorted%250Aperspective%2520images.%2520In%2520this%2520paper%252C%2520we%2520present%2520OmniGS%252C%2520a%2520novel%2520omnidirectional%250AGaussian%2520splatting%2520system%252C%2520to%2520take%2520advantage%2520of%2520omnidirectional%2520images%2520for%2520fast%250Aradiance%2520field%2520reconstruction.%2520Specifically%252C%2520we%2520conduct%2520a%2520theoretical%2520analysis%250Aof%2520spherical%2520camera%2520model%2520derivatives%2520in%25203D%2520Gaussian%2520Splatting.%2520According%2520to%250Athe%2520derivatives%252C%2520we%2520then%2520implement%2520a%2520new%2520GPU-accelerated%2520omnidirectional%250Arasterizer%2520that%2520directly%2520splats%25203D%2520Gaussians%2520onto%2520the%2520equirectangular%2520screen%250Aspace%2520for%2520omnidirectional%2520image%2520rendering.%2520We%2520realize%2520differentiable%250Aoptimization%2520of%2520the%2520omnidirectional%2520radiance%2520field%2520without%2520the%2520requirement%2520of%250Acube-map%2520rectification%2520or%2520tangent-plane%2520approximation.%2520Extensive%2520experiments%250Aconducted%2520in%2520egocentric%2520and%2520roaming%2520scenarios%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520state-of-the-art%2520reconstruction%2520quality%2520and%2520high%2520rendering%2520speed%2520using%250Aomnidirectional%2520images.%2520The%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03202v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniGS%3A%20Fast%20Radiance%20Field%20Reconstruction%20using%20Omnidirectional%0A%20%20Gaussian%20Splatting&entry.906535625=Longwei%20Li%20and%20Huajian%20Huang%20and%20Sai-Kit%20Yeung%20and%20Hui%20Cheng&entry.1292438233=%20%20Photorealistic%20reconstruction%20relying%20on%203D%20Gaussian%20Splatting%20has%20shown%0Apromising%20potential%20in%20various%20domains.%20However%2C%20the%20current%203D%20Gaussian%0ASplatting%20system%20only%20supports%20radiance%20field%20reconstruction%20using%20undistorted%0Aperspective%20images.%20In%20this%20paper%2C%20we%20present%20OmniGS%2C%20a%20novel%20omnidirectional%0AGaussian%20splatting%20system%2C%20to%20take%20advantage%20of%20omnidirectional%20images%20for%20fast%0Aradiance%20field%20reconstruction.%20Specifically%2C%20we%20conduct%20a%20theoretical%20analysis%0Aof%20spherical%20camera%20model%20derivatives%20in%203D%20Gaussian%20Splatting.%20According%20to%0Athe%20derivatives%2C%20we%20then%20implement%20a%20new%20GPU-accelerated%20omnidirectional%0Arasterizer%20that%20directly%20splats%203D%20Gaussians%20onto%20the%20equirectangular%20screen%0Aspace%20for%20omnidirectional%20image%20rendering.%20We%20realize%20differentiable%0Aoptimization%20of%20the%20omnidirectional%20radiance%20field%20without%20the%20requirement%20of%0Acube-map%20rectification%20or%20tangent-plane%20approximation.%20Extensive%20experiments%0Aconducted%20in%20egocentric%20and%20roaming%20scenarios%20demonstrate%20that%20our%20method%0Aachieves%20state-of-the-art%20reconstruction%20quality%20and%20high%20rendering%20speed%20using%0Aomnidirectional%20images.%20The%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03202v3&entry.124074799=Read"},
{"title": "3D Feature Distillation with Object-Centric Priors", "author": "Georgios Tziafas and Yucheng Xu and Zhibin Li and Hamidreza Kasaei", "abstract": "  Grounding natural language to the physical world is a ubiquitous topic with a\nwide range of applications in computer vision and robotics. Recently, 2D\nvision-language models such as CLIP have been widely popularized, due to their\nimpressive capabilities for open-vocabulary grounding in 2D images. Recent\nworks aim to elevate 2D CLIP features to 3D via feature distillation, but\neither learn neural fields that are scene-specific and hence lack\ngeneralization, or focus on indoor room scan data that require access to\nmultiple camera views, which is not practical in robot manipulation scenarios.\nAdditionally, related methods typically fuse features at pixel-level and assume\nthat all camera views are equally informative. In this work, we show that this\napproach leads to sub-optimal 3D features, both in terms of grounding accuracy,\nas well as segmentation crispness. To alleviate this, we propose a multi-view\nfeature fusion strategy that employs object-centric priors to eliminate\nuninformative views based on semantic information, and fuse features at\nobject-level via instance segmentation masks. To distill our object-centric 3D\nfeatures, we generate a large-scale synthetic multi-view dataset of cluttered\ntabletop scenes, spawning 15k scenes from over 3300 unique object instances,\nwhich we make publicly available. We show that our method reconstructs 3D CLIP\nfeatures with improved grounding capacity and spatial consistency, while doing\nso from single-view RGB-D, thus departing from the assumption of multiple\ncamera views at test time. Finally, we show that our approach can generalize to\nnovel tabletop domains and be re-purposed for 3D instance segmentation without\nfine-tuning, and demonstrate its utility for language-guided robotic grasping\nin clutter\n", "link": "http://arxiv.org/abs/2406.18742v3", "date": "2024-07-15", "relevancy": 3.0673, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6193}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6193}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Feature%20Distillation%20with%20Object-Centric%20Priors&body=Title%3A%203D%20Feature%20Distillation%20with%20Object-Centric%20Priors%0AAuthor%3A%20Georgios%20Tziafas%20and%20Yucheng%20Xu%20and%20Zhibin%20Li%20and%20Hamidreza%20Kasaei%0AAbstract%3A%20%20%20Grounding%20natural%20language%20to%20the%20physical%20world%20is%20a%20ubiquitous%20topic%20with%20a%0Awide%20range%20of%20applications%20in%20computer%20vision%20and%20robotics.%20Recently%2C%202D%0Avision-language%20models%20such%20as%20CLIP%20have%20been%20widely%20popularized%2C%20due%20to%20their%0Aimpressive%20capabilities%20for%20open-vocabulary%20grounding%20in%202D%20images.%20Recent%0Aworks%20aim%20to%20elevate%202D%20CLIP%20features%20to%203D%20via%20feature%20distillation%2C%20but%0Aeither%20learn%20neural%20fields%20that%20are%20scene-specific%20and%20hence%20lack%0Ageneralization%2C%20or%20focus%20on%20indoor%20room%20scan%20data%20that%20require%20access%20to%0Amultiple%20camera%20views%2C%20which%20is%20not%20practical%20in%20robot%20manipulation%20scenarios.%0AAdditionally%2C%20related%20methods%20typically%20fuse%20features%20at%20pixel-level%20and%20assume%0Athat%20all%20camera%20views%20are%20equally%20informative.%20In%20this%20work%2C%20we%20show%20that%20this%0Aapproach%20leads%20to%20sub-optimal%203D%20features%2C%20both%20in%20terms%20of%20grounding%20accuracy%2C%0Aas%20well%20as%20segmentation%20crispness.%20To%20alleviate%20this%2C%20we%20propose%20a%20multi-view%0Afeature%20fusion%20strategy%20that%20employs%20object-centric%20priors%20to%20eliminate%0Auninformative%20views%20based%20on%20semantic%20information%2C%20and%20fuse%20features%20at%0Aobject-level%20via%20instance%20segmentation%20masks.%20To%20distill%20our%20object-centric%203D%0Afeatures%2C%20we%20generate%20a%20large-scale%20synthetic%20multi-view%20dataset%20of%20cluttered%0Atabletop%20scenes%2C%20spawning%2015k%20scenes%20from%20over%203300%20unique%20object%20instances%2C%0Awhich%20we%20make%20publicly%20available.%20We%20show%20that%20our%20method%20reconstructs%203D%20CLIP%0Afeatures%20with%20improved%20grounding%20capacity%20and%20spatial%20consistency%2C%20while%20doing%0Aso%20from%20single-view%20RGB-D%2C%20thus%20departing%20from%20the%20assumption%20of%20multiple%0Acamera%20views%20at%20test%20time.%20Finally%2C%20we%20show%20that%20our%20approach%20can%20generalize%20to%0Anovel%20tabletop%20domains%20and%20be%20re-purposed%20for%203D%20instance%20segmentation%20without%0Afine-tuning%2C%20and%20demonstrate%20its%20utility%20for%20language-guided%20robotic%20grasping%0Ain%20clutter%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18742v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Feature%2520Distillation%2520with%2520Object-Centric%2520Priors%26entry.906535625%3DGeorgios%2520Tziafas%2520and%2520Yucheng%2520Xu%2520and%2520Zhibin%2520Li%2520and%2520Hamidreza%2520Kasaei%26entry.1292438233%3D%2520%2520Grounding%2520natural%2520language%2520to%2520the%2520physical%2520world%2520is%2520a%2520ubiquitous%2520topic%2520with%2520a%250Awide%2520range%2520of%2520applications%2520in%2520computer%2520vision%2520and%2520robotics.%2520Recently%252C%25202D%250Avision-language%2520models%2520such%2520as%2520CLIP%2520have%2520been%2520widely%2520popularized%252C%2520due%2520to%2520their%250Aimpressive%2520capabilities%2520for%2520open-vocabulary%2520grounding%2520in%25202D%2520images.%2520Recent%250Aworks%2520aim%2520to%2520elevate%25202D%2520CLIP%2520features%2520to%25203D%2520via%2520feature%2520distillation%252C%2520but%250Aeither%2520learn%2520neural%2520fields%2520that%2520are%2520scene-specific%2520and%2520hence%2520lack%250Ageneralization%252C%2520or%2520focus%2520on%2520indoor%2520room%2520scan%2520data%2520that%2520require%2520access%2520to%250Amultiple%2520camera%2520views%252C%2520which%2520is%2520not%2520practical%2520in%2520robot%2520manipulation%2520scenarios.%250AAdditionally%252C%2520related%2520methods%2520typically%2520fuse%2520features%2520at%2520pixel-level%2520and%2520assume%250Athat%2520all%2520camera%2520views%2520are%2520equally%2520informative.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520this%250Aapproach%2520leads%2520to%2520sub-optimal%25203D%2520features%252C%2520both%2520in%2520terms%2520of%2520grounding%2520accuracy%252C%250Aas%2520well%2520as%2520segmentation%2520crispness.%2520To%2520alleviate%2520this%252C%2520we%2520propose%2520a%2520multi-view%250Afeature%2520fusion%2520strategy%2520that%2520employs%2520object-centric%2520priors%2520to%2520eliminate%250Auninformative%2520views%2520based%2520on%2520semantic%2520information%252C%2520and%2520fuse%2520features%2520at%250Aobject-level%2520via%2520instance%2520segmentation%2520masks.%2520To%2520distill%2520our%2520object-centric%25203D%250Afeatures%252C%2520we%2520generate%2520a%2520large-scale%2520synthetic%2520multi-view%2520dataset%2520of%2520cluttered%250Atabletop%2520scenes%252C%2520spawning%252015k%2520scenes%2520from%2520over%25203300%2520unique%2520object%2520instances%252C%250Awhich%2520we%2520make%2520publicly%2520available.%2520We%2520show%2520that%2520our%2520method%2520reconstructs%25203D%2520CLIP%250Afeatures%2520with%2520improved%2520grounding%2520capacity%2520and%2520spatial%2520consistency%252C%2520while%2520doing%250Aso%2520from%2520single-view%2520RGB-D%252C%2520thus%2520departing%2520from%2520the%2520assumption%2520of%2520multiple%250Acamera%2520views%2520at%2520test%2520time.%2520Finally%252C%2520we%2520show%2520that%2520our%2520approach%2520can%2520generalize%2520to%250Anovel%2520tabletop%2520domains%2520and%2520be%2520re-purposed%2520for%25203D%2520instance%2520segmentation%2520without%250Afine-tuning%252C%2520and%2520demonstrate%2520its%2520utility%2520for%2520language-guided%2520robotic%2520grasping%250Ain%2520clutter%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18742v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Feature%20Distillation%20with%20Object-Centric%20Priors&entry.906535625=Georgios%20Tziafas%20and%20Yucheng%20Xu%20and%20Zhibin%20Li%20and%20Hamidreza%20Kasaei&entry.1292438233=%20%20Grounding%20natural%20language%20to%20the%20physical%20world%20is%20a%20ubiquitous%20topic%20with%20a%0Awide%20range%20of%20applications%20in%20computer%20vision%20and%20robotics.%20Recently%2C%202D%0Avision-language%20models%20such%20as%20CLIP%20have%20been%20widely%20popularized%2C%20due%20to%20their%0Aimpressive%20capabilities%20for%20open-vocabulary%20grounding%20in%202D%20images.%20Recent%0Aworks%20aim%20to%20elevate%202D%20CLIP%20features%20to%203D%20via%20feature%20distillation%2C%20but%0Aeither%20learn%20neural%20fields%20that%20are%20scene-specific%20and%20hence%20lack%0Ageneralization%2C%20or%20focus%20on%20indoor%20room%20scan%20data%20that%20require%20access%20to%0Amultiple%20camera%20views%2C%20which%20is%20not%20practical%20in%20robot%20manipulation%20scenarios.%0AAdditionally%2C%20related%20methods%20typically%20fuse%20features%20at%20pixel-level%20and%20assume%0Athat%20all%20camera%20views%20are%20equally%20informative.%20In%20this%20work%2C%20we%20show%20that%20this%0Aapproach%20leads%20to%20sub-optimal%203D%20features%2C%20both%20in%20terms%20of%20grounding%20accuracy%2C%0Aas%20well%20as%20segmentation%20crispness.%20To%20alleviate%20this%2C%20we%20propose%20a%20multi-view%0Afeature%20fusion%20strategy%20that%20employs%20object-centric%20priors%20to%20eliminate%0Auninformative%20views%20based%20on%20semantic%20information%2C%20and%20fuse%20features%20at%0Aobject-level%20via%20instance%20segmentation%20masks.%20To%20distill%20our%20object-centric%203D%0Afeatures%2C%20we%20generate%20a%20large-scale%20synthetic%20multi-view%20dataset%20of%20cluttered%0Atabletop%20scenes%2C%20spawning%2015k%20scenes%20from%20over%203300%20unique%20object%20instances%2C%0Awhich%20we%20make%20publicly%20available.%20We%20show%20that%20our%20method%20reconstructs%203D%20CLIP%0Afeatures%20with%20improved%20grounding%20capacity%20and%20spatial%20consistency%2C%20while%20doing%0Aso%20from%20single-view%20RGB-D%2C%20thus%20departing%20from%20the%20assumption%20of%20multiple%0Acamera%20views%20at%20test%20time.%20Finally%2C%20we%20show%20that%20our%20approach%20can%20generalize%20to%0Anovel%20tabletop%20domains%20and%20be%20re-purposed%20for%203D%20instance%20segmentation%20without%0Afine-tuning%2C%20and%20demonstrate%20its%20utility%20for%20language-guided%20robotic%20grasping%0Ain%20clutter%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18742v3&entry.124074799=Read"},
{"title": "LVCP: LiDAR-Vision Tightly Coupled Collaborative Real-time Relative\n  Positioning", "author": "Zhuozhu Jian and Qixuan Li and Shengtao Zheng and Xueqian Wang and Xinlei Chen", "abstract": "  In air-ground collaboration scenarios without GPS and prior maps, the\nrelative positioning of drones and unmanned ground vehicles (UGVs) has always\nbeen a challenge. For a drone equipped with monocular camera and an UGV\nequipped with LiDAR as an external sensor, we propose a robust and real-time\nrelative pose estimation method (LVCP) based on the tight coupling of vision\nand LiDAR point cloud information, which does not require prior information\nsuch as maps or precise initial poses. Given that large-scale point clouds\ngenerated by 3D sensors has more accurate spatial geometric information than\nthe feature point cloud generated by image, we utilize LiDAR point clouds to\ncorrect the drift in visual-inertial odometry (VIO) when the camera undergoes\nsignificant shaking or the IMU has a low signal-to-noise ratio. To achieve\nthis, we propose a novel coarse-to-fine framework for LiDAR-vision\ncollaborative localization. In this framework, we construct point-plane\nassociation based on spatial geometric information, and innovatively construct\na point-aided Bundle Adjustment (BA) problem as the backend to simultaneously\nestimate the relative pose of the camera and LiDAR and correct the VIO drift.\nIn this process, we propose a particle swarm optimization (PSO) based sampling\nalgorithm to complete the coarse estimation of the current camera-LiDAR pose.\nIn this process, the initial pose of the camera used for sampling is obtained\nbased on VIO propagation, and the valid feature-plane association number (VFPN)\nis used to trigger PSO-sampling process. Additionally, we propose a method that\ncombines Structure from Motion (SFM) and multi-level sampling to initialize the\nalgorithm, addressing the challenge of lacking initial values.\n", "link": "http://arxiv.org/abs/2407.10782v1", "date": "2024-07-15", "relevancy": 3.0327, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.661}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5865}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVCP%3A%20LiDAR-Vision%20Tightly%20Coupled%20Collaborative%20Real-time%20Relative%0A%20%20Positioning&body=Title%3A%20LVCP%3A%20LiDAR-Vision%20Tightly%20Coupled%20Collaborative%20Real-time%20Relative%0A%20%20Positioning%0AAuthor%3A%20Zhuozhu%20Jian%20and%20Qixuan%20Li%20and%20Shengtao%20Zheng%20and%20Xueqian%20Wang%20and%20Xinlei%20Chen%0AAbstract%3A%20%20%20In%20air-ground%20collaboration%20scenarios%20without%20GPS%20and%20prior%20maps%2C%20the%0Arelative%20positioning%20of%20drones%20and%20unmanned%20ground%20vehicles%20%28UGVs%29%20has%20always%0Abeen%20a%20challenge.%20For%20a%20drone%20equipped%20with%20monocular%20camera%20and%20an%20UGV%0Aequipped%20with%20LiDAR%20as%20an%20external%20sensor%2C%20we%20propose%20a%20robust%20and%20real-time%0Arelative%20pose%20estimation%20method%20%28LVCP%29%20based%20on%20the%20tight%20coupling%20of%20vision%0Aand%20LiDAR%20point%20cloud%20information%2C%20which%20does%20not%20require%20prior%20information%0Asuch%20as%20maps%20or%20precise%20initial%20poses.%20Given%20that%20large-scale%20point%20clouds%0Agenerated%20by%203D%20sensors%20has%20more%20accurate%20spatial%20geometric%20information%20than%0Athe%20feature%20point%20cloud%20generated%20by%20image%2C%20we%20utilize%20LiDAR%20point%20clouds%20to%0Acorrect%20the%20drift%20in%20visual-inertial%20odometry%20%28VIO%29%20when%20the%20camera%20undergoes%0Asignificant%20shaking%20or%20the%20IMU%20has%20a%20low%20signal-to-noise%20ratio.%20To%20achieve%0Athis%2C%20we%20propose%20a%20novel%20coarse-to-fine%20framework%20for%20LiDAR-vision%0Acollaborative%20localization.%20In%20this%20framework%2C%20we%20construct%20point-plane%0Aassociation%20based%20on%20spatial%20geometric%20information%2C%20and%20innovatively%20construct%0Aa%20point-aided%20Bundle%20Adjustment%20%28BA%29%20problem%20as%20the%20backend%20to%20simultaneously%0Aestimate%20the%20relative%20pose%20of%20the%20camera%20and%20LiDAR%20and%20correct%20the%20VIO%20drift.%0AIn%20this%20process%2C%20we%20propose%20a%20particle%20swarm%20optimization%20%28PSO%29%20based%20sampling%0Aalgorithm%20to%20complete%20the%20coarse%20estimation%20of%20the%20current%20camera-LiDAR%20pose.%0AIn%20this%20process%2C%20the%20initial%20pose%20of%20the%20camera%20used%20for%20sampling%20is%20obtained%0Abased%20on%20VIO%20propagation%2C%20and%20the%20valid%20feature-plane%20association%20number%20%28VFPN%29%0Ais%20used%20to%20trigger%20PSO-sampling%20process.%20Additionally%2C%20we%20propose%20a%20method%20that%0Acombines%20Structure%20from%20Motion%20%28SFM%29%20and%20multi-level%20sampling%20to%20initialize%20the%0Aalgorithm%2C%20addressing%20the%20challenge%20of%20lacking%20initial%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVCP%253A%2520LiDAR-Vision%2520Tightly%2520Coupled%2520Collaborative%2520Real-time%2520Relative%250A%2520%2520Positioning%26entry.906535625%3DZhuozhu%2520Jian%2520and%2520Qixuan%2520Li%2520and%2520Shengtao%2520Zheng%2520and%2520Xueqian%2520Wang%2520and%2520Xinlei%2520Chen%26entry.1292438233%3D%2520%2520In%2520air-ground%2520collaboration%2520scenarios%2520without%2520GPS%2520and%2520prior%2520maps%252C%2520the%250Arelative%2520positioning%2520of%2520drones%2520and%2520unmanned%2520ground%2520vehicles%2520%2528UGVs%2529%2520has%2520always%250Abeen%2520a%2520challenge.%2520For%2520a%2520drone%2520equipped%2520with%2520monocular%2520camera%2520and%2520an%2520UGV%250Aequipped%2520with%2520LiDAR%2520as%2520an%2520external%2520sensor%252C%2520we%2520propose%2520a%2520robust%2520and%2520real-time%250Arelative%2520pose%2520estimation%2520method%2520%2528LVCP%2529%2520based%2520on%2520the%2520tight%2520coupling%2520of%2520vision%250Aand%2520LiDAR%2520point%2520cloud%2520information%252C%2520which%2520does%2520not%2520require%2520prior%2520information%250Asuch%2520as%2520maps%2520or%2520precise%2520initial%2520poses.%2520Given%2520that%2520large-scale%2520point%2520clouds%250Agenerated%2520by%25203D%2520sensors%2520has%2520more%2520accurate%2520spatial%2520geometric%2520information%2520than%250Athe%2520feature%2520point%2520cloud%2520generated%2520by%2520image%252C%2520we%2520utilize%2520LiDAR%2520point%2520clouds%2520to%250Acorrect%2520the%2520drift%2520in%2520visual-inertial%2520odometry%2520%2528VIO%2529%2520when%2520the%2520camera%2520undergoes%250Asignificant%2520shaking%2520or%2520the%2520IMU%2520has%2520a%2520low%2520signal-to-noise%2520ratio.%2520To%2520achieve%250Athis%252C%2520we%2520propose%2520a%2520novel%2520coarse-to-fine%2520framework%2520for%2520LiDAR-vision%250Acollaborative%2520localization.%2520In%2520this%2520framework%252C%2520we%2520construct%2520point-plane%250Aassociation%2520based%2520on%2520spatial%2520geometric%2520information%252C%2520and%2520innovatively%2520construct%250Aa%2520point-aided%2520Bundle%2520Adjustment%2520%2528BA%2529%2520problem%2520as%2520the%2520backend%2520to%2520simultaneously%250Aestimate%2520the%2520relative%2520pose%2520of%2520the%2520camera%2520and%2520LiDAR%2520and%2520correct%2520the%2520VIO%2520drift.%250AIn%2520this%2520process%252C%2520we%2520propose%2520a%2520particle%2520swarm%2520optimization%2520%2528PSO%2529%2520based%2520sampling%250Aalgorithm%2520to%2520complete%2520the%2520coarse%2520estimation%2520of%2520the%2520current%2520camera-LiDAR%2520pose.%250AIn%2520this%2520process%252C%2520the%2520initial%2520pose%2520of%2520the%2520camera%2520used%2520for%2520sampling%2520is%2520obtained%250Abased%2520on%2520VIO%2520propagation%252C%2520and%2520the%2520valid%2520feature-plane%2520association%2520number%2520%2528VFPN%2529%250Ais%2520used%2520to%2520trigger%2520PSO-sampling%2520process.%2520Additionally%252C%2520we%2520propose%2520a%2520method%2520that%250Acombines%2520Structure%2520from%2520Motion%2520%2528SFM%2529%2520and%2520multi-level%2520sampling%2520to%2520initialize%2520the%250Aalgorithm%252C%2520addressing%2520the%2520challenge%2520of%2520lacking%2520initial%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVCP%3A%20LiDAR-Vision%20Tightly%20Coupled%20Collaborative%20Real-time%20Relative%0A%20%20Positioning&entry.906535625=Zhuozhu%20Jian%20and%20Qixuan%20Li%20and%20Shengtao%20Zheng%20and%20Xueqian%20Wang%20and%20Xinlei%20Chen&entry.1292438233=%20%20In%20air-ground%20collaboration%20scenarios%20without%20GPS%20and%20prior%20maps%2C%20the%0Arelative%20positioning%20of%20drones%20and%20unmanned%20ground%20vehicles%20%28UGVs%29%20has%20always%0Abeen%20a%20challenge.%20For%20a%20drone%20equipped%20with%20monocular%20camera%20and%20an%20UGV%0Aequipped%20with%20LiDAR%20as%20an%20external%20sensor%2C%20we%20propose%20a%20robust%20and%20real-time%0Arelative%20pose%20estimation%20method%20%28LVCP%29%20based%20on%20the%20tight%20coupling%20of%20vision%0Aand%20LiDAR%20point%20cloud%20information%2C%20which%20does%20not%20require%20prior%20information%0Asuch%20as%20maps%20or%20precise%20initial%20poses.%20Given%20that%20large-scale%20point%20clouds%0Agenerated%20by%203D%20sensors%20has%20more%20accurate%20spatial%20geometric%20information%20than%0Athe%20feature%20point%20cloud%20generated%20by%20image%2C%20we%20utilize%20LiDAR%20point%20clouds%20to%0Acorrect%20the%20drift%20in%20visual-inertial%20odometry%20%28VIO%29%20when%20the%20camera%20undergoes%0Asignificant%20shaking%20or%20the%20IMU%20has%20a%20low%20signal-to-noise%20ratio.%20To%20achieve%0Athis%2C%20we%20propose%20a%20novel%20coarse-to-fine%20framework%20for%20LiDAR-vision%0Acollaborative%20localization.%20In%20this%20framework%2C%20we%20construct%20point-plane%0Aassociation%20based%20on%20spatial%20geometric%20information%2C%20and%20innovatively%20construct%0Aa%20point-aided%20Bundle%20Adjustment%20%28BA%29%20problem%20as%20the%20backend%20to%20simultaneously%0Aestimate%20the%20relative%20pose%20of%20the%20camera%20and%20LiDAR%20and%20correct%20the%20VIO%20drift.%0AIn%20this%20process%2C%20we%20propose%20a%20particle%20swarm%20optimization%20%28PSO%29%20based%20sampling%0Aalgorithm%20to%20complete%20the%20coarse%20estimation%20of%20the%20current%20camera-LiDAR%20pose.%0AIn%20this%20process%2C%20the%20initial%20pose%20of%20the%20camera%20used%20for%20sampling%20is%20obtained%0Abased%20on%20VIO%20propagation%2C%20and%20the%20valid%20feature-plane%20association%20number%20%28VFPN%29%0Ais%20used%20to%20trigger%20PSO-sampling%20process.%20Additionally%2C%20we%20propose%20a%20method%20that%0Acombines%20Structure%20from%20Motion%20%28SFM%29%20and%20multi-level%20sampling%20to%20initialize%20the%0Aalgorithm%2C%20addressing%20the%20challenge%20of%20lacking%20initial%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10782v1&entry.124074799=Read"},
{"title": "DoubleTake: Geometry Guided Depth Estimation", "author": "Mohamed Sayed and Filippo Aleotti and Jamie Watson and Zawar Qureshi and Guillermo Garcia-Hernando and Gabriel Brostow and Sara Vicente and Michael Firman", "abstract": "  Estimating depth from a sequence of posed RGB images is a fundamental\ncomputer vision task, with applications in augmented reality, path planning\netc. Prior work typically makes use of previous frames in a multi view stereo\nframework, relying on matching textures in a local neighborhood. In contrast,\nour model leverages historical predictions by giving the latest 3D geometry\ndata as an extra input to our network. This self-generated geometric hint can\nencode information from areas of the scene not covered by the keyframes and it\nis more regularized when compared to individual predicted depth maps for\nprevious frames. We introduce a Hint MLP which combines cost volume features\nwith a hint of the prior geometry, rendered as a depth map from the current\ncamera location, together with a measure of the confidence in the prior\ngeometry. We demonstrate that our method, which can run at interactive speeds,\nachieves state-of-the-art estimates of depth and 3D scene reconstruction in\nboth offline and incremental evaluation scenarios.\n", "link": "http://arxiv.org/abs/2406.18387v2", "date": "2024-07-15", "relevancy": 2.9128, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5962}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.576}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation&body=Title%3A%20DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation%0AAuthor%3A%20Mohamed%20Sayed%20and%20Filippo%20Aleotti%20and%20Jamie%20Watson%20and%20Zawar%20Qureshi%20and%20Guillermo%20Garcia-Hernando%20and%20Gabriel%20Brostow%20and%20Sara%20Vicente%20and%20Michael%20Firman%0AAbstract%3A%20%20%20Estimating%20depth%20from%20a%20sequence%20of%20posed%20RGB%20images%20is%20a%20fundamental%0Acomputer%20vision%20task%2C%20with%20applications%20in%20augmented%20reality%2C%20path%20planning%0Aetc.%20Prior%20work%20typically%20makes%20use%20of%20previous%20frames%20in%20a%20multi%20view%20stereo%0Aframework%2C%20relying%20on%20matching%20textures%20in%20a%20local%20neighborhood.%20In%20contrast%2C%0Aour%20model%20leverages%20historical%20predictions%20by%20giving%20the%20latest%203D%20geometry%0Adata%20as%20an%20extra%20input%20to%20our%20network.%20This%20self-generated%20geometric%20hint%20can%0Aencode%20information%20from%20areas%20of%20the%20scene%20not%20covered%20by%20the%20keyframes%20and%20it%0Ais%20more%20regularized%20when%20compared%20to%20individual%20predicted%20depth%20maps%20for%0Aprevious%20frames.%20We%20introduce%20a%20Hint%20MLP%20which%20combines%20cost%20volume%20features%0Awith%20a%20hint%20of%20the%20prior%20geometry%2C%20rendered%20as%20a%20depth%20map%20from%20the%20current%0Acamera%20location%2C%20together%20with%20a%20measure%20of%20the%20confidence%20in%20the%20prior%0Ageometry.%20We%20demonstrate%20that%20our%20method%2C%20which%20can%20run%20at%20interactive%20speeds%2C%0Aachieves%20state-of-the-art%20estimates%20of%20depth%20and%203D%20scene%20reconstruction%20in%0Aboth%20offline%20and%20incremental%20evaluation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoubleTake%253A%2520Geometry%2520Guided%2520Depth%2520Estimation%26entry.906535625%3DMohamed%2520Sayed%2520and%2520Filippo%2520Aleotti%2520and%2520Jamie%2520Watson%2520and%2520Zawar%2520Qureshi%2520and%2520Guillermo%2520Garcia-Hernando%2520and%2520Gabriel%2520Brostow%2520and%2520Sara%2520Vicente%2520and%2520Michael%2520Firman%26entry.1292438233%3D%2520%2520Estimating%2520depth%2520from%2520a%2520sequence%2520of%2520posed%2520RGB%2520images%2520is%2520a%2520fundamental%250Acomputer%2520vision%2520task%252C%2520with%2520applications%2520in%2520augmented%2520reality%252C%2520path%2520planning%250Aetc.%2520Prior%2520work%2520typically%2520makes%2520use%2520of%2520previous%2520frames%2520in%2520a%2520multi%2520view%2520stereo%250Aframework%252C%2520relying%2520on%2520matching%2520textures%2520in%2520a%2520local%2520neighborhood.%2520In%2520contrast%252C%250Aour%2520model%2520leverages%2520historical%2520predictions%2520by%2520giving%2520the%2520latest%25203D%2520geometry%250Adata%2520as%2520an%2520extra%2520input%2520to%2520our%2520network.%2520This%2520self-generated%2520geometric%2520hint%2520can%250Aencode%2520information%2520from%2520areas%2520of%2520the%2520scene%2520not%2520covered%2520by%2520the%2520keyframes%2520and%2520it%250Ais%2520more%2520regularized%2520when%2520compared%2520to%2520individual%2520predicted%2520depth%2520maps%2520for%250Aprevious%2520frames.%2520We%2520introduce%2520a%2520Hint%2520MLP%2520which%2520combines%2520cost%2520volume%2520features%250Awith%2520a%2520hint%2520of%2520the%2520prior%2520geometry%252C%2520rendered%2520as%2520a%2520depth%2520map%2520from%2520the%2520current%250Acamera%2520location%252C%2520together%2520with%2520a%2520measure%2520of%2520the%2520confidence%2520in%2520the%2520prior%250Ageometry.%2520We%2520demonstrate%2520that%2520our%2520method%252C%2520which%2520can%2520run%2520at%2520interactive%2520speeds%252C%250Aachieves%2520state-of-the-art%2520estimates%2520of%2520depth%2520and%25203D%2520scene%2520reconstruction%2520in%250Aboth%2520offline%2520and%2520incremental%2520evaluation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation&entry.906535625=Mohamed%20Sayed%20and%20Filippo%20Aleotti%20and%20Jamie%20Watson%20and%20Zawar%20Qureshi%20and%20Guillermo%20Garcia-Hernando%20and%20Gabriel%20Brostow%20and%20Sara%20Vicente%20and%20Michael%20Firman&entry.1292438233=%20%20Estimating%20depth%20from%20a%20sequence%20of%20posed%20RGB%20images%20is%20a%20fundamental%0Acomputer%20vision%20task%2C%20with%20applications%20in%20augmented%20reality%2C%20path%20planning%0Aetc.%20Prior%20work%20typically%20makes%20use%20of%20previous%20frames%20in%20a%20multi%20view%20stereo%0Aframework%2C%20relying%20on%20matching%20textures%20in%20a%20local%20neighborhood.%20In%20contrast%2C%0Aour%20model%20leverages%20historical%20predictions%20by%20giving%20the%20latest%203D%20geometry%0Adata%20as%20an%20extra%20input%20to%20our%20network.%20This%20self-generated%20geometric%20hint%20can%0Aencode%20information%20from%20areas%20of%20the%20scene%20not%20covered%20by%20the%20keyframes%20and%20it%0Ais%20more%20regularized%20when%20compared%20to%20individual%20predicted%20depth%20maps%20for%0Aprevious%20frames.%20We%20introduce%20a%20Hint%20MLP%20which%20combines%20cost%20volume%20features%0Awith%20a%20hint%20of%20the%20prior%20geometry%2C%20rendered%20as%20a%20depth%20map%20from%20the%20current%0Acamera%20location%2C%20together%20with%20a%20measure%20of%20the%20confidence%20in%20the%20prior%0Ageometry.%20We%20demonstrate%20that%20our%20method%2C%20which%20can%20run%20at%20interactive%20speeds%2C%0Aachieves%20state-of-the-art%20estimates%20of%20depth%20and%203D%20scene%20reconstruction%20in%0Aboth%20offline%20and%20incremental%20evaluation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18387v2&entry.124074799=Read"},
{"title": "AirNeRF: 3D Reconstruction of Human with Drone and NeRF for Future\n  Communication Systems", "author": "Alexey Kotcov and Maria Dronova and Vladislav Cheremnykh and Sausar Karaf and Dzmitry Tsetserukou", "abstract": "  In the rapidly evolving landscape of digital content creation, the demand for\nfast, convenient, and autonomous methods of crafting detailed 3D\nreconstructions of humans has grown significantly. Addressing this pressing\nneed, our AirNeRF system presents an innovative pathway to the creation of a\nrealistic 3D human avatar. Our approach leverages Neural Radiance Fields (NeRF)\nwith an automated drone-based video capturing method. The acquired data\nprovides a swift and precise way to create high-quality human body\nreconstructions following several stages of our system. The rigged mesh derived\nfrom our system proves to be an excellent foundation for free-view synthesis of\ndynamic humans, particularly well-suited for the immersive experiences within\ngaming and virtual reality.\n", "link": "http://arxiv.org/abs/2407.10865v1", "date": "2024-07-15", "relevancy": 2.8374, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5745}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5745}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirNeRF%3A%203D%20Reconstruction%20of%20Human%20with%20Drone%20and%20NeRF%20for%20Future%0A%20%20Communication%20Systems&body=Title%3A%20AirNeRF%3A%203D%20Reconstruction%20of%20Human%20with%20Drone%20and%20NeRF%20for%20Future%0A%20%20Communication%20Systems%0AAuthor%3A%20Alexey%20Kotcov%20and%20Maria%20Dronova%20and%20Vladislav%20Cheremnykh%20and%20Sausar%20Karaf%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20landscape%20of%20digital%20content%20creation%2C%20the%20demand%20for%0Afast%2C%20convenient%2C%20and%20autonomous%20methods%20of%20crafting%20detailed%203D%0Areconstructions%20of%20humans%20has%20grown%20significantly.%20Addressing%20this%20pressing%0Aneed%2C%20our%20AirNeRF%20system%20presents%20an%20innovative%20pathway%20to%20the%20creation%20of%20a%0Arealistic%203D%20human%20avatar.%20Our%20approach%20leverages%20Neural%20Radiance%20Fields%20%28NeRF%29%0Awith%20an%20automated%20drone-based%20video%20capturing%20method.%20The%20acquired%20data%0Aprovides%20a%20swift%20and%20precise%20way%20to%20create%20high-quality%20human%20body%0Areconstructions%20following%20several%20stages%20of%20our%20system.%20The%20rigged%20mesh%20derived%0Afrom%20our%20system%20proves%20to%20be%20an%20excellent%20foundation%20for%20free-view%20synthesis%20of%0Adynamic%20humans%2C%20particularly%20well-suited%20for%20the%20immersive%20experiences%20within%0Agaming%20and%20virtual%20reality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirNeRF%253A%25203D%2520Reconstruction%2520of%2520Human%2520with%2520Drone%2520and%2520NeRF%2520for%2520Future%250A%2520%2520Communication%2520Systems%26entry.906535625%3DAlexey%2520Kotcov%2520and%2520Maria%2520Dronova%2520and%2520Vladislav%2520Cheremnykh%2520and%2520Sausar%2520Karaf%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520digital%2520content%2520creation%252C%2520the%2520demand%2520for%250Afast%252C%2520convenient%252C%2520and%2520autonomous%2520methods%2520of%2520crafting%2520detailed%25203D%250Areconstructions%2520of%2520humans%2520has%2520grown%2520significantly.%2520Addressing%2520this%2520pressing%250Aneed%252C%2520our%2520AirNeRF%2520system%2520presents%2520an%2520innovative%2520pathway%2520to%2520the%2520creation%2520of%2520a%250Arealistic%25203D%2520human%2520avatar.%2520Our%2520approach%2520leverages%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%250Awith%2520an%2520automated%2520drone-based%2520video%2520capturing%2520method.%2520The%2520acquired%2520data%250Aprovides%2520a%2520swift%2520and%2520precise%2520way%2520to%2520create%2520high-quality%2520human%2520body%250Areconstructions%2520following%2520several%2520stages%2520of%2520our%2520system.%2520The%2520rigged%2520mesh%2520derived%250Afrom%2520our%2520system%2520proves%2520to%2520be%2520an%2520excellent%2520foundation%2520for%2520free-view%2520synthesis%2520of%250Adynamic%2520humans%252C%2520particularly%2520well-suited%2520for%2520the%2520immersive%2520experiences%2520within%250Agaming%2520and%2520virtual%2520reality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirNeRF%3A%203D%20Reconstruction%20of%20Human%20with%20Drone%20and%20NeRF%20for%20Future%0A%20%20Communication%20Systems&entry.906535625=Alexey%20Kotcov%20and%20Maria%20Dronova%20and%20Vladislav%20Cheremnykh%20and%20Sausar%20Karaf%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20digital%20content%20creation%2C%20the%20demand%20for%0Afast%2C%20convenient%2C%20and%20autonomous%20methods%20of%20crafting%20detailed%203D%0Areconstructions%20of%20humans%20has%20grown%20significantly.%20Addressing%20this%20pressing%0Aneed%2C%20our%20AirNeRF%20system%20presents%20an%20innovative%20pathway%20to%20the%20creation%20of%20a%0Arealistic%203D%20human%20avatar.%20Our%20approach%20leverages%20Neural%20Radiance%20Fields%20%28NeRF%29%0Awith%20an%20automated%20drone-based%20video%20capturing%20method.%20The%20acquired%20data%0Aprovides%20a%20swift%20and%20precise%20way%20to%20create%20high-quality%20human%20body%0Areconstructions%20following%20several%20stages%20of%20our%20system.%20The%20rigged%20mesh%20derived%0Afrom%20our%20system%20proves%20to%20be%20an%20excellent%20foundation%20for%20free-view%20synthesis%20of%0Adynamic%20humans%2C%20particularly%20well-suited%20for%20the%20immersive%20experiences%20within%0Agaming%20and%20virtual%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10865v1&entry.124074799=Read"},
{"title": "Score-based Generative Priors Guided Model-driven Network for MRI\n  Reconstruction", "author": "Xiaoyu Qiao and Weisheng Li and Bin Xiao and Yuping Huang and Lijian Yang", "abstract": "  Score matching with Langevin dynamics (SMLD) method has been successfully\napplied to accelerated MRI. However, the hyperparameters in the sampling\nprocess require subtle tuning, otherwise the results can be severely corrupted\nby hallucination artifacts, especially with out-of-distribution test data. To\naddress the limitations, we proposed a novel workflow where naive SMLD samples\nserve as additional priors to guide model-driven network training. First, we\nadopted a pretrained score network to generate samples as preliminary guidance\nimages (PGI), obviating the need for network retraining, parameter tuning and\nin-distribution test data. Although PGIs are corrupted by hallucination\nartifacts, we believe they can provide extra information through effective\ndenoising steps to facilitate reconstruction. Therefore, we designed a\ndenoising module (DM) in the second step to coarsely eliminate artifacts and\nnoises from PGIs. The features are extracted from a score-based information\nextractor (SIE) and a cross-domain information extractor (CIE), which directly\nmap to the noise patterns. Third, we designed a model-driven network guided by\ndenoised PGIs (DGIs) to further recover fine details. DGIs are densely\nconnected with intermediate reconstructions in each cascade to enrich the\ninformation and are periodically updated to provide more accurate guidance. Our\nexperiments on different datasets reveal that despite the low average quality\nof PGIs, the proposed workflow can effectively extract valuable information to\nguide the network training, even with severely reduced training data and\nsampling steps. Our method outperforms other cutting-edge techniques by\neffectively mitigating hallucination artifacts, yielding robust and\nhigh-quality reconstruction results.\n", "link": "http://arxiv.org/abs/2405.02958v2", "date": "2024-07-15", "relevancy": 2.7696, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5698}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.548}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Score-based%20Generative%20Priors%20Guided%20Model-driven%20Network%20for%20MRI%0A%20%20Reconstruction&body=Title%3A%20Score-based%20Generative%20Priors%20Guided%20Model-driven%20Network%20for%20MRI%0A%20%20Reconstruction%0AAuthor%3A%20Xiaoyu%20Qiao%20and%20Weisheng%20Li%20and%20Bin%20Xiao%20and%20Yuping%20Huang%20and%20Lijian%20Yang%0AAbstract%3A%20%20%20Score%20matching%20with%20Langevin%20dynamics%20%28SMLD%29%20method%20has%20been%20successfully%0Aapplied%20to%20accelerated%20MRI.%20However%2C%20the%20hyperparameters%20in%20the%20sampling%0Aprocess%20require%20subtle%20tuning%2C%20otherwise%20the%20results%20can%20be%20severely%20corrupted%0Aby%20hallucination%20artifacts%2C%20especially%20with%20out-of-distribution%20test%20data.%20To%0Aaddress%20the%20limitations%2C%20we%20proposed%20a%20novel%20workflow%20where%20naive%20SMLD%20samples%0Aserve%20as%20additional%20priors%20to%20guide%20model-driven%20network%20training.%20First%2C%20we%0Aadopted%20a%20pretrained%20score%20network%20to%20generate%20samples%20as%20preliminary%20guidance%0Aimages%20%28PGI%29%2C%20obviating%20the%20need%20for%20network%20retraining%2C%20parameter%20tuning%20and%0Ain-distribution%20test%20data.%20Although%20PGIs%20are%20corrupted%20by%20hallucination%0Aartifacts%2C%20we%20believe%20they%20can%20provide%20extra%20information%20through%20effective%0Adenoising%20steps%20to%20facilitate%20reconstruction.%20Therefore%2C%20we%20designed%20a%0Adenoising%20module%20%28DM%29%20in%20the%20second%20step%20to%20coarsely%20eliminate%20artifacts%20and%0Anoises%20from%20PGIs.%20The%20features%20are%20extracted%20from%20a%20score-based%20information%0Aextractor%20%28SIE%29%20and%20a%20cross-domain%20information%20extractor%20%28CIE%29%2C%20which%20directly%0Amap%20to%20the%20noise%20patterns.%20Third%2C%20we%20designed%20a%20model-driven%20network%20guided%20by%0Adenoised%20PGIs%20%28DGIs%29%20to%20further%20recover%20fine%20details.%20DGIs%20are%20densely%0Aconnected%20with%20intermediate%20reconstructions%20in%20each%20cascade%20to%20enrich%20the%0Ainformation%20and%20are%20periodically%20updated%20to%20provide%20more%20accurate%20guidance.%20Our%0Aexperiments%20on%20different%20datasets%20reveal%20that%20despite%20the%20low%20average%20quality%0Aof%20PGIs%2C%20the%20proposed%20workflow%20can%20effectively%20extract%20valuable%20information%20to%0Aguide%20the%20network%20training%2C%20even%20with%20severely%20reduced%20training%20data%20and%0Asampling%20steps.%20Our%20method%20outperforms%20other%20cutting-edge%20techniques%20by%0Aeffectively%20mitigating%20hallucination%20artifacts%2C%20yielding%20robust%20and%0Ahigh-quality%20reconstruction%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScore-based%2520Generative%2520Priors%2520Guided%2520Model-driven%2520Network%2520for%2520MRI%250A%2520%2520Reconstruction%26entry.906535625%3DXiaoyu%2520Qiao%2520and%2520Weisheng%2520Li%2520and%2520Bin%2520Xiao%2520and%2520Yuping%2520Huang%2520and%2520Lijian%2520Yang%26entry.1292438233%3D%2520%2520Score%2520matching%2520with%2520Langevin%2520dynamics%2520%2528SMLD%2529%2520method%2520has%2520been%2520successfully%250Aapplied%2520to%2520accelerated%2520MRI.%2520However%252C%2520the%2520hyperparameters%2520in%2520the%2520sampling%250Aprocess%2520require%2520subtle%2520tuning%252C%2520otherwise%2520the%2520results%2520can%2520be%2520severely%2520corrupted%250Aby%2520hallucination%2520artifacts%252C%2520especially%2520with%2520out-of-distribution%2520test%2520data.%2520To%250Aaddress%2520the%2520limitations%252C%2520we%2520proposed%2520a%2520novel%2520workflow%2520where%2520naive%2520SMLD%2520samples%250Aserve%2520as%2520additional%2520priors%2520to%2520guide%2520model-driven%2520network%2520training.%2520First%252C%2520we%250Aadopted%2520a%2520pretrained%2520score%2520network%2520to%2520generate%2520samples%2520as%2520preliminary%2520guidance%250Aimages%2520%2528PGI%2529%252C%2520obviating%2520the%2520need%2520for%2520network%2520retraining%252C%2520parameter%2520tuning%2520and%250Ain-distribution%2520test%2520data.%2520Although%2520PGIs%2520are%2520corrupted%2520by%2520hallucination%250Aartifacts%252C%2520we%2520believe%2520they%2520can%2520provide%2520extra%2520information%2520through%2520effective%250Adenoising%2520steps%2520to%2520facilitate%2520reconstruction.%2520Therefore%252C%2520we%2520designed%2520a%250Adenoising%2520module%2520%2528DM%2529%2520in%2520the%2520second%2520step%2520to%2520coarsely%2520eliminate%2520artifacts%2520and%250Anoises%2520from%2520PGIs.%2520The%2520features%2520are%2520extracted%2520from%2520a%2520score-based%2520information%250Aextractor%2520%2528SIE%2529%2520and%2520a%2520cross-domain%2520information%2520extractor%2520%2528CIE%2529%252C%2520which%2520directly%250Amap%2520to%2520the%2520noise%2520patterns.%2520Third%252C%2520we%2520designed%2520a%2520model-driven%2520network%2520guided%2520by%250Adenoised%2520PGIs%2520%2528DGIs%2529%2520to%2520further%2520recover%2520fine%2520details.%2520DGIs%2520are%2520densely%250Aconnected%2520with%2520intermediate%2520reconstructions%2520in%2520each%2520cascade%2520to%2520enrich%2520the%250Ainformation%2520and%2520are%2520periodically%2520updated%2520to%2520provide%2520more%2520accurate%2520guidance.%2520Our%250Aexperiments%2520on%2520different%2520datasets%2520reveal%2520that%2520despite%2520the%2520low%2520average%2520quality%250Aof%2520PGIs%252C%2520the%2520proposed%2520workflow%2520can%2520effectively%2520extract%2520valuable%2520information%2520to%250Aguide%2520the%2520network%2520training%252C%2520even%2520with%2520severely%2520reduced%2520training%2520data%2520and%250Asampling%2520steps.%2520Our%2520method%2520outperforms%2520other%2520cutting-edge%2520techniques%2520by%250Aeffectively%2520mitigating%2520hallucination%2520artifacts%252C%2520yielding%2520robust%2520and%250Ahigh-quality%2520reconstruction%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Score-based%20Generative%20Priors%20Guided%20Model-driven%20Network%20for%20MRI%0A%20%20Reconstruction&entry.906535625=Xiaoyu%20Qiao%20and%20Weisheng%20Li%20and%20Bin%20Xiao%20and%20Yuping%20Huang%20and%20Lijian%20Yang&entry.1292438233=%20%20Score%20matching%20with%20Langevin%20dynamics%20%28SMLD%29%20method%20has%20been%20successfully%0Aapplied%20to%20accelerated%20MRI.%20However%2C%20the%20hyperparameters%20in%20the%20sampling%0Aprocess%20require%20subtle%20tuning%2C%20otherwise%20the%20results%20can%20be%20severely%20corrupted%0Aby%20hallucination%20artifacts%2C%20especially%20with%20out-of-distribution%20test%20data.%20To%0Aaddress%20the%20limitations%2C%20we%20proposed%20a%20novel%20workflow%20where%20naive%20SMLD%20samples%0Aserve%20as%20additional%20priors%20to%20guide%20model-driven%20network%20training.%20First%2C%20we%0Aadopted%20a%20pretrained%20score%20network%20to%20generate%20samples%20as%20preliminary%20guidance%0Aimages%20%28PGI%29%2C%20obviating%20the%20need%20for%20network%20retraining%2C%20parameter%20tuning%20and%0Ain-distribution%20test%20data.%20Although%20PGIs%20are%20corrupted%20by%20hallucination%0Aartifacts%2C%20we%20believe%20they%20can%20provide%20extra%20information%20through%20effective%0Adenoising%20steps%20to%20facilitate%20reconstruction.%20Therefore%2C%20we%20designed%20a%0Adenoising%20module%20%28DM%29%20in%20the%20second%20step%20to%20coarsely%20eliminate%20artifacts%20and%0Anoises%20from%20PGIs.%20The%20features%20are%20extracted%20from%20a%20score-based%20information%0Aextractor%20%28SIE%29%20and%20a%20cross-domain%20information%20extractor%20%28CIE%29%2C%20which%20directly%0Amap%20to%20the%20noise%20patterns.%20Third%2C%20we%20designed%20a%20model-driven%20network%20guided%20by%0Adenoised%20PGIs%20%28DGIs%29%20to%20further%20recover%20fine%20details.%20DGIs%20are%20densely%0Aconnected%20with%20intermediate%20reconstructions%20in%20each%20cascade%20to%20enrich%20the%0Ainformation%20and%20are%20periodically%20updated%20to%20provide%20more%20accurate%20guidance.%20Our%0Aexperiments%20on%20different%20datasets%20reveal%20that%20despite%20the%20low%20average%20quality%0Aof%20PGIs%2C%20the%20proposed%20workflow%20can%20effectively%20extract%20valuable%20information%20to%0Aguide%20the%20network%20training%2C%20even%20with%20severely%20reduced%20training%20data%20and%0Asampling%20steps.%20Our%20method%20outperforms%20other%20cutting-edge%20techniques%20by%0Aeffectively%20mitigating%20hallucination%20artifacts%2C%20yielding%20robust%20and%0Ahigh-quality%20reconstruction%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02958v2&entry.124074799=Read"},
{"title": "FRI-Net: Floorplan Reconstruction via Room-wise Implicit Representation", "author": "Honghao Xu and Juzhan Xu and Zeyu Huang and Pengfei Xu and Hui Huang and Ruizhen Hu", "abstract": "  In this paper, we introduce a novel method called FRI-Net for 2D floorplan\nreconstruction from 3D point cloud. Existing methods typically rely on corner\nregression or box regression, which lack consideration for the global shapes of\nrooms. To address these issues, we propose a novel approach using a room-wise\nimplicit representation with structural regularization to characterize the\nshapes of rooms in floorplans. By incorporating geometric priors of room\nlayouts in floorplans into our training strategy, the generated room polygons\nare more geometrically regular. We have conducted experiments on two\nchallenging datasets, Structured3D and SceneCAD. Our method demonstrates\nimproved performance compared to state-of-the-art methods, validating the\neffectiveness of our proposed representation for floorplan reconstruction.\n", "link": "http://arxiv.org/abs/2407.10687v1", "date": "2024-07-15", "relevancy": 2.7669, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5896}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5369}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRI-Net%3A%20Floorplan%20Reconstruction%20via%20Room-wise%20Implicit%20Representation&body=Title%3A%20FRI-Net%3A%20Floorplan%20Reconstruction%20via%20Room-wise%20Implicit%20Representation%0AAuthor%3A%20Honghao%20Xu%20and%20Juzhan%20Xu%20and%20Zeyu%20Huang%20and%20Pengfei%20Xu%20and%20Hui%20Huang%20and%20Ruizhen%20Hu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20called%20FRI-Net%20for%202D%20floorplan%0Areconstruction%20from%203D%20point%20cloud.%20Existing%20methods%20typically%20rely%20on%20corner%0Aregression%20or%20box%20regression%2C%20which%20lack%20consideration%20for%20the%20global%20shapes%20of%0Arooms.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20approach%20using%20a%20room-wise%0Aimplicit%20representation%20with%20structural%20regularization%20to%20characterize%20the%0Ashapes%20of%20rooms%20in%20floorplans.%20By%20incorporating%20geometric%20priors%20of%20room%0Alayouts%20in%20floorplans%20into%20our%20training%20strategy%2C%20the%20generated%20room%20polygons%0Aare%20more%20geometrically%20regular.%20We%20have%20conducted%20experiments%20on%20two%0Achallenging%20datasets%2C%20Structured3D%20and%20SceneCAD.%20Our%20method%20demonstrates%0Aimproved%20performance%20compared%20to%20state-of-the-art%20methods%2C%20validating%20the%0Aeffectiveness%20of%20our%20proposed%20representation%20for%20floorplan%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRI-Net%253A%2520Floorplan%2520Reconstruction%2520via%2520Room-wise%2520Implicit%2520Representation%26entry.906535625%3DHonghao%2520Xu%2520and%2520Juzhan%2520Xu%2520and%2520Zeyu%2520Huang%2520and%2520Pengfei%2520Xu%2520and%2520Hui%2520Huang%2520and%2520Ruizhen%2520Hu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520method%2520called%2520FRI-Net%2520for%25202D%2520floorplan%250Areconstruction%2520from%25203D%2520point%2520cloud.%2520Existing%2520methods%2520typically%2520rely%2520on%2520corner%250Aregression%2520or%2520box%2520regression%252C%2520which%2520lack%2520consideration%2520for%2520the%2520global%2520shapes%2520of%250Arooms.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520approach%2520using%2520a%2520room-wise%250Aimplicit%2520representation%2520with%2520structural%2520regularization%2520to%2520characterize%2520the%250Ashapes%2520of%2520rooms%2520in%2520floorplans.%2520By%2520incorporating%2520geometric%2520priors%2520of%2520room%250Alayouts%2520in%2520floorplans%2520into%2520our%2520training%2520strategy%252C%2520the%2520generated%2520room%2520polygons%250Aare%2520more%2520geometrically%2520regular.%2520We%2520have%2520conducted%2520experiments%2520on%2520two%250Achallenging%2520datasets%252C%2520Structured3D%2520and%2520SceneCAD.%2520Our%2520method%2520demonstrates%250Aimproved%2520performance%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520validating%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520representation%2520for%2520floorplan%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRI-Net%3A%20Floorplan%20Reconstruction%20via%20Room-wise%20Implicit%20Representation&entry.906535625=Honghao%20Xu%20and%20Juzhan%20Xu%20and%20Zeyu%20Huang%20and%20Pengfei%20Xu%20and%20Hui%20Huang%20and%20Ruizhen%20Hu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20called%20FRI-Net%20for%202D%20floorplan%0Areconstruction%20from%203D%20point%20cloud.%20Existing%20methods%20typically%20rely%20on%20corner%0Aregression%20or%20box%20regression%2C%20which%20lack%20consideration%20for%20the%20global%20shapes%20of%0Arooms.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20approach%20using%20a%20room-wise%0Aimplicit%20representation%20with%20structural%20regularization%20to%20characterize%20the%0Ashapes%20of%20rooms%20in%20floorplans.%20By%20incorporating%20geometric%20priors%20of%20room%0Alayouts%20in%20floorplans%20into%20our%20training%20strategy%2C%20the%20generated%20room%20polygons%0Aare%20more%20geometrically%20regular.%20We%20have%20conducted%20experiments%20on%20two%0Achallenging%20datasets%2C%20Structured3D%20and%20SceneCAD.%20Our%20method%20demonstrates%0Aimproved%20performance%20compared%20to%20state-of-the-art%20methods%2C%20validating%20the%0Aeffectiveness%20of%20our%20proposed%20representation%20for%20floorplan%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10687v1&entry.124074799=Read"},
{"title": "GOEmbed: Gradient Origin Embeddings for Representation Agnostic 3D\n  Feature Learning", "author": "Animesh Karnewar and Roman Shapovalov and Tom Monnier and Andrea Vedaldi and Niloy J. Mitra and David Novotny", "abstract": "  Encoding information from 2D views of an object into a 3D representation is\ncrucial for generalized 3D feature extraction. Such features can then enable 3D\nreconstruction, 3D generation, and other applications. We propose GOEmbed\n(Gradient Origin Embeddings) that encodes input 2D images into any 3D\nrepresentation, without requiring a pre-trained image feature extractor; unlike\ntypical prior approaches in which input images are either encoded using 2D\nfeatures extracted from large pre-trained models, or customized features are\ndesigned to handle different 3D representations; or worse, encoders may not yet\nbe available for specialized 3D neural representations such as MLPs and\nhash-grids. We extensively evaluate our proposed GOEmbed under different\nexperimental settings on the OmniObject3D benchmark. First, we evaluate how\nwell the mechanism compares against prior encoding mechanisms on multiple 3D\nrepresentations using an illustrative experiment called Plenoptic-Encoding.\nSecond, the efficacy of the GOEmbed mechanism is further demonstrated by\nachieving a new SOTA FID of 22.12 on the OmniObject3D generation task using a\ncombination of GOEmbed and DFM (Diffusion with Forward Models), which we call\nGOEmbedFusion. Finally, we evaluate how the GOEmbed mechanism bolsters\nsparse-view 3D reconstruction pipelines.\n", "link": "http://arxiv.org/abs/2312.08744v2", "date": "2024-07-15", "relevancy": 2.7526, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5622}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GOEmbed%3A%20Gradient%20Origin%20Embeddings%20for%20Representation%20Agnostic%203D%0A%20%20Feature%20Learning&body=Title%3A%20GOEmbed%3A%20Gradient%20Origin%20Embeddings%20for%20Representation%20Agnostic%203D%0A%20%20Feature%20Learning%0AAuthor%3A%20Animesh%20Karnewar%20and%20Roman%20Shapovalov%20and%20Tom%20Monnier%20and%20Andrea%20Vedaldi%20and%20Niloy%20J.%20Mitra%20and%20David%20Novotny%0AAbstract%3A%20%20%20Encoding%20information%20from%202D%20views%20of%20an%20object%20into%20a%203D%20representation%20is%0Acrucial%20for%20generalized%203D%20feature%20extraction.%20Such%20features%20can%20then%20enable%203D%0Areconstruction%2C%203D%20generation%2C%20and%20other%20applications.%20We%20propose%20GOEmbed%0A%28Gradient%20Origin%20Embeddings%29%20that%20encodes%20input%202D%20images%20into%20any%203D%0Arepresentation%2C%20without%20requiring%20a%20pre-trained%20image%20feature%20extractor%3B%20unlike%0Atypical%20prior%20approaches%20in%20which%20input%20images%20are%20either%20encoded%20using%202D%0Afeatures%20extracted%20from%20large%20pre-trained%20models%2C%20or%20customized%20features%20are%0Adesigned%20to%20handle%20different%203D%20representations%3B%20or%20worse%2C%20encoders%20may%20not%20yet%0Abe%20available%20for%20specialized%203D%20neural%20representations%20such%20as%20MLPs%20and%0Ahash-grids.%20We%20extensively%20evaluate%20our%20proposed%20GOEmbed%20under%20different%0Aexperimental%20settings%20on%20the%20OmniObject3D%20benchmark.%20First%2C%20we%20evaluate%20how%0Awell%20the%20mechanism%20compares%20against%20prior%20encoding%20mechanisms%20on%20multiple%203D%0Arepresentations%20using%20an%20illustrative%20experiment%20called%20Plenoptic-Encoding.%0ASecond%2C%20the%20efficacy%20of%20the%20GOEmbed%20mechanism%20is%20further%20demonstrated%20by%0Aachieving%20a%20new%20SOTA%20FID%20of%2022.12%20on%20the%20OmniObject3D%20generation%20task%20using%20a%0Acombination%20of%20GOEmbed%20and%20DFM%20%28Diffusion%20with%20Forward%20Models%29%2C%20which%20we%20call%0AGOEmbedFusion.%20Finally%2C%20we%20evaluate%20how%20the%20GOEmbed%20mechanism%20bolsters%0Asparse-view%203D%20reconstruction%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08744v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGOEmbed%253A%2520Gradient%2520Origin%2520Embeddings%2520for%2520Representation%2520Agnostic%25203D%250A%2520%2520Feature%2520Learning%26entry.906535625%3DAnimesh%2520Karnewar%2520and%2520Roman%2520Shapovalov%2520and%2520Tom%2520Monnier%2520and%2520Andrea%2520Vedaldi%2520and%2520Niloy%2520J.%2520Mitra%2520and%2520David%2520Novotny%26entry.1292438233%3D%2520%2520Encoding%2520information%2520from%25202D%2520views%2520of%2520an%2520object%2520into%2520a%25203D%2520representation%2520is%250Acrucial%2520for%2520generalized%25203D%2520feature%2520extraction.%2520Such%2520features%2520can%2520then%2520enable%25203D%250Areconstruction%252C%25203D%2520generation%252C%2520and%2520other%2520applications.%2520We%2520propose%2520GOEmbed%250A%2528Gradient%2520Origin%2520Embeddings%2529%2520that%2520encodes%2520input%25202D%2520images%2520into%2520any%25203D%250Arepresentation%252C%2520without%2520requiring%2520a%2520pre-trained%2520image%2520feature%2520extractor%253B%2520unlike%250Atypical%2520prior%2520approaches%2520in%2520which%2520input%2520images%2520are%2520either%2520encoded%2520using%25202D%250Afeatures%2520extracted%2520from%2520large%2520pre-trained%2520models%252C%2520or%2520customized%2520features%2520are%250Adesigned%2520to%2520handle%2520different%25203D%2520representations%253B%2520or%2520worse%252C%2520encoders%2520may%2520not%2520yet%250Abe%2520available%2520for%2520specialized%25203D%2520neural%2520representations%2520such%2520as%2520MLPs%2520and%250Ahash-grids.%2520We%2520extensively%2520evaluate%2520our%2520proposed%2520GOEmbed%2520under%2520different%250Aexperimental%2520settings%2520on%2520the%2520OmniObject3D%2520benchmark.%2520First%252C%2520we%2520evaluate%2520how%250Awell%2520the%2520mechanism%2520compares%2520against%2520prior%2520encoding%2520mechanisms%2520on%2520multiple%25203D%250Arepresentations%2520using%2520an%2520illustrative%2520experiment%2520called%2520Plenoptic-Encoding.%250ASecond%252C%2520the%2520efficacy%2520of%2520the%2520GOEmbed%2520mechanism%2520is%2520further%2520demonstrated%2520by%250Aachieving%2520a%2520new%2520SOTA%2520FID%2520of%252022.12%2520on%2520the%2520OmniObject3D%2520generation%2520task%2520using%2520a%250Acombination%2520of%2520GOEmbed%2520and%2520DFM%2520%2528Diffusion%2520with%2520Forward%2520Models%2529%252C%2520which%2520we%2520call%250AGOEmbedFusion.%2520Finally%252C%2520we%2520evaluate%2520how%2520the%2520GOEmbed%2520mechanism%2520bolsters%250Asparse-view%25203D%2520reconstruction%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08744v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOEmbed%3A%20Gradient%20Origin%20Embeddings%20for%20Representation%20Agnostic%203D%0A%20%20Feature%20Learning&entry.906535625=Animesh%20Karnewar%20and%20Roman%20Shapovalov%20and%20Tom%20Monnier%20and%20Andrea%20Vedaldi%20and%20Niloy%20J.%20Mitra%20and%20David%20Novotny&entry.1292438233=%20%20Encoding%20information%20from%202D%20views%20of%20an%20object%20into%20a%203D%20representation%20is%0Acrucial%20for%20generalized%203D%20feature%20extraction.%20Such%20features%20can%20then%20enable%203D%0Areconstruction%2C%203D%20generation%2C%20and%20other%20applications.%20We%20propose%20GOEmbed%0A%28Gradient%20Origin%20Embeddings%29%20that%20encodes%20input%202D%20images%20into%20any%203D%0Arepresentation%2C%20without%20requiring%20a%20pre-trained%20image%20feature%20extractor%3B%20unlike%0Atypical%20prior%20approaches%20in%20which%20input%20images%20are%20either%20encoded%20using%202D%0Afeatures%20extracted%20from%20large%20pre-trained%20models%2C%20or%20customized%20features%20are%0Adesigned%20to%20handle%20different%203D%20representations%3B%20or%20worse%2C%20encoders%20may%20not%20yet%0Abe%20available%20for%20specialized%203D%20neural%20representations%20such%20as%20MLPs%20and%0Ahash-grids.%20We%20extensively%20evaluate%20our%20proposed%20GOEmbed%20under%20different%0Aexperimental%20settings%20on%20the%20OmniObject3D%20benchmark.%20First%2C%20we%20evaluate%20how%0Awell%20the%20mechanism%20compares%20against%20prior%20encoding%20mechanisms%20on%20multiple%203D%0Arepresentations%20using%20an%20illustrative%20experiment%20called%20Plenoptic-Encoding.%0ASecond%2C%20the%20efficacy%20of%20the%20GOEmbed%20mechanism%20is%20further%20demonstrated%20by%0Aachieving%20a%20new%20SOTA%20FID%20of%2022.12%20on%20the%20OmniObject3D%20generation%20task%20using%20a%0Acombination%20of%20GOEmbed%20and%20DFM%20%28Diffusion%20with%20Forward%20Models%29%2C%20which%20we%20call%0AGOEmbedFusion.%20Finally%2C%20we%20evaluate%20how%20the%20GOEmbed%20mechanism%20bolsters%0Asparse-view%203D%20reconstruction%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08744v2&entry.124074799=Read"},
{"title": "Joint-Embedding Predictive Architecture for Self-Supervised Learning of\n  Mask Classification Architecture", "author": "Dong-Hee Kim and Sungduk Cho and Hyeonwoo Cho and Chanmin Park and Jinyoung Kim and Won Hwa Kim", "abstract": "  In this work, we introduce Mask-JEPA, a self-supervised learning framework\ntailored for mask classification architectures (MCA), to overcome the\ntraditional constraints associated with training segmentation models. Mask-JEPA\ncombines a Joint Embedding Predictive Architecture with MCA to adeptly capture\nintricate semantics and precise object boundaries. Our approach addresses two\ncritical challenges in self-supervised learning: 1) extracting comprehensive\nrepresentations for universal image segmentation from a pixel decoder, and 2)\neffectively training the transformer decoder. The use of the transformer\ndecoder as a predictor within the JEPA framework allows proficient training in\nuniversal image segmentation tasks. Through rigorous evaluations on datasets\nsuch as ADE20K, Cityscapes and COCO, Mask-JEPA demonstrates not only\ncompetitive results but also exceptional adaptability and robustness across\nvarious training scenarios. The architecture-agnostic nature of Mask-JEPA\nfurther underscores its versatility, allowing seamless adaptation to various\nmask classification family.\n", "link": "http://arxiv.org/abs/2407.10733v1", "date": "2024-07-15", "relevancy": 2.7441, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5692}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint-Embedding%20Predictive%20Architecture%20for%20Self-Supervised%20Learning%20of%0A%20%20Mask%20Classification%20Architecture&body=Title%3A%20Joint-Embedding%20Predictive%20Architecture%20for%20Self-Supervised%20Learning%20of%0A%20%20Mask%20Classification%20Architecture%0AAuthor%3A%20Dong-Hee%20Kim%20and%20Sungduk%20Cho%20and%20Hyeonwoo%20Cho%20and%20Chanmin%20Park%20and%20Jinyoung%20Kim%20and%20Won%20Hwa%20Kim%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Mask-JEPA%2C%20a%20self-supervised%20learning%20framework%0Atailored%20for%20mask%20classification%20architectures%20%28MCA%29%2C%20to%20overcome%20the%0Atraditional%20constraints%20associated%20with%20training%20segmentation%20models.%20Mask-JEPA%0Acombines%20a%20Joint%20Embedding%20Predictive%20Architecture%20with%20MCA%20to%20adeptly%20capture%0Aintricate%20semantics%20and%20precise%20object%20boundaries.%20Our%20approach%20addresses%20two%0Acritical%20challenges%20in%20self-supervised%20learning%3A%201%29%20extracting%20comprehensive%0Arepresentations%20for%20universal%20image%20segmentation%20from%20a%20pixel%20decoder%2C%20and%202%29%0Aeffectively%20training%20the%20transformer%20decoder.%20The%20use%20of%20the%20transformer%0Adecoder%20as%20a%20predictor%20within%20the%20JEPA%20framework%20allows%20proficient%20training%20in%0Auniversal%20image%20segmentation%20tasks.%20Through%20rigorous%20evaluations%20on%20datasets%0Asuch%20as%20ADE20K%2C%20Cityscapes%20and%20COCO%2C%20Mask-JEPA%20demonstrates%20not%20only%0Acompetitive%20results%20but%20also%20exceptional%20adaptability%20and%20robustness%20across%0Avarious%20training%20scenarios.%20The%20architecture-agnostic%20nature%20of%20Mask-JEPA%0Afurther%20underscores%20its%20versatility%2C%20allowing%20seamless%20adaptation%20to%20various%0Amask%20classification%20family.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint-Embedding%2520Predictive%2520Architecture%2520for%2520Self-Supervised%2520Learning%2520of%250A%2520%2520Mask%2520Classification%2520Architecture%26entry.906535625%3DDong-Hee%2520Kim%2520and%2520Sungduk%2520Cho%2520and%2520Hyeonwoo%2520Cho%2520and%2520Chanmin%2520Park%2520and%2520Jinyoung%2520Kim%2520and%2520Won%2520Hwa%2520Kim%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Mask-JEPA%252C%2520a%2520self-supervised%2520learning%2520framework%250Atailored%2520for%2520mask%2520classification%2520architectures%2520%2528MCA%2529%252C%2520to%2520overcome%2520the%250Atraditional%2520constraints%2520associated%2520with%2520training%2520segmentation%2520models.%2520Mask-JEPA%250Acombines%2520a%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520with%2520MCA%2520to%2520adeptly%2520capture%250Aintricate%2520semantics%2520and%2520precise%2520object%2520boundaries.%2520Our%2520approach%2520addresses%2520two%250Acritical%2520challenges%2520in%2520self-supervised%2520learning%253A%25201%2529%2520extracting%2520comprehensive%250Arepresentations%2520for%2520universal%2520image%2520segmentation%2520from%2520a%2520pixel%2520decoder%252C%2520and%25202%2529%250Aeffectively%2520training%2520the%2520transformer%2520decoder.%2520The%2520use%2520of%2520the%2520transformer%250Adecoder%2520as%2520a%2520predictor%2520within%2520the%2520JEPA%2520framework%2520allows%2520proficient%2520training%2520in%250Auniversal%2520image%2520segmentation%2520tasks.%2520Through%2520rigorous%2520evaluations%2520on%2520datasets%250Asuch%2520as%2520ADE20K%252C%2520Cityscapes%2520and%2520COCO%252C%2520Mask-JEPA%2520demonstrates%2520not%2520only%250Acompetitive%2520results%2520but%2520also%2520exceptional%2520adaptability%2520and%2520robustness%2520across%250Avarious%2520training%2520scenarios.%2520The%2520architecture-agnostic%2520nature%2520of%2520Mask-JEPA%250Afurther%2520underscores%2520its%2520versatility%252C%2520allowing%2520seamless%2520adaptation%2520to%2520various%250Amask%2520classification%2520family.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint-Embedding%20Predictive%20Architecture%20for%20Self-Supervised%20Learning%20of%0A%20%20Mask%20Classification%20Architecture&entry.906535625=Dong-Hee%20Kim%20and%20Sungduk%20Cho%20and%20Hyeonwoo%20Cho%20and%20Chanmin%20Park%20and%20Jinyoung%20Kim%20and%20Won%20Hwa%20Kim&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Mask-JEPA%2C%20a%20self-supervised%20learning%20framework%0Atailored%20for%20mask%20classification%20architectures%20%28MCA%29%2C%20to%20overcome%20the%0Atraditional%20constraints%20associated%20with%20training%20segmentation%20models.%20Mask-JEPA%0Acombines%20a%20Joint%20Embedding%20Predictive%20Architecture%20with%20MCA%20to%20adeptly%20capture%0Aintricate%20semantics%20and%20precise%20object%20boundaries.%20Our%20approach%20addresses%20two%0Acritical%20challenges%20in%20self-supervised%20learning%3A%201%29%20extracting%20comprehensive%0Arepresentations%20for%20universal%20image%20segmentation%20from%20a%20pixel%20decoder%2C%20and%202%29%0Aeffectively%20training%20the%20transformer%20decoder.%20The%20use%20of%20the%20transformer%0Adecoder%20as%20a%20predictor%20within%20the%20JEPA%20framework%20allows%20proficient%20training%20in%0Auniversal%20image%20segmentation%20tasks.%20Through%20rigorous%20evaluations%20on%20datasets%0Asuch%20as%20ADE20K%2C%20Cityscapes%20and%20COCO%2C%20Mask-JEPA%20demonstrates%20not%20only%0Acompetitive%20results%20but%20also%20exceptional%20adaptability%20and%20robustness%20across%0Avarious%20training%20scenarios.%20The%20architecture-agnostic%20nature%20of%20Mask-JEPA%0Afurther%20underscores%20its%20versatility%2C%20allowing%20seamless%20adaptation%20to%20various%0Amask%20classification%20family.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10733v1&entry.124074799=Read"},
{"title": "STARS: Self-supervised Tuning for 3D Action Recognition in Skeleton\n  Sequences", "author": "Soroush Mehraban and Mohammad Javad Rajabi and Babak Taati", "abstract": "  Self-supervised pretraining methods with masked prediction demonstrate\nremarkable within-dataset performance in skeleton-based action recognition.\nHowever, we show that, unlike contrastive learning approaches, they do not\nproduce well-separated clusters. Additionally, these methods struggle with\ngeneralization in few-shot settings. To address these issues, we propose\nSelf-supervised Tuning for 3D Action Recognition in Skeleton sequences (STARS).\nSpecifically, STARS first uses a masked prediction stage using an\nencoder-decoder architecture. It then employs nearest-neighbor contrastive\nlearning to partially tune the weights of the encoder, enhancing the formation\nof semantic clusters for different actions. By tuning the encoder for a few\nepochs, and without using hand-crafted data augmentations, STARS achieves\nstate-of-the-art self-supervised results in various benchmarks, including\nNTU-60, NTU-120, and PKU-MMD. In addition, STARS exhibits significantly better\nresults than masked prediction models in few-shot settings, where the model has\nnot seen the actions throughout pretraining. Project page:\nhttps://soroushmehraban.github.io/stars/\n", "link": "http://arxiv.org/abs/2407.10935v1", "date": "2024-07-15", "relevancy": 2.737, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STARS%3A%20Self-supervised%20Tuning%20for%203D%20Action%20Recognition%20in%20Skeleton%0A%20%20Sequences&body=Title%3A%20STARS%3A%20Self-supervised%20Tuning%20for%203D%20Action%20Recognition%20in%20Skeleton%0A%20%20Sequences%0AAuthor%3A%20Soroush%20Mehraban%20and%20Mohammad%20Javad%20Rajabi%20and%20Babak%20Taati%0AAbstract%3A%20%20%20Self-supervised%20pretraining%20methods%20with%20masked%20prediction%20demonstrate%0Aremarkable%20within-dataset%20performance%20in%20skeleton-based%20action%20recognition.%0AHowever%2C%20we%20show%20that%2C%20unlike%20contrastive%20learning%20approaches%2C%20they%20do%20not%0Aproduce%20well-separated%20clusters.%20Additionally%2C%20these%20methods%20struggle%20with%0Ageneralization%20in%20few-shot%20settings.%20To%20address%20these%20issues%2C%20we%20propose%0ASelf-supervised%20Tuning%20for%203D%20Action%20Recognition%20in%20Skeleton%20sequences%20%28STARS%29.%0ASpecifically%2C%20STARS%20first%20uses%20a%20masked%20prediction%20stage%20using%20an%0Aencoder-decoder%20architecture.%20It%20then%20employs%20nearest-neighbor%20contrastive%0Alearning%20to%20partially%20tune%20the%20weights%20of%20the%20encoder%2C%20enhancing%20the%20formation%0Aof%20semantic%20clusters%20for%20different%20actions.%20By%20tuning%20the%20encoder%20for%20a%20few%0Aepochs%2C%20and%20without%20using%20hand-crafted%20data%20augmentations%2C%20STARS%20achieves%0Astate-of-the-art%20self-supervised%20results%20in%20various%20benchmarks%2C%20including%0ANTU-60%2C%20NTU-120%2C%20and%20PKU-MMD.%20In%20addition%2C%20STARS%20exhibits%20significantly%20better%0Aresults%20than%20masked%20prediction%20models%20in%20few-shot%20settings%2C%20where%20the%20model%20has%0Anot%20seen%20the%20actions%20throughout%20pretraining.%20Project%20page%3A%0Ahttps%3A//soroushmehraban.github.io/stars/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTARS%253A%2520Self-supervised%2520Tuning%2520for%25203D%2520Action%2520Recognition%2520in%2520Skeleton%250A%2520%2520Sequences%26entry.906535625%3DSoroush%2520Mehraban%2520and%2520Mohammad%2520Javad%2520Rajabi%2520and%2520Babak%2520Taati%26entry.1292438233%3D%2520%2520Self-supervised%2520pretraining%2520methods%2520with%2520masked%2520prediction%2520demonstrate%250Aremarkable%2520within-dataset%2520performance%2520in%2520skeleton-based%2520action%2520recognition.%250AHowever%252C%2520we%2520show%2520that%252C%2520unlike%2520contrastive%2520learning%2520approaches%252C%2520they%2520do%2520not%250Aproduce%2520well-separated%2520clusters.%2520Additionally%252C%2520these%2520methods%2520struggle%2520with%250Ageneralization%2520in%2520few-shot%2520settings.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ASelf-supervised%2520Tuning%2520for%25203D%2520Action%2520Recognition%2520in%2520Skeleton%2520sequences%2520%2528STARS%2529.%250ASpecifically%252C%2520STARS%2520first%2520uses%2520a%2520masked%2520prediction%2520stage%2520using%2520an%250Aencoder-decoder%2520architecture.%2520It%2520then%2520employs%2520nearest-neighbor%2520contrastive%250Alearning%2520to%2520partially%2520tune%2520the%2520weights%2520of%2520the%2520encoder%252C%2520enhancing%2520the%2520formation%250Aof%2520semantic%2520clusters%2520for%2520different%2520actions.%2520By%2520tuning%2520the%2520encoder%2520for%2520a%2520few%250Aepochs%252C%2520and%2520without%2520using%2520hand-crafted%2520data%2520augmentations%252C%2520STARS%2520achieves%250Astate-of-the-art%2520self-supervised%2520results%2520in%2520various%2520benchmarks%252C%2520including%250ANTU-60%252C%2520NTU-120%252C%2520and%2520PKU-MMD.%2520In%2520addition%252C%2520STARS%2520exhibits%2520significantly%2520better%250Aresults%2520than%2520masked%2520prediction%2520models%2520in%2520few-shot%2520settings%252C%2520where%2520the%2520model%2520has%250Anot%2520seen%2520the%2520actions%2520throughout%2520pretraining.%2520Project%2520page%253A%250Ahttps%253A//soroushmehraban.github.io/stars/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STARS%3A%20Self-supervised%20Tuning%20for%203D%20Action%20Recognition%20in%20Skeleton%0A%20%20Sequences&entry.906535625=Soroush%20Mehraban%20and%20Mohammad%20Javad%20Rajabi%20and%20Babak%20Taati&entry.1292438233=%20%20Self-supervised%20pretraining%20methods%20with%20masked%20prediction%20demonstrate%0Aremarkable%20within-dataset%20performance%20in%20skeleton-based%20action%20recognition.%0AHowever%2C%20we%20show%20that%2C%20unlike%20contrastive%20learning%20approaches%2C%20they%20do%20not%0Aproduce%20well-separated%20clusters.%20Additionally%2C%20these%20methods%20struggle%20with%0Ageneralization%20in%20few-shot%20settings.%20To%20address%20these%20issues%2C%20we%20propose%0ASelf-supervised%20Tuning%20for%203D%20Action%20Recognition%20in%20Skeleton%20sequences%20%28STARS%29.%0ASpecifically%2C%20STARS%20first%20uses%20a%20masked%20prediction%20stage%20using%20an%0Aencoder-decoder%20architecture.%20It%20then%20employs%20nearest-neighbor%20contrastive%0Alearning%20to%20partially%20tune%20the%20weights%20of%20the%20encoder%2C%20enhancing%20the%20formation%0Aof%20semantic%20clusters%20for%20different%20actions.%20By%20tuning%20the%20encoder%20for%20a%20few%0Aepochs%2C%20and%20without%20using%20hand-crafted%20data%20augmentations%2C%20STARS%20achieves%0Astate-of-the-art%20self-supervised%20results%20in%20various%20benchmarks%2C%20including%0ANTU-60%2C%20NTU-120%2C%20and%20PKU-MMD.%20In%20addition%2C%20STARS%20exhibits%20significantly%20better%0Aresults%20than%20masked%20prediction%20models%20in%20few-shot%20settings%2C%20where%20the%20model%20has%0Anot%20seen%20the%20actions%20throughout%20pretraining.%20Project%20page%3A%0Ahttps%3A//soroushmehraban.github.io/stars/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10935v1&entry.124074799=Read"},
{"title": "Preventing Catastrophic Forgetting through Memory Networks in Continuous\n  Detection", "author": "Gaurav Bhatt and James Ross and Leonid Sigal", "abstract": "  Modern pre-trained architectures struggle to retain previous information\nwhile undergoing continuous fine-tuning on new tasks. Despite notable progress\nin continual classification, systems designed for complex vision tasks such as\ndetection or segmentation still struggle to attain satisfactory performance. In\nthis work, we introduce a memory-based detection transformer architecture to\nadapt a pre-trained DETR-style detector to new tasks while preserving knowledge\nfrom previous tasks. We propose a novel localized query function for efficient\ninformation retrieval from memory units, aiming to minimize forgetting.\nFurthermore, we identify a fundamental challenge in continual detection\nreferred to as background relegation. This arises when object categories from\nearlier tasks reappear in future tasks, potentially without labels, leading\nthem to be implicitly treated as background. This is an inevitable issue in\ncontinual detection or segmentation. The introduced continual optimization\ntechnique effectively tackles this challenge. Finally, we assess the\nperformance of our proposed system on continual detection benchmarks and\ndemonstrate that our approach surpasses the performance of existing\nstate-of-the-art resulting in 5-7% improvements on MS-COCO and PASCAL-VOC on\nthe task of continual detection.\n", "link": "http://arxiv.org/abs/2403.14797v2", "date": "2024-07-15", "relevancy": 2.7301, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5558}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preventing%20Catastrophic%20Forgetting%20through%20Memory%20Networks%20in%20Continuous%0A%20%20Detection&body=Title%3A%20Preventing%20Catastrophic%20Forgetting%20through%20Memory%20Networks%20in%20Continuous%0A%20%20Detection%0AAuthor%3A%20Gaurav%20Bhatt%20and%20James%20Ross%20and%20Leonid%20Sigal%0AAbstract%3A%20%20%20Modern%20pre-trained%20architectures%20struggle%20to%20retain%20previous%20information%0Awhile%20undergoing%20continuous%20fine-tuning%20on%20new%20tasks.%20Despite%20notable%20progress%0Ain%20continual%20classification%2C%20systems%20designed%20for%20complex%20vision%20tasks%20such%20as%0Adetection%20or%20segmentation%20still%20struggle%20to%20attain%20satisfactory%20performance.%20In%0Athis%20work%2C%20we%20introduce%20a%20memory-based%20detection%20transformer%20architecture%20to%0Aadapt%20a%20pre-trained%20DETR-style%20detector%20to%20new%20tasks%20while%20preserving%20knowledge%0Afrom%20previous%20tasks.%20We%20propose%20a%20novel%20localized%20query%20function%20for%20efficient%0Ainformation%20retrieval%20from%20memory%20units%2C%20aiming%20to%20minimize%20forgetting.%0AFurthermore%2C%20we%20identify%20a%20fundamental%20challenge%20in%20continual%20detection%0Areferred%20to%20as%20background%20relegation.%20This%20arises%20when%20object%20categories%20from%0Aearlier%20tasks%20reappear%20in%20future%20tasks%2C%20potentially%20without%20labels%2C%20leading%0Athem%20to%20be%20implicitly%20treated%20as%20background.%20This%20is%20an%20inevitable%20issue%20in%0Acontinual%20detection%20or%20segmentation.%20The%20introduced%20continual%20optimization%0Atechnique%20effectively%20tackles%20this%20challenge.%20Finally%2C%20we%20assess%20the%0Aperformance%20of%20our%20proposed%20system%20on%20continual%20detection%20benchmarks%20and%0Ademonstrate%20that%20our%20approach%20surpasses%20the%20performance%20of%20existing%0Astate-of-the-art%20resulting%20in%205-7%25%20improvements%20on%20MS-COCO%20and%20PASCAL-VOC%20on%0Athe%20task%20of%20continual%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreventing%2520Catastrophic%2520Forgetting%2520through%2520Memory%2520Networks%2520in%2520Continuous%250A%2520%2520Detection%26entry.906535625%3DGaurav%2520Bhatt%2520and%2520James%2520Ross%2520and%2520Leonid%2520Sigal%26entry.1292438233%3D%2520%2520Modern%2520pre-trained%2520architectures%2520struggle%2520to%2520retain%2520previous%2520information%250Awhile%2520undergoing%2520continuous%2520fine-tuning%2520on%2520new%2520tasks.%2520Despite%2520notable%2520progress%250Ain%2520continual%2520classification%252C%2520systems%2520designed%2520for%2520complex%2520vision%2520tasks%2520such%2520as%250Adetection%2520or%2520segmentation%2520still%2520struggle%2520to%2520attain%2520satisfactory%2520performance.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520a%2520memory-based%2520detection%2520transformer%2520architecture%2520to%250Aadapt%2520a%2520pre-trained%2520DETR-style%2520detector%2520to%2520new%2520tasks%2520while%2520preserving%2520knowledge%250Afrom%2520previous%2520tasks.%2520We%2520propose%2520a%2520novel%2520localized%2520query%2520function%2520for%2520efficient%250Ainformation%2520retrieval%2520from%2520memory%2520units%252C%2520aiming%2520to%2520minimize%2520forgetting.%250AFurthermore%252C%2520we%2520identify%2520a%2520fundamental%2520challenge%2520in%2520continual%2520detection%250Areferred%2520to%2520as%2520background%2520relegation.%2520This%2520arises%2520when%2520object%2520categories%2520from%250Aearlier%2520tasks%2520reappear%2520in%2520future%2520tasks%252C%2520potentially%2520without%2520labels%252C%2520leading%250Athem%2520to%2520be%2520implicitly%2520treated%2520as%2520background.%2520This%2520is%2520an%2520inevitable%2520issue%2520in%250Acontinual%2520detection%2520or%2520segmentation.%2520The%2520introduced%2520continual%2520optimization%250Atechnique%2520effectively%2520tackles%2520this%2520challenge.%2520Finally%252C%2520we%2520assess%2520the%250Aperformance%2520of%2520our%2520proposed%2520system%2520on%2520continual%2520detection%2520benchmarks%2520and%250Ademonstrate%2520that%2520our%2520approach%2520surpasses%2520the%2520performance%2520of%2520existing%250Astate-of-the-art%2520resulting%2520in%25205-7%2525%2520improvements%2520on%2520MS-COCO%2520and%2520PASCAL-VOC%2520on%250Athe%2520task%2520of%2520continual%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preventing%20Catastrophic%20Forgetting%20through%20Memory%20Networks%20in%20Continuous%0A%20%20Detection&entry.906535625=Gaurav%20Bhatt%20and%20James%20Ross%20and%20Leonid%20Sigal&entry.1292438233=%20%20Modern%20pre-trained%20architectures%20struggle%20to%20retain%20previous%20information%0Awhile%20undergoing%20continuous%20fine-tuning%20on%20new%20tasks.%20Despite%20notable%20progress%0Ain%20continual%20classification%2C%20systems%20designed%20for%20complex%20vision%20tasks%20such%20as%0Adetection%20or%20segmentation%20still%20struggle%20to%20attain%20satisfactory%20performance.%20In%0Athis%20work%2C%20we%20introduce%20a%20memory-based%20detection%20transformer%20architecture%20to%0Aadapt%20a%20pre-trained%20DETR-style%20detector%20to%20new%20tasks%20while%20preserving%20knowledge%0Afrom%20previous%20tasks.%20We%20propose%20a%20novel%20localized%20query%20function%20for%20efficient%0Ainformation%20retrieval%20from%20memory%20units%2C%20aiming%20to%20minimize%20forgetting.%0AFurthermore%2C%20we%20identify%20a%20fundamental%20challenge%20in%20continual%20detection%0Areferred%20to%20as%20background%20relegation.%20This%20arises%20when%20object%20categories%20from%0Aearlier%20tasks%20reappear%20in%20future%20tasks%2C%20potentially%20without%20labels%2C%20leading%0Athem%20to%20be%20implicitly%20treated%20as%20background.%20This%20is%20an%20inevitable%20issue%20in%0Acontinual%20detection%20or%20segmentation.%20The%20introduced%20continual%20optimization%0Atechnique%20effectively%20tackles%20this%20challenge.%20Finally%2C%20we%20assess%20the%0Aperformance%20of%20our%20proposed%20system%20on%20continual%20detection%20benchmarks%20and%0Ademonstrate%20that%20our%20approach%20surpasses%20the%20performance%20of%20existing%0Astate-of-the-art%20resulting%20in%205-7%25%20improvements%20on%20MS-COCO%20and%20PASCAL-VOC%20on%0Athe%20task%20of%20continual%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14797v2&entry.124074799=Read"},
{"title": "VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent\n  Space", "author": "Gu\u00e9nol\u00e9 Fiche and Simon Leglaive and Xavier Alameda-Pineda and Antonio Agudo and Francesc Moreno-Noguer", "abstract": "  Previous works on Human Pose and Shape Estimation (HPSE) from RGB images can\nbe broadly categorized into two main groups: parametric and non-parametric\napproaches. Parametric techniques leverage a low-dimensional statistical body\nmodel for realistic results, whereas recent non-parametric methods achieve\nhigher precision by directly regressing the 3D coordinates of the human body\nmesh. This work introduces a novel paradigm to address the HPSE problem,\ninvolving a low-dimensional discrete latent representation of the human mesh\nand framing HPSE as a classification task. Instead of predicting body model\nparameters or 3D vertex coordinates, we focus on predicting the proposed\ndiscrete latent representation, which can be decoded into a registered human\nmesh. This innovative paradigm offers two key advantages. Firstly, predicting a\nlow-dimensional discrete representation confines our predictions to the space\nof anthropomorphic poses and shapes even when little training data is\navailable. Secondly, by framing the problem as a classification task, we can\nharness the discriminative power inherent in neural networks. The proposed\nmodel, VQ-HPS, predicts the discrete latent representation of the mesh. The\nexperimental results demonstrate that VQ-HPS outperforms the current\nstate-of-the-art non-parametric approaches while yielding results as realistic\nas those produced by parametric methods when trained with little data. VQ-HPS\nalso shows promising results when training on large-scale datasets,\nhighlighting the significant potential of the classification approach for HPSE.\nSee the project page at https://g-fiche.github.io/research-pages/vqhps/\n", "link": "http://arxiv.org/abs/2312.08291v4", "date": "2024-07-15", "relevancy": 2.7051, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5646}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5394}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQ-HPS%3A%20Human%20Pose%20and%20Shape%20Estimation%20in%20a%20Vector-Quantized%20Latent%0A%20%20Space&body=Title%3A%20VQ-HPS%3A%20Human%20Pose%20and%20Shape%20Estimation%20in%20a%20Vector-Quantized%20Latent%0A%20%20Space%0AAuthor%3A%20Gu%C3%A9nol%C3%A9%20Fiche%20and%20Simon%20Leglaive%20and%20Xavier%20Alameda-Pineda%20and%20Antonio%20Agudo%20and%20Francesc%20Moreno-Noguer%0AAbstract%3A%20%20%20Previous%20works%20on%20Human%20Pose%20and%20Shape%20Estimation%20%28HPSE%29%20from%20RGB%20images%20can%0Abe%20broadly%20categorized%20into%20two%20main%20groups%3A%20parametric%20and%20non-parametric%0Aapproaches.%20Parametric%20techniques%20leverage%20a%20low-dimensional%20statistical%20body%0Amodel%20for%20realistic%20results%2C%20whereas%20recent%20non-parametric%20methods%20achieve%0Ahigher%20precision%20by%20directly%20regressing%20the%203D%20coordinates%20of%20the%20human%20body%0Amesh.%20This%20work%20introduces%20a%20novel%20paradigm%20to%20address%20the%20HPSE%20problem%2C%0Ainvolving%20a%20low-dimensional%20discrete%20latent%20representation%20of%20the%20human%20mesh%0Aand%20framing%20HPSE%20as%20a%20classification%20task.%20Instead%20of%20predicting%20body%20model%0Aparameters%20or%203D%20vertex%20coordinates%2C%20we%20focus%20on%20predicting%20the%20proposed%0Adiscrete%20latent%20representation%2C%20which%20can%20be%20decoded%20into%20a%20registered%20human%0Amesh.%20This%20innovative%20paradigm%20offers%20two%20key%20advantages.%20Firstly%2C%20predicting%20a%0Alow-dimensional%20discrete%20representation%20confines%20our%20predictions%20to%20the%20space%0Aof%20anthropomorphic%20poses%20and%20shapes%20even%20when%20little%20training%20data%20is%0Aavailable.%20Secondly%2C%20by%20framing%20the%20problem%20as%20a%20classification%20task%2C%20we%20can%0Aharness%20the%20discriminative%20power%20inherent%20in%20neural%20networks.%20The%20proposed%0Amodel%2C%20VQ-HPS%2C%20predicts%20the%20discrete%20latent%20representation%20of%20the%20mesh.%20The%0Aexperimental%20results%20demonstrate%20that%20VQ-HPS%20outperforms%20the%20current%0Astate-of-the-art%20non-parametric%20approaches%20while%20yielding%20results%20as%20realistic%0Aas%20those%20produced%20by%20parametric%20methods%20when%20trained%20with%20little%20data.%20VQ-HPS%0Aalso%20shows%20promising%20results%20when%20training%20on%20large-scale%20datasets%2C%0Ahighlighting%20the%20significant%20potential%20of%20the%20classification%20approach%20for%20HPSE.%0ASee%20the%20project%20page%20at%20https%3A//g-fiche.github.io/research-pages/vqhps/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08291v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQ-HPS%253A%2520Human%2520Pose%2520and%2520Shape%2520Estimation%2520in%2520a%2520Vector-Quantized%2520Latent%250A%2520%2520Space%26entry.906535625%3DGu%25C3%25A9nol%25C3%25A9%2520Fiche%2520and%2520Simon%2520Leglaive%2520and%2520Xavier%2520Alameda-Pineda%2520and%2520Antonio%2520Agudo%2520and%2520Francesc%2520Moreno-Noguer%26entry.1292438233%3D%2520%2520Previous%2520works%2520on%2520Human%2520Pose%2520and%2520Shape%2520Estimation%2520%2528HPSE%2529%2520from%2520RGB%2520images%2520can%250Abe%2520broadly%2520categorized%2520into%2520two%2520main%2520groups%253A%2520parametric%2520and%2520non-parametric%250Aapproaches.%2520Parametric%2520techniques%2520leverage%2520a%2520low-dimensional%2520statistical%2520body%250Amodel%2520for%2520realistic%2520results%252C%2520whereas%2520recent%2520non-parametric%2520methods%2520achieve%250Ahigher%2520precision%2520by%2520directly%2520regressing%2520the%25203D%2520coordinates%2520of%2520the%2520human%2520body%250Amesh.%2520This%2520work%2520introduces%2520a%2520novel%2520paradigm%2520to%2520address%2520the%2520HPSE%2520problem%252C%250Ainvolving%2520a%2520low-dimensional%2520discrete%2520latent%2520representation%2520of%2520the%2520human%2520mesh%250Aand%2520framing%2520HPSE%2520as%2520a%2520classification%2520task.%2520Instead%2520of%2520predicting%2520body%2520model%250Aparameters%2520or%25203D%2520vertex%2520coordinates%252C%2520we%2520focus%2520on%2520predicting%2520the%2520proposed%250Adiscrete%2520latent%2520representation%252C%2520which%2520can%2520be%2520decoded%2520into%2520a%2520registered%2520human%250Amesh.%2520This%2520innovative%2520paradigm%2520offers%2520two%2520key%2520advantages.%2520Firstly%252C%2520predicting%2520a%250Alow-dimensional%2520discrete%2520representation%2520confines%2520our%2520predictions%2520to%2520the%2520space%250Aof%2520anthropomorphic%2520poses%2520and%2520shapes%2520even%2520when%2520little%2520training%2520data%2520is%250Aavailable.%2520Secondly%252C%2520by%2520framing%2520the%2520problem%2520as%2520a%2520classification%2520task%252C%2520we%2520can%250Aharness%2520the%2520discriminative%2520power%2520inherent%2520in%2520neural%2520networks.%2520The%2520proposed%250Amodel%252C%2520VQ-HPS%252C%2520predicts%2520the%2520discrete%2520latent%2520representation%2520of%2520the%2520mesh.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520VQ-HPS%2520outperforms%2520the%2520current%250Astate-of-the-art%2520non-parametric%2520approaches%2520while%2520yielding%2520results%2520as%2520realistic%250Aas%2520those%2520produced%2520by%2520parametric%2520methods%2520when%2520trained%2520with%2520little%2520data.%2520VQ-HPS%250Aalso%2520shows%2520promising%2520results%2520when%2520training%2520on%2520large-scale%2520datasets%252C%250Ahighlighting%2520the%2520significant%2520potential%2520of%2520the%2520classification%2520approach%2520for%2520HPSE.%250ASee%2520the%2520project%2520page%2520at%2520https%253A//g-fiche.github.io/research-pages/vqhps/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08291v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQ-HPS%3A%20Human%20Pose%20and%20Shape%20Estimation%20in%20a%20Vector-Quantized%20Latent%0A%20%20Space&entry.906535625=Gu%C3%A9nol%C3%A9%20Fiche%20and%20Simon%20Leglaive%20and%20Xavier%20Alameda-Pineda%20and%20Antonio%20Agudo%20and%20Francesc%20Moreno-Noguer&entry.1292438233=%20%20Previous%20works%20on%20Human%20Pose%20and%20Shape%20Estimation%20%28HPSE%29%20from%20RGB%20images%20can%0Abe%20broadly%20categorized%20into%20two%20main%20groups%3A%20parametric%20and%20non-parametric%0Aapproaches.%20Parametric%20techniques%20leverage%20a%20low-dimensional%20statistical%20body%0Amodel%20for%20realistic%20results%2C%20whereas%20recent%20non-parametric%20methods%20achieve%0Ahigher%20precision%20by%20directly%20regressing%20the%203D%20coordinates%20of%20the%20human%20body%0Amesh.%20This%20work%20introduces%20a%20novel%20paradigm%20to%20address%20the%20HPSE%20problem%2C%0Ainvolving%20a%20low-dimensional%20discrete%20latent%20representation%20of%20the%20human%20mesh%0Aand%20framing%20HPSE%20as%20a%20classification%20task.%20Instead%20of%20predicting%20body%20model%0Aparameters%20or%203D%20vertex%20coordinates%2C%20we%20focus%20on%20predicting%20the%20proposed%0Adiscrete%20latent%20representation%2C%20which%20can%20be%20decoded%20into%20a%20registered%20human%0Amesh.%20This%20innovative%20paradigm%20offers%20two%20key%20advantages.%20Firstly%2C%20predicting%20a%0Alow-dimensional%20discrete%20representation%20confines%20our%20predictions%20to%20the%20space%0Aof%20anthropomorphic%20poses%20and%20shapes%20even%20when%20little%20training%20data%20is%0Aavailable.%20Secondly%2C%20by%20framing%20the%20problem%20as%20a%20classification%20task%2C%20we%20can%0Aharness%20the%20discriminative%20power%20inherent%20in%20neural%20networks.%20The%20proposed%0Amodel%2C%20VQ-HPS%2C%20predicts%20the%20discrete%20latent%20representation%20of%20the%20mesh.%20The%0Aexperimental%20results%20demonstrate%20that%20VQ-HPS%20outperforms%20the%20current%0Astate-of-the-art%20non-parametric%20approaches%20while%20yielding%20results%20as%20realistic%0Aas%20those%20produced%20by%20parametric%20methods%20when%20trained%20with%20little%20data.%20VQ-HPS%0Aalso%20shows%20promising%20results%20when%20training%20on%20large-scale%20datasets%2C%0Ahighlighting%20the%20significant%20potential%20of%20the%20classification%20approach%20for%20HPSE.%0ASee%20the%20project%20page%20at%20https%3A//g-fiche.github.io/research-pages/vqhps/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08291v4&entry.124074799=Read"},
{"title": "Domain Generalization for 6D Pose Estimation Through NeRF-based Image\n  Synthesis", "author": "Antoine Legrand and Renaud Detry and Christophe De Vleeschouwer", "abstract": "  This work introduces a novel augmentation method that increases the diversity\nof a train set to improve the generalization abilities of a 6D pose estimation\nnetwork. For this purpose, a Neural Radiance Field is trained from synthetic\nimages and exploited to generate an augmented set. Our method enriches the\ninitial set by enabling the synthesis of images with (i) unseen viewpoints,\n(ii) rich illumination conditions through appearance extrapolation, and (iii)\nrandomized textures. We validate our augmentation method on the challenging\nuse-case of spacecraft pose estimation and show that it significantly improves\nthe pose estimation generalization capabilities. On the SPEED+ dataset, our\nmethod reduces the error on the pose by 50% on both target domains.\n", "link": "http://arxiv.org/abs/2407.10762v1", "date": "2024-07-15", "relevancy": 2.6973, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5485}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5357}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalization%20for%206D%20Pose%20Estimation%20Through%20NeRF-based%20Image%0A%20%20Synthesis&body=Title%3A%20Domain%20Generalization%20for%206D%20Pose%20Estimation%20Through%20NeRF-based%20Image%0A%20%20Synthesis%0AAuthor%3A%20Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20novel%20augmentation%20method%20that%20increases%20the%20diversity%0Aof%20a%20train%20set%20to%20improve%20the%20generalization%20abilities%20of%20a%206D%20pose%20estimation%0Anetwork.%20For%20this%20purpose%2C%20a%20Neural%20Radiance%20Field%20is%20trained%20from%20synthetic%0Aimages%20and%20exploited%20to%20generate%20an%20augmented%20set.%20Our%20method%20enriches%20the%0Ainitial%20set%20by%20enabling%20the%20synthesis%20of%20images%20with%20%28i%29%20unseen%20viewpoints%2C%0A%28ii%29%20rich%20illumination%20conditions%20through%20appearance%20extrapolation%2C%20and%20%28iii%29%0Arandomized%20textures.%20We%20validate%20our%20augmentation%20method%20on%20the%20challenging%0Ause-case%20of%20spacecraft%20pose%20estimation%20and%20show%20that%20it%20significantly%20improves%0Athe%20pose%20estimation%20generalization%20capabilities.%20On%20the%20SPEED%2B%20dataset%2C%20our%0Amethod%20reduces%20the%20error%20on%20the%20pose%20by%2050%25%20on%20both%20target%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalization%2520for%25206D%2520Pose%2520Estimation%2520Through%2520NeRF-based%2520Image%250A%2520%2520Synthesis%26entry.906535625%3DAntoine%2520Legrand%2520and%2520Renaud%2520Detry%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520novel%2520augmentation%2520method%2520that%2520increases%2520the%2520diversity%250Aof%2520a%2520train%2520set%2520to%2520improve%2520the%2520generalization%2520abilities%2520of%2520a%25206D%2520pose%2520estimation%250Anetwork.%2520For%2520this%2520purpose%252C%2520a%2520Neural%2520Radiance%2520Field%2520is%2520trained%2520from%2520synthetic%250Aimages%2520and%2520exploited%2520to%2520generate%2520an%2520augmented%2520set.%2520Our%2520method%2520enriches%2520the%250Ainitial%2520set%2520by%2520enabling%2520the%2520synthesis%2520of%2520images%2520with%2520%2528i%2529%2520unseen%2520viewpoints%252C%250A%2528ii%2529%2520rich%2520illumination%2520conditions%2520through%2520appearance%2520extrapolation%252C%2520and%2520%2528iii%2529%250Arandomized%2520textures.%2520We%2520validate%2520our%2520augmentation%2520method%2520on%2520the%2520challenging%250Ause-case%2520of%2520spacecraft%2520pose%2520estimation%2520and%2520show%2520that%2520it%2520significantly%2520improves%250Athe%2520pose%2520estimation%2520generalization%2520capabilities.%2520On%2520the%2520SPEED%252B%2520dataset%252C%2520our%250Amethod%2520reduces%2520the%2520error%2520on%2520the%2520pose%2520by%252050%2525%2520on%2520both%2520target%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalization%20for%206D%20Pose%20Estimation%20Through%20NeRF-based%20Image%0A%20%20Synthesis&entry.906535625=Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20This%20work%20introduces%20a%20novel%20augmentation%20method%20that%20increases%20the%20diversity%0Aof%20a%20train%20set%20to%20improve%20the%20generalization%20abilities%20of%20a%206D%20pose%20estimation%0Anetwork.%20For%20this%20purpose%2C%20a%20Neural%20Radiance%20Field%20is%20trained%20from%20synthetic%0Aimages%20and%20exploited%20to%20generate%20an%20augmented%20set.%20Our%20method%20enriches%20the%0Ainitial%20set%20by%20enabling%20the%20synthesis%20of%20images%20with%20%28i%29%20unseen%20viewpoints%2C%0A%28ii%29%20rich%20illumination%20conditions%20through%20appearance%20extrapolation%2C%20and%20%28iii%29%0Arandomized%20textures.%20We%20validate%20our%20augmentation%20method%20on%20the%20challenging%0Ause-case%20of%20spacecraft%20pose%20estimation%20and%20show%20that%20it%20significantly%20improves%0Athe%20pose%20estimation%20generalization%20capabilities.%20On%20the%20SPEED%2B%20dataset%2C%20our%0Amethod%20reduces%20the%20error%20on%20the%20pose%20by%2050%25%20on%20both%20target%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10762v1&entry.124074799=Read"},
{"title": "Detecting Omissions in Geographic Maps through Computer Vision", "author": "Phuc D. A. Nguyen and Anh Do and Minh Hoai", "abstract": "  This paper explores the application of computer vision technologies to the\nanalysis of maps, an area with substantial historical, cultural, and political\nsignificance. Our focus is on developing and evaluating a method for\nautomatically identifying maps that depict specific regions and feature\nlandmarks with designated names, a task that involves complex challenges due to\nthe diverse styles and methods used in map creation. We address three main\nsubtasks: differentiating maps from non-maps, verifying the accuracy of the\nregion depicted, and confirming the presence or absence of particular landmark\nnames through advanced text recognition techniques. Our approach utilizes a\nConvolutional Neural Network and transfer learning to differentiate maps from\nnon-maps, verify the accuracy of depicted regions, and confirm landmark names\nthrough advanced text recognition. We also introduce the VinMap dataset,\ncontaining annotated map images of Vietnam, to train and test our method.\nExperiments on this dataset demonstrate that our technique achieves F1-score of\n85.51% for identifying maps excluding specific territorial landmarks. This\nresult suggests practical utility and indicates areas for future improvement.\n", "link": "http://arxiv.org/abs/2407.10709v1", "date": "2024-07-15", "relevancy": 2.6892, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5789}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.519}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Omissions%20in%20Geographic%20Maps%20through%20Computer%20Vision&body=Title%3A%20Detecting%20Omissions%20in%20Geographic%20Maps%20through%20Computer%20Vision%0AAuthor%3A%20Phuc%20D.%20A.%20Nguyen%20and%20Anh%20Do%20and%20Minh%20Hoai%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20application%20of%20computer%20vision%20technologies%20to%20the%0Aanalysis%20of%20maps%2C%20an%20area%20with%20substantial%20historical%2C%20cultural%2C%20and%20political%0Asignificance.%20Our%20focus%20is%20on%20developing%20and%20evaluating%20a%20method%20for%0Aautomatically%20identifying%20maps%20that%20depict%20specific%20regions%20and%20feature%0Alandmarks%20with%20designated%20names%2C%20a%20task%20that%20involves%20complex%20challenges%20due%20to%0Athe%20diverse%20styles%20and%20methods%20used%20in%20map%20creation.%20We%20address%20three%20main%0Asubtasks%3A%20differentiating%20maps%20from%20non-maps%2C%20verifying%20the%20accuracy%20of%20the%0Aregion%20depicted%2C%20and%20confirming%20the%20presence%20or%20absence%20of%20particular%20landmark%0Anames%20through%20advanced%20text%20recognition%20techniques.%20Our%20approach%20utilizes%20a%0AConvolutional%20Neural%20Network%20and%20transfer%20learning%20to%20differentiate%20maps%20from%0Anon-maps%2C%20verify%20the%20accuracy%20of%20depicted%20regions%2C%20and%20confirm%20landmark%20names%0Athrough%20advanced%20text%20recognition.%20We%20also%20introduce%20the%20VinMap%20dataset%2C%0Acontaining%20annotated%20map%20images%20of%20Vietnam%2C%20to%20train%20and%20test%20our%20method.%0AExperiments%20on%20this%20dataset%20demonstrate%20that%20our%20technique%20achieves%20F1-score%20of%0A85.51%25%20for%20identifying%20maps%20excluding%20specific%20territorial%20landmarks.%20This%0Aresult%20suggests%20practical%20utility%20and%20indicates%20areas%20for%20future%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Omissions%2520in%2520Geographic%2520Maps%2520through%2520Computer%2520Vision%26entry.906535625%3DPhuc%2520D.%2520A.%2520Nguyen%2520and%2520Anh%2520Do%2520and%2520Minh%2520Hoai%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520application%2520of%2520computer%2520vision%2520technologies%2520to%2520the%250Aanalysis%2520of%2520maps%252C%2520an%2520area%2520with%2520substantial%2520historical%252C%2520cultural%252C%2520and%2520political%250Asignificance.%2520Our%2520focus%2520is%2520on%2520developing%2520and%2520evaluating%2520a%2520method%2520for%250Aautomatically%2520identifying%2520maps%2520that%2520depict%2520specific%2520regions%2520and%2520feature%250Alandmarks%2520with%2520designated%2520names%252C%2520a%2520task%2520that%2520involves%2520complex%2520challenges%2520due%2520to%250Athe%2520diverse%2520styles%2520and%2520methods%2520used%2520in%2520map%2520creation.%2520We%2520address%2520three%2520main%250Asubtasks%253A%2520differentiating%2520maps%2520from%2520non-maps%252C%2520verifying%2520the%2520accuracy%2520of%2520the%250Aregion%2520depicted%252C%2520and%2520confirming%2520the%2520presence%2520or%2520absence%2520of%2520particular%2520landmark%250Anames%2520through%2520advanced%2520text%2520recognition%2520techniques.%2520Our%2520approach%2520utilizes%2520a%250AConvolutional%2520Neural%2520Network%2520and%2520transfer%2520learning%2520to%2520differentiate%2520maps%2520from%250Anon-maps%252C%2520verify%2520the%2520accuracy%2520of%2520depicted%2520regions%252C%2520and%2520confirm%2520landmark%2520names%250Athrough%2520advanced%2520text%2520recognition.%2520We%2520also%2520introduce%2520the%2520VinMap%2520dataset%252C%250Acontaining%2520annotated%2520map%2520images%2520of%2520Vietnam%252C%2520to%2520train%2520and%2520test%2520our%2520method.%250AExperiments%2520on%2520this%2520dataset%2520demonstrate%2520that%2520our%2520technique%2520achieves%2520F1-score%2520of%250A85.51%2525%2520for%2520identifying%2520maps%2520excluding%2520specific%2520territorial%2520landmarks.%2520This%250Aresult%2520suggests%2520practical%2520utility%2520and%2520indicates%2520areas%2520for%2520future%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Omissions%20in%20Geographic%20Maps%20through%20Computer%20Vision&entry.906535625=Phuc%20D.%20A.%20Nguyen%20and%20Anh%20Do%20and%20Minh%20Hoai&entry.1292438233=%20%20This%20paper%20explores%20the%20application%20of%20computer%20vision%20technologies%20to%20the%0Aanalysis%20of%20maps%2C%20an%20area%20with%20substantial%20historical%2C%20cultural%2C%20and%20political%0Asignificance.%20Our%20focus%20is%20on%20developing%20and%20evaluating%20a%20method%20for%0Aautomatically%20identifying%20maps%20that%20depict%20specific%20regions%20and%20feature%0Alandmarks%20with%20designated%20names%2C%20a%20task%20that%20involves%20complex%20challenges%20due%20to%0Athe%20diverse%20styles%20and%20methods%20used%20in%20map%20creation.%20We%20address%20three%20main%0Asubtasks%3A%20differentiating%20maps%20from%20non-maps%2C%20verifying%20the%20accuracy%20of%20the%0Aregion%20depicted%2C%20and%20confirming%20the%20presence%20or%20absence%20of%20particular%20landmark%0Anames%20through%20advanced%20text%20recognition%20techniques.%20Our%20approach%20utilizes%20a%0AConvolutional%20Neural%20Network%20and%20transfer%20learning%20to%20differentiate%20maps%20from%0Anon-maps%2C%20verify%20the%20accuracy%20of%20depicted%20regions%2C%20and%20confirm%20landmark%20names%0Athrough%20advanced%20text%20recognition.%20We%20also%20introduce%20the%20VinMap%20dataset%2C%0Acontaining%20annotated%20map%20images%20of%20Vietnam%2C%20to%20train%20and%20test%20our%20method.%0AExperiments%20on%20this%20dataset%20demonstrate%20that%20our%20technique%20achieves%20F1-score%20of%0A85.51%25%20for%20identifying%20maps%20excluding%20specific%20territorial%20landmarks.%20This%0Aresult%20suggests%20practical%20utility%20and%20indicates%20areas%20for%20future%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10709v1&entry.124074799=Read"},
{"title": "DINO Pre-training for Vision-based End-to-end Autonomous Driving", "author": "Shubham Juneja and Povilas Daniu\u0161is and Virginijus Marcinkevi\u010dius", "abstract": "  In this article, we focus on the pre-training of visual autonomous driving\nagents in the context of imitation learning. Current methods often rely on a\nclassification-based pre-training, which we hypothesise to be holding back from\nextending capabilities of implicit image understanding. We propose pre-training\nthe visual encoder of a driving agent using the self-distillation with no\nlabels (DINO) method, which relies on a self-supervised learning paradigm.% and\nis trained on an unrelated task. Our experiments in CARLA environment in\naccordance with the Leaderboard benchmark reveal that the proposed pre-training\nis more efficient than classification-based pre-training, and is on par with\nthe recently proposed pre-training based on visual place recognition (VPRPre).\n", "link": "http://arxiv.org/abs/2407.10803v1", "date": "2024-07-15", "relevancy": 2.6843, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5519}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5332}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO%20Pre-training%20for%20Vision-based%20End-to-end%20Autonomous%20Driving&body=Title%3A%20DINO%20Pre-training%20for%20Vision-based%20End-to-end%20Autonomous%20Driving%0AAuthor%3A%20Shubham%20Juneja%20and%20Povilas%20Daniu%C5%A1is%20and%20Virginijus%20Marcinkevi%C4%8Dius%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20focus%20on%20the%20pre-training%20of%20visual%20autonomous%20driving%0Aagents%20in%20the%20context%20of%20imitation%20learning.%20Current%20methods%20often%20rely%20on%20a%0Aclassification-based%20pre-training%2C%20which%20we%20hypothesise%20to%20be%20holding%20back%20from%0Aextending%20capabilities%20of%20implicit%20image%20understanding.%20We%20propose%20pre-training%0Athe%20visual%20encoder%20of%20a%20driving%20agent%20using%20the%20self-distillation%20with%20no%0Alabels%20%28DINO%29%20method%2C%20which%20relies%20on%20a%20self-supervised%20learning%20paradigm.%25%20and%0Ais%20trained%20on%20an%20unrelated%20task.%20Our%20experiments%20in%20CARLA%20environment%20in%0Aaccordance%20with%20the%20Leaderboard%20benchmark%20reveal%20that%20the%20proposed%20pre-training%0Ais%20more%20efficient%20than%20classification-based%20pre-training%2C%20and%20is%20on%20par%20with%0Athe%20recently%20proposed%20pre-training%20based%20on%20visual%20place%20recognition%20%28VPRPre%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO%2520Pre-training%2520for%2520Vision-based%2520End-to-end%2520Autonomous%2520Driving%26entry.906535625%3DShubham%2520Juneja%2520and%2520Povilas%2520Daniu%25C5%25A1is%2520and%2520Virginijus%2520Marcinkevi%25C4%258Dius%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520focus%2520on%2520the%2520pre-training%2520of%2520visual%2520autonomous%2520driving%250Aagents%2520in%2520the%2520context%2520of%2520imitation%2520learning.%2520Current%2520methods%2520often%2520rely%2520on%2520a%250Aclassification-based%2520pre-training%252C%2520which%2520we%2520hypothesise%2520to%2520be%2520holding%2520back%2520from%250Aextending%2520capabilities%2520of%2520implicit%2520image%2520understanding.%2520We%2520propose%2520pre-training%250Athe%2520visual%2520encoder%2520of%2520a%2520driving%2520agent%2520using%2520the%2520self-distillation%2520with%2520no%250Alabels%2520%2528DINO%2529%2520method%252C%2520which%2520relies%2520on%2520a%2520self-supervised%2520learning%2520paradigm.%2525%2520and%250Ais%2520trained%2520on%2520an%2520unrelated%2520task.%2520Our%2520experiments%2520in%2520CARLA%2520environment%2520in%250Aaccordance%2520with%2520the%2520Leaderboard%2520benchmark%2520reveal%2520that%2520the%2520proposed%2520pre-training%250Ais%2520more%2520efficient%2520than%2520classification-based%2520pre-training%252C%2520and%2520is%2520on%2520par%2520with%250Athe%2520recently%2520proposed%2520pre-training%2520based%2520on%2520visual%2520place%2520recognition%2520%2528VPRPre%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO%20Pre-training%20for%20Vision-based%20End-to-end%20Autonomous%20Driving&entry.906535625=Shubham%20Juneja%20and%20Povilas%20Daniu%C5%A1is%20and%20Virginijus%20Marcinkevi%C4%8Dius&entry.1292438233=%20%20In%20this%20article%2C%20we%20focus%20on%20the%20pre-training%20of%20visual%20autonomous%20driving%0Aagents%20in%20the%20context%20of%20imitation%20learning.%20Current%20methods%20often%20rely%20on%20a%0Aclassification-based%20pre-training%2C%20which%20we%20hypothesise%20to%20be%20holding%20back%20from%0Aextending%20capabilities%20of%20implicit%20image%20understanding.%20We%20propose%20pre-training%0Athe%20visual%20encoder%20of%20a%20driving%20agent%20using%20the%20self-distillation%20with%20no%0Alabels%20%28DINO%29%20method%2C%20which%20relies%20on%20a%20self-supervised%20learning%20paradigm.%25%20and%0Ais%20trained%20on%20an%20unrelated%20task.%20Our%20experiments%20in%20CARLA%20environment%20in%0Aaccordance%20with%20the%20Leaderboard%20benchmark%20reveal%20that%20the%20proposed%20pre-training%0Ais%20more%20efficient%20than%20classification-based%20pre-training%2C%20and%20is%20on%20par%20with%0Athe%20recently%20proposed%20pre-training%20based%20on%20visual%20place%20recognition%20%28VPRPre%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10803v1&entry.124074799=Read"},
{"title": "GTPT: Group-based Token Pruning Transformer for Efficient Human Pose\n  Estimation", "author": "Haonan Wang and Jie Liu and Jie Tang and Gangshan Wu and Bo Xu and Yanbing Chou and Yong Wang", "abstract": "  In recent years, 2D human pose estimation has made significant progress on\npublic benchmarks. However, many of these approaches face challenges of less\napplicability in the industrial community due to the large number of parametric\nquantities and computational overhead. Efficient human pose estimation remains\na hurdle, especially for whole-body pose estimation with numerous keypoints.\nWhile most current methods for efficient human pose estimation primarily rely\non CNNs, we propose the Group-based Token Pruning Transformer (GTPT) that fully\nharnesses the advantages of the Transformer. GTPT alleviates the computational\nburden by gradually introducing keypoints in a coarse-to-fine manner. It\nminimizes the computation overhead while ensuring high performance. Besides,\nGTPT groups keypoint tokens and prunes visual tokens to improve model\nperformance while reducing redundancy. We propose the Multi-Head Group\nAttention (MHGA) between different groups to achieve global interaction with\nlittle computational overhead. We conducted experiments on COCO and\nCOCO-WholeBody. Compared to other methods, the experimental results show that\nGTPT can achieve higher performance with less computation, especially in\nwhole-body with numerous keypoints.\n", "link": "http://arxiv.org/abs/2407.10756v1", "date": "2024-07-15", "relevancy": 2.6804, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.56}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5301}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTPT%3A%20Group-based%20Token%20Pruning%20Transformer%20for%20Efficient%20Human%20Pose%0A%20%20Estimation&body=Title%3A%20GTPT%3A%20Group-based%20Token%20Pruning%20Transformer%20for%20Efficient%20Human%20Pose%0A%20%20Estimation%0AAuthor%3A%20Haonan%20Wang%20and%20Jie%20Liu%20and%20Jie%20Tang%20and%20Gangshan%20Wu%20and%20Bo%20Xu%20and%20Yanbing%20Chou%20and%20Yong%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%202D%20human%20pose%20estimation%20has%20made%20significant%20progress%20on%0Apublic%20benchmarks.%20However%2C%20many%20of%20these%20approaches%20face%20challenges%20of%20less%0Aapplicability%20in%20the%20industrial%20community%20due%20to%20the%20large%20number%20of%20parametric%0Aquantities%20and%20computational%20overhead.%20Efficient%20human%20pose%20estimation%20remains%0Aa%20hurdle%2C%20especially%20for%20whole-body%20pose%20estimation%20with%20numerous%20keypoints.%0AWhile%20most%20current%20methods%20for%20efficient%20human%20pose%20estimation%20primarily%20rely%0Aon%20CNNs%2C%20we%20propose%20the%20Group-based%20Token%20Pruning%20Transformer%20%28GTPT%29%20that%20fully%0Aharnesses%20the%20advantages%20of%20the%20Transformer.%20GTPT%20alleviates%20the%20computational%0Aburden%20by%20gradually%20introducing%20keypoints%20in%20a%20coarse-to-fine%20manner.%20It%0Aminimizes%20the%20computation%20overhead%20while%20ensuring%20high%20performance.%20Besides%2C%0AGTPT%20groups%20keypoint%20tokens%20and%20prunes%20visual%20tokens%20to%20improve%20model%0Aperformance%20while%20reducing%20redundancy.%20We%20propose%20the%20Multi-Head%20Group%0AAttention%20%28MHGA%29%20between%20different%20groups%20to%20achieve%20global%20interaction%20with%0Alittle%20computational%20overhead.%20We%20conducted%20experiments%20on%20COCO%20and%0ACOCO-WholeBody.%20Compared%20to%20other%20methods%2C%20the%20experimental%20results%20show%20that%0AGTPT%20can%20achieve%20higher%20performance%20with%20less%20computation%2C%20especially%20in%0Awhole-body%20with%20numerous%20keypoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTPT%253A%2520Group-based%2520Token%2520Pruning%2520Transformer%2520for%2520Efficient%2520Human%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DHaonan%2520Wang%2520and%2520Jie%2520Liu%2520and%2520Jie%2520Tang%2520and%2520Gangshan%2520Wu%2520and%2520Bo%2520Xu%2520and%2520Yanbing%2520Chou%2520and%2520Yong%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%25202D%2520human%2520pose%2520estimation%2520has%2520made%2520significant%2520progress%2520on%250Apublic%2520benchmarks.%2520However%252C%2520many%2520of%2520these%2520approaches%2520face%2520challenges%2520of%2520less%250Aapplicability%2520in%2520the%2520industrial%2520community%2520due%2520to%2520the%2520large%2520number%2520of%2520parametric%250Aquantities%2520and%2520computational%2520overhead.%2520Efficient%2520human%2520pose%2520estimation%2520remains%250Aa%2520hurdle%252C%2520especially%2520for%2520whole-body%2520pose%2520estimation%2520with%2520numerous%2520keypoints.%250AWhile%2520most%2520current%2520methods%2520for%2520efficient%2520human%2520pose%2520estimation%2520primarily%2520rely%250Aon%2520CNNs%252C%2520we%2520propose%2520the%2520Group-based%2520Token%2520Pruning%2520Transformer%2520%2528GTPT%2529%2520that%2520fully%250Aharnesses%2520the%2520advantages%2520of%2520the%2520Transformer.%2520GTPT%2520alleviates%2520the%2520computational%250Aburden%2520by%2520gradually%2520introducing%2520keypoints%2520in%2520a%2520coarse-to-fine%2520manner.%2520It%250Aminimizes%2520the%2520computation%2520overhead%2520while%2520ensuring%2520high%2520performance.%2520Besides%252C%250AGTPT%2520groups%2520keypoint%2520tokens%2520and%2520prunes%2520visual%2520tokens%2520to%2520improve%2520model%250Aperformance%2520while%2520reducing%2520redundancy.%2520We%2520propose%2520the%2520Multi-Head%2520Group%250AAttention%2520%2528MHGA%2529%2520between%2520different%2520groups%2520to%2520achieve%2520global%2520interaction%2520with%250Alittle%2520computational%2520overhead.%2520We%2520conducted%2520experiments%2520on%2520COCO%2520and%250ACOCO-WholeBody.%2520Compared%2520to%2520other%2520methods%252C%2520the%2520experimental%2520results%2520show%2520that%250AGTPT%2520can%2520achieve%2520higher%2520performance%2520with%2520less%2520computation%252C%2520especially%2520in%250Awhole-body%2520with%2520numerous%2520keypoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTPT%3A%20Group-based%20Token%20Pruning%20Transformer%20for%20Efficient%20Human%20Pose%0A%20%20Estimation&entry.906535625=Haonan%20Wang%20and%20Jie%20Liu%20and%20Jie%20Tang%20and%20Gangshan%20Wu%20and%20Bo%20Xu%20and%20Yanbing%20Chou%20and%20Yong%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%202D%20human%20pose%20estimation%20has%20made%20significant%20progress%20on%0Apublic%20benchmarks.%20However%2C%20many%20of%20these%20approaches%20face%20challenges%20of%20less%0Aapplicability%20in%20the%20industrial%20community%20due%20to%20the%20large%20number%20of%20parametric%0Aquantities%20and%20computational%20overhead.%20Efficient%20human%20pose%20estimation%20remains%0Aa%20hurdle%2C%20especially%20for%20whole-body%20pose%20estimation%20with%20numerous%20keypoints.%0AWhile%20most%20current%20methods%20for%20efficient%20human%20pose%20estimation%20primarily%20rely%0Aon%20CNNs%2C%20we%20propose%20the%20Group-based%20Token%20Pruning%20Transformer%20%28GTPT%29%20that%20fully%0Aharnesses%20the%20advantages%20of%20the%20Transformer.%20GTPT%20alleviates%20the%20computational%0Aburden%20by%20gradually%20introducing%20keypoints%20in%20a%20coarse-to-fine%20manner.%20It%0Aminimizes%20the%20computation%20overhead%20while%20ensuring%20high%20performance.%20Besides%2C%0AGTPT%20groups%20keypoint%20tokens%20and%20prunes%20visual%20tokens%20to%20improve%20model%0Aperformance%20while%20reducing%20redundancy.%20We%20propose%20the%20Multi-Head%20Group%0AAttention%20%28MHGA%29%20between%20different%20groups%20to%20achieve%20global%20interaction%20with%0Alittle%20computational%20overhead.%20We%20conducted%20experiments%20on%20COCO%20and%0ACOCO-WholeBody.%20Compared%20to%20other%20methods%2C%20the%20experimental%20results%20show%20that%0AGTPT%20can%20achieve%20higher%20performance%20with%20less%20computation%2C%20especially%20in%0Awhole-body%20with%20numerous%20keypoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10756v1&entry.124074799=Read"},
{"title": "Leveraging Multimodal CycleGAN for the Generation of Anatomically\n  Accurate Synthetic CT Scans from MRIs", "author": "Leonardo Crespi and Samuele Camnasio and Damiano Dei and Nicola Lambri and Pietro Mancosu and Marta Scorsetti and Daniele Loiacono", "abstract": "  In many clinical settings, the use of both Computed Tomography (CT) and\nMagnetic Resonance (MRI) is necessary to pursue a thorough understanding of the\npatient's anatomy and to plan a suitable therapeutical strategy; this is often\nthe case in MRI-based radiotherapy, where CT is always necessary to prepare the\ndose delivery, as it provides the essential information about the radiation\nabsorption properties of the tissues. Sometimes, MRI is preferred to contour\nthe target volumes. However, this approach is often not the most efficient, as\nit is more expensive, time-consuming and, most importantly, stressful for the\npatients. To overcome this issue, in this work, we analyse the capabilities of\ndifferent configurations of Deep Learning models to generate synthetic CT scans\nfrom MRI, leveraging the power of Generative Adversarial Networks (GANs) and,\nin particular, the CycleGAN architecture, capable of working in an unsupervised\nmanner and without paired images, which were not available. Several CycleGAN\nmodels were trained unsupervised to generate CT scans from different MRI\nmodalities with and without contrast agents. To overcome the problem of not\nhaving a ground truth, distribution-based metrics were used to assess the\nmodel's performance quantitatively, together with a qualitative evaluation\nwhere physicians were asked to differentiate between real and synthetic images\nto understand how realistic the generated images were. The results show how,\ndepending on the input modalities, the models can have very different\nperformances; however, models with the best quantitative results, according to\nthe distribution-based metrics used, can generate very difficult images to\ndistinguish from the real ones, even for physicians, demonstrating the\napproach's potential.\n", "link": "http://arxiv.org/abs/2407.10888v1", "date": "2024-07-15", "relevancy": 2.6732, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Multimodal%20CycleGAN%20for%20the%20Generation%20of%20Anatomically%0A%20%20Accurate%20Synthetic%20CT%20Scans%20from%20MRIs&body=Title%3A%20Leveraging%20Multimodal%20CycleGAN%20for%20the%20Generation%20of%20Anatomically%0A%20%20Accurate%20Synthetic%20CT%20Scans%20from%20MRIs%0AAuthor%3A%20Leonardo%20Crespi%20and%20Samuele%20Camnasio%20and%20Damiano%20Dei%20and%20Nicola%20Lambri%20and%20Pietro%20Mancosu%20and%20Marta%20Scorsetti%20and%20Daniele%20Loiacono%0AAbstract%3A%20%20%20In%20many%20clinical%20settings%2C%20the%20use%20of%20both%20Computed%20Tomography%20%28CT%29%20and%0AMagnetic%20Resonance%20%28MRI%29%20is%20necessary%20to%20pursue%20a%20thorough%20understanding%20of%20the%0Apatient%27s%20anatomy%20and%20to%20plan%20a%20suitable%20therapeutical%20strategy%3B%20this%20is%20often%0Athe%20case%20in%20MRI-based%20radiotherapy%2C%20where%20CT%20is%20always%20necessary%20to%20prepare%20the%0Adose%20delivery%2C%20as%20it%20provides%20the%20essential%20information%20about%20the%20radiation%0Aabsorption%20properties%20of%20the%20tissues.%20Sometimes%2C%20MRI%20is%20preferred%20to%20contour%0Athe%20target%20volumes.%20However%2C%20this%20approach%20is%20often%20not%20the%20most%20efficient%2C%20as%0Ait%20is%20more%20expensive%2C%20time-consuming%20and%2C%20most%20importantly%2C%20stressful%20for%20the%0Apatients.%20To%20overcome%20this%20issue%2C%20in%20this%20work%2C%20we%20analyse%20the%20capabilities%20of%0Adifferent%20configurations%20of%20Deep%20Learning%20models%20to%20generate%20synthetic%20CT%20scans%0Afrom%20MRI%2C%20leveraging%20the%20power%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%2C%0Ain%20particular%2C%20the%20CycleGAN%20architecture%2C%20capable%20of%20working%20in%20an%20unsupervised%0Amanner%20and%20without%20paired%20images%2C%20which%20were%20not%20available.%20Several%20CycleGAN%0Amodels%20were%20trained%20unsupervised%20to%20generate%20CT%20scans%20from%20different%20MRI%0Amodalities%20with%20and%20without%20contrast%20agents.%20To%20overcome%20the%20problem%20of%20not%0Ahaving%20a%20ground%20truth%2C%20distribution-based%20metrics%20were%20used%20to%20assess%20the%0Amodel%27s%20performance%20quantitatively%2C%20together%20with%20a%20qualitative%20evaluation%0Awhere%20physicians%20were%20asked%20to%20differentiate%20between%20real%20and%20synthetic%20images%0Ato%20understand%20how%20realistic%20the%20generated%20images%20were.%20The%20results%20show%20how%2C%0Adepending%20on%20the%20input%20modalities%2C%20the%20models%20can%20have%20very%20different%0Aperformances%3B%20however%2C%20models%20with%20the%20best%20quantitative%20results%2C%20according%20to%0Athe%20distribution-based%20metrics%20used%2C%20can%20generate%20very%20difficult%20images%20to%0Adistinguish%20from%20the%20real%20ones%2C%20even%20for%20physicians%2C%20demonstrating%20the%0Aapproach%27s%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Multimodal%2520CycleGAN%2520for%2520the%2520Generation%2520of%2520Anatomically%250A%2520%2520Accurate%2520Synthetic%2520CT%2520Scans%2520from%2520MRIs%26entry.906535625%3DLeonardo%2520Crespi%2520and%2520Samuele%2520Camnasio%2520and%2520Damiano%2520Dei%2520and%2520Nicola%2520Lambri%2520and%2520Pietro%2520Mancosu%2520and%2520Marta%2520Scorsetti%2520and%2520Daniele%2520Loiacono%26entry.1292438233%3D%2520%2520In%2520many%2520clinical%2520settings%252C%2520the%2520use%2520of%2520both%2520Computed%2520Tomography%2520%2528CT%2529%2520and%250AMagnetic%2520Resonance%2520%2528MRI%2529%2520is%2520necessary%2520to%2520pursue%2520a%2520thorough%2520understanding%2520of%2520the%250Apatient%2527s%2520anatomy%2520and%2520to%2520plan%2520a%2520suitable%2520therapeutical%2520strategy%253B%2520this%2520is%2520often%250Athe%2520case%2520in%2520MRI-based%2520radiotherapy%252C%2520where%2520CT%2520is%2520always%2520necessary%2520to%2520prepare%2520the%250Adose%2520delivery%252C%2520as%2520it%2520provides%2520the%2520essential%2520information%2520about%2520the%2520radiation%250Aabsorption%2520properties%2520of%2520the%2520tissues.%2520Sometimes%252C%2520MRI%2520is%2520preferred%2520to%2520contour%250Athe%2520target%2520volumes.%2520However%252C%2520this%2520approach%2520is%2520often%2520not%2520the%2520most%2520efficient%252C%2520as%250Ait%2520is%2520more%2520expensive%252C%2520time-consuming%2520and%252C%2520most%2520importantly%252C%2520stressful%2520for%2520the%250Apatients.%2520To%2520overcome%2520this%2520issue%252C%2520in%2520this%2520work%252C%2520we%2520analyse%2520the%2520capabilities%2520of%250Adifferent%2520configurations%2520of%2520Deep%2520Learning%2520models%2520to%2520generate%2520synthetic%2520CT%2520scans%250Afrom%2520MRI%252C%2520leveraging%2520the%2520power%2520of%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520and%252C%250Ain%2520particular%252C%2520the%2520CycleGAN%2520architecture%252C%2520capable%2520of%2520working%2520in%2520an%2520unsupervised%250Amanner%2520and%2520without%2520paired%2520images%252C%2520which%2520were%2520not%2520available.%2520Several%2520CycleGAN%250Amodels%2520were%2520trained%2520unsupervised%2520to%2520generate%2520CT%2520scans%2520from%2520different%2520MRI%250Amodalities%2520with%2520and%2520without%2520contrast%2520agents.%2520To%2520overcome%2520the%2520problem%2520of%2520not%250Ahaving%2520a%2520ground%2520truth%252C%2520distribution-based%2520metrics%2520were%2520used%2520to%2520assess%2520the%250Amodel%2527s%2520performance%2520quantitatively%252C%2520together%2520with%2520a%2520qualitative%2520evaluation%250Awhere%2520physicians%2520were%2520asked%2520to%2520differentiate%2520between%2520real%2520and%2520synthetic%2520images%250Ato%2520understand%2520how%2520realistic%2520the%2520generated%2520images%2520were.%2520The%2520results%2520show%2520how%252C%250Adepending%2520on%2520the%2520input%2520modalities%252C%2520the%2520models%2520can%2520have%2520very%2520different%250Aperformances%253B%2520however%252C%2520models%2520with%2520the%2520best%2520quantitative%2520results%252C%2520according%2520to%250Athe%2520distribution-based%2520metrics%2520used%252C%2520can%2520generate%2520very%2520difficult%2520images%2520to%250Adistinguish%2520from%2520the%2520real%2520ones%252C%2520even%2520for%2520physicians%252C%2520demonstrating%2520the%250Aapproach%2527s%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Multimodal%20CycleGAN%20for%20the%20Generation%20of%20Anatomically%0A%20%20Accurate%20Synthetic%20CT%20Scans%20from%20MRIs&entry.906535625=Leonardo%20Crespi%20and%20Samuele%20Camnasio%20and%20Damiano%20Dei%20and%20Nicola%20Lambri%20and%20Pietro%20Mancosu%20and%20Marta%20Scorsetti%20and%20Daniele%20Loiacono&entry.1292438233=%20%20In%20many%20clinical%20settings%2C%20the%20use%20of%20both%20Computed%20Tomography%20%28CT%29%20and%0AMagnetic%20Resonance%20%28MRI%29%20is%20necessary%20to%20pursue%20a%20thorough%20understanding%20of%20the%0Apatient%27s%20anatomy%20and%20to%20plan%20a%20suitable%20therapeutical%20strategy%3B%20this%20is%20often%0Athe%20case%20in%20MRI-based%20radiotherapy%2C%20where%20CT%20is%20always%20necessary%20to%20prepare%20the%0Adose%20delivery%2C%20as%20it%20provides%20the%20essential%20information%20about%20the%20radiation%0Aabsorption%20properties%20of%20the%20tissues.%20Sometimes%2C%20MRI%20is%20preferred%20to%20contour%0Athe%20target%20volumes.%20However%2C%20this%20approach%20is%20often%20not%20the%20most%20efficient%2C%20as%0Ait%20is%20more%20expensive%2C%20time-consuming%20and%2C%20most%20importantly%2C%20stressful%20for%20the%0Apatients.%20To%20overcome%20this%20issue%2C%20in%20this%20work%2C%20we%20analyse%20the%20capabilities%20of%0Adifferent%20configurations%20of%20Deep%20Learning%20models%20to%20generate%20synthetic%20CT%20scans%0Afrom%20MRI%2C%20leveraging%20the%20power%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%2C%0Ain%20particular%2C%20the%20CycleGAN%20architecture%2C%20capable%20of%20working%20in%20an%20unsupervised%0Amanner%20and%20without%20paired%20images%2C%20which%20were%20not%20available.%20Several%20CycleGAN%0Amodels%20were%20trained%20unsupervised%20to%20generate%20CT%20scans%20from%20different%20MRI%0Amodalities%20with%20and%20without%20contrast%20agents.%20To%20overcome%20the%20problem%20of%20not%0Ahaving%20a%20ground%20truth%2C%20distribution-based%20metrics%20were%20used%20to%20assess%20the%0Amodel%27s%20performance%20quantitatively%2C%20together%20with%20a%20qualitative%20evaluation%0Awhere%20physicians%20were%20asked%20to%20differentiate%20between%20real%20and%20synthetic%20images%0Ato%20understand%20how%20realistic%20the%20generated%20images%20were.%20The%20results%20show%20how%2C%0Adepending%20on%20the%20input%20modalities%2C%20the%20models%20can%20have%20very%20different%0Aperformances%3B%20however%2C%20models%20with%20the%20best%20quantitative%20results%2C%20according%20to%0Athe%20distribution-based%20metrics%20used%2C%20can%20generate%20very%20difficult%20images%20to%0Adistinguish%20from%20the%20real%20ones%2C%20even%20for%20physicians%2C%20demonstrating%20the%0Aapproach%27s%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10888v1&entry.124074799=Read"},
{"title": "Deep Learning on Object-centric 3D Neural Fields", "author": "Pierluigi Zama Ramirez and Luca De Luigi and Daniele Sirocchi and Adriano Cardace and Riccardo Spezialetti and Francesco Ballerini and Samuele Salti and Luigi Di Stefano", "abstract": "  In recent years, Neural Fields (NFs) have emerged as an effective tool for\nencoding diverse continuous signals such as images, videos, audio, and 3D\nshapes. When applied to 3D data, NFs offer a solution to the fragmentation and\nlimitations associated with prevalent discrete representations. However, given\nthat NFs are essentially neural networks, it remains unclear whether and how\nthey can be seamlessly integrated into deep learning pipelines for solving\ndownstream tasks. This paper addresses this research problem and introduces\nnf2vec, a framework capable of generating a compact latent representation for\nan input NF in a single inference pass. We demonstrate that nf2vec effectively\nembeds 3D objects represented by the input NFs and showcase how the resulting\nembeddings can be employed in deep learning pipelines to successfully address\nvarious tasks, all while processing exclusively NFs. We test this framework on\nseveral NFs used to represent 3D surfaces, such as unsigned/signed distance and\noccupancy fields. Moreover, we demonstrate the effectiveness of our approach\nwith more complex NFs that encompass both geometry and appearance of 3D objects\nsuch as neural radiance fields.\n", "link": "http://arxiv.org/abs/2312.13277v2", "date": "2024-07-15", "relevancy": 2.6714, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5428}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5374}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20on%20Object-centric%203D%20Neural%20Fields&body=Title%3A%20Deep%20Learning%20on%20Object-centric%203D%20Neural%20Fields%0AAuthor%3A%20Pierluigi%20Zama%20Ramirez%20and%20Luca%20De%20Luigi%20and%20Daniele%20Sirocchi%20and%20Adriano%20Cardace%20and%20Riccardo%20Spezialetti%20and%20Francesco%20Ballerini%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Neural%20Fields%20%28NFs%29%20have%20emerged%20as%20an%20effective%20tool%20for%0Aencoding%20diverse%20continuous%20signals%20such%20as%20images%2C%20videos%2C%20audio%2C%20and%203D%0Ashapes.%20When%20applied%20to%203D%20data%2C%20NFs%20offer%20a%20solution%20to%20the%20fragmentation%20and%0Alimitations%20associated%20with%20prevalent%20discrete%20representations.%20However%2C%20given%0Athat%20NFs%20are%20essentially%20neural%20networks%2C%20it%20remains%20unclear%20whether%20and%20how%0Athey%20can%20be%20seamlessly%20integrated%20into%20deep%20learning%20pipelines%20for%20solving%0Adownstream%20tasks.%20This%20paper%20addresses%20this%20research%20problem%20and%20introduces%0Anf2vec%2C%20a%20framework%20capable%20of%20generating%20a%20compact%20latent%20representation%20for%0Aan%20input%20NF%20in%20a%20single%20inference%20pass.%20We%20demonstrate%20that%20nf2vec%20effectively%0Aembeds%203D%20objects%20represented%20by%20the%20input%20NFs%20and%20showcase%20how%20the%20resulting%0Aembeddings%20can%20be%20employed%20in%20deep%20learning%20pipelines%20to%20successfully%20address%0Avarious%20tasks%2C%20all%20while%20processing%20exclusively%20NFs.%20We%20test%20this%20framework%20on%0Aseveral%20NFs%20used%20to%20represent%203D%20surfaces%2C%20such%20as%20unsigned/signed%20distance%20and%0Aoccupancy%20fields.%20Moreover%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Awith%20more%20complex%20NFs%20that%20encompass%20both%20geometry%20and%20appearance%20of%203D%20objects%0Asuch%20as%20neural%20radiance%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520on%2520Object-centric%25203D%2520Neural%2520Fields%26entry.906535625%3DPierluigi%2520Zama%2520Ramirez%2520and%2520Luca%2520De%2520Luigi%2520and%2520Daniele%2520Sirocchi%2520and%2520Adriano%2520Cardace%2520and%2520Riccardo%2520Spezialetti%2520and%2520Francesco%2520Ballerini%2520and%2520Samuele%2520Salti%2520and%2520Luigi%2520Di%2520Stefano%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Neural%2520Fields%2520%2528NFs%2529%2520have%2520emerged%2520as%2520an%2520effective%2520tool%2520for%250Aencoding%2520diverse%2520continuous%2520signals%2520such%2520as%2520images%252C%2520videos%252C%2520audio%252C%2520and%25203D%250Ashapes.%2520When%2520applied%2520to%25203D%2520data%252C%2520NFs%2520offer%2520a%2520solution%2520to%2520the%2520fragmentation%2520and%250Alimitations%2520associated%2520with%2520prevalent%2520discrete%2520representations.%2520However%252C%2520given%250Athat%2520NFs%2520are%2520essentially%2520neural%2520networks%252C%2520it%2520remains%2520unclear%2520whether%2520and%2520how%250Athey%2520can%2520be%2520seamlessly%2520integrated%2520into%2520deep%2520learning%2520pipelines%2520for%2520solving%250Adownstream%2520tasks.%2520This%2520paper%2520addresses%2520this%2520research%2520problem%2520and%2520introduces%250Anf2vec%252C%2520a%2520framework%2520capable%2520of%2520generating%2520a%2520compact%2520latent%2520representation%2520for%250Aan%2520input%2520NF%2520in%2520a%2520single%2520inference%2520pass.%2520We%2520demonstrate%2520that%2520nf2vec%2520effectively%250Aembeds%25203D%2520objects%2520represented%2520by%2520the%2520input%2520NFs%2520and%2520showcase%2520how%2520the%2520resulting%250Aembeddings%2520can%2520be%2520employed%2520in%2520deep%2520learning%2520pipelines%2520to%2520successfully%2520address%250Avarious%2520tasks%252C%2520all%2520while%2520processing%2520exclusively%2520NFs.%2520We%2520test%2520this%2520framework%2520on%250Aseveral%2520NFs%2520used%2520to%2520represent%25203D%2520surfaces%252C%2520such%2520as%2520unsigned/signed%2520distance%2520and%250Aoccupancy%2520fields.%2520Moreover%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%250Awith%2520more%2520complex%2520NFs%2520that%2520encompass%2520both%2520geometry%2520and%2520appearance%2520of%25203D%2520objects%250Asuch%2520as%2520neural%2520radiance%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20on%20Object-centric%203D%20Neural%20Fields&entry.906535625=Pierluigi%20Zama%20Ramirez%20and%20Luca%20De%20Luigi%20and%20Daniele%20Sirocchi%20and%20Adriano%20Cardace%20and%20Riccardo%20Spezialetti%20and%20Francesco%20Ballerini%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano&entry.1292438233=%20%20In%20recent%20years%2C%20Neural%20Fields%20%28NFs%29%20have%20emerged%20as%20an%20effective%20tool%20for%0Aencoding%20diverse%20continuous%20signals%20such%20as%20images%2C%20videos%2C%20audio%2C%20and%203D%0Ashapes.%20When%20applied%20to%203D%20data%2C%20NFs%20offer%20a%20solution%20to%20the%20fragmentation%20and%0Alimitations%20associated%20with%20prevalent%20discrete%20representations.%20However%2C%20given%0Athat%20NFs%20are%20essentially%20neural%20networks%2C%20it%20remains%20unclear%20whether%20and%20how%0Athey%20can%20be%20seamlessly%20integrated%20into%20deep%20learning%20pipelines%20for%20solving%0Adownstream%20tasks.%20This%20paper%20addresses%20this%20research%20problem%20and%20introduces%0Anf2vec%2C%20a%20framework%20capable%20of%20generating%20a%20compact%20latent%20representation%20for%0Aan%20input%20NF%20in%20a%20single%20inference%20pass.%20We%20demonstrate%20that%20nf2vec%20effectively%0Aembeds%203D%20objects%20represented%20by%20the%20input%20NFs%20and%20showcase%20how%20the%20resulting%0Aembeddings%20can%20be%20employed%20in%20deep%20learning%20pipelines%20to%20successfully%20address%0Avarious%20tasks%2C%20all%20while%20processing%20exclusively%20NFs.%20We%20test%20this%20framework%20on%0Aseveral%20NFs%20used%20to%20represent%203D%20surfaces%2C%20such%20as%20unsigned/signed%20distance%20and%0Aoccupancy%20fields.%20Moreover%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Awith%20more%20complex%20NFs%20that%20encompass%20both%20geometry%20and%20appearance%20of%203D%20objects%0Asuch%20as%20neural%20radiance%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13277v2&entry.124074799=Read"},
{"title": "OPa-Ma: Text Guided Mamba for 360-degree Image Out-painting", "author": "Penglei Gao and Kai Yao and Tiandi Ye and Steven Wang and Yuan Yao and Xiaofeng Wang", "abstract": "  In this paper, we tackle the recently popular topic of generating 360-degree\nimages given the conventional narrow field of view (NFoV) images that could be\ntaken from a single camera or cellphone. This task aims to predict the\nreasonable and consistent surroundings from the NFoV images. Existing methods\nfor feature extraction and fusion, often built with transformer-based\narchitectures, incur substantial memory usage and computational expense. They\nalso have limitations in maintaining visual continuity across the entire\n360-degree images, which could cause inconsistent texture and style generation.\nTo solve the aforementioned issues, we propose a novel text-guided out-painting\nframework equipped with a State-Space Model called Mamba to utilize its\nlong-sequence modelling and spatial continuity. Furthermore, incorporating\ntextual information is an effective strategy for guiding image generation,\nenriching the process with detailed context and increasing diversity.\nEfficiently extracting textual features and integrating them with image\nattributes presents a significant challenge for 360-degree image out-painting.\nTo address this, we develop two modules, Visual-textual Consistency Refiner\n(VCR) and Global-local Mamba Adapter (GMA). VCR enhances contextual richness by\nfusing the modified text features with the image features, while GMA provides\nadaptive state-selective conditions by capturing the information flow from\nglobal to local representations. Our proposed method achieves state-of-the-art\nperformance with extensive experiments on two broadly used 360-degree image\ndatasets, including indoor and outdoor settings.\n", "link": "http://arxiv.org/abs/2407.10923v1", "date": "2024-07-15", "relevancy": 2.6683, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5368}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5324}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPa-Ma%3A%20Text%20Guided%20Mamba%20for%20360-degree%20Image%20Out-painting&body=Title%3A%20OPa-Ma%3A%20Text%20Guided%20Mamba%20for%20360-degree%20Image%20Out-painting%0AAuthor%3A%20Penglei%20Gao%20and%20Kai%20Yao%20and%20Tiandi%20Ye%20and%20Steven%20Wang%20and%20Yuan%20Yao%20and%20Xiaofeng%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20tackle%20the%20recently%20popular%20topic%20of%20generating%20360-degree%0Aimages%20given%20the%20conventional%20narrow%20field%20of%20view%20%28NFoV%29%20images%20that%20could%20be%0Ataken%20from%20a%20single%20camera%20or%20cellphone.%20This%20task%20aims%20to%20predict%20the%0Areasonable%20and%20consistent%20surroundings%20from%20the%20NFoV%20images.%20Existing%20methods%0Afor%20feature%20extraction%20and%20fusion%2C%20often%20built%20with%20transformer-based%0Aarchitectures%2C%20incur%20substantial%20memory%20usage%20and%20computational%20expense.%20They%0Aalso%20have%20limitations%20in%20maintaining%20visual%20continuity%20across%20the%20entire%0A360-degree%20images%2C%20which%20could%20cause%20inconsistent%20texture%20and%20style%20generation.%0ATo%20solve%20the%20aforementioned%20issues%2C%20we%20propose%20a%20novel%20text-guided%20out-painting%0Aframework%20equipped%20with%20a%20State-Space%20Model%20called%20Mamba%20to%20utilize%20its%0Along-sequence%20modelling%20and%20spatial%20continuity.%20Furthermore%2C%20incorporating%0Atextual%20information%20is%20an%20effective%20strategy%20for%20guiding%20image%20generation%2C%0Aenriching%20the%20process%20with%20detailed%20context%20and%20increasing%20diversity.%0AEfficiently%20extracting%20textual%20features%20and%20integrating%20them%20with%20image%0Aattributes%20presents%20a%20significant%20challenge%20for%20360-degree%20image%20out-painting.%0ATo%20address%20this%2C%20we%20develop%20two%20modules%2C%20Visual-textual%20Consistency%20Refiner%0A%28VCR%29%20and%20Global-local%20Mamba%20Adapter%20%28GMA%29.%20VCR%20enhances%20contextual%20richness%20by%0Afusing%20the%20modified%20text%20features%20with%20the%20image%20features%2C%20while%20GMA%20provides%0Aadaptive%20state-selective%20conditions%20by%20capturing%20the%20information%20flow%20from%0Aglobal%20to%20local%20representations.%20Our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20with%20extensive%20experiments%20on%20two%20broadly%20used%20360-degree%20image%0Adatasets%2C%20including%20indoor%20and%20outdoor%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPa-Ma%253A%2520Text%2520Guided%2520Mamba%2520for%2520360-degree%2520Image%2520Out-painting%26entry.906535625%3DPenglei%2520Gao%2520and%2520Kai%2520Yao%2520and%2520Tiandi%2520Ye%2520and%2520Steven%2520Wang%2520and%2520Yuan%2520Yao%2520and%2520Xiaofeng%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520recently%2520popular%2520topic%2520of%2520generating%2520360-degree%250Aimages%2520given%2520the%2520conventional%2520narrow%2520field%2520of%2520view%2520%2528NFoV%2529%2520images%2520that%2520could%2520be%250Ataken%2520from%2520a%2520single%2520camera%2520or%2520cellphone.%2520This%2520task%2520aims%2520to%2520predict%2520the%250Areasonable%2520and%2520consistent%2520surroundings%2520from%2520the%2520NFoV%2520images.%2520Existing%2520methods%250Afor%2520feature%2520extraction%2520and%2520fusion%252C%2520often%2520built%2520with%2520transformer-based%250Aarchitectures%252C%2520incur%2520substantial%2520memory%2520usage%2520and%2520computational%2520expense.%2520They%250Aalso%2520have%2520limitations%2520in%2520maintaining%2520visual%2520continuity%2520across%2520the%2520entire%250A360-degree%2520images%252C%2520which%2520could%2520cause%2520inconsistent%2520texture%2520and%2520style%2520generation.%250ATo%2520solve%2520the%2520aforementioned%2520issues%252C%2520we%2520propose%2520a%2520novel%2520text-guided%2520out-painting%250Aframework%2520equipped%2520with%2520a%2520State-Space%2520Model%2520called%2520Mamba%2520to%2520utilize%2520its%250Along-sequence%2520modelling%2520and%2520spatial%2520continuity.%2520Furthermore%252C%2520incorporating%250Atextual%2520information%2520is%2520an%2520effective%2520strategy%2520for%2520guiding%2520image%2520generation%252C%250Aenriching%2520the%2520process%2520with%2520detailed%2520context%2520and%2520increasing%2520diversity.%250AEfficiently%2520extracting%2520textual%2520features%2520and%2520integrating%2520them%2520with%2520image%250Aattributes%2520presents%2520a%2520significant%2520challenge%2520for%2520360-degree%2520image%2520out-painting.%250ATo%2520address%2520this%252C%2520we%2520develop%2520two%2520modules%252C%2520Visual-textual%2520Consistency%2520Refiner%250A%2528VCR%2529%2520and%2520Global-local%2520Mamba%2520Adapter%2520%2528GMA%2529.%2520VCR%2520enhances%2520contextual%2520richness%2520by%250Afusing%2520the%2520modified%2520text%2520features%2520with%2520the%2520image%2520features%252C%2520while%2520GMA%2520provides%250Aadaptive%2520state-selective%2520conditions%2520by%2520capturing%2520the%2520information%2520flow%2520from%250Aglobal%2520to%2520local%2520representations.%2520Our%2520proposed%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520with%2520extensive%2520experiments%2520on%2520two%2520broadly%2520used%2520360-degree%2520image%250Adatasets%252C%2520including%2520indoor%2520and%2520outdoor%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPa-Ma%3A%20Text%20Guided%20Mamba%20for%20360-degree%20Image%20Out-painting&entry.906535625=Penglei%20Gao%20and%20Kai%20Yao%20and%20Tiandi%20Ye%20and%20Steven%20Wang%20and%20Yuan%20Yao%20and%20Xiaofeng%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20tackle%20the%20recently%20popular%20topic%20of%20generating%20360-degree%0Aimages%20given%20the%20conventional%20narrow%20field%20of%20view%20%28NFoV%29%20images%20that%20could%20be%0Ataken%20from%20a%20single%20camera%20or%20cellphone.%20This%20task%20aims%20to%20predict%20the%0Areasonable%20and%20consistent%20surroundings%20from%20the%20NFoV%20images.%20Existing%20methods%0Afor%20feature%20extraction%20and%20fusion%2C%20often%20built%20with%20transformer-based%0Aarchitectures%2C%20incur%20substantial%20memory%20usage%20and%20computational%20expense.%20They%0Aalso%20have%20limitations%20in%20maintaining%20visual%20continuity%20across%20the%20entire%0A360-degree%20images%2C%20which%20could%20cause%20inconsistent%20texture%20and%20style%20generation.%0ATo%20solve%20the%20aforementioned%20issues%2C%20we%20propose%20a%20novel%20text-guided%20out-painting%0Aframework%20equipped%20with%20a%20State-Space%20Model%20called%20Mamba%20to%20utilize%20its%0Along-sequence%20modelling%20and%20spatial%20continuity.%20Furthermore%2C%20incorporating%0Atextual%20information%20is%20an%20effective%20strategy%20for%20guiding%20image%20generation%2C%0Aenriching%20the%20process%20with%20detailed%20context%20and%20increasing%20diversity.%0AEfficiently%20extracting%20textual%20features%20and%20integrating%20them%20with%20image%0Aattributes%20presents%20a%20significant%20challenge%20for%20360-degree%20image%20out-painting.%0ATo%20address%20this%2C%20we%20develop%20two%20modules%2C%20Visual-textual%20Consistency%20Refiner%0A%28VCR%29%20and%20Global-local%20Mamba%20Adapter%20%28GMA%29.%20VCR%20enhances%20contextual%20richness%20by%0Afusing%20the%20modified%20text%20features%20with%20the%20image%20features%2C%20while%20GMA%20provides%0Aadaptive%20state-selective%20conditions%20by%20capturing%20the%20information%20flow%20from%0Aglobal%20to%20local%20representations.%20Our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20with%20extensive%20experiments%20on%20two%20broadly%20used%20360-degree%20image%0Adatasets%2C%20including%20indoor%20and%20outdoor%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10923v1&entry.124074799=Read"},
{"title": "Towards Task-Compatible Compressible Representations", "author": "Anderson de Andrade and Ivan Baji\u0107", "abstract": "  We identify an issue in multi-task learnable compression, in which a\nrepresentation learned for one task does not positively contribute to the\nrate-distortion performance of a different task as much as expected, given the\nestimated amount of information available in it. We interpret this issue using\nthe predictive $\\mathcal{V}$-information framework. In learnable scalable\ncoding, previous work increased the utilization of side-information for input\nreconstruction by also rewarding input reconstruction when learning this shared\nrepresentation. We evaluate the impact of this idea in the context of input\nreconstruction more rigorously and extended it to other computer vision tasks.\nWe perform experiments using representations trained for object detection on\nCOCO 2017 and depth estimation on the Cityscapes dataset, and use them to\nassist in image reconstruction and semantic segmentation tasks. The results\nshow considerable improvements in the rate-distortion performance of the\nassisted tasks. Moreover, using the proposed representations, the performance\nof the base tasks are also improved. Results suggest that the proposed method\ninduces simpler representations that are more compatible with downstream\nprocesses.\n", "link": "http://arxiv.org/abs/2405.10244v3", "date": "2024-07-15", "relevancy": 2.6429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5162}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Task-Compatible%20Compressible%20Representations&body=Title%3A%20Towards%20Task-Compatible%20Compressible%20Representations%0AAuthor%3A%20Anderson%20de%20Andrade%20and%20Ivan%20Baji%C4%87%0AAbstract%3A%20%20%20We%20identify%20an%20issue%20in%20multi-task%20learnable%20compression%2C%20in%20which%20a%0Arepresentation%20learned%20for%20one%20task%20does%20not%20positively%20contribute%20to%20the%0Arate-distortion%20performance%20of%20a%20different%20task%20as%20much%20as%20expected%2C%20given%20the%0Aestimated%20amount%20of%20information%20available%20in%20it.%20We%20interpret%20this%20issue%20using%0Athe%20predictive%20%24%5Cmathcal%7BV%7D%24-information%20framework.%20In%20learnable%20scalable%0Acoding%2C%20previous%20work%20increased%20the%20utilization%20of%20side-information%20for%20input%0Areconstruction%20by%20also%20rewarding%20input%20reconstruction%20when%20learning%20this%20shared%0Arepresentation.%20We%20evaluate%20the%20impact%20of%20this%20idea%20in%20the%20context%20of%20input%0Areconstruction%20more%20rigorously%20and%20extended%20it%20to%20other%20computer%20vision%20tasks.%0AWe%20perform%20experiments%20using%20representations%20trained%20for%20object%20detection%20on%0ACOCO%202017%20and%20depth%20estimation%20on%20the%20Cityscapes%20dataset%2C%20and%20use%20them%20to%0Aassist%20in%20image%20reconstruction%20and%20semantic%20segmentation%20tasks.%20The%20results%0Ashow%20considerable%20improvements%20in%20the%20rate-distortion%20performance%20of%20the%0Aassisted%20tasks.%20Moreover%2C%20using%20the%20proposed%20representations%2C%20the%20performance%0Aof%20the%20base%20tasks%20are%20also%20improved.%20Results%20suggest%20that%20the%20proposed%20method%0Ainduces%20simpler%20representations%20that%20are%20more%20compatible%20with%20downstream%0Aprocesses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10244v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Task-Compatible%2520Compressible%2520Representations%26entry.906535625%3DAnderson%2520de%2520Andrade%2520and%2520Ivan%2520Baji%25C4%2587%26entry.1292438233%3D%2520%2520We%2520identify%2520an%2520issue%2520in%2520multi-task%2520learnable%2520compression%252C%2520in%2520which%2520a%250Arepresentation%2520learned%2520for%2520one%2520task%2520does%2520not%2520positively%2520contribute%2520to%2520the%250Arate-distortion%2520performance%2520of%2520a%2520different%2520task%2520as%2520much%2520as%2520expected%252C%2520given%2520the%250Aestimated%2520amount%2520of%2520information%2520available%2520in%2520it.%2520We%2520interpret%2520this%2520issue%2520using%250Athe%2520predictive%2520%2524%255Cmathcal%257BV%257D%2524-information%2520framework.%2520In%2520learnable%2520scalable%250Acoding%252C%2520previous%2520work%2520increased%2520the%2520utilization%2520of%2520side-information%2520for%2520input%250Areconstruction%2520by%2520also%2520rewarding%2520input%2520reconstruction%2520when%2520learning%2520this%2520shared%250Arepresentation.%2520We%2520evaluate%2520the%2520impact%2520of%2520this%2520idea%2520in%2520the%2520context%2520of%2520input%250Areconstruction%2520more%2520rigorously%2520and%2520extended%2520it%2520to%2520other%2520computer%2520vision%2520tasks.%250AWe%2520perform%2520experiments%2520using%2520representations%2520trained%2520for%2520object%2520detection%2520on%250ACOCO%25202017%2520and%2520depth%2520estimation%2520on%2520the%2520Cityscapes%2520dataset%252C%2520and%2520use%2520them%2520to%250Aassist%2520in%2520image%2520reconstruction%2520and%2520semantic%2520segmentation%2520tasks.%2520The%2520results%250Ashow%2520considerable%2520improvements%2520in%2520the%2520rate-distortion%2520performance%2520of%2520the%250Aassisted%2520tasks.%2520Moreover%252C%2520using%2520the%2520proposed%2520representations%252C%2520the%2520performance%250Aof%2520the%2520base%2520tasks%2520are%2520also%2520improved.%2520Results%2520suggest%2520that%2520the%2520proposed%2520method%250Ainduces%2520simpler%2520representations%2520that%2520are%2520more%2520compatible%2520with%2520downstream%250Aprocesses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10244v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Task-Compatible%20Compressible%20Representations&entry.906535625=Anderson%20de%20Andrade%20and%20Ivan%20Baji%C4%87&entry.1292438233=%20%20We%20identify%20an%20issue%20in%20multi-task%20learnable%20compression%2C%20in%20which%20a%0Arepresentation%20learned%20for%20one%20task%20does%20not%20positively%20contribute%20to%20the%0Arate-distortion%20performance%20of%20a%20different%20task%20as%20much%20as%20expected%2C%20given%20the%0Aestimated%20amount%20of%20information%20available%20in%20it.%20We%20interpret%20this%20issue%20using%0Athe%20predictive%20%24%5Cmathcal%7BV%7D%24-information%20framework.%20In%20learnable%20scalable%0Acoding%2C%20previous%20work%20increased%20the%20utilization%20of%20side-information%20for%20input%0Areconstruction%20by%20also%20rewarding%20input%20reconstruction%20when%20learning%20this%20shared%0Arepresentation.%20We%20evaluate%20the%20impact%20of%20this%20idea%20in%20the%20context%20of%20input%0Areconstruction%20more%20rigorously%20and%20extended%20it%20to%20other%20computer%20vision%20tasks.%0AWe%20perform%20experiments%20using%20representations%20trained%20for%20object%20detection%20on%0ACOCO%202017%20and%20depth%20estimation%20on%20the%20Cityscapes%20dataset%2C%20and%20use%20them%20to%0Aassist%20in%20image%20reconstruction%20and%20semantic%20segmentation%20tasks.%20The%20results%0Ashow%20considerable%20improvements%20in%20the%20rate-distortion%20performance%20of%20the%0Aassisted%20tasks.%20Moreover%2C%20using%20the%20proposed%20representations%2C%20the%20performance%0Aof%20the%20base%20tasks%20are%20also%20improved.%20Results%20suggest%20that%20the%20proposed%20method%0Ainduces%20simpler%20representations%20that%20are%20more%20compatible%20with%20downstream%0Aprocesses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10244v3&entry.124074799=Read"},
{"title": "DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM\n  Planning", "author": "Abhay Zala and Han Lin and Jaemin Cho and Mohit Bansal", "abstract": "  Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows/lines,\netc.). Existing state-of-the-art T2I models often fail at diagram generation\nbecause they lack fine-grained object layout control when many objects are\ndensely connected via complex relations such as arrows/lines, and also often\nfail to render comprehensible text labels. To address this gap, we present\nDiagrammerGPT, a novel two-stage text-to-diagram generation framework\nleveraging the layout guidance capabilities of LLMs to generate more accurate\ndiagrams. In the first stage, we use LLMs to generate and iteratively refine\n'diagram plans' (in a planner-auditor feedback loop). In the second stage, we\nuse a diagram generator, DiagramGLIGEN, and a text label rendering module to\ngenerate diagrams (with clear text labels) following the diagram plans. To\nbenchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nthat our DiagrammerGPT framework produces more accurate diagrams, outperforming\nexisting T2I models. We also provide comprehensive analysis, including\nopen-domain diagram generation, multi-platform vector graphic diagram\ngeneration, human-in-the-loop editing, and multimodal planner/auditor LLMs.\n", "link": "http://arxiv.org/abs/2310.12128v2", "date": "2024-07-15", "relevancy": 2.6294, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5258}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiagrammerGPT%3A%20Generating%20Open-Domain%2C%20Open-Platform%20Diagrams%20via%20LLM%0A%20%20Planning&body=Title%3A%20DiagrammerGPT%3A%20Generating%20Open-Domain%2C%20Open-Platform%20Diagrams%20via%20LLM%0A%20%20Planning%0AAuthor%3A%20Abhay%20Zala%20and%20Han%20Lin%20and%20Jaemin%20Cho%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20has%20seen%20significant%20growth%20over%20the%20past%20few%0Ayears.%20Despite%20this%2C%20there%20has%20been%20little%20work%20on%20generating%20diagrams%20with%20T2I%0Amodels.%20A%20diagram%20is%20a%20symbolic/schematic%20representation%20that%20explains%0Ainformation%20using%20structurally%20rich%20and%20spatially%20complex%20visualizations%20%28e.g.%2C%0Aa%20dense%20combination%20of%20related%20objects%2C%20text%20labels%2C%20directional%20arrows/lines%2C%0Aetc.%29.%20Existing%20state-of-the-art%20T2I%20models%20often%20fail%20at%20diagram%20generation%0Abecause%20they%20lack%20fine-grained%20object%20layout%20control%20when%20many%20objects%20are%0Adensely%20connected%20via%20complex%20relations%20such%20as%20arrows/lines%2C%20and%20also%20often%0Afail%20to%20render%20comprehensible%20text%20labels.%20To%20address%20this%20gap%2C%20we%20present%0ADiagrammerGPT%2C%20a%20novel%20two-stage%20text-to-diagram%20generation%20framework%0Aleveraging%20the%20layout%20guidance%20capabilities%20of%20LLMs%20to%20generate%20more%20accurate%0Adiagrams.%20In%20the%20first%20stage%2C%20we%20use%20LLMs%20to%20generate%20and%20iteratively%20refine%0A%27diagram%20plans%27%20%28in%20a%20planner-auditor%20feedback%20loop%29.%20In%20the%20second%20stage%2C%20we%0Ause%20a%20diagram%20generator%2C%20DiagramGLIGEN%2C%20and%20a%20text%20label%20rendering%20module%20to%0Agenerate%20diagrams%20%28with%20clear%20text%20labels%29%20following%20the%20diagram%20plans.%20To%0Abenchmark%20the%20text-to-diagram%20generation%20task%2C%20we%20introduce%20AI2D-Caption%2C%20a%0Adensely%20annotated%20diagram%20dataset%20built%20on%20top%20of%20the%20AI2D%20dataset.%20We%20show%0Athat%20our%20DiagrammerGPT%20framework%20produces%20more%20accurate%20diagrams%2C%20outperforming%0Aexisting%20T2I%20models.%20We%20also%20provide%20comprehensive%20analysis%2C%20including%0Aopen-domain%20diagram%20generation%2C%20multi-platform%20vector%20graphic%20diagram%0Ageneration%2C%20human-in-the-loop%20editing%2C%20and%20multimodal%20planner/auditor%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagrammerGPT%253A%2520Generating%2520Open-Domain%252C%2520Open-Platform%2520Diagrams%2520via%2520LLM%250A%2520%2520Planning%26entry.906535625%3DAbhay%2520Zala%2520and%2520Han%2520Lin%2520and%2520Jaemin%2520Cho%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520generation%2520has%2520seen%2520significant%2520growth%2520over%2520the%2520past%2520few%250Ayears.%2520Despite%2520this%252C%2520there%2520has%2520been%2520little%2520work%2520on%2520generating%2520diagrams%2520with%2520T2I%250Amodels.%2520A%2520diagram%2520is%2520a%2520symbolic/schematic%2520representation%2520that%2520explains%250Ainformation%2520using%2520structurally%2520rich%2520and%2520spatially%2520complex%2520visualizations%2520%2528e.g.%252C%250Aa%2520dense%2520combination%2520of%2520related%2520objects%252C%2520text%2520labels%252C%2520directional%2520arrows/lines%252C%250Aetc.%2529.%2520Existing%2520state-of-the-art%2520T2I%2520models%2520often%2520fail%2520at%2520diagram%2520generation%250Abecause%2520they%2520lack%2520fine-grained%2520object%2520layout%2520control%2520when%2520many%2520objects%2520are%250Adensely%2520connected%2520via%2520complex%2520relations%2520such%2520as%2520arrows/lines%252C%2520and%2520also%2520often%250Afail%2520to%2520render%2520comprehensible%2520text%2520labels.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%250ADiagrammerGPT%252C%2520a%2520novel%2520two-stage%2520text-to-diagram%2520generation%2520framework%250Aleveraging%2520the%2520layout%2520guidance%2520capabilities%2520of%2520LLMs%2520to%2520generate%2520more%2520accurate%250Adiagrams.%2520In%2520the%2520first%2520stage%252C%2520we%2520use%2520LLMs%2520to%2520generate%2520and%2520iteratively%2520refine%250A%2527diagram%2520plans%2527%2520%2528in%2520a%2520planner-auditor%2520feedback%2520loop%2529.%2520In%2520the%2520second%2520stage%252C%2520we%250Ause%2520a%2520diagram%2520generator%252C%2520DiagramGLIGEN%252C%2520and%2520a%2520text%2520label%2520rendering%2520module%2520to%250Agenerate%2520diagrams%2520%2528with%2520clear%2520text%2520labels%2529%2520following%2520the%2520diagram%2520plans.%2520To%250Abenchmark%2520the%2520text-to-diagram%2520generation%2520task%252C%2520we%2520introduce%2520AI2D-Caption%252C%2520a%250Adensely%2520annotated%2520diagram%2520dataset%2520built%2520on%2520top%2520of%2520the%2520AI2D%2520dataset.%2520We%2520show%250Athat%2520our%2520DiagrammerGPT%2520framework%2520produces%2520more%2520accurate%2520diagrams%252C%2520outperforming%250Aexisting%2520T2I%2520models.%2520We%2520also%2520provide%2520comprehensive%2520analysis%252C%2520including%250Aopen-domain%2520diagram%2520generation%252C%2520multi-platform%2520vector%2520graphic%2520diagram%250Ageneration%252C%2520human-in-the-loop%2520editing%252C%2520and%2520multimodal%2520planner/auditor%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiagrammerGPT%3A%20Generating%20Open-Domain%2C%20Open-Platform%20Diagrams%20via%20LLM%0A%20%20Planning&entry.906535625=Abhay%20Zala%20and%20Han%20Lin%20and%20Jaemin%20Cho%20and%20Mohit%20Bansal&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20has%20seen%20significant%20growth%20over%20the%20past%20few%0Ayears.%20Despite%20this%2C%20there%20has%20been%20little%20work%20on%20generating%20diagrams%20with%20T2I%0Amodels.%20A%20diagram%20is%20a%20symbolic/schematic%20representation%20that%20explains%0Ainformation%20using%20structurally%20rich%20and%20spatially%20complex%20visualizations%20%28e.g.%2C%0Aa%20dense%20combination%20of%20related%20objects%2C%20text%20labels%2C%20directional%20arrows/lines%2C%0Aetc.%29.%20Existing%20state-of-the-art%20T2I%20models%20often%20fail%20at%20diagram%20generation%0Abecause%20they%20lack%20fine-grained%20object%20layout%20control%20when%20many%20objects%20are%0Adensely%20connected%20via%20complex%20relations%20such%20as%20arrows/lines%2C%20and%20also%20often%0Afail%20to%20render%20comprehensible%20text%20labels.%20To%20address%20this%20gap%2C%20we%20present%0ADiagrammerGPT%2C%20a%20novel%20two-stage%20text-to-diagram%20generation%20framework%0Aleveraging%20the%20layout%20guidance%20capabilities%20of%20LLMs%20to%20generate%20more%20accurate%0Adiagrams.%20In%20the%20first%20stage%2C%20we%20use%20LLMs%20to%20generate%20and%20iteratively%20refine%0A%27diagram%20plans%27%20%28in%20a%20planner-auditor%20feedback%20loop%29.%20In%20the%20second%20stage%2C%20we%0Ause%20a%20diagram%20generator%2C%20DiagramGLIGEN%2C%20and%20a%20text%20label%20rendering%20module%20to%0Agenerate%20diagrams%20%28with%20clear%20text%20labels%29%20following%20the%20diagram%20plans.%20To%0Abenchmark%20the%20text-to-diagram%20generation%20task%2C%20we%20introduce%20AI2D-Caption%2C%20a%0Adensely%20annotated%20diagram%20dataset%20built%20on%20top%20of%20the%20AI2D%20dataset.%20We%20show%0Athat%20our%20DiagrammerGPT%20framework%20produces%20more%20accurate%20diagrams%2C%20outperforming%0Aexisting%20T2I%20models.%20We%20also%20provide%20comprehensive%20analysis%2C%20including%0Aopen-domain%20diagram%20generation%2C%20multi-platform%20vector%20graphic%20diagram%0Ageneration%2C%20human-in-the-loop%20editing%2C%20and%20multimodal%20planner/auditor%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12128v2&entry.124074799=Read"},
{"title": "Pan-cancer Histopathology WSI Pre-training with Position-aware Masked\n  Autoencoder", "author": "Kun Wu and Zhiguo Jiang and Kunming Tang and Jun Shi and Fengying Xie and Wei Wang and Haibo Wu and Yushan Zheng", "abstract": "  Large-scale pre-training models have promoted the development of\nhistopathology image analysis. However, existing self-supervised methods for\nhistopathology images focus on learning patch features, while there is still a\nlack of available pre-training models for WSI-level feature learning. In this\npaper, we propose a novel self-supervised learning framework for pan-cancer\nWSI-level representation pre-training with the designed position-aware masked\nautoencoder (PAMA). Meanwhile, we propose the position-aware cross-attention\n(PACA) module with a kernel reorientation (KRO) strategy and an anchor dropout\n(AD) mechanism. The KRO strategy can capture the complete semantic structure\nand eliminate ambiguity in WSIs, and the AD contributes to enhancing the\nrobustness and generalization of the model. We evaluated our method on 6\nlarge-scale datasets from multiple organs for pan-cancer classification tasks.\nThe results have demonstrated the effectiveness of PAMA in generalized and\ndiscriminative WSI representation learning and pan-cancer WSI pre-training. The\nproposed method was also compared with 7 WSI analysis methods. The experimental\nresults have indicated that our proposed PAMA is superior to the\nstate-of-the-art methods.The code and checkpoints are available at\nhttps://github.com/WkEEn/PAMA.\n", "link": "http://arxiv.org/abs/2407.07504v2", "date": "2024-07-15", "relevancy": 2.6286, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.537}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5229}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pan-cancer%20Histopathology%20WSI%20Pre-training%20with%20Position-aware%20Masked%0A%20%20Autoencoder&body=Title%3A%20Pan-cancer%20Histopathology%20WSI%20Pre-training%20with%20Position-aware%20Masked%0A%20%20Autoencoder%0AAuthor%3A%20Kun%20Wu%20and%20Zhiguo%20Jiang%20and%20Kunming%20Tang%20and%20Jun%20Shi%20and%20Fengying%20Xie%20and%20Wei%20Wang%20and%20Haibo%20Wu%20and%20Yushan%20Zheng%0AAbstract%3A%20%20%20Large-scale%20pre-training%20models%20have%20promoted%20the%20development%20of%0Ahistopathology%20image%20analysis.%20However%2C%20existing%20self-supervised%20methods%20for%0Ahistopathology%20images%20focus%20on%20learning%20patch%20features%2C%20while%20there%20is%20still%20a%0Alack%20of%20available%20pre-training%20models%20for%20WSI-level%20feature%20learning.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20self-supervised%20learning%20framework%20for%20pan-cancer%0AWSI-level%20representation%20pre-training%20with%20the%20designed%20position-aware%20masked%0Aautoencoder%20%28PAMA%29.%20Meanwhile%2C%20we%20propose%20the%20position-aware%20cross-attention%0A%28PACA%29%20module%20with%20a%20kernel%20reorientation%20%28KRO%29%20strategy%20and%20an%20anchor%20dropout%0A%28AD%29%20mechanism.%20The%20KRO%20strategy%20can%20capture%20the%20complete%20semantic%20structure%0Aand%20eliminate%20ambiguity%20in%20WSIs%2C%20and%20the%20AD%20contributes%20to%20enhancing%20the%0Arobustness%20and%20generalization%20of%20the%20model.%20We%20evaluated%20our%20method%20on%206%0Alarge-scale%20datasets%20from%20multiple%20organs%20for%20pan-cancer%20classification%20tasks.%0AThe%20results%20have%20demonstrated%20the%20effectiveness%20of%20PAMA%20in%20generalized%20and%0Adiscriminative%20WSI%20representation%20learning%20and%20pan-cancer%20WSI%20pre-training.%20The%0Aproposed%20method%20was%20also%20compared%20with%207%20WSI%20analysis%20methods.%20The%20experimental%0Aresults%20have%20indicated%20that%20our%20proposed%20PAMA%20is%20superior%20to%20the%0Astate-of-the-art%20methods.The%20code%20and%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/WkEEn/PAMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07504v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPan-cancer%2520Histopathology%2520WSI%2520Pre-training%2520with%2520Position-aware%2520Masked%250A%2520%2520Autoencoder%26entry.906535625%3DKun%2520Wu%2520and%2520Zhiguo%2520Jiang%2520and%2520Kunming%2520Tang%2520and%2520Jun%2520Shi%2520and%2520Fengying%2520Xie%2520and%2520Wei%2520Wang%2520and%2520Haibo%2520Wu%2520and%2520Yushan%2520Zheng%26entry.1292438233%3D%2520%2520Large-scale%2520pre-training%2520models%2520have%2520promoted%2520the%2520development%2520of%250Ahistopathology%2520image%2520analysis.%2520However%252C%2520existing%2520self-supervised%2520methods%2520for%250Ahistopathology%2520images%2520focus%2520on%2520learning%2520patch%2520features%252C%2520while%2520there%2520is%2520still%2520a%250Alack%2520of%2520available%2520pre-training%2520models%2520for%2520WSI-level%2520feature%2520learning.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520self-supervised%2520learning%2520framework%2520for%2520pan-cancer%250AWSI-level%2520representation%2520pre-training%2520with%2520the%2520designed%2520position-aware%2520masked%250Aautoencoder%2520%2528PAMA%2529.%2520Meanwhile%252C%2520we%2520propose%2520the%2520position-aware%2520cross-attention%250A%2528PACA%2529%2520module%2520with%2520a%2520kernel%2520reorientation%2520%2528KRO%2529%2520strategy%2520and%2520an%2520anchor%2520dropout%250A%2528AD%2529%2520mechanism.%2520The%2520KRO%2520strategy%2520can%2520capture%2520the%2520complete%2520semantic%2520structure%250Aand%2520eliminate%2520ambiguity%2520in%2520WSIs%252C%2520and%2520the%2520AD%2520contributes%2520to%2520enhancing%2520the%250Arobustness%2520and%2520generalization%2520of%2520the%2520model.%2520We%2520evaluated%2520our%2520method%2520on%25206%250Alarge-scale%2520datasets%2520from%2520multiple%2520organs%2520for%2520pan-cancer%2520classification%2520tasks.%250AThe%2520results%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520PAMA%2520in%2520generalized%2520and%250Adiscriminative%2520WSI%2520representation%2520learning%2520and%2520pan-cancer%2520WSI%2520pre-training.%2520The%250Aproposed%2520method%2520was%2520also%2520compared%2520with%25207%2520WSI%2520analysis%2520methods.%2520The%2520experimental%250Aresults%2520have%2520indicated%2520that%2520our%2520proposed%2520PAMA%2520is%2520superior%2520to%2520the%250Astate-of-the-art%2520methods.The%2520code%2520and%2520checkpoints%2520are%2520available%2520at%250Ahttps%253A//github.com/WkEEn/PAMA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07504v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pan-cancer%20Histopathology%20WSI%20Pre-training%20with%20Position-aware%20Masked%0A%20%20Autoencoder&entry.906535625=Kun%20Wu%20and%20Zhiguo%20Jiang%20and%20Kunming%20Tang%20and%20Jun%20Shi%20and%20Fengying%20Xie%20and%20Wei%20Wang%20and%20Haibo%20Wu%20and%20Yushan%20Zheng&entry.1292438233=%20%20Large-scale%20pre-training%20models%20have%20promoted%20the%20development%20of%0Ahistopathology%20image%20analysis.%20However%2C%20existing%20self-supervised%20methods%20for%0Ahistopathology%20images%20focus%20on%20learning%20patch%20features%2C%20while%20there%20is%20still%20a%0Alack%20of%20available%20pre-training%20models%20for%20WSI-level%20feature%20learning.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20self-supervised%20learning%20framework%20for%20pan-cancer%0AWSI-level%20representation%20pre-training%20with%20the%20designed%20position-aware%20masked%0Aautoencoder%20%28PAMA%29.%20Meanwhile%2C%20we%20propose%20the%20position-aware%20cross-attention%0A%28PACA%29%20module%20with%20a%20kernel%20reorientation%20%28KRO%29%20strategy%20and%20an%20anchor%20dropout%0A%28AD%29%20mechanism.%20The%20KRO%20strategy%20can%20capture%20the%20complete%20semantic%20structure%0Aand%20eliminate%20ambiguity%20in%20WSIs%2C%20and%20the%20AD%20contributes%20to%20enhancing%20the%0Arobustness%20and%20generalization%20of%20the%20model.%20We%20evaluated%20our%20method%20on%206%0Alarge-scale%20datasets%20from%20multiple%20organs%20for%20pan-cancer%20classification%20tasks.%0AThe%20results%20have%20demonstrated%20the%20effectiveness%20of%20PAMA%20in%20generalized%20and%0Adiscriminative%20WSI%20representation%20learning%20and%20pan-cancer%20WSI%20pre-training.%20The%0Aproposed%20method%20was%20also%20compared%20with%207%20WSI%20analysis%20methods.%20The%20experimental%0Aresults%20have%20indicated%20that%20our%20proposed%20PAMA%20is%20superior%20to%20the%0Astate-of-the-art%20methods.The%20code%20and%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/WkEEn/PAMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07504v2&entry.124074799=Read"},
{"title": "GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via\n  VLM", "author": "Keshav Bimbraw and Ye Wang and Jing Liu and Toshiaki Koike-Akino", "abstract": "  Large vision-language models (LVLMs), such as the Generative Pre-trained\nTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models which\nhave great potential as powerful artificial-intelligence (AI) assistance tools\nfor a myriad of applications, including healthcare, industrial, and academic\nsectors. Although such foundation models perform well in a wide range of\ngeneral tasks, their capability without fine-tuning is often limited in\nspecialized tasks. However, full fine-tuning of large foundation models is\nchallenging due to enormous computation/memory/dataset requirements. We show\nthat GPT-4o can decode hand gestures from forearm ultrasound data even with no\nfine-tuning, and improves with few-shot, in-context learning.\n", "link": "http://arxiv.org/abs/2407.10870v1", "date": "2024-07-15", "relevancy": 2.6162, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.583}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT%20Sonograpy%3A%20Hand%20Gesture%20Decoding%20from%20Forearm%20Ultrasound%20Images%20via%0A%20%20VLM&body=Title%3A%20GPT%20Sonograpy%3A%20Hand%20Gesture%20Decoding%20from%20Forearm%20Ultrasound%20Images%20via%0A%20%20VLM%0AAuthor%3A%20Keshav%20Bimbraw%20and%20Ye%20Wang%20and%20Jing%20Liu%20and%20Toshiaki%20Koike-Akino%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%2C%20such%20as%20the%20Generative%20Pre-trained%0ATransformer%204-omni%20%28GPT-4o%29%2C%20are%20emerging%20multi-modal%20foundation%20models%20which%0Ahave%20great%20potential%20as%20powerful%20artificial-intelligence%20%28AI%29%20assistance%20tools%0Afor%20a%20myriad%20of%20applications%2C%20including%20healthcare%2C%20industrial%2C%20and%20academic%0Asectors.%20Although%20such%20foundation%20models%20perform%20well%20in%20a%20wide%20range%20of%0Ageneral%20tasks%2C%20their%20capability%20without%20fine-tuning%20is%20often%20limited%20in%0Aspecialized%20tasks.%20However%2C%20full%20fine-tuning%20of%20large%20foundation%20models%20is%0Achallenging%20due%20to%20enormous%20computation/memory/dataset%20requirements.%20We%20show%0Athat%20GPT-4o%20can%20decode%20hand%20gestures%20from%20forearm%20ultrasound%20data%20even%20with%20no%0Afine-tuning%2C%20and%20improves%20with%20few-shot%2C%20in-context%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT%2520Sonograpy%253A%2520Hand%2520Gesture%2520Decoding%2520from%2520Forearm%2520Ultrasound%2520Images%2520via%250A%2520%2520VLM%26entry.906535625%3DKeshav%2520Bimbraw%2520and%2520Ye%2520Wang%2520and%2520Jing%2520Liu%2520and%2520Toshiaki%2520Koike-Akino%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%252C%2520such%2520as%2520the%2520Generative%2520Pre-trained%250ATransformer%25204-omni%2520%2528GPT-4o%2529%252C%2520are%2520emerging%2520multi-modal%2520foundation%2520models%2520which%250Ahave%2520great%2520potential%2520as%2520powerful%2520artificial-intelligence%2520%2528AI%2529%2520assistance%2520tools%250Afor%2520a%2520myriad%2520of%2520applications%252C%2520including%2520healthcare%252C%2520industrial%252C%2520and%2520academic%250Asectors.%2520Although%2520such%2520foundation%2520models%2520perform%2520well%2520in%2520a%2520wide%2520range%2520of%250Ageneral%2520tasks%252C%2520their%2520capability%2520without%2520fine-tuning%2520is%2520often%2520limited%2520in%250Aspecialized%2520tasks.%2520However%252C%2520full%2520fine-tuning%2520of%2520large%2520foundation%2520models%2520is%250Achallenging%2520due%2520to%2520enormous%2520computation/memory/dataset%2520requirements.%2520We%2520show%250Athat%2520GPT-4o%2520can%2520decode%2520hand%2520gestures%2520from%2520forearm%2520ultrasound%2520data%2520even%2520with%2520no%250Afine-tuning%252C%2520and%2520improves%2520with%2520few-shot%252C%2520in-context%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT%20Sonograpy%3A%20Hand%20Gesture%20Decoding%20from%20Forearm%20Ultrasound%20Images%20via%0A%20%20VLM&entry.906535625=Keshav%20Bimbraw%20and%20Ye%20Wang%20and%20Jing%20Liu%20and%20Toshiaki%20Koike-Akino&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%2C%20such%20as%20the%20Generative%20Pre-trained%0ATransformer%204-omni%20%28GPT-4o%29%2C%20are%20emerging%20multi-modal%20foundation%20models%20which%0Ahave%20great%20potential%20as%20powerful%20artificial-intelligence%20%28AI%29%20assistance%20tools%0Afor%20a%20myriad%20of%20applications%2C%20including%20healthcare%2C%20industrial%2C%20and%20academic%0Asectors.%20Although%20such%20foundation%20models%20perform%20well%20in%20a%20wide%20range%20of%0Ageneral%20tasks%2C%20their%20capability%20without%20fine-tuning%20is%20often%20limited%20in%0Aspecialized%20tasks.%20However%2C%20full%20fine-tuning%20of%20large%20foundation%20models%20is%0Achallenging%20due%20to%20enormous%20computation/memory/dataset%20requirements.%20We%20show%0Athat%20GPT-4o%20can%20decode%20hand%20gestures%20from%20forearm%20ultrasound%20data%20even%20with%20no%0Afine-tuning%2C%20and%20improves%20with%20few-shot%2C%20in-context%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10870v1&entry.124074799=Read"},
{"title": "PartImageNet++ Dataset: Scaling up Part-based Models for Robust\n  Recognition", "author": "Xiao Li and Yining Liu and Na Dong and Sitian Qin and Xiaolin Hu", "abstract": "  Deep learning-based object recognition systems can be easily fooled by\nvarious adversarial perturbations. One reason for the weak robustness may be\nthat they do not have part-based inductive bias like the human recognition\nprocess. Motivated by this, several part-based recognition models have been\nproposed to improve the adversarial robustness of recognition. However, due to\nthe lack of part annotations, the effectiveness of these methods is only\nvalidated on small-scale nonstandard datasets. In this work, we propose PIN++,\nshort for PartImageNet++, a dataset providing high-quality part segmentation\nannotations for all categories of ImageNet-1K (IN-1K). With these annotations,\nwe build part-based methods directly on the standard IN-1K dataset for robust\nrecognition. Different from previous two-stage part-based models, we propose a\nMulti-scale Part-supervised Model (MPM), to learn a robust representation with\npart annotations. Experiments show that MPM yielded better adversarial\nrobustness on the large-scale IN-1K over strong baselines across various attack\nsettings. Furthermore, MPM achieved improved robustness on common corruptions\nand several out-of-distribution datasets. The dataset, together with these\nresults, enables and encourages researchers to explore the potential of\npart-based models in more real applications.\n", "link": "http://arxiv.org/abs/2407.10918v1", "date": "2024-07-15", "relevancy": 2.614, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5311}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5187}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartImageNet%2B%2B%20Dataset%3A%20Scaling%20up%20Part-based%20Models%20for%20Robust%0A%20%20Recognition&body=Title%3A%20PartImageNet%2B%2B%20Dataset%3A%20Scaling%20up%20Part-based%20Models%20for%20Robust%0A%20%20Recognition%0AAuthor%3A%20Xiao%20Li%20and%20Yining%20Liu%20and%20Na%20Dong%20and%20Sitian%20Qin%20and%20Xiaolin%20Hu%0AAbstract%3A%20%20%20Deep%20learning-based%20object%20recognition%20systems%20can%20be%20easily%20fooled%20by%0Avarious%20adversarial%20perturbations.%20One%20reason%20for%20the%20weak%20robustness%20may%20be%0Athat%20they%20do%20not%20have%20part-based%20inductive%20bias%20like%20the%20human%20recognition%0Aprocess.%20Motivated%20by%20this%2C%20several%20part-based%20recognition%20models%20have%20been%0Aproposed%20to%20improve%20the%20adversarial%20robustness%20of%20recognition.%20However%2C%20due%20to%0Athe%20lack%20of%20part%20annotations%2C%20the%20effectiveness%20of%20these%20methods%20is%20only%0Avalidated%20on%20small-scale%20nonstandard%20datasets.%20In%20this%20work%2C%20we%20propose%20PIN%2B%2B%2C%0Ashort%20for%20PartImageNet%2B%2B%2C%20a%20dataset%20providing%20high-quality%20part%20segmentation%0Aannotations%20for%20all%20categories%20of%20ImageNet-1K%20%28IN-1K%29.%20With%20these%20annotations%2C%0Awe%20build%20part-based%20methods%20directly%20on%20the%20standard%20IN-1K%20dataset%20for%20robust%0Arecognition.%20Different%20from%20previous%20two-stage%20part-based%20models%2C%20we%20propose%20a%0AMulti-scale%20Part-supervised%20Model%20%28MPM%29%2C%20to%20learn%20a%20robust%20representation%20with%0Apart%20annotations.%20Experiments%20show%20that%20MPM%20yielded%20better%20adversarial%0Arobustness%20on%20the%20large-scale%20IN-1K%20over%20strong%20baselines%20across%20various%20attack%0Asettings.%20Furthermore%2C%20MPM%20achieved%20improved%20robustness%20on%20common%20corruptions%0Aand%20several%20out-of-distribution%20datasets.%20The%20dataset%2C%20together%20with%20these%0Aresults%2C%20enables%20and%20encourages%20researchers%20to%20explore%20the%20potential%20of%0Apart-based%20models%20in%20more%20real%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartImageNet%252B%252B%2520Dataset%253A%2520Scaling%2520up%2520Part-based%2520Models%2520for%2520Robust%250A%2520%2520Recognition%26entry.906535625%3DXiao%2520Li%2520and%2520Yining%2520Liu%2520and%2520Na%2520Dong%2520and%2520Sitian%2520Qin%2520and%2520Xiaolin%2520Hu%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520object%2520recognition%2520systems%2520can%2520be%2520easily%2520fooled%2520by%250Avarious%2520adversarial%2520perturbations.%2520One%2520reason%2520for%2520the%2520weak%2520robustness%2520may%2520be%250Athat%2520they%2520do%2520not%2520have%2520part-based%2520inductive%2520bias%2520like%2520the%2520human%2520recognition%250Aprocess.%2520Motivated%2520by%2520this%252C%2520several%2520part-based%2520recognition%2520models%2520have%2520been%250Aproposed%2520to%2520improve%2520the%2520adversarial%2520robustness%2520of%2520recognition.%2520However%252C%2520due%2520to%250Athe%2520lack%2520of%2520part%2520annotations%252C%2520the%2520effectiveness%2520of%2520these%2520methods%2520is%2520only%250Avalidated%2520on%2520small-scale%2520nonstandard%2520datasets.%2520In%2520this%2520work%252C%2520we%2520propose%2520PIN%252B%252B%252C%250Ashort%2520for%2520PartImageNet%252B%252B%252C%2520a%2520dataset%2520providing%2520high-quality%2520part%2520segmentation%250Aannotations%2520for%2520all%2520categories%2520of%2520ImageNet-1K%2520%2528IN-1K%2529.%2520With%2520these%2520annotations%252C%250Awe%2520build%2520part-based%2520methods%2520directly%2520on%2520the%2520standard%2520IN-1K%2520dataset%2520for%2520robust%250Arecognition.%2520Different%2520from%2520previous%2520two-stage%2520part-based%2520models%252C%2520we%2520propose%2520a%250AMulti-scale%2520Part-supervised%2520Model%2520%2528MPM%2529%252C%2520to%2520learn%2520a%2520robust%2520representation%2520with%250Apart%2520annotations.%2520Experiments%2520show%2520that%2520MPM%2520yielded%2520better%2520adversarial%250Arobustness%2520on%2520the%2520large-scale%2520IN-1K%2520over%2520strong%2520baselines%2520across%2520various%2520attack%250Asettings.%2520Furthermore%252C%2520MPM%2520achieved%2520improved%2520robustness%2520on%2520common%2520corruptions%250Aand%2520several%2520out-of-distribution%2520datasets.%2520The%2520dataset%252C%2520together%2520with%2520these%250Aresults%252C%2520enables%2520and%2520encourages%2520researchers%2520to%2520explore%2520the%2520potential%2520of%250Apart-based%2520models%2520in%2520more%2520real%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartImageNet%2B%2B%20Dataset%3A%20Scaling%20up%20Part-based%20Models%20for%20Robust%0A%20%20Recognition&entry.906535625=Xiao%20Li%20and%20Yining%20Liu%20and%20Na%20Dong%20and%20Sitian%20Qin%20and%20Xiaolin%20Hu&entry.1292438233=%20%20Deep%20learning-based%20object%20recognition%20systems%20can%20be%20easily%20fooled%20by%0Avarious%20adversarial%20perturbations.%20One%20reason%20for%20the%20weak%20robustness%20may%20be%0Athat%20they%20do%20not%20have%20part-based%20inductive%20bias%20like%20the%20human%20recognition%0Aprocess.%20Motivated%20by%20this%2C%20several%20part-based%20recognition%20models%20have%20been%0Aproposed%20to%20improve%20the%20adversarial%20robustness%20of%20recognition.%20However%2C%20due%20to%0Athe%20lack%20of%20part%20annotations%2C%20the%20effectiveness%20of%20these%20methods%20is%20only%0Avalidated%20on%20small-scale%20nonstandard%20datasets.%20In%20this%20work%2C%20we%20propose%20PIN%2B%2B%2C%0Ashort%20for%20PartImageNet%2B%2B%2C%20a%20dataset%20providing%20high-quality%20part%20segmentation%0Aannotations%20for%20all%20categories%20of%20ImageNet-1K%20%28IN-1K%29.%20With%20these%20annotations%2C%0Awe%20build%20part-based%20methods%20directly%20on%20the%20standard%20IN-1K%20dataset%20for%20robust%0Arecognition.%20Different%20from%20previous%20two-stage%20part-based%20models%2C%20we%20propose%20a%0AMulti-scale%20Part-supervised%20Model%20%28MPM%29%2C%20to%20learn%20a%20robust%20representation%20with%0Apart%20annotations.%20Experiments%20show%20that%20MPM%20yielded%20better%20adversarial%0Arobustness%20on%20the%20large-scale%20IN-1K%20over%20strong%20baselines%20across%20various%20attack%0Asettings.%20Furthermore%2C%20MPM%20achieved%20improved%20robustness%20on%20common%20corruptions%0Aand%20several%20out-of-distribution%20datasets.%20The%20dataset%2C%20together%20with%20these%0Aresults%2C%20enables%20and%20encourages%20researchers%20to%20explore%20the%20potential%20of%0Apart-based%20models%20in%20more%20real%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10918v1&entry.124074799=Read"},
{"title": "Anticipating Future Object Compositions without Forgetting", "author": "Youssef Zahran and Gertjan Burghouts and Yke Bauke Eisma", "abstract": "  Despite the significant advancements in computer vision models, their ability\nto generalize to novel object-attribute compositions remains limited. Existing\nmethods for Compositional Zero-Shot Learning (CZSL) mainly focus on image\nclassification. This paper aims to enhance CZSL in object detection without\nforgetting prior learned knowledge. We use Grounding DINO and incorporate\nCompositional Soft Prompting (CSP) into it and extend it with Compositional\nAnticipation. We achieve a 70.5% improvement over CSP on the harmonic mean (HM)\nbetween seen and unseen compositions on the CLEVR dataset. Furthermore, we\nintroduce Contrastive Prompt Tuning to incrementally address model confusion\nbetween similar compositions. We demonstrate the effectiveness of this method\nand achieve an increase of 14.5% in HM across the pretrain, increment, and\nunseen sets. Collectively, these methods provide a framework for learning\nvarious compositions with limited data, as well as improving the performance of\nunderperforming compositions when additional data becomes available.\n", "link": "http://arxiv.org/abs/2407.10723v1", "date": "2024-07-15", "relevancy": 2.6117, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anticipating%20Future%20Object%20Compositions%20without%20Forgetting&body=Title%3A%20Anticipating%20Future%20Object%20Compositions%20without%20Forgetting%0AAuthor%3A%20Youssef%20Zahran%20and%20Gertjan%20Burghouts%20and%20Yke%20Bauke%20Eisma%0AAbstract%3A%20%20%20Despite%20the%20significant%20advancements%20in%20computer%20vision%20models%2C%20their%20ability%0Ato%20generalize%20to%20novel%20object-attribute%20compositions%20remains%20limited.%20Existing%0Amethods%20for%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20mainly%20focus%20on%20image%0Aclassification.%20This%20paper%20aims%20to%20enhance%20CZSL%20in%20object%20detection%20without%0Aforgetting%20prior%20learned%20knowledge.%20We%20use%20Grounding%20DINO%20and%20incorporate%0ACompositional%20Soft%20Prompting%20%28CSP%29%20into%20it%20and%20extend%20it%20with%20Compositional%0AAnticipation.%20We%20achieve%20a%2070.5%25%20improvement%20over%20CSP%20on%20the%20harmonic%20mean%20%28HM%29%0Abetween%20seen%20and%20unseen%20compositions%20on%20the%20CLEVR%20dataset.%20Furthermore%2C%20we%0Aintroduce%20Contrastive%20Prompt%20Tuning%20to%20incrementally%20address%20model%20confusion%0Abetween%20similar%20compositions.%20We%20demonstrate%20the%20effectiveness%20of%20this%20method%0Aand%20achieve%20an%20increase%20of%2014.5%25%20in%20HM%20across%20the%20pretrain%2C%20increment%2C%20and%0Aunseen%20sets.%20Collectively%2C%20these%20methods%20provide%20a%20framework%20for%20learning%0Avarious%20compositions%20with%20limited%20data%2C%20as%20well%20as%20improving%20the%20performance%20of%0Aunderperforming%20compositions%20when%20additional%20data%20becomes%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnticipating%2520Future%2520Object%2520Compositions%2520without%2520Forgetting%26entry.906535625%3DYoussef%2520Zahran%2520and%2520Gertjan%2520Burghouts%2520and%2520Yke%2520Bauke%2520Eisma%26entry.1292438233%3D%2520%2520Despite%2520the%2520significant%2520advancements%2520in%2520computer%2520vision%2520models%252C%2520their%2520ability%250Ato%2520generalize%2520to%2520novel%2520object-attribute%2520compositions%2520remains%2520limited.%2520Existing%250Amethods%2520for%2520Compositional%2520Zero-Shot%2520Learning%2520%2528CZSL%2529%2520mainly%2520focus%2520on%2520image%250Aclassification.%2520This%2520paper%2520aims%2520to%2520enhance%2520CZSL%2520in%2520object%2520detection%2520without%250Aforgetting%2520prior%2520learned%2520knowledge.%2520We%2520use%2520Grounding%2520DINO%2520and%2520incorporate%250ACompositional%2520Soft%2520Prompting%2520%2528CSP%2529%2520into%2520it%2520and%2520extend%2520it%2520with%2520Compositional%250AAnticipation.%2520We%2520achieve%2520a%252070.5%2525%2520improvement%2520over%2520CSP%2520on%2520the%2520harmonic%2520mean%2520%2528HM%2529%250Abetween%2520seen%2520and%2520unseen%2520compositions%2520on%2520the%2520CLEVR%2520dataset.%2520Furthermore%252C%2520we%250Aintroduce%2520Contrastive%2520Prompt%2520Tuning%2520to%2520incrementally%2520address%2520model%2520confusion%250Abetween%2520similar%2520compositions.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520method%250Aand%2520achieve%2520an%2520increase%2520of%252014.5%2525%2520in%2520HM%2520across%2520the%2520pretrain%252C%2520increment%252C%2520and%250Aunseen%2520sets.%2520Collectively%252C%2520these%2520methods%2520provide%2520a%2520framework%2520for%2520learning%250Avarious%2520compositions%2520with%2520limited%2520data%252C%2520as%2520well%2520as%2520improving%2520the%2520performance%2520of%250Aunderperforming%2520compositions%2520when%2520additional%2520data%2520becomes%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anticipating%20Future%20Object%20Compositions%20without%20Forgetting&entry.906535625=Youssef%20Zahran%20and%20Gertjan%20Burghouts%20and%20Yke%20Bauke%20Eisma&entry.1292438233=%20%20Despite%20the%20significant%20advancements%20in%20computer%20vision%20models%2C%20their%20ability%0Ato%20generalize%20to%20novel%20object-attribute%20compositions%20remains%20limited.%20Existing%0Amethods%20for%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20mainly%20focus%20on%20image%0Aclassification.%20This%20paper%20aims%20to%20enhance%20CZSL%20in%20object%20detection%20without%0Aforgetting%20prior%20learned%20knowledge.%20We%20use%20Grounding%20DINO%20and%20incorporate%0ACompositional%20Soft%20Prompting%20%28CSP%29%20into%20it%20and%20extend%20it%20with%20Compositional%0AAnticipation.%20We%20achieve%20a%2070.5%25%20improvement%20over%20CSP%20on%20the%20harmonic%20mean%20%28HM%29%0Abetween%20seen%20and%20unseen%20compositions%20on%20the%20CLEVR%20dataset.%20Furthermore%2C%20we%0Aintroduce%20Contrastive%20Prompt%20Tuning%20to%20incrementally%20address%20model%20confusion%0Abetween%20similar%20compositions.%20We%20demonstrate%20the%20effectiveness%20of%20this%20method%0Aand%20achieve%20an%20increase%20of%2014.5%25%20in%20HM%20across%20the%20pretrain%2C%20increment%2C%20and%0Aunseen%20sets.%20Collectively%2C%20these%20methods%20provide%20a%20framework%20for%20learning%0Avarious%20compositions%20with%20limited%20data%2C%20as%20well%20as%20improving%20the%20performance%20of%0Aunderperforming%20compositions%20when%20additional%20data%20becomes%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10723v1&entry.124074799=Read"},
{"title": "Addressing Image Hallucination in Text-to-Image Generation through\n  Factual Image Retrieval", "author": "Youngsun Lim and Hyunjung Shim", "abstract": "  Text-to-image generation has shown remarkable progress with the emergence of\ndiffusion models. However, these models often generate factually inconsistent\nimages, failing to accurately reflect the factual information and common sense\nconveyed by the input text prompts. We refer to this issue as Image\nhallucination. Drawing from studies on hallucinations in language models, we\nclassify this problem into three types and propose a methodology that uses\nfactual images retrieved from external sources to generate realistic images.\nDepending on the nature of the hallucination, we employ off-the-shelf image\nediting tools, either InstructPix2Pix or IP-Adapter, to leverage factual\ninformation from the retrieved image. This approach enables the generation of\nimages that accurately reflect the facts and common sense.\n", "link": "http://arxiv.org/abs/2407.10683v1", "date": "2024-07-15", "relevancy": 2.6092, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5271}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5251}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20through%0A%20%20Factual%20Image%20Retrieval&body=Title%3A%20Addressing%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20through%0A%20%20Factual%20Image%20Retrieval%0AAuthor%3A%20Youngsun%20Lim%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Text-to-image%20generation%20has%20shown%20remarkable%20progress%20with%20the%20emergence%20of%0Adiffusion%20models.%20However%2C%20these%20models%20often%20generate%20factually%20inconsistent%0Aimages%2C%20failing%20to%20accurately%20reflect%20the%20factual%20information%20and%20common%20sense%0Aconveyed%20by%20the%20input%20text%20prompts.%20We%20refer%20to%20this%20issue%20as%20Image%0Ahallucination.%20Drawing%20from%20studies%20on%20hallucinations%20in%20language%20models%2C%20we%0Aclassify%20this%20problem%20into%20three%20types%20and%20propose%20a%20methodology%20that%20uses%0Afactual%20images%20retrieved%20from%20external%20sources%20to%20generate%20realistic%20images.%0ADepending%20on%20the%20nature%20of%20the%20hallucination%2C%20we%20employ%20off-the-shelf%20image%0Aediting%20tools%2C%20either%20InstructPix2Pix%20or%20IP-Adapter%2C%20to%20leverage%20factual%0Ainformation%20from%20the%20retrieved%20image.%20This%20approach%20enables%20the%20generation%20of%0Aimages%20that%20accurately%20reflect%20the%20facts%20and%20common%20sense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Image%2520Hallucination%2520in%2520Text-to-Image%2520Generation%2520through%250A%2520%2520Factual%2520Image%2520Retrieval%26entry.906535625%3DYoungsun%2520Lim%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Text-to-image%2520generation%2520has%2520shown%2520remarkable%2520progress%2520with%2520the%2520emergence%2520of%250Adiffusion%2520models.%2520However%252C%2520these%2520models%2520often%2520generate%2520factually%2520inconsistent%250Aimages%252C%2520failing%2520to%2520accurately%2520reflect%2520the%2520factual%2520information%2520and%2520common%2520sense%250Aconveyed%2520by%2520the%2520input%2520text%2520prompts.%2520We%2520refer%2520to%2520this%2520issue%2520as%2520Image%250Ahallucination.%2520Drawing%2520from%2520studies%2520on%2520hallucinations%2520in%2520language%2520models%252C%2520we%250Aclassify%2520this%2520problem%2520into%2520three%2520types%2520and%2520propose%2520a%2520methodology%2520that%2520uses%250Afactual%2520images%2520retrieved%2520from%2520external%2520sources%2520to%2520generate%2520realistic%2520images.%250ADepending%2520on%2520the%2520nature%2520of%2520the%2520hallucination%252C%2520we%2520employ%2520off-the-shelf%2520image%250Aediting%2520tools%252C%2520either%2520InstructPix2Pix%2520or%2520IP-Adapter%252C%2520to%2520leverage%2520factual%250Ainformation%2520from%2520the%2520retrieved%2520image.%2520This%2520approach%2520enables%2520the%2520generation%2520of%250Aimages%2520that%2520accurately%2520reflect%2520the%2520facts%2520and%2520common%2520sense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20through%0A%20%20Factual%20Image%20Retrieval&entry.906535625=Youngsun%20Lim%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Text-to-image%20generation%20has%20shown%20remarkable%20progress%20with%20the%20emergence%20of%0Adiffusion%20models.%20However%2C%20these%20models%20often%20generate%20factually%20inconsistent%0Aimages%2C%20failing%20to%20accurately%20reflect%20the%20factual%20information%20and%20common%20sense%0Aconveyed%20by%20the%20input%20text%20prompts.%20We%20refer%20to%20this%20issue%20as%20Image%0Ahallucination.%20Drawing%20from%20studies%20on%20hallucinations%20in%20language%20models%2C%20we%0Aclassify%20this%20problem%20into%20three%20types%20and%20propose%20a%20methodology%20that%20uses%0Afactual%20images%20retrieved%20from%20external%20sources%20to%20generate%20realistic%20images.%0ADepending%20on%20the%20nature%20of%20the%20hallucination%2C%20we%20employ%20off-the-shelf%20image%0Aediting%20tools%2C%20either%20InstructPix2Pix%20or%20IP-Adapter%2C%20to%20leverage%20factual%0Ainformation%20from%20the%20retrieved%20image.%20This%20approach%20enables%20the%20generation%20of%0Aimages%20that%20accurately%20reflect%20the%20facts%20and%20common%20sense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10683v1&entry.124074799=Read"},
{"title": "APC: Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation", "author": "Wangyu Wu and Tianhong Dai and Zhenhong Chen and Xiaowei Huang and Fei Ma and Jimin Xiao", "abstract": "  Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels\nhas gained significant attention due to its cost-effectiveness. The typical\nframework involves using image-level labels as training data to generate\npixel-level pseudo-labels with refinements. Recently, methods based on Vision\nTransformers (ViT) have demonstrated superior capabilities in generating\nreliable pseudo-labels, particularly in recognizing complete object regions,\ncompared to CNN methods. However, current ViT-based approaches have some\nlimitations in the use of patch embeddings, being prone to being dominated by\ncertain abnormal patches, as well as many multi-stage methods being\ntime-consuming and lengthy in training, thus lacking efficiency. Therefore, in\nthis paper, we introduce a novel ViT-based WSSS method named \\textit{Adaptive\nPatch Contrast} (APC) that significantly enhances patch embedding learning for\nimproved segmentation effectiveness. APC utilizes an Adaptive-K Pooling (AKP)\nlayer to address the limitations of previous max pooling selection methods.\nAdditionally, we propose a Patch Contrastive Learning (PCL) to enhance patch\nembeddings, thereby further improving the final results. Furthermore, we\nimprove upon the existing multi-stage training framework without CAM by\ntransforming it into an end-to-end single-stage training approach, thereby\nenhancing training efficiency. The experimental results show that our approach\nis effective and efficient, outperforming other state-of-the-art WSSS methods\non the PASCAL VOC 2012 and MS COCO 2014 dataset within a shorter training\nduration.\n", "link": "http://arxiv.org/abs/2407.10649v1", "date": "2024-07-15", "relevancy": 2.5971, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5171}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APC%3A%20Adaptive%20Patch%20Contrast%20for%20Weakly%20Supervised%20Semantic%20Segmentation&body=Title%3A%20APC%3A%20Adaptive%20Patch%20Contrast%20for%20Weakly%20Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Wangyu%20Wu%20and%20Tianhong%20Dai%20and%20Zhenhong%20Chen%20and%20Xiaowei%20Huang%20and%20Fei%20Ma%20and%20Jimin%20Xiao%0AAbstract%3A%20%20%20Weakly%20Supervised%20Semantic%20Segmentation%20%28WSSS%29%20using%20only%20image-level%20labels%0Ahas%20gained%20significant%20attention%20due%20to%20its%20cost-effectiveness.%20The%20typical%0Aframework%20involves%20using%20image-level%20labels%20as%20training%20data%20to%20generate%0Apixel-level%20pseudo-labels%20with%20refinements.%20Recently%2C%20methods%20based%20on%20Vision%0ATransformers%20%28ViT%29%20have%20demonstrated%20superior%20capabilities%20in%20generating%0Areliable%20pseudo-labels%2C%20particularly%20in%20recognizing%20complete%20object%20regions%2C%0Acompared%20to%20CNN%20methods.%20However%2C%20current%20ViT-based%20approaches%20have%20some%0Alimitations%20in%20the%20use%20of%20patch%20embeddings%2C%20being%20prone%20to%20being%20dominated%20by%0Acertain%20abnormal%20patches%2C%20as%20well%20as%20many%20multi-stage%20methods%20being%0Atime-consuming%20and%20lengthy%20in%20training%2C%20thus%20lacking%20efficiency.%20Therefore%2C%20in%0Athis%20paper%2C%20we%20introduce%20a%20novel%20ViT-based%20WSSS%20method%20named%20%5Ctextit%7BAdaptive%0APatch%20Contrast%7D%20%28APC%29%20that%20significantly%20enhances%20patch%20embedding%20learning%20for%0Aimproved%20segmentation%20effectiveness.%20APC%20utilizes%20an%20Adaptive-K%20Pooling%20%28AKP%29%0Alayer%20to%20address%20the%20limitations%20of%20previous%20max%20pooling%20selection%20methods.%0AAdditionally%2C%20we%20propose%20a%20Patch%20Contrastive%20Learning%20%28PCL%29%20to%20enhance%20patch%0Aembeddings%2C%20thereby%20further%20improving%20the%20final%20results.%20Furthermore%2C%20we%0Aimprove%20upon%20the%20existing%20multi-stage%20training%20framework%20without%20CAM%20by%0Atransforming%20it%20into%20an%20end-to-end%20single-stage%20training%20approach%2C%20thereby%0Aenhancing%20training%20efficiency.%20The%20experimental%20results%20show%20that%20our%20approach%0Ais%20effective%20and%20efficient%2C%20outperforming%20other%20state-of-the-art%20WSSS%20methods%0Aon%20the%20PASCAL%20VOC%202012%20and%20MS%20COCO%202014%20dataset%20within%20a%20shorter%20training%0Aduration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPC%253A%2520Adaptive%2520Patch%2520Contrast%2520for%2520Weakly%2520Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DWangyu%2520Wu%2520and%2520Tianhong%2520Dai%2520and%2520Zhenhong%2520Chen%2520and%2520Xiaowei%2520Huang%2520and%2520Fei%2520Ma%2520and%2520Jimin%2520Xiao%26entry.1292438233%3D%2520%2520Weakly%2520Supervised%2520Semantic%2520Segmentation%2520%2528WSSS%2529%2520using%2520only%2520image-level%2520labels%250Ahas%2520gained%2520significant%2520attention%2520due%2520to%2520its%2520cost-effectiveness.%2520The%2520typical%250Aframework%2520involves%2520using%2520image-level%2520labels%2520as%2520training%2520data%2520to%2520generate%250Apixel-level%2520pseudo-labels%2520with%2520refinements.%2520Recently%252C%2520methods%2520based%2520on%2520Vision%250ATransformers%2520%2528ViT%2529%2520have%2520demonstrated%2520superior%2520capabilities%2520in%2520generating%250Areliable%2520pseudo-labels%252C%2520particularly%2520in%2520recognizing%2520complete%2520object%2520regions%252C%250Acompared%2520to%2520CNN%2520methods.%2520However%252C%2520current%2520ViT-based%2520approaches%2520have%2520some%250Alimitations%2520in%2520the%2520use%2520of%2520patch%2520embeddings%252C%2520being%2520prone%2520to%2520being%2520dominated%2520by%250Acertain%2520abnormal%2520patches%252C%2520as%2520well%2520as%2520many%2520multi-stage%2520methods%2520being%250Atime-consuming%2520and%2520lengthy%2520in%2520training%252C%2520thus%2520lacking%2520efficiency.%2520Therefore%252C%2520in%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520ViT-based%2520WSSS%2520method%2520named%2520%255Ctextit%257BAdaptive%250APatch%2520Contrast%257D%2520%2528APC%2529%2520that%2520significantly%2520enhances%2520patch%2520embedding%2520learning%2520for%250Aimproved%2520segmentation%2520effectiveness.%2520APC%2520utilizes%2520an%2520Adaptive-K%2520Pooling%2520%2528AKP%2529%250Alayer%2520to%2520address%2520the%2520limitations%2520of%2520previous%2520max%2520pooling%2520selection%2520methods.%250AAdditionally%252C%2520we%2520propose%2520a%2520Patch%2520Contrastive%2520Learning%2520%2528PCL%2529%2520to%2520enhance%2520patch%250Aembeddings%252C%2520thereby%2520further%2520improving%2520the%2520final%2520results.%2520Furthermore%252C%2520we%250Aimprove%2520upon%2520the%2520existing%2520multi-stage%2520training%2520framework%2520without%2520CAM%2520by%250Atransforming%2520it%2520into%2520an%2520end-to-end%2520single-stage%2520training%2520approach%252C%2520thereby%250Aenhancing%2520training%2520efficiency.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520approach%250Ais%2520effective%2520and%2520efficient%252C%2520outperforming%2520other%2520state-of-the-art%2520WSSS%2520methods%250Aon%2520the%2520PASCAL%2520VOC%25202012%2520and%2520MS%2520COCO%25202014%2520dataset%2520within%2520a%2520shorter%2520training%250Aduration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APC%3A%20Adaptive%20Patch%20Contrast%20for%20Weakly%20Supervised%20Semantic%20Segmentation&entry.906535625=Wangyu%20Wu%20and%20Tianhong%20Dai%20and%20Zhenhong%20Chen%20and%20Xiaowei%20Huang%20and%20Fei%20Ma%20and%20Jimin%20Xiao&entry.1292438233=%20%20Weakly%20Supervised%20Semantic%20Segmentation%20%28WSSS%29%20using%20only%20image-level%20labels%0Ahas%20gained%20significant%20attention%20due%20to%20its%20cost-effectiveness.%20The%20typical%0Aframework%20involves%20using%20image-level%20labels%20as%20training%20data%20to%20generate%0Apixel-level%20pseudo-labels%20with%20refinements.%20Recently%2C%20methods%20based%20on%20Vision%0ATransformers%20%28ViT%29%20have%20demonstrated%20superior%20capabilities%20in%20generating%0Areliable%20pseudo-labels%2C%20particularly%20in%20recognizing%20complete%20object%20regions%2C%0Acompared%20to%20CNN%20methods.%20However%2C%20current%20ViT-based%20approaches%20have%20some%0Alimitations%20in%20the%20use%20of%20patch%20embeddings%2C%20being%20prone%20to%20being%20dominated%20by%0Acertain%20abnormal%20patches%2C%20as%20well%20as%20many%20multi-stage%20methods%20being%0Atime-consuming%20and%20lengthy%20in%20training%2C%20thus%20lacking%20efficiency.%20Therefore%2C%20in%0Athis%20paper%2C%20we%20introduce%20a%20novel%20ViT-based%20WSSS%20method%20named%20%5Ctextit%7BAdaptive%0APatch%20Contrast%7D%20%28APC%29%20that%20significantly%20enhances%20patch%20embedding%20learning%20for%0Aimproved%20segmentation%20effectiveness.%20APC%20utilizes%20an%20Adaptive-K%20Pooling%20%28AKP%29%0Alayer%20to%20address%20the%20limitations%20of%20previous%20max%20pooling%20selection%20methods.%0AAdditionally%2C%20we%20propose%20a%20Patch%20Contrastive%20Learning%20%28PCL%29%20to%20enhance%20patch%0Aembeddings%2C%20thereby%20further%20improving%20the%20final%20results.%20Furthermore%2C%20we%0Aimprove%20upon%20the%20existing%20multi-stage%20training%20framework%20without%20CAM%20by%0Atransforming%20it%20into%20an%20end-to-end%20single-stage%20training%20approach%2C%20thereby%0Aenhancing%20training%20efficiency.%20The%20experimental%20results%20show%20that%20our%20approach%0Ais%20effective%20and%20efficient%2C%20outperforming%20other%20state-of-the-art%20WSSS%20methods%0Aon%20the%20PASCAL%20VOC%202012%20and%20MS%20COCO%202014%20dataset%20within%20a%20shorter%20training%0Aduration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10649v1&entry.124074799=Read"},
{"title": "IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint\n  Video-Depth Generation", "author": "Yuanhao Zhai and Kevin Lin and Linjie Li and Chung-Ching Lin and Jianfeng Wang and Zhengyuan Yang and David Doermann and Junsong Yuan and Zicheng Liu and Lijuan Wang", "abstract": "  Significant advances have been made in human-centric video generation, yet\nthe joint video-depth generation problem remains underexplored. Most existing\nmonocular depth estimation methods may not generalize well to synthesized\nimages or videos, and multi-view-based methods have difficulty controlling the\nhuman appearance and motion. In this work, we present IDOL (unIfied Dual-mOdal\nLatent diffusion) for high-quality human-centric joint video-depth generation.\nOur IDOL consists of two novel designs. First, to enable dual-modal generation\nand maximize the information exchange between video and depth generation, we\npropose a unified dual-modal U-Net, a parameter-sharing framework for joint\nvideo and depth denoising, wherein a modality label guides the denoising\ntarget, and cross-modal attention enables the mutual information flow. Second,\nto ensure a precise video-depth spatial alignment, we propose a motion\nconsistency loss that enforces consistency between the video and depth feature\nmotion fields, leading to harmonized outputs. Additionally, a cross-attention\nmap consistency loss is applied to align the cross-attention map of the video\ndenoising with that of the depth denoising, further facilitating spatial\nalignment. Extensive experiments on the TikTok and NTU120 datasets show our\nsuperior performance, significantly surpassing existing methods in terms of\nvideo FVD and depth accuracy.\n", "link": "http://arxiv.org/abs/2407.10937v1", "date": "2024-07-15", "relevancy": 2.5882, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6796}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.627}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDOL%3A%20Unified%20Dual-Modal%20Latent%20Diffusion%20for%20Human-Centric%20Joint%0A%20%20Video-Depth%20Generation&body=Title%3A%20IDOL%3A%20Unified%20Dual-Modal%20Latent%20Diffusion%20for%20Human-Centric%20Joint%0A%20%20Video-Depth%20Generation%0AAuthor%3A%20Yuanhao%20Zhai%20and%20Kevin%20Lin%20and%20Linjie%20Li%20and%20Chung-Ching%20Lin%20and%20Jianfeng%20Wang%20and%20Zhengyuan%20Yang%20and%20David%20Doermann%20and%20Junsong%20Yuan%20and%20Zicheng%20Liu%20and%20Lijuan%20Wang%0AAbstract%3A%20%20%20Significant%20advances%20have%20been%20made%20in%20human-centric%20video%20generation%2C%20yet%0Athe%20joint%20video-depth%20generation%20problem%20remains%20underexplored.%20Most%20existing%0Amonocular%20depth%20estimation%20methods%20may%20not%20generalize%20well%20to%20synthesized%0Aimages%20or%20videos%2C%20and%20multi-view-based%20methods%20have%20difficulty%20controlling%20the%0Ahuman%20appearance%20and%20motion.%20In%20this%20work%2C%20we%20present%20IDOL%20%28unIfied%20Dual-mOdal%0ALatent%20diffusion%29%20for%20high-quality%20human-centric%20joint%20video-depth%20generation.%0AOur%20IDOL%20consists%20of%20two%20novel%20designs.%20First%2C%20to%20enable%20dual-modal%20generation%0Aand%20maximize%20the%20information%20exchange%20between%20video%20and%20depth%20generation%2C%20we%0Apropose%20a%20unified%20dual-modal%20U-Net%2C%20a%20parameter-sharing%20framework%20for%20joint%0Avideo%20and%20depth%20denoising%2C%20wherein%20a%20modality%20label%20guides%20the%20denoising%0Atarget%2C%20and%20cross-modal%20attention%20enables%20the%20mutual%20information%20flow.%20Second%2C%0Ato%20ensure%20a%20precise%20video-depth%20spatial%20alignment%2C%20we%20propose%20a%20motion%0Aconsistency%20loss%20that%20enforces%20consistency%20between%20the%20video%20and%20depth%20feature%0Amotion%20fields%2C%20leading%20to%20harmonized%20outputs.%20Additionally%2C%20a%20cross-attention%0Amap%20consistency%20loss%20is%20applied%20to%20align%20the%20cross-attention%20map%20of%20the%20video%0Adenoising%20with%20that%20of%20the%20depth%20denoising%2C%20further%20facilitating%20spatial%0Aalignment.%20Extensive%20experiments%20on%20the%20TikTok%20and%20NTU120%20datasets%20show%20our%0Asuperior%20performance%2C%20significantly%20surpassing%20existing%20methods%20in%20terms%20of%0Avideo%20FVD%20and%20depth%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDOL%253A%2520Unified%2520Dual-Modal%2520Latent%2520Diffusion%2520for%2520Human-Centric%2520Joint%250A%2520%2520Video-Depth%2520Generation%26entry.906535625%3DYuanhao%2520Zhai%2520and%2520Kevin%2520Lin%2520and%2520Linjie%2520Li%2520and%2520Chung-Ching%2520Lin%2520and%2520Jianfeng%2520Wang%2520and%2520Zhengyuan%2520Yang%2520and%2520David%2520Doermann%2520and%2520Junsong%2520Yuan%2520and%2520Zicheng%2520Liu%2520and%2520Lijuan%2520Wang%26entry.1292438233%3D%2520%2520Significant%2520advances%2520have%2520been%2520made%2520in%2520human-centric%2520video%2520generation%252C%2520yet%250Athe%2520joint%2520video-depth%2520generation%2520problem%2520remains%2520underexplored.%2520Most%2520existing%250Amonocular%2520depth%2520estimation%2520methods%2520may%2520not%2520generalize%2520well%2520to%2520synthesized%250Aimages%2520or%2520videos%252C%2520and%2520multi-view-based%2520methods%2520have%2520difficulty%2520controlling%2520the%250Ahuman%2520appearance%2520and%2520motion.%2520In%2520this%2520work%252C%2520we%2520present%2520IDOL%2520%2528unIfied%2520Dual-mOdal%250ALatent%2520diffusion%2529%2520for%2520high-quality%2520human-centric%2520joint%2520video-depth%2520generation.%250AOur%2520IDOL%2520consists%2520of%2520two%2520novel%2520designs.%2520First%252C%2520to%2520enable%2520dual-modal%2520generation%250Aand%2520maximize%2520the%2520information%2520exchange%2520between%2520video%2520and%2520depth%2520generation%252C%2520we%250Apropose%2520a%2520unified%2520dual-modal%2520U-Net%252C%2520a%2520parameter-sharing%2520framework%2520for%2520joint%250Avideo%2520and%2520depth%2520denoising%252C%2520wherein%2520a%2520modality%2520label%2520guides%2520the%2520denoising%250Atarget%252C%2520and%2520cross-modal%2520attention%2520enables%2520the%2520mutual%2520information%2520flow.%2520Second%252C%250Ato%2520ensure%2520a%2520precise%2520video-depth%2520spatial%2520alignment%252C%2520we%2520propose%2520a%2520motion%250Aconsistency%2520loss%2520that%2520enforces%2520consistency%2520between%2520the%2520video%2520and%2520depth%2520feature%250Amotion%2520fields%252C%2520leading%2520to%2520harmonized%2520outputs.%2520Additionally%252C%2520a%2520cross-attention%250Amap%2520consistency%2520loss%2520is%2520applied%2520to%2520align%2520the%2520cross-attention%2520map%2520of%2520the%2520video%250Adenoising%2520with%2520that%2520of%2520the%2520depth%2520denoising%252C%2520further%2520facilitating%2520spatial%250Aalignment.%2520Extensive%2520experiments%2520on%2520the%2520TikTok%2520and%2520NTU120%2520datasets%2520show%2520our%250Asuperior%2520performance%252C%2520significantly%2520surpassing%2520existing%2520methods%2520in%2520terms%2520of%250Avideo%2520FVD%2520and%2520depth%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDOL%3A%20Unified%20Dual-Modal%20Latent%20Diffusion%20for%20Human-Centric%20Joint%0A%20%20Video-Depth%20Generation&entry.906535625=Yuanhao%20Zhai%20and%20Kevin%20Lin%20and%20Linjie%20Li%20and%20Chung-Ching%20Lin%20and%20Jianfeng%20Wang%20and%20Zhengyuan%20Yang%20and%20David%20Doermann%20and%20Junsong%20Yuan%20and%20Zicheng%20Liu%20and%20Lijuan%20Wang&entry.1292438233=%20%20Significant%20advances%20have%20been%20made%20in%20human-centric%20video%20generation%2C%20yet%0Athe%20joint%20video-depth%20generation%20problem%20remains%20underexplored.%20Most%20existing%0Amonocular%20depth%20estimation%20methods%20may%20not%20generalize%20well%20to%20synthesized%0Aimages%20or%20videos%2C%20and%20multi-view-based%20methods%20have%20difficulty%20controlling%20the%0Ahuman%20appearance%20and%20motion.%20In%20this%20work%2C%20we%20present%20IDOL%20%28unIfied%20Dual-mOdal%0ALatent%20diffusion%29%20for%20high-quality%20human-centric%20joint%20video-depth%20generation.%0AOur%20IDOL%20consists%20of%20two%20novel%20designs.%20First%2C%20to%20enable%20dual-modal%20generation%0Aand%20maximize%20the%20information%20exchange%20between%20video%20and%20depth%20generation%2C%20we%0Apropose%20a%20unified%20dual-modal%20U-Net%2C%20a%20parameter-sharing%20framework%20for%20joint%0Avideo%20and%20depth%20denoising%2C%20wherein%20a%20modality%20label%20guides%20the%20denoising%0Atarget%2C%20and%20cross-modal%20attention%20enables%20the%20mutual%20information%20flow.%20Second%2C%0Ato%20ensure%20a%20precise%20video-depth%20spatial%20alignment%2C%20we%20propose%20a%20motion%0Aconsistency%20loss%20that%20enforces%20consistency%20between%20the%20video%20and%20depth%20feature%0Amotion%20fields%2C%20leading%20to%20harmonized%20outputs.%20Additionally%2C%20a%20cross-attention%0Amap%20consistency%20loss%20is%20applied%20to%20align%20the%20cross-attention%20map%20of%20the%20video%0Adenoising%20with%20that%20of%20the%20depth%20denoising%2C%20further%20facilitating%20spatial%0Aalignment.%20Extensive%20experiments%20on%20the%20TikTok%20and%20NTU120%20datasets%20show%20our%0Asuperior%20performance%2C%20significantly%20surpassing%20existing%20methods%20in%20terms%20of%0Avideo%20FVD%20and%20depth%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10937v1&entry.124074799=Read"},
{"title": "Enhancing Robustness to Noise Corruption for Point Cloud Model via\n  Spatial Sorting and Set-Mixing Aggregation Module", "author": "Dingxin Zhang and Jianhui Yu and Tengfei Xue and Chaoyi Zhang and Dongnan Liu and Weidong Cai", "abstract": "  Current models for point cloud recognition demonstrate promising performance\non synthetic datasets. However, real-world point cloud data inevitably contains\nnoise, impacting model robustness. While recent efforts focus on enhancing\nrobustness through various strategies, there still remains a gap in\ncomprehensive analyzes from the standpoint of network architecture design.\nUnlike traditional methods that rely on generic techniques, our approach\noptimizes model robustness to noise corruption through network architecture\ndesign. Inspired by the token-mixing technique applied in 2D images, we propose\nSet-Mixer, a noise-robust aggregation module which facilitates communication\namong all points to extract geometric shape information and mitigating the\ninfluence of individual noise points. A sorting strategy is designed to enable\nour module to be invariant to point permutation, which also tackles the\nunordered structure of point cloud and introduces consistent relative spatial\ninformation. Experiments conducted on ModelNet40-C indicate that Set-Mixer\nsignificantly enhances the model performance on noisy point clouds,\nunderscoring its potential to advance real-world applicability in 3D\nrecognition and perception tasks.\n", "link": "http://arxiv.org/abs/2407.10806v1", "date": "2024-07-15", "relevancy": 2.5858, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5307}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.513}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Robustness%20to%20Noise%20Corruption%20for%20Point%20Cloud%20Model%20via%0A%20%20Spatial%20Sorting%20and%20Set-Mixing%20Aggregation%20Module&body=Title%3A%20Enhancing%20Robustness%20to%20Noise%20Corruption%20for%20Point%20Cloud%20Model%20via%0A%20%20Spatial%20Sorting%20and%20Set-Mixing%20Aggregation%20Module%0AAuthor%3A%20Dingxin%20Zhang%20and%20Jianhui%20Yu%20and%20Tengfei%20Xue%20and%20Chaoyi%20Zhang%20and%20Dongnan%20Liu%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Current%20models%20for%20point%20cloud%20recognition%20demonstrate%20promising%20performance%0Aon%20synthetic%20datasets.%20However%2C%20real-world%20point%20cloud%20data%20inevitably%20contains%0Anoise%2C%20impacting%20model%20robustness.%20While%20recent%20efforts%20focus%20on%20enhancing%0Arobustness%20through%20various%20strategies%2C%20there%20still%20remains%20a%20gap%20in%0Acomprehensive%20analyzes%20from%20the%20standpoint%20of%20network%20architecture%20design.%0AUnlike%20traditional%20methods%20that%20rely%20on%20generic%20techniques%2C%20our%20approach%0Aoptimizes%20model%20robustness%20to%20noise%20corruption%20through%20network%20architecture%0Adesign.%20Inspired%20by%20the%20token-mixing%20technique%20applied%20in%202D%20images%2C%20we%20propose%0ASet-Mixer%2C%20a%20noise-robust%20aggregation%20module%20which%20facilitates%20communication%0Aamong%20all%20points%20to%20extract%20geometric%20shape%20information%20and%20mitigating%20the%0Ainfluence%20of%20individual%20noise%20points.%20A%20sorting%20strategy%20is%20designed%20to%20enable%0Aour%20module%20to%20be%20invariant%20to%20point%20permutation%2C%20which%20also%20tackles%20the%0Aunordered%20structure%20of%20point%20cloud%20and%20introduces%20consistent%20relative%20spatial%0Ainformation.%20Experiments%20conducted%20on%20ModelNet40-C%20indicate%20that%20Set-Mixer%0Asignificantly%20enhances%20the%20model%20performance%20on%20noisy%20point%20clouds%2C%0Aunderscoring%20its%20potential%20to%20advance%20real-world%20applicability%20in%203D%0Arecognition%20and%20perception%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Robustness%2520to%2520Noise%2520Corruption%2520for%2520Point%2520Cloud%2520Model%2520via%250A%2520%2520Spatial%2520Sorting%2520and%2520Set-Mixing%2520Aggregation%2520Module%26entry.906535625%3DDingxin%2520Zhang%2520and%2520Jianhui%2520Yu%2520and%2520Tengfei%2520Xue%2520and%2520Chaoyi%2520Zhang%2520and%2520Dongnan%2520Liu%2520and%2520Weidong%2520Cai%26entry.1292438233%3D%2520%2520Current%2520models%2520for%2520point%2520cloud%2520recognition%2520demonstrate%2520promising%2520performance%250Aon%2520synthetic%2520datasets.%2520However%252C%2520real-world%2520point%2520cloud%2520data%2520inevitably%2520contains%250Anoise%252C%2520impacting%2520model%2520robustness.%2520While%2520recent%2520efforts%2520focus%2520on%2520enhancing%250Arobustness%2520through%2520various%2520strategies%252C%2520there%2520still%2520remains%2520a%2520gap%2520in%250Acomprehensive%2520analyzes%2520from%2520the%2520standpoint%2520of%2520network%2520architecture%2520design.%250AUnlike%2520traditional%2520methods%2520that%2520rely%2520on%2520generic%2520techniques%252C%2520our%2520approach%250Aoptimizes%2520model%2520robustness%2520to%2520noise%2520corruption%2520through%2520network%2520architecture%250Adesign.%2520Inspired%2520by%2520the%2520token-mixing%2520technique%2520applied%2520in%25202D%2520images%252C%2520we%2520propose%250ASet-Mixer%252C%2520a%2520noise-robust%2520aggregation%2520module%2520which%2520facilitates%2520communication%250Aamong%2520all%2520points%2520to%2520extract%2520geometric%2520shape%2520information%2520and%2520mitigating%2520the%250Ainfluence%2520of%2520individual%2520noise%2520points.%2520A%2520sorting%2520strategy%2520is%2520designed%2520to%2520enable%250Aour%2520module%2520to%2520be%2520invariant%2520to%2520point%2520permutation%252C%2520which%2520also%2520tackles%2520the%250Aunordered%2520structure%2520of%2520point%2520cloud%2520and%2520introduces%2520consistent%2520relative%2520spatial%250Ainformation.%2520Experiments%2520conducted%2520on%2520ModelNet40-C%2520indicate%2520that%2520Set-Mixer%250Asignificantly%2520enhances%2520the%2520model%2520performance%2520on%2520noisy%2520point%2520clouds%252C%250Aunderscoring%2520its%2520potential%2520to%2520advance%2520real-world%2520applicability%2520in%25203D%250Arecognition%2520and%2520perception%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Robustness%20to%20Noise%20Corruption%20for%20Point%20Cloud%20Model%20via%0A%20%20Spatial%20Sorting%20and%20Set-Mixing%20Aggregation%20Module&entry.906535625=Dingxin%20Zhang%20and%20Jianhui%20Yu%20and%20Tengfei%20Xue%20and%20Chaoyi%20Zhang%20and%20Dongnan%20Liu%20and%20Weidong%20Cai&entry.1292438233=%20%20Current%20models%20for%20point%20cloud%20recognition%20demonstrate%20promising%20performance%0Aon%20synthetic%20datasets.%20However%2C%20real-world%20point%20cloud%20data%20inevitably%20contains%0Anoise%2C%20impacting%20model%20robustness.%20While%20recent%20efforts%20focus%20on%20enhancing%0Arobustness%20through%20various%20strategies%2C%20there%20still%20remains%20a%20gap%20in%0Acomprehensive%20analyzes%20from%20the%20standpoint%20of%20network%20architecture%20design.%0AUnlike%20traditional%20methods%20that%20rely%20on%20generic%20techniques%2C%20our%20approach%0Aoptimizes%20model%20robustness%20to%20noise%20corruption%20through%20network%20architecture%0Adesign.%20Inspired%20by%20the%20token-mixing%20technique%20applied%20in%202D%20images%2C%20we%20propose%0ASet-Mixer%2C%20a%20noise-robust%20aggregation%20module%20which%20facilitates%20communication%0Aamong%20all%20points%20to%20extract%20geometric%20shape%20information%20and%20mitigating%20the%0Ainfluence%20of%20individual%20noise%20points.%20A%20sorting%20strategy%20is%20designed%20to%20enable%0Aour%20module%20to%20be%20invariant%20to%20point%20permutation%2C%20which%20also%20tackles%20the%0Aunordered%20structure%20of%20point%20cloud%20and%20introduces%20consistent%20relative%20spatial%0Ainformation.%20Experiments%20conducted%20on%20ModelNet40-C%20indicate%20that%20Set-Mixer%0Asignificantly%20enhances%20the%20model%20performance%20on%20noisy%20point%20clouds%2C%0Aunderscoring%20its%20potential%20to%20advance%20real-world%20applicability%20in%203D%0Arecognition%20and%20perception%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10806v1&entry.124074799=Read"},
{"title": "Temporal Event Stereo via Joint Learning with Stereoscopic Flow", "author": "Hoonhee Cho and Jae-Young Kang and Kuk-Jin Yoon", "abstract": "  Event cameras are dynamic vision sensors inspired by the biological retina,\ncharacterized by their high dynamic range, high temporal resolution, and low\npower consumption. These features make them capable of perceiving 3D\nenvironments even in extreme conditions. Event data is continuous across the\ntime dimension, which allows a detailed description of each pixel's movements.\nTo fully utilize the temporally dense and continuous nature of event cameras,\nwe propose a novel temporal event stereo, a framework that continuously uses\ninformation from previous time steps. This is accomplished through the\nsimultaneous training of an event stereo matching network alongside\nstereoscopic flow, a new concept that captures all pixel movements from stereo\ncameras. Since obtaining ground truth for optical flow during training is\nchallenging, we propose a method that uses only disparity maps to train the\nstereoscopic flow. The performance of event-based stereo matching is enhanced\nby temporally aggregating information using the flows. We have achieved\nstate-of-the-art performance on the MVSEC and the DSEC datasets. The method is\ncomputationally efficient, as it stacks previous information in a cascading\nmanner. The code is available at\nhttps://github.com/mickeykang16/TemporalEventStereo.\n", "link": "http://arxiv.org/abs/2407.10831v1", "date": "2024-07-15", "relevancy": 2.572, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5238}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5098}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Event%20Stereo%20via%20Joint%20Learning%20with%20Stereoscopic%20Flow&body=Title%3A%20Temporal%20Event%20Stereo%20via%20Joint%20Learning%20with%20Stereoscopic%20Flow%0AAuthor%3A%20Hoonhee%20Cho%20and%20Jae-Young%20Kang%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%20Event%20cameras%20are%20dynamic%20vision%20sensors%20inspired%20by%20the%20biological%20retina%2C%0Acharacterized%20by%20their%20high%20dynamic%20range%2C%20high%20temporal%20resolution%2C%20and%20low%0Apower%20consumption.%20These%20features%20make%20them%20capable%20of%20perceiving%203D%0Aenvironments%20even%20in%20extreme%20conditions.%20Event%20data%20is%20continuous%20across%20the%0Atime%20dimension%2C%20which%20allows%20a%20detailed%20description%20of%20each%20pixel%27s%20movements.%0ATo%20fully%20utilize%20the%20temporally%20dense%20and%20continuous%20nature%20of%20event%20cameras%2C%0Awe%20propose%20a%20novel%20temporal%20event%20stereo%2C%20a%20framework%20that%20continuously%20uses%0Ainformation%20from%20previous%20time%20steps.%20This%20is%20accomplished%20through%20the%0Asimultaneous%20training%20of%20an%20event%20stereo%20matching%20network%20alongside%0Astereoscopic%20flow%2C%20a%20new%20concept%20that%20captures%20all%20pixel%20movements%20from%20stereo%0Acameras.%20Since%20obtaining%20ground%20truth%20for%20optical%20flow%20during%20training%20is%0Achallenging%2C%20we%20propose%20a%20method%20that%20uses%20only%20disparity%20maps%20to%20train%20the%0Astereoscopic%20flow.%20The%20performance%20of%20event-based%20stereo%20matching%20is%20enhanced%0Aby%20temporally%20aggregating%20information%20using%20the%20flows.%20We%20have%20achieved%0Astate-of-the-art%20performance%20on%20the%20MVSEC%20and%20the%20DSEC%20datasets.%20The%20method%20is%0Acomputationally%20efficient%2C%20as%20it%20stacks%20previous%20information%20in%20a%20cascading%0Amanner.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/mickeykang16/TemporalEventStereo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Event%2520Stereo%2520via%2520Joint%2520Learning%2520with%2520Stereoscopic%2520Flow%26entry.906535625%3DHoonhee%2520Cho%2520and%2520Jae-Young%2520Kang%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%2520Event%2520cameras%2520are%2520dynamic%2520vision%2520sensors%2520inspired%2520by%2520the%2520biological%2520retina%252C%250Acharacterized%2520by%2520their%2520high%2520dynamic%2520range%252C%2520high%2520temporal%2520resolution%252C%2520and%2520low%250Apower%2520consumption.%2520These%2520features%2520make%2520them%2520capable%2520of%2520perceiving%25203D%250Aenvironments%2520even%2520in%2520extreme%2520conditions.%2520Event%2520data%2520is%2520continuous%2520across%2520the%250Atime%2520dimension%252C%2520which%2520allows%2520a%2520detailed%2520description%2520of%2520each%2520pixel%2527s%2520movements.%250ATo%2520fully%2520utilize%2520the%2520temporally%2520dense%2520and%2520continuous%2520nature%2520of%2520event%2520cameras%252C%250Awe%2520propose%2520a%2520novel%2520temporal%2520event%2520stereo%252C%2520a%2520framework%2520that%2520continuously%2520uses%250Ainformation%2520from%2520previous%2520time%2520steps.%2520This%2520is%2520accomplished%2520through%2520the%250Asimultaneous%2520training%2520of%2520an%2520event%2520stereo%2520matching%2520network%2520alongside%250Astereoscopic%2520flow%252C%2520a%2520new%2520concept%2520that%2520captures%2520all%2520pixel%2520movements%2520from%2520stereo%250Acameras.%2520Since%2520obtaining%2520ground%2520truth%2520for%2520optical%2520flow%2520during%2520training%2520is%250Achallenging%252C%2520we%2520propose%2520a%2520method%2520that%2520uses%2520only%2520disparity%2520maps%2520to%2520train%2520the%250Astereoscopic%2520flow.%2520The%2520performance%2520of%2520event-based%2520stereo%2520matching%2520is%2520enhanced%250Aby%2520temporally%2520aggregating%2520information%2520using%2520the%2520flows.%2520We%2520have%2520achieved%250Astate-of-the-art%2520performance%2520on%2520the%2520MVSEC%2520and%2520the%2520DSEC%2520datasets.%2520The%2520method%2520is%250Acomputationally%2520efficient%252C%2520as%2520it%2520stacks%2520previous%2520information%2520in%2520a%2520cascading%250Amanner.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mickeykang16/TemporalEventStereo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Event%20Stereo%20via%20Joint%20Learning%20with%20Stereoscopic%20Flow&entry.906535625=Hoonhee%20Cho%20and%20Jae-Young%20Kang%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%20Event%20cameras%20are%20dynamic%20vision%20sensors%20inspired%20by%20the%20biological%20retina%2C%0Acharacterized%20by%20their%20high%20dynamic%20range%2C%20high%20temporal%20resolution%2C%20and%20low%0Apower%20consumption.%20These%20features%20make%20them%20capable%20of%20perceiving%203D%0Aenvironments%20even%20in%20extreme%20conditions.%20Event%20data%20is%20continuous%20across%20the%0Atime%20dimension%2C%20which%20allows%20a%20detailed%20description%20of%20each%20pixel%27s%20movements.%0ATo%20fully%20utilize%20the%20temporally%20dense%20and%20continuous%20nature%20of%20event%20cameras%2C%0Awe%20propose%20a%20novel%20temporal%20event%20stereo%2C%20a%20framework%20that%20continuously%20uses%0Ainformation%20from%20previous%20time%20steps.%20This%20is%20accomplished%20through%20the%0Asimultaneous%20training%20of%20an%20event%20stereo%20matching%20network%20alongside%0Astereoscopic%20flow%2C%20a%20new%20concept%20that%20captures%20all%20pixel%20movements%20from%20stereo%0Acameras.%20Since%20obtaining%20ground%20truth%20for%20optical%20flow%20during%20training%20is%0Achallenging%2C%20we%20propose%20a%20method%20that%20uses%20only%20disparity%20maps%20to%20train%20the%0Astereoscopic%20flow.%20The%20performance%20of%20event-based%20stereo%20matching%20is%20enhanced%0Aby%20temporally%20aggregating%20information%20using%20the%20flows.%20We%20have%20achieved%0Astate-of-the-art%20performance%20on%20the%20MVSEC%20and%20the%20DSEC%20datasets.%20The%20method%20is%0Acomputationally%20efficient%2C%20as%20it%20stacks%20previous%20information%20in%20a%20cascading%0Amanner.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/mickeykang16/TemporalEventStereo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10831v1&entry.124074799=Read"},
{"title": "No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen\n  Representations", "author": "Walter Simoncini and Spyros Gidaris and Andrei Bursuc and Yuki M. Asano", "abstract": "  This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of vision encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These are\nprojected to a lower dimension and then concatenated with the model's\nembedding. The resulting features are evaluated on k-nearest neighbor\nclassification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification and image retrieval, and that they significantly\nimprove the retrieval-based in-context scene understanding abilities of\npretrained models, for example improving upon DINO by +17% for semantic\nsegmentation - without any training.\n", "link": "http://arxiv.org/abs/2407.10964v1", "date": "2024-07-15", "relevancy": 2.5719, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5431}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5077}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Train%2C%20all%20Gain%3A%20Self-Supervised%20Gradients%20Improve%20Deep%20Frozen%0A%20%20Representations&body=Title%3A%20No%20Train%2C%20all%20Gain%3A%20Self-Supervised%20Gradients%20Improve%20Deep%20Frozen%0A%20%20Representations%0AAuthor%3A%20Walter%20Simoncini%20and%20Spyros%20Gidaris%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20This%20paper%20introduces%20FUNGI%2C%20Features%20from%20UNsupervised%20GradIents%2C%20a%20method%0Ato%20enhance%20the%20features%20of%20vision%20encoders%20by%20leveraging%20self-supervised%0Agradients.%20Our%20method%20is%20simple%3A%20given%20any%20pretrained%20model%2C%20we%20first%20compute%0Agradients%20from%20various%20self-supervised%20objectives%20for%20each%20input.%20These%20are%0Aprojected%20to%20a%20lower%20dimension%20and%20then%20concatenated%20with%20the%20model%27s%0Aembedding.%20The%20resulting%20features%20are%20evaluated%20on%20k-nearest%20neighbor%0Aclassification%20over%2011%20datasets%20from%20vision%2C%205%20from%20natural%20language%0Aprocessing%2C%20and%202%20from%20audio.%20Across%20backbones%20spanning%20various%20sizes%20and%0Apretraining%20strategies%2C%20FUNGI%20features%20provide%20consistent%20performance%0Aimprovements%20over%20the%20embeddings.%20We%20also%20show%20that%20using%20FUNGI%20features%20can%0Abenefit%20linear%20classification%20and%20image%20retrieval%2C%20and%20that%20they%20significantly%0Aimprove%20the%20retrieval-based%20in-context%20scene%20understanding%20abilities%20of%0Apretrained%20models%2C%20for%20example%20improving%20upon%20DINO%20by%20%2B17%25%20for%20semantic%0Asegmentation%20-%20without%20any%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Train%252C%2520all%2520Gain%253A%2520Self-Supervised%2520Gradients%2520Improve%2520Deep%2520Frozen%250A%2520%2520Representations%26entry.906535625%3DWalter%2520Simoncini%2520and%2520Spyros%2520Gidaris%2520and%2520Andrei%2520Bursuc%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520FUNGI%252C%2520Features%2520from%2520UNsupervised%2520GradIents%252C%2520a%2520method%250Ato%2520enhance%2520the%2520features%2520of%2520vision%2520encoders%2520by%2520leveraging%2520self-supervised%250Agradients.%2520Our%2520method%2520is%2520simple%253A%2520given%2520any%2520pretrained%2520model%252C%2520we%2520first%2520compute%250Agradients%2520from%2520various%2520self-supervised%2520objectives%2520for%2520each%2520input.%2520These%2520are%250Aprojected%2520to%2520a%2520lower%2520dimension%2520and%2520then%2520concatenated%2520with%2520the%2520model%2527s%250Aembedding.%2520The%2520resulting%2520features%2520are%2520evaluated%2520on%2520k-nearest%2520neighbor%250Aclassification%2520over%252011%2520datasets%2520from%2520vision%252C%25205%2520from%2520natural%2520language%250Aprocessing%252C%2520and%25202%2520from%2520audio.%2520Across%2520backbones%2520spanning%2520various%2520sizes%2520and%250Apretraining%2520strategies%252C%2520FUNGI%2520features%2520provide%2520consistent%2520performance%250Aimprovements%2520over%2520the%2520embeddings.%2520We%2520also%2520show%2520that%2520using%2520FUNGI%2520features%2520can%250Abenefit%2520linear%2520classification%2520and%2520image%2520retrieval%252C%2520and%2520that%2520they%2520significantly%250Aimprove%2520the%2520retrieval-based%2520in-context%2520scene%2520understanding%2520abilities%2520of%250Apretrained%2520models%252C%2520for%2520example%2520improving%2520upon%2520DINO%2520by%2520%252B17%2525%2520for%2520semantic%250Asegmentation%2520-%2520without%2520any%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Train%2C%20all%20Gain%3A%20Self-Supervised%20Gradients%20Improve%20Deep%20Frozen%0A%20%20Representations&entry.906535625=Walter%20Simoncini%20and%20Spyros%20Gidaris%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20This%20paper%20introduces%20FUNGI%2C%20Features%20from%20UNsupervised%20GradIents%2C%20a%20method%0Ato%20enhance%20the%20features%20of%20vision%20encoders%20by%20leveraging%20self-supervised%0Agradients.%20Our%20method%20is%20simple%3A%20given%20any%20pretrained%20model%2C%20we%20first%20compute%0Agradients%20from%20various%20self-supervised%20objectives%20for%20each%20input.%20These%20are%0Aprojected%20to%20a%20lower%20dimension%20and%20then%20concatenated%20with%20the%20model%27s%0Aembedding.%20The%20resulting%20features%20are%20evaluated%20on%20k-nearest%20neighbor%0Aclassification%20over%2011%20datasets%20from%20vision%2C%205%20from%20natural%20language%0Aprocessing%2C%20and%202%20from%20audio.%20Across%20backbones%20spanning%20various%20sizes%20and%0Apretraining%20strategies%2C%20FUNGI%20features%20provide%20consistent%20performance%0Aimprovements%20over%20the%20embeddings.%20We%20also%20show%20that%20using%20FUNGI%20features%20can%0Abenefit%20linear%20classification%20and%20image%20retrieval%2C%20and%20that%20they%20significantly%0Aimprove%20the%20retrieval-based%20in-context%20scene%20understanding%20abilities%20of%0Apretrained%20models%2C%20for%20example%20improving%20upon%20DINO%20by%20%2B17%25%20for%20semantic%0Asegmentation%20-%20without%20any%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10964v1&entry.124074799=Read"},
{"title": "GeoMix: Towards Geometry-Aware Data Augmentation", "author": "Wentao Zhao and Qitian Wu and Chenxiao Yang and Junchi Yan", "abstract": "  Mixup has shown considerable success in mitigating the challenges posed by\nlimited labeled data in image classification. By synthesizing samples through\nthe interpolation of features and labels, Mixup effectively addresses the issue\nof data scarcity. However, it has rarely been explored in graph learning tasks\ndue to the irregularity and connectivity of graph data. Specifically, in node\nclassification tasks, Mixup presents a challenge in creating connections for\nsynthetic data. In this paper, we propose Geometric Mixup (GeoMix), a simple\nand interpretable Mixup approach leveraging in-place graph editing. It\neffectively utilizes geometry information to interpolate features and labels\nwith those from the nearby neighborhood, generating synthetic nodes and\nestablishing connections for them. We conduct theoretical analysis to elucidate\nthe rationale behind employing geometry information for node Mixup, emphasizing\nthe significance of locality enhancement-a critical aspect of our method's\ndesign. Extensive experiments demonstrate that our lightweight Geometric Mixup\nachieves state-of-the-art results on a wide variety of standard datasets with\nlimited labeled data. Furthermore, it significantly improves the generalization\ncapability of underlying GNNs across various challenging out-of-distribution\ngeneralization tasks. Our code is available at\nhttps://github.com/WtaoZhao/geomix.\n", "link": "http://arxiv.org/abs/2407.10681v1", "date": "2024-07-15", "relevancy": 2.5615, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5222}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5182}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMix%3A%20Towards%20Geometry-Aware%20Data%20Augmentation&body=Title%3A%20GeoMix%3A%20Towards%20Geometry-Aware%20Data%20Augmentation%0AAuthor%3A%20Wentao%20Zhao%20and%20Qitian%20Wu%20and%20Chenxiao%20Yang%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20Mixup%20has%20shown%20considerable%20success%20in%20mitigating%20the%20challenges%20posed%20by%0Alimited%20labeled%20data%20in%20image%20classification.%20By%20synthesizing%20samples%20through%0Athe%20interpolation%20of%20features%20and%20labels%2C%20Mixup%20effectively%20addresses%20the%20issue%0Aof%20data%20scarcity.%20However%2C%20it%20has%20rarely%20been%20explored%20in%20graph%20learning%20tasks%0Adue%20to%20the%20irregularity%20and%20connectivity%20of%20graph%20data.%20Specifically%2C%20in%20node%0Aclassification%20tasks%2C%20Mixup%20presents%20a%20challenge%20in%20creating%20connections%20for%0Asynthetic%20data.%20In%20this%20paper%2C%20we%20propose%20Geometric%20Mixup%20%28GeoMix%29%2C%20a%20simple%0Aand%20interpretable%20Mixup%20approach%20leveraging%20in-place%20graph%20editing.%20It%0Aeffectively%20utilizes%20geometry%20information%20to%20interpolate%20features%20and%20labels%0Awith%20those%20from%20the%20nearby%20neighborhood%2C%20generating%20synthetic%20nodes%20and%0Aestablishing%20connections%20for%20them.%20We%20conduct%20theoretical%20analysis%20to%20elucidate%0Athe%20rationale%20behind%20employing%20geometry%20information%20for%20node%20Mixup%2C%20emphasizing%0Athe%20significance%20of%20locality%20enhancement-a%20critical%20aspect%20of%20our%20method%27s%0Adesign.%20Extensive%20experiments%20demonstrate%20that%20our%20lightweight%20Geometric%20Mixup%0Aachieves%20state-of-the-art%20results%20on%20a%20wide%20variety%20of%20standard%20datasets%20with%0Alimited%20labeled%20data.%20Furthermore%2C%20it%20significantly%20improves%20the%20generalization%0Acapability%20of%20underlying%20GNNs%20across%20various%20challenging%20out-of-distribution%0Ageneralization%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/WtaoZhao/geomix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMix%253A%2520Towards%2520Geometry-Aware%2520Data%2520Augmentation%26entry.906535625%3DWentao%2520Zhao%2520and%2520Qitian%2520Wu%2520and%2520Chenxiao%2520Yang%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520Mixup%2520has%2520shown%2520considerable%2520success%2520in%2520mitigating%2520the%2520challenges%2520posed%2520by%250Alimited%2520labeled%2520data%2520in%2520image%2520classification.%2520By%2520synthesizing%2520samples%2520through%250Athe%2520interpolation%2520of%2520features%2520and%2520labels%252C%2520Mixup%2520effectively%2520addresses%2520the%2520issue%250Aof%2520data%2520scarcity.%2520However%252C%2520it%2520has%2520rarely%2520been%2520explored%2520in%2520graph%2520learning%2520tasks%250Adue%2520to%2520the%2520irregularity%2520and%2520connectivity%2520of%2520graph%2520data.%2520Specifically%252C%2520in%2520node%250Aclassification%2520tasks%252C%2520Mixup%2520presents%2520a%2520challenge%2520in%2520creating%2520connections%2520for%250Asynthetic%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Geometric%2520Mixup%2520%2528GeoMix%2529%252C%2520a%2520simple%250Aand%2520interpretable%2520Mixup%2520approach%2520leveraging%2520in-place%2520graph%2520editing.%2520It%250Aeffectively%2520utilizes%2520geometry%2520information%2520to%2520interpolate%2520features%2520and%2520labels%250Awith%2520those%2520from%2520the%2520nearby%2520neighborhood%252C%2520generating%2520synthetic%2520nodes%2520and%250Aestablishing%2520connections%2520for%2520them.%2520We%2520conduct%2520theoretical%2520analysis%2520to%2520elucidate%250Athe%2520rationale%2520behind%2520employing%2520geometry%2520information%2520for%2520node%2520Mixup%252C%2520emphasizing%250Athe%2520significance%2520of%2520locality%2520enhancement-a%2520critical%2520aspect%2520of%2520our%2520method%2527s%250Adesign.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520lightweight%2520Geometric%2520Mixup%250Aachieves%2520state-of-the-art%2520results%2520on%2520a%2520wide%2520variety%2520of%2520standard%2520datasets%2520with%250Alimited%2520labeled%2520data.%2520Furthermore%252C%2520it%2520significantly%2520improves%2520the%2520generalization%250Acapability%2520of%2520underlying%2520GNNs%2520across%2520various%2520challenging%2520out-of-distribution%250Ageneralization%2520tasks.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/WtaoZhao/geomix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMix%3A%20Towards%20Geometry-Aware%20Data%20Augmentation&entry.906535625=Wentao%20Zhao%20and%20Qitian%20Wu%20and%20Chenxiao%20Yang%20and%20Junchi%20Yan&entry.1292438233=%20%20Mixup%20has%20shown%20considerable%20success%20in%20mitigating%20the%20challenges%20posed%20by%0Alimited%20labeled%20data%20in%20image%20classification.%20By%20synthesizing%20samples%20through%0Athe%20interpolation%20of%20features%20and%20labels%2C%20Mixup%20effectively%20addresses%20the%20issue%0Aof%20data%20scarcity.%20However%2C%20it%20has%20rarely%20been%20explored%20in%20graph%20learning%20tasks%0Adue%20to%20the%20irregularity%20and%20connectivity%20of%20graph%20data.%20Specifically%2C%20in%20node%0Aclassification%20tasks%2C%20Mixup%20presents%20a%20challenge%20in%20creating%20connections%20for%0Asynthetic%20data.%20In%20this%20paper%2C%20we%20propose%20Geometric%20Mixup%20%28GeoMix%29%2C%20a%20simple%0Aand%20interpretable%20Mixup%20approach%20leveraging%20in-place%20graph%20editing.%20It%0Aeffectively%20utilizes%20geometry%20information%20to%20interpolate%20features%20and%20labels%0Awith%20those%20from%20the%20nearby%20neighborhood%2C%20generating%20synthetic%20nodes%20and%0Aestablishing%20connections%20for%20them.%20We%20conduct%20theoretical%20analysis%20to%20elucidate%0Athe%20rationale%20behind%20employing%20geometry%20information%20for%20node%20Mixup%2C%20emphasizing%0Athe%20significance%20of%20locality%20enhancement-a%20critical%20aspect%20of%20our%20method%27s%0Adesign.%20Extensive%20experiments%20demonstrate%20that%20our%20lightweight%20Geometric%20Mixup%0Aachieves%20state-of-the-art%20results%20on%20a%20wide%20variety%20of%20standard%20datasets%20with%0Alimited%20labeled%20data.%20Furthermore%2C%20it%20significantly%20improves%20the%20generalization%0Acapability%20of%20underlying%20GNNs%20across%20various%20challenging%20out-of-distribution%0Ageneralization%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/WtaoZhao/geomix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10681v1&entry.124074799=Read"},
{"title": "Conceptual Codebook Learning for Vision-Language Models", "author": "Yi Zhang and Ke Yu and Siqi Wu and Zhihai He", "abstract": "  In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.\n", "link": "http://arxiv.org/abs/2407.02350v3", "date": "2024-07-15", "relevancy": 2.5533, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5228}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5125}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models&body=Title%3A%20Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models%0AAuthor%3A%20Yi%20Zhang%20and%20Ke%20Yu%20and%20Siqi%20Wu%20and%20Zhihai%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Conceptual%20Codebook%20Learning%20%28CoCoLe%29%2C%20a%20novel%0Afine-tuning%20method%20for%20vision-language%20models%20%28VLMs%29%20to%20address%20the%20challenge%0Aof%20improving%20the%20generalization%20capability%20of%20VLMs%20while%20fine-tuning%20them%20on%0Adownstream%20tasks%20in%20a%20few-shot%20setting.%20We%20recognize%20that%20visual%20concepts%2C%20such%0Aas%20textures%2C%20shapes%2C%20and%20colors%20are%20naturally%20transferable%20across%20domains%20and%0Aplay%20a%20crucial%20role%20in%20generalization%20tasks.%20Motivated%20by%20this%20interesting%0Afinding%2C%20we%20learn%20a%20conceptual%20codebook%20consisting%20of%20visual%20concepts%20as%20keys%0Aand%20conceptual%20prompts%20as%20values%2C%20which%20serves%20as%20a%20link%20between%20the%20image%0Aencoder%27s%20outputs%20and%20the%20text%20encoder%27s%20inputs.%20Specifically%2C%20for%20a%20given%0Aimage%2C%20we%20leverage%20the%20codebook%20to%20identify%20the%20most%20relevant%20conceptual%0Aprompts%20associated%20with%20the%20class%20embeddings%20to%20perform%20the%20classification.%0AAdditionally%2C%20we%20incorporate%20a%20handcrafted%20concept%20cache%20as%20a%20regularization%20to%0Aalleviate%20the%20overfitting%20issues%20in%20low-shot%20scenarios.%20We%20observe%20that%20this%0Aconceptual%20codebook%20learning%20method%20is%20able%20to%20achieve%20enhanced%20alignment%0Abetween%20visual%20and%20linguistic%20modalities.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20CoCoLe%20method%20remarkably%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20across%20various%20evaluation%20settings%2C%20including%0Abase-to-new%20generalization%2C%20cross-dataset%20evaluation%2C%20and%20domain%20generalization%0Atasks.%20Detailed%20ablation%20studies%20further%20confirm%20the%20efficacy%20of%20each%20component%0Ain%20CoCoLe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02350v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptual%2520Codebook%2520Learning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DYi%2520Zhang%2520and%2520Ke%2520Yu%2520and%2520Siqi%2520Wu%2520and%2520Zhihai%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Conceptual%2520Codebook%2520Learning%2520%2528CoCoLe%2529%252C%2520a%2520novel%250Afine-tuning%2520method%2520for%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520address%2520the%2520challenge%250Aof%2520improving%2520the%2520generalization%2520capability%2520of%2520VLMs%2520while%2520fine-tuning%2520them%2520on%250Adownstream%2520tasks%2520in%2520a%2520few-shot%2520setting.%2520We%2520recognize%2520that%2520visual%2520concepts%252C%2520such%250Aas%2520textures%252C%2520shapes%252C%2520and%2520colors%2520are%2520naturally%2520transferable%2520across%2520domains%2520and%250Aplay%2520a%2520crucial%2520role%2520in%2520generalization%2520tasks.%2520Motivated%2520by%2520this%2520interesting%250Afinding%252C%2520we%2520learn%2520a%2520conceptual%2520codebook%2520consisting%2520of%2520visual%2520concepts%2520as%2520keys%250Aand%2520conceptual%2520prompts%2520as%2520values%252C%2520which%2520serves%2520as%2520a%2520link%2520between%2520the%2520image%250Aencoder%2527s%2520outputs%2520and%2520the%2520text%2520encoder%2527s%2520inputs.%2520Specifically%252C%2520for%2520a%2520given%250Aimage%252C%2520we%2520leverage%2520the%2520codebook%2520to%2520identify%2520the%2520most%2520relevant%2520conceptual%250Aprompts%2520associated%2520with%2520the%2520class%2520embeddings%2520to%2520perform%2520the%2520classification.%250AAdditionally%252C%2520we%2520incorporate%2520a%2520handcrafted%2520concept%2520cache%2520as%2520a%2520regularization%2520to%250Aalleviate%2520the%2520overfitting%2520issues%2520in%2520low-shot%2520scenarios.%2520We%2520observe%2520that%2520this%250Aconceptual%2520codebook%2520learning%2520method%2520is%2520able%2520to%2520achieve%2520enhanced%2520alignment%250Abetween%2520visual%2520and%2520linguistic%2520modalities.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520CoCoLe%2520method%2520remarkably%2520outperforms%2520the%2520existing%250Astate-of-the-art%2520methods%2520across%2520various%2520evaluation%2520settings%252C%2520including%250Abase-to-new%2520generalization%252C%2520cross-dataset%2520evaluation%252C%2520and%2520domain%2520generalization%250Atasks.%2520Detailed%2520ablation%2520studies%2520further%2520confirm%2520the%2520efficacy%2520of%2520each%2520component%250Ain%2520CoCoLe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02350v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models&entry.906535625=Yi%20Zhang%20and%20Ke%20Yu%20and%20Siqi%20Wu%20and%20Zhihai%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Conceptual%20Codebook%20Learning%20%28CoCoLe%29%2C%20a%20novel%0Afine-tuning%20method%20for%20vision-language%20models%20%28VLMs%29%20to%20address%20the%20challenge%0Aof%20improving%20the%20generalization%20capability%20of%20VLMs%20while%20fine-tuning%20them%20on%0Adownstream%20tasks%20in%20a%20few-shot%20setting.%20We%20recognize%20that%20visual%20concepts%2C%20such%0Aas%20textures%2C%20shapes%2C%20and%20colors%20are%20naturally%20transferable%20across%20domains%20and%0Aplay%20a%20crucial%20role%20in%20generalization%20tasks.%20Motivated%20by%20this%20interesting%0Afinding%2C%20we%20learn%20a%20conceptual%20codebook%20consisting%20of%20visual%20concepts%20as%20keys%0Aand%20conceptual%20prompts%20as%20values%2C%20which%20serves%20as%20a%20link%20between%20the%20image%0Aencoder%27s%20outputs%20and%20the%20text%20encoder%27s%20inputs.%20Specifically%2C%20for%20a%20given%0Aimage%2C%20we%20leverage%20the%20codebook%20to%20identify%20the%20most%20relevant%20conceptual%0Aprompts%20associated%20with%20the%20class%20embeddings%20to%20perform%20the%20classification.%0AAdditionally%2C%20we%20incorporate%20a%20handcrafted%20concept%20cache%20as%20a%20regularization%20to%0Aalleviate%20the%20overfitting%20issues%20in%20low-shot%20scenarios.%20We%20observe%20that%20this%0Aconceptual%20codebook%20learning%20method%20is%20able%20to%20achieve%20enhanced%20alignment%0Abetween%20visual%20and%20linguistic%20modalities.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20CoCoLe%20method%20remarkably%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20across%20various%20evaluation%20settings%2C%20including%0Abase-to-new%20generalization%2C%20cross-dataset%20evaluation%2C%20and%20domain%20generalization%0Atasks.%20Detailed%20ablation%20studies%20further%20confirm%20the%20efficacy%20of%20each%20component%0Ain%20CoCoLe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02350v3&entry.124074799=Read"},
{"title": "Human-in-the-Loop Visual Re-ID for Population Size Estimation", "author": "Gustavo Perez and Daniel Sheldon and Grant Van Horn and Subhransu Maji", "abstract": "  Computer vision-based re-identification (Re-ID) systems are increasingly\nbeing deployed for estimating population size in large image collections.\nHowever, the estimated size can be significantly inaccurate when the task is\nchallenging or when deployed on data from new distributions. We propose a\nhuman-in-the-loop approach for estimating population size driven by a pairwise\nsimilarity derived from an off-the-shelf Re-ID system. Our approach, based on\nnested importance sampling, selects pairs of images for human vetting driven by\nthe pairwise similarity, and produces asymptotically unbiased population size\nestimates with associated confidence intervals. We perform experiments on\nvarious animal Re-ID datasets and demonstrate that our method outperforms\nstrong baselines and active clustering approaches. In many cases, we are able\nto reduce the error rates of the estimated size from around 80% using CV alone\nto less than 20% by vetting a fraction (often less than 0.002%) of the total\npairs. The cost of vetting reduces with the increase in accuracy and provides a\npractical approach for population size estimation within a desired tolerance\nwhen deploying Re-ID systems.\n", "link": "http://arxiv.org/abs/2312.05287v2", "date": "2024-07-15", "relevancy": 2.5454, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5279}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5058}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-in-the-Loop%20Visual%20Re-ID%20for%20Population%20Size%20Estimation&body=Title%3A%20Human-in-the-Loop%20Visual%20Re-ID%20for%20Population%20Size%20Estimation%0AAuthor%3A%20Gustavo%20Perez%20and%20Daniel%20Sheldon%20and%20Grant%20Van%20Horn%20and%20Subhransu%20Maji%0AAbstract%3A%20%20%20Computer%20vision-based%20re-identification%20%28Re-ID%29%20systems%20are%20increasingly%0Abeing%20deployed%20for%20estimating%20population%20size%20in%20large%20image%20collections.%0AHowever%2C%20the%20estimated%20size%20can%20be%20significantly%20inaccurate%20when%20the%20task%20is%0Achallenging%20or%20when%20deployed%20on%20data%20from%20new%20distributions.%20We%20propose%20a%0Ahuman-in-the-loop%20approach%20for%20estimating%20population%20size%20driven%20by%20a%20pairwise%0Asimilarity%20derived%20from%20an%20off-the-shelf%20Re-ID%20system.%20Our%20approach%2C%20based%20on%0Anested%20importance%20sampling%2C%20selects%20pairs%20of%20images%20for%20human%20vetting%20driven%20by%0Athe%20pairwise%20similarity%2C%20and%20produces%20asymptotically%20unbiased%20population%20size%0Aestimates%20with%20associated%20confidence%20intervals.%20We%20perform%20experiments%20on%0Avarious%20animal%20Re-ID%20datasets%20and%20demonstrate%20that%20our%20method%20outperforms%0Astrong%20baselines%20and%20active%20clustering%20approaches.%20In%20many%20cases%2C%20we%20are%20able%0Ato%20reduce%20the%20error%20rates%20of%20the%20estimated%20size%20from%20around%2080%25%20using%20CV%20alone%0Ato%20less%20than%2020%25%20by%20vetting%20a%20fraction%20%28often%20less%20than%200.002%25%29%20of%20the%20total%0Apairs.%20The%20cost%20of%20vetting%20reduces%20with%20the%20increase%20in%20accuracy%20and%20provides%20a%0Apractical%20approach%20for%20population%20size%20estimation%20within%20a%20desired%20tolerance%0Awhen%20deploying%20Re-ID%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05287v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-in-the-Loop%2520Visual%2520Re-ID%2520for%2520Population%2520Size%2520Estimation%26entry.906535625%3DGustavo%2520Perez%2520and%2520Daniel%2520Sheldon%2520and%2520Grant%2520Van%2520Horn%2520and%2520Subhransu%2520Maji%26entry.1292438233%3D%2520%2520Computer%2520vision-based%2520re-identification%2520%2528Re-ID%2529%2520systems%2520are%2520increasingly%250Abeing%2520deployed%2520for%2520estimating%2520population%2520size%2520in%2520large%2520image%2520collections.%250AHowever%252C%2520the%2520estimated%2520size%2520can%2520be%2520significantly%2520inaccurate%2520when%2520the%2520task%2520is%250Achallenging%2520or%2520when%2520deployed%2520on%2520data%2520from%2520new%2520distributions.%2520We%2520propose%2520a%250Ahuman-in-the-loop%2520approach%2520for%2520estimating%2520population%2520size%2520driven%2520by%2520a%2520pairwise%250Asimilarity%2520derived%2520from%2520an%2520off-the-shelf%2520Re-ID%2520system.%2520Our%2520approach%252C%2520based%2520on%250Anested%2520importance%2520sampling%252C%2520selects%2520pairs%2520of%2520images%2520for%2520human%2520vetting%2520driven%2520by%250Athe%2520pairwise%2520similarity%252C%2520and%2520produces%2520asymptotically%2520unbiased%2520population%2520size%250Aestimates%2520with%2520associated%2520confidence%2520intervals.%2520We%2520perform%2520experiments%2520on%250Avarious%2520animal%2520Re-ID%2520datasets%2520and%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Astrong%2520baselines%2520and%2520active%2520clustering%2520approaches.%2520In%2520many%2520cases%252C%2520we%2520are%2520able%250Ato%2520reduce%2520the%2520error%2520rates%2520of%2520the%2520estimated%2520size%2520from%2520around%252080%2525%2520using%2520CV%2520alone%250Ato%2520less%2520than%252020%2525%2520by%2520vetting%2520a%2520fraction%2520%2528often%2520less%2520than%25200.002%2525%2529%2520of%2520the%2520total%250Apairs.%2520The%2520cost%2520of%2520vetting%2520reduces%2520with%2520the%2520increase%2520in%2520accuracy%2520and%2520provides%2520a%250Apractical%2520approach%2520for%2520population%2520size%2520estimation%2520within%2520a%2520desired%2520tolerance%250Awhen%2520deploying%2520Re-ID%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05287v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-in-the-Loop%20Visual%20Re-ID%20for%20Population%20Size%20Estimation&entry.906535625=Gustavo%20Perez%20and%20Daniel%20Sheldon%20and%20Grant%20Van%20Horn%20and%20Subhransu%20Maji&entry.1292438233=%20%20Computer%20vision-based%20re-identification%20%28Re-ID%29%20systems%20are%20increasingly%0Abeing%20deployed%20for%20estimating%20population%20size%20in%20large%20image%20collections.%0AHowever%2C%20the%20estimated%20size%20can%20be%20significantly%20inaccurate%20when%20the%20task%20is%0Achallenging%20or%20when%20deployed%20on%20data%20from%20new%20distributions.%20We%20propose%20a%0Ahuman-in-the-loop%20approach%20for%20estimating%20population%20size%20driven%20by%20a%20pairwise%0Asimilarity%20derived%20from%20an%20off-the-shelf%20Re-ID%20system.%20Our%20approach%2C%20based%20on%0Anested%20importance%20sampling%2C%20selects%20pairs%20of%20images%20for%20human%20vetting%20driven%20by%0Athe%20pairwise%20similarity%2C%20and%20produces%20asymptotically%20unbiased%20population%20size%0Aestimates%20with%20associated%20confidence%20intervals.%20We%20perform%20experiments%20on%0Avarious%20animal%20Re-ID%20datasets%20and%20demonstrate%20that%20our%20method%20outperforms%0Astrong%20baselines%20and%20active%20clustering%20approaches.%20In%20many%20cases%2C%20we%20are%20able%0Ato%20reduce%20the%20error%20rates%20of%20the%20estimated%20size%20from%20around%2080%25%20using%20CV%20alone%0Ato%20less%20than%2020%25%20by%20vetting%20a%20fraction%20%28often%20less%20than%200.002%25%29%20of%20the%20total%0Apairs.%20The%20cost%20of%20vetting%20reduces%20with%20the%20increase%20in%20accuracy%20and%20provides%20a%0Apractical%20approach%20for%20population%20size%20estimation%20within%20a%20desired%20tolerance%0Awhen%20deploying%20Re-ID%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05287v2&entry.124074799=Read"},
{"title": "InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models", "author": "Nirat Saini and Navaneeth Bodla and Ashish Shrivastava and Avinash Ravichandran and Xiao Zhang and Abhinav Shrivastava and Bharat Singh", "abstract": "  We introduce InVi, an approach for inserting or replacing objects within\nvideos (referred to as inpainting) using off-the-shelf, text-to-image latent\ndiffusion models. InVi targets controlled manipulation of objects and blending\nthem seamlessly into a background video unlike existing video editing methods\nthat focus on comprehensive re-styling or entire scene alterations. To achieve\nthis goal, we tackle two key challenges. Firstly, for high quality control and\nblending, we employ a two-step process involving inpainting and matching. This\nprocess begins with inserting the object into a single frame using a\nControlNet-based inpainting diffusion model, and then generating subsequent\nframes conditioned on features from an inpainted frame as an anchor to minimize\nthe domain gap between the background and the object. Secondly, to ensure\ntemporal coherence, we replace the diffusion model's self-attention layers with\nextended-attention layers. The anchor frame features serve as the keys and\nvalues for these layers, enhancing consistency across frames. Our approach\nremoves the need for video-specific fine-tuning, presenting an efficient and\nadaptable solution. Experimental results demonstrate that InVi achieves\nrealistic object insertion with consistent blending and coherence across\nframes, outperforming existing methods.\n", "link": "http://arxiv.org/abs/2407.10958v1", "date": "2024-07-15", "relevancy": 2.5345, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6582}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6387}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InVi%3A%20Object%20Insertion%20In%20Videos%20Using%20Off-the-Shelf%20Diffusion%20Models&body=Title%3A%20InVi%3A%20Object%20Insertion%20In%20Videos%20Using%20Off-the-Shelf%20Diffusion%20Models%0AAuthor%3A%20Nirat%20Saini%20and%20Navaneeth%20Bodla%20and%20Ashish%20Shrivastava%20and%20Avinash%20Ravichandran%20and%20Xiao%20Zhang%20and%20Abhinav%20Shrivastava%20and%20Bharat%20Singh%0AAbstract%3A%20%20%20We%20introduce%20InVi%2C%20an%20approach%20for%20inserting%20or%20replacing%20objects%20within%0Avideos%20%28referred%20to%20as%20inpainting%29%20using%20off-the-shelf%2C%20text-to-image%20latent%0Adiffusion%20models.%20InVi%20targets%20controlled%20manipulation%20of%20objects%20and%20blending%0Athem%20seamlessly%20into%20a%20background%20video%20unlike%20existing%20video%20editing%20methods%0Athat%20focus%20on%20comprehensive%20re-styling%20or%20entire%20scene%20alterations.%20To%20achieve%0Athis%20goal%2C%20we%20tackle%20two%20key%20challenges.%20Firstly%2C%20for%20high%20quality%20control%20and%0Ablending%2C%20we%20employ%20a%20two-step%20process%20involving%20inpainting%20and%20matching.%20This%0Aprocess%20begins%20with%20inserting%20the%20object%20into%20a%20single%20frame%20using%20a%0AControlNet-based%20inpainting%20diffusion%20model%2C%20and%20then%20generating%20subsequent%0Aframes%20conditioned%20on%20features%20from%20an%20inpainted%20frame%20as%20an%20anchor%20to%20minimize%0Athe%20domain%20gap%20between%20the%20background%20and%20the%20object.%20Secondly%2C%20to%20ensure%0Atemporal%20coherence%2C%20we%20replace%20the%20diffusion%20model%27s%20self-attention%20layers%20with%0Aextended-attention%20layers.%20The%20anchor%20frame%20features%20serve%20as%20the%20keys%20and%0Avalues%20for%20these%20layers%2C%20enhancing%20consistency%20across%20frames.%20Our%20approach%0Aremoves%20the%20need%20for%20video-specific%20fine-tuning%2C%20presenting%20an%20efficient%20and%0Aadaptable%20solution.%20Experimental%20results%20demonstrate%20that%20InVi%20achieves%0Arealistic%20object%20insertion%20with%20consistent%20blending%20and%20coherence%20across%0Aframes%2C%20outperforming%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInVi%253A%2520Object%2520Insertion%2520In%2520Videos%2520Using%2520Off-the-Shelf%2520Diffusion%2520Models%26entry.906535625%3DNirat%2520Saini%2520and%2520Navaneeth%2520Bodla%2520and%2520Ashish%2520Shrivastava%2520and%2520Avinash%2520Ravichandran%2520and%2520Xiao%2520Zhang%2520and%2520Abhinav%2520Shrivastava%2520and%2520Bharat%2520Singh%26entry.1292438233%3D%2520%2520We%2520introduce%2520InVi%252C%2520an%2520approach%2520for%2520inserting%2520or%2520replacing%2520objects%2520within%250Avideos%2520%2528referred%2520to%2520as%2520inpainting%2529%2520using%2520off-the-shelf%252C%2520text-to-image%2520latent%250Adiffusion%2520models.%2520InVi%2520targets%2520controlled%2520manipulation%2520of%2520objects%2520and%2520blending%250Athem%2520seamlessly%2520into%2520a%2520background%2520video%2520unlike%2520existing%2520video%2520editing%2520methods%250Athat%2520focus%2520on%2520comprehensive%2520re-styling%2520or%2520entire%2520scene%2520alterations.%2520To%2520achieve%250Athis%2520goal%252C%2520we%2520tackle%2520two%2520key%2520challenges.%2520Firstly%252C%2520for%2520high%2520quality%2520control%2520and%250Ablending%252C%2520we%2520employ%2520a%2520two-step%2520process%2520involving%2520inpainting%2520and%2520matching.%2520This%250Aprocess%2520begins%2520with%2520inserting%2520the%2520object%2520into%2520a%2520single%2520frame%2520using%2520a%250AControlNet-based%2520inpainting%2520diffusion%2520model%252C%2520and%2520then%2520generating%2520subsequent%250Aframes%2520conditioned%2520on%2520features%2520from%2520an%2520inpainted%2520frame%2520as%2520an%2520anchor%2520to%2520minimize%250Athe%2520domain%2520gap%2520between%2520the%2520background%2520and%2520the%2520object.%2520Secondly%252C%2520to%2520ensure%250Atemporal%2520coherence%252C%2520we%2520replace%2520the%2520diffusion%2520model%2527s%2520self-attention%2520layers%2520with%250Aextended-attention%2520layers.%2520The%2520anchor%2520frame%2520features%2520serve%2520as%2520the%2520keys%2520and%250Avalues%2520for%2520these%2520layers%252C%2520enhancing%2520consistency%2520across%2520frames.%2520Our%2520approach%250Aremoves%2520the%2520need%2520for%2520video-specific%2520fine-tuning%252C%2520presenting%2520an%2520efficient%2520and%250Aadaptable%2520solution.%2520Experimental%2520results%2520demonstrate%2520that%2520InVi%2520achieves%250Arealistic%2520object%2520insertion%2520with%2520consistent%2520blending%2520and%2520coherence%2520across%250Aframes%252C%2520outperforming%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InVi%3A%20Object%20Insertion%20In%20Videos%20Using%20Off-the-Shelf%20Diffusion%20Models&entry.906535625=Nirat%20Saini%20and%20Navaneeth%20Bodla%20and%20Ashish%20Shrivastava%20and%20Avinash%20Ravichandran%20and%20Xiao%20Zhang%20and%20Abhinav%20Shrivastava%20and%20Bharat%20Singh&entry.1292438233=%20%20We%20introduce%20InVi%2C%20an%20approach%20for%20inserting%20or%20replacing%20objects%20within%0Avideos%20%28referred%20to%20as%20inpainting%29%20using%20off-the-shelf%2C%20text-to-image%20latent%0Adiffusion%20models.%20InVi%20targets%20controlled%20manipulation%20of%20objects%20and%20blending%0Athem%20seamlessly%20into%20a%20background%20video%20unlike%20existing%20video%20editing%20methods%0Athat%20focus%20on%20comprehensive%20re-styling%20or%20entire%20scene%20alterations.%20To%20achieve%0Athis%20goal%2C%20we%20tackle%20two%20key%20challenges.%20Firstly%2C%20for%20high%20quality%20control%20and%0Ablending%2C%20we%20employ%20a%20two-step%20process%20involving%20inpainting%20and%20matching.%20This%0Aprocess%20begins%20with%20inserting%20the%20object%20into%20a%20single%20frame%20using%20a%0AControlNet-based%20inpainting%20diffusion%20model%2C%20and%20then%20generating%20subsequent%0Aframes%20conditioned%20on%20features%20from%20an%20inpainted%20frame%20as%20an%20anchor%20to%20minimize%0Athe%20domain%20gap%20between%20the%20background%20and%20the%20object.%20Secondly%2C%20to%20ensure%0Atemporal%20coherence%2C%20we%20replace%20the%20diffusion%20model%27s%20self-attention%20layers%20with%0Aextended-attention%20layers.%20The%20anchor%20frame%20features%20serve%20as%20the%20keys%20and%0Avalues%20for%20these%20layers%2C%20enhancing%20consistency%20across%20frames.%20Our%20approach%0Aremoves%20the%20need%20for%20video-specific%20fine-tuning%2C%20presenting%20an%20efficient%20and%0Aadaptable%20solution.%20Experimental%20results%20demonstrate%20that%20InVi%20achieves%0Arealistic%20object%20insertion%20with%20consistent%20blending%20and%20coherence%20across%0Aframes%2C%20outperforming%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10958v1&entry.124074799=Read"},
{"title": "Class-Incremental Learning: A Survey", "author": "Da-Wei Zhou and Qi-Wei Wang and Zhi-Hong Qi and Han-Jia Ye and De-Chuan Zhan and Ziwei Liu", "abstract": "  Deep models, e.g., CNNs and Vision Transformers, have achieved impressive\nachievements in many vision tasks in the closed world. However, novel classes\nemerge from time to time in our ever-changing world, requiring a learning\nsystem to acquire new knowledge continually. Class-Incremental Learning (CIL)\nenables the learner to incorporate the knowledge of new classes incrementally\nand build a universal classifier among all seen classes. Correspondingly, when\ndirectly training the model with new class instances, a fatal problem occurs --\nthe model tends to catastrophically forget the characteristics of former ones,\nand its performance drastically degrades. There have been numerous efforts to\ntackle catastrophic forgetting in the machine learning community. In this\npaper, we survey comprehensively recent advances in class-incremental learning\nand summarize these methods from several aspects. We also provide a rigorous\nand unified evaluation of 17 methods in benchmark image classification tasks to\nfind out the characteristics of different algorithms empirically. Furthermore,\nwe notice that the current comparison protocol ignores the influence of memory\nbudget in model storage, which may result in unfair comparison and biased\nresults. Hence, we advocate fair comparison by aligning the memory budget in\nevaluation, as well as several memory-agnostic performance measures. The source\ncode is available at https://github.com/zhoudw-zdw/CIL_Survey/\n", "link": "http://arxiv.org/abs/2302.03648v2", "date": "2024-07-15", "relevancy": 2.5291, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5139}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5076}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class-Incremental%20Learning%3A%20A%20Survey&body=Title%3A%20Class-Incremental%20Learning%3A%20A%20Survey%0AAuthor%3A%20Da-Wei%20Zhou%20and%20Qi-Wei%20Wang%20and%20Zhi-Hong%20Qi%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Deep%20models%2C%20e.g.%2C%20CNNs%20and%20Vision%20Transformers%2C%20have%20achieved%20impressive%0Aachievements%20in%20many%20vision%20tasks%20in%20the%20closed%20world.%20However%2C%20novel%20classes%0Aemerge%20from%20time%20to%20time%20in%20our%20ever-changing%20world%2C%20requiring%20a%20learning%0Asystem%20to%20acquire%20new%20knowledge%20continually.%20Class-Incremental%20Learning%20%28CIL%29%0Aenables%20the%20learner%20to%20incorporate%20the%20knowledge%20of%20new%20classes%20incrementally%0Aand%20build%20a%20universal%20classifier%20among%20all%20seen%20classes.%20Correspondingly%2C%20when%0Adirectly%20training%20the%20model%20with%20new%20class%20instances%2C%20a%20fatal%20problem%20occurs%20--%0Athe%20model%20tends%20to%20catastrophically%20forget%20the%20characteristics%20of%20former%20ones%2C%0Aand%20its%20performance%20drastically%20degrades.%20There%20have%20been%20numerous%20efforts%20to%0Atackle%20catastrophic%20forgetting%20in%20the%20machine%20learning%20community.%20In%20this%0Apaper%2C%20we%20survey%20comprehensively%20recent%20advances%20in%20class-incremental%20learning%0Aand%20summarize%20these%20methods%20from%20several%20aspects.%20We%20also%20provide%20a%20rigorous%0Aand%20unified%20evaluation%20of%2017%20methods%20in%20benchmark%20image%20classification%20tasks%20to%0Afind%20out%20the%20characteristics%20of%20different%20algorithms%20empirically.%20Furthermore%2C%0Awe%20notice%20that%20the%20current%20comparison%20protocol%20ignores%20the%20influence%20of%20memory%0Abudget%20in%20model%20storage%2C%20which%20may%20result%20in%20unfair%20comparison%20and%20biased%0Aresults.%20Hence%2C%20we%20advocate%20fair%20comparison%20by%20aligning%20the%20memory%20budget%20in%0Aevaluation%2C%20as%20well%20as%20several%20memory-agnostic%20performance%20measures.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/zhoudw-zdw/CIL_Survey/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03648v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass-Incremental%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DDa-Wei%2520Zhou%2520and%2520Qi-Wei%2520Wang%2520and%2520Zhi-Hong%2520Qi%2520and%2520Han-Jia%2520Ye%2520and%2520De-Chuan%2520Zhan%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Deep%2520models%252C%2520e.g.%252C%2520CNNs%2520and%2520Vision%2520Transformers%252C%2520have%2520achieved%2520impressive%250Aachievements%2520in%2520many%2520vision%2520tasks%2520in%2520the%2520closed%2520world.%2520However%252C%2520novel%2520classes%250Aemerge%2520from%2520time%2520to%2520time%2520in%2520our%2520ever-changing%2520world%252C%2520requiring%2520a%2520learning%250Asystem%2520to%2520acquire%2520new%2520knowledge%2520continually.%2520Class-Incremental%2520Learning%2520%2528CIL%2529%250Aenables%2520the%2520learner%2520to%2520incorporate%2520the%2520knowledge%2520of%2520new%2520classes%2520incrementally%250Aand%2520build%2520a%2520universal%2520classifier%2520among%2520all%2520seen%2520classes.%2520Correspondingly%252C%2520when%250Adirectly%2520training%2520the%2520model%2520with%2520new%2520class%2520instances%252C%2520a%2520fatal%2520problem%2520occurs%2520--%250Athe%2520model%2520tends%2520to%2520catastrophically%2520forget%2520the%2520characteristics%2520of%2520former%2520ones%252C%250Aand%2520its%2520performance%2520drastically%2520degrades.%2520There%2520have%2520been%2520numerous%2520efforts%2520to%250Atackle%2520catastrophic%2520forgetting%2520in%2520the%2520machine%2520learning%2520community.%2520In%2520this%250Apaper%252C%2520we%2520survey%2520comprehensively%2520recent%2520advances%2520in%2520class-incremental%2520learning%250Aand%2520summarize%2520these%2520methods%2520from%2520several%2520aspects.%2520We%2520also%2520provide%2520a%2520rigorous%250Aand%2520unified%2520evaluation%2520of%252017%2520methods%2520in%2520benchmark%2520image%2520classification%2520tasks%2520to%250Afind%2520out%2520the%2520characteristics%2520of%2520different%2520algorithms%2520empirically.%2520Furthermore%252C%250Awe%2520notice%2520that%2520the%2520current%2520comparison%2520protocol%2520ignores%2520the%2520influence%2520of%2520memory%250Abudget%2520in%2520model%2520storage%252C%2520which%2520may%2520result%2520in%2520unfair%2520comparison%2520and%2520biased%250Aresults.%2520Hence%252C%2520we%2520advocate%2520fair%2520comparison%2520by%2520aligning%2520the%2520memory%2520budget%2520in%250Aevaluation%252C%2520as%2520well%2520as%2520several%2520memory-agnostic%2520performance%2520measures.%2520The%2520source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/zhoudw-zdw/CIL_Survey/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.03648v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class-Incremental%20Learning%3A%20A%20Survey&entry.906535625=Da-Wei%20Zhou%20and%20Qi-Wei%20Wang%20and%20Zhi-Hong%20Qi%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%20and%20Ziwei%20Liu&entry.1292438233=%20%20Deep%20models%2C%20e.g.%2C%20CNNs%20and%20Vision%20Transformers%2C%20have%20achieved%20impressive%0Aachievements%20in%20many%20vision%20tasks%20in%20the%20closed%20world.%20However%2C%20novel%20classes%0Aemerge%20from%20time%20to%20time%20in%20our%20ever-changing%20world%2C%20requiring%20a%20learning%0Asystem%20to%20acquire%20new%20knowledge%20continually.%20Class-Incremental%20Learning%20%28CIL%29%0Aenables%20the%20learner%20to%20incorporate%20the%20knowledge%20of%20new%20classes%20incrementally%0Aand%20build%20a%20universal%20classifier%20among%20all%20seen%20classes.%20Correspondingly%2C%20when%0Adirectly%20training%20the%20model%20with%20new%20class%20instances%2C%20a%20fatal%20problem%20occurs%20--%0Athe%20model%20tends%20to%20catastrophically%20forget%20the%20characteristics%20of%20former%20ones%2C%0Aand%20its%20performance%20drastically%20degrades.%20There%20have%20been%20numerous%20efforts%20to%0Atackle%20catastrophic%20forgetting%20in%20the%20machine%20learning%20community.%20In%20this%0Apaper%2C%20we%20survey%20comprehensively%20recent%20advances%20in%20class-incremental%20learning%0Aand%20summarize%20these%20methods%20from%20several%20aspects.%20We%20also%20provide%20a%20rigorous%0Aand%20unified%20evaluation%20of%2017%20methods%20in%20benchmark%20image%20classification%20tasks%20to%0Afind%20out%20the%20characteristics%20of%20different%20algorithms%20empirically.%20Furthermore%2C%0Awe%20notice%20that%20the%20current%20comparison%20protocol%20ignores%20the%20influence%20of%20memory%0Abudget%20in%20model%20storage%2C%20which%20may%20result%20in%20unfair%20comparison%20and%20biased%0Aresults.%20Hence%2C%20we%20advocate%20fair%20comparison%20by%20aligning%20the%20memory%20budget%20in%0Aevaluation%2C%20as%20well%20as%20several%20memory-agnostic%20performance%20measures.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/zhoudw-zdw/CIL_Survey/%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03648v2&entry.124074799=Read"},
{"title": "InsertDiffusion: Identity Preserving Visualization of Objects through a\n  Training-Free Diffusion Architecture", "author": "Phillip Mueller and Jannik Wiese and Ioan Craciun and Lars Mikelsons", "abstract": "  Recent advancements in image synthesis are fueled by the advent of\nlarge-scale diffusion models. Yet, integrating realistic object visualizations\nseamlessly into new or existing backgrounds without extensive training remains\na challenge. This paper introduces InsertDiffusion, a novel, training-free\ndiffusion architecture that efficiently embeds objects into images while\npreserving their structural and identity characteristics. Our approach utilizes\noff-the-shelf generative models and eliminates the need for fine-tuning, making\nit ideal for rapid and adaptable visualizations in product design and\nmarketing. We demonstrate superior performance over existing methods in terms\nof image realism and alignment with input conditions. By decomposing the\ngeneration task into independent steps, InsertDiffusion offers a scalable\nsolution that extends the capabilities of diffusion models for practical\napplications, achieving high-quality visualizations that maintain the\nauthenticity of the original objects.\n", "link": "http://arxiv.org/abs/2407.10592v1", "date": "2024-07-15", "relevancy": 2.513, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.643}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6311}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InsertDiffusion%3A%20Identity%20Preserving%20Visualization%20of%20Objects%20through%20a%0A%20%20Training-Free%20Diffusion%20Architecture&body=Title%3A%20InsertDiffusion%3A%20Identity%20Preserving%20Visualization%20of%20Objects%20through%20a%0A%20%20Training-Free%20Diffusion%20Architecture%0AAuthor%3A%20Phillip%20Mueller%20and%20Jannik%20Wiese%20and%20Ioan%20Craciun%20and%20Lars%20Mikelsons%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image%20synthesis%20are%20fueled%20by%20the%20advent%20of%0Alarge-scale%20diffusion%20models.%20Yet%2C%20integrating%20realistic%20object%20visualizations%0Aseamlessly%20into%20new%20or%20existing%20backgrounds%20without%20extensive%20training%20remains%0Aa%20challenge.%20This%20paper%20introduces%20InsertDiffusion%2C%20a%20novel%2C%20training-free%0Adiffusion%20architecture%20that%20efficiently%20embeds%20objects%20into%20images%20while%0Apreserving%20their%20structural%20and%20identity%20characteristics.%20Our%20approach%20utilizes%0Aoff-the-shelf%20generative%20models%20and%20eliminates%20the%20need%20for%20fine-tuning%2C%20making%0Ait%20ideal%20for%20rapid%20and%20adaptable%20visualizations%20in%20product%20design%20and%0Amarketing.%20We%20demonstrate%20superior%20performance%20over%20existing%20methods%20in%20terms%0Aof%20image%20realism%20and%20alignment%20with%20input%20conditions.%20By%20decomposing%20the%0Ageneration%20task%20into%20independent%20steps%2C%20InsertDiffusion%20offers%20a%20scalable%0Asolution%20that%20extends%20the%20capabilities%20of%20diffusion%20models%20for%20practical%0Aapplications%2C%20achieving%20high-quality%20visualizations%20that%20maintain%20the%0Aauthenticity%20of%20the%20original%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsertDiffusion%253A%2520Identity%2520Preserving%2520Visualization%2520of%2520Objects%2520through%2520a%250A%2520%2520Training-Free%2520Diffusion%2520Architecture%26entry.906535625%3DPhillip%2520Mueller%2520and%2520Jannik%2520Wiese%2520and%2520Ioan%2520Craciun%2520and%2520Lars%2520Mikelsons%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image%2520synthesis%2520are%2520fueled%2520by%2520the%2520advent%2520of%250Alarge-scale%2520diffusion%2520models.%2520Yet%252C%2520integrating%2520realistic%2520object%2520visualizations%250Aseamlessly%2520into%2520new%2520or%2520existing%2520backgrounds%2520without%2520extensive%2520training%2520remains%250Aa%2520challenge.%2520This%2520paper%2520introduces%2520InsertDiffusion%252C%2520a%2520novel%252C%2520training-free%250Adiffusion%2520architecture%2520that%2520efficiently%2520embeds%2520objects%2520into%2520images%2520while%250Apreserving%2520their%2520structural%2520and%2520identity%2520characteristics.%2520Our%2520approach%2520utilizes%250Aoff-the-shelf%2520generative%2520models%2520and%2520eliminates%2520the%2520need%2520for%2520fine-tuning%252C%2520making%250Ait%2520ideal%2520for%2520rapid%2520and%2520adaptable%2520visualizations%2520in%2520product%2520design%2520and%250Amarketing.%2520We%2520demonstrate%2520superior%2520performance%2520over%2520existing%2520methods%2520in%2520terms%250Aof%2520image%2520realism%2520and%2520alignment%2520with%2520input%2520conditions.%2520By%2520decomposing%2520the%250Ageneration%2520task%2520into%2520independent%2520steps%252C%2520InsertDiffusion%2520offers%2520a%2520scalable%250Asolution%2520that%2520extends%2520the%2520capabilities%2520of%2520diffusion%2520models%2520for%2520practical%250Aapplications%252C%2520achieving%2520high-quality%2520visualizations%2520that%2520maintain%2520the%250Aauthenticity%2520of%2520the%2520original%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsertDiffusion%3A%20Identity%20Preserving%20Visualization%20of%20Objects%20through%20a%0A%20%20Training-Free%20Diffusion%20Architecture&entry.906535625=Phillip%20Mueller%20and%20Jannik%20Wiese%20and%20Ioan%20Craciun%20and%20Lars%20Mikelsons&entry.1292438233=%20%20Recent%20advancements%20in%20image%20synthesis%20are%20fueled%20by%20the%20advent%20of%0Alarge-scale%20diffusion%20models.%20Yet%2C%20integrating%20realistic%20object%20visualizations%0Aseamlessly%20into%20new%20or%20existing%20backgrounds%20without%20extensive%20training%20remains%0Aa%20challenge.%20This%20paper%20introduces%20InsertDiffusion%2C%20a%20novel%2C%20training-free%0Adiffusion%20architecture%20that%20efficiently%20embeds%20objects%20into%20images%20while%0Apreserving%20their%20structural%20and%20identity%20characteristics.%20Our%20approach%20utilizes%0Aoff-the-shelf%20generative%20models%20and%20eliminates%20the%20need%20for%20fine-tuning%2C%20making%0Ait%20ideal%20for%20rapid%20and%20adaptable%20visualizations%20in%20product%20design%20and%0Amarketing.%20We%20demonstrate%20superior%20performance%20over%20existing%20methods%20in%20terms%0Aof%20image%20realism%20and%20alignment%20with%20input%20conditions.%20By%20decomposing%20the%0Ageneration%20task%20into%20independent%20steps%2C%20InsertDiffusion%20offers%20a%20scalable%0Asolution%20that%20extends%20the%20capabilities%20of%20diffusion%20models%20for%20practical%0Aapplications%2C%20achieving%20high-quality%20visualizations%20that%20maintain%20the%0Aauthenticity%20of%20the%20original%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10592v1&entry.124074799=Read"},
{"title": "Category Adaptation Meets Projected Distillation in Generalized\n  Continual Category Discovery", "author": "Grzegorz Rype\u015b\u0107 and Daniel Marczak and Sebastian Cygert and Tomasz Trzci\u0144ski and Bart\u0142omiej Twardowski", "abstract": "  Generalized Continual Category Discovery (GCCD) tackles learning from\nsequentially arriving, partially labeled datasets while uncovering new\ncategories. Traditional methods depend on feature distillation to prevent\nforgetting the old knowledge. However, this strategy restricts the model's\nability to adapt and effectively distinguish new categories. To address this,\nwe introduce a novel technique integrating a learnable projector with feature\ndistillation, thus enhancing model adaptability without sacrificing past\nknowledge. The resulting distribution shift of the previously learned\ncategories is mitigated with the auxiliary category adaptation network. We\ndemonstrate that while each component offers modest benefits individually,\ntheir combination - dubbed CAMP (Category Adaptation Meets Projected\ndistillation) - significantly improves the balance between learning new\ninformation and retaining old. CAMP exhibits superior performance across\nseveral GCCD and Class Incremental Learning scenarios. The code is available at\nhttps://github.com/grypesc/CAMP.\n", "link": "http://arxiv.org/abs/2308.12112v3", "date": "2024-07-15", "relevancy": 2.5014, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4887}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category%20Adaptation%20Meets%20Projected%20Distillation%20in%20Generalized%0A%20%20Continual%20Category%20Discovery&body=Title%3A%20Category%20Adaptation%20Meets%20Projected%20Distillation%20in%20Generalized%0A%20%20Continual%20Category%20Discovery%0AAuthor%3A%20Grzegorz%20Rype%C5%9B%C4%87%20and%20Daniel%20Marczak%20and%20Sebastian%20Cygert%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski%0AAbstract%3A%20%20%20Generalized%20Continual%20Category%20Discovery%20%28GCCD%29%20tackles%20learning%20from%0Asequentially%20arriving%2C%20partially%20labeled%20datasets%20while%20uncovering%20new%0Acategories.%20Traditional%20methods%20depend%20on%20feature%20distillation%20to%20prevent%0Aforgetting%20the%20old%20knowledge.%20However%2C%20this%20strategy%20restricts%20the%20model%27s%0Aability%20to%20adapt%20and%20effectively%20distinguish%20new%20categories.%20To%20address%20this%2C%0Awe%20introduce%20a%20novel%20technique%20integrating%20a%20learnable%20projector%20with%20feature%0Adistillation%2C%20thus%20enhancing%20model%20adaptability%20without%20sacrificing%20past%0Aknowledge.%20The%20resulting%20distribution%20shift%20of%20the%20previously%20learned%0Acategories%20is%20mitigated%20with%20the%20auxiliary%20category%20adaptation%20network.%20We%0Ademonstrate%20that%20while%20each%20component%20offers%20modest%20benefits%20individually%2C%0Atheir%20combination%20-%20dubbed%20CAMP%20%28Category%20Adaptation%20Meets%20Projected%0Adistillation%29%20-%20significantly%20improves%20the%20balance%20between%20learning%20new%0Ainformation%20and%20retaining%20old.%20CAMP%20exhibits%20superior%20performance%20across%0Aseveral%20GCCD%20and%20Class%20Incremental%20Learning%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/grypesc/CAMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12112v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory%2520Adaptation%2520Meets%2520Projected%2520Distillation%2520in%2520Generalized%250A%2520%2520Continual%2520Category%2520Discovery%26entry.906535625%3DGrzegorz%2520Rype%25C5%259B%25C4%2587%2520and%2520Daniel%2520Marczak%2520and%2520Sebastian%2520Cygert%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Bart%25C5%2582omiej%2520Twardowski%26entry.1292438233%3D%2520%2520Generalized%2520Continual%2520Category%2520Discovery%2520%2528GCCD%2529%2520tackles%2520learning%2520from%250Asequentially%2520arriving%252C%2520partially%2520labeled%2520datasets%2520while%2520uncovering%2520new%250Acategories.%2520Traditional%2520methods%2520depend%2520on%2520feature%2520distillation%2520to%2520prevent%250Aforgetting%2520the%2520old%2520knowledge.%2520However%252C%2520this%2520strategy%2520restricts%2520the%2520model%2527s%250Aability%2520to%2520adapt%2520and%2520effectively%2520distinguish%2520new%2520categories.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520a%2520novel%2520technique%2520integrating%2520a%2520learnable%2520projector%2520with%2520feature%250Adistillation%252C%2520thus%2520enhancing%2520model%2520adaptability%2520without%2520sacrificing%2520past%250Aknowledge.%2520The%2520resulting%2520distribution%2520shift%2520of%2520the%2520previously%2520learned%250Acategories%2520is%2520mitigated%2520with%2520the%2520auxiliary%2520category%2520adaptation%2520network.%2520We%250Ademonstrate%2520that%2520while%2520each%2520component%2520offers%2520modest%2520benefits%2520individually%252C%250Atheir%2520combination%2520-%2520dubbed%2520CAMP%2520%2528Category%2520Adaptation%2520Meets%2520Projected%250Adistillation%2529%2520-%2520significantly%2520improves%2520the%2520balance%2520between%2520learning%2520new%250Ainformation%2520and%2520retaining%2520old.%2520CAMP%2520exhibits%2520superior%2520performance%2520across%250Aseveral%2520GCCD%2520and%2520Class%2520Incremental%2520Learning%2520scenarios.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/grypesc/CAMP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.12112v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category%20Adaptation%20Meets%20Projected%20Distillation%20in%20Generalized%0A%20%20Continual%20Category%20Discovery&entry.906535625=Grzegorz%20Rype%C5%9B%C4%87%20and%20Daniel%20Marczak%20and%20Sebastian%20Cygert%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski&entry.1292438233=%20%20Generalized%20Continual%20Category%20Discovery%20%28GCCD%29%20tackles%20learning%20from%0Asequentially%20arriving%2C%20partially%20labeled%20datasets%20while%20uncovering%20new%0Acategories.%20Traditional%20methods%20depend%20on%20feature%20distillation%20to%20prevent%0Aforgetting%20the%20old%20knowledge.%20However%2C%20this%20strategy%20restricts%20the%20model%27s%0Aability%20to%20adapt%20and%20effectively%20distinguish%20new%20categories.%20To%20address%20this%2C%0Awe%20introduce%20a%20novel%20technique%20integrating%20a%20learnable%20projector%20with%20feature%0Adistillation%2C%20thus%20enhancing%20model%20adaptability%20without%20sacrificing%20past%0Aknowledge.%20The%20resulting%20distribution%20shift%20of%20the%20previously%20learned%0Acategories%20is%20mitigated%20with%20the%20auxiliary%20category%20adaptation%20network.%20We%0Ademonstrate%20that%20while%20each%20component%20offers%20modest%20benefits%20individually%2C%0Atheir%20combination%20-%20dubbed%20CAMP%20%28Category%20Adaptation%20Meets%20Projected%0Adistillation%29%20-%20significantly%20improves%20the%20balance%20between%20learning%20new%0Ainformation%20and%20retaining%20old.%20CAMP%20exhibits%20superior%20performance%20across%0Aseveral%20GCCD%20and%20Class%20Incremental%20Learning%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/grypesc/CAMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12112v3&entry.124074799=Read"},
{"title": "A Dual-Attention Aware Deep Convolutional Neural Network for Early\n  Alzheimer's Detection", "author": "Pandiyaraju V and Shravan Venkatraman and Abeshek A and Aravintakshan S A and Pavan Kumar S and Kannan A", "abstract": "  Alzheimer's disease (AD) represents the primary form of neurodegeneration,\nimpacting millions of individuals each year and causing progressive cognitive\ndecline. Accurately diagnosing and classifying AD using neuroimaging data\npresents ongoing challenges in medicine, necessitating advanced interventions\nthat will enhance treatment measures. In this research, we introduce a dual\nattention enhanced deep learning (DL) framework for classifying AD from\nneuroimaging data. Combined spatial and self-attention mechanisms play a vital\nrole in emphasizing focus on neurofibrillary tangles and amyloid plaques from\nthe MRI images, which are difficult to discern with regular imaging techniques.\nResults demonstrate that our model yielded remarkable performance in comparison\nto existing state of the art (SOTA) convolutional neural networks (CNNs), with\nan accuracy of 99.1%. Moreover, it recorded remarkable metrics, with an\nF1-Score of 99.31%, a precision of 99.24%, and a recall of 99.5%. These results\nhighlight the promise of cutting edge DL methods in medical diagnostics,\ncontributing to highly reliable and more efficient healthcare solutions.\n", "link": "http://arxiv.org/abs/2407.10921v1", "date": "2024-07-15", "relevancy": 2.4991, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5117}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dual-Attention%20Aware%20Deep%20Convolutional%20Neural%20Network%20for%20Early%0A%20%20Alzheimer%27s%20Detection&body=Title%3A%20A%20Dual-Attention%20Aware%20Deep%20Convolutional%20Neural%20Network%20for%20Early%0A%20%20Alzheimer%27s%20Detection%0AAuthor%3A%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Aravintakshan%20S%20A%20and%20Pavan%20Kumar%20S%20and%20Kannan%20A%0AAbstract%3A%20%20%20Alzheimer%27s%20disease%20%28AD%29%20represents%20the%20primary%20form%20of%20neurodegeneration%2C%0Aimpacting%20millions%20of%20individuals%20each%20year%20and%20causing%20progressive%20cognitive%0Adecline.%20Accurately%20diagnosing%20and%20classifying%20AD%20using%20neuroimaging%20data%0Apresents%20ongoing%20challenges%20in%20medicine%2C%20necessitating%20advanced%20interventions%0Athat%20will%20enhance%20treatment%20measures.%20In%20this%20research%2C%20we%20introduce%20a%20dual%0Aattention%20enhanced%20deep%20learning%20%28DL%29%20framework%20for%20classifying%20AD%20from%0Aneuroimaging%20data.%20Combined%20spatial%20and%20self-attention%20mechanisms%20play%20a%20vital%0Arole%20in%20emphasizing%20focus%20on%20neurofibrillary%20tangles%20and%20amyloid%20plaques%20from%0Athe%20MRI%20images%2C%20which%20are%20difficult%20to%20discern%20with%20regular%20imaging%20techniques.%0AResults%20demonstrate%20that%20our%20model%20yielded%20remarkable%20performance%20in%20comparison%0Ato%20existing%20state%20of%20the%20art%20%28SOTA%29%20convolutional%20neural%20networks%20%28CNNs%29%2C%20with%0Aan%20accuracy%20of%2099.1%25.%20Moreover%2C%20it%20recorded%20remarkable%20metrics%2C%20with%20an%0AF1-Score%20of%2099.31%25%2C%20a%20precision%20of%2099.24%25%2C%20and%20a%20recall%20of%2099.5%25.%20These%20results%0Ahighlight%20the%20promise%20of%20cutting%20edge%20DL%20methods%20in%20medical%20diagnostics%2C%0Acontributing%20to%20highly%20reliable%20and%20more%20efficient%20healthcare%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dual-Attention%2520Aware%2520Deep%2520Convolutional%2520Neural%2520Network%2520for%2520Early%250A%2520%2520Alzheimer%2527s%2520Detection%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Abeshek%2520A%2520and%2520Aravintakshan%2520S%2520A%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Kannan%2520A%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520represents%2520the%2520primary%2520form%2520of%2520neurodegeneration%252C%250Aimpacting%2520millions%2520of%2520individuals%2520each%2520year%2520and%2520causing%2520progressive%2520cognitive%250Adecline.%2520Accurately%2520diagnosing%2520and%2520classifying%2520AD%2520using%2520neuroimaging%2520data%250Apresents%2520ongoing%2520challenges%2520in%2520medicine%252C%2520necessitating%2520advanced%2520interventions%250Athat%2520will%2520enhance%2520treatment%2520measures.%2520In%2520this%2520research%252C%2520we%2520introduce%2520a%2520dual%250Aattention%2520enhanced%2520deep%2520learning%2520%2528DL%2529%2520framework%2520for%2520classifying%2520AD%2520from%250Aneuroimaging%2520data.%2520Combined%2520spatial%2520and%2520self-attention%2520mechanisms%2520play%2520a%2520vital%250Arole%2520in%2520emphasizing%2520focus%2520on%2520neurofibrillary%2520tangles%2520and%2520amyloid%2520plaques%2520from%250Athe%2520MRI%2520images%252C%2520which%2520are%2520difficult%2520to%2520discern%2520with%2520regular%2520imaging%2520techniques.%250AResults%2520demonstrate%2520that%2520our%2520model%2520yielded%2520remarkable%2520performance%2520in%2520comparison%250Ato%2520existing%2520state%2520of%2520the%2520art%2520%2528SOTA%2529%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520with%250Aan%2520accuracy%2520of%252099.1%2525.%2520Moreover%252C%2520it%2520recorded%2520remarkable%2520metrics%252C%2520with%2520an%250AF1-Score%2520of%252099.31%2525%252C%2520a%2520precision%2520of%252099.24%2525%252C%2520and%2520a%2520recall%2520of%252099.5%2525.%2520These%2520results%250Ahighlight%2520the%2520promise%2520of%2520cutting%2520edge%2520DL%2520methods%2520in%2520medical%2520diagnostics%252C%250Acontributing%2520to%2520highly%2520reliable%2520and%2520more%2520efficient%2520healthcare%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dual-Attention%20Aware%20Deep%20Convolutional%20Neural%20Network%20for%20Early%0A%20%20Alzheimer%27s%20Detection&entry.906535625=Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Aravintakshan%20S%20A%20and%20Pavan%20Kumar%20S%20and%20Kannan%20A&entry.1292438233=%20%20Alzheimer%27s%20disease%20%28AD%29%20represents%20the%20primary%20form%20of%20neurodegeneration%2C%0Aimpacting%20millions%20of%20individuals%20each%20year%20and%20causing%20progressive%20cognitive%0Adecline.%20Accurately%20diagnosing%20and%20classifying%20AD%20using%20neuroimaging%20data%0Apresents%20ongoing%20challenges%20in%20medicine%2C%20necessitating%20advanced%20interventions%0Athat%20will%20enhance%20treatment%20measures.%20In%20this%20research%2C%20we%20introduce%20a%20dual%0Aattention%20enhanced%20deep%20learning%20%28DL%29%20framework%20for%20classifying%20AD%20from%0Aneuroimaging%20data.%20Combined%20spatial%20and%20self-attention%20mechanisms%20play%20a%20vital%0Arole%20in%20emphasizing%20focus%20on%20neurofibrillary%20tangles%20and%20amyloid%20plaques%20from%0Athe%20MRI%20images%2C%20which%20are%20difficult%20to%20discern%20with%20regular%20imaging%20techniques.%0AResults%20demonstrate%20that%20our%20model%20yielded%20remarkable%20performance%20in%20comparison%0Ato%20existing%20state%20of%20the%20art%20%28SOTA%29%20convolutional%20neural%20networks%20%28CNNs%29%2C%20with%0Aan%20accuracy%20of%2099.1%25.%20Moreover%2C%20it%20recorded%20remarkable%20metrics%2C%20with%20an%0AF1-Score%20of%2099.31%25%2C%20a%20precision%20of%2099.24%25%2C%20and%20a%20recall%20of%2099.5%25.%20These%20results%0Ahighlight%20the%20promise%20of%20cutting%20edge%20DL%20methods%20in%20medical%20diagnostics%2C%0Acontributing%20to%20highly%20reliable%20and%20more%20efficient%20healthcare%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10921v1&entry.124074799=Read"},
{"title": "Data-Guided Physics-Informed Neural Networks for Solving Inverse\n  Problems in Partial Differential Equations", "author": "Wei Zhou and Y. F. Xu", "abstract": "  Physics-informed neural networks (PINNs) represent a significant advancement\nin scientific machine learning by integrating fundamental physical laws into\ntheir architecture through loss functions. PINNs have been successfully applied\nto solve various forward and inverse problems in partial differential equations\n(PDEs). However, a notable challenge can emerge during the early training\nstages when solving inverse problems. Specifically, data losses remain high\nwhile PDE residual losses are minimized rapidly, thereby exacerbating the\nimbalance between loss terms and impeding the overall efficiency of PINNs. To\naddress this challenge, this study proposes a novel framework termed\ndata-guided physics-informed neural networks (DG-PINNs). The DG-PINNs framework\nis structured into two distinct phases: a pre-training phase and a fine-tuning\nphase. In the pre-training phase, a loss function with only the data loss is\nminimized in a neural network. In the fine-tuning phase, a composite loss\nfunction, which consists of the data loss, PDE residual loss, and, if\navailable, initial and boundary condition losses, is minimized in the same\nneural network. Notably, the pre-training phase ensures that the data loss is\nalready at a low value before the fine-tuning phase commences. This approach\nenables the fine-tuning phase to converge to a minimal composite loss function\nwith fewer iterations compared to existing PINNs. To validate the\neffectiveness, noise-robustness, and efficiency of DG-PINNs, extensive\nnumerical investigations are conducted on inverse problems related to several\nclassical PDEs, including the heat equation, wave equation, Euler--Bernoulli\nbeam equation, and Navier--Stokes equation. The numerical results demonstrate\nthat DG-PINNs can accurately solve these inverse problems and exhibit\nrobustness against noise in training data.\n", "link": "http://arxiv.org/abs/2407.10836v1", "date": "2024-07-15", "relevancy": 2.4905, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5138}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4941}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Guided%20Physics-Informed%20Neural%20Networks%20for%20Solving%20Inverse%0A%20%20Problems%20in%20Partial%20Differential%20Equations&body=Title%3A%20Data-Guided%20Physics-Informed%20Neural%20Networks%20for%20Solving%20Inverse%0A%20%20Problems%20in%20Partial%20Differential%20Equations%0AAuthor%3A%20Wei%20Zhou%20and%20Y.%20F.%20Xu%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20represent%20a%20significant%20advancement%0Ain%20scientific%20machine%20learning%20by%20integrating%20fundamental%20physical%20laws%20into%0Atheir%20architecture%20through%20loss%20functions.%20PINNs%20have%20been%20successfully%20applied%0Ato%20solve%20various%20forward%20and%20inverse%20problems%20in%20partial%20differential%20equations%0A%28PDEs%29.%20However%2C%20a%20notable%20challenge%20can%20emerge%20during%20the%20early%20training%0Astages%20when%20solving%20inverse%20problems.%20Specifically%2C%20data%20losses%20remain%20high%0Awhile%20PDE%20residual%20losses%20are%20minimized%20rapidly%2C%20thereby%20exacerbating%20the%0Aimbalance%20between%20loss%20terms%20and%20impeding%20the%20overall%20efficiency%20of%20PINNs.%20To%0Aaddress%20this%20challenge%2C%20this%20study%20proposes%20a%20novel%20framework%20termed%0Adata-guided%20physics-informed%20neural%20networks%20%28DG-PINNs%29.%20The%20DG-PINNs%20framework%0Ais%20structured%20into%20two%20distinct%20phases%3A%20a%20pre-training%20phase%20and%20a%20fine-tuning%0Aphase.%20In%20the%20pre-training%20phase%2C%20a%20loss%20function%20with%20only%20the%20data%20loss%20is%0Aminimized%20in%20a%20neural%20network.%20In%20the%20fine-tuning%20phase%2C%20a%20composite%20loss%0Afunction%2C%20which%20consists%20of%20the%20data%20loss%2C%20PDE%20residual%20loss%2C%20and%2C%20if%0Aavailable%2C%20initial%20and%20boundary%20condition%20losses%2C%20is%20minimized%20in%20the%20same%0Aneural%20network.%20Notably%2C%20the%20pre-training%20phase%20ensures%20that%20the%20data%20loss%20is%0Aalready%20at%20a%20low%20value%20before%20the%20fine-tuning%20phase%20commences.%20This%20approach%0Aenables%20the%20fine-tuning%20phase%20to%20converge%20to%20a%20minimal%20composite%20loss%20function%0Awith%20fewer%20iterations%20compared%20to%20existing%20PINNs.%20To%20validate%20the%0Aeffectiveness%2C%20noise-robustness%2C%20and%20efficiency%20of%20DG-PINNs%2C%20extensive%0Anumerical%20investigations%20are%20conducted%20on%20inverse%20problems%20related%20to%20several%0Aclassical%20PDEs%2C%20including%20the%20heat%20equation%2C%20wave%20equation%2C%20Euler--Bernoulli%0Abeam%20equation%2C%20and%20Navier--Stokes%20equation.%20The%20numerical%20results%20demonstrate%0Athat%20DG-PINNs%20can%20accurately%20solve%20these%20inverse%20problems%20and%20exhibit%0Arobustness%20against%20noise%20in%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Guided%2520Physics-Informed%2520Neural%2520Networks%2520for%2520Solving%2520Inverse%250A%2520%2520Problems%2520in%2520Partial%2520Differential%2520Equations%26entry.906535625%3DWei%2520Zhou%2520and%2520Y.%2520F.%2520Xu%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520represent%2520a%2520significant%2520advancement%250Ain%2520scientific%2520machine%2520learning%2520by%2520integrating%2520fundamental%2520physical%2520laws%2520into%250Atheir%2520architecture%2520through%2520loss%2520functions.%2520PINNs%2520have%2520been%2520successfully%2520applied%250Ato%2520solve%2520various%2520forward%2520and%2520inverse%2520problems%2520in%2520partial%2520differential%2520equations%250A%2528PDEs%2529.%2520However%252C%2520a%2520notable%2520challenge%2520can%2520emerge%2520during%2520the%2520early%2520training%250Astages%2520when%2520solving%2520inverse%2520problems.%2520Specifically%252C%2520data%2520losses%2520remain%2520high%250Awhile%2520PDE%2520residual%2520losses%2520are%2520minimized%2520rapidly%252C%2520thereby%2520exacerbating%2520the%250Aimbalance%2520between%2520loss%2520terms%2520and%2520impeding%2520the%2520overall%2520efficiency%2520of%2520PINNs.%2520To%250Aaddress%2520this%2520challenge%252C%2520this%2520study%2520proposes%2520a%2520novel%2520framework%2520termed%250Adata-guided%2520physics-informed%2520neural%2520networks%2520%2528DG-PINNs%2529.%2520The%2520DG-PINNs%2520framework%250Ais%2520structured%2520into%2520two%2520distinct%2520phases%253A%2520a%2520pre-training%2520phase%2520and%2520a%2520fine-tuning%250Aphase.%2520In%2520the%2520pre-training%2520phase%252C%2520a%2520loss%2520function%2520with%2520only%2520the%2520data%2520loss%2520is%250Aminimized%2520in%2520a%2520neural%2520network.%2520In%2520the%2520fine-tuning%2520phase%252C%2520a%2520composite%2520loss%250Afunction%252C%2520which%2520consists%2520of%2520the%2520data%2520loss%252C%2520PDE%2520residual%2520loss%252C%2520and%252C%2520if%250Aavailable%252C%2520initial%2520and%2520boundary%2520condition%2520losses%252C%2520is%2520minimized%2520in%2520the%2520same%250Aneural%2520network.%2520Notably%252C%2520the%2520pre-training%2520phase%2520ensures%2520that%2520the%2520data%2520loss%2520is%250Aalready%2520at%2520a%2520low%2520value%2520before%2520the%2520fine-tuning%2520phase%2520commences.%2520This%2520approach%250Aenables%2520the%2520fine-tuning%2520phase%2520to%2520converge%2520to%2520a%2520minimal%2520composite%2520loss%2520function%250Awith%2520fewer%2520iterations%2520compared%2520to%2520existing%2520PINNs.%2520To%2520validate%2520the%250Aeffectiveness%252C%2520noise-robustness%252C%2520and%2520efficiency%2520of%2520DG-PINNs%252C%2520extensive%250Anumerical%2520investigations%2520are%2520conducted%2520on%2520inverse%2520problems%2520related%2520to%2520several%250Aclassical%2520PDEs%252C%2520including%2520the%2520heat%2520equation%252C%2520wave%2520equation%252C%2520Euler--Bernoulli%250Abeam%2520equation%252C%2520and%2520Navier--Stokes%2520equation.%2520The%2520numerical%2520results%2520demonstrate%250Athat%2520DG-PINNs%2520can%2520accurately%2520solve%2520these%2520inverse%2520problems%2520and%2520exhibit%250Arobustness%2520against%2520noise%2520in%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Guided%20Physics-Informed%20Neural%20Networks%20for%20Solving%20Inverse%0A%20%20Problems%20in%20Partial%20Differential%20Equations&entry.906535625=Wei%20Zhou%20and%20Y.%20F.%20Xu&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20represent%20a%20significant%20advancement%0Ain%20scientific%20machine%20learning%20by%20integrating%20fundamental%20physical%20laws%20into%0Atheir%20architecture%20through%20loss%20functions.%20PINNs%20have%20been%20successfully%20applied%0Ato%20solve%20various%20forward%20and%20inverse%20problems%20in%20partial%20differential%20equations%0A%28PDEs%29.%20However%2C%20a%20notable%20challenge%20can%20emerge%20during%20the%20early%20training%0Astages%20when%20solving%20inverse%20problems.%20Specifically%2C%20data%20losses%20remain%20high%0Awhile%20PDE%20residual%20losses%20are%20minimized%20rapidly%2C%20thereby%20exacerbating%20the%0Aimbalance%20between%20loss%20terms%20and%20impeding%20the%20overall%20efficiency%20of%20PINNs.%20To%0Aaddress%20this%20challenge%2C%20this%20study%20proposes%20a%20novel%20framework%20termed%0Adata-guided%20physics-informed%20neural%20networks%20%28DG-PINNs%29.%20The%20DG-PINNs%20framework%0Ais%20structured%20into%20two%20distinct%20phases%3A%20a%20pre-training%20phase%20and%20a%20fine-tuning%0Aphase.%20In%20the%20pre-training%20phase%2C%20a%20loss%20function%20with%20only%20the%20data%20loss%20is%0Aminimized%20in%20a%20neural%20network.%20In%20the%20fine-tuning%20phase%2C%20a%20composite%20loss%0Afunction%2C%20which%20consists%20of%20the%20data%20loss%2C%20PDE%20residual%20loss%2C%20and%2C%20if%0Aavailable%2C%20initial%20and%20boundary%20condition%20losses%2C%20is%20minimized%20in%20the%20same%0Aneural%20network.%20Notably%2C%20the%20pre-training%20phase%20ensures%20that%20the%20data%20loss%20is%0Aalready%20at%20a%20low%20value%20before%20the%20fine-tuning%20phase%20commences.%20This%20approach%0Aenables%20the%20fine-tuning%20phase%20to%20converge%20to%20a%20minimal%20composite%20loss%20function%0Awith%20fewer%20iterations%20compared%20to%20existing%20PINNs.%20To%20validate%20the%0Aeffectiveness%2C%20noise-robustness%2C%20and%20efficiency%20of%20DG-PINNs%2C%20extensive%0Anumerical%20investigations%20are%20conducted%20on%20inverse%20problems%20related%20to%20several%0Aclassical%20PDEs%2C%20including%20the%20heat%20equation%2C%20wave%20equation%2C%20Euler--Bernoulli%0Abeam%20equation%2C%20and%20Navier--Stokes%20equation.%20The%20numerical%20results%20demonstrate%0Athat%20DG-PINNs%20can%20accurately%20solve%20these%20inverse%20problems%20and%20exhibit%0Arobustness%20against%20noise%20in%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10836v1&entry.124074799=Read"},
{"title": "R3D-AD: Reconstruction via Diffusion for 3D Anomaly Detection", "author": "Zheyuan Zhou and Le Wang and Naiyu Fang and Zili Wang and Lemiao Qiu and Shuyou Zhang", "abstract": "  3D anomaly detection plays a crucial role in monitoring parts for localized\ninherent defects in precision manufacturing. Embedding-based and\nreconstruction-based approaches are among the most popular and successful\nmethods. However, there are two major challenges to the practical application\nof the current approaches: 1) the embedded models suffer the prohibitive\ncomputational and storage due to the memory bank structure; 2) the\nreconstructive models based on the MAE mechanism fail to detect anomalies in\nthe unmasked regions. In this paper, we propose R3D-AD, reconstructing\nanomalous point clouds by diffusion model for precise 3D anomaly detection. Our\napproach capitalizes on the data distribution conversion of the diffusion\nprocess to entirely obscure the input's anomalous geometry. It step-wisely\nlearns a strict point-level displacement behavior, which methodically corrects\nthe aberrant points. To increase the generalization of the model, we further\npresent a novel 3D anomaly simulation strategy named Patch-Gen to generate\nrealistic and diverse defect shapes, which narrows the domain gap between\ntraining and testing. Our R3D-AD ensures a uniform spatial transformation,\nwhich allows straightforwardly generating anomaly results by distance\ncomparison. Extensive experiments show that our R3D-AD outperforms previous\nstate-of-the-art methods, achieving 73.4% Image-level AUROC on the Real3D-AD\ndataset and 74.9% Image-level AUROC on the Anomaly-ShapeNet dataset with an\nexceptional efficiency.\n", "link": "http://arxiv.org/abs/2407.10862v1", "date": "2024-07-15", "relevancy": 2.4902, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6288}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6288}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R3D-AD%3A%20Reconstruction%20via%20Diffusion%20for%203D%20Anomaly%20Detection&body=Title%3A%20R3D-AD%3A%20Reconstruction%20via%20Diffusion%20for%203D%20Anomaly%20Detection%0AAuthor%3A%20Zheyuan%20Zhou%20and%20Le%20Wang%20and%20Naiyu%20Fang%20and%20Zili%20Wang%20and%20Lemiao%20Qiu%20and%20Shuyou%20Zhang%0AAbstract%3A%20%20%203D%20anomaly%20detection%20plays%20a%20crucial%20role%20in%20monitoring%20parts%20for%20localized%0Ainherent%20defects%20in%20precision%20manufacturing.%20Embedding-based%20and%0Areconstruction-based%20approaches%20are%20among%20the%20most%20popular%20and%20successful%0Amethods.%20However%2C%20there%20are%20two%20major%20challenges%20to%20the%20practical%20application%0Aof%20the%20current%20approaches%3A%201%29%20the%20embedded%20models%20suffer%20the%20prohibitive%0Acomputational%20and%20storage%20due%20to%20the%20memory%20bank%20structure%3B%202%29%20the%0Areconstructive%20models%20based%20on%20the%20MAE%20mechanism%20fail%20to%20detect%20anomalies%20in%0Athe%20unmasked%20regions.%20In%20this%20paper%2C%20we%20propose%20R3D-AD%2C%20reconstructing%0Aanomalous%20point%20clouds%20by%20diffusion%20model%20for%20precise%203D%20anomaly%20detection.%20Our%0Aapproach%20capitalizes%20on%20the%20data%20distribution%20conversion%20of%20the%20diffusion%0Aprocess%20to%20entirely%20obscure%20the%20input%27s%20anomalous%20geometry.%20It%20step-wisely%0Alearns%20a%20strict%20point-level%20displacement%20behavior%2C%20which%20methodically%20corrects%0Athe%20aberrant%20points.%20To%20increase%20the%20generalization%20of%20the%20model%2C%20we%20further%0Apresent%20a%20novel%203D%20anomaly%20simulation%20strategy%20named%20Patch-Gen%20to%20generate%0Arealistic%20and%20diverse%20defect%20shapes%2C%20which%20narrows%20the%20domain%20gap%20between%0Atraining%20and%20testing.%20Our%20R3D-AD%20ensures%20a%20uniform%20spatial%20transformation%2C%0Awhich%20allows%20straightforwardly%20generating%20anomaly%20results%20by%20distance%0Acomparison.%20Extensive%20experiments%20show%20that%20our%20R3D-AD%20outperforms%20previous%0Astate-of-the-art%20methods%2C%20achieving%2073.4%25%20Image-level%20AUROC%20on%20the%20Real3D-AD%0Adataset%20and%2074.9%25%20Image-level%20AUROC%20on%20the%20Anomaly-ShapeNet%20dataset%20with%20an%0Aexceptional%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR3D-AD%253A%2520Reconstruction%2520via%2520Diffusion%2520for%25203D%2520Anomaly%2520Detection%26entry.906535625%3DZheyuan%2520Zhou%2520and%2520Le%2520Wang%2520and%2520Naiyu%2520Fang%2520and%2520Zili%2520Wang%2520and%2520Lemiao%2520Qiu%2520and%2520Shuyou%2520Zhang%26entry.1292438233%3D%2520%25203D%2520anomaly%2520detection%2520plays%2520a%2520crucial%2520role%2520in%2520monitoring%2520parts%2520for%2520localized%250Ainherent%2520defects%2520in%2520precision%2520manufacturing.%2520Embedding-based%2520and%250Areconstruction-based%2520approaches%2520are%2520among%2520the%2520most%2520popular%2520and%2520successful%250Amethods.%2520However%252C%2520there%2520are%2520two%2520major%2520challenges%2520to%2520the%2520practical%2520application%250Aof%2520the%2520current%2520approaches%253A%25201%2529%2520the%2520embedded%2520models%2520suffer%2520the%2520prohibitive%250Acomputational%2520and%2520storage%2520due%2520to%2520the%2520memory%2520bank%2520structure%253B%25202%2529%2520the%250Areconstructive%2520models%2520based%2520on%2520the%2520MAE%2520mechanism%2520fail%2520to%2520detect%2520anomalies%2520in%250Athe%2520unmasked%2520regions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520R3D-AD%252C%2520reconstructing%250Aanomalous%2520point%2520clouds%2520by%2520diffusion%2520model%2520for%2520precise%25203D%2520anomaly%2520detection.%2520Our%250Aapproach%2520capitalizes%2520on%2520the%2520data%2520distribution%2520conversion%2520of%2520the%2520diffusion%250Aprocess%2520to%2520entirely%2520obscure%2520the%2520input%2527s%2520anomalous%2520geometry.%2520It%2520step-wisely%250Alearns%2520a%2520strict%2520point-level%2520displacement%2520behavior%252C%2520which%2520methodically%2520corrects%250Athe%2520aberrant%2520points.%2520To%2520increase%2520the%2520generalization%2520of%2520the%2520model%252C%2520we%2520further%250Apresent%2520a%2520novel%25203D%2520anomaly%2520simulation%2520strategy%2520named%2520Patch-Gen%2520to%2520generate%250Arealistic%2520and%2520diverse%2520defect%2520shapes%252C%2520which%2520narrows%2520the%2520domain%2520gap%2520between%250Atraining%2520and%2520testing.%2520Our%2520R3D-AD%2520ensures%2520a%2520uniform%2520spatial%2520transformation%252C%250Awhich%2520allows%2520straightforwardly%2520generating%2520anomaly%2520results%2520by%2520distance%250Acomparison.%2520Extensive%2520experiments%2520show%2520that%2520our%2520R3D-AD%2520outperforms%2520previous%250Astate-of-the-art%2520methods%252C%2520achieving%252073.4%2525%2520Image-level%2520AUROC%2520on%2520the%2520Real3D-AD%250Adataset%2520and%252074.9%2525%2520Image-level%2520AUROC%2520on%2520the%2520Anomaly-ShapeNet%2520dataset%2520with%2520an%250Aexceptional%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R3D-AD%3A%20Reconstruction%20via%20Diffusion%20for%203D%20Anomaly%20Detection&entry.906535625=Zheyuan%20Zhou%20and%20Le%20Wang%20and%20Naiyu%20Fang%20and%20Zili%20Wang%20and%20Lemiao%20Qiu%20and%20Shuyou%20Zhang&entry.1292438233=%20%203D%20anomaly%20detection%20plays%20a%20crucial%20role%20in%20monitoring%20parts%20for%20localized%0Ainherent%20defects%20in%20precision%20manufacturing.%20Embedding-based%20and%0Areconstruction-based%20approaches%20are%20among%20the%20most%20popular%20and%20successful%0Amethods.%20However%2C%20there%20are%20two%20major%20challenges%20to%20the%20practical%20application%0Aof%20the%20current%20approaches%3A%201%29%20the%20embedded%20models%20suffer%20the%20prohibitive%0Acomputational%20and%20storage%20due%20to%20the%20memory%20bank%20structure%3B%202%29%20the%0Areconstructive%20models%20based%20on%20the%20MAE%20mechanism%20fail%20to%20detect%20anomalies%20in%0Athe%20unmasked%20regions.%20In%20this%20paper%2C%20we%20propose%20R3D-AD%2C%20reconstructing%0Aanomalous%20point%20clouds%20by%20diffusion%20model%20for%20precise%203D%20anomaly%20detection.%20Our%0Aapproach%20capitalizes%20on%20the%20data%20distribution%20conversion%20of%20the%20diffusion%0Aprocess%20to%20entirely%20obscure%20the%20input%27s%20anomalous%20geometry.%20It%20step-wisely%0Alearns%20a%20strict%20point-level%20displacement%20behavior%2C%20which%20methodically%20corrects%0Athe%20aberrant%20points.%20To%20increase%20the%20generalization%20of%20the%20model%2C%20we%20further%0Apresent%20a%20novel%203D%20anomaly%20simulation%20strategy%20named%20Patch-Gen%20to%20generate%0Arealistic%20and%20diverse%20defect%20shapes%2C%20which%20narrows%20the%20domain%20gap%20between%0Atraining%20and%20testing.%20Our%20R3D-AD%20ensures%20a%20uniform%20spatial%20transformation%2C%0Awhich%20allows%20straightforwardly%20generating%20anomaly%20results%20by%20distance%0Acomparison.%20Extensive%20experiments%20show%20that%20our%20R3D-AD%20outperforms%20previous%0Astate-of-the-art%20methods%2C%20achieving%2073.4%25%20Image-level%20AUROC%20on%20the%20Real3D-AD%0Adataset%20and%2074.9%25%20Image-level%20AUROC%20on%20the%20Anomaly-ShapeNet%20dataset%20with%20an%0Aexceptional%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10862v1&entry.124074799=Read"},
{"title": "Any-Property-Conditional Molecule Generation with Self-Criticism using\n  Spanning Trees", "author": "Alexia Jolicoeur-Martineau and Aristide Baratin and Kisoo Kwon and Boris Knyazev and Yan Zhang", "abstract": "  Generating novel molecules is challenging, with most representations leading\nto generative models producing many invalid molecules. Spanning Tree-based\nGraph Generation (STGG) is a promising approach to ensure the generation of\nvalid molecules, outperforming state-of-the-art SMILES and graph diffusion\nmodels for unconditional generation. In the real world, we want to be able to\ngenerate molecules conditional on one or multiple desired properties rather\nthan unconditionally. Thus, in this work, we extend STGG to\nmulti-property-conditional generation. Our approach, STGG+, incorporates a\nmodern Transformer architecture, random masking of properties during training\n(enabling conditioning on any subset of properties and classifier-free\nguidance), an auxiliary property-prediction loss (allowing the model to\nself-criticize molecules and select the best ones), and other improvements. We\nshow that STGG+ achieves state-of-the-art performance on in-distribution and\nout-of-distribution conditional generation, and reward maximization.\n", "link": "http://arxiv.org/abs/2407.09357v2", "date": "2024-07-15", "relevancy": 2.4875, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.512}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4936}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any-Property-Conditional%20Molecule%20Generation%20with%20Self-Criticism%20using%0A%20%20Spanning%20Trees&body=Title%3A%20Any-Property-Conditional%20Molecule%20Generation%20with%20Self-Criticism%20using%0A%20%20Spanning%20Trees%0AAuthor%3A%20Alexia%20Jolicoeur-Martineau%20and%20Aristide%20Baratin%20and%20Kisoo%20Kwon%20and%20Boris%20Knyazev%20and%20Yan%20Zhang%0AAbstract%3A%20%20%20Generating%20novel%20molecules%20is%20challenging%2C%20with%20most%20representations%20leading%0Ato%20generative%20models%20producing%20many%20invalid%20molecules.%20Spanning%20Tree-based%0AGraph%20Generation%20%28STGG%29%20is%20a%20promising%20approach%20to%20ensure%20the%20generation%20of%0Avalid%20molecules%2C%20outperforming%20state-of-the-art%20SMILES%20and%20graph%20diffusion%0Amodels%20for%20unconditional%20generation.%20In%20the%20real%20world%2C%20we%20want%20to%20be%20able%20to%0Agenerate%20molecules%20conditional%20on%20one%20or%20multiple%20desired%20properties%20rather%0Athan%20unconditionally.%20Thus%2C%20in%20this%20work%2C%20we%20extend%20STGG%20to%0Amulti-property-conditional%20generation.%20Our%20approach%2C%20STGG%2B%2C%20incorporates%20a%0Amodern%20Transformer%20architecture%2C%20random%20masking%20of%20properties%20during%20training%0A%28enabling%20conditioning%20on%20any%20subset%20of%20properties%20and%20classifier-free%0Aguidance%29%2C%20an%20auxiliary%20property-prediction%20loss%20%28allowing%20the%20model%20to%0Aself-criticize%20molecules%20and%20select%20the%20best%20ones%29%2C%20and%20other%20improvements.%20We%0Ashow%20that%20STGG%2B%20achieves%20state-of-the-art%20performance%20on%20in-distribution%20and%0Aout-of-distribution%20conditional%20generation%2C%20and%20reward%20maximization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09357v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny-Property-Conditional%2520Molecule%2520Generation%2520with%2520Self-Criticism%2520using%250A%2520%2520Spanning%2520Trees%26entry.906535625%3DAlexia%2520Jolicoeur-Martineau%2520and%2520Aristide%2520Baratin%2520and%2520Kisoo%2520Kwon%2520and%2520Boris%2520Knyazev%2520and%2520Yan%2520Zhang%26entry.1292438233%3D%2520%2520Generating%2520novel%2520molecules%2520is%2520challenging%252C%2520with%2520most%2520representations%2520leading%250Ato%2520generative%2520models%2520producing%2520many%2520invalid%2520molecules.%2520Spanning%2520Tree-based%250AGraph%2520Generation%2520%2528STGG%2529%2520is%2520a%2520promising%2520approach%2520to%2520ensure%2520the%2520generation%2520of%250Avalid%2520molecules%252C%2520outperforming%2520state-of-the-art%2520SMILES%2520and%2520graph%2520diffusion%250Amodels%2520for%2520unconditional%2520generation.%2520In%2520the%2520real%2520world%252C%2520we%2520want%2520to%2520be%2520able%2520to%250Agenerate%2520molecules%2520conditional%2520on%2520one%2520or%2520multiple%2520desired%2520properties%2520rather%250Athan%2520unconditionally.%2520Thus%252C%2520in%2520this%2520work%252C%2520we%2520extend%2520STGG%2520to%250Amulti-property-conditional%2520generation.%2520Our%2520approach%252C%2520STGG%252B%252C%2520incorporates%2520a%250Amodern%2520Transformer%2520architecture%252C%2520random%2520masking%2520of%2520properties%2520during%2520training%250A%2528enabling%2520conditioning%2520on%2520any%2520subset%2520of%2520properties%2520and%2520classifier-free%250Aguidance%2529%252C%2520an%2520auxiliary%2520property-prediction%2520loss%2520%2528allowing%2520the%2520model%2520to%250Aself-criticize%2520molecules%2520and%2520select%2520the%2520best%2520ones%2529%252C%2520and%2520other%2520improvements.%2520We%250Ashow%2520that%2520STGG%252B%2520achieves%2520state-of-the-art%2520performance%2520on%2520in-distribution%2520and%250Aout-of-distribution%2520conditional%2520generation%252C%2520and%2520reward%2520maximization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09357v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any-Property-Conditional%20Molecule%20Generation%20with%20Self-Criticism%20using%0A%20%20Spanning%20Trees&entry.906535625=Alexia%20Jolicoeur-Martineau%20and%20Aristide%20Baratin%20and%20Kisoo%20Kwon%20and%20Boris%20Knyazev%20and%20Yan%20Zhang&entry.1292438233=%20%20Generating%20novel%20molecules%20is%20challenging%2C%20with%20most%20representations%20leading%0Ato%20generative%20models%20producing%20many%20invalid%20molecules.%20Spanning%20Tree-based%0AGraph%20Generation%20%28STGG%29%20is%20a%20promising%20approach%20to%20ensure%20the%20generation%20of%0Avalid%20molecules%2C%20outperforming%20state-of-the-art%20SMILES%20and%20graph%20diffusion%0Amodels%20for%20unconditional%20generation.%20In%20the%20real%20world%2C%20we%20want%20to%20be%20able%20to%0Agenerate%20molecules%20conditional%20on%20one%20or%20multiple%20desired%20properties%20rather%0Athan%20unconditionally.%20Thus%2C%20in%20this%20work%2C%20we%20extend%20STGG%20to%0Amulti-property-conditional%20generation.%20Our%20approach%2C%20STGG%2B%2C%20incorporates%20a%0Amodern%20Transformer%20architecture%2C%20random%20masking%20of%20properties%20during%20training%0A%28enabling%20conditioning%20on%20any%20subset%20of%20properties%20and%20classifier-free%0Aguidance%29%2C%20an%20auxiliary%20property-prediction%20loss%20%28allowing%20the%20model%20to%0Aself-criticize%20molecules%20and%20select%20the%20best%20ones%29%2C%20and%20other%20improvements.%20We%0Ashow%20that%20STGG%2B%20achieves%20state-of-the-art%20performance%20on%20in-distribution%20and%0Aout-of-distribution%20conditional%20generation%2C%20and%20reward%20maximization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09357v2&entry.124074799=Read"},
{"title": "Probability Passing for Graph Neural Networks: Graph Structure and\n  Representations Joint Learning", "author": "Ziyan Wang and YaXuan He and Bin Liu", "abstract": "  Graph Neural Networks (GNNs) have achieved notable success in the analysis of\nnon-Euclidean data across a wide range of domains. However, their applicability\nis constrained by the dependence on the observed graph structure. To solve this\nproblem, Latent Graph Inference (LGI) is proposed to infer a task-specific\nlatent structure by computing similarity or edge probability of node features\nand then apply a GNN to produce predictions. Even so, existing approaches\nneglect the noise from node features, which affects generated graph structure\nand performance. In this work, we introduce a novel method called Probability\nPassing to refine the generated graph structure by aggregating edge\nprobabilities of neighboring nodes based on observed graph. Furthermore, we\ncontinue to utilize the LGI framework, inputting the refined graph structure\nand node features into GNNs to obtain predictions. We name the proposed scheme\nas Probability Passing-based Graph Neural Network (PPGNN). Moreover, the\nanchor-based technique is employed to reduce complexity and improve efficiency.\nExperimental results demonstrate the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2407.10688v1", "date": "2024-07-15", "relevancy": 2.4757, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5051}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4921}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probability%20Passing%20for%20Graph%20Neural%20Networks%3A%20Graph%20Structure%20and%0A%20%20Representations%20Joint%20Learning&body=Title%3A%20Probability%20Passing%20for%20Graph%20Neural%20Networks%3A%20Graph%20Structure%20and%0A%20%20Representations%20Joint%20Learning%0AAuthor%3A%20Ziyan%20Wang%20and%20YaXuan%20He%20and%20Bin%20Liu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20notable%20success%20in%20the%20analysis%20of%0Anon-Euclidean%20data%20across%20a%20wide%20range%20of%20domains.%20However%2C%20their%20applicability%0Ais%20constrained%20by%20the%20dependence%20on%20the%20observed%20graph%20structure.%20To%20solve%20this%0Aproblem%2C%20Latent%20Graph%20Inference%20%28LGI%29%20is%20proposed%20to%20infer%20a%20task-specific%0Alatent%20structure%20by%20computing%20similarity%20or%20edge%20probability%20of%20node%20features%0Aand%20then%20apply%20a%20GNN%20to%20produce%20predictions.%20Even%20so%2C%20existing%20approaches%0Aneglect%20the%20noise%20from%20node%20features%2C%20which%20affects%20generated%20graph%20structure%0Aand%20performance.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20method%20called%20Probability%0APassing%20to%20refine%20the%20generated%20graph%20structure%20by%20aggregating%20edge%0Aprobabilities%20of%20neighboring%20nodes%20based%20on%20observed%20graph.%20Furthermore%2C%20we%0Acontinue%20to%20utilize%20the%20LGI%20framework%2C%20inputting%20the%20refined%20graph%20structure%0Aand%20node%20features%20into%20GNNs%20to%20obtain%20predictions.%20We%20name%20the%20proposed%20scheme%0Aas%20Probability%20Passing-based%20Graph%20Neural%20Network%20%28PPGNN%29.%20Moreover%2C%20the%0Aanchor-based%20technique%20is%20employed%20to%20reduce%20complexity%20and%20improve%20efficiency.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbability%2520Passing%2520for%2520Graph%2520Neural%2520Networks%253A%2520Graph%2520Structure%2520and%250A%2520%2520Representations%2520Joint%2520Learning%26entry.906535625%3DZiyan%2520Wang%2520and%2520YaXuan%2520He%2520and%2520Bin%2520Liu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520notable%2520success%2520in%2520the%2520analysis%2520of%250Anon-Euclidean%2520data%2520across%2520a%2520wide%2520range%2520of%2520domains.%2520However%252C%2520their%2520applicability%250Ais%2520constrained%2520by%2520the%2520dependence%2520on%2520the%2520observed%2520graph%2520structure.%2520To%2520solve%2520this%250Aproblem%252C%2520Latent%2520Graph%2520Inference%2520%2528LGI%2529%2520is%2520proposed%2520to%2520infer%2520a%2520task-specific%250Alatent%2520structure%2520by%2520computing%2520similarity%2520or%2520edge%2520probability%2520of%2520node%2520features%250Aand%2520then%2520apply%2520a%2520GNN%2520to%2520produce%2520predictions.%2520Even%2520so%252C%2520existing%2520approaches%250Aneglect%2520the%2520noise%2520from%2520node%2520features%252C%2520which%2520affects%2520generated%2520graph%2520structure%250Aand%2520performance.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520method%2520called%2520Probability%250APassing%2520to%2520refine%2520the%2520generated%2520graph%2520structure%2520by%2520aggregating%2520edge%250Aprobabilities%2520of%2520neighboring%2520nodes%2520based%2520on%2520observed%2520graph.%2520Furthermore%252C%2520we%250Acontinue%2520to%2520utilize%2520the%2520LGI%2520framework%252C%2520inputting%2520the%2520refined%2520graph%2520structure%250Aand%2520node%2520features%2520into%2520GNNs%2520to%2520obtain%2520predictions.%2520We%2520name%2520the%2520proposed%2520scheme%250Aas%2520Probability%2520Passing-based%2520Graph%2520Neural%2520Network%2520%2528PPGNN%2529.%2520Moreover%252C%2520the%250Aanchor-based%2520technique%2520is%2520employed%2520to%2520reduce%2520complexity%2520and%2520improve%2520efficiency.%250AExperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probability%20Passing%20for%20Graph%20Neural%20Networks%3A%20Graph%20Structure%20and%0A%20%20Representations%20Joint%20Learning&entry.906535625=Ziyan%20Wang%20and%20YaXuan%20He%20and%20Bin%20Liu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20notable%20success%20in%20the%20analysis%20of%0Anon-Euclidean%20data%20across%20a%20wide%20range%20of%20domains.%20However%2C%20their%20applicability%0Ais%20constrained%20by%20the%20dependence%20on%20the%20observed%20graph%20structure.%20To%20solve%20this%0Aproblem%2C%20Latent%20Graph%20Inference%20%28LGI%29%20is%20proposed%20to%20infer%20a%20task-specific%0Alatent%20structure%20by%20computing%20similarity%20or%20edge%20probability%20of%20node%20features%0Aand%20then%20apply%20a%20GNN%20to%20produce%20predictions.%20Even%20so%2C%20existing%20approaches%0Aneglect%20the%20noise%20from%20node%20features%2C%20which%20affects%20generated%20graph%20structure%0Aand%20performance.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20method%20called%20Probability%0APassing%20to%20refine%20the%20generated%20graph%20structure%20by%20aggregating%20edge%0Aprobabilities%20of%20neighboring%20nodes%20based%20on%20observed%20graph.%20Furthermore%2C%20we%0Acontinue%20to%20utilize%20the%20LGI%20framework%2C%20inputting%20the%20refined%20graph%20structure%0Aand%20node%20features%20into%20GNNs%20to%20obtain%20predictions.%20We%20name%20the%20proposed%20scheme%0Aas%20Probability%20Passing-based%20Graph%20Neural%20Network%20%28PPGNN%29.%20Moreover%2C%20the%0Aanchor-based%20technique%20is%20employed%20to%20reduce%20complexity%20and%20improve%20efficiency.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10688v1&entry.124074799=Read"},
{"title": "Constrained 6-DoF Grasp Generation on Complex Shapes for Improved\n  Dual-Arm Manipulation", "author": "Gaurav Singh and Sanket Kalwar and Md Faizal Karim and Bipasha Sen and Nagamanikandan Govindan and Srinath Sridhar and K Madhava Krishna", "abstract": "  Efficiently generating grasp poses tailored to specific regions of an object\nis vital for various robotic manipulation tasks, especially in a dual-arm\nsetup. This scenario presents a significant challenge due to the complex\ngeometries involved, requiring a deep understanding of the local geometry to\ngenerate grasps efficiently on the specified constrained regions. Existing\nmethods only explore settings involving table-top/small objects and require\naugmented datasets to train, limiting their performance on complex objects. We\npropose CGDF: Constrained Grasp Diffusion Fields, a diffusion-based grasp\ngenerative model that generalizes to objects with arbitrary geometries, as well\nas generates dense grasps on the target regions. CGDF uses a part-guided\ndiffusion approach that enables it to get high sample efficiency in constrained\ngrasping without explicitly training on massive constraint-augmented datasets.\nWe provide qualitative and quantitative comparisons using analytical metrics\nand in simulation, in both unconstrained and constrained settings to show that\nour method can generalize to generate stable grasps on complex objects,\nespecially useful for dual-arm manipulation settings, while existing methods\nstruggle to do so.\n", "link": "http://arxiv.org/abs/2404.04643v2", "date": "2024-07-15", "relevancy": 2.4679, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7083}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5616}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%206-DoF%20Grasp%20Generation%20on%20Complex%20Shapes%20for%20Improved%0A%20%20Dual-Arm%20Manipulation&body=Title%3A%20Constrained%206-DoF%20Grasp%20Generation%20on%20Complex%20Shapes%20for%20Improved%0A%20%20Dual-Arm%20Manipulation%0AAuthor%3A%20Gaurav%20Singh%20and%20Sanket%20Kalwar%20and%20Md%20Faizal%20Karim%20and%20Bipasha%20Sen%20and%20Nagamanikandan%20Govindan%20and%20Srinath%20Sridhar%20and%20K%20Madhava%20Krishna%0AAbstract%3A%20%20%20Efficiently%20generating%20grasp%20poses%20tailored%20to%20specific%20regions%20of%20an%20object%0Ais%20vital%20for%20various%20robotic%20manipulation%20tasks%2C%20especially%20in%20a%20dual-arm%0Asetup.%20This%20scenario%20presents%20a%20significant%20challenge%20due%20to%20the%20complex%0Ageometries%20involved%2C%20requiring%20a%20deep%20understanding%20of%20the%20local%20geometry%20to%0Agenerate%20grasps%20efficiently%20on%20the%20specified%20constrained%20regions.%20Existing%0Amethods%20only%20explore%20settings%20involving%20table-top/small%20objects%20and%20require%0Aaugmented%20datasets%20to%20train%2C%20limiting%20their%20performance%20on%20complex%20objects.%20We%0Apropose%20CGDF%3A%20Constrained%20Grasp%20Diffusion%20Fields%2C%20a%20diffusion-based%20grasp%0Agenerative%20model%20that%20generalizes%20to%20objects%20with%20arbitrary%20geometries%2C%20as%20well%0Aas%20generates%20dense%20grasps%20on%20the%20target%20regions.%20CGDF%20uses%20a%20part-guided%0Adiffusion%20approach%20that%20enables%20it%20to%20get%20high%20sample%20efficiency%20in%20constrained%0Agrasping%20without%20explicitly%20training%20on%20massive%20constraint-augmented%20datasets.%0AWe%20provide%20qualitative%20and%20quantitative%20comparisons%20using%20analytical%20metrics%0Aand%20in%20simulation%2C%20in%20both%20unconstrained%20and%20constrained%20settings%20to%20show%20that%0Aour%20method%20can%20generalize%20to%20generate%20stable%20grasps%20on%20complex%20objects%2C%0Aespecially%20useful%20for%20dual-arm%20manipulation%20settings%2C%20while%20existing%20methods%0Astruggle%20to%20do%20so.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%25206-DoF%2520Grasp%2520Generation%2520on%2520Complex%2520Shapes%2520for%2520Improved%250A%2520%2520Dual-Arm%2520Manipulation%26entry.906535625%3DGaurav%2520Singh%2520and%2520Sanket%2520Kalwar%2520and%2520Md%2520Faizal%2520Karim%2520and%2520Bipasha%2520Sen%2520and%2520Nagamanikandan%2520Govindan%2520and%2520Srinath%2520Sridhar%2520and%2520K%2520Madhava%2520Krishna%26entry.1292438233%3D%2520%2520Efficiently%2520generating%2520grasp%2520poses%2520tailored%2520to%2520specific%2520regions%2520of%2520an%2520object%250Ais%2520vital%2520for%2520various%2520robotic%2520manipulation%2520tasks%252C%2520especially%2520in%2520a%2520dual-arm%250Asetup.%2520This%2520scenario%2520presents%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520complex%250Ageometries%2520involved%252C%2520requiring%2520a%2520deep%2520understanding%2520of%2520the%2520local%2520geometry%2520to%250Agenerate%2520grasps%2520efficiently%2520on%2520the%2520specified%2520constrained%2520regions.%2520Existing%250Amethods%2520only%2520explore%2520settings%2520involving%2520table-top/small%2520objects%2520and%2520require%250Aaugmented%2520datasets%2520to%2520train%252C%2520limiting%2520their%2520performance%2520on%2520complex%2520objects.%2520We%250Apropose%2520CGDF%253A%2520Constrained%2520Grasp%2520Diffusion%2520Fields%252C%2520a%2520diffusion-based%2520grasp%250Agenerative%2520model%2520that%2520generalizes%2520to%2520objects%2520with%2520arbitrary%2520geometries%252C%2520as%2520well%250Aas%2520generates%2520dense%2520grasps%2520on%2520the%2520target%2520regions.%2520CGDF%2520uses%2520a%2520part-guided%250Adiffusion%2520approach%2520that%2520enables%2520it%2520to%2520get%2520high%2520sample%2520efficiency%2520in%2520constrained%250Agrasping%2520without%2520explicitly%2520training%2520on%2520massive%2520constraint-augmented%2520datasets.%250AWe%2520provide%2520qualitative%2520and%2520quantitative%2520comparisons%2520using%2520analytical%2520metrics%250Aand%2520in%2520simulation%252C%2520in%2520both%2520unconstrained%2520and%2520constrained%2520settings%2520to%2520show%2520that%250Aour%2520method%2520can%2520generalize%2520to%2520generate%2520stable%2520grasps%2520on%2520complex%2520objects%252C%250Aespecially%2520useful%2520for%2520dual-arm%2520manipulation%2520settings%252C%2520while%2520existing%2520methods%250Astruggle%2520to%2520do%2520so.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%206-DoF%20Grasp%20Generation%20on%20Complex%20Shapes%20for%20Improved%0A%20%20Dual-Arm%20Manipulation&entry.906535625=Gaurav%20Singh%20and%20Sanket%20Kalwar%20and%20Md%20Faizal%20Karim%20and%20Bipasha%20Sen%20and%20Nagamanikandan%20Govindan%20and%20Srinath%20Sridhar%20and%20K%20Madhava%20Krishna&entry.1292438233=%20%20Efficiently%20generating%20grasp%20poses%20tailored%20to%20specific%20regions%20of%20an%20object%0Ais%20vital%20for%20various%20robotic%20manipulation%20tasks%2C%20especially%20in%20a%20dual-arm%0Asetup.%20This%20scenario%20presents%20a%20significant%20challenge%20due%20to%20the%20complex%0Ageometries%20involved%2C%20requiring%20a%20deep%20understanding%20of%20the%20local%20geometry%20to%0Agenerate%20grasps%20efficiently%20on%20the%20specified%20constrained%20regions.%20Existing%0Amethods%20only%20explore%20settings%20involving%20table-top/small%20objects%20and%20require%0Aaugmented%20datasets%20to%20train%2C%20limiting%20their%20performance%20on%20complex%20objects.%20We%0Apropose%20CGDF%3A%20Constrained%20Grasp%20Diffusion%20Fields%2C%20a%20diffusion-based%20grasp%0Agenerative%20model%20that%20generalizes%20to%20objects%20with%20arbitrary%20geometries%2C%20as%20well%0Aas%20generates%20dense%20grasps%20on%20the%20target%20regions.%20CGDF%20uses%20a%20part-guided%0Adiffusion%20approach%20that%20enables%20it%20to%20get%20high%20sample%20efficiency%20in%20constrained%0Agrasping%20without%20explicitly%20training%20on%20massive%20constraint-augmented%20datasets.%0AWe%20provide%20qualitative%20and%20quantitative%20comparisons%20using%20analytical%20metrics%0Aand%20in%20simulation%2C%20in%20both%20unconstrained%20and%20constrained%20settings%20to%20show%20that%0Aour%20method%20can%20generalize%20to%20generate%20stable%20grasps%20on%20complex%20objects%2C%0Aespecially%20useful%20for%20dual-arm%20manipulation%20settings%2C%20while%20existing%20methods%0Astruggle%20to%20do%20so.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04643v2&entry.124074799=Read"},
{"title": "COSMU: Complete 3D human shape from monocular unconstrained images", "author": "Marco Pesavento and Marco Volino and Adrian Hilton", "abstract": "  We present a novel framework to reconstruct complete 3D human shapes from a\ngiven target image by leveraging monocular unconstrained images. The objective\nof this work is to reproduce high-quality details in regions of the\nreconstructed human body that are not visible in the input target. The proposed\nmethodology addresses the limitations of existing approaches for reconstructing\n3D human shapes from a single image, which cannot reproduce shape details in\noccluded body regions. The missing information of the monocular input can be\nrecovered by using multiple views captured from multiple cameras. However,\nmulti-view reconstruction methods necessitate accurately calibrated and\nregistered images, which can be challenging to obtain in real-world scenarios.\nGiven a target RGB image and a collection of multiple uncalibrated and\nunregistered images of the same individual, acquired using a single camera, we\npropose a novel framework to generate complete 3D human shapes. We introduce a\nnovel module to generate 2D multi-view normal maps of the person registered\nwith the target input image. The module consists of body part-based reference\nselection and body part-based registration. The generated 2D normal maps are\nthen processed by a multi-view attention-based neural implicit model that\nestimates an implicit representation of the 3D shape, ensuring the reproduction\nof details in both observed and occluded regions. Extensive experiments\ndemonstrate that the proposed approach estimates higher quality details in the\nnon-visible regions of the 3D clothed human shapes compared to related methods,\nwithout using parametric models.\n", "link": "http://arxiv.org/abs/2407.10586v1", "date": "2024-07-15", "relevancy": 2.4557, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6155}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6155}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COSMU%3A%20Complete%203D%20human%20shape%20from%20monocular%20unconstrained%20images&body=Title%3A%20COSMU%3A%20Complete%203D%20human%20shape%20from%20monocular%20unconstrained%20images%0AAuthor%3A%20Marco%20Pesavento%20and%20Marco%20Volino%20and%20Adrian%20Hilton%0AAbstract%3A%20%20%20We%20present%20a%20novel%20framework%20to%20reconstruct%20complete%203D%20human%20shapes%20from%20a%0Agiven%20target%20image%20by%20leveraging%20monocular%20unconstrained%20images.%20The%20objective%0Aof%20this%20work%20is%20to%20reproduce%20high-quality%20details%20in%20regions%20of%20the%0Areconstructed%20human%20body%20that%20are%20not%20visible%20in%20the%20input%20target.%20The%20proposed%0Amethodology%20addresses%20the%20limitations%20of%20existing%20approaches%20for%20reconstructing%0A3D%20human%20shapes%20from%20a%20single%20image%2C%20which%20cannot%20reproduce%20shape%20details%20in%0Aoccluded%20body%20regions.%20The%20missing%20information%20of%20the%20monocular%20input%20can%20be%0Arecovered%20by%20using%20multiple%20views%20captured%20from%20multiple%20cameras.%20However%2C%0Amulti-view%20reconstruction%20methods%20necessitate%20accurately%20calibrated%20and%0Aregistered%20images%2C%20which%20can%20be%20challenging%20to%20obtain%20in%20real-world%20scenarios.%0AGiven%20a%20target%20RGB%20image%20and%20a%20collection%20of%20multiple%20uncalibrated%20and%0Aunregistered%20images%20of%20the%20same%20individual%2C%20acquired%20using%20a%20single%20camera%2C%20we%0Apropose%20a%20novel%20framework%20to%20generate%20complete%203D%20human%20shapes.%20We%20introduce%20a%0Anovel%20module%20to%20generate%202D%20multi-view%20normal%20maps%20of%20the%20person%20registered%0Awith%20the%20target%20input%20image.%20The%20module%20consists%20of%20body%20part-based%20reference%0Aselection%20and%20body%20part-based%20registration.%20The%20generated%202D%20normal%20maps%20are%0Athen%20processed%20by%20a%20multi-view%20attention-based%20neural%20implicit%20model%20that%0Aestimates%20an%20implicit%20representation%20of%20the%203D%20shape%2C%20ensuring%20the%20reproduction%0Aof%20details%20in%20both%20observed%20and%20occluded%20regions.%20Extensive%20experiments%0Ademonstrate%20that%20the%20proposed%20approach%20estimates%20higher%20quality%20details%20in%20the%0Anon-visible%20regions%20of%20the%203D%20clothed%20human%20shapes%20compared%20to%20related%20methods%2C%0Awithout%20using%20parametric%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOSMU%253A%2520Complete%25203D%2520human%2520shape%2520from%2520monocular%2520unconstrained%2520images%26entry.906535625%3DMarco%2520Pesavento%2520and%2520Marco%2520Volino%2520and%2520Adrian%2520Hilton%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520framework%2520to%2520reconstruct%2520complete%25203D%2520human%2520shapes%2520from%2520a%250Agiven%2520target%2520image%2520by%2520leveraging%2520monocular%2520unconstrained%2520images.%2520The%2520objective%250Aof%2520this%2520work%2520is%2520to%2520reproduce%2520high-quality%2520details%2520in%2520regions%2520of%2520the%250Areconstructed%2520human%2520body%2520that%2520are%2520not%2520visible%2520in%2520the%2520input%2520target.%2520The%2520proposed%250Amethodology%2520addresses%2520the%2520limitations%2520of%2520existing%2520approaches%2520for%2520reconstructing%250A3D%2520human%2520shapes%2520from%2520a%2520single%2520image%252C%2520which%2520cannot%2520reproduce%2520shape%2520details%2520in%250Aoccluded%2520body%2520regions.%2520The%2520missing%2520information%2520of%2520the%2520monocular%2520input%2520can%2520be%250Arecovered%2520by%2520using%2520multiple%2520views%2520captured%2520from%2520multiple%2520cameras.%2520However%252C%250Amulti-view%2520reconstruction%2520methods%2520necessitate%2520accurately%2520calibrated%2520and%250Aregistered%2520images%252C%2520which%2520can%2520be%2520challenging%2520to%2520obtain%2520in%2520real-world%2520scenarios.%250AGiven%2520a%2520target%2520RGB%2520image%2520and%2520a%2520collection%2520of%2520multiple%2520uncalibrated%2520and%250Aunregistered%2520images%2520of%2520the%2520same%2520individual%252C%2520acquired%2520using%2520a%2520single%2520camera%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520to%2520generate%2520complete%25203D%2520human%2520shapes.%2520We%2520introduce%2520a%250Anovel%2520module%2520to%2520generate%25202D%2520multi-view%2520normal%2520maps%2520of%2520the%2520person%2520registered%250Awith%2520the%2520target%2520input%2520image.%2520The%2520module%2520consists%2520of%2520body%2520part-based%2520reference%250Aselection%2520and%2520body%2520part-based%2520registration.%2520The%2520generated%25202D%2520normal%2520maps%2520are%250Athen%2520processed%2520by%2520a%2520multi-view%2520attention-based%2520neural%2520implicit%2520model%2520that%250Aestimates%2520an%2520implicit%2520representation%2520of%2520the%25203D%2520shape%252C%2520ensuring%2520the%2520reproduction%250Aof%2520details%2520in%2520both%2520observed%2520and%2520occluded%2520regions.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520the%2520proposed%2520approach%2520estimates%2520higher%2520quality%2520details%2520in%2520the%250Anon-visible%2520regions%2520of%2520the%25203D%2520clothed%2520human%2520shapes%2520compared%2520to%2520related%2520methods%252C%250Awithout%2520using%2520parametric%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COSMU%3A%20Complete%203D%20human%20shape%20from%20monocular%20unconstrained%20images&entry.906535625=Marco%20Pesavento%20and%20Marco%20Volino%20and%20Adrian%20Hilton&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20to%20reconstruct%20complete%203D%20human%20shapes%20from%20a%0Agiven%20target%20image%20by%20leveraging%20monocular%20unconstrained%20images.%20The%20objective%0Aof%20this%20work%20is%20to%20reproduce%20high-quality%20details%20in%20regions%20of%20the%0Areconstructed%20human%20body%20that%20are%20not%20visible%20in%20the%20input%20target.%20The%20proposed%0Amethodology%20addresses%20the%20limitations%20of%20existing%20approaches%20for%20reconstructing%0A3D%20human%20shapes%20from%20a%20single%20image%2C%20which%20cannot%20reproduce%20shape%20details%20in%0Aoccluded%20body%20regions.%20The%20missing%20information%20of%20the%20monocular%20input%20can%20be%0Arecovered%20by%20using%20multiple%20views%20captured%20from%20multiple%20cameras.%20However%2C%0Amulti-view%20reconstruction%20methods%20necessitate%20accurately%20calibrated%20and%0Aregistered%20images%2C%20which%20can%20be%20challenging%20to%20obtain%20in%20real-world%20scenarios.%0AGiven%20a%20target%20RGB%20image%20and%20a%20collection%20of%20multiple%20uncalibrated%20and%0Aunregistered%20images%20of%20the%20same%20individual%2C%20acquired%20using%20a%20single%20camera%2C%20we%0Apropose%20a%20novel%20framework%20to%20generate%20complete%203D%20human%20shapes.%20We%20introduce%20a%0Anovel%20module%20to%20generate%202D%20multi-view%20normal%20maps%20of%20the%20person%20registered%0Awith%20the%20target%20input%20image.%20The%20module%20consists%20of%20body%20part-based%20reference%0Aselection%20and%20body%20part-based%20registration.%20The%20generated%202D%20normal%20maps%20are%0Athen%20processed%20by%20a%20multi-view%20attention-based%20neural%20implicit%20model%20that%0Aestimates%20an%20implicit%20representation%20of%20the%203D%20shape%2C%20ensuring%20the%20reproduction%0Aof%20details%20in%20both%20observed%20and%20occluded%20regions.%20Extensive%20experiments%0Ademonstrate%20that%20the%20proposed%20approach%20estimates%20higher%20quality%20details%20in%20the%0Anon-visible%20regions%20of%20the%203D%20clothed%20human%20shapes%20compared%20to%20related%20methods%2C%0Awithout%20using%20parametric%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10586v1&entry.124074799=Read"},
{"title": "SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional\n  Temporal Grounding", "author": "Zixu Cheng and Yujiang Pu and Shaogang Gong and Parisa Kordjamshidi and Yu Kong", "abstract": "  Temporal grounding, also known as video moment retrieval, aims at locating\nvideo segments corresponding to a given query sentence. The compositional\nnature of natural language enables the localization beyond predefined events,\nposing a certain challenge to the compositional generalizability of existing\nmethods. Recent studies establish the correspondence between videos and queries\nthrough a decompose-reconstruct manner to achieve compositional generalization.\nHowever, they only consider dominant primitives and build negative queries\nthrough random sampling and recombination, resulting in semantically\nimplausible negatives that hinder the models from learning rational\ncompositions. In addition, recent DETR-based methods still underperform in\ncompositional temporal grounding, showing irrational saliency responses when\ngiven negative queries that have subtle differences from positive queries. To\naddress these limitations, we first propose a large language model-driven\nmethod for negative query construction, utilizing GPT-3.5-Turbo to generate\nsemantically plausible hard negative queries. Subsequently, we introduce a\ncoarse-to-fine saliency ranking strategy, which encourages the model to learn\nthe multi-granularity semantic relationships between videos and hierarchical\nnegative queries to boost compositional generalization. Extensive experiments\non two challenging benchmarks validate the effectiveness and generalizability\nof our proposed method. Our code is available at\nhttps://github.com/zxccade/SHINE.\n", "link": "http://arxiv.org/abs/2407.05118v2", "date": "2024-07-15", "relevancy": 2.4529, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5047}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4843}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHINE%3A%20Saliency-aware%20HIerarchical%20NEgative%20Ranking%20for%20Compositional%0A%20%20Temporal%20Grounding&body=Title%3A%20SHINE%3A%20Saliency-aware%20HIerarchical%20NEgative%20Ranking%20for%20Compositional%0A%20%20Temporal%20Grounding%0AAuthor%3A%20Zixu%20Cheng%20and%20Yujiang%20Pu%20and%20Shaogang%20Gong%20and%20Parisa%20Kordjamshidi%20and%20Yu%20Kong%0AAbstract%3A%20%20%20Temporal%20grounding%2C%20also%20known%20as%20video%20moment%20retrieval%2C%20aims%20at%20locating%0Avideo%20segments%20corresponding%20to%20a%20given%20query%20sentence.%20The%20compositional%0Anature%20of%20natural%20language%20enables%20the%20localization%20beyond%20predefined%20events%2C%0Aposing%20a%20certain%20challenge%20to%20the%20compositional%20generalizability%20of%20existing%0Amethods.%20Recent%20studies%20establish%20the%20correspondence%20between%20videos%20and%20queries%0Athrough%20a%20decompose-reconstruct%20manner%20to%20achieve%20compositional%20generalization.%0AHowever%2C%20they%20only%20consider%20dominant%20primitives%20and%20build%20negative%20queries%0Athrough%20random%20sampling%20and%20recombination%2C%20resulting%20in%20semantically%0Aimplausible%20negatives%20that%20hinder%20the%20models%20from%20learning%20rational%0Acompositions.%20In%20addition%2C%20recent%20DETR-based%20methods%20still%20underperform%20in%0Acompositional%20temporal%20grounding%2C%20showing%20irrational%20saliency%20responses%20when%0Agiven%20negative%20queries%20that%20have%20subtle%20differences%20from%20positive%20queries.%20To%0Aaddress%20these%20limitations%2C%20we%20first%20propose%20a%20large%20language%20model-driven%0Amethod%20for%20negative%20query%20construction%2C%20utilizing%20GPT-3.5-Turbo%20to%20generate%0Asemantically%20plausible%20hard%20negative%20queries.%20Subsequently%2C%20we%20introduce%20a%0Acoarse-to-fine%20saliency%20ranking%20strategy%2C%20which%20encourages%20the%20model%20to%20learn%0Athe%20multi-granularity%20semantic%20relationships%20between%20videos%20and%20hierarchical%0Anegative%20queries%20to%20boost%20compositional%20generalization.%20Extensive%20experiments%0Aon%20two%20challenging%20benchmarks%20validate%20the%20effectiveness%20and%20generalizability%0Aof%20our%20proposed%20method.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zxccade/SHINE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHINE%253A%2520Saliency-aware%2520HIerarchical%2520NEgative%2520Ranking%2520for%2520Compositional%250A%2520%2520Temporal%2520Grounding%26entry.906535625%3DZixu%2520Cheng%2520and%2520Yujiang%2520Pu%2520and%2520Shaogang%2520Gong%2520and%2520Parisa%2520Kordjamshidi%2520and%2520Yu%2520Kong%26entry.1292438233%3D%2520%2520Temporal%2520grounding%252C%2520also%2520known%2520as%2520video%2520moment%2520retrieval%252C%2520aims%2520at%2520locating%250Avideo%2520segments%2520corresponding%2520to%2520a%2520given%2520query%2520sentence.%2520The%2520compositional%250Anature%2520of%2520natural%2520language%2520enables%2520the%2520localization%2520beyond%2520predefined%2520events%252C%250Aposing%2520a%2520certain%2520challenge%2520to%2520the%2520compositional%2520generalizability%2520of%2520existing%250Amethods.%2520Recent%2520studies%2520establish%2520the%2520correspondence%2520between%2520videos%2520and%2520queries%250Athrough%2520a%2520decompose-reconstruct%2520manner%2520to%2520achieve%2520compositional%2520generalization.%250AHowever%252C%2520they%2520only%2520consider%2520dominant%2520primitives%2520and%2520build%2520negative%2520queries%250Athrough%2520random%2520sampling%2520and%2520recombination%252C%2520resulting%2520in%2520semantically%250Aimplausible%2520negatives%2520that%2520hinder%2520the%2520models%2520from%2520learning%2520rational%250Acompositions.%2520In%2520addition%252C%2520recent%2520DETR-based%2520methods%2520still%2520underperform%2520in%250Acompositional%2520temporal%2520grounding%252C%2520showing%2520irrational%2520saliency%2520responses%2520when%250Agiven%2520negative%2520queries%2520that%2520have%2520subtle%2520differences%2520from%2520positive%2520queries.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520first%2520propose%2520a%2520large%2520language%2520model-driven%250Amethod%2520for%2520negative%2520query%2520construction%252C%2520utilizing%2520GPT-3.5-Turbo%2520to%2520generate%250Asemantically%2520plausible%2520hard%2520negative%2520queries.%2520Subsequently%252C%2520we%2520introduce%2520a%250Acoarse-to-fine%2520saliency%2520ranking%2520strategy%252C%2520which%2520encourages%2520the%2520model%2520to%2520learn%250Athe%2520multi-granularity%2520semantic%2520relationships%2520between%2520videos%2520and%2520hierarchical%250Anegative%2520queries%2520to%2520boost%2520compositional%2520generalization.%2520Extensive%2520experiments%250Aon%2520two%2520challenging%2520benchmarks%2520validate%2520the%2520effectiveness%2520and%2520generalizability%250Aof%2520our%2520proposed%2520method.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zxccade/SHINE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHINE%3A%20Saliency-aware%20HIerarchical%20NEgative%20Ranking%20for%20Compositional%0A%20%20Temporal%20Grounding&entry.906535625=Zixu%20Cheng%20and%20Yujiang%20Pu%20and%20Shaogang%20Gong%20and%20Parisa%20Kordjamshidi%20and%20Yu%20Kong&entry.1292438233=%20%20Temporal%20grounding%2C%20also%20known%20as%20video%20moment%20retrieval%2C%20aims%20at%20locating%0Avideo%20segments%20corresponding%20to%20a%20given%20query%20sentence.%20The%20compositional%0Anature%20of%20natural%20language%20enables%20the%20localization%20beyond%20predefined%20events%2C%0Aposing%20a%20certain%20challenge%20to%20the%20compositional%20generalizability%20of%20existing%0Amethods.%20Recent%20studies%20establish%20the%20correspondence%20between%20videos%20and%20queries%0Athrough%20a%20decompose-reconstruct%20manner%20to%20achieve%20compositional%20generalization.%0AHowever%2C%20they%20only%20consider%20dominant%20primitives%20and%20build%20negative%20queries%0Athrough%20random%20sampling%20and%20recombination%2C%20resulting%20in%20semantically%0Aimplausible%20negatives%20that%20hinder%20the%20models%20from%20learning%20rational%0Acompositions.%20In%20addition%2C%20recent%20DETR-based%20methods%20still%20underperform%20in%0Acompositional%20temporal%20grounding%2C%20showing%20irrational%20saliency%20responses%20when%0Agiven%20negative%20queries%20that%20have%20subtle%20differences%20from%20positive%20queries.%20To%0Aaddress%20these%20limitations%2C%20we%20first%20propose%20a%20large%20language%20model-driven%0Amethod%20for%20negative%20query%20construction%2C%20utilizing%20GPT-3.5-Turbo%20to%20generate%0Asemantically%20plausible%20hard%20negative%20queries.%20Subsequently%2C%20we%20introduce%20a%0Acoarse-to-fine%20saliency%20ranking%20strategy%2C%20which%20encourages%20the%20model%20to%20learn%0Athe%20multi-granularity%20semantic%20relationships%20between%20videos%20and%20hierarchical%0Anegative%20queries%20to%20boost%20compositional%20generalization.%20Extensive%20experiments%0Aon%20two%20challenging%20benchmarks%20validate%20the%20effectiveness%20and%20generalizability%0Aof%20our%20proposed%20method.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zxccade/SHINE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05118v2&entry.124074799=Read"},
{"title": "PILoRA: Prototype Guided Incremental LoRA for Federated\n  Class-Incremental Learning", "author": "Haiyang Guo and Fei Zhu and Wenzhuo Liu and Xu-Yao Zhang and Cheng-Lin Liu", "abstract": "  Existing federated learning methods have effectively dealt with decentralized\nlearning in scenarios involving data privacy and non-IID data. However, in\nreal-world situations, each client dynamically learns new classes, requiring\nthe global model to classify all seen classes. To effectively mitigate\ncatastrophic forgetting and data heterogeneity under low communication costs,\nwe propose a simple and effective method named PILoRA. On the one hand, we\nadopt prototype learning to learn better feature representations and leverage\nthe heuristic information between prototypes and class features to design a\nprototype re-weight module to solve the classifier bias caused by data\nheterogeneity without retraining the classifier. On the other hand, we view\nincremental learning as the process of learning distinct task vectors and\nencoding them within different LoRA parameters. Accordingly, we propose\nIncremental LoRA to mitigate catastrophic forgetting. Experimental results on\nstandard datasets indicate that our method outperforms the state-of-the-art\napproaches significantly. More importantly, our method exhibits strong\nrobustness and superiority in different settings and degrees of data\nheterogeneity. The code is available at\n\\url{https://github.com/Ghy0501/PILoRA}.\n", "link": "http://arxiv.org/abs/2401.02094v2", "date": "2024-07-15", "relevancy": 2.4519, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4855}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PILoRA%3A%20Prototype%20Guided%20Incremental%20LoRA%20for%20Federated%0A%20%20Class-Incremental%20Learning&body=Title%3A%20PILoRA%3A%20Prototype%20Guided%20Incremental%20LoRA%20for%20Federated%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Haiyang%20Guo%20and%20Fei%20Zhu%20and%20Wenzhuo%20Liu%20and%20Xu-Yao%20Zhang%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Existing%20federated%20learning%20methods%20have%20effectively%20dealt%20with%20decentralized%0Alearning%20in%20scenarios%20involving%20data%20privacy%20and%20non-IID%20data.%20However%2C%20in%0Areal-world%20situations%2C%20each%20client%20dynamically%20learns%20new%20classes%2C%20requiring%0Athe%20global%20model%20to%20classify%20all%20seen%20classes.%20To%20effectively%20mitigate%0Acatastrophic%20forgetting%20and%20data%20heterogeneity%20under%20low%20communication%20costs%2C%0Awe%20propose%20a%20simple%20and%20effective%20method%20named%20PILoRA.%20On%20the%20one%20hand%2C%20we%0Aadopt%20prototype%20learning%20to%20learn%20better%20feature%20representations%20and%20leverage%0Athe%20heuristic%20information%20between%20prototypes%20and%20class%20features%20to%20design%20a%0Aprototype%20re-weight%20module%20to%20solve%20the%20classifier%20bias%20caused%20by%20data%0Aheterogeneity%20without%20retraining%20the%20classifier.%20On%20the%20other%20hand%2C%20we%20view%0Aincremental%20learning%20as%20the%20process%20of%20learning%20distinct%20task%20vectors%20and%0Aencoding%20them%20within%20different%20LoRA%20parameters.%20Accordingly%2C%20we%20propose%0AIncremental%20LoRA%20to%20mitigate%20catastrophic%20forgetting.%20Experimental%20results%20on%0Astandard%20datasets%20indicate%20that%20our%20method%20outperforms%20the%20state-of-the-art%0Aapproaches%20significantly.%20More%20importantly%2C%20our%20method%20exhibits%20strong%0Arobustness%20and%20superiority%20in%20different%20settings%20and%20degrees%20of%20data%0Aheterogeneity.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Ghy0501/PILoRA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPILoRA%253A%2520Prototype%2520Guided%2520Incremental%2520LoRA%2520for%2520Federated%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DHaiyang%2520Guo%2520and%2520Fei%2520Zhu%2520and%2520Wenzhuo%2520Liu%2520and%2520Xu-Yao%2520Zhang%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520Existing%2520federated%2520learning%2520methods%2520have%2520effectively%2520dealt%2520with%2520decentralized%250Alearning%2520in%2520scenarios%2520involving%2520data%2520privacy%2520and%2520non-IID%2520data.%2520However%252C%2520in%250Areal-world%2520situations%252C%2520each%2520client%2520dynamically%2520learns%2520new%2520classes%252C%2520requiring%250Athe%2520global%2520model%2520to%2520classify%2520all%2520seen%2520classes.%2520To%2520effectively%2520mitigate%250Acatastrophic%2520forgetting%2520and%2520data%2520heterogeneity%2520under%2520low%2520communication%2520costs%252C%250Awe%2520propose%2520a%2520simple%2520and%2520effective%2520method%2520named%2520PILoRA.%2520On%2520the%2520one%2520hand%252C%2520we%250Aadopt%2520prototype%2520learning%2520to%2520learn%2520better%2520feature%2520representations%2520and%2520leverage%250Athe%2520heuristic%2520information%2520between%2520prototypes%2520and%2520class%2520features%2520to%2520design%2520a%250Aprototype%2520re-weight%2520module%2520to%2520solve%2520the%2520classifier%2520bias%2520caused%2520by%2520data%250Aheterogeneity%2520without%2520retraining%2520the%2520classifier.%2520On%2520the%2520other%2520hand%252C%2520we%2520view%250Aincremental%2520learning%2520as%2520the%2520process%2520of%2520learning%2520distinct%2520task%2520vectors%2520and%250Aencoding%2520them%2520within%2520different%2520LoRA%2520parameters.%2520Accordingly%252C%2520we%2520propose%250AIncremental%2520LoRA%2520to%2520mitigate%2520catastrophic%2520forgetting.%2520Experimental%2520results%2520on%250Astandard%2520datasets%2520indicate%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%250Aapproaches%2520significantly.%2520More%2520importantly%252C%2520our%2520method%2520exhibits%2520strong%250Arobustness%2520and%2520superiority%2520in%2520different%2520settings%2520and%2520degrees%2520of%2520data%250Aheterogeneity.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Ghy0501/PILoRA%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PILoRA%3A%20Prototype%20Guided%20Incremental%20LoRA%20for%20Federated%0A%20%20Class-Incremental%20Learning&entry.906535625=Haiyang%20Guo%20and%20Fei%20Zhu%20and%20Wenzhuo%20Liu%20and%20Xu-Yao%20Zhang%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Existing%20federated%20learning%20methods%20have%20effectively%20dealt%20with%20decentralized%0Alearning%20in%20scenarios%20involving%20data%20privacy%20and%20non-IID%20data.%20However%2C%20in%0Areal-world%20situations%2C%20each%20client%20dynamically%20learns%20new%20classes%2C%20requiring%0Athe%20global%20model%20to%20classify%20all%20seen%20classes.%20To%20effectively%20mitigate%0Acatastrophic%20forgetting%20and%20data%20heterogeneity%20under%20low%20communication%20costs%2C%0Awe%20propose%20a%20simple%20and%20effective%20method%20named%20PILoRA.%20On%20the%20one%20hand%2C%20we%0Aadopt%20prototype%20learning%20to%20learn%20better%20feature%20representations%20and%20leverage%0Athe%20heuristic%20information%20between%20prototypes%20and%20class%20features%20to%20design%20a%0Aprototype%20re-weight%20module%20to%20solve%20the%20classifier%20bias%20caused%20by%20data%0Aheterogeneity%20without%20retraining%20the%20classifier.%20On%20the%20other%20hand%2C%20we%20view%0Aincremental%20learning%20as%20the%20process%20of%20learning%20distinct%20task%20vectors%20and%0Aencoding%20them%20within%20different%20LoRA%20parameters.%20Accordingly%2C%20we%20propose%0AIncremental%20LoRA%20to%20mitigate%20catastrophic%20forgetting.%20Experimental%20results%20on%0Astandard%20datasets%20indicate%20that%20our%20method%20outperforms%20the%20state-of-the-art%0Aapproaches%20significantly.%20More%20importantly%2C%20our%20method%20exhibits%20strong%0Arobustness%20and%20superiority%20in%20different%20settings%20and%20degrees%20of%20data%0Aheterogeneity.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Ghy0501/PILoRA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02094v2&entry.124074799=Read"},
{"title": "Temporal Residual Guided Diffusion Framework for Event-Driven Video\n  Reconstruction", "author": "Lin Zhu and Yunlong Zheng and Yijun Zhang and Xiao Wang and Lizhi Wang and Hua Huang", "abstract": "  Event-based video reconstruction has garnered increasing attention due to its\nadvantages, such as high dynamic range and rapid motion capture capabilities.\nHowever, current methods often prioritize the extraction of temporal\ninformation from continuous event flow, leading to an overemphasis on\nlow-frequency texture features in the scene, resulting in over-smoothing and\nblurry artifacts. Addressing this challenge necessitates the integration of\nconditional information, encompassing temporal features, low-frequency texture,\nand high-frequency events, to guide the Denoising Diffusion Probabilistic Model\n(DDPM) in producing accurate and natural outputs. To tackle this issue, we\nintroduce a novel approach, the Temporal Residual Guided Diffusion Framework,\nwhich effectively leverages both temporal and frequency-based event priors. Our\nframework incorporates three key conditioning modules: a pre-trained\nlow-frequency intensity estimation module, a temporal recurrent encoder module,\nand an attention-based high-frequency prior enhancement module. In order to\ncapture temporal scene variations from the events at the current moment, we\nemploy a temporal-domain residual image as the target for the diffusion model.\nThrough the combination of these three conditioning paths and the temporal\nresidual framework, our framework excels in reconstructing high-quality videos\nfrom event flow, mitigating issues such as artifacts and over-smoothing\ncommonly observed in previous approaches. Extensive experiments conducted on\nmultiple benchmark datasets validate the superior performance of our framework\ncompared to prior event-based reconstruction methods.\n", "link": "http://arxiv.org/abs/2407.10636v1", "date": "2024-07-15", "relevancy": 2.4473, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6367}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6129}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Residual%20Guided%20Diffusion%20Framework%20for%20Event-Driven%20Video%0A%20%20Reconstruction&body=Title%3A%20Temporal%20Residual%20Guided%20Diffusion%20Framework%20for%20Event-Driven%20Video%0A%20%20Reconstruction%0AAuthor%3A%20Lin%20Zhu%20and%20Yunlong%20Zheng%20and%20Yijun%20Zhang%20and%20Xiao%20Wang%20and%20Lizhi%20Wang%20and%20Hua%20Huang%0AAbstract%3A%20%20%20Event-based%20video%20reconstruction%20has%20garnered%20increasing%20attention%20due%20to%20its%0Aadvantages%2C%20such%20as%20high%20dynamic%20range%20and%20rapid%20motion%20capture%20capabilities.%0AHowever%2C%20current%20methods%20often%20prioritize%20the%20extraction%20of%20temporal%0Ainformation%20from%20continuous%20event%20flow%2C%20leading%20to%20an%20overemphasis%20on%0Alow-frequency%20texture%20features%20in%20the%20scene%2C%20resulting%20in%20over-smoothing%20and%0Ablurry%20artifacts.%20Addressing%20this%20challenge%20necessitates%20the%20integration%20of%0Aconditional%20information%2C%20encompassing%20temporal%20features%2C%20low-frequency%20texture%2C%0Aand%20high-frequency%20events%2C%20to%20guide%20the%20Denoising%20Diffusion%20Probabilistic%20Model%0A%28DDPM%29%20in%20producing%20accurate%20and%20natural%20outputs.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20a%20novel%20approach%2C%20the%20Temporal%20Residual%20Guided%20Diffusion%20Framework%2C%0Awhich%20effectively%20leverages%20both%20temporal%20and%20frequency-based%20event%20priors.%20Our%0Aframework%20incorporates%20three%20key%20conditioning%20modules%3A%20a%20pre-trained%0Alow-frequency%20intensity%20estimation%20module%2C%20a%20temporal%20recurrent%20encoder%20module%2C%0Aand%20an%20attention-based%20high-frequency%20prior%20enhancement%20module.%20In%20order%20to%0Acapture%20temporal%20scene%20variations%20from%20the%20events%20at%20the%20current%20moment%2C%20we%0Aemploy%20a%20temporal-domain%20residual%20image%20as%20the%20target%20for%20the%20diffusion%20model.%0AThrough%20the%20combination%20of%20these%20three%20conditioning%20paths%20and%20the%20temporal%0Aresidual%20framework%2C%20our%20framework%20excels%20in%20reconstructing%20high-quality%20videos%0Afrom%20event%20flow%2C%20mitigating%20issues%20such%20as%20artifacts%20and%20over-smoothing%0Acommonly%20observed%20in%20previous%20approaches.%20Extensive%20experiments%20conducted%20on%0Amultiple%20benchmark%20datasets%20validate%20the%20superior%20performance%20of%20our%20framework%0Acompared%20to%20prior%20event-based%20reconstruction%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Residual%2520Guided%2520Diffusion%2520Framework%2520for%2520Event-Driven%2520Video%250A%2520%2520Reconstruction%26entry.906535625%3DLin%2520Zhu%2520and%2520Yunlong%2520Zheng%2520and%2520Yijun%2520Zhang%2520and%2520Xiao%2520Wang%2520and%2520Lizhi%2520Wang%2520and%2520Hua%2520Huang%26entry.1292438233%3D%2520%2520Event-based%2520video%2520reconstruction%2520has%2520garnered%2520increasing%2520attention%2520due%2520to%2520its%250Aadvantages%252C%2520such%2520as%2520high%2520dynamic%2520range%2520and%2520rapid%2520motion%2520capture%2520capabilities.%250AHowever%252C%2520current%2520methods%2520often%2520prioritize%2520the%2520extraction%2520of%2520temporal%250Ainformation%2520from%2520continuous%2520event%2520flow%252C%2520leading%2520to%2520an%2520overemphasis%2520on%250Alow-frequency%2520texture%2520features%2520in%2520the%2520scene%252C%2520resulting%2520in%2520over-smoothing%2520and%250Ablurry%2520artifacts.%2520Addressing%2520this%2520challenge%2520necessitates%2520the%2520integration%2520of%250Aconditional%2520information%252C%2520encompassing%2520temporal%2520features%252C%2520low-frequency%2520texture%252C%250Aand%2520high-frequency%2520events%252C%2520to%2520guide%2520the%2520Denoising%2520Diffusion%2520Probabilistic%2520Model%250A%2528DDPM%2529%2520in%2520producing%2520accurate%2520and%2520natural%2520outputs.%2520To%2520tackle%2520this%2520issue%252C%2520we%250Aintroduce%2520a%2520novel%2520approach%252C%2520the%2520Temporal%2520Residual%2520Guided%2520Diffusion%2520Framework%252C%250Awhich%2520effectively%2520leverages%2520both%2520temporal%2520and%2520frequency-based%2520event%2520priors.%2520Our%250Aframework%2520incorporates%2520three%2520key%2520conditioning%2520modules%253A%2520a%2520pre-trained%250Alow-frequency%2520intensity%2520estimation%2520module%252C%2520a%2520temporal%2520recurrent%2520encoder%2520module%252C%250Aand%2520an%2520attention-based%2520high-frequency%2520prior%2520enhancement%2520module.%2520In%2520order%2520to%250Acapture%2520temporal%2520scene%2520variations%2520from%2520the%2520events%2520at%2520the%2520current%2520moment%252C%2520we%250Aemploy%2520a%2520temporal-domain%2520residual%2520image%2520as%2520the%2520target%2520for%2520the%2520diffusion%2520model.%250AThrough%2520the%2520combination%2520of%2520these%2520three%2520conditioning%2520paths%2520and%2520the%2520temporal%250Aresidual%2520framework%252C%2520our%2520framework%2520excels%2520in%2520reconstructing%2520high-quality%2520videos%250Afrom%2520event%2520flow%252C%2520mitigating%2520issues%2520such%2520as%2520artifacts%2520and%2520over-smoothing%250Acommonly%2520observed%2520in%2520previous%2520approaches.%2520Extensive%2520experiments%2520conducted%2520on%250Amultiple%2520benchmark%2520datasets%2520validate%2520the%2520superior%2520performance%2520of%2520our%2520framework%250Acompared%2520to%2520prior%2520event-based%2520reconstruction%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Residual%20Guided%20Diffusion%20Framework%20for%20Event-Driven%20Video%0A%20%20Reconstruction&entry.906535625=Lin%20Zhu%20and%20Yunlong%20Zheng%20and%20Yijun%20Zhang%20and%20Xiao%20Wang%20and%20Lizhi%20Wang%20and%20Hua%20Huang&entry.1292438233=%20%20Event-based%20video%20reconstruction%20has%20garnered%20increasing%20attention%20due%20to%20its%0Aadvantages%2C%20such%20as%20high%20dynamic%20range%20and%20rapid%20motion%20capture%20capabilities.%0AHowever%2C%20current%20methods%20often%20prioritize%20the%20extraction%20of%20temporal%0Ainformation%20from%20continuous%20event%20flow%2C%20leading%20to%20an%20overemphasis%20on%0Alow-frequency%20texture%20features%20in%20the%20scene%2C%20resulting%20in%20over-smoothing%20and%0Ablurry%20artifacts.%20Addressing%20this%20challenge%20necessitates%20the%20integration%20of%0Aconditional%20information%2C%20encompassing%20temporal%20features%2C%20low-frequency%20texture%2C%0Aand%20high-frequency%20events%2C%20to%20guide%20the%20Denoising%20Diffusion%20Probabilistic%20Model%0A%28DDPM%29%20in%20producing%20accurate%20and%20natural%20outputs.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20a%20novel%20approach%2C%20the%20Temporal%20Residual%20Guided%20Diffusion%20Framework%2C%0Awhich%20effectively%20leverages%20both%20temporal%20and%20frequency-based%20event%20priors.%20Our%0Aframework%20incorporates%20three%20key%20conditioning%20modules%3A%20a%20pre-trained%0Alow-frequency%20intensity%20estimation%20module%2C%20a%20temporal%20recurrent%20encoder%20module%2C%0Aand%20an%20attention-based%20high-frequency%20prior%20enhancement%20module.%20In%20order%20to%0Acapture%20temporal%20scene%20variations%20from%20the%20events%20at%20the%20current%20moment%2C%20we%0Aemploy%20a%20temporal-domain%20residual%20image%20as%20the%20target%20for%20the%20diffusion%20model.%0AThrough%20the%20combination%20of%20these%20three%20conditioning%20paths%20and%20the%20temporal%0Aresidual%20framework%2C%20our%20framework%20excels%20in%20reconstructing%20high-quality%20videos%0Afrom%20event%20flow%2C%20mitigating%20issues%20such%20as%20artifacts%20and%20over-smoothing%0Acommonly%20observed%20in%20previous%20approaches.%20Extensive%20experiments%20conducted%20on%0Amultiple%20benchmark%20datasets%20validate%20the%20superior%20performance%20of%20our%20framework%0Acompared%20to%20prior%20event-based%20reconstruction%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10636v1&entry.124074799=Read"},
{"title": "GRIP: Generating Interaction Poses Using Spatial Cues and Latent\n  Consistency", "author": "Omid Taheri and Yi Zhou and Dimitrios Tzionas and Yang Zhou and Duygu Ceylan and Soren Pirk and Michael J. Black", "abstract": "  Hands are dexterous and highly versatile manipulators that are central to how\nhumans interact with objects and their environment. Consequently, modeling\nrealistic hand-object interactions, including the subtle motion of individual\nfingers, is critical for applications in computer graphics, computer vision,\nand mixed reality. Prior work on capturing and modeling humans interacting with\nobjects in 3D focuses on the body and object motion, often ignoring hand pose.\nIn contrast, we introduce GRIP, a learning-based method that takes, as input,\nthe 3D motion of the body and the object, and synthesizes realistic motion for\nboth hands before, during, and after object interaction. As a preliminary step\nbefore synthesizing the hand motion, we first use a network, ANet, to denoise\nthe arm motion. Then, we leverage the spatio-temporal relationship between the\nbody and the object to extract two types of novel temporal interaction cues,\nand use them in a two-stage inference pipeline to generate the hand motion. In\nthe first stage, we introduce a new approach to enforce motion temporal\nconsistency in the latent space (LTC), and generate consistent interaction\nmotions. In the second stage, GRIP generates refined hand poses to avoid\nhand-object penetrations. Given sequences of noisy body and object motion, GRIP\nupgrades them to include hand-object interaction. Quantitative experiments and\nperceptual studies demonstrate that GRIP outperforms baseline methods and\ngeneralizes to unseen objects and motions from different motion-capture\ndatasets.\n", "link": "http://arxiv.org/abs/2308.11617v2", "date": "2024-07-15", "relevancy": 2.4462, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6583}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5783}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRIP%3A%20Generating%20Interaction%20Poses%20Using%20Spatial%20Cues%20and%20Latent%0A%20%20Consistency&body=Title%3A%20GRIP%3A%20Generating%20Interaction%20Poses%20Using%20Spatial%20Cues%20and%20Latent%0A%20%20Consistency%0AAuthor%3A%20Omid%20Taheri%20and%20Yi%20Zhou%20and%20Dimitrios%20Tzionas%20and%20Yang%20Zhou%20and%20Duygu%20Ceylan%20and%20Soren%20Pirk%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20Hands%20are%20dexterous%20and%20highly%20versatile%20manipulators%20that%20are%20central%20to%20how%0Ahumans%20interact%20with%20objects%20and%20their%20environment.%20Consequently%2C%20modeling%0Arealistic%20hand-object%20interactions%2C%20including%20the%20subtle%20motion%20of%20individual%0Afingers%2C%20is%20critical%20for%20applications%20in%20computer%20graphics%2C%20computer%20vision%2C%0Aand%20mixed%20reality.%20Prior%20work%20on%20capturing%20and%20modeling%20humans%20interacting%20with%0Aobjects%20in%203D%20focuses%20on%20the%20body%20and%20object%20motion%2C%20often%20ignoring%20hand%20pose.%0AIn%20contrast%2C%20we%20introduce%20GRIP%2C%20a%20learning-based%20method%20that%20takes%2C%20as%20input%2C%0Athe%203D%20motion%20of%20the%20body%20and%20the%20object%2C%20and%20synthesizes%20realistic%20motion%20for%0Aboth%20hands%20before%2C%20during%2C%20and%20after%20object%20interaction.%20As%20a%20preliminary%20step%0Abefore%20synthesizing%20the%20hand%20motion%2C%20we%20first%20use%20a%20network%2C%20ANet%2C%20to%20denoise%0Athe%20arm%20motion.%20Then%2C%20we%20leverage%20the%20spatio-temporal%20relationship%20between%20the%0Abody%20and%20the%20object%20to%20extract%20two%20types%20of%20novel%20temporal%20interaction%20cues%2C%0Aand%20use%20them%20in%20a%20two-stage%20inference%20pipeline%20to%20generate%20the%20hand%20motion.%20In%0Athe%20first%20stage%2C%20we%20introduce%20a%20new%20approach%20to%20enforce%20motion%20temporal%0Aconsistency%20in%20the%20latent%20space%20%28LTC%29%2C%20and%20generate%20consistent%20interaction%0Amotions.%20In%20the%20second%20stage%2C%20GRIP%20generates%20refined%20hand%20poses%20to%20avoid%0Ahand-object%20penetrations.%20Given%20sequences%20of%20noisy%20body%20and%20object%20motion%2C%20GRIP%0Aupgrades%20them%20to%20include%20hand-object%20interaction.%20Quantitative%20experiments%20and%0Aperceptual%20studies%20demonstrate%20that%20GRIP%20outperforms%20baseline%20methods%20and%0Ageneralizes%20to%20unseen%20objects%20and%20motions%20from%20different%20motion-capture%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.11617v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRIP%253A%2520Generating%2520Interaction%2520Poses%2520Using%2520Spatial%2520Cues%2520and%2520Latent%250A%2520%2520Consistency%26entry.906535625%3DOmid%2520Taheri%2520and%2520Yi%2520Zhou%2520and%2520Dimitrios%2520Tzionas%2520and%2520Yang%2520Zhou%2520and%2520Duygu%2520Ceylan%2520and%2520Soren%2520Pirk%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520Hands%2520are%2520dexterous%2520and%2520highly%2520versatile%2520manipulators%2520that%2520are%2520central%2520to%2520how%250Ahumans%2520interact%2520with%2520objects%2520and%2520their%2520environment.%2520Consequently%252C%2520modeling%250Arealistic%2520hand-object%2520interactions%252C%2520including%2520the%2520subtle%2520motion%2520of%2520individual%250Afingers%252C%2520is%2520critical%2520for%2520applications%2520in%2520computer%2520graphics%252C%2520computer%2520vision%252C%250Aand%2520mixed%2520reality.%2520Prior%2520work%2520on%2520capturing%2520and%2520modeling%2520humans%2520interacting%2520with%250Aobjects%2520in%25203D%2520focuses%2520on%2520the%2520body%2520and%2520object%2520motion%252C%2520often%2520ignoring%2520hand%2520pose.%250AIn%2520contrast%252C%2520we%2520introduce%2520GRIP%252C%2520a%2520learning-based%2520method%2520that%2520takes%252C%2520as%2520input%252C%250Athe%25203D%2520motion%2520of%2520the%2520body%2520and%2520the%2520object%252C%2520and%2520synthesizes%2520realistic%2520motion%2520for%250Aboth%2520hands%2520before%252C%2520during%252C%2520and%2520after%2520object%2520interaction.%2520As%2520a%2520preliminary%2520step%250Abefore%2520synthesizing%2520the%2520hand%2520motion%252C%2520we%2520first%2520use%2520a%2520network%252C%2520ANet%252C%2520to%2520denoise%250Athe%2520arm%2520motion.%2520Then%252C%2520we%2520leverage%2520the%2520spatio-temporal%2520relationship%2520between%2520the%250Abody%2520and%2520the%2520object%2520to%2520extract%2520two%2520types%2520of%2520novel%2520temporal%2520interaction%2520cues%252C%250Aand%2520use%2520them%2520in%2520a%2520two-stage%2520inference%2520pipeline%2520to%2520generate%2520the%2520hand%2520motion.%2520In%250Athe%2520first%2520stage%252C%2520we%2520introduce%2520a%2520new%2520approach%2520to%2520enforce%2520motion%2520temporal%250Aconsistency%2520in%2520the%2520latent%2520space%2520%2528LTC%2529%252C%2520and%2520generate%2520consistent%2520interaction%250Amotions.%2520In%2520the%2520second%2520stage%252C%2520GRIP%2520generates%2520refined%2520hand%2520poses%2520to%2520avoid%250Ahand-object%2520penetrations.%2520Given%2520sequences%2520of%2520noisy%2520body%2520and%2520object%2520motion%252C%2520GRIP%250Aupgrades%2520them%2520to%2520include%2520hand-object%2520interaction.%2520Quantitative%2520experiments%2520and%250Aperceptual%2520studies%2520demonstrate%2520that%2520GRIP%2520outperforms%2520baseline%2520methods%2520and%250Ageneralizes%2520to%2520unseen%2520objects%2520and%2520motions%2520from%2520different%2520motion-capture%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.11617v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRIP%3A%20Generating%20Interaction%20Poses%20Using%20Spatial%20Cues%20and%20Latent%0A%20%20Consistency&entry.906535625=Omid%20Taheri%20and%20Yi%20Zhou%20and%20Dimitrios%20Tzionas%20and%20Yang%20Zhou%20and%20Duygu%20Ceylan%20and%20Soren%20Pirk%20and%20Michael%20J.%20Black&entry.1292438233=%20%20Hands%20are%20dexterous%20and%20highly%20versatile%20manipulators%20that%20are%20central%20to%20how%0Ahumans%20interact%20with%20objects%20and%20their%20environment.%20Consequently%2C%20modeling%0Arealistic%20hand-object%20interactions%2C%20including%20the%20subtle%20motion%20of%20individual%0Afingers%2C%20is%20critical%20for%20applications%20in%20computer%20graphics%2C%20computer%20vision%2C%0Aand%20mixed%20reality.%20Prior%20work%20on%20capturing%20and%20modeling%20humans%20interacting%20with%0Aobjects%20in%203D%20focuses%20on%20the%20body%20and%20object%20motion%2C%20often%20ignoring%20hand%20pose.%0AIn%20contrast%2C%20we%20introduce%20GRIP%2C%20a%20learning-based%20method%20that%20takes%2C%20as%20input%2C%0Athe%203D%20motion%20of%20the%20body%20and%20the%20object%2C%20and%20synthesizes%20realistic%20motion%20for%0Aboth%20hands%20before%2C%20during%2C%20and%20after%20object%20interaction.%20As%20a%20preliminary%20step%0Abefore%20synthesizing%20the%20hand%20motion%2C%20we%20first%20use%20a%20network%2C%20ANet%2C%20to%20denoise%0Athe%20arm%20motion.%20Then%2C%20we%20leverage%20the%20spatio-temporal%20relationship%20between%20the%0Abody%20and%20the%20object%20to%20extract%20two%20types%20of%20novel%20temporal%20interaction%20cues%2C%0Aand%20use%20them%20in%20a%20two-stage%20inference%20pipeline%20to%20generate%20the%20hand%20motion.%20In%0Athe%20first%20stage%2C%20we%20introduce%20a%20new%20approach%20to%20enforce%20motion%20temporal%0Aconsistency%20in%20the%20latent%20space%20%28LTC%29%2C%20and%20generate%20consistent%20interaction%0Amotions.%20In%20the%20second%20stage%2C%20GRIP%20generates%20refined%20hand%20poses%20to%20avoid%0Ahand-object%20penetrations.%20Given%20sequences%20of%20noisy%20body%20and%20object%20motion%2C%20GRIP%0Aupgrades%20them%20to%20include%20hand-object%20interaction.%20Quantitative%20experiments%20and%0Aperceptual%20studies%20demonstrate%20that%20GRIP%20outperforms%20baseline%20methods%20and%0Ageneralizes%20to%20unseen%20objects%20and%20motions%20from%20different%20motion-capture%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.11617v2&entry.124074799=Read"},
{"title": "Knowledge Distillation Meets Open-Set Semi-Supervised Learning", "author": "Jing Yang and Xiatian Zhu and Adrian Bulat and Brais Martinez and Georgios Tzimiropoulos", "abstract": "  Existing knowledge distillation methods mostly focus on distillation of\nteacher's prediction and intermediate activation. However, the structured\nrepresentation, which arguably is one of the most critical ingredients of deep\nmodels, is largely overlooked. In this work, we propose a novel {\\em\n\\modelname{}} ({\\bf\\em \\shortname{})} method dedicated for distilling\nrepresentational knowledge semantically from a pretrained teacher to a target\nstudent. The key idea is that we leverage the teacher's classifier as a\nsemantic critic for evaluating the representations of both teacher and student\nand distilling the semantic knowledge with high-order structured information\nover all feature dimensions. This is accomplished by introducing a notion of\ncross-network logit computed through passing student's representation into\nteacher's classifier. Further, considering the set of seen classes as a basis\nfor the semantic space in a combinatorial perspective, we scale \\shortname{} to\nunseen classes for enabling effective exploitation of largely available,\narbitrary unlabeled training data. At the problem level, this establishes an\ninteresting connection between knowledge distillation with open-set\nsemi-supervised learning (SSL). Extensive experiments show that our\n\\shortname{} outperforms significantly previous state-of-the-art knowledge\ndistillation methods on both coarse object classification and fine face\nrecognition tasks, as well as less studied yet practically crucial binary\nnetwork distillation. Under more realistic open-set SSL settings we introduce,\nwe reveal that knowledge distillation is generally more effective than existing\nOut-Of-Distribution (OOD) sample detection, and our proposed \\shortname{} is\nsuperior over both previous distillation and SSL competitors. The source code\nis available at \\url{https://github.com/jingyang2017/SRD\\_ossl}.\n", "link": "http://arxiv.org/abs/2205.06701v2", "date": "2024-07-15", "relevancy": 2.4427, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4941}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Distillation%20Meets%20Open-Set%20Semi-Supervised%20Learning&body=Title%3A%20Knowledge%20Distillation%20Meets%20Open-Set%20Semi-Supervised%20Learning%0AAuthor%3A%20Jing%20Yang%20and%20Xiatian%20Zhu%20and%20Adrian%20Bulat%20and%20Brais%20Martinez%20and%20Georgios%20Tzimiropoulos%0AAbstract%3A%20%20%20Existing%20knowledge%20distillation%20methods%20mostly%20focus%20on%20distillation%20of%0Ateacher%27s%20prediction%20and%20intermediate%20activation.%20However%2C%20the%20structured%0Arepresentation%2C%20which%20arguably%20is%20one%20of%20the%20most%20critical%20ingredients%20of%20deep%0Amodels%2C%20is%20largely%20overlooked.%20In%20this%20work%2C%20we%20propose%20a%20novel%20%7B%5Cem%0A%5Cmodelname%7B%7D%7D%20%28%7B%5Cbf%5Cem%20%5Cshortname%7B%7D%29%7D%20method%20dedicated%20for%20distilling%0Arepresentational%20knowledge%20semantically%20from%20a%20pretrained%20teacher%20to%20a%20target%0Astudent.%20The%20key%20idea%20is%20that%20we%20leverage%20the%20teacher%27s%20classifier%20as%20a%0Asemantic%20critic%20for%20evaluating%20the%20representations%20of%20both%20teacher%20and%20student%0Aand%20distilling%20the%20semantic%20knowledge%20with%20high-order%20structured%20information%0Aover%20all%20feature%20dimensions.%20This%20is%20accomplished%20by%20introducing%20a%20notion%20of%0Across-network%20logit%20computed%20through%20passing%20student%27s%20representation%20into%0Ateacher%27s%20classifier.%20Further%2C%20considering%20the%20set%20of%20seen%20classes%20as%20a%20basis%0Afor%20the%20semantic%20space%20in%20a%20combinatorial%20perspective%2C%20we%20scale%20%5Cshortname%7B%7D%20to%0Aunseen%20classes%20for%20enabling%20effective%20exploitation%20of%20largely%20available%2C%0Aarbitrary%20unlabeled%20training%20data.%20At%20the%20problem%20level%2C%20this%20establishes%20an%0Ainteresting%20connection%20between%20knowledge%20distillation%20with%20open-set%0Asemi-supervised%20learning%20%28SSL%29.%20Extensive%20experiments%20show%20that%20our%0A%5Cshortname%7B%7D%20outperforms%20significantly%20previous%20state-of-the-art%20knowledge%0Adistillation%20methods%20on%20both%20coarse%20object%20classification%20and%20fine%20face%0Arecognition%20tasks%2C%20as%20well%20as%20less%20studied%20yet%20practically%20crucial%20binary%0Anetwork%20distillation.%20Under%20more%20realistic%20open-set%20SSL%20settings%20we%20introduce%2C%0Awe%20reveal%20that%20knowledge%20distillation%20is%20generally%20more%20effective%20than%20existing%0AOut-Of-Distribution%20%28OOD%29%20sample%20detection%2C%20and%20our%20proposed%20%5Cshortname%7B%7D%20is%0Asuperior%20over%20both%20previous%20distillation%20and%20SSL%20competitors.%20The%20source%20code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/jingyang2017/SRD%5C_ossl%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.06701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Distillation%2520Meets%2520Open-Set%2520Semi-Supervised%2520Learning%26entry.906535625%3DJing%2520Yang%2520and%2520Xiatian%2520Zhu%2520and%2520Adrian%2520Bulat%2520and%2520Brais%2520Martinez%2520and%2520Georgios%2520Tzimiropoulos%26entry.1292438233%3D%2520%2520Existing%2520knowledge%2520distillation%2520methods%2520mostly%2520focus%2520on%2520distillation%2520of%250Ateacher%2527s%2520prediction%2520and%2520intermediate%2520activation.%2520However%252C%2520the%2520structured%250Arepresentation%252C%2520which%2520arguably%2520is%2520one%2520of%2520the%2520most%2520critical%2520ingredients%2520of%2520deep%250Amodels%252C%2520is%2520largely%2520overlooked.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520%257B%255Cem%250A%255Cmodelname%257B%257D%257D%2520%2528%257B%255Cbf%255Cem%2520%255Cshortname%257B%257D%2529%257D%2520method%2520dedicated%2520for%2520distilling%250Arepresentational%2520knowledge%2520semantically%2520from%2520a%2520pretrained%2520teacher%2520to%2520a%2520target%250Astudent.%2520The%2520key%2520idea%2520is%2520that%2520we%2520leverage%2520the%2520teacher%2527s%2520classifier%2520as%2520a%250Asemantic%2520critic%2520for%2520evaluating%2520the%2520representations%2520of%2520both%2520teacher%2520and%2520student%250Aand%2520distilling%2520the%2520semantic%2520knowledge%2520with%2520high-order%2520structured%2520information%250Aover%2520all%2520feature%2520dimensions.%2520This%2520is%2520accomplished%2520by%2520introducing%2520a%2520notion%2520of%250Across-network%2520logit%2520computed%2520through%2520passing%2520student%2527s%2520representation%2520into%250Ateacher%2527s%2520classifier.%2520Further%252C%2520considering%2520the%2520set%2520of%2520seen%2520classes%2520as%2520a%2520basis%250Afor%2520the%2520semantic%2520space%2520in%2520a%2520combinatorial%2520perspective%252C%2520we%2520scale%2520%255Cshortname%257B%257D%2520to%250Aunseen%2520classes%2520for%2520enabling%2520effective%2520exploitation%2520of%2520largely%2520available%252C%250Aarbitrary%2520unlabeled%2520training%2520data.%2520At%2520the%2520problem%2520level%252C%2520this%2520establishes%2520an%250Ainteresting%2520connection%2520between%2520knowledge%2520distillation%2520with%2520open-set%250Asemi-supervised%2520learning%2520%2528SSL%2529.%2520Extensive%2520experiments%2520show%2520that%2520our%250A%255Cshortname%257B%257D%2520outperforms%2520significantly%2520previous%2520state-of-the-art%2520knowledge%250Adistillation%2520methods%2520on%2520both%2520coarse%2520object%2520classification%2520and%2520fine%2520face%250Arecognition%2520tasks%252C%2520as%2520well%2520as%2520less%2520studied%2520yet%2520practically%2520crucial%2520binary%250Anetwork%2520distillation.%2520Under%2520more%2520realistic%2520open-set%2520SSL%2520settings%2520we%2520introduce%252C%250Awe%2520reveal%2520that%2520knowledge%2520distillation%2520is%2520generally%2520more%2520effective%2520than%2520existing%250AOut-Of-Distribution%2520%2528OOD%2529%2520sample%2520detection%252C%2520and%2520our%2520proposed%2520%255Cshortname%257B%257D%2520is%250Asuperior%2520over%2520both%2520previous%2520distillation%2520and%2520SSL%2520competitors.%2520The%2520source%2520code%250Ais%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/jingyang2017/SRD%255C_ossl%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.06701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Distillation%20Meets%20Open-Set%20Semi-Supervised%20Learning&entry.906535625=Jing%20Yang%20and%20Xiatian%20Zhu%20and%20Adrian%20Bulat%20and%20Brais%20Martinez%20and%20Georgios%20Tzimiropoulos&entry.1292438233=%20%20Existing%20knowledge%20distillation%20methods%20mostly%20focus%20on%20distillation%20of%0Ateacher%27s%20prediction%20and%20intermediate%20activation.%20However%2C%20the%20structured%0Arepresentation%2C%20which%20arguably%20is%20one%20of%20the%20most%20critical%20ingredients%20of%20deep%0Amodels%2C%20is%20largely%20overlooked.%20In%20this%20work%2C%20we%20propose%20a%20novel%20%7B%5Cem%0A%5Cmodelname%7B%7D%7D%20%28%7B%5Cbf%5Cem%20%5Cshortname%7B%7D%29%7D%20method%20dedicated%20for%20distilling%0Arepresentational%20knowledge%20semantically%20from%20a%20pretrained%20teacher%20to%20a%20target%0Astudent.%20The%20key%20idea%20is%20that%20we%20leverage%20the%20teacher%27s%20classifier%20as%20a%0Asemantic%20critic%20for%20evaluating%20the%20representations%20of%20both%20teacher%20and%20student%0Aand%20distilling%20the%20semantic%20knowledge%20with%20high-order%20structured%20information%0Aover%20all%20feature%20dimensions.%20This%20is%20accomplished%20by%20introducing%20a%20notion%20of%0Across-network%20logit%20computed%20through%20passing%20student%27s%20representation%20into%0Ateacher%27s%20classifier.%20Further%2C%20considering%20the%20set%20of%20seen%20classes%20as%20a%20basis%0Afor%20the%20semantic%20space%20in%20a%20combinatorial%20perspective%2C%20we%20scale%20%5Cshortname%7B%7D%20to%0Aunseen%20classes%20for%20enabling%20effective%20exploitation%20of%20largely%20available%2C%0Aarbitrary%20unlabeled%20training%20data.%20At%20the%20problem%20level%2C%20this%20establishes%20an%0Ainteresting%20connection%20between%20knowledge%20distillation%20with%20open-set%0Asemi-supervised%20learning%20%28SSL%29.%20Extensive%20experiments%20show%20that%20our%0A%5Cshortname%7B%7D%20outperforms%20significantly%20previous%20state-of-the-art%20knowledge%0Adistillation%20methods%20on%20both%20coarse%20object%20classification%20and%20fine%20face%0Arecognition%20tasks%2C%20as%20well%20as%20less%20studied%20yet%20practically%20crucial%20binary%0Anetwork%20distillation.%20Under%20more%20realistic%20open-set%20SSL%20settings%20we%20introduce%2C%0Awe%20reveal%20that%20knowledge%20distillation%20is%20generally%20more%20effective%20than%20existing%0AOut-Of-Distribution%20%28OOD%29%20sample%20detection%2C%20and%20our%20proposed%20%5Cshortname%7B%7D%20is%0Asuperior%20over%20both%20previous%20distillation%20and%20SSL%20competitors.%20The%20source%20code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/jingyang2017/SRD%5C_ossl%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.06701v2&entry.124074799=Read"},
{"title": "Interpreting Hand gestures using Object Detection and Digits\n  Classification", "author": "Sangeetha K and Balaji VS and Kamalesh P and Anirudh Ganapathy PS", "abstract": "  Hand gestures have evolved into a natural and intuitive means of engaging\nwith technology. The objective of this research is to develop a robust system\nthat can accurately recognize and classify hand gestures representing numbers.\nThe proposed approach involves collecting a dataset of hand gesture images,\npreprocessing and enhancing the images, extracting relevant features, and\ntraining a machine learning model. The advancement of computer vision\ntechnology and object detection techniques, in conjunction with OpenCV's\ncapability to analyze and comprehend hand gestures, presents a chance to\ntransform the identification of numerical digits and its potential\napplications. The advancement of computer vision technology and object\nidentification technologies, along with OpenCV's capacity to analyze and\ninterpret hand gestures, has the potential to revolutionize human interaction,\nboosting people's access to information, education, and employment\nopportunities. Keywords: Computer Vision, Machine learning, Deep Learning,\nNeural Networks\n", "link": "http://arxiv.org/abs/2407.10902v1", "date": "2024-07-15", "relevancy": 2.4398, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5062}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4912}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Hand%20gestures%20using%20Object%20Detection%20and%20Digits%0A%20%20Classification&body=Title%3A%20Interpreting%20Hand%20gestures%20using%20Object%20Detection%20and%20Digits%0A%20%20Classification%0AAuthor%3A%20Sangeetha%20K%20and%20Balaji%20VS%20and%20Kamalesh%20P%20and%20Anirudh%20Ganapathy%20PS%0AAbstract%3A%20%20%20Hand%20gestures%20have%20evolved%20into%20a%20natural%20and%20intuitive%20means%20of%20engaging%0Awith%20technology.%20The%20objective%20of%20this%20research%20is%20to%20develop%20a%20robust%20system%0Athat%20can%20accurately%20recognize%20and%20classify%20hand%20gestures%20representing%20numbers.%0AThe%20proposed%20approach%20involves%20collecting%20a%20dataset%20of%20hand%20gesture%20images%2C%0Apreprocessing%20and%20enhancing%20the%20images%2C%20extracting%20relevant%20features%2C%20and%0Atraining%20a%20machine%20learning%20model.%20The%20advancement%20of%20computer%20vision%0Atechnology%20and%20object%20detection%20techniques%2C%20in%20conjunction%20with%20OpenCV%27s%0Acapability%20to%20analyze%20and%20comprehend%20hand%20gestures%2C%20presents%20a%20chance%20to%0Atransform%20the%20identification%20of%20numerical%20digits%20and%20its%20potential%0Aapplications.%20The%20advancement%20of%20computer%20vision%20technology%20and%20object%0Aidentification%20technologies%2C%20along%20with%20OpenCV%27s%20capacity%20to%20analyze%20and%0Ainterpret%20hand%20gestures%2C%20has%20the%20potential%20to%20revolutionize%20human%20interaction%2C%0Aboosting%20people%27s%20access%20to%20information%2C%20education%2C%20and%20employment%0Aopportunities.%20Keywords%3A%20Computer%20Vision%2C%20Machine%20learning%2C%20Deep%20Learning%2C%0ANeural%20Networks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Hand%2520gestures%2520using%2520Object%2520Detection%2520and%2520Digits%250A%2520%2520Classification%26entry.906535625%3DSangeetha%2520K%2520and%2520Balaji%2520VS%2520and%2520Kamalesh%2520P%2520and%2520Anirudh%2520Ganapathy%2520PS%26entry.1292438233%3D%2520%2520Hand%2520gestures%2520have%2520evolved%2520into%2520a%2520natural%2520and%2520intuitive%2520means%2520of%2520engaging%250Awith%2520technology.%2520The%2520objective%2520of%2520this%2520research%2520is%2520to%2520develop%2520a%2520robust%2520system%250Athat%2520can%2520accurately%2520recognize%2520and%2520classify%2520hand%2520gestures%2520representing%2520numbers.%250AThe%2520proposed%2520approach%2520involves%2520collecting%2520a%2520dataset%2520of%2520hand%2520gesture%2520images%252C%250Apreprocessing%2520and%2520enhancing%2520the%2520images%252C%2520extracting%2520relevant%2520features%252C%2520and%250Atraining%2520a%2520machine%2520learning%2520model.%2520The%2520advancement%2520of%2520computer%2520vision%250Atechnology%2520and%2520object%2520detection%2520techniques%252C%2520in%2520conjunction%2520with%2520OpenCV%2527s%250Acapability%2520to%2520analyze%2520and%2520comprehend%2520hand%2520gestures%252C%2520presents%2520a%2520chance%2520to%250Atransform%2520the%2520identification%2520of%2520numerical%2520digits%2520and%2520its%2520potential%250Aapplications.%2520The%2520advancement%2520of%2520computer%2520vision%2520technology%2520and%2520object%250Aidentification%2520technologies%252C%2520along%2520with%2520OpenCV%2527s%2520capacity%2520to%2520analyze%2520and%250Ainterpret%2520hand%2520gestures%252C%2520has%2520the%2520potential%2520to%2520revolutionize%2520human%2520interaction%252C%250Aboosting%2520people%2527s%2520access%2520to%2520information%252C%2520education%252C%2520and%2520employment%250Aopportunities.%2520Keywords%253A%2520Computer%2520Vision%252C%2520Machine%2520learning%252C%2520Deep%2520Learning%252C%250ANeural%2520Networks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Hand%20gestures%20using%20Object%20Detection%20and%20Digits%0A%20%20Classification&entry.906535625=Sangeetha%20K%20and%20Balaji%20VS%20and%20Kamalesh%20P%20and%20Anirudh%20Ganapathy%20PS&entry.1292438233=%20%20Hand%20gestures%20have%20evolved%20into%20a%20natural%20and%20intuitive%20means%20of%20engaging%0Awith%20technology.%20The%20objective%20of%20this%20research%20is%20to%20develop%20a%20robust%20system%0Athat%20can%20accurately%20recognize%20and%20classify%20hand%20gestures%20representing%20numbers.%0AThe%20proposed%20approach%20involves%20collecting%20a%20dataset%20of%20hand%20gesture%20images%2C%0Apreprocessing%20and%20enhancing%20the%20images%2C%20extracting%20relevant%20features%2C%20and%0Atraining%20a%20machine%20learning%20model.%20The%20advancement%20of%20computer%20vision%0Atechnology%20and%20object%20detection%20techniques%2C%20in%20conjunction%20with%20OpenCV%27s%0Acapability%20to%20analyze%20and%20comprehend%20hand%20gestures%2C%20presents%20a%20chance%20to%0Atransform%20the%20identification%20of%20numerical%20digits%20and%20its%20potential%0Aapplications.%20The%20advancement%20of%20computer%20vision%20technology%20and%20object%0Aidentification%20technologies%2C%20along%20with%20OpenCV%27s%20capacity%20to%20analyze%20and%0Ainterpret%20hand%20gestures%2C%20has%20the%20potential%20to%20revolutionize%20human%20interaction%2C%0Aboosting%20people%27s%20access%20to%20information%2C%20education%2C%20and%20employment%0Aopportunities.%20Keywords%3A%20Computer%20Vision%2C%20Machine%20learning%2C%20Deep%20Learning%2C%0ANeural%20Networks%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10902v1&entry.124074799=Read"},
{"title": "DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot\n  Video Editing", "author": "Hyeonho Jeong and Jinho Chang and Geon Yeong Park and Jong Chul Ye", "abstract": "  Text-driven diffusion-based video editing presents a unique challenge not\nencountered in image editing literature: establishing real-world motion. Unlike\nexisting video editing approaches, here we focus on score distillation sampling\nto circumvent the standard reverse diffusion process and initiate optimization\nfrom videos that already exhibit natural motion. Our analysis reveals that\nwhile video score distillation can effectively introduce new content indicated\nby target text, it can also cause significant structure and motion deviation.\nTo counteract this, we propose to match space-time self-similarities of the\noriginal video and the edited video during the score distillation. Thanks to\nthe use of score distillation, our approach is model-agnostic, which can be\napplied for both cascaded and non-cascaded video diffusion frameworks. Through\nextensive comparisons with leading methods, our approach demonstrates its\nsuperiority in altering appearances while accurately preserving the original\nstructure and motion.\n", "link": "http://arxiv.org/abs/2403.12002v2", "date": "2024-07-15", "relevancy": 2.4344, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6935}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6134}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamMotion%3A%20Space-Time%20Self-Similar%20Score%20Distillation%20for%20Zero-Shot%0A%20%20Video%20Editing&body=Title%3A%20DreamMotion%3A%20Space-Time%20Self-Similar%20Score%20Distillation%20for%20Zero-Shot%0A%20%20Video%20Editing%0AAuthor%3A%20Hyeonho%20Jeong%20and%20Jinho%20Chang%20and%20Geon%20Yeong%20Park%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20%20%20Text-driven%20diffusion-based%20video%20editing%20presents%20a%20unique%20challenge%20not%0Aencountered%20in%20image%20editing%20literature%3A%20establishing%20real-world%20motion.%20Unlike%0Aexisting%20video%20editing%20approaches%2C%20here%20we%20focus%20on%20score%20distillation%20sampling%0Ato%20circumvent%20the%20standard%20reverse%20diffusion%20process%20and%20initiate%20optimization%0Afrom%20videos%20that%20already%20exhibit%20natural%20motion.%20Our%20analysis%20reveals%20that%0Awhile%20video%20score%20distillation%20can%20effectively%20introduce%20new%20content%20indicated%0Aby%20target%20text%2C%20it%20can%20also%20cause%20significant%20structure%20and%20motion%20deviation.%0ATo%20counteract%20this%2C%20we%20propose%20to%20match%20space-time%20self-similarities%20of%20the%0Aoriginal%20video%20and%20the%20edited%20video%20during%20the%20score%20distillation.%20Thanks%20to%0Athe%20use%20of%20score%20distillation%2C%20our%20approach%20is%20model-agnostic%2C%20which%20can%20be%0Aapplied%20for%20both%20cascaded%20and%20non-cascaded%20video%20diffusion%20frameworks.%20Through%0Aextensive%20comparisons%20with%20leading%20methods%2C%20our%20approach%20demonstrates%20its%0Asuperiority%20in%20altering%20appearances%20while%20accurately%20preserving%20the%20original%0Astructure%20and%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamMotion%253A%2520Space-Time%2520Self-Similar%2520Score%2520Distillation%2520for%2520Zero-Shot%250A%2520%2520Video%2520Editing%26entry.906535625%3DHyeonho%2520Jeong%2520and%2520Jinho%2520Chang%2520and%2520Geon%2520Yeong%2520Park%2520and%2520Jong%2520Chul%2520Ye%26entry.1292438233%3D%2520%2520Text-driven%2520diffusion-based%2520video%2520editing%2520presents%2520a%2520unique%2520challenge%2520not%250Aencountered%2520in%2520image%2520editing%2520literature%253A%2520establishing%2520real-world%2520motion.%2520Unlike%250Aexisting%2520video%2520editing%2520approaches%252C%2520here%2520we%2520focus%2520on%2520score%2520distillation%2520sampling%250Ato%2520circumvent%2520the%2520standard%2520reverse%2520diffusion%2520process%2520and%2520initiate%2520optimization%250Afrom%2520videos%2520that%2520already%2520exhibit%2520natural%2520motion.%2520Our%2520analysis%2520reveals%2520that%250Awhile%2520video%2520score%2520distillation%2520can%2520effectively%2520introduce%2520new%2520content%2520indicated%250Aby%2520target%2520text%252C%2520it%2520can%2520also%2520cause%2520significant%2520structure%2520and%2520motion%2520deviation.%250ATo%2520counteract%2520this%252C%2520we%2520propose%2520to%2520match%2520space-time%2520self-similarities%2520of%2520the%250Aoriginal%2520video%2520and%2520the%2520edited%2520video%2520during%2520the%2520score%2520distillation.%2520Thanks%2520to%250Athe%2520use%2520of%2520score%2520distillation%252C%2520our%2520approach%2520is%2520model-agnostic%252C%2520which%2520can%2520be%250Aapplied%2520for%2520both%2520cascaded%2520and%2520non-cascaded%2520video%2520diffusion%2520frameworks.%2520Through%250Aextensive%2520comparisons%2520with%2520leading%2520methods%252C%2520our%2520approach%2520demonstrates%2520its%250Asuperiority%2520in%2520altering%2520appearances%2520while%2520accurately%2520preserving%2520the%2520original%250Astructure%2520and%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamMotion%3A%20Space-Time%20Self-Similar%20Score%20Distillation%20for%20Zero-Shot%0A%20%20Video%20Editing&entry.906535625=Hyeonho%20Jeong%20and%20Jinho%20Chang%20and%20Geon%20Yeong%20Park%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20Text-driven%20diffusion-based%20video%20editing%20presents%20a%20unique%20challenge%20not%0Aencountered%20in%20image%20editing%20literature%3A%20establishing%20real-world%20motion.%20Unlike%0Aexisting%20video%20editing%20approaches%2C%20here%20we%20focus%20on%20score%20distillation%20sampling%0Ato%20circumvent%20the%20standard%20reverse%20diffusion%20process%20and%20initiate%20optimization%0Afrom%20videos%20that%20already%20exhibit%20natural%20motion.%20Our%20analysis%20reveals%20that%0Awhile%20video%20score%20distillation%20can%20effectively%20introduce%20new%20content%20indicated%0Aby%20target%20text%2C%20it%20can%20also%20cause%20significant%20structure%20and%20motion%20deviation.%0ATo%20counteract%20this%2C%20we%20propose%20to%20match%20space-time%20self-similarities%20of%20the%0Aoriginal%20video%20and%20the%20edited%20video%20during%20the%20score%20distillation.%20Thanks%20to%0Athe%20use%20of%20score%20distillation%2C%20our%20approach%20is%20model-agnostic%2C%20which%20can%20be%0Aapplied%20for%20both%20cascaded%20and%20non-cascaded%20video%20diffusion%20frameworks.%20Through%0Aextensive%20comparisons%20with%20leading%20methods%2C%20our%20approach%20demonstrates%20its%0Asuperiority%20in%20altering%20appearances%20while%20accurately%20preserving%20the%20original%0Astructure%20and%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12002v2&entry.124074799=Read"},
{"title": "Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse\n  Problems", "author": "Hyungjin Chung and Jong Chul Ye", "abstract": "  Recent inverse problem solvers that leverage generative diffusion priors have\ngarnered significant attention due to their exceptional quality. However,\nadaptation of the prior is necessary when there exists a discrepancy between\nthe training and testing distributions. In this work, we propose deep diffusion\nimage prior (DDIP), which generalizes the recent adaptation method of SCD by\nintroducing a formal connection to the deep image prior. Under this framework,\nwe propose an efficient adaptation method dubbed D3IP, specified for 3D\nmeasurements, which accelerates DDIP by orders of magnitude while achieving\nsuperior performance. D3IP enables seamless integration of 3D inverse solvers\nand thus leads to coherent 3D reconstruction. Moreover, we show that\nmeta-learning techniques can also be applied to yield even better performance.\nWe show that our method is capable of solving diverse 3D reconstructive tasks\nfrom the generative prior trained only with phantom images that are vastly\ndifferent from the training set, opening up new opportunities of applying\ndiffusion inverse solvers even when training with gold standard data is\nimpossible. Code: https://github.com/HJ-harry/DDIP3D\n", "link": "http://arxiv.org/abs/2407.10641v1", "date": "2024-07-15", "relevancy": 2.4229, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.629}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6011}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Diffusion%20Image%20Prior%20for%20Efficient%20OOD%20Adaptation%20in%203D%20Inverse%0A%20%20Problems&body=Title%3A%20Deep%20Diffusion%20Image%20Prior%20for%20Efficient%20OOD%20Adaptation%20in%203D%20Inverse%0A%20%20Problems%0AAuthor%3A%20Hyungjin%20Chung%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20%20%20Recent%20inverse%20problem%20solvers%20that%20leverage%20generative%20diffusion%20priors%20have%0Agarnered%20significant%20attention%20due%20to%20their%20exceptional%20quality.%20However%2C%0Aadaptation%20of%20the%20prior%20is%20necessary%20when%20there%20exists%20a%20discrepancy%20between%0Athe%20training%20and%20testing%20distributions.%20In%20this%20work%2C%20we%20propose%20deep%20diffusion%0Aimage%20prior%20%28DDIP%29%2C%20which%20generalizes%20the%20recent%20adaptation%20method%20of%20SCD%20by%0Aintroducing%20a%20formal%20connection%20to%20the%20deep%20image%20prior.%20Under%20this%20framework%2C%0Awe%20propose%20an%20efficient%20adaptation%20method%20dubbed%20D3IP%2C%20specified%20for%203D%0Ameasurements%2C%20which%20accelerates%20DDIP%20by%20orders%20of%20magnitude%20while%20achieving%0Asuperior%20performance.%20D3IP%20enables%20seamless%20integration%20of%203D%20inverse%20solvers%0Aand%20thus%20leads%20to%20coherent%203D%20reconstruction.%20Moreover%2C%20we%20show%20that%0Ameta-learning%20techniques%20can%20also%20be%20applied%20to%20yield%20even%20better%20performance.%0AWe%20show%20that%20our%20method%20is%20capable%20of%20solving%20diverse%203D%20reconstructive%20tasks%0Afrom%20the%20generative%20prior%20trained%20only%20with%20phantom%20images%20that%20are%20vastly%0Adifferent%20from%20the%20training%20set%2C%20opening%20up%20new%20opportunities%20of%20applying%0Adiffusion%20inverse%20solvers%20even%20when%20training%20with%20gold%20standard%20data%20is%0Aimpossible.%20Code%3A%20https%3A//github.com/HJ-harry/DDIP3D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Diffusion%2520Image%2520Prior%2520for%2520Efficient%2520OOD%2520Adaptation%2520in%25203D%2520Inverse%250A%2520%2520Problems%26entry.906535625%3DHyungjin%2520Chung%2520and%2520Jong%2520Chul%2520Ye%26entry.1292438233%3D%2520%2520Recent%2520inverse%2520problem%2520solvers%2520that%2520leverage%2520generative%2520diffusion%2520priors%2520have%250Agarnered%2520significant%2520attention%2520due%2520to%2520their%2520exceptional%2520quality.%2520However%252C%250Aadaptation%2520of%2520the%2520prior%2520is%2520necessary%2520when%2520there%2520exists%2520a%2520discrepancy%2520between%250Athe%2520training%2520and%2520testing%2520distributions.%2520In%2520this%2520work%252C%2520we%2520propose%2520deep%2520diffusion%250Aimage%2520prior%2520%2528DDIP%2529%252C%2520which%2520generalizes%2520the%2520recent%2520adaptation%2520method%2520of%2520SCD%2520by%250Aintroducing%2520a%2520formal%2520connection%2520to%2520the%2520deep%2520image%2520prior.%2520Under%2520this%2520framework%252C%250Awe%2520propose%2520an%2520efficient%2520adaptation%2520method%2520dubbed%2520D3IP%252C%2520specified%2520for%25203D%250Ameasurements%252C%2520which%2520accelerates%2520DDIP%2520by%2520orders%2520of%2520magnitude%2520while%2520achieving%250Asuperior%2520performance.%2520D3IP%2520enables%2520seamless%2520integration%2520of%25203D%2520inverse%2520solvers%250Aand%2520thus%2520leads%2520to%2520coherent%25203D%2520reconstruction.%2520Moreover%252C%2520we%2520show%2520that%250Ameta-learning%2520techniques%2520can%2520also%2520be%2520applied%2520to%2520yield%2520even%2520better%2520performance.%250AWe%2520show%2520that%2520our%2520method%2520is%2520capable%2520of%2520solving%2520diverse%25203D%2520reconstructive%2520tasks%250Afrom%2520the%2520generative%2520prior%2520trained%2520only%2520with%2520phantom%2520images%2520that%2520are%2520vastly%250Adifferent%2520from%2520the%2520training%2520set%252C%2520opening%2520up%2520new%2520opportunities%2520of%2520applying%250Adiffusion%2520inverse%2520solvers%2520even%2520when%2520training%2520with%2520gold%2520standard%2520data%2520is%250Aimpossible.%2520Code%253A%2520https%253A//github.com/HJ-harry/DDIP3D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Diffusion%20Image%20Prior%20for%20Efficient%20OOD%20Adaptation%20in%203D%20Inverse%0A%20%20Problems&entry.906535625=Hyungjin%20Chung%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20Recent%20inverse%20problem%20solvers%20that%20leverage%20generative%20diffusion%20priors%20have%0Agarnered%20significant%20attention%20due%20to%20their%20exceptional%20quality.%20However%2C%0Aadaptation%20of%20the%20prior%20is%20necessary%20when%20there%20exists%20a%20discrepancy%20between%0Athe%20training%20and%20testing%20distributions.%20In%20this%20work%2C%20we%20propose%20deep%20diffusion%0Aimage%20prior%20%28DDIP%29%2C%20which%20generalizes%20the%20recent%20adaptation%20method%20of%20SCD%20by%0Aintroducing%20a%20formal%20connection%20to%20the%20deep%20image%20prior.%20Under%20this%20framework%2C%0Awe%20propose%20an%20efficient%20adaptation%20method%20dubbed%20D3IP%2C%20specified%20for%203D%0Ameasurements%2C%20which%20accelerates%20DDIP%20by%20orders%20of%20magnitude%20while%20achieving%0Asuperior%20performance.%20D3IP%20enables%20seamless%20integration%20of%203D%20inverse%20solvers%0Aand%20thus%20leads%20to%20coherent%203D%20reconstruction.%20Moreover%2C%20we%20show%20that%0Ameta-learning%20techniques%20can%20also%20be%20applied%20to%20yield%20even%20better%20performance.%0AWe%20show%20that%20our%20method%20is%20capable%20of%20solving%20diverse%203D%20reconstructive%20tasks%0Afrom%20the%20generative%20prior%20trained%20only%20with%20phantom%20images%20that%20are%20vastly%0Adifferent%20from%20the%20training%20set%2C%20opening%20up%20new%20opportunities%20of%20applying%0Adiffusion%20inverse%20solvers%20even%20when%20training%20with%20gold%20standard%20data%20is%0Aimpossible.%20Code%3A%20https%3A//github.com/HJ-harry/DDIP3D%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10641v1&entry.124074799=Read"},
{"title": "An evaluation of CNN models and data augmentation techniques in\n  hierarchical localization of mobile robots", "author": "J. J. Cabrera and O. J. C\u00e9spedes and S. Cebollada and O. Reinoso and L. Pay\u00e1", "abstract": "  This work presents an evaluation of CNN models and data augmentation to carry\nout the hierarchical localization of a mobile robot by using omnidireccional\nimages. In this sense, an ablation study of different state-of-the-art CNN\nmodels used as backbone is presented and a variety of data augmentation visual\neffects are proposed for addressing the visual localization of the robot. The\nproposed method is based on the adaption and re-training of a CNN with a dual\npurpose: (1) to perform a rough localization step in which the model is used to\npredict the room from which an image was captured, and (2) to address the fine\nlocalization step, which consists in retrieving the most similar image of the\nvisual map among those contained in the previously predicted room by means of a\npairwise comparison between descriptors obtained from an intermediate layer of\nthe CNN. In this sense, we evaluate the impact of different state-of-the-art\nCNN models such as ConvNeXt for addressing the proposed localization. Finally,\na variety of data augmentation visual effects are separately employed for\ntraining the model and their impact is assessed. The performance of the\nresulting CNNs is evaluated under real operation conditions, including changes\nin the lighting conditions. Our code is publicly available on the project\nwebsite https://github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git\n", "link": "http://arxiv.org/abs/2407.10596v1", "date": "2024-07-15", "relevancy": 2.3919, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6142}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5978}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20evaluation%20of%20CNN%20models%20and%20data%20augmentation%20techniques%20in%0A%20%20hierarchical%20localization%20of%20mobile%20robots&body=Title%3A%20An%20evaluation%20of%20CNN%20models%20and%20data%20augmentation%20techniques%20in%0A%20%20hierarchical%20localization%20of%20mobile%20robots%0AAuthor%3A%20J.%20J.%20Cabrera%20and%20O.%20J.%20C%C3%A9spedes%20and%20S.%20Cebollada%20and%20O.%20Reinoso%20and%20L.%20Pay%C3%A1%0AAbstract%3A%20%20%20This%20work%20presents%20an%20evaluation%20of%20CNN%20models%20and%20data%20augmentation%20to%20carry%0Aout%20the%20hierarchical%20localization%20of%20a%20mobile%20robot%20by%20using%20omnidireccional%0Aimages.%20In%20this%20sense%2C%20an%20ablation%20study%20of%20different%20state-of-the-art%20CNN%0Amodels%20used%20as%20backbone%20is%20presented%20and%20a%20variety%20of%20data%20augmentation%20visual%0Aeffects%20are%20proposed%20for%20addressing%20the%20visual%20localization%20of%20the%20robot.%20The%0Aproposed%20method%20is%20based%20on%20the%20adaption%20and%20re-training%20of%20a%20CNN%20with%20a%20dual%0Apurpose%3A%20%281%29%20to%20perform%20a%20rough%20localization%20step%20in%20which%20the%20model%20is%20used%20to%0Apredict%20the%20room%20from%20which%20an%20image%20was%20captured%2C%20and%20%282%29%20to%20address%20the%20fine%0Alocalization%20step%2C%20which%20consists%20in%20retrieving%20the%20most%20similar%20image%20of%20the%0Avisual%20map%20among%20those%20contained%20in%20the%20previously%20predicted%20room%20by%20means%20of%20a%0Apairwise%20comparison%20between%20descriptors%20obtained%20from%20an%20intermediate%20layer%20of%0Athe%20CNN.%20In%20this%20sense%2C%20we%20evaluate%20the%20impact%20of%20different%20state-of-the-art%0ACNN%20models%20such%20as%20ConvNeXt%20for%20addressing%20the%20proposed%20localization.%20Finally%2C%0Aa%20variety%20of%20data%20augmentation%20visual%20effects%20are%20separately%20employed%20for%0Atraining%20the%20model%20and%20their%20impact%20is%20assessed.%20The%20performance%20of%20the%0Aresulting%20CNNs%20is%20evaluated%20under%20real%20operation%20conditions%2C%20including%20changes%0Ain%20the%20lighting%20conditions.%20Our%20code%20is%20publicly%20available%20on%20the%20project%0Awebsite%20https%3A//github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520evaluation%2520of%2520CNN%2520models%2520and%2520data%2520augmentation%2520techniques%2520in%250A%2520%2520hierarchical%2520localization%2520of%2520mobile%2520robots%26entry.906535625%3DJ.%2520J.%2520Cabrera%2520and%2520O.%2520J.%2520C%25C3%25A9spedes%2520and%2520S.%2520Cebollada%2520and%2520O.%2520Reinoso%2520and%2520L.%2520Pay%25C3%25A1%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520an%2520evaluation%2520of%2520CNN%2520models%2520and%2520data%2520augmentation%2520to%2520carry%250Aout%2520the%2520hierarchical%2520localization%2520of%2520a%2520mobile%2520robot%2520by%2520using%2520omnidireccional%250Aimages.%2520In%2520this%2520sense%252C%2520an%2520ablation%2520study%2520of%2520different%2520state-of-the-art%2520CNN%250Amodels%2520used%2520as%2520backbone%2520is%2520presented%2520and%2520a%2520variety%2520of%2520data%2520augmentation%2520visual%250Aeffects%2520are%2520proposed%2520for%2520addressing%2520the%2520visual%2520localization%2520of%2520the%2520robot.%2520The%250Aproposed%2520method%2520is%2520based%2520on%2520the%2520adaption%2520and%2520re-training%2520of%2520a%2520CNN%2520with%2520a%2520dual%250Apurpose%253A%2520%25281%2529%2520to%2520perform%2520a%2520rough%2520localization%2520step%2520in%2520which%2520the%2520model%2520is%2520used%2520to%250Apredict%2520the%2520room%2520from%2520which%2520an%2520image%2520was%2520captured%252C%2520and%2520%25282%2529%2520to%2520address%2520the%2520fine%250Alocalization%2520step%252C%2520which%2520consists%2520in%2520retrieving%2520the%2520most%2520similar%2520image%2520of%2520the%250Avisual%2520map%2520among%2520those%2520contained%2520in%2520the%2520previously%2520predicted%2520room%2520by%2520means%2520of%2520a%250Apairwise%2520comparison%2520between%2520descriptors%2520obtained%2520from%2520an%2520intermediate%2520layer%2520of%250Athe%2520CNN.%2520In%2520this%2520sense%252C%2520we%2520evaluate%2520the%2520impact%2520of%2520different%2520state-of-the-art%250ACNN%2520models%2520such%2520as%2520ConvNeXt%2520for%2520addressing%2520the%2520proposed%2520localization.%2520Finally%252C%250Aa%2520variety%2520of%2520data%2520augmentation%2520visual%2520effects%2520are%2520separately%2520employed%2520for%250Atraining%2520the%2520model%2520and%2520their%2520impact%2520is%2520assessed.%2520The%2520performance%2520of%2520the%250Aresulting%2520CNNs%2520is%2520evaluated%2520under%2520real%2520operation%2520conditions%252C%2520including%2520changes%250Ain%2520the%2520lighting%2520conditions.%2520Our%2520code%2520is%2520publicly%2520available%2520on%2520the%2520project%250Awebsite%2520https%253A//github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20evaluation%20of%20CNN%20models%20and%20data%20augmentation%20techniques%20in%0A%20%20hierarchical%20localization%20of%20mobile%20robots&entry.906535625=J.%20J.%20Cabrera%20and%20O.%20J.%20C%C3%A9spedes%20and%20S.%20Cebollada%20and%20O.%20Reinoso%20and%20L.%20Pay%C3%A1&entry.1292438233=%20%20This%20work%20presents%20an%20evaluation%20of%20CNN%20models%20and%20data%20augmentation%20to%20carry%0Aout%20the%20hierarchical%20localization%20of%20a%20mobile%20robot%20by%20using%20omnidireccional%0Aimages.%20In%20this%20sense%2C%20an%20ablation%20study%20of%20different%20state-of-the-art%20CNN%0Amodels%20used%20as%20backbone%20is%20presented%20and%20a%20variety%20of%20data%20augmentation%20visual%0Aeffects%20are%20proposed%20for%20addressing%20the%20visual%20localization%20of%20the%20robot.%20The%0Aproposed%20method%20is%20based%20on%20the%20adaption%20and%20re-training%20of%20a%20CNN%20with%20a%20dual%0Apurpose%3A%20%281%29%20to%20perform%20a%20rough%20localization%20step%20in%20which%20the%20model%20is%20used%20to%0Apredict%20the%20room%20from%20which%20an%20image%20was%20captured%2C%20and%20%282%29%20to%20address%20the%20fine%0Alocalization%20step%2C%20which%20consists%20in%20retrieving%20the%20most%20similar%20image%20of%20the%0Avisual%20map%20among%20those%20contained%20in%20the%20previously%20predicted%20room%20by%20means%20of%20a%0Apairwise%20comparison%20between%20descriptors%20obtained%20from%20an%20intermediate%20layer%20of%0Athe%20CNN.%20In%20this%20sense%2C%20we%20evaluate%20the%20impact%20of%20different%20state-of-the-art%0ACNN%20models%20such%20as%20ConvNeXt%20for%20addressing%20the%20proposed%20localization.%20Finally%2C%0Aa%20variety%20of%20data%20augmentation%20visual%20effects%20are%20separately%20employed%20for%0Atraining%20the%20model%20and%20their%20impact%20is%20assessed.%20The%20performance%20of%20the%0Aresulting%20CNNs%20is%20evaluated%20under%20real%20operation%20conditions%2C%20including%20changes%0Ain%20the%20lighting%20conditions.%20Our%20code%20is%20publicly%20available%20on%20the%20project%0Awebsite%20https%3A//github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10596v1&entry.124074799=Read"},
{"title": "Classification of Heart Sounds Using Multi-Branch Deep Convolutional\n  Network and LSTM-CNN", "author": "Seyed Amir Latifi and Hassan Ghassemian and Maryam Imani", "abstract": "  This paper presents a fast and cost-effective method for diagnosing cardiac\nabnormalities with high accuracy and reliability using low-cost systems in\nclinics. The primary limitation of automatic diagnosing of cardiac diseases is\nthe rarity of correct and acceptable labeled samples, which can be expensive to\nprepare. To address this issue, two methods are proposed in this work. The\nfirst method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)\narchitecture inspired by human auditory processing, specifically designed to\noptimize feature extraction by employing various sizes of convolutional filters\nand audio signal power spectrum as input. In the second method, called as Long\nshort-term memory-Convolutional Neural (LSCN) model, Additionally, the network\narchitecture includes Long Short-Term Memory (LSTM) network blocks to improve\nfeature extraction in the time domain. The innovative approach of combining\nmultiple parallel branches consisting of the one-dimensional convolutional\nlayers along with LSTM blocks helps in achieving superior results in audio\nsignal processing tasks. The experimental results demonstrate superiority of\nthe proposed methods over the state-of-the-art techniques. The overall\nclassification accuracy of heart sounds with the LSCN network is more than 96%.\nThe efficiency of this network is significant compared to common feature\nextraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and\nwavelet transform. Therefore, the proposed method shows promising results in\nthe automatic analysis of heart sounds and has potential applications in the\ndiagnosis and early detection of cardiovascular diseases.\n", "link": "http://arxiv.org/abs/2407.10689v1", "date": "2024-07-15", "relevancy": 2.3835, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4804}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4786}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN&body=Title%3A%20Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN%0AAuthor%3A%20Seyed%20Amir%20Latifi%20and%20Hassan%20Ghassemian%20and%20Maryam%20Imani%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20fast%20and%20cost-effective%20method%20for%20diagnosing%20cardiac%0Aabnormalities%20with%20high%20accuracy%20and%20reliability%20using%20low-cost%20systems%20in%0Aclinics.%20The%20primary%20limitation%20of%20automatic%20diagnosing%20of%20cardiac%20diseases%20is%0Athe%20rarity%20of%20correct%20and%20acceptable%20labeled%20samples%2C%20which%20can%20be%20expensive%20to%0Aprepare.%20To%20address%20this%20issue%2C%20two%20methods%20are%20proposed%20in%20this%20work.%20The%0Afirst%20method%20is%20a%20unique%20Multi-Branch%20Deep%20Convolutional%20Neural%20Network%20%28MBDCN%29%0Aarchitecture%20inspired%20by%20human%20auditory%20processing%2C%20specifically%20designed%20to%0Aoptimize%20feature%20extraction%20by%20employing%20various%20sizes%20of%20convolutional%20filters%0Aand%20audio%20signal%20power%20spectrum%20as%20input.%20In%20the%20second%20method%2C%20called%20as%20Long%0Ashort-term%20memory-Convolutional%20Neural%20%28LSCN%29%20model%2C%20Additionally%2C%20the%20network%0Aarchitecture%20includes%20Long%20Short-Term%20Memory%20%28LSTM%29%20network%20blocks%20to%20improve%0Afeature%20extraction%20in%20the%20time%20domain.%20The%20innovative%20approach%20of%20combining%0Amultiple%20parallel%20branches%20consisting%20of%20the%20one-dimensional%20convolutional%0Alayers%20along%20with%20LSTM%20blocks%20helps%20in%20achieving%20superior%20results%20in%20audio%0Asignal%20processing%20tasks.%20The%20experimental%20results%20demonstrate%20superiority%20of%0Athe%20proposed%20methods%20over%20the%20state-of-the-art%20techniques.%20The%20overall%0Aclassification%20accuracy%20of%20heart%20sounds%20with%20the%20LSCN%20network%20is%20more%20than%2096%25.%0AThe%20efficiency%20of%20this%20network%20is%20significant%20compared%20to%20common%20feature%0Aextraction%20methods%20such%20as%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20and%0Awavelet%20transform.%20Therefore%2C%20the%20proposed%20method%20shows%20promising%20results%20in%0Athe%20automatic%20analysis%20of%20heart%20sounds%20and%20has%20potential%20applications%20in%20the%0Adiagnosis%20and%20early%20detection%20of%20cardiovascular%20diseases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Heart%2520Sounds%2520Using%2520Multi-Branch%2520Deep%2520Convolutional%250A%2520%2520Network%2520and%2520LSTM-CNN%26entry.906535625%3DSeyed%2520Amir%2520Latifi%2520and%2520Hassan%2520Ghassemian%2520and%2520Maryam%2520Imani%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520fast%2520and%2520cost-effective%2520method%2520for%2520diagnosing%2520cardiac%250Aabnormalities%2520with%2520high%2520accuracy%2520and%2520reliability%2520using%2520low-cost%2520systems%2520in%250Aclinics.%2520The%2520primary%2520limitation%2520of%2520automatic%2520diagnosing%2520of%2520cardiac%2520diseases%2520is%250Athe%2520rarity%2520of%2520correct%2520and%2520acceptable%2520labeled%2520samples%252C%2520which%2520can%2520be%2520expensive%2520to%250Aprepare.%2520To%2520address%2520this%2520issue%252C%2520two%2520methods%2520are%2520proposed%2520in%2520this%2520work.%2520The%250Afirst%2520method%2520is%2520a%2520unique%2520Multi-Branch%2520Deep%2520Convolutional%2520Neural%2520Network%2520%2528MBDCN%2529%250Aarchitecture%2520inspired%2520by%2520human%2520auditory%2520processing%252C%2520specifically%2520designed%2520to%250Aoptimize%2520feature%2520extraction%2520by%2520employing%2520various%2520sizes%2520of%2520convolutional%2520filters%250Aand%2520audio%2520signal%2520power%2520spectrum%2520as%2520input.%2520In%2520the%2520second%2520method%252C%2520called%2520as%2520Long%250Ashort-term%2520memory-Convolutional%2520Neural%2520%2528LSCN%2529%2520model%252C%2520Additionally%252C%2520the%2520network%250Aarchitecture%2520includes%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520network%2520blocks%2520to%2520improve%250Afeature%2520extraction%2520in%2520the%2520time%2520domain.%2520The%2520innovative%2520approach%2520of%2520combining%250Amultiple%2520parallel%2520branches%2520consisting%2520of%2520the%2520one-dimensional%2520convolutional%250Alayers%2520along%2520with%2520LSTM%2520blocks%2520helps%2520in%2520achieving%2520superior%2520results%2520in%2520audio%250Asignal%2520processing%2520tasks.%2520The%2520experimental%2520results%2520demonstrate%2520superiority%2520of%250Athe%2520proposed%2520methods%2520over%2520the%2520state-of-the-art%2520techniques.%2520The%2520overall%250Aclassification%2520accuracy%2520of%2520heart%2520sounds%2520with%2520the%2520LSCN%2520network%2520is%2520more%2520than%252096%2525.%250AThe%2520efficiency%2520of%2520this%2520network%2520is%2520significant%2520compared%2520to%2520common%2520feature%250Aextraction%2520methods%2520such%2520as%2520Mel%2520Frequency%2520Cepstral%2520Coefficients%2520%2528MFCC%2529%2520and%250Awavelet%2520transform.%2520Therefore%252C%2520the%2520proposed%2520method%2520shows%2520promising%2520results%2520in%250Athe%2520automatic%2520analysis%2520of%2520heart%2520sounds%2520and%2520has%2520potential%2520applications%2520in%2520the%250Adiagnosis%2520and%2520early%2520detection%2520of%2520cardiovascular%2520diseases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN&entry.906535625=Seyed%20Amir%20Latifi%20and%20Hassan%20Ghassemian%20and%20Maryam%20Imani&entry.1292438233=%20%20This%20paper%20presents%20a%20fast%20and%20cost-effective%20method%20for%20diagnosing%20cardiac%0Aabnormalities%20with%20high%20accuracy%20and%20reliability%20using%20low-cost%20systems%20in%0Aclinics.%20The%20primary%20limitation%20of%20automatic%20diagnosing%20of%20cardiac%20diseases%20is%0Athe%20rarity%20of%20correct%20and%20acceptable%20labeled%20samples%2C%20which%20can%20be%20expensive%20to%0Aprepare.%20To%20address%20this%20issue%2C%20two%20methods%20are%20proposed%20in%20this%20work.%20The%0Afirst%20method%20is%20a%20unique%20Multi-Branch%20Deep%20Convolutional%20Neural%20Network%20%28MBDCN%29%0Aarchitecture%20inspired%20by%20human%20auditory%20processing%2C%20specifically%20designed%20to%0Aoptimize%20feature%20extraction%20by%20employing%20various%20sizes%20of%20convolutional%20filters%0Aand%20audio%20signal%20power%20spectrum%20as%20input.%20In%20the%20second%20method%2C%20called%20as%20Long%0Ashort-term%20memory-Convolutional%20Neural%20%28LSCN%29%20model%2C%20Additionally%2C%20the%20network%0Aarchitecture%20includes%20Long%20Short-Term%20Memory%20%28LSTM%29%20network%20blocks%20to%20improve%0Afeature%20extraction%20in%20the%20time%20domain.%20The%20innovative%20approach%20of%20combining%0Amultiple%20parallel%20branches%20consisting%20of%20the%20one-dimensional%20convolutional%0Alayers%20along%20with%20LSTM%20blocks%20helps%20in%20achieving%20superior%20results%20in%20audio%0Asignal%20processing%20tasks.%20The%20experimental%20results%20demonstrate%20superiority%20of%0Athe%20proposed%20methods%20over%20the%20state-of-the-art%20techniques.%20The%20overall%0Aclassification%20accuracy%20of%20heart%20sounds%20with%20the%20LSCN%20network%20is%20more%20than%2096%25.%0AThe%20efficiency%20of%20this%20network%20is%20significant%20compared%20to%20common%20feature%0Aextraction%20methods%20such%20as%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20and%0Awavelet%20transform.%20Therefore%2C%20the%20proposed%20method%20shows%20promising%20results%20in%0Athe%20automatic%20analysis%20of%20heart%20sounds%20and%20has%20potential%20applications%20in%20the%0Adiagnosis%20and%20early%20detection%20of%20cardiovascular%20diseases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10689v1&entry.124074799=Read"},
{"title": "A Neurosymbolic Approach to Adaptive Feature Extraction in SLAM", "author": "Yasra Chandio and Momin A. Khan and Khotso Selialia and Luis Garcia and Joseph DeGol and Fatima M. Anwar", "abstract": "  Autonomous robots, autonomous vehicles, and humans wearing mixed-reality\nheadsets require accurate and reliable tracking services for safety-critical\napplications in dynamically changing real-world environments. However, the\nexisting tracking approaches, such as Simultaneous Localization and Mapping\n(SLAM), do not adapt well to environmental changes and boundary conditions\ndespite extensive manual tuning. On the other hand, while deep learning-based\napproaches can better adapt to environmental changes, they typically demand\nsubstantial data for training and often lack flexibility in adapting to new\ndomains. To solve this problem, we propose leveraging the neurosymbolic program\nsynthesis approach to construct adaptable SLAM pipelines that integrate the\ndomain knowledge from traditional SLAM approaches while leveraging data to\nlearn complex relationships. While the approach can synthesize end-to-end SLAM\npipelines, we focus on synthesizing the feature extraction module. We first\ndevise a domain-specific language (DSL) that can encapsulate domain knowledge\non the important attributes for feature extraction and the real-world\nperformance of various feature extractors. Our neurosymbolic architecture then\nundertakes adaptive feature extraction, optimizing parameters via learning\nwhile employing symbolic reasoning to select the most suitable feature\nextractor. Our evaluations demonstrate that our approach, neurosymbolic Feature\nEXtraction (nFEX), yields higher-quality features. It also reduces the pose\nerror observed for the state-of-the-art baseline feature extractors ORB and\nSIFT by up to 90% and up to 66%, respectively, thereby enhancing the system's\nefficiency and adaptability to novel environments.\n", "link": "http://arxiv.org/abs/2407.06889v2", "date": "2024-07-15", "relevancy": 2.3821, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5976}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neurosymbolic%20Approach%20to%20Adaptive%20Feature%20Extraction%20in%20SLAM&body=Title%3A%20A%20Neurosymbolic%20Approach%20to%20Adaptive%20Feature%20Extraction%20in%20SLAM%0AAuthor%3A%20Yasra%20Chandio%20and%20Momin%20A.%20Khan%20and%20Khotso%20Selialia%20and%20Luis%20Garcia%20and%20Joseph%20DeGol%20and%20Fatima%20M.%20Anwar%0AAbstract%3A%20%20%20Autonomous%20robots%2C%20autonomous%20vehicles%2C%20and%20humans%20wearing%20mixed-reality%0Aheadsets%20require%20accurate%20and%20reliable%20tracking%20services%20for%20safety-critical%0Aapplications%20in%20dynamically%20changing%20real-world%20environments.%20However%2C%20the%0Aexisting%20tracking%20approaches%2C%20such%20as%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%2C%20do%20not%20adapt%20well%20to%20environmental%20changes%20and%20boundary%20conditions%0Adespite%20extensive%20manual%20tuning.%20On%20the%20other%20hand%2C%20while%20deep%20learning-based%0Aapproaches%20can%20better%20adapt%20to%20environmental%20changes%2C%20they%20typically%20demand%0Asubstantial%20data%20for%20training%20and%20often%20lack%20flexibility%20in%20adapting%20to%20new%0Adomains.%20To%20solve%20this%20problem%2C%20we%20propose%20leveraging%20the%20neurosymbolic%20program%0Asynthesis%20approach%20to%20construct%20adaptable%20SLAM%20pipelines%20that%20integrate%20the%0Adomain%20knowledge%20from%20traditional%20SLAM%20approaches%20while%20leveraging%20data%20to%0Alearn%20complex%20relationships.%20While%20the%20approach%20can%20synthesize%20end-to-end%20SLAM%0Apipelines%2C%20we%20focus%20on%20synthesizing%20the%20feature%20extraction%20module.%20We%20first%0Adevise%20a%20domain-specific%20language%20%28DSL%29%20that%20can%20encapsulate%20domain%20knowledge%0Aon%20the%20important%20attributes%20for%20feature%20extraction%20and%20the%20real-world%0Aperformance%20of%20various%20feature%20extractors.%20Our%20neurosymbolic%20architecture%20then%0Aundertakes%20adaptive%20feature%20extraction%2C%20optimizing%20parameters%20via%20learning%0Awhile%20employing%20symbolic%20reasoning%20to%20select%20the%20most%20suitable%20feature%0Aextractor.%20Our%20evaluations%20demonstrate%20that%20our%20approach%2C%20neurosymbolic%20Feature%0AEXtraction%20%28nFEX%29%2C%20yields%20higher-quality%20features.%20It%20also%20reduces%20the%20pose%0Aerror%20observed%20for%20the%20state-of-the-art%20baseline%20feature%20extractors%20ORB%20and%0ASIFT%20by%20up%20to%2090%25%20and%20up%20to%2066%25%2C%20respectively%2C%20thereby%20enhancing%20the%20system%27s%0Aefficiency%20and%20adaptability%20to%20novel%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neurosymbolic%2520Approach%2520to%2520Adaptive%2520Feature%2520Extraction%2520in%2520SLAM%26entry.906535625%3DYasra%2520Chandio%2520and%2520Momin%2520A.%2520Khan%2520and%2520Khotso%2520Selialia%2520and%2520Luis%2520Garcia%2520and%2520Joseph%2520DeGol%2520and%2520Fatima%2520M.%2520Anwar%26entry.1292438233%3D%2520%2520Autonomous%2520robots%252C%2520autonomous%2520vehicles%252C%2520and%2520humans%2520wearing%2520mixed-reality%250Aheadsets%2520require%2520accurate%2520and%2520reliable%2520tracking%2520services%2520for%2520safety-critical%250Aapplications%2520in%2520dynamically%2520changing%2520real-world%2520environments.%2520However%252C%2520the%250Aexisting%2520tracking%2520approaches%252C%2520such%2520as%2520Simultaneous%2520Localization%2520and%2520Mapping%250A%2528SLAM%2529%252C%2520do%2520not%2520adapt%2520well%2520to%2520environmental%2520changes%2520and%2520boundary%2520conditions%250Adespite%2520extensive%2520manual%2520tuning.%2520On%2520the%2520other%2520hand%252C%2520while%2520deep%2520learning-based%250Aapproaches%2520can%2520better%2520adapt%2520to%2520environmental%2520changes%252C%2520they%2520typically%2520demand%250Asubstantial%2520data%2520for%2520training%2520and%2520often%2520lack%2520flexibility%2520in%2520adapting%2520to%2520new%250Adomains.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520leveraging%2520the%2520neurosymbolic%2520program%250Asynthesis%2520approach%2520to%2520construct%2520adaptable%2520SLAM%2520pipelines%2520that%2520integrate%2520the%250Adomain%2520knowledge%2520from%2520traditional%2520SLAM%2520approaches%2520while%2520leveraging%2520data%2520to%250Alearn%2520complex%2520relationships.%2520While%2520the%2520approach%2520can%2520synthesize%2520end-to-end%2520SLAM%250Apipelines%252C%2520we%2520focus%2520on%2520synthesizing%2520the%2520feature%2520extraction%2520module.%2520We%2520first%250Adevise%2520a%2520domain-specific%2520language%2520%2528DSL%2529%2520that%2520can%2520encapsulate%2520domain%2520knowledge%250Aon%2520the%2520important%2520attributes%2520for%2520feature%2520extraction%2520and%2520the%2520real-world%250Aperformance%2520of%2520various%2520feature%2520extractors.%2520Our%2520neurosymbolic%2520architecture%2520then%250Aundertakes%2520adaptive%2520feature%2520extraction%252C%2520optimizing%2520parameters%2520via%2520learning%250Awhile%2520employing%2520symbolic%2520reasoning%2520to%2520select%2520the%2520most%2520suitable%2520feature%250Aextractor.%2520Our%2520evaluations%2520demonstrate%2520that%2520our%2520approach%252C%2520neurosymbolic%2520Feature%250AEXtraction%2520%2528nFEX%2529%252C%2520yields%2520higher-quality%2520features.%2520It%2520also%2520reduces%2520the%2520pose%250Aerror%2520observed%2520for%2520the%2520state-of-the-art%2520baseline%2520feature%2520extractors%2520ORB%2520and%250ASIFT%2520by%2520up%2520to%252090%2525%2520and%2520up%2520to%252066%2525%252C%2520respectively%252C%2520thereby%2520enhancing%2520the%2520system%2527s%250Aefficiency%2520and%2520adaptability%2520to%2520novel%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neurosymbolic%20Approach%20to%20Adaptive%20Feature%20Extraction%20in%20SLAM&entry.906535625=Yasra%20Chandio%20and%20Momin%20A.%20Khan%20and%20Khotso%20Selialia%20and%20Luis%20Garcia%20and%20Joseph%20DeGol%20and%20Fatima%20M.%20Anwar&entry.1292438233=%20%20Autonomous%20robots%2C%20autonomous%20vehicles%2C%20and%20humans%20wearing%20mixed-reality%0Aheadsets%20require%20accurate%20and%20reliable%20tracking%20services%20for%20safety-critical%0Aapplications%20in%20dynamically%20changing%20real-world%20environments.%20However%2C%20the%0Aexisting%20tracking%20approaches%2C%20such%20as%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%2C%20do%20not%20adapt%20well%20to%20environmental%20changes%20and%20boundary%20conditions%0Adespite%20extensive%20manual%20tuning.%20On%20the%20other%20hand%2C%20while%20deep%20learning-based%0Aapproaches%20can%20better%20adapt%20to%20environmental%20changes%2C%20they%20typically%20demand%0Asubstantial%20data%20for%20training%20and%20often%20lack%20flexibility%20in%20adapting%20to%20new%0Adomains.%20To%20solve%20this%20problem%2C%20we%20propose%20leveraging%20the%20neurosymbolic%20program%0Asynthesis%20approach%20to%20construct%20adaptable%20SLAM%20pipelines%20that%20integrate%20the%0Adomain%20knowledge%20from%20traditional%20SLAM%20approaches%20while%20leveraging%20data%20to%0Alearn%20complex%20relationships.%20While%20the%20approach%20can%20synthesize%20end-to-end%20SLAM%0Apipelines%2C%20we%20focus%20on%20synthesizing%20the%20feature%20extraction%20module.%20We%20first%0Adevise%20a%20domain-specific%20language%20%28DSL%29%20that%20can%20encapsulate%20domain%20knowledge%0Aon%20the%20important%20attributes%20for%20feature%20extraction%20and%20the%20real-world%0Aperformance%20of%20various%20feature%20extractors.%20Our%20neurosymbolic%20architecture%20then%0Aundertakes%20adaptive%20feature%20extraction%2C%20optimizing%20parameters%20via%20learning%0Awhile%20employing%20symbolic%20reasoning%20to%20select%20the%20most%20suitable%20feature%0Aextractor.%20Our%20evaluations%20demonstrate%20that%20our%20approach%2C%20neurosymbolic%20Feature%0AEXtraction%20%28nFEX%29%2C%20yields%20higher-quality%20features.%20It%20also%20reduces%20the%20pose%0Aerror%20observed%20for%20the%20state-of-the-art%20baseline%20feature%20extractors%20ORB%20and%0ASIFT%20by%20up%20to%2090%25%20and%20up%20to%2066%25%2C%20respectively%2C%20thereby%20enhancing%20the%20system%27s%0Aefficiency%20and%20adaptability%20to%20novel%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06889v2&entry.124074799=Read"},
{"title": "RepVF: A Unified Vector Fields Representation for Multi-task 3D\n  Perception", "author": "Chunliang Li and Wencheng Han and Junbo Yin and Sanyuan Zhao and Jianbing Shen", "abstract": "  Concurrent processing of multiple autonomous driving 3D perception tasks\nwithin the same spatiotemporal scene poses a significant challenge, in\nparticular due to the computational inefficiencies and feature competition\nbetween tasks when using traditional multi-task learning approaches. This paper\naddresses these issues by proposing a novel unified representation, RepVF,\nwhich harmonizes the representation of various perception tasks such as 3D\nobject detection and 3D lane detection within a single framework. RepVF\ncharacterizes the structure of different targets in the scene through a vector\nfield, enabling a single-head, multi-task learning model that significantly\nreduces computational redundancy and feature competition. Building upon RepVF,\nwe introduce RFTR, a network designed to exploit the inherent connections\nbetween different tasks by utilizing a hierarchical structure of queries that\nimplicitly model the relationships both between and within tasks. This approach\neliminates the need for task-specific heads and parameters, fundamentally\nreducing the conflicts inherent in traditional multi-task learning paradigms.\nWe validate our approach by combining labels from the OpenLane dataset with the\nWaymo Open dataset. Our work presents a significant advancement in the\nefficiency and effectiveness of multi-task perception in autonomous driving,\noffering a new perspective on handling multiple 3D perception tasks\nsynchronously and in parallel. The code will be available at:\nhttps://github.com/jbji/RepVF\n", "link": "http://arxiv.org/abs/2407.10876v1", "date": "2024-07-15", "relevancy": 2.347, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5916}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepVF%3A%20A%20Unified%20Vector%20Fields%20Representation%20for%20Multi-task%203D%0A%20%20Perception&body=Title%3A%20RepVF%3A%20A%20Unified%20Vector%20Fields%20Representation%20for%20Multi-task%203D%0A%20%20Perception%0AAuthor%3A%20Chunliang%20Li%20and%20Wencheng%20Han%20and%20Junbo%20Yin%20and%20Sanyuan%20Zhao%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Concurrent%20processing%20of%20multiple%20autonomous%20driving%203D%20perception%20tasks%0Awithin%20the%20same%20spatiotemporal%20scene%20poses%20a%20significant%20challenge%2C%20in%0Aparticular%20due%20to%20the%20computational%20inefficiencies%20and%20feature%20competition%0Abetween%20tasks%20when%20using%20traditional%20multi-task%20learning%20approaches.%20This%20paper%0Aaddresses%20these%20issues%20by%20proposing%20a%20novel%20unified%20representation%2C%20RepVF%2C%0Awhich%20harmonizes%20the%20representation%20of%20various%20perception%20tasks%20such%20as%203D%0Aobject%20detection%20and%203D%20lane%20detection%20within%20a%20single%20framework.%20RepVF%0Acharacterizes%20the%20structure%20of%20different%20targets%20in%20the%20scene%20through%20a%20vector%0Afield%2C%20enabling%20a%20single-head%2C%20multi-task%20learning%20model%20that%20significantly%0Areduces%20computational%20redundancy%20and%20feature%20competition.%20Building%20upon%20RepVF%2C%0Awe%20introduce%20RFTR%2C%20a%20network%20designed%20to%20exploit%20the%20inherent%20connections%0Abetween%20different%20tasks%20by%20utilizing%20a%20hierarchical%20structure%20of%20queries%20that%0Aimplicitly%20model%20the%20relationships%20both%20between%20and%20within%20tasks.%20This%20approach%0Aeliminates%20the%20need%20for%20task-specific%20heads%20and%20parameters%2C%20fundamentally%0Areducing%20the%20conflicts%20inherent%20in%20traditional%20multi-task%20learning%20paradigms.%0AWe%20validate%20our%20approach%20by%20combining%20labels%20from%20the%20OpenLane%20dataset%20with%20the%0AWaymo%20Open%20dataset.%20Our%20work%20presents%20a%20significant%20advancement%20in%20the%0Aefficiency%20and%20effectiveness%20of%20multi-task%20perception%20in%20autonomous%20driving%2C%0Aoffering%20a%20new%20perspective%20on%20handling%20multiple%203D%20perception%20tasks%0Asynchronously%20and%20in%20parallel.%20The%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/jbji/RepVF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepVF%253A%2520A%2520Unified%2520Vector%2520Fields%2520Representation%2520for%2520Multi-task%25203D%250A%2520%2520Perception%26entry.906535625%3DChunliang%2520Li%2520and%2520Wencheng%2520Han%2520and%2520Junbo%2520Yin%2520and%2520Sanyuan%2520Zhao%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Concurrent%2520processing%2520of%2520multiple%2520autonomous%2520driving%25203D%2520perception%2520tasks%250Awithin%2520the%2520same%2520spatiotemporal%2520scene%2520poses%2520a%2520significant%2520challenge%252C%2520in%250Aparticular%2520due%2520to%2520the%2520computational%2520inefficiencies%2520and%2520feature%2520competition%250Abetween%2520tasks%2520when%2520using%2520traditional%2520multi-task%2520learning%2520approaches.%2520This%2520paper%250Aaddresses%2520these%2520issues%2520by%2520proposing%2520a%2520novel%2520unified%2520representation%252C%2520RepVF%252C%250Awhich%2520harmonizes%2520the%2520representation%2520of%2520various%2520perception%2520tasks%2520such%2520as%25203D%250Aobject%2520detection%2520and%25203D%2520lane%2520detection%2520within%2520a%2520single%2520framework.%2520RepVF%250Acharacterizes%2520the%2520structure%2520of%2520different%2520targets%2520in%2520the%2520scene%2520through%2520a%2520vector%250Afield%252C%2520enabling%2520a%2520single-head%252C%2520multi-task%2520learning%2520model%2520that%2520significantly%250Areduces%2520computational%2520redundancy%2520and%2520feature%2520competition.%2520Building%2520upon%2520RepVF%252C%250Awe%2520introduce%2520RFTR%252C%2520a%2520network%2520designed%2520to%2520exploit%2520the%2520inherent%2520connections%250Abetween%2520different%2520tasks%2520by%2520utilizing%2520a%2520hierarchical%2520structure%2520of%2520queries%2520that%250Aimplicitly%2520model%2520the%2520relationships%2520both%2520between%2520and%2520within%2520tasks.%2520This%2520approach%250Aeliminates%2520the%2520need%2520for%2520task-specific%2520heads%2520and%2520parameters%252C%2520fundamentally%250Areducing%2520the%2520conflicts%2520inherent%2520in%2520traditional%2520multi-task%2520learning%2520paradigms.%250AWe%2520validate%2520our%2520approach%2520by%2520combining%2520labels%2520from%2520the%2520OpenLane%2520dataset%2520with%2520the%250AWaymo%2520Open%2520dataset.%2520Our%2520work%2520presents%2520a%2520significant%2520advancement%2520in%2520the%250Aefficiency%2520and%2520effectiveness%2520of%2520multi-task%2520perception%2520in%2520autonomous%2520driving%252C%250Aoffering%2520a%2520new%2520perspective%2520on%2520handling%2520multiple%25203D%2520perception%2520tasks%250Asynchronously%2520and%2520in%2520parallel.%2520The%2520code%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/jbji/RepVF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepVF%3A%20A%20Unified%20Vector%20Fields%20Representation%20for%20Multi-task%203D%0A%20%20Perception&entry.906535625=Chunliang%20Li%20and%20Wencheng%20Han%20and%20Junbo%20Yin%20and%20Sanyuan%20Zhao%20and%20Jianbing%20Shen&entry.1292438233=%20%20Concurrent%20processing%20of%20multiple%20autonomous%20driving%203D%20perception%20tasks%0Awithin%20the%20same%20spatiotemporal%20scene%20poses%20a%20significant%20challenge%2C%20in%0Aparticular%20due%20to%20the%20computational%20inefficiencies%20and%20feature%20competition%0Abetween%20tasks%20when%20using%20traditional%20multi-task%20learning%20approaches.%20This%20paper%0Aaddresses%20these%20issues%20by%20proposing%20a%20novel%20unified%20representation%2C%20RepVF%2C%0Awhich%20harmonizes%20the%20representation%20of%20various%20perception%20tasks%20such%20as%203D%0Aobject%20detection%20and%203D%20lane%20detection%20within%20a%20single%20framework.%20RepVF%0Acharacterizes%20the%20structure%20of%20different%20targets%20in%20the%20scene%20through%20a%20vector%0Afield%2C%20enabling%20a%20single-head%2C%20multi-task%20learning%20model%20that%20significantly%0Areduces%20computational%20redundancy%20and%20feature%20competition.%20Building%20upon%20RepVF%2C%0Awe%20introduce%20RFTR%2C%20a%20network%20designed%20to%20exploit%20the%20inherent%20connections%0Abetween%20different%20tasks%20by%20utilizing%20a%20hierarchical%20structure%20of%20queries%20that%0Aimplicitly%20model%20the%20relationships%20both%20between%20and%20within%20tasks.%20This%20approach%0Aeliminates%20the%20need%20for%20task-specific%20heads%20and%20parameters%2C%20fundamentally%0Areducing%20the%20conflicts%20inherent%20in%20traditional%20multi-task%20learning%20paradigms.%0AWe%20validate%20our%20approach%20by%20combining%20labels%20from%20the%20OpenLane%20dataset%20with%20the%0AWaymo%20Open%20dataset.%20Our%20work%20presents%20a%20significant%20advancement%20in%20the%0Aefficiency%20and%20effectiveness%20of%20multi-task%20perception%20in%20autonomous%20driving%2C%0Aoffering%20a%20new%20perspective%20on%20handling%20multiple%203D%20perception%20tasks%0Asynchronously%20and%20in%20parallel.%20The%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/jbji/RepVF%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10876v1&entry.124074799=Read"},
{"title": "GRUtopia: Dream General Robots in a City at Scale", "author": "Hanqing Wang and Jiahe Chen and Wensi Huang and Qingwei Ben and Tai Wang and Boyu Mi and Tao Huang and Siheng Zhao and Yilun Chen and Sizhe Yang and Peizhou Cao and Wenye Yu and Zichao Ye and Jialun Li and Junfeng Long and Zirui Wang and Huiling Wang and Ying Zhao and Zhongying Tu and Yu Qiao and Dahua Lin and Jiangmiao Pang", "abstract": "  Recent works have been exploring the scaling laws in the field of Embodied\nAI. Given the prohibitive costs of collecting real-world data, we believe the\nSimulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the\nlearning of embodied models. This paper introduces project GRUtopia, the first\nsimulated interactive 3D society designed for various robots. It features\nseveral advancements: (a) The scene dataset, GRScenes, includes 100k\ninteractive, finely annotated scenes, which can be freely combined into\ncity-scale environments. In contrast to previous works mainly focusing on home,\nGRScenes covers 89 diverse scene categories, bridging the gap of\nservice-oriented environments where general robots would be initially deployed.\n(b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC)\nsystem that is responsible for social interaction, task generation, and task\nassignment, thus simulating social scenarios for embodied AI applications. (c)\nThe benchmark, GRBench, supports various robots but focuses on legged robots as\nprimary agents and poses moderately challenging tasks involving Object\nLoco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that\nthis work can alleviate the scarcity of high-quality data in this field and\nprovide a more comprehensive assessment of Embodied AI research. The project is\navailable at https://github.com/OpenRobotLab/GRUtopia.\n", "link": "http://arxiv.org/abs/2407.10943v1", "date": "2024-07-15", "relevancy": 2.3336, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6446}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5885}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRUtopia%3A%20Dream%20General%20Robots%20in%20a%20City%20at%20Scale&body=Title%3A%20GRUtopia%3A%20Dream%20General%20Robots%20in%20a%20City%20at%20Scale%0AAuthor%3A%20Hanqing%20Wang%20and%20Jiahe%20Chen%20and%20Wensi%20Huang%20and%20Qingwei%20Ben%20and%20Tai%20Wang%20and%20Boyu%20Mi%20and%20Tao%20Huang%20and%20Siheng%20Zhao%20and%20Yilun%20Chen%20and%20Sizhe%20Yang%20and%20Peizhou%20Cao%20and%20Wenye%20Yu%20and%20Zichao%20Ye%20and%20Jialun%20Li%20and%20Junfeng%20Long%20and%20Zirui%20Wang%20and%20Huiling%20Wang%20and%20Ying%20Zhao%20and%20Zhongying%20Tu%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Recent%20works%20have%20been%20exploring%20the%20scaling%20laws%20in%20the%20field%20of%20Embodied%0AAI.%20Given%20the%20prohibitive%20costs%20of%20collecting%20real-world%20data%2C%20we%20believe%20the%0ASimulation-to-Real%20%28Sim2Real%29%20paradigm%20is%20a%20crucial%20step%20for%20scaling%20the%0Alearning%20of%20embodied%20models.%20This%20paper%20introduces%20project%20GRUtopia%2C%20the%20first%0Asimulated%20interactive%203D%20society%20designed%20for%20various%20robots.%20It%20features%0Aseveral%20advancements%3A%20%28a%29%20The%20scene%20dataset%2C%20GRScenes%2C%20includes%20100k%0Ainteractive%2C%20finely%20annotated%20scenes%2C%20which%20can%20be%20freely%20combined%20into%0Acity-scale%20environments.%20In%20contrast%20to%20previous%20works%20mainly%20focusing%20on%20home%2C%0AGRScenes%20covers%2089%20diverse%20scene%20categories%2C%20bridging%20the%20gap%20of%0Aservice-oriented%20environments%20where%20general%20robots%20would%20be%20initially%20deployed.%0A%28b%29%20GRResidents%2C%20a%20Large%20Language%20Model%20%28LLM%29%20driven%20Non-Player%20Character%20%28NPC%29%0Asystem%20that%20is%20responsible%20for%20social%20interaction%2C%20task%20generation%2C%20and%20task%0Aassignment%2C%20thus%20simulating%20social%20scenarios%20for%20embodied%20AI%20applications.%20%28c%29%0AThe%20benchmark%2C%20GRBench%2C%20supports%20various%20robots%20but%20focuses%20on%20legged%20robots%20as%0Aprimary%20agents%20and%20poses%20moderately%20challenging%20tasks%20involving%20Object%0ALoco-Navigation%2C%20Social%20Loco-Navigation%2C%20and%20Loco-Manipulation.%20We%20hope%20that%0Athis%20work%20can%20alleviate%20the%20scarcity%20of%20high-quality%20data%20in%20this%20field%20and%0Aprovide%20a%20more%20comprehensive%20assessment%20of%20Embodied%20AI%20research.%20The%20project%20is%0Aavailable%20at%20https%3A//github.com/OpenRobotLab/GRUtopia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRUtopia%253A%2520Dream%2520General%2520Robots%2520in%2520a%2520City%2520at%2520Scale%26entry.906535625%3DHanqing%2520Wang%2520and%2520Jiahe%2520Chen%2520and%2520Wensi%2520Huang%2520and%2520Qingwei%2520Ben%2520and%2520Tai%2520Wang%2520and%2520Boyu%2520Mi%2520and%2520Tao%2520Huang%2520and%2520Siheng%2520Zhao%2520and%2520Yilun%2520Chen%2520and%2520Sizhe%2520Yang%2520and%2520Peizhou%2520Cao%2520and%2520Wenye%2520Yu%2520and%2520Zichao%2520Ye%2520and%2520Jialun%2520Li%2520and%2520Junfeng%2520Long%2520and%2520Zirui%2520Wang%2520and%2520Huiling%2520Wang%2520and%2520Ying%2520Zhao%2520and%2520Zhongying%2520Tu%2520and%2520Yu%2520Qiao%2520and%2520Dahua%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520been%2520exploring%2520the%2520scaling%2520laws%2520in%2520the%2520field%2520of%2520Embodied%250AAI.%2520Given%2520the%2520prohibitive%2520costs%2520of%2520collecting%2520real-world%2520data%252C%2520we%2520believe%2520the%250ASimulation-to-Real%2520%2528Sim2Real%2529%2520paradigm%2520is%2520a%2520crucial%2520step%2520for%2520scaling%2520the%250Alearning%2520of%2520embodied%2520models.%2520This%2520paper%2520introduces%2520project%2520GRUtopia%252C%2520the%2520first%250Asimulated%2520interactive%25203D%2520society%2520designed%2520for%2520various%2520robots.%2520It%2520features%250Aseveral%2520advancements%253A%2520%2528a%2529%2520The%2520scene%2520dataset%252C%2520GRScenes%252C%2520includes%2520100k%250Ainteractive%252C%2520finely%2520annotated%2520scenes%252C%2520which%2520can%2520be%2520freely%2520combined%2520into%250Acity-scale%2520environments.%2520In%2520contrast%2520to%2520previous%2520works%2520mainly%2520focusing%2520on%2520home%252C%250AGRScenes%2520covers%252089%2520diverse%2520scene%2520categories%252C%2520bridging%2520the%2520gap%2520of%250Aservice-oriented%2520environments%2520where%2520general%2520robots%2520would%2520be%2520initially%2520deployed.%250A%2528b%2529%2520GRResidents%252C%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520driven%2520Non-Player%2520Character%2520%2528NPC%2529%250Asystem%2520that%2520is%2520responsible%2520for%2520social%2520interaction%252C%2520task%2520generation%252C%2520and%2520task%250Aassignment%252C%2520thus%2520simulating%2520social%2520scenarios%2520for%2520embodied%2520AI%2520applications.%2520%2528c%2529%250AThe%2520benchmark%252C%2520GRBench%252C%2520supports%2520various%2520robots%2520but%2520focuses%2520on%2520legged%2520robots%2520as%250Aprimary%2520agents%2520and%2520poses%2520moderately%2520challenging%2520tasks%2520involving%2520Object%250ALoco-Navigation%252C%2520Social%2520Loco-Navigation%252C%2520and%2520Loco-Manipulation.%2520We%2520hope%2520that%250Athis%2520work%2520can%2520alleviate%2520the%2520scarcity%2520of%2520high-quality%2520data%2520in%2520this%2520field%2520and%250Aprovide%2520a%2520more%2520comprehensive%2520assessment%2520of%2520Embodied%2520AI%2520research.%2520The%2520project%2520is%250Aavailable%2520at%2520https%253A//github.com/OpenRobotLab/GRUtopia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRUtopia%3A%20Dream%20General%20Robots%20in%20a%20City%20at%20Scale&entry.906535625=Hanqing%20Wang%20and%20Jiahe%20Chen%20and%20Wensi%20Huang%20and%20Qingwei%20Ben%20and%20Tai%20Wang%20and%20Boyu%20Mi%20and%20Tao%20Huang%20and%20Siheng%20Zhao%20and%20Yilun%20Chen%20and%20Sizhe%20Yang%20and%20Peizhou%20Cao%20and%20Wenye%20Yu%20and%20Zichao%20Ye%20and%20Jialun%20Li%20and%20Junfeng%20Long%20and%20Zirui%20Wang%20and%20Huiling%20Wang%20and%20Ying%20Zhao%20and%20Zhongying%20Tu%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Recent%20works%20have%20been%20exploring%20the%20scaling%20laws%20in%20the%20field%20of%20Embodied%0AAI.%20Given%20the%20prohibitive%20costs%20of%20collecting%20real-world%20data%2C%20we%20believe%20the%0ASimulation-to-Real%20%28Sim2Real%29%20paradigm%20is%20a%20crucial%20step%20for%20scaling%20the%0Alearning%20of%20embodied%20models.%20This%20paper%20introduces%20project%20GRUtopia%2C%20the%20first%0Asimulated%20interactive%203D%20society%20designed%20for%20various%20robots.%20It%20features%0Aseveral%20advancements%3A%20%28a%29%20The%20scene%20dataset%2C%20GRScenes%2C%20includes%20100k%0Ainteractive%2C%20finely%20annotated%20scenes%2C%20which%20can%20be%20freely%20combined%20into%0Acity-scale%20environments.%20In%20contrast%20to%20previous%20works%20mainly%20focusing%20on%20home%2C%0AGRScenes%20covers%2089%20diverse%20scene%20categories%2C%20bridging%20the%20gap%20of%0Aservice-oriented%20environments%20where%20general%20robots%20would%20be%20initially%20deployed.%0A%28b%29%20GRResidents%2C%20a%20Large%20Language%20Model%20%28LLM%29%20driven%20Non-Player%20Character%20%28NPC%29%0Asystem%20that%20is%20responsible%20for%20social%20interaction%2C%20task%20generation%2C%20and%20task%0Aassignment%2C%20thus%20simulating%20social%20scenarios%20for%20embodied%20AI%20applications.%20%28c%29%0AThe%20benchmark%2C%20GRBench%2C%20supports%20various%20robots%20but%20focuses%20on%20legged%20robots%20as%0Aprimary%20agents%20and%20poses%20moderately%20challenging%20tasks%20involving%20Object%0ALoco-Navigation%2C%20Social%20Loco-Navigation%2C%20and%20Loco-Manipulation.%20We%20hope%20that%0Athis%20work%20can%20alleviate%20the%20scarcity%20of%20high-quality%20data%20in%20this%20field%20and%0Aprovide%20a%20more%20comprehensive%20assessment%20of%20Embodied%20AI%20research.%20The%20project%20is%0Aavailable%20at%20https%3A//github.com/OpenRobotLab/GRUtopia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10943v1&entry.124074799=Read"},
{"title": "OPEN: Object-wise Position Embedding for Multi-view 3D Object Detection", "author": "Jinghua Hou and Tong Wang and Xiaoqing Ye and Zhe Liu and Shi Gong and Xiao Tan and Errui Ding and Jingdong Wang and Xiang Bai", "abstract": "  Accurate depth information is crucial for enhancing the performance of\nmulti-view 3D object detection. Despite the success of some existing multi-view\n3D detectors utilizing pixel-wise depth supervision, they overlook two\nsignificant phenomena: 1) the depth supervision obtained from LiDAR points is\nusually distributed on the surface of the object, which is not so friendly to\nexisting DETR-based 3D detectors due to the lack of the depth of 3D object\ncenter; 2) for distant objects, fine-grained depth estimation of the whole\nobject is more challenging. Therefore, we argue that the object-wise depth (or\n3D center of the object) is essential for accurate detection. In this paper, we\npropose a new multi-view 3D object detector named OPEN, whose main idea is to\neffectively inject object-wise depth information into the network through our\nproposed object-wise position embedding. Specifically, we first employ an\nobject-wise depth encoder, which takes the pixel-wise depth map as a prior, to\naccurately estimate the object-wise depth. Then, we utilize the proposed\nobject-wise position embedding to encode the object-wise depth information into\nthe transformer decoder, thereby producing 3D object-aware features for final\ndetection. Extensive experiments verify the effectiveness of our proposed\nmethod. Furthermore, OPEN achieves a new state-of-the-art performance with\n64.4% NDS and 56.7% mAP on the nuScenes test benchmark.\n", "link": "http://arxiv.org/abs/2407.10753v1", "date": "2024-07-15", "relevancy": 2.3249, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5907}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPEN%3A%20Object-wise%20Position%20Embedding%20for%20Multi-view%203D%20Object%20Detection&body=Title%3A%20OPEN%3A%20Object-wise%20Position%20Embedding%20for%20Multi-view%203D%20Object%20Detection%0AAuthor%3A%20Jinghua%20Hou%20and%20Tong%20Wang%20and%20Xiaoqing%20Ye%20and%20Zhe%20Liu%20and%20Shi%20Gong%20and%20Xiao%20Tan%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Accurate%20depth%20information%20is%20crucial%20for%20enhancing%20the%20performance%20of%0Amulti-view%203D%20object%20detection.%20Despite%20the%20success%20of%20some%20existing%20multi-view%0A3D%20detectors%20utilizing%20pixel-wise%20depth%20supervision%2C%20they%20overlook%20two%0Asignificant%20phenomena%3A%201%29%20the%20depth%20supervision%20obtained%20from%20LiDAR%20points%20is%0Ausually%20distributed%20on%20the%20surface%20of%20the%20object%2C%20which%20is%20not%20so%20friendly%20to%0Aexisting%20DETR-based%203D%20detectors%20due%20to%20the%20lack%20of%20the%20depth%20of%203D%20object%0Acenter%3B%202%29%20for%20distant%20objects%2C%20fine-grained%20depth%20estimation%20of%20the%20whole%0Aobject%20is%20more%20challenging.%20Therefore%2C%20we%20argue%20that%20the%20object-wise%20depth%20%28or%0A3D%20center%20of%20the%20object%29%20is%20essential%20for%20accurate%20detection.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20multi-view%203D%20object%20detector%20named%20OPEN%2C%20whose%20main%20idea%20is%20to%0Aeffectively%20inject%20object-wise%20depth%20information%20into%20the%20network%20through%20our%0Aproposed%20object-wise%20position%20embedding.%20Specifically%2C%20we%20first%20employ%20an%0Aobject-wise%20depth%20encoder%2C%20which%20takes%20the%20pixel-wise%20depth%20map%20as%20a%20prior%2C%20to%0Aaccurately%20estimate%20the%20object-wise%20depth.%20Then%2C%20we%20utilize%20the%20proposed%0Aobject-wise%20position%20embedding%20to%20encode%20the%20object-wise%20depth%20information%20into%0Athe%20transformer%20decoder%2C%20thereby%20producing%203D%20object-aware%20features%20for%20final%0Adetection.%20Extensive%20experiments%20verify%20the%20effectiveness%20of%20our%20proposed%0Amethod.%20Furthermore%2C%20OPEN%20achieves%20a%20new%20state-of-the-art%20performance%20with%0A64.4%25%20NDS%20and%2056.7%25%20mAP%20on%20the%20nuScenes%20test%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPEN%253A%2520Object-wise%2520Position%2520Embedding%2520for%2520Multi-view%25203D%2520Object%2520Detection%26entry.906535625%3DJinghua%2520Hou%2520and%2520Tong%2520Wang%2520and%2520Xiaoqing%2520Ye%2520and%2520Zhe%2520Liu%2520and%2520Shi%2520Gong%2520and%2520Xiao%2520Tan%2520and%2520Errui%2520Ding%2520and%2520Jingdong%2520Wang%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Accurate%2520depth%2520information%2520is%2520crucial%2520for%2520enhancing%2520the%2520performance%2520of%250Amulti-view%25203D%2520object%2520detection.%2520Despite%2520the%2520success%2520of%2520some%2520existing%2520multi-view%250A3D%2520detectors%2520utilizing%2520pixel-wise%2520depth%2520supervision%252C%2520they%2520overlook%2520two%250Asignificant%2520phenomena%253A%25201%2529%2520the%2520depth%2520supervision%2520obtained%2520from%2520LiDAR%2520points%2520is%250Ausually%2520distributed%2520on%2520the%2520surface%2520of%2520the%2520object%252C%2520which%2520is%2520not%2520so%2520friendly%2520to%250Aexisting%2520DETR-based%25203D%2520detectors%2520due%2520to%2520the%2520lack%2520of%2520the%2520depth%2520of%25203D%2520object%250Acenter%253B%25202%2529%2520for%2520distant%2520objects%252C%2520fine-grained%2520depth%2520estimation%2520of%2520the%2520whole%250Aobject%2520is%2520more%2520challenging.%2520Therefore%252C%2520we%2520argue%2520that%2520the%2520object-wise%2520depth%2520%2528or%250A3D%2520center%2520of%2520the%2520object%2529%2520is%2520essential%2520for%2520accurate%2520detection.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520new%2520multi-view%25203D%2520object%2520detector%2520named%2520OPEN%252C%2520whose%2520main%2520idea%2520is%2520to%250Aeffectively%2520inject%2520object-wise%2520depth%2520information%2520into%2520the%2520network%2520through%2520our%250Aproposed%2520object-wise%2520position%2520embedding.%2520Specifically%252C%2520we%2520first%2520employ%2520an%250Aobject-wise%2520depth%2520encoder%252C%2520which%2520takes%2520the%2520pixel-wise%2520depth%2520map%2520as%2520a%2520prior%252C%2520to%250Aaccurately%2520estimate%2520the%2520object-wise%2520depth.%2520Then%252C%2520we%2520utilize%2520the%2520proposed%250Aobject-wise%2520position%2520embedding%2520to%2520encode%2520the%2520object-wise%2520depth%2520information%2520into%250Athe%2520transformer%2520decoder%252C%2520thereby%2520producing%25203D%2520object-aware%2520features%2520for%2520final%250Adetection.%2520Extensive%2520experiments%2520verify%2520the%2520effectiveness%2520of%2520our%2520proposed%250Amethod.%2520Furthermore%252C%2520OPEN%2520achieves%2520a%2520new%2520state-of-the-art%2520performance%2520with%250A64.4%2525%2520NDS%2520and%252056.7%2525%2520mAP%2520on%2520the%2520nuScenes%2520test%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPEN%3A%20Object-wise%20Position%20Embedding%20for%20Multi-view%203D%20Object%20Detection&entry.906535625=Jinghua%20Hou%20and%20Tong%20Wang%20and%20Xiaoqing%20Ye%20and%20Zhe%20Liu%20and%20Shi%20Gong%20and%20Xiao%20Tan%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Xiang%20Bai&entry.1292438233=%20%20Accurate%20depth%20information%20is%20crucial%20for%20enhancing%20the%20performance%20of%0Amulti-view%203D%20object%20detection.%20Despite%20the%20success%20of%20some%20existing%20multi-view%0A3D%20detectors%20utilizing%20pixel-wise%20depth%20supervision%2C%20they%20overlook%20two%0Asignificant%20phenomena%3A%201%29%20the%20depth%20supervision%20obtained%20from%20LiDAR%20points%20is%0Ausually%20distributed%20on%20the%20surface%20of%20the%20object%2C%20which%20is%20not%20so%20friendly%20to%0Aexisting%20DETR-based%203D%20detectors%20due%20to%20the%20lack%20of%20the%20depth%20of%203D%20object%0Acenter%3B%202%29%20for%20distant%20objects%2C%20fine-grained%20depth%20estimation%20of%20the%20whole%0Aobject%20is%20more%20challenging.%20Therefore%2C%20we%20argue%20that%20the%20object-wise%20depth%20%28or%0A3D%20center%20of%20the%20object%29%20is%20essential%20for%20accurate%20detection.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20multi-view%203D%20object%20detector%20named%20OPEN%2C%20whose%20main%20idea%20is%20to%0Aeffectively%20inject%20object-wise%20depth%20information%20into%20the%20network%20through%20our%0Aproposed%20object-wise%20position%20embedding.%20Specifically%2C%20we%20first%20employ%20an%0Aobject-wise%20depth%20encoder%2C%20which%20takes%20the%20pixel-wise%20depth%20map%20as%20a%20prior%2C%20to%0Aaccurately%20estimate%20the%20object-wise%20depth.%20Then%2C%20we%20utilize%20the%20proposed%0Aobject-wise%20position%20embedding%20to%20encode%20the%20object-wise%20depth%20information%20into%0Athe%20transformer%20decoder%2C%20thereby%20producing%203D%20object-aware%20features%20for%20final%0Adetection.%20Extensive%20experiments%20verify%20the%20effectiveness%20of%20our%20proposed%0Amethod.%20Furthermore%2C%20OPEN%20achieves%20a%20new%20state-of-the-art%20performance%20with%0A64.4%25%20NDS%20and%2056.7%25%20mAP%20on%20the%20nuScenes%20test%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10753v1&entry.124074799=Read"},
{"title": "Learning the Unlearned: Mitigating Feature Suppression in Contrastive\n  Learning", "author": "Jihai Zhang and Xiang Lan and Xiaoye Qu and Yu Cheng and Mengling Feng and Bryan Hooi", "abstract": "  Self-Supervised Contrastive Learning has proven effective in deriving\nhigh-quality representations from unlabeled data. However, a major challenge\nthat hinders both unimodal and multimodal contrastive learning is feature\nsuppression, a phenomenon where the trained model captures only a limited\nportion of the information from the input data while overlooking other\npotentially valuable content. This issue often leads to indistinguishable\nrepresentations for visually similar but semantically different inputs,\nadversely affecting downstream task performance, particularly those requiring\nrigorous semantic comprehension. To address this challenge, we propose a novel\nmodel-agnostic Multistage Contrastive Learning (MCL) framework. Unlike standard\ncontrastive learning which inherently captures one single biased feature\ndistribution, MCL progressively learns previously unlearned features through\nfeature-aware negative sampling at each stage, where the negative samples of an\nanchor are exclusively selected from the cluster it was assigned to in\npreceding stages. Meanwhile, MCL preserves the previously well-learned features\nby cross-stage representation integration, integrating features across all\nstages to form final representations. Our comprehensive evaluation demonstrates\nMCL's effectiveness and superiority across both unimodal and multimodal\ncontrastive learning, spanning a range of model architectures from ResNet to\nVision Transformers (ViT). Remarkably, in tasks where the original CLIP model\nhas shown limitations, MCL dramatically enhances performance, with improvements\nup to threefold on specific attributes in the recently proposed MMVP benchmark.\n", "link": "http://arxiv.org/abs/2402.11816v3", "date": "2024-07-15", "relevancy": 2.3103, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6069}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5674}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20Unlearned%3A%20Mitigating%20Feature%20Suppression%20in%20Contrastive%0A%20%20Learning&body=Title%3A%20Learning%20the%20Unlearned%3A%20Mitigating%20Feature%20Suppression%20in%20Contrastive%0A%20%20Learning%0AAuthor%3A%20Jihai%20Zhang%20and%20Xiang%20Lan%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng%20and%20Mengling%20Feng%20and%20Bryan%20Hooi%0AAbstract%3A%20%20%20Self-Supervised%20Contrastive%20Learning%20has%20proven%20effective%20in%20deriving%0Ahigh-quality%20representations%20from%20unlabeled%20data.%20However%2C%20a%20major%20challenge%0Athat%20hinders%20both%20unimodal%20and%20multimodal%20contrastive%20learning%20is%20feature%0Asuppression%2C%20a%20phenomenon%20where%20the%20trained%20model%20captures%20only%20a%20limited%0Aportion%20of%20the%20information%20from%20the%20input%20data%20while%20overlooking%20other%0Apotentially%20valuable%20content.%20This%20issue%20often%20leads%20to%20indistinguishable%0Arepresentations%20for%20visually%20similar%20but%20semantically%20different%20inputs%2C%0Aadversely%20affecting%20downstream%20task%20performance%2C%20particularly%20those%20requiring%0Arigorous%20semantic%20comprehension.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Amodel-agnostic%20Multistage%20Contrastive%20Learning%20%28MCL%29%20framework.%20Unlike%20standard%0Acontrastive%20learning%20which%20inherently%20captures%20one%20single%20biased%20feature%0Adistribution%2C%20MCL%20progressively%20learns%20previously%20unlearned%20features%20through%0Afeature-aware%20negative%20sampling%20at%20each%20stage%2C%20where%20the%20negative%20samples%20of%20an%0Aanchor%20are%20exclusively%20selected%20from%20the%20cluster%20it%20was%20assigned%20to%20in%0Apreceding%20stages.%20Meanwhile%2C%20MCL%20preserves%20the%20previously%20well-learned%20features%0Aby%20cross-stage%20representation%20integration%2C%20integrating%20features%20across%20all%0Astages%20to%20form%20final%20representations.%20Our%20comprehensive%20evaluation%20demonstrates%0AMCL%27s%20effectiveness%20and%20superiority%20across%20both%20unimodal%20and%20multimodal%0Acontrastive%20learning%2C%20spanning%20a%20range%20of%20model%20architectures%20from%20ResNet%20to%0AVision%20Transformers%20%28ViT%29.%20Remarkably%2C%20in%20tasks%20where%20the%20original%20CLIP%20model%0Ahas%20shown%20limitations%2C%20MCL%20dramatically%20enhances%20performance%2C%20with%20improvements%0Aup%20to%20threefold%20on%20specific%20attributes%20in%20the%20recently%20proposed%20MMVP%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11816v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520the%2520Unlearned%253A%2520Mitigating%2520Feature%2520Suppression%2520in%2520Contrastive%250A%2520%2520Learning%26entry.906535625%3DJihai%2520Zhang%2520and%2520Xiang%2520Lan%2520and%2520Xiaoye%2520Qu%2520and%2520Yu%2520Cheng%2520and%2520Mengling%2520Feng%2520and%2520Bryan%2520Hooi%26entry.1292438233%3D%2520%2520Self-Supervised%2520Contrastive%2520Learning%2520has%2520proven%2520effective%2520in%2520deriving%250Ahigh-quality%2520representations%2520from%2520unlabeled%2520data.%2520However%252C%2520a%2520major%2520challenge%250Athat%2520hinders%2520both%2520unimodal%2520and%2520multimodal%2520contrastive%2520learning%2520is%2520feature%250Asuppression%252C%2520a%2520phenomenon%2520where%2520the%2520trained%2520model%2520captures%2520only%2520a%2520limited%250Aportion%2520of%2520the%2520information%2520from%2520the%2520input%2520data%2520while%2520overlooking%2520other%250Apotentially%2520valuable%2520content.%2520This%2520issue%2520often%2520leads%2520to%2520indistinguishable%250Arepresentations%2520for%2520visually%2520similar%2520but%2520semantically%2520different%2520inputs%252C%250Aadversely%2520affecting%2520downstream%2520task%2520performance%252C%2520particularly%2520those%2520requiring%250Arigorous%2520semantic%2520comprehension.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%250Amodel-agnostic%2520Multistage%2520Contrastive%2520Learning%2520%2528MCL%2529%2520framework.%2520Unlike%2520standard%250Acontrastive%2520learning%2520which%2520inherently%2520captures%2520one%2520single%2520biased%2520feature%250Adistribution%252C%2520MCL%2520progressively%2520learns%2520previously%2520unlearned%2520features%2520through%250Afeature-aware%2520negative%2520sampling%2520at%2520each%2520stage%252C%2520where%2520the%2520negative%2520samples%2520of%2520an%250Aanchor%2520are%2520exclusively%2520selected%2520from%2520the%2520cluster%2520it%2520was%2520assigned%2520to%2520in%250Apreceding%2520stages.%2520Meanwhile%252C%2520MCL%2520preserves%2520the%2520previously%2520well-learned%2520features%250Aby%2520cross-stage%2520representation%2520integration%252C%2520integrating%2520features%2520across%2520all%250Astages%2520to%2520form%2520final%2520representations.%2520Our%2520comprehensive%2520evaluation%2520demonstrates%250AMCL%2527s%2520effectiveness%2520and%2520superiority%2520across%2520both%2520unimodal%2520and%2520multimodal%250Acontrastive%2520learning%252C%2520spanning%2520a%2520range%2520of%2520model%2520architectures%2520from%2520ResNet%2520to%250AVision%2520Transformers%2520%2528ViT%2529.%2520Remarkably%252C%2520in%2520tasks%2520where%2520the%2520original%2520CLIP%2520model%250Ahas%2520shown%2520limitations%252C%2520MCL%2520dramatically%2520enhances%2520performance%252C%2520with%2520improvements%250Aup%2520to%2520threefold%2520on%2520specific%2520attributes%2520in%2520the%2520recently%2520proposed%2520MMVP%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11816v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20Unlearned%3A%20Mitigating%20Feature%20Suppression%20in%20Contrastive%0A%20%20Learning&entry.906535625=Jihai%20Zhang%20and%20Xiang%20Lan%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng%20and%20Mengling%20Feng%20and%20Bryan%20Hooi&entry.1292438233=%20%20Self-Supervised%20Contrastive%20Learning%20has%20proven%20effective%20in%20deriving%0Ahigh-quality%20representations%20from%20unlabeled%20data.%20However%2C%20a%20major%20challenge%0Athat%20hinders%20both%20unimodal%20and%20multimodal%20contrastive%20learning%20is%20feature%0Asuppression%2C%20a%20phenomenon%20where%20the%20trained%20model%20captures%20only%20a%20limited%0Aportion%20of%20the%20information%20from%20the%20input%20data%20while%20overlooking%20other%0Apotentially%20valuable%20content.%20This%20issue%20often%20leads%20to%20indistinguishable%0Arepresentations%20for%20visually%20similar%20but%20semantically%20different%20inputs%2C%0Aadversely%20affecting%20downstream%20task%20performance%2C%20particularly%20those%20requiring%0Arigorous%20semantic%20comprehension.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Amodel-agnostic%20Multistage%20Contrastive%20Learning%20%28MCL%29%20framework.%20Unlike%20standard%0Acontrastive%20learning%20which%20inherently%20captures%20one%20single%20biased%20feature%0Adistribution%2C%20MCL%20progressively%20learns%20previously%20unlearned%20features%20through%0Afeature-aware%20negative%20sampling%20at%20each%20stage%2C%20where%20the%20negative%20samples%20of%20an%0Aanchor%20are%20exclusively%20selected%20from%20the%20cluster%20it%20was%20assigned%20to%20in%0Apreceding%20stages.%20Meanwhile%2C%20MCL%20preserves%20the%20previously%20well-learned%20features%0Aby%20cross-stage%20representation%20integration%2C%20integrating%20features%20across%20all%0Astages%20to%20form%20final%20representations.%20Our%20comprehensive%20evaluation%20demonstrates%0AMCL%27s%20effectiveness%20and%20superiority%20across%20both%20unimodal%20and%20multimodal%0Acontrastive%20learning%2C%20spanning%20a%20range%20of%20model%20architectures%20from%20ResNet%20to%0AVision%20Transformers%20%28ViT%29.%20Remarkably%2C%20in%20tasks%20where%20the%20original%20CLIP%20model%0Ahas%20shown%20limitations%2C%20MCL%20dramatically%20enhances%20performance%2C%20with%20improvements%0Aup%20to%20threefold%20on%20specific%20attributes%20in%20the%20recently%20proposed%20MMVP%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11816v3&entry.124074799=Read"},
{"title": "SEED: A Simple and Effective 3D DETR in Point Clouds", "author": "Zhe Liu and Jinghua Hou and Xiaoqing Ye and Tong Wang and Jingdong Wang and Xiang Bai", "abstract": "  Recently, detection transformers (DETRs) have gradually taken a dominant\nposition in 2D detection thanks to their elegant framework. However, DETR-based\ndetectors for 3D point clouds are still difficult to achieve satisfactory\nperformance. We argue that the main challenges are twofold: 1) How to obtain\nthe appropriate object queries is challenging due to the high sparsity and\nuneven distribution of point clouds; 2) How to implement an effective query\ninteraction by exploiting the rich geometric structure of point clouds is not\nfully explored. To this end, we propose a simple and effective 3D DETR method\n(SEED) for detecting 3D objects from point clouds, which involves a dual query\nselection (DQS) module and a deformable grid attention (DGA) module. More\nconcretely, to obtain appropriate queries, DQS first ensures a high recall to\nretain a large number of queries by the predicted confidence scores and then\nfurther picks out high-quality queries according to the estimated quality\nscores. DGA uniformly divides each reference box into grids as the reference\npoints and then utilizes the predicted offsets to achieve a flexible receptive\nfield, allowing the network to focus on relevant regions and capture more\ninformative features. Extensive ablation studies on DQS and DGA demonstrate its\neffectiveness. Furthermore, our SEED achieves state-of-the-art detection\nperformance on both the large-scale Waymo and nuScenes datasets, illustrating\nthe superiority of our proposed method. The code is available at\nhttps://github.com/happinesslz/SEED\n", "link": "http://arxiv.org/abs/2407.10749v1", "date": "2024-07-15", "relevancy": 2.2959, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5951}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5881}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEED%3A%20A%20Simple%20and%20Effective%203D%20DETR%20in%20Point%20Clouds&body=Title%3A%20SEED%3A%20A%20Simple%20and%20Effective%203D%20DETR%20in%20Point%20Clouds%0AAuthor%3A%20Zhe%20Liu%20and%20Jinghua%20Hou%20and%20Xiaoqing%20Ye%20and%20Tong%20Wang%20and%20Jingdong%20Wang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Recently%2C%20detection%20transformers%20%28DETRs%29%20have%20gradually%20taken%20a%20dominant%0Aposition%20in%202D%20detection%20thanks%20to%20their%20elegant%20framework.%20However%2C%20DETR-based%0Adetectors%20for%203D%20point%20clouds%20are%20still%20difficult%20to%20achieve%20satisfactory%0Aperformance.%20We%20argue%20that%20the%20main%20challenges%20are%20twofold%3A%201%29%20How%20to%20obtain%0Athe%20appropriate%20object%20queries%20is%20challenging%20due%20to%20the%20high%20sparsity%20and%0Auneven%20distribution%20of%20point%20clouds%3B%202%29%20How%20to%20implement%20an%20effective%20query%0Ainteraction%20by%20exploiting%20the%20rich%20geometric%20structure%20of%20point%20clouds%20is%20not%0Afully%20explored.%20To%20this%20end%2C%20we%20propose%20a%20simple%20and%20effective%203D%20DETR%20method%0A%28SEED%29%20for%20detecting%203D%20objects%20from%20point%20clouds%2C%20which%20involves%20a%20dual%20query%0Aselection%20%28DQS%29%20module%20and%20a%20deformable%20grid%20attention%20%28DGA%29%20module.%20More%0Aconcretely%2C%20to%20obtain%20appropriate%20queries%2C%20DQS%20first%20ensures%20a%20high%20recall%20to%0Aretain%20a%20large%20number%20of%20queries%20by%20the%20predicted%20confidence%20scores%20and%20then%0Afurther%20picks%20out%20high-quality%20queries%20according%20to%20the%20estimated%20quality%0Ascores.%20DGA%20uniformly%20divides%20each%20reference%20box%20into%20grids%20as%20the%20reference%0Apoints%20and%20then%20utilizes%20the%20predicted%20offsets%20to%20achieve%20a%20flexible%20receptive%0Afield%2C%20allowing%20the%20network%20to%20focus%20on%20relevant%20regions%20and%20capture%20more%0Ainformative%20features.%20Extensive%20ablation%20studies%20on%20DQS%20and%20DGA%20demonstrate%20its%0Aeffectiveness.%20Furthermore%2C%20our%20SEED%20achieves%20state-of-the-art%20detection%0Aperformance%20on%20both%20the%20large-scale%20Waymo%20and%20nuScenes%20datasets%2C%20illustrating%0Athe%20superiority%20of%20our%20proposed%20method.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/happinesslz/SEED%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEED%253A%2520A%2520Simple%2520and%2520Effective%25203D%2520DETR%2520in%2520Point%2520Clouds%26entry.906535625%3DZhe%2520Liu%2520and%2520Jinghua%2520Hou%2520and%2520Xiaoqing%2520Ye%2520and%2520Tong%2520Wang%2520and%2520Jingdong%2520Wang%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Recently%252C%2520detection%2520transformers%2520%2528DETRs%2529%2520have%2520gradually%2520taken%2520a%2520dominant%250Aposition%2520in%25202D%2520detection%2520thanks%2520to%2520their%2520elegant%2520framework.%2520However%252C%2520DETR-based%250Adetectors%2520for%25203D%2520point%2520clouds%2520are%2520still%2520difficult%2520to%2520achieve%2520satisfactory%250Aperformance.%2520We%2520argue%2520that%2520the%2520main%2520challenges%2520are%2520twofold%253A%25201%2529%2520How%2520to%2520obtain%250Athe%2520appropriate%2520object%2520queries%2520is%2520challenging%2520due%2520to%2520the%2520high%2520sparsity%2520and%250Auneven%2520distribution%2520of%2520point%2520clouds%253B%25202%2529%2520How%2520to%2520implement%2520an%2520effective%2520query%250Ainteraction%2520by%2520exploiting%2520the%2520rich%2520geometric%2520structure%2520of%2520point%2520clouds%2520is%2520not%250Afully%2520explored.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520simple%2520and%2520effective%25203D%2520DETR%2520method%250A%2528SEED%2529%2520for%2520detecting%25203D%2520objects%2520from%2520point%2520clouds%252C%2520which%2520involves%2520a%2520dual%2520query%250Aselection%2520%2528DQS%2529%2520module%2520and%2520a%2520deformable%2520grid%2520attention%2520%2528DGA%2529%2520module.%2520More%250Aconcretely%252C%2520to%2520obtain%2520appropriate%2520queries%252C%2520DQS%2520first%2520ensures%2520a%2520high%2520recall%2520to%250Aretain%2520a%2520large%2520number%2520of%2520queries%2520by%2520the%2520predicted%2520confidence%2520scores%2520and%2520then%250Afurther%2520picks%2520out%2520high-quality%2520queries%2520according%2520to%2520the%2520estimated%2520quality%250Ascores.%2520DGA%2520uniformly%2520divides%2520each%2520reference%2520box%2520into%2520grids%2520as%2520the%2520reference%250Apoints%2520and%2520then%2520utilizes%2520the%2520predicted%2520offsets%2520to%2520achieve%2520a%2520flexible%2520receptive%250Afield%252C%2520allowing%2520the%2520network%2520to%2520focus%2520on%2520relevant%2520regions%2520and%2520capture%2520more%250Ainformative%2520features.%2520Extensive%2520ablation%2520studies%2520on%2520DQS%2520and%2520DGA%2520demonstrate%2520its%250Aeffectiveness.%2520Furthermore%252C%2520our%2520SEED%2520achieves%2520state-of-the-art%2520detection%250Aperformance%2520on%2520both%2520the%2520large-scale%2520Waymo%2520and%2520nuScenes%2520datasets%252C%2520illustrating%250Athe%2520superiority%2520of%2520our%2520proposed%2520method.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/happinesslz/SEED%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEED%3A%20A%20Simple%20and%20Effective%203D%20DETR%20in%20Point%20Clouds&entry.906535625=Zhe%20Liu%20and%20Jinghua%20Hou%20and%20Xiaoqing%20Ye%20and%20Tong%20Wang%20and%20Jingdong%20Wang%20and%20Xiang%20Bai&entry.1292438233=%20%20Recently%2C%20detection%20transformers%20%28DETRs%29%20have%20gradually%20taken%20a%20dominant%0Aposition%20in%202D%20detection%20thanks%20to%20their%20elegant%20framework.%20However%2C%20DETR-based%0Adetectors%20for%203D%20point%20clouds%20are%20still%20difficult%20to%20achieve%20satisfactory%0Aperformance.%20We%20argue%20that%20the%20main%20challenges%20are%20twofold%3A%201%29%20How%20to%20obtain%0Athe%20appropriate%20object%20queries%20is%20challenging%20due%20to%20the%20high%20sparsity%20and%0Auneven%20distribution%20of%20point%20clouds%3B%202%29%20How%20to%20implement%20an%20effective%20query%0Ainteraction%20by%20exploiting%20the%20rich%20geometric%20structure%20of%20point%20clouds%20is%20not%0Afully%20explored.%20To%20this%20end%2C%20we%20propose%20a%20simple%20and%20effective%203D%20DETR%20method%0A%28SEED%29%20for%20detecting%203D%20objects%20from%20point%20clouds%2C%20which%20involves%20a%20dual%20query%0Aselection%20%28DQS%29%20module%20and%20a%20deformable%20grid%20attention%20%28DGA%29%20module.%20More%0Aconcretely%2C%20to%20obtain%20appropriate%20queries%2C%20DQS%20first%20ensures%20a%20high%20recall%20to%0Aretain%20a%20large%20number%20of%20queries%20by%20the%20predicted%20confidence%20scores%20and%20then%0Afurther%20picks%20out%20high-quality%20queries%20according%20to%20the%20estimated%20quality%0Ascores.%20DGA%20uniformly%20divides%20each%20reference%20box%20into%20grids%20as%20the%20reference%0Apoints%20and%20then%20utilizes%20the%20predicted%20offsets%20to%20achieve%20a%20flexible%20receptive%0Afield%2C%20allowing%20the%20network%20to%20focus%20on%20relevant%20regions%20and%20capture%20more%0Ainformative%20features.%20Extensive%20ablation%20studies%20on%20DQS%20and%20DGA%20demonstrate%20its%0Aeffectiveness.%20Furthermore%2C%20our%20SEED%20achieves%20state-of-the-art%20detection%0Aperformance%20on%20both%20the%20large-scale%20Waymo%20and%20nuScenes%20datasets%2C%20illustrating%0Athe%20superiority%20of%20our%20proposed%20method.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/happinesslz/SEED%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10749v1&entry.124074799=Read"},
{"title": "PCT: Perspective Cue Training Framework for Multi-Camera BEV\n  Segmentation", "author": "Haruya Ishikawa and Takumi Iida and Yoshinori Konishi and Yoshimitsu Aoki", "abstract": "  Generating annotations for bird's-eye-view (BEV) segmentation presents\nsignificant challenges due to the scenes' complexity and the high manual\nannotation cost. In this work, we address these challenges by leveraging the\nabundance of unlabeled data available. We propose the Perspective Cue Training\n(PCT) framework, a novel training framework that utilizes pseudo-labels\ngenerated from unlabeled perspective images using publicly available semantic\nsegmentation models trained on large street-view datasets. PCT applies a\nperspective view task head to the image encoder shared with the BEV\nsegmentation head, effectively utilizing the unlabeled data to be trained with\nthe generated pseudo-labels. Since image encoders are present in nearly all\ncamera-based BEV segmentation architectures, PCT is flexible and applicable to\nvarious existing BEV architectures. PCT can be applied to various settings\nwhere unlabeled data is available. In this paper, we applied PCT for\nsemi-supervised learning (SSL) and unsupervised domain adaptation (UDA).\nAdditionally, we introduce strong input perturbation through Camera Dropout\n(CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are\ncrucial for enhancing SSL capabilities using our teacher-student framework. Our\ncomprehensive approach is simple and flexible but yields significant\nimprovements over various baselines for SSL and UDA, achieving competitive\nperformances even against the current state-of-the-art.\n", "link": "http://arxiv.org/abs/2403.12530v2", "date": "2024-07-15", "relevancy": 2.2941, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5867}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5819}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCT%3A%20Perspective%20Cue%20Training%20Framework%20for%20Multi-Camera%20BEV%0A%20%20Segmentation&body=Title%3A%20PCT%3A%20Perspective%20Cue%20Training%20Framework%20for%20Multi-Camera%20BEV%0A%20%20Segmentation%0AAuthor%3A%20Haruya%20Ishikawa%20and%20Takumi%20Iida%20and%20Yoshinori%20Konishi%20and%20Yoshimitsu%20Aoki%0AAbstract%3A%20%20%20Generating%20annotations%20for%20bird%27s-eye-view%20%28BEV%29%20segmentation%20presents%0Asignificant%20challenges%20due%20to%20the%20scenes%27%20complexity%20and%20the%20high%20manual%0Aannotation%20cost.%20In%20this%20work%2C%20we%20address%20these%20challenges%20by%20leveraging%20the%0Aabundance%20of%20unlabeled%20data%20available.%20We%20propose%20the%20Perspective%20Cue%20Training%0A%28PCT%29%20framework%2C%20a%20novel%20training%20framework%20that%20utilizes%20pseudo-labels%0Agenerated%20from%20unlabeled%20perspective%20images%20using%20publicly%20available%20semantic%0Asegmentation%20models%20trained%20on%20large%20street-view%20datasets.%20PCT%20applies%20a%0Aperspective%20view%20task%20head%20to%20the%20image%20encoder%20shared%20with%20the%20BEV%0Asegmentation%20head%2C%20effectively%20utilizing%20the%20unlabeled%20data%20to%20be%20trained%20with%0Athe%20generated%20pseudo-labels.%20Since%20image%20encoders%20are%20present%20in%20nearly%20all%0Acamera-based%20BEV%20segmentation%20architectures%2C%20PCT%20is%20flexible%20and%20applicable%20to%0Avarious%20existing%20BEV%20architectures.%20PCT%20can%20be%20applied%20to%20various%20settings%0Awhere%20unlabeled%20data%20is%20available.%20In%20this%20paper%2C%20we%20applied%20PCT%20for%0Asemi-supervised%20learning%20%28SSL%29%20and%20unsupervised%20domain%20adaptation%20%28UDA%29.%0AAdditionally%2C%20we%20introduce%20strong%20input%20perturbation%20through%20Camera%20Dropout%0A%28CamDrop%29%20and%20feature%20perturbation%20via%20BEV%20Feature%20Dropout%20%28BFD%29%2C%20which%20are%0Acrucial%20for%20enhancing%20SSL%20capabilities%20using%20our%20teacher-student%20framework.%20Our%0Acomprehensive%20approach%20is%20simple%20and%20flexible%20but%20yields%20significant%0Aimprovements%20over%20various%20baselines%20for%20SSL%20and%20UDA%2C%20achieving%20competitive%0Aperformances%20even%20against%20the%20current%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCT%253A%2520Perspective%2520Cue%2520Training%2520Framework%2520for%2520Multi-Camera%2520BEV%250A%2520%2520Segmentation%26entry.906535625%3DHaruya%2520Ishikawa%2520and%2520Takumi%2520Iida%2520and%2520Yoshinori%2520Konishi%2520and%2520Yoshimitsu%2520Aoki%26entry.1292438233%3D%2520%2520Generating%2520annotations%2520for%2520bird%2527s-eye-view%2520%2528BEV%2529%2520segmentation%2520presents%250Asignificant%2520challenges%2520due%2520to%2520the%2520scenes%2527%2520complexity%2520and%2520the%2520high%2520manual%250Aannotation%2520cost.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520challenges%2520by%2520leveraging%2520the%250Aabundance%2520of%2520unlabeled%2520data%2520available.%2520We%2520propose%2520the%2520Perspective%2520Cue%2520Training%250A%2528PCT%2529%2520framework%252C%2520a%2520novel%2520training%2520framework%2520that%2520utilizes%2520pseudo-labels%250Agenerated%2520from%2520unlabeled%2520perspective%2520images%2520using%2520publicly%2520available%2520semantic%250Asegmentation%2520models%2520trained%2520on%2520large%2520street-view%2520datasets.%2520PCT%2520applies%2520a%250Aperspective%2520view%2520task%2520head%2520to%2520the%2520image%2520encoder%2520shared%2520with%2520the%2520BEV%250Asegmentation%2520head%252C%2520effectively%2520utilizing%2520the%2520unlabeled%2520data%2520to%2520be%2520trained%2520with%250Athe%2520generated%2520pseudo-labels.%2520Since%2520image%2520encoders%2520are%2520present%2520in%2520nearly%2520all%250Acamera-based%2520BEV%2520segmentation%2520architectures%252C%2520PCT%2520is%2520flexible%2520and%2520applicable%2520to%250Avarious%2520existing%2520BEV%2520architectures.%2520PCT%2520can%2520be%2520applied%2520to%2520various%2520settings%250Awhere%2520unlabeled%2520data%2520is%2520available.%2520In%2520this%2520paper%252C%2520we%2520applied%2520PCT%2520for%250Asemi-supervised%2520learning%2520%2528SSL%2529%2520and%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529.%250AAdditionally%252C%2520we%2520introduce%2520strong%2520input%2520perturbation%2520through%2520Camera%2520Dropout%250A%2528CamDrop%2529%2520and%2520feature%2520perturbation%2520via%2520BEV%2520Feature%2520Dropout%2520%2528BFD%2529%252C%2520which%2520are%250Acrucial%2520for%2520enhancing%2520SSL%2520capabilities%2520using%2520our%2520teacher-student%2520framework.%2520Our%250Acomprehensive%2520approach%2520is%2520simple%2520and%2520flexible%2520but%2520yields%2520significant%250Aimprovements%2520over%2520various%2520baselines%2520for%2520SSL%2520and%2520UDA%252C%2520achieving%2520competitive%250Aperformances%2520even%2520against%2520the%2520current%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCT%3A%20Perspective%20Cue%20Training%20Framework%20for%20Multi-Camera%20BEV%0A%20%20Segmentation&entry.906535625=Haruya%20Ishikawa%20and%20Takumi%20Iida%20and%20Yoshinori%20Konishi%20and%20Yoshimitsu%20Aoki&entry.1292438233=%20%20Generating%20annotations%20for%20bird%27s-eye-view%20%28BEV%29%20segmentation%20presents%0Asignificant%20challenges%20due%20to%20the%20scenes%27%20complexity%20and%20the%20high%20manual%0Aannotation%20cost.%20In%20this%20work%2C%20we%20address%20these%20challenges%20by%20leveraging%20the%0Aabundance%20of%20unlabeled%20data%20available.%20We%20propose%20the%20Perspective%20Cue%20Training%0A%28PCT%29%20framework%2C%20a%20novel%20training%20framework%20that%20utilizes%20pseudo-labels%0Agenerated%20from%20unlabeled%20perspective%20images%20using%20publicly%20available%20semantic%0Asegmentation%20models%20trained%20on%20large%20street-view%20datasets.%20PCT%20applies%20a%0Aperspective%20view%20task%20head%20to%20the%20image%20encoder%20shared%20with%20the%20BEV%0Asegmentation%20head%2C%20effectively%20utilizing%20the%20unlabeled%20data%20to%20be%20trained%20with%0Athe%20generated%20pseudo-labels.%20Since%20image%20encoders%20are%20present%20in%20nearly%20all%0Acamera-based%20BEV%20segmentation%20architectures%2C%20PCT%20is%20flexible%20and%20applicable%20to%0Avarious%20existing%20BEV%20architectures.%20PCT%20can%20be%20applied%20to%20various%20settings%0Awhere%20unlabeled%20data%20is%20available.%20In%20this%20paper%2C%20we%20applied%20PCT%20for%0Asemi-supervised%20learning%20%28SSL%29%20and%20unsupervised%20domain%20adaptation%20%28UDA%29.%0AAdditionally%2C%20we%20introduce%20strong%20input%20perturbation%20through%20Camera%20Dropout%0A%28CamDrop%29%20and%20feature%20perturbation%20via%20BEV%20Feature%20Dropout%20%28BFD%29%2C%20which%20are%0Acrucial%20for%20enhancing%20SSL%20capabilities%20using%20our%20teacher-student%20framework.%20Our%0Acomprehensive%20approach%20is%20simple%20and%20flexible%20but%20yields%20significant%0Aimprovements%20over%20various%20baselines%20for%20SSL%20and%20UDA%2C%20achieving%20competitive%0Aperformances%20even%20against%20the%20current%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12530v2&entry.124074799=Read"},
{"title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding", "author": "Yue Fan and Xiaojian Ma and Rujie Wu and Yuntao Du and Jiaqi Li and Zhi Gao and Qing Li", "abstract": "  We explore how reconciling several foundation models (large language models\nand vision-language models) with a novel unified memory mechanism could tackle\nthe challenging video understanding problem, especially capturing the long-term\ntemporal relations in lengthy videos. In particular, the proposed multimodal\nagent VideoAgent: 1) constructs a structured memory to store both the generic\ntemporal event descriptions and object-centric tracking states of the video; 2)\ngiven an input task query, it employs tools including video segment\nlocalization and object memory querying along with other visual foundation\nmodels to interactively solve the task, utilizing the zero-shot tool-use\nability of LLMs. VideoAgent demonstrates impressive performances on several\nlong-horizon video understanding benchmarks, an average increase of 6.6% on\nNExT-QA and 26.0% on EgoSchema over baselines, closing the gap between\nopen-sourced models and private counterparts including Gemini 1.5 Pro.\n", "link": "http://arxiv.org/abs/2403.11481v2", "date": "2024-07-15", "relevancy": 2.2803, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5785}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5642}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoAgent%3A%20A%20Memory-augmented%20Multimodal%20Agent%20for%20Video%20Understanding&body=Title%3A%20VideoAgent%3A%20A%20Memory-augmented%20Multimodal%20Agent%20for%20Video%20Understanding%0AAuthor%3A%20Yue%20Fan%20and%20Xiaojian%20Ma%20and%20Rujie%20Wu%20and%20Yuntao%20Du%20and%20Jiaqi%20Li%20and%20Zhi%20Gao%20and%20Qing%20Li%0AAbstract%3A%20%20%20We%20explore%20how%20reconciling%20several%20foundation%20models%20%28large%20language%20models%0Aand%20vision-language%20models%29%20with%20a%20novel%20unified%20memory%20mechanism%20could%20tackle%0Athe%20challenging%20video%20understanding%20problem%2C%20especially%20capturing%20the%20long-term%0Atemporal%20relations%20in%20lengthy%20videos.%20In%20particular%2C%20the%20proposed%20multimodal%0Aagent%20VideoAgent%3A%201%29%20constructs%20a%20structured%20memory%20to%20store%20both%20the%20generic%0Atemporal%20event%20descriptions%20and%20object-centric%20tracking%20states%20of%20the%20video%3B%202%29%0Agiven%20an%20input%20task%20query%2C%20it%20employs%20tools%20including%20video%20segment%0Alocalization%20and%20object%20memory%20querying%20along%20with%20other%20visual%20foundation%0Amodels%20to%20interactively%20solve%20the%20task%2C%20utilizing%20the%20zero-shot%20tool-use%0Aability%20of%20LLMs.%20VideoAgent%20demonstrates%20impressive%20performances%20on%20several%0Along-horizon%20video%20understanding%20benchmarks%2C%20an%20average%20increase%20of%206.6%25%20on%0ANExT-QA%20and%2026.0%25%20on%20EgoSchema%20over%20baselines%2C%20closing%20the%20gap%20between%0Aopen-sourced%20models%20and%20private%20counterparts%20including%20Gemini%201.5%20Pro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11481v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoAgent%253A%2520A%2520Memory-augmented%2520Multimodal%2520Agent%2520for%2520Video%2520Understanding%26entry.906535625%3DYue%2520Fan%2520and%2520Xiaojian%2520Ma%2520and%2520Rujie%2520Wu%2520and%2520Yuntao%2520Du%2520and%2520Jiaqi%2520Li%2520and%2520Zhi%2520Gao%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520We%2520explore%2520how%2520reconciling%2520several%2520foundation%2520models%2520%2528large%2520language%2520models%250Aand%2520vision-language%2520models%2529%2520with%2520a%2520novel%2520unified%2520memory%2520mechanism%2520could%2520tackle%250Athe%2520challenging%2520video%2520understanding%2520problem%252C%2520especially%2520capturing%2520the%2520long-term%250Atemporal%2520relations%2520in%2520lengthy%2520videos.%2520In%2520particular%252C%2520the%2520proposed%2520multimodal%250Aagent%2520VideoAgent%253A%25201%2529%2520constructs%2520a%2520structured%2520memory%2520to%2520store%2520both%2520the%2520generic%250Atemporal%2520event%2520descriptions%2520and%2520object-centric%2520tracking%2520states%2520of%2520the%2520video%253B%25202%2529%250Agiven%2520an%2520input%2520task%2520query%252C%2520it%2520employs%2520tools%2520including%2520video%2520segment%250Alocalization%2520and%2520object%2520memory%2520querying%2520along%2520with%2520other%2520visual%2520foundation%250Amodels%2520to%2520interactively%2520solve%2520the%2520task%252C%2520utilizing%2520the%2520zero-shot%2520tool-use%250Aability%2520of%2520LLMs.%2520VideoAgent%2520demonstrates%2520impressive%2520performances%2520on%2520several%250Along-horizon%2520video%2520understanding%2520benchmarks%252C%2520an%2520average%2520increase%2520of%25206.6%2525%2520on%250ANExT-QA%2520and%252026.0%2525%2520on%2520EgoSchema%2520over%2520baselines%252C%2520closing%2520the%2520gap%2520between%250Aopen-sourced%2520models%2520and%2520private%2520counterparts%2520including%2520Gemini%25201.5%2520Pro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11481v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoAgent%3A%20A%20Memory-augmented%20Multimodal%20Agent%20for%20Video%20Understanding&entry.906535625=Yue%20Fan%20and%20Xiaojian%20Ma%20and%20Rujie%20Wu%20and%20Yuntao%20Du%20and%20Jiaqi%20Li%20and%20Zhi%20Gao%20and%20Qing%20Li&entry.1292438233=%20%20We%20explore%20how%20reconciling%20several%20foundation%20models%20%28large%20language%20models%0Aand%20vision-language%20models%29%20with%20a%20novel%20unified%20memory%20mechanism%20could%20tackle%0Athe%20challenging%20video%20understanding%20problem%2C%20especially%20capturing%20the%20long-term%0Atemporal%20relations%20in%20lengthy%20videos.%20In%20particular%2C%20the%20proposed%20multimodal%0Aagent%20VideoAgent%3A%201%29%20constructs%20a%20structured%20memory%20to%20store%20both%20the%20generic%0Atemporal%20event%20descriptions%20and%20object-centric%20tracking%20states%20of%20the%20video%3B%202%29%0Agiven%20an%20input%20task%20query%2C%20it%20employs%20tools%20including%20video%20segment%0Alocalization%20and%20object%20memory%20querying%20along%20with%20other%20visual%20foundation%0Amodels%20to%20interactively%20solve%20the%20task%2C%20utilizing%20the%20zero-shot%20tool-use%0Aability%20of%20LLMs.%20VideoAgent%20demonstrates%20impressive%20performances%20on%20several%0Along-horizon%20video%20understanding%20benchmarks%2C%20an%20average%20increase%20of%206.6%25%20on%0ANExT-QA%20and%2026.0%25%20on%20EgoSchema%20over%20baselines%2C%20closing%20the%20gap%20between%0Aopen-sourced%20models%20and%20private%20counterparts%20including%20Gemini%201.5%20Pro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11481v2&entry.124074799=Read"},
{"title": "DataDream: Few-shot Guided Dataset Generation", "author": "Jae Myung Kim and Jessica Bader and Stephan Alaniz and Cordelia Schmid and Zeynep Akata", "abstract": "  While text-to-image diffusion models have been shown to achieve\nstate-of-the-art results in image synthesis, they have yet to prove their\neffectiveness in downstream applications. Previous work has proposed to\ngenerate data for image classifier training given limited real data access.\nHowever, these methods struggle to generate in-distribution images or depict\nfine-grained features, thereby hindering the generalization of classification\nmodels trained on synthetic datasets. We propose DataDream, a framework for\nsynthesizing classification datasets that more faithfully represents the real\ndata distribution when guided by few-shot examples of the target classes.\nDataDream fine-tunes LoRA weights for the image generation model on the few\nreal images before generating the training data using the adapted model. We\nthen fine-tune LoRA weights for CLIP using the synthetic data to improve\ndownstream image classification over previous approaches on a large variety of\ndatasets. We demonstrate the efficacy of DataDream through extensive\nexperiments, surpassing state-of-the-art classification accuracy with few-shot\ndata across 7 out of 10 datasets, while being competitive on the other 3.\nAdditionally, we provide insights into the impact of various factors, such as\nthe number of real-shot and generated images as well as the fine-tuning compute\non model performance. The code is available at\nhttps://github.com/ExplainableML/DataDream.\n", "link": "http://arxiv.org/abs/2407.10910v1", "date": "2024-07-15", "relevancy": 2.2687, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.587}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataDream%3A%20Few-shot%20Guided%20Dataset%20Generation&body=Title%3A%20DataDream%3A%20Few-shot%20Guided%20Dataset%20Generation%0AAuthor%3A%20Jae%20Myung%20Kim%20and%20Jessica%20Bader%20and%20Stephan%20Alaniz%20and%20Cordelia%20Schmid%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20While%20text-to-image%20diffusion%20models%20have%20been%20shown%20to%20achieve%0Astate-of-the-art%20results%20in%20image%20synthesis%2C%20they%20have%20yet%20to%20prove%20their%0Aeffectiveness%20in%20downstream%20applications.%20Previous%20work%20has%20proposed%20to%0Agenerate%20data%20for%20image%20classifier%20training%20given%20limited%20real%20data%20access.%0AHowever%2C%20these%20methods%20struggle%20to%20generate%20in-distribution%20images%20or%20depict%0Afine-grained%20features%2C%20thereby%20hindering%20the%20generalization%20of%20classification%0Amodels%20trained%20on%20synthetic%20datasets.%20We%20propose%20DataDream%2C%20a%20framework%20for%0Asynthesizing%20classification%20datasets%20that%20more%20faithfully%20represents%20the%20real%0Adata%20distribution%20when%20guided%20by%20few-shot%20examples%20of%20the%20target%20classes.%0ADataDream%20fine-tunes%20LoRA%20weights%20for%20the%20image%20generation%20model%20on%20the%20few%0Areal%20images%20before%20generating%20the%20training%20data%20using%20the%20adapted%20model.%20We%0Athen%20fine-tune%20LoRA%20weights%20for%20CLIP%20using%20the%20synthetic%20data%20to%20improve%0Adownstream%20image%20classification%20over%20previous%20approaches%20on%20a%20large%20variety%20of%0Adatasets.%20We%20demonstrate%20the%20efficacy%20of%20DataDream%20through%20extensive%0Aexperiments%2C%20surpassing%20state-of-the-art%20classification%20accuracy%20with%20few-shot%0Adata%20across%207%20out%20of%2010%20datasets%2C%20while%20being%20competitive%20on%20the%20other%203.%0AAdditionally%2C%20we%20provide%20insights%20into%20the%20impact%20of%20various%20factors%2C%20such%20as%0Athe%20number%20of%20real-shot%20and%20generated%20images%20as%20well%20as%20the%20fine-tuning%20compute%0Aon%20model%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ExplainableML/DataDream.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataDream%253A%2520Few-shot%2520Guided%2520Dataset%2520Generation%26entry.906535625%3DJae%2520Myung%2520Kim%2520and%2520Jessica%2520Bader%2520and%2520Stephan%2520Alaniz%2520and%2520Cordelia%2520Schmid%2520and%2520Zeynep%2520Akata%26entry.1292438233%3D%2520%2520While%2520text-to-image%2520diffusion%2520models%2520have%2520been%2520shown%2520to%2520achieve%250Astate-of-the-art%2520results%2520in%2520image%2520synthesis%252C%2520they%2520have%2520yet%2520to%2520prove%2520their%250Aeffectiveness%2520in%2520downstream%2520applications.%2520Previous%2520work%2520has%2520proposed%2520to%250Agenerate%2520data%2520for%2520image%2520classifier%2520training%2520given%2520limited%2520real%2520data%2520access.%250AHowever%252C%2520these%2520methods%2520struggle%2520to%2520generate%2520in-distribution%2520images%2520or%2520depict%250Afine-grained%2520features%252C%2520thereby%2520hindering%2520the%2520generalization%2520of%2520classification%250Amodels%2520trained%2520on%2520synthetic%2520datasets.%2520We%2520propose%2520DataDream%252C%2520a%2520framework%2520for%250Asynthesizing%2520classification%2520datasets%2520that%2520more%2520faithfully%2520represents%2520the%2520real%250Adata%2520distribution%2520when%2520guided%2520by%2520few-shot%2520examples%2520of%2520the%2520target%2520classes.%250ADataDream%2520fine-tunes%2520LoRA%2520weights%2520for%2520the%2520image%2520generation%2520model%2520on%2520the%2520few%250Areal%2520images%2520before%2520generating%2520the%2520training%2520data%2520using%2520the%2520adapted%2520model.%2520We%250Athen%2520fine-tune%2520LoRA%2520weights%2520for%2520CLIP%2520using%2520the%2520synthetic%2520data%2520to%2520improve%250Adownstream%2520image%2520classification%2520over%2520previous%2520approaches%2520on%2520a%2520large%2520variety%2520of%250Adatasets.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520DataDream%2520through%2520extensive%250Aexperiments%252C%2520surpassing%2520state-of-the-art%2520classification%2520accuracy%2520with%2520few-shot%250Adata%2520across%25207%2520out%2520of%252010%2520datasets%252C%2520while%2520being%2520competitive%2520on%2520the%2520other%25203.%250AAdditionally%252C%2520we%2520provide%2520insights%2520into%2520the%2520impact%2520of%2520various%2520factors%252C%2520such%2520as%250Athe%2520number%2520of%2520real-shot%2520and%2520generated%2520images%2520as%2520well%2520as%2520the%2520fine-tuning%2520compute%250Aon%2520model%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ExplainableML/DataDream.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataDream%3A%20Few-shot%20Guided%20Dataset%20Generation&entry.906535625=Jae%20Myung%20Kim%20and%20Jessica%20Bader%20and%20Stephan%20Alaniz%20and%20Cordelia%20Schmid%20and%20Zeynep%20Akata&entry.1292438233=%20%20While%20text-to-image%20diffusion%20models%20have%20been%20shown%20to%20achieve%0Astate-of-the-art%20results%20in%20image%20synthesis%2C%20they%20have%20yet%20to%20prove%20their%0Aeffectiveness%20in%20downstream%20applications.%20Previous%20work%20has%20proposed%20to%0Agenerate%20data%20for%20image%20classifier%20training%20given%20limited%20real%20data%20access.%0AHowever%2C%20these%20methods%20struggle%20to%20generate%20in-distribution%20images%20or%20depict%0Afine-grained%20features%2C%20thereby%20hindering%20the%20generalization%20of%20classification%0Amodels%20trained%20on%20synthetic%20datasets.%20We%20propose%20DataDream%2C%20a%20framework%20for%0Asynthesizing%20classification%20datasets%20that%20more%20faithfully%20represents%20the%20real%0Adata%20distribution%20when%20guided%20by%20few-shot%20examples%20of%20the%20target%20classes.%0ADataDream%20fine-tunes%20LoRA%20weights%20for%20the%20image%20generation%20model%20on%20the%20few%0Areal%20images%20before%20generating%20the%20training%20data%20using%20the%20adapted%20model.%20We%0Athen%20fine-tune%20LoRA%20weights%20for%20CLIP%20using%20the%20synthetic%20data%20to%20improve%0Adownstream%20image%20classification%20over%20previous%20approaches%20on%20a%20large%20variety%20of%0Adatasets.%20We%20demonstrate%20the%20efficacy%20of%20DataDream%20through%20extensive%0Aexperiments%2C%20surpassing%20state-of-the-art%20classification%20accuracy%20with%20few-shot%0Adata%20across%207%20out%20of%2010%20datasets%2C%20while%20being%20competitive%20on%20the%20other%203.%0AAdditionally%2C%20we%20provide%20insights%20into%20the%20impact%20of%20various%20factors%2C%20such%20as%0Athe%20number%20of%20real-shot%20and%20generated%20images%20as%20well%20as%20the%20fine-tuning%20compute%0Aon%20model%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ExplainableML/DataDream.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10910v1&entry.124074799=Read"},
{"title": "IE-NeRF: Inpainting Enhanced Neural Radiance Fields in the Wild", "author": "Shuaixian Wang and Haoran Xu and Yaokun Li and Jiwei Chen and Guang Tan", "abstract": "  We present a novel approach for synthesizing realistic novel views using\nNeural Radiance Fields (NeRF) with uncontrolled photos in the wild. While NeRF\nhas shown impressive results in controlled settings, it struggles with\ntransient objects commonly found in dynamic and time-varying scenes. Our\nframework called \\textit{Inpainting Enhanced NeRF}, or \\ours, enhances the\nconventional NeRF by drawing inspiration from the technique of image\ninpainting. Specifically, our approach extends the Multi-Layer Perceptrons\n(MLP) of NeRF, enabling it to simultaneously generate intrinsic properties\n(static color, density) and extrinsic transient masks. We introduce an\ninpainting module that leverages the transient masks to effectively exclude\nocclusions, resulting in improved volume rendering quality. Additionally, we\npropose a new training strategy with frequency regularization to address the\nsparsity issue of low-frequency transient components. We evaluate our approach\non internet photo collections of landmarks, demonstrating its ability to\ngenerate high-quality novel views and achieve state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2407.10695v1", "date": "2024-07-15", "relevancy": 2.2686, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5971}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5475}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IE-NeRF%3A%20Inpainting%20Enhanced%20Neural%20Radiance%20Fields%20in%20the%20Wild&body=Title%3A%20IE-NeRF%3A%20Inpainting%20Enhanced%20Neural%20Radiance%20Fields%20in%20the%20Wild%0AAuthor%3A%20Shuaixian%20Wang%20and%20Haoran%20Xu%20and%20Yaokun%20Li%20and%20Jiwei%20Chen%20and%20Guang%20Tan%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20synthesizing%20realistic%20novel%20views%20using%0ANeural%20Radiance%20Fields%20%28NeRF%29%20with%20uncontrolled%20photos%20in%20the%20wild.%20While%20NeRF%0Ahas%20shown%20impressive%20results%20in%20controlled%20settings%2C%20it%20struggles%20with%0Atransient%20objects%20commonly%20found%20in%20dynamic%20and%20time-varying%20scenes.%20Our%0Aframework%20called%20%5Ctextit%7BInpainting%20Enhanced%20NeRF%7D%2C%20or%20%5Cours%2C%20enhances%20the%0Aconventional%20NeRF%20by%20drawing%20inspiration%20from%20the%20technique%20of%20image%0Ainpainting.%20Specifically%2C%20our%20approach%20extends%20the%20Multi-Layer%20Perceptrons%0A%28MLP%29%20of%20NeRF%2C%20enabling%20it%20to%20simultaneously%20generate%20intrinsic%20properties%0A%28static%20color%2C%20density%29%20and%20extrinsic%20transient%20masks.%20We%20introduce%20an%0Ainpainting%20module%20that%20leverages%20the%20transient%20masks%20to%20effectively%20exclude%0Aocclusions%2C%20resulting%20in%20improved%20volume%20rendering%20quality.%20Additionally%2C%20we%0Apropose%20a%20new%20training%20strategy%20with%20frequency%20regularization%20to%20address%20the%0Asparsity%20issue%20of%20low-frequency%20transient%20components.%20We%20evaluate%20our%20approach%0Aon%20internet%20photo%20collections%20of%20landmarks%2C%20demonstrating%20its%20ability%20to%0Agenerate%20high-quality%20novel%20views%20and%20achieve%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIE-NeRF%253A%2520Inpainting%2520Enhanced%2520Neural%2520Radiance%2520Fields%2520in%2520the%2520Wild%26entry.906535625%3DShuaixian%2520Wang%2520and%2520Haoran%2520Xu%2520and%2520Yaokun%2520Li%2520and%2520Jiwei%2520Chen%2520and%2520Guang%2520Tan%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520synthesizing%2520realistic%2520novel%2520views%2520using%250ANeural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520with%2520uncontrolled%2520photos%2520in%2520the%2520wild.%2520While%2520NeRF%250Ahas%2520shown%2520impressive%2520results%2520in%2520controlled%2520settings%252C%2520it%2520struggles%2520with%250Atransient%2520objects%2520commonly%2520found%2520in%2520dynamic%2520and%2520time-varying%2520scenes.%2520Our%250Aframework%2520called%2520%255Ctextit%257BInpainting%2520Enhanced%2520NeRF%257D%252C%2520or%2520%255Cours%252C%2520enhances%2520the%250Aconventional%2520NeRF%2520by%2520drawing%2520inspiration%2520from%2520the%2520technique%2520of%2520image%250Ainpainting.%2520Specifically%252C%2520our%2520approach%2520extends%2520the%2520Multi-Layer%2520Perceptrons%250A%2528MLP%2529%2520of%2520NeRF%252C%2520enabling%2520it%2520to%2520simultaneously%2520generate%2520intrinsic%2520properties%250A%2528static%2520color%252C%2520density%2529%2520and%2520extrinsic%2520transient%2520masks.%2520We%2520introduce%2520an%250Ainpainting%2520module%2520that%2520leverages%2520the%2520transient%2520masks%2520to%2520effectively%2520exclude%250Aocclusions%252C%2520resulting%2520in%2520improved%2520volume%2520rendering%2520quality.%2520Additionally%252C%2520we%250Apropose%2520a%2520new%2520training%2520strategy%2520with%2520frequency%2520regularization%2520to%2520address%2520the%250Asparsity%2520issue%2520of%2520low-frequency%2520transient%2520components.%2520We%2520evaluate%2520our%2520approach%250Aon%2520internet%2520photo%2520collections%2520of%2520landmarks%252C%2520demonstrating%2520its%2520ability%2520to%250Agenerate%2520high-quality%2520novel%2520views%2520and%2520achieve%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IE-NeRF%3A%20Inpainting%20Enhanced%20Neural%20Radiance%20Fields%20in%20the%20Wild&entry.906535625=Shuaixian%20Wang%20and%20Haoran%20Xu%20and%20Yaokun%20Li%20and%20Jiwei%20Chen%20and%20Guang%20Tan&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20synthesizing%20realistic%20novel%20views%20using%0ANeural%20Radiance%20Fields%20%28NeRF%29%20with%20uncontrolled%20photos%20in%20the%20wild.%20While%20NeRF%0Ahas%20shown%20impressive%20results%20in%20controlled%20settings%2C%20it%20struggles%20with%0Atransient%20objects%20commonly%20found%20in%20dynamic%20and%20time-varying%20scenes.%20Our%0Aframework%20called%20%5Ctextit%7BInpainting%20Enhanced%20NeRF%7D%2C%20or%20%5Cours%2C%20enhances%20the%0Aconventional%20NeRF%20by%20drawing%20inspiration%20from%20the%20technique%20of%20image%0Ainpainting.%20Specifically%2C%20our%20approach%20extends%20the%20Multi-Layer%20Perceptrons%0A%28MLP%29%20of%20NeRF%2C%20enabling%20it%20to%20simultaneously%20generate%20intrinsic%20properties%0A%28static%20color%2C%20density%29%20and%20extrinsic%20transient%20masks.%20We%20introduce%20an%0Ainpainting%20module%20that%20leverages%20the%20transient%20masks%20to%20effectively%20exclude%0Aocclusions%2C%20resulting%20in%20improved%20volume%20rendering%20quality.%20Additionally%2C%20we%0Apropose%20a%20new%20training%20strategy%20with%20frequency%20regularization%20to%20address%20the%0Asparsity%20issue%20of%20low-frequency%20transient%20components.%20We%20evaluate%20our%20approach%0Aon%20internet%20photo%20collections%20of%20landmarks%2C%20demonstrating%20its%20ability%20to%0Agenerate%20high-quality%20novel%20views%20and%20achieve%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10695v1&entry.124074799=Read"},
{"title": "TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking\n  Styles", "author": "Yifeng Ma and Suzhen Wang and Yu Ding and Lincheng Li and Bowen Ma and Tangjie Lv and Changjie Fan and Zhipeng Hu and Zhidong Deng and Xin Yu", "abstract": "  Audio-driven talking head generation has drawn growing attention. To produce\ntalking head videos with desired facial expressions, previous methods rely on\nextra reference videos to provide expression information, which may be\ndifficult to find and hence limits their usage. In this work, we propose\nTalkCLIP, a framework that can generate talking heads where the expressions are\nspecified by natural language, hence allowing for specifying expressions more\nconveniently. To model the mapping from text to expressions, we first construct\na text-video paired talking head dataset where each video has diverse text\ndescriptions that depict both coarse-grained emotions and fine-grained facial\nmovements. Leveraging the proposed dataset, we introduce a CLIP-based style\nencoder that projects natural language-based descriptions to the\nrepresentations of expressions. TalkCLIP can even infer expressions for\ndescriptions unseen during training. TalkCLIP can also use text to modulate\nexpression intensity and edit expressions. Extensive experiments demonstrate\nthat TalkCLIP achieves the advanced capability of generating photo-realistic\ntalking heads with vivid facial expressions guided by text descriptions.\n", "link": "http://arxiv.org/abs/2304.00334v3", "date": "2024-07-15", "relevancy": 2.2504, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5717}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5673}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TalkCLIP%3A%20Talking%20Head%20Generation%20with%20Text-Guided%20Expressive%20Speaking%0A%20%20Styles&body=Title%3A%20TalkCLIP%3A%20Talking%20Head%20Generation%20with%20Text-Guided%20Expressive%20Speaking%0A%20%20Styles%0AAuthor%3A%20Yifeng%20Ma%20and%20Suzhen%20Wang%20and%20Yu%20Ding%20and%20Lincheng%20Li%20and%20Bowen%20Ma%20and%20Tangjie%20Lv%20and%20Changjie%20Fan%20and%20Zhipeng%20Hu%20and%20Zhidong%20Deng%20and%20Xin%20Yu%0AAbstract%3A%20%20%20Audio-driven%20talking%20head%20generation%20has%20drawn%20growing%20attention.%20To%20produce%0Atalking%20head%20videos%20with%20desired%20facial%20expressions%2C%20previous%20methods%20rely%20on%0Aextra%20reference%20videos%20to%20provide%20expression%20information%2C%20which%20may%20be%0Adifficult%20to%20find%20and%20hence%20limits%20their%20usage.%20In%20this%20work%2C%20we%20propose%0ATalkCLIP%2C%20a%20framework%20that%20can%20generate%20talking%20heads%20where%20the%20expressions%20are%0Aspecified%20by%20natural%20language%2C%20hence%20allowing%20for%20specifying%20expressions%20more%0Aconveniently.%20To%20model%20the%20mapping%20from%20text%20to%20expressions%2C%20we%20first%20construct%0Aa%20text-video%20paired%20talking%20head%20dataset%20where%20each%20video%20has%20diverse%20text%0Adescriptions%20that%20depict%20both%20coarse-grained%20emotions%20and%20fine-grained%20facial%0Amovements.%20Leveraging%20the%20proposed%20dataset%2C%20we%20introduce%20a%20CLIP-based%20style%0Aencoder%20that%20projects%20natural%20language-based%20descriptions%20to%20the%0Arepresentations%20of%20expressions.%20TalkCLIP%20can%20even%20infer%20expressions%20for%0Adescriptions%20unseen%20during%20training.%20TalkCLIP%20can%20also%20use%20text%20to%20modulate%0Aexpression%20intensity%20and%20edit%20expressions.%20Extensive%20experiments%20demonstrate%0Athat%20TalkCLIP%20achieves%20the%20advanced%20capability%20of%20generating%20photo-realistic%0Atalking%20heads%20with%20vivid%20facial%20expressions%20guided%20by%20text%20descriptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.00334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalkCLIP%253A%2520Talking%2520Head%2520Generation%2520with%2520Text-Guided%2520Expressive%2520Speaking%250A%2520%2520Styles%26entry.906535625%3DYifeng%2520Ma%2520and%2520Suzhen%2520Wang%2520and%2520Yu%2520Ding%2520and%2520Lincheng%2520Li%2520and%2520Bowen%2520Ma%2520and%2520Tangjie%2520Lv%2520and%2520Changjie%2520Fan%2520and%2520Zhipeng%2520Hu%2520and%2520Zhidong%2520Deng%2520and%2520Xin%2520Yu%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520head%2520generation%2520has%2520drawn%2520growing%2520attention.%2520To%2520produce%250Atalking%2520head%2520videos%2520with%2520desired%2520facial%2520expressions%252C%2520previous%2520methods%2520rely%2520on%250Aextra%2520reference%2520videos%2520to%2520provide%2520expression%2520information%252C%2520which%2520may%2520be%250Adifficult%2520to%2520find%2520and%2520hence%2520limits%2520their%2520usage.%2520In%2520this%2520work%252C%2520we%2520propose%250ATalkCLIP%252C%2520a%2520framework%2520that%2520can%2520generate%2520talking%2520heads%2520where%2520the%2520expressions%2520are%250Aspecified%2520by%2520natural%2520language%252C%2520hence%2520allowing%2520for%2520specifying%2520expressions%2520more%250Aconveniently.%2520To%2520model%2520the%2520mapping%2520from%2520text%2520to%2520expressions%252C%2520we%2520first%2520construct%250Aa%2520text-video%2520paired%2520talking%2520head%2520dataset%2520where%2520each%2520video%2520has%2520diverse%2520text%250Adescriptions%2520that%2520depict%2520both%2520coarse-grained%2520emotions%2520and%2520fine-grained%2520facial%250Amovements.%2520Leveraging%2520the%2520proposed%2520dataset%252C%2520we%2520introduce%2520a%2520CLIP-based%2520style%250Aencoder%2520that%2520projects%2520natural%2520language-based%2520descriptions%2520to%2520the%250Arepresentations%2520of%2520expressions.%2520TalkCLIP%2520can%2520even%2520infer%2520expressions%2520for%250Adescriptions%2520unseen%2520during%2520training.%2520TalkCLIP%2520can%2520also%2520use%2520text%2520to%2520modulate%250Aexpression%2520intensity%2520and%2520edit%2520expressions.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520TalkCLIP%2520achieves%2520the%2520advanced%2520capability%2520of%2520generating%2520photo-realistic%250Atalking%2520heads%2520with%2520vivid%2520facial%2520expressions%2520guided%2520by%2520text%2520descriptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.00334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TalkCLIP%3A%20Talking%20Head%20Generation%20with%20Text-Guided%20Expressive%20Speaking%0A%20%20Styles&entry.906535625=Yifeng%20Ma%20and%20Suzhen%20Wang%20and%20Yu%20Ding%20and%20Lincheng%20Li%20and%20Bowen%20Ma%20and%20Tangjie%20Lv%20and%20Changjie%20Fan%20and%20Zhipeng%20Hu%20and%20Zhidong%20Deng%20and%20Xin%20Yu&entry.1292438233=%20%20Audio-driven%20talking%20head%20generation%20has%20drawn%20growing%20attention.%20To%20produce%0Atalking%20head%20videos%20with%20desired%20facial%20expressions%2C%20previous%20methods%20rely%20on%0Aextra%20reference%20videos%20to%20provide%20expression%20information%2C%20which%20may%20be%0Adifficult%20to%20find%20and%20hence%20limits%20their%20usage.%20In%20this%20work%2C%20we%20propose%0ATalkCLIP%2C%20a%20framework%20that%20can%20generate%20talking%20heads%20where%20the%20expressions%20are%0Aspecified%20by%20natural%20language%2C%20hence%20allowing%20for%20specifying%20expressions%20more%0Aconveniently.%20To%20model%20the%20mapping%20from%20text%20to%20expressions%2C%20we%20first%20construct%0Aa%20text-video%20paired%20talking%20head%20dataset%20where%20each%20video%20has%20diverse%20text%0Adescriptions%20that%20depict%20both%20coarse-grained%20emotions%20and%20fine-grained%20facial%0Amovements.%20Leveraging%20the%20proposed%20dataset%2C%20we%20introduce%20a%20CLIP-based%20style%0Aencoder%20that%20projects%20natural%20language-based%20descriptions%20to%20the%0Arepresentations%20of%20expressions.%20TalkCLIP%20can%20even%20infer%20expressions%20for%0Adescriptions%20unseen%20during%20training.%20TalkCLIP%20can%20also%20use%20text%20to%20modulate%0Aexpression%20intensity%20and%20edit%20expressions.%20Extensive%20experiments%20demonstrate%0Athat%20TalkCLIP%20achieves%20the%20advanced%20capability%20of%20generating%20photo-realistic%0Atalking%20heads%20with%20vivid%20facial%20expressions%20guided%20by%20text%20descriptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00334v3&entry.124074799=Read"},
{"title": "Continual Deep Learning on the Edge via Stochastic Local Competition\n  among Subnetworks", "author": "Theodoros Christophides and Kyriakos Tolias and Sotirios Chatzis", "abstract": "  Continual learning on edge devices poses unique challenges due to stringent\nresource constraints. This paper introduces a novel method that leverages\nstochastic competition principles to promote sparsity, significantly reducing\ndeep network memory footprint and computational demand. Specifically, we\npropose deep networks that comprise blocks of units that compete locally to win\nthe representation of each arising new task; competition takes place in a\nstochastic manner. This type of network organization results in sparse\ntask-specific representations from each network layer; the sparsity pattern is\nobtained during training and is different among tasks. Crucially, our method\nsparsifies both the weights and the weight gradients, thus facilitating\ntraining on edge devices. This is performed on the grounds of winning\nprobability for each unit in a block. During inference, the network retains\nonly the winning unit and zeroes-out all weights pertaining to non-winning\nunits for the task at hand. Thus, our approach is specifically tailored for\ndeployment on edge devices, providing an efficient and scalable solution for\ncontinual learning in resource-limited environments.\n", "link": "http://arxiv.org/abs/2407.10758v1", "date": "2024-07-15", "relevancy": 2.2483, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5782}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5527}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Deep%20Learning%20on%20the%20Edge%20via%20Stochastic%20Local%20Competition%0A%20%20among%20Subnetworks&body=Title%3A%20Continual%20Deep%20Learning%20on%20the%20Edge%20via%20Stochastic%20Local%20Competition%0A%20%20among%20Subnetworks%0AAuthor%3A%20Theodoros%20Christophides%20and%20Kyriakos%20Tolias%20and%20Sotirios%20Chatzis%0AAbstract%3A%20%20%20Continual%20learning%20on%20edge%20devices%20poses%20unique%20challenges%20due%20to%20stringent%0Aresource%20constraints.%20This%20paper%20introduces%20a%20novel%20method%20that%20leverages%0Astochastic%20competition%20principles%20to%20promote%20sparsity%2C%20significantly%20reducing%0Adeep%20network%20memory%20footprint%20and%20computational%20demand.%20Specifically%2C%20we%0Apropose%20deep%20networks%20that%20comprise%20blocks%20of%20units%20that%20compete%20locally%20to%20win%0Athe%20representation%20of%20each%20arising%20new%20task%3B%20competition%20takes%20place%20in%20a%0Astochastic%20manner.%20This%20type%20of%20network%20organization%20results%20in%20sparse%0Atask-specific%20representations%20from%20each%20network%20layer%3B%20the%20sparsity%20pattern%20is%0Aobtained%20during%20training%20and%20is%20different%20among%20tasks.%20Crucially%2C%20our%20method%0Asparsifies%20both%20the%20weights%20and%20the%20weight%20gradients%2C%20thus%20facilitating%0Atraining%20on%20edge%20devices.%20This%20is%20performed%20on%20the%20grounds%20of%20winning%0Aprobability%20for%20each%20unit%20in%20a%20block.%20During%20inference%2C%20the%20network%20retains%0Aonly%20the%20winning%20unit%20and%20zeroes-out%20all%20weights%20pertaining%20to%20non-winning%0Aunits%20for%20the%20task%20at%20hand.%20Thus%2C%20our%20approach%20is%20specifically%20tailored%20for%0Adeployment%20on%20edge%20devices%2C%20providing%20an%20efficient%20and%20scalable%20solution%20for%0Acontinual%20learning%20in%20resource-limited%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Deep%2520Learning%2520on%2520the%2520Edge%2520via%2520Stochastic%2520Local%2520Competition%250A%2520%2520among%2520Subnetworks%26entry.906535625%3DTheodoros%2520Christophides%2520and%2520Kyriakos%2520Tolias%2520and%2520Sotirios%2520Chatzis%26entry.1292438233%3D%2520%2520Continual%2520learning%2520on%2520edge%2520devices%2520poses%2520unique%2520challenges%2520due%2520to%2520stringent%250Aresource%2520constraints.%2520This%2520paper%2520introduces%2520a%2520novel%2520method%2520that%2520leverages%250Astochastic%2520competition%2520principles%2520to%2520promote%2520sparsity%252C%2520significantly%2520reducing%250Adeep%2520network%2520memory%2520footprint%2520and%2520computational%2520demand.%2520Specifically%252C%2520we%250Apropose%2520deep%2520networks%2520that%2520comprise%2520blocks%2520of%2520units%2520that%2520compete%2520locally%2520to%2520win%250Athe%2520representation%2520of%2520each%2520arising%2520new%2520task%253B%2520competition%2520takes%2520place%2520in%2520a%250Astochastic%2520manner.%2520This%2520type%2520of%2520network%2520organization%2520results%2520in%2520sparse%250Atask-specific%2520representations%2520from%2520each%2520network%2520layer%253B%2520the%2520sparsity%2520pattern%2520is%250Aobtained%2520during%2520training%2520and%2520is%2520different%2520among%2520tasks.%2520Crucially%252C%2520our%2520method%250Asparsifies%2520both%2520the%2520weights%2520and%2520the%2520weight%2520gradients%252C%2520thus%2520facilitating%250Atraining%2520on%2520edge%2520devices.%2520This%2520is%2520performed%2520on%2520the%2520grounds%2520of%2520winning%250Aprobability%2520for%2520each%2520unit%2520in%2520a%2520block.%2520During%2520inference%252C%2520the%2520network%2520retains%250Aonly%2520the%2520winning%2520unit%2520and%2520zeroes-out%2520all%2520weights%2520pertaining%2520to%2520non-winning%250Aunits%2520for%2520the%2520task%2520at%2520hand.%2520Thus%252C%2520our%2520approach%2520is%2520specifically%2520tailored%2520for%250Adeployment%2520on%2520edge%2520devices%252C%2520providing%2520an%2520efficient%2520and%2520scalable%2520solution%2520for%250Acontinual%2520learning%2520in%2520resource-limited%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Deep%20Learning%20on%20the%20Edge%20via%20Stochastic%20Local%20Competition%0A%20%20among%20Subnetworks&entry.906535625=Theodoros%20Christophides%20and%20Kyriakos%20Tolias%20and%20Sotirios%20Chatzis&entry.1292438233=%20%20Continual%20learning%20on%20edge%20devices%20poses%20unique%20challenges%20due%20to%20stringent%0Aresource%20constraints.%20This%20paper%20introduces%20a%20novel%20method%20that%20leverages%0Astochastic%20competition%20principles%20to%20promote%20sparsity%2C%20significantly%20reducing%0Adeep%20network%20memory%20footprint%20and%20computational%20demand.%20Specifically%2C%20we%0Apropose%20deep%20networks%20that%20comprise%20blocks%20of%20units%20that%20compete%20locally%20to%20win%0Athe%20representation%20of%20each%20arising%20new%20task%3B%20competition%20takes%20place%20in%20a%0Astochastic%20manner.%20This%20type%20of%20network%20organization%20results%20in%20sparse%0Atask-specific%20representations%20from%20each%20network%20layer%3B%20the%20sparsity%20pattern%20is%0Aobtained%20during%20training%20and%20is%20different%20among%20tasks.%20Crucially%2C%20our%20method%0Asparsifies%20both%20the%20weights%20and%20the%20weight%20gradients%2C%20thus%20facilitating%0Atraining%20on%20edge%20devices.%20This%20is%20performed%20on%20the%20grounds%20of%20winning%0Aprobability%20for%20each%20unit%20in%20a%20block.%20During%20inference%2C%20the%20network%20retains%0Aonly%20the%20winning%20unit%20and%20zeroes-out%20all%20weights%20pertaining%20to%20non-winning%0Aunits%20for%20the%20task%20at%20hand.%20Thus%2C%20our%20approach%20is%20specifically%20tailored%20for%0Adeployment%20on%20edge%20devices%2C%20providing%20an%20efficient%20and%20scalable%20solution%20for%0Acontinual%20learning%20in%20resource-limited%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10758v1&entry.124074799=Read"},
{"title": "Flexible Distribution Alignment: Towards Long-tailed Semi-supervised\n  Learning with Proper Calibration", "author": "Emanuel Sanchez Aimar and Nathaniel Helgesen and Yonghao Xu and Marco Kuhlmann and Michael Felsberg", "abstract": "  Long-tailed semi-supervised learning (LTSSL) represents a practical scenario\nfor semi-supervised applications, challenged by skewed labeled distributions\nthat bias classifiers. This problem is often aggravated by discrepancies\nbetween labeled and unlabeled class distributions, leading to biased\npseudo-labels, neglect of rare classes, and poorly calibrated probabilities. To\naddress these issues, we introduce Flexible Distribution Alignment (FlexDA), a\nnovel adaptive logit-adjusted loss framework designed to dynamically estimate\nand align predictions with the actual distribution of unlabeled data and\nachieve a balanced classifier by the end of training. FlexDA is further\nenhanced by a distillation-based consistency loss, promoting fair data usage\nacross classes and effectively leveraging underconfident samples. This method,\nencapsulated in ADELLO (Align and Distill Everything All at Once), proves\nrobust against label shift, significantly improves model calibration in LTSSL\ncontexts, and surpasses previous state-of-of-art approaches across multiple\nbenchmarks, including CIFAR100-LT, STL10-LT, and ImageNet127, addressing class\nimbalance challenges in semi-supervised learning. Our code is available at\nhttps://github.com/emasa/ADELLO-LTSSL.\n", "link": "http://arxiv.org/abs/2306.04621v3", "date": "2024-07-15", "relevancy": 2.2482, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5933}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5556}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flexible%20Distribution%20Alignment%3A%20Towards%20Long-tailed%20Semi-supervised%0A%20%20Learning%20with%20Proper%20Calibration&body=Title%3A%20Flexible%20Distribution%20Alignment%3A%20Towards%20Long-tailed%20Semi-supervised%0A%20%20Learning%20with%20Proper%20Calibration%0AAuthor%3A%20Emanuel%20Sanchez%20Aimar%20and%20Nathaniel%20Helgesen%20and%20Yonghao%20Xu%20and%20Marco%20Kuhlmann%20and%20Michael%20Felsberg%0AAbstract%3A%20%20%20Long-tailed%20semi-supervised%20learning%20%28LTSSL%29%20represents%20a%20practical%20scenario%0Afor%20semi-supervised%20applications%2C%20challenged%20by%20skewed%20labeled%20distributions%0Athat%20bias%20classifiers.%20This%20problem%20is%20often%20aggravated%20by%20discrepancies%0Abetween%20labeled%20and%20unlabeled%20class%20distributions%2C%20leading%20to%20biased%0Apseudo-labels%2C%20neglect%20of%20rare%20classes%2C%20and%20poorly%20calibrated%20probabilities.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20Flexible%20Distribution%20Alignment%20%28FlexDA%29%2C%20a%0Anovel%20adaptive%20logit-adjusted%20loss%20framework%20designed%20to%20dynamically%20estimate%0Aand%20align%20predictions%20with%20the%20actual%20distribution%20of%20unlabeled%20data%20and%0Aachieve%20a%20balanced%20classifier%20by%20the%20end%20of%20training.%20FlexDA%20is%20further%0Aenhanced%20by%20a%20distillation-based%20consistency%20loss%2C%20promoting%20fair%20data%20usage%0Aacross%20classes%20and%20effectively%20leveraging%20underconfident%20samples.%20This%20method%2C%0Aencapsulated%20in%20ADELLO%20%28Align%20and%20Distill%20Everything%20All%20at%20Once%29%2C%20proves%0Arobust%20against%20label%20shift%2C%20significantly%20improves%20model%20calibration%20in%20LTSSL%0Acontexts%2C%20and%20surpasses%20previous%20state-of-of-art%20approaches%20across%20multiple%0Abenchmarks%2C%20including%20CIFAR100-LT%2C%20STL10-LT%2C%20and%20ImageNet127%2C%20addressing%20class%0Aimbalance%20challenges%20in%20semi-supervised%20learning.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/emasa/ADELLO-LTSSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04621v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexible%2520Distribution%2520Alignment%253A%2520Towards%2520Long-tailed%2520Semi-supervised%250A%2520%2520Learning%2520with%2520Proper%2520Calibration%26entry.906535625%3DEmanuel%2520Sanchez%2520Aimar%2520and%2520Nathaniel%2520Helgesen%2520and%2520Yonghao%2520Xu%2520and%2520Marco%2520Kuhlmann%2520and%2520Michael%2520Felsberg%26entry.1292438233%3D%2520%2520Long-tailed%2520semi-supervised%2520learning%2520%2528LTSSL%2529%2520represents%2520a%2520practical%2520scenario%250Afor%2520semi-supervised%2520applications%252C%2520challenged%2520by%2520skewed%2520labeled%2520distributions%250Athat%2520bias%2520classifiers.%2520This%2520problem%2520is%2520often%2520aggravated%2520by%2520discrepancies%250Abetween%2520labeled%2520and%2520unlabeled%2520class%2520distributions%252C%2520leading%2520to%2520biased%250Apseudo-labels%252C%2520neglect%2520of%2520rare%2520classes%252C%2520and%2520poorly%2520calibrated%2520probabilities.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520Flexible%2520Distribution%2520Alignment%2520%2528FlexDA%2529%252C%2520a%250Anovel%2520adaptive%2520logit-adjusted%2520loss%2520framework%2520designed%2520to%2520dynamically%2520estimate%250Aand%2520align%2520predictions%2520with%2520the%2520actual%2520distribution%2520of%2520unlabeled%2520data%2520and%250Aachieve%2520a%2520balanced%2520classifier%2520by%2520the%2520end%2520of%2520training.%2520FlexDA%2520is%2520further%250Aenhanced%2520by%2520a%2520distillation-based%2520consistency%2520loss%252C%2520promoting%2520fair%2520data%2520usage%250Aacross%2520classes%2520and%2520effectively%2520leveraging%2520underconfident%2520samples.%2520This%2520method%252C%250Aencapsulated%2520in%2520ADELLO%2520%2528Align%2520and%2520Distill%2520Everything%2520All%2520at%2520Once%2529%252C%2520proves%250Arobust%2520against%2520label%2520shift%252C%2520significantly%2520improves%2520model%2520calibration%2520in%2520LTSSL%250Acontexts%252C%2520and%2520surpasses%2520previous%2520state-of-of-art%2520approaches%2520across%2520multiple%250Abenchmarks%252C%2520including%2520CIFAR100-LT%252C%2520STL10-LT%252C%2520and%2520ImageNet127%252C%2520addressing%2520class%250Aimbalance%2520challenges%2520in%2520semi-supervised%2520learning.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/emasa/ADELLO-LTSSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04621v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flexible%20Distribution%20Alignment%3A%20Towards%20Long-tailed%20Semi-supervised%0A%20%20Learning%20with%20Proper%20Calibration&entry.906535625=Emanuel%20Sanchez%20Aimar%20and%20Nathaniel%20Helgesen%20and%20Yonghao%20Xu%20and%20Marco%20Kuhlmann%20and%20Michael%20Felsberg&entry.1292438233=%20%20Long-tailed%20semi-supervised%20learning%20%28LTSSL%29%20represents%20a%20practical%20scenario%0Afor%20semi-supervised%20applications%2C%20challenged%20by%20skewed%20labeled%20distributions%0Athat%20bias%20classifiers.%20This%20problem%20is%20often%20aggravated%20by%20discrepancies%0Abetween%20labeled%20and%20unlabeled%20class%20distributions%2C%20leading%20to%20biased%0Apseudo-labels%2C%20neglect%20of%20rare%20classes%2C%20and%20poorly%20calibrated%20probabilities.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20Flexible%20Distribution%20Alignment%20%28FlexDA%29%2C%20a%0Anovel%20adaptive%20logit-adjusted%20loss%20framework%20designed%20to%20dynamically%20estimate%0Aand%20align%20predictions%20with%20the%20actual%20distribution%20of%20unlabeled%20data%20and%0Aachieve%20a%20balanced%20classifier%20by%20the%20end%20of%20training.%20FlexDA%20is%20further%0Aenhanced%20by%20a%20distillation-based%20consistency%20loss%2C%20promoting%20fair%20data%20usage%0Aacross%20classes%20and%20effectively%20leveraging%20underconfident%20samples.%20This%20method%2C%0Aencapsulated%20in%20ADELLO%20%28Align%20and%20Distill%20Everything%20All%20at%20Once%29%2C%20proves%0Arobust%20against%20label%20shift%2C%20significantly%20improves%20model%20calibration%20in%20LTSSL%0Acontexts%2C%20and%20surpasses%20previous%20state-of-of-art%20approaches%20across%20multiple%0Abenchmarks%2C%20including%20CIFAR100-LT%2C%20STL10-LT%2C%20and%20ImageNet127%2C%20addressing%20class%0Aimbalance%20challenges%20in%20semi-supervised%20learning.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/emasa/ADELLO-LTSSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04621v3&entry.124074799=Read"},
{"title": "A Survey on Uncertainty Quantification Methods for Deep Learning", "author": "Wenchong He and Zhe Jiang and Tingsong Xiao and Zelin Xu and Yukun Li", "abstract": "  Deep neural networks (DNNs) have achieved tremendous success in making\naccurate predictions for computer vision, natural language processing, as well\nas science and engineering domains. However, it is also well-recognized that\nDNNs sometimes make unexpected, incorrect, but overconfident predictions. This\ncan cause serious consequences in high-stake applications, such as autonomous\ndriving, medical diagnosis, and disaster response. Uncertainty quantification\n(UQ) aims to estimate the confidence of DNN predictions beyond prediction\naccuracy. In recent years, many UQ methods have been developed for DNNs. It is\nof great practical value to systematically categorize these UQ methods and\ncompare their advantages and disadvantages. However, existing surveys mostly\nfocus on categorizing UQ methodologies from a neural network architecture\nperspective or a Bayesian perspective and ignore the source of uncertainty that\neach methodology can incorporate, making it difficult to select an appropriate\nUQ method in practice. To fill the gap, this paper presents a systematic\ntaxonomy of UQ methods for DNNs based on the types of uncertainty sources (data\nuncertainty versus model uncertainty). We summarize the advantages and\ndisadvantages of methods in each category. We show how our taxonomy of UQ\nmethodologies can potentially help guide the choice of UQ method in different\nmachine learning problems (e.g., active learning, robustness, and reinforcement\nlearning). We also identify current research gaps and propose several future\nresearch directions.\n", "link": "http://arxiv.org/abs/2302.13425v5", "date": "2024-07-15", "relevancy": 2.2447, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6165}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5895}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Uncertainty%20Quantification%20Methods%20for%20Deep%20Learning&body=Title%3A%20A%20Survey%20on%20Uncertainty%20Quantification%20Methods%20for%20Deep%20Learning%0AAuthor%3A%20Wenchong%20He%20and%20Zhe%20Jiang%20and%20Tingsong%20Xiao%20and%20Zelin%20Xu%20and%20Yukun%20Li%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20achieved%20tremendous%20success%20in%20making%0Aaccurate%20predictions%20for%20computer%20vision%2C%20natural%20language%20processing%2C%20as%20well%0Aas%20science%20and%20engineering%20domains.%20However%2C%20it%20is%20also%20well-recognized%20that%0ADNNs%20sometimes%20make%20unexpected%2C%20incorrect%2C%20but%20overconfident%20predictions.%20This%0Acan%20cause%20serious%20consequences%20in%20high-stake%20applications%2C%20such%20as%20autonomous%0Adriving%2C%20medical%20diagnosis%2C%20and%20disaster%20response.%20Uncertainty%20quantification%0A%28UQ%29%20aims%20to%20estimate%20the%20confidence%20of%20DNN%20predictions%20beyond%20prediction%0Aaccuracy.%20In%20recent%20years%2C%20many%20UQ%20methods%20have%20been%20developed%20for%20DNNs.%20It%20is%0Aof%20great%20practical%20value%20to%20systematically%20categorize%20these%20UQ%20methods%20and%0Acompare%20their%20advantages%20and%20disadvantages.%20However%2C%20existing%20surveys%20mostly%0Afocus%20on%20categorizing%20UQ%20methodologies%20from%20a%20neural%20network%20architecture%0Aperspective%20or%20a%20Bayesian%20perspective%20and%20ignore%20the%20source%20of%20uncertainty%20that%0Aeach%20methodology%20can%20incorporate%2C%20making%20it%20difficult%20to%20select%20an%20appropriate%0AUQ%20method%20in%20practice.%20To%20fill%20the%20gap%2C%20this%20paper%20presents%20a%20systematic%0Ataxonomy%20of%20UQ%20methods%20for%20DNNs%20based%20on%20the%20types%20of%20uncertainty%20sources%20%28data%0Auncertainty%20versus%20model%20uncertainty%29.%20We%20summarize%20the%20advantages%20and%0Adisadvantages%20of%20methods%20in%20each%20category.%20We%20show%20how%20our%20taxonomy%20of%20UQ%0Amethodologies%20can%20potentially%20help%20guide%20the%20choice%20of%20UQ%20method%20in%20different%0Amachine%20learning%20problems%20%28e.g.%2C%20active%20learning%2C%20robustness%2C%20and%20reinforcement%0Alearning%29.%20We%20also%20identify%20current%20research%20gaps%20and%20propose%20several%20future%0Aresearch%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.13425v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Uncertainty%2520Quantification%2520Methods%2520for%2520Deep%2520Learning%26entry.906535625%3DWenchong%2520He%2520and%2520Zhe%2520Jiang%2520and%2520Tingsong%2520Xiao%2520and%2520Zelin%2520Xu%2520and%2520Yukun%2520Li%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520achieved%2520tremendous%2520success%2520in%2520making%250Aaccurate%2520predictions%2520for%2520computer%2520vision%252C%2520natural%2520language%2520processing%252C%2520as%2520well%250Aas%2520science%2520and%2520engineering%2520domains.%2520However%252C%2520it%2520is%2520also%2520well-recognized%2520that%250ADNNs%2520sometimes%2520make%2520unexpected%252C%2520incorrect%252C%2520but%2520overconfident%2520predictions.%2520This%250Acan%2520cause%2520serious%2520consequences%2520in%2520high-stake%2520applications%252C%2520such%2520as%2520autonomous%250Adriving%252C%2520medical%2520diagnosis%252C%2520and%2520disaster%2520response.%2520Uncertainty%2520quantification%250A%2528UQ%2529%2520aims%2520to%2520estimate%2520the%2520confidence%2520of%2520DNN%2520predictions%2520beyond%2520prediction%250Aaccuracy.%2520In%2520recent%2520years%252C%2520many%2520UQ%2520methods%2520have%2520been%2520developed%2520for%2520DNNs.%2520It%2520is%250Aof%2520great%2520practical%2520value%2520to%2520systematically%2520categorize%2520these%2520UQ%2520methods%2520and%250Acompare%2520their%2520advantages%2520and%2520disadvantages.%2520However%252C%2520existing%2520surveys%2520mostly%250Afocus%2520on%2520categorizing%2520UQ%2520methodologies%2520from%2520a%2520neural%2520network%2520architecture%250Aperspective%2520or%2520a%2520Bayesian%2520perspective%2520and%2520ignore%2520the%2520source%2520of%2520uncertainty%2520that%250Aeach%2520methodology%2520can%2520incorporate%252C%2520making%2520it%2520difficult%2520to%2520select%2520an%2520appropriate%250AUQ%2520method%2520in%2520practice.%2520To%2520fill%2520the%2520gap%252C%2520this%2520paper%2520presents%2520a%2520systematic%250Ataxonomy%2520of%2520UQ%2520methods%2520for%2520DNNs%2520based%2520on%2520the%2520types%2520of%2520uncertainty%2520sources%2520%2528data%250Auncertainty%2520versus%2520model%2520uncertainty%2529.%2520We%2520summarize%2520the%2520advantages%2520and%250Adisadvantages%2520of%2520methods%2520in%2520each%2520category.%2520We%2520show%2520how%2520our%2520taxonomy%2520of%2520UQ%250Amethodologies%2520can%2520potentially%2520help%2520guide%2520the%2520choice%2520of%2520UQ%2520method%2520in%2520different%250Amachine%2520learning%2520problems%2520%2528e.g.%252C%2520active%2520learning%252C%2520robustness%252C%2520and%2520reinforcement%250Alearning%2529.%2520We%2520also%2520identify%2520current%2520research%2520gaps%2520and%2520propose%2520several%2520future%250Aresearch%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.13425v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Uncertainty%20Quantification%20Methods%20for%20Deep%20Learning&entry.906535625=Wenchong%20He%20and%20Zhe%20Jiang%20and%20Tingsong%20Xiao%20and%20Zelin%20Xu%20and%20Yukun%20Li&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20achieved%20tremendous%20success%20in%20making%0Aaccurate%20predictions%20for%20computer%20vision%2C%20natural%20language%20processing%2C%20as%20well%0Aas%20science%20and%20engineering%20domains.%20However%2C%20it%20is%20also%20well-recognized%20that%0ADNNs%20sometimes%20make%20unexpected%2C%20incorrect%2C%20but%20overconfident%20predictions.%20This%0Acan%20cause%20serious%20consequences%20in%20high-stake%20applications%2C%20such%20as%20autonomous%0Adriving%2C%20medical%20diagnosis%2C%20and%20disaster%20response.%20Uncertainty%20quantification%0A%28UQ%29%20aims%20to%20estimate%20the%20confidence%20of%20DNN%20predictions%20beyond%20prediction%0Aaccuracy.%20In%20recent%20years%2C%20many%20UQ%20methods%20have%20been%20developed%20for%20DNNs.%20It%20is%0Aof%20great%20practical%20value%20to%20systematically%20categorize%20these%20UQ%20methods%20and%0Acompare%20their%20advantages%20and%20disadvantages.%20However%2C%20existing%20surveys%20mostly%0Afocus%20on%20categorizing%20UQ%20methodologies%20from%20a%20neural%20network%20architecture%0Aperspective%20or%20a%20Bayesian%20perspective%20and%20ignore%20the%20source%20of%20uncertainty%20that%0Aeach%20methodology%20can%20incorporate%2C%20making%20it%20difficult%20to%20select%20an%20appropriate%0AUQ%20method%20in%20practice.%20To%20fill%20the%20gap%2C%20this%20paper%20presents%20a%20systematic%0Ataxonomy%20of%20UQ%20methods%20for%20DNNs%20based%20on%20the%20types%20of%20uncertainty%20sources%20%28data%0Auncertainty%20versus%20model%20uncertainty%29.%20We%20summarize%20the%20advantages%20and%0Adisadvantages%20of%20methods%20in%20each%20category.%20We%20show%20how%20our%20taxonomy%20of%20UQ%0Amethodologies%20can%20potentially%20help%20guide%20the%20choice%20of%20UQ%20method%20in%20different%0Amachine%20learning%20problems%20%28e.g.%2C%20active%20learning%2C%20robustness%2C%20and%20reinforcement%0Alearning%29.%20We%20also%20identify%20current%20research%20gaps%20and%20propose%20several%20future%0Aresearch%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.13425v5&entry.124074799=Read"},
{"title": "EnTri: Ensemble Learning with Tri-level Representations for Explainable\n  Scene Recognition", "author": "Amirhossein Aminimehr and Amirali Molaei and Erik Cambria", "abstract": "  Scene recognition based on deep-learning has made significant progress, but\nthere are still limitations in its performance due to challenges posed by\ninter-class similarities and intra-class dissimilarities. Furthermore, prior\nresearch has primarily focused on improving classification accuracy, yet it has\ngiven less attention to achieving interpretable, precise scene classification.\nTherefore, we are motivated to propose EnTri, an ensemble scene recognition\nframework that employs ensemble learning using a hierarchy of visual features.\nEnTri represents features at three distinct levels of detail: pixel-level,\nsemantic segmentation-level, and object class and frequency level. By\nincorporating distinct feature encoding schemes of differing complexity and\nleveraging ensemble strategies, our approach aims to improve classification\naccuracy while enhancing transparency and interpretability via visual and\ntextual explanations. To achieve interpretability, we devised an extension\nalgorithm that generates both visual and textual explanations highlighting\nvarious properties of a given scene that contribute to the final prediction of\nits category. This includes information about objects, statistics, spatial\nlayout, and textural details. Through experiments on benchmark scene\nclassification datasets, EnTri has demonstrated superiority in terms of\nrecognition accuracy, achieving competitive performance compared to\nstate-of-the-art approaches, with an accuracy of 87.69%, 75.56%, and 99.17% on\nthe MIT67, SUN397, and UIUC8 datasets, respectively.\n", "link": "http://arxiv.org/abs/2307.12442v2", "date": "2024-07-15", "relevancy": 2.2421, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5802}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5482}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnTri%3A%20Ensemble%20Learning%20with%20Tri-level%20Representations%20for%20Explainable%0A%20%20Scene%20Recognition&body=Title%3A%20EnTri%3A%20Ensemble%20Learning%20with%20Tri-level%20Representations%20for%20Explainable%0A%20%20Scene%20Recognition%0AAuthor%3A%20Amirhossein%20Aminimehr%20and%20Amirali%20Molaei%20and%20Erik%20Cambria%0AAbstract%3A%20%20%20Scene%20recognition%20based%20on%20deep-learning%20has%20made%20significant%20progress%2C%20but%0Athere%20are%20still%20limitations%20in%20its%20performance%20due%20to%20challenges%20posed%20by%0Ainter-class%20similarities%20and%20intra-class%20dissimilarities.%20Furthermore%2C%20prior%0Aresearch%20has%20primarily%20focused%20on%20improving%20classification%20accuracy%2C%20yet%20it%20has%0Agiven%20less%20attention%20to%20achieving%20interpretable%2C%20precise%20scene%20classification.%0ATherefore%2C%20we%20are%20motivated%20to%20propose%20EnTri%2C%20an%20ensemble%20scene%20recognition%0Aframework%20that%20employs%20ensemble%20learning%20using%20a%20hierarchy%20of%20visual%20features.%0AEnTri%20represents%20features%20at%20three%20distinct%20levels%20of%20detail%3A%20pixel-level%2C%0Asemantic%20segmentation-level%2C%20and%20object%20class%20and%20frequency%20level.%20By%0Aincorporating%20distinct%20feature%20encoding%20schemes%20of%20differing%20complexity%20and%0Aleveraging%20ensemble%20strategies%2C%20our%20approach%20aims%20to%20improve%20classification%0Aaccuracy%20while%20enhancing%20transparency%20and%20interpretability%20via%20visual%20and%0Atextual%20explanations.%20To%20achieve%20interpretability%2C%20we%20devised%20an%20extension%0Aalgorithm%20that%20generates%20both%20visual%20and%20textual%20explanations%20highlighting%0Avarious%20properties%20of%20a%20given%20scene%20that%20contribute%20to%20the%20final%20prediction%20of%0Aits%20category.%20This%20includes%20information%20about%20objects%2C%20statistics%2C%20spatial%0Alayout%2C%20and%20textural%20details.%20Through%20experiments%20on%20benchmark%20scene%0Aclassification%20datasets%2C%20EnTri%20has%20demonstrated%20superiority%20in%20terms%20of%0Arecognition%20accuracy%2C%20achieving%20competitive%20performance%20compared%20to%0Astate-of-the-art%20approaches%2C%20with%20an%20accuracy%20of%2087.69%25%2C%2075.56%25%2C%20and%2099.17%25%20on%0Athe%20MIT67%2C%20SUN397%2C%20and%20UIUC8%20datasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.12442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnTri%253A%2520Ensemble%2520Learning%2520with%2520Tri-level%2520Representations%2520for%2520Explainable%250A%2520%2520Scene%2520Recognition%26entry.906535625%3DAmirhossein%2520Aminimehr%2520and%2520Amirali%2520Molaei%2520and%2520Erik%2520Cambria%26entry.1292438233%3D%2520%2520Scene%2520recognition%2520based%2520on%2520deep-learning%2520has%2520made%2520significant%2520progress%252C%2520but%250Athere%2520are%2520still%2520limitations%2520in%2520its%2520performance%2520due%2520to%2520challenges%2520posed%2520by%250Ainter-class%2520similarities%2520and%2520intra-class%2520dissimilarities.%2520Furthermore%252C%2520prior%250Aresearch%2520has%2520primarily%2520focused%2520on%2520improving%2520classification%2520accuracy%252C%2520yet%2520it%2520has%250Agiven%2520less%2520attention%2520to%2520achieving%2520interpretable%252C%2520precise%2520scene%2520classification.%250ATherefore%252C%2520we%2520are%2520motivated%2520to%2520propose%2520EnTri%252C%2520an%2520ensemble%2520scene%2520recognition%250Aframework%2520that%2520employs%2520ensemble%2520learning%2520using%2520a%2520hierarchy%2520of%2520visual%2520features.%250AEnTri%2520represents%2520features%2520at%2520three%2520distinct%2520levels%2520of%2520detail%253A%2520pixel-level%252C%250Asemantic%2520segmentation-level%252C%2520and%2520object%2520class%2520and%2520frequency%2520level.%2520By%250Aincorporating%2520distinct%2520feature%2520encoding%2520schemes%2520of%2520differing%2520complexity%2520and%250Aleveraging%2520ensemble%2520strategies%252C%2520our%2520approach%2520aims%2520to%2520improve%2520classification%250Aaccuracy%2520while%2520enhancing%2520transparency%2520and%2520interpretability%2520via%2520visual%2520and%250Atextual%2520explanations.%2520To%2520achieve%2520interpretability%252C%2520we%2520devised%2520an%2520extension%250Aalgorithm%2520that%2520generates%2520both%2520visual%2520and%2520textual%2520explanations%2520highlighting%250Avarious%2520properties%2520of%2520a%2520given%2520scene%2520that%2520contribute%2520to%2520the%2520final%2520prediction%2520of%250Aits%2520category.%2520This%2520includes%2520information%2520about%2520objects%252C%2520statistics%252C%2520spatial%250Alayout%252C%2520and%2520textural%2520details.%2520Through%2520experiments%2520on%2520benchmark%2520scene%250Aclassification%2520datasets%252C%2520EnTri%2520has%2520demonstrated%2520superiority%2520in%2520terms%2520of%250Arecognition%2520accuracy%252C%2520achieving%2520competitive%2520performance%2520compared%2520to%250Astate-of-the-art%2520approaches%252C%2520with%2520an%2520accuracy%2520of%252087.69%2525%252C%252075.56%2525%252C%2520and%252099.17%2525%2520on%250Athe%2520MIT67%252C%2520SUN397%252C%2520and%2520UIUC8%2520datasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.12442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnTri%3A%20Ensemble%20Learning%20with%20Tri-level%20Representations%20for%20Explainable%0A%20%20Scene%20Recognition&entry.906535625=Amirhossein%20Aminimehr%20and%20Amirali%20Molaei%20and%20Erik%20Cambria&entry.1292438233=%20%20Scene%20recognition%20based%20on%20deep-learning%20has%20made%20significant%20progress%2C%20but%0Athere%20are%20still%20limitations%20in%20its%20performance%20due%20to%20challenges%20posed%20by%0Ainter-class%20similarities%20and%20intra-class%20dissimilarities.%20Furthermore%2C%20prior%0Aresearch%20has%20primarily%20focused%20on%20improving%20classification%20accuracy%2C%20yet%20it%20has%0Agiven%20less%20attention%20to%20achieving%20interpretable%2C%20precise%20scene%20classification.%0ATherefore%2C%20we%20are%20motivated%20to%20propose%20EnTri%2C%20an%20ensemble%20scene%20recognition%0Aframework%20that%20employs%20ensemble%20learning%20using%20a%20hierarchy%20of%20visual%20features.%0AEnTri%20represents%20features%20at%20three%20distinct%20levels%20of%20detail%3A%20pixel-level%2C%0Asemantic%20segmentation-level%2C%20and%20object%20class%20and%20frequency%20level.%20By%0Aincorporating%20distinct%20feature%20encoding%20schemes%20of%20differing%20complexity%20and%0Aleveraging%20ensemble%20strategies%2C%20our%20approach%20aims%20to%20improve%20classification%0Aaccuracy%20while%20enhancing%20transparency%20and%20interpretability%20via%20visual%20and%0Atextual%20explanations.%20To%20achieve%20interpretability%2C%20we%20devised%20an%20extension%0Aalgorithm%20that%20generates%20both%20visual%20and%20textual%20explanations%20highlighting%0Avarious%20properties%20of%20a%20given%20scene%20that%20contribute%20to%20the%20final%20prediction%20of%0Aits%20category.%20This%20includes%20information%20about%20objects%2C%20statistics%2C%20spatial%0Alayout%2C%20and%20textural%20details.%20Through%20experiments%20on%20benchmark%20scene%0Aclassification%20datasets%2C%20EnTri%20has%20demonstrated%20superiority%20in%20terms%20of%0Arecognition%20accuracy%2C%20achieving%20competitive%20performance%20compared%20to%0Astate-of-the-art%20approaches%2C%20with%20an%20accuracy%20of%2087.69%25%2C%2075.56%25%2C%20and%2099.17%25%20on%0Athe%20MIT67%2C%20SUN397%2C%20and%20UIUC8%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12442v2&entry.124074799=Read"},
{"title": "Auto-Multilift: Distributed Learning and Control for Cooperative Load\n  Transportation With Quadrotors", "author": "Bingheng Wang and Rui Huang and Kuankuan Sima and Lin Zhao", "abstract": "  Designing motion control and planning algorithms for multilift systems\nremains challenging due to the complexities of dynamics, collision avoidance,\nactuator limits, and scalability. Existing methods that use optimization and\ndistributed techniques effectively address these constraints and scalability\nissues. However, they often require substantial manual tuning, leading to\nsuboptimal performance. This paper proposes Auto-Multilift, a novel framework\nthat automates the tuning of model predictive controllers (MPCs) for multilift\nsystems. We model the MPC cost functions with deep neural networks (DNNs),\nenabling fast online adaptation to various scenarios. We develop a distributed\npolicy gradient algorithm to train these DNNs efficiently in a closed-loop\nmanner. Central to our algorithm is distributed sensitivity propagation, which\nis built on fully exploiting the unique dynamic couplings within the multilift\nsystem. It parallelizes gradient computation across quadrotors and focuses on\nactual system state sensitivities relative to key MPC parameters. Extensive\nsimulations demonstrate favorable scalability to a large number of quadrotors.\nOur method outperforms a state-of-the-art open-loop MPC tuning approach by\neffectively learning adaptive MPCs from trajectory tracking errors. It also\nexcels in learning an adaptive reference for reconfiguring the system when\ntraversing multiple narrow slots.\n", "link": "http://arxiv.org/abs/2406.04858v2", "date": "2024-07-15", "relevancy": 2.2282, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5791}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5556}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors&body=Title%3A%20Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors%0AAuthor%3A%20Bingheng%20Wang%20and%20Rui%20Huang%20and%20Kuankuan%20Sima%20and%20Lin%20Zhao%0AAbstract%3A%20%20%20Designing%20motion%20control%20and%20planning%20algorithms%20for%20multilift%20systems%0Aremains%20challenging%20due%20to%20the%20complexities%20of%20dynamics%2C%20collision%20avoidance%2C%0Aactuator%20limits%2C%20and%20scalability.%20Existing%20methods%20that%20use%20optimization%20and%0Adistributed%20techniques%20effectively%20address%20these%20constraints%20and%20scalability%0Aissues.%20However%2C%20they%20often%20require%20substantial%20manual%20tuning%2C%20leading%20to%0Asuboptimal%20performance.%20This%20paper%20proposes%20Auto-Multilift%2C%20a%20novel%20framework%0Athat%20automates%20the%20tuning%20of%20model%20predictive%20controllers%20%28MPCs%29%20for%20multilift%0Asystems.%20We%20model%20the%20MPC%20cost%20functions%20with%20deep%20neural%20networks%20%28DNNs%29%2C%0Aenabling%20fast%20online%20adaptation%20to%20various%20scenarios.%20We%20develop%20a%20distributed%0Apolicy%20gradient%20algorithm%20to%20train%20these%20DNNs%20efficiently%20in%20a%20closed-loop%0Amanner.%20Central%20to%20our%20algorithm%20is%20distributed%20sensitivity%20propagation%2C%20which%0Ais%20built%20on%20fully%20exploiting%20the%20unique%20dynamic%20couplings%20within%20the%20multilift%0Asystem.%20It%20parallelizes%20gradient%20computation%20across%20quadrotors%20and%20focuses%20on%0Aactual%20system%20state%20sensitivities%20relative%20to%20key%20MPC%20parameters.%20Extensive%0Asimulations%20demonstrate%20favorable%20scalability%20to%20a%20large%20number%20of%20quadrotors.%0AOur%20method%20outperforms%20a%20state-of-the-art%20open-loop%20MPC%20tuning%20approach%20by%0Aeffectively%20learning%20adaptive%20MPCs%20from%20trajectory%20tracking%20errors.%20It%20also%0Aexcels%20in%20learning%20an%20adaptive%20reference%20for%20reconfiguring%20the%20system%20when%0Atraversing%20multiple%20narrow%20slots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Multilift%253A%2520Distributed%2520Learning%2520and%2520Control%2520for%2520Cooperative%2520Load%250A%2520%2520Transportation%2520With%2520Quadrotors%26entry.906535625%3DBingheng%2520Wang%2520and%2520Rui%2520Huang%2520and%2520Kuankuan%2520Sima%2520and%2520Lin%2520Zhao%26entry.1292438233%3D%2520%2520Designing%2520motion%2520control%2520and%2520planning%2520algorithms%2520for%2520multilift%2520systems%250Aremains%2520challenging%2520due%2520to%2520the%2520complexities%2520of%2520dynamics%252C%2520collision%2520avoidance%252C%250Aactuator%2520limits%252C%2520and%2520scalability.%2520Existing%2520methods%2520that%2520use%2520optimization%2520and%250Adistributed%2520techniques%2520effectively%2520address%2520these%2520constraints%2520and%2520scalability%250Aissues.%2520However%252C%2520they%2520often%2520require%2520substantial%2520manual%2520tuning%252C%2520leading%2520to%250Asuboptimal%2520performance.%2520This%2520paper%2520proposes%2520Auto-Multilift%252C%2520a%2520novel%2520framework%250Athat%2520automates%2520the%2520tuning%2520of%2520model%2520predictive%2520controllers%2520%2528MPCs%2529%2520for%2520multilift%250Asystems.%2520We%2520model%2520the%2520MPC%2520cost%2520functions%2520with%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%250Aenabling%2520fast%2520online%2520adaptation%2520to%2520various%2520scenarios.%2520We%2520develop%2520a%2520distributed%250Apolicy%2520gradient%2520algorithm%2520to%2520train%2520these%2520DNNs%2520efficiently%2520in%2520a%2520closed-loop%250Amanner.%2520Central%2520to%2520our%2520algorithm%2520is%2520distributed%2520sensitivity%2520propagation%252C%2520which%250Ais%2520built%2520on%2520fully%2520exploiting%2520the%2520unique%2520dynamic%2520couplings%2520within%2520the%2520multilift%250Asystem.%2520It%2520parallelizes%2520gradient%2520computation%2520across%2520quadrotors%2520and%2520focuses%2520on%250Aactual%2520system%2520state%2520sensitivities%2520relative%2520to%2520key%2520MPC%2520parameters.%2520Extensive%250Asimulations%2520demonstrate%2520favorable%2520scalability%2520to%2520a%2520large%2520number%2520of%2520quadrotors.%250AOur%2520method%2520outperforms%2520a%2520state-of-the-art%2520open-loop%2520MPC%2520tuning%2520approach%2520by%250Aeffectively%2520learning%2520adaptive%2520MPCs%2520from%2520trajectory%2520tracking%2520errors.%2520It%2520also%250Aexcels%2520in%2520learning%2520an%2520adaptive%2520reference%2520for%2520reconfiguring%2520the%2520system%2520when%250Atraversing%2520multiple%2520narrow%2520slots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors&entry.906535625=Bingheng%20Wang%20and%20Rui%20Huang%20and%20Kuankuan%20Sima%20and%20Lin%20Zhao&entry.1292438233=%20%20Designing%20motion%20control%20and%20planning%20algorithms%20for%20multilift%20systems%0Aremains%20challenging%20due%20to%20the%20complexities%20of%20dynamics%2C%20collision%20avoidance%2C%0Aactuator%20limits%2C%20and%20scalability.%20Existing%20methods%20that%20use%20optimization%20and%0Adistributed%20techniques%20effectively%20address%20these%20constraints%20and%20scalability%0Aissues.%20However%2C%20they%20often%20require%20substantial%20manual%20tuning%2C%20leading%20to%0Asuboptimal%20performance.%20This%20paper%20proposes%20Auto-Multilift%2C%20a%20novel%20framework%0Athat%20automates%20the%20tuning%20of%20model%20predictive%20controllers%20%28MPCs%29%20for%20multilift%0Asystems.%20We%20model%20the%20MPC%20cost%20functions%20with%20deep%20neural%20networks%20%28DNNs%29%2C%0Aenabling%20fast%20online%20adaptation%20to%20various%20scenarios.%20We%20develop%20a%20distributed%0Apolicy%20gradient%20algorithm%20to%20train%20these%20DNNs%20efficiently%20in%20a%20closed-loop%0Amanner.%20Central%20to%20our%20algorithm%20is%20distributed%20sensitivity%20propagation%2C%20which%0Ais%20built%20on%20fully%20exploiting%20the%20unique%20dynamic%20couplings%20within%20the%20multilift%0Asystem.%20It%20parallelizes%20gradient%20computation%20across%20quadrotors%20and%20focuses%20on%0Aactual%20system%20state%20sensitivities%20relative%20to%20key%20MPC%20parameters.%20Extensive%0Asimulations%20demonstrate%20favorable%20scalability%20to%20a%20large%20number%20of%20quadrotors.%0AOur%20method%20outperforms%20a%20state-of-the-art%20open-loop%20MPC%20tuning%20approach%20by%0Aeffectively%20learning%20adaptive%20MPCs%20from%20trajectory%20tracking%20errors.%20It%20also%0Aexcels%20in%20learning%20an%20adaptive%20reference%20for%20reconfiguring%20the%20system%20when%0Atraversing%20multiple%20narrow%20slots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04858v2&entry.124074799=Read"},
{"title": "Make-An-Agent: A Generalizable Policy Network Generator with\n  Behavior-Prompted Diffusion", "author": "Yongyuan Liang and Tingqiang Xu and Kaizhe Hu and Guangqi Jiang and Furong Huang and Huazhe Xu", "abstract": "  Can we generate a control policy for an agent using just one demonstration of\ndesired behaviors as a prompt, as effortlessly as creating an image from a\ntextual description? In this paper, we present Make-An-Agent, a novel policy\nparameter generator that leverages the power of conditional diffusion models\nfor behavior-to-policy generation. Guided by behavior embeddings that encode\ntrajectory information, our policy generator synthesizes latent parameter\nrepresentations, which can then be decoded into policy networks. Trained on\npolicy network checkpoints and their corresponding trajectories, our generation\nmodel demonstrates remarkable versatility and scalability on multiple tasks and\nhas a strong generalization ability on unseen tasks to output well-performed\npolicies with only few-shot demonstrations as inputs. We showcase its efficacy\nand efficiency on various domains and tasks, including varying objectives,\nbehaviors, and even across different robot manipulators. Beyond simulation, we\ndirectly deploy policies generated by Make-An-Agent onto real-world robots on\nlocomotion tasks.\n", "link": "http://arxiv.org/abs/2407.10973v1", "date": "2024-07-15", "relevancy": 2.2106, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5963}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5226}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Make-An-Agent%3A%20A%20Generalizable%20Policy%20Network%20Generator%20with%0A%20%20Behavior-Prompted%20Diffusion&body=Title%3A%20Make-An-Agent%3A%20A%20Generalizable%20Policy%20Network%20Generator%20with%0A%20%20Behavior-Prompted%20Diffusion%0AAuthor%3A%20Yongyuan%20Liang%20and%20Tingqiang%20Xu%20and%20Kaizhe%20Hu%20and%20Guangqi%20Jiang%20and%20Furong%20Huang%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Can%20we%20generate%20a%20control%20policy%20for%20an%20agent%20using%20just%20one%20demonstration%20of%0Adesired%20behaviors%20as%20a%20prompt%2C%20as%20effortlessly%20as%20creating%20an%20image%20from%20a%0Atextual%20description%3F%20In%20this%20paper%2C%20we%20present%20Make-An-Agent%2C%20a%20novel%20policy%0Aparameter%20generator%20that%20leverages%20the%20power%20of%20conditional%20diffusion%20models%0Afor%20behavior-to-policy%20generation.%20Guided%20by%20behavior%20embeddings%20that%20encode%0Atrajectory%20information%2C%20our%20policy%20generator%20synthesizes%20latent%20parameter%0Arepresentations%2C%20which%20can%20then%20be%20decoded%20into%20policy%20networks.%20Trained%20on%0Apolicy%20network%20checkpoints%20and%20their%20corresponding%20trajectories%2C%20our%20generation%0Amodel%20demonstrates%20remarkable%20versatility%20and%20scalability%20on%20multiple%20tasks%20and%0Ahas%20a%20strong%20generalization%20ability%20on%20unseen%20tasks%20to%20output%20well-performed%0Apolicies%20with%20only%20few-shot%20demonstrations%20as%20inputs.%20We%20showcase%20its%20efficacy%0Aand%20efficiency%20on%20various%20domains%20and%20tasks%2C%20including%20varying%20objectives%2C%0Abehaviors%2C%20and%20even%20across%20different%20robot%20manipulators.%20Beyond%20simulation%2C%20we%0Adirectly%20deploy%20policies%20generated%20by%20Make-An-Agent%20onto%20real-world%20robots%20on%0Alocomotion%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMake-An-Agent%253A%2520A%2520Generalizable%2520Policy%2520Network%2520Generator%2520with%250A%2520%2520Behavior-Prompted%2520Diffusion%26entry.906535625%3DYongyuan%2520Liang%2520and%2520Tingqiang%2520Xu%2520and%2520Kaizhe%2520Hu%2520and%2520Guangqi%2520Jiang%2520and%2520Furong%2520Huang%2520and%2520Huazhe%2520Xu%26entry.1292438233%3D%2520%2520Can%2520we%2520generate%2520a%2520control%2520policy%2520for%2520an%2520agent%2520using%2520just%2520one%2520demonstration%2520of%250Adesired%2520behaviors%2520as%2520a%2520prompt%252C%2520as%2520effortlessly%2520as%2520creating%2520an%2520image%2520from%2520a%250Atextual%2520description%253F%2520In%2520this%2520paper%252C%2520we%2520present%2520Make-An-Agent%252C%2520a%2520novel%2520policy%250Aparameter%2520generator%2520that%2520leverages%2520the%2520power%2520of%2520conditional%2520diffusion%2520models%250Afor%2520behavior-to-policy%2520generation.%2520Guided%2520by%2520behavior%2520embeddings%2520that%2520encode%250Atrajectory%2520information%252C%2520our%2520policy%2520generator%2520synthesizes%2520latent%2520parameter%250Arepresentations%252C%2520which%2520can%2520then%2520be%2520decoded%2520into%2520policy%2520networks.%2520Trained%2520on%250Apolicy%2520network%2520checkpoints%2520and%2520their%2520corresponding%2520trajectories%252C%2520our%2520generation%250Amodel%2520demonstrates%2520remarkable%2520versatility%2520and%2520scalability%2520on%2520multiple%2520tasks%2520and%250Ahas%2520a%2520strong%2520generalization%2520ability%2520on%2520unseen%2520tasks%2520to%2520output%2520well-performed%250Apolicies%2520with%2520only%2520few-shot%2520demonstrations%2520as%2520inputs.%2520We%2520showcase%2520its%2520efficacy%250Aand%2520efficiency%2520on%2520various%2520domains%2520and%2520tasks%252C%2520including%2520varying%2520objectives%252C%250Abehaviors%252C%2520and%2520even%2520across%2520different%2520robot%2520manipulators.%2520Beyond%2520simulation%252C%2520we%250Adirectly%2520deploy%2520policies%2520generated%2520by%2520Make-An-Agent%2520onto%2520real-world%2520robots%2520on%250Alocomotion%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Make-An-Agent%3A%20A%20Generalizable%20Policy%20Network%20Generator%20with%0A%20%20Behavior-Prompted%20Diffusion&entry.906535625=Yongyuan%20Liang%20and%20Tingqiang%20Xu%20and%20Kaizhe%20Hu%20and%20Guangqi%20Jiang%20and%20Furong%20Huang%20and%20Huazhe%20Xu&entry.1292438233=%20%20Can%20we%20generate%20a%20control%20policy%20for%20an%20agent%20using%20just%20one%20demonstration%20of%0Adesired%20behaviors%20as%20a%20prompt%2C%20as%20effortlessly%20as%20creating%20an%20image%20from%20a%0Atextual%20description%3F%20In%20this%20paper%2C%20we%20present%20Make-An-Agent%2C%20a%20novel%20policy%0Aparameter%20generator%20that%20leverages%20the%20power%20of%20conditional%20diffusion%20models%0Afor%20behavior-to-policy%20generation.%20Guided%20by%20behavior%20embeddings%20that%20encode%0Atrajectory%20information%2C%20our%20policy%20generator%20synthesizes%20latent%20parameter%0Arepresentations%2C%20which%20can%20then%20be%20decoded%20into%20policy%20networks.%20Trained%20on%0Apolicy%20network%20checkpoints%20and%20their%20corresponding%20trajectories%2C%20our%20generation%0Amodel%20demonstrates%20remarkable%20versatility%20and%20scalability%20on%20multiple%20tasks%20and%0Ahas%20a%20strong%20generalization%20ability%20on%20unseen%20tasks%20to%20output%20well-performed%0Apolicies%20with%20only%20few-shot%20demonstrations%20as%20inputs.%20We%20showcase%20its%20efficacy%0Aand%20efficiency%20on%20various%20domains%20and%20tasks%2C%20including%20varying%20objectives%2C%0Abehaviors%2C%20and%20even%20across%20different%20robot%20manipulators.%20Beyond%20simulation%2C%20we%0Adirectly%20deploy%20policies%20generated%20by%20Make-An-Agent%20onto%20real-world%20robots%20on%0Alocomotion%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10973v1&entry.124074799=Read"},
{"title": "Learning to Estimate the Pose of a Peer Robot in a Camera Image by\n  Predicting the States of its LEDs", "author": "Nicholas Carlotti and Mirko Nava and Alessandro Giusti", "abstract": "  We consider the problem of training a fully convolutional network to estimate\nthe relative 6D pose of a robot given a camera image, when the robot is\nequipped with independent controllable LEDs placed in different parts of its\nbody. The training data is composed by few (or zero) images labeled with a\nground truth relative pose and many images labeled only with the true state\n(\\textsc{on} or \\textsc{off}) of each of the peer LEDs. The former data is\nexpensive to acquire, requiring external infrastructure for tracking the two\nrobots; the latter is cheap as it can be acquired by two unsupervised robots\nmoving randomly and toggling their LEDs while sharing the true LED states via\nradio. Training with the latter dataset on estimating the LEDs' state of the\npeer robot (\\emph{pretext task}) promotes learning the relative localization\ntask (\\emph{end task}). Experiments on real-world data acquired by two\nautonomous wheeled robots show that a model trained only on the pretext task\nsuccessfully learns to localize a peer robot on the image plane; fine-tuning\nsuch model on the end task with few labeled images yields statistically\nsignificant improvements in 6D relative pose estimation with respect to\nbaselines that do not use pretext-task pre-training, and alternative\napproaches. Estimating the state of multiple independent LEDs promotes learning\nto estimate relative heading. The approach works even when a large fraction of\ntraining images do not include the peer robot and generalizes well to unseen\nenvironments.\n", "link": "http://arxiv.org/abs/2407.10661v1", "date": "2024-07-15", "relevancy": 2.1825, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5761}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Estimate%20the%20Pose%20of%20a%20Peer%20Robot%20in%20a%20Camera%20Image%20by%0A%20%20Predicting%20the%20States%20of%20its%20LEDs&body=Title%3A%20Learning%20to%20Estimate%20the%20Pose%20of%20a%20Peer%20Robot%20in%20a%20Camera%20Image%20by%0A%20%20Predicting%20the%20States%20of%20its%20LEDs%0AAuthor%3A%20Nicholas%20Carlotti%20and%20Mirko%20Nava%20and%20Alessandro%20Giusti%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20training%20a%20fully%20convolutional%20network%20to%20estimate%0Athe%20relative%206D%20pose%20of%20a%20robot%20given%20a%20camera%20image%2C%20when%20the%20robot%20is%0Aequipped%20with%20independent%20controllable%20LEDs%20placed%20in%20different%20parts%20of%20its%0Abody.%20The%20training%20data%20is%20composed%20by%20few%20%28or%20zero%29%20images%20labeled%20with%20a%0Aground%20truth%20relative%20pose%20and%20many%20images%20labeled%20only%20with%20the%20true%20state%0A%28%5Ctextsc%7Bon%7D%20or%20%5Ctextsc%7Boff%7D%29%20of%20each%20of%20the%20peer%20LEDs.%20The%20former%20data%20is%0Aexpensive%20to%20acquire%2C%20requiring%20external%20infrastructure%20for%20tracking%20the%20two%0Arobots%3B%20the%20latter%20is%20cheap%20as%20it%20can%20be%20acquired%20by%20two%20unsupervised%20robots%0Amoving%20randomly%20and%20toggling%20their%20LEDs%20while%20sharing%20the%20true%20LED%20states%20via%0Aradio.%20Training%20with%20the%20latter%20dataset%20on%20estimating%20the%20LEDs%27%20state%20of%20the%0Apeer%20robot%20%28%5Cemph%7Bpretext%20task%7D%29%20promotes%20learning%20the%20relative%20localization%0Atask%20%28%5Cemph%7Bend%20task%7D%29.%20Experiments%20on%20real-world%20data%20acquired%20by%20two%0Aautonomous%20wheeled%20robots%20show%20that%20a%20model%20trained%20only%20on%20the%20pretext%20task%0Asuccessfully%20learns%20to%20localize%20a%20peer%20robot%20on%20the%20image%20plane%3B%20fine-tuning%0Asuch%20model%20on%20the%20end%20task%20with%20few%20labeled%20images%20yields%20statistically%0Asignificant%20improvements%20in%206D%20relative%20pose%20estimation%20with%20respect%20to%0Abaselines%20that%20do%20not%20use%20pretext-task%20pre-training%2C%20and%20alternative%0Aapproaches.%20Estimating%20the%20state%20of%20multiple%20independent%20LEDs%20promotes%20learning%0Ato%20estimate%20relative%20heading.%20The%20approach%20works%20even%20when%20a%20large%20fraction%20of%0Atraining%20images%20do%20not%20include%20the%20peer%20robot%20and%20generalizes%20well%20to%20unseen%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Estimate%2520the%2520Pose%2520of%2520a%2520Peer%2520Robot%2520in%2520a%2520Camera%2520Image%2520by%250A%2520%2520Predicting%2520the%2520States%2520of%2520its%2520LEDs%26entry.906535625%3DNicholas%2520Carlotti%2520and%2520Mirko%2520Nava%2520and%2520Alessandro%2520Giusti%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520training%2520a%2520fully%2520convolutional%2520network%2520to%2520estimate%250Athe%2520relative%25206D%2520pose%2520of%2520a%2520robot%2520given%2520a%2520camera%2520image%252C%2520when%2520the%2520robot%2520is%250Aequipped%2520with%2520independent%2520controllable%2520LEDs%2520placed%2520in%2520different%2520parts%2520of%2520its%250Abody.%2520The%2520training%2520data%2520is%2520composed%2520by%2520few%2520%2528or%2520zero%2529%2520images%2520labeled%2520with%2520a%250Aground%2520truth%2520relative%2520pose%2520and%2520many%2520images%2520labeled%2520only%2520with%2520the%2520true%2520state%250A%2528%255Ctextsc%257Bon%257D%2520or%2520%255Ctextsc%257Boff%257D%2529%2520of%2520each%2520of%2520the%2520peer%2520LEDs.%2520The%2520former%2520data%2520is%250Aexpensive%2520to%2520acquire%252C%2520requiring%2520external%2520infrastructure%2520for%2520tracking%2520the%2520two%250Arobots%253B%2520the%2520latter%2520is%2520cheap%2520as%2520it%2520can%2520be%2520acquired%2520by%2520two%2520unsupervised%2520robots%250Amoving%2520randomly%2520and%2520toggling%2520their%2520LEDs%2520while%2520sharing%2520the%2520true%2520LED%2520states%2520via%250Aradio.%2520Training%2520with%2520the%2520latter%2520dataset%2520on%2520estimating%2520the%2520LEDs%2527%2520state%2520of%2520the%250Apeer%2520robot%2520%2528%255Cemph%257Bpretext%2520task%257D%2529%2520promotes%2520learning%2520the%2520relative%2520localization%250Atask%2520%2528%255Cemph%257Bend%2520task%257D%2529.%2520Experiments%2520on%2520real-world%2520data%2520acquired%2520by%2520two%250Aautonomous%2520wheeled%2520robots%2520show%2520that%2520a%2520model%2520trained%2520only%2520on%2520the%2520pretext%2520task%250Asuccessfully%2520learns%2520to%2520localize%2520a%2520peer%2520robot%2520on%2520the%2520image%2520plane%253B%2520fine-tuning%250Asuch%2520model%2520on%2520the%2520end%2520task%2520with%2520few%2520labeled%2520images%2520yields%2520statistically%250Asignificant%2520improvements%2520in%25206D%2520relative%2520pose%2520estimation%2520with%2520respect%2520to%250Abaselines%2520that%2520do%2520not%2520use%2520pretext-task%2520pre-training%252C%2520and%2520alternative%250Aapproaches.%2520Estimating%2520the%2520state%2520of%2520multiple%2520independent%2520LEDs%2520promotes%2520learning%250Ato%2520estimate%2520relative%2520heading.%2520The%2520approach%2520works%2520even%2520when%2520a%2520large%2520fraction%2520of%250Atraining%2520images%2520do%2520not%2520include%2520the%2520peer%2520robot%2520and%2520generalizes%2520well%2520to%2520unseen%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Estimate%20the%20Pose%20of%20a%20Peer%20Robot%20in%20a%20Camera%20Image%20by%0A%20%20Predicting%20the%20States%20of%20its%20LEDs&entry.906535625=Nicholas%20Carlotti%20and%20Mirko%20Nava%20and%20Alessandro%20Giusti&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20training%20a%20fully%20convolutional%20network%20to%20estimate%0Athe%20relative%206D%20pose%20of%20a%20robot%20given%20a%20camera%20image%2C%20when%20the%20robot%20is%0Aequipped%20with%20independent%20controllable%20LEDs%20placed%20in%20different%20parts%20of%20its%0Abody.%20The%20training%20data%20is%20composed%20by%20few%20%28or%20zero%29%20images%20labeled%20with%20a%0Aground%20truth%20relative%20pose%20and%20many%20images%20labeled%20only%20with%20the%20true%20state%0A%28%5Ctextsc%7Bon%7D%20or%20%5Ctextsc%7Boff%7D%29%20of%20each%20of%20the%20peer%20LEDs.%20The%20former%20data%20is%0Aexpensive%20to%20acquire%2C%20requiring%20external%20infrastructure%20for%20tracking%20the%20two%0Arobots%3B%20the%20latter%20is%20cheap%20as%20it%20can%20be%20acquired%20by%20two%20unsupervised%20robots%0Amoving%20randomly%20and%20toggling%20their%20LEDs%20while%20sharing%20the%20true%20LED%20states%20via%0Aradio.%20Training%20with%20the%20latter%20dataset%20on%20estimating%20the%20LEDs%27%20state%20of%20the%0Apeer%20robot%20%28%5Cemph%7Bpretext%20task%7D%29%20promotes%20learning%20the%20relative%20localization%0Atask%20%28%5Cemph%7Bend%20task%7D%29.%20Experiments%20on%20real-world%20data%20acquired%20by%20two%0Aautonomous%20wheeled%20robots%20show%20that%20a%20model%20trained%20only%20on%20the%20pretext%20task%0Asuccessfully%20learns%20to%20localize%20a%20peer%20robot%20on%20the%20image%20plane%3B%20fine-tuning%0Asuch%20model%20on%20the%20end%20task%20with%20few%20labeled%20images%20yields%20statistically%0Asignificant%20improvements%20in%206D%20relative%20pose%20estimation%20with%20respect%20to%0Abaselines%20that%20do%20not%20use%20pretext-task%20pre-training%2C%20and%20alternative%0Aapproaches.%20Estimating%20the%20state%20of%20multiple%20independent%20LEDs%20promotes%20learning%0Ato%20estimate%20relative%20heading.%20The%20approach%20works%20even%20when%20a%20large%20fraction%20of%0Atraining%20images%20do%20not%20include%20the%20peer%20robot%20and%20generalizes%20well%20to%20unseen%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10661v1&entry.124074799=Read"},
{"title": "Human-Centric Transformer for Domain Adaptive Action Recognition", "author": "Kun-Yu Lin and Jiaming Zhou and Wei-Shi Zheng", "abstract": "  We study the domain adaptation task for action recognition, namely domain\nadaptive action recognition, which aims to effectively transfer action\nrecognition power from a label-sufficient source domain to a label-free target\ndomain. Since actions are performed by humans, it is crucial to exploit human\ncues in videos when recognizing actions across domains. However, existing\nmethods are prone to losing human cues but prefer to exploit the correlation\nbetween non-human contexts and associated actions for recognition, and the\ncontexts of interest agnostic to actions would reduce recognition performance\nin the target domain. To overcome this problem, we focus on uncovering\nhuman-centric action cues for domain adaptive action recognition, and our\nconception is to investigate two aspects of human-centric action cues, namely\nhuman cues and human-context interaction cues. Accordingly, our proposed\nHuman-Centric Transformer (HCTransformer) develops a decoupled human-centric\nlearning paradigm to explicitly concentrate on human-centric action cues in\ndomain-variant video feature learning. Our HCTransformer first conducts\nhuman-aware temporal modeling by a human encoder, aiming to avoid a loss of\nhuman cues during domain-invariant video feature learning. Then, by a\nTransformer-like architecture, HCTransformer exploits domain-invariant and\naction-correlated contexts by a context encoder, and further models\ndomain-invariant interaction between humans and action-correlated contexts. We\nconduct extensive experiments on three benchmarks, namely UCF-HMDB,\nKinetics-NecDrone and EPIC-Kitchens-UDA, and the state-of-the-art performance\ndemonstrates the effectiveness of our proposed HCTransformer.\n", "link": "http://arxiv.org/abs/2407.10860v1", "date": "2024-07-15", "relevancy": 2.1566, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5561}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5362}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Centric%20Transformer%20for%20Domain%20Adaptive%20Action%20Recognition&body=Title%3A%20Human-Centric%20Transformer%20for%20Domain%20Adaptive%20Action%20Recognition%0AAuthor%3A%20Kun-Yu%20Lin%20and%20Jiaming%20Zhou%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20%20%20We%20study%20the%20domain%20adaptation%20task%20for%20action%20recognition%2C%20namely%20domain%0Aadaptive%20action%20recognition%2C%20which%20aims%20to%20effectively%20transfer%20action%0Arecognition%20power%20from%20a%20label-sufficient%20source%20domain%20to%20a%20label-free%20target%0Adomain.%20Since%20actions%20are%20performed%20by%20humans%2C%20it%20is%20crucial%20to%20exploit%20human%0Acues%20in%20videos%20when%20recognizing%20actions%20across%20domains.%20However%2C%20existing%0Amethods%20are%20prone%20to%20losing%20human%20cues%20but%20prefer%20to%20exploit%20the%20correlation%0Abetween%20non-human%20contexts%20and%20associated%20actions%20for%20recognition%2C%20and%20the%0Acontexts%20of%20interest%20agnostic%20to%20actions%20would%20reduce%20recognition%20performance%0Ain%20the%20target%20domain.%20To%20overcome%20this%20problem%2C%20we%20focus%20on%20uncovering%0Ahuman-centric%20action%20cues%20for%20domain%20adaptive%20action%20recognition%2C%20and%20our%0Aconception%20is%20to%20investigate%20two%20aspects%20of%20human-centric%20action%20cues%2C%20namely%0Ahuman%20cues%20and%20human-context%20interaction%20cues.%20Accordingly%2C%20our%20proposed%0AHuman-Centric%20Transformer%20%28HCTransformer%29%20develops%20a%20decoupled%20human-centric%0Alearning%20paradigm%20to%20explicitly%20concentrate%20on%20human-centric%20action%20cues%20in%0Adomain-variant%20video%20feature%20learning.%20Our%20HCTransformer%20first%20conducts%0Ahuman-aware%20temporal%20modeling%20by%20a%20human%20encoder%2C%20aiming%20to%20avoid%20a%20loss%20of%0Ahuman%20cues%20during%20domain-invariant%20video%20feature%20learning.%20Then%2C%20by%20a%0ATransformer-like%20architecture%2C%20HCTransformer%20exploits%20domain-invariant%20and%0Aaction-correlated%20contexts%20by%20a%20context%20encoder%2C%20and%20further%20models%0Adomain-invariant%20interaction%20between%20humans%20and%20action-correlated%20contexts.%20We%0Aconduct%20extensive%20experiments%20on%20three%20benchmarks%2C%20namely%20UCF-HMDB%2C%0AKinetics-NecDrone%20and%20EPIC-Kitchens-UDA%2C%20and%20the%20state-of-the-art%20performance%0Ademonstrates%20the%20effectiveness%20of%20our%20proposed%20HCTransformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Centric%2520Transformer%2520for%2520Domain%2520Adaptive%2520Action%2520Recognition%26entry.906535625%3DKun-Yu%2520Lin%2520and%2520Jiaming%2520Zhou%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520domain%2520adaptation%2520task%2520for%2520action%2520recognition%252C%2520namely%2520domain%250Aadaptive%2520action%2520recognition%252C%2520which%2520aims%2520to%2520effectively%2520transfer%2520action%250Arecognition%2520power%2520from%2520a%2520label-sufficient%2520source%2520domain%2520to%2520a%2520label-free%2520target%250Adomain.%2520Since%2520actions%2520are%2520performed%2520by%2520humans%252C%2520it%2520is%2520crucial%2520to%2520exploit%2520human%250Acues%2520in%2520videos%2520when%2520recognizing%2520actions%2520across%2520domains.%2520However%252C%2520existing%250Amethods%2520are%2520prone%2520to%2520losing%2520human%2520cues%2520but%2520prefer%2520to%2520exploit%2520the%2520correlation%250Abetween%2520non-human%2520contexts%2520and%2520associated%2520actions%2520for%2520recognition%252C%2520and%2520the%250Acontexts%2520of%2520interest%2520agnostic%2520to%2520actions%2520would%2520reduce%2520recognition%2520performance%250Ain%2520the%2520target%2520domain.%2520To%2520overcome%2520this%2520problem%252C%2520we%2520focus%2520on%2520uncovering%250Ahuman-centric%2520action%2520cues%2520for%2520domain%2520adaptive%2520action%2520recognition%252C%2520and%2520our%250Aconception%2520is%2520to%2520investigate%2520two%2520aspects%2520of%2520human-centric%2520action%2520cues%252C%2520namely%250Ahuman%2520cues%2520and%2520human-context%2520interaction%2520cues.%2520Accordingly%252C%2520our%2520proposed%250AHuman-Centric%2520Transformer%2520%2528HCTransformer%2529%2520develops%2520a%2520decoupled%2520human-centric%250Alearning%2520paradigm%2520to%2520explicitly%2520concentrate%2520on%2520human-centric%2520action%2520cues%2520in%250Adomain-variant%2520video%2520feature%2520learning.%2520Our%2520HCTransformer%2520first%2520conducts%250Ahuman-aware%2520temporal%2520modeling%2520by%2520a%2520human%2520encoder%252C%2520aiming%2520to%2520avoid%2520a%2520loss%2520of%250Ahuman%2520cues%2520during%2520domain-invariant%2520video%2520feature%2520learning.%2520Then%252C%2520by%2520a%250ATransformer-like%2520architecture%252C%2520HCTransformer%2520exploits%2520domain-invariant%2520and%250Aaction-correlated%2520contexts%2520by%2520a%2520context%2520encoder%252C%2520and%2520further%2520models%250Adomain-invariant%2520interaction%2520between%2520humans%2520and%2520action-correlated%2520contexts.%2520We%250Aconduct%2520extensive%2520experiments%2520on%2520three%2520benchmarks%252C%2520namely%2520UCF-HMDB%252C%250AKinetics-NecDrone%2520and%2520EPIC-Kitchens-UDA%252C%2520and%2520the%2520state-of-the-art%2520performance%250Ademonstrates%2520the%2520effectiveness%2520of%2520our%2520proposed%2520HCTransformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Centric%20Transformer%20for%20Domain%20Adaptive%20Action%20Recognition&entry.906535625=Kun-Yu%20Lin%20and%20Jiaming%20Zhou%20and%20Wei-Shi%20Zheng&entry.1292438233=%20%20We%20study%20the%20domain%20adaptation%20task%20for%20action%20recognition%2C%20namely%20domain%0Aadaptive%20action%20recognition%2C%20which%20aims%20to%20effectively%20transfer%20action%0Arecognition%20power%20from%20a%20label-sufficient%20source%20domain%20to%20a%20label-free%20target%0Adomain.%20Since%20actions%20are%20performed%20by%20humans%2C%20it%20is%20crucial%20to%20exploit%20human%0Acues%20in%20videos%20when%20recognizing%20actions%20across%20domains.%20However%2C%20existing%0Amethods%20are%20prone%20to%20losing%20human%20cues%20but%20prefer%20to%20exploit%20the%20correlation%0Abetween%20non-human%20contexts%20and%20associated%20actions%20for%20recognition%2C%20and%20the%0Acontexts%20of%20interest%20agnostic%20to%20actions%20would%20reduce%20recognition%20performance%0Ain%20the%20target%20domain.%20To%20overcome%20this%20problem%2C%20we%20focus%20on%20uncovering%0Ahuman-centric%20action%20cues%20for%20domain%20adaptive%20action%20recognition%2C%20and%20our%0Aconception%20is%20to%20investigate%20two%20aspects%20of%20human-centric%20action%20cues%2C%20namely%0Ahuman%20cues%20and%20human-context%20interaction%20cues.%20Accordingly%2C%20our%20proposed%0AHuman-Centric%20Transformer%20%28HCTransformer%29%20develops%20a%20decoupled%20human-centric%0Alearning%20paradigm%20to%20explicitly%20concentrate%20on%20human-centric%20action%20cues%20in%0Adomain-variant%20video%20feature%20learning.%20Our%20HCTransformer%20first%20conducts%0Ahuman-aware%20temporal%20modeling%20by%20a%20human%20encoder%2C%20aiming%20to%20avoid%20a%20loss%20of%0Ahuman%20cues%20during%20domain-invariant%20video%20feature%20learning.%20Then%2C%20by%20a%0ATransformer-like%20architecture%2C%20HCTransformer%20exploits%20domain-invariant%20and%0Aaction-correlated%20contexts%20by%20a%20context%20encoder%2C%20and%20further%20models%0Adomain-invariant%20interaction%20between%20humans%20and%20action-correlated%20contexts.%20We%0Aconduct%20extensive%20experiments%20on%20three%20benchmarks%2C%20namely%20UCF-HMDB%2C%0AKinetics-NecDrone%20and%20EPIC-Kitchens-UDA%2C%20and%20the%20state-of-the-art%20performance%0Ademonstrates%20the%20effectiveness%20of%20our%20proposed%20HCTransformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10860v1&entry.124074799=Read"},
{"title": "Mammographic Breast Positioning Assessment via Deep Learning", "author": "Toygar Tanyel and Nurper Denizoglu and Mustafa Ege Seker and Deniz Alis and Esma Cerekci and Ercan Karaarslan and Erkin Aribal and Ilkay Oksuz", "abstract": "  Breast cancer remains a leading cause of cancer-related deaths among women\nworldwide, with mammography screening as the most effective method for the\nearly detection. Ensuring proper positioning in mammography is critical, as\npoor positioning can lead to diagnostic errors, increased patient stress, and\nhigher costs due to recalls. Despite advancements in deep learning (DL) for\nbreast cancer diagnostics, limited focus has been given to evaluating\nmammography positioning. This paper introduces a novel DL methodology to\nquantitatively assess mammogram positioning quality, specifically in\nmediolateral oblique (MLO) views using attention and coordinate convolution\nmodules. Our method identifies key anatomical landmarks, such as the nipple and\npectoralis muscle, and automatically draws a posterior nipple line (PNL),\noffering robust and inherently explainable alternative to well-known\nclassification and regression-based approaches. We compare the performance of\nproposed methodology with various regression and classification-based models.\nThe CoordAtt UNet model achieved the highest accuracy of 88.63% $\\pm$ 2.84 and\nspecificity of 90.25% $\\pm$ 4.04, along with a noteworthy sensitivity of 86.04%\n$\\pm$ 3.41. In landmark detection, the same model also recorded the lowest mean\nerrors in key anatomical points and the smallest angular error of 2.42 degrees.\nOur results indicate that models incorporating attention mechanisms and\nCoordConv module increase the accuracy in classifying breast positioning\nquality and detecting anatomical landmarks. Furthermore, we make the labels and\nsource codes available to the community to initiate an open research area for\nmammography, accessible at https://github.com/tanyelai/deep-breast-positioning.\n", "link": "http://arxiv.org/abs/2407.10796v1", "date": "2024-07-15", "relevancy": 2.1521, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5623}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mammographic%20Breast%20Positioning%20Assessment%20via%20Deep%20Learning&body=Title%3A%20Mammographic%20Breast%20Positioning%20Assessment%20via%20Deep%20Learning%0AAuthor%3A%20Toygar%20Tanyel%20and%20Nurper%20Denizoglu%20and%20Mustafa%20Ege%20Seker%20and%20Deniz%20Alis%20and%20Esma%20Cerekci%20and%20Ercan%20Karaarslan%20and%20Erkin%20Aribal%20and%20Ilkay%20Oksuz%0AAbstract%3A%20%20%20Breast%20cancer%20remains%20a%20leading%20cause%20of%20cancer-related%20deaths%20among%20women%0Aworldwide%2C%20with%20mammography%20screening%20as%20the%20most%20effective%20method%20for%20the%0Aearly%20detection.%20Ensuring%20proper%20positioning%20in%20mammography%20is%20critical%2C%20as%0Apoor%20positioning%20can%20lead%20to%20diagnostic%20errors%2C%20increased%20patient%20stress%2C%20and%0Ahigher%20costs%20due%20to%20recalls.%20Despite%20advancements%20in%20deep%20learning%20%28DL%29%20for%0Abreast%20cancer%20diagnostics%2C%20limited%20focus%20has%20been%20given%20to%20evaluating%0Amammography%20positioning.%20This%20paper%20introduces%20a%20novel%20DL%20methodology%20to%0Aquantitatively%20assess%20mammogram%20positioning%20quality%2C%20specifically%20in%0Amediolateral%20oblique%20%28MLO%29%20views%20using%20attention%20and%20coordinate%20convolution%0Amodules.%20Our%20method%20identifies%20key%20anatomical%20landmarks%2C%20such%20as%20the%20nipple%20and%0Apectoralis%20muscle%2C%20and%20automatically%20draws%20a%20posterior%20nipple%20line%20%28PNL%29%2C%0Aoffering%20robust%20and%20inherently%20explainable%20alternative%20to%20well-known%0Aclassification%20and%20regression-based%20approaches.%20We%20compare%20the%20performance%20of%0Aproposed%20methodology%20with%20various%20regression%20and%20classification-based%20models.%0AThe%20CoordAtt%20UNet%20model%20achieved%20the%20highest%20accuracy%20of%2088.63%25%20%24%5Cpm%24%202.84%20and%0Aspecificity%20of%2090.25%25%20%24%5Cpm%24%204.04%2C%20along%20with%20a%20noteworthy%20sensitivity%20of%2086.04%25%0A%24%5Cpm%24%203.41.%20In%20landmark%20detection%2C%20the%20same%20model%20also%20recorded%20the%20lowest%20mean%0Aerrors%20in%20key%20anatomical%20points%20and%20the%20smallest%20angular%20error%20of%202.42%20degrees.%0AOur%20results%20indicate%20that%20models%20incorporating%20attention%20mechanisms%20and%0ACoordConv%20module%20increase%20the%20accuracy%20in%20classifying%20breast%20positioning%0Aquality%20and%20detecting%20anatomical%20landmarks.%20Furthermore%2C%20we%20make%20the%20labels%20and%0Asource%20codes%20available%20to%20the%20community%20to%20initiate%20an%20open%20research%20area%20for%0Amammography%2C%20accessible%20at%20https%3A//github.com/tanyelai/deep-breast-positioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMammographic%2520Breast%2520Positioning%2520Assessment%2520via%2520Deep%2520Learning%26entry.906535625%3DToygar%2520Tanyel%2520and%2520Nurper%2520Denizoglu%2520and%2520Mustafa%2520Ege%2520Seker%2520and%2520Deniz%2520Alis%2520and%2520Esma%2520Cerekci%2520and%2520Ercan%2520Karaarslan%2520and%2520Erkin%2520Aribal%2520and%2520Ilkay%2520Oksuz%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520remains%2520a%2520leading%2520cause%2520of%2520cancer-related%2520deaths%2520among%2520women%250Aworldwide%252C%2520with%2520mammography%2520screening%2520as%2520the%2520most%2520effective%2520method%2520for%2520the%250Aearly%2520detection.%2520Ensuring%2520proper%2520positioning%2520in%2520mammography%2520is%2520critical%252C%2520as%250Apoor%2520positioning%2520can%2520lead%2520to%2520diagnostic%2520errors%252C%2520increased%2520patient%2520stress%252C%2520and%250Ahigher%2520costs%2520due%2520to%2520recalls.%2520Despite%2520advancements%2520in%2520deep%2520learning%2520%2528DL%2529%2520for%250Abreast%2520cancer%2520diagnostics%252C%2520limited%2520focus%2520has%2520been%2520given%2520to%2520evaluating%250Amammography%2520positioning.%2520This%2520paper%2520introduces%2520a%2520novel%2520DL%2520methodology%2520to%250Aquantitatively%2520assess%2520mammogram%2520positioning%2520quality%252C%2520specifically%2520in%250Amediolateral%2520oblique%2520%2528MLO%2529%2520views%2520using%2520attention%2520and%2520coordinate%2520convolution%250Amodules.%2520Our%2520method%2520identifies%2520key%2520anatomical%2520landmarks%252C%2520such%2520as%2520the%2520nipple%2520and%250Apectoralis%2520muscle%252C%2520and%2520automatically%2520draws%2520a%2520posterior%2520nipple%2520line%2520%2528PNL%2529%252C%250Aoffering%2520robust%2520and%2520inherently%2520explainable%2520alternative%2520to%2520well-known%250Aclassification%2520and%2520regression-based%2520approaches.%2520We%2520compare%2520the%2520performance%2520of%250Aproposed%2520methodology%2520with%2520various%2520regression%2520and%2520classification-based%2520models.%250AThe%2520CoordAtt%2520UNet%2520model%2520achieved%2520the%2520highest%2520accuracy%2520of%252088.63%2525%2520%2524%255Cpm%2524%25202.84%2520and%250Aspecificity%2520of%252090.25%2525%2520%2524%255Cpm%2524%25204.04%252C%2520along%2520with%2520a%2520noteworthy%2520sensitivity%2520of%252086.04%2525%250A%2524%255Cpm%2524%25203.41.%2520In%2520landmark%2520detection%252C%2520the%2520same%2520model%2520also%2520recorded%2520the%2520lowest%2520mean%250Aerrors%2520in%2520key%2520anatomical%2520points%2520and%2520the%2520smallest%2520angular%2520error%2520of%25202.42%2520degrees.%250AOur%2520results%2520indicate%2520that%2520models%2520incorporating%2520attention%2520mechanisms%2520and%250ACoordConv%2520module%2520increase%2520the%2520accuracy%2520in%2520classifying%2520breast%2520positioning%250Aquality%2520and%2520detecting%2520anatomical%2520landmarks.%2520Furthermore%252C%2520we%2520make%2520the%2520labels%2520and%250Asource%2520codes%2520available%2520to%2520the%2520community%2520to%2520initiate%2520an%2520open%2520research%2520area%2520for%250Amammography%252C%2520accessible%2520at%2520https%253A//github.com/tanyelai/deep-breast-positioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mammographic%20Breast%20Positioning%20Assessment%20via%20Deep%20Learning&entry.906535625=Toygar%20Tanyel%20and%20Nurper%20Denizoglu%20and%20Mustafa%20Ege%20Seker%20and%20Deniz%20Alis%20and%20Esma%20Cerekci%20and%20Ercan%20Karaarslan%20and%20Erkin%20Aribal%20and%20Ilkay%20Oksuz&entry.1292438233=%20%20Breast%20cancer%20remains%20a%20leading%20cause%20of%20cancer-related%20deaths%20among%20women%0Aworldwide%2C%20with%20mammography%20screening%20as%20the%20most%20effective%20method%20for%20the%0Aearly%20detection.%20Ensuring%20proper%20positioning%20in%20mammography%20is%20critical%2C%20as%0Apoor%20positioning%20can%20lead%20to%20diagnostic%20errors%2C%20increased%20patient%20stress%2C%20and%0Ahigher%20costs%20due%20to%20recalls.%20Despite%20advancements%20in%20deep%20learning%20%28DL%29%20for%0Abreast%20cancer%20diagnostics%2C%20limited%20focus%20has%20been%20given%20to%20evaluating%0Amammography%20positioning.%20This%20paper%20introduces%20a%20novel%20DL%20methodology%20to%0Aquantitatively%20assess%20mammogram%20positioning%20quality%2C%20specifically%20in%0Amediolateral%20oblique%20%28MLO%29%20views%20using%20attention%20and%20coordinate%20convolution%0Amodules.%20Our%20method%20identifies%20key%20anatomical%20landmarks%2C%20such%20as%20the%20nipple%20and%0Apectoralis%20muscle%2C%20and%20automatically%20draws%20a%20posterior%20nipple%20line%20%28PNL%29%2C%0Aoffering%20robust%20and%20inherently%20explainable%20alternative%20to%20well-known%0Aclassification%20and%20regression-based%20approaches.%20We%20compare%20the%20performance%20of%0Aproposed%20methodology%20with%20various%20regression%20and%20classification-based%20models.%0AThe%20CoordAtt%20UNet%20model%20achieved%20the%20highest%20accuracy%20of%2088.63%25%20%24%5Cpm%24%202.84%20and%0Aspecificity%20of%2090.25%25%20%24%5Cpm%24%204.04%2C%20along%20with%20a%20noteworthy%20sensitivity%20of%2086.04%25%0A%24%5Cpm%24%203.41.%20In%20landmark%20detection%2C%20the%20same%20model%20also%20recorded%20the%20lowest%20mean%0Aerrors%20in%20key%20anatomical%20points%20and%20the%20smallest%20angular%20error%20of%202.42%20degrees.%0AOur%20results%20indicate%20that%20models%20incorporating%20attention%20mechanisms%20and%0ACoordConv%20module%20increase%20the%20accuracy%20in%20classifying%20breast%20positioning%0Aquality%20and%20detecting%20anatomical%20landmarks.%20Furthermore%2C%20we%20make%20the%20labels%20and%0Asource%20codes%20available%20to%20the%20community%20to%20initiate%20an%20open%20research%20area%20for%0Amammography%2C%20accessible%20at%20https%3A//github.com/tanyelai/deep-breast-positioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10796v1&entry.124074799=Read"},
{"title": "An Autonomous Drone Swarm for Detecting and Tracking Anomalies among\n  Dense Vegetation", "author": "Rakesh John Amala Arokia Nathan and Sigrid Strand and Daniel Mehrwald and Dmitriy Shutin and Oliver Bimber", "abstract": "  Swarms of drones offer an increased sensing aperture, and having them mimic\nbehaviors of natural swarms enhances sampling by adapting the aperture to local\nconditions. We demonstrate that such an approach makes detecting and tracking\nheavily occluded targets practically feasible. While object classification\napplied to conventional aerial images generalizes poorly the randomness of\nocclusion and is therefore inefficient even under lightly occluded conditions,\nanomaly detection applied to synthetic aperture integral images is robust for\ndense vegetation, such as forests, and is independent of pre-trained classes.\nOur autonomous swarm searches the environment for occurrences of the unknown or\nunexpected, tracking them while continuously adapting its sampling pattern to\noptimize for local viewing conditions. In our real-life field experiments with\na swarm of six drones, we achieved an average positional accuracy of 0.39 m\nwith an average precision of 93.2% and an average recall of 95.9%. Here,\nadapted particle swarm optimization considers detection confidences and\npredicted target appearance. We show that sensor noise can effectively be\nincluded in the synthetic aperture image integration process, removing the need\nfor a computationally costly optimization of high-dimensional parameter spaces.\nFinally, we present a complete hard- and software framework that supports\nlow-latency transmission (approx. 80 ms round-trip time) and fast processing\n(approx. 600 ms per formation step) of extensive (70-120 Mbit/s) video and\ntelemetry data, and swarm control for swarms of up to ten drones.\n", "link": "http://arxiv.org/abs/2407.10754v1", "date": "2024-07-15", "relevancy": 2.1513, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5298}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Autonomous%20Drone%20Swarm%20for%20Detecting%20and%20Tracking%20Anomalies%20among%0A%20%20Dense%20Vegetation&body=Title%3A%20An%20Autonomous%20Drone%20Swarm%20for%20Detecting%20and%20Tracking%20Anomalies%20among%0A%20%20Dense%20Vegetation%0AAuthor%3A%20Rakesh%20John%20Amala%20Arokia%20Nathan%20and%20Sigrid%20Strand%20and%20Daniel%20Mehrwald%20and%20Dmitriy%20Shutin%20and%20Oliver%20Bimber%0AAbstract%3A%20%20%20Swarms%20of%20drones%20offer%20an%20increased%20sensing%20aperture%2C%20and%20having%20them%20mimic%0Abehaviors%20of%20natural%20swarms%20enhances%20sampling%20by%20adapting%20the%20aperture%20to%20local%0Aconditions.%20We%20demonstrate%20that%20such%20an%20approach%20makes%20detecting%20and%20tracking%0Aheavily%20occluded%20targets%20practically%20feasible.%20While%20object%20classification%0Aapplied%20to%20conventional%20aerial%20images%20generalizes%20poorly%20the%20randomness%20of%0Aocclusion%20and%20is%20therefore%20inefficient%20even%20under%20lightly%20occluded%20conditions%2C%0Aanomaly%20detection%20applied%20to%20synthetic%20aperture%20integral%20images%20is%20robust%20for%0Adense%20vegetation%2C%20such%20as%20forests%2C%20and%20is%20independent%20of%20pre-trained%20classes.%0AOur%20autonomous%20swarm%20searches%20the%20environment%20for%20occurrences%20of%20the%20unknown%20or%0Aunexpected%2C%20tracking%20them%20while%20continuously%20adapting%20its%20sampling%20pattern%20to%0Aoptimize%20for%20local%20viewing%20conditions.%20In%20our%20real-life%20field%20experiments%20with%0Aa%20swarm%20of%20six%20drones%2C%20we%20achieved%20an%20average%20positional%20accuracy%20of%200.39%20m%0Awith%20an%20average%20precision%20of%2093.2%25%20and%20an%20average%20recall%20of%2095.9%25.%20Here%2C%0Aadapted%20particle%20swarm%20optimization%20considers%20detection%20confidences%20and%0Apredicted%20target%20appearance.%20We%20show%20that%20sensor%20noise%20can%20effectively%20be%0Aincluded%20in%20the%20synthetic%20aperture%20image%20integration%20process%2C%20removing%20the%20need%0Afor%20a%20computationally%20costly%20optimization%20of%20high-dimensional%20parameter%20spaces.%0AFinally%2C%20we%20present%20a%20complete%20hard-%20and%20software%20framework%20that%20supports%0Alow-latency%20transmission%20%28approx.%2080%20ms%20round-trip%20time%29%20and%20fast%20processing%0A%28approx.%20600%20ms%20per%20formation%20step%29%20of%20extensive%20%2870-120%20Mbit/s%29%20video%20and%0Atelemetry%20data%2C%20and%20swarm%20control%20for%20swarms%20of%20up%20to%20ten%20drones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Autonomous%2520Drone%2520Swarm%2520for%2520Detecting%2520and%2520Tracking%2520Anomalies%2520among%250A%2520%2520Dense%2520Vegetation%26entry.906535625%3DRakesh%2520John%2520Amala%2520Arokia%2520Nathan%2520and%2520Sigrid%2520Strand%2520and%2520Daniel%2520Mehrwald%2520and%2520Dmitriy%2520Shutin%2520and%2520Oliver%2520Bimber%26entry.1292438233%3D%2520%2520Swarms%2520of%2520drones%2520offer%2520an%2520increased%2520sensing%2520aperture%252C%2520and%2520having%2520them%2520mimic%250Abehaviors%2520of%2520natural%2520swarms%2520enhances%2520sampling%2520by%2520adapting%2520the%2520aperture%2520to%2520local%250Aconditions.%2520We%2520demonstrate%2520that%2520such%2520an%2520approach%2520makes%2520detecting%2520and%2520tracking%250Aheavily%2520occluded%2520targets%2520practically%2520feasible.%2520While%2520object%2520classification%250Aapplied%2520to%2520conventional%2520aerial%2520images%2520generalizes%2520poorly%2520the%2520randomness%2520of%250Aocclusion%2520and%2520is%2520therefore%2520inefficient%2520even%2520under%2520lightly%2520occluded%2520conditions%252C%250Aanomaly%2520detection%2520applied%2520to%2520synthetic%2520aperture%2520integral%2520images%2520is%2520robust%2520for%250Adense%2520vegetation%252C%2520such%2520as%2520forests%252C%2520and%2520is%2520independent%2520of%2520pre-trained%2520classes.%250AOur%2520autonomous%2520swarm%2520searches%2520the%2520environment%2520for%2520occurrences%2520of%2520the%2520unknown%2520or%250Aunexpected%252C%2520tracking%2520them%2520while%2520continuously%2520adapting%2520its%2520sampling%2520pattern%2520to%250Aoptimize%2520for%2520local%2520viewing%2520conditions.%2520In%2520our%2520real-life%2520field%2520experiments%2520with%250Aa%2520swarm%2520of%2520six%2520drones%252C%2520we%2520achieved%2520an%2520average%2520positional%2520accuracy%2520of%25200.39%2520m%250Awith%2520an%2520average%2520precision%2520of%252093.2%2525%2520and%2520an%2520average%2520recall%2520of%252095.9%2525.%2520Here%252C%250Aadapted%2520particle%2520swarm%2520optimization%2520considers%2520detection%2520confidences%2520and%250Apredicted%2520target%2520appearance.%2520We%2520show%2520that%2520sensor%2520noise%2520can%2520effectively%2520be%250Aincluded%2520in%2520the%2520synthetic%2520aperture%2520image%2520integration%2520process%252C%2520removing%2520the%2520need%250Afor%2520a%2520computationally%2520costly%2520optimization%2520of%2520high-dimensional%2520parameter%2520spaces.%250AFinally%252C%2520we%2520present%2520a%2520complete%2520hard-%2520and%2520software%2520framework%2520that%2520supports%250Alow-latency%2520transmission%2520%2528approx.%252080%2520ms%2520round-trip%2520time%2529%2520and%2520fast%2520processing%250A%2528approx.%2520600%2520ms%2520per%2520formation%2520step%2529%2520of%2520extensive%2520%252870-120%2520Mbit/s%2529%2520video%2520and%250Atelemetry%2520data%252C%2520and%2520swarm%2520control%2520for%2520swarms%2520of%2520up%2520to%2520ten%2520drones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Autonomous%20Drone%20Swarm%20for%20Detecting%20and%20Tracking%20Anomalies%20among%0A%20%20Dense%20Vegetation&entry.906535625=Rakesh%20John%20Amala%20Arokia%20Nathan%20and%20Sigrid%20Strand%20and%20Daniel%20Mehrwald%20and%20Dmitriy%20Shutin%20and%20Oliver%20Bimber&entry.1292438233=%20%20Swarms%20of%20drones%20offer%20an%20increased%20sensing%20aperture%2C%20and%20having%20them%20mimic%0Abehaviors%20of%20natural%20swarms%20enhances%20sampling%20by%20adapting%20the%20aperture%20to%20local%0Aconditions.%20We%20demonstrate%20that%20such%20an%20approach%20makes%20detecting%20and%20tracking%0Aheavily%20occluded%20targets%20practically%20feasible.%20While%20object%20classification%0Aapplied%20to%20conventional%20aerial%20images%20generalizes%20poorly%20the%20randomness%20of%0Aocclusion%20and%20is%20therefore%20inefficient%20even%20under%20lightly%20occluded%20conditions%2C%0Aanomaly%20detection%20applied%20to%20synthetic%20aperture%20integral%20images%20is%20robust%20for%0Adense%20vegetation%2C%20such%20as%20forests%2C%20and%20is%20independent%20of%20pre-trained%20classes.%0AOur%20autonomous%20swarm%20searches%20the%20environment%20for%20occurrences%20of%20the%20unknown%20or%0Aunexpected%2C%20tracking%20them%20while%20continuously%20adapting%20its%20sampling%20pattern%20to%0Aoptimize%20for%20local%20viewing%20conditions.%20In%20our%20real-life%20field%20experiments%20with%0Aa%20swarm%20of%20six%20drones%2C%20we%20achieved%20an%20average%20positional%20accuracy%20of%200.39%20m%0Awith%20an%20average%20precision%20of%2093.2%25%20and%20an%20average%20recall%20of%2095.9%25.%20Here%2C%0Aadapted%20particle%20swarm%20optimization%20considers%20detection%20confidences%20and%0Apredicted%20target%20appearance.%20We%20show%20that%20sensor%20noise%20can%20effectively%20be%0Aincluded%20in%20the%20synthetic%20aperture%20image%20integration%20process%2C%20removing%20the%20need%0Afor%20a%20computationally%20costly%20optimization%20of%20high-dimensional%20parameter%20spaces.%0AFinally%2C%20we%20present%20a%20complete%20hard-%20and%20software%20framework%20that%20supports%0Alow-latency%20transmission%20%28approx.%2080%20ms%20round-trip%20time%29%20and%20fast%20processing%0A%28approx.%20600%20ms%20per%20formation%20step%29%20of%20extensive%20%2870-120%20Mbit/s%29%20video%20and%0Atelemetry%20data%2C%20and%20swarm%20control%20for%20swarms%20of%20up%20to%20ten%20drones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10754v1&entry.124074799=Read"},
{"title": "M\u00f6bius Transform for Mitigating Perspective Distortions in\n  Representation Learning", "author": "Prakash Chandra Chhipa and Meenakshi Subhash Chippa and Kanjar De and Rajkumar Saini and Marcus Liwicki and Mubarak Shah", "abstract": "  Perspective distortion (PD) causes unprecedented changes in shape, size,\norientation, angles, and other spatial relationships of visual concepts in\nimages. Precisely estimating camera intrinsic and extrinsic parameters is a\nchallenging task that prevents synthesizing perspective distortion.\nNon-availability of dedicated training data poses a critical barrier to\ndeveloping robust computer vision methods. Additionally, distortion correction\nmethods make other computer vision tasks a multi-step approach and lack\nperformance. In this work, we propose mitigating perspective distortion (MPD)\nby employing a fine-grained parameter control on a specific family of M\\\"obius\ntransform to model real-world distortion without estimating camera intrinsic\nand extrinsic parameters and without the need for actual distorted data. Also,\nwe present a dedicated perspectively distorted benchmark dataset, ImageNet-PD,\nto benchmark the robustness of deep learning models against this new dataset.\nThe proposed method outperforms existing benchmarks, ImageNet-E and ImageNet-X.\nAdditionally, it significantly improves performance on ImageNet-PD while\nconsistently performing on standard data distribution. Notably, our method\nshows improved performance on three PD-affected real-world applications crowd\ncounting, fisheye image recognition, and person re-identification and one\nPD-affected challenging CV task: object detection. The source code, dataset,\nand models are available on the project webpage at\nhttps://prakashchhipa.github.io/projects/mpd.\n", "link": "http://arxiv.org/abs/2405.02296v2", "date": "2024-07-15", "relevancy": 2.147, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5361}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%C3%B6bius%20Transform%20for%20Mitigating%20Perspective%20Distortions%20in%0A%20%20Representation%20Learning&body=Title%3A%20M%C3%B6bius%20Transform%20for%20Mitigating%20Perspective%20Distortions%20in%0A%20%20Representation%20Learning%0AAuthor%3A%20Prakash%20Chandra%20Chhipa%20and%20Meenakshi%20Subhash%20Chippa%20and%20Kanjar%20De%20and%20Rajkumar%20Saini%20and%20Marcus%20Liwicki%20and%20Mubarak%20Shah%0AAbstract%3A%20%20%20Perspective%20distortion%20%28PD%29%20causes%20unprecedented%20changes%20in%20shape%2C%20size%2C%0Aorientation%2C%20angles%2C%20and%20other%20spatial%20relationships%20of%20visual%20concepts%20in%0Aimages.%20Precisely%20estimating%20camera%20intrinsic%20and%20extrinsic%20parameters%20is%20a%0Achallenging%20task%20that%20prevents%20synthesizing%20perspective%20distortion.%0ANon-availability%20of%20dedicated%20training%20data%20poses%20a%20critical%20barrier%20to%0Adeveloping%20robust%20computer%20vision%20methods.%20Additionally%2C%20distortion%20correction%0Amethods%20make%20other%20computer%20vision%20tasks%20a%20multi-step%20approach%20and%20lack%0Aperformance.%20In%20this%20work%2C%20we%20propose%20mitigating%20perspective%20distortion%20%28MPD%29%0Aby%20employing%20a%20fine-grained%20parameter%20control%20on%20a%20specific%20family%20of%20M%5C%22obius%0Atransform%20to%20model%20real-world%20distortion%20without%20estimating%20camera%20intrinsic%0Aand%20extrinsic%20parameters%20and%20without%20the%20need%20for%20actual%20distorted%20data.%20Also%2C%0Awe%20present%20a%20dedicated%20perspectively%20distorted%20benchmark%20dataset%2C%20ImageNet-PD%2C%0Ato%20benchmark%20the%20robustness%20of%20deep%20learning%20models%20against%20this%20new%20dataset.%0AThe%20proposed%20method%20outperforms%20existing%20benchmarks%2C%20ImageNet-E%20and%20ImageNet-X.%0AAdditionally%2C%20it%20significantly%20improves%20performance%20on%20ImageNet-PD%20while%0Aconsistently%20performing%20on%20standard%20data%20distribution.%20Notably%2C%20our%20method%0Ashows%20improved%20performance%20on%20three%20PD-affected%20real-world%20applications%20crowd%0Acounting%2C%20fisheye%20image%20recognition%2C%20and%20person%20re-identification%20and%20one%0APD-affected%20challenging%20CV%20task%3A%20object%20detection.%20The%20source%20code%2C%20dataset%2C%0Aand%20models%20are%20available%20on%20the%20project%20webpage%20at%0Ahttps%3A//prakashchhipa.github.io/projects/mpd.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02296v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%25C3%25B6bius%2520Transform%2520for%2520Mitigating%2520Perspective%2520Distortions%2520in%250A%2520%2520Representation%2520Learning%26entry.906535625%3DPrakash%2520Chandra%2520Chhipa%2520and%2520Meenakshi%2520Subhash%2520Chippa%2520and%2520Kanjar%2520De%2520and%2520Rajkumar%2520Saini%2520and%2520Marcus%2520Liwicki%2520and%2520Mubarak%2520Shah%26entry.1292438233%3D%2520%2520Perspective%2520distortion%2520%2528PD%2529%2520causes%2520unprecedented%2520changes%2520in%2520shape%252C%2520size%252C%250Aorientation%252C%2520angles%252C%2520and%2520other%2520spatial%2520relationships%2520of%2520visual%2520concepts%2520in%250Aimages.%2520Precisely%2520estimating%2520camera%2520intrinsic%2520and%2520extrinsic%2520parameters%2520is%2520a%250Achallenging%2520task%2520that%2520prevents%2520synthesizing%2520perspective%2520distortion.%250ANon-availability%2520of%2520dedicated%2520training%2520data%2520poses%2520a%2520critical%2520barrier%2520to%250Adeveloping%2520robust%2520computer%2520vision%2520methods.%2520Additionally%252C%2520distortion%2520correction%250Amethods%2520make%2520other%2520computer%2520vision%2520tasks%2520a%2520multi-step%2520approach%2520and%2520lack%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520propose%2520mitigating%2520perspective%2520distortion%2520%2528MPD%2529%250Aby%2520employing%2520a%2520fine-grained%2520parameter%2520control%2520on%2520a%2520specific%2520family%2520of%2520M%255C%2522obius%250Atransform%2520to%2520model%2520real-world%2520distortion%2520without%2520estimating%2520camera%2520intrinsic%250Aand%2520extrinsic%2520parameters%2520and%2520without%2520the%2520need%2520for%2520actual%2520distorted%2520data.%2520Also%252C%250Awe%2520present%2520a%2520dedicated%2520perspectively%2520distorted%2520benchmark%2520dataset%252C%2520ImageNet-PD%252C%250Ato%2520benchmark%2520the%2520robustness%2520of%2520deep%2520learning%2520models%2520against%2520this%2520new%2520dataset.%250AThe%2520proposed%2520method%2520outperforms%2520existing%2520benchmarks%252C%2520ImageNet-E%2520and%2520ImageNet-X.%250AAdditionally%252C%2520it%2520significantly%2520improves%2520performance%2520on%2520ImageNet-PD%2520while%250Aconsistently%2520performing%2520on%2520standard%2520data%2520distribution.%2520Notably%252C%2520our%2520method%250Ashows%2520improved%2520performance%2520on%2520three%2520PD-affected%2520real-world%2520applications%2520crowd%250Acounting%252C%2520fisheye%2520image%2520recognition%252C%2520and%2520person%2520re-identification%2520and%2520one%250APD-affected%2520challenging%2520CV%2520task%253A%2520object%2520detection.%2520The%2520source%2520code%252C%2520dataset%252C%250Aand%2520models%2520are%2520available%2520on%2520the%2520project%2520webpage%2520at%250Ahttps%253A//prakashchhipa.github.io/projects/mpd.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02296v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%C3%B6bius%20Transform%20for%20Mitigating%20Perspective%20Distortions%20in%0A%20%20Representation%20Learning&entry.906535625=Prakash%20Chandra%20Chhipa%20and%20Meenakshi%20Subhash%20Chippa%20and%20Kanjar%20De%20and%20Rajkumar%20Saini%20and%20Marcus%20Liwicki%20and%20Mubarak%20Shah&entry.1292438233=%20%20Perspective%20distortion%20%28PD%29%20causes%20unprecedented%20changes%20in%20shape%2C%20size%2C%0Aorientation%2C%20angles%2C%20and%20other%20spatial%20relationships%20of%20visual%20concepts%20in%0Aimages.%20Precisely%20estimating%20camera%20intrinsic%20and%20extrinsic%20parameters%20is%20a%0Achallenging%20task%20that%20prevents%20synthesizing%20perspective%20distortion.%0ANon-availability%20of%20dedicated%20training%20data%20poses%20a%20critical%20barrier%20to%0Adeveloping%20robust%20computer%20vision%20methods.%20Additionally%2C%20distortion%20correction%0Amethods%20make%20other%20computer%20vision%20tasks%20a%20multi-step%20approach%20and%20lack%0Aperformance.%20In%20this%20work%2C%20we%20propose%20mitigating%20perspective%20distortion%20%28MPD%29%0Aby%20employing%20a%20fine-grained%20parameter%20control%20on%20a%20specific%20family%20of%20M%5C%22obius%0Atransform%20to%20model%20real-world%20distortion%20without%20estimating%20camera%20intrinsic%0Aand%20extrinsic%20parameters%20and%20without%20the%20need%20for%20actual%20distorted%20data.%20Also%2C%0Awe%20present%20a%20dedicated%20perspectively%20distorted%20benchmark%20dataset%2C%20ImageNet-PD%2C%0Ato%20benchmark%20the%20robustness%20of%20deep%20learning%20models%20against%20this%20new%20dataset.%0AThe%20proposed%20method%20outperforms%20existing%20benchmarks%2C%20ImageNet-E%20and%20ImageNet-X.%0AAdditionally%2C%20it%20significantly%20improves%20performance%20on%20ImageNet-PD%20while%0Aconsistently%20performing%20on%20standard%20data%20distribution.%20Notably%2C%20our%20method%0Ashows%20improved%20performance%20on%20three%20PD-affected%20real-world%20applications%20crowd%0Acounting%2C%20fisheye%20image%20recognition%2C%20and%20person%20re-identification%20and%20one%0APD-affected%20challenging%20CV%20task%3A%20object%20detection.%20The%20source%20code%2C%20dataset%2C%0Aand%20models%20are%20available%20on%20the%20project%20webpage%20at%0Ahttps%3A//prakashchhipa.github.io/projects/mpd.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02296v2&entry.124074799=Read"},
{"title": "Towards Robust Event-based Networks for Nighttime via Unpaired\n  Day-to-Night Event Translation", "author": "Yuhwan Jeong and Hoonhee Cho and Kuk-Jin Yoon", "abstract": "  Event cameras with high dynamic range ensure scene capture even in low-light\nconditions. However, night events exhibit patterns different from those\ncaptured during the day. This difference causes performance degradation when\napplying night events to a model trained solely on day events. This limitation\npersists due to a lack of annotated night events. To overcome the limitation,\nwe aim to alleviate data imbalance by translating annotated day data into night\nevents. However, generating events from different modalities challenges\nreproducing their unique properties. Accordingly, we propose an unpaired\nevent-to-event day-to-night translation model that effectively learns to map\nfrom one domain to another using Diffusion GAN. The proposed translation model\nanalyzes events in spatio-temporal dimension with wavelet decomposition and\ndisentangled convolution layers. We also propose a new temporal contrastive\nlearning with a novel shuffling and sampling strategy to regularize temporal\ncontinuity. To validate the efficacy of the proposed methodology, we redesign\nmetrics for evaluating events translated in an unpaired setting, aligning them\nwith the event modality for the first time. Our framework shows the successful\nday-to-night event translation while preserving the characteristics of events.\nIn addition, through our translation method, we facilitate event-based modes to\nlearn about night events by translating annotated day events into night events.\nOur approach effectively mitigates the performance degradation of applying real\nnight events to downstream tasks. The code is available at\nhttps://github.com/jeongyh98/UDNET.\n", "link": "http://arxiv.org/abs/2407.10703v1", "date": "2024-07-15", "relevancy": 2.147, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.54}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5395}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Event-based%20Networks%20for%20Nighttime%20via%20Unpaired%0A%20%20Day-to-Night%20Event%20Translation&body=Title%3A%20Towards%20Robust%20Event-based%20Networks%20for%20Nighttime%20via%20Unpaired%0A%20%20Day-to-Night%20Event%20Translation%0AAuthor%3A%20Yuhwan%20Jeong%20and%20Hoonhee%20Cho%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%20Event%20cameras%20with%20high%20dynamic%20range%20ensure%20scene%20capture%20even%20in%20low-light%0Aconditions.%20However%2C%20night%20events%20exhibit%20patterns%20different%20from%20those%0Acaptured%20during%20the%20day.%20This%20difference%20causes%20performance%20degradation%20when%0Aapplying%20night%20events%20to%20a%20model%20trained%20solely%20on%20day%20events.%20This%20limitation%0Apersists%20due%20to%20a%20lack%20of%20annotated%20night%20events.%20To%20overcome%20the%20limitation%2C%0Awe%20aim%20to%20alleviate%20data%20imbalance%20by%20translating%20annotated%20day%20data%20into%20night%0Aevents.%20However%2C%20generating%20events%20from%20different%20modalities%20challenges%0Areproducing%20their%20unique%20properties.%20Accordingly%2C%20we%20propose%20an%20unpaired%0Aevent-to-event%20day-to-night%20translation%20model%20that%20effectively%20learns%20to%20map%0Afrom%20one%20domain%20to%20another%20using%20Diffusion%20GAN.%20The%20proposed%20translation%20model%0Aanalyzes%20events%20in%20spatio-temporal%20dimension%20with%20wavelet%20decomposition%20and%0Adisentangled%20convolution%20layers.%20We%20also%20propose%20a%20new%20temporal%20contrastive%0Alearning%20with%20a%20novel%20shuffling%20and%20sampling%20strategy%20to%20regularize%20temporal%0Acontinuity.%20To%20validate%20the%20efficacy%20of%20the%20proposed%20methodology%2C%20we%20redesign%0Ametrics%20for%20evaluating%20events%20translated%20in%20an%20unpaired%20setting%2C%20aligning%20them%0Awith%20the%20event%20modality%20for%20the%20first%20time.%20Our%20framework%20shows%20the%20successful%0Aday-to-night%20event%20translation%20while%20preserving%20the%20characteristics%20of%20events.%0AIn%20addition%2C%20through%20our%20translation%20method%2C%20we%20facilitate%20event-based%20modes%20to%0Alearn%20about%20night%20events%20by%20translating%20annotated%20day%20events%20into%20night%20events.%0AOur%20approach%20effectively%20mitigates%20the%20performance%20degradation%20of%20applying%20real%0Anight%20events%20to%20downstream%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/jeongyh98/UDNET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Event-based%2520Networks%2520for%2520Nighttime%2520via%2520Unpaired%250A%2520%2520Day-to-Night%2520Event%2520Translation%26entry.906535625%3DYuhwan%2520Jeong%2520and%2520Hoonhee%2520Cho%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%2520Event%2520cameras%2520with%2520high%2520dynamic%2520range%2520ensure%2520scene%2520capture%2520even%2520in%2520low-light%250Aconditions.%2520However%252C%2520night%2520events%2520exhibit%2520patterns%2520different%2520from%2520those%250Acaptured%2520during%2520the%2520day.%2520This%2520difference%2520causes%2520performance%2520degradation%2520when%250Aapplying%2520night%2520events%2520to%2520a%2520model%2520trained%2520solely%2520on%2520day%2520events.%2520This%2520limitation%250Apersists%2520due%2520to%2520a%2520lack%2520of%2520annotated%2520night%2520events.%2520To%2520overcome%2520the%2520limitation%252C%250Awe%2520aim%2520to%2520alleviate%2520data%2520imbalance%2520by%2520translating%2520annotated%2520day%2520data%2520into%2520night%250Aevents.%2520However%252C%2520generating%2520events%2520from%2520different%2520modalities%2520challenges%250Areproducing%2520their%2520unique%2520properties.%2520Accordingly%252C%2520we%2520propose%2520an%2520unpaired%250Aevent-to-event%2520day-to-night%2520translation%2520model%2520that%2520effectively%2520learns%2520to%2520map%250Afrom%2520one%2520domain%2520to%2520another%2520using%2520Diffusion%2520GAN.%2520The%2520proposed%2520translation%2520model%250Aanalyzes%2520events%2520in%2520spatio-temporal%2520dimension%2520with%2520wavelet%2520decomposition%2520and%250Adisentangled%2520convolution%2520layers.%2520We%2520also%2520propose%2520a%2520new%2520temporal%2520contrastive%250Alearning%2520with%2520a%2520novel%2520shuffling%2520and%2520sampling%2520strategy%2520to%2520regularize%2520temporal%250Acontinuity.%2520To%2520validate%2520the%2520efficacy%2520of%2520the%2520proposed%2520methodology%252C%2520we%2520redesign%250Ametrics%2520for%2520evaluating%2520events%2520translated%2520in%2520an%2520unpaired%2520setting%252C%2520aligning%2520them%250Awith%2520the%2520event%2520modality%2520for%2520the%2520first%2520time.%2520Our%2520framework%2520shows%2520the%2520successful%250Aday-to-night%2520event%2520translation%2520while%2520preserving%2520the%2520characteristics%2520of%2520events.%250AIn%2520addition%252C%2520through%2520our%2520translation%2520method%252C%2520we%2520facilitate%2520event-based%2520modes%2520to%250Alearn%2520about%2520night%2520events%2520by%2520translating%2520annotated%2520day%2520events%2520into%2520night%2520events.%250AOur%2520approach%2520effectively%2520mitigates%2520the%2520performance%2520degradation%2520of%2520applying%2520real%250Anight%2520events%2520to%2520downstream%2520tasks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/jeongyh98/UDNET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Event-based%20Networks%20for%20Nighttime%20via%20Unpaired%0A%20%20Day-to-Night%20Event%20Translation&entry.906535625=Yuhwan%20Jeong%20and%20Hoonhee%20Cho%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%20Event%20cameras%20with%20high%20dynamic%20range%20ensure%20scene%20capture%20even%20in%20low-light%0Aconditions.%20However%2C%20night%20events%20exhibit%20patterns%20different%20from%20those%0Acaptured%20during%20the%20day.%20This%20difference%20causes%20performance%20degradation%20when%0Aapplying%20night%20events%20to%20a%20model%20trained%20solely%20on%20day%20events.%20This%20limitation%0Apersists%20due%20to%20a%20lack%20of%20annotated%20night%20events.%20To%20overcome%20the%20limitation%2C%0Awe%20aim%20to%20alleviate%20data%20imbalance%20by%20translating%20annotated%20day%20data%20into%20night%0Aevents.%20However%2C%20generating%20events%20from%20different%20modalities%20challenges%0Areproducing%20their%20unique%20properties.%20Accordingly%2C%20we%20propose%20an%20unpaired%0Aevent-to-event%20day-to-night%20translation%20model%20that%20effectively%20learns%20to%20map%0Afrom%20one%20domain%20to%20another%20using%20Diffusion%20GAN.%20The%20proposed%20translation%20model%0Aanalyzes%20events%20in%20spatio-temporal%20dimension%20with%20wavelet%20decomposition%20and%0Adisentangled%20convolution%20layers.%20We%20also%20propose%20a%20new%20temporal%20contrastive%0Alearning%20with%20a%20novel%20shuffling%20and%20sampling%20strategy%20to%20regularize%20temporal%0Acontinuity.%20To%20validate%20the%20efficacy%20of%20the%20proposed%20methodology%2C%20we%20redesign%0Ametrics%20for%20evaluating%20events%20translated%20in%20an%20unpaired%20setting%2C%20aligning%20them%0Awith%20the%20event%20modality%20for%20the%20first%20time.%20Our%20framework%20shows%20the%20successful%0Aday-to-night%20event%20translation%20while%20preserving%20the%20characteristics%20of%20events.%0AIn%20addition%2C%20through%20our%20translation%20method%2C%20we%20facilitate%20event-based%20modes%20to%0Alearn%20about%20night%20events%20by%20translating%20annotated%20day%20events%20into%20night%20events.%0AOur%20approach%20effectively%20mitigates%20the%20performance%20degradation%20of%20applying%20real%0Anight%20events%20to%20downstream%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/jeongyh98/UDNET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10703v1&entry.124074799=Read"},
{"title": "Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for\n  Text-to-Image Generation", "author": "Seung Hyun Lee and Yinxiao Li and Junjie Ke and Innfarn Yoo and Han Zhang and Jiahui Yu and Qifei Wang and Fei Deng and Glenn Entis and Junfeng He and Gang Li and Sangpil Kim and Irfan Essa and Feng Yang", "abstract": "  Recent works have demonstrated that using reinforcement learning (RL) with\nmultiple quality rewards can improve the quality of generated images in\ntext-to-image (T2I) generation. However, manually adjusting reward weights\nposes challenges and may cause over-optimization in certain metrics. To solve\nthis, we propose Parrot, which addresses the issue through multi-objective\noptimization and introduces an effective multi-reward optimization strategy to\napproximate Pareto optimal. Utilizing batch-wise Pareto optimal selection,\nParrot automatically identifies the optimal trade-off among different rewards.\nWe use the novel multi-reward optimization algorithm to jointly optimize the\nT2I model and a prompt expansion network, resulting in significant improvement\nof image quality and also allow to control the trade-off of different rewards\nusing a reward related prompt during inference. Furthermore, we introduce\noriginal prompt-centered guidance at inference time, ensuring fidelity to user\ninput after prompt expansion. Extensive experiments and a user study validate\nthe superiority of Parrot over several baselines across various quality\ncriteria, including aesthetics, human preference, text-image alignment, and\nimage sentiment.\n", "link": "http://arxiv.org/abs/2401.05675v2", "date": "2024-07-15", "relevancy": 2.1269, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5466}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5428}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parrot%3A%20Pareto-optimal%20Multi-Reward%20Reinforcement%20Learning%20Framework%20for%0A%20%20Text-to-Image%20Generation&body=Title%3A%20Parrot%3A%20Pareto-optimal%20Multi-Reward%20Reinforcement%20Learning%20Framework%20for%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Seung%20Hyun%20Lee%20and%20Yinxiao%20Li%20and%20Junjie%20Ke%20and%20Innfarn%20Yoo%20and%20Han%20Zhang%20and%20Jiahui%20Yu%20and%20Qifei%20Wang%20and%20Fei%20Deng%20and%20Glenn%20Entis%20and%20Junfeng%20He%20and%20Gang%20Li%20and%20Sangpil%20Kim%20and%20Irfan%20Essa%20and%20Feng%20Yang%0AAbstract%3A%20%20%20Recent%20works%20have%20demonstrated%20that%20using%20reinforcement%20learning%20%28RL%29%20with%0Amultiple%20quality%20rewards%20can%20improve%20the%20quality%20of%20generated%20images%20in%0Atext-to-image%20%28T2I%29%20generation.%20However%2C%20manually%20adjusting%20reward%20weights%0Aposes%20challenges%20and%20may%20cause%20over-optimization%20in%20certain%20metrics.%20To%20solve%0Athis%2C%20we%20propose%20Parrot%2C%20which%20addresses%20the%20issue%20through%20multi-objective%0Aoptimization%20and%20introduces%20an%20effective%20multi-reward%20optimization%20strategy%20to%0Aapproximate%20Pareto%20optimal.%20Utilizing%20batch-wise%20Pareto%20optimal%20selection%2C%0AParrot%20automatically%20identifies%20the%20optimal%20trade-off%20among%20different%20rewards.%0AWe%20use%20the%20novel%20multi-reward%20optimization%20algorithm%20to%20jointly%20optimize%20the%0AT2I%20model%20and%20a%20prompt%20expansion%20network%2C%20resulting%20in%20significant%20improvement%0Aof%20image%20quality%20and%20also%20allow%20to%20control%20the%20trade-off%20of%20different%20rewards%0Ausing%20a%20reward%20related%20prompt%20during%20inference.%20Furthermore%2C%20we%20introduce%0Aoriginal%20prompt-centered%20guidance%20at%20inference%20time%2C%20ensuring%20fidelity%20to%20user%0Ainput%20after%20prompt%20expansion.%20Extensive%20experiments%20and%20a%20user%20study%20validate%0Athe%20superiority%20of%20Parrot%20over%20several%20baselines%20across%20various%20quality%0Acriteria%2C%20including%20aesthetics%2C%20human%20preference%2C%20text-image%20alignment%2C%20and%0Aimage%20sentiment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05675v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParrot%253A%2520Pareto-optimal%2520Multi-Reward%2520Reinforcement%2520Learning%2520Framework%2520for%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DSeung%2520Hyun%2520Lee%2520and%2520Yinxiao%2520Li%2520and%2520Junjie%2520Ke%2520and%2520Innfarn%2520Yoo%2520and%2520Han%2520Zhang%2520and%2520Jiahui%2520Yu%2520and%2520Qifei%2520Wang%2520and%2520Fei%2520Deng%2520and%2520Glenn%2520Entis%2520and%2520Junfeng%2520He%2520and%2520Gang%2520Li%2520and%2520Sangpil%2520Kim%2520and%2520Irfan%2520Essa%2520and%2520Feng%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520demonstrated%2520that%2520using%2520reinforcement%2520learning%2520%2528RL%2529%2520with%250Amultiple%2520quality%2520rewards%2520can%2520improve%2520the%2520quality%2520of%2520generated%2520images%2520in%250Atext-to-image%2520%2528T2I%2529%2520generation.%2520However%252C%2520manually%2520adjusting%2520reward%2520weights%250Aposes%2520challenges%2520and%2520may%2520cause%2520over-optimization%2520in%2520certain%2520metrics.%2520To%2520solve%250Athis%252C%2520we%2520propose%2520Parrot%252C%2520which%2520addresses%2520the%2520issue%2520through%2520multi-objective%250Aoptimization%2520and%2520introduces%2520an%2520effective%2520multi-reward%2520optimization%2520strategy%2520to%250Aapproximate%2520Pareto%2520optimal.%2520Utilizing%2520batch-wise%2520Pareto%2520optimal%2520selection%252C%250AParrot%2520automatically%2520identifies%2520the%2520optimal%2520trade-off%2520among%2520different%2520rewards.%250AWe%2520use%2520the%2520novel%2520multi-reward%2520optimization%2520algorithm%2520to%2520jointly%2520optimize%2520the%250AT2I%2520model%2520and%2520a%2520prompt%2520expansion%2520network%252C%2520resulting%2520in%2520significant%2520improvement%250Aof%2520image%2520quality%2520and%2520also%2520allow%2520to%2520control%2520the%2520trade-off%2520of%2520different%2520rewards%250Ausing%2520a%2520reward%2520related%2520prompt%2520during%2520inference.%2520Furthermore%252C%2520we%2520introduce%250Aoriginal%2520prompt-centered%2520guidance%2520at%2520inference%2520time%252C%2520ensuring%2520fidelity%2520to%2520user%250Ainput%2520after%2520prompt%2520expansion.%2520Extensive%2520experiments%2520and%2520a%2520user%2520study%2520validate%250Athe%2520superiority%2520of%2520Parrot%2520over%2520several%2520baselines%2520across%2520various%2520quality%250Acriteria%252C%2520including%2520aesthetics%252C%2520human%2520preference%252C%2520text-image%2520alignment%252C%2520and%250Aimage%2520sentiment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05675v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parrot%3A%20Pareto-optimal%20Multi-Reward%20Reinforcement%20Learning%20Framework%20for%0A%20%20Text-to-Image%20Generation&entry.906535625=Seung%20Hyun%20Lee%20and%20Yinxiao%20Li%20and%20Junjie%20Ke%20and%20Innfarn%20Yoo%20and%20Han%20Zhang%20and%20Jiahui%20Yu%20and%20Qifei%20Wang%20and%20Fei%20Deng%20and%20Glenn%20Entis%20and%20Junfeng%20He%20and%20Gang%20Li%20and%20Sangpil%20Kim%20and%20Irfan%20Essa%20and%20Feng%20Yang&entry.1292438233=%20%20Recent%20works%20have%20demonstrated%20that%20using%20reinforcement%20learning%20%28RL%29%20with%0Amultiple%20quality%20rewards%20can%20improve%20the%20quality%20of%20generated%20images%20in%0Atext-to-image%20%28T2I%29%20generation.%20However%2C%20manually%20adjusting%20reward%20weights%0Aposes%20challenges%20and%20may%20cause%20over-optimization%20in%20certain%20metrics.%20To%20solve%0Athis%2C%20we%20propose%20Parrot%2C%20which%20addresses%20the%20issue%20through%20multi-objective%0Aoptimization%20and%20introduces%20an%20effective%20multi-reward%20optimization%20strategy%20to%0Aapproximate%20Pareto%20optimal.%20Utilizing%20batch-wise%20Pareto%20optimal%20selection%2C%0AParrot%20automatically%20identifies%20the%20optimal%20trade-off%20among%20different%20rewards.%0AWe%20use%20the%20novel%20multi-reward%20optimization%20algorithm%20to%20jointly%20optimize%20the%0AT2I%20model%20and%20a%20prompt%20expansion%20network%2C%20resulting%20in%20significant%20improvement%0Aof%20image%20quality%20and%20also%20allow%20to%20control%20the%20trade-off%20of%20different%20rewards%0Ausing%20a%20reward%20related%20prompt%20during%20inference.%20Furthermore%2C%20we%20introduce%0Aoriginal%20prompt-centered%20guidance%20at%20inference%20time%2C%20ensuring%20fidelity%20to%20user%0Ainput%20after%20prompt%20expansion.%20Extensive%20experiments%20and%20a%20user%20study%20validate%0Athe%20superiority%20of%20Parrot%20over%20several%20baselines%20across%20various%20quality%0Acriteria%2C%20including%20aesthetics%2C%20human%20preference%2C%20text-image%20alignment%2C%20and%0Aimage%20sentiment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05675v2&entry.124074799=Read"},
{"title": "Deep ContourFlow: Advancing Active Contours with Deep Learning", "author": "Antoine Habis and Vannary Meas-Yedid and Elsa Angelini and Jean-Christophe Olivo-Marin", "abstract": "  This paper introduces a novel approach that combines unsupervised active\ncontour models with deep learning for robust and adaptive image segmentation.\nIndeed, traditional active contours, provide a flexible framework for contour\nevolution and learning offers the capacity to learn intricate features and\npatterns directly from raw data. Our proposed methodology leverages the\nstrengths of both paradigms, presenting a framework for both unsupervised and\none-shot approaches for image segmentation. It is capable of capturing complex\nobject boundaries without the need for extensive labeled training data. This is\nparticularly required in histology, a field facing a significant shortage of\nannotations due to the challenging and time-consuming nature of the annotation\nprocess. We illustrate and compare our results to state of the art methods on a\nhistology dataset and show significant improvements.\n", "link": "http://arxiv.org/abs/2407.10696v1", "date": "2024-07-15", "relevancy": 2.1238, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5544}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20ContourFlow%3A%20Advancing%20Active%20Contours%20with%20Deep%20Learning&body=Title%3A%20Deep%20ContourFlow%3A%20Advancing%20Active%20Contours%20with%20Deep%20Learning%0AAuthor%3A%20Antoine%20Habis%20and%20Vannary%20Meas-Yedid%20and%20Elsa%20Angelini%20and%20Jean-Christophe%20Olivo-Marin%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20that%20combines%20unsupervised%20active%0Acontour%20models%20with%20deep%20learning%20for%20robust%20and%20adaptive%20image%20segmentation.%0AIndeed%2C%20traditional%20active%20contours%2C%20provide%20a%20flexible%20framework%20for%20contour%0Aevolution%20and%20learning%20offers%20the%20capacity%20to%20learn%20intricate%20features%20and%0Apatterns%20directly%20from%20raw%20data.%20Our%20proposed%20methodology%20leverages%20the%0Astrengths%20of%20both%20paradigms%2C%20presenting%20a%20framework%20for%20both%20unsupervised%20and%0Aone-shot%20approaches%20for%20image%20segmentation.%20It%20is%20capable%20of%20capturing%20complex%0Aobject%20boundaries%20without%20the%20need%20for%20extensive%20labeled%20training%20data.%20This%20is%0Aparticularly%20required%20in%20histology%2C%20a%20field%20facing%20a%20significant%20shortage%20of%0Aannotations%20due%20to%20the%20challenging%20and%20time-consuming%20nature%20of%20the%20annotation%0Aprocess.%20We%20illustrate%20and%20compare%20our%20results%20to%20state%20of%20the%20art%20methods%20on%20a%0Ahistology%20dataset%20and%20show%20significant%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520ContourFlow%253A%2520Advancing%2520Active%2520Contours%2520with%2520Deep%2520Learning%26entry.906535625%3DAntoine%2520Habis%2520and%2520Vannary%2520Meas-Yedid%2520and%2520Elsa%2520Angelini%2520and%2520Jean-Christophe%2520Olivo-Marin%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520that%2520combines%2520unsupervised%2520active%250Acontour%2520models%2520with%2520deep%2520learning%2520for%2520robust%2520and%2520adaptive%2520image%2520segmentation.%250AIndeed%252C%2520traditional%2520active%2520contours%252C%2520provide%2520a%2520flexible%2520framework%2520for%2520contour%250Aevolution%2520and%2520learning%2520offers%2520the%2520capacity%2520to%2520learn%2520intricate%2520features%2520and%250Apatterns%2520directly%2520from%2520raw%2520data.%2520Our%2520proposed%2520methodology%2520leverages%2520the%250Astrengths%2520of%2520both%2520paradigms%252C%2520presenting%2520a%2520framework%2520for%2520both%2520unsupervised%2520and%250Aone-shot%2520approaches%2520for%2520image%2520segmentation.%2520It%2520is%2520capable%2520of%2520capturing%2520complex%250Aobject%2520boundaries%2520without%2520the%2520need%2520for%2520extensive%2520labeled%2520training%2520data.%2520This%2520is%250Aparticularly%2520required%2520in%2520histology%252C%2520a%2520field%2520facing%2520a%2520significant%2520shortage%2520of%250Aannotations%2520due%2520to%2520the%2520challenging%2520and%2520time-consuming%2520nature%2520of%2520the%2520annotation%250Aprocess.%2520We%2520illustrate%2520and%2520compare%2520our%2520results%2520to%2520state%2520of%2520the%2520art%2520methods%2520on%2520a%250Ahistology%2520dataset%2520and%2520show%2520significant%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20ContourFlow%3A%20Advancing%20Active%20Contours%20with%20Deep%20Learning&entry.906535625=Antoine%20Habis%20and%20Vannary%20Meas-Yedid%20and%20Elsa%20Angelini%20and%20Jean-Christophe%20Olivo-Marin&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20that%20combines%20unsupervised%20active%0Acontour%20models%20with%20deep%20learning%20for%20robust%20and%20adaptive%20image%20segmentation.%0AIndeed%2C%20traditional%20active%20contours%2C%20provide%20a%20flexible%20framework%20for%20contour%0Aevolution%20and%20learning%20offers%20the%20capacity%20to%20learn%20intricate%20features%20and%0Apatterns%20directly%20from%20raw%20data.%20Our%20proposed%20methodology%20leverages%20the%0Astrengths%20of%20both%20paradigms%2C%20presenting%20a%20framework%20for%20both%20unsupervised%20and%0Aone-shot%20approaches%20for%20image%20segmentation.%20It%20is%20capable%20of%20capturing%20complex%0Aobject%20boundaries%20without%20the%20need%20for%20extensive%20labeled%20training%20data.%20This%20is%0Aparticularly%20required%20in%20histology%2C%20a%20field%20facing%20a%20significant%20shortage%20of%0Aannotations%20due%20to%20the%20challenging%20and%20time-consuming%20nature%20of%20the%20annotation%0Aprocess.%20We%20illustrate%20and%20compare%20our%20results%20to%20state%20of%20the%20art%20methods%20on%20a%0Ahistology%20dataset%20and%20show%20significant%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10696v1&entry.124074799=Read"},
{"title": "Constrained Reinforcement Learning using Distributional Representation\n  for Trustworthy Quadrotor UAV Tracking Control", "author": "Yanran Wang and David Boyle", "abstract": "  Simultaneously accurate and reliable tracking control for quadrotors in\ncomplex dynamic environments is challenging. As aerodynamics derived from drag\nforces and moment variations are chaotic and difficult to precisely identify,\nmost current quadrotor tracking systems treat them as simple `disturbances' in\nconventional control approaches. We propose a novel, interpretable trajectory\ntracker integrating a Distributional Reinforcement Learning disturbance\nestimator for unknown aerodynamic effects with a Stochastic Model Predictive\nController (SMPC). The proposed estimator `Constrained Distributional\nReinforced disturbance estimator' (ConsDRED) accurately identifies\nuncertainties between true and estimated values of aerodynamic effects.\nSimplified Affine Disturbance Feedback is used for control parameterization to\nguarantee convexity, which we then integrate with a SMPC. We theoretically\nguarantee that ConsDRED achieves at least an optimal global convergence rate\nand a certain sublinear rate if constraints are violated with an error\ndecreases as the width and the layer of neural network increase. To demonstrate\npracticality, we show convergent training in simulation and real-world\nexperiments, and empirically verify that ConsDRED is less sensitive to\nhyperparameter settings compared with canonical constrained RL approaches. We\ndemonstrate our system improves accumulative tracking errors by at least 70%\ncompared with the recent art. Importantly, the proposed framework,\nConsDRED-SMPC, balances the tradeoff between pursuing high performance and\nobeying conservative constraints for practical implementations.\n", "link": "http://arxiv.org/abs/2302.11694v4", "date": "2024-07-15", "relevancy": 2.116, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5463}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5419}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Reinforcement%20Learning%20using%20Distributional%20Representation%0A%20%20for%20Trustworthy%20Quadrotor%20UAV%20Tracking%20Control&body=Title%3A%20Constrained%20Reinforcement%20Learning%20using%20Distributional%20Representation%0A%20%20for%20Trustworthy%20Quadrotor%20UAV%20Tracking%20Control%0AAuthor%3A%20Yanran%20Wang%20and%20David%20Boyle%0AAbstract%3A%20%20%20Simultaneously%20accurate%20and%20reliable%20tracking%20control%20for%20quadrotors%20in%0Acomplex%20dynamic%20environments%20is%20challenging.%20As%20aerodynamics%20derived%20from%20drag%0Aforces%20and%20moment%20variations%20are%20chaotic%20and%20difficult%20to%20precisely%20identify%2C%0Amost%20current%20quadrotor%20tracking%20systems%20treat%20them%20as%20simple%20%60disturbances%27%20in%0Aconventional%20control%20approaches.%20We%20propose%20a%20novel%2C%20interpretable%20trajectory%0Atracker%20integrating%20a%20Distributional%20Reinforcement%20Learning%20disturbance%0Aestimator%20for%20unknown%20aerodynamic%20effects%20with%20a%20Stochastic%20Model%20Predictive%0AController%20%28SMPC%29.%20The%20proposed%20estimator%20%60Constrained%20Distributional%0AReinforced%20disturbance%20estimator%27%20%28ConsDRED%29%20accurately%20identifies%0Auncertainties%20between%20true%20and%20estimated%20values%20of%20aerodynamic%20effects.%0ASimplified%20Affine%20Disturbance%20Feedback%20is%20used%20for%20control%20parameterization%20to%0Aguarantee%20convexity%2C%20which%20we%20then%20integrate%20with%20a%20SMPC.%20We%20theoretically%0Aguarantee%20that%20ConsDRED%20achieves%20at%20least%20an%20optimal%20global%20convergence%20rate%0Aand%20a%20certain%20sublinear%20rate%20if%20constraints%20are%20violated%20with%20an%20error%0Adecreases%20as%20the%20width%20and%20the%20layer%20of%20neural%20network%20increase.%20To%20demonstrate%0Apracticality%2C%20we%20show%20convergent%20training%20in%20simulation%20and%20real-world%0Aexperiments%2C%20and%20empirically%20verify%20that%20ConsDRED%20is%20less%20sensitive%20to%0Ahyperparameter%20settings%20compared%20with%20canonical%20constrained%20RL%20approaches.%20We%0Ademonstrate%20our%20system%20improves%20accumulative%20tracking%20errors%20by%20at%20least%2070%25%0Acompared%20with%20the%20recent%20art.%20Importantly%2C%20the%20proposed%20framework%2C%0AConsDRED-SMPC%2C%20balances%20the%20tradeoff%20between%20pursuing%20high%20performance%20and%0Aobeying%20conservative%20constraints%20for%20practical%20implementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.11694v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Reinforcement%2520Learning%2520using%2520Distributional%2520Representation%250A%2520%2520for%2520Trustworthy%2520Quadrotor%2520UAV%2520Tracking%2520Control%26entry.906535625%3DYanran%2520Wang%2520and%2520David%2520Boyle%26entry.1292438233%3D%2520%2520Simultaneously%2520accurate%2520and%2520reliable%2520tracking%2520control%2520for%2520quadrotors%2520in%250Acomplex%2520dynamic%2520environments%2520is%2520challenging.%2520As%2520aerodynamics%2520derived%2520from%2520drag%250Aforces%2520and%2520moment%2520variations%2520are%2520chaotic%2520and%2520difficult%2520to%2520precisely%2520identify%252C%250Amost%2520current%2520quadrotor%2520tracking%2520systems%2520treat%2520them%2520as%2520simple%2520%2560disturbances%2527%2520in%250Aconventional%2520control%2520approaches.%2520We%2520propose%2520a%2520novel%252C%2520interpretable%2520trajectory%250Atracker%2520integrating%2520a%2520Distributional%2520Reinforcement%2520Learning%2520disturbance%250Aestimator%2520for%2520unknown%2520aerodynamic%2520effects%2520with%2520a%2520Stochastic%2520Model%2520Predictive%250AController%2520%2528SMPC%2529.%2520The%2520proposed%2520estimator%2520%2560Constrained%2520Distributional%250AReinforced%2520disturbance%2520estimator%2527%2520%2528ConsDRED%2529%2520accurately%2520identifies%250Auncertainties%2520between%2520true%2520and%2520estimated%2520values%2520of%2520aerodynamic%2520effects.%250ASimplified%2520Affine%2520Disturbance%2520Feedback%2520is%2520used%2520for%2520control%2520parameterization%2520to%250Aguarantee%2520convexity%252C%2520which%2520we%2520then%2520integrate%2520with%2520a%2520SMPC.%2520We%2520theoretically%250Aguarantee%2520that%2520ConsDRED%2520achieves%2520at%2520least%2520an%2520optimal%2520global%2520convergence%2520rate%250Aand%2520a%2520certain%2520sublinear%2520rate%2520if%2520constraints%2520are%2520violated%2520with%2520an%2520error%250Adecreases%2520as%2520the%2520width%2520and%2520the%2520layer%2520of%2520neural%2520network%2520increase.%2520To%2520demonstrate%250Apracticality%252C%2520we%2520show%2520convergent%2520training%2520in%2520simulation%2520and%2520real-world%250Aexperiments%252C%2520and%2520empirically%2520verify%2520that%2520ConsDRED%2520is%2520less%2520sensitive%2520to%250Ahyperparameter%2520settings%2520compared%2520with%2520canonical%2520constrained%2520RL%2520approaches.%2520We%250Ademonstrate%2520our%2520system%2520improves%2520accumulative%2520tracking%2520errors%2520by%2520at%2520least%252070%2525%250Acompared%2520with%2520the%2520recent%2520art.%2520Importantly%252C%2520the%2520proposed%2520framework%252C%250AConsDRED-SMPC%252C%2520balances%2520the%2520tradeoff%2520between%2520pursuing%2520high%2520performance%2520and%250Aobeying%2520conservative%2520constraints%2520for%2520practical%2520implementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.11694v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Reinforcement%20Learning%20using%20Distributional%20Representation%0A%20%20for%20Trustworthy%20Quadrotor%20UAV%20Tracking%20Control&entry.906535625=Yanran%20Wang%20and%20David%20Boyle&entry.1292438233=%20%20Simultaneously%20accurate%20and%20reliable%20tracking%20control%20for%20quadrotors%20in%0Acomplex%20dynamic%20environments%20is%20challenging.%20As%20aerodynamics%20derived%20from%20drag%0Aforces%20and%20moment%20variations%20are%20chaotic%20and%20difficult%20to%20precisely%20identify%2C%0Amost%20current%20quadrotor%20tracking%20systems%20treat%20them%20as%20simple%20%60disturbances%27%20in%0Aconventional%20control%20approaches.%20We%20propose%20a%20novel%2C%20interpretable%20trajectory%0Atracker%20integrating%20a%20Distributional%20Reinforcement%20Learning%20disturbance%0Aestimator%20for%20unknown%20aerodynamic%20effects%20with%20a%20Stochastic%20Model%20Predictive%0AController%20%28SMPC%29.%20The%20proposed%20estimator%20%60Constrained%20Distributional%0AReinforced%20disturbance%20estimator%27%20%28ConsDRED%29%20accurately%20identifies%0Auncertainties%20between%20true%20and%20estimated%20values%20of%20aerodynamic%20effects.%0ASimplified%20Affine%20Disturbance%20Feedback%20is%20used%20for%20control%20parameterization%20to%0Aguarantee%20convexity%2C%20which%20we%20then%20integrate%20with%20a%20SMPC.%20We%20theoretically%0Aguarantee%20that%20ConsDRED%20achieves%20at%20least%20an%20optimal%20global%20convergence%20rate%0Aand%20a%20certain%20sublinear%20rate%20if%20constraints%20are%20violated%20with%20an%20error%0Adecreases%20as%20the%20width%20and%20the%20layer%20of%20neural%20network%20increase.%20To%20demonstrate%0Apracticality%2C%20we%20show%20convergent%20training%20in%20simulation%20and%20real-world%0Aexperiments%2C%20and%20empirically%20verify%20that%20ConsDRED%20is%20less%20sensitive%20to%0Ahyperparameter%20settings%20compared%20with%20canonical%20constrained%20RL%20approaches.%20We%0Ademonstrate%20our%20system%20improves%20accumulative%20tracking%20errors%20by%20at%20least%2070%25%0Acompared%20with%20the%20recent%20art.%20Importantly%2C%20the%20proposed%20framework%2C%0AConsDRED-SMPC%2C%20balances%20the%20tradeoff%20between%20pursuing%20high%20performance%20and%0Aobeying%20conservative%20constraints%20for%20practical%20implementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.11694v4&entry.124074799=Read"},
{"title": "Multi-Attention Integrated Deep Learning Frameworks for Enhanced Breast\n  Cancer Segmentation and Identification", "author": "Pandiyaraju V and Shravan Venkatraman and Pavan Kumar S and Santhosh Malarvannan and Kannan A", "abstract": "  Breast cancer poses a profound threat to lives globally, claiming numerous\nlives each year. Therefore, timely detection is crucial for early intervention\nand improved chances of survival. Accurately diagnosing and classifying breast\ntumors using ultrasound images is a persistent challenge in medicine, demanding\ncutting-edge solutions for improved treatment strategies. This research\nintroduces multiattention-enhanced deep learning (DL) frameworks designed for\nthe classification and segmentation of breast cancer tumors from ultrasound\nimages. A spatial channel attention mechanism is proposed for segmenting tumors\nfrom ultrasound images, utilizing a novel LinkNet DL framework with an\nInceptionResNet backbone. Following this, the paper proposes a deep\nconvolutional neural network with an integrated multi-attention framework\n(DCNNIMAF) to classify the segmented tumor as benign, malignant, or normal.\nFrom experimental results, it is observed that the segmentation model has\nrecorded an accuracy of 98.1%, with a minimal loss of 0.6%. It has also\nachieved high Intersection over Union (IoU) and Dice Coefficient scores of\n96.9% and 97.2%, respectively. Similarly, the classification model has attained\nan accuracy of 99.2%, with a low loss of 0.31%. Furthermore, the classification\nframework has achieved outstanding F1-Score, precision, and recall values of\n99.1%, 99.3%, and 99.1%, respectively. By offering a robust framework for early\ndetection and accurate classification of breast cancer, this proposed work\nsignificantly advances the field of medical image analysis, potentially\nimproving diagnostic precision and patient outcomes.\n", "link": "http://arxiv.org/abs/2407.02844v3", "date": "2024-07-15", "relevancy": 2.1154, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5367}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Attention%20Integrated%20Deep%20Learning%20Frameworks%20for%20Enhanced%20Breast%0A%20%20Cancer%20Segmentation%20and%20Identification&body=Title%3A%20Multi-Attention%20Integrated%20Deep%20Learning%20Frameworks%20for%20Enhanced%20Breast%0A%20%20Cancer%20Segmentation%20and%20Identification%0AAuthor%3A%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Santhosh%20Malarvannan%20and%20Kannan%20A%0AAbstract%3A%20%20%20Breast%20cancer%20poses%20a%20profound%20threat%20to%20lives%20globally%2C%20claiming%20numerous%0Alives%20each%20year.%20Therefore%2C%20timely%20detection%20is%20crucial%20for%20early%20intervention%0Aand%20improved%20chances%20of%20survival.%20Accurately%20diagnosing%20and%20classifying%20breast%0Atumors%20using%20ultrasound%20images%20is%20a%20persistent%20challenge%20in%20medicine%2C%20demanding%0Acutting-edge%20solutions%20for%20improved%20treatment%20strategies.%20This%20research%0Aintroduces%20multiattention-enhanced%20deep%20learning%20%28DL%29%20frameworks%20designed%20for%0Athe%20classification%20and%20segmentation%20of%20breast%20cancer%20tumors%20from%20ultrasound%0Aimages.%20A%20spatial%20channel%20attention%20mechanism%20is%20proposed%20for%20segmenting%20tumors%0Afrom%20ultrasound%20images%2C%20utilizing%20a%20novel%20LinkNet%20DL%20framework%20with%20an%0AInceptionResNet%20backbone.%20Following%20this%2C%20the%20paper%20proposes%20a%20deep%0Aconvolutional%20neural%20network%20with%20an%20integrated%20multi-attention%20framework%0A%28DCNNIMAF%29%20to%20classify%20the%20segmented%20tumor%20as%20benign%2C%20malignant%2C%20or%20normal.%0AFrom%20experimental%20results%2C%20it%20is%20observed%20that%20the%20segmentation%20model%20has%0Arecorded%20an%20accuracy%20of%2098.1%25%2C%20with%20a%20minimal%20loss%20of%200.6%25.%20It%20has%20also%0Aachieved%20high%20Intersection%20over%20Union%20%28IoU%29%20and%20Dice%20Coefficient%20scores%20of%0A96.9%25%20and%2097.2%25%2C%20respectively.%20Similarly%2C%20the%20classification%20model%20has%20attained%0Aan%20accuracy%20of%2099.2%25%2C%20with%20a%20low%20loss%20of%200.31%25.%20Furthermore%2C%20the%20classification%0Aframework%20has%20achieved%20outstanding%20F1-Score%2C%20precision%2C%20and%20recall%20values%20of%0A99.1%25%2C%2099.3%25%2C%20and%2099.1%25%2C%20respectively.%20By%20offering%20a%20robust%20framework%20for%20early%0Adetection%20and%20accurate%20classification%20of%20breast%20cancer%2C%20this%20proposed%20work%0Asignificantly%20advances%20the%20field%20of%20medical%20image%20analysis%2C%20potentially%0Aimproving%20diagnostic%20precision%20and%20patient%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02844v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Attention%2520Integrated%2520Deep%2520Learning%2520Frameworks%2520for%2520Enhanced%2520Breast%250A%2520%2520Cancer%2520Segmentation%2520and%2520Identification%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Santhosh%2520Malarvannan%2520and%2520Kannan%2520A%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520poses%2520a%2520profound%2520threat%2520to%2520lives%2520globally%252C%2520claiming%2520numerous%250Alives%2520each%2520year.%2520Therefore%252C%2520timely%2520detection%2520is%2520crucial%2520for%2520early%2520intervention%250Aand%2520improved%2520chances%2520of%2520survival.%2520Accurately%2520diagnosing%2520and%2520classifying%2520breast%250Atumors%2520using%2520ultrasound%2520images%2520is%2520a%2520persistent%2520challenge%2520in%2520medicine%252C%2520demanding%250Acutting-edge%2520solutions%2520for%2520improved%2520treatment%2520strategies.%2520This%2520research%250Aintroduces%2520multiattention-enhanced%2520deep%2520learning%2520%2528DL%2529%2520frameworks%2520designed%2520for%250Athe%2520classification%2520and%2520segmentation%2520of%2520breast%2520cancer%2520tumors%2520from%2520ultrasound%250Aimages.%2520A%2520spatial%2520channel%2520attention%2520mechanism%2520is%2520proposed%2520for%2520segmenting%2520tumors%250Afrom%2520ultrasound%2520images%252C%2520utilizing%2520a%2520novel%2520LinkNet%2520DL%2520framework%2520with%2520an%250AInceptionResNet%2520backbone.%2520Following%2520this%252C%2520the%2520paper%2520proposes%2520a%2520deep%250Aconvolutional%2520neural%2520network%2520with%2520an%2520integrated%2520multi-attention%2520framework%250A%2528DCNNIMAF%2529%2520to%2520classify%2520the%2520segmented%2520tumor%2520as%2520benign%252C%2520malignant%252C%2520or%2520normal.%250AFrom%2520experimental%2520results%252C%2520it%2520is%2520observed%2520that%2520the%2520segmentation%2520model%2520has%250Arecorded%2520an%2520accuracy%2520of%252098.1%2525%252C%2520with%2520a%2520minimal%2520loss%2520of%25200.6%2525.%2520It%2520has%2520also%250Aachieved%2520high%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520and%2520Dice%2520Coefficient%2520scores%2520of%250A96.9%2525%2520and%252097.2%2525%252C%2520respectively.%2520Similarly%252C%2520the%2520classification%2520model%2520has%2520attained%250Aan%2520accuracy%2520of%252099.2%2525%252C%2520with%2520a%2520low%2520loss%2520of%25200.31%2525.%2520Furthermore%252C%2520the%2520classification%250Aframework%2520has%2520achieved%2520outstanding%2520F1-Score%252C%2520precision%252C%2520and%2520recall%2520values%2520of%250A99.1%2525%252C%252099.3%2525%252C%2520and%252099.1%2525%252C%2520respectively.%2520By%2520offering%2520a%2520robust%2520framework%2520for%2520early%250Adetection%2520and%2520accurate%2520classification%2520of%2520breast%2520cancer%252C%2520this%2520proposed%2520work%250Asignificantly%2520advances%2520the%2520field%2520of%2520medical%2520image%2520analysis%252C%2520potentially%250Aimproving%2520diagnostic%2520precision%2520and%2520patient%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02844v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Attention%20Integrated%20Deep%20Learning%20Frameworks%20for%20Enhanced%20Breast%0A%20%20Cancer%20Segmentation%20and%20Identification&entry.906535625=Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Santhosh%20Malarvannan%20and%20Kannan%20A&entry.1292438233=%20%20Breast%20cancer%20poses%20a%20profound%20threat%20to%20lives%20globally%2C%20claiming%20numerous%0Alives%20each%20year.%20Therefore%2C%20timely%20detection%20is%20crucial%20for%20early%20intervention%0Aand%20improved%20chances%20of%20survival.%20Accurately%20diagnosing%20and%20classifying%20breast%0Atumors%20using%20ultrasound%20images%20is%20a%20persistent%20challenge%20in%20medicine%2C%20demanding%0Acutting-edge%20solutions%20for%20improved%20treatment%20strategies.%20This%20research%0Aintroduces%20multiattention-enhanced%20deep%20learning%20%28DL%29%20frameworks%20designed%20for%0Athe%20classification%20and%20segmentation%20of%20breast%20cancer%20tumors%20from%20ultrasound%0Aimages.%20A%20spatial%20channel%20attention%20mechanism%20is%20proposed%20for%20segmenting%20tumors%0Afrom%20ultrasound%20images%2C%20utilizing%20a%20novel%20LinkNet%20DL%20framework%20with%20an%0AInceptionResNet%20backbone.%20Following%20this%2C%20the%20paper%20proposes%20a%20deep%0Aconvolutional%20neural%20network%20with%20an%20integrated%20multi-attention%20framework%0A%28DCNNIMAF%29%20to%20classify%20the%20segmented%20tumor%20as%20benign%2C%20malignant%2C%20or%20normal.%0AFrom%20experimental%20results%2C%20it%20is%20observed%20that%20the%20segmentation%20model%20has%0Arecorded%20an%20accuracy%20of%2098.1%25%2C%20with%20a%20minimal%20loss%20of%200.6%25.%20It%20has%20also%0Aachieved%20high%20Intersection%20over%20Union%20%28IoU%29%20and%20Dice%20Coefficient%20scores%20of%0A96.9%25%20and%2097.2%25%2C%20respectively.%20Similarly%2C%20the%20classification%20model%20has%20attained%0Aan%20accuracy%20of%2099.2%25%2C%20with%20a%20low%20loss%20of%200.31%25.%20Furthermore%2C%20the%20classification%0Aframework%20has%20achieved%20outstanding%20F1-Score%2C%20precision%2C%20and%20recall%20values%20of%0A99.1%25%2C%2099.3%25%2C%20and%2099.1%25%2C%20respectively.%20By%20offering%20a%20robust%20framework%20for%20early%0Adetection%20and%20accurate%20classification%20of%20breast%20cancer%2C%20this%20proposed%20work%0Asignificantly%20advances%20the%20field%20of%20medical%20image%20analysis%2C%20potentially%0Aimproving%20diagnostic%20precision%20and%20patient%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02844v3&entry.124074799=Read"},
{"title": "Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot\n  Whole Slide Image Classification", "author": "Linhao Qu and Dingkang Yang and Dan Huang and Qinhao Guo and Rongkui Luo and Shaoting Zhang and Xiaosong Wang", "abstract": "  Current multi-instance learning algorithms for pathology image analysis often\nrequire a substantial number of Whole Slide Images for effective training but\nexhibit suboptimal performance in scenarios with limited learning data. In\nclinical settings, restricted access to pathology slides is inevitable due to\npatient privacy concerns and the prevalence of rare or emerging diseases. The\nemergence of the Few-shot Weakly Supervised WSI Classification accommodates the\nsignificant challenge of the limited slide data and sparse slide-level labels\nfor diagnosis. Prompt learning based on the pre-trained models (\\eg, CLIP)\nappears to be a promising scheme for this setting; however, current research in\nthis area is limited, and existing algorithms often focus solely on patch-level\nprompts or confine themselves to language prompts. This paper proposes a\nmulti-instance prompt learning framework enhanced with pathology knowledge,\n\\ie, integrating visual and textual prior knowledge into prompts at both patch\nand slide levels. The training process employs a combination of static and\nlearnable prompts, effectively guiding the activation of pre-trained models and\nfurther facilitating the diagnosis of key pathology patterns. Lightweight\nMessenger (self-attention) and Summary (attention-pooling) layers are\nintroduced to model relationships between patches and slides within the same\npatient data. Additionally, alignment-wise contrastive losses ensure the\nfeature-level alignment between visual and textual learnable prompts for both\npatches and slides. Our method demonstrates superior performance in three\nchallenging clinical tasks, significantly outperforming comparative few-shot\nmethods.\n", "link": "http://arxiv.org/abs/2407.10814v1", "date": "2024-07-15", "relevancy": 2.1046, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5492}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5119}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pathology-knowledge%20Enhanced%20Multi-instance%20Prompt%20Learning%20for%20Few-shot%0A%20%20Whole%20Slide%20Image%20Classification&body=Title%3A%20Pathology-knowledge%20Enhanced%20Multi-instance%20Prompt%20Learning%20for%20Few-shot%0A%20%20Whole%20Slide%20Image%20Classification%0AAuthor%3A%20Linhao%20Qu%20and%20Dingkang%20Yang%20and%20Dan%20Huang%20and%20Qinhao%20Guo%20and%20Rongkui%20Luo%20and%20Shaoting%20Zhang%20and%20Xiaosong%20Wang%0AAbstract%3A%20%20%20Current%20multi-instance%20learning%20algorithms%20for%20pathology%20image%20analysis%20often%0Arequire%20a%20substantial%20number%20of%20Whole%20Slide%20Images%20for%20effective%20training%20but%0Aexhibit%20suboptimal%20performance%20in%20scenarios%20with%20limited%20learning%20data.%20In%0Aclinical%20settings%2C%20restricted%20access%20to%20pathology%20slides%20is%20inevitable%20due%20to%0Apatient%20privacy%20concerns%20and%20the%20prevalence%20of%20rare%20or%20emerging%20diseases.%20The%0Aemergence%20of%20the%20Few-shot%20Weakly%20Supervised%20WSI%20Classification%20accommodates%20the%0Asignificant%20challenge%20of%20the%20limited%20slide%20data%20and%20sparse%20slide-level%20labels%0Afor%20diagnosis.%20Prompt%20learning%20based%20on%20the%20pre-trained%20models%20%28%5Ceg%2C%20CLIP%29%0Aappears%20to%20be%20a%20promising%20scheme%20for%20this%20setting%3B%20however%2C%20current%20research%20in%0Athis%20area%20is%20limited%2C%20and%20existing%20algorithms%20often%20focus%20solely%20on%20patch-level%0Aprompts%20or%20confine%20themselves%20to%20language%20prompts.%20This%20paper%20proposes%20a%0Amulti-instance%20prompt%20learning%20framework%20enhanced%20with%20pathology%20knowledge%2C%0A%5Cie%2C%20integrating%20visual%20and%20textual%20prior%20knowledge%20into%20prompts%20at%20both%20patch%0Aand%20slide%20levels.%20The%20training%20process%20employs%20a%20combination%20of%20static%20and%0Alearnable%20prompts%2C%20effectively%20guiding%20the%20activation%20of%20pre-trained%20models%20and%0Afurther%20facilitating%20the%20diagnosis%20of%20key%20pathology%20patterns.%20Lightweight%0AMessenger%20%28self-attention%29%20and%20Summary%20%28attention-pooling%29%20layers%20are%0Aintroduced%20to%20model%20relationships%20between%20patches%20and%20slides%20within%20the%20same%0Apatient%20data.%20Additionally%2C%20alignment-wise%20contrastive%20losses%20ensure%20the%0Afeature-level%20alignment%20between%20visual%20and%20textual%20learnable%20prompts%20for%20both%0Apatches%20and%20slides.%20Our%20method%20demonstrates%20superior%20performance%20in%20three%0Achallenging%20clinical%20tasks%2C%20significantly%20outperforming%20comparative%20few-shot%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathology-knowledge%2520Enhanced%2520Multi-instance%2520Prompt%2520Learning%2520for%2520Few-shot%250A%2520%2520Whole%2520Slide%2520Image%2520Classification%26entry.906535625%3DLinhao%2520Qu%2520and%2520Dingkang%2520Yang%2520and%2520Dan%2520Huang%2520and%2520Qinhao%2520Guo%2520and%2520Rongkui%2520Luo%2520and%2520Shaoting%2520Zhang%2520and%2520Xiaosong%2520Wang%26entry.1292438233%3D%2520%2520Current%2520multi-instance%2520learning%2520algorithms%2520for%2520pathology%2520image%2520analysis%2520often%250Arequire%2520a%2520substantial%2520number%2520of%2520Whole%2520Slide%2520Images%2520for%2520effective%2520training%2520but%250Aexhibit%2520suboptimal%2520performance%2520in%2520scenarios%2520with%2520limited%2520learning%2520data.%2520In%250Aclinical%2520settings%252C%2520restricted%2520access%2520to%2520pathology%2520slides%2520is%2520inevitable%2520due%2520to%250Apatient%2520privacy%2520concerns%2520and%2520the%2520prevalence%2520of%2520rare%2520or%2520emerging%2520diseases.%2520The%250Aemergence%2520of%2520the%2520Few-shot%2520Weakly%2520Supervised%2520WSI%2520Classification%2520accommodates%2520the%250Asignificant%2520challenge%2520of%2520the%2520limited%2520slide%2520data%2520and%2520sparse%2520slide-level%2520labels%250Afor%2520diagnosis.%2520Prompt%2520learning%2520based%2520on%2520the%2520pre-trained%2520models%2520%2528%255Ceg%252C%2520CLIP%2529%250Aappears%2520to%2520be%2520a%2520promising%2520scheme%2520for%2520this%2520setting%253B%2520however%252C%2520current%2520research%2520in%250Athis%2520area%2520is%2520limited%252C%2520and%2520existing%2520algorithms%2520often%2520focus%2520solely%2520on%2520patch-level%250Aprompts%2520or%2520confine%2520themselves%2520to%2520language%2520prompts.%2520This%2520paper%2520proposes%2520a%250Amulti-instance%2520prompt%2520learning%2520framework%2520enhanced%2520with%2520pathology%2520knowledge%252C%250A%255Cie%252C%2520integrating%2520visual%2520and%2520textual%2520prior%2520knowledge%2520into%2520prompts%2520at%2520both%2520patch%250Aand%2520slide%2520levels.%2520The%2520training%2520process%2520employs%2520a%2520combination%2520of%2520static%2520and%250Alearnable%2520prompts%252C%2520effectively%2520guiding%2520the%2520activation%2520of%2520pre-trained%2520models%2520and%250Afurther%2520facilitating%2520the%2520diagnosis%2520of%2520key%2520pathology%2520patterns.%2520Lightweight%250AMessenger%2520%2528self-attention%2529%2520and%2520Summary%2520%2528attention-pooling%2529%2520layers%2520are%250Aintroduced%2520to%2520model%2520relationships%2520between%2520patches%2520and%2520slides%2520within%2520the%2520same%250Apatient%2520data.%2520Additionally%252C%2520alignment-wise%2520contrastive%2520losses%2520ensure%2520the%250Afeature-level%2520alignment%2520between%2520visual%2520and%2520textual%2520learnable%2520prompts%2520for%2520both%250Apatches%2520and%2520slides.%2520Our%2520method%2520demonstrates%2520superior%2520performance%2520in%2520three%250Achallenging%2520clinical%2520tasks%252C%2520significantly%2520outperforming%2520comparative%2520few-shot%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pathology-knowledge%20Enhanced%20Multi-instance%20Prompt%20Learning%20for%20Few-shot%0A%20%20Whole%20Slide%20Image%20Classification&entry.906535625=Linhao%20Qu%20and%20Dingkang%20Yang%20and%20Dan%20Huang%20and%20Qinhao%20Guo%20and%20Rongkui%20Luo%20and%20Shaoting%20Zhang%20and%20Xiaosong%20Wang&entry.1292438233=%20%20Current%20multi-instance%20learning%20algorithms%20for%20pathology%20image%20analysis%20often%0Arequire%20a%20substantial%20number%20of%20Whole%20Slide%20Images%20for%20effective%20training%20but%0Aexhibit%20suboptimal%20performance%20in%20scenarios%20with%20limited%20learning%20data.%20In%0Aclinical%20settings%2C%20restricted%20access%20to%20pathology%20slides%20is%20inevitable%20due%20to%0Apatient%20privacy%20concerns%20and%20the%20prevalence%20of%20rare%20or%20emerging%20diseases.%20The%0Aemergence%20of%20the%20Few-shot%20Weakly%20Supervised%20WSI%20Classification%20accommodates%20the%0Asignificant%20challenge%20of%20the%20limited%20slide%20data%20and%20sparse%20slide-level%20labels%0Afor%20diagnosis.%20Prompt%20learning%20based%20on%20the%20pre-trained%20models%20%28%5Ceg%2C%20CLIP%29%0Aappears%20to%20be%20a%20promising%20scheme%20for%20this%20setting%3B%20however%2C%20current%20research%20in%0Athis%20area%20is%20limited%2C%20and%20existing%20algorithms%20often%20focus%20solely%20on%20patch-level%0Aprompts%20or%20confine%20themselves%20to%20language%20prompts.%20This%20paper%20proposes%20a%0Amulti-instance%20prompt%20learning%20framework%20enhanced%20with%20pathology%20knowledge%2C%0A%5Cie%2C%20integrating%20visual%20and%20textual%20prior%20knowledge%20into%20prompts%20at%20both%20patch%0Aand%20slide%20levels.%20The%20training%20process%20employs%20a%20combination%20of%20static%20and%0Alearnable%20prompts%2C%20effectively%20guiding%20the%20activation%20of%20pre-trained%20models%20and%0Afurther%20facilitating%20the%20diagnosis%20of%20key%20pathology%20patterns.%20Lightweight%0AMessenger%20%28self-attention%29%20and%20Summary%20%28attention-pooling%29%20layers%20are%0Aintroduced%20to%20model%20relationships%20between%20patches%20and%20slides%20within%20the%20same%0Apatient%20data.%20Additionally%2C%20alignment-wise%20contrastive%20losses%20ensure%20the%0Afeature-level%20alignment%20between%20visual%20and%20textual%20learnable%20prompts%20for%20both%0Apatches%20and%20slides.%20Our%20method%20demonstrates%20superior%20performance%20in%20three%0Achallenging%20clinical%20tasks%2C%20significantly%20outperforming%20comparative%20few-shot%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10814v1&entry.124074799=Read"},
{"title": "Features Reconstruction Disentanglement Cloth-Changing Person\n  Re-Identification", "author": "Zhihao Chen and Yiyuan Ge and Qing Yue", "abstract": "  Cloth-changing person re-identification (CC-ReID) aims to retrieve specific\npedestrians in a cloth-changing scenario. Its main challenge is to disentangle\nthe clothing-related and clothing-unrelated features. Most existing approaches\nforce the model to learn clothing-unrelated features by changing the color of\nthe clothes. However, due to the lack of ground truth, these methods inevitably\nintroduce noise, which destroys the discriminative features and leads to an\nuncontrollable disentanglement process. In this paper, we propose a new person\nre-identification network called features reconstruction disentanglement ReID\n(FRD-ReID), which can controllably decouple the clothing-unrelated and\nclothing-related features. Specifically, we first introduce the human parsing\nmask as the ground truth of the reconstruction process. At the same time, we\npropose the far away attention (FAA) mechanism and the person contour attention\n(PCA) mechanism for clothing-unrelated features and pedestrian contour features\nto improve the feature reconstruction efficiency. In the testing phase, we\ndirectly discard the clothing-related features for inference,which leads to a\ncontrollable disentanglement process. We conducted extensive experiments on the\nPRCC, LTCC, and Vc-Clothes datasets and demonstrated that our method\noutperforms existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.10694v1", "date": "2024-07-15", "relevancy": 2.0804, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5431}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5254}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Features%20Reconstruction%20Disentanglement%20Cloth-Changing%20Person%0A%20%20Re-Identification&body=Title%3A%20Features%20Reconstruction%20Disentanglement%20Cloth-Changing%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Zhihao%20Chen%20and%20Yiyuan%20Ge%20and%20Qing%20Yue%0AAbstract%3A%20%20%20Cloth-changing%20person%20re-identification%20%28CC-ReID%29%20aims%20to%20retrieve%20specific%0Apedestrians%20in%20a%20cloth-changing%20scenario.%20Its%20main%20challenge%20is%20to%20disentangle%0Athe%20clothing-related%20and%20clothing-unrelated%20features.%20Most%20existing%20approaches%0Aforce%20the%20model%20to%20learn%20clothing-unrelated%20features%20by%20changing%20the%20color%20of%0Athe%20clothes.%20However%2C%20due%20to%20the%20lack%20of%20ground%20truth%2C%20these%20methods%20inevitably%0Aintroduce%20noise%2C%20which%20destroys%20the%20discriminative%20features%20and%20leads%20to%20an%0Auncontrollable%20disentanglement%20process.%20In%20this%20paper%2C%20we%20propose%20a%20new%20person%0Are-identification%20network%20called%20features%20reconstruction%20disentanglement%20ReID%0A%28FRD-ReID%29%2C%20which%20can%20controllably%20decouple%20the%20clothing-unrelated%20and%0Aclothing-related%20features.%20Specifically%2C%20we%20first%20introduce%20the%20human%20parsing%0Amask%20as%20the%20ground%20truth%20of%20the%20reconstruction%20process.%20At%20the%20same%20time%2C%20we%0Apropose%20the%20far%20away%20attention%20%28FAA%29%20mechanism%20and%20the%20person%20contour%20attention%0A%28PCA%29%20mechanism%20for%20clothing-unrelated%20features%20and%20pedestrian%20contour%20features%0Ato%20improve%20the%20feature%20reconstruction%20efficiency.%20In%20the%20testing%20phase%2C%20we%0Adirectly%20discard%20the%20clothing-related%20features%20for%20inference%2Cwhich%20leads%20to%20a%0Acontrollable%20disentanglement%20process.%20We%20conducted%20extensive%20experiments%20on%20the%0APRCC%2C%20LTCC%2C%20and%20Vc-Clothes%20datasets%20and%20demonstrated%20that%20our%20method%0Aoutperforms%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeatures%2520Reconstruction%2520Disentanglement%2520Cloth-Changing%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DZhihao%2520Chen%2520and%2520Yiyuan%2520Ge%2520and%2520Qing%2520Yue%26entry.1292438233%3D%2520%2520Cloth-changing%2520person%2520re-identification%2520%2528CC-ReID%2529%2520aims%2520to%2520retrieve%2520specific%250Apedestrians%2520in%2520a%2520cloth-changing%2520scenario.%2520Its%2520main%2520challenge%2520is%2520to%2520disentangle%250Athe%2520clothing-related%2520and%2520clothing-unrelated%2520features.%2520Most%2520existing%2520approaches%250Aforce%2520the%2520model%2520to%2520learn%2520clothing-unrelated%2520features%2520by%2520changing%2520the%2520color%2520of%250Athe%2520clothes.%2520However%252C%2520due%2520to%2520the%2520lack%2520of%2520ground%2520truth%252C%2520these%2520methods%2520inevitably%250Aintroduce%2520noise%252C%2520which%2520destroys%2520the%2520discriminative%2520features%2520and%2520leads%2520to%2520an%250Auncontrollable%2520disentanglement%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520person%250Are-identification%2520network%2520called%2520features%2520reconstruction%2520disentanglement%2520ReID%250A%2528FRD-ReID%2529%252C%2520which%2520can%2520controllably%2520decouple%2520the%2520clothing-unrelated%2520and%250Aclothing-related%2520features.%2520Specifically%252C%2520we%2520first%2520introduce%2520the%2520human%2520parsing%250Amask%2520as%2520the%2520ground%2520truth%2520of%2520the%2520reconstruction%2520process.%2520At%2520the%2520same%2520time%252C%2520we%250Apropose%2520the%2520far%2520away%2520attention%2520%2528FAA%2529%2520mechanism%2520and%2520the%2520person%2520contour%2520attention%250A%2528PCA%2529%2520mechanism%2520for%2520clothing-unrelated%2520features%2520and%2520pedestrian%2520contour%2520features%250Ato%2520improve%2520the%2520feature%2520reconstruction%2520efficiency.%2520In%2520the%2520testing%2520phase%252C%2520we%250Adirectly%2520discard%2520the%2520clothing-related%2520features%2520for%2520inference%252Cwhich%2520leads%2520to%2520a%250Acontrollable%2520disentanglement%2520process.%2520We%2520conducted%2520extensive%2520experiments%2520on%2520the%250APRCC%252C%2520LTCC%252C%2520and%2520Vc-Clothes%2520datasets%2520and%2520demonstrated%2520that%2520our%2520method%250Aoutperforms%2520existing%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Features%20Reconstruction%20Disentanglement%20Cloth-Changing%20Person%0A%20%20Re-Identification&entry.906535625=Zhihao%20Chen%20and%20Yiyuan%20Ge%20and%20Qing%20Yue&entry.1292438233=%20%20Cloth-changing%20person%20re-identification%20%28CC-ReID%29%20aims%20to%20retrieve%20specific%0Apedestrians%20in%20a%20cloth-changing%20scenario.%20Its%20main%20challenge%20is%20to%20disentangle%0Athe%20clothing-related%20and%20clothing-unrelated%20features.%20Most%20existing%20approaches%0Aforce%20the%20model%20to%20learn%20clothing-unrelated%20features%20by%20changing%20the%20color%20of%0Athe%20clothes.%20However%2C%20due%20to%20the%20lack%20of%20ground%20truth%2C%20these%20methods%20inevitably%0Aintroduce%20noise%2C%20which%20destroys%20the%20discriminative%20features%20and%20leads%20to%20an%0Auncontrollable%20disentanglement%20process.%20In%20this%20paper%2C%20we%20propose%20a%20new%20person%0Are-identification%20network%20called%20features%20reconstruction%20disentanglement%20ReID%0A%28FRD-ReID%29%2C%20which%20can%20controllably%20decouple%20the%20clothing-unrelated%20and%0Aclothing-related%20features.%20Specifically%2C%20we%20first%20introduce%20the%20human%20parsing%0Amask%20as%20the%20ground%20truth%20of%20the%20reconstruction%20process.%20At%20the%20same%20time%2C%20we%0Apropose%20the%20far%20away%20attention%20%28FAA%29%20mechanism%20and%20the%20person%20contour%20attention%0A%28PCA%29%20mechanism%20for%20clothing-unrelated%20features%20and%20pedestrian%20contour%20features%0Ato%20improve%20the%20feature%20reconstruction%20efficiency.%20In%20the%20testing%20phase%2C%20we%0Adirectly%20discard%20the%20clothing-related%20features%20for%20inference%2Cwhich%20leads%20to%20a%0Acontrollable%20disentanglement%20process.%20We%20conducted%20extensive%20experiments%20on%20the%0APRCC%2C%20LTCC%2C%20and%20Vc-Clothes%20datasets%20and%20demonstrated%20that%20our%20method%0Aoutperforms%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10694v1&entry.124074799=Read"},
{"title": "Trajectory Tracking for Unmanned Aerial Vehicles in 3D Spaces under\n  Motion Constraints", "author": "Saurabh Kumar and Shashi Ranjan Kumar and Abhinav Sinha", "abstract": "  This article presents a three-dimensional nonlinear trajectory tracking\ncontrol strategy for unmanned aerial vehicles (UAVs) in the presence of spatial\nconstraints. As opposed to many existing control strategies, which do not\nconsider spatial constraints, the proposed strategy considers spatial\nconstraints on each degree of freedom movement of the UAV. Such consideration\nmakes the design appealing for many practical applications, such as pipeline\ninspection, boundary tracking, etc. The proposed design accounts for the\nlimited information about the inertia matrix, thereby affirming its inherent\nrobustness against unmodeled dynamics and other imperfections. We rigorously\nshow that the UAV will converge to its desired path by maintaining bounded\nposition, orientation, and linear and angular speeds. Finally, we demonstrate\nthe effectiveness of the proposed strategy through various numerical\nsimulations.\n", "link": "http://arxiv.org/abs/2407.10837v1", "date": "2024-07-15", "relevancy": 2.0793, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5396}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory%20Tracking%20for%20Unmanned%20Aerial%20Vehicles%20in%203D%20Spaces%20under%0A%20%20Motion%20Constraints&body=Title%3A%20Trajectory%20Tracking%20for%20Unmanned%20Aerial%20Vehicles%20in%203D%20Spaces%20under%0A%20%20Motion%20Constraints%0AAuthor%3A%20Saurabh%20Kumar%20and%20Shashi%20Ranjan%20Kumar%20and%20Abhinav%20Sinha%0AAbstract%3A%20%20%20This%20article%20presents%20a%20three-dimensional%20nonlinear%20trajectory%20tracking%0Acontrol%20strategy%20for%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20the%20presence%20of%20spatial%0Aconstraints.%20As%20opposed%20to%20many%20existing%20control%20strategies%2C%20which%20do%20not%0Aconsider%20spatial%20constraints%2C%20the%20proposed%20strategy%20considers%20spatial%0Aconstraints%20on%20each%20degree%20of%20freedom%20movement%20of%20the%20UAV.%20Such%20consideration%0Amakes%20the%20design%20appealing%20for%20many%20practical%20applications%2C%20such%20as%20pipeline%0Ainspection%2C%20boundary%20tracking%2C%20etc.%20The%20proposed%20design%20accounts%20for%20the%0Alimited%20information%20about%20the%20inertia%20matrix%2C%20thereby%20affirming%20its%20inherent%0Arobustness%20against%20unmodeled%20dynamics%20and%20other%20imperfections.%20We%20rigorously%0Ashow%20that%20the%20UAV%20will%20converge%20to%20its%20desired%20path%20by%20maintaining%20bounded%0Aposition%2C%20orientation%2C%20and%20linear%20and%20angular%20speeds.%20Finally%2C%20we%20demonstrate%0Athe%20effectiveness%20of%20the%20proposed%20strategy%20through%20various%20numerical%0Asimulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory%2520Tracking%2520for%2520Unmanned%2520Aerial%2520Vehicles%2520in%25203D%2520Spaces%2520under%250A%2520%2520Motion%2520Constraints%26entry.906535625%3DSaurabh%2520Kumar%2520and%2520Shashi%2520Ranjan%2520Kumar%2520and%2520Abhinav%2520Sinha%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520a%2520three-dimensional%2520nonlinear%2520trajectory%2520tracking%250Acontrol%2520strategy%2520for%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520in%2520the%2520presence%2520of%2520spatial%250Aconstraints.%2520As%2520opposed%2520to%2520many%2520existing%2520control%2520strategies%252C%2520which%2520do%2520not%250Aconsider%2520spatial%2520constraints%252C%2520the%2520proposed%2520strategy%2520considers%2520spatial%250Aconstraints%2520on%2520each%2520degree%2520of%2520freedom%2520movement%2520of%2520the%2520UAV.%2520Such%2520consideration%250Amakes%2520the%2520design%2520appealing%2520for%2520many%2520practical%2520applications%252C%2520such%2520as%2520pipeline%250Ainspection%252C%2520boundary%2520tracking%252C%2520etc.%2520The%2520proposed%2520design%2520accounts%2520for%2520the%250Alimited%2520information%2520about%2520the%2520inertia%2520matrix%252C%2520thereby%2520affirming%2520its%2520inherent%250Arobustness%2520against%2520unmodeled%2520dynamics%2520and%2520other%2520imperfections.%2520We%2520rigorously%250Ashow%2520that%2520the%2520UAV%2520will%2520converge%2520to%2520its%2520desired%2520path%2520by%2520maintaining%2520bounded%250Aposition%252C%2520orientation%252C%2520and%2520linear%2520and%2520angular%2520speeds.%2520Finally%252C%2520we%2520demonstrate%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520strategy%2520through%2520various%2520numerical%250Asimulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Tracking%20for%20Unmanned%20Aerial%20Vehicles%20in%203D%20Spaces%20under%0A%20%20Motion%20Constraints&entry.906535625=Saurabh%20Kumar%20and%20Shashi%20Ranjan%20Kumar%20and%20Abhinav%20Sinha&entry.1292438233=%20%20This%20article%20presents%20a%20three-dimensional%20nonlinear%20trajectory%20tracking%0Acontrol%20strategy%20for%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20the%20presence%20of%20spatial%0Aconstraints.%20As%20opposed%20to%20many%20existing%20control%20strategies%2C%20which%20do%20not%0Aconsider%20spatial%20constraints%2C%20the%20proposed%20strategy%20considers%20spatial%0Aconstraints%20on%20each%20degree%20of%20freedom%20movement%20of%20the%20UAV.%20Such%20consideration%0Amakes%20the%20design%20appealing%20for%20many%20practical%20applications%2C%20such%20as%20pipeline%0Ainspection%2C%20boundary%20tracking%2C%20etc.%20The%20proposed%20design%20accounts%20for%20the%0Alimited%20information%20about%20the%20inertia%20matrix%2C%20thereby%20affirming%20its%20inherent%0Arobustness%20against%20unmodeled%20dynamics%20and%20other%20imperfections.%20We%20rigorously%0Ashow%20that%20the%20UAV%20will%20converge%20to%20its%20desired%20path%20by%20maintaining%20bounded%0Aposition%2C%20orientation%2C%20and%20linear%20and%20angular%20speeds.%20Finally%2C%20we%20demonstrate%0Athe%20effectiveness%20of%20the%20proposed%20strategy%20through%20various%20numerical%0Asimulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10837v1&entry.124074799=Read"},
{"title": "Tailoring Solution Accuracy for Fast Whole-body Model Predictive Control\n  of Legged Robots", "author": "Charles Khazoom and Seungwoo Hong and Matthew Chignoli and Elijah Stanger-Jones and Sangbae Kim", "abstract": "  Thanks to recent advancements in accelerating non-linear model predictive\ncontrol (NMPC), it is now feasible to deploy whole-body NMPC at real-time rates\nfor humanoid robots. However, enforcing inequality constraints in real time for\nsuch high-dimensional systems remains challenging due to the need for\nadditional iterations. This paper presents an implementation of whole-body NMPC\nfor legged robots that provides low-accuracy solutions to NMPC with general\nequality and inequality constraints. Instead of aiming for highly accurate\noptimal solutions, we leverage the alternating direction method of multipliers\nto rapidly provide low-accuracy solutions to quadratic programming subproblems.\nOur extensive simulation results indicate that real robots often cannot benefit\nfrom highly accurate solutions due to dynamics discretization errors, inertial\nmodeling errors and delays. We incorporate control barrier functions (CBFs) at\nthe initial timestep of the NMPC for the self-collision constraints, resulting\nin up to a 26-fold reduction in the number of self-collisions without adding\ncomputational burden. The controller is reliably deployed on hardware at 90 Hz\nfor a problem involving 32 timesteps, 2004 variables, and 3768 constraints. The\nNMPC delivers sufficiently accurate solutions, enabling the MIT Humanoid to\nplan complex crossed-leg and arm motions that enhance stability when walking\nand recovering from significant disturbances.\n", "link": "http://arxiv.org/abs/2407.10789v1", "date": "2024-07-15", "relevancy": 2.0787, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5806}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5303}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailoring%20Solution%20Accuracy%20for%20Fast%20Whole-body%20Model%20Predictive%20Control%0A%20%20of%20Legged%20Robots&body=Title%3A%20Tailoring%20Solution%20Accuracy%20for%20Fast%20Whole-body%20Model%20Predictive%20Control%0A%20%20of%20Legged%20Robots%0AAuthor%3A%20Charles%20Khazoom%20and%20Seungwoo%20Hong%20and%20Matthew%20Chignoli%20and%20Elijah%20Stanger-Jones%20and%20Sangbae%20Kim%0AAbstract%3A%20%20%20Thanks%20to%20recent%20advancements%20in%20accelerating%20non-linear%20model%20predictive%0Acontrol%20%28NMPC%29%2C%20it%20is%20now%20feasible%20to%20deploy%20whole-body%20NMPC%20at%20real-time%20rates%0Afor%20humanoid%20robots.%20However%2C%20enforcing%20inequality%20constraints%20in%20real%20time%20for%0Asuch%20high-dimensional%20systems%20remains%20challenging%20due%20to%20the%20need%20for%0Aadditional%20iterations.%20This%20paper%20presents%20an%20implementation%20of%20whole-body%20NMPC%0Afor%20legged%20robots%20that%20provides%20low-accuracy%20solutions%20to%20NMPC%20with%20general%0Aequality%20and%20inequality%20constraints.%20Instead%20of%20aiming%20for%20highly%20accurate%0Aoptimal%20solutions%2C%20we%20leverage%20the%20alternating%20direction%20method%20of%20multipliers%0Ato%20rapidly%20provide%20low-accuracy%20solutions%20to%20quadratic%20programming%20subproblems.%0AOur%20extensive%20simulation%20results%20indicate%20that%20real%20robots%20often%20cannot%20benefit%0Afrom%20highly%20accurate%20solutions%20due%20to%20dynamics%20discretization%20errors%2C%20inertial%0Amodeling%20errors%20and%20delays.%20We%20incorporate%20control%20barrier%20functions%20%28CBFs%29%20at%0Athe%20initial%20timestep%20of%20the%20NMPC%20for%20the%20self-collision%20constraints%2C%20resulting%0Ain%20up%20to%20a%2026-fold%20reduction%20in%20the%20number%20of%20self-collisions%20without%20adding%0Acomputational%20burden.%20The%20controller%20is%20reliably%20deployed%20on%20hardware%20at%2090%20Hz%0Afor%20a%20problem%20involving%2032%20timesteps%2C%202004%20variables%2C%20and%203768%20constraints.%20The%0ANMPC%20delivers%20sufficiently%20accurate%20solutions%2C%20enabling%20the%20MIT%20Humanoid%20to%0Aplan%20complex%20crossed-leg%20and%20arm%20motions%20that%20enhance%20stability%20when%20walking%0Aand%20recovering%20from%20significant%20disturbances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailoring%2520Solution%2520Accuracy%2520for%2520Fast%2520Whole-body%2520Model%2520Predictive%2520Control%250A%2520%2520of%2520Legged%2520Robots%26entry.906535625%3DCharles%2520Khazoom%2520and%2520Seungwoo%2520Hong%2520and%2520Matthew%2520Chignoli%2520and%2520Elijah%2520Stanger-Jones%2520and%2520Sangbae%2520Kim%26entry.1292438233%3D%2520%2520Thanks%2520to%2520recent%2520advancements%2520in%2520accelerating%2520non-linear%2520model%2520predictive%250Acontrol%2520%2528NMPC%2529%252C%2520it%2520is%2520now%2520feasible%2520to%2520deploy%2520whole-body%2520NMPC%2520at%2520real-time%2520rates%250Afor%2520humanoid%2520robots.%2520However%252C%2520enforcing%2520inequality%2520constraints%2520in%2520real%2520time%2520for%250Asuch%2520high-dimensional%2520systems%2520remains%2520challenging%2520due%2520to%2520the%2520need%2520for%250Aadditional%2520iterations.%2520This%2520paper%2520presents%2520an%2520implementation%2520of%2520whole-body%2520NMPC%250Afor%2520legged%2520robots%2520that%2520provides%2520low-accuracy%2520solutions%2520to%2520NMPC%2520with%2520general%250Aequality%2520and%2520inequality%2520constraints.%2520Instead%2520of%2520aiming%2520for%2520highly%2520accurate%250Aoptimal%2520solutions%252C%2520we%2520leverage%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%250Ato%2520rapidly%2520provide%2520low-accuracy%2520solutions%2520to%2520quadratic%2520programming%2520subproblems.%250AOur%2520extensive%2520simulation%2520results%2520indicate%2520that%2520real%2520robots%2520often%2520cannot%2520benefit%250Afrom%2520highly%2520accurate%2520solutions%2520due%2520to%2520dynamics%2520discretization%2520errors%252C%2520inertial%250Amodeling%2520errors%2520and%2520delays.%2520We%2520incorporate%2520control%2520barrier%2520functions%2520%2528CBFs%2529%2520at%250Athe%2520initial%2520timestep%2520of%2520the%2520NMPC%2520for%2520the%2520self-collision%2520constraints%252C%2520resulting%250Ain%2520up%2520to%2520a%252026-fold%2520reduction%2520in%2520the%2520number%2520of%2520self-collisions%2520without%2520adding%250Acomputational%2520burden.%2520The%2520controller%2520is%2520reliably%2520deployed%2520on%2520hardware%2520at%252090%2520Hz%250Afor%2520a%2520problem%2520involving%252032%2520timesteps%252C%25202004%2520variables%252C%2520and%25203768%2520constraints.%2520The%250ANMPC%2520delivers%2520sufficiently%2520accurate%2520solutions%252C%2520enabling%2520the%2520MIT%2520Humanoid%2520to%250Aplan%2520complex%2520crossed-leg%2520and%2520arm%2520motions%2520that%2520enhance%2520stability%2520when%2520walking%250Aand%2520recovering%2520from%2520significant%2520disturbances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailoring%20Solution%20Accuracy%20for%20Fast%20Whole-body%20Model%20Predictive%20Control%0A%20%20of%20Legged%20Robots&entry.906535625=Charles%20Khazoom%20and%20Seungwoo%20Hong%20and%20Matthew%20Chignoli%20and%20Elijah%20Stanger-Jones%20and%20Sangbae%20Kim&entry.1292438233=%20%20Thanks%20to%20recent%20advancements%20in%20accelerating%20non-linear%20model%20predictive%0Acontrol%20%28NMPC%29%2C%20it%20is%20now%20feasible%20to%20deploy%20whole-body%20NMPC%20at%20real-time%20rates%0Afor%20humanoid%20robots.%20However%2C%20enforcing%20inequality%20constraints%20in%20real%20time%20for%0Asuch%20high-dimensional%20systems%20remains%20challenging%20due%20to%20the%20need%20for%0Aadditional%20iterations.%20This%20paper%20presents%20an%20implementation%20of%20whole-body%20NMPC%0Afor%20legged%20robots%20that%20provides%20low-accuracy%20solutions%20to%20NMPC%20with%20general%0Aequality%20and%20inequality%20constraints.%20Instead%20of%20aiming%20for%20highly%20accurate%0Aoptimal%20solutions%2C%20we%20leverage%20the%20alternating%20direction%20method%20of%20multipliers%0Ato%20rapidly%20provide%20low-accuracy%20solutions%20to%20quadratic%20programming%20subproblems.%0AOur%20extensive%20simulation%20results%20indicate%20that%20real%20robots%20often%20cannot%20benefit%0Afrom%20highly%20accurate%20solutions%20due%20to%20dynamics%20discretization%20errors%2C%20inertial%0Amodeling%20errors%20and%20delays.%20We%20incorporate%20control%20barrier%20functions%20%28CBFs%29%20at%0Athe%20initial%20timestep%20of%20the%20NMPC%20for%20the%20self-collision%20constraints%2C%20resulting%0Ain%20up%20to%20a%2026-fold%20reduction%20in%20the%20number%20of%20self-collisions%20without%20adding%0Acomputational%20burden.%20The%20controller%20is%20reliably%20deployed%20on%20hardware%20at%2090%20Hz%0Afor%20a%20problem%20involving%2032%20timesteps%2C%202004%20variables%2C%20and%203768%20constraints.%20The%0ANMPC%20delivers%20sufficiently%20accurate%20solutions%2C%20enabling%20the%20MIT%20Humanoid%20to%0Aplan%20complex%20crossed-leg%20and%20arm%20motions%20that%20enhance%20stability%20when%20walking%0Aand%20recovering%20from%20significant%20disturbances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10789v1&entry.124074799=Read"},
{"title": "LLM Circuit Analyses Are Consistent Across Training and Scale", "author": "Curt Tigges and Michael Hanna and Qinan Yu and Stella Biderman", "abstract": "  Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale.\n", "link": "http://arxiv.org/abs/2407.10827v1", "date": "2024-07-15", "relevancy": 1.8257, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5023}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4618}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Circuit%20Analyses%20Are%20Consistent%20Across%20Training%20and%20Scale&body=Title%3A%20LLM%20Circuit%20Analyses%20Are%20Consistent%20Across%20Training%20and%20Scale%0AAuthor%3A%20Curt%20Tigges%20and%20Michael%20Hanna%20and%20Qinan%20Yu%20and%20Stella%20Biderman%0AAbstract%3A%20%20%20Most%20currently%20deployed%20large%20language%20models%20%28LLMs%29%20undergo%20continuous%0Atraining%20or%20additional%20finetuning.%20By%20contrast%2C%20most%20research%20into%20LLMs%27%0Ainternal%20mechanisms%20focuses%20on%20models%20at%20one%20snapshot%20in%20time%20%28the%20end%20of%0Apre-training%29%2C%20raising%20the%20question%20of%20whether%20their%20results%20generalize%20to%0Areal-world%20settings.%20Existing%20studies%20of%20mechanisms%20over%20time%20focus%20on%0Aencoder-only%20or%20toy%20models%2C%20which%20differ%20significantly%20from%20most%20deployed%0Amodels.%20In%20this%20study%2C%20we%20track%20how%20model%20mechanisms%2C%20operationalized%20as%0Acircuits%2C%20emerge%20and%20evolve%20across%20300%20billion%20tokens%20of%20training%20in%0Adecoder-only%20LLMs%2C%20in%20models%20ranging%20from%2070%20million%20to%202.8%20billion%20parameters.%0AWe%20find%20that%20task%20abilities%20and%20the%20functional%20components%20that%20support%20them%0Aemerge%20consistently%20at%20similar%20token%20counts%20across%20scale.%20Moreover%2C%20although%0Asuch%20components%20may%20be%20implemented%20by%20different%20attention%20heads%20over%20time%2C%20the%0Aoverarching%20algorithm%20that%20they%20implement%20remains.%20Surprisingly%2C%20both%20these%0Aalgorithms%20and%20the%20types%20of%20components%20involved%20therein%20can%20replicate%20across%0Amodel%20scale.%20These%20results%20suggest%20that%20circuit%20analyses%20conducted%20on%20small%0Amodels%20at%20the%20end%20of%20pre-training%20can%20provide%20insights%20that%20still%20apply%20after%0Aadditional%20pre-training%20and%20over%20model%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Circuit%2520Analyses%2520Are%2520Consistent%2520Across%2520Training%2520and%2520Scale%26entry.906535625%3DCurt%2520Tigges%2520and%2520Michael%2520Hanna%2520and%2520Qinan%2520Yu%2520and%2520Stella%2520Biderman%26entry.1292438233%3D%2520%2520Most%2520currently%2520deployed%2520large%2520language%2520models%2520%2528LLMs%2529%2520undergo%2520continuous%250Atraining%2520or%2520additional%2520finetuning.%2520By%2520contrast%252C%2520most%2520research%2520into%2520LLMs%2527%250Ainternal%2520mechanisms%2520focuses%2520on%2520models%2520at%2520one%2520snapshot%2520in%2520time%2520%2528the%2520end%2520of%250Apre-training%2529%252C%2520raising%2520the%2520question%2520of%2520whether%2520their%2520results%2520generalize%2520to%250Areal-world%2520settings.%2520Existing%2520studies%2520of%2520mechanisms%2520over%2520time%2520focus%2520on%250Aencoder-only%2520or%2520toy%2520models%252C%2520which%2520differ%2520significantly%2520from%2520most%2520deployed%250Amodels.%2520In%2520this%2520study%252C%2520we%2520track%2520how%2520model%2520mechanisms%252C%2520operationalized%2520as%250Acircuits%252C%2520emerge%2520and%2520evolve%2520across%2520300%2520billion%2520tokens%2520of%2520training%2520in%250Adecoder-only%2520LLMs%252C%2520in%2520models%2520ranging%2520from%252070%2520million%2520to%25202.8%2520billion%2520parameters.%250AWe%2520find%2520that%2520task%2520abilities%2520and%2520the%2520functional%2520components%2520that%2520support%2520them%250Aemerge%2520consistently%2520at%2520similar%2520token%2520counts%2520across%2520scale.%2520Moreover%252C%2520although%250Asuch%2520components%2520may%2520be%2520implemented%2520by%2520different%2520attention%2520heads%2520over%2520time%252C%2520the%250Aoverarching%2520algorithm%2520that%2520they%2520implement%2520remains.%2520Surprisingly%252C%2520both%2520these%250Aalgorithms%2520and%2520the%2520types%2520of%2520components%2520involved%2520therein%2520can%2520replicate%2520across%250Amodel%2520scale.%2520These%2520results%2520suggest%2520that%2520circuit%2520analyses%2520conducted%2520on%2520small%250Amodels%2520at%2520the%2520end%2520of%2520pre-training%2520can%2520provide%2520insights%2520that%2520still%2520apply%2520after%250Aadditional%2520pre-training%2520and%2520over%2520model%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Circuit%20Analyses%20Are%20Consistent%20Across%20Training%20and%20Scale&entry.906535625=Curt%20Tigges%20and%20Michael%20Hanna%20and%20Qinan%20Yu%20and%20Stella%20Biderman&entry.1292438233=%20%20Most%20currently%20deployed%20large%20language%20models%20%28LLMs%29%20undergo%20continuous%0Atraining%20or%20additional%20finetuning.%20By%20contrast%2C%20most%20research%20into%20LLMs%27%0Ainternal%20mechanisms%20focuses%20on%20models%20at%20one%20snapshot%20in%20time%20%28the%20end%20of%0Apre-training%29%2C%20raising%20the%20question%20of%20whether%20their%20results%20generalize%20to%0Areal-world%20settings.%20Existing%20studies%20of%20mechanisms%20over%20time%20focus%20on%0Aencoder-only%20or%20toy%20models%2C%20which%20differ%20significantly%20from%20most%20deployed%0Amodels.%20In%20this%20study%2C%20we%20track%20how%20model%20mechanisms%2C%20operationalized%20as%0Acircuits%2C%20emerge%20and%20evolve%20across%20300%20billion%20tokens%20of%20training%20in%0Adecoder-only%20LLMs%2C%20in%20models%20ranging%20from%2070%20million%20to%202.8%20billion%20parameters.%0AWe%20find%20that%20task%20abilities%20and%20the%20functional%20components%20that%20support%20them%0Aemerge%20consistently%20at%20similar%20token%20counts%20across%20scale.%20Moreover%2C%20although%0Asuch%20components%20may%20be%20implemented%20by%20different%20attention%20heads%20over%20time%2C%20the%0Aoverarching%20algorithm%20that%20they%20implement%20remains.%20Surprisingly%2C%20both%20these%0Aalgorithms%20and%20the%20types%20of%20components%20involved%20therein%20can%20replicate%20across%0Amodel%20scale.%20These%20results%20suggest%20that%20circuit%20analyses%20conducted%20on%20small%0Amodels%20at%20the%20end%20of%20pre-training%20can%20provide%20insights%20that%20still%20apply%20after%0Aadditional%20pre-training%20and%20over%20model%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10827v1&entry.124074799=Read"},
{"title": "Lifelong Robot Library Learning: Bootstrapping Composable and\n  Generalizable Skills for Embodied Control with Language Models", "author": "Georgios Tziafas and Hamidreza Kasaei", "abstract": "  Large Language Models (LLMs) have emerged as a new paradigm for embodied\nreasoning and control, most recently by generating robot policy code that\nutilizes a custom library of vision and control primitive skills. However,\nprior arts fix their skills library and steer the LLM with carefully\nhand-crafted prompt engineering, limiting the agent to a stationary range of\naddressable tasks. In this work, we introduce LRLL, an LLM-based lifelong\nlearning agent that continuously grows the robot skill library to tackle\nmanipulation tasks of ever-growing complexity. LRLL achieves this with four\nnovel contributions: 1) a soft memory module that allows dynamic storage and\nretrieval of past experiences to serve as context, 2) a self-guided exploration\npolicy that proposes new tasks in simulation, 3) a skill abstractor that\ndistills recent experiences into new library skills, and 4) a lifelong learning\nalgorithm for enabling human users to bootstrap new skills with minimal online\ninteraction. LRLL continuously transfers knowledge from the memory to the\nlibrary, building composable, general and interpretable policies, while\nbypassing gradient-based optimization, thus relieving the learner from\ncatastrophic forgetting. Empirical evaluation in a simulated tabletop\nenvironment shows that LRLL outperforms end-to-end and vanilla LLM approaches\nin the lifelong setup while learning skills that are transferable to the real\nworld. Project material will become available at the webpage\nhttps://gtziafas.github.io/LRLL_project.\n", "link": "http://arxiv.org/abs/2406.18746v2", "date": "2024-07-15", "relevancy": 1.6942, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.61}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5721}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lifelong%20Robot%20Library%20Learning%3A%20Bootstrapping%20Composable%20and%0A%20%20Generalizable%20Skills%20for%20Embodied%20Control%20with%20Language%20Models&body=Title%3A%20Lifelong%20Robot%20Library%20Learning%3A%20Bootstrapping%20Composable%20and%0A%20%20Generalizable%20Skills%20for%20Embodied%20Control%20with%20Language%20Models%0AAuthor%3A%20Georgios%20Tziafas%20and%20Hamidreza%20Kasaei%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%20new%20paradigm%20for%20embodied%0Areasoning%20and%20control%2C%20most%20recently%20by%20generating%20robot%20policy%20code%20that%0Autilizes%20a%20custom%20library%20of%20vision%20and%20control%20primitive%20skills.%20However%2C%0Aprior%20arts%20fix%20their%20skills%20library%20and%20steer%20the%20LLM%20with%20carefully%0Ahand-crafted%20prompt%20engineering%2C%20limiting%20the%20agent%20to%20a%20stationary%20range%20of%0Aaddressable%20tasks.%20In%20this%20work%2C%20we%20introduce%20LRLL%2C%20an%20LLM-based%20lifelong%0Alearning%20agent%20that%20continuously%20grows%20the%20robot%20skill%20library%20to%20tackle%0Amanipulation%20tasks%20of%20ever-growing%20complexity.%20LRLL%20achieves%20this%20with%20four%0Anovel%20contributions%3A%201%29%20a%20soft%20memory%20module%20that%20allows%20dynamic%20storage%20and%0Aretrieval%20of%20past%20experiences%20to%20serve%20as%20context%2C%202%29%20a%20self-guided%20exploration%0Apolicy%20that%20proposes%20new%20tasks%20in%20simulation%2C%203%29%20a%20skill%20abstractor%20that%0Adistills%20recent%20experiences%20into%20new%20library%20skills%2C%20and%204%29%20a%20lifelong%20learning%0Aalgorithm%20for%20enabling%20human%20users%20to%20bootstrap%20new%20skills%20with%20minimal%20online%0Ainteraction.%20LRLL%20continuously%20transfers%20knowledge%20from%20the%20memory%20to%20the%0Alibrary%2C%20building%20composable%2C%20general%20and%20interpretable%20policies%2C%20while%0Abypassing%20gradient-based%20optimization%2C%20thus%20relieving%20the%20learner%20from%0Acatastrophic%20forgetting.%20Empirical%20evaluation%20in%20a%20simulated%20tabletop%0Aenvironment%20shows%20that%20LRLL%20outperforms%20end-to-end%20and%20vanilla%20LLM%20approaches%0Ain%20the%20lifelong%20setup%20while%20learning%20skills%20that%20are%20transferable%20to%20the%20real%0Aworld.%20Project%20material%20will%20become%20available%20at%20the%20webpage%0Ahttps%3A//gtziafas.github.io/LRLL_project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLifelong%2520Robot%2520Library%2520Learning%253A%2520Bootstrapping%2520Composable%2520and%250A%2520%2520Generalizable%2520Skills%2520for%2520Embodied%2520Control%2520with%2520Language%2520Models%26entry.906535625%3DGeorgios%2520Tziafas%2520and%2520Hamidreza%2520Kasaei%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520a%2520new%2520paradigm%2520for%2520embodied%250Areasoning%2520and%2520control%252C%2520most%2520recently%2520by%2520generating%2520robot%2520policy%2520code%2520that%250Autilizes%2520a%2520custom%2520library%2520of%2520vision%2520and%2520control%2520primitive%2520skills.%2520However%252C%250Aprior%2520arts%2520fix%2520their%2520skills%2520library%2520and%2520steer%2520the%2520LLM%2520with%2520carefully%250Ahand-crafted%2520prompt%2520engineering%252C%2520limiting%2520the%2520agent%2520to%2520a%2520stationary%2520range%2520of%250Aaddressable%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LRLL%252C%2520an%2520LLM-based%2520lifelong%250Alearning%2520agent%2520that%2520continuously%2520grows%2520the%2520robot%2520skill%2520library%2520to%2520tackle%250Amanipulation%2520tasks%2520of%2520ever-growing%2520complexity.%2520LRLL%2520achieves%2520this%2520with%2520four%250Anovel%2520contributions%253A%25201%2529%2520a%2520soft%2520memory%2520module%2520that%2520allows%2520dynamic%2520storage%2520and%250Aretrieval%2520of%2520past%2520experiences%2520to%2520serve%2520as%2520context%252C%25202%2529%2520a%2520self-guided%2520exploration%250Apolicy%2520that%2520proposes%2520new%2520tasks%2520in%2520simulation%252C%25203%2529%2520a%2520skill%2520abstractor%2520that%250Adistills%2520recent%2520experiences%2520into%2520new%2520library%2520skills%252C%2520and%25204%2529%2520a%2520lifelong%2520learning%250Aalgorithm%2520for%2520enabling%2520human%2520users%2520to%2520bootstrap%2520new%2520skills%2520with%2520minimal%2520online%250Ainteraction.%2520LRLL%2520continuously%2520transfers%2520knowledge%2520from%2520the%2520memory%2520to%2520the%250Alibrary%252C%2520building%2520composable%252C%2520general%2520and%2520interpretable%2520policies%252C%2520while%250Abypassing%2520gradient-based%2520optimization%252C%2520thus%2520relieving%2520the%2520learner%2520from%250Acatastrophic%2520forgetting.%2520Empirical%2520evaluation%2520in%2520a%2520simulated%2520tabletop%250Aenvironment%2520shows%2520that%2520LRLL%2520outperforms%2520end-to-end%2520and%2520vanilla%2520LLM%2520approaches%250Ain%2520the%2520lifelong%2520setup%2520while%2520learning%2520skills%2520that%2520are%2520transferable%2520to%2520the%2520real%250Aworld.%2520Project%2520material%2520will%2520become%2520available%2520at%2520the%2520webpage%250Ahttps%253A//gtziafas.github.io/LRLL_project.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lifelong%20Robot%20Library%20Learning%3A%20Bootstrapping%20Composable%20and%0A%20%20Generalizable%20Skills%20for%20Embodied%20Control%20with%20Language%20Models&entry.906535625=Georgios%20Tziafas%20and%20Hamidreza%20Kasaei&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%20new%20paradigm%20for%20embodied%0Areasoning%20and%20control%2C%20most%20recently%20by%20generating%20robot%20policy%20code%20that%0Autilizes%20a%20custom%20library%20of%20vision%20and%20control%20primitive%20skills.%20However%2C%0Aprior%20arts%20fix%20their%20skills%20library%20and%20steer%20the%20LLM%20with%20carefully%0Ahand-crafted%20prompt%20engineering%2C%20limiting%20the%20agent%20to%20a%20stationary%20range%20of%0Aaddressable%20tasks.%20In%20this%20work%2C%20we%20introduce%20LRLL%2C%20an%20LLM-based%20lifelong%0Alearning%20agent%20that%20continuously%20grows%20the%20robot%20skill%20library%20to%20tackle%0Amanipulation%20tasks%20of%20ever-growing%20complexity.%20LRLL%20achieves%20this%20with%20four%0Anovel%20contributions%3A%201%29%20a%20soft%20memory%20module%20that%20allows%20dynamic%20storage%20and%0Aretrieval%20of%20past%20experiences%20to%20serve%20as%20context%2C%202%29%20a%20self-guided%20exploration%0Apolicy%20that%20proposes%20new%20tasks%20in%20simulation%2C%203%29%20a%20skill%20abstractor%20that%0Adistills%20recent%20experiences%20into%20new%20library%20skills%2C%20and%204%29%20a%20lifelong%20learning%0Aalgorithm%20for%20enabling%20human%20users%20to%20bootstrap%20new%20skills%20with%20minimal%20online%0Ainteraction.%20LRLL%20continuously%20transfers%20knowledge%20from%20the%20memory%20to%20the%0Alibrary%2C%20building%20composable%2C%20general%20and%20interpretable%20policies%2C%20while%0Abypassing%20gradient-based%20optimization%2C%20thus%20relieving%20the%20learner%20from%0Acatastrophic%20forgetting.%20Empirical%20evaluation%20in%20a%20simulated%20tabletop%0Aenvironment%20shows%20that%20LRLL%20outperforms%20end-to-end%20and%20vanilla%20LLM%20approaches%0Ain%20the%20lifelong%20setup%20while%20learning%20skills%20that%20are%20transferable%20to%20the%20real%0Aworld.%20Project%20material%20will%20become%20available%20at%20the%20webpage%0Ahttps%3A//gtziafas.github.io/LRLL_project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18746v2&entry.124074799=Read"},
{"title": "SparQ Attention: Bandwidth-Efficient LLM Inference", "author": "Luka Ribar and Ivan Chelombiev and Luke Hudlass-Galley and Charlie Blake and Carlo Luschi and Douglas Orr", "abstract": "  The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.\n", "link": "http://arxiv.org/abs/2312.04985v4", "date": "2024-07-15", "relevancy": 2.0517, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5524}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4959}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparQ%20Attention%3A%20Bandwidth-Efficient%20LLM%20Inference&body=Title%3A%20SparQ%20Attention%3A%20Bandwidth-Efficient%20LLM%20Inference%0AAuthor%3A%20Luka%20Ribar%20and%20Ivan%20Chelombiev%20and%20Luke%20Hudlass-Galley%20and%20Charlie%20Blake%20and%20Carlo%20Luschi%20and%20Douglas%20Orr%0AAbstract%3A%20%20%20The%20computational%20difficulties%20of%20large%20language%20model%20%28LLM%29%20inference%20remain%0Aa%20significant%20obstacle%20to%20their%20widespread%20deployment.%20The%20need%20for%20many%0Aapplications%20to%20support%20long%20input%20sequences%20and%20process%20them%20in%20large%20batches%0Atypically%20causes%20token-generation%20to%20be%20bottlenecked%20by%20data%20transfer.%20For%20this%0Areason%2C%20we%20introduce%20SparQ%20Attention%2C%20a%20technique%20for%20increasing%20the%20inference%0Athroughput%20of%20LLMs%20by%20utilising%20memory%20bandwidth%20more%20efficiently%20within%20the%0Aattention%20layers%2C%20through%20selective%20fetching%20of%20the%20cached%20history.%20Our%0Aproposed%20technique%20can%20be%20applied%20directly%20to%20off-the-shelf%20LLMs%20during%0Ainference%2C%20without%20requiring%20any%20modification%20to%20the%20pre-training%20setup%20or%0Aadditional%20fine-tuning.%20We%20show%20that%20SparQ%20Attention%20brings%20up%20to%208x%20savings%20in%0Aattention%20data%20transfers%20without%20substantial%20drops%20in%20accuracy%2C%20by%20evaluating%0ALlama%202%20and%203%2C%20Mistral%2C%20Gemma%20and%20Pythia%20models%20on%20a%20wide%20range%20of%20downstream%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04985v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparQ%2520Attention%253A%2520Bandwidth-Efficient%2520LLM%2520Inference%26entry.906535625%3DLuka%2520Ribar%2520and%2520Ivan%2520Chelombiev%2520and%2520Luke%2520Hudlass-Galley%2520and%2520Charlie%2520Blake%2520and%2520Carlo%2520Luschi%2520and%2520Douglas%2520Orr%26entry.1292438233%3D%2520%2520The%2520computational%2520difficulties%2520of%2520large%2520language%2520model%2520%2528LLM%2529%2520inference%2520remain%250Aa%2520significant%2520obstacle%2520to%2520their%2520widespread%2520deployment.%2520The%2520need%2520for%2520many%250Aapplications%2520to%2520support%2520long%2520input%2520sequences%2520and%2520process%2520them%2520in%2520large%2520batches%250Atypically%2520causes%2520token-generation%2520to%2520be%2520bottlenecked%2520by%2520data%2520transfer.%2520For%2520this%250Areason%252C%2520we%2520introduce%2520SparQ%2520Attention%252C%2520a%2520technique%2520for%2520increasing%2520the%2520inference%250Athroughput%2520of%2520LLMs%2520by%2520utilising%2520memory%2520bandwidth%2520more%2520efficiently%2520within%2520the%250Aattention%2520layers%252C%2520through%2520selective%2520fetching%2520of%2520the%2520cached%2520history.%2520Our%250Aproposed%2520technique%2520can%2520be%2520applied%2520directly%2520to%2520off-the-shelf%2520LLMs%2520during%250Ainference%252C%2520without%2520requiring%2520any%2520modification%2520to%2520the%2520pre-training%2520setup%2520or%250Aadditional%2520fine-tuning.%2520We%2520show%2520that%2520SparQ%2520Attention%2520brings%2520up%2520to%25208x%2520savings%2520in%250Aattention%2520data%2520transfers%2520without%2520substantial%2520drops%2520in%2520accuracy%252C%2520by%2520evaluating%250ALlama%25202%2520and%25203%252C%2520Mistral%252C%2520Gemma%2520and%2520Pythia%2520models%2520on%2520a%2520wide%2520range%2520of%2520downstream%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04985v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparQ%20Attention%3A%20Bandwidth-Efficient%20LLM%20Inference&entry.906535625=Luka%20Ribar%20and%20Ivan%20Chelombiev%20and%20Luke%20Hudlass-Galley%20and%20Charlie%20Blake%20and%20Carlo%20Luschi%20and%20Douglas%20Orr&entry.1292438233=%20%20The%20computational%20difficulties%20of%20large%20language%20model%20%28LLM%29%20inference%20remain%0Aa%20significant%20obstacle%20to%20their%20widespread%20deployment.%20The%20need%20for%20many%0Aapplications%20to%20support%20long%20input%20sequences%20and%20process%20them%20in%20large%20batches%0Atypically%20causes%20token-generation%20to%20be%20bottlenecked%20by%20data%20transfer.%20For%20this%0Areason%2C%20we%20introduce%20SparQ%20Attention%2C%20a%20technique%20for%20increasing%20the%20inference%0Athroughput%20of%20LLMs%20by%20utilising%20memory%20bandwidth%20more%20efficiently%20within%20the%0Aattention%20layers%2C%20through%20selective%20fetching%20of%20the%20cached%20history.%20Our%0Aproposed%20technique%20can%20be%20applied%20directly%20to%20off-the-shelf%20LLMs%20during%0Ainference%2C%20without%20requiring%20any%20modification%20to%20the%20pre-training%20setup%20or%0Aadditional%20fine-tuning.%20We%20show%20that%20SparQ%20Attention%20brings%20up%20to%208x%20savings%20in%0Aattention%20data%20transfers%20without%20substantial%20drops%20in%20accuracy%2C%20by%20evaluating%0ALlama%202%20and%203%2C%20Mistral%2C%20Gemma%20and%20Pythia%20models%20on%20a%20wide%20range%20of%20downstream%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04985v4&entry.124074799=Read"},
{"title": "Accelerating Aquatic Soft Robots with Elastic Instability Effects", "author": "Zechen Xiong and Suyu Luohong and Jeong Hun Lee and Hod Lipson", "abstract": "  Sinusoidal undulation has long been considered the most successful swimming\npattern for fish and bionic aquatic robots [1]. However, a swimming pattern\ngenerated by the hair clip mechanism (HCM, part iii, Figure 1A) [2]~[5] may\nchallenge this knowledge. HCM is an in-plane prestressed bi-stable mechanism\nthat stores elastic energy and releases the stored energy quickly via its\nsnap-through buckling. When used for fish robots, the HCM functions as the fish\nbody and creates unique swimming patterns that we term HCM undulation. With the\nsame energy consumption [3], HCM fish outperforms the traditionally designed\nsoft fish with a two-fold increase in cruising speed. We reproduce this\nphenomenon in a single-link simulation with Aquarium [6]. HCM undulation\ngenerates an average propulsion of 16.7 N/m, 2-3 times larger than the\nreference undulation (6.78 N/m), sine pattern (5.34 N/m/s), and cambering sine\npattern (6.36 N/m), and achieves an efficiency close to the sine pattern. These\nresults can aid in developing fish robots and faster swimming patterns.\n", "link": "http://arxiv.org/abs/2310.14119v3", "date": "2024-07-15", "relevancy": 1.2731, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4384}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4235}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Aquatic%20Soft%20Robots%20with%20Elastic%20Instability%20Effects&body=Title%3A%20Accelerating%20Aquatic%20Soft%20Robots%20with%20Elastic%20Instability%20Effects%0AAuthor%3A%20Zechen%20Xiong%20and%20Suyu%20Luohong%20and%20Jeong%20Hun%20Lee%20and%20Hod%20Lipson%0AAbstract%3A%20%20%20Sinusoidal%20undulation%20has%20long%20been%20considered%20the%20most%20successful%20swimming%0Apattern%20for%20fish%20and%20bionic%20aquatic%20robots%20%5B1%5D.%20However%2C%20a%20swimming%20pattern%0Agenerated%20by%20the%20hair%20clip%20mechanism%20%28HCM%2C%20part%20iii%2C%20Figure%201A%29%20%5B2%5D~%5B5%5D%20may%0Achallenge%20this%20knowledge.%20HCM%20is%20an%20in-plane%20prestressed%20bi-stable%20mechanism%0Athat%20stores%20elastic%20energy%20and%20releases%20the%20stored%20energy%20quickly%20via%20its%0Asnap-through%20buckling.%20When%20used%20for%20fish%20robots%2C%20the%20HCM%20functions%20as%20the%20fish%0Abody%20and%20creates%20unique%20swimming%20patterns%20that%20we%20term%20HCM%20undulation.%20With%20the%0Asame%20energy%20consumption%20%5B3%5D%2C%20HCM%20fish%20outperforms%20the%20traditionally%20designed%0Asoft%20fish%20with%20a%20two-fold%20increase%20in%20cruising%20speed.%20We%20reproduce%20this%0Aphenomenon%20in%20a%20single-link%20simulation%20with%20Aquarium%20%5B6%5D.%20HCM%20undulation%0Agenerates%20an%20average%20propulsion%20of%2016.7%20N/m%2C%202-3%20times%20larger%20than%20the%0Areference%20undulation%20%286.78%20N/m%29%2C%20sine%20pattern%20%285.34%20N/m/s%29%2C%20and%20cambering%20sine%0Apattern%20%286.36%20N/m%29%2C%20and%20achieves%20an%20efficiency%20close%20to%20the%20sine%20pattern.%20These%0Aresults%20can%20aid%20in%20developing%20fish%20robots%20and%20faster%20swimming%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14119v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Aquatic%2520Soft%2520Robots%2520with%2520Elastic%2520Instability%2520Effects%26entry.906535625%3DZechen%2520Xiong%2520and%2520Suyu%2520Luohong%2520and%2520Jeong%2520Hun%2520Lee%2520and%2520Hod%2520Lipson%26entry.1292438233%3D%2520%2520Sinusoidal%2520undulation%2520has%2520long%2520been%2520considered%2520the%2520most%2520successful%2520swimming%250Apattern%2520for%2520fish%2520and%2520bionic%2520aquatic%2520robots%2520%255B1%255D.%2520However%252C%2520a%2520swimming%2520pattern%250Agenerated%2520by%2520the%2520hair%2520clip%2520mechanism%2520%2528HCM%252C%2520part%2520iii%252C%2520Figure%25201A%2529%2520%255B2%255D~%255B5%255D%2520may%250Achallenge%2520this%2520knowledge.%2520HCM%2520is%2520an%2520in-plane%2520prestressed%2520bi-stable%2520mechanism%250Athat%2520stores%2520elastic%2520energy%2520and%2520releases%2520the%2520stored%2520energy%2520quickly%2520via%2520its%250Asnap-through%2520buckling.%2520When%2520used%2520for%2520fish%2520robots%252C%2520the%2520HCM%2520functions%2520as%2520the%2520fish%250Abody%2520and%2520creates%2520unique%2520swimming%2520patterns%2520that%2520we%2520term%2520HCM%2520undulation.%2520With%2520the%250Asame%2520energy%2520consumption%2520%255B3%255D%252C%2520HCM%2520fish%2520outperforms%2520the%2520traditionally%2520designed%250Asoft%2520fish%2520with%2520a%2520two-fold%2520increase%2520in%2520cruising%2520speed.%2520We%2520reproduce%2520this%250Aphenomenon%2520in%2520a%2520single-link%2520simulation%2520with%2520Aquarium%2520%255B6%255D.%2520HCM%2520undulation%250Agenerates%2520an%2520average%2520propulsion%2520of%252016.7%2520N/m%252C%25202-3%2520times%2520larger%2520than%2520the%250Areference%2520undulation%2520%25286.78%2520N/m%2529%252C%2520sine%2520pattern%2520%25285.34%2520N/m/s%2529%252C%2520and%2520cambering%2520sine%250Apattern%2520%25286.36%2520N/m%2529%252C%2520and%2520achieves%2520an%2520efficiency%2520close%2520to%2520the%2520sine%2520pattern.%2520These%250Aresults%2520can%2520aid%2520in%2520developing%2520fish%2520robots%2520and%2520faster%2520swimming%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.14119v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Aquatic%20Soft%20Robots%20with%20Elastic%20Instability%20Effects&entry.906535625=Zechen%20Xiong%20and%20Suyu%20Luohong%20and%20Jeong%20Hun%20Lee%20and%20Hod%20Lipson&entry.1292438233=%20%20Sinusoidal%20undulation%20has%20long%20been%20considered%20the%20most%20successful%20swimming%0Apattern%20for%20fish%20and%20bionic%20aquatic%20robots%20%5B1%5D.%20However%2C%20a%20swimming%20pattern%0Agenerated%20by%20the%20hair%20clip%20mechanism%20%28HCM%2C%20part%20iii%2C%20Figure%201A%29%20%5B2%5D~%5B5%5D%20may%0Achallenge%20this%20knowledge.%20HCM%20is%20an%20in-plane%20prestressed%20bi-stable%20mechanism%0Athat%20stores%20elastic%20energy%20and%20releases%20the%20stored%20energy%20quickly%20via%20its%0Asnap-through%20buckling.%20When%20used%20for%20fish%20robots%2C%20the%20HCM%20functions%20as%20the%20fish%0Abody%20and%20creates%20unique%20swimming%20patterns%20that%20we%20term%20HCM%20undulation.%20With%20the%0Asame%20energy%20consumption%20%5B3%5D%2C%20HCM%20fish%20outperforms%20the%20traditionally%20designed%0Asoft%20fish%20with%20a%20two-fold%20increase%20in%20cruising%20speed.%20We%20reproduce%20this%0Aphenomenon%20in%20a%20single-link%20simulation%20with%20Aquarium%20%5B6%5D.%20HCM%20undulation%0Agenerates%20an%20average%20propulsion%20of%2016.7%20N/m%2C%202-3%20times%20larger%20than%20the%0Areference%20undulation%20%286.78%20N/m%29%2C%20sine%20pattern%20%285.34%20N/m/s%29%2C%20and%20cambering%20sine%0Apattern%20%286.36%20N/m%29%2C%20and%20achieves%20an%20efficiency%20close%20to%20the%20sine%20pattern.%20These%0Aresults%20can%20aid%20in%20developing%20fish%20robots%20and%20faster%20swimming%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14119v3&entry.124074799=Read"},
{"title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant", "author": "Guohao Sun and Can Qin and Jiamian Wang and Zeyuan Chen and Ran Xu and Zhiqiang Tao", "abstract": "  Recent advances in vision-language models have shown notable generalization\nin broad tasks through visual instruction tuning. However, bridging the gap\nbetween the pre-trained vision encoder and the large language models (LLMs)\nbecomes the whole network's bottleneck. To improve cross-modality alignment,\nexisting works usually consider more visual instruction data covering a broader\nrange of vision tasks to fine-tune the model for question-answering, which,\nhowever, is costly to obtain and has not thoroughly explored the rich\ncontextual information contained in images. This paper first attempts to\nharness the overlooked context within visual instruction data, training the\nmodel to self-supervised \"learning\" how to ask high-quality questions. In this\nway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large\nVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible\nand meaningful image-related questions while analyzing the visual clue and\nprior language knowledge, signifying an advanced level of generalized visual\nunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction\ndata shows a performance improvement compared with traditional\nvisual-instruction tuning methods. This improvement highlights the efficacy of\nself-questioning techniques in achieving a deeper and more nuanced\ncomprehension of visual content across various contexts.\n", "link": "http://arxiv.org/abs/2403.11299v2", "date": "2024-07-15", "relevancy": 2.0366, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5325}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5016}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SQ-LLaVA%3A%20Self-Questioning%20for%20Large%20Vision-Language%20Assistant&body=Title%3A%20SQ-LLaVA%3A%20Self-Questioning%20for%20Large%20Vision-Language%20Assistant%0AAuthor%3A%20Guohao%20Sun%20and%20Can%20Qin%20and%20Jiamian%20Wang%20and%20Zeyuan%20Chen%20and%20Ran%20Xu%20and%20Zhiqiang%20Tao%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision-language%20models%20have%20shown%20notable%20generalization%0Ain%20broad%20tasks%20through%20visual%20instruction%20tuning.%20However%2C%20bridging%20the%20gap%0Abetween%20the%20pre-trained%20vision%20encoder%20and%20the%20large%20language%20models%20%28LLMs%29%0Abecomes%20the%20whole%20network%27s%20bottleneck.%20To%20improve%20cross-modality%20alignment%2C%0Aexisting%20works%20usually%20consider%20more%20visual%20instruction%20data%20covering%20a%20broader%0Arange%20of%20vision%20tasks%20to%20fine-tune%20the%20model%20for%20question-answering%2C%20which%2C%0Ahowever%2C%20is%20costly%20to%20obtain%20and%20has%20not%20thoroughly%20explored%20the%20rich%0Acontextual%20information%20contained%20in%20images.%20This%20paper%20first%20attempts%20to%0Aharness%20the%20overlooked%20context%20within%20visual%20instruction%20data%2C%20training%20the%0Amodel%20to%20self-supervised%20%22learning%22%20how%20to%20ask%20high-quality%20questions.%20In%20this%0Away%2C%20we%20introduce%20a%20novel%20framework%20named%20SQ-LLaVA%3A%20Self-Questioning%20for%20Large%0AVision-Language%20Assistant.%20SQ-LLaVA%20exhibits%20proficiency%20in%20generating%20flexible%0Aand%20meaningful%20image-related%20questions%20while%20analyzing%20the%20visual%20clue%20and%0Aprior%20language%20knowledge%2C%20signifying%20an%20advanced%20level%20of%20generalized%20visual%0Aunderstanding.%20Moreover%2C%20fine-tuning%20SQ-LLaVA%20on%20higher-quality%20instruction%0Adata%20shows%20a%20performance%20improvement%20compared%20with%20traditional%0Avisual-instruction%20tuning%20methods.%20This%20improvement%20highlights%20the%20efficacy%20of%0Aself-questioning%20techniques%20in%20achieving%20a%20deeper%20and%20more%20nuanced%0Acomprehension%20of%20visual%20content%20across%20various%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSQ-LLaVA%253A%2520Self-Questioning%2520for%2520Large%2520Vision-Language%2520Assistant%26entry.906535625%3DGuohao%2520Sun%2520and%2520Can%2520Qin%2520and%2520Jiamian%2520Wang%2520and%2520Zeyuan%2520Chen%2520and%2520Ran%2520Xu%2520and%2520Zhiqiang%2520Tao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision-language%2520models%2520have%2520shown%2520notable%2520generalization%250Ain%2520broad%2520tasks%2520through%2520visual%2520instruction%2520tuning.%2520However%252C%2520bridging%2520the%2520gap%250Abetween%2520the%2520pre-trained%2520vision%2520encoder%2520and%2520the%2520large%2520language%2520models%2520%2528LLMs%2529%250Abecomes%2520the%2520whole%2520network%2527s%2520bottleneck.%2520To%2520improve%2520cross-modality%2520alignment%252C%250Aexisting%2520works%2520usually%2520consider%2520more%2520visual%2520instruction%2520data%2520covering%2520a%2520broader%250Arange%2520of%2520vision%2520tasks%2520to%2520fine-tune%2520the%2520model%2520for%2520question-answering%252C%2520which%252C%250Ahowever%252C%2520is%2520costly%2520to%2520obtain%2520and%2520has%2520not%2520thoroughly%2520explored%2520the%2520rich%250Acontextual%2520information%2520contained%2520in%2520images.%2520This%2520paper%2520first%2520attempts%2520to%250Aharness%2520the%2520overlooked%2520context%2520within%2520visual%2520instruction%2520data%252C%2520training%2520the%250Amodel%2520to%2520self-supervised%2520%2522learning%2522%2520how%2520to%2520ask%2520high-quality%2520questions.%2520In%2520this%250Away%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520named%2520SQ-LLaVA%253A%2520Self-Questioning%2520for%2520Large%250AVision-Language%2520Assistant.%2520SQ-LLaVA%2520exhibits%2520proficiency%2520in%2520generating%2520flexible%250Aand%2520meaningful%2520image-related%2520questions%2520while%2520analyzing%2520the%2520visual%2520clue%2520and%250Aprior%2520language%2520knowledge%252C%2520signifying%2520an%2520advanced%2520level%2520of%2520generalized%2520visual%250Aunderstanding.%2520Moreover%252C%2520fine-tuning%2520SQ-LLaVA%2520on%2520higher-quality%2520instruction%250Adata%2520shows%2520a%2520performance%2520improvement%2520compared%2520with%2520traditional%250Avisual-instruction%2520tuning%2520methods.%2520This%2520improvement%2520highlights%2520the%2520efficacy%2520of%250Aself-questioning%2520techniques%2520in%2520achieving%2520a%2520deeper%2520and%2520more%2520nuanced%250Acomprehension%2520of%2520visual%2520content%2520across%2520various%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SQ-LLaVA%3A%20Self-Questioning%20for%20Large%20Vision-Language%20Assistant&entry.906535625=Guohao%20Sun%20and%20Can%20Qin%20and%20Jiamian%20Wang%20and%20Zeyuan%20Chen%20and%20Ran%20Xu%20and%20Zhiqiang%20Tao&entry.1292438233=%20%20Recent%20advances%20in%20vision-language%20models%20have%20shown%20notable%20generalization%0Ain%20broad%20tasks%20through%20visual%20instruction%20tuning.%20However%2C%20bridging%20the%20gap%0Abetween%20the%20pre-trained%20vision%20encoder%20and%20the%20large%20language%20models%20%28LLMs%29%0Abecomes%20the%20whole%20network%27s%20bottleneck.%20To%20improve%20cross-modality%20alignment%2C%0Aexisting%20works%20usually%20consider%20more%20visual%20instruction%20data%20covering%20a%20broader%0Arange%20of%20vision%20tasks%20to%20fine-tune%20the%20model%20for%20question-answering%2C%20which%2C%0Ahowever%2C%20is%20costly%20to%20obtain%20and%20has%20not%20thoroughly%20explored%20the%20rich%0Acontextual%20information%20contained%20in%20images.%20This%20paper%20first%20attempts%20to%0Aharness%20the%20overlooked%20context%20within%20visual%20instruction%20data%2C%20training%20the%0Amodel%20to%20self-supervised%20%22learning%22%20how%20to%20ask%20high-quality%20questions.%20In%20this%0Away%2C%20we%20introduce%20a%20novel%20framework%20named%20SQ-LLaVA%3A%20Self-Questioning%20for%20Large%0AVision-Language%20Assistant.%20SQ-LLaVA%20exhibits%20proficiency%20in%20generating%20flexible%0Aand%20meaningful%20image-related%20questions%20while%20analyzing%20the%20visual%20clue%20and%0Aprior%20language%20knowledge%2C%20signifying%20an%20advanced%20level%20of%20generalized%20visual%0Aunderstanding.%20Moreover%2C%20fine-tuning%20SQ-LLaVA%20on%20higher-quality%20instruction%0Adata%20shows%20a%20performance%20improvement%20compared%20with%20traditional%0Avisual-instruction%20tuning%20methods.%20This%20improvement%20highlights%20the%20efficacy%20of%0Aself-questioning%20techniques%20in%20achieving%20a%20deeper%20and%20more%20nuanced%0Acomprehension%20of%20visual%20content%20across%20various%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11299v2&entry.124074799=Read"},
{"title": "OVLW-DETR: Open-Vocabulary Light-Weighted Detection Transformer", "author": "Yu Wang and Xiangbo Su and Qiang Chen and Xinyu Zhang and Teng Xi and Kun Yao and Errui Ding and Gang Zhang and Jingdong Wang", "abstract": "  Open-vocabulary object detection focusing on detecting novel categories\nguided by natural language. In this report, we propose Open-Vocabulary\nLight-Weighted Detection Transformer (OVLW-DETR), a deployment friendly\nopen-vocabulary detector with strong performance and low latency. Building upon\nOVLW-DETR, we provide an end-to-end training recipe that transferring knowledge\nfrom vision-language model (VLM) to object detector with simple alignment. We\nalign detector with the text encoder from VLM by replacing the fixed\nclassification layer weights in detector with the class-name embeddings\nextracted from the text encoder. Without additional fusing module, OVLW-DETR is\nflexible and deployment friendly, making it easier to implement and modulate.\nimproving the efficiency of interleaved attention computation. Experimental\nresults demonstrate that the proposed approach is superior over existing\nreal-time open-vocabulary detectors on standard Zero-Shot LVIS benchmark.\nSource code and pre-trained models are available at\n[https://github.com/Atten4Vis/LW-DETR].\n", "link": "http://arxiv.org/abs/2407.10655v1", "date": "2024-07-15", "relevancy": 2.0516, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OVLW-DETR%3A%20Open-Vocabulary%20Light-Weighted%20Detection%20Transformer&body=Title%3A%20OVLW-DETR%3A%20Open-Vocabulary%20Light-Weighted%20Detection%20Transformer%0AAuthor%3A%20Yu%20Wang%20and%20Xiangbo%20Su%20and%20Qiang%20Chen%20and%20Xinyu%20Zhang%20and%20Teng%20Xi%20and%20Kun%20Yao%20and%20Errui%20Ding%20and%20Gang%20Zhang%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Open-vocabulary%20object%20detection%20focusing%20on%20detecting%20novel%20categories%0Aguided%20by%20natural%20language.%20In%20this%20report%2C%20we%20propose%20Open-Vocabulary%0ALight-Weighted%20Detection%20Transformer%20%28OVLW-DETR%29%2C%20a%20deployment%20friendly%0Aopen-vocabulary%20detector%20with%20strong%20performance%20and%20low%20latency.%20Building%20upon%0AOVLW-DETR%2C%20we%20provide%20an%20end-to-end%20training%20recipe%20that%20transferring%20knowledge%0Afrom%20vision-language%20model%20%28VLM%29%20to%20object%20detector%20with%20simple%20alignment.%20We%0Aalign%20detector%20with%20the%20text%20encoder%20from%20VLM%20by%20replacing%20the%20fixed%0Aclassification%20layer%20weights%20in%20detector%20with%20the%20class-name%20embeddings%0Aextracted%20from%20the%20text%20encoder.%20Without%20additional%20fusing%20module%2C%20OVLW-DETR%20is%0Aflexible%20and%20deployment%20friendly%2C%20making%20it%20easier%20to%20implement%20and%20modulate.%0Aimproving%20the%20efficiency%20of%20interleaved%20attention%20computation.%20Experimental%0Aresults%20demonstrate%20that%20the%20proposed%20approach%20is%20superior%20over%20existing%0Areal-time%20open-vocabulary%20detectors%20on%20standard%20Zero-Shot%20LVIS%20benchmark.%0ASource%20code%20and%20pre-trained%20models%20are%20available%20at%0A%5Bhttps%3A//github.com/Atten4Vis/LW-DETR%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOVLW-DETR%253A%2520Open-Vocabulary%2520Light-Weighted%2520Detection%2520Transformer%26entry.906535625%3DYu%2520Wang%2520and%2520Xiangbo%2520Su%2520and%2520Qiang%2520Chen%2520and%2520Xinyu%2520Zhang%2520and%2520Teng%2520Xi%2520and%2520Kun%2520Yao%2520and%2520Errui%2520Ding%2520and%2520Gang%2520Zhang%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520Open-vocabulary%2520object%2520detection%2520focusing%2520on%2520detecting%2520novel%2520categories%250Aguided%2520by%2520natural%2520language.%2520In%2520this%2520report%252C%2520we%2520propose%2520Open-Vocabulary%250ALight-Weighted%2520Detection%2520Transformer%2520%2528OVLW-DETR%2529%252C%2520a%2520deployment%2520friendly%250Aopen-vocabulary%2520detector%2520with%2520strong%2520performance%2520and%2520low%2520latency.%2520Building%2520upon%250AOVLW-DETR%252C%2520we%2520provide%2520an%2520end-to-end%2520training%2520recipe%2520that%2520transferring%2520knowledge%250Afrom%2520vision-language%2520model%2520%2528VLM%2529%2520to%2520object%2520detector%2520with%2520simple%2520alignment.%2520We%250Aalign%2520detector%2520with%2520the%2520text%2520encoder%2520from%2520VLM%2520by%2520replacing%2520the%2520fixed%250Aclassification%2520layer%2520weights%2520in%2520detector%2520with%2520the%2520class-name%2520embeddings%250Aextracted%2520from%2520the%2520text%2520encoder.%2520Without%2520additional%2520fusing%2520module%252C%2520OVLW-DETR%2520is%250Aflexible%2520and%2520deployment%2520friendly%252C%2520making%2520it%2520easier%2520to%2520implement%2520and%2520modulate.%250Aimproving%2520the%2520efficiency%2520of%2520interleaved%2520attention%2520computation.%2520Experimental%250Aresults%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520is%2520superior%2520over%2520existing%250Areal-time%2520open-vocabulary%2520detectors%2520on%2520standard%2520Zero-Shot%2520LVIS%2520benchmark.%250ASource%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250A%255Bhttps%253A//github.com/Atten4Vis/LW-DETR%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OVLW-DETR%3A%20Open-Vocabulary%20Light-Weighted%20Detection%20Transformer&entry.906535625=Yu%20Wang%20and%20Xiangbo%20Su%20and%20Qiang%20Chen%20and%20Xinyu%20Zhang%20and%20Teng%20Xi%20and%20Kun%20Yao%20and%20Errui%20Ding%20and%20Gang%20Zhang%20and%20Jingdong%20Wang&entry.1292438233=%20%20Open-vocabulary%20object%20detection%20focusing%20on%20detecting%20novel%20categories%0Aguided%20by%20natural%20language.%20In%20this%20report%2C%20we%20propose%20Open-Vocabulary%0ALight-Weighted%20Detection%20Transformer%20%28OVLW-DETR%29%2C%20a%20deployment%20friendly%0Aopen-vocabulary%20detector%20with%20strong%20performance%20and%20low%20latency.%20Building%20upon%0AOVLW-DETR%2C%20we%20provide%20an%20end-to-end%20training%20recipe%20that%20transferring%20knowledge%0Afrom%20vision-language%20model%20%28VLM%29%20to%20object%20detector%20with%20simple%20alignment.%20We%0Aalign%20detector%20with%20the%20text%20encoder%20from%20VLM%20by%20replacing%20the%20fixed%0Aclassification%20layer%20weights%20in%20detector%20with%20the%20class-name%20embeddings%0Aextracted%20from%20the%20text%20encoder.%20Without%20additional%20fusing%20module%2C%20OVLW-DETR%20is%0Aflexible%20and%20deployment%20friendly%2C%20making%20it%20easier%20to%20implement%20and%20modulate.%0Aimproving%20the%20efficiency%20of%20interleaved%20attention%20computation.%20Experimental%0Aresults%20demonstrate%20that%20the%20proposed%20approach%20is%20superior%20over%20existing%0Areal-time%20open-vocabulary%20detectors%20on%20standard%20Zero-Shot%20LVIS%20benchmark.%0ASource%20code%20and%20pre-trained%20models%20are%20available%20at%0A%5Bhttps%3A//github.com/Atten4Vis/LW-DETR%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10655v1&entry.124074799=Read"},
{"title": "Patch-Consistent Optical Translation Across Sensors: Large-Scale\n  Denoising Diffusion with Heterogeneous Change Detection as a Use Case", "author": "Jo\u00e3o Gabriel Vinholi and Marco Chini and Anis Amziane and Renato Machado and Danilo Silva and Patrick Matgen", "abstract": "  In the field of remote sensing, the challenge of comparing images captured by\ndisparate sensors is a common obstacle. This requires image translation --\nconverting imagery from one sensor domain to another while preserving the\noriginal content. Denoising Diffusion Implicit Models (DDIM) are potential\nstate-of-the-art solutions for such domain translation due to their proven\nsuperiority in multiple image-to-image translation tasks in classic computer\nvision. However, these models struggle with large-scale multi-patch imagery,\noften focusing solely on small patches and resulting in inconsistencies across\nthe full image. To overcome these limitations, we propose a novel method that\nleverages DDIM for effective optical image translation over large areas. Our\napproach is tailored to super-resolve large-scale low spatial resolution images\ninto high-resolution equivalents from disparate optical sensors, ensuring\nuniformity across hundreds of patches. Extensive experiments with a dataset of\npaired Sentinel-II and Planet Dove images show that our approach provides\nprecise domain adaptation and artifact reduction. Our technique preserves the\nimage content while also improving radiometric (color) accuracy and feature\nrepresentations. The outcome is a high-resolution large-scale image with\nconsistent patches, vital for applications such as heterogeneous change\ndetection (HCD). We present a unique training and testing algorithm rooted in\nDDIMs, a thorough image quality assessment, and a comparative study against the\nstandard classifier-free guided DDIM framework and five other leading methods.\nThe efficacy of our approach is further demonstrated by substantial\nenhancements in HCD tasks performed in the urban settings of Beirut, Lebanon,\nand Austin, USA.\n", "link": "http://arxiv.org/abs/2404.11243v2", "date": "2024-07-15", "relevancy": 1.8151, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6328}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5995}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch-Consistent%20Optical%20Translation%20Across%20Sensors%3A%20Large-Scale%0A%20%20Denoising%20Diffusion%20with%20Heterogeneous%20Change%20Detection%20as%20a%20Use%20Case&body=Title%3A%20Patch-Consistent%20Optical%20Translation%20Across%20Sensors%3A%20Large-Scale%0A%20%20Denoising%20Diffusion%20with%20Heterogeneous%20Change%20Detection%20as%20a%20Use%20Case%0AAuthor%3A%20Jo%C3%A3o%20Gabriel%20Vinholi%20and%20Marco%20Chini%20and%20Anis%20Amziane%20and%20Renato%20Machado%20and%20Danilo%20Silva%20and%20Patrick%20Matgen%0AAbstract%3A%20%20%20In%20the%20field%20of%20remote%20sensing%2C%20the%20challenge%20of%20comparing%20images%20captured%20by%0Adisparate%20sensors%20is%20a%20common%20obstacle.%20This%20requires%20image%20translation%20--%0Aconverting%20imagery%20from%20one%20sensor%20domain%20to%20another%20while%20preserving%20the%0Aoriginal%20content.%20Denoising%20Diffusion%20Implicit%20Models%20%28DDIM%29%20are%20potential%0Astate-of-the-art%20solutions%20for%20such%20domain%20translation%20due%20to%20their%20proven%0Asuperiority%20in%20multiple%20image-to-image%20translation%20tasks%20in%20classic%20computer%0Avision.%20However%2C%20these%20models%20struggle%20with%20large-scale%20multi-patch%20imagery%2C%0Aoften%20focusing%20solely%20on%20small%20patches%20and%20resulting%20in%20inconsistencies%20across%0Athe%20full%20image.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20method%20that%0Aleverages%20DDIM%20for%20effective%20optical%20image%20translation%20over%20large%20areas.%20Our%0Aapproach%20is%20tailored%20to%20super-resolve%20large-scale%20low%20spatial%20resolution%20images%0Ainto%20high-resolution%20equivalents%20from%20disparate%20optical%20sensors%2C%20ensuring%0Auniformity%20across%20hundreds%20of%20patches.%20Extensive%20experiments%20with%20a%20dataset%20of%0Apaired%20Sentinel-II%20and%20Planet%20Dove%20images%20show%20that%20our%20approach%20provides%0Aprecise%20domain%20adaptation%20and%20artifact%20reduction.%20Our%20technique%20preserves%20the%0Aimage%20content%20while%20also%20improving%20radiometric%20%28color%29%20accuracy%20and%20feature%0Arepresentations.%20The%20outcome%20is%20a%20high-resolution%20large-scale%20image%20with%0Aconsistent%20patches%2C%20vital%20for%20applications%20such%20as%20heterogeneous%20change%0Adetection%20%28HCD%29.%20We%20present%20a%20unique%20training%20and%20testing%20algorithm%20rooted%20in%0ADDIMs%2C%20a%20thorough%20image%20quality%20assessment%2C%20and%20a%20comparative%20study%20against%20the%0Astandard%20classifier-free%20guided%20DDIM%20framework%20and%20five%20other%20leading%20methods.%0AThe%20efficacy%20of%20our%20approach%20is%20further%20demonstrated%20by%20substantial%0Aenhancements%20in%20HCD%20tasks%20performed%20in%20the%20urban%20settings%20of%20Beirut%2C%20Lebanon%2C%0Aand%20Austin%2C%20USA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11243v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch-Consistent%2520Optical%2520Translation%2520Across%2520Sensors%253A%2520Large-Scale%250A%2520%2520Denoising%2520Diffusion%2520with%2520Heterogeneous%2520Change%2520Detection%2520as%2520a%2520Use%2520Case%26entry.906535625%3DJo%25C3%25A3o%2520Gabriel%2520Vinholi%2520and%2520Marco%2520Chini%2520and%2520Anis%2520Amziane%2520and%2520Renato%2520Machado%2520and%2520Danilo%2520Silva%2520and%2520Patrick%2520Matgen%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520remote%2520sensing%252C%2520the%2520challenge%2520of%2520comparing%2520images%2520captured%2520by%250Adisparate%2520sensors%2520is%2520a%2520common%2520obstacle.%2520This%2520requires%2520image%2520translation%2520--%250Aconverting%2520imagery%2520from%2520one%2520sensor%2520domain%2520to%2520another%2520while%2520preserving%2520the%250Aoriginal%2520content.%2520Denoising%2520Diffusion%2520Implicit%2520Models%2520%2528DDIM%2529%2520are%2520potential%250Astate-of-the-art%2520solutions%2520for%2520such%2520domain%2520translation%2520due%2520to%2520their%2520proven%250Asuperiority%2520in%2520multiple%2520image-to-image%2520translation%2520tasks%2520in%2520classic%2520computer%250Avision.%2520However%252C%2520these%2520models%2520struggle%2520with%2520large-scale%2520multi-patch%2520imagery%252C%250Aoften%2520focusing%2520solely%2520on%2520small%2520patches%2520and%2520resulting%2520in%2520inconsistencies%2520across%250Athe%2520full%2520image.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520method%2520that%250Aleverages%2520DDIM%2520for%2520effective%2520optical%2520image%2520translation%2520over%2520large%2520areas.%2520Our%250Aapproach%2520is%2520tailored%2520to%2520super-resolve%2520large-scale%2520low%2520spatial%2520resolution%2520images%250Ainto%2520high-resolution%2520equivalents%2520from%2520disparate%2520optical%2520sensors%252C%2520ensuring%250Auniformity%2520across%2520hundreds%2520of%2520patches.%2520Extensive%2520experiments%2520with%2520a%2520dataset%2520of%250Apaired%2520Sentinel-II%2520and%2520Planet%2520Dove%2520images%2520show%2520that%2520our%2520approach%2520provides%250Aprecise%2520domain%2520adaptation%2520and%2520artifact%2520reduction.%2520Our%2520technique%2520preserves%2520the%250Aimage%2520content%2520while%2520also%2520improving%2520radiometric%2520%2528color%2529%2520accuracy%2520and%2520feature%250Arepresentations.%2520The%2520outcome%2520is%2520a%2520high-resolution%2520large-scale%2520image%2520with%250Aconsistent%2520patches%252C%2520vital%2520for%2520applications%2520such%2520as%2520heterogeneous%2520change%250Adetection%2520%2528HCD%2529.%2520We%2520present%2520a%2520unique%2520training%2520and%2520testing%2520algorithm%2520rooted%2520in%250ADDIMs%252C%2520a%2520thorough%2520image%2520quality%2520assessment%252C%2520and%2520a%2520comparative%2520study%2520against%2520the%250Astandard%2520classifier-free%2520guided%2520DDIM%2520framework%2520and%2520five%2520other%2520leading%2520methods.%250AThe%2520efficacy%2520of%2520our%2520approach%2520is%2520further%2520demonstrated%2520by%2520substantial%250Aenhancements%2520in%2520HCD%2520tasks%2520performed%2520in%2520the%2520urban%2520settings%2520of%2520Beirut%252C%2520Lebanon%252C%250Aand%2520Austin%252C%2520USA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11243v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch-Consistent%20Optical%20Translation%20Across%20Sensors%3A%20Large-Scale%0A%20%20Denoising%20Diffusion%20with%20Heterogeneous%20Change%20Detection%20as%20a%20Use%20Case&entry.906535625=Jo%C3%A3o%20Gabriel%20Vinholi%20and%20Marco%20Chini%20and%20Anis%20Amziane%20and%20Renato%20Machado%20and%20Danilo%20Silva%20and%20Patrick%20Matgen&entry.1292438233=%20%20In%20the%20field%20of%20remote%20sensing%2C%20the%20challenge%20of%20comparing%20images%20captured%20by%0Adisparate%20sensors%20is%20a%20common%20obstacle.%20This%20requires%20image%20translation%20--%0Aconverting%20imagery%20from%20one%20sensor%20domain%20to%20another%20while%20preserving%20the%0Aoriginal%20content.%20Denoising%20Diffusion%20Implicit%20Models%20%28DDIM%29%20are%20potential%0Astate-of-the-art%20solutions%20for%20such%20domain%20translation%20due%20to%20their%20proven%0Asuperiority%20in%20multiple%20image-to-image%20translation%20tasks%20in%20classic%20computer%0Avision.%20However%2C%20these%20models%20struggle%20with%20large-scale%20multi-patch%20imagery%2C%0Aoften%20focusing%20solely%20on%20small%20patches%20and%20resulting%20in%20inconsistencies%20across%0Athe%20full%20image.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20method%20that%0Aleverages%20DDIM%20for%20effective%20optical%20image%20translation%20over%20large%20areas.%20Our%0Aapproach%20is%20tailored%20to%20super-resolve%20large-scale%20low%20spatial%20resolution%20images%0Ainto%20high-resolution%20equivalents%20from%20disparate%20optical%20sensors%2C%20ensuring%0Auniformity%20across%20hundreds%20of%20patches.%20Extensive%20experiments%20with%20a%20dataset%20of%0Apaired%20Sentinel-II%20and%20Planet%20Dove%20images%20show%20that%20our%20approach%20provides%0Aprecise%20domain%20adaptation%20and%20artifact%20reduction.%20Our%20technique%20preserves%20the%0Aimage%20content%20while%20also%20improving%20radiometric%20%28color%29%20accuracy%20and%20feature%0Arepresentations.%20The%20outcome%20is%20a%20high-resolution%20large-scale%20image%20with%0Aconsistent%20patches%2C%20vital%20for%20applications%20such%20as%20heterogeneous%20change%0Adetection%20%28HCD%29.%20We%20present%20a%20unique%20training%20and%20testing%20algorithm%20rooted%20in%0ADDIMs%2C%20a%20thorough%20image%20quality%20assessment%2C%20and%20a%20comparative%20study%20against%20the%0Astandard%20classifier-free%20guided%20DDIM%20framework%20and%20five%20other%20leading%20methods.%0AThe%20efficacy%20of%20our%20approach%20is%20further%20demonstrated%20by%20substantial%0Aenhancements%20in%20HCD%20tasks%20performed%20in%20the%20urban%20settings%20of%20Beirut%2C%20Lebanon%2C%0Aand%20Austin%2C%20USA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11243v2&entry.124074799=Read"},
{"title": "When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion\n  Image Laundering", "author": "Sara Mandelli and Paolo Bestagini and Stefano Tubaro", "abstract": "  In recent years, methods for producing highly realistic synthetic images have\nsignificantly advanced, allowing the creation of high-quality images from text\nprompts that describe the desired content. Even more impressively, Stable\nDiffusion (SD) models now provide users with the option of creating synthetic\nimages in an image-to-image translation fashion, modifying images in the latent\nspace of advanced autoencoders. This striking evolution, however, brings an\nalarming consequence: it is possible to pass an image through SD autoencoders\nto reproduce a synthetic copy of the image with high realism and almost no\nvisual artifacts. This process, known as SD image laundering, can transform\nreal images into lookalike synthetic ones and risks complicating forensic\nanalysis for content authenticity verification. Our paper investigates the\nforensic implications of image laundering, revealing a serious potential to\nobscure traces of real content, including sensitive and harmful materials that\ncould be mistakenly classified as synthetic, thereby undermining the protection\nof individuals depicted. To address this issue, we propose a two-stage\ndetection pipeline that effectively differentiates between pristine, laundered,\nand fully synthetic images (those generated from text prompts), showing\nrobustness across various conditions. Finally, we highlight another alarming\nproperty of image laundering, which appears to mask the unique artifacts\nexploited by forensic detectors to solve the camera model identification task,\nstrongly undermining their performance. Our experimental code is available at\nhttps://github.com/polimi-ispl/synthetic-image-detection.\n", "link": "http://arxiv.org/abs/2407.10736v1", "date": "2024-07-15", "relevancy": 1.1564, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5909}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5879}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Synthetic%20Traces%20Hide%20Real%20Content%3A%20Analysis%20of%20Stable%20Diffusion%0A%20%20Image%20Laundering&body=Title%3A%20When%20Synthetic%20Traces%20Hide%20Real%20Content%3A%20Analysis%20of%20Stable%20Diffusion%0A%20%20Image%20Laundering%0AAuthor%3A%20Sara%20Mandelli%20and%20Paolo%20Bestagini%20and%20Stefano%20Tubaro%0AAbstract%3A%20%20%20In%20recent%20years%2C%20methods%20for%20producing%20highly%20realistic%20synthetic%20images%20have%0Asignificantly%20advanced%2C%20allowing%20the%20creation%20of%20high-quality%20images%20from%20text%0Aprompts%20that%20describe%20the%20desired%20content.%20Even%20more%20impressively%2C%20Stable%0ADiffusion%20%28SD%29%20models%20now%20provide%20users%20with%20the%20option%20of%20creating%20synthetic%0Aimages%20in%20an%20image-to-image%20translation%20fashion%2C%20modifying%20images%20in%20the%20latent%0Aspace%20of%20advanced%20autoencoders.%20This%20striking%20evolution%2C%20however%2C%20brings%20an%0Aalarming%20consequence%3A%20it%20is%20possible%20to%20pass%20an%20image%20through%20SD%20autoencoders%0Ato%20reproduce%20a%20synthetic%20copy%20of%20the%20image%20with%20high%20realism%20and%20almost%20no%0Avisual%20artifacts.%20This%20process%2C%20known%20as%20SD%20image%20laundering%2C%20can%20transform%0Areal%20images%20into%20lookalike%20synthetic%20ones%20and%20risks%20complicating%20forensic%0Aanalysis%20for%20content%20authenticity%20verification.%20Our%20paper%20investigates%20the%0Aforensic%20implications%20of%20image%20laundering%2C%20revealing%20a%20serious%20potential%20to%0Aobscure%20traces%20of%20real%20content%2C%20including%20sensitive%20and%20harmful%20materials%20that%0Acould%20be%20mistakenly%20classified%20as%20synthetic%2C%20thereby%20undermining%20the%20protection%0Aof%20individuals%20depicted.%20To%20address%20this%20issue%2C%20we%20propose%20a%20two-stage%0Adetection%20pipeline%20that%20effectively%20differentiates%20between%20pristine%2C%20laundered%2C%0Aand%20fully%20synthetic%20images%20%28those%20generated%20from%20text%20prompts%29%2C%20showing%0Arobustness%20across%20various%20conditions.%20Finally%2C%20we%20highlight%20another%20alarming%0Aproperty%20of%20image%20laundering%2C%20which%20appears%20to%20mask%20the%20unique%20artifacts%0Aexploited%20by%20forensic%20detectors%20to%20solve%20the%20camera%20model%20identification%20task%2C%0Astrongly%20undermining%20their%20performance.%20Our%20experimental%20code%20is%20available%20at%0Ahttps%3A//github.com/polimi-ispl/synthetic-image-detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Synthetic%2520Traces%2520Hide%2520Real%2520Content%253A%2520Analysis%2520of%2520Stable%2520Diffusion%250A%2520%2520Image%2520Laundering%26entry.906535625%3DSara%2520Mandelli%2520and%2520Paolo%2520Bestagini%2520and%2520Stefano%2520Tubaro%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520methods%2520for%2520producing%2520highly%2520realistic%2520synthetic%2520images%2520have%250Asignificantly%2520advanced%252C%2520allowing%2520the%2520creation%2520of%2520high-quality%2520images%2520from%2520text%250Aprompts%2520that%2520describe%2520the%2520desired%2520content.%2520Even%2520more%2520impressively%252C%2520Stable%250ADiffusion%2520%2528SD%2529%2520models%2520now%2520provide%2520users%2520with%2520the%2520option%2520of%2520creating%2520synthetic%250Aimages%2520in%2520an%2520image-to-image%2520translation%2520fashion%252C%2520modifying%2520images%2520in%2520the%2520latent%250Aspace%2520of%2520advanced%2520autoencoders.%2520This%2520striking%2520evolution%252C%2520however%252C%2520brings%2520an%250Aalarming%2520consequence%253A%2520it%2520is%2520possible%2520to%2520pass%2520an%2520image%2520through%2520SD%2520autoencoders%250Ato%2520reproduce%2520a%2520synthetic%2520copy%2520of%2520the%2520image%2520with%2520high%2520realism%2520and%2520almost%2520no%250Avisual%2520artifacts.%2520This%2520process%252C%2520known%2520as%2520SD%2520image%2520laundering%252C%2520can%2520transform%250Areal%2520images%2520into%2520lookalike%2520synthetic%2520ones%2520and%2520risks%2520complicating%2520forensic%250Aanalysis%2520for%2520content%2520authenticity%2520verification.%2520Our%2520paper%2520investigates%2520the%250Aforensic%2520implications%2520of%2520image%2520laundering%252C%2520revealing%2520a%2520serious%2520potential%2520to%250Aobscure%2520traces%2520of%2520real%2520content%252C%2520including%2520sensitive%2520and%2520harmful%2520materials%2520that%250Acould%2520be%2520mistakenly%2520classified%2520as%2520synthetic%252C%2520thereby%2520undermining%2520the%2520protection%250Aof%2520individuals%2520depicted.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520two-stage%250Adetection%2520pipeline%2520that%2520effectively%2520differentiates%2520between%2520pristine%252C%2520laundered%252C%250Aand%2520fully%2520synthetic%2520images%2520%2528those%2520generated%2520from%2520text%2520prompts%2529%252C%2520showing%250Arobustness%2520across%2520various%2520conditions.%2520Finally%252C%2520we%2520highlight%2520another%2520alarming%250Aproperty%2520of%2520image%2520laundering%252C%2520which%2520appears%2520to%2520mask%2520the%2520unique%2520artifacts%250Aexploited%2520by%2520forensic%2520detectors%2520to%2520solve%2520the%2520camera%2520model%2520identification%2520task%252C%250Astrongly%2520undermining%2520their%2520performance.%2520Our%2520experimental%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/polimi-ispl/synthetic-image-detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Synthetic%20Traces%20Hide%20Real%20Content%3A%20Analysis%20of%20Stable%20Diffusion%0A%20%20Image%20Laundering&entry.906535625=Sara%20Mandelli%20and%20Paolo%20Bestagini%20and%20Stefano%20Tubaro&entry.1292438233=%20%20In%20recent%20years%2C%20methods%20for%20producing%20highly%20realistic%20synthetic%20images%20have%0Asignificantly%20advanced%2C%20allowing%20the%20creation%20of%20high-quality%20images%20from%20text%0Aprompts%20that%20describe%20the%20desired%20content.%20Even%20more%20impressively%2C%20Stable%0ADiffusion%20%28SD%29%20models%20now%20provide%20users%20with%20the%20option%20of%20creating%20synthetic%0Aimages%20in%20an%20image-to-image%20translation%20fashion%2C%20modifying%20images%20in%20the%20latent%0Aspace%20of%20advanced%20autoencoders.%20This%20striking%20evolution%2C%20however%2C%20brings%20an%0Aalarming%20consequence%3A%20it%20is%20possible%20to%20pass%20an%20image%20through%20SD%20autoencoders%0Ato%20reproduce%20a%20synthetic%20copy%20of%20the%20image%20with%20high%20realism%20and%20almost%20no%0Avisual%20artifacts.%20This%20process%2C%20known%20as%20SD%20image%20laundering%2C%20can%20transform%0Areal%20images%20into%20lookalike%20synthetic%20ones%20and%20risks%20complicating%20forensic%0Aanalysis%20for%20content%20authenticity%20verification.%20Our%20paper%20investigates%20the%0Aforensic%20implications%20of%20image%20laundering%2C%20revealing%20a%20serious%20potential%20to%0Aobscure%20traces%20of%20real%20content%2C%20including%20sensitive%20and%20harmful%20materials%20that%0Acould%20be%20mistakenly%20classified%20as%20synthetic%2C%20thereby%20undermining%20the%20protection%0Aof%20individuals%20depicted.%20To%20address%20this%20issue%2C%20we%20propose%20a%20two-stage%0Adetection%20pipeline%20that%20effectively%20differentiates%20between%20pristine%2C%20laundered%2C%0Aand%20fully%20synthetic%20images%20%28those%20generated%20from%20text%20prompts%29%2C%20showing%0Arobustness%20across%20various%20conditions.%20Finally%2C%20we%20highlight%20another%20alarming%0Aproperty%20of%20image%20laundering%2C%20which%20appears%20to%20mask%20the%20unique%20artifacts%0Aexploited%20by%20forensic%20detectors%20to%20solve%20the%20camera%20model%20identification%20task%2C%0Astrongly%20undermining%20their%20performance.%20Our%20experimental%20code%20is%20available%20at%0Ahttps%3A//github.com/polimi-ispl/synthetic-image-detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10736v1&entry.124074799=Read"},
{"title": "Towards Open-World Grasping with Large Vision-Language Models", "author": "Georgios Tziafas and Hamidreza Kasaei", "abstract": "  The ability to grasp objects in-the-wild from open-ended language\ninstructions constitutes a fundamental challenge in robotics. An open-world\ngrasping system should be able to combine high-level contextual with low-level\nphysical-geometric reasoning in order to be applicable in arbitrary scenarios.\nRecent works exploit the web-scale knowledge inherent in large language models\n(LLMs) to plan and reason in robotic context, but rely on external vision and\naction models to ground such knowledge into the environment and parameterize\nactuation. This setup suffers from two major bottlenecks: a) the LLM's\nreasoning capacity is constrained by the quality of visual grounding, and b)\nLLMs do not contain low-level spatial understanding of the world, which is\nessential for grasping in contact-rich scenarios. In this work we demonstrate\nthat modern vision-language models (VLMs) are capable of tackling such\nlimitations, as they are implicitly grounded and can jointly reason about\nsemantics and geometry. We propose OWG, an open-world grasping pipeline that\ncombines VLMs with segmentation and grasp synthesis models to unlock grounded\nworld understanding in three stages: open-ended referring segmentation,\ngrounded grasp planning and grasp ranking via contact reasoning, all of which\ncan be applied zero-shot via suitable visual prompting mechanisms. We conduct\nextensive evaluation in cluttered indoor scene datasets to showcase OWG's\nrobustness in grounding from open-ended language, as well as open-world robotic\ngrasping experiments in both simulation and hardware that demonstrate superior\nperformance compared to previous supervised and zero-shot LLM-based methods.\n", "link": "http://arxiv.org/abs/2406.18722v3", "date": "2024-07-15", "relevancy": 1.8144, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6229}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.614}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Open-World%20Grasping%20with%20Large%20Vision-Language%20Models&body=Title%3A%20Towards%20Open-World%20Grasping%20with%20Large%20Vision-Language%20Models%0AAuthor%3A%20Georgios%20Tziafas%20and%20Hamidreza%20Kasaei%0AAbstract%3A%20%20%20The%20ability%20to%20grasp%20objects%20in-the-wild%20from%20open-ended%20language%0Ainstructions%20constitutes%20a%20fundamental%20challenge%20in%20robotics.%20An%20open-world%0Agrasping%20system%20should%20be%20able%20to%20combine%20high-level%20contextual%20with%20low-level%0Aphysical-geometric%20reasoning%20in%20order%20to%20be%20applicable%20in%20arbitrary%20scenarios.%0ARecent%20works%20exploit%20the%20web-scale%20knowledge%20inherent%20in%20large%20language%20models%0A%28LLMs%29%20to%20plan%20and%20reason%20in%20robotic%20context%2C%20but%20rely%20on%20external%20vision%20and%0Aaction%20models%20to%20ground%20such%20knowledge%20into%20the%20environment%20and%20parameterize%0Aactuation.%20This%20setup%20suffers%20from%20two%20major%20bottlenecks%3A%20a%29%20the%20LLM%27s%0Areasoning%20capacity%20is%20constrained%20by%20the%20quality%20of%20visual%20grounding%2C%20and%20b%29%0ALLMs%20do%20not%20contain%20low-level%20spatial%20understanding%20of%20the%20world%2C%20which%20is%0Aessential%20for%20grasping%20in%20contact-rich%20scenarios.%20In%20this%20work%20we%20demonstrate%0Athat%20modern%20vision-language%20models%20%28VLMs%29%20are%20capable%20of%20tackling%20such%0Alimitations%2C%20as%20they%20are%20implicitly%20grounded%20and%20can%20jointly%20reason%20about%0Asemantics%20and%20geometry.%20We%20propose%20OWG%2C%20an%20open-world%20grasping%20pipeline%20that%0Acombines%20VLMs%20with%20segmentation%20and%20grasp%20synthesis%20models%20to%20unlock%20grounded%0Aworld%20understanding%20in%20three%20stages%3A%20open-ended%20referring%20segmentation%2C%0Agrounded%20grasp%20planning%20and%20grasp%20ranking%20via%20contact%20reasoning%2C%20all%20of%20which%0Acan%20be%20applied%20zero-shot%20via%20suitable%20visual%20prompting%20mechanisms.%20We%20conduct%0Aextensive%20evaluation%20in%20cluttered%20indoor%20scene%20datasets%20to%20showcase%20OWG%27s%0Arobustness%20in%20grounding%20from%20open-ended%20language%2C%20as%20well%20as%20open-world%20robotic%0Agrasping%20experiments%20in%20both%20simulation%20and%20hardware%20that%20demonstrate%20superior%0Aperformance%20compared%20to%20previous%20supervised%20and%20zero-shot%20LLM-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18722v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Open-World%2520Grasping%2520with%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DGeorgios%2520Tziafas%2520and%2520Hamidreza%2520Kasaei%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520grasp%2520objects%2520in-the-wild%2520from%2520open-ended%2520language%250Ainstructions%2520constitutes%2520a%2520fundamental%2520challenge%2520in%2520robotics.%2520An%2520open-world%250Agrasping%2520system%2520should%2520be%2520able%2520to%2520combine%2520high-level%2520contextual%2520with%2520low-level%250Aphysical-geometric%2520reasoning%2520in%2520order%2520to%2520be%2520applicable%2520in%2520arbitrary%2520scenarios.%250ARecent%2520works%2520exploit%2520the%2520web-scale%2520knowledge%2520inherent%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520plan%2520and%2520reason%2520in%2520robotic%2520context%252C%2520but%2520rely%2520on%2520external%2520vision%2520and%250Aaction%2520models%2520to%2520ground%2520such%2520knowledge%2520into%2520the%2520environment%2520and%2520parameterize%250Aactuation.%2520This%2520setup%2520suffers%2520from%2520two%2520major%2520bottlenecks%253A%2520a%2529%2520the%2520LLM%2527s%250Areasoning%2520capacity%2520is%2520constrained%2520by%2520the%2520quality%2520of%2520visual%2520grounding%252C%2520and%2520b%2529%250ALLMs%2520do%2520not%2520contain%2520low-level%2520spatial%2520understanding%2520of%2520the%2520world%252C%2520which%2520is%250Aessential%2520for%2520grasping%2520in%2520contact-rich%2520scenarios.%2520In%2520this%2520work%2520we%2520demonstrate%250Athat%2520modern%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520capable%2520of%2520tackling%2520such%250Alimitations%252C%2520as%2520they%2520are%2520implicitly%2520grounded%2520and%2520can%2520jointly%2520reason%2520about%250Asemantics%2520and%2520geometry.%2520We%2520propose%2520OWG%252C%2520an%2520open-world%2520grasping%2520pipeline%2520that%250Acombines%2520VLMs%2520with%2520segmentation%2520and%2520grasp%2520synthesis%2520models%2520to%2520unlock%2520grounded%250Aworld%2520understanding%2520in%2520three%2520stages%253A%2520open-ended%2520referring%2520segmentation%252C%250Agrounded%2520grasp%2520planning%2520and%2520grasp%2520ranking%2520via%2520contact%2520reasoning%252C%2520all%2520of%2520which%250Acan%2520be%2520applied%2520zero-shot%2520via%2520suitable%2520visual%2520prompting%2520mechanisms.%2520We%2520conduct%250Aextensive%2520evaluation%2520in%2520cluttered%2520indoor%2520scene%2520datasets%2520to%2520showcase%2520OWG%2527s%250Arobustness%2520in%2520grounding%2520from%2520open-ended%2520language%252C%2520as%2520well%2520as%2520open-world%2520robotic%250Agrasping%2520experiments%2520in%2520both%2520simulation%2520and%2520hardware%2520that%2520demonstrate%2520superior%250Aperformance%2520compared%2520to%2520previous%2520supervised%2520and%2520zero-shot%2520LLM-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18722v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Open-World%20Grasping%20with%20Large%20Vision-Language%20Models&entry.906535625=Georgios%20Tziafas%20and%20Hamidreza%20Kasaei&entry.1292438233=%20%20The%20ability%20to%20grasp%20objects%20in-the-wild%20from%20open-ended%20language%0Ainstructions%20constitutes%20a%20fundamental%20challenge%20in%20robotics.%20An%20open-world%0Agrasping%20system%20should%20be%20able%20to%20combine%20high-level%20contextual%20with%20low-level%0Aphysical-geometric%20reasoning%20in%20order%20to%20be%20applicable%20in%20arbitrary%20scenarios.%0ARecent%20works%20exploit%20the%20web-scale%20knowledge%20inherent%20in%20large%20language%20models%0A%28LLMs%29%20to%20plan%20and%20reason%20in%20robotic%20context%2C%20but%20rely%20on%20external%20vision%20and%0Aaction%20models%20to%20ground%20such%20knowledge%20into%20the%20environment%20and%20parameterize%0Aactuation.%20This%20setup%20suffers%20from%20two%20major%20bottlenecks%3A%20a%29%20the%20LLM%27s%0Areasoning%20capacity%20is%20constrained%20by%20the%20quality%20of%20visual%20grounding%2C%20and%20b%29%0ALLMs%20do%20not%20contain%20low-level%20spatial%20understanding%20of%20the%20world%2C%20which%20is%0Aessential%20for%20grasping%20in%20contact-rich%20scenarios.%20In%20this%20work%20we%20demonstrate%0Athat%20modern%20vision-language%20models%20%28VLMs%29%20are%20capable%20of%20tackling%20such%0Alimitations%2C%20as%20they%20are%20implicitly%20grounded%20and%20can%20jointly%20reason%20about%0Asemantics%20and%20geometry.%20We%20propose%20OWG%2C%20an%20open-world%20grasping%20pipeline%20that%0Acombines%20VLMs%20with%20segmentation%20and%20grasp%20synthesis%20models%20to%20unlock%20grounded%0Aworld%20understanding%20in%20three%20stages%3A%20open-ended%20referring%20segmentation%2C%0Agrounded%20grasp%20planning%20and%20grasp%20ranking%20via%20contact%20reasoning%2C%20all%20of%20which%0Acan%20be%20applied%20zero-shot%20via%20suitable%20visual%20prompting%20mechanisms.%20We%20conduct%0Aextensive%20evaluation%20in%20cluttered%20indoor%20scene%20datasets%20to%20showcase%20OWG%27s%0Arobustness%20in%20grounding%20from%20open-ended%20language%2C%20as%20well%20as%20open-world%20robotic%0Agrasping%20experiments%20in%20both%20simulation%20and%20hardware%20that%20demonstrate%20superior%0Aperformance%20compared%20to%20previous%20supervised%20and%20zero-shot%20LLM-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18722v3&entry.124074799=Read"},
{"title": "Refusal in Language Models Is Mediated by a Single Direction", "author": "Andy Arditi and Oscar Obeso and Aaquib Syed and Daniel Paleka and Nina Panickssery and Wes Gurnee and Neel Nanda", "abstract": "  Conversational large language models are fine-tuned for both\ninstruction-following and safety, resulting in models that obey benign requests\nbut refuse harmful ones. While this refusal behavior is widespread across chat\nmodels, its underlying mechanisms remain poorly understood. In this work, we\nshow that refusal is mediated by a one-dimensional subspace, across 13 popular\nopen-source chat models up to 72B parameters in size. Specifically, for each\nmodel, we find a single direction such that erasing this direction from the\nmodel's residual stream activations prevents it from refusing harmful\ninstructions, while adding this direction elicits refusal on even harmless\ninstructions. Leveraging this insight, we propose a novel white-box jailbreak\nmethod that surgically disables refusal with minimal effect on other\ncapabilities. Finally, we mechanistically analyze how adversarial suffixes\nsuppress propagation of the refusal-mediating direction. Our findings\nunderscore the brittleness of current safety fine-tuning methods. More broadly,\nour work showcases how an understanding of model internals can be leveraged to\ndevelop practical methods for controlling model behavior.\n", "link": "http://arxiv.org/abs/2406.11717v2", "date": "2024-07-15", "relevancy": 1.3427, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4493}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refusal%20in%20Language%20Models%20Is%20Mediated%20by%20a%20Single%20Direction&body=Title%3A%20Refusal%20in%20Language%20Models%20Is%20Mediated%20by%20a%20Single%20Direction%0AAuthor%3A%20Andy%20Arditi%20and%20Oscar%20Obeso%20and%20Aaquib%20Syed%20and%20Daniel%20Paleka%20and%20Nina%20Panickssery%20and%20Wes%20Gurnee%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Conversational%20large%20language%20models%20are%20fine-tuned%20for%20both%0Ainstruction-following%20and%20safety%2C%20resulting%20in%20models%20that%20obey%20benign%20requests%0Abut%20refuse%20harmful%20ones.%20While%20this%20refusal%20behavior%20is%20widespread%20across%20chat%0Amodels%2C%20its%20underlying%20mechanisms%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%0Ashow%20that%20refusal%20is%20mediated%20by%20a%20one-dimensional%20subspace%2C%20across%2013%20popular%0Aopen-source%20chat%20models%20up%20to%2072B%20parameters%20in%20size.%20Specifically%2C%20for%20each%0Amodel%2C%20we%20find%20a%20single%20direction%20such%20that%20erasing%20this%20direction%20from%20the%0Amodel%27s%20residual%20stream%20activations%20prevents%20it%20from%20refusing%20harmful%0Ainstructions%2C%20while%20adding%20this%20direction%20elicits%20refusal%20on%20even%20harmless%0Ainstructions.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20novel%20white-box%20jailbreak%0Amethod%20that%20surgically%20disables%20refusal%20with%20minimal%20effect%20on%20other%0Acapabilities.%20Finally%2C%20we%20mechanistically%20analyze%20how%20adversarial%20suffixes%0Asuppress%20propagation%20of%20the%20refusal-mediating%20direction.%20Our%20findings%0Aunderscore%20the%20brittleness%20of%20current%20safety%20fine-tuning%20methods.%20More%20broadly%2C%0Aour%20work%20showcases%20how%20an%20understanding%20of%20model%20internals%20can%20be%20leveraged%20to%0Adevelop%20practical%20methods%20for%20controlling%20model%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefusal%2520in%2520Language%2520Models%2520Is%2520Mediated%2520by%2520a%2520Single%2520Direction%26entry.906535625%3DAndy%2520Arditi%2520and%2520Oscar%2520Obeso%2520and%2520Aaquib%2520Syed%2520and%2520Daniel%2520Paleka%2520and%2520Nina%2520Panickssery%2520and%2520Wes%2520Gurnee%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Conversational%2520large%2520language%2520models%2520are%2520fine-tuned%2520for%2520both%250Ainstruction-following%2520and%2520safety%252C%2520resulting%2520in%2520models%2520that%2520obey%2520benign%2520requests%250Abut%2520refuse%2520harmful%2520ones.%2520While%2520this%2520refusal%2520behavior%2520is%2520widespread%2520across%2520chat%250Amodels%252C%2520its%2520underlying%2520mechanisms%2520remain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%250Ashow%2520that%2520refusal%2520is%2520mediated%2520by%2520a%2520one-dimensional%2520subspace%252C%2520across%252013%2520popular%250Aopen-source%2520chat%2520models%2520up%2520to%252072B%2520parameters%2520in%2520size.%2520Specifically%252C%2520for%2520each%250Amodel%252C%2520we%2520find%2520a%2520single%2520direction%2520such%2520that%2520erasing%2520this%2520direction%2520from%2520the%250Amodel%2527s%2520residual%2520stream%2520activations%2520prevents%2520it%2520from%2520refusing%2520harmful%250Ainstructions%252C%2520while%2520adding%2520this%2520direction%2520elicits%2520refusal%2520on%2520even%2520harmless%250Ainstructions.%2520Leveraging%2520this%2520insight%252C%2520we%2520propose%2520a%2520novel%2520white-box%2520jailbreak%250Amethod%2520that%2520surgically%2520disables%2520refusal%2520with%2520minimal%2520effect%2520on%2520other%250Acapabilities.%2520Finally%252C%2520we%2520mechanistically%2520analyze%2520how%2520adversarial%2520suffixes%250Asuppress%2520propagation%2520of%2520the%2520refusal-mediating%2520direction.%2520Our%2520findings%250Aunderscore%2520the%2520brittleness%2520of%2520current%2520safety%2520fine-tuning%2520methods.%2520More%2520broadly%252C%250Aour%2520work%2520showcases%2520how%2520an%2520understanding%2520of%2520model%2520internals%2520can%2520be%2520leveraged%2520to%250Adevelop%2520practical%2520methods%2520for%2520controlling%2520model%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refusal%20in%20Language%20Models%20Is%20Mediated%20by%20a%20Single%20Direction&entry.906535625=Andy%20Arditi%20and%20Oscar%20Obeso%20and%20Aaquib%20Syed%20and%20Daniel%20Paleka%20and%20Nina%20Panickssery%20and%20Wes%20Gurnee%20and%20Neel%20Nanda&entry.1292438233=%20%20Conversational%20large%20language%20models%20are%20fine-tuned%20for%20both%0Ainstruction-following%20and%20safety%2C%20resulting%20in%20models%20that%20obey%20benign%20requests%0Abut%20refuse%20harmful%20ones.%20While%20this%20refusal%20behavior%20is%20widespread%20across%20chat%0Amodels%2C%20its%20underlying%20mechanisms%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%0Ashow%20that%20refusal%20is%20mediated%20by%20a%20one-dimensional%20subspace%2C%20across%2013%20popular%0Aopen-source%20chat%20models%20up%20to%2072B%20parameters%20in%20size.%20Specifically%2C%20for%20each%0Amodel%2C%20we%20find%20a%20single%20direction%20such%20that%20erasing%20this%20direction%20from%20the%0Amodel%27s%20residual%20stream%20activations%20prevents%20it%20from%20refusing%20harmful%0Ainstructions%2C%20while%20adding%20this%20direction%20elicits%20refusal%20on%20even%20harmless%0Ainstructions.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20novel%20white-box%20jailbreak%0Amethod%20that%20surgically%20disables%20refusal%20with%20minimal%20effect%20on%20other%0Acapabilities.%20Finally%2C%20we%20mechanistically%20analyze%20how%20adversarial%20suffixes%0Asuppress%20propagation%20of%20the%20refusal-mediating%20direction.%20Our%20findings%0Aunderscore%20the%20brittleness%20of%20current%20safety%20fine-tuning%20methods.%20More%20broadly%2C%0Aour%20work%20showcases%20how%20an%20understanding%20of%20model%20internals%20can%20be%20leveraged%20to%0Adevelop%20practical%20methods%20for%20controlling%20model%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11717v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


