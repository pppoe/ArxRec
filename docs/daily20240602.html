<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240530.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single\n  Image", "author": "Kailu Wu and Fangfu Liu and Zhihan Cai and Runjie Yan and Hanyang Wang and Yating Hu and Yueqi Duan and Kaisheng Ma", "abstract": "  In this work, we introduce Unique3D, a novel image-to-3D framework for\nefficiently generating high-quality 3D meshes from single-view images,\nfeaturing state-of-the-art generation fidelity and strong generalizability.\nPrevious methods based on Score Distillation Sampling (SDS) can produce\ndiversified 3D results by distilling 3D knowledge from large 2D diffusion\nmodels, but they usually suffer from long per-case optimization time with\ninconsistent issues. Recent works address the problem and generate better 3D\nresults either by finetuning a multi-view diffusion model or training a fast\nfeed-forward model. However, they still lack intricate textures and complex\ngeometries due to inconsistency and limited generated resolution. To\nsimultaneously achieve high fidelity, consistency, and efficiency in single\nimage-to-3D, we propose a novel framework Unique3D that includes a multi-view\ndiffusion model with a corresponding normal diffusion model to generate\nmulti-view images with their normal maps, a multi-level upscale process to\nprogressively improve the resolution of generated orthographic multi-views, as\nwell as an instant and consistent mesh reconstruction algorithm called ISOMER,\nwhich fully integrates the color and geometric priors into mesh results.\nExtensive experiments demonstrate that our Unique3D significantly outperforms\nother image-to-3D baselines in terms of geometric and textural details.\n", "link": "http://arxiv.org/abs/2405.20343v1", "date": "2024-05-30", "relevancy": 3.4514, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7215}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7215}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unique3D%3A%20High-Quality%20and%20Efficient%203D%20Mesh%20Generation%20from%20a%20Single%0A%20%20Image&body=Title%3A%20Unique3D%3A%20High-Quality%20and%20Efficient%203D%20Mesh%20Generation%20from%20a%20Single%0A%20%20Image%0AAuthor%3A%20Kailu%20Wu%20and%20Fangfu%20Liu%20and%20Zhihan%20Cai%20and%20Runjie%20Yan%20and%20Hanyang%20Wang%20and%20Yating%20Hu%20and%20Yueqi%20Duan%20and%20Kaisheng%20Ma%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Unique3D%2C%20a%20novel%20image-to-3D%20framework%20for%0Aefficiently%20generating%20high-quality%203D%20meshes%20from%20single-view%20images%2C%0Afeaturing%20state-of-the-art%20generation%20fidelity%20and%20strong%20generalizability.%0APrevious%20methods%20based%20on%20Score%20Distillation%20Sampling%20%28SDS%29%20can%20produce%0Adiversified%203D%20results%20by%20distilling%203D%20knowledge%20from%20large%202D%20diffusion%0Amodels%2C%20but%20they%20usually%20suffer%20from%20long%20per-case%20optimization%20time%20with%0Ainconsistent%20issues.%20Recent%20works%20address%20the%20problem%20and%20generate%20better%203D%0Aresults%20either%20by%20finetuning%20a%20multi-view%20diffusion%20model%20or%20training%20a%20fast%0Afeed-forward%20model.%20However%2C%20they%20still%20lack%20intricate%20textures%20and%20complex%0Ageometries%20due%20to%20inconsistency%20and%20limited%20generated%20resolution.%20To%0Asimultaneously%20achieve%20high%20fidelity%2C%20consistency%2C%20and%20efficiency%20in%20single%0Aimage-to-3D%2C%20we%20propose%20a%20novel%20framework%20Unique3D%20that%20includes%20a%20multi-view%0Adiffusion%20model%20with%20a%20corresponding%20normal%20diffusion%20model%20to%20generate%0Amulti-view%20images%20with%20their%20normal%20maps%2C%20a%20multi-level%20upscale%20process%20to%0Aprogressively%20improve%20the%20resolution%20of%20generated%20orthographic%20multi-views%2C%20as%0Awell%20as%20an%20instant%20and%20consistent%20mesh%20reconstruction%20algorithm%20called%20ISOMER%2C%0Awhich%20fully%20integrates%20the%20color%20and%20geometric%20priors%20into%20mesh%20results.%0AExtensive%20experiments%20demonstrate%20that%20our%20Unique3D%20significantly%20outperforms%0Aother%20image-to-3D%20baselines%20in%20terms%20of%20geometric%20and%20textural%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnique3D%253A%2520High-Quality%2520and%2520Efficient%25203D%2520Mesh%2520Generation%2520from%2520a%2520Single%250A%2520%2520Image%26entry.906535625%3DKailu%2520Wu%2520and%2520Fangfu%2520Liu%2520and%2520Zhihan%2520Cai%2520and%2520Runjie%2520Yan%2520and%2520Hanyang%2520Wang%2520and%2520Yating%2520Hu%2520and%2520Yueqi%2520Duan%2520and%2520Kaisheng%2520Ma%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Unique3D%252C%2520a%2520novel%2520image-to-3D%2520framework%2520for%250Aefficiently%2520generating%2520high-quality%25203D%2520meshes%2520from%2520single-view%2520images%252C%250Afeaturing%2520state-of-the-art%2520generation%2520fidelity%2520and%2520strong%2520generalizability.%250APrevious%2520methods%2520based%2520on%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520can%2520produce%250Adiversified%25203D%2520results%2520by%2520distilling%25203D%2520knowledge%2520from%2520large%25202D%2520diffusion%250Amodels%252C%2520but%2520they%2520usually%2520suffer%2520from%2520long%2520per-case%2520optimization%2520time%2520with%250Ainconsistent%2520issues.%2520Recent%2520works%2520address%2520the%2520problem%2520and%2520generate%2520better%25203D%250Aresults%2520either%2520by%2520finetuning%2520a%2520multi-view%2520diffusion%2520model%2520or%2520training%2520a%2520fast%250Afeed-forward%2520model.%2520However%252C%2520they%2520still%2520lack%2520intricate%2520textures%2520and%2520complex%250Ageometries%2520due%2520to%2520inconsistency%2520and%2520limited%2520generated%2520resolution.%2520To%250Asimultaneously%2520achieve%2520high%2520fidelity%252C%2520consistency%252C%2520and%2520efficiency%2520in%2520single%250Aimage-to-3D%252C%2520we%2520propose%2520a%2520novel%2520framework%2520Unique3D%2520that%2520includes%2520a%2520multi-view%250Adiffusion%2520model%2520with%2520a%2520corresponding%2520normal%2520diffusion%2520model%2520to%2520generate%250Amulti-view%2520images%2520with%2520their%2520normal%2520maps%252C%2520a%2520multi-level%2520upscale%2520process%2520to%250Aprogressively%2520improve%2520the%2520resolution%2520of%2520generated%2520orthographic%2520multi-views%252C%2520as%250Awell%2520as%2520an%2520instant%2520and%2520consistent%2520mesh%2520reconstruction%2520algorithm%2520called%2520ISOMER%252C%250Awhich%2520fully%2520integrates%2520the%2520color%2520and%2520geometric%2520priors%2520into%2520mesh%2520results.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520Unique3D%2520significantly%2520outperforms%250Aother%2520image-to-3D%2520baselines%2520in%2520terms%2520of%2520geometric%2520and%2520textural%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unique3D%3A%20High-Quality%20and%20Efficient%203D%20Mesh%20Generation%20from%20a%20Single%0A%20%20Image&entry.906535625=Kailu%20Wu%20and%20Fangfu%20Liu%20and%20Zhihan%20Cai%20and%20Runjie%20Yan%20and%20Hanyang%20Wang%20and%20Yating%20Hu%20and%20Yueqi%20Duan%20and%20Kaisheng%20Ma&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Unique3D%2C%20a%20novel%20image-to-3D%20framework%20for%0Aefficiently%20generating%20high-quality%203D%20meshes%20from%20single-view%20images%2C%0Afeaturing%20state-of-the-art%20generation%20fidelity%20and%20strong%20generalizability.%0APrevious%20methods%20based%20on%20Score%20Distillation%20Sampling%20%28SDS%29%20can%20produce%0Adiversified%203D%20results%20by%20distilling%203D%20knowledge%20from%20large%202D%20diffusion%0Amodels%2C%20but%20they%20usually%20suffer%20from%20long%20per-case%20optimization%20time%20with%0Ainconsistent%20issues.%20Recent%20works%20address%20the%20problem%20and%20generate%20better%203D%0Aresults%20either%20by%20finetuning%20a%20multi-view%20diffusion%20model%20or%20training%20a%20fast%0Afeed-forward%20model.%20However%2C%20they%20still%20lack%20intricate%20textures%20and%20complex%0Ageometries%20due%20to%20inconsistency%20and%20limited%20generated%20resolution.%20To%0Asimultaneously%20achieve%20high%20fidelity%2C%20consistency%2C%20and%20efficiency%20in%20single%0Aimage-to-3D%2C%20we%20propose%20a%20novel%20framework%20Unique3D%20that%20includes%20a%20multi-view%0Adiffusion%20model%20with%20a%20corresponding%20normal%20diffusion%20model%20to%20generate%0Amulti-view%20images%20with%20their%20normal%20maps%2C%20a%20multi-level%20upscale%20process%20to%0Aprogressively%20improve%20the%20resolution%20of%20generated%20orthographic%20multi-views%2C%20as%0Awell%20as%20an%20instant%20and%20consistent%20mesh%20reconstruction%20algorithm%20called%20ISOMER%2C%0Awhich%20fully%20integrates%20the%20color%20and%20geometric%20priors%20into%20mesh%20results.%0AExtensive%20experiments%20demonstrate%20that%20our%20Unique3D%20significantly%20outperforms%0Aother%20image-to-3D%20baselines%20in%20terms%20of%20geometric%20and%20textural%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20343v1&entry.124074799=Read"},
{"title": "$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation", "author": "Weitian Zhang and Yichao Yan and Yunhui Liu and Xingdong Sheng and Xiaokang Yang", "abstract": "  This paper aims to introduce 3D Gaussian for efficient, expressive, and\neditable digital avatar generation. This task faces two major challenges: (1)\nThe unstructured nature of 3D Gaussian makes it incompatible with current\ngeneration pipelines; (2) the expressive animation of 3D Gaussian in a\ngenerative setting that involves training with multiple subjects remains\nunexplored. In this paper, we propose a novel avatar generation method named\n$E^3$Gen, to effectively address these challenges. First, we propose a novel\ngenerative UV features plane representation that encodes unstructured 3D\nGaussian onto a structured 2D UV space defined by the SMPL-X parametric model.\nThis novel representation not only preserves the representation ability of the\noriginal 3D Gaussian but also introduces a shared structure among subjects to\nenable generative learning of the diffusion model. To tackle the second\nchallenge, we propose a part-aware deformation module to achieve robust and\naccurate full-body expressive pose control. Extensive experiments demonstrate\nthat our method achieves superior performance in avatar generation and enables\nexpressive full-body pose control and editing. Our project page is\nhttps://olivia23333.github.io/E3Gen.\n", "link": "http://arxiv.org/abs/2405.19203v2", "date": "2024-05-30", "relevancy": 3.3509, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7088}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7088}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24E%5E%7B3%7D%24Gen%3A%20Efficient%2C%20Expressive%20and%20Editable%20Avatars%20Generation&body=Title%3A%20%24E%5E%7B3%7D%24Gen%3A%20Efficient%2C%20Expressive%20and%20Editable%20Avatars%20Generation%0AAuthor%3A%20Weitian%20Zhang%20and%20Yichao%20Yan%20and%20Yunhui%20Liu%20and%20Xingdong%20Sheng%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20introduce%203D%20Gaussian%20for%20efficient%2C%20expressive%2C%20and%0Aeditable%20digital%20avatar%20generation.%20This%20task%20faces%20two%20major%20challenges%3A%20%281%29%0AThe%20unstructured%20nature%20of%203D%20Gaussian%20makes%20it%20incompatible%20with%20current%0Ageneration%20pipelines%3B%20%282%29%20the%20expressive%20animation%20of%203D%20Gaussian%20in%20a%0Agenerative%20setting%20that%20involves%20training%20with%20multiple%20subjects%20remains%0Aunexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20avatar%20generation%20method%20named%0A%24E%5E3%24Gen%2C%20to%20effectively%20address%20these%20challenges.%20First%2C%20we%20propose%20a%20novel%0Agenerative%20UV%20features%20plane%20representation%20that%20encodes%20unstructured%203D%0AGaussian%20onto%20a%20structured%202D%20UV%20space%20defined%20by%20the%20SMPL-X%20parametric%20model.%0AThis%20novel%20representation%20not%20only%20preserves%20the%20representation%20ability%20of%20the%0Aoriginal%203D%20Gaussian%20but%20also%20introduces%20a%20shared%20structure%20among%20subjects%20to%0Aenable%20generative%20learning%20of%20the%20diffusion%20model.%20To%20tackle%20the%20second%0Achallenge%2C%20we%20propose%20a%20part-aware%20deformation%20module%20to%20achieve%20robust%20and%0Aaccurate%20full-body%20expressive%20pose%20control.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20achieves%20superior%20performance%20in%20avatar%20generation%20and%20enables%0Aexpressive%20full-body%20pose%20control%20and%20editing.%20Our%20project%20page%20is%0Ahttps%3A//olivia23333.github.io/E3Gen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19203v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524E%255E%257B3%257D%2524Gen%253A%2520Efficient%252C%2520Expressive%2520and%2520Editable%2520Avatars%2520Generation%26entry.906535625%3DWeitian%2520Zhang%2520and%2520Yichao%2520Yan%2520and%2520Yunhui%2520Liu%2520and%2520Xingdong%2520Sheng%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520introduce%25203D%2520Gaussian%2520for%2520efficient%252C%2520expressive%252C%2520and%250Aeditable%2520digital%2520avatar%2520generation.%2520This%2520task%2520faces%2520two%2520major%2520challenges%253A%2520%25281%2529%250AThe%2520unstructured%2520nature%2520of%25203D%2520Gaussian%2520makes%2520it%2520incompatible%2520with%2520current%250Ageneration%2520pipelines%253B%2520%25282%2529%2520the%2520expressive%2520animation%2520of%25203D%2520Gaussian%2520in%2520a%250Agenerative%2520setting%2520that%2520involves%2520training%2520with%2520multiple%2520subjects%2520remains%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520avatar%2520generation%2520method%2520named%250A%2524E%255E3%2524Gen%252C%2520to%2520effectively%2520address%2520these%2520challenges.%2520First%252C%2520we%2520propose%2520a%2520novel%250Agenerative%2520UV%2520features%2520plane%2520representation%2520that%2520encodes%2520unstructured%25203D%250AGaussian%2520onto%2520a%2520structured%25202D%2520UV%2520space%2520defined%2520by%2520the%2520SMPL-X%2520parametric%2520model.%250AThis%2520novel%2520representation%2520not%2520only%2520preserves%2520the%2520representation%2520ability%2520of%2520the%250Aoriginal%25203D%2520Gaussian%2520but%2520also%2520introduces%2520a%2520shared%2520structure%2520among%2520subjects%2520to%250Aenable%2520generative%2520learning%2520of%2520the%2520diffusion%2520model.%2520To%2520tackle%2520the%2520second%250Achallenge%252C%2520we%2520propose%2520a%2520part-aware%2520deformation%2520module%2520to%2520achieve%2520robust%2520and%250Aaccurate%2520full-body%2520expressive%2520pose%2520control.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520superior%2520performance%2520in%2520avatar%2520generation%2520and%2520enables%250Aexpressive%2520full-body%2520pose%2520control%2520and%2520editing.%2520Our%2520project%2520page%2520is%250Ahttps%253A//olivia23333.github.io/E3Gen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19203v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24E%5E%7B3%7D%24Gen%3A%20Efficient%2C%20Expressive%20and%20Editable%20Avatars%20Generation&entry.906535625=Weitian%20Zhang%20and%20Yichao%20Yan%20and%20Yunhui%20Liu%20and%20Xingdong%20Sheng%20and%20Xiaokang%20Yang&entry.1292438233=%20%20This%20paper%20aims%20to%20introduce%203D%20Gaussian%20for%20efficient%2C%20expressive%2C%20and%0Aeditable%20digital%20avatar%20generation.%20This%20task%20faces%20two%20major%20challenges%3A%20%281%29%0AThe%20unstructured%20nature%20of%203D%20Gaussian%20makes%20it%20incompatible%20with%20current%0Ageneration%20pipelines%3B%20%282%29%20the%20expressive%20animation%20of%203D%20Gaussian%20in%20a%0Agenerative%20setting%20that%20involves%20training%20with%20multiple%20subjects%20remains%0Aunexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20avatar%20generation%20method%20named%0A%24E%5E3%24Gen%2C%20to%20effectively%20address%20these%20challenges.%20First%2C%20we%20propose%20a%20novel%0Agenerative%20UV%20features%20plane%20representation%20that%20encodes%20unstructured%203D%0AGaussian%20onto%20a%20structured%202D%20UV%20space%20defined%20by%20the%20SMPL-X%20parametric%20model.%0AThis%20novel%20representation%20not%20only%20preserves%20the%20representation%20ability%20of%20the%0Aoriginal%203D%20Gaussian%20but%20also%20introduces%20a%20shared%20structure%20among%20subjects%20to%0Aenable%20generative%20learning%20of%20the%20diffusion%20model.%20To%20tackle%20the%20second%0Achallenge%2C%20we%20propose%20a%20part-aware%20deformation%20module%20to%20achieve%20robust%20and%0Aaccurate%20full-body%20expressive%20pose%20control.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20achieves%20superior%20performance%20in%20avatar%20generation%20and%20enables%0Aexpressive%20full-body%20pose%20control%20and%20editing.%20Our%20project%20page%20is%0Ahttps%3A//olivia23333.github.io/E3Gen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19203v2&entry.124074799=Read"},
{"title": "Object-centric Reconstruction and Tracking of Dynamic Unknown Objects\n  using 3D Gaussian Splatting", "author": "Kuldeep R Barad and Antoine Richard and Jan Dentler and Miguel Olivares-Mendez and Carol Martinez", "abstract": "  Generalizable perception is one of the pillars of high-level autonomy in\nspace robotics. Estimating the structure and motion of unknown objects in\ndynamic environments is fundamental for such autonomous systems. Traditionally,\nthe solutions have relied on prior knowledge of target objects, multiple\ndisparate representations, or low-fidelity outputs unsuitable for robotic\noperations. This work proposes a novel approach to incrementally reconstruct\nand track a dynamic unknown object using a unified representation -- a set of\n3D Gaussian blobs that describe its geometry and appearance. The differentiable\n3D Gaussian Splatting framework is adapted to a dynamic object-centric setting.\nThe input to the pipeline is a sequential set of RGB-D images. 3D\nreconstruction and 6-DoF pose tracking tasks are tackled using first-order\ngradient-based optimization. The formulation is simple, requires no\npre-training, assumes no prior knowledge of the object or its motion, and is\nsuitable for online applications. The proposed approach is validated on a\ndataset of 10 unknown spacecraft of diverse geometry and texture under\narbitrary relative motion. The experiments demonstrate successful 3D\nreconstruction and accurate 6-DoF tracking of the target object in proximity\noperations over a short to medium duration. The causes of tracking drift are\ndiscussed and potential solutions are outlined.\n", "link": "http://arxiv.org/abs/2405.20104v1", "date": "2024-05-30", "relevancy": 3.3208, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7268}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6764}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-centric%20Reconstruction%20and%20Tracking%20of%20Dynamic%20Unknown%20Objects%0A%20%20using%203D%20Gaussian%20Splatting&body=Title%3A%20Object-centric%20Reconstruction%20and%20Tracking%20of%20Dynamic%20Unknown%20Objects%0A%20%20using%203D%20Gaussian%20Splatting%0AAuthor%3A%20Kuldeep%20R%20Barad%20and%20Antoine%20Richard%20and%20Jan%20Dentler%20and%20Miguel%20Olivares-Mendez%20and%20Carol%20Martinez%0AAbstract%3A%20%20%20Generalizable%20perception%20is%20one%20of%20the%20pillars%20of%20high-level%20autonomy%20in%0Aspace%20robotics.%20Estimating%20the%20structure%20and%20motion%20of%20unknown%20objects%20in%0Adynamic%20environments%20is%20fundamental%20for%20such%20autonomous%20systems.%20Traditionally%2C%0Athe%20solutions%20have%20relied%20on%20prior%20knowledge%20of%20target%20objects%2C%20multiple%0Adisparate%20representations%2C%20or%20low-fidelity%20outputs%20unsuitable%20for%20robotic%0Aoperations.%20This%20work%20proposes%20a%20novel%20approach%20to%20incrementally%20reconstruct%0Aand%20track%20a%20dynamic%20unknown%20object%20using%20a%20unified%20representation%20--%20a%20set%20of%0A3D%20Gaussian%20blobs%20that%20describe%20its%20geometry%20and%20appearance.%20The%20differentiable%0A3D%20Gaussian%20Splatting%20framework%20is%20adapted%20to%20a%20dynamic%20object-centric%20setting.%0AThe%20input%20to%20the%20pipeline%20is%20a%20sequential%20set%20of%20RGB-D%20images.%203D%0Areconstruction%20and%206-DoF%20pose%20tracking%20tasks%20are%20tackled%20using%20first-order%0Agradient-based%20optimization.%20The%20formulation%20is%20simple%2C%20requires%20no%0Apre-training%2C%20assumes%20no%20prior%20knowledge%20of%20the%20object%20or%20its%20motion%2C%20and%20is%0Asuitable%20for%20online%20applications.%20The%20proposed%20approach%20is%20validated%20on%20a%0Adataset%20of%2010%20unknown%20spacecraft%20of%20diverse%20geometry%20and%20texture%20under%0Aarbitrary%20relative%20motion.%20The%20experiments%20demonstrate%20successful%203D%0Areconstruction%20and%20accurate%206-DoF%20tracking%20of%20the%20target%20object%20in%20proximity%0Aoperations%20over%20a%20short%20to%20medium%20duration.%20The%20causes%20of%20tracking%20drift%20are%0Adiscussed%20and%20potential%20solutions%20are%20outlined.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-centric%2520Reconstruction%2520and%2520Tracking%2520of%2520Dynamic%2520Unknown%2520Objects%250A%2520%2520using%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DKuldeep%2520R%2520Barad%2520and%2520Antoine%2520Richard%2520and%2520Jan%2520Dentler%2520and%2520Miguel%2520Olivares-Mendez%2520and%2520Carol%2520Martinez%26entry.1292438233%3D%2520%2520Generalizable%2520perception%2520is%2520one%2520of%2520the%2520pillars%2520of%2520high-level%2520autonomy%2520in%250Aspace%2520robotics.%2520Estimating%2520the%2520structure%2520and%2520motion%2520of%2520unknown%2520objects%2520in%250Adynamic%2520environments%2520is%2520fundamental%2520for%2520such%2520autonomous%2520systems.%2520Traditionally%252C%250Athe%2520solutions%2520have%2520relied%2520on%2520prior%2520knowledge%2520of%2520target%2520objects%252C%2520multiple%250Adisparate%2520representations%252C%2520or%2520low-fidelity%2520outputs%2520unsuitable%2520for%2520robotic%250Aoperations.%2520This%2520work%2520proposes%2520a%2520novel%2520approach%2520to%2520incrementally%2520reconstruct%250Aand%2520track%2520a%2520dynamic%2520unknown%2520object%2520using%2520a%2520unified%2520representation%2520--%2520a%2520set%2520of%250A3D%2520Gaussian%2520blobs%2520that%2520describe%2520its%2520geometry%2520and%2520appearance.%2520The%2520differentiable%250A3D%2520Gaussian%2520Splatting%2520framework%2520is%2520adapted%2520to%2520a%2520dynamic%2520object-centric%2520setting.%250AThe%2520input%2520to%2520the%2520pipeline%2520is%2520a%2520sequential%2520set%2520of%2520RGB-D%2520images.%25203D%250Areconstruction%2520and%25206-DoF%2520pose%2520tracking%2520tasks%2520are%2520tackled%2520using%2520first-order%250Agradient-based%2520optimization.%2520The%2520formulation%2520is%2520simple%252C%2520requires%2520no%250Apre-training%252C%2520assumes%2520no%2520prior%2520knowledge%2520of%2520the%2520object%2520or%2520its%2520motion%252C%2520and%2520is%250Asuitable%2520for%2520online%2520applications.%2520The%2520proposed%2520approach%2520is%2520validated%2520on%2520a%250Adataset%2520of%252010%2520unknown%2520spacecraft%2520of%2520diverse%2520geometry%2520and%2520texture%2520under%250Aarbitrary%2520relative%2520motion.%2520The%2520experiments%2520demonstrate%2520successful%25203D%250Areconstruction%2520and%2520accurate%25206-DoF%2520tracking%2520of%2520the%2520target%2520object%2520in%2520proximity%250Aoperations%2520over%2520a%2520short%2520to%2520medium%2520duration.%2520The%2520causes%2520of%2520tracking%2520drift%2520are%250Adiscussed%2520and%2520potential%2520solutions%2520are%2520outlined.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-centric%20Reconstruction%20and%20Tracking%20of%20Dynamic%20Unknown%20Objects%0A%20%20using%203D%20Gaussian%20Splatting&entry.906535625=Kuldeep%20R%20Barad%20and%20Antoine%20Richard%20and%20Jan%20Dentler%20and%20Miguel%20Olivares-Mendez%20and%20Carol%20Martinez&entry.1292438233=%20%20Generalizable%20perception%20is%20one%20of%20the%20pillars%20of%20high-level%20autonomy%20in%0Aspace%20robotics.%20Estimating%20the%20structure%20and%20motion%20of%20unknown%20objects%20in%0Adynamic%20environments%20is%20fundamental%20for%20such%20autonomous%20systems.%20Traditionally%2C%0Athe%20solutions%20have%20relied%20on%20prior%20knowledge%20of%20target%20objects%2C%20multiple%0Adisparate%20representations%2C%20or%20low-fidelity%20outputs%20unsuitable%20for%20robotic%0Aoperations.%20This%20work%20proposes%20a%20novel%20approach%20to%20incrementally%20reconstruct%0Aand%20track%20a%20dynamic%20unknown%20object%20using%20a%20unified%20representation%20--%20a%20set%20of%0A3D%20Gaussian%20blobs%20that%20describe%20its%20geometry%20and%20appearance.%20The%20differentiable%0A3D%20Gaussian%20Splatting%20framework%20is%20adapted%20to%20a%20dynamic%20object-centric%20setting.%0AThe%20input%20to%20the%20pipeline%20is%20a%20sequential%20set%20of%20RGB-D%20images.%203D%0Areconstruction%20and%206-DoF%20pose%20tracking%20tasks%20are%20tackled%20using%20first-order%0Agradient-based%20optimization.%20The%20formulation%20is%20simple%2C%20requires%20no%0Apre-training%2C%20assumes%20no%20prior%20knowledge%20of%20the%20object%20or%20its%20motion%2C%20and%20is%0Asuitable%20for%20online%20applications.%20The%20proposed%20approach%20is%20validated%20on%20a%0Adataset%20of%2010%20unknown%20spacecraft%20of%20diverse%20geometry%20and%20texture%20under%0Aarbitrary%20relative%20motion.%20The%20experiments%20demonstrate%20successful%203D%0Areconstruction%20and%20accurate%206-DoF%20tracking%20of%20the%20target%20object%20in%20proximity%0Aoperations%20over%20a%20short%20to%20medium%20duration.%20The%20causes%20of%20tracking%20drift%20are%0Adiscussed%20and%20potential%20solutions%20are%20outlined.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20104v1&entry.124074799=Read"},
{"title": "Deform3DGS: Flexible Deformation for Fast Surgical Scene Reconstruction\n  with Gaussian Splatting", "author": "Shuojue Yang and Qian Li and Daiyun Shen and Bingchen Gong and Qi Dou and Yueming Jin", "abstract": "  Tissue deformation poses a key challenge for accurate surgical scene\nreconstruction. Despite yielding high reconstruction quality, existing methods\nsuffer from slow rendering speeds and long training times, limiting their\nintraoperative applicability. Motivated by recent progress in 3D Gaussian\nSplatting, an emerging technology in real-time 3D rendering, this work presents\na novel fast reconstruction framework, termed Deform3DGS, for deformable\ntissues during endoscopic surgery. Specifically, we introduce 3D GS into\nsurgical scenes by integrating a point cloud initialization to improve\nreconstruction. Furthermore, we propose a novel flexible deformation modeling\nscheme (FDM) to learn tissue deformation dynamics at the level of individual\nGaussians. Our FDM can model the surface deformation with efficient\nrepresentations, allowing for real-time rendering performance. More\nimportantly, FDM significantly accelerates surgical scene reconstruction,\ndemonstrating considerable clinical values, particularly in intraoperative\nsettings where time efficiency is crucial. Experiments on DaVinci robotic\nsurgery videos indicate the efficacy of our approach, showcasing superior\nreconstruction fidelity PSNR: (37.90) and rendering speed (338.8 FPS) while\nsubstantially reducing training time to only 1 minute/scene. Our code is\navailable at https://github.com/jinlab-imvr/Deform3DGS.\n", "link": "http://arxiv.org/abs/2405.17835v3", "date": "2024-05-30", "relevancy": 3.2568, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7051}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6727}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deform3DGS%3A%20Flexible%20Deformation%20for%20Fast%20Surgical%20Scene%20Reconstruction%0A%20%20with%20Gaussian%20Splatting&body=Title%3A%20Deform3DGS%3A%20Flexible%20Deformation%20for%20Fast%20Surgical%20Scene%20Reconstruction%0A%20%20with%20Gaussian%20Splatting%0AAuthor%3A%20Shuojue%20Yang%20and%20Qian%20Li%20and%20Daiyun%20Shen%20and%20Bingchen%20Gong%20and%20Qi%20Dou%20and%20Yueming%20Jin%0AAbstract%3A%20%20%20Tissue%20deformation%20poses%20a%20key%20challenge%20for%20accurate%20surgical%20scene%0Areconstruction.%20Despite%20yielding%20high%20reconstruction%20quality%2C%20existing%20methods%0Asuffer%20from%20slow%20rendering%20speeds%20and%20long%20training%20times%2C%20limiting%20their%0Aintraoperative%20applicability.%20Motivated%20by%20recent%20progress%20in%203D%20Gaussian%0ASplatting%2C%20an%20emerging%20technology%20in%20real-time%203D%20rendering%2C%20this%20work%20presents%0Aa%20novel%20fast%20reconstruction%20framework%2C%20termed%20Deform3DGS%2C%20for%20deformable%0Atissues%20during%20endoscopic%20surgery.%20Specifically%2C%20we%20introduce%203D%20GS%20into%0Asurgical%20scenes%20by%20integrating%20a%20point%20cloud%20initialization%20to%20improve%0Areconstruction.%20Furthermore%2C%20we%20propose%20a%20novel%20flexible%20deformation%20modeling%0Ascheme%20%28FDM%29%20to%20learn%20tissue%20deformation%20dynamics%20at%20the%20level%20of%20individual%0AGaussians.%20Our%20FDM%20can%20model%20the%20surface%20deformation%20with%20efficient%0Arepresentations%2C%20allowing%20for%20real-time%20rendering%20performance.%20More%0Aimportantly%2C%20FDM%20significantly%20accelerates%20surgical%20scene%20reconstruction%2C%0Ademonstrating%20considerable%20clinical%20values%2C%20particularly%20in%20intraoperative%0Asettings%20where%20time%20efficiency%20is%20crucial.%20Experiments%20on%20DaVinci%20robotic%0Asurgery%20videos%20indicate%20the%20efficacy%20of%20our%20approach%2C%20showcasing%20superior%0Areconstruction%20fidelity%20PSNR%3A%20%2837.90%29%20and%20rendering%20speed%20%28338.8%20FPS%29%20while%0Asubstantially%20reducing%20training%20time%20to%20only%201%20minute/scene.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/jinlab-imvr/Deform3DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17835v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeform3DGS%253A%2520Flexible%2520Deformation%2520for%2520Fast%2520Surgical%2520Scene%2520Reconstruction%250A%2520%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DShuojue%2520Yang%2520and%2520Qian%2520Li%2520and%2520Daiyun%2520Shen%2520and%2520Bingchen%2520Gong%2520and%2520Qi%2520Dou%2520and%2520Yueming%2520Jin%26entry.1292438233%3D%2520%2520Tissue%2520deformation%2520poses%2520a%2520key%2520challenge%2520for%2520accurate%2520surgical%2520scene%250Areconstruction.%2520Despite%2520yielding%2520high%2520reconstruction%2520quality%252C%2520existing%2520methods%250Asuffer%2520from%2520slow%2520rendering%2520speeds%2520and%2520long%2520training%2520times%252C%2520limiting%2520their%250Aintraoperative%2520applicability.%2520Motivated%2520by%2520recent%2520progress%2520in%25203D%2520Gaussian%250ASplatting%252C%2520an%2520emerging%2520technology%2520in%2520real-time%25203D%2520rendering%252C%2520this%2520work%2520presents%250Aa%2520novel%2520fast%2520reconstruction%2520framework%252C%2520termed%2520Deform3DGS%252C%2520for%2520deformable%250Atissues%2520during%2520endoscopic%2520surgery.%2520Specifically%252C%2520we%2520introduce%25203D%2520GS%2520into%250Asurgical%2520scenes%2520by%2520integrating%2520a%2520point%2520cloud%2520initialization%2520to%2520improve%250Areconstruction.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520flexible%2520deformation%2520modeling%250Ascheme%2520%2528FDM%2529%2520to%2520learn%2520tissue%2520deformation%2520dynamics%2520at%2520the%2520level%2520of%2520individual%250AGaussians.%2520Our%2520FDM%2520can%2520model%2520the%2520surface%2520deformation%2520with%2520efficient%250Arepresentations%252C%2520allowing%2520for%2520real-time%2520rendering%2520performance.%2520More%250Aimportantly%252C%2520FDM%2520significantly%2520accelerates%2520surgical%2520scene%2520reconstruction%252C%250Ademonstrating%2520considerable%2520clinical%2520values%252C%2520particularly%2520in%2520intraoperative%250Asettings%2520where%2520time%2520efficiency%2520is%2520crucial.%2520Experiments%2520on%2520DaVinci%2520robotic%250Asurgery%2520videos%2520indicate%2520the%2520efficacy%2520of%2520our%2520approach%252C%2520showcasing%2520superior%250Areconstruction%2520fidelity%2520PSNR%253A%2520%252837.90%2529%2520and%2520rendering%2520speed%2520%2528338.8%2520FPS%2529%2520while%250Asubstantially%2520reducing%2520training%2520time%2520to%2520only%25201%2520minute/scene.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/jinlab-imvr/Deform3DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17835v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deform3DGS%3A%20Flexible%20Deformation%20for%20Fast%20Surgical%20Scene%20Reconstruction%0A%20%20with%20Gaussian%20Splatting&entry.906535625=Shuojue%20Yang%20and%20Qian%20Li%20and%20Daiyun%20Shen%20and%20Bingchen%20Gong%20and%20Qi%20Dou%20and%20Yueming%20Jin&entry.1292438233=%20%20Tissue%20deformation%20poses%20a%20key%20challenge%20for%20accurate%20surgical%20scene%0Areconstruction.%20Despite%20yielding%20high%20reconstruction%20quality%2C%20existing%20methods%0Asuffer%20from%20slow%20rendering%20speeds%20and%20long%20training%20times%2C%20limiting%20their%0Aintraoperative%20applicability.%20Motivated%20by%20recent%20progress%20in%203D%20Gaussian%0ASplatting%2C%20an%20emerging%20technology%20in%20real-time%203D%20rendering%2C%20this%20work%20presents%0Aa%20novel%20fast%20reconstruction%20framework%2C%20termed%20Deform3DGS%2C%20for%20deformable%0Atissues%20during%20endoscopic%20surgery.%20Specifically%2C%20we%20introduce%203D%20GS%20into%0Asurgical%20scenes%20by%20integrating%20a%20point%20cloud%20initialization%20to%20improve%0Areconstruction.%20Furthermore%2C%20we%20propose%20a%20novel%20flexible%20deformation%20modeling%0Ascheme%20%28FDM%29%20to%20learn%20tissue%20deformation%20dynamics%20at%20the%20level%20of%20individual%0AGaussians.%20Our%20FDM%20can%20model%20the%20surface%20deformation%20with%20efficient%0Arepresentations%2C%20allowing%20for%20real-time%20rendering%20performance.%20More%0Aimportantly%2C%20FDM%20significantly%20accelerates%20surgical%20scene%20reconstruction%2C%0Ademonstrating%20considerable%20clinical%20values%2C%20particularly%20in%20intraoperative%0Asettings%20where%20time%20efficiency%20is%20crucial.%20Experiments%20on%20DaVinci%20robotic%0Asurgery%20videos%20indicate%20the%20efficacy%20of%20our%20approach%2C%20showcasing%20superior%0Areconstruction%20fidelity%20PSNR%3A%20%2837.90%29%20and%20rendering%20speed%20%28338.8%20FPS%29%20while%0Asubstantially%20reducing%20training%20time%20to%20only%201%20minute/scene.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/jinlab-imvr/Deform3DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17835v3&entry.124074799=Read"},
{"title": "VividDream: Generating 3D Scene with Ambient Dynamics", "author": "Yao-Chih Lee and Yi-Ting Chen and Andrew Wang and Ting-Hsuan Liao and Brandon Y. Feng and Jia-Bin Huang", "abstract": "  We introduce VividDream, a method for generating explorable 4D scenes with\nambient dynamics from a single input image or text prompt. VividDream first\nexpands an input image into a static 3D point cloud through iterative\ninpainting and geometry merging. An ensemble of animated videos is then\ngenerated using video diffusion models with quality refinement techniques and\nconditioned on renderings of the static 3D scene from the sampled camera\ntrajectories. We then optimize a canonical 4D scene representation using an\nanimated video ensemble, with per-video motion embeddings and visibility masks\nto mitigate inconsistencies. The resulting 4D scene enables free-view\nexploration of a 3D scene with plausible ambient scene dynamics. Experiments\ndemonstrate that VividDream can provide human viewers with compelling 4D\nexperiences generated based on diverse real images and text prompts.\n", "link": "http://arxiv.org/abs/2405.20334v1", "date": "2024-05-30", "relevancy": 3.1926, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.67}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.67}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VividDream%3A%20Generating%203D%20Scene%20with%20Ambient%20Dynamics&body=Title%3A%20VividDream%3A%20Generating%203D%20Scene%20with%20Ambient%20Dynamics%0AAuthor%3A%20Yao-Chih%20Lee%20and%20Yi-Ting%20Chen%20and%20Andrew%20Wang%20and%20Ting-Hsuan%20Liao%20and%20Brandon%20Y.%20Feng%20and%20Jia-Bin%20Huang%0AAbstract%3A%20%20%20We%20introduce%20VividDream%2C%20a%20method%20for%20generating%20explorable%204D%20scenes%20with%0Aambient%20dynamics%20from%20a%20single%20input%20image%20or%20text%20prompt.%20VividDream%20first%0Aexpands%20an%20input%20image%20into%20a%20static%203D%20point%20cloud%20through%20iterative%0Ainpainting%20and%20geometry%20merging.%20An%20ensemble%20of%20animated%20videos%20is%20then%0Agenerated%20using%20video%20diffusion%20models%20with%20quality%20refinement%20techniques%20and%0Aconditioned%20on%20renderings%20of%20the%20static%203D%20scene%20from%20the%20sampled%20camera%0Atrajectories.%20We%20then%20optimize%20a%20canonical%204D%20scene%20representation%20using%20an%0Aanimated%20video%20ensemble%2C%20with%20per-video%20motion%20embeddings%20and%20visibility%20masks%0Ato%20mitigate%20inconsistencies.%20The%20resulting%204D%20scene%20enables%20free-view%0Aexploration%20of%20a%203D%20scene%20with%20plausible%20ambient%20scene%20dynamics.%20Experiments%0Ademonstrate%20that%20VividDream%20can%20provide%20human%20viewers%20with%20compelling%204D%0Aexperiences%20generated%20based%20on%20diverse%20real%20images%20and%20text%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVividDream%253A%2520Generating%25203D%2520Scene%2520with%2520Ambient%2520Dynamics%26entry.906535625%3DYao-Chih%2520Lee%2520and%2520Yi-Ting%2520Chen%2520and%2520Andrew%2520Wang%2520and%2520Ting-Hsuan%2520Liao%2520and%2520Brandon%2520Y.%2520Feng%2520and%2520Jia-Bin%2520Huang%26entry.1292438233%3D%2520%2520We%2520introduce%2520VividDream%252C%2520a%2520method%2520for%2520generating%2520explorable%25204D%2520scenes%2520with%250Aambient%2520dynamics%2520from%2520a%2520single%2520input%2520image%2520or%2520text%2520prompt.%2520VividDream%2520first%250Aexpands%2520an%2520input%2520image%2520into%2520a%2520static%25203D%2520point%2520cloud%2520through%2520iterative%250Ainpainting%2520and%2520geometry%2520merging.%2520An%2520ensemble%2520of%2520animated%2520videos%2520is%2520then%250Agenerated%2520using%2520video%2520diffusion%2520models%2520with%2520quality%2520refinement%2520techniques%2520and%250Aconditioned%2520on%2520renderings%2520of%2520the%2520static%25203D%2520scene%2520from%2520the%2520sampled%2520camera%250Atrajectories.%2520We%2520then%2520optimize%2520a%2520canonical%25204D%2520scene%2520representation%2520using%2520an%250Aanimated%2520video%2520ensemble%252C%2520with%2520per-video%2520motion%2520embeddings%2520and%2520visibility%2520masks%250Ato%2520mitigate%2520inconsistencies.%2520The%2520resulting%25204D%2520scene%2520enables%2520free-view%250Aexploration%2520of%2520a%25203D%2520scene%2520with%2520plausible%2520ambient%2520scene%2520dynamics.%2520Experiments%250Ademonstrate%2520that%2520VividDream%2520can%2520provide%2520human%2520viewers%2520with%2520compelling%25204D%250Aexperiences%2520generated%2520based%2520on%2520diverse%2520real%2520images%2520and%2520text%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VividDream%3A%20Generating%203D%20Scene%20with%20Ambient%20Dynamics&entry.906535625=Yao-Chih%20Lee%20and%20Yi-Ting%20Chen%20and%20Andrew%20Wang%20and%20Ting-Hsuan%20Liao%20and%20Brandon%20Y.%20Feng%20and%20Jia-Bin%20Huang&entry.1292438233=%20%20We%20introduce%20VividDream%2C%20a%20method%20for%20generating%20explorable%204D%20scenes%20with%0Aambient%20dynamics%20from%20a%20single%20input%20image%20or%20text%20prompt.%20VividDream%20first%0Aexpands%20an%20input%20image%20into%20a%20static%203D%20point%20cloud%20through%20iterative%0Ainpainting%20and%20geometry%20merging.%20An%20ensemble%20of%20animated%20videos%20is%20then%0Agenerated%20using%20video%20diffusion%20models%20with%20quality%20refinement%20techniques%20and%0Aconditioned%20on%20renderings%20of%20the%20static%203D%20scene%20from%20the%20sampled%20camera%0Atrajectories.%20We%20then%20optimize%20a%20canonical%204D%20scene%20representation%20using%20an%0Aanimated%20video%20ensemble%2C%20with%20per-video%20motion%20embeddings%20and%20visibility%20masks%0Ato%20mitigate%20inconsistencies.%20The%20resulting%204D%20scene%20enables%20free-view%0Aexploration%20of%20a%203D%20scene%20with%20plausible%20ambient%20scene%20dynamics.%20Experiments%0Ademonstrate%20that%20VividDream%20can%20provide%20human%20viewers%20with%20compelling%204D%0Aexperiences%20generated%20based%20on%20diverse%20real%20images%20and%20text%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20334v1&entry.124074799=Read"},
{"title": "A Pixel Is Worth More Than One 3D Gaussians in Single-View 3D\n  Reconstruction", "author": "Jianghao Shen and Tianfu Wu", "abstract": "  Learning 3D scene representation from a single-view image is a long-standing\nfundamental problem in computer vision, with the inherent ambiguity in\npredicting contents unseen from the input view. Built on the recently proposed\n3D Gaussian Splatting (3DGS), the Splatter Image method has made promising\nprogress on fast single-image novel view synthesis via learning a single 3D\nGaussian for each pixel based on the U-Net feature map of an input image.\nHowever, it has limited expressive power to represent occluded components that\nare not observable in the input view. To address this problem, this paper\npresents a Hierarchical Splatter Image method in which a pixel is worth more\nthan one 3D Gaussians. Specifically,\n  each pixel is represented by a parent 3D Gaussian and a small number of child\n3D Gaussians. Parent 3D Gaussians are learned as done in the vanilla Splatter\nImage. Child 3D Gaussians are learned via a lightweight Multi-Layer Perceptron\n(MLP) which takes as input the projected image features of a parent 3D Gaussian\nand the embedding of a target camera view. Both parent and child 3D Gaussians\nare learned end-to-end in a stage-wise way. The joint condition of input image\nfeatures from eyes of the parent Gaussians and the target camera position\nfacilitates learning to allocate child Gaussians to ``see the unseen'',\nrecovering the occluded details that are often missed by parent Gaussians.\n  In experiments, the proposed method is tested on the ShapeNet-SRN and CO3D\ndatasets with state-of-the-art performance obtained, especially showing\npromising capabilities of reconstructing occluded contents in the input view.\n", "link": "http://arxiv.org/abs/2405.20310v1", "date": "2024-05-30", "relevancy": 3.1779, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6943}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6448}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction&body=Title%3A%20A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction%0AAuthor%3A%20Jianghao%20Shen%20and%20Tianfu%20Wu%0AAbstract%3A%20%20%20Learning%203D%20scene%20representation%20from%20a%20single-view%20image%20is%20a%20long-standing%0Afundamental%20problem%20in%20computer%20vision%2C%20with%20the%20inherent%20ambiguity%20in%0Apredicting%20contents%20unseen%20from%20the%20input%20view.%20Built%20on%20the%20recently%20proposed%0A3D%20Gaussian%20Splatting%20%283DGS%29%2C%20the%20Splatter%20Image%20method%20has%20made%20promising%0Aprogress%20on%20fast%20single-image%20novel%20view%20synthesis%20via%20learning%20a%20single%203D%0AGaussian%20for%20each%20pixel%20based%20on%20the%20U-Net%20feature%20map%20of%20an%20input%20image.%0AHowever%2C%20it%20has%20limited%20expressive%20power%20to%20represent%20occluded%20components%20that%0Aare%20not%20observable%20in%20the%20input%20view.%20To%20address%20this%20problem%2C%20this%20paper%0Apresents%20a%20Hierarchical%20Splatter%20Image%20method%20in%20which%20a%20pixel%20is%20worth%20more%0Athan%20one%203D%20Gaussians.%20Specifically%2C%0A%20%20each%20pixel%20is%20represented%20by%20a%20parent%203D%20Gaussian%20and%20a%20small%20number%20of%20child%0A3D%20Gaussians.%20Parent%203D%20Gaussians%20are%20learned%20as%20done%20in%20the%20vanilla%20Splatter%0AImage.%20Child%203D%20Gaussians%20are%20learned%20via%20a%20lightweight%20Multi-Layer%20Perceptron%0A%28MLP%29%20which%20takes%20as%20input%20the%20projected%20image%20features%20of%20a%20parent%203D%20Gaussian%0Aand%20the%20embedding%20of%20a%20target%20camera%20view.%20Both%20parent%20and%20child%203D%20Gaussians%0Aare%20learned%20end-to-end%20in%20a%20stage-wise%20way.%20The%20joint%20condition%20of%20input%20image%0Afeatures%20from%20eyes%20of%20the%20parent%20Gaussians%20and%20the%20target%20camera%20position%0Afacilitates%20learning%20to%20allocate%20child%20Gaussians%20to%20%60%60see%20the%20unseen%27%27%2C%0Arecovering%20the%20occluded%20details%20that%20are%20often%20missed%20by%20parent%20Gaussians.%0A%20%20In%20experiments%2C%20the%20proposed%20method%20is%20tested%20on%20the%20ShapeNet-SRN%20and%20CO3D%0Adatasets%20with%20state-of-the-art%20performance%20obtained%2C%20especially%20showing%0Apromising%20capabilities%20of%20reconstructing%20occluded%20contents%20in%20the%20input%20view.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pixel%2520Is%2520Worth%2520More%2520Than%2520One%25203D%2520Gaussians%2520in%2520Single-View%25203D%250A%2520%2520Reconstruction%26entry.906535625%3DJianghao%2520Shen%2520and%2520Tianfu%2520Wu%26entry.1292438233%3D%2520%2520Learning%25203D%2520scene%2520representation%2520from%2520a%2520single-view%2520image%2520is%2520a%2520long-standing%250Afundamental%2520problem%2520in%2520computer%2520vision%252C%2520with%2520the%2520inherent%2520ambiguity%2520in%250Apredicting%2520contents%2520unseen%2520from%2520the%2520input%2520view.%2520Built%2520on%2520the%2520recently%2520proposed%250A3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520the%2520Splatter%2520Image%2520method%2520has%2520made%2520promising%250Aprogress%2520on%2520fast%2520single-image%2520novel%2520view%2520synthesis%2520via%2520learning%2520a%2520single%25203D%250AGaussian%2520for%2520each%2520pixel%2520based%2520on%2520the%2520U-Net%2520feature%2520map%2520of%2520an%2520input%2520image.%250AHowever%252C%2520it%2520has%2520limited%2520expressive%2520power%2520to%2520represent%2520occluded%2520components%2520that%250Aare%2520not%2520observable%2520in%2520the%2520input%2520view.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%250Apresents%2520a%2520Hierarchical%2520Splatter%2520Image%2520method%2520in%2520which%2520a%2520pixel%2520is%2520worth%2520more%250Athan%2520one%25203D%2520Gaussians.%2520Specifically%252C%250A%2520%2520each%2520pixel%2520is%2520represented%2520by%2520a%2520parent%25203D%2520Gaussian%2520and%2520a%2520small%2520number%2520of%2520child%250A3D%2520Gaussians.%2520Parent%25203D%2520Gaussians%2520are%2520learned%2520as%2520done%2520in%2520the%2520vanilla%2520Splatter%250AImage.%2520Child%25203D%2520Gaussians%2520are%2520learned%2520via%2520a%2520lightweight%2520Multi-Layer%2520Perceptron%250A%2528MLP%2529%2520which%2520takes%2520as%2520input%2520the%2520projected%2520image%2520features%2520of%2520a%2520parent%25203D%2520Gaussian%250Aand%2520the%2520embedding%2520of%2520a%2520target%2520camera%2520view.%2520Both%2520parent%2520and%2520child%25203D%2520Gaussians%250Aare%2520learned%2520end-to-end%2520in%2520a%2520stage-wise%2520way.%2520The%2520joint%2520condition%2520of%2520input%2520image%250Afeatures%2520from%2520eyes%2520of%2520the%2520parent%2520Gaussians%2520and%2520the%2520target%2520camera%2520position%250Afacilitates%2520learning%2520to%2520allocate%2520child%2520Gaussians%2520to%2520%2560%2560see%2520the%2520unseen%2527%2527%252C%250Arecovering%2520the%2520occluded%2520details%2520that%2520are%2520often%2520missed%2520by%2520parent%2520Gaussians.%250A%2520%2520In%2520experiments%252C%2520the%2520proposed%2520method%2520is%2520tested%2520on%2520the%2520ShapeNet-SRN%2520and%2520CO3D%250Adatasets%2520with%2520state-of-the-art%2520performance%2520obtained%252C%2520especially%2520showing%250Apromising%2520capabilities%2520of%2520reconstructing%2520occluded%2520contents%2520in%2520the%2520input%2520view.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction&entry.906535625=Jianghao%20Shen%20and%20Tianfu%20Wu&entry.1292438233=%20%20Learning%203D%20scene%20representation%20from%20a%20single-view%20image%20is%20a%20long-standing%0Afundamental%20problem%20in%20computer%20vision%2C%20with%20the%20inherent%20ambiguity%20in%0Apredicting%20contents%20unseen%20from%20the%20input%20view.%20Built%20on%20the%20recently%20proposed%0A3D%20Gaussian%20Splatting%20%283DGS%29%2C%20the%20Splatter%20Image%20method%20has%20made%20promising%0Aprogress%20on%20fast%20single-image%20novel%20view%20synthesis%20via%20learning%20a%20single%203D%0AGaussian%20for%20each%20pixel%20based%20on%20the%20U-Net%20feature%20map%20of%20an%20input%20image.%0AHowever%2C%20it%20has%20limited%20expressive%20power%20to%20represent%20occluded%20components%20that%0Aare%20not%20observable%20in%20the%20input%20view.%20To%20address%20this%20problem%2C%20this%20paper%0Apresents%20a%20Hierarchical%20Splatter%20Image%20method%20in%20which%20a%20pixel%20is%20worth%20more%0Athan%20one%203D%20Gaussians.%20Specifically%2C%0A%20%20each%20pixel%20is%20represented%20by%20a%20parent%203D%20Gaussian%20and%20a%20small%20number%20of%20child%0A3D%20Gaussians.%20Parent%203D%20Gaussians%20are%20learned%20as%20done%20in%20the%20vanilla%20Splatter%0AImage.%20Child%203D%20Gaussians%20are%20learned%20via%20a%20lightweight%20Multi-Layer%20Perceptron%0A%28MLP%29%20which%20takes%20as%20input%20the%20projected%20image%20features%20of%20a%20parent%203D%20Gaussian%0Aand%20the%20embedding%20of%20a%20target%20camera%20view.%20Both%20parent%20and%20child%203D%20Gaussians%0Aare%20learned%20end-to-end%20in%20a%20stage-wise%20way.%20The%20joint%20condition%20of%20input%20image%0Afeatures%20from%20eyes%20of%20the%20parent%20Gaussians%20and%20the%20target%20camera%20position%0Afacilitates%20learning%20to%20allocate%20child%20Gaussians%20to%20%60%60see%20the%20unseen%27%27%2C%0Arecovering%20the%20occluded%20details%20that%20are%20often%20missed%20by%20parent%20Gaussians.%0A%20%20In%20experiments%2C%20the%20proposed%20method%20is%20tested%20on%20the%20ShapeNet-SRN%20and%20CO3D%0Adatasets%20with%20state-of-the-art%20performance%20obtained%2C%20especially%20showing%0Apromising%20capabilities%20of%20reconstructing%20occluded%20contents%20in%20the%20input%20view.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20310v1&entry.124074799=Read"},
{"title": "$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous\n  Driving", "author": "Nan Huang and Xiaobao Wei and Wenzhao Zheng and Pengju An and Ming Lu and Wei Zhan and Masayoshi Tomizuka and Kurt Keutzer and Shanghang Zhang", "abstract": "  Photorealistic 3D reconstruction of street scenes is a critical technique for\ndeveloping real-world simulators for autonomous driving. Despite the efficacy\nof Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting\n(3DGS) emerges as a promising direction due to its faster speed and more\nexplicit representation. However, most existing street 3DGS methods require\ntracked 3D vehicle bounding boxes to decompose the static and dynamic elements\nfor effective reconstruction, limiting their applications for in-the-wild\nscenarios. To facilitate efficient 3D scene reconstruction without costly\nannotations, we propose a self-supervised street Gaussian\n($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from\n4D consistency. We represent each scene with 3D Gaussians to preserve the\nexplicitness and further accompany them with a spatial-temporal field network\nto compactly model the 4D dynamics. We conduct extensive experiments on the\nchallenging Waymo-Open dataset to evaluate the effectiveness of our method. Our\n$\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic\nscenes and achieves the best performance without using 3D annotations. Code is\navailable at: https://github.com/nnanhuang/S3Gaussian/.\n", "link": "http://arxiv.org/abs/2405.20323v1", "date": "2024-05-30", "relevancy": 3.1669, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7025}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6522}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Ctextit%7BS%7D%5E3%24Gaussian%3A%20Self-Supervised%20Street%20Gaussians%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20%24%5Ctextit%7BS%7D%5E3%24Gaussian%3A%20Self-Supervised%20Street%20Gaussians%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Nan%20Huang%20and%20Xiaobao%20Wei%20and%20Wenzhao%20Zheng%20and%20Pengju%20An%20and%20Ming%20Lu%20and%20Wei%20Zhan%20and%20Masayoshi%20Tomizuka%20and%20Kurt%20Keutzer%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Photorealistic%203D%20reconstruction%20of%20street%20scenes%20is%20a%20critical%20technique%20for%0Adeveloping%20real-world%20simulators%20for%20autonomous%20driving.%20Despite%20the%20efficacy%0Aof%20Neural%20Radiance%20Fields%20%28NeRF%29%20for%20driving%20scenes%2C%203D%20Gaussian%20Splatting%0A%283DGS%29%20emerges%20as%20a%20promising%20direction%20due%20to%20its%20faster%20speed%20and%20more%0Aexplicit%20representation.%20However%2C%20most%20existing%20street%203DGS%20methods%20require%0Atracked%203D%20vehicle%20bounding%20boxes%20to%20decompose%20the%20static%20and%20dynamic%20elements%0Afor%20effective%20reconstruction%2C%20limiting%20their%20applications%20for%20in-the-wild%0Ascenarios.%20To%20facilitate%20efficient%203D%20scene%20reconstruction%20without%20costly%0Aannotations%2C%20we%20propose%20a%20self-supervised%20street%20Gaussian%0A%28%24%5Ctextit%7BS%7D%5E3%24Gaussian%29%20method%20to%20decompose%20dynamic%20and%20static%20elements%20from%0A4D%20consistency.%20We%20represent%20each%20scene%20with%203D%20Gaussians%20to%20preserve%20the%0Aexplicitness%20and%20further%20accompany%20them%20with%20a%20spatial-temporal%20field%20network%0Ato%20compactly%20model%20the%204D%20dynamics.%20We%20conduct%20extensive%20experiments%20on%20the%0Achallenging%20Waymo-Open%20dataset%20to%20evaluate%20the%20effectiveness%20of%20our%20method.%20Our%0A%24%5Ctextit%7BS%7D%5E3%24Gaussian%20demonstrates%20the%20ability%20to%20decompose%20static%20and%20dynamic%0Ascenes%20and%20achieves%20the%20best%20performance%20without%20using%203D%20annotations.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/nnanhuang/S3Gaussian/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Ctextit%257BS%257D%255E3%2524Gaussian%253A%2520Self-Supervised%2520Street%2520Gaussians%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DNan%2520Huang%2520and%2520Xiaobao%2520Wei%2520and%2520Wenzhao%2520Zheng%2520and%2520Pengju%2520An%2520and%2520Ming%2520Lu%2520and%2520Wei%2520Zhan%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Kurt%2520Keutzer%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Photorealistic%25203D%2520reconstruction%2520of%2520street%2520scenes%2520is%2520a%2520critical%2520technique%2520for%250Adeveloping%2520real-world%2520simulators%2520for%2520autonomous%2520driving.%2520Despite%2520the%2520efficacy%250Aof%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520for%2520driving%2520scenes%252C%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%2520emerges%2520as%2520a%2520promising%2520direction%2520due%2520to%2520its%2520faster%2520speed%2520and%2520more%250Aexplicit%2520representation.%2520However%252C%2520most%2520existing%2520street%25203DGS%2520methods%2520require%250Atracked%25203D%2520vehicle%2520bounding%2520boxes%2520to%2520decompose%2520the%2520static%2520and%2520dynamic%2520elements%250Afor%2520effective%2520reconstruction%252C%2520limiting%2520their%2520applications%2520for%2520in-the-wild%250Ascenarios.%2520To%2520facilitate%2520efficient%25203D%2520scene%2520reconstruction%2520without%2520costly%250Aannotations%252C%2520we%2520propose%2520a%2520self-supervised%2520street%2520Gaussian%250A%2528%2524%255Ctextit%257BS%257D%255E3%2524Gaussian%2529%2520method%2520to%2520decompose%2520dynamic%2520and%2520static%2520elements%2520from%250A4D%2520consistency.%2520We%2520represent%2520each%2520scene%2520with%25203D%2520Gaussians%2520to%2520preserve%2520the%250Aexplicitness%2520and%2520further%2520accompany%2520them%2520with%2520a%2520spatial-temporal%2520field%2520network%250Ato%2520compactly%2520model%2520the%25204D%2520dynamics.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%250Achallenging%2520Waymo-Open%2520dataset%2520to%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520method.%2520Our%250A%2524%255Ctextit%257BS%257D%255E3%2524Gaussian%2520demonstrates%2520the%2520ability%2520to%2520decompose%2520static%2520and%2520dynamic%250Ascenes%2520and%2520achieves%2520the%2520best%2520performance%2520without%2520using%25203D%2520annotations.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/nnanhuang/S3Gaussian/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Ctextit%7BS%7D%5E3%24Gaussian%3A%20Self-Supervised%20Street%20Gaussians%20for%20Autonomous%0A%20%20Driving&entry.906535625=Nan%20Huang%20and%20Xiaobao%20Wei%20and%20Wenzhao%20Zheng%20and%20Pengju%20An%20and%20Ming%20Lu%20and%20Wei%20Zhan%20and%20Masayoshi%20Tomizuka%20and%20Kurt%20Keutzer%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Photorealistic%203D%20reconstruction%20of%20street%20scenes%20is%20a%20critical%20technique%20for%0Adeveloping%20real-world%20simulators%20for%20autonomous%20driving.%20Despite%20the%20efficacy%0Aof%20Neural%20Radiance%20Fields%20%28NeRF%29%20for%20driving%20scenes%2C%203D%20Gaussian%20Splatting%0A%283DGS%29%20emerges%20as%20a%20promising%20direction%20due%20to%20its%20faster%20speed%20and%20more%0Aexplicit%20representation.%20However%2C%20most%20existing%20street%203DGS%20methods%20require%0Atracked%203D%20vehicle%20bounding%20boxes%20to%20decompose%20the%20static%20and%20dynamic%20elements%0Afor%20effective%20reconstruction%2C%20limiting%20their%20applications%20for%20in-the-wild%0Ascenarios.%20To%20facilitate%20efficient%203D%20scene%20reconstruction%20without%20costly%0Aannotations%2C%20we%20propose%20a%20self-supervised%20street%20Gaussian%0A%28%24%5Ctextit%7BS%7D%5E3%24Gaussian%29%20method%20to%20decompose%20dynamic%20and%20static%20elements%20from%0A4D%20consistency.%20We%20represent%20each%20scene%20with%203D%20Gaussians%20to%20preserve%20the%0Aexplicitness%20and%20further%20accompany%20them%20with%20a%20spatial-temporal%20field%20network%0Ato%20compactly%20model%20the%204D%20dynamics.%20We%20conduct%20extensive%20experiments%20on%20the%0Achallenging%20Waymo-Open%20dataset%20to%20evaluate%20the%20effectiveness%20of%20our%20method.%20Our%0A%24%5Ctextit%7BS%7D%5E3%24Gaussian%20demonstrates%20the%20ability%20to%20decompose%20static%20and%20dynamic%0Ascenes%20and%20achieves%20the%20best%20performance%20without%20using%203D%20annotations.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/nnanhuang/S3Gaussian/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20323v1&entry.124074799=Read"},
{"title": "Structure Gaussian SLAM with Manhattan World Hypothesis", "author": "Shuhong Liu and Heng Zhou and Liuzhuozheng Li and Yun Liu and Tianchen Deng and Yiming Zhou and Mingrui Li", "abstract": "  Gaussian SLAM systems have made significant advancements in improving the\nefficiency and fidelity of real-time reconstructions. However, these systems\noften encounter incomplete reconstructions in complex indoor environments,\ncharacterized by substantial holes due to unobserved geometry caused by\nobstacles or limited view angles. To address this challenge, we present\nManhattan Gaussian SLAM (MG-SLAM), an RGB-D system that leverages the Manhattan\nWorld hypothesis to enhance geometric accuracy and completeness. By seamlessly\nintegrating fused line segments derived from structured scenes, MG-SLAM ensures\nrobust tracking in textureless indoor areas. Moreover, The extracted lines and\nplanar surface assumption allow strategic interpolation of new Gaussians in\nregions of missing geometry, enabling efficient scene completion. Extensive\nexperiments conducted on both synthetic and real-world scenes demonstrate that\nthese advancements enable our method to achieve state-of-the-art performance,\nmarking a substantial improvement in the capabilities of Gaussian SLAM systems.\n", "link": "http://arxiv.org/abs/2405.20031v1", "date": "2024-05-30", "relevancy": 3.1586, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.714}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6273}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20Gaussian%20SLAM%20with%20Manhattan%20World%20Hypothesis&body=Title%3A%20Structure%20Gaussian%20SLAM%20with%20Manhattan%20World%20Hypothesis%0AAuthor%3A%20Shuhong%20Liu%20and%20Heng%20Zhou%20and%20Liuzhuozheng%20Li%20and%20Yun%20Liu%20and%20Tianchen%20Deng%20and%20Yiming%20Zhou%20and%20Mingrui%20Li%0AAbstract%3A%20%20%20Gaussian%20SLAM%20systems%20have%20made%20significant%20advancements%20in%20improving%20the%0Aefficiency%20and%20fidelity%20of%20real-time%20reconstructions.%20However%2C%20these%20systems%0Aoften%20encounter%20incomplete%20reconstructions%20in%20complex%20indoor%20environments%2C%0Acharacterized%20by%20substantial%20holes%20due%20to%20unobserved%20geometry%20caused%20by%0Aobstacles%20or%20limited%20view%20angles.%20To%20address%20this%20challenge%2C%20we%20present%0AManhattan%20Gaussian%20SLAM%20%28MG-SLAM%29%2C%20an%20RGB-D%20system%20that%20leverages%20the%20Manhattan%0AWorld%20hypothesis%20to%20enhance%20geometric%20accuracy%20and%20completeness.%20By%20seamlessly%0Aintegrating%20fused%20line%20segments%20derived%20from%20structured%20scenes%2C%20MG-SLAM%20ensures%0Arobust%20tracking%20in%20textureless%20indoor%20areas.%20Moreover%2C%20The%20extracted%20lines%20and%0Aplanar%20surface%20assumption%20allow%20strategic%20interpolation%20of%20new%20Gaussians%20in%0Aregions%20of%20missing%20geometry%2C%20enabling%20efficient%20scene%20completion.%20Extensive%0Aexperiments%20conducted%20on%20both%20synthetic%20and%20real-world%20scenes%20demonstrate%20that%0Athese%20advancements%20enable%20our%20method%20to%20achieve%20state-of-the-art%20performance%2C%0Amarking%20a%20substantial%20improvement%20in%20the%20capabilities%20of%20Gaussian%20SLAM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520Gaussian%2520SLAM%2520with%2520Manhattan%2520World%2520Hypothesis%26entry.906535625%3DShuhong%2520Liu%2520and%2520Heng%2520Zhou%2520and%2520Liuzhuozheng%2520Li%2520and%2520Yun%2520Liu%2520and%2520Tianchen%2520Deng%2520and%2520Yiming%2520Zhou%2520and%2520Mingrui%2520Li%26entry.1292438233%3D%2520%2520Gaussian%2520SLAM%2520systems%2520have%2520made%2520significant%2520advancements%2520in%2520improving%2520the%250Aefficiency%2520and%2520fidelity%2520of%2520real-time%2520reconstructions.%2520However%252C%2520these%2520systems%250Aoften%2520encounter%2520incomplete%2520reconstructions%2520in%2520complex%2520indoor%2520environments%252C%250Acharacterized%2520by%2520substantial%2520holes%2520due%2520to%2520unobserved%2520geometry%2520caused%2520by%250Aobstacles%2520or%2520limited%2520view%2520angles.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%250AManhattan%2520Gaussian%2520SLAM%2520%2528MG-SLAM%2529%252C%2520an%2520RGB-D%2520system%2520that%2520leverages%2520the%2520Manhattan%250AWorld%2520hypothesis%2520to%2520enhance%2520geometric%2520accuracy%2520and%2520completeness.%2520By%2520seamlessly%250Aintegrating%2520fused%2520line%2520segments%2520derived%2520from%2520structured%2520scenes%252C%2520MG-SLAM%2520ensures%250Arobust%2520tracking%2520in%2520textureless%2520indoor%2520areas.%2520Moreover%252C%2520The%2520extracted%2520lines%2520and%250Aplanar%2520surface%2520assumption%2520allow%2520strategic%2520interpolation%2520of%2520new%2520Gaussians%2520in%250Aregions%2520of%2520missing%2520geometry%252C%2520enabling%2520efficient%2520scene%2520completion.%2520Extensive%250Aexperiments%2520conducted%2520on%2520both%2520synthetic%2520and%2520real-world%2520scenes%2520demonstrate%2520that%250Athese%2520advancements%2520enable%2520our%2520method%2520to%2520achieve%2520state-of-the-art%2520performance%252C%250Amarking%2520a%2520substantial%2520improvement%2520in%2520the%2520capabilities%2520of%2520Gaussian%2520SLAM%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20Gaussian%20SLAM%20with%20Manhattan%20World%20Hypothesis&entry.906535625=Shuhong%20Liu%20and%20Heng%20Zhou%20and%20Liuzhuozheng%20Li%20and%20Yun%20Liu%20and%20Tianchen%20Deng%20and%20Yiming%20Zhou%20and%20Mingrui%20Li&entry.1292438233=%20%20Gaussian%20SLAM%20systems%20have%20made%20significant%20advancements%20in%20improving%20the%0Aefficiency%20and%20fidelity%20of%20real-time%20reconstructions.%20However%2C%20these%20systems%0Aoften%20encounter%20incomplete%20reconstructions%20in%20complex%20indoor%20environments%2C%0Acharacterized%20by%20substantial%20holes%20due%20to%20unobserved%20geometry%20caused%20by%0Aobstacles%20or%20limited%20view%20angles.%20To%20address%20this%20challenge%2C%20we%20present%0AManhattan%20Gaussian%20SLAM%20%28MG-SLAM%29%2C%20an%20RGB-D%20system%20that%20leverages%20the%20Manhattan%0AWorld%20hypothesis%20to%20enhance%20geometric%20accuracy%20and%20completeness.%20By%20seamlessly%0Aintegrating%20fused%20line%20segments%20derived%20from%20structured%20scenes%2C%20MG-SLAM%20ensures%0Arobust%20tracking%20in%20textureless%20indoor%20areas.%20Moreover%2C%20The%20extracted%20lines%20and%0Aplanar%20surface%20assumption%20allow%20strategic%20interpolation%20of%20new%20Gaussians%20in%0Aregions%20of%20missing%20geometry%2C%20enabling%20efficient%20scene%20completion.%20Extensive%0Aexperiments%20conducted%20on%20both%20synthetic%20and%20real-world%20scenes%20demonstrate%20that%0Athese%20advancements%20enable%20our%20method%20to%20achieve%20state-of-the-art%20performance%2C%0Amarking%20a%20substantial%20improvement%20in%20the%20capabilities%20of%20Gaussian%20SLAM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20031v1&entry.124074799=Read"},
{"title": "3D StreetUnveiler with Semantic-Aware 2DGS", "author": "Jingwei Xu and Yikai Wang and Yiqun Zhao and Yanwei Fu and Shenghua Gao", "abstract": "  Unveiling an empty street from crowded observations captured by in-car\ncameras is crucial for autonomous driving. However, removing all temporarily\nstatic objects, such as stopped vehicles and standing pedestrians, presents a\nsignificant challenge. Unlike object-centric 3D inpainting, which relies on\nthorough observation in a small scene, street scene cases involve long\ntrajectories that differ from previous 3D inpainting tasks. The camera-centric\nmoving environment of captured videos further complicates the task due to the\nlimited degree and time duration of object observation. To address these\nobstacles, we introduce StreetUnveiler to reconstruct an empty street.\nStreetUnveiler learns a 3D representation of the empty street from crowded\nobservations. Our representation is based on the hard-label semantic 2D\nGaussian Splatting (2DGS) for its scalability and ability to identify Gaussians\nto be removed. We inpaint rendered image after removing unwanted Gaussians to\nprovide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal\ncontinuous movement, we divide the empty street scene into observed,\npartial-observed, and unobserved regions, which we propose to locate through a\nrendered alpha map. This decomposition helps us to minimize the regions that\nneed to be inpainted. To enhance the temporal consistency of the inpainting, we\nintroduce a novel time-reversal framework to inpaint frames in reverse order\nand use later frames as references for earlier frames to fully utilize the\nlong-trajectory observations. Our experiments conducted on the street scene\ndataset successfully reconstructed a 3D representation of the empty street. The\nmesh representation of the empty street can be extracted for further\napplications. The project page and more visualizations can be found at:\nhttps://streetunveiler.github.io\n", "link": "http://arxiv.org/abs/2405.18416v2", "date": "2024-05-30", "relevancy": 3.0384, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6329}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6075}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20StreetUnveiler%20with%20Semantic-Aware%202DGS&body=Title%3A%203D%20StreetUnveiler%20with%20Semantic-Aware%202DGS%0AAuthor%3A%20Jingwei%20Xu%20and%20Yikai%20Wang%20and%20Yiqun%20Zhao%20and%20Yanwei%20Fu%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20Unveiling%20an%20empty%20street%20from%20crowded%20observations%20captured%20by%20in-car%0Acameras%20is%20crucial%20for%20autonomous%20driving.%20However%2C%20removing%20all%20temporarily%0Astatic%20objects%2C%20such%20as%20stopped%20vehicles%20and%20standing%20pedestrians%2C%20presents%20a%0Asignificant%20challenge.%20Unlike%20object-centric%203D%20inpainting%2C%20which%20relies%20on%0Athorough%20observation%20in%20a%20small%20scene%2C%20street%20scene%20cases%20involve%20long%0Atrajectories%20that%20differ%20from%20previous%203D%20inpainting%20tasks.%20The%20camera-centric%0Amoving%20environment%20of%20captured%20videos%20further%20complicates%20the%20task%20due%20to%20the%0Alimited%20degree%20and%20time%20duration%20of%20object%20observation.%20To%20address%20these%0Aobstacles%2C%20we%20introduce%20StreetUnveiler%20to%20reconstruct%20an%20empty%20street.%0AStreetUnveiler%20learns%20a%203D%20representation%20of%20the%20empty%20street%20from%20crowded%0Aobservations.%20Our%20representation%20is%20based%20on%20the%20hard-label%20semantic%202D%0AGaussian%20Splatting%20%282DGS%29%20for%20its%20scalability%20and%20ability%20to%20identify%20Gaussians%0Ato%20be%20removed.%20We%20inpaint%20rendered%20image%20after%20removing%20unwanted%20Gaussians%20to%0Aprovide%20pseudo-labels%20and%20subsequently%20re-optimize%20the%202DGS.%20Given%20its%20temporal%0Acontinuous%20movement%2C%20we%20divide%20the%20empty%20street%20scene%20into%20observed%2C%0Apartial-observed%2C%20and%20unobserved%20regions%2C%20which%20we%20propose%20to%20locate%20through%20a%0Arendered%20alpha%20map.%20This%20decomposition%20helps%20us%20to%20minimize%20the%20regions%20that%0Aneed%20to%20be%20inpainted.%20To%20enhance%20the%20temporal%20consistency%20of%20the%20inpainting%2C%20we%0Aintroduce%20a%20novel%20time-reversal%20framework%20to%20inpaint%20frames%20in%20reverse%20order%0Aand%20use%20later%20frames%20as%20references%20for%20earlier%20frames%20to%20fully%20utilize%20the%0Along-trajectory%20observations.%20Our%20experiments%20conducted%20on%20the%20street%20scene%0Adataset%20successfully%20reconstructed%20a%203D%20representation%20of%20the%20empty%20street.%20The%0Amesh%20representation%20of%20the%20empty%20street%20can%20be%20extracted%20for%20further%0Aapplications.%20The%20project%20page%20and%20more%20visualizations%20can%20be%20found%20at%3A%0Ahttps%3A//streetunveiler.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18416v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520StreetUnveiler%2520with%2520Semantic-Aware%25202DGS%26entry.906535625%3DJingwei%2520Xu%2520and%2520Yikai%2520Wang%2520and%2520Yiqun%2520Zhao%2520and%2520Yanwei%2520Fu%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520Unveiling%2520an%2520empty%2520street%2520from%2520crowded%2520observations%2520captured%2520by%2520in-car%250Acameras%2520is%2520crucial%2520for%2520autonomous%2520driving.%2520However%252C%2520removing%2520all%2520temporarily%250Astatic%2520objects%252C%2520such%2520as%2520stopped%2520vehicles%2520and%2520standing%2520pedestrians%252C%2520presents%2520a%250Asignificant%2520challenge.%2520Unlike%2520object-centric%25203D%2520inpainting%252C%2520which%2520relies%2520on%250Athorough%2520observation%2520in%2520a%2520small%2520scene%252C%2520street%2520scene%2520cases%2520involve%2520long%250Atrajectories%2520that%2520differ%2520from%2520previous%25203D%2520inpainting%2520tasks.%2520The%2520camera-centric%250Amoving%2520environment%2520of%2520captured%2520videos%2520further%2520complicates%2520the%2520task%2520due%2520to%2520the%250Alimited%2520degree%2520and%2520time%2520duration%2520of%2520object%2520observation.%2520To%2520address%2520these%250Aobstacles%252C%2520we%2520introduce%2520StreetUnveiler%2520to%2520reconstruct%2520an%2520empty%2520street.%250AStreetUnveiler%2520learns%2520a%25203D%2520representation%2520of%2520the%2520empty%2520street%2520from%2520crowded%250Aobservations.%2520Our%2520representation%2520is%2520based%2520on%2520the%2520hard-label%2520semantic%25202D%250AGaussian%2520Splatting%2520%25282DGS%2529%2520for%2520its%2520scalability%2520and%2520ability%2520to%2520identify%2520Gaussians%250Ato%2520be%2520removed.%2520We%2520inpaint%2520rendered%2520image%2520after%2520removing%2520unwanted%2520Gaussians%2520to%250Aprovide%2520pseudo-labels%2520and%2520subsequently%2520re-optimize%2520the%25202DGS.%2520Given%2520its%2520temporal%250Acontinuous%2520movement%252C%2520we%2520divide%2520the%2520empty%2520street%2520scene%2520into%2520observed%252C%250Apartial-observed%252C%2520and%2520unobserved%2520regions%252C%2520which%2520we%2520propose%2520to%2520locate%2520through%2520a%250Arendered%2520alpha%2520map.%2520This%2520decomposition%2520helps%2520us%2520to%2520minimize%2520the%2520regions%2520that%250Aneed%2520to%2520be%2520inpainted.%2520To%2520enhance%2520the%2520temporal%2520consistency%2520of%2520the%2520inpainting%252C%2520we%250Aintroduce%2520a%2520novel%2520time-reversal%2520framework%2520to%2520inpaint%2520frames%2520in%2520reverse%2520order%250Aand%2520use%2520later%2520frames%2520as%2520references%2520for%2520earlier%2520frames%2520to%2520fully%2520utilize%2520the%250Along-trajectory%2520observations.%2520Our%2520experiments%2520conducted%2520on%2520the%2520street%2520scene%250Adataset%2520successfully%2520reconstructed%2520a%25203D%2520representation%2520of%2520the%2520empty%2520street.%2520The%250Amesh%2520representation%2520of%2520the%2520empty%2520street%2520can%2520be%2520extracted%2520for%2520further%250Aapplications.%2520The%2520project%2520page%2520and%2520more%2520visualizations%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//streetunveiler.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18416v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20StreetUnveiler%20with%20Semantic-Aware%202DGS&entry.906535625=Jingwei%20Xu%20and%20Yikai%20Wang%20and%20Yiqun%20Zhao%20and%20Yanwei%20Fu%20and%20Shenghua%20Gao&entry.1292438233=%20%20Unveiling%20an%20empty%20street%20from%20crowded%20observations%20captured%20by%20in-car%0Acameras%20is%20crucial%20for%20autonomous%20driving.%20However%2C%20removing%20all%20temporarily%0Astatic%20objects%2C%20such%20as%20stopped%20vehicles%20and%20standing%20pedestrians%2C%20presents%20a%0Asignificant%20challenge.%20Unlike%20object-centric%203D%20inpainting%2C%20which%20relies%20on%0Athorough%20observation%20in%20a%20small%20scene%2C%20street%20scene%20cases%20involve%20long%0Atrajectories%20that%20differ%20from%20previous%203D%20inpainting%20tasks.%20The%20camera-centric%0Amoving%20environment%20of%20captured%20videos%20further%20complicates%20the%20task%20due%20to%20the%0Alimited%20degree%20and%20time%20duration%20of%20object%20observation.%20To%20address%20these%0Aobstacles%2C%20we%20introduce%20StreetUnveiler%20to%20reconstruct%20an%20empty%20street.%0AStreetUnveiler%20learns%20a%203D%20representation%20of%20the%20empty%20street%20from%20crowded%0Aobservations.%20Our%20representation%20is%20based%20on%20the%20hard-label%20semantic%202D%0AGaussian%20Splatting%20%282DGS%29%20for%20its%20scalability%20and%20ability%20to%20identify%20Gaussians%0Ato%20be%20removed.%20We%20inpaint%20rendered%20image%20after%20removing%20unwanted%20Gaussians%20to%0Aprovide%20pseudo-labels%20and%20subsequently%20re-optimize%20the%202DGS.%20Given%20its%20temporal%0Acontinuous%20movement%2C%20we%20divide%20the%20empty%20street%20scene%20into%20observed%2C%0Apartial-observed%2C%20and%20unobserved%20regions%2C%20which%20we%20propose%20to%20locate%20through%20a%0Arendered%20alpha%20map.%20This%20decomposition%20helps%20us%20to%20minimize%20the%20regions%20that%0Aneed%20to%20be%20inpainted.%20To%20enhance%20the%20temporal%20consistency%20of%20the%20inpainting%2C%20we%0Aintroduce%20a%20novel%20time-reversal%20framework%20to%20inpaint%20frames%20in%20reverse%20order%0Aand%20use%20later%20frames%20as%20references%20for%20earlier%20frames%20to%20fully%20utilize%20the%0Along-trajectory%20observations.%20Our%20experiments%20conducted%20on%20the%20street%20scene%0Adataset%20successfully%20reconstructed%20a%203D%20representation%20of%20the%20empty%20street.%20The%0Amesh%20representation%20of%20the%20empty%20street%20can%20be%20extracted%20for%20further%0Aapplications.%20The%20project%20page%20and%20more%20visualizations%20can%20be%20found%20at%3A%0Ahttps%3A//streetunveiler.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18416v2&entry.124074799=Read"},
{"title": "GECO: Generative Image-to-3D within a SECOnd", "author": "Chen Wang and Jiatao Gu and Xiaoxiao Long and Yuan Liu and Lingjie Liu", "abstract": "  3D generation has seen remarkable progress in recent years. Existing\ntechniques, such as score distillation methods, produce notable results but\nrequire extensive per-scene optimization, impacting time efficiency.\nAlternatively, reconstruction-based approaches prioritize efficiency but\ncompromise quality due to their limited handling of uncertainty. We introduce\nGECO, a novel method for high-quality 3D generative modeling that operates\nwithin a second. Our approach addresses the prevalent issues of uncertainty and\ninefficiency in current methods through a two-stage approach. In the initial\nstage, we train a single-step multi-view generative model with score\ndistillation. Then, a second-stage distillation is applied to address the\nchallenge of view inconsistency from the multi-view prediction. This two-stage\nprocess ensures a balanced approach to 3D generation, optimizing both quality\nand efficiency. Our comprehensive experiments demonstrate that GECO achieves\nhigh-quality image-to-3D generation with an unprecedented level of efficiency.\n", "link": "http://arxiv.org/abs/2405.20327v1", "date": "2024-05-30", "relevancy": 3.0343, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.614}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.614}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GECO%3A%20Generative%20Image-to-3D%20within%20a%20SECOnd&body=Title%3A%20GECO%3A%20Generative%20Image-to-3D%20within%20a%20SECOnd%0AAuthor%3A%20Chen%20Wang%20and%20Jiatao%20Gu%20and%20Xiaoxiao%20Long%20and%20Yuan%20Liu%20and%20Lingjie%20Liu%0AAbstract%3A%20%20%203D%20generation%20has%20seen%20remarkable%20progress%20in%20recent%20years.%20Existing%0Atechniques%2C%20such%20as%20score%20distillation%20methods%2C%20produce%20notable%20results%20but%0Arequire%20extensive%20per-scene%20optimization%2C%20impacting%20time%20efficiency.%0AAlternatively%2C%20reconstruction-based%20approaches%20prioritize%20efficiency%20but%0Acompromise%20quality%20due%20to%20their%20limited%20handling%20of%20uncertainty.%20We%20introduce%0AGECO%2C%20a%20novel%20method%20for%20high-quality%203D%20generative%20modeling%20that%20operates%0Awithin%20a%20second.%20Our%20approach%20addresses%20the%20prevalent%20issues%20of%20uncertainty%20and%0Ainefficiency%20in%20current%20methods%20through%20a%20two-stage%20approach.%20In%20the%20initial%0Astage%2C%20we%20train%20a%20single-step%20multi-view%20generative%20model%20with%20score%0Adistillation.%20Then%2C%20a%20second-stage%20distillation%20is%20applied%20to%20address%20the%0Achallenge%20of%20view%20inconsistency%20from%20the%20multi-view%20prediction.%20This%20two-stage%0Aprocess%20ensures%20a%20balanced%20approach%20to%203D%20generation%2C%20optimizing%20both%20quality%0Aand%20efficiency.%20Our%20comprehensive%20experiments%20demonstrate%20that%20GECO%20achieves%0Ahigh-quality%20image-to-3D%20generation%20with%20an%20unprecedented%20level%20of%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGECO%253A%2520Generative%2520Image-to-3D%2520within%2520a%2520SECOnd%26entry.906535625%3DChen%2520Wang%2520and%2520Jiatao%2520Gu%2520and%2520Xiaoxiao%2520Long%2520and%2520Yuan%2520Liu%2520and%2520Lingjie%2520Liu%26entry.1292438233%3D%2520%25203D%2520generation%2520has%2520seen%2520remarkable%2520progress%2520in%2520recent%2520years.%2520Existing%250Atechniques%252C%2520such%2520as%2520score%2520distillation%2520methods%252C%2520produce%2520notable%2520results%2520but%250Arequire%2520extensive%2520per-scene%2520optimization%252C%2520impacting%2520time%2520efficiency.%250AAlternatively%252C%2520reconstruction-based%2520approaches%2520prioritize%2520efficiency%2520but%250Acompromise%2520quality%2520due%2520to%2520their%2520limited%2520handling%2520of%2520uncertainty.%2520We%2520introduce%250AGECO%252C%2520a%2520novel%2520method%2520for%2520high-quality%25203D%2520generative%2520modeling%2520that%2520operates%250Awithin%2520a%2520second.%2520Our%2520approach%2520addresses%2520the%2520prevalent%2520issues%2520of%2520uncertainty%2520and%250Ainefficiency%2520in%2520current%2520methods%2520through%2520a%2520two-stage%2520approach.%2520In%2520the%2520initial%250Astage%252C%2520we%2520train%2520a%2520single-step%2520multi-view%2520generative%2520model%2520with%2520score%250Adistillation.%2520Then%252C%2520a%2520second-stage%2520distillation%2520is%2520applied%2520to%2520address%2520the%250Achallenge%2520of%2520view%2520inconsistency%2520from%2520the%2520multi-view%2520prediction.%2520This%2520two-stage%250Aprocess%2520ensures%2520a%2520balanced%2520approach%2520to%25203D%2520generation%252C%2520optimizing%2520both%2520quality%250Aand%2520efficiency.%2520Our%2520comprehensive%2520experiments%2520demonstrate%2520that%2520GECO%2520achieves%250Ahigh-quality%2520image-to-3D%2520generation%2520with%2520an%2520unprecedented%2520level%2520of%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GECO%3A%20Generative%20Image-to-3D%20within%20a%20SECOnd&entry.906535625=Chen%20Wang%20and%20Jiatao%20Gu%20and%20Xiaoxiao%20Long%20and%20Yuan%20Liu%20and%20Lingjie%20Liu&entry.1292438233=%20%203D%20generation%20has%20seen%20remarkable%20progress%20in%20recent%20years.%20Existing%0Atechniques%2C%20such%20as%20score%20distillation%20methods%2C%20produce%20notable%20results%20but%0Arequire%20extensive%20per-scene%20optimization%2C%20impacting%20time%20efficiency.%0AAlternatively%2C%20reconstruction-based%20approaches%20prioritize%20efficiency%20but%0Acompromise%20quality%20due%20to%20their%20limited%20handling%20of%20uncertainty.%20We%20introduce%0AGECO%2C%20a%20novel%20method%20for%20high-quality%203D%20generative%20modeling%20that%20operates%0Awithin%20a%20second.%20Our%20approach%20addresses%20the%20prevalent%20issues%20of%20uncertainty%20and%0Ainefficiency%20in%20current%20methods%20through%20a%20two-stage%20approach.%20In%20the%20initial%0Astage%2C%20we%20train%20a%20single-step%20multi-view%20generative%20model%20with%20score%0Adistillation.%20Then%2C%20a%20second-stage%20distillation%20is%20applied%20to%20address%20the%0Achallenge%20of%20view%20inconsistency%20from%20the%20multi-view%20prediction.%20This%20two-stage%0Aprocess%20ensures%20a%20balanced%20approach%20to%203D%20generation%2C%20optimizing%20both%20quality%0Aand%20efficiency.%20Our%20comprehensive%20experiments%20demonstrate%20that%20GECO%20achieves%0Ahigh-quality%20image-to-3D%20generation%20with%20an%20unprecedented%20level%20of%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20327v1&entry.124074799=Read"},
{"title": "N-Dimensional Gaussians for Fitting of High Dimensional Functions", "author": "Stavros Diolatzis and Tobias Zirr and Alexandr Kuznetsov and Georgios Kopanas and Anton Kaplanyan", "abstract": "  In the wake of many new ML-inspired approaches for reconstructing and\nrepresenting high-quality 3D content, recent hybrid and explicitly learned\nrepresentations exhibit promising performance and quality characteristics.\nHowever, their scaling to higher dimensions is challenging, e.g. when\naccounting for dynamic content with respect to additional parameters such as\nmaterial properties, illumination, or time. In this paper, we tackle these\nchallenges for an explicit representations based on Gaussian mixture models.\nWith our solutions, we arrive at efficient fitting of compact N-dimensional\nGaussian mixtures and enable efficient evaluation at render time: For fast\nfitting and evaluation, we introduce a high-dimensional culling scheme that\nefficiently bounds N-D Gaussians, inspired by Locality Sensitive Hashing. For\nadaptive refinement yet compact representation, we introduce a loss-adaptive\ndensity control scheme that incrementally guides the use of additional capacity\ntowards missing details. With these tools we can for the first time represent\ncomplex appearance that depends on many input dimensions beyond position or\nviewing angle within a compact, explicit representation optimized in minutes\nand rendered in milliseconds.\n", "link": "http://arxiv.org/abs/2405.20067v1", "date": "2024-05-30", "relevancy": 3.0295, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6372}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5902}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20N-Dimensional%20Gaussians%20for%20Fitting%20of%20High%20Dimensional%20Functions&body=Title%3A%20N-Dimensional%20Gaussians%20for%20Fitting%20of%20High%20Dimensional%20Functions%0AAuthor%3A%20Stavros%20Diolatzis%20and%20Tobias%20Zirr%20and%20Alexandr%20Kuznetsov%20and%20Georgios%20Kopanas%20and%20Anton%20Kaplanyan%0AAbstract%3A%20%20%20In%20the%20wake%20of%20many%20new%20ML-inspired%20approaches%20for%20reconstructing%20and%0Arepresenting%20high-quality%203D%20content%2C%20recent%20hybrid%20and%20explicitly%20learned%0Arepresentations%20exhibit%20promising%20performance%20and%20quality%20characteristics.%0AHowever%2C%20their%20scaling%20to%20higher%20dimensions%20is%20challenging%2C%20e.g.%20when%0Aaccounting%20for%20dynamic%20content%20with%20respect%20to%20additional%20parameters%20such%20as%0Amaterial%20properties%2C%20illumination%2C%20or%20time.%20In%20this%20paper%2C%20we%20tackle%20these%0Achallenges%20for%20an%20explicit%20representations%20based%20on%20Gaussian%20mixture%20models.%0AWith%20our%20solutions%2C%20we%20arrive%20at%20efficient%20fitting%20of%20compact%20N-dimensional%0AGaussian%20mixtures%20and%20enable%20efficient%20evaluation%20at%20render%20time%3A%20For%20fast%0Afitting%20and%20evaluation%2C%20we%20introduce%20a%20high-dimensional%20culling%20scheme%20that%0Aefficiently%20bounds%20N-D%20Gaussians%2C%20inspired%20by%20Locality%20Sensitive%20Hashing.%20For%0Aadaptive%20refinement%20yet%20compact%20representation%2C%20we%20introduce%20a%20loss-adaptive%0Adensity%20control%20scheme%20that%20incrementally%20guides%20the%20use%20of%20additional%20capacity%0Atowards%20missing%20details.%20With%20these%20tools%20we%20can%20for%20the%20first%20time%20represent%0Acomplex%20appearance%20that%20depends%20on%20many%20input%20dimensions%20beyond%20position%20or%0Aviewing%20angle%20within%20a%20compact%2C%20explicit%20representation%20optimized%20in%20minutes%0Aand%20rendered%20in%20milliseconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DN-Dimensional%2520Gaussians%2520for%2520Fitting%2520of%2520High%2520Dimensional%2520Functions%26entry.906535625%3DStavros%2520Diolatzis%2520and%2520Tobias%2520Zirr%2520and%2520Alexandr%2520Kuznetsov%2520and%2520Georgios%2520Kopanas%2520and%2520Anton%2520Kaplanyan%26entry.1292438233%3D%2520%2520In%2520the%2520wake%2520of%2520many%2520new%2520ML-inspired%2520approaches%2520for%2520reconstructing%2520and%250Arepresenting%2520high-quality%25203D%2520content%252C%2520recent%2520hybrid%2520and%2520explicitly%2520learned%250Arepresentations%2520exhibit%2520promising%2520performance%2520and%2520quality%2520characteristics.%250AHowever%252C%2520their%2520scaling%2520to%2520higher%2520dimensions%2520is%2520challenging%252C%2520e.g.%2520when%250Aaccounting%2520for%2520dynamic%2520content%2520with%2520respect%2520to%2520additional%2520parameters%2520such%2520as%250Amaterial%2520properties%252C%2520illumination%252C%2520or%2520time.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520these%250Achallenges%2520for%2520an%2520explicit%2520representations%2520based%2520on%2520Gaussian%2520mixture%2520models.%250AWith%2520our%2520solutions%252C%2520we%2520arrive%2520at%2520efficient%2520fitting%2520of%2520compact%2520N-dimensional%250AGaussian%2520mixtures%2520and%2520enable%2520efficient%2520evaluation%2520at%2520render%2520time%253A%2520For%2520fast%250Afitting%2520and%2520evaluation%252C%2520we%2520introduce%2520a%2520high-dimensional%2520culling%2520scheme%2520that%250Aefficiently%2520bounds%2520N-D%2520Gaussians%252C%2520inspired%2520by%2520Locality%2520Sensitive%2520Hashing.%2520For%250Aadaptive%2520refinement%2520yet%2520compact%2520representation%252C%2520we%2520introduce%2520a%2520loss-adaptive%250Adensity%2520control%2520scheme%2520that%2520incrementally%2520guides%2520the%2520use%2520of%2520additional%2520capacity%250Atowards%2520missing%2520details.%2520With%2520these%2520tools%2520we%2520can%2520for%2520the%2520first%2520time%2520represent%250Acomplex%2520appearance%2520that%2520depends%2520on%2520many%2520input%2520dimensions%2520beyond%2520position%2520or%250Aviewing%2520angle%2520within%2520a%2520compact%252C%2520explicit%2520representation%2520optimized%2520in%2520minutes%250Aand%2520rendered%2520in%2520milliseconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=N-Dimensional%20Gaussians%20for%20Fitting%20of%20High%20Dimensional%20Functions&entry.906535625=Stavros%20Diolatzis%20and%20Tobias%20Zirr%20and%20Alexandr%20Kuznetsov%20and%20Georgios%20Kopanas%20and%20Anton%20Kaplanyan&entry.1292438233=%20%20In%20the%20wake%20of%20many%20new%20ML-inspired%20approaches%20for%20reconstructing%20and%0Arepresenting%20high-quality%203D%20content%2C%20recent%20hybrid%20and%20explicitly%20learned%0Arepresentations%20exhibit%20promising%20performance%20and%20quality%20characteristics.%0AHowever%2C%20their%20scaling%20to%20higher%20dimensions%20is%20challenging%2C%20e.g.%20when%0Aaccounting%20for%20dynamic%20content%20with%20respect%20to%20additional%20parameters%20such%20as%0Amaterial%20properties%2C%20illumination%2C%20or%20time.%20In%20this%20paper%2C%20we%20tackle%20these%0Achallenges%20for%20an%20explicit%20representations%20based%20on%20Gaussian%20mixture%20models.%0AWith%20our%20solutions%2C%20we%20arrive%20at%20efficient%20fitting%20of%20compact%20N-dimensional%0AGaussian%20mixtures%20and%20enable%20efficient%20evaluation%20at%20render%20time%3A%20For%20fast%0Afitting%20and%20evaluation%2C%20we%20introduce%20a%20high-dimensional%20culling%20scheme%20that%0Aefficiently%20bounds%20N-D%20Gaussians%2C%20inspired%20by%20Locality%20Sensitive%20Hashing.%20For%0Aadaptive%20refinement%20yet%20compact%20representation%2C%20we%20introduce%20a%20loss-adaptive%0Adensity%20control%20scheme%20that%20incrementally%20guides%20the%20use%20of%20additional%20capacity%0Atowards%20missing%20details.%20With%20these%20tools%20we%20can%20for%20the%20first%20time%20represent%0Acomplex%20appearance%20that%20depends%20on%20many%20input%20dimensions%20beyond%20position%20or%0Aviewing%20angle%20within%20a%20compact%2C%20explicit%20representation%20optimized%20in%20minutes%0Aand%20rendered%20in%20milliseconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20067v1&entry.124074799=Read"},
{"title": "Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback", "author": "Sanghyeon Na and Yonggyu Kim and Hyunjoon Lee", "abstract": "  The generation of high-quality human images through text-to-image (T2I)\nmethods is a significant yet challenging task. Distinct from general image\ngeneration, human image synthesis must satisfy stringent criteria related to\nhuman pose, anatomy, and alignment with textual prompts, making it particularly\ndifficult to achieve realistic results. Recent advancements in T2I generation\nbased on diffusion models have shown promise, yet challenges remain in meeting\nhuman-specific preferences. In this paper, we introduce a novel approach\ntailored specifically for human image generation utilizing Direct Preference\nOptimization (DPO). Specifically, we introduce an efficient method for\nconstructing a specialized DPO dataset for training human image generation\nmodels without the need for costly human feedback. We also propose a modified\nloss function that enhances the DPO training process by minimizing artifacts\nand improving image fidelity. Our method demonstrates its versatility and\neffectiveness in generating human images, including personalized text-to-image\ngeneration. Through comprehensive evaluations, we show that our approach\nsignificantly advances the state of human image generation, achieving superior\nresults in terms of natural anatomies, poses, and text-image alignment.\n", "link": "http://arxiv.org/abs/2405.20216v1", "date": "2024-05-30", "relevancy": 3.0015, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.625}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5944}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boost%20Your%20Own%20Human%20Image%20Generation%20Model%20via%20Direct%20Preference%0A%20%20Optimization%20with%20AI%20Feedback&body=Title%3A%20Boost%20Your%20Own%20Human%20Image%20Generation%20Model%20via%20Direct%20Preference%0A%20%20Optimization%20with%20AI%20Feedback%0AAuthor%3A%20Sanghyeon%20Na%20and%20Yonggyu%20Kim%20and%20Hyunjoon%20Lee%0AAbstract%3A%20%20%20The%20generation%20of%20high-quality%20human%20images%20through%20text-to-image%20%28T2I%29%0Amethods%20is%20a%20significant%20yet%20challenging%20task.%20Distinct%20from%20general%20image%0Ageneration%2C%20human%20image%20synthesis%20must%20satisfy%20stringent%20criteria%20related%20to%0Ahuman%20pose%2C%20anatomy%2C%20and%20alignment%20with%20textual%20prompts%2C%20making%20it%20particularly%0Adifficult%20to%20achieve%20realistic%20results.%20Recent%20advancements%20in%20T2I%20generation%0Abased%20on%20diffusion%20models%20have%20shown%20promise%2C%20yet%20challenges%20remain%20in%20meeting%0Ahuman-specific%20preferences.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%0Atailored%20specifically%20for%20human%20image%20generation%20utilizing%20Direct%20Preference%0AOptimization%20%28DPO%29.%20Specifically%2C%20we%20introduce%20an%20efficient%20method%20for%0Aconstructing%20a%20specialized%20DPO%20dataset%20for%20training%20human%20image%20generation%0Amodels%20without%20the%20need%20for%20costly%20human%20feedback.%20We%20also%20propose%20a%20modified%0Aloss%20function%20that%20enhances%20the%20DPO%20training%20process%20by%20minimizing%20artifacts%0Aand%20improving%20image%20fidelity.%20Our%20method%20demonstrates%20its%20versatility%20and%0Aeffectiveness%20in%20generating%20human%20images%2C%20including%20personalized%20text-to-image%0Ageneration.%20Through%20comprehensive%20evaluations%2C%20we%20show%20that%20our%20approach%0Asignificantly%20advances%20the%20state%20of%20human%20image%20generation%2C%20achieving%20superior%0Aresults%20in%20terms%20of%20natural%20anatomies%2C%20poses%2C%20and%20text-image%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoost%2520Your%2520Own%2520Human%2520Image%2520Generation%2520Model%2520via%2520Direct%2520Preference%250A%2520%2520Optimization%2520with%2520AI%2520Feedback%26entry.906535625%3DSanghyeon%2520Na%2520and%2520Yonggyu%2520Kim%2520and%2520Hyunjoon%2520Lee%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520high-quality%2520human%2520images%2520through%2520text-to-image%2520%2528T2I%2529%250Amethods%2520is%2520a%2520significant%2520yet%2520challenging%2520task.%2520Distinct%2520from%2520general%2520image%250Ageneration%252C%2520human%2520image%2520synthesis%2520must%2520satisfy%2520stringent%2520criteria%2520related%2520to%250Ahuman%2520pose%252C%2520anatomy%252C%2520and%2520alignment%2520with%2520textual%2520prompts%252C%2520making%2520it%2520particularly%250Adifficult%2520to%2520achieve%2520realistic%2520results.%2520Recent%2520advancements%2520in%2520T2I%2520generation%250Abased%2520on%2520diffusion%2520models%2520have%2520shown%2520promise%252C%2520yet%2520challenges%2520remain%2520in%2520meeting%250Ahuman-specific%2520preferences.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%250Atailored%2520specifically%2520for%2520human%2520image%2520generation%2520utilizing%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529.%2520Specifically%252C%2520we%2520introduce%2520an%2520efficient%2520method%2520for%250Aconstructing%2520a%2520specialized%2520DPO%2520dataset%2520for%2520training%2520human%2520image%2520generation%250Amodels%2520without%2520the%2520need%2520for%2520costly%2520human%2520feedback.%2520We%2520also%2520propose%2520a%2520modified%250Aloss%2520function%2520that%2520enhances%2520the%2520DPO%2520training%2520process%2520by%2520minimizing%2520artifacts%250Aand%2520improving%2520image%2520fidelity.%2520Our%2520method%2520demonstrates%2520its%2520versatility%2520and%250Aeffectiveness%2520in%2520generating%2520human%2520images%252C%2520including%2520personalized%2520text-to-image%250Ageneration.%2520Through%2520comprehensive%2520evaluations%252C%2520we%2520show%2520that%2520our%2520approach%250Asignificantly%2520advances%2520the%2520state%2520of%2520human%2520image%2520generation%252C%2520achieving%2520superior%250Aresults%2520in%2520terms%2520of%2520natural%2520anatomies%252C%2520poses%252C%2520and%2520text-image%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boost%20Your%20Own%20Human%20Image%20Generation%20Model%20via%20Direct%20Preference%0A%20%20Optimization%20with%20AI%20Feedback&entry.906535625=Sanghyeon%20Na%20and%20Yonggyu%20Kim%20and%20Hyunjoon%20Lee&entry.1292438233=%20%20The%20generation%20of%20high-quality%20human%20images%20through%20text-to-image%20%28T2I%29%0Amethods%20is%20a%20significant%20yet%20challenging%20task.%20Distinct%20from%20general%20image%0Ageneration%2C%20human%20image%20synthesis%20must%20satisfy%20stringent%20criteria%20related%20to%0Ahuman%20pose%2C%20anatomy%2C%20and%20alignment%20with%20textual%20prompts%2C%20making%20it%20particularly%0Adifficult%20to%20achieve%20realistic%20results.%20Recent%20advancements%20in%20T2I%20generation%0Abased%20on%20diffusion%20models%20have%20shown%20promise%2C%20yet%20challenges%20remain%20in%20meeting%0Ahuman-specific%20preferences.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%0Atailored%20specifically%20for%20human%20image%20generation%20utilizing%20Direct%20Preference%0AOptimization%20%28DPO%29.%20Specifically%2C%20we%20introduce%20an%20efficient%20method%20for%0Aconstructing%20a%20specialized%20DPO%20dataset%20for%20training%20human%20image%20generation%0Amodels%20without%20the%20need%20for%20costly%20human%20feedback.%20We%20also%20propose%20a%20modified%0Aloss%20function%20that%20enhances%20the%20DPO%20training%20process%20by%20minimizing%20artifacts%0Aand%20improving%20image%20fidelity.%20Our%20method%20demonstrates%20its%20versatility%20and%0Aeffectiveness%20in%20generating%20human%20images%2C%20including%20personalized%20text-to-image%0Ageneration.%20Through%20comprehensive%20evaluations%2C%20we%20show%20that%20our%20approach%0Asignificantly%20advances%20the%20state%20of%20human%20image%20generation%2C%20achieving%20superior%0Aresults%20in%20terms%20of%20natural%20anatomies%2C%20poses%2C%20and%20text-image%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20216v1&entry.124074799=Read"},
{"title": "TetSphere Splatting: Representing High-Quality Geometry with Lagrangian\n  Volumetric Meshes", "author": "Minghao Guo and Bohan Wang and Kaiming He and Wojciech Matusik", "abstract": "  We present TetSphere splatting, an explicit, Lagrangian representation for\nreconstructing 3D shapes with high-quality geometry. In contrast to\nconventional object reconstruction methods which predominantly use Eulerian\nrepresentations, including both neural implicit (e.g., NeRF, NeuS) and explicit\nrepresentations (e.g., DMTet), and often struggle with high computational\ndemands and suboptimal mesh quality, TetSphere splatting utilizes an underused\nbut highly effective geometric primitive -- tetrahedral meshes. This approach\ndirectly yields superior mesh quality without relying on neural networks or\npost-processing. It deforms multiple initial tetrahedral spheres to accurately\nreconstruct the 3D shape through a combination of differentiable rendering and\ngeometric energy optimization, resulting in significant computational\nefficiency. Serving as a robust and versatile geometry representation,\nTet-Sphere splatting seamlessly integrates into diverse applications, including\nsingle-view 3D reconstruction, image-/text-to-3D content generation.\nExperimental results demonstrate that TetSphere splatting outperforms existing\nrepresentations, delivering faster optimization speed, enhanced mesh quality,\nand reliable preservation of thin structures.\n", "link": "http://arxiv.org/abs/2405.20283v1", "date": "2024-05-30", "relevancy": 2.9648, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6399}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6106}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TetSphere%20Splatting%3A%20Representing%20High-Quality%20Geometry%20with%20Lagrangian%0A%20%20Volumetric%20Meshes&body=Title%3A%20TetSphere%20Splatting%3A%20Representing%20High-Quality%20Geometry%20with%20Lagrangian%0A%20%20Volumetric%20Meshes%0AAuthor%3A%20Minghao%20Guo%20and%20Bohan%20Wang%20and%20Kaiming%20He%20and%20Wojciech%20Matusik%0AAbstract%3A%20%20%20We%20present%20TetSphere%20splatting%2C%20an%20explicit%2C%20Lagrangian%20representation%20for%0Areconstructing%203D%20shapes%20with%20high-quality%20geometry.%20In%20contrast%20to%0Aconventional%20object%20reconstruction%20methods%20which%20predominantly%20use%20Eulerian%0Arepresentations%2C%20including%20both%20neural%20implicit%20%28e.g.%2C%20NeRF%2C%20NeuS%29%20and%20explicit%0Arepresentations%20%28e.g.%2C%20DMTet%29%2C%20and%20often%20struggle%20with%20high%20computational%0Ademands%20and%20suboptimal%20mesh%20quality%2C%20TetSphere%20splatting%20utilizes%20an%20underused%0Abut%20highly%20effective%20geometric%20primitive%20--%20tetrahedral%20meshes.%20This%20approach%0Adirectly%20yields%20superior%20mesh%20quality%20without%20relying%20on%20neural%20networks%20or%0Apost-processing.%20It%20deforms%20multiple%20initial%20tetrahedral%20spheres%20to%20accurately%0Areconstruct%20the%203D%20shape%20through%20a%20combination%20of%20differentiable%20rendering%20and%0Ageometric%20energy%20optimization%2C%20resulting%20in%20significant%20computational%0Aefficiency.%20Serving%20as%20a%20robust%20and%20versatile%20geometry%20representation%2C%0ATet-Sphere%20splatting%20seamlessly%20integrates%20into%20diverse%20applications%2C%20including%0Asingle-view%203D%20reconstruction%2C%20image-/text-to-3D%20content%20generation.%0AExperimental%20results%20demonstrate%20that%20TetSphere%20splatting%20outperforms%20existing%0Arepresentations%2C%20delivering%20faster%20optimization%20speed%2C%20enhanced%20mesh%20quality%2C%0Aand%20reliable%20preservation%20of%20thin%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTetSphere%2520Splatting%253A%2520Representing%2520High-Quality%2520Geometry%2520with%2520Lagrangian%250A%2520%2520Volumetric%2520Meshes%26entry.906535625%3DMinghao%2520Guo%2520and%2520Bohan%2520Wang%2520and%2520Kaiming%2520He%2520and%2520Wojciech%2520Matusik%26entry.1292438233%3D%2520%2520We%2520present%2520TetSphere%2520splatting%252C%2520an%2520explicit%252C%2520Lagrangian%2520representation%2520for%250Areconstructing%25203D%2520shapes%2520with%2520high-quality%2520geometry.%2520In%2520contrast%2520to%250Aconventional%2520object%2520reconstruction%2520methods%2520which%2520predominantly%2520use%2520Eulerian%250Arepresentations%252C%2520including%2520both%2520neural%2520implicit%2520%2528e.g.%252C%2520NeRF%252C%2520NeuS%2529%2520and%2520explicit%250Arepresentations%2520%2528e.g.%252C%2520DMTet%2529%252C%2520and%2520often%2520struggle%2520with%2520high%2520computational%250Ademands%2520and%2520suboptimal%2520mesh%2520quality%252C%2520TetSphere%2520splatting%2520utilizes%2520an%2520underused%250Abut%2520highly%2520effective%2520geometric%2520primitive%2520--%2520tetrahedral%2520meshes.%2520This%2520approach%250Adirectly%2520yields%2520superior%2520mesh%2520quality%2520without%2520relying%2520on%2520neural%2520networks%2520or%250Apost-processing.%2520It%2520deforms%2520multiple%2520initial%2520tetrahedral%2520spheres%2520to%2520accurately%250Areconstruct%2520the%25203D%2520shape%2520through%2520a%2520combination%2520of%2520differentiable%2520rendering%2520and%250Ageometric%2520energy%2520optimization%252C%2520resulting%2520in%2520significant%2520computational%250Aefficiency.%2520Serving%2520as%2520a%2520robust%2520and%2520versatile%2520geometry%2520representation%252C%250ATet-Sphere%2520splatting%2520seamlessly%2520integrates%2520into%2520diverse%2520applications%252C%2520including%250Asingle-view%25203D%2520reconstruction%252C%2520image-/text-to-3D%2520content%2520generation.%250AExperimental%2520results%2520demonstrate%2520that%2520TetSphere%2520splatting%2520outperforms%2520existing%250Arepresentations%252C%2520delivering%2520faster%2520optimization%2520speed%252C%2520enhanced%2520mesh%2520quality%252C%250Aand%2520reliable%2520preservation%2520of%2520thin%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TetSphere%20Splatting%3A%20Representing%20High-Quality%20Geometry%20with%20Lagrangian%0A%20%20Volumetric%20Meshes&entry.906535625=Minghao%20Guo%20and%20Bohan%20Wang%20and%20Kaiming%20He%20and%20Wojciech%20Matusik&entry.1292438233=%20%20We%20present%20TetSphere%20splatting%2C%20an%20explicit%2C%20Lagrangian%20representation%20for%0Areconstructing%203D%20shapes%20with%20high-quality%20geometry.%20In%20contrast%20to%0Aconventional%20object%20reconstruction%20methods%20which%20predominantly%20use%20Eulerian%0Arepresentations%2C%20including%20both%20neural%20implicit%20%28e.g.%2C%20NeRF%2C%20NeuS%29%20and%20explicit%0Arepresentations%20%28e.g.%2C%20DMTet%29%2C%20and%20often%20struggle%20with%20high%20computational%0Ademands%20and%20suboptimal%20mesh%20quality%2C%20TetSphere%20splatting%20utilizes%20an%20underused%0Abut%20highly%20effective%20geometric%20primitive%20--%20tetrahedral%20meshes.%20This%20approach%0Adirectly%20yields%20superior%20mesh%20quality%20without%20relying%20on%20neural%20networks%20or%0Apost-processing.%20It%20deforms%20multiple%20initial%20tetrahedral%20spheres%20to%20accurately%0Areconstruct%20the%203D%20shape%20through%20a%20combination%20of%20differentiable%20rendering%20and%0Ageometric%20energy%20optimization%2C%20resulting%20in%20significant%20computational%0Aefficiency.%20Serving%20as%20a%20robust%20and%20versatile%20geometry%20representation%2C%0ATet-Sphere%20splatting%20seamlessly%20integrates%20into%20diverse%20applications%2C%20including%0Asingle-view%203D%20reconstruction%2C%20image-/text-to-3D%20content%20generation.%0AExperimental%20results%20demonstrate%20that%20TetSphere%20splatting%20outperforms%20existing%0Arepresentations%2C%20delivering%20faster%20optimization%20speed%2C%20enhanced%20mesh%20quality%2C%0Aand%20reliable%20preservation%20of%20thin%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20283v1&entry.124074799=Read"},
{"title": "Infinite 3D Landmarks: Improving Continuous 2D Facial Landmark Detection", "author": "Prashanth Chandran and Gaspard Zoss and Paulo Gotardo and Derek Bradley", "abstract": "  In this paper, we examine 3 important issues in the practical use of\nstate-of-the-art facial landmark detectors and show how a combination of\nspecific architectural modifications can directly improve their accuracy and\ntemporal stability. First, many facial landmark detectors require face\nnormalization as a preprocessing step, which is accomplished by a\nseparately-trained neural network that crops and resizes the face in the input\nimage. There is no guarantee that this pre-trained network performs the optimal\nface normalization for landmark detection. We instead analyze the use of a\nspatial transformer network that is trained alongside the landmark detector in\nan unsupervised manner, and jointly learn optimal face normalization and\nlandmark detection. Second, we show that modifying the output head of the\nlandmark predictor to infer landmarks in a canonical 3D space can further\nimprove accuracy. To convert the predicted 3D landmarks into screen-space, we\nadditionally predict the camera intrinsics and head pose from the input image.\nAs a side benefit, this allows to predict the 3D face shape from a given image\nonly using 2D landmarks as supervision, which is useful in determining landmark\nvisibility among other things. Finally, when training a landmark detector on\nmultiple datasets at the same time, annotation inconsistencies across datasets\nforces the network to produce a suboptimal average. We propose to add a\nsemantic correction network to address this issue. This additional lightweight\nneural network is trained alongside the landmark detector, without requiring\nany additional supervision. While the insights of this paper can be applied to\nmost common landmark detectors, we specifically target a recently-proposed\ncontinuous 2D landmark detector to demonstrate how each of our additions leads\nto meaningful improvements over the state-of-the-art on standard benchmarks.\n", "link": "http://arxiv.org/abs/2405.20117v1", "date": "2024-05-30", "relevancy": 2.8678, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6182}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinite%203D%20Landmarks%3A%20Improving%20Continuous%202D%20Facial%20Landmark%20Detection&body=Title%3A%20Infinite%203D%20Landmarks%3A%20Improving%20Continuous%202D%20Facial%20Landmark%20Detection%0AAuthor%3A%20Prashanth%20Chandran%20and%20Gaspard%20Zoss%20and%20Paulo%20Gotardo%20and%20Derek%20Bradley%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20examine%203%20important%20issues%20in%20the%20practical%20use%20of%0Astate-of-the-art%20facial%20landmark%20detectors%20and%20show%20how%20a%20combination%20of%0Aspecific%20architectural%20modifications%20can%20directly%20improve%20their%20accuracy%20and%0Atemporal%20stability.%20First%2C%20many%20facial%20landmark%20detectors%20require%20face%0Anormalization%20as%20a%20preprocessing%20step%2C%20which%20is%20accomplished%20by%20a%0Aseparately-trained%20neural%20network%20that%20crops%20and%20resizes%20the%20face%20in%20the%20input%0Aimage.%20There%20is%20no%20guarantee%20that%20this%20pre-trained%20network%20performs%20the%20optimal%0Aface%20normalization%20for%20landmark%20detection.%20We%20instead%20analyze%20the%20use%20of%20a%0Aspatial%20transformer%20network%20that%20is%20trained%20alongside%20the%20landmark%20detector%20in%0Aan%20unsupervised%20manner%2C%20and%20jointly%20learn%20optimal%20face%20normalization%20and%0Alandmark%20detection.%20Second%2C%20we%20show%20that%20modifying%20the%20output%20head%20of%20the%0Alandmark%20predictor%20to%20infer%20landmarks%20in%20a%20canonical%203D%20space%20can%20further%0Aimprove%20accuracy.%20To%20convert%20the%20predicted%203D%20landmarks%20into%20screen-space%2C%20we%0Aadditionally%20predict%20the%20camera%20intrinsics%20and%20head%20pose%20from%20the%20input%20image.%0AAs%20a%20side%20benefit%2C%20this%20allows%20to%20predict%20the%203D%20face%20shape%20from%20a%20given%20image%0Aonly%20using%202D%20landmarks%20as%20supervision%2C%20which%20is%20useful%20in%20determining%20landmark%0Avisibility%20among%20other%20things.%20Finally%2C%20when%20training%20a%20landmark%20detector%20on%0Amultiple%20datasets%20at%20the%20same%20time%2C%20annotation%20inconsistencies%20across%20datasets%0Aforces%20the%20network%20to%20produce%20a%20suboptimal%20average.%20We%20propose%20to%20add%20a%0Asemantic%20correction%20network%20to%20address%20this%20issue.%20This%20additional%20lightweight%0Aneural%20network%20is%20trained%20alongside%20the%20landmark%20detector%2C%20without%20requiring%0Aany%20additional%20supervision.%20While%20the%20insights%20of%20this%20paper%20can%20be%20applied%20to%0Amost%20common%20landmark%20detectors%2C%20we%20specifically%20target%20a%20recently-proposed%0Acontinuous%202D%20landmark%20detector%20to%20demonstrate%20how%20each%20of%20our%20additions%20leads%0Ato%20meaningful%20improvements%20over%20the%20state-of-the-art%20on%20standard%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinite%25203D%2520Landmarks%253A%2520Improving%2520Continuous%25202D%2520Facial%2520Landmark%2520Detection%26entry.906535625%3DPrashanth%2520Chandran%2520and%2520Gaspard%2520Zoss%2520and%2520Paulo%2520Gotardo%2520and%2520Derek%2520Bradley%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520examine%25203%2520important%2520issues%2520in%2520the%2520practical%2520use%2520of%250Astate-of-the-art%2520facial%2520landmark%2520detectors%2520and%2520show%2520how%2520a%2520combination%2520of%250Aspecific%2520architectural%2520modifications%2520can%2520directly%2520improve%2520their%2520accuracy%2520and%250Atemporal%2520stability.%2520First%252C%2520many%2520facial%2520landmark%2520detectors%2520require%2520face%250Anormalization%2520as%2520a%2520preprocessing%2520step%252C%2520which%2520is%2520accomplished%2520by%2520a%250Aseparately-trained%2520neural%2520network%2520that%2520crops%2520and%2520resizes%2520the%2520face%2520in%2520the%2520input%250Aimage.%2520There%2520is%2520no%2520guarantee%2520that%2520this%2520pre-trained%2520network%2520performs%2520the%2520optimal%250Aface%2520normalization%2520for%2520landmark%2520detection.%2520We%2520instead%2520analyze%2520the%2520use%2520of%2520a%250Aspatial%2520transformer%2520network%2520that%2520is%2520trained%2520alongside%2520the%2520landmark%2520detector%2520in%250Aan%2520unsupervised%2520manner%252C%2520and%2520jointly%2520learn%2520optimal%2520face%2520normalization%2520and%250Alandmark%2520detection.%2520Second%252C%2520we%2520show%2520that%2520modifying%2520the%2520output%2520head%2520of%2520the%250Alandmark%2520predictor%2520to%2520infer%2520landmarks%2520in%2520a%2520canonical%25203D%2520space%2520can%2520further%250Aimprove%2520accuracy.%2520To%2520convert%2520the%2520predicted%25203D%2520landmarks%2520into%2520screen-space%252C%2520we%250Aadditionally%2520predict%2520the%2520camera%2520intrinsics%2520and%2520head%2520pose%2520from%2520the%2520input%2520image.%250AAs%2520a%2520side%2520benefit%252C%2520this%2520allows%2520to%2520predict%2520the%25203D%2520face%2520shape%2520from%2520a%2520given%2520image%250Aonly%2520using%25202D%2520landmarks%2520as%2520supervision%252C%2520which%2520is%2520useful%2520in%2520determining%2520landmark%250Avisibility%2520among%2520other%2520things.%2520Finally%252C%2520when%2520training%2520a%2520landmark%2520detector%2520on%250Amultiple%2520datasets%2520at%2520the%2520same%2520time%252C%2520annotation%2520inconsistencies%2520across%2520datasets%250Aforces%2520the%2520network%2520to%2520produce%2520a%2520suboptimal%2520average.%2520We%2520propose%2520to%2520add%2520a%250Asemantic%2520correction%2520network%2520to%2520address%2520this%2520issue.%2520This%2520additional%2520lightweight%250Aneural%2520network%2520is%2520trained%2520alongside%2520the%2520landmark%2520detector%252C%2520without%2520requiring%250Aany%2520additional%2520supervision.%2520While%2520the%2520insights%2520of%2520this%2520paper%2520can%2520be%2520applied%2520to%250Amost%2520common%2520landmark%2520detectors%252C%2520we%2520specifically%2520target%2520a%2520recently-proposed%250Acontinuous%25202D%2520landmark%2520detector%2520to%2520demonstrate%2520how%2520each%2520of%2520our%2520additions%2520leads%250Ato%2520meaningful%2520improvements%2520over%2520the%2520state-of-the-art%2520on%2520standard%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinite%203D%20Landmarks%3A%20Improving%20Continuous%202D%20Facial%20Landmark%20Detection&entry.906535625=Prashanth%20Chandran%20and%20Gaspard%20Zoss%20and%20Paulo%20Gotardo%20and%20Derek%20Bradley&entry.1292438233=%20%20In%20this%20paper%2C%20we%20examine%203%20important%20issues%20in%20the%20practical%20use%20of%0Astate-of-the-art%20facial%20landmark%20detectors%20and%20show%20how%20a%20combination%20of%0Aspecific%20architectural%20modifications%20can%20directly%20improve%20their%20accuracy%20and%0Atemporal%20stability.%20First%2C%20many%20facial%20landmark%20detectors%20require%20face%0Anormalization%20as%20a%20preprocessing%20step%2C%20which%20is%20accomplished%20by%20a%0Aseparately-trained%20neural%20network%20that%20crops%20and%20resizes%20the%20face%20in%20the%20input%0Aimage.%20There%20is%20no%20guarantee%20that%20this%20pre-trained%20network%20performs%20the%20optimal%0Aface%20normalization%20for%20landmark%20detection.%20We%20instead%20analyze%20the%20use%20of%20a%0Aspatial%20transformer%20network%20that%20is%20trained%20alongside%20the%20landmark%20detector%20in%0Aan%20unsupervised%20manner%2C%20and%20jointly%20learn%20optimal%20face%20normalization%20and%0Alandmark%20detection.%20Second%2C%20we%20show%20that%20modifying%20the%20output%20head%20of%20the%0Alandmark%20predictor%20to%20infer%20landmarks%20in%20a%20canonical%203D%20space%20can%20further%0Aimprove%20accuracy.%20To%20convert%20the%20predicted%203D%20landmarks%20into%20screen-space%2C%20we%0Aadditionally%20predict%20the%20camera%20intrinsics%20and%20head%20pose%20from%20the%20input%20image.%0AAs%20a%20side%20benefit%2C%20this%20allows%20to%20predict%20the%203D%20face%20shape%20from%20a%20given%20image%0Aonly%20using%202D%20landmarks%20as%20supervision%2C%20which%20is%20useful%20in%20determining%20landmark%0Avisibility%20among%20other%20things.%20Finally%2C%20when%20training%20a%20landmark%20detector%20on%0Amultiple%20datasets%20at%20the%20same%20time%2C%20annotation%20inconsistencies%20across%20datasets%0Aforces%20the%20network%20to%20produce%20a%20suboptimal%20average.%20We%20propose%20to%20add%20a%0Asemantic%20correction%20network%20to%20address%20this%20issue.%20This%20additional%20lightweight%0Aneural%20network%20is%20trained%20alongside%20the%20landmark%20detector%2C%20without%20requiring%0Aany%20additional%20supervision.%20While%20the%20insights%20of%20this%20paper%20can%20be%20applied%20to%0Amost%20common%20landmark%20detectors%2C%20we%20specifically%20target%20a%20recently-proposed%0Acontinuous%202D%20landmark%20detector%20to%20demonstrate%20how%20each%20of%20our%20additions%20leads%0Ato%20meaningful%20improvements%20over%20the%20state-of-the-art%20on%20standard%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20117v1&entry.124074799=Read"},
{"title": "SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder\n  and Transformer Network", "author": "Hamza Bouzid and Lahoucine Ballihi", "abstract": "  Recent technological advancements have significantly expanded the potential\nof human action recognition through harnessing the power of 3D data. This data\nprovides a richer understanding of actions, including depth information that\nenables more accurate analysis of spatial and temporal characteristics. In this\ncontext, We study the challenge of 3D human action recognition.Unlike prior\nmethods, that rely on sampling 2D depth images, skeleton points, or point\nclouds, often leading to substantial memory requirements and the ability to\nhandle only short sequences, we introduce a novel approach for 3D human action\nrecognition, denoted as SpATr (Spiral Auto-encoder and Transformer Network),\nspecifically designed for fixed-topology mesh sequences. The SpATr model\ndisentangles space and time in the mesh sequences. A lightweight auto-encoder,\nbased on spiral convolutions, is employed to extract spatial geometrical\nfeatures from each 3D mesh. These convolutions are lightweight and specifically\ndesigned for fix-topology mesh data. Subsequently, a temporal transformer,\nbased on self-attention, captures the temporal context within the feature\nsequence. The self-attention mechanism enables long-range dependencies\ncapturing and parallel processing, ensuring scalability for long sequences. The\nproposed method is evaluated on three prominent 3D human action datasets:\nBabel, MoVi, and BMLrub, from the Archive of Motion Capture As Surface Shapes\n(AMASS). Our results analysis demonstrates the competitive performance of our\nSpATr model in 3D human action recognition while maintaining efficient memory\nusage. The code and the training results will soon be made publicly available\nat https://github.com/h-bouzid/spatr.\n", "link": "http://arxiv.org/abs/2306.17574v2", "date": "2024-05-30", "relevancy": 2.8324, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6155}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5486}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpATr%3A%20MoCap%203D%20Human%20Action%20Recognition%20based%20on%20Spiral%20Auto-encoder%0A%20%20and%20Transformer%20Network&body=Title%3A%20SpATr%3A%20MoCap%203D%20Human%20Action%20Recognition%20based%20on%20Spiral%20Auto-encoder%0A%20%20and%20Transformer%20Network%0AAuthor%3A%20Hamza%20Bouzid%20and%20Lahoucine%20Ballihi%0AAbstract%3A%20%20%20Recent%20technological%20advancements%20have%20significantly%20expanded%20the%20potential%0Aof%20human%20action%20recognition%20through%20harnessing%20the%20power%20of%203D%20data.%20This%20data%0Aprovides%20a%20richer%20understanding%20of%20actions%2C%20including%20depth%20information%20that%0Aenables%20more%20accurate%20analysis%20of%20spatial%20and%20temporal%20characteristics.%20In%20this%0Acontext%2C%20We%20study%20the%20challenge%20of%203D%20human%20action%20recognition.Unlike%20prior%0Amethods%2C%20that%20rely%20on%20sampling%202D%20depth%20images%2C%20skeleton%20points%2C%20or%20point%0Aclouds%2C%20often%20leading%20to%20substantial%20memory%20requirements%20and%20the%20ability%20to%0Ahandle%20only%20short%20sequences%2C%20we%20introduce%20a%20novel%20approach%20for%203D%20human%20action%0Arecognition%2C%20denoted%20as%20SpATr%20%28Spiral%20Auto-encoder%20and%20Transformer%20Network%29%2C%0Aspecifically%20designed%20for%20fixed-topology%20mesh%20sequences.%20The%20SpATr%20model%0Adisentangles%20space%20and%20time%20in%20the%20mesh%20sequences.%20A%20lightweight%20auto-encoder%2C%0Abased%20on%20spiral%20convolutions%2C%20is%20employed%20to%20extract%20spatial%20geometrical%0Afeatures%20from%20each%203D%20mesh.%20These%20convolutions%20are%20lightweight%20and%20specifically%0Adesigned%20for%20fix-topology%20mesh%20data.%20Subsequently%2C%20a%20temporal%20transformer%2C%0Abased%20on%20self-attention%2C%20captures%20the%20temporal%20context%20within%20the%20feature%0Asequence.%20The%20self-attention%20mechanism%20enables%20long-range%20dependencies%0Acapturing%20and%20parallel%20processing%2C%20ensuring%20scalability%20for%20long%20sequences.%20The%0Aproposed%20method%20is%20evaluated%20on%20three%20prominent%203D%20human%20action%20datasets%3A%0ABabel%2C%20MoVi%2C%20and%20BMLrub%2C%20from%20the%20Archive%20of%20Motion%20Capture%20As%20Surface%20Shapes%0A%28AMASS%29.%20Our%20results%20analysis%20demonstrates%20the%20competitive%20performance%20of%20our%0ASpATr%20model%20in%203D%20human%20action%20recognition%20while%20maintaining%20efficient%20memory%0Ausage.%20The%20code%20and%20the%20training%20results%20will%20soon%20be%20made%20publicly%20available%0Aat%20https%3A//github.com/h-bouzid/spatr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.17574v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpATr%253A%2520MoCap%25203D%2520Human%2520Action%2520Recognition%2520based%2520on%2520Spiral%2520Auto-encoder%250A%2520%2520and%2520Transformer%2520Network%26entry.906535625%3DHamza%2520Bouzid%2520and%2520Lahoucine%2520Ballihi%26entry.1292438233%3D%2520%2520Recent%2520technological%2520advancements%2520have%2520significantly%2520expanded%2520the%2520potential%250Aof%2520human%2520action%2520recognition%2520through%2520harnessing%2520the%2520power%2520of%25203D%2520data.%2520This%2520data%250Aprovides%2520a%2520richer%2520understanding%2520of%2520actions%252C%2520including%2520depth%2520information%2520that%250Aenables%2520more%2520accurate%2520analysis%2520of%2520spatial%2520and%2520temporal%2520characteristics.%2520In%2520this%250Acontext%252C%2520We%2520study%2520the%2520challenge%2520of%25203D%2520human%2520action%2520recognition.Unlike%2520prior%250Amethods%252C%2520that%2520rely%2520on%2520sampling%25202D%2520depth%2520images%252C%2520skeleton%2520points%252C%2520or%2520point%250Aclouds%252C%2520often%2520leading%2520to%2520substantial%2520memory%2520requirements%2520and%2520the%2520ability%2520to%250Ahandle%2520only%2520short%2520sequences%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520for%25203D%2520human%2520action%250Arecognition%252C%2520denoted%2520as%2520SpATr%2520%2528Spiral%2520Auto-encoder%2520and%2520Transformer%2520Network%2529%252C%250Aspecifically%2520designed%2520for%2520fixed-topology%2520mesh%2520sequences.%2520The%2520SpATr%2520model%250Adisentangles%2520space%2520and%2520time%2520in%2520the%2520mesh%2520sequences.%2520A%2520lightweight%2520auto-encoder%252C%250Abased%2520on%2520spiral%2520convolutions%252C%2520is%2520employed%2520to%2520extract%2520spatial%2520geometrical%250Afeatures%2520from%2520each%25203D%2520mesh.%2520These%2520convolutions%2520are%2520lightweight%2520and%2520specifically%250Adesigned%2520for%2520fix-topology%2520mesh%2520data.%2520Subsequently%252C%2520a%2520temporal%2520transformer%252C%250Abased%2520on%2520self-attention%252C%2520captures%2520the%2520temporal%2520context%2520within%2520the%2520feature%250Asequence.%2520The%2520self-attention%2520mechanism%2520enables%2520long-range%2520dependencies%250Acapturing%2520and%2520parallel%2520processing%252C%2520ensuring%2520scalability%2520for%2520long%2520sequences.%2520The%250Aproposed%2520method%2520is%2520evaluated%2520on%2520three%2520prominent%25203D%2520human%2520action%2520datasets%253A%250ABabel%252C%2520MoVi%252C%2520and%2520BMLrub%252C%2520from%2520the%2520Archive%2520of%2520Motion%2520Capture%2520As%2520Surface%2520Shapes%250A%2528AMASS%2529.%2520Our%2520results%2520analysis%2520demonstrates%2520the%2520competitive%2520performance%2520of%2520our%250ASpATr%2520model%2520in%25203D%2520human%2520action%2520recognition%2520while%2520maintaining%2520efficient%2520memory%250Ausage.%2520The%2520code%2520and%2520the%2520training%2520results%2520will%2520soon%2520be%2520made%2520publicly%2520available%250Aat%2520https%253A//github.com/h-bouzid/spatr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.17574v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpATr%3A%20MoCap%203D%20Human%20Action%20Recognition%20based%20on%20Spiral%20Auto-encoder%0A%20%20and%20Transformer%20Network&entry.906535625=Hamza%20Bouzid%20and%20Lahoucine%20Ballihi&entry.1292438233=%20%20Recent%20technological%20advancements%20have%20significantly%20expanded%20the%20potential%0Aof%20human%20action%20recognition%20through%20harnessing%20the%20power%20of%203D%20data.%20This%20data%0Aprovides%20a%20richer%20understanding%20of%20actions%2C%20including%20depth%20information%20that%0Aenables%20more%20accurate%20analysis%20of%20spatial%20and%20temporal%20characteristics.%20In%20this%0Acontext%2C%20We%20study%20the%20challenge%20of%203D%20human%20action%20recognition.Unlike%20prior%0Amethods%2C%20that%20rely%20on%20sampling%202D%20depth%20images%2C%20skeleton%20points%2C%20or%20point%0Aclouds%2C%20often%20leading%20to%20substantial%20memory%20requirements%20and%20the%20ability%20to%0Ahandle%20only%20short%20sequences%2C%20we%20introduce%20a%20novel%20approach%20for%203D%20human%20action%0Arecognition%2C%20denoted%20as%20SpATr%20%28Spiral%20Auto-encoder%20and%20Transformer%20Network%29%2C%0Aspecifically%20designed%20for%20fixed-topology%20mesh%20sequences.%20The%20SpATr%20model%0Adisentangles%20space%20and%20time%20in%20the%20mesh%20sequences.%20A%20lightweight%20auto-encoder%2C%0Abased%20on%20spiral%20convolutions%2C%20is%20employed%20to%20extract%20spatial%20geometrical%0Afeatures%20from%20each%203D%20mesh.%20These%20convolutions%20are%20lightweight%20and%20specifically%0Adesigned%20for%20fix-topology%20mesh%20data.%20Subsequently%2C%20a%20temporal%20transformer%2C%0Abased%20on%20self-attention%2C%20captures%20the%20temporal%20context%20within%20the%20feature%0Asequence.%20The%20self-attention%20mechanism%20enables%20long-range%20dependencies%0Acapturing%20and%20parallel%20processing%2C%20ensuring%20scalability%20for%20long%20sequences.%20The%0Aproposed%20method%20is%20evaluated%20on%20three%20prominent%203D%20human%20action%20datasets%3A%0ABabel%2C%20MoVi%2C%20and%20BMLrub%2C%20from%20the%20Archive%20of%20Motion%20Capture%20As%20Surface%20Shapes%0A%28AMASS%29.%20Our%20results%20analysis%20demonstrates%20the%20competitive%20performance%20of%20our%0ASpATr%20model%20in%203D%20human%20action%20recognition%20while%20maintaining%20efficient%20memory%0Ausage.%20The%20code%20and%20the%20training%20results%20will%20soon%20be%20made%20publicly%20available%0Aat%20https%3A//github.com/h-bouzid/spatr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.17574v2&entry.124074799=Read"},
{"title": "A Point-Neighborhood Learning Framework for Nasal Endoscope Image\n  Segmentation", "author": "Pengyu Jie and Wanquan Liu and Chenqiang Gao and Yihui Wen and Rui He and Pengcheng Li and Jintao Zhang and Deyu Meng", "abstract": "  The lesion segmentation on endoscopic images is challenging due to its\ncomplex and ambiguous features. Fully-supervised deep learning segmentation\nmethods can receive good performance based on entirely pixel-level labeled\ndataset but greatly increase experts' labeling burden. Semi-supervised and\nweakly supervised methods can ease labeling burden, but heavily strengthen the\nlearning difficulty. To alleviate this difficulty, weakly semi-supervised\nsegmentation adopts a new annotation protocol of adding a large number of point\nannotation samples into a few pixel-level annotation samples. However, existing\nmethods only mine points' limited information while ignoring reliable prior\nsurrounding the point annotations. In this paper, we propose a weakly\nsemi-supervised method called Point-Neighborhood Learning (PNL) framework. To\nmine the prior of the pixels surrounding the annotated point, we transform a\nsingle-point annotation into a circular area named a point-neighborhood. We\npropose point-neighborhood supervision loss and pseudo-label scoring mechanism\nto enhance training supervision. Point-neighborhoods are also used to augment\nthe data diversity. Our method greatly improves performance without changing\nthe structure of segmentation network. Comprehensive experiments show the\nsuperiority of our method over the other existing methods, demonstrating its\neffectiveness in point-annotated medical images. The project code will be\navailable on: https://github.com/ParryJay/PNL.\n", "link": "http://arxiv.org/abs/2405.20044v1", "date": "2024-05-30", "relevancy": 2.8185, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5997}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5535}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Point-Neighborhood%20Learning%20Framework%20for%20Nasal%20Endoscope%20Image%0A%20%20Segmentation&body=Title%3A%20A%20Point-Neighborhood%20Learning%20Framework%20for%20Nasal%20Endoscope%20Image%0A%20%20Segmentation%0AAuthor%3A%20Pengyu%20Jie%20and%20Wanquan%20Liu%20and%20Chenqiang%20Gao%20and%20Yihui%20Wen%20and%20Rui%20He%20and%20Pengcheng%20Li%20and%20Jintao%20Zhang%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20The%20lesion%20segmentation%20on%20endoscopic%20images%20is%20challenging%20due%20to%20its%0Acomplex%20and%20ambiguous%20features.%20Fully-supervised%20deep%20learning%20segmentation%0Amethods%20can%20receive%20good%20performance%20based%20on%20entirely%20pixel-level%20labeled%0Adataset%20but%20greatly%20increase%20experts%27%20labeling%20burden.%20Semi-supervised%20and%0Aweakly%20supervised%20methods%20can%20ease%20labeling%20burden%2C%20but%20heavily%20strengthen%20the%0Alearning%20difficulty.%20To%20alleviate%20this%20difficulty%2C%20weakly%20semi-supervised%0Asegmentation%20adopts%20a%20new%20annotation%20protocol%20of%20adding%20a%20large%20number%20of%20point%0Aannotation%20samples%20into%20a%20few%20pixel-level%20annotation%20samples.%20However%2C%20existing%0Amethods%20only%20mine%20points%27%20limited%20information%20while%20ignoring%20reliable%20prior%0Asurrounding%20the%20point%20annotations.%20In%20this%20paper%2C%20we%20propose%20a%20weakly%0Asemi-supervised%20method%20called%20Point-Neighborhood%20Learning%20%28PNL%29%20framework.%20To%0Amine%20the%20prior%20of%20the%20pixels%20surrounding%20the%20annotated%20point%2C%20we%20transform%20a%0Asingle-point%20annotation%20into%20a%20circular%20area%20named%20a%20point-neighborhood.%20We%0Apropose%20point-neighborhood%20supervision%20loss%20and%20pseudo-label%20scoring%20mechanism%0Ato%20enhance%20training%20supervision.%20Point-neighborhoods%20are%20also%20used%20to%20augment%0Athe%20data%20diversity.%20Our%20method%20greatly%20improves%20performance%20without%20changing%0Athe%20structure%20of%20segmentation%20network.%20Comprehensive%20experiments%20show%20the%0Asuperiority%20of%20our%20method%20over%20the%20other%20existing%20methods%2C%20demonstrating%20its%0Aeffectiveness%20in%20point-annotated%20medical%20images.%20The%20project%20code%20will%20be%0Aavailable%20on%3A%20https%3A//github.com/ParryJay/PNL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Point-Neighborhood%2520Learning%2520Framework%2520for%2520Nasal%2520Endoscope%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DPengyu%2520Jie%2520and%2520Wanquan%2520Liu%2520and%2520Chenqiang%2520Gao%2520and%2520Yihui%2520Wen%2520and%2520Rui%2520He%2520and%2520Pengcheng%2520Li%2520and%2520Jintao%2520Zhang%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520The%2520lesion%2520segmentation%2520on%2520endoscopic%2520images%2520is%2520challenging%2520due%2520to%2520its%250Acomplex%2520and%2520ambiguous%2520features.%2520Fully-supervised%2520deep%2520learning%2520segmentation%250Amethods%2520can%2520receive%2520good%2520performance%2520based%2520on%2520entirely%2520pixel-level%2520labeled%250Adataset%2520but%2520greatly%2520increase%2520experts%2527%2520labeling%2520burden.%2520Semi-supervised%2520and%250Aweakly%2520supervised%2520methods%2520can%2520ease%2520labeling%2520burden%252C%2520but%2520heavily%2520strengthen%2520the%250Alearning%2520difficulty.%2520To%2520alleviate%2520this%2520difficulty%252C%2520weakly%2520semi-supervised%250Asegmentation%2520adopts%2520a%2520new%2520annotation%2520protocol%2520of%2520adding%2520a%2520large%2520number%2520of%2520point%250Aannotation%2520samples%2520into%2520a%2520few%2520pixel-level%2520annotation%2520samples.%2520However%252C%2520existing%250Amethods%2520only%2520mine%2520points%2527%2520limited%2520information%2520while%2520ignoring%2520reliable%2520prior%250Asurrounding%2520the%2520point%2520annotations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520weakly%250Asemi-supervised%2520method%2520called%2520Point-Neighborhood%2520Learning%2520%2528PNL%2529%2520framework.%2520To%250Amine%2520the%2520prior%2520of%2520the%2520pixels%2520surrounding%2520the%2520annotated%2520point%252C%2520we%2520transform%2520a%250Asingle-point%2520annotation%2520into%2520a%2520circular%2520area%2520named%2520a%2520point-neighborhood.%2520We%250Apropose%2520point-neighborhood%2520supervision%2520loss%2520and%2520pseudo-label%2520scoring%2520mechanism%250Ato%2520enhance%2520training%2520supervision.%2520Point-neighborhoods%2520are%2520also%2520used%2520to%2520augment%250Athe%2520data%2520diversity.%2520Our%2520method%2520greatly%2520improves%2520performance%2520without%2520changing%250Athe%2520structure%2520of%2520segmentation%2520network.%2520Comprehensive%2520experiments%2520show%2520the%250Asuperiority%2520of%2520our%2520method%2520over%2520the%2520other%2520existing%2520methods%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520point-annotated%2520medical%2520images.%2520The%2520project%2520code%2520will%2520be%250Aavailable%2520on%253A%2520https%253A//github.com/ParryJay/PNL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Point-Neighborhood%20Learning%20Framework%20for%20Nasal%20Endoscope%20Image%0A%20%20Segmentation&entry.906535625=Pengyu%20Jie%20and%20Wanquan%20Liu%20and%20Chenqiang%20Gao%20and%20Yihui%20Wen%20and%20Rui%20He%20and%20Pengcheng%20Li%20and%20Jintao%20Zhang%20and%20Deyu%20Meng&entry.1292438233=%20%20The%20lesion%20segmentation%20on%20endoscopic%20images%20is%20challenging%20due%20to%20its%0Acomplex%20and%20ambiguous%20features.%20Fully-supervised%20deep%20learning%20segmentation%0Amethods%20can%20receive%20good%20performance%20based%20on%20entirely%20pixel-level%20labeled%0Adataset%20but%20greatly%20increase%20experts%27%20labeling%20burden.%20Semi-supervised%20and%0Aweakly%20supervised%20methods%20can%20ease%20labeling%20burden%2C%20but%20heavily%20strengthen%20the%0Alearning%20difficulty.%20To%20alleviate%20this%20difficulty%2C%20weakly%20semi-supervised%0Asegmentation%20adopts%20a%20new%20annotation%20protocol%20of%20adding%20a%20large%20number%20of%20point%0Aannotation%20samples%20into%20a%20few%20pixel-level%20annotation%20samples.%20However%2C%20existing%0Amethods%20only%20mine%20points%27%20limited%20information%20while%20ignoring%20reliable%20prior%0Asurrounding%20the%20point%20annotations.%20In%20this%20paper%2C%20we%20propose%20a%20weakly%0Asemi-supervised%20method%20called%20Point-Neighborhood%20Learning%20%28PNL%29%20framework.%20To%0Amine%20the%20prior%20of%20the%20pixels%20surrounding%20the%20annotated%20point%2C%20we%20transform%20a%0Asingle-point%20annotation%20into%20a%20circular%20area%20named%20a%20point-neighborhood.%20We%0Apropose%20point-neighborhood%20supervision%20loss%20and%20pseudo-label%20scoring%20mechanism%0Ato%20enhance%20training%20supervision.%20Point-neighborhoods%20are%20also%20used%20to%20augment%0Athe%20data%20diversity.%20Our%20method%20greatly%20improves%20performance%20without%20changing%0Athe%20structure%20of%20segmentation%20network.%20Comprehensive%20experiments%20show%20the%0Asuperiority%20of%20our%20method%20over%20the%20other%20existing%20methods%2C%20demonstrating%20its%0Aeffectiveness%20in%20point-annotated%20medical%20images.%20The%20project%20code%20will%20be%0Aavailable%20on%3A%20https%3A//github.com/ParryJay/PNL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20044v1&entry.124074799=Read"},
{"title": "4DHands: Reconstructing Interactive Hands in 4D with Transformers", "author": "Dixuan Lin and Yuxiang Zhang and Mengcheng Li and Yebin Liu and Wei Jing and Qi Yan and Qianying Wang and Hongwen Zhang", "abstract": "  In this paper, we introduce 4DHands, a robust approach to recovering\ninteractive hand meshes and their relative movement from monocular inputs. Our\napproach addresses two major limitations of previous methods: lacking a unified\nsolution for handling various hand image inputs and neglecting the positional\nrelationship of two hands within images. To overcome these challenges, we\ndevelop a transformer-based architecture with novel tokenization and feature\nfusion strategies. Specifically, we propose a Relation-aware Two-Hand\nTokenization (RAT) method to embed positional relation information into the\nhand tokens. In this way, our network can handle both single-hand and two-hand\ninputs and explicitly leverage relative hand positions, facilitating the\nreconstruction of intricate hand interactions in real-world scenarios. As such\ntokenization indicates the relative relationship of two hands, it also supports\nmore effective feature fusion. To this end, we further develop a\nSpatio-temporal Interaction Reasoning (SIR) module to fuse hand tokens in 4D\nwith attention and decode them into 3D hand meshes and relative temporal\nmovements. The efficacy of our approach is validated on several benchmark\ndatasets. The results on in-the-wild videos and real-world scenarios\ndemonstrate the superior performances of our approach for interactive hand\nreconstruction. More video results can be found on the project page:\nhttps://4dhands.github.io.\n", "link": "http://arxiv.org/abs/2405.20330v1", "date": "2024-05-30", "relevancy": 2.7993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5588}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DHands%3A%20Reconstructing%20Interactive%20Hands%20in%204D%20with%20Transformers&body=Title%3A%204DHands%3A%20Reconstructing%20Interactive%20Hands%20in%204D%20with%20Transformers%0AAuthor%3A%20Dixuan%20Lin%20and%20Yuxiang%20Zhang%20and%20Mengcheng%20Li%20and%20Yebin%20Liu%20and%20Wei%20Jing%20and%20Qi%20Yan%20and%20Qianying%20Wang%20and%20Hongwen%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%204DHands%2C%20a%20robust%20approach%20to%20recovering%0Ainteractive%20hand%20meshes%20and%20their%20relative%20movement%20from%20monocular%20inputs.%20Our%0Aapproach%20addresses%20two%20major%20limitations%20of%20previous%20methods%3A%20lacking%20a%20unified%0Asolution%20for%20handling%20various%20hand%20image%20inputs%20and%20neglecting%20the%20positional%0Arelationship%20of%20two%20hands%20within%20images.%20To%20overcome%20these%20challenges%2C%20we%0Adevelop%20a%20transformer-based%20architecture%20with%20novel%20tokenization%20and%20feature%0Afusion%20strategies.%20Specifically%2C%20we%20propose%20a%20Relation-aware%20Two-Hand%0ATokenization%20%28RAT%29%20method%20to%20embed%20positional%20relation%20information%20into%20the%0Ahand%20tokens.%20In%20this%20way%2C%20our%20network%20can%20handle%20both%20single-hand%20and%20two-hand%0Ainputs%20and%20explicitly%20leverage%20relative%20hand%20positions%2C%20facilitating%20the%0Areconstruction%20of%20intricate%20hand%20interactions%20in%20real-world%20scenarios.%20As%20such%0Atokenization%20indicates%20the%20relative%20relationship%20of%20two%20hands%2C%20it%20also%20supports%0Amore%20effective%20feature%20fusion.%20To%20this%20end%2C%20we%20further%20develop%20a%0ASpatio-temporal%20Interaction%20Reasoning%20%28SIR%29%20module%20to%20fuse%20hand%20tokens%20in%204D%0Awith%20attention%20and%20decode%20them%20into%203D%20hand%20meshes%20and%20relative%20temporal%0Amovements.%20The%20efficacy%20of%20our%20approach%20is%20validated%20on%20several%20benchmark%0Adatasets.%20The%20results%20on%20in-the-wild%20videos%20and%20real-world%20scenarios%0Ademonstrate%20the%20superior%20performances%20of%20our%20approach%20for%20interactive%20hand%0Areconstruction.%20More%20video%20results%20can%20be%20found%20on%20the%20project%20page%3A%0Ahttps%3A//4dhands.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DHands%253A%2520Reconstructing%2520Interactive%2520Hands%2520in%25204D%2520with%2520Transformers%26entry.906535625%3DDixuan%2520Lin%2520and%2520Yuxiang%2520Zhang%2520and%2520Mengcheng%2520Li%2520and%2520Yebin%2520Liu%2520and%2520Wei%2520Jing%2520and%2520Qi%2520Yan%2520and%2520Qianying%2520Wang%2520and%2520Hongwen%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%25204DHands%252C%2520a%2520robust%2520approach%2520to%2520recovering%250Ainteractive%2520hand%2520meshes%2520and%2520their%2520relative%2520movement%2520from%2520monocular%2520inputs.%2520Our%250Aapproach%2520addresses%2520two%2520major%2520limitations%2520of%2520previous%2520methods%253A%2520lacking%2520a%2520unified%250Asolution%2520for%2520handling%2520various%2520hand%2520image%2520inputs%2520and%2520neglecting%2520the%2520positional%250Arelationship%2520of%2520two%2520hands%2520within%2520images.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Adevelop%2520a%2520transformer-based%2520architecture%2520with%2520novel%2520tokenization%2520and%2520feature%250Afusion%2520strategies.%2520Specifically%252C%2520we%2520propose%2520a%2520Relation-aware%2520Two-Hand%250ATokenization%2520%2528RAT%2529%2520method%2520to%2520embed%2520positional%2520relation%2520information%2520into%2520the%250Ahand%2520tokens.%2520In%2520this%2520way%252C%2520our%2520network%2520can%2520handle%2520both%2520single-hand%2520and%2520two-hand%250Ainputs%2520and%2520explicitly%2520leverage%2520relative%2520hand%2520positions%252C%2520facilitating%2520the%250Areconstruction%2520of%2520intricate%2520hand%2520interactions%2520in%2520real-world%2520scenarios.%2520As%2520such%250Atokenization%2520indicates%2520the%2520relative%2520relationship%2520of%2520two%2520hands%252C%2520it%2520also%2520supports%250Amore%2520effective%2520feature%2520fusion.%2520To%2520this%2520end%252C%2520we%2520further%2520develop%2520a%250ASpatio-temporal%2520Interaction%2520Reasoning%2520%2528SIR%2529%2520module%2520to%2520fuse%2520hand%2520tokens%2520in%25204D%250Awith%2520attention%2520and%2520decode%2520them%2520into%25203D%2520hand%2520meshes%2520and%2520relative%2520temporal%250Amovements.%2520The%2520efficacy%2520of%2520our%2520approach%2520is%2520validated%2520on%2520several%2520benchmark%250Adatasets.%2520The%2520results%2520on%2520in-the-wild%2520videos%2520and%2520real-world%2520scenarios%250Ademonstrate%2520the%2520superior%2520performances%2520of%2520our%2520approach%2520for%2520interactive%2520hand%250Areconstruction.%2520More%2520video%2520results%2520can%2520be%2520found%2520on%2520the%2520project%2520page%253A%250Ahttps%253A//4dhands.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DHands%3A%20Reconstructing%20Interactive%20Hands%20in%204D%20with%20Transformers&entry.906535625=Dixuan%20Lin%20and%20Yuxiang%20Zhang%20and%20Mengcheng%20Li%20and%20Yebin%20Liu%20and%20Wei%20Jing%20and%20Qi%20Yan%20and%20Qianying%20Wang%20and%20Hongwen%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%204DHands%2C%20a%20robust%20approach%20to%20recovering%0Ainteractive%20hand%20meshes%20and%20their%20relative%20movement%20from%20monocular%20inputs.%20Our%0Aapproach%20addresses%20two%20major%20limitations%20of%20previous%20methods%3A%20lacking%20a%20unified%0Asolution%20for%20handling%20various%20hand%20image%20inputs%20and%20neglecting%20the%20positional%0Arelationship%20of%20two%20hands%20within%20images.%20To%20overcome%20these%20challenges%2C%20we%0Adevelop%20a%20transformer-based%20architecture%20with%20novel%20tokenization%20and%20feature%0Afusion%20strategies.%20Specifically%2C%20we%20propose%20a%20Relation-aware%20Two-Hand%0ATokenization%20%28RAT%29%20method%20to%20embed%20positional%20relation%20information%20into%20the%0Ahand%20tokens.%20In%20this%20way%2C%20our%20network%20can%20handle%20both%20single-hand%20and%20two-hand%0Ainputs%20and%20explicitly%20leverage%20relative%20hand%20positions%2C%20facilitating%20the%0Areconstruction%20of%20intricate%20hand%20interactions%20in%20real-world%20scenarios.%20As%20such%0Atokenization%20indicates%20the%20relative%20relationship%20of%20two%20hands%2C%20it%20also%20supports%0Amore%20effective%20feature%20fusion.%20To%20this%20end%2C%20we%20further%20develop%20a%0ASpatio-temporal%20Interaction%20Reasoning%20%28SIR%29%20module%20to%20fuse%20hand%20tokens%20in%204D%0Awith%20attention%20and%20decode%20them%20into%203D%20hand%20meshes%20and%20relative%20temporal%0Amovements.%20The%20efficacy%20of%20our%20approach%20is%20validated%20on%20several%20benchmark%0Adatasets.%20The%20results%20on%20in-the-wild%20videos%20and%20real-world%20scenarios%0Ademonstrate%20the%20superior%20performances%20of%20our%20approach%20for%20interactive%20hand%0Areconstruction.%20More%20video%20results%20can%20be%20found%20on%20the%20project%20page%3A%0Ahttps%3A//4dhands.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20330v1&entry.124074799=Read"},
{"title": "CL-MRI: Self-Supervised Contrastive Learning to Improve the Accuracy of\n  Undersampled MRI Reconstruction", "author": "Mevan Ekanayake and Zhifeng Chen and Mehrtash Harandi and Gary Egan and Zhaolin Chen", "abstract": "  In Magnetic Resonance Imaging (MRI), image acquisitions are often\nundersampled in the measurement domain to accelerate the scanning process, at\nthe expense of image quality. However, image quality is a crucial factor that\ninfluences the accuracy of clinical diagnosis; hence, high-quality image\nreconstruction from undersampled measurements has been a key area of research.\nRecently, deep learning (DL) methods have emerged as the state-of-the-art for\nMRI reconstruction, typically involving deep neural networks to transform\nundersampled MRI images into high-quality MRI images through data-driven\nprocesses. Nevertheless, there is clear and significant room for improvement in\nundersampled DL MRI reconstruction to meet the high standards required for\nclinical diagnosis, in terms of eliminating aliasing artifacts and reducing\nimage noise. In this paper, we introduce a self-supervised pretraining\nprocedure using contrastive learning to improve the accuracy of undersampled DL\nMRI reconstruction. We use contrastive learning to transform the MRI image\nrepresentations into a latent space that maximizes mutual information among\ndifferent undersampled representations and optimizes the information content at\nthe input of the downstream DL reconstruction models. Our experiments\ndemonstrate improved reconstruction accuracy across a range of acceleration\nfactors and datasets, both quantitatively and qualitatively. Furthermore, our\nextended experiments validate the proposed framework's robustness under\nadversarial conditions, such as measurement noise, different k-space sampling\npatterns, and pathological abnormalities, and also prove the transfer learning\ncapabilities on MRI datasets with completely different anatomy. Additionally,\nwe conducted experiments to visualize and analyze the properties of the\nproposed MRI contrastive learning latent space.\n", "link": "http://arxiv.org/abs/2306.00530v3", "date": "2024-05-30", "relevancy": 2.7422, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5676}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CL-MRI%3A%20Self-Supervised%20Contrastive%20Learning%20to%20Improve%20the%20Accuracy%20of%0A%20%20Undersampled%20MRI%20Reconstruction&body=Title%3A%20CL-MRI%3A%20Self-Supervised%20Contrastive%20Learning%20to%20Improve%20the%20Accuracy%20of%0A%20%20Undersampled%20MRI%20Reconstruction%0AAuthor%3A%20Mevan%20Ekanayake%20and%20Zhifeng%20Chen%20and%20Mehrtash%20Harandi%20and%20Gary%20Egan%20and%20Zhaolin%20Chen%0AAbstract%3A%20%20%20In%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%20image%20acquisitions%20are%20often%0Aundersampled%20in%20the%20measurement%20domain%20to%20accelerate%20the%20scanning%20process%2C%20at%0Athe%20expense%20of%20image%20quality.%20However%2C%20image%20quality%20is%20a%20crucial%20factor%20that%0Ainfluences%20the%20accuracy%20of%20clinical%20diagnosis%3B%20hence%2C%20high-quality%20image%0Areconstruction%20from%20undersampled%20measurements%20has%20been%20a%20key%20area%20of%20research.%0ARecently%2C%20deep%20learning%20%28DL%29%20methods%20have%20emerged%20as%20the%20state-of-the-art%20for%0AMRI%20reconstruction%2C%20typically%20involving%20deep%20neural%20networks%20to%20transform%0Aundersampled%20MRI%20images%20into%20high-quality%20MRI%20images%20through%20data-driven%0Aprocesses.%20Nevertheless%2C%20there%20is%20clear%20and%20significant%20room%20for%20improvement%20in%0Aundersampled%20DL%20MRI%20reconstruction%20to%20meet%20the%20high%20standards%20required%20for%0Aclinical%20diagnosis%2C%20in%20terms%20of%20eliminating%20aliasing%20artifacts%20and%20reducing%0Aimage%20noise.%20In%20this%20paper%2C%20we%20introduce%20a%20self-supervised%20pretraining%0Aprocedure%20using%20contrastive%20learning%20to%20improve%20the%20accuracy%20of%20undersampled%20DL%0AMRI%20reconstruction.%20We%20use%20contrastive%20learning%20to%20transform%20the%20MRI%20image%0Arepresentations%20into%20a%20latent%20space%20that%20maximizes%20mutual%20information%20among%0Adifferent%20undersampled%20representations%20and%20optimizes%20the%20information%20content%20at%0Athe%20input%20of%20the%20downstream%20DL%20reconstruction%20models.%20Our%20experiments%0Ademonstrate%20improved%20reconstruction%20accuracy%20across%20a%20range%20of%20acceleration%0Afactors%20and%20datasets%2C%20both%20quantitatively%20and%20qualitatively.%20Furthermore%2C%20our%0Aextended%20experiments%20validate%20the%20proposed%20framework%27s%20robustness%20under%0Aadversarial%20conditions%2C%20such%20as%20measurement%20noise%2C%20different%20k-space%20sampling%0Apatterns%2C%20and%20pathological%20abnormalities%2C%20and%20also%20prove%20the%20transfer%20learning%0Acapabilities%20on%20MRI%20datasets%20with%20completely%20different%20anatomy.%20Additionally%2C%0Awe%20conducted%20experiments%20to%20visualize%20and%20analyze%20the%20properties%20of%20the%0Aproposed%20MRI%20contrastive%20learning%20latent%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00530v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCL-MRI%253A%2520Self-Supervised%2520Contrastive%2520Learning%2520to%2520Improve%2520the%2520Accuracy%2520of%250A%2520%2520Undersampled%2520MRI%2520Reconstruction%26entry.906535625%3DMevan%2520Ekanayake%2520and%2520Zhifeng%2520Chen%2520and%2520Mehrtash%2520Harandi%2520and%2520Gary%2520Egan%2520and%2520Zhaolin%2520Chen%26entry.1292438233%3D%2520%2520In%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%252C%2520image%2520acquisitions%2520are%2520often%250Aundersampled%2520in%2520the%2520measurement%2520domain%2520to%2520accelerate%2520the%2520scanning%2520process%252C%2520at%250Athe%2520expense%2520of%2520image%2520quality.%2520However%252C%2520image%2520quality%2520is%2520a%2520crucial%2520factor%2520that%250Ainfluences%2520the%2520accuracy%2520of%2520clinical%2520diagnosis%253B%2520hence%252C%2520high-quality%2520image%250Areconstruction%2520from%2520undersampled%2520measurements%2520has%2520been%2520a%2520key%2520area%2520of%2520research.%250ARecently%252C%2520deep%2520learning%2520%2528DL%2529%2520methods%2520have%2520emerged%2520as%2520the%2520state-of-the-art%2520for%250AMRI%2520reconstruction%252C%2520typically%2520involving%2520deep%2520neural%2520networks%2520to%2520transform%250Aundersampled%2520MRI%2520images%2520into%2520high-quality%2520MRI%2520images%2520through%2520data-driven%250Aprocesses.%2520Nevertheless%252C%2520there%2520is%2520clear%2520and%2520significant%2520room%2520for%2520improvement%2520in%250Aundersampled%2520DL%2520MRI%2520reconstruction%2520to%2520meet%2520the%2520high%2520standards%2520required%2520for%250Aclinical%2520diagnosis%252C%2520in%2520terms%2520of%2520eliminating%2520aliasing%2520artifacts%2520and%2520reducing%250Aimage%2520noise.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520self-supervised%2520pretraining%250Aprocedure%2520using%2520contrastive%2520learning%2520to%2520improve%2520the%2520accuracy%2520of%2520undersampled%2520DL%250AMRI%2520reconstruction.%2520We%2520use%2520contrastive%2520learning%2520to%2520transform%2520the%2520MRI%2520image%250Arepresentations%2520into%2520a%2520latent%2520space%2520that%2520maximizes%2520mutual%2520information%2520among%250Adifferent%2520undersampled%2520representations%2520and%2520optimizes%2520the%2520information%2520content%2520at%250Athe%2520input%2520of%2520the%2520downstream%2520DL%2520reconstruction%2520models.%2520Our%2520experiments%250Ademonstrate%2520improved%2520reconstruction%2520accuracy%2520across%2520a%2520range%2520of%2520acceleration%250Afactors%2520and%2520datasets%252C%2520both%2520quantitatively%2520and%2520qualitatively.%2520Furthermore%252C%2520our%250Aextended%2520experiments%2520validate%2520the%2520proposed%2520framework%2527s%2520robustness%2520under%250Aadversarial%2520conditions%252C%2520such%2520as%2520measurement%2520noise%252C%2520different%2520k-space%2520sampling%250Apatterns%252C%2520and%2520pathological%2520abnormalities%252C%2520and%2520also%2520prove%2520the%2520transfer%2520learning%250Acapabilities%2520on%2520MRI%2520datasets%2520with%2520completely%2520different%2520anatomy.%2520Additionally%252C%250Awe%2520conducted%2520experiments%2520to%2520visualize%2520and%2520analyze%2520the%2520properties%2520of%2520the%250Aproposed%2520MRI%2520contrastive%2520learning%2520latent%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00530v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CL-MRI%3A%20Self-Supervised%20Contrastive%20Learning%20to%20Improve%20the%20Accuracy%20of%0A%20%20Undersampled%20MRI%20Reconstruction&entry.906535625=Mevan%20Ekanayake%20and%20Zhifeng%20Chen%20and%20Mehrtash%20Harandi%20and%20Gary%20Egan%20and%20Zhaolin%20Chen&entry.1292438233=%20%20In%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%20image%20acquisitions%20are%20often%0Aundersampled%20in%20the%20measurement%20domain%20to%20accelerate%20the%20scanning%20process%2C%20at%0Athe%20expense%20of%20image%20quality.%20However%2C%20image%20quality%20is%20a%20crucial%20factor%20that%0Ainfluences%20the%20accuracy%20of%20clinical%20diagnosis%3B%20hence%2C%20high-quality%20image%0Areconstruction%20from%20undersampled%20measurements%20has%20been%20a%20key%20area%20of%20research.%0ARecently%2C%20deep%20learning%20%28DL%29%20methods%20have%20emerged%20as%20the%20state-of-the-art%20for%0AMRI%20reconstruction%2C%20typically%20involving%20deep%20neural%20networks%20to%20transform%0Aundersampled%20MRI%20images%20into%20high-quality%20MRI%20images%20through%20data-driven%0Aprocesses.%20Nevertheless%2C%20there%20is%20clear%20and%20significant%20room%20for%20improvement%20in%0Aundersampled%20DL%20MRI%20reconstruction%20to%20meet%20the%20high%20standards%20required%20for%0Aclinical%20diagnosis%2C%20in%20terms%20of%20eliminating%20aliasing%20artifacts%20and%20reducing%0Aimage%20noise.%20In%20this%20paper%2C%20we%20introduce%20a%20self-supervised%20pretraining%0Aprocedure%20using%20contrastive%20learning%20to%20improve%20the%20accuracy%20of%20undersampled%20DL%0AMRI%20reconstruction.%20We%20use%20contrastive%20learning%20to%20transform%20the%20MRI%20image%0Arepresentations%20into%20a%20latent%20space%20that%20maximizes%20mutual%20information%20among%0Adifferent%20undersampled%20representations%20and%20optimizes%20the%20information%20content%20at%0Athe%20input%20of%20the%20downstream%20DL%20reconstruction%20models.%20Our%20experiments%0Ademonstrate%20improved%20reconstruction%20accuracy%20across%20a%20range%20of%20acceleration%0Afactors%20and%20datasets%2C%20both%20quantitatively%20and%20qualitatively.%20Furthermore%2C%20our%0Aextended%20experiments%20validate%20the%20proposed%20framework%27s%20robustness%20under%0Aadversarial%20conditions%2C%20such%20as%20measurement%20noise%2C%20different%20k-space%20sampling%0Apatterns%2C%20and%20pathological%20abnormalities%2C%20and%20also%20prove%20the%20transfer%20learning%0Acapabilities%20on%20MRI%20datasets%20with%20completely%20different%20anatomy.%20Additionally%2C%0Awe%20conducted%20experiments%20to%20visualize%20and%20analyze%20the%20properties%20of%20the%0Aproposed%20MRI%20contrastive%20learning%20latent%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00530v3&entry.124074799=Read"},
{"title": "Multi-Aspect Controllable Text Generation with Disentangled\n  Counterfactual Augmentation", "author": "Yi Liu and Xiangyu Liu and Xiangrong Zhu and Wei Hu", "abstract": "  Multi-aspect controllable text generation aims to control the generated texts\nin attributes from multiple aspects (e.g., \"positive\" from sentiment and\n\"sport\" from topic). For ease of obtaining training samples, existing works\nneglect attribute correlations formed by the intertwining of different\nattributes. Particularly, the stereotype formed by imbalanced attribute\ncorrelations significantly affects multi-aspect control. In this paper, we\npropose MAGIC, a new multi-aspect controllable text generation method with\ndisentangled counterfactual augmentation. We alleviate the issue of imbalanced\nattribute correlations during training using counterfactual feature vectors in\nthe attribute latent space by disentanglement. During inference, we enhance\nattribute correlations by target-guided counterfactual augmentation to further\nimprove multi-aspect control. Experiments show that MAGIC outperforms\nstate-of-the-art baselines in both imbalanced and balanced attribute\ncorrelation scenarios. Our source code and data are available at\nhttps://github.com/nju-websoft/MAGIC.\n", "link": "http://arxiv.org/abs/2405.19958v1", "date": "2024-05-30", "relevancy": 2.7331, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5812}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5341}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Aspect%20Controllable%20Text%20Generation%20with%20Disentangled%0A%20%20Counterfactual%20Augmentation&body=Title%3A%20Multi-Aspect%20Controllable%20Text%20Generation%20with%20Disentangled%0A%20%20Counterfactual%20Augmentation%0AAuthor%3A%20Yi%20Liu%20and%20Xiangyu%20Liu%20and%20Xiangrong%20Zhu%20and%20Wei%20Hu%0AAbstract%3A%20%20%20Multi-aspect%20controllable%20text%20generation%20aims%20to%20control%20the%20generated%20texts%0Ain%20attributes%20from%20multiple%20aspects%20%28e.g.%2C%20%22positive%22%20from%20sentiment%20and%0A%22sport%22%20from%20topic%29.%20For%20ease%20of%20obtaining%20training%20samples%2C%20existing%20works%0Aneglect%20attribute%20correlations%20formed%20by%20the%20intertwining%20of%20different%0Aattributes.%20Particularly%2C%20the%20stereotype%20formed%20by%20imbalanced%20attribute%0Acorrelations%20significantly%20affects%20multi-aspect%20control.%20In%20this%20paper%2C%20we%0Apropose%20MAGIC%2C%20a%20new%20multi-aspect%20controllable%20text%20generation%20method%20with%0Adisentangled%20counterfactual%20augmentation.%20We%20alleviate%20the%20issue%20of%20imbalanced%0Aattribute%20correlations%20during%20training%20using%20counterfactual%20feature%20vectors%20in%0Athe%20attribute%20latent%20space%20by%20disentanglement.%20During%20inference%2C%20we%20enhance%0Aattribute%20correlations%20by%20target-guided%20counterfactual%20augmentation%20to%20further%0Aimprove%20multi-aspect%20control.%20Experiments%20show%20that%20MAGIC%20outperforms%0Astate-of-the-art%20baselines%20in%20both%20imbalanced%20and%20balanced%20attribute%0Acorrelation%20scenarios.%20Our%20source%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/nju-websoft/MAGIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Aspect%2520Controllable%2520Text%2520Generation%2520with%2520Disentangled%250A%2520%2520Counterfactual%2520Augmentation%26entry.906535625%3DYi%2520Liu%2520and%2520Xiangyu%2520Liu%2520and%2520Xiangrong%2520Zhu%2520and%2520Wei%2520Hu%26entry.1292438233%3D%2520%2520Multi-aspect%2520controllable%2520text%2520generation%2520aims%2520to%2520control%2520the%2520generated%2520texts%250Ain%2520attributes%2520from%2520multiple%2520aspects%2520%2528e.g.%252C%2520%2522positive%2522%2520from%2520sentiment%2520and%250A%2522sport%2522%2520from%2520topic%2529.%2520For%2520ease%2520of%2520obtaining%2520training%2520samples%252C%2520existing%2520works%250Aneglect%2520attribute%2520correlations%2520formed%2520by%2520the%2520intertwining%2520of%2520different%250Aattributes.%2520Particularly%252C%2520the%2520stereotype%2520formed%2520by%2520imbalanced%2520attribute%250Acorrelations%2520significantly%2520affects%2520multi-aspect%2520control.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520MAGIC%252C%2520a%2520new%2520multi-aspect%2520controllable%2520text%2520generation%2520method%2520with%250Adisentangled%2520counterfactual%2520augmentation.%2520We%2520alleviate%2520the%2520issue%2520of%2520imbalanced%250Aattribute%2520correlations%2520during%2520training%2520using%2520counterfactual%2520feature%2520vectors%2520in%250Athe%2520attribute%2520latent%2520space%2520by%2520disentanglement.%2520During%2520inference%252C%2520we%2520enhance%250Aattribute%2520correlations%2520by%2520target-guided%2520counterfactual%2520augmentation%2520to%2520further%250Aimprove%2520multi-aspect%2520control.%2520Experiments%2520show%2520that%2520MAGIC%2520outperforms%250Astate-of-the-art%2520baselines%2520in%2520both%2520imbalanced%2520and%2520balanced%2520attribute%250Acorrelation%2520scenarios.%2520Our%2520source%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/nju-websoft/MAGIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Aspect%20Controllable%20Text%20Generation%20with%20Disentangled%0A%20%20Counterfactual%20Augmentation&entry.906535625=Yi%20Liu%20and%20Xiangyu%20Liu%20and%20Xiangrong%20Zhu%20and%20Wei%20Hu&entry.1292438233=%20%20Multi-aspect%20controllable%20text%20generation%20aims%20to%20control%20the%20generated%20texts%0Ain%20attributes%20from%20multiple%20aspects%20%28e.g.%2C%20%22positive%22%20from%20sentiment%20and%0A%22sport%22%20from%20topic%29.%20For%20ease%20of%20obtaining%20training%20samples%2C%20existing%20works%0Aneglect%20attribute%20correlations%20formed%20by%20the%20intertwining%20of%20different%0Aattributes.%20Particularly%2C%20the%20stereotype%20formed%20by%20imbalanced%20attribute%0Acorrelations%20significantly%20affects%20multi-aspect%20control.%20In%20this%20paper%2C%20we%0Apropose%20MAGIC%2C%20a%20new%20multi-aspect%20controllable%20text%20generation%20method%20with%0Adisentangled%20counterfactual%20augmentation.%20We%20alleviate%20the%20issue%20of%20imbalanced%0Aattribute%20correlations%20during%20training%20using%20counterfactual%20feature%20vectors%20in%0Athe%20attribute%20latent%20space%20by%20disentanglement.%20During%20inference%2C%20we%20enhance%0Aattribute%20correlations%20by%20target-guided%20counterfactual%20augmentation%20to%20further%0Aimprove%20multi-aspect%20control.%20Experiments%20show%20that%20MAGIC%20outperforms%0Astate-of-the-art%20baselines%20in%20both%20imbalanced%20and%20balanced%20attribute%0Acorrelation%20scenarios.%20Our%20source%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/nju-websoft/MAGIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19958v1&entry.124074799=Read"},
{"title": "Multimodal Cross-Domain Few-Shot Learning for Egocentric Action\n  Recognition", "author": "Masashi Hatano and Ryo Hachiuma and Ryo Fuji and Hideo Saito", "abstract": "  We address a novel cross-domain few-shot learning task (CD-FSL) with\nmultimodal input and unlabeled target data for egocentric action recognition.\nThis paper simultaneously tackles two critical challenges associated with\negocentric action recognition in CD-FSL settings: (1) the extreme domain gap in\negocentric videos (\\eg, daily life vs. industrial domain) and (2) the\ncomputational cost for real-world applications. We propose MM-CDFSL, a\ndomain-adaptive and computationally efficient approach designed to enhance\nadaptability to the target domain and improve inference speed. To address the\nfirst challenge, we propose the incorporation of multimodal distillation into\nthe student RGB model using teacher models. Each teacher model is trained\nindependently on source and target data for its respective modality. Leveraging\nonly unlabeled target data during multimodal distillation enhances the student\nmodel's adaptability to the target domain. We further introduce ensemble masked\ninference, a technique that reduces the number of input tokens through masking.\nIn this approach, ensemble prediction mitigates the performance degradation\ncaused by masking, effectively addressing the second issue. Our approach\noutperformed the state-of-the-art CD-FSL approaches with a substantial margin\non multiple egocentric datasets, improving by an average of 6.12/6.10 points\nfor 1-shot/5-shot settings while achieving $2.2$ times faster inference speed.\nProject page: https://masashi-hatano.github.io/MM-CDFSL/\n", "link": "http://arxiv.org/abs/2405.19917v1", "date": "2024-05-30", "relevancy": 2.7155, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5711}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5337}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Cross-Domain%20Few-Shot%20Learning%20for%20Egocentric%20Action%0A%20%20Recognition&body=Title%3A%20Multimodal%20Cross-Domain%20Few-Shot%20Learning%20for%20Egocentric%20Action%0A%20%20Recognition%0AAuthor%3A%20Masashi%20Hatano%20and%20Ryo%20Hachiuma%20and%20Ryo%20Fuji%20and%20Hideo%20Saito%0AAbstract%3A%20%20%20We%20address%20a%20novel%20cross-domain%20few-shot%20learning%20task%20%28CD-FSL%29%20with%0Amultimodal%20input%20and%20unlabeled%20target%20data%20for%20egocentric%20action%20recognition.%0AThis%20paper%20simultaneously%20tackles%20two%20critical%20challenges%20associated%20with%0Aegocentric%20action%20recognition%20in%20CD-FSL%20settings%3A%20%281%29%20the%20extreme%20domain%20gap%20in%0Aegocentric%20videos%20%28%5Ceg%2C%20daily%20life%20vs.%20industrial%20domain%29%20and%20%282%29%20the%0Acomputational%20cost%20for%20real-world%20applications.%20We%20propose%20MM-CDFSL%2C%20a%0Adomain-adaptive%20and%20computationally%20efficient%20approach%20designed%20to%20enhance%0Aadaptability%20to%20the%20target%20domain%20and%20improve%20inference%20speed.%20To%20address%20the%0Afirst%20challenge%2C%20we%20propose%20the%20incorporation%20of%20multimodal%20distillation%20into%0Athe%20student%20RGB%20model%20using%20teacher%20models.%20Each%20teacher%20model%20is%20trained%0Aindependently%20on%20source%20and%20target%20data%20for%20its%20respective%20modality.%20Leveraging%0Aonly%20unlabeled%20target%20data%20during%20multimodal%20distillation%20enhances%20the%20student%0Amodel%27s%20adaptability%20to%20the%20target%20domain.%20We%20further%20introduce%20ensemble%20masked%0Ainference%2C%20a%20technique%20that%20reduces%20the%20number%20of%20input%20tokens%20through%20masking.%0AIn%20this%20approach%2C%20ensemble%20prediction%20mitigates%20the%20performance%20degradation%0Acaused%20by%20masking%2C%20effectively%20addressing%20the%20second%20issue.%20Our%20approach%0Aoutperformed%20the%20state-of-the-art%20CD-FSL%20approaches%20with%20a%20substantial%20margin%0Aon%20multiple%20egocentric%20datasets%2C%20improving%20by%20an%20average%20of%206.12/6.10%20points%0Afor%201-shot/5-shot%20settings%20while%20achieving%20%242.2%24%20times%20faster%20inference%20speed.%0AProject%20page%3A%20https%3A//masashi-hatano.github.io/MM-CDFSL/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Cross-Domain%2520Few-Shot%2520Learning%2520for%2520Egocentric%2520Action%250A%2520%2520Recognition%26entry.906535625%3DMasashi%2520Hatano%2520and%2520Ryo%2520Hachiuma%2520and%2520Ryo%2520Fuji%2520and%2520Hideo%2520Saito%26entry.1292438233%3D%2520%2520We%2520address%2520a%2520novel%2520cross-domain%2520few-shot%2520learning%2520task%2520%2528CD-FSL%2529%2520with%250Amultimodal%2520input%2520and%2520unlabeled%2520target%2520data%2520for%2520egocentric%2520action%2520recognition.%250AThis%2520paper%2520simultaneously%2520tackles%2520two%2520critical%2520challenges%2520associated%2520with%250Aegocentric%2520action%2520recognition%2520in%2520CD-FSL%2520settings%253A%2520%25281%2529%2520the%2520extreme%2520domain%2520gap%2520in%250Aegocentric%2520videos%2520%2528%255Ceg%252C%2520daily%2520life%2520vs.%2520industrial%2520domain%2529%2520and%2520%25282%2529%2520the%250Acomputational%2520cost%2520for%2520real-world%2520applications.%2520We%2520propose%2520MM-CDFSL%252C%2520a%250Adomain-adaptive%2520and%2520computationally%2520efficient%2520approach%2520designed%2520to%2520enhance%250Aadaptability%2520to%2520the%2520target%2520domain%2520and%2520improve%2520inference%2520speed.%2520To%2520address%2520the%250Afirst%2520challenge%252C%2520we%2520propose%2520the%2520incorporation%2520of%2520multimodal%2520distillation%2520into%250Athe%2520student%2520RGB%2520model%2520using%2520teacher%2520models.%2520Each%2520teacher%2520model%2520is%2520trained%250Aindependently%2520on%2520source%2520and%2520target%2520data%2520for%2520its%2520respective%2520modality.%2520Leveraging%250Aonly%2520unlabeled%2520target%2520data%2520during%2520multimodal%2520distillation%2520enhances%2520the%2520student%250Amodel%2527s%2520adaptability%2520to%2520the%2520target%2520domain.%2520We%2520further%2520introduce%2520ensemble%2520masked%250Ainference%252C%2520a%2520technique%2520that%2520reduces%2520the%2520number%2520of%2520input%2520tokens%2520through%2520masking.%250AIn%2520this%2520approach%252C%2520ensemble%2520prediction%2520mitigates%2520the%2520performance%2520degradation%250Acaused%2520by%2520masking%252C%2520effectively%2520addressing%2520the%2520second%2520issue.%2520Our%2520approach%250Aoutperformed%2520the%2520state-of-the-art%2520CD-FSL%2520approaches%2520with%2520a%2520substantial%2520margin%250Aon%2520multiple%2520egocentric%2520datasets%252C%2520improving%2520by%2520an%2520average%2520of%25206.12/6.10%2520points%250Afor%25201-shot/5-shot%2520settings%2520while%2520achieving%2520%25242.2%2524%2520times%2520faster%2520inference%2520speed.%250AProject%2520page%253A%2520https%253A//masashi-hatano.github.io/MM-CDFSL/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Cross-Domain%20Few-Shot%20Learning%20for%20Egocentric%20Action%0A%20%20Recognition&entry.906535625=Masashi%20Hatano%20and%20Ryo%20Hachiuma%20and%20Ryo%20Fuji%20and%20Hideo%20Saito&entry.1292438233=%20%20We%20address%20a%20novel%20cross-domain%20few-shot%20learning%20task%20%28CD-FSL%29%20with%0Amultimodal%20input%20and%20unlabeled%20target%20data%20for%20egocentric%20action%20recognition.%0AThis%20paper%20simultaneously%20tackles%20two%20critical%20challenges%20associated%20with%0Aegocentric%20action%20recognition%20in%20CD-FSL%20settings%3A%20%281%29%20the%20extreme%20domain%20gap%20in%0Aegocentric%20videos%20%28%5Ceg%2C%20daily%20life%20vs.%20industrial%20domain%29%20and%20%282%29%20the%0Acomputational%20cost%20for%20real-world%20applications.%20We%20propose%20MM-CDFSL%2C%20a%0Adomain-adaptive%20and%20computationally%20efficient%20approach%20designed%20to%20enhance%0Aadaptability%20to%20the%20target%20domain%20and%20improve%20inference%20speed.%20To%20address%20the%0Afirst%20challenge%2C%20we%20propose%20the%20incorporation%20of%20multimodal%20distillation%20into%0Athe%20student%20RGB%20model%20using%20teacher%20models.%20Each%20teacher%20model%20is%20trained%0Aindependently%20on%20source%20and%20target%20data%20for%20its%20respective%20modality.%20Leveraging%0Aonly%20unlabeled%20target%20data%20during%20multimodal%20distillation%20enhances%20the%20student%0Amodel%27s%20adaptability%20to%20the%20target%20domain.%20We%20further%20introduce%20ensemble%20masked%0Ainference%2C%20a%20technique%20that%20reduces%20the%20number%20of%20input%20tokens%20through%20masking.%0AIn%20this%20approach%2C%20ensemble%20prediction%20mitigates%20the%20performance%20degradation%0Acaused%20by%20masking%2C%20effectively%20addressing%20the%20second%20issue.%20Our%20approach%0Aoutperformed%20the%20state-of-the-art%20CD-FSL%20approaches%20with%20a%20substantial%20margin%0Aon%20multiple%20egocentric%20datasets%2C%20improving%20by%20an%20average%20of%206.12/6.10%20points%0Afor%201-shot/5-shot%20settings%20while%20achieving%20%242.2%24%20times%20faster%20inference%20speed.%0AProject%20page%3A%20https%3A//masashi-hatano.github.io/MM-CDFSL/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19917v1&entry.124074799=Read"},
{"title": "Image Coding for Machines with Edge Information Learning Using Segment\n  Anything", "author": "Takahiro Shindo and Kein Yamada and Taiju Watanabe and Hiroshi Watanabe", "abstract": "  Image Coding for Machines (ICM) is an image compression technique for image\nrecognition.\n  This technique is essential due to the growing demand for image recognition\nAI.\n  In this paper, we propose a method for ICM that focuses on encoding and\ndecoding only the edge information of object parts in an image, which we call\nSA-ICM.\n  This is an Learned Image Compression (LIC) model trained using edge\ninformation created by Segment Anything.\n  Our method can be used for image recognition models with various tasks.\n  SA-ICM is also robust to changes in input data, making it effective for a\nvariety of use cases.\n  Additionally, our method provides benefits from a privacy point of view, as\nit removes human facial information on the encoder's side, thus protecting\none's privacy.\n  Furthermore, this LIC model training method can be used to train Neural\nRepresentations for Videos (NeRV), which is a video compression model.\n  By training NeRV using edge information created by Segment Anything, it is\npossible to create a NeRV that is effective for image recognition (SA-NeRV).\n  Experimental results confirm the advantages of SA-ICM, presenting the best\nperformance in image compression for image recognition.\n  We also show that SA-NeRV is superior to ordinary NeRV in video compression\nfor machines.\n", "link": "http://arxiv.org/abs/2403.04173v2", "date": "2024-05-30", "relevancy": 2.6481, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5505}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5308}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Coding%20for%20Machines%20with%20Edge%20Information%20Learning%20Using%20Segment%0A%20%20Anything&body=Title%3A%20Image%20Coding%20for%20Machines%20with%20Edge%20Information%20Learning%20Using%20Segment%0A%20%20Anything%0AAuthor%3A%20Takahiro%20Shindo%20and%20Kein%20Yamada%20and%20Taiju%20Watanabe%20and%20Hiroshi%20Watanabe%0AAbstract%3A%20%20%20Image%20Coding%20for%20Machines%20%28ICM%29%20is%20an%20image%20compression%20technique%20for%20image%0Arecognition.%0A%20%20This%20technique%20is%20essential%20due%20to%20the%20growing%20demand%20for%20image%20recognition%0AAI.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20ICM%20that%20focuses%20on%20encoding%20and%0Adecoding%20only%20the%20edge%20information%20of%20object%20parts%20in%20an%20image%2C%20which%20we%20call%0ASA-ICM.%0A%20%20This%20is%20an%20Learned%20Image%20Compression%20%28LIC%29%20model%20trained%20using%20edge%0Ainformation%20created%20by%20Segment%20Anything.%0A%20%20Our%20method%20can%20be%20used%20for%20image%20recognition%20models%20with%20various%20tasks.%0A%20%20SA-ICM%20is%20also%20robust%20to%20changes%20in%20input%20data%2C%20making%20it%20effective%20for%20a%0Avariety%20of%20use%20cases.%0A%20%20Additionally%2C%20our%20method%20provides%20benefits%20from%20a%20privacy%20point%20of%20view%2C%20as%0Ait%20removes%20human%20facial%20information%20on%20the%20encoder%27s%20side%2C%20thus%20protecting%0Aone%27s%20privacy.%0A%20%20Furthermore%2C%20this%20LIC%20model%20training%20method%20can%20be%20used%20to%20train%20Neural%0ARepresentations%20for%20Videos%20%28NeRV%29%2C%20which%20is%20a%20video%20compression%20model.%0A%20%20By%20training%20NeRV%20using%20edge%20information%20created%20by%20Segment%20Anything%2C%20it%20is%0Apossible%20to%20create%20a%20NeRV%20that%20is%20effective%20for%20image%20recognition%20%28SA-NeRV%29.%0A%20%20Experimental%20results%20confirm%20the%20advantages%20of%20SA-ICM%2C%20presenting%20the%20best%0Aperformance%20in%20image%20compression%20for%20image%20recognition.%0A%20%20We%20also%20show%20that%20SA-NeRV%20is%20superior%20to%20ordinary%20NeRV%20in%20video%20compression%0Afor%20machines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Coding%2520for%2520Machines%2520with%2520Edge%2520Information%2520Learning%2520Using%2520Segment%250A%2520%2520Anything%26entry.906535625%3DTakahiro%2520Shindo%2520and%2520Kein%2520Yamada%2520and%2520Taiju%2520Watanabe%2520and%2520Hiroshi%2520Watanabe%26entry.1292438233%3D%2520%2520Image%2520Coding%2520for%2520Machines%2520%2528ICM%2529%2520is%2520an%2520image%2520compression%2520technique%2520for%2520image%250Arecognition.%250A%2520%2520This%2520technique%2520is%2520essential%2520due%2520to%2520the%2520growing%2520demand%2520for%2520image%2520recognition%250AAI.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520for%2520ICM%2520that%2520focuses%2520on%2520encoding%2520and%250Adecoding%2520only%2520the%2520edge%2520information%2520of%2520object%2520parts%2520in%2520an%2520image%252C%2520which%2520we%2520call%250ASA-ICM.%250A%2520%2520This%2520is%2520an%2520Learned%2520Image%2520Compression%2520%2528LIC%2529%2520model%2520trained%2520using%2520edge%250Ainformation%2520created%2520by%2520Segment%2520Anything.%250A%2520%2520Our%2520method%2520can%2520be%2520used%2520for%2520image%2520recognition%2520models%2520with%2520various%2520tasks.%250A%2520%2520SA-ICM%2520is%2520also%2520robust%2520to%2520changes%2520in%2520input%2520data%252C%2520making%2520it%2520effective%2520for%2520a%250Avariety%2520of%2520use%2520cases.%250A%2520%2520Additionally%252C%2520our%2520method%2520provides%2520benefits%2520from%2520a%2520privacy%2520point%2520of%2520view%252C%2520as%250Ait%2520removes%2520human%2520facial%2520information%2520on%2520the%2520encoder%2527s%2520side%252C%2520thus%2520protecting%250Aone%2527s%2520privacy.%250A%2520%2520Furthermore%252C%2520this%2520LIC%2520model%2520training%2520method%2520can%2520be%2520used%2520to%2520train%2520Neural%250ARepresentations%2520for%2520Videos%2520%2528NeRV%2529%252C%2520which%2520is%2520a%2520video%2520compression%2520model.%250A%2520%2520By%2520training%2520NeRV%2520using%2520edge%2520information%2520created%2520by%2520Segment%2520Anything%252C%2520it%2520is%250Apossible%2520to%2520create%2520a%2520NeRV%2520that%2520is%2520effective%2520for%2520image%2520recognition%2520%2528SA-NeRV%2529.%250A%2520%2520Experimental%2520results%2520confirm%2520the%2520advantages%2520of%2520SA-ICM%252C%2520presenting%2520the%2520best%250Aperformance%2520in%2520image%2520compression%2520for%2520image%2520recognition.%250A%2520%2520We%2520also%2520show%2520that%2520SA-NeRV%2520is%2520superior%2520to%2520ordinary%2520NeRV%2520in%2520video%2520compression%250Afor%2520machines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Coding%20for%20Machines%20with%20Edge%20Information%20Learning%20Using%20Segment%0A%20%20Anything&entry.906535625=Takahiro%20Shindo%20and%20Kein%20Yamada%20and%20Taiju%20Watanabe%20and%20Hiroshi%20Watanabe&entry.1292438233=%20%20Image%20Coding%20for%20Machines%20%28ICM%29%20is%20an%20image%20compression%20technique%20for%20image%0Arecognition.%0A%20%20This%20technique%20is%20essential%20due%20to%20the%20growing%20demand%20for%20image%20recognition%0AAI.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20ICM%20that%20focuses%20on%20encoding%20and%0Adecoding%20only%20the%20edge%20information%20of%20object%20parts%20in%20an%20image%2C%20which%20we%20call%0ASA-ICM.%0A%20%20This%20is%20an%20Learned%20Image%20Compression%20%28LIC%29%20model%20trained%20using%20edge%0Ainformation%20created%20by%20Segment%20Anything.%0A%20%20Our%20method%20can%20be%20used%20for%20image%20recognition%20models%20with%20various%20tasks.%0A%20%20SA-ICM%20is%20also%20robust%20to%20changes%20in%20input%20data%2C%20making%20it%20effective%20for%20a%0Avariety%20of%20use%20cases.%0A%20%20Additionally%2C%20our%20method%20provides%20benefits%20from%20a%20privacy%20point%20of%20view%2C%20as%0Ait%20removes%20human%20facial%20information%20on%20the%20encoder%27s%20side%2C%20thus%20protecting%0Aone%27s%20privacy.%0A%20%20Furthermore%2C%20this%20LIC%20model%20training%20method%20can%20be%20used%20to%20train%20Neural%0ARepresentations%20for%20Videos%20%28NeRV%29%2C%20which%20is%20a%20video%20compression%20model.%0A%20%20By%20training%20NeRV%20using%20edge%20information%20created%20by%20Segment%20Anything%2C%20it%20is%0Apossible%20to%20create%20a%20NeRV%20that%20is%20effective%20for%20image%20recognition%20%28SA-NeRV%29.%0A%20%20Experimental%20results%20confirm%20the%20advantages%20of%20SA-ICM%2C%20presenting%20the%20best%0Aperformance%20in%20image%20compression%20for%20image%20recognition.%0A%20%20We%20also%20show%20that%20SA-NeRV%20is%20superior%20to%20ordinary%20NeRV%20in%20video%20compression%0Afor%20machines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04173v2&entry.124074799=Read"},
{"title": "COIN-LIO: Complementary Intensity-Augmented LiDAR Inertial Odometry", "author": "Patrick Pfreundschuh and Helen Oleynikova and Cesar Cadena and Roland Siegwart and Olov Andersson", "abstract": "  We present COIN-LIO, a LiDAR Inertial Odometry pipeline that tightly couples\ninformation from LiDAR intensity with geometry-based point cloud registration.\nThe focus of our work is to improve the robustness of LiDAR-inertial odometry\nin geometrically degenerate scenarios, like tunnels or flat fields. We project\nLiDAR intensity returns into an intensity image, and propose an image\nprocessing pipeline that produces filtered images with improved brightness\nconsistency within the image as well as across different scenes. To effectively\nleverage intensity as an additional modality, we present a novel feature\nselection scheme that detects uninformative directions in the point cloud\nregistration and explicitly selects patches with complementary image\ninformation. Photometric error minimization in the image patches is then fused\nwith inertial measurements and point-to-plane registration in an iterated\nExtended Kalman Filter. The proposed approach improves accuracy and robustness\non a public dataset. We additionally publish a new dataset, that captures five\nreal-world environments in challenging, geometrically degenerate scenes. By\nusing the additional photometric information, our approach shows drastically\nimproved robustness against geometric degeneracy in environments where all\ncompared baseline approaches fail.\n", "link": "http://arxiv.org/abs/2310.01235v4", "date": "2024-05-30", "relevancy": 2.6361, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5424}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5234}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COIN-LIO%3A%20Complementary%20Intensity-Augmented%20LiDAR%20Inertial%20Odometry&body=Title%3A%20COIN-LIO%3A%20Complementary%20Intensity-Augmented%20LiDAR%20Inertial%20Odometry%0AAuthor%3A%20Patrick%20Pfreundschuh%20and%20Helen%20Oleynikova%20and%20Cesar%20Cadena%20and%20Roland%20Siegwart%20and%20Olov%20Andersson%0AAbstract%3A%20%20%20We%20present%20COIN-LIO%2C%20a%20LiDAR%20Inertial%20Odometry%20pipeline%20that%20tightly%20couples%0Ainformation%20from%20LiDAR%20intensity%20with%20geometry-based%20point%20cloud%20registration.%0AThe%20focus%20of%20our%20work%20is%20to%20improve%20the%20robustness%20of%20LiDAR-inertial%20odometry%0Ain%20geometrically%20degenerate%20scenarios%2C%20like%20tunnels%20or%20flat%20fields.%20We%20project%0ALiDAR%20intensity%20returns%20into%20an%20intensity%20image%2C%20and%20propose%20an%20image%0Aprocessing%20pipeline%20that%20produces%20filtered%20images%20with%20improved%20brightness%0Aconsistency%20within%20the%20image%20as%20well%20as%20across%20different%20scenes.%20To%20effectively%0Aleverage%20intensity%20as%20an%20additional%20modality%2C%20we%20present%20a%20novel%20feature%0Aselection%20scheme%20that%20detects%20uninformative%20directions%20in%20the%20point%20cloud%0Aregistration%20and%20explicitly%20selects%20patches%20with%20complementary%20image%0Ainformation.%20Photometric%20error%20minimization%20in%20the%20image%20patches%20is%20then%20fused%0Awith%20inertial%20measurements%20and%20point-to-plane%20registration%20in%20an%20iterated%0AExtended%20Kalman%20Filter.%20The%20proposed%20approach%20improves%20accuracy%20and%20robustness%0Aon%20a%20public%20dataset.%20We%20additionally%20publish%20a%20new%20dataset%2C%20that%20captures%20five%0Areal-world%20environments%20in%20challenging%2C%20geometrically%20degenerate%20scenes.%20By%0Ausing%20the%20additional%20photometric%20information%2C%20our%20approach%20shows%20drastically%0Aimproved%20robustness%20against%20geometric%20degeneracy%20in%20environments%20where%20all%0Acompared%20baseline%20approaches%20fail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01235v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOIN-LIO%253A%2520Complementary%2520Intensity-Augmented%2520LiDAR%2520Inertial%2520Odometry%26entry.906535625%3DPatrick%2520Pfreundschuh%2520and%2520Helen%2520Oleynikova%2520and%2520Cesar%2520Cadena%2520and%2520Roland%2520Siegwart%2520and%2520Olov%2520Andersson%26entry.1292438233%3D%2520%2520We%2520present%2520COIN-LIO%252C%2520a%2520LiDAR%2520Inertial%2520Odometry%2520pipeline%2520that%2520tightly%2520couples%250Ainformation%2520from%2520LiDAR%2520intensity%2520with%2520geometry-based%2520point%2520cloud%2520registration.%250AThe%2520focus%2520of%2520our%2520work%2520is%2520to%2520improve%2520the%2520robustness%2520of%2520LiDAR-inertial%2520odometry%250Ain%2520geometrically%2520degenerate%2520scenarios%252C%2520like%2520tunnels%2520or%2520flat%2520fields.%2520We%2520project%250ALiDAR%2520intensity%2520returns%2520into%2520an%2520intensity%2520image%252C%2520and%2520propose%2520an%2520image%250Aprocessing%2520pipeline%2520that%2520produces%2520filtered%2520images%2520with%2520improved%2520brightness%250Aconsistency%2520within%2520the%2520image%2520as%2520well%2520as%2520across%2520different%2520scenes.%2520To%2520effectively%250Aleverage%2520intensity%2520as%2520an%2520additional%2520modality%252C%2520we%2520present%2520a%2520novel%2520feature%250Aselection%2520scheme%2520that%2520detects%2520uninformative%2520directions%2520in%2520the%2520point%2520cloud%250Aregistration%2520and%2520explicitly%2520selects%2520patches%2520with%2520complementary%2520image%250Ainformation.%2520Photometric%2520error%2520minimization%2520in%2520the%2520image%2520patches%2520is%2520then%2520fused%250Awith%2520inertial%2520measurements%2520and%2520point-to-plane%2520registration%2520in%2520an%2520iterated%250AExtended%2520Kalman%2520Filter.%2520The%2520proposed%2520approach%2520improves%2520accuracy%2520and%2520robustness%250Aon%2520a%2520public%2520dataset.%2520We%2520additionally%2520publish%2520a%2520new%2520dataset%252C%2520that%2520captures%2520five%250Areal-world%2520environments%2520in%2520challenging%252C%2520geometrically%2520degenerate%2520scenes.%2520By%250Ausing%2520the%2520additional%2520photometric%2520information%252C%2520our%2520approach%2520shows%2520drastically%250Aimproved%2520robustness%2520against%2520geometric%2520degeneracy%2520in%2520environments%2520where%2520all%250Acompared%2520baseline%2520approaches%2520fail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01235v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COIN-LIO%3A%20Complementary%20Intensity-Augmented%20LiDAR%20Inertial%20Odometry&entry.906535625=Patrick%20Pfreundschuh%20and%20Helen%20Oleynikova%20and%20Cesar%20Cadena%20and%20Roland%20Siegwart%20and%20Olov%20Andersson&entry.1292438233=%20%20We%20present%20COIN-LIO%2C%20a%20LiDAR%20Inertial%20Odometry%20pipeline%20that%20tightly%20couples%0Ainformation%20from%20LiDAR%20intensity%20with%20geometry-based%20point%20cloud%20registration.%0AThe%20focus%20of%20our%20work%20is%20to%20improve%20the%20robustness%20of%20LiDAR-inertial%20odometry%0Ain%20geometrically%20degenerate%20scenarios%2C%20like%20tunnels%20or%20flat%20fields.%20We%20project%0ALiDAR%20intensity%20returns%20into%20an%20intensity%20image%2C%20and%20propose%20an%20image%0Aprocessing%20pipeline%20that%20produces%20filtered%20images%20with%20improved%20brightness%0Aconsistency%20within%20the%20image%20as%20well%20as%20across%20different%20scenes.%20To%20effectively%0Aleverage%20intensity%20as%20an%20additional%20modality%2C%20we%20present%20a%20novel%20feature%0Aselection%20scheme%20that%20detects%20uninformative%20directions%20in%20the%20point%20cloud%0Aregistration%20and%20explicitly%20selects%20patches%20with%20complementary%20image%0Ainformation.%20Photometric%20error%20minimization%20in%20the%20image%20patches%20is%20then%20fused%0Awith%20inertial%20measurements%20and%20point-to-plane%20registration%20in%20an%20iterated%0AExtended%20Kalman%20Filter.%20The%20proposed%20approach%20improves%20accuracy%20and%20robustness%0Aon%20a%20public%20dataset.%20We%20additionally%20publish%20a%20new%20dataset%2C%20that%20captures%20five%0Areal-world%20environments%20in%20challenging%2C%20geometrically%20degenerate%20scenes.%20By%0Ausing%20the%20additional%20photometric%20information%2C%20our%20approach%20shows%20drastically%0Aimproved%20robustness%20against%20geometric%20degeneracy%20in%20environments%20where%20all%0Acompared%20baseline%20approaches%20fail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01235v4&entry.124074799=Read"},
{"title": "Multi-View People Detection in Large Scenes via Supervised View-Wise\n  Contribution Weighting", "author": "Qi Zhang and Yunfei Gong and Daijie Chen and Antoni B. Chan and Hui Huang", "abstract": "  Recent deep learning-based multi-view people detection (MVD) methods have\nshown promising results on existing datasets. However, current methods are\nmainly trained and evaluated on small, single scenes with a limited number of\nmulti-view frames and fixed camera views. As a result, these methods may not be\npractical for detecting people in larger, more complex scenes with severe\nocclusions and camera calibration errors. This paper focuses on improving\nmulti-view people detection by developing a supervised view-wise contribution\nweighting approach that better fuses multi-camera information under large\nscenes. Besides, a large synthetic dataset is adopted to enhance the model's\ngeneralization ability and enable more practical evaluation and comparison. The\nmodel's performance on new testing scenes is further improved with a simple\ndomain adaptation technique. Experimental results demonstrate the effectiveness\nof our approach in achieving promising cross-scene multi-view people detection\nperformance. See code here: https://vcc.tech/research/2024/MVD.\n", "link": "http://arxiv.org/abs/2405.19943v1", "date": "2024-05-30", "relevancy": 2.6123, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5284}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5195}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20People%20Detection%20in%20Large%20Scenes%20via%20Supervised%20View-Wise%0A%20%20Contribution%20Weighting&body=Title%3A%20Multi-View%20People%20Detection%20in%20Large%20Scenes%20via%20Supervised%20View-Wise%0A%20%20Contribution%20Weighting%0AAuthor%3A%20Qi%20Zhang%20and%20Yunfei%20Gong%20and%20Daijie%20Chen%20and%20Antoni%20B.%20Chan%20and%20Hui%20Huang%0AAbstract%3A%20%20%20Recent%20deep%20learning-based%20multi-view%20people%20detection%20%28MVD%29%20methods%20have%0Ashown%20promising%20results%20on%20existing%20datasets.%20However%2C%20current%20methods%20are%0Amainly%20trained%20and%20evaluated%20on%20small%2C%20single%20scenes%20with%20a%20limited%20number%20of%0Amulti-view%20frames%20and%20fixed%20camera%20views.%20As%20a%20result%2C%20these%20methods%20may%20not%20be%0Apractical%20for%20detecting%20people%20in%20larger%2C%20more%20complex%20scenes%20with%20severe%0Aocclusions%20and%20camera%20calibration%20errors.%20This%20paper%20focuses%20on%20improving%0Amulti-view%20people%20detection%20by%20developing%20a%20supervised%20view-wise%20contribution%0Aweighting%20approach%20that%20better%20fuses%20multi-camera%20information%20under%20large%0Ascenes.%20Besides%2C%20a%20large%20synthetic%20dataset%20is%20adopted%20to%20enhance%20the%20model%27s%0Ageneralization%20ability%20and%20enable%20more%20practical%20evaluation%20and%20comparison.%20The%0Amodel%27s%20performance%20on%20new%20testing%20scenes%20is%20further%20improved%20with%20a%20simple%0Adomain%20adaptation%20technique.%20Experimental%20results%20demonstrate%20the%20effectiveness%0Aof%20our%20approach%20in%20achieving%20promising%20cross-scene%20multi-view%20people%20detection%0Aperformance.%20See%20code%20here%3A%20https%3A//vcc.tech/research/2024/MVD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520People%2520Detection%2520in%2520Large%2520Scenes%2520via%2520Supervised%2520View-Wise%250A%2520%2520Contribution%2520Weighting%26entry.906535625%3DQi%2520Zhang%2520and%2520Yunfei%2520Gong%2520and%2520Daijie%2520Chen%2520and%2520Antoni%2520B.%2520Chan%2520and%2520Hui%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520deep%2520learning-based%2520multi-view%2520people%2520detection%2520%2528MVD%2529%2520methods%2520have%250Ashown%2520promising%2520results%2520on%2520existing%2520datasets.%2520However%252C%2520current%2520methods%2520are%250Amainly%2520trained%2520and%2520evaluated%2520on%2520small%252C%2520single%2520scenes%2520with%2520a%2520limited%2520number%2520of%250Amulti-view%2520frames%2520and%2520fixed%2520camera%2520views.%2520As%2520a%2520result%252C%2520these%2520methods%2520may%2520not%2520be%250Apractical%2520for%2520detecting%2520people%2520in%2520larger%252C%2520more%2520complex%2520scenes%2520with%2520severe%250Aocclusions%2520and%2520camera%2520calibration%2520errors.%2520This%2520paper%2520focuses%2520on%2520improving%250Amulti-view%2520people%2520detection%2520by%2520developing%2520a%2520supervised%2520view-wise%2520contribution%250Aweighting%2520approach%2520that%2520better%2520fuses%2520multi-camera%2520information%2520under%2520large%250Ascenes.%2520Besides%252C%2520a%2520large%2520synthetic%2520dataset%2520is%2520adopted%2520to%2520enhance%2520the%2520model%2527s%250Ageneralization%2520ability%2520and%2520enable%2520more%2520practical%2520evaluation%2520and%2520comparison.%2520The%250Amodel%2527s%2520performance%2520on%2520new%2520testing%2520scenes%2520is%2520further%2520improved%2520with%2520a%2520simple%250Adomain%2520adaptation%2520technique.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520approach%2520in%2520achieving%2520promising%2520cross-scene%2520multi-view%2520people%2520detection%250Aperformance.%2520See%2520code%2520here%253A%2520https%253A//vcc.tech/research/2024/MVD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20People%20Detection%20in%20Large%20Scenes%20via%20Supervised%20View-Wise%0A%20%20Contribution%20Weighting&entry.906535625=Qi%20Zhang%20and%20Yunfei%20Gong%20and%20Daijie%20Chen%20and%20Antoni%20B.%20Chan%20and%20Hui%20Huang&entry.1292438233=%20%20Recent%20deep%20learning-based%20multi-view%20people%20detection%20%28MVD%29%20methods%20have%0Ashown%20promising%20results%20on%20existing%20datasets.%20However%2C%20current%20methods%20are%0Amainly%20trained%20and%20evaluated%20on%20small%2C%20single%20scenes%20with%20a%20limited%20number%20of%0Amulti-view%20frames%20and%20fixed%20camera%20views.%20As%20a%20result%2C%20these%20methods%20may%20not%20be%0Apractical%20for%20detecting%20people%20in%20larger%2C%20more%20complex%20scenes%20with%20severe%0Aocclusions%20and%20camera%20calibration%20errors.%20This%20paper%20focuses%20on%20improving%0Amulti-view%20people%20detection%20by%20developing%20a%20supervised%20view-wise%20contribution%0Aweighting%20approach%20that%20better%20fuses%20multi-camera%20information%20under%20large%0Ascenes.%20Besides%2C%20a%20large%20synthetic%20dataset%20is%20adopted%20to%20enhance%20the%20model%27s%0Ageneralization%20ability%20and%20enable%20more%20practical%20evaluation%20and%20comparison.%20The%0Amodel%27s%20performance%20on%20new%20testing%20scenes%20is%20further%20improved%20with%20a%20simple%0Adomain%20adaptation%20technique.%20Experimental%20results%20demonstrate%20the%20effectiveness%0Aof%20our%20approach%20in%20achieving%20promising%20cross-scene%20multi-view%20people%20detection%0Aperformance.%20See%20code%20here%3A%20https%3A//vcc.tech/research/2024/MVD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19943v1&entry.124074799=Read"},
{"title": "Grokfast: Accelerated Grokking by Amplifying Slow Gradients", "author": "Jaerin Lee and Bong Gyun Kang and Kihoon Kim and Kyoung Mu Lee", "abstract": "  One puzzling artifact in machine learning dubbed grokking is where delayed\ngeneralization is achieved tenfolds of iterations after near perfect\noverfitting to the training data. Focusing on the long delay itself on behalf\nof machine learning practitioners, our goal is to accelerate generalization of\na model under grokking phenomenon. By regarding a series of gradients of a\nparameter over training iterations as a random signal over time, we can\nspectrally decompose the parameter trajectories under gradient descent into two\ncomponents: the fast-varying, overfitting-yielding component and the\nslow-varying, generalization-inducing component. This analysis allows us to\naccelerate the grokking phenomenon more than $\\times 50$ with only a few lines\nof code that amplifies the slow-varying components of gradients. The\nexperiments show that our algorithm applies to diverse tasks involving images,\nlanguages, and graphs, enabling practical availability of this peculiar\nartifact of sudden generalization. Our code is available at\n\\url{https://github.com/ironjr/grokfast}.\n", "link": "http://arxiv.org/abs/2405.20233v1", "date": "2024-05-30", "relevancy": 2.5628, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5255}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5074}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grokfast%3A%20Accelerated%20Grokking%20by%20Amplifying%20Slow%20Gradients&body=Title%3A%20Grokfast%3A%20Accelerated%20Grokking%20by%20Amplifying%20Slow%20Gradients%0AAuthor%3A%20Jaerin%20Lee%20and%20Bong%20Gyun%20Kang%20and%20Kihoon%20Kim%20and%20Kyoung%20Mu%20Lee%0AAbstract%3A%20%20%20One%20puzzling%20artifact%20in%20machine%20learning%20dubbed%20grokking%20is%20where%20delayed%0Ageneralization%20is%20achieved%20tenfolds%20of%20iterations%20after%20near%20perfect%0Aoverfitting%20to%20the%20training%20data.%20Focusing%20on%20the%20long%20delay%20itself%20on%20behalf%0Aof%20machine%20learning%20practitioners%2C%20our%20goal%20is%20to%20accelerate%20generalization%20of%0Aa%20model%20under%20grokking%20phenomenon.%20By%20regarding%20a%20series%20of%20gradients%20of%20a%0Aparameter%20over%20training%20iterations%20as%20a%20random%20signal%20over%20time%2C%20we%20can%0Aspectrally%20decompose%20the%20parameter%20trajectories%20under%20gradient%20descent%20into%20two%0Acomponents%3A%20the%20fast-varying%2C%20overfitting-yielding%20component%20and%20the%0Aslow-varying%2C%20generalization-inducing%20component.%20This%20analysis%20allows%20us%20to%0Aaccelerate%20the%20grokking%20phenomenon%20more%20than%20%24%5Ctimes%2050%24%20with%20only%20a%20few%20lines%0Aof%20code%20that%20amplifies%20the%20slow-varying%20components%20of%20gradients.%20The%0Aexperiments%20show%20that%20our%20algorithm%20applies%20to%20diverse%20tasks%20involving%20images%2C%0Alanguages%2C%20and%20graphs%2C%20enabling%20practical%20availability%20of%20this%20peculiar%0Aartifact%20of%20sudden%20generalization.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ironjr/grokfast%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrokfast%253A%2520Accelerated%2520Grokking%2520by%2520Amplifying%2520Slow%2520Gradients%26entry.906535625%3DJaerin%2520Lee%2520and%2520Bong%2520Gyun%2520Kang%2520and%2520Kihoon%2520Kim%2520and%2520Kyoung%2520Mu%2520Lee%26entry.1292438233%3D%2520%2520One%2520puzzling%2520artifact%2520in%2520machine%2520learning%2520dubbed%2520grokking%2520is%2520where%2520delayed%250Ageneralization%2520is%2520achieved%2520tenfolds%2520of%2520iterations%2520after%2520near%2520perfect%250Aoverfitting%2520to%2520the%2520training%2520data.%2520Focusing%2520on%2520the%2520long%2520delay%2520itself%2520on%2520behalf%250Aof%2520machine%2520learning%2520practitioners%252C%2520our%2520goal%2520is%2520to%2520accelerate%2520generalization%2520of%250Aa%2520model%2520under%2520grokking%2520phenomenon.%2520By%2520regarding%2520a%2520series%2520of%2520gradients%2520of%2520a%250Aparameter%2520over%2520training%2520iterations%2520as%2520a%2520random%2520signal%2520over%2520time%252C%2520we%2520can%250Aspectrally%2520decompose%2520the%2520parameter%2520trajectories%2520under%2520gradient%2520descent%2520into%2520two%250Acomponents%253A%2520the%2520fast-varying%252C%2520overfitting-yielding%2520component%2520and%2520the%250Aslow-varying%252C%2520generalization-inducing%2520component.%2520This%2520analysis%2520allows%2520us%2520to%250Aaccelerate%2520the%2520grokking%2520phenomenon%2520more%2520than%2520%2524%255Ctimes%252050%2524%2520with%2520only%2520a%2520few%2520lines%250Aof%2520code%2520that%2520amplifies%2520the%2520slow-varying%2520components%2520of%2520gradients.%2520The%250Aexperiments%2520show%2520that%2520our%2520algorithm%2520applies%2520to%2520diverse%2520tasks%2520involving%2520images%252C%250Alanguages%252C%2520and%2520graphs%252C%2520enabling%2520practical%2520availability%2520of%2520this%2520peculiar%250Aartifact%2520of%2520sudden%2520generalization.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/ironjr/grokfast%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grokfast%3A%20Accelerated%20Grokking%20by%20Amplifying%20Slow%20Gradients&entry.906535625=Jaerin%20Lee%20and%20Bong%20Gyun%20Kang%20and%20Kihoon%20Kim%20and%20Kyoung%20Mu%20Lee&entry.1292438233=%20%20One%20puzzling%20artifact%20in%20machine%20learning%20dubbed%20grokking%20is%20where%20delayed%0Ageneralization%20is%20achieved%20tenfolds%20of%20iterations%20after%20near%20perfect%0Aoverfitting%20to%20the%20training%20data.%20Focusing%20on%20the%20long%20delay%20itself%20on%20behalf%0Aof%20machine%20learning%20practitioners%2C%20our%20goal%20is%20to%20accelerate%20generalization%20of%0Aa%20model%20under%20grokking%20phenomenon.%20By%20regarding%20a%20series%20of%20gradients%20of%20a%0Aparameter%20over%20training%20iterations%20as%20a%20random%20signal%20over%20time%2C%20we%20can%0Aspectrally%20decompose%20the%20parameter%20trajectories%20under%20gradient%20descent%20into%20two%0Acomponents%3A%20the%20fast-varying%2C%20overfitting-yielding%20component%20and%20the%0Aslow-varying%2C%20generalization-inducing%20component.%20This%20analysis%20allows%20us%20to%0Aaccelerate%20the%20grokking%20phenomenon%20more%20than%20%24%5Ctimes%2050%24%20with%20only%20a%20few%20lines%0Aof%20code%20that%20amplifies%20the%20slow-varying%20components%20of%20gradients.%20The%0Aexperiments%20show%20that%20our%20algorithm%20applies%20to%20diverse%20tasks%20involving%20images%2C%0Alanguages%2C%20and%20graphs%2C%20enabling%20practical%20availability%20of%20this%20peculiar%0Aartifact%20of%20sudden%20generalization.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ironjr/grokfast%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20233v1&entry.124074799=Read"},
{"title": "Unraveling the Impact of Heterophilic Structures on Graph\n  Positive-Unlabeled Learning", "author": "Yuhao Wu and Jiangchao Yao and Bo Han and Lina Yao and Tongliang Liu", "abstract": "  While Positive-Unlabeled (PU) learning is vital in many real-world scenarios,\nits application to graph data still remains under-explored. We unveil that a\ncritical challenge for PU learning on graph lies on the edge heterophily, which\ndirectly violates the irreducibility assumption for Class-Prior Estimation\n(class prior is essential for building PU learning algorithms) and degenerates\nthe latent label inference on unlabeled nodes during classifier training. In\nresponse to this challenge, we introduce a new method, named Graph PU Learning\nwith Label Propagation Loss (GPL). Specifically, GPL considers learning from PU\nnodes along with an intermediate heterophily reduction, which helps mitigate\nthe negative impact of the heterophilic structure. We formulate this procedure\nas a bilevel optimization that reduces heterophily in the inner loop and\nefficiently learns a classifier in the outer loop. Extensive experiments across\na variety of datasets have shown that GPL significantly outperforms baseline\nmethods, confirming its effectiveness and superiority.\n", "link": "http://arxiv.org/abs/2405.19919v1", "date": "2024-05-30", "relevancy": 2.5599, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5624}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20the%20Impact%20of%20Heterophilic%20Structures%20on%20Graph%0A%20%20Positive-Unlabeled%20Learning&body=Title%3A%20Unraveling%20the%20Impact%20of%20Heterophilic%20Structures%20on%20Graph%0A%20%20Positive-Unlabeled%20Learning%0AAuthor%3A%20Yuhao%20Wu%20and%20Jiangchao%20Yao%20and%20Bo%20Han%20and%20Lina%20Yao%20and%20Tongliang%20Liu%0AAbstract%3A%20%20%20While%20Positive-Unlabeled%20%28PU%29%20learning%20is%20vital%20in%20many%20real-world%20scenarios%2C%0Aits%20application%20to%20graph%20data%20still%20remains%20under-explored.%20We%20unveil%20that%20a%0Acritical%20challenge%20for%20PU%20learning%20on%20graph%20lies%20on%20the%20edge%20heterophily%2C%20which%0Adirectly%20violates%20the%20irreducibility%20assumption%20for%20Class-Prior%20Estimation%0A%28class%20prior%20is%20essential%20for%20building%20PU%20learning%20algorithms%29%20and%20degenerates%0Athe%20latent%20label%20inference%20on%20unlabeled%20nodes%20during%20classifier%20training.%20In%0Aresponse%20to%20this%20challenge%2C%20we%20introduce%20a%20new%20method%2C%20named%20Graph%20PU%20Learning%0Awith%20Label%20Propagation%20Loss%20%28GPL%29.%20Specifically%2C%20GPL%20considers%20learning%20from%20PU%0Anodes%20along%20with%20an%20intermediate%20heterophily%20reduction%2C%20which%20helps%20mitigate%0Athe%20negative%20impact%20of%20the%20heterophilic%20structure.%20We%20formulate%20this%20procedure%0Aas%20a%20bilevel%20optimization%20that%20reduces%20heterophily%20in%20the%20inner%20loop%20and%0Aefficiently%20learns%20a%20classifier%20in%20the%20outer%20loop.%20Extensive%20experiments%20across%0Aa%20variety%20of%20datasets%20have%20shown%20that%20GPL%20significantly%20outperforms%20baseline%0Amethods%2C%20confirming%20its%20effectiveness%20and%20superiority.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520the%2520Impact%2520of%2520Heterophilic%2520Structures%2520on%2520Graph%250A%2520%2520Positive-Unlabeled%2520Learning%26entry.906535625%3DYuhao%2520Wu%2520and%2520Jiangchao%2520Yao%2520and%2520Bo%2520Han%2520and%2520Lina%2520Yao%2520and%2520Tongliang%2520Liu%26entry.1292438233%3D%2520%2520While%2520Positive-Unlabeled%2520%2528PU%2529%2520learning%2520is%2520vital%2520in%2520many%2520real-world%2520scenarios%252C%250Aits%2520application%2520to%2520graph%2520data%2520still%2520remains%2520under-explored.%2520We%2520unveil%2520that%2520a%250Acritical%2520challenge%2520for%2520PU%2520learning%2520on%2520graph%2520lies%2520on%2520the%2520edge%2520heterophily%252C%2520which%250Adirectly%2520violates%2520the%2520irreducibility%2520assumption%2520for%2520Class-Prior%2520Estimation%250A%2528class%2520prior%2520is%2520essential%2520for%2520building%2520PU%2520learning%2520algorithms%2529%2520and%2520degenerates%250Athe%2520latent%2520label%2520inference%2520on%2520unlabeled%2520nodes%2520during%2520classifier%2520training.%2520In%250Aresponse%2520to%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520new%2520method%252C%2520named%2520Graph%2520PU%2520Learning%250Awith%2520Label%2520Propagation%2520Loss%2520%2528GPL%2529.%2520Specifically%252C%2520GPL%2520considers%2520learning%2520from%2520PU%250Anodes%2520along%2520with%2520an%2520intermediate%2520heterophily%2520reduction%252C%2520which%2520helps%2520mitigate%250Athe%2520negative%2520impact%2520of%2520the%2520heterophilic%2520structure.%2520We%2520formulate%2520this%2520procedure%250Aas%2520a%2520bilevel%2520optimization%2520that%2520reduces%2520heterophily%2520in%2520the%2520inner%2520loop%2520and%250Aefficiently%2520learns%2520a%2520classifier%2520in%2520the%2520outer%2520loop.%2520Extensive%2520experiments%2520across%250Aa%2520variety%2520of%2520datasets%2520have%2520shown%2520that%2520GPL%2520significantly%2520outperforms%2520baseline%250Amethods%252C%2520confirming%2520its%2520effectiveness%2520and%2520superiority.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20the%20Impact%20of%20Heterophilic%20Structures%20on%20Graph%0A%20%20Positive-Unlabeled%20Learning&entry.906535625=Yuhao%20Wu%20and%20Jiangchao%20Yao%20and%20Bo%20Han%20and%20Lina%20Yao%20and%20Tongliang%20Liu&entry.1292438233=%20%20While%20Positive-Unlabeled%20%28PU%29%20learning%20is%20vital%20in%20many%20real-world%20scenarios%2C%0Aits%20application%20to%20graph%20data%20still%20remains%20under-explored.%20We%20unveil%20that%20a%0Acritical%20challenge%20for%20PU%20learning%20on%20graph%20lies%20on%20the%20edge%20heterophily%2C%20which%0Adirectly%20violates%20the%20irreducibility%20assumption%20for%20Class-Prior%20Estimation%0A%28class%20prior%20is%20essential%20for%20building%20PU%20learning%20algorithms%29%20and%20degenerates%0Athe%20latent%20label%20inference%20on%20unlabeled%20nodes%20during%20classifier%20training.%20In%0Aresponse%20to%20this%20challenge%2C%20we%20introduce%20a%20new%20method%2C%20named%20Graph%20PU%20Learning%0Awith%20Label%20Propagation%20Loss%20%28GPL%29.%20Specifically%2C%20GPL%20considers%20learning%20from%20PU%0Anodes%20along%20with%20an%20intermediate%20heterophily%20reduction%2C%20which%20helps%20mitigate%0Athe%20negative%20impact%20of%20the%20heterophilic%20structure.%20We%20formulate%20this%20procedure%0Aas%20a%20bilevel%20optimization%20that%20reduces%20heterophily%20in%20the%20inner%20loop%20and%0Aefficiently%20learns%20a%20classifier%20in%20the%20outer%20loop.%20Extensive%20experiments%20across%0Aa%20variety%20of%20datasets%20have%20shown%20that%20GPL%20significantly%20outperforms%20baseline%0Amethods%2C%20confirming%20its%20effectiveness%20and%20superiority.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19919v1&entry.124074799=Read"},
{"title": "FlexiDrop: Theoretical Insights and Practical Advances in Random Dropout\n  Method on GNNs", "author": "Zhiheng Zhou and Sihao Liu and Weichen Zhao", "abstract": "  Graph Neural Networks (GNNs) are powerful tools for handling graph-type data.\nRecently, GNNs have been widely applied in various domains, but they also face\nsome issues, such as overfitting, over-smoothing and non-robustness. The\nexisting research indicates that random dropout methods are an effective way to\naddress these issues. However, random dropout methods in GNNs still face\nunresolved problems. Currently, the choice of dropout rate, often determined by\nheuristic or grid search methods, can increase the generalization error,\ncontradicting the principal aims of dropout. In this paper, we propose a novel\nrandom dropout method for GNNs called FlexiDrop. First, we conduct a\ntheoretical analysis of dropout in GNNs using rademacher complexity and\ndemonstrate that the generalization error of traditional random dropout methods\nis constrained by a function related to the dropout rate. Subsequently, we use\nthis function as a regularizer to unify the dropout rate and empirical loss\nwithin a single loss function, optimizing them simultaneously. Therefore, our\nmethod enables adaptive adjustment of the dropout rate and theoretically\nbalances the trade-off between model complexity and generalization ability.\nFurthermore, extensive experimental results on benchmark datasets show that\nFlexiDrop outperforms traditional random dropout methods in GNNs.\n", "link": "http://arxiv.org/abs/2405.20012v1", "date": "2024-05-30", "relevancy": 2.5467, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5547}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4994}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexiDrop%3A%20Theoretical%20Insights%20and%20Practical%20Advances%20in%20Random%20Dropout%0A%20%20Method%20on%20GNNs&body=Title%3A%20FlexiDrop%3A%20Theoretical%20Insights%20and%20Practical%20Advances%20in%20Random%20Dropout%0A%20%20Method%20on%20GNNs%0AAuthor%3A%20Zhiheng%20Zhou%20and%20Sihao%20Liu%20and%20Weichen%20Zhao%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20tools%20for%20handling%20graph-type%20data.%0ARecently%2C%20GNNs%20have%20been%20widely%20applied%20in%20various%20domains%2C%20but%20they%20also%20face%0Asome%20issues%2C%20such%20as%20overfitting%2C%20over-smoothing%20and%20non-robustness.%20The%0Aexisting%20research%20indicates%20that%20random%20dropout%20methods%20are%20an%20effective%20way%20to%0Aaddress%20these%20issues.%20However%2C%20random%20dropout%20methods%20in%20GNNs%20still%20face%0Aunresolved%20problems.%20Currently%2C%20the%20choice%20of%20dropout%20rate%2C%20often%20determined%20by%0Aheuristic%20or%20grid%20search%20methods%2C%20can%20increase%20the%20generalization%20error%2C%0Acontradicting%20the%20principal%20aims%20of%20dropout.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Arandom%20dropout%20method%20for%20GNNs%20called%20FlexiDrop.%20First%2C%20we%20conduct%20a%0Atheoretical%20analysis%20of%20dropout%20in%20GNNs%20using%20rademacher%20complexity%20and%0Ademonstrate%20that%20the%20generalization%20error%20of%20traditional%20random%20dropout%20methods%0Ais%20constrained%20by%20a%20function%20related%20to%20the%20dropout%20rate.%20Subsequently%2C%20we%20use%0Athis%20function%20as%20a%20regularizer%20to%20unify%20the%20dropout%20rate%20and%20empirical%20loss%0Awithin%20a%20single%20loss%20function%2C%20optimizing%20them%20simultaneously.%20Therefore%2C%20our%0Amethod%20enables%20adaptive%20adjustment%20of%20the%20dropout%20rate%20and%20theoretically%0Abalances%20the%20trade-off%20between%20model%20complexity%20and%20generalization%20ability.%0AFurthermore%2C%20extensive%20experimental%20results%20on%20benchmark%20datasets%20show%20that%0AFlexiDrop%20outperforms%20traditional%20random%20dropout%20methods%20in%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexiDrop%253A%2520Theoretical%2520Insights%2520and%2520Practical%2520Advances%2520in%2520Random%2520Dropout%250A%2520%2520Method%2520on%2520GNNs%26entry.906535625%3DZhiheng%2520Zhou%2520and%2520Sihao%2520Liu%2520and%2520Weichen%2520Zhao%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520powerful%2520tools%2520for%2520handling%2520graph-type%2520data.%250ARecently%252C%2520GNNs%2520have%2520been%2520widely%2520applied%2520in%2520various%2520domains%252C%2520but%2520they%2520also%2520face%250Asome%2520issues%252C%2520such%2520as%2520overfitting%252C%2520over-smoothing%2520and%2520non-robustness.%2520The%250Aexisting%2520research%2520indicates%2520that%2520random%2520dropout%2520methods%2520are%2520an%2520effective%2520way%2520to%250Aaddress%2520these%2520issues.%2520However%252C%2520random%2520dropout%2520methods%2520in%2520GNNs%2520still%2520face%250Aunresolved%2520problems.%2520Currently%252C%2520the%2520choice%2520of%2520dropout%2520rate%252C%2520often%2520determined%2520by%250Aheuristic%2520or%2520grid%2520search%2520methods%252C%2520can%2520increase%2520the%2520generalization%2520error%252C%250Acontradicting%2520the%2520principal%2520aims%2520of%2520dropout.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Arandom%2520dropout%2520method%2520for%2520GNNs%2520called%2520FlexiDrop.%2520First%252C%2520we%2520conduct%2520a%250Atheoretical%2520analysis%2520of%2520dropout%2520in%2520GNNs%2520using%2520rademacher%2520complexity%2520and%250Ademonstrate%2520that%2520the%2520generalization%2520error%2520of%2520traditional%2520random%2520dropout%2520methods%250Ais%2520constrained%2520by%2520a%2520function%2520related%2520to%2520the%2520dropout%2520rate.%2520Subsequently%252C%2520we%2520use%250Athis%2520function%2520as%2520a%2520regularizer%2520to%2520unify%2520the%2520dropout%2520rate%2520and%2520empirical%2520loss%250Awithin%2520a%2520single%2520loss%2520function%252C%2520optimizing%2520them%2520simultaneously.%2520Therefore%252C%2520our%250Amethod%2520enables%2520adaptive%2520adjustment%2520of%2520the%2520dropout%2520rate%2520and%2520theoretically%250Abalances%2520the%2520trade-off%2520between%2520model%2520complexity%2520and%2520generalization%2520ability.%250AFurthermore%252C%2520extensive%2520experimental%2520results%2520on%2520benchmark%2520datasets%2520show%2520that%250AFlexiDrop%2520outperforms%2520traditional%2520random%2520dropout%2520methods%2520in%2520GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiDrop%3A%20Theoretical%20Insights%20and%20Practical%20Advances%20in%20Random%20Dropout%0A%20%20Method%20on%20GNNs&entry.906535625=Zhiheng%20Zhou%20and%20Sihao%20Liu%20and%20Weichen%20Zhao&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20tools%20for%20handling%20graph-type%20data.%0ARecently%2C%20GNNs%20have%20been%20widely%20applied%20in%20various%20domains%2C%20but%20they%20also%20face%0Asome%20issues%2C%20such%20as%20overfitting%2C%20over-smoothing%20and%20non-robustness.%20The%0Aexisting%20research%20indicates%20that%20random%20dropout%20methods%20are%20an%20effective%20way%20to%0Aaddress%20these%20issues.%20However%2C%20random%20dropout%20methods%20in%20GNNs%20still%20face%0Aunresolved%20problems.%20Currently%2C%20the%20choice%20of%20dropout%20rate%2C%20often%20determined%20by%0Aheuristic%20or%20grid%20search%20methods%2C%20can%20increase%20the%20generalization%20error%2C%0Acontradicting%20the%20principal%20aims%20of%20dropout.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Arandom%20dropout%20method%20for%20GNNs%20called%20FlexiDrop.%20First%2C%20we%20conduct%20a%0Atheoretical%20analysis%20of%20dropout%20in%20GNNs%20using%20rademacher%20complexity%20and%0Ademonstrate%20that%20the%20generalization%20error%20of%20traditional%20random%20dropout%20methods%0Ais%20constrained%20by%20a%20function%20related%20to%20the%20dropout%20rate.%20Subsequently%2C%20we%20use%0Athis%20function%20as%20a%20regularizer%20to%20unify%20the%20dropout%20rate%20and%20empirical%20loss%0Awithin%20a%20single%20loss%20function%2C%20optimizing%20them%20simultaneously.%20Therefore%2C%20our%0Amethod%20enables%20adaptive%20adjustment%20of%20the%20dropout%20rate%20and%20theoretically%0Abalances%20the%20trade-off%20between%20model%20complexity%20and%20generalization%20ability.%0AFurthermore%2C%20extensive%20experimental%20results%20on%20benchmark%20datasets%20show%20that%0AFlexiDrop%20outperforms%20traditional%20random%20dropout%20methods%20in%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20012v1&entry.124074799=Read"},
{"title": "MotionDreamer: Zero-Shot 3D Mesh Animation from Video Diffusion Models", "author": "Lukas Uzolas and Elmar Eisemann and Petr Kellnhofer", "abstract": "  Animation techniques bring digital 3D worlds and characters to life. However,\nmanual animation is tedious and automated techniques are often specialized to\nnarrow shape classes. In our work, we propose a technique for automatic\nre-animation of arbitrary 3D shapes based on a motion prior extracted from a\nvideo diffusion model. Unlike existing 4D generation methods, we focus solely\non the motion, and we leverage an explicit mesh-based representation compatible\nwith existing computer-graphics pipelines. Furthermore, our utilization of\ndiffusion features enhances accuracy of our motion fitting. We analyze efficacy\nof these features for animation fitting and we experimentally validate our\napproach for two different diffusion models and four animation models. Finally,\nwe demonstrate that our time-efficient zero-shot method achieves a superior\nperformance re-animating a diverse set of 3D shapes when compared to existing\ntechniques in a user study. The project website is located at\nhttps://lukas.uzolas.com/MotionDreamer.\n", "link": "http://arxiv.org/abs/2405.20155v1", "date": "2024-05-30", "relevancy": 2.5368, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6442}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6384}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionDreamer%3A%20Zero-Shot%203D%20Mesh%20Animation%20from%20Video%20Diffusion%20Models&body=Title%3A%20MotionDreamer%3A%20Zero-Shot%203D%20Mesh%20Animation%20from%20Video%20Diffusion%20Models%0AAuthor%3A%20Lukas%20Uzolas%20and%20Elmar%20Eisemann%20and%20Petr%20Kellnhofer%0AAbstract%3A%20%20%20Animation%20techniques%20bring%20digital%203D%20worlds%20and%20characters%20to%20life.%20However%2C%0Amanual%20animation%20is%20tedious%20and%20automated%20techniques%20are%20often%20specialized%20to%0Anarrow%20shape%20classes.%20In%20our%20work%2C%20we%20propose%20a%20technique%20for%20automatic%0Are-animation%20of%20arbitrary%203D%20shapes%20based%20on%20a%20motion%20prior%20extracted%20from%20a%0Avideo%20diffusion%20model.%20Unlike%20existing%204D%20generation%20methods%2C%20we%20focus%20solely%0Aon%20the%20motion%2C%20and%20we%20leverage%20an%20explicit%20mesh-based%20representation%20compatible%0Awith%20existing%20computer-graphics%20pipelines.%20Furthermore%2C%20our%20utilization%20of%0Adiffusion%20features%20enhances%20accuracy%20of%20our%20motion%20fitting.%20We%20analyze%20efficacy%0Aof%20these%20features%20for%20animation%20fitting%20and%20we%20experimentally%20validate%20our%0Aapproach%20for%20two%20different%20diffusion%20models%20and%20four%20animation%20models.%20Finally%2C%0Awe%20demonstrate%20that%20our%20time-efficient%20zero-shot%20method%20achieves%20a%20superior%0Aperformance%20re-animating%20a%20diverse%20set%20of%203D%20shapes%20when%20compared%20to%20existing%0Atechniques%20in%20a%20user%20study.%20The%20project%20website%20is%20located%20at%0Ahttps%3A//lukas.uzolas.com/MotionDreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionDreamer%253A%2520Zero-Shot%25203D%2520Mesh%2520Animation%2520from%2520Video%2520Diffusion%2520Models%26entry.906535625%3DLukas%2520Uzolas%2520and%2520Elmar%2520Eisemann%2520and%2520Petr%2520Kellnhofer%26entry.1292438233%3D%2520%2520Animation%2520techniques%2520bring%2520digital%25203D%2520worlds%2520and%2520characters%2520to%2520life.%2520However%252C%250Amanual%2520animation%2520is%2520tedious%2520and%2520automated%2520techniques%2520are%2520often%2520specialized%2520to%250Anarrow%2520shape%2520classes.%2520In%2520our%2520work%252C%2520we%2520propose%2520a%2520technique%2520for%2520automatic%250Are-animation%2520of%2520arbitrary%25203D%2520shapes%2520based%2520on%2520a%2520motion%2520prior%2520extracted%2520from%2520a%250Avideo%2520diffusion%2520model.%2520Unlike%2520existing%25204D%2520generation%2520methods%252C%2520we%2520focus%2520solely%250Aon%2520the%2520motion%252C%2520and%2520we%2520leverage%2520an%2520explicit%2520mesh-based%2520representation%2520compatible%250Awith%2520existing%2520computer-graphics%2520pipelines.%2520Furthermore%252C%2520our%2520utilization%2520of%250Adiffusion%2520features%2520enhances%2520accuracy%2520of%2520our%2520motion%2520fitting.%2520We%2520analyze%2520efficacy%250Aof%2520these%2520features%2520for%2520animation%2520fitting%2520and%2520we%2520experimentally%2520validate%2520our%250Aapproach%2520for%2520two%2520different%2520diffusion%2520models%2520and%2520four%2520animation%2520models.%2520Finally%252C%250Awe%2520demonstrate%2520that%2520our%2520time-efficient%2520zero-shot%2520method%2520achieves%2520a%2520superior%250Aperformance%2520re-animating%2520a%2520diverse%2520set%2520of%25203D%2520shapes%2520when%2520compared%2520to%2520existing%250Atechniques%2520in%2520a%2520user%2520study.%2520The%2520project%2520website%2520is%2520located%2520at%250Ahttps%253A//lukas.uzolas.com/MotionDreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionDreamer%3A%20Zero-Shot%203D%20Mesh%20Animation%20from%20Video%20Diffusion%20Models&entry.906535625=Lukas%20Uzolas%20and%20Elmar%20Eisemann%20and%20Petr%20Kellnhofer&entry.1292438233=%20%20Animation%20techniques%20bring%20digital%203D%20worlds%20and%20characters%20to%20life.%20However%2C%0Amanual%20animation%20is%20tedious%20and%20automated%20techniques%20are%20often%20specialized%20to%0Anarrow%20shape%20classes.%20In%20our%20work%2C%20we%20propose%20a%20technique%20for%20automatic%0Are-animation%20of%20arbitrary%203D%20shapes%20based%20on%20a%20motion%20prior%20extracted%20from%20a%0Avideo%20diffusion%20model.%20Unlike%20existing%204D%20generation%20methods%2C%20we%20focus%20solely%0Aon%20the%20motion%2C%20and%20we%20leverage%20an%20explicit%20mesh-based%20representation%20compatible%0Awith%20existing%20computer-graphics%20pipelines.%20Furthermore%2C%20our%20utilization%20of%0Adiffusion%20features%20enhances%20accuracy%20of%20our%20motion%20fitting.%20We%20analyze%20efficacy%0Aof%20these%20features%20for%20animation%20fitting%20and%20we%20experimentally%20validate%20our%0Aapproach%20for%20two%20different%20diffusion%20models%20and%20four%20animation%20models.%20Finally%2C%0Awe%20demonstrate%20that%20our%20time-efficient%20zero-shot%20method%20achieves%20a%20superior%0Aperformance%20re-animating%20a%20diverse%20set%20of%203D%20shapes%20when%20compared%20to%20existing%0Atechniques%20in%20a%20user%20study.%20The%20project%20website%20is%20located%20at%0Ahttps%3A//lukas.uzolas.com/MotionDreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20155v1&entry.124074799=Read"},
{"title": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting", "author": "Qiaowei Miao and Yawei Luo and Yi Yang", "abstract": "  As text-conditioned diffusion models (DMs) achieve breakthroughs in image,\nvideo, and 3D generation, the research community's focus has shifted to the\nmore challenging task of text-to-4D synthesis, which introduces a temporal\ndimension to generate dynamic 3D objects. In this context, we identify Score\nDistillation Sampling (SDS), a widely used technique for text-to-3D synthesis,\nas a significant hindrance to text-to-4D performance due to its Janus-faced and\ntexture-unrealistic problems coupled with high computational costs. In this\npaper, we propose \\textbf{P}ixel-\\textbf{L}evel \\textbf{A}lignments for\nText-to-\\textbf{4D} Gaussian Splatting (\\textbf{PLA4D}), a novel method that\nutilizes text-to-video frames as explicit pixel alignment targets to generate\nstatic 3D objects and inject motion into them. Specifically, we introduce Focal\nAlignment to calibrate camera poses for rendering and GS-Mesh Contrastive\nLearning to distill geometry priors from rendered image contrasts at the pixel\nlevel. Additionally, we develop Motion Alignment using a deformation network to\ndrive changes in Gaussians and implement Reference Refinement for smooth 4D\nobject surfaces. These techniques enable 4D Gaussian Splatting to align\ngeometry, texture, and motion with generated videos at the pixel level.\nCompared to previous methods, PLA4D produces synthesized outputs with better\ntexture details in less time and effectively mitigates the Janus-faced problem.\nPLA4D is fully implemented using open-source models, offering an accessible,\nuser-friendly, and promising direction for 4D digital content creation. Our\nproject page:\n\\href{https://github.com/MiaoQiaowei/PLA4D.github.io}{https://github.com/MiaoQiaowei/PLA4D.github.io}.\n", "link": "http://arxiv.org/abs/2405.19957v1", "date": "2024-05-30", "relevancy": 2.5156, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6427}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6318}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLA4D%3A%20Pixel-Level%20Alignments%20for%20Text-to-4D%20Gaussian%20Splatting&body=Title%3A%20PLA4D%3A%20Pixel-Level%20Alignments%20for%20Text-to-4D%20Gaussian%20Splatting%0AAuthor%3A%20Qiaowei%20Miao%20and%20Yawei%20Luo%20and%20Yi%20Yang%0AAbstract%3A%20%20%20As%20text-conditioned%20diffusion%20models%20%28DMs%29%20achieve%20breakthroughs%20in%20image%2C%0Avideo%2C%20and%203D%20generation%2C%20the%20research%20community%27s%20focus%20has%20shifted%20to%20the%0Amore%20challenging%20task%20of%20text-to-4D%20synthesis%2C%20which%20introduces%20a%20temporal%0Adimension%20to%20generate%20dynamic%203D%20objects.%20In%20this%20context%2C%20we%20identify%20Score%0ADistillation%20Sampling%20%28SDS%29%2C%20a%20widely%20used%20technique%20for%20text-to-3D%20synthesis%2C%0Aas%20a%20significant%20hindrance%20to%20text-to-4D%20performance%20due%20to%20its%20Janus-faced%20and%0Atexture-unrealistic%20problems%20coupled%20with%20high%20computational%20costs.%20In%20this%0Apaper%2C%20we%20propose%20%5Ctextbf%7BP%7Dixel-%5Ctextbf%7BL%7Devel%20%5Ctextbf%7BA%7Dlignments%20for%0AText-to-%5Ctextbf%7B4D%7D%20Gaussian%20Splatting%20%28%5Ctextbf%7BPLA4D%7D%29%2C%20a%20novel%20method%20that%0Autilizes%20text-to-video%20frames%20as%20explicit%20pixel%20alignment%20targets%20to%20generate%0Astatic%203D%20objects%20and%20inject%20motion%20into%20them.%20Specifically%2C%20we%20introduce%20Focal%0AAlignment%20to%20calibrate%20camera%20poses%20for%20rendering%20and%20GS-Mesh%20Contrastive%0ALearning%20to%20distill%20geometry%20priors%20from%20rendered%20image%20contrasts%20at%20the%20pixel%0Alevel.%20Additionally%2C%20we%20develop%20Motion%20Alignment%20using%20a%20deformation%20network%20to%0Adrive%20changes%20in%20Gaussians%20and%20implement%20Reference%20Refinement%20for%20smooth%204D%0Aobject%20surfaces.%20These%20techniques%20enable%204D%20Gaussian%20Splatting%20to%20align%0Ageometry%2C%20texture%2C%20and%20motion%20with%20generated%20videos%20at%20the%20pixel%20level.%0ACompared%20to%20previous%20methods%2C%20PLA4D%20produces%20synthesized%20outputs%20with%20better%0Atexture%20details%20in%20less%20time%20and%20effectively%20mitigates%20the%20Janus-faced%20problem.%0APLA4D%20is%20fully%20implemented%20using%20open-source%20models%2C%20offering%20an%20accessible%2C%0Auser-friendly%2C%20and%20promising%20direction%20for%204D%20digital%20content%20creation.%20Our%0Aproject%20page%3A%0A%5Chref%7Bhttps%3A//github.com/MiaoQiaowei/PLA4D.github.io%7D%7Bhttps%3A//github.com/MiaoQiaowei/PLA4D.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLA4D%253A%2520Pixel-Level%2520Alignments%2520for%2520Text-to-4D%2520Gaussian%2520Splatting%26entry.906535625%3DQiaowei%2520Miao%2520and%2520Yawei%2520Luo%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520As%2520text-conditioned%2520diffusion%2520models%2520%2528DMs%2529%2520achieve%2520breakthroughs%2520in%2520image%252C%250Avideo%252C%2520and%25203D%2520generation%252C%2520the%2520research%2520community%2527s%2520focus%2520has%2520shifted%2520to%2520the%250Amore%2520challenging%2520task%2520of%2520text-to-4D%2520synthesis%252C%2520which%2520introduces%2520a%2520temporal%250Adimension%2520to%2520generate%2520dynamic%25203D%2520objects.%2520In%2520this%2520context%252C%2520we%2520identify%2520Score%250ADistillation%2520Sampling%2520%2528SDS%2529%252C%2520a%2520widely%2520used%2520technique%2520for%2520text-to-3D%2520synthesis%252C%250Aas%2520a%2520significant%2520hindrance%2520to%2520text-to-4D%2520performance%2520due%2520to%2520its%2520Janus-faced%2520and%250Atexture-unrealistic%2520problems%2520coupled%2520with%2520high%2520computational%2520costs.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520%255Ctextbf%257BP%257Dixel-%255Ctextbf%257BL%257Devel%2520%255Ctextbf%257BA%257Dlignments%2520for%250AText-to-%255Ctextbf%257B4D%257D%2520Gaussian%2520Splatting%2520%2528%255Ctextbf%257BPLA4D%257D%2529%252C%2520a%2520novel%2520method%2520that%250Autilizes%2520text-to-video%2520frames%2520as%2520explicit%2520pixel%2520alignment%2520targets%2520to%2520generate%250Astatic%25203D%2520objects%2520and%2520inject%2520motion%2520into%2520them.%2520Specifically%252C%2520we%2520introduce%2520Focal%250AAlignment%2520to%2520calibrate%2520camera%2520poses%2520for%2520rendering%2520and%2520GS-Mesh%2520Contrastive%250ALearning%2520to%2520distill%2520geometry%2520priors%2520from%2520rendered%2520image%2520contrasts%2520at%2520the%2520pixel%250Alevel.%2520Additionally%252C%2520we%2520develop%2520Motion%2520Alignment%2520using%2520a%2520deformation%2520network%2520to%250Adrive%2520changes%2520in%2520Gaussians%2520and%2520implement%2520Reference%2520Refinement%2520for%2520smooth%25204D%250Aobject%2520surfaces.%2520These%2520techniques%2520enable%25204D%2520Gaussian%2520Splatting%2520to%2520align%250Ageometry%252C%2520texture%252C%2520and%2520motion%2520with%2520generated%2520videos%2520at%2520the%2520pixel%2520level.%250ACompared%2520to%2520previous%2520methods%252C%2520PLA4D%2520produces%2520synthesized%2520outputs%2520with%2520better%250Atexture%2520details%2520in%2520less%2520time%2520and%2520effectively%2520mitigates%2520the%2520Janus-faced%2520problem.%250APLA4D%2520is%2520fully%2520implemented%2520using%2520open-source%2520models%252C%2520offering%2520an%2520accessible%252C%250Auser-friendly%252C%2520and%2520promising%2520direction%2520for%25204D%2520digital%2520content%2520creation.%2520Our%250Aproject%2520page%253A%250A%255Chref%257Bhttps%253A//github.com/MiaoQiaowei/PLA4D.github.io%257D%257Bhttps%253A//github.com/MiaoQiaowei/PLA4D.github.io%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLA4D%3A%20Pixel-Level%20Alignments%20for%20Text-to-4D%20Gaussian%20Splatting&entry.906535625=Qiaowei%20Miao%20and%20Yawei%20Luo%20and%20Yi%20Yang&entry.1292438233=%20%20As%20text-conditioned%20diffusion%20models%20%28DMs%29%20achieve%20breakthroughs%20in%20image%2C%0Avideo%2C%20and%203D%20generation%2C%20the%20research%20community%27s%20focus%20has%20shifted%20to%20the%0Amore%20challenging%20task%20of%20text-to-4D%20synthesis%2C%20which%20introduces%20a%20temporal%0Adimension%20to%20generate%20dynamic%203D%20objects.%20In%20this%20context%2C%20we%20identify%20Score%0ADistillation%20Sampling%20%28SDS%29%2C%20a%20widely%20used%20technique%20for%20text-to-3D%20synthesis%2C%0Aas%20a%20significant%20hindrance%20to%20text-to-4D%20performance%20due%20to%20its%20Janus-faced%20and%0Atexture-unrealistic%20problems%20coupled%20with%20high%20computational%20costs.%20In%20this%0Apaper%2C%20we%20propose%20%5Ctextbf%7BP%7Dixel-%5Ctextbf%7BL%7Devel%20%5Ctextbf%7BA%7Dlignments%20for%0AText-to-%5Ctextbf%7B4D%7D%20Gaussian%20Splatting%20%28%5Ctextbf%7BPLA4D%7D%29%2C%20a%20novel%20method%20that%0Autilizes%20text-to-video%20frames%20as%20explicit%20pixel%20alignment%20targets%20to%20generate%0Astatic%203D%20objects%20and%20inject%20motion%20into%20them.%20Specifically%2C%20we%20introduce%20Focal%0AAlignment%20to%20calibrate%20camera%20poses%20for%20rendering%20and%20GS-Mesh%20Contrastive%0ALearning%20to%20distill%20geometry%20priors%20from%20rendered%20image%20contrasts%20at%20the%20pixel%0Alevel.%20Additionally%2C%20we%20develop%20Motion%20Alignment%20using%20a%20deformation%20network%20to%0Adrive%20changes%20in%20Gaussians%20and%20implement%20Reference%20Refinement%20for%20smooth%204D%0Aobject%20surfaces.%20These%20techniques%20enable%204D%20Gaussian%20Splatting%20to%20align%0Ageometry%2C%20texture%2C%20and%20motion%20with%20generated%20videos%20at%20the%20pixel%20level.%0ACompared%20to%20previous%20methods%2C%20PLA4D%20produces%20synthesized%20outputs%20with%20better%0Atexture%20details%20in%20less%20time%20and%20effectively%20mitigates%20the%20Janus-faced%20problem.%0APLA4D%20is%20fully%20implemented%20using%20open-source%20models%2C%20offering%20an%20accessible%2C%0Auser-friendly%2C%20and%20promising%20direction%20for%204D%20digital%20content%20creation.%20Our%0Aproject%20page%3A%0A%5Chref%7Bhttps%3A//github.com/MiaoQiaowei/PLA4D.github.io%7D%7Bhttps%3A//github.com/MiaoQiaowei/PLA4D.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19957v1&entry.124074799=Read"},
{"title": "Segment, Shuffle, and Stitch: A Simple Mechanism for Improving\n  Time-Series Representations", "author": "Shivam Grover and Amin Jalali and Ali Etemad", "abstract": "  Existing approaches for learning representations of time-series keep the\ntemporal arrangement of the time-steps intact with the presumption that the\noriginal order is the most optimal for learning. However, non-adjacent sections\nof real-world time-series may have strong dependencies. Accordingly we raise\nthe question: Is there an alternative arrangement for time-series which could\nenable more effective representation learning? To address this, we propose a\nsimple plug-and-play mechanism called Segment, Shuffle, and Stitch (S3)\ndesigned to improve time-series representation learning of existing models. S3\nworks by creating non-overlapping segments from the original sequence and\nshuffling them in a learned manner that is the most optimal for the task at\nhand. It then re-attaches the shuffled segments back together and performs a\nlearned weighted sum with the original input to capture both the newly shuffled\nsequence along with the original sequence. S3 is modular and can be stacked to\ncreate various degrees of granularity, and can be added to many forms of neural\narchitectures including CNNs or Transformers with negligible computation\noverhead. Through extensive experiments on several datasets and\nstate-of-the-art baselines, we show that incorporating S3 results in\nsignificant improvements for the tasks of time-series classification and\nforecasting, improving performance on certain datasets by up to 68\\%. We also\nshow that S3 makes the learning more stable with a smoother training loss curve\nand loss landscape compared to the original baseline. The code is available at\nhttps://github.com/shivam-grover/S3-TimeSeries .\n", "link": "http://arxiv.org/abs/2405.20082v1", "date": "2024-05-30", "relevancy": 2.5102, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4899}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%2C%20Shuffle%2C%20and%20Stitch%3A%20A%20Simple%20Mechanism%20for%20Improving%0A%20%20Time-Series%20Representations&body=Title%3A%20Segment%2C%20Shuffle%2C%20and%20Stitch%3A%20A%20Simple%20Mechanism%20for%20Improving%0A%20%20Time-Series%20Representations%0AAuthor%3A%20Shivam%20Grover%20and%20Amin%20Jalali%20and%20Ali%20Etemad%0AAbstract%3A%20%20%20Existing%20approaches%20for%20learning%20representations%20of%20time-series%20keep%20the%0Atemporal%20arrangement%20of%20the%20time-steps%20intact%20with%20the%20presumption%20that%20the%0Aoriginal%20order%20is%20the%20most%20optimal%20for%20learning.%20However%2C%20non-adjacent%20sections%0Aof%20real-world%20time-series%20may%20have%20strong%20dependencies.%20Accordingly%20we%20raise%0Athe%20question%3A%20Is%20there%20an%20alternative%20arrangement%20for%20time-series%20which%20could%0Aenable%20more%20effective%20representation%20learning%3F%20To%20address%20this%2C%20we%20propose%20a%0Asimple%20plug-and-play%20mechanism%20called%20Segment%2C%20Shuffle%2C%20and%20Stitch%20%28S3%29%0Adesigned%20to%20improve%20time-series%20representation%20learning%20of%20existing%20models.%20S3%0Aworks%20by%20creating%20non-overlapping%20segments%20from%20the%20original%20sequence%20and%0Ashuffling%20them%20in%20a%20learned%20manner%20that%20is%20the%20most%20optimal%20for%20the%20task%20at%0Ahand.%20It%20then%20re-attaches%20the%20shuffled%20segments%20back%20together%20and%20performs%20a%0Alearned%20weighted%20sum%20with%20the%20original%20input%20to%20capture%20both%20the%20newly%20shuffled%0Asequence%20along%20with%20the%20original%20sequence.%20S3%20is%20modular%20and%20can%20be%20stacked%20to%0Acreate%20various%20degrees%20of%20granularity%2C%20and%20can%20be%20added%20to%20many%20forms%20of%20neural%0Aarchitectures%20including%20CNNs%20or%20Transformers%20with%20negligible%20computation%0Aoverhead.%20Through%20extensive%20experiments%20on%20several%20datasets%20and%0Astate-of-the-art%20baselines%2C%20we%20show%20that%20incorporating%20S3%20results%20in%0Asignificant%20improvements%20for%20the%20tasks%20of%20time-series%20classification%20and%0Aforecasting%2C%20improving%20performance%20on%20certain%20datasets%20by%20up%20to%2068%5C%25.%20We%20also%0Ashow%20that%20S3%20makes%20the%20learning%20more%20stable%20with%20a%20smoother%20training%20loss%20curve%0Aand%20loss%20landscape%20compared%20to%20the%20original%20baseline.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shivam-grover/S3-TimeSeries%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%252C%2520Shuffle%252C%2520and%2520Stitch%253A%2520A%2520Simple%2520Mechanism%2520for%2520Improving%250A%2520%2520Time-Series%2520Representations%26entry.906535625%3DShivam%2520Grover%2520and%2520Amin%2520Jalali%2520and%2520Ali%2520Etemad%26entry.1292438233%3D%2520%2520Existing%2520approaches%2520for%2520learning%2520representations%2520of%2520time-series%2520keep%2520the%250Atemporal%2520arrangement%2520of%2520the%2520time-steps%2520intact%2520with%2520the%2520presumption%2520that%2520the%250Aoriginal%2520order%2520is%2520the%2520most%2520optimal%2520for%2520learning.%2520However%252C%2520non-adjacent%2520sections%250Aof%2520real-world%2520time-series%2520may%2520have%2520strong%2520dependencies.%2520Accordingly%2520we%2520raise%250Athe%2520question%253A%2520Is%2520there%2520an%2520alternative%2520arrangement%2520for%2520time-series%2520which%2520could%250Aenable%2520more%2520effective%2520representation%2520learning%253F%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Asimple%2520plug-and-play%2520mechanism%2520called%2520Segment%252C%2520Shuffle%252C%2520and%2520Stitch%2520%2528S3%2529%250Adesigned%2520to%2520improve%2520time-series%2520representation%2520learning%2520of%2520existing%2520models.%2520S3%250Aworks%2520by%2520creating%2520non-overlapping%2520segments%2520from%2520the%2520original%2520sequence%2520and%250Ashuffling%2520them%2520in%2520a%2520learned%2520manner%2520that%2520is%2520the%2520most%2520optimal%2520for%2520the%2520task%2520at%250Ahand.%2520It%2520then%2520re-attaches%2520the%2520shuffled%2520segments%2520back%2520together%2520and%2520performs%2520a%250Alearned%2520weighted%2520sum%2520with%2520the%2520original%2520input%2520to%2520capture%2520both%2520the%2520newly%2520shuffled%250Asequence%2520along%2520with%2520the%2520original%2520sequence.%2520S3%2520is%2520modular%2520and%2520can%2520be%2520stacked%2520to%250Acreate%2520various%2520degrees%2520of%2520granularity%252C%2520and%2520can%2520be%2520added%2520to%2520many%2520forms%2520of%2520neural%250Aarchitectures%2520including%2520CNNs%2520or%2520Transformers%2520with%2520negligible%2520computation%250Aoverhead.%2520Through%2520extensive%2520experiments%2520on%2520several%2520datasets%2520and%250Astate-of-the-art%2520baselines%252C%2520we%2520show%2520that%2520incorporating%2520S3%2520results%2520in%250Asignificant%2520improvements%2520for%2520the%2520tasks%2520of%2520time-series%2520classification%2520and%250Aforecasting%252C%2520improving%2520performance%2520on%2520certain%2520datasets%2520by%2520up%2520to%252068%255C%2525.%2520We%2520also%250Ashow%2520that%2520S3%2520makes%2520the%2520learning%2520more%2520stable%2520with%2520a%2520smoother%2520training%2520loss%2520curve%250Aand%2520loss%2520landscape%2520compared%2520to%2520the%2520original%2520baseline.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shivam-grover/S3-TimeSeries%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%2C%20Shuffle%2C%20and%20Stitch%3A%20A%20Simple%20Mechanism%20for%20Improving%0A%20%20Time-Series%20Representations&entry.906535625=Shivam%20Grover%20and%20Amin%20Jalali%20and%20Ali%20Etemad&entry.1292438233=%20%20Existing%20approaches%20for%20learning%20representations%20of%20time-series%20keep%20the%0Atemporal%20arrangement%20of%20the%20time-steps%20intact%20with%20the%20presumption%20that%20the%0Aoriginal%20order%20is%20the%20most%20optimal%20for%20learning.%20However%2C%20non-adjacent%20sections%0Aof%20real-world%20time-series%20may%20have%20strong%20dependencies.%20Accordingly%20we%20raise%0Athe%20question%3A%20Is%20there%20an%20alternative%20arrangement%20for%20time-series%20which%20could%0Aenable%20more%20effective%20representation%20learning%3F%20To%20address%20this%2C%20we%20propose%20a%0Asimple%20plug-and-play%20mechanism%20called%20Segment%2C%20Shuffle%2C%20and%20Stitch%20%28S3%29%0Adesigned%20to%20improve%20time-series%20representation%20learning%20of%20existing%20models.%20S3%0Aworks%20by%20creating%20non-overlapping%20segments%20from%20the%20original%20sequence%20and%0Ashuffling%20them%20in%20a%20learned%20manner%20that%20is%20the%20most%20optimal%20for%20the%20task%20at%0Ahand.%20It%20then%20re-attaches%20the%20shuffled%20segments%20back%20together%20and%20performs%20a%0Alearned%20weighted%20sum%20with%20the%20original%20input%20to%20capture%20both%20the%20newly%20shuffled%0Asequence%20along%20with%20the%20original%20sequence.%20S3%20is%20modular%20and%20can%20be%20stacked%20to%0Acreate%20various%20degrees%20of%20granularity%2C%20and%20can%20be%20added%20to%20many%20forms%20of%20neural%0Aarchitectures%20including%20CNNs%20or%20Transformers%20with%20negligible%20computation%0Aoverhead.%20Through%20extensive%20experiments%20on%20several%20datasets%20and%0Astate-of-the-art%20baselines%2C%20we%20show%20that%20incorporating%20S3%20results%20in%0Asignificant%20improvements%20for%20the%20tasks%20of%20time-series%20classification%20and%0Aforecasting%2C%20improving%20performance%20on%20certain%20datasets%20by%20up%20to%2068%5C%25.%20We%20also%0Ashow%20that%20S3%20makes%20the%20learning%20more%20stable%20with%20a%20smoother%20training%20loss%20curve%0Aand%20loss%20landscape%20compared%20to%20the%20original%20baseline.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shivam-grover/S3-TimeSeries%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20082v1&entry.124074799=Read"},
{"title": "SPARE: Symmetrized Point-to-Plane Distance for Robust Non-Rigid\n  Registration", "author": "Yuxin Yao and Bailin Deng and Junhui Hou and Juyong Zhang", "abstract": "  Existing optimization-based methods for non-rigid registration typically\nminimize an alignment error metric based on the point-to-point or\npoint-to-plane distance between corresponding point pairs on the source surface\nand target surface. However, these metrics can result in slow convergence or a\nloss of detail. In this paper, we propose SPARE, a novel formulation that\nutilizes a symmetrized point-to-plane distance for robust non-rigid\nregistration. The symmetrized point-to-plane distance relies on both the\npositions and normals of the corresponding points, resulting in a more accurate\napproximation of the underlying geometry and can achieve higher accuracy than\nexisting methods. To solve this optimization problem efficiently, we propose an\nalternating minimization solver using a majorization-minimization strategy.\nMoreover, for effective initialization of the solver, we incorporate a\ndeformation graph-based coarse alignment that improves registration quality and\nefficiency. Extensive experiments show that the proposed method greatly\nimproves the accuracy of non-rigid registration problems and maintains\nrelatively high solution efficiency. The code is publicly available at\nhttps://github.com/yaoyx689/spare.\n", "link": "http://arxiv.org/abs/2405.20188v1", "date": "2024-05-30", "relevancy": 2.5072, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5108}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.507}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARE%3A%20Symmetrized%20Point-to-Plane%20Distance%20for%20Robust%20Non-Rigid%0A%20%20Registration&body=Title%3A%20SPARE%3A%20Symmetrized%20Point-to-Plane%20Distance%20for%20Robust%20Non-Rigid%0A%20%20Registration%0AAuthor%3A%20Yuxin%20Yao%20and%20Bailin%20Deng%20and%20Junhui%20Hou%20and%20Juyong%20Zhang%0AAbstract%3A%20%20%20Existing%20optimization-based%20methods%20for%20non-rigid%20registration%20typically%0Aminimize%20an%20alignment%20error%20metric%20based%20on%20the%20point-to-point%20or%0Apoint-to-plane%20distance%20between%20corresponding%20point%20pairs%20on%20the%20source%20surface%0Aand%20target%20surface.%20However%2C%20these%20metrics%20can%20result%20in%20slow%20convergence%20or%20a%0Aloss%20of%20detail.%20In%20this%20paper%2C%20we%20propose%20SPARE%2C%20a%20novel%20formulation%20that%0Autilizes%20a%20symmetrized%20point-to-plane%20distance%20for%20robust%20non-rigid%0Aregistration.%20The%20symmetrized%20point-to-plane%20distance%20relies%20on%20both%20the%0Apositions%20and%20normals%20of%20the%20corresponding%20points%2C%20resulting%20in%20a%20more%20accurate%0Aapproximation%20of%20the%20underlying%20geometry%20and%20can%20achieve%20higher%20accuracy%20than%0Aexisting%20methods.%20To%20solve%20this%20optimization%20problem%20efficiently%2C%20we%20propose%20an%0Aalternating%20minimization%20solver%20using%20a%20majorization-minimization%20strategy.%0AMoreover%2C%20for%20effective%20initialization%20of%20the%20solver%2C%20we%20incorporate%20a%0Adeformation%20graph-based%20coarse%20alignment%20that%20improves%20registration%20quality%20and%0Aefficiency.%20Extensive%20experiments%20show%20that%20the%20proposed%20method%20greatly%0Aimproves%20the%20accuracy%20of%20non-rigid%20registration%20problems%20and%20maintains%0Arelatively%20high%20solution%20efficiency.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yaoyx689/spare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARE%253A%2520Symmetrized%2520Point-to-Plane%2520Distance%2520for%2520Robust%2520Non-Rigid%250A%2520%2520Registration%26entry.906535625%3DYuxin%2520Yao%2520and%2520Bailin%2520Deng%2520and%2520Junhui%2520Hou%2520and%2520Juyong%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520optimization-based%2520methods%2520for%2520non-rigid%2520registration%2520typically%250Aminimize%2520an%2520alignment%2520error%2520metric%2520based%2520on%2520the%2520point-to-point%2520or%250Apoint-to-plane%2520distance%2520between%2520corresponding%2520point%2520pairs%2520on%2520the%2520source%2520surface%250Aand%2520target%2520surface.%2520However%252C%2520these%2520metrics%2520can%2520result%2520in%2520slow%2520convergence%2520or%2520a%250Aloss%2520of%2520detail.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SPARE%252C%2520a%2520novel%2520formulation%2520that%250Autilizes%2520a%2520symmetrized%2520point-to-plane%2520distance%2520for%2520robust%2520non-rigid%250Aregistration.%2520The%2520symmetrized%2520point-to-plane%2520distance%2520relies%2520on%2520both%2520the%250Apositions%2520and%2520normals%2520of%2520the%2520corresponding%2520points%252C%2520resulting%2520in%2520a%2520more%2520accurate%250Aapproximation%2520of%2520the%2520underlying%2520geometry%2520and%2520can%2520achieve%2520higher%2520accuracy%2520than%250Aexisting%2520methods.%2520To%2520solve%2520this%2520optimization%2520problem%2520efficiently%252C%2520we%2520propose%2520an%250Aalternating%2520minimization%2520solver%2520using%2520a%2520majorization-minimization%2520strategy.%250AMoreover%252C%2520for%2520effective%2520initialization%2520of%2520the%2520solver%252C%2520we%2520incorporate%2520a%250Adeformation%2520graph-based%2520coarse%2520alignment%2520that%2520improves%2520registration%2520quality%2520and%250Aefficiency.%2520Extensive%2520experiments%2520show%2520that%2520the%2520proposed%2520method%2520greatly%250Aimproves%2520the%2520accuracy%2520of%2520non-rigid%2520registration%2520problems%2520and%2520maintains%250Arelatively%2520high%2520solution%2520efficiency.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yaoyx689/spare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARE%3A%20Symmetrized%20Point-to-Plane%20Distance%20for%20Robust%20Non-Rigid%0A%20%20Registration&entry.906535625=Yuxin%20Yao%20and%20Bailin%20Deng%20and%20Junhui%20Hou%20and%20Juyong%20Zhang&entry.1292438233=%20%20Existing%20optimization-based%20methods%20for%20non-rigid%20registration%20typically%0Aminimize%20an%20alignment%20error%20metric%20based%20on%20the%20point-to-point%20or%0Apoint-to-plane%20distance%20between%20corresponding%20point%20pairs%20on%20the%20source%20surface%0Aand%20target%20surface.%20However%2C%20these%20metrics%20can%20result%20in%20slow%20convergence%20or%20a%0Aloss%20of%20detail.%20In%20this%20paper%2C%20we%20propose%20SPARE%2C%20a%20novel%20formulation%20that%0Autilizes%20a%20symmetrized%20point-to-plane%20distance%20for%20robust%20non-rigid%0Aregistration.%20The%20symmetrized%20point-to-plane%20distance%20relies%20on%20both%20the%0Apositions%20and%20normals%20of%20the%20corresponding%20points%2C%20resulting%20in%20a%20more%20accurate%0Aapproximation%20of%20the%20underlying%20geometry%20and%20can%20achieve%20higher%20accuracy%20than%0Aexisting%20methods.%20To%20solve%20this%20optimization%20problem%20efficiently%2C%20we%20propose%20an%0Aalternating%20minimization%20solver%20using%20a%20majorization-minimization%20strategy.%0AMoreover%2C%20for%20effective%20initialization%20of%20the%20solver%2C%20we%20incorporate%20a%0Adeformation%20graph-based%20coarse%20alignment%20that%20improves%20registration%20quality%20and%0Aefficiency.%20Extensive%20experiments%20show%20that%20the%20proposed%20method%20greatly%0Aimproves%20the%20accuracy%20of%20non-rigid%20registration%20problems%20and%20maintains%0Arelatively%20high%20solution%20efficiency.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yaoyx689/spare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20188v1&entry.124074799=Read"},
{"title": "CV-VAE: A Compatible Video VAE for Latent Generative Video Models", "author": "Sijie Zhao and Yong Zhang and Xiaodong Cun and Shaoshu Yang and Muyao Niu and Xiaoyu Li and Wenbo Hu and Ying Shan", "abstract": "  Spatio-temporal compression of videos, utilizing networks such as Variational\nAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other\nvideo generative models. For instance, many LLM-like video models learn the\ndistribution of discrete tokens derived from 3D VAEs within the VQVAE\nframework, while most diffusion-based video models capture the distribution of\ncontinuous latent extracted by 2D VAEs without quantization. The temporal\ncompression is simply realized by uniform frame sampling which results in\nunsmooth motion between consecutive frames. Currently, there lacks of a\ncommonly used continuous video (3D) VAE for latent diffusion-based video models\nin the research community. Moreover, since current diffusion-based approaches\nare often implemented using pre-trained text-to-image (T2I) models, directly\ntraining a video VAE without considering the compatibility with existing T2I\nmodels will result in a latent space gap between them, which will take huge\ncomputational resources for training to bridge the gap even with the T2I models\nas initialization. To address this issue, we propose a method for training a\nvideo VAE of latent video models, namely CV-VAE, whose latent space is\ncompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion\n(SD). The compatibility is achieved by the proposed novel latent space\nregularization, which involves formulating a regularization loss using the\nimage VAE. Benefiting from the latent space compatibility, video models can be\ntrained seamlessly from pre-trained T2I or video models in a truly\nspatio-temporally compressed latent space, rather than simply sampling video\nframes at equal intervals. With our CV-VAE, existing video models can generate\nfour times more frames with minimal finetuning. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed video VAE.\n", "link": "http://arxiv.org/abs/2405.20279v1", "date": "2024-05-30", "relevancy": 2.4803, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6456}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6367}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CV-VAE%3A%20A%20Compatible%20Video%20VAE%20for%20Latent%20Generative%20Video%20Models&body=Title%3A%20CV-VAE%3A%20A%20Compatible%20Video%20VAE%20for%20Latent%20Generative%20Video%20Models%0AAuthor%3A%20Sijie%20Zhao%20and%20Yong%20Zhang%20and%20Xiaodong%20Cun%20and%20Shaoshu%20Yang%20and%20Muyao%20Niu%20and%20Xiaoyu%20Li%20and%20Wenbo%20Hu%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Spatio-temporal%20compression%20of%20videos%2C%20utilizing%20networks%20such%20as%20Variational%0AAutoencoders%20%28VAE%29%2C%20plays%20a%20crucial%20role%20in%20OpenAI%27s%20SORA%20and%20numerous%20other%0Avideo%20generative%20models.%20For%20instance%2C%20many%20LLM-like%20video%20models%20learn%20the%0Adistribution%20of%20discrete%20tokens%20derived%20from%203D%20VAEs%20within%20the%20VQVAE%0Aframework%2C%20while%20most%20diffusion-based%20video%20models%20capture%20the%20distribution%20of%0Acontinuous%20latent%20extracted%20by%202D%20VAEs%20without%20quantization.%20The%20temporal%0Acompression%20is%20simply%20realized%20by%20uniform%20frame%20sampling%20which%20results%20in%0Aunsmooth%20motion%20between%20consecutive%20frames.%20Currently%2C%20there%20lacks%20of%20a%0Acommonly%20used%20continuous%20video%20%283D%29%20VAE%20for%20latent%20diffusion-based%20video%20models%0Ain%20the%20research%20community.%20Moreover%2C%20since%20current%20diffusion-based%20approaches%0Aare%20often%20implemented%20using%20pre-trained%20text-to-image%20%28T2I%29%20models%2C%20directly%0Atraining%20a%20video%20VAE%20without%20considering%20the%20compatibility%20with%20existing%20T2I%0Amodels%20will%20result%20in%20a%20latent%20space%20gap%20between%20them%2C%20which%20will%20take%20huge%0Acomputational%20resources%20for%20training%20to%20bridge%20the%20gap%20even%20with%20the%20T2I%20models%0Aas%20initialization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20method%20for%20training%20a%0Avideo%20VAE%20of%20latent%20video%20models%2C%20namely%20CV-VAE%2C%20whose%20latent%20space%20is%0Acompatible%20with%20that%20of%20a%20given%20image%20VAE%2C%20e.g.%2C%20image%20VAE%20of%20Stable%20Diffusion%0A%28SD%29.%20The%20compatibility%20is%20achieved%20by%20the%20proposed%20novel%20latent%20space%0Aregularization%2C%20which%20involves%20formulating%20a%20regularization%20loss%20using%20the%0Aimage%20VAE.%20Benefiting%20from%20the%20latent%20space%20compatibility%2C%20video%20models%20can%20be%0Atrained%20seamlessly%20from%20pre-trained%20T2I%20or%20video%20models%20in%20a%20truly%0Aspatio-temporally%20compressed%20latent%20space%2C%20rather%20than%20simply%20sampling%20video%0Aframes%20at%20equal%20intervals.%20With%20our%20CV-VAE%2C%20existing%20video%20models%20can%20generate%0Afour%20times%20more%20frames%20with%20minimal%20finetuning.%20Extensive%20experiments%20are%0Aconducted%20to%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20video%20VAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCV-VAE%253A%2520A%2520Compatible%2520Video%2520VAE%2520for%2520Latent%2520Generative%2520Video%2520Models%26entry.906535625%3DSijie%2520Zhao%2520and%2520Yong%2520Zhang%2520and%2520Xiaodong%2520Cun%2520and%2520Shaoshu%2520Yang%2520and%2520Muyao%2520Niu%2520and%2520Xiaoyu%2520Li%2520and%2520Wenbo%2520Hu%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Spatio-temporal%2520compression%2520of%2520videos%252C%2520utilizing%2520networks%2520such%2520as%2520Variational%250AAutoencoders%2520%2528VAE%2529%252C%2520plays%2520a%2520crucial%2520role%2520in%2520OpenAI%2527s%2520SORA%2520and%2520numerous%2520other%250Avideo%2520generative%2520models.%2520For%2520instance%252C%2520many%2520LLM-like%2520video%2520models%2520learn%2520the%250Adistribution%2520of%2520discrete%2520tokens%2520derived%2520from%25203D%2520VAEs%2520within%2520the%2520VQVAE%250Aframework%252C%2520while%2520most%2520diffusion-based%2520video%2520models%2520capture%2520the%2520distribution%2520of%250Acontinuous%2520latent%2520extracted%2520by%25202D%2520VAEs%2520without%2520quantization.%2520The%2520temporal%250Acompression%2520is%2520simply%2520realized%2520by%2520uniform%2520frame%2520sampling%2520which%2520results%2520in%250Aunsmooth%2520motion%2520between%2520consecutive%2520frames.%2520Currently%252C%2520there%2520lacks%2520of%2520a%250Acommonly%2520used%2520continuous%2520video%2520%25283D%2529%2520VAE%2520for%2520latent%2520diffusion-based%2520video%2520models%250Ain%2520the%2520research%2520community.%2520Moreover%252C%2520since%2520current%2520diffusion-based%2520approaches%250Aare%2520often%2520implemented%2520using%2520pre-trained%2520text-to-image%2520%2528T2I%2529%2520models%252C%2520directly%250Atraining%2520a%2520video%2520VAE%2520without%2520considering%2520the%2520compatibility%2520with%2520existing%2520T2I%250Amodels%2520will%2520result%2520in%2520a%2520latent%2520space%2520gap%2520between%2520them%252C%2520which%2520will%2520take%2520huge%250Acomputational%2520resources%2520for%2520training%2520to%2520bridge%2520the%2520gap%2520even%2520with%2520the%2520T2I%2520models%250Aas%2520initialization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520method%2520for%2520training%2520a%250Avideo%2520VAE%2520of%2520latent%2520video%2520models%252C%2520namely%2520CV-VAE%252C%2520whose%2520latent%2520space%2520is%250Acompatible%2520with%2520that%2520of%2520a%2520given%2520image%2520VAE%252C%2520e.g.%252C%2520image%2520VAE%2520of%2520Stable%2520Diffusion%250A%2528SD%2529.%2520The%2520compatibility%2520is%2520achieved%2520by%2520the%2520proposed%2520novel%2520latent%2520space%250Aregularization%252C%2520which%2520involves%2520formulating%2520a%2520regularization%2520loss%2520using%2520the%250Aimage%2520VAE.%2520Benefiting%2520from%2520the%2520latent%2520space%2520compatibility%252C%2520video%2520models%2520can%2520be%250Atrained%2520seamlessly%2520from%2520pre-trained%2520T2I%2520or%2520video%2520models%2520in%2520a%2520truly%250Aspatio-temporally%2520compressed%2520latent%2520space%252C%2520rather%2520than%2520simply%2520sampling%2520video%250Aframes%2520at%2520equal%2520intervals.%2520With%2520our%2520CV-VAE%252C%2520existing%2520video%2520models%2520can%2520generate%250Afour%2520times%2520more%2520frames%2520with%2520minimal%2520finetuning.%2520Extensive%2520experiments%2520are%250Aconducted%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520video%2520VAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CV-VAE%3A%20A%20Compatible%20Video%20VAE%20for%20Latent%20Generative%20Video%20Models&entry.906535625=Sijie%20Zhao%20and%20Yong%20Zhang%20and%20Xiaodong%20Cun%20and%20Shaoshu%20Yang%20and%20Muyao%20Niu%20and%20Xiaoyu%20Li%20and%20Wenbo%20Hu%20and%20Ying%20Shan&entry.1292438233=%20%20Spatio-temporal%20compression%20of%20videos%2C%20utilizing%20networks%20such%20as%20Variational%0AAutoencoders%20%28VAE%29%2C%20plays%20a%20crucial%20role%20in%20OpenAI%27s%20SORA%20and%20numerous%20other%0Avideo%20generative%20models.%20For%20instance%2C%20many%20LLM-like%20video%20models%20learn%20the%0Adistribution%20of%20discrete%20tokens%20derived%20from%203D%20VAEs%20within%20the%20VQVAE%0Aframework%2C%20while%20most%20diffusion-based%20video%20models%20capture%20the%20distribution%20of%0Acontinuous%20latent%20extracted%20by%202D%20VAEs%20without%20quantization.%20The%20temporal%0Acompression%20is%20simply%20realized%20by%20uniform%20frame%20sampling%20which%20results%20in%0Aunsmooth%20motion%20between%20consecutive%20frames.%20Currently%2C%20there%20lacks%20of%20a%0Acommonly%20used%20continuous%20video%20%283D%29%20VAE%20for%20latent%20diffusion-based%20video%20models%0Ain%20the%20research%20community.%20Moreover%2C%20since%20current%20diffusion-based%20approaches%0Aare%20often%20implemented%20using%20pre-trained%20text-to-image%20%28T2I%29%20models%2C%20directly%0Atraining%20a%20video%20VAE%20without%20considering%20the%20compatibility%20with%20existing%20T2I%0Amodels%20will%20result%20in%20a%20latent%20space%20gap%20between%20them%2C%20which%20will%20take%20huge%0Acomputational%20resources%20for%20training%20to%20bridge%20the%20gap%20even%20with%20the%20T2I%20models%0Aas%20initialization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20method%20for%20training%20a%0Avideo%20VAE%20of%20latent%20video%20models%2C%20namely%20CV-VAE%2C%20whose%20latent%20space%20is%0Acompatible%20with%20that%20of%20a%20given%20image%20VAE%2C%20e.g.%2C%20image%20VAE%20of%20Stable%20Diffusion%0A%28SD%29.%20The%20compatibility%20is%20achieved%20by%20the%20proposed%20novel%20latent%20space%0Aregularization%2C%20which%20involves%20formulating%20a%20regularization%20loss%20using%20the%0Aimage%20VAE.%20Benefiting%20from%20the%20latent%20space%20compatibility%2C%20video%20models%20can%20be%0Atrained%20seamlessly%20from%20pre-trained%20T2I%20or%20video%20models%20in%20a%20truly%0Aspatio-temporally%20compressed%20latent%20space%2C%20rather%20than%20simply%20sampling%20video%0Aframes%20at%20equal%20intervals.%20With%20our%20CV-VAE%2C%20existing%20video%20models%20can%20generate%0Afour%20times%20more%20frames%20with%20minimal%20finetuning.%20Extensive%20experiments%20are%0Aconducted%20to%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20video%20VAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20279v1&entry.124074799=Read"},
{"title": "MOFA-Video: Controllable Image Animation via Generative Motion Field\n  Adaptions in Frozen Image-to-Video Diffusion Model", "author": "Muyao Niu and Xiaodong Cun and Xintao Wang and Yong Zhang and Ying Shan and Yinqiang Zheng", "abstract": "  We present MOFA-Video, an advanced controllable image animation method that\ngenerates video from the given image using various additional controllable\nsignals (such as human landmarks reference, manual trajectories, and another\neven provided video) or their combinations. This is different from previous\nmethods which only can work on a specific motion domain or show weak control\nabilities with diffusion prior. To achieve our goal, we design several\ndomain-aware motion field adapters (\\ie, MOFA-Adapters) to control the\ngenerated motions in the video generation pipeline. For MOFA-Adapters, we\nconsider the temporal motion consistency of the video and generate the dense\nmotion flow from the given sparse control conditions first, and then, the\nmulti-scale features of the given image are wrapped as a guided feature for\nstable video diffusion generation. We naively train two motion adapters for the\nmanual trajectories and the human landmarks individually since they both\ncontain sparse information about the control. After training, the MOFA-Adapters\nin different domains can also work together for more controllable video\ngeneration.\n", "link": "http://arxiv.org/abs/2405.20222v1", "date": "2024-05-30", "relevancy": 2.4557, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.654}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6178}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOFA-Video%3A%20Controllable%20Image%20Animation%20via%20Generative%20Motion%20Field%0A%20%20Adaptions%20in%20Frozen%20Image-to-Video%20Diffusion%20Model&body=Title%3A%20MOFA-Video%3A%20Controllable%20Image%20Animation%20via%20Generative%20Motion%20Field%0A%20%20Adaptions%20in%20Frozen%20Image-to-Video%20Diffusion%20Model%0AAuthor%3A%20Muyao%20Niu%20and%20Xiaodong%20Cun%20and%20Xintao%20Wang%20and%20Yong%20Zhang%20and%20Ying%20Shan%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20We%20present%20MOFA-Video%2C%20an%20advanced%20controllable%20image%20animation%20method%20that%0Agenerates%20video%20from%20the%20given%20image%20using%20various%20additional%20controllable%0Asignals%20%28such%20as%20human%20landmarks%20reference%2C%20manual%20trajectories%2C%20and%20another%0Aeven%20provided%20video%29%20or%20their%20combinations.%20This%20is%20different%20from%20previous%0Amethods%20which%20only%20can%20work%20on%20a%20specific%20motion%20domain%20or%20show%20weak%20control%0Aabilities%20with%20diffusion%20prior.%20To%20achieve%20our%20goal%2C%20we%20design%20several%0Adomain-aware%20motion%20field%20adapters%20%28%5Cie%2C%20MOFA-Adapters%29%20to%20control%20the%0Agenerated%20motions%20in%20the%20video%20generation%20pipeline.%20For%20MOFA-Adapters%2C%20we%0Aconsider%20the%20temporal%20motion%20consistency%20of%20the%20video%20and%20generate%20the%20dense%0Amotion%20flow%20from%20the%20given%20sparse%20control%20conditions%20first%2C%20and%20then%2C%20the%0Amulti-scale%20features%20of%20the%20given%20image%20are%20wrapped%20as%20a%20guided%20feature%20for%0Astable%20video%20diffusion%20generation.%20We%20naively%20train%20two%20motion%20adapters%20for%20the%0Amanual%20trajectories%20and%20the%20human%20landmarks%20individually%20since%20they%20both%0Acontain%20sparse%20information%20about%20the%20control.%20After%20training%2C%20the%20MOFA-Adapters%0Ain%20different%20domains%20can%20also%20work%20together%20for%20more%20controllable%20video%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOFA-Video%253A%2520Controllable%2520Image%2520Animation%2520via%2520Generative%2520Motion%2520Field%250A%2520%2520Adaptions%2520in%2520Frozen%2520Image-to-Video%2520Diffusion%2520Model%26entry.906535625%3DMuyao%2520Niu%2520and%2520Xiaodong%2520Cun%2520and%2520Xintao%2520Wang%2520and%2520Yong%2520Zhang%2520and%2520Ying%2520Shan%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520We%2520present%2520MOFA-Video%252C%2520an%2520advanced%2520controllable%2520image%2520animation%2520method%2520that%250Agenerates%2520video%2520from%2520the%2520given%2520image%2520using%2520various%2520additional%2520controllable%250Asignals%2520%2528such%2520as%2520human%2520landmarks%2520reference%252C%2520manual%2520trajectories%252C%2520and%2520another%250Aeven%2520provided%2520video%2529%2520or%2520their%2520combinations.%2520This%2520is%2520different%2520from%2520previous%250Amethods%2520which%2520only%2520can%2520work%2520on%2520a%2520specific%2520motion%2520domain%2520or%2520show%2520weak%2520control%250Aabilities%2520with%2520diffusion%2520prior.%2520To%2520achieve%2520our%2520goal%252C%2520we%2520design%2520several%250Adomain-aware%2520motion%2520field%2520adapters%2520%2528%255Cie%252C%2520MOFA-Adapters%2529%2520to%2520control%2520the%250Agenerated%2520motions%2520in%2520the%2520video%2520generation%2520pipeline.%2520For%2520MOFA-Adapters%252C%2520we%250Aconsider%2520the%2520temporal%2520motion%2520consistency%2520of%2520the%2520video%2520and%2520generate%2520the%2520dense%250Amotion%2520flow%2520from%2520the%2520given%2520sparse%2520control%2520conditions%2520first%252C%2520and%2520then%252C%2520the%250Amulti-scale%2520features%2520of%2520the%2520given%2520image%2520are%2520wrapped%2520as%2520a%2520guided%2520feature%2520for%250Astable%2520video%2520diffusion%2520generation.%2520We%2520naively%2520train%2520two%2520motion%2520adapters%2520for%2520the%250Amanual%2520trajectories%2520and%2520the%2520human%2520landmarks%2520individually%2520since%2520they%2520both%250Acontain%2520sparse%2520information%2520about%2520the%2520control.%2520After%2520training%252C%2520the%2520MOFA-Adapters%250Ain%2520different%2520domains%2520can%2520also%2520work%2520together%2520for%2520more%2520controllable%2520video%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOFA-Video%3A%20Controllable%20Image%20Animation%20via%20Generative%20Motion%20Field%0A%20%20Adaptions%20in%20Frozen%20Image-to-Video%20Diffusion%20Model&entry.906535625=Muyao%20Niu%20and%20Xiaodong%20Cun%20and%20Xintao%20Wang%20and%20Yong%20Zhang%20and%20Ying%20Shan%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20We%20present%20MOFA-Video%2C%20an%20advanced%20controllable%20image%20animation%20method%20that%0Agenerates%20video%20from%20the%20given%20image%20using%20various%20additional%20controllable%0Asignals%20%28such%20as%20human%20landmarks%20reference%2C%20manual%20trajectories%2C%20and%20another%0Aeven%20provided%20video%29%20or%20their%20combinations.%20This%20is%20different%20from%20previous%0Amethods%20which%20only%20can%20work%20on%20a%20specific%20motion%20domain%20or%20show%20weak%20control%0Aabilities%20with%20diffusion%20prior.%20To%20achieve%20our%20goal%2C%20we%20design%20several%0Adomain-aware%20motion%20field%20adapters%20%28%5Cie%2C%20MOFA-Adapters%29%20to%20control%20the%0Agenerated%20motions%20in%20the%20video%20generation%20pipeline.%20For%20MOFA-Adapters%2C%20we%0Aconsider%20the%20temporal%20motion%20consistency%20of%20the%20video%20and%20generate%20the%20dense%0Amotion%20flow%20from%20the%20given%20sparse%20control%20conditions%20first%2C%20and%20then%2C%20the%0Amulti-scale%20features%20of%20the%20given%20image%20are%20wrapped%20as%20a%20guided%20feature%20for%0Astable%20video%20diffusion%20generation.%20We%20naively%20train%20two%20motion%20adapters%20for%20the%0Amanual%20trajectories%20and%20the%20human%20landmarks%20individually%20since%20they%20both%0Acontain%20sparse%20information%20about%20the%20control.%20After%20training%2C%20the%20MOFA-Adapters%0Ain%20different%20domains%20can%20also%20work%20together%20for%20more%20controllable%20video%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20222v1&entry.124074799=Read"},
{"title": "Defining Neural Network Architecture through Polytope Structures of\n  Dataset", "author": "Sangmin Lee and Abbas Mammadov and Jong Chul Ye", "abstract": "  Current theoretical and empirical research in neural networks suggests that\ncomplex datasets require large network architectures for thorough\nclassification, yet the precise nature of this relationship remains unclear.\nThis paper tackles this issue by defining upper and lower bounds for neural\nnetwork widths, which are informed by the polytope structure of the dataset in\nquestion. We also delve into the application of these principles to simplicial\ncomplexes and specific manifold shapes, explaining how the requirement for\nnetwork width varies in accordance with the geometric complexity of the\ndataset. Moreover, we develop an algorithm to investigate a converse situation\nwhere the polytope structure of a dataset can be inferred from its\ncorresponding trained neural networks. Through our algorithm, it is established\nthat popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be\nefficiently encapsulated using no more than two polytopes with a small number\nof faces.\n", "link": "http://arxiv.org/abs/2402.02407v2", "date": "2024-05-30", "relevancy": 2.4294, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5104}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defining%20Neural%20Network%20Architecture%20through%20Polytope%20Structures%20of%0A%20%20Dataset&body=Title%3A%20Defining%20Neural%20Network%20Architecture%20through%20Polytope%20Structures%20of%0A%20%20Dataset%0AAuthor%3A%20Sangmin%20Lee%20and%20Abbas%20Mammadov%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20%20%20Current%20theoretical%20and%20empirical%20research%20in%20neural%20networks%20suggests%20that%0Acomplex%20datasets%20require%20large%20network%20architectures%20for%20thorough%0Aclassification%2C%20yet%20the%20precise%20nature%20of%20this%20relationship%20remains%20unclear.%0AThis%20paper%20tackles%20this%20issue%20by%20defining%20upper%20and%20lower%20bounds%20for%20neural%0Anetwork%20widths%2C%20which%20are%20informed%20by%20the%20polytope%20structure%20of%20the%20dataset%20in%0Aquestion.%20We%20also%20delve%20into%20the%20application%20of%20these%20principles%20to%20simplicial%0Acomplexes%20and%20specific%20manifold%20shapes%2C%20explaining%20how%20the%20requirement%20for%0Anetwork%20width%20varies%20in%20accordance%20with%20the%20geometric%20complexity%20of%20the%0Adataset.%20Moreover%2C%20we%20develop%20an%20algorithm%20to%20investigate%20a%20converse%20situation%0Awhere%20the%20polytope%20structure%20of%20a%20dataset%20can%20be%20inferred%20from%20its%0Acorresponding%20trained%20neural%20networks.%20Through%20our%20algorithm%2C%20it%20is%20established%0Athat%20popular%20datasets%20such%20as%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR10%20can%20be%0Aefficiently%20encapsulated%20using%20no%20more%20than%20two%20polytopes%20with%20a%20small%20number%0Aof%20faces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefining%2520Neural%2520Network%2520Architecture%2520through%2520Polytope%2520Structures%2520of%250A%2520%2520Dataset%26entry.906535625%3DSangmin%2520Lee%2520and%2520Abbas%2520Mammadov%2520and%2520Jong%2520Chul%2520Ye%26entry.1292438233%3D%2520%2520Current%2520theoretical%2520and%2520empirical%2520research%2520in%2520neural%2520networks%2520suggests%2520that%250Acomplex%2520datasets%2520require%2520large%2520network%2520architectures%2520for%2520thorough%250Aclassification%252C%2520yet%2520the%2520precise%2520nature%2520of%2520this%2520relationship%2520remains%2520unclear.%250AThis%2520paper%2520tackles%2520this%2520issue%2520by%2520defining%2520upper%2520and%2520lower%2520bounds%2520for%2520neural%250Anetwork%2520widths%252C%2520which%2520are%2520informed%2520by%2520the%2520polytope%2520structure%2520of%2520the%2520dataset%2520in%250Aquestion.%2520We%2520also%2520delve%2520into%2520the%2520application%2520of%2520these%2520principles%2520to%2520simplicial%250Acomplexes%2520and%2520specific%2520manifold%2520shapes%252C%2520explaining%2520how%2520the%2520requirement%2520for%250Anetwork%2520width%2520varies%2520in%2520accordance%2520with%2520the%2520geometric%2520complexity%2520of%2520the%250Adataset.%2520Moreover%252C%2520we%2520develop%2520an%2520algorithm%2520to%2520investigate%2520a%2520converse%2520situation%250Awhere%2520the%2520polytope%2520structure%2520of%2520a%2520dataset%2520can%2520be%2520inferred%2520from%2520its%250Acorresponding%2520trained%2520neural%2520networks.%2520Through%2520our%2520algorithm%252C%2520it%2520is%2520established%250Athat%2520popular%2520datasets%2520such%2520as%2520MNIST%252C%2520Fashion-MNIST%252C%2520and%2520CIFAR10%2520can%2520be%250Aefficiently%2520encapsulated%2520using%2520no%2520more%2520than%2520two%2520polytopes%2520with%2520a%2520small%2520number%250Aof%2520faces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defining%20Neural%20Network%20Architecture%20through%20Polytope%20Structures%20of%0A%20%20Dataset&entry.906535625=Sangmin%20Lee%20and%20Abbas%20Mammadov%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20Current%20theoretical%20and%20empirical%20research%20in%20neural%20networks%20suggests%20that%0Acomplex%20datasets%20require%20large%20network%20architectures%20for%20thorough%0Aclassification%2C%20yet%20the%20precise%20nature%20of%20this%20relationship%20remains%20unclear.%0AThis%20paper%20tackles%20this%20issue%20by%20defining%20upper%20and%20lower%20bounds%20for%20neural%0Anetwork%20widths%2C%20which%20are%20informed%20by%20the%20polytope%20structure%20of%20the%20dataset%20in%0Aquestion.%20We%20also%20delve%20into%20the%20application%20of%20these%20principles%20to%20simplicial%0Acomplexes%20and%20specific%20manifold%20shapes%2C%20explaining%20how%20the%20requirement%20for%0Anetwork%20width%20varies%20in%20accordance%20with%20the%20geometric%20complexity%20of%20the%0Adataset.%20Moreover%2C%20we%20develop%20an%20algorithm%20to%20investigate%20a%20converse%20situation%0Awhere%20the%20polytope%20structure%20of%20a%20dataset%20can%20be%20inferred%20from%20its%0Acorresponding%20trained%20neural%20networks.%20Through%20our%20algorithm%2C%20it%20is%20established%0Athat%20popular%20datasets%20such%20as%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR10%20can%20be%0Aefficiently%20encapsulated%20using%20no%20more%20than%20two%20polytopes%20with%20a%20small%20number%0Aof%20faces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02407v2&entry.124074799=Read"},
{"title": "OccSora: 4D Occupancy Generation Models as World Simulators for\n  Autonomous Driving", "author": "Lening Wang and Wenzhao Zheng and Yilong Ren and Han Jiang and Zhiyong Cui and Haiyang Yu and Jiwen Lu", "abstract": "  Understanding the evolution of 3D scenes is important for effective\nautonomous driving. While conventional methods mode scene development with the\nmotion of individual instances, world models emerge as a generative framework\nto describe the general scene dynamics. However, most existing methods adopt an\nautoregressive framework to perform next-token prediction, which suffer from\ninefficiency in modeling long-term temporal evolutions. To address this, we\npropose a diffusion-based 4D occupancy generation model, OccSora, to simulate\nthe development of the 3D world for autonomous driving. We employ a 4D scene\ntokenizer to obtain compact discrete spatial-temporal representations for 4D\noccupancy input and achieve high-quality reconstruction for long-sequence\noccupancy videos. We then learn a diffusion transformer on the spatial-temporal\nrepresentations and generate 4D occupancy conditioned on a trajectory prompt.\nWe conduct extensive experiments on the widely used nuScenes dataset with Occ3D\noccupancy annotations. OccSora can generate 16s-videos with authentic 3D layout\nand temporal consistency, demonstrating its ability to understand the spatial\nand temporal distributions of driving scenes. With trajectory-aware 4D\ngeneration, OccSora has the potential to serve as a world simulator for the\ndecision-making of autonomous driving. Code is available at:\nhttps://github.com/wzzheng/OccSora.\n", "link": "http://arxiv.org/abs/2405.20337v1", "date": "2024-05-30", "relevancy": 2.4203, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6103}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6103}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OccSora%3A%204D%20Occupancy%20Generation%20Models%20as%20World%20Simulators%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20OccSora%3A%204D%20Occupancy%20Generation%20Models%20as%20World%20Simulators%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Lening%20Wang%20and%20Wenzhao%20Zheng%20and%20Yilong%20Ren%20and%20Han%20Jiang%20and%20Zhiyong%20Cui%20and%20Haiyang%20Yu%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Understanding%20the%20evolution%20of%203D%20scenes%20is%20important%20for%20effective%0Aautonomous%20driving.%20While%20conventional%20methods%20mode%20scene%20development%20with%20the%0Amotion%20of%20individual%20instances%2C%20world%20models%20emerge%20as%20a%20generative%20framework%0Ato%20describe%20the%20general%20scene%20dynamics.%20However%2C%20most%20existing%20methods%20adopt%20an%0Aautoregressive%20framework%20to%20perform%20next-token%20prediction%2C%20which%20suffer%20from%0Ainefficiency%20in%20modeling%20long-term%20temporal%20evolutions.%20To%20address%20this%2C%20we%0Apropose%20a%20diffusion-based%204D%20occupancy%20generation%20model%2C%20OccSora%2C%20to%20simulate%0Athe%20development%20of%20the%203D%20world%20for%20autonomous%20driving.%20We%20employ%20a%204D%20scene%0Atokenizer%20to%20obtain%20compact%20discrete%20spatial-temporal%20representations%20for%204D%0Aoccupancy%20input%20and%20achieve%20high-quality%20reconstruction%20for%20long-sequence%0Aoccupancy%20videos.%20We%20then%20learn%20a%20diffusion%20transformer%20on%20the%20spatial-temporal%0Arepresentations%20and%20generate%204D%20occupancy%20conditioned%20on%20a%20trajectory%20prompt.%0AWe%20conduct%20extensive%20experiments%20on%20the%20widely%20used%20nuScenes%20dataset%20with%20Occ3D%0Aoccupancy%20annotations.%20OccSora%20can%20generate%2016s-videos%20with%20authentic%203D%20layout%0Aand%20temporal%20consistency%2C%20demonstrating%20its%20ability%20to%20understand%20the%20spatial%0Aand%20temporal%20distributions%20of%20driving%20scenes.%20With%20trajectory-aware%204D%0Ageneration%2C%20OccSora%20has%20the%20potential%20to%20serve%20as%20a%20world%20simulator%20for%20the%0Adecision-making%20of%20autonomous%20driving.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/wzzheng/OccSora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccSora%253A%25204D%2520Occupancy%2520Generation%2520Models%2520as%2520World%2520Simulators%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DLening%2520Wang%2520and%2520Wenzhao%2520Zheng%2520and%2520Yilong%2520Ren%2520and%2520Han%2520Jiang%2520and%2520Zhiyong%2520Cui%2520and%2520Haiyang%2520Yu%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Understanding%2520the%2520evolution%2520of%25203D%2520scenes%2520is%2520important%2520for%2520effective%250Aautonomous%2520driving.%2520While%2520conventional%2520methods%2520mode%2520scene%2520development%2520with%2520the%250Amotion%2520of%2520individual%2520instances%252C%2520world%2520models%2520emerge%2520as%2520a%2520generative%2520framework%250Ato%2520describe%2520the%2520general%2520scene%2520dynamics.%2520However%252C%2520most%2520existing%2520methods%2520adopt%2520an%250Aautoregressive%2520framework%2520to%2520perform%2520next-token%2520prediction%252C%2520which%2520suffer%2520from%250Ainefficiency%2520in%2520modeling%2520long-term%2520temporal%2520evolutions.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520diffusion-based%25204D%2520occupancy%2520generation%2520model%252C%2520OccSora%252C%2520to%2520simulate%250Athe%2520development%2520of%2520the%25203D%2520world%2520for%2520autonomous%2520driving.%2520We%2520employ%2520a%25204D%2520scene%250Atokenizer%2520to%2520obtain%2520compact%2520discrete%2520spatial-temporal%2520representations%2520for%25204D%250Aoccupancy%2520input%2520and%2520achieve%2520high-quality%2520reconstruction%2520for%2520long-sequence%250Aoccupancy%2520videos.%2520We%2520then%2520learn%2520a%2520diffusion%2520transformer%2520on%2520the%2520spatial-temporal%250Arepresentations%2520and%2520generate%25204D%2520occupancy%2520conditioned%2520on%2520a%2520trajectory%2520prompt.%250AWe%2520conduct%2520extensive%2520experiments%2520on%2520the%2520widely%2520used%2520nuScenes%2520dataset%2520with%2520Occ3D%250Aoccupancy%2520annotations.%2520OccSora%2520can%2520generate%252016s-videos%2520with%2520authentic%25203D%2520layout%250Aand%2520temporal%2520consistency%252C%2520demonstrating%2520its%2520ability%2520to%2520understand%2520the%2520spatial%250Aand%2520temporal%2520distributions%2520of%2520driving%2520scenes.%2520With%2520trajectory-aware%25204D%250Ageneration%252C%2520OccSora%2520has%2520the%2520potential%2520to%2520serve%2520as%2520a%2520world%2520simulator%2520for%2520the%250Adecision-making%2520of%2520autonomous%2520driving.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/wzzheng/OccSora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccSora%3A%204D%20Occupancy%20Generation%20Models%20as%20World%20Simulators%20for%0A%20%20Autonomous%20Driving&entry.906535625=Lening%20Wang%20and%20Wenzhao%20Zheng%20and%20Yilong%20Ren%20and%20Han%20Jiang%20and%20Zhiyong%20Cui%20and%20Haiyang%20Yu%20and%20Jiwen%20Lu&entry.1292438233=%20%20Understanding%20the%20evolution%20of%203D%20scenes%20is%20important%20for%20effective%0Aautonomous%20driving.%20While%20conventional%20methods%20mode%20scene%20development%20with%20the%0Amotion%20of%20individual%20instances%2C%20world%20models%20emerge%20as%20a%20generative%20framework%0Ato%20describe%20the%20general%20scene%20dynamics.%20However%2C%20most%20existing%20methods%20adopt%20an%0Aautoregressive%20framework%20to%20perform%20next-token%20prediction%2C%20which%20suffer%20from%0Ainefficiency%20in%20modeling%20long-term%20temporal%20evolutions.%20To%20address%20this%2C%20we%0Apropose%20a%20diffusion-based%204D%20occupancy%20generation%20model%2C%20OccSora%2C%20to%20simulate%0Athe%20development%20of%20the%203D%20world%20for%20autonomous%20driving.%20We%20employ%20a%204D%20scene%0Atokenizer%20to%20obtain%20compact%20discrete%20spatial-temporal%20representations%20for%204D%0Aoccupancy%20input%20and%20achieve%20high-quality%20reconstruction%20for%20long-sequence%0Aoccupancy%20videos.%20We%20then%20learn%20a%20diffusion%20transformer%20on%20the%20spatial-temporal%0Arepresentations%20and%20generate%204D%20occupancy%20conditioned%20on%20a%20trajectory%20prompt.%0AWe%20conduct%20extensive%20experiments%20on%20the%20widely%20used%20nuScenes%20dataset%20with%20Occ3D%0Aoccupancy%20annotations.%20OccSora%20can%20generate%2016s-videos%20with%20authentic%203D%20layout%0Aand%20temporal%20consistency%2C%20demonstrating%20its%20ability%20to%20understand%20the%20spatial%0Aand%20temporal%20distributions%20of%20driving%20scenes.%20With%20trajectory-aware%204D%0Ageneration%2C%20OccSora%20has%20the%20potential%20to%20serve%20as%20a%20world%20simulator%20for%20the%0Adecision-making%20of%20autonomous%20driving.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/wzzheng/OccSora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20337v1&entry.124074799=Read"},
{"title": "Rapid Wildfire Hotspot Detection Using Self-Supervised Learning on\n  Temporal Remote Sensing Data", "author": "Luca Barco and Angelica Urbanelli and Claudio Rossi", "abstract": "  Rapid detection and well-timed intervention are essential to mitigate the\nimpacts of wildfires. Leveraging remote sensed data from satellite networks and\nadvanced AI models to automatically detect hotspots (i.e., thermal anomalies\ncaused by active fires) is an effective way to build wildfire monitoring\nsystems. In this work, we propose a novel dataset containing time series of\nremotely sensed data related to European fire events and a Self-Supervised\nLearning (SSL)-based model able to analyse multi-temporal data and identify\nhotspots in potentially near real time. We train and evaluate the performance\nof our model using our dataset and Thraws, a dataset of thermal anomalies\nincluding several fire events, obtaining an F1 score of 63.58.\n", "link": "http://arxiv.org/abs/2405.20093v1", "date": "2024-05-30", "relevancy": 2.4048, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5174}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4718}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Wildfire%20Hotspot%20Detection%20Using%20Self-Supervised%20Learning%20on%0A%20%20Temporal%20Remote%20Sensing%20Data&body=Title%3A%20Rapid%20Wildfire%20Hotspot%20Detection%20Using%20Self-Supervised%20Learning%20on%0A%20%20Temporal%20Remote%20Sensing%20Data%0AAuthor%3A%20Luca%20Barco%20and%20Angelica%20Urbanelli%20and%20Claudio%20Rossi%0AAbstract%3A%20%20%20Rapid%20detection%20and%20well-timed%20intervention%20are%20essential%20to%20mitigate%20the%0Aimpacts%20of%20wildfires.%20Leveraging%20remote%20sensed%20data%20from%20satellite%20networks%20and%0Aadvanced%20AI%20models%20to%20automatically%20detect%20hotspots%20%28i.e.%2C%20thermal%20anomalies%0Acaused%20by%20active%20fires%29%20is%20an%20effective%20way%20to%20build%20wildfire%20monitoring%0Asystems.%20In%20this%20work%2C%20we%20propose%20a%20novel%20dataset%20containing%20time%20series%20of%0Aremotely%20sensed%20data%20related%20to%20European%20fire%20events%20and%20a%20Self-Supervised%0ALearning%20%28SSL%29-based%20model%20able%20to%20analyse%20multi-temporal%20data%20and%20identify%0Ahotspots%20in%20potentially%20near%20real%20time.%20We%20train%20and%20evaluate%20the%20performance%0Aof%20our%20model%20using%20our%20dataset%20and%20Thraws%2C%20a%20dataset%20of%20thermal%20anomalies%0Aincluding%20several%20fire%20events%2C%20obtaining%20an%20F1%20score%20of%2063.58.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Wildfire%2520Hotspot%2520Detection%2520Using%2520Self-Supervised%2520Learning%2520on%250A%2520%2520Temporal%2520Remote%2520Sensing%2520Data%26entry.906535625%3DLuca%2520Barco%2520and%2520Angelica%2520Urbanelli%2520and%2520Claudio%2520Rossi%26entry.1292438233%3D%2520%2520Rapid%2520detection%2520and%2520well-timed%2520intervention%2520are%2520essential%2520to%2520mitigate%2520the%250Aimpacts%2520of%2520wildfires.%2520Leveraging%2520remote%2520sensed%2520data%2520from%2520satellite%2520networks%2520and%250Aadvanced%2520AI%2520models%2520to%2520automatically%2520detect%2520hotspots%2520%2528i.e.%252C%2520thermal%2520anomalies%250Acaused%2520by%2520active%2520fires%2529%2520is%2520an%2520effective%2520way%2520to%2520build%2520wildfire%2520monitoring%250Asystems.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520dataset%2520containing%2520time%2520series%2520of%250Aremotely%2520sensed%2520data%2520related%2520to%2520European%2520fire%2520events%2520and%2520a%2520Self-Supervised%250ALearning%2520%2528SSL%2529-based%2520model%2520able%2520to%2520analyse%2520multi-temporal%2520data%2520and%2520identify%250Ahotspots%2520in%2520potentially%2520near%2520real%2520time.%2520We%2520train%2520and%2520evaluate%2520the%2520performance%250Aof%2520our%2520model%2520using%2520our%2520dataset%2520and%2520Thraws%252C%2520a%2520dataset%2520of%2520thermal%2520anomalies%250Aincluding%2520several%2520fire%2520events%252C%2520obtaining%2520an%2520F1%2520score%2520of%252063.58.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Wildfire%20Hotspot%20Detection%20Using%20Self-Supervised%20Learning%20on%0A%20%20Temporal%20Remote%20Sensing%20Data&entry.906535625=Luca%20Barco%20and%20Angelica%20Urbanelli%20and%20Claudio%20Rossi&entry.1292438233=%20%20Rapid%20detection%20and%20well-timed%20intervention%20are%20essential%20to%20mitigate%20the%0Aimpacts%20of%20wildfires.%20Leveraging%20remote%20sensed%20data%20from%20satellite%20networks%20and%0Aadvanced%20AI%20models%20to%20automatically%20detect%20hotspots%20%28i.e.%2C%20thermal%20anomalies%0Acaused%20by%20active%20fires%29%20is%20an%20effective%20way%20to%20build%20wildfire%20monitoring%0Asystems.%20In%20this%20work%2C%20we%20propose%20a%20novel%20dataset%20containing%20time%20series%20of%0Aremotely%20sensed%20data%20related%20to%20European%20fire%20events%20and%20a%20Self-Supervised%0ALearning%20%28SSL%29-based%20model%20able%20to%20analyse%20multi-temporal%20data%20and%20identify%0Ahotspots%20in%20potentially%20near%20real%20time.%20We%20train%20and%20evaluate%20the%20performance%0Aof%20our%20model%20using%20our%20dataset%20and%20Thraws%2C%20a%20dataset%20of%20thermal%20anomalies%0Aincluding%20several%20fire%20events%2C%20obtaining%20an%20F1%20score%20of%2063.58.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20093v1&entry.124074799=Read"},
{"title": "InstructionCP: A fast approach to transfer Large Language Models into\n  target language", "author": "Kuang-Ming Chen and Hung-yi Lee", "abstract": "  The rapid development of large language models (LLMs) in recent years has\nlargely focused on English, resulting in models that respond exclusively in\nEnglish. To adapt these models to other languages, continual pre-training (CP)\nis often employed, followed by supervised fine-tuning (SFT) to maintain\nconversational abilities. However, CP and SFT can reduce a model's ability to\nfilter harmful content. We propose Instruction Continual Pre-training (InsCP),\nwhich integrates instruction tags into the CP process to prevent loss of\nconversational proficiency while acquiring new languages. Our experiments\ndemonstrate that InsCP retains conversational and Reinforcement Learning from\nHuman Feedback (RLHF) abilities. Empirical evaluations on language alignment,\nreliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably,\nthis approach requires only 0.1 billion tokens of high-quality\ninstruction-following data, thereby reducing resource consumption.\n", "link": "http://arxiv.org/abs/2405.20175v1", "date": "2024-05-30", "relevancy": 2.4004, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5126}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4682}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructionCP%3A%20A%20fast%20approach%20to%20transfer%20Large%20Language%20Models%20into%0A%20%20target%20language&body=Title%3A%20InstructionCP%3A%20A%20fast%20approach%20to%20transfer%20Large%20Language%20Models%20into%0A%20%20target%20language%0AAuthor%3A%20Kuang-Ming%20Chen%20and%20Hung-yi%20Lee%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20in%20recent%20years%20has%0Alargely%20focused%20on%20English%2C%20resulting%20in%20models%20that%20respond%20exclusively%20in%0AEnglish.%20To%20adapt%20these%20models%20to%20other%20languages%2C%20continual%20pre-training%20%28CP%29%0Ais%20often%20employed%2C%20followed%20by%20supervised%20fine-tuning%20%28SFT%29%20to%20maintain%0Aconversational%20abilities.%20However%2C%20CP%20and%20SFT%20can%20reduce%20a%20model%27s%20ability%20to%0Afilter%20harmful%20content.%20We%20propose%20Instruction%20Continual%20Pre-training%20%28InsCP%29%2C%0Awhich%20integrates%20instruction%20tags%20into%20the%20CP%20process%20to%20prevent%20loss%20of%0Aconversational%20proficiency%20while%20acquiring%20new%20languages.%20Our%20experiments%0Ademonstrate%20that%20InsCP%20retains%20conversational%20and%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20abilities.%20Empirical%20evaluations%20on%20language%20alignment%2C%0Areliability%2C%20and%20knowledge%20benchmarks%20confirm%20the%20efficacy%20of%20InsCP.%20Notably%2C%0Athis%20approach%20requires%20only%200.1%20billion%20tokens%20of%20high-quality%0Ainstruction-following%20data%2C%20thereby%20reducing%20resource%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructionCP%253A%2520A%2520fast%2520approach%2520to%2520transfer%2520Large%2520Language%2520Models%2520into%250A%2520%2520target%2520language%26entry.906535625%3DKuang-Ming%2520Chen%2520and%2520Hung-yi%2520Lee%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520recent%2520years%2520has%250Alargely%2520focused%2520on%2520English%252C%2520resulting%2520in%2520models%2520that%2520respond%2520exclusively%2520in%250AEnglish.%2520To%2520adapt%2520these%2520models%2520to%2520other%2520languages%252C%2520continual%2520pre-training%2520%2528CP%2529%250Ais%2520often%2520employed%252C%2520followed%2520by%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520to%2520maintain%250Aconversational%2520abilities.%2520However%252C%2520CP%2520and%2520SFT%2520can%2520reduce%2520a%2520model%2527s%2520ability%2520to%250Afilter%2520harmful%2520content.%2520We%2520propose%2520Instruction%2520Continual%2520Pre-training%2520%2528InsCP%2529%252C%250Awhich%2520integrates%2520instruction%2520tags%2520into%2520the%2520CP%2520process%2520to%2520prevent%2520loss%2520of%250Aconversational%2520proficiency%2520while%2520acquiring%2520new%2520languages.%2520Our%2520experiments%250Ademonstrate%2520that%2520InsCP%2520retains%2520conversational%2520and%2520Reinforcement%2520Learning%2520from%250AHuman%2520Feedback%2520%2528RLHF%2529%2520abilities.%2520Empirical%2520evaluations%2520on%2520language%2520alignment%252C%250Areliability%252C%2520and%2520knowledge%2520benchmarks%2520confirm%2520the%2520efficacy%2520of%2520InsCP.%2520Notably%252C%250Athis%2520approach%2520requires%2520only%25200.1%2520billion%2520tokens%2520of%2520high-quality%250Ainstruction-following%2520data%252C%2520thereby%2520reducing%2520resource%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructionCP%3A%20A%20fast%20approach%20to%20transfer%20Large%20Language%20Models%20into%0A%20%20target%20language&entry.906535625=Kuang-Ming%20Chen%20and%20Hung-yi%20Lee&entry.1292438233=%20%20The%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20in%20recent%20years%20has%0Alargely%20focused%20on%20English%2C%20resulting%20in%20models%20that%20respond%20exclusively%20in%0AEnglish.%20To%20adapt%20these%20models%20to%20other%20languages%2C%20continual%20pre-training%20%28CP%29%0Ais%20often%20employed%2C%20followed%20by%20supervised%20fine-tuning%20%28SFT%29%20to%20maintain%0Aconversational%20abilities.%20However%2C%20CP%20and%20SFT%20can%20reduce%20a%20model%27s%20ability%20to%0Afilter%20harmful%20content.%20We%20propose%20Instruction%20Continual%20Pre-training%20%28InsCP%29%2C%0Awhich%20integrates%20instruction%20tags%20into%20the%20CP%20process%20to%20prevent%20loss%20of%0Aconversational%20proficiency%20while%20acquiring%20new%20languages.%20Our%20experiments%0Ademonstrate%20that%20InsCP%20retains%20conversational%20and%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20abilities.%20Empirical%20evaluations%20on%20language%20alignment%2C%0Areliability%2C%20and%20knowledge%20benchmarks%20confirm%20the%20efficacy%20of%20InsCP.%20Notably%2C%0Athis%20approach%20requires%20only%200.1%20billion%20tokens%20of%20high-quality%0Ainstruction-following%20data%2C%20thereby%20reducing%20resource%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20175v1&entry.124074799=Read"},
{"title": "Understanding Adam Optimizer via Online Learning of Updates: Adam is\n  FTRL in Disguise", "author": "Kwangjun Ahn and Zhiyu Zhang and Yunbum Kook and Yan Dai", "abstract": "  Despite the success of the Adam optimizer in practice, the theoretical\nunderstanding of its algorithmic components still remains limited. In\nparticular, most existing analyses of Adam show the convergence rate that can\nbe simply achieved by non-adative algorithms like SGD. In this work, we provide\na different perspective based on online learning that underscores the\nimportance of Adam's algorithmic components. Inspired by Cutkosky et al.\n(2023), we consider the framework called online learning of updates/increments,\nwhere we choose the updates/increments of an optimizer based on an online\nlearner. With this framework, the design of a good optimizer is reduced to the\ndesign of a good online learner. Our main observation is that Adam corresponds\nto a principled online learning framework called Follow-the-Regularized-Leader\n(FTRL). Building on this observation, we study the benefits of its algorithmic\ncomponents from the online learning perspective.\n", "link": "http://arxiv.org/abs/2402.01567v2", "date": "2024-05-30", "relevancy": 2.3713, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Adam%20Optimizer%20via%20Online%20Learning%20of%20Updates%3A%20Adam%20is%0A%20%20FTRL%20in%20Disguise&body=Title%3A%20Understanding%20Adam%20Optimizer%20via%20Online%20Learning%20of%20Updates%3A%20Adam%20is%0A%20%20FTRL%20in%20Disguise%0AAuthor%3A%20Kwangjun%20Ahn%20and%20Zhiyu%20Zhang%20and%20Yunbum%20Kook%20and%20Yan%20Dai%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20the%20Adam%20optimizer%20in%20practice%2C%20the%20theoretical%0Aunderstanding%20of%20its%20algorithmic%20components%20still%20remains%20limited.%20In%0Aparticular%2C%20most%20existing%20analyses%20of%20Adam%20show%20the%20convergence%20rate%20that%20can%0Abe%20simply%20achieved%20by%20non-adative%20algorithms%20like%20SGD.%20In%20this%20work%2C%20we%20provide%0Aa%20different%20perspective%20based%20on%20online%20learning%20that%20underscores%20the%0Aimportance%20of%20Adam%27s%20algorithmic%20components.%20Inspired%20by%20Cutkosky%20et%20al.%0A%282023%29%2C%20we%20consider%20the%20framework%20called%20online%20learning%20of%20updates/increments%2C%0Awhere%20we%20choose%20the%20updates/increments%20of%20an%20optimizer%20based%20on%20an%20online%0Alearner.%20With%20this%20framework%2C%20the%20design%20of%20a%20good%20optimizer%20is%20reduced%20to%20the%0Adesign%20of%20a%20good%20online%20learner.%20Our%20main%20observation%20is%20that%20Adam%20corresponds%0Ato%20a%20principled%20online%20learning%20framework%20called%20Follow-the-Regularized-Leader%0A%28FTRL%29.%20Building%20on%20this%20observation%2C%20we%20study%20the%20benefits%20of%20its%20algorithmic%0Acomponents%20from%20the%20online%20learning%20perspective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Adam%2520Optimizer%2520via%2520Online%2520Learning%2520of%2520Updates%253A%2520Adam%2520is%250A%2520%2520FTRL%2520in%2520Disguise%26entry.906535625%3DKwangjun%2520Ahn%2520and%2520Zhiyu%2520Zhang%2520and%2520Yunbum%2520Kook%2520and%2520Yan%2520Dai%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520of%2520the%2520Adam%2520optimizer%2520in%2520practice%252C%2520the%2520theoretical%250Aunderstanding%2520of%2520its%2520algorithmic%2520components%2520still%2520remains%2520limited.%2520In%250Aparticular%252C%2520most%2520existing%2520analyses%2520of%2520Adam%2520show%2520the%2520convergence%2520rate%2520that%2520can%250Abe%2520simply%2520achieved%2520by%2520non-adative%2520algorithms%2520like%2520SGD.%2520In%2520this%2520work%252C%2520we%2520provide%250Aa%2520different%2520perspective%2520based%2520on%2520online%2520learning%2520that%2520underscores%2520the%250Aimportance%2520of%2520Adam%2527s%2520algorithmic%2520components.%2520Inspired%2520by%2520Cutkosky%2520et%2520al.%250A%25282023%2529%252C%2520we%2520consider%2520the%2520framework%2520called%2520online%2520learning%2520of%2520updates/increments%252C%250Awhere%2520we%2520choose%2520the%2520updates/increments%2520of%2520an%2520optimizer%2520based%2520on%2520an%2520online%250Alearner.%2520With%2520this%2520framework%252C%2520the%2520design%2520of%2520a%2520good%2520optimizer%2520is%2520reduced%2520to%2520the%250Adesign%2520of%2520a%2520good%2520online%2520learner.%2520Our%2520main%2520observation%2520is%2520that%2520Adam%2520corresponds%250Ato%2520a%2520principled%2520online%2520learning%2520framework%2520called%2520Follow-the-Regularized-Leader%250A%2528FTRL%2529.%2520Building%2520on%2520this%2520observation%252C%2520we%2520study%2520the%2520benefits%2520of%2520its%2520algorithmic%250Acomponents%2520from%2520the%2520online%2520learning%2520perspective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Adam%20Optimizer%20via%20Online%20Learning%20of%20Updates%3A%20Adam%20is%0A%20%20FTRL%20in%20Disguise&entry.906535625=Kwangjun%20Ahn%20and%20Zhiyu%20Zhang%20and%20Yunbum%20Kook%20and%20Yan%20Dai&entry.1292438233=%20%20Despite%20the%20success%20of%20the%20Adam%20optimizer%20in%20practice%2C%20the%20theoretical%0Aunderstanding%20of%20its%20algorithmic%20components%20still%20remains%20limited.%20In%0Aparticular%2C%20most%20existing%20analyses%20of%20Adam%20show%20the%20convergence%20rate%20that%20can%0Abe%20simply%20achieved%20by%20non-adative%20algorithms%20like%20SGD.%20In%20this%20work%2C%20we%20provide%0Aa%20different%20perspective%20based%20on%20online%20learning%20that%20underscores%20the%0Aimportance%20of%20Adam%27s%20algorithmic%20components.%20Inspired%20by%20Cutkosky%20et%20al.%0A%282023%29%2C%20we%20consider%20the%20framework%20called%20online%20learning%20of%20updates/increments%2C%0Awhere%20we%20choose%20the%20updates/increments%20of%20an%20optimizer%20based%20on%20an%20online%0Alearner.%20With%20this%20framework%2C%20the%20design%20of%20a%20good%20optimizer%20is%20reduced%20to%20the%0Adesign%20of%20a%20good%20online%20learner.%20Our%20main%20observation%20is%20that%20Adam%20corresponds%0Ato%20a%20principled%20online%20learning%20framework%20called%20Follow-the-Regularized-Leader%0A%28FTRL%29.%20Building%20on%20this%20observation%2C%20we%20study%20the%20benefits%20of%20its%20algorithmic%0Acomponents%20from%20the%20online%20learning%20perspective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01567v2&entry.124074799=Read"},
{"title": "Near Optimal Decentralized Optimization with Compression and Momentum\n  Tracking", "author": "Rustem Islamov and Yuan Gao and Sebastian U. Stich", "abstract": "  Communication efficiency has garnered significant attention as it is\nconsidered the main bottleneck for large-scale decentralized Machine Learning\napplications in distributed and federated settings. In this regime, clients are\nrestricted to transmitting small amounts of quantized information to their\nneighbors over a communication graph. Numerous endeavors have been made to\naddress this challenging problem by developing algorithms with compressed\ncommunication for decentralized non-convex optimization problems. Despite\nconsiderable efforts, the current results suffer from various issues such as\nnon-scalability with the number of clients, requirements for large batches, or\nbounded gradient assumption. In this paper, we introduce MoTEF, a novel\napproach that integrates communication compression with Momentum Tracking and\nError Feedback. Our analysis demonstrates that MoTEF achieves most of the\ndesired properties, and significantly outperforms existing methods under\narbitrary data heterogeneity. We provide numerical experiments to validate our\ntheoretical findings and confirm the practical superiority of MoTEF.\n", "link": "http://arxiv.org/abs/2405.20114v1", "date": "2024-05-30", "relevancy": 2.3689, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4928}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4664}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near%20Optimal%20Decentralized%20Optimization%20with%20Compression%20and%20Momentum%0A%20%20Tracking&body=Title%3A%20Near%20Optimal%20Decentralized%20Optimization%20with%20Compression%20and%20Momentum%0A%20%20Tracking%0AAuthor%3A%20Rustem%20Islamov%20and%20Yuan%20Gao%20and%20Sebastian%20U.%20Stich%0AAbstract%3A%20%20%20Communication%20efficiency%20has%20garnered%20significant%20attention%20as%20it%20is%0Aconsidered%20the%20main%20bottleneck%20for%20large-scale%20decentralized%20Machine%20Learning%0Aapplications%20in%20distributed%20and%20federated%20settings.%20In%20this%20regime%2C%20clients%20are%0Arestricted%20to%20transmitting%20small%20amounts%20of%20quantized%20information%20to%20their%0Aneighbors%20over%20a%20communication%20graph.%20Numerous%20endeavors%20have%20been%20made%20to%0Aaddress%20this%20challenging%20problem%20by%20developing%20algorithms%20with%20compressed%0Acommunication%20for%20decentralized%20non-convex%20optimization%20problems.%20Despite%0Aconsiderable%20efforts%2C%20the%20current%20results%20suffer%20from%20various%20issues%20such%20as%0Anon-scalability%20with%20the%20number%20of%20clients%2C%20requirements%20for%20large%20batches%2C%20or%0Abounded%20gradient%20assumption.%20In%20this%20paper%2C%20we%20introduce%20MoTEF%2C%20a%20novel%0Aapproach%20that%20integrates%20communication%20compression%20with%20Momentum%20Tracking%20and%0AError%20Feedback.%20Our%20analysis%20demonstrates%20that%20MoTEF%20achieves%20most%20of%20the%0Adesired%20properties%2C%20and%20significantly%20outperforms%20existing%20methods%20under%0Aarbitrary%20data%20heterogeneity.%20We%20provide%20numerical%20experiments%20to%20validate%20our%0Atheoretical%20findings%20and%20confirm%20the%20practical%20superiority%20of%20MoTEF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear%2520Optimal%2520Decentralized%2520Optimization%2520with%2520Compression%2520and%2520Momentum%250A%2520%2520Tracking%26entry.906535625%3DRustem%2520Islamov%2520and%2520Yuan%2520Gao%2520and%2520Sebastian%2520U.%2520Stich%26entry.1292438233%3D%2520%2520Communication%2520efficiency%2520has%2520garnered%2520significant%2520attention%2520as%2520it%2520is%250Aconsidered%2520the%2520main%2520bottleneck%2520for%2520large-scale%2520decentralized%2520Machine%2520Learning%250Aapplications%2520in%2520distributed%2520and%2520federated%2520settings.%2520In%2520this%2520regime%252C%2520clients%2520are%250Arestricted%2520to%2520transmitting%2520small%2520amounts%2520of%2520quantized%2520information%2520to%2520their%250Aneighbors%2520over%2520a%2520communication%2520graph.%2520Numerous%2520endeavors%2520have%2520been%2520made%2520to%250Aaddress%2520this%2520challenging%2520problem%2520by%2520developing%2520algorithms%2520with%2520compressed%250Acommunication%2520for%2520decentralized%2520non-convex%2520optimization%2520problems.%2520Despite%250Aconsiderable%2520efforts%252C%2520the%2520current%2520results%2520suffer%2520from%2520various%2520issues%2520such%2520as%250Anon-scalability%2520with%2520the%2520number%2520of%2520clients%252C%2520requirements%2520for%2520large%2520batches%252C%2520or%250Abounded%2520gradient%2520assumption.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MoTEF%252C%2520a%2520novel%250Aapproach%2520that%2520integrates%2520communication%2520compression%2520with%2520Momentum%2520Tracking%2520and%250AError%2520Feedback.%2520Our%2520analysis%2520demonstrates%2520that%2520MoTEF%2520achieves%2520most%2520of%2520the%250Adesired%2520properties%252C%2520and%2520significantly%2520outperforms%2520existing%2520methods%2520under%250Aarbitrary%2520data%2520heterogeneity.%2520We%2520provide%2520numerical%2520experiments%2520to%2520validate%2520our%250Atheoretical%2520findings%2520and%2520confirm%2520the%2520practical%2520superiority%2520of%2520MoTEF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near%20Optimal%20Decentralized%20Optimization%20with%20Compression%20and%20Momentum%0A%20%20Tracking&entry.906535625=Rustem%20Islamov%20and%20Yuan%20Gao%20and%20Sebastian%20U.%20Stich&entry.1292438233=%20%20Communication%20efficiency%20has%20garnered%20significant%20attention%20as%20it%20is%0Aconsidered%20the%20main%20bottleneck%20for%20large-scale%20decentralized%20Machine%20Learning%0Aapplications%20in%20distributed%20and%20federated%20settings.%20In%20this%20regime%2C%20clients%20are%0Arestricted%20to%20transmitting%20small%20amounts%20of%20quantized%20information%20to%20their%0Aneighbors%20over%20a%20communication%20graph.%20Numerous%20endeavors%20have%20been%20made%20to%0Aaddress%20this%20challenging%20problem%20by%20developing%20algorithms%20with%20compressed%0Acommunication%20for%20decentralized%20non-convex%20optimization%20problems.%20Despite%0Aconsiderable%20efforts%2C%20the%20current%20results%20suffer%20from%20various%20issues%20such%20as%0Anon-scalability%20with%20the%20number%20of%20clients%2C%20requirements%20for%20large%20batches%2C%20or%0Abounded%20gradient%20assumption.%20In%20this%20paper%2C%20we%20introduce%20MoTEF%2C%20a%20novel%0Aapproach%20that%20integrates%20communication%20compression%20with%20Momentum%20Tracking%20and%0AError%20Feedback.%20Our%20analysis%20demonstrates%20that%20MoTEF%20achieves%20most%20of%20the%0Adesired%20properties%2C%20and%20significantly%20outperforms%20existing%20methods%20under%0Aarbitrary%20data%20heterogeneity.%20We%20provide%20numerical%20experiments%20to%20validate%20our%0Atheoretical%20findings%20and%20confirm%20the%20practical%20superiority%20of%20MoTEF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20114v1&entry.124074799=Read"},
{"title": "Applications of Generative AI (GAI) for Mobile and Wireless Networking:\n  A Survey", "author": "Thai-Hoc Vu and Senthil Kumar Jagatheesaperumal and Minh-Duong Nguyen and Nguyen Van Huynh and Sunghwan Kim and Quoc-Viet Pham", "abstract": "  The success of Artificial Intelligence (AI) in multiple disciplines and\nvertical domains in recent years has promoted the evolution of mobile\nnetworking and the future Internet toward an AI-integrated Internet-of-Things\n(IoT) era. Nevertheless, most AI techniques rely on data generated by physical\ndevices (e.g., mobile devices and network nodes) or specific applications\n(e.g., fitness trackers and mobile gaming). To bypass this circumvent,\nGenerative AI (GAI), a.k.a. AI-generated content (AIGC), has emerged as a\npowerful AI paradigm; thanks to its ability to efficiently learn complex data\ndistributions and generate synthetic data to represent the original data in\nvarious forms. This impressive feature is projected to transform the management\nof mobile networking and diversify the current services and applications\nprovided. On this basis, this work presents a concise tutorial on the role of\nGAIs in mobile and wireless networking. In particular, this survey first\nprovides the fundamentals of GAI and representative GAI models, serving as an\nessential preliminary to the understanding of the applications of GAI in mobile\nand wireless networking. Then, this work provides a comprehensive review of\nstate-of-the-art studies and GAI applications in network management, wireless\nsecurity, semantic communication, and lessons learned from the open literature.\nFinally, this work summarizes the current research on GAI for mobile and\nwireless networking by outlining important challenges that need to be resolved\nto facilitate the development and applicability of GAI in this edge-cutting\narea.\n", "link": "http://arxiv.org/abs/2405.20024v1", "date": "2024-05-30", "relevancy": 2.3562, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5303}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4449}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applications%20of%20Generative%20AI%20%28GAI%29%20for%20Mobile%20and%20Wireless%20Networking%3A%0A%20%20A%20Survey&body=Title%3A%20Applications%20of%20Generative%20AI%20%28GAI%29%20for%20Mobile%20and%20Wireless%20Networking%3A%0A%20%20A%20Survey%0AAuthor%3A%20Thai-Hoc%20Vu%20and%20Senthil%20Kumar%20Jagatheesaperumal%20and%20Minh-Duong%20Nguyen%20and%20Nguyen%20Van%20Huynh%20and%20Sunghwan%20Kim%20and%20Quoc-Viet%20Pham%0AAbstract%3A%20%20%20The%20success%20of%20Artificial%20Intelligence%20%28AI%29%20in%20multiple%20disciplines%20and%0Avertical%20domains%20in%20recent%20years%20has%20promoted%20the%20evolution%20of%20mobile%0Anetworking%20and%20the%20future%20Internet%20toward%20an%20AI-integrated%20Internet-of-Things%0A%28IoT%29%20era.%20Nevertheless%2C%20most%20AI%20techniques%20rely%20on%20data%20generated%20by%20physical%0Adevices%20%28e.g.%2C%20mobile%20devices%20and%20network%20nodes%29%20or%20specific%20applications%0A%28e.g.%2C%20fitness%20trackers%20and%20mobile%20gaming%29.%20To%20bypass%20this%20circumvent%2C%0AGenerative%20AI%20%28GAI%29%2C%20a.k.a.%20AI-generated%20content%20%28AIGC%29%2C%20has%20emerged%20as%20a%0Apowerful%20AI%20paradigm%3B%20thanks%20to%20its%20ability%20to%20efficiently%20learn%20complex%20data%0Adistributions%20and%20generate%20synthetic%20data%20to%20represent%20the%20original%20data%20in%0Avarious%20forms.%20This%20impressive%20feature%20is%20projected%20to%20transform%20the%20management%0Aof%20mobile%20networking%20and%20diversify%20the%20current%20services%20and%20applications%0Aprovided.%20On%20this%20basis%2C%20this%20work%20presents%20a%20concise%20tutorial%20on%20the%20role%20of%0AGAIs%20in%20mobile%20and%20wireless%20networking.%20In%20particular%2C%20this%20survey%20first%0Aprovides%20the%20fundamentals%20of%20GAI%20and%20representative%20GAI%20models%2C%20serving%20as%20an%0Aessential%20preliminary%20to%20the%20understanding%20of%20the%20applications%20of%20GAI%20in%20mobile%0Aand%20wireless%20networking.%20Then%2C%20this%20work%20provides%20a%20comprehensive%20review%20of%0Astate-of-the-art%20studies%20and%20GAI%20applications%20in%20network%20management%2C%20wireless%0Asecurity%2C%20semantic%20communication%2C%20and%20lessons%20learned%20from%20the%20open%20literature.%0AFinally%2C%20this%20work%20summarizes%20the%20current%20research%20on%20GAI%20for%20mobile%20and%0Awireless%20networking%20by%20outlining%20important%20challenges%20that%20need%20to%20be%20resolved%0Ato%20facilitate%20the%20development%20and%20applicability%20of%20GAI%20in%20this%20edge-cutting%0Aarea.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplications%2520of%2520Generative%2520AI%2520%2528GAI%2529%2520for%2520Mobile%2520and%2520Wireless%2520Networking%253A%250A%2520%2520A%2520Survey%26entry.906535625%3DThai-Hoc%2520Vu%2520and%2520Senthil%2520Kumar%2520Jagatheesaperumal%2520and%2520Minh-Duong%2520Nguyen%2520and%2520Nguyen%2520Van%2520Huynh%2520and%2520Sunghwan%2520Kim%2520and%2520Quoc-Viet%2520Pham%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520in%2520multiple%2520disciplines%2520and%250Avertical%2520domains%2520in%2520recent%2520years%2520has%2520promoted%2520the%2520evolution%2520of%2520mobile%250Anetworking%2520and%2520the%2520future%2520Internet%2520toward%2520an%2520AI-integrated%2520Internet-of-Things%250A%2528IoT%2529%2520era.%2520Nevertheless%252C%2520most%2520AI%2520techniques%2520rely%2520on%2520data%2520generated%2520by%2520physical%250Adevices%2520%2528e.g.%252C%2520mobile%2520devices%2520and%2520network%2520nodes%2529%2520or%2520specific%2520applications%250A%2528e.g.%252C%2520fitness%2520trackers%2520and%2520mobile%2520gaming%2529.%2520To%2520bypass%2520this%2520circumvent%252C%250AGenerative%2520AI%2520%2528GAI%2529%252C%2520a.k.a.%2520AI-generated%2520content%2520%2528AIGC%2529%252C%2520has%2520emerged%2520as%2520a%250Apowerful%2520AI%2520paradigm%253B%2520thanks%2520to%2520its%2520ability%2520to%2520efficiently%2520learn%2520complex%2520data%250Adistributions%2520and%2520generate%2520synthetic%2520data%2520to%2520represent%2520the%2520original%2520data%2520in%250Avarious%2520forms.%2520This%2520impressive%2520feature%2520is%2520projected%2520to%2520transform%2520the%2520management%250Aof%2520mobile%2520networking%2520and%2520diversify%2520the%2520current%2520services%2520and%2520applications%250Aprovided.%2520On%2520this%2520basis%252C%2520this%2520work%2520presents%2520a%2520concise%2520tutorial%2520on%2520the%2520role%2520of%250AGAIs%2520in%2520mobile%2520and%2520wireless%2520networking.%2520In%2520particular%252C%2520this%2520survey%2520first%250Aprovides%2520the%2520fundamentals%2520of%2520GAI%2520and%2520representative%2520GAI%2520models%252C%2520serving%2520as%2520an%250Aessential%2520preliminary%2520to%2520the%2520understanding%2520of%2520the%2520applications%2520of%2520GAI%2520in%2520mobile%250Aand%2520wireless%2520networking.%2520Then%252C%2520this%2520work%2520provides%2520a%2520comprehensive%2520review%2520of%250Astate-of-the-art%2520studies%2520and%2520GAI%2520applications%2520in%2520network%2520management%252C%2520wireless%250Asecurity%252C%2520semantic%2520communication%252C%2520and%2520lessons%2520learned%2520from%2520the%2520open%2520literature.%250AFinally%252C%2520this%2520work%2520summarizes%2520the%2520current%2520research%2520on%2520GAI%2520for%2520mobile%2520and%250Awireless%2520networking%2520by%2520outlining%2520important%2520challenges%2520that%2520need%2520to%2520be%2520resolved%250Ato%2520facilitate%2520the%2520development%2520and%2520applicability%2520of%2520GAI%2520in%2520this%2520edge-cutting%250Aarea.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applications%20of%20Generative%20AI%20%28GAI%29%20for%20Mobile%20and%20Wireless%20Networking%3A%0A%20%20A%20Survey&entry.906535625=Thai-Hoc%20Vu%20and%20Senthil%20Kumar%20Jagatheesaperumal%20and%20Minh-Duong%20Nguyen%20and%20Nguyen%20Van%20Huynh%20and%20Sunghwan%20Kim%20and%20Quoc-Viet%20Pham&entry.1292438233=%20%20The%20success%20of%20Artificial%20Intelligence%20%28AI%29%20in%20multiple%20disciplines%20and%0Avertical%20domains%20in%20recent%20years%20has%20promoted%20the%20evolution%20of%20mobile%0Anetworking%20and%20the%20future%20Internet%20toward%20an%20AI-integrated%20Internet-of-Things%0A%28IoT%29%20era.%20Nevertheless%2C%20most%20AI%20techniques%20rely%20on%20data%20generated%20by%20physical%0Adevices%20%28e.g.%2C%20mobile%20devices%20and%20network%20nodes%29%20or%20specific%20applications%0A%28e.g.%2C%20fitness%20trackers%20and%20mobile%20gaming%29.%20To%20bypass%20this%20circumvent%2C%0AGenerative%20AI%20%28GAI%29%2C%20a.k.a.%20AI-generated%20content%20%28AIGC%29%2C%20has%20emerged%20as%20a%0Apowerful%20AI%20paradigm%3B%20thanks%20to%20its%20ability%20to%20efficiently%20learn%20complex%20data%0Adistributions%20and%20generate%20synthetic%20data%20to%20represent%20the%20original%20data%20in%0Avarious%20forms.%20This%20impressive%20feature%20is%20projected%20to%20transform%20the%20management%0Aof%20mobile%20networking%20and%20diversify%20the%20current%20services%20and%20applications%0Aprovided.%20On%20this%20basis%2C%20this%20work%20presents%20a%20concise%20tutorial%20on%20the%20role%20of%0AGAIs%20in%20mobile%20and%20wireless%20networking.%20In%20particular%2C%20this%20survey%20first%0Aprovides%20the%20fundamentals%20of%20GAI%20and%20representative%20GAI%20models%2C%20serving%20as%20an%0Aessential%20preliminary%20to%20the%20understanding%20of%20the%20applications%20of%20GAI%20in%20mobile%0Aand%20wireless%20networking.%20Then%2C%20this%20work%20provides%20a%20comprehensive%20review%20of%0Astate-of-the-art%20studies%20and%20GAI%20applications%20in%20network%20management%2C%20wireless%0Asecurity%2C%20semantic%20communication%2C%20and%20lessons%20learned%20from%20the%20open%20literature.%0AFinally%2C%20this%20work%20summarizes%20the%20current%20research%20on%20GAI%20for%20mobile%20and%0Awireless%20networking%20by%20outlining%20important%20challenges%20that%20need%20to%20be%20resolved%0Ato%20facilitate%20the%20development%20and%20applicability%20of%20GAI%20in%20this%20edge-cutting%0Aarea.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20024v1&entry.124074799=Read"},
{"title": "DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music\n  Generation", "author": "Zachary Novack and Julian McAuley and Taylor Berg-Kirkpatrick and Nicholas Bryan", "abstract": "  Controllable music generation methods are critical for human-centered\nAI-based music creation, but are currently limited by speed, quality, and\ncontrol design trade-offs. Diffusion Inference-Time T-optimization (DITTO), in\nparticular, offers state-of-the-art results, but is over 10x slower than\nreal-time, limiting practical use. We propose Distilled Diffusion\nInference-Time T -Optimization (or DITTO-2), a new method to speed up\ninference-time optimization-based control and unlock faster-than-real-time\ngeneration for a wide-variety of applications such as music inpainting,\noutpainting, intensity, melody, and musical structure control. Our method works\nby (1) distilling a pre-trained diffusion model for fast sampling via an\nefficient, modified consistency or consistency trajectory distillation process\n(2) performing inference-time optimization using our distilled model with\none-step sampling as an efficient surrogate optimization task and (3) running a\nfinal multi-step sampling generation (decoding) using our estimated noise\nlatents for best-quality, fast, controllable generation. Through thorough\nevaluation, we find our method not only speeds up generation over 10-20x, but\nsimultaneously improves control adherence and generation quality all at once.\nFurthermore, we apply our approach to a new application of maximizing text\nadherence (CLAP score) and show we can convert an unconditional diffusion model\nwithout text inputs into a model that yields state-of-the-art text control.\nSound examples can be found at https://ditto-music.github.io/ditto2/.\n", "link": "http://arxiv.org/abs/2405.20289v1", "date": "2024-05-30", "relevancy": 2.3462, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5872}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DITTO-2%3A%20Distilled%20Diffusion%20Inference-Time%20T-Optimization%20for%20Music%0A%20%20Generation&body=Title%3A%20DITTO-2%3A%20Distilled%20Diffusion%20Inference-Time%20T-Optimization%20for%20Music%0A%20%20Generation%0AAuthor%3A%20Zachary%20Novack%20and%20Julian%20McAuley%20and%20Taylor%20Berg-Kirkpatrick%20and%20Nicholas%20Bryan%0AAbstract%3A%20%20%20Controllable%20music%20generation%20methods%20are%20critical%20for%20human-centered%0AAI-based%20music%20creation%2C%20but%20are%20currently%20limited%20by%20speed%2C%20quality%2C%20and%0Acontrol%20design%20trade-offs.%20Diffusion%20Inference-Time%20T-optimization%20%28DITTO%29%2C%20in%0Aparticular%2C%20offers%20state-of-the-art%20results%2C%20but%20is%20over%2010x%20slower%20than%0Areal-time%2C%20limiting%20practical%20use.%20We%20propose%20Distilled%20Diffusion%0AInference-Time%20T%20-Optimization%20%28or%20DITTO-2%29%2C%20a%20new%20method%20to%20speed%20up%0Ainference-time%20optimization-based%20control%20and%20unlock%20faster-than-real-time%0Ageneration%20for%20a%20wide-variety%20of%20applications%20such%20as%20music%20inpainting%2C%0Aoutpainting%2C%20intensity%2C%20melody%2C%20and%20musical%20structure%20control.%20Our%20method%20works%0Aby%20%281%29%20distilling%20a%20pre-trained%20diffusion%20model%20for%20fast%20sampling%20via%20an%0Aefficient%2C%20modified%20consistency%20or%20consistency%20trajectory%20distillation%20process%0A%282%29%20performing%20inference-time%20optimization%20using%20our%20distilled%20model%20with%0Aone-step%20sampling%20as%20an%20efficient%20surrogate%20optimization%20task%20and%20%283%29%20running%20a%0Afinal%20multi-step%20sampling%20generation%20%28decoding%29%20using%20our%20estimated%20noise%0Alatents%20for%20best-quality%2C%20fast%2C%20controllable%20generation.%20Through%20thorough%0Aevaluation%2C%20we%20find%20our%20method%20not%20only%20speeds%20up%20generation%20over%2010-20x%2C%20but%0Asimultaneously%20improves%20control%20adherence%20and%20generation%20quality%20all%20at%20once.%0AFurthermore%2C%20we%20apply%20our%20approach%20to%20a%20new%20application%20of%20maximizing%20text%0Aadherence%20%28CLAP%20score%29%20and%20show%20we%20can%20convert%20an%20unconditional%20diffusion%20model%0Awithout%20text%20inputs%20into%20a%20model%20that%20yields%20state-of-the-art%20text%20control.%0ASound%20examples%20can%20be%20found%20at%20https%3A//ditto-music.github.io/ditto2/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDITTO-2%253A%2520Distilled%2520Diffusion%2520Inference-Time%2520T-Optimization%2520for%2520Music%250A%2520%2520Generation%26entry.906535625%3DZachary%2520Novack%2520and%2520Julian%2520McAuley%2520and%2520Taylor%2520Berg-Kirkpatrick%2520and%2520Nicholas%2520Bryan%26entry.1292438233%3D%2520%2520Controllable%2520music%2520generation%2520methods%2520are%2520critical%2520for%2520human-centered%250AAI-based%2520music%2520creation%252C%2520but%2520are%2520currently%2520limited%2520by%2520speed%252C%2520quality%252C%2520and%250Acontrol%2520design%2520trade-offs.%2520Diffusion%2520Inference-Time%2520T-optimization%2520%2528DITTO%2529%252C%2520in%250Aparticular%252C%2520offers%2520state-of-the-art%2520results%252C%2520but%2520is%2520over%252010x%2520slower%2520than%250Areal-time%252C%2520limiting%2520practical%2520use.%2520We%2520propose%2520Distilled%2520Diffusion%250AInference-Time%2520T%2520-Optimization%2520%2528or%2520DITTO-2%2529%252C%2520a%2520new%2520method%2520to%2520speed%2520up%250Ainference-time%2520optimization-based%2520control%2520and%2520unlock%2520faster-than-real-time%250Ageneration%2520for%2520a%2520wide-variety%2520of%2520applications%2520such%2520as%2520music%2520inpainting%252C%250Aoutpainting%252C%2520intensity%252C%2520melody%252C%2520and%2520musical%2520structure%2520control.%2520Our%2520method%2520works%250Aby%2520%25281%2529%2520distilling%2520a%2520pre-trained%2520diffusion%2520model%2520for%2520fast%2520sampling%2520via%2520an%250Aefficient%252C%2520modified%2520consistency%2520or%2520consistency%2520trajectory%2520distillation%2520process%250A%25282%2529%2520performing%2520inference-time%2520optimization%2520using%2520our%2520distilled%2520model%2520with%250Aone-step%2520sampling%2520as%2520an%2520efficient%2520surrogate%2520optimization%2520task%2520and%2520%25283%2529%2520running%2520a%250Afinal%2520multi-step%2520sampling%2520generation%2520%2528decoding%2529%2520using%2520our%2520estimated%2520noise%250Alatents%2520for%2520best-quality%252C%2520fast%252C%2520controllable%2520generation.%2520Through%2520thorough%250Aevaluation%252C%2520we%2520find%2520our%2520method%2520not%2520only%2520speeds%2520up%2520generation%2520over%252010-20x%252C%2520but%250Asimultaneously%2520improves%2520control%2520adherence%2520and%2520generation%2520quality%2520all%2520at%2520once.%250AFurthermore%252C%2520we%2520apply%2520our%2520approach%2520to%2520a%2520new%2520application%2520of%2520maximizing%2520text%250Aadherence%2520%2528CLAP%2520score%2529%2520and%2520show%2520we%2520can%2520convert%2520an%2520unconditional%2520diffusion%2520model%250Awithout%2520text%2520inputs%2520into%2520a%2520model%2520that%2520yields%2520state-of-the-art%2520text%2520control.%250ASound%2520examples%2520can%2520be%2520found%2520at%2520https%253A//ditto-music.github.io/ditto2/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DITTO-2%3A%20Distilled%20Diffusion%20Inference-Time%20T-Optimization%20for%20Music%0A%20%20Generation&entry.906535625=Zachary%20Novack%20and%20Julian%20McAuley%20and%20Taylor%20Berg-Kirkpatrick%20and%20Nicholas%20Bryan&entry.1292438233=%20%20Controllable%20music%20generation%20methods%20are%20critical%20for%20human-centered%0AAI-based%20music%20creation%2C%20but%20are%20currently%20limited%20by%20speed%2C%20quality%2C%20and%0Acontrol%20design%20trade-offs.%20Diffusion%20Inference-Time%20T-optimization%20%28DITTO%29%2C%20in%0Aparticular%2C%20offers%20state-of-the-art%20results%2C%20but%20is%20over%2010x%20slower%20than%0Areal-time%2C%20limiting%20practical%20use.%20We%20propose%20Distilled%20Diffusion%0AInference-Time%20T%20-Optimization%20%28or%20DITTO-2%29%2C%20a%20new%20method%20to%20speed%20up%0Ainference-time%20optimization-based%20control%20and%20unlock%20faster-than-real-time%0Ageneration%20for%20a%20wide-variety%20of%20applications%20such%20as%20music%20inpainting%2C%0Aoutpainting%2C%20intensity%2C%20melody%2C%20and%20musical%20structure%20control.%20Our%20method%20works%0Aby%20%281%29%20distilling%20a%20pre-trained%20diffusion%20model%20for%20fast%20sampling%20via%20an%0Aefficient%2C%20modified%20consistency%20or%20consistency%20trajectory%20distillation%20process%0A%282%29%20performing%20inference-time%20optimization%20using%20our%20distilled%20model%20with%0Aone-step%20sampling%20as%20an%20efficient%20surrogate%20optimization%20task%20and%20%283%29%20running%20a%0Afinal%20multi-step%20sampling%20generation%20%28decoding%29%20using%20our%20estimated%20noise%0Alatents%20for%20best-quality%2C%20fast%2C%20controllable%20generation.%20Through%20thorough%0Aevaluation%2C%20we%20find%20our%20method%20not%20only%20speeds%20up%20generation%20over%2010-20x%2C%20but%0Asimultaneously%20improves%20control%20adherence%20and%20generation%20quality%20all%20at%20once.%0AFurthermore%2C%20we%20apply%20our%20approach%20to%20a%20new%20application%20of%20maximizing%20text%0Aadherence%20%28CLAP%20score%29%20and%20show%20we%20can%20convert%20an%20unconditional%20diffusion%20model%0Awithout%20text%20inputs%20into%20a%20model%20that%20yields%20state-of-the-art%20text%20control.%0ASound%20examples%20can%20be%20found%20at%20https%3A//ditto-music.github.io/ditto2/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20289v1&entry.124074799=Read"},
{"title": "KerasCV and KerasNLP: Vision and Language Power-Ups", "author": "Matthew Watson and Divyashree Shivakumar Sreepathihalli and Francois Chollet and Martin Gorner and Kiranbir Sodhia and Ramesh Sampath and Tirth Patel and Haifeng Jin and Neel Kovelamudi and Gabriel Rasskin and Samaneh Saadat and Luke Wood and Chen Qian and Jonathan Bischof and Ian Stenbit", "abstract": "  We present the Keras domain packages KerasCV and KerasNLP, extensions of the\nKeras API for Computer Vision and Natural Language Processing workflows,\ncapable of running on either JAX, TensorFlow, or PyTorch. These domain packages\nare designed to enable fast experimentation, with a focus on ease-of-use and\nperformance. We adopt a modular, layered design: at the library's lowest level\nof abstraction, we provide building blocks for creating models and data\npreprocessing pipelines, and at the library's highest level of abstraction, we\nprovide pretrained ``task\" models for popular architectures such as Stable\nDiffusion, YOLOv8, GPT2, BERT, Mistral, CLIP, Gemma, T5, etc. Task models have\nbuilt-in preprocessing, pretrained weights, and can be fine-tuned on raw\ninputs. To enable efficient training, we support XLA compilation for all\nmodels, and run all preprocessing via a compiled graph of TensorFlow operations\nusing the tf.data API. The libraries are fully open-source (Apache 2.0 license)\nand available on GitHub.\n", "link": "http://arxiv.org/abs/2405.20247v1", "date": "2024-05-30", "relevancy": 2.3423, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4971}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4609}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KerasCV%20and%20KerasNLP%3A%20Vision%20and%20Language%20Power-Ups&body=Title%3A%20KerasCV%20and%20KerasNLP%3A%20Vision%20and%20Language%20Power-Ups%0AAuthor%3A%20Matthew%20Watson%20and%20Divyashree%20Shivakumar%20Sreepathihalli%20and%20Francois%20Chollet%20and%20Martin%20Gorner%20and%20Kiranbir%20Sodhia%20and%20Ramesh%20Sampath%20and%20Tirth%20Patel%20and%20Haifeng%20Jin%20and%20Neel%20Kovelamudi%20and%20Gabriel%20Rasskin%20and%20Samaneh%20Saadat%20and%20Luke%20Wood%20and%20Chen%20Qian%20and%20Jonathan%20Bischof%20and%20Ian%20Stenbit%0AAbstract%3A%20%20%20We%20present%20the%20Keras%20domain%20packages%20KerasCV%20and%20KerasNLP%2C%20extensions%20of%20the%0AKeras%20API%20for%20Computer%20Vision%20and%20Natural%20Language%20Processing%20workflows%2C%0Acapable%20of%20running%20on%20either%20JAX%2C%20TensorFlow%2C%20or%20PyTorch.%20These%20domain%20packages%0Aare%20designed%20to%20enable%20fast%20experimentation%2C%20with%20a%20focus%20on%20ease-of-use%20and%0Aperformance.%20We%20adopt%20a%20modular%2C%20layered%20design%3A%20at%20the%20library%27s%20lowest%20level%0Aof%20abstraction%2C%20we%20provide%20building%20blocks%20for%20creating%20models%20and%20data%0Apreprocessing%20pipelines%2C%20and%20at%20the%20library%27s%20highest%20level%20of%20abstraction%2C%20we%0Aprovide%20pretrained%20%60%60task%22%20models%20for%20popular%20architectures%20such%20as%20Stable%0ADiffusion%2C%20YOLOv8%2C%20GPT2%2C%20BERT%2C%20Mistral%2C%20CLIP%2C%20Gemma%2C%20T5%2C%20etc.%20Task%20models%20have%0Abuilt-in%20preprocessing%2C%20pretrained%20weights%2C%20and%20can%20be%20fine-tuned%20on%20raw%0Ainputs.%20To%20enable%20efficient%20training%2C%20we%20support%20XLA%20compilation%20for%20all%0Amodels%2C%20and%20run%20all%20preprocessing%20via%20a%20compiled%20graph%20of%20TensorFlow%20operations%0Ausing%20the%20tf.data%20API.%20The%20libraries%20are%20fully%20open-source%20%28Apache%202.0%20license%29%0Aand%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKerasCV%2520and%2520KerasNLP%253A%2520Vision%2520and%2520Language%2520Power-Ups%26entry.906535625%3DMatthew%2520Watson%2520and%2520Divyashree%2520Shivakumar%2520Sreepathihalli%2520and%2520Francois%2520Chollet%2520and%2520Martin%2520Gorner%2520and%2520Kiranbir%2520Sodhia%2520and%2520Ramesh%2520Sampath%2520and%2520Tirth%2520Patel%2520and%2520Haifeng%2520Jin%2520and%2520Neel%2520Kovelamudi%2520and%2520Gabriel%2520Rasskin%2520and%2520Samaneh%2520Saadat%2520and%2520Luke%2520Wood%2520and%2520Chen%2520Qian%2520and%2520Jonathan%2520Bischof%2520and%2520Ian%2520Stenbit%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Keras%2520domain%2520packages%2520KerasCV%2520and%2520KerasNLP%252C%2520extensions%2520of%2520the%250AKeras%2520API%2520for%2520Computer%2520Vision%2520and%2520Natural%2520Language%2520Processing%2520workflows%252C%250Acapable%2520of%2520running%2520on%2520either%2520JAX%252C%2520TensorFlow%252C%2520or%2520PyTorch.%2520These%2520domain%2520packages%250Aare%2520designed%2520to%2520enable%2520fast%2520experimentation%252C%2520with%2520a%2520focus%2520on%2520ease-of-use%2520and%250Aperformance.%2520We%2520adopt%2520a%2520modular%252C%2520layered%2520design%253A%2520at%2520the%2520library%2527s%2520lowest%2520level%250Aof%2520abstraction%252C%2520we%2520provide%2520building%2520blocks%2520for%2520creating%2520models%2520and%2520data%250Apreprocessing%2520pipelines%252C%2520and%2520at%2520the%2520library%2527s%2520highest%2520level%2520of%2520abstraction%252C%2520we%250Aprovide%2520pretrained%2520%2560%2560task%2522%2520models%2520for%2520popular%2520architectures%2520such%2520as%2520Stable%250ADiffusion%252C%2520YOLOv8%252C%2520GPT2%252C%2520BERT%252C%2520Mistral%252C%2520CLIP%252C%2520Gemma%252C%2520T5%252C%2520etc.%2520Task%2520models%2520have%250Abuilt-in%2520preprocessing%252C%2520pretrained%2520weights%252C%2520and%2520can%2520be%2520fine-tuned%2520on%2520raw%250Ainputs.%2520To%2520enable%2520efficient%2520training%252C%2520we%2520support%2520XLA%2520compilation%2520for%2520all%250Amodels%252C%2520and%2520run%2520all%2520preprocessing%2520via%2520a%2520compiled%2520graph%2520of%2520TensorFlow%2520operations%250Ausing%2520the%2520tf.data%2520API.%2520The%2520libraries%2520are%2520fully%2520open-source%2520%2528Apache%25202.0%2520license%2529%250Aand%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KerasCV%20and%20KerasNLP%3A%20Vision%20and%20Language%20Power-Ups&entry.906535625=Matthew%20Watson%20and%20Divyashree%20Shivakumar%20Sreepathihalli%20and%20Francois%20Chollet%20and%20Martin%20Gorner%20and%20Kiranbir%20Sodhia%20and%20Ramesh%20Sampath%20and%20Tirth%20Patel%20and%20Haifeng%20Jin%20and%20Neel%20Kovelamudi%20and%20Gabriel%20Rasskin%20and%20Samaneh%20Saadat%20and%20Luke%20Wood%20and%20Chen%20Qian%20and%20Jonathan%20Bischof%20and%20Ian%20Stenbit&entry.1292438233=%20%20We%20present%20the%20Keras%20domain%20packages%20KerasCV%20and%20KerasNLP%2C%20extensions%20of%20the%0AKeras%20API%20for%20Computer%20Vision%20and%20Natural%20Language%20Processing%20workflows%2C%0Acapable%20of%20running%20on%20either%20JAX%2C%20TensorFlow%2C%20or%20PyTorch.%20These%20domain%20packages%0Aare%20designed%20to%20enable%20fast%20experimentation%2C%20with%20a%20focus%20on%20ease-of-use%20and%0Aperformance.%20We%20adopt%20a%20modular%2C%20layered%20design%3A%20at%20the%20library%27s%20lowest%20level%0Aof%20abstraction%2C%20we%20provide%20building%20blocks%20for%20creating%20models%20and%20data%0Apreprocessing%20pipelines%2C%20and%20at%20the%20library%27s%20highest%20level%20of%20abstraction%2C%20we%0Aprovide%20pretrained%20%60%60task%22%20models%20for%20popular%20architectures%20such%20as%20Stable%0ADiffusion%2C%20YOLOv8%2C%20GPT2%2C%20BERT%2C%20Mistral%2C%20CLIP%2C%20Gemma%2C%20T5%2C%20etc.%20Task%20models%20have%0Abuilt-in%20preprocessing%2C%20pretrained%20weights%2C%20and%20can%20be%20fine-tuned%20on%20raw%0Ainputs.%20To%20enable%20efficient%20training%2C%20we%20support%20XLA%20compilation%20for%20all%0Amodels%2C%20and%20run%20all%20preprocessing%20via%20a%20compiled%20graph%20of%20TensorFlow%20operations%0Ausing%20the%20tf.data%20API.%20The%20libraries%20are%20fully%20open-source%20%28Apache%202.0%20license%29%0Aand%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20247v1&entry.124074799=Read"},
{"title": "DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in\n  the Wild", "author": "Honghao Fu and Yufei Wang and Wenhan Yang and Bihan Wen", "abstract": "  Image quality assessment (IQA) plays a critical role in selecting\nhigh-quality images and guiding compression and enhancement methods in a series\nof applications. The blind IQA, which assesses the quality of in-the-wild\nimages containing complex authentic distortions without reference images, poses\ngreater challenges. Existing methods are limited to modeling a uniform\ndistribution with local patches and are bothered by the gap between low and\nhigh-level visions (caused by widely adopted pre-trained classification\nnetworks). In this paper, we propose a novel IQA method called diffusion\npriors-based IQA (DP-IQA), which leverages the prior knowledge from the\npre-trained diffusion model with its excellent powers to bridge semantic gaps\nin the perception of the visual quality of images. Specifically, we use\npre-trained stable diffusion as the backbone, extract multi-level features from\nthe denoising U-Net during the upsampling process at a specified timestep, and\ndecode them to estimate the image quality score. The text and image adapters\nare adopted to mitigate the domain gap for downstream tasks and correct the\ninformation loss caused by the variational autoencoder bottleneck. Finally, we\ndistill the knowledge in the above model into a CNN-based student model,\nsignificantly reducing the parameter to enhance applicability, with the student\nmodel performing similarly or even better than the teacher model surprisingly.\nExperimental results demonstrate that our DP-IQA achieves state-of-the-art\nresults on various in-the-wild datasets with better generalization capability,\nwhich shows the superiority of our method in global modeling and utilizing the\nhierarchical feature clues of diffusion for evaluating image quality.\n", "link": "http://arxiv.org/abs/2405.19996v1", "date": "2024-05-30", "relevancy": 2.2934, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6329}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5632}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild&body=Title%3A%20DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild%0AAuthor%3A%20Honghao%20Fu%20and%20Yufei%20Wang%20and%20Wenhan%20Yang%20and%20Bihan%20Wen%0AAbstract%3A%20%20%20Image%20quality%20assessment%20%28IQA%29%20plays%20a%20critical%20role%20in%20selecting%0Ahigh-quality%20images%20and%20guiding%20compression%20and%20enhancement%20methods%20in%20a%20series%0Aof%20applications.%20The%20blind%20IQA%2C%20which%20assesses%20the%20quality%20of%20in-the-wild%0Aimages%20containing%20complex%20authentic%20distortions%20without%20reference%20images%2C%20poses%0Agreater%20challenges.%20Existing%20methods%20are%20limited%20to%20modeling%20a%20uniform%0Adistribution%20with%20local%20patches%20and%20are%20bothered%20by%20the%20gap%20between%20low%20and%0Ahigh-level%20visions%20%28caused%20by%20widely%20adopted%20pre-trained%20classification%0Anetworks%29.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20IQA%20method%20called%20diffusion%0Apriors-based%20IQA%20%28DP-IQA%29%2C%20which%20leverages%20the%20prior%20knowledge%20from%20the%0Apre-trained%20diffusion%20model%20with%20its%20excellent%20powers%20to%20bridge%20semantic%20gaps%0Ain%20the%20perception%20of%20the%20visual%20quality%20of%20images.%20Specifically%2C%20we%20use%0Apre-trained%20stable%20diffusion%20as%20the%20backbone%2C%20extract%20multi-level%20features%20from%0Athe%20denoising%20U-Net%20during%20the%20upsampling%20process%20at%20a%20specified%20timestep%2C%20and%0Adecode%20them%20to%20estimate%20the%20image%20quality%20score.%20The%20text%20and%20image%20adapters%0Aare%20adopted%20to%20mitigate%20the%20domain%20gap%20for%20downstream%20tasks%20and%20correct%20the%0Ainformation%20loss%20caused%20by%20the%20variational%20autoencoder%20bottleneck.%20Finally%2C%20we%0Adistill%20the%20knowledge%20in%20the%20above%20model%20into%20a%20CNN-based%20student%20model%2C%0Asignificantly%20reducing%20the%20parameter%20to%20enhance%20applicability%2C%20with%20the%20student%0Amodel%20performing%20similarly%20or%20even%20better%20than%20the%20teacher%20model%20surprisingly.%0AExperimental%20results%20demonstrate%20that%20our%20DP-IQA%20achieves%20state-of-the-art%0Aresults%20on%20various%20in-the-wild%20datasets%20with%20better%20generalization%20capability%2C%0Awhich%20shows%20the%20superiority%20of%20our%20method%20in%20global%20modeling%20and%20utilizing%20the%0Ahierarchical%20feature%20clues%20of%20diffusion%20for%20evaluating%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-IQA%253A%2520Utilizing%2520Diffusion%2520Prior%2520for%2520Blind%2520Image%2520Quality%2520Assessment%2520in%250A%2520%2520the%2520Wild%26entry.906535625%3DHonghao%2520Fu%2520and%2520Yufei%2520Wang%2520and%2520Wenhan%2520Yang%2520and%2520Bihan%2520Wen%26entry.1292438233%3D%2520%2520Image%2520quality%2520assessment%2520%2528IQA%2529%2520plays%2520a%2520critical%2520role%2520in%2520selecting%250Ahigh-quality%2520images%2520and%2520guiding%2520compression%2520and%2520enhancement%2520methods%2520in%2520a%2520series%250Aof%2520applications.%2520The%2520blind%2520IQA%252C%2520which%2520assesses%2520the%2520quality%2520of%2520in-the-wild%250Aimages%2520containing%2520complex%2520authentic%2520distortions%2520without%2520reference%2520images%252C%2520poses%250Agreater%2520challenges.%2520Existing%2520methods%2520are%2520limited%2520to%2520modeling%2520a%2520uniform%250Adistribution%2520with%2520local%2520patches%2520and%2520are%2520bothered%2520by%2520the%2520gap%2520between%2520low%2520and%250Ahigh-level%2520visions%2520%2528caused%2520by%2520widely%2520adopted%2520pre-trained%2520classification%250Anetworks%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520IQA%2520method%2520called%2520diffusion%250Apriors-based%2520IQA%2520%2528DP-IQA%2529%252C%2520which%2520leverages%2520the%2520prior%2520knowledge%2520from%2520the%250Apre-trained%2520diffusion%2520model%2520with%2520its%2520excellent%2520powers%2520to%2520bridge%2520semantic%2520gaps%250Ain%2520the%2520perception%2520of%2520the%2520visual%2520quality%2520of%2520images.%2520Specifically%252C%2520we%2520use%250Apre-trained%2520stable%2520diffusion%2520as%2520the%2520backbone%252C%2520extract%2520multi-level%2520features%2520from%250Athe%2520denoising%2520U-Net%2520during%2520the%2520upsampling%2520process%2520at%2520a%2520specified%2520timestep%252C%2520and%250Adecode%2520them%2520to%2520estimate%2520the%2520image%2520quality%2520score.%2520The%2520text%2520and%2520image%2520adapters%250Aare%2520adopted%2520to%2520mitigate%2520the%2520domain%2520gap%2520for%2520downstream%2520tasks%2520and%2520correct%2520the%250Ainformation%2520loss%2520caused%2520by%2520the%2520variational%2520autoencoder%2520bottleneck.%2520Finally%252C%2520we%250Adistill%2520the%2520knowledge%2520in%2520the%2520above%2520model%2520into%2520a%2520CNN-based%2520student%2520model%252C%250Asignificantly%2520reducing%2520the%2520parameter%2520to%2520enhance%2520applicability%252C%2520with%2520the%2520student%250Amodel%2520performing%2520similarly%2520or%2520even%2520better%2520than%2520the%2520teacher%2520model%2520surprisingly.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520DP-IQA%2520achieves%2520state-of-the-art%250Aresults%2520on%2520various%2520in-the-wild%2520datasets%2520with%2520better%2520generalization%2520capability%252C%250Awhich%2520shows%2520the%2520superiority%2520of%2520our%2520method%2520in%2520global%2520modeling%2520and%2520utilizing%2520the%250Ahierarchical%2520feature%2520clues%2520of%2520diffusion%2520for%2520evaluating%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild&entry.906535625=Honghao%20Fu%20and%20Yufei%20Wang%20and%20Wenhan%20Yang%20and%20Bihan%20Wen&entry.1292438233=%20%20Image%20quality%20assessment%20%28IQA%29%20plays%20a%20critical%20role%20in%20selecting%0Ahigh-quality%20images%20and%20guiding%20compression%20and%20enhancement%20methods%20in%20a%20series%0Aof%20applications.%20The%20blind%20IQA%2C%20which%20assesses%20the%20quality%20of%20in-the-wild%0Aimages%20containing%20complex%20authentic%20distortions%20without%20reference%20images%2C%20poses%0Agreater%20challenges.%20Existing%20methods%20are%20limited%20to%20modeling%20a%20uniform%0Adistribution%20with%20local%20patches%20and%20are%20bothered%20by%20the%20gap%20between%20low%20and%0Ahigh-level%20visions%20%28caused%20by%20widely%20adopted%20pre-trained%20classification%0Anetworks%29.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20IQA%20method%20called%20diffusion%0Apriors-based%20IQA%20%28DP-IQA%29%2C%20which%20leverages%20the%20prior%20knowledge%20from%20the%0Apre-trained%20diffusion%20model%20with%20its%20excellent%20powers%20to%20bridge%20semantic%20gaps%0Ain%20the%20perception%20of%20the%20visual%20quality%20of%20images.%20Specifically%2C%20we%20use%0Apre-trained%20stable%20diffusion%20as%20the%20backbone%2C%20extract%20multi-level%20features%20from%0Athe%20denoising%20U-Net%20during%20the%20upsampling%20process%20at%20a%20specified%20timestep%2C%20and%0Adecode%20them%20to%20estimate%20the%20image%20quality%20score.%20The%20text%20and%20image%20adapters%0Aare%20adopted%20to%20mitigate%20the%20domain%20gap%20for%20downstream%20tasks%20and%20correct%20the%0Ainformation%20loss%20caused%20by%20the%20variational%20autoencoder%20bottleneck.%20Finally%2C%20we%0Adistill%20the%20knowledge%20in%20the%20above%20model%20into%20a%20CNN-based%20student%20model%2C%0Asignificantly%20reducing%20the%20parameter%20to%20enhance%20applicability%2C%20with%20the%20student%0Amodel%20performing%20similarly%20or%20even%20better%20than%20the%20teacher%20model%20surprisingly.%0AExperimental%20results%20demonstrate%20that%20our%20DP-IQA%20achieves%20state-of-the-art%0Aresults%20on%20various%20in-the-wild%20datasets%20with%20better%20generalization%20capability%2C%0Awhich%20shows%20the%20superiority%20of%20our%20method%20in%20global%20modeling%20and%20utilizing%20the%0Ahierarchical%20feature%20clues%20of%20diffusion%20for%20evaluating%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19996v1&entry.124074799=Read"},
{"title": "Iterative Learning Control of Fast, Nonlinear, Oscillatory Dynamics\n  (Preprint)", "author": "John W. Brooks and Christine M. Greve", "abstract": "  The sudden onset of deleterious and oscillatory dynamics (often called\ninstabilities) is a known challenge in many fluid, plasma, and aerospace\nsystems. These dynamics are difficult to address because they are nonlinear,\nchaotic, and are often too fast for active control schemes. In this work, we\ndevelop an alternative active controls system using an iterative,\ntrajectory-optimization and parameter-tuning approach based on Iterative\nLearning Control (ILC), Time-Lagged Phase Portraits (TLPP) and Gaussian Process\nRegression (GPR). The novelty of this approach is that it can control a\nsystem's dynamics despite the controller being much slower than the dynamics.\nWe demonstrate this controller on the Lorenz system of equations where it\niteratively adjusts (tunes) the system's input parameters to successfully\nreproduce a desired oscillatory trajectory or state. Additionally, we\ninvestigate the system's dynamical sensitivity to its control parameters,\nidentify continuous and bounded regions of desired dynamical trajectories, and\ndemonstrate that the controller is robust to missing information and\nuncontrollable parameters as long as certain requirements are met. The\ncontroller presented in this work provides a framework for low-speed control\nfor a variety of fast, nonlinear systems that may aid in instability\nsuppression and mitigation.\n", "link": "http://arxiv.org/abs/2405.20045v1", "date": "2024-05-30", "relevancy": 2.2884, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.506}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Learning%20Control%20of%20Fast%2C%20Nonlinear%2C%20Oscillatory%20Dynamics%0A%20%20%28Preprint%29&body=Title%3A%20Iterative%20Learning%20Control%20of%20Fast%2C%20Nonlinear%2C%20Oscillatory%20Dynamics%0A%20%20%28Preprint%29%0AAuthor%3A%20John%20W.%20Brooks%20and%20Christine%20M.%20Greve%0AAbstract%3A%20%20%20The%20sudden%20onset%20of%20deleterious%20and%20oscillatory%20dynamics%20%28often%20called%0Ainstabilities%29%20is%20a%20known%20challenge%20in%20many%20fluid%2C%20plasma%2C%20and%20aerospace%0Asystems.%20These%20dynamics%20are%20difficult%20to%20address%20because%20they%20are%20nonlinear%2C%0Achaotic%2C%20and%20are%20often%20too%20fast%20for%20active%20control%20schemes.%20In%20this%20work%2C%20we%0Adevelop%20an%20alternative%20active%20controls%20system%20using%20an%20iterative%2C%0Atrajectory-optimization%20and%20parameter-tuning%20approach%20based%20on%20Iterative%0ALearning%20Control%20%28ILC%29%2C%20Time-Lagged%20Phase%20Portraits%20%28TLPP%29%20and%20Gaussian%20Process%0ARegression%20%28GPR%29.%20The%20novelty%20of%20this%20approach%20is%20that%20it%20can%20control%20a%0Asystem%27s%20dynamics%20despite%20the%20controller%20being%20much%20slower%20than%20the%20dynamics.%0AWe%20demonstrate%20this%20controller%20on%20the%20Lorenz%20system%20of%20equations%20where%20it%0Aiteratively%20adjusts%20%28tunes%29%20the%20system%27s%20input%20parameters%20to%20successfully%0Areproduce%20a%20desired%20oscillatory%20trajectory%20or%20state.%20Additionally%2C%20we%0Ainvestigate%20the%20system%27s%20dynamical%20sensitivity%20to%20its%20control%20parameters%2C%0Aidentify%20continuous%20and%20bounded%20regions%20of%20desired%20dynamical%20trajectories%2C%20and%0Ademonstrate%20that%20the%20controller%20is%20robust%20to%20missing%20information%20and%0Auncontrollable%20parameters%20as%20long%20as%20certain%20requirements%20are%20met.%20The%0Acontroller%20presented%20in%20this%20work%20provides%20a%20framework%20for%20low-speed%20control%0Afor%20a%20variety%20of%20fast%2C%20nonlinear%20systems%20that%20may%20aid%20in%20instability%0Asuppression%20and%20mitigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Learning%2520Control%2520of%2520Fast%252C%2520Nonlinear%252C%2520Oscillatory%2520Dynamics%250A%2520%2520%2528Preprint%2529%26entry.906535625%3DJohn%2520W.%2520Brooks%2520and%2520Christine%2520M.%2520Greve%26entry.1292438233%3D%2520%2520The%2520sudden%2520onset%2520of%2520deleterious%2520and%2520oscillatory%2520dynamics%2520%2528often%2520called%250Ainstabilities%2529%2520is%2520a%2520known%2520challenge%2520in%2520many%2520fluid%252C%2520plasma%252C%2520and%2520aerospace%250Asystems.%2520These%2520dynamics%2520are%2520difficult%2520to%2520address%2520because%2520they%2520are%2520nonlinear%252C%250Achaotic%252C%2520and%2520are%2520often%2520too%2520fast%2520for%2520active%2520control%2520schemes.%2520In%2520this%2520work%252C%2520we%250Adevelop%2520an%2520alternative%2520active%2520controls%2520system%2520using%2520an%2520iterative%252C%250Atrajectory-optimization%2520and%2520parameter-tuning%2520approach%2520based%2520on%2520Iterative%250ALearning%2520Control%2520%2528ILC%2529%252C%2520Time-Lagged%2520Phase%2520Portraits%2520%2528TLPP%2529%2520and%2520Gaussian%2520Process%250ARegression%2520%2528GPR%2529.%2520The%2520novelty%2520of%2520this%2520approach%2520is%2520that%2520it%2520can%2520control%2520a%250Asystem%2527s%2520dynamics%2520despite%2520the%2520controller%2520being%2520much%2520slower%2520than%2520the%2520dynamics.%250AWe%2520demonstrate%2520this%2520controller%2520on%2520the%2520Lorenz%2520system%2520of%2520equations%2520where%2520it%250Aiteratively%2520adjusts%2520%2528tunes%2529%2520the%2520system%2527s%2520input%2520parameters%2520to%2520successfully%250Areproduce%2520a%2520desired%2520oscillatory%2520trajectory%2520or%2520state.%2520Additionally%252C%2520we%250Ainvestigate%2520the%2520system%2527s%2520dynamical%2520sensitivity%2520to%2520its%2520control%2520parameters%252C%250Aidentify%2520continuous%2520and%2520bounded%2520regions%2520of%2520desired%2520dynamical%2520trajectories%252C%2520and%250Ademonstrate%2520that%2520the%2520controller%2520is%2520robust%2520to%2520missing%2520information%2520and%250Auncontrollable%2520parameters%2520as%2520long%2520as%2520certain%2520requirements%2520are%2520met.%2520The%250Acontroller%2520presented%2520in%2520this%2520work%2520provides%2520a%2520framework%2520for%2520low-speed%2520control%250Afor%2520a%2520variety%2520of%2520fast%252C%2520nonlinear%2520systems%2520that%2520may%2520aid%2520in%2520instability%250Asuppression%2520and%2520mitigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Learning%20Control%20of%20Fast%2C%20Nonlinear%2C%20Oscillatory%20Dynamics%0A%20%20%28Preprint%29&entry.906535625=John%20W.%20Brooks%20and%20Christine%20M.%20Greve&entry.1292438233=%20%20The%20sudden%20onset%20of%20deleterious%20and%20oscillatory%20dynamics%20%28often%20called%0Ainstabilities%29%20is%20a%20known%20challenge%20in%20many%20fluid%2C%20plasma%2C%20and%20aerospace%0Asystems.%20These%20dynamics%20are%20difficult%20to%20address%20because%20they%20are%20nonlinear%2C%0Achaotic%2C%20and%20are%20often%20too%20fast%20for%20active%20control%20schemes.%20In%20this%20work%2C%20we%0Adevelop%20an%20alternative%20active%20controls%20system%20using%20an%20iterative%2C%0Atrajectory-optimization%20and%20parameter-tuning%20approach%20based%20on%20Iterative%0ALearning%20Control%20%28ILC%29%2C%20Time-Lagged%20Phase%20Portraits%20%28TLPP%29%20and%20Gaussian%20Process%0ARegression%20%28GPR%29.%20The%20novelty%20of%20this%20approach%20is%20that%20it%20can%20control%20a%0Asystem%27s%20dynamics%20despite%20the%20controller%20being%20much%20slower%20than%20the%20dynamics.%0AWe%20demonstrate%20this%20controller%20on%20the%20Lorenz%20system%20of%20equations%20where%20it%0Aiteratively%20adjusts%20%28tunes%29%20the%20system%27s%20input%20parameters%20to%20successfully%0Areproduce%20a%20desired%20oscillatory%20trajectory%20or%20state.%20Additionally%2C%20we%0Ainvestigate%20the%20system%27s%20dynamical%20sensitivity%20to%20its%20control%20parameters%2C%0Aidentify%20continuous%20and%20bounded%20regions%20of%20desired%20dynamical%20trajectories%2C%20and%0Ademonstrate%20that%20the%20controller%20is%20robust%20to%20missing%20information%20and%0Auncontrollable%20parameters%20as%20long%20as%20certain%20requirements%20are%20met.%20The%0Acontroller%20presented%20in%20this%20work%20provides%20a%20framework%20for%20low-speed%20control%0Afor%20a%20variety%20of%20fast%2C%20nonlinear%20systems%20that%20may%20aid%20in%20instability%0Asuppression%20and%20mitigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20045v1&entry.124074799=Read"},
{"title": "AnalogCoder: Analog Circuit Design via Training-Free Code Generation", "author": "Yao Lai and Sungyoung Lee and Guojin Chen and Souradip Poddar and Mengkang Hu and David Z. Pan and Ping Luo", "abstract": "  Analog circuit design is a significant task in modern chip technology,\nfocusing on the selection of component types, connectivity, and parameters to\nensure proper circuit functionality. Despite advances made by Large Language\nModels (LLMs) in digital circuit design, the complexity and scarcity of data in\nanalog circuitry pose significant challenges. To mitigate these issues, we\nintroduce AnalogCoder, the first training-free LLM agent for designing analog\ncircuits through Python code generation. Firstly, AnalogCoder incorporates a\nfeedback-enhanced flow with tailored domain-specific prompts, enabling the\nautomated and self-correcting design of analog circuits with a high success\nrate. Secondly, it proposes a circuit tool library to archive successful\ndesigns as reusable modular sub-circuits, simplifying composite circuit\ncreation. Thirdly, extensive experiments on a benchmark designed to cover a\nwide range of analog circuit tasks show that AnalogCoder outperforms other\nLLM-based methods. It has successfully designed 20 circuits, 5 more than\nstandard GPT-4o. We believe AnalogCoder can significantly improve the\nlabor-intensive chip design process, enabling non-experts to design analog\ncircuits efficiently.\n", "link": "http://arxiv.org/abs/2405.14918v2", "date": "2024-05-30", "relevancy": 2.2696, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.457}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4561}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnalogCoder%3A%20Analog%20Circuit%20Design%20via%20Training-Free%20Code%20Generation&body=Title%3A%20AnalogCoder%3A%20Analog%20Circuit%20Design%20via%20Training-Free%20Code%20Generation%0AAuthor%3A%20Yao%20Lai%20and%20Sungyoung%20Lee%20and%20Guojin%20Chen%20and%20Souradip%20Poddar%20and%20Mengkang%20Hu%20and%20David%20Z.%20Pan%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Analog%20circuit%20design%20is%20a%20significant%20task%20in%20modern%20chip%20technology%2C%0Afocusing%20on%20the%20selection%20of%20component%20types%2C%20connectivity%2C%20and%20parameters%20to%0Aensure%20proper%20circuit%20functionality.%20Despite%20advances%20made%20by%20Large%20Language%0AModels%20%28LLMs%29%20in%20digital%20circuit%20design%2C%20the%20complexity%20and%20scarcity%20of%20data%20in%0Aanalog%20circuitry%20pose%20significant%20challenges.%20To%20mitigate%20these%20issues%2C%20we%0Aintroduce%20AnalogCoder%2C%20the%20first%20training-free%20LLM%20agent%20for%20designing%20analog%0Acircuits%20through%20Python%20code%20generation.%20Firstly%2C%20AnalogCoder%20incorporates%20a%0Afeedback-enhanced%20flow%20with%20tailored%20domain-specific%20prompts%2C%20enabling%20the%0Aautomated%20and%20self-correcting%20design%20of%20analog%20circuits%20with%20a%20high%20success%0Arate.%20Secondly%2C%20it%20proposes%20a%20circuit%20tool%20library%20to%20archive%20successful%0Adesigns%20as%20reusable%20modular%20sub-circuits%2C%20simplifying%20composite%20circuit%0Acreation.%20Thirdly%2C%20extensive%20experiments%20on%20a%20benchmark%20designed%20to%20cover%20a%0Awide%20range%20of%20analog%20circuit%20tasks%20show%20that%20AnalogCoder%20outperforms%20other%0ALLM-based%20methods.%20It%20has%20successfully%20designed%2020%20circuits%2C%205%20more%20than%0Astandard%20GPT-4o.%20We%20believe%20AnalogCoder%20can%20significantly%20improve%20the%0Alabor-intensive%20chip%20design%20process%2C%20enabling%20non-experts%20to%20design%20analog%0Acircuits%20efficiently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalogCoder%253A%2520Analog%2520Circuit%2520Design%2520via%2520Training-Free%2520Code%2520Generation%26entry.906535625%3DYao%2520Lai%2520and%2520Sungyoung%2520Lee%2520and%2520Guojin%2520Chen%2520and%2520Souradip%2520Poddar%2520and%2520Mengkang%2520Hu%2520and%2520David%2520Z.%2520Pan%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Analog%2520circuit%2520design%2520is%2520a%2520significant%2520task%2520in%2520modern%2520chip%2520technology%252C%250Afocusing%2520on%2520the%2520selection%2520of%2520component%2520types%252C%2520connectivity%252C%2520and%2520parameters%2520to%250Aensure%2520proper%2520circuit%2520functionality.%2520Despite%2520advances%2520made%2520by%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520in%2520digital%2520circuit%2520design%252C%2520the%2520complexity%2520and%2520scarcity%2520of%2520data%2520in%250Aanalog%2520circuitry%2520pose%2520significant%2520challenges.%2520To%2520mitigate%2520these%2520issues%252C%2520we%250Aintroduce%2520AnalogCoder%252C%2520the%2520first%2520training-free%2520LLM%2520agent%2520for%2520designing%2520analog%250Acircuits%2520through%2520Python%2520code%2520generation.%2520Firstly%252C%2520AnalogCoder%2520incorporates%2520a%250Afeedback-enhanced%2520flow%2520with%2520tailored%2520domain-specific%2520prompts%252C%2520enabling%2520the%250Aautomated%2520and%2520self-correcting%2520design%2520of%2520analog%2520circuits%2520with%2520a%2520high%2520success%250Arate.%2520Secondly%252C%2520it%2520proposes%2520a%2520circuit%2520tool%2520library%2520to%2520archive%2520successful%250Adesigns%2520as%2520reusable%2520modular%2520sub-circuits%252C%2520simplifying%2520composite%2520circuit%250Acreation.%2520Thirdly%252C%2520extensive%2520experiments%2520on%2520a%2520benchmark%2520designed%2520to%2520cover%2520a%250Awide%2520range%2520of%2520analog%2520circuit%2520tasks%2520show%2520that%2520AnalogCoder%2520outperforms%2520other%250ALLM-based%2520methods.%2520It%2520has%2520successfully%2520designed%252020%2520circuits%252C%25205%2520more%2520than%250Astandard%2520GPT-4o.%2520We%2520believe%2520AnalogCoder%2520can%2520significantly%2520improve%2520the%250Alabor-intensive%2520chip%2520design%2520process%252C%2520enabling%2520non-experts%2520to%2520design%2520analog%250Acircuits%2520efficiently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnalogCoder%3A%20Analog%20Circuit%20Design%20via%20Training-Free%20Code%20Generation&entry.906535625=Yao%20Lai%20and%20Sungyoung%20Lee%20and%20Guojin%20Chen%20and%20Souradip%20Poddar%20and%20Mengkang%20Hu%20and%20David%20Z.%20Pan%20and%20Ping%20Luo&entry.1292438233=%20%20Analog%20circuit%20design%20is%20a%20significant%20task%20in%20modern%20chip%20technology%2C%0Afocusing%20on%20the%20selection%20of%20component%20types%2C%20connectivity%2C%20and%20parameters%20to%0Aensure%20proper%20circuit%20functionality.%20Despite%20advances%20made%20by%20Large%20Language%0AModels%20%28LLMs%29%20in%20digital%20circuit%20design%2C%20the%20complexity%20and%20scarcity%20of%20data%20in%0Aanalog%20circuitry%20pose%20significant%20challenges.%20To%20mitigate%20these%20issues%2C%20we%0Aintroduce%20AnalogCoder%2C%20the%20first%20training-free%20LLM%20agent%20for%20designing%20analog%0Acircuits%20through%20Python%20code%20generation.%20Firstly%2C%20AnalogCoder%20incorporates%20a%0Afeedback-enhanced%20flow%20with%20tailored%20domain-specific%20prompts%2C%20enabling%20the%0Aautomated%20and%20self-correcting%20design%20of%20analog%20circuits%20with%20a%20high%20success%0Arate.%20Secondly%2C%20it%20proposes%20a%20circuit%20tool%20library%20to%20archive%20successful%0Adesigns%20as%20reusable%20modular%20sub-circuits%2C%20simplifying%20composite%20circuit%0Acreation.%20Thirdly%2C%20extensive%20experiments%20on%20a%20benchmark%20designed%20to%20cover%20a%0Awide%20range%20of%20analog%20circuit%20tasks%20show%20that%20AnalogCoder%20outperforms%20other%0ALLM-based%20methods.%20It%20has%20successfully%20designed%2020%20circuits%2C%205%20more%20than%0Astandard%20GPT-4o.%20We%20believe%20AnalogCoder%20can%20significantly%20improve%20the%0Alabor-intensive%20chip%20design%20process%2C%20enabling%20non-experts%20to%20design%20analog%0Acircuits%20efficiently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14918v2&entry.124074799=Read"},
{"title": "Beyond the Visible: A Survey on Cross-spectral Face Recognition", "author": "David Anghelone and Cunjian Chen and Arun Ross and Antitza Dantcheva", "abstract": "  Cross-spectral face recognition (CFR) refers to recognizing individuals using\nface images stemming from different spectral bands, such as infrared vs.\nvisible. While CFR is inherently more challenging than classical face\nrecognition due to significant variation in facial appearance caused by the\nmodality gap, it is useful in many scenarios including night-vision biometrics\nand detecting presentation attacks. Recent advances in convolutional neural\nnetworks (CNNs) have resulted in significant improvement in the performance of\nCFR systems. Given these developments, the contributions of this survey are\nthree-fold. First, we provide an overview of CFR, by formalizing the CFR\nproblem and presenting related applications. Secondly, we discuss the\nappropriate spectral bands for face recognition and discuss recent CFR methods,\nplacing emphasis on deep neural networks. In particular we describe techniques\nthat have been proposed to extract and compare heterogeneous features emerging\nfrom different spectral bands. We also discuss the datasets that have been used\nfor evaluating CFR methods. Finally, we discuss the challenges and future lines\nof research on this topic.\n", "link": "http://arxiv.org/abs/2201.04435v3", "date": "2024-05-30", "relevancy": 2.2671, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4626}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.451}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Visible%3A%20A%20Survey%20on%20Cross-spectral%20Face%20Recognition&body=Title%3A%20Beyond%20the%20Visible%3A%20A%20Survey%20on%20Cross-spectral%20Face%20Recognition%0AAuthor%3A%20David%20Anghelone%20and%20Cunjian%20Chen%20and%20Arun%20Ross%20and%20Antitza%20Dantcheva%0AAbstract%3A%20%20%20Cross-spectral%20face%20recognition%20%28CFR%29%20refers%20to%20recognizing%20individuals%20using%0Aface%20images%20stemming%20from%20different%20spectral%20bands%2C%20such%20as%20infrared%20vs.%0Avisible.%20While%20CFR%20is%20inherently%20more%20challenging%20than%20classical%20face%0Arecognition%20due%20to%20significant%20variation%20in%20facial%20appearance%20caused%20by%20the%0Amodality%20gap%2C%20it%20is%20useful%20in%20many%20scenarios%20including%20night-vision%20biometrics%0Aand%20detecting%20presentation%20attacks.%20Recent%20advances%20in%20convolutional%20neural%0Anetworks%20%28CNNs%29%20have%20resulted%20in%20significant%20improvement%20in%20the%20performance%20of%0ACFR%20systems.%20Given%20these%20developments%2C%20the%20contributions%20of%20this%20survey%20are%0Athree-fold.%20First%2C%20we%20provide%20an%20overview%20of%20CFR%2C%20by%20formalizing%20the%20CFR%0Aproblem%20and%20presenting%20related%20applications.%20Secondly%2C%20we%20discuss%20the%0Aappropriate%20spectral%20bands%20for%20face%20recognition%20and%20discuss%20recent%20CFR%20methods%2C%0Aplacing%20emphasis%20on%20deep%20neural%20networks.%20In%20particular%20we%20describe%20techniques%0Athat%20have%20been%20proposed%20to%20extract%20and%20compare%20heterogeneous%20features%20emerging%0Afrom%20different%20spectral%20bands.%20We%20also%20discuss%20the%20datasets%20that%20have%20been%20used%0Afor%20evaluating%20CFR%20methods.%20Finally%2C%20we%20discuss%20the%20challenges%20and%20future%20lines%0Aof%20research%20on%20this%20topic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.04435v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Visible%253A%2520A%2520Survey%2520on%2520Cross-spectral%2520Face%2520Recognition%26entry.906535625%3DDavid%2520Anghelone%2520and%2520Cunjian%2520Chen%2520and%2520Arun%2520Ross%2520and%2520Antitza%2520Dantcheva%26entry.1292438233%3D%2520%2520Cross-spectral%2520face%2520recognition%2520%2528CFR%2529%2520refers%2520to%2520recognizing%2520individuals%2520using%250Aface%2520images%2520stemming%2520from%2520different%2520spectral%2520bands%252C%2520such%2520as%2520infrared%2520vs.%250Avisible.%2520While%2520CFR%2520is%2520inherently%2520more%2520challenging%2520than%2520classical%2520face%250Arecognition%2520due%2520to%2520significant%2520variation%2520in%2520facial%2520appearance%2520caused%2520by%2520the%250Amodality%2520gap%252C%2520it%2520is%2520useful%2520in%2520many%2520scenarios%2520including%2520night-vision%2520biometrics%250Aand%2520detecting%2520presentation%2520attacks.%2520Recent%2520advances%2520in%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520have%2520resulted%2520in%2520significant%2520improvement%2520in%2520the%2520performance%2520of%250ACFR%2520systems.%2520Given%2520these%2520developments%252C%2520the%2520contributions%2520of%2520this%2520survey%2520are%250Athree-fold.%2520First%252C%2520we%2520provide%2520an%2520overview%2520of%2520CFR%252C%2520by%2520formalizing%2520the%2520CFR%250Aproblem%2520and%2520presenting%2520related%2520applications.%2520Secondly%252C%2520we%2520discuss%2520the%250Aappropriate%2520spectral%2520bands%2520for%2520face%2520recognition%2520and%2520discuss%2520recent%2520CFR%2520methods%252C%250Aplacing%2520emphasis%2520on%2520deep%2520neural%2520networks.%2520In%2520particular%2520we%2520describe%2520techniques%250Athat%2520have%2520been%2520proposed%2520to%2520extract%2520and%2520compare%2520heterogeneous%2520features%2520emerging%250Afrom%2520different%2520spectral%2520bands.%2520We%2520also%2520discuss%2520the%2520datasets%2520that%2520have%2520been%2520used%250Afor%2520evaluating%2520CFR%2520methods.%2520Finally%252C%2520we%2520discuss%2520the%2520challenges%2520and%2520future%2520lines%250Aof%2520research%2520on%2520this%2520topic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.04435v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Visible%3A%20A%20Survey%20on%20Cross-spectral%20Face%20Recognition&entry.906535625=David%20Anghelone%20and%20Cunjian%20Chen%20and%20Arun%20Ross%20and%20Antitza%20Dantcheva&entry.1292438233=%20%20Cross-spectral%20face%20recognition%20%28CFR%29%20refers%20to%20recognizing%20individuals%20using%0Aface%20images%20stemming%20from%20different%20spectral%20bands%2C%20such%20as%20infrared%20vs.%0Avisible.%20While%20CFR%20is%20inherently%20more%20challenging%20than%20classical%20face%0Arecognition%20due%20to%20significant%20variation%20in%20facial%20appearance%20caused%20by%20the%0Amodality%20gap%2C%20it%20is%20useful%20in%20many%20scenarios%20including%20night-vision%20biometrics%0Aand%20detecting%20presentation%20attacks.%20Recent%20advances%20in%20convolutional%20neural%0Anetworks%20%28CNNs%29%20have%20resulted%20in%20significant%20improvement%20in%20the%20performance%20of%0ACFR%20systems.%20Given%20these%20developments%2C%20the%20contributions%20of%20this%20survey%20are%0Athree-fold.%20First%2C%20we%20provide%20an%20overview%20of%20CFR%2C%20by%20formalizing%20the%20CFR%0Aproblem%20and%20presenting%20related%20applications.%20Secondly%2C%20we%20discuss%20the%0Aappropriate%20spectral%20bands%20for%20face%20recognition%20and%20discuss%20recent%20CFR%20methods%2C%0Aplacing%20emphasis%20on%20deep%20neural%20networks.%20In%20particular%20we%20describe%20techniques%0Athat%20have%20been%20proposed%20to%20extract%20and%20compare%20heterogeneous%20features%20emerging%0Afrom%20different%20spectral%20bands.%20We%20also%20discuss%20the%20datasets%20that%20have%20been%20used%0Afor%20evaluating%20CFR%20methods.%20Finally%2C%20we%20discuss%20the%20challenges%20and%20future%20lines%0Aof%20research%20on%20this%20topic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.04435v3&entry.124074799=Read"},
{"title": "RIGID: A Training-free and Model-Agnostic Framework for Robust\n  AI-Generated Image Detection", "author": "Zhiyuan He and Pin-Yu Chen and Tsung-Yi Ho", "abstract": "  The rapid advances in generative AI models have empowered the creation of\nhighly realistic images with arbitrary content, raising concerns about\npotential misuse and harm, such as Deepfakes. Current research focuses on\ntraining detectors using large datasets of generated images. However, these\ntraining-based solutions are often computationally expensive and show limited\ngeneralization to unseen generated images. In this paper, we propose a\ntraining-free method to distinguish between real and AI-generated images. We\nfirst observe that real images are more robust to tiny noise perturbations than\nAI-generated images in the representation space of vision foundation models.\nBased on this observation, we propose RIGID, a training-free and model-agnostic\nmethod for robust AI-generated image detection. RIGID is a simple yet effective\napproach that identifies whether an image is AI-generated by comparing the\nrepresentation similarity between the original and the noise-perturbed\ncounterpart. Our evaluation on a diverse set of AI-generated images and\nbenchmarks shows that RIGID significantly outperforms existing trainingbased\nand training-free detectors. In particular, the average performance of RIGID\nexceeds the current best training-free method by more than 25%. Importantly,\nRIGID exhibits strong generalization across different image generation methods\nand robustness to image corruptions.\n", "link": "http://arxiv.org/abs/2405.20112v1", "date": "2024-05-30", "relevancy": 2.2628, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5913}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5531}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIGID%3A%20A%20Training-free%20and%20Model-Agnostic%20Framework%20for%20Robust%0A%20%20AI-Generated%20Image%20Detection&body=Title%3A%20RIGID%3A%20A%20Training-free%20and%20Model-Agnostic%20Framework%20for%20Robust%0A%20%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Zhiyuan%20He%20and%20Pin-Yu%20Chen%20and%20Tsung-Yi%20Ho%0AAbstract%3A%20%20%20The%20rapid%20advances%20in%20generative%20AI%20models%20have%20empowered%20the%20creation%20of%0Ahighly%20realistic%20images%20with%20arbitrary%20content%2C%20raising%20concerns%20about%0Apotential%20misuse%20and%20harm%2C%20such%20as%20Deepfakes.%20Current%20research%20focuses%20on%0Atraining%20detectors%20using%20large%20datasets%20of%20generated%20images.%20However%2C%20these%0Atraining-based%20solutions%20are%20often%20computationally%20expensive%20and%20show%20limited%0Ageneralization%20to%20unseen%20generated%20images.%20In%20this%20paper%2C%20we%20propose%20a%0Atraining-free%20method%20to%20distinguish%20between%20real%20and%20AI-generated%20images.%20We%0Afirst%20observe%20that%20real%20images%20are%20more%20robust%20to%20tiny%20noise%20perturbations%20than%0AAI-generated%20images%20in%20the%20representation%20space%20of%20vision%20foundation%20models.%0ABased%20on%20this%20observation%2C%20we%20propose%20RIGID%2C%20a%20training-free%20and%20model-agnostic%0Amethod%20for%20robust%20AI-generated%20image%20detection.%20RIGID%20is%20a%20simple%20yet%20effective%0Aapproach%20that%20identifies%20whether%20an%20image%20is%20AI-generated%20by%20comparing%20the%0Arepresentation%20similarity%20between%20the%20original%20and%20the%20noise-perturbed%0Acounterpart.%20Our%20evaluation%20on%20a%20diverse%20set%20of%20AI-generated%20images%20and%0Abenchmarks%20shows%20that%20RIGID%20significantly%20outperforms%20existing%20trainingbased%0Aand%20training-free%20detectors.%20In%20particular%2C%20the%20average%20performance%20of%20RIGID%0Aexceeds%20the%20current%20best%20training-free%20method%20by%20more%20than%2025%25.%20Importantly%2C%0ARIGID%20exhibits%20strong%20generalization%20across%20different%20image%20generation%20methods%0Aand%20robustness%20to%20image%20corruptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIGID%253A%2520A%2520Training-free%2520and%2520Model-Agnostic%2520Framework%2520for%2520Robust%250A%2520%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DZhiyuan%2520He%2520and%2520Pin-Yu%2520Chen%2520and%2520Tsung-Yi%2520Ho%26entry.1292438233%3D%2520%2520The%2520rapid%2520advances%2520in%2520generative%2520AI%2520models%2520have%2520empowered%2520the%2520creation%2520of%250Ahighly%2520realistic%2520images%2520with%2520arbitrary%2520content%252C%2520raising%2520concerns%2520about%250Apotential%2520misuse%2520and%2520harm%252C%2520such%2520as%2520Deepfakes.%2520Current%2520research%2520focuses%2520on%250Atraining%2520detectors%2520using%2520large%2520datasets%2520of%2520generated%2520images.%2520However%252C%2520these%250Atraining-based%2520solutions%2520are%2520often%2520computationally%2520expensive%2520and%2520show%2520limited%250Ageneralization%2520to%2520unseen%2520generated%2520images.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Atraining-free%2520method%2520to%2520distinguish%2520between%2520real%2520and%2520AI-generated%2520images.%2520We%250Afirst%2520observe%2520that%2520real%2520images%2520are%2520more%2520robust%2520to%2520tiny%2520noise%2520perturbations%2520than%250AAI-generated%2520images%2520in%2520the%2520representation%2520space%2520of%2520vision%2520foundation%2520models.%250ABased%2520on%2520this%2520observation%252C%2520we%2520propose%2520RIGID%252C%2520a%2520training-free%2520and%2520model-agnostic%250Amethod%2520for%2520robust%2520AI-generated%2520image%2520detection.%2520RIGID%2520is%2520a%2520simple%2520yet%2520effective%250Aapproach%2520that%2520identifies%2520whether%2520an%2520image%2520is%2520AI-generated%2520by%2520comparing%2520the%250Arepresentation%2520similarity%2520between%2520the%2520original%2520and%2520the%2520noise-perturbed%250Acounterpart.%2520Our%2520evaluation%2520on%2520a%2520diverse%2520set%2520of%2520AI-generated%2520images%2520and%250Abenchmarks%2520shows%2520that%2520RIGID%2520significantly%2520outperforms%2520existing%2520trainingbased%250Aand%2520training-free%2520detectors.%2520In%2520particular%252C%2520the%2520average%2520performance%2520of%2520RIGID%250Aexceeds%2520the%2520current%2520best%2520training-free%2520method%2520by%2520more%2520than%252025%2525.%2520Importantly%252C%250ARIGID%2520exhibits%2520strong%2520generalization%2520across%2520different%2520image%2520generation%2520methods%250Aand%2520robustness%2520to%2520image%2520corruptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIGID%3A%20A%20Training-free%20and%20Model-Agnostic%20Framework%20for%20Robust%0A%20%20AI-Generated%20Image%20Detection&entry.906535625=Zhiyuan%20He%20and%20Pin-Yu%20Chen%20and%20Tsung-Yi%20Ho&entry.1292438233=%20%20The%20rapid%20advances%20in%20generative%20AI%20models%20have%20empowered%20the%20creation%20of%0Ahighly%20realistic%20images%20with%20arbitrary%20content%2C%20raising%20concerns%20about%0Apotential%20misuse%20and%20harm%2C%20such%20as%20Deepfakes.%20Current%20research%20focuses%20on%0Atraining%20detectors%20using%20large%20datasets%20of%20generated%20images.%20However%2C%20these%0Atraining-based%20solutions%20are%20often%20computationally%20expensive%20and%20show%20limited%0Ageneralization%20to%20unseen%20generated%20images.%20In%20this%20paper%2C%20we%20propose%20a%0Atraining-free%20method%20to%20distinguish%20between%20real%20and%20AI-generated%20images.%20We%0Afirst%20observe%20that%20real%20images%20are%20more%20robust%20to%20tiny%20noise%20perturbations%20than%0AAI-generated%20images%20in%20the%20representation%20space%20of%20vision%20foundation%20models.%0ABased%20on%20this%20observation%2C%20we%20propose%20RIGID%2C%20a%20training-free%20and%20model-agnostic%0Amethod%20for%20robust%20AI-generated%20image%20detection.%20RIGID%20is%20a%20simple%20yet%20effective%0Aapproach%20that%20identifies%20whether%20an%20image%20is%20AI-generated%20by%20comparing%20the%0Arepresentation%20similarity%20between%20the%20original%20and%20the%20noise-perturbed%0Acounterpart.%20Our%20evaluation%20on%20a%20diverse%20set%20of%20AI-generated%20images%20and%0Abenchmarks%20shows%20that%20RIGID%20significantly%20outperforms%20existing%20trainingbased%0Aand%20training-free%20detectors.%20In%20particular%2C%20the%20average%20performance%20of%20RIGID%0Aexceeds%20the%20current%20best%20training-free%20method%20by%20more%20than%2025%25.%20Importantly%2C%0ARIGID%20exhibits%20strong%20generalization%20across%20different%20image%20generation%20methods%0Aand%20robustness%20to%20image%20corruptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20112v1&entry.124074799=Read"},
{"title": "An Empirical Study of Training State-of-the-Art LiDAR Segmentation\n  Models", "author": "Jiahao Sun and Chunmei Qing and Xiang Xu and Lingdong Kong and Youquan Liu and Li Li and Chenming Zhu and Jingwei Zhang and Zeqi Xiao and Runnan Chen and Tai Wang and Wenwei Zhang and Kai Chen", "abstract": "  In the rapidly evolving field of autonomous driving, precise segmentation of\nLiDAR data is crucial for understanding complex 3D environments. Traditional\napproaches often rely on disparate, standalone codebases, hindering unified\nadvancements and fair benchmarking across models. To address these challenges,\nwe introduce MMDetection3D-lidarseg, a comprehensive toolbox designed for the\nefficient training and evaluation of state-of-the-art LiDAR segmentation\nmodels. We support a wide range of segmentation models and integrate advanced\ndata augmentation techniques to enhance robustness and generalization.\nAdditionally, the toolbox provides support for multiple leading sparse\nconvolution backends, optimizing computational efficiency and performance. By\nfostering a unified framework, MMDetection3D-lidarseg streamlines development\nand benchmarking, setting new standards for research and application. Our\nextensive benchmark experiments on widely-used datasets demonstrate the\neffectiveness of the toolbox. The codebase and trained models have been\npublicly available, promoting further research and innovation in the field of\nLiDAR segmentation for autonomous driving.\n", "link": "http://arxiv.org/abs/2405.14870v2", "date": "2024-05-30", "relevancy": 2.26, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5922}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5659}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20of%20Training%20State-of-the-Art%20LiDAR%20Segmentation%0A%20%20Models&body=Title%3A%20An%20Empirical%20Study%20of%20Training%20State-of-the-Art%20LiDAR%20Segmentation%0A%20%20Models%0AAuthor%3A%20Jiahao%20Sun%20and%20Chunmei%20Qing%20and%20Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Youquan%20Liu%20and%20Li%20Li%20and%20Chenming%20Zhu%20and%20Jingwei%20Zhang%20and%20Zeqi%20Xiao%20and%20Runnan%20Chen%20and%20Tai%20Wang%20and%20Wenwei%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20autonomous%20driving%2C%20precise%20segmentation%20of%0ALiDAR%20data%20is%20crucial%20for%20understanding%20complex%203D%20environments.%20Traditional%0Aapproaches%20often%20rely%20on%20disparate%2C%20standalone%20codebases%2C%20hindering%20unified%0Aadvancements%20and%20fair%20benchmarking%20across%20models.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20MMDetection3D-lidarseg%2C%20a%20comprehensive%20toolbox%20designed%20for%20the%0Aefficient%20training%20and%20evaluation%20of%20state-of-the-art%20LiDAR%20segmentation%0Amodels.%20We%20support%20a%20wide%20range%20of%20segmentation%20models%20and%20integrate%20advanced%0Adata%20augmentation%20techniques%20to%20enhance%20robustness%20and%20generalization.%0AAdditionally%2C%20the%20toolbox%20provides%20support%20for%20multiple%20leading%20sparse%0Aconvolution%20backends%2C%20optimizing%20computational%20efficiency%20and%20performance.%20By%0Afostering%20a%20unified%20framework%2C%20MMDetection3D-lidarseg%20streamlines%20development%0Aand%20benchmarking%2C%20setting%20new%20standards%20for%20research%20and%20application.%20Our%0Aextensive%20benchmark%20experiments%20on%20widely-used%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20the%20toolbox.%20The%20codebase%20and%20trained%20models%20have%20been%0Apublicly%20available%2C%20promoting%20further%20research%20and%20innovation%20in%20the%20field%20of%0ALiDAR%20segmentation%20for%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520of%2520Training%2520State-of-the-Art%2520LiDAR%2520Segmentation%250A%2520%2520Models%26entry.906535625%3DJiahao%2520Sun%2520and%2520Chunmei%2520Qing%2520and%2520Xiang%2520Xu%2520and%2520Lingdong%2520Kong%2520and%2520Youquan%2520Liu%2520and%2520Li%2520Li%2520and%2520Chenming%2520Zhu%2520and%2520Jingwei%2520Zhang%2520and%2520Zeqi%2520Xiao%2520and%2520Runnan%2520Chen%2520and%2520Tai%2520Wang%2520and%2520Wenwei%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520autonomous%2520driving%252C%2520precise%2520segmentation%2520of%250ALiDAR%2520data%2520is%2520crucial%2520for%2520understanding%2520complex%25203D%2520environments.%2520Traditional%250Aapproaches%2520often%2520rely%2520on%2520disparate%252C%2520standalone%2520codebases%252C%2520hindering%2520unified%250Aadvancements%2520and%2520fair%2520benchmarking%2520across%2520models.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520introduce%2520MMDetection3D-lidarseg%252C%2520a%2520comprehensive%2520toolbox%2520designed%2520for%2520the%250Aefficient%2520training%2520and%2520evaluation%2520of%2520state-of-the-art%2520LiDAR%2520segmentation%250Amodels.%2520We%2520support%2520a%2520wide%2520range%2520of%2520segmentation%2520models%2520and%2520integrate%2520advanced%250Adata%2520augmentation%2520techniques%2520to%2520enhance%2520robustness%2520and%2520generalization.%250AAdditionally%252C%2520the%2520toolbox%2520provides%2520support%2520for%2520multiple%2520leading%2520sparse%250Aconvolution%2520backends%252C%2520optimizing%2520computational%2520efficiency%2520and%2520performance.%2520By%250Afostering%2520a%2520unified%2520framework%252C%2520MMDetection3D-lidarseg%2520streamlines%2520development%250Aand%2520benchmarking%252C%2520setting%2520new%2520standards%2520for%2520research%2520and%2520application.%2520Our%250Aextensive%2520benchmark%2520experiments%2520on%2520widely-used%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520toolbox.%2520The%2520codebase%2520and%2520trained%2520models%2520have%2520been%250Apublicly%2520available%252C%2520promoting%2520further%2520research%2520and%2520innovation%2520in%2520the%2520field%2520of%250ALiDAR%2520segmentation%2520for%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20of%20Training%20State-of-the-Art%20LiDAR%20Segmentation%0A%20%20Models&entry.906535625=Jiahao%20Sun%20and%20Chunmei%20Qing%20and%20Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Youquan%20Liu%20and%20Li%20Li%20and%20Chenming%20Zhu%20and%20Jingwei%20Zhang%20and%20Zeqi%20Xiao%20and%20Runnan%20Chen%20and%20Tai%20Wang%20and%20Wenwei%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20autonomous%20driving%2C%20precise%20segmentation%20of%0ALiDAR%20data%20is%20crucial%20for%20understanding%20complex%203D%20environments.%20Traditional%0Aapproaches%20often%20rely%20on%20disparate%2C%20standalone%20codebases%2C%20hindering%20unified%0Aadvancements%20and%20fair%20benchmarking%20across%20models.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20MMDetection3D-lidarseg%2C%20a%20comprehensive%20toolbox%20designed%20for%20the%0Aefficient%20training%20and%20evaluation%20of%20state-of-the-art%20LiDAR%20segmentation%0Amodels.%20We%20support%20a%20wide%20range%20of%20segmentation%20models%20and%20integrate%20advanced%0Adata%20augmentation%20techniques%20to%20enhance%20robustness%20and%20generalization.%0AAdditionally%2C%20the%20toolbox%20provides%20support%20for%20multiple%20leading%20sparse%0Aconvolution%20backends%2C%20optimizing%20computational%20efficiency%20and%20performance.%20By%0Afostering%20a%20unified%20framework%2C%20MMDetection3D-lidarseg%20streamlines%20development%0Aand%20benchmarking%2C%20setting%20new%20standards%20for%20research%20and%20application.%20Our%0Aextensive%20benchmark%20experiments%20on%20widely-used%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20the%20toolbox.%20The%20codebase%20and%20trained%20models%20have%20been%0Apublicly%20available%2C%20promoting%20further%20research%20and%20innovation%20in%20the%20field%20of%0ALiDAR%20segmentation%20for%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14870v2&entry.124074799=Read"},
{"title": "Physics-Based Rigid Body Object Tracking and Friction Filtering From\n  RGB-D Videos", "author": "Rama Krishna Kandukuri and Michael Strecke and Joerg Stueckler", "abstract": "  Physics-based understanding of object interactions from sensory observations\nis an essential capability in augmented reality and robotics. It enables to\ncapture the properties of a scene for simulation and control. In this paper, we\npropose a novel approach for real-to-sim which tracks rigid objects in 3D from\nRGB-D images and infers physical properties of the objects. We use a\ndifferentiable physics simulation as state-transition model in an Extended\nKalman Filter which can model contact and friction for arbitrary mesh-based\nshapes and in this way estimate physically plausible trajectories. We\ndemonstrate that our approach can filter position, orientation, velocities, and\nconcurrently can estimate the coefficient of friction of the objects. We\nanalyze our approach on various sliding scenarios in synthetic image sequences\nof single objects and colliding objects. We also demonstrate and evaluate our\napproach on a real-world dataset. We make our novel benchmark datasets publicly\navailable to foster future research in this novel problem setting and\ncomparison with our method.\n", "link": "http://arxiv.org/abs/2309.15703v3", "date": "2024-05-30", "relevancy": 2.2524, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6491}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5706}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Based%20Rigid%20Body%20Object%20Tracking%20and%20Friction%20Filtering%20From%0A%20%20RGB-D%20Videos&body=Title%3A%20Physics-Based%20Rigid%20Body%20Object%20Tracking%20and%20Friction%20Filtering%20From%0A%20%20RGB-D%20Videos%0AAuthor%3A%20Rama%20Krishna%20Kandukuri%20and%20Michael%20Strecke%20and%20Joerg%20Stueckler%0AAbstract%3A%20%20%20Physics-based%20understanding%20of%20object%20interactions%20from%20sensory%20observations%0Ais%20an%20essential%20capability%20in%20augmented%20reality%20and%20robotics.%20It%20enables%20to%0Acapture%20the%20properties%20of%20a%20scene%20for%20simulation%20and%20control.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20for%20real-to-sim%20which%20tracks%20rigid%20objects%20in%203D%20from%0ARGB-D%20images%20and%20infers%20physical%20properties%20of%20the%20objects.%20We%20use%20a%0Adifferentiable%20physics%20simulation%20as%20state-transition%20model%20in%20an%20Extended%0AKalman%20Filter%20which%20can%20model%20contact%20and%20friction%20for%20arbitrary%20mesh-based%0Ashapes%20and%20in%20this%20way%20estimate%20physically%20plausible%20trajectories.%20We%0Ademonstrate%20that%20our%20approach%20can%20filter%20position%2C%20orientation%2C%20velocities%2C%20and%0Aconcurrently%20can%20estimate%20the%20coefficient%20of%20friction%20of%20the%20objects.%20We%0Aanalyze%20our%20approach%20on%20various%20sliding%20scenarios%20in%20synthetic%20image%20sequences%0Aof%20single%20objects%20and%20colliding%20objects.%20We%20also%20demonstrate%20and%20evaluate%20our%0Aapproach%20on%20a%20real-world%20dataset.%20We%20make%20our%20novel%20benchmark%20datasets%20publicly%0Aavailable%20to%20foster%20future%20research%20in%20this%20novel%20problem%20setting%20and%0Acomparison%20with%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15703v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Based%2520Rigid%2520Body%2520Object%2520Tracking%2520and%2520Friction%2520Filtering%2520From%250A%2520%2520RGB-D%2520Videos%26entry.906535625%3DRama%2520Krishna%2520Kandukuri%2520and%2520Michael%2520Strecke%2520and%2520Joerg%2520Stueckler%26entry.1292438233%3D%2520%2520Physics-based%2520understanding%2520of%2520object%2520interactions%2520from%2520sensory%2520observations%250Ais%2520an%2520essential%2520capability%2520in%2520augmented%2520reality%2520and%2520robotics.%2520It%2520enables%2520to%250Acapture%2520the%2520properties%2520of%2520a%2520scene%2520for%2520simulation%2520and%2520control.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520for%2520real-to-sim%2520which%2520tracks%2520rigid%2520objects%2520in%25203D%2520from%250ARGB-D%2520images%2520and%2520infers%2520physical%2520properties%2520of%2520the%2520objects.%2520We%2520use%2520a%250Adifferentiable%2520physics%2520simulation%2520as%2520state-transition%2520model%2520in%2520an%2520Extended%250AKalman%2520Filter%2520which%2520can%2520model%2520contact%2520and%2520friction%2520for%2520arbitrary%2520mesh-based%250Ashapes%2520and%2520in%2520this%2520way%2520estimate%2520physically%2520plausible%2520trajectories.%2520We%250Ademonstrate%2520that%2520our%2520approach%2520can%2520filter%2520position%252C%2520orientation%252C%2520velocities%252C%2520and%250Aconcurrently%2520can%2520estimate%2520the%2520coefficient%2520of%2520friction%2520of%2520the%2520objects.%2520We%250Aanalyze%2520our%2520approach%2520on%2520various%2520sliding%2520scenarios%2520in%2520synthetic%2520image%2520sequences%250Aof%2520single%2520objects%2520and%2520colliding%2520objects.%2520We%2520also%2520demonstrate%2520and%2520evaluate%2520our%250Aapproach%2520on%2520a%2520real-world%2520dataset.%2520We%2520make%2520our%2520novel%2520benchmark%2520datasets%2520publicly%250Aavailable%2520to%2520foster%2520future%2520research%2520in%2520this%2520novel%2520problem%2520setting%2520and%250Acomparison%2520with%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.15703v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Based%20Rigid%20Body%20Object%20Tracking%20and%20Friction%20Filtering%20From%0A%20%20RGB-D%20Videos&entry.906535625=Rama%20Krishna%20Kandukuri%20and%20Michael%20Strecke%20and%20Joerg%20Stueckler&entry.1292438233=%20%20Physics-based%20understanding%20of%20object%20interactions%20from%20sensory%20observations%0Ais%20an%20essential%20capability%20in%20augmented%20reality%20and%20robotics.%20It%20enables%20to%0Acapture%20the%20properties%20of%20a%20scene%20for%20simulation%20and%20control.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20for%20real-to-sim%20which%20tracks%20rigid%20objects%20in%203D%20from%0ARGB-D%20images%20and%20infers%20physical%20properties%20of%20the%20objects.%20We%20use%20a%0Adifferentiable%20physics%20simulation%20as%20state-transition%20model%20in%20an%20Extended%0AKalman%20Filter%20which%20can%20model%20contact%20and%20friction%20for%20arbitrary%20mesh-based%0Ashapes%20and%20in%20this%20way%20estimate%20physically%20plausible%20trajectories.%20We%0Ademonstrate%20that%20our%20approach%20can%20filter%20position%2C%20orientation%2C%20velocities%2C%20and%0Aconcurrently%20can%20estimate%20the%20coefficient%20of%20friction%20of%20the%20objects.%20We%0Aanalyze%20our%20approach%20on%20various%20sliding%20scenarios%20in%20synthetic%20image%20sequences%0Aof%20single%20objects%20and%20colliding%20objects.%20We%20also%20demonstrate%20and%20evaluate%20our%0Aapproach%20on%20a%20real-world%20dataset.%20We%20make%20our%20novel%20benchmark%20datasets%20publicly%0Aavailable%20to%20foster%20future%20research%20in%20this%20novel%20problem%20setting%20and%0Acomparison%20with%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15703v3&entry.124074799=Read"},
{"title": "Enabling Uncertainty Estimation in Iterative Neural Networks", "author": "Nikita Durasov and Doruk Oner and Jonathan Donier and Hieu Le and Pascal Fua", "abstract": "  Turning pass-through network architectures into iterative ones, which use\ntheir own output as input, is a well-known approach for boosting performance.\nIn this paper, we argue that such architectures offer an additional benefit:\nThe convergence rate of their successive outputs is highly correlated with the\naccuracy of the value to which they converge. Thus, we can use the convergence\nrate as a useful proxy for uncertainty. This results in an approach to\nuncertainty estimation that provides state-of-the-art estimates at a much lower\ncomputational cost than techniques like Ensembles, and without requiring any\nmodifications to the original iterative model. We demonstrate its practical\nvalue by embedding it in two application domains: road detection in aerial\nimages and the estimation of aerodynamic properties of 2D and 3D shapes.\n", "link": "http://arxiv.org/abs/2403.16732v2", "date": "2024-05-30", "relevancy": 2.252, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5736}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5584}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Uncertainty%20Estimation%20in%20Iterative%20Neural%20Networks&body=Title%3A%20Enabling%20Uncertainty%20Estimation%20in%20Iterative%20Neural%20Networks%0AAuthor%3A%20Nikita%20Durasov%20and%20Doruk%20Oner%20and%20Jonathan%20Donier%20and%20Hieu%20Le%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Turning%20pass-through%20network%20architectures%20into%20iterative%20ones%2C%20which%20use%0Atheir%20own%20output%20as%20input%2C%20is%20a%20well-known%20approach%20for%20boosting%20performance.%0AIn%20this%20paper%2C%20we%20argue%20that%20such%20architectures%20offer%20an%20additional%20benefit%3A%0AThe%20convergence%20rate%20of%20their%20successive%20outputs%20is%20highly%20correlated%20with%20the%0Aaccuracy%20of%20the%20value%20to%20which%20they%20converge.%20Thus%2C%20we%20can%20use%20the%20convergence%0Arate%20as%20a%20useful%20proxy%20for%20uncertainty.%20This%20results%20in%20an%20approach%20to%0Auncertainty%20estimation%20that%20provides%20state-of-the-art%20estimates%20at%20a%20much%20lower%0Acomputational%20cost%20than%20techniques%20like%20Ensembles%2C%20and%20without%20requiring%20any%0Amodifications%20to%20the%20original%20iterative%20model.%20We%20demonstrate%20its%20practical%0Avalue%20by%20embedding%20it%20in%20two%20application%20domains%3A%20road%20detection%20in%20aerial%0Aimages%20and%20the%20estimation%20of%20aerodynamic%20properties%20of%202D%20and%203D%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Uncertainty%2520Estimation%2520in%2520Iterative%2520Neural%2520Networks%26entry.906535625%3DNikita%2520Durasov%2520and%2520Doruk%2520Oner%2520and%2520Jonathan%2520Donier%2520and%2520Hieu%2520Le%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520Turning%2520pass-through%2520network%2520architectures%2520into%2520iterative%2520ones%252C%2520which%2520use%250Atheir%2520own%2520output%2520as%2520input%252C%2520is%2520a%2520well-known%2520approach%2520for%2520boosting%2520performance.%250AIn%2520this%2520paper%252C%2520we%2520argue%2520that%2520such%2520architectures%2520offer%2520an%2520additional%2520benefit%253A%250AThe%2520convergence%2520rate%2520of%2520their%2520successive%2520outputs%2520is%2520highly%2520correlated%2520with%2520the%250Aaccuracy%2520of%2520the%2520value%2520to%2520which%2520they%2520converge.%2520Thus%252C%2520we%2520can%2520use%2520the%2520convergence%250Arate%2520as%2520a%2520useful%2520proxy%2520for%2520uncertainty.%2520This%2520results%2520in%2520an%2520approach%2520to%250Auncertainty%2520estimation%2520that%2520provides%2520state-of-the-art%2520estimates%2520at%2520a%2520much%2520lower%250Acomputational%2520cost%2520than%2520techniques%2520like%2520Ensembles%252C%2520and%2520without%2520requiring%2520any%250Amodifications%2520to%2520the%2520original%2520iterative%2520model.%2520We%2520demonstrate%2520its%2520practical%250Avalue%2520by%2520embedding%2520it%2520in%2520two%2520application%2520domains%253A%2520road%2520detection%2520in%2520aerial%250Aimages%2520and%2520the%2520estimation%2520of%2520aerodynamic%2520properties%2520of%25202D%2520and%25203D%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Uncertainty%20Estimation%20in%20Iterative%20Neural%20Networks&entry.906535625=Nikita%20Durasov%20and%20Doruk%20Oner%20and%20Jonathan%20Donier%20and%20Hieu%20Le%20and%20Pascal%20Fua&entry.1292438233=%20%20Turning%20pass-through%20network%20architectures%20into%20iterative%20ones%2C%20which%20use%0Atheir%20own%20output%20as%20input%2C%20is%20a%20well-known%20approach%20for%20boosting%20performance.%0AIn%20this%20paper%2C%20we%20argue%20that%20such%20architectures%20offer%20an%20additional%20benefit%3A%0AThe%20convergence%20rate%20of%20their%20successive%20outputs%20is%20highly%20correlated%20with%20the%0Aaccuracy%20of%20the%20value%20to%20which%20they%20converge.%20Thus%2C%20we%20can%20use%20the%20convergence%0Arate%20as%20a%20useful%20proxy%20for%20uncertainty.%20This%20results%20in%20an%20approach%20to%0Auncertainty%20estimation%20that%20provides%20state-of-the-art%20estimates%20at%20a%20much%20lower%0Acomputational%20cost%20than%20techniques%20like%20Ensembles%2C%20and%20without%20requiring%20any%0Amodifications%20to%20the%20original%20iterative%20model.%20We%20demonstrate%20its%20practical%0Avalue%20by%20embedding%20it%20in%20two%20application%20domains%3A%20road%20detection%20in%20aerial%0Aimages%20and%20the%20estimation%20of%20aerodynamic%20properties%20of%202D%20and%203D%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16732v2&entry.124074799=Read"},
{"title": "MCDS-VSS: Moving Camera Dynamic Scene Video Semantic Segmentation by\n  Filtering with Self-Supervised Geometry and Motion", "author": "Angel Villar-Corrales and Moritz Austermann and Sven Behnke", "abstract": "  Autonomous systems, such as self-driving cars, rely on reliable semantic\nenvironment perception for decision making. Despite great advances in video\nsemantic segmentation, existing approaches ignore important inductive biases\nand lack structured and interpretable internal representations. In this work,\nwe propose MCDS-VSS, a structured filter model that learns in a self-supervised\nmanner to estimate scene geometry and ego-motion of the camera, while also\nestimating the motion of external objects. Our model leverages these\nrepresentations to improve the temporal consistency of semantic segmentation\nwithout sacrificing segmentation accuracy. MCDS-VSS follows a prediction-fusion\napproach in which scene geometry and camera motion are first used to compensate\nfor ego-motion, then residual flow is used to compensate motion of dynamic\nobjects, and finally the predicted scene features are fused with the current\nfeatures to obtain a temporally consistent scene segmentation. Our model parses\nautomotive scenes into multiple decoupled interpretable representations such as\nscene geometry, ego-motion, and object motion. Quantitative evaluation shows\nthat MCDS-VSS achieves superior temporal consistency on video sequences while\nretaining competitive segmentation performance.\n", "link": "http://arxiv.org/abs/2405.19921v1", "date": "2024-05-30", "relevancy": 2.2444, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5729}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5656}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCDS-VSS%3A%20Moving%20Camera%20Dynamic%20Scene%20Video%20Semantic%20Segmentation%20by%0A%20%20Filtering%20with%20Self-Supervised%20Geometry%20and%20Motion&body=Title%3A%20MCDS-VSS%3A%20Moving%20Camera%20Dynamic%20Scene%20Video%20Semantic%20Segmentation%20by%0A%20%20Filtering%20with%20Self-Supervised%20Geometry%20and%20Motion%0AAuthor%3A%20Angel%20Villar-Corrales%20and%20Moritz%20Austermann%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20Autonomous%20systems%2C%20such%20as%20self-driving%20cars%2C%20rely%20on%20reliable%20semantic%0Aenvironment%20perception%20for%20decision%20making.%20Despite%20great%20advances%20in%20video%0Asemantic%20segmentation%2C%20existing%20approaches%20ignore%20important%20inductive%20biases%0Aand%20lack%20structured%20and%20interpretable%20internal%20representations.%20In%20this%20work%2C%0Awe%20propose%20MCDS-VSS%2C%20a%20structured%20filter%20model%20that%20learns%20in%20a%20self-supervised%0Amanner%20to%20estimate%20scene%20geometry%20and%20ego-motion%20of%20the%20camera%2C%20while%20also%0Aestimating%20the%20motion%20of%20external%20objects.%20Our%20model%20leverages%20these%0Arepresentations%20to%20improve%20the%20temporal%20consistency%20of%20semantic%20segmentation%0Awithout%20sacrificing%20segmentation%20accuracy.%20MCDS-VSS%20follows%20a%20prediction-fusion%0Aapproach%20in%20which%20scene%20geometry%20and%20camera%20motion%20are%20first%20used%20to%20compensate%0Afor%20ego-motion%2C%20then%20residual%20flow%20is%20used%20to%20compensate%20motion%20of%20dynamic%0Aobjects%2C%20and%20finally%20the%20predicted%20scene%20features%20are%20fused%20with%20the%20current%0Afeatures%20to%20obtain%20a%20temporally%20consistent%20scene%20segmentation.%20Our%20model%20parses%0Aautomotive%20scenes%20into%20multiple%20decoupled%20interpretable%20representations%20such%20as%0Ascene%20geometry%2C%20ego-motion%2C%20and%20object%20motion.%20Quantitative%20evaluation%20shows%0Athat%20MCDS-VSS%20achieves%20superior%20temporal%20consistency%20on%20video%20sequences%20while%0Aretaining%20competitive%20segmentation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCDS-VSS%253A%2520Moving%2520Camera%2520Dynamic%2520Scene%2520Video%2520Semantic%2520Segmentation%2520by%250A%2520%2520Filtering%2520with%2520Self-Supervised%2520Geometry%2520and%2520Motion%26entry.906535625%3DAngel%2520Villar-Corrales%2520and%2520Moritz%2520Austermann%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520Autonomous%2520systems%252C%2520such%2520as%2520self-driving%2520cars%252C%2520rely%2520on%2520reliable%2520semantic%250Aenvironment%2520perception%2520for%2520decision%2520making.%2520Despite%2520great%2520advances%2520in%2520video%250Asemantic%2520segmentation%252C%2520existing%2520approaches%2520ignore%2520important%2520inductive%2520biases%250Aand%2520lack%2520structured%2520and%2520interpretable%2520internal%2520representations.%2520In%2520this%2520work%252C%250Awe%2520propose%2520MCDS-VSS%252C%2520a%2520structured%2520filter%2520model%2520that%2520learns%2520in%2520a%2520self-supervised%250Amanner%2520to%2520estimate%2520scene%2520geometry%2520and%2520ego-motion%2520of%2520the%2520camera%252C%2520while%2520also%250Aestimating%2520the%2520motion%2520of%2520external%2520objects.%2520Our%2520model%2520leverages%2520these%250Arepresentations%2520to%2520improve%2520the%2520temporal%2520consistency%2520of%2520semantic%2520segmentation%250Awithout%2520sacrificing%2520segmentation%2520accuracy.%2520MCDS-VSS%2520follows%2520a%2520prediction-fusion%250Aapproach%2520in%2520which%2520scene%2520geometry%2520and%2520camera%2520motion%2520are%2520first%2520used%2520to%2520compensate%250Afor%2520ego-motion%252C%2520then%2520residual%2520flow%2520is%2520used%2520to%2520compensate%2520motion%2520of%2520dynamic%250Aobjects%252C%2520and%2520finally%2520the%2520predicted%2520scene%2520features%2520are%2520fused%2520with%2520the%2520current%250Afeatures%2520to%2520obtain%2520a%2520temporally%2520consistent%2520scene%2520segmentation.%2520Our%2520model%2520parses%250Aautomotive%2520scenes%2520into%2520multiple%2520decoupled%2520interpretable%2520representations%2520such%2520as%250Ascene%2520geometry%252C%2520ego-motion%252C%2520and%2520object%2520motion.%2520Quantitative%2520evaluation%2520shows%250Athat%2520MCDS-VSS%2520achieves%2520superior%2520temporal%2520consistency%2520on%2520video%2520sequences%2520while%250Aretaining%2520competitive%2520segmentation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCDS-VSS%3A%20Moving%20Camera%20Dynamic%20Scene%20Video%20Semantic%20Segmentation%20by%0A%20%20Filtering%20with%20Self-Supervised%20Geometry%20and%20Motion&entry.906535625=Angel%20Villar-Corrales%20and%20Moritz%20Austermann%20and%20Sven%20Behnke&entry.1292438233=%20%20Autonomous%20systems%2C%20such%20as%20self-driving%20cars%2C%20rely%20on%20reliable%20semantic%0Aenvironment%20perception%20for%20decision%20making.%20Despite%20great%20advances%20in%20video%0Asemantic%20segmentation%2C%20existing%20approaches%20ignore%20important%20inductive%20biases%0Aand%20lack%20structured%20and%20interpretable%20internal%20representations.%20In%20this%20work%2C%0Awe%20propose%20MCDS-VSS%2C%20a%20structured%20filter%20model%20that%20learns%20in%20a%20self-supervised%0Amanner%20to%20estimate%20scene%20geometry%20and%20ego-motion%20of%20the%20camera%2C%20while%20also%0Aestimating%20the%20motion%20of%20external%20objects.%20Our%20model%20leverages%20these%0Arepresentations%20to%20improve%20the%20temporal%20consistency%20of%20semantic%20segmentation%0Awithout%20sacrificing%20segmentation%20accuracy.%20MCDS-VSS%20follows%20a%20prediction-fusion%0Aapproach%20in%20which%20scene%20geometry%20and%20camera%20motion%20are%20first%20used%20to%20compensate%0Afor%20ego-motion%2C%20then%20residual%20flow%20is%20used%20to%20compensate%20motion%20of%20dynamic%0Aobjects%2C%20and%20finally%20the%20predicted%20scene%20features%20are%20fused%20with%20the%20current%0Afeatures%20to%20obtain%20a%20temporally%20consistent%20scene%20segmentation.%20Our%20model%20parses%0Aautomotive%20scenes%20into%20multiple%20decoupled%20interpretable%20representations%20such%20as%0Ascene%20geometry%2C%20ego-motion%2C%20and%20object%20motion.%20Quantitative%20evaluation%20shows%0Athat%20MCDS-VSS%20achieves%20superior%20temporal%20consistency%20on%20video%20sequences%20while%0Aretaining%20competitive%20segmentation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19921v1&entry.124074799=Read"},
{"title": "EMAG: Ego-motion Aware and Generalizable 2D Hand Forecasting from\n  Egocentric Videos", "author": "Masashi Hatano and Ryo Hachiuma and Hideo Saito", "abstract": "  Predicting future human behavior from egocentric videos is a challenging but\ncritical task for human intention understanding. Existing methods for\nforecasting 2D hand positions rely on visual representations and mainly focus\non hand-object interactions. In this paper, we investigate the hand forecasting\ntask and tackle two significant issues that persist in the existing methods:\n(1) 2D hand positions in future frames are severely affected by ego-motions in\negocentric videos; (2) prediction based on visual information tends to overfit\nto background or scene textures, posing a challenge for generalization on novel\nscenes or human behaviors. To solve the aforementioned problems, we propose\nEMAG, an ego-motion-aware and generalizable 2D hand forecasting method. In\nresponse to the first problem, we propose a method that considers ego-motion,\nrepresented by a sequence of homography matrices of two consecutive frames. We\nfurther leverage modalities such as optical flow, trajectories of hands and\ninteracting objects, and ego-motions, thereby alleviating the second issue.\nExtensive experiments on two large-scale egocentric video datasets, Ego4D and\nEPIC-Kitchens 55, verify the effectiveness of the proposed method. In\nparticular, our model outperforms prior methods by $7.0$\\% on cross-dataset\nevaluations. Project page: https://masashi-hatano.github.io/EMAG/\n", "link": "http://arxiv.org/abs/2405.20030v1", "date": "2024-05-30", "relevancy": 2.2108, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5651}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5586}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMAG%3A%20Ego-motion%20Aware%20and%20Generalizable%202D%20Hand%20Forecasting%20from%0A%20%20Egocentric%20Videos&body=Title%3A%20EMAG%3A%20Ego-motion%20Aware%20and%20Generalizable%202D%20Hand%20Forecasting%20from%0A%20%20Egocentric%20Videos%0AAuthor%3A%20Masashi%20Hatano%20and%20Ryo%20Hachiuma%20and%20Hideo%20Saito%0AAbstract%3A%20%20%20Predicting%20future%20human%20behavior%20from%20egocentric%20videos%20is%20a%20challenging%20but%0Acritical%20task%20for%20human%20intention%20understanding.%20Existing%20methods%20for%0Aforecasting%202D%20hand%20positions%20rely%20on%20visual%20representations%20and%20mainly%20focus%0Aon%20hand-object%20interactions.%20In%20this%20paper%2C%20we%20investigate%20the%20hand%20forecasting%0Atask%20and%20tackle%20two%20significant%20issues%20that%20persist%20in%20the%20existing%20methods%3A%0A%281%29%202D%20hand%20positions%20in%20future%20frames%20are%20severely%20affected%20by%20ego-motions%20in%0Aegocentric%20videos%3B%20%282%29%20prediction%20based%20on%20visual%20information%20tends%20to%20overfit%0Ato%20background%20or%20scene%20textures%2C%20posing%20a%20challenge%20for%20generalization%20on%20novel%0Ascenes%20or%20human%20behaviors.%20To%20solve%20the%20aforementioned%20problems%2C%20we%20propose%0AEMAG%2C%20an%20ego-motion-aware%20and%20generalizable%202D%20hand%20forecasting%20method.%20In%0Aresponse%20to%20the%20first%20problem%2C%20we%20propose%20a%20method%20that%20considers%20ego-motion%2C%0Arepresented%20by%20a%20sequence%20of%20homography%20matrices%20of%20two%20consecutive%20frames.%20We%0Afurther%20leverage%20modalities%20such%20as%20optical%20flow%2C%20trajectories%20of%20hands%20and%0Ainteracting%20objects%2C%20and%20ego-motions%2C%20thereby%20alleviating%20the%20second%20issue.%0AExtensive%20experiments%20on%20two%20large-scale%20egocentric%20video%20datasets%2C%20Ego4D%20and%0AEPIC-Kitchens%2055%2C%20verify%20the%20effectiveness%20of%20the%20proposed%20method.%20In%0Aparticular%2C%20our%20model%20outperforms%20prior%20methods%20by%20%247.0%24%5C%25%20on%20cross-dataset%0Aevaluations.%20Project%20page%3A%20https%3A//masashi-hatano.github.io/EMAG/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMAG%253A%2520Ego-motion%2520Aware%2520and%2520Generalizable%25202D%2520Hand%2520Forecasting%2520from%250A%2520%2520Egocentric%2520Videos%26entry.906535625%3DMasashi%2520Hatano%2520and%2520Ryo%2520Hachiuma%2520and%2520Hideo%2520Saito%26entry.1292438233%3D%2520%2520Predicting%2520future%2520human%2520behavior%2520from%2520egocentric%2520videos%2520is%2520a%2520challenging%2520but%250Acritical%2520task%2520for%2520human%2520intention%2520understanding.%2520Existing%2520methods%2520for%250Aforecasting%25202D%2520hand%2520positions%2520rely%2520on%2520visual%2520representations%2520and%2520mainly%2520focus%250Aon%2520hand-object%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520hand%2520forecasting%250Atask%2520and%2520tackle%2520two%2520significant%2520issues%2520that%2520persist%2520in%2520the%2520existing%2520methods%253A%250A%25281%2529%25202D%2520hand%2520positions%2520in%2520future%2520frames%2520are%2520severely%2520affected%2520by%2520ego-motions%2520in%250Aegocentric%2520videos%253B%2520%25282%2529%2520prediction%2520based%2520on%2520visual%2520information%2520tends%2520to%2520overfit%250Ato%2520background%2520or%2520scene%2520textures%252C%2520posing%2520a%2520challenge%2520for%2520generalization%2520on%2520novel%250Ascenes%2520or%2520human%2520behaviors.%2520To%2520solve%2520the%2520aforementioned%2520problems%252C%2520we%2520propose%250AEMAG%252C%2520an%2520ego-motion-aware%2520and%2520generalizable%25202D%2520hand%2520forecasting%2520method.%2520In%250Aresponse%2520to%2520the%2520first%2520problem%252C%2520we%2520propose%2520a%2520method%2520that%2520considers%2520ego-motion%252C%250Arepresented%2520by%2520a%2520sequence%2520of%2520homography%2520matrices%2520of%2520two%2520consecutive%2520frames.%2520We%250Afurther%2520leverage%2520modalities%2520such%2520as%2520optical%2520flow%252C%2520trajectories%2520of%2520hands%2520and%250Ainteracting%2520objects%252C%2520and%2520ego-motions%252C%2520thereby%2520alleviating%2520the%2520second%2520issue.%250AExtensive%2520experiments%2520on%2520two%2520large-scale%2520egocentric%2520video%2520datasets%252C%2520Ego4D%2520and%250AEPIC-Kitchens%252055%252C%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%2520In%250Aparticular%252C%2520our%2520model%2520outperforms%2520prior%2520methods%2520by%2520%25247.0%2524%255C%2525%2520on%2520cross-dataset%250Aevaluations.%2520Project%2520page%253A%2520https%253A//masashi-hatano.github.io/EMAG/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMAG%3A%20Ego-motion%20Aware%20and%20Generalizable%202D%20Hand%20Forecasting%20from%0A%20%20Egocentric%20Videos&entry.906535625=Masashi%20Hatano%20and%20Ryo%20Hachiuma%20and%20Hideo%20Saito&entry.1292438233=%20%20Predicting%20future%20human%20behavior%20from%20egocentric%20videos%20is%20a%20challenging%20but%0Acritical%20task%20for%20human%20intention%20understanding.%20Existing%20methods%20for%0Aforecasting%202D%20hand%20positions%20rely%20on%20visual%20representations%20and%20mainly%20focus%0Aon%20hand-object%20interactions.%20In%20this%20paper%2C%20we%20investigate%20the%20hand%20forecasting%0Atask%20and%20tackle%20two%20significant%20issues%20that%20persist%20in%20the%20existing%20methods%3A%0A%281%29%202D%20hand%20positions%20in%20future%20frames%20are%20severely%20affected%20by%20ego-motions%20in%0Aegocentric%20videos%3B%20%282%29%20prediction%20based%20on%20visual%20information%20tends%20to%20overfit%0Ato%20background%20or%20scene%20textures%2C%20posing%20a%20challenge%20for%20generalization%20on%20novel%0Ascenes%20or%20human%20behaviors.%20To%20solve%20the%20aforementioned%20problems%2C%20we%20propose%0AEMAG%2C%20an%20ego-motion-aware%20and%20generalizable%202D%20hand%20forecasting%20method.%20In%0Aresponse%20to%20the%20first%20problem%2C%20we%20propose%20a%20method%20that%20considers%20ego-motion%2C%0Arepresented%20by%20a%20sequence%20of%20homography%20matrices%20of%20two%20consecutive%20frames.%20We%0Afurther%20leverage%20modalities%20such%20as%20optical%20flow%2C%20trajectories%20of%20hands%20and%0Ainteracting%20objects%2C%20and%20ego-motions%2C%20thereby%20alleviating%20the%20second%20issue.%0AExtensive%20experiments%20on%20two%20large-scale%20egocentric%20video%20datasets%2C%20Ego4D%20and%0AEPIC-Kitchens%2055%2C%20verify%20the%20effectiveness%20of%20the%20proposed%20method.%20In%0Aparticular%2C%20our%20model%20outperforms%20prior%20methods%20by%20%247.0%24%5C%25%20on%20cross-dataset%0Aevaluations.%20Project%20page%3A%20https%3A//masashi-hatano.github.io/EMAG/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20030v1&entry.124074799=Read"},
{"title": "Nonuniqueness and Convergence to Equivalent Solutions in Observer-based\n  Inverse Reinforcement Learning", "author": "Jared Town and Zachary Morrison and Rushikesh Kamalapurkar", "abstract": "  A key challenge in solving the deterministic inverse reinforcement learning\n(IRL) problem online and in real-time is the existence of multiple solutions.\nNonuniqueness necessitates the study of the notion of equivalent solutions,\ni.e., solutions that result in a different cost functional but same feedback\nmatrix, and convergence to such solutions. While offline algorithms that result\nin convergence to equivalent solutions have been developed in the literature,\nonline, real-time techniques that address nonuniqueness are not available. In\nthis paper, a regularized history stack observer that converges to\napproximately equivalent solutions of the IRL problem is developed. Novel\ndata-richness conditions are developed to facilitate the analysis and\nsimulation results are provided to demonstrate the effectiveness of the\ndeveloped technique.\n", "link": "http://arxiv.org/abs/2210.16299v4", "date": "2024-05-30", "relevancy": 2.2097, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4623}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4319}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonuniqueness%20and%20Convergence%20to%20Equivalent%20Solutions%20in%20Observer-based%0A%20%20Inverse%20Reinforcement%20Learning&body=Title%3A%20Nonuniqueness%20and%20Convergence%20to%20Equivalent%20Solutions%20in%20Observer-based%0A%20%20Inverse%20Reinforcement%20Learning%0AAuthor%3A%20Jared%20Town%20and%20Zachary%20Morrison%20and%20Rushikesh%20Kamalapurkar%0AAbstract%3A%20%20%20A%20key%20challenge%20in%20solving%20the%20deterministic%20inverse%20reinforcement%20learning%0A%28IRL%29%20problem%20online%20and%20in%20real-time%20is%20the%20existence%20of%20multiple%20solutions.%0ANonuniqueness%20necessitates%20the%20study%20of%20the%20notion%20of%20equivalent%20solutions%2C%0Ai.e.%2C%20solutions%20that%20result%20in%20a%20different%20cost%20functional%20but%20same%20feedback%0Amatrix%2C%20and%20convergence%20to%20such%20solutions.%20While%20offline%20algorithms%20that%20result%0Ain%20convergence%20to%20equivalent%20solutions%20have%20been%20developed%20in%20the%20literature%2C%0Aonline%2C%20real-time%20techniques%20that%20address%20nonuniqueness%20are%20not%20available.%20In%0Athis%20paper%2C%20a%20regularized%20history%20stack%20observer%20that%20converges%20to%0Aapproximately%20equivalent%20solutions%20of%20the%20IRL%20problem%20is%20developed.%20Novel%0Adata-richness%20conditions%20are%20developed%20to%20facilitate%20the%20analysis%20and%0Asimulation%20results%20are%20provided%20to%20demonstrate%20the%20effectiveness%20of%20the%0Adeveloped%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.16299v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonuniqueness%2520and%2520Convergence%2520to%2520Equivalent%2520Solutions%2520in%2520Observer-based%250A%2520%2520Inverse%2520Reinforcement%2520Learning%26entry.906535625%3DJared%2520Town%2520and%2520Zachary%2520Morrison%2520and%2520Rushikesh%2520Kamalapurkar%26entry.1292438233%3D%2520%2520A%2520key%2520challenge%2520in%2520solving%2520the%2520deterministic%2520inverse%2520reinforcement%2520learning%250A%2528IRL%2529%2520problem%2520online%2520and%2520in%2520real-time%2520is%2520the%2520existence%2520of%2520multiple%2520solutions.%250ANonuniqueness%2520necessitates%2520the%2520study%2520of%2520the%2520notion%2520of%2520equivalent%2520solutions%252C%250Ai.e.%252C%2520solutions%2520that%2520result%2520in%2520a%2520different%2520cost%2520functional%2520but%2520same%2520feedback%250Amatrix%252C%2520and%2520convergence%2520to%2520such%2520solutions.%2520While%2520offline%2520algorithms%2520that%2520result%250Ain%2520convergence%2520to%2520equivalent%2520solutions%2520have%2520been%2520developed%2520in%2520the%2520literature%252C%250Aonline%252C%2520real-time%2520techniques%2520that%2520address%2520nonuniqueness%2520are%2520not%2520available.%2520In%250Athis%2520paper%252C%2520a%2520regularized%2520history%2520stack%2520observer%2520that%2520converges%2520to%250Aapproximately%2520equivalent%2520solutions%2520of%2520the%2520IRL%2520problem%2520is%2520developed.%2520Novel%250Adata-richness%2520conditions%2520are%2520developed%2520to%2520facilitate%2520the%2520analysis%2520and%250Asimulation%2520results%2520are%2520provided%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Adeveloped%2520technique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.16299v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonuniqueness%20and%20Convergence%20to%20Equivalent%20Solutions%20in%20Observer-based%0A%20%20Inverse%20Reinforcement%20Learning&entry.906535625=Jared%20Town%20and%20Zachary%20Morrison%20and%20Rushikesh%20Kamalapurkar&entry.1292438233=%20%20A%20key%20challenge%20in%20solving%20the%20deterministic%20inverse%20reinforcement%20learning%0A%28IRL%29%20problem%20online%20and%20in%20real-time%20is%20the%20existence%20of%20multiple%20solutions.%0ANonuniqueness%20necessitates%20the%20study%20of%20the%20notion%20of%20equivalent%20solutions%2C%0Ai.e.%2C%20solutions%20that%20result%20in%20a%20different%20cost%20functional%20but%20same%20feedback%0Amatrix%2C%20and%20convergence%20to%20such%20solutions.%20While%20offline%20algorithms%20that%20result%0Ain%20convergence%20to%20equivalent%20solutions%20have%20been%20developed%20in%20the%20literature%2C%0Aonline%2C%20real-time%20techniques%20that%20address%20nonuniqueness%20are%20not%20available.%20In%0Athis%20paper%2C%20a%20regularized%20history%20stack%20observer%20that%20converges%20to%0Aapproximately%20equivalent%20solutions%20of%20the%20IRL%20problem%20is%20developed.%20Novel%0Adata-richness%20conditions%20are%20developed%20to%20facilitate%20the%20analysis%20and%0Asimulation%20results%20are%20provided%20to%20demonstrate%20the%20effectiveness%20of%20the%0Adeveloped%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.16299v4&entry.124074799=Read"},
{"title": "A Geometric Unification of Distributionally Robust Covariance\n  Estimators: Shrinking the Spectrum by Inflating the Ambiguity Set", "author": "Man-Chung Yue and Yves Rychener and Daniel Kuhn and Viet Anh Nguyen", "abstract": "  The state-of-the-art methods for estimating high-dimensional covariance\nmatrices all shrink the eigenvalues of the sample covariance matrix towards a\ndata-insensitive shrinkage target. The underlying shrinkage transformation is\neither chosen heuristically - without compelling theoretical justification - or\noptimally in view of restrictive distributional assumptions. In this paper, we\npropose a principled approach to construct covariance estimators without\nimposing restrictive assumptions. That is, we study distributionally robust\ncovariance estimation problems that minimize the worst-case Frobenius error\nwith respect to all data distributions close to a nominal distribution, where\nthe proximity of distributions is measured via a divergence on the space of\ncovariance matrices. We identify mild conditions on this divergence under which\nthe resulting minimizers represent shrinkage estimators. We show that the\ncorresponding shrinkage transformations are intimately related to the\ngeometrical properties of the underlying divergence. We also prove that our\nrobust estimators are efficiently computable and asymptotically consistent and\nthat they enjoy finite-sample performance guarantees. We exemplify our general\nmethodology by synthesizing explicit estimators induced by the\nKullback-Leibler, Fisher-Rao, and Wasserstein divergences. Numerical\nexperiments based on synthetic and real data show that our robust estimators\nare competitive with state-of-the-art estimators.\n", "link": "http://arxiv.org/abs/2405.20124v1", "date": "2024-05-30", "relevancy": 2.2051, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4573}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4339}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Geometric%20Unification%20of%20Distributionally%20Robust%20Covariance%0A%20%20Estimators%3A%20Shrinking%20the%20Spectrum%20by%20Inflating%20the%20Ambiguity%20Set&body=Title%3A%20A%20Geometric%20Unification%20of%20Distributionally%20Robust%20Covariance%0A%20%20Estimators%3A%20Shrinking%20the%20Spectrum%20by%20Inflating%20the%20Ambiguity%20Set%0AAuthor%3A%20Man-Chung%20Yue%20and%20Yves%20Rychener%20and%20Daniel%20Kuhn%20and%20Viet%20Anh%20Nguyen%0AAbstract%3A%20%20%20The%20state-of-the-art%20methods%20for%20estimating%20high-dimensional%20covariance%0Amatrices%20all%20shrink%20the%20eigenvalues%20of%20the%20sample%20covariance%20matrix%20towards%20a%0Adata-insensitive%20shrinkage%20target.%20The%20underlying%20shrinkage%20transformation%20is%0Aeither%20chosen%20heuristically%20-%20without%20compelling%20theoretical%20justification%20-%20or%0Aoptimally%20in%20view%20of%20restrictive%20distributional%20assumptions.%20In%20this%20paper%2C%20we%0Apropose%20a%20principled%20approach%20to%20construct%20covariance%20estimators%20without%0Aimposing%20restrictive%20assumptions.%20That%20is%2C%20we%20study%20distributionally%20robust%0Acovariance%20estimation%20problems%20that%20minimize%20the%20worst-case%20Frobenius%20error%0Awith%20respect%20to%20all%20data%20distributions%20close%20to%20a%20nominal%20distribution%2C%20where%0Athe%20proximity%20of%20distributions%20is%20measured%20via%20a%20divergence%20on%20the%20space%20of%0Acovariance%20matrices.%20We%20identify%20mild%20conditions%20on%20this%20divergence%20under%20which%0Athe%20resulting%20minimizers%20represent%20shrinkage%20estimators.%20We%20show%20that%20the%0Acorresponding%20shrinkage%20transformations%20are%20intimately%20related%20to%20the%0Ageometrical%20properties%20of%20the%20underlying%20divergence.%20We%20also%20prove%20that%20our%0Arobust%20estimators%20are%20efficiently%20computable%20and%20asymptotically%20consistent%20and%0Athat%20they%20enjoy%20finite-sample%20performance%20guarantees.%20We%20exemplify%20our%20general%0Amethodology%20by%20synthesizing%20explicit%20estimators%20induced%20by%20the%0AKullback-Leibler%2C%20Fisher-Rao%2C%20and%20Wasserstein%20divergences.%20Numerical%0Aexperiments%20based%20on%20synthetic%20and%20real%20data%20show%20that%20our%20robust%20estimators%0Aare%20competitive%20with%20state-of-the-art%20estimators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Geometric%2520Unification%2520of%2520Distributionally%2520Robust%2520Covariance%250A%2520%2520Estimators%253A%2520Shrinking%2520the%2520Spectrum%2520by%2520Inflating%2520the%2520Ambiguity%2520Set%26entry.906535625%3DMan-Chung%2520Yue%2520and%2520Yves%2520Rychener%2520and%2520Daniel%2520Kuhn%2520and%2520Viet%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520The%2520state-of-the-art%2520methods%2520for%2520estimating%2520high-dimensional%2520covariance%250Amatrices%2520all%2520shrink%2520the%2520eigenvalues%2520of%2520the%2520sample%2520covariance%2520matrix%2520towards%2520a%250Adata-insensitive%2520shrinkage%2520target.%2520The%2520underlying%2520shrinkage%2520transformation%2520is%250Aeither%2520chosen%2520heuristically%2520-%2520without%2520compelling%2520theoretical%2520justification%2520-%2520or%250Aoptimally%2520in%2520view%2520of%2520restrictive%2520distributional%2520assumptions.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520principled%2520approach%2520to%2520construct%2520covariance%2520estimators%2520without%250Aimposing%2520restrictive%2520assumptions.%2520That%2520is%252C%2520we%2520study%2520distributionally%2520robust%250Acovariance%2520estimation%2520problems%2520that%2520minimize%2520the%2520worst-case%2520Frobenius%2520error%250Awith%2520respect%2520to%2520all%2520data%2520distributions%2520close%2520to%2520a%2520nominal%2520distribution%252C%2520where%250Athe%2520proximity%2520of%2520distributions%2520is%2520measured%2520via%2520a%2520divergence%2520on%2520the%2520space%2520of%250Acovariance%2520matrices.%2520We%2520identify%2520mild%2520conditions%2520on%2520this%2520divergence%2520under%2520which%250Athe%2520resulting%2520minimizers%2520represent%2520shrinkage%2520estimators.%2520We%2520show%2520that%2520the%250Acorresponding%2520shrinkage%2520transformations%2520are%2520intimately%2520related%2520to%2520the%250Ageometrical%2520properties%2520of%2520the%2520underlying%2520divergence.%2520We%2520also%2520prove%2520that%2520our%250Arobust%2520estimators%2520are%2520efficiently%2520computable%2520and%2520asymptotically%2520consistent%2520and%250Athat%2520they%2520enjoy%2520finite-sample%2520performance%2520guarantees.%2520We%2520exemplify%2520our%2520general%250Amethodology%2520by%2520synthesizing%2520explicit%2520estimators%2520induced%2520by%2520the%250AKullback-Leibler%252C%2520Fisher-Rao%252C%2520and%2520Wasserstein%2520divergences.%2520Numerical%250Aexperiments%2520based%2520on%2520synthetic%2520and%2520real%2520data%2520show%2520that%2520our%2520robust%2520estimators%250Aare%2520competitive%2520with%2520state-of-the-art%2520estimators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Geometric%20Unification%20of%20Distributionally%20Robust%20Covariance%0A%20%20Estimators%3A%20Shrinking%20the%20Spectrum%20by%20Inflating%20the%20Ambiguity%20Set&entry.906535625=Man-Chung%20Yue%20and%20Yves%20Rychener%20and%20Daniel%20Kuhn%20and%20Viet%20Anh%20Nguyen&entry.1292438233=%20%20The%20state-of-the-art%20methods%20for%20estimating%20high-dimensional%20covariance%0Amatrices%20all%20shrink%20the%20eigenvalues%20of%20the%20sample%20covariance%20matrix%20towards%20a%0Adata-insensitive%20shrinkage%20target.%20The%20underlying%20shrinkage%20transformation%20is%0Aeither%20chosen%20heuristically%20-%20without%20compelling%20theoretical%20justification%20-%20or%0Aoptimally%20in%20view%20of%20restrictive%20distributional%20assumptions.%20In%20this%20paper%2C%20we%0Apropose%20a%20principled%20approach%20to%20construct%20covariance%20estimators%20without%0Aimposing%20restrictive%20assumptions.%20That%20is%2C%20we%20study%20distributionally%20robust%0Acovariance%20estimation%20problems%20that%20minimize%20the%20worst-case%20Frobenius%20error%0Awith%20respect%20to%20all%20data%20distributions%20close%20to%20a%20nominal%20distribution%2C%20where%0Athe%20proximity%20of%20distributions%20is%20measured%20via%20a%20divergence%20on%20the%20space%20of%0Acovariance%20matrices.%20We%20identify%20mild%20conditions%20on%20this%20divergence%20under%20which%0Athe%20resulting%20minimizers%20represent%20shrinkage%20estimators.%20We%20show%20that%20the%0Acorresponding%20shrinkage%20transformations%20are%20intimately%20related%20to%20the%0Ageometrical%20properties%20of%20the%20underlying%20divergence.%20We%20also%20prove%20that%20our%0Arobust%20estimators%20are%20efficiently%20computable%20and%20asymptotically%20consistent%20and%0Athat%20they%20enjoy%20finite-sample%20performance%20guarantees.%20We%20exemplify%20our%20general%0Amethodology%20by%20synthesizing%20explicit%20estimators%20induced%20by%20the%0AKullback-Leibler%2C%20Fisher-Rao%2C%20and%20Wasserstein%20divergences.%20Numerical%0Aexperiments%20based%20on%20synthetic%20and%20real%20data%20show%20that%20our%20robust%20estimators%0Aare%20competitive%20with%20state-of-the-art%20estimators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20124v1&entry.124074799=Read"},
{"title": "Spectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation", "author": "Adam Sorrenti", "abstract": "  Separating vocal elements from musical tracks is a longstanding challenge in\naudio signal processing. This study tackles the distinct separation of vocal\ncomponents from musical spectrograms. We employ the Short Time Fourier\nTransform (STFT) to extract audio waves into detailed frequency-time\nspectrograms, utilizing the benchmark MUSDB18 dataset for music separation.\nSubsequently, we implement a UNet neural network to segment the spectrogram\nimage, aiming to delineate and extract singing voice components accurately. We\nachieved noteworthy results in audio source separation using of our U-Net-based\nmodels. The combination of frequency-axis normalization with Min/Max scaling\nand the Mean Absolute Error (MAE) loss function achieved the highest\nSource-to-Distortion Ratio (SDR) of 7.1 dB, indicating a high level of accuracy\nin preserving the quality of the original signal during separation. This setup\nalso recorded impressive Source-to-Interference Ratio (SIR) and\nSource-to-Artifact Ratio (SAR) scores of 25.2 dB and 7.2 dB, respectively.\nThese values significantly outperformed other configurations, particularly\nthose using Quantile-based normalization or a Mean Squared Error (MSE) loss\nfunction. Our source code, model weights, and demo material can be found at the\nproject's GitHub repository: https://github.com/mbrotos/SoundSeg\n", "link": "http://arxiv.org/abs/2405.20059v1", "date": "2024-05-30", "relevancy": 2.197, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4505}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4342}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Mapping%20of%20Singing%20Voices%3A%20U-Net-Assisted%20Vocal%20Segmentation&body=Title%3A%20Spectral%20Mapping%20of%20Singing%20Voices%3A%20U-Net-Assisted%20Vocal%20Segmentation%0AAuthor%3A%20Adam%20Sorrenti%0AAbstract%3A%20%20%20Separating%20vocal%20elements%20from%20musical%20tracks%20is%20a%20longstanding%20challenge%20in%0Aaudio%20signal%20processing.%20This%20study%20tackles%20the%20distinct%20separation%20of%20vocal%0Acomponents%20from%20musical%20spectrograms.%20We%20employ%20the%20Short%20Time%20Fourier%0ATransform%20%28STFT%29%20to%20extract%20audio%20waves%20into%20detailed%20frequency-time%0Aspectrograms%2C%20utilizing%20the%20benchmark%20MUSDB18%20dataset%20for%20music%20separation.%0ASubsequently%2C%20we%20implement%20a%20UNet%20neural%20network%20to%20segment%20the%20spectrogram%0Aimage%2C%20aiming%20to%20delineate%20and%20extract%20singing%20voice%20components%20accurately.%20We%0Aachieved%20noteworthy%20results%20in%20audio%20source%20separation%20using%20of%20our%20U-Net-based%0Amodels.%20The%20combination%20of%20frequency-axis%20normalization%20with%20Min/Max%20scaling%0Aand%20the%20Mean%20Absolute%20Error%20%28MAE%29%20loss%20function%20achieved%20the%20highest%0ASource-to-Distortion%20Ratio%20%28SDR%29%20of%207.1%20dB%2C%20indicating%20a%20high%20level%20of%20accuracy%0Ain%20preserving%20the%20quality%20of%20the%20original%20signal%20during%20separation.%20This%20setup%0Aalso%20recorded%20impressive%20Source-to-Interference%20Ratio%20%28SIR%29%20and%0ASource-to-Artifact%20Ratio%20%28SAR%29%20scores%20of%2025.2%20dB%20and%207.2%20dB%2C%20respectively.%0AThese%20values%20significantly%20outperformed%20other%20configurations%2C%20particularly%0Athose%20using%20Quantile-based%20normalization%20or%20a%20Mean%20Squared%20Error%20%28MSE%29%20loss%0Afunction.%20Our%20source%20code%2C%20model%20weights%2C%20and%20demo%20material%20can%20be%20found%20at%20the%0Aproject%27s%20GitHub%20repository%3A%20https%3A//github.com/mbrotos/SoundSeg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Mapping%2520of%2520Singing%2520Voices%253A%2520U-Net-Assisted%2520Vocal%2520Segmentation%26entry.906535625%3DAdam%2520Sorrenti%26entry.1292438233%3D%2520%2520Separating%2520vocal%2520elements%2520from%2520musical%2520tracks%2520is%2520a%2520longstanding%2520challenge%2520in%250Aaudio%2520signal%2520processing.%2520This%2520study%2520tackles%2520the%2520distinct%2520separation%2520of%2520vocal%250Acomponents%2520from%2520musical%2520spectrograms.%2520We%2520employ%2520the%2520Short%2520Time%2520Fourier%250ATransform%2520%2528STFT%2529%2520to%2520extract%2520audio%2520waves%2520into%2520detailed%2520frequency-time%250Aspectrograms%252C%2520utilizing%2520the%2520benchmark%2520MUSDB18%2520dataset%2520for%2520music%2520separation.%250ASubsequently%252C%2520we%2520implement%2520a%2520UNet%2520neural%2520network%2520to%2520segment%2520the%2520spectrogram%250Aimage%252C%2520aiming%2520to%2520delineate%2520and%2520extract%2520singing%2520voice%2520components%2520accurately.%2520We%250Aachieved%2520noteworthy%2520results%2520in%2520audio%2520source%2520separation%2520using%2520of%2520our%2520U-Net-based%250Amodels.%2520The%2520combination%2520of%2520frequency-axis%2520normalization%2520with%2520Min/Max%2520scaling%250Aand%2520the%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520loss%2520function%2520achieved%2520the%2520highest%250ASource-to-Distortion%2520Ratio%2520%2528SDR%2529%2520of%25207.1%2520dB%252C%2520indicating%2520a%2520high%2520level%2520of%2520accuracy%250Ain%2520preserving%2520the%2520quality%2520of%2520the%2520original%2520signal%2520during%2520separation.%2520This%2520setup%250Aalso%2520recorded%2520impressive%2520Source-to-Interference%2520Ratio%2520%2528SIR%2529%2520and%250ASource-to-Artifact%2520Ratio%2520%2528SAR%2529%2520scores%2520of%252025.2%2520dB%2520and%25207.2%2520dB%252C%2520respectively.%250AThese%2520values%2520significantly%2520outperformed%2520other%2520configurations%252C%2520particularly%250Athose%2520using%2520Quantile-based%2520normalization%2520or%2520a%2520Mean%2520Squared%2520Error%2520%2528MSE%2529%2520loss%250Afunction.%2520Our%2520source%2520code%252C%2520model%2520weights%252C%2520and%2520demo%2520material%2520can%2520be%2520found%2520at%2520the%250Aproject%2527s%2520GitHub%2520repository%253A%2520https%253A//github.com/mbrotos/SoundSeg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Mapping%20of%20Singing%20Voices%3A%20U-Net-Assisted%20Vocal%20Segmentation&entry.906535625=Adam%20Sorrenti&entry.1292438233=%20%20Separating%20vocal%20elements%20from%20musical%20tracks%20is%20a%20longstanding%20challenge%20in%0Aaudio%20signal%20processing.%20This%20study%20tackles%20the%20distinct%20separation%20of%20vocal%0Acomponents%20from%20musical%20spectrograms.%20We%20employ%20the%20Short%20Time%20Fourier%0ATransform%20%28STFT%29%20to%20extract%20audio%20waves%20into%20detailed%20frequency-time%0Aspectrograms%2C%20utilizing%20the%20benchmark%20MUSDB18%20dataset%20for%20music%20separation.%0ASubsequently%2C%20we%20implement%20a%20UNet%20neural%20network%20to%20segment%20the%20spectrogram%0Aimage%2C%20aiming%20to%20delineate%20and%20extract%20singing%20voice%20components%20accurately.%20We%0Aachieved%20noteworthy%20results%20in%20audio%20source%20separation%20using%20of%20our%20U-Net-based%0Amodels.%20The%20combination%20of%20frequency-axis%20normalization%20with%20Min/Max%20scaling%0Aand%20the%20Mean%20Absolute%20Error%20%28MAE%29%20loss%20function%20achieved%20the%20highest%0ASource-to-Distortion%20Ratio%20%28SDR%29%20of%207.1%20dB%2C%20indicating%20a%20high%20level%20of%20accuracy%0Ain%20preserving%20the%20quality%20of%20the%20original%20signal%20during%20separation.%20This%20setup%0Aalso%20recorded%20impressive%20Source-to-Interference%20Ratio%20%28SIR%29%20and%0ASource-to-Artifact%20Ratio%20%28SAR%29%20scores%20of%2025.2%20dB%20and%207.2%20dB%2C%20respectively.%0AThese%20values%20significantly%20outperformed%20other%20configurations%2C%20particularly%0Athose%20using%20Quantile-based%20normalization%20or%20a%20Mean%20Squared%20Error%20%28MSE%29%20loss%0Afunction.%20Our%20source%20code%2C%20model%20weights%2C%20and%20demo%20material%20can%20be%20found%20at%20the%0Aproject%27s%20GitHub%20repository%3A%20https%3A//github.com/mbrotos/SoundSeg%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20059v1&entry.124074799=Read"},
{"title": "From Zero to Hero: Cold-Start Anomaly Detection", "author": "Tal Reiss and George Kour and Naama Zwerdling and Ateret Anaby-Tavor and Yedid Hoshen", "abstract": "  When first deploying an anomaly detection system, e.g., to detect\nout-of-scope queries in chatbots, there are no observed data, making\ndata-driven approaches ineffective. Zero-shot anomaly detection methods offer a\nsolution to such \"cold-start\" cases, but unfortunately they are often not\naccurate enough. This paper studies the realistic but underexplored cold-start\nsetting where an anomaly detection model is initialized using zero-shot\nguidance, but subsequently receives a small number of contaminated observations\n(namely, that may include anomalies). The goal is to make efficient use of both\nthe zero-shot guidance and the observations. We propose ColdFusion, a method\nthat effectively adapts the zero-shot anomaly detector to contaminated\nobservations. To support future development of this new setting, we propose an\nevaluation suite consisting of evaluation protocols and metrics.\n", "link": "http://arxiv.org/abs/2405.20341v1", "date": "2024-05-30", "relevancy": 2.1832, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4406}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.435}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Zero%20to%20Hero%3A%20Cold-Start%20Anomaly%20Detection&body=Title%3A%20From%20Zero%20to%20Hero%3A%20Cold-Start%20Anomaly%20Detection%0AAuthor%3A%20Tal%20Reiss%20and%20George%20Kour%20and%20Naama%20Zwerdling%20and%20Ateret%20Anaby-Tavor%20and%20Yedid%20Hoshen%0AAbstract%3A%20%20%20When%20first%20deploying%20an%20anomaly%20detection%20system%2C%20e.g.%2C%20to%20detect%0Aout-of-scope%20queries%20in%20chatbots%2C%20there%20are%20no%20observed%20data%2C%20making%0Adata-driven%20approaches%20ineffective.%20Zero-shot%20anomaly%20detection%20methods%20offer%20a%0Asolution%20to%20such%20%22cold-start%22%20cases%2C%20but%20unfortunately%20they%20are%20often%20not%0Aaccurate%20enough.%20This%20paper%20studies%20the%20realistic%20but%20underexplored%20cold-start%0Asetting%20where%20an%20anomaly%20detection%20model%20is%20initialized%20using%20zero-shot%0Aguidance%2C%20but%20subsequently%20receives%20a%20small%20number%20of%20contaminated%20observations%0A%28namely%2C%20that%20may%20include%20anomalies%29.%20The%20goal%20is%20to%20make%20efficient%20use%20of%20both%0Athe%20zero-shot%20guidance%20and%20the%20observations.%20We%20propose%20ColdFusion%2C%20a%20method%0Athat%20effectively%20adapts%20the%20zero-shot%20anomaly%20detector%20to%20contaminated%0Aobservations.%20To%20support%20future%20development%20of%20this%20new%20setting%2C%20we%20propose%20an%0Aevaluation%20suite%20consisting%20of%20evaluation%20protocols%20and%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Zero%2520to%2520Hero%253A%2520Cold-Start%2520Anomaly%2520Detection%26entry.906535625%3DTal%2520Reiss%2520and%2520George%2520Kour%2520and%2520Naama%2520Zwerdling%2520and%2520Ateret%2520Anaby-Tavor%2520and%2520Yedid%2520Hoshen%26entry.1292438233%3D%2520%2520When%2520first%2520deploying%2520an%2520anomaly%2520detection%2520system%252C%2520e.g.%252C%2520to%2520detect%250Aout-of-scope%2520queries%2520in%2520chatbots%252C%2520there%2520are%2520no%2520observed%2520data%252C%2520making%250Adata-driven%2520approaches%2520ineffective.%2520Zero-shot%2520anomaly%2520detection%2520methods%2520offer%2520a%250Asolution%2520to%2520such%2520%2522cold-start%2522%2520cases%252C%2520but%2520unfortunately%2520they%2520are%2520often%2520not%250Aaccurate%2520enough.%2520This%2520paper%2520studies%2520the%2520realistic%2520but%2520underexplored%2520cold-start%250Asetting%2520where%2520an%2520anomaly%2520detection%2520model%2520is%2520initialized%2520using%2520zero-shot%250Aguidance%252C%2520but%2520subsequently%2520receives%2520a%2520small%2520number%2520of%2520contaminated%2520observations%250A%2528namely%252C%2520that%2520may%2520include%2520anomalies%2529.%2520The%2520goal%2520is%2520to%2520make%2520efficient%2520use%2520of%2520both%250Athe%2520zero-shot%2520guidance%2520and%2520the%2520observations.%2520We%2520propose%2520ColdFusion%252C%2520a%2520method%250Athat%2520effectively%2520adapts%2520the%2520zero-shot%2520anomaly%2520detector%2520to%2520contaminated%250Aobservations.%2520To%2520support%2520future%2520development%2520of%2520this%2520new%2520setting%252C%2520we%2520propose%2520an%250Aevaluation%2520suite%2520consisting%2520of%2520evaluation%2520protocols%2520and%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Zero%20to%20Hero%3A%20Cold-Start%20Anomaly%20Detection&entry.906535625=Tal%20Reiss%20and%20George%20Kour%20and%20Naama%20Zwerdling%20and%20Ateret%20Anaby-Tavor%20and%20Yedid%20Hoshen&entry.1292438233=%20%20When%20first%20deploying%20an%20anomaly%20detection%20system%2C%20e.g.%2C%20to%20detect%0Aout-of-scope%20queries%20in%20chatbots%2C%20there%20are%20no%20observed%20data%2C%20making%0Adata-driven%20approaches%20ineffective.%20Zero-shot%20anomaly%20detection%20methods%20offer%20a%0Asolution%20to%20such%20%22cold-start%22%20cases%2C%20but%20unfortunately%20they%20are%20often%20not%0Aaccurate%20enough.%20This%20paper%20studies%20the%20realistic%20but%20underexplored%20cold-start%0Asetting%20where%20an%20anomaly%20detection%20model%20is%20initialized%20using%20zero-shot%0Aguidance%2C%20but%20subsequently%20receives%20a%20small%20number%20of%20contaminated%20observations%0A%28namely%2C%20that%20may%20include%20anomalies%29.%20The%20goal%20is%20to%20make%20efficient%20use%20of%20both%0Athe%20zero-shot%20guidance%20and%20the%20observations.%20We%20propose%20ColdFusion%2C%20a%20method%0Athat%20effectively%20adapts%20the%20zero-shot%20anomaly%20detector%20to%20contaminated%0Aobservations.%20To%20support%20future%20development%20of%20this%20new%20setting%2C%20we%20propose%20an%0Aevaluation%20suite%20consisting%20of%20evaluation%20protocols%20and%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20341v1&entry.124074799=Read"},
{"title": "ReMatch: Retrieval Enhanced Schema Matching with LLMs", "author": "Eitam Sheetrit and Menachem Brief and Moshik Mishaeli and Oren Elisha", "abstract": "  Schema matching is a crucial task in data integration, involving the\nalignment of a source schema with a target schema to establish correspondence\nbetween their elements. This task is challenging due to textual and semantic\nheterogeneity, as well as differences in schema sizes. Although\nmachine-learning-based solutions have been explored in numerous studies, they\noften suffer from low accuracy, require manual mapping of the schemas for model\ntraining, or need access to source schema data which might be unavailable due\nto privacy concerns. In this paper we present a novel method, named ReMatch,\nfor matching schemas using retrieval-enhanced Large Language Models (LLMs). Our\nmethod avoids the need for predefined mapping, any model training, or access to\ndata in the source database. Our experimental results on large real-world\nschemas demonstrate that ReMatch is an effective matcher. By eliminating the\nrequirement for training data, ReMatch becomes a viable solution for real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2403.01567v2", "date": "2024-05-30", "relevancy": 2.1758, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4451}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4329}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReMatch%3A%20Retrieval%20Enhanced%20Schema%20Matching%20with%20LLMs&body=Title%3A%20ReMatch%3A%20Retrieval%20Enhanced%20Schema%20Matching%20with%20LLMs%0AAuthor%3A%20Eitam%20Sheetrit%20and%20Menachem%20Brief%20and%20Moshik%20Mishaeli%20and%20Oren%20Elisha%0AAbstract%3A%20%20%20Schema%20matching%20is%20a%20crucial%20task%20in%20data%20integration%2C%20involving%20the%0Aalignment%20of%20a%20source%20schema%20with%20a%20target%20schema%20to%20establish%20correspondence%0Abetween%20their%20elements.%20This%20task%20is%20challenging%20due%20to%20textual%20and%20semantic%0Aheterogeneity%2C%20as%20well%20as%20differences%20in%20schema%20sizes.%20Although%0Amachine-learning-based%20solutions%20have%20been%20explored%20in%20numerous%20studies%2C%20they%0Aoften%20suffer%20from%20low%20accuracy%2C%20require%20manual%20mapping%20of%20the%20schemas%20for%20model%0Atraining%2C%20or%20need%20access%20to%20source%20schema%20data%20which%20might%20be%20unavailable%20due%0Ato%20privacy%20concerns.%20In%20this%20paper%20we%20present%20a%20novel%20method%2C%20named%20ReMatch%2C%0Afor%20matching%20schemas%20using%20retrieval-enhanced%20Large%20Language%20Models%20%28LLMs%29.%20Our%0Amethod%20avoids%20the%20need%20for%20predefined%20mapping%2C%20any%20model%20training%2C%20or%20access%20to%0Adata%20in%20the%20source%20database.%20Our%20experimental%20results%20on%20large%20real-world%0Aschemas%20demonstrate%20that%20ReMatch%20is%20an%20effective%20matcher.%20By%20eliminating%20the%0Arequirement%20for%20training%20data%2C%20ReMatch%20becomes%20a%20viable%20solution%20for%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReMatch%253A%2520Retrieval%2520Enhanced%2520Schema%2520Matching%2520with%2520LLMs%26entry.906535625%3DEitam%2520Sheetrit%2520and%2520Menachem%2520Brief%2520and%2520Moshik%2520Mishaeli%2520and%2520Oren%2520Elisha%26entry.1292438233%3D%2520%2520Schema%2520matching%2520is%2520a%2520crucial%2520task%2520in%2520data%2520integration%252C%2520involving%2520the%250Aalignment%2520of%2520a%2520source%2520schema%2520with%2520a%2520target%2520schema%2520to%2520establish%2520correspondence%250Abetween%2520their%2520elements.%2520This%2520task%2520is%2520challenging%2520due%2520to%2520textual%2520and%2520semantic%250Aheterogeneity%252C%2520as%2520well%2520as%2520differences%2520in%2520schema%2520sizes.%2520Although%250Amachine-learning-based%2520solutions%2520have%2520been%2520explored%2520in%2520numerous%2520studies%252C%2520they%250Aoften%2520suffer%2520from%2520low%2520accuracy%252C%2520require%2520manual%2520mapping%2520of%2520the%2520schemas%2520for%2520model%250Atraining%252C%2520or%2520need%2520access%2520to%2520source%2520schema%2520data%2520which%2520might%2520be%2520unavailable%2520due%250Ato%2520privacy%2520concerns.%2520In%2520this%2520paper%2520we%2520present%2520a%2520novel%2520method%252C%2520named%2520ReMatch%252C%250Afor%2520matching%2520schemas%2520using%2520retrieval-enhanced%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Our%250Amethod%2520avoids%2520the%2520need%2520for%2520predefined%2520mapping%252C%2520any%2520model%2520training%252C%2520or%2520access%2520to%250Adata%2520in%2520the%2520source%2520database.%2520Our%2520experimental%2520results%2520on%2520large%2520real-world%250Aschemas%2520demonstrate%2520that%2520ReMatch%2520is%2520an%2520effective%2520matcher.%2520By%2520eliminating%2520the%250Arequirement%2520for%2520training%2520data%252C%2520ReMatch%2520becomes%2520a%2520viable%2520solution%2520for%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReMatch%3A%20Retrieval%20Enhanced%20Schema%20Matching%20with%20LLMs&entry.906535625=Eitam%20Sheetrit%20and%20Menachem%20Brief%20and%20Moshik%20Mishaeli%20and%20Oren%20Elisha&entry.1292438233=%20%20Schema%20matching%20is%20a%20crucial%20task%20in%20data%20integration%2C%20involving%20the%0Aalignment%20of%20a%20source%20schema%20with%20a%20target%20schema%20to%20establish%20correspondence%0Abetween%20their%20elements.%20This%20task%20is%20challenging%20due%20to%20textual%20and%20semantic%0Aheterogeneity%2C%20as%20well%20as%20differences%20in%20schema%20sizes.%20Although%0Amachine-learning-based%20solutions%20have%20been%20explored%20in%20numerous%20studies%2C%20they%0Aoften%20suffer%20from%20low%20accuracy%2C%20require%20manual%20mapping%20of%20the%20schemas%20for%20model%0Atraining%2C%20or%20need%20access%20to%20source%20schema%20data%20which%20might%20be%20unavailable%20due%0Ato%20privacy%20concerns.%20In%20this%20paper%20we%20present%20a%20novel%20method%2C%20named%20ReMatch%2C%0Afor%20matching%20schemas%20using%20retrieval-enhanced%20Large%20Language%20Models%20%28LLMs%29.%20Our%0Amethod%20avoids%20the%20need%20for%20predefined%20mapping%2C%20any%20model%20training%2C%20or%20access%20to%0Adata%20in%20the%20source%20database.%20Our%20experimental%20results%20on%20large%20real-world%0Aschemas%20demonstrate%20that%20ReMatch%20is%20an%20effective%20matcher.%20By%20eliminating%20the%0Arequirement%20for%20training%20data%2C%20ReMatch%20becomes%20a%20viable%20solution%20for%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01567v2&entry.124074799=Read"},
{"title": "Consistent Submodular Maximization", "author": "Paul D\u00fctting and Federico Fusco and Silvio Lattanzi and Ashkan Norouzi-Fard and Morteza Zadimoghaddam", "abstract": "  Maximizing monotone submodular functions under cardinality constraints is a\nclassic optimization task with several applications in data mining and machine\nlearning. In this paper we study this problem in a dynamic environment with\nconsistency constraints: elements arrive in a streaming fashion and the goal is\nmaintaining a constant approximation to the optimal solution while having a\nstable solution (i.e., the number of changes between two consecutive solutions\nis bounded). We provide algorithms in this setting with different trade-offs\nbetween consistency and approximation quality. We also complement our\ntheoretical results with an experimental analysis showing the effectiveness of\nour algorithms in real-world instances.\n", "link": "http://arxiv.org/abs/2405.19977v1", "date": "2024-05-30", "relevancy": 2.1557, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.439}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4336}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Submodular%20Maximization&body=Title%3A%20Consistent%20Submodular%20Maximization%0AAuthor%3A%20Paul%20D%C3%BCtting%20and%20Federico%20Fusco%20and%20Silvio%20Lattanzi%20and%20Ashkan%20Norouzi-Fard%20and%20Morteza%20Zadimoghaddam%0AAbstract%3A%20%20%20Maximizing%20monotone%20submodular%20functions%20under%20cardinality%20constraints%20is%20a%0Aclassic%20optimization%20task%20with%20several%20applications%20in%20data%20mining%20and%20machine%0Alearning.%20In%20this%20paper%20we%20study%20this%20problem%20in%20a%20dynamic%20environment%20with%0Aconsistency%20constraints%3A%20elements%20arrive%20in%20a%20streaming%20fashion%20and%20the%20goal%20is%0Amaintaining%20a%20constant%20approximation%20to%20the%20optimal%20solution%20while%20having%20a%0Astable%20solution%20%28i.e.%2C%20the%20number%20of%20changes%20between%20two%20consecutive%20solutions%0Ais%20bounded%29.%20We%20provide%20algorithms%20in%20this%20setting%20with%20different%20trade-offs%0Abetween%20consistency%20and%20approximation%20quality.%20We%20also%20complement%20our%0Atheoretical%20results%20with%20an%20experimental%20analysis%20showing%20the%20effectiveness%20of%0Aour%20algorithms%20in%20real-world%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Submodular%2520Maximization%26entry.906535625%3DPaul%2520D%25C3%25BCtting%2520and%2520Federico%2520Fusco%2520and%2520Silvio%2520Lattanzi%2520and%2520Ashkan%2520Norouzi-Fard%2520and%2520Morteza%2520Zadimoghaddam%26entry.1292438233%3D%2520%2520Maximizing%2520monotone%2520submodular%2520functions%2520under%2520cardinality%2520constraints%2520is%2520a%250Aclassic%2520optimization%2520task%2520with%2520several%2520applications%2520in%2520data%2520mining%2520and%2520machine%250Alearning.%2520In%2520this%2520paper%2520we%2520study%2520this%2520problem%2520in%2520a%2520dynamic%2520environment%2520with%250Aconsistency%2520constraints%253A%2520elements%2520arrive%2520in%2520a%2520streaming%2520fashion%2520and%2520the%2520goal%2520is%250Amaintaining%2520a%2520constant%2520approximation%2520to%2520the%2520optimal%2520solution%2520while%2520having%2520a%250Astable%2520solution%2520%2528i.e.%252C%2520the%2520number%2520of%2520changes%2520between%2520two%2520consecutive%2520solutions%250Ais%2520bounded%2529.%2520We%2520provide%2520algorithms%2520in%2520this%2520setting%2520with%2520different%2520trade-offs%250Abetween%2520consistency%2520and%2520approximation%2520quality.%2520We%2520also%2520complement%2520our%250Atheoretical%2520results%2520with%2520an%2520experimental%2520analysis%2520showing%2520the%2520effectiveness%2520of%250Aour%2520algorithms%2520in%2520real-world%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Submodular%20Maximization&entry.906535625=Paul%20D%C3%BCtting%20and%20Federico%20Fusco%20and%20Silvio%20Lattanzi%20and%20Ashkan%20Norouzi-Fard%20and%20Morteza%20Zadimoghaddam&entry.1292438233=%20%20Maximizing%20monotone%20submodular%20functions%20under%20cardinality%20constraints%20is%20a%0Aclassic%20optimization%20task%20with%20several%20applications%20in%20data%20mining%20and%20machine%0Alearning.%20In%20this%20paper%20we%20study%20this%20problem%20in%20a%20dynamic%20environment%20with%0Aconsistency%20constraints%3A%20elements%20arrive%20in%20a%20streaming%20fashion%20and%20the%20goal%20is%0Amaintaining%20a%20constant%20approximation%20to%20the%20optimal%20solution%20while%20having%20a%0Astable%20solution%20%28i.e.%2C%20the%20number%20of%20changes%20between%20two%20consecutive%20solutions%0Ais%20bounded%29.%20We%20provide%20algorithms%20in%20this%20setting%20with%20different%20trade-offs%0Abetween%20consistency%20and%20approximation%20quality.%20We%20also%20complement%20our%0Atheoretical%20results%20with%20an%20experimental%20analysis%20showing%20the%20effectiveness%20of%0Aour%20algorithms%20in%20real-world%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19977v1&entry.124074799=Read"},
{"title": "Image-to-Joint Inverse Kinematic of a Supportive Continuum Arm Using\n  Deep Learning", "author": "Shayan Sepahvand and Guanghui Wang and Farrokh Janabi-Sharifi", "abstract": "  In this work, a deep learning-based technique is used to study the\nimage-to-joint inverse kinematics of a tendon-driven supportive continuum arm.\nAn eye-off-hand configuration is considered by mounting a camera at a fixed\npose with respect to the inertial frame attached at the arm base. This camera\ncaptures an image for each distinct joint variable at each sampling time to\nconstruct the training dataset. This dataset is then employed to adapt a\nfeed-forward deep convolutional neural network, namely the modified VGG-16\nmodel, to estimate the joint variable. One thousand images are recorded to\ntrain the deep network, and transfer learning and fine-tuning techniques are\napplied to the modified VGG-16 to further improve the training. Finally,\ntraining is also completed with a larger dataset of images that are affected by\nvarious types of noises, changes in illumination, and partial occlusion. The\nmain contribution of this research is the development of an image-to-joint\nnetwork that can estimate the joint variable given an image of the arm, even if\nthe image is not captured in an ideal condition. The key benefits of this\nresearch are twofold: 1) image-to-joint mapping can offer a real-time\nalternative to computationally complex inverse kinematic mapping through\nanalytical models; and 2) the proposed technique can provide robustness against\nnoise, occlusion, and changes in illumination. The dataset is publicly\navailable on Kaggle.\n", "link": "http://arxiv.org/abs/2405.20248v1", "date": "2024-05-30", "relevancy": 2.1542, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5471}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.537}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-to-Joint%20Inverse%20Kinematic%20of%20a%20Supportive%20Continuum%20Arm%20Using%0A%20%20Deep%20Learning&body=Title%3A%20Image-to-Joint%20Inverse%20Kinematic%20of%20a%20Supportive%20Continuum%20Arm%20Using%0A%20%20Deep%20Learning%0AAuthor%3A%20Shayan%20Sepahvand%20and%20Guanghui%20Wang%20and%20Farrokh%20Janabi-Sharifi%0AAbstract%3A%20%20%20In%20this%20work%2C%20a%20deep%20learning-based%20technique%20is%20used%20to%20study%20the%0Aimage-to-joint%20inverse%20kinematics%20of%20a%20tendon-driven%20supportive%20continuum%20arm.%0AAn%20eye-off-hand%20configuration%20is%20considered%20by%20mounting%20a%20camera%20at%20a%20fixed%0Apose%20with%20respect%20to%20the%20inertial%20frame%20attached%20at%20the%20arm%20base.%20This%20camera%0Acaptures%20an%20image%20for%20each%20distinct%20joint%20variable%20at%20each%20sampling%20time%20to%0Aconstruct%20the%20training%20dataset.%20This%20dataset%20is%20then%20employed%20to%20adapt%20a%0Afeed-forward%20deep%20convolutional%20neural%20network%2C%20namely%20the%20modified%20VGG-16%0Amodel%2C%20to%20estimate%20the%20joint%20variable.%20One%20thousand%20images%20are%20recorded%20to%0Atrain%20the%20deep%20network%2C%20and%20transfer%20learning%20and%20fine-tuning%20techniques%20are%0Aapplied%20to%20the%20modified%20VGG-16%20to%20further%20improve%20the%20training.%20Finally%2C%0Atraining%20is%20also%20completed%20with%20a%20larger%20dataset%20of%20images%20that%20are%20affected%20by%0Avarious%20types%20of%20noises%2C%20changes%20in%20illumination%2C%20and%20partial%20occlusion.%20The%0Amain%20contribution%20of%20this%20research%20is%20the%20development%20of%20an%20image-to-joint%0Anetwork%20that%20can%20estimate%20the%20joint%20variable%20given%20an%20image%20of%20the%20arm%2C%20even%20if%0Athe%20image%20is%20not%20captured%20in%20an%20ideal%20condition.%20The%20key%20benefits%20of%20this%0Aresearch%20are%20twofold%3A%201%29%20image-to-joint%20mapping%20can%20offer%20a%20real-time%0Aalternative%20to%20computationally%20complex%20inverse%20kinematic%20mapping%20through%0Aanalytical%20models%3B%20and%202%29%20the%20proposed%20technique%20can%20provide%20robustness%20against%0Anoise%2C%20occlusion%2C%20and%20changes%20in%20illumination.%20The%20dataset%20is%20publicly%0Aavailable%20on%20Kaggle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-to-Joint%2520Inverse%2520Kinematic%2520of%2520a%2520Supportive%2520Continuum%2520Arm%2520Using%250A%2520%2520Deep%2520Learning%26entry.906535625%3DShayan%2520Sepahvand%2520and%2520Guanghui%2520Wang%2520and%2520Farrokh%2520Janabi-Sharifi%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520a%2520deep%2520learning-based%2520technique%2520is%2520used%2520to%2520study%2520the%250Aimage-to-joint%2520inverse%2520kinematics%2520of%2520a%2520tendon-driven%2520supportive%2520continuum%2520arm.%250AAn%2520eye-off-hand%2520configuration%2520is%2520considered%2520by%2520mounting%2520a%2520camera%2520at%2520a%2520fixed%250Apose%2520with%2520respect%2520to%2520the%2520inertial%2520frame%2520attached%2520at%2520the%2520arm%2520base.%2520This%2520camera%250Acaptures%2520an%2520image%2520for%2520each%2520distinct%2520joint%2520variable%2520at%2520each%2520sampling%2520time%2520to%250Aconstruct%2520the%2520training%2520dataset.%2520This%2520dataset%2520is%2520then%2520employed%2520to%2520adapt%2520a%250Afeed-forward%2520deep%2520convolutional%2520neural%2520network%252C%2520namely%2520the%2520modified%2520VGG-16%250Amodel%252C%2520to%2520estimate%2520the%2520joint%2520variable.%2520One%2520thousand%2520images%2520are%2520recorded%2520to%250Atrain%2520the%2520deep%2520network%252C%2520and%2520transfer%2520learning%2520and%2520fine-tuning%2520techniques%2520are%250Aapplied%2520to%2520the%2520modified%2520VGG-16%2520to%2520further%2520improve%2520the%2520training.%2520Finally%252C%250Atraining%2520is%2520also%2520completed%2520with%2520a%2520larger%2520dataset%2520of%2520images%2520that%2520are%2520affected%2520by%250Avarious%2520types%2520of%2520noises%252C%2520changes%2520in%2520illumination%252C%2520and%2520partial%2520occlusion.%2520The%250Amain%2520contribution%2520of%2520this%2520research%2520is%2520the%2520development%2520of%2520an%2520image-to-joint%250Anetwork%2520that%2520can%2520estimate%2520the%2520joint%2520variable%2520given%2520an%2520image%2520of%2520the%2520arm%252C%2520even%2520if%250Athe%2520image%2520is%2520not%2520captured%2520in%2520an%2520ideal%2520condition.%2520The%2520key%2520benefits%2520of%2520this%250Aresearch%2520are%2520twofold%253A%25201%2529%2520image-to-joint%2520mapping%2520can%2520offer%2520a%2520real-time%250Aalternative%2520to%2520computationally%2520complex%2520inverse%2520kinematic%2520mapping%2520through%250Aanalytical%2520models%253B%2520and%25202%2529%2520the%2520proposed%2520technique%2520can%2520provide%2520robustness%2520against%250Anoise%252C%2520occlusion%252C%2520and%2520changes%2520in%2520illumination.%2520The%2520dataset%2520is%2520publicly%250Aavailable%2520on%2520Kaggle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-to-Joint%20Inverse%20Kinematic%20of%20a%20Supportive%20Continuum%20Arm%20Using%0A%20%20Deep%20Learning&entry.906535625=Shayan%20Sepahvand%20and%20Guanghui%20Wang%20and%20Farrokh%20Janabi-Sharifi&entry.1292438233=%20%20In%20this%20work%2C%20a%20deep%20learning-based%20technique%20is%20used%20to%20study%20the%0Aimage-to-joint%20inverse%20kinematics%20of%20a%20tendon-driven%20supportive%20continuum%20arm.%0AAn%20eye-off-hand%20configuration%20is%20considered%20by%20mounting%20a%20camera%20at%20a%20fixed%0Apose%20with%20respect%20to%20the%20inertial%20frame%20attached%20at%20the%20arm%20base.%20This%20camera%0Acaptures%20an%20image%20for%20each%20distinct%20joint%20variable%20at%20each%20sampling%20time%20to%0Aconstruct%20the%20training%20dataset.%20This%20dataset%20is%20then%20employed%20to%20adapt%20a%0Afeed-forward%20deep%20convolutional%20neural%20network%2C%20namely%20the%20modified%20VGG-16%0Amodel%2C%20to%20estimate%20the%20joint%20variable.%20One%20thousand%20images%20are%20recorded%20to%0Atrain%20the%20deep%20network%2C%20and%20transfer%20learning%20and%20fine-tuning%20techniques%20are%0Aapplied%20to%20the%20modified%20VGG-16%20to%20further%20improve%20the%20training.%20Finally%2C%0Atraining%20is%20also%20completed%20with%20a%20larger%20dataset%20of%20images%20that%20are%20affected%20by%0Avarious%20types%20of%20noises%2C%20changes%20in%20illumination%2C%20and%20partial%20occlusion.%20The%0Amain%20contribution%20of%20this%20research%20is%20the%20development%20of%20an%20image-to-joint%0Anetwork%20that%20can%20estimate%20the%20joint%20variable%20given%20an%20image%20of%20the%20arm%2C%20even%20if%0Athe%20image%20is%20not%20captured%20in%20an%20ideal%20condition.%20The%20key%20benefits%20of%20this%0Aresearch%20are%20twofold%3A%201%29%20image-to-joint%20mapping%20can%20offer%20a%20real-time%0Aalternative%20to%20computationally%20complex%20inverse%20kinematic%20mapping%20through%0Aanalytical%20models%3B%20and%202%29%20the%20proposed%20technique%20can%20provide%20robustness%20against%0Anoise%2C%20occlusion%2C%20and%20changes%20in%20illumination.%20The%20dataset%20is%20publicly%0Aavailable%20on%20Kaggle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20248v1&entry.124074799=Read"},
{"title": "Towards RGB-NIR Cross-modality Image Registration and Beyond", "author": "Huadong Li and Shichao Dong and Jin Wang and Rong Fu and Minhao Jing and Jiajun Liang and Haoqiang Fan and Renhe Ji", "abstract": "  This paper focuses on the area of RGB(visible)-NIR(near-infrared)\ncross-modality image registration, which is crucial for many downstream vision\ntasks to fully leverage the complementary information present in visible and\ninfrared images. In this field, researchers face two primary challenges - the\nabsence of a correctly-annotated benchmark with viewpoint variations for\nevaluating RGB-NIR cross-modality registration methods and the problem of\ninconsistent local features caused by the appearance discrepancy between\nRGB-NIR cross-modality images. To address these challenges, we first present\nthe RGB-NIR Image Registration (RGB-NIR-IRegis) benchmark, which, for the first\ntime, enables fair and comprehensive evaluations for the task of RGB-NIR\ncross-modality image registration. Evaluations of previous methods highlight\nthe significant challenges posed by our RGB-NIR-IRegis benchmark, especially on\nRGB-NIR image pairs with viewpoint variations. To analyze the causes of the\nunsatisfying performance, we then design several metrics to reveal the toxic\nimpact of inconsistent local features between visible and infrared images on\nthe model performance. This further motivates us to develop a baseline method\nnamed Semantic Guidance Transformer (SGFormer), which utilizes high-level\nsemantic guidance to mitigate the negative impact of local inconsistent\nfeatures. Despite the simplicity of our motivation, extensive experimental\nresults show the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2405.19914v1", "date": "2024-05-30", "relevancy": 2.1532, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5441}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5342}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20RGB-NIR%20Cross-modality%20Image%20Registration%20and%20Beyond&body=Title%3A%20Towards%20RGB-NIR%20Cross-modality%20Image%20Registration%20and%20Beyond%0AAuthor%3A%20Huadong%20Li%20and%20Shichao%20Dong%20and%20Jin%20Wang%20and%20Rong%20Fu%20and%20Minhao%20Jing%20and%20Jiajun%20Liang%20and%20Haoqiang%20Fan%20and%20Renhe%20Ji%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20the%20area%20of%20RGB%28visible%29-NIR%28near-infrared%29%0Across-modality%20image%20registration%2C%20which%20is%20crucial%20for%20many%20downstream%20vision%0Atasks%20to%20fully%20leverage%20the%20complementary%20information%20present%20in%20visible%20and%0Ainfrared%20images.%20In%20this%20field%2C%20researchers%20face%20two%20primary%20challenges%20-%20the%0Aabsence%20of%20a%20correctly-annotated%20benchmark%20with%20viewpoint%20variations%20for%0Aevaluating%20RGB-NIR%20cross-modality%20registration%20methods%20and%20the%20problem%20of%0Ainconsistent%20local%20features%20caused%20by%20the%20appearance%20discrepancy%20between%0ARGB-NIR%20cross-modality%20images.%20To%20address%20these%20challenges%2C%20we%20first%20present%0Athe%20RGB-NIR%20Image%20Registration%20%28RGB-NIR-IRegis%29%20benchmark%2C%20which%2C%20for%20the%20first%0Atime%2C%20enables%20fair%20and%20comprehensive%20evaluations%20for%20the%20task%20of%20RGB-NIR%0Across-modality%20image%20registration.%20Evaluations%20of%20previous%20methods%20highlight%0Athe%20significant%20challenges%20posed%20by%20our%20RGB-NIR-IRegis%20benchmark%2C%20especially%20on%0ARGB-NIR%20image%20pairs%20with%20viewpoint%20variations.%20To%20analyze%20the%20causes%20of%20the%0Aunsatisfying%20performance%2C%20we%20then%20design%20several%20metrics%20to%20reveal%20the%20toxic%0Aimpact%20of%20inconsistent%20local%20features%20between%20visible%20and%20infrared%20images%20on%0Athe%20model%20performance.%20This%20further%20motivates%20us%20to%20develop%20a%20baseline%20method%0Anamed%20Semantic%20Guidance%20Transformer%20%28SGFormer%29%2C%20which%20utilizes%20high-level%0Asemantic%20guidance%20to%20mitigate%20the%20negative%20impact%20of%20local%20inconsistent%0Afeatures.%20Despite%20the%20simplicity%20of%20our%20motivation%2C%20extensive%20experimental%0Aresults%20show%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520RGB-NIR%2520Cross-modality%2520Image%2520Registration%2520and%2520Beyond%26entry.906535625%3DHuadong%2520Li%2520and%2520Shichao%2520Dong%2520and%2520Jin%2520Wang%2520and%2520Rong%2520Fu%2520and%2520Minhao%2520Jing%2520and%2520Jiajun%2520Liang%2520and%2520Haoqiang%2520Fan%2520and%2520Renhe%2520Ji%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520the%2520area%2520of%2520RGB%2528visible%2529-NIR%2528near-infrared%2529%250Across-modality%2520image%2520registration%252C%2520which%2520is%2520crucial%2520for%2520many%2520downstream%2520vision%250Atasks%2520to%2520fully%2520leverage%2520the%2520complementary%2520information%2520present%2520in%2520visible%2520and%250Ainfrared%2520images.%2520In%2520this%2520field%252C%2520researchers%2520face%2520two%2520primary%2520challenges%2520-%2520the%250Aabsence%2520of%2520a%2520correctly-annotated%2520benchmark%2520with%2520viewpoint%2520variations%2520for%250Aevaluating%2520RGB-NIR%2520cross-modality%2520registration%2520methods%2520and%2520the%2520problem%2520of%250Ainconsistent%2520local%2520features%2520caused%2520by%2520the%2520appearance%2520discrepancy%2520between%250ARGB-NIR%2520cross-modality%2520images.%2520To%2520address%2520these%2520challenges%252C%2520we%2520first%2520present%250Athe%2520RGB-NIR%2520Image%2520Registration%2520%2528RGB-NIR-IRegis%2529%2520benchmark%252C%2520which%252C%2520for%2520the%2520first%250Atime%252C%2520enables%2520fair%2520and%2520comprehensive%2520evaluations%2520for%2520the%2520task%2520of%2520RGB-NIR%250Across-modality%2520image%2520registration.%2520Evaluations%2520of%2520previous%2520methods%2520highlight%250Athe%2520significant%2520challenges%2520posed%2520by%2520our%2520RGB-NIR-IRegis%2520benchmark%252C%2520especially%2520on%250ARGB-NIR%2520image%2520pairs%2520with%2520viewpoint%2520variations.%2520To%2520analyze%2520the%2520causes%2520of%2520the%250Aunsatisfying%2520performance%252C%2520we%2520then%2520design%2520several%2520metrics%2520to%2520reveal%2520the%2520toxic%250Aimpact%2520of%2520inconsistent%2520local%2520features%2520between%2520visible%2520and%2520infrared%2520images%2520on%250Athe%2520model%2520performance.%2520This%2520further%2520motivates%2520us%2520to%2520develop%2520a%2520baseline%2520method%250Anamed%2520Semantic%2520Guidance%2520Transformer%2520%2528SGFormer%2529%252C%2520which%2520utilizes%2520high-level%250Asemantic%2520guidance%2520to%2520mitigate%2520the%2520negative%2520impact%2520of%2520local%2520inconsistent%250Afeatures.%2520Despite%2520the%2520simplicity%2520of%2520our%2520motivation%252C%2520extensive%2520experimental%250Aresults%2520show%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20RGB-NIR%20Cross-modality%20Image%20Registration%20and%20Beyond&entry.906535625=Huadong%20Li%20and%20Shichao%20Dong%20and%20Jin%20Wang%20and%20Rong%20Fu%20and%20Minhao%20Jing%20and%20Jiajun%20Liang%20and%20Haoqiang%20Fan%20and%20Renhe%20Ji&entry.1292438233=%20%20This%20paper%20focuses%20on%20the%20area%20of%20RGB%28visible%29-NIR%28near-infrared%29%0Across-modality%20image%20registration%2C%20which%20is%20crucial%20for%20many%20downstream%20vision%0Atasks%20to%20fully%20leverage%20the%20complementary%20information%20present%20in%20visible%20and%0Ainfrared%20images.%20In%20this%20field%2C%20researchers%20face%20two%20primary%20challenges%20-%20the%0Aabsence%20of%20a%20correctly-annotated%20benchmark%20with%20viewpoint%20variations%20for%0Aevaluating%20RGB-NIR%20cross-modality%20registration%20methods%20and%20the%20problem%20of%0Ainconsistent%20local%20features%20caused%20by%20the%20appearance%20discrepancy%20between%0ARGB-NIR%20cross-modality%20images.%20To%20address%20these%20challenges%2C%20we%20first%20present%0Athe%20RGB-NIR%20Image%20Registration%20%28RGB-NIR-IRegis%29%20benchmark%2C%20which%2C%20for%20the%20first%0Atime%2C%20enables%20fair%20and%20comprehensive%20evaluations%20for%20the%20task%20of%20RGB-NIR%0Across-modality%20image%20registration.%20Evaluations%20of%20previous%20methods%20highlight%0Athe%20significant%20challenges%20posed%20by%20our%20RGB-NIR-IRegis%20benchmark%2C%20especially%20on%0ARGB-NIR%20image%20pairs%20with%20viewpoint%20variations.%20To%20analyze%20the%20causes%20of%20the%0Aunsatisfying%20performance%2C%20we%20then%20design%20several%20metrics%20to%20reveal%20the%20toxic%0Aimpact%20of%20inconsistent%20local%20features%20between%20visible%20and%20infrared%20images%20on%0Athe%20model%20performance.%20This%20further%20motivates%20us%20to%20develop%20a%20baseline%20method%0Anamed%20Semantic%20Guidance%20Transformer%20%28SGFormer%29%2C%20which%20utilizes%20high-level%0Asemantic%20guidance%20to%20mitigate%20the%20negative%20impact%20of%20local%20inconsistent%0Afeatures.%20Despite%20the%20simplicity%20of%20our%20motivation%2C%20extensive%20experimental%0Aresults%20show%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19914v1&entry.124074799=Read"},
{"title": "Kernel Language Entropy: Fine-grained Uncertainty Quantification for\n  LLMs from Semantic Similarities", "author": "Alexander Nikitin and Jannik Kossen and Yarin Gal and Pekka Marttinen", "abstract": "  Uncertainty quantification in Large Language Models (LLMs) is crucial for\napplications where safety and reliability are important. In particular,\nuncertainty can be used to improve the trustworthiness of LLMs by detecting\nfactually incorrect model responses, commonly called hallucinations.\nCritically, one should seek to capture the model's semantic uncertainty, i.e.,\nthe uncertainty over the meanings of LLM outputs, rather than uncertainty over\nlexical or syntactic variations that do not affect answer correctness. To\naddress this problem, we propose Kernel Language Entropy (KLE), a novel method\nfor uncertainty estimation in white- and black-box LLMs. KLE defines positive\nsemidefinite unit trace kernels to encode the semantic similarities of LLM\noutputs and quantifies uncertainty using the von Neumann entropy. It considers\npairwise semantic dependencies between answers (or semantic clusters),\nproviding more fine-grained uncertainty estimates than previous methods based\non hard clustering of answers. We theoretically prove that KLE generalizes the\nprevious state-of-the-art method called semantic entropy and empirically\ndemonstrate that it improves uncertainty quantification performance across\nmultiple natural language generation datasets and LLM architectures.\n", "link": "http://arxiv.org/abs/2405.20003v1", "date": "2024-05-30", "relevancy": 2.1408, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5755}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5348}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20Language%20Entropy%3A%20Fine-grained%20Uncertainty%20Quantification%20for%0A%20%20LLMs%20from%20Semantic%20Similarities&body=Title%3A%20Kernel%20Language%20Entropy%3A%20Fine-grained%20Uncertainty%20Quantification%20for%0A%20%20LLMs%20from%20Semantic%20Similarities%0AAuthor%3A%20Alexander%20Nikitin%20and%20Jannik%20Kossen%20and%20Yarin%20Gal%20and%20Pekka%20Marttinen%0AAbstract%3A%20%20%20Uncertainty%20quantification%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20crucial%20for%0Aapplications%20where%20safety%20and%20reliability%20are%20important.%20In%20particular%2C%0Auncertainty%20can%20be%20used%20to%20improve%20the%20trustworthiness%20of%20LLMs%20by%20detecting%0Afactually%20incorrect%20model%20responses%2C%20commonly%20called%20hallucinations.%0ACritically%2C%20one%20should%20seek%20to%20capture%20the%20model%27s%20semantic%20uncertainty%2C%20i.e.%2C%0Athe%20uncertainty%20over%20the%20meanings%20of%20LLM%20outputs%2C%20rather%20than%20uncertainty%20over%0Alexical%20or%20syntactic%20variations%20that%20do%20not%20affect%20answer%20correctness.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20Kernel%20Language%20Entropy%20%28KLE%29%2C%20a%20novel%20method%0Afor%20uncertainty%20estimation%20in%20white-%20and%20black-box%20LLMs.%20KLE%20defines%20positive%0Asemidefinite%20unit%20trace%20kernels%20to%20encode%20the%20semantic%20similarities%20of%20LLM%0Aoutputs%20and%20quantifies%20uncertainty%20using%20the%20von%20Neumann%20entropy.%20It%20considers%0Apairwise%20semantic%20dependencies%20between%20answers%20%28or%20semantic%20clusters%29%2C%0Aproviding%20more%20fine-grained%20uncertainty%20estimates%20than%20previous%20methods%20based%0Aon%20hard%20clustering%20of%20answers.%20We%20theoretically%20prove%20that%20KLE%20generalizes%20the%0Aprevious%20state-of-the-art%20method%20called%20semantic%20entropy%20and%20empirically%0Ademonstrate%20that%20it%20improves%20uncertainty%20quantification%20performance%20across%0Amultiple%20natural%20language%20generation%20datasets%20and%20LLM%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520Language%2520Entropy%253A%2520Fine-grained%2520Uncertainty%2520Quantification%2520for%250A%2520%2520LLMs%2520from%2520Semantic%2520Similarities%26entry.906535625%3DAlexander%2520Nikitin%2520and%2520Jannik%2520Kossen%2520and%2520Yarin%2520Gal%2520and%2520Pekka%2520Marttinen%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520crucial%2520for%250Aapplications%2520where%2520safety%2520and%2520reliability%2520are%2520important.%2520In%2520particular%252C%250Auncertainty%2520can%2520be%2520used%2520to%2520improve%2520the%2520trustworthiness%2520of%2520LLMs%2520by%2520detecting%250Afactually%2520incorrect%2520model%2520responses%252C%2520commonly%2520called%2520hallucinations.%250ACritically%252C%2520one%2520should%2520seek%2520to%2520capture%2520the%2520model%2527s%2520semantic%2520uncertainty%252C%2520i.e.%252C%250Athe%2520uncertainty%2520over%2520the%2520meanings%2520of%2520LLM%2520outputs%252C%2520rather%2520than%2520uncertainty%2520over%250Alexical%2520or%2520syntactic%2520variations%2520that%2520do%2520not%2520affect%2520answer%2520correctness.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520Kernel%2520Language%2520Entropy%2520%2528KLE%2529%252C%2520a%2520novel%2520method%250Afor%2520uncertainty%2520estimation%2520in%2520white-%2520and%2520black-box%2520LLMs.%2520KLE%2520defines%2520positive%250Asemidefinite%2520unit%2520trace%2520kernels%2520to%2520encode%2520the%2520semantic%2520similarities%2520of%2520LLM%250Aoutputs%2520and%2520quantifies%2520uncertainty%2520using%2520the%2520von%2520Neumann%2520entropy.%2520It%2520considers%250Apairwise%2520semantic%2520dependencies%2520between%2520answers%2520%2528or%2520semantic%2520clusters%2529%252C%250Aproviding%2520more%2520fine-grained%2520uncertainty%2520estimates%2520than%2520previous%2520methods%2520based%250Aon%2520hard%2520clustering%2520of%2520answers.%2520We%2520theoretically%2520prove%2520that%2520KLE%2520generalizes%2520the%250Aprevious%2520state-of-the-art%2520method%2520called%2520semantic%2520entropy%2520and%2520empirically%250Ademonstrate%2520that%2520it%2520improves%2520uncertainty%2520quantification%2520performance%2520across%250Amultiple%2520natural%2520language%2520generation%2520datasets%2520and%2520LLM%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20Language%20Entropy%3A%20Fine-grained%20Uncertainty%20Quantification%20for%0A%20%20LLMs%20from%20Semantic%20Similarities&entry.906535625=Alexander%20Nikitin%20and%20Jannik%20Kossen%20and%20Yarin%20Gal%20and%20Pekka%20Marttinen&entry.1292438233=%20%20Uncertainty%20quantification%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20crucial%20for%0Aapplications%20where%20safety%20and%20reliability%20are%20important.%20In%20particular%2C%0Auncertainty%20can%20be%20used%20to%20improve%20the%20trustworthiness%20of%20LLMs%20by%20detecting%0Afactually%20incorrect%20model%20responses%2C%20commonly%20called%20hallucinations.%0ACritically%2C%20one%20should%20seek%20to%20capture%20the%20model%27s%20semantic%20uncertainty%2C%20i.e.%2C%0Athe%20uncertainty%20over%20the%20meanings%20of%20LLM%20outputs%2C%20rather%20than%20uncertainty%20over%0Alexical%20or%20syntactic%20variations%20that%20do%20not%20affect%20answer%20correctness.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20Kernel%20Language%20Entropy%20%28KLE%29%2C%20a%20novel%20method%0Afor%20uncertainty%20estimation%20in%20white-%20and%20black-box%20LLMs.%20KLE%20defines%20positive%0Asemidefinite%20unit%20trace%20kernels%20to%20encode%20the%20semantic%20similarities%20of%20LLM%0Aoutputs%20and%20quantifies%20uncertainty%20using%20the%20von%20Neumann%20entropy.%20It%20considers%0Apairwise%20semantic%20dependencies%20between%20answers%20%28or%20semantic%20clusters%29%2C%0Aproviding%20more%20fine-grained%20uncertainty%20estimates%20than%20previous%20methods%20based%0Aon%20hard%20clustering%20of%20answers.%20We%20theoretically%20prove%20that%20KLE%20generalizes%20the%0Aprevious%20state-of-the-art%20method%20called%20semantic%20entropy%20and%20empirically%0Ademonstrate%20that%20it%20improves%20uncertainty%20quantification%20performance%20across%0Amultiple%20natural%20language%20generation%20datasets%20and%20LLM%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20003v1&entry.124074799=Read"},
{"title": "Sharing Key Semantics in Transformer Makes Efficient Image Restoration", "author": "Bin Ren and Yawei Li and Jingyun Liang and Rakesh Ranjan and Mengyuan Liu and Rita Cucchiara and Luc Van Gool and Ming-Hsuan Yang and Nicu Sebe", "abstract": "  Image Restoration (IR), a classic low-level vision task, has witnessed\nsignificant advancements through deep models that effectively model global\ninformation. Notably, the Vision Transformers (ViTs) emergence has further\npropelled these advancements. When computing, the self-attention mechanism, a\ncornerstone of ViTs, tends to encompass all global cues, even those from\nsemantically unrelated objects or regions. This inclusivity introduces\ncomputational inefficiencies, particularly noticeable with high input\nresolution, as it requires processing irrelevant information, thereby impeding\nefficiency. Additionally, for IR, it is commonly noted that small segments of a\ndegraded image, particularly those closely aligned semantically, provide\nparticularly relevant information to aid in the restoration process, as they\ncontribute essential contextual cues crucial for accurate reconstruction. To\naddress these challenges, we propose boosting IR's performance by sharing the\nkey semantics via Transformer for IR (i.e., SemanIR) in this paper.\nSpecifically, SemanIR initially constructs a sparse yet comprehensive\nkey-semantic dictionary within each transformer stage by establishing essential\nsemantic connections for every degraded patch. Subsequently, this dictionary is\nshared across all subsequent transformer blocks within the same stage. This\nstrategy optimizes attention calculation within each block by focusing\nexclusively on semantically related components stored in the key-semantic\ndictionary. As a result, attention calculation achieves linear computational\ncomplexity within each window. Extensive experiments across 6 IR tasks confirm\nthe proposed SemanIR's state-of-the-art performance, quantitatively and\nqualitatively showcasing advancements.\n", "link": "http://arxiv.org/abs/2405.20008v1", "date": "2024-05-30", "relevancy": 2.1373, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5804}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5328}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharing%20Key%20Semantics%20in%20Transformer%20Makes%20Efficient%20Image%20Restoration&body=Title%3A%20Sharing%20Key%20Semantics%20in%20Transformer%20Makes%20Efficient%20Image%20Restoration%0AAuthor%3A%20Bin%20Ren%20and%20Yawei%20Li%20and%20Jingyun%20Liang%20and%20Rakesh%20Ranjan%20and%20Mengyuan%20Liu%20and%20Rita%20Cucchiara%20and%20Luc%20Van%20Gool%20and%20Ming-Hsuan%20Yang%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Image%20Restoration%20%28IR%29%2C%20a%20classic%20low-level%20vision%20task%2C%20has%20witnessed%0Asignificant%20advancements%20through%20deep%20models%20that%20effectively%20model%20global%0Ainformation.%20Notably%2C%20the%20Vision%20Transformers%20%28ViTs%29%20emergence%20has%20further%0Apropelled%20these%20advancements.%20When%20computing%2C%20the%20self-attention%20mechanism%2C%20a%0Acornerstone%20of%20ViTs%2C%20tends%20to%20encompass%20all%20global%20cues%2C%20even%20those%20from%0Asemantically%20unrelated%20objects%20or%20regions.%20This%20inclusivity%20introduces%0Acomputational%20inefficiencies%2C%20particularly%20noticeable%20with%20high%20input%0Aresolution%2C%20as%20it%20requires%20processing%20irrelevant%20information%2C%20thereby%20impeding%0Aefficiency.%20Additionally%2C%20for%20IR%2C%20it%20is%20commonly%20noted%20that%20small%20segments%20of%20a%0Adegraded%20image%2C%20particularly%20those%20closely%20aligned%20semantically%2C%20provide%0Aparticularly%20relevant%20information%20to%20aid%20in%20the%20restoration%20process%2C%20as%20they%0Acontribute%20essential%20contextual%20cues%20crucial%20for%20accurate%20reconstruction.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20boosting%20IR%27s%20performance%20by%20sharing%20the%0Akey%20semantics%20via%20Transformer%20for%20IR%20%28i.e.%2C%20SemanIR%29%20in%20this%20paper.%0ASpecifically%2C%20SemanIR%20initially%20constructs%20a%20sparse%20yet%20comprehensive%0Akey-semantic%20dictionary%20within%20each%20transformer%20stage%20by%20establishing%20essential%0Asemantic%20connections%20for%20every%20degraded%20patch.%20Subsequently%2C%20this%20dictionary%20is%0Ashared%20across%20all%20subsequent%20transformer%20blocks%20within%20the%20same%20stage.%20This%0Astrategy%20optimizes%20attention%20calculation%20within%20each%20block%20by%20focusing%0Aexclusively%20on%20semantically%20related%20components%20stored%20in%20the%20key-semantic%0Adictionary.%20As%20a%20result%2C%20attention%20calculation%20achieves%20linear%20computational%0Acomplexity%20within%20each%20window.%20Extensive%20experiments%20across%206%20IR%20tasks%20confirm%0Athe%20proposed%20SemanIR%27s%20state-of-the-art%20performance%2C%20quantitatively%20and%0Aqualitatively%20showcasing%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharing%2520Key%2520Semantics%2520in%2520Transformer%2520Makes%2520Efficient%2520Image%2520Restoration%26entry.906535625%3DBin%2520Ren%2520and%2520Yawei%2520Li%2520and%2520Jingyun%2520Liang%2520and%2520Rakesh%2520Ranjan%2520and%2520Mengyuan%2520Liu%2520and%2520Rita%2520Cucchiara%2520and%2520Luc%2520Van%2520Gool%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Image%2520Restoration%2520%2528IR%2529%252C%2520a%2520classic%2520low-level%2520vision%2520task%252C%2520has%2520witnessed%250Asignificant%2520advancements%2520through%2520deep%2520models%2520that%2520effectively%2520model%2520global%250Ainformation.%2520Notably%252C%2520the%2520Vision%2520Transformers%2520%2528ViTs%2529%2520emergence%2520has%2520further%250Apropelled%2520these%2520advancements.%2520When%2520computing%252C%2520the%2520self-attention%2520mechanism%252C%2520a%250Acornerstone%2520of%2520ViTs%252C%2520tends%2520to%2520encompass%2520all%2520global%2520cues%252C%2520even%2520those%2520from%250Asemantically%2520unrelated%2520objects%2520or%2520regions.%2520This%2520inclusivity%2520introduces%250Acomputational%2520inefficiencies%252C%2520particularly%2520noticeable%2520with%2520high%2520input%250Aresolution%252C%2520as%2520it%2520requires%2520processing%2520irrelevant%2520information%252C%2520thereby%2520impeding%250Aefficiency.%2520Additionally%252C%2520for%2520IR%252C%2520it%2520is%2520commonly%2520noted%2520that%2520small%2520segments%2520of%2520a%250Adegraded%2520image%252C%2520particularly%2520those%2520closely%2520aligned%2520semantically%252C%2520provide%250Aparticularly%2520relevant%2520information%2520to%2520aid%2520in%2520the%2520restoration%2520process%252C%2520as%2520they%250Acontribute%2520essential%2520contextual%2520cues%2520crucial%2520for%2520accurate%2520reconstruction.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520boosting%2520IR%2527s%2520performance%2520by%2520sharing%2520the%250Akey%2520semantics%2520via%2520Transformer%2520for%2520IR%2520%2528i.e.%252C%2520SemanIR%2529%2520in%2520this%2520paper.%250ASpecifically%252C%2520SemanIR%2520initially%2520constructs%2520a%2520sparse%2520yet%2520comprehensive%250Akey-semantic%2520dictionary%2520within%2520each%2520transformer%2520stage%2520by%2520establishing%2520essential%250Asemantic%2520connections%2520for%2520every%2520degraded%2520patch.%2520Subsequently%252C%2520this%2520dictionary%2520is%250Ashared%2520across%2520all%2520subsequent%2520transformer%2520blocks%2520within%2520the%2520same%2520stage.%2520This%250Astrategy%2520optimizes%2520attention%2520calculation%2520within%2520each%2520block%2520by%2520focusing%250Aexclusively%2520on%2520semantically%2520related%2520components%2520stored%2520in%2520the%2520key-semantic%250Adictionary.%2520As%2520a%2520result%252C%2520attention%2520calculation%2520achieves%2520linear%2520computational%250Acomplexity%2520within%2520each%2520window.%2520Extensive%2520experiments%2520across%25206%2520IR%2520tasks%2520confirm%250Athe%2520proposed%2520SemanIR%2527s%2520state-of-the-art%2520performance%252C%2520quantitatively%2520and%250Aqualitatively%2520showcasing%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharing%20Key%20Semantics%20in%20Transformer%20Makes%20Efficient%20Image%20Restoration&entry.906535625=Bin%20Ren%20and%20Yawei%20Li%20and%20Jingyun%20Liang%20and%20Rakesh%20Ranjan%20and%20Mengyuan%20Liu%20and%20Rita%20Cucchiara%20and%20Luc%20Van%20Gool%20and%20Ming-Hsuan%20Yang%20and%20Nicu%20Sebe&entry.1292438233=%20%20Image%20Restoration%20%28IR%29%2C%20a%20classic%20low-level%20vision%20task%2C%20has%20witnessed%0Asignificant%20advancements%20through%20deep%20models%20that%20effectively%20model%20global%0Ainformation.%20Notably%2C%20the%20Vision%20Transformers%20%28ViTs%29%20emergence%20has%20further%0Apropelled%20these%20advancements.%20When%20computing%2C%20the%20self-attention%20mechanism%2C%20a%0Acornerstone%20of%20ViTs%2C%20tends%20to%20encompass%20all%20global%20cues%2C%20even%20those%20from%0Asemantically%20unrelated%20objects%20or%20regions.%20This%20inclusivity%20introduces%0Acomputational%20inefficiencies%2C%20particularly%20noticeable%20with%20high%20input%0Aresolution%2C%20as%20it%20requires%20processing%20irrelevant%20information%2C%20thereby%20impeding%0Aefficiency.%20Additionally%2C%20for%20IR%2C%20it%20is%20commonly%20noted%20that%20small%20segments%20of%20a%0Adegraded%20image%2C%20particularly%20those%20closely%20aligned%20semantically%2C%20provide%0Aparticularly%20relevant%20information%20to%20aid%20in%20the%20restoration%20process%2C%20as%20they%0Acontribute%20essential%20contextual%20cues%20crucial%20for%20accurate%20reconstruction.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20boosting%20IR%27s%20performance%20by%20sharing%20the%0Akey%20semantics%20via%20Transformer%20for%20IR%20%28i.e.%2C%20SemanIR%29%20in%20this%20paper.%0ASpecifically%2C%20SemanIR%20initially%20constructs%20a%20sparse%20yet%20comprehensive%0Akey-semantic%20dictionary%20within%20each%20transformer%20stage%20by%20establishing%20essential%0Asemantic%20connections%20for%20every%20degraded%20patch.%20Subsequently%2C%20this%20dictionary%20is%0Ashared%20across%20all%20subsequent%20transformer%20blocks%20within%20the%20same%20stage.%20This%0Astrategy%20optimizes%20attention%20calculation%20within%20each%20block%20by%20focusing%0Aexclusively%20on%20semantically%20related%20components%20stored%20in%20the%20key-semantic%0Adictionary.%20As%20a%20result%2C%20attention%20calculation%20achieves%20linear%20computational%0Acomplexity%20within%20each%20window.%20Extensive%20experiments%20across%206%20IR%20tasks%20confirm%0Athe%20proposed%20SemanIR%27s%20state-of-the-art%20performance%2C%20quantitatively%20and%0Aqualitatively%20showcasing%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20008v1&entry.124074799=Read"},
{"title": "CaLa: Complementary Association Learning for Augmenting Composed Image\n  Retrieval", "author": "Xintong Jiang and Yaxiong Wang and Mengjian Li and Yujiao Wu and Bingwen Hu and Xueming Qian", "abstract": "  Composed Image Retrieval (CIR) involves searching for target images based on\nan image-text pair query. While current methods treat this as a query-target\nmatching problem, we argue that CIR triplets contain additional associations\nbeyond this primary relation. In our paper, we identify two new relations\nwithin triplets, treating each triplet as a graph node. Firstly, we introduce\nthe concept of text-bridged image alignment, where the query text serves as a\nbridge between the query image and the target image. We propose a hinge-based\ncross-attention mechanism to incorporate this relation into network learning.\nSecondly, we explore complementary text reasoning, considering CIR as a form of\ncross-modal retrieval where two images compose to reason about complementary\ntext. To integrate these perspectives effectively, we design a twin\nattention-based compositor. By combining these complementary associations with\nthe explicit query pair-target image relation, we establish a comprehensive set\nof constraints for CIR. Our framework, CaLa (Complementary Association Learning\nfor Augmenting Composed Image Retrieval), leverages these insights. We evaluate\nCaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating\nits superiority in composed image retrieval.\n", "link": "http://arxiv.org/abs/2405.19149v2", "date": "2024-05-30", "relevancy": 2.1263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5443}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5309}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaLa%3A%20Complementary%20Association%20Learning%20for%20Augmenting%20Composed%20Image%0A%20%20Retrieval&body=Title%3A%20CaLa%3A%20Complementary%20Association%20Learning%20for%20Augmenting%20Composed%20Image%0A%20%20Retrieval%0AAuthor%3A%20Xintong%20Jiang%20and%20Yaxiong%20Wang%20and%20Mengjian%20Li%20and%20Yujiao%20Wu%20and%20Bingwen%20Hu%20and%20Xueming%20Qian%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CIR%29%20involves%20searching%20for%20target%20images%20based%20on%0Aan%20image-text%20pair%20query.%20While%20current%20methods%20treat%20this%20as%20a%20query-target%0Amatching%20problem%2C%20we%20argue%20that%20CIR%20triplets%20contain%20additional%20associations%0Abeyond%20this%20primary%20relation.%20In%20our%20paper%2C%20we%20identify%20two%20new%20relations%0Awithin%20triplets%2C%20treating%20each%20triplet%20as%20a%20graph%20node.%20Firstly%2C%20we%20introduce%0Athe%20concept%20of%20text-bridged%20image%20alignment%2C%20where%20the%20query%20text%20serves%20as%20a%0Abridge%20between%20the%20query%20image%20and%20the%20target%20image.%20We%20propose%20a%20hinge-based%0Across-attention%20mechanism%20to%20incorporate%20this%20relation%20into%20network%20learning.%0ASecondly%2C%20we%20explore%20complementary%20text%20reasoning%2C%20considering%20CIR%20as%20a%20form%20of%0Across-modal%20retrieval%20where%20two%20images%20compose%20to%20reason%20about%20complementary%0Atext.%20To%20integrate%20these%20perspectives%20effectively%2C%20we%20design%20a%20twin%0Aattention-based%20compositor.%20By%20combining%20these%20complementary%20associations%20with%0Athe%20explicit%20query%20pair-target%20image%20relation%2C%20we%20establish%20a%20comprehensive%20set%0Aof%20constraints%20for%20CIR.%20Our%20framework%2C%20CaLa%20%28Complementary%20Association%20Learning%0Afor%20Augmenting%20Composed%20Image%20Retrieval%29%2C%20leverages%20these%20insights.%20We%20evaluate%0ACaLa%20on%20CIRR%20and%20FashionIQ%20benchmarks%20with%20multiple%20backbones%2C%20demonstrating%0Aits%20superiority%20in%20composed%20image%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19149v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaLa%253A%2520Complementary%2520Association%2520Learning%2520for%2520Augmenting%2520Composed%2520Image%250A%2520%2520Retrieval%26entry.906535625%3DXintong%2520Jiang%2520and%2520Yaxiong%2520Wang%2520and%2520Mengjian%2520Li%2520and%2520Yujiao%2520Wu%2520and%2520Bingwen%2520Hu%2520and%2520Xueming%2520Qian%26entry.1292438233%3D%2520%2520Composed%2520Image%2520Retrieval%2520%2528CIR%2529%2520involves%2520searching%2520for%2520target%2520images%2520based%2520on%250Aan%2520image-text%2520pair%2520query.%2520While%2520current%2520methods%2520treat%2520this%2520as%2520a%2520query-target%250Amatching%2520problem%252C%2520we%2520argue%2520that%2520CIR%2520triplets%2520contain%2520additional%2520associations%250Abeyond%2520this%2520primary%2520relation.%2520In%2520our%2520paper%252C%2520we%2520identify%2520two%2520new%2520relations%250Awithin%2520triplets%252C%2520treating%2520each%2520triplet%2520as%2520a%2520graph%2520node.%2520Firstly%252C%2520we%2520introduce%250Athe%2520concept%2520of%2520text-bridged%2520image%2520alignment%252C%2520where%2520the%2520query%2520text%2520serves%2520as%2520a%250Abridge%2520between%2520the%2520query%2520image%2520and%2520the%2520target%2520image.%2520We%2520propose%2520a%2520hinge-based%250Across-attention%2520mechanism%2520to%2520incorporate%2520this%2520relation%2520into%2520network%2520learning.%250ASecondly%252C%2520we%2520explore%2520complementary%2520text%2520reasoning%252C%2520considering%2520CIR%2520as%2520a%2520form%2520of%250Across-modal%2520retrieval%2520where%2520two%2520images%2520compose%2520to%2520reason%2520about%2520complementary%250Atext.%2520To%2520integrate%2520these%2520perspectives%2520effectively%252C%2520we%2520design%2520a%2520twin%250Aattention-based%2520compositor.%2520By%2520combining%2520these%2520complementary%2520associations%2520with%250Athe%2520explicit%2520query%2520pair-target%2520image%2520relation%252C%2520we%2520establish%2520a%2520comprehensive%2520set%250Aof%2520constraints%2520for%2520CIR.%2520Our%2520framework%252C%2520CaLa%2520%2528Complementary%2520Association%2520Learning%250Afor%2520Augmenting%2520Composed%2520Image%2520Retrieval%2529%252C%2520leverages%2520these%2520insights.%2520We%2520evaluate%250ACaLa%2520on%2520CIRR%2520and%2520FashionIQ%2520benchmarks%2520with%2520multiple%2520backbones%252C%2520demonstrating%250Aits%2520superiority%2520in%2520composed%2520image%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19149v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaLa%3A%20Complementary%20Association%20Learning%20for%20Augmenting%20Composed%20Image%0A%20%20Retrieval&entry.906535625=Xintong%20Jiang%20and%20Yaxiong%20Wang%20and%20Mengjian%20Li%20and%20Yujiao%20Wu%20and%20Bingwen%20Hu%20and%20Xueming%20Qian&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CIR%29%20involves%20searching%20for%20target%20images%20based%20on%0Aan%20image-text%20pair%20query.%20While%20current%20methods%20treat%20this%20as%20a%20query-target%0Amatching%20problem%2C%20we%20argue%20that%20CIR%20triplets%20contain%20additional%20associations%0Abeyond%20this%20primary%20relation.%20In%20our%20paper%2C%20we%20identify%20two%20new%20relations%0Awithin%20triplets%2C%20treating%20each%20triplet%20as%20a%20graph%20node.%20Firstly%2C%20we%20introduce%0Athe%20concept%20of%20text-bridged%20image%20alignment%2C%20where%20the%20query%20text%20serves%20as%20a%0Abridge%20between%20the%20query%20image%20and%20the%20target%20image.%20We%20propose%20a%20hinge-based%0Across-attention%20mechanism%20to%20incorporate%20this%20relation%20into%20network%20learning.%0ASecondly%2C%20we%20explore%20complementary%20text%20reasoning%2C%20considering%20CIR%20as%20a%20form%20of%0Across-modal%20retrieval%20where%20two%20images%20compose%20to%20reason%20about%20complementary%0Atext.%20To%20integrate%20these%20perspectives%20effectively%2C%20we%20design%20a%20twin%0Aattention-based%20compositor.%20By%20combining%20these%20complementary%20associations%20with%0Athe%20explicit%20query%20pair-target%20image%20relation%2C%20we%20establish%20a%20comprehensive%20set%0Aof%20constraints%20for%20CIR.%20Our%20framework%2C%20CaLa%20%28Complementary%20Association%20Learning%0Afor%20Augmenting%20Composed%20Image%20Retrieval%29%2C%20leverages%20these%20insights.%20We%20evaluate%0ACaLa%20on%20CIRR%20and%20FashionIQ%20benchmarks%20with%20multiple%20backbones%2C%20demonstrating%0Aits%20superiority%20in%20composed%20image%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19149v2&entry.124074799=Read"},
{"title": "DiffPhysBA: Diffusion-based Physical Backdoor Attack against Person\n  Re-Identification in Real-World", "author": "Wenli Sun and Xinyang Jiang and Dongsheng Li and Cairong Zhao", "abstract": "  Person Re-Identification (ReID) systems pose a significant security risk from\nbackdoor attacks, allowing adversaries to evade tracking or impersonate others.\nBeyond recognizing this issue, we investigate how backdoor attacks can be\ndeployed in real-world scenarios, where a ReID model is typically trained on\ndata collected in the digital domain and then deployed in a physical\nenvironment. This attack scenario requires an attack flow that embeds backdoor\ntriggers in the digital domain realistically enough to also activate the buried\nbackdoor in person ReID models in the physical domain. This paper realizes this\nattack flow by leveraging a diffusion model to generate realistic accessories\non pedestrian images (e.g., bags, hats, etc.) as backdoor triggers. However,\nthe noticeable domain gap between the triggers generated by the off-the-shelf\ndiffusion model and their physical counterparts results in a low attack success\nrate. Therefore, we introduce a novel diffusion-based physical backdoor attack\n(DiffPhysBA) method that adopts a training-free similarity-guided sampling\nprocess to enhance the resemblance between generated and physical triggers.\nConsequently, DiffPhysBA can generate realistic attributes as semantic-level\ntriggers in the digital domain and provides higher physical ASR compared to the\ndirect paste method by 25.6% on the real-world test set. Through evaluations on\nnewly proposed real-world and synthetic ReID test sets, DiffPhysBA demonstrates\nan impressive success rate exceeding 90% in both the digital and physical\ndomains. Notably, it excels in digital stealth metrics and can effectively\nevade state-of-the-art defense methods.\n", "link": "http://arxiv.org/abs/2405.19990v1", "date": "2024-05-30", "relevancy": 2.1234, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5331}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5297}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffPhysBA%3A%20Diffusion-based%20Physical%20Backdoor%20Attack%20against%20Person%0A%20%20Re-Identification%20in%20Real-World&body=Title%3A%20DiffPhysBA%3A%20Diffusion-based%20Physical%20Backdoor%20Attack%20against%20Person%0A%20%20Re-Identification%20in%20Real-World%0AAuthor%3A%20Wenli%20Sun%20and%20Xinyang%20Jiang%20and%20Dongsheng%20Li%20and%20Cairong%20Zhao%0AAbstract%3A%20%20%20Person%20Re-Identification%20%28ReID%29%20systems%20pose%20a%20significant%20security%20risk%20from%0Abackdoor%20attacks%2C%20allowing%20adversaries%20to%20evade%20tracking%20or%20impersonate%20others.%0ABeyond%20recognizing%20this%20issue%2C%20we%20investigate%20how%20backdoor%20attacks%20can%20be%0Adeployed%20in%20real-world%20scenarios%2C%20where%20a%20ReID%20model%20is%20typically%20trained%20on%0Adata%20collected%20in%20the%20digital%20domain%20and%20then%20deployed%20in%20a%20physical%0Aenvironment.%20This%20attack%20scenario%20requires%20an%20attack%20flow%20that%20embeds%20backdoor%0Atriggers%20in%20the%20digital%20domain%20realistically%20enough%20to%20also%20activate%20the%20buried%0Abackdoor%20in%20person%20ReID%20models%20in%20the%20physical%20domain.%20This%20paper%20realizes%20this%0Aattack%20flow%20by%20leveraging%20a%20diffusion%20model%20to%20generate%20realistic%20accessories%0Aon%20pedestrian%20images%20%28e.g.%2C%20bags%2C%20hats%2C%20etc.%29%20as%20backdoor%20triggers.%20However%2C%0Athe%20noticeable%20domain%20gap%20between%20the%20triggers%20generated%20by%20the%20off-the-shelf%0Adiffusion%20model%20and%20their%20physical%20counterparts%20results%20in%20a%20low%20attack%20success%0Arate.%20Therefore%2C%20we%20introduce%20a%20novel%20diffusion-based%20physical%20backdoor%20attack%0A%28DiffPhysBA%29%20method%20that%20adopts%20a%20training-free%20similarity-guided%20sampling%0Aprocess%20to%20enhance%20the%20resemblance%20between%20generated%20and%20physical%20triggers.%0AConsequently%2C%20DiffPhysBA%20can%20generate%20realistic%20attributes%20as%20semantic-level%0Atriggers%20in%20the%20digital%20domain%20and%20provides%20higher%20physical%20ASR%20compared%20to%20the%0Adirect%20paste%20method%20by%2025.6%25%20on%20the%20real-world%20test%20set.%20Through%20evaluations%20on%0Anewly%20proposed%20real-world%20and%20synthetic%20ReID%20test%20sets%2C%20DiffPhysBA%20demonstrates%0Aan%20impressive%20success%20rate%20exceeding%2090%25%20in%20both%20the%20digital%20and%20physical%0Adomains.%20Notably%2C%20it%20excels%20in%20digital%20stealth%20metrics%20and%20can%20effectively%0Aevade%20state-of-the-art%20defense%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffPhysBA%253A%2520Diffusion-based%2520Physical%2520Backdoor%2520Attack%2520against%2520Person%250A%2520%2520Re-Identification%2520in%2520Real-World%26entry.906535625%3DWenli%2520Sun%2520and%2520Xinyang%2520Jiang%2520and%2520Dongsheng%2520Li%2520and%2520Cairong%2520Zhao%26entry.1292438233%3D%2520%2520Person%2520Re-Identification%2520%2528ReID%2529%2520systems%2520pose%2520a%2520significant%2520security%2520risk%2520from%250Abackdoor%2520attacks%252C%2520allowing%2520adversaries%2520to%2520evade%2520tracking%2520or%2520impersonate%2520others.%250ABeyond%2520recognizing%2520this%2520issue%252C%2520we%2520investigate%2520how%2520backdoor%2520attacks%2520can%2520be%250Adeployed%2520in%2520real-world%2520scenarios%252C%2520where%2520a%2520ReID%2520model%2520is%2520typically%2520trained%2520on%250Adata%2520collected%2520in%2520the%2520digital%2520domain%2520and%2520then%2520deployed%2520in%2520a%2520physical%250Aenvironment.%2520This%2520attack%2520scenario%2520requires%2520an%2520attack%2520flow%2520that%2520embeds%2520backdoor%250Atriggers%2520in%2520the%2520digital%2520domain%2520realistically%2520enough%2520to%2520also%2520activate%2520the%2520buried%250Abackdoor%2520in%2520person%2520ReID%2520models%2520in%2520the%2520physical%2520domain.%2520This%2520paper%2520realizes%2520this%250Aattack%2520flow%2520by%2520leveraging%2520a%2520diffusion%2520model%2520to%2520generate%2520realistic%2520accessories%250Aon%2520pedestrian%2520images%2520%2528e.g.%252C%2520bags%252C%2520hats%252C%2520etc.%2529%2520as%2520backdoor%2520triggers.%2520However%252C%250Athe%2520noticeable%2520domain%2520gap%2520between%2520the%2520triggers%2520generated%2520by%2520the%2520off-the-shelf%250Adiffusion%2520model%2520and%2520their%2520physical%2520counterparts%2520results%2520in%2520a%2520low%2520attack%2520success%250Arate.%2520Therefore%252C%2520we%2520introduce%2520a%2520novel%2520diffusion-based%2520physical%2520backdoor%2520attack%250A%2528DiffPhysBA%2529%2520method%2520that%2520adopts%2520a%2520training-free%2520similarity-guided%2520sampling%250Aprocess%2520to%2520enhance%2520the%2520resemblance%2520between%2520generated%2520and%2520physical%2520triggers.%250AConsequently%252C%2520DiffPhysBA%2520can%2520generate%2520realistic%2520attributes%2520as%2520semantic-level%250Atriggers%2520in%2520the%2520digital%2520domain%2520and%2520provides%2520higher%2520physical%2520ASR%2520compared%2520to%2520the%250Adirect%2520paste%2520method%2520by%252025.6%2525%2520on%2520the%2520real-world%2520test%2520set.%2520Through%2520evaluations%2520on%250Anewly%2520proposed%2520real-world%2520and%2520synthetic%2520ReID%2520test%2520sets%252C%2520DiffPhysBA%2520demonstrates%250Aan%2520impressive%2520success%2520rate%2520exceeding%252090%2525%2520in%2520both%2520the%2520digital%2520and%2520physical%250Adomains.%2520Notably%252C%2520it%2520excels%2520in%2520digital%2520stealth%2520metrics%2520and%2520can%2520effectively%250Aevade%2520state-of-the-art%2520defense%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffPhysBA%3A%20Diffusion-based%20Physical%20Backdoor%20Attack%20against%20Person%0A%20%20Re-Identification%20in%20Real-World&entry.906535625=Wenli%20Sun%20and%20Xinyang%20Jiang%20and%20Dongsheng%20Li%20and%20Cairong%20Zhao&entry.1292438233=%20%20Person%20Re-Identification%20%28ReID%29%20systems%20pose%20a%20significant%20security%20risk%20from%0Abackdoor%20attacks%2C%20allowing%20adversaries%20to%20evade%20tracking%20or%20impersonate%20others.%0ABeyond%20recognizing%20this%20issue%2C%20we%20investigate%20how%20backdoor%20attacks%20can%20be%0Adeployed%20in%20real-world%20scenarios%2C%20where%20a%20ReID%20model%20is%20typically%20trained%20on%0Adata%20collected%20in%20the%20digital%20domain%20and%20then%20deployed%20in%20a%20physical%0Aenvironment.%20This%20attack%20scenario%20requires%20an%20attack%20flow%20that%20embeds%20backdoor%0Atriggers%20in%20the%20digital%20domain%20realistically%20enough%20to%20also%20activate%20the%20buried%0Abackdoor%20in%20person%20ReID%20models%20in%20the%20physical%20domain.%20This%20paper%20realizes%20this%0Aattack%20flow%20by%20leveraging%20a%20diffusion%20model%20to%20generate%20realistic%20accessories%0Aon%20pedestrian%20images%20%28e.g.%2C%20bags%2C%20hats%2C%20etc.%29%20as%20backdoor%20triggers.%20However%2C%0Athe%20noticeable%20domain%20gap%20between%20the%20triggers%20generated%20by%20the%20off-the-shelf%0Adiffusion%20model%20and%20their%20physical%20counterparts%20results%20in%20a%20low%20attack%20success%0Arate.%20Therefore%2C%20we%20introduce%20a%20novel%20diffusion-based%20physical%20backdoor%20attack%0A%28DiffPhysBA%29%20method%20that%20adopts%20a%20training-free%20similarity-guided%20sampling%0Aprocess%20to%20enhance%20the%20resemblance%20between%20generated%20and%20physical%20triggers.%0AConsequently%2C%20DiffPhysBA%20can%20generate%20realistic%20attributes%20as%20semantic-level%0Atriggers%20in%20the%20digital%20domain%20and%20provides%20higher%20physical%20ASR%20compared%20to%20the%0Adirect%20paste%20method%20by%2025.6%25%20on%20the%20real-world%20test%20set.%20Through%20evaluations%20on%0Anewly%20proposed%20real-world%20and%20synthetic%20ReID%20test%20sets%2C%20DiffPhysBA%20demonstrates%0Aan%20impressive%20success%20rate%20exceeding%2090%25%20in%20both%20the%20digital%20and%20physical%0Adomains.%20Notably%2C%20it%20excels%20in%20digital%20stealth%20metrics%20and%20can%20effectively%0Aevade%20state-of-the-art%20defense%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19990v1&entry.124074799=Read"},
{"title": "CoVR: Learning Composed Video Retrieval from Web Video Captions", "author": "Lucas Ventura and Antoine Yang and Cordelia Schmid and G\u00fcl Varol", "abstract": "  Composed Image Retrieval (CoIR) has recently gained popularity as a task that\nconsiders both text and image queries together, to search for relevant images\nin a database. Most CoIR approaches require manually annotated datasets,\ncomprising image-text-image triplets, where the text describes a modification\nfrom the query image to the target image. However, manual curation of CoIR\ntriplets is expensive and prevents scalability. In this work, we instead\npropose a scalable automatic dataset creation methodology that generates\ntriplets given video-caption pairs, while also expanding the scope of the task\nto include composed video retrieval (CoVR). To this end, we mine paired videos\nwith a similar caption from a large database, and leverage a large language\nmodel to generate the corresponding modification text. Applying this\nmethodology to the extensive WebVid2M collection, we automatically construct\nour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, we\nintroduce a new benchmark for CoVR with a manually annotated evaluation set,\nalong with baseline results. Our experiments further demonstrate that training\na CoVR model on our dataset effectively transfers to CoIR, leading to improved\nstate-of-the-art performance in the zero-shot setup on both the CIRR and\nFashionIQ benchmarks. Our code, datasets, and models are publicly available at\nhttps://imagine.enpc.fr/~ventural/covr.\n", "link": "http://arxiv.org/abs/2308.14746v3", "date": "2024-05-30", "relevancy": 2.1232, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5412}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.526}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoVR%3A%20Learning%20Composed%20Video%20Retrieval%20from%20Web%20Video%20Captions&body=Title%3A%20CoVR%3A%20Learning%20Composed%20Video%20Retrieval%20from%20Web%20Video%20Captions%0AAuthor%3A%20Lucas%20Ventura%20and%20Antoine%20Yang%20and%20Cordelia%20Schmid%20and%20G%C3%BCl%20Varol%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CoIR%29%20has%20recently%20gained%20popularity%20as%20a%20task%20that%0Aconsiders%20both%20text%20and%20image%20queries%20together%2C%20to%20search%20for%20relevant%20images%0Ain%20a%20database.%20Most%20CoIR%20approaches%20require%20manually%20annotated%20datasets%2C%0Acomprising%20image-text-image%20triplets%2C%20where%20the%20text%20describes%20a%20modification%0Afrom%20the%20query%20image%20to%20the%20target%20image.%20However%2C%20manual%20curation%20of%20CoIR%0Atriplets%20is%20expensive%20and%20prevents%20scalability.%20In%20this%20work%2C%20we%20instead%0Apropose%20a%20scalable%20automatic%20dataset%20creation%20methodology%20that%20generates%0Atriplets%20given%20video-caption%20pairs%2C%20while%20also%20expanding%20the%20scope%20of%20the%20task%0Ato%20include%20composed%20video%20retrieval%20%28CoVR%29.%20To%20this%20end%2C%20we%20mine%20paired%20videos%0Awith%20a%20similar%20caption%20from%20a%20large%20database%2C%20and%20leverage%20a%20large%20language%0Amodel%20to%20generate%20the%20corresponding%20modification%20text.%20Applying%20this%0Amethodology%20to%20the%20extensive%20WebVid2M%20collection%2C%20we%20automatically%20construct%0Aour%20WebVid-CoVR%20dataset%2C%20resulting%20in%201.6%20million%20triplets.%20Moreover%2C%20we%0Aintroduce%20a%20new%20benchmark%20for%20CoVR%20with%20a%20manually%20annotated%20evaluation%20set%2C%0Aalong%20with%20baseline%20results.%20Our%20experiments%20further%20demonstrate%20that%20training%0Aa%20CoVR%20model%20on%20our%20dataset%20effectively%20transfers%20to%20CoIR%2C%20leading%20to%20improved%0Astate-of-the-art%20performance%20in%20the%20zero-shot%20setup%20on%20both%20the%20CIRR%20and%0AFashionIQ%20benchmarks.%20Our%20code%2C%20datasets%2C%20and%20models%20are%20publicly%20available%20at%0Ahttps%3A//imagine.enpc.fr/~ventural/covr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14746v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoVR%253A%2520Learning%2520Composed%2520Video%2520Retrieval%2520from%2520Web%2520Video%2520Captions%26entry.906535625%3DLucas%2520Ventura%2520and%2520Antoine%2520Yang%2520and%2520Cordelia%2520Schmid%2520and%2520G%25C3%25BCl%2520Varol%26entry.1292438233%3D%2520%2520Composed%2520Image%2520Retrieval%2520%2528CoIR%2529%2520has%2520recently%2520gained%2520popularity%2520as%2520a%2520task%2520that%250Aconsiders%2520both%2520text%2520and%2520image%2520queries%2520together%252C%2520to%2520search%2520for%2520relevant%2520images%250Ain%2520a%2520database.%2520Most%2520CoIR%2520approaches%2520require%2520manually%2520annotated%2520datasets%252C%250Acomprising%2520image-text-image%2520triplets%252C%2520where%2520the%2520text%2520describes%2520a%2520modification%250Afrom%2520the%2520query%2520image%2520to%2520the%2520target%2520image.%2520However%252C%2520manual%2520curation%2520of%2520CoIR%250Atriplets%2520is%2520expensive%2520and%2520prevents%2520scalability.%2520In%2520this%2520work%252C%2520we%2520instead%250Apropose%2520a%2520scalable%2520automatic%2520dataset%2520creation%2520methodology%2520that%2520generates%250Atriplets%2520given%2520video-caption%2520pairs%252C%2520while%2520also%2520expanding%2520the%2520scope%2520of%2520the%2520task%250Ato%2520include%2520composed%2520video%2520retrieval%2520%2528CoVR%2529.%2520To%2520this%2520end%252C%2520we%2520mine%2520paired%2520videos%250Awith%2520a%2520similar%2520caption%2520from%2520a%2520large%2520database%252C%2520and%2520leverage%2520a%2520large%2520language%250Amodel%2520to%2520generate%2520the%2520corresponding%2520modification%2520text.%2520Applying%2520this%250Amethodology%2520to%2520the%2520extensive%2520WebVid2M%2520collection%252C%2520we%2520automatically%2520construct%250Aour%2520WebVid-CoVR%2520dataset%252C%2520resulting%2520in%25201.6%2520million%2520triplets.%2520Moreover%252C%2520we%250Aintroduce%2520a%2520new%2520benchmark%2520for%2520CoVR%2520with%2520a%2520manually%2520annotated%2520evaluation%2520set%252C%250Aalong%2520with%2520baseline%2520results.%2520Our%2520experiments%2520further%2520demonstrate%2520that%2520training%250Aa%2520CoVR%2520model%2520on%2520our%2520dataset%2520effectively%2520transfers%2520to%2520CoIR%252C%2520leading%2520to%2520improved%250Astate-of-the-art%2520performance%2520in%2520the%2520zero-shot%2520setup%2520on%2520both%2520the%2520CIRR%2520and%250AFashionIQ%2520benchmarks.%2520Our%2520code%252C%2520datasets%252C%2520and%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//imagine.enpc.fr/~ventural/covr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.14746v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoVR%3A%20Learning%20Composed%20Video%20Retrieval%20from%20Web%20Video%20Captions&entry.906535625=Lucas%20Ventura%20and%20Antoine%20Yang%20and%20Cordelia%20Schmid%20and%20G%C3%BCl%20Varol&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CoIR%29%20has%20recently%20gained%20popularity%20as%20a%20task%20that%0Aconsiders%20both%20text%20and%20image%20queries%20together%2C%20to%20search%20for%20relevant%20images%0Ain%20a%20database.%20Most%20CoIR%20approaches%20require%20manually%20annotated%20datasets%2C%0Acomprising%20image-text-image%20triplets%2C%20where%20the%20text%20describes%20a%20modification%0Afrom%20the%20query%20image%20to%20the%20target%20image.%20However%2C%20manual%20curation%20of%20CoIR%0Atriplets%20is%20expensive%20and%20prevents%20scalability.%20In%20this%20work%2C%20we%20instead%0Apropose%20a%20scalable%20automatic%20dataset%20creation%20methodology%20that%20generates%0Atriplets%20given%20video-caption%20pairs%2C%20while%20also%20expanding%20the%20scope%20of%20the%20task%0Ato%20include%20composed%20video%20retrieval%20%28CoVR%29.%20To%20this%20end%2C%20we%20mine%20paired%20videos%0Awith%20a%20similar%20caption%20from%20a%20large%20database%2C%20and%20leverage%20a%20large%20language%0Amodel%20to%20generate%20the%20corresponding%20modification%20text.%20Applying%20this%0Amethodology%20to%20the%20extensive%20WebVid2M%20collection%2C%20we%20automatically%20construct%0Aour%20WebVid-CoVR%20dataset%2C%20resulting%20in%201.6%20million%20triplets.%20Moreover%2C%20we%0Aintroduce%20a%20new%20benchmark%20for%20CoVR%20with%20a%20manually%20annotated%20evaluation%20set%2C%0Aalong%20with%20baseline%20results.%20Our%20experiments%20further%20demonstrate%20that%20training%0Aa%20CoVR%20model%20on%20our%20dataset%20effectively%20transfers%20to%20CoIR%2C%20leading%20to%20improved%0Astate-of-the-art%20performance%20in%20the%20zero-shot%20setup%20on%20both%20the%20CIRR%20and%0AFashionIQ%20benchmarks.%20Our%20code%2C%20datasets%2C%20and%20models%20are%20publicly%20available%20at%0Ahttps%3A//imagine.enpc.fr/~ventural/covr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14746v3&entry.124074799=Read"},
{"title": "PostDoc: Generating Poster from a Long Multimodal Document Using Deep\n  Submodular Optimization", "author": "Vijay Jaisankar and Sambaran Bandyopadhyay and Kalp Vyas and Varre Chaitanya and Shwetha Somasundaram", "abstract": "  A poster from a long input document can be considered as a one-page\neasy-to-read multimodal (text and images) summary presented on a nice template\nwith good design elements. Automatic transformation of a long document into a\nposter is a very less studied but challenging task. It involves content\nsummarization of the input document followed by template generation and\nharmonization. In this work, we propose a novel deep submodular function which\ncan be trained on ground truth summaries to extract multimodal content from the\ndocument and explicitly ensures good coverage, diversity and alignment of text\nand images. Then, we use an LLM based paraphraser and propose to generate a\ntemplate with various design aspects conditioned on the input content. We show\nthe merits of our approach through extensive automated and human evaluations.\n", "link": "http://arxiv.org/abs/2405.20213v1", "date": "2024-05-30", "relevancy": 2.1231, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5419}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5306}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PostDoc%3A%20Generating%20Poster%20from%20a%20Long%20Multimodal%20Document%20Using%20Deep%0A%20%20Submodular%20Optimization&body=Title%3A%20PostDoc%3A%20Generating%20Poster%20from%20a%20Long%20Multimodal%20Document%20Using%20Deep%0A%20%20Submodular%20Optimization%0AAuthor%3A%20Vijay%20Jaisankar%20and%20Sambaran%20Bandyopadhyay%20and%20Kalp%20Vyas%20and%20Varre%20Chaitanya%20and%20Shwetha%20Somasundaram%0AAbstract%3A%20%20%20A%20poster%20from%20a%20long%20input%20document%20can%20be%20considered%20as%20a%20one-page%0Aeasy-to-read%20multimodal%20%28text%20and%20images%29%20summary%20presented%20on%20a%20nice%20template%0Awith%20good%20design%20elements.%20Automatic%20transformation%20of%20a%20long%20document%20into%20a%0Aposter%20is%20a%20very%20less%20studied%20but%20challenging%20task.%20It%20involves%20content%0Asummarization%20of%20the%20input%20document%20followed%20by%20template%20generation%20and%0Aharmonization.%20In%20this%20work%2C%20we%20propose%20a%20novel%20deep%20submodular%20function%20which%0Acan%20be%20trained%20on%20ground%20truth%20summaries%20to%20extract%20multimodal%20content%20from%20the%0Adocument%20and%20explicitly%20ensures%20good%20coverage%2C%20diversity%20and%20alignment%20of%20text%0Aand%20images.%20Then%2C%20we%20use%20an%20LLM%20based%20paraphraser%20and%20propose%20to%20generate%20a%0Atemplate%20with%20various%20design%20aspects%20conditioned%20on%20the%20input%20content.%20We%20show%0Athe%20merits%20of%20our%20approach%20through%20extensive%20automated%20and%20human%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPostDoc%253A%2520Generating%2520Poster%2520from%2520a%2520Long%2520Multimodal%2520Document%2520Using%2520Deep%250A%2520%2520Submodular%2520Optimization%26entry.906535625%3DVijay%2520Jaisankar%2520and%2520Sambaran%2520Bandyopadhyay%2520and%2520Kalp%2520Vyas%2520and%2520Varre%2520Chaitanya%2520and%2520Shwetha%2520Somasundaram%26entry.1292438233%3D%2520%2520A%2520poster%2520from%2520a%2520long%2520input%2520document%2520can%2520be%2520considered%2520as%2520a%2520one-page%250Aeasy-to-read%2520multimodal%2520%2528text%2520and%2520images%2529%2520summary%2520presented%2520on%2520a%2520nice%2520template%250Awith%2520good%2520design%2520elements.%2520Automatic%2520transformation%2520of%2520a%2520long%2520document%2520into%2520a%250Aposter%2520is%2520a%2520very%2520less%2520studied%2520but%2520challenging%2520task.%2520It%2520involves%2520content%250Asummarization%2520of%2520the%2520input%2520document%2520followed%2520by%2520template%2520generation%2520and%250Aharmonization.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520deep%2520submodular%2520function%2520which%250Acan%2520be%2520trained%2520on%2520ground%2520truth%2520summaries%2520to%2520extract%2520multimodal%2520content%2520from%2520the%250Adocument%2520and%2520explicitly%2520ensures%2520good%2520coverage%252C%2520diversity%2520and%2520alignment%2520of%2520text%250Aand%2520images.%2520Then%252C%2520we%2520use%2520an%2520LLM%2520based%2520paraphraser%2520and%2520propose%2520to%2520generate%2520a%250Atemplate%2520with%2520various%2520design%2520aspects%2520conditioned%2520on%2520the%2520input%2520content.%2520We%2520show%250Athe%2520merits%2520of%2520our%2520approach%2520through%2520extensive%2520automated%2520and%2520human%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PostDoc%3A%20Generating%20Poster%20from%20a%20Long%20Multimodal%20Document%20Using%20Deep%0A%20%20Submodular%20Optimization&entry.906535625=Vijay%20Jaisankar%20and%20Sambaran%20Bandyopadhyay%20and%20Kalp%20Vyas%20and%20Varre%20Chaitanya%20and%20Shwetha%20Somasundaram&entry.1292438233=%20%20A%20poster%20from%20a%20long%20input%20document%20can%20be%20considered%20as%20a%20one-page%0Aeasy-to-read%20multimodal%20%28text%20and%20images%29%20summary%20presented%20on%20a%20nice%20template%0Awith%20good%20design%20elements.%20Automatic%20transformation%20of%20a%20long%20document%20into%20a%0Aposter%20is%20a%20very%20less%20studied%20but%20challenging%20task.%20It%20involves%20content%0Asummarization%20of%20the%20input%20document%20followed%20by%20template%20generation%20and%0Aharmonization.%20In%20this%20work%2C%20we%20propose%20a%20novel%20deep%20submodular%20function%20which%0Acan%20be%20trained%20on%20ground%20truth%20summaries%20to%20extract%20multimodal%20content%20from%20the%0Adocument%20and%20explicitly%20ensures%20good%20coverage%2C%20diversity%20and%20alignment%20of%20text%0Aand%20images.%20Then%2C%20we%20use%20an%20LLM%20based%20paraphraser%20and%20propose%20to%20generate%20a%0Atemplate%20with%20various%20design%20aspects%20conditioned%20on%20the%20input%20content.%20We%20show%0Athe%20merits%20of%20our%20approach%20through%20extensive%20automated%20and%20human%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20213v1&entry.124074799=Read"},
{"title": "Scaling White-Box Transformers for Vision", "author": "Jinrui Yang and Xianhang Li and Druv Pai and Yuyin Zhou and Yi Ma and Yaodong Yu and Cihang Xie", "abstract": "  CRATE, a white-box transformer architecture designed to learn compressed and\nsparse representations, offers an intriguing alternative to standard vision\ntransformers (ViTs) due to its inherent mathematical interpretability. Despite\nextensive investigations into the scaling behaviors of language and vision\ntransformers, the scalability of CRATE remains an open question which this\npaper aims to address. Specifically, we propose CRATE-$\\alpha$, featuring\nstrategic yet minimal modifications to the sparse coding block in the CRATE\narchitecture design, and a light training recipe designed to improve the\nscalability of CRATE. Through extensive experiments, we demonstrate that\nCRATE-$\\alpha$ can effectively scale with larger model sizes and datasets. For\nexample, our CRATE-$\\alpha$-B substantially outperforms the prior best CRATE-B\nmodel accuracy on ImageNet classification by 3.7%, achieving an accuracy of\n83.2%. Meanwhile, when scaling further, our CRATE-$\\alpha$-L obtains an\nImageNet classification accuracy of 85.1%. More notably, these model\nperformance improvements are achieved while preserving, and potentially even\nenhancing the interpretability of learned CRATE models, as we demonstrate\nthrough showing that the learned token representations of increasingly larger\ntrained CRATE-$\\alpha$ models yield increasingly higher-quality unsupervised\nobject segmentation of images. The project page is\nhttps://rayjryang.github.io/CRATE-alpha/.\n", "link": "http://arxiv.org/abs/2405.20299v1", "date": "2024-05-30", "relevancy": 2.1214, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6078}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20White-Box%20Transformers%20for%20Vision&body=Title%3A%20Scaling%20White-Box%20Transformers%20for%20Vision%0AAuthor%3A%20Jinrui%20Yang%20and%20Xianhang%20Li%20and%20Druv%20Pai%20and%20Yuyin%20Zhou%20and%20Yi%20Ma%20and%20Yaodong%20Yu%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20CRATE%2C%20a%20white-box%20transformer%20architecture%20designed%20to%20learn%20compressed%20and%0Asparse%20representations%2C%20offers%20an%20intriguing%20alternative%20to%20standard%20vision%0Atransformers%20%28ViTs%29%20due%20to%20its%20inherent%20mathematical%20interpretability.%20Despite%0Aextensive%20investigations%20into%20the%20scaling%20behaviors%20of%20language%20and%20vision%0Atransformers%2C%20the%20scalability%20of%20CRATE%20remains%20an%20open%20question%20which%20this%0Apaper%20aims%20to%20address.%20Specifically%2C%20we%20propose%20CRATE-%24%5Calpha%24%2C%20featuring%0Astrategic%20yet%20minimal%20modifications%20to%20the%20sparse%20coding%20block%20in%20the%20CRATE%0Aarchitecture%20design%2C%20and%20a%20light%20training%20recipe%20designed%20to%20improve%20the%0Ascalability%20of%20CRATE.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%0ACRATE-%24%5Calpha%24%20can%20effectively%20scale%20with%20larger%20model%20sizes%20and%20datasets.%20For%0Aexample%2C%20our%20CRATE-%24%5Calpha%24-B%20substantially%20outperforms%20the%20prior%20best%20CRATE-B%0Amodel%20accuracy%20on%20ImageNet%20classification%20by%203.7%25%2C%20achieving%20an%20accuracy%20of%0A83.2%25.%20Meanwhile%2C%20when%20scaling%20further%2C%20our%20CRATE-%24%5Calpha%24-L%20obtains%20an%0AImageNet%20classification%20accuracy%20of%2085.1%25.%20More%20notably%2C%20these%20model%0Aperformance%20improvements%20are%20achieved%20while%20preserving%2C%20and%20potentially%20even%0Aenhancing%20the%20interpretability%20of%20learned%20CRATE%20models%2C%20as%20we%20demonstrate%0Athrough%20showing%20that%20the%20learned%20token%20representations%20of%20increasingly%20larger%0Atrained%20CRATE-%24%5Calpha%24%20models%20yield%20increasingly%20higher-quality%20unsupervised%0Aobject%20segmentation%20of%20images.%20The%20project%20page%20is%0Ahttps%3A//rayjryang.github.io/CRATE-alpha/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520White-Box%2520Transformers%2520for%2520Vision%26entry.906535625%3DJinrui%2520Yang%2520and%2520Xianhang%2520Li%2520and%2520Druv%2520Pai%2520and%2520Yuyin%2520Zhou%2520and%2520Yi%2520Ma%2520and%2520Yaodong%2520Yu%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520CRATE%252C%2520a%2520white-box%2520transformer%2520architecture%2520designed%2520to%2520learn%2520compressed%2520and%250Asparse%2520representations%252C%2520offers%2520an%2520intriguing%2520alternative%2520to%2520standard%2520vision%250Atransformers%2520%2528ViTs%2529%2520due%2520to%2520its%2520inherent%2520mathematical%2520interpretability.%2520Despite%250Aextensive%2520investigations%2520into%2520the%2520scaling%2520behaviors%2520of%2520language%2520and%2520vision%250Atransformers%252C%2520the%2520scalability%2520of%2520CRATE%2520remains%2520an%2520open%2520question%2520which%2520this%250Apaper%2520aims%2520to%2520address.%2520Specifically%252C%2520we%2520propose%2520CRATE-%2524%255Calpha%2524%252C%2520featuring%250Astrategic%2520yet%2520minimal%2520modifications%2520to%2520the%2520sparse%2520coding%2520block%2520in%2520the%2520CRATE%250Aarchitecture%2520design%252C%2520and%2520a%2520light%2520training%2520recipe%2520designed%2520to%2520improve%2520the%250Ascalability%2520of%2520CRATE.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%250ACRATE-%2524%255Calpha%2524%2520can%2520effectively%2520scale%2520with%2520larger%2520model%2520sizes%2520and%2520datasets.%2520For%250Aexample%252C%2520our%2520CRATE-%2524%255Calpha%2524-B%2520substantially%2520outperforms%2520the%2520prior%2520best%2520CRATE-B%250Amodel%2520accuracy%2520on%2520ImageNet%2520classification%2520by%25203.7%2525%252C%2520achieving%2520an%2520accuracy%2520of%250A83.2%2525.%2520Meanwhile%252C%2520when%2520scaling%2520further%252C%2520our%2520CRATE-%2524%255Calpha%2524-L%2520obtains%2520an%250AImageNet%2520classification%2520accuracy%2520of%252085.1%2525.%2520More%2520notably%252C%2520these%2520model%250Aperformance%2520improvements%2520are%2520achieved%2520while%2520preserving%252C%2520and%2520potentially%2520even%250Aenhancing%2520the%2520interpretability%2520of%2520learned%2520CRATE%2520models%252C%2520as%2520we%2520demonstrate%250Athrough%2520showing%2520that%2520the%2520learned%2520token%2520representations%2520of%2520increasingly%2520larger%250Atrained%2520CRATE-%2524%255Calpha%2524%2520models%2520yield%2520increasingly%2520higher-quality%2520unsupervised%250Aobject%2520segmentation%2520of%2520images.%2520The%2520project%2520page%2520is%250Ahttps%253A//rayjryang.github.io/CRATE-alpha/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20White-Box%20Transformers%20for%20Vision&entry.906535625=Jinrui%20Yang%20and%20Xianhang%20Li%20and%20Druv%20Pai%20and%20Yuyin%20Zhou%20and%20Yi%20Ma%20and%20Yaodong%20Yu%20and%20Cihang%20Xie&entry.1292438233=%20%20CRATE%2C%20a%20white-box%20transformer%20architecture%20designed%20to%20learn%20compressed%20and%0Asparse%20representations%2C%20offers%20an%20intriguing%20alternative%20to%20standard%20vision%0Atransformers%20%28ViTs%29%20due%20to%20its%20inherent%20mathematical%20interpretability.%20Despite%0Aextensive%20investigations%20into%20the%20scaling%20behaviors%20of%20language%20and%20vision%0Atransformers%2C%20the%20scalability%20of%20CRATE%20remains%20an%20open%20question%20which%20this%0Apaper%20aims%20to%20address.%20Specifically%2C%20we%20propose%20CRATE-%24%5Calpha%24%2C%20featuring%0Astrategic%20yet%20minimal%20modifications%20to%20the%20sparse%20coding%20block%20in%20the%20CRATE%0Aarchitecture%20design%2C%20and%20a%20light%20training%20recipe%20designed%20to%20improve%20the%0Ascalability%20of%20CRATE.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%0ACRATE-%24%5Calpha%24%20can%20effectively%20scale%20with%20larger%20model%20sizes%20and%20datasets.%20For%0Aexample%2C%20our%20CRATE-%24%5Calpha%24-B%20substantially%20outperforms%20the%20prior%20best%20CRATE-B%0Amodel%20accuracy%20on%20ImageNet%20classification%20by%203.7%25%2C%20achieving%20an%20accuracy%20of%0A83.2%25.%20Meanwhile%2C%20when%20scaling%20further%2C%20our%20CRATE-%24%5Calpha%24-L%20obtains%20an%0AImageNet%20classification%20accuracy%20of%2085.1%25.%20More%20notably%2C%20these%20model%0Aperformance%20improvements%20are%20achieved%20while%20preserving%2C%20and%20potentially%20even%0Aenhancing%20the%20interpretability%20of%20learned%20CRATE%20models%2C%20as%20we%20demonstrate%0Athrough%20showing%20that%20the%20learned%20token%20representations%20of%20increasingly%20larger%0Atrained%20CRATE-%24%5Calpha%24%20models%20yield%20increasingly%20higher-quality%20unsupervised%0Aobject%20segmentation%20of%20images.%20The%20project%20page%20is%0Ahttps%3A//rayjryang.github.io/CRATE-alpha/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20299v1&entry.124074799=Read"},
{"title": "Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation", "author": "Haoran Chen and Xintong Han and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Most existing methods for unsupervised domain adaptation (UDA) rely on a\nshared network to extract domain-invariant features. However, when facing\nmultiple source domains, optimizing such a network involves updating the\nparameters of the entire network, making it both computationally expensive and\nchallenging, particularly when coupled with min-max objectives. Inspired by\nrecent advances in prompt learning that adapts high-capacity models for\ndownstream tasks in a computationally economic way, we introduce Multi-Prompt\nAlignment (MPA), a simple yet efficient framework for multi-source UDA. Given a\nsource and target domain pair, MPA first trains an individual prompt to\nminimize the domain gap through a contrastive loss. Then, MPA denoises the\nlearned prompts through an auto-encoding process and aligns them by maximizing\nthe agreement of all the reconstructed prompts. Moreover, we show that the\nresulting subspace acquired from the auto-encoding process can easily\ngeneralize to a streamlined set of target domains, making our method more\nefficient for practical usage. Extensive experiments show that MPA achieves\nstate-of-the-art results on three popular datasets with an impressive average\naccuracy of 54.1% on DomainNet.\n", "link": "http://arxiv.org/abs/2209.15210v5", "date": "2024-05-30", "relevancy": 2.1212, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5348}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Prompt%20Alignment%20for%20Multi-Source%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20Multi-Prompt%20Alignment%20for%20Multi-Source%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Haoran%20Chen%20and%20Xintong%20Han%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Most%20existing%20methods%20for%20unsupervised%20domain%20adaptation%20%28UDA%29%20rely%20on%20a%0Ashared%20network%20to%20extract%20domain-invariant%20features.%20However%2C%20when%20facing%0Amultiple%20source%20domains%2C%20optimizing%20such%20a%20network%20involves%20updating%20the%0Aparameters%20of%20the%20entire%20network%2C%20making%20it%20both%20computationally%20expensive%20and%0Achallenging%2C%20particularly%20when%20coupled%20with%20min-max%20objectives.%20Inspired%20by%0Arecent%20advances%20in%20prompt%20learning%20that%20adapts%20high-capacity%20models%20for%0Adownstream%20tasks%20in%20a%20computationally%20economic%20way%2C%20we%20introduce%20Multi-Prompt%0AAlignment%20%28MPA%29%2C%20a%20simple%20yet%20efficient%20framework%20for%20multi-source%20UDA.%20Given%20a%0Asource%20and%20target%20domain%20pair%2C%20MPA%20first%20trains%20an%20individual%20prompt%20to%0Aminimize%20the%20domain%20gap%20through%20a%20contrastive%20loss.%20Then%2C%20MPA%20denoises%20the%0Alearned%20prompts%20through%20an%20auto-encoding%20process%20and%20aligns%20them%20by%20maximizing%0Athe%20agreement%20of%20all%20the%20reconstructed%20prompts.%20Moreover%2C%20we%20show%20that%20the%0Aresulting%20subspace%20acquired%20from%20the%20auto-encoding%20process%20can%20easily%0Ageneralize%20to%20a%20streamlined%20set%20of%20target%20domains%2C%20making%20our%20method%20more%0Aefficient%20for%20practical%20usage.%20Extensive%20experiments%20show%20that%20MPA%20achieves%0Astate-of-the-art%20results%20on%20three%20popular%20datasets%20with%20an%20impressive%20average%0Aaccuracy%20of%2054.1%25%20on%20DomainNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.15210v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Prompt%2520Alignment%2520for%2520Multi-Source%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DHaoran%2520Chen%2520and%2520Xintong%2520Han%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Most%2520existing%2520methods%2520for%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520rely%2520on%2520a%250Ashared%2520network%2520to%2520extract%2520domain-invariant%2520features.%2520However%252C%2520when%2520facing%250Amultiple%2520source%2520domains%252C%2520optimizing%2520such%2520a%2520network%2520involves%2520updating%2520the%250Aparameters%2520of%2520the%2520entire%2520network%252C%2520making%2520it%2520both%2520computationally%2520expensive%2520and%250Achallenging%252C%2520particularly%2520when%2520coupled%2520with%2520min-max%2520objectives.%2520Inspired%2520by%250Arecent%2520advances%2520in%2520prompt%2520learning%2520that%2520adapts%2520high-capacity%2520models%2520for%250Adownstream%2520tasks%2520in%2520a%2520computationally%2520economic%2520way%252C%2520we%2520introduce%2520Multi-Prompt%250AAlignment%2520%2528MPA%2529%252C%2520a%2520simple%2520yet%2520efficient%2520framework%2520for%2520multi-source%2520UDA.%2520Given%2520a%250Asource%2520and%2520target%2520domain%2520pair%252C%2520MPA%2520first%2520trains%2520an%2520individual%2520prompt%2520to%250Aminimize%2520the%2520domain%2520gap%2520through%2520a%2520contrastive%2520loss.%2520Then%252C%2520MPA%2520denoises%2520the%250Alearned%2520prompts%2520through%2520an%2520auto-encoding%2520process%2520and%2520aligns%2520them%2520by%2520maximizing%250Athe%2520agreement%2520of%2520all%2520the%2520reconstructed%2520prompts.%2520Moreover%252C%2520we%2520show%2520that%2520the%250Aresulting%2520subspace%2520acquired%2520from%2520the%2520auto-encoding%2520process%2520can%2520easily%250Ageneralize%2520to%2520a%2520streamlined%2520set%2520of%2520target%2520domains%252C%2520making%2520our%2520method%2520more%250Aefficient%2520for%2520practical%2520usage.%2520Extensive%2520experiments%2520show%2520that%2520MPA%2520achieves%250Astate-of-the-art%2520results%2520on%2520three%2520popular%2520datasets%2520with%2520an%2520impressive%2520average%250Aaccuracy%2520of%252054.1%2525%2520on%2520DomainNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.15210v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Prompt%20Alignment%20for%20Multi-Source%20Unsupervised%20Domain%20Adaptation&entry.906535625=Haoran%20Chen%20and%20Xintong%20Han%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Most%20existing%20methods%20for%20unsupervised%20domain%20adaptation%20%28UDA%29%20rely%20on%20a%0Ashared%20network%20to%20extract%20domain-invariant%20features.%20However%2C%20when%20facing%0Amultiple%20source%20domains%2C%20optimizing%20such%20a%20network%20involves%20updating%20the%0Aparameters%20of%20the%20entire%20network%2C%20making%20it%20both%20computationally%20expensive%20and%0Achallenging%2C%20particularly%20when%20coupled%20with%20min-max%20objectives.%20Inspired%20by%0Arecent%20advances%20in%20prompt%20learning%20that%20adapts%20high-capacity%20models%20for%0Adownstream%20tasks%20in%20a%20computationally%20economic%20way%2C%20we%20introduce%20Multi-Prompt%0AAlignment%20%28MPA%29%2C%20a%20simple%20yet%20efficient%20framework%20for%20multi-source%20UDA.%20Given%20a%0Asource%20and%20target%20domain%20pair%2C%20MPA%20first%20trains%20an%20individual%20prompt%20to%0Aminimize%20the%20domain%20gap%20through%20a%20contrastive%20loss.%20Then%2C%20MPA%20denoises%20the%0Alearned%20prompts%20through%20an%20auto-encoding%20process%20and%20aligns%20them%20by%20maximizing%0Athe%20agreement%20of%20all%20the%20reconstructed%20prompts.%20Moreover%2C%20we%20show%20that%20the%0Aresulting%20subspace%20acquired%20from%20the%20auto-encoding%20process%20can%20easily%0Ageneralize%20to%20a%20streamlined%20set%20of%20target%20domains%2C%20making%20our%20method%20more%0Aefficient%20for%20practical%20usage.%20Extensive%20experiments%20show%20that%20MPA%20achieves%0Astate-of-the-art%20results%20on%20three%20popular%20datasets%20with%20an%20impressive%20average%0Aaccuracy%20of%2054.1%25%20on%20DomainNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.15210v5&entry.124074799=Read"},
{"title": "Tactile-based Exploration, Mapping and Navigation with\n  Collision-Resilient Aerial Vehicles", "author": "Karishma Patnaik and Aravind Adhith Pandian Saravanakumaran and Wenlong Zhang", "abstract": "  In this article, we introduce novel tactile-based motion primitives termed\n\"tactile-traversal\", \"tactile-turning\" and \"ricocheting\" for unmanned aerial\nvehicles (UAVs). These primitives enable contact-rich UAV missions such as\ntactile-based exploration, mapping, and collision-inclusive navigation. We\nbegin by introducing XPLORER, a passive deformable UAV that sustains collisions\nand establishes smooth contacts by exploiting its spring-augmented chassis.\nNext, an improved and fast converging external force estimation algorithm is\nproposed to detect contacts/collisions. We also design three distinct reaction\ncontrollers for (i) static-wrench application, (ii) disturbance rejection, and\n(iii) collision recovery. Finally, the three new tactile-based motion\nprimitives are proposed by leveraging the reactions obtained from deploying\nthese controllers to interact with surroundings. We showcase the effectiveness\nof these primitives to facilitate efficient exploration and rapid navigation in\nunknown environments by capitalizing on collisions and contacts.\n", "link": "http://arxiv.org/abs/2305.17217v3", "date": "2024-05-30", "relevancy": 2.1178, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5373}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5248}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tactile-based%20Exploration%2C%20Mapping%20and%20Navigation%20with%0A%20%20Collision-Resilient%20Aerial%20Vehicles&body=Title%3A%20Tactile-based%20Exploration%2C%20Mapping%20and%20Navigation%20with%0A%20%20Collision-Resilient%20Aerial%20Vehicles%0AAuthor%3A%20Karishma%20Patnaik%20and%20Aravind%20Adhith%20Pandian%20Saravanakumaran%20and%20Wenlong%20Zhang%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20introduce%20novel%20tactile-based%20motion%20primitives%20termed%0A%22tactile-traversal%22%2C%20%22tactile-turning%22%20and%20%22ricocheting%22%20for%20unmanned%20aerial%0Avehicles%20%28UAVs%29.%20These%20primitives%20enable%20contact-rich%20UAV%20missions%20such%20as%0Atactile-based%20exploration%2C%20mapping%2C%20and%20collision-inclusive%20navigation.%20We%0Abegin%20by%20introducing%20XPLORER%2C%20a%20passive%20deformable%20UAV%20that%20sustains%20collisions%0Aand%20establishes%20smooth%20contacts%20by%20exploiting%20its%20spring-augmented%20chassis.%0ANext%2C%20an%20improved%20and%20fast%20converging%20external%20force%20estimation%20algorithm%20is%0Aproposed%20to%20detect%20contacts/collisions.%20We%20also%20design%20three%20distinct%20reaction%0Acontrollers%20for%20%28i%29%20static-wrench%20application%2C%20%28ii%29%20disturbance%20rejection%2C%20and%0A%28iii%29%20collision%20recovery.%20Finally%2C%20the%20three%20new%20tactile-based%20motion%0Aprimitives%20are%20proposed%20by%20leveraging%20the%20reactions%20obtained%20from%20deploying%0Athese%20controllers%20to%20interact%20with%20surroundings.%20We%20showcase%20the%20effectiveness%0Aof%20these%20primitives%20to%20facilitate%20efficient%20exploration%20and%20rapid%20navigation%20in%0Aunknown%20environments%20by%20capitalizing%20on%20collisions%20and%20contacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17217v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTactile-based%2520Exploration%252C%2520Mapping%2520and%2520Navigation%2520with%250A%2520%2520Collision-Resilient%2520Aerial%2520Vehicles%26entry.906535625%3DKarishma%2520Patnaik%2520and%2520Aravind%2520Adhith%2520Pandian%2520Saravanakumaran%2520and%2520Wenlong%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520introduce%2520novel%2520tactile-based%2520motion%2520primitives%2520termed%250A%2522tactile-traversal%2522%252C%2520%2522tactile-turning%2522%2520and%2520%2522ricocheting%2522%2520for%2520unmanned%2520aerial%250Avehicles%2520%2528UAVs%2529.%2520These%2520primitives%2520enable%2520contact-rich%2520UAV%2520missions%2520such%2520as%250Atactile-based%2520exploration%252C%2520mapping%252C%2520and%2520collision-inclusive%2520navigation.%2520We%250Abegin%2520by%2520introducing%2520XPLORER%252C%2520a%2520passive%2520deformable%2520UAV%2520that%2520sustains%2520collisions%250Aand%2520establishes%2520smooth%2520contacts%2520by%2520exploiting%2520its%2520spring-augmented%2520chassis.%250ANext%252C%2520an%2520improved%2520and%2520fast%2520converging%2520external%2520force%2520estimation%2520algorithm%2520is%250Aproposed%2520to%2520detect%2520contacts/collisions.%2520We%2520also%2520design%2520three%2520distinct%2520reaction%250Acontrollers%2520for%2520%2528i%2529%2520static-wrench%2520application%252C%2520%2528ii%2529%2520disturbance%2520rejection%252C%2520and%250A%2528iii%2529%2520collision%2520recovery.%2520Finally%252C%2520the%2520three%2520new%2520tactile-based%2520motion%250Aprimitives%2520are%2520proposed%2520by%2520leveraging%2520the%2520reactions%2520obtained%2520from%2520deploying%250Athese%2520controllers%2520to%2520interact%2520with%2520surroundings.%2520We%2520showcase%2520the%2520effectiveness%250Aof%2520these%2520primitives%2520to%2520facilitate%2520efficient%2520exploration%2520and%2520rapid%2520navigation%2520in%250Aunknown%2520environments%2520by%2520capitalizing%2520on%2520collisions%2520and%2520contacts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.17217v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tactile-based%20Exploration%2C%20Mapping%20and%20Navigation%20with%0A%20%20Collision-Resilient%20Aerial%20Vehicles&entry.906535625=Karishma%20Patnaik%20and%20Aravind%20Adhith%20Pandian%20Saravanakumaran%20and%20Wenlong%20Zhang&entry.1292438233=%20%20In%20this%20article%2C%20we%20introduce%20novel%20tactile-based%20motion%20primitives%20termed%0A%22tactile-traversal%22%2C%20%22tactile-turning%22%20and%20%22ricocheting%22%20for%20unmanned%20aerial%0Avehicles%20%28UAVs%29.%20These%20primitives%20enable%20contact-rich%20UAV%20missions%20such%20as%0Atactile-based%20exploration%2C%20mapping%2C%20and%20collision-inclusive%20navigation.%20We%0Abegin%20by%20introducing%20XPLORER%2C%20a%20passive%20deformable%20UAV%20that%20sustains%20collisions%0Aand%20establishes%20smooth%20contacts%20by%20exploiting%20its%20spring-augmented%20chassis.%0ANext%2C%20an%20improved%20and%20fast%20converging%20external%20force%20estimation%20algorithm%20is%0Aproposed%20to%20detect%20contacts/collisions.%20We%20also%20design%20three%20distinct%20reaction%0Acontrollers%20for%20%28i%29%20static-wrench%20application%2C%20%28ii%29%20disturbance%20rejection%2C%20and%0A%28iii%29%20collision%20recovery.%20Finally%2C%20the%20three%20new%20tactile-based%20motion%0Aprimitives%20are%20proposed%20by%20leveraging%20the%20reactions%20obtained%20from%20deploying%0Athese%20controllers%20to%20interact%20with%20surroundings.%20We%20showcase%20the%20effectiveness%0Aof%20these%20primitives%20to%20facilitate%20efficient%20exploration%20and%20rapid%20navigation%20in%0Aunknown%20environments%20by%20capitalizing%20on%20collisions%20and%20contacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17217v3&entry.124074799=Read"},
{"title": "Trustworthy Partial Label Learning with Out-of-distribution Detection", "author": "Jintao Huang and Yiu-Ming Cheung", "abstract": "  Partial Label Learning (PLL) grapples with learning from ambiguously labelled\ndata, and it has been successfully applied in fields such as image recognition.\nNevertheless, traditional PLL methods rely on the closed-world assumption,\nwhich can be limiting in open-world scenarios and negatively impact model\nperformance and generalization. To tackle these challenges, our study\nintroduces a novel method called PLL-OOD, which is the first to incorporate\nOut-of-Distribution (OOD) detection into the PLL framework. PLL-OOD\nsignificantly enhances model adaptability and accuracy by merging\nself-supervised learning with partial label loss and pioneering the\nPartial-Energy (PE) score for OOD detection. This approach improves data\nfeature representation and effectively disambiguates candidate labels, using a\ndynamic label confidence matrix to refine predictions. The PE score, adjusted\nby label confidence, precisely identifies OOD instances, optimizing model\ntraining towards in-distribution data. This innovative method markedly boosts\nPLL model robustness and performance in open-world settings. To validate our\napproach, we conducted a comprehensive comparative experiment combining the\nexisting state-of-the-art PLL model with multiple OOD scores on the CIFAR-10\nand CIFAR-100 datasets with various OOD datasets. The results demonstrate that\nthe proposed PLL-OOD framework is highly effective and effectiveness\noutperforms existing models, showcasing its superiority and effectiveness.\n", "link": "http://arxiv.org/abs/2403.06681v2", "date": "2024-05-30", "relevancy": 2.0974, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5703}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5179}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trustworthy%20Partial%20Label%20Learning%20with%20Out-of-distribution%20Detection&body=Title%3A%20Trustworthy%20Partial%20Label%20Learning%20with%20Out-of-distribution%20Detection%0AAuthor%3A%20Jintao%20Huang%20and%20Yiu-Ming%20Cheung%0AAbstract%3A%20%20%20Partial%20Label%20Learning%20%28PLL%29%20grapples%20with%20learning%20from%20ambiguously%20labelled%0Adata%2C%20and%20it%20has%20been%20successfully%20applied%20in%20fields%20such%20as%20image%20recognition.%0ANevertheless%2C%20traditional%20PLL%20methods%20rely%20on%20the%20closed-world%20assumption%2C%0Awhich%20can%20be%20limiting%20in%20open-world%20scenarios%20and%20negatively%20impact%20model%0Aperformance%20and%20generalization.%20To%20tackle%20these%20challenges%2C%20our%20study%0Aintroduces%20a%20novel%20method%20called%20PLL-OOD%2C%20which%20is%20the%20first%20to%20incorporate%0AOut-of-Distribution%20%28OOD%29%20detection%20into%20the%20PLL%20framework.%20PLL-OOD%0Asignificantly%20enhances%20model%20adaptability%20and%20accuracy%20by%20merging%0Aself-supervised%20learning%20with%20partial%20label%20loss%20and%20pioneering%20the%0APartial-Energy%20%28PE%29%20score%20for%20OOD%20detection.%20This%20approach%20improves%20data%0Afeature%20representation%20and%20effectively%20disambiguates%20candidate%20labels%2C%20using%20a%0Adynamic%20label%20confidence%20matrix%20to%20refine%20predictions.%20The%20PE%20score%2C%20adjusted%0Aby%20label%20confidence%2C%20precisely%20identifies%20OOD%20instances%2C%20optimizing%20model%0Atraining%20towards%20in-distribution%20data.%20This%20innovative%20method%20markedly%20boosts%0APLL%20model%20robustness%20and%20performance%20in%20open-world%20settings.%20To%20validate%20our%0Aapproach%2C%20we%20conducted%20a%20comprehensive%20comparative%20experiment%20combining%20the%0Aexisting%20state-of-the-art%20PLL%20model%20with%20multiple%20OOD%20scores%20on%20the%20CIFAR-10%0Aand%20CIFAR-100%20datasets%20with%20various%20OOD%20datasets.%20The%20results%20demonstrate%20that%0Athe%20proposed%20PLL-OOD%20framework%20is%20highly%20effective%20and%20effectiveness%0Aoutperforms%20existing%20models%2C%20showcasing%20its%20superiority%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustworthy%2520Partial%2520Label%2520Learning%2520with%2520Out-of-distribution%2520Detection%26entry.906535625%3DJintao%2520Huang%2520and%2520Yiu-Ming%2520Cheung%26entry.1292438233%3D%2520%2520Partial%2520Label%2520Learning%2520%2528PLL%2529%2520grapples%2520with%2520learning%2520from%2520ambiguously%2520labelled%250Adata%252C%2520and%2520it%2520has%2520been%2520successfully%2520applied%2520in%2520fields%2520such%2520as%2520image%2520recognition.%250ANevertheless%252C%2520traditional%2520PLL%2520methods%2520rely%2520on%2520the%2520closed-world%2520assumption%252C%250Awhich%2520can%2520be%2520limiting%2520in%2520open-world%2520scenarios%2520and%2520negatively%2520impact%2520model%250Aperformance%2520and%2520generalization.%2520To%2520tackle%2520these%2520challenges%252C%2520our%2520study%250Aintroduces%2520a%2520novel%2520method%2520called%2520PLL-OOD%252C%2520which%2520is%2520the%2520first%2520to%2520incorporate%250AOut-of-Distribution%2520%2528OOD%2529%2520detection%2520into%2520the%2520PLL%2520framework.%2520PLL-OOD%250Asignificantly%2520enhances%2520model%2520adaptability%2520and%2520accuracy%2520by%2520merging%250Aself-supervised%2520learning%2520with%2520partial%2520label%2520loss%2520and%2520pioneering%2520the%250APartial-Energy%2520%2528PE%2529%2520score%2520for%2520OOD%2520detection.%2520This%2520approach%2520improves%2520data%250Afeature%2520representation%2520and%2520effectively%2520disambiguates%2520candidate%2520labels%252C%2520using%2520a%250Adynamic%2520label%2520confidence%2520matrix%2520to%2520refine%2520predictions.%2520The%2520PE%2520score%252C%2520adjusted%250Aby%2520label%2520confidence%252C%2520precisely%2520identifies%2520OOD%2520instances%252C%2520optimizing%2520model%250Atraining%2520towards%2520in-distribution%2520data.%2520This%2520innovative%2520method%2520markedly%2520boosts%250APLL%2520model%2520robustness%2520and%2520performance%2520in%2520open-world%2520settings.%2520To%2520validate%2520our%250Aapproach%252C%2520we%2520conducted%2520a%2520comprehensive%2520comparative%2520experiment%2520combining%2520the%250Aexisting%2520state-of-the-art%2520PLL%2520model%2520with%2520multiple%2520OOD%2520scores%2520on%2520the%2520CIFAR-10%250Aand%2520CIFAR-100%2520datasets%2520with%2520various%2520OOD%2520datasets.%2520The%2520results%2520demonstrate%2520that%250Athe%2520proposed%2520PLL-OOD%2520framework%2520is%2520highly%2520effective%2520and%2520effectiveness%250Aoutperforms%2520existing%2520models%252C%2520showcasing%2520its%2520superiority%2520and%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%20Partial%20Label%20Learning%20with%20Out-of-distribution%20Detection&entry.906535625=Jintao%20Huang%20and%20Yiu-Ming%20Cheung&entry.1292438233=%20%20Partial%20Label%20Learning%20%28PLL%29%20grapples%20with%20learning%20from%20ambiguously%20labelled%0Adata%2C%20and%20it%20has%20been%20successfully%20applied%20in%20fields%20such%20as%20image%20recognition.%0ANevertheless%2C%20traditional%20PLL%20methods%20rely%20on%20the%20closed-world%20assumption%2C%0Awhich%20can%20be%20limiting%20in%20open-world%20scenarios%20and%20negatively%20impact%20model%0Aperformance%20and%20generalization.%20To%20tackle%20these%20challenges%2C%20our%20study%0Aintroduces%20a%20novel%20method%20called%20PLL-OOD%2C%20which%20is%20the%20first%20to%20incorporate%0AOut-of-Distribution%20%28OOD%29%20detection%20into%20the%20PLL%20framework.%20PLL-OOD%0Asignificantly%20enhances%20model%20adaptability%20and%20accuracy%20by%20merging%0Aself-supervised%20learning%20with%20partial%20label%20loss%20and%20pioneering%20the%0APartial-Energy%20%28PE%29%20score%20for%20OOD%20detection.%20This%20approach%20improves%20data%0Afeature%20representation%20and%20effectively%20disambiguates%20candidate%20labels%2C%20using%20a%0Adynamic%20label%20confidence%20matrix%20to%20refine%20predictions.%20The%20PE%20score%2C%20adjusted%0Aby%20label%20confidence%2C%20precisely%20identifies%20OOD%20instances%2C%20optimizing%20model%0Atraining%20towards%20in-distribution%20data.%20This%20innovative%20method%20markedly%20boosts%0APLL%20model%20robustness%20and%20performance%20in%20open-world%20settings.%20To%20validate%20our%0Aapproach%2C%20we%20conducted%20a%20comprehensive%20comparative%20experiment%20combining%20the%0Aexisting%20state-of-the-art%20PLL%20model%20with%20multiple%20OOD%20scores%20on%20the%20CIFAR-10%0Aand%20CIFAR-100%20datasets%20with%20various%20OOD%20datasets.%20The%20results%20demonstrate%20that%0Athe%20proposed%20PLL-OOD%20framework%20is%20highly%20effective%20and%20effectiveness%0Aoutperforms%20existing%20models%2C%20showcasing%20its%20superiority%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06681v2&entry.124074799=Read"},
{"title": "Identifying Drivers of Predictive Aleatoric Uncertainty", "author": "Pascal Iversen and Simon Witzke and Katharina Baum and Bernhard Y. Renard", "abstract": "  Explainability and uncertainty quantification are two pillars of trustable\nartificial intelligence. However, the reasoning behind uncertainty estimates is\ngenerally left unexplained. Identifying the drivers of uncertainty complements\nexplanations of point predictions in recognizing model limitations and enhances\ntrust in decisions and their communication. So far, explanations of\nuncertainties have been rarely studied. The few exceptions rely on Bayesian\nneural networks or technically intricate approaches, such as auxiliary\ngenerative models, thereby hindering their broad adoption. We present a simple\napproach to explain predictive aleatoric uncertainties. We estimate uncertainty\nas predictive variance by adapting a neural network with a Gaussian output\ndistribution. Subsequently, we apply out-of-the-box explainers to the model's\nvariance output. This approach can explain uncertainty influences more reliably\nthan literature baselines, which we evaluate in a synthetic setting with a\nknown data-generating process. We further adapt multiple metrics from\nconventional XAI research to uncertainty explanations. We quantify our findings\nwith a nuanced benchmark analysis that includes real-world datasets. Finally,\nwe apply our approach to an age regression model and discover reasonable\nsources of uncertainty. Overall, we explain uncertainty estimates with little\nmodifications to the model architecture and demonstrate that our approach\ncompetes effectively with more intricate methods.\n", "link": "http://arxiv.org/abs/2312.07252v2", "date": "2024-05-30", "relevancy": 2.0938, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5599}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5205}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Drivers%20of%20Predictive%20Aleatoric%20Uncertainty&body=Title%3A%20Identifying%20Drivers%20of%20Predictive%20Aleatoric%20Uncertainty%0AAuthor%3A%20Pascal%20Iversen%20and%20Simon%20Witzke%20and%20Katharina%20Baum%20and%20Bernhard%20Y.%20Renard%0AAbstract%3A%20%20%20Explainability%20and%20uncertainty%20quantification%20are%20two%20pillars%20of%20trustable%0Aartificial%20intelligence.%20However%2C%20the%20reasoning%20behind%20uncertainty%20estimates%20is%0Agenerally%20left%20unexplained.%20Identifying%20the%20drivers%20of%20uncertainty%20complements%0Aexplanations%20of%20point%20predictions%20in%20recognizing%20model%20limitations%20and%20enhances%0Atrust%20in%20decisions%20and%20their%20communication.%20So%20far%2C%20explanations%20of%0Auncertainties%20have%20been%20rarely%20studied.%20The%20few%20exceptions%20rely%20on%20Bayesian%0Aneural%20networks%20or%20technically%20intricate%20approaches%2C%20such%20as%20auxiliary%0Agenerative%20models%2C%20thereby%20hindering%20their%20broad%20adoption.%20We%20present%20a%20simple%0Aapproach%20to%20explain%20predictive%20aleatoric%20uncertainties.%20We%20estimate%20uncertainty%0Aas%20predictive%20variance%20by%20adapting%20a%20neural%20network%20with%20a%20Gaussian%20output%0Adistribution.%20Subsequently%2C%20we%20apply%20out-of-the-box%20explainers%20to%20the%20model%27s%0Avariance%20output.%20This%20approach%20can%20explain%20uncertainty%20influences%20more%20reliably%0Athan%20literature%20baselines%2C%20which%20we%20evaluate%20in%20a%20synthetic%20setting%20with%20a%0Aknown%20data-generating%20process.%20We%20further%20adapt%20multiple%20metrics%20from%0Aconventional%20XAI%20research%20to%20uncertainty%20explanations.%20We%20quantify%20our%20findings%0Awith%20a%20nuanced%20benchmark%20analysis%20that%20includes%20real-world%20datasets.%20Finally%2C%0Awe%20apply%20our%20approach%20to%20an%20age%20regression%20model%20and%20discover%20reasonable%0Asources%20of%20uncertainty.%20Overall%2C%20we%20explain%20uncertainty%20estimates%20with%20little%0Amodifications%20to%20the%20model%20architecture%20and%20demonstrate%20that%20our%20approach%0Acompetes%20effectively%20with%20more%20intricate%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Drivers%2520of%2520Predictive%2520Aleatoric%2520Uncertainty%26entry.906535625%3DPascal%2520Iversen%2520and%2520Simon%2520Witzke%2520and%2520Katharina%2520Baum%2520and%2520Bernhard%2520Y.%2520Renard%26entry.1292438233%3D%2520%2520Explainability%2520and%2520uncertainty%2520quantification%2520are%2520two%2520pillars%2520of%2520trustable%250Aartificial%2520intelligence.%2520However%252C%2520the%2520reasoning%2520behind%2520uncertainty%2520estimates%2520is%250Agenerally%2520left%2520unexplained.%2520Identifying%2520the%2520drivers%2520of%2520uncertainty%2520complements%250Aexplanations%2520of%2520point%2520predictions%2520in%2520recognizing%2520model%2520limitations%2520and%2520enhances%250Atrust%2520in%2520decisions%2520and%2520their%2520communication.%2520So%2520far%252C%2520explanations%2520of%250Auncertainties%2520have%2520been%2520rarely%2520studied.%2520The%2520few%2520exceptions%2520rely%2520on%2520Bayesian%250Aneural%2520networks%2520or%2520technically%2520intricate%2520approaches%252C%2520such%2520as%2520auxiliary%250Agenerative%2520models%252C%2520thereby%2520hindering%2520their%2520broad%2520adoption.%2520We%2520present%2520a%2520simple%250Aapproach%2520to%2520explain%2520predictive%2520aleatoric%2520uncertainties.%2520We%2520estimate%2520uncertainty%250Aas%2520predictive%2520variance%2520by%2520adapting%2520a%2520neural%2520network%2520with%2520a%2520Gaussian%2520output%250Adistribution.%2520Subsequently%252C%2520we%2520apply%2520out-of-the-box%2520explainers%2520to%2520the%2520model%2527s%250Avariance%2520output.%2520This%2520approach%2520can%2520explain%2520uncertainty%2520influences%2520more%2520reliably%250Athan%2520literature%2520baselines%252C%2520which%2520we%2520evaluate%2520in%2520a%2520synthetic%2520setting%2520with%2520a%250Aknown%2520data-generating%2520process.%2520We%2520further%2520adapt%2520multiple%2520metrics%2520from%250Aconventional%2520XAI%2520research%2520to%2520uncertainty%2520explanations.%2520We%2520quantify%2520our%2520findings%250Awith%2520a%2520nuanced%2520benchmark%2520analysis%2520that%2520includes%2520real-world%2520datasets.%2520Finally%252C%250Awe%2520apply%2520our%2520approach%2520to%2520an%2520age%2520regression%2520model%2520and%2520discover%2520reasonable%250Asources%2520of%2520uncertainty.%2520Overall%252C%2520we%2520explain%2520uncertainty%2520estimates%2520with%2520little%250Amodifications%2520to%2520the%2520model%2520architecture%2520and%2520demonstrate%2520that%2520our%2520approach%250Acompetes%2520effectively%2520with%2520more%2520intricate%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Drivers%20of%20Predictive%20Aleatoric%20Uncertainty&entry.906535625=Pascal%20Iversen%20and%20Simon%20Witzke%20and%20Katharina%20Baum%20and%20Bernhard%20Y.%20Renard&entry.1292438233=%20%20Explainability%20and%20uncertainty%20quantification%20are%20two%20pillars%20of%20trustable%0Aartificial%20intelligence.%20However%2C%20the%20reasoning%20behind%20uncertainty%20estimates%20is%0Agenerally%20left%20unexplained.%20Identifying%20the%20drivers%20of%20uncertainty%20complements%0Aexplanations%20of%20point%20predictions%20in%20recognizing%20model%20limitations%20and%20enhances%0Atrust%20in%20decisions%20and%20their%20communication.%20So%20far%2C%20explanations%20of%0Auncertainties%20have%20been%20rarely%20studied.%20The%20few%20exceptions%20rely%20on%20Bayesian%0Aneural%20networks%20or%20technically%20intricate%20approaches%2C%20such%20as%20auxiliary%0Agenerative%20models%2C%20thereby%20hindering%20their%20broad%20adoption.%20We%20present%20a%20simple%0Aapproach%20to%20explain%20predictive%20aleatoric%20uncertainties.%20We%20estimate%20uncertainty%0Aas%20predictive%20variance%20by%20adapting%20a%20neural%20network%20with%20a%20Gaussian%20output%0Adistribution.%20Subsequently%2C%20we%20apply%20out-of-the-box%20explainers%20to%20the%20model%27s%0Avariance%20output.%20This%20approach%20can%20explain%20uncertainty%20influences%20more%20reliably%0Athan%20literature%20baselines%2C%20which%20we%20evaluate%20in%20a%20synthetic%20setting%20with%20a%0Aknown%20data-generating%20process.%20We%20further%20adapt%20multiple%20metrics%20from%0Aconventional%20XAI%20research%20to%20uncertainty%20explanations.%20We%20quantify%20our%20findings%0Awith%20a%20nuanced%20benchmark%20analysis%20that%20includes%20real-world%20datasets.%20Finally%2C%0Awe%20apply%20our%20approach%20to%20an%20age%20regression%20model%20and%20discover%20reasonable%0Asources%20of%20uncertainty.%20Overall%2C%20we%20explain%20uncertainty%20estimates%20with%20little%0Amodifications%20to%20the%20model%20architecture%20and%20demonstrate%20that%20our%20approach%0Acompetes%20effectively%20with%20more%20intricate%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07252v2&entry.124074799=Read"},
{"title": "P$^2$-ViT: Power-of-Two Post-Training Quantization and Acceleration for\n  Fully Quantized Vision Transformer", "author": "Huihong Shi and Xin Cheng and Wendong Mao and Zhongfeng Wang", "abstract": "  Vision Transformers (ViTs) have excelled in computer vision tasks but are\nmemory-consuming and computation-intensive, challenging their deployment on\nresource-constrained devices. To tackle this limitation, prior works have\nexplored ViT-tailored quantization algorithms but retained floating-point\nscaling factors, which yield non-negligible re-quantization overhead, limiting\nViTs' hardware efficiency and motivating more hardware-friendly solutions. To\nthis end, we propose \\emph{P$^2$-ViT}, the first \\underline{P}ower-of-Two (PoT)\n\\underline{p}ost-training quantization and acceleration framework to accelerate\nfully quantized ViTs. Specifically, {as for quantization,} we explore a\ndedicated quantization scheme to effectively quantize ViTs with PoT scaling\nfactors, thus minimizing the re-quantization overhead. Furthermore, we propose\ncoarse-to-fine automatic mixed-precision quantization to enable better\naccuracy-efficiency trade-offs. {In terms of hardware,} we develop {a dedicated\nchunk-based accelerator} featuring multiple tailored sub-processors to\nindividually handle ViTs' different types of operations, alleviating\nreconfigurable overhead. Additionally, we design {a tailored row-stationary\ndataflow} to seize the pipeline processing opportunity introduced by our PoT\nscaling factors, thereby enhancing throughput. Extensive experiments\nconsistently validate P$^2$-ViT's effectiveness. {Particularly, we offer\ncomparable or even superior quantization performance with PoT scaling factors\nwhen compared to the counterpart with floating-point scaling factors. Besides,\nwe achieve up to $\\mathbf{10.1\\times}$ speedup and $\\mathbf{36.8\\times}$ energy\nsaving over GPU's Turing Tensor Cores, and up to $\\mathbf{1.84\\times}$ higher\ncomputation utilization efficiency against SOTA quantization-based ViT\naccelerators. Codes are available at\n\\url{https://github.com/shihuihong214/P2-ViT}.\n", "link": "http://arxiv.org/abs/2405.19915v1", "date": "2024-05-30", "relevancy": 2.093, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5302}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5286}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P%24%5E2%24-ViT%3A%20Power-of-Two%20Post-Training%20Quantization%20and%20Acceleration%20for%0A%20%20Fully%20Quantized%20Vision%20Transformer&body=Title%3A%20P%24%5E2%24-ViT%3A%20Power-of-Two%20Post-Training%20Quantization%20and%20Acceleration%20for%0A%20%20Fully%20Quantized%20Vision%20Transformer%0AAuthor%3A%20Huihong%20Shi%20and%20Xin%20Cheng%20and%20Wendong%20Mao%20and%20Zhongfeng%20Wang%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20excelled%20in%20computer%20vision%20tasks%20but%20are%0Amemory-consuming%20and%20computation-intensive%2C%20challenging%20their%20deployment%20on%0Aresource-constrained%20devices.%20To%20tackle%20this%20limitation%2C%20prior%20works%20have%0Aexplored%20ViT-tailored%20quantization%20algorithms%20but%20retained%20floating-point%0Ascaling%20factors%2C%20which%20yield%20non-negligible%20re-quantization%20overhead%2C%20limiting%0AViTs%27%20hardware%20efficiency%20and%20motivating%20more%20hardware-friendly%20solutions.%20To%0Athis%20end%2C%20we%20propose%20%5Cemph%7BP%24%5E2%24-ViT%7D%2C%20the%20first%20%5Cunderline%7BP%7Dower-of-Two%20%28PoT%29%0A%5Cunderline%7Bp%7Dost-training%20quantization%20and%20acceleration%20framework%20to%20accelerate%0Afully%20quantized%20ViTs.%20Specifically%2C%20%7Bas%20for%20quantization%2C%7D%20we%20explore%20a%0Adedicated%20quantization%20scheme%20to%20effectively%20quantize%20ViTs%20with%20PoT%20scaling%0Afactors%2C%20thus%20minimizing%20the%20re-quantization%20overhead.%20Furthermore%2C%20we%20propose%0Acoarse-to-fine%20automatic%20mixed-precision%20quantization%20to%20enable%20better%0Aaccuracy-efficiency%20trade-offs.%20%7BIn%20terms%20of%20hardware%2C%7D%20we%20develop%20%7Ba%20dedicated%0Achunk-based%20accelerator%7D%20featuring%20multiple%20tailored%20sub-processors%20to%0Aindividually%20handle%20ViTs%27%20different%20types%20of%20operations%2C%20alleviating%0Areconfigurable%20overhead.%20Additionally%2C%20we%20design%20%7Ba%20tailored%20row-stationary%0Adataflow%7D%20to%20seize%20the%20pipeline%20processing%20opportunity%20introduced%20by%20our%20PoT%0Ascaling%20factors%2C%20thereby%20enhancing%20throughput.%20Extensive%20experiments%0Aconsistently%20validate%20P%24%5E2%24-ViT%27s%20effectiveness.%20%7BParticularly%2C%20we%20offer%0Acomparable%20or%20even%20superior%20quantization%20performance%20with%20PoT%20scaling%20factors%0Awhen%20compared%20to%20the%20counterpart%20with%20floating-point%20scaling%20factors.%20Besides%2C%0Awe%20achieve%20up%20to%20%24%5Cmathbf%7B10.1%5Ctimes%7D%24%20speedup%20and%20%24%5Cmathbf%7B36.8%5Ctimes%7D%24%20energy%0Asaving%20over%20GPU%27s%20Turing%20Tensor%20Cores%2C%20and%20up%20to%20%24%5Cmathbf%7B1.84%5Ctimes%7D%24%20higher%0Acomputation%20utilization%20efficiency%20against%20SOTA%20quantization-based%20ViT%0Aaccelerators.%20Codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/shihuihong214/P2-ViT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP%2524%255E2%2524-ViT%253A%2520Power-of-Two%2520Post-Training%2520Quantization%2520and%2520Acceleration%2520for%250A%2520%2520Fully%2520Quantized%2520Vision%2520Transformer%26entry.906535625%3DHuihong%2520Shi%2520and%2520Xin%2520Cheng%2520and%2520Wendong%2520Mao%2520and%2520Zhongfeng%2520Wang%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520excelled%2520in%2520computer%2520vision%2520tasks%2520but%2520are%250Amemory-consuming%2520and%2520computation-intensive%252C%2520challenging%2520their%2520deployment%2520on%250Aresource-constrained%2520devices.%2520To%2520tackle%2520this%2520limitation%252C%2520prior%2520works%2520have%250Aexplored%2520ViT-tailored%2520quantization%2520algorithms%2520but%2520retained%2520floating-point%250Ascaling%2520factors%252C%2520which%2520yield%2520non-negligible%2520re-quantization%2520overhead%252C%2520limiting%250AViTs%2527%2520hardware%2520efficiency%2520and%2520motivating%2520more%2520hardware-friendly%2520solutions.%2520To%250Athis%2520end%252C%2520we%2520propose%2520%255Cemph%257BP%2524%255E2%2524-ViT%257D%252C%2520the%2520first%2520%255Cunderline%257BP%257Dower-of-Two%2520%2528PoT%2529%250A%255Cunderline%257Bp%257Dost-training%2520quantization%2520and%2520acceleration%2520framework%2520to%2520accelerate%250Afully%2520quantized%2520ViTs.%2520Specifically%252C%2520%257Bas%2520for%2520quantization%252C%257D%2520we%2520explore%2520a%250Adedicated%2520quantization%2520scheme%2520to%2520effectively%2520quantize%2520ViTs%2520with%2520PoT%2520scaling%250Afactors%252C%2520thus%2520minimizing%2520the%2520re-quantization%2520overhead.%2520Furthermore%252C%2520we%2520propose%250Acoarse-to-fine%2520automatic%2520mixed-precision%2520quantization%2520to%2520enable%2520better%250Aaccuracy-efficiency%2520trade-offs.%2520%257BIn%2520terms%2520of%2520hardware%252C%257D%2520we%2520develop%2520%257Ba%2520dedicated%250Achunk-based%2520accelerator%257D%2520featuring%2520multiple%2520tailored%2520sub-processors%2520to%250Aindividually%2520handle%2520ViTs%2527%2520different%2520types%2520of%2520operations%252C%2520alleviating%250Areconfigurable%2520overhead.%2520Additionally%252C%2520we%2520design%2520%257Ba%2520tailored%2520row-stationary%250Adataflow%257D%2520to%2520seize%2520the%2520pipeline%2520processing%2520opportunity%2520introduced%2520by%2520our%2520PoT%250Ascaling%2520factors%252C%2520thereby%2520enhancing%2520throughput.%2520Extensive%2520experiments%250Aconsistently%2520validate%2520P%2524%255E2%2524-ViT%2527s%2520effectiveness.%2520%257BParticularly%252C%2520we%2520offer%250Acomparable%2520or%2520even%2520superior%2520quantization%2520performance%2520with%2520PoT%2520scaling%2520factors%250Awhen%2520compared%2520to%2520the%2520counterpart%2520with%2520floating-point%2520scaling%2520factors.%2520Besides%252C%250Awe%2520achieve%2520up%2520to%2520%2524%255Cmathbf%257B10.1%255Ctimes%257D%2524%2520speedup%2520and%2520%2524%255Cmathbf%257B36.8%255Ctimes%257D%2524%2520energy%250Asaving%2520over%2520GPU%2527s%2520Turing%2520Tensor%2520Cores%252C%2520and%2520up%2520to%2520%2524%255Cmathbf%257B1.84%255Ctimes%257D%2524%2520higher%250Acomputation%2520utilization%2520efficiency%2520against%2520SOTA%2520quantization-based%2520ViT%250Aaccelerators.%2520Codes%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/shihuihong214/P2-ViT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P%24%5E2%24-ViT%3A%20Power-of-Two%20Post-Training%20Quantization%20and%20Acceleration%20for%0A%20%20Fully%20Quantized%20Vision%20Transformer&entry.906535625=Huihong%20Shi%20and%20Xin%20Cheng%20and%20Wendong%20Mao%20and%20Zhongfeng%20Wang&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20excelled%20in%20computer%20vision%20tasks%20but%20are%0Amemory-consuming%20and%20computation-intensive%2C%20challenging%20their%20deployment%20on%0Aresource-constrained%20devices.%20To%20tackle%20this%20limitation%2C%20prior%20works%20have%0Aexplored%20ViT-tailored%20quantization%20algorithms%20but%20retained%20floating-point%0Ascaling%20factors%2C%20which%20yield%20non-negligible%20re-quantization%20overhead%2C%20limiting%0AViTs%27%20hardware%20efficiency%20and%20motivating%20more%20hardware-friendly%20solutions.%20To%0Athis%20end%2C%20we%20propose%20%5Cemph%7BP%24%5E2%24-ViT%7D%2C%20the%20first%20%5Cunderline%7BP%7Dower-of-Two%20%28PoT%29%0A%5Cunderline%7Bp%7Dost-training%20quantization%20and%20acceleration%20framework%20to%20accelerate%0Afully%20quantized%20ViTs.%20Specifically%2C%20%7Bas%20for%20quantization%2C%7D%20we%20explore%20a%0Adedicated%20quantization%20scheme%20to%20effectively%20quantize%20ViTs%20with%20PoT%20scaling%0Afactors%2C%20thus%20minimizing%20the%20re-quantization%20overhead.%20Furthermore%2C%20we%20propose%0Acoarse-to-fine%20automatic%20mixed-precision%20quantization%20to%20enable%20better%0Aaccuracy-efficiency%20trade-offs.%20%7BIn%20terms%20of%20hardware%2C%7D%20we%20develop%20%7Ba%20dedicated%0Achunk-based%20accelerator%7D%20featuring%20multiple%20tailored%20sub-processors%20to%0Aindividually%20handle%20ViTs%27%20different%20types%20of%20operations%2C%20alleviating%0Areconfigurable%20overhead.%20Additionally%2C%20we%20design%20%7Ba%20tailored%20row-stationary%0Adataflow%7D%20to%20seize%20the%20pipeline%20processing%20opportunity%20introduced%20by%20our%20PoT%0Ascaling%20factors%2C%20thereby%20enhancing%20throughput.%20Extensive%20experiments%0Aconsistently%20validate%20P%24%5E2%24-ViT%27s%20effectiveness.%20%7BParticularly%2C%20we%20offer%0Acomparable%20or%20even%20superior%20quantization%20performance%20with%20PoT%20scaling%20factors%0Awhen%20compared%20to%20the%20counterpart%20with%20floating-point%20scaling%20factors.%20Besides%2C%0Awe%20achieve%20up%20to%20%24%5Cmathbf%7B10.1%5Ctimes%7D%24%20speedup%20and%20%24%5Cmathbf%7B36.8%5Ctimes%7D%24%20energy%0Asaving%20over%20GPU%27s%20Turing%20Tensor%20Cores%2C%20and%20up%20to%20%24%5Cmathbf%7B1.84%5Ctimes%7D%24%20higher%0Acomputation%20utilization%20efficiency%20against%20SOTA%20quantization-based%20ViT%0Aaccelerators.%20Codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/shihuihong214/P2-ViT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19915v1&entry.124074799=Read"},
{"title": "ParSEL: Parameterized Shape Editing with Language", "author": "Aditya Ganeshan and Ryan Y. Huang and Xianghao Xu and R. Kenny Jones and Daniel Ritchie", "abstract": "  The ability to edit 3D assets from natural language presents a compelling\nparadigm to aid in the democratization of 3D content creation. However, while\nnatural language is often effective at communicating general intent, it is\npoorly suited for specifying precise manipulation. To address this gap, we\nintroduce ParSEL, a system that enables controllable editing of high-quality 3D\nassets from natural language. Given a segmented 3D mesh and an editing request,\nParSEL produces a parameterized editing program. Adjusting the program\nparameters allows users to explore shape variations with a precise control over\nthe magnitudes of edits. To infer editing programs which align with an input\nedit request, we leverage the abilities of large-language models (LLMs).\nHowever, while we find that LLMs excel at identifying initial edit operations,\nthey often fail to infer complete editing programs, and produce outputs that\nviolate shape semantics. To overcome this issue, we introduce Analytical Edit\nPropagation (AEP), an algorithm which extends a seed edit with additional\noperations until a complete editing program has been formed. Unlike prior\nmethods, AEP searches for analytical editing operations compatible with a range\nof possible user edits through the integration of computer algebra systems for\ngeometric analysis. Experimentally we demonstrate ParSEL's effectiveness in\nenabling controllable editing of 3D objects through natural language requests\nover alternative system designs.\n", "link": "http://arxiv.org/abs/2405.20319v1", "date": "2024-05-30", "relevancy": 2.0915, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ParSEL%3A%20Parameterized%20Shape%20Editing%20with%20Language&body=Title%3A%20ParSEL%3A%20Parameterized%20Shape%20Editing%20with%20Language%0AAuthor%3A%20Aditya%20Ganeshan%20and%20Ryan%20Y.%20Huang%20and%20Xianghao%20Xu%20and%20R.%20Kenny%20Jones%20and%20Daniel%20Ritchie%0AAbstract%3A%20%20%20The%20ability%20to%20edit%203D%20assets%20from%20natural%20language%20presents%20a%20compelling%0Aparadigm%20to%20aid%20in%20the%20democratization%20of%203D%20content%20creation.%20However%2C%20while%0Anatural%20language%20is%20often%20effective%20at%20communicating%20general%20intent%2C%20it%20is%0Apoorly%20suited%20for%20specifying%20precise%20manipulation.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20ParSEL%2C%20a%20system%20that%20enables%20controllable%20editing%20of%20high-quality%203D%0Aassets%20from%20natural%20language.%20Given%20a%20segmented%203D%20mesh%20and%20an%20editing%20request%2C%0AParSEL%20produces%20a%20parameterized%20editing%20program.%20Adjusting%20the%20program%0Aparameters%20allows%20users%20to%20explore%20shape%20variations%20with%20a%20precise%20control%20over%0Athe%20magnitudes%20of%20edits.%20To%20infer%20editing%20programs%20which%20align%20with%20an%20input%0Aedit%20request%2C%20we%20leverage%20the%20abilities%20of%20large-language%20models%20%28LLMs%29.%0AHowever%2C%20while%20we%20find%20that%20LLMs%20excel%20at%20identifying%20initial%20edit%20operations%2C%0Athey%20often%20fail%20to%20infer%20complete%20editing%20programs%2C%20and%20produce%20outputs%20that%0Aviolate%20shape%20semantics.%20To%20overcome%20this%20issue%2C%20we%20introduce%20Analytical%20Edit%0APropagation%20%28AEP%29%2C%20an%20algorithm%20which%20extends%20a%20seed%20edit%20with%20additional%0Aoperations%20until%20a%20complete%20editing%20program%20has%20been%20formed.%20Unlike%20prior%0Amethods%2C%20AEP%20searches%20for%20analytical%20editing%20operations%20compatible%20with%20a%20range%0Aof%20possible%20user%20edits%20through%20the%20integration%20of%20computer%20algebra%20systems%20for%0Ageometric%20analysis.%20Experimentally%20we%20demonstrate%20ParSEL%27s%20effectiveness%20in%0Aenabling%20controllable%20editing%20of%203D%20objects%20through%20natural%20language%20requests%0Aover%20alternative%20system%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParSEL%253A%2520Parameterized%2520Shape%2520Editing%2520with%2520Language%26entry.906535625%3DAditya%2520Ganeshan%2520and%2520Ryan%2520Y.%2520Huang%2520and%2520Xianghao%2520Xu%2520and%2520R.%2520Kenny%2520Jones%2520and%2520Daniel%2520Ritchie%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520edit%25203D%2520assets%2520from%2520natural%2520language%2520presents%2520a%2520compelling%250Aparadigm%2520to%2520aid%2520in%2520the%2520democratization%2520of%25203D%2520content%2520creation.%2520However%252C%2520while%250Anatural%2520language%2520is%2520often%2520effective%2520at%2520communicating%2520general%2520intent%252C%2520it%2520is%250Apoorly%2520suited%2520for%2520specifying%2520precise%2520manipulation.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520ParSEL%252C%2520a%2520system%2520that%2520enables%2520controllable%2520editing%2520of%2520high-quality%25203D%250Aassets%2520from%2520natural%2520language.%2520Given%2520a%2520segmented%25203D%2520mesh%2520and%2520an%2520editing%2520request%252C%250AParSEL%2520produces%2520a%2520parameterized%2520editing%2520program.%2520Adjusting%2520the%2520program%250Aparameters%2520allows%2520users%2520to%2520explore%2520shape%2520variations%2520with%2520a%2520precise%2520control%2520over%250Athe%2520magnitudes%2520of%2520edits.%2520To%2520infer%2520editing%2520programs%2520which%2520align%2520with%2520an%2520input%250Aedit%2520request%252C%2520we%2520leverage%2520the%2520abilities%2520of%2520large-language%2520models%2520%2528LLMs%2529.%250AHowever%252C%2520while%2520we%2520find%2520that%2520LLMs%2520excel%2520at%2520identifying%2520initial%2520edit%2520operations%252C%250Athey%2520often%2520fail%2520to%2520infer%2520complete%2520editing%2520programs%252C%2520and%2520produce%2520outputs%2520that%250Aviolate%2520shape%2520semantics.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520introduce%2520Analytical%2520Edit%250APropagation%2520%2528AEP%2529%252C%2520an%2520algorithm%2520which%2520extends%2520a%2520seed%2520edit%2520with%2520additional%250Aoperations%2520until%2520a%2520complete%2520editing%2520program%2520has%2520been%2520formed.%2520Unlike%2520prior%250Amethods%252C%2520AEP%2520searches%2520for%2520analytical%2520editing%2520operations%2520compatible%2520with%2520a%2520range%250Aof%2520possible%2520user%2520edits%2520through%2520the%2520integration%2520of%2520computer%2520algebra%2520systems%2520for%250Ageometric%2520analysis.%2520Experimentally%2520we%2520demonstrate%2520ParSEL%2527s%2520effectiveness%2520in%250Aenabling%2520controllable%2520editing%2520of%25203D%2520objects%2520through%2520natural%2520language%2520requests%250Aover%2520alternative%2520system%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ParSEL%3A%20Parameterized%20Shape%20Editing%20with%20Language&entry.906535625=Aditya%20Ganeshan%20and%20Ryan%20Y.%20Huang%20and%20Xianghao%20Xu%20and%20R.%20Kenny%20Jones%20and%20Daniel%20Ritchie&entry.1292438233=%20%20The%20ability%20to%20edit%203D%20assets%20from%20natural%20language%20presents%20a%20compelling%0Aparadigm%20to%20aid%20in%20the%20democratization%20of%203D%20content%20creation.%20However%2C%20while%0Anatural%20language%20is%20often%20effective%20at%20communicating%20general%20intent%2C%20it%20is%0Apoorly%20suited%20for%20specifying%20precise%20manipulation.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20ParSEL%2C%20a%20system%20that%20enables%20controllable%20editing%20of%20high-quality%203D%0Aassets%20from%20natural%20language.%20Given%20a%20segmented%203D%20mesh%20and%20an%20editing%20request%2C%0AParSEL%20produces%20a%20parameterized%20editing%20program.%20Adjusting%20the%20program%0Aparameters%20allows%20users%20to%20explore%20shape%20variations%20with%20a%20precise%20control%20over%0Athe%20magnitudes%20of%20edits.%20To%20infer%20editing%20programs%20which%20align%20with%20an%20input%0Aedit%20request%2C%20we%20leverage%20the%20abilities%20of%20large-language%20models%20%28LLMs%29.%0AHowever%2C%20while%20we%20find%20that%20LLMs%20excel%20at%20identifying%20initial%20edit%20operations%2C%0Athey%20often%20fail%20to%20infer%20complete%20editing%20programs%2C%20and%20produce%20outputs%20that%0Aviolate%20shape%20semantics.%20To%20overcome%20this%20issue%2C%20we%20introduce%20Analytical%20Edit%0APropagation%20%28AEP%29%2C%20an%20algorithm%20which%20extends%20a%20seed%20edit%20with%20additional%0Aoperations%20until%20a%20complete%20editing%20program%20has%20been%20formed.%20Unlike%20prior%0Amethods%2C%20AEP%20searches%20for%20analytical%20editing%20operations%20compatible%20with%20a%20range%0Aof%20possible%20user%20edits%20through%20the%20integration%20of%20computer%20algebra%20systems%20for%0Ageometric%20analysis.%20Experimentally%20we%20demonstrate%20ParSEL%27s%20effectiveness%20in%0Aenabling%20controllable%20editing%20of%203D%20objects%20through%20natural%20language%20requests%0Aover%20alternative%20system%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20319v1&entry.124074799=Read"},
{"title": "CLIP-Guided Source-Free Object Detection in Aerial Images", "author": "Nanqing Liu and Xun Xu and Yongyi Su and Chengxin Liu and Peiliang Gong and Heng-Chao Li", "abstract": "  Domain adaptation is crucial in aerial imagery, as the visual representation\nof these images can significantly vary based on factors such as geographic\nlocation, time, and weather conditions. Additionally, high-resolution aerial\nimages often require substantial storage space and may not be readily\naccessible to the public. To address these challenges, we propose a novel\nSource-Free Object Detection (SFOD) method. Specifically, our approach begins\nwith a self-training framework, which significantly enhances the performance of\nbaseline methods. To alleviate the noisy labels in self-training, we utilize\nContrastive Language-Image Pre-training (CLIP) to guide the generation of\npseudo-labels, termed CLIP-guided Aggregation (CGA). By leveraging CLIP's\nzero-shot classification capability, we aggregate its scores with the original\npredicted bounding boxes, enabling us to obtain refined scores for the\npseudo-labels. To validate the effectiveness of our method, we constructed two\nnew datasets from different domains based on the DIOR dataset, named DIOR-C and\nDIOR-Cloudy. Experimental results demonstrate that our method outperforms other\ncomparative algorithms. The code is available at\nhttps://github.com/Lans1ng/SFOD-RS.\n", "link": "http://arxiv.org/abs/2401.05168v2", "date": "2024-05-30", "relevancy": 2.0888, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5362}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5129}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-Guided%20Source-Free%20Object%20Detection%20in%20Aerial%20Images&body=Title%3A%20CLIP-Guided%20Source-Free%20Object%20Detection%20in%20Aerial%20Images%0AAuthor%3A%20Nanqing%20Liu%20and%20Xun%20Xu%20and%20Yongyi%20Su%20and%20Chengxin%20Liu%20and%20Peiliang%20Gong%20and%20Heng-Chao%20Li%0AAbstract%3A%20%20%20Domain%20adaptation%20is%20crucial%20in%20aerial%20imagery%2C%20as%20the%20visual%20representation%0Aof%20these%20images%20can%20significantly%20vary%20based%20on%20factors%20such%20as%20geographic%0Alocation%2C%20time%2C%20and%20weather%20conditions.%20Additionally%2C%20high-resolution%20aerial%0Aimages%20often%20require%20substantial%20storage%20space%20and%20may%20not%20be%20readily%0Aaccessible%20to%20the%20public.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0ASource-Free%20Object%20Detection%20%28SFOD%29%20method.%20Specifically%2C%20our%20approach%20begins%0Awith%20a%20self-training%20framework%2C%20which%20significantly%20enhances%20the%20performance%20of%0Abaseline%20methods.%20To%20alleviate%20the%20noisy%20labels%20in%20self-training%2C%20we%20utilize%0AContrastive%20Language-Image%20Pre-training%20%28CLIP%29%20to%20guide%20the%20generation%20of%0Apseudo-labels%2C%20termed%20CLIP-guided%20Aggregation%20%28CGA%29.%20By%20leveraging%20CLIP%27s%0Azero-shot%20classification%20capability%2C%20we%20aggregate%20its%20scores%20with%20the%20original%0Apredicted%20bounding%20boxes%2C%20enabling%20us%20to%20obtain%20refined%20scores%20for%20the%0Apseudo-labels.%20To%20validate%20the%20effectiveness%20of%20our%20method%2C%20we%20constructed%20two%0Anew%20datasets%20from%20different%20domains%20based%20on%20the%20DIOR%20dataset%2C%20named%20DIOR-C%20and%0ADIOR-Cloudy.%20Experimental%20results%20demonstrate%20that%20our%20method%20outperforms%20other%0Acomparative%20algorithms.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Lans1ng/SFOD-RS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05168v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-Guided%2520Source-Free%2520Object%2520Detection%2520in%2520Aerial%2520Images%26entry.906535625%3DNanqing%2520Liu%2520and%2520Xun%2520Xu%2520and%2520Yongyi%2520Su%2520and%2520Chengxin%2520Liu%2520and%2520Peiliang%2520Gong%2520and%2520Heng-Chao%2520Li%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520is%2520crucial%2520in%2520aerial%2520imagery%252C%2520as%2520the%2520visual%2520representation%250Aof%2520these%2520images%2520can%2520significantly%2520vary%2520based%2520on%2520factors%2520such%2520as%2520geographic%250Alocation%252C%2520time%252C%2520and%2520weather%2520conditions.%2520Additionally%252C%2520high-resolution%2520aerial%250Aimages%2520often%2520require%2520substantial%2520storage%2520space%2520and%2520may%2520not%2520be%2520readily%250Aaccessible%2520to%2520the%2520public.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%250ASource-Free%2520Object%2520Detection%2520%2528SFOD%2529%2520method.%2520Specifically%252C%2520our%2520approach%2520begins%250Awith%2520a%2520self-training%2520framework%252C%2520which%2520significantly%2520enhances%2520the%2520performance%2520of%250Abaseline%2520methods.%2520To%2520alleviate%2520the%2520noisy%2520labels%2520in%2520self-training%252C%2520we%2520utilize%250AContrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520to%2520guide%2520the%2520generation%2520of%250Apseudo-labels%252C%2520termed%2520CLIP-guided%2520Aggregation%2520%2528CGA%2529.%2520By%2520leveraging%2520CLIP%2527s%250Azero-shot%2520classification%2520capability%252C%2520we%2520aggregate%2520its%2520scores%2520with%2520the%2520original%250Apredicted%2520bounding%2520boxes%252C%2520enabling%2520us%2520to%2520obtain%2520refined%2520scores%2520for%2520the%250Apseudo-labels.%2520To%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520we%2520constructed%2520two%250Anew%2520datasets%2520from%2520different%2520domains%2520based%2520on%2520the%2520DIOR%2520dataset%252C%2520named%2520DIOR-C%2520and%250ADIOR-Cloudy.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520other%250Acomparative%2520algorithms.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Lans1ng/SFOD-RS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05168v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-Guided%20Source-Free%20Object%20Detection%20in%20Aerial%20Images&entry.906535625=Nanqing%20Liu%20and%20Xun%20Xu%20and%20Yongyi%20Su%20and%20Chengxin%20Liu%20and%20Peiliang%20Gong%20and%20Heng-Chao%20Li&entry.1292438233=%20%20Domain%20adaptation%20is%20crucial%20in%20aerial%20imagery%2C%20as%20the%20visual%20representation%0Aof%20these%20images%20can%20significantly%20vary%20based%20on%20factors%20such%20as%20geographic%0Alocation%2C%20time%2C%20and%20weather%20conditions.%20Additionally%2C%20high-resolution%20aerial%0Aimages%20often%20require%20substantial%20storage%20space%20and%20may%20not%20be%20readily%0Aaccessible%20to%20the%20public.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0ASource-Free%20Object%20Detection%20%28SFOD%29%20method.%20Specifically%2C%20our%20approach%20begins%0Awith%20a%20self-training%20framework%2C%20which%20significantly%20enhances%20the%20performance%20of%0Abaseline%20methods.%20To%20alleviate%20the%20noisy%20labels%20in%20self-training%2C%20we%20utilize%0AContrastive%20Language-Image%20Pre-training%20%28CLIP%29%20to%20guide%20the%20generation%20of%0Apseudo-labels%2C%20termed%20CLIP-guided%20Aggregation%20%28CGA%29.%20By%20leveraging%20CLIP%27s%0Azero-shot%20classification%20capability%2C%20we%20aggregate%20its%20scores%20with%20the%20original%0Apredicted%20bounding%20boxes%2C%20enabling%20us%20to%20obtain%20refined%20scores%20for%20the%0Apseudo-labels.%20To%20validate%20the%20effectiveness%20of%20our%20method%2C%20we%20constructed%20two%0Anew%20datasets%20from%20different%20domains%20based%20on%20the%20DIOR%20dataset%2C%20named%20DIOR-C%20and%0ADIOR-Cloudy.%20Experimental%20results%20demonstrate%20that%20our%20method%20outperforms%20other%0Acomparative%20algorithms.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Lans1ng/SFOD-RS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05168v2&entry.124074799=Read"},
{"title": "On the Last-Iterate Convergence of Shuffling Gradient Methods", "author": "Zijian Liu and Zhengyuan Zhou", "abstract": "  Shuffling gradient methods are widely implemented in practice, particularly\nincluding three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO),\nand Incremental Gradient (IG). Compared to the empirical success, the\ntheoretical guarantee of shuffling gradient methods was not well-understood for\na long time. Until recently, the convergence rates had just been established\nfor the average iterate for convex functions and the last iterate for strongly\nconvex problems (using squared distance as the metric). However, when using the\nfunction value gap as the convergence criterion, existing theories cannot\ninterpret the good performance of the last iterate in different settings (e.g.,\nconstrained optimization). To bridge this gap between practice and theory, we\nprove the first last-iterate convergence rates for shuffling gradient methods\nwith respect to the objective value even without strong convexity. Our new\nresults either (nearly) match the existing last-iterate lower bounds or are as\nfast as the previous best upper bounds for the average iterate.\n", "link": "http://arxiv.org/abs/2403.07723v2", "date": "2024-05-30", "relevancy": 2.0886, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4238}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4163}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Last-Iterate%20Convergence%20of%20Shuffling%20Gradient%20Methods&body=Title%3A%20On%20the%20Last-Iterate%20Convergence%20of%20Shuffling%20Gradient%20Methods%0AAuthor%3A%20Zijian%20Liu%20and%20Zhengyuan%20Zhou%0AAbstract%3A%20%20%20Shuffling%20gradient%20methods%20are%20widely%20implemented%20in%20practice%2C%20particularly%0Aincluding%20three%20popular%20algorithms%3A%20Random%20Reshuffle%20%28RR%29%2C%20Shuffle%20Once%20%28SO%29%2C%0Aand%20Incremental%20Gradient%20%28IG%29.%20Compared%20to%20the%20empirical%20success%2C%20the%0Atheoretical%20guarantee%20of%20shuffling%20gradient%20methods%20was%20not%20well-understood%20for%0Aa%20long%20time.%20Until%20recently%2C%20the%20convergence%20rates%20had%20just%20been%20established%0Afor%20the%20average%20iterate%20for%20convex%20functions%20and%20the%20last%20iterate%20for%20strongly%0Aconvex%20problems%20%28using%20squared%20distance%20as%20the%20metric%29.%20However%2C%20when%20using%20the%0Afunction%20value%20gap%20as%20the%20convergence%20criterion%2C%20existing%20theories%20cannot%0Ainterpret%20the%20good%20performance%20of%20the%20last%20iterate%20in%20different%20settings%20%28e.g.%2C%0Aconstrained%20optimization%29.%20To%20bridge%20this%20gap%20between%20practice%20and%20theory%2C%20we%0Aprove%20the%20first%20last-iterate%20convergence%20rates%20for%20shuffling%20gradient%20methods%0Awith%20respect%20to%20the%20objective%20value%20even%20without%20strong%20convexity.%20Our%20new%0Aresults%20either%20%28nearly%29%20match%20the%20existing%20last-iterate%20lower%20bounds%20or%20are%20as%0Afast%20as%20the%20previous%20best%20upper%20bounds%20for%20the%20average%20iterate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Last-Iterate%2520Convergence%2520of%2520Shuffling%2520Gradient%2520Methods%26entry.906535625%3DZijian%2520Liu%2520and%2520Zhengyuan%2520Zhou%26entry.1292438233%3D%2520%2520Shuffling%2520gradient%2520methods%2520are%2520widely%2520implemented%2520in%2520practice%252C%2520particularly%250Aincluding%2520three%2520popular%2520algorithms%253A%2520Random%2520Reshuffle%2520%2528RR%2529%252C%2520Shuffle%2520Once%2520%2528SO%2529%252C%250Aand%2520Incremental%2520Gradient%2520%2528IG%2529.%2520Compared%2520to%2520the%2520empirical%2520success%252C%2520the%250Atheoretical%2520guarantee%2520of%2520shuffling%2520gradient%2520methods%2520was%2520not%2520well-understood%2520for%250Aa%2520long%2520time.%2520Until%2520recently%252C%2520the%2520convergence%2520rates%2520had%2520just%2520been%2520established%250Afor%2520the%2520average%2520iterate%2520for%2520convex%2520functions%2520and%2520the%2520last%2520iterate%2520for%2520strongly%250Aconvex%2520problems%2520%2528using%2520squared%2520distance%2520as%2520the%2520metric%2529.%2520However%252C%2520when%2520using%2520the%250Afunction%2520value%2520gap%2520as%2520the%2520convergence%2520criterion%252C%2520existing%2520theories%2520cannot%250Ainterpret%2520the%2520good%2520performance%2520of%2520the%2520last%2520iterate%2520in%2520different%2520settings%2520%2528e.g.%252C%250Aconstrained%2520optimization%2529.%2520To%2520bridge%2520this%2520gap%2520between%2520practice%2520and%2520theory%252C%2520we%250Aprove%2520the%2520first%2520last-iterate%2520convergence%2520rates%2520for%2520shuffling%2520gradient%2520methods%250Awith%2520respect%2520to%2520the%2520objective%2520value%2520even%2520without%2520strong%2520convexity.%2520Our%2520new%250Aresults%2520either%2520%2528nearly%2529%2520match%2520the%2520existing%2520last-iterate%2520lower%2520bounds%2520or%2520are%2520as%250Afast%2520as%2520the%2520previous%2520best%2520upper%2520bounds%2520for%2520the%2520average%2520iterate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Last-Iterate%20Convergence%20of%20Shuffling%20Gradient%20Methods&entry.906535625=Zijian%20Liu%20and%20Zhengyuan%20Zhou&entry.1292438233=%20%20Shuffling%20gradient%20methods%20are%20widely%20implemented%20in%20practice%2C%20particularly%0Aincluding%20three%20popular%20algorithms%3A%20Random%20Reshuffle%20%28RR%29%2C%20Shuffle%20Once%20%28SO%29%2C%0Aand%20Incremental%20Gradient%20%28IG%29.%20Compared%20to%20the%20empirical%20success%2C%20the%0Atheoretical%20guarantee%20of%20shuffling%20gradient%20methods%20was%20not%20well-understood%20for%0Aa%20long%20time.%20Until%20recently%2C%20the%20convergence%20rates%20had%20just%20been%20established%0Afor%20the%20average%20iterate%20for%20convex%20functions%20and%20the%20last%20iterate%20for%20strongly%0Aconvex%20problems%20%28using%20squared%20distance%20as%20the%20metric%29.%20However%2C%20when%20using%20the%0Afunction%20value%20gap%20as%20the%20convergence%20criterion%2C%20existing%20theories%20cannot%0Ainterpret%20the%20good%20performance%20of%20the%20last%20iterate%20in%20different%20settings%20%28e.g.%2C%0Aconstrained%20optimization%29.%20To%20bridge%20this%20gap%20between%20practice%20and%20theory%2C%20we%0Aprove%20the%20first%20last-iterate%20convergence%20rates%20for%20shuffling%20gradient%20methods%0Awith%20respect%20to%20the%20objective%20value%20even%20without%20strong%20convexity.%20Our%20new%0Aresults%20either%20%28nearly%29%20match%20the%20existing%20last-iterate%20lower%20bounds%20or%20are%20as%0Afast%20as%20the%20previous%20best%20upper%20bounds%20for%20the%20average%20iterate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07723v2&entry.124074799=Read"},
{"title": "Open-Set Domain Adaptation for Semantic Segmentation", "author": "Seun-An Choe and Ah-Hyung Shin and Keon-Hee Park and Jinwoo Choi and Gyeong-Moon Park", "abstract": "  Unsupervised domain adaptation (UDA) for semantic segmentation aims to\ntransfer the pixel-wise knowledge from the labeled source domain to the\nunlabeled target domain. However, current UDA methods typically assume a shared\nlabel space between source and target, limiting their applicability in\nreal-world scenarios where novel categories may emerge in the target domain. In\nthis paper, we introduce Open-Set Domain Adaptation for Semantic Segmentation\n(OSDA-SS) for the first time, where the target domain includes unknown classes.\nWe identify two major problems in the OSDA-SS scenario as follows: 1) the\nexisting UDA methods struggle to predict the exact boundary of the unknown\nclasses, and 2) they fail to accurately predict the shape of the unknown\nclasses. To address these issues, we propose Boundary and Unknown Shape-Aware\nopen-set domain adaptation, coined BUS. Our BUS can accurately discern the\nboundaries between known and unknown classes in a contrastive manner using a\nnovel dilation-erosion-based contrastive loss. In addition, we propose\nOpenReMix, a new domain mixing augmentation method that guides our model to\neffectively learn domain and size-invariant features for improving the shape\ndetection of the known and unknown classes. Through extensive experiments, we\ndemonstrate that our proposed BUS effectively detects unknown classes in the\nchallenging OSDA-SS scenario compared to the previous methods by a large\nmargin. The code is available at https://github.com/KHU-AGI/BUS.\n", "link": "http://arxiv.org/abs/2405.19899v1", "date": "2024-05-30", "relevancy": 2.0865, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5254}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5227}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Set%20Domain%20Adaptation%20for%20Semantic%20Segmentation&body=Title%3A%20Open-Set%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0AAuthor%3A%20Seun-An%20Choe%20and%20Ah-Hyung%20Shin%20and%20Keon-Hee%20Park%20and%20Jinwoo%20Choi%20and%20Gyeong-Moon%20Park%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20for%20semantic%20segmentation%20aims%20to%0Atransfer%20the%20pixel-wise%20knowledge%20from%20the%20labeled%20source%20domain%20to%20the%0Aunlabeled%20target%20domain.%20However%2C%20current%20UDA%20methods%20typically%20assume%20a%20shared%0Alabel%20space%20between%20source%20and%20target%2C%20limiting%20their%20applicability%20in%0Areal-world%20scenarios%20where%20novel%20categories%20may%20emerge%20in%20the%20target%20domain.%20In%0Athis%20paper%2C%20we%20introduce%20Open-Set%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0A%28OSDA-SS%29%20for%20the%20first%20time%2C%20where%20the%20target%20domain%20includes%20unknown%20classes.%0AWe%20identify%20two%20major%20problems%20in%20the%20OSDA-SS%20scenario%20as%20follows%3A%201%29%20the%0Aexisting%20UDA%20methods%20struggle%20to%20predict%20the%20exact%20boundary%20of%20the%20unknown%0Aclasses%2C%20and%202%29%20they%20fail%20to%20accurately%20predict%20the%20shape%20of%20the%20unknown%0Aclasses.%20To%20address%20these%20issues%2C%20we%20propose%20Boundary%20and%20Unknown%20Shape-Aware%0Aopen-set%20domain%20adaptation%2C%20coined%20BUS.%20Our%20BUS%20can%20accurately%20discern%20the%0Aboundaries%20between%20known%20and%20unknown%20classes%20in%20a%20contrastive%20manner%20using%20a%0Anovel%20dilation-erosion-based%20contrastive%20loss.%20In%20addition%2C%20we%20propose%0AOpenReMix%2C%20a%20new%20domain%20mixing%20augmentation%20method%20that%20guides%20our%20model%20to%0Aeffectively%20learn%20domain%20and%20size-invariant%20features%20for%20improving%20the%20shape%0Adetection%20of%20the%20known%20and%20unknown%20classes.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20proposed%20BUS%20effectively%20detects%20unknown%20classes%20in%20the%0Achallenging%20OSDA-SS%20scenario%20compared%20to%20the%20previous%20methods%20by%20a%20large%0Amargin.%20The%20code%20is%20available%20at%20https%3A//github.com/KHU-AGI/BUS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Set%2520Domain%2520Adaptation%2520for%2520Semantic%2520Segmentation%26entry.906535625%3DSeun-An%2520Choe%2520and%2520Ah-Hyung%2520Shin%2520and%2520Keon-Hee%2520Park%2520and%2520Jinwoo%2520Choi%2520and%2520Gyeong-Moon%2520Park%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520for%2520semantic%2520segmentation%2520aims%2520to%250Atransfer%2520the%2520pixel-wise%2520knowledge%2520from%2520the%2520labeled%2520source%2520domain%2520to%2520the%250Aunlabeled%2520target%2520domain.%2520However%252C%2520current%2520UDA%2520methods%2520typically%2520assume%2520a%2520shared%250Alabel%2520space%2520between%2520source%2520and%2520target%252C%2520limiting%2520their%2520applicability%2520in%250Areal-world%2520scenarios%2520where%2520novel%2520categories%2520may%2520emerge%2520in%2520the%2520target%2520domain.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520Open-Set%2520Domain%2520Adaptation%2520for%2520Semantic%2520Segmentation%250A%2528OSDA-SS%2529%2520for%2520the%2520first%2520time%252C%2520where%2520the%2520target%2520domain%2520includes%2520unknown%2520classes.%250AWe%2520identify%2520two%2520major%2520problems%2520in%2520the%2520OSDA-SS%2520scenario%2520as%2520follows%253A%25201%2529%2520the%250Aexisting%2520UDA%2520methods%2520struggle%2520to%2520predict%2520the%2520exact%2520boundary%2520of%2520the%2520unknown%250Aclasses%252C%2520and%25202%2529%2520they%2520fail%2520to%2520accurately%2520predict%2520the%2520shape%2520of%2520the%2520unknown%250Aclasses.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Boundary%2520and%2520Unknown%2520Shape-Aware%250Aopen-set%2520domain%2520adaptation%252C%2520coined%2520BUS.%2520Our%2520BUS%2520can%2520accurately%2520discern%2520the%250Aboundaries%2520between%2520known%2520and%2520unknown%2520classes%2520in%2520a%2520contrastive%2520manner%2520using%2520a%250Anovel%2520dilation-erosion-based%2520contrastive%2520loss.%2520In%2520addition%252C%2520we%2520propose%250AOpenReMix%252C%2520a%2520new%2520domain%2520mixing%2520augmentation%2520method%2520that%2520guides%2520our%2520model%2520to%250Aeffectively%2520learn%2520domain%2520and%2520size-invariant%2520features%2520for%2520improving%2520the%2520shape%250Adetection%2520of%2520the%2520known%2520and%2520unknown%2520classes.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520proposed%2520BUS%2520effectively%2520detects%2520unknown%2520classes%2520in%2520the%250Achallenging%2520OSDA-SS%2520scenario%2520compared%2520to%2520the%2520previous%2520methods%2520by%2520a%2520large%250Amargin.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/KHU-AGI/BUS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Set%20Domain%20Adaptation%20for%20Semantic%20Segmentation&entry.906535625=Seun-An%20Choe%20and%20Ah-Hyung%20Shin%20and%20Keon-Hee%20Park%20and%20Jinwoo%20Choi%20and%20Gyeong-Moon%20Park&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20for%20semantic%20segmentation%20aims%20to%0Atransfer%20the%20pixel-wise%20knowledge%20from%20the%20labeled%20source%20domain%20to%20the%0Aunlabeled%20target%20domain.%20However%2C%20current%20UDA%20methods%20typically%20assume%20a%20shared%0Alabel%20space%20between%20source%20and%20target%2C%20limiting%20their%20applicability%20in%0Areal-world%20scenarios%20where%20novel%20categories%20may%20emerge%20in%20the%20target%20domain.%20In%0Athis%20paper%2C%20we%20introduce%20Open-Set%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0A%28OSDA-SS%29%20for%20the%20first%20time%2C%20where%20the%20target%20domain%20includes%20unknown%20classes.%0AWe%20identify%20two%20major%20problems%20in%20the%20OSDA-SS%20scenario%20as%20follows%3A%201%29%20the%0Aexisting%20UDA%20methods%20struggle%20to%20predict%20the%20exact%20boundary%20of%20the%20unknown%0Aclasses%2C%20and%202%29%20they%20fail%20to%20accurately%20predict%20the%20shape%20of%20the%20unknown%0Aclasses.%20To%20address%20these%20issues%2C%20we%20propose%20Boundary%20and%20Unknown%20Shape-Aware%0Aopen-set%20domain%20adaptation%2C%20coined%20BUS.%20Our%20BUS%20can%20accurately%20discern%20the%0Aboundaries%20between%20known%20and%20unknown%20classes%20in%20a%20contrastive%20manner%20using%20a%0Anovel%20dilation-erosion-based%20contrastive%20loss.%20In%20addition%2C%20we%20propose%0AOpenReMix%2C%20a%20new%20domain%20mixing%20augmentation%20method%20that%20guides%20our%20model%20to%0Aeffectively%20learn%20domain%20and%20size-invariant%20features%20for%20improving%20the%20shape%0Adetection%20of%20the%20known%20and%20unknown%20classes.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20proposed%20BUS%20effectively%20detects%20unknown%20classes%20in%20the%0Achallenging%20OSDA-SS%20scenario%20compared%20to%20the%20previous%20methods%20by%20a%20large%0Amargin.%20The%20code%20is%20available%20at%20https%3A//github.com/KHU-AGI/BUS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19899v1&entry.124074799=Read"},
{"title": "Occam Gradient Descent", "author": "B. N. Kausik", "abstract": "  Deep learning neural network models must be large enough to adapt to their\nproblem domain, while small enough to avoid overfitting training data during\ngradient descent. To balance these competing demands, overprovisioned deep\nlearning models such as transformers are trained for a single epoch on large\ndata sets, and hence inefficient with both computing resources and training\ndata. In response to these inefficiencies, we exploit learning theory to derive\nOccam Gradient Descent, an algorithm that interleaves adaptive reduction of\nmodel size to minimize generalization error, with gradient descent on model\nweights to minimize fitting error. In contrast, traditional gradient descent\ngreedily minimizes fitting error without regard to generalization error. Our\nalgorithm simultaneously descends the space of weights and topological size of\nany neural network without modification, and is effective in our experiments in\noutperforming traditional gradient descent with or without post-train pruning\nin accuracy, compute and model compression.\n", "link": "http://arxiv.org/abs/2405.20194v1", "date": "2024-05-30", "relevancy": 2.0858, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5413}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.525}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occam%20Gradient%20Descent&body=Title%3A%20Occam%20Gradient%20Descent%0AAuthor%3A%20B.%20N.%20Kausik%0AAbstract%3A%20%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification%2C%20and%20is%20effective%20in%20our%20experiments%20in%0Aoutperforming%20traditional%20gradient%20descent%20with%20or%20without%20post-train%20pruning%0Ain%20accuracy%2C%20compute%20and%20model%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccam%2520Gradient%2520Descent%26entry.906535625%3DB.%2520N.%2520Kausik%26entry.1292438233%3D%2520%2520Deep%2520learning%2520neural%2520network%2520models%2520must%2520be%2520large%2520enough%2520to%2520adapt%2520to%2520their%250Aproblem%2520domain%252C%2520while%2520small%2520enough%2520to%2520avoid%2520overfitting%2520training%2520data%2520during%250Agradient%2520descent.%2520To%2520balance%2520these%2520competing%2520demands%252C%2520overprovisioned%2520deep%250Alearning%2520models%2520such%2520as%2520transformers%2520are%2520trained%2520for%2520a%2520single%2520epoch%2520on%2520large%250Adata%2520sets%252C%2520and%2520hence%2520inefficient%2520with%2520both%2520computing%2520resources%2520and%2520training%250Adata.%2520In%2520response%2520to%2520these%2520inefficiencies%252C%2520we%2520exploit%2520learning%2520theory%2520to%2520derive%250AOccam%2520Gradient%2520Descent%252C%2520an%2520algorithm%2520that%2520interleaves%2520adaptive%2520reduction%2520of%250Amodel%2520size%2520to%2520minimize%2520generalization%2520error%252C%2520with%2520gradient%2520descent%2520on%2520model%250Aweights%2520to%2520minimize%2520fitting%2520error.%2520In%2520contrast%252C%2520traditional%2520gradient%2520descent%250Agreedily%2520minimizes%2520fitting%2520error%2520without%2520regard%2520to%2520generalization%2520error.%2520Our%250Aalgorithm%2520simultaneously%2520descends%2520the%2520space%2520of%2520weights%2520and%2520topological%2520size%2520of%250Aany%2520neural%2520network%2520without%2520modification%252C%2520and%2520is%2520effective%2520in%2520our%2520experiments%2520in%250Aoutperforming%2520traditional%2520gradient%2520descent%2520with%2520or%2520without%2520post-train%2520pruning%250Ain%2520accuracy%252C%2520compute%2520and%2520model%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occam%20Gradient%20Descent&entry.906535625=B.%20N.%20Kausik&entry.1292438233=%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification%2C%20and%20is%20effective%20in%20our%20experiments%20in%0Aoutperforming%20traditional%20gradient%20descent%20with%20or%20without%20post-train%20pruning%0Ain%20accuracy%2C%20compute%20and%20model%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20194v1&entry.124074799=Read"},
{"title": "Visual Perception by Large Language Model's Weights", "author": "Feipeng Ma and Hongwei Xue and Guangting Wang and Yizhou Zhou and Fengyun Rao and Shilin Yan and Yueyi Zhang and Siying Wu and Mike Zheng Shou and Xiaoyan Sun", "abstract": "  Existing Multimodal Large Language Models (MLLMs) follow the paradigm that\nperceives visual information by aligning visual features with the input space\nof Large Language Models (LLMs), and concatenating visual tokens with text\ntokens to form a unified sequence input for LLMs. These methods demonstrate\npromising results on various vision-language tasks but are limited by the high\ncomputational effort due to the extended input sequence resulting from the\ninvolvement of visual tokens. In this paper, instead of input space alignment,\nwe propose a novel parameter space alignment paradigm that represents visual\ninformation as model weights. For each input image, we use a vision encoder to\nextract visual features, convert features into perceptual weights, and merge\nthe perceptual weights with LLM's weights. In this way, the input of LLM does\nnot require visual tokens, which reduces the length of the input sequence and\ngreatly improves efficiency. Following this paradigm, we propose VLoRA with the\nperceptual weights generator. The perceptual weights generator is designed to\nconvert visual features to perceptual weights with low-rank property,\nexhibiting a form similar to LoRA. The experimental results show that our VLoRA\nachieves comparable performance on various benchmarks for MLLMs, while\nsignificantly reducing the computational costs for both training and inference.\nThe code and models will be made open-source.\n", "link": "http://arxiv.org/abs/2405.20339v1", "date": "2024-05-30", "relevancy": 2.0683, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5281}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5153}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Perception%20by%20Large%20Language%20Model%27s%20Weights&body=Title%3A%20Visual%20Perception%20by%20Large%20Language%20Model%27s%20Weights%0AAuthor%3A%20Feipeng%20Ma%20and%20Hongwei%20Xue%20and%20Guangting%20Wang%20and%20Yizhou%20Zhou%20and%20Fengyun%20Rao%20and%20Shilin%20Yan%20and%20Yueyi%20Zhang%20and%20Siying%20Wu%20and%20Mike%20Zheng%20Shou%20and%20Xiaoyan%20Sun%0AAbstract%3A%20%20%20Existing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20follow%20the%20paradigm%20that%0Aperceives%20visual%20information%20by%20aligning%20visual%20features%20with%20the%20input%20space%0Aof%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20concatenating%20visual%20tokens%20with%20text%0Atokens%20to%20form%20a%20unified%20sequence%20input%20for%20LLMs.%20These%20methods%20demonstrate%0Apromising%20results%20on%20various%20vision-language%20tasks%20but%20are%20limited%20by%20the%20high%0Acomputational%20effort%20due%20to%20the%20extended%20input%20sequence%20resulting%20from%20the%0Ainvolvement%20of%20visual%20tokens.%20In%20this%20paper%2C%20instead%20of%20input%20space%20alignment%2C%0Awe%20propose%20a%20novel%20parameter%20space%20alignment%20paradigm%20that%20represents%20visual%0Ainformation%20as%20model%20weights.%20For%20each%20input%20image%2C%20we%20use%20a%20vision%20encoder%20to%0Aextract%20visual%20features%2C%20convert%20features%20into%20perceptual%20weights%2C%20and%20merge%0Athe%20perceptual%20weights%20with%20LLM%27s%20weights.%20In%20this%20way%2C%20the%20input%20of%20LLM%20does%0Anot%20require%20visual%20tokens%2C%20which%20reduces%20the%20length%20of%20the%20input%20sequence%20and%0Agreatly%20improves%20efficiency.%20Following%20this%20paradigm%2C%20we%20propose%20VLoRA%20with%20the%0Aperceptual%20weights%20generator.%20The%20perceptual%20weights%20generator%20is%20designed%20to%0Aconvert%20visual%20features%20to%20perceptual%20weights%20with%20low-rank%20property%2C%0Aexhibiting%20a%20form%20similar%20to%20LoRA.%20The%20experimental%20results%20show%20that%20our%20VLoRA%0Aachieves%20comparable%20performance%20on%20various%20benchmarks%20for%20MLLMs%2C%20while%0Asignificantly%20reducing%20the%20computational%20costs%20for%20both%20training%20and%20inference.%0AThe%20code%20and%20models%20will%20be%20made%20open-source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Perception%2520by%2520Large%2520Language%2520Model%2527s%2520Weights%26entry.906535625%3DFeipeng%2520Ma%2520and%2520Hongwei%2520Xue%2520and%2520Guangting%2520Wang%2520and%2520Yizhou%2520Zhou%2520and%2520Fengyun%2520Rao%2520and%2520Shilin%2520Yan%2520and%2520Yueyi%2520Zhang%2520and%2520Siying%2520Wu%2520and%2520Mike%2520Zheng%2520Shou%2520and%2520Xiaoyan%2520Sun%26entry.1292438233%3D%2520%2520Existing%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520follow%2520the%2520paradigm%2520that%250Aperceives%2520visual%2520information%2520by%2520aligning%2520visual%2520features%2520with%2520the%2520input%2520space%250Aof%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520and%2520concatenating%2520visual%2520tokens%2520with%2520text%250Atokens%2520to%2520form%2520a%2520unified%2520sequence%2520input%2520for%2520LLMs.%2520These%2520methods%2520demonstrate%250Apromising%2520results%2520on%2520various%2520vision-language%2520tasks%2520but%2520are%2520limited%2520by%2520the%2520high%250Acomputational%2520effort%2520due%2520to%2520the%2520extended%2520input%2520sequence%2520resulting%2520from%2520the%250Ainvolvement%2520of%2520visual%2520tokens.%2520In%2520this%2520paper%252C%2520instead%2520of%2520input%2520space%2520alignment%252C%250Awe%2520propose%2520a%2520novel%2520parameter%2520space%2520alignment%2520paradigm%2520that%2520represents%2520visual%250Ainformation%2520as%2520model%2520weights.%2520For%2520each%2520input%2520image%252C%2520we%2520use%2520a%2520vision%2520encoder%2520to%250Aextract%2520visual%2520features%252C%2520convert%2520features%2520into%2520perceptual%2520weights%252C%2520and%2520merge%250Athe%2520perceptual%2520weights%2520with%2520LLM%2527s%2520weights.%2520In%2520this%2520way%252C%2520the%2520input%2520of%2520LLM%2520does%250Anot%2520require%2520visual%2520tokens%252C%2520which%2520reduces%2520the%2520length%2520of%2520the%2520input%2520sequence%2520and%250Agreatly%2520improves%2520efficiency.%2520Following%2520this%2520paradigm%252C%2520we%2520propose%2520VLoRA%2520with%2520the%250Aperceptual%2520weights%2520generator.%2520The%2520perceptual%2520weights%2520generator%2520is%2520designed%2520to%250Aconvert%2520visual%2520features%2520to%2520perceptual%2520weights%2520with%2520low-rank%2520property%252C%250Aexhibiting%2520a%2520form%2520similar%2520to%2520LoRA.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520VLoRA%250Aachieves%2520comparable%2520performance%2520on%2520various%2520benchmarks%2520for%2520MLLMs%252C%2520while%250Asignificantly%2520reducing%2520the%2520computational%2520costs%2520for%2520both%2520training%2520and%2520inference.%250AThe%2520code%2520and%2520models%2520will%2520be%2520made%2520open-source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Perception%20by%20Large%20Language%20Model%27s%20Weights&entry.906535625=Feipeng%20Ma%20and%20Hongwei%20Xue%20and%20Guangting%20Wang%20and%20Yizhou%20Zhou%20and%20Fengyun%20Rao%20and%20Shilin%20Yan%20and%20Yueyi%20Zhang%20and%20Siying%20Wu%20and%20Mike%20Zheng%20Shou%20and%20Xiaoyan%20Sun&entry.1292438233=%20%20Existing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20follow%20the%20paradigm%20that%0Aperceives%20visual%20information%20by%20aligning%20visual%20features%20with%20the%20input%20space%0Aof%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20concatenating%20visual%20tokens%20with%20text%0Atokens%20to%20form%20a%20unified%20sequence%20input%20for%20LLMs.%20These%20methods%20demonstrate%0Apromising%20results%20on%20various%20vision-language%20tasks%20but%20are%20limited%20by%20the%20high%0Acomputational%20effort%20due%20to%20the%20extended%20input%20sequence%20resulting%20from%20the%0Ainvolvement%20of%20visual%20tokens.%20In%20this%20paper%2C%20instead%20of%20input%20space%20alignment%2C%0Awe%20propose%20a%20novel%20parameter%20space%20alignment%20paradigm%20that%20represents%20visual%0Ainformation%20as%20model%20weights.%20For%20each%20input%20image%2C%20we%20use%20a%20vision%20encoder%20to%0Aextract%20visual%20features%2C%20convert%20features%20into%20perceptual%20weights%2C%20and%20merge%0Athe%20perceptual%20weights%20with%20LLM%27s%20weights.%20In%20this%20way%2C%20the%20input%20of%20LLM%20does%0Anot%20require%20visual%20tokens%2C%20which%20reduces%20the%20length%20of%20the%20input%20sequence%20and%0Agreatly%20improves%20efficiency.%20Following%20this%20paradigm%2C%20we%20propose%20VLoRA%20with%20the%0Aperceptual%20weights%20generator.%20The%20perceptual%20weights%20generator%20is%20designed%20to%0Aconvert%20visual%20features%20to%20perceptual%20weights%20with%20low-rank%20property%2C%0Aexhibiting%20a%20form%20similar%20to%20LoRA.%20The%20experimental%20results%20show%20that%20our%20VLoRA%0Aachieves%20comparable%20performance%20on%20various%20benchmarks%20for%20MLLMs%2C%20while%0Asignificantly%20reducing%20the%20computational%20costs%20for%20both%20training%20and%20inference.%0AThe%20code%20and%20models%20will%20be%20made%20open-source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20339v1&entry.124074799=Read"},
{"title": "GasTrace: Detecting Sandwich Attack Malicious Accounts in Ethereum", "author": "Zekai Liu and Xiaoqi Li and Hongli Peng and Wenkai Li", "abstract": "  The openness and transparency of Ethereum transaction data make it easy to be\nexploited by any entities, executing malicious attacks. The sandwich attack\nmanipulates the Automated Market Maker (AMM) mechanism, profiting from\nmanipulating the market price through front or after-running transactions. To\nidentify and prevent sandwich attacks, we propose a cascade classification\nframework GasTrace. GasTrace analyzes various transaction features to detect\nmalicious accounts, notably through the analysis and modeling of Gas features.\nIn the initial classification, we utilize the Support Vector Machine (SVM) with\nthe Radial Basis Function (RBF) kernel to generate the predicted probabilities\nof accounts, further constructing a detailed transaction network. Subsequently,\nthe behavior features are captured by the Graph Attention Network (GAT)\ntechnique in the second classification. Through cascade classification,\nGasTrace can analyze and classify the sandwich attacks. Our experimental\nresults demonstrate that GasTrace achieves a remarkable detection and\ngeneration capability, performing an accuracy of 96.73\\% and an F1 score of\n95.71\\% for identifying sandwich attack accounts.\n", "link": "http://arxiv.org/abs/2405.19971v1", "date": "2024-05-30", "relevancy": 2.0428, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4077}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GasTrace%3A%20Detecting%20Sandwich%20Attack%20Malicious%20Accounts%20in%20Ethereum&body=Title%3A%20GasTrace%3A%20Detecting%20Sandwich%20Attack%20Malicious%20Accounts%20in%20Ethereum%0AAuthor%3A%20Zekai%20Liu%20and%20Xiaoqi%20Li%20and%20Hongli%20Peng%20and%20Wenkai%20Li%0AAbstract%3A%20%20%20The%20openness%20and%20transparency%20of%20Ethereum%20transaction%20data%20make%20it%20easy%20to%20be%0Aexploited%20by%20any%20entities%2C%20executing%20malicious%20attacks.%20The%20sandwich%20attack%0Amanipulates%20the%20Automated%20Market%20Maker%20%28AMM%29%20mechanism%2C%20profiting%20from%0Amanipulating%20the%20market%20price%20through%20front%20or%20after-running%20transactions.%20To%0Aidentify%20and%20prevent%20sandwich%20attacks%2C%20we%20propose%20a%20cascade%20classification%0Aframework%20GasTrace.%20GasTrace%20analyzes%20various%20transaction%20features%20to%20detect%0Amalicious%20accounts%2C%20notably%20through%20the%20analysis%20and%20modeling%20of%20Gas%20features.%0AIn%20the%20initial%20classification%2C%20we%20utilize%20the%20Support%20Vector%20Machine%20%28SVM%29%20with%0Athe%20Radial%20Basis%20Function%20%28RBF%29%20kernel%20to%20generate%20the%20predicted%20probabilities%0Aof%20accounts%2C%20further%20constructing%20a%20detailed%20transaction%20network.%20Subsequently%2C%0Athe%20behavior%20features%20are%20captured%20by%20the%20Graph%20Attention%20Network%20%28GAT%29%0Atechnique%20in%20the%20second%20classification.%20Through%20cascade%20classification%2C%0AGasTrace%20can%20analyze%20and%20classify%20the%20sandwich%20attacks.%20Our%20experimental%0Aresults%20demonstrate%20that%20GasTrace%20achieves%20a%20remarkable%20detection%20and%0Ageneration%20capability%2C%20performing%20an%20accuracy%20of%2096.73%5C%25%20and%20an%20F1%20score%20of%0A95.71%5C%25%20for%20identifying%20sandwich%20attack%20accounts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGasTrace%253A%2520Detecting%2520Sandwich%2520Attack%2520Malicious%2520Accounts%2520in%2520Ethereum%26entry.906535625%3DZekai%2520Liu%2520and%2520Xiaoqi%2520Li%2520and%2520Hongli%2520Peng%2520and%2520Wenkai%2520Li%26entry.1292438233%3D%2520%2520The%2520openness%2520and%2520transparency%2520of%2520Ethereum%2520transaction%2520data%2520make%2520it%2520easy%2520to%2520be%250Aexploited%2520by%2520any%2520entities%252C%2520executing%2520malicious%2520attacks.%2520The%2520sandwich%2520attack%250Amanipulates%2520the%2520Automated%2520Market%2520Maker%2520%2528AMM%2529%2520mechanism%252C%2520profiting%2520from%250Amanipulating%2520the%2520market%2520price%2520through%2520front%2520or%2520after-running%2520transactions.%2520To%250Aidentify%2520and%2520prevent%2520sandwich%2520attacks%252C%2520we%2520propose%2520a%2520cascade%2520classification%250Aframework%2520GasTrace.%2520GasTrace%2520analyzes%2520various%2520transaction%2520features%2520to%2520detect%250Amalicious%2520accounts%252C%2520notably%2520through%2520the%2520analysis%2520and%2520modeling%2520of%2520Gas%2520features.%250AIn%2520the%2520initial%2520classification%252C%2520we%2520utilize%2520the%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%2520with%250Athe%2520Radial%2520Basis%2520Function%2520%2528RBF%2529%2520kernel%2520to%2520generate%2520the%2520predicted%2520probabilities%250Aof%2520accounts%252C%2520further%2520constructing%2520a%2520detailed%2520transaction%2520network.%2520Subsequently%252C%250Athe%2520behavior%2520features%2520are%2520captured%2520by%2520the%2520Graph%2520Attention%2520Network%2520%2528GAT%2529%250Atechnique%2520in%2520the%2520second%2520classification.%2520Through%2520cascade%2520classification%252C%250AGasTrace%2520can%2520analyze%2520and%2520classify%2520the%2520sandwich%2520attacks.%2520Our%2520experimental%250Aresults%2520demonstrate%2520that%2520GasTrace%2520achieves%2520a%2520remarkable%2520detection%2520and%250Ageneration%2520capability%252C%2520performing%2520an%2520accuracy%2520of%252096.73%255C%2525%2520and%2520an%2520F1%2520score%2520of%250A95.71%255C%2525%2520for%2520identifying%2520sandwich%2520attack%2520accounts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GasTrace%3A%20Detecting%20Sandwich%20Attack%20Malicious%20Accounts%20in%20Ethereum&entry.906535625=Zekai%20Liu%20and%20Xiaoqi%20Li%20and%20Hongli%20Peng%20and%20Wenkai%20Li&entry.1292438233=%20%20The%20openness%20and%20transparency%20of%20Ethereum%20transaction%20data%20make%20it%20easy%20to%20be%0Aexploited%20by%20any%20entities%2C%20executing%20malicious%20attacks.%20The%20sandwich%20attack%0Amanipulates%20the%20Automated%20Market%20Maker%20%28AMM%29%20mechanism%2C%20profiting%20from%0Amanipulating%20the%20market%20price%20through%20front%20or%20after-running%20transactions.%20To%0Aidentify%20and%20prevent%20sandwich%20attacks%2C%20we%20propose%20a%20cascade%20classification%0Aframework%20GasTrace.%20GasTrace%20analyzes%20various%20transaction%20features%20to%20detect%0Amalicious%20accounts%2C%20notably%20through%20the%20analysis%20and%20modeling%20of%20Gas%20features.%0AIn%20the%20initial%20classification%2C%20we%20utilize%20the%20Support%20Vector%20Machine%20%28SVM%29%20with%0Athe%20Radial%20Basis%20Function%20%28RBF%29%20kernel%20to%20generate%20the%20predicted%20probabilities%0Aof%20accounts%2C%20further%20constructing%20a%20detailed%20transaction%20network.%20Subsequently%2C%0Athe%20behavior%20features%20are%20captured%20by%20the%20Graph%20Attention%20Network%20%28GAT%29%0Atechnique%20in%20the%20second%20classification.%20Through%20cascade%20classification%2C%0AGasTrace%20can%20analyze%20and%20classify%20the%20sandwich%20attacks.%20Our%20experimental%0Aresults%20demonstrate%20that%20GasTrace%20achieves%20a%20remarkable%20detection%20and%0Ageneration%20capability%2C%20performing%20an%20accuracy%20of%2096.73%5C%25%20and%20an%20F1%20score%20of%0A95.71%5C%25%20for%20identifying%20sandwich%20attack%20accounts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19971v1&entry.124074799=Read"},
{"title": "Oja's Algorithm for Sparse PCA", "author": "Syamantak Kumar and Purnamrita Sarkar", "abstract": "  Oja's algorithm for streaming Principal Component Analysis (PCA) for $n$\ndatapoints in a $d$ dimensional space achieves the same sin-squared error\n$O(r_\\mathsf{eff}/n)$ as the offline algorithm in $O(d)$ space and $O(nd)$ time\nand a single pass through the datapoints. Here $r_\\mathsf{eff}$ is the\neffective rank (ratio of the trace and the principal eigenvalue of the\npopulation covariance matrix $\\Sigma$). Under this computational budget, we\nconsider the problem of sparse PCA, where the principal eigenvector of $\\Sigma$\nis $s$-sparse, and $r_\\mathsf{eff}$ can be large. In this setting, to our\nknowledge, \\textit{there are no known single-pass algorithms} that achieve the\nminimax error bound in $O(d)$ space and $O(nd)$ time without either requiring\nstrong initialization conditions or assuming further structure (e.g., spiked)\nof the covariance matrix. We show that a simple single-pass procedure that\nthresholds the output of Oja's algorithm (the Oja vector) can achieve the\nminimax error bound under some regularity conditions in $O(d)$ space and\n$O(nd)$ time as long as $r_\\mathsf{eff}=O(n/\\log n)$. We present a nontrivial\nand novel analysis of the entries of the unnormalized Oja vector, which\ninvolves the projection of a product of independent random matrices on a random\ninitial vector. This is completely different from previous analyses of Oja's\nalgorithm and matrix products, which have been done when the $r_\\mathsf{eff}$\nis bounded.\n", "link": "http://arxiv.org/abs/2402.07240v3", "date": "2024-05-30", "relevancy": 2.0419, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4199}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4044}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Oja%27s%20Algorithm%20for%20Sparse%20PCA&body=Title%3A%20Oja%27s%20Algorithm%20for%20Sparse%20PCA%0AAuthor%3A%20Syamantak%20Kumar%20and%20Purnamrita%20Sarkar%0AAbstract%3A%20%20%20Oja%27s%20algorithm%20for%20streaming%20Principal%20Component%20Analysis%20%28PCA%29%20for%20%24n%24%0Adatapoints%20in%20a%20%24d%24%20dimensional%20space%20achieves%20the%20same%20sin-squared%20error%0A%24O%28r_%5Cmathsf%7Beff%7D/n%29%24%20as%20the%20offline%20algorithm%20in%20%24O%28d%29%24%20space%20and%20%24O%28nd%29%24%20time%0Aand%20a%20single%20pass%20through%20the%20datapoints.%20Here%20%24r_%5Cmathsf%7Beff%7D%24%20is%20the%0Aeffective%20rank%20%28ratio%20of%20the%20trace%20and%20the%20principal%20eigenvalue%20of%20the%0Apopulation%20covariance%20matrix%20%24%5CSigma%24%29.%20Under%20this%20computational%20budget%2C%20we%0Aconsider%20the%20problem%20of%20sparse%20PCA%2C%20where%20the%20principal%20eigenvector%20of%20%24%5CSigma%24%0Ais%20%24s%24-sparse%2C%20and%20%24r_%5Cmathsf%7Beff%7D%24%20can%20be%20large.%20In%20this%20setting%2C%20to%20our%0Aknowledge%2C%20%5Ctextit%7Bthere%20are%20no%20known%20single-pass%20algorithms%7D%20that%20achieve%20the%0Aminimax%20error%20bound%20in%20%24O%28d%29%24%20space%20and%20%24O%28nd%29%24%20time%20without%20either%20requiring%0Astrong%20initialization%20conditions%20or%20assuming%20further%20structure%20%28e.g.%2C%20spiked%29%0Aof%20the%20covariance%20matrix.%20We%20show%20that%20a%20simple%20single-pass%20procedure%20that%0Athresholds%20the%20output%20of%20Oja%27s%20algorithm%20%28the%20Oja%20vector%29%20can%20achieve%20the%0Aminimax%20error%20bound%20under%20some%20regularity%20conditions%20in%20%24O%28d%29%24%20space%20and%0A%24O%28nd%29%24%20time%20as%20long%20as%20%24r_%5Cmathsf%7Beff%7D%3DO%28n/%5Clog%20n%29%24.%20We%20present%20a%20nontrivial%0Aand%20novel%20analysis%20of%20the%20entries%20of%20the%20unnormalized%20Oja%20vector%2C%20which%0Ainvolves%20the%20projection%20of%20a%20product%20of%20independent%20random%20matrices%20on%20a%20random%0Ainitial%20vector.%20This%20is%20completely%20different%20from%20previous%20analyses%20of%20Oja%27s%0Aalgorithm%20and%20matrix%20products%2C%20which%20have%20been%20done%20when%20the%20%24r_%5Cmathsf%7Beff%7D%24%0Ais%20bounded.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07240v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOja%2527s%2520Algorithm%2520for%2520Sparse%2520PCA%26entry.906535625%3DSyamantak%2520Kumar%2520and%2520Purnamrita%2520Sarkar%26entry.1292438233%3D%2520%2520Oja%2527s%2520algorithm%2520for%2520streaming%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%2520for%2520%2524n%2524%250Adatapoints%2520in%2520a%2520%2524d%2524%2520dimensional%2520space%2520achieves%2520the%2520same%2520sin-squared%2520error%250A%2524O%2528r_%255Cmathsf%257Beff%257D/n%2529%2524%2520as%2520the%2520offline%2520algorithm%2520in%2520%2524O%2528d%2529%2524%2520space%2520and%2520%2524O%2528nd%2529%2524%2520time%250Aand%2520a%2520single%2520pass%2520through%2520the%2520datapoints.%2520Here%2520%2524r_%255Cmathsf%257Beff%257D%2524%2520is%2520the%250Aeffective%2520rank%2520%2528ratio%2520of%2520the%2520trace%2520and%2520the%2520principal%2520eigenvalue%2520of%2520the%250Apopulation%2520covariance%2520matrix%2520%2524%255CSigma%2524%2529.%2520Under%2520this%2520computational%2520budget%252C%2520we%250Aconsider%2520the%2520problem%2520of%2520sparse%2520PCA%252C%2520where%2520the%2520principal%2520eigenvector%2520of%2520%2524%255CSigma%2524%250Ais%2520%2524s%2524-sparse%252C%2520and%2520%2524r_%255Cmathsf%257Beff%257D%2524%2520can%2520be%2520large.%2520In%2520this%2520setting%252C%2520to%2520our%250Aknowledge%252C%2520%255Ctextit%257Bthere%2520are%2520no%2520known%2520single-pass%2520algorithms%257D%2520that%2520achieve%2520the%250Aminimax%2520error%2520bound%2520in%2520%2524O%2528d%2529%2524%2520space%2520and%2520%2524O%2528nd%2529%2524%2520time%2520without%2520either%2520requiring%250Astrong%2520initialization%2520conditions%2520or%2520assuming%2520further%2520structure%2520%2528e.g.%252C%2520spiked%2529%250Aof%2520the%2520covariance%2520matrix.%2520We%2520show%2520that%2520a%2520simple%2520single-pass%2520procedure%2520that%250Athresholds%2520the%2520output%2520of%2520Oja%2527s%2520algorithm%2520%2528the%2520Oja%2520vector%2529%2520can%2520achieve%2520the%250Aminimax%2520error%2520bound%2520under%2520some%2520regularity%2520conditions%2520in%2520%2524O%2528d%2529%2524%2520space%2520and%250A%2524O%2528nd%2529%2524%2520time%2520as%2520long%2520as%2520%2524r_%255Cmathsf%257Beff%257D%253DO%2528n/%255Clog%2520n%2529%2524.%2520We%2520present%2520a%2520nontrivial%250Aand%2520novel%2520analysis%2520of%2520the%2520entries%2520of%2520the%2520unnormalized%2520Oja%2520vector%252C%2520which%250Ainvolves%2520the%2520projection%2520of%2520a%2520product%2520of%2520independent%2520random%2520matrices%2520on%2520a%2520random%250Ainitial%2520vector.%2520This%2520is%2520completely%2520different%2520from%2520previous%2520analyses%2520of%2520Oja%2527s%250Aalgorithm%2520and%2520matrix%2520products%252C%2520which%2520have%2520been%2520done%2520when%2520the%2520%2524r_%255Cmathsf%257Beff%257D%2524%250Ais%2520bounded.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07240v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Oja%27s%20Algorithm%20for%20Sparse%20PCA&entry.906535625=Syamantak%20Kumar%20and%20Purnamrita%20Sarkar&entry.1292438233=%20%20Oja%27s%20algorithm%20for%20streaming%20Principal%20Component%20Analysis%20%28PCA%29%20for%20%24n%24%0Adatapoints%20in%20a%20%24d%24%20dimensional%20space%20achieves%20the%20same%20sin-squared%20error%0A%24O%28r_%5Cmathsf%7Beff%7D/n%29%24%20as%20the%20offline%20algorithm%20in%20%24O%28d%29%24%20space%20and%20%24O%28nd%29%24%20time%0Aand%20a%20single%20pass%20through%20the%20datapoints.%20Here%20%24r_%5Cmathsf%7Beff%7D%24%20is%20the%0Aeffective%20rank%20%28ratio%20of%20the%20trace%20and%20the%20principal%20eigenvalue%20of%20the%0Apopulation%20covariance%20matrix%20%24%5CSigma%24%29.%20Under%20this%20computational%20budget%2C%20we%0Aconsider%20the%20problem%20of%20sparse%20PCA%2C%20where%20the%20principal%20eigenvector%20of%20%24%5CSigma%24%0Ais%20%24s%24-sparse%2C%20and%20%24r_%5Cmathsf%7Beff%7D%24%20can%20be%20large.%20In%20this%20setting%2C%20to%20our%0Aknowledge%2C%20%5Ctextit%7Bthere%20are%20no%20known%20single-pass%20algorithms%7D%20that%20achieve%20the%0Aminimax%20error%20bound%20in%20%24O%28d%29%24%20space%20and%20%24O%28nd%29%24%20time%20without%20either%20requiring%0Astrong%20initialization%20conditions%20or%20assuming%20further%20structure%20%28e.g.%2C%20spiked%29%0Aof%20the%20covariance%20matrix.%20We%20show%20that%20a%20simple%20single-pass%20procedure%20that%0Athresholds%20the%20output%20of%20Oja%27s%20algorithm%20%28the%20Oja%20vector%29%20can%20achieve%20the%0Aminimax%20error%20bound%20under%20some%20regularity%20conditions%20in%20%24O%28d%29%24%20space%20and%0A%24O%28nd%29%24%20time%20as%20long%20as%20%24r_%5Cmathsf%7Beff%7D%3DO%28n/%5Clog%20n%29%24.%20We%20present%20a%20nontrivial%0Aand%20novel%20analysis%20of%20the%20entries%20of%20the%20unnormalized%20Oja%20vector%2C%20which%0Ainvolves%20the%20projection%20of%20a%20product%20of%20independent%20random%20matrices%20on%20a%20random%0Ainitial%20vector.%20This%20is%20completely%20different%20from%20previous%20analyses%20of%20Oja%27s%0Aalgorithm%20and%20matrix%20products%2C%20which%20have%20been%20done%20when%20the%20%24r_%5Cmathsf%7Beff%7D%24%0Ais%20bounded.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07240v3&entry.124074799=Read"},
{"title": "Cross-Training with Multi-View Knowledge Fusion for Heterogenous\n  Federated Learning", "author": "Zhuang Qi and Lei Meng and Weihao He and Ruohan Zhang and Yu Wang and Xin Qi and Xiangxu Meng", "abstract": "  Federated learning benefits from cross-training strategies, which enables\nmodels to train on data from distinct sources to improve the generalization\ncapability. However, the data heterogeneity between sources may lead models to\ngradually forget previously acquired knowledge when undergoing cross-training\nto adapt to new tasks or data sources. We argue that integrating personalized\nand global knowledge to gather information from multiple perspectives could\npotentially improve performance. To achieve this goal, this paper presents a\nnovel approach that enhances federated learning through a cross-training scheme\nincorporating multi-view information. Specifically, the proposed method, termed\nFedCT, includes three main modules, where the consistency-aware knowledge\nbroadcasting module aims to optimize model assignment strategies, which\nenhances collaborative advantages between clients and achieves an efficient\nfederated learning process. The multi-view knowledge-guided representation\nlearning module leverages fused prototypical knowledge from both global and\nlocal views to enhance the preservation of local knowledge before and after\nmodel exchange, as well as to ensure consistency between local and global\nknowledge. The mixup-based feature augmentation module aggregates rich\ninformation to further increase the diversity of feature spaces, which enables\nthe model to better discriminate complex samples. Extensive experiments were\nconducted on four datasets in terms of performance comparison, ablation study,\nin-depth analysis and case study. The results demonstrated that FedCT\nalleviates knowledge forgetting from both local and global views, which enables\nit outperform state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.20046v1", "date": "2024-05-30", "relevancy": 2.0386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5362}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4943}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Training%20with%20Multi-View%20Knowledge%20Fusion%20for%20Heterogenous%0A%20%20Federated%20Learning&body=Title%3A%20Cross-Training%20with%20Multi-View%20Knowledge%20Fusion%20for%20Heterogenous%0A%20%20Federated%20Learning%0AAuthor%3A%20Zhuang%20Qi%20and%20Lei%20Meng%20and%20Weihao%20He%20and%20Ruohan%20Zhang%20and%20Yu%20Wang%20and%20Xin%20Qi%20and%20Xiangxu%20Meng%0AAbstract%3A%20%20%20Federated%20learning%20benefits%20from%20cross-training%20strategies%2C%20which%20enables%0Amodels%20to%20train%20on%20data%20from%20distinct%20sources%20to%20improve%20the%20generalization%0Acapability.%20However%2C%20the%20data%20heterogeneity%20between%20sources%20may%20lead%20models%20to%0Agradually%20forget%20previously%20acquired%20knowledge%20when%20undergoing%20cross-training%0Ato%20adapt%20to%20new%20tasks%20or%20data%20sources.%20We%20argue%20that%20integrating%20personalized%0Aand%20global%20knowledge%20to%20gather%20information%20from%20multiple%20perspectives%20could%0Apotentially%20improve%20performance.%20To%20achieve%20this%20goal%2C%20this%20paper%20presents%20a%0Anovel%20approach%20that%20enhances%20federated%20learning%20through%20a%20cross-training%20scheme%0Aincorporating%20multi-view%20information.%20Specifically%2C%20the%20proposed%20method%2C%20termed%0AFedCT%2C%20includes%20three%20main%20modules%2C%20where%20the%20consistency-aware%20knowledge%0Abroadcasting%20module%20aims%20to%20optimize%20model%20assignment%20strategies%2C%20which%0Aenhances%20collaborative%20advantages%20between%20clients%20and%20achieves%20an%20efficient%0Afederated%20learning%20process.%20The%20multi-view%20knowledge-guided%20representation%0Alearning%20module%20leverages%20fused%20prototypical%20knowledge%20from%20both%20global%20and%0Alocal%20views%20to%20enhance%20the%20preservation%20of%20local%20knowledge%20before%20and%20after%0Amodel%20exchange%2C%20as%20well%20as%20to%20ensure%20consistency%20between%20local%20and%20global%0Aknowledge.%20The%20mixup-based%20feature%20augmentation%20module%20aggregates%20rich%0Ainformation%20to%20further%20increase%20the%20diversity%20of%20feature%20spaces%2C%20which%20enables%0Athe%20model%20to%20better%20discriminate%20complex%20samples.%20Extensive%20experiments%20were%0Aconducted%20on%20four%20datasets%20in%20terms%20of%20performance%20comparison%2C%20ablation%20study%2C%0Ain-depth%20analysis%20and%20case%20study.%20The%20results%20demonstrated%20that%20FedCT%0Aalleviates%20knowledge%20forgetting%20from%20both%20local%20and%20global%20views%2C%20which%20enables%0Ait%20outperform%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Training%2520with%2520Multi-View%2520Knowledge%2520Fusion%2520for%2520Heterogenous%250A%2520%2520Federated%2520Learning%26entry.906535625%3DZhuang%2520Qi%2520and%2520Lei%2520Meng%2520and%2520Weihao%2520He%2520and%2520Ruohan%2520Zhang%2520and%2520Yu%2520Wang%2520and%2520Xin%2520Qi%2520and%2520Xiangxu%2520Meng%26entry.1292438233%3D%2520%2520Federated%2520learning%2520benefits%2520from%2520cross-training%2520strategies%252C%2520which%2520enables%250Amodels%2520to%2520train%2520on%2520data%2520from%2520distinct%2520sources%2520to%2520improve%2520the%2520generalization%250Acapability.%2520However%252C%2520the%2520data%2520heterogeneity%2520between%2520sources%2520may%2520lead%2520models%2520to%250Agradually%2520forget%2520previously%2520acquired%2520knowledge%2520when%2520undergoing%2520cross-training%250Ato%2520adapt%2520to%2520new%2520tasks%2520or%2520data%2520sources.%2520We%2520argue%2520that%2520integrating%2520personalized%250Aand%2520global%2520knowledge%2520to%2520gather%2520information%2520from%2520multiple%2520perspectives%2520could%250Apotentially%2520improve%2520performance.%2520To%2520achieve%2520this%2520goal%252C%2520this%2520paper%2520presents%2520a%250Anovel%2520approach%2520that%2520enhances%2520federated%2520learning%2520through%2520a%2520cross-training%2520scheme%250Aincorporating%2520multi-view%2520information.%2520Specifically%252C%2520the%2520proposed%2520method%252C%2520termed%250AFedCT%252C%2520includes%2520three%2520main%2520modules%252C%2520where%2520the%2520consistency-aware%2520knowledge%250Abroadcasting%2520module%2520aims%2520to%2520optimize%2520model%2520assignment%2520strategies%252C%2520which%250Aenhances%2520collaborative%2520advantages%2520between%2520clients%2520and%2520achieves%2520an%2520efficient%250Afederated%2520learning%2520process.%2520The%2520multi-view%2520knowledge-guided%2520representation%250Alearning%2520module%2520leverages%2520fused%2520prototypical%2520knowledge%2520from%2520both%2520global%2520and%250Alocal%2520views%2520to%2520enhance%2520the%2520preservation%2520of%2520local%2520knowledge%2520before%2520and%2520after%250Amodel%2520exchange%252C%2520as%2520well%2520as%2520to%2520ensure%2520consistency%2520between%2520local%2520and%2520global%250Aknowledge.%2520The%2520mixup-based%2520feature%2520augmentation%2520module%2520aggregates%2520rich%250Ainformation%2520to%2520further%2520increase%2520the%2520diversity%2520of%2520feature%2520spaces%252C%2520which%2520enables%250Athe%2520model%2520to%2520better%2520discriminate%2520complex%2520samples.%2520Extensive%2520experiments%2520were%250Aconducted%2520on%2520four%2520datasets%2520in%2520terms%2520of%2520performance%2520comparison%252C%2520ablation%2520study%252C%250Ain-depth%2520analysis%2520and%2520case%2520study.%2520The%2520results%2520demonstrated%2520that%2520FedCT%250Aalleviates%2520knowledge%2520forgetting%2520from%2520both%2520local%2520and%2520global%2520views%252C%2520which%2520enables%250Ait%2520outperform%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Training%20with%20Multi-View%20Knowledge%20Fusion%20for%20Heterogenous%0A%20%20Federated%20Learning&entry.906535625=Zhuang%20Qi%20and%20Lei%20Meng%20and%20Weihao%20He%20and%20Ruohan%20Zhang%20and%20Yu%20Wang%20and%20Xin%20Qi%20and%20Xiangxu%20Meng&entry.1292438233=%20%20Federated%20learning%20benefits%20from%20cross-training%20strategies%2C%20which%20enables%0Amodels%20to%20train%20on%20data%20from%20distinct%20sources%20to%20improve%20the%20generalization%0Acapability.%20However%2C%20the%20data%20heterogeneity%20between%20sources%20may%20lead%20models%20to%0Agradually%20forget%20previously%20acquired%20knowledge%20when%20undergoing%20cross-training%0Ato%20adapt%20to%20new%20tasks%20or%20data%20sources.%20We%20argue%20that%20integrating%20personalized%0Aand%20global%20knowledge%20to%20gather%20information%20from%20multiple%20perspectives%20could%0Apotentially%20improve%20performance.%20To%20achieve%20this%20goal%2C%20this%20paper%20presents%20a%0Anovel%20approach%20that%20enhances%20federated%20learning%20through%20a%20cross-training%20scheme%0Aincorporating%20multi-view%20information.%20Specifically%2C%20the%20proposed%20method%2C%20termed%0AFedCT%2C%20includes%20three%20main%20modules%2C%20where%20the%20consistency-aware%20knowledge%0Abroadcasting%20module%20aims%20to%20optimize%20model%20assignment%20strategies%2C%20which%0Aenhances%20collaborative%20advantages%20between%20clients%20and%20achieves%20an%20efficient%0Afederated%20learning%20process.%20The%20multi-view%20knowledge-guided%20representation%0Alearning%20module%20leverages%20fused%20prototypical%20knowledge%20from%20both%20global%20and%0Alocal%20views%20to%20enhance%20the%20preservation%20of%20local%20knowledge%20before%20and%20after%0Amodel%20exchange%2C%20as%20well%20as%20to%20ensure%20consistency%20between%20local%20and%20global%0Aknowledge.%20The%20mixup-based%20feature%20augmentation%20module%20aggregates%20rich%0Ainformation%20to%20further%20increase%20the%20diversity%20of%20feature%20spaces%2C%20which%20enables%0Athe%20model%20to%20better%20discriminate%20complex%20samples.%20Extensive%20experiments%20were%0Aconducted%20on%20four%20datasets%20in%20terms%20of%20performance%20comparison%2C%20ablation%20study%2C%0Ain-depth%20analysis%20and%20case%20study.%20The%20results%20demonstrated%20that%20FedCT%0Aalleviates%20knowledge%20forgetting%20from%20both%20local%20and%20global%20views%2C%20which%20enables%0Ait%20outperform%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20046v1&entry.124074799=Read"},
{"title": "Systematic Analysis for Pretrained Language Model Priming for\n  Parameter-Efficient Fine-tuning", "author": "Shih-Cheng Huang and Shih-Heng Wang and Min-Han Shih and Saurav Sahay and Hung-yi Lee", "abstract": "  Parameter-efficient (PE) methods (like Prompts or Adapters) for adapting\npre-trained language models (PLM) to downstream tasks have been popular\nrecently. However, hindrances still prevent these methods from reaching their\nfull potential. For example, two significant challenges are few-shot adaptation\nand cross-task generalization. To tackle these issues, we propose a general PE\npriming framework to enhance and explore the few-shot adaptation and\ngeneralization ability of PE methods. In this framework, PLMs are primed with\nPE methods for rapidly adapting to various target tasks. To evaluate the\ngeneralization ability of these PE methods, we conduct experiments on a\nfew-shot cross-domain benchmark containing 160 diverse NLP tasks. Our\nexperiment not only reveals the best priming strategy but also verifies that\npriming facilitates the adaptation to target tasks.\n", "link": "http://arxiv.org/abs/2212.01032v2", "date": "2024-05-30", "relevancy": 2.0193, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5384}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4942}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematic%20Analysis%20for%20Pretrained%20Language%20Model%20Priming%20for%0A%20%20Parameter-Efficient%20Fine-tuning&body=Title%3A%20Systematic%20Analysis%20for%20Pretrained%20Language%20Model%20Priming%20for%0A%20%20Parameter-Efficient%20Fine-tuning%0AAuthor%3A%20Shih-Cheng%20Huang%20and%20Shih-Heng%20Wang%20and%20Min-Han%20Shih%20and%20Saurav%20Sahay%20and%20Hung-yi%20Lee%0AAbstract%3A%20%20%20Parameter-efficient%20%28PE%29%20methods%20%28like%20Prompts%20or%20Adapters%29%20for%20adapting%0Apre-trained%20language%20models%20%28PLM%29%20to%20downstream%20tasks%20have%20been%20popular%0Arecently.%20However%2C%20hindrances%20still%20prevent%20these%20methods%20from%20reaching%20their%0Afull%20potential.%20For%20example%2C%20two%20significant%20challenges%20are%20few-shot%20adaptation%0Aand%20cross-task%20generalization.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%20general%20PE%0Apriming%20framework%20to%20enhance%20and%20explore%20the%20few-shot%20adaptation%20and%0Ageneralization%20ability%20of%20PE%20methods.%20In%20this%20framework%2C%20PLMs%20are%20primed%20with%0APE%20methods%20for%20rapidly%20adapting%20to%20various%20target%20tasks.%20To%20evaluate%20the%0Ageneralization%20ability%20of%20these%20PE%20methods%2C%20we%20conduct%20experiments%20on%20a%0Afew-shot%20cross-domain%20benchmark%20containing%20160%20diverse%20NLP%20tasks.%20Our%0Aexperiment%20not%20only%20reveals%20the%20best%20priming%20strategy%20but%20also%20verifies%20that%0Apriming%20facilitates%20the%20adaptation%20to%20target%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.01032v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematic%2520Analysis%2520for%2520Pretrained%2520Language%2520Model%2520Priming%2520for%250A%2520%2520Parameter-Efficient%2520Fine-tuning%26entry.906535625%3DShih-Cheng%2520Huang%2520and%2520Shih-Heng%2520Wang%2520and%2520Min-Han%2520Shih%2520and%2520Saurav%2520Sahay%2520and%2520Hung-yi%2520Lee%26entry.1292438233%3D%2520%2520Parameter-efficient%2520%2528PE%2529%2520methods%2520%2528like%2520Prompts%2520or%2520Adapters%2529%2520for%2520adapting%250Apre-trained%2520language%2520models%2520%2528PLM%2529%2520to%2520downstream%2520tasks%2520have%2520been%2520popular%250Arecently.%2520However%252C%2520hindrances%2520still%2520prevent%2520these%2520methods%2520from%2520reaching%2520their%250Afull%2520potential.%2520For%2520example%252C%2520two%2520significant%2520challenges%2520are%2520few-shot%2520adaptation%250Aand%2520cross-task%2520generalization.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520a%2520general%2520PE%250Apriming%2520framework%2520to%2520enhance%2520and%2520explore%2520the%2520few-shot%2520adaptation%2520and%250Ageneralization%2520ability%2520of%2520PE%2520methods.%2520In%2520this%2520framework%252C%2520PLMs%2520are%2520primed%2520with%250APE%2520methods%2520for%2520rapidly%2520adapting%2520to%2520various%2520target%2520tasks.%2520To%2520evaluate%2520the%250Ageneralization%2520ability%2520of%2520these%2520PE%2520methods%252C%2520we%2520conduct%2520experiments%2520on%2520a%250Afew-shot%2520cross-domain%2520benchmark%2520containing%2520160%2520diverse%2520NLP%2520tasks.%2520Our%250Aexperiment%2520not%2520only%2520reveals%2520the%2520best%2520priming%2520strategy%2520but%2520also%2520verifies%2520that%250Apriming%2520facilitates%2520the%2520adaptation%2520to%2520target%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.01032v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematic%20Analysis%20for%20Pretrained%20Language%20Model%20Priming%20for%0A%20%20Parameter-Efficient%20Fine-tuning&entry.906535625=Shih-Cheng%20Huang%20and%20Shih-Heng%20Wang%20and%20Min-Han%20Shih%20and%20Saurav%20Sahay%20and%20Hung-yi%20Lee&entry.1292438233=%20%20Parameter-efficient%20%28PE%29%20methods%20%28like%20Prompts%20or%20Adapters%29%20for%20adapting%0Apre-trained%20language%20models%20%28PLM%29%20to%20downstream%20tasks%20have%20been%20popular%0Arecently.%20However%2C%20hindrances%20still%20prevent%20these%20methods%20from%20reaching%20their%0Afull%20potential.%20For%20example%2C%20two%20significant%20challenges%20are%20few-shot%20adaptation%0Aand%20cross-task%20generalization.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%20general%20PE%0Apriming%20framework%20to%20enhance%20and%20explore%20the%20few-shot%20adaptation%20and%0Ageneralization%20ability%20of%20PE%20methods.%20In%20this%20framework%2C%20PLMs%20are%20primed%20with%0APE%20methods%20for%20rapidly%20adapting%20to%20various%20target%20tasks.%20To%20evaluate%20the%0Ageneralization%20ability%20of%20these%20PE%20methods%2C%20we%20conduct%20experiments%20on%20a%0Afew-shot%20cross-domain%20benchmark%20containing%20160%20diverse%20NLP%20tasks.%20Our%0Aexperiment%20not%20only%20reveals%20the%20best%20priming%20strategy%20but%20also%20verifies%20that%0Apriming%20facilitates%20the%20adaptation%20to%20target%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.01032v2&entry.124074799=Read"},
{"title": "A Hardware-Efficient EMG Decoder with an Attractor-based Neural Network\n  for Next-Generation Hand Prostheses", "author": "Mohammad Kalbasi and MohammadAli Shaeri and Vincent Alexandre Mendez and Solaiman Shokur and Silvestro Micera and Mahsa Shoaran", "abstract": "  Advancements in neural engineering have enabled the development of Robotic\nProsthetic Hands (RPHs) aimed at restoring hand functionality. Current\ncommercial RPHs offer limited control through basic on/off commands. Recent\nprogresses in machine learning enable finger movement decoding with higher\ndegrees of freedom, yet the high computational complexity of such models limits\ntheir application in portable devices. Future RPH designs must balance\nportability, low power consumption, and high decoding accuracy to be practical\nfor individuals with disabilities. To this end, we introduce a novel\nattractor-based neural network to realize on-chip movement decoding for\nnext-generation portable RPHs. The proposed architecture comprises an encoder,\nan attention layer, an attractor network, and a refinement regressor. We tested\nour model on four healthy subjects and achieved a decoding accuracy of\n80.6\\pm3.3\\%. Our proposed model is over 120 and 50 times more compact compared\nto state-of-the-art LSTM and CNN models, respectively, with comparable (or\nsuperior) decoding accuracy. Therefore, it exhibits minimal hardware complexity\nand can be effectively integrated as a System-on-Chip.\n", "link": "http://arxiv.org/abs/2405.20052v1", "date": "2024-05-30", "relevancy": 2.0158, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5086}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hardware-Efficient%20EMG%20Decoder%20with%20an%20Attractor-based%20Neural%20Network%0A%20%20for%20Next-Generation%20Hand%20Prostheses&body=Title%3A%20A%20Hardware-Efficient%20EMG%20Decoder%20with%20an%20Attractor-based%20Neural%20Network%0A%20%20for%20Next-Generation%20Hand%20Prostheses%0AAuthor%3A%20Mohammad%20Kalbasi%20and%20MohammadAli%20Shaeri%20and%20Vincent%20Alexandre%20Mendez%20and%20Solaiman%20Shokur%20and%20Silvestro%20Micera%20and%20Mahsa%20Shoaran%0AAbstract%3A%20%20%20Advancements%20in%20neural%20engineering%20have%20enabled%20the%20development%20of%20Robotic%0AProsthetic%20Hands%20%28RPHs%29%20aimed%20at%20restoring%20hand%20functionality.%20Current%0Acommercial%20RPHs%20offer%20limited%20control%20through%20basic%20on/off%20commands.%20Recent%0Aprogresses%20in%20machine%20learning%20enable%20finger%20movement%20decoding%20with%20higher%0Adegrees%20of%20freedom%2C%20yet%20the%20high%20computational%20complexity%20of%20such%20models%20limits%0Atheir%20application%20in%20portable%20devices.%20Future%20RPH%20designs%20must%20balance%0Aportability%2C%20low%20power%20consumption%2C%20and%20high%20decoding%20accuracy%20to%20be%20practical%0Afor%20individuals%20with%20disabilities.%20To%20this%20end%2C%20we%20introduce%20a%20novel%0Aattractor-based%20neural%20network%20to%20realize%20on-chip%20movement%20decoding%20for%0Anext-generation%20portable%20RPHs.%20The%20proposed%20architecture%20comprises%20an%20encoder%2C%0Aan%20attention%20layer%2C%20an%20attractor%20network%2C%20and%20a%20refinement%20regressor.%20We%20tested%0Aour%20model%20on%20four%20healthy%20subjects%20and%20achieved%20a%20decoding%20accuracy%20of%0A80.6%5Cpm3.3%5C%25.%20Our%20proposed%20model%20is%20over%20120%20and%2050%20times%20more%20compact%20compared%0Ato%20state-of-the-art%20LSTM%20and%20CNN%20models%2C%20respectively%2C%20with%20comparable%20%28or%0Asuperior%29%20decoding%20accuracy.%20Therefore%2C%20it%20exhibits%20minimal%20hardware%20complexity%0Aand%20can%20be%20effectively%20integrated%20as%20a%20System-on-Chip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hardware-Efficient%2520EMG%2520Decoder%2520with%2520an%2520Attractor-based%2520Neural%2520Network%250A%2520%2520for%2520Next-Generation%2520Hand%2520Prostheses%26entry.906535625%3DMohammad%2520Kalbasi%2520and%2520MohammadAli%2520Shaeri%2520and%2520Vincent%2520Alexandre%2520Mendez%2520and%2520Solaiman%2520Shokur%2520and%2520Silvestro%2520Micera%2520and%2520Mahsa%2520Shoaran%26entry.1292438233%3D%2520%2520Advancements%2520in%2520neural%2520engineering%2520have%2520enabled%2520the%2520development%2520of%2520Robotic%250AProsthetic%2520Hands%2520%2528RPHs%2529%2520aimed%2520at%2520restoring%2520hand%2520functionality.%2520Current%250Acommercial%2520RPHs%2520offer%2520limited%2520control%2520through%2520basic%2520on/off%2520commands.%2520Recent%250Aprogresses%2520in%2520machine%2520learning%2520enable%2520finger%2520movement%2520decoding%2520with%2520higher%250Adegrees%2520of%2520freedom%252C%2520yet%2520the%2520high%2520computational%2520complexity%2520of%2520such%2520models%2520limits%250Atheir%2520application%2520in%2520portable%2520devices.%2520Future%2520RPH%2520designs%2520must%2520balance%250Aportability%252C%2520low%2520power%2520consumption%252C%2520and%2520high%2520decoding%2520accuracy%2520to%2520be%2520practical%250Afor%2520individuals%2520with%2520disabilities.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520novel%250Aattractor-based%2520neural%2520network%2520to%2520realize%2520on-chip%2520movement%2520decoding%2520for%250Anext-generation%2520portable%2520RPHs.%2520The%2520proposed%2520architecture%2520comprises%2520an%2520encoder%252C%250Aan%2520attention%2520layer%252C%2520an%2520attractor%2520network%252C%2520and%2520a%2520refinement%2520regressor.%2520We%2520tested%250Aour%2520model%2520on%2520four%2520healthy%2520subjects%2520and%2520achieved%2520a%2520decoding%2520accuracy%2520of%250A80.6%255Cpm3.3%255C%2525.%2520Our%2520proposed%2520model%2520is%2520over%2520120%2520and%252050%2520times%2520more%2520compact%2520compared%250Ato%2520state-of-the-art%2520LSTM%2520and%2520CNN%2520models%252C%2520respectively%252C%2520with%2520comparable%2520%2528or%250Asuperior%2529%2520decoding%2520accuracy.%2520Therefore%252C%2520it%2520exhibits%2520minimal%2520hardware%2520complexity%250Aand%2520can%2520be%2520effectively%2520integrated%2520as%2520a%2520System-on-Chip.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hardware-Efficient%20EMG%20Decoder%20with%20an%20Attractor-based%20Neural%20Network%0A%20%20for%20Next-Generation%20Hand%20Prostheses&entry.906535625=Mohammad%20Kalbasi%20and%20MohammadAli%20Shaeri%20and%20Vincent%20Alexandre%20Mendez%20and%20Solaiman%20Shokur%20and%20Silvestro%20Micera%20and%20Mahsa%20Shoaran&entry.1292438233=%20%20Advancements%20in%20neural%20engineering%20have%20enabled%20the%20development%20of%20Robotic%0AProsthetic%20Hands%20%28RPHs%29%20aimed%20at%20restoring%20hand%20functionality.%20Current%0Acommercial%20RPHs%20offer%20limited%20control%20through%20basic%20on/off%20commands.%20Recent%0Aprogresses%20in%20machine%20learning%20enable%20finger%20movement%20decoding%20with%20higher%0Adegrees%20of%20freedom%2C%20yet%20the%20high%20computational%20complexity%20of%20such%20models%20limits%0Atheir%20application%20in%20portable%20devices.%20Future%20RPH%20designs%20must%20balance%0Aportability%2C%20low%20power%20consumption%2C%20and%20high%20decoding%20accuracy%20to%20be%20practical%0Afor%20individuals%20with%20disabilities.%20To%20this%20end%2C%20we%20introduce%20a%20novel%0Aattractor-based%20neural%20network%20to%20realize%20on-chip%20movement%20decoding%20for%0Anext-generation%20portable%20RPHs.%20The%20proposed%20architecture%20comprises%20an%20encoder%2C%0Aan%20attention%20layer%2C%20an%20attractor%20network%2C%20and%20a%20refinement%20regressor.%20We%20tested%0Aour%20model%20on%20four%20healthy%20subjects%20and%20achieved%20a%20decoding%20accuracy%20of%0A80.6%5Cpm3.3%5C%25.%20Our%20proposed%20model%20is%20over%20120%20and%2050%20times%20more%20compact%20compared%0Ato%20state-of-the-art%20LSTM%20and%20CNN%20models%2C%20respectively%2C%20with%20comparable%20%28or%0Asuperior%29%20decoding%20accuracy.%20Therefore%2C%20it%20exhibits%20minimal%20hardware%20complexity%0Aand%20can%20be%20effectively%20integrated%20as%20a%20System-on-Chip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20052v1&entry.124074799=Read"},
{"title": "SPAM: Stochastic Proximal Point Method with Momentum Variance Reduction\n  for Non-convex Cross-Device Federated Learning", "author": "Avetik Karagulyan and Egor Shulgin and Abdurakhmon Sadiev and Peter Richt\u00e1rik", "abstract": "  Cross-device training is a crucial subfield of federated learning, where the\nnumber of clients can reach into the billions. Standard approaches and local\nmethods are prone to issues such as client drift and insensitivity to data\nsimilarities. We propose a novel algorithm (SPAM) for cross-device federated\nlearning with non-convex losses, which solves both issues. We provide sharp\nanalysis under second-order (Hessian) similarity, a condition satisfied by a\nvariety of machine learning problems in practice. Additionally, we extend our\nresults to the partial participation setting, where a cohort of selected\nclients communicate with the server at each communication round. Our method is\nthe first in its kind, that does not require the smoothness of the objective\nand provably benefits from clients having similar data.\n", "link": "http://arxiv.org/abs/2405.20127v1", "date": "2024-05-30", "relevancy": 2.0147, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5069}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5026}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPAM%3A%20Stochastic%20Proximal%20Point%20Method%20with%20Momentum%20Variance%20Reduction%0A%20%20for%20Non-convex%20Cross-Device%20Federated%20Learning&body=Title%3A%20SPAM%3A%20Stochastic%20Proximal%20Point%20Method%20with%20Momentum%20Variance%20Reduction%0A%20%20for%20Non-convex%20Cross-Device%20Federated%20Learning%0AAuthor%3A%20Avetik%20Karagulyan%20and%20Egor%20Shulgin%20and%20Abdurakhmon%20Sadiev%20and%20Peter%20Richt%C3%A1rik%0AAbstract%3A%20%20%20Cross-device%20training%20is%20a%20crucial%20subfield%20of%20federated%20learning%2C%20where%20the%0Anumber%20of%20clients%20can%20reach%20into%20the%20billions.%20Standard%20approaches%20and%20local%0Amethods%20are%20prone%20to%20issues%20such%20as%20client%20drift%20and%20insensitivity%20to%20data%0Asimilarities.%20We%20propose%20a%20novel%20algorithm%20%28SPAM%29%20for%20cross-device%20federated%0Alearning%20with%20non-convex%20losses%2C%20which%20solves%20both%20issues.%20We%20provide%20sharp%0Aanalysis%20under%20second-order%20%28Hessian%29%20similarity%2C%20a%20condition%20satisfied%20by%20a%0Avariety%20of%20machine%20learning%20problems%20in%20practice.%20Additionally%2C%20we%20extend%20our%0Aresults%20to%20the%20partial%20participation%20setting%2C%20where%20a%20cohort%20of%20selected%0Aclients%20communicate%20with%20the%20server%20at%20each%20communication%20round.%20Our%20method%20is%0Athe%20first%20in%20its%20kind%2C%20that%20does%20not%20require%20the%20smoothness%20of%20the%20objective%0Aand%20provably%20benefits%20from%20clients%20having%20similar%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPAM%253A%2520Stochastic%2520Proximal%2520Point%2520Method%2520with%2520Momentum%2520Variance%2520Reduction%250A%2520%2520for%2520Non-convex%2520Cross-Device%2520Federated%2520Learning%26entry.906535625%3DAvetik%2520Karagulyan%2520and%2520Egor%2520Shulgin%2520and%2520Abdurakhmon%2520Sadiev%2520and%2520Peter%2520Richt%25C3%25A1rik%26entry.1292438233%3D%2520%2520Cross-device%2520training%2520is%2520a%2520crucial%2520subfield%2520of%2520federated%2520learning%252C%2520where%2520the%250Anumber%2520of%2520clients%2520can%2520reach%2520into%2520the%2520billions.%2520Standard%2520approaches%2520and%2520local%250Amethods%2520are%2520prone%2520to%2520issues%2520such%2520as%2520client%2520drift%2520and%2520insensitivity%2520to%2520data%250Asimilarities.%2520We%2520propose%2520a%2520novel%2520algorithm%2520%2528SPAM%2529%2520for%2520cross-device%2520federated%250Alearning%2520with%2520non-convex%2520losses%252C%2520which%2520solves%2520both%2520issues.%2520We%2520provide%2520sharp%250Aanalysis%2520under%2520second-order%2520%2528Hessian%2529%2520similarity%252C%2520a%2520condition%2520satisfied%2520by%2520a%250Avariety%2520of%2520machine%2520learning%2520problems%2520in%2520practice.%2520Additionally%252C%2520we%2520extend%2520our%250Aresults%2520to%2520the%2520partial%2520participation%2520setting%252C%2520where%2520a%2520cohort%2520of%2520selected%250Aclients%2520communicate%2520with%2520the%2520server%2520at%2520each%2520communication%2520round.%2520Our%2520method%2520is%250Athe%2520first%2520in%2520its%2520kind%252C%2520that%2520does%2520not%2520require%2520the%2520smoothness%2520of%2520the%2520objective%250Aand%2520provably%2520benefits%2520from%2520clients%2520having%2520similar%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPAM%3A%20Stochastic%20Proximal%20Point%20Method%20with%20Momentum%20Variance%20Reduction%0A%20%20for%20Non-convex%20Cross-Device%20Federated%20Learning&entry.906535625=Avetik%20Karagulyan%20and%20Egor%20Shulgin%20and%20Abdurakhmon%20Sadiev%20and%20Peter%20Richt%C3%A1rik&entry.1292438233=%20%20Cross-device%20training%20is%20a%20crucial%20subfield%20of%20federated%20learning%2C%20where%20the%0Anumber%20of%20clients%20can%20reach%20into%20the%20billions.%20Standard%20approaches%20and%20local%0Amethods%20are%20prone%20to%20issues%20such%20as%20client%20drift%20and%20insensitivity%20to%20data%0Asimilarities.%20We%20propose%20a%20novel%20algorithm%20%28SPAM%29%20for%20cross-device%20federated%0Alearning%20with%20non-convex%20losses%2C%20which%20solves%20both%20issues.%20We%20provide%20sharp%0Aanalysis%20under%20second-order%20%28Hessian%29%20similarity%2C%20a%20condition%20satisfied%20by%20a%0Avariety%20of%20machine%20learning%20problems%20in%20practice.%20Additionally%2C%20we%20extend%20our%0Aresults%20to%20the%20partial%20participation%20setting%2C%20where%20a%20cohort%20of%20selected%0Aclients%20communicate%20with%20the%20server%20at%20each%20communication%20round.%20Our%20method%20is%0Athe%20first%20in%20its%20kind%2C%20that%20does%20not%20require%20the%20smoothness%20of%20the%20objective%0Aand%20provably%20benefits%20from%20clients%20having%20similar%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20127v1&entry.124074799=Read"},
{"title": "Disentangling and Mitigating the Impact of Task Similarity for Continual\n  Learning", "author": "Naoki Hiratani", "abstract": "  Continual learning of partially similar tasks poses a challenge for\nartificial neural networks, as task similarity presents both an opportunity for\nknowledge transfer and a risk of interference and catastrophic forgetting.\nHowever, it remains unclear how task similarity in input features and readout\npatterns influences knowledge transfer and forgetting, as well as how they\ninteract with common algorithms for continual learning. Here, we develop a\nlinear teacher-student model with latent structure and show analytically that\nhigh input feature similarity coupled with low readout similarity is\ncatastrophic for both knowledge transfer and retention. Conversely, the\nopposite scenario is relatively benign. Our analysis further reveals that\ntask-dependent activity gating improves knowledge retention at the expense of\ntransfer, while task-dependent plasticity gating does not affect either\nretention or transfer performance at the over-parameterized limit. In contrast,\nweight regularization based on the Fisher information metric significantly\nimproves retention, regardless of task similarity, without compromising\ntransfer performance. Nevertheless, its diagonal approximation and\nregularization in the Euclidean space are much less robust against task\nsimilarity. We demonstrate consistent results in a permuted MNIST task with\nlatent variables. Overall, this work provides insights into when continual\nlearning is difficult and how to mitigate it.\n", "link": "http://arxiv.org/abs/2405.20236v1", "date": "2024-05-30", "relevancy": 2.0104, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5145}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4947}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20and%20Mitigating%20the%20Impact%20of%20Task%20Similarity%20for%20Continual%0A%20%20Learning&body=Title%3A%20Disentangling%20and%20Mitigating%20the%20Impact%20of%20Task%20Similarity%20for%20Continual%0A%20%20Learning%0AAuthor%3A%20Naoki%20Hiratani%0AAbstract%3A%20%20%20Continual%20learning%20of%20partially%20similar%20tasks%20poses%20a%20challenge%20for%0Aartificial%20neural%20networks%2C%20as%20task%20similarity%20presents%20both%20an%20opportunity%20for%0Aknowledge%20transfer%20and%20a%20risk%20of%20interference%20and%20catastrophic%20forgetting.%0AHowever%2C%20it%20remains%20unclear%20how%20task%20similarity%20in%20input%20features%20and%20readout%0Apatterns%20influences%20knowledge%20transfer%20and%20forgetting%2C%20as%20well%20as%20how%20they%0Ainteract%20with%20common%20algorithms%20for%20continual%20learning.%20Here%2C%20we%20develop%20a%0Alinear%20teacher-student%20model%20with%20latent%20structure%20and%20show%20analytically%20that%0Ahigh%20input%20feature%20similarity%20coupled%20with%20low%20readout%20similarity%20is%0Acatastrophic%20for%20both%20knowledge%20transfer%20and%20retention.%20Conversely%2C%20the%0Aopposite%20scenario%20is%20relatively%20benign.%20Our%20analysis%20further%20reveals%20that%0Atask-dependent%20activity%20gating%20improves%20knowledge%20retention%20at%20the%20expense%20of%0Atransfer%2C%20while%20task-dependent%20plasticity%20gating%20does%20not%20affect%20either%0Aretention%20or%20transfer%20performance%20at%20the%20over-parameterized%20limit.%20In%20contrast%2C%0Aweight%20regularization%20based%20on%20the%20Fisher%20information%20metric%20significantly%0Aimproves%20retention%2C%20regardless%20of%20task%20similarity%2C%20without%20compromising%0Atransfer%20performance.%20Nevertheless%2C%20its%20diagonal%20approximation%20and%0Aregularization%20in%20the%20Euclidean%20space%20are%20much%20less%20robust%20against%20task%0Asimilarity.%20We%20demonstrate%20consistent%20results%20in%20a%20permuted%20MNIST%20task%20with%0Alatent%20variables.%20Overall%2C%20this%20work%20provides%20insights%20into%20when%20continual%0Alearning%20is%20difficult%20and%20how%20to%20mitigate%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520and%2520Mitigating%2520the%2520Impact%2520of%2520Task%2520Similarity%2520for%2520Continual%250A%2520%2520Learning%26entry.906535625%3DNaoki%2520Hiratani%26entry.1292438233%3D%2520%2520Continual%2520learning%2520of%2520partially%2520similar%2520tasks%2520poses%2520a%2520challenge%2520for%250Aartificial%2520neural%2520networks%252C%2520as%2520task%2520similarity%2520presents%2520both%2520an%2520opportunity%2520for%250Aknowledge%2520transfer%2520and%2520a%2520risk%2520of%2520interference%2520and%2520catastrophic%2520forgetting.%250AHowever%252C%2520it%2520remains%2520unclear%2520how%2520task%2520similarity%2520in%2520input%2520features%2520and%2520readout%250Apatterns%2520influences%2520knowledge%2520transfer%2520and%2520forgetting%252C%2520as%2520well%2520as%2520how%2520they%250Ainteract%2520with%2520common%2520algorithms%2520for%2520continual%2520learning.%2520Here%252C%2520we%2520develop%2520a%250Alinear%2520teacher-student%2520model%2520with%2520latent%2520structure%2520and%2520show%2520analytically%2520that%250Ahigh%2520input%2520feature%2520similarity%2520coupled%2520with%2520low%2520readout%2520similarity%2520is%250Acatastrophic%2520for%2520both%2520knowledge%2520transfer%2520and%2520retention.%2520Conversely%252C%2520the%250Aopposite%2520scenario%2520is%2520relatively%2520benign.%2520Our%2520analysis%2520further%2520reveals%2520that%250Atask-dependent%2520activity%2520gating%2520improves%2520knowledge%2520retention%2520at%2520the%2520expense%2520of%250Atransfer%252C%2520while%2520task-dependent%2520plasticity%2520gating%2520does%2520not%2520affect%2520either%250Aretention%2520or%2520transfer%2520performance%2520at%2520the%2520over-parameterized%2520limit.%2520In%2520contrast%252C%250Aweight%2520regularization%2520based%2520on%2520the%2520Fisher%2520information%2520metric%2520significantly%250Aimproves%2520retention%252C%2520regardless%2520of%2520task%2520similarity%252C%2520without%2520compromising%250Atransfer%2520performance.%2520Nevertheless%252C%2520its%2520diagonal%2520approximation%2520and%250Aregularization%2520in%2520the%2520Euclidean%2520space%2520are%2520much%2520less%2520robust%2520against%2520task%250Asimilarity.%2520We%2520demonstrate%2520consistent%2520results%2520in%2520a%2520permuted%2520MNIST%2520task%2520with%250Alatent%2520variables.%2520Overall%252C%2520this%2520work%2520provides%2520insights%2520into%2520when%2520continual%250Alearning%2520is%2520difficult%2520and%2520how%2520to%2520mitigate%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20and%20Mitigating%20the%20Impact%20of%20Task%20Similarity%20for%20Continual%0A%20%20Learning&entry.906535625=Naoki%20Hiratani&entry.1292438233=%20%20Continual%20learning%20of%20partially%20similar%20tasks%20poses%20a%20challenge%20for%0Aartificial%20neural%20networks%2C%20as%20task%20similarity%20presents%20both%20an%20opportunity%20for%0Aknowledge%20transfer%20and%20a%20risk%20of%20interference%20and%20catastrophic%20forgetting.%0AHowever%2C%20it%20remains%20unclear%20how%20task%20similarity%20in%20input%20features%20and%20readout%0Apatterns%20influences%20knowledge%20transfer%20and%20forgetting%2C%20as%20well%20as%20how%20they%0Ainteract%20with%20common%20algorithms%20for%20continual%20learning.%20Here%2C%20we%20develop%20a%0Alinear%20teacher-student%20model%20with%20latent%20structure%20and%20show%20analytically%20that%0Ahigh%20input%20feature%20similarity%20coupled%20with%20low%20readout%20similarity%20is%0Acatastrophic%20for%20both%20knowledge%20transfer%20and%20retention.%20Conversely%2C%20the%0Aopposite%20scenario%20is%20relatively%20benign.%20Our%20analysis%20further%20reveals%20that%0Atask-dependent%20activity%20gating%20improves%20knowledge%20retention%20at%20the%20expense%20of%0Atransfer%2C%20while%20task-dependent%20plasticity%20gating%20does%20not%20affect%20either%0Aretention%20or%20transfer%20performance%20at%20the%20over-parameterized%20limit.%20In%20contrast%2C%0Aweight%20regularization%20based%20on%20the%20Fisher%20information%20metric%20significantly%0Aimproves%20retention%2C%20regardless%20of%20task%20similarity%2C%20without%20compromising%0Atransfer%20performance.%20Nevertheless%2C%20its%20diagonal%20approximation%20and%0Aregularization%20in%20the%20Euclidean%20space%20are%20much%20less%20robust%20against%20task%0Asimilarity.%20We%20demonstrate%20consistent%20results%20in%20a%20permuted%20MNIST%20task%20with%0Alatent%20variables.%20Overall%2C%20this%20work%20provides%20insights%20into%20when%20continual%0Alearning%20is%20difficult%20and%20how%20to%20mitigate%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20236v1&entry.124074799=Read"},
{"title": "Goals as Reward-Producing Programs", "author": "Guy Davidson and Graham Todd and Julian Togelius and Todd M. Gureckis and Brenden M. Lake", "abstract": "  People are remarkably capable of generating their own goals, beginning with\nchild's play and continuing into adulthood. Despite considerable empirical and\ncomputational work on goals and goal-oriented behavior, models are still far\nfrom capturing the richness of everyday human goals. Here, we bridge this gap\nby collecting a dataset of human-generated playful goals, modeling them as\nreward-producing programs, and generating novel human-like goals through\nprogram synthesis. Reward-producing programs capture the rich semantics of\ngoals through symbolic operations that compose, add temporal constraints, and\nallow for program execution on behavioral traces to evaluate progress. To build\na generative model of goals, we learn a fitness function over the infinite set\nof possible goal programs and sample novel goals with a quality-diversity\nalgorithm. Human evaluators found that model-generated goals, when sampled from\npartitions of program space occupied by human examples, were indistinguishable\nfrom human-created games. We also discovered that our model's internal fitness\nscores predict games that are evaluated as more fun to play and more\nhuman-like.\n", "link": "http://arxiv.org/abs/2405.13242v2", "date": "2024-05-30", "relevancy": 2.0095, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5448}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4856}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goals%20as%20Reward-Producing%20Programs&body=Title%3A%20Goals%20as%20Reward-Producing%20Programs%0AAuthor%3A%20Guy%20Davidson%20and%20Graham%20Todd%20and%20Julian%20Togelius%20and%20Todd%20M.%20Gureckis%20and%20Brenden%20M.%20Lake%0AAbstract%3A%20%20%20People%20are%20remarkably%20capable%20of%20generating%20their%20own%20goals%2C%20beginning%20with%0Achild%27s%20play%20and%20continuing%20into%20adulthood.%20Despite%20considerable%20empirical%20and%0Acomputational%20work%20on%20goals%20and%20goal-oriented%20behavior%2C%20models%20are%20still%20far%0Afrom%20capturing%20the%20richness%20of%20everyday%20human%20goals.%20Here%2C%20we%20bridge%20this%20gap%0Aby%20collecting%20a%20dataset%20of%20human-generated%20playful%20goals%2C%20modeling%20them%20as%0Areward-producing%20programs%2C%20and%20generating%20novel%20human-like%20goals%20through%0Aprogram%20synthesis.%20Reward-producing%20programs%20capture%20the%20rich%20semantics%20of%0Agoals%20through%20symbolic%20operations%20that%20compose%2C%20add%20temporal%20constraints%2C%20and%0Aallow%20for%20program%20execution%20on%20behavioral%20traces%20to%20evaluate%20progress.%20To%20build%0Aa%20generative%20model%20of%20goals%2C%20we%20learn%20a%20fitness%20function%20over%20the%20infinite%20set%0Aof%20possible%20goal%20programs%20and%20sample%20novel%20goals%20with%20a%20quality-diversity%0Aalgorithm.%20Human%20evaluators%20found%20that%20model-generated%20goals%2C%20when%20sampled%20from%0Apartitions%20of%20program%20space%20occupied%20by%20human%20examples%2C%20were%20indistinguishable%0Afrom%20human-created%20games.%20We%20also%20discovered%20that%20our%20model%27s%20internal%20fitness%0Ascores%20predict%20games%20that%20are%20evaluated%20as%20more%20fun%20to%20play%20and%20more%0Ahuman-like.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoals%2520as%2520Reward-Producing%2520Programs%26entry.906535625%3DGuy%2520Davidson%2520and%2520Graham%2520Todd%2520and%2520Julian%2520Togelius%2520and%2520Todd%2520M.%2520Gureckis%2520and%2520Brenden%2520M.%2520Lake%26entry.1292438233%3D%2520%2520People%2520are%2520remarkably%2520capable%2520of%2520generating%2520their%2520own%2520goals%252C%2520beginning%2520with%250Achild%2527s%2520play%2520and%2520continuing%2520into%2520adulthood.%2520Despite%2520considerable%2520empirical%2520and%250Acomputational%2520work%2520on%2520goals%2520and%2520goal-oriented%2520behavior%252C%2520models%2520are%2520still%2520far%250Afrom%2520capturing%2520the%2520richness%2520of%2520everyday%2520human%2520goals.%2520Here%252C%2520we%2520bridge%2520this%2520gap%250Aby%2520collecting%2520a%2520dataset%2520of%2520human-generated%2520playful%2520goals%252C%2520modeling%2520them%2520as%250Areward-producing%2520programs%252C%2520and%2520generating%2520novel%2520human-like%2520goals%2520through%250Aprogram%2520synthesis.%2520Reward-producing%2520programs%2520capture%2520the%2520rich%2520semantics%2520of%250Agoals%2520through%2520symbolic%2520operations%2520that%2520compose%252C%2520add%2520temporal%2520constraints%252C%2520and%250Aallow%2520for%2520program%2520execution%2520on%2520behavioral%2520traces%2520to%2520evaluate%2520progress.%2520To%2520build%250Aa%2520generative%2520model%2520of%2520goals%252C%2520we%2520learn%2520a%2520fitness%2520function%2520over%2520the%2520infinite%2520set%250Aof%2520possible%2520goal%2520programs%2520and%2520sample%2520novel%2520goals%2520with%2520a%2520quality-diversity%250Aalgorithm.%2520Human%2520evaluators%2520found%2520that%2520model-generated%2520goals%252C%2520when%2520sampled%2520from%250Apartitions%2520of%2520program%2520space%2520occupied%2520by%2520human%2520examples%252C%2520were%2520indistinguishable%250Afrom%2520human-created%2520games.%2520We%2520also%2520discovered%2520that%2520our%2520model%2527s%2520internal%2520fitness%250Ascores%2520predict%2520games%2520that%2520are%2520evaluated%2520as%2520more%2520fun%2520to%2520play%2520and%2520more%250Ahuman-like.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goals%20as%20Reward-Producing%20Programs&entry.906535625=Guy%20Davidson%20and%20Graham%20Todd%20and%20Julian%20Togelius%20and%20Todd%20M.%20Gureckis%20and%20Brenden%20M.%20Lake&entry.1292438233=%20%20People%20are%20remarkably%20capable%20of%20generating%20their%20own%20goals%2C%20beginning%20with%0Achild%27s%20play%20and%20continuing%20into%20adulthood.%20Despite%20considerable%20empirical%20and%0Acomputational%20work%20on%20goals%20and%20goal-oriented%20behavior%2C%20models%20are%20still%20far%0Afrom%20capturing%20the%20richness%20of%20everyday%20human%20goals.%20Here%2C%20we%20bridge%20this%20gap%0Aby%20collecting%20a%20dataset%20of%20human-generated%20playful%20goals%2C%20modeling%20them%20as%0Areward-producing%20programs%2C%20and%20generating%20novel%20human-like%20goals%20through%0Aprogram%20synthesis.%20Reward-producing%20programs%20capture%20the%20rich%20semantics%20of%0Agoals%20through%20symbolic%20operations%20that%20compose%2C%20add%20temporal%20constraints%2C%20and%0Aallow%20for%20program%20execution%20on%20behavioral%20traces%20to%20evaluate%20progress.%20To%20build%0Aa%20generative%20model%20of%20goals%2C%20we%20learn%20a%20fitness%20function%20over%20the%20infinite%20set%0Aof%20possible%20goal%20programs%20and%20sample%20novel%20goals%20with%20a%20quality-diversity%0Aalgorithm.%20Human%20evaluators%20found%20that%20model-generated%20goals%2C%20when%20sampled%20from%0Apartitions%20of%20program%20space%20occupied%20by%20human%20examples%2C%20were%20indistinguishable%0Afrom%20human-created%20games.%20We%20also%20discovered%20that%20our%20model%27s%20internal%20fitness%0Ascores%20predict%20games%20that%20are%20evaluated%20as%20more%20fun%20to%20play%20and%20more%0Ahuman-like.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13242v2&entry.124074799=Read"},
{"title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model\n  Series", "author": "Ge Zhang and Scott Qu and Jiaheng Liu and Chenchen Zhang and Chenghua Lin and Chou Leuang Yu and Danny Pan and Esther Cheng and Jie Liu and Qunshu Lin and Raven Yuan and Tuney Zheng and Wei Pang and Xinrun Du and Yiming Liang and Yinghao Ma and Yizhi Li and Ziyang Ma and Bill Lin and Emmanouil Benetos and Huan Yang and Junting Zhou and Kaijing Ma and Minghao Liu and Morry Niu and Noah Wang and Quehry Que and Ruibo Liu and Sine Liu and Shawn Guo and Soren Gao and Wangchunshu Zhou and Xinyue Zhang and Yizhi Zhou and Yubo Wang and Yuelin Bai and Yuhan Zhang and Yuxiang Zhang and Zenith Wang and Zhenzhu Yang and Zijian Zhao and Jiajun Zhang and Wanli Ouyang and Wenhao Huang and Wenhu Chen", "abstract": "  Large Language Models (LLMs) have made great strides in recent years to\nachieve unprecedented performance across different tasks. However, due to\ncommercial interest, the most competitive models like GPT, Gemini, and Claude\nhave been gated behind proprietary interfaces without disclosing the training\ndetails. Recently, many institutions have open-sourced several strong LLMs like\nLLaMA-3, comparable to existing closed-source LLMs. However, only the model's\nweights are provided with most details (e.g., intermediate checkpoints,\npre-training corpus, and training code, etc.) being undisclosed. To improve the\ntransparency of LLMs, the research community has formed to open-source truly\nopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training\ncorpus and training code) are being provided. These models have greatly\nadvanced the scientific study of these large models including their strengths,\nweaknesses, biases and risks. However, we observe that the existing truly open\nLLMs on reasoning, knowledge, and coding tasks are still inferior to existing\nstate-of-the-art LLMs with similar model sizes. To this end, we open-source\nMAP-Neo, a highly capable and transparent bilingual language model with 7B\nparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the\nfirst fully open-sourced bilingual LLM with comparable performance compared to\nexisting state-of-the-art LLMs. Moreover, we open-source all details to\nreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning\npipeline, checkpoints, and well-optimized training/evaluation framework are\nprovided. Finally, we hope our MAP-Neo will enhance and strengthen the open\nresearch community and inspire more innovations and creativities to facilitate\nthe further improvements of LLMs.\n", "link": "http://arxiv.org/abs/2405.19327v2", "date": "2024-05-30", "relevancy": 2.0034, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5054}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAP-Neo%3A%20Highly%20Capable%20and%20Transparent%20Bilingual%20Large%20Language%20Model%0A%20%20Series&body=Title%3A%20MAP-Neo%3A%20Highly%20Capable%20and%20Transparent%20Bilingual%20Large%20Language%20Model%0A%20%20Series%0AAuthor%3A%20Ge%20Zhang%20and%20Scott%20Qu%20and%20Jiaheng%20Liu%20and%20Chenchen%20Zhang%20and%20Chenghua%20Lin%20and%20Chou%20Leuang%20Yu%20and%20Danny%20Pan%20and%20Esther%20Cheng%20and%20Jie%20Liu%20and%20Qunshu%20Lin%20and%20Raven%20Yuan%20and%20Tuney%20Zheng%20and%20Wei%20Pang%20and%20Xinrun%20Du%20and%20Yiming%20Liang%20and%20Yinghao%20Ma%20and%20Yizhi%20Li%20and%20Ziyang%20Ma%20and%20Bill%20Lin%20and%20Emmanouil%20Benetos%20and%20Huan%20Yang%20and%20Junting%20Zhou%20and%20Kaijing%20Ma%20and%20Minghao%20Liu%20and%20Morry%20Niu%20and%20Noah%20Wang%20and%20Quehry%20Que%20and%20Ruibo%20Liu%20and%20Sine%20Liu%20and%20Shawn%20Guo%20and%20Soren%20Gao%20and%20Wangchunshu%20Zhou%20and%20Xinyue%20Zhang%20and%20Yizhi%20Zhou%20and%20Yubo%20Wang%20and%20Yuelin%20Bai%20and%20Yuhan%20Zhang%20and%20Yuxiang%20Zhang%20and%20Zenith%20Wang%20and%20Zhenzhu%20Yang%20and%20Zijian%20Zhao%20and%20Jiajun%20Zhang%20and%20Wanli%20Ouyang%20and%20Wenhao%20Huang%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20great%20strides%20in%20recent%20years%20to%0Aachieve%20unprecedented%20performance%20across%20different%20tasks.%20However%2C%20due%20to%0Acommercial%20interest%2C%20the%20most%20competitive%20models%20like%20GPT%2C%20Gemini%2C%20and%20Claude%0Ahave%20been%20gated%20behind%20proprietary%20interfaces%20without%20disclosing%20the%20training%0Adetails.%20Recently%2C%20many%20institutions%20have%20open-sourced%20several%20strong%20LLMs%20like%0ALLaMA-3%2C%20comparable%20to%20existing%20closed-source%20LLMs.%20However%2C%20only%20the%20model%27s%0Aweights%20are%20provided%20with%20most%20details%20%28e.g.%2C%20intermediate%20checkpoints%2C%0Apre-training%20corpus%2C%20and%20training%20code%2C%20etc.%29%20being%20undisclosed.%20To%20improve%20the%0Atransparency%20of%20LLMs%2C%20the%20research%20community%20has%20formed%20to%20open-source%20truly%0Aopen%20LLMs%20%28e.g.%2C%20Pythia%2C%20Amber%2C%20OLMo%29%2C%20where%20more%20details%20%28e.g.%2C%20pre-training%0Acorpus%20and%20training%20code%29%20are%20being%20provided.%20These%20models%20have%20greatly%0Aadvanced%20the%20scientific%20study%20of%20these%20large%20models%20including%20their%20strengths%2C%0Aweaknesses%2C%20biases%20and%20risks.%20However%2C%20we%20observe%20that%20the%20existing%20truly%20open%0ALLMs%20on%20reasoning%2C%20knowledge%2C%20and%20coding%20tasks%20are%20still%20inferior%20to%20existing%0Astate-of-the-art%20LLMs%20with%20similar%20model%20sizes.%20To%20this%20end%2C%20we%20open-source%0AMAP-Neo%2C%20a%20highly%20capable%20and%20transparent%20bilingual%20language%20model%20with%207B%0Aparameters%20trained%20from%20scratch%20on%204.5T%20high-quality%20tokens.%20Our%20MAP-Neo%20is%20the%0Afirst%20fully%20open-sourced%20bilingual%20LLM%20with%20comparable%20performance%20compared%20to%0Aexisting%20state-of-the-art%20LLMs.%20Moreover%2C%20we%20open-source%20all%20details%20to%0Areproduce%20our%20MAP-Neo%2C%20where%20the%20cleaned%20pre-training%20corpus%2C%20data%20cleaning%0Apipeline%2C%20checkpoints%2C%20and%20well-optimized%20training/evaluation%20framework%20are%0Aprovided.%20Finally%2C%20we%20hope%20our%20MAP-Neo%20will%20enhance%20and%20strengthen%20the%20open%0Aresearch%20community%20and%20inspire%20more%20innovations%20and%20creativities%20to%20facilitate%0Athe%20further%20improvements%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAP-Neo%253A%2520Highly%2520Capable%2520and%2520Transparent%2520Bilingual%2520Large%2520Language%2520Model%250A%2520%2520Series%26entry.906535625%3DGe%2520Zhang%2520and%2520Scott%2520Qu%2520and%2520Jiaheng%2520Liu%2520and%2520Chenchen%2520Zhang%2520and%2520Chenghua%2520Lin%2520and%2520Chou%2520Leuang%2520Yu%2520and%2520Danny%2520Pan%2520and%2520Esther%2520Cheng%2520and%2520Jie%2520Liu%2520and%2520Qunshu%2520Lin%2520and%2520Raven%2520Yuan%2520and%2520Tuney%2520Zheng%2520and%2520Wei%2520Pang%2520and%2520Xinrun%2520Du%2520and%2520Yiming%2520Liang%2520and%2520Yinghao%2520Ma%2520and%2520Yizhi%2520Li%2520and%2520Ziyang%2520Ma%2520and%2520Bill%2520Lin%2520and%2520Emmanouil%2520Benetos%2520and%2520Huan%2520Yang%2520and%2520Junting%2520Zhou%2520and%2520Kaijing%2520Ma%2520and%2520Minghao%2520Liu%2520and%2520Morry%2520Niu%2520and%2520Noah%2520Wang%2520and%2520Quehry%2520Que%2520and%2520Ruibo%2520Liu%2520and%2520Sine%2520Liu%2520and%2520Shawn%2520Guo%2520and%2520Soren%2520Gao%2520and%2520Wangchunshu%2520Zhou%2520and%2520Xinyue%2520Zhang%2520and%2520Yizhi%2520Zhou%2520and%2520Yubo%2520Wang%2520and%2520Yuelin%2520Bai%2520and%2520Yuhan%2520Zhang%2520and%2520Yuxiang%2520Zhang%2520and%2520Zenith%2520Wang%2520and%2520Zhenzhu%2520Yang%2520and%2520Zijian%2520Zhao%2520and%2520Jiajun%2520Zhang%2520and%2520Wanli%2520Ouyang%2520and%2520Wenhao%2520Huang%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520great%2520strides%2520in%2520recent%2520years%2520to%250Aachieve%2520unprecedented%2520performance%2520across%2520different%2520tasks.%2520However%252C%2520due%2520to%250Acommercial%2520interest%252C%2520the%2520most%2520competitive%2520models%2520like%2520GPT%252C%2520Gemini%252C%2520and%2520Claude%250Ahave%2520been%2520gated%2520behind%2520proprietary%2520interfaces%2520without%2520disclosing%2520the%2520training%250Adetails.%2520Recently%252C%2520many%2520institutions%2520have%2520open-sourced%2520several%2520strong%2520LLMs%2520like%250ALLaMA-3%252C%2520comparable%2520to%2520existing%2520closed-source%2520LLMs.%2520However%252C%2520only%2520the%2520model%2527s%250Aweights%2520are%2520provided%2520with%2520most%2520details%2520%2528e.g.%252C%2520intermediate%2520checkpoints%252C%250Apre-training%2520corpus%252C%2520and%2520training%2520code%252C%2520etc.%2529%2520being%2520undisclosed.%2520To%2520improve%2520the%250Atransparency%2520of%2520LLMs%252C%2520the%2520research%2520community%2520has%2520formed%2520to%2520open-source%2520truly%250Aopen%2520LLMs%2520%2528e.g.%252C%2520Pythia%252C%2520Amber%252C%2520OLMo%2529%252C%2520where%2520more%2520details%2520%2528e.g.%252C%2520pre-training%250Acorpus%2520and%2520training%2520code%2529%2520are%2520being%2520provided.%2520These%2520models%2520have%2520greatly%250Aadvanced%2520the%2520scientific%2520study%2520of%2520these%2520large%2520models%2520including%2520their%2520strengths%252C%250Aweaknesses%252C%2520biases%2520and%2520risks.%2520However%252C%2520we%2520observe%2520that%2520the%2520existing%2520truly%2520open%250ALLMs%2520on%2520reasoning%252C%2520knowledge%252C%2520and%2520coding%2520tasks%2520are%2520still%2520inferior%2520to%2520existing%250Astate-of-the-art%2520LLMs%2520with%2520similar%2520model%2520sizes.%2520To%2520this%2520end%252C%2520we%2520open-source%250AMAP-Neo%252C%2520a%2520highly%2520capable%2520and%2520transparent%2520bilingual%2520language%2520model%2520with%25207B%250Aparameters%2520trained%2520from%2520scratch%2520on%25204.5T%2520high-quality%2520tokens.%2520Our%2520MAP-Neo%2520is%2520the%250Afirst%2520fully%2520open-sourced%2520bilingual%2520LLM%2520with%2520comparable%2520performance%2520compared%2520to%250Aexisting%2520state-of-the-art%2520LLMs.%2520Moreover%252C%2520we%2520open-source%2520all%2520details%2520to%250Areproduce%2520our%2520MAP-Neo%252C%2520where%2520the%2520cleaned%2520pre-training%2520corpus%252C%2520data%2520cleaning%250Apipeline%252C%2520checkpoints%252C%2520and%2520well-optimized%2520training/evaluation%2520framework%2520are%250Aprovided.%2520Finally%252C%2520we%2520hope%2520our%2520MAP-Neo%2520will%2520enhance%2520and%2520strengthen%2520the%2520open%250Aresearch%2520community%2520and%2520inspire%2520more%2520innovations%2520and%2520creativities%2520to%2520facilitate%250Athe%2520further%2520improvements%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAP-Neo%3A%20Highly%20Capable%20and%20Transparent%20Bilingual%20Large%20Language%20Model%0A%20%20Series&entry.906535625=Ge%20Zhang%20and%20Scott%20Qu%20and%20Jiaheng%20Liu%20and%20Chenchen%20Zhang%20and%20Chenghua%20Lin%20and%20Chou%20Leuang%20Yu%20and%20Danny%20Pan%20and%20Esther%20Cheng%20and%20Jie%20Liu%20and%20Qunshu%20Lin%20and%20Raven%20Yuan%20and%20Tuney%20Zheng%20and%20Wei%20Pang%20and%20Xinrun%20Du%20and%20Yiming%20Liang%20and%20Yinghao%20Ma%20and%20Yizhi%20Li%20and%20Ziyang%20Ma%20and%20Bill%20Lin%20and%20Emmanouil%20Benetos%20and%20Huan%20Yang%20and%20Junting%20Zhou%20and%20Kaijing%20Ma%20and%20Minghao%20Liu%20and%20Morry%20Niu%20and%20Noah%20Wang%20and%20Quehry%20Que%20and%20Ruibo%20Liu%20and%20Sine%20Liu%20and%20Shawn%20Guo%20and%20Soren%20Gao%20and%20Wangchunshu%20Zhou%20and%20Xinyue%20Zhang%20and%20Yizhi%20Zhou%20and%20Yubo%20Wang%20and%20Yuelin%20Bai%20and%20Yuhan%20Zhang%20and%20Yuxiang%20Zhang%20and%20Zenith%20Wang%20and%20Zhenzhu%20Yang%20and%20Zijian%20Zhao%20and%20Jiajun%20Zhang%20and%20Wanli%20Ouyang%20and%20Wenhao%20Huang%20and%20Wenhu%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20great%20strides%20in%20recent%20years%20to%0Aachieve%20unprecedented%20performance%20across%20different%20tasks.%20However%2C%20due%20to%0Acommercial%20interest%2C%20the%20most%20competitive%20models%20like%20GPT%2C%20Gemini%2C%20and%20Claude%0Ahave%20been%20gated%20behind%20proprietary%20interfaces%20without%20disclosing%20the%20training%0Adetails.%20Recently%2C%20many%20institutions%20have%20open-sourced%20several%20strong%20LLMs%20like%0ALLaMA-3%2C%20comparable%20to%20existing%20closed-source%20LLMs.%20However%2C%20only%20the%20model%27s%0Aweights%20are%20provided%20with%20most%20details%20%28e.g.%2C%20intermediate%20checkpoints%2C%0Apre-training%20corpus%2C%20and%20training%20code%2C%20etc.%29%20being%20undisclosed.%20To%20improve%20the%0Atransparency%20of%20LLMs%2C%20the%20research%20community%20has%20formed%20to%20open-source%20truly%0Aopen%20LLMs%20%28e.g.%2C%20Pythia%2C%20Amber%2C%20OLMo%29%2C%20where%20more%20details%20%28e.g.%2C%20pre-training%0Acorpus%20and%20training%20code%29%20are%20being%20provided.%20These%20models%20have%20greatly%0Aadvanced%20the%20scientific%20study%20of%20these%20large%20models%20including%20their%20strengths%2C%0Aweaknesses%2C%20biases%20and%20risks.%20However%2C%20we%20observe%20that%20the%20existing%20truly%20open%0ALLMs%20on%20reasoning%2C%20knowledge%2C%20and%20coding%20tasks%20are%20still%20inferior%20to%20existing%0Astate-of-the-art%20LLMs%20with%20similar%20model%20sizes.%20To%20this%20end%2C%20we%20open-source%0AMAP-Neo%2C%20a%20highly%20capable%20and%20transparent%20bilingual%20language%20model%20with%207B%0Aparameters%20trained%20from%20scratch%20on%204.5T%20high-quality%20tokens.%20Our%20MAP-Neo%20is%20the%0Afirst%20fully%20open-sourced%20bilingual%20LLM%20with%20comparable%20performance%20compared%20to%0Aexisting%20state-of-the-art%20LLMs.%20Moreover%2C%20we%20open-source%20all%20details%20to%0Areproduce%20our%20MAP-Neo%2C%20where%20the%20cleaned%20pre-training%20corpus%2C%20data%20cleaning%0Apipeline%2C%20checkpoints%2C%20and%20well-optimized%20training/evaluation%20framework%20are%0Aprovided.%20Finally%2C%20we%20hope%20our%20MAP-Neo%20will%20enhance%20and%20strengthen%20the%20open%0Aresearch%20community%20and%20inspire%20more%20innovations%20and%20creativities%20to%20facilitate%0Athe%20further%20improvements%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19327v2&entry.124074799=Read"},
{"title": "Large Language Models Can Self-Improve At Web Agent Tasks", "author": "Ajay Patel and Markus Hofmarcher and Claudiu Leoveanu-Condrei and Marius-Constantin Dinu and Chris Callison-Burch and Sepp Hochreiter", "abstract": "  Training models to act as agents that can effectively navigate and perform\nactions in a complex environment, such as a web browser, has typically been\nchallenging due to lack of training data. Large language models (LLMs) have\nrecently demonstrated some capability to navigate novel environments as agents\nin a zero-shot or few-shot fashion, purely guided by natural language\ninstructions as prompts. Recent research has also demonstrated LLMs have the\ncapability to exceed their base performance through self-improvement, i.e.\nfine-tuning on data generated by the model itself. In this work, we explore the\nextent to which LLMs can self-improve their performance as agents in\nlong-horizon tasks in a complex environment using the WebArena benchmark. In\nWebArena, an agent must autonomously navigate and perform actions on web pages\nto achieve a specified objective. We explore fine-tuning on three distinct\nsynthetic training data mixtures and achieve a 31\\% improvement in task\ncompletion rate over the base model on the WebArena benchmark through a\nself-improvement procedure. We additionally contribute novel evaluation metrics\nfor assessing the performance, robustness, capabilities, and quality of\ntrajectories of our fine-tuned agent models to a greater degree than simple,\naggregate-level benchmark scores currently used to measure self-improvement.\n", "link": "http://arxiv.org/abs/2405.20309v1", "date": "2024-05-30", "relevancy": 2.002, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5059}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4994}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Can%20Self-Improve%20At%20Web%20Agent%20Tasks&body=Title%3A%20Large%20Language%20Models%20Can%20Self-Improve%20At%20Web%20Agent%20Tasks%0AAuthor%3A%20Ajay%20Patel%20and%20Markus%20Hofmarcher%20and%20Claudiu%20Leoveanu-Condrei%20and%20Marius-Constantin%20Dinu%20and%20Chris%20Callison-Burch%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20Training%20models%20to%20act%20as%20agents%20that%20can%20effectively%20navigate%20and%20perform%0Aactions%20in%20a%20complex%20environment%2C%20such%20as%20a%20web%20browser%2C%20has%20typically%20been%0Achallenging%20due%20to%20lack%20of%20training%20data.%20Large%20language%20models%20%28LLMs%29%20have%0Arecently%20demonstrated%20some%20capability%20to%20navigate%20novel%20environments%20as%20agents%0Ain%20a%20zero-shot%20or%20few-shot%20fashion%2C%20purely%20guided%20by%20natural%20language%0Ainstructions%20as%20prompts.%20Recent%20research%20has%20also%20demonstrated%20LLMs%20have%20the%0Acapability%20to%20exceed%20their%20base%20performance%20through%20self-improvement%2C%20i.e.%0Afine-tuning%20on%20data%20generated%20by%20the%20model%20itself.%20In%20this%20work%2C%20we%20explore%20the%0Aextent%20to%20which%20LLMs%20can%20self-improve%20their%20performance%20as%20agents%20in%0Along-horizon%20tasks%20in%20a%20complex%20environment%20using%20the%20WebArena%20benchmark.%20In%0AWebArena%2C%20an%20agent%20must%20autonomously%20navigate%20and%20perform%20actions%20on%20web%20pages%0Ato%20achieve%20a%20specified%20objective.%20We%20explore%20fine-tuning%20on%20three%20distinct%0Asynthetic%20training%20data%20mixtures%20and%20achieve%20a%2031%5C%25%20improvement%20in%20task%0Acompletion%20rate%20over%20the%20base%20model%20on%20the%20WebArena%20benchmark%20through%20a%0Aself-improvement%20procedure.%20We%20additionally%20contribute%20novel%20evaluation%20metrics%0Afor%20assessing%20the%20performance%2C%20robustness%2C%20capabilities%2C%20and%20quality%20of%0Atrajectories%20of%20our%20fine-tuned%20agent%20models%20to%20a%20greater%20degree%20than%20simple%2C%0Aaggregate-level%20benchmark%20scores%20currently%20used%20to%20measure%20self-improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Can%2520Self-Improve%2520At%2520Web%2520Agent%2520Tasks%26entry.906535625%3DAjay%2520Patel%2520and%2520Markus%2520Hofmarcher%2520and%2520Claudiu%2520Leoveanu-Condrei%2520and%2520Marius-Constantin%2520Dinu%2520and%2520Chris%2520Callison-Burch%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520Training%2520models%2520to%2520act%2520as%2520agents%2520that%2520can%2520effectively%2520navigate%2520and%2520perform%250Aactions%2520in%2520a%2520complex%2520environment%252C%2520such%2520as%2520a%2520web%2520browser%252C%2520has%2520typically%2520been%250Achallenging%2520due%2520to%2520lack%2520of%2520training%2520data.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%250Arecently%2520demonstrated%2520some%2520capability%2520to%2520navigate%2520novel%2520environments%2520as%2520agents%250Ain%2520a%2520zero-shot%2520or%2520few-shot%2520fashion%252C%2520purely%2520guided%2520by%2520natural%2520language%250Ainstructions%2520as%2520prompts.%2520Recent%2520research%2520has%2520also%2520demonstrated%2520LLMs%2520have%2520the%250Acapability%2520to%2520exceed%2520their%2520base%2520performance%2520through%2520self-improvement%252C%2520i.e.%250Afine-tuning%2520on%2520data%2520generated%2520by%2520the%2520model%2520itself.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Aextent%2520to%2520which%2520LLMs%2520can%2520self-improve%2520their%2520performance%2520as%2520agents%2520in%250Along-horizon%2520tasks%2520in%2520a%2520complex%2520environment%2520using%2520the%2520WebArena%2520benchmark.%2520In%250AWebArena%252C%2520an%2520agent%2520must%2520autonomously%2520navigate%2520and%2520perform%2520actions%2520on%2520web%2520pages%250Ato%2520achieve%2520a%2520specified%2520objective.%2520We%2520explore%2520fine-tuning%2520on%2520three%2520distinct%250Asynthetic%2520training%2520data%2520mixtures%2520and%2520achieve%2520a%252031%255C%2525%2520improvement%2520in%2520task%250Acompletion%2520rate%2520over%2520the%2520base%2520model%2520on%2520the%2520WebArena%2520benchmark%2520through%2520a%250Aself-improvement%2520procedure.%2520We%2520additionally%2520contribute%2520novel%2520evaluation%2520metrics%250Afor%2520assessing%2520the%2520performance%252C%2520robustness%252C%2520capabilities%252C%2520and%2520quality%2520of%250Atrajectories%2520of%2520our%2520fine-tuned%2520agent%2520models%2520to%2520a%2520greater%2520degree%2520than%2520simple%252C%250Aaggregate-level%2520benchmark%2520scores%2520currently%2520used%2520to%2520measure%2520self-improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Can%20Self-Improve%20At%20Web%20Agent%20Tasks&entry.906535625=Ajay%20Patel%20and%20Markus%20Hofmarcher%20and%20Claudiu%20Leoveanu-Condrei%20and%20Marius-Constantin%20Dinu%20and%20Chris%20Callison-Burch%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20Training%20models%20to%20act%20as%20agents%20that%20can%20effectively%20navigate%20and%20perform%0Aactions%20in%20a%20complex%20environment%2C%20such%20as%20a%20web%20browser%2C%20has%20typically%20been%0Achallenging%20due%20to%20lack%20of%20training%20data.%20Large%20language%20models%20%28LLMs%29%20have%0Arecently%20demonstrated%20some%20capability%20to%20navigate%20novel%20environments%20as%20agents%0Ain%20a%20zero-shot%20or%20few-shot%20fashion%2C%20purely%20guided%20by%20natural%20language%0Ainstructions%20as%20prompts.%20Recent%20research%20has%20also%20demonstrated%20LLMs%20have%20the%0Acapability%20to%20exceed%20their%20base%20performance%20through%20self-improvement%2C%20i.e.%0Afine-tuning%20on%20data%20generated%20by%20the%20model%20itself.%20In%20this%20work%2C%20we%20explore%20the%0Aextent%20to%20which%20LLMs%20can%20self-improve%20their%20performance%20as%20agents%20in%0Along-horizon%20tasks%20in%20a%20complex%20environment%20using%20the%20WebArena%20benchmark.%20In%0AWebArena%2C%20an%20agent%20must%20autonomously%20navigate%20and%20perform%20actions%20on%20web%20pages%0Ato%20achieve%20a%20specified%20objective.%20We%20explore%20fine-tuning%20on%20three%20distinct%0Asynthetic%20training%20data%20mixtures%20and%20achieve%20a%2031%5C%25%20improvement%20in%20task%0Acompletion%20rate%20over%20the%20base%20model%20on%20the%20WebArena%20benchmark%20through%20a%0Aself-improvement%20procedure.%20We%20additionally%20contribute%20novel%20evaluation%20metrics%0Afor%20assessing%20the%20performance%2C%20robustness%2C%20capabilities%2C%20and%20quality%20of%0Atrajectories%20of%20our%20fine-tuned%20agent%20models%20to%20a%20greater%20degree%20than%20simple%2C%0Aaggregate-level%20benchmark%20scores%20currently%20used%20to%20measure%20self-improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20309v1&entry.124074799=Read"},
{"title": "SurgiTrack: Fine-Grained Multi-Class Multi-Tool Tracking in Surgical\n  Videos", "author": "Chinedu Innocent Nwoye and Nicolas Padoy", "abstract": "  Accurate tool tracking is essential for the success of computer-assisted\nintervention. Previous efforts often modeled tool trajectories rigidly,\noverlooking the dynamic nature of surgical procedures, especially tracking\nscenarios like out-of-body and out-of-camera views. Addressing this limitation,\nthe new CholecTrack20 dataset provides detailed labels that account for\nmultiple tool trajectories in three perspectives: (1) intraoperative, (2)\nintracorporeal, and (3) visibility, representing the different types of\ntemporal duration of tool tracks. These fine-grained labels enhance tracking\nflexibility but also increase the task complexity. Re-identifying tools after\nocclusion or re-insertion into the body remains challenging due to high visual\nsimilarity, especially among tools of the same category. This work recognizes\nthe critical role of the tool operators in distinguishing tool track instances,\nespecially those belonging to the same tool category. The operators'\ninformation are however not explicitly captured in surgical videos. We\ntherefore propose SurgiTrack, a novel deep learning method that leverages\nYOLOv7 for precise tool detection and employs an attention mechanism to model\nthe originating direction of the tools, as a proxy to their operators, for tool\nre-identification. To handle diverse tool trajectory perspectives, SurgiTrack\nemploys a harmonizing bipartite matching graph, minimizing conflicts and\nensuring accurate tool identity association. Experimental results on\nCholecTrack20 demonstrate SurgiTrack's effectiveness, outperforming baselines\nand state-of-the-art methods with real-time inference capability. This work\nsets a new standard in surgical tool tracking, providing dynamic trajectories\nfor more adaptable and precise assistance in minimally invasive surgeries.\n", "link": "http://arxiv.org/abs/2405.20333v1", "date": "2024-05-30", "relevancy": 1.9966, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5241}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.502}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgiTrack%3A%20Fine-Grained%20Multi-Class%20Multi-Tool%20Tracking%20in%20Surgical%0A%20%20Videos&body=Title%3A%20SurgiTrack%3A%20Fine-Grained%20Multi-Class%20Multi-Tool%20Tracking%20in%20Surgical%0A%20%20Videos%0AAuthor%3A%20Chinedu%20Innocent%20Nwoye%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Accurate%20tool%20tracking%20is%20essential%20for%20the%20success%20of%20computer-assisted%0Aintervention.%20Previous%20efforts%20often%20modeled%20tool%20trajectories%20rigidly%2C%0Aoverlooking%20the%20dynamic%20nature%20of%20surgical%20procedures%2C%20especially%20tracking%0Ascenarios%20like%20out-of-body%20and%20out-of-camera%20views.%20Addressing%20this%20limitation%2C%0Athe%20new%20CholecTrack20%20dataset%20provides%20detailed%20labels%20that%20account%20for%0Amultiple%20tool%20trajectories%20in%20three%20perspectives%3A%20%281%29%20intraoperative%2C%20%282%29%0Aintracorporeal%2C%20and%20%283%29%20visibility%2C%20representing%20the%20different%20types%20of%0Atemporal%20duration%20of%20tool%20tracks.%20These%20fine-grained%20labels%20enhance%20tracking%0Aflexibility%20but%20also%20increase%20the%20task%20complexity.%20Re-identifying%20tools%20after%0Aocclusion%20or%20re-insertion%20into%20the%20body%20remains%20challenging%20due%20to%20high%20visual%0Asimilarity%2C%20especially%20among%20tools%20of%20the%20same%20category.%20This%20work%20recognizes%0Athe%20critical%20role%20of%20the%20tool%20operators%20in%20distinguishing%20tool%20track%20instances%2C%0Aespecially%20those%20belonging%20to%20the%20same%20tool%20category.%20The%20operators%27%0Ainformation%20are%20however%20not%20explicitly%20captured%20in%20surgical%20videos.%20We%0Atherefore%20propose%20SurgiTrack%2C%20a%20novel%20deep%20learning%20method%20that%20leverages%0AYOLOv7%20for%20precise%20tool%20detection%20and%20employs%20an%20attention%20mechanism%20to%20model%0Athe%20originating%20direction%20of%20the%20tools%2C%20as%20a%20proxy%20to%20their%20operators%2C%20for%20tool%0Are-identification.%20To%20handle%20diverse%20tool%20trajectory%20perspectives%2C%20SurgiTrack%0Aemploys%20a%20harmonizing%20bipartite%20matching%20graph%2C%20minimizing%20conflicts%20and%0Aensuring%20accurate%20tool%20identity%20association.%20Experimental%20results%20on%0ACholecTrack20%20demonstrate%20SurgiTrack%27s%20effectiveness%2C%20outperforming%20baselines%0Aand%20state-of-the-art%20methods%20with%20real-time%20inference%20capability.%20This%20work%0Asets%20a%20new%20standard%20in%20surgical%20tool%20tracking%2C%20providing%20dynamic%20trajectories%0Afor%20more%20adaptable%20and%20precise%20assistance%20in%20minimally%20invasive%20surgeries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgiTrack%253A%2520Fine-Grained%2520Multi-Class%2520Multi-Tool%2520Tracking%2520in%2520Surgical%250A%2520%2520Videos%26entry.906535625%3DChinedu%2520Innocent%2520Nwoye%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Accurate%2520tool%2520tracking%2520is%2520essential%2520for%2520the%2520success%2520of%2520computer-assisted%250Aintervention.%2520Previous%2520efforts%2520often%2520modeled%2520tool%2520trajectories%2520rigidly%252C%250Aoverlooking%2520the%2520dynamic%2520nature%2520of%2520surgical%2520procedures%252C%2520especially%2520tracking%250Ascenarios%2520like%2520out-of-body%2520and%2520out-of-camera%2520views.%2520Addressing%2520this%2520limitation%252C%250Athe%2520new%2520CholecTrack20%2520dataset%2520provides%2520detailed%2520labels%2520that%2520account%2520for%250Amultiple%2520tool%2520trajectories%2520in%2520three%2520perspectives%253A%2520%25281%2529%2520intraoperative%252C%2520%25282%2529%250Aintracorporeal%252C%2520and%2520%25283%2529%2520visibility%252C%2520representing%2520the%2520different%2520types%2520of%250Atemporal%2520duration%2520of%2520tool%2520tracks.%2520These%2520fine-grained%2520labels%2520enhance%2520tracking%250Aflexibility%2520but%2520also%2520increase%2520the%2520task%2520complexity.%2520Re-identifying%2520tools%2520after%250Aocclusion%2520or%2520re-insertion%2520into%2520the%2520body%2520remains%2520challenging%2520due%2520to%2520high%2520visual%250Asimilarity%252C%2520especially%2520among%2520tools%2520of%2520the%2520same%2520category.%2520This%2520work%2520recognizes%250Athe%2520critical%2520role%2520of%2520the%2520tool%2520operators%2520in%2520distinguishing%2520tool%2520track%2520instances%252C%250Aespecially%2520those%2520belonging%2520to%2520the%2520same%2520tool%2520category.%2520The%2520operators%2527%250Ainformation%2520are%2520however%2520not%2520explicitly%2520captured%2520in%2520surgical%2520videos.%2520We%250Atherefore%2520propose%2520SurgiTrack%252C%2520a%2520novel%2520deep%2520learning%2520method%2520that%2520leverages%250AYOLOv7%2520for%2520precise%2520tool%2520detection%2520and%2520employs%2520an%2520attention%2520mechanism%2520to%2520model%250Athe%2520originating%2520direction%2520of%2520the%2520tools%252C%2520as%2520a%2520proxy%2520to%2520their%2520operators%252C%2520for%2520tool%250Are-identification.%2520To%2520handle%2520diverse%2520tool%2520trajectory%2520perspectives%252C%2520SurgiTrack%250Aemploys%2520a%2520harmonizing%2520bipartite%2520matching%2520graph%252C%2520minimizing%2520conflicts%2520and%250Aensuring%2520accurate%2520tool%2520identity%2520association.%2520Experimental%2520results%2520on%250ACholecTrack20%2520demonstrate%2520SurgiTrack%2527s%2520effectiveness%252C%2520outperforming%2520baselines%250Aand%2520state-of-the-art%2520methods%2520with%2520real-time%2520inference%2520capability.%2520This%2520work%250Asets%2520a%2520new%2520standard%2520in%2520surgical%2520tool%2520tracking%252C%2520providing%2520dynamic%2520trajectories%250Afor%2520more%2520adaptable%2520and%2520precise%2520assistance%2520in%2520minimally%2520invasive%2520surgeries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgiTrack%3A%20Fine-Grained%20Multi-Class%20Multi-Tool%20Tracking%20in%20Surgical%0A%20%20Videos&entry.906535625=Chinedu%20Innocent%20Nwoye%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Accurate%20tool%20tracking%20is%20essential%20for%20the%20success%20of%20computer-assisted%0Aintervention.%20Previous%20efforts%20often%20modeled%20tool%20trajectories%20rigidly%2C%0Aoverlooking%20the%20dynamic%20nature%20of%20surgical%20procedures%2C%20especially%20tracking%0Ascenarios%20like%20out-of-body%20and%20out-of-camera%20views.%20Addressing%20this%20limitation%2C%0Athe%20new%20CholecTrack20%20dataset%20provides%20detailed%20labels%20that%20account%20for%0Amultiple%20tool%20trajectories%20in%20three%20perspectives%3A%20%281%29%20intraoperative%2C%20%282%29%0Aintracorporeal%2C%20and%20%283%29%20visibility%2C%20representing%20the%20different%20types%20of%0Atemporal%20duration%20of%20tool%20tracks.%20These%20fine-grained%20labels%20enhance%20tracking%0Aflexibility%20but%20also%20increase%20the%20task%20complexity.%20Re-identifying%20tools%20after%0Aocclusion%20or%20re-insertion%20into%20the%20body%20remains%20challenging%20due%20to%20high%20visual%0Asimilarity%2C%20especially%20among%20tools%20of%20the%20same%20category.%20This%20work%20recognizes%0Athe%20critical%20role%20of%20the%20tool%20operators%20in%20distinguishing%20tool%20track%20instances%2C%0Aespecially%20those%20belonging%20to%20the%20same%20tool%20category.%20The%20operators%27%0Ainformation%20are%20however%20not%20explicitly%20captured%20in%20surgical%20videos.%20We%0Atherefore%20propose%20SurgiTrack%2C%20a%20novel%20deep%20learning%20method%20that%20leverages%0AYOLOv7%20for%20precise%20tool%20detection%20and%20employs%20an%20attention%20mechanism%20to%20model%0Athe%20originating%20direction%20of%20the%20tools%2C%20as%20a%20proxy%20to%20their%20operators%2C%20for%20tool%0Are-identification.%20To%20handle%20diverse%20tool%20trajectory%20perspectives%2C%20SurgiTrack%0Aemploys%20a%20harmonizing%20bipartite%20matching%20graph%2C%20minimizing%20conflicts%20and%0Aensuring%20accurate%20tool%20identity%20association.%20Experimental%20results%20on%0ACholecTrack20%20demonstrate%20SurgiTrack%27s%20effectiveness%2C%20outperforming%20baselines%0Aand%20state-of-the-art%20methods%20with%20real-time%20inference%20capability.%20This%20work%0Asets%20a%20new%20standard%20in%20surgical%20tool%20tracking%2C%20providing%20dynamic%20trajectories%0Afor%20more%20adaptable%20and%20precise%20assistance%20in%20minimally%20invasive%20surgeries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20333v1&entry.124074799=Read"},
{"title": "Proof of Quality: A Costless Paradigm for Trustless Generative AI Model\n  Inference on Blockchains", "author": "Zhenjie Zhang and Yuyang Rao and Hao Xiao and Xiaokui Xiao and Yin Yang", "abstract": "  Generative AI models, such as GPT-4 and Stable Diffusion, have demonstrated\npowerful and disruptive capabilities in natural language and image tasks.\nHowever, deploying these models in decentralized environments remains\nchallenging. Unlike traditional centralized deployment, systematically\nguaranteeing the integrity of AI model services in fully decentralized\nenvironments, particularly on trustless blockchains, is both crucial and\ndifficult. In this paper, we present a new inference paradigm called\n\\emph{proof of quality} (PoQ) to enable the deployment of arbitrarily large\ngenerative models on blockchain architecture. Unlike traditional approaches\nbased on validating inference procedures, such as ZKML or OPML, our PoQ\nparadigm focuses on the outcome quality of model inference. Using lightweight\nBERT-based cross-encoders as our underlying quality evaluation model, we design\nand implement PQML, the first practical protocol for real-world NLP generative\nmodel inference on blockchains, tailored for popular open-source models such as\nLlama 3 and Mixtral. Our analysis demonstrates that our protocol is robust\nagainst adversarial but rational participants in ecosystems, where lazy or\ndishonest behavior results in fewer benefits compared to well-behaving\nparticipants. The computational overhead of validating the quality evaluation\nis minimal, allowing quality validators to complete the quality check within a\nsecond, even using only a CPU. Preliminary simulation results show that PoQ\nconsensus is generated in milliseconds, 1,000 times faster than any existing\nscheme.\n", "link": "http://arxiv.org/abs/2405.17934v2", "date": "2024-05-30", "relevancy": 1.9942, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5195}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proof%20of%20Quality%3A%20A%20Costless%20Paradigm%20for%20Trustless%20Generative%20AI%20Model%0A%20%20Inference%20on%20Blockchains&body=Title%3A%20Proof%20of%20Quality%3A%20A%20Costless%20Paradigm%20for%20Trustless%20Generative%20AI%20Model%0A%20%20Inference%20on%20Blockchains%0AAuthor%3A%20Zhenjie%20Zhang%20and%20Yuyang%20Rao%20and%20Hao%20Xiao%20and%20Xiaokui%20Xiao%20and%20Yin%20Yang%0AAbstract%3A%20%20%20Generative%20AI%20models%2C%20such%20as%20GPT-4%20and%20Stable%20Diffusion%2C%20have%20demonstrated%0Apowerful%20and%20disruptive%20capabilities%20in%20natural%20language%20and%20image%20tasks.%0AHowever%2C%20deploying%20these%20models%20in%20decentralized%20environments%20remains%0Achallenging.%20Unlike%20traditional%20centralized%20deployment%2C%20systematically%0Aguaranteeing%20the%20integrity%20of%20AI%20model%20services%20in%20fully%20decentralized%0Aenvironments%2C%20particularly%20on%20trustless%20blockchains%2C%20is%20both%20crucial%20and%0Adifficult.%20In%20this%20paper%2C%20we%20present%20a%20new%20inference%20paradigm%20called%0A%5Cemph%7Bproof%20of%20quality%7D%20%28PoQ%29%20to%20enable%20the%20deployment%20of%20arbitrarily%20large%0Agenerative%20models%20on%20blockchain%20architecture.%20Unlike%20traditional%20approaches%0Abased%20on%20validating%20inference%20procedures%2C%20such%20as%20ZKML%20or%20OPML%2C%20our%20PoQ%0Aparadigm%20focuses%20on%20the%20outcome%20quality%20of%20model%20inference.%20Using%20lightweight%0ABERT-based%20cross-encoders%20as%20our%20underlying%20quality%20evaluation%20model%2C%20we%20design%0Aand%20implement%20PQML%2C%20the%20first%20practical%20protocol%20for%20real-world%20NLP%20generative%0Amodel%20inference%20on%20blockchains%2C%20tailored%20for%20popular%20open-source%20models%20such%20as%0ALlama%203%20and%20Mixtral.%20Our%20analysis%20demonstrates%20that%20our%20protocol%20is%20robust%0Aagainst%20adversarial%20but%20rational%20participants%20in%20ecosystems%2C%20where%20lazy%20or%0Adishonest%20behavior%20results%20in%20fewer%20benefits%20compared%20to%20well-behaving%0Aparticipants.%20The%20computational%20overhead%20of%20validating%20the%20quality%20evaluation%0Ais%20minimal%2C%20allowing%20quality%20validators%20to%20complete%20the%20quality%20check%20within%20a%0Asecond%2C%20even%20using%20only%20a%20CPU.%20Preliminary%20simulation%20results%20show%20that%20PoQ%0Aconsensus%20is%20generated%20in%20milliseconds%2C%201%2C000%20times%20faster%20than%20any%20existing%0Ascheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17934v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProof%2520of%2520Quality%253A%2520A%2520Costless%2520Paradigm%2520for%2520Trustless%2520Generative%2520AI%2520Model%250A%2520%2520Inference%2520on%2520Blockchains%26entry.906535625%3DZhenjie%2520Zhang%2520and%2520Yuyang%2520Rao%2520and%2520Hao%2520Xiao%2520and%2520Xiaokui%2520Xiao%2520and%2520Yin%2520Yang%26entry.1292438233%3D%2520%2520Generative%2520AI%2520models%252C%2520such%2520as%2520GPT-4%2520and%2520Stable%2520Diffusion%252C%2520have%2520demonstrated%250Apowerful%2520and%2520disruptive%2520capabilities%2520in%2520natural%2520language%2520and%2520image%2520tasks.%250AHowever%252C%2520deploying%2520these%2520models%2520in%2520decentralized%2520environments%2520remains%250Achallenging.%2520Unlike%2520traditional%2520centralized%2520deployment%252C%2520systematically%250Aguaranteeing%2520the%2520integrity%2520of%2520AI%2520model%2520services%2520in%2520fully%2520decentralized%250Aenvironments%252C%2520particularly%2520on%2520trustless%2520blockchains%252C%2520is%2520both%2520crucial%2520and%250Adifficult.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520inference%2520paradigm%2520called%250A%255Cemph%257Bproof%2520of%2520quality%257D%2520%2528PoQ%2529%2520to%2520enable%2520the%2520deployment%2520of%2520arbitrarily%2520large%250Agenerative%2520models%2520on%2520blockchain%2520architecture.%2520Unlike%2520traditional%2520approaches%250Abased%2520on%2520validating%2520inference%2520procedures%252C%2520such%2520as%2520ZKML%2520or%2520OPML%252C%2520our%2520PoQ%250Aparadigm%2520focuses%2520on%2520the%2520outcome%2520quality%2520of%2520model%2520inference.%2520Using%2520lightweight%250ABERT-based%2520cross-encoders%2520as%2520our%2520underlying%2520quality%2520evaluation%2520model%252C%2520we%2520design%250Aand%2520implement%2520PQML%252C%2520the%2520first%2520practical%2520protocol%2520for%2520real-world%2520NLP%2520generative%250Amodel%2520inference%2520on%2520blockchains%252C%2520tailored%2520for%2520popular%2520open-source%2520models%2520such%2520as%250ALlama%25203%2520and%2520Mixtral.%2520Our%2520analysis%2520demonstrates%2520that%2520our%2520protocol%2520is%2520robust%250Aagainst%2520adversarial%2520but%2520rational%2520participants%2520in%2520ecosystems%252C%2520where%2520lazy%2520or%250Adishonest%2520behavior%2520results%2520in%2520fewer%2520benefits%2520compared%2520to%2520well-behaving%250Aparticipants.%2520The%2520computational%2520overhead%2520of%2520validating%2520the%2520quality%2520evaluation%250Ais%2520minimal%252C%2520allowing%2520quality%2520validators%2520to%2520complete%2520the%2520quality%2520check%2520within%2520a%250Asecond%252C%2520even%2520using%2520only%2520a%2520CPU.%2520Preliminary%2520simulation%2520results%2520show%2520that%2520PoQ%250Aconsensus%2520is%2520generated%2520in%2520milliseconds%252C%25201%252C000%2520times%2520faster%2520than%2520any%2520existing%250Ascheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17934v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proof%20of%20Quality%3A%20A%20Costless%20Paradigm%20for%20Trustless%20Generative%20AI%20Model%0A%20%20Inference%20on%20Blockchains&entry.906535625=Zhenjie%20Zhang%20and%20Yuyang%20Rao%20and%20Hao%20Xiao%20and%20Xiaokui%20Xiao%20and%20Yin%20Yang&entry.1292438233=%20%20Generative%20AI%20models%2C%20such%20as%20GPT-4%20and%20Stable%20Diffusion%2C%20have%20demonstrated%0Apowerful%20and%20disruptive%20capabilities%20in%20natural%20language%20and%20image%20tasks.%0AHowever%2C%20deploying%20these%20models%20in%20decentralized%20environments%20remains%0Achallenging.%20Unlike%20traditional%20centralized%20deployment%2C%20systematically%0Aguaranteeing%20the%20integrity%20of%20AI%20model%20services%20in%20fully%20decentralized%0Aenvironments%2C%20particularly%20on%20trustless%20blockchains%2C%20is%20both%20crucial%20and%0Adifficult.%20In%20this%20paper%2C%20we%20present%20a%20new%20inference%20paradigm%20called%0A%5Cemph%7Bproof%20of%20quality%7D%20%28PoQ%29%20to%20enable%20the%20deployment%20of%20arbitrarily%20large%0Agenerative%20models%20on%20blockchain%20architecture.%20Unlike%20traditional%20approaches%0Abased%20on%20validating%20inference%20procedures%2C%20such%20as%20ZKML%20or%20OPML%2C%20our%20PoQ%0Aparadigm%20focuses%20on%20the%20outcome%20quality%20of%20model%20inference.%20Using%20lightweight%0ABERT-based%20cross-encoders%20as%20our%20underlying%20quality%20evaluation%20model%2C%20we%20design%0Aand%20implement%20PQML%2C%20the%20first%20practical%20protocol%20for%20real-world%20NLP%20generative%0Amodel%20inference%20on%20blockchains%2C%20tailored%20for%20popular%20open-source%20models%20such%20as%0ALlama%203%20and%20Mixtral.%20Our%20analysis%20demonstrates%20that%20our%20protocol%20is%20robust%0Aagainst%20adversarial%20but%20rational%20participants%20in%20ecosystems%2C%20where%20lazy%20or%0Adishonest%20behavior%20results%20in%20fewer%20benefits%20compared%20to%20well-behaving%0Aparticipants.%20The%20computational%20overhead%20of%20validating%20the%20quality%20evaluation%0Ais%20minimal%2C%20allowing%20quality%20validators%20to%20complete%20the%20quality%20check%20within%20a%0Asecond%2C%20even%20using%20only%20a%20CPU.%20Preliminary%20simulation%20results%20show%20that%20PoQ%0Aconsensus%20is%20generated%20in%20milliseconds%2C%201%2C000%20times%20faster%20than%20any%20existing%0Ascheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17934v2&entry.124074799=Read"},
{"title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains", "author": "Junhong Shen and Neil Tenenholtz and James Brian Hall and David Alvarez-Melis and Nicolo Fusi", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating natural language. However, their capabilities wane\nin highly specialized domains underrepresented in the pretraining corpus, such\nas physical and biomedical sciences. This work explores how to repurpose\ngeneral LLMs into effective task solvers for specialized domains. We introduce\na novel, model-agnostic framework for learning custom input tags, which are\nparameterized as continuous vectors appended to the LLM's embedding layer, to\ncondition the LLM. We design two types of input tags: domain tags are used to\ndelimit specialized representations (e.g., chemical formulas) and provide\ndomain-relevant context; function tags are used to represent specific functions\n(e.g., predicting molecular properties) and compress function-solving\ninstructions. We develop a three-stage protocol to learn these tags using\nauxiliary data and domain knowledge. By explicitly disentangling task domains\nfrom task functions, our method enables zero-shot generalization to unseen\nproblems through diverse combinations of the input tags. It also boosts LLM's\nperformance in various specialized domains, such as predicting protein or\nchemical properties and modeling drug-target interactions, outperforming expert\nmodels tailored to these tasks.\n", "link": "http://arxiv.org/abs/2402.05140v2", "date": "2024-05-30", "relevancy": 1.9914, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.525}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4833}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tag-LLM%3A%20Repurposing%20General-Purpose%20LLMs%20for%20Specialized%20Domains&body=Title%3A%20Tag-LLM%3A%20Repurposing%20General-Purpose%20LLMs%20for%20Specialized%20Domains%0AAuthor%3A%20Junhong%20Shen%20and%20Neil%20Tenenholtz%20and%20James%20Brian%20Hall%20and%20David%20Alvarez-Melis%20and%20Nicolo%20Fusi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20proficiency%20in%0Aunderstanding%20and%20generating%20natural%20language.%20However%2C%20their%20capabilities%20wane%0Ain%20highly%20specialized%20domains%20underrepresented%20in%20the%20pretraining%20corpus%2C%20such%0Aas%20physical%20and%20biomedical%20sciences.%20This%20work%20explores%20how%20to%20repurpose%0Ageneral%20LLMs%20into%20effective%20task%20solvers%20for%20specialized%20domains.%20We%20introduce%0Aa%20novel%2C%20model-agnostic%20framework%20for%20learning%20custom%20input%20tags%2C%20which%20are%0Aparameterized%20as%20continuous%20vectors%20appended%20to%20the%20LLM%27s%20embedding%20layer%2C%20to%0Acondition%20the%20LLM.%20We%20design%20two%20types%20of%20input%20tags%3A%20domain%20tags%20are%20used%20to%0Adelimit%20specialized%20representations%20%28e.g.%2C%20chemical%20formulas%29%20and%20provide%0Adomain-relevant%20context%3B%20function%20tags%20are%20used%20to%20represent%20specific%20functions%0A%28e.g.%2C%20predicting%20molecular%20properties%29%20and%20compress%20function-solving%0Ainstructions.%20We%20develop%20a%20three-stage%20protocol%20to%20learn%20these%20tags%20using%0Aauxiliary%20data%20and%20domain%20knowledge.%20By%20explicitly%20disentangling%20task%20domains%0Afrom%20task%20functions%2C%20our%20method%20enables%20zero-shot%20generalization%20to%20unseen%0Aproblems%20through%20diverse%20combinations%20of%20the%20input%20tags.%20It%20also%20boosts%20LLM%27s%0Aperformance%20in%20various%20specialized%20domains%2C%20such%20as%20predicting%20protein%20or%0Achemical%20properties%20and%20modeling%20drug-target%20interactions%2C%20outperforming%20expert%0Amodels%20tailored%20to%20these%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTag-LLM%253A%2520Repurposing%2520General-Purpose%2520LLMs%2520for%2520Specialized%2520Domains%26entry.906535625%3DJunhong%2520Shen%2520and%2520Neil%2520Tenenholtz%2520and%2520James%2520Brian%2520Hall%2520and%2520David%2520Alvarez-Melis%2520and%2520Nicolo%2520Fusi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520proficiency%2520in%250Aunderstanding%2520and%2520generating%2520natural%2520language.%2520However%252C%2520their%2520capabilities%2520wane%250Ain%2520highly%2520specialized%2520domains%2520underrepresented%2520in%2520the%2520pretraining%2520corpus%252C%2520such%250Aas%2520physical%2520and%2520biomedical%2520sciences.%2520This%2520work%2520explores%2520how%2520to%2520repurpose%250Ageneral%2520LLMs%2520into%2520effective%2520task%2520solvers%2520for%2520specialized%2520domains.%2520We%2520introduce%250Aa%2520novel%252C%2520model-agnostic%2520framework%2520for%2520learning%2520custom%2520input%2520tags%252C%2520which%2520are%250Aparameterized%2520as%2520continuous%2520vectors%2520appended%2520to%2520the%2520LLM%2527s%2520embedding%2520layer%252C%2520to%250Acondition%2520the%2520LLM.%2520We%2520design%2520two%2520types%2520of%2520input%2520tags%253A%2520domain%2520tags%2520are%2520used%2520to%250Adelimit%2520specialized%2520representations%2520%2528e.g.%252C%2520chemical%2520formulas%2529%2520and%2520provide%250Adomain-relevant%2520context%253B%2520function%2520tags%2520are%2520used%2520to%2520represent%2520specific%2520functions%250A%2528e.g.%252C%2520predicting%2520molecular%2520properties%2529%2520and%2520compress%2520function-solving%250Ainstructions.%2520We%2520develop%2520a%2520three-stage%2520protocol%2520to%2520learn%2520these%2520tags%2520using%250Aauxiliary%2520data%2520and%2520domain%2520knowledge.%2520By%2520explicitly%2520disentangling%2520task%2520domains%250Afrom%2520task%2520functions%252C%2520our%2520method%2520enables%2520zero-shot%2520generalization%2520to%2520unseen%250Aproblems%2520through%2520diverse%2520combinations%2520of%2520the%2520input%2520tags.%2520It%2520also%2520boosts%2520LLM%2527s%250Aperformance%2520in%2520various%2520specialized%2520domains%252C%2520such%2520as%2520predicting%2520protein%2520or%250Achemical%2520properties%2520and%2520modeling%2520drug-target%2520interactions%252C%2520outperforming%2520expert%250Amodels%2520tailored%2520to%2520these%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tag-LLM%3A%20Repurposing%20General-Purpose%20LLMs%20for%20Specialized%20Domains&entry.906535625=Junhong%20Shen%20and%20Neil%20Tenenholtz%20and%20James%20Brian%20Hall%20and%20David%20Alvarez-Melis%20and%20Nicolo%20Fusi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20proficiency%20in%0Aunderstanding%20and%20generating%20natural%20language.%20However%2C%20their%20capabilities%20wane%0Ain%20highly%20specialized%20domains%20underrepresented%20in%20the%20pretraining%20corpus%2C%20such%0Aas%20physical%20and%20biomedical%20sciences.%20This%20work%20explores%20how%20to%20repurpose%0Ageneral%20LLMs%20into%20effective%20task%20solvers%20for%20specialized%20domains.%20We%20introduce%0Aa%20novel%2C%20model-agnostic%20framework%20for%20learning%20custom%20input%20tags%2C%20which%20are%0Aparameterized%20as%20continuous%20vectors%20appended%20to%20the%20LLM%27s%20embedding%20layer%2C%20to%0Acondition%20the%20LLM.%20We%20design%20two%20types%20of%20input%20tags%3A%20domain%20tags%20are%20used%20to%0Adelimit%20specialized%20representations%20%28e.g.%2C%20chemical%20formulas%29%20and%20provide%0Adomain-relevant%20context%3B%20function%20tags%20are%20used%20to%20represent%20specific%20functions%0A%28e.g.%2C%20predicting%20molecular%20properties%29%20and%20compress%20function-solving%0Ainstructions.%20We%20develop%20a%20three-stage%20protocol%20to%20learn%20these%20tags%20using%0Aauxiliary%20data%20and%20domain%20knowledge.%20By%20explicitly%20disentangling%20task%20domains%0Afrom%20task%20functions%2C%20our%20method%20enables%20zero-shot%20generalization%20to%20unseen%0Aproblems%20through%20diverse%20combinations%20of%20the%20input%20tags.%20It%20also%20boosts%20LLM%27s%0Aperformance%20in%20various%20specialized%20domains%2C%20such%20as%20predicting%20protein%20or%0Achemical%20properties%20and%20modeling%20drug-target%20interactions%2C%20outperforming%20expert%0Amodels%20tailored%20to%20these%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05140v2&entry.124074799=Read"},
{"title": "Improved Out-of-Scope Intent Classification with Dual Encoding and\n  Threshold-based Re-Classification", "author": "Hossam M. Zawbaa and Wael Rashwan and Sourav Dutta and Haytham Assem", "abstract": "  Detecting out-of-scope user utterances is essential for task-oriented\ndialogues and intent classification. Current methodologies face difficulties\nwith the unpredictable distribution of outliers and often rely on assumptions\nabout data distributions. We present the Dual Encoder for Threshold-Based\nRe-Classification (DETER) to address these challenges. This end-to-end\nframework efficiently detects out-of-scope intents without requiring\nassumptions on data distributions or additional post-processing steps. The core\nof DETER utilizes dual text encoders, the Universal Sentence Encoder (USE) and\nthe Transformer-based Denoising AutoEncoder (TSDAE), to generate user utterance\nembeddings, which are classified through a branched neural architecture.\nFurther, DETER generates synthetic outliers using self-supervision and\nincorporates out-of-scope phrases from open-domain datasets. This approach\nensures a comprehensive training set for out-of-scope detection. Additionally,\na threshold-based re-classification mechanism refines the model's initial\npredictions. Evaluations on the CLINC-150, Stackoverflow, and Banking77\ndatasets demonstrate DETER's efficacy. Our model outperforms previous\nbenchmarks, increasing up to 13% and 5% in F1 score for known and unknown\nintents on CLINC-150 and Stackoverflow, and 16% for known and 24% % for unknown\nintents on Banking77. The source code has been released at\nhttps://github.com/Hossam-Mohammed-tech/Intent\\_Classification\\_OOS.\n", "link": "http://arxiv.org/abs/2405.19967v1", "date": "2024-05-30", "relevancy": 1.4628, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.504}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4904}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Out-of-Scope%20Intent%20Classification%20with%20Dual%20Encoding%20and%0A%20%20Threshold-based%20Re-Classification&body=Title%3A%20Improved%20Out-of-Scope%20Intent%20Classification%20with%20Dual%20Encoding%20and%0A%20%20Threshold-based%20Re-Classification%0AAuthor%3A%20Hossam%20M.%20Zawbaa%20and%20Wael%20Rashwan%20and%20Sourav%20Dutta%20and%20Haytham%20Assem%0AAbstract%3A%20%20%20Detecting%20out-of-scope%20user%20utterances%20is%20essential%20for%20task-oriented%0Adialogues%20and%20intent%20classification.%20Current%20methodologies%20face%20difficulties%0Awith%20the%20unpredictable%20distribution%20of%20outliers%20and%20often%20rely%20on%20assumptions%0Aabout%20data%20distributions.%20We%20present%20the%20Dual%20Encoder%20for%20Threshold-Based%0ARe-Classification%20%28DETER%29%20to%20address%20these%20challenges.%20This%20end-to-end%0Aframework%20efficiently%20detects%20out-of-scope%20intents%20without%20requiring%0Aassumptions%20on%20data%20distributions%20or%20additional%20post-processing%20steps.%20The%20core%0Aof%20DETER%20utilizes%20dual%20text%20encoders%2C%20the%20Universal%20Sentence%20Encoder%20%28USE%29%20and%0Athe%20Transformer-based%20Denoising%20AutoEncoder%20%28TSDAE%29%2C%20to%20generate%20user%20utterance%0Aembeddings%2C%20which%20are%20classified%20through%20a%20branched%20neural%20architecture.%0AFurther%2C%20DETER%20generates%20synthetic%20outliers%20using%20self-supervision%20and%0Aincorporates%20out-of-scope%20phrases%20from%20open-domain%20datasets.%20This%20approach%0Aensures%20a%20comprehensive%20training%20set%20for%20out-of-scope%20detection.%20Additionally%2C%0Aa%20threshold-based%20re-classification%20mechanism%20refines%20the%20model%27s%20initial%0Apredictions.%20Evaluations%20on%20the%20CLINC-150%2C%20Stackoverflow%2C%20and%20Banking77%0Adatasets%20demonstrate%20DETER%27s%20efficacy.%20Our%20model%20outperforms%20previous%0Abenchmarks%2C%20increasing%20up%20to%2013%25%20and%205%25%20in%20F1%20score%20for%20known%20and%20unknown%0Aintents%20on%20CLINC-150%20and%20Stackoverflow%2C%20and%2016%25%20for%20known%20and%2024%25%20%25%20for%20unknown%0Aintents%20on%20Banking77.%20The%20source%20code%20has%20been%20released%20at%0Ahttps%3A//github.com/Hossam-Mohammed-tech/Intent%5C_Classification%5C_OOS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Out-of-Scope%2520Intent%2520Classification%2520with%2520Dual%2520Encoding%2520and%250A%2520%2520Threshold-based%2520Re-Classification%26entry.906535625%3DHossam%2520M.%2520Zawbaa%2520and%2520Wael%2520Rashwan%2520and%2520Sourav%2520Dutta%2520and%2520Haytham%2520Assem%26entry.1292438233%3D%2520%2520Detecting%2520out-of-scope%2520user%2520utterances%2520is%2520essential%2520for%2520task-oriented%250Adialogues%2520and%2520intent%2520classification.%2520Current%2520methodologies%2520face%2520difficulties%250Awith%2520the%2520unpredictable%2520distribution%2520of%2520outliers%2520and%2520often%2520rely%2520on%2520assumptions%250Aabout%2520data%2520distributions.%2520We%2520present%2520the%2520Dual%2520Encoder%2520for%2520Threshold-Based%250ARe-Classification%2520%2528DETER%2529%2520to%2520address%2520these%2520challenges.%2520This%2520end-to-end%250Aframework%2520efficiently%2520detects%2520out-of-scope%2520intents%2520without%2520requiring%250Aassumptions%2520on%2520data%2520distributions%2520or%2520additional%2520post-processing%2520steps.%2520The%2520core%250Aof%2520DETER%2520utilizes%2520dual%2520text%2520encoders%252C%2520the%2520Universal%2520Sentence%2520Encoder%2520%2528USE%2529%2520and%250Athe%2520Transformer-based%2520Denoising%2520AutoEncoder%2520%2528TSDAE%2529%252C%2520to%2520generate%2520user%2520utterance%250Aembeddings%252C%2520which%2520are%2520classified%2520through%2520a%2520branched%2520neural%2520architecture.%250AFurther%252C%2520DETER%2520generates%2520synthetic%2520outliers%2520using%2520self-supervision%2520and%250Aincorporates%2520out-of-scope%2520phrases%2520from%2520open-domain%2520datasets.%2520This%2520approach%250Aensures%2520a%2520comprehensive%2520training%2520set%2520for%2520out-of-scope%2520detection.%2520Additionally%252C%250Aa%2520threshold-based%2520re-classification%2520mechanism%2520refines%2520the%2520model%2527s%2520initial%250Apredictions.%2520Evaluations%2520on%2520the%2520CLINC-150%252C%2520Stackoverflow%252C%2520and%2520Banking77%250Adatasets%2520demonstrate%2520DETER%2527s%2520efficacy.%2520Our%2520model%2520outperforms%2520previous%250Abenchmarks%252C%2520increasing%2520up%2520to%252013%2525%2520and%25205%2525%2520in%2520F1%2520score%2520for%2520known%2520and%2520unknown%250Aintents%2520on%2520CLINC-150%2520and%2520Stackoverflow%252C%2520and%252016%2525%2520for%2520known%2520and%252024%2525%2520%2525%2520for%2520unknown%250Aintents%2520on%2520Banking77.%2520The%2520source%2520code%2520has%2520been%2520released%2520at%250Ahttps%253A//github.com/Hossam-Mohammed-tech/Intent%255C_Classification%255C_OOS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Out-of-Scope%20Intent%20Classification%20with%20Dual%20Encoding%20and%0A%20%20Threshold-based%20Re-Classification&entry.906535625=Hossam%20M.%20Zawbaa%20and%20Wael%20Rashwan%20and%20Sourav%20Dutta%20and%20Haytham%20Assem&entry.1292438233=%20%20Detecting%20out-of-scope%20user%20utterances%20is%20essential%20for%20task-oriented%0Adialogues%20and%20intent%20classification.%20Current%20methodologies%20face%20difficulties%0Awith%20the%20unpredictable%20distribution%20of%20outliers%20and%20often%20rely%20on%20assumptions%0Aabout%20data%20distributions.%20We%20present%20the%20Dual%20Encoder%20for%20Threshold-Based%0ARe-Classification%20%28DETER%29%20to%20address%20these%20challenges.%20This%20end-to-end%0Aframework%20efficiently%20detects%20out-of-scope%20intents%20without%20requiring%0Aassumptions%20on%20data%20distributions%20or%20additional%20post-processing%20steps.%20The%20core%0Aof%20DETER%20utilizes%20dual%20text%20encoders%2C%20the%20Universal%20Sentence%20Encoder%20%28USE%29%20and%0Athe%20Transformer-based%20Denoising%20AutoEncoder%20%28TSDAE%29%2C%20to%20generate%20user%20utterance%0Aembeddings%2C%20which%20are%20classified%20through%20a%20branched%20neural%20architecture.%0AFurther%2C%20DETER%20generates%20synthetic%20outliers%20using%20self-supervision%20and%0Aincorporates%20out-of-scope%20phrases%20from%20open-domain%20datasets.%20This%20approach%0Aensures%20a%20comprehensive%20training%20set%20for%20out-of-scope%20detection.%20Additionally%2C%0Aa%20threshold-based%20re-classification%20mechanism%20refines%20the%20model%27s%20initial%0Apredictions.%20Evaluations%20on%20the%20CLINC-150%2C%20Stackoverflow%2C%20and%20Banking77%0Adatasets%20demonstrate%20DETER%27s%20efficacy.%20Our%20model%20outperforms%20previous%0Abenchmarks%2C%20increasing%20up%20to%2013%25%20and%205%25%20in%20F1%20score%20for%20known%20and%20unknown%0Aintents%20on%20CLINC-150%20and%20Stackoverflow%2C%20and%2016%25%20for%20known%20and%2024%25%20%25%20for%20unknown%0Aintents%20on%20Banking77.%20The%20source%20code%20has%20been%20released%20at%0Ahttps%3A//github.com/Hossam-Mohammed-tech/Intent%5C_Classification%5C_OOS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19967v1&entry.124074799=Read"},
{"title": "You Need to Pay Better Attention: Rethinking the Mathematics of\n  Attention Mechanism", "author": "Mehran Hosseini and Peyman Hosseini", "abstract": "  Scaled Dot Product Attention (SDPA) is the backbone of many modern\ndeep-learning models. It is so versatile that it has been used in natural\nlanguage, vision, and multi-modal domains with very little change compared to\nits original formulation. This paper discusses why the current formulation is\ninefficient by delving into the mathematical details of the attention\nmechanism. We propose three improvements to mitigate these inefficiencies,\nthereby, introducing three enhanced attention mechanisms: Optimised, Efficient,\nand Super Attention. Optimised and Efficient Attention have one and two matrix\nmultiplications fewer per head, respectively, and 25% and 50% fewer parameters,\nrespectively, than standard SDPA, but perform similarly to standard SDPA in\nboth vision and natural language tasks. They can be used in all applications\nwhere SDPA is used while offering smaller model sizes and faster training and\ninference without noticeable loss in performance. Super Attention introduces a\nnew linear transformation on the values, transforming them from the left. It\noutperforms standard SPDA on vision and natural language tasks by up to 17%\nwhile having one fewer matrix multiplication per head and 25% fewer parameters\nthan standard SDPA. Consequently, it is also faster than standard SDPA. Super\nAttention is ideal in applications where the attention layer's context length\nis fixed, such as Vision Transformers. In addition to providing mathematical\nreasoning, we evaluate the presented attention mechanisms on several datasets\nincluding MNIST, CIFAR100, ImageNet, IMDB Movie Reviews, and Amazon Reviews\ndatasets, as well as combined Europarl and Anki English-Spanish datasets for\nneural machine translation.\n", "link": "http://arxiv.org/abs/2403.01643v2", "date": "2024-05-30", "relevancy": 1.6836, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5963}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5328}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Need%20to%20Pay%20Better%20Attention%3A%20Rethinking%20the%20Mathematics%20of%0A%20%20Attention%20Mechanism&body=Title%3A%20You%20Need%20to%20Pay%20Better%20Attention%3A%20Rethinking%20the%20Mathematics%20of%0A%20%20Attention%20Mechanism%0AAuthor%3A%20Mehran%20Hosseini%20and%20Peyman%20Hosseini%0AAbstract%3A%20%20%20Scaled%20Dot%20Product%20Attention%20%28SDPA%29%20is%20the%20backbone%20of%20many%20modern%0Adeep-learning%20models.%20It%20is%20so%20versatile%20that%20it%20has%20been%20used%20in%20natural%0Alanguage%2C%20vision%2C%20and%20multi-modal%20domains%20with%20very%20little%20change%20compared%20to%0Aits%20original%20formulation.%20This%20paper%20discusses%20why%20the%20current%20formulation%20is%0Ainefficient%20by%20delving%20into%20the%20mathematical%20details%20of%20the%20attention%0Amechanism.%20We%20propose%20three%20improvements%20to%20mitigate%20these%20inefficiencies%2C%0Athereby%2C%20introducing%20three%20enhanced%20attention%20mechanisms%3A%20Optimised%2C%20Efficient%2C%0Aand%20Super%20Attention.%20Optimised%20and%20Efficient%20Attention%20have%20one%20and%20two%20matrix%0Amultiplications%20fewer%20per%20head%2C%20respectively%2C%20and%2025%25%20and%2050%25%20fewer%20parameters%2C%0Arespectively%2C%20than%20standard%20SDPA%2C%20but%20perform%20similarly%20to%20standard%20SDPA%20in%0Aboth%20vision%20and%20natural%20language%20tasks.%20They%20can%20be%20used%20in%20all%20applications%0Awhere%20SDPA%20is%20used%20while%20offering%20smaller%20model%20sizes%20and%20faster%20training%20and%0Ainference%20without%20noticeable%20loss%20in%20performance.%20Super%20Attention%20introduces%20a%0Anew%20linear%20transformation%20on%20the%20values%2C%20transforming%20them%20from%20the%20left.%20It%0Aoutperforms%20standard%20SPDA%20on%20vision%20and%20natural%20language%20tasks%20by%20up%20to%2017%25%0Awhile%20having%20one%20fewer%20matrix%20multiplication%20per%20head%20and%2025%25%20fewer%20parameters%0Athan%20standard%20SDPA.%20Consequently%2C%20it%20is%20also%20faster%20than%20standard%20SDPA.%20Super%0AAttention%20is%20ideal%20in%20applications%20where%20the%20attention%20layer%27s%20context%20length%0Ais%20fixed%2C%20such%20as%20Vision%20Transformers.%20In%20addition%20to%20providing%20mathematical%0Areasoning%2C%20we%20evaluate%20the%20presented%20attention%20mechanisms%20on%20several%20datasets%0Aincluding%20MNIST%2C%20CIFAR100%2C%20ImageNet%2C%20IMDB%20Movie%20Reviews%2C%20and%20Amazon%20Reviews%0Adatasets%2C%20as%20well%20as%20combined%20Europarl%20and%20Anki%20English-Spanish%20datasets%20for%0Aneural%20machine%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Need%2520to%2520Pay%2520Better%2520Attention%253A%2520Rethinking%2520the%2520Mathematics%2520of%250A%2520%2520Attention%2520Mechanism%26entry.906535625%3DMehran%2520Hosseini%2520and%2520Peyman%2520Hosseini%26entry.1292438233%3D%2520%2520Scaled%2520Dot%2520Product%2520Attention%2520%2528SDPA%2529%2520is%2520the%2520backbone%2520of%2520many%2520modern%250Adeep-learning%2520models.%2520It%2520is%2520so%2520versatile%2520that%2520it%2520has%2520been%2520used%2520in%2520natural%250Alanguage%252C%2520vision%252C%2520and%2520multi-modal%2520domains%2520with%2520very%2520little%2520change%2520compared%2520to%250Aits%2520original%2520formulation.%2520This%2520paper%2520discusses%2520why%2520the%2520current%2520formulation%2520is%250Ainefficient%2520by%2520delving%2520into%2520the%2520mathematical%2520details%2520of%2520the%2520attention%250Amechanism.%2520We%2520propose%2520three%2520improvements%2520to%2520mitigate%2520these%2520inefficiencies%252C%250Athereby%252C%2520introducing%2520three%2520enhanced%2520attention%2520mechanisms%253A%2520Optimised%252C%2520Efficient%252C%250Aand%2520Super%2520Attention.%2520Optimised%2520and%2520Efficient%2520Attention%2520have%2520one%2520and%2520two%2520matrix%250Amultiplications%2520fewer%2520per%2520head%252C%2520respectively%252C%2520and%252025%2525%2520and%252050%2525%2520fewer%2520parameters%252C%250Arespectively%252C%2520than%2520standard%2520SDPA%252C%2520but%2520perform%2520similarly%2520to%2520standard%2520SDPA%2520in%250Aboth%2520vision%2520and%2520natural%2520language%2520tasks.%2520They%2520can%2520be%2520used%2520in%2520all%2520applications%250Awhere%2520SDPA%2520is%2520used%2520while%2520offering%2520smaller%2520model%2520sizes%2520and%2520faster%2520training%2520and%250Ainference%2520without%2520noticeable%2520loss%2520in%2520performance.%2520Super%2520Attention%2520introduces%2520a%250Anew%2520linear%2520transformation%2520on%2520the%2520values%252C%2520transforming%2520them%2520from%2520the%2520left.%2520It%250Aoutperforms%2520standard%2520SPDA%2520on%2520vision%2520and%2520natural%2520language%2520tasks%2520by%2520up%2520to%252017%2525%250Awhile%2520having%2520one%2520fewer%2520matrix%2520multiplication%2520per%2520head%2520and%252025%2525%2520fewer%2520parameters%250Athan%2520standard%2520SDPA.%2520Consequently%252C%2520it%2520is%2520also%2520faster%2520than%2520standard%2520SDPA.%2520Super%250AAttention%2520is%2520ideal%2520in%2520applications%2520where%2520the%2520attention%2520layer%2527s%2520context%2520length%250Ais%2520fixed%252C%2520such%2520as%2520Vision%2520Transformers.%2520In%2520addition%2520to%2520providing%2520mathematical%250Areasoning%252C%2520we%2520evaluate%2520the%2520presented%2520attention%2520mechanisms%2520on%2520several%2520datasets%250Aincluding%2520MNIST%252C%2520CIFAR100%252C%2520ImageNet%252C%2520IMDB%2520Movie%2520Reviews%252C%2520and%2520Amazon%2520Reviews%250Adatasets%252C%2520as%2520well%2520as%2520combined%2520Europarl%2520and%2520Anki%2520English-Spanish%2520datasets%2520for%250Aneural%2520machine%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Need%20to%20Pay%20Better%20Attention%3A%20Rethinking%20the%20Mathematics%20of%0A%20%20Attention%20Mechanism&entry.906535625=Mehran%20Hosseini%20and%20Peyman%20Hosseini&entry.1292438233=%20%20Scaled%20Dot%20Product%20Attention%20%28SDPA%29%20is%20the%20backbone%20of%20many%20modern%0Adeep-learning%20models.%20It%20is%20so%20versatile%20that%20it%20has%20been%20used%20in%20natural%0Alanguage%2C%20vision%2C%20and%20multi-modal%20domains%20with%20very%20little%20change%20compared%20to%0Aits%20original%20formulation.%20This%20paper%20discusses%20why%20the%20current%20formulation%20is%0Ainefficient%20by%20delving%20into%20the%20mathematical%20details%20of%20the%20attention%0Amechanism.%20We%20propose%20three%20improvements%20to%20mitigate%20these%20inefficiencies%2C%0Athereby%2C%20introducing%20three%20enhanced%20attention%20mechanisms%3A%20Optimised%2C%20Efficient%2C%0Aand%20Super%20Attention.%20Optimised%20and%20Efficient%20Attention%20have%20one%20and%20two%20matrix%0Amultiplications%20fewer%20per%20head%2C%20respectively%2C%20and%2025%25%20and%2050%25%20fewer%20parameters%2C%0Arespectively%2C%20than%20standard%20SDPA%2C%20but%20perform%20similarly%20to%20standard%20SDPA%20in%0Aboth%20vision%20and%20natural%20language%20tasks.%20They%20can%20be%20used%20in%20all%20applications%0Awhere%20SDPA%20is%20used%20while%20offering%20smaller%20model%20sizes%20and%20faster%20training%20and%0Ainference%20without%20noticeable%20loss%20in%20performance.%20Super%20Attention%20introduces%20a%0Anew%20linear%20transformation%20on%20the%20values%2C%20transforming%20them%20from%20the%20left.%20It%0Aoutperforms%20standard%20SPDA%20on%20vision%20and%20natural%20language%20tasks%20by%20up%20to%2017%25%0Awhile%20having%20one%20fewer%20matrix%20multiplication%20per%20head%20and%2025%25%20fewer%20parameters%0Athan%20standard%20SDPA.%20Consequently%2C%20it%20is%20also%20faster%20than%20standard%20SDPA.%20Super%0AAttention%20is%20ideal%20in%20applications%20where%20the%20attention%20layer%27s%20context%20length%0Ais%20fixed%2C%20such%20as%20Vision%20Transformers.%20In%20addition%20to%20providing%20mathematical%0Areasoning%2C%20we%20evaluate%20the%20presented%20attention%20mechanisms%20on%20several%20datasets%0Aincluding%20MNIST%2C%20CIFAR100%2C%20ImageNet%2C%20IMDB%20Movie%20Reviews%2C%20and%20Amazon%20Reviews%0Adatasets%2C%20as%20well%20as%20combined%20Europarl%20and%20Anki%20English-Spanish%20datasets%20for%0Aneural%20machine%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01643v2&entry.124074799=Read"},
{"title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned\n  Language Models", "author": "Siddharth Karamcheti and Suraj Nair and Ashwin Balakrishna and Percy Liang and Thomas Kollar and Dorsa Sadigh", "abstract": "  Visually-conditioned language models (VLMs) have seen growing adoption in\napplications such as visual dialogue, scene understanding, and robotic task\nplanning; adoption that has fueled a wealth of new models such as LLaVa,\nInstructBLIP, and PaLI-3. Despite the volume of new releases, key design\ndecisions around image preprocessing, architecture, and optimization are\nunder-explored, making it challenging to understand what factors account for\nmodel performance $-$ a challenge further complicated by the lack of objective,\nconsistent evaluations. To address these gaps, we first compile a suite of\nstandardized evaluations spanning visual question answering, object\nlocalization, and challenge sets that probe properties such as hallucination;\nevaluations that provide fine-grained insight VLM capabilities. Second, we\nrigorously investigate VLMs along key design axes, including pretrained visual\nrepresentations and training from base vs. instruct-tuned language models,\namongst others. We couple our analysis with three resource contributions: (1) a\nunified framework for evaluating VLMs, (2) optimized, flexible training code,\nand (3) checkpoints for all models, including a family of VLMs at the 7-13B\nscale that strictly outperform InstructBLIP and LLaVa v1.5, the\nstate-of-the-art in open VLMs.\n", "link": "http://arxiv.org/abs/2402.07865v2", "date": "2024-05-30", "relevancy": 1.7025, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5758}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5737}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prismatic%20VLMs%3A%20Investigating%20the%20Design%20Space%20of%20Visually-Conditioned%0A%20%20Language%20Models&body=Title%3A%20Prismatic%20VLMs%3A%20Investigating%20the%20Design%20Space%20of%20Visually-Conditioned%0A%20%20Language%20Models%0AAuthor%3A%20Siddharth%20Karamcheti%20and%20Suraj%20Nair%20and%20Ashwin%20Balakrishna%20and%20Percy%20Liang%20and%20Thomas%20Kollar%20and%20Dorsa%20Sadigh%0AAbstract%3A%20%20%20Visually-conditioned%20language%20models%20%28VLMs%29%20have%20seen%20growing%20adoption%20in%0Aapplications%20such%20as%20visual%20dialogue%2C%20scene%20understanding%2C%20and%20robotic%20task%0Aplanning%3B%20adoption%20that%20has%20fueled%20a%20wealth%20of%20new%20models%20such%20as%20LLaVa%2C%0AInstructBLIP%2C%20and%20PaLI-3.%20Despite%20the%20volume%20of%20new%20releases%2C%20key%20design%0Adecisions%20around%20image%20preprocessing%2C%20architecture%2C%20and%20optimization%20are%0Aunder-explored%2C%20making%20it%20challenging%20to%20understand%20what%20factors%20account%20for%0Amodel%20performance%20%24-%24%20a%20challenge%20further%20complicated%20by%20the%20lack%20of%20objective%2C%0Aconsistent%20evaluations.%20To%20address%20these%20gaps%2C%20we%20first%20compile%20a%20suite%20of%0Astandardized%20evaluations%20spanning%20visual%20question%20answering%2C%20object%0Alocalization%2C%20and%20challenge%20sets%20that%20probe%20properties%20such%20as%20hallucination%3B%0Aevaluations%20that%20provide%20fine-grained%20insight%20VLM%20capabilities.%20Second%2C%20we%0Arigorously%20investigate%20VLMs%20along%20key%20design%20axes%2C%20including%20pretrained%20visual%0Arepresentations%20and%20training%20from%20base%20vs.%20instruct-tuned%20language%20models%2C%0Aamongst%20others.%20We%20couple%20our%20analysis%20with%20three%20resource%20contributions%3A%20%281%29%20a%0Aunified%20framework%20for%20evaluating%20VLMs%2C%20%282%29%20optimized%2C%20flexible%20training%20code%2C%0Aand%20%283%29%20checkpoints%20for%20all%20models%2C%20including%20a%20family%20of%20VLMs%20at%20the%207-13B%0Ascale%20that%20strictly%20outperform%20InstructBLIP%20and%20LLaVa%20v1.5%2C%20the%0Astate-of-the-art%20in%20open%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07865v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrismatic%2520VLMs%253A%2520Investigating%2520the%2520Design%2520Space%2520of%2520Visually-Conditioned%250A%2520%2520Language%2520Models%26entry.906535625%3DSiddharth%2520Karamcheti%2520and%2520Suraj%2520Nair%2520and%2520Ashwin%2520Balakrishna%2520and%2520Percy%2520Liang%2520and%2520Thomas%2520Kollar%2520and%2520Dorsa%2520Sadigh%26entry.1292438233%3D%2520%2520Visually-conditioned%2520language%2520models%2520%2528VLMs%2529%2520have%2520seen%2520growing%2520adoption%2520in%250Aapplications%2520such%2520as%2520visual%2520dialogue%252C%2520scene%2520understanding%252C%2520and%2520robotic%2520task%250Aplanning%253B%2520adoption%2520that%2520has%2520fueled%2520a%2520wealth%2520of%2520new%2520models%2520such%2520as%2520LLaVa%252C%250AInstructBLIP%252C%2520and%2520PaLI-3.%2520Despite%2520the%2520volume%2520of%2520new%2520releases%252C%2520key%2520design%250Adecisions%2520around%2520image%2520preprocessing%252C%2520architecture%252C%2520and%2520optimization%2520are%250Aunder-explored%252C%2520making%2520it%2520challenging%2520to%2520understand%2520what%2520factors%2520account%2520for%250Amodel%2520performance%2520%2524-%2524%2520a%2520challenge%2520further%2520complicated%2520by%2520the%2520lack%2520of%2520objective%252C%250Aconsistent%2520evaluations.%2520To%2520address%2520these%2520gaps%252C%2520we%2520first%2520compile%2520a%2520suite%2520of%250Astandardized%2520evaluations%2520spanning%2520visual%2520question%2520answering%252C%2520object%250Alocalization%252C%2520and%2520challenge%2520sets%2520that%2520probe%2520properties%2520such%2520as%2520hallucination%253B%250Aevaluations%2520that%2520provide%2520fine-grained%2520insight%2520VLM%2520capabilities.%2520Second%252C%2520we%250Arigorously%2520investigate%2520VLMs%2520along%2520key%2520design%2520axes%252C%2520including%2520pretrained%2520visual%250Arepresentations%2520and%2520training%2520from%2520base%2520vs.%2520instruct-tuned%2520language%2520models%252C%250Aamongst%2520others.%2520We%2520couple%2520our%2520analysis%2520with%2520three%2520resource%2520contributions%253A%2520%25281%2529%2520a%250Aunified%2520framework%2520for%2520evaluating%2520VLMs%252C%2520%25282%2529%2520optimized%252C%2520flexible%2520training%2520code%252C%250Aand%2520%25283%2529%2520checkpoints%2520for%2520all%2520models%252C%2520including%2520a%2520family%2520of%2520VLMs%2520at%2520the%25207-13B%250Ascale%2520that%2520strictly%2520outperform%2520InstructBLIP%2520and%2520LLaVa%2520v1.5%252C%2520the%250Astate-of-the-art%2520in%2520open%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07865v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prismatic%20VLMs%3A%20Investigating%20the%20Design%20Space%20of%20Visually-Conditioned%0A%20%20Language%20Models&entry.906535625=Siddharth%20Karamcheti%20and%20Suraj%20Nair%20and%20Ashwin%20Balakrishna%20and%20Percy%20Liang%20and%20Thomas%20Kollar%20and%20Dorsa%20Sadigh&entry.1292438233=%20%20Visually-conditioned%20language%20models%20%28VLMs%29%20have%20seen%20growing%20adoption%20in%0Aapplications%20such%20as%20visual%20dialogue%2C%20scene%20understanding%2C%20and%20robotic%20task%0Aplanning%3B%20adoption%20that%20has%20fueled%20a%20wealth%20of%20new%20models%20such%20as%20LLaVa%2C%0AInstructBLIP%2C%20and%20PaLI-3.%20Despite%20the%20volume%20of%20new%20releases%2C%20key%20design%0Adecisions%20around%20image%20preprocessing%2C%20architecture%2C%20and%20optimization%20are%0Aunder-explored%2C%20making%20it%20challenging%20to%20understand%20what%20factors%20account%20for%0Amodel%20performance%20%24-%24%20a%20challenge%20further%20complicated%20by%20the%20lack%20of%20objective%2C%0Aconsistent%20evaluations.%20To%20address%20these%20gaps%2C%20we%20first%20compile%20a%20suite%20of%0Astandardized%20evaluations%20spanning%20visual%20question%20answering%2C%20object%0Alocalization%2C%20and%20challenge%20sets%20that%20probe%20properties%20such%20as%20hallucination%3B%0Aevaluations%20that%20provide%20fine-grained%20insight%20VLM%20capabilities.%20Second%2C%20we%0Arigorously%20investigate%20VLMs%20along%20key%20design%20axes%2C%20including%20pretrained%20visual%0Arepresentations%20and%20training%20from%20base%20vs.%20instruct-tuned%20language%20models%2C%0Aamongst%20others.%20We%20couple%20our%20analysis%20with%20three%20resource%20contributions%3A%20%281%29%20a%0Aunified%20framework%20for%20evaluating%20VLMs%2C%20%282%29%20optimized%2C%20flexible%20training%20code%2C%0Aand%20%283%29%20checkpoints%20for%20all%20models%2C%20including%20a%20family%20of%20VLMs%20at%20the%207-13B%0Ascale%20that%20strictly%20outperform%20InstructBLIP%20and%20LLaVa%20v1.5%2C%20the%0Astate-of-the-art%20in%20open%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07865v2&entry.124074799=Read"},
{"title": "Croissant: A Metadata Format for ML-Ready Datasets", "author": "Mubashara Akhtar and Omar Benjelloun and Costanza Conforti and Pieter Gijsbers and Joan Giner-Miguelez and Nitisha Jain and Michael Kuchnik and Quentin Lhoest and Pierre Marcenac and Manil Maskey and Peter Mattson and Luis Oala and Pierre Ruyssen and Rajat Shinde and Elena Simperl and Goeffry Thomas and Slava Tykhonov and Joaquin Vanschoren and Jos van der Velde and Steffen Vogler and Carole-Jean Wu", "abstract": "  Data is a critical resource for Machine Learning (ML), yet working with data\nremains a key friction point. This paper introduces Croissant, a metadata\nformat for datasets that simplifies how data is used by ML tools and\nframeworks. Croissant makes datasets more discoverable, portable and\ninteroperable, thereby addressing significant challenges in ML data management\nand responsible AI. Croissant is already supported by several popular dataset\nrepositories, spanning hundreds of thousands of datasets, ready to be loaded\ninto the most popular ML frameworks.\n", "link": "http://arxiv.org/abs/2403.19546v2", "date": "2024-05-30", "relevancy": 1.2535, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4298}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.408}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Croissant%3A%20A%20Metadata%20Format%20for%20ML-Ready%20Datasets&body=Title%3A%20Croissant%3A%20A%20Metadata%20Format%20for%20ML-Ready%20Datasets%0AAuthor%3A%20Mubashara%20Akhtar%20and%20Omar%20Benjelloun%20and%20Costanza%20Conforti%20and%20Pieter%20Gijsbers%20and%20Joan%20Giner-Miguelez%20and%20Nitisha%20Jain%20and%20Michael%20Kuchnik%20and%20Quentin%20Lhoest%20and%20Pierre%20Marcenac%20and%20Manil%20Maskey%20and%20Peter%20Mattson%20and%20Luis%20Oala%20and%20Pierre%20Ruyssen%20and%20Rajat%20Shinde%20and%20Elena%20Simperl%20and%20Goeffry%20Thomas%20and%20Slava%20Tykhonov%20and%20Joaquin%20Vanschoren%20and%20Jos%20van%20der%20Velde%20and%20Steffen%20Vogler%20and%20Carole-Jean%20Wu%0AAbstract%3A%20%20%20Data%20is%20a%20critical%20resource%20for%20Machine%20Learning%20%28ML%29%2C%20yet%20working%20with%20data%0Aremains%20a%20key%20friction%20point.%20This%20paper%20introduces%20Croissant%2C%20a%20metadata%0Aformat%20for%20datasets%20that%20simplifies%20how%20data%20is%20used%20by%20ML%20tools%20and%0Aframeworks.%20Croissant%20makes%20datasets%20more%20discoverable%2C%20portable%20and%0Ainteroperable%2C%20thereby%20addressing%20significant%20challenges%20in%20ML%20data%20management%0Aand%20responsible%20AI.%20Croissant%20is%20already%20supported%20by%20several%20popular%20dataset%0Arepositories%2C%20spanning%20hundreds%20of%20thousands%20of%20datasets%2C%20ready%20to%20be%20loaded%0Ainto%20the%20most%20popular%20ML%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCroissant%253A%2520A%2520Metadata%2520Format%2520for%2520ML-Ready%2520Datasets%26entry.906535625%3DMubashara%2520Akhtar%2520and%2520Omar%2520Benjelloun%2520and%2520Costanza%2520Conforti%2520and%2520Pieter%2520Gijsbers%2520and%2520Joan%2520Giner-Miguelez%2520and%2520Nitisha%2520Jain%2520and%2520Michael%2520Kuchnik%2520and%2520Quentin%2520Lhoest%2520and%2520Pierre%2520Marcenac%2520and%2520Manil%2520Maskey%2520and%2520Peter%2520Mattson%2520and%2520Luis%2520Oala%2520and%2520Pierre%2520Ruyssen%2520and%2520Rajat%2520Shinde%2520and%2520Elena%2520Simperl%2520and%2520Goeffry%2520Thomas%2520and%2520Slava%2520Tykhonov%2520and%2520Joaquin%2520Vanschoren%2520and%2520Jos%2520van%2520der%2520Velde%2520and%2520Steffen%2520Vogler%2520and%2520Carole-Jean%2520Wu%26entry.1292438233%3D%2520%2520Data%2520is%2520a%2520critical%2520resource%2520for%2520Machine%2520Learning%2520%2528ML%2529%252C%2520yet%2520working%2520with%2520data%250Aremains%2520a%2520key%2520friction%2520point.%2520This%2520paper%2520introduces%2520Croissant%252C%2520a%2520metadata%250Aformat%2520for%2520datasets%2520that%2520simplifies%2520how%2520data%2520is%2520used%2520by%2520ML%2520tools%2520and%250Aframeworks.%2520Croissant%2520makes%2520datasets%2520more%2520discoverable%252C%2520portable%2520and%250Ainteroperable%252C%2520thereby%2520addressing%2520significant%2520challenges%2520in%2520ML%2520data%2520management%250Aand%2520responsible%2520AI.%2520Croissant%2520is%2520already%2520supported%2520by%2520several%2520popular%2520dataset%250Arepositories%252C%2520spanning%2520hundreds%2520of%2520thousands%2520of%2520datasets%252C%2520ready%2520to%2520be%2520loaded%250Ainto%2520the%2520most%2520popular%2520ML%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Croissant%3A%20A%20Metadata%20Format%20for%20ML-Ready%20Datasets&entry.906535625=Mubashara%20Akhtar%20and%20Omar%20Benjelloun%20and%20Costanza%20Conforti%20and%20Pieter%20Gijsbers%20and%20Joan%20Giner-Miguelez%20and%20Nitisha%20Jain%20and%20Michael%20Kuchnik%20and%20Quentin%20Lhoest%20and%20Pierre%20Marcenac%20and%20Manil%20Maskey%20and%20Peter%20Mattson%20and%20Luis%20Oala%20and%20Pierre%20Ruyssen%20and%20Rajat%20Shinde%20and%20Elena%20Simperl%20and%20Goeffry%20Thomas%20and%20Slava%20Tykhonov%20and%20Joaquin%20Vanschoren%20and%20Jos%20van%20der%20Velde%20and%20Steffen%20Vogler%20and%20Carole-Jean%20Wu&entry.1292438233=%20%20Data%20is%20a%20critical%20resource%20for%20Machine%20Learning%20%28ML%29%2C%20yet%20working%20with%20data%0Aremains%20a%20key%20friction%20point.%20This%20paper%20introduces%20Croissant%2C%20a%20metadata%0Aformat%20for%20datasets%20that%20simplifies%20how%20data%20is%20used%20by%20ML%20tools%20and%0Aframeworks.%20Croissant%20makes%20datasets%20more%20discoverable%2C%20portable%20and%0Ainteroperable%2C%20thereby%20addressing%20significant%20challenges%20in%20ML%20data%20management%0Aand%20responsible%20AI.%20Croissant%20is%20already%20supported%20by%20several%20popular%20dataset%0Arepositories%2C%20spanning%20hundreds%20of%20thousands%20of%20datasets%2C%20ready%20to%20be%20loaded%0Ainto%20the%20most%20popular%20ML%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19546v2&entry.124074799=Read"},
{"title": "FMARS: Annotating Remote Sensing Images for Disaster Management using\n  Foundation Models", "author": "Edoardo Arnaudo and Jacopo Lungo Vaschetti and Lorenzo Innocenti and Luca Barco and Davide Lisi and Vanina Fissore and Claudio Rossi", "abstract": "  Very-High Resolution (VHR) remote sensing imagery is increasingly accessible,\nbut often lacks annotations for effective machine learning applications. Recent\nfoundation models like GroundingDINO and Segment Anything (SAM) provide\nopportunities to automatically generate annotations. This study introduces\nFMARS (Foundation Model Annotations in Remote Sensing), a methodology\nleveraging VHR imagery and foundation models for fast and robust annotation. We\nfocus on disaster management and provide a large-scale dataset with labels\nobtained from pre-event imagery over 19 disaster events, derived from the Maxar\nOpen Data initiative. We train segmentation models on the generated labels,\nusing Unsupervised Domain Adaptation (UDA) techniques to increase\ntransferability to real-world scenarios. Our results demonstrate the\neffectiveness of leveraging foundation models to automatically annotate remote\nsensing data at scale, enabling robust downstream models for critical\napplications. Code and dataset are available at\n\\url{https://github.com/links-ads/igarss-fmars}.\n", "link": "http://arxiv.org/abs/2405.20109v1", "date": "2024-05-30", "relevancy": 1.5256, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5244}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5084}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FMARS%3A%20Annotating%20Remote%20Sensing%20Images%20for%20Disaster%20Management%20using%0A%20%20Foundation%20Models&body=Title%3A%20FMARS%3A%20Annotating%20Remote%20Sensing%20Images%20for%20Disaster%20Management%20using%0A%20%20Foundation%20Models%0AAuthor%3A%20Edoardo%20Arnaudo%20and%20Jacopo%20Lungo%20Vaschetti%20and%20Lorenzo%20Innocenti%20and%20Luca%20Barco%20and%20Davide%20Lisi%20and%20Vanina%20Fissore%20and%20Claudio%20Rossi%0AAbstract%3A%20%20%20Very-High%20Resolution%20%28VHR%29%20remote%20sensing%20imagery%20is%20increasingly%20accessible%2C%0Abut%20often%20lacks%20annotations%20for%20effective%20machine%20learning%20applications.%20Recent%0Afoundation%20models%20like%20GroundingDINO%20and%20Segment%20Anything%20%28SAM%29%20provide%0Aopportunities%20to%20automatically%20generate%20annotations.%20This%20study%20introduces%0AFMARS%20%28Foundation%20Model%20Annotations%20in%20Remote%20Sensing%29%2C%20a%20methodology%0Aleveraging%20VHR%20imagery%20and%20foundation%20models%20for%20fast%20and%20robust%20annotation.%20We%0Afocus%20on%20disaster%20management%20and%20provide%20a%20large-scale%20dataset%20with%20labels%0Aobtained%20from%20pre-event%20imagery%20over%2019%20disaster%20events%2C%20derived%20from%20the%20Maxar%0AOpen%20Data%20initiative.%20We%20train%20segmentation%20models%20on%20the%20generated%20labels%2C%0Ausing%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20techniques%20to%20increase%0Atransferability%20to%20real-world%20scenarios.%20Our%20results%20demonstrate%20the%0Aeffectiveness%20of%20leveraging%20foundation%20models%20to%20automatically%20annotate%20remote%0Asensing%20data%20at%20scale%2C%20enabling%20robust%20downstream%20models%20for%20critical%0Aapplications.%20Code%20and%20dataset%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/links-ads/igarss-fmars%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFMARS%253A%2520Annotating%2520Remote%2520Sensing%2520Images%2520for%2520Disaster%2520Management%2520using%250A%2520%2520Foundation%2520Models%26entry.906535625%3DEdoardo%2520Arnaudo%2520and%2520Jacopo%2520Lungo%2520Vaschetti%2520and%2520Lorenzo%2520Innocenti%2520and%2520Luca%2520Barco%2520and%2520Davide%2520Lisi%2520and%2520Vanina%2520Fissore%2520and%2520Claudio%2520Rossi%26entry.1292438233%3D%2520%2520Very-High%2520Resolution%2520%2528VHR%2529%2520remote%2520sensing%2520imagery%2520is%2520increasingly%2520accessible%252C%250Abut%2520often%2520lacks%2520annotations%2520for%2520effective%2520machine%2520learning%2520applications.%2520Recent%250Afoundation%2520models%2520like%2520GroundingDINO%2520and%2520Segment%2520Anything%2520%2528SAM%2529%2520provide%250Aopportunities%2520to%2520automatically%2520generate%2520annotations.%2520This%2520study%2520introduces%250AFMARS%2520%2528Foundation%2520Model%2520Annotations%2520in%2520Remote%2520Sensing%2529%252C%2520a%2520methodology%250Aleveraging%2520VHR%2520imagery%2520and%2520foundation%2520models%2520for%2520fast%2520and%2520robust%2520annotation.%2520We%250Afocus%2520on%2520disaster%2520management%2520and%2520provide%2520a%2520large-scale%2520dataset%2520with%2520labels%250Aobtained%2520from%2520pre-event%2520imagery%2520over%252019%2520disaster%2520events%252C%2520derived%2520from%2520the%2520Maxar%250AOpen%2520Data%2520initiative.%2520We%2520train%2520segmentation%2520models%2520on%2520the%2520generated%2520labels%252C%250Ausing%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520techniques%2520to%2520increase%250Atransferability%2520to%2520real-world%2520scenarios.%2520Our%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520leveraging%2520foundation%2520models%2520to%2520automatically%2520annotate%2520remote%250Asensing%2520data%2520at%2520scale%252C%2520enabling%2520robust%2520downstream%2520models%2520for%2520critical%250Aapplications.%2520Code%2520and%2520dataset%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/links-ads/igarss-fmars%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FMARS%3A%20Annotating%20Remote%20Sensing%20Images%20for%20Disaster%20Management%20using%0A%20%20Foundation%20Models&entry.906535625=Edoardo%20Arnaudo%20and%20Jacopo%20Lungo%20Vaschetti%20and%20Lorenzo%20Innocenti%20and%20Luca%20Barco%20and%20Davide%20Lisi%20and%20Vanina%20Fissore%20and%20Claudio%20Rossi&entry.1292438233=%20%20Very-High%20Resolution%20%28VHR%29%20remote%20sensing%20imagery%20is%20increasingly%20accessible%2C%0Abut%20often%20lacks%20annotations%20for%20effective%20machine%20learning%20applications.%20Recent%0Afoundation%20models%20like%20GroundingDINO%20and%20Segment%20Anything%20%28SAM%29%20provide%0Aopportunities%20to%20automatically%20generate%20annotations.%20This%20study%20introduces%0AFMARS%20%28Foundation%20Model%20Annotations%20in%20Remote%20Sensing%29%2C%20a%20methodology%0Aleveraging%20VHR%20imagery%20and%20foundation%20models%20for%20fast%20and%20robust%20annotation.%20We%0Afocus%20on%20disaster%20management%20and%20provide%20a%20large-scale%20dataset%20with%20labels%0Aobtained%20from%20pre-event%20imagery%20over%2019%20disaster%20events%2C%20derived%20from%20the%20Maxar%0AOpen%20Data%20initiative.%20We%20train%20segmentation%20models%20on%20the%20generated%20labels%2C%0Ausing%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20techniques%20to%20increase%0Atransferability%20to%20real-world%20scenarios.%20Our%20results%20demonstrate%20the%0Aeffectiveness%20of%20leveraging%20foundation%20models%20to%20automatically%20annotate%20remote%0Asensing%20data%20at%20scale%2C%20enabling%20robust%20downstream%20models%20for%20critical%0Aapplications.%20Code%20and%20dataset%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/links-ads/igarss-fmars%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20109v1&entry.124074799=Read"},
{"title": "A Multimodal Dangerous State Recognition and Early Warning System for\n  Elderly with Intermittent Dementia", "author": "Liyun Deng and Lei Jin and Guangcheng Wang and Quan Shi and Han Wang", "abstract": "  In response to the social issue of the increasing number of elderly\nvulnerable groups going missing due to the aggravating aging population in\nChina, our team has developed a wearable anti-loss device and intelligent early\nwarning system for elderly individuals with intermittent dementia using\nartificial intelligence and IoT technology. This system comprises an anti-loss\nsmart helmet, a cloud computing module, and an intelligent early warning\napplication on the caregiver's mobile device. The smart helmet integrates a\nminiature camera module, a GPS module, and a 5G communication module to collect\nfirst-person images and location information of the elderly. Data is\ntransmitted remotely via 5G, FTP, and TCP protocols. In the cloud computing\nmodule, our team has proposed for the first time a multimodal dangerous state\nrecognition network based on scene and location information to accurately\nassess the risk of elderly individuals going missing. Finally, the application\nsoftware interface designed for the caregiver's mobile device implements\nmulti-level early warnings. The system developed by our team requires no\noperation or response from the elderly, achieving fully automatic environmental\nperception, risk assessment, and proactive alarming. This overcomes the\nlimitations of traditional monitoring devices, which require active operation\nand response, thus avoiding the issue of the digital divide for the elderly. It\neffectively prevents accidental loss and potential dangers for elderly\nindividuals with dementia.\n", "link": "http://arxiv.org/abs/2405.20136v1", "date": "2024-05-30", "relevancy": 1.9544, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.51}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Dangerous%20State%20Recognition%20and%20Early%20Warning%20System%20for%0A%20%20Elderly%20with%20Intermittent%20Dementia&body=Title%3A%20A%20Multimodal%20Dangerous%20State%20Recognition%20and%20Early%20Warning%20System%20for%0A%20%20Elderly%20with%20Intermittent%20Dementia%0AAuthor%3A%20Liyun%20Deng%20and%20Lei%20Jin%20and%20Guangcheng%20Wang%20and%20Quan%20Shi%20and%20Han%20Wang%0AAbstract%3A%20%20%20In%20response%20to%20the%20social%20issue%20of%20the%20increasing%20number%20of%20elderly%0Avulnerable%20groups%20going%20missing%20due%20to%20the%20aggravating%20aging%20population%20in%0AChina%2C%20our%20team%20has%20developed%20a%20wearable%20anti-loss%20device%20and%20intelligent%20early%0Awarning%20system%20for%20elderly%20individuals%20with%20intermittent%20dementia%20using%0Aartificial%20intelligence%20and%20IoT%20technology.%20This%20system%20comprises%20an%20anti-loss%0Asmart%20helmet%2C%20a%20cloud%20computing%20module%2C%20and%20an%20intelligent%20early%20warning%0Aapplication%20on%20the%20caregiver%27s%20mobile%20device.%20The%20smart%20helmet%20integrates%20a%0Aminiature%20camera%20module%2C%20a%20GPS%20module%2C%20and%20a%205G%20communication%20module%20to%20collect%0Afirst-person%20images%20and%20location%20information%20of%20the%20elderly.%20Data%20is%0Atransmitted%20remotely%20via%205G%2C%20FTP%2C%20and%20TCP%20protocols.%20In%20the%20cloud%20computing%0Amodule%2C%20our%20team%20has%20proposed%20for%20the%20first%20time%20a%20multimodal%20dangerous%20state%0Arecognition%20network%20based%20on%20scene%20and%20location%20information%20to%20accurately%0Aassess%20the%20risk%20of%20elderly%20individuals%20going%20missing.%20Finally%2C%20the%20application%0Asoftware%20interface%20designed%20for%20the%20caregiver%27s%20mobile%20device%20implements%0Amulti-level%20early%20warnings.%20The%20system%20developed%20by%20our%20team%20requires%20no%0Aoperation%20or%20response%20from%20the%20elderly%2C%20achieving%20fully%20automatic%20environmental%0Aperception%2C%20risk%20assessment%2C%20and%20proactive%20alarming.%20This%20overcomes%20the%0Alimitations%20of%20traditional%20monitoring%20devices%2C%20which%20require%20active%20operation%0Aand%20response%2C%20thus%20avoiding%20the%20issue%20of%20the%20digital%20divide%20for%20the%20elderly.%20It%0Aeffectively%20prevents%20accidental%20loss%20and%20potential%20dangers%20for%20elderly%0Aindividuals%20with%20dementia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Dangerous%2520State%2520Recognition%2520and%2520Early%2520Warning%2520System%2520for%250A%2520%2520Elderly%2520with%2520Intermittent%2520Dementia%26entry.906535625%3DLiyun%2520Deng%2520and%2520Lei%2520Jin%2520and%2520Guangcheng%2520Wang%2520and%2520Quan%2520Shi%2520and%2520Han%2520Wang%26entry.1292438233%3D%2520%2520In%2520response%2520to%2520the%2520social%2520issue%2520of%2520the%2520increasing%2520number%2520of%2520elderly%250Avulnerable%2520groups%2520going%2520missing%2520due%2520to%2520the%2520aggravating%2520aging%2520population%2520in%250AChina%252C%2520our%2520team%2520has%2520developed%2520a%2520wearable%2520anti-loss%2520device%2520and%2520intelligent%2520early%250Awarning%2520system%2520for%2520elderly%2520individuals%2520with%2520intermittent%2520dementia%2520using%250Aartificial%2520intelligence%2520and%2520IoT%2520technology.%2520This%2520system%2520comprises%2520an%2520anti-loss%250Asmart%2520helmet%252C%2520a%2520cloud%2520computing%2520module%252C%2520and%2520an%2520intelligent%2520early%2520warning%250Aapplication%2520on%2520the%2520caregiver%2527s%2520mobile%2520device.%2520The%2520smart%2520helmet%2520integrates%2520a%250Aminiature%2520camera%2520module%252C%2520a%2520GPS%2520module%252C%2520and%2520a%25205G%2520communication%2520module%2520to%2520collect%250Afirst-person%2520images%2520and%2520location%2520information%2520of%2520the%2520elderly.%2520Data%2520is%250Atransmitted%2520remotely%2520via%25205G%252C%2520FTP%252C%2520and%2520TCP%2520protocols.%2520In%2520the%2520cloud%2520computing%250Amodule%252C%2520our%2520team%2520has%2520proposed%2520for%2520the%2520first%2520time%2520a%2520multimodal%2520dangerous%2520state%250Arecognition%2520network%2520based%2520on%2520scene%2520and%2520location%2520information%2520to%2520accurately%250Aassess%2520the%2520risk%2520of%2520elderly%2520individuals%2520going%2520missing.%2520Finally%252C%2520the%2520application%250Asoftware%2520interface%2520designed%2520for%2520the%2520caregiver%2527s%2520mobile%2520device%2520implements%250Amulti-level%2520early%2520warnings.%2520The%2520system%2520developed%2520by%2520our%2520team%2520requires%2520no%250Aoperation%2520or%2520response%2520from%2520the%2520elderly%252C%2520achieving%2520fully%2520automatic%2520environmental%250Aperception%252C%2520risk%2520assessment%252C%2520and%2520proactive%2520alarming.%2520This%2520overcomes%2520the%250Alimitations%2520of%2520traditional%2520monitoring%2520devices%252C%2520which%2520require%2520active%2520operation%250Aand%2520response%252C%2520thus%2520avoiding%2520the%2520issue%2520of%2520the%2520digital%2520divide%2520for%2520the%2520elderly.%2520It%250Aeffectively%2520prevents%2520accidental%2520loss%2520and%2520potential%2520dangers%2520for%2520elderly%250Aindividuals%2520with%2520dementia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Dangerous%20State%20Recognition%20and%20Early%20Warning%20System%20for%0A%20%20Elderly%20with%20Intermittent%20Dementia&entry.906535625=Liyun%20Deng%20and%20Lei%20Jin%20and%20Guangcheng%20Wang%20and%20Quan%20Shi%20and%20Han%20Wang&entry.1292438233=%20%20In%20response%20to%20the%20social%20issue%20of%20the%20increasing%20number%20of%20elderly%0Avulnerable%20groups%20going%20missing%20due%20to%20the%20aggravating%20aging%20population%20in%0AChina%2C%20our%20team%20has%20developed%20a%20wearable%20anti-loss%20device%20and%20intelligent%20early%0Awarning%20system%20for%20elderly%20individuals%20with%20intermittent%20dementia%20using%0Aartificial%20intelligence%20and%20IoT%20technology.%20This%20system%20comprises%20an%20anti-loss%0Asmart%20helmet%2C%20a%20cloud%20computing%20module%2C%20and%20an%20intelligent%20early%20warning%0Aapplication%20on%20the%20caregiver%27s%20mobile%20device.%20The%20smart%20helmet%20integrates%20a%0Aminiature%20camera%20module%2C%20a%20GPS%20module%2C%20and%20a%205G%20communication%20module%20to%20collect%0Afirst-person%20images%20and%20location%20information%20of%20the%20elderly.%20Data%20is%0Atransmitted%20remotely%20via%205G%2C%20FTP%2C%20and%20TCP%20protocols.%20In%20the%20cloud%20computing%0Amodule%2C%20our%20team%20has%20proposed%20for%20the%20first%20time%20a%20multimodal%20dangerous%20state%0Arecognition%20network%20based%20on%20scene%20and%20location%20information%20to%20accurately%0Aassess%20the%20risk%20of%20elderly%20individuals%20going%20missing.%20Finally%2C%20the%20application%0Asoftware%20interface%20designed%20for%20the%20caregiver%27s%20mobile%20device%20implements%0Amulti-level%20early%20warnings.%20The%20system%20developed%20by%20our%20team%20requires%20no%0Aoperation%20or%20response%20from%20the%20elderly%2C%20achieving%20fully%20automatic%20environmental%0Aperception%2C%20risk%20assessment%2C%20and%20proactive%20alarming.%20This%20overcomes%20the%0Alimitations%20of%20traditional%20monitoring%20devices%2C%20which%20require%20active%20operation%0Aand%20response%2C%20thus%20avoiding%20the%20issue%20of%20the%20digital%20divide%20for%20the%20elderly.%20It%0Aeffectively%20prevents%20accidental%20loss%20and%20potential%20dangers%20for%20elderly%0Aindividuals%20with%20dementia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20136v1&entry.124074799=Read"},
{"title": "Would I Lie To You? Inference Time Alignment of Language Models using\n  Direct Preference Heads", "author": "Avelina Asada Hadji-Kyriacou and Ognjen Arandjelovic", "abstract": "  Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context\nlearning capabilities; however, their behaviors are often difficult to control.\nBy utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible\nto fine-tune unsupervised LMs to follow instructions and produce outputs that\nreflect human preferences. Despite its benefits, RLHF has been shown to\npotentially harm a language model's reasoning capabilities and introduce\nartifacts such as hallucinations where the model may fabricate facts. To\naddress this issue we introduce Direct Preference Heads (DPH), a fine-tuning\nframework that enables LMs to learn human preference signals through an\nauxiliary reward head without directly affecting the output distribution of the\nlanguage modeling head. We perform a theoretical analysis of our objective\nfunction and find strong ties to Conservative Direct Preference Optimization\n(cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All\nevaluation suite and demonstrate that our method produces models which achieve\nhigher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct\nPreference Optimization (DPO) alone.\n", "link": "http://arxiv.org/abs/2405.20053v1", "date": "2024-05-30", "relevancy": 1.4934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5034}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Would%20I%20Lie%20To%20You%3F%20Inference%20Time%20Alignment%20of%20Language%20Models%20using%0A%20%20Direct%20Preference%20Heads&body=Title%3A%20Would%20I%20Lie%20To%20You%3F%20Inference%20Time%20Alignment%20of%20Language%20Models%20using%0A%20%20Direct%20Preference%20Heads%0AAuthor%3A%20Avelina%20Asada%20Hadji-Kyriacou%20and%20Ognjen%20Arandjelovic%0AAbstract%3A%20%20%20Pre-trained%20Language%20Models%20%28LMs%29%20exhibit%20strong%20zero-shot%20and%20in-context%0Alearning%20capabilities%3B%20however%2C%20their%20behaviors%20are%20often%20difficult%20to%20control.%0ABy%20utilizing%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20it%20is%20possible%0Ato%20fine-tune%20unsupervised%20LMs%20to%20follow%20instructions%20and%20produce%20outputs%20that%0Areflect%20human%20preferences.%20Despite%20its%20benefits%2C%20RLHF%20has%20been%20shown%20to%0Apotentially%20harm%20a%20language%20model%27s%20reasoning%20capabilities%20and%20introduce%0Aartifacts%20such%20as%20hallucinations%20where%20the%20model%20may%20fabricate%20facts.%20To%0Aaddress%20this%20issue%20we%20introduce%20Direct%20Preference%20Heads%20%28DPH%29%2C%20a%20fine-tuning%0Aframework%20that%20enables%20LMs%20to%20learn%20human%20preference%20signals%20through%20an%0Aauxiliary%20reward%20head%20without%20directly%20affecting%20the%20output%20distribution%20of%20the%0Alanguage%20modeling%20head.%20We%20perform%20a%20theoretical%20analysis%20of%20our%20objective%0Afunction%20and%20find%20strong%20ties%20to%20Conservative%20Direct%20Preference%20Optimization%0A%28cDPO%29.%20Finally%20we%20evaluate%20our%20models%20on%20GLUE%2C%20RACE%2C%20and%20the%20GPT4All%0Aevaluation%20suite%20and%20demonstrate%20that%20our%20method%20produces%20models%20which%20achieve%0Ahigher%20scores%20than%20those%20fine-tuned%20with%20Supervised%20Fine-Tuning%20%28SFT%29%20or%20Direct%0APreference%20Optimization%20%28DPO%29%20alone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWould%2520I%2520Lie%2520To%2520You%253F%2520Inference%2520Time%2520Alignment%2520of%2520Language%2520Models%2520using%250A%2520%2520Direct%2520Preference%2520Heads%26entry.906535625%3DAvelina%2520Asada%2520Hadji-Kyriacou%2520and%2520Ognjen%2520Arandjelovic%26entry.1292438233%3D%2520%2520Pre-trained%2520Language%2520Models%2520%2528LMs%2529%2520exhibit%2520strong%2520zero-shot%2520and%2520in-context%250Alearning%2520capabilities%253B%2520however%252C%2520their%2520behaviors%2520are%2520often%2520difficult%2520to%2520control.%250ABy%2520utilizing%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%252C%2520it%2520is%2520possible%250Ato%2520fine-tune%2520unsupervised%2520LMs%2520to%2520follow%2520instructions%2520and%2520produce%2520outputs%2520that%250Areflect%2520human%2520preferences.%2520Despite%2520its%2520benefits%252C%2520RLHF%2520has%2520been%2520shown%2520to%250Apotentially%2520harm%2520a%2520language%2520model%2527s%2520reasoning%2520capabilities%2520and%2520introduce%250Aartifacts%2520such%2520as%2520hallucinations%2520where%2520the%2520model%2520may%2520fabricate%2520facts.%2520To%250Aaddress%2520this%2520issue%2520we%2520introduce%2520Direct%2520Preference%2520Heads%2520%2528DPH%2529%252C%2520a%2520fine-tuning%250Aframework%2520that%2520enables%2520LMs%2520to%2520learn%2520human%2520preference%2520signals%2520through%2520an%250Aauxiliary%2520reward%2520head%2520without%2520directly%2520affecting%2520the%2520output%2520distribution%2520of%2520the%250Alanguage%2520modeling%2520head.%2520We%2520perform%2520a%2520theoretical%2520analysis%2520of%2520our%2520objective%250Afunction%2520and%2520find%2520strong%2520ties%2520to%2520Conservative%2520Direct%2520Preference%2520Optimization%250A%2528cDPO%2529.%2520Finally%2520we%2520evaluate%2520our%2520models%2520on%2520GLUE%252C%2520RACE%252C%2520and%2520the%2520GPT4All%250Aevaluation%2520suite%2520and%2520demonstrate%2520that%2520our%2520method%2520produces%2520models%2520which%2520achieve%250Ahigher%2520scores%2520than%2520those%2520fine-tuned%2520with%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520or%2520Direct%250APreference%2520Optimization%2520%2528DPO%2529%2520alone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Would%20I%20Lie%20To%20You%3F%20Inference%20Time%20Alignment%20of%20Language%20Models%20using%0A%20%20Direct%20Preference%20Heads&entry.906535625=Avelina%20Asada%20Hadji-Kyriacou%20and%20Ognjen%20Arandjelovic&entry.1292438233=%20%20Pre-trained%20Language%20Models%20%28LMs%29%20exhibit%20strong%20zero-shot%20and%20in-context%0Alearning%20capabilities%3B%20however%2C%20their%20behaviors%20are%20often%20difficult%20to%20control.%0ABy%20utilizing%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20it%20is%20possible%0Ato%20fine-tune%20unsupervised%20LMs%20to%20follow%20instructions%20and%20produce%20outputs%20that%0Areflect%20human%20preferences.%20Despite%20its%20benefits%2C%20RLHF%20has%20been%20shown%20to%0Apotentially%20harm%20a%20language%20model%27s%20reasoning%20capabilities%20and%20introduce%0Aartifacts%20such%20as%20hallucinations%20where%20the%20model%20may%20fabricate%20facts.%20To%0Aaddress%20this%20issue%20we%20introduce%20Direct%20Preference%20Heads%20%28DPH%29%2C%20a%20fine-tuning%0Aframework%20that%20enables%20LMs%20to%20learn%20human%20preference%20signals%20through%20an%0Aauxiliary%20reward%20head%20without%20directly%20affecting%20the%20output%20distribution%20of%20the%0Alanguage%20modeling%20head.%20We%20perform%20a%20theoretical%20analysis%20of%20our%20objective%0Afunction%20and%20find%20strong%20ties%20to%20Conservative%20Direct%20Preference%20Optimization%0A%28cDPO%29.%20Finally%20we%20evaluate%20our%20models%20on%20GLUE%2C%20RACE%2C%20and%20the%20GPT4All%0Aevaluation%20suite%20and%20demonstrate%20that%20our%20method%20produces%20models%20which%20achieve%0Ahigher%20scores%20than%20those%20fine-tuned%20with%20Supervised%20Fine-Tuning%20%28SFT%29%20or%20Direct%0APreference%20Optimization%20%28DPO%29%20alone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20053v1&entry.124074799=Read"},
{"title": "Simultaneous identification of models and parameters of scientific\n  simulators", "author": "Cornelius Schr\u00f6der and Jakob H. Macke", "abstract": "  Many scientific models are composed of multiple discrete components, and\nscientists often make heuristic decisions about which components to include.\nBayesian inference provides a mathematical framework for systematically\nselecting model components, but defining prior distributions over model\ncomponents and developing associated inference schemes has been challenging. We\napproach this problem in a simulation-based inference framework: We define\nmodel priors over candidate components and, from model simulations, train\nneural networks to infer joint probability distributions over both model\ncomponents and associated parameters. Our method, simulation-based model\ninference (SBMI), represents distributions over model components as a\nconditional mixture of multivariate binary distributions in the Grassmann\nformalism. SBMI can be applied to any compositional stochastic simulator\nwithout requiring likelihood evaluations. We evaluate SBMI on a simple time\nseries model and on two scientific models from neuroscience, and show that it\ncan discover multiple data-consistent model configurations, and that it reveals\nnon-identifiable model components and parameters. SBMI provides a powerful tool\nfor data-driven scientific inquiry which will allow scientists to identify\nessential model components and make uncertainty-informed modelling decisions.\n", "link": "http://arxiv.org/abs/2305.15174v3", "date": "2024-05-30", "relevancy": 1.4619, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5742}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.477}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20identification%20of%20models%20and%20parameters%20of%20scientific%0A%20%20simulators&body=Title%3A%20Simultaneous%20identification%20of%20models%20and%20parameters%20of%20scientific%0A%20%20simulators%0AAuthor%3A%20Cornelius%20Schr%C3%B6der%20and%20Jakob%20H.%20Macke%0AAbstract%3A%20%20%20Many%20scientific%20models%20are%20composed%20of%20multiple%20discrete%20components%2C%20and%0Ascientists%20often%20make%20heuristic%20decisions%20about%20which%20components%20to%20include.%0ABayesian%20inference%20provides%20a%20mathematical%20framework%20for%20systematically%0Aselecting%20model%20components%2C%20but%20defining%20prior%20distributions%20over%20model%0Acomponents%20and%20developing%20associated%20inference%20schemes%20has%20been%20challenging.%20We%0Aapproach%20this%20problem%20in%20a%20simulation-based%20inference%20framework%3A%20We%20define%0Amodel%20priors%20over%20candidate%20components%20and%2C%20from%20model%20simulations%2C%20train%0Aneural%20networks%20to%20infer%20joint%20probability%20distributions%20over%20both%20model%0Acomponents%20and%20associated%20parameters.%20Our%20method%2C%20simulation-based%20model%0Ainference%20%28SBMI%29%2C%20represents%20distributions%20over%20model%20components%20as%20a%0Aconditional%20mixture%20of%20multivariate%20binary%20distributions%20in%20the%20Grassmann%0Aformalism.%20SBMI%20can%20be%20applied%20to%20any%20compositional%20stochastic%20simulator%0Awithout%20requiring%20likelihood%20evaluations.%20We%20evaluate%20SBMI%20on%20a%20simple%20time%0Aseries%20model%20and%20on%20two%20scientific%20models%20from%20neuroscience%2C%20and%20show%20that%20it%0Acan%20discover%20multiple%20data-consistent%20model%20configurations%2C%20and%20that%20it%20reveals%0Anon-identifiable%20model%20components%20and%20parameters.%20SBMI%20provides%20a%20powerful%20tool%0Afor%20data-driven%20scientific%20inquiry%20which%20will%20allow%20scientists%20to%20identify%0Aessential%20model%20components%20and%20make%20uncertainty-informed%20modelling%20decisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15174v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520identification%2520of%2520models%2520and%2520parameters%2520of%2520scientific%250A%2520%2520simulators%26entry.906535625%3DCornelius%2520Schr%25C3%25B6der%2520and%2520Jakob%2520H.%2520Macke%26entry.1292438233%3D%2520%2520Many%2520scientific%2520models%2520are%2520composed%2520of%2520multiple%2520discrete%2520components%252C%2520and%250Ascientists%2520often%2520make%2520heuristic%2520decisions%2520about%2520which%2520components%2520to%2520include.%250ABayesian%2520inference%2520provides%2520a%2520mathematical%2520framework%2520for%2520systematically%250Aselecting%2520model%2520components%252C%2520but%2520defining%2520prior%2520distributions%2520over%2520model%250Acomponents%2520and%2520developing%2520associated%2520inference%2520schemes%2520has%2520been%2520challenging.%2520We%250Aapproach%2520this%2520problem%2520in%2520a%2520simulation-based%2520inference%2520framework%253A%2520We%2520define%250Amodel%2520priors%2520over%2520candidate%2520components%2520and%252C%2520from%2520model%2520simulations%252C%2520train%250Aneural%2520networks%2520to%2520infer%2520joint%2520probability%2520distributions%2520over%2520both%2520model%250Acomponents%2520and%2520associated%2520parameters.%2520Our%2520method%252C%2520simulation-based%2520model%250Ainference%2520%2528SBMI%2529%252C%2520represents%2520distributions%2520over%2520model%2520components%2520as%2520a%250Aconditional%2520mixture%2520of%2520multivariate%2520binary%2520distributions%2520in%2520the%2520Grassmann%250Aformalism.%2520SBMI%2520can%2520be%2520applied%2520to%2520any%2520compositional%2520stochastic%2520simulator%250Awithout%2520requiring%2520likelihood%2520evaluations.%2520We%2520evaluate%2520SBMI%2520on%2520a%2520simple%2520time%250Aseries%2520model%2520and%2520on%2520two%2520scientific%2520models%2520from%2520neuroscience%252C%2520and%2520show%2520that%2520it%250Acan%2520discover%2520multiple%2520data-consistent%2520model%2520configurations%252C%2520and%2520that%2520it%2520reveals%250Anon-identifiable%2520model%2520components%2520and%2520parameters.%2520SBMI%2520provides%2520a%2520powerful%2520tool%250Afor%2520data-driven%2520scientific%2520inquiry%2520which%2520will%2520allow%2520scientists%2520to%2520identify%250Aessential%2520model%2520components%2520and%2520make%2520uncertainty-informed%2520modelling%2520decisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15174v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20identification%20of%20models%20and%20parameters%20of%20scientific%0A%20%20simulators&entry.906535625=Cornelius%20Schr%C3%B6der%20and%20Jakob%20H.%20Macke&entry.1292438233=%20%20Many%20scientific%20models%20are%20composed%20of%20multiple%20discrete%20components%2C%20and%0Ascientists%20often%20make%20heuristic%20decisions%20about%20which%20components%20to%20include.%0ABayesian%20inference%20provides%20a%20mathematical%20framework%20for%20systematically%0Aselecting%20model%20components%2C%20but%20defining%20prior%20distributions%20over%20model%0Acomponents%20and%20developing%20associated%20inference%20schemes%20has%20been%20challenging.%20We%0Aapproach%20this%20problem%20in%20a%20simulation-based%20inference%20framework%3A%20We%20define%0Amodel%20priors%20over%20candidate%20components%20and%2C%20from%20model%20simulations%2C%20train%0Aneural%20networks%20to%20infer%20joint%20probability%20distributions%20over%20both%20model%0Acomponents%20and%20associated%20parameters.%20Our%20method%2C%20simulation-based%20model%0Ainference%20%28SBMI%29%2C%20represents%20distributions%20over%20model%20components%20as%20a%0Aconditional%20mixture%20of%20multivariate%20binary%20distributions%20in%20the%20Grassmann%0Aformalism.%20SBMI%20can%20be%20applied%20to%20any%20compositional%20stochastic%20simulator%0Awithout%20requiring%20likelihood%20evaluations.%20We%20evaluate%20SBMI%20on%20a%20simple%20time%0Aseries%20model%20and%20on%20two%20scientific%20models%20from%20neuroscience%2C%20and%20show%20that%20it%0Acan%20discover%20multiple%20data-consistent%20model%20configurations%2C%20and%20that%20it%20reveals%0Anon-identifiable%20model%20components%20and%20parameters.%20SBMI%20provides%20a%20powerful%20tool%0Afor%20data-driven%20scientific%20inquiry%20which%20will%20allow%20scientists%20to%20identify%0Aessential%20model%20components%20and%20make%20uncertainty-informed%20modelling%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15174v3&entry.124074799=Read"},
{"title": "Unified Explanations in Machine Learning Models: A Perturbation Approach", "author": "Jacob Dineen and Don Kridel and Daniel Dolk and David Castillo", "abstract": "  A high-velocity paradigm shift towards Explainable Artificial Intelligence\n(XAI) has emerged in recent years. Highly complex Machine Learning (ML) models\nhave flourished in many tasks of intelligence, and the questions have started\nto shift away from traditional metrics of validity towards something deeper:\nWhat is this model telling me about my data, and how is it arriving at these\nconclusions? Inconsistencies between XAI and modeling techniques can have the\nundesirable effect of casting doubt upon the efficacy of these explainability\napproaches. To address these problems, we propose a systematic,\nperturbation-based analysis against a popular, model-agnostic method in XAI,\nSHapley Additive exPlanations (Shap). We devise algorithms to generate relative\nfeature importance in settings of dynamic inference amongst a suite of popular\nmachine learning and deep learning methods, and metrics that allow us to\nquantify how well explanations generated under the static case hold. We propose\na taxonomy for feature importance methodology, measure alignment, and observe\nquantifiable similarity amongst explanation models across several datasets.\n", "link": "http://arxiv.org/abs/2405.20200v1", "date": "2024-05-30", "relevancy": 1.4494, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4797}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Explanations%20in%20Machine%20Learning%20Models%3A%20A%20Perturbation%20Approach&body=Title%3A%20Unified%20Explanations%20in%20Machine%20Learning%20Models%3A%20A%20Perturbation%20Approach%0AAuthor%3A%20Jacob%20Dineen%20and%20Don%20Kridel%20and%20Daniel%20Dolk%20and%20David%20Castillo%0AAbstract%3A%20%20%20A%20high-velocity%20paradigm%20shift%20towards%20Explainable%20Artificial%20Intelligence%0A%28XAI%29%20has%20emerged%20in%20recent%20years.%20Highly%20complex%20Machine%20Learning%20%28ML%29%20models%0Ahave%20flourished%20in%20many%20tasks%20of%20intelligence%2C%20and%20the%20questions%20have%20started%0Ato%20shift%20away%20from%20traditional%20metrics%20of%20validity%20towards%20something%20deeper%3A%0AWhat%20is%20this%20model%20telling%20me%20about%20my%20data%2C%20and%20how%20is%20it%20arriving%20at%20these%0Aconclusions%3F%20Inconsistencies%20between%20XAI%20and%20modeling%20techniques%20can%20have%20the%0Aundesirable%20effect%20of%20casting%20doubt%20upon%20the%20efficacy%20of%20these%20explainability%0Aapproaches.%20To%20address%20these%20problems%2C%20we%20propose%20a%20systematic%2C%0Aperturbation-based%20analysis%20against%20a%20popular%2C%20model-agnostic%20method%20in%20XAI%2C%0ASHapley%20Additive%20exPlanations%20%28Shap%29.%20We%20devise%20algorithms%20to%20generate%20relative%0Afeature%20importance%20in%20settings%20of%20dynamic%20inference%20amongst%20a%20suite%20of%20popular%0Amachine%20learning%20and%20deep%20learning%20methods%2C%20and%20metrics%20that%20allow%20us%20to%0Aquantify%20how%20well%20explanations%20generated%20under%20the%20static%20case%20hold.%20We%20propose%0Aa%20taxonomy%20for%20feature%20importance%20methodology%2C%20measure%20alignment%2C%20and%20observe%0Aquantifiable%20similarity%20amongst%20explanation%20models%20across%20several%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Explanations%2520in%2520Machine%2520Learning%2520Models%253A%2520A%2520Perturbation%2520Approach%26entry.906535625%3DJacob%2520Dineen%2520and%2520Don%2520Kridel%2520and%2520Daniel%2520Dolk%2520and%2520David%2520Castillo%26entry.1292438233%3D%2520%2520A%2520high-velocity%2520paradigm%2520shift%2520towards%2520Explainable%2520Artificial%2520Intelligence%250A%2528XAI%2529%2520has%2520emerged%2520in%2520recent%2520years.%2520Highly%2520complex%2520Machine%2520Learning%2520%2528ML%2529%2520models%250Ahave%2520flourished%2520in%2520many%2520tasks%2520of%2520intelligence%252C%2520and%2520the%2520questions%2520have%2520started%250Ato%2520shift%2520away%2520from%2520traditional%2520metrics%2520of%2520validity%2520towards%2520something%2520deeper%253A%250AWhat%2520is%2520this%2520model%2520telling%2520me%2520about%2520my%2520data%252C%2520and%2520how%2520is%2520it%2520arriving%2520at%2520these%250Aconclusions%253F%2520Inconsistencies%2520between%2520XAI%2520and%2520modeling%2520techniques%2520can%2520have%2520the%250Aundesirable%2520effect%2520of%2520casting%2520doubt%2520upon%2520the%2520efficacy%2520of%2520these%2520explainability%250Aapproaches.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520systematic%252C%250Aperturbation-based%2520analysis%2520against%2520a%2520popular%252C%2520model-agnostic%2520method%2520in%2520XAI%252C%250ASHapley%2520Additive%2520exPlanations%2520%2528Shap%2529.%2520We%2520devise%2520algorithms%2520to%2520generate%2520relative%250Afeature%2520importance%2520in%2520settings%2520of%2520dynamic%2520inference%2520amongst%2520a%2520suite%2520of%2520popular%250Amachine%2520learning%2520and%2520deep%2520learning%2520methods%252C%2520and%2520metrics%2520that%2520allow%2520us%2520to%250Aquantify%2520how%2520well%2520explanations%2520generated%2520under%2520the%2520static%2520case%2520hold.%2520We%2520propose%250Aa%2520taxonomy%2520for%2520feature%2520importance%2520methodology%252C%2520measure%2520alignment%252C%2520and%2520observe%250Aquantifiable%2520similarity%2520amongst%2520explanation%2520models%2520across%2520several%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Explanations%20in%20Machine%20Learning%20Models%3A%20A%20Perturbation%20Approach&entry.906535625=Jacob%20Dineen%20and%20Don%20Kridel%20and%20Daniel%20Dolk%20and%20David%20Castillo&entry.1292438233=%20%20A%20high-velocity%20paradigm%20shift%20towards%20Explainable%20Artificial%20Intelligence%0A%28XAI%29%20has%20emerged%20in%20recent%20years.%20Highly%20complex%20Machine%20Learning%20%28ML%29%20models%0Ahave%20flourished%20in%20many%20tasks%20of%20intelligence%2C%20and%20the%20questions%20have%20started%0Ato%20shift%20away%20from%20traditional%20metrics%20of%20validity%20towards%20something%20deeper%3A%0AWhat%20is%20this%20model%20telling%20me%20about%20my%20data%2C%20and%20how%20is%20it%20arriving%20at%20these%0Aconclusions%3F%20Inconsistencies%20between%20XAI%20and%20modeling%20techniques%20can%20have%20the%0Aundesirable%20effect%20of%20casting%20doubt%20upon%20the%20efficacy%20of%20these%20explainability%0Aapproaches.%20To%20address%20these%20problems%2C%20we%20propose%20a%20systematic%2C%0Aperturbation-based%20analysis%20against%20a%20popular%2C%20model-agnostic%20method%20in%20XAI%2C%0ASHapley%20Additive%20exPlanations%20%28Shap%29.%20We%20devise%20algorithms%20to%20generate%20relative%0Afeature%20importance%20in%20settings%20of%20dynamic%20inference%20amongst%20a%20suite%20of%20popular%0Amachine%20learning%20and%20deep%20learning%20methods%2C%20and%20metrics%20that%20allow%20us%20to%0Aquantify%20how%20well%20explanations%20generated%20under%20the%20static%20case%20hold.%20We%20propose%0Aa%20taxonomy%20for%20feature%20importance%20methodology%2C%20measure%20alignment%2C%20and%20observe%0Aquantifiable%20similarity%20amongst%20explanation%20models%20across%20several%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20200v1&entry.124074799=Read"},
{"title": "Abstract Weighted Based Gradual Semantics in Argumentation Theory", "author": "Assaf Libman and Nir Oren and Bruno Yun", "abstract": "  Weighted gradual semantics provide an acceptability degree to each argument\nrepresenting the strength of the argument, computed based on factors including\nbackground evidence for the argument, and taking into account interactions\nbetween this argument and others. We introduce four important problems linking\ngradual semantics and acceptability degrees. First, we reexamine the inverse\nproblem, seeking to identify the argument weights of the argumentation\nframework which lead to a specific final acceptability degree. Second, we ask\nwhether the function mapping between argument weights and acceptability degrees\nis injective or a homeomorphism onto its image. Third, we ask whether argument\nweights can be found when preferences, rather than acceptability degrees for\narguments are considered. Fourth, we consider the topology of the space of\nvalid acceptability degrees, asking whether \"gaps\" exist in this space. While\ndifferent gradual semantics have been proposed in the literature, in this\npaper, we identify a large family of weighted gradual semantics, called\nabstract weighted based gradual semantics. These generalise many of the\nexisting semantics while maintaining desirable properties such as convergence\nto a unique fixed point. We also show that a sub-family of the weighted gradual\nsemantics, called abstract weighted (L^p,\\lambda,\\mu)-based gradual semantics\nand which include well-known semantics, solve all four of the aforementioned\nproblems.\n", "link": "http://arxiv.org/abs/2401.11472v2", "date": "2024-05-30", "relevancy": 1.2761, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4988}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4096}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstract%20Weighted%20Based%20Gradual%20Semantics%20in%20Argumentation%20Theory&body=Title%3A%20Abstract%20Weighted%20Based%20Gradual%20Semantics%20in%20Argumentation%20Theory%0AAuthor%3A%20Assaf%20Libman%20and%20Nir%20Oren%20and%20Bruno%20Yun%0AAbstract%3A%20%20%20Weighted%20gradual%20semantics%20provide%20an%20acceptability%20degree%20to%20each%20argument%0Arepresenting%20the%20strength%20of%20the%20argument%2C%20computed%20based%20on%20factors%20including%0Abackground%20evidence%20for%20the%20argument%2C%20and%20taking%20into%20account%20interactions%0Abetween%20this%20argument%20and%20others.%20We%20introduce%20four%20important%20problems%20linking%0Agradual%20semantics%20and%20acceptability%20degrees.%20First%2C%20we%20reexamine%20the%20inverse%0Aproblem%2C%20seeking%20to%20identify%20the%20argument%20weights%20of%20the%20argumentation%0Aframework%20which%20lead%20to%20a%20specific%20final%20acceptability%20degree.%20Second%2C%20we%20ask%0Awhether%20the%20function%20mapping%20between%20argument%20weights%20and%20acceptability%20degrees%0Ais%20injective%20or%20a%20homeomorphism%20onto%20its%20image.%20Third%2C%20we%20ask%20whether%20argument%0Aweights%20can%20be%20found%20when%20preferences%2C%20rather%20than%20acceptability%20degrees%20for%0Aarguments%20are%20considered.%20Fourth%2C%20we%20consider%20the%20topology%20of%20the%20space%20of%0Avalid%20acceptability%20degrees%2C%20asking%20whether%20%22gaps%22%20exist%20in%20this%20space.%20While%0Adifferent%20gradual%20semantics%20have%20been%20proposed%20in%20the%20literature%2C%20in%20this%0Apaper%2C%20we%20identify%20a%20large%20family%20of%20weighted%20gradual%20semantics%2C%20called%0Aabstract%20weighted%20based%20gradual%20semantics.%20These%20generalise%20many%20of%20the%0Aexisting%20semantics%20while%20maintaining%20desirable%20properties%20such%20as%20convergence%0Ato%20a%20unique%20fixed%20point.%20We%20also%20show%20that%20a%20sub-family%20of%20the%20weighted%20gradual%0Asemantics%2C%20called%20abstract%20weighted%20%28L%5Ep%2C%5Clambda%2C%5Cmu%29-based%20gradual%20semantics%0Aand%20which%20include%20well-known%20semantics%2C%20solve%20all%20four%20of%20the%20aforementioned%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstract%2520Weighted%2520Based%2520Gradual%2520Semantics%2520in%2520Argumentation%2520Theory%26entry.906535625%3DAssaf%2520Libman%2520and%2520Nir%2520Oren%2520and%2520Bruno%2520Yun%26entry.1292438233%3D%2520%2520Weighted%2520gradual%2520semantics%2520provide%2520an%2520acceptability%2520degree%2520to%2520each%2520argument%250Arepresenting%2520the%2520strength%2520of%2520the%2520argument%252C%2520computed%2520based%2520on%2520factors%2520including%250Abackground%2520evidence%2520for%2520the%2520argument%252C%2520and%2520taking%2520into%2520account%2520interactions%250Abetween%2520this%2520argument%2520and%2520others.%2520We%2520introduce%2520four%2520important%2520problems%2520linking%250Agradual%2520semantics%2520and%2520acceptability%2520degrees.%2520First%252C%2520we%2520reexamine%2520the%2520inverse%250Aproblem%252C%2520seeking%2520to%2520identify%2520the%2520argument%2520weights%2520of%2520the%2520argumentation%250Aframework%2520which%2520lead%2520to%2520a%2520specific%2520final%2520acceptability%2520degree.%2520Second%252C%2520we%2520ask%250Awhether%2520the%2520function%2520mapping%2520between%2520argument%2520weights%2520and%2520acceptability%2520degrees%250Ais%2520injective%2520or%2520a%2520homeomorphism%2520onto%2520its%2520image.%2520Third%252C%2520we%2520ask%2520whether%2520argument%250Aweights%2520can%2520be%2520found%2520when%2520preferences%252C%2520rather%2520than%2520acceptability%2520degrees%2520for%250Aarguments%2520are%2520considered.%2520Fourth%252C%2520we%2520consider%2520the%2520topology%2520of%2520the%2520space%2520of%250Avalid%2520acceptability%2520degrees%252C%2520asking%2520whether%2520%2522gaps%2522%2520exist%2520in%2520this%2520space.%2520While%250Adifferent%2520gradual%2520semantics%2520have%2520been%2520proposed%2520in%2520the%2520literature%252C%2520in%2520this%250Apaper%252C%2520we%2520identify%2520a%2520large%2520family%2520of%2520weighted%2520gradual%2520semantics%252C%2520called%250Aabstract%2520weighted%2520based%2520gradual%2520semantics.%2520These%2520generalise%2520many%2520of%2520the%250Aexisting%2520semantics%2520while%2520maintaining%2520desirable%2520properties%2520such%2520as%2520convergence%250Ato%2520a%2520unique%2520fixed%2520point.%2520We%2520also%2520show%2520that%2520a%2520sub-family%2520of%2520the%2520weighted%2520gradual%250Asemantics%252C%2520called%2520abstract%2520weighted%2520%2528L%255Ep%252C%255Clambda%252C%255Cmu%2529-based%2520gradual%2520semantics%250Aand%2520which%2520include%2520well-known%2520semantics%252C%2520solve%2520all%2520four%2520of%2520the%2520aforementioned%250Aproblems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstract%20Weighted%20Based%20Gradual%20Semantics%20in%20Argumentation%20Theory&entry.906535625=Assaf%20Libman%20and%20Nir%20Oren%20and%20Bruno%20Yun&entry.1292438233=%20%20Weighted%20gradual%20semantics%20provide%20an%20acceptability%20degree%20to%20each%20argument%0Arepresenting%20the%20strength%20of%20the%20argument%2C%20computed%20based%20on%20factors%20including%0Abackground%20evidence%20for%20the%20argument%2C%20and%20taking%20into%20account%20interactions%0Abetween%20this%20argument%20and%20others.%20We%20introduce%20four%20important%20problems%20linking%0Agradual%20semantics%20and%20acceptability%20degrees.%20First%2C%20we%20reexamine%20the%20inverse%0Aproblem%2C%20seeking%20to%20identify%20the%20argument%20weights%20of%20the%20argumentation%0Aframework%20which%20lead%20to%20a%20specific%20final%20acceptability%20degree.%20Second%2C%20we%20ask%0Awhether%20the%20function%20mapping%20between%20argument%20weights%20and%20acceptability%20degrees%0Ais%20injective%20or%20a%20homeomorphism%20onto%20its%20image.%20Third%2C%20we%20ask%20whether%20argument%0Aweights%20can%20be%20found%20when%20preferences%2C%20rather%20than%20acceptability%20degrees%20for%0Aarguments%20are%20considered.%20Fourth%2C%20we%20consider%20the%20topology%20of%20the%20space%20of%0Avalid%20acceptability%20degrees%2C%20asking%20whether%20%22gaps%22%20exist%20in%20this%20space.%20While%0Adifferent%20gradual%20semantics%20have%20been%20proposed%20in%20the%20literature%2C%20in%20this%0Apaper%2C%20we%20identify%20a%20large%20family%20of%20weighted%20gradual%20semantics%2C%20called%0Aabstract%20weighted%20based%20gradual%20semantics.%20These%20generalise%20many%20of%20the%0Aexisting%20semantics%20while%20maintaining%20desirable%20properties%20such%20as%20convergence%0Ato%20a%20unique%20fixed%20point.%20We%20also%20show%20that%20a%20sub-family%20of%20the%20weighted%20gradual%0Asemantics%2C%20called%20abstract%20weighted%20%28L%5Ep%2C%5Clambda%2C%5Cmu%29-based%20gradual%20semantics%0Aand%20which%20include%20well-known%20semantics%2C%20solve%20all%20four%20of%20the%20aforementioned%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11472v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


