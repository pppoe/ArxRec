<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240424.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Interactive3D: Create What You Want by Interactive 3D Generation", "author": "Shaocong Dong and Lihe Ding and Zhanpeng Huang and Zibin Wang and Tianfan Xue and Dan Xu", "abstract": "  3D object generation has undergone significant advancements, yielding\nhigh-quality results. However, fall short of achieving precise user control,\noften yielding results that do not align with user expectations, thus limiting\ntheir applicability. User-envisioning 3D object generation faces significant\nchallenges in realizing its concepts using current generative models due to\nlimited interaction capabilities. Existing methods mainly offer two approaches:\n(i) interpreting textual instructions with constrained controllability, or (ii)\nreconstructing 3D objects from 2D images. Both of them limit customization to\nthe confines of the 2D reference and potentially introduce undesirable\nartifacts during the 3D lifting process, restricting the scope for direct and\nversatile 3D modifications. In this work, we introduce Interactive3D, an\ninnovative framework for interactive 3D generation that grants users precise\ncontrol over the generative process through extensive 3D interaction\ncapabilities. Interactive3D is constructed in two cascading stages, utilizing\ndistinct 3D representations. The first stage employs Gaussian Splatting for\ndirect user interaction, allowing modifications and guidance of the generative\ndirection at any intermediate step through (i) Adding and Removing components,\n(ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv)\nSemantic Editing. Subsequently, the Gaussian splats are transformed into\nInstantNGP. We introduce a novel (v) Interactive Hash Refinement module to\nfurther add details and extract the geometry in the second stage. Our\nexperiments demonstrate that Interactive3D markedly improves the\ncontrollability and quality of 3D generation. Our project webpage is available\nat \\url{https://interactive-3d.github.io/}.\n", "link": "http://arxiv.org/abs/2404.16510v1", "date": "2024-04-25", "relevancy": 2.9267, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6117}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5733}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5711}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Interactive3D%3A%20Create%20What%20You%20Want%20by%20Interactive%203D%20Generation&body=Title%3A%20Interactive3D%3A%20Create%20What%20You%20Want%20by%20Interactive%203D%20Generation%0AAuthor%3A%20Shaocong%20Dong%20and%20Lihe%20Ding%20and%20Zhanpeng%20Huang%20and%20Zibin%20Wang%20and%20Tianfan%20Xue%20and%20Dan%20Xu%0AAbstract%3A%20%20%203D%20object%20generation%20has%20undergone%20significant%20advancements%2C%20yielding%0Ahigh-quality%20results.%20However%2C%20fall%20short%20of%20achieving%20precise%20user%20control%2C%0Aoften%20yielding%20results%20that%20do%20not%20align%20with%20user%20expectations%2C%20thus%20limiting%0Atheir%20applicability.%20User-envisioning%203D%20object%20generation%20faces%20significant%0Achallenges%20in%20realizing%20its%20concepts%20using%20current%20generative%20models%20due%20to%0Alimited%20interaction%20capabilities.%20Existing%20methods%20mainly%20offer%20two%20approaches%3A%0A%28i%29%20interpreting%20textual%20instructions%20with%20constrained%20controllability%2C%20or%20%28ii%29%0Areconstructing%203D%20objects%20from%202D%20images.%20Both%20of%20them%20limit%20customization%20to%0Athe%20confines%20of%20the%202D%20reference%20and%20potentially%20introduce%20undesirable%0Aartifacts%20during%20the%203D%20lifting%20process%2C%20restricting%20the%20scope%20for%20direct%20and%0Aversatile%203D%20modifications.%20In%20this%20work%2C%20we%20introduce%20Interactive3D%2C%20an%0Ainnovative%20framework%20for%20interactive%203D%20generation%20that%20grants%20users%20precise%0Acontrol%20over%20the%20generative%20process%20through%20extensive%203D%20interaction%0Acapabilities.%20Interactive3D%20is%20constructed%20in%20two%20cascading%20stages%2C%20utilizing%0Adistinct%203D%20representations.%20The%20first%20stage%20employs%20Gaussian%20Splatting%20for%0Adirect%20user%20interaction%2C%20allowing%20modifications%20and%20guidance%20of%20the%20generative%0Adirection%20at%20any%20intermediate%20step%20through%20%28i%29%20Adding%20and%20Removing%20components%2C%0A%28ii%29%20Deformable%20and%20Rigid%20Dragging%2C%20%28iii%29%20Geometric%20Transformations%2C%20and%20%28iv%29%0ASemantic%20Editing.%20Subsequently%2C%20the%20Gaussian%20splats%20are%20transformed%20into%0AInstantNGP.%20We%20introduce%20a%20novel%20%28v%29%20Interactive%20Hash%20Refinement%20module%20to%0Afurther%20add%20details%20and%20extract%20the%20geometry%20in%20the%20second%20stage.%20Our%0Aexperiments%20demonstrate%20that%20Interactive3D%20markedly%20improves%20the%0Acontrollability%20and%20quality%20of%203D%20generation.%20Our%20project%20webpage%20is%20available%0Aat%20%5Curl%7Bhttps%3A//interactive-3d.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16510v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive3D%3A%20Create%20What%20You%20Want%20by%20Interactive%203D%20Generation&entry.906535625=Shaocong%20Dong%20and%20Lihe%20Ding%20and%20Zhanpeng%20Huang%20and%20Zibin%20Wang%20and%20Tianfan%20Xue%20and%20Dan%20Xu&entry.1292438233=%20%203D%20object%20generation%20has%20undergone%20significant%20advancements%2C%20yielding%0Ahigh-quality%20results.%20However%2C%20fall%20short%20of%20achieving%20precise%20user%20control%2C%0Aoften%20yielding%20results%20that%20do%20not%20align%20with%20user%20expectations%2C%20thus%20limiting%0Atheir%20applicability.%20User-envisioning%203D%20object%20generation%20faces%20significant%0Achallenges%20in%20realizing%20its%20concepts%20using%20current%20generative%20models%20due%20to%0Alimited%20interaction%20capabilities.%20Existing%20methods%20mainly%20offer%20two%20approaches%3A%0A%28i%29%20interpreting%20textual%20instructions%20with%20constrained%20controllability%2C%20or%20%28ii%29%0Areconstructing%203D%20objects%20from%202D%20images.%20Both%20of%20them%20limit%20customization%20to%0Athe%20confines%20of%20the%202D%20reference%20and%20potentially%20introduce%20undesirable%0Aartifacts%20during%20the%203D%20lifting%20process%2C%20restricting%20the%20scope%20for%20direct%20and%0Aversatile%203D%20modifications.%20In%20this%20work%2C%20we%20introduce%20Interactive3D%2C%20an%0Ainnovative%20framework%20for%20interactive%203D%20generation%20that%20grants%20users%20precise%0Acontrol%20over%20the%20generative%20process%20through%20extensive%203D%20interaction%0Acapabilities.%20Interactive3D%20is%20constructed%20in%20two%20cascading%20stages%2C%20utilizing%0Adistinct%203D%20representations.%20The%20first%20stage%20employs%20Gaussian%20Splatting%20for%0Adirect%20user%20interaction%2C%20allowing%20modifications%20and%20guidance%20of%20the%20generative%0Adirection%20at%20any%20intermediate%20step%20through%20%28i%29%20Adding%20and%20Removing%20components%2C%0A%28ii%29%20Deformable%20and%20Rigid%20Dragging%2C%20%28iii%29%20Geometric%20Transformations%2C%20and%20%28iv%29%0ASemantic%20Editing.%20Subsequently%2C%20the%20Gaussian%20splats%20are%20transformed%20into%0AInstantNGP.%20We%20introduce%20a%20novel%20%28v%29%20Interactive%20Hash%20Refinement%20module%20to%0Afurther%20add%20details%20and%20extract%20the%20geometry%20in%20the%20second%20stage.%20Our%0Aexperiments%20demonstrate%20that%20Interactive3D%20markedly%20improves%20the%0Acontrollability%20and%20quality%20of%203D%20generation.%20Our%20project%20webpage%20is%20available%0Aat%20%5Curl%7Bhttps%3A//interactive-3d.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16510v1&entry.124074799=Read"},
{"title": "Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in\n  Autonomous Vehicle Perception System", "author": "Daniel Dworak and Mateusz Komorkiewicz and Pawe\u0142 Skruch and Jerzy Baranowski", "abstract": "  In this paper, we propose a novel approach to address the problem of camera\nand radar sensor fusion for 3D object detection in autonomous vehicle\nperception systems. Our approach builds on recent advances in deep learning and\nleverages the strengths of both sensors to improve object detection\nperformance. Precisely, we extract 2D features from camera images using a\nstate-of-the-art deep learning architecture and then apply a novel Cross-Domain\nSpatial Matching (CDSM) transformation method to convert these features into 3D\nspace. We then fuse them with extracted radar data using a complementary fusion\nstrategy to produce a final 3D object representation. To demonstrate the\neffectiveness of our approach, we evaluate it on the NuScenes dataset. We\ncompare our approach to both single-sensor performance and current\nstate-of-the-art fusion methods. Our results show that the proposed approach\nachieves superior performance over single-sensor solutions and could directly\ncompete with other top-level fusion methods.\n", "link": "http://arxiv.org/abs/2404.16548v1", "date": "2024-04-25", "relevancy": 2.864, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5981}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Spatial%20Matching%20for%20Camera%20and%20Radar%20Sensor%20Data%20Fusion%20in%0A%20%20Autonomous%20Vehicle%20Perception%20System&body=Title%3A%20Cross-Domain%20Spatial%20Matching%20for%20Camera%20and%20Radar%20Sensor%20Data%20Fusion%20in%0A%20%20Autonomous%20Vehicle%20Perception%20System%0AAuthor%3A%20Daniel%20Dworak%20and%20Mateusz%20Komorkiewicz%20and%20Pawe%C5%82%20Skruch%20and%20Jerzy%20Baranowski%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20address%20the%20problem%20of%20camera%0Aand%20radar%20sensor%20fusion%20for%203D%20object%20detection%20in%20autonomous%20vehicle%0Aperception%20systems.%20Our%20approach%20builds%20on%20recent%20advances%20in%20deep%20learning%20and%0Aleverages%20the%20strengths%20of%20both%20sensors%20to%20improve%20object%20detection%0Aperformance.%20Precisely%2C%20we%20extract%202D%20features%20from%20camera%20images%20using%20a%0Astate-of-the-art%20deep%20learning%20architecture%20and%20then%20apply%20a%20novel%20Cross-Domain%0ASpatial%20Matching%20%28CDSM%29%20transformation%20method%20to%20convert%20these%20features%20into%203D%0Aspace.%20We%20then%20fuse%20them%20with%20extracted%20radar%20data%20using%20a%20complementary%20fusion%0Astrategy%20to%20produce%20a%20final%203D%20object%20representation.%20To%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%2C%20we%20evaluate%20it%20on%20the%20NuScenes%20dataset.%20We%0Acompare%20our%20approach%20to%20both%20single-sensor%20performance%20and%20current%0Astate-of-the-art%20fusion%20methods.%20Our%20results%20show%20that%20the%20proposed%20approach%0Aachieves%20superior%20performance%20over%20single-sensor%20solutions%20and%20could%20directly%0Acompete%20with%20other%20top-level%20fusion%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16548v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Spatial%20Matching%20for%20Camera%20and%20Radar%20Sensor%20Data%20Fusion%20in%0A%20%20Autonomous%20Vehicle%20Perception%20System&entry.906535625=Daniel%20Dworak%20and%20Mateusz%20Komorkiewicz%20and%20Pawe%C5%82%20Skruch%20and%20Jerzy%20Baranowski&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20address%20the%20problem%20of%20camera%0Aand%20radar%20sensor%20fusion%20for%203D%20object%20detection%20in%20autonomous%20vehicle%0Aperception%20systems.%20Our%20approach%20builds%20on%20recent%20advances%20in%20deep%20learning%20and%0Aleverages%20the%20strengths%20of%20both%20sensors%20to%20improve%20object%20detection%0Aperformance.%20Precisely%2C%20we%20extract%202D%20features%20from%20camera%20images%20using%20a%0Astate-of-the-art%20deep%20learning%20architecture%20and%20then%20apply%20a%20novel%20Cross-Domain%0ASpatial%20Matching%20%28CDSM%29%20transformation%20method%20to%20convert%20these%20features%20into%203D%0Aspace.%20We%20then%20fuse%20them%20with%20extracted%20radar%20data%20using%20a%20complementary%20fusion%0Astrategy%20to%20produce%20a%20final%203D%20object%20representation.%20To%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%2C%20we%20evaluate%20it%20on%20the%20NuScenes%20dataset.%20We%0Acompare%20our%20approach%20to%20both%20single-sensor%20performance%20and%20current%0Astate-of-the-art%20fusion%20methods.%20Our%20results%20show%20that%20the%20proposed%20approach%0Aachieves%20superior%20performance%20over%20single-sensor%20solutions%20and%20could%20directly%0Acompete%20with%20other%20top-level%20fusion%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16548v1&entry.124074799=Read"},
{"title": "CoCoG: Controllable Visual Stimuli Generation based on Human Concept\n  Representations", "author": "Chen Wei and Jiachen Zou and Dietmar Heinke and Quanying Liu", "abstract": "  A central question for cognitive science is to understand how humans process\nvisual objects, i.e, to uncover human low-dimensional concept representation\nspace from high-dimensional visual stimuli. Generating visual stimuli with\ncontrolling concepts is the key. However, there are currently no generative\nmodels in AI to solve this problem. Here, we present the Concept based\nControllable Generation (CoCoG) framework. CoCoG consists of two components, a\nsimple yet efficient AI agent for extracting interpretable concept and\npredicting human decision-making in visual similarity judgment tasks, and a\nconditional generation model for generating visual stimuli given the concepts.\nWe quantify the performance of CoCoG from two aspects, the human behavior\nprediction accuracy and the controllable generation ability. The experiments\nwith CoCoG indicate that 1) the reliable concept embeddings in CoCoG allows to\npredict human behavior with 64.07\\% accuracy in the THINGS-similarity dataset;\n2) CoCoG can generate diverse objects through the control of concepts; 3) CoCoG\ncan manipulate human similarity judgment behavior by intervening key concepts.\nCoCoG offers visual objects with controlling concepts to advance our\nunderstanding of causality in human cognition. The code of CoCoG is available\nat \\url{https://github.com/ncclab-sustech/CoCoG}.\n", "link": "http://arxiv.org/abs/2404.16482v1", "date": "2024-04-25", "relevancy": 2.8248, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5866}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5595}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5487}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CoCoG%3A%20Controllable%20Visual%20Stimuli%20Generation%20based%20on%20Human%20Concept%0A%20%20Representations&body=Title%3A%20CoCoG%3A%20Controllable%20Visual%20Stimuli%20Generation%20based%20on%20Human%20Concept%0A%20%20Representations%0AAuthor%3A%20Chen%20Wei%20and%20Jiachen%20Zou%20and%20Dietmar%20Heinke%20and%20Quanying%20Liu%0AAbstract%3A%20%20%20A%20central%20question%20for%20cognitive%20science%20is%20to%20understand%20how%20humans%20process%0Avisual%20objects%2C%20i.e%2C%20to%20uncover%20human%20low-dimensional%20concept%20representation%0Aspace%20from%20high-dimensional%20visual%20stimuli.%20Generating%20visual%20stimuli%20with%0Acontrolling%20concepts%20is%20the%20key.%20However%2C%20there%20are%20currently%20no%20generative%0Amodels%20in%20AI%20to%20solve%20this%20problem.%20Here%2C%20we%20present%20the%20Concept%20based%0AControllable%20Generation%20%28CoCoG%29%20framework.%20CoCoG%20consists%20of%20two%20components%2C%20a%0Asimple%20yet%20efficient%20AI%20agent%20for%20extracting%20interpretable%20concept%20and%0Apredicting%20human%20decision-making%20in%20visual%20similarity%20judgment%20tasks%2C%20and%20a%0Aconditional%20generation%20model%20for%20generating%20visual%20stimuli%20given%20the%20concepts.%0AWe%20quantify%20the%20performance%20of%20CoCoG%20from%20two%20aspects%2C%20the%20human%20behavior%0Aprediction%20accuracy%20and%20the%20controllable%20generation%20ability.%20The%20experiments%0Awith%20CoCoG%20indicate%20that%201%29%20the%20reliable%20concept%20embeddings%20in%20CoCoG%20allows%20to%0Apredict%20human%20behavior%20with%2064.07%5C%25%20accuracy%20in%20the%20THINGS-similarity%20dataset%3B%0A2%29%20CoCoG%20can%20generate%20diverse%20objects%20through%20the%20control%20of%20concepts%3B%203%29%20CoCoG%0Acan%20manipulate%20human%20similarity%20judgment%20behavior%20by%20intervening%20key%20concepts.%0ACoCoG%20offers%20visual%20objects%20with%20controlling%20concepts%20to%20advance%20our%0Aunderstanding%20of%20causality%20in%20human%20cognition.%20The%20code%20of%20CoCoG%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/ncclab-sustech/CoCoG%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16482v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoCoG%3A%20Controllable%20Visual%20Stimuli%20Generation%20based%20on%20Human%20Concept%0A%20%20Representations&entry.906535625=Chen%20Wei%20and%20Jiachen%20Zou%20and%20Dietmar%20Heinke%20and%20Quanying%20Liu&entry.1292438233=%20%20A%20central%20question%20for%20cognitive%20science%20is%20to%20understand%20how%20humans%20process%0Avisual%20objects%2C%20i.e%2C%20to%20uncover%20human%20low-dimensional%20concept%20representation%0Aspace%20from%20high-dimensional%20visual%20stimuli.%20Generating%20visual%20stimuli%20with%0Acontrolling%20concepts%20is%20the%20key.%20However%2C%20there%20are%20currently%20no%20generative%0Amodels%20in%20AI%20to%20solve%20this%20problem.%20Here%2C%20we%20present%20the%20Concept%20based%0AControllable%20Generation%20%28CoCoG%29%20framework.%20CoCoG%20consists%20of%20two%20components%2C%20a%0Asimple%20yet%20efficient%20AI%20agent%20for%20extracting%20interpretable%20concept%20and%0Apredicting%20human%20decision-making%20in%20visual%20similarity%20judgment%20tasks%2C%20and%20a%0Aconditional%20generation%20model%20for%20generating%20visual%20stimuli%20given%20the%20concepts.%0AWe%20quantify%20the%20performance%20of%20CoCoG%20from%20two%20aspects%2C%20the%20human%20behavior%0Aprediction%20accuracy%20and%20the%20controllable%20generation%20ability.%20The%20experiments%0Awith%20CoCoG%20indicate%20that%201%29%20the%20reliable%20concept%20embeddings%20in%20CoCoG%20allows%20to%0Apredict%20human%20behavior%20with%2064.07%5C%25%20accuracy%20in%20the%20THINGS-similarity%20dataset%3B%0A2%29%20CoCoG%20can%20generate%20diverse%20objects%20through%20the%20control%20of%20concepts%3B%203%29%20CoCoG%0Acan%20manipulate%20human%20similarity%20judgment%20behavior%20by%20intervening%20key%20concepts.%0ACoCoG%20offers%20visual%20objects%20with%20controlling%20concepts%20to%20advance%20our%0Aunderstanding%20of%20causality%20in%20human%20cognition.%20The%20code%20of%20CoCoG%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/ncclab-sustech/CoCoG%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16482v1&entry.124074799=Read"},
{"title": "Guided Interpretable Facial Expression Recognition via Spatial Action\n  Unit Cues", "author": "Soufiane Belharbi and Marco Pedersoli and Alessandro Lameiras Koerich and Simon Bacon and Eric Granger", "abstract": "  Although state-of-the-art classifiers for facial expression recognition (FER)\ncan achieve a high level of accuracy, they lack interpretability, an important\nfeature for end-users. Experts typically associate spatial action units (\\aus)\nfrom a codebook to facial regions for the visual interpretation of expressions.\nIn this paper, the same expert steps are followed. A new learning strategy is\nproposed to explicitly incorporate \\au cues into classifier training, allowing\nto train deep interpretable models. During training, this \\au codebook is used,\nalong with the input image expression label, and facial landmarks, to construct\na \\au heatmap that indicates the most discriminative image regions of interest\nw.r.t the facial expression. This valuable spatial cue is leveraged to train a\ndeep interpretable classifier for FER. This is achieved by constraining the\nspatial layer features of a classifier to be correlated with \\au heatmaps.\nUsing a composite loss, the classifier is trained to correctly classify an\nimage while yielding interpretable visual layer-wise attention correlated with\n\\au maps, simulating the expert decision process. Our strategy only relies on\nimage class expression for supervision, without additional manual annotations.\nOur new strategy is generic, and can be applied to any deep CNN- or\ntransformer-based classifier without requiring any architectural change or\nsignificant additional training time. Our extensive evaluation on two public\nbenchmarks \\rafdb, and \\affectnet datasets shows that our proposed strategy can\nimprove layer-wise interpretability without degrading classification\nperformance. In addition, we explore a common type of interpretable classifiers\nthat rely on class activation mapping (CAM) methods, and show that our approach\ncan also improve CAM interpretability.\n", "link": "http://arxiv.org/abs/2402.00281v3", "date": "2024-04-25", "relevancy": 2.7398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5604}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5399}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%0A%20%20Unit%20Cues&body=Title%3A%20Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%0A%20%20Unit%20Cues%0AAuthor%3A%20Soufiane%20Belharbi%20and%20Marco%20Pedersoli%20and%20Alessandro%20Lameiras%20Koerich%20and%20Simon%20Bacon%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Although%20state-of-the-art%20classifiers%20for%20facial%20expression%20recognition%20%28FER%29%0Acan%20achieve%20a%20high%20level%20of%20accuracy%2C%20they%20lack%20interpretability%2C%20an%20important%0Afeature%20for%20end-users.%20Experts%20typically%20associate%20spatial%20action%20units%20%28%5Caus%29%0Afrom%20a%20codebook%20to%20facial%20regions%20for%20the%20visual%20interpretation%20of%20expressions.%0AIn%20this%20paper%2C%20the%20same%20expert%20steps%20are%20followed.%20A%20new%20learning%20strategy%20is%0Aproposed%20to%20explicitly%20incorporate%20%5Cau%20cues%20into%20classifier%20training%2C%20allowing%0Ato%20train%20deep%20interpretable%20models.%20During%20training%2C%20this%20%5Cau%20codebook%20is%20used%2C%0Aalong%20with%20the%20input%20image%20expression%20label%2C%20and%20facial%20landmarks%2C%20to%20construct%0Aa%20%5Cau%20heatmap%20that%20indicates%20the%20most%20discriminative%20image%20regions%20of%20interest%0Aw.r.t%20the%20facial%20expression.%20This%20valuable%20spatial%20cue%20is%20leveraged%20to%20train%20a%0Adeep%20interpretable%20classifier%20for%20FER.%20This%20is%20achieved%20by%20constraining%20the%0Aspatial%20layer%20features%20of%20a%20classifier%20to%20be%20correlated%20with%20%5Cau%20heatmaps.%0AUsing%20a%20composite%20loss%2C%20the%20classifier%20is%20trained%20to%20correctly%20classify%20an%0Aimage%20while%20yielding%20interpretable%20visual%20layer-wise%20attention%20correlated%20with%0A%5Cau%20maps%2C%20simulating%20the%20expert%20decision%20process.%20Our%20strategy%20only%20relies%20on%0Aimage%20class%20expression%20for%20supervision%2C%20without%20additional%20manual%20annotations.%0AOur%20new%20strategy%20is%20generic%2C%20and%20can%20be%20applied%20to%20any%20deep%20CNN-%20or%0Atransformer-based%20classifier%20without%20requiring%20any%20architectural%20change%20or%0Asignificant%20additional%20training%20time.%20Our%20extensive%20evaluation%20on%20two%20public%0Abenchmarks%20%5Crafdb%2C%20and%20%5Caffectnet%20datasets%20shows%20that%20our%20proposed%20strategy%20can%0Aimprove%20layer-wise%20interpretability%20without%20degrading%20classification%0Aperformance.%20In%20addition%2C%20we%20explore%20a%20common%20type%20of%20interpretable%20classifiers%0Athat%20rely%20on%20class%20activation%20mapping%20%28CAM%29%20methods%2C%20and%20show%20that%20our%20approach%0Acan%20also%20improve%20CAM%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00281v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%0A%20%20Unit%20Cues&entry.906535625=Soufiane%20Belharbi%20and%20Marco%20Pedersoli%20and%20Alessandro%20Lameiras%20Koerich%20and%20Simon%20Bacon%20and%20Eric%20Granger&entry.1292438233=%20%20Although%20state-of-the-art%20classifiers%20for%20facial%20expression%20recognition%20%28FER%29%0Acan%20achieve%20a%20high%20level%20of%20accuracy%2C%20they%20lack%20interpretability%2C%20an%20important%0Afeature%20for%20end-users.%20Experts%20typically%20associate%20spatial%20action%20units%20%28%5Caus%29%0Afrom%20a%20codebook%20to%20facial%20regions%20for%20the%20visual%20interpretation%20of%20expressions.%0AIn%20this%20paper%2C%20the%20same%20expert%20steps%20are%20followed.%20A%20new%20learning%20strategy%20is%0Aproposed%20to%20explicitly%20incorporate%20%5Cau%20cues%20into%20classifier%20training%2C%20allowing%0Ato%20train%20deep%20interpretable%20models.%20During%20training%2C%20this%20%5Cau%20codebook%20is%20used%2C%0Aalong%20with%20the%20input%20image%20expression%20label%2C%20and%20facial%20landmarks%2C%20to%20construct%0Aa%20%5Cau%20heatmap%20that%20indicates%20the%20most%20discriminative%20image%20regions%20of%20interest%0Aw.r.t%20the%20facial%20expression.%20This%20valuable%20spatial%20cue%20is%20leveraged%20to%20train%20a%0Adeep%20interpretable%20classifier%20for%20FER.%20This%20is%20achieved%20by%20constraining%20the%0Aspatial%20layer%20features%20of%20a%20classifier%20to%20be%20correlated%20with%20%5Cau%20heatmaps.%0AUsing%20a%20composite%20loss%2C%20the%20classifier%20is%20trained%20to%20correctly%20classify%20an%0Aimage%20while%20yielding%20interpretable%20visual%20layer-wise%20attention%20correlated%20with%0A%5Cau%20maps%2C%20simulating%20the%20expert%20decision%20process.%20Our%20strategy%20only%20relies%20on%0Aimage%20class%20expression%20for%20supervision%2C%20without%20additional%20manual%20annotations.%0AOur%20new%20strategy%20is%20generic%2C%20and%20can%20be%20applied%20to%20any%20deep%20CNN-%20or%0Atransformer-based%20classifier%20without%20requiring%20any%20architectural%20change%20or%0Asignificant%20additional%20training%20time.%20Our%20extensive%20evaluation%20on%20two%20public%0Abenchmarks%20%5Crafdb%2C%20and%20%5Caffectnet%20datasets%20shows%20that%20our%20proposed%20strategy%20can%0Aimprove%20layer-wise%20interpretability%20without%20degrading%20classification%0Aperformance.%20In%20addition%2C%20we%20explore%20a%20common%20type%20of%20interpretable%20classifiers%0Athat%20rely%20on%20class%20activation%20mapping%20%28CAM%29%20methods%2C%20and%20show%20that%20our%20approach%0Acan%20also%20improve%20CAM%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00281v3&entry.124074799=Read"},
{"title": "NTIRE 2024 Quality Assessment of AI-Generated Content Challenge", "author": "Xiaohong Liu and Xiongkuo Min and Guangtao Zhai and Chunyi Li and Tengchuan Kou and Wei Sun and Haoning Wu and Yixuan Gao and Yuqin Cao and Zicheng Zhang and Xiele Wu and Radu Timofte", "abstract": "  This paper reports on the NTIRE 2024 Quality Assessment of AI-Generated\nContent Challenge, which will be held in conjunction with the New Trends in\nImage Restoration and Enhancement Workshop (NTIRE) at CVPR 2024. This challenge\nis to address a major challenge in the field of image and video processing,\nnamely, Image Quality Assessment (IQA) and Video Quality Assessment (VQA) for\nAI-Generated Content (AIGC). The challenge is divided into the image track and\nthe video track. The image track uses the AIGIQA-20K, which contains 20,000\nAI-Generated Images (AIGIs) generated by 15 popular generative models. The\nimage track has a total of 318 registered participants. A total of 1,646\nsubmissions are received in the development phase, and 221 submissions are\nreceived in the test phase. Finally, 16 participating teams submitted their\nmodels and fact sheets. The video track uses the T2VQA-DB, which contains\n10,000 AI-Generated Videos (AIGVs) generated by 9 popular Text-to-Video (T2V)\nmodels. A total of 196 participants have registered in the video track. A total\nof 991 submissions are received in the development phase, and 185 submissions\nare received in the test phase. Finally, 12 participating teams submitted their\nmodels and fact sheets. Some methods have achieved better results than baseline\nmethods, and the winning methods in both tracks have demonstrated superior\nprediction performance on AIGC.\n", "link": "http://arxiv.org/abs/2404.16687v1", "date": "2024-04-25", "relevancy": 2.7345, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5765}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5643}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4999}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%20Content%20Challenge&body=Title%3A%20NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%20Content%20Challenge%0AAuthor%3A%20Xiaohong%20Liu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Chunyi%20Li%20and%20Tengchuan%20Kou%20and%20Wei%20Sun%20and%20Haoning%20Wu%20and%20Yixuan%20Gao%20and%20Yuqin%20Cao%20and%20Zicheng%20Zhang%20and%20Xiele%20Wu%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20This%20paper%20reports%20on%20the%20NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%0AContent%20Challenge%2C%20which%20will%20be%20held%20in%20conjunction%20with%20the%20New%20Trends%20in%0AImage%20Restoration%20and%20Enhancement%20Workshop%20%28NTIRE%29%20at%20CVPR%202024.%20This%20challenge%0Ais%20to%20address%20a%20major%20challenge%20in%20the%20field%20of%20image%20and%20video%20processing%2C%0Anamely%2C%20Image%20Quality%20Assessment%20%28IQA%29%20and%20Video%20Quality%20Assessment%20%28VQA%29%20for%0AAI-Generated%20Content%20%28AIGC%29.%20The%20challenge%20is%20divided%20into%20the%20image%20track%20and%0Athe%20video%20track.%20The%20image%20track%20uses%20the%20AIGIQA-20K%2C%20which%20contains%2020%2C000%0AAI-Generated%20Images%20%28AIGIs%29%20generated%20by%2015%20popular%20generative%20models.%20The%0Aimage%20track%20has%20a%20total%20of%20318%20registered%20participants.%20A%20total%20of%201%2C646%0Asubmissions%20are%20received%20in%20the%20development%20phase%2C%20and%20221%20submissions%20are%0Areceived%20in%20the%20test%20phase.%20Finally%2C%2016%20participating%20teams%20submitted%20their%0Amodels%20and%20fact%20sheets.%20The%20video%20track%20uses%20the%20T2VQA-DB%2C%20which%20contains%0A10%2C000%20AI-Generated%20Videos%20%28AIGVs%29%20generated%20by%209%20popular%20Text-to-Video%20%28T2V%29%0Amodels.%20A%20total%20of%20196%20participants%20have%20registered%20in%20the%20video%20track.%20A%20total%0Aof%20991%20submissions%20are%20received%20in%20the%20development%20phase%2C%20and%20185%20submissions%0Aare%20received%20in%20the%20test%20phase.%20Finally%2C%2012%20participating%20teams%20submitted%20their%0Amodels%20and%20fact%20sheets.%20Some%20methods%20have%20achieved%20better%20results%20than%20baseline%0Amethods%2C%20and%20the%20winning%20methods%20in%20both%20tracks%20have%20demonstrated%20superior%0Aprediction%20performance%20on%20AIGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16687v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%20Content%20Challenge&entry.906535625=Xiaohong%20Liu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Chunyi%20Li%20and%20Tengchuan%20Kou%20and%20Wei%20Sun%20and%20Haoning%20Wu%20and%20Yixuan%20Gao%20and%20Yuqin%20Cao%20and%20Zicheng%20Zhang%20and%20Xiele%20Wu%20and%20Radu%20Timofte&entry.1292438233=%20%20This%20paper%20reports%20on%20the%20NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%0AContent%20Challenge%2C%20which%20will%20be%20held%20in%20conjunction%20with%20the%20New%20Trends%20in%0AImage%20Restoration%20and%20Enhancement%20Workshop%20%28NTIRE%29%20at%20CVPR%202024.%20This%20challenge%0Ais%20to%20address%20a%20major%20challenge%20in%20the%20field%20of%20image%20and%20video%20processing%2C%0Anamely%2C%20Image%20Quality%20Assessment%20%28IQA%29%20and%20Video%20Quality%20Assessment%20%28VQA%29%20for%0AAI-Generated%20Content%20%28AIGC%29.%20The%20challenge%20is%20divided%20into%20the%20image%20track%20and%0Athe%20video%20track.%20The%20image%20track%20uses%20the%20AIGIQA-20K%2C%20which%20contains%2020%2C000%0AAI-Generated%20Images%20%28AIGIs%29%20generated%20by%2015%20popular%20generative%20models.%20The%0Aimage%20track%20has%20a%20total%20of%20318%20registered%20participants.%20A%20total%20of%201%2C646%0Asubmissions%20are%20received%20in%20the%20development%20phase%2C%20and%20221%20submissions%20are%0Areceived%20in%20the%20test%20phase.%20Finally%2C%2016%20participating%20teams%20submitted%20their%0Amodels%20and%20fact%20sheets.%20The%20video%20track%20uses%20the%20T2VQA-DB%2C%20which%20contains%0A10%2C000%20AI-Generated%20Videos%20%28AIGVs%29%20generated%20by%209%20popular%20Text-to-Video%20%28T2V%29%0Amodels.%20A%20total%20of%20196%20participants%20have%20registered%20in%20the%20video%20track.%20A%20total%0Aof%20991%20submissions%20are%20received%20in%20the%20development%20phase%2C%20and%20185%20submissions%0Aare%20received%20in%20the%20test%20phase.%20Finally%2C%2012%20participating%20teams%20submitted%20their%0Amodels%20and%20fact%20sheets.%20Some%20methods%20have%20achieved%20better%20results%20than%20baseline%0Amethods%2C%20and%20the%20winning%20methods%20in%20both%20tracks%20have%20demonstrated%20superior%0Aprediction%20performance%20on%20AIGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16687v1&entry.124074799=Read"},
{"title": "Embracing Diversity: Interpretable Zero-shot classification beyond one\n  vector per class", "author": "Mazda Moayeri and Michael Rabbat and Mark Ibrahim and Diane Bouchacourt", "abstract": "  Vision-language models enable open-world classification of objects without\nthe need for any retraining. While this zero-shot paradigm marks a significant\nadvance, even today's best models exhibit skewed performance when objects are\ndissimilar from their typical depiction. Real world objects such as pears\nappear in a variety of forms -- from diced to whole, on a table or in a bowl --\nyet standard VLM classifiers map all instances of a class to a \\it{single\nvector based on the class label}. We argue that to represent this rich\ndiversity within a class, zero-shot classification should move beyond a single\nvector. We propose a method to encode and account for diversity within a class\nusing inferred attributes, still in the zero-shot setting without retraining.\nWe find our method consistently outperforms standard zero-shot classification\nover a large suite of datasets encompassing hierarchies, diverse object states,\nand real-world geographic diversity, as well finer-grained datasets where\nintra-class diversity may be less prevalent. Importantly, our method is\ninherently interpretable, offering faithful explanations for each inference to\nfacilitate model debugging and enhance transparency. We also find our method\nscales efficiently to a large number of attributes to account for diversity --\nleading to more accurate predictions for atypical instances. Finally, we\ncharacterize a principled trade-off between overall and worst class accuracy,\nwhich can be tuned via a hyperparameter of our method. We hope this work spurs\nfurther research into the promise of zero-shot classification beyond a single\nclass vector for capturing diversity in the world, and building transparent AI\nsystems without compromising performance.\n", "link": "http://arxiv.org/abs/2404.16717v1", "date": "2024-04-25", "relevancy": 2.7144, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5559}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5369}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5358}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Embracing%20Diversity%3A%20Interpretable%20Zero-shot%20classification%20beyond%20one%0A%20%20vector%20per%20class&body=Title%3A%20Embracing%20Diversity%3A%20Interpretable%20Zero-shot%20classification%20beyond%20one%0A%20%20vector%20per%20class%0AAuthor%3A%20Mazda%20Moayeri%20and%20Michael%20Rabbat%20and%20Mark%20Ibrahim%20and%20Diane%20Bouchacourt%0AAbstract%3A%20%20%20Vision-language%20models%20enable%20open-world%20classification%20of%20objects%20without%0Athe%20need%20for%20any%20retraining.%20While%20this%20zero-shot%20paradigm%20marks%20a%20significant%0Aadvance%2C%20even%20today%27s%20best%20models%20exhibit%20skewed%20performance%20when%20objects%20are%0Adissimilar%20from%20their%20typical%20depiction.%20Real%20world%20objects%20such%20as%20pears%0Aappear%20in%20a%20variety%20of%20forms%20--%20from%20diced%20to%20whole%2C%20on%20a%20table%20or%20in%20a%20bowl%20--%0Ayet%20standard%20VLM%20classifiers%20map%20all%20instances%20of%20a%20class%20to%20a%20%5Cit%7Bsingle%0Avector%20based%20on%20the%20class%20label%7D.%20We%20argue%20that%20to%20represent%20this%20rich%0Adiversity%20within%20a%20class%2C%20zero-shot%20classification%20should%20move%20beyond%20a%20single%0Avector.%20We%20propose%20a%20method%20to%20encode%20and%20account%20for%20diversity%20within%20a%20class%0Ausing%20inferred%20attributes%2C%20still%20in%20the%20zero-shot%20setting%20without%20retraining.%0AWe%20find%20our%20method%20consistently%20outperforms%20standard%20zero-shot%20classification%0Aover%20a%20large%20suite%20of%20datasets%20encompassing%20hierarchies%2C%20diverse%20object%20states%2C%0Aand%20real-world%20geographic%20diversity%2C%20as%20well%20finer-grained%20datasets%20where%0Aintra-class%20diversity%20may%20be%20less%20prevalent.%20Importantly%2C%20our%20method%20is%0Ainherently%20interpretable%2C%20offering%20faithful%20explanations%20for%20each%20inference%20to%0Afacilitate%20model%20debugging%20and%20enhance%20transparency.%20We%20also%20find%20our%20method%0Ascales%20efficiently%20to%20a%20large%20number%20of%20attributes%20to%20account%20for%20diversity%20--%0Aleading%20to%20more%20accurate%20predictions%20for%20atypical%20instances.%20Finally%2C%20we%0Acharacterize%20a%20principled%20trade-off%20between%20overall%20and%20worst%20class%20accuracy%2C%0Awhich%20can%20be%20tuned%20via%20a%20hyperparameter%20of%20our%20method.%20We%20hope%20this%20work%20spurs%0Afurther%20research%20into%20the%20promise%20of%20zero-shot%20classification%20beyond%20a%20single%0Aclass%20vector%20for%20capturing%20diversity%20in%20the%20world%2C%20and%20building%20transparent%20AI%0Asystems%20without%20compromising%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16717v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embracing%20Diversity%3A%20Interpretable%20Zero-shot%20classification%20beyond%20one%0A%20%20vector%20per%20class&entry.906535625=Mazda%20Moayeri%20and%20Michael%20Rabbat%20and%20Mark%20Ibrahim%20and%20Diane%20Bouchacourt&entry.1292438233=%20%20Vision-language%20models%20enable%20open-world%20classification%20of%20objects%20without%0Athe%20need%20for%20any%20retraining.%20While%20this%20zero-shot%20paradigm%20marks%20a%20significant%0Aadvance%2C%20even%20today%27s%20best%20models%20exhibit%20skewed%20performance%20when%20objects%20are%0Adissimilar%20from%20their%20typical%20depiction.%20Real%20world%20objects%20such%20as%20pears%0Aappear%20in%20a%20variety%20of%20forms%20--%20from%20diced%20to%20whole%2C%20on%20a%20table%20or%20in%20a%20bowl%20--%0Ayet%20standard%20VLM%20classifiers%20map%20all%20instances%20of%20a%20class%20to%20a%20%5Cit%7Bsingle%0Avector%20based%20on%20the%20class%20label%7D.%20We%20argue%20that%20to%20represent%20this%20rich%0Adiversity%20within%20a%20class%2C%20zero-shot%20classification%20should%20move%20beyond%20a%20single%0Avector.%20We%20propose%20a%20method%20to%20encode%20and%20account%20for%20diversity%20within%20a%20class%0Ausing%20inferred%20attributes%2C%20still%20in%20the%20zero-shot%20setting%20without%20retraining.%0AWe%20find%20our%20method%20consistently%20outperforms%20standard%20zero-shot%20classification%0Aover%20a%20large%20suite%20of%20datasets%20encompassing%20hierarchies%2C%20diverse%20object%20states%2C%0Aand%20real-world%20geographic%20diversity%2C%20as%20well%20finer-grained%20datasets%20where%0Aintra-class%20diversity%20may%20be%20less%20prevalent.%20Importantly%2C%20our%20method%20is%0Ainherently%20interpretable%2C%20offering%20faithful%20explanations%20for%20each%20inference%20to%0Afacilitate%20model%20debugging%20and%20enhance%20transparency.%20We%20also%20find%20our%20method%0Ascales%20efficiently%20to%20a%20large%20number%20of%20attributes%20to%20account%20for%20diversity%20--%0Aleading%20to%20more%20accurate%20predictions%20for%20atypical%20instances.%20Finally%2C%20we%0Acharacterize%20a%20principled%20trade-off%20between%20overall%20and%20worst%20class%20accuracy%2C%0Awhich%20can%20be%20tuned%20via%20a%20hyperparameter%20of%20our%20method.%20We%20hope%20this%20work%20spurs%0Afurther%20research%20into%20the%20promise%20of%20zero-shot%20classification%20beyond%20a%20single%0Aclass%20vector%20for%20capturing%20diversity%20in%20the%20world%2C%20and%20building%20transparent%20AI%0Asystems%20without%20compromising%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16717v1&entry.124074799=Read"},
{"title": "COIN-LIO: Complementary Intensity-Augmented LiDAR Inertial Odometry", "author": "Patrick Pfreundschuh and Helen Oleynikova and Cesar Cadena and Roland Siegwart and Olov Andersson", "abstract": "  We present COIN-LIO, a LiDAR Inertial Odometry pipeline that tightly couples\ninformation from LiDAR intensity with geometry-based point cloud registration.\nThe focus of our work is to improve the robustness of LiDAR-inertial odometry\nin geometrically degenerate scenarios, like tunnels or flat fields. We project\nLiDAR intensity returns into an intensity image, and propose an image\nprocessing pipeline that produces filtered images with improved brightness\nconsistency within the image as well as across different scenes. To effectively\nleverage intensity as an additional modality, we present a novel feature\nselection scheme that detects uninformative directions in the point cloud\nregistration and explicitly selects patches with complementary image\ninformation. Photometric error minimization in the image patches is then fused\nwith inertial measurements and point-to-plane registration in an iterated\nExtended Kalman Filter. The proposed approach improves accuracy and robustness\non a public dataset. We additionally publish a new dataset, that captures five\nreal-world environments in challenging, geometrically degenerate scenes. By\nusing the additional photometric information, our approach shows drastically\nimproved robustness against geometric degeneracy in environments where all\ncompared baseline approaches fail.\n", "link": "http://arxiv.org/abs/2310.01235v3", "date": "2024-04-25", "relevancy": 2.6362, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5424}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5234}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20COIN-LIO%3A%20Complementary%20Intensity-Augmented%20LiDAR%20Inertial%20Odometry&body=Title%3A%20COIN-LIO%3A%20Complementary%20Intensity-Augmented%20LiDAR%20Inertial%20Odometry%0AAuthor%3A%20Patrick%20Pfreundschuh%20and%20Helen%20Oleynikova%20and%20Cesar%20Cadena%20and%20Roland%20Siegwart%20and%20Olov%20Andersson%0AAbstract%3A%20%20%20We%20present%20COIN-LIO%2C%20a%20LiDAR%20Inertial%20Odometry%20pipeline%20that%20tightly%20couples%0Ainformation%20from%20LiDAR%20intensity%20with%20geometry-based%20point%20cloud%20registration.%0AThe%20focus%20of%20our%20work%20is%20to%20improve%20the%20robustness%20of%20LiDAR-inertial%20odometry%0Ain%20geometrically%20degenerate%20scenarios%2C%20like%20tunnels%20or%20flat%20fields.%20We%20project%0ALiDAR%20intensity%20returns%20into%20an%20intensity%20image%2C%20and%20propose%20an%20image%0Aprocessing%20pipeline%20that%20produces%20filtered%20images%20with%20improved%20brightness%0Aconsistency%20within%20the%20image%20as%20well%20as%20across%20different%20scenes.%20To%20effectively%0Aleverage%20intensity%20as%20an%20additional%20modality%2C%20we%20present%20a%20novel%20feature%0Aselection%20scheme%20that%20detects%20uninformative%20directions%20in%20the%20point%20cloud%0Aregistration%20and%20explicitly%20selects%20patches%20with%20complementary%20image%0Ainformation.%20Photometric%20error%20minimization%20in%20the%20image%20patches%20is%20then%20fused%0Awith%20inertial%20measurements%20and%20point-to-plane%20registration%20in%20an%20iterated%0AExtended%20Kalman%20Filter.%20The%20proposed%20approach%20improves%20accuracy%20and%20robustness%0Aon%20a%20public%20dataset.%20We%20additionally%20publish%20a%20new%20dataset%2C%20that%20captures%20five%0Areal-world%20environments%20in%20challenging%2C%20geometrically%20degenerate%20scenes.%20By%0Ausing%20the%20additional%20photometric%20information%2C%20our%20approach%20shows%20drastically%0Aimproved%20robustness%20against%20geometric%20degeneracy%20in%20environments%20where%20all%0Acompared%20baseline%20approaches%20fail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01235v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COIN-LIO%3A%20Complementary%20Intensity-Augmented%20LiDAR%20Inertial%20Odometry&entry.906535625=Patrick%20Pfreundschuh%20and%20Helen%20Oleynikova%20and%20Cesar%20Cadena%20and%20Roland%20Siegwart%20and%20Olov%20Andersson&entry.1292438233=%20%20We%20present%20COIN-LIO%2C%20a%20LiDAR%20Inertial%20Odometry%20pipeline%20that%20tightly%20couples%0Ainformation%20from%20LiDAR%20intensity%20with%20geometry-based%20point%20cloud%20registration.%0AThe%20focus%20of%20our%20work%20is%20to%20improve%20the%20robustness%20of%20LiDAR-inertial%20odometry%0Ain%20geometrically%20degenerate%20scenarios%2C%20like%20tunnels%20or%20flat%20fields.%20We%20project%0ALiDAR%20intensity%20returns%20into%20an%20intensity%20image%2C%20and%20propose%20an%20image%0Aprocessing%20pipeline%20that%20produces%20filtered%20images%20with%20improved%20brightness%0Aconsistency%20within%20the%20image%20as%20well%20as%20across%20different%20scenes.%20To%20effectively%0Aleverage%20intensity%20as%20an%20additional%20modality%2C%20we%20present%20a%20novel%20feature%0Aselection%20scheme%20that%20detects%20uninformative%20directions%20in%20the%20point%20cloud%0Aregistration%20and%20explicitly%20selects%20patches%20with%20complementary%20image%0Ainformation.%20Photometric%20error%20minimization%20in%20the%20image%20patches%20is%20then%20fused%0Awith%20inertial%20measurements%20and%20point-to-plane%20registration%20in%20an%20iterated%0AExtended%20Kalman%20Filter.%20The%20proposed%20approach%20improves%20accuracy%20and%20robustness%0Aon%20a%20public%20dataset.%20We%20additionally%20publish%20a%20new%20dataset%2C%20that%20captures%20five%0Areal-world%20environments%20in%20challenging%2C%20geometrically%20degenerate%20scenes.%20By%0Ausing%20the%20additional%20photometric%20information%2C%20our%20approach%20shows%20drastically%0Aimproved%20robustness%20against%20geometric%20degeneracy%20in%20environments%20where%20all%0Acompared%20baseline%20approaches%20fail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01235v3&entry.124074799=Read"},
{"title": "MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth\n  Estimation of Endoscopic Images", "author": "Zhiwei Wang and Ying Zhou and Shiquan He and Ting Li and Yitong Zhang and Xinxia Feng and Mei Liu and Qiang Li", "abstract": "  Photometric constraint is indispensable for self-supervised monocular depth\nestimation. It involves warping a source image onto a target view using\nestimated depth&pose, and then minimizing the difference between the warped and\ntarget images. However, the endoscopic built-in light causes significant\nbrightness fluctuations, and thus makes the photometric constraint unreliable.\nPrevious efforts only mitigate this relying on extra models to calibrate image\nbrightness. In this paper, we propose MonoPCC to address the brightness\ninconsistency radically by reshaping the photometric constraint into a cycle\nform. Instead of only warping the source image, MonoPCC constructs a closed\nloop consisting of two opposite forward-backward warping paths: from target to\nsource and then back to target. Thus, the target image finally receives an\nimage cycle-warped from itself, which naturally makes the constraint invariant\nto brightness changes. Moreover, MonoPCC transplants the source image's\nphase-frequency into the intermediate warped image to avoid structure lost, and\nalso stabilizes the training via an exponential moving average (EMA) strategy\nto avoid frequent changes in the forward warping. The comprehensive and\nextensive experimental results on three datasets demonstrate that our proposed\nMonoPCC shows a great robustness to the brightness inconsistency, and exceeds\nother state-of-the-arts by reducing the absolute relative error by at least\n7.27%.\n", "link": "http://arxiv.org/abs/2404.16571v1", "date": "2024-04-25", "relevancy": 2.6187, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5361}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5242}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5109}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images&body=Title%3A%20MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images%0AAuthor%3A%20Zhiwei%20Wang%20and%20Ying%20Zhou%20and%20Shiquan%20He%20and%20Ting%20Li%20and%20Yitong%20Zhang%20and%20Xinxia%20Feng%20and%20Mei%20Liu%20and%20Qiang%20Li%0AAbstract%3A%20%20%20Photometric%20constraint%20is%20indispensable%20for%20self-supervised%20monocular%20depth%0Aestimation.%20It%20involves%20warping%20a%20source%20image%20onto%20a%20target%20view%20using%0Aestimated%20depth%26pose%2C%20and%20then%20minimizing%20the%20difference%20between%20the%20warped%20and%0Atarget%20images.%20However%2C%20the%20endoscopic%20built-in%20light%20causes%20significant%0Abrightness%20fluctuations%2C%20and%20thus%20makes%20the%20photometric%20constraint%20unreliable.%0APrevious%20efforts%20only%20mitigate%20this%20relying%20on%20extra%20models%20to%20calibrate%20image%0Abrightness.%20In%20this%20paper%2C%20we%20propose%20MonoPCC%20to%20address%20the%20brightness%0Ainconsistency%20radically%20by%20reshaping%20the%20photometric%20constraint%20into%20a%20cycle%0Aform.%20Instead%20of%20only%20warping%20the%20source%20image%2C%20MonoPCC%20constructs%20a%20closed%0Aloop%20consisting%20of%20two%20opposite%20forward-backward%20warping%20paths%3A%20from%20target%20to%0Asource%20and%20then%20back%20to%20target.%20Thus%2C%20the%20target%20image%20finally%20receives%20an%0Aimage%20cycle-warped%20from%20itself%2C%20which%20naturally%20makes%20the%20constraint%20invariant%0Ato%20brightness%20changes.%20Moreover%2C%20MonoPCC%20transplants%20the%20source%20image%27s%0Aphase-frequency%20into%20the%20intermediate%20warped%20image%20to%20avoid%20structure%20lost%2C%20and%0Aalso%20stabilizes%20the%20training%20via%20an%20exponential%20moving%20average%20%28EMA%29%20strategy%0Ato%20avoid%20frequent%20changes%20in%20the%20forward%20warping.%20The%20comprehensive%20and%0Aextensive%20experimental%20results%20on%20three%20datasets%20demonstrate%20that%20our%20proposed%0AMonoPCC%20shows%20a%20great%20robustness%20to%20the%20brightness%20inconsistency%2C%20and%20exceeds%0Aother%20state-of-the-arts%20by%20reducing%20the%20absolute%20relative%20error%20by%20at%20least%0A7.27%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16571v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images&entry.906535625=Zhiwei%20Wang%20and%20Ying%20Zhou%20and%20Shiquan%20He%20and%20Ting%20Li%20and%20Yitong%20Zhang%20and%20Xinxia%20Feng%20and%20Mei%20Liu%20and%20Qiang%20Li&entry.1292438233=%20%20Photometric%20constraint%20is%20indispensable%20for%20self-supervised%20monocular%20depth%0Aestimation.%20It%20involves%20warping%20a%20source%20image%20onto%20a%20target%20view%20using%0Aestimated%20depth%26pose%2C%20and%20then%20minimizing%20the%20difference%20between%20the%20warped%20and%0Atarget%20images.%20However%2C%20the%20endoscopic%20built-in%20light%20causes%20significant%0Abrightness%20fluctuations%2C%20and%20thus%20makes%20the%20photometric%20constraint%20unreliable.%0APrevious%20efforts%20only%20mitigate%20this%20relying%20on%20extra%20models%20to%20calibrate%20image%0Abrightness.%20In%20this%20paper%2C%20we%20propose%20MonoPCC%20to%20address%20the%20brightness%0Ainconsistency%20radically%20by%20reshaping%20the%20photometric%20constraint%20into%20a%20cycle%0Aform.%20Instead%20of%20only%20warping%20the%20source%20image%2C%20MonoPCC%20constructs%20a%20closed%0Aloop%20consisting%20of%20two%20opposite%20forward-backward%20warping%20paths%3A%20from%20target%20to%0Asource%20and%20then%20back%20to%20target.%20Thus%2C%20the%20target%20image%20finally%20receives%20an%0Aimage%20cycle-warped%20from%20itself%2C%20which%20naturally%20makes%20the%20constraint%20invariant%0Ato%20brightness%20changes.%20Moreover%2C%20MonoPCC%20transplants%20the%20source%20image%27s%0Aphase-frequency%20into%20the%20intermediate%20warped%20image%20to%20avoid%20structure%20lost%2C%20and%0Aalso%20stabilizes%20the%20training%20via%20an%20exponential%20moving%20average%20%28EMA%29%20strategy%0Ato%20avoid%20frequent%20changes%20in%20the%20forward%20warping.%20The%20comprehensive%20and%0Aextensive%20experimental%20results%20on%20three%20datasets%20demonstrate%20that%20our%20proposed%0AMonoPCC%20shows%20a%20great%20robustness%20to%20the%20brightness%20inconsistency%2C%20and%20exceeds%0Aother%20state-of-the-arts%20by%20reducing%20the%20absolute%20relative%20error%20by%20at%20least%0A7.27%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16571v1&entry.124074799=Read"},
{"title": "ConKeD++ -- Improving descriptor learning for retinal image\n  registration: A comprehensive study of contrastive losses", "author": "David Rivas-Villar and \u00c1lvaro S. Hervella and Jos\u00e9 Rouco and Jorge Novo", "abstract": "  Self-supervised contrastive learning has emerged as one of the most\nsuccessful deep learning paradigms. In this regard, it has seen extensive use\nin image registration and, more recently, in the particular field of medical\nimage registration. In this work, we propose to test and extend and improve a\nstate-of-the-art framework for color fundus image registration, ConKeD. Using\nthe ConKeD framework we test multiple loss functions, adapting them to the\nframework and the application domain. Furthermore, we evaluate our models using\nthe standarized benchmark dataset FIRE as well as several datasets that have\nnever been used before for color fundus registration, for which we are\nreleasing the pairing data as well as a standardized evaluation approach. Our\nwork demonstrates state-of-the-art performance across all datasets and metrics\ndemonstrating several advantages over current SOTA color fundus registration\nmethods\n", "link": "http://arxiv.org/abs/2404.16773v1", "date": "2024-04-25", "relevancy": 2.6065, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5578}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5128}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ConKeD%2B%2B%20--%20Improving%20descriptor%20learning%20for%20retinal%20image%0A%20%20registration%3A%20A%20comprehensive%20study%20of%20contrastive%20losses&body=Title%3A%20ConKeD%2B%2B%20--%20Improving%20descriptor%20learning%20for%20retinal%20image%0A%20%20registration%3A%20A%20comprehensive%20study%20of%20contrastive%20losses%0AAuthor%3A%20David%20Rivas-Villar%20and%20%C3%81lvaro%20S.%20Hervella%20and%20Jos%C3%A9%20Rouco%20and%20Jorge%20Novo%0AAbstract%3A%20%20%20Self-supervised%20contrastive%20learning%20has%20emerged%20as%20one%20of%20the%20most%0Asuccessful%20deep%20learning%20paradigms.%20In%20this%20regard%2C%20it%20has%20seen%20extensive%20use%0Ain%20image%20registration%20and%2C%20more%20recently%2C%20in%20the%20particular%20field%20of%20medical%0Aimage%20registration.%20In%20this%20work%2C%20we%20propose%20to%20test%20and%20extend%20and%20improve%20a%0Astate-of-the-art%20framework%20for%20color%20fundus%20image%20registration%2C%20ConKeD.%20Using%0Athe%20ConKeD%20framework%20we%20test%20multiple%20loss%20functions%2C%20adapting%20them%20to%20the%0Aframework%20and%20the%20application%20domain.%20Furthermore%2C%20we%20evaluate%20our%20models%20using%0Athe%20standarized%20benchmark%20dataset%20FIRE%20as%20well%20as%20several%20datasets%20that%20have%0Anever%20been%20used%20before%20for%20color%20fundus%20registration%2C%20for%20which%20we%20are%0Areleasing%20the%20pairing%20data%20as%20well%20as%20a%20standardized%20evaluation%20approach.%20Our%0Awork%20demonstrates%20state-of-the-art%20performance%20across%20all%20datasets%20and%20metrics%0Ademonstrating%20several%20advantages%20over%20current%20SOTA%20color%20fundus%20registration%0Amethods%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16773v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConKeD%2B%2B%20--%20Improving%20descriptor%20learning%20for%20retinal%20image%0A%20%20registration%3A%20A%20comprehensive%20study%20of%20contrastive%20losses&entry.906535625=David%20Rivas-Villar%20and%20%C3%81lvaro%20S.%20Hervella%20and%20Jos%C3%A9%20Rouco%20and%20Jorge%20Novo&entry.1292438233=%20%20Self-supervised%20contrastive%20learning%20has%20emerged%20as%20one%20of%20the%20most%0Asuccessful%20deep%20learning%20paradigms.%20In%20this%20regard%2C%20it%20has%20seen%20extensive%20use%0Ain%20image%20registration%20and%2C%20more%20recently%2C%20in%20the%20particular%20field%20of%20medical%0Aimage%20registration.%20In%20this%20work%2C%20we%20propose%20to%20test%20and%20extend%20and%20improve%20a%0Astate-of-the-art%20framework%20for%20color%20fundus%20image%20registration%2C%20ConKeD.%20Using%0Athe%20ConKeD%20framework%20we%20test%20multiple%20loss%20functions%2C%20adapting%20them%20to%20the%0Aframework%20and%20the%20application%20domain.%20Furthermore%2C%20we%20evaluate%20our%20models%20using%0Athe%20standarized%20benchmark%20dataset%20FIRE%20as%20well%20as%20several%20datasets%20that%20have%0Anever%20been%20used%20before%20for%20color%20fundus%20registration%2C%20for%20which%20we%20are%0Areleasing%20the%20pairing%20data%20as%20well%20as%20a%20standardized%20evaluation%20approach.%20Our%0Awork%20demonstrates%20state-of-the-art%20performance%20across%20all%20datasets%20and%20metrics%0Ademonstrating%20several%20advantages%20over%20current%20SOTA%20color%20fundus%20registration%0Amethods%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16773v1&entry.124074799=Read"},
{"title": "Boosting Unsupervised Semantic Segmentation with Principal Mask\n  Proposals", "author": "Oliver Hahn and Nikita Araslanov and Simone Schaub-Meyer and Stefan Roth", "abstract": "  Unsupervised semantic segmentation aims to automatically partition images\ninto semantically meaningful regions by identifying global categories within an\nimage corpus without any form of annotation. Building upon recent advances in\nself-supervised representation learning, we focus on how to leverage these\nlarge pre-trained models for the downstream task of unsupervised segmentation.\nWe present PriMaPs - Principal Mask Proposals - decomposing images into\nsemantically meaningful masks based on their feature representation. This\nallows us to realize unsupervised semantic segmentation by fitting class\nprototypes to PriMaPs with a stochastic expectation-maximization algorithm,\nPriMaPs-EM. Despite its conceptual simplicity, PriMaPs-EM leads to competitive\nresults across various pre-trained backbone models, including DINO and DINOv2,\nand across datasets, such as Cityscapes, COCO-Stuff, and Potsdam-3.\nImportantly, PriMaPs-EM is able to boost results when applied orthogonally to\ncurrent state-of-the-art unsupervised semantic segmentation pipelines.\n", "link": "http://arxiv.org/abs/2404.16818v1", "date": "2024-04-25", "relevancy": 2.5549, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5201}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5096}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5032}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Boosting%20Unsupervised%20Semantic%20Segmentation%20with%20Principal%20Mask%0A%20%20Proposals&body=Title%3A%20Boosting%20Unsupervised%20Semantic%20Segmentation%20with%20Principal%20Mask%0A%20%20Proposals%0AAuthor%3A%20Oliver%20Hahn%20and%20Nikita%20Araslanov%20and%20Simone%20Schaub-Meyer%20and%20Stefan%20Roth%0AAbstract%3A%20%20%20Unsupervised%20semantic%20segmentation%20aims%20to%20automatically%20partition%20images%0Ainto%20semantically%20meaningful%20regions%20by%20identifying%20global%20categories%20within%20an%0Aimage%20corpus%20without%20any%20form%20of%20annotation.%20Building%20upon%20recent%20advances%20in%0Aself-supervised%20representation%20learning%2C%20we%20focus%20on%20how%20to%20leverage%20these%0Alarge%20pre-trained%20models%20for%20the%20downstream%20task%20of%20unsupervised%20segmentation.%0AWe%20present%20PriMaPs%20-%20Principal%20Mask%20Proposals%20-%20decomposing%20images%20into%0Asemantically%20meaningful%20masks%20based%20on%20their%20feature%20representation.%20This%0Aallows%20us%20to%20realize%20unsupervised%20semantic%20segmentation%20by%20fitting%20class%0Aprototypes%20to%20PriMaPs%20with%20a%20stochastic%20expectation-maximization%20algorithm%2C%0APriMaPs-EM.%20Despite%20its%20conceptual%20simplicity%2C%20PriMaPs-EM%20leads%20to%20competitive%0Aresults%20across%20various%20pre-trained%20backbone%20models%2C%20including%20DINO%20and%20DINOv2%2C%0Aand%20across%20datasets%2C%20such%20as%20Cityscapes%2C%20COCO-Stuff%2C%20and%20Potsdam-3.%0AImportantly%2C%20PriMaPs-EM%20is%20able%20to%20boost%20results%20when%20applied%20orthogonally%20to%0Acurrent%20state-of-the-art%20unsupervised%20semantic%20segmentation%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16818v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Unsupervised%20Semantic%20Segmentation%20with%20Principal%20Mask%0A%20%20Proposals&entry.906535625=Oliver%20Hahn%20and%20Nikita%20Araslanov%20and%20Simone%20Schaub-Meyer%20and%20Stefan%20Roth&entry.1292438233=%20%20Unsupervised%20semantic%20segmentation%20aims%20to%20automatically%20partition%20images%0Ainto%20semantically%20meaningful%20regions%20by%20identifying%20global%20categories%20within%20an%0Aimage%20corpus%20without%20any%20form%20of%20annotation.%20Building%20upon%20recent%20advances%20in%0Aself-supervised%20representation%20learning%2C%20we%20focus%20on%20how%20to%20leverage%20these%0Alarge%20pre-trained%20models%20for%20the%20downstream%20task%20of%20unsupervised%20segmentation.%0AWe%20present%20PriMaPs%20-%20Principal%20Mask%20Proposals%20-%20decomposing%20images%20into%0Asemantically%20meaningful%20masks%20based%20on%20their%20feature%20representation.%20This%0Aallows%20us%20to%20realize%20unsupervised%20semantic%20segmentation%20by%20fitting%20class%0Aprototypes%20to%20PriMaPs%20with%20a%20stochastic%20expectation-maximization%20algorithm%2C%0APriMaPs-EM.%20Despite%20its%20conceptual%20simplicity%2C%20PriMaPs-EM%20leads%20to%20competitive%0Aresults%20across%20various%20pre-trained%20backbone%20models%2C%20including%20DINO%20and%20DINOv2%2C%0Aand%20across%20datasets%2C%20such%20as%20Cityscapes%2C%20COCO-Stuff%2C%20and%20Potsdam-3.%0AImportantly%2C%20PriMaPs-EM%20is%20able%20to%20boost%20results%20when%20applied%20orthogonally%20to%0Acurrent%20state-of-the-art%20unsupervised%20semantic%20segmentation%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16818v1&entry.124074799=Read"},
{"title": "OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images", "author": "Ye Mao and Junpeng Jing and Krystian Mikolajczyk", "abstract": "  Recent advances in Vision and Language Models (VLMs) have improved open-world\n3D representation, facilitating 3D zero-shot capability in unseen categories.\nExisting open-world methods pre-train an extra 3D encoder to align features\nfrom 3D data (e.g., depth maps or point clouds) with CAD-rendered images and\ncorresponding texts. However, the limited color and texture variations in CAD\nimages can compromise the alignment robustness. Furthermore, the volume\ndiscrepancy between pre-training datasets of the 3D encoder and VLM leads to\nsub-optimal 2D to 3D knowledge transfer. To overcome these issues, we propose\nOpenDlign, a novel framework for learning open-world 3D representations, that\nleverages depth-aligned images generated from point cloud-projected depth maps.\nUnlike CAD-rendered images, our generated images provide rich, realistic color\nand texture diversity while preserving geometric and semantic consistency with\nthe depth maps. OpenDlign also optimizes depth map projection and integrates\ndepth-specific text prompts, improving 2D VLM knowledge adaptation for 3D\nlearning efficient fine-tuning. Experimental results show that OpenDlign\nsignificantly outperforms existing benchmarks in zero-shot and few-shot 3D\ntasks, exceeding prior scores by 8.0% on ModelNet40 and 16.4% on OmniObject3D\nwith just 6 million tuned parameters. Moreover, integrating generated\ndepth-aligned images into existing 3D learning pipelines consistently improves\ntheir performance.\n", "link": "http://arxiv.org/abs/2404.16538v1", "date": "2024-04-25", "relevancy": 2.539, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6531}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6219}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6216}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images&body=Title%3A%20OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images%0AAuthor%3A%20Ye%20Mao%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk%0AAbstract%3A%20%20%20Recent%20advances%20in%20Vision%20and%20Language%20Models%20%28VLMs%29%20have%20improved%20open-world%0A3D%20representation%2C%20facilitating%203D%20zero-shot%20capability%20in%20unseen%20categories.%0AExisting%20open-world%20methods%20pre-train%20an%20extra%203D%20encoder%20to%20align%20features%0Afrom%203D%20data%20%28e.g.%2C%20depth%20maps%20or%20point%20clouds%29%20with%20CAD-rendered%20images%20and%0Acorresponding%20texts.%20However%2C%20the%20limited%20color%20and%20texture%20variations%20in%20CAD%0Aimages%20can%20compromise%20the%20alignment%20robustness.%20Furthermore%2C%20the%20volume%0Adiscrepancy%20between%20pre-training%20datasets%20of%20the%203D%20encoder%20and%20VLM%20leads%20to%0Asub-optimal%202D%20to%203D%20knowledge%20transfer.%20To%20overcome%20these%20issues%2C%20we%20propose%0AOpenDlign%2C%20a%20novel%20framework%20for%20learning%20open-world%203D%20representations%2C%20that%0Aleverages%20depth-aligned%20images%20generated%20from%20point%20cloud-projected%20depth%20maps.%0AUnlike%20CAD-rendered%20images%2C%20our%20generated%20images%20provide%20rich%2C%20realistic%20color%0Aand%20texture%20diversity%20while%20preserving%20geometric%20and%20semantic%20consistency%20with%0Athe%20depth%20maps.%20OpenDlign%20also%20optimizes%20depth%20map%20projection%20and%20integrates%0Adepth-specific%20text%20prompts%2C%20improving%202D%20VLM%20knowledge%20adaptation%20for%203D%0Alearning%20efficient%20fine-tuning.%20Experimental%20results%20show%20that%20OpenDlign%0Asignificantly%20outperforms%20existing%20benchmarks%20in%20zero-shot%20and%20few-shot%203D%0Atasks%2C%20exceeding%20prior%20scores%20by%208.0%25%20on%20ModelNet40%20and%2016.4%25%20on%20OmniObject3D%0Awith%20just%206%20million%20tuned%20parameters.%20Moreover%2C%20integrating%20generated%0Adepth-aligned%20images%20into%20existing%203D%20learning%20pipelines%20consistently%20improves%0Atheir%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16538v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images&entry.906535625=Ye%20Mao%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk&entry.1292438233=%20%20Recent%20advances%20in%20Vision%20and%20Language%20Models%20%28VLMs%29%20have%20improved%20open-world%0A3D%20representation%2C%20facilitating%203D%20zero-shot%20capability%20in%20unseen%20categories.%0AExisting%20open-world%20methods%20pre-train%20an%20extra%203D%20encoder%20to%20align%20features%0Afrom%203D%20data%20%28e.g.%2C%20depth%20maps%20or%20point%20clouds%29%20with%20CAD-rendered%20images%20and%0Acorresponding%20texts.%20However%2C%20the%20limited%20color%20and%20texture%20variations%20in%20CAD%0Aimages%20can%20compromise%20the%20alignment%20robustness.%20Furthermore%2C%20the%20volume%0Adiscrepancy%20between%20pre-training%20datasets%20of%20the%203D%20encoder%20and%20VLM%20leads%20to%0Asub-optimal%202D%20to%203D%20knowledge%20transfer.%20To%20overcome%20these%20issues%2C%20we%20propose%0AOpenDlign%2C%20a%20novel%20framework%20for%20learning%20open-world%203D%20representations%2C%20that%0Aleverages%20depth-aligned%20images%20generated%20from%20point%20cloud-projected%20depth%20maps.%0AUnlike%20CAD-rendered%20images%2C%20our%20generated%20images%20provide%20rich%2C%20realistic%20color%0Aand%20texture%20diversity%20while%20preserving%20geometric%20and%20semantic%20consistency%20with%0Athe%20depth%20maps.%20OpenDlign%20also%20optimizes%20depth%20map%20projection%20and%20integrates%0Adepth-specific%20text%20prompts%2C%20improving%202D%20VLM%20knowledge%20adaptation%20for%203D%0Alearning%20efficient%20fine-tuning.%20Experimental%20results%20show%20that%20OpenDlign%0Asignificantly%20outperforms%20existing%20benchmarks%20in%20zero-shot%20and%20few-shot%203D%0Atasks%2C%20exceeding%20prior%20scores%20by%208.0%25%20on%20ModelNet40%20and%2016.4%25%20on%20OmniObject3D%0Awith%20just%206%20million%20tuned%20parameters.%20Moreover%2C%20integrating%20generated%0Adepth-aligned%20images%20into%20existing%203D%20learning%20pipelines%20consistently%20improves%0Atheir%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16538v1&entry.124074799=Read"},
{"title": "Continual Learning of Large Language Models: A Comprehensive Survey", "author": "Haizhou Shi and Zihao Xu and Hengyi Wang and Weiyi Qin and Wenyuan Wang and Yibin Wang and Hao Wang", "abstract": "  The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.\n", "link": "http://arxiv.org/abs/2404.16789v1", "date": "2024-04-25", "relevancy": 2.501, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5444}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4765}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20of%20Large%20Language%20Models%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Continual%20Learning%20of%20Large%20Language%20Models%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Haizhou%20Shi%20and%20Zihao%20Xu%20and%20Hengyi%20Wang%20and%20Weiyi%20Qin%20and%20Wenyuan%20Wang%20and%20Yibin%20Wang%20and%20Hao%20Wang%0AAbstract%3A%20%20%20The%20recent%20success%20of%20large%20language%20models%20%28LLMs%29%20trained%20on%20static%2C%0Apre-collected%2C%20general%20datasets%20has%20sparked%20numerous%20research%20directions%20and%0Aapplications.%20One%20such%20direction%20addresses%20the%20non-trivial%20challenge%20of%0Aintegrating%20pre-trained%20LLMs%20into%20dynamic%20data%20distributions%2C%20task%20structures%2C%0Aand%20user%20preferences.%20Pre-trained%20LLMs%2C%20when%20tailored%20for%20specific%20needs%2C%20often%0Aexperience%20significant%20performance%20degradation%20in%20previous%20knowledge%20domains%20--%0Aa%20phenomenon%20known%20as%20%22catastrophic%20forgetting%22.%20While%20extensively%20studied%20in%0Athe%20continual%20learning%20%28CL%29%20community%2C%20it%20presents%20new%20manifestations%20in%20the%0Arealm%20of%20LLMs.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20overview%20of%20the%0Acurrent%20research%20progress%20on%20LLMs%20within%20the%20context%20of%20CL.%20This%20survey%20is%0Astructured%20into%20four%20main%20sections%3A%20we%20first%20describe%20an%20overview%20of%0Acontinually%20learning%20LLMs%2C%20consisting%20of%20two%20directions%20of%20continuity%3A%20vertical%0Acontinuity%20%28or%20vertical%20continual%20learning%29%2C%20i.e.%2C%20continual%20adaptation%20from%0Ageneral%20to%20specific%20capabilities%2C%20and%20horizontal%20continuity%20%28or%20horizontal%0Acontinual%20learning%29%2C%20i.e.%2C%20continual%20adaptation%20across%20time%20and%20domains%0A%28Section%203%29.%20We%20then%20summarize%20three%20stages%20of%20learning%20LLMs%20in%20the%20context%20of%0Amodern%20CL%3A%20Continual%20Pre-Training%20%28CPT%29%2C%20Domain-Adaptive%20Pre-training%20%28DAP%29%2C%0Aand%20Continual%20Fine-Tuning%20%28CFT%29%20%28Section%204%29.%20Then%20we%20provide%20an%20overview%20of%0Aevaluation%20protocols%20for%20continual%20learning%20with%20LLMs%2C%20along%20with%20the%20current%0Aavailable%20data%20sources%20%28Section%205%29.%20Finally%2C%20we%20discuss%20intriguing%20questions%0Apertaining%20to%20continual%20learning%20for%20LLMs%20%28Section%206%29.%20The%20full%20list%20of%20papers%0Aexamined%20in%20this%20survey%20is%20available%20at%0Ahttps%3A//github.com/Wang-ML-Lab/llm-continual-learning-survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16789v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20of%20Large%20Language%20Models%3A%20A%20Comprehensive%20Survey&entry.906535625=Haizhou%20Shi%20and%20Zihao%20Xu%20and%20Hengyi%20Wang%20and%20Weiyi%20Qin%20and%20Wenyuan%20Wang%20and%20Yibin%20Wang%20and%20Hao%20Wang&entry.1292438233=%20%20The%20recent%20success%20of%20large%20language%20models%20%28LLMs%29%20trained%20on%20static%2C%0Apre-collected%2C%20general%20datasets%20has%20sparked%20numerous%20research%20directions%20and%0Aapplications.%20One%20such%20direction%20addresses%20the%20non-trivial%20challenge%20of%0Aintegrating%20pre-trained%20LLMs%20into%20dynamic%20data%20distributions%2C%20task%20structures%2C%0Aand%20user%20preferences.%20Pre-trained%20LLMs%2C%20when%20tailored%20for%20specific%20needs%2C%20often%0Aexperience%20significant%20performance%20degradation%20in%20previous%20knowledge%20domains%20--%0Aa%20phenomenon%20known%20as%20%22catastrophic%20forgetting%22.%20While%20extensively%20studied%20in%0Athe%20continual%20learning%20%28CL%29%20community%2C%20it%20presents%20new%20manifestations%20in%20the%0Arealm%20of%20LLMs.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20overview%20of%20the%0Acurrent%20research%20progress%20on%20LLMs%20within%20the%20context%20of%20CL.%20This%20survey%20is%0Astructured%20into%20four%20main%20sections%3A%20we%20first%20describe%20an%20overview%20of%0Acontinually%20learning%20LLMs%2C%20consisting%20of%20two%20directions%20of%20continuity%3A%20vertical%0Acontinuity%20%28or%20vertical%20continual%20learning%29%2C%20i.e.%2C%20continual%20adaptation%20from%0Ageneral%20to%20specific%20capabilities%2C%20and%20horizontal%20continuity%20%28or%20horizontal%0Acontinual%20learning%29%2C%20i.e.%2C%20continual%20adaptation%20across%20time%20and%20domains%0A%28Section%203%29.%20We%20then%20summarize%20three%20stages%20of%20learning%20LLMs%20in%20the%20context%20of%0Amodern%20CL%3A%20Continual%20Pre-Training%20%28CPT%29%2C%20Domain-Adaptive%20Pre-training%20%28DAP%29%2C%0Aand%20Continual%20Fine-Tuning%20%28CFT%29%20%28Section%204%29.%20Then%20we%20provide%20an%20overview%20of%0Aevaluation%20protocols%20for%20continual%20learning%20with%20LLMs%2C%20along%20with%20the%20current%0Aavailable%20data%20sources%20%28Section%205%29.%20Finally%2C%20we%20discuss%20intriguing%20questions%0Apertaining%20to%20continual%20learning%20for%20LLMs%20%28Section%206%29.%20The%20full%20list%20of%20papers%0Aexamined%20in%20this%20survey%20is%20available%20at%0Ahttps%3A//github.com/Wang-ML-Lab/llm-continual-learning-survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16789v1&entry.124074799=Read"},
{"title": "Self-Balanced R-CNN for Instance Segmentation", "author": "Leonardo Rossi and Akbar Karimi and Andrea Prati", "abstract": "  Current state-of-the-art two-stage models on instance segmentation task\nsuffer from several types of imbalances. In this paper, we address the\nIntersection over the Union (IoU) distribution imbalance of positive input\nRegions of Interest (RoIs) during the training of the second stage. Our\nSelf-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade\n(HTC) model, brings brand new loop mechanisms of bounding box and mask\nrefinements. With an improved Generic RoI Extraction (GRoIE), we also address\nthe feature-level imbalance at the Feature Pyramid Network (FPN) level,\noriginated by a non-uniform integration between low- and high-level features\nfrom the backbone layers. In addition, the redesign of the architecture heads\ntoward a fully convolutional approach with FCC further reduces the number of\nparameters and obtains more clues to the connection between the task to solve\nand the layers used. Moreover, our SBR-CNN model shows the same or even better\nimprovements if adopted in conjunction with other state-of-the-art models. In\nfact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017\ndataset, our model reaches 45.3% and 41.5% AP for object detection and instance\nsegmentation, with 12 epochs and without extra tricks. The code is available at\nhttps://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn\n", "link": "http://arxiv.org/abs/2404.16633v1", "date": "2024-04-25", "relevancy": 2.4938, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5052}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4967}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4944}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Balanced%20R-CNN%20for%20Instance%20Segmentation&body=Title%3A%20Self-Balanced%20R-CNN%20for%20Instance%20Segmentation%0AAuthor%3A%20Leonardo%20Rossi%20and%20Akbar%20Karimi%20and%20Andrea%20Prati%0AAbstract%3A%20%20%20Current%20state-of-the-art%20two-stage%20models%20on%20instance%20segmentation%20task%0Asuffer%20from%20several%20types%20of%20imbalances.%20In%20this%20paper%2C%20we%20address%20the%0AIntersection%20over%20the%20Union%20%28IoU%29%20distribution%20imbalance%20of%20positive%20input%0ARegions%20of%20Interest%20%28RoIs%29%20during%20the%20training%20of%20the%20second%20stage.%20Our%0ASelf-Balanced%20R-CNN%20%28SBR-CNN%29%2C%20an%20evolved%20version%20of%20the%20Hybrid%20Task%20Cascade%0A%28HTC%29%20model%2C%20brings%20brand%20new%20loop%20mechanisms%20of%20bounding%20box%20and%20mask%0Arefinements.%20With%20an%20improved%20Generic%20RoI%20Extraction%20%28GRoIE%29%2C%20we%20also%20address%0Athe%20feature-level%20imbalance%20at%20the%20Feature%20Pyramid%20Network%20%28FPN%29%20level%2C%0Aoriginated%20by%20a%20non-uniform%20integration%20between%20low-%20and%20high-level%20features%0Afrom%20the%20backbone%20layers.%20In%20addition%2C%20the%20redesign%20of%20the%20architecture%20heads%0Atoward%20a%20fully%20convolutional%20approach%20with%20FCC%20further%20reduces%20the%20number%20of%0Aparameters%20and%20obtains%20more%20clues%20to%20the%20connection%20between%20the%20task%20to%20solve%0Aand%20the%20layers%20used.%20Moreover%2C%20our%20SBR-CNN%20model%20shows%20the%20same%20or%20even%20better%0Aimprovements%20if%20adopted%20in%20conjunction%20with%20other%20state-of-the-art%20models.%20In%0Afact%2C%20with%20a%20lightweight%20ResNet-50%20as%20backbone%2C%20evaluated%20on%20COCO%20minival%202017%0Adataset%2C%20our%20model%20reaches%2045.3%25%20and%2041.5%25%20AP%20for%20object%20detection%20and%20instance%0Asegmentation%2C%20with%2012%20epochs%20and%20without%20extra%20tricks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16633v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Balanced%20R-CNN%20for%20Instance%20Segmentation&entry.906535625=Leonardo%20Rossi%20and%20Akbar%20Karimi%20and%20Andrea%20Prati&entry.1292438233=%20%20Current%20state-of-the-art%20two-stage%20models%20on%20instance%20segmentation%20task%0Asuffer%20from%20several%20types%20of%20imbalances.%20In%20this%20paper%2C%20we%20address%20the%0AIntersection%20over%20the%20Union%20%28IoU%29%20distribution%20imbalance%20of%20positive%20input%0ARegions%20of%20Interest%20%28RoIs%29%20during%20the%20training%20of%20the%20second%20stage.%20Our%0ASelf-Balanced%20R-CNN%20%28SBR-CNN%29%2C%20an%20evolved%20version%20of%20the%20Hybrid%20Task%20Cascade%0A%28HTC%29%20model%2C%20brings%20brand%20new%20loop%20mechanisms%20of%20bounding%20box%20and%20mask%0Arefinements.%20With%20an%20improved%20Generic%20RoI%20Extraction%20%28GRoIE%29%2C%20we%20also%20address%0Athe%20feature-level%20imbalance%20at%20the%20Feature%20Pyramid%20Network%20%28FPN%29%20level%2C%0Aoriginated%20by%20a%20non-uniform%20integration%20between%20low-%20and%20high-level%20features%0Afrom%20the%20backbone%20layers.%20In%20addition%2C%20the%20redesign%20of%20the%20architecture%20heads%0Atoward%20a%20fully%20convolutional%20approach%20with%20FCC%20further%20reduces%20the%20number%20of%0Aparameters%20and%20obtains%20more%20clues%20to%20the%20connection%20between%20the%20task%20to%20solve%0Aand%20the%20layers%20used.%20Moreover%2C%20our%20SBR-CNN%20model%20shows%20the%20same%20or%20even%20better%0Aimprovements%20if%20adopted%20in%20conjunction%20with%20other%20state-of-the-art%20models.%20In%0Afact%2C%20with%20a%20lightweight%20ResNet-50%20as%20backbone%2C%20evaluated%20on%20COCO%20minival%202017%0Adataset%2C%20our%20model%20reaches%2045.3%25%20and%2041.5%25%20AP%20for%20object%20detection%20and%20instance%0Asegmentation%2C%20with%2012%20epochs%20and%20without%20extra%20tricks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16633v1&entry.124074799=Read"},
{"title": "OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian\n  Segmentation", "author": "Lizhi Wang and Feng Zhou and Jianqin Yin", "abstract": "  Recent advancements in 3D reconstruction technologies have paved the way for\nhigh-quality and real-time rendering of complex 3D scenes. Despite these\nachievements, a notable challenge persists: it is difficult to precisely\nreconstruct specific objects from large scenes. Current scene reconstruction\ntechniques frequently result in the loss of object detail textures and are\nunable to reconstruct object portions that are occluded or unseen in views. To\naddress this challenge, we delve into the meticulous 3D reconstruction of\nspecific objects within large scenes and propose a framework termed OMEGAS:\nObject Mesh Extraction from Large Scenes Guided by GAussian Segmentation.\nOMEGAS employs a multi-step approach, grounded in several excellent\noff-the-shelf methodologies. Specifically, initially, we utilize the Segment\nAnything Model (SAM) to guide the segmentation of 3D Gaussian Splatting (3DGS),\nthereby creating a basic 3DGS model of the target object. Then, we leverage\nlarge-scale diffusion priors to further refine the details of the 3DGS model,\nespecially aimed at addressing invisible or occluded object portions from the\noriginal scene views. Subsequently, by re-rendering the 3DGS model onto the\nscene views, we achieve accurate object segmentation and effectively remove the\nbackground. Finally, these target-only images are used to improve the 3DGS\nmodel further and extract the definitive 3D object mesh by the SuGaR model. In\nvarious scenarios, our experiments demonstrate that OMEGAS significantly\nsurpasses existing scene reconstruction methods. Our project page is at:\nhttps://github.com/CrystalWlz/OMEGAS\n", "link": "http://arxiv.org/abs/2404.15891v2", "date": "2024-04-25", "relevancy": 2.4848, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6919}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5854}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5648}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OMEGAS%3A%20Object%20Mesh%20Extraction%20from%20Large%20Scenes%20Guided%20by%20Gaussian%0A%20%20Segmentation&body=Title%3A%20OMEGAS%3A%20Object%20Mesh%20Extraction%20from%20Large%20Scenes%20Guided%20by%20Gaussian%0A%20%20Segmentation%0AAuthor%3A%20Lizhi%20Wang%20and%20Feng%20Zhou%20and%20Jianqin%20Yin%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20reconstruction%20technologies%20have%20paved%20the%20way%20for%0Ahigh-quality%20and%20real-time%20rendering%20of%20complex%203D%20scenes.%20Despite%20these%0Aachievements%2C%20a%20notable%20challenge%20persists%3A%20it%20is%20difficult%20to%20precisely%0Areconstruct%20specific%20objects%20from%20large%20scenes.%20Current%20scene%20reconstruction%0Atechniques%20frequently%20result%20in%20the%20loss%20of%20object%20detail%20textures%20and%20are%0Aunable%20to%20reconstruct%20object%20portions%20that%20are%20occluded%20or%20unseen%20in%20views.%20To%0Aaddress%20this%20challenge%2C%20we%20delve%20into%20the%20meticulous%203D%20reconstruction%20of%0Aspecific%20objects%20within%20large%20scenes%20and%20propose%20a%20framework%20termed%20OMEGAS%3A%0AObject%20Mesh%20Extraction%20from%20Large%20Scenes%20Guided%20by%20GAussian%20Segmentation.%0AOMEGAS%20employs%20a%20multi-step%20approach%2C%20grounded%20in%20several%20excellent%0Aoff-the-shelf%20methodologies.%20Specifically%2C%20initially%2C%20we%20utilize%20the%20Segment%0AAnything%20Model%20%28SAM%29%20to%20guide%20the%20segmentation%20of%203D%20Gaussian%20Splatting%20%283DGS%29%2C%0Athereby%20creating%20a%20basic%203DGS%20model%20of%20the%20target%20object.%20Then%2C%20we%20leverage%0Alarge-scale%20diffusion%20priors%20to%20further%20refine%20the%20details%20of%20the%203DGS%20model%2C%0Aespecially%20aimed%20at%20addressing%20invisible%20or%20occluded%20object%20portions%20from%20the%0Aoriginal%20scene%20views.%20Subsequently%2C%20by%20re-rendering%20the%203DGS%20model%20onto%20the%0Ascene%20views%2C%20we%20achieve%20accurate%20object%20segmentation%20and%20effectively%20remove%20the%0Abackground.%20Finally%2C%20these%20target-only%20images%20are%20used%20to%20improve%20the%203DGS%0Amodel%20further%20and%20extract%20the%20definitive%203D%20object%20mesh%20by%20the%20SuGaR%20model.%20In%0Avarious%20scenarios%2C%20our%20experiments%20demonstrate%20that%20OMEGAS%20significantly%0Asurpasses%20existing%20scene%20reconstruction%20methods.%20Our%20project%20page%20is%20at%3A%0Ahttps%3A//github.com/CrystalWlz/OMEGAS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15891v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMEGAS%3A%20Object%20Mesh%20Extraction%20from%20Large%20Scenes%20Guided%20by%20Gaussian%0A%20%20Segmentation&entry.906535625=Lizhi%20Wang%20and%20Feng%20Zhou%20and%20Jianqin%20Yin&entry.1292438233=%20%20Recent%20advancements%20in%203D%20reconstruction%20technologies%20have%20paved%20the%20way%20for%0Ahigh-quality%20and%20real-time%20rendering%20of%20complex%203D%20scenes.%20Despite%20these%0Aachievements%2C%20a%20notable%20challenge%20persists%3A%20it%20is%20difficult%20to%20precisely%0Areconstruct%20specific%20objects%20from%20large%20scenes.%20Current%20scene%20reconstruction%0Atechniques%20frequently%20result%20in%20the%20loss%20of%20object%20detail%20textures%20and%20are%0Aunable%20to%20reconstruct%20object%20portions%20that%20are%20occluded%20or%20unseen%20in%20views.%20To%0Aaddress%20this%20challenge%2C%20we%20delve%20into%20the%20meticulous%203D%20reconstruction%20of%0Aspecific%20objects%20within%20large%20scenes%20and%20propose%20a%20framework%20termed%20OMEGAS%3A%0AObject%20Mesh%20Extraction%20from%20Large%20Scenes%20Guided%20by%20GAussian%20Segmentation.%0AOMEGAS%20employs%20a%20multi-step%20approach%2C%20grounded%20in%20several%20excellent%0Aoff-the-shelf%20methodologies.%20Specifically%2C%20initially%2C%20we%20utilize%20the%20Segment%0AAnything%20Model%20%28SAM%29%20to%20guide%20the%20segmentation%20of%203D%20Gaussian%20Splatting%20%283DGS%29%2C%0Athereby%20creating%20a%20basic%203DGS%20model%20of%20the%20target%20object.%20Then%2C%20we%20leverage%0Alarge-scale%20diffusion%20priors%20to%20further%20refine%20the%20details%20of%20the%203DGS%20model%2C%0Aespecially%20aimed%20at%20addressing%20invisible%20or%20occluded%20object%20portions%20from%20the%0Aoriginal%20scene%20views.%20Subsequently%2C%20by%20re-rendering%20the%203DGS%20model%20onto%20the%0Ascene%20views%2C%20we%20achieve%20accurate%20object%20segmentation%20and%20effectively%20remove%20the%0Abackground.%20Finally%2C%20these%20target-only%20images%20are%20used%20to%20improve%20the%203DGS%0Amodel%20further%20and%20extract%20the%20definitive%203D%20object%20mesh%20by%20the%20SuGaR%20model.%20In%0Avarious%20scenarios%2C%20our%20experiments%20demonstrate%20that%20OMEGAS%20significantly%0Asurpasses%20existing%20scene%20reconstruction%20methods.%20Our%20project%20page%20is%20at%3A%0Ahttps%3A//github.com/CrystalWlz/OMEGAS%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15891v2&entry.124074799=Read"},
{"title": "Research on geometric figure classification algorithm based on Deep\n  Learning", "author": "Ruiyang Wang and Haonan Wang and Junfeng Sun and Mingjia Zhao and Meng Liu", "abstract": "  In recent years, with the rapid development of computer information\ntechnology, the development of artificial intelligence has been accelerating.\nThe traditional geometry recognition technology is relatively backward and the\nrecognition rate is low. In the face of massive information database, the\ntraditional algorithm model inevitably has the problems of low recognition\naccuracy and poor performance. Deep learning theory has gradually become a very\nimportant part of machine learning. The implementation of convolutional neural\nnetwork (CNN) reduces the difficulty of graphics generation algorithm. In this\npaper, using the advantages of lenet-5 architecture sharing weights and feature\nextraction and classification, the proposed geometric pattern recognition\nalgorithm model is faster in the training data set. By constructing the shared\nfeature parameters of the algorithm model, the cross-entropy loss function is\nused in the recognition process to improve the generalization of the model and\nimprove the average recognition accuracy of the test data set.\n", "link": "http://arxiv.org/abs/2404.16561v1", "date": "2024-04-25", "relevancy": 2.432, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5172}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4839}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4581}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Research%20on%20geometric%20figure%20classification%20algorithm%20based%20on%20Deep%0A%20%20Learning&body=Title%3A%20Research%20on%20geometric%20figure%20classification%20algorithm%20based%20on%20Deep%0A%20%20Learning%0AAuthor%3A%20Ruiyang%20Wang%20and%20Haonan%20Wang%20and%20Junfeng%20Sun%20and%20Mingjia%20Zhao%20and%20Meng%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20with%20the%20rapid%20development%20of%20computer%20information%0Atechnology%2C%20the%20development%20of%20artificial%20intelligence%20has%20been%20accelerating.%0AThe%20traditional%20geometry%20recognition%20technology%20is%20relatively%20backward%20and%20the%0Arecognition%20rate%20is%20low.%20In%20the%20face%20of%20massive%20information%20database%2C%20the%0Atraditional%20algorithm%20model%20inevitably%20has%20the%20problems%20of%20low%20recognition%0Aaccuracy%20and%20poor%20performance.%20Deep%20learning%20theory%20has%20gradually%20become%20a%20very%0Aimportant%20part%20of%20machine%20learning.%20The%20implementation%20of%20convolutional%20neural%0Anetwork%20%28CNN%29%20reduces%20the%20difficulty%20of%20graphics%20generation%20algorithm.%20In%20this%0Apaper%2C%20using%20the%20advantages%20of%20lenet-5%20architecture%20sharing%20weights%20and%20feature%0Aextraction%20and%20classification%2C%20the%20proposed%20geometric%20pattern%20recognition%0Aalgorithm%20model%20is%20faster%20in%20the%20training%20data%20set.%20By%20constructing%20the%20shared%0Afeature%20parameters%20of%20the%20algorithm%20model%2C%20the%20cross-entropy%20loss%20function%20is%0Aused%20in%20the%20recognition%20process%20to%20improve%20the%20generalization%20of%20the%20model%20and%0Aimprove%20the%20average%20recognition%20accuracy%20of%20the%20test%20data%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16561v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20geometric%20figure%20classification%20algorithm%20based%20on%20Deep%0A%20%20Learning&entry.906535625=Ruiyang%20Wang%20and%20Haonan%20Wang%20and%20Junfeng%20Sun%20and%20Mingjia%20Zhao%20and%20Meng%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20with%20the%20rapid%20development%20of%20computer%20information%0Atechnology%2C%20the%20development%20of%20artificial%20intelligence%20has%20been%20accelerating.%0AThe%20traditional%20geometry%20recognition%20technology%20is%20relatively%20backward%20and%20the%0Arecognition%20rate%20is%20low.%20In%20the%20face%20of%20massive%20information%20database%2C%20the%0Atraditional%20algorithm%20model%20inevitably%20has%20the%20problems%20of%20low%20recognition%0Aaccuracy%20and%20poor%20performance.%20Deep%20learning%20theory%20has%20gradually%20become%20a%20very%0Aimportant%20part%20of%20machine%20learning.%20The%20implementation%20of%20convolutional%20neural%0Anetwork%20%28CNN%29%20reduces%20the%20difficulty%20of%20graphics%20generation%20algorithm.%20In%20this%0Apaper%2C%20using%20the%20advantages%20of%20lenet-5%20architecture%20sharing%20weights%20and%20feature%0Aextraction%20and%20classification%2C%20the%20proposed%20geometric%20pattern%20recognition%0Aalgorithm%20model%20is%20faster%20in%20the%20training%20data%20set.%20By%20constructing%20the%20shared%0Afeature%20parameters%20of%20the%20algorithm%20model%2C%20the%20cross-entropy%20loss%20function%20is%0Aused%20in%20the%20recognition%20process%20to%20improve%20the%20generalization%20of%20the%20model%20and%0Aimprove%20the%20average%20recognition%20accuracy%20of%20the%20test%20data%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16561v1&entry.124074799=Read"},
{"title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context", "author": " Gemini Team and Machel Reid and Nikolay Savinov and Denis Teplyashin and  Dmitry and  Lepikhin and Timothy Lillicrap and Jean-baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser and Ioannis Antonoglou and Rohan Anil and Sebastian Borgeaud and Andrew Dai and Katie Millican and Ethan Dyer and Mia Glaese and Thibault Sottiaux and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and James Molloy and Jilin Chen and Michael Isard and Paul Barham and Tom Hennigan and Ross McIlroy and Melvin Johnson and Johan Schalkwyk and Eli Collins and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Clemens Meyer and Gregory Thornton and Zhen Yang and Henryk Michalewski and Zaheer Abbas and Nathan Schucher and Ankesh Anand and Richard Ives and James Keeling and Karel Lenc and Salem Haykal and Siamak Shakeri and Pranav Shyam and Aakanksha Chowdhery and Roman Ring and Stephen Spencer and Eren Sezener and Luke Vilnis and Oscar Chang and Nobuyuki Morioka and George Tucker and Ce Zheng and Oliver Woodman and Nithya Attaluri and Tomas Kocisky and Evgenii Eltyshev and Xi Chen and Timothy Chung and Vittorio Selo and Siddhartha Brahma and Petko Georgiev and Ambrose Slone and Zhenkai Zhu and James Lottes and Siyuan Qiao and Ben Caine and Sebastian Riedel and Alex Tomala and Martin Chadwick and Juliette Love and Peter Choy and Sid Mittal and Neil Houlsby and Yunhao Tang and Matthew Lamm and Libin Bai and Qiao Zhang and Luheng He and Yong Cheng and Peter Humphreys and Yujia Li and Sergey Brin and Albin Cassirer and Yingjie Miao and Lukas Zilka and Taylor Tobin and Kelvin Xu and Lev Proleev and Daniel Sohn and Alberto Magni and Lisa Anne Hendricks and Isabel Gao and Santiago Ontanon and Oskar Bunyan and Nathan Byrd and Abhanshu Sharma and Biao Zhang and Mario Pinto and Rishika Sinha and Harsh Mehta and Dawei Jia and Sergi Caelles and Albert Webson and Alex Morris and Becca Roelofs and Yifan Ding and Robin Strudel and Xuehan Xiong and Marvin Ritter and Mostafa Dehghani and Rahma Chaabouni and Abhijit Karmarkar and Guangda Lai and Fabian Mentzer and Bibo Xu and YaGuang Li and Yujing Zhang and Tom Le Paine and Alex Goldin and Behnam Neyshabur and Kate Baumli and Anselm Levskaya and Michael Laskin and Wenhao Jia and Jack W. Rae and Kefan Xiao and Antoine He and Skye Giordano and Lakshman Yagati and Jean-Baptiste Lespiau and Paul Natsev and Sanjay Ganapathy and Fangyu Liu and Danilo Martins and Nanxin Chen and Yunhan Xu and Megan Barnes and Rhys May and Arpi Vezer and Junhyuk Oh and Ken Franko and Sophie Bridgers and Ruizhe Zhao and Boxi Wu and Basil Mustafa and Sean Sechrist and Emilio Parisotto and Thanumalayan Sankaranarayana Pillai and Chris Larkin and Chenjie Gu and Christina Sorokin and Maxim Krikun and Alexey Guseynov and Jessica Landon and Romina Datta and Alexander Pritzel and Phoebe Thacker and Fan Yang and Kevin Hui and Anja Hauth and Chih-Kuan Yeh and David Barker and Justin Mao-Jones and Sophia Austin and Hannah Sheahan and Parker Schuh and James Svensson and Rohan Jain and Vinay Ramasesh and Anton Briukhov and Da-Woon Chung and Tamara von Glehn and Christina Butterfield and Priya Jhakra and Matthew Wiethoff and Justin Frye and Jordan Grimstad and Beer Changpinyo and Charline Le Lan and Anna Bortsova and Yonghui Wu and Paul Voigtlaender and Tara Sainath and Shane Gu and Charlotte Smith and Will Hawkins and Kris Cao and James Besley and Srivatsan Srinivasan and Mark Omernick and Colin Gaffney and Gabriela Surita and Ryan Burnell and Bogdan Damoc and Junwhan Ahn and Andrew Brock and Mantas Pajarskas and Anastasia Petrushkina and Seb Noury and Lorenzo Blanco and Kevin Swersky and Arun Ahuja and Thi Avrahami and Vedant Misra and Raoul de Liedekerke and Mariko Iinuma and Alex Polozov and Sarah York and George van den Driessche and Paul Michel and Justin Chiu and Rory Blevins and Zach Gleicher and Adri\u00e0 Recasens and Alban Rrustemi and Elena Gribovskaya and Aurko Roy and Wiktor Gworek and S\u00e9bastien M. R. Arnold and Lisa Lee and James Lee-Thorp and Marcello Maggioni and Enrique Piqueras and Kartikeya Badola and Sharad Vikram and Lucas Gonzalez and Anirudh Baddepudi and Evan Senter and Jacob Devlin and James Qin and Michael Azzam and Maja Trebacz and Martin Polacek and Kashyap Krishnakumar and Shuo-yiin Chang and Matthew Tung and Ivo Penchev and Rishabh Joshi and Kate Olszewska and Carrie Muir and Mateo Wirth and Ale Jakse Hartman and Josh Newlan and Sheleem Kashem and Vijay Bolina and Elahe Dabir and Joost van Amersfoort and Zafarali Ahmed and James Cobon-Kerr and Aishwarya Kamath and Arnar Mar Hrafnkelsson and Le Hou and Ian Mackinnon and Alexandre Frechette and Eric Noland and Xiance Si and Emanuel Taropa and Dong Li and Phil Crone and Anmol Gulati and S\u00e9bastien Cevey and Jonas Adler and Ada Ma and David Silver and Simon Tokumine and Richard Powell and Stephan Lee and Kiran Vodrahalli and Samer Hassan and Diana Mincu and Antoine Yang and Nir Levine and Jenny Brennan and Mingqiu Wang and Sarah Hodkinson and Jeffrey Zhao and Josh Lipschultz and Aedan Pope and Michael B. Chang and Cheng Li and Laurent El Shafey and Michela Paganini and Sholto Douglas and Bernd Bohnet and Fabio Pardo and Seth Odoom and Mihaela Rosca and Cicero Nogueira dos Santos and Kedar Soparkar and Arthur Guez and Tom Hudson and Steven Hansen and Chulayuth Asawaroengchai and Ravi Addanki and Tianhe Yu and Wojciech Stokowiec and Mina Khan and Justin Gilmer and Jaehoon Lee and Carrie Grimes Bostock and Keran Rong and Jonathan Caton and Pedram Pejman and Filip Pavetic and Geoff Brown and Vivek Sharma and Mario Lu\u010di\u0107 and Rajkumar Samuel and Josip Djolonga and Amol Mandhane and Lars Lowe Sj\u00f6sund and Elena Buchatskaya and Elspeth White and Natalie Clay and Jiepu Jiang and Hyeontaek Lim and Ross Hemsley and Zeyncep Cankara and Jane Labanowski and Nicola De Cao and David Steiner and Sayed Hadi Hashemi and Jacob Austin and Anita Gergely and Tim Blyth and Joe Stanton and Kaushik Shivakumar and Aditya Siddhant and Anders Andreassen and Carlos Araya and Nikhil Sethi and Rakesh Shivanna and Steven Hand and Ankur Bapna and Ali Khodaei and Antoine Miech and Garrett Tanzer and Andy Swing and Shantanu Thakoor and Lora Aroyo and Zhufeng Pan and Zachary Nado and Jakub Sygnowski and Stephanie Winkler and Dian Yu and Mohammad Saleh and Loren Maggiore and Yamini Bansal and Xavier Garcia and Mehran Kazemi and Piyush Patil and Ishita Dasgupta and Iain Barr and Minh Giang and Thais Kagohara and Ivo Danihelka and Amit Marathe and Vladimir Feinberg and Mohamed Elhawaty and Nimesh Ghelani and Dan Horgan and Helen Miller and Lexi Walker and Richard Tanburn and Mukarram Tariq and Disha Shrivastava and Fei Xia and Qingze Wang and Chung-Cheng Chiu and Zoe Ashwood and Khuslen Baatarsukh and Sina Samangooei and Rapha\u00ebl Lopez Kaufman and Fred Alcober and Axel Stjerngren and Paul Komarek and Katerina Tsihlas and Anudhyan Boral and Ramona Comanescu and Jeremy Chen and Ruibo Liu and Chris Welty and Dawn Bloxwich and Charlie Chen and Yanhua Sun and Fangxiaoyu Feng and Matthew Mauger and Xerxes Dotiwalla and Vincent Hellendoorn and Michael Sharman and Ivy Zheng and Krishna Haridasan and Gabe Barth-Maron and Craig Swanson and Dominika Rogozi\u0144ska and Alek Andreev and Paul Kishan Rubenstein and Ruoxin Sang and Dan Hurt and Gamaleldin Elsayed and Renshen Wang and Dave Lacey and Anastasija Ili\u0107 and Yao Zhao and Adam Iwanicki and Alejandro Lince and Alexander Chen and Christina Lyu and Carl Lebsack and Jordan Griffith and Meenu Gaba and Paramjit Sandhu and Phil Chen and Anna Koop and Ravi Rajwar and Soheil Hassas Yeganeh and Solomon Chang and Rui Zhu and Soroush Radpour and Elnaz Davoodi and Ving Ian Lei and Yang Xu and Daniel Toyama and Constant Segal and Martin Wicke and Hanzhao Lin and Anna Bulanova and Adri\u00e0 Puigdom\u00e8nech Badia and Nemanja Raki\u0107evi\u0107 and Pablo Sprechmann and Angelos Filos and Shaobo Hou and V\u00edctor Campos and Nora Kassner and Devendra Sachan and Meire Fortunato and Chimezie Iwuanyanwu and Vitaly Nikolaev and Balaji Lakshminarayanan and Sadegh Jazayeri and Mani Varadarajan and Chetan Tekur and Doug Fritz and Misha Khalman and David Reitter and Kingshuk Dasgupta and Shourya Sarcar and Tina Ornduff and Javier Snaider and Fantine Huot and Johnson Jia and Rupert Kemp and Nejc Trdin and Anitha Vijayakumar and Lucy Kim and Christof Angermueller and Li Lao and Tianqi Liu and Haibin Zhang and David Engel and Somer Greene and Ana\u00efs White and Jessica Austin and Lilly Taylor and Shereen Ashraf and Dangyi Liu and Maria Georgaki and Irene Cai and Yana Kulizhskaya and Sonam Goenka and Brennan Saeta and Ying Xu and Christian Frank and Dario de Cesare and Brona Robenek and Harry Richardson and Mahmoud Alnahlawi and Christopher Yew and Priya Ponnapalli and Marco Tagliasacchi and Alex Korchemniy and Yelin Kim and Dinghua Li and Bill Rosgen and Kyle Levin and Jeremy Wiesner and Praseem Banzal and Praveen Srinivasan and Hongkun Yu and \u00c7a\u011flar \u00dcnl\u00fc and David Reid and Zora Tung and Daniel Finchelstein and Ravin Kumar and Andre Elisseeff and Jin Huang and Ming Zhang and Ricardo Aguilar and Mai Gim\u00e9nez and Jiawei Xia and Olivier Dousse and Willi Gierke and Damion Yates and Komal Jalan and Lu Li and Eri Latorre-Chimoto and Duc Dung Nguyen and Ken Durden and Praveen Kallakuri and Yaxin Liu and Matthew Johnson and Tomy Tsai and Alice Talbert and Jasmine Liu and Alexander Neitz and Chen Elkind and Marco Selvi and Mimi Jasarevic and Livio Baldini Soares and Albert Cui and Pidong Wang and Alek Wenjiao Wang and Xinyu Ye and Krystal Kallarackal and Lucia Loher and Hoi Lam and Josef Broder and Dan Holtmann-Rice and Nina Martin and Bramandia Ramadhana and Mrinal Shukla and Sujoy Basu and Abhi Mohan and Nick Fernando and Noah Fiedel and Kim Paterson and Hui Li and Ankush Garg and Jane Park and DongHyun Choi and Diane Wu and Sankalp Singh and Zhishuai Zhang and Amir Globerson and Lily Yu and John Carpenter and F\u00e9lix de Chaumont Quitry and Carey Radebaugh and Chu-Cheng Lin and Alex Tudor and Prakash Shroff and Drew Garmon and Dayou Du and Neera Vats and Han Lu and Shariq Iqbal and Alex Yakubovich and Nilesh Tripuraneni and James Manyika and Haroon Qureshi and Nan Hua and Christel Ngani and Maria Abi Raad and Hannah Forbes and Jeff Stanway and Mukund Sundararajan and Victor Ungureanu and Colton Bishop and Yunjie Li and Balaji Venkatraman and Bo Li and Chloe Thornton and Salvatore Scellato and Nishesh Gupta and Yicheng Wang and Ian Tenney and Xihui Wu and Ashish Shenoy and Gabriel Carvajal and Diana Gage Wright and Ben Bariach and Zhuyun Xiao and Peter Hawkins and Sid Dalmia and Clement Farabet and Pedro Valenzuela and Quan Yuan and Ananth Agarwal and Mia Chen and Wooyeol Kim and Brice Hulse and Nandita Dukkipati and Adam Paszke and Andrew Bolt and Kiam Choo and Jennifer Beattie and Jennifer Prendki and Harsha Vashisht and Rebeca Santamaria-Fernandez and Luis C. Cobo and Jarek Wilkiewicz and David Madras and Ali Elqursh and Grant Uy and Kevin Ramirez and Matt Harvey and Tyler Liechty and Heiga Zen and Jeff Seibert and Clara Huiyi Hu and Andrey Khorlin and Maigo Le and Asaf Aharoni and Megan Li and Lily Wang and Sandeep Kumar and Norman Casagrande and Jay Hoover and Dalia El Badawy and David Soergel and Denis Vnukov and Matt Miecnikowski and Jiri Simsa and Praveen Kumar and Thibault Sellam and Daniel Vlasic and Samira Daruki and Nir Shabat and John Zhang and Guolong Su and Jiageng Zhang and Jeremiah Liu and Yi Sun and Evan Palmer and Alireza Ghaffarkhah and Xi Xiong and Victor Cotruta and Michael Fink and Lucas Dixon and Ashwin Sreevatsa and Adrian Goedeckemeyer and Alek Dimitriev and Mohsen Jafari and Remi Crocker and Nicholas FitzGerald and Aviral Kumar and Sanjay Ghemawat and Ivan Philips and Frederick Liu and Yannie Liang and Rachel Sterneck and Alena Repina and Marcus Wu and Laura Knight and Marin Georgiev and Hyo Lee and Harry Askham and Abhishek Chakladar and Annie Louis and Carl Crous and Hardie Cate and Dessie Petrova and Michael Quinn and Denese Owusu-Afriyie and Achintya Singhal and Nan Wei and Solomon Kim and Damien Vincent and Milad Nasr and Christopher A. Choquette-Choo and Reiko Tojo and Shawn Lu and Diego de Las Casas and Yuchung Cheng and Tolga Bolukbasi and Katherine Lee and Saaber Fatehi and Rajagopal Ananthanarayanan and Miteyan Patel and Charbel Kaed and Jing Li and Shreyas Rammohan Belle and Zhe Chen and Jaclyn Konzelmann and Siim P\u00f5der and Roopal Garg and Vinod Koverkathu and Adam Brown and Chris Dyer and Rosanne Liu and Azade Nova and Jun Xu and Alanna Walton and Alicia Parrish and Mark Epstein and Sara McCarthy and Slav Petrov and Demis Hassabis and Koray Kavukcuoglu and Jeffrey Dean and Oriol Vinyals", "abstract": "  In this report, we present the latest model of the Gemini family, Gemini 1.5\nPro, a highly compute-efficient multimodal mixture-of-experts model capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio.\nGemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks\nacross modalities, improves the state-of-the-art in long-document QA,\nlong-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's\nstate-of-the-art performance across a broad set of benchmarks. Studying the\nlimits of Gemini 1.5 Pro's long-context ability, we find continued improvement\nin next-token prediction and near-perfect retrieval (>99%) up to at least 10M\ntokens, a generational leap over existing models such as Claude 2.1 (200k) and\nGPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large\nlanguage models at the frontier; when given a grammar manual for Kalamang, a\nlanguage with fewer than 200 speakers worldwide, the model learns to translate\nEnglish to Kalamang at a similar level to a person who learned from the same\ncontent.\n", "link": "http://arxiv.org/abs/2403.05530v2", "date": "2024-04-25", "relevancy": 2.4086, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4918}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4887}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4647}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context&body=Title%3A%20Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context%0AAuthor%3A%20%20Gemini%20Team%20and%20Machel%20Reid%20and%20Nikolay%20Savinov%20and%20Denis%20Teplyashin%20and%20%20Dmitry%20and%20%20Lepikhin%20and%20Timothy%20Lillicrap%20and%20Jean-baptiste%20Alayrac%20and%20Radu%20Soricut%20and%20Angeliki%20Lazaridou%20and%20Orhan%20Firat%20and%20Julian%20Schrittwieser%20and%20Ioannis%20Antonoglou%20and%20Rohan%20Anil%20and%20Sebastian%20Borgeaud%20and%20Andrew%20Dai%20and%20Katie%20Millican%20and%20Ethan%20Dyer%20and%20Mia%20Glaese%20and%20Thibault%20Sottiaux%20and%20Benjamin%20Lee%20and%20Fabio%20Viola%20and%20Malcolm%20Reynolds%20and%20Yuanzhong%20Xu%20and%20James%20Molloy%20and%20Jilin%20Chen%20and%20Michael%20Isard%20and%20Paul%20Barham%20and%20Tom%20Hennigan%20and%20Ross%20McIlroy%20and%20Melvin%20Johnson%20and%20Johan%20Schalkwyk%20and%20Eli%20Collins%20and%20Eliza%20Rutherford%20and%20Erica%20Moreira%20and%20Kareem%20Ayoub%20and%20Megha%20Goel%20and%20Clemens%20Meyer%20and%20Gregory%20Thornton%20and%20Zhen%20Yang%20and%20Henryk%20Michalewski%20and%20Zaheer%20Abbas%20and%20Nathan%20Schucher%20and%20Ankesh%20Anand%20and%20Richard%20Ives%20and%20James%20Keeling%20and%20Karel%20Lenc%20and%20Salem%20Haykal%20and%20Siamak%20Shakeri%20and%20Pranav%20Shyam%20and%20Aakanksha%20Chowdhery%20and%20Roman%20Ring%20and%20Stephen%20Spencer%20and%20Eren%20Sezener%20and%20Luke%20Vilnis%20and%20Oscar%20Chang%20and%20Nobuyuki%20Morioka%20and%20George%20Tucker%20and%20Ce%20Zheng%20and%20Oliver%20Woodman%20and%20Nithya%20Attaluri%20and%20Tomas%20Kocisky%20and%20Evgenii%20Eltyshev%20and%20Xi%20Chen%20and%20Timothy%20Chung%20and%20Vittorio%20Selo%20and%20Siddhartha%20Brahma%20and%20Petko%20Georgiev%20and%20Ambrose%20Slone%20and%20Zhenkai%20Zhu%20and%20James%20Lottes%20and%20Siyuan%20Qiao%20and%20Ben%20Caine%20and%20Sebastian%20Riedel%20and%20Alex%20Tomala%20and%20Martin%20Chadwick%20and%20Juliette%20Love%20and%20Peter%20Choy%20and%20Sid%20Mittal%20and%20Neil%20Houlsby%20and%20Yunhao%20Tang%20and%20Matthew%20Lamm%20and%20Libin%20Bai%20and%20Qiao%20Zhang%20and%20Luheng%20He%20and%20Yong%20Cheng%20and%20Peter%20Humphreys%20and%20Yujia%20Li%20and%20Sergey%20Brin%20and%20Albin%20Cassirer%20and%20Yingjie%20Miao%20and%20Lukas%20Zilka%20and%20Taylor%20Tobin%20and%20Kelvin%20Xu%20and%20Lev%20Proleev%20and%20Daniel%20Sohn%20and%20Alberto%20Magni%20and%20Lisa%20Anne%20Hendricks%20and%20Isabel%20Gao%20and%20Santiago%20Ontanon%20and%20Oskar%20Bunyan%20and%20Nathan%20Byrd%20and%20Abhanshu%20Sharma%20and%20Biao%20Zhang%20and%20Mario%20Pinto%20and%20Rishika%20Sinha%20and%20Harsh%20Mehta%20and%20Dawei%20Jia%20and%20Sergi%20Caelles%20and%20Albert%20Webson%20and%20Alex%20Morris%20and%20Becca%20Roelofs%20and%20Yifan%20Ding%20and%20Robin%20Strudel%20and%20Xuehan%20Xiong%20and%20Marvin%20Ritter%20and%20Mostafa%20Dehghani%20and%20Rahma%20Chaabouni%20and%20Abhijit%20Karmarkar%20and%20Guangda%20Lai%20and%20Fabian%20Mentzer%20and%20Bibo%20Xu%20and%20YaGuang%20Li%20and%20Yujing%20Zhang%20and%20Tom%20Le%20Paine%20and%20Alex%20Goldin%20and%20Behnam%20Neyshabur%20and%20Kate%20Baumli%20and%20Anselm%20Levskaya%20and%20Michael%20Laskin%20and%20Wenhao%20Jia%20and%20Jack%20W.%20Rae%20and%20Kefan%20Xiao%20and%20Antoine%20He%20and%20Skye%20Giordano%20and%20Lakshman%20Yagati%20and%20Jean-Baptiste%20Lespiau%20and%20Paul%20Natsev%20and%20Sanjay%20Ganapathy%20and%20Fangyu%20Liu%20and%20Danilo%20Martins%20and%20Nanxin%20Chen%20and%20Yunhan%20Xu%20and%20Megan%20Barnes%20and%20Rhys%20May%20and%20Arpi%20Vezer%20and%20Junhyuk%20Oh%20and%20Ken%20Franko%20and%20Sophie%20Bridgers%20and%20Ruizhe%20Zhao%20and%20Boxi%20Wu%20and%20Basil%20Mustafa%20and%20Sean%20Sechrist%20and%20Emilio%20Parisotto%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Chris%20Larkin%20and%20Chenjie%20Gu%20and%20Christina%20Sorokin%20and%20Maxim%20Krikun%20and%20Alexey%20Guseynov%20and%20Jessica%20Landon%20and%20Romina%20Datta%20and%20Alexander%20Pritzel%20and%20Phoebe%20Thacker%20and%20Fan%20Yang%20and%20Kevin%20Hui%20and%20Anja%20Hauth%20and%20Chih-Kuan%20Yeh%20and%20David%20Barker%20and%20Justin%20Mao-Jones%20and%20Sophia%20Austin%20and%20Hannah%20Sheahan%20and%20Parker%20Schuh%20and%20James%20Svensson%20and%20Rohan%20Jain%20and%20Vinay%20Ramasesh%20and%20Anton%20Briukhov%20and%20Da-Woon%20Chung%20and%20Tamara%20von%20Glehn%20and%20Christina%20Butterfield%20and%20Priya%20Jhakra%20and%20Matthew%20Wiethoff%20and%20Justin%20Frye%20and%20Jordan%20Grimstad%20and%20Beer%20Changpinyo%20and%20Charline%20Le%20Lan%20and%20Anna%20Bortsova%20and%20Yonghui%20Wu%20and%20Paul%20Voigtlaender%20and%20Tara%20Sainath%20and%20Shane%20Gu%20and%20Charlotte%20Smith%20and%20Will%20Hawkins%20and%20Kris%20Cao%20and%20James%20Besley%20and%20Srivatsan%20Srinivasan%20and%20Mark%20Omernick%20and%20Colin%20Gaffney%20and%20Gabriela%20Surita%20and%20Ryan%20Burnell%20and%20Bogdan%20Damoc%20and%20Junwhan%20Ahn%20and%20Andrew%20Brock%20and%20Mantas%20Pajarskas%20and%20Anastasia%20Petrushkina%20and%20Seb%20Noury%20and%20Lorenzo%20Blanco%20and%20Kevin%20Swersky%20and%20Arun%20Ahuja%20and%20Thi%20Avrahami%20and%20Vedant%20Misra%20and%20Raoul%20de%20Liedekerke%20and%20Mariko%20Iinuma%20and%20Alex%20Polozov%20and%20Sarah%20York%20and%20George%20van%20den%20Driessche%20and%20Paul%20Michel%20and%20Justin%20Chiu%20and%20Rory%20Blevins%20and%20Zach%20Gleicher%20and%20Adri%C3%A0%20Recasens%20and%20Alban%20Rrustemi%20and%20Elena%20Gribovskaya%20and%20Aurko%20Roy%20and%20Wiktor%20Gworek%20and%20S%C3%A9bastien%20M.%20R.%20Arnold%20and%20Lisa%20Lee%20and%20James%20Lee-Thorp%20and%20Marcello%20Maggioni%20and%20Enrique%20Piqueras%20and%20Kartikeya%20Badola%20and%20Sharad%20Vikram%20and%20Lucas%20Gonzalez%20and%20Anirudh%20Baddepudi%20and%20Evan%20Senter%20and%20Jacob%20Devlin%20and%20James%20Qin%20and%20Michael%20Azzam%20and%20Maja%20Trebacz%20and%20Martin%20Polacek%20and%20Kashyap%20Krishnakumar%20and%20Shuo-yiin%20Chang%20and%20Matthew%20Tung%20and%20Ivo%20Penchev%20and%20Rishabh%20Joshi%20and%20Kate%20Olszewska%20and%20Carrie%20Muir%20and%20Mateo%20Wirth%20and%20Ale%20Jakse%20Hartman%20and%20Josh%20Newlan%20and%20Sheleem%20Kashem%20and%20Vijay%20Bolina%20and%20Elahe%20Dabir%20and%20Joost%20van%20Amersfoort%20and%20Zafarali%20Ahmed%20and%20James%20Cobon-Kerr%20and%20Aishwarya%20Kamath%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Le%20Hou%20and%20Ian%20Mackinnon%20and%20Alexandre%20Frechette%20and%20Eric%20Noland%20and%20Xiance%20Si%20and%20Emanuel%20Taropa%20and%20Dong%20Li%20and%20Phil%20Crone%20and%20Anmol%20Gulati%20and%20S%C3%A9bastien%20Cevey%20and%20Jonas%20Adler%20and%20Ada%20Ma%20and%20David%20Silver%20and%20Simon%20Tokumine%20and%20Richard%20Powell%20and%20Stephan%20Lee%20and%20Kiran%20Vodrahalli%20and%20Samer%20Hassan%20and%20Diana%20Mincu%20and%20Antoine%20Yang%20and%20Nir%20Levine%20and%20Jenny%20Brennan%20and%20Mingqiu%20Wang%20and%20Sarah%20Hodkinson%20and%20Jeffrey%20Zhao%20and%20Josh%20Lipschultz%20and%20Aedan%20Pope%20and%20Michael%20B.%20Chang%20and%20Cheng%20Li%20and%20Laurent%20El%20Shafey%20and%20Michela%20Paganini%20and%20Sholto%20Douglas%20and%20Bernd%20Bohnet%20and%20Fabio%20Pardo%20and%20Seth%20Odoom%20and%20Mihaela%20Rosca%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Kedar%20Soparkar%20and%20Arthur%20Guez%20and%20Tom%20Hudson%20and%20Steven%20Hansen%20and%20Chulayuth%20Asawaroengchai%20and%20Ravi%20Addanki%20and%20Tianhe%20Yu%20and%20Wojciech%20Stokowiec%20and%20Mina%20Khan%20and%20Justin%20Gilmer%20and%20Jaehoon%20Lee%20and%20Carrie%20Grimes%20Bostock%20and%20Keran%20Rong%20and%20Jonathan%20Caton%20and%20Pedram%20Pejman%20and%20Filip%20Pavetic%20and%20Geoff%20Brown%20and%20Vivek%20Sharma%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Rajkumar%20Samuel%20and%20Josip%20Djolonga%20and%20Amol%20Mandhane%20and%20Lars%20Lowe%20Sj%C3%B6sund%20and%20Elena%20Buchatskaya%20and%20Elspeth%20White%20and%20Natalie%20Clay%20and%20Jiepu%20Jiang%20and%20Hyeontaek%20Lim%20and%20Ross%20Hemsley%20and%20Zeyncep%20Cankara%20and%20Jane%20Labanowski%20and%20Nicola%20De%20Cao%20and%20David%20Steiner%20and%20Sayed%20Hadi%20Hashemi%20and%20Jacob%20Austin%20and%20Anita%20Gergely%20and%20Tim%20Blyth%20and%20Joe%20Stanton%20and%20Kaushik%20Shivakumar%20and%20Aditya%20Siddhant%20and%20Anders%20Andreassen%20and%20Carlos%20Araya%20and%20Nikhil%20Sethi%20and%20Rakesh%20Shivanna%20and%20Steven%20Hand%20and%20Ankur%20Bapna%20and%20Ali%20Khodaei%20and%20Antoine%20Miech%20and%20Garrett%20Tanzer%20and%20Andy%20Swing%20and%20Shantanu%20Thakoor%20and%20Lora%20Aroyo%20and%20Zhufeng%20Pan%20and%20Zachary%20Nado%20and%20Jakub%20Sygnowski%20and%20Stephanie%20Winkler%20and%20Dian%20Yu%20and%20Mohammad%20Saleh%20and%20Loren%20Maggiore%20and%20Yamini%20Bansal%20and%20Xavier%20Garcia%20and%20Mehran%20Kazemi%20and%20Piyush%20Patil%20and%20Ishita%20Dasgupta%20and%20Iain%20Barr%20and%20Minh%20Giang%20and%20Thais%20Kagohara%20and%20Ivo%20Danihelka%20and%20Amit%20Marathe%20and%20Vladimir%20Feinberg%20and%20Mohamed%20Elhawaty%20and%20Nimesh%20Ghelani%20and%20Dan%20Horgan%20and%20Helen%20Miller%20and%20Lexi%20Walker%20and%20Richard%20Tanburn%20and%20Mukarram%20Tariq%20and%20Disha%20Shrivastava%20and%20Fei%20Xia%20and%20Qingze%20Wang%20and%20Chung-Cheng%20Chiu%20and%20Zoe%20Ashwood%20and%20Khuslen%20Baatarsukh%20and%20Sina%20Samangooei%20and%20Rapha%C3%ABl%20Lopez%20Kaufman%20and%20Fred%20Alcober%20and%20Axel%20Stjerngren%20and%20Paul%20Komarek%20and%20Katerina%20Tsihlas%20and%20Anudhyan%20Boral%20and%20Ramona%20Comanescu%20and%20Jeremy%20Chen%20and%20Ruibo%20Liu%20and%20Chris%20Welty%20and%20Dawn%20Bloxwich%20and%20Charlie%20Chen%20and%20Yanhua%20Sun%20and%20Fangxiaoyu%20Feng%20and%20Matthew%20Mauger%20and%20Xerxes%20Dotiwalla%20and%20Vincent%20Hellendoorn%20and%20Michael%20Sharman%20and%20Ivy%20Zheng%20and%20Krishna%20Haridasan%20and%20Gabe%20Barth-Maron%20and%20Craig%20Swanson%20and%20Dominika%20Rogozi%C5%84ska%20and%20Alek%20Andreev%20and%20Paul%20Kishan%20Rubenstein%20and%20Ruoxin%20Sang%20and%20Dan%20Hurt%20and%20Gamaleldin%20Elsayed%20and%20Renshen%20Wang%20and%20Dave%20Lacey%20and%20Anastasija%20Ili%C4%87%20and%20Yao%20Zhao%20and%20Adam%20Iwanicki%20and%20Alejandro%20Lince%20and%20Alexander%20Chen%20and%20Christina%20Lyu%20and%20Carl%20Lebsack%20and%20Jordan%20Griffith%20and%20Meenu%20Gaba%20and%20Paramjit%20Sandhu%20and%20Phil%20Chen%20and%20Anna%20Koop%20and%20Ravi%20Rajwar%20and%20Soheil%20Hassas%20Yeganeh%20and%20Solomon%20Chang%20and%20Rui%20Zhu%20and%20Soroush%20Radpour%20and%20Elnaz%20Davoodi%20and%20Ving%20Ian%20Lei%20and%20Yang%20Xu%20and%20Daniel%20Toyama%20and%20Constant%20Segal%20and%20Martin%20Wicke%20and%20Hanzhao%20Lin%20and%20Anna%20Bulanova%20and%20Adri%C3%A0%20Puigdom%C3%A8nech%20Badia%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Pablo%20Sprechmann%20and%20Angelos%20Filos%20and%20Shaobo%20Hou%20and%20V%C3%ADctor%20Campos%20and%20Nora%20Kassner%20and%20Devendra%20Sachan%20and%20Meire%20Fortunato%20and%20Chimezie%20Iwuanyanwu%20and%20Vitaly%20Nikolaev%20and%20Balaji%20Lakshminarayanan%20and%20Sadegh%20Jazayeri%20and%20Mani%20Varadarajan%20and%20Chetan%20Tekur%20and%20Doug%20Fritz%20and%20Misha%20Khalman%20and%20David%20Reitter%20and%20Kingshuk%20Dasgupta%20and%20Shourya%20Sarcar%20and%20Tina%20Ornduff%20and%20Javier%20Snaider%20and%20Fantine%20Huot%20and%20Johnson%20Jia%20and%20Rupert%20Kemp%20and%20Nejc%20Trdin%20and%20Anitha%20Vijayakumar%20and%20Lucy%20Kim%20and%20Christof%20Angermueller%20and%20Li%20Lao%20and%20Tianqi%20Liu%20and%20Haibin%20Zhang%20and%20David%20Engel%20and%20Somer%20Greene%20and%20Ana%C3%AFs%20White%20and%20Jessica%20Austin%20and%20Lilly%20Taylor%20and%20Shereen%20Ashraf%20and%20Dangyi%20Liu%20and%20Maria%20Georgaki%20and%20Irene%20Cai%20and%20Yana%20Kulizhskaya%20and%20Sonam%20Goenka%20and%20Brennan%20Saeta%20and%20Ying%20Xu%20and%20Christian%20Frank%20and%20Dario%20de%20Cesare%20and%20Brona%20Robenek%20and%20Harry%20Richardson%20and%20Mahmoud%20Alnahlawi%20and%20Christopher%20Yew%20and%20Priya%20Ponnapalli%20and%20Marco%20Tagliasacchi%20and%20Alex%20Korchemniy%20and%20Yelin%20Kim%20and%20Dinghua%20Li%20and%20Bill%20Rosgen%20and%20Kyle%20Levin%20and%20Jeremy%20Wiesner%20and%20Praseem%20Banzal%20and%20Praveen%20Srinivasan%20and%20Hongkun%20Yu%20and%20%C3%87a%C4%9Flar%20%C3%9Cnl%C3%BC%20and%20David%20Reid%20and%20Zora%20Tung%20and%20Daniel%20Finchelstein%20and%20Ravin%20Kumar%20and%20Andre%20Elisseeff%20and%20Jin%20Huang%20and%20Ming%20Zhang%20and%20Ricardo%20Aguilar%20and%20Mai%20Gim%C3%A9nez%20and%20Jiawei%20Xia%20and%20Olivier%20Dousse%20and%20Willi%20Gierke%20and%20Damion%20Yates%20and%20Komal%20Jalan%20and%20Lu%20Li%20and%20Eri%20Latorre-Chimoto%20and%20Duc%20Dung%20Nguyen%20and%20Ken%20Durden%20and%20Praveen%20Kallakuri%20and%20Yaxin%20Liu%20and%20Matthew%20Johnson%20and%20Tomy%20Tsai%20and%20Alice%20Talbert%20and%20Jasmine%20Liu%20and%20Alexander%20Neitz%20and%20Chen%20Elkind%20and%20Marco%20Selvi%20and%20Mimi%20Jasarevic%20and%20Livio%20Baldini%20Soares%20and%20Albert%20Cui%20and%20Pidong%20Wang%20and%20Alek%20Wenjiao%20Wang%20and%20Xinyu%20Ye%20and%20Krystal%20Kallarackal%20and%20Lucia%20Loher%20and%20Hoi%20Lam%20and%20Josef%20Broder%20and%20Dan%20Holtmann-Rice%20and%20Nina%20Martin%20and%20Bramandia%20Ramadhana%20and%20Mrinal%20Shukla%20and%20Sujoy%20Basu%20and%20Abhi%20Mohan%20and%20Nick%20Fernando%20and%20Noah%20Fiedel%20and%20Kim%20Paterson%20and%20Hui%20Li%20and%20Ankush%20Garg%20and%20Jane%20Park%20and%20DongHyun%20Choi%20and%20Diane%20Wu%20and%20Sankalp%20Singh%20and%20Zhishuai%20Zhang%20and%20Amir%20Globerson%20and%20Lily%20Yu%20and%20John%20Carpenter%20and%20F%C3%A9lix%20de%20Chaumont%20Quitry%20and%20Carey%20Radebaugh%20and%20Chu-Cheng%20Lin%20and%20Alex%20Tudor%20and%20Prakash%20Shroff%20and%20Drew%20Garmon%20and%20Dayou%20Du%20and%20Neera%20Vats%20and%20Han%20Lu%20and%20Shariq%20Iqbal%20and%20Alex%20Yakubovich%20and%20Nilesh%20Tripuraneni%20and%20James%20Manyika%20and%20Haroon%20Qureshi%20and%20Nan%20Hua%20and%20Christel%20Ngani%20and%20Maria%20Abi%20Raad%20and%20Hannah%20Forbes%20and%20Jeff%20Stanway%20and%20Mukund%20Sundararajan%20and%20Victor%20Ungureanu%20and%20Colton%20Bishop%20and%20Yunjie%20Li%20and%20Balaji%20Venkatraman%20and%20Bo%20Li%20and%20Chloe%20Thornton%20and%20Salvatore%20Scellato%20and%20Nishesh%20Gupta%20and%20Yicheng%20Wang%20and%20Ian%20Tenney%20and%20Xihui%20Wu%20and%20Ashish%20Shenoy%20and%20Gabriel%20Carvajal%20and%20Diana%20Gage%20Wright%20and%20Ben%20Bariach%20and%20Zhuyun%20Xiao%20and%20Peter%20Hawkins%20and%20Sid%20Dalmia%20and%20Clement%20Farabet%20and%20Pedro%20Valenzuela%20and%20Quan%20Yuan%20and%20Ananth%20Agarwal%20and%20Mia%20Chen%20and%20Wooyeol%20Kim%20and%20Brice%20Hulse%20and%20Nandita%20Dukkipati%20and%20Adam%20Paszke%20and%20Andrew%20Bolt%20and%20Kiam%20Choo%20and%20Jennifer%20Beattie%20and%20Jennifer%20Prendki%20and%20Harsha%20Vashisht%20and%20Rebeca%20Santamaria-Fernandez%20and%20Luis%20C.%20Cobo%20and%20Jarek%20Wilkiewicz%20and%20David%20Madras%20and%20Ali%20Elqursh%20and%20Grant%20Uy%20and%20Kevin%20Ramirez%20and%20Matt%20Harvey%20and%20Tyler%20Liechty%20and%20Heiga%20Zen%20and%20Jeff%20Seibert%20and%20Clara%20Huiyi%20Hu%20and%20Andrey%20Khorlin%20and%20Maigo%20Le%20and%20Asaf%20Aharoni%20and%20Megan%20Li%20and%20Lily%20Wang%20and%20Sandeep%20Kumar%20and%20Norman%20Casagrande%20and%20Jay%20Hoover%20and%20Dalia%20El%20Badawy%20and%20David%20Soergel%20and%20Denis%20Vnukov%20and%20Matt%20Miecnikowski%20and%20Jiri%20Simsa%20and%20Praveen%20Kumar%20and%20Thibault%20Sellam%20and%20Daniel%20Vlasic%20and%20Samira%20Daruki%20and%20Nir%20Shabat%20and%20John%20Zhang%20and%20Guolong%20Su%20and%20Jiageng%20Zhang%20and%20Jeremiah%20Liu%20and%20Yi%20Sun%20and%20Evan%20Palmer%20and%20Alireza%20Ghaffarkhah%20and%20Xi%20Xiong%20and%20Victor%20Cotruta%20and%20Michael%20Fink%20and%20Lucas%20Dixon%20and%20Ashwin%20Sreevatsa%20and%20Adrian%20Goedeckemeyer%20and%20Alek%20Dimitriev%20and%20Mohsen%20Jafari%20and%20Remi%20Crocker%20and%20Nicholas%20FitzGerald%20and%20Aviral%20Kumar%20and%20Sanjay%20Ghemawat%20and%20Ivan%20Philips%20and%20Frederick%20Liu%20and%20Yannie%20Liang%20and%20Rachel%20Sterneck%20and%20Alena%20Repina%20and%20Marcus%20Wu%20and%20Laura%20Knight%20and%20Marin%20Georgiev%20and%20Hyo%20Lee%20and%20Harry%20Askham%20and%20Abhishek%20Chakladar%20and%20Annie%20Louis%20and%20Carl%20Crous%20and%20Hardie%20Cate%20and%20Dessie%20Petrova%20and%20Michael%20Quinn%20and%20Denese%20Owusu-Afriyie%20and%20Achintya%20Singhal%20and%20Nan%20Wei%20and%20Solomon%20Kim%20and%20Damien%20Vincent%20and%20Milad%20Nasr%20and%20Christopher%20A.%20Choquette-Choo%20and%20Reiko%20Tojo%20and%20Shawn%20Lu%20and%20Diego%20de%20Las%20Casas%20and%20Yuchung%20Cheng%20and%20Tolga%20Bolukbasi%20and%20Katherine%20Lee%20and%20Saaber%20Fatehi%20and%20Rajagopal%20Ananthanarayanan%20and%20Miteyan%20Patel%20and%20Charbel%20Kaed%20and%20Jing%20Li%20and%20Shreyas%20Rammohan%20Belle%20and%20Zhe%20Chen%20and%20Jaclyn%20Konzelmann%20and%20Siim%20P%C3%B5der%20and%20Roopal%20Garg%20and%20Vinod%20Koverkathu%20and%20Adam%20Brown%20and%20Chris%20Dyer%20and%20Rosanne%20Liu%20and%20Azade%20Nova%20and%20Jun%20Xu%20and%20Alanna%20Walton%20and%20Alicia%20Parrish%20and%20Mark%20Epstein%20and%20Sara%20McCarthy%20and%20Slav%20Petrov%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Jeffrey%20Dean%20and%20Oriol%20Vinyals%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20present%20the%20latest%20model%20of%20the%20Gemini%20family%2C%20Gemini%201.5%0APro%2C%20a%20highly%20compute-efficient%20multimodal%20mixture-of-experts%20model%20capable%20of%0Arecalling%20and%20reasoning%20over%20fine-grained%20information%20from%20millions%20of%20tokens%0Aof%20context%2C%20including%20multiple%20long%20documents%20and%20hours%20of%20video%20and%20audio.%0AGemini%201.5%20Pro%20achieves%20near-perfect%20recall%20on%20long-context%20retrieval%20tasks%0Aacross%20modalities%2C%20improves%20the%20state-of-the-art%20in%20long-document%20QA%2C%0Along-video%20QA%20and%20long-context%20ASR%2C%20and%20matches%20or%20surpasses%20Gemini%201.0%20Ultra%27s%0Astate-of-the-art%20performance%20across%20a%20broad%20set%20of%20benchmarks.%20Studying%20the%0Alimits%20of%20Gemini%201.5%20Pro%27s%20long-context%20ability%2C%20we%20find%20continued%20improvement%0Ain%20next-token%20prediction%20and%20near-perfect%20retrieval%20%28%3E99%25%29%20up%20to%20at%20least%2010M%0Atokens%2C%20a%20generational%20leap%20over%20existing%20models%20such%20as%20Claude%202.1%20%28200k%29%20and%0AGPT-4%20Turbo%20%28128k%29.%20Finally%2C%20we%20highlight%20surprising%20new%20capabilities%20of%20large%0Alanguage%20models%20at%20the%20frontier%3B%20when%20given%20a%20grammar%20manual%20for%20Kalamang%2C%20a%0Alanguage%20with%20fewer%20than%20200%20speakers%20worldwide%2C%20the%20model%20learns%20to%20translate%0AEnglish%20to%20Kalamang%20at%20a%20similar%20level%20to%20a%20person%20who%20learned%20from%20the%20same%0Acontent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05530v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context&entry.906535625=%20Gemini%20Team%20and%20Machel%20Reid%20and%20Nikolay%20Savinov%20and%20Denis%20Teplyashin%20and%20%20Dmitry%20and%20%20Lepikhin%20and%20Timothy%20Lillicrap%20and%20Jean-baptiste%20Alayrac%20and%20Radu%20Soricut%20and%20Angeliki%20Lazaridou%20and%20Orhan%20Firat%20and%20Julian%20Schrittwieser%20and%20Ioannis%20Antonoglou%20and%20Rohan%20Anil%20and%20Sebastian%20Borgeaud%20and%20Andrew%20Dai%20and%20Katie%20Millican%20and%20Ethan%20Dyer%20and%20Mia%20Glaese%20and%20Thibault%20Sottiaux%20and%20Benjamin%20Lee%20and%20Fabio%20Viola%20and%20Malcolm%20Reynolds%20and%20Yuanzhong%20Xu%20and%20James%20Molloy%20and%20Jilin%20Chen%20and%20Michael%20Isard%20and%20Paul%20Barham%20and%20Tom%20Hennigan%20and%20Ross%20McIlroy%20and%20Melvin%20Johnson%20and%20Johan%20Schalkwyk%20and%20Eli%20Collins%20and%20Eliza%20Rutherford%20and%20Erica%20Moreira%20and%20Kareem%20Ayoub%20and%20Megha%20Goel%20and%20Clemens%20Meyer%20and%20Gregory%20Thornton%20and%20Zhen%20Yang%20and%20Henryk%20Michalewski%20and%20Zaheer%20Abbas%20and%20Nathan%20Schucher%20and%20Ankesh%20Anand%20and%20Richard%20Ives%20and%20James%20Keeling%20and%20Karel%20Lenc%20and%20Salem%20Haykal%20and%20Siamak%20Shakeri%20and%20Pranav%20Shyam%20and%20Aakanksha%20Chowdhery%20and%20Roman%20Ring%20and%20Stephen%20Spencer%20and%20Eren%20Sezener%20and%20Luke%20Vilnis%20and%20Oscar%20Chang%20and%20Nobuyuki%20Morioka%20and%20George%20Tucker%20and%20Ce%20Zheng%20and%20Oliver%20Woodman%20and%20Nithya%20Attaluri%20and%20Tomas%20Kocisky%20and%20Evgenii%20Eltyshev%20and%20Xi%20Chen%20and%20Timothy%20Chung%20and%20Vittorio%20Selo%20and%20Siddhartha%20Brahma%20and%20Petko%20Georgiev%20and%20Ambrose%20Slone%20and%20Zhenkai%20Zhu%20and%20James%20Lottes%20and%20Siyuan%20Qiao%20and%20Ben%20Caine%20and%20Sebastian%20Riedel%20and%20Alex%20Tomala%20and%20Martin%20Chadwick%20and%20Juliette%20Love%20and%20Peter%20Choy%20and%20Sid%20Mittal%20and%20Neil%20Houlsby%20and%20Yunhao%20Tang%20and%20Matthew%20Lamm%20and%20Libin%20Bai%20and%20Qiao%20Zhang%20and%20Luheng%20He%20and%20Yong%20Cheng%20and%20Peter%20Humphreys%20and%20Yujia%20Li%20and%20Sergey%20Brin%20and%20Albin%20Cassirer%20and%20Yingjie%20Miao%20and%20Lukas%20Zilka%20and%20Taylor%20Tobin%20and%20Kelvin%20Xu%20and%20Lev%20Proleev%20and%20Daniel%20Sohn%20and%20Alberto%20Magni%20and%20Lisa%20Anne%20Hendricks%20and%20Isabel%20Gao%20and%20Santiago%20Ontanon%20and%20Oskar%20Bunyan%20and%20Nathan%20Byrd%20and%20Abhanshu%20Sharma%20and%20Biao%20Zhang%20and%20Mario%20Pinto%20and%20Rishika%20Sinha%20and%20Harsh%20Mehta%20and%20Dawei%20Jia%20and%20Sergi%20Caelles%20and%20Albert%20Webson%20and%20Alex%20Morris%20and%20Becca%20Roelofs%20and%20Yifan%20Ding%20and%20Robin%20Strudel%20and%20Xuehan%20Xiong%20and%20Marvin%20Ritter%20and%20Mostafa%20Dehghani%20and%20Rahma%20Chaabouni%20and%20Abhijit%20Karmarkar%20and%20Guangda%20Lai%20and%20Fabian%20Mentzer%20and%20Bibo%20Xu%20and%20YaGuang%20Li%20and%20Yujing%20Zhang%20and%20Tom%20Le%20Paine%20and%20Alex%20Goldin%20and%20Behnam%20Neyshabur%20and%20Kate%20Baumli%20and%20Anselm%20Levskaya%20and%20Michael%20Laskin%20and%20Wenhao%20Jia%20and%20Jack%20W.%20Rae%20and%20Kefan%20Xiao%20and%20Antoine%20He%20and%20Skye%20Giordano%20and%20Lakshman%20Yagati%20and%20Jean-Baptiste%20Lespiau%20and%20Paul%20Natsev%20and%20Sanjay%20Ganapathy%20and%20Fangyu%20Liu%20and%20Danilo%20Martins%20and%20Nanxin%20Chen%20and%20Yunhan%20Xu%20and%20Megan%20Barnes%20and%20Rhys%20May%20and%20Arpi%20Vezer%20and%20Junhyuk%20Oh%20and%20Ken%20Franko%20and%20Sophie%20Bridgers%20and%20Ruizhe%20Zhao%20and%20Boxi%20Wu%20and%20Basil%20Mustafa%20and%20Sean%20Sechrist%20and%20Emilio%20Parisotto%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Chris%20Larkin%20and%20Chenjie%20Gu%20and%20Christina%20Sorokin%20and%20Maxim%20Krikun%20and%20Alexey%20Guseynov%20and%20Jessica%20Landon%20and%20Romina%20Datta%20and%20Alexander%20Pritzel%20and%20Phoebe%20Thacker%20and%20Fan%20Yang%20and%20Kevin%20Hui%20and%20Anja%20Hauth%20and%20Chih-Kuan%20Yeh%20and%20David%20Barker%20and%20Justin%20Mao-Jones%20and%20Sophia%20Austin%20and%20Hannah%20Sheahan%20and%20Parker%20Schuh%20and%20James%20Svensson%20and%20Rohan%20Jain%20and%20Vinay%20Ramasesh%20and%20Anton%20Briukhov%20and%20Da-Woon%20Chung%20and%20Tamara%20von%20Glehn%20and%20Christina%20Butterfield%20and%20Priya%20Jhakra%20and%20Matthew%20Wiethoff%20and%20Justin%20Frye%20and%20Jordan%20Grimstad%20and%20Beer%20Changpinyo%20and%20Charline%20Le%20Lan%20and%20Anna%20Bortsova%20and%20Yonghui%20Wu%20and%20Paul%20Voigtlaender%20and%20Tara%20Sainath%20and%20Shane%20Gu%20and%20Charlotte%20Smith%20and%20Will%20Hawkins%20and%20Kris%20Cao%20and%20James%20Besley%20and%20Srivatsan%20Srinivasan%20and%20Mark%20Omernick%20and%20Colin%20Gaffney%20and%20Gabriela%20Surita%20and%20Ryan%20Burnell%20and%20Bogdan%20Damoc%20and%20Junwhan%20Ahn%20and%20Andrew%20Brock%20and%20Mantas%20Pajarskas%20and%20Anastasia%20Petrushkina%20and%20Seb%20Noury%20and%20Lorenzo%20Blanco%20and%20Kevin%20Swersky%20and%20Arun%20Ahuja%20and%20Thi%20Avrahami%20and%20Vedant%20Misra%20and%20Raoul%20de%20Liedekerke%20and%20Mariko%20Iinuma%20and%20Alex%20Polozov%20and%20Sarah%20York%20and%20George%20van%20den%20Driessche%20and%20Paul%20Michel%20and%20Justin%20Chiu%20and%20Rory%20Blevins%20and%20Zach%20Gleicher%20and%20Adri%C3%A0%20Recasens%20and%20Alban%20Rrustemi%20and%20Elena%20Gribovskaya%20and%20Aurko%20Roy%20and%20Wiktor%20Gworek%20and%20S%C3%A9bastien%20M.%20R.%20Arnold%20and%20Lisa%20Lee%20and%20James%20Lee-Thorp%20and%20Marcello%20Maggioni%20and%20Enrique%20Piqueras%20and%20Kartikeya%20Badola%20and%20Sharad%20Vikram%20and%20Lucas%20Gonzalez%20and%20Anirudh%20Baddepudi%20and%20Evan%20Senter%20and%20Jacob%20Devlin%20and%20James%20Qin%20and%20Michael%20Azzam%20and%20Maja%20Trebacz%20and%20Martin%20Polacek%20and%20Kashyap%20Krishnakumar%20and%20Shuo-yiin%20Chang%20and%20Matthew%20Tung%20and%20Ivo%20Penchev%20and%20Rishabh%20Joshi%20and%20Kate%20Olszewska%20and%20Carrie%20Muir%20and%20Mateo%20Wirth%20and%20Ale%20Jakse%20Hartman%20and%20Josh%20Newlan%20and%20Sheleem%20Kashem%20and%20Vijay%20Bolina%20and%20Elahe%20Dabir%20and%20Joost%20van%20Amersfoort%20and%20Zafarali%20Ahmed%20and%20James%20Cobon-Kerr%20and%20Aishwarya%20Kamath%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Le%20Hou%20and%20Ian%20Mackinnon%20and%20Alexandre%20Frechette%20and%20Eric%20Noland%20and%20Xiance%20Si%20and%20Emanuel%20Taropa%20and%20Dong%20Li%20and%20Phil%20Crone%20and%20Anmol%20Gulati%20and%20S%C3%A9bastien%20Cevey%20and%20Jonas%20Adler%20and%20Ada%20Ma%20and%20David%20Silver%20and%20Simon%20Tokumine%20and%20Richard%20Powell%20and%20Stephan%20Lee%20and%20Kiran%20Vodrahalli%20and%20Samer%20Hassan%20and%20Diana%20Mincu%20and%20Antoine%20Yang%20and%20Nir%20Levine%20and%20Jenny%20Brennan%20and%20Mingqiu%20Wang%20and%20Sarah%20Hodkinson%20and%20Jeffrey%20Zhao%20and%20Josh%20Lipschultz%20and%20Aedan%20Pope%20and%20Michael%20B.%20Chang%20and%20Cheng%20Li%20and%20Laurent%20El%20Shafey%20and%20Michela%20Paganini%20and%20Sholto%20Douglas%20and%20Bernd%20Bohnet%20and%20Fabio%20Pardo%20and%20Seth%20Odoom%20and%20Mihaela%20Rosca%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Kedar%20Soparkar%20and%20Arthur%20Guez%20and%20Tom%20Hudson%20and%20Steven%20Hansen%20and%20Chulayuth%20Asawaroengchai%20and%20Ravi%20Addanki%20and%20Tianhe%20Yu%20and%20Wojciech%20Stokowiec%20and%20Mina%20Khan%20and%20Justin%20Gilmer%20and%20Jaehoon%20Lee%20and%20Carrie%20Grimes%20Bostock%20and%20Keran%20Rong%20and%20Jonathan%20Caton%20and%20Pedram%20Pejman%20and%20Filip%20Pavetic%20and%20Geoff%20Brown%20and%20Vivek%20Sharma%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Rajkumar%20Samuel%20and%20Josip%20Djolonga%20and%20Amol%20Mandhane%20and%20Lars%20Lowe%20Sj%C3%B6sund%20and%20Elena%20Buchatskaya%20and%20Elspeth%20White%20and%20Natalie%20Clay%20and%20Jiepu%20Jiang%20and%20Hyeontaek%20Lim%20and%20Ross%20Hemsley%20and%20Zeyncep%20Cankara%20and%20Jane%20Labanowski%20and%20Nicola%20De%20Cao%20and%20David%20Steiner%20and%20Sayed%20Hadi%20Hashemi%20and%20Jacob%20Austin%20and%20Anita%20Gergely%20and%20Tim%20Blyth%20and%20Joe%20Stanton%20and%20Kaushik%20Shivakumar%20and%20Aditya%20Siddhant%20and%20Anders%20Andreassen%20and%20Carlos%20Araya%20and%20Nikhil%20Sethi%20and%20Rakesh%20Shivanna%20and%20Steven%20Hand%20and%20Ankur%20Bapna%20and%20Ali%20Khodaei%20and%20Antoine%20Miech%20and%20Garrett%20Tanzer%20and%20Andy%20Swing%20and%20Shantanu%20Thakoor%20and%20Lora%20Aroyo%20and%20Zhufeng%20Pan%20and%20Zachary%20Nado%20and%20Jakub%20Sygnowski%20and%20Stephanie%20Winkler%20and%20Dian%20Yu%20and%20Mohammad%20Saleh%20and%20Loren%20Maggiore%20and%20Yamini%20Bansal%20and%20Xavier%20Garcia%20and%20Mehran%20Kazemi%20and%20Piyush%20Patil%20and%20Ishita%20Dasgupta%20and%20Iain%20Barr%20and%20Minh%20Giang%20and%20Thais%20Kagohara%20and%20Ivo%20Danihelka%20and%20Amit%20Marathe%20and%20Vladimir%20Feinberg%20and%20Mohamed%20Elhawaty%20and%20Nimesh%20Ghelani%20and%20Dan%20Horgan%20and%20Helen%20Miller%20and%20Lexi%20Walker%20and%20Richard%20Tanburn%20and%20Mukarram%20Tariq%20and%20Disha%20Shrivastava%20and%20Fei%20Xia%20and%20Qingze%20Wang%20and%20Chung-Cheng%20Chiu%20and%20Zoe%20Ashwood%20and%20Khuslen%20Baatarsukh%20and%20Sina%20Samangooei%20and%20Rapha%C3%ABl%20Lopez%20Kaufman%20and%20Fred%20Alcober%20and%20Axel%20Stjerngren%20and%20Paul%20Komarek%20and%20Katerina%20Tsihlas%20and%20Anudhyan%20Boral%20and%20Ramona%20Comanescu%20and%20Jeremy%20Chen%20and%20Ruibo%20Liu%20and%20Chris%20Welty%20and%20Dawn%20Bloxwich%20and%20Charlie%20Chen%20and%20Yanhua%20Sun%20and%20Fangxiaoyu%20Feng%20and%20Matthew%20Mauger%20and%20Xerxes%20Dotiwalla%20and%20Vincent%20Hellendoorn%20and%20Michael%20Sharman%20and%20Ivy%20Zheng%20and%20Krishna%20Haridasan%20and%20Gabe%20Barth-Maron%20and%20Craig%20Swanson%20and%20Dominika%20Rogozi%C5%84ska%20and%20Alek%20Andreev%20and%20Paul%20Kishan%20Rubenstein%20and%20Ruoxin%20Sang%20and%20Dan%20Hurt%20and%20Gamaleldin%20Elsayed%20and%20Renshen%20Wang%20and%20Dave%20Lacey%20and%20Anastasija%20Ili%C4%87%20and%20Yao%20Zhao%20and%20Adam%20Iwanicki%20and%20Alejandro%20Lince%20and%20Alexander%20Chen%20and%20Christina%20Lyu%20and%20Carl%20Lebsack%20and%20Jordan%20Griffith%20and%20Meenu%20Gaba%20and%20Paramjit%20Sandhu%20and%20Phil%20Chen%20and%20Anna%20Koop%20and%20Ravi%20Rajwar%20and%20Soheil%20Hassas%20Yeganeh%20and%20Solomon%20Chang%20and%20Rui%20Zhu%20and%20Soroush%20Radpour%20and%20Elnaz%20Davoodi%20and%20Ving%20Ian%20Lei%20and%20Yang%20Xu%20and%20Daniel%20Toyama%20and%20Constant%20Segal%20and%20Martin%20Wicke%20and%20Hanzhao%20Lin%20and%20Anna%20Bulanova%20and%20Adri%C3%A0%20Puigdom%C3%A8nech%20Badia%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Pablo%20Sprechmann%20and%20Angelos%20Filos%20and%20Shaobo%20Hou%20and%20V%C3%ADctor%20Campos%20and%20Nora%20Kassner%20and%20Devendra%20Sachan%20and%20Meire%20Fortunato%20and%20Chimezie%20Iwuanyanwu%20and%20Vitaly%20Nikolaev%20and%20Balaji%20Lakshminarayanan%20and%20Sadegh%20Jazayeri%20and%20Mani%20Varadarajan%20and%20Chetan%20Tekur%20and%20Doug%20Fritz%20and%20Misha%20Khalman%20and%20David%20Reitter%20and%20Kingshuk%20Dasgupta%20and%20Shourya%20Sarcar%20and%20Tina%20Ornduff%20and%20Javier%20Snaider%20and%20Fantine%20Huot%20and%20Johnson%20Jia%20and%20Rupert%20Kemp%20and%20Nejc%20Trdin%20and%20Anitha%20Vijayakumar%20and%20Lucy%20Kim%20and%20Christof%20Angermueller%20and%20Li%20Lao%20and%20Tianqi%20Liu%20and%20Haibin%20Zhang%20and%20David%20Engel%20and%20Somer%20Greene%20and%20Ana%C3%AFs%20White%20and%20Jessica%20Austin%20and%20Lilly%20Taylor%20and%20Shereen%20Ashraf%20and%20Dangyi%20Liu%20and%20Maria%20Georgaki%20and%20Irene%20Cai%20and%20Yana%20Kulizhskaya%20and%20Sonam%20Goenka%20and%20Brennan%20Saeta%20and%20Ying%20Xu%20and%20Christian%20Frank%20and%20Dario%20de%20Cesare%20and%20Brona%20Robenek%20and%20Harry%20Richardson%20and%20Mahmoud%20Alnahlawi%20and%20Christopher%20Yew%20and%20Priya%20Ponnapalli%20and%20Marco%20Tagliasacchi%20and%20Alex%20Korchemniy%20and%20Yelin%20Kim%20and%20Dinghua%20Li%20and%20Bill%20Rosgen%20and%20Kyle%20Levin%20and%20Jeremy%20Wiesner%20and%20Praseem%20Banzal%20and%20Praveen%20Srinivasan%20and%20Hongkun%20Yu%20and%20%C3%87a%C4%9Flar%20%C3%9Cnl%C3%BC%20and%20David%20Reid%20and%20Zora%20Tung%20and%20Daniel%20Finchelstein%20and%20Ravin%20Kumar%20and%20Andre%20Elisseeff%20and%20Jin%20Huang%20and%20Ming%20Zhang%20and%20Ricardo%20Aguilar%20and%20Mai%20Gim%C3%A9nez%20and%20Jiawei%20Xia%20and%20Olivier%20Dousse%20and%20Willi%20Gierke%20and%20Damion%20Yates%20and%20Komal%20Jalan%20and%20Lu%20Li%20and%20Eri%20Latorre-Chimoto%20and%20Duc%20Dung%20Nguyen%20and%20Ken%20Durden%20and%20Praveen%20Kallakuri%20and%20Yaxin%20Liu%20and%20Matthew%20Johnson%20and%20Tomy%20Tsai%20and%20Alice%20Talbert%20and%20Jasmine%20Liu%20and%20Alexander%20Neitz%20and%20Chen%20Elkind%20and%20Marco%20Selvi%20and%20Mimi%20Jasarevic%20and%20Livio%20Baldini%20Soares%20and%20Albert%20Cui%20and%20Pidong%20Wang%20and%20Alek%20Wenjiao%20Wang%20and%20Xinyu%20Ye%20and%20Krystal%20Kallarackal%20and%20Lucia%20Loher%20and%20Hoi%20Lam%20and%20Josef%20Broder%20and%20Dan%20Holtmann-Rice%20and%20Nina%20Martin%20and%20Bramandia%20Ramadhana%20and%20Mrinal%20Shukla%20and%20Sujoy%20Basu%20and%20Abhi%20Mohan%20and%20Nick%20Fernando%20and%20Noah%20Fiedel%20and%20Kim%20Paterson%20and%20Hui%20Li%20and%20Ankush%20Garg%20and%20Jane%20Park%20and%20DongHyun%20Choi%20and%20Diane%20Wu%20and%20Sankalp%20Singh%20and%20Zhishuai%20Zhang%20and%20Amir%20Globerson%20and%20Lily%20Yu%20and%20John%20Carpenter%20and%20F%C3%A9lix%20de%20Chaumont%20Quitry%20and%20Carey%20Radebaugh%20and%20Chu-Cheng%20Lin%20and%20Alex%20Tudor%20and%20Prakash%20Shroff%20and%20Drew%20Garmon%20and%20Dayou%20Du%20and%20Neera%20Vats%20and%20Han%20Lu%20and%20Shariq%20Iqbal%20and%20Alex%20Yakubovich%20and%20Nilesh%20Tripuraneni%20and%20James%20Manyika%20and%20Haroon%20Qureshi%20and%20Nan%20Hua%20and%20Christel%20Ngani%20and%20Maria%20Abi%20Raad%20and%20Hannah%20Forbes%20and%20Jeff%20Stanway%20and%20Mukund%20Sundararajan%20and%20Victor%20Ungureanu%20and%20Colton%20Bishop%20and%20Yunjie%20Li%20and%20Balaji%20Venkatraman%20and%20Bo%20Li%20and%20Chloe%20Thornton%20and%20Salvatore%20Scellato%20and%20Nishesh%20Gupta%20and%20Yicheng%20Wang%20and%20Ian%20Tenney%20and%20Xihui%20Wu%20and%20Ashish%20Shenoy%20and%20Gabriel%20Carvajal%20and%20Diana%20Gage%20Wright%20and%20Ben%20Bariach%20and%20Zhuyun%20Xiao%20and%20Peter%20Hawkins%20and%20Sid%20Dalmia%20and%20Clement%20Farabet%20and%20Pedro%20Valenzuela%20and%20Quan%20Yuan%20and%20Ananth%20Agarwal%20and%20Mia%20Chen%20and%20Wooyeol%20Kim%20and%20Brice%20Hulse%20and%20Nandita%20Dukkipati%20and%20Adam%20Paszke%20and%20Andrew%20Bolt%20and%20Kiam%20Choo%20and%20Jennifer%20Beattie%20and%20Jennifer%20Prendki%20and%20Harsha%20Vashisht%20and%20Rebeca%20Santamaria-Fernandez%20and%20Luis%20C.%20Cobo%20and%20Jarek%20Wilkiewicz%20and%20David%20Madras%20and%20Ali%20Elqursh%20and%20Grant%20Uy%20and%20Kevin%20Ramirez%20and%20Matt%20Harvey%20and%20Tyler%20Liechty%20and%20Heiga%20Zen%20and%20Jeff%20Seibert%20and%20Clara%20Huiyi%20Hu%20and%20Andrey%20Khorlin%20and%20Maigo%20Le%20and%20Asaf%20Aharoni%20and%20Megan%20Li%20and%20Lily%20Wang%20and%20Sandeep%20Kumar%20and%20Norman%20Casagrande%20and%20Jay%20Hoover%20and%20Dalia%20El%20Badawy%20and%20David%20Soergel%20and%20Denis%20Vnukov%20and%20Matt%20Miecnikowski%20and%20Jiri%20Simsa%20and%20Praveen%20Kumar%20and%20Thibault%20Sellam%20and%20Daniel%20Vlasic%20and%20Samira%20Daruki%20and%20Nir%20Shabat%20and%20John%20Zhang%20and%20Guolong%20Su%20and%20Jiageng%20Zhang%20and%20Jeremiah%20Liu%20and%20Yi%20Sun%20and%20Evan%20Palmer%20and%20Alireza%20Ghaffarkhah%20and%20Xi%20Xiong%20and%20Victor%20Cotruta%20and%20Michael%20Fink%20and%20Lucas%20Dixon%20and%20Ashwin%20Sreevatsa%20and%20Adrian%20Goedeckemeyer%20and%20Alek%20Dimitriev%20and%20Mohsen%20Jafari%20and%20Remi%20Crocker%20and%20Nicholas%20FitzGerald%20and%20Aviral%20Kumar%20and%20Sanjay%20Ghemawat%20and%20Ivan%20Philips%20and%20Frederick%20Liu%20and%20Yannie%20Liang%20and%20Rachel%20Sterneck%20and%20Alena%20Repina%20and%20Marcus%20Wu%20and%20Laura%20Knight%20and%20Marin%20Georgiev%20and%20Hyo%20Lee%20and%20Harry%20Askham%20and%20Abhishek%20Chakladar%20and%20Annie%20Louis%20and%20Carl%20Crous%20and%20Hardie%20Cate%20and%20Dessie%20Petrova%20and%20Michael%20Quinn%20and%20Denese%20Owusu-Afriyie%20and%20Achintya%20Singhal%20and%20Nan%20Wei%20and%20Solomon%20Kim%20and%20Damien%20Vincent%20and%20Milad%20Nasr%20and%20Christopher%20A.%20Choquette-Choo%20and%20Reiko%20Tojo%20and%20Shawn%20Lu%20and%20Diego%20de%20Las%20Casas%20and%20Yuchung%20Cheng%20and%20Tolga%20Bolukbasi%20and%20Katherine%20Lee%20and%20Saaber%20Fatehi%20and%20Rajagopal%20Ananthanarayanan%20and%20Miteyan%20Patel%20and%20Charbel%20Kaed%20and%20Jing%20Li%20and%20Shreyas%20Rammohan%20Belle%20and%20Zhe%20Chen%20and%20Jaclyn%20Konzelmann%20and%20Siim%20P%C3%B5der%20and%20Roopal%20Garg%20and%20Vinod%20Koverkathu%20and%20Adam%20Brown%20and%20Chris%20Dyer%20and%20Rosanne%20Liu%20and%20Azade%20Nova%20and%20Jun%20Xu%20and%20Alanna%20Walton%20and%20Alicia%20Parrish%20and%20Mark%20Epstein%20and%20Sara%20McCarthy%20and%20Slav%20Petrov%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Jeffrey%20Dean%20and%20Oriol%20Vinyals&entry.1292438233=%20%20In%20this%20report%2C%20we%20present%20the%20latest%20model%20of%20the%20Gemini%20family%2C%20Gemini%201.5%0APro%2C%20a%20highly%20compute-efficient%20multimodal%20mixture-of-experts%20model%20capable%20of%0Arecalling%20and%20reasoning%20over%20fine-grained%20information%20from%20millions%20of%20tokens%0Aof%20context%2C%20including%20multiple%20long%20documents%20and%20hours%20of%20video%20and%20audio.%0AGemini%201.5%20Pro%20achieves%20near-perfect%20recall%20on%20long-context%20retrieval%20tasks%0Aacross%20modalities%2C%20improves%20the%20state-of-the-art%20in%20long-document%20QA%2C%0Along-video%20QA%20and%20long-context%20ASR%2C%20and%20matches%20or%20surpasses%20Gemini%201.0%20Ultra%27s%0Astate-of-the-art%20performance%20across%20a%20broad%20set%20of%20benchmarks.%20Studying%20the%0Alimits%20of%20Gemini%201.5%20Pro%27s%20long-context%20ability%2C%20we%20find%20continued%20improvement%0Ain%20next-token%20prediction%20and%20near-perfect%20retrieval%20%28%3E99%25%29%20up%20to%20at%20least%2010M%0Atokens%2C%20a%20generational%20leap%20over%20existing%20models%20such%20as%20Claude%202.1%20%28200k%29%20and%0AGPT-4%20Turbo%20%28128k%29.%20Finally%2C%20we%20highlight%20surprising%20new%20capabilities%20of%20large%0Alanguage%20models%20at%20the%20frontier%3B%20when%20given%20a%20grammar%20manual%20for%20Kalamang%2C%20a%0Alanguage%20with%20fewer%20than%20200%20speakers%20worldwide%2C%20the%20model%20learns%20to%20translate%0AEnglish%20to%20Kalamang%20at%20a%20similar%20level%20to%20a%20person%20who%20learned%20from%20the%20same%0Acontent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05530v2&entry.124074799=Read"},
{"title": "360SFUDA++: Towards Source-free UDA for Panoramic Segmentation by\n  Learning Reliable Category Prototypes", "author": "Xu Zheng and Pengyuan Zhou and Athanasios V. Vasilakos and Lin Wang", "abstract": "  In this paper, we address the challenging source-free unsupervised domain\nadaptation (SFUDA) for pinhole-to-panoramic semantic segmentation, given only a\npinhole image pre-trained model (i.e., source) and unlabeled panoramic images\n(i.e., target). Tackling this problem is non-trivial due to three critical\nchallenges: 1) semantic mismatches from the distinct Field-of-View (FoV)\nbetween domains, 2) style discrepancies inherent in the UDA problem, and 3)\ninevitable distortion of the panoramic images. To tackle these problems, we\npropose 360SFUDA++ that effectively extracts knowledge from the source pinhole\nmodel with only unlabeled panoramic images and transfers the reliable knowledge\nto the target panoramic domain. Specifically, we first utilize Tangent\nProjection (TP) as it has less distortion and meanwhile slits the\nequirectangular projection (ERP) to patches with fixed FoV projection (FFP) to\nmimic the pinhole images. Both projections are shown effective in extracting\nknowledge from the source model. However, as the distinct projections make it\nless possible to directly transfer knowledge between domains, we then propose\nReliable Panoramic Prototype Adaptation Module (RP2AM) to transfer knowledge at\nboth prediction and prototype levels. RP$^2$AM selects the confident knowledge\nand integrates panoramic prototypes for reliable knowledge adaptation.\nMoreover, we introduce Cross-projection Dual Attention Module (CDAM), which\nbetter aligns the spatial and channel characteristics across projections at the\nfeature level between domains. Both knowledge extraction and transfer processes\nare synchronously updated to reach the best performance. Extensive experiments\non the synthetic and real-world benchmarks, including outdoor and indoor\nscenarios, demonstrate that our 360SFUDA++ achieves significantly better\nperformance than prior SFUDA methods.\n", "link": "http://arxiv.org/abs/2404.16501v1", "date": "2024-04-25", "relevancy": 2.396, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6231}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5819}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5815}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20360SFUDA%2B%2B%3A%20Towards%20Source-free%20UDA%20for%20Panoramic%20Segmentation%20by%0A%20%20Learning%20Reliable%20Category%20Prototypes&body=Title%3A%20360SFUDA%2B%2B%3A%20Towards%20Source-free%20UDA%20for%20Panoramic%20Segmentation%20by%0A%20%20Learning%20Reliable%20Category%20Prototypes%0AAuthor%3A%20Xu%20Zheng%20and%20Pengyuan%20Zhou%20and%20Athanasios%20V.%20Vasilakos%20and%20Lin%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenging%20source-free%20unsupervised%20domain%0Aadaptation%20%28SFUDA%29%20for%20pinhole-to-panoramic%20semantic%20segmentation%2C%20given%20only%20a%0Apinhole%20image%20pre-trained%20model%20%28i.e.%2C%20source%29%20and%20unlabeled%20panoramic%20images%0A%28i.e.%2C%20target%29.%20Tackling%20this%20problem%20is%20non-trivial%20due%20to%20three%20critical%0Achallenges%3A%201%29%20semantic%20mismatches%20from%20the%20distinct%20Field-of-View%20%28FoV%29%0Abetween%20domains%2C%202%29%20style%20discrepancies%20inherent%20in%20the%20UDA%20problem%2C%20and%203%29%0Ainevitable%20distortion%20of%20the%20panoramic%20images.%20To%20tackle%20these%20problems%2C%20we%0Apropose%20360SFUDA%2B%2B%20that%20effectively%20extracts%20knowledge%20from%20the%20source%20pinhole%0Amodel%20with%20only%20unlabeled%20panoramic%20images%20and%20transfers%20the%20reliable%20knowledge%0Ato%20the%20target%20panoramic%20domain.%20Specifically%2C%20we%20first%20utilize%20Tangent%0AProjection%20%28TP%29%20as%20it%20has%20less%20distortion%20and%20meanwhile%20slits%20the%0Aequirectangular%20projection%20%28ERP%29%20to%20patches%20with%20fixed%20FoV%20projection%20%28FFP%29%20to%0Amimic%20the%20pinhole%20images.%20Both%20projections%20are%20shown%20effective%20in%20extracting%0Aknowledge%20from%20the%20source%20model.%20However%2C%20as%20the%20distinct%20projections%20make%20it%0Aless%20possible%20to%20directly%20transfer%20knowledge%20between%20domains%2C%20we%20then%20propose%0AReliable%20Panoramic%20Prototype%20Adaptation%20Module%20%28RP2AM%29%20to%20transfer%20knowledge%20at%0Aboth%20prediction%20and%20prototype%20levels.%20RP%24%5E2%24AM%20selects%20the%20confident%20knowledge%0Aand%20integrates%20panoramic%20prototypes%20for%20reliable%20knowledge%20adaptation.%0AMoreover%2C%20we%20introduce%20Cross-projection%20Dual%20Attention%20Module%20%28CDAM%29%2C%20which%0Abetter%20aligns%20the%20spatial%20and%20channel%20characteristics%20across%20projections%20at%20the%0Afeature%20level%20between%20domains.%20Both%20knowledge%20extraction%20and%20transfer%20processes%0Aare%20synchronously%20updated%20to%20reach%20the%20best%20performance.%20Extensive%20experiments%0Aon%20the%20synthetic%20and%20real-world%20benchmarks%2C%20including%20outdoor%20and%20indoor%0Ascenarios%2C%20demonstrate%20that%20our%20360SFUDA%2B%2B%20achieves%20significantly%20better%0Aperformance%20than%20prior%20SFUDA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16501v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360SFUDA%2B%2B%3A%20Towards%20Source-free%20UDA%20for%20Panoramic%20Segmentation%20by%0A%20%20Learning%20Reliable%20Category%20Prototypes&entry.906535625=Xu%20Zheng%20and%20Pengyuan%20Zhou%20and%20Athanasios%20V.%20Vasilakos%20and%20Lin%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenging%20source-free%20unsupervised%20domain%0Aadaptation%20%28SFUDA%29%20for%20pinhole-to-panoramic%20semantic%20segmentation%2C%20given%20only%20a%0Apinhole%20image%20pre-trained%20model%20%28i.e.%2C%20source%29%20and%20unlabeled%20panoramic%20images%0A%28i.e.%2C%20target%29.%20Tackling%20this%20problem%20is%20non-trivial%20due%20to%20three%20critical%0Achallenges%3A%201%29%20semantic%20mismatches%20from%20the%20distinct%20Field-of-View%20%28FoV%29%0Abetween%20domains%2C%202%29%20style%20discrepancies%20inherent%20in%20the%20UDA%20problem%2C%20and%203%29%0Ainevitable%20distortion%20of%20the%20panoramic%20images.%20To%20tackle%20these%20problems%2C%20we%0Apropose%20360SFUDA%2B%2B%20that%20effectively%20extracts%20knowledge%20from%20the%20source%20pinhole%0Amodel%20with%20only%20unlabeled%20panoramic%20images%20and%20transfers%20the%20reliable%20knowledge%0Ato%20the%20target%20panoramic%20domain.%20Specifically%2C%20we%20first%20utilize%20Tangent%0AProjection%20%28TP%29%20as%20it%20has%20less%20distortion%20and%20meanwhile%20slits%20the%0Aequirectangular%20projection%20%28ERP%29%20to%20patches%20with%20fixed%20FoV%20projection%20%28FFP%29%20to%0Amimic%20the%20pinhole%20images.%20Both%20projections%20are%20shown%20effective%20in%20extracting%0Aknowledge%20from%20the%20source%20model.%20However%2C%20as%20the%20distinct%20projections%20make%20it%0Aless%20possible%20to%20directly%20transfer%20knowledge%20between%20domains%2C%20we%20then%20propose%0AReliable%20Panoramic%20Prototype%20Adaptation%20Module%20%28RP2AM%29%20to%20transfer%20knowledge%20at%0Aboth%20prediction%20and%20prototype%20levels.%20RP%24%5E2%24AM%20selects%20the%20confident%20knowledge%0Aand%20integrates%20panoramic%20prototypes%20for%20reliable%20knowledge%20adaptation.%0AMoreover%2C%20we%20introduce%20Cross-projection%20Dual%20Attention%20Module%20%28CDAM%29%2C%20which%0Abetter%20aligns%20the%20spatial%20and%20channel%20characteristics%20across%20projections%20at%20the%0Afeature%20level%20between%20domains.%20Both%20knowledge%20extraction%20and%20transfer%20processes%0Aare%20synchronously%20updated%20to%20reach%20the%20best%20performance.%20Extensive%20experiments%0Aon%20the%20synthetic%20and%20real-world%20benchmarks%2C%20including%20outdoor%20and%20indoor%0Ascenarios%2C%20demonstrate%20that%20our%20360SFUDA%2B%2B%20achieves%20significantly%20better%0Aperformance%20than%20prior%20SFUDA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16501v1&entry.124074799=Read"},
{"title": "Distilling Privileged Information for Dubins Traveling Salesman Problems\n  with Neighborhoods", "author": "Min Kyu Shin and Su-Jeong Park and Seung-Keol Ryu and Heeyeon Kim and Han-Lim Choi", "abstract": "  This paper presents a novel learning approach for Dubins Traveling Salesman\nProblems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of a\nnon-holonomic vehicle passing through neighborhoods of given task points. The\nmethod involves two learning phases: initially, a model-free reinforcement\nlearning approach leverages privileged information to distill knowledge from\nexpert trajectories generated by the LinKernighan heuristic (LKH) algorithm.\nSubsequently, a supervised learning phase trains an adaptation network to solve\nproblems independently of privileged information. Before the first learning\nphase, a parameter initialization technique using the demonstration data was\nalso devised to enhance training efficiency. The proposed learning method\nproduces a solution about 50 times faster than LKH and substantially\noutperforms other imitation learning and RL with demonstration schemes, most of\nwhich fail to sense all the task points.\n", "link": "http://arxiv.org/abs/2404.16721v1", "date": "2024-04-25", "relevancy": 2.3804, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4857}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4746}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.468}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Distilling%20Privileged%20Information%20for%20Dubins%20Traveling%20Salesman%20Problems%0A%20%20with%20Neighborhoods&body=Title%3A%20Distilling%20Privileged%20Information%20for%20Dubins%20Traveling%20Salesman%20Problems%0A%20%20with%20Neighborhoods%0AAuthor%3A%20Min%20Kyu%20Shin%20and%20Su-Jeong%20Park%20and%20Seung-Keol%20Ryu%20and%20Heeyeon%20Kim%20and%20Han-Lim%20Choi%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20learning%20approach%20for%20Dubins%20Traveling%20Salesman%0AProblems%28DTSP%29%20with%20Neighborhood%20%28DTSPN%29%20to%20quickly%20produce%20a%20tour%20of%20a%0Anon-holonomic%20vehicle%20passing%20through%20neighborhoods%20of%20given%20task%20points.%20The%0Amethod%20involves%20two%20learning%20phases%3A%20initially%2C%20a%20model-free%20reinforcement%0Alearning%20approach%20leverages%20privileged%20information%20to%20distill%20knowledge%20from%0Aexpert%20trajectories%20generated%20by%20the%20LinKernighan%20heuristic%20%28LKH%29%20algorithm.%0ASubsequently%2C%20a%20supervised%20learning%20phase%20trains%20an%20adaptation%20network%20to%20solve%0Aproblems%20independently%20of%20privileged%20information.%20Before%20the%20first%20learning%0Aphase%2C%20a%20parameter%20initialization%20technique%20using%20the%20demonstration%20data%20was%0Aalso%20devised%20to%20enhance%20training%20efficiency.%20The%20proposed%20learning%20method%0Aproduces%20a%20solution%20about%2050%20times%20faster%20than%20LKH%20and%20substantially%0Aoutperforms%20other%20imitation%20learning%20and%20RL%20with%20demonstration%20schemes%2C%20most%20of%0Awhich%20fail%20to%20sense%20all%20the%20task%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16721v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Privileged%20Information%20for%20Dubins%20Traveling%20Salesman%20Problems%0A%20%20with%20Neighborhoods&entry.906535625=Min%20Kyu%20Shin%20and%20Su-Jeong%20Park%20and%20Seung-Keol%20Ryu%20and%20Heeyeon%20Kim%20and%20Han-Lim%20Choi&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20learning%20approach%20for%20Dubins%20Traveling%20Salesman%0AProblems%28DTSP%29%20with%20Neighborhood%20%28DTSPN%29%20to%20quickly%20produce%20a%20tour%20of%20a%0Anon-holonomic%20vehicle%20passing%20through%20neighborhoods%20of%20given%20task%20points.%20The%0Amethod%20involves%20two%20learning%20phases%3A%20initially%2C%20a%20model-free%20reinforcement%0Alearning%20approach%20leverages%20privileged%20information%20to%20distill%20knowledge%20from%0Aexpert%20trajectories%20generated%20by%20the%20LinKernighan%20heuristic%20%28LKH%29%20algorithm.%0ASubsequently%2C%20a%20supervised%20learning%20phase%20trains%20an%20adaptation%20network%20to%20solve%0Aproblems%20independently%20of%20privileged%20information.%20Before%20the%20first%20learning%0Aphase%2C%20a%20parameter%20initialization%20technique%20using%20the%20demonstration%20data%20was%0Aalso%20devised%20to%20enhance%20training%20efficiency.%20The%20proposed%20learning%20method%0Aproduces%20a%20solution%20about%2050%20times%20faster%20than%20LKH%20and%20substantially%0Aoutperforms%20other%20imitation%20learning%20and%20RL%20with%20demonstration%20schemes%2C%20most%20of%0Awhich%20fail%20to%20sense%20all%20the%20task%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16721v1&entry.124074799=Read"},
{"title": "V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation\n  Localization and Copyright Protection", "author": "Xuanyu Zhang and Youmin Xu and Runyi Li and Jiwen Yu and Weiqi Li and Zhipei Xu and Jian Zhang", "abstract": "  AI-generated video has revolutionized short video production, filmmaking, and\npersonalized media, making video local editing an essential tool. However, this\nprogress also blurs the line between reality and fiction, posing challenges in\nmultimedia forensics. To solve this urgent issue, V2A-Mark is proposed to\naddress the limitations of current video tampering forensics, such as poor\ngeneralizability, singular function, and single modality focus. Combining the\nfragility of video-into-video steganography with deep robust watermarking, our\nmethod can embed invisible visual-audio localization watermarks and copyright\nwatermarks into the original video frames and audio, enabling precise\nmanipulation localization and copyright protection. We also design a temporal\nalignment and fusion module and degradation prompt learning to enhance the\nlocalization accuracy and decoding robustness. Meanwhile, we introduce a\nsample-level audio localization method and a cross-modal copyright extraction\nmechanism to couple the information of audio and video frames. The\neffectiveness of V2A-Mark has been verified on a visual-audio tampering\ndataset, emphasizing its superiority in localization precision and copyright\naccuracy, crucial for the sustainable development of video editing in the AIGC\nvideo era.\n", "link": "http://arxiv.org/abs/2404.16824v1", "date": "2024-04-25", "relevancy": 2.3784, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6165}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6066}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5679}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20V2A-Mark%3A%20Versatile%20Deep%20Visual-Audio%20Watermarking%20for%20Manipulation%0A%20%20Localization%20and%20Copyright%20Protection&body=Title%3A%20V2A-Mark%3A%20Versatile%20Deep%20Visual-Audio%20Watermarking%20for%20Manipulation%0A%20%20Localization%20and%20Copyright%20Protection%0AAuthor%3A%20Xuanyu%20Zhang%20and%20Youmin%20Xu%20and%20Runyi%20Li%20and%20Jiwen%20Yu%20and%20Weiqi%20Li%20and%20Zhipei%20Xu%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20AI-generated%20video%20has%20revolutionized%20short%20video%20production%2C%20filmmaking%2C%20and%0Apersonalized%20media%2C%20making%20video%20local%20editing%20an%20essential%20tool.%20However%2C%20this%0Aprogress%20also%20blurs%20the%20line%20between%20reality%20and%20fiction%2C%20posing%20challenges%20in%0Amultimedia%20forensics.%20To%20solve%20this%20urgent%20issue%2C%20V2A-Mark%20is%20proposed%20to%0Aaddress%20the%20limitations%20of%20current%20video%20tampering%20forensics%2C%20such%20as%20poor%0Ageneralizability%2C%20singular%20function%2C%20and%20single%20modality%20focus.%20Combining%20the%0Afragility%20of%20video-into-video%20steganography%20with%20deep%20robust%20watermarking%2C%20our%0Amethod%20can%20embed%20invisible%20visual-audio%20localization%20watermarks%20and%20copyright%0Awatermarks%20into%20the%20original%20video%20frames%20and%20audio%2C%20enabling%20precise%0Amanipulation%20localization%20and%20copyright%20protection.%20We%20also%20design%20a%20temporal%0Aalignment%20and%20fusion%20module%20and%20degradation%20prompt%20learning%20to%20enhance%20the%0Alocalization%20accuracy%20and%20decoding%20robustness.%20Meanwhile%2C%20we%20introduce%20a%0Asample-level%20audio%20localization%20method%20and%20a%20cross-modal%20copyright%20extraction%0Amechanism%20to%20couple%20the%20information%20of%20audio%20and%20video%20frames.%20The%0Aeffectiveness%20of%20V2A-Mark%20has%20been%20verified%20on%20a%20visual-audio%20tampering%0Adataset%2C%20emphasizing%20its%20superiority%20in%20localization%20precision%20and%20copyright%0Aaccuracy%2C%20crucial%20for%20the%20sustainable%20development%20of%20video%20editing%20in%20the%20AIGC%0Avideo%20era.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16824v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2A-Mark%3A%20Versatile%20Deep%20Visual-Audio%20Watermarking%20for%20Manipulation%0A%20%20Localization%20and%20Copyright%20Protection&entry.906535625=Xuanyu%20Zhang%20and%20Youmin%20Xu%20and%20Runyi%20Li%20and%20Jiwen%20Yu%20and%20Weiqi%20Li%20and%20Zhipei%20Xu%20and%20Jian%20Zhang&entry.1292438233=%20%20AI-generated%20video%20has%20revolutionized%20short%20video%20production%2C%20filmmaking%2C%20and%0Apersonalized%20media%2C%20making%20video%20local%20editing%20an%20essential%20tool.%20However%2C%20this%0Aprogress%20also%20blurs%20the%20line%20between%20reality%20and%20fiction%2C%20posing%20challenges%20in%0Amultimedia%20forensics.%20To%20solve%20this%20urgent%20issue%2C%20V2A-Mark%20is%20proposed%20to%0Aaddress%20the%20limitations%20of%20current%20video%20tampering%20forensics%2C%20such%20as%20poor%0Ageneralizability%2C%20singular%20function%2C%20and%20single%20modality%20focus.%20Combining%20the%0Afragility%20of%20video-into-video%20steganography%20with%20deep%20robust%20watermarking%2C%20our%0Amethod%20can%20embed%20invisible%20visual-audio%20localization%20watermarks%20and%20copyright%0Awatermarks%20into%20the%20original%20video%20frames%20and%20audio%2C%20enabling%20precise%0Amanipulation%20localization%20and%20copyright%20protection.%20We%20also%20design%20a%20temporal%0Aalignment%20and%20fusion%20module%20and%20degradation%20prompt%20learning%20to%20enhance%20the%0Alocalization%20accuracy%20and%20decoding%20robustness.%20Meanwhile%2C%20we%20introduce%20a%0Asample-level%20audio%20localization%20method%20and%20a%20cross-modal%20copyright%20extraction%0Amechanism%20to%20couple%20the%20information%20of%20audio%20and%20video%20frames.%20The%0Aeffectiveness%20of%20V2A-Mark%20has%20been%20verified%20on%20a%20visual-audio%20tampering%0Adataset%2C%20emphasizing%20its%20superiority%20in%20localization%20precision%20and%20copyright%0Aaccuracy%2C%20crucial%20for%20the%20sustainable%20development%20of%20video%20editing%20in%20the%20AIGC%0Avideo%20era.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16824v1&entry.124074799=Read"},
{"title": "CBRW: A Novel Approach for Cancelable Biometric Template Generation\n  based on", "author": "Nitin Kumar and  Manisha", "abstract": "  Cancelable Biometric is a challenging research field in which security of an\noriginal biometric image is ensured by transforming the original biometric into\nanother irreversible domain. Several approaches have been suggested in\nliterature for generating cancelable biometric templates. In this paper, two\nnovel and simple cancelable biometric template generation methods based on\nRandom Walk (CBRW) have been proposed. By employing random walk and other steps\ngiven in the proposed two algorithms viz. CBRW-BitXOR and CBRW-BitCMP, the\noriginal biometric is transformed into a cancellable template. The performance\nof the proposed methods is compared with other state-of-the-art methods.\nExperiments have been performed on eight publicly available gray and color\ndatasets i.e. CP (ear) (gray and color), UTIRIS (iris) (gray and color), ORL\n(face) (gray), IIT Delhi (iris) (gray and color), and AR (face) (color).\nPerformance of the generated templates is measured in terms of Correlation\nCoefficient (Cr), Root Mean Square Error (RMSE), Peak Signal to Noise Ratio\n(PSNR), Structural Similarity (SSIM), Mean Absolute Error (MAE), Number of\nPixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI). By\nexperimental results, it has been proved that proposed methods are superior\nthan other state-of-the-art methods in qualitative as well as quantitative\nanalysis. Furthermore, CBRW performs better on both gray as well as color\nimages.\n", "link": "http://arxiv.org/abs/2404.16739v1", "date": "2024-04-25", "relevancy": 2.3551, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4878}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4662}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.459}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CBRW%3A%20A%20Novel%20Approach%20for%20Cancelable%20Biometric%20Template%20Generation%0A%20%20based%20on&body=Title%3A%20CBRW%3A%20A%20Novel%20Approach%20for%20Cancelable%20Biometric%20Template%20Generation%0A%20%20based%20on%0AAuthor%3A%20Nitin%20Kumar%20and%20%20Manisha%0AAbstract%3A%20%20%20Cancelable%20Biometric%20is%20a%20challenging%20research%20field%20in%20which%20security%20of%20an%0Aoriginal%20biometric%20image%20is%20ensured%20by%20transforming%20the%20original%20biometric%20into%0Aanother%20irreversible%20domain.%20Several%20approaches%20have%20been%20suggested%20in%0Aliterature%20for%20generating%20cancelable%20biometric%20templates.%20In%20this%20paper%2C%20two%0Anovel%20and%20simple%20cancelable%20biometric%20template%20generation%20methods%20based%20on%0ARandom%20Walk%20%28CBRW%29%20have%20been%20proposed.%20By%20employing%20random%20walk%20and%20other%20steps%0Agiven%20in%20the%20proposed%20two%20algorithms%20viz.%20CBRW-BitXOR%20and%20CBRW-BitCMP%2C%20the%0Aoriginal%20biometric%20is%20transformed%20into%20a%20cancellable%20template.%20The%20performance%0Aof%20the%20proposed%20methods%20is%20compared%20with%20other%20state-of-the-art%20methods.%0AExperiments%20have%20been%20performed%20on%20eight%20publicly%20available%20gray%20and%20color%0Adatasets%20i.e.%20CP%20%28ear%29%20%28gray%20and%20color%29%2C%20UTIRIS%20%28iris%29%20%28gray%20and%20color%29%2C%20ORL%0A%28face%29%20%28gray%29%2C%20IIT%20Delhi%20%28iris%29%20%28gray%20and%20color%29%2C%20and%20AR%20%28face%29%20%28color%29.%0APerformance%20of%20the%20generated%20templates%20is%20measured%20in%20terms%20of%20Correlation%0ACoefficient%20%28Cr%29%2C%20Root%20Mean%20Square%20Error%20%28RMSE%29%2C%20Peak%20Signal%20to%20Noise%20Ratio%0A%28PSNR%29%2C%20Structural%20Similarity%20%28SSIM%29%2C%20Mean%20Absolute%20Error%20%28MAE%29%2C%20Number%20of%0APixel%20Change%20Rate%20%28NPCR%29%2C%20and%20Unified%20Average%20Changing%20Intensity%20%28UACI%29.%20By%0Aexperimental%20results%2C%20it%20has%20been%20proved%20that%20proposed%20methods%20are%20superior%0Athan%20other%20state-of-the-art%20methods%20in%20qualitative%20as%20well%20as%20quantitative%0Aanalysis.%20Furthermore%2C%20CBRW%20performs%20better%20on%20both%20gray%20as%20well%20as%20color%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16739v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CBRW%3A%20A%20Novel%20Approach%20for%20Cancelable%20Biometric%20Template%20Generation%0A%20%20based%20on&entry.906535625=Nitin%20Kumar%20and%20%20Manisha&entry.1292438233=%20%20Cancelable%20Biometric%20is%20a%20challenging%20research%20field%20in%20which%20security%20of%20an%0Aoriginal%20biometric%20image%20is%20ensured%20by%20transforming%20the%20original%20biometric%20into%0Aanother%20irreversible%20domain.%20Several%20approaches%20have%20been%20suggested%20in%0Aliterature%20for%20generating%20cancelable%20biometric%20templates.%20In%20this%20paper%2C%20two%0Anovel%20and%20simple%20cancelable%20biometric%20template%20generation%20methods%20based%20on%0ARandom%20Walk%20%28CBRW%29%20have%20been%20proposed.%20By%20employing%20random%20walk%20and%20other%20steps%0Agiven%20in%20the%20proposed%20two%20algorithms%20viz.%20CBRW-BitXOR%20and%20CBRW-BitCMP%2C%20the%0Aoriginal%20biometric%20is%20transformed%20into%20a%20cancellable%20template.%20The%20performance%0Aof%20the%20proposed%20methods%20is%20compared%20with%20other%20state-of-the-art%20methods.%0AExperiments%20have%20been%20performed%20on%20eight%20publicly%20available%20gray%20and%20color%0Adatasets%20i.e.%20CP%20%28ear%29%20%28gray%20and%20color%29%2C%20UTIRIS%20%28iris%29%20%28gray%20and%20color%29%2C%20ORL%0A%28face%29%20%28gray%29%2C%20IIT%20Delhi%20%28iris%29%20%28gray%20and%20color%29%2C%20and%20AR%20%28face%29%20%28color%29.%0APerformance%20of%20the%20generated%20templates%20is%20measured%20in%20terms%20of%20Correlation%0ACoefficient%20%28Cr%29%2C%20Root%20Mean%20Square%20Error%20%28RMSE%29%2C%20Peak%20Signal%20to%20Noise%20Ratio%0A%28PSNR%29%2C%20Structural%20Similarity%20%28SSIM%29%2C%20Mean%20Absolute%20Error%20%28MAE%29%2C%20Number%20of%0APixel%20Change%20Rate%20%28NPCR%29%2C%20and%20Unified%20Average%20Changing%20Intensity%20%28UACI%29.%20By%0Aexperimental%20results%2C%20it%20has%20been%20proved%20that%20proposed%20methods%20are%20superior%0Athan%20other%20state-of-the-art%20methods%20in%20qualitative%20as%20well%20as%20quantitative%0Aanalysis.%20Furthermore%2C%20CBRW%20performs%20better%20on%20both%20gray%20as%20well%20as%20color%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16739v1&entry.124074799=Read"},
{"title": "PhyRecon: Physically Plausible Neural Scene Reconstruction", "author": "Junfeng Ni and Yixin Chen and Bohan Jing and Nan Jiang and Bin Wang and Bo Dai and Yixin Zhu and Song-Chun Zhu and Siyuan Huang", "abstract": "  While neural implicit representations have gained popularity in multi-view 3D\nreconstruction, previous work struggles to yield physically plausible results,\nthereby limiting their applications in physics-demanding domains like embodied\nAI and robotics. The lack of plausibility originates from both the absence of\nphysics modeling in the existing pipeline and their inability to recover\nintricate geometrical structures. In this paper, we introduce PhyRecon, which\nstands as the first approach to harness both differentiable rendering and\ndifferentiable physics simulation to learn implicit surface representations.\nOur framework proposes a novel differentiable particle-based physical simulator\nseamlessly integrated with the neural implicit representation. At its core is\nan efficient transformation between SDF-based implicit representation and\nexplicit surface points by our proposed algorithm, Surface Points Marching\nCubes (SP-MC), enabling differentiable learning with both rendering and\nphysical losses. Moreover, we model both rendering and physical uncertainty to\nidentify and compensate for the inconsistent and inaccurate monocular geometric\npriors. The physical uncertainty additionally enables a physics-guided pixel\nsampling to enhance the learning of slender structures. By amalgamating these\ntechniques, our model facilitates efficient joint modeling with appearance,\ngeometry, and physics. Extensive experiments demonstrate that PhyRecon\nsignificantly outperforms all state-of-the-art methods in terms of\nreconstruction quality. Our reconstruction results also yield superior physical\nstability, verified by Isaac Gym, with at least a 40% improvement across all\ndatasets, opening broader avenues for future physics-based applications.\n", "link": "http://arxiv.org/abs/2404.16666v1", "date": "2024-04-25", "relevancy": 2.3239, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5739}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5642}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PhyRecon%3A%20Physically%20Plausible%20Neural%20Scene%20Reconstruction&body=Title%3A%20PhyRecon%3A%20Physically%20Plausible%20Neural%20Scene%20Reconstruction%0AAuthor%3A%20Junfeng%20Ni%20and%20Yixin%20Chen%20and%20Bohan%20Jing%20and%20Nan%20Jiang%20and%20Bin%20Wang%20and%20Bo%20Dai%20and%20Yixin%20Zhu%20and%20Song-Chun%20Zhu%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20While%20neural%20implicit%20representations%20have%20gained%20popularity%20in%20multi-view%203D%0Areconstruction%2C%20previous%20work%20struggles%20to%20yield%20physically%20plausible%20results%2C%0Athereby%20limiting%20their%20applications%20in%20physics-demanding%20domains%20like%20embodied%0AAI%20and%20robotics.%20The%20lack%20of%20plausibility%20originates%20from%20both%20the%20absence%20of%0Aphysics%20modeling%20in%20the%20existing%20pipeline%20and%20their%20inability%20to%20recover%0Aintricate%20geometrical%20structures.%20In%20this%20paper%2C%20we%20introduce%20PhyRecon%2C%20which%0Astands%20as%20the%20first%20approach%20to%20harness%20both%20differentiable%20rendering%20and%0Adifferentiable%20physics%20simulation%20to%20learn%20implicit%20surface%20representations.%0AOur%20framework%20proposes%20a%20novel%20differentiable%20particle-based%20physical%20simulator%0Aseamlessly%20integrated%20with%20the%20neural%20implicit%20representation.%20At%20its%20core%20is%0Aan%20efficient%20transformation%20between%20SDF-based%20implicit%20representation%20and%0Aexplicit%20surface%20points%20by%20our%20proposed%20algorithm%2C%20Surface%20Points%20Marching%0ACubes%20%28SP-MC%29%2C%20enabling%20differentiable%20learning%20with%20both%20rendering%20and%0Aphysical%20losses.%20Moreover%2C%20we%20model%20both%20rendering%20and%20physical%20uncertainty%20to%0Aidentify%20and%20compensate%20for%20the%20inconsistent%20and%20inaccurate%20monocular%20geometric%0Apriors.%20The%20physical%20uncertainty%20additionally%20enables%20a%20physics-guided%20pixel%0Asampling%20to%20enhance%20the%20learning%20of%20slender%20structures.%20By%20amalgamating%20these%0Atechniques%2C%20our%20model%20facilitates%20efficient%20joint%20modeling%20with%20appearance%2C%0Ageometry%2C%20and%20physics.%20Extensive%20experiments%20demonstrate%20that%20PhyRecon%0Asignificantly%20outperforms%20all%20state-of-the-art%20methods%20in%20terms%20of%0Areconstruction%20quality.%20Our%20reconstruction%20results%20also%20yield%20superior%20physical%0Astability%2C%20verified%20by%20Isaac%20Gym%2C%20with%20at%20least%20a%2040%25%20improvement%20across%20all%0Adatasets%2C%20opening%20broader%20avenues%20for%20future%20physics-based%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16666v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhyRecon%3A%20Physically%20Plausible%20Neural%20Scene%20Reconstruction&entry.906535625=Junfeng%20Ni%20and%20Yixin%20Chen%20and%20Bohan%20Jing%20and%20Nan%20Jiang%20and%20Bin%20Wang%20and%20Bo%20Dai%20and%20Yixin%20Zhu%20and%20Song-Chun%20Zhu%20and%20Siyuan%20Huang&entry.1292438233=%20%20While%20neural%20implicit%20representations%20have%20gained%20popularity%20in%20multi-view%203D%0Areconstruction%2C%20previous%20work%20struggles%20to%20yield%20physically%20plausible%20results%2C%0Athereby%20limiting%20their%20applications%20in%20physics-demanding%20domains%20like%20embodied%0AAI%20and%20robotics.%20The%20lack%20of%20plausibility%20originates%20from%20both%20the%20absence%20of%0Aphysics%20modeling%20in%20the%20existing%20pipeline%20and%20their%20inability%20to%20recover%0Aintricate%20geometrical%20structures.%20In%20this%20paper%2C%20we%20introduce%20PhyRecon%2C%20which%0Astands%20as%20the%20first%20approach%20to%20harness%20both%20differentiable%20rendering%20and%0Adifferentiable%20physics%20simulation%20to%20learn%20implicit%20surface%20representations.%0AOur%20framework%20proposes%20a%20novel%20differentiable%20particle-based%20physical%20simulator%0Aseamlessly%20integrated%20with%20the%20neural%20implicit%20representation.%20At%20its%20core%20is%0Aan%20efficient%20transformation%20between%20SDF-based%20implicit%20representation%20and%0Aexplicit%20surface%20points%20by%20our%20proposed%20algorithm%2C%20Surface%20Points%20Marching%0ACubes%20%28SP-MC%29%2C%20enabling%20differentiable%20learning%20with%20both%20rendering%20and%0Aphysical%20losses.%20Moreover%2C%20we%20model%20both%20rendering%20and%20physical%20uncertainty%20to%0Aidentify%20and%20compensate%20for%20the%20inconsistent%20and%20inaccurate%20monocular%20geometric%0Apriors.%20The%20physical%20uncertainty%20additionally%20enables%20a%20physics-guided%20pixel%0Asampling%20to%20enhance%20the%20learning%20of%20slender%20structures.%20By%20amalgamating%20these%0Atechniques%2C%20our%20model%20facilitates%20efficient%20joint%20modeling%20with%20appearance%2C%0Ageometry%2C%20and%20physics.%20Extensive%20experiments%20demonstrate%20that%20PhyRecon%0Asignificantly%20outperforms%20all%20state-of-the-art%20methods%20in%20terms%20of%0Areconstruction%20quality.%20Our%20reconstruction%20results%20also%20yield%20superior%20physical%0Astability%2C%20verified%20by%20Isaac%20Gym%2C%20with%20at%20least%20a%2040%25%20improvement%20across%20all%0Adatasets%2C%20opening%20broader%20avenues%20for%20future%20physics-based%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16666v1&entry.124074799=Read"},
{"title": "3D Face Modeling via Weakly-supervised Disentanglement Network joint\n  Identity-consistency Prior", "author": "Guohao Li and Hongyu Yang and Di Huang and Yunhong Wang", "abstract": "  Generative 3D face models featuring disentangled controlling factors hold\nimmense potential for diverse applications in computer vision and computer\ngraphics. However, previous 3D face modeling methods face a challenge as they\ndemand specific labels to effectively disentangle these factors. This becomes\nparticularly problematic when integrating multiple 3D face datasets to improve\nthe generalization of the model. Addressing this issue, this paper introduces a\nWeakly-Supervised Disentanglement Framework, denoted as WSDF, to facilitate the\ntraining of controllable 3D face models without an overly stringent labeling\nrequirement. Adhering to the paradigm of Variational Autoencoders (VAEs), the\nproposed model achieves disentanglement of identity and expression controlling\nfactors through a two-branch encoder equipped with dedicated\nidentity-consistency prior. It then faithfully re-entangles these factors via a\ntensor-based combination mechanism. Notably, the introduction of the Neutral\nBank allows precise acquisition of subject-specific information using only\nidentity labels, thereby averting degeneration due to insufficient supervision.\nAdditionally, the framework incorporates a label-free second-order loss\nfunction for the expression factor to regulate deformation space and eliminate\nextraneous information, resulting in enhanced disentanglement. Extensive\nexperiments have been conducted to substantiate the superior performance of\nWSDF. Our code is available at https://github.com/liguohao96/WSDF.\n", "link": "http://arxiv.org/abs/2404.16536v1", "date": "2024-04-25", "relevancy": 2.309, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5867}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5652}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Face%20Modeling%20via%20Weakly-supervised%20Disentanglement%20Network%20joint%0A%20%20Identity-consistency%20Prior&body=Title%3A%203D%20Face%20Modeling%20via%20Weakly-supervised%20Disentanglement%20Network%20joint%0A%20%20Identity-consistency%20Prior%0AAuthor%3A%20Guohao%20Li%20and%20Hongyu%20Yang%20and%20Di%20Huang%20and%20Yunhong%20Wang%0AAbstract%3A%20%20%20Generative%203D%20face%20models%20featuring%20disentangled%20controlling%20factors%20hold%0Aimmense%20potential%20for%20diverse%20applications%20in%20computer%20vision%20and%20computer%0Agraphics.%20However%2C%20previous%203D%20face%20modeling%20methods%20face%20a%20challenge%20as%20they%0Ademand%20specific%20labels%20to%20effectively%20disentangle%20these%20factors.%20This%20becomes%0Aparticularly%20problematic%20when%20integrating%20multiple%203D%20face%20datasets%20to%20improve%0Athe%20generalization%20of%20the%20model.%20Addressing%20this%20issue%2C%20this%20paper%20introduces%20a%0AWeakly-Supervised%20Disentanglement%20Framework%2C%20denoted%20as%20WSDF%2C%20to%20facilitate%20the%0Atraining%20of%20controllable%203D%20face%20models%20without%20an%20overly%20stringent%20labeling%0Arequirement.%20Adhering%20to%20the%20paradigm%20of%20Variational%20Autoencoders%20%28VAEs%29%2C%20the%0Aproposed%20model%20achieves%20disentanglement%20of%20identity%20and%20expression%20controlling%0Afactors%20through%20a%20two-branch%20encoder%20equipped%20with%20dedicated%0Aidentity-consistency%20prior.%20It%20then%20faithfully%20re-entangles%20these%20factors%20via%20a%0Atensor-based%20combination%20mechanism.%20Notably%2C%20the%20introduction%20of%20the%20Neutral%0ABank%20allows%20precise%20acquisition%20of%20subject-specific%20information%20using%20only%0Aidentity%20labels%2C%20thereby%20averting%20degeneration%20due%20to%20insufficient%20supervision.%0AAdditionally%2C%20the%20framework%20incorporates%20a%20label-free%20second-order%20loss%0Afunction%20for%20the%20expression%20factor%20to%20regulate%20deformation%20space%20and%20eliminate%0Aextraneous%20information%2C%20resulting%20in%20enhanced%20disentanglement.%20Extensive%0Aexperiments%20have%20been%20conducted%20to%20substantiate%20the%20superior%20performance%20of%0AWSDF.%20Our%20code%20is%20available%20at%20https%3A//github.com/liguohao96/WSDF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16536v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Face%20Modeling%20via%20Weakly-supervised%20Disentanglement%20Network%20joint%0A%20%20Identity-consistency%20Prior&entry.906535625=Guohao%20Li%20and%20Hongyu%20Yang%20and%20Di%20Huang%20and%20Yunhong%20Wang&entry.1292438233=%20%20Generative%203D%20face%20models%20featuring%20disentangled%20controlling%20factors%20hold%0Aimmense%20potential%20for%20diverse%20applications%20in%20computer%20vision%20and%20computer%0Agraphics.%20However%2C%20previous%203D%20face%20modeling%20methods%20face%20a%20challenge%20as%20they%0Ademand%20specific%20labels%20to%20effectively%20disentangle%20these%20factors.%20This%20becomes%0Aparticularly%20problematic%20when%20integrating%20multiple%203D%20face%20datasets%20to%20improve%0Athe%20generalization%20of%20the%20model.%20Addressing%20this%20issue%2C%20this%20paper%20introduces%20a%0AWeakly-Supervised%20Disentanglement%20Framework%2C%20denoted%20as%20WSDF%2C%20to%20facilitate%20the%0Atraining%20of%20controllable%203D%20face%20models%20without%20an%20overly%20stringent%20labeling%0Arequirement.%20Adhering%20to%20the%20paradigm%20of%20Variational%20Autoencoders%20%28VAEs%29%2C%20the%0Aproposed%20model%20achieves%20disentanglement%20of%20identity%20and%20expression%20controlling%0Afactors%20through%20a%20two-branch%20encoder%20equipped%20with%20dedicated%0Aidentity-consistency%20prior.%20It%20then%20faithfully%20re-entangles%20these%20factors%20via%20a%0Atensor-based%20combination%20mechanism.%20Notably%2C%20the%20introduction%20of%20the%20Neutral%0ABank%20allows%20precise%20acquisition%20of%20subject-specific%20information%20using%20only%0Aidentity%20labels%2C%20thereby%20averting%20degeneration%20due%20to%20insufficient%20supervision.%0AAdditionally%2C%20the%20framework%20incorporates%20a%20label-free%20second-order%20loss%0Afunction%20for%20the%20expression%20factor%20to%20regulate%20deformation%20space%20and%20eliminate%0Aextraneous%20information%2C%20resulting%20in%20enhanced%20disentanglement.%20Extensive%0Aexperiments%20have%20been%20conducted%20to%20substantiate%20the%20superior%20performance%20of%0AWSDF.%20Our%20code%20is%20available%20at%20https%3A//github.com/liguohao96/WSDF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16536v1&entry.124074799=Read"},
{"title": "Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to\n  Enhance Visio-Linguistic Compositional Understanding", "author": "Le Zhang and Rabiul Awal and Aishwarya Agrawal", "abstract": "  Vision-Language Models (VLMs), such as CLIP, exhibit strong image-text\ncomprehension abilities, facilitating advances in several downstream tasks such\nas zero-shot image classification, image-text retrieval, and text-to-image\ngeneration. However, the compositional reasoning abilities of existing VLMs\nremains subpar. The root of this limitation lies in the inadequate alignment\nbetween the images and captions in the pretraining datasets. Additionally, the\ncurrent contrastive learning objective fails to focus on fine-grained grounding\ncomponents like relations, actions, and attributes, resulting in \"bag-of-words\"\nrepresentations. We introduce a simple and effective method to improve\ncompositional reasoning in VLMs. Our method better leverages available datasets\nby refining and expanding the standard image-text contrastive learning\nframework. Our approach does not require specific annotations and does not\nincur extra parameters. When integrated with CLIP, our technique yields notable\nimprovement over state-of-the-art baselines across five vision-language\ncompositional benchmarks. We open-source our code at\nhttps://github.com/lezhang7/Enhance-FineGrained.\n", "link": "http://arxiv.org/abs/2306.08832v4", "date": "2024-04-25", "relevancy": 2.3074, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6064}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5569}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5529}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contrasting%20Intra-Modal%20and%20Ranking%20Cross-Modal%20Hard%20Negatives%20to%0A%20%20Enhance%20Visio-Linguistic%20Compositional%20Understanding&body=Title%3A%20Contrasting%20Intra-Modal%20and%20Ranking%20Cross-Modal%20Hard%20Negatives%20to%0A%20%20Enhance%20Visio-Linguistic%20Compositional%20Understanding%0AAuthor%3A%20Le%20Zhang%20and%20Rabiul%20Awal%20and%20Aishwarya%20Agrawal%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20exhibit%20strong%20image-text%0Acomprehension%20abilities%2C%20facilitating%20advances%20in%20several%20downstream%20tasks%20such%0Aas%20zero-shot%20image%20classification%2C%20image-text%20retrieval%2C%20and%20text-to-image%0Ageneration.%20However%2C%20the%20compositional%20reasoning%20abilities%20of%20existing%20VLMs%0Aremains%20subpar.%20The%20root%20of%20this%20limitation%20lies%20in%20the%20inadequate%20alignment%0Abetween%20the%20images%20and%20captions%20in%20the%20pretraining%20datasets.%20Additionally%2C%20the%0Acurrent%20contrastive%20learning%20objective%20fails%20to%20focus%20on%20fine-grained%20grounding%0Acomponents%20like%20relations%2C%20actions%2C%20and%20attributes%2C%20resulting%20in%20%22bag-of-words%22%0Arepresentations.%20We%20introduce%20a%20simple%20and%20effective%20method%20to%20improve%0Acompositional%20reasoning%20in%20VLMs.%20Our%20method%20better%20leverages%20available%20datasets%0Aby%20refining%20and%20expanding%20the%20standard%20image-text%20contrastive%20learning%0Aframework.%20Our%20approach%20does%20not%20require%20specific%20annotations%20and%20does%20not%0Aincur%20extra%20parameters.%20When%20integrated%20with%20CLIP%2C%20our%20technique%20yields%20notable%0Aimprovement%20over%20state-of-the-art%20baselines%20across%20five%20vision-language%0Acompositional%20benchmarks.%20We%20open-source%20our%20code%20at%0Ahttps%3A//github.com/lezhang7/Enhance-FineGrained.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.08832v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrasting%20Intra-Modal%20and%20Ranking%20Cross-Modal%20Hard%20Negatives%20to%0A%20%20Enhance%20Visio-Linguistic%20Compositional%20Understanding&entry.906535625=Le%20Zhang%20and%20Rabiul%20Awal%20and%20Aishwarya%20Agrawal&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20exhibit%20strong%20image-text%0Acomprehension%20abilities%2C%20facilitating%20advances%20in%20several%20downstream%20tasks%20such%0Aas%20zero-shot%20image%20classification%2C%20image-text%20retrieval%2C%20and%20text-to-image%0Ageneration.%20However%2C%20the%20compositional%20reasoning%20abilities%20of%20existing%20VLMs%0Aremains%20subpar.%20The%20root%20of%20this%20limitation%20lies%20in%20the%20inadequate%20alignment%0Abetween%20the%20images%20and%20captions%20in%20the%20pretraining%20datasets.%20Additionally%2C%20the%0Acurrent%20contrastive%20learning%20objective%20fails%20to%20focus%20on%20fine-grained%20grounding%0Acomponents%20like%20relations%2C%20actions%2C%20and%20attributes%2C%20resulting%20in%20%22bag-of-words%22%0Arepresentations.%20We%20introduce%20a%20simple%20and%20effective%20method%20to%20improve%0Acompositional%20reasoning%20in%20VLMs.%20Our%20method%20better%20leverages%20available%20datasets%0Aby%20refining%20and%20expanding%20the%20standard%20image-text%20contrastive%20learning%0Aframework.%20Our%20approach%20does%20not%20require%20specific%20annotations%20and%20does%20not%0Aincur%20extra%20parameters.%20When%20integrated%20with%20CLIP%2C%20our%20technique%20yields%20notable%0Aimprovement%20over%20state-of-the-art%20baselines%20across%20five%20vision-language%0Acompositional%20benchmarks.%20We%20open-source%20our%20code%20at%0Ahttps%3A//github.com/lezhang7/Enhance-FineGrained.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.08832v4&entry.124074799=Read"},
{"title": "Commonsense Prototype for Outdoor Unsupervised 3D Object Detection", "author": "Hai Wu and Shijia Zhao and Xun Huang and Chenglu Wen and Xin Li and Cheng Wang", "abstract": "  The prevalent approaches of unsupervised 3D object detection follow\ncluster-based pseudo-label generation and iterative self-training processes.\nHowever, the challenge arises due to the sparsity of LiDAR scans, which leads\nto pseudo-labels with erroneous size and position, resulting in subpar\ndetection performance. To tackle this problem, this paper introduces a\nCommonsense Prototype-based Detector, termed CPD, for unsupervised 3D object\ndetection. CPD first constructs Commonsense Prototype (CProto) characterized by\nhigh-quality bounding box and dense points, based on commonsense intuition.\nSubsequently, CPD refines the low-quality pseudo-labels by leveraging the size\nprior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely\nscanned objects by the geometric knowledge from CProto. CPD outperforms\nstate-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD),\nPandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD\nand testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on\neasy and moderate car classes, respectively. These achievements position CPD in\nclose proximity to fully supervised detectors, highlighting the significance of\nour method. The code will be available at https://github.com/hailanyi/CPD.\n", "link": "http://arxiv.org/abs/2404.16493v1", "date": "2024-04-25", "relevancy": 2.2754, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5843}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5756}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5559}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection&body=Title%3A%20Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection%0AAuthor%3A%20Hai%20Wu%20and%20Shijia%20Zhao%20and%20Xun%20Huang%20and%20Chenglu%20Wen%20and%20Xin%20Li%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20The%20prevalent%20approaches%20of%20unsupervised%203D%20object%20detection%20follow%0Acluster-based%20pseudo-label%20generation%20and%20iterative%20self-training%20processes.%0AHowever%2C%20the%20challenge%20arises%20due%20to%20the%20sparsity%20of%20LiDAR%20scans%2C%20which%20leads%0Ato%20pseudo-labels%20with%20erroneous%20size%20and%20position%2C%20resulting%20in%20subpar%0Adetection%20performance.%20To%20tackle%20this%20problem%2C%20this%20paper%20introduces%20a%0ACommonsense%20Prototype-based%20Detector%2C%20termed%20CPD%2C%20for%20unsupervised%203D%20object%0Adetection.%20CPD%20first%20constructs%20Commonsense%20Prototype%20%28CProto%29%20characterized%20by%0Ahigh-quality%20bounding%20box%20and%20dense%20points%2C%20based%20on%20commonsense%20intuition.%0ASubsequently%2C%20CPD%20refines%20the%20low-quality%20pseudo-labels%20by%20leveraging%20the%20size%0Aprior%20from%20CProto.%20Furthermore%2C%20CPD%20enhances%20the%20detection%20accuracy%20of%20sparsely%0Ascanned%20objects%20by%20the%20geometric%20knowledge%20from%20CProto.%20CPD%20outperforms%0Astate-of-the-art%20unsupervised%203D%20detectors%20on%20Waymo%20Open%20Dataset%20%28WOD%29%2C%0APandaSet%2C%20and%20KITTI%20datasets%20by%20a%20large%20margin.%20Besides%2C%20by%20training%20CPD%20on%20WOD%0Aand%20testing%20on%20KITTI%2C%20CPD%20attains%2090.85%25%20and%2081.01%25%203D%20Average%20Precision%20on%0Aeasy%20and%20moderate%20car%20classes%2C%20respectively.%20These%20achievements%20position%20CPD%20in%0Aclose%20proximity%20to%20fully%20supervised%20detectors%2C%20highlighting%20the%20significance%20of%0Aour%20method.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/hailanyi/CPD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16493v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection&entry.906535625=Hai%20Wu%20and%20Shijia%20Zhao%20and%20Xun%20Huang%20and%20Chenglu%20Wen%20and%20Xin%20Li%20and%20Cheng%20Wang&entry.1292438233=%20%20The%20prevalent%20approaches%20of%20unsupervised%203D%20object%20detection%20follow%0Acluster-based%20pseudo-label%20generation%20and%20iterative%20self-training%20processes.%0AHowever%2C%20the%20challenge%20arises%20due%20to%20the%20sparsity%20of%20LiDAR%20scans%2C%20which%20leads%0Ato%20pseudo-labels%20with%20erroneous%20size%20and%20position%2C%20resulting%20in%20subpar%0Adetection%20performance.%20To%20tackle%20this%20problem%2C%20this%20paper%20introduces%20a%0ACommonsense%20Prototype-based%20Detector%2C%20termed%20CPD%2C%20for%20unsupervised%203D%20object%0Adetection.%20CPD%20first%20constructs%20Commonsense%20Prototype%20%28CProto%29%20characterized%20by%0Ahigh-quality%20bounding%20box%20and%20dense%20points%2C%20based%20on%20commonsense%20intuition.%0ASubsequently%2C%20CPD%20refines%20the%20low-quality%20pseudo-labels%20by%20leveraging%20the%20size%0Aprior%20from%20CProto.%20Furthermore%2C%20CPD%20enhances%20the%20detection%20accuracy%20of%20sparsely%0Ascanned%20objects%20by%20the%20geometric%20knowledge%20from%20CProto.%20CPD%20outperforms%0Astate-of-the-art%20unsupervised%203D%20detectors%20on%20Waymo%20Open%20Dataset%20%28WOD%29%2C%0APandaSet%2C%20and%20KITTI%20datasets%20by%20a%20large%20margin.%20Besides%2C%20by%20training%20CPD%20on%20WOD%0Aand%20testing%20on%20KITTI%2C%20CPD%20attains%2090.85%25%20and%2081.01%25%203D%20Average%20Precision%20on%0Aeasy%20and%20moderate%20car%20classes%2C%20respectively.%20These%20achievements%20position%20CPD%20in%0Aclose%20proximity%20to%20fully%20supervised%20detectors%2C%20highlighting%20the%20significance%20of%0Aour%20method.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/hailanyi/CPD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16493v1&entry.124074799=Read"},
{"title": "Denoising: from classical methods to deep CNNs", "author": "Jean-Eric Campagne", "abstract": "  This paper aims to explore the evolution of image denoising in a\npedagological way. We briefly review classical methods such as Fourier analysis\nand wavelet bases, highlighting the challenges they faced until the emergence\nof neural networks, notably the U-Net, in the 2010s. The remarkable performance\nof these networks has been demonstrated in studies such as Kadkhodaie et al.\n(2024). They exhibit adaptability to various image types, including those with\nfixed regularity, facial images, and bedroom scenes, achieving optimal results\nand biased towards geometry-adaptive harmonic basis. The introduction of score\ndiffusion has played a crucial role in image generation. In this context,\ndenoising becomes essential as it facilitates the estimation of probability\ndensity scores. We discuss the prerequisites for genuine learning of\nprobability densities, offering insights that extend from mathematical research\nto the implications of universal structures.\n", "link": "http://arxiv.org/abs/2404.16617v1", "date": "2024-04-25", "relevancy": 2.2633, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5794}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5582}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5507}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Denoising%3A%20from%20classical%20methods%20to%20deep%20CNNs&body=Title%3A%20Denoising%3A%20from%20classical%20methods%20to%20deep%20CNNs%0AAuthor%3A%20Jean-Eric%20Campagne%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20explore%20the%20evolution%20of%20image%20denoising%20in%20a%0Apedagological%20way.%20We%20briefly%20review%20classical%20methods%20such%20as%20Fourier%20analysis%0Aand%20wavelet%20bases%2C%20highlighting%20the%20challenges%20they%20faced%20until%20the%20emergence%0Aof%20neural%20networks%2C%20notably%20the%20U-Net%2C%20in%20the%202010s.%20The%20remarkable%20performance%0Aof%20these%20networks%20has%20been%20demonstrated%20in%20studies%20such%20as%20Kadkhodaie%20et%20al.%0A%282024%29.%20They%20exhibit%20adaptability%20to%20various%20image%20types%2C%20including%20those%20with%0Afixed%20regularity%2C%20facial%20images%2C%20and%20bedroom%20scenes%2C%20achieving%20optimal%20results%0Aand%20biased%20towards%20geometry-adaptive%20harmonic%20basis.%20The%20introduction%20of%20score%0Adiffusion%20has%20played%20a%20crucial%20role%20in%20image%20generation.%20In%20this%20context%2C%0Adenoising%20becomes%20essential%20as%20it%20facilitates%20the%20estimation%20of%20probability%0Adensity%20scores.%20We%20discuss%20the%20prerequisites%20for%20genuine%20learning%20of%0Aprobability%20densities%2C%20offering%20insights%20that%20extend%20from%20mathematical%20research%0Ato%20the%20implications%20of%20universal%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16617v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%3A%20from%20classical%20methods%20to%20deep%20CNNs&entry.906535625=Jean-Eric%20Campagne&entry.1292438233=%20%20This%20paper%20aims%20to%20explore%20the%20evolution%20of%20image%20denoising%20in%20a%0Apedagological%20way.%20We%20briefly%20review%20classical%20methods%20such%20as%20Fourier%20analysis%0Aand%20wavelet%20bases%2C%20highlighting%20the%20challenges%20they%20faced%20until%20the%20emergence%0Aof%20neural%20networks%2C%20notably%20the%20U-Net%2C%20in%20the%202010s.%20The%20remarkable%20performance%0Aof%20these%20networks%20has%20been%20demonstrated%20in%20studies%20such%20as%20Kadkhodaie%20et%20al.%0A%282024%29.%20They%20exhibit%20adaptability%20to%20various%20image%20types%2C%20including%20those%20with%0Afixed%20regularity%2C%20facial%20images%2C%20and%20bedroom%20scenes%2C%20achieving%20optimal%20results%0Aand%20biased%20towards%20geometry-adaptive%20harmonic%20basis.%20The%20introduction%20of%20score%0Adiffusion%20has%20played%20a%20crucial%20role%20in%20image%20generation.%20In%20this%20context%2C%0Adenoising%20becomes%20essential%20as%20it%20facilitates%20the%20estimation%20of%20probability%0Adensity%20scores.%20We%20discuss%20the%20prerequisites%20for%20genuine%20learning%20of%0Aprobability%20densities%2C%20offering%20insights%20that%20extend%20from%20mathematical%20research%0Ato%20the%20implications%20of%20universal%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16617v1&entry.124074799=Read"},
{"title": "Learning Visuotactile Skills with Two Multifingered Hands", "author": "Toru Lin and Yu Zhang and Qiyang Li and Haozhi Qi and Brent Yi and Sergey Levine and Jitendra Malik", "abstract": "  Aiming to replicate human-like dexterity, perceptual experiences, and motion\npatterns, we explore learning from human demonstrations using a bimanual system\nwith multifingered hands and visuotactile data. Two significant challenges\nexist: the lack of an affordable and accessible teleoperation system suitable\nfor a dual-arm setup with multifingered hands, and the scarcity of\nmultifingered hand hardware equipped with touch sensing. To tackle the first\nchallenge, we develop HATO, a low-cost hands-arms teleoperation system that\nleverages off-the-shelf electronics, complemented with a software suite that\nenables efficient data collection; the comprehensive software suite also\nsupports multimodal data processing, scalable policy learning, and smooth\npolicy deployment. To tackle the latter challenge, we introduce a novel\nhardware adaptation by repurposing two prosthetic hands equipped with touch\nsensors for research. Using visuotactile data collected from our system, we\nlearn skills to complete long-horizon, high-precision tasks which are difficult\nto achieve without multifingered dexterity and touch feedback. Furthermore, we\nempirically investigate the effects of dataset size, sensing modality, and\nvisual input preprocessing on policy learning. Our results mark a promising\nstep forward in bimanual multifingered manipulation from visuotactile data.\nVideos, code, and datasets can be found at https://toruowo.github.io/hato/ .\n", "link": "http://arxiv.org/abs/2404.16823v1", "date": "2024-04-25", "relevancy": 2.2601, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.59}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5815}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5386}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Visuotactile%20Skills%20with%20Two%20Multifingered%20Hands&body=Title%3A%20Learning%20Visuotactile%20Skills%20with%20Two%20Multifingered%20Hands%0AAuthor%3A%20Toru%20Lin%20and%20Yu%20Zhang%20and%20Qiyang%20Li%20and%20Haozhi%20Qi%20and%20Brent%20Yi%20and%20Sergey%20Levine%20and%20Jitendra%20Malik%0AAbstract%3A%20%20%20Aiming%20to%20replicate%20human-like%20dexterity%2C%20perceptual%20experiences%2C%20and%20motion%0Apatterns%2C%20we%20explore%20learning%20from%20human%20demonstrations%20using%20a%20bimanual%20system%0Awith%20multifingered%20hands%20and%20visuotactile%20data.%20Two%20significant%20challenges%0Aexist%3A%20the%20lack%20of%20an%20affordable%20and%20accessible%20teleoperation%20system%20suitable%0Afor%20a%20dual-arm%20setup%20with%20multifingered%20hands%2C%20and%20the%20scarcity%20of%0Amultifingered%20hand%20hardware%20equipped%20with%20touch%20sensing.%20To%20tackle%20the%20first%0Achallenge%2C%20we%20develop%20HATO%2C%20a%20low-cost%20hands-arms%20teleoperation%20system%20that%0Aleverages%20off-the-shelf%20electronics%2C%20complemented%20with%20a%20software%20suite%20that%0Aenables%20efficient%20data%20collection%3B%20the%20comprehensive%20software%20suite%20also%0Asupports%20multimodal%20data%20processing%2C%20scalable%20policy%20learning%2C%20and%20smooth%0Apolicy%20deployment.%20To%20tackle%20the%20latter%20challenge%2C%20we%20introduce%20a%20novel%0Ahardware%20adaptation%20by%20repurposing%20two%20prosthetic%20hands%20equipped%20with%20touch%0Asensors%20for%20research.%20Using%20visuotactile%20data%20collected%20from%20our%20system%2C%20we%0Alearn%20skills%20to%20complete%20long-horizon%2C%20high-precision%20tasks%20which%20are%20difficult%0Ato%20achieve%20without%20multifingered%20dexterity%20and%20touch%20feedback.%20Furthermore%2C%20we%0Aempirically%20investigate%20the%20effects%20of%20dataset%20size%2C%20sensing%20modality%2C%20and%0Avisual%20input%20preprocessing%20on%20policy%20learning.%20Our%20results%20mark%20a%20promising%0Astep%20forward%20in%20bimanual%20multifingered%20manipulation%20from%20visuotactile%20data.%0AVideos%2C%20code%2C%20and%20datasets%20can%20be%20found%20at%20https%3A//toruowo.github.io/hato/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16823v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Visuotactile%20Skills%20with%20Two%20Multifingered%20Hands&entry.906535625=Toru%20Lin%20and%20Yu%20Zhang%20and%20Qiyang%20Li%20and%20Haozhi%20Qi%20and%20Brent%20Yi%20and%20Sergey%20Levine%20and%20Jitendra%20Malik&entry.1292438233=%20%20Aiming%20to%20replicate%20human-like%20dexterity%2C%20perceptual%20experiences%2C%20and%20motion%0Apatterns%2C%20we%20explore%20learning%20from%20human%20demonstrations%20using%20a%20bimanual%20system%0Awith%20multifingered%20hands%20and%20visuotactile%20data.%20Two%20significant%20challenges%0Aexist%3A%20the%20lack%20of%20an%20affordable%20and%20accessible%20teleoperation%20system%20suitable%0Afor%20a%20dual-arm%20setup%20with%20multifingered%20hands%2C%20and%20the%20scarcity%20of%0Amultifingered%20hand%20hardware%20equipped%20with%20touch%20sensing.%20To%20tackle%20the%20first%0Achallenge%2C%20we%20develop%20HATO%2C%20a%20low-cost%20hands-arms%20teleoperation%20system%20that%0Aleverages%20off-the-shelf%20electronics%2C%20complemented%20with%20a%20software%20suite%20that%0Aenables%20efficient%20data%20collection%3B%20the%20comprehensive%20software%20suite%20also%0Asupports%20multimodal%20data%20processing%2C%20scalable%20policy%20learning%2C%20and%20smooth%0Apolicy%20deployment.%20To%20tackle%20the%20latter%20challenge%2C%20we%20introduce%20a%20novel%0Ahardware%20adaptation%20by%20repurposing%20two%20prosthetic%20hands%20equipped%20with%20touch%0Asensors%20for%20research.%20Using%20visuotactile%20data%20collected%20from%20our%20system%2C%20we%0Alearn%20skills%20to%20complete%20long-horizon%2C%20high-precision%20tasks%20which%20are%20difficult%0Ato%20achieve%20without%20multifingered%20dexterity%20and%20touch%20feedback.%20Furthermore%2C%20we%0Aempirically%20investigate%20the%20effects%20of%20dataset%20size%2C%20sensing%20modality%2C%20and%0Avisual%20input%20preprocessing%20on%20policy%20learning.%20Our%20results%20mark%20a%20promising%0Astep%20forward%20in%20bimanual%20multifingered%20manipulation%20from%20visuotactile%20data.%0AVideos%2C%20code%2C%20and%20datasets%20can%20be%20found%20at%20https%3A//toruowo.github.io/hato/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16823v1&entry.124074799=Read"},
{"title": "Semantic-aware Next-Best-View for Multi-DoFs Mobile System in\n  Search-and-Acquisition based Visual Perception", "author": "Xiaotong Yu and Chang-Wen Chen", "abstract": "  Efficient visual perception using mobile systems is crucial, particularly in\nunknown environments such as search and rescue operations, where swift and\ncomprehensive perception of objects of interest is essential. In such\nreal-world applications, objects of interest are often situated in complex\nenvironments, making the selection of the 'Next Best' view based solely on\nmaximizing visibility gain suboptimal. Semantics, providing a higher-level\ninterpretation of perception, should significantly contribute to the selection\nof the next viewpoint for various perception tasks. In this study, we formulate\na novel information gain that integrates both visibility gain and semantic gain\nin a unified form to select the semantic-aware Next-Best-View. Additionally, we\ndesign an adaptive strategy with termination criterion to support a two-stage\nsearch-and-acquisition manoeuvre on multiple objects of interest aided by a\nmulti-degree-of-freedoms (Multi-DoFs) mobile system. Several semantically\nrelevant reconstruction metrics, including perspective directivity and region\nof interest (ROI)-to-full reconstruction volume ratio, are introduced to\nevaluate the performance of the proposed approach. Simulation experiments\ndemonstrate the advantages of the proposed approach over existing methods,\nachieving improvements of up to 27.13% for the ROI-to-full reconstruction\nvolume ratio and a 0.88234 average perspective directivity. Furthermore, the\nplanned motion trajectory exhibits better perceiving coverage toward the\ntarget.\n", "link": "http://arxiv.org/abs/2404.16507v1", "date": "2024-04-25", "relevancy": 2.2568, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6169}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5764}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.531}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantic-aware%20Next-Best-View%20for%20Multi-DoFs%20Mobile%20System%20in%0A%20%20Search-and-Acquisition%20based%20Visual%20Perception&body=Title%3A%20Semantic-aware%20Next-Best-View%20for%20Multi-DoFs%20Mobile%20System%20in%0A%20%20Search-and-Acquisition%20based%20Visual%20Perception%0AAuthor%3A%20Xiaotong%20Yu%20and%20Chang-Wen%20Chen%0AAbstract%3A%20%20%20Efficient%20visual%20perception%20using%20mobile%20systems%20is%20crucial%2C%20particularly%20in%0Aunknown%20environments%20such%20as%20search%20and%20rescue%20operations%2C%20where%20swift%20and%0Acomprehensive%20perception%20of%20objects%20of%20interest%20is%20essential.%20In%20such%0Areal-world%20applications%2C%20objects%20of%20interest%20are%20often%20situated%20in%20complex%0Aenvironments%2C%20making%20the%20selection%20of%20the%20%27Next%20Best%27%20view%20based%20solely%20on%0Amaximizing%20visibility%20gain%20suboptimal.%20Semantics%2C%20providing%20a%20higher-level%0Ainterpretation%20of%20perception%2C%20should%20significantly%20contribute%20to%20the%20selection%0Aof%20the%20next%20viewpoint%20for%20various%20perception%20tasks.%20In%20this%20study%2C%20we%20formulate%0Aa%20novel%20information%20gain%20that%20integrates%20both%20visibility%20gain%20and%20semantic%20gain%0Ain%20a%20unified%20form%20to%20select%20the%20semantic-aware%20Next-Best-View.%20Additionally%2C%20we%0Adesign%20an%20adaptive%20strategy%20with%20termination%20criterion%20to%20support%20a%20two-stage%0Asearch-and-acquisition%20manoeuvre%20on%20multiple%20objects%20of%20interest%20aided%20by%20a%0Amulti-degree-of-freedoms%20%28Multi-DoFs%29%20mobile%20system.%20Several%20semantically%0Arelevant%20reconstruction%20metrics%2C%20including%20perspective%20directivity%20and%20region%0Aof%20interest%20%28ROI%29-to-full%20reconstruction%20volume%20ratio%2C%20are%20introduced%20to%0Aevaluate%20the%20performance%20of%20the%20proposed%20approach.%20Simulation%20experiments%0Ademonstrate%20the%20advantages%20of%20the%20proposed%20approach%20over%20existing%20methods%2C%0Aachieving%20improvements%20of%20up%20to%2027.13%25%20for%20the%20ROI-to-full%20reconstruction%0Avolume%20ratio%20and%20a%200.88234%20average%20perspective%20directivity.%20Furthermore%2C%20the%0Aplanned%20motion%20trajectory%20exhibits%20better%20perceiving%20coverage%20toward%20the%0Atarget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16507v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-aware%20Next-Best-View%20for%20Multi-DoFs%20Mobile%20System%20in%0A%20%20Search-and-Acquisition%20based%20Visual%20Perception&entry.906535625=Xiaotong%20Yu%20and%20Chang-Wen%20Chen&entry.1292438233=%20%20Efficient%20visual%20perception%20using%20mobile%20systems%20is%20crucial%2C%20particularly%20in%0Aunknown%20environments%20such%20as%20search%20and%20rescue%20operations%2C%20where%20swift%20and%0Acomprehensive%20perception%20of%20objects%20of%20interest%20is%20essential.%20In%20such%0Areal-world%20applications%2C%20objects%20of%20interest%20are%20often%20situated%20in%20complex%0Aenvironments%2C%20making%20the%20selection%20of%20the%20%27Next%20Best%27%20view%20based%20solely%20on%0Amaximizing%20visibility%20gain%20suboptimal.%20Semantics%2C%20providing%20a%20higher-level%0Ainterpretation%20of%20perception%2C%20should%20significantly%20contribute%20to%20the%20selection%0Aof%20the%20next%20viewpoint%20for%20various%20perception%20tasks.%20In%20this%20study%2C%20we%20formulate%0Aa%20novel%20information%20gain%20that%20integrates%20both%20visibility%20gain%20and%20semantic%20gain%0Ain%20a%20unified%20form%20to%20select%20the%20semantic-aware%20Next-Best-View.%20Additionally%2C%20we%0Adesign%20an%20adaptive%20strategy%20with%20termination%20criterion%20to%20support%20a%20two-stage%0Asearch-and-acquisition%20manoeuvre%20on%20multiple%20objects%20of%20interest%20aided%20by%20a%0Amulti-degree-of-freedoms%20%28Multi-DoFs%29%20mobile%20system.%20Several%20semantically%0Arelevant%20reconstruction%20metrics%2C%20including%20perspective%20directivity%20and%20region%0Aof%20interest%20%28ROI%29-to-full%20reconstruction%20volume%20ratio%2C%20are%20introduced%20to%0Aevaluate%20the%20performance%20of%20the%20proposed%20approach.%20Simulation%20experiments%0Ademonstrate%20the%20advantages%20of%20the%20proposed%20approach%20over%20existing%20methods%2C%0Aachieving%20improvements%20of%20up%20to%2027.13%25%20for%20the%20ROI-to-full%20reconstruction%0Avolume%20ratio%20and%20a%200.88234%20average%20perspective%20directivity.%20Furthermore%2C%20the%0Aplanned%20motion%20trajectory%20exhibits%20better%20perceiving%20coverage%20toward%20the%0Atarget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16507v1&entry.124074799=Read"},
{"title": "TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose\n  Representation", "author": "Sai Kumar Dwivedi and Yu Sun and Priyanka Patel and Yao Feng and Michael J. Black", "abstract": "  We address the problem of regressing 3D human pose and shape from a single\nimage, with a focus on 3D accuracy. The current best methods leverage large\ndatasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust\nperformance. With such methods, we observe a paradoxical decline in 3D pose\naccuracy with increasing 2D accuracy. This is caused by biases in the p-GT and\nthe use of an approximate camera projection model. We quantify the error\ninduced by current camera models and show that fitting 2D keypoints and p-GT\naccurately causes incorrect 3D poses. Our analysis defines the invalid\ndistances within which minimizing 2D and p-GT losses is detrimental. We use\nthis to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) that\npenalizes gross 2D and p-GT losses but not smaller ones. With such a loss,\nthere are many 3D poses that could equally explain the 2D evidence. To reduce\nthis ambiguity we need a prior over valid human poses but such priors can\nintroduce unwanted bias. To address this, we exploit a tokenized representation\nof human pose and reformulate the problem as token prediction. This restricts\nthe estimated poses to the space of valid poses, effectively providing a\nuniform prior. Extensive experiments on the EMDB and 3DPW datasets show that\nour reformulated keypoint loss and tokenization allows us to train on\nin-the-wild data while improving 3D accuracy over the state-of-the-art. Our\nmodels and code are available for research at https://tokenhmr.is.tue.mpg.de.\n", "link": "http://arxiv.org/abs/2404.16752v1", "date": "2024-04-25", "relevancy": 2.2536, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5785}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5558}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TokenHMR%3A%20Advancing%20Human%20Mesh%20Recovery%20with%20a%20Tokenized%20Pose%0A%20%20Representation&body=Title%3A%20TokenHMR%3A%20Advancing%20Human%20Mesh%20Recovery%20with%20a%20Tokenized%20Pose%0A%20%20Representation%0AAuthor%3A%20Sai%20Kumar%20Dwivedi%20and%20Yu%20Sun%20and%20Priyanka%20Patel%20and%20Yao%20Feng%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20regressing%203D%20human%20pose%20and%20shape%20from%20a%20single%0Aimage%2C%20with%20a%20focus%20on%203D%20accuracy.%20The%20current%20best%20methods%20leverage%20large%0Adatasets%20of%203D%20pseudo-ground-truth%20%28p-GT%29%20and%202D%20keypoints%2C%20leading%20to%20robust%0Aperformance.%20With%20such%20methods%2C%20we%20observe%20a%20paradoxical%20decline%20in%203D%20pose%0Aaccuracy%20with%20increasing%202D%20accuracy.%20This%20is%20caused%20by%20biases%20in%20the%20p-GT%20and%0Athe%20use%20of%20an%20approximate%20camera%20projection%20model.%20We%20quantify%20the%20error%0Ainduced%20by%20current%20camera%20models%20and%20show%20that%20fitting%202D%20keypoints%20and%20p-GT%0Aaccurately%20causes%20incorrect%203D%20poses.%20Our%20analysis%20defines%20the%20invalid%0Adistances%20within%20which%20minimizing%202D%20and%20p-GT%20losses%20is%20detrimental.%20We%20use%0Athis%20to%20formulate%20a%20new%20loss%20Threshold-Adaptive%20Loss%20Scaling%20%28TALS%29%20that%0Apenalizes%20gross%202D%20and%20p-GT%20losses%20but%20not%20smaller%20ones.%20With%20such%20a%20loss%2C%0Athere%20are%20many%203D%20poses%20that%20could%20equally%20explain%20the%202D%20evidence.%20To%20reduce%0Athis%20ambiguity%20we%20need%20a%20prior%20over%20valid%20human%20poses%20but%20such%20priors%20can%0Aintroduce%20unwanted%20bias.%20To%20address%20this%2C%20we%20exploit%20a%20tokenized%20representation%0Aof%20human%20pose%20and%20reformulate%20the%20problem%20as%20token%20prediction.%20This%20restricts%0Athe%20estimated%20poses%20to%20the%20space%20of%20valid%20poses%2C%20effectively%20providing%20a%0Auniform%20prior.%20Extensive%20experiments%20on%20the%20EMDB%20and%203DPW%20datasets%20show%20that%0Aour%20reformulated%20keypoint%20loss%20and%20tokenization%20allows%20us%20to%20train%20on%0Ain-the-wild%20data%20while%20improving%203D%20accuracy%20over%20the%20state-of-the-art.%20Our%0Amodels%20and%20code%20are%20available%20for%20research%20at%20https%3A//tokenhmr.is.tue.mpg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16752v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenHMR%3A%20Advancing%20Human%20Mesh%20Recovery%20with%20a%20Tokenized%20Pose%0A%20%20Representation&entry.906535625=Sai%20Kumar%20Dwivedi%20and%20Yu%20Sun%20and%20Priyanka%20Patel%20and%20Yao%20Feng%20and%20Michael%20J.%20Black&entry.1292438233=%20%20We%20address%20the%20problem%20of%20regressing%203D%20human%20pose%20and%20shape%20from%20a%20single%0Aimage%2C%20with%20a%20focus%20on%203D%20accuracy.%20The%20current%20best%20methods%20leverage%20large%0Adatasets%20of%203D%20pseudo-ground-truth%20%28p-GT%29%20and%202D%20keypoints%2C%20leading%20to%20robust%0Aperformance.%20With%20such%20methods%2C%20we%20observe%20a%20paradoxical%20decline%20in%203D%20pose%0Aaccuracy%20with%20increasing%202D%20accuracy.%20This%20is%20caused%20by%20biases%20in%20the%20p-GT%20and%0Athe%20use%20of%20an%20approximate%20camera%20projection%20model.%20We%20quantify%20the%20error%0Ainduced%20by%20current%20camera%20models%20and%20show%20that%20fitting%202D%20keypoints%20and%20p-GT%0Aaccurately%20causes%20incorrect%203D%20poses.%20Our%20analysis%20defines%20the%20invalid%0Adistances%20within%20which%20minimizing%202D%20and%20p-GT%20losses%20is%20detrimental.%20We%20use%0Athis%20to%20formulate%20a%20new%20loss%20Threshold-Adaptive%20Loss%20Scaling%20%28TALS%29%20that%0Apenalizes%20gross%202D%20and%20p-GT%20losses%20but%20not%20smaller%20ones.%20With%20such%20a%20loss%2C%0Athere%20are%20many%203D%20poses%20that%20could%20equally%20explain%20the%202D%20evidence.%20To%20reduce%0Athis%20ambiguity%20we%20need%20a%20prior%20over%20valid%20human%20poses%20but%20such%20priors%20can%0Aintroduce%20unwanted%20bias.%20To%20address%20this%2C%20we%20exploit%20a%20tokenized%20representation%0Aof%20human%20pose%20and%20reformulate%20the%20problem%20as%20token%20prediction.%20This%20restricts%0Athe%20estimated%20poses%20to%20the%20space%20of%20valid%20poses%2C%20effectively%20providing%20a%0Auniform%20prior.%20Extensive%20experiments%20on%20the%20EMDB%20and%203DPW%20datasets%20show%20that%0Aour%20reformulated%20keypoint%20loss%20and%20tokenization%20allows%20us%20to%20train%20on%0Ain-the-wild%20data%20while%20improving%203D%20accuracy%20over%20the%20state-of-the-art.%20Our%0Amodels%20and%20code%20are%20available%20for%20research%20at%20https%3A//tokenhmr.is.tue.mpg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16752v1&entry.124074799=Read"},
{"title": "Legal Aspects for Software Developers Interested in Generative AI\n  Applications", "author": "Steffen Herbold and Brian Valerius and Anamaria Mojica-Hanke and Isabella Lex and Joel Mittel", "abstract": "  Recent successes in Generative Artificial Intelligence (GenAI) have led to\nnew technologies capable of generating high-quality code, natural language, and\nimages. The next step is to integrate GenAI technology into products, a task\ntypically conducted by software developers. Such product development always\ncomes with a certain risk of liability. Within this article, we want to shed\nlight on the current state of two such risks: data protection and copyright.\nBoth aspects are crucial for GenAI. This technology deals with data for both\nmodel training and generated output. We summarize key aspects regarding our\ncurrent knowledge that every software developer involved in product development\nusing GenAI should be aware of to avoid critical mistakes that may expose them\nto liability claims.\n", "link": "http://arxiv.org/abs/2404.16630v1", "date": "2024-04-25", "relevancy": 2.2533, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4956}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4295}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.427}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Legal%20Aspects%20for%20Software%20Developers%20Interested%20in%20Generative%20AI%0A%20%20Applications&body=Title%3A%20Legal%20Aspects%20for%20Software%20Developers%20Interested%20in%20Generative%20AI%0A%20%20Applications%0AAuthor%3A%20Steffen%20Herbold%20and%20Brian%20Valerius%20and%20Anamaria%20Mojica-Hanke%20and%20Isabella%20Lex%20and%20Joel%20Mittel%0AAbstract%3A%20%20%20Recent%20successes%20in%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20have%20led%20to%0Anew%20technologies%20capable%20of%20generating%20high-quality%20code%2C%20natural%20language%2C%20and%0Aimages.%20The%20next%20step%20is%20to%20integrate%20GenAI%20technology%20into%20products%2C%20a%20task%0Atypically%20conducted%20by%20software%20developers.%20Such%20product%20development%20always%0Acomes%20with%20a%20certain%20risk%20of%20liability.%20Within%20this%20article%2C%20we%20want%20to%20shed%0Alight%20on%20the%20current%20state%20of%20two%20such%20risks%3A%20data%20protection%20and%20copyright.%0ABoth%20aspects%20are%20crucial%20for%20GenAI.%20This%20technology%20deals%20with%20data%20for%20both%0Amodel%20training%20and%20generated%20output.%20We%20summarize%20key%20aspects%20regarding%20our%0Acurrent%20knowledge%20that%20every%20software%20developer%20involved%20in%20product%20development%0Ausing%20GenAI%20should%20be%20aware%20of%20to%20avoid%20critical%20mistakes%20that%20may%20expose%20them%0Ato%20liability%20claims.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16630v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Legal%20Aspects%20for%20Software%20Developers%20Interested%20in%20Generative%20AI%0A%20%20Applications&entry.906535625=Steffen%20Herbold%20and%20Brian%20Valerius%20and%20Anamaria%20Mojica-Hanke%20and%20Isabella%20Lex%20and%20Joel%20Mittel&entry.1292438233=%20%20Recent%20successes%20in%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20have%20led%20to%0Anew%20technologies%20capable%20of%20generating%20high-quality%20code%2C%20natural%20language%2C%20and%0Aimages.%20The%20next%20step%20is%20to%20integrate%20GenAI%20technology%20into%20products%2C%20a%20task%0Atypically%20conducted%20by%20software%20developers.%20Such%20product%20development%20always%0Acomes%20with%20a%20certain%20risk%20of%20liability.%20Within%20this%20article%2C%20we%20want%20to%20shed%0Alight%20on%20the%20current%20state%20of%20two%20such%20risks%3A%20data%20protection%20and%20copyright.%0ABoth%20aspects%20are%20crucial%20for%20GenAI.%20This%20technology%20deals%20with%20data%20for%20both%0Amodel%20training%20and%20generated%20output.%20We%20summarize%20key%20aspects%20regarding%20our%0Acurrent%20knowledge%20that%20every%20software%20developer%20involved%20in%20product%20development%0Ausing%20GenAI%20should%20be%20aware%20of%20to%20avoid%20critical%20mistakes%20that%20may%20expose%20them%0Ato%20liability%20claims.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16630v1&entry.124074799=Read"},
{"title": "Multi-view Cardiac Image Segmentation via Trans-Dimensional Priors", "author": "Abbas Khan and Muhammad Asad and Martin Benning and Caroline Roney and Gregory Slabaugh", "abstract": "  We propose a novel multi-stage trans-dimensional architecture for multi-view\ncardiac image segmentation. Our method exploits the relationship between\nlong-axis (2D) and short-axis (3D) magnetic resonance (MR) images to perform a\nsequential 3D-to-2D-to-3D segmentation, segmenting the long-axis and short-axis\nimages. In the first stage, 3D segmentation is performed using the short-axis\nimage, and the prediction is transformed to the long-axis view and used as a\nsegmentation prior in the next stage. In the second step, the heart region is\nlocalized and cropped around the segmentation prior using a Heart Localization\nand Cropping (HLC) module, focusing the subsequent model on the heart region of\nthe image, where a 2D segmentation is performed. Similarly, we transform the\nlong-axis prediction to the short-axis view, localize and crop the heart region\nand again perform a 3D segmentation to refine the initial short-axis\nsegmentation. We evaluate our proposed method on the Multi-Disease, Multi-View\n& Multi-Center Right Ventricular Segmentation in Cardiac MRI (M&Ms-2) dataset,\nwhere our method outperforms state-of-the-art methods in segmenting cardiac\nregions of interest in both short-axis and long-axis images. The pre-trained\nmodels, source code, and implementation details will be publicly available.\n", "link": "http://arxiv.org/abs/2404.16708v1", "date": "2024-04-25", "relevancy": 2.2474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5498}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5273}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Cardiac%20Image%20Segmentation%20via%20Trans-Dimensional%20Priors&body=Title%3A%20Multi-view%20Cardiac%20Image%20Segmentation%20via%20Trans-Dimensional%20Priors%0AAuthor%3A%20Abbas%20Khan%20and%20Muhammad%20Asad%20and%20Martin%20Benning%20and%20Caroline%20Roney%20and%20Gregory%20Slabaugh%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20multi-stage%20trans-dimensional%20architecture%20for%20multi-view%0Acardiac%20image%20segmentation.%20Our%20method%20exploits%20the%20relationship%20between%0Along-axis%20%282D%29%20and%20short-axis%20%283D%29%20magnetic%20resonance%20%28MR%29%20images%20to%20perform%20a%0Asequential%203D-to-2D-to-3D%20segmentation%2C%20segmenting%20the%20long-axis%20and%20short-axis%0Aimages.%20In%20the%20first%20stage%2C%203D%20segmentation%20is%20performed%20using%20the%20short-axis%0Aimage%2C%20and%20the%20prediction%20is%20transformed%20to%20the%20long-axis%20view%20and%20used%20as%20a%0Asegmentation%20prior%20in%20the%20next%20stage.%20In%20the%20second%20step%2C%20the%20heart%20region%20is%0Alocalized%20and%20cropped%20around%20the%20segmentation%20prior%20using%20a%20Heart%20Localization%0Aand%20Cropping%20%28HLC%29%20module%2C%20focusing%20the%20subsequent%20model%20on%20the%20heart%20region%20of%0Athe%20image%2C%20where%20a%202D%20segmentation%20is%20performed.%20Similarly%2C%20we%20transform%20the%0Along-axis%20prediction%20to%20the%20short-axis%20view%2C%20localize%20and%20crop%20the%20heart%20region%0Aand%20again%20perform%20a%203D%20segmentation%20to%20refine%20the%20initial%20short-axis%0Asegmentation.%20We%20evaluate%20our%20proposed%20method%20on%20the%20Multi-Disease%2C%20Multi-View%0A%26%20Multi-Center%20Right%20Ventricular%20Segmentation%20in%20Cardiac%20MRI%20%28M%26Ms-2%29%20dataset%2C%0Awhere%20our%20method%20outperforms%20state-of-the-art%20methods%20in%20segmenting%20cardiac%0Aregions%20of%20interest%20in%20both%20short-axis%20and%20long-axis%20images.%20The%20pre-trained%0Amodels%2C%20source%20code%2C%20and%20implementation%20details%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16708v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Cardiac%20Image%20Segmentation%20via%20Trans-Dimensional%20Priors&entry.906535625=Abbas%20Khan%20and%20Muhammad%20Asad%20and%20Martin%20Benning%20and%20Caroline%20Roney%20and%20Gregory%20Slabaugh&entry.1292438233=%20%20We%20propose%20a%20novel%20multi-stage%20trans-dimensional%20architecture%20for%20multi-view%0Acardiac%20image%20segmentation.%20Our%20method%20exploits%20the%20relationship%20between%0Along-axis%20%282D%29%20and%20short-axis%20%283D%29%20magnetic%20resonance%20%28MR%29%20images%20to%20perform%20a%0Asequential%203D-to-2D-to-3D%20segmentation%2C%20segmenting%20the%20long-axis%20and%20short-axis%0Aimages.%20In%20the%20first%20stage%2C%203D%20segmentation%20is%20performed%20using%20the%20short-axis%0Aimage%2C%20and%20the%20prediction%20is%20transformed%20to%20the%20long-axis%20view%20and%20used%20as%20a%0Asegmentation%20prior%20in%20the%20next%20stage.%20In%20the%20second%20step%2C%20the%20heart%20region%20is%0Alocalized%20and%20cropped%20around%20the%20segmentation%20prior%20using%20a%20Heart%20Localization%0Aand%20Cropping%20%28HLC%29%20module%2C%20focusing%20the%20subsequent%20model%20on%20the%20heart%20region%20of%0Athe%20image%2C%20where%20a%202D%20segmentation%20is%20performed.%20Similarly%2C%20we%20transform%20the%0Along-axis%20prediction%20to%20the%20short-axis%20view%2C%20localize%20and%20crop%20the%20heart%20region%0Aand%20again%20perform%20a%203D%20segmentation%20to%20refine%20the%20initial%20short-axis%0Asegmentation.%20We%20evaluate%20our%20proposed%20method%20on%20the%20Multi-Disease%2C%20Multi-View%0A%26%20Multi-Center%20Right%20Ventricular%20Segmentation%20in%20Cardiac%20MRI%20%28M%26Ms-2%29%20dataset%2C%0Awhere%20our%20method%20outperforms%20state-of-the-art%20methods%20in%20segmenting%20cardiac%0Aregions%20of%20interest%20in%20both%20short-axis%20and%20long-axis%20images.%20The%20pre-trained%0Amodels%2C%20source%20code%2C%20and%20implementation%20details%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16708v1&entry.124074799=Read"},
{"title": "Leveraging Pretrained Latent Representations for Few-Shot Imitation\n  Learning on a Dexterous Robotic Hand", "author": "Davide Liconti and Yasunori Toshimitsu and Robert Katzschmann", "abstract": "  In the context of imitation learning applied to dexterous robotic hands, the\nhigh complexity of the systems makes learning complex manipulation tasks\nchallenging. However, the numerous datasets depicting human hands in various\ndifferent tasks could provide us with better knowledge regarding human hand\nmotion. We propose a method to leverage multiple large-scale task-agnostic\ndatasets to obtain latent representations that effectively encode motion\nsubtrajectories that we included in a transformer-based behavior cloning\nmethod. Our results demonstrate that employing latent representations yields\nenhanced performance compared to conventional behavior cloning methods,\nparticularly regarding resilience to errors and noise in perception and\nproprioception. Furthermore, the proposed approach solely relies on human\ndemonstrations, eliminating the need for teleoperation and, therefore,\naccelerating the data acquisition process. Accurate inverse kinematics for\nfingertip retargeting ensures precise transfer from human hand data to the\nrobot, facilitating effective learning and deployment of manipulation policies.\nFinally, the trained policies have been successfully transferred to a\nreal-world 23Dof robotic system.\n", "link": "http://arxiv.org/abs/2404.16483v1", "date": "2024-04-25", "relevancy": 2.2346, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5475}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Pretrained%20Latent%20Representations%20for%20Few-Shot%20Imitation%0A%20%20Learning%20on%20a%20Dexterous%20Robotic%20Hand&body=Title%3A%20Leveraging%20Pretrained%20Latent%20Representations%20for%20Few-Shot%20Imitation%0A%20%20Learning%20on%20a%20Dexterous%20Robotic%20Hand%0AAuthor%3A%20Davide%20Liconti%20and%20Yasunori%20Toshimitsu%20and%20Robert%20Katzschmann%0AAbstract%3A%20%20%20In%20the%20context%20of%20imitation%20learning%20applied%20to%20dexterous%20robotic%20hands%2C%20the%0Ahigh%20complexity%20of%20the%20systems%20makes%20learning%20complex%20manipulation%20tasks%0Achallenging.%20However%2C%20the%20numerous%20datasets%20depicting%20human%20hands%20in%20various%0Adifferent%20tasks%20could%20provide%20us%20with%20better%20knowledge%20regarding%20human%20hand%0Amotion.%20We%20propose%20a%20method%20to%20leverage%20multiple%20large-scale%20task-agnostic%0Adatasets%20to%20obtain%20latent%20representations%20that%20effectively%20encode%20motion%0Asubtrajectories%20that%20we%20included%20in%20a%20transformer-based%20behavior%20cloning%0Amethod.%20Our%20results%20demonstrate%20that%20employing%20latent%20representations%20yields%0Aenhanced%20performance%20compared%20to%20conventional%20behavior%20cloning%20methods%2C%0Aparticularly%20regarding%20resilience%20to%20errors%20and%20noise%20in%20perception%20and%0Aproprioception.%20Furthermore%2C%20the%20proposed%20approach%20solely%20relies%20on%20human%0Ademonstrations%2C%20eliminating%20the%20need%20for%20teleoperation%20and%2C%20therefore%2C%0Aaccelerating%20the%20data%20acquisition%20process.%20Accurate%20inverse%20kinematics%20for%0Afingertip%20retargeting%20ensures%20precise%20transfer%20from%20human%20hand%20data%20to%20the%0Arobot%2C%20facilitating%20effective%20learning%20and%20deployment%20of%20manipulation%20policies.%0AFinally%2C%20the%20trained%20policies%20have%20been%20successfully%20transferred%20to%20a%0Areal-world%2023Dof%20robotic%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16483v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Pretrained%20Latent%20Representations%20for%20Few-Shot%20Imitation%0A%20%20Learning%20on%20a%20Dexterous%20Robotic%20Hand&entry.906535625=Davide%20Liconti%20and%20Yasunori%20Toshimitsu%20and%20Robert%20Katzschmann&entry.1292438233=%20%20In%20the%20context%20of%20imitation%20learning%20applied%20to%20dexterous%20robotic%20hands%2C%20the%0Ahigh%20complexity%20of%20the%20systems%20makes%20learning%20complex%20manipulation%20tasks%0Achallenging.%20However%2C%20the%20numerous%20datasets%20depicting%20human%20hands%20in%20various%0Adifferent%20tasks%20could%20provide%20us%20with%20better%20knowledge%20regarding%20human%20hand%0Amotion.%20We%20propose%20a%20method%20to%20leverage%20multiple%20large-scale%20task-agnostic%0Adatasets%20to%20obtain%20latent%20representations%20that%20effectively%20encode%20motion%0Asubtrajectories%20that%20we%20included%20in%20a%20transformer-based%20behavior%20cloning%0Amethod.%20Our%20results%20demonstrate%20that%20employing%20latent%20representations%20yields%0Aenhanced%20performance%20compared%20to%20conventional%20behavior%20cloning%20methods%2C%0Aparticularly%20regarding%20resilience%20to%20errors%20and%20noise%20in%20perception%20and%0Aproprioception.%20Furthermore%2C%20the%20proposed%20approach%20solely%20relies%20on%20human%0Ademonstrations%2C%20eliminating%20the%20need%20for%20teleoperation%20and%2C%20therefore%2C%0Aaccelerating%20the%20data%20acquisition%20process.%20Accurate%20inverse%20kinematics%20for%0Afingertip%20retargeting%20ensures%20precise%20transfer%20from%20human%20hand%20data%20to%20the%0Arobot%2C%20facilitating%20effective%20learning%20and%20deployment%20of%20manipulation%20policies.%0AFinally%2C%20the%20trained%20policies%20have%20been%20successfully%20transferred%20to%20a%0Areal-world%2023Dof%20robotic%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16483v1&entry.124074799=Read"},
{"title": "Made to Order: Discovering monotonic temporal changes via\n  self-supervised video ordering", "author": "Charig Yang and Weidi Xie and Andrew Zisserman", "abstract": "  Our objective is to discover and localize monotonic temporal changes in a\nsequence of images. To achieve this, we exploit a simple proxy task of ordering\na shuffled image sequence, with `time' serving as a supervisory signal since\nonly changes that are monotonic with time can give rise to the correct\nordering. We also introduce a flexible transformer-based model for\ngeneral-purpose ordering of image sequences of arbitrary length with built-in\nattribution maps. After training, the model successfully discovers and\nlocalizes monotonic changes while ignoring cyclic and stochastic ones. We\ndemonstrate applications of the model in multiple video settings covering\ndifferent scene and object types, discovering both object-level and\nenvironmental changes in unseen sequences. We also demonstrate that the\nattention-based attribution maps function as effective prompts for segmenting\nthe changing regions, and that the learned representations can be used for\ndownstream applications. Finally, we show that the model achieves the state of\nthe art on standard benchmarks for ordering a set of images.\n", "link": "http://arxiv.org/abs/2404.16828v1", "date": "2024-04-25", "relevancy": 2.2168, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5759}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5705}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5293}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Made%20to%20Order%3A%20Discovering%20monotonic%20temporal%20changes%20via%0A%20%20self-supervised%20video%20ordering&body=Title%3A%20Made%20to%20Order%3A%20Discovering%20monotonic%20temporal%20changes%20via%0A%20%20self-supervised%20video%20ordering%0AAuthor%3A%20Charig%20Yang%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Our%20objective%20is%20to%20discover%20and%20localize%20monotonic%20temporal%20changes%20in%20a%0Asequence%20of%20images.%20To%20achieve%20this%2C%20we%20exploit%20a%20simple%20proxy%20task%20of%20ordering%0Aa%20shuffled%20image%20sequence%2C%20with%20%60time%27%20serving%20as%20a%20supervisory%20signal%20since%0Aonly%20changes%20that%20are%20monotonic%20with%20time%20can%20give%20rise%20to%20the%20correct%0Aordering.%20We%20also%20introduce%20a%20flexible%20transformer-based%20model%20for%0Ageneral-purpose%20ordering%20of%20image%20sequences%20of%20arbitrary%20length%20with%20built-in%0Aattribution%20maps.%20After%20training%2C%20the%20model%20successfully%20discovers%20and%0Alocalizes%20monotonic%20changes%20while%20ignoring%20cyclic%20and%20stochastic%20ones.%20We%0Ademonstrate%20applications%20of%20the%20model%20in%20multiple%20video%20settings%20covering%0Adifferent%20scene%20and%20object%20types%2C%20discovering%20both%20object-level%20and%0Aenvironmental%20changes%20in%20unseen%20sequences.%20We%20also%20demonstrate%20that%20the%0Aattention-based%20attribution%20maps%20function%20as%20effective%20prompts%20for%20segmenting%0Athe%20changing%20regions%2C%20and%20that%20the%20learned%20representations%20can%20be%20used%20for%0Adownstream%20applications.%20Finally%2C%20we%20show%20that%20the%20model%20achieves%20the%20state%20of%0Athe%20art%20on%20standard%20benchmarks%20for%20ordering%20a%20set%20of%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16828v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Made%20to%20Order%3A%20Discovering%20monotonic%20temporal%20changes%20via%0A%20%20self-supervised%20video%20ordering&entry.906535625=Charig%20Yang%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Our%20objective%20is%20to%20discover%20and%20localize%20monotonic%20temporal%20changes%20in%20a%0Asequence%20of%20images.%20To%20achieve%20this%2C%20we%20exploit%20a%20simple%20proxy%20task%20of%20ordering%0Aa%20shuffled%20image%20sequence%2C%20with%20%60time%27%20serving%20as%20a%20supervisory%20signal%20since%0Aonly%20changes%20that%20are%20monotonic%20with%20time%20can%20give%20rise%20to%20the%20correct%0Aordering.%20We%20also%20introduce%20a%20flexible%20transformer-based%20model%20for%0Ageneral-purpose%20ordering%20of%20image%20sequences%20of%20arbitrary%20length%20with%20built-in%0Aattribution%20maps.%20After%20training%2C%20the%20model%20successfully%20discovers%20and%0Alocalizes%20monotonic%20changes%20while%20ignoring%20cyclic%20and%20stochastic%20ones.%20We%0Ademonstrate%20applications%20of%20the%20model%20in%20multiple%20video%20settings%20covering%0Adifferent%20scene%20and%20object%20types%2C%20discovering%20both%20object-level%20and%0Aenvironmental%20changes%20in%20unseen%20sequences.%20We%20also%20demonstrate%20that%20the%0Aattention-based%20attribution%20maps%20function%20as%20effective%20prompts%20for%20segmenting%0Athe%20changing%20regions%2C%20and%20that%20the%20learned%20representations%20can%20be%20used%20for%0Adownstream%20applications.%20Finally%2C%20we%20show%20that%20the%20model%20achieves%20the%20state%20of%0Athe%20art%20on%20standard%20benchmarks%20for%20ordering%20a%20set%20of%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16828v1&entry.124074799=Read"},
{"title": "Pix2HDR -- A pixel-wise acquisition and deep learning-based synthesis\n  approach for high-speed HDR videos", "author": "Caixin Wang and Jie Zhang and Matthew A. Wilson and Ralph Etienne-Cummings", "abstract": "  Accurately capturing dynamic scenes with wide-ranging motion and light\nintensity is crucial for many vision applications. However, acquiring\nhigh-speed high dynamic range (HDR) video is challenging because the camera's\nframe rate restricts its dynamic range. Existing methods sacrifice speed to\nacquire multi-exposure frames. Yet, misaligned motion in these frames can still\npose complications for HDR fusion algorithms, resulting in artifacts. Instead\nof frame-based exposures, we sample the videos using individual pixels at\nvarying exposures and phase offsets. Implemented on a monochrome pixel-wise\nprogrammable image sensor, our sampling pattern simultaneously captures fast\nmotion at a high dynamic range. We then transform pixel-wise outputs into an\nHDR video using end-to-end learned weights from deep neural networks, achieving\nhigh spatiotemporal resolution with minimized motion blurring. We demonstrate\naliasing-free HDR video acquisition at 1000 FPS, resolving fast motion under\nlow-light conditions and against bright backgrounds - both challenging\nconditions for conventional cameras. By combining the versatility of pixel-wise\nsampling patterns with the strength of deep neural networks at decoding complex\nscenes, our method greatly enhances the vision system's adaptability and\nperformance in dynamic conditions.\n", "link": "http://arxiv.org/abs/2310.16139v2", "date": "2024-04-25", "relevancy": 2.2152, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5852}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5614}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5337}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pix2HDR%20--%20A%20pixel-wise%20acquisition%20and%20deep%20learning-based%20synthesis%0A%20%20approach%20for%20high-speed%20HDR%20videos&body=Title%3A%20Pix2HDR%20--%20A%20pixel-wise%20acquisition%20and%20deep%20learning-based%20synthesis%0A%20%20approach%20for%20high-speed%20HDR%20videos%0AAuthor%3A%20Caixin%20Wang%20and%20Jie%20Zhang%20and%20Matthew%20A.%20Wilson%20and%20Ralph%20Etienne-Cummings%0AAbstract%3A%20%20%20Accurately%20capturing%20dynamic%20scenes%20with%20wide-ranging%20motion%20and%20light%0Aintensity%20is%20crucial%20for%20many%20vision%20applications.%20However%2C%20acquiring%0Ahigh-speed%20high%20dynamic%20range%20%28HDR%29%20video%20is%20challenging%20because%20the%20camera%27s%0Aframe%20rate%20restricts%20its%20dynamic%20range.%20Existing%20methods%20sacrifice%20speed%20to%0Aacquire%20multi-exposure%20frames.%20Yet%2C%20misaligned%20motion%20in%20these%20frames%20can%20still%0Apose%20complications%20for%20HDR%20fusion%20algorithms%2C%20resulting%20in%20artifacts.%20Instead%0Aof%20frame-based%20exposures%2C%20we%20sample%20the%20videos%20using%20individual%20pixels%20at%0Avarying%20exposures%20and%20phase%20offsets.%20Implemented%20on%20a%20monochrome%20pixel-wise%0Aprogrammable%20image%20sensor%2C%20our%20sampling%20pattern%20simultaneously%20captures%20fast%0Amotion%20at%20a%20high%20dynamic%20range.%20We%20then%20transform%20pixel-wise%20outputs%20into%20an%0AHDR%20video%20using%20end-to-end%20learned%20weights%20from%20deep%20neural%20networks%2C%20achieving%0Ahigh%20spatiotemporal%20resolution%20with%20minimized%20motion%20blurring.%20We%20demonstrate%0Aaliasing-free%20HDR%20video%20acquisition%20at%201000%20FPS%2C%20resolving%20fast%20motion%20under%0Alow-light%20conditions%20and%20against%20bright%20backgrounds%20-%20both%20challenging%0Aconditions%20for%20conventional%20cameras.%20By%20combining%20the%20versatility%20of%20pixel-wise%0Asampling%20patterns%20with%20the%20strength%20of%20deep%20neural%20networks%20at%20decoding%20complex%0Ascenes%2C%20our%20method%20greatly%20enhances%20the%20vision%20system%27s%20adaptability%20and%0Aperformance%20in%20dynamic%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16139v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pix2HDR%20--%20A%20pixel-wise%20acquisition%20and%20deep%20learning-based%20synthesis%0A%20%20approach%20for%20high-speed%20HDR%20videos&entry.906535625=Caixin%20Wang%20and%20Jie%20Zhang%20and%20Matthew%20A.%20Wilson%20and%20Ralph%20Etienne-Cummings&entry.1292438233=%20%20Accurately%20capturing%20dynamic%20scenes%20with%20wide-ranging%20motion%20and%20light%0Aintensity%20is%20crucial%20for%20many%20vision%20applications.%20However%2C%20acquiring%0Ahigh-speed%20high%20dynamic%20range%20%28HDR%29%20video%20is%20challenging%20because%20the%20camera%27s%0Aframe%20rate%20restricts%20its%20dynamic%20range.%20Existing%20methods%20sacrifice%20speed%20to%0Aacquire%20multi-exposure%20frames.%20Yet%2C%20misaligned%20motion%20in%20these%20frames%20can%20still%0Apose%20complications%20for%20HDR%20fusion%20algorithms%2C%20resulting%20in%20artifacts.%20Instead%0Aof%20frame-based%20exposures%2C%20we%20sample%20the%20videos%20using%20individual%20pixels%20at%0Avarying%20exposures%20and%20phase%20offsets.%20Implemented%20on%20a%20monochrome%20pixel-wise%0Aprogrammable%20image%20sensor%2C%20our%20sampling%20pattern%20simultaneously%20captures%20fast%0Amotion%20at%20a%20high%20dynamic%20range.%20We%20then%20transform%20pixel-wise%20outputs%20into%20an%0AHDR%20video%20using%20end-to-end%20learned%20weights%20from%20deep%20neural%20networks%2C%20achieving%0Ahigh%20spatiotemporal%20resolution%20with%20minimized%20motion%20blurring.%20We%20demonstrate%0Aaliasing-free%20HDR%20video%20acquisition%20at%201000%20FPS%2C%20resolving%20fast%20motion%20under%0Alow-light%20conditions%20and%20against%20bright%20backgrounds%20-%20both%20challenging%0Aconditions%20for%20conventional%20cameras.%20By%20combining%20the%20versatility%20of%20pixel-wise%0Asampling%20patterns%20with%20the%20strength%20of%20deep%20neural%20networks%20at%20decoding%20complex%0Ascenes%2C%20our%20method%20greatly%20enhances%20the%20vision%20system%27s%20adaptability%20and%0Aperformance%20in%20dynamic%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16139v2&entry.124074799=Read"},
{"title": "Semantic Positive Pairs for Enhancing Visual Representation Learning of\n  Instance Discrimination methods", "author": "Mohammad Alkhalefi and Georgios Leontidis and Mingjun Zhong", "abstract": "  Self-supervised learning algorithms (SSL) based on instance discrimination\nhave shown promising results, performing competitively or even outperforming\nsupervised learning counterparts in some downstream tasks. Such approaches\nemploy data augmentation to create two views of the same instance (i.e.,\npositive pairs) and encourage the model to learn good representations by\nattracting these views closer in the embedding space without collapsing to the\ntrivial solution. However, data augmentation is limited in representing\npositive pairs, and the repulsion process between the instances during\ncontrastive learning may discard important features for instances that have\nsimilar categories. To address this issue, we propose an approach to identify\nthose images with similar semantic content and treat them as positive\ninstances, thereby reducing the chance of discarding important features during\nrepresentation learning and increasing the richness of the latent\nrepresentation. Our approach is generic and could work with any self-supervised\ninstance discrimination frameworks such as MoCo and SimSiam. To evaluate our\nmethod, we run experiments on three benchmark datasets: ImageNet, STL-10 and\nCIFAR-10 with different instance discrimination SSL approaches. The\nexperimental results show that our approach consistently outperforms the\nbaseline methods across all three datasets; for instance, we improve upon the\nvanilla MoCo-v2 by 4.1% on ImageNet under a linear evaluation protocol over 800\nepochs. We also report results on semi-supervised learning, transfer learning\non downstream tasks, and object detection.\n", "link": "http://arxiv.org/abs/2306.16122v2", "date": "2024-04-25", "relevancy": 2.2123, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5594}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5254}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantic%20Positive%20Pairs%20for%20Enhancing%20Visual%20Representation%20Learning%20of%0A%20%20Instance%20Discrimination%20methods&body=Title%3A%20Semantic%20Positive%20Pairs%20for%20Enhancing%20Visual%20Representation%20Learning%20of%0A%20%20Instance%20Discrimination%20methods%0AAuthor%3A%20Mohammad%20Alkhalefi%20and%20Georgios%20Leontidis%20and%20Mingjun%20Zhong%0AAbstract%3A%20%20%20Self-supervised%20learning%20algorithms%20%28SSL%29%20based%20on%20instance%20discrimination%0Ahave%20shown%20promising%20results%2C%20performing%20competitively%20or%20even%20outperforming%0Asupervised%20learning%20counterparts%20in%20some%20downstream%20tasks.%20Such%20approaches%0Aemploy%20data%20augmentation%20to%20create%20two%20views%20of%20the%20same%20instance%20%28i.e.%2C%0Apositive%20pairs%29%20and%20encourage%20the%20model%20to%20learn%20good%20representations%20by%0Aattracting%20these%20views%20closer%20in%20the%20embedding%20space%20without%20collapsing%20to%20the%0Atrivial%20solution.%20However%2C%20data%20augmentation%20is%20limited%20in%20representing%0Apositive%20pairs%2C%20and%20the%20repulsion%20process%20between%20the%20instances%20during%0Acontrastive%20learning%20may%20discard%20important%20features%20for%20instances%20that%20have%0Asimilar%20categories.%20To%20address%20this%20issue%2C%20we%20propose%20an%20approach%20to%20identify%0Athose%20images%20with%20similar%20semantic%20content%20and%20treat%20them%20as%20positive%0Ainstances%2C%20thereby%20reducing%20the%20chance%20of%20discarding%20important%20features%20during%0Arepresentation%20learning%20and%20increasing%20the%20richness%20of%20the%20latent%0Arepresentation.%20Our%20approach%20is%20generic%20and%20could%20work%20with%20any%20self-supervised%0Ainstance%20discrimination%20frameworks%20such%20as%20MoCo%20and%20SimSiam.%20To%20evaluate%20our%0Amethod%2C%20we%20run%20experiments%20on%20three%20benchmark%20datasets%3A%20ImageNet%2C%20STL-10%20and%0ACIFAR-10%20with%20different%20instance%20discrimination%20SSL%20approaches.%20The%0Aexperimental%20results%20show%20that%20our%20approach%20consistently%20outperforms%20the%0Abaseline%20methods%20across%20all%20three%20datasets%3B%20for%20instance%2C%20we%20improve%20upon%20the%0Avanilla%20MoCo-v2%20by%204.1%25%20on%20ImageNet%20under%20a%20linear%20evaluation%20protocol%20over%20800%0Aepochs.%20We%20also%20report%20results%20on%20semi-supervised%20learning%2C%20transfer%20learning%0Aon%20downstream%20tasks%2C%20and%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.16122v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Positive%20Pairs%20for%20Enhancing%20Visual%20Representation%20Learning%20of%0A%20%20Instance%20Discrimination%20methods&entry.906535625=Mohammad%20Alkhalefi%20and%20Georgios%20Leontidis%20and%20Mingjun%20Zhong&entry.1292438233=%20%20Self-supervised%20learning%20algorithms%20%28SSL%29%20based%20on%20instance%20discrimination%0Ahave%20shown%20promising%20results%2C%20performing%20competitively%20or%20even%20outperforming%0Asupervised%20learning%20counterparts%20in%20some%20downstream%20tasks.%20Such%20approaches%0Aemploy%20data%20augmentation%20to%20create%20two%20views%20of%20the%20same%20instance%20%28i.e.%2C%0Apositive%20pairs%29%20and%20encourage%20the%20model%20to%20learn%20good%20representations%20by%0Aattracting%20these%20views%20closer%20in%20the%20embedding%20space%20without%20collapsing%20to%20the%0Atrivial%20solution.%20However%2C%20data%20augmentation%20is%20limited%20in%20representing%0Apositive%20pairs%2C%20and%20the%20repulsion%20process%20between%20the%20instances%20during%0Acontrastive%20learning%20may%20discard%20important%20features%20for%20instances%20that%20have%0Asimilar%20categories.%20To%20address%20this%20issue%2C%20we%20propose%20an%20approach%20to%20identify%0Athose%20images%20with%20similar%20semantic%20content%20and%20treat%20them%20as%20positive%0Ainstances%2C%20thereby%20reducing%20the%20chance%20of%20discarding%20important%20features%20during%0Arepresentation%20learning%20and%20increasing%20the%20richness%20of%20the%20latent%0Arepresentation.%20Our%20approach%20is%20generic%20and%20could%20work%20with%20any%20self-supervised%0Ainstance%20discrimination%20frameworks%20such%20as%20MoCo%20and%20SimSiam.%20To%20evaluate%20our%0Amethod%2C%20we%20run%20experiments%20on%20three%20benchmark%20datasets%3A%20ImageNet%2C%20STL-10%20and%0ACIFAR-10%20with%20different%20instance%20discrimination%20SSL%20approaches.%20The%0Aexperimental%20results%20show%20that%20our%20approach%20consistently%20outperforms%20the%0Abaseline%20methods%20across%20all%20three%20datasets%3B%20for%20instance%2C%20we%20improve%20upon%20the%0Avanilla%20MoCo-v2%20by%204.1%25%20on%20ImageNet%20under%20a%20linear%20evaluation%20protocol%20over%20800%0Aepochs.%20We%20also%20report%20results%20on%20semi-supervised%20learning%2C%20transfer%20learning%0Aon%20downstream%20tasks%2C%20and%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16122v2&entry.124074799=Read"},
{"title": "TELA: Text to Layer-wise 3D Clothed Human Generation", "author": "Junting Dong and Qi Fang and Zehuan Huang and Xudong Xu and Jingbo Wang and Sida Peng and Bo Dai", "abstract": "  This paper addresses the task of 3D clothed human generation from textural\ndescriptions. Previous works usually encode the human body and clothes as a\nholistic model and generate the whole model in a single-stage optimization,\nwhich makes them struggle for clothing editing and meanwhile lose fine-grained\ncontrol over the whole generation process. To solve this, we propose a\nlayer-wise clothed human representation combined with a progressive\noptimization strategy, which produces clothing-disentangled 3D human models\nwhile providing control capacity for the generation process. The basic idea is\nprogressively generating a minimal-clothed human body and layer-wise clothes.\nDuring clothing generation, a novel stratified compositional rendering method\nis proposed to fuse multi-layer human models, and a new loss function is\nutilized to help decouple the clothing model from the human body. The proposed\nmethod achieves high-quality disentanglement, which thereby provides an\neffective way for 3D garment generation. Extensive experiments demonstrate that\nour approach achieves state-of-the-art 3D clothed human generation while also\nsupporting cloth editing applications such as virtual try-on. Project page:\nhttp://jtdong.com/tela_layer/\n", "link": "http://arxiv.org/abs/2404.16748v1", "date": "2024-04-25", "relevancy": 2.2078, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5893}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5447}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5443}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TELA%3A%20Text%20to%20Layer-wise%203D%20Clothed%20Human%20Generation&body=Title%3A%20TELA%3A%20Text%20to%20Layer-wise%203D%20Clothed%20Human%20Generation%0AAuthor%3A%20Junting%20Dong%20and%20Qi%20Fang%20and%20Zehuan%20Huang%20and%20Xudong%20Xu%20and%20Jingbo%20Wang%20and%20Sida%20Peng%20and%20Bo%20Dai%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20task%20of%203D%20clothed%20human%20generation%20from%20textural%0Adescriptions.%20Previous%20works%20usually%20encode%20the%20human%20body%20and%20clothes%20as%20a%0Aholistic%20model%20and%20generate%20the%20whole%20model%20in%20a%20single-stage%20optimization%2C%0Awhich%20makes%20them%20struggle%20for%20clothing%20editing%20and%20meanwhile%20lose%20fine-grained%0Acontrol%20over%20the%20whole%20generation%20process.%20To%20solve%20this%2C%20we%20propose%20a%0Alayer-wise%20clothed%20human%20representation%20combined%20with%20a%20progressive%0Aoptimization%20strategy%2C%20which%20produces%20clothing-disentangled%203D%20human%20models%0Awhile%20providing%20control%20capacity%20for%20the%20generation%20process.%20The%20basic%20idea%20is%0Aprogressively%20generating%20a%20minimal-clothed%20human%20body%20and%20layer-wise%20clothes.%0ADuring%20clothing%20generation%2C%20a%20novel%20stratified%20compositional%20rendering%20method%0Ais%20proposed%20to%20fuse%20multi-layer%20human%20models%2C%20and%20a%20new%20loss%20function%20is%0Autilized%20to%20help%20decouple%20the%20clothing%20model%20from%20the%20human%20body.%20The%20proposed%0Amethod%20achieves%20high-quality%20disentanglement%2C%20which%20thereby%20provides%20an%0Aeffective%20way%20for%203D%20garment%20generation.%20Extensive%20experiments%20demonstrate%20that%0Aour%20approach%20achieves%20state-of-the-art%203D%20clothed%20human%20generation%20while%20also%0Asupporting%20cloth%20editing%20applications%20such%20as%20virtual%20try-on.%20Project%20page%3A%0Ahttp%3A//jtdong.com/tela_layer/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16748v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TELA%3A%20Text%20to%20Layer-wise%203D%20Clothed%20Human%20Generation&entry.906535625=Junting%20Dong%20and%20Qi%20Fang%20and%20Zehuan%20Huang%20and%20Xudong%20Xu%20and%20Jingbo%20Wang%20and%20Sida%20Peng%20and%20Bo%20Dai&entry.1292438233=%20%20This%20paper%20addresses%20the%20task%20of%203D%20clothed%20human%20generation%20from%20textural%0Adescriptions.%20Previous%20works%20usually%20encode%20the%20human%20body%20and%20clothes%20as%20a%0Aholistic%20model%20and%20generate%20the%20whole%20model%20in%20a%20single-stage%20optimization%2C%0Awhich%20makes%20them%20struggle%20for%20clothing%20editing%20and%20meanwhile%20lose%20fine-grained%0Acontrol%20over%20the%20whole%20generation%20process.%20To%20solve%20this%2C%20we%20propose%20a%0Alayer-wise%20clothed%20human%20representation%20combined%20with%20a%20progressive%0Aoptimization%20strategy%2C%20which%20produces%20clothing-disentangled%203D%20human%20models%0Awhile%20providing%20control%20capacity%20for%20the%20generation%20process.%20The%20basic%20idea%20is%0Aprogressively%20generating%20a%20minimal-clothed%20human%20body%20and%20layer-wise%20clothes.%0ADuring%20clothing%20generation%2C%20a%20novel%20stratified%20compositional%20rendering%20method%0Ais%20proposed%20to%20fuse%20multi-layer%20human%20models%2C%20and%20a%20new%20loss%20function%20is%0Autilized%20to%20help%20decouple%20the%20clothing%20model%20from%20the%20human%20body.%20The%20proposed%0Amethod%20achieves%20high-quality%20disentanglement%2C%20which%20thereby%20provides%20an%0Aeffective%20way%20for%203D%20garment%20generation.%20Extensive%20experiments%20demonstrate%20that%0Aour%20approach%20achieves%20state-of-the-art%203D%20clothed%20human%20generation%20while%20also%0Asupporting%20cloth%20editing%20applications%20such%20as%20virtual%20try-on.%20Project%20page%3A%0Ahttp%3A//jtdong.com/tela_layer/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16748v1&entry.124074799=Read"},
{"title": "The Third Monocular Depth Estimation Challenge", "author": "Jaime Spencer and Fabio Tosi and Matteo Poggi and Ripudaman Singh Arora and Chris Russell and Simon Hadfield and Richard Bowden and GuangYuan Zhou and ZhengXin Li and Qiang Rao and YiPing Bao and Xiao Liu and Dohyeong Kim and Jinseong Kim and Myunghyun Kim and Mykola Lavreniuk and Rui Li and Qing Mao and Jiang Wu and Yu Zhu and Jinqiu Sun and Yanning Zhang and Suraj Patni and Aradhye Agarwal and Chetan Arora and Pihai Sun and Kui Jiang and Gang Wu and Jian Liu and Xianming Liu and Junjun Jiang and Xidan Zhang and Jianing Wei and Fangjun Wang and Zhiming Tan and Jiabao Wang and Albert Luginov and Muhammad Shahzad and Seyed Hosseini and Aleksander Trajcevski and James H. Elder", "abstract": "  This paper discusses the results of the third edition of the Monocular Depth\nEstimation Challenge (MDEC). The challenge focuses on zero-shot generalization\nto the challenging SYNS-Patches dataset, featuring complex scenes in natural\nand indoor settings. As with the previous edition, methods can use any form of\nsupervision, i.e. supervised or self-supervised. The challenge received a total\nof 19 submissions outperforming the baseline on the test set: 10 among them\nsubmitted a report describing their approach, highlighting a diffused use of\nfoundational models such as Depth Anything at the core of their method. The\nchallenge winners drastically improved 3D F-Score performance, from 17.51% to\n23.72%.\n", "link": "http://arxiv.org/abs/2404.16831v1", "date": "2024-04-25", "relevancy": 2.1786, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5813}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Third%20Monocular%20Depth%20Estimation%20Challenge&body=Title%3A%20The%20Third%20Monocular%20Depth%20Estimation%20Challenge%0AAuthor%3A%20Jaime%20Spencer%20and%20Fabio%20Tosi%20and%20Matteo%20Poggi%20and%20Ripudaman%20Singh%20Arora%20and%20Chris%20Russell%20and%20Simon%20Hadfield%20and%20Richard%20Bowden%20and%20GuangYuan%20Zhou%20and%20ZhengXin%20Li%20and%20Qiang%20Rao%20and%20YiPing%20Bao%20and%20Xiao%20Liu%20and%20Dohyeong%20Kim%20and%20Jinseong%20Kim%20and%20Myunghyun%20Kim%20and%20Mykola%20Lavreniuk%20and%20Rui%20Li%20and%20Qing%20Mao%20and%20Jiang%20Wu%20and%20Yu%20Zhu%20and%20Jinqiu%20Sun%20and%20Yanning%20Zhang%20and%20Suraj%20Patni%20and%20Aradhye%20Agarwal%20and%20Chetan%20Arora%20and%20Pihai%20Sun%20and%20Kui%20Jiang%20and%20Gang%20Wu%20and%20Jian%20Liu%20and%20Xianming%20Liu%20and%20Junjun%20Jiang%20and%20Xidan%20Zhang%20and%20Jianing%20Wei%20and%20Fangjun%20Wang%20and%20Zhiming%20Tan%20and%20Jiabao%20Wang%20and%20Albert%20Luginov%20and%20Muhammad%20Shahzad%20and%20Seyed%20Hosseini%20and%20Aleksander%20Trajcevski%20and%20James%20H.%20Elder%0AAbstract%3A%20%20%20This%20paper%20discusses%20the%20results%20of%20the%20third%20edition%20of%20the%20Monocular%20Depth%0AEstimation%20Challenge%20%28MDEC%29.%20The%20challenge%20focuses%20on%20zero-shot%20generalization%0Ato%20the%20challenging%20SYNS-Patches%20dataset%2C%20featuring%20complex%20scenes%20in%20natural%0Aand%20indoor%20settings.%20As%20with%20the%20previous%20edition%2C%20methods%20can%20use%20any%20form%20of%0Asupervision%2C%20i.e.%20supervised%20or%20self-supervised.%20The%20challenge%20received%20a%20total%0Aof%2019%20submissions%20outperforming%20the%20baseline%20on%20the%20test%20set%3A%2010%20among%20them%0Asubmitted%20a%20report%20describing%20their%20approach%2C%20highlighting%20a%20diffused%20use%20of%0Afoundational%20models%20such%20as%20Depth%20Anything%20at%20the%20core%20of%20their%20method.%20The%0Achallenge%20winners%20drastically%20improved%203D%20F-Score%20performance%2C%20from%2017.51%25%20to%0A23.72%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16831v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Third%20Monocular%20Depth%20Estimation%20Challenge&entry.906535625=Jaime%20Spencer%20and%20Fabio%20Tosi%20and%20Matteo%20Poggi%20and%20Ripudaman%20Singh%20Arora%20and%20Chris%20Russell%20and%20Simon%20Hadfield%20and%20Richard%20Bowden%20and%20GuangYuan%20Zhou%20and%20ZhengXin%20Li%20and%20Qiang%20Rao%20and%20YiPing%20Bao%20and%20Xiao%20Liu%20and%20Dohyeong%20Kim%20and%20Jinseong%20Kim%20and%20Myunghyun%20Kim%20and%20Mykola%20Lavreniuk%20and%20Rui%20Li%20and%20Qing%20Mao%20and%20Jiang%20Wu%20and%20Yu%20Zhu%20and%20Jinqiu%20Sun%20and%20Yanning%20Zhang%20and%20Suraj%20Patni%20and%20Aradhye%20Agarwal%20and%20Chetan%20Arora%20and%20Pihai%20Sun%20and%20Kui%20Jiang%20and%20Gang%20Wu%20and%20Jian%20Liu%20and%20Xianming%20Liu%20and%20Junjun%20Jiang%20and%20Xidan%20Zhang%20and%20Jianing%20Wei%20and%20Fangjun%20Wang%20and%20Zhiming%20Tan%20and%20Jiabao%20Wang%20and%20Albert%20Luginov%20and%20Muhammad%20Shahzad%20and%20Seyed%20Hosseini%20and%20Aleksander%20Trajcevski%20and%20James%20H.%20Elder&entry.1292438233=%20%20This%20paper%20discusses%20the%20results%20of%20the%20third%20edition%20of%20the%20Monocular%20Depth%0AEstimation%20Challenge%20%28MDEC%29.%20The%20challenge%20focuses%20on%20zero-shot%20generalization%0Ato%20the%20challenging%20SYNS-Patches%20dataset%2C%20featuring%20complex%20scenes%20in%20natural%0Aand%20indoor%20settings.%20As%20with%20the%20previous%20edition%2C%20methods%20can%20use%20any%20form%20of%0Asupervision%2C%20i.e.%20supervised%20or%20self-supervised.%20The%20challenge%20received%20a%20total%0Aof%2019%20submissions%20outperforming%20the%20baseline%20on%20the%20test%20set%3A%2010%20among%20them%0Asubmitted%20a%20report%20describing%20their%20approach%2C%20highlighting%20a%20diffused%20use%20of%0Afoundational%20models%20such%20as%20Depth%20Anything%20at%20the%20core%20of%20their%20method.%20The%0Achallenge%20winners%20drastically%20improved%203D%20F-Score%20performance%2C%20from%2017.51%25%20to%0A23.72%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16831v1&entry.124074799=Read"},
{"title": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning", "author": "Hongxia Xie and Chu-Jun Peng and Yu-Wen Tseng and Hung-Jen Chen and Chan-Feng Hsu and Hong-Han Shuai and Wen-Huang Cheng", "abstract": "  Visual Instruction Tuning represents a novel learning paradigm involving the\nfine-tuning of pre-trained language models using task-specific instructions.\nThis paradigm shows promising zero-shot results in various natural language\nprocessing tasks but is still unexplored in vision emotion understanding. In\nthis work, we focus on enhancing the model's proficiency in understanding and\nadhering to instructions related to emotional contexts. Initially, we identify\nkey visual clues critical to visual emotion recognition. Subsequently, we\nintroduce a novel GPT-assisted pipeline for generating emotion visual\ninstruction data, effectively addressing the scarcity of annotated instruction\ndata in this domain. Expanding on the groundwork established by InstructBLIP,\nour proposed EmoVIT architecture incorporates emotion-specific instruction\ndata, leveraging the powerful capabilities of Large Language Models to enhance\nperformance. Through extensive experiments, our model showcases its proficiency\nin emotion classification, adeptness in affective reasoning, and competence in\ncomprehending humor. The comparative analysis provides a robust benchmark for\nEmotion Visual Instruction Tuning in the era of LLMs, providing valuable\ninsights and opening avenues for future exploration in this domain. Our code is\navailable at \\url{https://github.com/aimmemotion/EmoVIT}.\n", "link": "http://arxiv.org/abs/2404.16670v1", "date": "2024-04-25", "relevancy": 2.1564, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5539}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5436}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5287}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EmoVIT%3A%20Revolutionizing%20Emotion%20Insights%20with%20Visual%20Instruction%20Tuning&body=Title%3A%20EmoVIT%3A%20Revolutionizing%20Emotion%20Insights%20with%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Hongxia%20Xie%20and%20Chu-Jun%20Peng%20and%20Yu-Wen%20Tseng%20and%20Hung-Jen%20Chen%20and%20Chan-Feng%20Hsu%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng%0AAbstract%3A%20%20%20Visual%20Instruction%20Tuning%20represents%20a%20novel%20learning%20paradigm%20involving%20the%0Afine-tuning%20of%20pre-trained%20language%20models%20using%20task-specific%20instructions.%0AThis%20paradigm%20shows%20promising%20zero-shot%20results%20in%20various%20natural%20language%0Aprocessing%20tasks%20but%20is%20still%20unexplored%20in%20vision%20emotion%20understanding.%20In%0Athis%20work%2C%20we%20focus%20on%20enhancing%20the%20model%27s%20proficiency%20in%20understanding%20and%0Aadhering%20to%20instructions%20related%20to%20emotional%20contexts.%20Initially%2C%20we%20identify%0Akey%20visual%20clues%20critical%20to%20visual%20emotion%20recognition.%20Subsequently%2C%20we%0Aintroduce%20a%20novel%20GPT-assisted%20pipeline%20for%20generating%20emotion%20visual%0Ainstruction%20data%2C%20effectively%20addressing%20the%20scarcity%20of%20annotated%20instruction%0Adata%20in%20this%20domain.%20Expanding%20on%20the%20groundwork%20established%20by%20InstructBLIP%2C%0Aour%20proposed%20EmoVIT%20architecture%20incorporates%20emotion-specific%20instruction%0Adata%2C%20leveraging%20the%20powerful%20capabilities%20of%20Large%20Language%20Models%20to%20enhance%0Aperformance.%20Through%20extensive%20experiments%2C%20our%20model%20showcases%20its%20proficiency%0Ain%20emotion%20classification%2C%20adeptness%20in%20affective%20reasoning%2C%20and%20competence%20in%0Acomprehending%20humor.%20The%20comparative%20analysis%20provides%20a%20robust%20benchmark%20for%0AEmotion%20Visual%20Instruction%20Tuning%20in%20the%20era%20of%20LLMs%2C%20providing%20valuable%0Ainsights%20and%20opening%20avenues%20for%20future%20exploration%20in%20this%20domain.%20Our%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/aimmemotion/EmoVIT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16670v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoVIT%3A%20Revolutionizing%20Emotion%20Insights%20with%20Visual%20Instruction%20Tuning&entry.906535625=Hongxia%20Xie%20and%20Chu-Jun%20Peng%20and%20Yu-Wen%20Tseng%20and%20Hung-Jen%20Chen%20and%20Chan-Feng%20Hsu%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng&entry.1292438233=%20%20Visual%20Instruction%20Tuning%20represents%20a%20novel%20learning%20paradigm%20involving%20the%0Afine-tuning%20of%20pre-trained%20language%20models%20using%20task-specific%20instructions.%0AThis%20paradigm%20shows%20promising%20zero-shot%20results%20in%20various%20natural%20language%0Aprocessing%20tasks%20but%20is%20still%20unexplored%20in%20vision%20emotion%20understanding.%20In%0Athis%20work%2C%20we%20focus%20on%20enhancing%20the%20model%27s%20proficiency%20in%20understanding%20and%0Aadhering%20to%20instructions%20related%20to%20emotional%20contexts.%20Initially%2C%20we%20identify%0Akey%20visual%20clues%20critical%20to%20visual%20emotion%20recognition.%20Subsequently%2C%20we%0Aintroduce%20a%20novel%20GPT-assisted%20pipeline%20for%20generating%20emotion%20visual%0Ainstruction%20data%2C%20effectively%20addressing%20the%20scarcity%20of%20annotated%20instruction%0Adata%20in%20this%20domain.%20Expanding%20on%20the%20groundwork%20established%20by%20InstructBLIP%2C%0Aour%20proposed%20EmoVIT%20architecture%20incorporates%20emotion-specific%20instruction%0Adata%2C%20leveraging%20the%20powerful%20capabilities%20of%20Large%20Language%20Models%20to%20enhance%0Aperformance.%20Through%20extensive%20experiments%2C%20our%20model%20showcases%20its%20proficiency%0Ain%20emotion%20classification%2C%20adeptness%20in%20affective%20reasoning%2C%20and%20competence%20in%0Acomprehending%20humor.%20The%20comparative%20analysis%20provides%20a%20robust%20benchmark%20for%0AEmotion%20Visual%20Instruction%20Tuning%20in%20the%20era%20of%20LLMs%2C%20providing%20valuable%0Ainsights%20and%20opening%20avenues%20for%20future%20exploration%20in%20this%20domain.%20Our%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/aimmemotion/EmoVIT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16670v1&entry.124074799=Read"},
{"title": "Multi-Scale Representations by Varying Window Attention for Semantic\n  Segmentation", "author": "Haotian Yan and Ming Wu and Chuang Zhang", "abstract": "  Multi-scale learning is central to semantic segmentation. We visualize the\neffective receptive field (ERF) of canonical multi-scale representations and\npoint out two risks in learning them: scale inadequacy and field inactivation.\nA novel multi-scale learner, varying window attention (VWA), is presented to\naddress these issues. VWA leverages the local window attention (LWA) and\ndisentangles LWA into the query window and context window, allowing the\ncontext's scale to vary for the query to learn representations at multiple\nscales. However, varying the context to large-scale windows (enlarging ratio R)\ncan significantly increase the memory footprint and computation cost (R^2 times\nlarger than LWA). We propose a simple but professional re-scaling strategy to\nzero the extra induced cost without compromising performance. Consequently, VWA\nuses the same cost as LWA to overcome the receptive limitation of the local\nwindow. Furthermore, depending on VWA and employing various MLPs, we introduce\na multi-scale decoder (MSD), VWFormer, to improve multi-scale representations\nfor semantic segmentation. VWFormer achieves efficiency competitive with the\nmost compute-friendly MSDs, like FPN and MLP decoder, but performs much better\nthan any MSDs. For instance, using nearly half of UPerNet's computation,\nVWFormer outperforms it by 1.0%-2.5% mIoU on ADE20K. With little extra\noverhead, ~10G FLOPs, Mask2Former armed with VWFormer improves by 1.0%-1.3%.\n", "link": "http://arxiv.org/abs/2404.16573v1", "date": "2024-04-25", "relevancy": 2.1426, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5172}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Representations%20by%20Varying%20Window%20Attention%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20Multi-Scale%20Representations%20by%20Varying%20Window%20Attention%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Haotian%20Yan%20and%20Ming%20Wu%20and%20Chuang%20Zhang%0AAbstract%3A%20%20%20Multi-scale%20learning%20is%20central%20to%20semantic%20segmentation.%20We%20visualize%20the%0Aeffective%20receptive%20field%20%28ERF%29%20of%20canonical%20multi-scale%20representations%20and%0Apoint%20out%20two%20risks%20in%20learning%20them%3A%20scale%20inadequacy%20and%20field%20inactivation.%0AA%20novel%20multi-scale%20learner%2C%20varying%20window%20attention%20%28VWA%29%2C%20is%20presented%20to%0Aaddress%20these%20issues.%20VWA%20leverages%20the%20local%20window%20attention%20%28LWA%29%20and%0Adisentangles%20LWA%20into%20the%20query%20window%20and%20context%20window%2C%20allowing%20the%0Acontext%27s%20scale%20to%20vary%20for%20the%20query%20to%20learn%20representations%20at%20multiple%0Ascales.%20However%2C%20varying%20the%20context%20to%20large-scale%20windows%20%28enlarging%20ratio%20R%29%0Acan%20significantly%20increase%20the%20memory%20footprint%20and%20computation%20cost%20%28R%5E2%20times%0Alarger%20than%20LWA%29.%20We%20propose%20a%20simple%20but%20professional%20re-scaling%20strategy%20to%0Azero%20the%20extra%20induced%20cost%20without%20compromising%20performance.%20Consequently%2C%20VWA%0Auses%20the%20same%20cost%20as%20LWA%20to%20overcome%20the%20receptive%20limitation%20of%20the%20local%0Awindow.%20Furthermore%2C%20depending%20on%20VWA%20and%20employing%20various%20MLPs%2C%20we%20introduce%0Aa%20multi-scale%20decoder%20%28MSD%29%2C%20VWFormer%2C%20to%20improve%20multi-scale%20representations%0Afor%20semantic%20segmentation.%20VWFormer%20achieves%20efficiency%20competitive%20with%20the%0Amost%20compute-friendly%20MSDs%2C%20like%20FPN%20and%20MLP%20decoder%2C%20but%20performs%20much%20better%0Athan%20any%20MSDs.%20For%20instance%2C%20using%20nearly%20half%20of%20UPerNet%27s%20computation%2C%0AVWFormer%20outperforms%20it%20by%201.0%25-2.5%25%20mIoU%20on%20ADE20K.%20With%20little%20extra%0Aoverhead%2C%20~10G%20FLOPs%2C%20Mask2Former%20armed%20with%20VWFormer%20improves%20by%201.0%25-1.3%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16573v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Representations%20by%20Varying%20Window%20Attention%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Haotian%20Yan%20and%20Ming%20Wu%20and%20Chuang%20Zhang&entry.1292438233=%20%20Multi-scale%20learning%20is%20central%20to%20semantic%20segmentation.%20We%20visualize%20the%0Aeffective%20receptive%20field%20%28ERF%29%20of%20canonical%20multi-scale%20representations%20and%0Apoint%20out%20two%20risks%20in%20learning%20them%3A%20scale%20inadequacy%20and%20field%20inactivation.%0AA%20novel%20multi-scale%20learner%2C%20varying%20window%20attention%20%28VWA%29%2C%20is%20presented%20to%0Aaddress%20these%20issues.%20VWA%20leverages%20the%20local%20window%20attention%20%28LWA%29%20and%0Adisentangles%20LWA%20into%20the%20query%20window%20and%20context%20window%2C%20allowing%20the%0Acontext%27s%20scale%20to%20vary%20for%20the%20query%20to%20learn%20representations%20at%20multiple%0Ascales.%20However%2C%20varying%20the%20context%20to%20large-scale%20windows%20%28enlarging%20ratio%20R%29%0Acan%20significantly%20increase%20the%20memory%20footprint%20and%20computation%20cost%20%28R%5E2%20times%0Alarger%20than%20LWA%29.%20We%20propose%20a%20simple%20but%20professional%20re-scaling%20strategy%20to%0Azero%20the%20extra%20induced%20cost%20without%20compromising%20performance.%20Consequently%2C%20VWA%0Auses%20the%20same%20cost%20as%20LWA%20to%20overcome%20the%20receptive%20limitation%20of%20the%20local%0Awindow.%20Furthermore%2C%20depending%20on%20VWA%20and%20employing%20various%20MLPs%2C%20we%20introduce%0Aa%20multi-scale%20decoder%20%28MSD%29%2C%20VWFormer%2C%20to%20improve%20multi-scale%20representations%0Afor%20semantic%20segmentation.%20VWFormer%20achieves%20efficiency%20competitive%20with%20the%0Amost%20compute-friendly%20MSDs%2C%20like%20FPN%20and%20MLP%20decoder%2C%20but%20performs%20much%20better%0Athan%20any%20MSDs.%20For%20instance%2C%20using%20nearly%20half%20of%20UPerNet%27s%20computation%2C%0AVWFormer%20outperforms%20it%20by%201.0%25-2.5%25%20mIoU%20on%20ADE20K.%20With%20little%20extra%0Aoverhead%2C%20~10G%20FLOPs%2C%20Mask2Former%20armed%20with%20VWFormer%20improves%20by%201.0%25-1.3%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16573v1&entry.124074799=Read"},
{"title": "Two-Stage Aggregation with Dynamic Local Attention for Irregular Time\n  Series", "author": "Xingyu Chen and Xiaochen Zheng and Amina Mollaysa and Manuel Sch\u00fcrch and Ahmed Allam and Michael Krauthammer", "abstract": "  Irregular multivariate time series data is characterized by varying time\nintervals between consecutive observations of measured variables/signals (i.e.,\nfeatures) and varying sampling rates (i.e., recordings/measurement) across\nthese features. Modeling time series while taking into account these\nirregularities is still a challenging task for machine learning methods. Here,\nwe introduce TADA, a Two-stageAggregation process with Dynamic local Attention\nto harmonize time-wise and feature-wise irregularities in multivariate time\nseries. In the first stage, the irregular time series undergoes temporal\nembedding (TE) using all available features at each time step. This process\npreserves the contribution of each available feature and generates a\nfixed-dimensional representation per time step. The second stage introduces a\ndynamic local attention (DLA) mechanism with adaptive window sizes. DLA\naggregates time recordings using feature-specific windows to harmonize\nirregular time intervals capturing feature-specific sampling rates. Then\nhierarchical MLP mixer layers process the output of DLA through multiscale\npatching to leverage information at various scales for the downstream tasks.\nTADA outperforms state-of-the-art methods on three real-world datasets,\nincluding the latest MIMIC IV dataset, and highlights its effectiveness in\nhandling irregular multivariate time series and its potential for various\nreal-world applications.\n", "link": "http://arxiv.org/abs/2311.07744v2", "date": "2024-04-25", "relevancy": 2.1229, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5326}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5066}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Two-Stage%20Aggregation%20with%20Dynamic%20Local%20Attention%20for%20Irregular%20Time%0A%20%20Series&body=Title%3A%20Two-Stage%20Aggregation%20with%20Dynamic%20Local%20Attention%20for%20Irregular%20Time%0A%20%20Series%0AAuthor%3A%20Xingyu%20Chen%20and%20Xiaochen%20Zheng%20and%20Amina%20Mollaysa%20and%20Manuel%20Sch%C3%BCrch%20and%20Ahmed%20Allam%20and%20Michael%20Krauthammer%0AAbstract%3A%20%20%20Irregular%20multivariate%20time%20series%20data%20is%20characterized%20by%20varying%20time%0Aintervals%20between%20consecutive%20observations%20of%20measured%20variables/signals%20%28i.e.%2C%0Afeatures%29%20and%20varying%20sampling%20rates%20%28i.e.%2C%20recordings/measurement%29%20across%0Athese%20features.%20Modeling%20time%20series%20while%20taking%20into%20account%20these%0Airregularities%20is%20still%20a%20challenging%20task%20for%20machine%20learning%20methods.%20Here%2C%0Awe%20introduce%20TADA%2C%20a%20Two-stageAggregation%20process%20with%20Dynamic%20local%20Attention%0Ato%20harmonize%20time-wise%20and%20feature-wise%20irregularities%20in%20multivariate%20time%0Aseries.%20In%20the%20first%20stage%2C%20the%20irregular%20time%20series%20undergoes%20temporal%0Aembedding%20%28TE%29%20using%20all%20available%20features%20at%20each%20time%20step.%20This%20process%0Apreserves%20the%20contribution%20of%20each%20available%20feature%20and%20generates%20a%0Afixed-dimensional%20representation%20per%20time%20step.%20The%20second%20stage%20introduces%20a%0Adynamic%20local%20attention%20%28DLA%29%20mechanism%20with%20adaptive%20window%20sizes.%20DLA%0Aaggregates%20time%20recordings%20using%20feature-specific%20windows%20to%20harmonize%0Airregular%20time%20intervals%20capturing%20feature-specific%20sampling%20rates.%20Then%0Ahierarchical%20MLP%20mixer%20layers%20process%20the%20output%20of%20DLA%20through%20multiscale%0Apatching%20to%20leverage%20information%20at%20various%20scales%20for%20the%20downstream%20tasks.%0ATADA%20outperforms%20state-of-the-art%20methods%20on%20three%20real-world%20datasets%2C%0Aincluding%20the%20latest%20MIMIC%20IV%20dataset%2C%20and%20highlights%20its%20effectiveness%20in%0Ahandling%20irregular%20multivariate%20time%20series%20and%20its%20potential%20for%20various%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07744v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Stage%20Aggregation%20with%20Dynamic%20Local%20Attention%20for%20Irregular%20Time%0A%20%20Series&entry.906535625=Xingyu%20Chen%20and%20Xiaochen%20Zheng%20and%20Amina%20Mollaysa%20and%20Manuel%20Sch%C3%BCrch%20and%20Ahmed%20Allam%20and%20Michael%20Krauthammer&entry.1292438233=%20%20Irregular%20multivariate%20time%20series%20data%20is%20characterized%20by%20varying%20time%0Aintervals%20between%20consecutive%20observations%20of%20measured%20variables/signals%20%28i.e.%2C%0Afeatures%29%20and%20varying%20sampling%20rates%20%28i.e.%2C%20recordings/measurement%29%20across%0Athese%20features.%20Modeling%20time%20series%20while%20taking%20into%20account%20these%0Airregularities%20is%20still%20a%20challenging%20task%20for%20machine%20learning%20methods.%20Here%2C%0Awe%20introduce%20TADA%2C%20a%20Two-stageAggregation%20process%20with%20Dynamic%20local%20Attention%0Ato%20harmonize%20time-wise%20and%20feature-wise%20irregularities%20in%20multivariate%20time%0Aseries.%20In%20the%20first%20stage%2C%20the%20irregular%20time%20series%20undergoes%20temporal%0Aembedding%20%28TE%29%20using%20all%20available%20features%20at%20each%20time%20step.%20This%20process%0Apreserves%20the%20contribution%20of%20each%20available%20feature%20and%20generates%20a%0Afixed-dimensional%20representation%20per%20time%20step.%20The%20second%20stage%20introduces%20a%0Adynamic%20local%20attention%20%28DLA%29%20mechanism%20with%20adaptive%20window%20sizes.%20DLA%0Aaggregates%20time%20recordings%20using%20feature-specific%20windows%20to%20harmonize%0Airregular%20time%20intervals%20capturing%20feature-specific%20sampling%20rates.%20Then%0Ahierarchical%20MLP%20mixer%20layers%20process%20the%20output%20of%20DLA%20through%20multiscale%0Apatching%20to%20leverage%20information%20at%20various%20scales%20for%20the%20downstream%20tasks.%0ATADA%20outperforms%20state-of-the-art%20methods%20on%20three%20real-world%20datasets%2C%0Aincluding%20the%20latest%20MIMIC%20IV%20dataset%2C%20and%20highlights%20its%20effectiveness%20in%0Ahandling%20irregular%20multivariate%20time%20series%20and%20its%20potential%20for%20various%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07744v2&entry.124074799=Read"},
{"title": "DeepKalPose: An Enhanced Deep-Learning Kalman Filter for Temporally\n  Consistent Monocular Vehicle Pose Estimation", "author": "Leandro Di Bella and Yangxintong Lyu and Adrian Munteanu", "abstract": "  This paper presents DeepKalPose, a novel approach for enhancing temporal\nconsistency in monocular vehicle pose estimation applied on video through a\ndeep-learning-based Kalman Filter. By integrating a Bi-directional Kalman\nfilter strategy utilizing forward and backward time-series processing, combined\nwith a learnable motion model to represent complex motion patterns, our method\nsignificantly improves pose accuracy and robustness across various conditions,\nparticularly for occluded or distant vehicles. Experimental validation on the\nKITTI dataset confirms that DeepKalPose outperforms existing methods in both\npose accuracy and temporal consistency.\n", "link": "http://arxiv.org/abs/2404.16558v1", "date": "2024-04-25", "relevancy": 2.0799, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5302}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.519}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5101}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DeepKalPose%3A%20An%20Enhanced%20Deep-Learning%20Kalman%20Filter%20for%20Temporally%0A%20%20Consistent%20Monocular%20Vehicle%20Pose%20Estimation&body=Title%3A%20DeepKalPose%3A%20An%20Enhanced%20Deep-Learning%20Kalman%20Filter%20for%20Temporally%0A%20%20Consistent%20Monocular%20Vehicle%20Pose%20Estimation%0AAuthor%3A%20Leandro%20Di%20Bella%20and%20Yangxintong%20Lyu%20and%20Adrian%20Munteanu%0AAbstract%3A%20%20%20This%20paper%20presents%20DeepKalPose%2C%20a%20novel%20approach%20for%20enhancing%20temporal%0Aconsistency%20in%20monocular%20vehicle%20pose%20estimation%20applied%20on%20video%20through%20a%0Adeep-learning-based%20Kalman%20Filter.%20By%20integrating%20a%20Bi-directional%20Kalman%0Afilter%20strategy%20utilizing%20forward%20and%20backward%20time-series%20processing%2C%20combined%0Awith%20a%20learnable%20motion%20model%20to%20represent%20complex%20motion%20patterns%2C%20our%20method%0Asignificantly%20improves%20pose%20accuracy%20and%20robustness%20across%20various%20conditions%2C%0Aparticularly%20for%20occluded%20or%20distant%20vehicles.%20Experimental%20validation%20on%20the%0AKITTI%20dataset%20confirms%20that%20DeepKalPose%20outperforms%20existing%20methods%20in%20both%0Apose%20accuracy%20and%20temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16558v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepKalPose%3A%20An%20Enhanced%20Deep-Learning%20Kalman%20Filter%20for%20Temporally%0A%20%20Consistent%20Monocular%20Vehicle%20Pose%20Estimation&entry.906535625=Leandro%20Di%20Bella%20and%20Yangxintong%20Lyu%20and%20Adrian%20Munteanu&entry.1292438233=%20%20This%20paper%20presents%20DeepKalPose%2C%20a%20novel%20approach%20for%20enhancing%20temporal%0Aconsistency%20in%20monocular%20vehicle%20pose%20estimation%20applied%20on%20video%20through%20a%0Adeep-learning-based%20Kalman%20Filter.%20By%20integrating%20a%20Bi-directional%20Kalman%0Afilter%20strategy%20utilizing%20forward%20and%20backward%20time-series%20processing%2C%20combined%0Awith%20a%20learnable%20motion%20model%20to%20represent%20complex%20motion%20patterns%2C%20our%20method%0Asignificantly%20improves%20pose%20accuracy%20and%20robustness%20across%20various%20conditions%2C%0Aparticularly%20for%20occluded%20or%20distant%20vehicles.%20Experimental%20validation%20on%20the%0AKITTI%20dataset%20confirms%20that%20DeepKalPose%20outperforms%20existing%20methods%20in%20both%0Apose%20accuracy%20and%20temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16558v1&entry.124074799=Read"},
{"title": "AAPL: Adding Attributes to Prompt Learning for Vision-Language Models", "author": "Gahyeon Kim and Sohee Kim and Seokju Lee", "abstract": "  Recent advances in large pre-trained vision-language models have demonstrated\nremarkable performance on zero-shot downstream tasks. Building upon this,\nrecent studies, such as CoOp and CoCoOp, have proposed the use of prompt\nlearning, where context within a prompt is replaced with learnable vectors,\nleading to significant improvements over manually crafted prompts. However, the\nperformance improvement for unseen classes is still marginal, and to tackle\nthis problem, data augmentation has been frequently used in traditional\nzero-shot learning techniques. Through our experiments, we have identified\nimportant issues in CoOp and CoCoOp: the context learned through traditional\nimage augmentation is biased toward seen classes, negatively impacting\ngeneralization to unseen classes. To address this problem, we propose\nadversarial token embedding to disentangle low-level visual augmentation\nfeatures from high-level class information when inducing bias in learnable\nprompts. Through our novel mechanism called \"Adding Attributes to Prompt\nLearning\", AAPL, we guide the learnable context to effectively extract text\nfeatures by focusing on high-level features for unseen classes. We have\nconducted experiments across 11 datasets, and overall, AAPL shows favorable\nperformances compared to the existing methods in few-shot learning, zero-shot\nlearning, cross-dataset, and domain generalization tasks.\n", "link": "http://arxiv.org/abs/2404.16804v1", "date": "2024-04-25", "relevancy": 2.0798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5087}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5076}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AAPL%3A%20Adding%20Attributes%20to%20Prompt%20Learning%20for%20Vision-Language%20Models&body=Title%3A%20AAPL%3A%20Adding%20Attributes%20to%20Prompt%20Learning%20for%20Vision-Language%20Models%0AAuthor%3A%20Gahyeon%20Kim%20and%20Sohee%20Kim%20and%20Seokju%20Lee%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20pre-trained%20vision-language%20models%20have%20demonstrated%0Aremarkable%20performance%20on%20zero-shot%20downstream%20tasks.%20Building%20upon%20this%2C%0Arecent%20studies%2C%20such%20as%20CoOp%20and%20CoCoOp%2C%20have%20proposed%20the%20use%20of%20prompt%0Alearning%2C%20where%20context%20within%20a%20prompt%20is%20replaced%20with%20learnable%20vectors%2C%0Aleading%20to%20significant%20improvements%20over%20manually%20crafted%20prompts.%20However%2C%20the%0Aperformance%20improvement%20for%20unseen%20classes%20is%20still%20marginal%2C%20and%20to%20tackle%0Athis%20problem%2C%20data%20augmentation%20has%20been%20frequently%20used%20in%20traditional%0Azero-shot%20learning%20techniques.%20Through%20our%20experiments%2C%20we%20have%20identified%0Aimportant%20issues%20in%20CoOp%20and%20CoCoOp%3A%20the%20context%20learned%20through%20traditional%0Aimage%20augmentation%20is%20biased%20toward%20seen%20classes%2C%20negatively%20impacting%0Ageneralization%20to%20unseen%20classes.%20To%20address%20this%20problem%2C%20we%20propose%0Aadversarial%20token%20embedding%20to%20disentangle%20low-level%20visual%20augmentation%0Afeatures%20from%20high-level%20class%20information%20when%20inducing%20bias%20in%20learnable%0Aprompts.%20Through%20our%20novel%20mechanism%20called%20%22Adding%20Attributes%20to%20Prompt%0ALearning%22%2C%20AAPL%2C%20we%20guide%20the%20learnable%20context%20to%20effectively%20extract%20text%0Afeatures%20by%20focusing%20on%20high-level%20features%20for%20unseen%20classes.%20We%20have%0Aconducted%20experiments%20across%2011%20datasets%2C%20and%20overall%2C%20AAPL%20shows%20favorable%0Aperformances%20compared%20to%20the%20existing%20methods%20in%20few-shot%20learning%2C%20zero-shot%0Alearning%2C%20cross-dataset%2C%20and%20domain%20generalization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16804v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AAPL%3A%20Adding%20Attributes%20to%20Prompt%20Learning%20for%20Vision-Language%20Models&entry.906535625=Gahyeon%20Kim%20and%20Sohee%20Kim%20and%20Seokju%20Lee&entry.1292438233=%20%20Recent%20advances%20in%20large%20pre-trained%20vision-language%20models%20have%20demonstrated%0Aremarkable%20performance%20on%20zero-shot%20downstream%20tasks.%20Building%20upon%20this%2C%0Arecent%20studies%2C%20such%20as%20CoOp%20and%20CoCoOp%2C%20have%20proposed%20the%20use%20of%20prompt%0Alearning%2C%20where%20context%20within%20a%20prompt%20is%20replaced%20with%20learnable%20vectors%2C%0Aleading%20to%20significant%20improvements%20over%20manually%20crafted%20prompts.%20However%2C%20the%0Aperformance%20improvement%20for%20unseen%20classes%20is%20still%20marginal%2C%20and%20to%20tackle%0Athis%20problem%2C%20data%20augmentation%20has%20been%20frequently%20used%20in%20traditional%0Azero-shot%20learning%20techniques.%20Through%20our%20experiments%2C%20we%20have%20identified%0Aimportant%20issues%20in%20CoOp%20and%20CoCoOp%3A%20the%20context%20learned%20through%20traditional%0Aimage%20augmentation%20is%20biased%20toward%20seen%20classes%2C%20negatively%20impacting%0Ageneralization%20to%20unseen%20classes.%20To%20address%20this%20problem%2C%20we%20propose%0Aadversarial%20token%20embedding%20to%20disentangle%20low-level%20visual%20augmentation%0Afeatures%20from%20high-level%20class%20information%20when%20inducing%20bias%20in%20learnable%0Aprompts.%20Through%20our%20novel%20mechanism%20called%20%22Adding%20Attributes%20to%20Prompt%0ALearning%22%2C%20AAPL%2C%20we%20guide%20the%20learnable%20context%20to%20effectively%20extract%20text%0Afeatures%20by%20focusing%20on%20high-level%20features%20for%20unseen%20classes.%20We%20have%0Aconducted%20experiments%20across%2011%20datasets%2C%20and%20overall%2C%20AAPL%20shows%20favorable%0Aperformances%20compared%20to%20the%20existing%20methods%20in%20few-shot%20learning%2C%20zero-shot%0Alearning%2C%20cross-dataset%2C%20and%20domain%20generalization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16804v1&entry.124074799=Read"},
{"title": "Adaptive Local Binary Pattern: A Novel Feature Descriptor for Enhanced\n  Analysis of Kidney Abnormalities in CT Scan Images using ensemble based\n  Machine Learning Approach", "author": "Tahmim Hossain and Faisal Sayed and Solehin Islam", "abstract": "  The shortage of nephrologists and the growing public health concern over\nrenal failure have spurred the demand for AI systems capable of autonomously\ndetecting kidney abnormalities. Renal failure, marked by a gradual decline in\nkidney function, can result from factors like cysts, stones, and tumors.\nChronic kidney disease may go unnoticed initially, leading to untreated cases\nuntil they reach an advanced stage. The dataset, comprising 12,427 images from\nmultiple hospitals in Dhaka, was categorized into four groups: cyst, tumor,\nstone, and normal. Our methodology aims to enhance CT scan image quality using\nCropping, Resizing, and CALHE techniques, followed by feature extraction with\nour proposed Adaptive Local Binary Pattern (A-LBP) feature extraction method\ncompared with the state-of-the-art local binary pattern (LBP) method. Our\nproposed features fed into classifiers such as Random Forest, Decision Tree,\nNaive Bayes, K-Nearest Neighbor, and SVM. We explored an ensemble model with\nsoft voting to get a more robust model for our task. We got the highest of more\nthan 99% in accuracy using our feature descriptor and ensembling five\nclassifiers (Random Forest, Decision Tree, Naive Bayes, K-Nearest Neighbor,\nSupport Vector Machine) with the soft voting method.\n", "link": "http://arxiv.org/abs/2404.14560v2", "date": "2024-04-25", "relevancy": 2.0739, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5269}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5137}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5094}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Local%20Binary%20Pattern%3A%20A%20Novel%20Feature%20Descriptor%20for%20Enhanced%0A%20%20Analysis%20of%20Kidney%20Abnormalities%20in%20CT%20Scan%20Images%20using%20ensemble%20based%0A%20%20Machine%20Learning%20Approach&body=Title%3A%20Adaptive%20Local%20Binary%20Pattern%3A%20A%20Novel%20Feature%20Descriptor%20for%20Enhanced%0A%20%20Analysis%20of%20Kidney%20Abnormalities%20in%20CT%20Scan%20Images%20using%20ensemble%20based%0A%20%20Machine%20Learning%20Approach%0AAuthor%3A%20Tahmim%20Hossain%20and%20Faisal%20Sayed%20and%20Solehin%20Islam%0AAbstract%3A%20%20%20The%20shortage%20of%20nephrologists%20and%20the%20growing%20public%20health%20concern%20over%0Arenal%20failure%20have%20spurred%20the%20demand%20for%20AI%20systems%20capable%20of%20autonomously%0Adetecting%20kidney%20abnormalities.%20Renal%20failure%2C%20marked%20by%20a%20gradual%20decline%20in%0Akidney%20function%2C%20can%20result%20from%20factors%20like%20cysts%2C%20stones%2C%20and%20tumors.%0AChronic%20kidney%20disease%20may%20go%20unnoticed%20initially%2C%20leading%20to%20untreated%20cases%0Auntil%20they%20reach%20an%20advanced%20stage.%20The%20dataset%2C%20comprising%2012%2C427%20images%20from%0Amultiple%20hospitals%20in%20Dhaka%2C%20was%20categorized%20into%20four%20groups%3A%20cyst%2C%20tumor%2C%0Astone%2C%20and%20normal.%20Our%20methodology%20aims%20to%20enhance%20CT%20scan%20image%20quality%20using%0ACropping%2C%20Resizing%2C%20and%20CALHE%20techniques%2C%20followed%20by%20feature%20extraction%20with%0Aour%20proposed%20Adaptive%20Local%20Binary%20Pattern%20%28A-LBP%29%20feature%20extraction%20method%0Acompared%20with%20the%20state-of-the-art%20local%20binary%20pattern%20%28LBP%29%20method.%20Our%0Aproposed%20features%20fed%20into%20classifiers%20such%20as%20Random%20Forest%2C%20Decision%20Tree%2C%0ANaive%20Bayes%2C%20K-Nearest%20Neighbor%2C%20and%20SVM.%20We%20explored%20an%20ensemble%20model%20with%0Asoft%20voting%20to%20get%20a%20more%20robust%20model%20for%20our%20task.%20We%20got%20the%20highest%20of%20more%0Athan%2099%25%20in%20accuracy%20using%20our%20feature%20descriptor%20and%20ensembling%20five%0Aclassifiers%20%28Random%20Forest%2C%20Decision%20Tree%2C%20Naive%20Bayes%2C%20K-Nearest%20Neighbor%2C%0ASupport%20Vector%20Machine%29%20with%20the%20soft%20voting%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14560v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Local%20Binary%20Pattern%3A%20A%20Novel%20Feature%20Descriptor%20for%20Enhanced%0A%20%20Analysis%20of%20Kidney%20Abnormalities%20in%20CT%20Scan%20Images%20using%20ensemble%20based%0A%20%20Machine%20Learning%20Approach&entry.906535625=Tahmim%20Hossain%20and%20Faisal%20Sayed%20and%20Solehin%20Islam&entry.1292438233=%20%20The%20shortage%20of%20nephrologists%20and%20the%20growing%20public%20health%20concern%20over%0Arenal%20failure%20have%20spurred%20the%20demand%20for%20AI%20systems%20capable%20of%20autonomously%0Adetecting%20kidney%20abnormalities.%20Renal%20failure%2C%20marked%20by%20a%20gradual%20decline%20in%0Akidney%20function%2C%20can%20result%20from%20factors%20like%20cysts%2C%20stones%2C%20and%20tumors.%0AChronic%20kidney%20disease%20may%20go%20unnoticed%20initially%2C%20leading%20to%20untreated%20cases%0Auntil%20they%20reach%20an%20advanced%20stage.%20The%20dataset%2C%20comprising%2012%2C427%20images%20from%0Amultiple%20hospitals%20in%20Dhaka%2C%20was%20categorized%20into%20four%20groups%3A%20cyst%2C%20tumor%2C%0Astone%2C%20and%20normal.%20Our%20methodology%20aims%20to%20enhance%20CT%20scan%20image%20quality%20using%0ACropping%2C%20Resizing%2C%20and%20CALHE%20techniques%2C%20followed%20by%20feature%20extraction%20with%0Aour%20proposed%20Adaptive%20Local%20Binary%20Pattern%20%28A-LBP%29%20feature%20extraction%20method%0Acompared%20with%20the%20state-of-the-art%20local%20binary%20pattern%20%28LBP%29%20method.%20Our%0Aproposed%20features%20fed%20into%20classifiers%20such%20as%20Random%20Forest%2C%20Decision%20Tree%2C%0ANaive%20Bayes%2C%20K-Nearest%20Neighbor%2C%20and%20SVM.%20We%20explored%20an%20ensemble%20model%20with%0Asoft%20voting%20to%20get%20a%20more%20robust%20model%20for%20our%20task.%20We%20got%20the%20highest%20of%20more%0Athan%2099%25%20in%20accuracy%20using%20our%20feature%20descriptor%20and%20ensembling%20five%0Aclassifiers%20%28Random%20Forest%2C%20Decision%20Tree%2C%20Naive%20Bayes%2C%20K-Nearest%20Neighbor%2C%0ASupport%20Vector%20Machine%29%20with%20the%20soft%20voting%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14560v2&entry.124074799=Read"},
{"title": "NeuraChip: Accelerating GNN Computations with a Hash-based Decoupled\n  Spatial Accelerator", "author": "Kaustubh Shivdikar and Nicolas Bohm Agostini and Malith Jayaweera and Gilbert Jonatan and Jose L. Abellan and Ajay Joshi and John Kim and David Kaeli", "abstract": "  Graph Neural Networks (GNNs) are emerging as a formidable tool for processing\nnon-euclidean data across various domains, ranging from social network analysis\nto bioinformatics. Despite their effectiveness, their adoption has not been\npervasive because of scalability challenges associated with large-scale graph\ndatasets, particularly when leveraging message passing.\n  To tackle these challenges, we introduce NeuraChip, a novel GNN spatial\naccelerator based on Gustavson's algorithm. NeuraChip decouples the\nmultiplication and addition computations in sparse matrix multiplication. This\nseparation allows for independent exploitation of their unique data\ndependencies, facilitating efficient resource allocation. We introduce a\nrolling eviction strategy to mitigate data idling in on-chip memory as well as\naddress the prevalent issue of memory bloat in sparse graph computations.\nFurthermore, the compute resource load balancing is achieved through a dynamic\nreseeding hash-based mapping, ensuring uniform utilization of computing\nresources agnostic of sparsity patterns. Finally, we present NeuraSim, an\nopen-source, cycle-accurate, multi-threaded, modular simulator for\ncomprehensive performance analysis.\n  Overall, NeuraChip presents a significant improvement, yielding an average\nspeedup of 22.1x over Intel's MKL, 17.1x over NVIDIA's cuSPARSE, 16.7x over\nAMD's hipSPARSE, and 1.5x over prior state-of-the-art SpGEMM accelerator and\n1.3x over GNN accelerator. The source code for our open-sourced simulator and\nperformance visualizer is publicly accessible on GitHub https://neurachip.us\n", "link": "http://arxiv.org/abs/2404.15510v2", "date": "2024-04-25", "relevancy": 2.0714, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.536}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5052}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5043}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NeuraChip%3A%20Accelerating%20GNN%20Computations%20with%20a%20Hash-based%20Decoupled%0A%20%20Spatial%20Accelerator&body=Title%3A%20NeuraChip%3A%20Accelerating%20GNN%20Computations%20with%20a%20Hash-based%20Decoupled%0A%20%20Spatial%20Accelerator%0AAuthor%3A%20Kaustubh%20Shivdikar%20and%20Nicolas%20Bohm%20Agostini%20and%20Malith%20Jayaweera%20and%20Gilbert%20Jonatan%20and%20Jose%20L.%20Abellan%20and%20Ajay%20Joshi%20and%20John%20Kim%20and%20David%20Kaeli%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20emerging%20as%20a%20formidable%20tool%20for%20processing%0Anon-euclidean%20data%20across%20various%20domains%2C%20ranging%20from%20social%20network%20analysis%0Ato%20bioinformatics.%20Despite%20their%20effectiveness%2C%20their%20adoption%20has%20not%20been%0Apervasive%20because%20of%20scalability%20challenges%20associated%20with%20large-scale%20graph%0Adatasets%2C%20particularly%20when%20leveraging%20message%20passing.%0A%20%20To%20tackle%20these%20challenges%2C%20we%20introduce%20NeuraChip%2C%20a%20novel%20GNN%20spatial%0Aaccelerator%20based%20on%20Gustavson%27s%20algorithm.%20NeuraChip%20decouples%20the%0Amultiplication%20and%20addition%20computations%20in%20sparse%20matrix%20multiplication.%20This%0Aseparation%20allows%20for%20independent%20exploitation%20of%20their%20unique%20data%0Adependencies%2C%20facilitating%20efficient%20resource%20allocation.%20We%20introduce%20a%0Arolling%20eviction%20strategy%20to%20mitigate%20data%20idling%20in%20on-chip%20memory%20as%20well%20as%0Aaddress%20the%20prevalent%20issue%20of%20memory%20bloat%20in%20sparse%20graph%20computations.%0AFurthermore%2C%20the%20compute%20resource%20load%20balancing%20is%20achieved%20through%20a%20dynamic%0Areseeding%20hash-based%20mapping%2C%20ensuring%20uniform%20utilization%20of%20computing%0Aresources%20agnostic%20of%20sparsity%20patterns.%20Finally%2C%20we%20present%20NeuraSim%2C%20an%0Aopen-source%2C%20cycle-accurate%2C%20multi-threaded%2C%20modular%20simulator%20for%0Acomprehensive%20performance%20analysis.%0A%20%20Overall%2C%20NeuraChip%20presents%20a%20significant%20improvement%2C%20yielding%20an%20average%0Aspeedup%20of%2022.1x%20over%20Intel%27s%20MKL%2C%2017.1x%20over%20NVIDIA%27s%20cuSPARSE%2C%2016.7x%20over%0AAMD%27s%20hipSPARSE%2C%20and%201.5x%20over%20prior%20state-of-the-art%20SpGEMM%20accelerator%20and%0A1.3x%20over%20GNN%20accelerator.%20The%20source%20code%20for%20our%20open-sourced%20simulator%20and%0Aperformance%20visualizer%20is%20publicly%20accessible%20on%20GitHub%20https%3A//neurachip.us%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15510v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuraChip%3A%20Accelerating%20GNN%20Computations%20with%20a%20Hash-based%20Decoupled%0A%20%20Spatial%20Accelerator&entry.906535625=Kaustubh%20Shivdikar%20and%20Nicolas%20Bohm%20Agostini%20and%20Malith%20Jayaweera%20and%20Gilbert%20Jonatan%20and%20Jose%20L.%20Abellan%20and%20Ajay%20Joshi%20and%20John%20Kim%20and%20David%20Kaeli&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20emerging%20as%20a%20formidable%20tool%20for%20processing%0Anon-euclidean%20data%20across%20various%20domains%2C%20ranging%20from%20social%20network%20analysis%0Ato%20bioinformatics.%20Despite%20their%20effectiveness%2C%20their%20adoption%20has%20not%20been%0Apervasive%20because%20of%20scalability%20challenges%20associated%20with%20large-scale%20graph%0Adatasets%2C%20particularly%20when%20leveraging%20message%20passing.%0A%20%20To%20tackle%20these%20challenges%2C%20we%20introduce%20NeuraChip%2C%20a%20novel%20GNN%20spatial%0Aaccelerator%20based%20on%20Gustavson%27s%20algorithm.%20NeuraChip%20decouples%20the%0Amultiplication%20and%20addition%20computations%20in%20sparse%20matrix%20multiplication.%20This%0Aseparation%20allows%20for%20independent%20exploitation%20of%20their%20unique%20data%0Adependencies%2C%20facilitating%20efficient%20resource%20allocation.%20We%20introduce%20a%0Arolling%20eviction%20strategy%20to%20mitigate%20data%20idling%20in%20on-chip%20memory%20as%20well%20as%0Aaddress%20the%20prevalent%20issue%20of%20memory%20bloat%20in%20sparse%20graph%20computations.%0AFurthermore%2C%20the%20compute%20resource%20load%20balancing%20is%20achieved%20through%20a%20dynamic%0Areseeding%20hash-based%20mapping%2C%20ensuring%20uniform%20utilization%20of%20computing%0Aresources%20agnostic%20of%20sparsity%20patterns.%20Finally%2C%20we%20present%20NeuraSim%2C%20an%0Aopen-source%2C%20cycle-accurate%2C%20multi-threaded%2C%20modular%20simulator%20for%0Acomprehensive%20performance%20analysis.%0A%20%20Overall%2C%20NeuraChip%20presents%20a%20significant%20improvement%2C%20yielding%20an%20average%0Aspeedup%20of%2022.1x%20over%20Intel%27s%20MKL%2C%2017.1x%20over%20NVIDIA%27s%20cuSPARSE%2C%2016.7x%20over%0AAMD%27s%20hipSPARSE%2C%20and%201.5x%20over%20prior%20state-of-the-art%20SpGEMM%20accelerator%20and%0A1.3x%20over%20GNN%20accelerator.%20The%20source%20code%20for%20our%20open-sourced%20simulator%20and%0Aperformance%20visualizer%20is%20publicly%20accessible%20on%20GitHub%20https%3A//neurachip.us%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15510v2&entry.124074799=Read"},
{"title": "RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT\n  Analysis", "author": "Xiaoman Zhang and Chaoyi Wu and Ziheng Zhao and Jiayu Lei and Ya Zhang and Yanfeng Wang and Weidi Xie", "abstract": "  Developing generalist foundation model has recently attracted tremendous\nattention among researchers in the field of AI for Medicine (AI4Medicine). A\npivotal insight in developing these models is their reliance on dataset\nscaling, which emphasizes the requirements on developing open-source medical\nimage datasets that incorporate diverse supervision signals across various\nimaging modalities. In this paper, we introduce RadGenome-Chest CT, a\ncomprehensive, large-scale, region-guided 3D chest CT interpretation dataset\nbased on CT-RATE. Specifically, we leverage the latest powerful universal\nsegmentation and large language models, to extend the original datasets (over\n25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) from\nthe following aspects: (i) organ-level segmentation masks covering 197\ncategories, which provide intermediate reasoning visual clues for\ninterpretation; (ii) 665 K multi-granularity grounded reports, where each\nsentence of the report is linked to the corresponding anatomical region of CT\nvolume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs,\nwhere questions and answers are all linked with reference segmentation masks,\nenabling models to associate visual evidence with textual explanations. All\ngrounded reports and VQA pairs in the validation set have gone through manual\nverification to ensure dataset quality. We believe that RadGenome-Chest CT can\nsignificantly advance the development of multimodal medical foundation models,\nby training to generate texts based on given segmentation regions, which is\nunattainable with previous relevant datasets. We will release all segmentation\nmasks, grounded reports, and VQA pairs to facilitate further research and\ndevelopment in this field.\n", "link": "http://arxiv.org/abs/2404.16754v1", "date": "2024-04-25", "relevancy": 2.0693, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5226}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4994}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RadGenome-Chest%20CT%3A%20A%20Grounded%20Vision-Language%20Dataset%20for%20Chest%20CT%0A%20%20Analysis&body=Title%3A%20RadGenome-Chest%20CT%3A%20A%20Grounded%20Vision-Language%20Dataset%20for%20Chest%20CT%0A%20%20Analysis%0AAuthor%3A%20Xiaoman%20Zhang%20and%20Chaoyi%20Wu%20and%20Ziheng%20Zhao%20and%20Jiayu%20Lei%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20Developing%20generalist%20foundation%20model%20has%20recently%20attracted%20tremendous%0Aattention%20among%20researchers%20in%20the%20field%20of%20AI%20for%20Medicine%20%28AI4Medicine%29.%20A%0Apivotal%20insight%20in%20developing%20these%20models%20is%20their%20reliance%20on%20dataset%0Ascaling%2C%20which%20emphasizes%20the%20requirements%20on%20developing%20open-source%20medical%0Aimage%20datasets%20that%20incorporate%20diverse%20supervision%20signals%20across%20various%0Aimaging%20modalities.%20In%20this%20paper%2C%20we%20introduce%20RadGenome-Chest%20CT%2C%20a%0Acomprehensive%2C%20large-scale%2C%20region-guided%203D%20chest%20CT%20interpretation%20dataset%0Abased%20on%20CT-RATE.%20Specifically%2C%20we%20leverage%20the%20latest%20powerful%20universal%0Asegmentation%20and%20large%20language%20models%2C%20to%20extend%20the%20original%20datasets%20%28over%0A25%2C692%20non-contrast%203D%20chest%20CT%20volume%20and%20reports%20from%2020%2C000%20patients%29%20from%0Athe%20following%20aspects%3A%20%28i%29%20organ-level%20segmentation%20masks%20covering%20197%0Acategories%2C%20which%20provide%20intermediate%20reasoning%20visual%20clues%20for%0Ainterpretation%3B%20%28ii%29%20665%20K%20multi-granularity%20grounded%20reports%2C%20where%20each%0Asentence%20of%20the%20report%20is%20linked%20to%20the%20corresponding%20anatomical%20region%20of%20CT%0Avolume%20in%20the%20form%20of%20a%20segmentation%20mask%3B%20%28iii%29%201.3%20M%20grounded%20VQA%20pairs%2C%0Awhere%20questions%20and%20answers%20are%20all%20linked%20with%20reference%20segmentation%20masks%2C%0Aenabling%20models%20to%20associate%20visual%20evidence%20with%20textual%20explanations.%20All%0Agrounded%20reports%20and%20VQA%20pairs%20in%20the%20validation%20set%20have%20gone%20through%20manual%0Averification%20to%20ensure%20dataset%20quality.%20We%20believe%20that%20RadGenome-Chest%20CT%20can%0Asignificantly%20advance%20the%20development%20of%20multimodal%20medical%20foundation%20models%2C%0Aby%20training%20to%20generate%20texts%20based%20on%20given%20segmentation%20regions%2C%20which%20is%0Aunattainable%20with%20previous%20relevant%20datasets.%20We%20will%20release%20all%20segmentation%0Amasks%2C%20grounded%20reports%2C%20and%20VQA%20pairs%20to%20facilitate%20further%20research%20and%0Adevelopment%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16754v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadGenome-Chest%20CT%3A%20A%20Grounded%20Vision-Language%20Dataset%20for%20Chest%20CT%0A%20%20Analysis&entry.906535625=Xiaoman%20Zhang%20and%20Chaoyi%20Wu%20and%20Ziheng%20Zhao%20and%20Jiayu%20Lei%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie&entry.1292438233=%20%20Developing%20generalist%20foundation%20model%20has%20recently%20attracted%20tremendous%0Aattention%20among%20researchers%20in%20the%20field%20of%20AI%20for%20Medicine%20%28AI4Medicine%29.%20A%0Apivotal%20insight%20in%20developing%20these%20models%20is%20their%20reliance%20on%20dataset%0Ascaling%2C%20which%20emphasizes%20the%20requirements%20on%20developing%20open-source%20medical%0Aimage%20datasets%20that%20incorporate%20diverse%20supervision%20signals%20across%20various%0Aimaging%20modalities.%20In%20this%20paper%2C%20we%20introduce%20RadGenome-Chest%20CT%2C%20a%0Acomprehensive%2C%20large-scale%2C%20region-guided%203D%20chest%20CT%20interpretation%20dataset%0Abased%20on%20CT-RATE.%20Specifically%2C%20we%20leverage%20the%20latest%20powerful%20universal%0Asegmentation%20and%20large%20language%20models%2C%20to%20extend%20the%20original%20datasets%20%28over%0A25%2C692%20non-contrast%203D%20chest%20CT%20volume%20and%20reports%20from%2020%2C000%20patients%29%20from%0Athe%20following%20aspects%3A%20%28i%29%20organ-level%20segmentation%20masks%20covering%20197%0Acategories%2C%20which%20provide%20intermediate%20reasoning%20visual%20clues%20for%0Ainterpretation%3B%20%28ii%29%20665%20K%20multi-granularity%20grounded%20reports%2C%20where%20each%0Asentence%20of%20the%20report%20is%20linked%20to%20the%20corresponding%20anatomical%20region%20of%20CT%0Avolume%20in%20the%20form%20of%20a%20segmentation%20mask%3B%20%28iii%29%201.3%20M%20grounded%20VQA%20pairs%2C%0Awhere%20questions%20and%20answers%20are%20all%20linked%20with%20reference%20segmentation%20masks%2C%0Aenabling%20models%20to%20associate%20visual%20evidence%20with%20textual%20explanations.%20All%0Agrounded%20reports%20and%20VQA%20pairs%20in%20the%20validation%20set%20have%20gone%20through%20manual%0Averification%20to%20ensure%20dataset%20quality.%20We%20believe%20that%20RadGenome-Chest%20CT%20can%0Asignificantly%20advance%20the%20development%20of%20multimodal%20medical%20foundation%20models%2C%0Aby%20training%20to%20generate%20texts%20based%20on%20given%20segmentation%20regions%2C%20which%20is%0Aunattainable%20with%20previous%20relevant%20datasets.%20We%20will%20release%20all%20segmentation%0Amasks%2C%20grounded%20reports%2C%20and%20VQA%20pairs%20to%20facilitate%20further%20research%20and%0Adevelopment%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16754v1&entry.124074799=Read"},
{"title": "Conformal Prediction of Motion Control Performance for an Automated\n  Vehicle in Presence of Actuator Degradations and Failures", "author": "Richard Schubert and Marvin Loba and Jasper S\u00fcnnemann and Torben Stolte and Markus Maurer", "abstract": "  Automated driving systems require monitoring mechanisms to ensure safe\noperation, especially if system components degrade or fail. Their runtime\nself-representation plays a key role as it provides a-priori knowledge about\nthe system's capabilities and limitations. In this paper, we propose a\ndata-driven approach for deriving such a self-representation model for the\nmotion controller of an automated vehicle. A conformalized prediction model is\nlearned and allows estimating how operational conditions as well as potential\ndegradations and failures of the vehicle's actuators impact motion control\nperformance. During runtime behavior generation, our predictor can provide a\nheuristic for determining the admissible action space.\n", "link": "http://arxiv.org/abs/2404.16500v1", "date": "2024-04-25", "relevancy": 2.0692, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5828}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5032}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20of%20Motion%20Control%20Performance%20for%20an%20Automated%0A%20%20Vehicle%20in%20Presence%20of%20Actuator%20Degradations%20and%20Failures&body=Title%3A%20Conformal%20Prediction%20of%20Motion%20Control%20Performance%20for%20an%20Automated%0A%20%20Vehicle%20in%20Presence%20of%20Actuator%20Degradations%20and%20Failures%0AAuthor%3A%20Richard%20Schubert%20and%20Marvin%20Loba%20and%20Jasper%20S%C3%BCnnemann%20and%20Torben%20Stolte%20and%20Markus%20Maurer%0AAbstract%3A%20%20%20Automated%20driving%20systems%20require%20monitoring%20mechanisms%20to%20ensure%20safe%0Aoperation%2C%20especially%20if%20system%20components%20degrade%20or%20fail.%20Their%20runtime%0Aself-representation%20plays%20a%20key%20role%20as%20it%20provides%20a-priori%20knowledge%20about%0Athe%20system%27s%20capabilities%20and%20limitations.%20In%20this%20paper%2C%20we%20propose%20a%0Adata-driven%20approach%20for%20deriving%20such%20a%20self-representation%20model%20for%20the%0Amotion%20controller%20of%20an%20automated%20vehicle.%20A%20conformalized%20prediction%20model%20is%0Alearned%20and%20allows%20estimating%20how%20operational%20conditions%20as%20well%20as%20potential%0Adegradations%20and%20failures%20of%20the%20vehicle%27s%20actuators%20impact%20motion%20control%0Aperformance.%20During%20runtime%20behavior%20generation%2C%20our%20predictor%20can%20provide%20a%0Aheuristic%20for%20determining%20the%20admissible%20action%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16500v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20of%20Motion%20Control%20Performance%20for%20an%20Automated%0A%20%20Vehicle%20in%20Presence%20of%20Actuator%20Degradations%20and%20Failures&entry.906535625=Richard%20Schubert%20and%20Marvin%20Loba%20and%20Jasper%20S%C3%BCnnemann%20and%20Torben%20Stolte%20and%20Markus%20Maurer&entry.1292438233=%20%20Automated%20driving%20systems%20require%20monitoring%20mechanisms%20to%20ensure%20safe%0Aoperation%2C%20especially%20if%20system%20components%20degrade%20or%20fail.%20Their%20runtime%0Aself-representation%20plays%20a%20key%20role%20as%20it%20provides%20a-priori%20knowledge%20about%0Athe%20system%27s%20capabilities%20and%20limitations.%20In%20this%20paper%2C%20we%20propose%20a%0Adata-driven%20approach%20for%20deriving%20such%20a%20self-representation%20model%20for%20the%0Amotion%20controller%20of%20an%20automated%20vehicle.%20A%20conformalized%20prediction%20model%20is%0Alearned%20and%20allows%20estimating%20how%20operational%20conditions%20as%20well%20as%20potential%0Adegradations%20and%20failures%20of%20the%20vehicle%27s%20actuators%20impact%20motion%20control%0Aperformance.%20During%20runtime%20behavior%20generation%2C%20our%20predictor%20can%20provide%20a%0Aheuristic%20for%20determining%20the%20admissible%20action%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16500v1&entry.124074799=Read"},
{"title": "Multi-scale HSV Color Feature Embedding for High-fidelity NIR-to-RGB\n  Spectrum Translation", "author": "Huiyu Zhai and Mo Chen and Xingxing Yang and Gusheng Kang", "abstract": "  The NIR-to-RGB spectral domain translation is a formidable task due to the\ninherent spectral mapping ambiguities within NIR inputs and RGB outputs. Thus,\nexisting methods fail to reconcile the tension between maintaining texture\ndetail fidelity and achieving diverse color variations. In this paper, we\npropose a Multi-scale HSV Color Feature Embedding Network (MCFNet) that\ndecomposes the mapping process into three sub-tasks, including NIR texture\nmaintenance, coarse geometry reconstruction, and RGB color prediction. Thus, we\npropose three key modules for each corresponding sub-task: the Texture\nPreserving Block (TPB), the HSV Color Feature Embedding Module (HSV-CFEM), and\nthe Geometry Reconstruction Module (GRM). These modules contribute to our\nMCFNet methodically tackling spectral translation through a series of\nescalating resolutions, progressively enriching images with color and texture\nfidelity in a scale-coherent fashion. The proposed MCFNet demonstrates\nsubstantial performance gains over the NIR image colorization task. Code is\nreleased at: https://github.com/AlexYangxx/MCFNet.\n", "link": "http://arxiv.org/abs/2404.16685v1", "date": "2024-04-25", "relevancy": 2.0647, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5227}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5182}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4948}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20HSV%20Color%20Feature%20Embedding%20for%20High-fidelity%20NIR-to-RGB%0A%20%20Spectrum%20Translation&body=Title%3A%20Multi-scale%20HSV%20Color%20Feature%20Embedding%20for%20High-fidelity%20NIR-to-RGB%0A%20%20Spectrum%20Translation%0AAuthor%3A%20Huiyu%20Zhai%20and%20Mo%20Chen%20and%20Xingxing%20Yang%20and%20Gusheng%20Kang%0AAbstract%3A%20%20%20The%20NIR-to-RGB%20spectral%20domain%20translation%20is%20a%20formidable%20task%20due%20to%20the%0Ainherent%20spectral%20mapping%20ambiguities%20within%20NIR%20inputs%20and%20RGB%20outputs.%20Thus%2C%0Aexisting%20methods%20fail%20to%20reconcile%20the%20tension%20between%20maintaining%20texture%0Adetail%20fidelity%20and%20achieving%20diverse%20color%20variations.%20In%20this%20paper%2C%20we%0Apropose%20a%20Multi-scale%20HSV%20Color%20Feature%20Embedding%20Network%20%28MCFNet%29%20that%0Adecomposes%20the%20mapping%20process%20into%20three%20sub-tasks%2C%20including%20NIR%20texture%0Amaintenance%2C%20coarse%20geometry%20reconstruction%2C%20and%20RGB%20color%20prediction.%20Thus%2C%20we%0Apropose%20three%20key%20modules%20for%20each%20corresponding%20sub-task%3A%20the%20Texture%0APreserving%20Block%20%28TPB%29%2C%20the%20HSV%20Color%20Feature%20Embedding%20Module%20%28HSV-CFEM%29%2C%20and%0Athe%20Geometry%20Reconstruction%20Module%20%28GRM%29.%20These%20modules%20contribute%20to%20our%0AMCFNet%20methodically%20tackling%20spectral%20translation%20through%20a%20series%20of%0Aescalating%20resolutions%2C%20progressively%20enriching%20images%20with%20color%20and%20texture%0Afidelity%20in%20a%20scale-coherent%20fashion.%20The%20proposed%20MCFNet%20demonstrates%0Asubstantial%20performance%20gains%20over%20the%20NIR%20image%20colorization%20task.%20Code%20is%0Areleased%20at%3A%20https%3A//github.com/AlexYangxx/MCFNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16685v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20HSV%20Color%20Feature%20Embedding%20for%20High-fidelity%20NIR-to-RGB%0A%20%20Spectrum%20Translation&entry.906535625=Huiyu%20Zhai%20and%20Mo%20Chen%20and%20Xingxing%20Yang%20and%20Gusheng%20Kang&entry.1292438233=%20%20The%20NIR-to-RGB%20spectral%20domain%20translation%20is%20a%20formidable%20task%20due%20to%20the%0Ainherent%20spectral%20mapping%20ambiguities%20within%20NIR%20inputs%20and%20RGB%20outputs.%20Thus%2C%0Aexisting%20methods%20fail%20to%20reconcile%20the%20tension%20between%20maintaining%20texture%0Adetail%20fidelity%20and%20achieving%20diverse%20color%20variations.%20In%20this%20paper%2C%20we%0Apropose%20a%20Multi-scale%20HSV%20Color%20Feature%20Embedding%20Network%20%28MCFNet%29%20that%0Adecomposes%20the%20mapping%20process%20into%20three%20sub-tasks%2C%20including%20NIR%20texture%0Amaintenance%2C%20coarse%20geometry%20reconstruction%2C%20and%20RGB%20color%20prediction.%20Thus%2C%20we%0Apropose%20three%20key%20modules%20for%20each%20corresponding%20sub-task%3A%20the%20Texture%0APreserving%20Block%20%28TPB%29%2C%20the%20HSV%20Color%20Feature%20Embedding%20Module%20%28HSV-CFEM%29%2C%20and%0Athe%20Geometry%20Reconstruction%20Module%20%28GRM%29.%20These%20modules%20contribute%20to%20our%0AMCFNet%20methodically%20tackling%20spectral%20translation%20through%20a%20series%20of%0Aescalating%20resolutions%2C%20progressively%20enriching%20images%20with%20color%20and%20texture%0Afidelity%20in%20a%20scale-coherent%20fashion.%20The%20proposed%20MCFNet%20demonstrates%0Asubstantial%20performance%20gains%20over%20the%20NIR%20image%20colorization%20task.%20Code%20is%0Areleased%20at%3A%20https%3A//github.com/AlexYangxx/MCFNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16685v1&entry.124074799=Read"},
{"title": "TinyChart: Efficient Chart Understanding with Visual Token Merging and\n  Program-of-Thoughts Learning", "author": "Liang Zhang and Anwen Hu and Haiyang Xu and Ming Yan and Yichen Xu and Qin Jin and Ji Zhang and Fei Huang", "abstract": "  Charts are important for presenting and explaining complex data\nrelationships. Recently, multimodal large language models (MLLMs) have shown\nremarkable capabilities in various chart understanding tasks. However, the\nsheer size of these models in terms of parameters and computational\nrequirements limits their use in resource-constrained environments. In this\npaper, we present TinyChart, an efficient MLLM for chart understanding with\nonly 3B parameters. TinyChart overcomes two key challenges in efficient chart\nunderstanding: (1) reduce the burden of learning numerical computations through\na Program-of-Thoughts (PoT) learning strategy, which trains the model to\ngenerate Python programs for numerical calculations, and (2) reduce lengthy\nvision feature sequences produced by the vision transformer for high-resolution\nimages through a Vision Token Merging module, which gradually merges most\nsimilar vision tokens. Extensive experiments demonstrate that our 3B TinyChart\nachieves SOTA performance on a variety of chart understanding benchmarks\nincluding ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It\noutperforms several chart understanding MLLM with up to 13B parameters such as\nChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on\nChartQA. It also demonstrates its superior efficiency with higher throughput\nduring inference due to a smaller model scale and more efficient vision\nencoding. Our code and model are available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.\n", "link": "http://arxiv.org/abs/2404.16635v1", "date": "2024-04-25", "relevancy": 2.0584, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4905}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4885}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TinyChart%3A%20Efficient%20Chart%20Understanding%20with%20Visual%20Token%20Merging%20and%0A%20%20Program-of-Thoughts%20Learning&body=Title%3A%20TinyChart%3A%20Efficient%20Chart%20Understanding%20with%20Visual%20Token%20Merging%20and%0A%20%20Program-of-Thoughts%20Learning%0AAuthor%3A%20Liang%20Zhang%20and%20Anwen%20Hu%20and%20Haiyang%20Xu%20and%20Ming%20Yan%20and%20Yichen%20Xu%20and%20Qin%20Jin%20and%20Ji%20Zhang%20and%20Fei%20Huang%0AAbstract%3A%20%20%20Charts%20are%20important%20for%20presenting%20and%20explaining%20complex%20data%0Arelationships.%20Recently%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%0Aremarkable%20capabilities%20in%20various%20chart%20understanding%20tasks.%20However%2C%20the%0Asheer%20size%20of%20these%20models%20in%20terms%20of%20parameters%20and%20computational%0Arequirements%20limits%20their%20use%20in%20resource-constrained%20environments.%20In%20this%0Apaper%2C%20we%20present%20TinyChart%2C%20an%20efficient%20MLLM%20for%20chart%20understanding%20with%0Aonly%203B%20parameters.%20TinyChart%20overcomes%20two%20key%20challenges%20in%20efficient%20chart%0Aunderstanding%3A%20%281%29%20reduce%20the%20burden%20of%20learning%20numerical%20computations%20through%0Aa%20Program-of-Thoughts%20%28PoT%29%20learning%20strategy%2C%20which%20trains%20the%20model%20to%0Agenerate%20Python%20programs%20for%20numerical%20calculations%2C%20and%20%282%29%20reduce%20lengthy%0Avision%20feature%20sequences%20produced%20by%20the%20vision%20transformer%20for%20high-resolution%0Aimages%20through%20a%20Vision%20Token%20Merging%20module%2C%20which%20gradually%20merges%20most%0Asimilar%20vision%20tokens.%20Extensive%20experiments%20demonstrate%20that%20our%203B%20TinyChart%0Aachieves%20SOTA%20performance%20on%20a%20variety%20of%20chart%20understanding%20benchmarks%0Aincluding%20ChartQA%2C%20Chart-to-Text%2C%20Chart-to-Table%2C%20OpenCQA%2C%20and%20ChartX.%20It%0Aoutperforms%20several%20chart%20understanding%20MLLM%20with%20up%20to%2013B%20parameters%20such%20as%0AChartLlama%20and%20ChartAst%2C%20and%20close-sourced%20general-purpose%20MLLM%20GPT-4V%20on%0AChartQA.%20It%20also%20demonstrates%20its%20superior%20efficiency%20with%20higher%20throughput%0Aduring%20inference%20due%20to%20a%20smaller%20model%20scale%20and%20more%20efficient%20vision%0Aencoding.%20Our%20code%20and%20model%20are%20available%20at%0Ahttps%3A//github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16635v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinyChart%3A%20Efficient%20Chart%20Understanding%20with%20Visual%20Token%20Merging%20and%0A%20%20Program-of-Thoughts%20Learning&entry.906535625=Liang%20Zhang%20and%20Anwen%20Hu%20and%20Haiyang%20Xu%20and%20Ming%20Yan%20and%20Yichen%20Xu%20and%20Qin%20Jin%20and%20Ji%20Zhang%20and%20Fei%20Huang&entry.1292438233=%20%20Charts%20are%20important%20for%20presenting%20and%20explaining%20complex%20data%0Arelationships.%20Recently%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%0Aremarkable%20capabilities%20in%20various%20chart%20understanding%20tasks.%20However%2C%20the%0Asheer%20size%20of%20these%20models%20in%20terms%20of%20parameters%20and%20computational%0Arequirements%20limits%20their%20use%20in%20resource-constrained%20environments.%20In%20this%0Apaper%2C%20we%20present%20TinyChart%2C%20an%20efficient%20MLLM%20for%20chart%20understanding%20with%0Aonly%203B%20parameters.%20TinyChart%20overcomes%20two%20key%20challenges%20in%20efficient%20chart%0Aunderstanding%3A%20%281%29%20reduce%20the%20burden%20of%20learning%20numerical%20computations%20through%0Aa%20Program-of-Thoughts%20%28PoT%29%20learning%20strategy%2C%20which%20trains%20the%20model%20to%0Agenerate%20Python%20programs%20for%20numerical%20calculations%2C%20and%20%282%29%20reduce%20lengthy%0Avision%20feature%20sequences%20produced%20by%20the%20vision%20transformer%20for%20high-resolution%0Aimages%20through%20a%20Vision%20Token%20Merging%20module%2C%20which%20gradually%20merges%20most%0Asimilar%20vision%20tokens.%20Extensive%20experiments%20demonstrate%20that%20our%203B%20TinyChart%0Aachieves%20SOTA%20performance%20on%20a%20variety%20of%20chart%20understanding%20benchmarks%0Aincluding%20ChartQA%2C%20Chart-to-Text%2C%20Chart-to-Table%2C%20OpenCQA%2C%20and%20ChartX.%20It%0Aoutperforms%20several%20chart%20understanding%20MLLM%20with%20up%20to%2013B%20parameters%20such%20as%0AChartLlama%20and%20ChartAst%2C%20and%20close-sourced%20general-purpose%20MLLM%20GPT-4V%20on%0AChartQA.%20It%20also%20demonstrates%20its%20superior%20efficiency%20with%20higher%20throughput%0Aduring%20inference%20due%20to%20a%20smaller%20model%20scale%20and%20more%20efficient%20vision%0Aencoding.%20Our%20code%20and%20model%20are%20available%20at%0Ahttps%3A//github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16635v1&entry.124074799=Read"},
{"title": "On the Stability of a non-hyperbolic nonlinear map with non-bounded set\n  of non-isolated fixed points with applications to Machine Learning", "author": "Roberta Hansen and Matias Vera and Lautaro Estienne and Luciana Ferrer and Pablo Piantanida", "abstract": "  This paper deals with the convergence analysis of the SUCPA (Semi\nUnsupervised Calibration through Prior Adaptation) algorithm, defined from a\nfirst-order non-linear difference equations, first developed to correct the\nscores output by a supervised machine learning classifier. The convergence\nanalysis is addressed as a dynamical system problem, by studying the local and\nglobal stability of the nonlinear map derived from the algorithm. This map,\nwhich is defined by a composition of exponential and rational functions, turns\nout to be non-hyperbolic with a non-bounded set of non-isolated fixed points.\nHence, a non-standard method for solving the convergence analysis is used\nconsisting of an ad-hoc geometrical approach. For a binary classification\nproblem (two-dimensional map), we rigorously prove that the map is globally\nasymptotically stable. Numerical experiments on real-world application are\nperformed to support the theoretical results by means of two different\nclassification problems: Sentiment Polarity performed with a Large Language\nModel and Cat-Dog Image classification. For a greater number of classes, the\nnumerical evidence shows the same behavior of the algorithm, and this is\nillustrated with a Natural Language Inference example. The experiment codes are\npublicly accessible online at the following repository:\nhttps://github.com/LautaroEst/sucpa-convergence\n", "link": "http://arxiv.org/abs/2401.03051v2", "date": "2024-04-25", "relevancy": 2.0531, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4984}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4494}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Stability%20of%20a%20non-hyperbolic%20nonlinear%20map%20with%20non-bounded%20set%0A%20%20of%20non-isolated%20fixed%20points%20with%20applications%20to%20Machine%20Learning&body=Title%3A%20On%20the%20Stability%20of%20a%20non-hyperbolic%20nonlinear%20map%20with%20non-bounded%20set%0A%20%20of%20non-isolated%20fixed%20points%20with%20applications%20to%20Machine%20Learning%0AAuthor%3A%20Roberta%20Hansen%20and%20Matias%20Vera%20and%20Lautaro%20Estienne%20and%20Luciana%20Ferrer%20and%20Pablo%20Piantanida%0AAbstract%3A%20%20%20This%20paper%20deals%20with%20the%20convergence%20analysis%20of%20the%20SUCPA%20%28Semi%0AUnsupervised%20Calibration%20through%20Prior%20Adaptation%29%20algorithm%2C%20defined%20from%20a%0Afirst-order%20non-linear%20difference%20equations%2C%20first%20developed%20to%20correct%20the%0Ascores%20output%20by%20a%20supervised%20machine%20learning%20classifier.%20The%20convergence%0Aanalysis%20is%20addressed%20as%20a%20dynamical%20system%20problem%2C%20by%20studying%20the%20local%20and%0Aglobal%20stability%20of%20the%20nonlinear%20map%20derived%20from%20the%20algorithm.%20This%20map%2C%0Awhich%20is%20defined%20by%20a%20composition%20of%20exponential%20and%20rational%20functions%2C%20turns%0Aout%20to%20be%20non-hyperbolic%20with%20a%20non-bounded%20set%20of%20non-isolated%20fixed%20points.%0AHence%2C%20a%20non-standard%20method%20for%20solving%20the%20convergence%20analysis%20is%20used%0Aconsisting%20of%20an%20ad-hoc%20geometrical%20approach.%20For%20a%20binary%20classification%0Aproblem%20%28two-dimensional%20map%29%2C%20we%20rigorously%20prove%20that%20the%20map%20is%20globally%0Aasymptotically%20stable.%20Numerical%20experiments%20on%20real-world%20application%20are%0Aperformed%20to%20support%20the%20theoretical%20results%20by%20means%20of%20two%20different%0Aclassification%20problems%3A%20Sentiment%20Polarity%20performed%20with%20a%20Large%20Language%0AModel%20and%20Cat-Dog%20Image%20classification.%20For%20a%20greater%20number%20of%20classes%2C%20the%0Anumerical%20evidence%20shows%20the%20same%20behavior%20of%20the%20algorithm%2C%20and%20this%20is%0Aillustrated%20with%20a%20Natural%20Language%20Inference%20example.%20The%20experiment%20codes%20are%0Apublicly%20accessible%20online%20at%20the%20following%20repository%3A%0Ahttps%3A//github.com/LautaroEst/sucpa-convergence%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03051v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Stability%20of%20a%20non-hyperbolic%20nonlinear%20map%20with%20non-bounded%20set%0A%20%20of%20non-isolated%20fixed%20points%20with%20applications%20to%20Machine%20Learning&entry.906535625=Roberta%20Hansen%20and%20Matias%20Vera%20and%20Lautaro%20Estienne%20and%20Luciana%20Ferrer%20and%20Pablo%20Piantanida&entry.1292438233=%20%20This%20paper%20deals%20with%20the%20convergence%20analysis%20of%20the%20SUCPA%20%28Semi%0AUnsupervised%20Calibration%20through%20Prior%20Adaptation%29%20algorithm%2C%20defined%20from%20a%0Afirst-order%20non-linear%20difference%20equations%2C%20first%20developed%20to%20correct%20the%0Ascores%20output%20by%20a%20supervised%20machine%20learning%20classifier.%20The%20convergence%0Aanalysis%20is%20addressed%20as%20a%20dynamical%20system%20problem%2C%20by%20studying%20the%20local%20and%0Aglobal%20stability%20of%20the%20nonlinear%20map%20derived%20from%20the%20algorithm.%20This%20map%2C%0Awhich%20is%20defined%20by%20a%20composition%20of%20exponential%20and%20rational%20functions%2C%20turns%0Aout%20to%20be%20non-hyperbolic%20with%20a%20non-bounded%20set%20of%20non-isolated%20fixed%20points.%0AHence%2C%20a%20non-standard%20method%20for%20solving%20the%20convergence%20analysis%20is%20used%0Aconsisting%20of%20an%20ad-hoc%20geometrical%20approach.%20For%20a%20binary%20classification%0Aproblem%20%28two-dimensional%20map%29%2C%20we%20rigorously%20prove%20that%20the%20map%20is%20globally%0Aasymptotically%20stable.%20Numerical%20experiments%20on%20real-world%20application%20are%0Aperformed%20to%20support%20the%20theoretical%20results%20by%20means%20of%20two%20different%0Aclassification%20problems%3A%20Sentiment%20Polarity%20performed%20with%20a%20Large%20Language%0AModel%20and%20Cat-Dog%20Image%20classification.%20For%20a%20greater%20number%20of%20classes%2C%20the%0Anumerical%20evidence%20shows%20the%20same%20behavior%20of%20the%20algorithm%2C%20and%20this%20is%0Aillustrated%20with%20a%20Natural%20Language%20Inference%20example.%20The%20experiment%20codes%20are%0Apublicly%20accessible%20online%20at%20the%20following%20repository%3A%0Ahttps%3A//github.com/LautaroEst/sucpa-convergence%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03051v2&entry.124074799=Read"},
{"title": "Neural Interaction Energy for Multi-Agent Trajectory Prediction", "author": "Kaixin Shen and Ruijie Quan and Linchao Zhu and Jun Xiao and Yi Yang", "abstract": "  Maintaining temporal stability is crucial in multi-agent trajectory\nprediction. Insufficient regularization to uphold this stability often results\nin fluctuations in kinematic states, leading to inconsistent predictions and\nthe amplification of errors. In this study, we introduce a framework called\nMulti-Agent Trajectory prediction via neural interaction Energy (MATE). This\nframework assesses the interactive motion of agents by employing neural\ninteraction energy, which captures the dynamics of interactions and illustrates\ntheir influence on the future trajectories of agents. To bolster temporal\nstability, we introduce two constraints: inter-agent interaction constraint and\nintra-agent motion constraint. These constraints work together to ensure\ntemporal stability at both the system and agent levels, effectively mitigating\nprediction fluctuations inherent in multi-agent systems. Comparative\nevaluations against previous methods on four diverse datasets highlight the\nsuperior prediction accuracy and generalization capabilities of our model.\n", "link": "http://arxiv.org/abs/2404.16579v1", "date": "2024-04-25", "relevancy": 2.0504, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5865}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5012}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4944}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Interaction%20Energy%20for%20Multi-Agent%20Trajectory%20Prediction&body=Title%3A%20Neural%20Interaction%20Energy%20for%20Multi-Agent%20Trajectory%20Prediction%0AAuthor%3A%20Kaixin%20Shen%20and%20Ruijie%20Quan%20and%20Linchao%20Zhu%20and%20Jun%20Xiao%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Maintaining%20temporal%20stability%20is%20crucial%20in%20multi-agent%20trajectory%0Aprediction.%20Insufficient%20regularization%20to%20uphold%20this%20stability%20often%20results%0Ain%20fluctuations%20in%20kinematic%20states%2C%20leading%20to%20inconsistent%20predictions%20and%0Athe%20amplification%20of%20errors.%20In%20this%20study%2C%20we%20introduce%20a%20framework%20called%0AMulti-Agent%20Trajectory%20prediction%20via%20neural%20interaction%20Energy%20%28MATE%29.%20This%0Aframework%20assesses%20the%20interactive%20motion%20of%20agents%20by%20employing%20neural%0Ainteraction%20energy%2C%20which%20captures%20the%20dynamics%20of%20interactions%20and%20illustrates%0Atheir%20influence%20on%20the%20future%20trajectories%20of%20agents.%20To%20bolster%20temporal%0Astability%2C%20we%20introduce%20two%20constraints%3A%20inter-agent%20interaction%20constraint%20and%0Aintra-agent%20motion%20constraint.%20These%20constraints%20work%20together%20to%20ensure%0Atemporal%20stability%20at%20both%20the%20system%20and%20agent%20levels%2C%20effectively%20mitigating%0Aprediction%20fluctuations%20inherent%20in%20multi-agent%20systems.%20Comparative%0Aevaluations%20against%20previous%20methods%20on%20four%20diverse%20datasets%20highlight%20the%0Asuperior%20prediction%20accuracy%20and%20generalization%20capabilities%20of%20our%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16579v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Interaction%20Energy%20for%20Multi-Agent%20Trajectory%20Prediction&entry.906535625=Kaixin%20Shen%20and%20Ruijie%20Quan%20and%20Linchao%20Zhu%20and%20Jun%20Xiao%20and%20Yi%20Yang&entry.1292438233=%20%20Maintaining%20temporal%20stability%20is%20crucial%20in%20multi-agent%20trajectory%0Aprediction.%20Insufficient%20regularization%20to%20uphold%20this%20stability%20often%20results%0Ain%20fluctuations%20in%20kinematic%20states%2C%20leading%20to%20inconsistent%20predictions%20and%0Athe%20amplification%20of%20errors.%20In%20this%20study%2C%20we%20introduce%20a%20framework%20called%0AMulti-Agent%20Trajectory%20prediction%20via%20neural%20interaction%20Energy%20%28MATE%29.%20This%0Aframework%20assesses%20the%20interactive%20motion%20of%20agents%20by%20employing%20neural%0Ainteraction%20energy%2C%20which%20captures%20the%20dynamics%20of%20interactions%20and%20illustrates%0Atheir%20influence%20on%20the%20future%20trajectories%20of%20agents.%20To%20bolster%20temporal%0Astability%2C%20we%20introduce%20two%20constraints%3A%20inter-agent%20interaction%20constraint%20and%0Aintra-agent%20motion%20constraint.%20These%20constraints%20work%20together%20to%20ensure%0Atemporal%20stability%20at%20both%20the%20system%20and%20agent%20levels%2C%20effectively%20mitigating%0Aprediction%20fluctuations%20inherent%20in%20multi-agent%20systems.%20Comparative%0Aevaluations%20against%20previous%20methods%20on%20four%20diverse%20datasets%20highlight%20the%0Asuperior%20prediction%20accuracy%20and%20generalization%20capabilities%20of%20our%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16579v1&entry.124074799=Read"},
{"title": "SFMViT: SlowFast Meet ViT in Chaotic World", "author": "Jiaying Lin and Jiajun Wen and Mengyuan Liu and Jinfu Liu and Baiqiao Yin and Yue Li", "abstract": "  The task of spatiotemporal action localization in chaotic scenes is a\nchallenging task toward advanced video understanding. Paving the way with\nhigh-quality video feature extraction and enhancing the precision of\ndetector-predicted anchors can effectively improve model performance. To this\nend, we propose a high-performance dual-stream spatiotemporal feature\nextraction network SFMViT with an anchor pruning strategy. The backbone of our\nSFMViT is composed of ViT and SlowFast with prior knowledge of spatiotemporal\naction localization, which fully utilizes ViT's excellent global feature\nextraction capabilities and SlowFast's spatiotemporal sequence modeling\ncapabilities. Secondly, we introduce the confidence maximum heap to prune the\nanchors detected in each frame of the picture to filter out the effective\nanchors. These designs enable our SFMViT to achieve a mAP of 26.62% in the\nChaotic World dataset, far exceeding existing models. Code is available at\nhttps://github.com/jfightyr/SlowFast-Meet-ViT.\n", "link": "http://arxiv.org/abs/2404.16609v1", "date": "2024-04-25", "relevancy": 2.0449, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5274}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5151}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4935}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SFMViT%3A%20SlowFast%20Meet%20ViT%20in%20Chaotic%20World&body=Title%3A%20SFMViT%3A%20SlowFast%20Meet%20ViT%20in%20Chaotic%20World%0AAuthor%3A%20Jiaying%20Lin%20and%20Jiajun%20Wen%20and%20Mengyuan%20Liu%20and%20Jinfu%20Liu%20and%20Baiqiao%20Yin%20and%20Yue%20Li%0AAbstract%3A%20%20%20The%20task%20of%20spatiotemporal%20action%20localization%20in%20chaotic%20scenes%20is%20a%0Achallenging%20task%20toward%20advanced%20video%20understanding.%20Paving%20the%20way%20with%0Ahigh-quality%20video%20feature%20extraction%20and%20enhancing%20the%20precision%20of%0Adetector-predicted%20anchors%20can%20effectively%20improve%20model%20performance.%20To%20this%0Aend%2C%20we%20propose%20a%20high-performance%20dual-stream%20spatiotemporal%20feature%0Aextraction%20network%20SFMViT%20with%20an%20anchor%20pruning%20strategy.%20The%20backbone%20of%20our%0ASFMViT%20is%20composed%20of%20ViT%20and%20SlowFast%20with%20prior%20knowledge%20of%20spatiotemporal%0Aaction%20localization%2C%20which%20fully%20utilizes%20ViT%27s%20excellent%20global%20feature%0Aextraction%20capabilities%20and%20SlowFast%27s%20spatiotemporal%20sequence%20modeling%0Acapabilities.%20Secondly%2C%20we%20introduce%20the%20confidence%20maximum%20heap%20to%20prune%20the%0Aanchors%20detected%20in%20each%20frame%20of%20the%20picture%20to%20filter%20out%20the%20effective%0Aanchors.%20These%20designs%20enable%20our%20SFMViT%20to%20achieve%20a%20mAP%20of%2026.62%25%20in%20the%0AChaotic%20World%20dataset%2C%20far%20exceeding%20existing%20models.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jfightyr/SlowFast-Meet-ViT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16609v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFMViT%3A%20SlowFast%20Meet%20ViT%20in%20Chaotic%20World&entry.906535625=Jiaying%20Lin%20and%20Jiajun%20Wen%20and%20Mengyuan%20Liu%20and%20Jinfu%20Liu%20and%20Baiqiao%20Yin%20and%20Yue%20Li&entry.1292438233=%20%20The%20task%20of%20spatiotemporal%20action%20localization%20in%20chaotic%20scenes%20is%20a%0Achallenging%20task%20toward%20advanced%20video%20understanding.%20Paving%20the%20way%20with%0Ahigh-quality%20video%20feature%20extraction%20and%20enhancing%20the%20precision%20of%0Adetector-predicted%20anchors%20can%20effectively%20improve%20model%20performance.%20To%20this%0Aend%2C%20we%20propose%20a%20high-performance%20dual-stream%20spatiotemporal%20feature%0Aextraction%20network%20SFMViT%20with%20an%20anchor%20pruning%20strategy.%20The%20backbone%20of%20our%0ASFMViT%20is%20composed%20of%20ViT%20and%20SlowFast%20with%20prior%20knowledge%20of%20spatiotemporal%0Aaction%20localization%2C%20which%20fully%20utilizes%20ViT%27s%20excellent%20global%20feature%0Aextraction%20capabilities%20and%20SlowFast%27s%20spatiotemporal%20sequence%20modeling%0Acapabilities.%20Secondly%2C%20we%20introduce%20the%20confidence%20maximum%20heap%20to%20prune%20the%0Aanchors%20detected%20in%20each%20frame%20of%20the%20picture%20to%20filter%20out%20the%20effective%0Aanchors.%20These%20designs%20enable%20our%20SFMViT%20to%20achieve%20a%20mAP%20of%2026.62%25%20in%20the%0AChaotic%20World%20dataset%2C%20far%20exceeding%20existing%20models.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jfightyr/SlowFast-Meet-ViT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16609v1&entry.124074799=Read"},
{"title": "Road Surface Friction Estimation for Winter Conditions Utilising General\n  Visual Features", "author": "Risto Ojala and Eerik Alamikkotervo", "abstract": "  In below freezing winter conditions, road surface friction can greatly vary\nbased on the mixture of snow, ice, and water on the road. Friction between the\nroad and vehicle tyres is a critical parameter defining vehicle dynamics, and\ntherefore road surface friction information is essential to acquire for several\nintelligent transportation applications, such as safe control of automated\nvehicles or alerting drivers of slippery road conditions. This paper explores\ncomputer vision-based evaluation of road surface friction from roadside\ncameras. Previous studies have extensively investigated the application of\nconvolutional neural networks for the task of evaluating the road surface\ncondition from images. Here, we propose a hybrid deep learning architecture,\nWCamNet, consisting of a pretrained visual transformer model and convolutional\nblocks. The motivation of the architecture is to combine general visual\nfeatures provided by the transformer model, as well as finetuned feature\nextraction properties of the convolutional blocks. To benchmark the approach,\nan extensive dataset was gathered from national Finnish road infrastructure\nnetwork of roadside cameras and optical road surface friction sensors. Acquired\nresults highlight that the proposed WCamNet outperforms previous approaches in\nthe task of predicting the road surface friction from the roadside camera\nimages.\n", "link": "http://arxiv.org/abs/2404.16578v1", "date": "2024-04-25", "relevancy": 2.0441, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5662}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5134}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4866}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Road%20Surface%20Friction%20Estimation%20for%20Winter%20Conditions%20Utilising%20General%0A%20%20Visual%20Features&body=Title%3A%20Road%20Surface%20Friction%20Estimation%20for%20Winter%20Conditions%20Utilising%20General%0A%20%20Visual%20Features%0AAuthor%3A%20Risto%20Ojala%20and%20Eerik%20Alamikkotervo%0AAbstract%3A%20%20%20In%20below%20freezing%20winter%20conditions%2C%20road%20surface%20friction%20can%20greatly%20vary%0Abased%20on%20the%20mixture%20of%20snow%2C%20ice%2C%20and%20water%20on%20the%20road.%20Friction%20between%20the%0Aroad%20and%20vehicle%20tyres%20is%20a%20critical%20parameter%20defining%20vehicle%20dynamics%2C%20and%0Atherefore%20road%20surface%20friction%20information%20is%20essential%20to%20acquire%20for%20several%0Aintelligent%20transportation%20applications%2C%20such%20as%20safe%20control%20of%20automated%0Avehicles%20or%20alerting%20drivers%20of%20slippery%20road%20conditions.%20This%20paper%20explores%0Acomputer%20vision-based%20evaluation%20of%20road%20surface%20friction%20from%20roadside%0Acameras.%20Previous%20studies%20have%20extensively%20investigated%20the%20application%20of%0Aconvolutional%20neural%20networks%20for%20the%20task%20of%20evaluating%20the%20road%20surface%0Acondition%20from%20images.%20Here%2C%20we%20propose%20a%20hybrid%20deep%20learning%20architecture%2C%0AWCamNet%2C%20consisting%20of%20a%20pretrained%20visual%20transformer%20model%20and%20convolutional%0Ablocks.%20The%20motivation%20of%20the%20architecture%20is%20to%20combine%20general%20visual%0Afeatures%20provided%20by%20the%20transformer%20model%2C%20as%20well%20as%20finetuned%20feature%0Aextraction%20properties%20of%20the%20convolutional%20blocks.%20To%20benchmark%20the%20approach%2C%0Aan%20extensive%20dataset%20was%20gathered%20from%20national%20Finnish%20road%20infrastructure%0Anetwork%20of%20roadside%20cameras%20and%20optical%20road%20surface%20friction%20sensors.%20Acquired%0Aresults%20highlight%20that%20the%20proposed%20WCamNet%20outperforms%20previous%20approaches%20in%0Athe%20task%20of%20predicting%20the%20road%20surface%20friction%20from%20the%20roadside%20camera%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16578v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Road%20Surface%20Friction%20Estimation%20for%20Winter%20Conditions%20Utilising%20General%0A%20%20Visual%20Features&entry.906535625=Risto%20Ojala%20and%20Eerik%20Alamikkotervo&entry.1292438233=%20%20In%20below%20freezing%20winter%20conditions%2C%20road%20surface%20friction%20can%20greatly%20vary%0Abased%20on%20the%20mixture%20of%20snow%2C%20ice%2C%20and%20water%20on%20the%20road.%20Friction%20between%20the%0Aroad%20and%20vehicle%20tyres%20is%20a%20critical%20parameter%20defining%20vehicle%20dynamics%2C%20and%0Atherefore%20road%20surface%20friction%20information%20is%20essential%20to%20acquire%20for%20several%0Aintelligent%20transportation%20applications%2C%20such%20as%20safe%20control%20of%20automated%0Avehicles%20or%20alerting%20drivers%20of%20slippery%20road%20conditions.%20This%20paper%20explores%0Acomputer%20vision-based%20evaluation%20of%20road%20surface%20friction%20from%20roadside%0Acameras.%20Previous%20studies%20have%20extensively%20investigated%20the%20application%20of%0Aconvolutional%20neural%20networks%20for%20the%20task%20of%20evaluating%20the%20road%20surface%0Acondition%20from%20images.%20Here%2C%20we%20propose%20a%20hybrid%20deep%20learning%20architecture%2C%0AWCamNet%2C%20consisting%20of%20a%20pretrained%20visual%20transformer%20model%20and%20convolutional%0Ablocks.%20The%20motivation%20of%20the%20architecture%20is%20to%20combine%20general%20visual%0Afeatures%20provided%20by%20the%20transformer%20model%2C%20as%20well%20as%20finetuned%20feature%0Aextraction%20properties%20of%20the%20convolutional%20blocks.%20To%20benchmark%20the%20approach%2C%0Aan%20extensive%20dataset%20was%20gathered%20from%20national%20Finnish%20road%20infrastructure%0Anetwork%20of%20roadside%20cameras%20and%20optical%20road%20surface%20friction%20sensors.%20Acquired%0Aresults%20highlight%20that%20the%20proposed%20WCamNet%20outperforms%20previous%20approaches%20in%0Athe%20task%20of%20predicting%20the%20road%20surface%20friction%20from%20the%20roadside%20camera%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16578v1&entry.124074799=Read"},
{"title": "Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks", "author": "Benjamin David Evans and Raphael Trumpp and Marco Caccamo and Felix Jahncke and Johannes Betz and Hendrik Willem Jordaan and Herman Arnold Engelbrecht", "abstract": "  The F1TENTH autonomous driving platform, consisting of 1:10-scale\nremote-controlled cars, has evolved into a well-established education and\nresearch platform. The many publications and real-world competitions span many\ndomains, from classical path planning to novel learning-based algorithms.\nConsequently, the field is wide and disjointed, hindering direct comparison of\ndeveloped methods and making it difficult to assess the state-of-the-art.\nTherefore, we aim to unify the field by surveying current approaches,\ndescribing common methods, and providing benchmark results to facilitate clear\ncomparisons and establish a baseline for future work. This research aims to\nsurvey past and current work with F1TENTH vehicles in the classical and\nlearning categories and explain the different solution approaches. We describe\nparticle filter localisation, trajectory optimisation and tracking, model\npredictive contouring control, follow-the-gap, and end-to-end reinforcement\nlearning. We provide an open-source evaluation of benchmark methods and\ninvestigate overlooked factors of control frequency and localisation accuracy\nfor classical methods as well as reward signal and training map for learning\nmethods. The evaluation shows that the optimisation and tracking method\nachieves the fastest lap times, followed by the online planning approach.\nFinally, our work identifies and outlines the relevant research aspects to help\nmotivate future work in the F1TENTH domain.\n", "link": "http://arxiv.org/abs/2402.18558v2", "date": "2024-04-25", "relevancy": 2.0354, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5236}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5061}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4952}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unifying%20F1TENTH%20Autonomous%20Racing%3A%20Survey%2C%20Methods%20and%20Benchmarks&body=Title%3A%20Unifying%20F1TENTH%20Autonomous%20Racing%3A%20Survey%2C%20Methods%20and%20Benchmarks%0AAuthor%3A%20Benjamin%20David%20Evans%20and%20Raphael%20Trumpp%20and%20Marco%20Caccamo%20and%20Felix%20Jahncke%20and%20Johannes%20Betz%20and%20Hendrik%20Willem%20Jordaan%20and%20Herman%20Arnold%20Engelbrecht%0AAbstract%3A%20%20%20The%20F1TENTH%20autonomous%20driving%20platform%2C%20consisting%20of%201%3A10-scale%0Aremote-controlled%20cars%2C%20has%20evolved%20into%20a%20well-established%20education%20and%0Aresearch%20platform.%20The%20many%20publications%20and%20real-world%20competitions%20span%20many%0Adomains%2C%20from%20classical%20path%20planning%20to%20novel%20learning-based%20algorithms.%0AConsequently%2C%20the%20field%20is%20wide%20and%20disjointed%2C%20hindering%20direct%20comparison%20of%0Adeveloped%20methods%20and%20making%20it%20difficult%20to%20assess%20the%20state-of-the-art.%0ATherefore%2C%20we%20aim%20to%20unify%20the%20field%20by%20surveying%20current%20approaches%2C%0Adescribing%20common%20methods%2C%20and%20providing%20benchmark%20results%20to%20facilitate%20clear%0Acomparisons%20and%20establish%20a%20baseline%20for%20future%20work.%20This%20research%20aims%20to%0Asurvey%20past%20and%20current%20work%20with%20F1TENTH%20vehicles%20in%20the%20classical%20and%0Alearning%20categories%20and%20explain%20the%20different%20solution%20approaches.%20We%20describe%0Aparticle%20filter%20localisation%2C%20trajectory%20optimisation%20and%20tracking%2C%20model%0Apredictive%20contouring%20control%2C%20follow-the-gap%2C%20and%20end-to-end%20reinforcement%0Alearning.%20We%20provide%20an%20open-source%20evaluation%20of%20benchmark%20methods%20and%0Ainvestigate%20overlooked%20factors%20of%20control%20frequency%20and%20localisation%20accuracy%0Afor%20classical%20methods%20as%20well%20as%20reward%20signal%20and%20training%20map%20for%20learning%0Amethods.%20The%20evaluation%20shows%20that%20the%20optimisation%20and%20tracking%20method%0Aachieves%20the%20fastest%20lap%20times%2C%20followed%20by%20the%20online%20planning%20approach.%0AFinally%2C%20our%20work%20identifies%20and%20outlines%20the%20relevant%20research%20aspects%20to%20help%0Amotivate%20future%20work%20in%20the%20F1TENTH%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18558v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20F1TENTH%20Autonomous%20Racing%3A%20Survey%2C%20Methods%20and%20Benchmarks&entry.906535625=Benjamin%20David%20Evans%20and%20Raphael%20Trumpp%20and%20Marco%20Caccamo%20and%20Felix%20Jahncke%20and%20Johannes%20Betz%20and%20Hendrik%20Willem%20Jordaan%20and%20Herman%20Arnold%20Engelbrecht&entry.1292438233=%20%20The%20F1TENTH%20autonomous%20driving%20platform%2C%20consisting%20of%201%3A10-scale%0Aremote-controlled%20cars%2C%20has%20evolved%20into%20a%20well-established%20education%20and%0Aresearch%20platform.%20The%20many%20publications%20and%20real-world%20competitions%20span%20many%0Adomains%2C%20from%20classical%20path%20planning%20to%20novel%20learning-based%20algorithms.%0AConsequently%2C%20the%20field%20is%20wide%20and%20disjointed%2C%20hindering%20direct%20comparison%20of%0Adeveloped%20methods%20and%20making%20it%20difficult%20to%20assess%20the%20state-of-the-art.%0ATherefore%2C%20we%20aim%20to%20unify%20the%20field%20by%20surveying%20current%20approaches%2C%0Adescribing%20common%20methods%2C%20and%20providing%20benchmark%20results%20to%20facilitate%20clear%0Acomparisons%20and%20establish%20a%20baseline%20for%20future%20work.%20This%20research%20aims%20to%0Asurvey%20past%20and%20current%20work%20with%20F1TENTH%20vehicles%20in%20the%20classical%20and%0Alearning%20categories%20and%20explain%20the%20different%20solution%20approaches.%20We%20describe%0Aparticle%20filter%20localisation%2C%20trajectory%20optimisation%20and%20tracking%2C%20model%0Apredictive%20contouring%20control%2C%20follow-the-gap%2C%20and%20end-to-end%20reinforcement%0Alearning.%20We%20provide%20an%20open-source%20evaluation%20of%20benchmark%20methods%20and%0Ainvestigate%20overlooked%20factors%20of%20control%20frequency%20and%20localisation%20accuracy%0Afor%20classical%20methods%20as%20well%20as%20reward%20signal%20and%20training%20map%20for%20learning%0Amethods.%20The%20evaluation%20shows%20that%20the%20optimisation%20and%20tracking%20method%0Aachieves%20the%20fastest%20lap%20times%2C%20followed%20by%20the%20online%20planning%20approach.%0AFinally%2C%20our%20work%20identifies%20and%20outlines%20the%20relevant%20research%20aspects%20to%20help%0Amotivate%20future%20work%20in%20the%20F1TENTH%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18558v2&entry.124074799=Read"},
{"title": "AudioScenic: Audio-Driven Video Scene Editing", "author": "Kaixin Shen and Ruijie Quan and Linchao Zhu and Jun Xiao and Yi Yang", "abstract": "  Audio-driven visual scene editing endeavors to manipulate the visual\nbackground while leaving the foreground content unchanged, according to the\ngiven audio signals. Unlike current efforts focusing primarily on image\nediting, audio-driven video scene editing has not been extensively addressed.\nIn this paper, we introduce AudioScenic, an audio-driven framework designed for\nvideo scene editing. AudioScenic integrates audio semantics into the visual\nscene through a temporal-aware audio semantic injection process. As our focus\nis on background editing, we further introduce a SceneMasker module, which\nmaintains the integrity of the foreground content during the editing process.\nAudioScenic exploits the inherent properties of audio, namely, audio magnitude\nand frequency, to guide the editing process, aiming to control the temporal\ndynamics and enhance the temporal consistency. First, we present an audio\nMagnitude Modulator module that adjusts the temporal dynamics of the scene in\nresponse to changes in audio magnitude, enhancing the visual dynamics. Second,\nthe audio Frequency Fuser module is designed to ensure temporal consistency by\naligning the frequency of the audio with the dynamics of the video scenes, thus\nimproving the overall temporal coherence of the edited videos. These integrated\nfeatures enable AudioScenic to not only enhance visual diversity but also\nmaintain temporal consistency throughout the video. We present a new metric\nnamed temporal score for more comprehensive validation of temporal consistency.\nWe demonstrate substantial advancements of AudioScenic over competing methods\non DAVIS and Audioset datasets.\n", "link": "http://arxiv.org/abs/2404.16581v1", "date": "2024-04-25", "relevancy": 2.0326, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.522}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5143}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4964}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AudioScenic%3A%20Audio-Driven%20Video%20Scene%20Editing&body=Title%3A%20AudioScenic%3A%20Audio-Driven%20Video%20Scene%20Editing%0AAuthor%3A%20Kaixin%20Shen%20and%20Ruijie%20Quan%20and%20Linchao%20Zhu%20and%20Jun%20Xiao%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Audio-driven%20visual%20scene%20editing%20endeavors%20to%20manipulate%20the%20visual%0Abackground%20while%20leaving%20the%20foreground%20content%20unchanged%2C%20according%20to%20the%0Agiven%20audio%20signals.%20Unlike%20current%20efforts%20focusing%20primarily%20on%20image%0Aediting%2C%20audio-driven%20video%20scene%20editing%20has%20not%20been%20extensively%20addressed.%0AIn%20this%20paper%2C%20we%20introduce%20AudioScenic%2C%20an%20audio-driven%20framework%20designed%20for%0Avideo%20scene%20editing.%20AudioScenic%20integrates%20audio%20semantics%20into%20the%20visual%0Ascene%20through%20a%20temporal-aware%20audio%20semantic%20injection%20process.%20As%20our%20focus%0Ais%20on%20background%20editing%2C%20we%20further%20introduce%20a%20SceneMasker%20module%2C%20which%0Amaintains%20the%20integrity%20of%20the%20foreground%20content%20during%20the%20editing%20process.%0AAudioScenic%20exploits%20the%20inherent%20properties%20of%20audio%2C%20namely%2C%20audio%20magnitude%0Aand%20frequency%2C%20to%20guide%20the%20editing%20process%2C%20aiming%20to%20control%20the%20temporal%0Adynamics%20and%20enhance%20the%20temporal%20consistency.%20First%2C%20we%20present%20an%20audio%0AMagnitude%20Modulator%20module%20that%20adjusts%20the%20temporal%20dynamics%20of%20the%20scene%20in%0Aresponse%20to%20changes%20in%20audio%20magnitude%2C%20enhancing%20the%20visual%20dynamics.%20Second%2C%0Athe%20audio%20Frequency%20Fuser%20module%20is%20designed%20to%20ensure%20temporal%20consistency%20by%0Aaligning%20the%20frequency%20of%20the%20audio%20with%20the%20dynamics%20of%20the%20video%20scenes%2C%20thus%0Aimproving%20the%20overall%20temporal%20coherence%20of%20the%20edited%20videos.%20These%20integrated%0Afeatures%20enable%20AudioScenic%20to%20not%20only%20enhance%20visual%20diversity%20but%20also%0Amaintain%20temporal%20consistency%20throughout%20the%20video.%20We%20present%20a%20new%20metric%0Anamed%20temporal%20score%20for%20more%20comprehensive%20validation%20of%20temporal%20consistency.%0AWe%20demonstrate%20substantial%20advancements%20of%20AudioScenic%20over%20competing%20methods%0Aon%20DAVIS%20and%20Audioset%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16581v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioScenic%3A%20Audio-Driven%20Video%20Scene%20Editing&entry.906535625=Kaixin%20Shen%20and%20Ruijie%20Quan%20and%20Linchao%20Zhu%20and%20Jun%20Xiao%20and%20Yi%20Yang&entry.1292438233=%20%20Audio-driven%20visual%20scene%20editing%20endeavors%20to%20manipulate%20the%20visual%0Abackground%20while%20leaving%20the%20foreground%20content%20unchanged%2C%20according%20to%20the%0Agiven%20audio%20signals.%20Unlike%20current%20efforts%20focusing%20primarily%20on%20image%0Aediting%2C%20audio-driven%20video%20scene%20editing%20has%20not%20been%20extensively%20addressed.%0AIn%20this%20paper%2C%20we%20introduce%20AudioScenic%2C%20an%20audio-driven%20framework%20designed%20for%0Avideo%20scene%20editing.%20AudioScenic%20integrates%20audio%20semantics%20into%20the%20visual%0Ascene%20through%20a%20temporal-aware%20audio%20semantic%20injection%20process.%20As%20our%20focus%0Ais%20on%20background%20editing%2C%20we%20further%20introduce%20a%20SceneMasker%20module%2C%20which%0Amaintains%20the%20integrity%20of%20the%20foreground%20content%20during%20the%20editing%20process.%0AAudioScenic%20exploits%20the%20inherent%20properties%20of%20audio%2C%20namely%2C%20audio%20magnitude%0Aand%20frequency%2C%20to%20guide%20the%20editing%20process%2C%20aiming%20to%20control%20the%20temporal%0Adynamics%20and%20enhance%20the%20temporal%20consistency.%20First%2C%20we%20present%20an%20audio%0AMagnitude%20Modulator%20module%20that%20adjusts%20the%20temporal%20dynamics%20of%20the%20scene%20in%0Aresponse%20to%20changes%20in%20audio%20magnitude%2C%20enhancing%20the%20visual%20dynamics.%20Second%2C%0Athe%20audio%20Frequency%20Fuser%20module%20is%20designed%20to%20ensure%20temporal%20consistency%20by%0Aaligning%20the%20frequency%20of%20the%20audio%20with%20the%20dynamics%20of%20the%20video%20scenes%2C%20thus%0Aimproving%20the%20overall%20temporal%20coherence%20of%20the%20edited%20videos.%20These%20integrated%0Afeatures%20enable%20AudioScenic%20to%20not%20only%20enhance%20visual%20diversity%20but%20also%0Amaintain%20temporal%20consistency%20throughout%20the%20video.%20We%20present%20a%20new%20metric%0Anamed%20temporal%20score%20for%20more%20comprehensive%20validation%20of%20temporal%20consistency.%0AWe%20demonstrate%20substantial%20advancements%20of%20AudioScenic%20over%20competing%20methods%0Aon%20DAVIS%20and%20Audioset%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16581v1&entry.124074799=Read"},
{"title": "ResVR: Joint Rescaling and Viewport Rendering of Omnidirectional Images", "author": "Weiqi Li and Shijie Zhao and Bin Chen and Xinhua Cheng and Junlin Li and Li Zhang and Jian Zhang", "abstract": "  With the advent of virtual reality technology, omnidirectional image (ODI)\nrescaling techniques are increasingly embraced for reducing transmitted and\nstored file sizes while preserving high image quality. Despite this progress,\ncurrent ODI rescaling methods predominantly focus on enhancing the quality of\nimages in equirectangular projection (ERP) format, which overlooks the fact\nthat the content viewed on head mounted displays (HMDs) is actually a rendered\nviewport instead of an ERP image. In this work, we emphasize that focusing\nsolely on ERP quality results in inferior viewport visual experiences for\nusers. Thus, we propose ResVR, which is the first comprehensive framework for\nthe joint Rescaling and Viewport Rendering of ODIs. ResVR allows obtaining LR\nERP images for transmission while rendering high-quality viewports for users to\nwatch on HMDs. In our ResVR, a novel discrete pixel sampling strategy is\ndeveloped to tackle the complex mapping between the viewport and ERP, enabling\nend-to-end training of ResVR pipeline. Furthermore, a spherical pixel shape\nrepresentation technique is innovatively derived from spherical differentiation\nto significantly improve the visual quality of rendered viewports. Extensive\nexperiments demonstrate that our ResVR outperforms existing methods in viewport\nrendering tasks across different fields of view, resolutions, and view\ndirections while keeping a low transmission overhead.\n", "link": "http://arxiv.org/abs/2404.16825v1", "date": "2024-04-25", "relevancy": 2.0296, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5094}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5078}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5063}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ResVR%3A%20Joint%20Rescaling%20and%20Viewport%20Rendering%20of%20Omnidirectional%20Images&body=Title%3A%20ResVR%3A%20Joint%20Rescaling%20and%20Viewport%20Rendering%20of%20Omnidirectional%20Images%0AAuthor%3A%20Weiqi%20Li%20and%20Shijie%20Zhao%20and%20Bin%20Chen%20and%20Xinhua%20Cheng%20and%20Junlin%20Li%20and%20Li%20Zhang%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20With%20the%20advent%20of%20virtual%20reality%20technology%2C%20omnidirectional%20image%20%28ODI%29%0Arescaling%20techniques%20are%20increasingly%20embraced%20for%20reducing%20transmitted%20and%0Astored%20file%20sizes%20while%20preserving%20high%20image%20quality.%20Despite%20this%20progress%2C%0Acurrent%20ODI%20rescaling%20methods%20predominantly%20focus%20on%20enhancing%20the%20quality%20of%0Aimages%20in%20equirectangular%20projection%20%28ERP%29%20format%2C%20which%20overlooks%20the%20fact%0Athat%20the%20content%20viewed%20on%20head%20mounted%20displays%20%28HMDs%29%20is%20actually%20a%20rendered%0Aviewport%20instead%20of%20an%20ERP%20image.%20In%20this%20work%2C%20we%20emphasize%20that%20focusing%0Asolely%20on%20ERP%20quality%20results%20in%20inferior%20viewport%20visual%20experiences%20for%0Ausers.%20Thus%2C%20we%20propose%20ResVR%2C%20which%20is%20the%20first%20comprehensive%20framework%20for%0Athe%20joint%20Rescaling%20and%20Viewport%20Rendering%20of%20ODIs.%20ResVR%20allows%20obtaining%20LR%0AERP%20images%20for%20transmission%20while%20rendering%20high-quality%20viewports%20for%20users%20to%0Awatch%20on%20HMDs.%20In%20our%20ResVR%2C%20a%20novel%20discrete%20pixel%20sampling%20strategy%20is%0Adeveloped%20to%20tackle%20the%20complex%20mapping%20between%20the%20viewport%20and%20ERP%2C%20enabling%0Aend-to-end%20training%20of%20ResVR%20pipeline.%20Furthermore%2C%20a%20spherical%20pixel%20shape%0Arepresentation%20technique%20is%20innovatively%20derived%20from%20spherical%20differentiation%0Ato%20significantly%20improve%20the%20visual%20quality%20of%20rendered%20viewports.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20ResVR%20outperforms%20existing%20methods%20in%20viewport%0Arendering%20tasks%20across%20different%20fields%20of%20view%2C%20resolutions%2C%20and%20view%0Adirections%20while%20keeping%20a%20low%20transmission%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16825v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResVR%3A%20Joint%20Rescaling%20and%20Viewport%20Rendering%20of%20Omnidirectional%20Images&entry.906535625=Weiqi%20Li%20and%20Shijie%20Zhao%20and%20Bin%20Chen%20and%20Xinhua%20Cheng%20and%20Junlin%20Li%20and%20Li%20Zhang%20and%20Jian%20Zhang&entry.1292438233=%20%20With%20the%20advent%20of%20virtual%20reality%20technology%2C%20omnidirectional%20image%20%28ODI%29%0Arescaling%20techniques%20are%20increasingly%20embraced%20for%20reducing%20transmitted%20and%0Astored%20file%20sizes%20while%20preserving%20high%20image%20quality.%20Despite%20this%20progress%2C%0Acurrent%20ODI%20rescaling%20methods%20predominantly%20focus%20on%20enhancing%20the%20quality%20of%0Aimages%20in%20equirectangular%20projection%20%28ERP%29%20format%2C%20which%20overlooks%20the%20fact%0Athat%20the%20content%20viewed%20on%20head%20mounted%20displays%20%28HMDs%29%20is%20actually%20a%20rendered%0Aviewport%20instead%20of%20an%20ERP%20image.%20In%20this%20work%2C%20we%20emphasize%20that%20focusing%0Asolely%20on%20ERP%20quality%20results%20in%20inferior%20viewport%20visual%20experiences%20for%0Ausers.%20Thus%2C%20we%20propose%20ResVR%2C%20which%20is%20the%20first%20comprehensive%20framework%20for%0Athe%20joint%20Rescaling%20and%20Viewport%20Rendering%20of%20ODIs.%20ResVR%20allows%20obtaining%20LR%0AERP%20images%20for%20transmission%20while%20rendering%20high-quality%20viewports%20for%20users%20to%0Awatch%20on%20HMDs.%20In%20our%20ResVR%2C%20a%20novel%20discrete%20pixel%20sampling%20strategy%20is%0Adeveloped%20to%20tackle%20the%20complex%20mapping%20between%20the%20viewport%20and%20ERP%2C%20enabling%0Aend-to-end%20training%20of%20ResVR%20pipeline.%20Furthermore%2C%20a%20spherical%20pixel%20shape%0Arepresentation%20technique%20is%20innovatively%20derived%20from%20spherical%20differentiation%0Ato%20significantly%20improve%20the%20visual%20quality%20of%20rendered%20viewports.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20ResVR%20outperforms%20existing%20methods%20in%20viewport%0Arendering%20tasks%20across%20different%20fields%20of%20view%2C%20resolutions%2C%20and%20view%0Adirections%20while%20keeping%20a%20low%20transmission%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16825v1&entry.124074799=Read"},
{"title": "Features Fusion for Dual-View Mammography Mass Detection", "author": "Arina Varlamova and Valery Belotsky and Grigory Novikov and Anton Konushin and Evgeny Sidorov", "abstract": "  Detection of malignant lesions on mammography images is extremely important\nfor early breast cancer diagnosis. In clinical practice, images are acquired\nfrom two different angles, and radiologists can fully utilize information from\nboth views, simultaneously locating the same lesion. However, for automatic\ndetection approaches such information fusion remains a challenge. In this\npaper, we propose a new model called MAMM-Net, which allows the processing of\nboth mammography views simultaneously by sharing information not only on an\nobject level, as seen in existing works, but also on a feature level.\nMAMM-Net's key component is the Fusion Layer, based on deformable attention and\ndesigned to increase detection precision while keeping high recall. Our\nexperiments show superior performance on the public DDSM dataset compared to\nthe previous state-of-the-art model, while introducing new helpful features\nsuch as lesion annotation on pixel-level and classification of lesions\nmalignancy.\n", "link": "http://arxiv.org/abs/2404.16718v1", "date": "2024-04-25", "relevancy": 2.0221, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5126}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4939}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Features%20Fusion%20for%20Dual-View%20Mammography%20Mass%20Detection&body=Title%3A%20Features%20Fusion%20for%20Dual-View%20Mammography%20Mass%20Detection%0AAuthor%3A%20Arina%20Varlamova%20and%20Valery%20Belotsky%20and%20Grigory%20Novikov%20and%20Anton%20Konushin%20and%20Evgeny%20Sidorov%0AAbstract%3A%20%20%20Detection%20of%20malignant%20lesions%20on%20mammography%20images%20is%20extremely%20important%0Afor%20early%20breast%20cancer%20diagnosis.%20In%20clinical%20practice%2C%20images%20are%20acquired%0Afrom%20two%20different%20angles%2C%20and%20radiologists%20can%20fully%20utilize%20information%20from%0Aboth%20views%2C%20simultaneously%20locating%20the%20same%20lesion.%20However%2C%20for%20automatic%0Adetection%20approaches%20such%20information%20fusion%20remains%20a%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20model%20called%20MAMM-Net%2C%20which%20allows%20the%20processing%20of%0Aboth%20mammography%20views%20simultaneously%20by%20sharing%20information%20not%20only%20on%20an%0Aobject%20level%2C%20as%20seen%20in%20existing%20works%2C%20but%20also%20on%20a%20feature%20level.%0AMAMM-Net%27s%20key%20component%20is%20the%20Fusion%20Layer%2C%20based%20on%20deformable%20attention%20and%0Adesigned%20to%20increase%20detection%20precision%20while%20keeping%20high%20recall.%20Our%0Aexperiments%20show%20superior%20performance%20on%20the%20public%20DDSM%20dataset%20compared%20to%0Athe%20previous%20state-of-the-art%20model%2C%20while%20introducing%20new%20helpful%20features%0Asuch%20as%20lesion%20annotation%20on%20pixel-level%20and%20classification%20of%20lesions%0Amalignancy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16718v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Features%20Fusion%20for%20Dual-View%20Mammography%20Mass%20Detection&entry.906535625=Arina%20Varlamova%20and%20Valery%20Belotsky%20and%20Grigory%20Novikov%20and%20Anton%20Konushin%20and%20Evgeny%20Sidorov&entry.1292438233=%20%20Detection%20of%20malignant%20lesions%20on%20mammography%20images%20is%20extremely%20important%0Afor%20early%20breast%20cancer%20diagnosis.%20In%20clinical%20practice%2C%20images%20are%20acquired%0Afrom%20two%20different%20angles%2C%20and%20radiologists%20can%20fully%20utilize%20information%20from%0Aboth%20views%2C%20simultaneously%20locating%20the%20same%20lesion.%20However%2C%20for%20automatic%0Adetection%20approaches%20such%20information%20fusion%20remains%20a%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20model%20called%20MAMM-Net%2C%20which%20allows%20the%20processing%20of%0Aboth%20mammography%20views%20simultaneously%20by%20sharing%20information%20not%20only%20on%20an%0Aobject%20level%2C%20as%20seen%20in%20existing%20works%2C%20but%20also%20on%20a%20feature%20level.%0AMAMM-Net%27s%20key%20component%20is%20the%20Fusion%20Layer%2C%20based%20on%20deformable%20attention%20and%0Adesigned%20to%20increase%20detection%20precision%20while%20keeping%20high%20recall.%20Our%0Aexperiments%20show%20superior%20performance%20on%20the%20public%20DDSM%20dataset%20compared%20to%0Athe%20previous%20state-of-the-art%20model%2C%20while%20introducing%20new%20helpful%20features%0Asuch%20as%20lesion%20annotation%20on%20pixel-level%20and%20classification%20of%20lesions%0Amalignancy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16718v1&entry.124074799=Read"},
{"title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving\n  with VQ-VAE and Differentiable Optimization", "author": "Simon Idoko and Basant Sharma and Arun Kumar Singh", "abstract": "  Sampling trajectories from a distribution followed by ranking them based on a\nspecified cost function is a common approach in autonomous driving. Typically,\nthe sampling distribution is hand-crafted (e.g a Gaussian, or a grid).\nRecently, there have been efforts towards learning the sampling distribution\nthrough generative models such as Conditional Variational Autoencoder (CVAE).\nHowever, these approaches fail to capture the multi-modality of the driving\nbehaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we\nre-imagine the distribution learning through vector quantized variational\nautoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture\nmulti-modal sampling distribution. The VQ-VAE is trained with demonstration\ndata of optimal trajectories. We further propose a differentiable optimization\nbased safety filter to minimally correct the VQVAE sampled trajectories to\nensure collision avoidance. We use backpropagation through the optimization\nlayers in a self-supervised learning set-up to learn good initialization and\noptimal parameters of the safety filter. We perform extensive comparisons with\nstate-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios\nand show a reduction of up to 12 times in collision-rate while being\ncompetitive in driving speeds.\n", "link": "http://arxiv.org/abs/2403.19461v2", "date": "2024-04-25", "relevancy": 2.021, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5297}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5034}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4974}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Sampling%20Distribution%20and%20Safety%20Filter%20for%20Autonomous%20Driving%0A%20%20with%20VQ-VAE%20and%20Differentiable%20Optimization&body=Title%3A%20Learning%20Sampling%20Distribution%20and%20Safety%20Filter%20for%20Autonomous%20Driving%0A%20%20with%20VQ-VAE%20and%20Differentiable%20Optimization%0AAuthor%3A%20Simon%20Idoko%20and%20Basant%20Sharma%20and%20Arun%20Kumar%20Singh%0AAbstract%3A%20%20%20Sampling%20trajectories%20from%20a%20distribution%20followed%20by%20ranking%20them%20based%20on%20a%0Aspecified%20cost%20function%20is%20a%20common%20approach%20in%20autonomous%20driving.%20Typically%2C%0Athe%20sampling%20distribution%20is%20hand-crafted%20%28e.g%20a%20Gaussian%2C%20or%20a%20grid%29.%0ARecently%2C%20there%20have%20been%20efforts%20towards%20learning%20the%20sampling%20distribution%0Athrough%20generative%20models%20such%20as%20Conditional%20Variational%20Autoencoder%20%28CVAE%29.%0AHowever%2C%20these%20approaches%20fail%20to%20capture%20the%20multi-modality%20of%20the%20driving%0Abehaviour%20due%20to%20the%20Gaussian%20latent%20prior%20of%20the%20CVAE.%20Thus%2C%20in%20this%20paper%2C%20we%0Are-imagine%20the%20distribution%20learning%20through%20vector%20quantized%20variational%0Aautoencoder%20%28VQ-VAE%29%2C%20whose%20discrete%20latent-space%20is%20well%20equipped%20to%20capture%0Amulti-modal%20sampling%20distribution.%20The%20VQ-VAE%20is%20trained%20with%20demonstration%0Adata%20of%20optimal%20trajectories.%20We%20further%20propose%20a%20differentiable%20optimization%0Abased%20safety%20filter%20to%20minimally%20correct%20the%20VQVAE%20sampled%20trajectories%20to%0Aensure%20collision%20avoidance.%20We%20use%20backpropagation%20through%20the%20optimization%0Alayers%20in%20a%20self-supervised%20learning%20set-up%20to%20learn%20good%20initialization%20and%0Aoptimal%20parameters%20of%20the%20safety%20filter.%20We%20perform%20extensive%20comparisons%20with%0Astate-of-the-art%20CVAE-based%20baseline%20in%20dense%20and%20aggressive%20traffic%20scenarios%0Aand%20show%20a%20reduction%20of%20up%20to%2012%20times%20in%20collision-rate%20while%20being%0Acompetitive%20in%20driving%20speeds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19461v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Sampling%20Distribution%20and%20Safety%20Filter%20for%20Autonomous%20Driving%0A%20%20with%20VQ-VAE%20and%20Differentiable%20Optimization&entry.906535625=Simon%20Idoko%20and%20Basant%20Sharma%20and%20Arun%20Kumar%20Singh&entry.1292438233=%20%20Sampling%20trajectories%20from%20a%20distribution%20followed%20by%20ranking%20them%20based%20on%20a%0Aspecified%20cost%20function%20is%20a%20common%20approach%20in%20autonomous%20driving.%20Typically%2C%0Athe%20sampling%20distribution%20is%20hand-crafted%20%28e.g%20a%20Gaussian%2C%20or%20a%20grid%29.%0ARecently%2C%20there%20have%20been%20efforts%20towards%20learning%20the%20sampling%20distribution%0Athrough%20generative%20models%20such%20as%20Conditional%20Variational%20Autoencoder%20%28CVAE%29.%0AHowever%2C%20these%20approaches%20fail%20to%20capture%20the%20multi-modality%20of%20the%20driving%0Abehaviour%20due%20to%20the%20Gaussian%20latent%20prior%20of%20the%20CVAE.%20Thus%2C%20in%20this%20paper%2C%20we%0Are-imagine%20the%20distribution%20learning%20through%20vector%20quantized%20variational%0Aautoencoder%20%28VQ-VAE%29%2C%20whose%20discrete%20latent-space%20is%20well%20equipped%20to%20capture%0Amulti-modal%20sampling%20distribution.%20The%20VQ-VAE%20is%20trained%20with%20demonstration%0Adata%20of%20optimal%20trajectories.%20We%20further%20propose%20a%20differentiable%20optimization%0Abased%20safety%20filter%20to%20minimally%20correct%20the%20VQVAE%20sampled%20trajectories%20to%0Aensure%20collision%20avoidance.%20We%20use%20backpropagation%20through%20the%20optimization%0Alayers%20in%20a%20self-supervised%20learning%20set-up%20to%20learn%20good%20initialization%20and%0Aoptimal%20parameters%20of%20the%20safety%20filter.%20We%20perform%20extensive%20comparisons%20with%0Astate-of-the-art%20CVAE-based%20baseline%20in%20dense%20and%20aggressive%20traffic%20scenarios%0Aand%20show%20a%20reduction%20of%20up%20to%2012%20times%20in%20collision-rate%20while%20being%0Acompetitive%20in%20driving%20speeds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19461v2&entry.124074799=Read"},
{"title": "SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems", "author": "Moon Ye-Bin and Nam Hyeon-Woo and Wonseok Choi and Nayeong Kim and Suha Kwak and Tae-Hyun Oh", "abstract": "  Data imbalance in training data often leads to biased predictions from\ntrained models, which in turn causes ethical and social issues. A\nstraightforward solution is to carefully curate training data, but given the\nenormous scale of modern neural networks, this is prohibitively labor-intensive\nand thus impractical. Inspired by recent developments in generative models,\nthis paper explores the potential of synthetic data to address the data\nimbalance problem. To be specific, our method, dubbed SYNAuG, leverages\nsynthetic data to equalize the unbalanced distribution of training data. Our\nexperiments demonstrate that, although a domain gap between real and synthetic\ndata exists, training with SYNAuG followed by fine-tuning with a few real\nsamples allows to achieve impressive performance on diverse tasks with\ndifferent data imbalance issues, surpassing existing task-specific methods for\nthe same purpose.\n", "link": "http://arxiv.org/abs/2308.00994v3", "date": "2024-04-25", "relevancy": 2.0159, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5148}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5029}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5007}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SYNAuG%3A%20Exploiting%20Synthetic%20Data%20for%20Data%20Imbalance%20Problems&body=Title%3A%20SYNAuG%3A%20Exploiting%20Synthetic%20Data%20for%20Data%20Imbalance%20Problems%0AAuthor%3A%20Moon%20Ye-Bin%20and%20Nam%20Hyeon-Woo%20and%20Wonseok%20Choi%20and%20Nayeong%20Kim%20and%20Suha%20Kwak%20and%20Tae-Hyun%20Oh%0AAbstract%3A%20%20%20Data%20imbalance%20in%20training%20data%20often%20leads%20to%20biased%20predictions%20from%0Atrained%20models%2C%20which%20in%20turn%20causes%20ethical%20and%20social%20issues.%20A%0Astraightforward%20solution%20is%20to%20carefully%20curate%20training%20data%2C%20but%20given%20the%0Aenormous%20scale%20of%20modern%20neural%20networks%2C%20this%20is%20prohibitively%20labor-intensive%0Aand%20thus%20impractical.%20Inspired%20by%20recent%20developments%20in%20generative%20models%2C%0Athis%20paper%20explores%20the%20potential%20of%20synthetic%20data%20to%20address%20the%20data%0Aimbalance%20problem.%20To%20be%20specific%2C%20our%20method%2C%20dubbed%20SYNAuG%2C%20leverages%0Asynthetic%20data%20to%20equalize%20the%20unbalanced%20distribution%20of%20training%20data.%20Our%0Aexperiments%20demonstrate%20that%2C%20although%20a%20domain%20gap%20between%20real%20and%20synthetic%0Adata%20exists%2C%20training%20with%20SYNAuG%20followed%20by%20fine-tuning%20with%20a%20few%20real%0Asamples%20allows%20to%20achieve%20impressive%20performance%20on%20diverse%20tasks%20with%0Adifferent%20data%20imbalance%20issues%2C%20surpassing%20existing%20task-specific%20methods%20for%0Athe%20same%20purpose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.00994v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SYNAuG%3A%20Exploiting%20Synthetic%20Data%20for%20Data%20Imbalance%20Problems&entry.906535625=Moon%20Ye-Bin%20and%20Nam%20Hyeon-Woo%20and%20Wonseok%20Choi%20and%20Nayeong%20Kim%20and%20Suha%20Kwak%20and%20Tae-Hyun%20Oh&entry.1292438233=%20%20Data%20imbalance%20in%20training%20data%20often%20leads%20to%20biased%20predictions%20from%0Atrained%20models%2C%20which%20in%20turn%20causes%20ethical%20and%20social%20issues.%20A%0Astraightforward%20solution%20is%20to%20carefully%20curate%20training%20data%2C%20but%20given%20the%0Aenormous%20scale%20of%20modern%20neural%20networks%2C%20this%20is%20prohibitively%20labor-intensive%0Aand%20thus%20impractical.%20Inspired%20by%20recent%20developments%20in%20generative%20models%2C%0Athis%20paper%20explores%20the%20potential%20of%20synthetic%20data%20to%20address%20the%20data%0Aimbalance%20problem.%20To%20be%20specific%2C%20our%20method%2C%20dubbed%20SYNAuG%2C%20leverages%0Asynthetic%20data%20to%20equalize%20the%20unbalanced%20distribution%20of%20training%20data.%20Our%0Aexperiments%20demonstrate%20that%2C%20although%20a%20domain%20gap%20between%20real%20and%20synthetic%0Adata%20exists%2C%20training%20with%20SYNAuG%20followed%20by%20fine-tuning%20with%20a%20few%20real%0Asamples%20allows%20to%20achieve%20impressive%20performance%20on%20diverse%20tasks%20with%0Adifferent%20data%20imbalance%20issues%2C%20surpassing%20existing%20task-specific%20methods%20for%0Athe%20same%20purpose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00994v3&entry.124074799=Read"},
{"title": "Formal Specification, Assessment, and Enforcement of Fairness for\n  Generative AIs", "author": "Chih-Hong Cheng and Changshun Wu and Harald Ruess and Xingyu Zhao and Saddek Bensalem", "abstract": "  The risk of reinforcing or exacerbating societal biases and inequalities is\ngrowing as generative AI increasingly produces content that resembles human\noutput, from text to images and beyond. Here we formally characterize the\nnotion of fairness for generative AI as a basis for monitoring and enforcing\nfairness. We define two levels of fairness utilizing the concept of infinite\nwords. The first is the fairness demonstrated on the generated sequences, which\nis only evaluated on the outputs while agnostic to the prompts/models used. The\nsecond is the inherent fairness of the generative AI model, which requires that\nfairness be manifested when input prompts are neutral, that is, they do not\nexplicitly instruct the generative AI to produce a particular type of output.\nWe also study relative intersectional fairness to counteract the combinatorial\nexplosion of fairness when considering multiple categories together with lazy\nfairness enforcement. Our implemented specification monitoring and enforcement\ntool shows interesting results when tested against several generative AI\nmodels.\n", "link": "http://arxiv.org/abs/2404.16663v1", "date": "2024-04-25", "relevancy": 2.0095, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5424}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4759}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4687}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Formal%20Specification%2C%20Assessment%2C%20and%20Enforcement%20of%20Fairness%20for%0A%20%20Generative%20AIs&body=Title%3A%20Formal%20Specification%2C%20Assessment%2C%20and%20Enforcement%20of%20Fairness%20for%0A%20%20Generative%20AIs%0AAuthor%3A%20Chih-Hong%20Cheng%20and%20Changshun%20Wu%20and%20Harald%20Ruess%20and%20Xingyu%20Zhao%20and%20Saddek%20Bensalem%0AAbstract%3A%20%20%20The%20risk%20of%20reinforcing%20or%20exacerbating%20societal%20biases%20and%20inequalities%20is%0Agrowing%20as%20generative%20AI%20increasingly%20produces%20content%20that%20resembles%20human%0Aoutput%2C%20from%20text%20to%20images%20and%20beyond.%20Here%20we%20formally%20characterize%20the%0Anotion%20of%20fairness%20for%20generative%20AI%20as%20a%20basis%20for%20monitoring%20and%20enforcing%0Afairness.%20We%20define%20two%20levels%20of%20fairness%20utilizing%20the%20concept%20of%20infinite%0Awords.%20The%20first%20is%20the%20fairness%20demonstrated%20on%20the%20generated%20sequences%2C%20which%0Ais%20only%20evaluated%20on%20the%20outputs%20while%20agnostic%20to%20the%20prompts/models%20used.%20The%0Asecond%20is%20the%20inherent%20fairness%20of%20the%20generative%20AI%20model%2C%20which%20requires%20that%0Afairness%20be%20manifested%20when%20input%20prompts%20are%20neutral%2C%20that%20is%2C%20they%20do%20not%0Aexplicitly%20instruct%20the%20generative%20AI%20to%20produce%20a%20particular%20type%20of%20output.%0AWe%20also%20study%20relative%20intersectional%20fairness%20to%20counteract%20the%20combinatorial%0Aexplosion%20of%20fairness%20when%20considering%20multiple%20categories%20together%20with%20lazy%0Afairness%20enforcement.%20Our%20implemented%20specification%20monitoring%20and%20enforcement%0Atool%20shows%20interesting%20results%20when%20tested%20against%20several%20generative%20AI%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16663v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Formal%20Specification%2C%20Assessment%2C%20and%20Enforcement%20of%20Fairness%20for%0A%20%20Generative%20AIs&entry.906535625=Chih-Hong%20Cheng%20and%20Changshun%20Wu%20and%20Harald%20Ruess%20and%20Xingyu%20Zhao%20and%20Saddek%20Bensalem&entry.1292438233=%20%20The%20risk%20of%20reinforcing%20or%20exacerbating%20societal%20biases%20and%20inequalities%20is%0Agrowing%20as%20generative%20AI%20increasingly%20produces%20content%20that%20resembles%20human%0Aoutput%2C%20from%20text%20to%20images%20and%20beyond.%20Here%20we%20formally%20characterize%20the%0Anotion%20of%20fairness%20for%20generative%20AI%20as%20a%20basis%20for%20monitoring%20and%20enforcing%0Afairness.%20We%20define%20two%20levels%20of%20fairness%20utilizing%20the%20concept%20of%20infinite%0Awords.%20The%20first%20is%20the%20fairness%20demonstrated%20on%20the%20generated%20sequences%2C%20which%0Ais%20only%20evaluated%20on%20the%20outputs%20while%20agnostic%20to%20the%20prompts/models%20used.%20The%0Asecond%20is%20the%20inherent%20fairness%20of%20the%20generative%20AI%20model%2C%20which%20requires%20that%0Afairness%20be%20manifested%20when%20input%20prompts%20are%20neutral%2C%20that%20is%2C%20they%20do%20not%0Aexplicitly%20instruct%20the%20generative%20AI%20to%20produce%20a%20particular%20type%20of%20output.%0AWe%20also%20study%20relative%20intersectional%20fairness%20to%20counteract%20the%20combinatorial%0Aexplosion%20of%20fairness%20when%20considering%20multiple%20categories%20together%20with%20lazy%0Afairness%20enforcement.%20Our%20implemented%20specification%20monitoring%20and%20enforcement%0Atool%20shows%20interesting%20results%20when%20tested%20against%20several%20generative%20AI%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16663v1&entry.124074799=Read"},
{"title": "A Self-Organizing Clustering System for Unsupervised Distribution Shift\n  Detection", "author": "Sebasti\u00e1n Basterrech and Line Clemmensen and Gerardo Rubino", "abstract": "  Modeling non-stationary data is a challenging problem in the field of\ncontinual learning, and data distribution shifts may result in negative\nconsequences on the performance of a machine learning model. Classic learning\ntools are often vulnerable to perturbations of the input covariates, and are\nsensitive to outliers and noise, and some tools are based on rigid algebraic\nassumptions. Distribution shifts are frequently occurring due to changes in raw\nmaterials for production, seasonality, a different user base, or even\nadversarial attacks. Therefore, there is a need for more effective distribution\nshift detection techniques.\n  In this work, we propose a continual learning framework for monitoring and\ndetecting distribution changes. We explore the problem in a latent space\ngenerated by a bio-inspired self-organizing clustering and statistical aspects\nof the latent space. In particular, we investigate the projections made by two\ntopology-preserving maps: the Self-Organizing Map and the Scale Invariant Map.\nOur method can be applied in both a supervised and an unsupervised context. We\nconstruct the assessment of changes in the data distribution as a comparison of\nGaussian signals, making the proposed method fast and robust. We compare it to\nother unsupervised techniques, specifically Principal Component Analysis (PCA)\nand Kernel-PCA. Our comparison involves conducting experiments using sequences\nof images (based on MNIST and injected shifts with adversarial samples),\nchemical sensor measurements, and the environmental variable related to ozone\nlevels. The empirical study reveals the potential of the proposed approach.\n", "link": "http://arxiv.org/abs/2404.16656v1", "date": "2024-04-25", "relevancy": 2.0078, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5188}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4936}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4807}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Self-Organizing%20Clustering%20System%20for%20Unsupervised%20Distribution%20Shift%0A%20%20Detection&body=Title%3A%20A%20Self-Organizing%20Clustering%20System%20for%20Unsupervised%20Distribution%20Shift%0A%20%20Detection%0AAuthor%3A%20Sebasti%C3%A1n%20Basterrech%20and%20Line%20Clemmensen%20and%20Gerardo%20Rubino%0AAbstract%3A%20%20%20Modeling%20non-stationary%20data%20is%20a%20challenging%20problem%20in%20the%20field%20of%0Acontinual%20learning%2C%20and%20data%20distribution%20shifts%20may%20result%20in%20negative%0Aconsequences%20on%20the%20performance%20of%20a%20machine%20learning%20model.%20Classic%20learning%0Atools%20are%20often%20vulnerable%20to%20perturbations%20of%20the%20input%20covariates%2C%20and%20are%0Asensitive%20to%20outliers%20and%20noise%2C%20and%20some%20tools%20are%20based%20on%20rigid%20algebraic%0Aassumptions.%20Distribution%20shifts%20are%20frequently%20occurring%20due%20to%20changes%20in%20raw%0Amaterials%20for%20production%2C%20seasonality%2C%20a%20different%20user%20base%2C%20or%20even%0Aadversarial%20attacks.%20Therefore%2C%20there%20is%20a%20need%20for%20more%20effective%20distribution%0Ashift%20detection%20techniques.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20continual%20learning%20framework%20for%20monitoring%20and%0Adetecting%20distribution%20changes.%20We%20explore%20the%20problem%20in%20a%20latent%20space%0Agenerated%20by%20a%20bio-inspired%20self-organizing%20clustering%20and%20statistical%20aspects%0Aof%20the%20latent%20space.%20In%20particular%2C%20we%20investigate%20the%20projections%20made%20by%20two%0Atopology-preserving%20maps%3A%20the%20Self-Organizing%20Map%20and%20the%20Scale%20Invariant%20Map.%0AOur%20method%20can%20be%20applied%20in%20both%20a%20supervised%20and%20an%20unsupervised%20context.%20We%0Aconstruct%20the%20assessment%20of%20changes%20in%20the%20data%20distribution%20as%20a%20comparison%20of%0AGaussian%20signals%2C%20making%20the%20proposed%20method%20fast%20and%20robust.%20We%20compare%20it%20to%0Aother%20unsupervised%20techniques%2C%20specifically%20Principal%20Component%20Analysis%20%28PCA%29%0Aand%20Kernel-PCA.%20Our%20comparison%20involves%20conducting%20experiments%20using%20sequences%0Aof%20images%20%28based%20on%20MNIST%20and%20injected%20shifts%20with%20adversarial%20samples%29%2C%0Achemical%20sensor%20measurements%2C%20and%20the%20environmental%20variable%20related%20to%20ozone%0Alevels.%20The%20empirical%20study%20reveals%20the%20potential%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16656v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Self-Organizing%20Clustering%20System%20for%20Unsupervised%20Distribution%20Shift%0A%20%20Detection&entry.906535625=Sebasti%C3%A1n%20Basterrech%20and%20Line%20Clemmensen%20and%20Gerardo%20Rubino&entry.1292438233=%20%20Modeling%20non-stationary%20data%20is%20a%20challenging%20problem%20in%20the%20field%20of%0Acontinual%20learning%2C%20and%20data%20distribution%20shifts%20may%20result%20in%20negative%0Aconsequences%20on%20the%20performance%20of%20a%20machine%20learning%20model.%20Classic%20learning%0Atools%20are%20often%20vulnerable%20to%20perturbations%20of%20the%20input%20covariates%2C%20and%20are%0Asensitive%20to%20outliers%20and%20noise%2C%20and%20some%20tools%20are%20based%20on%20rigid%20algebraic%0Aassumptions.%20Distribution%20shifts%20are%20frequently%20occurring%20due%20to%20changes%20in%20raw%0Amaterials%20for%20production%2C%20seasonality%2C%20a%20different%20user%20base%2C%20or%20even%0Aadversarial%20attacks.%20Therefore%2C%20there%20is%20a%20need%20for%20more%20effective%20distribution%0Ashift%20detection%20techniques.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20continual%20learning%20framework%20for%20monitoring%20and%0Adetecting%20distribution%20changes.%20We%20explore%20the%20problem%20in%20a%20latent%20space%0Agenerated%20by%20a%20bio-inspired%20self-organizing%20clustering%20and%20statistical%20aspects%0Aof%20the%20latent%20space.%20In%20particular%2C%20we%20investigate%20the%20projections%20made%20by%20two%0Atopology-preserving%20maps%3A%20the%20Self-Organizing%20Map%20and%20the%20Scale%20Invariant%20Map.%0AOur%20method%20can%20be%20applied%20in%20both%20a%20supervised%20and%20an%20unsupervised%20context.%20We%0Aconstruct%20the%20assessment%20of%20changes%20in%20the%20data%20distribution%20as%20a%20comparison%20of%0AGaussian%20signals%2C%20making%20the%20proposed%20method%20fast%20and%20robust.%20We%20compare%20it%20to%0Aother%20unsupervised%20techniques%2C%20specifically%20Principal%20Component%20Analysis%20%28PCA%29%0Aand%20Kernel-PCA.%20Our%20comparison%20involves%20conducting%20experiments%20using%20sequences%0Aof%20images%20%28based%20on%20MNIST%20and%20injected%20shifts%20with%20adversarial%20samples%29%2C%0Achemical%20sensor%20measurements%2C%20and%20the%20environmental%20variable%20related%20to%20ozone%0Alevels.%20The%20empirical%20study%20reveals%20the%20potential%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16656v1&entry.124074799=Read"},
{"title": "Real-Time 4K Super-Resolution of Compressed AVIF Images. AIS 2024\n  Challenge Survey", "author": "Marcos V. Conde and Zhijun Lei and Wen Li and Cosmin Stejerean and Ioannis Katsavounidis and Radu Timofte and Kihwan Yoon and Ganzorig Gankhuyag and Jiangtao Lv and Long Sun and Jinshan Pan and Jiangxin Dong and Jinhui Tang and Zhiyuan Li and Hao Wei and Chenyang Ge and Dongyang Zhang and Tianle Liu and Huaian Chen and Yi Jin and Menghan Zhou and Yiqiang Yan and Si Gao and Biao Wu and Shaoli Liu and Chengjian Zheng and Diankai Zhang and Ning Wang and Xintao Qiu and Yuanbo Zhou and Kongxian Wu and Xinwei Dai and Hui Tang and Wei Deng and Qingquan Gao and Tong Tong and Jae-Hyeon Lee and Ui-Jin Choi and Min Yan and Xin Liu and Qian Wang and Xiaoqian Ye and Zhan Du and Tiansen Zhang and Long Peng and Jiaming Guo and Xin Di and Bohao Liao and Zhibo Du and Peize Xia and Renjing Pei and Yang Wang and Yang Cao and Zhengjun Zha and Bingnan Han and Hongyuan Yu and Zhuoyuan Wu and Cheng Wan and Yuqing Liu and Haodong Yu and Jizhe Li and Zhijuan Huang and Yuan Huang and Yajun Zou and Xianyu Guan and Qi Jia and Heng Zhang and Xuanwu Yin and Kunlong Zuo and Hyeon-Cheol Moon and Tae-hyun Jeong and Yoonmo Yang and Jae-Gon Kim and Jinwoo Jeong and Sunjei Kim", "abstract": "  This paper introduces a novel benchmark as part of the AIS 2024 Real-Time\nImage Super-Resolution (RTSR) Challenge, which aims to upscale compressed\nimages from 540p to 4K resolution (4x factor) in real-time on commercial GPUs.\nFor this, we use a diverse test set containing a variety of 4K images ranging\nfrom digital art to gaming and photography. The images are compressed using the\nmodern AVIF codec, instead of JPEG. All the proposed methods improve PSNR\nfidelity over Lanczos interpolation, and process images under 10ms. Out of the\n160 participants, 25 teams submitted their code and models. The solutions\npresent novel designs tailored for memory-efficiency and runtime on edge\ndevices. This survey describes the best solutions for real-time SR of\ncompressed high-resolution images.\n", "link": "http://arxiv.org/abs/2404.16484v1", "date": "2024-04-25", "relevancy": 2.0, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5266}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4958}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4936}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Real-Time%204K%20Super-Resolution%20of%20Compressed%20AVIF%20Images.%20AIS%202024%0A%20%20Challenge%20Survey&body=Title%3A%20Real-Time%204K%20Super-Resolution%20of%20Compressed%20AVIF%20Images.%20AIS%202024%0A%20%20Challenge%20Survey%0AAuthor%3A%20Marcos%20V.%20Conde%20and%20Zhijun%20Lei%20and%20Wen%20Li%20and%20Cosmin%20Stejerean%20and%20Ioannis%20Katsavounidis%20and%20Radu%20Timofte%20and%20Kihwan%20Yoon%20and%20Ganzorig%20Gankhuyag%20and%20Jiangtao%20Lv%20and%20Long%20Sun%20and%20Jinshan%20Pan%20and%20Jiangxin%20Dong%20and%20Jinhui%20Tang%20and%20Zhiyuan%20Li%20and%20Hao%20Wei%20and%20Chenyang%20Ge%20and%20Dongyang%20Zhang%20and%20Tianle%20Liu%20and%20Huaian%20Chen%20and%20Yi%20Jin%20and%20Menghan%20Zhou%20and%20Yiqiang%20Yan%20and%20Si%20Gao%20and%20Biao%20Wu%20and%20Shaoli%20Liu%20and%20Chengjian%20Zheng%20and%20Diankai%20Zhang%20and%20Ning%20Wang%20and%20Xintao%20Qiu%20and%20Yuanbo%20Zhou%20and%20Kongxian%20Wu%20and%20Xinwei%20Dai%20and%20Hui%20Tang%20and%20Wei%20Deng%20and%20Qingquan%20Gao%20and%20Tong%20Tong%20and%20Jae-Hyeon%20Lee%20and%20Ui-Jin%20Choi%20and%20Min%20Yan%20and%20Xin%20Liu%20and%20Qian%20Wang%20and%20Xiaoqian%20Ye%20and%20Zhan%20Du%20and%20Tiansen%20Zhang%20and%20Long%20Peng%20and%20Jiaming%20Guo%20and%20Xin%20Di%20and%20Bohao%20Liao%20and%20Zhibo%20Du%20and%20Peize%20Xia%20and%20Renjing%20Pei%20and%20Yang%20Wang%20and%20Yang%20Cao%20and%20Zhengjun%20Zha%20and%20Bingnan%20Han%20and%20Hongyuan%20Yu%20and%20Zhuoyuan%20Wu%20and%20Cheng%20Wan%20and%20Yuqing%20Liu%20and%20Haodong%20Yu%20and%20Jizhe%20Li%20and%20Zhijuan%20Huang%20and%20Yuan%20Huang%20and%20Yajun%20Zou%20and%20Xianyu%20Guan%20and%20Qi%20Jia%20and%20Heng%20Zhang%20and%20Xuanwu%20Yin%20and%20Kunlong%20Zuo%20and%20Hyeon-Cheol%20Moon%20and%20Tae-hyun%20Jeong%20and%20Yoonmo%20Yang%20and%20Jae-Gon%20Kim%20and%20Jinwoo%20Jeong%20and%20Sunjei%20Kim%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20benchmark%20as%20part%20of%20the%20AIS%202024%20Real-Time%0AImage%20Super-Resolution%20%28RTSR%29%20Challenge%2C%20which%20aims%20to%20upscale%20compressed%0Aimages%20from%20540p%20to%204K%20resolution%20%284x%20factor%29%20in%20real-time%20on%20commercial%20GPUs.%0AFor%20this%2C%20we%20use%20a%20diverse%20test%20set%20containing%20a%20variety%20of%204K%20images%20ranging%0Afrom%20digital%20art%20to%20gaming%20and%20photography.%20The%20images%20are%20compressed%20using%20the%0Amodern%20AVIF%20codec%2C%20instead%20of%20JPEG.%20All%20the%20proposed%20methods%20improve%20PSNR%0Afidelity%20over%20Lanczos%20interpolation%2C%20and%20process%20images%20under%2010ms.%20Out%20of%20the%0A160%20participants%2C%2025%20teams%20submitted%20their%20code%20and%20models.%20The%20solutions%0Apresent%20novel%20designs%20tailored%20for%20memory-efficiency%20and%20runtime%20on%20edge%0Adevices.%20This%20survey%20describes%20the%20best%20solutions%20for%20real-time%20SR%20of%0Acompressed%20high-resolution%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16484v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%204K%20Super-Resolution%20of%20Compressed%20AVIF%20Images.%20AIS%202024%0A%20%20Challenge%20Survey&entry.906535625=Marcos%20V.%20Conde%20and%20Zhijun%20Lei%20and%20Wen%20Li%20and%20Cosmin%20Stejerean%20and%20Ioannis%20Katsavounidis%20and%20Radu%20Timofte%20and%20Kihwan%20Yoon%20and%20Ganzorig%20Gankhuyag%20and%20Jiangtao%20Lv%20and%20Long%20Sun%20and%20Jinshan%20Pan%20and%20Jiangxin%20Dong%20and%20Jinhui%20Tang%20and%20Zhiyuan%20Li%20and%20Hao%20Wei%20and%20Chenyang%20Ge%20and%20Dongyang%20Zhang%20and%20Tianle%20Liu%20and%20Huaian%20Chen%20and%20Yi%20Jin%20and%20Menghan%20Zhou%20and%20Yiqiang%20Yan%20and%20Si%20Gao%20and%20Biao%20Wu%20and%20Shaoli%20Liu%20and%20Chengjian%20Zheng%20and%20Diankai%20Zhang%20and%20Ning%20Wang%20and%20Xintao%20Qiu%20and%20Yuanbo%20Zhou%20and%20Kongxian%20Wu%20and%20Xinwei%20Dai%20and%20Hui%20Tang%20and%20Wei%20Deng%20and%20Qingquan%20Gao%20and%20Tong%20Tong%20and%20Jae-Hyeon%20Lee%20and%20Ui-Jin%20Choi%20and%20Min%20Yan%20and%20Xin%20Liu%20and%20Qian%20Wang%20and%20Xiaoqian%20Ye%20and%20Zhan%20Du%20and%20Tiansen%20Zhang%20and%20Long%20Peng%20and%20Jiaming%20Guo%20and%20Xin%20Di%20and%20Bohao%20Liao%20and%20Zhibo%20Du%20and%20Peize%20Xia%20and%20Renjing%20Pei%20and%20Yang%20Wang%20and%20Yang%20Cao%20and%20Zhengjun%20Zha%20and%20Bingnan%20Han%20and%20Hongyuan%20Yu%20and%20Zhuoyuan%20Wu%20and%20Cheng%20Wan%20and%20Yuqing%20Liu%20and%20Haodong%20Yu%20and%20Jizhe%20Li%20and%20Zhijuan%20Huang%20and%20Yuan%20Huang%20and%20Yajun%20Zou%20and%20Xianyu%20Guan%20and%20Qi%20Jia%20and%20Heng%20Zhang%20and%20Xuanwu%20Yin%20and%20Kunlong%20Zuo%20and%20Hyeon-Cheol%20Moon%20and%20Tae-hyun%20Jeong%20and%20Yoonmo%20Yang%20and%20Jae-Gon%20Kim%20and%20Jinwoo%20Jeong%20and%20Sunjei%20Kim&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20benchmark%20as%20part%20of%20the%20AIS%202024%20Real-Time%0AImage%20Super-Resolution%20%28RTSR%29%20Challenge%2C%20which%20aims%20to%20upscale%20compressed%0Aimages%20from%20540p%20to%204K%20resolution%20%284x%20factor%29%20in%20real-time%20on%20commercial%20GPUs.%0AFor%20this%2C%20we%20use%20a%20diverse%20test%20set%20containing%20a%20variety%20of%204K%20images%20ranging%0Afrom%20digital%20art%20to%20gaming%20and%20photography.%20The%20images%20are%20compressed%20using%20the%0Amodern%20AVIF%20codec%2C%20instead%20of%20JPEG.%20All%20the%20proposed%20methods%20improve%20PSNR%0Afidelity%20over%20Lanczos%20interpolation%2C%20and%20process%20images%20under%2010ms.%20Out%20of%20the%0A160%20participants%2C%2025%20teams%20submitted%20their%20code%20and%20models.%20The%20solutions%0Apresent%20novel%20designs%20tailored%20for%20memory-efficiency%20and%20runtime%20on%20edge%0Adevices.%20This%20survey%20describes%20the%20best%20solutions%20for%20real-time%20SR%20of%0Acompressed%20high-resolution%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16484v1&entry.124074799=Read"},
{"title": "Efficient and Near-Optimal Noise Generation for Streaming Differential\n  Privacy", "author": " Krishnamurthy and  Dvijotham and H. Brendan McMahan and Krishna Pillutla and Thomas Steinke and Abhradeep Thakurta", "abstract": "  In the task of differentially private (DP) continual counting, we receive a\nstream of increments and our goal is to output an approximate running total of\nthese increments, without revealing too much about any specific increment.\nDespite its simplicity, differentially private continual counting has attracted\nsignificant attention both in theory and in practice. Existing algorithms for\ndifferentially private continual counting are either inefficient in terms of\ntheir space usage or add an excessive amount of noise, inducing suboptimal\nutility.\n  The most practical DP continual counting algorithms add carefully correlated\nGaussian noise to the values. The task of choosing the covariance for this\nnoise can be expressed in terms of factoring the lower-triangular matrix of\nones (which computes prefix sums). We present two approaches from this class\n(for different parameter regimes) that achieve near-optimal utility for DP\ncontinual counting and only require logarithmic or polylogarithmic space (and\ntime).\n  Our first approach is based on a space-efficient streaming matrix\nmultiplication algorithm for a class of Toeplitz matrices. We show that to\ninstantiate this algorithm for DP continual counting, it is sufficient to find\na low-degree rational function that approximates the square root on a circle in\nthe complex plane. We then apply and extend tools from approximation theory to\nachieve this. We also derive efficient closed-forms for the objective function\nfor arbitrarily many steps, and show direct numerical optimization yields a\nhighly practical solution to the problem. Our second approach combines our\nfirst approach with a recursive construction similar to the binary tree\nmechanism.\n", "link": "http://arxiv.org/abs/2404.16706v1", "date": "2024-04-25", "relevancy": 1.9916, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5046}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4953}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4876}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Near-Optimal%20Noise%20Generation%20for%20Streaming%20Differential%0A%20%20Privacy&body=Title%3A%20Efficient%20and%20Near-Optimal%20Noise%20Generation%20for%20Streaming%20Differential%0A%20%20Privacy%0AAuthor%3A%20%20Krishnamurthy%20and%20%20Dvijotham%20and%20H.%20Brendan%20McMahan%20and%20Krishna%20Pillutla%20and%20Thomas%20Steinke%20and%20Abhradeep%20Thakurta%0AAbstract%3A%20%20%20In%20the%20task%20of%20differentially%20private%20%28DP%29%20continual%20counting%2C%20we%20receive%20a%0Astream%20of%20increments%20and%20our%20goal%20is%20to%20output%20an%20approximate%20running%20total%20of%0Athese%20increments%2C%20without%20revealing%20too%20much%20about%20any%20specific%20increment.%0ADespite%20its%20simplicity%2C%20differentially%20private%20continual%20counting%20has%20attracted%0Asignificant%20attention%20both%20in%20theory%20and%20in%20practice.%20Existing%20algorithms%20for%0Adifferentially%20private%20continual%20counting%20are%20either%20inefficient%20in%20terms%20of%0Atheir%20space%20usage%20or%20add%20an%20excessive%20amount%20of%20noise%2C%20inducing%20suboptimal%0Autility.%0A%20%20The%20most%20practical%20DP%20continual%20counting%20algorithms%20add%20carefully%20correlated%0AGaussian%20noise%20to%20the%20values.%20The%20task%20of%20choosing%20the%20covariance%20for%20this%0Anoise%20can%20be%20expressed%20in%20terms%20of%20factoring%20the%20lower-triangular%20matrix%20of%0Aones%20%28which%20computes%20prefix%20sums%29.%20We%20present%20two%20approaches%20from%20this%20class%0A%28for%20different%20parameter%20regimes%29%20that%20achieve%20near-optimal%20utility%20for%20DP%0Acontinual%20counting%20and%20only%20require%20logarithmic%20or%20polylogarithmic%20space%20%28and%0Atime%29.%0A%20%20Our%20first%20approach%20is%20based%20on%20a%20space-efficient%20streaming%20matrix%0Amultiplication%20algorithm%20for%20a%20class%20of%20Toeplitz%20matrices.%20We%20show%20that%20to%0Ainstantiate%20this%20algorithm%20for%20DP%20continual%20counting%2C%20it%20is%20sufficient%20to%20find%0Aa%20low-degree%20rational%20function%20that%20approximates%20the%20square%20root%20on%20a%20circle%20in%0Athe%20complex%20plane.%20We%20then%20apply%20and%20extend%20tools%20from%20approximation%20theory%20to%0Aachieve%20this.%20We%20also%20derive%20efficient%20closed-forms%20for%20the%20objective%20function%0Afor%20arbitrarily%20many%20steps%2C%20and%20show%20direct%20numerical%20optimization%20yields%20a%0Ahighly%20practical%20solution%20to%20the%20problem.%20Our%20second%20approach%20combines%20our%0Afirst%20approach%20with%20a%20recursive%20construction%20similar%20to%20the%20binary%20tree%0Amechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16706v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Near-Optimal%20Noise%20Generation%20for%20Streaming%20Differential%0A%20%20Privacy&entry.906535625=%20Krishnamurthy%20and%20%20Dvijotham%20and%20H.%20Brendan%20McMahan%20and%20Krishna%20Pillutla%20and%20Thomas%20Steinke%20and%20Abhradeep%20Thakurta&entry.1292438233=%20%20In%20the%20task%20of%20differentially%20private%20%28DP%29%20continual%20counting%2C%20we%20receive%20a%0Astream%20of%20increments%20and%20our%20goal%20is%20to%20output%20an%20approximate%20running%20total%20of%0Athese%20increments%2C%20without%20revealing%20too%20much%20about%20any%20specific%20increment.%0ADespite%20its%20simplicity%2C%20differentially%20private%20continual%20counting%20has%20attracted%0Asignificant%20attention%20both%20in%20theory%20and%20in%20practice.%20Existing%20algorithms%20for%0Adifferentially%20private%20continual%20counting%20are%20either%20inefficient%20in%20terms%20of%0Atheir%20space%20usage%20or%20add%20an%20excessive%20amount%20of%20noise%2C%20inducing%20suboptimal%0Autility.%0A%20%20The%20most%20practical%20DP%20continual%20counting%20algorithms%20add%20carefully%20correlated%0AGaussian%20noise%20to%20the%20values.%20The%20task%20of%20choosing%20the%20covariance%20for%20this%0Anoise%20can%20be%20expressed%20in%20terms%20of%20factoring%20the%20lower-triangular%20matrix%20of%0Aones%20%28which%20computes%20prefix%20sums%29.%20We%20present%20two%20approaches%20from%20this%20class%0A%28for%20different%20parameter%20regimes%29%20that%20achieve%20near-optimal%20utility%20for%20DP%0Acontinual%20counting%20and%20only%20require%20logarithmic%20or%20polylogarithmic%20space%20%28and%0Atime%29.%0A%20%20Our%20first%20approach%20is%20based%20on%20a%20space-efficient%20streaming%20matrix%0Amultiplication%20algorithm%20for%20a%20class%20of%20Toeplitz%20matrices.%20We%20show%20that%20to%0Ainstantiate%20this%20algorithm%20for%20DP%20continual%20counting%2C%20it%20is%20sufficient%20to%20find%0Aa%20low-degree%20rational%20function%20that%20approximates%20the%20square%20root%20on%20a%20circle%20in%0Athe%20complex%20plane.%20We%20then%20apply%20and%20extend%20tools%20from%20approximation%20theory%20to%0Aachieve%20this.%20We%20also%20derive%20efficient%20closed-forms%20for%20the%20objective%20function%0Afor%20arbitrarily%20many%20steps%2C%20and%20show%20direct%20numerical%20optimization%20yields%20a%0Ahighly%20practical%20solution%20to%20the%20problem.%20Our%20second%20approach%20combines%20our%0Afirst%20approach%20with%20a%20recursive%20construction%20similar%20to%20the%20binary%20tree%0Amechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16706v1&entry.124074799=Read"},
{"title": "Registration by Regression (RbR): a framework for interpretable and\n  flexible atlas registration", "author": "Karthik Gopinath and Xiaoling Hu and Malte Hoffmann and Oula Puonti and Juan Eugenio Iglesias", "abstract": "  In human neuroimaging studies, atlas registration enables mapping MRI scans\nto a common coordinate frame, which is necessary to aggregate data from\nmultiple subjects. Machine learning registration methods have achieved\nexcellent speed and accuracy but lack interpretability. More recently,\nkeypoint-based methods have been proposed to tackle this issue, but their\naccuracy is still subpar, particularly when fitting nonlinear transforms. Here\nwe propose Registration by Regression (RbR), a novel atlas registration\nframework that is highly robust and flexible, conceptually simple, and can be\ntrained with cheaply obtained data. RbR predicts the (x,y,z) atlas coordinates\nfor every voxel of the input scan (i.e., every voxel is a keypoint), and then\nuses closed-form expressions to quickly fit transforms using a wide array of\npossible deformation models, including affine and nonlinear (e.g., Bspline,\nDemons, invertible diffeomorphic models, etc.). Robustness is provided by the\nlarge number of voxels informing the registration and can be further increased\nby robust estimators like RANSAC. Experiments on independent public datasets\nshow that RbR yields more accurate registration than competing keypoint\napproaches, while providing full control of the deformation model.\n", "link": "http://arxiv.org/abs/2404.16781v1", "date": "2024-04-25", "relevancy": 1.9859, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4996}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4967}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4932}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Registration%20by%20Regression%20%28RbR%29%3A%20a%20framework%20for%20interpretable%20and%0A%20%20flexible%20atlas%20registration&body=Title%3A%20Registration%20by%20Regression%20%28RbR%29%3A%20a%20framework%20for%20interpretable%20and%0A%20%20flexible%20atlas%20registration%0AAuthor%3A%20Karthik%20Gopinath%20and%20Xiaoling%20Hu%20and%20Malte%20Hoffmann%20and%20Oula%20Puonti%20and%20Juan%20Eugenio%20Iglesias%0AAbstract%3A%20%20%20In%20human%20neuroimaging%20studies%2C%20atlas%20registration%20enables%20mapping%20MRI%20scans%0Ato%20a%20common%20coordinate%20frame%2C%20which%20is%20necessary%20to%20aggregate%20data%20from%0Amultiple%20subjects.%20Machine%20learning%20registration%20methods%20have%20achieved%0Aexcellent%20speed%20and%20accuracy%20but%20lack%20interpretability.%20More%20recently%2C%0Akeypoint-based%20methods%20have%20been%20proposed%20to%20tackle%20this%20issue%2C%20but%20their%0Aaccuracy%20is%20still%20subpar%2C%20particularly%20when%20fitting%20nonlinear%20transforms.%20Here%0Awe%20propose%20Registration%20by%20Regression%20%28RbR%29%2C%20a%20novel%20atlas%20registration%0Aframework%20that%20is%20highly%20robust%20and%20flexible%2C%20conceptually%20simple%2C%20and%20can%20be%0Atrained%20with%20cheaply%20obtained%20data.%20RbR%20predicts%20the%20%28x%2Cy%2Cz%29%20atlas%20coordinates%0Afor%20every%20voxel%20of%20the%20input%20scan%20%28i.e.%2C%20every%20voxel%20is%20a%20keypoint%29%2C%20and%20then%0Auses%20closed-form%20expressions%20to%20quickly%20fit%20transforms%20using%20a%20wide%20array%20of%0Apossible%20deformation%20models%2C%20including%20affine%20and%20nonlinear%20%28e.g.%2C%20Bspline%2C%0ADemons%2C%20invertible%20diffeomorphic%20models%2C%20etc.%29.%20Robustness%20is%20provided%20by%20the%0Alarge%20number%20of%20voxels%20informing%20the%20registration%20and%20can%20be%20further%20increased%0Aby%20robust%20estimators%20like%20RANSAC.%20Experiments%20on%20independent%20public%20datasets%0Ashow%20that%20RbR%20yields%20more%20accurate%20registration%20than%20competing%20keypoint%0Aapproaches%2C%20while%20providing%20full%20control%20of%20the%20deformation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16781v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Registration%20by%20Regression%20%28RbR%29%3A%20a%20framework%20for%20interpretable%20and%0A%20%20flexible%20atlas%20registration&entry.906535625=Karthik%20Gopinath%20and%20Xiaoling%20Hu%20and%20Malte%20Hoffmann%20and%20Oula%20Puonti%20and%20Juan%20Eugenio%20Iglesias&entry.1292438233=%20%20In%20human%20neuroimaging%20studies%2C%20atlas%20registration%20enables%20mapping%20MRI%20scans%0Ato%20a%20common%20coordinate%20frame%2C%20which%20is%20necessary%20to%20aggregate%20data%20from%0Amultiple%20subjects.%20Machine%20learning%20registration%20methods%20have%20achieved%0Aexcellent%20speed%20and%20accuracy%20but%20lack%20interpretability.%20More%20recently%2C%0Akeypoint-based%20methods%20have%20been%20proposed%20to%20tackle%20this%20issue%2C%20but%20their%0Aaccuracy%20is%20still%20subpar%2C%20particularly%20when%20fitting%20nonlinear%20transforms.%20Here%0Awe%20propose%20Registration%20by%20Regression%20%28RbR%29%2C%20a%20novel%20atlas%20registration%0Aframework%20that%20is%20highly%20robust%20and%20flexible%2C%20conceptually%20simple%2C%20and%20can%20be%0Atrained%20with%20cheaply%20obtained%20data.%20RbR%20predicts%20the%20%28x%2Cy%2Cz%29%20atlas%20coordinates%0Afor%20every%20voxel%20of%20the%20input%20scan%20%28i.e.%2C%20every%20voxel%20is%20a%20keypoint%29%2C%20and%20then%0Auses%20closed-form%20expressions%20to%20quickly%20fit%20transforms%20using%20a%20wide%20array%20of%0Apossible%20deformation%20models%2C%20including%20affine%20and%20nonlinear%20%28e.g.%2C%20Bspline%2C%0ADemons%2C%20invertible%20diffeomorphic%20models%2C%20etc.%29.%20Robustness%20is%20provided%20by%20the%0Alarge%20number%20of%20voxels%20informing%20the%20registration%20and%20can%20be%20further%20increased%0Aby%20robust%20estimators%20like%20RANSAC.%20Experiments%20on%20independent%20public%20datasets%0Ashow%20that%20RbR%20yields%20more%20accurate%20registration%20than%20competing%20keypoint%0Aapproaches%2C%20while%20providing%20full%20control%20of%20the%20deformation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16781v1&entry.124074799=Read"},
{"title": "Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding", "author": "Mostafa Elhoushi and Akshat Shrivastava and Diana Liskovich and Basil Hosmer and Bram Wasti and Liangzhen Lai and Anas Mahmoud and Bilge Acun and Saurabh Agarwal and Ahmed Roman and Ahmed A Aly and Beidi Chen and Carole-Jean Wu", "abstract": "  We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task.\n", "link": "http://arxiv.org/abs/2404.16710v1", "date": "2024-04-25", "relevancy": 1.9794, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4895}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Layer%20Skip%3A%20Enabling%20Early%20Exit%20Inference%20and%20Self-Speculative%20Decoding&body=Title%3A%20Layer%20Skip%3A%20Enabling%20Early%20Exit%20Inference%20and%20Self-Speculative%20Decoding%0AAuthor%3A%20Mostafa%20Elhoushi%20and%20Akshat%20Shrivastava%20and%20Diana%20Liskovich%20and%20Basil%20Hosmer%20and%20Bram%20Wasti%20and%20Liangzhen%20Lai%20and%20Anas%20Mahmoud%20and%20Bilge%20Acun%20and%20Saurabh%20Agarwal%20and%20Ahmed%20Roman%20and%20Ahmed%20A%20Aly%20and%20Beidi%20Chen%20and%20Carole-Jean%20Wu%0AAbstract%3A%20%20%20We%20present%20LayerSkip%2C%20an%20end-to-end%20solution%20to%20speed-up%20inference%20of%20large%0Alanguage%20models%20%28LLMs%29.%20First%2C%20during%20training%20we%20apply%20layer%20dropout%2C%20with%20low%0Adropout%20rates%20for%20earlier%20layers%20and%20higher%20dropout%20rates%20for%20later%20layers%2C%20and%0Aan%20early%20exit%20loss%20where%20all%20transformer%20layers%20share%20the%20same%20exit.%20Second%2C%0Aduring%20inference%2C%20we%20show%20that%20this%20training%20recipe%20increases%20the%20accuracy%20of%0Aearly%20exit%20at%20earlier%20layers%2C%20without%20adding%20any%20auxiliary%20layers%20or%20modules%20to%0Athe%20model.%20Third%2C%20we%20present%20a%20novel%20self-speculative%20decoding%20solution%20where%0Awe%20exit%20at%20early%20layers%20and%20verify%20and%20correct%20with%20remaining%20layers%20of%20the%0Amodel.%20Our%20proposed%20self-speculative%20decoding%20approach%20has%20less%20memory%0Afootprint%20than%20other%20speculative%20decoding%20approaches%20and%20benefits%20from%20shared%0Acompute%20and%20activations%20of%20the%20draft%20and%20verification%20stages.%20We%20run%0Aexperiments%20on%20different%20Llama%20model%20sizes%20on%20different%20types%20of%20training%3A%0Apretraining%20from%20scratch%2C%20continual%20pretraining%2C%20finetuning%20on%20specific%20data%0Adomain%2C%20and%20finetuning%20on%20specific%20task.%20We%20implement%20our%20inference%20solution%0Aand%20show%20speedups%20of%20up%20to%202.16x%20on%20summarization%20for%20CNN/DM%20documents%2C%201.82x%0Aon%20coding%2C%20and%202.0x%20on%20TOPv2%20semantic%20parsing%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16710v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layer%20Skip%3A%20Enabling%20Early%20Exit%20Inference%20and%20Self-Speculative%20Decoding&entry.906535625=Mostafa%20Elhoushi%20and%20Akshat%20Shrivastava%20and%20Diana%20Liskovich%20and%20Basil%20Hosmer%20and%20Bram%20Wasti%20and%20Liangzhen%20Lai%20and%20Anas%20Mahmoud%20and%20Bilge%20Acun%20and%20Saurabh%20Agarwal%20and%20Ahmed%20Roman%20and%20Ahmed%20A%20Aly%20and%20Beidi%20Chen%20and%20Carole-Jean%20Wu&entry.1292438233=%20%20We%20present%20LayerSkip%2C%20an%20end-to-end%20solution%20to%20speed-up%20inference%20of%20large%0Alanguage%20models%20%28LLMs%29.%20First%2C%20during%20training%20we%20apply%20layer%20dropout%2C%20with%20low%0Adropout%20rates%20for%20earlier%20layers%20and%20higher%20dropout%20rates%20for%20later%20layers%2C%20and%0Aan%20early%20exit%20loss%20where%20all%20transformer%20layers%20share%20the%20same%20exit.%20Second%2C%0Aduring%20inference%2C%20we%20show%20that%20this%20training%20recipe%20increases%20the%20accuracy%20of%0Aearly%20exit%20at%20earlier%20layers%2C%20without%20adding%20any%20auxiliary%20layers%20or%20modules%20to%0Athe%20model.%20Third%2C%20we%20present%20a%20novel%20self-speculative%20decoding%20solution%20where%0Awe%20exit%20at%20early%20layers%20and%20verify%20and%20correct%20with%20remaining%20layers%20of%20the%0Amodel.%20Our%20proposed%20self-speculative%20decoding%20approach%20has%20less%20memory%0Afootprint%20than%20other%20speculative%20decoding%20approaches%20and%20benefits%20from%20shared%0Acompute%20and%20activations%20of%20the%20draft%20and%20verification%20stages.%20We%20run%0Aexperiments%20on%20different%20Llama%20model%20sizes%20on%20different%20types%20of%20training%3A%0Apretraining%20from%20scratch%2C%20continual%20pretraining%2C%20finetuning%20on%20specific%20data%0Adomain%2C%20and%20finetuning%20on%20specific%20task.%20We%20implement%20our%20inference%20solution%0Aand%20show%20speedups%20of%20up%20to%202.16x%20on%20summarization%20for%20CNN/DM%20documents%2C%201.82x%0Aon%20coding%2C%20and%202.0x%20on%20TOPv2%20semantic%20parsing%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16710v1&entry.124074799=Read"},
{"title": "Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and\n  Human Ratings", "author": "Olivia Wiles and Chuhan Zhang and Isabela Albuquerque and Ivana Kaji\u0107 and Su Wang and Emanuele Bugliarello and Yasumasa Onoe and Chris Knutsen and Cyrus Rashtchian and Jordi Pont-Tuset and Aida Nematzadeh", "abstract": "  While text-to-image (T2I) generative models have become ubiquitous, they do\nnot necessarily generate images that align with a given prompt. While previous\nwork has evaluated T2I alignment by proposing metrics, benchmarks, and\ntemplates for collecting human judgements, the quality of these components is\nnot systematically measured. Human-rated prompt sets are generally small and\nthe reliability of the ratings -- and thereby the prompt set used to compare\nmodels -- is not evaluated. We address this gap by performing an extensive\nstudy evaluating auto-eval metrics and human templates. We provide three main\ncontributions: (1) We introduce a comprehensive skills-based benchmark that can\ndiscriminate models across different human templates. This skills-based\nbenchmark categorises prompts into sub-skills, allowing a practitioner to\npinpoint not only which skills are challenging, but at what level of complexity\na skill becomes challenging. (2) We gather human ratings across four templates\nand four T2I models for a total of >100K annotations. This allows us to\nunderstand where differences arise due to inherent ambiguity in the prompt and\nwhere they arise due to differences in metric and model quality. (3) Finally,\nwe introduce a new QA-based auto-eval metric that is better correlated with\nhuman ratings than existing metrics for our new dataset, across different human\ntemplates, and on TIFA160.\n", "link": "http://arxiv.org/abs/2404.16820v1", "date": "2024-04-25", "relevancy": 1.9776, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4915}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4871}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Text-to-Image%20Evaluation%20with%20Gecko%3A%20On%20Metrics%2C%20Prompts%2C%20and%0A%20%20Human%20Ratings&body=Title%3A%20Revisiting%20Text-to-Image%20Evaluation%20with%20Gecko%3A%20On%20Metrics%2C%20Prompts%2C%20and%0A%20%20Human%20Ratings%0AAuthor%3A%20Olivia%20Wiles%20and%20Chuhan%20Zhang%20and%20Isabela%20Albuquerque%20and%20Ivana%20Kaji%C4%87%20and%20Su%20Wang%20and%20Emanuele%20Bugliarello%20and%20Yasumasa%20Onoe%20and%20Chris%20Knutsen%20and%20Cyrus%20Rashtchian%20and%20Jordi%20Pont-Tuset%20and%20Aida%20Nematzadeh%0AAbstract%3A%20%20%20While%20text-to-image%20%28T2I%29%20generative%20models%20have%20become%20ubiquitous%2C%20they%20do%0Anot%20necessarily%20generate%20images%20that%20align%20with%20a%20given%20prompt.%20While%20previous%0Awork%20has%20evaluated%20T2I%20alignment%20by%20proposing%20metrics%2C%20benchmarks%2C%20and%0Atemplates%20for%20collecting%20human%20judgements%2C%20the%20quality%20of%20these%20components%20is%0Anot%20systematically%20measured.%20Human-rated%20prompt%20sets%20are%20generally%20small%20and%0Athe%20reliability%20of%20the%20ratings%20--%20and%20thereby%20the%20prompt%20set%20used%20to%20compare%0Amodels%20--%20is%20not%20evaluated.%20We%20address%20this%20gap%20by%20performing%20an%20extensive%0Astudy%20evaluating%20auto-eval%20metrics%20and%20human%20templates.%20We%20provide%20three%20main%0Acontributions%3A%20%281%29%20We%20introduce%20a%20comprehensive%20skills-based%20benchmark%20that%20can%0Adiscriminate%20models%20across%20different%20human%20templates.%20This%20skills-based%0Abenchmark%20categorises%20prompts%20into%20sub-skills%2C%20allowing%20a%20practitioner%20to%0Apinpoint%20not%20only%20which%20skills%20are%20challenging%2C%20but%20at%20what%20level%20of%20complexity%0Aa%20skill%20becomes%20challenging.%20%282%29%20We%20gather%20human%20ratings%20across%20four%20templates%0Aand%20four%20T2I%20models%20for%20a%20total%20of%20%3E100K%20annotations.%20This%20allows%20us%20to%0Aunderstand%20where%20differences%20arise%20due%20to%20inherent%20ambiguity%20in%20the%20prompt%20and%0Awhere%20they%20arise%20due%20to%20differences%20in%20metric%20and%20model%20quality.%20%283%29%20Finally%2C%0Awe%20introduce%20a%20new%20QA-based%20auto-eval%20metric%20that%20is%20better%20correlated%20with%0Ahuman%20ratings%20than%20existing%20metrics%20for%20our%20new%20dataset%2C%20across%20different%20human%0Atemplates%2C%20and%20on%20TIFA160.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16820v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Text-to-Image%20Evaluation%20with%20Gecko%3A%20On%20Metrics%2C%20Prompts%2C%20and%0A%20%20Human%20Ratings&entry.906535625=Olivia%20Wiles%20and%20Chuhan%20Zhang%20and%20Isabela%20Albuquerque%20and%20Ivana%20Kaji%C4%87%20and%20Su%20Wang%20and%20Emanuele%20Bugliarello%20and%20Yasumasa%20Onoe%20and%20Chris%20Knutsen%20and%20Cyrus%20Rashtchian%20and%20Jordi%20Pont-Tuset%20and%20Aida%20Nematzadeh&entry.1292438233=%20%20While%20text-to-image%20%28T2I%29%20generative%20models%20have%20become%20ubiquitous%2C%20they%20do%0Anot%20necessarily%20generate%20images%20that%20align%20with%20a%20given%20prompt.%20While%20previous%0Awork%20has%20evaluated%20T2I%20alignment%20by%20proposing%20metrics%2C%20benchmarks%2C%20and%0Atemplates%20for%20collecting%20human%20judgements%2C%20the%20quality%20of%20these%20components%20is%0Anot%20systematically%20measured.%20Human-rated%20prompt%20sets%20are%20generally%20small%20and%0Athe%20reliability%20of%20the%20ratings%20--%20and%20thereby%20the%20prompt%20set%20used%20to%20compare%0Amodels%20--%20is%20not%20evaluated.%20We%20address%20this%20gap%20by%20performing%20an%20extensive%0Astudy%20evaluating%20auto-eval%20metrics%20and%20human%20templates.%20We%20provide%20three%20main%0Acontributions%3A%20%281%29%20We%20introduce%20a%20comprehensive%20skills-based%20benchmark%20that%20can%0Adiscriminate%20models%20across%20different%20human%20templates.%20This%20skills-based%0Abenchmark%20categorises%20prompts%20into%20sub-skills%2C%20allowing%20a%20practitioner%20to%0Apinpoint%20not%20only%20which%20skills%20are%20challenging%2C%20but%20at%20what%20level%20of%20complexity%0Aa%20skill%20becomes%20challenging.%20%282%29%20We%20gather%20human%20ratings%20across%20four%20templates%0Aand%20four%20T2I%20models%20for%20a%20total%20of%20%3E100K%20annotations.%20This%20allows%20us%20to%0Aunderstand%20where%20differences%20arise%20due%20to%20inherent%20ambiguity%20in%20the%20prompt%20and%0Awhere%20they%20arise%20due%20to%20differences%20in%20metric%20and%20model%20quality.%20%283%29%20Finally%2C%0Awe%20introduce%20a%20new%20QA-based%20auto-eval%20metric%20that%20is%20better%20correlated%20with%0Ahuman%20ratings%20than%20existing%20metrics%20for%20our%20new%20dataset%2C%20across%20different%20human%0Atemplates%2C%20and%20on%20TIFA160.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16820v1&entry.124074799=Read"},
{"title": "Exploring the Dynamics of Data Transmission in 5G Networks: A Conceptual\n  Analysis", "author": "Nikita Smirnov and Sven Tomforde", "abstract": "  This conceptual analysis examines the dynamics of data transmission in 5G\nnetworks. It addresses various aspects of sending data from cameras and LiDARs\ninstalled on a remote-controlled ferry to a land-based control center. The\nrange of topics includes all stages of video and LiDAR data processing from\nacquisition and encoding to final decoding, all aspects of their transmission\nand reception via the WebRTC protocol, and all possible types of network\nproblems such as handovers or congestion that could affect the quality of\nexperience for end-users. A series of experiments were conducted to evaluate\nthe key aspects of the data transmission. These include simulation-based\nreproducible runs and real-world experiments conducted using open-source\nsolutions we developed: \"Gymir5G\" - an OMNeT++-based 5G simulation and\n\"GstWebRTCApp\" - a GStreamer-based application for adaptive control of media\nstreams over the WebRTC protocol. One of the goals of this study is to\nformulate the bandwidth and latency requirements for reliable real-time\ncommunication and to estimate their approximate values. This goal was achieved\nthrough simulation-based experiments involving docking maneuvers in the Bay of\nKiel, Germany. The final latency for the entire data processing pipeline was\nalso estimated during the real tests. In addition, a series of simulation-based\nexperiments showed the impact of key WebRTC features and demonstrated the\neffectiveness of the WebRTC protocol, while the conducted video codec\ncomparison showed that the hardware-accelerated H.264 codec is the best.\nFinally, the research addresses the topic of adaptive communication, where the\ntraditional congestion avoidance and deep reinforcement learning approaches\nwere analyzed. The comparison in a sandbox scenario shows that the AI-based\nsolution outperforms the WebRTC baseline GCC algorithm in terms of data rates,\nlatency, and packet loss.\n", "link": "http://arxiv.org/abs/2404.16508v1", "date": "2024-04-25", "relevancy": 1.9672, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4984}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4903}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4792}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Dynamics%20of%20Data%20Transmission%20in%205G%20Networks%3A%20A%20Conceptual%0A%20%20Analysis&body=Title%3A%20Exploring%20the%20Dynamics%20of%20Data%20Transmission%20in%205G%20Networks%3A%20A%20Conceptual%0A%20%20Analysis%0AAuthor%3A%20Nikita%20Smirnov%20and%20Sven%20Tomforde%0AAbstract%3A%20%20%20This%20conceptual%20analysis%20examines%20the%20dynamics%20of%20data%20transmission%20in%205G%0Anetworks.%20It%20addresses%20various%20aspects%20of%20sending%20data%20from%20cameras%20and%20LiDARs%0Ainstalled%20on%20a%20remote-controlled%20ferry%20to%20a%20land-based%20control%20center.%20The%0Arange%20of%20topics%20includes%20all%20stages%20of%20video%20and%20LiDAR%20data%20processing%20from%0Aacquisition%20and%20encoding%20to%20final%20decoding%2C%20all%20aspects%20of%20their%20transmission%0Aand%20reception%20via%20the%20WebRTC%20protocol%2C%20and%20all%20possible%20types%20of%20network%0Aproblems%20such%20as%20handovers%20or%20congestion%20that%20could%20affect%20the%20quality%20of%0Aexperience%20for%20end-users.%20A%20series%20of%20experiments%20were%20conducted%20to%20evaluate%0Athe%20key%20aspects%20of%20the%20data%20transmission.%20These%20include%20simulation-based%0Areproducible%20runs%20and%20real-world%20experiments%20conducted%20using%20open-source%0Asolutions%20we%20developed%3A%20%22Gymir5G%22%20-%20an%20OMNeT%2B%2B-based%205G%20simulation%20and%0A%22GstWebRTCApp%22%20-%20a%20GStreamer-based%20application%20for%20adaptive%20control%20of%20media%0Astreams%20over%20the%20WebRTC%20protocol.%20One%20of%20the%20goals%20of%20this%20study%20is%20to%0Aformulate%20the%20bandwidth%20and%20latency%20requirements%20for%20reliable%20real-time%0Acommunication%20and%20to%20estimate%20their%20approximate%20values.%20This%20goal%20was%20achieved%0Athrough%20simulation-based%20experiments%20involving%20docking%20maneuvers%20in%20the%20Bay%20of%0AKiel%2C%20Germany.%20The%20final%20latency%20for%20the%20entire%20data%20processing%20pipeline%20was%0Aalso%20estimated%20during%20the%20real%20tests.%20In%20addition%2C%20a%20series%20of%20simulation-based%0Aexperiments%20showed%20the%20impact%20of%20key%20WebRTC%20features%20and%20demonstrated%20the%0Aeffectiveness%20of%20the%20WebRTC%20protocol%2C%20while%20the%20conducted%20video%20codec%0Acomparison%20showed%20that%20the%20hardware-accelerated%20H.264%20codec%20is%20the%20best.%0AFinally%2C%20the%20research%20addresses%20the%20topic%20of%20adaptive%20communication%2C%20where%20the%0Atraditional%20congestion%20avoidance%20and%20deep%20reinforcement%20learning%20approaches%0Awere%20analyzed.%20The%20comparison%20in%20a%20sandbox%20scenario%20shows%20that%20the%20AI-based%0Asolution%20outperforms%20the%20WebRTC%20baseline%20GCC%20algorithm%20in%20terms%20of%20data%20rates%2C%0Alatency%2C%20and%20packet%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16508v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Dynamics%20of%20Data%20Transmission%20in%205G%20Networks%3A%20A%20Conceptual%0A%20%20Analysis&entry.906535625=Nikita%20Smirnov%20and%20Sven%20Tomforde&entry.1292438233=%20%20This%20conceptual%20analysis%20examines%20the%20dynamics%20of%20data%20transmission%20in%205G%0Anetworks.%20It%20addresses%20various%20aspects%20of%20sending%20data%20from%20cameras%20and%20LiDARs%0Ainstalled%20on%20a%20remote-controlled%20ferry%20to%20a%20land-based%20control%20center.%20The%0Arange%20of%20topics%20includes%20all%20stages%20of%20video%20and%20LiDAR%20data%20processing%20from%0Aacquisition%20and%20encoding%20to%20final%20decoding%2C%20all%20aspects%20of%20their%20transmission%0Aand%20reception%20via%20the%20WebRTC%20protocol%2C%20and%20all%20possible%20types%20of%20network%0Aproblems%20such%20as%20handovers%20or%20congestion%20that%20could%20affect%20the%20quality%20of%0Aexperience%20for%20end-users.%20A%20series%20of%20experiments%20were%20conducted%20to%20evaluate%0Athe%20key%20aspects%20of%20the%20data%20transmission.%20These%20include%20simulation-based%0Areproducible%20runs%20and%20real-world%20experiments%20conducted%20using%20open-source%0Asolutions%20we%20developed%3A%20%22Gymir5G%22%20-%20an%20OMNeT%2B%2B-based%205G%20simulation%20and%0A%22GstWebRTCApp%22%20-%20a%20GStreamer-based%20application%20for%20adaptive%20control%20of%20media%0Astreams%20over%20the%20WebRTC%20protocol.%20One%20of%20the%20goals%20of%20this%20study%20is%20to%0Aformulate%20the%20bandwidth%20and%20latency%20requirements%20for%20reliable%20real-time%0Acommunication%20and%20to%20estimate%20their%20approximate%20values.%20This%20goal%20was%20achieved%0Athrough%20simulation-based%20experiments%20involving%20docking%20maneuvers%20in%20the%20Bay%20of%0AKiel%2C%20Germany.%20The%20final%20latency%20for%20the%20entire%20data%20processing%20pipeline%20was%0Aalso%20estimated%20during%20the%20real%20tests.%20In%20addition%2C%20a%20series%20of%20simulation-based%0Aexperiments%20showed%20the%20impact%20of%20key%20WebRTC%20features%20and%20demonstrated%20the%0Aeffectiveness%20of%20the%20WebRTC%20protocol%2C%20while%20the%20conducted%20video%20codec%0Acomparison%20showed%20that%20the%20hardware-accelerated%20H.264%20codec%20is%20the%20best.%0AFinally%2C%20the%20research%20addresses%20the%20topic%20of%20adaptive%20communication%2C%20where%20the%0Atraditional%20congestion%20avoidance%20and%20deep%20reinforcement%20learning%20approaches%0Awere%20analyzed.%20The%20comparison%20in%20a%20sandbox%20scenario%20shows%20that%20the%20AI-based%0Asolution%20outperforms%20the%20WebRTC%20baseline%20GCC%20algorithm%20in%20terms%20of%20data%20rates%2C%0Alatency%2C%20and%20packet%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16508v1&entry.124074799=Read"},
{"title": "Redefining Safety for Autonomous Vehicles", "author": "Philip Koopman and William Widen", "abstract": "  Existing definitions and associated conceptual frameworks for computer-based\nsystem safety should be revisited in light of real-world experiences from\ndeploying autonomous vehicles. Current terminology used by industry safety\nstandards emphasizes mitigation of risk from specifically identified hazards,\nand carries assumptions based on human-supervised vehicle operation. Operation\nwithout a human driver dramatically increases the scope of safety concerns,\nespecially due to operation in an open world environment, a requirement to\nself-enforce operational limits, participation in an ad hoc sociotechnical\nsystem of systems, and a requirement to conform to both legal and ethical\nconstraints. Existing standards and terminology only partially address these\nnew challenges. We propose updated definitions for core system safety concepts\nthat encompass these additional considerations as a starting point for evolving\nsafe-ty approaches to address these additional safety challenges. These results\nmight additionally inform framing safety terminology for other autonomous\nsystem applications.\n", "link": "http://arxiv.org/abs/2404.16768v1", "date": "2024-04-25", "relevancy": 1.9552, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4724}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Redefining%20Safety%20for%20Autonomous%20Vehicles&body=Title%3A%20Redefining%20Safety%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Philip%20Koopman%20and%20William%20Widen%0AAbstract%3A%20%20%20Existing%20definitions%20and%20associated%20conceptual%20frameworks%20for%20computer-based%0Asystem%20safety%20should%20be%20revisited%20in%20light%20of%20real-world%20experiences%20from%0Adeploying%20autonomous%20vehicles.%20Current%20terminology%20used%20by%20industry%20safety%0Astandards%20emphasizes%20mitigation%20of%20risk%20from%20specifically%20identified%20hazards%2C%0Aand%20carries%20assumptions%20based%20on%20human-supervised%20vehicle%20operation.%20Operation%0Awithout%20a%20human%20driver%20dramatically%20increases%20the%20scope%20of%20safety%20concerns%2C%0Aespecially%20due%20to%20operation%20in%20an%20open%20world%20environment%2C%20a%20requirement%20to%0Aself-enforce%20operational%20limits%2C%20participation%20in%20an%20ad%20hoc%20sociotechnical%0Asystem%20of%20systems%2C%20and%20a%20requirement%20to%20conform%20to%20both%20legal%20and%20ethical%0Aconstraints.%20Existing%20standards%20and%20terminology%20only%20partially%20address%20these%0Anew%20challenges.%20We%20propose%20updated%20definitions%20for%20core%20system%20safety%20concepts%0Athat%20encompass%20these%20additional%20considerations%20as%20a%20starting%20point%20for%20evolving%0Asafe-ty%20approaches%20to%20address%20these%20additional%20safety%20challenges.%20These%20results%0Amight%20additionally%20inform%20framing%20safety%20terminology%20for%20other%20autonomous%0Asystem%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16768v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redefining%20Safety%20for%20Autonomous%20Vehicles&entry.906535625=Philip%20Koopman%20and%20William%20Widen&entry.1292438233=%20%20Existing%20definitions%20and%20associated%20conceptual%20frameworks%20for%20computer-based%0Asystem%20safety%20should%20be%20revisited%20in%20light%20of%20real-world%20experiences%20from%0Adeploying%20autonomous%20vehicles.%20Current%20terminology%20used%20by%20industry%20safety%0Astandards%20emphasizes%20mitigation%20of%20risk%20from%20specifically%20identified%20hazards%2C%0Aand%20carries%20assumptions%20based%20on%20human-supervised%20vehicle%20operation.%20Operation%0Awithout%20a%20human%20driver%20dramatically%20increases%20the%20scope%20of%20safety%20concerns%2C%0Aespecially%20due%20to%20operation%20in%20an%20open%20world%20environment%2C%20a%20requirement%20to%0Aself-enforce%20operational%20limits%2C%20participation%20in%20an%20ad%20hoc%20sociotechnical%0Asystem%20of%20systems%2C%20and%20a%20requirement%20to%20conform%20to%20both%20legal%20and%20ethical%0Aconstraints.%20Existing%20standards%20and%20terminology%20only%20partially%20address%20these%0Anew%20challenges.%20We%20propose%20updated%20definitions%20for%20core%20system%20safety%20concepts%0Athat%20encompass%20these%20additional%20considerations%20as%20a%20starting%20point%20for%20evolving%0Asafe-ty%20approaches%20to%20address%20these%20additional%20safety%20challenges.%20These%20results%0Amight%20additionally%20inform%20framing%20safety%20terminology%20for%20other%20autonomous%0Asystem%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16768v1&entry.124074799=Read"},
{"title": "COBRA -- COnfidence score Based on shape Regression Analysis for\n  method-independent quality assessment of object pose estimation from single\n  images", "author": "Panagiotis Sapoutzoglou and Georgios Giapitzakis Tzintanos and George Terzakis and Maria Pateraki", "abstract": "  We present a generic algorithm for scoring pose estimation methods that rely\non single image semantic analysis. The algorithm employs a lightweight putative\nshape representation using a combination of multiple Gaussian Processes. Each\nGaussian Process (GP) yields distance normal distributions from multiple\nreference points in the object's coordinate system to its surface, thus\nproviding a geometric evaluation framework for scoring predicted poses. Our\nconfidence measure comprises the average mixture probability of pixel\nback-projections onto the shape template. In the reported experiments, we\ncompare the accuracy of our GP based representation of objects versus the\nactual geometric models and demonstrate the ability of our method to capture\nthe influence of outliers as opposed to the corresponding intrinsic measures\nthat ship with the segmentation and pose estimation methods.\n", "link": "http://arxiv.org/abs/2404.16471v1", "date": "2024-04-25", "relevancy": 1.9502, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5129}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4915}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4735}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20COBRA%20--%20COnfidence%20score%20Based%20on%20shape%20Regression%20Analysis%20for%0A%20%20method-independent%20quality%20assessment%20of%20object%20pose%20estimation%20from%20single%0A%20%20images&body=Title%3A%20COBRA%20--%20COnfidence%20score%20Based%20on%20shape%20Regression%20Analysis%20for%0A%20%20method-independent%20quality%20assessment%20of%20object%20pose%20estimation%20from%20single%0A%20%20images%0AAuthor%3A%20Panagiotis%20Sapoutzoglou%20and%20Georgios%20Giapitzakis%20Tzintanos%20and%20George%20Terzakis%20and%20Maria%20Pateraki%0AAbstract%3A%20%20%20We%20present%20a%20generic%20algorithm%20for%20scoring%20pose%20estimation%20methods%20that%20rely%0Aon%20single%20image%20semantic%20analysis.%20The%20algorithm%20employs%20a%20lightweight%20putative%0Ashape%20representation%20using%20a%20combination%20of%20multiple%20Gaussian%20Processes.%20Each%0AGaussian%20Process%20%28GP%29%20yields%20distance%20normal%20distributions%20from%20multiple%0Areference%20points%20in%20the%20object%27s%20coordinate%20system%20to%20its%20surface%2C%20thus%0Aproviding%20a%20geometric%20evaluation%20framework%20for%20scoring%20predicted%20poses.%20Our%0Aconfidence%20measure%20comprises%20the%20average%20mixture%20probability%20of%20pixel%0Aback-projections%20onto%20the%20shape%20template.%20In%20the%20reported%20experiments%2C%20we%0Acompare%20the%20accuracy%20of%20our%20GP%20based%20representation%20of%20objects%20versus%20the%0Aactual%20geometric%20models%20and%20demonstrate%20the%20ability%20of%20our%20method%20to%20capture%0Athe%20influence%20of%20outliers%20as%20opposed%20to%20the%20corresponding%20intrinsic%20measures%0Athat%20ship%20with%20the%20segmentation%20and%20pose%20estimation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16471v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COBRA%20--%20COnfidence%20score%20Based%20on%20shape%20Regression%20Analysis%20for%0A%20%20method-independent%20quality%20assessment%20of%20object%20pose%20estimation%20from%20single%0A%20%20images&entry.906535625=Panagiotis%20Sapoutzoglou%20and%20Georgios%20Giapitzakis%20Tzintanos%20and%20George%20Terzakis%20and%20Maria%20Pateraki&entry.1292438233=%20%20We%20present%20a%20generic%20algorithm%20for%20scoring%20pose%20estimation%20methods%20that%20rely%0Aon%20single%20image%20semantic%20analysis.%20The%20algorithm%20employs%20a%20lightweight%20putative%0Ashape%20representation%20using%20a%20combination%20of%20multiple%20Gaussian%20Processes.%20Each%0AGaussian%20Process%20%28GP%29%20yields%20distance%20normal%20distributions%20from%20multiple%0Areference%20points%20in%20the%20object%27s%20coordinate%20system%20to%20its%20surface%2C%20thus%0Aproviding%20a%20geometric%20evaluation%20framework%20for%20scoring%20predicted%20poses.%20Our%0Aconfidence%20measure%20comprises%20the%20average%20mixture%20probability%20of%20pixel%0Aback-projections%20onto%20the%20shape%20template.%20In%20the%20reported%20experiments%2C%20we%0Acompare%20the%20accuracy%20of%20our%20GP%20based%20representation%20of%20objects%20versus%20the%0Aactual%20geometric%20models%20and%20demonstrate%20the%20ability%20of%20our%20method%20to%20capture%0Athe%20influence%20of%20outliers%20as%20opposed%20to%20the%20corresponding%20intrinsic%20measures%0Athat%20ship%20with%20the%20segmentation%20and%20pose%20estimation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16471v1&entry.124074799=Read"},
{"title": "Understanding Privacy Risks of Embeddings Induced by Large Language\n  Models", "author": "Zhihao Zhu and Ninglu Shao and Defu Lian and Chenwang Wu and Zheng Liu and Yi Yang and Enhong Chen", "abstract": "  Large language models (LLMs) show early signs of artificial general\nintelligence but struggle with hallucinations. One promising solution to\nmitigate these hallucinations is to store external knowledge as embeddings,\naiding LLMs in retrieval-augmented generation. However, such a solution risks\ncompromising privacy, as recent studies experimentally showed that the original\ntext can be partially reconstructed from text embeddings by pre-trained\nlanguage models. The significant advantage of LLMs over traditional pre-trained\nmodels may exacerbate these concerns. To this end, we investigate the\neffectiveness of reconstructing original knowledge and predicting entity\nattributes from these embeddings when LLMs are employed. Empirical findings\nindicate that LLMs significantly improve the accuracy of two evaluated tasks\nover those from pre-trained models, regardless of whether the texts are\nin-distribution or out-of-distribution. This underscores a heightened potential\nfor LLMs to jeopardize user privacy, highlighting the negative consequences of\ntheir widespread use. We further discuss preliminary strategies to mitigate\nthis risk.\n", "link": "http://arxiv.org/abs/2404.16587v1", "date": "2024-04-25", "relevancy": 1.938, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5286}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4792}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4721}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Understanding%20Privacy%20Risks%20of%20Embeddings%20Induced%20by%20Large%20Language%0A%20%20Models&body=Title%3A%20Understanding%20Privacy%20Risks%20of%20Embeddings%20Induced%20by%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Zhihao%20Zhu%20and%20Ninglu%20Shao%20and%20Defu%20Lian%20and%20Chenwang%20Wu%20and%20Zheng%20Liu%20and%20Yi%20Yang%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20show%20early%20signs%20of%20artificial%20general%0Aintelligence%20but%20struggle%20with%20hallucinations.%20One%20promising%20solution%20to%0Amitigate%20these%20hallucinations%20is%20to%20store%20external%20knowledge%20as%20embeddings%2C%0Aaiding%20LLMs%20in%20retrieval-augmented%20generation.%20However%2C%20such%20a%20solution%20risks%0Acompromising%20privacy%2C%20as%20recent%20studies%20experimentally%20showed%20that%20the%20original%0Atext%20can%20be%20partially%20reconstructed%20from%20text%20embeddings%20by%20pre-trained%0Alanguage%20models.%20The%20significant%20advantage%20of%20LLMs%20over%20traditional%20pre-trained%0Amodels%20may%20exacerbate%20these%20concerns.%20To%20this%20end%2C%20we%20investigate%20the%0Aeffectiveness%20of%20reconstructing%20original%20knowledge%20and%20predicting%20entity%0Aattributes%20from%20these%20embeddings%20when%20LLMs%20are%20employed.%20Empirical%20findings%0Aindicate%20that%20LLMs%20significantly%20improve%20the%20accuracy%20of%20two%20evaluated%20tasks%0Aover%20those%20from%20pre-trained%20models%2C%20regardless%20of%20whether%20the%20texts%20are%0Ain-distribution%20or%20out-of-distribution.%20This%20underscores%20a%20heightened%20potential%0Afor%20LLMs%20to%20jeopardize%20user%20privacy%2C%20highlighting%20the%20negative%20consequences%20of%0Atheir%20widespread%20use.%20We%20further%20discuss%20preliminary%20strategies%20to%20mitigate%0Athis%20risk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16587v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Privacy%20Risks%20of%20Embeddings%20Induced%20by%20Large%20Language%0A%20%20Models&entry.906535625=Zhihao%20Zhu%20and%20Ninglu%20Shao%20and%20Defu%20Lian%20and%20Chenwang%20Wu%20and%20Zheng%20Liu%20and%20Yi%20Yang%20and%20Enhong%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20show%20early%20signs%20of%20artificial%20general%0Aintelligence%20but%20struggle%20with%20hallucinations.%20One%20promising%20solution%20to%0Amitigate%20these%20hallucinations%20is%20to%20store%20external%20knowledge%20as%20embeddings%2C%0Aaiding%20LLMs%20in%20retrieval-augmented%20generation.%20However%2C%20such%20a%20solution%20risks%0Acompromising%20privacy%2C%20as%20recent%20studies%20experimentally%20showed%20that%20the%20original%0Atext%20can%20be%20partially%20reconstructed%20from%20text%20embeddings%20by%20pre-trained%0Alanguage%20models.%20The%20significant%20advantage%20of%20LLMs%20over%20traditional%20pre-trained%0Amodels%20may%20exacerbate%20these%20concerns.%20To%20this%20end%2C%20we%20investigate%20the%0Aeffectiveness%20of%20reconstructing%20original%20knowledge%20and%20predicting%20entity%0Aattributes%20from%20these%20embeddings%20when%20LLMs%20are%20employed.%20Empirical%20findings%0Aindicate%20that%20LLMs%20significantly%20improve%20the%20accuracy%20of%20two%20evaluated%20tasks%0Aover%20those%20from%20pre-trained%20models%2C%20regardless%20of%20whether%20the%20texts%20are%0Ain-distribution%20or%20out-of-distribution.%20This%20underscores%20a%20heightened%20potential%0Afor%20LLMs%20to%20jeopardize%20user%20privacy%2C%20highlighting%20the%20negative%20consequences%20of%0Atheir%20widespread%20use.%20We%20further%20discuss%20preliminary%20strategies%20to%20mitigate%0Athis%20risk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16587v1&entry.124074799=Read"},
{"title": "Weak-to-Strong Extrapolation Expedites Alignment", "author": "Chujie Zheng and Ziqi Wang and Heng Ji and Minlie Huang and Nanyun Peng", "abstract": "  Although the capabilities of large language models (LLMs) ideally scale up\nwith increasing data and compute, they are inevitably constrained by limited\nresources in reality. Suppose we have a moderately trained LLM (e.g., trained\nto align with human preference) in hand, can we further exploit its potential\nand cheaply acquire a stronger model? In this paper, we propose a simple method\ncalled ExPO to boost LLMs' alignment with human preference. ExPO assumes that a\nmedium-aligned model can be interpolated between a less-aligned (weaker) model,\ne.g., the initial SFT model, and a better-aligned (stronger) one, thereby\ndirectly obtaining this stronger model by extrapolating from the weights of the\nformer two relatively weaker models. On the AlpacaEval 2.0 benchmark, we show\nthat ExPO pushes models trained with less preference data (e.g., 10% or 20%) to\nreach and even surpass the fully-trained one, without any additional training.\nFurthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and\nexhibits decent scalability across model sizes from 7B to 70B. Our work\ndemonstrates the efficacy of model extrapolation in exploiting LLMs'\ncapabilities, suggesting a promising direction that deserves future\nexploration.\n", "link": "http://arxiv.org/abs/2404.16792v1", "date": "2024-04-25", "relevancy": 1.9338, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4889}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4755}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Weak-to-Strong%20Extrapolation%20Expedites%20Alignment&body=Title%3A%20Weak-to-Strong%20Extrapolation%20Expedites%20Alignment%0AAuthor%3A%20Chujie%20Zheng%20and%20Ziqi%20Wang%20and%20Heng%20Ji%20and%20Minlie%20Huang%20and%20Nanyun%20Peng%0AAbstract%3A%20%20%20Although%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20ideally%20scale%20up%0Awith%20increasing%20data%20and%20compute%2C%20they%20are%20inevitably%20constrained%20by%20limited%0Aresources%20in%20reality.%20Suppose%20we%20have%20a%20moderately%20trained%20LLM%20%28e.g.%2C%20trained%0Ato%20align%20with%20human%20preference%29%20in%20hand%2C%20can%20we%20further%20exploit%20its%20potential%0Aand%20cheaply%20acquire%20a%20stronger%20model%3F%20In%20this%20paper%2C%20we%20propose%20a%20simple%20method%0Acalled%20ExPO%20to%20boost%20LLMs%27%20alignment%20with%20human%20preference.%20ExPO%20assumes%20that%20a%0Amedium-aligned%20model%20can%20be%20interpolated%20between%20a%20less-aligned%20%28weaker%29%20model%2C%0Ae.g.%2C%20the%20initial%20SFT%20model%2C%20and%20a%20better-aligned%20%28stronger%29%20one%2C%20thereby%0Adirectly%20obtaining%20this%20stronger%20model%20by%20extrapolating%20from%20the%20weights%20of%20the%0Aformer%20two%20relatively%20weaker%20models.%20On%20the%20AlpacaEval%202.0%20benchmark%2C%20we%20show%0Athat%20ExPO%20pushes%20models%20trained%20with%20less%20preference%20data%20%28e.g.%2C%2010%25%20or%2020%25%29%20to%0Areach%20and%20even%20surpass%20the%20fully-trained%20one%2C%20without%20any%20additional%20training.%0AFurthermore%2C%20ExPO%20also%20significantly%20improves%20off-the-shelf%20DPO/RLHF%20models%20and%0Aexhibits%20decent%20scalability%20across%20model%20sizes%20from%207B%20to%2070B.%20Our%20work%0Ademonstrates%20the%20efficacy%20of%20model%20extrapolation%20in%20exploiting%20LLMs%27%0Acapabilities%2C%20suggesting%20a%20promising%20direction%20that%20deserves%20future%0Aexploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16792v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak-to-Strong%20Extrapolation%20Expedites%20Alignment&entry.906535625=Chujie%20Zheng%20and%20Ziqi%20Wang%20and%20Heng%20Ji%20and%20Minlie%20Huang%20and%20Nanyun%20Peng&entry.1292438233=%20%20Although%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20ideally%20scale%20up%0Awith%20increasing%20data%20and%20compute%2C%20they%20are%20inevitably%20constrained%20by%20limited%0Aresources%20in%20reality.%20Suppose%20we%20have%20a%20moderately%20trained%20LLM%20%28e.g.%2C%20trained%0Ato%20align%20with%20human%20preference%29%20in%20hand%2C%20can%20we%20further%20exploit%20its%20potential%0Aand%20cheaply%20acquire%20a%20stronger%20model%3F%20In%20this%20paper%2C%20we%20propose%20a%20simple%20method%0Acalled%20ExPO%20to%20boost%20LLMs%27%20alignment%20with%20human%20preference.%20ExPO%20assumes%20that%20a%0Amedium-aligned%20model%20can%20be%20interpolated%20between%20a%20less-aligned%20%28weaker%29%20model%2C%0Ae.g.%2C%20the%20initial%20SFT%20model%2C%20and%20a%20better-aligned%20%28stronger%29%20one%2C%20thereby%0Adirectly%20obtaining%20this%20stronger%20model%20by%20extrapolating%20from%20the%20weights%20of%20the%0Aformer%20two%20relatively%20weaker%20models.%20On%20the%20AlpacaEval%202.0%20benchmark%2C%20we%20show%0Athat%20ExPO%20pushes%20models%20trained%20with%20less%20preference%20data%20%28e.g.%2C%2010%25%20or%2020%25%29%20to%0Areach%20and%20even%20surpass%20the%20fully-trained%20one%2C%20without%20any%20additional%20training.%0AFurthermore%2C%20ExPO%20also%20significantly%20improves%20off-the-shelf%20DPO/RLHF%20models%20and%0Aexhibits%20decent%20scalability%20across%20model%20sizes%20from%207B%20to%2070B.%20Our%20work%0Ademonstrates%20the%20efficacy%20of%20model%20extrapolation%20in%20exploiting%20LLMs%27%0Acapabilities%2C%20suggesting%20a%20promising%20direction%20that%20deserves%20future%0Aexploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16792v1&entry.124074799=Read"},
{"title": "Efficient Solution of Point-Line Absolute Pose", "author": "Petr Hruby and Timothy Duff and Marc Pollefeys", "abstract": "  We revisit certain problems of pose estimation based on 3D--2D\ncorrespondences between features which may be points or lines. Specifically, we\naddress the two previously-studied minimal problems of estimating camera\nextrinsics from $p \\in \\{ 1, 2 \\}$ point--point correspondences and $l=3-p$\nline--line correspondences. To the best of our knowledge, all of the\npreviously-known practical solutions to these problems required computing the\nroots of degree $\\ge 4$ (univariate) polynomials when $p=2$, or degree $\\ge 8$\npolynomials when $p=1.$ We describe and implement two elementary solutions\nwhich reduce the degrees of the needed polynomials from $4$ to $2$ and from $8$\nto $4$, respectively. We show experimentally that the resulting solvers are\nnumerically stable and fast: when compared to the previous state-of-the art, we\nmay obtain nearly an order of magnitude speedup. The code is available at\n\\url{https://github.com/petrhruby97/efficient\\_absolute}\n", "link": "http://arxiv.org/abs/2404.16552v1", "date": "2024-04-25", "relevancy": 1.9247, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5113}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4647}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4576}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Solution%20of%20Point-Line%20Absolute%20Pose&body=Title%3A%20Efficient%20Solution%20of%20Point-Line%20Absolute%20Pose%0AAuthor%3A%20Petr%20Hruby%20and%20Timothy%20Duff%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20We%20revisit%20certain%20problems%20of%20pose%20estimation%20based%20on%203D--2D%0Acorrespondences%20between%20features%20which%20may%20be%20points%20or%20lines.%20Specifically%2C%20we%0Aaddress%20the%20two%20previously-studied%20minimal%20problems%20of%20estimating%20camera%0Aextrinsics%20from%20%24p%20%5Cin%20%5C%7B%201%2C%202%20%5C%7D%24%20point--point%20correspondences%20and%20%24l%3D3-p%24%0Aline--line%20correspondences.%20To%20the%20best%20of%20our%20knowledge%2C%20all%20of%20the%0Apreviously-known%20practical%20solutions%20to%20these%20problems%20required%20computing%20the%0Aroots%20of%20degree%20%24%5Cge%204%24%20%28univariate%29%20polynomials%20when%20%24p%3D2%24%2C%20or%20degree%20%24%5Cge%208%24%0Apolynomials%20when%20%24p%3D1.%24%20We%20describe%20and%20implement%20two%20elementary%20solutions%0Awhich%20reduce%20the%20degrees%20of%20the%20needed%20polynomials%20from%20%244%24%20to%20%242%24%20and%20from%20%248%24%0Ato%20%244%24%2C%20respectively.%20We%20show%20experimentally%20that%20the%20resulting%20solvers%20are%0Anumerically%20stable%20and%20fast%3A%20when%20compared%20to%20the%20previous%20state-of-the%20art%2C%20we%0Amay%20obtain%20nearly%20an%20order%20of%20magnitude%20speedup.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/petrhruby97/efficient%5C_absolute%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16552v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Solution%20of%20Point-Line%20Absolute%20Pose&entry.906535625=Petr%20Hruby%20and%20Timothy%20Duff%20and%20Marc%20Pollefeys&entry.1292438233=%20%20We%20revisit%20certain%20problems%20of%20pose%20estimation%20based%20on%203D--2D%0Acorrespondences%20between%20features%20which%20may%20be%20points%20or%20lines.%20Specifically%2C%20we%0Aaddress%20the%20two%20previously-studied%20minimal%20problems%20of%20estimating%20camera%0Aextrinsics%20from%20%24p%20%5Cin%20%5C%7B%201%2C%202%20%5C%7D%24%20point--point%20correspondences%20and%20%24l%3D3-p%24%0Aline--line%20correspondences.%20To%20the%20best%20of%20our%20knowledge%2C%20all%20of%20the%0Apreviously-known%20practical%20solutions%20to%20these%20problems%20required%20computing%20the%0Aroots%20of%20degree%20%24%5Cge%204%24%20%28univariate%29%20polynomials%20when%20%24p%3D2%24%2C%20or%20degree%20%24%5Cge%208%24%0Apolynomials%20when%20%24p%3D1.%24%20We%20describe%20and%20implement%20two%20elementary%20solutions%0Awhich%20reduce%20the%20degrees%20of%20the%20needed%20polynomials%20from%20%244%24%20to%20%242%24%20and%20from%20%248%24%0Ato%20%244%24%2C%20respectively.%20We%20show%20experimentally%20that%20the%20resulting%20solvers%20are%0Anumerically%20stable%20and%20fast%3A%20when%20compared%20to%20the%20previous%20state-of-the%20art%2C%20we%0Amay%20obtain%20nearly%20an%20order%20of%20magnitude%20speedup.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/petrhruby97/efficient%5C_absolute%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16552v1&entry.124074799=Read"},
{"title": "Prompt Cache: Modular Attention Reuse for Low-Latency Inference", "author": "In Gim and Guojun Chen and Seung-seob Lee and Nikhil Sarda and Anurag Khandelwal and Lin Zhong", "abstract": "  We present Prompt Cache, an approach for accelerating inference for large\nlanguage models (LLM) by reusing attention states across different LLM prompts.\nMany input prompts have overlapping text segments, such as system messages,\nprompt templates, and documents provided for context. Our key insight is that\nby precomputing and storing the attention states of these frequently occurring\ntext segments on the inference server, we can efficiently reuse them when these\nsegments appear in user prompts. Prompt Cache employs a schema to explicitly\ndefine such reusable text segments, called prompt modules. The schema ensures\npositional accuracy during attention state reuse and provides users with an\ninterface to access cached states in their prompt. Using a prototype\nimplementation, we evaluate Prompt Cache across several LLMs. We show that\nPrompt Cache significantly reduce latency in time-to-first-token, especially\nfor longer prompts such as document-based question answering and\nrecommendations. The improvements range from 8x for GPU-based inference to 60x\nfor CPU-based inference, all while maintaining output accuracy and without the\nneed for model parameter modifications.\n", "link": "http://arxiv.org/abs/2311.04934v2", "date": "2024-04-25", "relevancy": 1.9062, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4998}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4873}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4566}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Prompt%20Cache%3A%20Modular%20Attention%20Reuse%20for%20Low-Latency%20Inference&body=Title%3A%20Prompt%20Cache%3A%20Modular%20Attention%20Reuse%20for%20Low-Latency%20Inference%0AAuthor%3A%20In%20Gim%20and%20Guojun%20Chen%20and%20Seung-seob%20Lee%20and%20Nikhil%20Sarda%20and%20Anurag%20Khandelwal%20and%20Lin%20Zhong%0AAbstract%3A%20%20%20We%20present%20Prompt%20Cache%2C%20an%20approach%20for%20accelerating%20inference%20for%20large%0Alanguage%20models%20%28LLM%29%20by%20reusing%20attention%20states%20across%20different%20LLM%20prompts.%0AMany%20input%20prompts%20have%20overlapping%20text%20segments%2C%20such%20as%20system%20messages%2C%0Aprompt%20templates%2C%20and%20documents%20provided%20for%20context.%20Our%20key%20insight%20is%20that%0Aby%20precomputing%20and%20storing%20the%20attention%20states%20of%20these%20frequently%20occurring%0Atext%20segments%20on%20the%20inference%20server%2C%20we%20can%20efficiently%20reuse%20them%20when%20these%0Asegments%20appear%20in%20user%20prompts.%20Prompt%20Cache%20employs%20a%20schema%20to%20explicitly%0Adefine%20such%20reusable%20text%20segments%2C%20called%20prompt%20modules.%20The%20schema%20ensures%0Apositional%20accuracy%20during%20attention%20state%20reuse%20and%20provides%20users%20with%20an%0Ainterface%20to%20access%20cached%20states%20in%20their%20prompt.%20Using%20a%20prototype%0Aimplementation%2C%20we%20evaluate%20Prompt%20Cache%20across%20several%20LLMs.%20We%20show%20that%0APrompt%20Cache%20significantly%20reduce%20latency%20in%20time-to-first-token%2C%20especially%0Afor%20longer%20prompts%20such%20as%20document-based%20question%20answering%20and%0Arecommendations.%20The%20improvements%20range%20from%208x%20for%20GPU-based%20inference%20to%2060x%0Afor%20CPU-based%20inference%2C%20all%20while%20maintaining%20output%20accuracy%20and%20without%20the%0Aneed%20for%20model%20parameter%20modifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04934v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Cache%3A%20Modular%20Attention%20Reuse%20for%20Low-Latency%20Inference&entry.906535625=In%20Gim%20and%20Guojun%20Chen%20and%20Seung-seob%20Lee%20and%20Nikhil%20Sarda%20and%20Anurag%20Khandelwal%20and%20Lin%20Zhong&entry.1292438233=%20%20We%20present%20Prompt%20Cache%2C%20an%20approach%20for%20accelerating%20inference%20for%20large%0Alanguage%20models%20%28LLM%29%20by%20reusing%20attention%20states%20across%20different%20LLM%20prompts.%0AMany%20input%20prompts%20have%20overlapping%20text%20segments%2C%20such%20as%20system%20messages%2C%0Aprompt%20templates%2C%20and%20documents%20provided%20for%20context.%20Our%20key%20insight%20is%20that%0Aby%20precomputing%20and%20storing%20the%20attention%20states%20of%20these%20frequently%20occurring%0Atext%20segments%20on%20the%20inference%20server%2C%20we%20can%20efficiently%20reuse%20them%20when%20these%0Asegments%20appear%20in%20user%20prompts.%20Prompt%20Cache%20employs%20a%20schema%20to%20explicitly%0Adefine%20such%20reusable%20text%20segments%2C%20called%20prompt%20modules.%20The%20schema%20ensures%0Apositional%20accuracy%20during%20attention%20state%20reuse%20and%20provides%20users%20with%20an%0Ainterface%20to%20access%20cached%20states%20in%20their%20prompt.%20Using%20a%20prototype%0Aimplementation%2C%20we%20evaluate%20Prompt%20Cache%20across%20several%20LLMs.%20We%20show%20that%0APrompt%20Cache%20significantly%20reduce%20latency%20in%20time-to-first-token%2C%20especially%0Afor%20longer%20prompts%20such%20as%20document-based%20question%20answering%20and%0Arecommendations.%20The%20improvements%20range%20from%208x%20for%20GPU-based%20inference%20to%2060x%0Afor%20CPU-based%20inference%2C%20all%20while%20maintaining%20output%20accuracy%20and%20without%20the%0Aneed%20for%20model%20parameter%20modifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04934v2&entry.124074799=Read"},
{"title": "T-Explainer: A Model-Agnostic Explainability Framework Based on\n  Gradients", "author": "Evandro S. Ortigossa and F\u00e1bio F. Dias and Brian Barr and Claudio T. Silva and Luis Gustavo Nonato", "abstract": "  The development of machine learning applications has increased significantly\nin recent years, motivated by the remarkable ability of learning-powered\nsystems to discover and generalize intricate patterns hidden in massive\ndatasets. Modern learning models, while powerful, often exhibit a level of\ncomplexity that renders them opaque black boxes, resulting in a notable lack of\ntransparency that hinders our ability to decipher their decision-making\nprocesses. Opacity challenges the interpretability and practical application of\nmachine learning, especially in critical domains where understanding the\nunderlying reasons is essential for informed decision-making. Explainable\nArtificial Intelligence (XAI) rises to meet that challenge, unraveling the\ncomplexity of black boxes by providing elucidating explanations. Among the\nvarious XAI approaches, feature attribution/importance XAI stands out for its\ncapacity to delineate the significance of input features in the prediction\nprocess. However, most existing attribution methods have limitations, such as\ninstability, when divergent explanations may result from similar or even the\nsame instance. In this work, we introduce T-Explainer, a novel local additive\nattribution explainer based on Taylor expansion endowed with desirable\nproperties, such as local accuracy and consistency, while stable over multiple\nruns. We demonstrate T-Explainer's effectiveness through benchmark experiments\nwith well-known attribution methods. In addition, T-Explainer is developed as a\ncomprehensive XAI framework comprising quantitative metrics to assess and\nvisualize attribution explanations.\n", "link": "http://arxiv.org/abs/2404.16495v1", "date": "2024-04-25", "relevancy": 1.8986, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4952}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4727}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4684}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20T-Explainer%3A%20A%20Model-Agnostic%20Explainability%20Framework%20Based%20on%0A%20%20Gradients&body=Title%3A%20T-Explainer%3A%20A%20Model-Agnostic%20Explainability%20Framework%20Based%20on%0A%20%20Gradients%0AAuthor%3A%20Evandro%20S.%20Ortigossa%20and%20F%C3%A1bio%20F.%20Dias%20and%20Brian%20Barr%20and%20Claudio%20T.%20Silva%20and%20Luis%20Gustavo%20Nonato%0AAbstract%3A%20%20%20The%20development%20of%20machine%20learning%20applications%20has%20increased%20significantly%0Ain%20recent%20years%2C%20motivated%20by%20the%20remarkable%20ability%20of%20learning-powered%0Asystems%20to%20discover%20and%20generalize%20intricate%20patterns%20hidden%20in%20massive%0Adatasets.%20Modern%20learning%20models%2C%20while%20powerful%2C%20often%20exhibit%20a%20level%20of%0Acomplexity%20that%20renders%20them%20opaque%20black%20boxes%2C%20resulting%20in%20a%20notable%20lack%20of%0Atransparency%20that%20hinders%20our%20ability%20to%20decipher%20their%20decision-making%0Aprocesses.%20Opacity%20challenges%20the%20interpretability%20and%20practical%20application%20of%0Amachine%20learning%2C%20especially%20in%20critical%20domains%20where%20understanding%20the%0Aunderlying%20reasons%20is%20essential%20for%20informed%20decision-making.%20Explainable%0AArtificial%20Intelligence%20%28XAI%29%20rises%20to%20meet%20that%20challenge%2C%20unraveling%20the%0Acomplexity%20of%20black%20boxes%20by%20providing%20elucidating%20explanations.%20Among%20the%0Avarious%20XAI%20approaches%2C%20feature%20attribution/importance%20XAI%20stands%20out%20for%20its%0Acapacity%20to%20delineate%20the%20significance%20of%20input%20features%20in%20the%20prediction%0Aprocess.%20However%2C%20most%20existing%20attribution%20methods%20have%20limitations%2C%20such%20as%0Ainstability%2C%20when%20divergent%20explanations%20may%20result%20from%20similar%20or%20even%20the%0Asame%20instance.%20In%20this%20work%2C%20we%20introduce%20T-Explainer%2C%20a%20novel%20local%20additive%0Aattribution%20explainer%20based%20on%20Taylor%20expansion%20endowed%20with%20desirable%0Aproperties%2C%20such%20as%20local%20accuracy%20and%20consistency%2C%20while%20stable%20over%20multiple%0Aruns.%20We%20demonstrate%20T-Explainer%27s%20effectiveness%20through%20benchmark%20experiments%0Awith%20well-known%20attribution%20methods.%20In%20addition%2C%20T-Explainer%20is%20developed%20as%20a%0Acomprehensive%20XAI%20framework%20comprising%20quantitative%20metrics%20to%20assess%20and%0Avisualize%20attribution%20explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16495v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-Explainer%3A%20A%20Model-Agnostic%20Explainability%20Framework%20Based%20on%0A%20%20Gradients&entry.906535625=Evandro%20S.%20Ortigossa%20and%20F%C3%A1bio%20F.%20Dias%20and%20Brian%20Barr%20and%20Claudio%20T.%20Silva%20and%20Luis%20Gustavo%20Nonato&entry.1292438233=%20%20The%20development%20of%20machine%20learning%20applications%20has%20increased%20significantly%0Ain%20recent%20years%2C%20motivated%20by%20the%20remarkable%20ability%20of%20learning-powered%0Asystems%20to%20discover%20and%20generalize%20intricate%20patterns%20hidden%20in%20massive%0Adatasets.%20Modern%20learning%20models%2C%20while%20powerful%2C%20often%20exhibit%20a%20level%20of%0Acomplexity%20that%20renders%20them%20opaque%20black%20boxes%2C%20resulting%20in%20a%20notable%20lack%20of%0Atransparency%20that%20hinders%20our%20ability%20to%20decipher%20their%20decision-making%0Aprocesses.%20Opacity%20challenges%20the%20interpretability%20and%20practical%20application%20of%0Amachine%20learning%2C%20especially%20in%20critical%20domains%20where%20understanding%20the%0Aunderlying%20reasons%20is%20essential%20for%20informed%20decision-making.%20Explainable%0AArtificial%20Intelligence%20%28XAI%29%20rises%20to%20meet%20that%20challenge%2C%20unraveling%20the%0Acomplexity%20of%20black%20boxes%20by%20providing%20elucidating%20explanations.%20Among%20the%0Avarious%20XAI%20approaches%2C%20feature%20attribution/importance%20XAI%20stands%20out%20for%20its%0Acapacity%20to%20delineate%20the%20significance%20of%20input%20features%20in%20the%20prediction%0Aprocess.%20However%2C%20most%20existing%20attribution%20methods%20have%20limitations%2C%20such%20as%0Ainstability%2C%20when%20divergent%20explanations%20may%20result%20from%20similar%20or%20even%20the%0Asame%20instance.%20In%20this%20work%2C%20we%20introduce%20T-Explainer%2C%20a%20novel%20local%20additive%0Aattribution%20explainer%20based%20on%20Taylor%20expansion%20endowed%20with%20desirable%0Aproperties%2C%20such%20as%20local%20accuracy%20and%20consistency%2C%20while%20stable%20over%20multiple%0Aruns.%20We%20demonstrate%20T-Explainer%27s%20effectiveness%20through%20benchmark%20experiments%0Awith%20well-known%20attribution%20methods.%20In%20addition%2C%20T-Explainer%20is%20developed%20as%20a%0Acomprehensive%20XAI%20framework%20comprising%20quantitative%20metrics%20to%20assess%20and%0Avisualize%20attribution%20explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16495v1&entry.124074799=Read"},
{"title": "REBEL: Reinforcement Learning via Regressing Relative Rewards", "author": "Zhaolin Gao and Jonathan D. Chang and Wenhao Zhan and Owen Oertell and Gokul Swamy and Kiant\u00e9 Brantley and Thorsten Joachims and J. Andrew Bagnell and Jason D. Lee and Wen Sun", "abstract": "  While originally developed for continuous control problems, Proximal Policy\nOptimization (PPO) has emerged as the work-horse of a variety of reinforcement\nlearning (RL) applications including the fine-tuning of generative models.\nUnfortunately, PPO requires multiple heuristics to enable stable convergence\n(e.g. value networks, clipping) and is notorious for its sensitivity to the\nprecise implementation of these components. In response, we take a step back\nand ask what a minimalist RL algorithm for the era of generative models would\nlook like. We propose REBEL, an algorithm that cleanly reduces the problem of\npolicy optimization to regressing the relative rewards via a direct policy\nparameterization between two completions to a prompt, enabling strikingly\nlightweight implementation. In theory, we prove that fundamental RL algorithms\nlike Natural Policy Gradient can be seen as variants of REBEL, which allows us\nto match the strongest known theoretical guarantees in terms of convergence and\nsample complexity in the RL literature. REBEL can also cleanly incorporate\noffline data and handle the intransitive preferences we frequently see in\npractice. Empirically, we find that REBEL provides a unified approach to\nlanguage modeling and image generation with stronger or similar performance as\nPPO and DPO, all while being simpler to implement and more computationally\ntractable than PPO.\n", "link": "http://arxiv.org/abs/2404.16767v1", "date": "2024-04-25", "relevancy": 1.8937, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4913}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4641}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4593}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20REBEL%3A%20Reinforcement%20Learning%20via%20Regressing%20Relative%20Rewards&body=Title%3A%20REBEL%3A%20Reinforcement%20Learning%20via%20Regressing%20Relative%20Rewards%0AAuthor%3A%20Zhaolin%20Gao%20and%20Jonathan%20D.%20Chang%20and%20Wenhao%20Zhan%20and%20Owen%20Oertell%20and%20Gokul%20Swamy%20and%20Kiant%C3%A9%20Brantley%20and%20Thorsten%20Joachims%20and%20J.%20Andrew%20Bagnell%20and%20Jason%20D.%20Lee%20and%20Wen%20Sun%0AAbstract%3A%20%20%20While%20originally%20developed%20for%20continuous%20control%20problems%2C%20Proximal%20Policy%0AOptimization%20%28PPO%29%20has%20emerged%20as%20the%20work-horse%20of%20a%20variety%20of%20reinforcement%0Alearning%20%28RL%29%20applications%20including%20the%20fine-tuning%20of%20generative%20models.%0AUnfortunately%2C%20PPO%20requires%20multiple%20heuristics%20to%20enable%20stable%20convergence%0A%28e.g.%20value%20networks%2C%20clipping%29%20and%20is%20notorious%20for%20its%20sensitivity%20to%20the%0Aprecise%20implementation%20of%20these%20components.%20In%20response%2C%20we%20take%20a%20step%20back%0Aand%20ask%20what%20a%20minimalist%20RL%20algorithm%20for%20the%20era%20of%20generative%20models%20would%0Alook%20like.%20We%20propose%20REBEL%2C%20an%20algorithm%20that%20cleanly%20reduces%20the%20problem%20of%0Apolicy%20optimization%20to%20regressing%20the%20relative%20rewards%20via%20a%20direct%20policy%0Aparameterization%20between%20two%20completions%20to%20a%20prompt%2C%20enabling%20strikingly%0Alightweight%20implementation.%20In%20theory%2C%20we%20prove%20that%20fundamental%20RL%20algorithms%0Alike%20Natural%20Policy%20Gradient%20can%20be%20seen%20as%20variants%20of%20REBEL%2C%20which%20allows%20us%0Ato%20match%20the%20strongest%20known%20theoretical%20guarantees%20in%20terms%20of%20convergence%20and%0Asample%20complexity%20in%20the%20RL%20literature.%20REBEL%20can%20also%20cleanly%20incorporate%0Aoffline%20data%20and%20handle%20the%20intransitive%20preferences%20we%20frequently%20see%20in%0Apractice.%20Empirically%2C%20we%20find%20that%20REBEL%20provides%20a%20unified%20approach%20to%0Alanguage%20modeling%20and%20image%20generation%20with%20stronger%20or%20similar%20performance%20as%0APPO%20and%20DPO%2C%20all%20while%20being%20simpler%20to%20implement%20and%20more%20computationally%0Atractable%20than%20PPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16767v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REBEL%3A%20Reinforcement%20Learning%20via%20Regressing%20Relative%20Rewards&entry.906535625=Zhaolin%20Gao%20and%20Jonathan%20D.%20Chang%20and%20Wenhao%20Zhan%20and%20Owen%20Oertell%20and%20Gokul%20Swamy%20and%20Kiant%C3%A9%20Brantley%20and%20Thorsten%20Joachims%20and%20J.%20Andrew%20Bagnell%20and%20Jason%20D.%20Lee%20and%20Wen%20Sun&entry.1292438233=%20%20While%20originally%20developed%20for%20continuous%20control%20problems%2C%20Proximal%20Policy%0AOptimization%20%28PPO%29%20has%20emerged%20as%20the%20work-horse%20of%20a%20variety%20of%20reinforcement%0Alearning%20%28RL%29%20applications%20including%20the%20fine-tuning%20of%20generative%20models.%0AUnfortunately%2C%20PPO%20requires%20multiple%20heuristics%20to%20enable%20stable%20convergence%0A%28e.g.%20value%20networks%2C%20clipping%29%20and%20is%20notorious%20for%20its%20sensitivity%20to%20the%0Aprecise%20implementation%20of%20these%20components.%20In%20response%2C%20we%20take%20a%20step%20back%0Aand%20ask%20what%20a%20minimalist%20RL%20algorithm%20for%20the%20era%20of%20generative%20models%20would%0Alook%20like.%20We%20propose%20REBEL%2C%20an%20algorithm%20that%20cleanly%20reduces%20the%20problem%20of%0Apolicy%20optimization%20to%20regressing%20the%20relative%20rewards%20via%20a%20direct%20policy%0Aparameterization%20between%20two%20completions%20to%20a%20prompt%2C%20enabling%20strikingly%0Alightweight%20implementation.%20In%20theory%2C%20we%20prove%20that%20fundamental%20RL%20algorithms%0Alike%20Natural%20Policy%20Gradient%20can%20be%20seen%20as%20variants%20of%20REBEL%2C%20which%20allows%20us%0Ato%20match%20the%20strongest%20known%20theoretical%20guarantees%20in%20terms%20of%20convergence%20and%0Asample%20complexity%20in%20the%20RL%20literature.%20REBEL%20can%20also%20cleanly%20incorporate%0Aoffline%20data%20and%20handle%20the%20intransitive%20preferences%20we%20frequently%20see%20in%0Apractice.%20Empirically%2C%20we%20find%20that%20REBEL%20provides%20a%20unified%20approach%20to%0Alanguage%20modeling%20and%20image%20generation%20with%20stronger%20or%20similar%20performance%20as%0APPO%20and%20DPO%2C%20all%20while%20being%20simpler%20to%20implement%20and%20more%20computationally%0Atractable%20than%20PPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16767v1&entry.124074799=Read"},
{"title": "ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity\n  Preserving", "author": "Jiehui Huang and Xiao Dong and Wenhui Song and Hanhui Li and Jun Zhou and Yuhao Cheng and Shutao Liao and Long Chen and Yiqiang Yan and Shengcai Liao and Xiaodan Liang", "abstract": "  Diffusion-based technologies have made significant strides, particularly in\npersonalized and customized facialgeneration. However, existing methods face\nchallenges in achieving high-fidelity and detailed identity (ID)consistency,\nprimarily due to insufficient fine-grained control over facial areas and the\nlack of a comprehensive strategy for ID preservation by fully considering\nintricate facial details and the overall face. To address these limitations, we\nintroduce ConsistentID, an innovative method crafted for\ndiverseidentity-preserving portrait generation under fine-grained multimodal\nfacial prompts, utilizing only a single reference image. ConsistentID comprises\ntwo key components: a multimodal facial prompt generator that combines facial\nfeatures, corresponding facial descriptions and the overall facial context to\nenhance precision in facial details, and an ID-preservation network optimized\nthrough the facial attention localization strategy, aimed at preserving ID\nconsistency in facial regions. Together, these components significantly enhance\nthe accuracy of ID preservation by introducing fine-grained multimodal ID\ninformation from facial regions. To facilitate training of ConsistentID, we\npresent a fine-grained portrait dataset, FGID, with over 500,000 facial images,\noffering greater diversity and comprehensiveness than existing public facial\ndatasets. % such as LAION-Face, CelebA, FFHQ, and SFHQ. Experimental results\nsubstantiate that our ConsistentID achieves exceptional precision and diversity\nin personalized facial generation, surpassing existing methods in the MyStyle\ndataset. Furthermore, while ConsistentID introduces more multimodal ID\ninformation, it maintains a fast inference speed during generation.\n", "link": "http://arxiv.org/abs/2404.16771v1", "date": "2024-04-25", "relevancy": 1.8776, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6644}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5868}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5686}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ConsistentID%3A%20Portrait%20Generation%20with%20Multimodal%20Fine-Grained%20Identity%0A%20%20Preserving&body=Title%3A%20ConsistentID%3A%20Portrait%20Generation%20with%20Multimodal%20Fine-Grained%20Identity%0A%20%20Preserving%0AAuthor%3A%20Jiehui%20Huang%20and%20Xiao%20Dong%20and%20Wenhui%20Song%20and%20Hanhui%20Li%20and%20Jun%20Zhou%20and%20Yuhao%20Cheng%20and%20Shutao%20Liao%20and%20Long%20Chen%20and%20Yiqiang%20Yan%20and%20Shengcai%20Liao%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Diffusion-based%20technologies%20have%20made%20significant%20strides%2C%20particularly%20in%0Apersonalized%20and%20customized%20facialgeneration.%20However%2C%20existing%20methods%20face%0Achallenges%20in%20achieving%20high-fidelity%20and%20detailed%20identity%20%28ID%29consistency%2C%0Aprimarily%20due%20to%20insufficient%20fine-grained%20control%20over%20facial%20areas%20and%20the%0Alack%20of%20a%20comprehensive%20strategy%20for%20ID%20preservation%20by%20fully%20considering%0Aintricate%20facial%20details%20and%20the%20overall%20face.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20ConsistentID%2C%20an%20innovative%20method%20crafted%20for%0Adiverseidentity-preserving%20portrait%20generation%20under%20fine-grained%20multimodal%0Afacial%20prompts%2C%20utilizing%20only%20a%20single%20reference%20image.%20ConsistentID%20comprises%0Atwo%20key%20components%3A%20a%20multimodal%20facial%20prompt%20generator%20that%20combines%20facial%0Afeatures%2C%20corresponding%20facial%20descriptions%20and%20the%20overall%20facial%20context%20to%0Aenhance%20precision%20in%20facial%20details%2C%20and%20an%20ID-preservation%20network%20optimized%0Athrough%20the%20facial%20attention%20localization%20strategy%2C%20aimed%20at%20preserving%20ID%0Aconsistency%20in%20facial%20regions.%20Together%2C%20these%20components%20significantly%20enhance%0Athe%20accuracy%20of%20ID%20preservation%20by%20introducing%20fine-grained%20multimodal%20ID%0Ainformation%20from%20facial%20regions.%20To%20facilitate%20training%20of%20ConsistentID%2C%20we%0Apresent%20a%20fine-grained%20portrait%20dataset%2C%20FGID%2C%20with%20over%20500%2C000%20facial%20images%2C%0Aoffering%20greater%20diversity%20and%20comprehensiveness%20than%20existing%20public%20facial%0Adatasets.%20%25%20such%20as%20LAION-Face%2C%20CelebA%2C%20FFHQ%2C%20and%20SFHQ.%20Experimental%20results%0Asubstantiate%20that%20our%20ConsistentID%20achieves%20exceptional%20precision%20and%20diversity%0Ain%20personalized%20facial%20generation%2C%20surpassing%20existing%20methods%20in%20the%20MyStyle%0Adataset.%20Furthermore%2C%20while%20ConsistentID%20introduces%20more%20multimodal%20ID%0Ainformation%2C%20it%20maintains%20a%20fast%20inference%20speed%20during%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16771v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConsistentID%3A%20Portrait%20Generation%20with%20Multimodal%20Fine-Grained%20Identity%0A%20%20Preserving&entry.906535625=Jiehui%20Huang%20and%20Xiao%20Dong%20and%20Wenhui%20Song%20and%20Hanhui%20Li%20and%20Jun%20Zhou%20and%20Yuhao%20Cheng%20and%20Shutao%20Liao%20and%20Long%20Chen%20and%20Yiqiang%20Yan%20and%20Shengcai%20Liao%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Diffusion-based%20technologies%20have%20made%20significant%20strides%2C%20particularly%20in%0Apersonalized%20and%20customized%20facialgeneration.%20However%2C%20existing%20methods%20face%0Achallenges%20in%20achieving%20high-fidelity%20and%20detailed%20identity%20%28ID%29consistency%2C%0Aprimarily%20due%20to%20insufficient%20fine-grained%20control%20over%20facial%20areas%20and%20the%0Alack%20of%20a%20comprehensive%20strategy%20for%20ID%20preservation%20by%20fully%20considering%0Aintricate%20facial%20details%20and%20the%20overall%20face.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20ConsistentID%2C%20an%20innovative%20method%20crafted%20for%0Adiverseidentity-preserving%20portrait%20generation%20under%20fine-grained%20multimodal%0Afacial%20prompts%2C%20utilizing%20only%20a%20single%20reference%20image.%20ConsistentID%20comprises%0Atwo%20key%20components%3A%20a%20multimodal%20facial%20prompt%20generator%20that%20combines%20facial%0Afeatures%2C%20corresponding%20facial%20descriptions%20and%20the%20overall%20facial%20context%20to%0Aenhance%20precision%20in%20facial%20details%2C%20and%20an%20ID-preservation%20network%20optimized%0Athrough%20the%20facial%20attention%20localization%20strategy%2C%20aimed%20at%20preserving%20ID%0Aconsistency%20in%20facial%20regions.%20Together%2C%20these%20components%20significantly%20enhance%0Athe%20accuracy%20of%20ID%20preservation%20by%20introducing%20fine-grained%20multimodal%20ID%0Ainformation%20from%20facial%20regions.%20To%20facilitate%20training%20of%20ConsistentID%2C%20we%0Apresent%20a%20fine-grained%20portrait%20dataset%2C%20FGID%2C%20with%20over%20500%2C000%20facial%20images%2C%0Aoffering%20greater%20diversity%20and%20comprehensiveness%20than%20existing%20public%20facial%0Adatasets.%20%25%20such%20as%20LAION-Face%2C%20CelebA%2C%20FFHQ%2C%20and%20SFHQ.%20Experimental%20results%0Asubstantiate%20that%20our%20ConsistentID%20achieves%20exceptional%20precision%20and%20diversity%0Ain%20personalized%20facial%20generation%2C%20surpassing%20existing%20methods%20in%20the%20MyStyle%0Adataset.%20Furthermore%2C%20while%20ConsistentID%20introduces%20more%20multimodal%20ID%0Ainformation%2C%20it%20maintains%20a%20fast%20inference%20speed%20during%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16771v1&entry.124074799=Read"},
{"title": "Conditional Distribution Modelling for Few-Shot Image Synthesis with\n  Diffusion Models", "author": "Parul Gupta and Munawar Hayat and Abhinav Dhall and Thanh-Toan Do", "abstract": "  Few-shot image synthesis entails generating diverse and realistic images of\nnovel categories using only a few example images. While multiple recent efforts\nin this direction have achieved impressive results, the existing approaches are\ndependent only upon the few novel samples available at test time in order to\ngenerate new images, which restricts the diversity of the generated images. To\novercome this limitation, we propose Conditional Distribution Modelling (CDM)\n-- a framework which effectively utilizes Diffusion models for few-shot image\ngeneration. By modelling the distribution of the latent space used to condition\na Diffusion process, CDM leverages the learnt statistics of the training data\nto get a better approximation of the unseen class distribution, thereby\nremoving the bias arising due to limited number of few shot samples.\nSimultaneously, we devise a novel inversion based optimization strategy that\nfurther improves the approximated unseen class distribution, and ensures the\nfidelity of the generated samples to the unseen class. The experimental results\non four benchmark datasets demonstrate the effectiveness of our proposed CDM\nfor few-shot generation.\n", "link": "http://arxiv.org/abs/2404.16556v1", "date": "2024-04-25", "relevancy": 1.8546, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6652}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6255}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5965}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Conditional%20Distribution%20Modelling%20for%20Few-Shot%20Image%20Synthesis%20with%0A%20%20Diffusion%20Models&body=Title%3A%20Conditional%20Distribution%20Modelling%20for%20Few-Shot%20Image%20Synthesis%20with%0A%20%20Diffusion%20Models%0AAuthor%3A%20Parul%20Gupta%20and%20Munawar%20Hayat%20and%20Abhinav%20Dhall%20and%20Thanh-Toan%20Do%0AAbstract%3A%20%20%20Few-shot%20image%20synthesis%20entails%20generating%20diverse%20and%20realistic%20images%20of%0Anovel%20categories%20using%20only%20a%20few%20example%20images.%20While%20multiple%20recent%20efforts%0Ain%20this%20direction%20have%20achieved%20impressive%20results%2C%20the%20existing%20approaches%20are%0Adependent%20only%20upon%20the%20few%20novel%20samples%20available%20at%20test%20time%20in%20order%20to%0Agenerate%20new%20images%2C%20which%20restricts%20the%20diversity%20of%20the%20generated%20images.%20To%0Aovercome%20this%20limitation%2C%20we%20propose%20Conditional%20Distribution%20Modelling%20%28CDM%29%0A--%20a%20framework%20which%20effectively%20utilizes%20Diffusion%20models%20for%20few-shot%20image%0Ageneration.%20By%20modelling%20the%20distribution%20of%20the%20latent%20space%20used%20to%20condition%0Aa%20Diffusion%20process%2C%20CDM%20leverages%20the%20learnt%20statistics%20of%20the%20training%20data%0Ato%20get%20a%20better%20approximation%20of%20the%20unseen%20class%20distribution%2C%20thereby%0Aremoving%20the%20bias%20arising%20due%20to%20limited%20number%20of%20few%20shot%20samples.%0ASimultaneously%2C%20we%20devise%20a%20novel%20inversion%20based%20optimization%20strategy%20that%0Afurther%20improves%20the%20approximated%20unseen%20class%20distribution%2C%20and%20ensures%20the%0Afidelity%20of%20the%20generated%20samples%20to%20the%20unseen%20class.%20The%20experimental%20results%0Aon%20four%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20CDM%0Afor%20few-shot%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16556v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Distribution%20Modelling%20for%20Few-Shot%20Image%20Synthesis%20with%0A%20%20Diffusion%20Models&entry.906535625=Parul%20Gupta%20and%20Munawar%20Hayat%20and%20Abhinav%20Dhall%20and%20Thanh-Toan%20Do&entry.1292438233=%20%20Few-shot%20image%20synthesis%20entails%20generating%20diverse%20and%20realistic%20images%20of%0Anovel%20categories%20using%20only%20a%20few%20example%20images.%20While%20multiple%20recent%20efforts%0Ain%20this%20direction%20have%20achieved%20impressive%20results%2C%20the%20existing%20approaches%20are%0Adependent%20only%20upon%20the%20few%20novel%20samples%20available%20at%20test%20time%20in%20order%20to%0Agenerate%20new%20images%2C%20which%20restricts%20the%20diversity%20of%20the%20generated%20images.%20To%0Aovercome%20this%20limitation%2C%20we%20propose%20Conditional%20Distribution%20Modelling%20%28CDM%29%0A--%20a%20framework%20which%20effectively%20utilizes%20Diffusion%20models%20for%20few-shot%20image%0Ageneration.%20By%20modelling%20the%20distribution%20of%20the%20latent%20space%20used%20to%20condition%0Aa%20Diffusion%20process%2C%20CDM%20leverages%20the%20learnt%20statistics%20of%20the%20training%20data%0Ato%20get%20a%20better%20approximation%20of%20the%20unseen%20class%20distribution%2C%20thereby%0Aremoving%20the%20bias%20arising%20due%20to%20limited%20number%20of%20few%20shot%20samples.%0ASimultaneously%2C%20we%20devise%20a%20novel%20inversion%20based%20optimization%20strategy%20that%0Afurther%20improves%20the%20approximated%20unseen%20class%20distribution%2C%20and%20ensures%20the%0Afidelity%20of%20the%20generated%20samples%20to%20the%20unseen%20class.%20The%20experimental%20results%0Aon%20four%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20CDM%0Afor%20few-shot%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16556v1&entry.124074799=Read"},
{"title": "Robust Capped lp-Norm Support Vector Ordinal Regression", "author": "Haorui Xiang and Zhichang Wu and Guoxu Li and Rong Wang and Feiping Nie and Xuelong Li", "abstract": "  Ordinal regression is a specialized supervised problem where the labels show\nan inherent order. The order distinguishes it from normal multi-class problem.\nSupport Vector Ordinal Regression, as an outstanding ordinal regression model,\nis widely used in many ordinal regression tasks. However, like most supervised\nlearning algorithms, the design of SVOR is based on the assumption that the\ntraining data are real and reliable, which is difficult to satisfy in\nreal-world data. In many practical applications, outliers are frequently\npresent in the training set, potentially leading to misguide the learning\nprocess, such that the performance is non-optimal. In this paper, we propose a\nnovel capped $\\ell_{p}$-norm loss function that is theoretically robust to both\nlight and heavy outliers. The capped $\\ell_{p}$-norm loss can help the model\ndetect and eliminate outliers during training process. Adhering to this\nconcept, we introduce a new model, Capped $\\ell_{p}$-Norm Support Vector\nOrdinal Regression(CSVOR), that is robust to outliers. CSVOR uses a weight\nmatrix to detect and eliminate outliers during the training process to improve\nthe robustness to outliers. Moreover, a Re-Weighted algorithm algorithm which\nis illustrated convergence by our theoretical results is proposed to\neffectively minimize the corresponding problem. Extensive experimental results\ndemonstrate that our model outperforms state-of-the-art(SOTA) methods,\nparticularly in the presence of outliers.\n", "link": "http://arxiv.org/abs/2404.16616v1", "date": "2024-04-25", "relevancy": 1.8442, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4785}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4481}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Capped%20lp-Norm%20Support%20Vector%20Ordinal%20Regression&body=Title%3A%20Robust%20Capped%20lp-Norm%20Support%20Vector%20Ordinal%20Regression%0AAuthor%3A%20Haorui%20Xiang%20and%20Zhichang%20Wu%20and%20Guoxu%20Li%20and%20Rong%20Wang%20and%20Feiping%20Nie%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Ordinal%20regression%20is%20a%20specialized%20supervised%20problem%20where%20the%20labels%20show%0Aan%20inherent%20order.%20The%20order%20distinguishes%20it%20from%20normal%20multi-class%20problem.%0ASupport%20Vector%20Ordinal%20Regression%2C%20as%20an%20outstanding%20ordinal%20regression%20model%2C%0Ais%20widely%20used%20in%20many%20ordinal%20regression%20tasks.%20However%2C%20like%20most%20supervised%0Alearning%20algorithms%2C%20the%20design%20of%20SVOR%20is%20based%20on%20the%20assumption%20that%20the%0Atraining%20data%20are%20real%20and%20reliable%2C%20which%20is%20difficult%20to%20satisfy%20in%0Areal-world%20data.%20In%20many%20practical%20applications%2C%20outliers%20are%20frequently%0Apresent%20in%20the%20training%20set%2C%20potentially%20leading%20to%20misguide%20the%20learning%0Aprocess%2C%20such%20that%20the%20performance%20is%20non-optimal.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20capped%20%24%5Cell_%7Bp%7D%24-norm%20loss%20function%20that%20is%20theoretically%20robust%20to%20both%0Alight%20and%20heavy%20outliers.%20The%20capped%20%24%5Cell_%7Bp%7D%24-norm%20loss%20can%20help%20the%20model%0Adetect%20and%20eliminate%20outliers%20during%20training%20process.%20Adhering%20to%20this%0Aconcept%2C%20we%20introduce%20a%20new%20model%2C%20Capped%20%24%5Cell_%7Bp%7D%24-Norm%20Support%20Vector%0AOrdinal%20Regression%28CSVOR%29%2C%20that%20is%20robust%20to%20outliers.%20CSVOR%20uses%20a%20weight%0Amatrix%20to%20detect%20and%20eliminate%20outliers%20during%20the%20training%20process%20to%20improve%0Athe%20robustness%20to%20outliers.%20Moreover%2C%20a%20Re-Weighted%20algorithm%20algorithm%20which%0Ais%20illustrated%20convergence%20by%20our%20theoretical%20results%20is%20proposed%20to%0Aeffectively%20minimize%20the%20corresponding%20problem.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20model%20outperforms%20state-of-the-art%28SOTA%29%20methods%2C%0Aparticularly%20in%20the%20presence%20of%20outliers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16616v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Capped%20lp-Norm%20Support%20Vector%20Ordinal%20Regression&entry.906535625=Haorui%20Xiang%20and%20Zhichang%20Wu%20and%20Guoxu%20Li%20and%20Rong%20Wang%20and%20Feiping%20Nie%20and%20Xuelong%20Li&entry.1292438233=%20%20Ordinal%20regression%20is%20a%20specialized%20supervised%20problem%20where%20the%20labels%20show%0Aan%20inherent%20order.%20The%20order%20distinguishes%20it%20from%20normal%20multi-class%20problem.%0ASupport%20Vector%20Ordinal%20Regression%2C%20as%20an%20outstanding%20ordinal%20regression%20model%2C%0Ais%20widely%20used%20in%20many%20ordinal%20regression%20tasks.%20However%2C%20like%20most%20supervised%0Alearning%20algorithms%2C%20the%20design%20of%20SVOR%20is%20based%20on%20the%20assumption%20that%20the%0Atraining%20data%20are%20real%20and%20reliable%2C%20which%20is%20difficult%20to%20satisfy%20in%0Areal-world%20data.%20In%20many%20practical%20applications%2C%20outliers%20are%20frequently%0Apresent%20in%20the%20training%20set%2C%20potentially%20leading%20to%20misguide%20the%20learning%0Aprocess%2C%20such%20that%20the%20performance%20is%20non-optimal.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20capped%20%24%5Cell_%7Bp%7D%24-norm%20loss%20function%20that%20is%20theoretically%20robust%20to%20both%0Alight%20and%20heavy%20outliers.%20The%20capped%20%24%5Cell_%7Bp%7D%24-norm%20loss%20can%20help%20the%20model%0Adetect%20and%20eliminate%20outliers%20during%20training%20process.%20Adhering%20to%20this%0Aconcept%2C%20we%20introduce%20a%20new%20model%2C%20Capped%20%24%5Cell_%7Bp%7D%24-Norm%20Support%20Vector%0AOrdinal%20Regression%28CSVOR%29%2C%20that%20is%20robust%20to%20outliers.%20CSVOR%20uses%20a%20weight%0Amatrix%20to%20detect%20and%20eliminate%20outliers%20during%20the%20training%20process%20to%20improve%0Athe%20robustness%20to%20outliers.%20Moreover%2C%20a%20Re-Weighted%20algorithm%20algorithm%20which%0Ais%20illustrated%20convergence%20by%20our%20theoretical%20results%20is%20proposed%20to%0Aeffectively%20minimize%20the%20corresponding%20problem.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20model%20outperforms%20state-of-the-art%28SOTA%29%20methods%2C%0Aparticularly%20in%20the%20presence%20of%20outliers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16616v1&entry.124074799=Read"},
{"title": "Digital Over-the-Air Federated Learning in Multi-Antenna Systems", "author": "Sihua Wang and Mingzhe Chen and Cong Shen and Changchuan Yin and Christopher G. Brinton", "abstract": "  In this paper, the performance optimization of federated learning (FL), when\ndeployed over a realistic wireless multiple-input multiple-output (MIMO)\ncommunication system with digital modulation and over-the-air computation\n(AirComp) is studied. In particular, a MIMO system is considered in which edge\ndevices transmit their local FL models (trained using their locally collected\ndata) to a parameter server (PS) using beamforming to maximize the number of\ndevices scheduled for transmission. The PS, acting as a central controller,\ngenerates a global FL model using the received local FL models and broadcasts\nit back to all devices. Due to the limited bandwidth in a wireless network,\nAirComp is adopted to enable efficient wireless data aggregation. However,\nfading of wireless channels can produce aggregate distortions in an\nAirComp-based FL scheme. To tackle this challenge, we propose a modified\nfederated averaging (FedAvg) algorithm that combines digital modulation with\nAirComp to mitigate wireless fading while ensuring the communication\nefficiency. This is achieved by a joint transmit and receive beamforming\ndesign, which is formulated as an optimization problem to dynamically adjust\nthe beamforming matrices based on current FL model parameters so as to minimize\nthe transmitting error and ensure the FL performance. To achieve this goal, we\nfirst analytically characterize how the beamforming matrices affect the\nperformance of the FedAvg in different iterations. Based on this relationship,\nan artificial neural network (ANN) is used to estimate the local FL models of\nall devices and adjust the beamforming matrices at the PS for future model\ntransmission. The algorithmic advantages and improved performance of the\nproposed methodologies are demonstrated through extensive numerical\nexperiments.\n", "link": "http://arxiv.org/abs/2302.14648v3", "date": "2024-04-25", "relevancy": 1.8422, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4776}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4482}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Digital%20Over-the-Air%20Federated%20Learning%20in%20Multi-Antenna%20Systems&body=Title%3A%20Digital%20Over-the-Air%20Federated%20Learning%20in%20Multi-Antenna%20Systems%0AAuthor%3A%20Sihua%20Wang%20and%20Mingzhe%20Chen%20and%20Cong%20Shen%20and%20Changchuan%20Yin%20and%20Christopher%20G.%20Brinton%0AAbstract%3A%20%20%20In%20this%20paper%2C%20the%20performance%20optimization%20of%20federated%20learning%20%28FL%29%2C%20when%0Adeployed%20over%20a%20realistic%20wireless%20multiple-input%20multiple-output%20%28MIMO%29%0Acommunication%20system%20with%20digital%20modulation%20and%20over-the-air%20computation%0A%28AirComp%29%20is%20studied.%20In%20particular%2C%20a%20MIMO%20system%20is%20considered%20in%20which%20edge%0Adevices%20transmit%20their%20local%20FL%20models%20%28trained%20using%20their%20locally%20collected%0Adata%29%20to%20a%20parameter%20server%20%28PS%29%20using%20beamforming%20to%20maximize%20the%20number%20of%0Adevices%20scheduled%20for%20transmission.%20The%20PS%2C%20acting%20as%20a%20central%20controller%2C%0Agenerates%20a%20global%20FL%20model%20using%20the%20received%20local%20FL%20models%20and%20broadcasts%0Ait%20back%20to%20all%20devices.%20Due%20to%20the%20limited%20bandwidth%20in%20a%20wireless%20network%2C%0AAirComp%20is%20adopted%20to%20enable%20efficient%20wireless%20data%20aggregation.%20However%2C%0Afading%20of%20wireless%20channels%20can%20produce%20aggregate%20distortions%20in%20an%0AAirComp-based%20FL%20scheme.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20modified%0Afederated%20averaging%20%28FedAvg%29%20algorithm%20that%20combines%20digital%20modulation%20with%0AAirComp%20to%20mitigate%20wireless%20fading%20while%20ensuring%20the%20communication%0Aefficiency.%20This%20is%20achieved%20by%20a%20joint%20transmit%20and%20receive%20beamforming%0Adesign%2C%20which%20is%20formulated%20as%20an%20optimization%20problem%20to%20dynamically%20adjust%0Athe%20beamforming%20matrices%20based%20on%20current%20FL%20model%20parameters%20so%20as%20to%20minimize%0Athe%20transmitting%20error%20and%20ensure%20the%20FL%20performance.%20To%20achieve%20this%20goal%2C%20we%0Afirst%20analytically%20characterize%20how%20the%20beamforming%20matrices%20affect%20the%0Aperformance%20of%20the%20FedAvg%20in%20different%20iterations.%20Based%20on%20this%20relationship%2C%0Aan%20artificial%20neural%20network%20%28ANN%29%20is%20used%20to%20estimate%20the%20local%20FL%20models%20of%0Aall%20devices%20and%20adjust%20the%20beamforming%20matrices%20at%20the%20PS%20for%20future%20model%0Atransmission.%20The%20algorithmic%20advantages%20and%20improved%20performance%20of%20the%0Aproposed%20methodologies%20are%20demonstrated%20through%20extensive%20numerical%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.14648v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Over-the-Air%20Federated%20Learning%20in%20Multi-Antenna%20Systems&entry.906535625=Sihua%20Wang%20and%20Mingzhe%20Chen%20and%20Cong%20Shen%20and%20Changchuan%20Yin%20and%20Christopher%20G.%20Brinton&entry.1292438233=%20%20In%20this%20paper%2C%20the%20performance%20optimization%20of%20federated%20learning%20%28FL%29%2C%20when%0Adeployed%20over%20a%20realistic%20wireless%20multiple-input%20multiple-output%20%28MIMO%29%0Acommunication%20system%20with%20digital%20modulation%20and%20over-the-air%20computation%0A%28AirComp%29%20is%20studied.%20In%20particular%2C%20a%20MIMO%20system%20is%20considered%20in%20which%20edge%0Adevices%20transmit%20their%20local%20FL%20models%20%28trained%20using%20their%20locally%20collected%0Adata%29%20to%20a%20parameter%20server%20%28PS%29%20using%20beamforming%20to%20maximize%20the%20number%20of%0Adevices%20scheduled%20for%20transmission.%20The%20PS%2C%20acting%20as%20a%20central%20controller%2C%0Agenerates%20a%20global%20FL%20model%20using%20the%20received%20local%20FL%20models%20and%20broadcasts%0Ait%20back%20to%20all%20devices.%20Due%20to%20the%20limited%20bandwidth%20in%20a%20wireless%20network%2C%0AAirComp%20is%20adopted%20to%20enable%20efficient%20wireless%20data%20aggregation.%20However%2C%0Afading%20of%20wireless%20channels%20can%20produce%20aggregate%20distortions%20in%20an%0AAirComp-based%20FL%20scheme.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20modified%0Afederated%20averaging%20%28FedAvg%29%20algorithm%20that%20combines%20digital%20modulation%20with%0AAirComp%20to%20mitigate%20wireless%20fading%20while%20ensuring%20the%20communication%0Aefficiency.%20This%20is%20achieved%20by%20a%20joint%20transmit%20and%20receive%20beamforming%0Adesign%2C%20which%20is%20formulated%20as%20an%20optimization%20problem%20to%20dynamically%20adjust%0Athe%20beamforming%20matrices%20based%20on%20current%20FL%20model%20parameters%20so%20as%20to%20minimize%0Athe%20transmitting%20error%20and%20ensure%20the%20FL%20performance.%20To%20achieve%20this%20goal%2C%20we%0Afirst%20analytically%20characterize%20how%20the%20beamforming%20matrices%20affect%20the%0Aperformance%20of%20the%20FedAvg%20in%20different%20iterations.%20Based%20on%20this%20relationship%2C%0Aan%20artificial%20neural%20network%20%28ANN%29%20is%20used%20to%20estimate%20the%20local%20FL%20models%20of%0Aall%20devices%20and%20adjust%20the%20beamforming%20matrices%20at%20the%20PS%20for%20future%20model%0Atransmission.%20The%20algorithmic%20advantages%20and%20improved%20performance%20of%20the%0Aproposed%20methodologies%20are%20demonstrated%20through%20extensive%20numerical%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.14648v3&entry.124074799=Read"},
{"title": "Hippocrates: An Open-Source Framework for Advancing Large Language\n  Models in Healthcare", "author": "Emre Can Acikgoz and Osman Batur \u0130nce and Rayene Bench and Arda An\u0131l Boz and \u0130lker Kesen and Aykut Erdem and Erkut Erdem", "abstract": "  The integration of Large Language Models (LLMs) into healthcare promises to\ntransform medical diagnostics, research, and patient care. Yet, the progression\nof medical LLMs faces obstacles such as complex training requirements, rigorous\nevaluation demands, and the dominance of proprietary models that restrict\nacademic exploration. Transparent, comprehensive access to LLM resources is\nessential for advancing the field, fostering reproducibility, and encouraging\ninnovation in healthcare AI. We present Hippocrates, an open-source LLM\nframework specifically developed for the medical domain. In stark contrast to\nprevious efforts, it offers unrestricted access to its training datasets,\ncodebase, checkpoints, and evaluation protocols. This open approach is designed\nto stimulate collaborative research, allowing the community to build upon,\nrefine, and rigorously evaluate medical LLMs within a transparent ecosystem.\nAlso, we introduce Hippo, a family of 7B models tailored for the medical\ndomain, fine-tuned from Mistral and LLaMA2 through continual pre-training,\ninstruction tuning, and reinforcement learning from human and AI feedback. Our\nmodels outperform existing open medical LLMs models by a large-margin, even\nsurpassing models with 70B parameters. Through Hippocrates, we aspire to unlock\nthe full potential of LLMs not just to advance medical knowledge and patient\ncare but also to democratize the benefits of AI research in healthcare, making\nthem available across the globe.\n", "link": "http://arxiv.org/abs/2404.16621v1", "date": "2024-04-25", "relevancy": 1.8391, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4401}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hippocrates%3A%20An%20Open-Source%20Framework%20for%20Advancing%20Large%20Language%0A%20%20Models%20in%20Healthcare&body=Title%3A%20Hippocrates%3A%20An%20Open-Source%20Framework%20for%20Advancing%20Large%20Language%0A%20%20Models%20in%20Healthcare%0AAuthor%3A%20Emre%20Can%20Acikgoz%20and%20Osman%20Batur%20%C4%B0nce%20and%20Rayene%20Bench%20and%20Arda%20An%C4%B1l%20Boz%20and%20%C4%B0lker%20Kesen%20and%20Aykut%20Erdem%20and%20Erkut%20Erdem%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20healthcare%20promises%20to%0Atransform%20medical%20diagnostics%2C%20research%2C%20and%20patient%20care.%20Yet%2C%20the%20progression%0Aof%20medical%20LLMs%20faces%20obstacles%20such%20as%20complex%20training%20requirements%2C%20rigorous%0Aevaluation%20demands%2C%20and%20the%20dominance%20of%20proprietary%20models%20that%20restrict%0Aacademic%20exploration.%20Transparent%2C%20comprehensive%20access%20to%20LLM%20resources%20is%0Aessential%20for%20advancing%20the%20field%2C%20fostering%20reproducibility%2C%20and%20encouraging%0Ainnovation%20in%20healthcare%20AI.%20We%20present%20Hippocrates%2C%20an%20open-source%20LLM%0Aframework%20specifically%20developed%20for%20the%20medical%20domain.%20In%20stark%20contrast%20to%0Aprevious%20efforts%2C%20it%20offers%20unrestricted%20access%20to%20its%20training%20datasets%2C%0Acodebase%2C%20checkpoints%2C%20and%20evaluation%20protocols.%20This%20open%20approach%20is%20designed%0Ato%20stimulate%20collaborative%20research%2C%20allowing%20the%20community%20to%20build%20upon%2C%0Arefine%2C%20and%20rigorously%20evaluate%20medical%20LLMs%20within%20a%20transparent%20ecosystem.%0AAlso%2C%20we%20introduce%20Hippo%2C%20a%20family%20of%207B%20models%20tailored%20for%20the%20medical%0Adomain%2C%20fine-tuned%20from%20Mistral%20and%20LLaMA2%20through%20continual%20pre-training%2C%0Ainstruction%20tuning%2C%20and%20reinforcement%20learning%20from%20human%20and%20AI%20feedback.%20Our%0Amodels%20outperform%20existing%20open%20medical%20LLMs%20models%20by%20a%20large-margin%2C%20even%0Asurpassing%20models%20with%2070B%20parameters.%20Through%20Hippocrates%2C%20we%20aspire%20to%20unlock%0Athe%20full%20potential%20of%20LLMs%20not%20just%20to%20advance%20medical%20knowledge%20and%20patient%0Acare%20but%20also%20to%20democratize%20the%20benefits%20of%20AI%20research%20in%20healthcare%2C%20making%0Athem%20available%20across%20the%20globe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16621v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hippocrates%3A%20An%20Open-Source%20Framework%20for%20Advancing%20Large%20Language%0A%20%20Models%20in%20Healthcare&entry.906535625=Emre%20Can%20Acikgoz%20and%20Osman%20Batur%20%C4%B0nce%20and%20Rayene%20Bench%20and%20Arda%20An%C4%B1l%20Boz%20and%20%C4%B0lker%20Kesen%20and%20Aykut%20Erdem%20and%20Erkut%20Erdem&entry.1292438233=%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20healthcare%20promises%20to%0Atransform%20medical%20diagnostics%2C%20research%2C%20and%20patient%20care.%20Yet%2C%20the%20progression%0Aof%20medical%20LLMs%20faces%20obstacles%20such%20as%20complex%20training%20requirements%2C%20rigorous%0Aevaluation%20demands%2C%20and%20the%20dominance%20of%20proprietary%20models%20that%20restrict%0Aacademic%20exploration.%20Transparent%2C%20comprehensive%20access%20to%20LLM%20resources%20is%0Aessential%20for%20advancing%20the%20field%2C%20fostering%20reproducibility%2C%20and%20encouraging%0Ainnovation%20in%20healthcare%20AI.%20We%20present%20Hippocrates%2C%20an%20open-source%20LLM%0Aframework%20specifically%20developed%20for%20the%20medical%20domain.%20In%20stark%20contrast%20to%0Aprevious%20efforts%2C%20it%20offers%20unrestricted%20access%20to%20its%20training%20datasets%2C%0Acodebase%2C%20checkpoints%2C%20and%20evaluation%20protocols.%20This%20open%20approach%20is%20designed%0Ato%20stimulate%20collaborative%20research%2C%20allowing%20the%20community%20to%20build%20upon%2C%0Arefine%2C%20and%20rigorously%20evaluate%20medical%20LLMs%20within%20a%20transparent%20ecosystem.%0AAlso%2C%20we%20introduce%20Hippo%2C%20a%20family%20of%207B%20models%20tailored%20for%20the%20medical%0Adomain%2C%20fine-tuned%20from%20Mistral%20and%20LLaMA2%20through%20continual%20pre-training%2C%0Ainstruction%20tuning%2C%20and%20reinforcement%20learning%20from%20human%20and%20AI%20feedback.%20Our%0Amodels%20outperform%20existing%20open%20medical%20LLMs%20models%20by%20a%20large-margin%2C%20even%0Asurpassing%20models%20with%2070B%20parameters.%20Through%20Hippocrates%2C%20we%20aspire%20to%20unlock%0Athe%20full%20potential%20of%20LLMs%20not%20just%20to%20advance%20medical%20knowledge%20and%20patient%0Acare%20but%20also%20to%20democratize%20the%20benefits%20of%20AI%20research%20in%20healthcare%2C%20making%0Athem%20available%20across%20the%20globe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16621v1&entry.124074799=Read"},
{"title": "Efficient algorithms for regularized Poisson Non-negative Matrix\n  Factorization", "author": "Nathana\u00ebl Perraudin and Adrien Teutrie and C\u00e9cile H\u00e9bert and Guillaume Obozinski", "abstract": "  We consider the problem of regularized Poisson Non-negative Matrix\nFactorization (NMF) problem, encompassing various regularization terms such as\nLipschitz and relatively smooth functions, alongside linear constraints. This\nproblem holds significant relevance in numerous Machine Learning applications,\nparticularly within the domain of physical linear unmixing problems. A notable\nchallenge arises from the main loss term in the Poisson NMF problem being a KL\ndivergence, which is non-Lipschitz, rendering traditional gradient\ndescent-based approaches inefficient. In this contribution, we explore the\nutilization of Block Successive Upper Minimization (BSUM) to overcome this\nchallenge. We build approriate majorizing function for Lipschitz and relatively\nsmooth functions, and show how to introduce linear constraints into the\nproblem. This results in the development of two novel algorithms for\nregularized Poisson NMF. We conduct numerical simulations to showcase the\neffectiveness of our approach.\n", "link": "http://arxiv.org/abs/2404.16505v1", "date": "2024-04-25", "relevancy": 1.8305, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4581}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4576}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4565}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20algorithms%20for%20regularized%20Poisson%20Non-negative%20Matrix%0A%20%20Factorization&body=Title%3A%20Efficient%20algorithms%20for%20regularized%20Poisson%20Non-negative%20Matrix%0A%20%20Factorization%0AAuthor%3A%20Nathana%C3%ABl%20Perraudin%20and%20Adrien%20Teutrie%20and%20C%C3%A9cile%20H%C3%A9bert%20and%20Guillaume%20Obozinski%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20regularized%20Poisson%20Non-negative%20Matrix%0AFactorization%20%28NMF%29%20problem%2C%20encompassing%20various%20regularization%20terms%20such%20as%0ALipschitz%20and%20relatively%20smooth%20functions%2C%20alongside%20linear%20constraints.%20This%0Aproblem%20holds%20significant%20relevance%20in%20numerous%20Machine%20Learning%20applications%2C%0Aparticularly%20within%20the%20domain%20of%20physical%20linear%20unmixing%20problems.%20A%20notable%0Achallenge%20arises%20from%20the%20main%20loss%20term%20in%20the%20Poisson%20NMF%20problem%20being%20a%20KL%0Adivergence%2C%20which%20is%20non-Lipschitz%2C%20rendering%20traditional%20gradient%0Adescent-based%20approaches%20inefficient.%20In%20this%20contribution%2C%20we%20explore%20the%0Autilization%20of%20Block%20Successive%20Upper%20Minimization%20%28BSUM%29%20to%20overcome%20this%0Achallenge.%20We%20build%20approriate%20majorizing%20function%20for%20Lipschitz%20and%20relatively%0Asmooth%20functions%2C%20and%20show%20how%20to%20introduce%20linear%20constraints%20into%20the%0Aproblem.%20This%20results%20in%20the%20development%20of%20two%20novel%20algorithms%20for%0Aregularized%20Poisson%20NMF.%20We%20conduct%20numerical%20simulations%20to%20showcase%20the%0Aeffectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16505v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20algorithms%20for%20regularized%20Poisson%20Non-negative%20Matrix%0A%20%20Factorization&entry.906535625=Nathana%C3%ABl%20Perraudin%20and%20Adrien%20Teutrie%20and%20C%C3%A9cile%20H%C3%A9bert%20and%20Guillaume%20Obozinski&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20regularized%20Poisson%20Non-negative%20Matrix%0AFactorization%20%28NMF%29%20problem%2C%20encompassing%20various%20regularization%20terms%20such%20as%0ALipschitz%20and%20relatively%20smooth%20functions%2C%20alongside%20linear%20constraints.%20This%0Aproblem%20holds%20significant%20relevance%20in%20numerous%20Machine%20Learning%20applications%2C%0Aparticularly%20within%20the%20domain%20of%20physical%20linear%20unmixing%20problems.%20A%20notable%0Achallenge%20arises%20from%20the%20main%20loss%20term%20in%20the%20Poisson%20NMF%20problem%20being%20a%20KL%0Adivergence%2C%20which%20is%20non-Lipschitz%2C%20rendering%20traditional%20gradient%0Adescent-based%20approaches%20inefficient.%20In%20this%20contribution%2C%20we%20explore%20the%0Autilization%20of%20Block%20Successive%20Upper%20Minimization%20%28BSUM%29%20to%20overcome%20this%0Achallenge.%20We%20build%20approriate%20majorizing%20function%20for%20Lipschitz%20and%20relatively%0Asmooth%20functions%2C%20and%20show%20how%20to%20introduce%20linear%20constraints%20into%20the%0Aproblem.%20This%20results%20in%20the%20development%20of%20two%20novel%20algorithms%20for%0Aregularized%20Poisson%20NMF.%20We%20conduct%20numerical%20simulations%20to%20showcase%20the%0Aeffectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16505v1&entry.124074799=Read"},
{"title": "GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting\n  Editing", "author": "Jing Wu and Jia-Wang Bian and Xinghui Li and Guangrun Wang and Ian Reid and Philip Torr and Victor Adrian Prisacariu", "abstract": "  We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed\nby the 3D Gaussian Splatting (3DGS).\n  Our method first renders a collection of images by using the 3DGS and edits\nthem by using a pre-trained 2D diffusion model (ControlNet) based on the input\nprompt, which is then used to optimise the 3D model.\n  Our key contribution is multi-view consistent editing, which enables editing\nall images together instead of iteratively editing one image while updating the\n3D model as in previous works.\n  It leads to faster editing as well as higher visual quality.\n  This is achieved by the two terms:\n  (a) depth-conditioned editing that enforces geometric consistency across\nmulti-view images by leveraging naturally consistent depth maps.\n  (b) attention-based latent code alignment that unifies the appearance of\nedited images by conditioning their editing to several reference views through\nself and cross-view attention between images' latent representations.\n  Experiments demonstrate that our method achieves faster editing and better\nvisual results than previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.08733v3", "date": "2024-04-25", "relevancy": 1.8219, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6693}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5418}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5178}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GaussCtrl%3A%20Multi-View%20Consistent%20Text-Driven%203D%20Gaussian%20Splatting%0A%20%20Editing&body=Title%3A%20GaussCtrl%3A%20Multi-View%20Consistent%20Text-Driven%203D%20Gaussian%20Splatting%0A%20%20Editing%0AAuthor%3A%20Jing%20Wu%20and%20Jia-Wang%20Bian%20and%20Xinghui%20Li%20and%20Guangrun%20Wang%20and%20Ian%20Reid%20and%20Philip%20Torr%20and%20Victor%20Adrian%20Prisacariu%0AAbstract%3A%20%20%20We%20propose%20GaussCtrl%2C%20a%20text-driven%20method%20to%20edit%20a%203D%20scene%20reconstructed%0Aby%20the%203D%20Gaussian%20Splatting%20%283DGS%29.%0A%20%20Our%20method%20first%20renders%20a%20collection%20of%20images%20by%20using%20the%203DGS%20and%20edits%0Athem%20by%20using%20a%20pre-trained%202D%20diffusion%20model%20%28ControlNet%29%20based%20on%20the%20input%0Aprompt%2C%20which%20is%20then%20used%20to%20optimise%20the%203D%20model.%0A%20%20Our%20key%20contribution%20is%20multi-view%20consistent%20editing%2C%20which%20enables%20editing%0Aall%20images%20together%20instead%20of%20iteratively%20editing%20one%20image%20while%20updating%20the%0A3D%20model%20as%20in%20previous%20works.%0A%20%20It%20leads%20to%20faster%20editing%20as%20well%20as%20higher%20visual%20quality.%0A%20%20This%20is%20achieved%20by%20the%20two%20terms%3A%0A%20%20%28a%29%20depth-conditioned%20editing%20that%20enforces%20geometric%20consistency%20across%0Amulti-view%20images%20by%20leveraging%20naturally%20consistent%20depth%20maps.%0A%20%20%28b%29%20attention-based%20latent%20code%20alignment%20that%20unifies%20the%20appearance%20of%0Aedited%20images%20by%20conditioning%20their%20editing%20to%20several%20reference%20views%20through%0Aself%20and%20cross-view%20attention%20between%20images%27%20latent%20representations.%0A%20%20Experiments%20demonstrate%20that%20our%20method%20achieves%20faster%20editing%20and%20better%0Avisual%20results%20than%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08733v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussCtrl%3A%20Multi-View%20Consistent%20Text-Driven%203D%20Gaussian%20Splatting%0A%20%20Editing&entry.906535625=Jing%20Wu%20and%20Jia-Wang%20Bian%20and%20Xinghui%20Li%20and%20Guangrun%20Wang%20and%20Ian%20Reid%20and%20Philip%20Torr%20and%20Victor%20Adrian%20Prisacariu&entry.1292438233=%20%20We%20propose%20GaussCtrl%2C%20a%20text-driven%20method%20to%20edit%20a%203D%20scene%20reconstructed%0Aby%20the%203D%20Gaussian%20Splatting%20%283DGS%29.%0A%20%20Our%20method%20first%20renders%20a%20collection%20of%20images%20by%20using%20the%203DGS%20and%20edits%0Athem%20by%20using%20a%20pre-trained%202D%20diffusion%20model%20%28ControlNet%29%20based%20on%20the%20input%0Aprompt%2C%20which%20is%20then%20used%20to%20optimise%20the%203D%20model.%0A%20%20Our%20key%20contribution%20is%20multi-view%20consistent%20editing%2C%20which%20enables%20editing%0Aall%20images%20together%20instead%20of%20iteratively%20editing%20one%20image%20while%20updating%20the%0A3D%20model%20as%20in%20previous%20works.%0A%20%20It%20leads%20to%20faster%20editing%20as%20well%20as%20higher%20visual%20quality.%0A%20%20This%20is%20achieved%20by%20the%20two%20terms%3A%0A%20%20%28a%29%20depth-conditioned%20editing%20that%20enforces%20geometric%20consistency%20across%0Amulti-view%20images%20by%20leveraging%20naturally%20consistent%20depth%20maps.%0A%20%20%28b%29%20attention-based%20latent%20code%20alignment%20that%20unifies%20the%20appearance%20of%0Aedited%20images%20by%20conditioning%20their%20editing%20to%20several%20reference%20views%20through%0Aself%20and%20cross-view%20attention%20between%20images%27%20latent%20representations.%0A%20%20Experiments%20demonstrate%20that%20our%20method%20achieves%20faster%20editing%20and%20better%0Avisual%20results%20than%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08733v3&entry.124074799=Read"},
{"title": "Tverberg's theorem and multi-class support vector machines", "author": "Pablo Sober\u00f3n", "abstract": "  We show how, using linear-algebraic tools developed to prove Tverberg's\ntheorem in combinatorial geometry, we can design new models of multi-class\nsupport vector machines (SVMs). These supervised learning protocols require\nfewer conditions to classify sets of points, and can be computed using existing\nbinary SVM algorithms in higher-dimensional spaces, including soft-margin SVM\nalgorithms. We describe how the theoretical guarantees of standard support\nvector machines transfer to these new classes of multi-class support vector\nmachines. We give a new simple proof of a geometric characterization of support\nvectors for largest margin SVMs by Veelaert.\n", "link": "http://arxiv.org/abs/2404.16724v1", "date": "2024-04-25", "relevancy": 1.8126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.473}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4656}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4283}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tverberg%27s%20theorem%20and%20multi-class%20support%20vector%20machines&body=Title%3A%20Tverberg%27s%20theorem%20and%20multi-class%20support%20vector%20machines%0AAuthor%3A%20Pablo%20Sober%C3%B3n%0AAbstract%3A%20%20%20We%20show%20how%2C%20using%20linear-algebraic%20tools%20developed%20to%20prove%20Tverberg%27s%0Atheorem%20in%20combinatorial%20geometry%2C%20we%20can%20design%20new%20models%20of%20multi-class%0Asupport%20vector%20machines%20%28SVMs%29.%20These%20supervised%20learning%20protocols%20require%0Afewer%20conditions%20to%20classify%20sets%20of%20points%2C%20and%20can%20be%20computed%20using%20existing%0Abinary%20SVM%20algorithms%20in%20higher-dimensional%20spaces%2C%20including%20soft-margin%20SVM%0Aalgorithms.%20We%20describe%20how%20the%20theoretical%20guarantees%20of%20standard%20support%0Avector%20machines%20transfer%20to%20these%20new%20classes%20of%20multi-class%20support%20vector%0Amachines.%20We%20give%20a%20new%20simple%20proof%20of%20a%20geometric%20characterization%20of%20support%0Avectors%20for%20largest%20margin%20SVMs%20by%20Veelaert.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16724v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tverberg%27s%20theorem%20and%20multi-class%20support%20vector%20machines&entry.906535625=Pablo%20Sober%C3%B3n&entry.1292438233=%20%20We%20show%20how%2C%20using%20linear-algebraic%20tools%20developed%20to%20prove%20Tverberg%27s%0Atheorem%20in%20combinatorial%20geometry%2C%20we%20can%20design%20new%20models%20of%20multi-class%0Asupport%20vector%20machines%20%28SVMs%29.%20These%20supervised%20learning%20protocols%20require%0Afewer%20conditions%20to%20classify%20sets%20of%20points%2C%20and%20can%20be%20computed%20using%20existing%0Abinary%20SVM%20algorithms%20in%20higher-dimensional%20spaces%2C%20including%20soft-margin%20SVM%0Aalgorithms.%20We%20describe%20how%20the%20theoretical%20guarantees%20of%20standard%20support%0Avector%20machines%20transfer%20to%20these%20new%20classes%20of%20multi-class%20support%20vector%0Amachines.%20We%20give%20a%20new%20simple%20proof%20of%20a%20geometric%20characterization%20of%20support%0Avectors%20for%20largest%20margin%20SVMs%20by%20Veelaert.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16724v1&entry.124074799=Read"},
{"title": "SIDEs: Separating Idealization from Deceptive Explanations in xAI", "author": "Emily Sullivan", "abstract": "  Explainable AI (xAI) methods are important for establishing trust in using\nblack-box models. However, recent criticism has mounted against current xAI\nmethods that they disagree, are necessarily false, and can be manipulated,\nwhich has started to undermine the deployment of black-box models. Rudin (2019)\ngoes so far as to say that we should stop using black-box models altogether in\nhigh-stakes cases because xAI explanations \"must be wrong\". However, strict\nfidelity to the truth is historically not a desideratum in science.\nIdealizations -- the intentional distortions introduced to scientific theories\nand models -- are commonplace in the natural sciences and are seen as a\nsuccessful scientific tool. Thus, it is not falsehood qua falsehood that is the\nissue. In this paper, I outline the need for xAI research to engage in\nidealization evaluation. Drawing on the use of idealizations in the natural\nsciences and philosophy of science, I introduce a novel framework for\nevaluating whether xAI methods engage in successful idealizations or deceptive\nexplanations (SIDEs). SIDEs evaluates whether the limitations of xAI methods,\nand the distortions that they introduce, can be part of a successful\nidealization or are indeed deceptive distortions as critics suggest. I discuss\nthe role that existing research can play in idealization evaluation and where\ninnovation is necessary. Through a qualitative analysis we find that leading\nfeature importance methods and counterfactual explanations are subject to\nidealization failure and suggest remedies for ameliorating idealization\nfailure.\n", "link": "http://arxiv.org/abs/2404.16534v1", "date": "2024-04-25", "relevancy": 1.8018, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4612}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4527}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SIDEs%3A%20Separating%20Idealization%20from%20Deceptive%20Explanations%20in%20xAI&body=Title%3A%20SIDEs%3A%20Separating%20Idealization%20from%20Deceptive%20Explanations%20in%20xAI%0AAuthor%3A%20Emily%20Sullivan%0AAbstract%3A%20%20%20Explainable%20AI%20%28xAI%29%20methods%20are%20important%20for%20establishing%20trust%20in%20using%0Ablack-box%20models.%20However%2C%20recent%20criticism%20has%20mounted%20against%20current%20xAI%0Amethods%20that%20they%20disagree%2C%20are%20necessarily%20false%2C%20and%20can%20be%20manipulated%2C%0Awhich%20has%20started%20to%20undermine%20the%20deployment%20of%20black-box%20models.%20Rudin%20%282019%29%0Agoes%20so%20far%20as%20to%20say%20that%20we%20should%20stop%20using%20black-box%20models%20altogether%20in%0Ahigh-stakes%20cases%20because%20xAI%20explanations%20%22must%20be%20wrong%22.%20However%2C%20strict%0Afidelity%20to%20the%20truth%20is%20historically%20not%20a%20desideratum%20in%20science.%0AIdealizations%20--%20the%20intentional%20distortions%20introduced%20to%20scientific%20theories%0Aand%20models%20--%20are%20commonplace%20in%20the%20natural%20sciences%20and%20are%20seen%20as%20a%0Asuccessful%20scientific%20tool.%20Thus%2C%20it%20is%20not%20falsehood%20qua%20falsehood%20that%20is%20the%0Aissue.%20In%20this%20paper%2C%20I%20outline%20the%20need%20for%20xAI%20research%20to%20engage%20in%0Aidealization%20evaluation.%20Drawing%20on%20the%20use%20of%20idealizations%20in%20the%20natural%0Asciences%20and%20philosophy%20of%20science%2C%20I%20introduce%20a%20novel%20framework%20for%0Aevaluating%20whether%20xAI%20methods%20engage%20in%20successful%20idealizations%20or%20deceptive%0Aexplanations%20%28SIDEs%29.%20SIDEs%20evaluates%20whether%20the%20limitations%20of%20xAI%20methods%2C%0Aand%20the%20distortions%20that%20they%20introduce%2C%20can%20be%20part%20of%20a%20successful%0Aidealization%20or%20are%20indeed%20deceptive%20distortions%20as%20critics%20suggest.%20I%20discuss%0Athe%20role%20that%20existing%20research%20can%20play%20in%20idealization%20evaluation%20and%20where%0Ainnovation%20is%20necessary.%20Through%20a%20qualitative%20analysis%20we%20find%20that%20leading%0Afeature%20importance%20methods%20and%20counterfactual%20explanations%20are%20subject%20to%0Aidealization%20failure%20and%20suggest%20remedies%20for%20ameliorating%20idealization%0Afailure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16534v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIDEs%3A%20Separating%20Idealization%20from%20Deceptive%20Explanations%20in%20xAI&entry.906535625=Emily%20Sullivan&entry.1292438233=%20%20Explainable%20AI%20%28xAI%29%20methods%20are%20important%20for%20establishing%20trust%20in%20using%0Ablack-box%20models.%20However%2C%20recent%20criticism%20has%20mounted%20against%20current%20xAI%0Amethods%20that%20they%20disagree%2C%20are%20necessarily%20false%2C%20and%20can%20be%20manipulated%2C%0Awhich%20has%20started%20to%20undermine%20the%20deployment%20of%20black-box%20models.%20Rudin%20%282019%29%0Agoes%20so%20far%20as%20to%20say%20that%20we%20should%20stop%20using%20black-box%20models%20altogether%20in%0Ahigh-stakes%20cases%20because%20xAI%20explanations%20%22must%20be%20wrong%22.%20However%2C%20strict%0Afidelity%20to%20the%20truth%20is%20historically%20not%20a%20desideratum%20in%20science.%0AIdealizations%20--%20the%20intentional%20distortions%20introduced%20to%20scientific%20theories%0Aand%20models%20--%20are%20commonplace%20in%20the%20natural%20sciences%20and%20are%20seen%20as%20a%0Asuccessful%20scientific%20tool.%20Thus%2C%20it%20is%20not%20falsehood%20qua%20falsehood%20that%20is%20the%0Aissue.%20In%20this%20paper%2C%20I%20outline%20the%20need%20for%20xAI%20research%20to%20engage%20in%0Aidealization%20evaluation.%20Drawing%20on%20the%20use%20of%20idealizations%20in%20the%20natural%0Asciences%20and%20philosophy%20of%20science%2C%20I%20introduce%20a%20novel%20framework%20for%0Aevaluating%20whether%20xAI%20methods%20engage%20in%20successful%20idealizations%20or%20deceptive%0Aexplanations%20%28SIDEs%29.%20SIDEs%20evaluates%20whether%20the%20limitations%20of%20xAI%20methods%2C%0Aand%20the%20distortions%20that%20they%20introduce%2C%20can%20be%20part%20of%20a%20successful%0Aidealization%20or%20are%20indeed%20deceptive%20distortions%20as%20critics%20suggest.%20I%20discuss%0Athe%20role%20that%20existing%20research%20can%20play%20in%20idealization%20evaluation%20and%20where%0Ainnovation%20is%20necessary.%20Through%20a%20qualitative%20analysis%20we%20find%20that%20leading%0Afeature%20importance%20methods%20and%20counterfactual%20explanations%20are%20subject%20to%0Aidealization%20failure%20and%20suggest%20remedies%20for%20ameliorating%20idealization%0Afailure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16534v1&entry.124074799=Read"},
{"title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation\n  Language Model", "author": "Runzhe Zhan and Xinyi Yang and Derek F. Wong and Lidia S. Chao and Yue Zhang", "abstract": "  While supervised fine-tuning (SFT) has been a straightforward approach for\ntailoring the output of foundation large language model (LLM) to specific\npreferences, concerns have been raised about the depth of this alignment, with\nsome critiques suggesting it is merely \"superficial\". We critically examine\nthis hypothesis within the scope of cross-lingual generation tasks, proposing\nthat the effectiveness of SFT may be constrained by its reliance on prior\ntokens to guide cross-lingual generation. Based on this crucial insight, and in\nresponse to the challenges posed by the costly and limited availability of\nnon-English data for SFT, we introduce a novel training-free alignment method\nnamed PreTTY, which employs minimal task-related prior tokens to bridge the\nfoundation LLM and the SFT LLM, achieving comparable performance without\ntraining. Experiments on machine translation and part-of-speech tagging across\neight languages demonstrate the efficacy of PreTTY in cross-lingual settings.\nRemarkably, by initiating the decoding process with only one or two prior\ntokens, foundation LLMs can achieve performance comparable to their SFT\ncounterparts. This method presents a cost-effective alternative to SFT and\nadvances the democratization of multilingual LLMs.\n", "link": "http://arxiv.org/abs/2404.16766v1", "date": "2024-04-25", "relevancy": 1.8013, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4651}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4459}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4244}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Prefix%20Text%20as%20a%20Yarn%3A%20Eliciting%20Non-English%20Alignment%20in%20Foundation%0A%20%20Language%20Model&body=Title%3A%20Prefix%20Text%20as%20a%20Yarn%3A%20Eliciting%20Non-English%20Alignment%20in%20Foundation%0A%20%20Language%20Model%0AAuthor%3A%20Runzhe%20Zhan%20and%20Xinyi%20Yang%20and%20Derek%20F.%20Wong%20and%20Lidia%20S.%20Chao%20and%20Yue%20Zhang%0AAbstract%3A%20%20%20While%20supervised%20fine-tuning%20%28SFT%29%20has%20been%20a%20straightforward%20approach%20for%0Atailoring%20the%20output%20of%20foundation%20large%20language%20model%20%28LLM%29%20to%20specific%0Apreferences%2C%20concerns%20have%20been%20raised%20about%20the%20depth%20of%20this%20alignment%2C%20with%0Asome%20critiques%20suggesting%20it%20is%20merely%20%22superficial%22.%20We%20critically%20examine%0Athis%20hypothesis%20within%20the%20scope%20of%20cross-lingual%20generation%20tasks%2C%20proposing%0Athat%20the%20effectiveness%20of%20SFT%20may%20be%20constrained%20by%20its%20reliance%20on%20prior%0Atokens%20to%20guide%20cross-lingual%20generation.%20Based%20on%20this%20crucial%20insight%2C%20and%20in%0Aresponse%20to%20the%20challenges%20posed%20by%20the%20costly%20and%20limited%20availability%20of%0Anon-English%20data%20for%20SFT%2C%20we%20introduce%20a%20novel%20training-free%20alignment%20method%0Anamed%20PreTTY%2C%20which%20employs%20minimal%20task-related%20prior%20tokens%20to%20bridge%20the%0Afoundation%20LLM%20and%20the%20SFT%20LLM%2C%20achieving%20comparable%20performance%20without%0Atraining.%20Experiments%20on%20machine%20translation%20and%20part-of-speech%20tagging%20across%0Aeight%20languages%20demonstrate%20the%20efficacy%20of%20PreTTY%20in%20cross-lingual%20settings.%0ARemarkably%2C%20by%20initiating%20the%20decoding%20process%20with%20only%20one%20or%20two%20prior%0Atokens%2C%20foundation%20LLMs%20can%20achieve%20performance%20comparable%20to%20their%20SFT%0Acounterparts.%20This%20method%20presents%20a%20cost-effective%20alternative%20to%20SFT%20and%0Aadvances%20the%20democratization%20of%20multilingual%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16766v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prefix%20Text%20as%20a%20Yarn%3A%20Eliciting%20Non-English%20Alignment%20in%20Foundation%0A%20%20Language%20Model&entry.906535625=Runzhe%20Zhan%20and%20Xinyi%20Yang%20and%20Derek%20F.%20Wong%20and%20Lidia%20S.%20Chao%20and%20Yue%20Zhang&entry.1292438233=%20%20While%20supervised%20fine-tuning%20%28SFT%29%20has%20been%20a%20straightforward%20approach%20for%0Atailoring%20the%20output%20of%20foundation%20large%20language%20model%20%28LLM%29%20to%20specific%0Apreferences%2C%20concerns%20have%20been%20raised%20about%20the%20depth%20of%20this%20alignment%2C%20with%0Asome%20critiques%20suggesting%20it%20is%20merely%20%22superficial%22.%20We%20critically%20examine%0Athis%20hypothesis%20within%20the%20scope%20of%20cross-lingual%20generation%20tasks%2C%20proposing%0Athat%20the%20effectiveness%20of%20SFT%20may%20be%20constrained%20by%20its%20reliance%20on%20prior%0Atokens%20to%20guide%20cross-lingual%20generation.%20Based%20on%20this%20crucial%20insight%2C%20and%20in%0Aresponse%20to%20the%20challenges%20posed%20by%20the%20costly%20and%20limited%20availability%20of%0Anon-English%20data%20for%20SFT%2C%20we%20introduce%20a%20novel%20training-free%20alignment%20method%0Anamed%20PreTTY%2C%20which%20employs%20minimal%20task-related%20prior%20tokens%20to%20bridge%20the%0Afoundation%20LLM%20and%20the%20SFT%20LLM%2C%20achieving%20comparable%20performance%20without%0Atraining.%20Experiments%20on%20machine%20translation%20and%20part-of-speech%20tagging%20across%0Aeight%20languages%20demonstrate%20the%20efficacy%20of%20PreTTY%20in%20cross-lingual%20settings.%0ARemarkably%2C%20by%20initiating%20the%20decoding%20process%20with%20only%20one%20or%20two%20prior%0Atokens%2C%20foundation%20LLMs%20can%20achieve%20performance%20comparable%20to%20their%20SFT%0Acounterparts.%20This%20method%20presents%20a%20cost-effective%20alternative%20to%20SFT%20and%0Aadvances%20the%20democratization%20of%20multilingual%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16766v1&entry.124074799=Read"},
{"title": "Learning to Beat ByteRL: Exploitability of Collectible Card Game Agents", "author": "Radovan Haluska and Martin Schmid", "abstract": "  While Poker, as a family of games, has been studied extensively in the last\ndecades, collectible card games have seen relatively little attention. Only\nrecently have we seen an agent that can compete with professional human players\nin Hearthstone, one of the most popular collectible card games. Although\nartificial agents must be able to work with imperfect information in both of\nthese genres, collectible card games pose another set of distinct challenges.\nUnlike in many poker variants, agents must deal with state space so vast that\neven enumerating all states consistent with the agent's beliefs is intractable,\nrendering the current search methods unusable and requiring the agents to opt\nfor other techniques. In this paper, we investigate the strength of such\ntechniques for this class of games. Namely, we present preliminary analysis\nresults of ByteRL, the state-of-the-art agent in Legends of Code and Magic and\nHearthstone. Although ByteRL beat a top-10 Hearthstone player from China, we\nshow that its play in Legends of Code and Magic is highly exploitable.\n", "link": "http://arxiv.org/abs/2404.16689v1", "date": "2024-04-25", "relevancy": 1.7973, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.464}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4515}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4413}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Beat%20ByteRL%3A%20Exploitability%20of%20Collectible%20Card%20Game%20Agents&body=Title%3A%20Learning%20to%20Beat%20ByteRL%3A%20Exploitability%20of%20Collectible%20Card%20Game%20Agents%0AAuthor%3A%20Radovan%20Haluska%20and%20Martin%20Schmid%0AAbstract%3A%20%20%20While%20Poker%2C%20as%20a%20family%20of%20games%2C%20has%20been%20studied%20extensively%20in%20the%20last%0Adecades%2C%20collectible%20card%20games%20have%20seen%20relatively%20little%20attention.%20Only%0Arecently%20have%20we%20seen%20an%20agent%20that%20can%20compete%20with%20professional%20human%20players%0Ain%20Hearthstone%2C%20one%20of%20the%20most%20popular%20collectible%20card%20games.%20Although%0Aartificial%20agents%20must%20be%20able%20to%20work%20with%20imperfect%20information%20in%20both%20of%0Athese%20genres%2C%20collectible%20card%20games%20pose%20another%20set%20of%20distinct%20challenges.%0AUnlike%20in%20many%20poker%20variants%2C%20agents%20must%20deal%20with%20state%20space%20so%20vast%20that%0Aeven%20enumerating%20all%20states%20consistent%20with%20the%20agent%27s%20beliefs%20is%20intractable%2C%0Arendering%20the%20current%20search%20methods%20unusable%20and%20requiring%20the%20agents%20to%20opt%0Afor%20other%20techniques.%20In%20this%20paper%2C%20we%20investigate%20the%20strength%20of%20such%0Atechniques%20for%20this%20class%20of%20games.%20Namely%2C%20we%20present%20preliminary%20analysis%0Aresults%20of%20ByteRL%2C%20the%20state-of-the-art%20agent%20in%20Legends%20of%20Code%20and%20Magic%20and%0AHearthstone.%20Although%20ByteRL%20beat%20a%20top-10%20Hearthstone%20player%20from%20China%2C%20we%0Ashow%20that%20its%20play%20in%20Legends%20of%20Code%20and%20Magic%20is%20highly%20exploitable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16689v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Beat%20ByteRL%3A%20Exploitability%20of%20Collectible%20Card%20Game%20Agents&entry.906535625=Radovan%20Haluska%20and%20Martin%20Schmid&entry.1292438233=%20%20While%20Poker%2C%20as%20a%20family%20of%20games%2C%20has%20been%20studied%20extensively%20in%20the%20last%0Adecades%2C%20collectible%20card%20games%20have%20seen%20relatively%20little%20attention.%20Only%0Arecently%20have%20we%20seen%20an%20agent%20that%20can%20compete%20with%20professional%20human%20players%0Ain%20Hearthstone%2C%20one%20of%20the%20most%20popular%20collectible%20card%20games.%20Although%0Aartificial%20agents%20must%20be%20able%20to%20work%20with%20imperfect%20information%20in%20both%20of%0Athese%20genres%2C%20collectible%20card%20games%20pose%20another%20set%20of%20distinct%20challenges.%0AUnlike%20in%20many%20poker%20variants%2C%20agents%20must%20deal%20with%20state%20space%20so%20vast%20that%0Aeven%20enumerating%20all%20states%20consistent%20with%20the%20agent%27s%20beliefs%20is%20intractable%2C%0Arendering%20the%20current%20search%20methods%20unusable%20and%20requiring%20the%20agents%20to%20opt%0Afor%20other%20techniques.%20In%20this%20paper%2C%20we%20investigate%20the%20strength%20of%20such%0Atechniques%20for%20this%20class%20of%20games.%20Namely%2C%20we%20present%20preliminary%20analysis%0Aresults%20of%20ByteRL%2C%20the%20state-of-the-art%20agent%20in%20Legends%20of%20Code%20and%20Magic%20and%0AHearthstone.%20Although%20ByteRL%20beat%20a%20top-10%20Hearthstone%20player%20from%20China%2C%20we%0Ashow%20that%20its%20play%20in%20Legends%20of%20Code%20and%20Magic%20is%20highly%20exploitable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16689v1&entry.124074799=Read"},
{"title": "DAVE -- A Detect-and-Verify Paradigm for Low-Shot Counting", "author": "Jer Pelhan and Alan Luke\u017ei\u010d and Vitjan Zavrtanik and Matej Kristan", "abstract": "  Low-shot counters estimate the number of objects corresponding to a selected\ncategory, based on only few or no exemplars annotated in the image. The current\nstate-of-the-art estimates the total counts as the sum over the object location\ndensity map, but does not provide individual object locations and sizes, which\nare crucial for many applications. This is addressed by detection-based\ncounters, which, however fall behind in the total count accuracy. Furthermore,\nboth approaches tend to overestimate the counts in the presence of other object\nclasses due to many false positives. We propose DAVE, a low-shot counter based\non a detect-and-verify paradigm, that avoids the aforementioned issues by first\ngenerating a high-recall detection set and then verifying the detections to\nidentify and remove the outliers. This jointly increases the recall and\nprecision, leading to accurate counts. DAVE outperforms the top density-based\ncounters by ~20% in the total count MAE, it outperforms the most recent\ndetection-based counter by ~20% in detection quality and sets a new\nstate-of-the-art in zero-shot as well as text-prompt-based counting.\n", "link": "http://arxiv.org/abs/2404.16622v1", "date": "2024-04-25", "relevancy": 1.7679, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4545}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4403}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4301}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DAVE%20--%20A%20Detect-and-Verify%20Paradigm%20for%20Low-Shot%20Counting&body=Title%3A%20DAVE%20--%20A%20Detect-and-Verify%20Paradigm%20for%20Low-Shot%20Counting%0AAuthor%3A%20Jer%20Pelhan%20and%20Alan%20Luke%C5%BEi%C4%8D%20and%20Vitjan%20Zavrtanik%20and%20Matej%20Kristan%0AAbstract%3A%20%20%20Low-shot%20counters%20estimate%20the%20number%20of%20objects%20corresponding%20to%20a%20selected%0Acategory%2C%20based%20on%20only%20few%20or%20no%20exemplars%20annotated%20in%20the%20image.%20The%20current%0Astate-of-the-art%20estimates%20the%20total%20counts%20as%20the%20sum%20over%20the%20object%20location%0Adensity%20map%2C%20but%20does%20not%20provide%20individual%20object%20locations%20and%20sizes%2C%20which%0Aare%20crucial%20for%20many%20applications.%20This%20is%20addressed%20by%20detection-based%0Acounters%2C%20which%2C%20however%20fall%20behind%20in%20the%20total%20count%20accuracy.%20Furthermore%2C%0Aboth%20approaches%20tend%20to%20overestimate%20the%20counts%20in%20the%20presence%20of%20other%20object%0Aclasses%20due%20to%20many%20false%20positives.%20We%20propose%20DAVE%2C%20a%20low-shot%20counter%20based%0Aon%20a%20detect-and-verify%20paradigm%2C%20that%20avoids%20the%20aforementioned%20issues%20by%20first%0Agenerating%20a%20high-recall%20detection%20set%20and%20then%20verifying%20the%20detections%20to%0Aidentify%20and%20remove%20the%20outliers.%20This%20jointly%20increases%20the%20recall%20and%0Aprecision%2C%20leading%20to%20accurate%20counts.%20DAVE%20outperforms%20the%20top%20density-based%0Acounters%20by%20~20%25%20in%20the%20total%20count%20MAE%2C%20it%20outperforms%20the%20most%20recent%0Adetection-based%20counter%20by%20~20%25%20in%20detection%20quality%20and%20sets%20a%20new%0Astate-of-the-art%20in%20zero-shot%20as%20well%20as%20text-prompt-based%20counting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16622v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAVE%20--%20A%20Detect-and-Verify%20Paradigm%20for%20Low-Shot%20Counting&entry.906535625=Jer%20Pelhan%20and%20Alan%20Luke%C5%BEi%C4%8D%20and%20Vitjan%20Zavrtanik%20and%20Matej%20Kristan&entry.1292438233=%20%20Low-shot%20counters%20estimate%20the%20number%20of%20objects%20corresponding%20to%20a%20selected%0Acategory%2C%20based%20on%20only%20few%20or%20no%20exemplars%20annotated%20in%20the%20image.%20The%20current%0Astate-of-the-art%20estimates%20the%20total%20counts%20as%20the%20sum%20over%20the%20object%20location%0Adensity%20map%2C%20but%20does%20not%20provide%20individual%20object%20locations%20and%20sizes%2C%20which%0Aare%20crucial%20for%20many%20applications.%20This%20is%20addressed%20by%20detection-based%0Acounters%2C%20which%2C%20however%20fall%20behind%20in%20the%20total%20count%20accuracy.%20Furthermore%2C%0Aboth%20approaches%20tend%20to%20overestimate%20the%20counts%20in%20the%20presence%20of%20other%20object%0Aclasses%20due%20to%20many%20false%20positives.%20We%20propose%20DAVE%2C%20a%20low-shot%20counter%20based%0Aon%20a%20detect-and-verify%20paradigm%2C%20that%20avoids%20the%20aforementioned%20issues%20by%20first%0Agenerating%20a%20high-recall%20detection%20set%20and%20then%20verifying%20the%20detections%20to%0Aidentify%20and%20remove%20the%20outliers.%20This%20jointly%20increases%20the%20recall%20and%0Aprecision%2C%20leading%20to%20accurate%20counts.%20DAVE%20outperforms%20the%20top%20density-based%0Acounters%20by%20~20%25%20in%20the%20total%20count%20MAE%2C%20it%20outperforms%20the%20most%20recent%0Adetection-based%20counter%20by%20~20%25%20in%20detection%20quality%20and%20sets%20a%20new%0Astate-of-the-art%20in%20zero-shot%20as%20well%20as%20text-prompt-based%20counting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16622v1&entry.124074799=Read"},
{"title": "RUMOR: Reinforcement learning for Understanding a Model of the Real\n  World for Navigation in Dynamic Environments", "author": "Diego Martinez-Baselga and Luis Riazuelo and Luis Montano", "abstract": "  Autonomous navigation in dynamic environments is a complex but essential task\nfor autonomous robots, with recent deep reinforcement learning approaches\nshowing promising results. However, the complexity of the real world makes it\ninfeasible to train agents in every possible scenario configuration. Moreover,\nexisting methods typically overlook factors such as robot kinodynamic\nconstraints, or assume perfect knowledge of the environment. In this work, we\npresent RUMOR, a novel planner for differential-drive robots that uses deep\nreinforcement learning to navigate in highly dynamic environments. Unlike other\nend-to-end DRL planners, it uses a descriptive robocentric velocity space model\nto extract the dynamic environment information, enhancing training\neffectiveness and scenario interpretation. Additionally, we propose an action\nspace that inherently considers robot kinodynamics and train it in a simulator\nthat reproduces the real world problematic aspects, reducing the gap between\nthe reality and simulation. We extensively compare RUMOR with other\nstate-of-the-art approaches, demonstrating a better performance, and provide a\ndetailed analysis of the results. Finally, we validate RUMOR's performance in\nreal-world settings by deploying it on a ground robot. Our experiments,\nconducted in crowded scenarios and unseen environments, confirm the algorithm's\nrobustness and transferability.\n", "link": "http://arxiv.org/abs/2404.16672v1", "date": "2024-04-25", "relevancy": 1.7492, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6103}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6026}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5644}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RUMOR%3A%20Reinforcement%20learning%20for%20Understanding%20a%20Model%20of%20the%20Real%0A%20%20World%20for%20Navigation%20in%20Dynamic%20Environments&body=Title%3A%20RUMOR%3A%20Reinforcement%20learning%20for%20Understanding%20a%20Model%20of%20the%20Real%0A%20%20World%20for%20Navigation%20in%20Dynamic%20Environments%0AAuthor%3A%20Diego%20Martinez-Baselga%20and%20Luis%20Riazuelo%20and%20Luis%20Montano%0AAbstract%3A%20%20%20Autonomous%20navigation%20in%20dynamic%20environments%20is%20a%20complex%20but%20essential%20task%0Afor%20autonomous%20robots%2C%20with%20recent%20deep%20reinforcement%20learning%20approaches%0Ashowing%20promising%20results.%20However%2C%20the%20complexity%20of%20the%20real%20world%20makes%20it%0Ainfeasible%20to%20train%20agents%20in%20every%20possible%20scenario%20configuration.%20Moreover%2C%0Aexisting%20methods%20typically%20overlook%20factors%20such%20as%20robot%20kinodynamic%0Aconstraints%2C%20or%20assume%20perfect%20knowledge%20of%20the%20environment.%20In%20this%20work%2C%20we%0Apresent%20RUMOR%2C%20a%20novel%20planner%20for%20differential-drive%20robots%20that%20uses%20deep%0Areinforcement%20learning%20to%20navigate%20in%20highly%20dynamic%20environments.%20Unlike%20other%0Aend-to-end%20DRL%20planners%2C%20it%20uses%20a%20descriptive%20robocentric%20velocity%20space%20model%0Ato%20extract%20the%20dynamic%20environment%20information%2C%20enhancing%20training%0Aeffectiveness%20and%20scenario%20interpretation.%20Additionally%2C%20we%20propose%20an%20action%0Aspace%20that%20inherently%20considers%20robot%20kinodynamics%20and%20train%20it%20in%20a%20simulator%0Athat%20reproduces%20the%20real%20world%20problematic%20aspects%2C%20reducing%20the%20gap%20between%0Athe%20reality%20and%20simulation.%20We%20extensively%20compare%20RUMOR%20with%20other%0Astate-of-the-art%20approaches%2C%20demonstrating%20a%20better%20performance%2C%20and%20provide%20a%0Adetailed%20analysis%20of%20the%20results.%20Finally%2C%20we%20validate%20RUMOR%27s%20performance%20in%0Areal-world%20settings%20by%20deploying%20it%20on%20a%20ground%20robot.%20Our%20experiments%2C%0Aconducted%20in%20crowded%20scenarios%20and%20unseen%20environments%2C%20confirm%20the%20algorithm%27s%0Arobustness%20and%20transferability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16672v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RUMOR%3A%20Reinforcement%20learning%20for%20Understanding%20a%20Model%20of%20the%20Real%0A%20%20World%20for%20Navigation%20in%20Dynamic%20Environments&entry.906535625=Diego%20Martinez-Baselga%20and%20Luis%20Riazuelo%20and%20Luis%20Montano&entry.1292438233=%20%20Autonomous%20navigation%20in%20dynamic%20environments%20is%20a%20complex%20but%20essential%20task%0Afor%20autonomous%20robots%2C%20with%20recent%20deep%20reinforcement%20learning%20approaches%0Ashowing%20promising%20results.%20However%2C%20the%20complexity%20of%20the%20real%20world%20makes%20it%0Ainfeasible%20to%20train%20agents%20in%20every%20possible%20scenario%20configuration.%20Moreover%2C%0Aexisting%20methods%20typically%20overlook%20factors%20such%20as%20robot%20kinodynamic%0Aconstraints%2C%20or%20assume%20perfect%20knowledge%20of%20the%20environment.%20In%20this%20work%2C%20we%0Apresent%20RUMOR%2C%20a%20novel%20planner%20for%20differential-drive%20robots%20that%20uses%20deep%0Areinforcement%20learning%20to%20navigate%20in%20highly%20dynamic%20environments.%20Unlike%20other%0Aend-to-end%20DRL%20planners%2C%20it%20uses%20a%20descriptive%20robocentric%20velocity%20space%20model%0Ato%20extract%20the%20dynamic%20environment%20information%2C%20enhancing%20training%0Aeffectiveness%20and%20scenario%20interpretation.%20Additionally%2C%20we%20propose%20an%20action%0Aspace%20that%20inherently%20considers%20robot%20kinodynamics%20and%20train%20it%20in%20a%20simulator%0Athat%20reproduces%20the%20real%20world%20problematic%20aspects%2C%20reducing%20the%20gap%20between%0Athe%20reality%20and%20simulation.%20We%20extensively%20compare%20RUMOR%20with%20other%0Astate-of-the-art%20approaches%2C%20demonstrating%20a%20better%20performance%2C%20and%20provide%20a%0Adetailed%20analysis%20of%20the%20results.%20Finally%2C%20we%20validate%20RUMOR%27s%20performance%20in%0Areal-world%20settings%20by%20deploying%20it%20on%20a%20ground%20robot.%20Our%20experiments%2C%0Aconducted%20in%20crowded%20scenarios%20and%20unseen%20environments%2C%20confirm%20the%20algorithm%27s%0Arobustness%20and%20transferability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16672v1&entry.124074799=Read"},
{"title": "Efficient Frontier Management for Collaborative Active SLAM", "author": "Matteo Maragliano and Muhammad Farhan Ahmed and Carmine Tommaso Recchiuto and Antonio Sgorbissa and Vincent Fremont", "abstract": "  In autonomous robotics, a critical challenge lies in developing robust\nsolutions for Active Collaborative SLAM, wherein multiple robots\ncollaboratively explore and map an unknown environment while intelligently\ncoordinating their movements and sensor data acquisitions. In this article, we\npresent an approach for coordinating a system consisting of multiple robots to\nperform Active Collaborative SLAM (AC-SLAM) for environmental exploration. Our\nmethod efficiently spreads the robots for maximum exploration while keeping\nSLAM uncertainty low. Additionally, We also present two coordination\napproaches, synchronous and asynchronous to prioritize robot goal assignments\nby the central server. The proposed method is implemented in ROS and evaluated\nthrough simulation and experiments on publicly available datasets and similar\nmethods, rendering promising results.\n", "link": "http://arxiv.org/abs/2310.01967v3", "date": "2024-04-25", "relevancy": 1.724, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6099}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5706}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5496}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Frontier%20Management%20for%20Collaborative%20Active%20SLAM&body=Title%3A%20Efficient%20Frontier%20Management%20for%20Collaborative%20Active%20SLAM%0AAuthor%3A%20Matteo%20Maragliano%20and%20Muhammad%20Farhan%20Ahmed%20and%20Carmine%20Tommaso%20Recchiuto%20and%20Antonio%20Sgorbissa%20and%20Vincent%20Fremont%0AAbstract%3A%20%20%20In%20autonomous%20robotics%2C%20a%20critical%20challenge%20lies%20in%20developing%20robust%0Asolutions%20for%20Active%20Collaborative%20SLAM%2C%20wherein%20multiple%20robots%0Acollaboratively%20explore%20and%20map%20an%20unknown%20environment%20while%20intelligently%0Acoordinating%20their%20movements%20and%20sensor%20data%20acquisitions.%20In%20this%20article%2C%20we%0Apresent%20an%20approach%20for%20coordinating%20a%20system%20consisting%20of%20multiple%20robots%20to%0Aperform%20Active%20Collaborative%20SLAM%20%28AC-SLAM%29%20for%20environmental%20exploration.%20Our%0Amethod%20efficiently%20spreads%20the%20robots%20for%20maximum%20exploration%20while%20keeping%0ASLAM%20uncertainty%20low.%20Additionally%2C%20We%20also%20present%20two%20coordination%0Aapproaches%2C%20synchronous%20and%20asynchronous%20to%20prioritize%20robot%20goal%20assignments%0Aby%20the%20central%20server.%20The%20proposed%20method%20is%20implemented%20in%20ROS%20and%20evaluated%0Athrough%20simulation%20and%20experiments%20on%20publicly%20available%20datasets%20and%20similar%0Amethods%2C%20rendering%20promising%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01967v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Frontier%20Management%20for%20Collaborative%20Active%20SLAM&entry.906535625=Matteo%20Maragliano%20and%20Muhammad%20Farhan%20Ahmed%20and%20Carmine%20Tommaso%20Recchiuto%20and%20Antonio%20Sgorbissa%20and%20Vincent%20Fremont&entry.1292438233=%20%20In%20autonomous%20robotics%2C%20a%20critical%20challenge%20lies%20in%20developing%20robust%0Asolutions%20for%20Active%20Collaborative%20SLAM%2C%20wherein%20multiple%20robots%0Acollaboratively%20explore%20and%20map%20an%20unknown%20environment%20while%20intelligently%0Acoordinating%20their%20movements%20and%20sensor%20data%20acquisitions.%20In%20this%20article%2C%20we%0Apresent%20an%20approach%20for%20coordinating%20a%20system%20consisting%20of%20multiple%20robots%20to%0Aperform%20Active%20Collaborative%20SLAM%20%28AC-SLAM%29%20for%20environmental%20exploration.%20Our%0Amethod%20efficiently%20spreads%20the%20robots%20for%20maximum%20exploration%20while%20keeping%0ASLAM%20uncertainty%20low.%20Additionally%2C%20We%20also%20present%20two%20coordination%0Aapproaches%2C%20synchronous%20and%20asynchronous%20to%20prioritize%20robot%20goal%20assignments%0Aby%20the%20central%20server.%20The%20proposed%20method%20is%20implemented%20in%20ROS%20and%20evaluated%0Athrough%20simulation%20and%20experiments%20on%20publicly%20available%20datasets%20and%20similar%0Amethods%2C%20rendering%20promising%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01967v3&entry.124074799=Read"},
{"title": "Bagging Provides Assumption-free Stability", "author": "Jake A. Soloff and Rina Foygel Barber and Rebecca Willett", "abstract": "  Bagging is an important technique for stabilizing machine learning models. In\nthis paper, we derive a finite-sample guarantee on the stability of bagging for\nany model. Our result places no assumptions on the distribution of the data, on\nthe properties of the base algorithm, or on the dimensionality of the\ncovariates. Our guarantee applies to many variants of bagging and is optimal up\nto a constant. Empirical results validate our findings, showing that bagging\nsuccessfully stabilizes even highly unstable base algorithms.\n", "link": "http://arxiv.org/abs/2301.12600v3", "date": "2024-04-25", "relevancy": 1.7161, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4248}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4145}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bagging%20Provides%20Assumption-free%20Stability&body=Title%3A%20Bagging%20Provides%20Assumption-free%20Stability%0AAuthor%3A%20Jake%20A.%20Soloff%20and%20Rina%20Foygel%20Barber%20and%20Rebecca%20Willett%0AAbstract%3A%20%20%20Bagging%20is%20an%20important%20technique%20for%20stabilizing%20machine%20learning%20models.%20In%0Athis%20paper%2C%20we%20derive%20a%20finite-sample%20guarantee%20on%20the%20stability%20of%20bagging%20for%0Aany%20model.%20Our%20result%20places%20no%20assumptions%20on%20the%20distribution%20of%20the%20data%2C%20on%0Athe%20properties%20of%20the%20base%20algorithm%2C%20or%20on%20the%20dimensionality%20of%20the%0Acovariates.%20Our%20guarantee%20applies%20to%20many%20variants%20of%20bagging%20and%20is%20optimal%20up%0Ato%20a%20constant.%20Empirical%20results%20validate%20our%20findings%2C%20showing%20that%20bagging%0Asuccessfully%20stabilizes%20even%20highly%20unstable%20base%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.12600v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bagging%20Provides%20Assumption-free%20Stability&entry.906535625=Jake%20A.%20Soloff%20and%20Rina%20Foygel%20Barber%20and%20Rebecca%20Willett&entry.1292438233=%20%20Bagging%20is%20an%20important%20technique%20for%20stabilizing%20machine%20learning%20models.%20In%0Athis%20paper%2C%20we%20derive%20a%20finite-sample%20guarantee%20on%20the%20stability%20of%20bagging%20for%0Aany%20model.%20Our%20result%20places%20no%20assumptions%20on%20the%20distribution%20of%20the%20data%2C%20on%0Athe%20properties%20of%20the%20base%20algorithm%2C%20or%20on%20the%20dimensionality%20of%20the%0Acovariates.%20Our%20guarantee%20applies%20to%20many%20variants%20of%20bagging%20and%20is%20optimal%20up%0Ato%20a%20constant.%20Empirical%20results%20validate%20our%20findings%2C%20showing%20that%20bagging%0Asuccessfully%20stabilizes%20even%20highly%20unstable%20base%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.12600v3&entry.124074799=Read"},
{"title": "MuseumMaker: Continual Style Customization without Catastrophic\n  Forgetting", "author": "Chenxi Liu and Gan Sun and Wenqi Liang and Jiahua Dong and Can Qin and Yang Cong", "abstract": "  Pre-trained large text-to-image (T2I) models with an appropriate text prompt\nhas attracted growing interests in customized images generation field. However,\ncatastrophic forgetting issue make it hard to continually synthesize new\nuser-provided styles while retaining the satisfying results amongst learned\nstyles. In this paper, we propose MuseumMaker, a method that enables the\nsynthesis of images by following a set of customized styles in a never-end\nmanner, and gradually accumulate these creative artistic works as a Museum.\nWhen facing with a new customization style, we develop a style distillation\nloss module to transfer the style of the whole dataset into generation of\nimages. It can minimize the learning biases caused by content of images, and\naddress the catastrophic overfitting issue induced by few-shot images. To deal\nwith catastrophic forgetting amongst past learned styles, we devise a dual\nregularization for shared-LoRA module to optimize the direction of model\nupdate, which could regularize the diffusion model from both weight and feature\naspects, respectively. Meanwhile, a unique token embedding corresponding to\nthis new style is learned by a task-wise token learning module, which could\npreserve historical knowledge from past styles with the limitation of LoRA\nparameter quantity. As any new user-provided style come, our MuseumMaker can\ncapture the nuances of the new styles while maintaining the details of learned\nstyles. Experimental results on diverse style datasets validate the\neffectiveness of our proposed MuseumMaker method, showcasing its robustness and\nversatility across various scenarios.\n", "link": "http://arxiv.org/abs/2404.16612v1", "date": "2024-04-25", "relevancy": 1.7023, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6004}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5805}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5491}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MuseumMaker%3A%20Continual%20Style%20Customization%20without%20Catastrophic%0A%20%20Forgetting&body=Title%3A%20MuseumMaker%3A%20Continual%20Style%20Customization%20without%20Catastrophic%0A%20%20Forgetting%0AAuthor%3A%20Chenxi%20Liu%20and%20Gan%20Sun%20and%20Wenqi%20Liang%20and%20Jiahua%20Dong%20and%20Can%20Qin%20and%20Yang%20Cong%0AAbstract%3A%20%20%20Pre-trained%20large%20text-to-image%20%28T2I%29%20models%20with%20an%20appropriate%20text%20prompt%0Ahas%20attracted%20growing%20interests%20in%20customized%20images%20generation%20field.%20However%2C%0Acatastrophic%20forgetting%20issue%20make%20it%20hard%20to%20continually%20synthesize%20new%0Auser-provided%20styles%20while%20retaining%20the%20satisfying%20results%20amongst%20learned%0Astyles.%20In%20this%20paper%2C%20we%20propose%20MuseumMaker%2C%20a%20method%20that%20enables%20the%0Asynthesis%20of%20images%20by%20following%20a%20set%20of%20customized%20styles%20in%20a%20never-end%0Amanner%2C%20and%20gradually%20accumulate%20these%20creative%20artistic%20works%20as%20a%20Museum.%0AWhen%20facing%20with%20a%20new%20customization%20style%2C%20we%20develop%20a%20style%20distillation%0Aloss%20module%20to%20transfer%20the%20style%20of%20the%20whole%20dataset%20into%20generation%20of%0Aimages.%20It%20can%20minimize%20the%20learning%20biases%20caused%20by%20content%20of%20images%2C%20and%0Aaddress%20the%20catastrophic%20overfitting%20issue%20induced%20by%20few-shot%20images.%20To%20deal%0Awith%20catastrophic%20forgetting%20amongst%20past%20learned%20styles%2C%20we%20devise%20a%20dual%0Aregularization%20for%20shared-LoRA%20module%20to%20optimize%20the%20direction%20of%20model%0Aupdate%2C%20which%20could%20regularize%20the%20diffusion%20model%20from%20both%20weight%20and%20feature%0Aaspects%2C%20respectively.%20Meanwhile%2C%20a%20unique%20token%20embedding%20corresponding%20to%0Athis%20new%20style%20is%20learned%20by%20a%20task-wise%20token%20learning%20module%2C%20which%20could%0Apreserve%20historical%20knowledge%20from%20past%20styles%20with%20the%20limitation%20of%20LoRA%0Aparameter%20quantity.%20As%20any%20new%20user-provided%20style%20come%2C%20our%20MuseumMaker%20can%0Acapture%20the%20nuances%20of%20the%20new%20styles%20while%20maintaining%20the%20details%20of%20learned%0Astyles.%20Experimental%20results%20on%20diverse%20style%20datasets%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20MuseumMaker%20method%2C%20showcasing%20its%20robustness%20and%0Aversatility%20across%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16612v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuseumMaker%3A%20Continual%20Style%20Customization%20without%20Catastrophic%0A%20%20Forgetting&entry.906535625=Chenxi%20Liu%20and%20Gan%20Sun%20and%20Wenqi%20Liang%20and%20Jiahua%20Dong%20and%20Can%20Qin%20and%20Yang%20Cong&entry.1292438233=%20%20Pre-trained%20large%20text-to-image%20%28T2I%29%20models%20with%20an%20appropriate%20text%20prompt%0Ahas%20attracted%20growing%20interests%20in%20customized%20images%20generation%20field.%20However%2C%0Acatastrophic%20forgetting%20issue%20make%20it%20hard%20to%20continually%20synthesize%20new%0Auser-provided%20styles%20while%20retaining%20the%20satisfying%20results%20amongst%20learned%0Astyles.%20In%20this%20paper%2C%20we%20propose%20MuseumMaker%2C%20a%20method%20that%20enables%20the%0Asynthesis%20of%20images%20by%20following%20a%20set%20of%20customized%20styles%20in%20a%20never-end%0Amanner%2C%20and%20gradually%20accumulate%20these%20creative%20artistic%20works%20as%20a%20Museum.%0AWhen%20facing%20with%20a%20new%20customization%20style%2C%20we%20develop%20a%20style%20distillation%0Aloss%20module%20to%20transfer%20the%20style%20of%20the%20whole%20dataset%20into%20generation%20of%0Aimages.%20It%20can%20minimize%20the%20learning%20biases%20caused%20by%20content%20of%20images%2C%20and%0Aaddress%20the%20catastrophic%20overfitting%20issue%20induced%20by%20few-shot%20images.%20To%20deal%0Awith%20catastrophic%20forgetting%20amongst%20past%20learned%20styles%2C%20we%20devise%20a%20dual%0Aregularization%20for%20shared-LoRA%20module%20to%20optimize%20the%20direction%20of%20model%0Aupdate%2C%20which%20could%20regularize%20the%20diffusion%20model%20from%20both%20weight%20and%20feature%0Aaspects%2C%20respectively.%20Meanwhile%2C%20a%20unique%20token%20embedding%20corresponding%20to%0Athis%20new%20style%20is%20learned%20by%20a%20task-wise%20token%20learning%20module%2C%20which%20could%0Apreserve%20historical%20knowledge%20from%20past%20styles%20with%20the%20limitation%20of%20LoRA%0Aparameter%20quantity.%20As%20any%20new%20user-provided%20style%20come%2C%20our%20MuseumMaker%20can%0Acapture%20the%20nuances%20of%20the%20new%20styles%20while%20maintaining%20the%20details%20of%20learned%0Astyles.%20Experimental%20results%20on%20diverse%20style%20datasets%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20MuseumMaker%20method%2C%20showcasing%20its%20robustness%20and%0Aversatility%20across%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16612v1&entry.124074799=Read"},
{"title": "GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with\n  Audio-Driven 3D Gaussian Splatting", "author": "Kyusun Cho and Joungbin Lee and Heeji Yoon and Yeobin Hong and Jaehoon Ko and Sangjun Ahn and Seungryong Kim", "abstract": "  We propose GaussianTalker, a novel framework for real-time generation of\npose-controllable talking heads. It leverages the fast rendering capabilities\nof 3D Gaussian Splatting (3DGS) while addressing the challenges of directly\ncontrolling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS\nrepresentation of the head and deforms it in sync with the audio. A key insight\nis to encode the 3D Gaussian attributes into a shared implicit feature\nrepresentation, where it is merged with audio features to manipulate each\nGaussian attribute. This design exploits the spatial-aware features and\nenforces interactions between neighboring points. The feature embeddings are\nthen fed to a spatial-audio attention module, which predicts frame-wise offsets\nfor the attributes of each Gaussian. It is more stable than previous\nconcatenation or multiplication approaches for manipulating the numerous\nGaussians and their intricate parameters. Experimental results showcase\nGaussianTalker's superiority in facial fidelity, lip synchronization accuracy,\nand rendering speed compared to previous methods. Specifically, GaussianTalker\nachieves a remarkable rendering speed up to 120 FPS, surpassing previous\nbenchmarks. Our code is made available at\nhttps://github.com/KU-CVLAB/GaussianTalker/ .\n", "link": "http://arxiv.org/abs/2404.16012v2", "date": "2024-04-25", "relevancy": 1.7013, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.628}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4955}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4865}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GaussianTalker%3A%20Real-Time%20High-Fidelity%20Talking%20Head%20Synthesis%20with%0A%20%20Audio-Driven%203D%20Gaussian%20Splatting&body=Title%3A%20GaussianTalker%3A%20Real-Time%20High-Fidelity%20Talking%20Head%20Synthesis%20with%0A%20%20Audio-Driven%203D%20Gaussian%20Splatting%0AAuthor%3A%20Kyusun%20Cho%20and%20Joungbin%20Lee%20and%20Heeji%20Yoon%20and%20Yeobin%20Hong%20and%20Jaehoon%20Ko%20and%20Sangjun%20Ahn%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20We%20propose%20GaussianTalker%2C%20a%20novel%20framework%20for%20real-time%20generation%20of%0Apose-controllable%20talking%20heads.%20It%20leverages%20the%20fast%20rendering%20capabilities%0Aof%203D%20Gaussian%20Splatting%20%283DGS%29%20while%20addressing%20the%20challenges%20of%20directly%0Acontrolling%203DGS%20with%20speech%20audio.%20GaussianTalker%20constructs%20a%20canonical%203DGS%0Arepresentation%20of%20the%20head%20and%20deforms%20it%20in%20sync%20with%20the%20audio.%20A%20key%20insight%0Ais%20to%20encode%20the%203D%20Gaussian%20attributes%20into%20a%20shared%20implicit%20feature%0Arepresentation%2C%20where%20it%20is%20merged%20with%20audio%20features%20to%20manipulate%20each%0AGaussian%20attribute.%20This%20design%20exploits%20the%20spatial-aware%20features%20and%0Aenforces%20interactions%20between%20neighboring%20points.%20The%20feature%20embeddings%20are%0Athen%20fed%20to%20a%20spatial-audio%20attention%20module%2C%20which%20predicts%20frame-wise%20offsets%0Afor%20the%20attributes%20of%20each%20Gaussian.%20It%20is%20more%20stable%20than%20previous%0Aconcatenation%20or%20multiplication%20approaches%20for%20manipulating%20the%20numerous%0AGaussians%20and%20their%20intricate%20parameters.%20Experimental%20results%20showcase%0AGaussianTalker%27s%20superiority%20in%20facial%20fidelity%2C%20lip%20synchronization%20accuracy%2C%0Aand%20rendering%20speed%20compared%20to%20previous%20methods.%20Specifically%2C%20GaussianTalker%0Aachieves%20a%20remarkable%20rendering%20speed%20up%20to%20120%20FPS%2C%20surpassing%20previous%0Abenchmarks.%20Our%20code%20is%20made%20available%20at%0Ahttps%3A//github.com/KU-CVLAB/GaussianTalker/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16012v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianTalker%3A%20Real-Time%20High-Fidelity%20Talking%20Head%20Synthesis%20with%0A%20%20Audio-Driven%203D%20Gaussian%20Splatting&entry.906535625=Kyusun%20Cho%20and%20Joungbin%20Lee%20and%20Heeji%20Yoon%20and%20Yeobin%20Hong%20and%20Jaehoon%20Ko%20and%20Sangjun%20Ahn%20and%20Seungryong%20Kim&entry.1292438233=%20%20We%20propose%20GaussianTalker%2C%20a%20novel%20framework%20for%20real-time%20generation%20of%0Apose-controllable%20talking%20heads.%20It%20leverages%20the%20fast%20rendering%20capabilities%0Aof%203D%20Gaussian%20Splatting%20%283DGS%29%20while%20addressing%20the%20challenges%20of%20directly%0Acontrolling%203DGS%20with%20speech%20audio.%20GaussianTalker%20constructs%20a%20canonical%203DGS%0Arepresentation%20of%20the%20head%20and%20deforms%20it%20in%20sync%20with%20the%20audio.%20A%20key%20insight%0Ais%20to%20encode%20the%203D%20Gaussian%20attributes%20into%20a%20shared%20implicit%20feature%0Arepresentation%2C%20where%20it%20is%20merged%20with%20audio%20features%20to%20manipulate%20each%0AGaussian%20attribute.%20This%20design%20exploits%20the%20spatial-aware%20features%20and%0Aenforces%20interactions%20between%20neighboring%20points.%20The%20feature%20embeddings%20are%0Athen%20fed%20to%20a%20spatial-audio%20attention%20module%2C%20which%20predicts%20frame-wise%20offsets%0Afor%20the%20attributes%20of%20each%20Gaussian.%20It%20is%20more%20stable%20than%20previous%0Aconcatenation%20or%20multiplication%20approaches%20for%20manipulating%20the%20numerous%0AGaussians%20and%20their%20intricate%20parameters.%20Experimental%20results%20showcase%0AGaussianTalker%27s%20superiority%20in%20facial%20fidelity%2C%20lip%20synchronization%20accuracy%2C%0Aand%20rendering%20speed%20compared%20to%20previous%20methods.%20Specifically%2C%20GaussianTalker%0Aachieves%20a%20remarkable%20rendering%20speed%20up%20to%20120%20FPS%2C%20surpassing%20previous%0Abenchmarks.%20Our%20code%20is%20made%20available%20at%0Ahttps%3A//github.com/KU-CVLAB/GaussianTalker/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16012v2&entry.124074799=Read"},
{"title": "Interpretable Clustering with the Distinguishability Criterion", "author": "Ali Turfah and Xiaoquan Wen", "abstract": "  Cluster analysis is a popular unsupervised learning tool used in many\ndisciplines to identify heterogeneous sub-populations within a sample. However,\nvalidating cluster analysis results and determining the number of clusters in a\ndata set remains an outstanding problem. In this work, we present a global\ncriterion called the Distinguishability criterion to quantify the separability\nof identified clusters and validate inferred cluster configurations. Our\ncomputational implementation of the Distinguishability criterion corresponds to\nthe Bayes risk of a randomized classifier under the 0-1 loss. We propose a\ncombined loss function-based computational framework that integrates the\nDistinguishability criterion with many commonly used clustering procedures,\nsuch as hierarchical clustering, k-means, and finite mixture models. We present\nthese new algorithms as well as the results from comprehensive data analysis\nbased on simulation studies and real data applications.\n", "link": "http://arxiv.org/abs/2404.15967v2", "date": "2024-04-25", "relevancy": 1.3122, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4591}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4042}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Clustering%20with%20the%20Distinguishability%20Criterion&body=Title%3A%20Interpretable%20Clustering%20with%20the%20Distinguishability%20Criterion%0AAuthor%3A%20Ali%20Turfah%20and%20Xiaoquan%20Wen%0AAbstract%3A%20%20%20Cluster%20analysis%20is%20a%20popular%20unsupervised%20learning%20tool%20used%20in%20many%0Adisciplines%20to%20identify%20heterogeneous%20sub-populations%20within%20a%20sample.%20However%2C%0Avalidating%20cluster%20analysis%20results%20and%20determining%20the%20number%20of%20clusters%20in%20a%0Adata%20set%20remains%20an%20outstanding%20problem.%20In%20this%20work%2C%20we%20present%20a%20global%0Acriterion%20called%20the%20Distinguishability%20criterion%20to%20quantify%20the%20separability%0Aof%20identified%20clusters%20and%20validate%20inferred%20cluster%20configurations.%20Our%0Acomputational%20implementation%20of%20the%20Distinguishability%20criterion%20corresponds%20to%0Athe%20Bayes%20risk%20of%20a%20randomized%20classifier%20under%20the%200-1%20loss.%20We%20propose%20a%0Acombined%20loss%20function-based%20computational%20framework%20that%20integrates%20the%0ADistinguishability%20criterion%20with%20many%20commonly%20used%20clustering%20procedures%2C%0Asuch%20as%20hierarchical%20clustering%2C%20k-means%2C%20and%20finite%20mixture%20models.%20We%20present%0Athese%20new%20algorithms%20as%20well%20as%20the%20results%20from%20comprehensive%20data%20analysis%0Abased%20on%20simulation%20studies%20and%20real%20data%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15967v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Clustering%20with%20the%20Distinguishability%20Criterion&entry.906535625=Ali%20Turfah%20and%20Xiaoquan%20Wen&entry.1292438233=%20%20Cluster%20analysis%20is%20a%20popular%20unsupervised%20learning%20tool%20used%20in%20many%0Adisciplines%20to%20identify%20heterogeneous%20sub-populations%20within%20a%20sample.%20However%2C%0Avalidating%20cluster%20analysis%20results%20and%20determining%20the%20number%20of%20clusters%20in%20a%0Adata%20set%20remains%20an%20outstanding%20problem.%20In%20this%20work%2C%20we%20present%20a%20global%0Acriterion%20called%20the%20Distinguishability%20criterion%20to%20quantify%20the%20separability%0Aof%20identified%20clusters%20and%20validate%20inferred%20cluster%20configurations.%20Our%0Acomputational%20implementation%20of%20the%20Distinguishability%20criterion%20corresponds%20to%0Athe%20Bayes%20risk%20of%20a%20randomized%20classifier%20under%20the%200-1%20loss.%20We%20propose%20a%0Acombined%20loss%20function-based%20computational%20framework%20that%20integrates%20the%0ADistinguishability%20criterion%20with%20many%20commonly%20used%20clustering%20procedures%2C%0Asuch%20as%20hierarchical%20clustering%2C%20k-means%2C%20and%20finite%20mixture%20models.%20We%20present%0Athese%20new%20algorithms%20as%20well%20as%20the%20results%20from%20comprehensive%20data%20analysis%0Abased%20on%20simulation%20studies%20and%20real%20data%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15967v2&entry.124074799=Read"},
{"title": "Application of Long-Short Term Memory and Convolutional Neural Networks\n  for Real-Time Bridge Scour Forecast", "author": "Tahrima Hashem and Negin Yousefpour", "abstract": "  Scour around bridge piers is a critical challenge for infrastructures around\nthe world. In the absence of analytical models and due to the complexity of the\nscour process, it is difficult for current empirical methods to achieve\naccurate predictions. In this paper, we exploit the power of deep learning\nalgorithms to forecast the scour depth variations around bridge piers based on\nhistorical sensor monitoring data, including riverbed elevation, flow\nelevation, and flow velocity. We investigated the performance of Long\nShort-Term Memory (LSTM) and Convolutional Neural Network (CNN) models for\nreal-time scour forecasting using data collected from bridges in Alaska and\nOregon from 2006 to 2021. The LSTM models achieved mean absolute error (MAE)\nranging from 0.1m to 0.5m for predicting bed level variations a week in\nadvance, showing a reasonable performance. The Fully Convolutional Network\n(FCN) variant of CNN outperformed other CNN configurations, showing a\ncomparable performance to LSTMs with significantly lower computational costs.\nWe explored various innovative random-search heuristics for hyperparameter\ntuning and model optimisation which resulted in reduced computational cost\ncompared to grid-search method. The impact of different combinations of sensor\nfeatures on scour prediction showed the significance of the historical time\nseries of scour for predicting upcoming events. Overall, this study provides a\ngreater understanding of the potential of Deep Learning (DL) for real-time\nscour forecasting and early warning in bridges with diverse scour and flow\ncharacteristics including riverine and tidal/coastal bridges.\n", "link": "http://arxiv.org/abs/2404.16549v1", "date": "2024-04-25", "relevancy": 1.4644, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4961}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4885}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4848}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Long-Short%20Term%20Memory%20and%20Convolutional%20Neural%20Networks%0A%20%20for%20Real-Time%20Bridge%20Scour%20Forecast&body=Title%3A%20Application%20of%20Long-Short%20Term%20Memory%20and%20Convolutional%20Neural%20Networks%0A%20%20for%20Real-Time%20Bridge%20Scour%20Forecast%0AAuthor%3A%20Tahrima%20Hashem%20and%20Negin%20Yousefpour%0AAbstract%3A%20%20%20Scour%20around%20bridge%20piers%20is%20a%20critical%20challenge%20for%20infrastructures%20around%0Athe%20world.%20In%20the%20absence%20of%20analytical%20models%20and%20due%20to%20the%20complexity%20of%20the%0Ascour%20process%2C%20it%20is%20difficult%20for%20current%20empirical%20methods%20to%20achieve%0Aaccurate%20predictions.%20In%20this%20paper%2C%20we%20exploit%20the%20power%20of%20deep%20learning%0Aalgorithms%20to%20forecast%20the%20scour%20depth%20variations%20around%20bridge%20piers%20based%20on%0Ahistorical%20sensor%20monitoring%20data%2C%20including%20riverbed%20elevation%2C%20flow%0Aelevation%2C%20and%20flow%20velocity.%20We%20investigated%20the%20performance%20of%20Long%0AShort-Term%20Memory%20%28LSTM%29%20and%20Convolutional%20Neural%20Network%20%28CNN%29%20models%20for%0Areal-time%20scour%20forecasting%20using%20data%20collected%20from%20bridges%20in%20Alaska%20and%0AOregon%20from%202006%20to%202021.%20The%20LSTM%20models%20achieved%20mean%20absolute%20error%20%28MAE%29%0Aranging%20from%200.1m%20to%200.5m%20for%20predicting%20bed%20level%20variations%20a%20week%20in%0Aadvance%2C%20showing%20a%20reasonable%20performance.%20The%20Fully%20Convolutional%20Network%0A%28FCN%29%20variant%20of%20CNN%20outperformed%20other%20CNN%20configurations%2C%20showing%20a%0Acomparable%20performance%20to%20LSTMs%20with%20significantly%20lower%20computational%20costs.%0AWe%20explored%20various%20innovative%20random-search%20heuristics%20for%20hyperparameter%0Atuning%20and%20model%20optimisation%20which%20resulted%20in%20reduced%20computational%20cost%0Acompared%20to%20grid-search%20method.%20The%20impact%20of%20different%20combinations%20of%20sensor%0Afeatures%20on%20scour%20prediction%20showed%20the%20significance%20of%20the%20historical%20time%0Aseries%20of%20scour%20for%20predicting%20upcoming%20events.%20Overall%2C%20this%20study%20provides%20a%0Agreater%20understanding%20of%20the%20potential%20of%20Deep%20Learning%20%28DL%29%20for%20real-time%0Ascour%20forecasting%20and%20early%20warning%20in%20bridges%20with%20diverse%20scour%20and%20flow%0Acharacteristics%20including%20riverine%20and%20tidal/coastal%20bridges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16549v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Long-Short%20Term%20Memory%20and%20Convolutional%20Neural%20Networks%0A%20%20for%20Real-Time%20Bridge%20Scour%20Forecast&entry.906535625=Tahrima%20Hashem%20and%20Negin%20Yousefpour&entry.1292438233=%20%20Scour%20around%20bridge%20piers%20is%20a%20critical%20challenge%20for%20infrastructures%20around%0Athe%20world.%20In%20the%20absence%20of%20analytical%20models%20and%20due%20to%20the%20complexity%20of%20the%0Ascour%20process%2C%20it%20is%20difficult%20for%20current%20empirical%20methods%20to%20achieve%0Aaccurate%20predictions.%20In%20this%20paper%2C%20we%20exploit%20the%20power%20of%20deep%20learning%0Aalgorithms%20to%20forecast%20the%20scour%20depth%20variations%20around%20bridge%20piers%20based%20on%0Ahistorical%20sensor%20monitoring%20data%2C%20including%20riverbed%20elevation%2C%20flow%0Aelevation%2C%20and%20flow%20velocity.%20We%20investigated%20the%20performance%20of%20Long%0AShort-Term%20Memory%20%28LSTM%29%20and%20Convolutional%20Neural%20Network%20%28CNN%29%20models%20for%0Areal-time%20scour%20forecasting%20using%20data%20collected%20from%20bridges%20in%20Alaska%20and%0AOregon%20from%202006%20to%202021.%20The%20LSTM%20models%20achieved%20mean%20absolute%20error%20%28MAE%29%0Aranging%20from%200.1m%20to%200.5m%20for%20predicting%20bed%20level%20variations%20a%20week%20in%0Aadvance%2C%20showing%20a%20reasonable%20performance.%20The%20Fully%20Convolutional%20Network%0A%28FCN%29%20variant%20of%20CNN%20outperformed%20other%20CNN%20configurations%2C%20showing%20a%0Acomparable%20performance%20to%20LSTMs%20with%20significantly%20lower%20computational%20costs.%0AWe%20explored%20various%20innovative%20random-search%20heuristics%20for%20hyperparameter%0Atuning%20and%20model%20optimisation%20which%20resulted%20in%20reduced%20computational%20cost%0Acompared%20to%20grid-search%20method.%20The%20impact%20of%20different%20combinations%20of%20sensor%0Afeatures%20on%20scour%20prediction%20showed%20the%20significance%20of%20the%20historical%20time%0Aseries%20of%20scour%20for%20predicting%20upcoming%20events.%20Overall%2C%20this%20study%20provides%20a%0Agreater%20understanding%20of%20the%20potential%20of%20Deep%20Learning%20%28DL%29%20for%20real-time%0Ascour%20forecasting%20and%20early%20warning%20in%20bridges%20with%20diverse%20scour%20and%20flow%0Acharacteristics%20including%20riverine%20and%20tidal/coastal%20bridges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16549v1&entry.124074799=Read"},
{"title": "Probabilistic Multi-Layer Perceptrons for Wind Farm Condition Monitoring", "author": "Filippo Fiocchi and Domna Ladopoulou and Petros Dellaportas", "abstract": "  We provide a condition monitoring system for wind farms, based on normal\nbehaviour modelling using a probabilistic multi-layer perceptron with transfer\nlearning via fine-tuning. The model predicts the output power of the wind\nturbine under normal behaviour based on features retrieved from supervisory\ncontrol and data acquisition (SCADA) systems. Its advantages are that (i) it\ncan be trained with SCADA data of at least a few years, (ii) it can incorporate\nall SCADA data of all wind turbines in a wind farm as features, (iii) it\nassumes that the output power follows a normal density with heteroscedastic\nvariance and (iv) it can predict the output of one wind turbine by borrowing\nstrength from the data of all other wind turbines in a farm. Probabilistic\nguidelines for condition monitoring are given via a CUSUM control chart. We\nillustrate the performance of our model in a real SCADA data example which\nprovides evidence that it outperforms other probabilistic prediction models.\n", "link": "http://arxiv.org/abs/2404.16496v1", "date": "2024-04-25", "relevancy": 1.4188, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4668}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4483}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Multi-Layer%20Perceptrons%20for%20Wind%20Farm%20Condition%20Monitoring&body=Title%3A%20Probabilistic%20Multi-Layer%20Perceptrons%20for%20Wind%20Farm%20Condition%20Monitoring%0AAuthor%3A%20Filippo%20Fiocchi%20and%20Domna%20Ladopoulou%20and%20Petros%20Dellaportas%0AAbstract%3A%20%20%20We%20provide%20a%20condition%20monitoring%20system%20for%20wind%20farms%2C%20based%20on%20normal%0Abehaviour%20modelling%20using%20a%20probabilistic%20multi-layer%20perceptron%20with%20transfer%0Alearning%20via%20fine-tuning.%20The%20model%20predicts%20the%20output%20power%20of%20the%20wind%0Aturbine%20under%20normal%20behaviour%20based%20on%20features%20retrieved%20from%20supervisory%0Acontrol%20and%20data%20acquisition%20%28SCADA%29%20systems.%20Its%20advantages%20are%20that%20%28i%29%20it%0Acan%20be%20trained%20with%20SCADA%20data%20of%20at%20least%20a%20few%20years%2C%20%28ii%29%20it%20can%20incorporate%0Aall%20SCADA%20data%20of%20all%20wind%20turbines%20in%20a%20wind%20farm%20as%20features%2C%20%28iii%29%20it%0Aassumes%20that%20the%20output%20power%20follows%20a%20normal%20density%20with%20heteroscedastic%0Avariance%20and%20%28iv%29%20it%20can%20predict%20the%20output%20of%20one%20wind%20turbine%20by%20borrowing%0Astrength%20from%20the%20data%20of%20all%20other%20wind%20turbines%20in%20a%20farm.%20Probabilistic%0Aguidelines%20for%20condition%20monitoring%20are%20given%20via%20a%20CUSUM%20control%20chart.%20We%0Aillustrate%20the%20performance%20of%20our%20model%20in%20a%20real%20SCADA%20data%20example%20which%0Aprovides%20evidence%20that%20it%20outperforms%20other%20probabilistic%20prediction%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16496v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Multi-Layer%20Perceptrons%20for%20Wind%20Farm%20Condition%20Monitoring&entry.906535625=Filippo%20Fiocchi%20and%20Domna%20Ladopoulou%20and%20Petros%20Dellaportas&entry.1292438233=%20%20We%20provide%20a%20condition%20monitoring%20system%20for%20wind%20farms%2C%20based%20on%20normal%0Abehaviour%20modelling%20using%20a%20probabilistic%20multi-layer%20perceptron%20with%20transfer%0Alearning%20via%20fine-tuning.%20The%20model%20predicts%20the%20output%20power%20of%20the%20wind%0Aturbine%20under%20normal%20behaviour%20based%20on%20features%20retrieved%20from%20supervisory%0Acontrol%20and%20data%20acquisition%20%28SCADA%29%20systems.%20Its%20advantages%20are%20that%20%28i%29%20it%0Acan%20be%20trained%20with%20SCADA%20data%20of%20at%20least%20a%20few%20years%2C%20%28ii%29%20it%20can%20incorporate%0Aall%20SCADA%20data%20of%20all%20wind%20turbines%20in%20a%20wind%20farm%20as%20features%2C%20%28iii%29%20it%0Aassumes%20that%20the%20output%20power%20follows%20a%20normal%20density%20with%20heteroscedastic%0Avariance%20and%20%28iv%29%20it%20can%20predict%20the%20output%20of%20one%20wind%20turbine%20by%20borrowing%0Astrength%20from%20the%20data%20of%20all%20other%20wind%20turbines%20in%20a%20farm.%20Probabilistic%0Aguidelines%20for%20condition%20monitoring%20are%20given%20via%20a%20CUSUM%20control%20chart.%20We%0Aillustrate%20the%20performance%20of%20our%20model%20in%20a%20real%20SCADA%20data%20example%20which%0Aprovides%20evidence%20that%20it%20outperforms%20other%20probabilistic%20prediction%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16496v1&entry.124074799=Read"},
{"title": "Privacy-Preserving Statistical Data Generation: Application to Sepsis\n  Detection", "author": "Eric Macias-Fassio and Aythami Morales and Cristina Pruenza and Julian Fierrez", "abstract": "  The biomedical field is among the sectors most impacted by the increasing\nregulation of Artificial Intelligence (AI) and data protection legislation,\ngiven the sensitivity of patient information. However, the rise of synthetic\ndata generation methods offers a promising opportunity for data-driven\ntechnologies. In this study, we propose a statistical approach for synthetic\ndata generation applicable in classification problems. We assess the utility\nand privacy implications of synthetic data generated by Kernel Density\nEstimator and K-Nearest Neighbors sampling (KDE-KNN) within a real-world\ncontext, specifically focusing on its application in sepsis detection. The\ndetection of sepsis is a critical challenge in clinical practice due to its\nrapid progression and potentially life-threatening consequences. Moreover, we\nemphasize the benefits of KDE-KNN compared to current synthetic data generation\nmethodologies. Additionally, our study examines the effects of incorporating\nsynthetic data into model training procedures. This investigation provides\nvaluable insights into the effectiveness of synthetic data generation\ntechniques in mitigating regulatory constraints within the biomedical field.\n", "link": "http://arxiv.org/abs/2404.16638v1", "date": "2024-04-25", "relevancy": 1.3518, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4541}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4476}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Privacy-Preserving%20Statistical%20Data%20Generation%3A%20Application%20to%20Sepsis%0A%20%20Detection&body=Title%3A%20Privacy-Preserving%20Statistical%20Data%20Generation%3A%20Application%20to%20Sepsis%0A%20%20Detection%0AAuthor%3A%20Eric%20Macias-Fassio%20and%20Aythami%20Morales%20and%20Cristina%20Pruenza%20and%20Julian%20Fierrez%0AAbstract%3A%20%20%20The%20biomedical%20field%20is%20among%20the%20sectors%20most%20impacted%20by%20the%20increasing%0Aregulation%20of%20Artificial%20Intelligence%20%28AI%29%20and%20data%20protection%20legislation%2C%0Agiven%20the%20sensitivity%20of%20patient%20information.%20However%2C%20the%20rise%20of%20synthetic%0Adata%20generation%20methods%20offers%20a%20promising%20opportunity%20for%20data-driven%0Atechnologies.%20In%20this%20study%2C%20we%20propose%20a%20statistical%20approach%20for%20synthetic%0Adata%20generation%20applicable%20in%20classification%20problems.%20We%20assess%20the%20utility%0Aand%20privacy%20implications%20of%20synthetic%20data%20generated%20by%20Kernel%20Density%0AEstimator%20and%20K-Nearest%20Neighbors%20sampling%20%28KDE-KNN%29%20within%20a%20real-world%0Acontext%2C%20specifically%20focusing%20on%20its%20application%20in%20sepsis%20detection.%20The%0Adetection%20of%20sepsis%20is%20a%20critical%20challenge%20in%20clinical%20practice%20due%20to%20its%0Arapid%20progression%20and%20potentially%20life-threatening%20consequences.%20Moreover%2C%20we%0Aemphasize%20the%20benefits%20of%20KDE-KNN%20compared%20to%20current%20synthetic%20data%20generation%0Amethodologies.%20Additionally%2C%20our%20study%20examines%20the%20effects%20of%20incorporating%0Asynthetic%20data%20into%20model%20training%20procedures.%20This%20investigation%20provides%0Avaluable%20insights%20into%20the%20effectiveness%20of%20synthetic%20data%20generation%0Atechniques%20in%20mitigating%20regulatory%20constraints%20within%20the%20biomedical%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16638v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-Preserving%20Statistical%20Data%20Generation%3A%20Application%20to%20Sepsis%0A%20%20Detection&entry.906535625=Eric%20Macias-Fassio%20and%20Aythami%20Morales%20and%20Cristina%20Pruenza%20and%20Julian%20Fierrez&entry.1292438233=%20%20The%20biomedical%20field%20is%20among%20the%20sectors%20most%20impacted%20by%20the%20increasing%0Aregulation%20of%20Artificial%20Intelligence%20%28AI%29%20and%20data%20protection%20legislation%2C%0Agiven%20the%20sensitivity%20of%20patient%20information.%20However%2C%20the%20rise%20of%20synthetic%0Adata%20generation%20methods%20offers%20a%20promising%20opportunity%20for%20data-driven%0Atechnologies.%20In%20this%20study%2C%20we%20propose%20a%20statistical%20approach%20for%20synthetic%0Adata%20generation%20applicable%20in%20classification%20problems.%20We%20assess%20the%20utility%0Aand%20privacy%20implications%20of%20synthetic%20data%20generated%20by%20Kernel%20Density%0AEstimator%20and%20K-Nearest%20Neighbors%20sampling%20%28KDE-KNN%29%20within%20a%20real-world%0Acontext%2C%20specifically%20focusing%20on%20its%20application%20in%20sepsis%20detection.%20The%0Adetection%20of%20sepsis%20is%20a%20critical%20challenge%20in%20clinical%20practice%20due%20to%20its%0Arapid%20progression%20and%20potentially%20life-threatening%20consequences.%20Moreover%2C%20we%0Aemphasize%20the%20benefits%20of%20KDE-KNN%20compared%20to%20current%20synthetic%20data%20generation%0Amethodologies.%20Additionally%2C%20our%20study%20examines%20the%20effects%20of%20incorporating%0Asynthetic%20data%20into%20model%20training%20procedures.%20This%20investigation%20provides%0Avaluable%20insights%20into%20the%20effectiveness%20of%20synthetic%20data%20generation%0Atechniques%20in%20mitigating%20regulatory%20constraints%20within%20the%20biomedical%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16638v1&entry.124074799=Read"},
{"title": "Automated Model Selection for Generalized Linear Models", "author": "Benjamin Schwendinger and Florian Schwendinger and Laura Vana-G\u00fcr", "abstract": "  In this paper, we show how mixed-integer conic optimization can be used to\ncombine feature subset selection with holistic generalized linear models to\nfully automate the model selection process. Concretely, we directly optimize\nfor the Akaike and Bayesian information criteria while imposing constraints\ndesigned to deal with multicollinearity in the feature selection task.\nSpecifically, we propose a novel pairwise correlation constraint that combines\nthe sign coherence constraint with ideas from classical statistical models like\nRidge regression and the OSCAR model.\n", "link": "http://arxiv.org/abs/2404.16560v1", "date": "2024-04-25", "relevancy": 1.6796, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4417}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4234}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4076}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automated%20Model%20Selection%20for%20Generalized%20Linear%20Models&body=Title%3A%20Automated%20Model%20Selection%20for%20Generalized%20Linear%20Models%0AAuthor%3A%20Benjamin%20Schwendinger%20and%20Florian%20Schwendinger%20and%20Laura%20Vana-G%C3%BCr%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20show%20how%20mixed-integer%20conic%20optimization%20can%20be%20used%20to%0Acombine%20feature%20subset%20selection%20with%20holistic%20generalized%20linear%20models%20to%0Afully%20automate%20the%20model%20selection%20process.%20Concretely%2C%20we%20directly%20optimize%0Afor%20the%20Akaike%20and%20Bayesian%20information%20criteria%20while%20imposing%20constraints%0Adesigned%20to%20deal%20with%20multicollinearity%20in%20the%20feature%20selection%20task.%0ASpecifically%2C%20we%20propose%20a%20novel%20pairwise%20correlation%20constraint%20that%20combines%0Athe%20sign%20coherence%20constraint%20with%20ideas%20from%20classical%20statistical%20models%20like%0ARidge%20regression%20and%20the%20OSCAR%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16560v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Model%20Selection%20for%20Generalized%20Linear%20Models&entry.906535625=Benjamin%20Schwendinger%20and%20Florian%20Schwendinger%20and%20Laura%20Vana-G%C3%BCr&entry.1292438233=%20%20In%20this%20paper%2C%20we%20show%20how%20mixed-integer%20conic%20optimization%20can%20be%20used%20to%0Acombine%20feature%20subset%20selection%20with%20holistic%20generalized%20linear%20models%20to%0Afully%20automate%20the%20model%20selection%20process.%20Concretely%2C%20we%20directly%20optimize%0Afor%20the%20Akaike%20and%20Bayesian%20information%20criteria%20while%20imposing%20constraints%0Adesigned%20to%20deal%20with%20multicollinearity%20in%20the%20feature%20selection%20task.%0ASpecifically%2C%20we%20propose%20a%20novel%20pairwise%20correlation%20constraint%20that%20combines%0Athe%20sign%20coherence%20constraint%20with%20ideas%20from%20classical%20statistical%20models%20like%0ARidge%20regression%20and%20the%20OSCAR%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16560v1&entry.124074799=Read"},
{"title": "Unraveling the Enigma of Double Descent: An In-depth Analysis through\n  the Lens of Learned Feature Space", "author": "Yufei Gu and Xiaoqing Zheng and Tomaso Aste", "abstract": "  Double descent presents a counter-intuitive aspect within the machine\nlearning domain, and researchers have observed its manifestation in various\nmodels and tasks. While some theoretical explanations have been proposed for\nthis phenomenon in specific contexts, an accepted theory to account for its\noccurrence in deep learning remains yet to be established. In this study, we\nrevisit the phenomenon of double descent and demonstrate that its occurrence is\nstrongly influenced by the presence of noisy data. Through conducting a\ncomprehensive analysis of the feature space of learned representations, we\nunveil that double descent arises in imperfect models trained with noisy data.\nWe argue that double descent is a consequence of the model first learning the\nnoisy data until interpolation and then adding implicit regularization via\nover-parameterization acquiring therefore capability to separate the\ninformation from the noise.\n", "link": "http://arxiv.org/abs/2310.13572v3", "date": "2024-04-25", "relevancy": 1.4869, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5005}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4966}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4933}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unraveling%20the%20Enigma%20of%20Double%20Descent%3A%20An%20In-depth%20Analysis%20through%0A%20%20the%20Lens%20of%20Learned%20Feature%20Space&body=Title%3A%20Unraveling%20the%20Enigma%20of%20Double%20Descent%3A%20An%20In-depth%20Analysis%20through%0A%20%20the%20Lens%20of%20Learned%20Feature%20Space%0AAuthor%3A%20Yufei%20Gu%20and%20Xiaoqing%20Zheng%20and%20Tomaso%20Aste%0AAbstract%3A%20%20%20Double%20descent%20presents%20a%20counter-intuitive%20aspect%20within%20the%20machine%0Alearning%20domain%2C%20and%20researchers%20have%20observed%20its%20manifestation%20in%20various%0Amodels%20and%20tasks.%20While%20some%20theoretical%20explanations%20have%20been%20proposed%20for%0Athis%20phenomenon%20in%20specific%20contexts%2C%20an%20accepted%20theory%20to%20account%20for%20its%0Aoccurrence%20in%20deep%20learning%20remains%20yet%20to%20be%20established.%20In%20this%20study%2C%20we%0Arevisit%20the%20phenomenon%20of%20double%20descent%20and%20demonstrate%20that%20its%20occurrence%20is%0Astrongly%20influenced%20by%20the%20presence%20of%20noisy%20data.%20Through%20conducting%20a%0Acomprehensive%20analysis%20of%20the%20feature%20space%20of%20learned%20representations%2C%20we%0Aunveil%20that%20double%20descent%20arises%20in%20imperfect%20models%20trained%20with%20noisy%20data.%0AWe%20argue%20that%20double%20descent%20is%20a%20consequence%20of%20the%20model%20first%20learning%20the%0Anoisy%20data%20until%20interpolation%20and%20then%20adding%20implicit%20regularization%20via%0Aover-parameterization%20acquiring%20therefore%20capability%20to%20separate%20the%0Ainformation%20from%20the%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13572v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20the%20Enigma%20of%20Double%20Descent%3A%20An%20In-depth%20Analysis%20through%0A%20%20the%20Lens%20of%20Learned%20Feature%20Space&entry.906535625=Yufei%20Gu%20and%20Xiaoqing%20Zheng%20and%20Tomaso%20Aste&entry.1292438233=%20%20Double%20descent%20presents%20a%20counter-intuitive%20aspect%20within%20the%20machine%0Alearning%20domain%2C%20and%20researchers%20have%20observed%20its%20manifestation%20in%20various%0Amodels%20and%20tasks.%20While%20some%20theoretical%20explanations%20have%20been%20proposed%20for%0Athis%20phenomenon%20in%20specific%20contexts%2C%20an%20accepted%20theory%20to%20account%20for%20its%0Aoccurrence%20in%20deep%20learning%20remains%20yet%20to%20be%20established.%20In%20this%20study%2C%20we%0Arevisit%20the%20phenomenon%20of%20double%20descent%20and%20demonstrate%20that%20its%20occurrence%20is%0Astrongly%20influenced%20by%20the%20presence%20of%20noisy%20data.%20Through%20conducting%20a%0Acomprehensive%20analysis%20of%20the%20feature%20space%20of%20learned%20representations%2C%20we%0Aunveil%20that%20double%20descent%20arises%20in%20imperfect%20models%20trained%20with%20noisy%20data.%0AWe%20argue%20that%20double%20descent%20is%20a%20consequence%20of%20the%20model%20first%20learning%20the%0Anoisy%20data%20until%20interpolation%20and%20then%20adding%20implicit%20regularization%20via%0Aover-parameterization%20acquiring%20therefore%20capability%20to%20separate%20the%0Ainformation%20from%20the%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13572v3&entry.124074799=Read"},
{"title": "Global Concept Explanations for Graphs by Contrastive Learning", "author": "Jonas Teufel and Pascal Friederich", "abstract": "  Beyond improving trust and validating model fairness, xAI practices also have\nthe potential to recover valuable scientific insights in application domains\nwhere little to no prior human intuition exists. To that end, we propose a\nmethod to extract global concept explanations from the predictions of graph\nneural networks to develop a deeper understanding of the tasks underlying\nstructure-property relationships. We identify concept explanations as dense\nclusters in the self-explaining Megan models subgraph latent space. For each\nconcept, we optimize a representative prototype graph and optionally use GPT-4\nto provide hypotheses about why each structure has a certain effect on the\nprediction. We conduct computational experiments on synthetic and real-world\ngraph property prediction tasks. For the synthetic tasks we find that our\nmethod correctly reproduces the structural rules by which they were created.\nFor real-world molecular property regression and classification tasks, we find\nthat our method rediscovers established rules of thumb. More specifically, our\nresults for molecular mutagenicity prediction indicate more fine-grained\nresolution of structural details than existing explainability methods,\nconsistent with previous results from chemistry literature. Overall, our\nresults show promising capability to extract the underlying structure-property\nrelationships for complex graph property prediction tasks.\n", "link": "http://arxiv.org/abs/2404.16532v1", "date": "2024-04-25", "relevancy": 1.5537, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5267}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5092}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Global%20Concept%20Explanations%20for%20Graphs%20by%20Contrastive%20Learning&body=Title%3A%20Global%20Concept%20Explanations%20for%20Graphs%20by%20Contrastive%20Learning%0AAuthor%3A%20Jonas%20Teufel%20and%20Pascal%20Friederich%0AAbstract%3A%20%20%20Beyond%20improving%20trust%20and%20validating%20model%20fairness%2C%20xAI%20practices%20also%20have%0Athe%20potential%20to%20recover%20valuable%20scientific%20insights%20in%20application%20domains%0Awhere%20little%20to%20no%20prior%20human%20intuition%20exists.%20To%20that%20end%2C%20we%20propose%20a%0Amethod%20to%20extract%20global%20concept%20explanations%20from%20the%20predictions%20of%20graph%0Aneural%20networks%20to%20develop%20a%20deeper%20understanding%20of%20the%20tasks%20underlying%0Astructure-property%20relationships.%20We%20identify%20concept%20explanations%20as%20dense%0Aclusters%20in%20the%20self-explaining%20Megan%20models%20subgraph%20latent%20space.%20For%20each%0Aconcept%2C%20we%20optimize%20a%20representative%20prototype%20graph%20and%20optionally%20use%20GPT-4%0Ato%20provide%20hypotheses%20about%20why%20each%20structure%20has%20a%20certain%20effect%20on%20the%0Aprediction.%20We%20conduct%20computational%20experiments%20on%20synthetic%20and%20real-world%0Agraph%20property%20prediction%20tasks.%20For%20the%20synthetic%20tasks%20we%20find%20that%20our%0Amethod%20correctly%20reproduces%20the%20structural%20rules%20by%20which%20they%20were%20created.%0AFor%20real-world%20molecular%20property%20regression%20and%20classification%20tasks%2C%20we%20find%0Athat%20our%20method%20rediscovers%20established%20rules%20of%20thumb.%20More%20specifically%2C%20our%0Aresults%20for%20molecular%20mutagenicity%20prediction%20indicate%20more%20fine-grained%0Aresolution%20of%20structural%20details%20than%20existing%20explainability%20methods%2C%0Aconsistent%20with%20previous%20results%20from%20chemistry%20literature.%20Overall%2C%20our%0Aresults%20show%20promising%20capability%20to%20extract%20the%20underlying%20structure-property%0Arelationships%20for%20complex%20graph%20property%20prediction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16532v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Concept%20Explanations%20for%20Graphs%20by%20Contrastive%20Learning&entry.906535625=Jonas%20Teufel%20and%20Pascal%20Friederich&entry.1292438233=%20%20Beyond%20improving%20trust%20and%20validating%20model%20fairness%2C%20xAI%20practices%20also%20have%0Athe%20potential%20to%20recover%20valuable%20scientific%20insights%20in%20application%20domains%0Awhere%20little%20to%20no%20prior%20human%20intuition%20exists.%20To%20that%20end%2C%20we%20propose%20a%0Amethod%20to%20extract%20global%20concept%20explanations%20from%20the%20predictions%20of%20graph%0Aneural%20networks%20to%20develop%20a%20deeper%20understanding%20of%20the%20tasks%20underlying%0Astructure-property%20relationships.%20We%20identify%20concept%20explanations%20as%20dense%0Aclusters%20in%20the%20self-explaining%20Megan%20models%20subgraph%20latent%20space.%20For%20each%0Aconcept%2C%20we%20optimize%20a%20representative%20prototype%20graph%20and%20optionally%20use%20GPT-4%0Ato%20provide%20hypotheses%20about%20why%20each%20structure%20has%20a%20certain%20effect%20on%20the%0Aprediction.%20We%20conduct%20computational%20experiments%20on%20synthetic%20and%20real-world%0Agraph%20property%20prediction%20tasks.%20For%20the%20synthetic%20tasks%20we%20find%20that%20our%0Amethod%20correctly%20reproduces%20the%20structural%20rules%20by%20which%20they%20were%20created.%0AFor%20real-world%20molecular%20property%20regression%20and%20classification%20tasks%2C%20we%20find%0Athat%20our%20method%20rediscovers%20established%20rules%20of%20thumb.%20More%20specifically%2C%20our%0Aresults%20for%20molecular%20mutagenicity%20prediction%20indicate%20more%20fine-grained%0Aresolution%20of%20structural%20details%20than%20existing%20explainability%20methods%2C%0Aconsistent%20with%20previous%20results%20from%20chemistry%20literature.%20Overall%2C%20our%0Aresults%20show%20promising%20capability%20to%20extract%20the%20underlying%20structure-property%0Arelationships%20for%20complex%20graph%20property%20prediction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16532v1&entry.124074799=Read"},
{"title": "Vision-based robot manipulation of transparent liquid containers in a\n  laboratory setting", "author": "Daniel Schober and Ronja G\u00fcldenring and James Love and Lazaros Nalpantidis", "abstract": "  Laboratory processes involving small volumes of solutions and active\ningredients are often performed manually due to challenges in automation, such\nas high initial costs, semi-structured environments and protocol variability.\nIn this work, we develop a flexible and cost-effective approach to address this\ngap by introducing a vision-based system for liquid volume estimation and a\nsimulation-driven pouring method particularly designed for containers with\nsmall openings. We evaluate both components individually, followed by an\napplied real-world integration of cell culture automation using a UR5 robotic\narm. Our work is fully reproducible: we share our code at at\n\\url{https://github.com/DaniSchober/LabLiquidVision} and the newly introduced\ndataset LabLiquidVolume is available at\nhttps://data.dtu.dk/articles/dataset/LabLiquidVision/25103102.\n", "link": "http://arxiv.org/abs/2404.16529v1", "date": "2024-04-25", "relevancy": 1.5179, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5347}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5011}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4965}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Vision-based%20robot%20manipulation%20of%20transparent%20liquid%20containers%20in%20a%0A%20%20laboratory%20setting&body=Title%3A%20Vision-based%20robot%20manipulation%20of%20transparent%20liquid%20containers%20in%20a%0A%20%20laboratory%20setting%0AAuthor%3A%20Daniel%20Schober%20and%20Ronja%20G%C3%BCldenring%20and%20James%20Love%20and%20Lazaros%20Nalpantidis%0AAbstract%3A%20%20%20Laboratory%20processes%20involving%20small%20volumes%20of%20solutions%20and%20active%0Aingredients%20are%20often%20performed%20manually%20due%20to%20challenges%20in%20automation%2C%20such%0Aas%20high%20initial%20costs%2C%20semi-structured%20environments%20and%20protocol%20variability.%0AIn%20this%20work%2C%20we%20develop%20a%20flexible%20and%20cost-effective%20approach%20to%20address%20this%0Agap%20by%20introducing%20a%20vision-based%20system%20for%20liquid%20volume%20estimation%20and%20a%0Asimulation-driven%20pouring%20method%20particularly%20designed%20for%20containers%20with%0Asmall%20openings.%20We%20evaluate%20both%20components%20individually%2C%20followed%20by%20an%0Aapplied%20real-world%20integration%20of%20cell%20culture%20automation%20using%20a%20UR5%20robotic%0Aarm.%20Our%20work%20is%20fully%20reproducible%3A%20we%20share%20our%20code%20at%20at%0A%5Curl%7Bhttps%3A//github.com/DaniSchober/LabLiquidVision%7D%20and%20the%20newly%20introduced%0Adataset%20LabLiquidVolume%20is%20available%20at%0Ahttps%3A//data.dtu.dk/articles/dataset/LabLiquidVision/25103102.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16529v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-based%20robot%20manipulation%20of%20transparent%20liquid%20containers%20in%20a%0A%20%20laboratory%20setting&entry.906535625=Daniel%20Schober%20and%20Ronja%20G%C3%BCldenring%20and%20James%20Love%20and%20Lazaros%20Nalpantidis&entry.1292438233=%20%20Laboratory%20processes%20involving%20small%20volumes%20of%20solutions%20and%20active%0Aingredients%20are%20often%20performed%20manually%20due%20to%20challenges%20in%20automation%2C%20such%0Aas%20high%20initial%20costs%2C%20semi-structured%20environments%20and%20protocol%20variability.%0AIn%20this%20work%2C%20we%20develop%20a%20flexible%20and%20cost-effective%20approach%20to%20address%20this%0Agap%20by%20introducing%20a%20vision-based%20system%20for%20liquid%20volume%20estimation%20and%20a%0Asimulation-driven%20pouring%20method%20particularly%20designed%20for%20containers%20with%0Asmall%20openings.%20We%20evaluate%20both%20components%20individually%2C%20followed%20by%20an%0Aapplied%20real-world%20integration%20of%20cell%20culture%20automation%20using%20a%20UR5%20robotic%0Aarm.%20Our%20work%20is%20fully%20reproducible%3A%20we%20share%20our%20code%20at%20at%0A%5Curl%7Bhttps%3A//github.com/DaniSchober/LabLiquidVision%7D%20and%20the%20newly%20introduced%0Adataset%20LabLiquidVolume%20is%20available%20at%0Ahttps%3A//data.dtu.dk/articles/dataset/LabLiquidVision/25103102.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16529v1&entry.124074799=Read"},
{"title": "A Deep Learning-Driven Pipeline for Differentiating Hypertrophic\n  Cardiomyopathy from Cardiac Amyloidosis Using 2D Multi-View Echocardiography", "author": "Bo Peng and Xiaofeng Li and Xinyu Li and Zhenghan Wang and Hui Deng and Xiaoxian Luo and Lixue Yin and Hongmei Zhang", "abstract": "  Hypertrophic cardiomyopathy (HCM) and cardiac amyloidosis (CA) are both heart\nconditions that can progress to heart failure if untreated. They exhibit\nsimilar echocardiographic characteristics, often leading to diagnostic\nchallenges. This paper introduces a novel multi-view deep learning approach\nthat utilizes 2D echocardiography for differentiating between HCM and CA. The\nmethod begins by classifying 2D echocardiography data into five distinct\nechocardiographic views: apical 4-chamber, parasternal long axis of left\nventricle, parasternal short axis at levels of the mitral valve, papillary\nmuscle, and apex. It then extracts features of each view separately and\ncombines five features for disease classification. A total of 212 patients\ndiagnosed with HCM, and 30 patients diagnosed with CA, along with 200\nindividuals with normal cardiac function(Normal), were enrolled in this study\nfrom 2018 to 2022. This approach achieved a precision, recall of 0.905, and\nmicro-F1 score of 0.904, demonstrating its effectiveness in accurately\nidentifying HCM and CA using a multi-view analysis.\n", "link": "http://arxiv.org/abs/2404.16522v1", "date": "2024-04-25", "relevancy": 1.4385, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4643}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4542}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Learning-Driven%20Pipeline%20for%20Differentiating%20Hypertrophic%0A%20%20Cardiomyopathy%20from%20Cardiac%20Amyloidosis%20Using%202D%20Multi-View%20Echocardiography&body=Title%3A%20A%20Deep%20Learning-Driven%20Pipeline%20for%20Differentiating%20Hypertrophic%0A%20%20Cardiomyopathy%20from%20Cardiac%20Amyloidosis%20Using%202D%20Multi-View%20Echocardiography%0AAuthor%3A%20Bo%20Peng%20and%20Xiaofeng%20Li%20and%20Xinyu%20Li%20and%20Zhenghan%20Wang%20and%20Hui%20Deng%20and%20Xiaoxian%20Luo%20and%20Lixue%20Yin%20and%20Hongmei%20Zhang%0AAbstract%3A%20%20%20Hypertrophic%20cardiomyopathy%20%28HCM%29%20and%20cardiac%20amyloidosis%20%28CA%29%20are%20both%20heart%0Aconditions%20that%20can%20progress%20to%20heart%20failure%20if%20untreated.%20They%20exhibit%0Asimilar%20echocardiographic%20characteristics%2C%20often%20leading%20to%20diagnostic%0Achallenges.%20This%20paper%20introduces%20a%20novel%20multi-view%20deep%20learning%20approach%0Athat%20utilizes%202D%20echocardiography%20for%20differentiating%20between%20HCM%20and%20CA.%20The%0Amethod%20begins%20by%20classifying%202D%20echocardiography%20data%20into%20five%20distinct%0Aechocardiographic%20views%3A%20apical%204-chamber%2C%20parasternal%20long%20axis%20of%20left%0Aventricle%2C%20parasternal%20short%20axis%20at%20levels%20of%20the%20mitral%20valve%2C%20papillary%0Amuscle%2C%20and%20apex.%20It%20then%20extracts%20features%20of%20each%20view%20separately%20and%0Acombines%20five%20features%20for%20disease%20classification.%20A%20total%20of%20212%20patients%0Adiagnosed%20with%20HCM%2C%20and%2030%20patients%20diagnosed%20with%20CA%2C%20along%20with%20200%0Aindividuals%20with%20normal%20cardiac%20function%28Normal%29%2C%20were%20enrolled%20in%20this%20study%0Afrom%202018%20to%202022.%20This%20approach%20achieved%20a%20precision%2C%20recall%20of%200.905%2C%20and%0Amicro-F1%20score%20of%200.904%2C%20demonstrating%20its%20effectiveness%20in%20accurately%0Aidentifying%20HCM%20and%20CA%20using%20a%20multi-view%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16522v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Learning-Driven%20Pipeline%20for%20Differentiating%20Hypertrophic%0A%20%20Cardiomyopathy%20from%20Cardiac%20Amyloidosis%20Using%202D%20Multi-View%20Echocardiography&entry.906535625=Bo%20Peng%20and%20Xiaofeng%20Li%20and%20Xinyu%20Li%20and%20Zhenghan%20Wang%20and%20Hui%20Deng%20and%20Xiaoxian%20Luo%20and%20Lixue%20Yin%20and%20Hongmei%20Zhang&entry.1292438233=%20%20Hypertrophic%20cardiomyopathy%20%28HCM%29%20and%20cardiac%20amyloidosis%20%28CA%29%20are%20both%20heart%0Aconditions%20that%20can%20progress%20to%20heart%20failure%20if%20untreated.%20They%20exhibit%0Asimilar%20echocardiographic%20characteristics%2C%20often%20leading%20to%20diagnostic%0Achallenges.%20This%20paper%20introduces%20a%20novel%20multi-view%20deep%20learning%20approach%0Athat%20utilizes%202D%20echocardiography%20for%20differentiating%20between%20HCM%20and%20CA.%20The%0Amethod%20begins%20by%20classifying%202D%20echocardiography%20data%20into%20five%20distinct%0Aechocardiographic%20views%3A%20apical%204-chamber%2C%20parasternal%20long%20axis%20of%20left%0Aventricle%2C%20parasternal%20short%20axis%20at%20levels%20of%20the%20mitral%20valve%2C%20papillary%0Amuscle%2C%20and%20apex.%20It%20then%20extracts%20features%20of%20each%20view%20separately%20and%0Acombines%20five%20features%20for%20disease%20classification.%20A%20total%20of%20212%20patients%0Adiagnosed%20with%20HCM%2C%20and%2030%20patients%20diagnosed%20with%20CA%2C%20along%20with%20200%0Aindividuals%20with%20normal%20cardiac%20function%28Normal%29%2C%20were%20enrolled%20in%20this%20study%0Afrom%202018%20to%202022.%20This%20approach%20achieved%20a%20precision%2C%20recall%20of%200.905%2C%20and%0Amicro-F1%20score%20of%200.904%2C%20demonstrating%20its%20effectiveness%20in%20accurately%0Aidentifying%20HCM%20and%20CA%20using%20a%20multi-view%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16522v1&entry.124074799=Read"},
{"title": "Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting\n  3D Objects with Realistic Materials", "author": "Ye Fang and Zeyi Sun and Tong Wu and Jiaqi Wang and Ziwei Liu and Gordon Wetzstein and Dahua Lin", "abstract": "  Physically realistic materials are pivotal in augmenting the realism of 3D\nassets across various applications and lighting conditions. However, existing\n3D assets and generative models often lack authentic material properties.\nManual assignment of materials using graphic software is a tedious and\ntime-consuming task. In this paper, we exploit advancements in Multimodal Large\nLanguage Models (MLLMs), particularly GPT-4V, to present a novel approach,\nMake-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and\ndescribe materials, allowing the construction of a detailed material library.\n2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V\nprecisely identifies and aligns materials with the corresponding components of\n3D objects. 3) The correctly matched materials are then meticulously applied as\nreference for the new SVBRDF material generation according to the original\ndiffuse map, significantly enhancing their visual authenticity. Make-it-Real\noffers a streamlined integration into the 3D content creation workflow,\nshowcasing its utility as an essential tool for developers of 3D assets.\n", "link": "http://arxiv.org/abs/2404.16829v1", "date": "2024-04-25", "relevancy": 1.6428, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5804}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Make-it-Real%3A%20Unleashing%20Large%20Multimodal%20Model%27s%20Ability%20for%20Painting%0A%20%203D%20Objects%20with%20Realistic%20Materials&body=Title%3A%20Make-it-Real%3A%20Unleashing%20Large%20Multimodal%20Model%27s%20Ability%20for%20Painting%0A%20%203D%20Objects%20with%20Realistic%20Materials%0AAuthor%3A%20Ye%20Fang%20and%20Zeyi%20Sun%20and%20Tong%20Wu%20and%20Jiaqi%20Wang%20and%20Ziwei%20Liu%20and%20Gordon%20Wetzstein%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20Physically%20realistic%20materials%20are%20pivotal%20in%20augmenting%20the%20realism%20of%203D%0Aassets%20across%20various%20applications%20and%20lighting%20conditions.%20However%2C%20existing%0A3D%20assets%20and%20generative%20models%20often%20lack%20authentic%20material%20properties.%0AManual%20assignment%20of%20materials%20using%20graphic%20software%20is%20a%20tedious%20and%0Atime-consuming%20task.%20In%20this%20paper%2C%20we%20exploit%20advancements%20in%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%2C%20particularly%20GPT-4V%2C%20to%20present%20a%20novel%20approach%2C%0AMake-it-Real%3A%201%29%20We%20demonstrate%20that%20GPT-4V%20can%20effectively%20recognize%20and%0Adescribe%20materials%2C%20allowing%20the%20construction%20of%20a%20detailed%20material%20library.%0A2%29%20Utilizing%20a%20combination%20of%20visual%20cues%20and%20hierarchical%20text%20prompts%2C%20GPT-4V%0Aprecisely%20identifies%20and%20aligns%20materials%20with%20the%20corresponding%20components%20of%0A3D%20objects.%203%29%20The%20correctly%20matched%20materials%20are%20then%20meticulously%20applied%20as%0Areference%20for%20the%20new%20SVBRDF%20material%20generation%20according%20to%20the%20original%0Adiffuse%20map%2C%20significantly%20enhancing%20their%20visual%20authenticity.%20Make-it-Real%0Aoffers%20a%20streamlined%20integration%20into%20the%203D%20content%20creation%20workflow%2C%0Ashowcasing%20its%20utility%20as%20an%20essential%20tool%20for%20developers%20of%203D%20assets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16829v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Make-it-Real%3A%20Unleashing%20Large%20Multimodal%20Model%27s%20Ability%20for%20Painting%0A%20%203D%20Objects%20with%20Realistic%20Materials&entry.906535625=Ye%20Fang%20and%20Zeyi%20Sun%20and%20Tong%20Wu%20and%20Jiaqi%20Wang%20and%20Ziwei%20Liu%20and%20Gordon%20Wetzstein%20and%20Dahua%20Lin&entry.1292438233=%20%20Physically%20realistic%20materials%20are%20pivotal%20in%20augmenting%20the%20realism%20of%203D%0Aassets%20across%20various%20applications%20and%20lighting%20conditions.%20However%2C%20existing%0A3D%20assets%20and%20generative%20models%20often%20lack%20authentic%20material%20properties.%0AManual%20assignment%20of%20materials%20using%20graphic%20software%20is%20a%20tedious%20and%0Atime-consuming%20task.%20In%20this%20paper%2C%20we%20exploit%20advancements%20in%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%2C%20particularly%20GPT-4V%2C%20to%20present%20a%20novel%20approach%2C%0AMake-it-Real%3A%201%29%20We%20demonstrate%20that%20GPT-4V%20can%20effectively%20recognize%20and%0Adescribe%20materials%2C%20allowing%20the%20construction%20of%20a%20detailed%20material%20library.%0A2%29%20Utilizing%20a%20combination%20of%20visual%20cues%20and%20hierarchical%20text%20prompts%2C%20GPT-4V%0Aprecisely%20identifies%20and%20aligns%20materials%20with%20the%20corresponding%20components%20of%0A3D%20objects.%203%29%20The%20correctly%20matched%20materials%20are%20then%20meticulously%20applied%20as%0Areference%20for%20the%20new%20SVBRDF%20material%20generation%20according%20to%20the%20original%0Adiffuse%20map%2C%20significantly%20enhancing%20their%20visual%20authenticity.%20Make-it-Real%0Aoffers%20a%20streamlined%20integration%20into%20the%203D%20content%20creation%20workflow%2C%0Ashowcasing%20its%20utility%20as%20an%20essential%20tool%20for%20developers%20of%203D%20assets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16829v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


