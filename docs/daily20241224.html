<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241223.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ActiveGS: Active Scene Reconstruction using Gaussian Splatting", "author": "Liren Jin and Xingguang Zhong and Yue Pan and Jens Behley and Cyrill Stachniss and Marija Popovi\u0107", "abstract": "  Robotics applications often rely on scene reconstructions to enable\ndownstream tasks. In this work, we tackle the challenge of actively building an\naccurate map of an unknown scene using an on-board RGB-D camera. We propose a\nhybrid map representation that combines a Gaussian splatting map with a coarse\nvoxel map, leveraging the strengths of both representations: the high-fidelity\nscene reconstruction capabilities of Gaussian splatting and the spatial\nmodelling strengths of the voxel map. The core of our framework is an effective\nconfidence modelling technique for the Gaussian splatting map to identify\nunder-reconstructed areas, while utilising spatial information from the voxel\nmap to target unexplored areas and assist in collision-free path planning. By\nactively collecting scene information in under-reconstructed and unexplored\nareas for map updates, our approach achieves superior Gaussian splatting\nreconstruction results compared to state-of-the-art approaches. Additionally,\nwe demonstrate the applicability of our active scene reconstruction framework\nin the real world using an unmanned aerial vehicle.\n", "link": "http://arxiv.org/abs/2412.17769v1", "date": "2024-12-23", "relevancy": 3.4734, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7628}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6652}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActiveGS%3A%20Active%20Scene%20Reconstruction%20using%20Gaussian%20Splatting&body=Title%3A%20ActiveGS%3A%20Active%20Scene%20Reconstruction%20using%20Gaussian%20Splatting%0AAuthor%3A%20Liren%20Jin%20and%20Xingguang%20Zhong%20and%20Yue%20Pan%20and%20Jens%20Behley%20and%20Cyrill%20Stachniss%20and%20Marija%20Popovi%C4%87%0AAbstract%3A%20%20%20Robotics%20applications%20often%20rely%20on%20scene%20reconstructions%20to%20enable%0Adownstream%20tasks.%20In%20this%20work%2C%20we%20tackle%20the%20challenge%20of%20actively%20building%20an%0Aaccurate%20map%20of%20an%20unknown%20scene%20using%20an%20on-board%20RGB-D%20camera.%20We%20propose%20a%0Ahybrid%20map%20representation%20that%20combines%20a%20Gaussian%20splatting%20map%20with%20a%20coarse%0Avoxel%20map%2C%20leveraging%20the%20strengths%20of%20both%20representations%3A%20the%20high-fidelity%0Ascene%20reconstruction%20capabilities%20of%20Gaussian%20splatting%20and%20the%20spatial%0Amodelling%20strengths%20of%20the%20voxel%20map.%20The%20core%20of%20our%20framework%20is%20an%20effective%0Aconfidence%20modelling%20technique%20for%20the%20Gaussian%20splatting%20map%20to%20identify%0Aunder-reconstructed%20areas%2C%20while%20utilising%20spatial%20information%20from%20the%20voxel%0Amap%20to%20target%20unexplored%20areas%20and%20assist%20in%20collision-free%20path%20planning.%20By%0Aactively%20collecting%20scene%20information%20in%20under-reconstructed%20and%20unexplored%0Aareas%20for%20map%20updates%2C%20our%20approach%20achieves%20superior%20Gaussian%20splatting%0Areconstruction%20results%20compared%20to%20state-of-the-art%20approaches.%20Additionally%2C%0Awe%20demonstrate%20the%20applicability%20of%20our%20active%20scene%20reconstruction%20framework%0Ain%20the%20real%20world%20using%20an%20unmanned%20aerial%20vehicle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActiveGS%253A%2520Active%2520Scene%2520Reconstruction%2520using%2520Gaussian%2520Splatting%26entry.906535625%3DLiren%2520Jin%2520and%2520Xingguang%2520Zhong%2520and%2520Yue%2520Pan%2520and%2520Jens%2520Behley%2520and%2520Cyrill%2520Stachniss%2520and%2520Marija%2520Popovi%25C4%2587%26entry.1292438233%3D%2520%2520Robotics%2520applications%2520often%2520rely%2520on%2520scene%2520reconstructions%2520to%2520enable%250Adownstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520challenge%2520of%2520actively%2520building%2520an%250Aaccurate%2520map%2520of%2520an%2520unknown%2520scene%2520using%2520an%2520on-board%2520RGB-D%2520camera.%2520We%2520propose%2520a%250Ahybrid%2520map%2520representation%2520that%2520combines%2520a%2520Gaussian%2520splatting%2520map%2520with%2520a%2520coarse%250Avoxel%2520map%252C%2520leveraging%2520the%2520strengths%2520of%2520both%2520representations%253A%2520the%2520high-fidelity%250Ascene%2520reconstruction%2520capabilities%2520of%2520Gaussian%2520splatting%2520and%2520the%2520spatial%250Amodelling%2520strengths%2520of%2520the%2520voxel%2520map.%2520The%2520core%2520of%2520our%2520framework%2520is%2520an%2520effective%250Aconfidence%2520modelling%2520technique%2520for%2520the%2520Gaussian%2520splatting%2520map%2520to%2520identify%250Aunder-reconstructed%2520areas%252C%2520while%2520utilising%2520spatial%2520information%2520from%2520the%2520voxel%250Amap%2520to%2520target%2520unexplored%2520areas%2520and%2520assist%2520in%2520collision-free%2520path%2520planning.%2520By%250Aactively%2520collecting%2520scene%2520information%2520in%2520under-reconstructed%2520and%2520unexplored%250Aareas%2520for%2520map%2520updates%252C%2520our%2520approach%2520achieves%2520superior%2520Gaussian%2520splatting%250Areconstruction%2520results%2520compared%2520to%2520state-of-the-art%2520approaches.%2520Additionally%252C%250Awe%2520demonstrate%2520the%2520applicability%2520of%2520our%2520active%2520scene%2520reconstruction%2520framework%250Ain%2520the%2520real%2520world%2520using%2520an%2520unmanned%2520aerial%2520vehicle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActiveGS%3A%20Active%20Scene%20Reconstruction%20using%20Gaussian%20Splatting&entry.906535625=Liren%20Jin%20and%20Xingguang%20Zhong%20and%20Yue%20Pan%20and%20Jens%20Behley%20and%20Cyrill%20Stachniss%20and%20Marija%20Popovi%C4%87&entry.1292438233=%20%20Robotics%20applications%20often%20rely%20on%20scene%20reconstructions%20to%20enable%0Adownstream%20tasks.%20In%20this%20work%2C%20we%20tackle%20the%20challenge%20of%20actively%20building%20an%0Aaccurate%20map%20of%20an%20unknown%20scene%20using%20an%20on-board%20RGB-D%20camera.%20We%20propose%20a%0Ahybrid%20map%20representation%20that%20combines%20a%20Gaussian%20splatting%20map%20with%20a%20coarse%0Avoxel%20map%2C%20leveraging%20the%20strengths%20of%20both%20representations%3A%20the%20high-fidelity%0Ascene%20reconstruction%20capabilities%20of%20Gaussian%20splatting%20and%20the%20spatial%0Amodelling%20strengths%20of%20the%20voxel%20map.%20The%20core%20of%20our%20framework%20is%20an%20effective%0Aconfidence%20modelling%20technique%20for%20the%20Gaussian%20splatting%20map%20to%20identify%0Aunder-reconstructed%20areas%2C%20while%20utilising%20spatial%20information%20from%20the%20voxel%0Amap%20to%20target%20unexplored%20areas%20and%20assist%20in%20collision-free%20path%20planning.%20By%0Aactively%20collecting%20scene%20information%20in%20under-reconstructed%20and%20unexplored%0Aareas%20for%20map%20updates%2C%20our%20approach%20achieves%20superior%20Gaussian%20splatting%0Areconstruction%20results%20compared%20to%20state-of-the-art%20approaches.%20Additionally%2C%0Awe%20demonstrate%20the%20applicability%20of%20our%20active%20scene%20reconstruction%20framework%0Ain%20the%20real%20world%20using%20an%20unmanned%20aerial%20vehicle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17769v1&entry.124074799=Read"},
{"title": "CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed\n  Learning for Large Scene Reconstruction", "author": "Yuanyuan Gao and Yalun Dai and Hao Li and Weicai Ye and Junyi Chen and Danpeng Chen and Dingwen Zhang and Tong He and Guofeng Zhang and Junwei Han", "abstract": "  3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene\nreconstruction. However, most existing GS-based surface reconstruction methods\nfocus on 3D objects or limited scenes. Directly applying these methods to\nlarge-scale scene reconstruction will pose challenges such as high memory\ncosts, excessive time consumption, and lack of geometric detail, which makes it\ndifficult to implement in practical applications. To address these issues, we\npropose a multi-agent collaborative fast 3DGS surface reconstruction framework\nbased on distributed learning for large-scale surface reconstruction.\nSpecifically, we develop local model compression (LMC) and model aggregation\nschemes (MAS) to achieve high-quality surface representation of large scenes\nwhile reducing GPU memory consumption. Extensive experiments on Urban3d,\nMegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast\nand scalable high-fidelity surface reconstruction and photorealistic rendering.\nOur project page is available at \\url{https://gyy456.github.io/CoSurfGS}.\n", "link": "http://arxiv.org/abs/2412.17612v1", "date": "2024-12-23", "relevancy": 3.3372, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7053}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.649}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoSurfGS%3ACollaborative%203D%20Surface%20Gaussian%20Splatting%20with%20Distributed%0A%20%20Learning%20for%20Large%20Scene%20Reconstruction&body=Title%3A%20CoSurfGS%3ACollaborative%203D%20Surface%20Gaussian%20Splatting%20with%20Distributed%0A%20%20Learning%20for%20Large%20Scene%20Reconstruction%0AAuthor%3A%20Yuanyuan%20Gao%20and%20Yalun%20Dai%20and%20Hao%20Li%20and%20Weicai%20Ye%20and%20Junyi%20Chen%20and%20Danpeng%20Chen%20and%20Dingwen%20Zhang%20and%20Tong%20He%20and%20Guofeng%20Zhang%20and%20Junwei%20Han%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20impressive%20performance%20in%20scene%0Areconstruction.%20However%2C%20most%20existing%20GS-based%20surface%20reconstruction%20methods%0Afocus%20on%203D%20objects%20or%20limited%20scenes.%20Directly%20applying%20these%20methods%20to%0Alarge-scale%20scene%20reconstruction%20will%20pose%20challenges%20such%20as%20high%20memory%0Acosts%2C%20excessive%20time%20consumption%2C%20and%20lack%20of%20geometric%20detail%2C%20which%20makes%20it%0Adifficult%20to%20implement%20in%20practical%20applications.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20multi-agent%20collaborative%20fast%203DGS%20surface%20reconstruction%20framework%0Abased%20on%20distributed%20learning%20for%20large-scale%20surface%20reconstruction.%0ASpecifically%2C%20we%20develop%20local%20model%20compression%20%28LMC%29%20and%20model%20aggregation%0Aschemes%20%28MAS%29%20to%20achieve%20high-quality%20surface%20representation%20of%20large%20scenes%0Awhile%20reducing%20GPU%20memory%20consumption.%20Extensive%20experiments%20on%20Urban3d%2C%0AMegaNeRF%2C%20and%20BlendedMVS%20demonstrate%20that%20our%20proposed%20method%20can%20achieve%20fast%0Aand%20scalable%20high-fidelity%20surface%20reconstruction%20and%20photorealistic%20rendering.%0AOur%20project%20page%20is%20available%20at%20%5Curl%7Bhttps%3A//gyy456.github.io/CoSurfGS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoSurfGS%253ACollaborative%25203D%2520Surface%2520Gaussian%2520Splatting%2520with%2520Distributed%250A%2520%2520Learning%2520for%2520Large%2520Scene%2520Reconstruction%26entry.906535625%3DYuanyuan%2520Gao%2520and%2520Yalun%2520Dai%2520and%2520Hao%2520Li%2520and%2520Weicai%2520Ye%2520and%2520Junyi%2520Chen%2520and%2520Danpeng%2520Chen%2520and%2520Dingwen%2520Zhang%2520and%2520Tong%2520He%2520and%2520Guofeng%2520Zhang%2520and%2520Junwei%2520Han%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520impressive%2520performance%2520in%2520scene%250Areconstruction.%2520However%252C%2520most%2520existing%2520GS-based%2520surface%2520reconstruction%2520methods%250Afocus%2520on%25203D%2520objects%2520or%2520limited%2520scenes.%2520Directly%2520applying%2520these%2520methods%2520to%250Alarge-scale%2520scene%2520reconstruction%2520will%2520pose%2520challenges%2520such%2520as%2520high%2520memory%250Acosts%252C%2520excessive%2520time%2520consumption%252C%2520and%2520lack%2520of%2520geometric%2520detail%252C%2520which%2520makes%2520it%250Adifficult%2520to%2520implement%2520in%2520practical%2520applications.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520multi-agent%2520collaborative%2520fast%25203DGS%2520surface%2520reconstruction%2520framework%250Abased%2520on%2520distributed%2520learning%2520for%2520large-scale%2520surface%2520reconstruction.%250ASpecifically%252C%2520we%2520develop%2520local%2520model%2520compression%2520%2528LMC%2529%2520and%2520model%2520aggregation%250Aschemes%2520%2528MAS%2529%2520to%2520achieve%2520high-quality%2520surface%2520representation%2520of%2520large%2520scenes%250Awhile%2520reducing%2520GPU%2520memory%2520consumption.%2520Extensive%2520experiments%2520on%2520Urban3d%252C%250AMegaNeRF%252C%2520and%2520BlendedMVS%2520demonstrate%2520that%2520our%2520proposed%2520method%2520can%2520achieve%2520fast%250Aand%2520scalable%2520high-fidelity%2520surface%2520reconstruction%2520and%2520photorealistic%2520rendering.%250AOur%2520project%2520page%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//gyy456.github.io/CoSurfGS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoSurfGS%3ACollaborative%203D%20Surface%20Gaussian%20Splatting%20with%20Distributed%0A%20%20Learning%20for%20Large%20Scene%20Reconstruction&entry.906535625=Yuanyuan%20Gao%20and%20Yalun%20Dai%20and%20Hao%20Li%20and%20Weicai%20Ye%20and%20Junyi%20Chen%20and%20Danpeng%20Chen%20and%20Dingwen%20Zhang%20and%20Tong%20He%20and%20Guofeng%20Zhang%20and%20Junwei%20Han&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20impressive%20performance%20in%20scene%0Areconstruction.%20However%2C%20most%20existing%20GS-based%20surface%20reconstruction%20methods%0Afocus%20on%203D%20objects%20or%20limited%20scenes.%20Directly%20applying%20these%20methods%20to%0Alarge-scale%20scene%20reconstruction%20will%20pose%20challenges%20such%20as%20high%20memory%0Acosts%2C%20excessive%20time%20consumption%2C%20and%20lack%20of%20geometric%20detail%2C%20which%20makes%20it%0Adifficult%20to%20implement%20in%20practical%20applications.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20multi-agent%20collaborative%20fast%203DGS%20surface%20reconstruction%20framework%0Abased%20on%20distributed%20learning%20for%20large-scale%20surface%20reconstruction.%0ASpecifically%2C%20we%20develop%20local%20model%20compression%20%28LMC%29%20and%20model%20aggregation%0Aschemes%20%28MAS%29%20to%20achieve%20high-quality%20surface%20representation%20of%20large%20scenes%0Awhile%20reducing%20GPU%20memory%20consumption.%20Extensive%20experiments%20on%20Urban3d%2C%0AMegaNeRF%2C%20and%20BlendedMVS%20demonstrate%20that%20our%20proposed%20method%20can%20achieve%20fast%0Aand%20scalable%20high-fidelity%20surface%20reconstruction%20and%20photorealistic%20rendering.%0AOur%20project%20page%20is%20available%20at%20%5Curl%7Bhttps%3A//gyy456.github.io/CoSurfGS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17612v1&entry.124074799=Read"},
{"title": "UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater\n  Scene Reconstruction", "author": "Haoran Wang and Nantheera Anantrasirichai and Fan Zhang and David Bull", "abstract": "  3D Gaussian splatting (3DGS) offers the capability to achieve real-time high\nquality 3D scene rendering. However, 3DGS assumes that the scene is in a clear\nmedium environment and struggles to generate satisfactory representations in\nunderwater scenes, where light absorption and scattering are prevalent and\nmoving objects are involved. To overcome these, we introduce a novel Gaussian\nSplatting-based method, UW-GS, designed specifically for underwater\napplications. It introduces a color appearance that models distance-dependent\ncolor variation, employs a new physics-based density control strategy to\nenhance clarity for distant objects, and uses a binary motion mask to handle\ndynamic content. Optimized with a well-designed loss function supporting for\nscattering media and strengthened by pseudo-depth maps, UW-GS outperforms\nexisting methods with PSNR gains up to 1.26dB. To fully verify the\neffectiveness of the model, we also developed a new underwater dataset, S-UW,\nwith dynamic object masks.\n", "link": "http://arxiv.org/abs/2410.01517v2", "date": "2024-12-23", "relevancy": 3.274, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6859}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6554}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UW-GS%3A%20Distractor-Aware%203D%20Gaussian%20Splatting%20for%20Enhanced%20Underwater%0A%20%20Scene%20Reconstruction&body=Title%3A%20UW-GS%3A%20Distractor-Aware%203D%20Gaussian%20Splatting%20for%20Enhanced%20Underwater%0A%20%20Scene%20Reconstruction%0AAuthor%3A%20Haoran%20Wang%20and%20Nantheera%20Anantrasirichai%20and%20Fan%20Zhang%20and%20David%20Bull%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20offers%20the%20capability%20to%20achieve%20real-time%20high%0Aquality%203D%20scene%20rendering.%20However%2C%203DGS%20assumes%20that%20the%20scene%20is%20in%20a%20clear%0Amedium%20environment%20and%20struggles%20to%20generate%20satisfactory%20representations%20in%0Aunderwater%20scenes%2C%20where%20light%20absorption%20and%20scattering%20are%20prevalent%20and%0Amoving%20objects%20are%20involved.%20To%20overcome%20these%2C%20we%20introduce%20a%20novel%20Gaussian%0ASplatting-based%20method%2C%20UW-GS%2C%20designed%20specifically%20for%20underwater%0Aapplications.%20It%20introduces%20a%20color%20appearance%20that%20models%20distance-dependent%0Acolor%20variation%2C%20employs%20a%20new%20physics-based%20density%20control%20strategy%20to%0Aenhance%20clarity%20for%20distant%20objects%2C%20and%20uses%20a%20binary%20motion%20mask%20to%20handle%0Adynamic%20content.%20Optimized%20with%20a%20well-designed%20loss%20function%20supporting%20for%0Ascattering%20media%20and%20strengthened%20by%20pseudo-depth%20maps%2C%20UW-GS%20outperforms%0Aexisting%20methods%20with%20PSNR%20gains%20up%20to%201.26dB.%20To%20fully%20verify%20the%0Aeffectiveness%20of%20the%20model%2C%20we%20also%20developed%20a%20new%20underwater%20dataset%2C%20S-UW%2C%0Awith%20dynamic%20object%20masks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01517v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUW-GS%253A%2520Distractor-Aware%25203D%2520Gaussian%2520Splatting%2520for%2520Enhanced%2520Underwater%250A%2520%2520Scene%2520Reconstruction%26entry.906535625%3DHaoran%2520Wang%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520Fan%2520Zhang%2520and%2520David%2520Bull%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520offers%2520the%2520capability%2520to%2520achieve%2520real-time%2520high%250Aquality%25203D%2520scene%2520rendering.%2520However%252C%25203DGS%2520assumes%2520that%2520the%2520scene%2520is%2520in%2520a%2520clear%250Amedium%2520environment%2520and%2520struggles%2520to%2520generate%2520satisfactory%2520representations%2520in%250Aunderwater%2520scenes%252C%2520where%2520light%2520absorption%2520and%2520scattering%2520are%2520prevalent%2520and%250Amoving%2520objects%2520are%2520involved.%2520To%2520overcome%2520these%252C%2520we%2520introduce%2520a%2520novel%2520Gaussian%250ASplatting-based%2520method%252C%2520UW-GS%252C%2520designed%2520specifically%2520for%2520underwater%250Aapplications.%2520It%2520introduces%2520a%2520color%2520appearance%2520that%2520models%2520distance-dependent%250Acolor%2520variation%252C%2520employs%2520a%2520new%2520physics-based%2520density%2520control%2520strategy%2520to%250Aenhance%2520clarity%2520for%2520distant%2520objects%252C%2520and%2520uses%2520a%2520binary%2520motion%2520mask%2520to%2520handle%250Adynamic%2520content.%2520Optimized%2520with%2520a%2520well-designed%2520loss%2520function%2520supporting%2520for%250Ascattering%2520media%2520and%2520strengthened%2520by%2520pseudo-depth%2520maps%252C%2520UW-GS%2520outperforms%250Aexisting%2520methods%2520with%2520PSNR%2520gains%2520up%2520to%25201.26dB.%2520To%2520fully%2520verify%2520the%250Aeffectiveness%2520of%2520the%2520model%252C%2520we%2520also%2520developed%2520a%2520new%2520underwater%2520dataset%252C%2520S-UW%252C%250Awith%2520dynamic%2520object%2520masks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01517v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UW-GS%3A%20Distractor-Aware%203D%20Gaussian%20Splatting%20for%20Enhanced%20Underwater%0A%20%20Scene%20Reconstruction&entry.906535625=Haoran%20Wang%20and%20Nantheera%20Anantrasirichai%20and%20Fan%20Zhang%20and%20David%20Bull&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20offers%20the%20capability%20to%20achieve%20real-time%20high%0Aquality%203D%20scene%20rendering.%20However%2C%203DGS%20assumes%20that%20the%20scene%20is%20in%20a%20clear%0Amedium%20environment%20and%20struggles%20to%20generate%20satisfactory%20representations%20in%0Aunderwater%20scenes%2C%20where%20light%20absorption%20and%20scattering%20are%20prevalent%20and%0Amoving%20objects%20are%20involved.%20To%20overcome%20these%2C%20we%20introduce%20a%20novel%20Gaussian%0ASplatting-based%20method%2C%20UW-GS%2C%20designed%20specifically%20for%20underwater%0Aapplications.%20It%20introduces%20a%20color%20appearance%20that%20models%20distance-dependent%0Acolor%20variation%2C%20employs%20a%20new%20physics-based%20density%20control%20strategy%20to%0Aenhance%20clarity%20for%20distant%20objects%2C%20and%20uses%20a%20binary%20motion%20mask%20to%20handle%0Adynamic%20content.%20Optimized%20with%20a%20well-designed%20loss%20function%20supporting%20for%0Ascattering%20media%20and%20strengthened%20by%20pseudo-depth%20maps%2C%20UW-GS%20outperforms%0Aexisting%20methods%20with%20PSNR%20gains%20up%20to%201.26dB.%20To%20fully%20verify%20the%0Aeffectiveness%20of%20the%20model%2C%20we%20also%20developed%20a%20new%20underwater%20dataset%2C%20S-UW%2C%0Awith%20dynamic%20object%20masks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01517v2&entry.124074799=Read"},
{"title": "FaceLift: Single Image to 3D Head with View Generation and GS-LRM", "author": "Weijie Lyu and Yi Zhou and Ming-Hsuan Yang and Zhixin Shu", "abstract": "  We present FaceLift, a feed-forward approach for rapid, high-quality,\n360-degree head reconstruction from a single image. Our pipeline begins by\nemploying a multi-view latent diffusion model that generates consistent side\nand back views of the head from a single facial input. These generated views\nthen serve as input to a GS-LRM reconstructor, which produces a comprehensive\n3D representation using Gaussian splats. To train our system, we develop a\ndataset of multi-view renderings using synthetic 3D human head as-sets. The\ndiffusion-based multi-view generator is trained exclusively on synthetic head\nimages, while the GS-LRM reconstructor undergoes initial training on Objaverse\nfollowed by fine-tuning on synthetic head data. FaceLift excels at preserving\nidentity and maintaining view consistency across views. Despite being trained\nsolely on synthetic data, FaceLift demonstrates remarkable generalization to\nreal-world images. Through extensive qualitative and quantitative evaluations,\nwe show that FaceLift outperforms state-of-the-art methods in 3D head\nreconstruction, highlighting its practical applicability and robust performance\non real-world images. In addition to single image reconstruction, FaceLift\nsupports video inputs for 4D novel view synthesis and seamlessly integrates\nwith 2D reanimation techniques to enable 3D facial animation. Project page:\nhttps://weijielyu.github.io/FaceLift.\n", "link": "http://arxiv.org/abs/2412.17812v1", "date": "2024-12-23", "relevancy": 3.2467, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6498}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6498}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceLift%3A%20Single%20Image%20to%203D%20Head%20with%20View%20Generation%20and%20GS-LRM&body=Title%3A%20FaceLift%3A%20Single%20Image%20to%203D%20Head%20with%20View%20Generation%20and%20GS-LRM%0AAuthor%3A%20Weijie%20Lyu%20and%20Yi%20Zhou%20and%20Ming-Hsuan%20Yang%20and%20Zhixin%20Shu%0AAbstract%3A%20%20%20We%20present%20FaceLift%2C%20a%20feed-forward%20approach%20for%20rapid%2C%20high-quality%2C%0A360-degree%20head%20reconstruction%20from%20a%20single%20image.%20Our%20pipeline%20begins%20by%0Aemploying%20a%20multi-view%20latent%20diffusion%20model%20that%20generates%20consistent%20side%0Aand%20back%20views%20of%20the%20head%20from%20a%20single%20facial%20input.%20These%20generated%20views%0Athen%20serve%20as%20input%20to%20a%20GS-LRM%20reconstructor%2C%20which%20produces%20a%20comprehensive%0A3D%20representation%20using%20Gaussian%20splats.%20To%20train%20our%20system%2C%20we%20develop%20a%0Adataset%20of%20multi-view%20renderings%20using%20synthetic%203D%20human%20head%20as-sets.%20The%0Adiffusion-based%20multi-view%20generator%20is%20trained%20exclusively%20on%20synthetic%20head%0Aimages%2C%20while%20the%20GS-LRM%20reconstructor%20undergoes%20initial%20training%20on%20Objaverse%0Afollowed%20by%20fine-tuning%20on%20synthetic%20head%20data.%20FaceLift%20excels%20at%20preserving%0Aidentity%20and%20maintaining%20view%20consistency%20across%20views.%20Despite%20being%20trained%0Asolely%20on%20synthetic%20data%2C%20FaceLift%20demonstrates%20remarkable%20generalization%20to%0Areal-world%20images.%20Through%20extensive%20qualitative%20and%20quantitative%20evaluations%2C%0Awe%20show%20that%20FaceLift%20outperforms%20state-of-the-art%20methods%20in%203D%20head%0Areconstruction%2C%20highlighting%20its%20practical%20applicability%20and%20robust%20performance%0Aon%20real-world%20images.%20In%20addition%20to%20single%20image%20reconstruction%2C%20FaceLift%0Asupports%20video%20inputs%20for%204D%20novel%20view%20synthesis%20and%20seamlessly%20integrates%0Awith%202D%20reanimation%20techniques%20to%20enable%203D%20facial%20animation.%20Project%20page%3A%0Ahttps%3A//weijielyu.github.io/FaceLift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceLift%253A%2520Single%2520Image%2520to%25203D%2520Head%2520with%2520View%2520Generation%2520and%2520GS-LRM%26entry.906535625%3DWeijie%2520Lyu%2520and%2520Yi%2520Zhou%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Zhixin%2520Shu%26entry.1292438233%3D%2520%2520We%2520present%2520FaceLift%252C%2520a%2520feed-forward%2520approach%2520for%2520rapid%252C%2520high-quality%252C%250A360-degree%2520head%2520reconstruction%2520from%2520a%2520single%2520image.%2520Our%2520pipeline%2520begins%2520by%250Aemploying%2520a%2520multi-view%2520latent%2520diffusion%2520model%2520that%2520generates%2520consistent%2520side%250Aand%2520back%2520views%2520of%2520the%2520head%2520from%2520a%2520single%2520facial%2520input.%2520These%2520generated%2520views%250Athen%2520serve%2520as%2520input%2520to%2520a%2520GS-LRM%2520reconstructor%252C%2520which%2520produces%2520a%2520comprehensive%250A3D%2520representation%2520using%2520Gaussian%2520splats.%2520To%2520train%2520our%2520system%252C%2520we%2520develop%2520a%250Adataset%2520of%2520multi-view%2520renderings%2520using%2520synthetic%25203D%2520human%2520head%2520as-sets.%2520The%250Adiffusion-based%2520multi-view%2520generator%2520is%2520trained%2520exclusively%2520on%2520synthetic%2520head%250Aimages%252C%2520while%2520the%2520GS-LRM%2520reconstructor%2520undergoes%2520initial%2520training%2520on%2520Objaverse%250Afollowed%2520by%2520fine-tuning%2520on%2520synthetic%2520head%2520data.%2520FaceLift%2520excels%2520at%2520preserving%250Aidentity%2520and%2520maintaining%2520view%2520consistency%2520across%2520views.%2520Despite%2520being%2520trained%250Asolely%2520on%2520synthetic%2520data%252C%2520FaceLift%2520demonstrates%2520remarkable%2520generalization%2520to%250Areal-world%2520images.%2520Through%2520extensive%2520qualitative%2520and%2520quantitative%2520evaluations%252C%250Awe%2520show%2520that%2520FaceLift%2520outperforms%2520state-of-the-art%2520methods%2520in%25203D%2520head%250Areconstruction%252C%2520highlighting%2520its%2520practical%2520applicability%2520and%2520robust%2520performance%250Aon%2520real-world%2520images.%2520In%2520addition%2520to%2520single%2520image%2520reconstruction%252C%2520FaceLift%250Asupports%2520video%2520inputs%2520for%25204D%2520novel%2520view%2520synthesis%2520and%2520seamlessly%2520integrates%250Awith%25202D%2520reanimation%2520techniques%2520to%2520enable%25203D%2520facial%2520animation.%2520Project%2520page%253A%250Ahttps%253A//weijielyu.github.io/FaceLift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceLift%3A%20Single%20Image%20to%203D%20Head%20with%20View%20Generation%20and%20GS-LRM&entry.906535625=Weijie%20Lyu%20and%20Yi%20Zhou%20and%20Ming-Hsuan%20Yang%20and%20Zhixin%20Shu&entry.1292438233=%20%20We%20present%20FaceLift%2C%20a%20feed-forward%20approach%20for%20rapid%2C%20high-quality%2C%0A360-degree%20head%20reconstruction%20from%20a%20single%20image.%20Our%20pipeline%20begins%20by%0Aemploying%20a%20multi-view%20latent%20diffusion%20model%20that%20generates%20consistent%20side%0Aand%20back%20views%20of%20the%20head%20from%20a%20single%20facial%20input.%20These%20generated%20views%0Athen%20serve%20as%20input%20to%20a%20GS-LRM%20reconstructor%2C%20which%20produces%20a%20comprehensive%0A3D%20representation%20using%20Gaussian%20splats.%20To%20train%20our%20system%2C%20we%20develop%20a%0Adataset%20of%20multi-view%20renderings%20using%20synthetic%203D%20human%20head%20as-sets.%20The%0Adiffusion-based%20multi-view%20generator%20is%20trained%20exclusively%20on%20synthetic%20head%0Aimages%2C%20while%20the%20GS-LRM%20reconstructor%20undergoes%20initial%20training%20on%20Objaverse%0Afollowed%20by%20fine-tuning%20on%20synthetic%20head%20data.%20FaceLift%20excels%20at%20preserving%0Aidentity%20and%20maintaining%20view%20consistency%20across%20views.%20Despite%20being%20trained%0Asolely%20on%20synthetic%20data%2C%20FaceLift%20demonstrates%20remarkable%20generalization%20to%0Areal-world%20images.%20Through%20extensive%20qualitative%20and%20quantitative%20evaluations%2C%0Awe%20show%20that%20FaceLift%20outperforms%20state-of-the-art%20methods%20in%203D%20head%0Areconstruction%2C%20highlighting%20its%20practical%20applicability%20and%20robust%20performance%0Aon%20real-world%20images.%20In%20addition%20to%20single%20image%20reconstruction%2C%20FaceLift%0Asupports%20video%20inputs%20for%204D%20novel%20view%20synthesis%20and%20seamlessly%20integrates%0Awith%202D%20reanimation%20techniques%20to%20enable%203D%20facial%20animation.%20Project%20page%3A%0Ahttps%3A//weijielyu.github.io/FaceLift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17812v1&entry.124074799=Read"},
{"title": "DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions", "author": "Sammy Christen and Shreyas Hampali and Fadime Sener and Edoardo Remelli and Tomas Hodan and Eric Sauser and Shugao Ma and Bugra Tekin", "abstract": "  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. In\nthis paper, we propose a novel method, dubbed DiffH2O, which can synthesize\nrealistic, one or two-handed object interactions from provided text prompts and\ngeometry of the object. The method introduces three techniques that enable\neffective learning from limited data. First, we decompose the task into a\ngrasping stage and an text-based manipulation stage and use separate diffusion\nmodels for each. In the grasping stage, the model only generates hand motions,\nwhereas in the manipulation phase both hand and object poses are synthesized.\nSecond, we propose a compact representation that tightly couples hand and\nobject poses and helps in generating realistic hand-object interactions. Third,\nwe propose two different guidance schemes to allow more control of the\ngenerated motions: grasp guidance and detailed textual guidance. Grasp guidance\ntakes a single target grasping pose and guides the diffusion model to reach\nthis grasp at the end of the grasping stage, which provides control over the\ngrasping pose. Given a grasping motion from this stage, multiple different\nactions can be prompted in the manipulation phase. For the textual guidance, we\ncontribute comprehensive text descriptions to the GRAB dataset and show that\nthey enable our method to have more fine-grained control over hand-object\ninteractions. Our quantitative and qualitative evaluation demonstrates that the\nproposed method outperforms baseline methods and leads to natural hand-object\nmotions.\n", "link": "http://arxiv.org/abs/2403.17827v2", "date": "2024-12-23", "relevancy": 3.2189, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6831}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6425}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffH2O%3A%20Diffusion-Based%20Synthesis%20of%20Hand-Object%20Interactions%20from%0A%20%20Textual%20Descriptions&body=Title%3A%20DiffH2O%3A%20Diffusion-Based%20Synthesis%20of%20Hand-Object%20Interactions%20from%0A%20%20Textual%20Descriptions%0AAuthor%3A%20Sammy%20Christen%20and%20Shreyas%20Hampali%20and%20Fadime%20Sener%20and%20Edoardo%20Remelli%20and%20Tomas%20Hodan%20and%20Eric%20Sauser%20and%20Shugao%20Ma%20and%20Bugra%20Tekin%0AAbstract%3A%20%20%20Generating%20natural%20hand-object%20interactions%20in%203D%20is%20challenging%20as%20the%0Aresulting%20hand%20and%20object%20motions%20are%20expected%20to%20be%20physically%20plausible%20and%0Asemantically%20meaningful.%20Furthermore%2C%20generalization%20to%20unseen%20objects%20is%0Ahindered%20by%20the%20limited%20scale%20of%20available%20hand-object%20interaction%20datasets.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20method%2C%20dubbed%20DiffH2O%2C%20which%20can%20synthesize%0Arealistic%2C%20one%20or%20two-handed%20object%20interactions%20from%20provided%20text%20prompts%20and%0Ageometry%20of%20the%20object.%20The%20method%20introduces%20three%20techniques%20that%20enable%0Aeffective%20learning%20from%20limited%20data.%20First%2C%20we%20decompose%20the%20task%20into%20a%0Agrasping%20stage%20and%20an%20text-based%20manipulation%20stage%20and%20use%20separate%20diffusion%0Amodels%20for%20each.%20In%20the%20grasping%20stage%2C%20the%20model%20only%20generates%20hand%20motions%2C%0Awhereas%20in%20the%20manipulation%20phase%20both%20hand%20and%20object%20poses%20are%20synthesized.%0ASecond%2C%20we%20propose%20a%20compact%20representation%20that%20tightly%20couples%20hand%20and%0Aobject%20poses%20and%20helps%20in%20generating%20realistic%20hand-object%20interactions.%20Third%2C%0Awe%20propose%20two%20different%20guidance%20schemes%20to%20allow%20more%20control%20of%20the%0Agenerated%20motions%3A%20grasp%20guidance%20and%20detailed%20textual%20guidance.%20Grasp%20guidance%0Atakes%20a%20single%20target%20grasping%20pose%20and%20guides%20the%20diffusion%20model%20to%20reach%0Athis%20grasp%20at%20the%20end%20of%20the%20grasping%20stage%2C%20which%20provides%20control%20over%20the%0Agrasping%20pose.%20Given%20a%20grasping%20motion%20from%20this%20stage%2C%20multiple%20different%0Aactions%20can%20be%20prompted%20in%20the%20manipulation%20phase.%20For%20the%20textual%20guidance%2C%20we%0Acontribute%20comprehensive%20text%20descriptions%20to%20the%20GRAB%20dataset%20and%20show%20that%0Athey%20enable%20our%20method%20to%20have%20more%20fine-grained%20control%20over%20hand-object%0Ainteractions.%20Our%20quantitative%20and%20qualitative%20evaluation%20demonstrates%20that%20the%0Aproposed%20method%20outperforms%20baseline%20methods%20and%20leads%20to%20natural%20hand-object%0Amotions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17827v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffH2O%253A%2520Diffusion-Based%2520Synthesis%2520of%2520Hand-Object%2520Interactions%2520from%250A%2520%2520Textual%2520Descriptions%26entry.906535625%3DSammy%2520Christen%2520and%2520Shreyas%2520Hampali%2520and%2520Fadime%2520Sener%2520and%2520Edoardo%2520Remelli%2520and%2520Tomas%2520Hodan%2520and%2520Eric%2520Sauser%2520and%2520Shugao%2520Ma%2520and%2520Bugra%2520Tekin%26entry.1292438233%3D%2520%2520Generating%2520natural%2520hand-object%2520interactions%2520in%25203D%2520is%2520challenging%2520as%2520the%250Aresulting%2520hand%2520and%2520object%2520motions%2520are%2520expected%2520to%2520be%2520physically%2520plausible%2520and%250Asemantically%2520meaningful.%2520Furthermore%252C%2520generalization%2520to%2520unseen%2520objects%2520is%250Ahindered%2520by%2520the%2520limited%2520scale%2520of%2520available%2520hand-object%2520interaction%2520datasets.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520dubbed%2520DiffH2O%252C%2520which%2520can%2520synthesize%250Arealistic%252C%2520one%2520or%2520two-handed%2520object%2520interactions%2520from%2520provided%2520text%2520prompts%2520and%250Ageometry%2520of%2520the%2520object.%2520The%2520method%2520introduces%2520three%2520techniques%2520that%2520enable%250Aeffective%2520learning%2520from%2520limited%2520data.%2520First%252C%2520we%2520decompose%2520the%2520task%2520into%2520a%250Agrasping%2520stage%2520and%2520an%2520text-based%2520manipulation%2520stage%2520and%2520use%2520separate%2520diffusion%250Amodels%2520for%2520each.%2520In%2520the%2520grasping%2520stage%252C%2520the%2520model%2520only%2520generates%2520hand%2520motions%252C%250Awhereas%2520in%2520the%2520manipulation%2520phase%2520both%2520hand%2520and%2520object%2520poses%2520are%2520synthesized.%250ASecond%252C%2520we%2520propose%2520a%2520compact%2520representation%2520that%2520tightly%2520couples%2520hand%2520and%250Aobject%2520poses%2520and%2520helps%2520in%2520generating%2520realistic%2520hand-object%2520interactions.%2520Third%252C%250Awe%2520propose%2520two%2520different%2520guidance%2520schemes%2520to%2520allow%2520more%2520control%2520of%2520the%250Agenerated%2520motions%253A%2520grasp%2520guidance%2520and%2520detailed%2520textual%2520guidance.%2520Grasp%2520guidance%250Atakes%2520a%2520single%2520target%2520grasping%2520pose%2520and%2520guides%2520the%2520diffusion%2520model%2520to%2520reach%250Athis%2520grasp%2520at%2520the%2520end%2520of%2520the%2520grasping%2520stage%252C%2520which%2520provides%2520control%2520over%2520the%250Agrasping%2520pose.%2520Given%2520a%2520grasping%2520motion%2520from%2520this%2520stage%252C%2520multiple%2520different%250Aactions%2520can%2520be%2520prompted%2520in%2520the%2520manipulation%2520phase.%2520For%2520the%2520textual%2520guidance%252C%2520we%250Acontribute%2520comprehensive%2520text%2520descriptions%2520to%2520the%2520GRAB%2520dataset%2520and%2520show%2520that%250Athey%2520enable%2520our%2520method%2520to%2520have%2520more%2520fine-grained%2520control%2520over%2520hand-object%250Ainteractions.%2520Our%2520quantitative%2520and%2520qualitative%2520evaluation%2520demonstrates%2520that%2520the%250Aproposed%2520method%2520outperforms%2520baseline%2520methods%2520and%2520leads%2520to%2520natural%2520hand-object%250Amotions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17827v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffH2O%3A%20Diffusion-Based%20Synthesis%20of%20Hand-Object%20Interactions%20from%0A%20%20Textual%20Descriptions&entry.906535625=Sammy%20Christen%20and%20Shreyas%20Hampali%20and%20Fadime%20Sener%20and%20Edoardo%20Remelli%20and%20Tomas%20Hodan%20and%20Eric%20Sauser%20and%20Shugao%20Ma%20and%20Bugra%20Tekin&entry.1292438233=%20%20Generating%20natural%20hand-object%20interactions%20in%203D%20is%20challenging%20as%20the%0Aresulting%20hand%20and%20object%20motions%20are%20expected%20to%20be%20physically%20plausible%20and%0Asemantically%20meaningful.%20Furthermore%2C%20generalization%20to%20unseen%20objects%20is%0Ahindered%20by%20the%20limited%20scale%20of%20available%20hand-object%20interaction%20datasets.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20method%2C%20dubbed%20DiffH2O%2C%20which%20can%20synthesize%0Arealistic%2C%20one%20or%20two-handed%20object%20interactions%20from%20provided%20text%20prompts%20and%0Ageometry%20of%20the%20object.%20The%20method%20introduces%20three%20techniques%20that%20enable%0Aeffective%20learning%20from%20limited%20data.%20First%2C%20we%20decompose%20the%20task%20into%20a%0Agrasping%20stage%20and%20an%20text-based%20manipulation%20stage%20and%20use%20separate%20diffusion%0Amodels%20for%20each.%20In%20the%20grasping%20stage%2C%20the%20model%20only%20generates%20hand%20motions%2C%0Awhereas%20in%20the%20manipulation%20phase%20both%20hand%20and%20object%20poses%20are%20synthesized.%0ASecond%2C%20we%20propose%20a%20compact%20representation%20that%20tightly%20couples%20hand%20and%0Aobject%20poses%20and%20helps%20in%20generating%20realistic%20hand-object%20interactions.%20Third%2C%0Awe%20propose%20two%20different%20guidance%20schemes%20to%20allow%20more%20control%20of%20the%0Agenerated%20motions%3A%20grasp%20guidance%20and%20detailed%20textual%20guidance.%20Grasp%20guidance%0Atakes%20a%20single%20target%20grasping%20pose%20and%20guides%20the%20diffusion%20model%20to%20reach%0Athis%20grasp%20at%20the%20end%20of%20the%20grasping%20stage%2C%20which%20provides%20control%20over%20the%0Agrasping%20pose.%20Given%20a%20grasping%20motion%20from%20this%20stage%2C%20multiple%20different%0Aactions%20can%20be%20prompted%20in%20the%20manipulation%20phase.%20For%20the%20textual%20guidance%2C%20we%0Acontribute%20comprehensive%20text%20descriptions%20to%20the%20GRAB%20dataset%20and%20show%20that%0Athey%20enable%20our%20method%20to%20have%20more%20fine-grained%20control%20over%20hand-object%0Ainteractions.%20Our%20quantitative%20and%20qualitative%20evaluation%20demonstrates%20that%20the%0Aproposed%20method%20outperforms%20baseline%20methods%20and%20leads%20to%20natural%20hand-object%0Amotions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17827v2&entry.124074799=Read"},
{"title": "InfoGaussian: Structure-Aware Dynamic Gaussians through Lightweight\n  Information Shaping", "author": "Yunchao Zhang and Guandao Yang and Leonidas Guibas and Yanchao Yang", "abstract": "  3D Gaussians, as a low-level scene representation, typically involve\nthousands to millions of Gaussians. This makes it difficult to control the\nscene in ways that reflect the underlying dynamic structure, where the number\nof independent entities is typically much smaller. In particular, it can be\nchallenging to animate and move objects in the scene, which requires\ncoordination among many Gaussians. To address this issue, we develop a mutual\ninformation shaping technique that enforces movement resonance between\ncorrelated Gaussians in a motion network. Such correlations can be learned from\nputative 2D object masks in different views. By approximating the mutual\ninformation with the Jacobians of the motions, our method ensures consistent\nmovements of the Gaussians composing different objects under various\nperturbations. In particular, we develop an efficient contrastive training\npipeline with lightweight optimization to shape the motion network, avoiding\nthe need for re-shaping throughout the motion sequence. Notably, our training\nonly touches a small fraction of all Gaussians in the scene yet attains the\ndesired compositional behavior according to the underlying dynamic structure.\nThe proposed technique is evaluated on challenging scenes and demonstrates\nsignificant performance improvement in promoting consistent movements and 3D\nobject segmentation while inducing low computation and memory requirements.\n", "link": "http://arxiv.org/abs/2406.05897v2", "date": "2024-12-23", "relevancy": 3.2143, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6991}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6147}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoGaussian%3A%20Structure-Aware%20Dynamic%20Gaussians%20through%20Lightweight%0A%20%20Information%20Shaping&body=Title%3A%20InfoGaussian%3A%20Structure-Aware%20Dynamic%20Gaussians%20through%20Lightweight%0A%20%20Information%20Shaping%0AAuthor%3A%20Yunchao%20Zhang%20and%20Guandao%20Yang%20and%20Leonidas%20Guibas%20and%20Yanchao%20Yang%0AAbstract%3A%20%20%203D%20Gaussians%2C%20as%20a%20low-level%20scene%20representation%2C%20typically%20involve%0Athousands%20to%20millions%20of%20Gaussians.%20This%20makes%20it%20difficult%20to%20control%20the%0Ascene%20in%20ways%20that%20reflect%20the%20underlying%20dynamic%20structure%2C%20where%20the%20number%0Aof%20independent%20entities%20is%20typically%20much%20smaller.%20In%20particular%2C%20it%20can%20be%0Achallenging%20to%20animate%20and%20move%20objects%20in%20the%20scene%2C%20which%20requires%0Acoordination%20among%20many%20Gaussians.%20To%20address%20this%20issue%2C%20we%20develop%20a%20mutual%0Ainformation%20shaping%20technique%20that%20enforces%20movement%20resonance%20between%0Acorrelated%20Gaussians%20in%20a%20motion%20network.%20Such%20correlations%20can%20be%20learned%20from%0Aputative%202D%20object%20masks%20in%20different%20views.%20By%20approximating%20the%20mutual%0Ainformation%20with%20the%20Jacobians%20of%20the%20motions%2C%20our%20method%20ensures%20consistent%0Amovements%20of%20the%20Gaussians%20composing%20different%20objects%20under%20various%0Aperturbations.%20In%20particular%2C%20we%20develop%20an%20efficient%20contrastive%20training%0Apipeline%20with%20lightweight%20optimization%20to%20shape%20the%20motion%20network%2C%20avoiding%0Athe%20need%20for%20re-shaping%20throughout%20the%20motion%20sequence.%20Notably%2C%20our%20training%0Aonly%20touches%20a%20small%20fraction%20of%20all%20Gaussians%20in%20the%20scene%20yet%20attains%20the%0Adesired%20compositional%20behavior%20according%20to%20the%20underlying%20dynamic%20structure.%0AThe%20proposed%20technique%20is%20evaluated%20on%20challenging%20scenes%20and%20demonstrates%0Asignificant%20performance%20improvement%20in%20promoting%20consistent%20movements%20and%203D%0Aobject%20segmentation%20while%20inducing%20low%20computation%20and%20memory%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05897v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoGaussian%253A%2520Structure-Aware%2520Dynamic%2520Gaussians%2520through%2520Lightweight%250A%2520%2520Information%2520Shaping%26entry.906535625%3DYunchao%2520Zhang%2520and%2520Guandao%2520Yang%2520and%2520Leonidas%2520Guibas%2520and%2520Yanchao%2520Yang%26entry.1292438233%3D%2520%25203D%2520Gaussians%252C%2520as%2520a%2520low-level%2520scene%2520representation%252C%2520typically%2520involve%250Athousands%2520to%2520millions%2520of%2520Gaussians.%2520This%2520makes%2520it%2520difficult%2520to%2520control%2520the%250Ascene%2520in%2520ways%2520that%2520reflect%2520the%2520underlying%2520dynamic%2520structure%252C%2520where%2520the%2520number%250Aof%2520independent%2520entities%2520is%2520typically%2520much%2520smaller.%2520In%2520particular%252C%2520it%2520can%2520be%250Achallenging%2520to%2520animate%2520and%2520move%2520objects%2520in%2520the%2520scene%252C%2520which%2520requires%250Acoordination%2520among%2520many%2520Gaussians.%2520To%2520address%2520this%2520issue%252C%2520we%2520develop%2520a%2520mutual%250Ainformation%2520shaping%2520technique%2520that%2520enforces%2520movement%2520resonance%2520between%250Acorrelated%2520Gaussians%2520in%2520a%2520motion%2520network.%2520Such%2520correlations%2520can%2520be%2520learned%2520from%250Aputative%25202D%2520object%2520masks%2520in%2520different%2520views.%2520By%2520approximating%2520the%2520mutual%250Ainformation%2520with%2520the%2520Jacobians%2520of%2520the%2520motions%252C%2520our%2520method%2520ensures%2520consistent%250Amovements%2520of%2520the%2520Gaussians%2520composing%2520different%2520objects%2520under%2520various%250Aperturbations.%2520In%2520particular%252C%2520we%2520develop%2520an%2520efficient%2520contrastive%2520training%250Apipeline%2520with%2520lightweight%2520optimization%2520to%2520shape%2520the%2520motion%2520network%252C%2520avoiding%250Athe%2520need%2520for%2520re-shaping%2520throughout%2520the%2520motion%2520sequence.%2520Notably%252C%2520our%2520training%250Aonly%2520touches%2520a%2520small%2520fraction%2520of%2520all%2520Gaussians%2520in%2520the%2520scene%2520yet%2520attains%2520the%250Adesired%2520compositional%2520behavior%2520according%2520to%2520the%2520underlying%2520dynamic%2520structure.%250AThe%2520proposed%2520technique%2520is%2520evaluated%2520on%2520challenging%2520scenes%2520and%2520demonstrates%250Asignificant%2520performance%2520improvement%2520in%2520promoting%2520consistent%2520movements%2520and%25203D%250Aobject%2520segmentation%2520while%2520inducing%2520low%2520computation%2520and%2520memory%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05897v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoGaussian%3A%20Structure-Aware%20Dynamic%20Gaussians%20through%20Lightweight%0A%20%20Information%20Shaping&entry.906535625=Yunchao%20Zhang%20and%20Guandao%20Yang%20and%20Leonidas%20Guibas%20and%20Yanchao%20Yang&entry.1292438233=%20%203D%20Gaussians%2C%20as%20a%20low-level%20scene%20representation%2C%20typically%20involve%0Athousands%20to%20millions%20of%20Gaussians.%20This%20makes%20it%20difficult%20to%20control%20the%0Ascene%20in%20ways%20that%20reflect%20the%20underlying%20dynamic%20structure%2C%20where%20the%20number%0Aof%20independent%20entities%20is%20typically%20much%20smaller.%20In%20particular%2C%20it%20can%20be%0Achallenging%20to%20animate%20and%20move%20objects%20in%20the%20scene%2C%20which%20requires%0Acoordination%20among%20many%20Gaussians.%20To%20address%20this%20issue%2C%20we%20develop%20a%20mutual%0Ainformation%20shaping%20technique%20that%20enforces%20movement%20resonance%20between%0Acorrelated%20Gaussians%20in%20a%20motion%20network.%20Such%20correlations%20can%20be%20learned%20from%0Aputative%202D%20object%20masks%20in%20different%20views.%20By%20approximating%20the%20mutual%0Ainformation%20with%20the%20Jacobians%20of%20the%20motions%2C%20our%20method%20ensures%20consistent%0Amovements%20of%20the%20Gaussians%20composing%20different%20objects%20under%20various%0Aperturbations.%20In%20particular%2C%20we%20develop%20an%20efficient%20contrastive%20training%0Apipeline%20with%20lightweight%20optimization%20to%20shape%20the%20motion%20network%2C%20avoiding%0Athe%20need%20for%20re-shaping%20throughout%20the%20motion%20sequence.%20Notably%2C%20our%20training%0Aonly%20touches%20a%20small%20fraction%20of%20all%20Gaussians%20in%20the%20scene%20yet%20attains%20the%0Adesired%20compositional%20behavior%20according%20to%20the%20underlying%20dynamic%20structure.%0AThe%20proposed%20technique%20is%20evaluated%20on%20challenging%20scenes%20and%20demonstrates%0Asignificant%20performance%20improvement%20in%20promoting%20consistent%20movements%20and%203D%0Aobject%20segmentation%20while%20inducing%20low%20computation%20and%20memory%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05897v2&entry.124074799=Read"},
{"title": "LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding", "author": "Hao Li and Roy Qin and Zhengyu Zou and Diqi He and Bohan Li and Bingquan Dai and Dingewn Zhang and Junwei Han", "abstract": "  Applying Gaussian Splatting to perception tasks for 3D scene understanding is\nbecoming increasingly popular. Most existing works primarily focus on rendering\n2D feature maps from novel viewpoints, which leads to an imprecise 3D language\nfield with outlier languages, ultimately failing to align objects in 3D space.\nBy utilizing masked images for feature extraction, these approaches also lack\nessential contextual information, leading to inaccurate feature representation.\nTo this end, we propose a Language-Embedded Surface Field (LangSurf), which\naccurately aligns the 3D language fields with the surface of objects,\nfacilitating precise 2D and 3D segmentation with text query, widely expanding\nthe downstream tasks such as removal and editing. The core of LangSurf is a\njoint training strategy that flattens the language Gaussian on the object\nsurfaces using geometry supervision and contrastive losses to assign accurate\nlanguage features to the Gaussians of objects. In addition, we also introduce\nthe Hierarchical-Context Awareness Module to extract features at the image\nlevel for contextual information then perform hierarchical mask pooling using\nmasks segmented by SAM to obtain fine-grained language features in different\nhierarchies. Extensive experiments on open-vocabulary 2D and 3D semantic\nsegmentation demonstrate that LangSurf outperforms the previous\nstate-of-the-art method LangSplat by a large margin. As shown in\nFig.~\\ref{fig:teaser}, our method is capable of segmenting objects in 3D space,\nthus boosting the effectiveness of our approach in instance recognition,\nremoval, and editing, which is also supported by comprehensive experiments.\n\\url{https://langsurf.github.io}{Project Page}.\n", "link": "http://arxiv.org/abs/2412.17635v1", "date": "2024-12-23", "relevancy": 3.091, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6354}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6282}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangSurf%3A%20Language-Embedded%20Surface%20Gaussians%20for%203D%20Scene%20Understanding&body=Title%3A%20LangSurf%3A%20Language-Embedded%20Surface%20Gaussians%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Hao%20Li%20and%20Roy%20Qin%20and%20Zhengyu%20Zou%20and%20Diqi%20He%20and%20Bohan%20Li%20and%20Bingquan%20Dai%20and%20Dingewn%20Zhang%20and%20Junwei%20Han%0AAbstract%3A%20%20%20Applying%20Gaussian%20Splatting%20to%20perception%20tasks%20for%203D%20scene%20understanding%20is%0Abecoming%20increasingly%20popular.%20Most%20existing%20works%20primarily%20focus%20on%20rendering%0A2D%20feature%20maps%20from%20novel%20viewpoints%2C%20which%20leads%20to%20an%20imprecise%203D%20language%0Afield%20with%20outlier%20languages%2C%20ultimately%20failing%20to%20align%20objects%20in%203D%20space.%0ABy%20utilizing%20masked%20images%20for%20feature%20extraction%2C%20these%20approaches%20also%20lack%0Aessential%20contextual%20information%2C%20leading%20to%20inaccurate%20feature%20representation.%0ATo%20this%20end%2C%20we%20propose%20a%20Language-Embedded%20Surface%20Field%20%28LangSurf%29%2C%20which%0Aaccurately%20aligns%20the%203D%20language%20fields%20with%20the%20surface%20of%20objects%2C%0Afacilitating%20precise%202D%20and%203D%20segmentation%20with%20text%20query%2C%20widely%20expanding%0Athe%20downstream%20tasks%20such%20as%20removal%20and%20editing.%20The%20core%20of%20LangSurf%20is%20a%0Ajoint%20training%20strategy%20that%20flattens%20the%20language%20Gaussian%20on%20the%20object%0Asurfaces%20using%20geometry%20supervision%20and%20contrastive%20losses%20to%20assign%20accurate%0Alanguage%20features%20to%20the%20Gaussians%20of%20objects.%20In%20addition%2C%20we%20also%20introduce%0Athe%20Hierarchical-Context%20Awareness%20Module%20to%20extract%20features%20at%20the%20image%0Alevel%20for%20contextual%20information%20then%20perform%20hierarchical%20mask%20pooling%20using%0Amasks%20segmented%20by%20SAM%20to%20obtain%20fine-grained%20language%20features%20in%20different%0Ahierarchies.%20Extensive%20experiments%20on%20open-vocabulary%202D%20and%203D%20semantic%0Asegmentation%20demonstrate%20that%20LangSurf%20outperforms%20the%20previous%0Astate-of-the-art%20method%20LangSplat%20by%20a%20large%20margin.%20As%20shown%20in%0AFig.~%5Cref%7Bfig%3Ateaser%7D%2C%20our%20method%20is%20capable%20of%20segmenting%20objects%20in%203D%20space%2C%0Athus%20boosting%20the%20effectiveness%20of%20our%20approach%20in%20instance%20recognition%2C%0Aremoval%2C%20and%20editing%2C%20which%20is%20also%20supported%20by%20comprehensive%20experiments.%0A%5Curl%7Bhttps%3A//langsurf.github.io%7D%7BProject%20Page%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangSurf%253A%2520Language-Embedded%2520Surface%2520Gaussians%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DHao%2520Li%2520and%2520Roy%2520Qin%2520and%2520Zhengyu%2520Zou%2520and%2520Diqi%2520He%2520and%2520Bohan%2520Li%2520and%2520Bingquan%2520Dai%2520and%2520Dingewn%2520Zhang%2520and%2520Junwei%2520Han%26entry.1292438233%3D%2520%2520Applying%2520Gaussian%2520Splatting%2520to%2520perception%2520tasks%2520for%25203D%2520scene%2520understanding%2520is%250Abecoming%2520increasingly%2520popular.%2520Most%2520existing%2520works%2520primarily%2520focus%2520on%2520rendering%250A2D%2520feature%2520maps%2520from%2520novel%2520viewpoints%252C%2520which%2520leads%2520to%2520an%2520imprecise%25203D%2520language%250Afield%2520with%2520outlier%2520languages%252C%2520ultimately%2520failing%2520to%2520align%2520objects%2520in%25203D%2520space.%250ABy%2520utilizing%2520masked%2520images%2520for%2520feature%2520extraction%252C%2520these%2520approaches%2520also%2520lack%250Aessential%2520contextual%2520information%252C%2520leading%2520to%2520inaccurate%2520feature%2520representation.%250ATo%2520this%2520end%252C%2520we%2520propose%2520a%2520Language-Embedded%2520Surface%2520Field%2520%2528LangSurf%2529%252C%2520which%250Aaccurately%2520aligns%2520the%25203D%2520language%2520fields%2520with%2520the%2520surface%2520of%2520objects%252C%250Afacilitating%2520precise%25202D%2520and%25203D%2520segmentation%2520with%2520text%2520query%252C%2520widely%2520expanding%250Athe%2520downstream%2520tasks%2520such%2520as%2520removal%2520and%2520editing.%2520The%2520core%2520of%2520LangSurf%2520is%2520a%250Ajoint%2520training%2520strategy%2520that%2520flattens%2520the%2520language%2520Gaussian%2520on%2520the%2520object%250Asurfaces%2520using%2520geometry%2520supervision%2520and%2520contrastive%2520losses%2520to%2520assign%2520accurate%250Alanguage%2520features%2520to%2520the%2520Gaussians%2520of%2520objects.%2520In%2520addition%252C%2520we%2520also%2520introduce%250Athe%2520Hierarchical-Context%2520Awareness%2520Module%2520to%2520extract%2520features%2520at%2520the%2520image%250Alevel%2520for%2520contextual%2520information%2520then%2520perform%2520hierarchical%2520mask%2520pooling%2520using%250Amasks%2520segmented%2520by%2520SAM%2520to%2520obtain%2520fine-grained%2520language%2520features%2520in%2520different%250Ahierarchies.%2520Extensive%2520experiments%2520on%2520open-vocabulary%25202D%2520and%25203D%2520semantic%250Asegmentation%2520demonstrate%2520that%2520LangSurf%2520outperforms%2520the%2520previous%250Astate-of-the-art%2520method%2520LangSplat%2520by%2520a%2520large%2520margin.%2520As%2520shown%2520in%250AFig.~%255Cref%257Bfig%253Ateaser%257D%252C%2520our%2520method%2520is%2520capable%2520of%2520segmenting%2520objects%2520in%25203D%2520space%252C%250Athus%2520boosting%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520instance%2520recognition%252C%250Aremoval%252C%2520and%2520editing%252C%2520which%2520is%2520also%2520supported%2520by%2520comprehensive%2520experiments.%250A%255Curl%257Bhttps%253A//langsurf.github.io%257D%257BProject%2520Page%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangSurf%3A%20Language-Embedded%20Surface%20Gaussians%20for%203D%20Scene%20Understanding&entry.906535625=Hao%20Li%20and%20Roy%20Qin%20and%20Zhengyu%20Zou%20and%20Diqi%20He%20and%20Bohan%20Li%20and%20Bingquan%20Dai%20and%20Dingewn%20Zhang%20and%20Junwei%20Han&entry.1292438233=%20%20Applying%20Gaussian%20Splatting%20to%20perception%20tasks%20for%203D%20scene%20understanding%20is%0Abecoming%20increasingly%20popular.%20Most%20existing%20works%20primarily%20focus%20on%20rendering%0A2D%20feature%20maps%20from%20novel%20viewpoints%2C%20which%20leads%20to%20an%20imprecise%203D%20language%0Afield%20with%20outlier%20languages%2C%20ultimately%20failing%20to%20align%20objects%20in%203D%20space.%0ABy%20utilizing%20masked%20images%20for%20feature%20extraction%2C%20these%20approaches%20also%20lack%0Aessential%20contextual%20information%2C%20leading%20to%20inaccurate%20feature%20representation.%0ATo%20this%20end%2C%20we%20propose%20a%20Language-Embedded%20Surface%20Field%20%28LangSurf%29%2C%20which%0Aaccurately%20aligns%20the%203D%20language%20fields%20with%20the%20surface%20of%20objects%2C%0Afacilitating%20precise%202D%20and%203D%20segmentation%20with%20text%20query%2C%20widely%20expanding%0Athe%20downstream%20tasks%20such%20as%20removal%20and%20editing.%20The%20core%20of%20LangSurf%20is%20a%0Ajoint%20training%20strategy%20that%20flattens%20the%20language%20Gaussian%20on%20the%20object%0Asurfaces%20using%20geometry%20supervision%20and%20contrastive%20losses%20to%20assign%20accurate%0Alanguage%20features%20to%20the%20Gaussians%20of%20objects.%20In%20addition%2C%20we%20also%20introduce%0Athe%20Hierarchical-Context%20Awareness%20Module%20to%20extract%20features%20at%20the%20image%0Alevel%20for%20contextual%20information%20then%20perform%20hierarchical%20mask%20pooling%20using%0Amasks%20segmented%20by%20SAM%20to%20obtain%20fine-grained%20language%20features%20in%20different%0Ahierarchies.%20Extensive%20experiments%20on%20open-vocabulary%202D%20and%203D%20semantic%0Asegmentation%20demonstrate%20that%20LangSurf%20outperforms%20the%20previous%0Astate-of-the-art%20method%20LangSplat%20by%20a%20large%20margin.%20As%20shown%20in%0AFig.~%5Cref%7Bfig%3Ateaser%7D%2C%20our%20method%20is%20capable%20of%20segmenting%20objects%20in%203D%20space%2C%0Athus%20boosting%20the%20effectiveness%20of%20our%20approach%20in%20instance%20recognition%2C%0Aremoval%2C%20and%20editing%2C%20which%20is%20also%20supported%20by%20comprehensive%20experiments.%0A%5Curl%7Bhttps%3A//langsurf.github.io%7D%7BProject%20Page%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17635v1&entry.124074799=Read"},
{"title": "Cross-Lingual Text-Rich Visual Comprehension: An Information Theory\n  Perspective", "author": "Xinmiao Yu and Xiaocheng Feng and Yun Li and Minghui Liao and Ya-Qi Yu and Xiachong Feng and Weihong Zhong and Ruihan Chen and Mengkang Hu and Jihao Wu and Dandan Tu and Duyu Tang and Bing Qin", "abstract": "  Recent Large Vision-Language Models (LVLMs) have shown promising reasoning\ncapabilities on text-rich images from charts, tables, and documents. However,\nthe abundant text within such images may increase the model's sensitivity to\nlanguage. This raises the need to evaluate LVLM performance on cross-lingual\ntext-rich visual inputs, where the language in the image differs from the\nlanguage of the instructions. To address this, we introduce XT-VQA\n(Cross-Lingual Text-Rich Visual Question Answering), a benchmark designed to\nassess how LVLMs handle language inconsistency between image text and\nquestions. XT-VQA integrates five existing text-rich VQA datasets and a newly\ncollected dataset, XPaperQA, covering diverse scenarios that require faithful\nrecognition and comprehension of visual information despite language\ninconsistency. Our evaluation of prominent LVLMs on XT-VQA reveals a\nsignificant drop in performance for cross-lingual scenarios, even for models\nwith multilingual capabilities. A mutual information analysis suggests that\nthis performance gap stems from cross-lingual questions failing to adequately\nactivate relevant visual information. To mitigate this issue, we propose\nMVCL-MI (Maximization of Vision-Language Cross-Lingual Mutual Information),\nwhere a visual-text cross-lingual alignment is built by maximizing mutual\ninformation between the model's outputs and visual information. This is\nachieved by distilling knowledge from monolingual to cross-lingual settings\nthrough KL divergence minimization, where monolingual output logits serve as a\nteacher. Experimental results on the XT-VQA demonstrate that MVCL-MI\neffectively reduces the visual-text cross-lingual performance disparity while\npreserving the inherent capabilities of LVLMs, shedding new light on the\npotential practice for improving LVLMs. Codes are available at:\nhttps://github.com/Stardust-y/XTVQA.git\n", "link": "http://arxiv.org/abs/2412.17787v1", "date": "2024-12-23", "relevancy": 3.0629, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Lingual%20Text-Rich%20Visual%20Comprehension%3A%20An%20Information%20Theory%0A%20%20Perspective&body=Title%3A%20Cross-Lingual%20Text-Rich%20Visual%20Comprehension%3A%20An%20Information%20Theory%0A%20%20Perspective%0AAuthor%3A%20Xinmiao%20Yu%20and%20Xiaocheng%20Feng%20and%20Yun%20Li%20and%20Minghui%20Liao%20and%20Ya-Qi%20Yu%20and%20Xiachong%20Feng%20and%20Weihong%20Zhong%20and%20Ruihan%20Chen%20and%20Mengkang%20Hu%20and%20Jihao%20Wu%20and%20Dandan%20Tu%20and%20Duyu%20Tang%20and%20Bing%20Qin%0AAbstract%3A%20%20%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20shown%20promising%20reasoning%0Acapabilities%20on%20text-rich%20images%20from%20charts%2C%20tables%2C%20and%20documents.%20However%2C%0Athe%20abundant%20text%20within%20such%20images%20may%20increase%20the%20model%27s%20sensitivity%20to%0Alanguage.%20This%20raises%20the%20need%20to%20evaluate%20LVLM%20performance%20on%20cross-lingual%0Atext-rich%20visual%20inputs%2C%20where%20the%20language%20in%20the%20image%20differs%20from%20the%0Alanguage%20of%20the%20instructions.%20To%20address%20this%2C%20we%20introduce%20XT-VQA%0A%28Cross-Lingual%20Text-Rich%20Visual%20Question%20Answering%29%2C%20a%20benchmark%20designed%20to%0Aassess%20how%20LVLMs%20handle%20language%20inconsistency%20between%20image%20text%20and%0Aquestions.%20XT-VQA%20integrates%20five%20existing%20text-rich%20VQA%20datasets%20and%20a%20newly%0Acollected%20dataset%2C%20XPaperQA%2C%20covering%20diverse%20scenarios%20that%20require%20faithful%0Arecognition%20and%20comprehension%20of%20visual%20information%20despite%20language%0Ainconsistency.%20Our%20evaluation%20of%20prominent%20LVLMs%20on%20XT-VQA%20reveals%20a%0Asignificant%20drop%20in%20performance%20for%20cross-lingual%20scenarios%2C%20even%20for%20models%0Awith%20multilingual%20capabilities.%20A%20mutual%20information%20analysis%20suggests%20that%0Athis%20performance%20gap%20stems%20from%20cross-lingual%20questions%20failing%20to%20adequately%0Aactivate%20relevant%20visual%20information.%20To%20mitigate%20this%20issue%2C%20we%20propose%0AMVCL-MI%20%28Maximization%20of%20Vision-Language%20Cross-Lingual%20Mutual%20Information%29%2C%0Awhere%20a%20visual-text%20cross-lingual%20alignment%20is%20built%20by%20maximizing%20mutual%0Ainformation%20between%20the%20model%27s%20outputs%20and%20visual%20information.%20This%20is%0Aachieved%20by%20distilling%20knowledge%20from%20monolingual%20to%20cross-lingual%20settings%0Athrough%20KL%20divergence%20minimization%2C%20where%20monolingual%20output%20logits%20serve%20as%20a%0Ateacher.%20Experimental%20results%20on%20the%20XT-VQA%20demonstrate%20that%20MVCL-MI%0Aeffectively%20reduces%20the%20visual-text%20cross-lingual%20performance%20disparity%20while%0Apreserving%20the%20inherent%20capabilities%20of%20LVLMs%2C%20shedding%20new%20light%20on%20the%0Apotential%20practice%20for%20improving%20LVLMs.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/Stardust-y/XTVQA.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Lingual%2520Text-Rich%2520Visual%2520Comprehension%253A%2520An%2520Information%2520Theory%250A%2520%2520Perspective%26entry.906535625%3DXinmiao%2520Yu%2520and%2520Xiaocheng%2520Feng%2520and%2520Yun%2520Li%2520and%2520Minghui%2520Liao%2520and%2520Ya-Qi%2520Yu%2520and%2520Xiachong%2520Feng%2520and%2520Weihong%2520Zhong%2520and%2520Ruihan%2520Chen%2520and%2520Mengkang%2520Hu%2520and%2520Jihao%2520Wu%2520and%2520Dandan%2520Tu%2520and%2520Duyu%2520Tang%2520and%2520Bing%2520Qin%26entry.1292438233%3D%2520%2520Recent%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520shown%2520promising%2520reasoning%250Acapabilities%2520on%2520text-rich%2520images%2520from%2520charts%252C%2520tables%252C%2520and%2520documents.%2520However%252C%250Athe%2520abundant%2520text%2520within%2520such%2520images%2520may%2520increase%2520the%2520model%2527s%2520sensitivity%2520to%250Alanguage.%2520This%2520raises%2520the%2520need%2520to%2520evaluate%2520LVLM%2520performance%2520on%2520cross-lingual%250Atext-rich%2520visual%2520inputs%252C%2520where%2520the%2520language%2520in%2520the%2520image%2520differs%2520from%2520the%250Alanguage%2520of%2520the%2520instructions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520XT-VQA%250A%2528Cross-Lingual%2520Text-Rich%2520Visual%2520Question%2520Answering%2529%252C%2520a%2520benchmark%2520designed%2520to%250Aassess%2520how%2520LVLMs%2520handle%2520language%2520inconsistency%2520between%2520image%2520text%2520and%250Aquestions.%2520XT-VQA%2520integrates%2520five%2520existing%2520text-rich%2520VQA%2520datasets%2520and%2520a%2520newly%250Acollected%2520dataset%252C%2520XPaperQA%252C%2520covering%2520diverse%2520scenarios%2520that%2520require%2520faithful%250Arecognition%2520and%2520comprehension%2520of%2520visual%2520information%2520despite%2520language%250Ainconsistency.%2520Our%2520evaluation%2520of%2520prominent%2520LVLMs%2520on%2520XT-VQA%2520reveals%2520a%250Asignificant%2520drop%2520in%2520performance%2520for%2520cross-lingual%2520scenarios%252C%2520even%2520for%2520models%250Awith%2520multilingual%2520capabilities.%2520A%2520mutual%2520information%2520analysis%2520suggests%2520that%250Athis%2520performance%2520gap%2520stems%2520from%2520cross-lingual%2520questions%2520failing%2520to%2520adequately%250Aactivate%2520relevant%2520visual%2520information.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520propose%250AMVCL-MI%2520%2528Maximization%2520of%2520Vision-Language%2520Cross-Lingual%2520Mutual%2520Information%2529%252C%250Awhere%2520a%2520visual-text%2520cross-lingual%2520alignment%2520is%2520built%2520by%2520maximizing%2520mutual%250Ainformation%2520between%2520the%2520model%2527s%2520outputs%2520and%2520visual%2520information.%2520This%2520is%250Aachieved%2520by%2520distilling%2520knowledge%2520from%2520monolingual%2520to%2520cross-lingual%2520settings%250Athrough%2520KL%2520divergence%2520minimization%252C%2520where%2520monolingual%2520output%2520logits%2520serve%2520as%2520a%250Ateacher.%2520Experimental%2520results%2520on%2520the%2520XT-VQA%2520demonstrate%2520that%2520MVCL-MI%250Aeffectively%2520reduces%2520the%2520visual-text%2520cross-lingual%2520performance%2520disparity%2520while%250Apreserving%2520the%2520inherent%2520capabilities%2520of%2520LVLMs%252C%2520shedding%2520new%2520light%2520on%2520the%250Apotential%2520practice%2520for%2520improving%2520LVLMs.%2520Codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Stardust-y/XTVQA.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Lingual%20Text-Rich%20Visual%20Comprehension%3A%20An%20Information%20Theory%0A%20%20Perspective&entry.906535625=Xinmiao%20Yu%20and%20Xiaocheng%20Feng%20and%20Yun%20Li%20and%20Minghui%20Liao%20and%20Ya-Qi%20Yu%20and%20Xiachong%20Feng%20and%20Weihong%20Zhong%20and%20Ruihan%20Chen%20and%20Mengkang%20Hu%20and%20Jihao%20Wu%20and%20Dandan%20Tu%20and%20Duyu%20Tang%20and%20Bing%20Qin&entry.1292438233=%20%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20shown%20promising%20reasoning%0Acapabilities%20on%20text-rich%20images%20from%20charts%2C%20tables%2C%20and%20documents.%20However%2C%0Athe%20abundant%20text%20within%20such%20images%20may%20increase%20the%20model%27s%20sensitivity%20to%0Alanguage.%20This%20raises%20the%20need%20to%20evaluate%20LVLM%20performance%20on%20cross-lingual%0Atext-rich%20visual%20inputs%2C%20where%20the%20language%20in%20the%20image%20differs%20from%20the%0Alanguage%20of%20the%20instructions.%20To%20address%20this%2C%20we%20introduce%20XT-VQA%0A%28Cross-Lingual%20Text-Rich%20Visual%20Question%20Answering%29%2C%20a%20benchmark%20designed%20to%0Aassess%20how%20LVLMs%20handle%20language%20inconsistency%20between%20image%20text%20and%0Aquestions.%20XT-VQA%20integrates%20five%20existing%20text-rich%20VQA%20datasets%20and%20a%20newly%0Acollected%20dataset%2C%20XPaperQA%2C%20covering%20diverse%20scenarios%20that%20require%20faithful%0Arecognition%20and%20comprehension%20of%20visual%20information%20despite%20language%0Ainconsistency.%20Our%20evaluation%20of%20prominent%20LVLMs%20on%20XT-VQA%20reveals%20a%0Asignificant%20drop%20in%20performance%20for%20cross-lingual%20scenarios%2C%20even%20for%20models%0Awith%20multilingual%20capabilities.%20A%20mutual%20information%20analysis%20suggests%20that%0Athis%20performance%20gap%20stems%20from%20cross-lingual%20questions%20failing%20to%20adequately%0Aactivate%20relevant%20visual%20information.%20To%20mitigate%20this%20issue%2C%20we%20propose%0AMVCL-MI%20%28Maximization%20of%20Vision-Language%20Cross-Lingual%20Mutual%20Information%29%2C%0Awhere%20a%20visual-text%20cross-lingual%20alignment%20is%20built%20by%20maximizing%20mutual%0Ainformation%20between%20the%20model%27s%20outputs%20and%20visual%20information.%20This%20is%0Aachieved%20by%20distilling%20knowledge%20from%20monolingual%20to%20cross-lingual%20settings%0Athrough%20KL%20divergence%20minimization%2C%20where%20monolingual%20output%20logits%20serve%20as%20a%0Ateacher.%20Experimental%20results%20on%20the%20XT-VQA%20demonstrate%20that%20MVCL-MI%0Aeffectively%20reduces%20the%20visual-text%20cross-lingual%20performance%20disparity%20while%0Apreserving%20the%20inherent%20capabilities%20of%20LVLMs%2C%20shedding%20new%20light%20on%20the%0Apotential%20practice%20for%20improving%20LVLMs.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/Stardust-y/XTVQA.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17787v1&entry.124074799=Read"},
{"title": "Reconstructing People, Places, and Cameras", "author": "Lea M\u00fcller and Hongsuk Choi and Anthony Zhang and Brent Yi and Jitendra Malik and Angjoo Kanazawa", "abstract": "  We present \"Humans and Structure from Motion\" (HSfM), a method for jointly\nreconstructing multiple human meshes, scene point clouds, and camera parameters\nin a metric world coordinate system from a sparse set of uncalibrated\nmulti-view images featuring people. Our approach combines data-driven scene\nreconstruction with the traditional Structure-from-Motion (SfM) framework to\nachieve more accurate scene reconstruction and camera estimation, while\nsimultaneously recovering human meshes. In contrast to existing scene\nreconstruction and SfM methods that lack metric scale information, our method\nestimates approximate metric scale by leveraging a human statistical model.\nFurthermore, it reconstructs multiple human meshes within the same world\ncoordinate system alongside the scene point cloud, effectively capturing\nspatial relationships among individuals and their positions in the environment.\nWe initialize the reconstruction of humans, scenes, and cameras using robust\nfoundational models and jointly optimize these elements. This joint\noptimization synergistically improves the accuracy of each component. We\ncompare our method to existing approaches on two challenging benchmarks,\nEgoHumans and EgoExo4D, demonstrating significant improvements in human\nlocalization accuracy within the world coordinate frame (reducing error from\n3.51m to 1.04m in EgoHumans and from 2.9m to 0.56m in EgoExo4D). Notably, our\nresults show that incorporating human data into the SfM pipeline improves\ncamera pose estimation (e.g., increasing RRA@15 by 20.3% on EgoHumans).\nAdditionally, qualitative results show that our approach improves overall scene\nreconstruction quality. Our code is available at: muelea.github.io/hsfm.\n", "link": "http://arxiv.org/abs/2412.17806v1", "date": "2024-12-23", "relevancy": 3.0551, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6271}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6209}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20People%2C%20Places%2C%20and%20Cameras&body=Title%3A%20Reconstructing%20People%2C%20Places%2C%20and%20Cameras%0AAuthor%3A%20Lea%20M%C3%BCller%20and%20Hongsuk%20Choi%20and%20Anthony%20Zhang%20and%20Brent%20Yi%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20We%20present%20%22Humans%20and%20Structure%20from%20Motion%22%20%28HSfM%29%2C%20a%20method%20for%20jointly%0Areconstructing%20multiple%20human%20meshes%2C%20scene%20point%20clouds%2C%20and%20camera%20parameters%0Ain%20a%20metric%20world%20coordinate%20system%20from%20a%20sparse%20set%20of%20uncalibrated%0Amulti-view%20images%20featuring%20people.%20Our%20approach%20combines%20data-driven%20scene%0Areconstruction%20with%20the%20traditional%20Structure-from-Motion%20%28SfM%29%20framework%20to%0Aachieve%20more%20accurate%20scene%20reconstruction%20and%20camera%20estimation%2C%20while%0Asimultaneously%20recovering%20human%20meshes.%20In%20contrast%20to%20existing%20scene%0Areconstruction%20and%20SfM%20methods%20that%20lack%20metric%20scale%20information%2C%20our%20method%0Aestimates%20approximate%20metric%20scale%20by%20leveraging%20a%20human%20statistical%20model.%0AFurthermore%2C%20it%20reconstructs%20multiple%20human%20meshes%20within%20the%20same%20world%0Acoordinate%20system%20alongside%20the%20scene%20point%20cloud%2C%20effectively%20capturing%0Aspatial%20relationships%20among%20individuals%20and%20their%20positions%20in%20the%20environment.%0AWe%20initialize%20the%20reconstruction%20of%20humans%2C%20scenes%2C%20and%20cameras%20using%20robust%0Afoundational%20models%20and%20jointly%20optimize%20these%20elements.%20This%20joint%0Aoptimization%20synergistically%20improves%20the%20accuracy%20of%20each%20component.%20We%0Acompare%20our%20method%20to%20existing%20approaches%20on%20two%20challenging%20benchmarks%2C%0AEgoHumans%20and%20EgoExo4D%2C%20demonstrating%20significant%20improvements%20in%20human%0Alocalization%20accuracy%20within%20the%20world%20coordinate%20frame%20%28reducing%20error%20from%0A3.51m%20to%201.04m%20in%20EgoHumans%20and%20from%202.9m%20to%200.56m%20in%20EgoExo4D%29.%20Notably%2C%20our%0Aresults%20show%20that%20incorporating%20human%20data%20into%20the%20SfM%20pipeline%20improves%0Acamera%20pose%20estimation%20%28e.g.%2C%20increasing%20RRA%4015%20by%2020.3%25%20on%20EgoHumans%29.%0AAdditionally%2C%20qualitative%20results%20show%20that%20our%20approach%20improves%20overall%20scene%0Areconstruction%20quality.%20Our%20code%20is%20available%20at%3A%20muelea.github.io/hsfm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520People%252C%2520Places%252C%2520and%2520Cameras%26entry.906535625%3DLea%2520M%25C3%25BCller%2520and%2520Hongsuk%2520Choi%2520and%2520Anthony%2520Zhang%2520and%2520Brent%2520Yi%2520and%2520Jitendra%2520Malik%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520We%2520present%2520%2522Humans%2520and%2520Structure%2520from%2520Motion%2522%2520%2528HSfM%2529%252C%2520a%2520method%2520for%2520jointly%250Areconstructing%2520multiple%2520human%2520meshes%252C%2520scene%2520point%2520clouds%252C%2520and%2520camera%2520parameters%250Ain%2520a%2520metric%2520world%2520coordinate%2520system%2520from%2520a%2520sparse%2520set%2520of%2520uncalibrated%250Amulti-view%2520images%2520featuring%2520people.%2520Our%2520approach%2520combines%2520data-driven%2520scene%250Areconstruction%2520with%2520the%2520traditional%2520Structure-from-Motion%2520%2528SfM%2529%2520framework%2520to%250Aachieve%2520more%2520accurate%2520scene%2520reconstruction%2520and%2520camera%2520estimation%252C%2520while%250Asimultaneously%2520recovering%2520human%2520meshes.%2520In%2520contrast%2520to%2520existing%2520scene%250Areconstruction%2520and%2520SfM%2520methods%2520that%2520lack%2520metric%2520scale%2520information%252C%2520our%2520method%250Aestimates%2520approximate%2520metric%2520scale%2520by%2520leveraging%2520a%2520human%2520statistical%2520model.%250AFurthermore%252C%2520it%2520reconstructs%2520multiple%2520human%2520meshes%2520within%2520the%2520same%2520world%250Acoordinate%2520system%2520alongside%2520the%2520scene%2520point%2520cloud%252C%2520effectively%2520capturing%250Aspatial%2520relationships%2520among%2520individuals%2520and%2520their%2520positions%2520in%2520the%2520environment.%250AWe%2520initialize%2520the%2520reconstruction%2520of%2520humans%252C%2520scenes%252C%2520and%2520cameras%2520using%2520robust%250Afoundational%2520models%2520and%2520jointly%2520optimize%2520these%2520elements.%2520This%2520joint%250Aoptimization%2520synergistically%2520improves%2520the%2520accuracy%2520of%2520each%2520component.%2520We%250Acompare%2520our%2520method%2520to%2520existing%2520approaches%2520on%2520two%2520challenging%2520benchmarks%252C%250AEgoHumans%2520and%2520EgoExo4D%252C%2520demonstrating%2520significant%2520improvements%2520in%2520human%250Alocalization%2520accuracy%2520within%2520the%2520world%2520coordinate%2520frame%2520%2528reducing%2520error%2520from%250A3.51m%2520to%25201.04m%2520in%2520EgoHumans%2520and%2520from%25202.9m%2520to%25200.56m%2520in%2520EgoExo4D%2529.%2520Notably%252C%2520our%250Aresults%2520show%2520that%2520incorporating%2520human%2520data%2520into%2520the%2520SfM%2520pipeline%2520improves%250Acamera%2520pose%2520estimation%2520%2528e.g.%252C%2520increasing%2520RRA%254015%2520by%252020.3%2525%2520on%2520EgoHumans%2529.%250AAdditionally%252C%2520qualitative%2520results%2520show%2520that%2520our%2520approach%2520improves%2520overall%2520scene%250Areconstruction%2520quality.%2520Our%2520code%2520is%2520available%2520at%253A%2520muelea.github.io/hsfm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20People%2C%20Places%2C%20and%20Cameras&entry.906535625=Lea%20M%C3%BCller%20and%20Hongsuk%20Choi%20and%20Anthony%20Zhang%20and%20Brent%20Yi%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20We%20present%20%22Humans%20and%20Structure%20from%20Motion%22%20%28HSfM%29%2C%20a%20method%20for%20jointly%0Areconstructing%20multiple%20human%20meshes%2C%20scene%20point%20clouds%2C%20and%20camera%20parameters%0Ain%20a%20metric%20world%20coordinate%20system%20from%20a%20sparse%20set%20of%20uncalibrated%0Amulti-view%20images%20featuring%20people.%20Our%20approach%20combines%20data-driven%20scene%0Areconstruction%20with%20the%20traditional%20Structure-from-Motion%20%28SfM%29%20framework%20to%0Aachieve%20more%20accurate%20scene%20reconstruction%20and%20camera%20estimation%2C%20while%0Asimultaneously%20recovering%20human%20meshes.%20In%20contrast%20to%20existing%20scene%0Areconstruction%20and%20SfM%20methods%20that%20lack%20metric%20scale%20information%2C%20our%20method%0Aestimates%20approximate%20metric%20scale%20by%20leveraging%20a%20human%20statistical%20model.%0AFurthermore%2C%20it%20reconstructs%20multiple%20human%20meshes%20within%20the%20same%20world%0Acoordinate%20system%20alongside%20the%20scene%20point%20cloud%2C%20effectively%20capturing%0Aspatial%20relationships%20among%20individuals%20and%20their%20positions%20in%20the%20environment.%0AWe%20initialize%20the%20reconstruction%20of%20humans%2C%20scenes%2C%20and%20cameras%20using%20robust%0Afoundational%20models%20and%20jointly%20optimize%20these%20elements.%20This%20joint%0Aoptimization%20synergistically%20improves%20the%20accuracy%20of%20each%20component.%20We%0Acompare%20our%20method%20to%20existing%20approaches%20on%20two%20challenging%20benchmarks%2C%0AEgoHumans%20and%20EgoExo4D%2C%20demonstrating%20significant%20improvements%20in%20human%0Alocalization%20accuracy%20within%20the%20world%20coordinate%20frame%20%28reducing%20error%20from%0A3.51m%20to%201.04m%20in%20EgoHumans%20and%20from%202.9m%20to%200.56m%20in%20EgoExo4D%29.%20Notably%2C%20our%0Aresults%20show%20that%20incorporating%20human%20data%20into%20the%20SfM%20pipeline%20improves%0Acamera%20pose%20estimation%20%28e.g.%2C%20increasing%20RRA%4015%20by%2020.3%25%20on%20EgoHumans%29.%0AAdditionally%2C%20qualitative%20results%20show%20that%20our%20approach%20improves%20overall%20scene%0Areconstruction%20quality.%20Our%20code%20is%20available%20at%3A%20muelea.github.io/hsfm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17806v1&entry.124074799=Read"},
{"title": "GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal\n  Guidance", "author": "Jingqiu Zhou and Lue Fan and Xuesong Chen and Linjiang Huang and Si Liu and Hongsheng Li", "abstract": "  In this paper, we present GaussianPainter, the first method to paint a point\ncloud into 3D Gaussians given a reference image. GaussianPainter introduces an\ninnovative feed-forward approach to overcome the limitations of time-consuming\ntest-time optimization in 3D Gaussian splatting. Our method addresses a\ncritical challenge in the field: the non-uniqueness problem inherent in the\nlarge parameter space of 3D Gaussian splatting. This space, encompassing\nrotation, anisotropic scales, and spherical harmonic coefficients, introduces\nthe challenge of rendering similar images from substantially different Gaussian\nfields. As a result, feed-forward networks face instability when attempting to\ndirectly predict high-quality Gaussian fields, struggling to converge on\nconsistent parameters for a given output. To address this issue, we propose to\nestimate a surface normal for each point to determine its Gaussian rotation.\nThis strategy enables the network to effectively predict the remaining Gaussian\nparameters in the constrained space. We further enhance our approach with an\nappearance injection module, incorporating reference image appearance into\nGaussian fields via a multiscale triplane representation. Our method\nsuccessfully balances efficiency and fidelity in 3D Gaussian generation,\nachieving high-quality, diverse, and robust 3D content creation from point\nclouds in a single forward pass.\n", "link": "http://arxiv.org/abs/2412.17715v1", "date": "2024-12-23", "relevancy": 3.0454, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6247}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6018}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianPainter%3A%20Painting%20Point%20Cloud%20into%203D%20Gaussians%20with%20Normal%0A%20%20Guidance&body=Title%3A%20GaussianPainter%3A%20Painting%20Point%20Cloud%20into%203D%20Gaussians%20with%20Normal%0A%20%20Guidance%0AAuthor%3A%20Jingqiu%20Zhou%20and%20Lue%20Fan%20and%20Xuesong%20Chen%20and%20Linjiang%20Huang%20and%20Si%20Liu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20GaussianPainter%2C%20the%20first%20method%20to%20paint%20a%20point%0Acloud%20into%203D%20Gaussians%20given%20a%20reference%20image.%20GaussianPainter%20introduces%20an%0Ainnovative%20feed-forward%20approach%20to%20overcome%20the%20limitations%20of%20time-consuming%0Atest-time%20optimization%20in%203D%20Gaussian%20splatting.%20Our%20method%20addresses%20a%0Acritical%20challenge%20in%20the%20field%3A%20the%20non-uniqueness%20problem%20inherent%20in%20the%0Alarge%20parameter%20space%20of%203D%20Gaussian%20splatting.%20This%20space%2C%20encompassing%0Arotation%2C%20anisotropic%20scales%2C%20and%20spherical%20harmonic%20coefficients%2C%20introduces%0Athe%20challenge%20of%20rendering%20similar%20images%20from%20substantially%20different%20Gaussian%0Afields.%20As%20a%20result%2C%20feed-forward%20networks%20face%20instability%20when%20attempting%20to%0Adirectly%20predict%20high-quality%20Gaussian%20fields%2C%20struggling%20to%20converge%20on%0Aconsistent%20parameters%20for%20a%20given%20output.%20To%20address%20this%20issue%2C%20we%20propose%20to%0Aestimate%20a%20surface%20normal%20for%20each%20point%20to%20determine%20its%20Gaussian%20rotation.%0AThis%20strategy%20enables%20the%20network%20to%20effectively%20predict%20the%20remaining%20Gaussian%0Aparameters%20in%20the%20constrained%20space.%20We%20further%20enhance%20our%20approach%20with%20an%0Aappearance%20injection%20module%2C%20incorporating%20reference%20image%20appearance%20into%0AGaussian%20fields%20via%20a%20multiscale%20triplane%20representation.%20Our%20method%0Asuccessfully%20balances%20efficiency%20and%20fidelity%20in%203D%20Gaussian%20generation%2C%0Aachieving%20high-quality%2C%20diverse%2C%20and%20robust%203D%20content%20creation%20from%20point%0Aclouds%20in%20a%20single%20forward%20pass.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianPainter%253A%2520Painting%2520Point%2520Cloud%2520into%25203D%2520Gaussians%2520with%2520Normal%250A%2520%2520Guidance%26entry.906535625%3DJingqiu%2520Zhou%2520and%2520Lue%2520Fan%2520and%2520Xuesong%2520Chen%2520and%2520Linjiang%2520Huang%2520and%2520Si%2520Liu%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520GaussianPainter%252C%2520the%2520first%2520method%2520to%2520paint%2520a%2520point%250Acloud%2520into%25203D%2520Gaussians%2520given%2520a%2520reference%2520image.%2520GaussianPainter%2520introduces%2520an%250Ainnovative%2520feed-forward%2520approach%2520to%2520overcome%2520the%2520limitations%2520of%2520time-consuming%250Atest-time%2520optimization%2520in%25203D%2520Gaussian%2520splatting.%2520Our%2520method%2520addresses%2520a%250Acritical%2520challenge%2520in%2520the%2520field%253A%2520the%2520non-uniqueness%2520problem%2520inherent%2520in%2520the%250Alarge%2520parameter%2520space%2520of%25203D%2520Gaussian%2520splatting.%2520This%2520space%252C%2520encompassing%250Arotation%252C%2520anisotropic%2520scales%252C%2520and%2520spherical%2520harmonic%2520coefficients%252C%2520introduces%250Athe%2520challenge%2520of%2520rendering%2520similar%2520images%2520from%2520substantially%2520different%2520Gaussian%250Afields.%2520As%2520a%2520result%252C%2520feed-forward%2520networks%2520face%2520instability%2520when%2520attempting%2520to%250Adirectly%2520predict%2520high-quality%2520Gaussian%2520fields%252C%2520struggling%2520to%2520converge%2520on%250Aconsistent%2520parameters%2520for%2520a%2520given%2520output.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520to%250Aestimate%2520a%2520surface%2520normal%2520for%2520each%2520point%2520to%2520determine%2520its%2520Gaussian%2520rotation.%250AThis%2520strategy%2520enables%2520the%2520network%2520to%2520effectively%2520predict%2520the%2520remaining%2520Gaussian%250Aparameters%2520in%2520the%2520constrained%2520space.%2520We%2520further%2520enhance%2520our%2520approach%2520with%2520an%250Aappearance%2520injection%2520module%252C%2520incorporating%2520reference%2520image%2520appearance%2520into%250AGaussian%2520fields%2520via%2520a%2520multiscale%2520triplane%2520representation.%2520Our%2520method%250Asuccessfully%2520balances%2520efficiency%2520and%2520fidelity%2520in%25203D%2520Gaussian%2520generation%252C%250Aachieving%2520high-quality%252C%2520diverse%252C%2520and%2520robust%25203D%2520content%2520creation%2520from%2520point%250Aclouds%2520in%2520a%2520single%2520forward%2520pass.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianPainter%3A%20Painting%20Point%20Cloud%20into%203D%20Gaussians%20with%20Normal%0A%20%20Guidance&entry.906535625=Jingqiu%20Zhou%20and%20Lue%20Fan%20and%20Xuesong%20Chen%20and%20Linjiang%20Huang%20and%20Si%20Liu%20and%20Hongsheng%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20GaussianPainter%2C%20the%20first%20method%20to%20paint%20a%20point%0Acloud%20into%203D%20Gaussians%20given%20a%20reference%20image.%20GaussianPainter%20introduces%20an%0Ainnovative%20feed-forward%20approach%20to%20overcome%20the%20limitations%20of%20time-consuming%0Atest-time%20optimization%20in%203D%20Gaussian%20splatting.%20Our%20method%20addresses%20a%0Acritical%20challenge%20in%20the%20field%3A%20the%20non-uniqueness%20problem%20inherent%20in%20the%0Alarge%20parameter%20space%20of%203D%20Gaussian%20splatting.%20This%20space%2C%20encompassing%0Arotation%2C%20anisotropic%20scales%2C%20and%20spherical%20harmonic%20coefficients%2C%20introduces%0Athe%20challenge%20of%20rendering%20similar%20images%20from%20substantially%20different%20Gaussian%0Afields.%20As%20a%20result%2C%20feed-forward%20networks%20face%20instability%20when%20attempting%20to%0Adirectly%20predict%20high-quality%20Gaussian%20fields%2C%20struggling%20to%20converge%20on%0Aconsistent%20parameters%20for%20a%20given%20output.%20To%20address%20this%20issue%2C%20we%20propose%20to%0Aestimate%20a%20surface%20normal%20for%20each%20point%20to%20determine%20its%20Gaussian%20rotation.%0AThis%20strategy%20enables%20the%20network%20to%20effectively%20predict%20the%20remaining%20Gaussian%0Aparameters%20in%20the%20constrained%20space.%20We%20further%20enhance%20our%20approach%20with%20an%0Aappearance%20injection%20module%2C%20incorporating%20reference%20image%20appearance%20into%0AGaussian%20fields%20via%20a%20multiscale%20triplane%20representation.%20Our%20method%0Asuccessfully%20balances%20efficiency%20and%20fidelity%20in%203D%20Gaussian%20generation%2C%0Aachieving%20high-quality%2C%20diverse%2C%20and%20robust%203D%20content%20creation%20from%20point%0Aclouds%20in%20a%20single%20forward%20pass.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17715v1&entry.124074799=Read"},
{"title": "Relative Distance Guided Dynamic Partition Learning for Scale-Invariant\n  UAV-View Geo-Localization", "author": "Quan Chen and Tingyu Wang and Rongfeng Lu and Bolun Zheng and Zhedong Zheng and Chenggang Yan", "abstract": "  UAV-view Geo-Localization~(UVGL) presents substantial challenges,\nparticularly due to the disparity in visual appearance between drone-captured\nimagery and satellite perspectives. Existing methods usually assume consistent\nscaling factor across different views. Therefore, they adopt predefined\npartition alignment and extract viewpoint-invariant representation by\nconstructing a variety of part-level features. However, the scaling assumption\nis not always hold in the real-world scenarios that variations of UAV flight\nstate leads to the scale mismatch of cross-views, resulting in serious\nperformance degradation. To overcome this issue, we propose a partition\nlearning framework based on relative distance, which alleviates the dependence\non scale consistency while mining fine-grained features. Specifically, we\npropose a distance guided dynamic partition learning strategy~(DGDPL),\nconsisting of a square partition strategy and a distance-guided adjustment\nstrategy. The former is utilized to extract fine-grained features and global\nfeatures in a simple manner. The latter calculates the relative distance ratio\nbetween drone- and satellite-view to adjust the partition size, thereby\nexplicitly aligning the semantic information between partition pairs.\nFurthermore, we propose a saliency-guided refinement strategy to refine\npart-level features, so as to further improve the retrieval accuracy. Extensive\nexperiments show that our approach achieves superior geo-localization accuracy\nacross various scale-inconsistent scenarios, and exhibits remarkable robustness\nagainst scale variations. The code will be released.\n", "link": "http://arxiv.org/abs/2412.11535v2", "date": "2024-12-23", "relevancy": 3.038, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6345}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6213}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Distance%20Guided%20Dynamic%20Partition%20Learning%20for%20Scale-Invariant%0A%20%20UAV-View%20Geo-Localization&body=Title%3A%20Relative%20Distance%20Guided%20Dynamic%20Partition%20Learning%20for%20Scale-Invariant%0A%20%20UAV-View%20Geo-Localization%0AAuthor%3A%20Quan%20Chen%20and%20Tingyu%20Wang%20and%20Rongfeng%20Lu%20and%20Bolun%20Zheng%20and%20Zhedong%20Zheng%20and%20Chenggang%20Yan%0AAbstract%3A%20%20%20UAV-view%20Geo-Localization~%28UVGL%29%20presents%20substantial%20challenges%2C%0Aparticularly%20due%20to%20the%20disparity%20in%20visual%20appearance%20between%20drone-captured%0Aimagery%20and%20satellite%20perspectives.%20Existing%20methods%20usually%20assume%20consistent%0Ascaling%20factor%20across%20different%20views.%20Therefore%2C%20they%20adopt%20predefined%0Apartition%20alignment%20and%20extract%20viewpoint-invariant%20representation%20by%0Aconstructing%20a%20variety%20of%20part-level%20features.%20However%2C%20the%20scaling%20assumption%0Ais%20not%20always%20hold%20in%20the%20real-world%20scenarios%20that%20variations%20of%20UAV%20flight%0Astate%20leads%20to%20the%20scale%20mismatch%20of%20cross-views%2C%20resulting%20in%20serious%0Aperformance%20degradation.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20partition%0Alearning%20framework%20based%20on%20relative%20distance%2C%20which%20alleviates%20the%20dependence%0Aon%20scale%20consistency%20while%20mining%20fine-grained%20features.%20Specifically%2C%20we%0Apropose%20a%20distance%20guided%20dynamic%20partition%20learning%20strategy~%28DGDPL%29%2C%0Aconsisting%20of%20a%20square%20partition%20strategy%20and%20a%20distance-guided%20adjustment%0Astrategy.%20The%20former%20is%20utilized%20to%20extract%20fine-grained%20features%20and%20global%0Afeatures%20in%20a%20simple%20manner.%20The%20latter%20calculates%20the%20relative%20distance%20ratio%0Abetween%20drone-%20and%20satellite-view%20to%20adjust%20the%20partition%20size%2C%20thereby%0Aexplicitly%20aligning%20the%20semantic%20information%20between%20partition%20pairs.%0AFurthermore%2C%20we%20propose%20a%20saliency-guided%20refinement%20strategy%20to%20refine%0Apart-level%20features%2C%20so%20as%20to%20further%20improve%20the%20retrieval%20accuracy.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20achieves%20superior%20geo-localization%20accuracy%0Aacross%20various%20scale-inconsistent%20scenarios%2C%20and%20exhibits%20remarkable%20robustness%0Aagainst%20scale%20variations.%20The%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Distance%2520Guided%2520Dynamic%2520Partition%2520Learning%2520for%2520Scale-Invariant%250A%2520%2520UAV-View%2520Geo-Localization%26entry.906535625%3DQuan%2520Chen%2520and%2520Tingyu%2520Wang%2520and%2520Rongfeng%2520Lu%2520and%2520Bolun%2520Zheng%2520and%2520Zhedong%2520Zheng%2520and%2520Chenggang%2520Yan%26entry.1292438233%3D%2520%2520UAV-view%2520Geo-Localization~%2528UVGL%2529%2520presents%2520substantial%2520challenges%252C%250Aparticularly%2520due%2520to%2520the%2520disparity%2520in%2520visual%2520appearance%2520between%2520drone-captured%250Aimagery%2520and%2520satellite%2520perspectives.%2520Existing%2520methods%2520usually%2520assume%2520consistent%250Ascaling%2520factor%2520across%2520different%2520views.%2520Therefore%252C%2520they%2520adopt%2520predefined%250Apartition%2520alignment%2520and%2520extract%2520viewpoint-invariant%2520representation%2520by%250Aconstructing%2520a%2520variety%2520of%2520part-level%2520features.%2520However%252C%2520the%2520scaling%2520assumption%250Ais%2520not%2520always%2520hold%2520in%2520the%2520real-world%2520scenarios%2520that%2520variations%2520of%2520UAV%2520flight%250Astate%2520leads%2520to%2520the%2520scale%2520mismatch%2520of%2520cross-views%252C%2520resulting%2520in%2520serious%250Aperformance%2520degradation.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520propose%2520a%2520partition%250Alearning%2520framework%2520based%2520on%2520relative%2520distance%252C%2520which%2520alleviates%2520the%2520dependence%250Aon%2520scale%2520consistency%2520while%2520mining%2520fine-grained%2520features.%2520Specifically%252C%2520we%250Apropose%2520a%2520distance%2520guided%2520dynamic%2520partition%2520learning%2520strategy~%2528DGDPL%2529%252C%250Aconsisting%2520of%2520a%2520square%2520partition%2520strategy%2520and%2520a%2520distance-guided%2520adjustment%250Astrategy.%2520The%2520former%2520is%2520utilized%2520to%2520extract%2520fine-grained%2520features%2520and%2520global%250Afeatures%2520in%2520a%2520simple%2520manner.%2520The%2520latter%2520calculates%2520the%2520relative%2520distance%2520ratio%250Abetween%2520drone-%2520and%2520satellite-view%2520to%2520adjust%2520the%2520partition%2520size%252C%2520thereby%250Aexplicitly%2520aligning%2520the%2520semantic%2520information%2520between%2520partition%2520pairs.%250AFurthermore%252C%2520we%2520propose%2520a%2520saliency-guided%2520refinement%2520strategy%2520to%2520refine%250Apart-level%2520features%252C%2520so%2520as%2520to%2520further%2520improve%2520the%2520retrieval%2520accuracy.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520approach%2520achieves%2520superior%2520geo-localization%2520accuracy%250Aacross%2520various%2520scale-inconsistent%2520scenarios%252C%2520and%2520exhibits%2520remarkable%2520robustness%250Aagainst%2520scale%2520variations.%2520The%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Distance%20Guided%20Dynamic%20Partition%20Learning%20for%20Scale-Invariant%0A%20%20UAV-View%20Geo-Localization&entry.906535625=Quan%20Chen%20and%20Tingyu%20Wang%20and%20Rongfeng%20Lu%20and%20Bolun%20Zheng%20and%20Zhedong%20Zheng%20and%20Chenggang%20Yan&entry.1292438233=%20%20UAV-view%20Geo-Localization~%28UVGL%29%20presents%20substantial%20challenges%2C%0Aparticularly%20due%20to%20the%20disparity%20in%20visual%20appearance%20between%20drone-captured%0Aimagery%20and%20satellite%20perspectives.%20Existing%20methods%20usually%20assume%20consistent%0Ascaling%20factor%20across%20different%20views.%20Therefore%2C%20they%20adopt%20predefined%0Apartition%20alignment%20and%20extract%20viewpoint-invariant%20representation%20by%0Aconstructing%20a%20variety%20of%20part-level%20features.%20However%2C%20the%20scaling%20assumption%0Ais%20not%20always%20hold%20in%20the%20real-world%20scenarios%20that%20variations%20of%20UAV%20flight%0Astate%20leads%20to%20the%20scale%20mismatch%20of%20cross-views%2C%20resulting%20in%20serious%0Aperformance%20degradation.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20partition%0Alearning%20framework%20based%20on%20relative%20distance%2C%20which%20alleviates%20the%20dependence%0Aon%20scale%20consistency%20while%20mining%20fine-grained%20features.%20Specifically%2C%20we%0Apropose%20a%20distance%20guided%20dynamic%20partition%20learning%20strategy~%28DGDPL%29%2C%0Aconsisting%20of%20a%20square%20partition%20strategy%20and%20a%20distance-guided%20adjustment%0Astrategy.%20The%20former%20is%20utilized%20to%20extract%20fine-grained%20features%20and%20global%0Afeatures%20in%20a%20simple%20manner.%20The%20latter%20calculates%20the%20relative%20distance%20ratio%0Abetween%20drone-%20and%20satellite-view%20to%20adjust%20the%20partition%20size%2C%20thereby%0Aexplicitly%20aligning%20the%20semantic%20information%20between%20partition%20pairs.%0AFurthermore%2C%20we%20propose%20a%20saliency-guided%20refinement%20strategy%20to%20refine%0Apart-level%20features%2C%20so%20as%20to%20further%20improve%20the%20retrieval%20accuracy.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20achieves%20superior%20geo-localization%20accuracy%0Aacross%20various%20scale-inconsistent%20scenarios%2C%20and%20exhibits%20remarkable%20robustness%0Aagainst%20scale%20variations.%20The%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11535v2&entry.124074799=Read"},
{"title": "Comprehensive Multi-Modal Prototypes are Simple and Effective\n  Classifiers for Vast-Vocabulary Object Detection", "author": "Yitong Chen and Wenhao Yao and Lingchen Meng and Sihong Wu and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Enabling models to recognize vast open-world categories has been a\nlongstanding pursuit in object detection. By leveraging the generalization\ncapabilities of vision-language models, current open-world detectors can\nrecognize a broader range of vocabularies, despite being trained on limited\ncategories. However, when the scale of the category vocabularies during\ntraining expands to a real-world level, previous classifiers aligned with\ncoarse class names significantly reduce the recognition performance of these\ndetectors. In this paper, we introduce Prova, a multi-modal prototype\nclassifier for vast-vocabulary object detection. Prova extracts comprehensive\nmulti-modal prototypes as initialization of alignment classifiers to tackle the\nvast-vocabulary object recognition failure problem. On V3Det, this simple\nmethod greatly enhances the performance among one-stage, two-stage, and\nDETR-based detectors with only additional projection layers in both supervised\nand open-vocabulary settings. In particular, Prova improves Faster R-CNN, FCOS,\nand DINO by 3.3, 6.2, and 2.9 AP respectively in the supervised setting of\nV3Det. For the open-vocabulary setting, Prova achieves a new state-of-the-art\nperformance with 32.8 base AP and 11.0 novel AP, which is of 2.6 and 4.3 gain\nover the previous methods.\n", "link": "http://arxiv.org/abs/2412.17800v1", "date": "2024-12-23", "relevancy": 3.0371, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6169}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6169}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Multi-Modal%20Prototypes%20are%20Simple%20and%20Effective%0A%20%20Classifiers%20for%20Vast-Vocabulary%20Object%20Detection&body=Title%3A%20Comprehensive%20Multi-Modal%20Prototypes%20are%20Simple%20and%20Effective%0A%20%20Classifiers%20for%20Vast-Vocabulary%20Object%20Detection%0AAuthor%3A%20Yitong%20Chen%20and%20Wenhao%20Yao%20and%20Lingchen%20Meng%20and%20Sihong%20Wu%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Enabling%20models%20to%20recognize%20vast%20open-world%20categories%20has%20been%20a%0Alongstanding%20pursuit%20in%20object%20detection.%20By%20leveraging%20the%20generalization%0Acapabilities%20of%20vision-language%20models%2C%20current%20open-world%20detectors%20can%0Arecognize%20a%20broader%20range%20of%20vocabularies%2C%20despite%20being%20trained%20on%20limited%0Acategories.%20However%2C%20when%20the%20scale%20of%20the%20category%20vocabularies%20during%0Atraining%20expands%20to%20a%20real-world%20level%2C%20previous%20classifiers%20aligned%20with%0Acoarse%20class%20names%20significantly%20reduce%20the%20recognition%20performance%20of%20these%0Adetectors.%20In%20this%20paper%2C%20we%20introduce%20Prova%2C%20a%20multi-modal%20prototype%0Aclassifier%20for%20vast-vocabulary%20object%20detection.%20Prova%20extracts%20comprehensive%0Amulti-modal%20prototypes%20as%20initialization%20of%20alignment%20classifiers%20to%20tackle%20the%0Avast-vocabulary%20object%20recognition%20failure%20problem.%20On%20V3Det%2C%20this%20simple%0Amethod%20greatly%20enhances%20the%20performance%20among%20one-stage%2C%20two-stage%2C%20and%0ADETR-based%20detectors%20with%20only%20additional%20projection%20layers%20in%20both%20supervised%0Aand%20open-vocabulary%20settings.%20In%20particular%2C%20Prova%20improves%20Faster%20R-CNN%2C%20FCOS%2C%0Aand%20DINO%20by%203.3%2C%206.2%2C%20and%202.9%20AP%20respectively%20in%20the%20supervised%20setting%20of%0AV3Det.%20For%20the%20open-vocabulary%20setting%2C%20Prova%20achieves%20a%20new%20state-of-the-art%0Aperformance%20with%2032.8%20base%20AP%20and%2011.0%20novel%20AP%2C%20which%20is%20of%202.6%20and%204.3%20gain%0Aover%20the%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Multi-Modal%2520Prototypes%2520are%2520Simple%2520and%2520Effective%250A%2520%2520Classifiers%2520for%2520Vast-Vocabulary%2520Object%2520Detection%26entry.906535625%3DYitong%2520Chen%2520and%2520Wenhao%2520Yao%2520and%2520Lingchen%2520Meng%2520and%2520Sihong%2520Wu%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Enabling%2520models%2520to%2520recognize%2520vast%2520open-world%2520categories%2520has%2520been%2520a%250Alongstanding%2520pursuit%2520in%2520object%2520detection.%2520By%2520leveraging%2520the%2520generalization%250Acapabilities%2520of%2520vision-language%2520models%252C%2520current%2520open-world%2520detectors%2520can%250Arecognize%2520a%2520broader%2520range%2520of%2520vocabularies%252C%2520despite%2520being%2520trained%2520on%2520limited%250Acategories.%2520However%252C%2520when%2520the%2520scale%2520of%2520the%2520category%2520vocabularies%2520during%250Atraining%2520expands%2520to%2520a%2520real-world%2520level%252C%2520previous%2520classifiers%2520aligned%2520with%250Acoarse%2520class%2520names%2520significantly%2520reduce%2520the%2520recognition%2520performance%2520of%2520these%250Adetectors.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Prova%252C%2520a%2520multi-modal%2520prototype%250Aclassifier%2520for%2520vast-vocabulary%2520object%2520detection.%2520Prova%2520extracts%2520comprehensive%250Amulti-modal%2520prototypes%2520as%2520initialization%2520of%2520alignment%2520classifiers%2520to%2520tackle%2520the%250Avast-vocabulary%2520object%2520recognition%2520failure%2520problem.%2520On%2520V3Det%252C%2520this%2520simple%250Amethod%2520greatly%2520enhances%2520the%2520performance%2520among%2520one-stage%252C%2520two-stage%252C%2520and%250ADETR-based%2520detectors%2520with%2520only%2520additional%2520projection%2520layers%2520in%2520both%2520supervised%250Aand%2520open-vocabulary%2520settings.%2520In%2520particular%252C%2520Prova%2520improves%2520Faster%2520R-CNN%252C%2520FCOS%252C%250Aand%2520DINO%2520by%25203.3%252C%25206.2%252C%2520and%25202.9%2520AP%2520respectively%2520in%2520the%2520supervised%2520setting%2520of%250AV3Det.%2520For%2520the%2520open-vocabulary%2520setting%252C%2520Prova%2520achieves%2520a%2520new%2520state-of-the-art%250Aperformance%2520with%252032.8%2520base%2520AP%2520and%252011.0%2520novel%2520AP%252C%2520which%2520is%2520of%25202.6%2520and%25204.3%2520gain%250Aover%2520the%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Multi-Modal%20Prototypes%20are%20Simple%20and%20Effective%0A%20%20Classifiers%20for%20Vast-Vocabulary%20Object%20Detection&entry.906535625=Yitong%20Chen%20and%20Wenhao%20Yao%20and%20Lingchen%20Meng%20and%20Sihong%20Wu%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Enabling%20models%20to%20recognize%20vast%20open-world%20categories%20has%20been%20a%0Alongstanding%20pursuit%20in%20object%20detection.%20By%20leveraging%20the%20generalization%0Acapabilities%20of%20vision-language%20models%2C%20current%20open-world%20detectors%20can%0Arecognize%20a%20broader%20range%20of%20vocabularies%2C%20despite%20being%20trained%20on%20limited%0Acategories.%20However%2C%20when%20the%20scale%20of%20the%20category%20vocabularies%20during%0Atraining%20expands%20to%20a%20real-world%20level%2C%20previous%20classifiers%20aligned%20with%0Acoarse%20class%20names%20significantly%20reduce%20the%20recognition%20performance%20of%20these%0Adetectors.%20In%20this%20paper%2C%20we%20introduce%20Prova%2C%20a%20multi-modal%20prototype%0Aclassifier%20for%20vast-vocabulary%20object%20detection.%20Prova%20extracts%20comprehensive%0Amulti-modal%20prototypes%20as%20initialization%20of%20alignment%20classifiers%20to%20tackle%20the%0Avast-vocabulary%20object%20recognition%20failure%20problem.%20On%20V3Det%2C%20this%20simple%0Amethod%20greatly%20enhances%20the%20performance%20among%20one-stage%2C%20two-stage%2C%20and%0ADETR-based%20detectors%20with%20only%20additional%20projection%20layers%20in%20both%20supervised%0Aand%20open-vocabulary%20settings.%20In%20particular%2C%20Prova%20improves%20Faster%20R-CNN%2C%20FCOS%2C%0Aand%20DINO%20by%203.3%2C%206.2%2C%20and%202.9%20AP%20respectively%20in%20the%20supervised%20setting%20of%0AV3Det.%20For%20the%20open-vocabulary%20setting%2C%20Prova%20achieves%20a%20new%20state-of-the-art%0Aperformance%20with%2032.8%20base%20AP%20and%2011.0%20novel%20AP%2C%20which%20is%20of%202.6%20and%204.3%20gain%0Aover%20the%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17800v1&entry.124074799=Read"},
{"title": "Reasoning to Attend: Try to Understand How <SEG> Token Works", "author": "Rui Qian and Xin Yin and Dejing Dou", "abstract": "  Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion.Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.\n", "link": "http://arxiv.org/abs/2412.17741v1", "date": "2024-12-23", "relevancy": 2.9843, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6163}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20to%20Attend%3A%20Try%20to%20Understand%20How%20%3CSEG%3E%20Token%20Works&body=Title%3A%20Reasoning%20to%20Attend%3A%20Try%20to%20Understand%20How%20%3CSEG%3E%20Token%20Works%0AAuthor%3A%20Rui%20Qian%20and%20Xin%20Yin%20and%20Dejing%20Dou%0AAbstract%3A%20%20%20Current%20Large%20Multimodal%20Models%20%28LMMs%29%20empowered%20visual%20grounding%20typically%0Arely%20on%20%24%5Ctexttt%7B%3CSEG%3E%7D%24%20token%20as%20a%20text%20prompt%20to%20jointly%20optimize%20the%0Avision-language%20model%20%28e.g.%2C%20LLaVA%29%20and%20the%20downstream%20task-specified%20model%0A%28%5Ceg%2C%20SAM%29.%20However%2C%20we%20observe%20that%20little%20research%20has%20looked%20into%20how%20it%0Aworks.In%20this%20work%2C%20we%20first%20visualize%20the%20similarity%20maps%2C%20which%20are%20obtained%0Aby%20computing%20the%20semantic%20similarity%20between%20the%20%24%5Ctexttt%7B%3CSEG%3E%7D%24%20token%20and%20the%0Aimage%20token%20embeddings%20derived%20from%20the%20last%20hidden%20layer%20in%20both%20the%20LLaVA%0Aencoder%20and%20SAM%20decoder.%20Intriguingly%2C%20we%20have%20found%20that%20a%20striking%0Aconsistency%20holds%20in%20terms%20of%20activation%20responses%20in%20the%20similarity%20map%2Cwhich%0Areveals%20that%20what%20%24%5Ctexttt%7B%3CSEG%3E%7D%24%20token%20contributes%20to%20is%20the%20semantic%0Asimilarity%20within%20image-text%20pairs.%20Specifically%2C%20%24%5Ctexttt%7B%3CSEG%3E%7D%24%20token%2C%20a%0Aplaceholder%20expanded%20in%20text%20vocabulary%2C%20extensively%20queries%20among%20individual%0Atokenized%20image%20patches%20to%20match%20the%20semantics%20of%20an%20object%20from%20text%20to%20the%0Apaired%20image%20while%20the%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20fine-tuned.%20Upon%0Athe%20above%20findings%2C%20we%20present%20READ%2C%20which%20facilitates%20LMMs%27%20resilient%0A%24%5Ctextbf%7BREA%7D%24soning%20capability%20of%20where%20to%20atten%24%5Ctextbf%7BD%7D%24%20under%20the%0Aguidance%20of%20highly%20activated%20points%20borrowed%20from%20similarity%20maps.%20Remarkably%2C%0AREAD%20features%20an%20intuitive%20design%2C%20Similarity%20as%20Points%20module%20%28SasP%29%2C%20which%0Acan%20be%20seamlessly%20applied%20to%20%24%5Ctexttt%7B%3CSEG%3E%7D%24-like%20paradigms%20in%20a%20plug-and-play%0Afashion.Also%2C%20extensive%20experiments%20have%20been%20conducted%20on%20the%20ReasonSeg%20and%0ARefCOCO%28%2B/g%29%20datasets.%20To%20validate%20whether%20READ%20suffers%20from%20catastrophic%0Aforgetting%20of%20previous%20skills%20after%20fine-tuning%2C%20we%20further%20assess%20its%0Ageneration%20ability%20on%20an%20augmented%20FP-RefCOCO%28%2B/g%29%20dataset.%20All%20codes%20and%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/rui-qian/READ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520to%2520Attend%253A%2520Try%2520to%2520Understand%2520How%2520%253CSEG%253E%2520Token%2520Works%26entry.906535625%3DRui%2520Qian%2520and%2520Xin%2520Yin%2520and%2520Dejing%2520Dou%26entry.1292438233%3D%2520%2520Current%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520empowered%2520visual%2520grounding%2520typically%250Arely%2520on%2520%2524%255Ctexttt%257B%253CSEG%253E%257D%2524%2520token%2520as%2520a%2520text%2520prompt%2520to%2520jointly%2520optimize%2520the%250Avision-language%2520model%2520%2528e.g.%252C%2520LLaVA%2529%2520and%2520the%2520downstream%2520task-specified%2520model%250A%2528%255Ceg%252C%2520SAM%2529.%2520However%252C%2520we%2520observe%2520that%2520little%2520research%2520has%2520looked%2520into%2520how%2520it%250Aworks.In%2520this%2520work%252C%2520we%2520first%2520visualize%2520the%2520similarity%2520maps%252C%2520which%2520are%2520obtained%250Aby%2520computing%2520the%2520semantic%2520similarity%2520between%2520the%2520%2524%255Ctexttt%257B%253CSEG%253E%257D%2524%2520token%2520and%2520the%250Aimage%2520token%2520embeddings%2520derived%2520from%2520the%2520last%2520hidden%2520layer%2520in%2520both%2520the%2520LLaVA%250Aencoder%2520and%2520SAM%2520decoder.%2520Intriguingly%252C%2520we%2520have%2520found%2520that%2520a%2520striking%250Aconsistency%2520holds%2520in%2520terms%2520of%2520activation%2520responses%2520in%2520the%2520similarity%2520map%252Cwhich%250Areveals%2520that%2520what%2520%2524%255Ctexttt%257B%253CSEG%253E%257D%2524%2520token%2520contributes%2520to%2520is%2520the%2520semantic%250Asimilarity%2520within%2520image-text%2520pairs.%2520Specifically%252C%2520%2524%255Ctexttt%257B%253CSEG%253E%257D%2524%2520token%252C%2520a%250Aplaceholder%2520expanded%2520in%2520text%2520vocabulary%252C%2520extensively%2520queries%2520among%2520individual%250Atokenized%2520image%2520patches%2520to%2520match%2520the%2520semantics%2520of%2520an%2520object%2520from%2520text%2520to%2520the%250Apaired%2520image%2520while%2520the%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520being%2520fine-tuned.%2520Upon%250Athe%2520above%2520findings%252C%2520we%2520present%2520READ%252C%2520which%2520facilitates%2520LMMs%2527%2520resilient%250A%2524%255Ctextbf%257BREA%257D%2524soning%2520capability%2520of%2520where%2520to%2520atten%2524%255Ctextbf%257BD%257D%2524%2520under%2520the%250Aguidance%2520of%2520highly%2520activated%2520points%2520borrowed%2520from%2520similarity%2520maps.%2520Remarkably%252C%250AREAD%2520features%2520an%2520intuitive%2520design%252C%2520Similarity%2520as%2520Points%2520module%2520%2528SasP%2529%252C%2520which%250Acan%2520be%2520seamlessly%2520applied%2520to%2520%2524%255Ctexttt%257B%253CSEG%253E%257D%2524-like%2520paradigms%2520in%2520a%2520plug-and-play%250Afashion.Also%252C%2520extensive%2520experiments%2520have%2520been%2520conducted%2520on%2520the%2520ReasonSeg%2520and%250ARefCOCO%2528%252B/g%2529%2520datasets.%2520To%2520validate%2520whether%2520READ%2520suffers%2520from%2520catastrophic%250Aforgetting%2520of%2520previous%2520skills%2520after%2520fine-tuning%252C%2520we%2520further%2520assess%2520its%250Ageneration%2520ability%2520on%2520an%2520augmented%2520FP-RefCOCO%2528%252B/g%2529%2520dataset.%2520All%2520codes%2520and%250Amodels%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/rui-qian/READ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20to%20Attend%3A%20Try%20to%20Understand%20How%20%3CSEG%3E%20Token%20Works&entry.906535625=Rui%20Qian%20and%20Xin%20Yin%20and%20Dejing%20Dou&entry.1292438233=%20%20Current%20Large%20Multimodal%20Models%20%28LMMs%29%20empowered%20visual%20grounding%20typically%0Arely%20on%20%24%5Ctexttt%7B%3CSEG%3E%7D%24%20token%20as%20a%20text%20prompt%20to%20jointly%20optimize%20the%0Avision-language%20model%20%28e.g.%2C%20LLaVA%29%20and%20the%20downstream%20task-specified%20model%0A%28%5Ceg%2C%20SAM%29.%20However%2C%20we%20observe%20that%20little%20research%20has%20looked%20into%20how%20it%0Aworks.In%20this%20work%2C%20we%20first%20visualize%20the%20similarity%20maps%2C%20which%20are%20obtained%0Aby%20computing%20the%20semantic%20similarity%20between%20the%20%24%5Ctexttt%7B%3CSEG%3E%7D%24%20token%20and%20the%0Aimage%20token%20embeddings%20derived%20from%20the%20last%20hidden%20layer%20in%20both%20the%20LLaVA%0Aencoder%20and%20SAM%20decoder.%20Intriguingly%2C%20we%20have%20found%20that%20a%20striking%0Aconsistency%20holds%20in%20terms%20of%20activation%20responses%20in%20the%20similarity%20map%2Cwhich%0Areveals%20that%20what%20%24%5Ctexttt%7B%3CSEG%3E%7D%24%20token%20contributes%20to%20is%20the%20semantic%0Asimilarity%20within%20image-text%20pairs.%20Specifically%2C%20%24%5Ctexttt%7B%3CSEG%3E%7D%24%20token%2C%20a%0Aplaceholder%20expanded%20in%20text%20vocabulary%2C%20extensively%20queries%20among%20individual%0Atokenized%20image%20patches%20to%20match%20the%20semantics%20of%20an%20object%20from%20text%20to%20the%0Apaired%20image%20while%20the%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20fine-tuned.%20Upon%0Athe%20above%20findings%2C%20we%20present%20READ%2C%20which%20facilitates%20LMMs%27%20resilient%0A%24%5Ctextbf%7BREA%7D%24soning%20capability%20of%20where%20to%20atten%24%5Ctextbf%7BD%7D%24%20under%20the%0Aguidance%20of%20highly%20activated%20points%20borrowed%20from%20similarity%20maps.%20Remarkably%2C%0AREAD%20features%20an%20intuitive%20design%2C%20Similarity%20as%20Points%20module%20%28SasP%29%2C%20which%0Acan%20be%20seamlessly%20applied%20to%20%24%5Ctexttt%7B%3CSEG%3E%7D%24-like%20paradigms%20in%20a%20plug-and-play%0Afashion.Also%2C%20extensive%20experiments%20have%20been%20conducted%20on%20the%20ReasonSeg%20and%0ARefCOCO%28%2B/g%29%20datasets.%20To%20validate%20whether%20READ%20suffers%20from%20catastrophic%0Aforgetting%20of%20previous%20skills%20after%20fine-tuning%2C%20we%20further%20assess%20its%0Ageneration%20ability%20on%20an%20augmented%20FP-RefCOCO%28%2B/g%29%20dataset.%20All%20codes%20and%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/rui-qian/READ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17741v1&entry.124074799=Read"},
{"title": "EasyHOI: Unleashing the Power of Large Models for Reconstructing\n  Hand-Object Interactions in the Wild", "author": "Yumeng Liu and Xiaoxiao Long and Zemin Yang and Yuan Liu and Marc Habermann and Christian Theobalt and Yuexin Ma and Wenping Wang", "abstract": "  Our work aims to reconstruct hand-object interactions from a single-view\nimage, which is a fundamental but ill-posed task. Unlike methods that\nreconstruct from videos, multi-view images, or predefined 3D templates,\nsingle-view reconstruction faces significant challenges due to inherent\nambiguities and occlusions. These challenges are further amplified by the\ndiverse nature of hand poses and the vast variety of object shapes and sizes.\nOur key insight is that current foundational models for segmentation,\ninpainting, and 3D reconstruction robustly generalize to in-the-wild images,\nwhich could provide strong visual and geometric priors for reconstructing\nhand-object interactions. Specifically, given a single image, we first design a\nnovel pipeline to estimate the underlying hand pose and object shape using\noff-the-shelf large models. Furthermore, with the initial reconstruction, we\nemploy a prior-guided optimization scheme, which optimizes hand pose to comply\nwith 3D physical constraints and the 2D input image content. We perform\nexperiments across several datasets and show that our method consistently\noutperforms baselines and faithfully reconstructs a diverse set of hand-object\ninteractions. Here is the link of our project page:\nhttps://lym29.github.io/EasyHOI-page/\n", "link": "http://arxiv.org/abs/2411.14280v3", "date": "2024-12-23", "relevancy": 2.9701, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6041}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5899}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyHOI%3A%20Unleashing%20the%20Power%20of%20Large%20Models%20for%20Reconstructing%0A%20%20Hand-Object%20Interactions%20in%20the%20Wild&body=Title%3A%20EasyHOI%3A%20Unleashing%20the%20Power%20of%20Large%20Models%20for%20Reconstructing%0A%20%20Hand-Object%20Interactions%20in%20the%20Wild%0AAuthor%3A%20Yumeng%20Liu%20and%20Xiaoxiao%20Long%20and%20Zemin%20Yang%20and%20Yuan%20Liu%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%20and%20Yuexin%20Ma%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20Our%20work%20aims%20to%20reconstruct%20hand-object%20interactions%20from%20a%20single-view%0Aimage%2C%20which%20is%20a%20fundamental%20but%20ill-posed%20task.%20Unlike%20methods%20that%0Areconstruct%20from%20videos%2C%20multi-view%20images%2C%20or%20predefined%203D%20templates%2C%0Asingle-view%20reconstruction%20faces%20significant%20challenges%20due%20to%20inherent%0Aambiguities%20and%20occlusions.%20These%20challenges%20are%20further%20amplified%20by%20the%0Adiverse%20nature%20of%20hand%20poses%20and%20the%20vast%20variety%20of%20object%20shapes%20and%20sizes.%0AOur%20key%20insight%20is%20that%20current%20foundational%20models%20for%20segmentation%2C%0Ainpainting%2C%20and%203D%20reconstruction%20robustly%20generalize%20to%20in-the-wild%20images%2C%0Awhich%20could%20provide%20strong%20visual%20and%20geometric%20priors%20for%20reconstructing%0Ahand-object%20interactions.%20Specifically%2C%20given%20a%20single%20image%2C%20we%20first%20design%20a%0Anovel%20pipeline%20to%20estimate%20the%20underlying%20hand%20pose%20and%20object%20shape%20using%0Aoff-the-shelf%20large%20models.%20Furthermore%2C%20with%20the%20initial%20reconstruction%2C%20we%0Aemploy%20a%20prior-guided%20optimization%20scheme%2C%20which%20optimizes%20hand%20pose%20to%20comply%0Awith%203D%20physical%20constraints%20and%20the%202D%20input%20image%20content.%20We%20perform%0Aexperiments%20across%20several%20datasets%20and%20show%20that%20our%20method%20consistently%0Aoutperforms%20baselines%20and%20faithfully%20reconstructs%20a%20diverse%20set%20of%20hand-object%0Ainteractions.%20Here%20is%20the%20link%20of%20our%20project%20page%3A%0Ahttps%3A//lym29.github.io/EasyHOI-page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14280v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyHOI%253A%2520Unleashing%2520the%2520Power%2520of%2520Large%2520Models%2520for%2520Reconstructing%250A%2520%2520Hand-Object%2520Interactions%2520in%2520the%2520Wild%26entry.906535625%3DYumeng%2520Liu%2520and%2520Xiaoxiao%2520Long%2520and%2520Zemin%2520Yang%2520and%2520Yuan%2520Liu%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%2520and%2520Yuexin%2520Ma%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%2520Our%2520work%2520aims%2520to%2520reconstruct%2520hand-object%2520interactions%2520from%2520a%2520single-view%250Aimage%252C%2520which%2520is%2520a%2520fundamental%2520but%2520ill-posed%2520task.%2520Unlike%2520methods%2520that%250Areconstruct%2520from%2520videos%252C%2520multi-view%2520images%252C%2520or%2520predefined%25203D%2520templates%252C%250Asingle-view%2520reconstruction%2520faces%2520significant%2520challenges%2520due%2520to%2520inherent%250Aambiguities%2520and%2520occlusions.%2520These%2520challenges%2520are%2520further%2520amplified%2520by%2520the%250Adiverse%2520nature%2520of%2520hand%2520poses%2520and%2520the%2520vast%2520variety%2520of%2520object%2520shapes%2520and%2520sizes.%250AOur%2520key%2520insight%2520is%2520that%2520current%2520foundational%2520models%2520for%2520segmentation%252C%250Ainpainting%252C%2520and%25203D%2520reconstruction%2520robustly%2520generalize%2520to%2520in-the-wild%2520images%252C%250Awhich%2520could%2520provide%2520strong%2520visual%2520and%2520geometric%2520priors%2520for%2520reconstructing%250Ahand-object%2520interactions.%2520Specifically%252C%2520given%2520a%2520single%2520image%252C%2520we%2520first%2520design%2520a%250Anovel%2520pipeline%2520to%2520estimate%2520the%2520underlying%2520hand%2520pose%2520and%2520object%2520shape%2520using%250Aoff-the-shelf%2520large%2520models.%2520Furthermore%252C%2520with%2520the%2520initial%2520reconstruction%252C%2520we%250Aemploy%2520a%2520prior-guided%2520optimization%2520scheme%252C%2520which%2520optimizes%2520hand%2520pose%2520to%2520comply%250Awith%25203D%2520physical%2520constraints%2520and%2520the%25202D%2520input%2520image%2520content.%2520We%2520perform%250Aexperiments%2520across%2520several%2520datasets%2520and%2520show%2520that%2520our%2520method%2520consistently%250Aoutperforms%2520baselines%2520and%2520faithfully%2520reconstructs%2520a%2520diverse%2520set%2520of%2520hand-object%250Ainteractions.%2520Here%2520is%2520the%2520link%2520of%2520our%2520project%2520page%253A%250Ahttps%253A//lym29.github.io/EasyHOI-page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14280v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyHOI%3A%20Unleashing%20the%20Power%20of%20Large%20Models%20for%20Reconstructing%0A%20%20Hand-Object%20Interactions%20in%20the%20Wild&entry.906535625=Yumeng%20Liu%20and%20Xiaoxiao%20Long%20and%20Zemin%20Yang%20and%20Yuan%20Liu%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%20and%20Yuexin%20Ma%20and%20Wenping%20Wang&entry.1292438233=%20%20Our%20work%20aims%20to%20reconstruct%20hand-object%20interactions%20from%20a%20single-view%0Aimage%2C%20which%20is%20a%20fundamental%20but%20ill-posed%20task.%20Unlike%20methods%20that%0Areconstruct%20from%20videos%2C%20multi-view%20images%2C%20or%20predefined%203D%20templates%2C%0Asingle-view%20reconstruction%20faces%20significant%20challenges%20due%20to%20inherent%0Aambiguities%20and%20occlusions.%20These%20challenges%20are%20further%20amplified%20by%20the%0Adiverse%20nature%20of%20hand%20poses%20and%20the%20vast%20variety%20of%20object%20shapes%20and%20sizes.%0AOur%20key%20insight%20is%20that%20current%20foundational%20models%20for%20segmentation%2C%0Ainpainting%2C%20and%203D%20reconstruction%20robustly%20generalize%20to%20in-the-wild%20images%2C%0Awhich%20could%20provide%20strong%20visual%20and%20geometric%20priors%20for%20reconstructing%0Ahand-object%20interactions.%20Specifically%2C%20given%20a%20single%20image%2C%20we%20first%20design%20a%0Anovel%20pipeline%20to%20estimate%20the%20underlying%20hand%20pose%20and%20object%20shape%20using%0Aoff-the-shelf%20large%20models.%20Furthermore%2C%20with%20the%20initial%20reconstruction%2C%20we%0Aemploy%20a%20prior-guided%20optimization%20scheme%2C%20which%20optimizes%20hand%20pose%20to%20comply%0Awith%203D%20physical%20constraints%20and%20the%202D%20input%20image%20content.%20We%20perform%0Aexperiments%20across%20several%20datasets%20and%20show%20that%20our%20method%20consistently%0Aoutperforms%20baselines%20and%20faithfully%20reconstructs%20a%20diverse%20set%20of%20hand-object%0Ainteractions.%20Here%20is%20the%20link%20of%20our%20project%20page%3A%0Ahttps%3A//lym29.github.io/EasyHOI-page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14280v3&entry.124074799=Read"},
{"title": "GauSim: Registering Elastic Objects into Digital World by Gaussian\n  Simulator", "author": "Yidi Shao and Mu Huang and Chen Change Loy and Bo Dai", "abstract": "  In this work, we introduce GauSim, a novel neural network-based simulator\ndesigned to capture the dynamic behaviors of real-world elastic objects\nrepresented through Gaussian kernels. Unlike traditional methods that treat\nkernels as particles within particle-based simulations, we leverage continuum\nmechanics, modeling each kernel as a continuous piece of matter to account for\nrealistic deformations without idealized assumptions. To improve computational\nefficiency and fidelity, we employ a hierarchical structure that organizes\nkernels into Center of Mass Systems (CMS) with explicit formulations, enabling\na coarse-to-fine simulation approach. This structure significantly reduces\ncomputational overhead while preserving detailed dynamics. In addition, GauSim\nincorporates explicit physics constraints, such as mass and momentum\nconservation, ensuring interpretable results and robust, physically plausible\nsimulations. To validate our approach, we present a new dataset, READY,\ncontaining multi-view videos of real-world elastic deformations. Experimental\nresults demonstrate that GauSim achieves superior performance compared to\nexisting physics-driven baselines, offering a practical and accurate solution\nfor simulating complex dynamic behaviors. Code and model will be released.\nProject page: https://www.mmlab-ntu.com/project/gausim/index.html .\n", "link": "http://arxiv.org/abs/2412.17804v1", "date": "2024-12-23", "relevancy": 2.9653, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6301}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5807}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GauSim%3A%20Registering%20Elastic%20Objects%20into%20Digital%20World%20by%20Gaussian%0A%20%20Simulator&body=Title%3A%20GauSim%3A%20Registering%20Elastic%20Objects%20into%20Digital%20World%20by%20Gaussian%0A%20%20Simulator%0AAuthor%3A%20Yidi%20Shao%20and%20Mu%20Huang%20and%20Chen%20Change%20Loy%20and%20Bo%20Dai%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20GauSim%2C%20a%20novel%20neural%20network-based%20simulator%0Adesigned%20to%20capture%20the%20dynamic%20behaviors%20of%20real-world%20elastic%20objects%0Arepresented%20through%20Gaussian%20kernels.%20Unlike%20traditional%20methods%20that%20treat%0Akernels%20as%20particles%20within%20particle-based%20simulations%2C%20we%20leverage%20continuum%0Amechanics%2C%20modeling%20each%20kernel%20as%20a%20continuous%20piece%20of%20matter%20to%20account%20for%0Arealistic%20deformations%20without%20idealized%20assumptions.%20To%20improve%20computational%0Aefficiency%20and%20fidelity%2C%20we%20employ%20a%20hierarchical%20structure%20that%20organizes%0Akernels%20into%20Center%20of%20Mass%20Systems%20%28CMS%29%20with%20explicit%20formulations%2C%20enabling%0Aa%20coarse-to-fine%20simulation%20approach.%20This%20structure%20significantly%20reduces%0Acomputational%20overhead%20while%20preserving%20detailed%20dynamics.%20In%20addition%2C%20GauSim%0Aincorporates%20explicit%20physics%20constraints%2C%20such%20as%20mass%20and%20momentum%0Aconservation%2C%20ensuring%20interpretable%20results%20and%20robust%2C%20physically%20plausible%0Asimulations.%20To%20validate%20our%20approach%2C%20we%20present%20a%20new%20dataset%2C%20READY%2C%0Acontaining%20multi-view%20videos%20of%20real-world%20elastic%20deformations.%20Experimental%0Aresults%20demonstrate%20that%20GauSim%20achieves%20superior%20performance%20compared%20to%0Aexisting%20physics-driven%20baselines%2C%20offering%20a%20practical%20and%20accurate%20solution%0Afor%20simulating%20complex%20dynamic%20behaviors.%20Code%20and%20model%20will%20be%20released.%0AProject%20page%3A%20https%3A//www.mmlab-ntu.com/project/gausim/index.html%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGauSim%253A%2520Registering%2520Elastic%2520Objects%2520into%2520Digital%2520World%2520by%2520Gaussian%250A%2520%2520Simulator%26entry.906535625%3DYidi%2520Shao%2520and%2520Mu%2520Huang%2520and%2520Chen%2520Change%2520Loy%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520GauSim%252C%2520a%2520novel%2520neural%2520network-based%2520simulator%250Adesigned%2520to%2520capture%2520the%2520dynamic%2520behaviors%2520of%2520real-world%2520elastic%2520objects%250Arepresented%2520through%2520Gaussian%2520kernels.%2520Unlike%2520traditional%2520methods%2520that%2520treat%250Akernels%2520as%2520particles%2520within%2520particle-based%2520simulations%252C%2520we%2520leverage%2520continuum%250Amechanics%252C%2520modeling%2520each%2520kernel%2520as%2520a%2520continuous%2520piece%2520of%2520matter%2520to%2520account%2520for%250Arealistic%2520deformations%2520without%2520idealized%2520assumptions.%2520To%2520improve%2520computational%250Aefficiency%2520and%2520fidelity%252C%2520we%2520employ%2520a%2520hierarchical%2520structure%2520that%2520organizes%250Akernels%2520into%2520Center%2520of%2520Mass%2520Systems%2520%2528CMS%2529%2520with%2520explicit%2520formulations%252C%2520enabling%250Aa%2520coarse-to-fine%2520simulation%2520approach.%2520This%2520structure%2520significantly%2520reduces%250Acomputational%2520overhead%2520while%2520preserving%2520detailed%2520dynamics.%2520In%2520addition%252C%2520GauSim%250Aincorporates%2520explicit%2520physics%2520constraints%252C%2520such%2520as%2520mass%2520and%2520momentum%250Aconservation%252C%2520ensuring%2520interpretable%2520results%2520and%2520robust%252C%2520physically%2520plausible%250Asimulations.%2520To%2520validate%2520our%2520approach%252C%2520we%2520present%2520a%2520new%2520dataset%252C%2520READY%252C%250Acontaining%2520multi-view%2520videos%2520of%2520real-world%2520elastic%2520deformations.%2520Experimental%250Aresults%2520demonstrate%2520that%2520GauSim%2520achieves%2520superior%2520performance%2520compared%2520to%250Aexisting%2520physics-driven%2520baselines%252C%2520offering%2520a%2520practical%2520and%2520accurate%2520solution%250Afor%2520simulating%2520complex%2520dynamic%2520behaviors.%2520Code%2520and%2520model%2520will%2520be%2520released.%250AProject%2520page%253A%2520https%253A//www.mmlab-ntu.com/project/gausim/index.html%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GauSim%3A%20Registering%20Elastic%20Objects%20into%20Digital%20World%20by%20Gaussian%0A%20%20Simulator&entry.906535625=Yidi%20Shao%20and%20Mu%20Huang%20and%20Chen%20Change%20Loy%20and%20Bo%20Dai&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20GauSim%2C%20a%20novel%20neural%20network-based%20simulator%0Adesigned%20to%20capture%20the%20dynamic%20behaviors%20of%20real-world%20elastic%20objects%0Arepresented%20through%20Gaussian%20kernels.%20Unlike%20traditional%20methods%20that%20treat%0Akernels%20as%20particles%20within%20particle-based%20simulations%2C%20we%20leverage%20continuum%0Amechanics%2C%20modeling%20each%20kernel%20as%20a%20continuous%20piece%20of%20matter%20to%20account%20for%0Arealistic%20deformations%20without%20idealized%20assumptions.%20To%20improve%20computational%0Aefficiency%20and%20fidelity%2C%20we%20employ%20a%20hierarchical%20structure%20that%20organizes%0Akernels%20into%20Center%20of%20Mass%20Systems%20%28CMS%29%20with%20explicit%20formulations%2C%20enabling%0Aa%20coarse-to-fine%20simulation%20approach.%20This%20structure%20significantly%20reduces%0Acomputational%20overhead%20while%20preserving%20detailed%20dynamics.%20In%20addition%2C%20GauSim%0Aincorporates%20explicit%20physics%20constraints%2C%20such%20as%20mass%20and%20momentum%0Aconservation%2C%20ensuring%20interpretable%20results%20and%20robust%2C%20physically%20plausible%0Asimulations.%20To%20validate%20our%20approach%2C%20we%20present%20a%20new%20dataset%2C%20READY%2C%0Acontaining%20multi-view%20videos%20of%20real-world%20elastic%20deformations.%20Experimental%0Aresults%20demonstrate%20that%20GauSim%20achieves%20superior%20performance%20compared%20to%0Aexisting%20physics-driven%20baselines%2C%20offering%20a%20practical%20and%20accurate%20solution%0Afor%20simulating%20complex%20dynamic%20behaviors.%20Code%20and%20model%20will%20be%20released.%0AProject%20page%3A%20https%3A//www.mmlab-ntu.com/project/gausim/index.html%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17804v1&entry.124074799=Read"},
{"title": "S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit\n  Neural Field", "author": "Zixi Liang and Guowei Xu and Haifeng Wu and Ye Huang and Wen Li and Lixin Duan", "abstract": "  Learning-based methods have become increasingly popular in 3D indoor scene\nsynthesis (ISS), showing superior performance over traditional\noptimization-based approaches. These learning-based methods typically model\ndistributions on simple yet explicit scene representations using generative\nmodels. However, due to the oversimplified explicit representations that\noverlook detailed information and the lack of guidance from multimodal\nrelationships within the scene, most learning-based methods struggle to\ngenerate indoor scenes with realistic object arrangements and styles. In this\npaper, we introduce a new method, Scene Implicit Neural Field (S-INF), for\nindoor scene synthesis, aiming to learn meaningful representations of\nmultimodal relationships, to enhance the realism of indoor scene synthesis.\nS-INF assumes that the scene layout is often related to the object-detailed\ninformation. It disentangles the multimodal relationships into scene layout\nrelationships and detailed object relationships, fusing them later through\nimplicit neural fields (INFs). By learning specialized scene layout\nrelationships and projecting them into S-INF, we achieve a realistic generation\nof scene layout. Additionally, S-INF captures dense and detailed object\nrelationships through differentiable rendering, ensuring stylistic consistency\nacross objects. Through extensive experiments on the benchmark 3D-FRONT\ndataset, we demonstrate that our method consistently achieves state-of-the-art\nperformance under different types of ISS.\n", "link": "http://arxiv.org/abs/2412.17561v1", "date": "2024-12-23", "relevancy": 2.8644, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S-INF%3A%20Towards%20Realistic%20Indoor%20Scene%20Synthesis%20via%20Scene%20Implicit%0A%20%20Neural%20Field&body=Title%3A%20S-INF%3A%20Towards%20Realistic%20Indoor%20Scene%20Synthesis%20via%20Scene%20Implicit%0A%20%20Neural%20Field%0AAuthor%3A%20Zixi%20Liang%20and%20Guowei%20Xu%20and%20Haifeng%20Wu%20and%20Ye%20Huang%20and%20Wen%20Li%20and%20Lixin%20Duan%0AAbstract%3A%20%20%20Learning-based%20methods%20have%20become%20increasingly%20popular%20in%203D%20indoor%20scene%0Asynthesis%20%28ISS%29%2C%20showing%20superior%20performance%20over%20traditional%0Aoptimization-based%20approaches.%20These%20learning-based%20methods%20typically%20model%0Adistributions%20on%20simple%20yet%20explicit%20scene%20representations%20using%20generative%0Amodels.%20However%2C%20due%20to%20the%20oversimplified%20explicit%20representations%20that%0Aoverlook%20detailed%20information%20and%20the%20lack%20of%20guidance%20from%20multimodal%0Arelationships%20within%20the%20scene%2C%20most%20learning-based%20methods%20struggle%20to%0Agenerate%20indoor%20scenes%20with%20realistic%20object%20arrangements%20and%20styles.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20method%2C%20Scene%20Implicit%20Neural%20Field%20%28S-INF%29%2C%20for%0Aindoor%20scene%20synthesis%2C%20aiming%20to%20learn%20meaningful%20representations%20of%0Amultimodal%20relationships%2C%20to%20enhance%20the%20realism%20of%20indoor%20scene%20synthesis.%0AS-INF%20assumes%20that%20the%20scene%20layout%20is%20often%20related%20to%20the%20object-detailed%0Ainformation.%20It%20disentangles%20the%20multimodal%20relationships%20into%20scene%20layout%0Arelationships%20and%20detailed%20object%20relationships%2C%20fusing%20them%20later%20through%0Aimplicit%20neural%20fields%20%28INFs%29.%20By%20learning%20specialized%20scene%20layout%0Arelationships%20and%20projecting%20them%20into%20S-INF%2C%20we%20achieve%20a%20realistic%20generation%0Aof%20scene%20layout.%20Additionally%2C%20S-INF%20captures%20dense%20and%20detailed%20object%0Arelationships%20through%20differentiable%20rendering%2C%20ensuring%20stylistic%20consistency%0Aacross%20objects.%20Through%20extensive%20experiments%20on%20the%20benchmark%203D-FRONT%0Adataset%2C%20we%20demonstrate%20that%20our%20method%20consistently%20achieves%20state-of-the-art%0Aperformance%20under%20different%20types%20of%20ISS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS-INF%253A%2520Towards%2520Realistic%2520Indoor%2520Scene%2520Synthesis%2520via%2520Scene%2520Implicit%250A%2520%2520Neural%2520Field%26entry.906535625%3DZixi%2520Liang%2520and%2520Guowei%2520Xu%2520and%2520Haifeng%2520Wu%2520and%2520Ye%2520Huang%2520and%2520Wen%2520Li%2520and%2520Lixin%2520Duan%26entry.1292438233%3D%2520%2520Learning-based%2520methods%2520have%2520become%2520increasingly%2520popular%2520in%25203D%2520indoor%2520scene%250Asynthesis%2520%2528ISS%2529%252C%2520showing%2520superior%2520performance%2520over%2520traditional%250Aoptimization-based%2520approaches.%2520These%2520learning-based%2520methods%2520typically%2520model%250Adistributions%2520on%2520simple%2520yet%2520explicit%2520scene%2520representations%2520using%2520generative%250Amodels.%2520However%252C%2520due%2520to%2520the%2520oversimplified%2520explicit%2520representations%2520that%250Aoverlook%2520detailed%2520information%2520and%2520the%2520lack%2520of%2520guidance%2520from%2520multimodal%250Arelationships%2520within%2520the%2520scene%252C%2520most%2520learning-based%2520methods%2520struggle%2520to%250Agenerate%2520indoor%2520scenes%2520with%2520realistic%2520object%2520arrangements%2520and%2520styles.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520new%2520method%252C%2520Scene%2520Implicit%2520Neural%2520Field%2520%2528S-INF%2529%252C%2520for%250Aindoor%2520scene%2520synthesis%252C%2520aiming%2520to%2520learn%2520meaningful%2520representations%2520of%250Amultimodal%2520relationships%252C%2520to%2520enhance%2520the%2520realism%2520of%2520indoor%2520scene%2520synthesis.%250AS-INF%2520assumes%2520that%2520the%2520scene%2520layout%2520is%2520often%2520related%2520to%2520the%2520object-detailed%250Ainformation.%2520It%2520disentangles%2520the%2520multimodal%2520relationships%2520into%2520scene%2520layout%250Arelationships%2520and%2520detailed%2520object%2520relationships%252C%2520fusing%2520them%2520later%2520through%250Aimplicit%2520neural%2520fields%2520%2528INFs%2529.%2520By%2520learning%2520specialized%2520scene%2520layout%250Arelationships%2520and%2520projecting%2520them%2520into%2520S-INF%252C%2520we%2520achieve%2520a%2520realistic%2520generation%250Aof%2520scene%2520layout.%2520Additionally%252C%2520S-INF%2520captures%2520dense%2520and%2520detailed%2520object%250Arelationships%2520through%2520differentiable%2520rendering%252C%2520ensuring%2520stylistic%2520consistency%250Aacross%2520objects.%2520Through%2520extensive%2520experiments%2520on%2520the%2520benchmark%25203D-FRONT%250Adataset%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520consistently%2520achieves%2520state-of-the-art%250Aperformance%2520under%2520different%2520types%2520of%2520ISS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S-INF%3A%20Towards%20Realistic%20Indoor%20Scene%20Synthesis%20via%20Scene%20Implicit%0A%20%20Neural%20Field&entry.906535625=Zixi%20Liang%20and%20Guowei%20Xu%20and%20Haifeng%20Wu%20and%20Ye%20Huang%20and%20Wen%20Li%20and%20Lixin%20Duan&entry.1292438233=%20%20Learning-based%20methods%20have%20become%20increasingly%20popular%20in%203D%20indoor%20scene%0Asynthesis%20%28ISS%29%2C%20showing%20superior%20performance%20over%20traditional%0Aoptimization-based%20approaches.%20These%20learning-based%20methods%20typically%20model%0Adistributions%20on%20simple%20yet%20explicit%20scene%20representations%20using%20generative%0Amodels.%20However%2C%20due%20to%20the%20oversimplified%20explicit%20representations%20that%0Aoverlook%20detailed%20information%20and%20the%20lack%20of%20guidance%20from%20multimodal%0Arelationships%20within%20the%20scene%2C%20most%20learning-based%20methods%20struggle%20to%0Agenerate%20indoor%20scenes%20with%20realistic%20object%20arrangements%20and%20styles.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20method%2C%20Scene%20Implicit%20Neural%20Field%20%28S-INF%29%2C%20for%0Aindoor%20scene%20synthesis%2C%20aiming%20to%20learn%20meaningful%20representations%20of%0Amultimodal%20relationships%2C%20to%20enhance%20the%20realism%20of%20indoor%20scene%20synthesis.%0AS-INF%20assumes%20that%20the%20scene%20layout%20is%20often%20related%20to%20the%20object-detailed%0Ainformation.%20It%20disentangles%20the%20multimodal%20relationships%20into%20scene%20layout%0Arelationships%20and%20detailed%20object%20relationships%2C%20fusing%20them%20later%20through%0Aimplicit%20neural%20fields%20%28INFs%29.%20By%20learning%20specialized%20scene%20layout%0Arelationships%20and%20projecting%20them%20into%20S-INF%2C%20we%20achieve%20a%20realistic%20generation%0Aof%20scene%20layout.%20Additionally%2C%20S-INF%20captures%20dense%20and%20detailed%20object%0Arelationships%20through%20differentiable%20rendering%2C%20ensuring%20stylistic%20consistency%0Aacross%20objects.%20Through%20extensive%20experiments%20on%20the%20benchmark%203D-FRONT%0Adataset%2C%20we%20demonstrate%20that%20our%20method%20consistently%20achieves%20state-of-the-art%0Aperformance%20under%20different%20types%20of%20ISS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17561v1&entry.124074799=Read"},
{"title": "Exploring Dynamic Novel View Synthesis Technologies for Cinematography", "author": "Adrian Azzarelli and Nantheera Anantrasirichai and David R Bull", "abstract": "  Novel view synthesis (NVS) has shown significant promise for applications in\ncinematographic production, particularly through the exploitation of Neural\nRadiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D\nscenes, enabling the creation of new shots that are challenging to capture in\nthe real world due to set topology or expensive equipment requirement. This\ninnovation also offers cinematographic advantages such as smooth camera\nmovements, virtual re-shoots, slow-motion effects, etc. This paper explores\ndynamic NVS with the aim of facilitating the model selection process. We\nshowcase its potential through a short montage filmed using various NVS models.\n", "link": "http://arxiv.org/abs/2412.17532v1", "date": "2024-12-23", "relevancy": 2.8558, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5773}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5773}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Dynamic%20Novel%20View%20Synthesis%20Technologies%20for%20Cinematography&body=Title%3A%20Exploring%20Dynamic%20Novel%20View%20Synthesis%20Technologies%20for%20Cinematography%0AAuthor%3A%20Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20%28NVS%29%20has%20shown%20significant%20promise%20for%20applications%20in%0Acinematographic%20production%2C%20particularly%20through%20the%20exploitation%20of%20Neural%0ARadiance%20Fields%20%28NeRF%29%20and%20Gaussian%20Splatting%20%28GS%29.%20These%20methods%20model%20real%203D%0Ascenes%2C%20enabling%20the%20creation%20of%20new%20shots%20that%20are%20challenging%20to%20capture%20in%0Athe%20real%20world%20due%20to%20set%20topology%20or%20expensive%20equipment%20requirement.%20This%0Ainnovation%20also%20offers%20cinematographic%20advantages%20such%20as%20smooth%20camera%0Amovements%2C%20virtual%20re-shoots%2C%20slow-motion%20effects%2C%20etc.%20This%20paper%20explores%0Adynamic%20NVS%20with%20the%20aim%20of%20facilitating%20the%20model%20selection%20process.%20We%0Ashowcase%20its%20potential%20through%20a%20short%20montage%20filmed%20using%20various%20NVS%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Dynamic%2520Novel%2520View%2520Synthesis%2520Technologies%2520for%2520Cinematography%26entry.906535625%3DAdrian%2520Azzarelli%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520David%2520R%2520Bull%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520%2528NVS%2529%2520has%2520shown%2520significant%2520promise%2520for%2520applications%2520in%250Acinematographic%2520production%252C%2520particularly%2520through%2520the%2520exploitation%2520of%2520Neural%250ARadiance%2520Fields%2520%2528NeRF%2529%2520and%2520Gaussian%2520Splatting%2520%2528GS%2529.%2520These%2520methods%2520model%2520real%25203D%250Ascenes%252C%2520enabling%2520the%2520creation%2520of%2520new%2520shots%2520that%2520are%2520challenging%2520to%2520capture%2520in%250Athe%2520real%2520world%2520due%2520to%2520set%2520topology%2520or%2520expensive%2520equipment%2520requirement.%2520This%250Ainnovation%2520also%2520offers%2520cinematographic%2520advantages%2520such%2520as%2520smooth%2520camera%250Amovements%252C%2520virtual%2520re-shoots%252C%2520slow-motion%2520effects%252C%2520etc.%2520This%2520paper%2520explores%250Adynamic%2520NVS%2520with%2520the%2520aim%2520of%2520facilitating%2520the%2520model%2520selection%2520process.%2520We%250Ashowcase%2520its%2520potential%2520through%2520a%2520short%2520montage%2520filmed%2520using%2520various%2520NVS%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Dynamic%20Novel%20View%20Synthesis%20Technologies%20for%20Cinematography&entry.906535625=Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull&entry.1292438233=%20%20Novel%20view%20synthesis%20%28NVS%29%20has%20shown%20significant%20promise%20for%20applications%20in%0Acinematographic%20production%2C%20particularly%20through%20the%20exploitation%20of%20Neural%0ARadiance%20Fields%20%28NeRF%29%20and%20Gaussian%20Splatting%20%28GS%29.%20These%20methods%20model%20real%203D%0Ascenes%2C%20enabling%20the%20creation%20of%20new%20shots%20that%20are%20challenging%20to%20capture%20in%0Athe%20real%20world%20due%20to%20set%20topology%20or%20expensive%20equipment%20requirement.%20This%0Ainnovation%20also%20offers%20cinematographic%20advantages%20such%20as%20smooth%20camera%0Amovements%2C%20virtual%20re-shoots%2C%20slow-motion%20effects%2C%20etc.%20This%20paper%20explores%0Adynamic%20NVS%20with%20the%20aim%20of%20facilitating%20the%20model%20selection%20process.%20We%0Ashowcase%20its%20potential%20through%20a%20short%20montage%20filmed%20using%20various%20NVS%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17532v1&entry.124074799=Read"},
{"title": "ANID: How Far Are We? Evaluating the Discrepancies Between\n  AI-synthesized Images and Natural Images through Multimodal Guidance", "author": "Renyang Liu and Ziyu Lyu and Wei Zhou and See-Kiong Ng", "abstract": "  In the rapidly evolving field of Artificial Intelligence Generated Content\n(AIGC), one of the key challenges is distinguishing AI-synthesized images from\nnatural images. Despite the remarkable capabilities of advanced AI generative\nmodels in producing visually compelling images, significant discrepancies\nremain when these images are compared to natural ones. To systematically\ninvestigate and quantify these discrepancies, we introduce an AI-Natural Image\nDiscrepancy Evaluation benchmark aimed at addressing the critical question:\n\\textit{how far are AI-generated images (AIGIs) from truly realistic images?}\nWe have constructed a large-scale multimodal dataset, the Distinguishing\nNatural and AI-generated Images (DNAI) dataset, which includes over 440,000\nAIGI samples generated by 8 representative models using both unimodal and\nmultimodal prompts, such as Text-to-Image (T2I), Image-to-Image (I2I), and Text\n\\textit{vs.} Image-to-Image (TI2I). Our fine-grained assessment framework\nprovides a comprehensive evaluation of the DNAI dataset across five key\ndimensions: naive visual feature quality, semantic alignment in multimodal\ngeneration, aesthetic appeal, downstream task applicability, and coordinated\nhuman validation. Extensive evaluation results highlight significant\ndiscrepancies across these dimensions, underscoring the necessity of aligning\nquantitative metrics with human judgment to achieve a holistic understanding of\nAI-generated image quality. Code is available at\n\\href{https://github.com/ryliu68/ANID}{https://github.com/ryliu68/ANID}.\n", "link": "http://arxiv.org/abs/2412.17632v1", "date": "2024-12-23", "relevancy": 2.8219, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5877}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5602}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANID%3A%20How%20Far%20Are%20We%3F%20Evaluating%20the%20Discrepancies%20Between%0A%20%20AI-synthesized%20Images%20and%20Natural%20Images%20through%20Multimodal%20Guidance&body=Title%3A%20ANID%3A%20How%20Far%20Are%20We%3F%20Evaluating%20the%20Discrepancies%20Between%0A%20%20AI-synthesized%20Images%20and%20Natural%20Images%20through%20Multimodal%20Guidance%0AAuthor%3A%20Renyang%20Liu%20and%20Ziyu%20Lyu%20and%20Wei%20Zhou%20and%20See-Kiong%20Ng%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20Artificial%20Intelligence%20Generated%20Content%0A%28AIGC%29%2C%20one%20of%20the%20key%20challenges%20is%20distinguishing%20AI-synthesized%20images%20from%0Anatural%20images.%20Despite%20the%20remarkable%20capabilities%20of%20advanced%20AI%20generative%0Amodels%20in%20producing%20visually%20compelling%20images%2C%20significant%20discrepancies%0Aremain%20when%20these%20images%20are%20compared%20to%20natural%20ones.%20To%20systematically%0Ainvestigate%20and%20quantify%20these%20discrepancies%2C%20we%20introduce%20an%20AI-Natural%20Image%0ADiscrepancy%20Evaluation%20benchmark%20aimed%20at%20addressing%20the%20critical%20question%3A%0A%5Ctextit%7Bhow%20far%20are%20AI-generated%20images%20%28AIGIs%29%20from%20truly%20realistic%20images%3F%7D%0AWe%20have%20constructed%20a%20large-scale%20multimodal%20dataset%2C%20the%20Distinguishing%0ANatural%20and%20AI-generated%20Images%20%28DNAI%29%20dataset%2C%20which%20includes%20over%20440%2C000%0AAIGI%20samples%20generated%20by%208%20representative%20models%20using%20both%20unimodal%20and%0Amultimodal%20prompts%2C%20such%20as%20Text-to-Image%20%28T2I%29%2C%20Image-to-Image%20%28I2I%29%2C%20and%20Text%0A%5Ctextit%7Bvs.%7D%20Image-to-Image%20%28TI2I%29.%20Our%20fine-grained%20assessment%20framework%0Aprovides%20a%20comprehensive%20evaluation%20of%20the%20DNAI%20dataset%20across%20five%20key%0Adimensions%3A%20naive%20visual%20feature%20quality%2C%20semantic%20alignment%20in%20multimodal%0Ageneration%2C%20aesthetic%20appeal%2C%20downstream%20task%20applicability%2C%20and%20coordinated%0Ahuman%20validation.%20Extensive%20evaluation%20results%20highlight%20significant%0Adiscrepancies%20across%20these%20dimensions%2C%20underscoring%20the%20necessity%20of%20aligning%0Aquantitative%20metrics%20with%20human%20judgment%20to%20achieve%20a%20holistic%20understanding%20of%0AAI-generated%20image%20quality.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/ryliu68/ANID%7D%7Bhttps%3A//github.com/ryliu68/ANID%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANID%253A%2520How%2520Far%2520Are%2520We%253F%2520Evaluating%2520the%2520Discrepancies%2520Between%250A%2520%2520AI-synthesized%2520Images%2520and%2520Natural%2520Images%2520through%2520Multimodal%2520Guidance%26entry.906535625%3DRenyang%2520Liu%2520and%2520Ziyu%2520Lyu%2520and%2520Wei%2520Zhou%2520and%2520See-Kiong%2520Ng%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520Artificial%2520Intelligence%2520Generated%2520Content%250A%2528AIGC%2529%252C%2520one%2520of%2520the%2520key%2520challenges%2520is%2520distinguishing%2520AI-synthesized%2520images%2520from%250Anatural%2520images.%2520Despite%2520the%2520remarkable%2520capabilities%2520of%2520advanced%2520AI%2520generative%250Amodels%2520in%2520producing%2520visually%2520compelling%2520images%252C%2520significant%2520discrepancies%250Aremain%2520when%2520these%2520images%2520are%2520compared%2520to%2520natural%2520ones.%2520To%2520systematically%250Ainvestigate%2520and%2520quantify%2520these%2520discrepancies%252C%2520we%2520introduce%2520an%2520AI-Natural%2520Image%250ADiscrepancy%2520Evaluation%2520benchmark%2520aimed%2520at%2520addressing%2520the%2520critical%2520question%253A%250A%255Ctextit%257Bhow%2520far%2520are%2520AI-generated%2520images%2520%2528AIGIs%2529%2520from%2520truly%2520realistic%2520images%253F%257D%250AWe%2520have%2520constructed%2520a%2520large-scale%2520multimodal%2520dataset%252C%2520the%2520Distinguishing%250ANatural%2520and%2520AI-generated%2520Images%2520%2528DNAI%2529%2520dataset%252C%2520which%2520includes%2520over%2520440%252C000%250AAIGI%2520samples%2520generated%2520by%25208%2520representative%2520models%2520using%2520both%2520unimodal%2520and%250Amultimodal%2520prompts%252C%2520such%2520as%2520Text-to-Image%2520%2528T2I%2529%252C%2520Image-to-Image%2520%2528I2I%2529%252C%2520and%2520Text%250A%255Ctextit%257Bvs.%257D%2520Image-to-Image%2520%2528TI2I%2529.%2520Our%2520fine-grained%2520assessment%2520framework%250Aprovides%2520a%2520comprehensive%2520evaluation%2520of%2520the%2520DNAI%2520dataset%2520across%2520five%2520key%250Adimensions%253A%2520naive%2520visual%2520feature%2520quality%252C%2520semantic%2520alignment%2520in%2520multimodal%250Ageneration%252C%2520aesthetic%2520appeal%252C%2520downstream%2520task%2520applicability%252C%2520and%2520coordinated%250Ahuman%2520validation.%2520Extensive%2520evaluation%2520results%2520highlight%2520significant%250Adiscrepancies%2520across%2520these%2520dimensions%252C%2520underscoring%2520the%2520necessity%2520of%2520aligning%250Aquantitative%2520metrics%2520with%2520human%2520judgment%2520to%2520achieve%2520a%2520holistic%2520understanding%2520of%250AAI-generated%2520image%2520quality.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/ryliu68/ANID%257D%257Bhttps%253A//github.com/ryliu68/ANID%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANID%3A%20How%20Far%20Are%20We%3F%20Evaluating%20the%20Discrepancies%20Between%0A%20%20AI-synthesized%20Images%20and%20Natural%20Images%20through%20Multimodal%20Guidance&entry.906535625=Renyang%20Liu%20and%20Ziyu%20Lyu%20and%20Wei%20Zhou%20and%20See-Kiong%20Ng&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20Artificial%20Intelligence%20Generated%20Content%0A%28AIGC%29%2C%20one%20of%20the%20key%20challenges%20is%20distinguishing%20AI-synthesized%20images%20from%0Anatural%20images.%20Despite%20the%20remarkable%20capabilities%20of%20advanced%20AI%20generative%0Amodels%20in%20producing%20visually%20compelling%20images%2C%20significant%20discrepancies%0Aremain%20when%20these%20images%20are%20compared%20to%20natural%20ones.%20To%20systematically%0Ainvestigate%20and%20quantify%20these%20discrepancies%2C%20we%20introduce%20an%20AI-Natural%20Image%0ADiscrepancy%20Evaluation%20benchmark%20aimed%20at%20addressing%20the%20critical%20question%3A%0A%5Ctextit%7Bhow%20far%20are%20AI-generated%20images%20%28AIGIs%29%20from%20truly%20realistic%20images%3F%7D%0AWe%20have%20constructed%20a%20large-scale%20multimodal%20dataset%2C%20the%20Distinguishing%0ANatural%20and%20AI-generated%20Images%20%28DNAI%29%20dataset%2C%20which%20includes%20over%20440%2C000%0AAIGI%20samples%20generated%20by%208%20representative%20models%20using%20both%20unimodal%20and%0Amultimodal%20prompts%2C%20such%20as%20Text-to-Image%20%28T2I%29%2C%20Image-to-Image%20%28I2I%29%2C%20and%20Text%0A%5Ctextit%7Bvs.%7D%20Image-to-Image%20%28TI2I%29.%20Our%20fine-grained%20assessment%20framework%0Aprovides%20a%20comprehensive%20evaluation%20of%20the%20DNAI%20dataset%20across%20five%20key%0Adimensions%3A%20naive%20visual%20feature%20quality%2C%20semantic%20alignment%20in%20multimodal%0Ageneration%2C%20aesthetic%20appeal%2C%20downstream%20task%20applicability%2C%20and%20coordinated%0Ahuman%20validation.%20Extensive%20evaluation%20results%20highlight%20significant%0Adiscrepancies%20across%20these%20dimensions%2C%20underscoring%20the%20necessity%20of%20aligning%0Aquantitative%20metrics%20with%20human%20judgment%20to%20achieve%20a%20holistic%20understanding%20of%0AAI-generated%20image%20quality.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/ryliu68/ANID%7D%7Bhttps%3A//github.com/ryliu68/ANID%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17632v1&entry.124074799=Read"},
{"title": "DreamFit: Garment-Centric Human Generation via a Lightweight\n  Anything-Dressing Encoder", "author": "Ente Lin and Xujie Zhang and Fuwei Zhao and Yuxuan Luo and Xin Dong and Long Zeng and Xiaodan Liang", "abstract": "  Diffusion models for garment-centric human generation from text or image\nprompts have garnered emerging attention for their great application potential.\nHowever, existing methods often face a dilemma: lightweight approaches, such as\nadapters, are prone to generate inconsistent textures; while finetune-based\nmethods involve high training costs and struggle to maintain the generalization\ncapabilities of pretrained diffusion models, limiting their performance across\ndiverse scenarios. To address these challenges, we propose DreamFit, which\nincorporates a lightweight Anything-Dressing Encoder specifically tailored for\nthe garment-centric human generation. DreamFit has three key advantages: (1)\n\\textbf{Lightweight training}: with the proposed adaptive attention and LoRA\nmodules, DreamFit significantly minimizes the model complexity to 83.4M\ntrainable parameters. (2)\\textbf{Anything-Dressing}: Our model generalizes\nsurprisingly well to a wide range of (non-)garments, creative styles, and\nprompt instructions, consistently delivering high-quality results across\ndiverse scenarios. (3) \\textbf{Plug-and-play}: DreamFit is engineered for\nsmooth integration with any community control plugins for diffusion models,\nensuring easy compatibility and minimizing adoption barriers. To further\nenhance generation quality, DreamFit leverages pretrained large multi-modal\nmodels (LMMs) to enrich the prompt with fine-grained garment descriptions,\nthereby reducing the prompt gap between training and inference. We conduct\ncomprehensive experiments on both $768 \\times 512$ high-resolution benchmarks\nand in-the-wild images. DreamFit surpasses all existing methods, highlighting\nits state-of-the-art capabilities of garment-centric human generation.\n", "link": "http://arxiv.org/abs/2412.17644v1", "date": "2024-12-23", "relevancy": 2.7978, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7179}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6895}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamFit%3A%20Garment-Centric%20Human%20Generation%20via%20a%20Lightweight%0A%20%20Anything-Dressing%20Encoder&body=Title%3A%20DreamFit%3A%20Garment-Centric%20Human%20Generation%20via%20a%20Lightweight%0A%20%20Anything-Dressing%20Encoder%0AAuthor%3A%20Ente%20Lin%20and%20Xujie%20Zhang%20and%20Fuwei%20Zhao%20and%20Yuxuan%20Luo%20and%20Xin%20Dong%20and%20Long%20Zeng%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Diffusion%20models%20for%20garment-centric%20human%20generation%20from%20text%20or%20image%0Aprompts%20have%20garnered%20emerging%20attention%20for%20their%20great%20application%20potential.%0AHowever%2C%20existing%20methods%20often%20face%20a%20dilemma%3A%20lightweight%20approaches%2C%20such%20as%0Aadapters%2C%20are%20prone%20to%20generate%20inconsistent%20textures%3B%20while%20finetune-based%0Amethods%20involve%20high%20training%20costs%20and%20struggle%20to%20maintain%20the%20generalization%0Acapabilities%20of%20pretrained%20diffusion%20models%2C%20limiting%20their%20performance%20across%0Adiverse%20scenarios.%20To%20address%20these%20challenges%2C%20we%20propose%20DreamFit%2C%20which%0Aincorporates%20a%20lightweight%20Anything-Dressing%20Encoder%20specifically%20tailored%20for%0Athe%20garment-centric%20human%20generation.%20DreamFit%20has%20three%20key%20advantages%3A%20%281%29%0A%5Ctextbf%7BLightweight%20training%7D%3A%20with%20the%20proposed%20adaptive%20attention%20and%20LoRA%0Amodules%2C%20DreamFit%20significantly%20minimizes%20the%20model%20complexity%20to%2083.4M%0Atrainable%20parameters.%20%282%29%5Ctextbf%7BAnything-Dressing%7D%3A%20Our%20model%20generalizes%0Asurprisingly%20well%20to%20a%20wide%20range%20of%20%28non-%29garments%2C%20creative%20styles%2C%20and%0Aprompt%20instructions%2C%20consistently%20delivering%20high-quality%20results%20across%0Adiverse%20scenarios.%20%283%29%20%5Ctextbf%7BPlug-and-play%7D%3A%20DreamFit%20is%20engineered%20for%0Asmooth%20integration%20with%20any%20community%20control%20plugins%20for%20diffusion%20models%2C%0Aensuring%20easy%20compatibility%20and%20minimizing%20adoption%20barriers.%20To%20further%0Aenhance%20generation%20quality%2C%20DreamFit%20leverages%20pretrained%20large%20multi-modal%0Amodels%20%28LMMs%29%20to%20enrich%20the%20prompt%20with%20fine-grained%20garment%20descriptions%2C%0Athereby%20reducing%20the%20prompt%20gap%20between%20training%20and%20inference.%20We%20conduct%0Acomprehensive%20experiments%20on%20both%20%24768%20%5Ctimes%20512%24%20high-resolution%20benchmarks%0Aand%20in-the-wild%20images.%20DreamFit%20surpasses%20all%20existing%20methods%2C%20highlighting%0Aits%20state-of-the-art%20capabilities%20of%20garment-centric%20human%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamFit%253A%2520Garment-Centric%2520Human%2520Generation%2520via%2520a%2520Lightweight%250A%2520%2520Anything-Dressing%2520Encoder%26entry.906535625%3DEnte%2520Lin%2520and%2520Xujie%2520Zhang%2520and%2520Fuwei%2520Zhao%2520and%2520Yuxuan%2520Luo%2520and%2520Xin%2520Dong%2520and%2520Long%2520Zeng%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520for%2520garment-centric%2520human%2520generation%2520from%2520text%2520or%2520image%250Aprompts%2520have%2520garnered%2520emerging%2520attention%2520for%2520their%2520great%2520application%2520potential.%250AHowever%252C%2520existing%2520methods%2520often%2520face%2520a%2520dilemma%253A%2520lightweight%2520approaches%252C%2520such%2520as%250Aadapters%252C%2520are%2520prone%2520to%2520generate%2520inconsistent%2520textures%253B%2520while%2520finetune-based%250Amethods%2520involve%2520high%2520training%2520costs%2520and%2520struggle%2520to%2520maintain%2520the%2520generalization%250Acapabilities%2520of%2520pretrained%2520diffusion%2520models%252C%2520limiting%2520their%2520performance%2520across%250Adiverse%2520scenarios.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DreamFit%252C%2520which%250Aincorporates%2520a%2520lightweight%2520Anything-Dressing%2520Encoder%2520specifically%2520tailored%2520for%250Athe%2520garment-centric%2520human%2520generation.%2520DreamFit%2520has%2520three%2520key%2520advantages%253A%2520%25281%2529%250A%255Ctextbf%257BLightweight%2520training%257D%253A%2520with%2520the%2520proposed%2520adaptive%2520attention%2520and%2520LoRA%250Amodules%252C%2520DreamFit%2520significantly%2520minimizes%2520the%2520model%2520complexity%2520to%252083.4M%250Atrainable%2520parameters.%2520%25282%2529%255Ctextbf%257BAnything-Dressing%257D%253A%2520Our%2520model%2520generalizes%250Asurprisingly%2520well%2520to%2520a%2520wide%2520range%2520of%2520%2528non-%2529garments%252C%2520creative%2520styles%252C%2520and%250Aprompt%2520instructions%252C%2520consistently%2520delivering%2520high-quality%2520results%2520across%250Adiverse%2520scenarios.%2520%25283%2529%2520%255Ctextbf%257BPlug-and-play%257D%253A%2520DreamFit%2520is%2520engineered%2520for%250Asmooth%2520integration%2520with%2520any%2520community%2520control%2520plugins%2520for%2520diffusion%2520models%252C%250Aensuring%2520easy%2520compatibility%2520and%2520minimizing%2520adoption%2520barriers.%2520To%2520further%250Aenhance%2520generation%2520quality%252C%2520DreamFit%2520leverages%2520pretrained%2520large%2520multi-modal%250Amodels%2520%2528LMMs%2529%2520to%2520enrich%2520the%2520prompt%2520with%2520fine-grained%2520garment%2520descriptions%252C%250Athereby%2520reducing%2520the%2520prompt%2520gap%2520between%2520training%2520and%2520inference.%2520We%2520conduct%250Acomprehensive%2520experiments%2520on%2520both%2520%2524768%2520%255Ctimes%2520512%2524%2520high-resolution%2520benchmarks%250Aand%2520in-the-wild%2520images.%2520DreamFit%2520surpasses%2520all%2520existing%2520methods%252C%2520highlighting%250Aits%2520state-of-the-art%2520capabilities%2520of%2520garment-centric%2520human%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamFit%3A%20Garment-Centric%20Human%20Generation%20via%20a%20Lightweight%0A%20%20Anything-Dressing%20Encoder&entry.906535625=Ente%20Lin%20and%20Xujie%20Zhang%20and%20Fuwei%20Zhao%20and%20Yuxuan%20Luo%20and%20Xin%20Dong%20and%20Long%20Zeng%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Diffusion%20models%20for%20garment-centric%20human%20generation%20from%20text%20or%20image%0Aprompts%20have%20garnered%20emerging%20attention%20for%20their%20great%20application%20potential.%0AHowever%2C%20existing%20methods%20often%20face%20a%20dilemma%3A%20lightweight%20approaches%2C%20such%20as%0Aadapters%2C%20are%20prone%20to%20generate%20inconsistent%20textures%3B%20while%20finetune-based%0Amethods%20involve%20high%20training%20costs%20and%20struggle%20to%20maintain%20the%20generalization%0Acapabilities%20of%20pretrained%20diffusion%20models%2C%20limiting%20their%20performance%20across%0Adiverse%20scenarios.%20To%20address%20these%20challenges%2C%20we%20propose%20DreamFit%2C%20which%0Aincorporates%20a%20lightweight%20Anything-Dressing%20Encoder%20specifically%20tailored%20for%0Athe%20garment-centric%20human%20generation.%20DreamFit%20has%20three%20key%20advantages%3A%20%281%29%0A%5Ctextbf%7BLightweight%20training%7D%3A%20with%20the%20proposed%20adaptive%20attention%20and%20LoRA%0Amodules%2C%20DreamFit%20significantly%20minimizes%20the%20model%20complexity%20to%2083.4M%0Atrainable%20parameters.%20%282%29%5Ctextbf%7BAnything-Dressing%7D%3A%20Our%20model%20generalizes%0Asurprisingly%20well%20to%20a%20wide%20range%20of%20%28non-%29garments%2C%20creative%20styles%2C%20and%0Aprompt%20instructions%2C%20consistently%20delivering%20high-quality%20results%20across%0Adiverse%20scenarios.%20%283%29%20%5Ctextbf%7BPlug-and-play%7D%3A%20DreamFit%20is%20engineered%20for%0Asmooth%20integration%20with%20any%20community%20control%20plugins%20for%20diffusion%20models%2C%0Aensuring%20easy%20compatibility%20and%20minimizing%20adoption%20barriers.%20To%20further%0Aenhance%20generation%20quality%2C%20DreamFit%20leverages%20pretrained%20large%20multi-modal%0Amodels%20%28LMMs%29%20to%20enrich%20the%20prompt%20with%20fine-grained%20garment%20descriptions%2C%0Athereby%20reducing%20the%20prompt%20gap%20between%20training%20and%20inference.%20We%20conduct%0Acomprehensive%20experiments%20on%20both%20%24768%20%5Ctimes%20512%24%20high-resolution%20benchmarks%0Aand%20in-the-wild%20images.%20DreamFit%20surpasses%20all%20existing%20methods%2C%20highlighting%0Aits%20state-of-the-art%20capabilities%20of%20garment-centric%20human%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17644v1&entry.124074799=Read"},
{"title": "LokiTalk: Learning Fine-Grained and Generalizable Correspondences to\n  Enhance NeRF-based Talking Head Synthesis", "author": "Tianqi Li and Ruobing Zheng and Bonan Li and Zicheng Zhang and Meng Wang and Jingdong Chen and Ming Yang", "abstract": "  Despite significant progress in talking head synthesis since the introduction\nof Neural Radiance Fields (NeRF), visual artifacts and high training costs\npersist as major obstacles to large-scale commercial adoption. We propose that\nidentifying and establishing fine-grained and generalizable correspondences\nbetween driving signals and generated results can simultaneously resolve both\nproblems. Here we present LokiTalk, a novel framework designed to enhance\nNeRF-based talking heads with lifelike facial dynamics and improved training\nefficiency. To achieve fine-grained correspondences, we introduce\nRegion-Specific Deformation Fields, which decompose the overall portrait motion\ninto lip movements, eye blinking, head pose, and torso movements. By\nhierarchically modeling the driving signals and their associated regions\nthrough two cascaded deformation fields, we significantly improve dynamic\naccuracy and minimize synthetic artifacts. Furthermore, we propose ID-Aware\nKnowledge Transfer, a plug-and-play module that learns generalizable dynamic\nand static correspondences from multi-identity videos, while simultaneously\nextracting ID-specific dynamic and static features to refine the depiction of\nindividual characters. Comprehensive evaluations demonstrate that LokiTalk\ndelivers superior high-fidelity results and training efficiency compared to\nprevious methods. The code will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2411.19525v2", "date": "2024-12-23", "relevancy": 2.7644, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.583}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.542}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LokiTalk%3A%20Learning%20Fine-Grained%20and%20Generalizable%20Correspondences%20to%0A%20%20Enhance%20NeRF-based%20Talking%20Head%20Synthesis&body=Title%3A%20LokiTalk%3A%20Learning%20Fine-Grained%20and%20Generalizable%20Correspondences%20to%0A%20%20Enhance%20NeRF-based%20Talking%20Head%20Synthesis%0AAuthor%3A%20Tianqi%20Li%20and%20Ruobing%20Zheng%20and%20Bonan%20Li%20and%20Zicheng%20Zhang%20and%20Meng%20Wang%20and%20Jingdong%20Chen%20and%20Ming%20Yang%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%20talking%20head%20synthesis%20since%20the%20introduction%0Aof%20Neural%20Radiance%20Fields%20%28NeRF%29%2C%20visual%20artifacts%20and%20high%20training%20costs%0Apersist%20as%20major%20obstacles%20to%20large-scale%20commercial%20adoption.%20We%20propose%20that%0Aidentifying%20and%20establishing%20fine-grained%20and%20generalizable%20correspondences%0Abetween%20driving%20signals%20and%20generated%20results%20can%20simultaneously%20resolve%20both%0Aproblems.%20Here%20we%20present%20LokiTalk%2C%20a%20novel%20framework%20designed%20to%20enhance%0ANeRF-based%20talking%20heads%20with%20lifelike%20facial%20dynamics%20and%20improved%20training%0Aefficiency.%20To%20achieve%20fine-grained%20correspondences%2C%20we%20introduce%0ARegion-Specific%20Deformation%20Fields%2C%20which%20decompose%20the%20overall%20portrait%20motion%0Ainto%20lip%20movements%2C%20eye%20blinking%2C%20head%20pose%2C%20and%20torso%20movements.%20By%0Ahierarchically%20modeling%20the%20driving%20signals%20and%20their%20associated%20regions%0Athrough%20two%20cascaded%20deformation%20fields%2C%20we%20significantly%20improve%20dynamic%0Aaccuracy%20and%20minimize%20synthetic%20artifacts.%20Furthermore%2C%20we%20propose%20ID-Aware%0AKnowledge%20Transfer%2C%20a%20plug-and-play%20module%20that%20learns%20generalizable%20dynamic%0Aand%20static%20correspondences%20from%20multi-identity%20videos%2C%20while%20simultaneously%0Aextracting%20ID-specific%20dynamic%20and%20static%20features%20to%20refine%20the%20depiction%20of%0Aindividual%20characters.%20Comprehensive%20evaluations%20demonstrate%20that%20LokiTalk%0Adelivers%20superior%20high-fidelity%20results%20and%20training%20efficiency%20compared%20to%0Aprevious%20methods.%20The%20code%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19525v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLokiTalk%253A%2520Learning%2520Fine-Grained%2520and%2520Generalizable%2520Correspondences%2520to%250A%2520%2520Enhance%2520NeRF-based%2520Talking%2520Head%2520Synthesis%26entry.906535625%3DTianqi%2520Li%2520and%2520Ruobing%2520Zheng%2520and%2520Bonan%2520Li%2520and%2520Zicheng%2520Zhang%2520and%2520Meng%2520Wang%2520and%2520Jingdong%2520Chen%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%2520talking%2520head%2520synthesis%2520since%2520the%2520introduction%250Aof%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%252C%2520visual%2520artifacts%2520and%2520high%2520training%2520costs%250Apersist%2520as%2520major%2520obstacles%2520to%2520large-scale%2520commercial%2520adoption.%2520We%2520propose%2520that%250Aidentifying%2520and%2520establishing%2520fine-grained%2520and%2520generalizable%2520correspondences%250Abetween%2520driving%2520signals%2520and%2520generated%2520results%2520can%2520simultaneously%2520resolve%2520both%250Aproblems.%2520Here%2520we%2520present%2520LokiTalk%252C%2520a%2520novel%2520framework%2520designed%2520to%2520enhance%250ANeRF-based%2520talking%2520heads%2520with%2520lifelike%2520facial%2520dynamics%2520and%2520improved%2520training%250Aefficiency.%2520To%2520achieve%2520fine-grained%2520correspondences%252C%2520we%2520introduce%250ARegion-Specific%2520Deformation%2520Fields%252C%2520which%2520decompose%2520the%2520overall%2520portrait%2520motion%250Ainto%2520lip%2520movements%252C%2520eye%2520blinking%252C%2520head%2520pose%252C%2520and%2520torso%2520movements.%2520By%250Ahierarchically%2520modeling%2520the%2520driving%2520signals%2520and%2520their%2520associated%2520regions%250Athrough%2520two%2520cascaded%2520deformation%2520fields%252C%2520we%2520significantly%2520improve%2520dynamic%250Aaccuracy%2520and%2520minimize%2520synthetic%2520artifacts.%2520Furthermore%252C%2520we%2520propose%2520ID-Aware%250AKnowledge%2520Transfer%252C%2520a%2520plug-and-play%2520module%2520that%2520learns%2520generalizable%2520dynamic%250Aand%2520static%2520correspondences%2520from%2520multi-identity%2520videos%252C%2520while%2520simultaneously%250Aextracting%2520ID-specific%2520dynamic%2520and%2520static%2520features%2520to%2520refine%2520the%2520depiction%2520of%250Aindividual%2520characters.%2520Comprehensive%2520evaluations%2520demonstrate%2520that%2520LokiTalk%250Adelivers%2520superior%2520high-fidelity%2520results%2520and%2520training%2520efficiency%2520compared%2520to%250Aprevious%2520methods.%2520The%2520code%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19525v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LokiTalk%3A%20Learning%20Fine-Grained%20and%20Generalizable%20Correspondences%20to%0A%20%20Enhance%20NeRF-based%20Talking%20Head%20Synthesis&entry.906535625=Tianqi%20Li%20and%20Ruobing%20Zheng%20and%20Bonan%20Li%20and%20Zicheng%20Zhang%20and%20Meng%20Wang%20and%20Jingdong%20Chen%20and%20Ming%20Yang&entry.1292438233=%20%20Despite%20significant%20progress%20in%20talking%20head%20synthesis%20since%20the%20introduction%0Aof%20Neural%20Radiance%20Fields%20%28NeRF%29%2C%20visual%20artifacts%20and%20high%20training%20costs%0Apersist%20as%20major%20obstacles%20to%20large-scale%20commercial%20adoption.%20We%20propose%20that%0Aidentifying%20and%20establishing%20fine-grained%20and%20generalizable%20correspondences%0Abetween%20driving%20signals%20and%20generated%20results%20can%20simultaneously%20resolve%20both%0Aproblems.%20Here%20we%20present%20LokiTalk%2C%20a%20novel%20framework%20designed%20to%20enhance%0ANeRF-based%20talking%20heads%20with%20lifelike%20facial%20dynamics%20and%20improved%20training%0Aefficiency.%20To%20achieve%20fine-grained%20correspondences%2C%20we%20introduce%0ARegion-Specific%20Deformation%20Fields%2C%20which%20decompose%20the%20overall%20portrait%20motion%0Ainto%20lip%20movements%2C%20eye%20blinking%2C%20head%20pose%2C%20and%20torso%20movements.%20By%0Ahierarchically%20modeling%20the%20driving%20signals%20and%20their%20associated%20regions%0Athrough%20two%20cascaded%20deformation%20fields%2C%20we%20significantly%20improve%20dynamic%0Aaccuracy%20and%20minimize%20synthetic%20artifacts.%20Furthermore%2C%20we%20propose%20ID-Aware%0AKnowledge%20Transfer%2C%20a%20plug-and-play%20module%20that%20learns%20generalizable%20dynamic%0Aand%20static%20correspondences%20from%20multi-identity%20videos%2C%20while%20simultaneously%0Aextracting%20ID-specific%20dynamic%20and%20static%20features%20to%20refine%20the%20depiction%20of%0Aindividual%20characters.%20Comprehensive%20evaluations%20demonstrate%20that%20LokiTalk%0Adelivers%20superior%20high-fidelity%20results%20and%20training%20efficiency%20compared%20to%0Aprevious%20methods.%20The%20code%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19525v2&entry.124074799=Read"},
{"title": "Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders", "author": "Rui Chen and Jianfeng Zhang and Yixun Liang and Guan Luo and Weiyu Li and Jiarui Liu and Xiu Li and Xiaoxiao Long and Jiashi Feng and Ping Tan", "abstract": "  Recent 3D content generation pipelines commonly employ Variational\nAutoencoders (VAEs) to encode shapes into compact latent representations for\ndiffusion-based generation. However, the widely adopted uniform point sampling\nstrategy in Shape VAE training often leads to a significant loss of geometric\ndetails, limiting the quality of shape reconstruction and downstream generation\ntasks. We present Dora-VAE, a novel approach that enhances VAE reconstruction\nthrough our proposed sharp edge sampling strategy and a dual cross-attention\nmechanism. By identifying and prioritizing regions with high geometric\ncomplexity during training, our method significantly improves the preservation\nof fine-grained shape features. Such sampling strategy and the dual attention\nmechanism enable the VAE to focus on crucial geometric details that are\ntypically missed by uniform sampling approaches. To systematically evaluate VAE\nreconstruction quality, we additionally propose Dora-bench, a benchmark that\nquantifies shape complexity through the density of sharp edges, introducing a\nnew metric focused on reconstruction accuracy at these salient geometric\nfeatures. Extensive experiments on the Dora-bench demonstrate that Dora-VAE\nachieves comparable reconstruction quality to the state-of-the-art dense\nXCube-VAE while requiring a latent space at least 8$\\times$ smaller (1,280 vs.\n> 10,000 codes). We will release our code and benchmark dataset to facilitate\nfuture research in 3D shape modeling.\n", "link": "http://arxiv.org/abs/2412.17808v1", "date": "2024-12-23", "relevancy": 2.7614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dora%3A%20Sampling%20and%20Benchmarking%20for%203D%20Shape%20Variational%20Auto-Encoders&body=Title%3A%20Dora%3A%20Sampling%20and%20Benchmarking%20for%203D%20Shape%20Variational%20Auto-Encoders%0AAuthor%3A%20Rui%20Chen%20and%20Jianfeng%20Zhang%20and%20Yixun%20Liang%20and%20Guan%20Luo%20and%20Weiyu%20Li%20and%20Jiarui%20Liu%20and%20Xiu%20Li%20and%20Xiaoxiao%20Long%20and%20Jiashi%20Feng%20and%20Ping%20Tan%0AAbstract%3A%20%20%20Recent%203D%20content%20generation%20pipelines%20commonly%20employ%20Variational%0AAutoencoders%20%28VAEs%29%20to%20encode%20shapes%20into%20compact%20latent%20representations%20for%0Adiffusion-based%20generation.%20However%2C%20the%20widely%20adopted%20uniform%20point%20sampling%0Astrategy%20in%20Shape%20VAE%20training%20often%20leads%20to%20a%20significant%20loss%20of%20geometric%0Adetails%2C%20limiting%20the%20quality%20of%20shape%20reconstruction%20and%20downstream%20generation%0Atasks.%20We%20present%20Dora-VAE%2C%20a%20novel%20approach%20that%20enhances%20VAE%20reconstruction%0Athrough%20our%20proposed%20sharp%20edge%20sampling%20strategy%20and%20a%20dual%20cross-attention%0Amechanism.%20By%20identifying%20and%20prioritizing%20regions%20with%20high%20geometric%0Acomplexity%20during%20training%2C%20our%20method%20significantly%20improves%20the%20preservation%0Aof%20fine-grained%20shape%20features.%20Such%20sampling%20strategy%20and%20the%20dual%20attention%0Amechanism%20enable%20the%20VAE%20to%20focus%20on%20crucial%20geometric%20details%20that%20are%0Atypically%20missed%20by%20uniform%20sampling%20approaches.%20To%20systematically%20evaluate%20VAE%0Areconstruction%20quality%2C%20we%20additionally%20propose%20Dora-bench%2C%20a%20benchmark%20that%0Aquantifies%20shape%20complexity%20through%20the%20density%20of%20sharp%20edges%2C%20introducing%20a%0Anew%20metric%20focused%20on%20reconstruction%20accuracy%20at%20these%20salient%20geometric%0Afeatures.%20Extensive%20experiments%20on%20the%20Dora-bench%20demonstrate%20that%20Dora-VAE%0Aachieves%20comparable%20reconstruction%20quality%20to%20the%20state-of-the-art%20dense%0AXCube-VAE%20while%20requiring%20a%20latent%20space%20at%20least%208%24%5Ctimes%24%20smaller%20%281%2C280%20vs.%0A%3E%2010%2C000%20codes%29.%20We%20will%20release%20our%20code%20and%20benchmark%20dataset%20to%20facilitate%0Afuture%20research%20in%203D%20shape%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDora%253A%2520Sampling%2520and%2520Benchmarking%2520for%25203D%2520Shape%2520Variational%2520Auto-Encoders%26entry.906535625%3DRui%2520Chen%2520and%2520Jianfeng%2520Zhang%2520and%2520Yixun%2520Liang%2520and%2520Guan%2520Luo%2520and%2520Weiyu%2520Li%2520and%2520Jiarui%2520Liu%2520and%2520Xiu%2520Li%2520and%2520Xiaoxiao%2520Long%2520and%2520Jiashi%2520Feng%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520Recent%25203D%2520content%2520generation%2520pipelines%2520commonly%2520employ%2520Variational%250AAutoencoders%2520%2528VAEs%2529%2520to%2520encode%2520shapes%2520into%2520compact%2520latent%2520representations%2520for%250Adiffusion-based%2520generation.%2520However%252C%2520the%2520widely%2520adopted%2520uniform%2520point%2520sampling%250Astrategy%2520in%2520Shape%2520VAE%2520training%2520often%2520leads%2520to%2520a%2520significant%2520loss%2520of%2520geometric%250Adetails%252C%2520limiting%2520the%2520quality%2520of%2520shape%2520reconstruction%2520and%2520downstream%2520generation%250Atasks.%2520We%2520present%2520Dora-VAE%252C%2520a%2520novel%2520approach%2520that%2520enhances%2520VAE%2520reconstruction%250Athrough%2520our%2520proposed%2520sharp%2520edge%2520sampling%2520strategy%2520and%2520a%2520dual%2520cross-attention%250Amechanism.%2520By%2520identifying%2520and%2520prioritizing%2520regions%2520with%2520high%2520geometric%250Acomplexity%2520during%2520training%252C%2520our%2520method%2520significantly%2520improves%2520the%2520preservation%250Aof%2520fine-grained%2520shape%2520features.%2520Such%2520sampling%2520strategy%2520and%2520the%2520dual%2520attention%250Amechanism%2520enable%2520the%2520VAE%2520to%2520focus%2520on%2520crucial%2520geometric%2520details%2520that%2520are%250Atypically%2520missed%2520by%2520uniform%2520sampling%2520approaches.%2520To%2520systematically%2520evaluate%2520VAE%250Areconstruction%2520quality%252C%2520we%2520additionally%2520propose%2520Dora-bench%252C%2520a%2520benchmark%2520that%250Aquantifies%2520shape%2520complexity%2520through%2520the%2520density%2520of%2520sharp%2520edges%252C%2520introducing%2520a%250Anew%2520metric%2520focused%2520on%2520reconstruction%2520accuracy%2520at%2520these%2520salient%2520geometric%250Afeatures.%2520Extensive%2520experiments%2520on%2520the%2520Dora-bench%2520demonstrate%2520that%2520Dora-VAE%250Aachieves%2520comparable%2520reconstruction%2520quality%2520to%2520the%2520state-of-the-art%2520dense%250AXCube-VAE%2520while%2520requiring%2520a%2520latent%2520space%2520at%2520least%25208%2524%255Ctimes%2524%2520smaller%2520%25281%252C280%2520vs.%250A%253E%252010%252C000%2520codes%2529.%2520We%2520will%2520release%2520our%2520code%2520and%2520benchmark%2520dataset%2520to%2520facilitate%250Afuture%2520research%2520in%25203D%2520shape%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dora%3A%20Sampling%20and%20Benchmarking%20for%203D%20Shape%20Variational%20Auto-Encoders&entry.906535625=Rui%20Chen%20and%20Jianfeng%20Zhang%20and%20Yixun%20Liang%20and%20Guan%20Luo%20and%20Weiyu%20Li%20and%20Jiarui%20Liu%20and%20Xiu%20Li%20and%20Xiaoxiao%20Long%20and%20Jiashi%20Feng%20and%20Ping%20Tan&entry.1292438233=%20%20Recent%203D%20content%20generation%20pipelines%20commonly%20employ%20Variational%0AAutoencoders%20%28VAEs%29%20to%20encode%20shapes%20into%20compact%20latent%20representations%20for%0Adiffusion-based%20generation.%20However%2C%20the%20widely%20adopted%20uniform%20point%20sampling%0Astrategy%20in%20Shape%20VAE%20training%20often%20leads%20to%20a%20significant%20loss%20of%20geometric%0Adetails%2C%20limiting%20the%20quality%20of%20shape%20reconstruction%20and%20downstream%20generation%0Atasks.%20We%20present%20Dora-VAE%2C%20a%20novel%20approach%20that%20enhances%20VAE%20reconstruction%0Athrough%20our%20proposed%20sharp%20edge%20sampling%20strategy%20and%20a%20dual%20cross-attention%0Amechanism.%20By%20identifying%20and%20prioritizing%20regions%20with%20high%20geometric%0Acomplexity%20during%20training%2C%20our%20method%20significantly%20improves%20the%20preservation%0Aof%20fine-grained%20shape%20features.%20Such%20sampling%20strategy%20and%20the%20dual%20attention%0Amechanism%20enable%20the%20VAE%20to%20focus%20on%20crucial%20geometric%20details%20that%20are%0Atypically%20missed%20by%20uniform%20sampling%20approaches.%20To%20systematically%20evaluate%20VAE%0Areconstruction%20quality%2C%20we%20additionally%20propose%20Dora-bench%2C%20a%20benchmark%20that%0Aquantifies%20shape%20complexity%20through%20the%20density%20of%20sharp%20edges%2C%20introducing%20a%0Anew%20metric%20focused%20on%20reconstruction%20accuracy%20at%20these%20salient%20geometric%0Afeatures.%20Extensive%20experiments%20on%20the%20Dora-bench%20demonstrate%20that%20Dora-VAE%0Aachieves%20comparable%20reconstruction%20quality%20to%20the%20state-of-the-art%20dense%0AXCube-VAE%20while%20requiring%20a%20latent%20space%20at%20least%208%24%5Ctimes%24%20smaller%20%281%2C280%20vs.%0A%3E%2010%2C000%20codes%29.%20We%20will%20release%20our%20code%20and%20benchmark%20dataset%20to%20facilitate%0Afuture%20research%20in%203D%20shape%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17808v1&entry.124074799=Read"},
{"title": "The Dynamic Duo of Collaborative Masking and Target for Advanced Masked\n  Autoencoder Learning", "author": "Shentong Mo", "abstract": "  Masked autoencoders (MAE) have recently succeeded in self-supervised vision\nrepresentation learning. Previous work mainly applied custom-designed (e.g.,\nrandom, block-wise) masking or teacher (e.g., CLIP)-guided masking and targets.\nHowever, they ignore the potential role of the self-training (student) model in\ngiving feedback to the teacher for masking and targets. In this work, we\npresent to integrate Collaborative Masking and Targets for boosting Masked\nAutoEncoders, namely CMT-MAE. Specifically, CMT-MAE leverages a simple\ncollaborative masking mechanism through linear aggregation across attentions\nfrom both teacher and student models. We further propose using the output\nfeatures from those two models as the collaborative target of the decoder. Our\nsimple and effective framework pre-trained on ImageNet-1K achieves\nstate-of-the-art linear probing and fine-tuning performance. In particular,\nusing ViT-base, we improve the fine-tuning results of the vanilla MAE from\n83.6% to 85.7%.\n", "link": "http://arxiv.org/abs/2412.17566v1", "date": "2024-12-23", "relevancy": 2.7268, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Dynamic%20Duo%20of%20Collaborative%20Masking%20and%20Target%20for%20Advanced%20Masked%0A%20%20Autoencoder%20Learning&body=Title%3A%20The%20Dynamic%20Duo%20of%20Collaborative%20Masking%20and%20Target%20for%20Advanced%20Masked%0A%20%20Autoencoder%20Learning%0AAuthor%3A%20Shentong%20Mo%0AAbstract%3A%20%20%20Masked%20autoencoders%20%28MAE%29%20have%20recently%20succeeded%20in%20self-supervised%20vision%0Arepresentation%20learning.%20Previous%20work%20mainly%20applied%20custom-designed%20%28e.g.%2C%0Arandom%2C%20block-wise%29%20masking%20or%20teacher%20%28e.g.%2C%20CLIP%29-guided%20masking%20and%20targets.%0AHowever%2C%20they%20ignore%20the%20potential%20role%20of%20the%20self-training%20%28student%29%20model%20in%0Agiving%20feedback%20to%20the%20teacher%20for%20masking%20and%20targets.%20In%20this%20work%2C%20we%0Apresent%20to%20integrate%20Collaborative%20Masking%20and%20Targets%20for%20boosting%20Masked%0AAutoEncoders%2C%20namely%20CMT-MAE.%20Specifically%2C%20CMT-MAE%20leverages%20a%20simple%0Acollaborative%20masking%20mechanism%20through%20linear%20aggregation%20across%20attentions%0Afrom%20both%20teacher%20and%20student%20models.%20We%20further%20propose%20using%20the%20output%0Afeatures%20from%20those%20two%20models%20as%20the%20collaborative%20target%20of%20the%20decoder.%20Our%0Asimple%20and%20effective%20framework%20pre-trained%20on%20ImageNet-1K%20achieves%0Astate-of-the-art%20linear%20probing%20and%20fine-tuning%20performance.%20In%20particular%2C%0Ausing%20ViT-base%2C%20we%20improve%20the%20fine-tuning%20results%20of%20the%20vanilla%20MAE%20from%0A83.6%25%20to%2085.7%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Dynamic%2520Duo%2520of%2520Collaborative%2520Masking%2520and%2520Target%2520for%2520Advanced%2520Masked%250A%2520%2520Autoencoder%2520Learning%26entry.906535625%3DShentong%2520Mo%26entry.1292438233%3D%2520%2520Masked%2520autoencoders%2520%2528MAE%2529%2520have%2520recently%2520succeeded%2520in%2520self-supervised%2520vision%250Arepresentation%2520learning.%2520Previous%2520work%2520mainly%2520applied%2520custom-designed%2520%2528e.g.%252C%250Arandom%252C%2520block-wise%2529%2520masking%2520or%2520teacher%2520%2528e.g.%252C%2520CLIP%2529-guided%2520masking%2520and%2520targets.%250AHowever%252C%2520they%2520ignore%2520the%2520potential%2520role%2520of%2520the%2520self-training%2520%2528student%2529%2520model%2520in%250Agiving%2520feedback%2520to%2520the%2520teacher%2520for%2520masking%2520and%2520targets.%2520In%2520this%2520work%252C%2520we%250Apresent%2520to%2520integrate%2520Collaborative%2520Masking%2520and%2520Targets%2520for%2520boosting%2520Masked%250AAutoEncoders%252C%2520namely%2520CMT-MAE.%2520Specifically%252C%2520CMT-MAE%2520leverages%2520a%2520simple%250Acollaborative%2520masking%2520mechanism%2520through%2520linear%2520aggregation%2520across%2520attentions%250Afrom%2520both%2520teacher%2520and%2520student%2520models.%2520We%2520further%2520propose%2520using%2520the%2520output%250Afeatures%2520from%2520those%2520two%2520models%2520as%2520the%2520collaborative%2520target%2520of%2520the%2520decoder.%2520Our%250Asimple%2520and%2520effective%2520framework%2520pre-trained%2520on%2520ImageNet-1K%2520achieves%250Astate-of-the-art%2520linear%2520probing%2520and%2520fine-tuning%2520performance.%2520In%2520particular%252C%250Ausing%2520ViT-base%252C%2520we%2520improve%2520the%2520fine-tuning%2520results%2520of%2520the%2520vanilla%2520MAE%2520from%250A83.6%2525%2520to%252085.7%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Dynamic%20Duo%20of%20Collaborative%20Masking%20and%20Target%20for%20Advanced%20Masked%0A%20%20Autoencoder%20Learning&entry.906535625=Shentong%20Mo&entry.1292438233=%20%20Masked%20autoencoders%20%28MAE%29%20have%20recently%20succeeded%20in%20self-supervised%20vision%0Arepresentation%20learning.%20Previous%20work%20mainly%20applied%20custom-designed%20%28e.g.%2C%0Arandom%2C%20block-wise%29%20masking%20or%20teacher%20%28e.g.%2C%20CLIP%29-guided%20masking%20and%20targets.%0AHowever%2C%20they%20ignore%20the%20potential%20role%20of%20the%20self-training%20%28student%29%20model%20in%0Agiving%20feedback%20to%20the%20teacher%20for%20masking%20and%20targets.%20In%20this%20work%2C%20we%0Apresent%20to%20integrate%20Collaborative%20Masking%20and%20Targets%20for%20boosting%20Masked%0AAutoEncoders%2C%20namely%20CMT-MAE.%20Specifically%2C%20CMT-MAE%20leverages%20a%20simple%0Acollaborative%20masking%20mechanism%20through%20linear%20aggregation%20across%20attentions%0Afrom%20both%20teacher%20and%20student%20models.%20We%20further%20propose%20using%20the%20output%0Afeatures%20from%20those%20two%20models%20as%20the%20collaborative%20target%20of%20the%20decoder.%20Our%0Asimple%20and%20effective%20framework%20pre-trained%20on%20ImageNet-1K%20achieves%0Astate-of-the-art%20linear%20probing%20and%20fine-tuning%20performance.%20In%20particular%2C%0Ausing%20ViT-base%2C%20we%20improve%20the%20fine-tuning%20results%20of%20the%20vanilla%20MAE%20from%0A83.6%25%20to%2085.7%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17566v1&entry.124074799=Read"},
{"title": "ChatGarment: Garment Estimation, Generation and Editing via Large\n  Language Models", "author": "Siyuan Bian and Chenghao Xu and Yuliang Xiu and Artur Grigorev and Zhen Liu and Cewu Lu and Michael J. Black and Yao Feng", "abstract": "  We introduce ChatGarment, a novel approach that leverages large\nvision-language models (VLMs) to automate the estimation, generation, and\nediting of 3D garments from images or text descriptions. Unlike previous\nmethods that struggle in real-world scenarios or lack interactive editing\ncapabilities, ChatGarment can estimate sewing patterns from in-the-wild images\nor sketches, generate them from text descriptions, and edit garments based on\nuser instructions, all within an interactive dialogue. These sewing patterns\ncan then be draped into 3D garments, which are easily animatable and\nsimulatable. This is achieved by finetuning a VLM to directly generate a JSON\nfile that includes both textual descriptions of garment types and styles, as\nwell as continuous numerical attributes. This JSON file is then used to create\nsewing patterns through a programming parametric model. To support this, we\nrefine the existing programming model, GarmentCode, by expanding its garment\ntype coverage and simplifying its structure for efficient VLM fine-tuning.\nAdditionally, we construct a large-scale dataset of image-to-sewing-pattern and\ntext-to-sewing-pattern pairs through an automated data pipeline. Extensive\nevaluations demonstrate ChatGarment's ability to accurately reconstruct,\ngenerate, and edit garments from multimodal inputs, highlighting its potential\nto revolutionize workflows in fashion and gaming applications. Code and data\nwill be available at https://chatgarment.github.io/.\n", "link": "http://arxiv.org/abs/2412.17811v1", "date": "2024-12-23", "relevancy": 2.706, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7736}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6102}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatGarment%3A%20Garment%20Estimation%2C%20Generation%20and%20Editing%20via%20Large%0A%20%20Language%20Models&body=Title%3A%20ChatGarment%3A%20Garment%20Estimation%2C%20Generation%20and%20Editing%20via%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Siyuan%20Bian%20and%20Chenghao%20Xu%20and%20Yuliang%20Xiu%20and%20Artur%20Grigorev%20and%20Zhen%20Liu%20and%20Cewu%20Lu%20and%20Michael%20J.%20Black%20and%20Yao%20Feng%0AAbstract%3A%20%20%20We%20introduce%20ChatGarment%2C%20a%20novel%20approach%20that%20leverages%20large%0Avision-language%20models%20%28VLMs%29%20to%20automate%20the%20estimation%2C%20generation%2C%20and%0Aediting%20of%203D%20garments%20from%20images%20or%20text%20descriptions.%20Unlike%20previous%0Amethods%20that%20struggle%20in%20real-world%20scenarios%20or%20lack%20interactive%20editing%0Acapabilities%2C%20ChatGarment%20can%20estimate%20sewing%20patterns%20from%20in-the-wild%20images%0Aor%20sketches%2C%20generate%20them%20from%20text%20descriptions%2C%20and%20edit%20garments%20based%20on%0Auser%20instructions%2C%20all%20within%20an%20interactive%20dialogue.%20These%20sewing%20patterns%0Acan%20then%20be%20draped%20into%203D%20garments%2C%20which%20are%20easily%20animatable%20and%0Asimulatable.%20This%20is%20achieved%20by%20finetuning%20a%20VLM%20to%20directly%20generate%20a%20JSON%0Afile%20that%20includes%20both%20textual%20descriptions%20of%20garment%20types%20and%20styles%2C%20as%0Awell%20as%20continuous%20numerical%20attributes.%20This%20JSON%20file%20is%20then%20used%20to%20create%0Asewing%20patterns%20through%20a%20programming%20parametric%20model.%20To%20support%20this%2C%20we%0Arefine%20the%20existing%20programming%20model%2C%20GarmentCode%2C%20by%20expanding%20its%20garment%0Atype%20coverage%20and%20simplifying%20its%20structure%20for%20efficient%20VLM%20fine-tuning.%0AAdditionally%2C%20we%20construct%20a%20large-scale%20dataset%20of%20image-to-sewing-pattern%20and%0Atext-to-sewing-pattern%20pairs%20through%20an%20automated%20data%20pipeline.%20Extensive%0Aevaluations%20demonstrate%20ChatGarment%27s%20ability%20to%20accurately%20reconstruct%2C%0Agenerate%2C%20and%20edit%20garments%20from%20multimodal%20inputs%2C%20highlighting%20its%20potential%0Ato%20revolutionize%20workflows%20in%20fashion%20and%20gaming%20applications.%20Code%20and%20data%0Awill%20be%20available%20at%20https%3A//chatgarment.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatGarment%253A%2520Garment%2520Estimation%252C%2520Generation%2520and%2520Editing%2520via%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DSiyuan%2520Bian%2520and%2520Chenghao%2520Xu%2520and%2520Yuliang%2520Xiu%2520and%2520Artur%2520Grigorev%2520and%2520Zhen%2520Liu%2520and%2520Cewu%2520Lu%2520and%2520Michael%2520J.%2520Black%2520and%2520Yao%2520Feng%26entry.1292438233%3D%2520%2520We%2520introduce%2520ChatGarment%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520large%250Avision-language%2520models%2520%2528VLMs%2529%2520to%2520automate%2520the%2520estimation%252C%2520generation%252C%2520and%250Aediting%2520of%25203D%2520garments%2520from%2520images%2520or%2520text%2520descriptions.%2520Unlike%2520previous%250Amethods%2520that%2520struggle%2520in%2520real-world%2520scenarios%2520or%2520lack%2520interactive%2520editing%250Acapabilities%252C%2520ChatGarment%2520can%2520estimate%2520sewing%2520patterns%2520from%2520in-the-wild%2520images%250Aor%2520sketches%252C%2520generate%2520them%2520from%2520text%2520descriptions%252C%2520and%2520edit%2520garments%2520based%2520on%250Auser%2520instructions%252C%2520all%2520within%2520an%2520interactive%2520dialogue.%2520These%2520sewing%2520patterns%250Acan%2520then%2520be%2520draped%2520into%25203D%2520garments%252C%2520which%2520are%2520easily%2520animatable%2520and%250Asimulatable.%2520This%2520is%2520achieved%2520by%2520finetuning%2520a%2520VLM%2520to%2520directly%2520generate%2520a%2520JSON%250Afile%2520that%2520includes%2520both%2520textual%2520descriptions%2520of%2520garment%2520types%2520and%2520styles%252C%2520as%250Awell%2520as%2520continuous%2520numerical%2520attributes.%2520This%2520JSON%2520file%2520is%2520then%2520used%2520to%2520create%250Asewing%2520patterns%2520through%2520a%2520programming%2520parametric%2520model.%2520To%2520support%2520this%252C%2520we%250Arefine%2520the%2520existing%2520programming%2520model%252C%2520GarmentCode%252C%2520by%2520expanding%2520its%2520garment%250Atype%2520coverage%2520and%2520simplifying%2520its%2520structure%2520for%2520efficient%2520VLM%2520fine-tuning.%250AAdditionally%252C%2520we%2520construct%2520a%2520large-scale%2520dataset%2520of%2520image-to-sewing-pattern%2520and%250Atext-to-sewing-pattern%2520pairs%2520through%2520an%2520automated%2520data%2520pipeline.%2520Extensive%250Aevaluations%2520demonstrate%2520ChatGarment%2527s%2520ability%2520to%2520accurately%2520reconstruct%252C%250Agenerate%252C%2520and%2520edit%2520garments%2520from%2520multimodal%2520inputs%252C%2520highlighting%2520its%2520potential%250Ato%2520revolutionize%2520workflows%2520in%2520fashion%2520and%2520gaming%2520applications.%2520Code%2520and%2520data%250Awill%2520be%2520available%2520at%2520https%253A//chatgarment.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatGarment%3A%20Garment%20Estimation%2C%20Generation%20and%20Editing%20via%20Large%0A%20%20Language%20Models&entry.906535625=Siyuan%20Bian%20and%20Chenghao%20Xu%20and%20Yuliang%20Xiu%20and%20Artur%20Grigorev%20and%20Zhen%20Liu%20and%20Cewu%20Lu%20and%20Michael%20J.%20Black%20and%20Yao%20Feng&entry.1292438233=%20%20We%20introduce%20ChatGarment%2C%20a%20novel%20approach%20that%20leverages%20large%0Avision-language%20models%20%28VLMs%29%20to%20automate%20the%20estimation%2C%20generation%2C%20and%0Aediting%20of%203D%20garments%20from%20images%20or%20text%20descriptions.%20Unlike%20previous%0Amethods%20that%20struggle%20in%20real-world%20scenarios%20or%20lack%20interactive%20editing%0Acapabilities%2C%20ChatGarment%20can%20estimate%20sewing%20patterns%20from%20in-the-wild%20images%0Aor%20sketches%2C%20generate%20them%20from%20text%20descriptions%2C%20and%20edit%20garments%20based%20on%0Auser%20instructions%2C%20all%20within%20an%20interactive%20dialogue.%20These%20sewing%20patterns%0Acan%20then%20be%20draped%20into%203D%20garments%2C%20which%20are%20easily%20animatable%20and%0Asimulatable.%20This%20is%20achieved%20by%20finetuning%20a%20VLM%20to%20directly%20generate%20a%20JSON%0Afile%20that%20includes%20both%20textual%20descriptions%20of%20garment%20types%20and%20styles%2C%20as%0Awell%20as%20continuous%20numerical%20attributes.%20This%20JSON%20file%20is%20then%20used%20to%20create%0Asewing%20patterns%20through%20a%20programming%20parametric%20model.%20To%20support%20this%2C%20we%0Arefine%20the%20existing%20programming%20model%2C%20GarmentCode%2C%20by%20expanding%20its%20garment%0Atype%20coverage%20and%20simplifying%20its%20structure%20for%20efficient%20VLM%20fine-tuning.%0AAdditionally%2C%20we%20construct%20a%20large-scale%20dataset%20of%20image-to-sewing-pattern%20and%0Atext-to-sewing-pattern%20pairs%20through%20an%20automated%20data%20pipeline.%20Extensive%0Aevaluations%20demonstrate%20ChatGarment%27s%20ability%20to%20accurately%20reconstruct%2C%0Agenerate%2C%20and%20edit%20garments%20from%20multimodal%20inputs%2C%20highlighting%20its%20potential%0Ato%20revolutionize%20workflows%20in%20fashion%20and%20gaming%20applications.%20Code%20and%20data%0Awill%20be%20available%20at%20https%3A//chatgarment.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17811v1&entry.124074799=Read"},
{"title": "Establishing Reality-Virtuality Interconnections in Urban Digital Twins\n  for Superior Intelligent Road Inspection", "author": "Yikang Zhang and Chuang-Wei Liu and Jiahang Li and Yingbing Chen and Jie Cheng and Rui Fan", "abstract": "  Road inspection is essential for ensuring road maintenance and traffic\nsafety, as road defects gradually emerge and compromise road functionality.\nTraditional methods, which rely on manual evaluations, are labor-intensive,\ncostly, and time-consuming. Although data-driven approaches are gaining\ntraction, the scarcity and spatial sparsity of road defects in the real world\npose significant challenges in acquiring high-quality datasets. Existing\nsimulators designed to generate detailed synthetic driving scenes, however,\nlack models for road defects. Furthermore, advanced driving tasks involving\ninteractions with road surfaces, such as planning and control in defective\nareas, remain underexplored. To address these limitations, we propose a system\nbased on Urban Digital Twin (UDT) technology for intelligent road inspection.\nFirst, hierarchical road models are constructed from real-world driving data,\ncreating highly detailed representations of road defect structures and surface\nelevations. Next, digital road twins are generated to create simulation\nenvironments for comprehensive analysis and evaluation. These scenarios are\nsubsequently imported into a simulator to enable both data acquisition and\nphysical simulation. Experimental results demonstrate that driving tasks,\nincluding perception and decision-making, can be significantly improved using\nthe high-fidelity road defect scenes generated by our system.\n", "link": "http://arxiv.org/abs/2412.17699v1", "date": "2024-12-23", "relevancy": 2.6619, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Establishing%20Reality-Virtuality%20Interconnections%20in%20Urban%20Digital%20Twins%0A%20%20for%20Superior%20Intelligent%20Road%20Inspection&body=Title%3A%20Establishing%20Reality-Virtuality%20Interconnections%20in%20Urban%20Digital%20Twins%0A%20%20for%20Superior%20Intelligent%20Road%20Inspection%0AAuthor%3A%20Yikang%20Zhang%20and%20Chuang-Wei%20Liu%20and%20Jiahang%20Li%20and%20Yingbing%20Chen%20and%20Jie%20Cheng%20and%20Rui%20Fan%0AAbstract%3A%20%20%20Road%20inspection%20is%20essential%20for%20ensuring%20road%20maintenance%20and%20traffic%0Asafety%2C%20as%20road%20defects%20gradually%20emerge%20and%20compromise%20road%20functionality.%0ATraditional%20methods%2C%20which%20rely%20on%20manual%20evaluations%2C%20are%20labor-intensive%2C%0Acostly%2C%20and%20time-consuming.%20Although%20data-driven%20approaches%20are%20gaining%0Atraction%2C%20the%20scarcity%20and%20spatial%20sparsity%20of%20road%20defects%20in%20the%20real%20world%0Apose%20significant%20challenges%20in%20acquiring%20high-quality%20datasets.%20Existing%0Asimulators%20designed%20to%20generate%20detailed%20synthetic%20driving%20scenes%2C%20however%2C%0Alack%20models%20for%20road%20defects.%20Furthermore%2C%20advanced%20driving%20tasks%20involving%0Ainteractions%20with%20road%20surfaces%2C%20such%20as%20planning%20and%20control%20in%20defective%0Aareas%2C%20remain%20underexplored.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20system%0Abased%20on%20Urban%20Digital%20Twin%20%28UDT%29%20technology%20for%20intelligent%20road%20inspection.%0AFirst%2C%20hierarchical%20road%20models%20are%20constructed%20from%20real-world%20driving%20data%2C%0Acreating%20highly%20detailed%20representations%20of%20road%20defect%20structures%20and%20surface%0Aelevations.%20Next%2C%20digital%20road%20twins%20are%20generated%20to%20create%20simulation%0Aenvironments%20for%20comprehensive%20analysis%20and%20evaluation.%20These%20scenarios%20are%0Asubsequently%20imported%20into%20a%20simulator%20to%20enable%20both%20data%20acquisition%20and%0Aphysical%20simulation.%20Experimental%20results%20demonstrate%20that%20driving%20tasks%2C%0Aincluding%20perception%20and%20decision-making%2C%20can%20be%20significantly%20improved%20using%0Athe%20high-fidelity%20road%20defect%20scenes%20generated%20by%20our%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstablishing%2520Reality-Virtuality%2520Interconnections%2520in%2520Urban%2520Digital%2520Twins%250A%2520%2520for%2520Superior%2520Intelligent%2520Road%2520Inspection%26entry.906535625%3DYikang%2520Zhang%2520and%2520Chuang-Wei%2520Liu%2520and%2520Jiahang%2520Li%2520and%2520Yingbing%2520Chen%2520and%2520Jie%2520Cheng%2520and%2520Rui%2520Fan%26entry.1292438233%3D%2520%2520Road%2520inspection%2520is%2520essential%2520for%2520ensuring%2520road%2520maintenance%2520and%2520traffic%250Asafety%252C%2520as%2520road%2520defects%2520gradually%2520emerge%2520and%2520compromise%2520road%2520functionality.%250ATraditional%2520methods%252C%2520which%2520rely%2520on%2520manual%2520evaluations%252C%2520are%2520labor-intensive%252C%250Acostly%252C%2520and%2520time-consuming.%2520Although%2520data-driven%2520approaches%2520are%2520gaining%250Atraction%252C%2520the%2520scarcity%2520and%2520spatial%2520sparsity%2520of%2520road%2520defects%2520in%2520the%2520real%2520world%250Apose%2520significant%2520challenges%2520in%2520acquiring%2520high-quality%2520datasets.%2520Existing%250Asimulators%2520designed%2520to%2520generate%2520detailed%2520synthetic%2520driving%2520scenes%252C%2520however%252C%250Alack%2520models%2520for%2520road%2520defects.%2520Furthermore%252C%2520advanced%2520driving%2520tasks%2520involving%250Ainteractions%2520with%2520road%2520surfaces%252C%2520such%2520as%2520planning%2520and%2520control%2520in%2520defective%250Aareas%252C%2520remain%2520underexplored.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520system%250Abased%2520on%2520Urban%2520Digital%2520Twin%2520%2528UDT%2529%2520technology%2520for%2520intelligent%2520road%2520inspection.%250AFirst%252C%2520hierarchical%2520road%2520models%2520are%2520constructed%2520from%2520real-world%2520driving%2520data%252C%250Acreating%2520highly%2520detailed%2520representations%2520of%2520road%2520defect%2520structures%2520and%2520surface%250Aelevations.%2520Next%252C%2520digital%2520road%2520twins%2520are%2520generated%2520to%2520create%2520simulation%250Aenvironments%2520for%2520comprehensive%2520analysis%2520and%2520evaluation.%2520These%2520scenarios%2520are%250Asubsequently%2520imported%2520into%2520a%2520simulator%2520to%2520enable%2520both%2520data%2520acquisition%2520and%250Aphysical%2520simulation.%2520Experimental%2520results%2520demonstrate%2520that%2520driving%2520tasks%252C%250Aincluding%2520perception%2520and%2520decision-making%252C%2520can%2520be%2520significantly%2520improved%2520using%250Athe%2520high-fidelity%2520road%2520defect%2520scenes%2520generated%2520by%2520our%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Establishing%20Reality-Virtuality%20Interconnections%20in%20Urban%20Digital%20Twins%0A%20%20for%20Superior%20Intelligent%20Road%20Inspection&entry.906535625=Yikang%20Zhang%20and%20Chuang-Wei%20Liu%20and%20Jiahang%20Li%20and%20Yingbing%20Chen%20and%20Jie%20Cheng%20and%20Rui%20Fan&entry.1292438233=%20%20Road%20inspection%20is%20essential%20for%20ensuring%20road%20maintenance%20and%20traffic%0Asafety%2C%20as%20road%20defects%20gradually%20emerge%20and%20compromise%20road%20functionality.%0ATraditional%20methods%2C%20which%20rely%20on%20manual%20evaluations%2C%20are%20labor-intensive%2C%0Acostly%2C%20and%20time-consuming.%20Although%20data-driven%20approaches%20are%20gaining%0Atraction%2C%20the%20scarcity%20and%20spatial%20sparsity%20of%20road%20defects%20in%20the%20real%20world%0Apose%20significant%20challenges%20in%20acquiring%20high-quality%20datasets.%20Existing%0Asimulators%20designed%20to%20generate%20detailed%20synthetic%20driving%20scenes%2C%20however%2C%0Alack%20models%20for%20road%20defects.%20Furthermore%2C%20advanced%20driving%20tasks%20involving%0Ainteractions%20with%20road%20surfaces%2C%20such%20as%20planning%20and%20control%20in%20defective%0Aareas%2C%20remain%20underexplored.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20system%0Abased%20on%20Urban%20Digital%20Twin%20%28UDT%29%20technology%20for%20intelligent%20road%20inspection.%0AFirst%2C%20hierarchical%20road%20models%20are%20constructed%20from%20real-world%20driving%20data%2C%0Acreating%20highly%20detailed%20representations%20of%20road%20defect%20structures%20and%20surface%0Aelevations.%20Next%2C%20digital%20road%20twins%20are%20generated%20to%20create%20simulation%0Aenvironments%20for%20comprehensive%20analysis%20and%20evaluation.%20These%20scenarios%20are%0Asubsequently%20imported%20into%20a%20simulator%20to%20enable%20both%20data%20acquisition%20and%0Aphysical%20simulation.%20Experimental%20results%20demonstrate%20that%20driving%20tasks%2C%0Aincluding%20perception%20and%20decision-making%2C%20can%20be%20significantly%20improved%20using%0Athe%20high-fidelity%20road%20defect%20scenes%20generated%20by%20our%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17699v1&entry.124074799=Read"},
{"title": "URoadNet: Dual Sparse Attentive U-Net for Multiscale Road Network\n  Extraction", "author": "Jie Song and Yue Sun and Ziyun Cai and Liang Xiao and Yawen Huang and Yefeng Zheng", "abstract": "  The challenges of road network segmentation demand an algorithm capable of\nadapting to the sparse and irregular shapes, as well as the diverse context,\nwhich often leads traditional encoding-decoding methods and simple Transformer\nembeddings to failure. We introduce a computationally efficient and powerful\nframework for elegant road-aware segmentation. Our method, called URoadNet,\neffectively encodes fine-grained local road connectivity and holistic global\ntopological semantics while decoding multiscale road network information.\nURoadNet offers a novel alternative to the U-Net architecture by integrating\nconnectivity attention, which can exploit intra-road interactions across\nmulti-level sampling features with reduced computational complexity. This local\ninteraction serves as valuable prior information for learning global\ninteractions between road networks and the background through another\nintegrality attention mechanism. The two forms of sparse attention are arranged\nalternatively and complementarily, and trained jointly, resulting in\nperformance improvements without significant increases in computational\ncomplexity. Extensive experiments on various datasets with different\nresolutions, including Massachusetts, DeepGlobe, SpaceNet, and Large-Scale\nremote sensing images, demonstrate that URoadNet outperforms state-of-the-art\ntechniques. Our approach represents a significant advancement in the field of\nroad network extraction, providing a computationally feasible solution that\nachieves high-quality segmentation results.\n", "link": "http://arxiv.org/abs/2412.17573v1", "date": "2024-12-23", "relevancy": 2.6278, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5688}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20URoadNet%3A%20Dual%20Sparse%20Attentive%20U-Net%20for%20Multiscale%20Road%20Network%0A%20%20Extraction&body=Title%3A%20URoadNet%3A%20Dual%20Sparse%20Attentive%20U-Net%20for%20Multiscale%20Road%20Network%0A%20%20Extraction%0AAuthor%3A%20Jie%20Song%20and%20Yue%20Sun%20and%20Ziyun%20Cai%20and%20Liang%20Xiao%20and%20Yawen%20Huang%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20The%20challenges%20of%20road%20network%20segmentation%20demand%20an%20algorithm%20capable%20of%0Aadapting%20to%20the%20sparse%20and%20irregular%20shapes%2C%20as%20well%20as%20the%20diverse%20context%2C%0Awhich%20often%20leads%20traditional%20encoding-decoding%20methods%20and%20simple%20Transformer%0Aembeddings%20to%20failure.%20We%20introduce%20a%20computationally%20efficient%20and%20powerful%0Aframework%20for%20elegant%20road-aware%20segmentation.%20Our%20method%2C%20called%20URoadNet%2C%0Aeffectively%20encodes%20fine-grained%20local%20road%20connectivity%20and%20holistic%20global%0Atopological%20semantics%20while%20decoding%20multiscale%20road%20network%20information.%0AURoadNet%20offers%20a%20novel%20alternative%20to%20the%20U-Net%20architecture%20by%20integrating%0Aconnectivity%20attention%2C%20which%20can%20exploit%20intra-road%20interactions%20across%0Amulti-level%20sampling%20features%20with%20reduced%20computational%20complexity.%20This%20local%0Ainteraction%20serves%20as%20valuable%20prior%20information%20for%20learning%20global%0Ainteractions%20between%20road%20networks%20and%20the%20background%20through%20another%0Aintegrality%20attention%20mechanism.%20The%20two%20forms%20of%20sparse%20attention%20are%20arranged%0Aalternatively%20and%20complementarily%2C%20and%20trained%20jointly%2C%20resulting%20in%0Aperformance%20improvements%20without%20significant%20increases%20in%20computational%0Acomplexity.%20Extensive%20experiments%20on%20various%20datasets%20with%20different%0Aresolutions%2C%20including%20Massachusetts%2C%20DeepGlobe%2C%20SpaceNet%2C%20and%20Large-Scale%0Aremote%20sensing%20images%2C%20demonstrate%20that%20URoadNet%20outperforms%20state-of-the-art%0Atechniques.%20Our%20approach%20represents%20a%20significant%20advancement%20in%20the%20field%20of%0Aroad%20network%20extraction%2C%20providing%20a%20computationally%20feasible%20solution%20that%0Aachieves%20high-quality%20segmentation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DURoadNet%253A%2520Dual%2520Sparse%2520Attentive%2520U-Net%2520for%2520Multiscale%2520Road%2520Network%250A%2520%2520Extraction%26entry.906535625%3DJie%2520Song%2520and%2520Yue%2520Sun%2520and%2520Ziyun%2520Cai%2520and%2520Liang%2520Xiao%2520and%2520Yawen%2520Huang%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D%2520%2520The%2520challenges%2520of%2520road%2520network%2520segmentation%2520demand%2520an%2520algorithm%2520capable%2520of%250Aadapting%2520to%2520the%2520sparse%2520and%2520irregular%2520shapes%252C%2520as%2520well%2520as%2520the%2520diverse%2520context%252C%250Awhich%2520often%2520leads%2520traditional%2520encoding-decoding%2520methods%2520and%2520simple%2520Transformer%250Aembeddings%2520to%2520failure.%2520We%2520introduce%2520a%2520computationally%2520efficient%2520and%2520powerful%250Aframework%2520for%2520elegant%2520road-aware%2520segmentation.%2520Our%2520method%252C%2520called%2520URoadNet%252C%250Aeffectively%2520encodes%2520fine-grained%2520local%2520road%2520connectivity%2520and%2520holistic%2520global%250Atopological%2520semantics%2520while%2520decoding%2520multiscale%2520road%2520network%2520information.%250AURoadNet%2520offers%2520a%2520novel%2520alternative%2520to%2520the%2520U-Net%2520architecture%2520by%2520integrating%250Aconnectivity%2520attention%252C%2520which%2520can%2520exploit%2520intra-road%2520interactions%2520across%250Amulti-level%2520sampling%2520features%2520with%2520reduced%2520computational%2520complexity.%2520This%2520local%250Ainteraction%2520serves%2520as%2520valuable%2520prior%2520information%2520for%2520learning%2520global%250Ainteractions%2520between%2520road%2520networks%2520and%2520the%2520background%2520through%2520another%250Aintegrality%2520attention%2520mechanism.%2520The%2520two%2520forms%2520of%2520sparse%2520attention%2520are%2520arranged%250Aalternatively%2520and%2520complementarily%252C%2520and%2520trained%2520jointly%252C%2520resulting%2520in%250Aperformance%2520improvements%2520without%2520significant%2520increases%2520in%2520computational%250Acomplexity.%2520Extensive%2520experiments%2520on%2520various%2520datasets%2520with%2520different%250Aresolutions%252C%2520including%2520Massachusetts%252C%2520DeepGlobe%252C%2520SpaceNet%252C%2520and%2520Large-Scale%250Aremote%2520sensing%2520images%252C%2520demonstrate%2520that%2520URoadNet%2520outperforms%2520state-of-the-art%250Atechniques.%2520Our%2520approach%2520represents%2520a%2520significant%2520advancement%2520in%2520the%2520field%2520of%250Aroad%2520network%2520extraction%252C%2520providing%2520a%2520computationally%2520feasible%2520solution%2520that%250Aachieves%2520high-quality%2520segmentation%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=URoadNet%3A%20Dual%20Sparse%20Attentive%20U-Net%20for%20Multiscale%20Road%20Network%0A%20%20Extraction&entry.906535625=Jie%20Song%20and%20Yue%20Sun%20and%20Ziyun%20Cai%20and%20Liang%20Xiao%20and%20Yawen%20Huang%20and%20Yefeng%20Zheng&entry.1292438233=%20%20The%20challenges%20of%20road%20network%20segmentation%20demand%20an%20algorithm%20capable%20of%0Aadapting%20to%20the%20sparse%20and%20irregular%20shapes%2C%20as%20well%20as%20the%20diverse%20context%2C%0Awhich%20often%20leads%20traditional%20encoding-decoding%20methods%20and%20simple%20Transformer%0Aembeddings%20to%20failure.%20We%20introduce%20a%20computationally%20efficient%20and%20powerful%0Aframework%20for%20elegant%20road-aware%20segmentation.%20Our%20method%2C%20called%20URoadNet%2C%0Aeffectively%20encodes%20fine-grained%20local%20road%20connectivity%20and%20holistic%20global%0Atopological%20semantics%20while%20decoding%20multiscale%20road%20network%20information.%0AURoadNet%20offers%20a%20novel%20alternative%20to%20the%20U-Net%20architecture%20by%20integrating%0Aconnectivity%20attention%2C%20which%20can%20exploit%20intra-road%20interactions%20across%0Amulti-level%20sampling%20features%20with%20reduced%20computational%20complexity.%20This%20local%0Ainteraction%20serves%20as%20valuable%20prior%20information%20for%20learning%20global%0Ainteractions%20between%20road%20networks%20and%20the%20background%20through%20another%0Aintegrality%20attention%20mechanism.%20The%20two%20forms%20of%20sparse%20attention%20are%20arranged%0Aalternatively%20and%20complementarily%2C%20and%20trained%20jointly%2C%20resulting%20in%0Aperformance%20improvements%20without%20significant%20increases%20in%20computational%0Acomplexity.%20Extensive%20experiments%20on%20various%20datasets%20with%20different%0Aresolutions%2C%20including%20Massachusetts%2C%20DeepGlobe%2C%20SpaceNet%2C%20and%20Large-Scale%0Aremote%20sensing%20images%2C%20demonstrate%20that%20URoadNet%20outperforms%20state-of-the-art%0Atechniques.%20Our%20approach%20represents%20a%20significant%20advancement%20in%20the%20field%20of%0Aroad%20network%20extraction%2C%20providing%20a%20computationally%20feasible%20solution%20that%0Aachieves%20high-quality%20segmentation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17573v1&entry.124074799=Read"},
{"title": "LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea\n  Generation with Minimal Context", "author": "Kai Ruan and Xuan Wang and Jixiang Hong and Hao Sun", "abstract": "  While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin scientific tasks, existing evaluation frameworks primarily assess their\nperformance using rich contextual inputs, overlooking their ability to generate\nnovel ideas from minimal information. We introduce LiveIdeaBench, a\ncomprehensive benchmark that evaluates LLMs' scientific creativity and\ndivergent thinking capabilities using single-keyword prompts. Drawing from\nGuilford's creativity theory, our framework employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across four key dimensions:\noriginality, feasibility, fluency, and flexibility. Through extensive\nexperimentation with 20 leading models across 1,180 keywords spanning 18\nscientific domains, we reveal that scientific creative ability shows distinct\npatterns from general intelligence metrics. Notably, our results demonstrate\nthat models like QwQ-32B-preview achieve comparable creative performance to\ntop-tier models like o1-preview, despite significant gaps in their general\nintelligence scores. These findings highlight the importance of specialized\nevaluation frameworks for scientific creativity and suggest that the\ndevelopment of creative capabilities in LLMs may follow different trajectories\nthan traditional problem-solving abilities.\n", "link": "http://arxiv.org/abs/2412.17596v1", "date": "2024-12-23", "relevancy": 2.6163, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiveIdeaBench%3A%20Evaluating%20LLMs%27%20Scientific%20Creativity%20and%20Idea%0A%20%20Generation%20with%20Minimal%20Context&body=Title%3A%20LiveIdeaBench%3A%20Evaluating%20LLMs%27%20Scientific%20Creativity%20and%20Idea%0A%20%20Generation%20with%20Minimal%20Context%0AAuthor%3A%20Kai%20Ruan%20and%20Xuan%20Wang%20and%20Jixiang%20Hong%20and%20Hao%20Sun%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%0Ain%20scientific%20tasks%2C%20existing%20evaluation%20frameworks%20primarily%20assess%20their%0Aperformance%20using%20rich%20contextual%20inputs%2C%20overlooking%20their%20ability%20to%20generate%0Anovel%20ideas%20from%20minimal%20information.%20We%20introduce%20LiveIdeaBench%2C%20a%0Acomprehensive%20benchmark%20that%20evaluates%20LLMs%27%20scientific%20creativity%20and%0Adivergent%20thinking%20capabilities%20using%20single-keyword%20prompts.%20Drawing%20from%0AGuilford%27s%20creativity%20theory%2C%20our%20framework%20employs%20a%20dynamic%20panel%20of%0Astate-of-the-art%20LLMs%20to%20assess%20generated%20ideas%20across%20four%20key%20dimensions%3A%0Aoriginality%2C%20feasibility%2C%20fluency%2C%20and%20flexibility.%20Through%20extensive%0Aexperimentation%20with%2020%20leading%20models%20across%201%2C180%20keywords%20spanning%2018%0Ascientific%20domains%2C%20we%20reveal%20that%20scientific%20creative%20ability%20shows%20distinct%0Apatterns%20from%20general%20intelligence%20metrics.%20Notably%2C%20our%20results%20demonstrate%0Athat%20models%20like%20QwQ-32B-preview%20achieve%20comparable%20creative%20performance%20to%0Atop-tier%20models%20like%20o1-preview%2C%20despite%20significant%20gaps%20in%20their%20general%0Aintelligence%20scores.%20These%20findings%20highlight%20the%20importance%20of%20specialized%0Aevaluation%20frameworks%20for%20scientific%20creativity%20and%20suggest%20that%20the%0Adevelopment%20of%20creative%20capabilities%20in%20LLMs%20may%20follow%20different%20trajectories%0Athan%20traditional%20problem-solving%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiveIdeaBench%253A%2520Evaluating%2520LLMs%2527%2520Scientific%2520Creativity%2520and%2520Idea%250A%2520%2520Generation%2520with%2520Minimal%2520Context%26entry.906535625%3DKai%2520Ruan%2520and%2520Xuan%2520Wang%2520and%2520Jixiang%2520Hong%2520and%2520Hao%2520Sun%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%250Ain%2520scientific%2520tasks%252C%2520existing%2520evaluation%2520frameworks%2520primarily%2520assess%2520their%250Aperformance%2520using%2520rich%2520contextual%2520inputs%252C%2520overlooking%2520their%2520ability%2520to%2520generate%250Anovel%2520ideas%2520from%2520minimal%2520information.%2520We%2520introduce%2520LiveIdeaBench%252C%2520a%250Acomprehensive%2520benchmark%2520that%2520evaluates%2520LLMs%2527%2520scientific%2520creativity%2520and%250Adivergent%2520thinking%2520capabilities%2520using%2520single-keyword%2520prompts.%2520Drawing%2520from%250AGuilford%2527s%2520creativity%2520theory%252C%2520our%2520framework%2520employs%2520a%2520dynamic%2520panel%2520of%250Astate-of-the-art%2520LLMs%2520to%2520assess%2520generated%2520ideas%2520across%2520four%2520key%2520dimensions%253A%250Aoriginality%252C%2520feasibility%252C%2520fluency%252C%2520and%2520flexibility.%2520Through%2520extensive%250Aexperimentation%2520with%252020%2520leading%2520models%2520across%25201%252C180%2520keywords%2520spanning%252018%250Ascientific%2520domains%252C%2520we%2520reveal%2520that%2520scientific%2520creative%2520ability%2520shows%2520distinct%250Apatterns%2520from%2520general%2520intelligence%2520metrics.%2520Notably%252C%2520our%2520results%2520demonstrate%250Athat%2520models%2520like%2520QwQ-32B-preview%2520achieve%2520comparable%2520creative%2520performance%2520to%250Atop-tier%2520models%2520like%2520o1-preview%252C%2520despite%2520significant%2520gaps%2520in%2520their%2520general%250Aintelligence%2520scores.%2520These%2520findings%2520highlight%2520the%2520importance%2520of%2520specialized%250Aevaluation%2520frameworks%2520for%2520scientific%2520creativity%2520and%2520suggest%2520that%2520the%250Adevelopment%2520of%2520creative%2520capabilities%2520in%2520LLMs%2520may%2520follow%2520different%2520trajectories%250Athan%2520traditional%2520problem-solving%2520abilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiveIdeaBench%3A%20Evaluating%20LLMs%27%20Scientific%20Creativity%20and%20Idea%0A%20%20Generation%20with%20Minimal%20Context&entry.906535625=Kai%20Ruan%20and%20Xuan%20Wang%20and%20Jixiang%20Hong%20and%20Hao%20Sun&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%0Ain%20scientific%20tasks%2C%20existing%20evaluation%20frameworks%20primarily%20assess%20their%0Aperformance%20using%20rich%20contextual%20inputs%2C%20overlooking%20their%20ability%20to%20generate%0Anovel%20ideas%20from%20minimal%20information.%20We%20introduce%20LiveIdeaBench%2C%20a%0Acomprehensive%20benchmark%20that%20evaluates%20LLMs%27%20scientific%20creativity%20and%0Adivergent%20thinking%20capabilities%20using%20single-keyword%20prompts.%20Drawing%20from%0AGuilford%27s%20creativity%20theory%2C%20our%20framework%20employs%20a%20dynamic%20panel%20of%0Astate-of-the-art%20LLMs%20to%20assess%20generated%20ideas%20across%20four%20key%20dimensions%3A%0Aoriginality%2C%20feasibility%2C%20fluency%2C%20and%20flexibility.%20Through%20extensive%0Aexperimentation%20with%2020%20leading%20models%20across%201%2C180%20keywords%20spanning%2018%0Ascientific%20domains%2C%20we%20reveal%20that%20scientific%20creative%20ability%20shows%20distinct%0Apatterns%20from%20general%20intelligence%20metrics.%20Notably%2C%20our%20results%20demonstrate%0Athat%20models%20like%20QwQ-32B-preview%20achieve%20comparable%20creative%20performance%20to%0Atop-tier%20models%20like%20o1-preview%2C%20despite%20significant%20gaps%20in%20their%20general%0Aintelligence%20scores.%20These%20findings%20highlight%20the%20importance%20of%20specialized%0Aevaluation%20frameworks%20for%20scientific%20creativity%20and%20suggest%20that%20the%0Adevelopment%20of%20creative%20capabilities%20in%20LLMs%20may%20follow%20different%20trajectories%0Athan%20traditional%20problem-solving%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17596v1&entry.124074799=Read"},
{"title": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis", "author": "Tianqi Li and Ruobing Zheng and Minghui Yang and Jingdong Chen and Ming Yang", "abstract": "  Recent advances in diffusion models have revolutionized audio-driven talking\nhead synthesis. Beyond precise lip synchronization, diffusion-based methods\nexcel in generating subtle expressions and natural head movements that are\nwell-aligned with the audio signal. However, these methods are confronted by\nslow inference speed, insufficient fine-grained control over facial motions,\nand occasional visual artifacts largely due to an implicit latent space derived\nfrom Variational Auto-Encoders (VAE), which prevent their adoption in realtime\ninteraction applications. To address these issues, we introduce Ditto, a\ndiffusion-based framework that enables controllable realtime talking head\nsynthesis. Our key innovation lies in bridging motion generation and\nphotorealistic neural rendering through an explicit identity-agnostic motion\nspace, replacing conventional VAE representations. This design substantially\nreduces the complexity of diffusion learning while enabling precise control\nover the synthesized talking heads. We further propose an inference strategy\nthat jointly optimizes three key components: audio feature extraction, motion\ngeneration, and video synthesis. This optimization enables streaming\nprocessing, realtime inference, and low first-frame delay, which are the\nfunctionalities crucial for interactive applications such as AI assistants.\nExtensive experimental results demonstrate that Ditto generates compelling\ntalking head videos and substantially outperforms existing methods in both\nmotion control and realtime performance.\n", "link": "http://arxiv.org/abs/2411.19509v2", "date": "2024-12-23", "relevancy": 2.6107, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6931}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.632}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ditto%3A%20Motion-Space%20Diffusion%20for%20Controllable%20Realtime%20Talking%20Head%0A%20%20Synthesis&body=Title%3A%20Ditto%3A%20Motion-Space%20Diffusion%20for%20Controllable%20Realtime%20Talking%20Head%0A%20%20Synthesis%0AAuthor%3A%20Tianqi%20Li%20and%20Ruobing%20Zheng%20and%20Minghui%20Yang%20and%20Jingdong%20Chen%20and%20Ming%20Yang%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20revolutionized%20audio-driven%20talking%0Ahead%20synthesis.%20Beyond%20precise%20lip%20synchronization%2C%20diffusion-based%20methods%0Aexcel%20in%20generating%20subtle%20expressions%20and%20natural%20head%20movements%20that%20are%0Awell-aligned%20with%20the%20audio%20signal.%20However%2C%20these%20methods%20are%20confronted%20by%0Aslow%20inference%20speed%2C%20insufficient%20fine-grained%20control%20over%20facial%20motions%2C%0Aand%20occasional%20visual%20artifacts%20largely%20due%20to%20an%20implicit%20latent%20space%20derived%0Afrom%20Variational%20Auto-Encoders%20%28VAE%29%2C%20which%20prevent%20their%20adoption%20in%20realtime%0Ainteraction%20applications.%20To%20address%20these%20issues%2C%20we%20introduce%20Ditto%2C%20a%0Adiffusion-based%20framework%20that%20enables%20controllable%20realtime%20talking%20head%0Asynthesis.%20Our%20key%20innovation%20lies%20in%20bridging%20motion%20generation%20and%0Aphotorealistic%20neural%20rendering%20through%20an%20explicit%20identity-agnostic%20motion%0Aspace%2C%20replacing%20conventional%20VAE%20representations.%20This%20design%20substantially%0Areduces%20the%20complexity%20of%20diffusion%20learning%20while%20enabling%20precise%20control%0Aover%20the%20synthesized%20talking%20heads.%20We%20further%20propose%20an%20inference%20strategy%0Athat%20jointly%20optimizes%20three%20key%20components%3A%20audio%20feature%20extraction%2C%20motion%0Ageneration%2C%20and%20video%20synthesis.%20This%20optimization%20enables%20streaming%0Aprocessing%2C%20realtime%20inference%2C%20and%20low%20first-frame%20delay%2C%20which%20are%20the%0Afunctionalities%20crucial%20for%20interactive%20applications%20such%20as%20AI%20assistants.%0AExtensive%20experimental%20results%20demonstrate%20that%20Ditto%20generates%20compelling%0Atalking%20head%20videos%20and%20substantially%20outperforms%20existing%20methods%20in%20both%0Amotion%20control%20and%20realtime%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDitto%253A%2520Motion-Space%2520Diffusion%2520for%2520Controllable%2520Realtime%2520Talking%2520Head%250A%2520%2520Synthesis%26entry.906535625%3DTianqi%2520Li%2520and%2520Ruobing%2520Zheng%2520and%2520Minghui%2520Yang%2520and%2520Jingdong%2520Chen%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520revolutionized%2520audio-driven%2520talking%250Ahead%2520synthesis.%2520Beyond%2520precise%2520lip%2520synchronization%252C%2520diffusion-based%2520methods%250Aexcel%2520in%2520generating%2520subtle%2520expressions%2520and%2520natural%2520head%2520movements%2520that%2520are%250Awell-aligned%2520with%2520the%2520audio%2520signal.%2520However%252C%2520these%2520methods%2520are%2520confronted%2520by%250Aslow%2520inference%2520speed%252C%2520insufficient%2520fine-grained%2520control%2520over%2520facial%2520motions%252C%250Aand%2520occasional%2520visual%2520artifacts%2520largely%2520due%2520to%2520an%2520implicit%2520latent%2520space%2520derived%250Afrom%2520Variational%2520Auto-Encoders%2520%2528VAE%2529%252C%2520which%2520prevent%2520their%2520adoption%2520in%2520realtime%250Ainteraction%2520applications.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Ditto%252C%2520a%250Adiffusion-based%2520framework%2520that%2520enables%2520controllable%2520realtime%2520talking%2520head%250Asynthesis.%2520Our%2520key%2520innovation%2520lies%2520in%2520bridging%2520motion%2520generation%2520and%250Aphotorealistic%2520neural%2520rendering%2520through%2520an%2520explicit%2520identity-agnostic%2520motion%250Aspace%252C%2520replacing%2520conventional%2520VAE%2520representations.%2520This%2520design%2520substantially%250Areduces%2520the%2520complexity%2520of%2520diffusion%2520learning%2520while%2520enabling%2520precise%2520control%250Aover%2520the%2520synthesized%2520talking%2520heads.%2520We%2520further%2520propose%2520an%2520inference%2520strategy%250Athat%2520jointly%2520optimizes%2520three%2520key%2520components%253A%2520audio%2520feature%2520extraction%252C%2520motion%250Ageneration%252C%2520and%2520video%2520synthesis.%2520This%2520optimization%2520enables%2520streaming%250Aprocessing%252C%2520realtime%2520inference%252C%2520and%2520low%2520first-frame%2520delay%252C%2520which%2520are%2520the%250Afunctionalities%2520crucial%2520for%2520interactive%2520applications%2520such%2520as%2520AI%2520assistants.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520Ditto%2520generates%2520compelling%250Atalking%2520head%2520videos%2520and%2520substantially%2520outperforms%2520existing%2520methods%2520in%2520both%250Amotion%2520control%2520and%2520realtime%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ditto%3A%20Motion-Space%20Diffusion%20for%20Controllable%20Realtime%20Talking%20Head%0A%20%20Synthesis&entry.906535625=Tianqi%20Li%20and%20Ruobing%20Zheng%20and%20Minghui%20Yang%20and%20Jingdong%20Chen%20and%20Ming%20Yang&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20revolutionized%20audio-driven%20talking%0Ahead%20synthesis.%20Beyond%20precise%20lip%20synchronization%2C%20diffusion-based%20methods%0Aexcel%20in%20generating%20subtle%20expressions%20and%20natural%20head%20movements%20that%20are%0Awell-aligned%20with%20the%20audio%20signal.%20However%2C%20these%20methods%20are%20confronted%20by%0Aslow%20inference%20speed%2C%20insufficient%20fine-grained%20control%20over%20facial%20motions%2C%0Aand%20occasional%20visual%20artifacts%20largely%20due%20to%20an%20implicit%20latent%20space%20derived%0Afrom%20Variational%20Auto-Encoders%20%28VAE%29%2C%20which%20prevent%20their%20adoption%20in%20realtime%0Ainteraction%20applications.%20To%20address%20these%20issues%2C%20we%20introduce%20Ditto%2C%20a%0Adiffusion-based%20framework%20that%20enables%20controllable%20realtime%20talking%20head%0Asynthesis.%20Our%20key%20innovation%20lies%20in%20bridging%20motion%20generation%20and%0Aphotorealistic%20neural%20rendering%20through%20an%20explicit%20identity-agnostic%20motion%0Aspace%2C%20replacing%20conventional%20VAE%20representations.%20This%20design%20substantially%0Areduces%20the%20complexity%20of%20diffusion%20learning%20while%20enabling%20precise%20control%0Aover%20the%20synthesized%20talking%20heads.%20We%20further%20propose%20an%20inference%20strategy%0Athat%20jointly%20optimizes%20three%20key%20components%3A%20audio%20feature%20extraction%2C%20motion%0Ageneration%2C%20and%20video%20synthesis.%20This%20optimization%20enables%20streaming%0Aprocessing%2C%20realtime%20inference%2C%20and%20low%20first-frame%20delay%2C%20which%20are%20the%0Afunctionalities%20crucial%20for%20interactive%20applications%20such%20as%20AI%20assistants.%0AExtensive%20experimental%20results%20demonstrate%20that%20Ditto%20generates%20compelling%0Atalking%20head%20videos%20and%20substantially%20outperforms%20existing%20methods%20in%20both%0Amotion%20control%20and%20realtime%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19509v2&entry.124074799=Read"},
{"title": "A Toolkit for Virtual Reality Data Collection", "author": "Tim Rolff and Niklas Hypki and Markus Lappe and Frank Steinicke", "abstract": "  Due to the still relatively low number of users, acquiring large-scale and\nmultidimensional virtual reality datasets remains a significant challenge.\nConsequently, VR datasets comparable in size to state-of-the-art collections in\nnatural language processing or computer vision are rare or absent. However, the\navailability of such datasets could unlock groundbreaking advancements in\ndeep-learning, psychological modeling, and data analysis in the context of VR.\nIn this paper, we present a versatile data collection toolkit designed to\nfacilitate the capturing of extensive VR datasets. Our toolkit seamlessly\nintegrates with any device, either directly via OpenXR or through the use of a\nvirtual device. Additionally, we introduce a robust data collection pipeline\nthat emphasizes ethical practices (e.g., ensuring data protection and\nregulation) and ensures a standardized, reproducible methodology.\n", "link": "http://arxiv.org/abs/2412.17490v1", "date": "2024-12-23", "relevancy": 2.6069, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5272}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5272}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Toolkit%20for%20Virtual%20Reality%20Data%20Collection&body=Title%3A%20A%20Toolkit%20for%20Virtual%20Reality%20Data%20Collection%0AAuthor%3A%20Tim%20Rolff%20and%20Niklas%20Hypki%20and%20Markus%20Lappe%20and%20Frank%20Steinicke%0AAbstract%3A%20%20%20Due%20to%20the%20still%20relatively%20low%20number%20of%20users%2C%20acquiring%20large-scale%20and%0Amultidimensional%20virtual%20reality%20datasets%20remains%20a%20significant%20challenge.%0AConsequently%2C%20VR%20datasets%20comparable%20in%20size%20to%20state-of-the-art%20collections%20in%0Anatural%20language%20processing%20or%20computer%20vision%20are%20rare%20or%20absent.%20However%2C%20the%0Aavailability%20of%20such%20datasets%20could%20unlock%20groundbreaking%20advancements%20in%0Adeep-learning%2C%20psychological%20modeling%2C%20and%20data%20analysis%20in%20the%20context%20of%20VR.%0AIn%20this%20paper%2C%20we%20present%20a%20versatile%20data%20collection%20toolkit%20designed%20to%0Afacilitate%20the%20capturing%20of%20extensive%20VR%20datasets.%20Our%20toolkit%20seamlessly%0Aintegrates%20with%20any%20device%2C%20either%20directly%20via%20OpenXR%20or%20through%20the%20use%20of%20a%0Avirtual%20device.%20Additionally%2C%20we%20introduce%20a%20robust%20data%20collection%20pipeline%0Athat%20emphasizes%20ethical%20practices%20%28e.g.%2C%20ensuring%20data%20protection%20and%0Aregulation%29%20and%20ensures%20a%20standardized%2C%20reproducible%20methodology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Toolkit%2520for%2520Virtual%2520Reality%2520Data%2520Collection%26entry.906535625%3DTim%2520Rolff%2520and%2520Niklas%2520Hypki%2520and%2520Markus%2520Lappe%2520and%2520Frank%2520Steinicke%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520still%2520relatively%2520low%2520number%2520of%2520users%252C%2520acquiring%2520large-scale%2520and%250Amultidimensional%2520virtual%2520reality%2520datasets%2520remains%2520a%2520significant%2520challenge.%250AConsequently%252C%2520VR%2520datasets%2520comparable%2520in%2520size%2520to%2520state-of-the-art%2520collections%2520in%250Anatural%2520language%2520processing%2520or%2520computer%2520vision%2520are%2520rare%2520or%2520absent.%2520However%252C%2520the%250Aavailability%2520of%2520such%2520datasets%2520could%2520unlock%2520groundbreaking%2520advancements%2520in%250Adeep-learning%252C%2520psychological%2520modeling%252C%2520and%2520data%2520analysis%2520in%2520the%2520context%2520of%2520VR.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520versatile%2520data%2520collection%2520toolkit%2520designed%2520to%250Afacilitate%2520the%2520capturing%2520of%2520extensive%2520VR%2520datasets.%2520Our%2520toolkit%2520seamlessly%250Aintegrates%2520with%2520any%2520device%252C%2520either%2520directly%2520via%2520OpenXR%2520or%2520through%2520the%2520use%2520of%2520a%250Avirtual%2520device.%2520Additionally%252C%2520we%2520introduce%2520a%2520robust%2520data%2520collection%2520pipeline%250Athat%2520emphasizes%2520ethical%2520practices%2520%2528e.g.%252C%2520ensuring%2520data%2520protection%2520and%250Aregulation%2529%2520and%2520ensures%2520a%2520standardized%252C%2520reproducible%2520methodology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Toolkit%20for%20Virtual%20Reality%20Data%20Collection&entry.906535625=Tim%20Rolff%20and%20Niklas%20Hypki%20and%20Markus%20Lappe%20and%20Frank%20Steinicke&entry.1292438233=%20%20Due%20to%20the%20still%20relatively%20low%20number%20of%20users%2C%20acquiring%20large-scale%20and%0Amultidimensional%20virtual%20reality%20datasets%20remains%20a%20significant%20challenge.%0AConsequently%2C%20VR%20datasets%20comparable%20in%20size%20to%20state-of-the-art%20collections%20in%0Anatural%20language%20processing%20or%20computer%20vision%20are%20rare%20or%20absent.%20However%2C%20the%0Aavailability%20of%20such%20datasets%20could%20unlock%20groundbreaking%20advancements%20in%0Adeep-learning%2C%20psychological%20modeling%2C%20and%20data%20analysis%20in%20the%20context%20of%20VR.%0AIn%20this%20paper%2C%20we%20present%20a%20versatile%20data%20collection%20toolkit%20designed%20to%0Afacilitate%20the%20capturing%20of%20extensive%20VR%20datasets.%20Our%20toolkit%20seamlessly%0Aintegrates%20with%20any%20device%2C%20either%20directly%20via%20OpenXR%20or%20through%20the%20use%20of%20a%0Avirtual%20device.%20Additionally%2C%20we%20introduce%20a%20robust%20data%20collection%20pipeline%0Athat%20emphasizes%20ethical%20practices%20%28e.g.%2C%20ensuring%20data%20protection%20and%0Aregulation%29%20and%20ensures%20a%20standardized%2C%20reproducible%20methodology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17490v1&entry.124074799=Read"},
{"title": "GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation", "author": "Haoran Lu and Ruihai Wu and Yitong Li and Sijie Li and Ziyu Zhu and Chuanruo Ning and Yan Shen and Longzan Luo and Yuanpei Chen and Hao Dong", "abstract": "  Manipulating garments and fabrics has long been a critical endeavor in the\ndevelopment of home-assistant robots. However, due to complex dynamics and\ntopological structures, garment manipulations pose significant challenges.\nRecent successes in reinforcement learning and vision-based methods offer\npromising avenues for learning garment manipulation. Nevertheless, these\napproaches are severely constrained by current benchmarks, which offer limited\ndiversity of tasks and unrealistic simulation behavior. Therefore, we present\nGarmentLab, a content-rich benchmark and realistic simulation designed for\ndeformable object and garment manipulation. Our benchmark encompasses a diverse\nrange of garment types, robotic systems and manipulators. The abundant tasks in\nthe benchmark further explores of the interactions between garments, deformable\nobjects, rigid bodies, fluids, and human body. Moreover, by incorporating\nmultiple simulation methods such as FEM and PBD, along with our proposed\nsim-to-real algorithms and real-world benchmark, we aim to significantly narrow\nthe sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement\nlearning, and imitation learning approaches on these tasks, highlighting the\nchallenges faced by current algorithms, notably their limited generalization\ncapabilities. Our proposed open-source environments and comprehensive analysis\nshow promising boost to future research in garment manipulation by unlocking\nthe full potential of these methods. We guarantee that we will open-source our\ncode as soon as possible. You can watch the videos in supplementary files to\nlearn more about the details of our work. Our project page is available at:\nhttps://garmentlab.github.io/\n", "link": "http://arxiv.org/abs/2411.01200v3", "date": "2024-12-23", "relevancy": 2.6055, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6867}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6458}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GarmentLab%3A%20A%20Unified%20Simulation%20and%20Benchmark%20for%20Garment%20Manipulation&body=Title%3A%20GarmentLab%3A%20A%20Unified%20Simulation%20and%20Benchmark%20for%20Garment%20Manipulation%0AAuthor%3A%20Haoran%20Lu%20and%20Ruihai%20Wu%20and%20Yitong%20Li%20and%20Sijie%20Li%20and%20Ziyu%20Zhu%20and%20Chuanruo%20Ning%20and%20Yan%20Shen%20and%20Longzan%20Luo%20and%20Yuanpei%20Chen%20and%20Hao%20Dong%0AAbstract%3A%20%20%20Manipulating%20garments%20and%20fabrics%20has%20long%20been%20a%20critical%20endeavor%20in%20the%0Adevelopment%20of%20home-assistant%20robots.%20However%2C%20due%20to%20complex%20dynamics%20and%0Atopological%20structures%2C%20garment%20manipulations%20pose%20significant%20challenges.%0ARecent%20successes%20in%20reinforcement%20learning%20and%20vision-based%20methods%20offer%0Apromising%20avenues%20for%20learning%20garment%20manipulation.%20Nevertheless%2C%20these%0Aapproaches%20are%20severely%20constrained%20by%20current%20benchmarks%2C%20which%20offer%20limited%0Adiversity%20of%20tasks%20and%20unrealistic%20simulation%20behavior.%20Therefore%2C%20we%20present%0AGarmentLab%2C%20a%20content-rich%20benchmark%20and%20realistic%20simulation%20designed%20for%0Adeformable%20object%20and%20garment%20manipulation.%20Our%20benchmark%20encompasses%20a%20diverse%0Arange%20of%20garment%20types%2C%20robotic%20systems%20and%20manipulators.%20The%20abundant%20tasks%20in%0Athe%20benchmark%20further%20explores%20of%20the%20interactions%20between%20garments%2C%20deformable%0Aobjects%2C%20rigid%20bodies%2C%20fluids%2C%20and%20human%20body.%20Moreover%2C%20by%20incorporating%0Amultiple%20simulation%20methods%20such%20as%20FEM%20and%20PBD%2C%20along%20with%20our%20proposed%0Asim-to-real%20algorithms%20and%20real-world%20benchmark%2C%20we%20aim%20to%20significantly%20narrow%0Athe%20sim-to-real%20gap.%20We%20evaluate%20state-of-the-art%20vision%20methods%2C%20reinforcement%0Alearning%2C%20and%20imitation%20learning%20approaches%20on%20these%20tasks%2C%20highlighting%20the%0Achallenges%20faced%20by%20current%20algorithms%2C%20notably%20their%20limited%20generalization%0Acapabilities.%20Our%20proposed%20open-source%20environments%20and%20comprehensive%20analysis%0Ashow%20promising%20boost%20to%20future%20research%20in%20garment%20manipulation%20by%20unlocking%0Athe%20full%20potential%20of%20these%20methods.%20We%20guarantee%20that%20we%20will%20open-source%20our%0Acode%20as%20soon%20as%20possible.%20You%20can%20watch%20the%20videos%20in%20supplementary%20files%20to%0Alearn%20more%20about%20the%20details%20of%20our%20work.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//garmentlab.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01200v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarmentLab%253A%2520A%2520Unified%2520Simulation%2520and%2520Benchmark%2520for%2520Garment%2520Manipulation%26entry.906535625%3DHaoran%2520Lu%2520and%2520Ruihai%2520Wu%2520and%2520Yitong%2520Li%2520and%2520Sijie%2520Li%2520and%2520Ziyu%2520Zhu%2520and%2520Chuanruo%2520Ning%2520and%2520Yan%2520Shen%2520and%2520Longzan%2520Luo%2520and%2520Yuanpei%2520Chen%2520and%2520Hao%2520Dong%26entry.1292438233%3D%2520%2520Manipulating%2520garments%2520and%2520fabrics%2520has%2520long%2520been%2520a%2520critical%2520endeavor%2520in%2520the%250Adevelopment%2520of%2520home-assistant%2520robots.%2520However%252C%2520due%2520to%2520complex%2520dynamics%2520and%250Atopological%2520structures%252C%2520garment%2520manipulations%2520pose%2520significant%2520challenges.%250ARecent%2520successes%2520in%2520reinforcement%2520learning%2520and%2520vision-based%2520methods%2520offer%250Apromising%2520avenues%2520for%2520learning%2520garment%2520manipulation.%2520Nevertheless%252C%2520these%250Aapproaches%2520are%2520severely%2520constrained%2520by%2520current%2520benchmarks%252C%2520which%2520offer%2520limited%250Adiversity%2520of%2520tasks%2520and%2520unrealistic%2520simulation%2520behavior.%2520Therefore%252C%2520we%2520present%250AGarmentLab%252C%2520a%2520content-rich%2520benchmark%2520and%2520realistic%2520simulation%2520designed%2520for%250Adeformable%2520object%2520and%2520garment%2520manipulation.%2520Our%2520benchmark%2520encompasses%2520a%2520diverse%250Arange%2520of%2520garment%2520types%252C%2520robotic%2520systems%2520and%2520manipulators.%2520The%2520abundant%2520tasks%2520in%250Athe%2520benchmark%2520further%2520explores%2520of%2520the%2520interactions%2520between%2520garments%252C%2520deformable%250Aobjects%252C%2520rigid%2520bodies%252C%2520fluids%252C%2520and%2520human%2520body.%2520Moreover%252C%2520by%2520incorporating%250Amultiple%2520simulation%2520methods%2520such%2520as%2520FEM%2520and%2520PBD%252C%2520along%2520with%2520our%2520proposed%250Asim-to-real%2520algorithms%2520and%2520real-world%2520benchmark%252C%2520we%2520aim%2520to%2520significantly%2520narrow%250Athe%2520sim-to-real%2520gap.%2520We%2520evaluate%2520state-of-the-art%2520vision%2520methods%252C%2520reinforcement%250Alearning%252C%2520and%2520imitation%2520learning%2520approaches%2520on%2520these%2520tasks%252C%2520highlighting%2520the%250Achallenges%2520faced%2520by%2520current%2520algorithms%252C%2520notably%2520their%2520limited%2520generalization%250Acapabilities.%2520Our%2520proposed%2520open-source%2520environments%2520and%2520comprehensive%2520analysis%250Ashow%2520promising%2520boost%2520to%2520future%2520research%2520in%2520garment%2520manipulation%2520by%2520unlocking%250Athe%2520full%2520potential%2520of%2520these%2520methods.%2520We%2520guarantee%2520that%2520we%2520will%2520open-source%2520our%250Acode%2520as%2520soon%2520as%2520possible.%2520You%2520can%2520watch%2520the%2520videos%2520in%2520supplementary%2520files%2520to%250Alearn%2520more%2520about%2520the%2520details%2520of%2520our%2520work.%2520Our%2520project%2520page%2520is%2520available%2520at%253A%250Ahttps%253A//garmentlab.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01200v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GarmentLab%3A%20A%20Unified%20Simulation%20and%20Benchmark%20for%20Garment%20Manipulation&entry.906535625=Haoran%20Lu%20and%20Ruihai%20Wu%20and%20Yitong%20Li%20and%20Sijie%20Li%20and%20Ziyu%20Zhu%20and%20Chuanruo%20Ning%20and%20Yan%20Shen%20and%20Longzan%20Luo%20and%20Yuanpei%20Chen%20and%20Hao%20Dong&entry.1292438233=%20%20Manipulating%20garments%20and%20fabrics%20has%20long%20been%20a%20critical%20endeavor%20in%20the%0Adevelopment%20of%20home-assistant%20robots.%20However%2C%20due%20to%20complex%20dynamics%20and%0Atopological%20structures%2C%20garment%20manipulations%20pose%20significant%20challenges.%0ARecent%20successes%20in%20reinforcement%20learning%20and%20vision-based%20methods%20offer%0Apromising%20avenues%20for%20learning%20garment%20manipulation.%20Nevertheless%2C%20these%0Aapproaches%20are%20severely%20constrained%20by%20current%20benchmarks%2C%20which%20offer%20limited%0Adiversity%20of%20tasks%20and%20unrealistic%20simulation%20behavior.%20Therefore%2C%20we%20present%0AGarmentLab%2C%20a%20content-rich%20benchmark%20and%20realistic%20simulation%20designed%20for%0Adeformable%20object%20and%20garment%20manipulation.%20Our%20benchmark%20encompasses%20a%20diverse%0Arange%20of%20garment%20types%2C%20robotic%20systems%20and%20manipulators.%20The%20abundant%20tasks%20in%0Athe%20benchmark%20further%20explores%20of%20the%20interactions%20between%20garments%2C%20deformable%0Aobjects%2C%20rigid%20bodies%2C%20fluids%2C%20and%20human%20body.%20Moreover%2C%20by%20incorporating%0Amultiple%20simulation%20methods%20such%20as%20FEM%20and%20PBD%2C%20along%20with%20our%20proposed%0Asim-to-real%20algorithms%20and%20real-world%20benchmark%2C%20we%20aim%20to%20significantly%20narrow%0Athe%20sim-to-real%20gap.%20We%20evaluate%20state-of-the-art%20vision%20methods%2C%20reinforcement%0Alearning%2C%20and%20imitation%20learning%20approaches%20on%20these%20tasks%2C%20highlighting%20the%0Achallenges%20faced%20by%20current%20algorithms%2C%20notably%20their%20limited%20generalization%0Acapabilities.%20Our%20proposed%20open-source%20environments%20and%20comprehensive%20analysis%0Ashow%20promising%20boost%20to%20future%20research%20in%20garment%20manipulation%20by%20unlocking%0Athe%20full%20potential%20of%20these%20methods.%20We%20guarantee%20that%20we%20will%20open-source%20our%0Acode%20as%20soon%20as%20possible.%20You%20can%20watch%20the%20videos%20in%20supplementary%20files%20to%0Alearn%20more%20about%20the%20details%20of%20our%20work.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//garmentlab.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01200v3&entry.124074799=Read"},
{"title": "Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering", "author": "Youngsun Lim and Hojun Choi and Hyunjung Shim", "abstract": "  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing TTI models can correctly\nrespond to these questions. The I-HallA v1.0 dataset comprises 1.2K diverse\nimage-text pairs across nine categories with 1,000 rigorously curated questions\ncovering various compositional challenges. We evaluate five TTI models using\nI-HallA and reveal that these state-of-the-art models often fail to accurately\nconvey factual information. Moreover, we validate the reliability of our metric\nby demonstrating a strong Spearman correlation ($\\rho$=0.95) with human\njudgments. We believe our benchmark dataset and metric can serve as a\nfoundation for developing factually accurate TTI generation models. Additional\nresources can be found on our project page: https://sgt-lim.github.io/I-HallA/.\n", "link": "http://arxiv.org/abs/2409.12784v6", "date": "2024-12-23", "relevancy": 2.6026, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5254}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5182}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering&body=Title%3A%20Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering%0AAuthor%3A%20Youngsun%20Lim%20and%20Hojun%20Choi%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Despite%20the%20impressive%20success%20of%20text-to-image%20%28TTI%29%20generation%20models%2C%0Aexisting%20studies%20overlook%20the%20issue%20of%20whether%20these%20models%20accurately%20convey%0Afactual%20information.%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20image%0Ahallucination%2C%20where%20images%20created%20by%20generation%20models%20fail%20to%20faithfully%0Adepict%20factual%20content.%20To%20address%20this%2C%20we%20introduce%20I-HallA%20%28Image%0AHallucination%20evaluation%20with%20Question%20Answering%29%2C%20a%20novel%20automated%20evaluation%0Ametric%20that%20measures%20the%20factuality%20of%20generated%20images%20through%20visual%20question%0Aanswering%20%28VQA%29.%20We%20also%20introduce%20I-HallA%20v1.0%2C%20a%20curated%20benchmark%20dataset%0Afor%20this%20purpose.%20As%20part%20of%20this%20process%2C%20we%20develop%20a%20pipeline%20that%20generates%0Ahigh-quality%20question-answer%20pairs%20using%20multiple%20GPT-4%20Omni-based%20agents%2C%20with%0Ahuman%20judgments%20to%20ensure%20accuracy.%20Our%20evaluation%20protocols%20measure%20image%0Ahallucination%20by%20testing%20if%20images%20from%20existing%20TTI%20models%20can%20correctly%0Arespond%20to%20these%20questions.%20The%20I-HallA%20v1.0%20dataset%20comprises%201.2K%20diverse%0Aimage-text%20pairs%20across%20nine%20categories%20with%201%2C000%20rigorously%20curated%20questions%0Acovering%20various%20compositional%20challenges.%20We%20evaluate%20five%20TTI%20models%20using%0AI-HallA%20and%20reveal%20that%20these%20state-of-the-art%20models%20often%20fail%20to%20accurately%0Aconvey%20factual%20information.%20Moreover%2C%20we%20validate%20the%20reliability%20of%20our%20metric%0Aby%20demonstrating%20a%20strong%20Spearman%20correlation%20%28%24%5Crho%24%3D0.95%29%20with%20human%0Ajudgments.%20We%20believe%20our%20benchmark%20dataset%20and%20metric%20can%20serve%20as%20a%0Afoundation%20for%20developing%20factually%20accurate%20TTI%20generation%20models.%20Additional%0Aresources%20can%20be%20found%20on%20our%20project%20page%3A%20https%3A//sgt-lim.github.io/I-HallA/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12784v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Image%2520Hallucination%2520in%2520Text-to-Image%2520Generation%2520with%250A%2520%2520Question-Answering%26entry.906535625%3DYoungsun%2520Lim%2520and%2520Hojun%2520Choi%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520success%2520of%2520text-to-image%2520%2528TTI%2529%2520generation%2520models%252C%250Aexisting%2520studies%2520overlook%2520the%2520issue%2520of%2520whether%2520these%2520models%2520accurately%2520convey%250Afactual%2520information.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520problem%2520of%2520image%250Ahallucination%252C%2520where%2520images%2520created%2520by%2520generation%2520models%2520fail%2520to%2520faithfully%250Adepict%2520factual%2520content.%2520To%2520address%2520this%252C%2520we%2520introduce%2520I-HallA%2520%2528Image%250AHallucination%2520evaluation%2520with%2520Question%2520Answering%2529%252C%2520a%2520novel%2520automated%2520evaluation%250Ametric%2520that%2520measures%2520the%2520factuality%2520of%2520generated%2520images%2520through%2520visual%2520question%250Aanswering%2520%2528VQA%2529.%2520We%2520also%2520introduce%2520I-HallA%2520v1.0%252C%2520a%2520curated%2520benchmark%2520dataset%250Afor%2520this%2520purpose.%2520As%2520part%2520of%2520this%2520process%252C%2520we%2520develop%2520a%2520pipeline%2520that%2520generates%250Ahigh-quality%2520question-answer%2520pairs%2520using%2520multiple%2520GPT-4%2520Omni-based%2520agents%252C%2520with%250Ahuman%2520judgments%2520to%2520ensure%2520accuracy.%2520Our%2520evaluation%2520protocols%2520measure%2520image%250Ahallucination%2520by%2520testing%2520if%2520images%2520from%2520existing%2520TTI%2520models%2520can%2520correctly%250Arespond%2520to%2520these%2520questions.%2520The%2520I-HallA%2520v1.0%2520dataset%2520comprises%25201.2K%2520diverse%250Aimage-text%2520pairs%2520across%2520nine%2520categories%2520with%25201%252C000%2520rigorously%2520curated%2520questions%250Acovering%2520various%2520compositional%2520challenges.%2520We%2520evaluate%2520five%2520TTI%2520models%2520using%250AI-HallA%2520and%2520reveal%2520that%2520these%2520state-of-the-art%2520models%2520often%2520fail%2520to%2520accurately%250Aconvey%2520factual%2520information.%2520Moreover%252C%2520we%2520validate%2520the%2520reliability%2520of%2520our%2520metric%250Aby%2520demonstrating%2520a%2520strong%2520Spearman%2520correlation%2520%2528%2524%255Crho%2524%253D0.95%2529%2520with%2520human%250Ajudgments.%2520We%2520believe%2520our%2520benchmark%2520dataset%2520and%2520metric%2520can%2520serve%2520as%2520a%250Afoundation%2520for%2520developing%2520factually%2520accurate%2520TTI%2520generation%2520models.%2520Additional%250Aresources%2520can%2520be%2520found%2520on%2520our%2520project%2520page%253A%2520https%253A//sgt-lim.github.io/I-HallA/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12784v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering&entry.906535625=Youngsun%20Lim%20and%20Hojun%20Choi%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Despite%20the%20impressive%20success%20of%20text-to-image%20%28TTI%29%20generation%20models%2C%0Aexisting%20studies%20overlook%20the%20issue%20of%20whether%20these%20models%20accurately%20convey%0Afactual%20information.%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20image%0Ahallucination%2C%20where%20images%20created%20by%20generation%20models%20fail%20to%20faithfully%0Adepict%20factual%20content.%20To%20address%20this%2C%20we%20introduce%20I-HallA%20%28Image%0AHallucination%20evaluation%20with%20Question%20Answering%29%2C%20a%20novel%20automated%20evaluation%0Ametric%20that%20measures%20the%20factuality%20of%20generated%20images%20through%20visual%20question%0Aanswering%20%28VQA%29.%20We%20also%20introduce%20I-HallA%20v1.0%2C%20a%20curated%20benchmark%20dataset%0Afor%20this%20purpose.%20As%20part%20of%20this%20process%2C%20we%20develop%20a%20pipeline%20that%20generates%0Ahigh-quality%20question-answer%20pairs%20using%20multiple%20GPT-4%20Omni-based%20agents%2C%20with%0Ahuman%20judgments%20to%20ensure%20accuracy.%20Our%20evaluation%20protocols%20measure%20image%0Ahallucination%20by%20testing%20if%20images%20from%20existing%20TTI%20models%20can%20correctly%0Arespond%20to%20these%20questions.%20The%20I-HallA%20v1.0%20dataset%20comprises%201.2K%20diverse%0Aimage-text%20pairs%20across%20nine%20categories%20with%201%2C000%20rigorously%20curated%20questions%0Acovering%20various%20compositional%20challenges.%20We%20evaluate%20five%20TTI%20models%20using%0AI-HallA%20and%20reveal%20that%20these%20state-of-the-art%20models%20often%20fail%20to%20accurately%0Aconvey%20factual%20information.%20Moreover%2C%20we%20validate%20the%20reliability%20of%20our%20metric%0Aby%20demonstrating%20a%20strong%20Spearman%20correlation%20%28%24%5Crho%24%3D0.95%29%20with%20human%0Ajudgments.%20We%20believe%20our%20benchmark%20dataset%20and%20metric%20can%20serve%20as%20a%0Afoundation%20for%20developing%20factually%20accurate%20TTI%20generation%20models.%20Additional%0Aresources%20can%20be%20found%20on%20our%20project%20page%3A%20https%3A//sgt-lim.github.io/I-HallA/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12784v6&entry.124074799=Read"},
{"title": "AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot\n  Semantic Segmentation", "author": "Jiaqi Ma and Guo-Sen Xie and Fang Zhao and Zechao Li", "abstract": "  Few-shot learning aims to recognize novel concepts by leveraging prior\nknowledge learned from a few samples. However, for visually intensive tasks\nsuch as few-shot semantic segmentation, pixel-level annotations are\ntime-consuming and costly. Therefore, in this paper, we utilize the more\nchallenging image-level annotations and propose an adaptive frequency-aware\nnetwork (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS).\nSpecifically, we first propose a cross-granularity frequency-aware module (CFM)\nthat decouples RGB images into high-frequency and low-frequency distributions\nand further optimizes semantic structural information by realigning them.\nUnlike most existing WFSS methods using the textual information from the\nmulti-modal language-vision model, e.g., CLIP, in an offline learning manner,\nwe further propose a CLIP-guided spatial-adapter module (CSM), which performs\nspatial domain adaptive transformation on textual information through online\nlearning, thus providing enriched cross-modal semantic information for CFM.\nExtensive experiments on the Pascal-5\\textsuperscript{i} and\nCOCO-20\\textsuperscript{i} datasets demonstrate that AFANet has achieved\nstate-of-the-art performance. The code is available at\nhttps://github.com/jarch-ma/AFANet.\n", "link": "http://arxiv.org/abs/2412.17601v1", "date": "2024-12-23", "relevancy": 2.5859, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5405}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5139}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AFANet%3A%20Adaptive%20Frequency-Aware%20Network%20for%20Weakly-Supervised%20Few-Shot%0A%20%20Semantic%20Segmentation&body=Title%3A%20AFANet%3A%20Adaptive%20Frequency-Aware%20Network%20for%20Weakly-Supervised%20Few-Shot%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Jiaqi%20Ma%20and%20Guo-Sen%20Xie%20and%20Fang%20Zhao%20and%20Zechao%20Li%0AAbstract%3A%20%20%20Few-shot%20learning%20aims%20to%20recognize%20novel%20concepts%20by%20leveraging%20prior%0Aknowledge%20learned%20from%20a%20few%20samples.%20However%2C%20for%20visually%20intensive%20tasks%0Asuch%20as%20few-shot%20semantic%20segmentation%2C%20pixel-level%20annotations%20are%0Atime-consuming%20and%20costly.%20Therefore%2C%20in%20this%20paper%2C%20we%20utilize%20the%20more%0Achallenging%20image-level%20annotations%20and%20propose%20an%20adaptive%20frequency-aware%0Anetwork%20%28AFANet%29%20for%20weakly-supervised%20few-shot%20semantic%20segmentation%20%28WFSS%29.%0ASpecifically%2C%20we%20first%20propose%20a%20cross-granularity%20frequency-aware%20module%20%28CFM%29%0Athat%20decouples%20RGB%20images%20into%20high-frequency%20and%20low-frequency%20distributions%0Aand%20further%20optimizes%20semantic%20structural%20information%20by%20realigning%20them.%0AUnlike%20most%20existing%20WFSS%20methods%20using%20the%20textual%20information%20from%20the%0Amulti-modal%20language-vision%20model%2C%20e.g.%2C%20CLIP%2C%20in%20an%20offline%20learning%20manner%2C%0Awe%20further%20propose%20a%20CLIP-guided%20spatial-adapter%20module%20%28CSM%29%2C%20which%20performs%0Aspatial%20domain%20adaptive%20transformation%20on%20textual%20information%20through%20online%0Alearning%2C%20thus%20providing%20enriched%20cross-modal%20semantic%20information%20for%20CFM.%0AExtensive%20experiments%20on%20the%20Pascal-5%5Ctextsuperscript%7Bi%7D%20and%0ACOCO-20%5Ctextsuperscript%7Bi%7D%20datasets%20demonstrate%20that%20AFANet%20has%20achieved%0Astate-of-the-art%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/jarch-ma/AFANet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAFANet%253A%2520Adaptive%2520Frequency-Aware%2520Network%2520for%2520Weakly-Supervised%2520Few-Shot%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DJiaqi%2520Ma%2520and%2520Guo-Sen%2520Xie%2520and%2520Fang%2520Zhao%2520and%2520Zechao%2520Li%26entry.1292438233%3D%2520%2520Few-shot%2520learning%2520aims%2520to%2520recognize%2520novel%2520concepts%2520by%2520leveraging%2520prior%250Aknowledge%2520learned%2520from%2520a%2520few%2520samples.%2520However%252C%2520for%2520visually%2520intensive%2520tasks%250Asuch%2520as%2520few-shot%2520semantic%2520segmentation%252C%2520pixel-level%2520annotations%2520are%250Atime-consuming%2520and%2520costly.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520utilize%2520the%2520more%250Achallenging%2520image-level%2520annotations%2520and%2520propose%2520an%2520adaptive%2520frequency-aware%250Anetwork%2520%2528AFANet%2529%2520for%2520weakly-supervised%2520few-shot%2520semantic%2520segmentation%2520%2528WFSS%2529.%250ASpecifically%252C%2520we%2520first%2520propose%2520a%2520cross-granularity%2520frequency-aware%2520module%2520%2528CFM%2529%250Athat%2520decouples%2520RGB%2520images%2520into%2520high-frequency%2520and%2520low-frequency%2520distributions%250Aand%2520further%2520optimizes%2520semantic%2520structural%2520information%2520by%2520realigning%2520them.%250AUnlike%2520most%2520existing%2520WFSS%2520methods%2520using%2520the%2520textual%2520information%2520from%2520the%250Amulti-modal%2520language-vision%2520model%252C%2520e.g.%252C%2520CLIP%252C%2520in%2520an%2520offline%2520learning%2520manner%252C%250Awe%2520further%2520propose%2520a%2520CLIP-guided%2520spatial-adapter%2520module%2520%2528CSM%2529%252C%2520which%2520performs%250Aspatial%2520domain%2520adaptive%2520transformation%2520on%2520textual%2520information%2520through%2520online%250Alearning%252C%2520thus%2520providing%2520enriched%2520cross-modal%2520semantic%2520information%2520for%2520CFM.%250AExtensive%2520experiments%2520on%2520the%2520Pascal-5%255Ctextsuperscript%257Bi%257D%2520and%250ACOCO-20%255Ctextsuperscript%257Bi%257D%2520datasets%2520demonstrate%2520that%2520AFANet%2520has%2520achieved%250Astate-of-the-art%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/jarch-ma/AFANet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AFANet%3A%20Adaptive%20Frequency-Aware%20Network%20for%20Weakly-Supervised%20Few-Shot%0A%20%20Semantic%20Segmentation&entry.906535625=Jiaqi%20Ma%20and%20Guo-Sen%20Xie%20and%20Fang%20Zhao%20and%20Zechao%20Li&entry.1292438233=%20%20Few-shot%20learning%20aims%20to%20recognize%20novel%20concepts%20by%20leveraging%20prior%0Aknowledge%20learned%20from%20a%20few%20samples.%20However%2C%20for%20visually%20intensive%20tasks%0Asuch%20as%20few-shot%20semantic%20segmentation%2C%20pixel-level%20annotations%20are%0Atime-consuming%20and%20costly.%20Therefore%2C%20in%20this%20paper%2C%20we%20utilize%20the%20more%0Achallenging%20image-level%20annotations%20and%20propose%20an%20adaptive%20frequency-aware%0Anetwork%20%28AFANet%29%20for%20weakly-supervised%20few-shot%20semantic%20segmentation%20%28WFSS%29.%0ASpecifically%2C%20we%20first%20propose%20a%20cross-granularity%20frequency-aware%20module%20%28CFM%29%0Athat%20decouples%20RGB%20images%20into%20high-frequency%20and%20low-frequency%20distributions%0Aand%20further%20optimizes%20semantic%20structural%20information%20by%20realigning%20them.%0AUnlike%20most%20existing%20WFSS%20methods%20using%20the%20textual%20information%20from%20the%0Amulti-modal%20language-vision%20model%2C%20e.g.%2C%20CLIP%2C%20in%20an%20offline%20learning%20manner%2C%0Awe%20further%20propose%20a%20CLIP-guided%20spatial-adapter%20module%20%28CSM%29%2C%20which%20performs%0Aspatial%20domain%20adaptive%20transformation%20on%20textual%20information%20through%20online%0Alearning%2C%20thus%20providing%20enriched%20cross-modal%20semantic%20information%20for%20CFM.%0AExtensive%20experiments%20on%20the%20Pascal-5%5Ctextsuperscript%7Bi%7D%20and%0ACOCO-20%5Ctextsuperscript%7Bi%7D%20datasets%20demonstrate%20that%20AFANet%20has%20achieved%0Astate-of-the-art%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/jarch-ma/AFANet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17601v1&entry.124074799=Read"},
{"title": "Variational Graph Generator for Multi-View Graph Clustering", "author": "Jianpeng Chen and Yawen Ling and Jie Xu and Yazhou Ren and Shudong Huang and Xiaorong Pu and Zhifeng Hao and Philip S. Yu and Lifang He", "abstract": "  Multi-view graph clustering (MGC) methods are increasingly being studied due\nto the explosion of multi-view data with graph structural information. The\ncritical point of MGC is to better utilize view-specific and view-common\ninformation in features and graphs of multiple views. However, existing works\nhave an inherent limitation that they are unable to concurrently utilize the\nconsensus graph information across multiple graphs and the view-specific\nfeature information. To address this issue, we propose Variational Graph\nGenerator for Multi-View Graph Clustering (VGMGC). Specifically, a novel\nvariational graph generator is proposed to extract common information among\nmultiple graphs. This generator infers a reliable variational consensus graph\nbased on a priori assumption over multiple graphs. Then a simple yet effective\ngraph encoder in conjunction with the multi-view clustering objective is\npresented to learn the desired graph embeddings for clustering, which embeds\nthe inferred view-common graph and view-specific graphs together with features.\nFinally, theoretical results illustrate the rationality of the VGMGC by\nanalyzing the uncertainty of the inferred consensus graph with the information\nbottleneck principle.Extensive experiments demonstrate the superior performance\nof our VGMGC over SOTAs. The source code is publicly available at\nhttps://github.com/cjpcool/VGMGC.\n", "link": "http://arxiv.org/abs/2210.07011v3", "date": "2024-12-23", "relevancy": 2.554, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5172}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5154}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Graph%20Generator%20for%20Multi-View%20Graph%20Clustering&body=Title%3A%20Variational%20Graph%20Generator%20for%20Multi-View%20Graph%20Clustering%0AAuthor%3A%20Jianpeng%20Chen%20and%20Yawen%20Ling%20and%20Jie%20Xu%20and%20Yazhou%20Ren%20and%20Shudong%20Huang%20and%20Xiaorong%20Pu%20and%20Zhifeng%20Hao%20and%20Philip%20S.%20Yu%20and%20Lifang%20He%0AAbstract%3A%20%20%20Multi-view%20graph%20clustering%20%28MGC%29%20methods%20are%20increasingly%20being%20studied%20due%0Ato%20the%20explosion%20of%20multi-view%20data%20with%20graph%20structural%20information.%20The%0Acritical%20point%20of%20MGC%20is%20to%20better%20utilize%20view-specific%20and%20view-common%0Ainformation%20in%20features%20and%20graphs%20of%20multiple%20views.%20However%2C%20existing%20works%0Ahave%20an%20inherent%20limitation%20that%20they%20are%20unable%20to%20concurrently%20utilize%20the%0Aconsensus%20graph%20information%20across%20multiple%20graphs%20and%20the%20view-specific%0Afeature%20information.%20To%20address%20this%20issue%2C%20we%20propose%20Variational%20Graph%0AGenerator%20for%20Multi-View%20Graph%20Clustering%20%28VGMGC%29.%20Specifically%2C%20a%20novel%0Avariational%20graph%20generator%20is%20proposed%20to%20extract%20common%20information%20among%0Amultiple%20graphs.%20This%20generator%20infers%20a%20reliable%20variational%20consensus%20graph%0Abased%20on%20a%20priori%20assumption%20over%20multiple%20graphs.%20Then%20a%20simple%20yet%20effective%0Agraph%20encoder%20in%20conjunction%20with%20the%20multi-view%20clustering%20objective%20is%0Apresented%20to%20learn%20the%20desired%20graph%20embeddings%20for%20clustering%2C%20which%20embeds%0Athe%20inferred%20view-common%20graph%20and%20view-specific%20graphs%20together%20with%20features.%0AFinally%2C%20theoretical%20results%20illustrate%20the%20rationality%20of%20the%20VGMGC%20by%0Aanalyzing%20the%20uncertainty%20of%20the%20inferred%20consensus%20graph%20with%20the%20information%0Abottleneck%20principle.Extensive%20experiments%20demonstrate%20the%20superior%20performance%0Aof%20our%20VGMGC%20over%20SOTAs.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/cjpcool/VGMGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.07011v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Graph%2520Generator%2520for%2520Multi-View%2520Graph%2520Clustering%26entry.906535625%3DJianpeng%2520Chen%2520and%2520Yawen%2520Ling%2520and%2520Jie%2520Xu%2520and%2520Yazhou%2520Ren%2520and%2520Shudong%2520Huang%2520and%2520Xiaorong%2520Pu%2520and%2520Zhifeng%2520Hao%2520and%2520Philip%2520S.%2520Yu%2520and%2520Lifang%2520He%26entry.1292438233%3D%2520%2520Multi-view%2520graph%2520clustering%2520%2528MGC%2529%2520methods%2520are%2520increasingly%2520being%2520studied%2520due%250Ato%2520the%2520explosion%2520of%2520multi-view%2520data%2520with%2520graph%2520structural%2520information.%2520The%250Acritical%2520point%2520of%2520MGC%2520is%2520to%2520better%2520utilize%2520view-specific%2520and%2520view-common%250Ainformation%2520in%2520features%2520and%2520graphs%2520of%2520multiple%2520views.%2520However%252C%2520existing%2520works%250Ahave%2520an%2520inherent%2520limitation%2520that%2520they%2520are%2520unable%2520to%2520concurrently%2520utilize%2520the%250Aconsensus%2520graph%2520information%2520across%2520multiple%2520graphs%2520and%2520the%2520view-specific%250Afeature%2520information.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Variational%2520Graph%250AGenerator%2520for%2520Multi-View%2520Graph%2520Clustering%2520%2528VGMGC%2529.%2520Specifically%252C%2520a%2520novel%250Avariational%2520graph%2520generator%2520is%2520proposed%2520to%2520extract%2520common%2520information%2520among%250Amultiple%2520graphs.%2520This%2520generator%2520infers%2520a%2520reliable%2520variational%2520consensus%2520graph%250Abased%2520on%2520a%2520priori%2520assumption%2520over%2520multiple%2520graphs.%2520Then%2520a%2520simple%2520yet%2520effective%250Agraph%2520encoder%2520in%2520conjunction%2520with%2520the%2520multi-view%2520clustering%2520objective%2520is%250Apresented%2520to%2520learn%2520the%2520desired%2520graph%2520embeddings%2520for%2520clustering%252C%2520which%2520embeds%250Athe%2520inferred%2520view-common%2520graph%2520and%2520view-specific%2520graphs%2520together%2520with%2520features.%250AFinally%252C%2520theoretical%2520results%2520illustrate%2520the%2520rationality%2520of%2520the%2520VGMGC%2520by%250Aanalyzing%2520the%2520uncertainty%2520of%2520the%2520inferred%2520consensus%2520graph%2520with%2520the%2520information%250Abottleneck%2520principle.Extensive%2520experiments%2520demonstrate%2520the%2520superior%2520performance%250Aof%2520our%2520VGMGC%2520over%2520SOTAs.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/cjpcool/VGMGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.07011v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Graph%20Generator%20for%20Multi-View%20Graph%20Clustering&entry.906535625=Jianpeng%20Chen%20and%20Yawen%20Ling%20and%20Jie%20Xu%20and%20Yazhou%20Ren%20and%20Shudong%20Huang%20and%20Xiaorong%20Pu%20and%20Zhifeng%20Hao%20and%20Philip%20S.%20Yu%20and%20Lifang%20He&entry.1292438233=%20%20Multi-view%20graph%20clustering%20%28MGC%29%20methods%20are%20increasingly%20being%20studied%20due%0Ato%20the%20explosion%20of%20multi-view%20data%20with%20graph%20structural%20information.%20The%0Acritical%20point%20of%20MGC%20is%20to%20better%20utilize%20view-specific%20and%20view-common%0Ainformation%20in%20features%20and%20graphs%20of%20multiple%20views.%20However%2C%20existing%20works%0Ahave%20an%20inherent%20limitation%20that%20they%20are%20unable%20to%20concurrently%20utilize%20the%0Aconsensus%20graph%20information%20across%20multiple%20graphs%20and%20the%20view-specific%0Afeature%20information.%20To%20address%20this%20issue%2C%20we%20propose%20Variational%20Graph%0AGenerator%20for%20Multi-View%20Graph%20Clustering%20%28VGMGC%29.%20Specifically%2C%20a%20novel%0Avariational%20graph%20generator%20is%20proposed%20to%20extract%20common%20information%20among%0Amultiple%20graphs.%20This%20generator%20infers%20a%20reliable%20variational%20consensus%20graph%0Abased%20on%20a%20priori%20assumption%20over%20multiple%20graphs.%20Then%20a%20simple%20yet%20effective%0Agraph%20encoder%20in%20conjunction%20with%20the%20multi-view%20clustering%20objective%20is%0Apresented%20to%20learn%20the%20desired%20graph%20embeddings%20for%20clustering%2C%20which%20embeds%0Athe%20inferred%20view-common%20graph%20and%20view-specific%20graphs%20together%20with%20features.%0AFinally%2C%20theoretical%20results%20illustrate%20the%20rationality%20of%20the%20VGMGC%20by%0Aanalyzing%20the%20uncertainty%20of%20the%20inferred%20consensus%20graph%20with%20the%20information%0Abottleneck%20principle.Extensive%20experiments%20demonstrate%20the%20superior%20performance%0Aof%20our%20VGMGC%20over%20SOTAs.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/cjpcool/VGMGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.07011v3&entry.124074799=Read"},
{"title": "COBRA: COmBinatorial Retrieval Augmentation for Few-Shot Learning", "author": "Arnav M. Das and Gantavya Bhatt and Lilly Kumari and Sahil Verma and Jeff Bilmes", "abstract": "  Retrieval augmentation, the practice of retrieving additional data from large\nauxiliary pools, has emerged as an effective technique for enhancing model\nperformance in the low-data regime, e.g. few-shot learning. Prior approaches\nhave employed only nearest-neighbor based strategies for data selection, which\nretrieve auxiliary samples with high similarity to instances in the target\ntask. However, these approaches are prone to selecting highly redundant\nsamples, since they fail to incorporate any notion of diversity. In our work,\nwe first demonstrate that data selection strategies used in prior\nretrieval-augmented few-shot learning settings can be generalized using a class\nof functions known as Combinatorial Mutual Information (CMI) measures. We then\npropose COBRA (COmBinatorial Retrieval Augmentation), which employs an\nalternative CMI measure that considers both diversity and similarity to a\ntarget dataset. COBRA consistently outperforms previous retrieval approaches\nacross image classification tasks and few-shot learning techniques when used to\nretrieve samples from LAION-2B. COBRA introduces negligible computational\noverhead to the cost of retrieval while providing significant gains in\ndownstream model performance.\n", "link": "http://arxiv.org/abs/2412.17684v1", "date": "2024-12-23", "relevancy": 2.5475, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COBRA%3A%20COmBinatorial%20Retrieval%20Augmentation%20for%20Few-Shot%20Learning&body=Title%3A%20COBRA%3A%20COmBinatorial%20Retrieval%20Augmentation%20for%20Few-Shot%20Learning%0AAuthor%3A%20Arnav%20M.%20Das%20and%20Gantavya%20Bhatt%20and%20Lilly%20Kumari%20and%20Sahil%20Verma%20and%20Jeff%20Bilmes%0AAbstract%3A%20%20%20Retrieval%20augmentation%2C%20the%20practice%20of%20retrieving%20additional%20data%20from%20large%0Aauxiliary%20pools%2C%20has%20emerged%20as%20an%20effective%20technique%20for%20enhancing%20model%0Aperformance%20in%20the%20low-data%20regime%2C%20e.g.%20few-shot%20learning.%20Prior%20approaches%0Ahave%20employed%20only%20nearest-neighbor%20based%20strategies%20for%20data%20selection%2C%20which%0Aretrieve%20auxiliary%20samples%20with%20high%20similarity%20to%20instances%20in%20the%20target%0Atask.%20However%2C%20these%20approaches%20are%20prone%20to%20selecting%20highly%20redundant%0Asamples%2C%20since%20they%20fail%20to%20incorporate%20any%20notion%20of%20diversity.%20In%20our%20work%2C%0Awe%20first%20demonstrate%20that%20data%20selection%20strategies%20used%20in%20prior%0Aretrieval-augmented%20few-shot%20learning%20settings%20can%20be%20generalized%20using%20a%20class%0Aof%20functions%20known%20as%20Combinatorial%20Mutual%20Information%20%28CMI%29%20measures.%20We%20then%0Apropose%20COBRA%20%28COmBinatorial%20Retrieval%20Augmentation%29%2C%20which%20employs%20an%0Aalternative%20CMI%20measure%20that%20considers%20both%20diversity%20and%20similarity%20to%20a%0Atarget%20dataset.%20COBRA%20consistently%20outperforms%20previous%20retrieval%20approaches%0Aacross%20image%20classification%20tasks%20and%20few-shot%20learning%20techniques%20when%20used%20to%0Aretrieve%20samples%20from%20LAION-2B.%20COBRA%20introduces%20negligible%20computational%0Aoverhead%20to%20the%20cost%20of%20retrieval%20while%20providing%20significant%20gains%20in%0Adownstream%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOBRA%253A%2520COmBinatorial%2520Retrieval%2520Augmentation%2520for%2520Few-Shot%2520Learning%26entry.906535625%3DArnav%2520M.%2520Das%2520and%2520Gantavya%2520Bhatt%2520and%2520Lilly%2520Kumari%2520and%2520Sahil%2520Verma%2520and%2520Jeff%2520Bilmes%26entry.1292438233%3D%2520%2520Retrieval%2520augmentation%252C%2520the%2520practice%2520of%2520retrieving%2520additional%2520data%2520from%2520large%250Aauxiliary%2520pools%252C%2520has%2520emerged%2520as%2520an%2520effective%2520technique%2520for%2520enhancing%2520model%250Aperformance%2520in%2520the%2520low-data%2520regime%252C%2520e.g.%2520few-shot%2520learning.%2520Prior%2520approaches%250Ahave%2520employed%2520only%2520nearest-neighbor%2520based%2520strategies%2520for%2520data%2520selection%252C%2520which%250Aretrieve%2520auxiliary%2520samples%2520with%2520high%2520similarity%2520to%2520instances%2520in%2520the%2520target%250Atask.%2520However%252C%2520these%2520approaches%2520are%2520prone%2520to%2520selecting%2520highly%2520redundant%250Asamples%252C%2520since%2520they%2520fail%2520to%2520incorporate%2520any%2520notion%2520of%2520diversity.%2520In%2520our%2520work%252C%250Awe%2520first%2520demonstrate%2520that%2520data%2520selection%2520strategies%2520used%2520in%2520prior%250Aretrieval-augmented%2520few-shot%2520learning%2520settings%2520can%2520be%2520generalized%2520using%2520a%2520class%250Aof%2520functions%2520known%2520as%2520Combinatorial%2520Mutual%2520Information%2520%2528CMI%2529%2520measures.%2520We%2520then%250Apropose%2520COBRA%2520%2528COmBinatorial%2520Retrieval%2520Augmentation%2529%252C%2520which%2520employs%2520an%250Aalternative%2520CMI%2520measure%2520that%2520considers%2520both%2520diversity%2520and%2520similarity%2520to%2520a%250Atarget%2520dataset.%2520COBRA%2520consistently%2520outperforms%2520previous%2520retrieval%2520approaches%250Aacross%2520image%2520classification%2520tasks%2520and%2520few-shot%2520learning%2520techniques%2520when%2520used%2520to%250Aretrieve%2520samples%2520from%2520LAION-2B.%2520COBRA%2520introduces%2520negligible%2520computational%250Aoverhead%2520to%2520the%2520cost%2520of%2520retrieval%2520while%2520providing%2520significant%2520gains%2520in%250Adownstream%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COBRA%3A%20COmBinatorial%20Retrieval%20Augmentation%20for%20Few-Shot%20Learning&entry.906535625=Arnav%20M.%20Das%20and%20Gantavya%20Bhatt%20and%20Lilly%20Kumari%20and%20Sahil%20Verma%20and%20Jeff%20Bilmes&entry.1292438233=%20%20Retrieval%20augmentation%2C%20the%20practice%20of%20retrieving%20additional%20data%20from%20large%0Aauxiliary%20pools%2C%20has%20emerged%20as%20an%20effective%20technique%20for%20enhancing%20model%0Aperformance%20in%20the%20low-data%20regime%2C%20e.g.%20few-shot%20learning.%20Prior%20approaches%0Ahave%20employed%20only%20nearest-neighbor%20based%20strategies%20for%20data%20selection%2C%20which%0Aretrieve%20auxiliary%20samples%20with%20high%20similarity%20to%20instances%20in%20the%20target%0Atask.%20However%2C%20these%20approaches%20are%20prone%20to%20selecting%20highly%20redundant%0Asamples%2C%20since%20they%20fail%20to%20incorporate%20any%20notion%20of%20diversity.%20In%20our%20work%2C%0Awe%20first%20demonstrate%20that%20data%20selection%20strategies%20used%20in%20prior%0Aretrieval-augmented%20few-shot%20learning%20settings%20can%20be%20generalized%20using%20a%20class%0Aof%20functions%20known%20as%20Combinatorial%20Mutual%20Information%20%28CMI%29%20measures.%20We%20then%0Apropose%20COBRA%20%28COmBinatorial%20Retrieval%20Augmentation%29%2C%20which%20employs%20an%0Aalternative%20CMI%20measure%20that%20considers%20both%20diversity%20and%20similarity%20to%20a%0Atarget%20dataset.%20COBRA%20consistently%20outperforms%20previous%20retrieval%20approaches%0Aacross%20image%20classification%20tasks%20and%20few-shot%20learning%20techniques%20when%20used%20to%0Aretrieve%20samples%20from%20LAION-2B.%20COBRA%20introduces%20negligible%20computational%0Aoverhead%20to%20the%20cost%20of%20retrieval%20while%20providing%20significant%20gains%20in%0Adownstream%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17684v1&entry.124074799=Read"},
{"title": "Towards Foundation Models on Graphs: An Analysis on Cross-Dataset\n  Transfer of Pretrained GNNs", "author": "Fabrizio Frasca and Fabian Jogl and Moshe Eliasof and Matan Ostrovsky and Carola-Bibiane Sch\u00f6nlieb and Thomas G\u00e4rtner and Haggai Maron", "abstract": "  To develop a preliminary understanding towards Graph Foundation Models, we\nstudy the extent to which pretrained Graph Neural Networks can be applied\nacross datasets, an effort requiring to be agnostic to dataset-specific\nfeatures and their encodings. We build upon a purely structural pretraining\napproach and propose an extension to capture feature information while still\nbeing feature-agnostic. We evaluate pretrained models on downstream tasks for\nvarying amounts of training samples and choices of pretraining datasets. Our\npreliminary results indicate that embeddings from pretrained models improve\ngeneralization only with enough downstream data points and in a degree which\ndepends on the quantity and properties of pretraining data. Feature information\ncan lead to improvements, but currently requires some similarities between\npretraining and downstream feature spaces.\n", "link": "http://arxiv.org/abs/2412.17609v1", "date": "2024-12-23", "relevancy": 2.5106, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Foundation%20Models%20on%20Graphs%3A%20An%20Analysis%20on%20Cross-Dataset%0A%20%20Transfer%20of%20Pretrained%20GNNs&body=Title%3A%20Towards%20Foundation%20Models%20on%20Graphs%3A%20An%20Analysis%20on%20Cross-Dataset%0A%20%20Transfer%20of%20Pretrained%20GNNs%0AAuthor%3A%20Fabrizio%20Frasca%20and%20Fabian%20Jogl%20and%20Moshe%20Eliasof%20and%20Matan%20Ostrovsky%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Thomas%20G%C3%A4rtner%20and%20Haggai%20Maron%0AAbstract%3A%20%20%20To%20develop%20a%20preliminary%20understanding%20towards%20Graph%20Foundation%20Models%2C%20we%0Astudy%20the%20extent%20to%20which%20pretrained%20Graph%20Neural%20Networks%20can%20be%20applied%0Aacross%20datasets%2C%20an%20effort%20requiring%20to%20be%20agnostic%20to%20dataset-specific%0Afeatures%20and%20their%20encodings.%20We%20build%20upon%20a%20purely%20structural%20pretraining%0Aapproach%20and%20propose%20an%20extension%20to%20capture%20feature%20information%20while%20still%0Abeing%20feature-agnostic.%20We%20evaluate%20pretrained%20models%20on%20downstream%20tasks%20for%0Avarying%20amounts%20of%20training%20samples%20and%20choices%20of%20pretraining%20datasets.%20Our%0Apreliminary%20results%20indicate%20that%20embeddings%20from%20pretrained%20models%20improve%0Ageneralization%20only%20with%20enough%20downstream%20data%20points%20and%20in%20a%20degree%20which%0Adepends%20on%20the%20quantity%20and%20properties%20of%20pretraining%20data.%20Feature%20information%0Acan%20lead%20to%20improvements%2C%20but%20currently%20requires%20some%20similarities%20between%0Apretraining%20and%20downstream%20feature%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Foundation%2520Models%2520on%2520Graphs%253A%2520An%2520Analysis%2520on%2520Cross-Dataset%250A%2520%2520Transfer%2520of%2520Pretrained%2520GNNs%26entry.906535625%3DFabrizio%2520Frasca%2520and%2520Fabian%2520Jogl%2520and%2520Moshe%2520Eliasof%2520and%2520Matan%2520Ostrovsky%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Thomas%2520G%25C3%25A4rtner%2520and%2520Haggai%2520Maron%26entry.1292438233%3D%2520%2520To%2520develop%2520a%2520preliminary%2520understanding%2520towards%2520Graph%2520Foundation%2520Models%252C%2520we%250Astudy%2520the%2520extent%2520to%2520which%2520pretrained%2520Graph%2520Neural%2520Networks%2520can%2520be%2520applied%250Aacross%2520datasets%252C%2520an%2520effort%2520requiring%2520to%2520be%2520agnostic%2520to%2520dataset-specific%250Afeatures%2520and%2520their%2520encodings.%2520We%2520build%2520upon%2520a%2520purely%2520structural%2520pretraining%250Aapproach%2520and%2520propose%2520an%2520extension%2520to%2520capture%2520feature%2520information%2520while%2520still%250Abeing%2520feature-agnostic.%2520We%2520evaluate%2520pretrained%2520models%2520on%2520downstream%2520tasks%2520for%250Avarying%2520amounts%2520of%2520training%2520samples%2520and%2520choices%2520of%2520pretraining%2520datasets.%2520Our%250Apreliminary%2520results%2520indicate%2520that%2520embeddings%2520from%2520pretrained%2520models%2520improve%250Ageneralization%2520only%2520with%2520enough%2520downstream%2520data%2520points%2520and%2520in%2520a%2520degree%2520which%250Adepends%2520on%2520the%2520quantity%2520and%2520properties%2520of%2520pretraining%2520data.%2520Feature%2520information%250Acan%2520lead%2520to%2520improvements%252C%2520but%2520currently%2520requires%2520some%2520similarities%2520between%250Apretraining%2520and%2520downstream%2520feature%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Foundation%20Models%20on%20Graphs%3A%20An%20Analysis%20on%20Cross-Dataset%0A%20%20Transfer%20of%20Pretrained%20GNNs&entry.906535625=Fabrizio%20Frasca%20and%20Fabian%20Jogl%20and%20Moshe%20Eliasof%20and%20Matan%20Ostrovsky%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Thomas%20G%C3%A4rtner%20and%20Haggai%20Maron&entry.1292438233=%20%20To%20develop%20a%20preliminary%20understanding%20towards%20Graph%20Foundation%20Models%2C%20we%0Astudy%20the%20extent%20to%20which%20pretrained%20Graph%20Neural%20Networks%20can%20be%20applied%0Aacross%20datasets%2C%20an%20effort%20requiring%20to%20be%20agnostic%20to%20dataset-specific%0Afeatures%20and%20their%20encodings.%20We%20build%20upon%20a%20purely%20structural%20pretraining%0Aapproach%20and%20propose%20an%20extension%20to%20capture%20feature%20information%20while%20still%0Abeing%20feature-agnostic.%20We%20evaluate%20pretrained%20models%20on%20downstream%20tasks%20for%0Avarying%20amounts%20of%20training%20samples%20and%20choices%20of%20pretraining%20datasets.%20Our%0Apreliminary%20results%20indicate%20that%20embeddings%20from%20pretrained%20models%20improve%0Ageneralization%20only%20with%20enough%20downstream%20data%20points%20and%20in%20a%20degree%20which%0Adepends%20on%20the%20quantity%20and%20properties%20of%20pretraining%20data.%20Feature%20information%0Acan%20lead%20to%20improvements%2C%20but%20currently%20requires%20some%20similarities%20between%0Apretraining%20and%20downstream%20feature%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17609v1&entry.124074799=Read"},
{"title": "Towards structure-preserving quantum encodings", "author": "Arthur J. Parzygnat and Tai-Danae Bradley and Andrew Vlasic and Anh Pham", "abstract": "  Harnessing the potential computational advantage of quantum computers for\nmachine learning tasks relies on the uploading of classical data onto quantum\ncomputers through what are commonly referred to as quantum encodings. The\nchoice of such encodings may vary substantially from one task to another, and\nthere exist only a few cases where structure has provided insight into their\ndesign and implementation, such as symmetry in geometric quantum learning.\nHere, we propose the perspective that category theory offers a natural\nmathematical framework for analyzing encodings that respect structure inherent\nin datasets and learning tasks. We illustrate this with pedagogical examples,\nwhich include geometric quantum machine learning, quantum metric learning,\ntopological data analysis, and more. Moreover, our perspective provides a\nlanguage in which to ask meaningful and mathematically precise questions for\nthe design of quantum encodings and circuits for quantum machine learning\ntasks.\n", "link": "http://arxiv.org/abs/2412.17772v1", "date": "2024-12-23", "relevancy": 2.508, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20structure-preserving%20quantum%20encodings&body=Title%3A%20Towards%20structure-preserving%20quantum%20encodings%0AAuthor%3A%20Arthur%20J.%20Parzygnat%20and%20Tai-Danae%20Bradley%20and%20Andrew%20Vlasic%20and%20Anh%20Pham%0AAbstract%3A%20%20%20Harnessing%20the%20potential%20computational%20advantage%20of%20quantum%20computers%20for%0Amachine%20learning%20tasks%20relies%20on%20the%20uploading%20of%20classical%20data%20onto%20quantum%0Acomputers%20through%20what%20are%20commonly%20referred%20to%20as%20quantum%20encodings.%20The%0Achoice%20of%20such%20encodings%20may%20vary%20substantially%20from%20one%20task%20to%20another%2C%20and%0Athere%20exist%20only%20a%20few%20cases%20where%20structure%20has%20provided%20insight%20into%20their%0Adesign%20and%20implementation%2C%20such%20as%20symmetry%20in%20geometric%20quantum%20learning.%0AHere%2C%20we%20propose%20the%20perspective%20that%20category%20theory%20offers%20a%20natural%0Amathematical%20framework%20for%20analyzing%20encodings%20that%20respect%20structure%20inherent%0Ain%20datasets%20and%20learning%20tasks.%20We%20illustrate%20this%20with%20pedagogical%20examples%2C%0Awhich%20include%20geometric%20quantum%20machine%20learning%2C%20quantum%20metric%20learning%2C%0Atopological%20data%20analysis%2C%20and%20more.%20Moreover%2C%20our%20perspective%20provides%20a%0Alanguage%20in%20which%20to%20ask%20meaningful%20and%20mathematically%20precise%20questions%20for%0Athe%20design%20of%20quantum%20encodings%20and%20circuits%20for%20quantum%20machine%20learning%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520structure-preserving%2520quantum%2520encodings%26entry.906535625%3DArthur%2520J.%2520Parzygnat%2520and%2520Tai-Danae%2520Bradley%2520and%2520Andrew%2520Vlasic%2520and%2520Anh%2520Pham%26entry.1292438233%3D%2520%2520Harnessing%2520the%2520potential%2520computational%2520advantage%2520of%2520quantum%2520computers%2520for%250Amachine%2520learning%2520tasks%2520relies%2520on%2520the%2520uploading%2520of%2520classical%2520data%2520onto%2520quantum%250Acomputers%2520through%2520what%2520are%2520commonly%2520referred%2520to%2520as%2520quantum%2520encodings.%2520The%250Achoice%2520of%2520such%2520encodings%2520may%2520vary%2520substantially%2520from%2520one%2520task%2520to%2520another%252C%2520and%250Athere%2520exist%2520only%2520a%2520few%2520cases%2520where%2520structure%2520has%2520provided%2520insight%2520into%2520their%250Adesign%2520and%2520implementation%252C%2520such%2520as%2520symmetry%2520in%2520geometric%2520quantum%2520learning.%250AHere%252C%2520we%2520propose%2520the%2520perspective%2520that%2520category%2520theory%2520offers%2520a%2520natural%250Amathematical%2520framework%2520for%2520analyzing%2520encodings%2520that%2520respect%2520structure%2520inherent%250Ain%2520datasets%2520and%2520learning%2520tasks.%2520We%2520illustrate%2520this%2520with%2520pedagogical%2520examples%252C%250Awhich%2520include%2520geometric%2520quantum%2520machine%2520learning%252C%2520quantum%2520metric%2520learning%252C%250Atopological%2520data%2520analysis%252C%2520and%2520more.%2520Moreover%252C%2520our%2520perspective%2520provides%2520a%250Alanguage%2520in%2520which%2520to%2520ask%2520meaningful%2520and%2520mathematically%2520precise%2520questions%2520for%250Athe%2520design%2520of%2520quantum%2520encodings%2520and%2520circuits%2520for%2520quantum%2520machine%2520learning%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20structure-preserving%20quantum%20encodings&entry.906535625=Arthur%20J.%20Parzygnat%20and%20Tai-Danae%20Bradley%20and%20Andrew%20Vlasic%20and%20Anh%20Pham&entry.1292438233=%20%20Harnessing%20the%20potential%20computational%20advantage%20of%20quantum%20computers%20for%0Amachine%20learning%20tasks%20relies%20on%20the%20uploading%20of%20classical%20data%20onto%20quantum%0Acomputers%20through%20what%20are%20commonly%20referred%20to%20as%20quantum%20encodings.%20The%0Achoice%20of%20such%20encodings%20may%20vary%20substantially%20from%20one%20task%20to%20another%2C%20and%0Athere%20exist%20only%20a%20few%20cases%20where%20structure%20has%20provided%20insight%20into%20their%0Adesign%20and%20implementation%2C%20such%20as%20symmetry%20in%20geometric%20quantum%20learning.%0AHere%2C%20we%20propose%20the%20perspective%20that%20category%20theory%20offers%20a%20natural%0Amathematical%20framework%20for%20analyzing%20encodings%20that%20respect%20structure%20inherent%0Ain%20datasets%20and%20learning%20tasks.%20We%20illustrate%20this%20with%20pedagogical%20examples%2C%0Awhich%20include%20geometric%20quantum%20machine%20learning%2C%20quantum%20metric%20learning%2C%0Atopological%20data%20analysis%2C%20and%20more.%20Moreover%2C%20our%20perspective%20provides%20a%0Alanguage%20in%20which%20to%20ask%20meaningful%20and%20mathematically%20precise%20questions%20for%0Athe%20design%20of%20quantum%20encodings%20and%20circuits%20for%20quantum%20machine%20learning%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17772v1&entry.124074799=Read"},
{"title": "Concept Discovery in Deep Neural Networks for Explainable Face\n  Anti-Spoofing", "author": "Haoyuan Zhang and Xiangyu Zhu and Li Gao and Jiawei Pan and Kai Pang and Guoying Zhao and Stan Z. Li and Zhen Lei", "abstract": "  With the rapid growth usage of face recognition in people's daily life, face\nanti-spoofing becomes increasingly important to avoid malicious attacks. Recent\nface anti-spoofing models can reach a high classification accuracy on multiple\ndatasets but these models can only tell people ``this face is fake'' while\nlacking the explanation to answer ``why it is fake''. Such a system undermines\ntrustworthiness and causes user confusion, as it denies their requests without\nproviding any explanations. In this paper, we incorporate XAI into face\nanti-spoofing and propose a new problem termed X-FAS (eXplainable Face\nAnti-Spoofing) empowering face anti-spoofing models to provide an explanation.\nWe propose SPED (SPoofing Evidence Discovery), an X-FAS method which can\ndiscover spoof concepts and provide reliable explanations on the basis of\ndiscovered concepts. To evaluate the quality of X-FAS methods, we propose an\nX-FAS benchmark with annotated spoofing evidence by experts. We analyze SPED\nexplanations on face anti-spoofing dataset and compare SPED quantitatively and\nqualitatively with previous XAI methods on proposed X-FAS benchmark.\nExperimental results demonstrate SPED's ability to generate reliable\nexplanations.\n", "link": "http://arxiv.org/abs/2412.17541v1", "date": "2024-12-23", "relevancy": 2.4918, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5042}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20Discovery%20in%20Deep%20Neural%20Networks%20for%20Explainable%20Face%0A%20%20Anti-Spoofing&body=Title%3A%20Concept%20Discovery%20in%20Deep%20Neural%20Networks%20for%20Explainable%20Face%0A%20%20Anti-Spoofing%0AAuthor%3A%20Haoyuan%20Zhang%20and%20Xiangyu%20Zhu%20and%20Li%20Gao%20and%20Jiawei%20Pan%20and%20Kai%20Pang%20and%20Guoying%20Zhao%20and%20Stan%20Z.%20Li%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20With%20the%20rapid%20growth%20usage%20of%20face%20recognition%20in%20people%27s%20daily%20life%2C%20face%0Aanti-spoofing%20becomes%20increasingly%20important%20to%20avoid%20malicious%20attacks.%20Recent%0Aface%20anti-spoofing%20models%20can%20reach%20a%20high%20classification%20accuracy%20on%20multiple%0Adatasets%20but%20these%20models%20can%20only%20tell%20people%20%60%60this%20face%20is%20fake%27%27%20while%0Alacking%20the%20explanation%20to%20answer%20%60%60why%20it%20is%20fake%27%27.%20Such%20a%20system%20undermines%0Atrustworthiness%20and%20causes%20user%20confusion%2C%20as%20it%20denies%20their%20requests%20without%0Aproviding%20any%20explanations.%20In%20this%20paper%2C%20we%20incorporate%20XAI%20into%20face%0Aanti-spoofing%20and%20propose%20a%20new%20problem%20termed%20X-FAS%20%28eXplainable%20Face%0AAnti-Spoofing%29%20empowering%20face%20anti-spoofing%20models%20to%20provide%20an%20explanation.%0AWe%20propose%20SPED%20%28SPoofing%20Evidence%20Discovery%29%2C%20an%20X-FAS%20method%20which%20can%0Adiscover%20spoof%20concepts%20and%20provide%20reliable%20explanations%20on%20the%20basis%20of%0Adiscovered%20concepts.%20To%20evaluate%20the%20quality%20of%20X-FAS%20methods%2C%20we%20propose%20an%0AX-FAS%20benchmark%20with%20annotated%20spoofing%20evidence%20by%20experts.%20We%20analyze%20SPED%0Aexplanations%20on%20face%20anti-spoofing%20dataset%20and%20compare%20SPED%20quantitatively%20and%0Aqualitatively%20with%20previous%20XAI%20methods%20on%20proposed%20X-FAS%20benchmark.%0AExperimental%20results%20demonstrate%20SPED%27s%20ability%20to%20generate%20reliable%0Aexplanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520Discovery%2520in%2520Deep%2520Neural%2520Networks%2520for%2520Explainable%2520Face%250A%2520%2520Anti-Spoofing%26entry.906535625%3DHaoyuan%2520Zhang%2520and%2520Xiangyu%2520Zhu%2520and%2520Li%2520Gao%2520and%2520Jiawei%2520Pan%2520and%2520Kai%2520Pang%2520and%2520Guoying%2520Zhao%2520and%2520Stan%2520Z.%2520Li%2520and%2520Zhen%2520Lei%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520growth%2520usage%2520of%2520face%2520recognition%2520in%2520people%2527s%2520daily%2520life%252C%2520face%250Aanti-spoofing%2520becomes%2520increasingly%2520important%2520to%2520avoid%2520malicious%2520attacks.%2520Recent%250Aface%2520anti-spoofing%2520models%2520can%2520reach%2520a%2520high%2520classification%2520accuracy%2520on%2520multiple%250Adatasets%2520but%2520these%2520models%2520can%2520only%2520tell%2520people%2520%2560%2560this%2520face%2520is%2520fake%2527%2527%2520while%250Alacking%2520the%2520explanation%2520to%2520answer%2520%2560%2560why%2520it%2520is%2520fake%2527%2527.%2520Such%2520a%2520system%2520undermines%250Atrustworthiness%2520and%2520causes%2520user%2520confusion%252C%2520as%2520it%2520denies%2520their%2520requests%2520without%250Aproviding%2520any%2520explanations.%2520In%2520this%2520paper%252C%2520we%2520incorporate%2520XAI%2520into%2520face%250Aanti-spoofing%2520and%2520propose%2520a%2520new%2520problem%2520termed%2520X-FAS%2520%2528eXplainable%2520Face%250AAnti-Spoofing%2529%2520empowering%2520face%2520anti-spoofing%2520models%2520to%2520provide%2520an%2520explanation.%250AWe%2520propose%2520SPED%2520%2528SPoofing%2520Evidence%2520Discovery%2529%252C%2520an%2520X-FAS%2520method%2520which%2520can%250Adiscover%2520spoof%2520concepts%2520and%2520provide%2520reliable%2520explanations%2520on%2520the%2520basis%2520of%250Adiscovered%2520concepts.%2520To%2520evaluate%2520the%2520quality%2520of%2520X-FAS%2520methods%252C%2520we%2520propose%2520an%250AX-FAS%2520benchmark%2520with%2520annotated%2520spoofing%2520evidence%2520by%2520experts.%2520We%2520analyze%2520SPED%250Aexplanations%2520on%2520face%2520anti-spoofing%2520dataset%2520and%2520compare%2520SPED%2520quantitatively%2520and%250Aqualitatively%2520with%2520previous%2520XAI%2520methods%2520on%2520proposed%2520X-FAS%2520benchmark.%250AExperimental%2520results%2520demonstrate%2520SPED%2527s%2520ability%2520to%2520generate%2520reliable%250Aexplanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Discovery%20in%20Deep%20Neural%20Networks%20for%20Explainable%20Face%0A%20%20Anti-Spoofing&entry.906535625=Haoyuan%20Zhang%20and%20Xiangyu%20Zhu%20and%20Li%20Gao%20and%20Jiawei%20Pan%20and%20Kai%20Pang%20and%20Guoying%20Zhao%20and%20Stan%20Z.%20Li%20and%20Zhen%20Lei&entry.1292438233=%20%20With%20the%20rapid%20growth%20usage%20of%20face%20recognition%20in%20people%27s%20daily%20life%2C%20face%0Aanti-spoofing%20becomes%20increasingly%20important%20to%20avoid%20malicious%20attacks.%20Recent%0Aface%20anti-spoofing%20models%20can%20reach%20a%20high%20classification%20accuracy%20on%20multiple%0Adatasets%20but%20these%20models%20can%20only%20tell%20people%20%60%60this%20face%20is%20fake%27%27%20while%0Alacking%20the%20explanation%20to%20answer%20%60%60why%20it%20is%20fake%27%27.%20Such%20a%20system%20undermines%0Atrustworthiness%20and%20causes%20user%20confusion%2C%20as%20it%20denies%20their%20requests%20without%0Aproviding%20any%20explanations.%20In%20this%20paper%2C%20we%20incorporate%20XAI%20into%20face%0Aanti-spoofing%20and%20propose%20a%20new%20problem%20termed%20X-FAS%20%28eXplainable%20Face%0AAnti-Spoofing%29%20empowering%20face%20anti-spoofing%20models%20to%20provide%20an%20explanation.%0AWe%20propose%20SPED%20%28SPoofing%20Evidence%20Discovery%29%2C%20an%20X-FAS%20method%20which%20can%0Adiscover%20spoof%20concepts%20and%20provide%20reliable%20explanations%20on%20the%20basis%20of%0Adiscovered%20concepts.%20To%20evaluate%20the%20quality%20of%20X-FAS%20methods%2C%20we%20propose%20an%0AX-FAS%20benchmark%20with%20annotated%20spoofing%20evidence%20by%20experts.%20We%20analyze%20SPED%0Aexplanations%20on%20face%20anti-spoofing%20dataset%20and%20compare%20SPED%20quantitatively%20and%0Aqualitatively%20with%20previous%20XAI%20methods%20on%20proposed%20X-FAS%20benchmark.%0AExperimental%20results%20demonstrate%20SPED%27s%20ability%20to%20generate%20reliable%0Aexplanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17541v1&entry.124074799=Read"},
{"title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for\n  Length Generalization", "author": "Ermo Hua and Che Jiang and Xingtai Lv and Kaiyan Zhang and Ning Ding and Youbang Sun and Biqing Qi and Yuchen Fan and Xue Kai Zhu and Bowen Zhou", "abstract": "  Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While existing works mainly\naddress RoPE's limitations within attention mechanism, this paper provides an\nanalysis across nearly all parts of LMs, uncovering their adverse effects on\nlength generalization for RoPE-based attention. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectral damage caused by: 1) linear layers and activation\nfunctions outside of attention; 2) insufficiently trained frequency components\nbrought by time-domain truncation. Building on our observations, we propose\nFourier Position Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs Fourier Series and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales show that, within varying context\nwindows, FoPE can maintain a more stable perplexity and a more consistent\naccuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several\nanalyses and ablations bring further support to our method and theoretical\nmodeling.\n", "link": "http://arxiv.org/abs/2412.17739v1", "date": "2024-12-23", "relevancy": 2.473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier%20Position%20Embedding%3A%20Enhancing%20Attention%27s%20Periodic%20Extension%20for%0A%20%20Length%20Generalization&body=Title%3A%20Fourier%20Position%20Embedding%3A%20Enhancing%20Attention%27s%20Periodic%20Extension%20for%0A%20%20Length%20Generalization%0AAuthor%3A%20Ermo%20Hua%20and%20Che%20Jiang%20and%20Xingtai%20Lv%20and%20Kaiyan%20Zhang%20and%20Ning%20Ding%20and%20Youbang%20Sun%20and%20Biqing%20Qi%20and%20Yuchen%20Fan%20and%20Xue%20Kai%20Zhu%20and%20Bowen%20Zhou%0AAbstract%3A%20%20%20Extending%20the%20context%20length%20of%20Language%20Models%20%28LMs%29%20by%20improving%20Rotary%0APosition%20Embedding%20%28RoPE%29%20has%20become%20a%20trend.%20While%20existing%20works%20mainly%0Aaddress%20RoPE%27s%20limitations%20within%20attention%20mechanism%2C%20this%20paper%20provides%20an%0Aanalysis%20across%20nearly%20all%20parts%20of%20LMs%2C%20uncovering%20their%20adverse%20effects%20on%0Alength%20generalization%20for%20RoPE-based%20attention.%20Using%20Discrete%20Signal%0AProcessing%20theory%2C%20we%20show%20that%20RoPE%20enables%20periodic%20attention%20by%20implicitly%0Aachieving%20Non-Uniform%20Discrete%20Fourier%20Transform.%20However%2C%20this%20periodicity%20is%0Aundermined%20by%20the%20spectral%20damage%20caused%20by%3A%201%29%20linear%20layers%20and%20activation%0Afunctions%20outside%20of%20attention%3B%202%29%20insufficiently%20trained%20frequency%20components%0Abrought%20by%20time-domain%20truncation.%20Building%20on%20our%20observations%2C%20we%20propose%0AFourier%20Position%20Embedding%20%28FoPE%29%2C%20which%20enhances%20attention%27s%20frequency-domain%0Aproperties%20to%20improve%20both%20its%20periodic%20extension%20and%20length%20generalization.%0AFoPE%20constructs%20Fourier%20Series%20and%20zero-outs%20the%20destructive%20frequency%0Acomponents%2C%20increasing%20model%20robustness%20against%20the%20spectrum%20damage.%0AExperiments%20across%20various%20model%20scales%20show%20that%2C%20within%20varying%20context%0Awindows%2C%20FoPE%20can%20maintain%20a%20more%20stable%20perplexity%20and%20a%20more%20consistent%0Aaccuracy%20in%20a%20needle-in-haystack%20task%20compared%20to%20RoPE%20and%20ALiBi.%20Several%0Aanalyses%20and%20ablations%20bring%20further%20support%20to%20our%20method%20and%20theoretical%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier%2520Position%2520Embedding%253A%2520Enhancing%2520Attention%2527s%2520Periodic%2520Extension%2520for%250A%2520%2520Length%2520Generalization%26entry.906535625%3DErmo%2520Hua%2520and%2520Che%2520Jiang%2520and%2520Xingtai%2520Lv%2520and%2520Kaiyan%2520Zhang%2520and%2520Ning%2520Ding%2520and%2520Youbang%2520Sun%2520and%2520Biqing%2520Qi%2520and%2520Yuchen%2520Fan%2520and%2520Xue%2520Kai%2520Zhu%2520and%2520Bowen%2520Zhou%26entry.1292438233%3D%2520%2520Extending%2520the%2520context%2520length%2520of%2520Language%2520Models%2520%2528LMs%2529%2520by%2520improving%2520Rotary%250APosition%2520Embedding%2520%2528RoPE%2529%2520has%2520become%2520a%2520trend.%2520While%2520existing%2520works%2520mainly%250Aaddress%2520RoPE%2527s%2520limitations%2520within%2520attention%2520mechanism%252C%2520this%2520paper%2520provides%2520an%250Aanalysis%2520across%2520nearly%2520all%2520parts%2520of%2520LMs%252C%2520uncovering%2520their%2520adverse%2520effects%2520on%250Alength%2520generalization%2520for%2520RoPE-based%2520attention.%2520Using%2520Discrete%2520Signal%250AProcessing%2520theory%252C%2520we%2520show%2520that%2520RoPE%2520enables%2520periodic%2520attention%2520by%2520implicitly%250Aachieving%2520Non-Uniform%2520Discrete%2520Fourier%2520Transform.%2520However%252C%2520this%2520periodicity%2520is%250Aundermined%2520by%2520the%2520spectral%2520damage%2520caused%2520by%253A%25201%2529%2520linear%2520layers%2520and%2520activation%250Afunctions%2520outside%2520of%2520attention%253B%25202%2529%2520insufficiently%2520trained%2520frequency%2520components%250Abrought%2520by%2520time-domain%2520truncation.%2520Building%2520on%2520our%2520observations%252C%2520we%2520propose%250AFourier%2520Position%2520Embedding%2520%2528FoPE%2529%252C%2520which%2520enhances%2520attention%2527s%2520frequency-domain%250Aproperties%2520to%2520improve%2520both%2520its%2520periodic%2520extension%2520and%2520length%2520generalization.%250AFoPE%2520constructs%2520Fourier%2520Series%2520and%2520zero-outs%2520the%2520destructive%2520frequency%250Acomponents%252C%2520increasing%2520model%2520robustness%2520against%2520the%2520spectrum%2520damage.%250AExperiments%2520across%2520various%2520model%2520scales%2520show%2520that%252C%2520within%2520varying%2520context%250Awindows%252C%2520FoPE%2520can%2520maintain%2520a%2520more%2520stable%2520perplexity%2520and%2520a%2520more%2520consistent%250Aaccuracy%2520in%2520a%2520needle-in-haystack%2520task%2520compared%2520to%2520RoPE%2520and%2520ALiBi.%2520Several%250Aanalyses%2520and%2520ablations%2520bring%2520further%2520support%2520to%2520our%2520method%2520and%2520theoretical%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier%20Position%20Embedding%3A%20Enhancing%20Attention%27s%20Periodic%20Extension%20for%0A%20%20Length%20Generalization&entry.906535625=Ermo%20Hua%20and%20Che%20Jiang%20and%20Xingtai%20Lv%20and%20Kaiyan%20Zhang%20and%20Ning%20Ding%20and%20Youbang%20Sun%20and%20Biqing%20Qi%20and%20Yuchen%20Fan%20and%20Xue%20Kai%20Zhu%20and%20Bowen%20Zhou&entry.1292438233=%20%20Extending%20the%20context%20length%20of%20Language%20Models%20%28LMs%29%20by%20improving%20Rotary%0APosition%20Embedding%20%28RoPE%29%20has%20become%20a%20trend.%20While%20existing%20works%20mainly%0Aaddress%20RoPE%27s%20limitations%20within%20attention%20mechanism%2C%20this%20paper%20provides%20an%0Aanalysis%20across%20nearly%20all%20parts%20of%20LMs%2C%20uncovering%20their%20adverse%20effects%20on%0Alength%20generalization%20for%20RoPE-based%20attention.%20Using%20Discrete%20Signal%0AProcessing%20theory%2C%20we%20show%20that%20RoPE%20enables%20periodic%20attention%20by%20implicitly%0Aachieving%20Non-Uniform%20Discrete%20Fourier%20Transform.%20However%2C%20this%20periodicity%20is%0Aundermined%20by%20the%20spectral%20damage%20caused%20by%3A%201%29%20linear%20layers%20and%20activation%0Afunctions%20outside%20of%20attention%3B%202%29%20insufficiently%20trained%20frequency%20components%0Abrought%20by%20time-domain%20truncation.%20Building%20on%20our%20observations%2C%20we%20propose%0AFourier%20Position%20Embedding%20%28FoPE%29%2C%20which%20enhances%20attention%27s%20frequency-domain%0Aproperties%20to%20improve%20both%20its%20periodic%20extension%20and%20length%20generalization.%0AFoPE%20constructs%20Fourier%20Series%20and%20zero-outs%20the%20destructive%20frequency%0Acomponents%2C%20increasing%20model%20robustness%20against%20the%20spectrum%20damage.%0AExperiments%20across%20various%20model%20scales%20show%20that%2C%20within%20varying%20context%0Awindows%2C%20FoPE%20can%20maintain%20a%20more%20stable%20perplexity%20and%20a%20more%20consistent%0Aaccuracy%20in%20a%20needle-in-haystack%20task%20compared%20to%20RoPE%20and%20ALiBi.%20Several%0Aanalyses%20and%20ablations%20bring%20further%20support%20to%20our%20method%20and%20theoretical%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17739v1&entry.124074799=Read"},
{"title": "LASE: Learned Adjacency Spectral Embeddings", "author": "Sof\u00eda P\u00e9rez Casulo and Marcelo Fiori and Federico Larroca and Gonzalo Mateos", "abstract": "  We put forth a principled design of a neural architecture to learn nodal\nAdjacency Spectral Embeddings (ASE) from graph inputs. By bringing to bear the\ngradient descent (GD) method and leveraging the principle of algorithm\nunrolling, we truncate and re-interpret each GD iteration as a layer in a graph\nneural network (GNN) that is trained to approximate the ASE. Accordingly, we\ncall the resulting embeddings and our parametric model Learned ASE (LASE),\nwhich is interpretable, parameter efficient, robust to inputs with unobserved\nedges, and offers controllable complexity during inference. LASE layers combine\nGraph Convolutional Network (GCN) and fully-connected Graph Attention Network\n(GAT) modules, which is intuitively pleasing since GCN-based local aggregations\nalone are insufficient to express the sought graph eigenvectors. We propose\nseveral refinements to the unrolled LASE architecture (such as sparse attention\nin the GAT module and decoupled layerwise parameters) that offer favorable\napproximation error versus computation tradeoffs; even outperforming\nheavily-optimized eigendecomposition routines from scientific computing\nlibraries. Because LASE is a differentiable function with respect to its\nparameters as well as its graph input, we can seamlessly integrate it as a\ntrainable module within a larger (semi-)supervised graph representation\nlearning pipeline. The resulting end-to-end system effectively learns\n``discriminative ASEs'' that exhibit competitive performance in supervised link\nprediction and node classification tasks, outperforming a GNN even when the\nlatter is endowed with open loop, meaning task-agnostic, precomputed spectral\npositional encodings.\n", "link": "http://arxiv.org/abs/2412.17734v1", "date": "2024-12-23", "relevancy": 2.4689, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5471}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4707}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LASE%3A%20Learned%20Adjacency%20Spectral%20Embeddings&body=Title%3A%20LASE%3A%20Learned%20Adjacency%20Spectral%20Embeddings%0AAuthor%3A%20Sof%C3%ADa%20P%C3%A9rez%20Casulo%20and%20Marcelo%20Fiori%20and%20Federico%20Larroca%20and%20Gonzalo%20Mateos%0AAbstract%3A%20%20%20We%20put%20forth%20a%20principled%20design%20of%20a%20neural%20architecture%20to%20learn%20nodal%0AAdjacency%20Spectral%20Embeddings%20%28ASE%29%20from%20graph%20inputs.%20By%20bringing%20to%20bear%20the%0Agradient%20descent%20%28GD%29%20method%20and%20leveraging%20the%20principle%20of%20algorithm%0Aunrolling%2C%20we%20truncate%20and%20re-interpret%20each%20GD%20iteration%20as%20a%20layer%20in%20a%20graph%0Aneural%20network%20%28GNN%29%20that%20is%20trained%20to%20approximate%20the%20ASE.%20Accordingly%2C%20we%0Acall%20the%20resulting%20embeddings%20and%20our%20parametric%20model%20Learned%20ASE%20%28LASE%29%2C%0Awhich%20is%20interpretable%2C%20parameter%20efficient%2C%20robust%20to%20inputs%20with%20unobserved%0Aedges%2C%20and%20offers%20controllable%20complexity%20during%20inference.%20LASE%20layers%20combine%0AGraph%20Convolutional%20Network%20%28GCN%29%20and%20fully-connected%20Graph%20Attention%20Network%0A%28GAT%29%20modules%2C%20which%20is%20intuitively%20pleasing%20since%20GCN-based%20local%20aggregations%0Aalone%20are%20insufficient%20to%20express%20the%20sought%20graph%20eigenvectors.%20We%20propose%0Aseveral%20refinements%20to%20the%20unrolled%20LASE%20architecture%20%28such%20as%20sparse%20attention%0Ain%20the%20GAT%20module%20and%20decoupled%20layerwise%20parameters%29%20that%20offer%20favorable%0Aapproximation%20error%20versus%20computation%20tradeoffs%3B%20even%20outperforming%0Aheavily-optimized%20eigendecomposition%20routines%20from%20scientific%20computing%0Alibraries.%20Because%20LASE%20is%20a%20differentiable%20function%20with%20respect%20to%20its%0Aparameters%20as%20well%20as%20its%20graph%20input%2C%20we%20can%20seamlessly%20integrate%20it%20as%20a%0Atrainable%20module%20within%20a%20larger%20%28semi-%29supervised%20graph%20representation%0Alearning%20pipeline.%20The%20resulting%20end-to-end%20system%20effectively%20learns%0A%60%60discriminative%20ASEs%27%27%20that%20exhibit%20competitive%20performance%20in%20supervised%20link%0Aprediction%20and%20node%20classification%20tasks%2C%20outperforming%20a%20GNN%20even%20when%20the%0Alatter%20is%20endowed%20with%20open%20loop%2C%20meaning%20task-agnostic%2C%20precomputed%20spectral%0Apositional%20encodings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLASE%253A%2520Learned%2520Adjacency%2520Spectral%2520Embeddings%26entry.906535625%3DSof%25C3%25ADa%2520P%25C3%25A9rez%2520Casulo%2520and%2520Marcelo%2520Fiori%2520and%2520Federico%2520Larroca%2520and%2520Gonzalo%2520Mateos%26entry.1292438233%3D%2520%2520We%2520put%2520forth%2520a%2520principled%2520design%2520of%2520a%2520neural%2520architecture%2520to%2520learn%2520nodal%250AAdjacency%2520Spectral%2520Embeddings%2520%2528ASE%2529%2520from%2520graph%2520inputs.%2520By%2520bringing%2520to%2520bear%2520the%250Agradient%2520descent%2520%2528GD%2529%2520method%2520and%2520leveraging%2520the%2520principle%2520of%2520algorithm%250Aunrolling%252C%2520we%2520truncate%2520and%2520re-interpret%2520each%2520GD%2520iteration%2520as%2520a%2520layer%2520in%2520a%2520graph%250Aneural%2520network%2520%2528GNN%2529%2520that%2520is%2520trained%2520to%2520approximate%2520the%2520ASE.%2520Accordingly%252C%2520we%250Acall%2520the%2520resulting%2520embeddings%2520and%2520our%2520parametric%2520model%2520Learned%2520ASE%2520%2528LASE%2529%252C%250Awhich%2520is%2520interpretable%252C%2520parameter%2520efficient%252C%2520robust%2520to%2520inputs%2520with%2520unobserved%250Aedges%252C%2520and%2520offers%2520controllable%2520complexity%2520during%2520inference.%2520LASE%2520layers%2520combine%250AGraph%2520Convolutional%2520Network%2520%2528GCN%2529%2520and%2520fully-connected%2520Graph%2520Attention%2520Network%250A%2528GAT%2529%2520modules%252C%2520which%2520is%2520intuitively%2520pleasing%2520since%2520GCN-based%2520local%2520aggregations%250Aalone%2520are%2520insufficient%2520to%2520express%2520the%2520sought%2520graph%2520eigenvectors.%2520We%2520propose%250Aseveral%2520refinements%2520to%2520the%2520unrolled%2520LASE%2520architecture%2520%2528such%2520as%2520sparse%2520attention%250Ain%2520the%2520GAT%2520module%2520and%2520decoupled%2520layerwise%2520parameters%2529%2520that%2520offer%2520favorable%250Aapproximation%2520error%2520versus%2520computation%2520tradeoffs%253B%2520even%2520outperforming%250Aheavily-optimized%2520eigendecomposition%2520routines%2520from%2520scientific%2520computing%250Alibraries.%2520Because%2520LASE%2520is%2520a%2520differentiable%2520function%2520with%2520respect%2520to%2520its%250Aparameters%2520as%2520well%2520as%2520its%2520graph%2520input%252C%2520we%2520can%2520seamlessly%2520integrate%2520it%2520as%2520a%250Atrainable%2520module%2520within%2520a%2520larger%2520%2528semi-%2529supervised%2520graph%2520representation%250Alearning%2520pipeline.%2520The%2520resulting%2520end-to-end%2520system%2520effectively%2520learns%250A%2560%2560discriminative%2520ASEs%2527%2527%2520that%2520exhibit%2520competitive%2520performance%2520in%2520supervised%2520link%250Aprediction%2520and%2520node%2520classification%2520tasks%252C%2520outperforming%2520a%2520GNN%2520even%2520when%2520the%250Alatter%2520is%2520endowed%2520with%2520open%2520loop%252C%2520meaning%2520task-agnostic%252C%2520precomputed%2520spectral%250Apositional%2520encodings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASE%3A%20Learned%20Adjacency%20Spectral%20Embeddings&entry.906535625=Sof%C3%ADa%20P%C3%A9rez%20Casulo%20and%20Marcelo%20Fiori%20and%20Federico%20Larroca%20and%20Gonzalo%20Mateos&entry.1292438233=%20%20We%20put%20forth%20a%20principled%20design%20of%20a%20neural%20architecture%20to%20learn%20nodal%0AAdjacency%20Spectral%20Embeddings%20%28ASE%29%20from%20graph%20inputs.%20By%20bringing%20to%20bear%20the%0Agradient%20descent%20%28GD%29%20method%20and%20leveraging%20the%20principle%20of%20algorithm%0Aunrolling%2C%20we%20truncate%20and%20re-interpret%20each%20GD%20iteration%20as%20a%20layer%20in%20a%20graph%0Aneural%20network%20%28GNN%29%20that%20is%20trained%20to%20approximate%20the%20ASE.%20Accordingly%2C%20we%0Acall%20the%20resulting%20embeddings%20and%20our%20parametric%20model%20Learned%20ASE%20%28LASE%29%2C%0Awhich%20is%20interpretable%2C%20parameter%20efficient%2C%20robust%20to%20inputs%20with%20unobserved%0Aedges%2C%20and%20offers%20controllable%20complexity%20during%20inference.%20LASE%20layers%20combine%0AGraph%20Convolutional%20Network%20%28GCN%29%20and%20fully-connected%20Graph%20Attention%20Network%0A%28GAT%29%20modules%2C%20which%20is%20intuitively%20pleasing%20since%20GCN-based%20local%20aggregations%0Aalone%20are%20insufficient%20to%20express%20the%20sought%20graph%20eigenvectors.%20We%20propose%0Aseveral%20refinements%20to%20the%20unrolled%20LASE%20architecture%20%28such%20as%20sparse%20attention%0Ain%20the%20GAT%20module%20and%20decoupled%20layerwise%20parameters%29%20that%20offer%20favorable%0Aapproximation%20error%20versus%20computation%20tradeoffs%3B%20even%20outperforming%0Aheavily-optimized%20eigendecomposition%20routines%20from%20scientific%20computing%0Alibraries.%20Because%20LASE%20is%20a%20differentiable%20function%20with%20respect%20to%20its%0Aparameters%20as%20well%20as%20its%20graph%20input%2C%20we%20can%20seamlessly%20integrate%20it%20as%20a%0Atrainable%20module%20within%20a%20larger%20%28semi-%29supervised%20graph%20representation%0Alearning%20pipeline.%20The%20resulting%20end-to-end%20system%20effectively%20learns%0A%60%60discriminative%20ASEs%27%27%20that%20exhibit%20competitive%20performance%20in%20supervised%20link%0Aprediction%20and%20node%20classification%20tasks%2C%20outperforming%20a%20GNN%20even%20when%20the%0Alatter%20is%20endowed%20with%20open%20loop%2C%20meaning%20task-agnostic%2C%20precomputed%20spectral%0Apositional%20encodings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17734v1&entry.124074799=Read"},
{"title": "Mirage: A Multi-Level Superoptimizer for Tensor Programs", "author": "Mengdi Wu and Xinhao Cheng and Shengyu Liu and Chunan Shi and Jianan Ji and Kit Ao and Praveen Velliengiri and Xupeng Miao and Oded Padon and Zhihao Jia", "abstract": "  We introduce Mirage, the first multi-level superoptimizer for tensor\nprograms. A key idea in Mirage is $\\mu$Graphs, a uniform representation of\ntensor programs at the kernel, thread block, and thread levels of the GPU\ncompute hierarchy. $\\mu$Graphs enable Mirage to discover novel optimizations\nthat combine algebraic transformations, schedule transformations, and\ngeneration of new custom kernels. To navigate the large search space, Mirage\nintroduces a pruning technique based on abstraction that significantly reduces\nthe search space and provides a certain optimality guarantee. To ensure that\nthe optimized $\\mu$Graph is equivalent to the input program, Mirage introduces\na probabilistic equivalence verification procedure with strong theoretical\nguarantees. Our evaluation shows that Mirage outperforms existing approaches by\n1.1-2.9$\\times$ even for DNNs that are widely used and heavily optimized.\nMirage is publicly available at https://github.com/mirage-project/mirage.\n", "link": "http://arxiv.org/abs/2405.05751v2", "date": "2024-12-23", "relevancy": 2.4687, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5358}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4758}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mirage%3A%20A%20Multi-Level%20Superoptimizer%20for%20Tensor%20Programs&body=Title%3A%20Mirage%3A%20A%20Multi-Level%20Superoptimizer%20for%20Tensor%20Programs%0AAuthor%3A%20Mengdi%20Wu%20and%20Xinhao%20Cheng%20and%20Shengyu%20Liu%20and%20Chunan%20Shi%20and%20Jianan%20Ji%20and%20Kit%20Ao%20and%20Praveen%20Velliengiri%20and%20Xupeng%20Miao%20and%20Oded%20Padon%20and%20Zhihao%20Jia%0AAbstract%3A%20%20%20We%20introduce%20Mirage%2C%20the%20first%20multi-level%20superoptimizer%20for%20tensor%0Aprograms.%20A%20key%20idea%20in%20Mirage%20is%20%24%5Cmu%24Graphs%2C%20a%20uniform%20representation%20of%0Atensor%20programs%20at%20the%20kernel%2C%20thread%20block%2C%20and%20thread%20levels%20of%20the%20GPU%0Acompute%20hierarchy.%20%24%5Cmu%24Graphs%20enable%20Mirage%20to%20discover%20novel%20optimizations%0Athat%20combine%20algebraic%20transformations%2C%20schedule%20transformations%2C%20and%0Ageneration%20of%20new%20custom%20kernels.%20To%20navigate%20the%20large%20search%20space%2C%20Mirage%0Aintroduces%20a%20pruning%20technique%20based%20on%20abstraction%20that%20significantly%20reduces%0Athe%20search%20space%20and%20provides%20a%20certain%20optimality%20guarantee.%20To%20ensure%20that%0Athe%20optimized%20%24%5Cmu%24Graph%20is%20equivalent%20to%20the%20input%20program%2C%20Mirage%20introduces%0Aa%20probabilistic%20equivalence%20verification%20procedure%20with%20strong%20theoretical%0Aguarantees.%20Our%20evaluation%20shows%20that%20Mirage%20outperforms%20existing%20approaches%20by%0A1.1-2.9%24%5Ctimes%24%20even%20for%20DNNs%20that%20are%20widely%20used%20and%20heavily%20optimized.%0AMirage%20is%20publicly%20available%20at%20https%3A//github.com/mirage-project/mirage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMirage%253A%2520A%2520Multi-Level%2520Superoptimizer%2520for%2520Tensor%2520Programs%26entry.906535625%3DMengdi%2520Wu%2520and%2520Xinhao%2520Cheng%2520and%2520Shengyu%2520Liu%2520and%2520Chunan%2520Shi%2520and%2520Jianan%2520Ji%2520and%2520Kit%2520Ao%2520and%2520Praveen%2520Velliengiri%2520and%2520Xupeng%2520Miao%2520and%2520Oded%2520Padon%2520and%2520Zhihao%2520Jia%26entry.1292438233%3D%2520%2520We%2520introduce%2520Mirage%252C%2520the%2520first%2520multi-level%2520superoptimizer%2520for%2520tensor%250Aprograms.%2520A%2520key%2520idea%2520in%2520Mirage%2520is%2520%2524%255Cmu%2524Graphs%252C%2520a%2520uniform%2520representation%2520of%250Atensor%2520programs%2520at%2520the%2520kernel%252C%2520thread%2520block%252C%2520and%2520thread%2520levels%2520of%2520the%2520GPU%250Acompute%2520hierarchy.%2520%2524%255Cmu%2524Graphs%2520enable%2520Mirage%2520to%2520discover%2520novel%2520optimizations%250Athat%2520combine%2520algebraic%2520transformations%252C%2520schedule%2520transformations%252C%2520and%250Ageneration%2520of%2520new%2520custom%2520kernels.%2520To%2520navigate%2520the%2520large%2520search%2520space%252C%2520Mirage%250Aintroduces%2520a%2520pruning%2520technique%2520based%2520on%2520abstraction%2520that%2520significantly%2520reduces%250Athe%2520search%2520space%2520and%2520provides%2520a%2520certain%2520optimality%2520guarantee.%2520To%2520ensure%2520that%250Athe%2520optimized%2520%2524%255Cmu%2524Graph%2520is%2520equivalent%2520to%2520the%2520input%2520program%252C%2520Mirage%2520introduces%250Aa%2520probabilistic%2520equivalence%2520verification%2520procedure%2520with%2520strong%2520theoretical%250Aguarantees.%2520Our%2520evaluation%2520shows%2520that%2520Mirage%2520outperforms%2520existing%2520approaches%2520by%250A1.1-2.9%2524%255Ctimes%2524%2520even%2520for%2520DNNs%2520that%2520are%2520widely%2520used%2520and%2520heavily%2520optimized.%250AMirage%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/mirage-project/mirage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mirage%3A%20A%20Multi-Level%20Superoptimizer%20for%20Tensor%20Programs&entry.906535625=Mengdi%20Wu%20and%20Xinhao%20Cheng%20and%20Shengyu%20Liu%20and%20Chunan%20Shi%20and%20Jianan%20Ji%20and%20Kit%20Ao%20and%20Praveen%20Velliengiri%20and%20Xupeng%20Miao%20and%20Oded%20Padon%20and%20Zhihao%20Jia&entry.1292438233=%20%20We%20introduce%20Mirage%2C%20the%20first%20multi-level%20superoptimizer%20for%20tensor%0Aprograms.%20A%20key%20idea%20in%20Mirage%20is%20%24%5Cmu%24Graphs%2C%20a%20uniform%20representation%20of%0Atensor%20programs%20at%20the%20kernel%2C%20thread%20block%2C%20and%20thread%20levels%20of%20the%20GPU%0Acompute%20hierarchy.%20%24%5Cmu%24Graphs%20enable%20Mirage%20to%20discover%20novel%20optimizations%0Athat%20combine%20algebraic%20transformations%2C%20schedule%20transformations%2C%20and%0Ageneration%20of%20new%20custom%20kernels.%20To%20navigate%20the%20large%20search%20space%2C%20Mirage%0Aintroduces%20a%20pruning%20technique%20based%20on%20abstraction%20that%20significantly%20reduces%0Athe%20search%20space%20and%20provides%20a%20certain%20optimality%20guarantee.%20To%20ensure%20that%0Athe%20optimized%20%24%5Cmu%24Graph%20is%20equivalent%20to%20the%20input%20program%2C%20Mirage%20introduces%0Aa%20probabilistic%20equivalence%20verification%20procedure%20with%20strong%20theoretical%0Aguarantees.%20Our%20evaluation%20shows%20that%20Mirage%20outperforms%20existing%20approaches%20by%0A1.1-2.9%24%5Ctimes%24%20even%20for%20DNNs%20that%20are%20widely%20used%20and%20heavily%20optimized.%0AMirage%20is%20publicly%20available%20at%20https%3A//github.com/mirage-project/mirage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05751v2&entry.124074799=Read"},
{"title": "TempoKGAT: A Novel Graph Attention Network Approach for Temporal Graph\n  Analysis", "author": "Lena Sasal and Daniel Busby and Abdenour Hadid", "abstract": "  Graph neural networks (GNN) have shown significant capabilities in handling\nstructured data, yet their application to dynamic, temporal data remains\nlimited. This paper presents a new type of graph attention network, called\nTempoKGAT, which combines time-decaying weight and a selective neighbor\naggregation mechanism on the spatial domain, which helps uncover latent\npatterns in the graph data. In this approach, a top-k neighbor selection based\non the edge weights is introduced to represent the evolving features of the\ngraph data. We evaluated the performance of our TempoKGAT on multiple datasets\nfrom the traffic, energy, and health sectors involving spatio-temporal data. We\ncompared the performance of our approach to several state-of-the-art methods\nfound in the literature on several open-source datasets. Our method shows\nsuperior accuracy on all datasets. These results indicate that TempoKGAT builds\non existing methodologies to optimize prediction accuracy and provide new\ninsights into model interpretation in temporal contexts.\n", "link": "http://arxiv.org/abs/2408.16391v2", "date": "2024-12-23", "relevancy": 2.4681, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5014}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4927}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempoKGAT%3A%20A%20Novel%20Graph%20Attention%20Network%20Approach%20for%20Temporal%20Graph%0A%20%20Analysis&body=Title%3A%20TempoKGAT%3A%20A%20Novel%20Graph%20Attention%20Network%20Approach%20for%20Temporal%20Graph%0A%20%20Analysis%0AAuthor%3A%20Lena%20Sasal%20and%20Daniel%20Busby%20and%20Abdenour%20Hadid%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNN%29%20have%20shown%20significant%20capabilities%20in%20handling%0Astructured%20data%2C%20yet%20their%20application%20to%20dynamic%2C%20temporal%20data%20remains%0Alimited.%20This%20paper%20presents%20a%20new%20type%20of%20graph%20attention%20network%2C%20called%0ATempoKGAT%2C%20which%20combines%20time-decaying%20weight%20and%20a%20selective%20neighbor%0Aaggregation%20mechanism%20on%20the%20spatial%20domain%2C%20which%20helps%20uncover%20latent%0Apatterns%20in%20the%20graph%20data.%20In%20this%20approach%2C%20a%20top-k%20neighbor%20selection%20based%0Aon%20the%20edge%20weights%20is%20introduced%20to%20represent%20the%20evolving%20features%20of%20the%0Agraph%20data.%20We%20evaluated%20the%20performance%20of%20our%20TempoKGAT%20on%20multiple%20datasets%0Afrom%20the%20traffic%2C%20energy%2C%20and%20health%20sectors%20involving%20spatio-temporal%20data.%20We%0Acompared%20the%20performance%20of%20our%20approach%20to%20several%20state-of-the-art%20methods%0Afound%20in%20the%20literature%20on%20several%20open-source%20datasets.%20Our%20method%20shows%0Asuperior%20accuracy%20on%20all%20datasets.%20These%20results%20indicate%20that%20TempoKGAT%20builds%0Aon%20existing%20methodologies%20to%20optimize%20prediction%20accuracy%20and%20provide%20new%0Ainsights%20into%20model%20interpretation%20in%20temporal%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempoKGAT%253A%2520A%2520Novel%2520Graph%2520Attention%2520Network%2520Approach%2520for%2520Temporal%2520Graph%250A%2520%2520Analysis%26entry.906535625%3DLena%2520Sasal%2520and%2520Daniel%2520Busby%2520and%2520Abdenour%2520Hadid%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNN%2529%2520have%2520shown%2520significant%2520capabilities%2520in%2520handling%250Astructured%2520data%252C%2520yet%2520their%2520application%2520to%2520dynamic%252C%2520temporal%2520data%2520remains%250Alimited.%2520This%2520paper%2520presents%2520a%2520new%2520type%2520of%2520graph%2520attention%2520network%252C%2520called%250ATempoKGAT%252C%2520which%2520combines%2520time-decaying%2520weight%2520and%2520a%2520selective%2520neighbor%250Aaggregation%2520mechanism%2520on%2520the%2520spatial%2520domain%252C%2520which%2520helps%2520uncover%2520latent%250Apatterns%2520in%2520the%2520graph%2520data.%2520In%2520this%2520approach%252C%2520a%2520top-k%2520neighbor%2520selection%2520based%250Aon%2520the%2520edge%2520weights%2520is%2520introduced%2520to%2520represent%2520the%2520evolving%2520features%2520of%2520the%250Agraph%2520data.%2520We%2520evaluated%2520the%2520performance%2520of%2520our%2520TempoKGAT%2520on%2520multiple%2520datasets%250Afrom%2520the%2520traffic%252C%2520energy%252C%2520and%2520health%2520sectors%2520involving%2520spatio-temporal%2520data.%2520We%250Acompared%2520the%2520performance%2520of%2520our%2520approach%2520to%2520several%2520state-of-the-art%2520methods%250Afound%2520in%2520the%2520literature%2520on%2520several%2520open-source%2520datasets.%2520Our%2520method%2520shows%250Asuperior%2520accuracy%2520on%2520all%2520datasets.%2520These%2520results%2520indicate%2520that%2520TempoKGAT%2520builds%250Aon%2520existing%2520methodologies%2520to%2520optimize%2520prediction%2520accuracy%2520and%2520provide%2520new%250Ainsights%2520into%2520model%2520interpretation%2520in%2520temporal%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempoKGAT%3A%20A%20Novel%20Graph%20Attention%20Network%20Approach%20for%20Temporal%20Graph%0A%20%20Analysis&entry.906535625=Lena%20Sasal%20and%20Daniel%20Busby%20and%20Abdenour%20Hadid&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNN%29%20have%20shown%20significant%20capabilities%20in%20handling%0Astructured%20data%2C%20yet%20their%20application%20to%20dynamic%2C%20temporal%20data%20remains%0Alimited.%20This%20paper%20presents%20a%20new%20type%20of%20graph%20attention%20network%2C%20called%0ATempoKGAT%2C%20which%20combines%20time-decaying%20weight%20and%20a%20selective%20neighbor%0Aaggregation%20mechanism%20on%20the%20spatial%20domain%2C%20which%20helps%20uncover%20latent%0Apatterns%20in%20the%20graph%20data.%20In%20this%20approach%2C%20a%20top-k%20neighbor%20selection%20based%0Aon%20the%20edge%20weights%20is%20introduced%20to%20represent%20the%20evolving%20features%20of%20the%0Agraph%20data.%20We%20evaluated%20the%20performance%20of%20our%20TempoKGAT%20on%20multiple%20datasets%0Afrom%20the%20traffic%2C%20energy%2C%20and%20health%20sectors%20involving%20spatio-temporal%20data.%20We%0Acompared%20the%20performance%20of%20our%20approach%20to%20several%20state-of-the-art%20methods%0Afound%20in%20the%20literature%20on%20several%20open-source%20datasets.%20Our%20method%20shows%0Asuperior%20accuracy%20on%20all%20datasets.%20These%20results%20indicate%20that%20TempoKGAT%20builds%0Aon%20existing%20methodologies%20to%20optimize%20prediction%20accuracy%20and%20provide%20new%0Ainsights%20into%20model%20interpretation%20in%20temporal%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16391v2&entry.124074799=Read"},
{"title": "Reviewing Intelligent Cinematography: AI research for camera-based video\n  production", "author": "Adrian Azzarelli and Nantheera Anantrasirichai and David R Bull", "abstract": "  This paper offers the first comprehensive review of artificial intelligence\n(AI) research in the context of real camera content acquisition for\nentertainment purposes and is aimed at both researchers and cinematographers.\nAddressing the lack of review papers in the field of intelligent\ncinematography} (IC) and the breadth of related computer vision research, we\npresent a holistic view of the IC landscape while providing technical insight,\nimportant for experts across disciplines. We provide technical background on\ngenerative AI, object detection, automated camera calibration and 3-D content\nacquisition, with references to assist non-technical readers. The application\nsections categorize work in terms of four production types: General Production,\nVirtual Production, Live Production and Aerial Production. Within each\napplication section, we (1) sub-classify work according to research topic and\n(2) describe the trends and challenges relevant to each type of production. In\nthe final chapter, we address the greater scope of IC research and summarize\nthe significant potential of this area to influence the creative industries\nsector. We suggest that work relating to virtual production has the greatest\npotential to impact other mediums of production, driven by the growing interest\nin LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D\ncapture for virtual modeling of real world scenes and actors. We also address\nethical and legal concerns regarding the use of creative AI that impact on\nartists, actors, technologists and the general public.\n", "link": "http://arxiv.org/abs/2405.05039v2", "date": "2024-12-23", "relevancy": 2.4514, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production&body=Title%3A%20Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production%0AAuthor%3A%20Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull%0AAbstract%3A%20%20%20This%20paper%20offers%20the%20first%20comprehensive%20review%20of%20artificial%20intelligence%0A%28AI%29%20research%20in%20the%20context%20of%20real%20camera%20content%20acquisition%20for%0Aentertainment%20purposes%20and%20is%20aimed%20at%20both%20researchers%20and%20cinematographers.%0AAddressing%20the%20lack%20of%20review%20papers%20in%20the%20field%20of%20intelligent%0Acinematography%7D%20%28IC%29%20and%20the%20breadth%20of%20related%20computer%20vision%20research%2C%20we%0Apresent%20a%20holistic%20view%20of%20the%20IC%20landscape%20while%20providing%20technical%20insight%2C%0Aimportant%20for%20experts%20across%20disciplines.%20We%20provide%20technical%20background%20on%0Agenerative%20AI%2C%20object%20detection%2C%20automated%20camera%20calibration%20and%203-D%20content%0Aacquisition%2C%20with%20references%20to%20assist%20non-technical%20readers.%20The%20application%0Asections%20categorize%20work%20in%20terms%20of%20four%20production%20types%3A%20General%20Production%2C%0AVirtual%20Production%2C%20Live%20Production%20and%20Aerial%20Production.%20Within%20each%0Aapplication%20section%2C%20we%20%281%29%20sub-classify%20work%20according%20to%20research%20topic%20and%0A%282%29%20describe%20the%20trends%20and%20challenges%20relevant%20to%20each%20type%20of%20production.%20In%0Athe%20final%20chapter%2C%20we%20address%20the%20greater%20scope%20of%20IC%20research%20and%20summarize%0Athe%20significant%20potential%20of%20this%20area%20to%20influence%20the%20creative%20industries%0Asector.%20We%20suggest%20that%20work%20relating%20to%20virtual%20production%20has%20the%20greatest%0Apotential%20to%20impact%20other%20mediums%20of%20production%2C%20driven%20by%20the%20growing%20interest%0Ain%20LED%20volumes/stages%20for%20in-camera%20virtual%20effects%20%28ICVFX%29%20and%20automated%203-D%0Acapture%20for%20virtual%20modeling%20of%20real%20world%20scenes%20and%20actors.%20We%20also%20address%0Aethical%20and%20legal%20concerns%20regarding%20the%20use%20of%20creative%20AI%20that%20impact%20on%0Aartists%2C%20actors%2C%20technologists%20and%20the%20general%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReviewing%2520Intelligent%2520Cinematography%253A%2520AI%2520research%2520for%2520camera-based%2520video%250A%2520%2520production%26entry.906535625%3DAdrian%2520Azzarelli%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520David%2520R%2520Bull%26entry.1292438233%3D%2520%2520This%2520paper%2520offers%2520the%2520first%2520comprehensive%2520review%2520of%2520artificial%2520intelligence%250A%2528AI%2529%2520research%2520in%2520the%2520context%2520of%2520real%2520camera%2520content%2520acquisition%2520for%250Aentertainment%2520purposes%2520and%2520is%2520aimed%2520at%2520both%2520researchers%2520and%2520cinematographers.%250AAddressing%2520the%2520lack%2520of%2520review%2520papers%2520in%2520the%2520field%2520of%2520intelligent%250Acinematography%257D%2520%2528IC%2529%2520and%2520the%2520breadth%2520of%2520related%2520computer%2520vision%2520research%252C%2520we%250Apresent%2520a%2520holistic%2520view%2520of%2520the%2520IC%2520landscape%2520while%2520providing%2520technical%2520insight%252C%250Aimportant%2520for%2520experts%2520across%2520disciplines.%2520We%2520provide%2520technical%2520background%2520on%250Agenerative%2520AI%252C%2520object%2520detection%252C%2520automated%2520camera%2520calibration%2520and%25203-D%2520content%250Aacquisition%252C%2520with%2520references%2520to%2520assist%2520non-technical%2520readers.%2520The%2520application%250Asections%2520categorize%2520work%2520in%2520terms%2520of%2520four%2520production%2520types%253A%2520General%2520Production%252C%250AVirtual%2520Production%252C%2520Live%2520Production%2520and%2520Aerial%2520Production.%2520Within%2520each%250Aapplication%2520section%252C%2520we%2520%25281%2529%2520sub-classify%2520work%2520according%2520to%2520research%2520topic%2520and%250A%25282%2529%2520describe%2520the%2520trends%2520and%2520challenges%2520relevant%2520to%2520each%2520type%2520of%2520production.%2520In%250Athe%2520final%2520chapter%252C%2520we%2520address%2520the%2520greater%2520scope%2520of%2520IC%2520research%2520and%2520summarize%250Athe%2520significant%2520potential%2520of%2520this%2520area%2520to%2520influence%2520the%2520creative%2520industries%250Asector.%2520We%2520suggest%2520that%2520work%2520relating%2520to%2520virtual%2520production%2520has%2520the%2520greatest%250Apotential%2520to%2520impact%2520other%2520mediums%2520of%2520production%252C%2520driven%2520by%2520the%2520growing%2520interest%250Ain%2520LED%2520volumes/stages%2520for%2520in-camera%2520virtual%2520effects%2520%2528ICVFX%2529%2520and%2520automated%25203-D%250Acapture%2520for%2520virtual%2520modeling%2520of%2520real%2520world%2520scenes%2520and%2520actors.%2520We%2520also%2520address%250Aethical%2520and%2520legal%2520concerns%2520regarding%2520the%2520use%2520of%2520creative%2520AI%2520that%2520impact%2520on%250Aartists%252C%2520actors%252C%2520technologists%2520and%2520the%2520general%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production&entry.906535625=Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull&entry.1292438233=%20%20This%20paper%20offers%20the%20first%20comprehensive%20review%20of%20artificial%20intelligence%0A%28AI%29%20research%20in%20the%20context%20of%20real%20camera%20content%20acquisition%20for%0Aentertainment%20purposes%20and%20is%20aimed%20at%20both%20researchers%20and%20cinematographers.%0AAddressing%20the%20lack%20of%20review%20papers%20in%20the%20field%20of%20intelligent%0Acinematography%7D%20%28IC%29%20and%20the%20breadth%20of%20related%20computer%20vision%20research%2C%20we%0Apresent%20a%20holistic%20view%20of%20the%20IC%20landscape%20while%20providing%20technical%20insight%2C%0Aimportant%20for%20experts%20across%20disciplines.%20We%20provide%20technical%20background%20on%0Agenerative%20AI%2C%20object%20detection%2C%20automated%20camera%20calibration%20and%203-D%20content%0Aacquisition%2C%20with%20references%20to%20assist%20non-technical%20readers.%20The%20application%0Asections%20categorize%20work%20in%20terms%20of%20four%20production%20types%3A%20General%20Production%2C%0AVirtual%20Production%2C%20Live%20Production%20and%20Aerial%20Production.%20Within%20each%0Aapplication%20section%2C%20we%20%281%29%20sub-classify%20work%20according%20to%20research%20topic%20and%0A%282%29%20describe%20the%20trends%20and%20challenges%20relevant%20to%20each%20type%20of%20production.%20In%0Athe%20final%20chapter%2C%20we%20address%20the%20greater%20scope%20of%20IC%20research%20and%20summarize%0Athe%20significant%20potential%20of%20this%20area%20to%20influence%20the%20creative%20industries%0Asector.%20We%20suggest%20that%20work%20relating%20to%20virtual%20production%20has%20the%20greatest%0Apotential%20to%20impact%20other%20mediums%20of%20production%2C%20driven%20by%20the%20growing%20interest%0Ain%20LED%20volumes/stages%20for%20in-camera%20virtual%20effects%20%28ICVFX%29%20and%20automated%203-D%0Acapture%20for%20virtual%20modeling%20of%20real%20world%20scenes%20and%20actors.%20We%20also%20address%0Aethical%20and%20legal%20concerns%20regarding%20the%20use%20of%20creative%20AI%20that%20impact%20on%0Aartists%2C%20actors%2C%20technologists%20and%20the%20general%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05039v2&entry.124074799=Read"},
{"title": "Applying LLM and Topic Modelling in Psychotherapeutic Contexts", "author": "Alexander Vanin and Vadim Bolshev and Anastasia Panfilova", "abstract": "  This study explores the use of Large language models to analyze therapist\nremarks in a psychotherapeutic setting. The paper focuses on the application of\nBERTopic, a machine learning-based topic modeling tool, to the dialogue of two\ndifferent groups of therapists (classical and modern), which makes it possible\nto identify and describe a set of topics that consistently emerge across these\ngroups. The paper describes in detail the chosen algorithm for BERTopic, which\nincluded creating a vector space from a corpus of therapist remarks, reducing\nits dimensionality, clustering the space, and creating and optimizing topic\nrepresentation. Along with the automatic topical modeling by the BERTopic, the\nresearch involved an expert assessment of the findings and manual topic\nstructure optimization. The topic modeling results highlighted the most common\nand stable topics in therapists speech, offering insights into how language\npatterns in therapy develop and remain stable across different therapeutic\nstyles. This work contributes to the growing field of machine learning in\npsychotherapy by demonstrating the potential of automated methods to improve\nboth the practice and training of therapists. The study highlights the value of\ntopic modeling as a tool for gaining a deeper understanding of therapeutic\ndialogue and offers new opportunities for improving therapeutic effectiveness\nand clinical supervision.\n", "link": "http://arxiv.org/abs/2412.17449v1", "date": "2024-12-23", "relevancy": 2.4474, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20LLM%20and%20Topic%20Modelling%20in%20Psychotherapeutic%20Contexts&body=Title%3A%20Applying%20LLM%20and%20Topic%20Modelling%20in%20Psychotherapeutic%20Contexts%0AAuthor%3A%20Alexander%20Vanin%20and%20Vadim%20Bolshev%20and%20Anastasia%20Panfilova%0AAbstract%3A%20%20%20This%20study%20explores%20the%20use%20of%20Large%20language%20models%20to%20analyze%20therapist%0Aremarks%20in%20a%20psychotherapeutic%20setting.%20The%20paper%20focuses%20on%20the%20application%20of%0ABERTopic%2C%20a%20machine%20learning-based%20topic%20modeling%20tool%2C%20to%20the%20dialogue%20of%20two%0Adifferent%20groups%20of%20therapists%20%28classical%20and%20modern%29%2C%20which%20makes%20it%20possible%0Ato%20identify%20and%20describe%20a%20set%20of%20topics%20that%20consistently%20emerge%20across%20these%0Agroups.%20The%20paper%20describes%20in%20detail%20the%20chosen%20algorithm%20for%20BERTopic%2C%20which%0Aincluded%20creating%20a%20vector%20space%20from%20a%20corpus%20of%20therapist%20remarks%2C%20reducing%0Aits%20dimensionality%2C%20clustering%20the%20space%2C%20and%20creating%20and%20optimizing%20topic%0Arepresentation.%20Along%20with%20the%20automatic%20topical%20modeling%20by%20the%20BERTopic%2C%20the%0Aresearch%20involved%20an%20expert%20assessment%20of%20the%20findings%20and%20manual%20topic%0Astructure%20optimization.%20The%20topic%20modeling%20results%20highlighted%20the%20most%20common%0Aand%20stable%20topics%20in%20therapists%20speech%2C%20offering%20insights%20into%20how%20language%0Apatterns%20in%20therapy%20develop%20and%20remain%20stable%20across%20different%20therapeutic%0Astyles.%20This%20work%20contributes%20to%20the%20growing%20field%20of%20machine%20learning%20in%0Apsychotherapy%20by%20demonstrating%20the%20potential%20of%20automated%20methods%20to%20improve%0Aboth%20the%20practice%20and%20training%20of%20therapists.%20The%20study%20highlights%20the%20value%20of%0Atopic%20modeling%20as%20a%20tool%20for%20gaining%20a%20deeper%20understanding%20of%20therapeutic%0Adialogue%20and%20offers%20new%20opportunities%20for%20improving%20therapeutic%20effectiveness%0Aand%20clinical%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520LLM%2520and%2520Topic%2520Modelling%2520in%2520Psychotherapeutic%2520Contexts%26entry.906535625%3DAlexander%2520Vanin%2520and%2520Vadim%2520Bolshev%2520and%2520Anastasia%2520Panfilova%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520use%2520of%2520Large%2520language%2520models%2520to%2520analyze%2520therapist%250Aremarks%2520in%2520a%2520psychotherapeutic%2520setting.%2520The%2520paper%2520focuses%2520on%2520the%2520application%2520of%250ABERTopic%252C%2520a%2520machine%2520learning-based%2520topic%2520modeling%2520tool%252C%2520to%2520the%2520dialogue%2520of%2520two%250Adifferent%2520groups%2520of%2520therapists%2520%2528classical%2520and%2520modern%2529%252C%2520which%2520makes%2520it%2520possible%250Ato%2520identify%2520and%2520describe%2520a%2520set%2520of%2520topics%2520that%2520consistently%2520emerge%2520across%2520these%250Agroups.%2520The%2520paper%2520describes%2520in%2520detail%2520the%2520chosen%2520algorithm%2520for%2520BERTopic%252C%2520which%250Aincluded%2520creating%2520a%2520vector%2520space%2520from%2520a%2520corpus%2520of%2520therapist%2520remarks%252C%2520reducing%250Aits%2520dimensionality%252C%2520clustering%2520the%2520space%252C%2520and%2520creating%2520and%2520optimizing%2520topic%250Arepresentation.%2520Along%2520with%2520the%2520automatic%2520topical%2520modeling%2520by%2520the%2520BERTopic%252C%2520the%250Aresearch%2520involved%2520an%2520expert%2520assessment%2520of%2520the%2520findings%2520and%2520manual%2520topic%250Astructure%2520optimization.%2520The%2520topic%2520modeling%2520results%2520highlighted%2520the%2520most%2520common%250Aand%2520stable%2520topics%2520in%2520therapists%2520speech%252C%2520offering%2520insights%2520into%2520how%2520language%250Apatterns%2520in%2520therapy%2520develop%2520and%2520remain%2520stable%2520across%2520different%2520therapeutic%250Astyles.%2520This%2520work%2520contributes%2520to%2520the%2520growing%2520field%2520of%2520machine%2520learning%2520in%250Apsychotherapy%2520by%2520demonstrating%2520the%2520potential%2520of%2520automated%2520methods%2520to%2520improve%250Aboth%2520the%2520practice%2520and%2520training%2520of%2520therapists.%2520The%2520study%2520highlights%2520the%2520value%2520of%250Atopic%2520modeling%2520as%2520a%2520tool%2520for%2520gaining%2520a%2520deeper%2520understanding%2520of%2520therapeutic%250Adialogue%2520and%2520offers%2520new%2520opportunities%2520for%2520improving%2520therapeutic%2520effectiveness%250Aand%2520clinical%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20LLM%20and%20Topic%20Modelling%20in%20Psychotherapeutic%20Contexts&entry.906535625=Alexander%20Vanin%20and%20Vadim%20Bolshev%20and%20Anastasia%20Panfilova&entry.1292438233=%20%20This%20study%20explores%20the%20use%20of%20Large%20language%20models%20to%20analyze%20therapist%0Aremarks%20in%20a%20psychotherapeutic%20setting.%20The%20paper%20focuses%20on%20the%20application%20of%0ABERTopic%2C%20a%20machine%20learning-based%20topic%20modeling%20tool%2C%20to%20the%20dialogue%20of%20two%0Adifferent%20groups%20of%20therapists%20%28classical%20and%20modern%29%2C%20which%20makes%20it%20possible%0Ato%20identify%20and%20describe%20a%20set%20of%20topics%20that%20consistently%20emerge%20across%20these%0Agroups.%20The%20paper%20describes%20in%20detail%20the%20chosen%20algorithm%20for%20BERTopic%2C%20which%0Aincluded%20creating%20a%20vector%20space%20from%20a%20corpus%20of%20therapist%20remarks%2C%20reducing%0Aits%20dimensionality%2C%20clustering%20the%20space%2C%20and%20creating%20and%20optimizing%20topic%0Arepresentation.%20Along%20with%20the%20automatic%20topical%20modeling%20by%20the%20BERTopic%2C%20the%0Aresearch%20involved%20an%20expert%20assessment%20of%20the%20findings%20and%20manual%20topic%0Astructure%20optimization.%20The%20topic%20modeling%20results%20highlighted%20the%20most%20common%0Aand%20stable%20topics%20in%20therapists%20speech%2C%20offering%20insights%20into%20how%20language%0Apatterns%20in%20therapy%20develop%20and%20remain%20stable%20across%20different%20therapeutic%0Astyles.%20This%20work%20contributes%20to%20the%20growing%20field%20of%20machine%20learning%20in%0Apsychotherapy%20by%20demonstrating%20the%20potential%20of%20automated%20methods%20to%20improve%0Aboth%20the%20practice%20and%20training%20of%20therapists.%20The%20study%20highlights%20the%20value%20of%0Atopic%20modeling%20as%20a%20tool%20for%20gaining%20a%20deeper%20understanding%20of%20therapeutic%0Adialogue%20and%20offers%20new%20opportunities%20for%20improving%20therapeutic%20effectiveness%0Aand%20clinical%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17449v1&entry.124074799=Read"},
{"title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized\n  Attention", "author": "Siyuan Huang and Yunchong Song and Jiayue Zhou and Zhouhan Lin", "abstract": "  In the realm of graph learning, there is a category of methods that\nconceptualize graphs as hierarchical structures, utilizing node clustering to\ncapture broader structural information. While generally effective, these\nmethods often rely on a fixed graph coarsening routine, leading to overly\nhomogeneous cluster representations and loss of node-level information. In this\npaper, we envision the graph as a network of interconnected node sets without\ncompressing each cluster into a single embedding. To enable effective\ninformation transfer among these node sets, we propose the Node-to-Cluster\nAttention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple\nKernel Learning into the kernelized attention framework, effectively capturing\ninformation at both node and cluster levels. We then devise an efficient form\nfor N2C-Attn using the cluster-wise message-passing framework, achieving linear\ntime complexity. We further analyze how N2C-Attn combines bi-level feature maps\nof queries and keys, demonstrating its capability to merge dual-granularity\ninformation. The resulting architecture, Cluster-wise Graph Transformer\n(Cluster-GT), which uses node clusters as tokens and employs our proposed\nN2C-Attn module, shows superior performance on various graph-level tasks. Code\nis available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.\n", "link": "http://arxiv.org/abs/2410.06746v2", "date": "2024-12-23", "relevancy": 2.4368, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention&body=Title%3A%20Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention%0AAuthor%3A%20Siyuan%20Huang%20and%20Yunchong%20Song%20and%20Jiayue%20Zhou%20and%20Zhouhan%20Lin%0AAbstract%3A%20%20%20In%20the%20realm%20of%20graph%20learning%2C%20there%20is%20a%20category%20of%20methods%20that%0Aconceptualize%20graphs%20as%20hierarchical%20structures%2C%20utilizing%20node%20clustering%20to%0Acapture%20broader%20structural%20information.%20While%20generally%20effective%2C%20these%0Amethods%20often%20rely%20on%20a%20fixed%20graph%20coarsening%20routine%2C%20leading%20to%20overly%0Ahomogeneous%20cluster%20representations%20and%20loss%20of%20node-level%20information.%20In%20this%0Apaper%2C%20we%20envision%20the%20graph%20as%20a%20network%20of%20interconnected%20node%20sets%20without%0Acompressing%20each%20cluster%20into%20a%20single%20embedding.%20To%20enable%20effective%0Ainformation%20transfer%20among%20these%20node%20sets%2C%20we%20propose%20the%20Node-to-Cluster%0AAttention%20%28N2C-Attn%29%20mechanism.%20N2C-Attn%20incorporates%20techniques%20from%20Multiple%0AKernel%20Learning%20into%20the%20kernelized%20attention%20framework%2C%20effectively%20capturing%0Ainformation%20at%20both%20node%20and%20cluster%20levels.%20We%20then%20devise%20an%20efficient%20form%0Afor%20N2C-Attn%20using%20the%20cluster-wise%20message-passing%20framework%2C%20achieving%20linear%0Atime%20complexity.%20We%20further%20analyze%20how%20N2C-Attn%20combines%20bi-level%20feature%20maps%0Aof%20queries%20and%20keys%2C%20demonstrating%20its%20capability%20to%20merge%20dual-granularity%0Ainformation.%20The%20resulting%20architecture%2C%20Cluster-wise%20Graph%20Transformer%0A%28Cluster-GT%29%2C%20which%20uses%20node%20clusters%20as%20tokens%20and%20employs%20our%20proposed%0AN2C-Attn%20module%2C%20shows%20superior%20performance%20on%20various%20graph-level%20tasks.%20Code%0Ais%20available%20at%20https%3A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster-wise%2520Graph%2520Transformer%2520with%2520Dual-granularity%2520Kernelized%250A%2520%2520Attention%26entry.906535625%3DSiyuan%2520Huang%2520and%2520Yunchong%2520Song%2520and%2520Jiayue%2520Zhou%2520and%2520Zhouhan%2520Lin%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520graph%2520learning%252C%2520there%2520is%2520a%2520category%2520of%2520methods%2520that%250Aconceptualize%2520graphs%2520as%2520hierarchical%2520structures%252C%2520utilizing%2520node%2520clustering%2520to%250Acapture%2520broader%2520structural%2520information.%2520While%2520generally%2520effective%252C%2520these%250Amethods%2520often%2520rely%2520on%2520a%2520fixed%2520graph%2520coarsening%2520routine%252C%2520leading%2520to%2520overly%250Ahomogeneous%2520cluster%2520representations%2520and%2520loss%2520of%2520node-level%2520information.%2520In%2520this%250Apaper%252C%2520we%2520envision%2520the%2520graph%2520as%2520a%2520network%2520of%2520interconnected%2520node%2520sets%2520without%250Acompressing%2520each%2520cluster%2520into%2520a%2520single%2520embedding.%2520To%2520enable%2520effective%250Ainformation%2520transfer%2520among%2520these%2520node%2520sets%252C%2520we%2520propose%2520the%2520Node-to-Cluster%250AAttention%2520%2528N2C-Attn%2529%2520mechanism.%2520N2C-Attn%2520incorporates%2520techniques%2520from%2520Multiple%250AKernel%2520Learning%2520into%2520the%2520kernelized%2520attention%2520framework%252C%2520effectively%2520capturing%250Ainformation%2520at%2520both%2520node%2520and%2520cluster%2520levels.%2520We%2520then%2520devise%2520an%2520efficient%2520form%250Afor%2520N2C-Attn%2520using%2520the%2520cluster-wise%2520message-passing%2520framework%252C%2520achieving%2520linear%250Atime%2520complexity.%2520We%2520further%2520analyze%2520how%2520N2C-Attn%2520combines%2520bi-level%2520feature%2520maps%250Aof%2520queries%2520and%2520keys%252C%2520demonstrating%2520its%2520capability%2520to%2520merge%2520dual-granularity%250Ainformation.%2520The%2520resulting%2520architecture%252C%2520Cluster-wise%2520Graph%2520Transformer%250A%2528Cluster-GT%2529%252C%2520which%2520uses%2520node%2520clusters%2520as%2520tokens%2520and%2520employs%2520our%2520proposed%250AN2C-Attn%2520module%252C%2520shows%2520superior%2520performance%2520on%2520various%2520graph-level%2520tasks.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention&entry.906535625=Siyuan%20Huang%20and%20Yunchong%20Song%20and%20Jiayue%20Zhou%20and%20Zhouhan%20Lin&entry.1292438233=%20%20In%20the%20realm%20of%20graph%20learning%2C%20there%20is%20a%20category%20of%20methods%20that%0Aconceptualize%20graphs%20as%20hierarchical%20structures%2C%20utilizing%20node%20clustering%20to%0Acapture%20broader%20structural%20information.%20While%20generally%20effective%2C%20these%0Amethods%20often%20rely%20on%20a%20fixed%20graph%20coarsening%20routine%2C%20leading%20to%20overly%0Ahomogeneous%20cluster%20representations%20and%20loss%20of%20node-level%20information.%20In%20this%0Apaper%2C%20we%20envision%20the%20graph%20as%20a%20network%20of%20interconnected%20node%20sets%20without%0Acompressing%20each%20cluster%20into%20a%20single%20embedding.%20To%20enable%20effective%0Ainformation%20transfer%20among%20these%20node%20sets%2C%20we%20propose%20the%20Node-to-Cluster%0AAttention%20%28N2C-Attn%29%20mechanism.%20N2C-Attn%20incorporates%20techniques%20from%20Multiple%0AKernel%20Learning%20into%20the%20kernelized%20attention%20framework%2C%20effectively%20capturing%0Ainformation%20at%20both%20node%20and%20cluster%20levels.%20We%20then%20devise%20an%20efficient%20form%0Afor%20N2C-Attn%20using%20the%20cluster-wise%20message-passing%20framework%2C%20achieving%20linear%0Atime%20complexity.%20We%20further%20analyze%20how%20N2C-Attn%20combines%20bi-level%20feature%20maps%0Aof%20queries%20and%20keys%2C%20demonstrating%20its%20capability%20to%20merge%20dual-granularity%0Ainformation.%20The%20resulting%20architecture%2C%20Cluster-wise%20Graph%20Transformer%0A%28Cluster-GT%29%2C%20which%20uses%20node%20clusters%20as%20tokens%20and%20employs%20our%20proposed%0AN2C-Attn%20module%2C%20shows%20superior%20performance%20on%20various%20graph-level%20tasks.%20Code%0Ais%20available%20at%20https%3A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06746v2&entry.124074799=Read"},
{"title": "Large Motion Video Autoencoding with Cross-modal Video VAE", "author": "Yazhou Xing and Yang Fei and Yingqing He and Jingye Chen and Jiaxin Xie and Xiaowei Chi and Qifeng Chen", "abstract": "  Learning a robust video Variational Autoencoder (VAE) is essential for\nreducing video redundancy and facilitating efficient video generation. Directly\napplying image VAEs to individual frames in isolation can result in temporal\ninconsistencies and suboptimal compression rates due to a lack of temporal\ncompression. Existing Video VAEs have begun to address temporal compression;\nhowever, they often suffer from inadequate reconstruction performance. In this\npaper, we present a novel and powerful video autoencoder capable of\nhigh-fidelity video encoding. First, we observe that entangling spatial and\ntemporal compression by merely extending the image VAE to a 3D VAE can\nintroduce motion blur and detail distortion artifacts. Thus, we propose\ntemporal-aware spatial compression to better encode and decode the spatial\ninformation. Additionally, we integrate a lightweight motion compression model\nfor further temporal compression. Second, we propose to leverage the textual\ninformation inherent in text-to-video datasets and incorporate text guidance\ninto our model. This significantly enhances reconstruction quality,\nparticularly in terms of detail preservation and temporal stability. Third, we\nfurther improve the versatility of our model through joint training on both\nimages and videos, which not only enhances reconstruction quality but also\nenables the model to perform both image and video autoencoding. Extensive\nevaluations against strong recent baselines demonstrate the superior\nperformance of our method. The project website can be found\nat~\\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.\n", "link": "http://arxiv.org/abs/2412.17805v1", "date": "2024-12-23", "relevancy": 2.4311, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.632}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6175}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Motion%20Video%20Autoencoding%20with%20Cross-modal%20Video%20VAE&body=Title%3A%20Large%20Motion%20Video%20Autoencoding%20with%20Cross-modal%20Video%20VAE%0AAuthor%3A%20Yazhou%20Xing%20and%20Yang%20Fei%20and%20Yingqing%20He%20and%20Jingye%20Chen%20and%20Jiaxin%20Xie%20and%20Xiaowei%20Chi%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20Learning%20a%20robust%20video%20Variational%20Autoencoder%20%28VAE%29%20is%20essential%20for%0Areducing%20video%20redundancy%20and%20facilitating%20efficient%20video%20generation.%20Directly%0Aapplying%20image%20VAEs%20to%20individual%20frames%20in%20isolation%20can%20result%20in%20temporal%0Ainconsistencies%20and%20suboptimal%20compression%20rates%20due%20to%20a%20lack%20of%20temporal%0Acompression.%20Existing%20Video%20VAEs%20have%20begun%20to%20address%20temporal%20compression%3B%0Ahowever%2C%20they%20often%20suffer%20from%20inadequate%20reconstruction%20performance.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20and%20powerful%20video%20autoencoder%20capable%20of%0Ahigh-fidelity%20video%20encoding.%20First%2C%20we%20observe%20that%20entangling%20spatial%20and%0Atemporal%20compression%20by%20merely%20extending%20the%20image%20VAE%20to%20a%203D%20VAE%20can%0Aintroduce%20motion%20blur%20and%20detail%20distortion%20artifacts.%20Thus%2C%20we%20propose%0Atemporal-aware%20spatial%20compression%20to%20better%20encode%20and%20decode%20the%20spatial%0Ainformation.%20Additionally%2C%20we%20integrate%20a%20lightweight%20motion%20compression%20model%0Afor%20further%20temporal%20compression.%20Second%2C%20we%20propose%20to%20leverage%20the%20textual%0Ainformation%20inherent%20in%20text-to-video%20datasets%20and%20incorporate%20text%20guidance%0Ainto%20our%20model.%20This%20significantly%20enhances%20reconstruction%20quality%2C%0Aparticularly%20in%20terms%20of%20detail%20preservation%20and%20temporal%20stability.%20Third%2C%20we%0Afurther%20improve%20the%20versatility%20of%20our%20model%20through%20joint%20training%20on%20both%0Aimages%20and%20videos%2C%20which%20not%20only%20enhances%20reconstruction%20quality%20but%20also%0Aenables%20the%20model%20to%20perform%20both%20image%20and%20video%20autoencoding.%20Extensive%0Aevaluations%20against%20strong%20recent%20baselines%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method.%20The%20project%20website%20can%20be%20found%0Aat~%5Chref%7Bhttps%3A//yzxing87.github.io/vae/%7D%7Bhttps%3A//yzxing87.github.io/vae/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Motion%2520Video%2520Autoencoding%2520with%2520Cross-modal%2520Video%2520VAE%26entry.906535625%3DYazhou%2520Xing%2520and%2520Yang%2520Fei%2520and%2520Yingqing%2520He%2520and%2520Jingye%2520Chen%2520and%2520Jiaxin%2520Xie%2520and%2520Xiaowei%2520Chi%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520Learning%2520a%2520robust%2520video%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520is%2520essential%2520for%250Areducing%2520video%2520redundancy%2520and%2520facilitating%2520efficient%2520video%2520generation.%2520Directly%250Aapplying%2520image%2520VAEs%2520to%2520individual%2520frames%2520in%2520isolation%2520can%2520result%2520in%2520temporal%250Ainconsistencies%2520and%2520suboptimal%2520compression%2520rates%2520due%2520to%2520a%2520lack%2520of%2520temporal%250Acompression.%2520Existing%2520Video%2520VAEs%2520have%2520begun%2520to%2520address%2520temporal%2520compression%253B%250Ahowever%252C%2520they%2520often%2520suffer%2520from%2520inadequate%2520reconstruction%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520novel%2520and%2520powerful%2520video%2520autoencoder%2520capable%2520of%250Ahigh-fidelity%2520video%2520encoding.%2520First%252C%2520we%2520observe%2520that%2520entangling%2520spatial%2520and%250Atemporal%2520compression%2520by%2520merely%2520extending%2520the%2520image%2520VAE%2520to%2520a%25203D%2520VAE%2520can%250Aintroduce%2520motion%2520blur%2520and%2520detail%2520distortion%2520artifacts.%2520Thus%252C%2520we%2520propose%250Atemporal-aware%2520spatial%2520compression%2520to%2520better%2520encode%2520and%2520decode%2520the%2520spatial%250Ainformation.%2520Additionally%252C%2520we%2520integrate%2520a%2520lightweight%2520motion%2520compression%2520model%250Afor%2520further%2520temporal%2520compression.%2520Second%252C%2520we%2520propose%2520to%2520leverage%2520the%2520textual%250Ainformation%2520inherent%2520in%2520text-to-video%2520datasets%2520and%2520incorporate%2520text%2520guidance%250Ainto%2520our%2520model.%2520This%2520significantly%2520enhances%2520reconstruction%2520quality%252C%250Aparticularly%2520in%2520terms%2520of%2520detail%2520preservation%2520and%2520temporal%2520stability.%2520Third%252C%2520we%250Afurther%2520improve%2520the%2520versatility%2520of%2520our%2520model%2520through%2520joint%2520training%2520on%2520both%250Aimages%2520and%2520videos%252C%2520which%2520not%2520only%2520enhances%2520reconstruction%2520quality%2520but%2520also%250Aenables%2520the%2520model%2520to%2520perform%2520both%2520image%2520and%2520video%2520autoencoding.%2520Extensive%250Aevaluations%2520against%2520strong%2520recent%2520baselines%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520method.%2520The%2520project%2520website%2520can%2520be%2520found%250Aat~%255Chref%257Bhttps%253A//yzxing87.github.io/vae/%257D%257Bhttps%253A//yzxing87.github.io/vae/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Motion%20Video%20Autoencoding%20with%20Cross-modal%20Video%20VAE&entry.906535625=Yazhou%20Xing%20and%20Yang%20Fei%20and%20Yingqing%20He%20and%20Jingye%20Chen%20and%20Jiaxin%20Xie%20and%20Xiaowei%20Chi%20and%20Qifeng%20Chen&entry.1292438233=%20%20Learning%20a%20robust%20video%20Variational%20Autoencoder%20%28VAE%29%20is%20essential%20for%0Areducing%20video%20redundancy%20and%20facilitating%20efficient%20video%20generation.%20Directly%0Aapplying%20image%20VAEs%20to%20individual%20frames%20in%20isolation%20can%20result%20in%20temporal%0Ainconsistencies%20and%20suboptimal%20compression%20rates%20due%20to%20a%20lack%20of%20temporal%0Acompression.%20Existing%20Video%20VAEs%20have%20begun%20to%20address%20temporal%20compression%3B%0Ahowever%2C%20they%20often%20suffer%20from%20inadequate%20reconstruction%20performance.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20and%20powerful%20video%20autoencoder%20capable%20of%0Ahigh-fidelity%20video%20encoding.%20First%2C%20we%20observe%20that%20entangling%20spatial%20and%0Atemporal%20compression%20by%20merely%20extending%20the%20image%20VAE%20to%20a%203D%20VAE%20can%0Aintroduce%20motion%20blur%20and%20detail%20distortion%20artifacts.%20Thus%2C%20we%20propose%0Atemporal-aware%20spatial%20compression%20to%20better%20encode%20and%20decode%20the%20spatial%0Ainformation.%20Additionally%2C%20we%20integrate%20a%20lightweight%20motion%20compression%20model%0Afor%20further%20temporal%20compression.%20Second%2C%20we%20propose%20to%20leverage%20the%20textual%0Ainformation%20inherent%20in%20text-to-video%20datasets%20and%20incorporate%20text%20guidance%0Ainto%20our%20model.%20This%20significantly%20enhances%20reconstruction%20quality%2C%0Aparticularly%20in%20terms%20of%20detail%20preservation%20and%20temporal%20stability.%20Third%2C%20we%0Afurther%20improve%20the%20versatility%20of%20our%20model%20through%20joint%20training%20on%20both%0Aimages%20and%20videos%2C%20which%20not%20only%20enhances%20reconstruction%20quality%20but%20also%0Aenables%20the%20model%20to%20perform%20both%20image%20and%20video%20autoencoding.%20Extensive%0Aevaluations%20against%20strong%20recent%20baselines%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method.%20The%20project%20website%20can%20be%20found%0Aat~%5Chref%7Bhttps%3A//yzxing87.github.io/vae/%7D%7Bhttps%3A//yzxing87.github.io/vae/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17805v1&entry.124074799=Read"},
{"title": "Can Stability be Detrimental? Better Generalization through Gradient\n  Descent Instabilities", "author": "Lawrence Wang and Stephen J. Roberts", "abstract": "  Traditional analyses of gradient descent optimization show that, when the\nlargest eigenvalue of the loss Hessian - often referred to as the sharpness -\nis below a critical learning-rate threshold, then training is 'stable' and\ntraining loss decreases monotonically. Recent studies, however, have suggested\nthat the majority of modern deep neural networks achieve good performance\ndespite operating outside this stable regime. In this work, we demonstrate that\nsuch instabilities, induced by large learning rates, move model parameters\ntoward flatter regions of the loss landscape. Our crucial insight lies in\nnoting that, during these instabilities, the orientation of the Hessian\neigenvectors rotate. This, we conjecture, allows the model to explore regions\nof the loss landscape that display more desirable geometrical properties for\ngeneralization, such as flatness. These rotations are a consequence of network\ndepth, and we prove that for any network with depth > 1, unstable growth in\nparameters cause rotations in the principal components of the Hessian, which\npromote exploration of the parameter space away from unstable directions. Our\nempirical studies reveal an implicit regularization effect in gradient descent\nwith large learning rates operating beyond the stability threshold. We find\nthese lead to excellent generalization performance on modern benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2412.17613v1", "date": "2024-12-23", "relevancy": 2.4209, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5171}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4762}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Stability%20be%20Detrimental%3F%20Better%20Generalization%20through%20Gradient%0A%20%20Descent%20Instabilities&body=Title%3A%20Can%20Stability%20be%20Detrimental%3F%20Better%20Generalization%20through%20Gradient%0A%20%20Descent%20Instabilities%0AAuthor%3A%20Lawrence%20Wang%20and%20Stephen%20J.%20Roberts%0AAbstract%3A%20%20%20Traditional%20analyses%20of%20gradient%20descent%20optimization%20show%20that%2C%20when%20the%0Alargest%20eigenvalue%20of%20the%20loss%20Hessian%20-%20often%20referred%20to%20as%20the%20sharpness%20-%0Ais%20below%20a%20critical%20learning-rate%20threshold%2C%20then%20training%20is%20%27stable%27%20and%0Atraining%20loss%20decreases%20monotonically.%20Recent%20studies%2C%20however%2C%20have%20suggested%0Athat%20the%20majority%20of%20modern%20deep%20neural%20networks%20achieve%20good%20performance%0Adespite%20operating%20outside%20this%20stable%20regime.%20In%20this%20work%2C%20we%20demonstrate%20that%0Asuch%20instabilities%2C%20induced%20by%20large%20learning%20rates%2C%20move%20model%20parameters%0Atoward%20flatter%20regions%20of%20the%20loss%20landscape.%20Our%20crucial%20insight%20lies%20in%0Anoting%20that%2C%20during%20these%20instabilities%2C%20the%20orientation%20of%20the%20Hessian%0Aeigenvectors%20rotate.%20This%2C%20we%20conjecture%2C%20allows%20the%20model%20to%20explore%20regions%0Aof%20the%20loss%20landscape%20that%20display%20more%20desirable%20geometrical%20properties%20for%0Ageneralization%2C%20such%20as%20flatness.%20These%20rotations%20are%20a%20consequence%20of%20network%0Adepth%2C%20and%20we%20prove%20that%20for%20any%20network%20with%20depth%20%3E%201%2C%20unstable%20growth%20in%0Aparameters%20cause%20rotations%20in%20the%20principal%20components%20of%20the%20Hessian%2C%20which%0Apromote%20exploration%20of%20the%20parameter%20space%20away%20from%20unstable%20directions.%20Our%0Aempirical%20studies%20reveal%20an%20implicit%20regularization%20effect%20in%20gradient%20descent%0Awith%20large%20learning%20rates%20operating%20beyond%20the%20stability%20threshold.%20We%20find%0Athese%20lead%20to%20excellent%20generalization%20performance%20on%20modern%20benchmark%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Stability%2520be%2520Detrimental%253F%2520Better%2520Generalization%2520through%2520Gradient%250A%2520%2520Descent%2520Instabilities%26entry.906535625%3DLawrence%2520Wang%2520and%2520Stephen%2520J.%2520Roberts%26entry.1292438233%3D%2520%2520Traditional%2520analyses%2520of%2520gradient%2520descent%2520optimization%2520show%2520that%252C%2520when%2520the%250Alargest%2520eigenvalue%2520of%2520the%2520loss%2520Hessian%2520-%2520often%2520referred%2520to%2520as%2520the%2520sharpness%2520-%250Ais%2520below%2520a%2520critical%2520learning-rate%2520threshold%252C%2520then%2520training%2520is%2520%2527stable%2527%2520and%250Atraining%2520loss%2520decreases%2520monotonically.%2520Recent%2520studies%252C%2520however%252C%2520have%2520suggested%250Athat%2520the%2520majority%2520of%2520modern%2520deep%2520neural%2520networks%2520achieve%2520good%2520performance%250Adespite%2520operating%2520outside%2520this%2520stable%2520regime.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%250Asuch%2520instabilities%252C%2520induced%2520by%2520large%2520learning%2520rates%252C%2520move%2520model%2520parameters%250Atoward%2520flatter%2520regions%2520of%2520the%2520loss%2520landscape.%2520Our%2520crucial%2520insight%2520lies%2520in%250Anoting%2520that%252C%2520during%2520these%2520instabilities%252C%2520the%2520orientation%2520of%2520the%2520Hessian%250Aeigenvectors%2520rotate.%2520This%252C%2520we%2520conjecture%252C%2520allows%2520the%2520model%2520to%2520explore%2520regions%250Aof%2520the%2520loss%2520landscape%2520that%2520display%2520more%2520desirable%2520geometrical%2520properties%2520for%250Ageneralization%252C%2520such%2520as%2520flatness.%2520These%2520rotations%2520are%2520a%2520consequence%2520of%2520network%250Adepth%252C%2520and%2520we%2520prove%2520that%2520for%2520any%2520network%2520with%2520depth%2520%253E%25201%252C%2520unstable%2520growth%2520in%250Aparameters%2520cause%2520rotations%2520in%2520the%2520principal%2520components%2520of%2520the%2520Hessian%252C%2520which%250Apromote%2520exploration%2520of%2520the%2520parameter%2520space%2520away%2520from%2520unstable%2520directions.%2520Our%250Aempirical%2520studies%2520reveal%2520an%2520implicit%2520regularization%2520effect%2520in%2520gradient%2520descent%250Awith%2520large%2520learning%2520rates%2520operating%2520beyond%2520the%2520stability%2520threshold.%2520We%2520find%250Athese%2520lead%2520to%2520excellent%2520generalization%2520performance%2520on%2520modern%2520benchmark%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Stability%20be%20Detrimental%3F%20Better%20Generalization%20through%20Gradient%0A%20%20Descent%20Instabilities&entry.906535625=Lawrence%20Wang%20and%20Stephen%20J.%20Roberts&entry.1292438233=%20%20Traditional%20analyses%20of%20gradient%20descent%20optimization%20show%20that%2C%20when%20the%0Alargest%20eigenvalue%20of%20the%20loss%20Hessian%20-%20often%20referred%20to%20as%20the%20sharpness%20-%0Ais%20below%20a%20critical%20learning-rate%20threshold%2C%20then%20training%20is%20%27stable%27%20and%0Atraining%20loss%20decreases%20monotonically.%20Recent%20studies%2C%20however%2C%20have%20suggested%0Athat%20the%20majority%20of%20modern%20deep%20neural%20networks%20achieve%20good%20performance%0Adespite%20operating%20outside%20this%20stable%20regime.%20In%20this%20work%2C%20we%20demonstrate%20that%0Asuch%20instabilities%2C%20induced%20by%20large%20learning%20rates%2C%20move%20model%20parameters%0Atoward%20flatter%20regions%20of%20the%20loss%20landscape.%20Our%20crucial%20insight%20lies%20in%0Anoting%20that%2C%20during%20these%20instabilities%2C%20the%20orientation%20of%20the%20Hessian%0Aeigenvectors%20rotate.%20This%2C%20we%20conjecture%2C%20allows%20the%20model%20to%20explore%20regions%0Aof%20the%20loss%20landscape%20that%20display%20more%20desirable%20geometrical%20properties%20for%0Ageneralization%2C%20such%20as%20flatness.%20These%20rotations%20are%20a%20consequence%20of%20network%0Adepth%2C%20and%20we%20prove%20that%20for%20any%20network%20with%20depth%20%3E%201%2C%20unstable%20growth%20in%0Aparameters%20cause%20rotations%20in%20the%20principal%20components%20of%20the%20Hessian%2C%20which%0Apromote%20exploration%20of%20the%20parameter%20space%20away%20from%20unstable%20directions.%20Our%0Aempirical%20studies%20reveal%20an%20implicit%20regularization%20effect%20in%20gradient%20descent%0Awith%20large%20learning%20rates%20operating%20beyond%20the%20stability%20threshold.%20We%20find%0Athese%20lead%20to%20excellent%20generalization%20performance%20on%20modern%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17613v1&entry.124074799=Read"},
{"title": "Graph Neural Networks Are Evolutionary Algorithms", "author": "Kaichen Ouyang and Shengwei Fu", "abstract": "  In this paper, we reveal the intrinsic duality between graph neural networks\n(GNNs) and evolutionary algorithms (EAs), bridging two traditionally distinct\nfields. Building on this insight, we propose Graph Neural Evolution (GNE), a\nnovel evolutionary algorithm that models individuals as nodes in a graph and\nleverages designed frequency-domain filters to balance global exploration and\nlocal exploitation. Through the use of these filters, GNE aggregates\nhigh-frequency (diversity-enhancing) and low-frequency (stability-promoting)\ninformation, transforming EAs into interpretable and tunable mechanisms in the\nfrequency domain. Extensive experiments on benchmark functions demonstrate that\nGNE consistently outperforms state-of-the-art algorithms such as GA, DE,\nCMA-ES, SDAES, and RL-SHADE, excelling in complex landscapes, optimal solution\nshifts, and noisy environments. Its robustness, adaptability, and superior\nconvergence highlight its practical and theoretical value. Beyond optimization,\nGNE establishes a conceptual and mathematical foundation linking EAs and GNNs,\noffering new perspectives for both fields. Its framework encourages the\ndevelopment of task-adaptive filters and hybrid approaches for EAs, while its\ninsights can inspire advances in GNNs, such as improved global information\npropagation and mitigation of oversmoothing. GNE's versatility extends to\nsolving challenges in machine learning, including hyperparameter tuning and\nneural architecture search, as well as real-world applications in engineering\nand operations research. By uniting the dynamics of EAs with the structural\ninsights of GNNs, this work provides a foundation for interdisciplinary\ninnovation, paving the way for scalable and interpretable solutions to complex\noptimization problems.\n", "link": "http://arxiv.org/abs/2412.17629v1", "date": "2024-12-23", "relevancy": 2.4175, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5428}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4573}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20Are%20Evolutionary%20Algorithms&body=Title%3A%20Graph%20Neural%20Networks%20Are%20Evolutionary%20Algorithms%0AAuthor%3A%20Kaichen%20Ouyang%20and%20Shengwei%20Fu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20reveal%20the%20intrinsic%20duality%20between%20graph%20neural%20networks%0A%28GNNs%29%20and%20evolutionary%20algorithms%20%28EAs%29%2C%20bridging%20two%20traditionally%20distinct%0Afields.%20Building%20on%20this%20insight%2C%20we%20propose%20Graph%20Neural%20Evolution%20%28GNE%29%2C%20a%0Anovel%20evolutionary%20algorithm%20that%20models%20individuals%20as%20nodes%20in%20a%20graph%20and%0Aleverages%20designed%20frequency-domain%20filters%20to%20balance%20global%20exploration%20and%0Alocal%20exploitation.%20Through%20the%20use%20of%20these%20filters%2C%20GNE%20aggregates%0Ahigh-frequency%20%28diversity-enhancing%29%20and%20low-frequency%20%28stability-promoting%29%0Ainformation%2C%20transforming%20EAs%20into%20interpretable%20and%20tunable%20mechanisms%20in%20the%0Afrequency%20domain.%20Extensive%20experiments%20on%20benchmark%20functions%20demonstrate%20that%0AGNE%20consistently%20outperforms%20state-of-the-art%20algorithms%20such%20as%20GA%2C%20DE%2C%0ACMA-ES%2C%20SDAES%2C%20and%20RL-SHADE%2C%20excelling%20in%20complex%20landscapes%2C%20optimal%20solution%0Ashifts%2C%20and%20noisy%20environments.%20Its%20robustness%2C%20adaptability%2C%20and%20superior%0Aconvergence%20highlight%20its%20practical%20and%20theoretical%20value.%20Beyond%20optimization%2C%0AGNE%20establishes%20a%20conceptual%20and%20mathematical%20foundation%20linking%20EAs%20and%20GNNs%2C%0Aoffering%20new%20perspectives%20for%20both%20fields.%20Its%20framework%20encourages%20the%0Adevelopment%20of%20task-adaptive%20filters%20and%20hybrid%20approaches%20for%20EAs%2C%20while%20its%0Ainsights%20can%20inspire%20advances%20in%20GNNs%2C%20such%20as%20improved%20global%20information%0Apropagation%20and%20mitigation%20of%20oversmoothing.%20GNE%27s%20versatility%20extends%20to%0Asolving%20challenges%20in%20machine%20learning%2C%20including%20hyperparameter%20tuning%20and%0Aneural%20architecture%20search%2C%20as%20well%20as%20real-world%20applications%20in%20engineering%0Aand%20operations%20research.%20By%20uniting%20the%20dynamics%20of%20EAs%20with%20the%20structural%0Ainsights%20of%20GNNs%2C%20this%20work%20provides%20a%20foundation%20for%20interdisciplinary%0Ainnovation%2C%20paving%20the%20way%20for%20scalable%20and%20interpretable%20solutions%20to%20complex%0Aoptimization%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520Are%2520Evolutionary%2520Algorithms%26entry.906535625%3DKaichen%2520Ouyang%2520and%2520Shengwei%2520Fu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520reveal%2520the%2520intrinsic%2520duality%2520between%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520and%2520evolutionary%2520algorithms%2520%2528EAs%2529%252C%2520bridging%2520two%2520traditionally%2520distinct%250Afields.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520Graph%2520Neural%2520Evolution%2520%2528GNE%2529%252C%2520a%250Anovel%2520evolutionary%2520algorithm%2520that%2520models%2520individuals%2520as%2520nodes%2520in%2520a%2520graph%2520and%250Aleverages%2520designed%2520frequency-domain%2520filters%2520to%2520balance%2520global%2520exploration%2520and%250Alocal%2520exploitation.%2520Through%2520the%2520use%2520of%2520these%2520filters%252C%2520GNE%2520aggregates%250Ahigh-frequency%2520%2528diversity-enhancing%2529%2520and%2520low-frequency%2520%2528stability-promoting%2529%250Ainformation%252C%2520transforming%2520EAs%2520into%2520interpretable%2520and%2520tunable%2520mechanisms%2520in%2520the%250Afrequency%2520domain.%2520Extensive%2520experiments%2520on%2520benchmark%2520functions%2520demonstrate%2520that%250AGNE%2520consistently%2520outperforms%2520state-of-the-art%2520algorithms%2520such%2520as%2520GA%252C%2520DE%252C%250ACMA-ES%252C%2520SDAES%252C%2520and%2520RL-SHADE%252C%2520excelling%2520in%2520complex%2520landscapes%252C%2520optimal%2520solution%250Ashifts%252C%2520and%2520noisy%2520environments.%2520Its%2520robustness%252C%2520adaptability%252C%2520and%2520superior%250Aconvergence%2520highlight%2520its%2520practical%2520and%2520theoretical%2520value.%2520Beyond%2520optimization%252C%250AGNE%2520establishes%2520a%2520conceptual%2520and%2520mathematical%2520foundation%2520linking%2520EAs%2520and%2520GNNs%252C%250Aoffering%2520new%2520perspectives%2520for%2520both%2520fields.%2520Its%2520framework%2520encourages%2520the%250Adevelopment%2520of%2520task-adaptive%2520filters%2520and%2520hybrid%2520approaches%2520for%2520EAs%252C%2520while%2520its%250Ainsights%2520can%2520inspire%2520advances%2520in%2520GNNs%252C%2520such%2520as%2520improved%2520global%2520information%250Apropagation%2520and%2520mitigation%2520of%2520oversmoothing.%2520GNE%2527s%2520versatility%2520extends%2520to%250Asolving%2520challenges%2520in%2520machine%2520learning%252C%2520including%2520hyperparameter%2520tuning%2520and%250Aneural%2520architecture%2520search%252C%2520as%2520well%2520as%2520real-world%2520applications%2520in%2520engineering%250Aand%2520operations%2520research.%2520By%2520uniting%2520the%2520dynamics%2520of%2520EAs%2520with%2520the%2520structural%250Ainsights%2520of%2520GNNs%252C%2520this%2520work%2520provides%2520a%2520foundation%2520for%2520interdisciplinary%250Ainnovation%252C%2520paving%2520the%2520way%2520for%2520scalable%2520and%2520interpretable%2520solutions%2520to%2520complex%250Aoptimization%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20Are%20Evolutionary%20Algorithms&entry.906535625=Kaichen%20Ouyang%20and%20Shengwei%20Fu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20reveal%20the%20intrinsic%20duality%20between%20graph%20neural%20networks%0A%28GNNs%29%20and%20evolutionary%20algorithms%20%28EAs%29%2C%20bridging%20two%20traditionally%20distinct%0Afields.%20Building%20on%20this%20insight%2C%20we%20propose%20Graph%20Neural%20Evolution%20%28GNE%29%2C%20a%0Anovel%20evolutionary%20algorithm%20that%20models%20individuals%20as%20nodes%20in%20a%20graph%20and%0Aleverages%20designed%20frequency-domain%20filters%20to%20balance%20global%20exploration%20and%0Alocal%20exploitation.%20Through%20the%20use%20of%20these%20filters%2C%20GNE%20aggregates%0Ahigh-frequency%20%28diversity-enhancing%29%20and%20low-frequency%20%28stability-promoting%29%0Ainformation%2C%20transforming%20EAs%20into%20interpretable%20and%20tunable%20mechanisms%20in%20the%0Afrequency%20domain.%20Extensive%20experiments%20on%20benchmark%20functions%20demonstrate%20that%0AGNE%20consistently%20outperforms%20state-of-the-art%20algorithms%20such%20as%20GA%2C%20DE%2C%0ACMA-ES%2C%20SDAES%2C%20and%20RL-SHADE%2C%20excelling%20in%20complex%20landscapes%2C%20optimal%20solution%0Ashifts%2C%20and%20noisy%20environments.%20Its%20robustness%2C%20adaptability%2C%20and%20superior%0Aconvergence%20highlight%20its%20practical%20and%20theoretical%20value.%20Beyond%20optimization%2C%0AGNE%20establishes%20a%20conceptual%20and%20mathematical%20foundation%20linking%20EAs%20and%20GNNs%2C%0Aoffering%20new%20perspectives%20for%20both%20fields.%20Its%20framework%20encourages%20the%0Adevelopment%20of%20task-adaptive%20filters%20and%20hybrid%20approaches%20for%20EAs%2C%20while%20its%0Ainsights%20can%20inspire%20advances%20in%20GNNs%2C%20such%20as%20improved%20global%20information%0Apropagation%20and%20mitigation%20of%20oversmoothing.%20GNE%27s%20versatility%20extends%20to%0Asolving%20challenges%20in%20machine%20learning%2C%20including%20hyperparameter%20tuning%20and%0Aneural%20architecture%20search%2C%20as%20well%20as%20real-world%20applications%20in%20engineering%0Aand%20operations%20research.%20By%20uniting%20the%20dynamics%20of%20EAs%20with%20the%20structural%0Ainsights%20of%20GNNs%2C%20this%20work%20provides%20a%20foundation%20for%20interdisciplinary%0Ainnovation%2C%20paving%20the%20way%20for%20scalable%20and%20interpretable%20solutions%20to%20complex%0Aoptimization%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17629v1&entry.124074799=Read"},
{"title": "Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and\n  Multi-Domain Testing", "author": "Prakash Aryan", "abstract": "  This paper presents a novel approach to fine-tuning the Qwen2-1.5B model for\nArabic language processing using Quantized Low-Rank Adaptation (QLoRA) on a\nsystem with only 4GB VRAM. We detail the process of adapting this large\nlanguage model to the Arabic domain, using diverse datasets including Bactrian,\nOpenAssistant, and Wikipedia Arabic corpora. Our methodology involves custom\ndata preprocessing, model configuration, and training optimization techniques\nsuch as gradient accumulation and mixed-precision training. We address specific\nchallenges in Arabic NLP, including morphological complexity, dialectal\nvariations, and diacritical mark handling. Experimental results over 10,000\ntraining steps show significant performance improvements, with the final loss\nconverging to 0.1083. We provide comprehensive analysis of GPU memory usage,\ntraining dynamics, and model evaluation across various Arabic language tasks,\nincluding text classification, question answering, and dialect identification.\nThe fine-tuned model demonstrates robustness to input perturbations and\nimproved handling of Arabic-specific linguistic phenomena. This research\ncontributes to multilingual AI by demonstrating a resource-efficient approach\nfor creating specialized language models, potentially democratizing access to\nadvanced NLP technologies for diverse linguistic communities. Our work paves\nthe way for future research in low-resource language adaptation and efficient\nfine-tuning of large language models.\n", "link": "http://arxiv.org/abs/2412.17548v1", "date": "2024-12-23", "relevancy": 2.3991, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource-Aware%20Arabic%20LLM%20Creation%3A%20Model%20Adaptation%2C%20Integration%2C%20and%0A%20%20Multi-Domain%20Testing&body=Title%3A%20Resource-Aware%20Arabic%20LLM%20Creation%3A%20Model%20Adaptation%2C%20Integration%2C%20and%0A%20%20Multi-Domain%20Testing%0AAuthor%3A%20Prakash%20Aryan%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20fine-tuning%20the%20Qwen2-1.5B%20model%20for%0AArabic%20language%20processing%20using%20Quantized%20Low-Rank%20Adaptation%20%28QLoRA%29%20on%20a%0Asystem%20with%20only%204GB%20VRAM.%20We%20detail%20the%20process%20of%20adapting%20this%20large%0Alanguage%20model%20to%20the%20Arabic%20domain%2C%20using%20diverse%20datasets%20including%20Bactrian%2C%0AOpenAssistant%2C%20and%20Wikipedia%20Arabic%20corpora.%20Our%20methodology%20involves%20custom%0Adata%20preprocessing%2C%20model%20configuration%2C%20and%20training%20optimization%20techniques%0Asuch%20as%20gradient%20accumulation%20and%20mixed-precision%20training.%20We%20address%20specific%0Achallenges%20in%20Arabic%20NLP%2C%20including%20morphological%20complexity%2C%20dialectal%0Avariations%2C%20and%20diacritical%20mark%20handling.%20Experimental%20results%20over%2010%2C000%0Atraining%20steps%20show%20significant%20performance%20improvements%2C%20with%20the%20final%20loss%0Aconverging%20to%200.1083.%20We%20provide%20comprehensive%20analysis%20of%20GPU%20memory%20usage%2C%0Atraining%20dynamics%2C%20and%20model%20evaluation%20across%20various%20Arabic%20language%20tasks%2C%0Aincluding%20text%20classification%2C%20question%20answering%2C%20and%20dialect%20identification.%0AThe%20fine-tuned%20model%20demonstrates%20robustness%20to%20input%20perturbations%20and%0Aimproved%20handling%20of%20Arabic-specific%20linguistic%20phenomena.%20This%20research%0Acontributes%20to%20multilingual%20AI%20by%20demonstrating%20a%20resource-efficient%20approach%0Afor%20creating%20specialized%20language%20models%2C%20potentially%20democratizing%20access%20to%0Aadvanced%20NLP%20technologies%20for%20diverse%20linguistic%20communities.%20Our%20work%20paves%0Athe%20way%20for%20future%20research%20in%20low-resource%20language%20adaptation%20and%20efficient%0Afine-tuning%20of%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource-Aware%2520Arabic%2520LLM%2520Creation%253A%2520Model%2520Adaptation%252C%2520Integration%252C%2520and%250A%2520%2520Multi-Domain%2520Testing%26entry.906535625%3DPrakash%2520Aryan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520fine-tuning%2520the%2520Qwen2-1.5B%2520model%2520for%250AArabic%2520language%2520processing%2520using%2520Quantized%2520Low-Rank%2520Adaptation%2520%2528QLoRA%2529%2520on%2520a%250Asystem%2520with%2520only%25204GB%2520VRAM.%2520We%2520detail%2520the%2520process%2520of%2520adapting%2520this%2520large%250Alanguage%2520model%2520to%2520the%2520Arabic%2520domain%252C%2520using%2520diverse%2520datasets%2520including%2520Bactrian%252C%250AOpenAssistant%252C%2520and%2520Wikipedia%2520Arabic%2520corpora.%2520Our%2520methodology%2520involves%2520custom%250Adata%2520preprocessing%252C%2520model%2520configuration%252C%2520and%2520training%2520optimization%2520techniques%250Asuch%2520as%2520gradient%2520accumulation%2520and%2520mixed-precision%2520training.%2520We%2520address%2520specific%250Achallenges%2520in%2520Arabic%2520NLP%252C%2520including%2520morphological%2520complexity%252C%2520dialectal%250Avariations%252C%2520and%2520diacritical%2520mark%2520handling.%2520Experimental%2520results%2520over%252010%252C000%250Atraining%2520steps%2520show%2520significant%2520performance%2520improvements%252C%2520with%2520the%2520final%2520loss%250Aconverging%2520to%25200.1083.%2520We%2520provide%2520comprehensive%2520analysis%2520of%2520GPU%2520memory%2520usage%252C%250Atraining%2520dynamics%252C%2520and%2520model%2520evaluation%2520across%2520various%2520Arabic%2520language%2520tasks%252C%250Aincluding%2520text%2520classification%252C%2520question%2520answering%252C%2520and%2520dialect%2520identification.%250AThe%2520fine-tuned%2520model%2520demonstrates%2520robustness%2520to%2520input%2520perturbations%2520and%250Aimproved%2520handling%2520of%2520Arabic-specific%2520linguistic%2520phenomena.%2520This%2520research%250Acontributes%2520to%2520multilingual%2520AI%2520by%2520demonstrating%2520a%2520resource-efficient%2520approach%250Afor%2520creating%2520specialized%2520language%2520models%252C%2520potentially%2520democratizing%2520access%2520to%250Aadvanced%2520NLP%2520technologies%2520for%2520diverse%2520linguistic%2520communities.%2520Our%2520work%2520paves%250Athe%2520way%2520for%2520future%2520research%2520in%2520low-resource%2520language%2520adaptation%2520and%2520efficient%250Afine-tuning%2520of%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-Aware%20Arabic%20LLM%20Creation%3A%20Model%20Adaptation%2C%20Integration%2C%20and%0A%20%20Multi-Domain%20Testing&entry.906535625=Prakash%20Aryan&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20fine-tuning%20the%20Qwen2-1.5B%20model%20for%0AArabic%20language%20processing%20using%20Quantized%20Low-Rank%20Adaptation%20%28QLoRA%29%20on%20a%0Asystem%20with%20only%204GB%20VRAM.%20We%20detail%20the%20process%20of%20adapting%20this%20large%0Alanguage%20model%20to%20the%20Arabic%20domain%2C%20using%20diverse%20datasets%20including%20Bactrian%2C%0AOpenAssistant%2C%20and%20Wikipedia%20Arabic%20corpora.%20Our%20methodology%20involves%20custom%0Adata%20preprocessing%2C%20model%20configuration%2C%20and%20training%20optimization%20techniques%0Asuch%20as%20gradient%20accumulation%20and%20mixed-precision%20training.%20We%20address%20specific%0Achallenges%20in%20Arabic%20NLP%2C%20including%20morphological%20complexity%2C%20dialectal%0Avariations%2C%20and%20diacritical%20mark%20handling.%20Experimental%20results%20over%2010%2C000%0Atraining%20steps%20show%20significant%20performance%20improvements%2C%20with%20the%20final%20loss%0Aconverging%20to%200.1083.%20We%20provide%20comprehensive%20analysis%20of%20GPU%20memory%20usage%2C%0Atraining%20dynamics%2C%20and%20model%20evaluation%20across%20various%20Arabic%20language%20tasks%2C%0Aincluding%20text%20classification%2C%20question%20answering%2C%20and%20dialect%20identification.%0AThe%20fine-tuned%20model%20demonstrates%20robustness%20to%20input%20perturbations%20and%0Aimproved%20handling%20of%20Arabic-specific%20linguistic%20phenomena.%20This%20research%0Acontributes%20to%20multilingual%20AI%20by%20demonstrating%20a%20resource-efficient%20approach%0Afor%20creating%20specialized%20language%20models%2C%20potentially%20democratizing%20access%20to%0Aadvanced%20NLP%20technologies%20for%20diverse%20linguistic%20communities.%20Our%20work%20paves%0Athe%20way%20for%20future%20research%20in%20low-resource%20language%20adaptation%20and%20efficient%0Afine-tuning%20of%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17548v1&entry.124074799=Read"},
{"title": "Contextual Backpropagation Loops: Amplifying Deep Reasoning with\n  Iterative Top-Down Feedback", "author": "Jacob Fein-Ashley", "abstract": "  Deep neural networks typically rely on a single forward pass for inference,\nwhich can limit their capacity to resolve ambiguous inputs. We introduce\nContextual Backpropagation Loops (CBLs) as an iterative mechanism that\nincorporates top-down feedback to refine intermediate representations, thereby\nimproving accuracy and robustness. This repeated process mirrors how humans\ncontinuously re-interpret sensory information in daily life-by checking and\nre-checking our perceptions using contextual cues. Our results suggest that\nCBLs can offer a straightforward yet powerful way to incorporate such\ncontextual reasoning in modern deep learning architectures.\n", "link": "http://arxiv.org/abs/2412.17737v1", "date": "2024-12-23", "relevancy": 2.3916, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Backpropagation%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%0A%20%20Iterative%20Top-Down%20Feedback&body=Title%3A%20Contextual%20Backpropagation%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%0A%20%20Iterative%20Top-Down%20Feedback%0AAuthor%3A%20Jacob%20Fein-Ashley%0AAbstract%3A%20%20%20Deep%20neural%20networks%20typically%20rely%20on%20a%20single%20forward%20pass%20for%20inference%2C%0Awhich%20can%20limit%20their%20capacity%20to%20resolve%20ambiguous%20inputs.%20We%20introduce%0AContextual%20Backpropagation%20Loops%20%28CBLs%29%20as%20an%20iterative%20mechanism%20that%0Aincorporates%20top-down%20feedback%20to%20refine%20intermediate%20representations%2C%20thereby%0Aimproving%20accuracy%20and%20robustness.%20This%20repeated%20process%20mirrors%20how%20humans%0Acontinuously%20re-interpret%20sensory%20information%20in%20daily%20life-by%20checking%20and%0Are-checking%20our%20perceptions%20using%20contextual%20cues.%20Our%20results%20suggest%20that%0ACBLs%20can%20offer%20a%20straightforward%20yet%20powerful%20way%20to%20incorporate%20such%0Acontextual%20reasoning%20in%20modern%20deep%20learning%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Backpropagation%2520Loops%253A%2520Amplifying%2520Deep%2520Reasoning%2520with%250A%2520%2520Iterative%2520Top-Down%2520Feedback%26entry.906535625%3DJacob%2520Fein-Ashley%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520typically%2520rely%2520on%2520a%2520single%2520forward%2520pass%2520for%2520inference%252C%250Awhich%2520can%2520limit%2520their%2520capacity%2520to%2520resolve%2520ambiguous%2520inputs.%2520We%2520introduce%250AContextual%2520Backpropagation%2520Loops%2520%2528CBLs%2529%2520as%2520an%2520iterative%2520mechanism%2520that%250Aincorporates%2520top-down%2520feedback%2520to%2520refine%2520intermediate%2520representations%252C%2520thereby%250Aimproving%2520accuracy%2520and%2520robustness.%2520This%2520repeated%2520process%2520mirrors%2520how%2520humans%250Acontinuously%2520re-interpret%2520sensory%2520information%2520in%2520daily%2520life-by%2520checking%2520and%250Are-checking%2520our%2520perceptions%2520using%2520contextual%2520cues.%2520Our%2520results%2520suggest%2520that%250ACBLs%2520can%2520offer%2520a%2520straightforward%2520yet%2520powerful%2520way%2520to%2520incorporate%2520such%250Acontextual%2520reasoning%2520in%2520modern%2520deep%2520learning%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Backpropagation%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%0A%20%20Iterative%20Top-Down%20Feedback&entry.906535625=Jacob%20Fein-Ashley&entry.1292438233=%20%20Deep%20neural%20networks%20typically%20rely%20on%20a%20single%20forward%20pass%20for%20inference%2C%0Awhich%20can%20limit%20their%20capacity%20to%20resolve%20ambiguous%20inputs.%20We%20introduce%0AContextual%20Backpropagation%20Loops%20%28CBLs%29%20as%20an%20iterative%20mechanism%20that%0Aincorporates%20top-down%20feedback%20to%20refine%20intermediate%20representations%2C%20thereby%0Aimproving%20accuracy%20and%20robustness.%20This%20repeated%20process%20mirrors%20how%20humans%0Acontinuously%20re-interpret%20sensory%20information%20in%20daily%20life-by%20checking%20and%0Are-checking%20our%20perceptions%20using%20contextual%20cues.%20Our%20results%20suggest%20that%0ACBLs%20can%20offer%20a%20straightforward%20yet%20powerful%20way%20to%20incorporate%20such%0Acontextual%20reasoning%20in%20modern%20deep%20learning%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17737v1&entry.124074799=Read"},
{"title": "VidTwin: Video VAE with Decoupled Structure and Dynamics", "author": "Yuchi Wang and Junliang Guo and Xinyi Xie and Tianyu He and Xu Sun and Jiang Bian", "abstract": "  Recent advancements in video autoencoders (Video AEs) have significantly\nimproved the quality and efficiency of video generation. In this paper, we\npropose a novel and compact video autoencoder, VidTwin, that decouples video\ninto two distinct latent spaces: Structure latent vectors, which capture\noverall content and global movement, and Dynamics latent vectors, which\nrepresent fine-grained details and rapid movements. Specifically, our approach\nleverages an Encoder-Decoder backbone, augmented with two submodules for\nextracting these latent spaces, respectively. The first submodule employs a\nQ-Former to extract low-frequency motion trends, followed by downsampling\nblocks to remove redundant content details. The second averages the latent\nvectors along the spatial dimension to capture rapid motion. Extensive\nexperiments show that VidTwin achieves a high compression rate of 0.20% with\nhigh reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and\nperforms efficiently and effectively in downstream generative tasks. Moreover,\nour model demonstrates explainability and scalability, paving the way for\nfuture research in video latent representation and generation. Our code has\nbeen released at https://github.com/microsoft/VidTok/tree/main/vidtwin.\n", "link": "http://arxiv.org/abs/2412.17726v1", "date": "2024-12-23", "relevancy": 2.3849, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6152}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5932}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidTwin%3A%20Video%20VAE%20with%20Decoupled%20Structure%20and%20Dynamics&body=Title%3A%20VidTwin%3A%20Video%20VAE%20with%20Decoupled%20Structure%20and%20Dynamics%0AAuthor%3A%20Yuchi%20Wang%20and%20Junliang%20Guo%20and%20Xinyi%20Xie%20and%20Tianyu%20He%20and%20Xu%20Sun%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Recent%20advancements%20in%20video%20autoencoders%20%28Video%20AEs%29%20have%20significantly%0Aimproved%20the%20quality%20and%20efficiency%20of%20video%20generation.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20and%20compact%20video%20autoencoder%2C%20VidTwin%2C%20that%20decouples%20video%0Ainto%20two%20distinct%20latent%20spaces%3A%20Structure%20latent%20vectors%2C%20which%20capture%0Aoverall%20content%20and%20global%20movement%2C%20and%20Dynamics%20latent%20vectors%2C%20which%0Arepresent%20fine-grained%20details%20and%20rapid%20movements.%20Specifically%2C%20our%20approach%0Aleverages%20an%20Encoder-Decoder%20backbone%2C%20augmented%20with%20two%20submodules%20for%0Aextracting%20these%20latent%20spaces%2C%20respectively.%20The%20first%20submodule%20employs%20a%0AQ-Former%20to%20extract%20low-frequency%20motion%20trends%2C%20followed%20by%20downsampling%0Ablocks%20to%20remove%20redundant%20content%20details.%20The%20second%20averages%20the%20latent%0Avectors%20along%20the%20spatial%20dimension%20to%20capture%20rapid%20motion.%20Extensive%0Aexperiments%20show%20that%20VidTwin%20achieves%20a%20high%20compression%20rate%20of%200.20%25%20with%0Ahigh%20reconstruction%20quality%20%28PSNR%20of%2028.14%20on%20the%20MCL-JCV%20dataset%29%2C%20and%0Aperforms%20efficiently%20and%20effectively%20in%20downstream%20generative%20tasks.%20Moreover%2C%0Aour%20model%20demonstrates%20explainability%20and%20scalability%2C%20paving%20the%20way%20for%0Afuture%20research%20in%20video%20latent%20representation%20and%20generation.%20Our%20code%20has%0Abeen%20released%20at%20https%3A//github.com/microsoft/VidTok/tree/main/vidtwin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidTwin%253A%2520Video%2520VAE%2520with%2520Decoupled%2520Structure%2520and%2520Dynamics%26entry.906535625%3DYuchi%2520Wang%2520and%2520Junliang%2520Guo%2520and%2520Xinyi%2520Xie%2520and%2520Tianyu%2520He%2520and%2520Xu%2520Sun%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520video%2520autoencoders%2520%2528Video%2520AEs%2529%2520have%2520significantly%250Aimproved%2520the%2520quality%2520and%2520efficiency%2520of%2520video%2520generation.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520and%2520compact%2520video%2520autoencoder%252C%2520VidTwin%252C%2520that%2520decouples%2520video%250Ainto%2520two%2520distinct%2520latent%2520spaces%253A%2520Structure%2520latent%2520vectors%252C%2520which%2520capture%250Aoverall%2520content%2520and%2520global%2520movement%252C%2520and%2520Dynamics%2520latent%2520vectors%252C%2520which%250Arepresent%2520fine-grained%2520details%2520and%2520rapid%2520movements.%2520Specifically%252C%2520our%2520approach%250Aleverages%2520an%2520Encoder-Decoder%2520backbone%252C%2520augmented%2520with%2520two%2520submodules%2520for%250Aextracting%2520these%2520latent%2520spaces%252C%2520respectively.%2520The%2520first%2520submodule%2520employs%2520a%250AQ-Former%2520to%2520extract%2520low-frequency%2520motion%2520trends%252C%2520followed%2520by%2520downsampling%250Ablocks%2520to%2520remove%2520redundant%2520content%2520details.%2520The%2520second%2520averages%2520the%2520latent%250Avectors%2520along%2520the%2520spatial%2520dimension%2520to%2520capture%2520rapid%2520motion.%2520Extensive%250Aexperiments%2520show%2520that%2520VidTwin%2520achieves%2520a%2520high%2520compression%2520rate%2520of%25200.20%2525%2520with%250Ahigh%2520reconstruction%2520quality%2520%2528PSNR%2520of%252028.14%2520on%2520the%2520MCL-JCV%2520dataset%2529%252C%2520and%250Aperforms%2520efficiently%2520and%2520effectively%2520in%2520downstream%2520generative%2520tasks.%2520Moreover%252C%250Aour%2520model%2520demonstrates%2520explainability%2520and%2520scalability%252C%2520paving%2520the%2520way%2520for%250Afuture%2520research%2520in%2520video%2520latent%2520representation%2520and%2520generation.%2520Our%2520code%2520has%250Abeen%2520released%2520at%2520https%253A//github.com/microsoft/VidTok/tree/main/vidtwin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidTwin%3A%20Video%20VAE%20with%20Decoupled%20Structure%20and%20Dynamics&entry.906535625=Yuchi%20Wang%20and%20Junliang%20Guo%20and%20Xinyi%20Xie%20and%20Tianyu%20He%20and%20Xu%20Sun%20and%20Jiang%20Bian&entry.1292438233=%20%20Recent%20advancements%20in%20video%20autoencoders%20%28Video%20AEs%29%20have%20significantly%0Aimproved%20the%20quality%20and%20efficiency%20of%20video%20generation.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20and%20compact%20video%20autoencoder%2C%20VidTwin%2C%20that%20decouples%20video%0Ainto%20two%20distinct%20latent%20spaces%3A%20Structure%20latent%20vectors%2C%20which%20capture%0Aoverall%20content%20and%20global%20movement%2C%20and%20Dynamics%20latent%20vectors%2C%20which%0Arepresent%20fine-grained%20details%20and%20rapid%20movements.%20Specifically%2C%20our%20approach%0Aleverages%20an%20Encoder-Decoder%20backbone%2C%20augmented%20with%20two%20submodules%20for%0Aextracting%20these%20latent%20spaces%2C%20respectively.%20The%20first%20submodule%20employs%20a%0AQ-Former%20to%20extract%20low-frequency%20motion%20trends%2C%20followed%20by%20downsampling%0Ablocks%20to%20remove%20redundant%20content%20details.%20The%20second%20averages%20the%20latent%0Avectors%20along%20the%20spatial%20dimension%20to%20capture%20rapid%20motion.%20Extensive%0Aexperiments%20show%20that%20VidTwin%20achieves%20a%20high%20compression%20rate%20of%200.20%25%20with%0Ahigh%20reconstruction%20quality%20%28PSNR%20of%2028.14%20on%20the%20MCL-JCV%20dataset%29%2C%20and%0Aperforms%20efficiently%20and%20effectively%20in%20downstream%20generative%20tasks.%20Moreover%2C%0Aour%20model%20demonstrates%20explainability%20and%20scalability%2C%20paving%20the%20way%20for%0Afuture%20research%20in%20video%20latent%20representation%20and%20generation.%20Our%20code%20has%0Abeen%20released%20at%20https%3A//github.com/microsoft/VidTok/tree/main/vidtwin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17726v1&entry.124074799=Read"},
{"title": "Enhanced Generative Data Augmentation for Semantic Segmentation via\n  Stronger Guidance", "author": "Quang-Huy Che and Duc-Tri Le and Bich-Nga Pham and Duc-Khai Lam and Vinh-Tiep Nguyen", "abstract": "  Data augmentation is crucial for pixel-wise annotation tasks like semantic\nsegmentation, where labeling requires significant effort and intensive labor.\nTraditional methods, involving simple transformations such as rotations and\nflips, create new images but often lack diversity along key semantic dimensions\nand fail to alter high-level semantic properties. To address this issue,\ngenerative models have emerged as an effective solution for augmenting data by\ngenerating synthetic images. Controllable Generative models offer data\naugmentation methods for semantic segmentation tasks by using prompts and\nvisual references from the original image. However, these models face\nchallenges in generating synthetic images that accurately reflect the content\nand structure of the original image due to difficulties in creating effective\nprompts and visual references. In this work, we introduce an effective data\naugmentation pipeline for semantic segmentation using Controllable Diffusion\nmodel. Our proposed method includes efficient prompt generation using\n\\textit{Class-Prompt Appending} and \\textit{Visual Prior Blending} to enhance\nattention to labeled classes in real images, allowing the pipeline to generate\na precise number of augmented images while preserving the structure of\nsegmentation-labeled classes. In addition, we implement a \\textit{class\nbalancing algorithm} to ensure a balanced training dataset when merging the\nsynthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline\ndemonstrates its effectiveness in generating high-quality synthetic images for\nsemantic segmentation. Our code is available at\n\\href{https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this\nhttps URL}.\n", "link": "http://arxiv.org/abs/2409.06002v3", "date": "2024-12-23", "relevancy": 2.382, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6113}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Generative%20Data%20Augmentation%20for%20Semantic%20Segmentation%20via%0A%20%20Stronger%20Guidance&body=Title%3A%20Enhanced%20Generative%20Data%20Augmentation%20for%20Semantic%20Segmentation%20via%0A%20%20Stronger%20Guidance%0AAuthor%3A%20Quang-Huy%20Che%20and%20Duc-Tri%20Le%20and%20Bich-Nga%20Pham%20and%20Duc-Khai%20Lam%20and%20Vinh-Tiep%20Nguyen%0AAbstract%3A%20%20%20Data%20augmentation%20is%20crucial%20for%20pixel-wise%20annotation%20tasks%20like%20semantic%0Asegmentation%2C%20where%20labeling%20requires%20significant%20effort%20and%20intensive%20labor.%0ATraditional%20methods%2C%20involving%20simple%20transformations%20such%20as%20rotations%20and%0Aflips%2C%20create%20new%20images%20but%20often%20lack%20diversity%20along%20key%20semantic%20dimensions%0Aand%20fail%20to%20alter%20high-level%20semantic%20properties.%20To%20address%20this%20issue%2C%0Agenerative%20models%20have%20emerged%20as%20an%20effective%20solution%20for%20augmenting%20data%20by%0Agenerating%20synthetic%20images.%20Controllable%20Generative%20models%20offer%20data%0Aaugmentation%20methods%20for%20semantic%20segmentation%20tasks%20by%20using%20prompts%20and%0Avisual%20references%20from%20the%20original%20image.%20However%2C%20these%20models%20face%0Achallenges%20in%20generating%20synthetic%20images%20that%20accurately%20reflect%20the%20content%0Aand%20structure%20of%20the%20original%20image%20due%20to%20difficulties%20in%20creating%20effective%0Aprompts%20and%20visual%20references.%20In%20this%20work%2C%20we%20introduce%20an%20effective%20data%0Aaugmentation%20pipeline%20for%20semantic%20segmentation%20using%20Controllable%20Diffusion%0Amodel.%20Our%20proposed%20method%20includes%20efficient%20prompt%20generation%20using%0A%5Ctextit%7BClass-Prompt%20Appending%7D%20and%20%5Ctextit%7BVisual%20Prior%20Blending%7D%20to%20enhance%0Aattention%20to%20labeled%20classes%20in%20real%20images%2C%20allowing%20the%20pipeline%20to%20generate%0Aa%20precise%20number%20of%20augmented%20images%20while%20preserving%20the%20structure%20of%0Asegmentation-labeled%20classes.%20In%20addition%2C%20we%20implement%20a%20%5Ctextit%7Bclass%0Abalancing%20algorithm%7D%20to%20ensure%20a%20balanced%20training%20dataset%20when%20merging%20the%0Asynthetic%20and%20original%20images.%20Evaluation%20on%20PASCAL%20VOC%20datasets%2C%20our%20pipeline%0Ademonstrates%20its%20effectiveness%20in%20generating%20high-quality%20synthetic%20images%20for%0Asemantic%20segmentation.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance%7D%7Bthis%0Ahttps%20URL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06002v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Generative%2520Data%2520Augmentation%2520for%2520Semantic%2520Segmentation%2520via%250A%2520%2520Stronger%2520Guidance%26entry.906535625%3DQuang-Huy%2520Che%2520and%2520Duc-Tri%2520Le%2520and%2520Bich-Nga%2520Pham%2520and%2520Duc-Khai%2520Lam%2520and%2520Vinh-Tiep%2520Nguyen%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520crucial%2520for%2520pixel-wise%2520annotation%2520tasks%2520like%2520semantic%250Asegmentation%252C%2520where%2520labeling%2520requires%2520significant%2520effort%2520and%2520intensive%2520labor.%250ATraditional%2520methods%252C%2520involving%2520simple%2520transformations%2520such%2520as%2520rotations%2520and%250Aflips%252C%2520create%2520new%2520images%2520but%2520often%2520lack%2520diversity%2520along%2520key%2520semantic%2520dimensions%250Aand%2520fail%2520to%2520alter%2520high-level%2520semantic%2520properties.%2520To%2520address%2520this%2520issue%252C%250Agenerative%2520models%2520have%2520emerged%2520as%2520an%2520effective%2520solution%2520for%2520augmenting%2520data%2520by%250Agenerating%2520synthetic%2520images.%2520Controllable%2520Generative%2520models%2520offer%2520data%250Aaugmentation%2520methods%2520for%2520semantic%2520segmentation%2520tasks%2520by%2520using%2520prompts%2520and%250Avisual%2520references%2520from%2520the%2520original%2520image.%2520However%252C%2520these%2520models%2520face%250Achallenges%2520in%2520generating%2520synthetic%2520images%2520that%2520accurately%2520reflect%2520the%2520content%250Aand%2520structure%2520of%2520the%2520original%2520image%2520due%2520to%2520difficulties%2520in%2520creating%2520effective%250Aprompts%2520and%2520visual%2520references.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520effective%2520data%250Aaugmentation%2520pipeline%2520for%2520semantic%2520segmentation%2520using%2520Controllable%2520Diffusion%250Amodel.%2520Our%2520proposed%2520method%2520includes%2520efficient%2520prompt%2520generation%2520using%250A%255Ctextit%257BClass-Prompt%2520Appending%257D%2520and%2520%255Ctextit%257BVisual%2520Prior%2520Blending%257D%2520to%2520enhance%250Aattention%2520to%2520labeled%2520classes%2520in%2520real%2520images%252C%2520allowing%2520the%2520pipeline%2520to%2520generate%250Aa%2520precise%2520number%2520of%2520augmented%2520images%2520while%2520preserving%2520the%2520structure%2520of%250Asegmentation-labeled%2520classes.%2520In%2520addition%252C%2520we%2520implement%2520a%2520%255Ctextit%257Bclass%250Abalancing%2520algorithm%257D%2520to%2520ensure%2520a%2520balanced%2520training%2520dataset%2520when%2520merging%2520the%250Asynthetic%2520and%2520original%2520images.%2520Evaluation%2520on%2520PASCAL%2520VOC%2520datasets%252C%2520our%2520pipeline%250Ademonstrates%2520its%2520effectiveness%2520in%2520generating%2520high-quality%2520synthetic%2520images%2520for%250Asemantic%2520segmentation.%2520Our%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance%257D%257Bthis%250Ahttps%2520URL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06002v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Generative%20Data%20Augmentation%20for%20Semantic%20Segmentation%20via%0A%20%20Stronger%20Guidance&entry.906535625=Quang-Huy%20Che%20and%20Duc-Tri%20Le%20and%20Bich-Nga%20Pham%20and%20Duc-Khai%20Lam%20and%20Vinh-Tiep%20Nguyen&entry.1292438233=%20%20Data%20augmentation%20is%20crucial%20for%20pixel-wise%20annotation%20tasks%20like%20semantic%0Asegmentation%2C%20where%20labeling%20requires%20significant%20effort%20and%20intensive%20labor.%0ATraditional%20methods%2C%20involving%20simple%20transformations%20such%20as%20rotations%20and%0Aflips%2C%20create%20new%20images%20but%20often%20lack%20diversity%20along%20key%20semantic%20dimensions%0Aand%20fail%20to%20alter%20high-level%20semantic%20properties.%20To%20address%20this%20issue%2C%0Agenerative%20models%20have%20emerged%20as%20an%20effective%20solution%20for%20augmenting%20data%20by%0Agenerating%20synthetic%20images.%20Controllable%20Generative%20models%20offer%20data%0Aaugmentation%20methods%20for%20semantic%20segmentation%20tasks%20by%20using%20prompts%20and%0Avisual%20references%20from%20the%20original%20image.%20However%2C%20these%20models%20face%0Achallenges%20in%20generating%20synthetic%20images%20that%20accurately%20reflect%20the%20content%0Aand%20structure%20of%20the%20original%20image%20due%20to%20difficulties%20in%20creating%20effective%0Aprompts%20and%20visual%20references.%20In%20this%20work%2C%20we%20introduce%20an%20effective%20data%0Aaugmentation%20pipeline%20for%20semantic%20segmentation%20using%20Controllable%20Diffusion%0Amodel.%20Our%20proposed%20method%20includes%20efficient%20prompt%20generation%20using%0A%5Ctextit%7BClass-Prompt%20Appending%7D%20and%20%5Ctextit%7BVisual%20Prior%20Blending%7D%20to%20enhance%0Aattention%20to%20labeled%20classes%20in%20real%20images%2C%20allowing%20the%20pipeline%20to%20generate%0Aa%20precise%20number%20of%20augmented%20images%20while%20preserving%20the%20structure%20of%0Asegmentation-labeled%20classes.%20In%20addition%2C%20we%20implement%20a%20%5Ctextit%7Bclass%0Abalancing%20algorithm%7D%20to%20ensure%20a%20balanced%20training%20dataset%20when%20merging%20the%0Asynthetic%20and%20original%20images.%20Evaluation%20on%20PASCAL%20VOC%20datasets%2C%20our%20pipeline%0Ademonstrates%20its%20effectiveness%20in%20generating%20high-quality%20synthetic%20images%20for%0Asemantic%20segmentation.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance%7D%7Bthis%0Ahttps%20URL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06002v3&entry.124074799=Read"},
{"title": "HumanVBench: Exploring Human-Centric Video Understanding Capabilities of\n  MLLMs with Synthetic Benchmark Data", "author": "Ting Zhou and Daoyuan Chen and Qirui Jiao and Bolin Ding and Yaliang Li and Ying Shen", "abstract": "  In the domain of Multimodal Large Language Models (MLLMs), achieving\nhuman-centric video understanding remains a formidable challenge. Existing\nbenchmarks primarily emphasize object and action recognition, often neglecting\nthe intricate nuances of human emotions, behaviors, and speech visual alignment\nwithin video content. We present HumanVBench, an innovative benchmark\nmeticulously crafted to bridge these gaps in the evaluation of video MLLMs.\nHumanVBench comprises 17 carefully designed tasks that explore two primary\ndimensions: inner emotion and outer manifestations, spanning static and\ndynamic, basic and complex, as well as single-modal and cross-modal aspects.\nWith two advanced automated pipelines for video annotation and\ndistractor-included QA generation, HumanVBench utilizes diverse\nstate-of-the-art (SOTA) techniques to streamline benchmark data synthesis and\nquality assessment, minimizing human annotation dependency tailored to\nhuman-centric multimodal attributes. A comprehensive evaluation across 16 SOTA\nvideo MLLMs reveals notable limitations in current performance, especially in\ncross-modal and temporal alignment, underscoring the necessity for further\nrefinement toward achieving more human-like understanding. HumanVBench is\nopen-sourced to facilitate future advancements and real-world applications in\nvideo MLLMs.\n", "link": "http://arxiv.org/abs/2412.17574v1", "date": "2024-12-23", "relevancy": 2.3719, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5972}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanVBench%3A%20Exploring%20Human-Centric%20Video%20Understanding%20Capabilities%20of%0A%20%20MLLMs%20with%20Synthetic%20Benchmark%20Data&body=Title%3A%20HumanVBench%3A%20Exploring%20Human-Centric%20Video%20Understanding%20Capabilities%20of%0A%20%20MLLMs%20with%20Synthetic%20Benchmark%20Data%0AAuthor%3A%20Ting%20Zhou%20and%20Daoyuan%20Chen%20and%20Qirui%20Jiao%20and%20Bolin%20Ding%20and%20Yaliang%20Li%20and%20Ying%20Shen%0AAbstract%3A%20%20%20In%20the%20domain%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20achieving%0Ahuman-centric%20video%20understanding%20remains%20a%20formidable%20challenge.%20Existing%0Abenchmarks%20primarily%20emphasize%20object%20and%20action%20recognition%2C%20often%20neglecting%0Athe%20intricate%20nuances%20of%20human%20emotions%2C%20behaviors%2C%20and%20speech%20visual%20alignment%0Awithin%20video%20content.%20We%20present%20HumanVBench%2C%20an%20innovative%20benchmark%0Ameticulously%20crafted%20to%20bridge%20these%20gaps%20in%20the%20evaluation%20of%20video%20MLLMs.%0AHumanVBench%20comprises%2017%20carefully%20designed%20tasks%20that%20explore%20two%20primary%0Adimensions%3A%20inner%20emotion%20and%20outer%20manifestations%2C%20spanning%20static%20and%0Adynamic%2C%20basic%20and%20complex%2C%20as%20well%20as%20single-modal%20and%20cross-modal%20aspects.%0AWith%20two%20advanced%20automated%20pipelines%20for%20video%20annotation%20and%0Adistractor-included%20QA%20generation%2C%20HumanVBench%20utilizes%20diverse%0Astate-of-the-art%20%28SOTA%29%20techniques%20to%20streamline%20benchmark%20data%20synthesis%20and%0Aquality%20assessment%2C%20minimizing%20human%20annotation%20dependency%20tailored%20to%0Ahuman-centric%20multimodal%20attributes.%20A%20comprehensive%20evaluation%20across%2016%20SOTA%0Avideo%20MLLMs%20reveals%20notable%20limitations%20in%20current%20performance%2C%20especially%20in%0Across-modal%20and%20temporal%20alignment%2C%20underscoring%20the%20necessity%20for%20further%0Arefinement%20toward%20achieving%20more%20human-like%20understanding.%20HumanVBench%20is%0Aopen-sourced%20to%20facilitate%20future%20advancements%20and%20real-world%20applications%20in%0Avideo%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanVBench%253A%2520Exploring%2520Human-Centric%2520Video%2520Understanding%2520Capabilities%2520of%250A%2520%2520MLLMs%2520with%2520Synthetic%2520Benchmark%2520Data%26entry.906535625%3DTing%2520Zhou%2520and%2520Daoyuan%2520Chen%2520and%2520Qirui%2520Jiao%2520and%2520Bolin%2520Ding%2520and%2520Yaliang%2520Li%2520and%2520Ying%2520Shen%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520achieving%250Ahuman-centric%2520video%2520understanding%2520remains%2520a%2520formidable%2520challenge.%2520Existing%250Abenchmarks%2520primarily%2520emphasize%2520object%2520and%2520action%2520recognition%252C%2520often%2520neglecting%250Athe%2520intricate%2520nuances%2520of%2520human%2520emotions%252C%2520behaviors%252C%2520and%2520speech%2520visual%2520alignment%250Awithin%2520video%2520content.%2520We%2520present%2520HumanVBench%252C%2520an%2520innovative%2520benchmark%250Ameticulously%2520crafted%2520to%2520bridge%2520these%2520gaps%2520in%2520the%2520evaluation%2520of%2520video%2520MLLMs.%250AHumanVBench%2520comprises%252017%2520carefully%2520designed%2520tasks%2520that%2520explore%2520two%2520primary%250Adimensions%253A%2520inner%2520emotion%2520and%2520outer%2520manifestations%252C%2520spanning%2520static%2520and%250Adynamic%252C%2520basic%2520and%2520complex%252C%2520as%2520well%2520as%2520single-modal%2520and%2520cross-modal%2520aspects.%250AWith%2520two%2520advanced%2520automated%2520pipelines%2520for%2520video%2520annotation%2520and%250Adistractor-included%2520QA%2520generation%252C%2520HumanVBench%2520utilizes%2520diverse%250Astate-of-the-art%2520%2528SOTA%2529%2520techniques%2520to%2520streamline%2520benchmark%2520data%2520synthesis%2520and%250Aquality%2520assessment%252C%2520minimizing%2520human%2520annotation%2520dependency%2520tailored%2520to%250Ahuman-centric%2520multimodal%2520attributes.%2520A%2520comprehensive%2520evaluation%2520across%252016%2520SOTA%250Avideo%2520MLLMs%2520reveals%2520notable%2520limitations%2520in%2520current%2520performance%252C%2520especially%2520in%250Across-modal%2520and%2520temporal%2520alignment%252C%2520underscoring%2520the%2520necessity%2520for%2520further%250Arefinement%2520toward%2520achieving%2520more%2520human-like%2520understanding.%2520HumanVBench%2520is%250Aopen-sourced%2520to%2520facilitate%2520future%2520advancements%2520and%2520real-world%2520applications%2520in%250Avideo%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanVBench%3A%20Exploring%20Human-Centric%20Video%20Understanding%20Capabilities%20of%0A%20%20MLLMs%20with%20Synthetic%20Benchmark%20Data&entry.906535625=Ting%20Zhou%20and%20Daoyuan%20Chen%20and%20Qirui%20Jiao%20and%20Bolin%20Ding%20and%20Yaliang%20Li%20and%20Ying%20Shen&entry.1292438233=%20%20In%20the%20domain%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20achieving%0Ahuman-centric%20video%20understanding%20remains%20a%20formidable%20challenge.%20Existing%0Abenchmarks%20primarily%20emphasize%20object%20and%20action%20recognition%2C%20often%20neglecting%0Athe%20intricate%20nuances%20of%20human%20emotions%2C%20behaviors%2C%20and%20speech%20visual%20alignment%0Awithin%20video%20content.%20We%20present%20HumanVBench%2C%20an%20innovative%20benchmark%0Ameticulously%20crafted%20to%20bridge%20these%20gaps%20in%20the%20evaluation%20of%20video%20MLLMs.%0AHumanVBench%20comprises%2017%20carefully%20designed%20tasks%20that%20explore%20two%20primary%0Adimensions%3A%20inner%20emotion%20and%20outer%20manifestations%2C%20spanning%20static%20and%0Adynamic%2C%20basic%20and%20complex%2C%20as%20well%20as%20single-modal%20and%20cross-modal%20aspects.%0AWith%20two%20advanced%20automated%20pipelines%20for%20video%20annotation%20and%0Adistractor-included%20QA%20generation%2C%20HumanVBench%20utilizes%20diverse%0Astate-of-the-art%20%28SOTA%29%20techniques%20to%20streamline%20benchmark%20data%20synthesis%20and%0Aquality%20assessment%2C%20minimizing%20human%20annotation%20dependency%20tailored%20to%0Ahuman-centric%20multimodal%20attributes.%20A%20comprehensive%20evaluation%20across%2016%20SOTA%0Avideo%20MLLMs%20reveals%20notable%20limitations%20in%20current%20performance%2C%20especially%20in%0Across-modal%20and%20temporal%20alignment%2C%20underscoring%20the%20necessity%20for%20further%0Arefinement%20toward%20achieving%20more%20human-like%20understanding.%20HumanVBench%20is%0Aopen-sourced%20to%20facilitate%20future%20advancements%20and%20real-world%20applications%20in%0Avideo%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17574v1&entry.124074799=Read"},
{"title": "MRANet: A Modified Residual Attention Networks for Lung and Colon Cancer\n  Classification", "author": "Diponkor Bala and S M Rakib Ul Karim and Rownak Ara Rasul", "abstract": "  Lung and colon cancers are predominant contributors to cancer mortality.\nEarly and accurate diagnosis is crucial for effective treatment. By utilizing\nimaging technology in different image detection, learning models have shown\npromise in automating cancer classification from histopathological images. This\nincludes the histopathological diagnosis, an important factor in cancer type\nidentification. This research focuses on creating a high-efficiency\ndeep-learning model for identifying lung and colon cancer from\nhistopathological images. We proposed a novel approach based on a modified\nresidual attention network architecture. The model was trained on a dataset of\n25,000 high-resolution histopathological images across several classes. Our\nproposed model achieved an exceptional accuracy of 99.30%, 96.63%, and 97.56%\nfor two, three, and five classes, respectively; those are outperforming other\nstate-of-the-art architectures. This study presents a highly accurate deep\nlearning model for lung and colon cancer classification. The superior\nperformance of our proposed model addresses a critical need in medical AI\napplications.\n", "link": "http://arxiv.org/abs/2412.17700v1", "date": "2024-12-23", "relevancy": 2.3584, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4769}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4753}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRANet%3A%20A%20Modified%20Residual%20Attention%20Networks%20for%20Lung%20and%20Colon%20Cancer%0A%20%20Classification&body=Title%3A%20MRANet%3A%20A%20Modified%20Residual%20Attention%20Networks%20for%20Lung%20and%20Colon%20Cancer%0A%20%20Classification%0AAuthor%3A%20Diponkor%20Bala%20and%20S%20M%20Rakib%20Ul%20Karim%20and%20Rownak%20Ara%20Rasul%0AAbstract%3A%20%20%20Lung%20and%20colon%20cancers%20are%20predominant%20contributors%20to%20cancer%20mortality.%0AEarly%20and%20accurate%20diagnosis%20is%20crucial%20for%20effective%20treatment.%20By%20utilizing%0Aimaging%20technology%20in%20different%20image%20detection%2C%20learning%20models%20have%20shown%0Apromise%20in%20automating%20cancer%20classification%20from%20histopathological%20images.%20This%0Aincludes%20the%20histopathological%20diagnosis%2C%20an%20important%20factor%20in%20cancer%20type%0Aidentification.%20This%20research%20focuses%20on%20creating%20a%20high-efficiency%0Adeep-learning%20model%20for%20identifying%20lung%20and%20colon%20cancer%20from%0Ahistopathological%20images.%20We%20proposed%20a%20novel%20approach%20based%20on%20a%20modified%0Aresidual%20attention%20network%20architecture.%20The%20model%20was%20trained%20on%20a%20dataset%20of%0A25%2C000%20high-resolution%20histopathological%20images%20across%20several%20classes.%20Our%0Aproposed%20model%20achieved%20an%20exceptional%20accuracy%20of%2099.30%25%2C%2096.63%25%2C%20and%2097.56%25%0Afor%20two%2C%20three%2C%20and%20five%20classes%2C%20respectively%3B%20those%20are%20outperforming%20other%0Astate-of-the-art%20architectures.%20This%20study%20presents%20a%20highly%20accurate%20deep%0Alearning%20model%20for%20lung%20and%20colon%20cancer%20classification.%20The%20superior%0Aperformance%20of%20our%20proposed%20model%20addresses%20a%20critical%20need%20in%20medical%20AI%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRANet%253A%2520A%2520Modified%2520Residual%2520Attention%2520Networks%2520for%2520Lung%2520and%2520Colon%2520Cancer%250A%2520%2520Classification%26entry.906535625%3DDiponkor%2520Bala%2520and%2520S%2520M%2520Rakib%2520Ul%2520Karim%2520and%2520Rownak%2520Ara%2520Rasul%26entry.1292438233%3D%2520%2520Lung%2520and%2520colon%2520cancers%2520are%2520predominant%2520contributors%2520to%2520cancer%2520mortality.%250AEarly%2520and%2520accurate%2520diagnosis%2520is%2520crucial%2520for%2520effective%2520treatment.%2520By%2520utilizing%250Aimaging%2520technology%2520in%2520different%2520image%2520detection%252C%2520learning%2520models%2520have%2520shown%250Apromise%2520in%2520automating%2520cancer%2520classification%2520from%2520histopathological%2520images.%2520This%250Aincludes%2520the%2520histopathological%2520diagnosis%252C%2520an%2520important%2520factor%2520in%2520cancer%2520type%250Aidentification.%2520This%2520research%2520focuses%2520on%2520creating%2520a%2520high-efficiency%250Adeep-learning%2520model%2520for%2520identifying%2520lung%2520and%2520colon%2520cancer%2520from%250Ahistopathological%2520images.%2520We%2520proposed%2520a%2520novel%2520approach%2520based%2520on%2520a%2520modified%250Aresidual%2520attention%2520network%2520architecture.%2520The%2520model%2520was%2520trained%2520on%2520a%2520dataset%2520of%250A25%252C000%2520high-resolution%2520histopathological%2520images%2520across%2520several%2520classes.%2520Our%250Aproposed%2520model%2520achieved%2520an%2520exceptional%2520accuracy%2520of%252099.30%2525%252C%252096.63%2525%252C%2520and%252097.56%2525%250Afor%2520two%252C%2520three%252C%2520and%2520five%2520classes%252C%2520respectively%253B%2520those%2520are%2520outperforming%2520other%250Astate-of-the-art%2520architectures.%2520This%2520study%2520presents%2520a%2520highly%2520accurate%2520deep%250Alearning%2520model%2520for%2520lung%2520and%2520colon%2520cancer%2520classification.%2520The%2520superior%250Aperformance%2520of%2520our%2520proposed%2520model%2520addresses%2520a%2520critical%2520need%2520in%2520medical%2520AI%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRANet%3A%20A%20Modified%20Residual%20Attention%20Networks%20for%20Lung%20and%20Colon%20Cancer%0A%20%20Classification&entry.906535625=Diponkor%20Bala%20and%20S%20M%20Rakib%20Ul%20Karim%20and%20Rownak%20Ara%20Rasul&entry.1292438233=%20%20Lung%20and%20colon%20cancers%20are%20predominant%20contributors%20to%20cancer%20mortality.%0AEarly%20and%20accurate%20diagnosis%20is%20crucial%20for%20effective%20treatment.%20By%20utilizing%0Aimaging%20technology%20in%20different%20image%20detection%2C%20learning%20models%20have%20shown%0Apromise%20in%20automating%20cancer%20classification%20from%20histopathological%20images.%20This%0Aincludes%20the%20histopathological%20diagnosis%2C%20an%20important%20factor%20in%20cancer%20type%0Aidentification.%20This%20research%20focuses%20on%20creating%20a%20high-efficiency%0Adeep-learning%20model%20for%20identifying%20lung%20and%20colon%20cancer%20from%0Ahistopathological%20images.%20We%20proposed%20a%20novel%20approach%20based%20on%20a%20modified%0Aresidual%20attention%20network%20architecture.%20The%20model%20was%20trained%20on%20a%20dataset%20of%0A25%2C000%20high-resolution%20histopathological%20images%20across%20several%20classes.%20Our%0Aproposed%20model%20achieved%20an%20exceptional%20accuracy%20of%2099.30%25%2C%2096.63%25%2C%20and%2097.56%25%0Afor%20two%2C%20three%2C%20and%20five%20classes%2C%20respectively%3B%20those%20are%20outperforming%20other%0Astate-of-the-art%20architectures.%20This%20study%20presents%20a%20highly%20accurate%20deep%0Alearning%20model%20for%20lung%20and%20colon%20cancer%20classification.%20The%20superior%0Aperformance%20of%20our%20proposed%20model%20addresses%20a%20critical%20need%20in%20medical%20AI%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17700v1&entry.124074799=Read"},
{"title": "Retention Score: Quantifying Jailbreak Risks for Vision Language Models", "author": "Zaitang Li and Pin-Yu Chen and Tsung-Yi Ho", "abstract": "  The emergence of Vision-Language Models (VLMs) is a significant advancement\nin integrating computer vision with Large Language Models (LLMs) to enhance\nmulti-modal machine learning capabilities. However, this progress has also made\nVLMs vulnerable to sophisticated adversarial attacks, raising concerns about\ntheir reliability. The objective of this paper is to assess the resilience of\nVLMs against jailbreak attacks that can compromise model safety compliance and\nresult in harmful outputs. To evaluate a VLM's ability to maintain its\nrobustness against adversarial input perturbations, we propose a novel metric\ncalled the \\textbf{Retention Score}. Retention Score is a multi-modal\nevaluation metric that includes Retention-I and Retention-T scores for\nquantifying jailbreak risks in visual and textual components of VLMs. Our\nprocess involves generating synthetic image-text pairs using a conditional\ndiffusion model. These pairs are then predicted for toxicity score by a VLM\nalongside a toxicity judgment classifier. By calculating the margin in toxicity\nscores, we can quantify the robustness of the VLM in an attack-agnostic manner.\nOur work has four main contributions. First, we prove that Retention Score can\nserve as a certified robustness metric. Second, we demonstrate that most VLMs\nwith visual components are less robust against jailbreak attacks than the\ncorresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find\nthat the security settings in Google Gemini significantly affect the score and\nrobustness. Moreover, the robustness of GPT4V is similar to the medium settings\nof Gemini. Finally, our approach offers a time-efficient alternative to\nexisting adversarial attack methods and provides consistent model robustness\nrankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA.\n", "link": "http://arxiv.org/abs/2412.17544v1", "date": "2024-12-23", "relevancy": 2.3479, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4766}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retention%20Score%3A%20Quantifying%20Jailbreak%20Risks%20for%20Vision%20Language%20Models&body=Title%3A%20Retention%20Score%3A%20Quantifying%20Jailbreak%20Risks%20for%20Vision%20Language%20Models%0AAuthor%3A%20Zaitang%20Li%20and%20Pin-Yu%20Chen%20and%20Tsung-Yi%20Ho%0AAbstract%3A%20%20%20The%20emergence%20of%20Vision-Language%20Models%20%28VLMs%29%20is%20a%20significant%20advancement%0Ain%20integrating%20computer%20vision%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20enhance%0Amulti-modal%20machine%20learning%20capabilities.%20However%2C%20this%20progress%20has%20also%20made%0AVLMs%20vulnerable%20to%20sophisticated%20adversarial%20attacks%2C%20raising%20concerns%20about%0Atheir%20reliability.%20The%20objective%20of%20this%20paper%20is%20to%20assess%20the%20resilience%20of%0AVLMs%20against%20jailbreak%20attacks%20that%20can%20compromise%20model%20safety%20compliance%20and%0Aresult%20in%20harmful%20outputs.%20To%20evaluate%20a%20VLM%27s%20ability%20to%20maintain%20its%0Arobustness%20against%20adversarial%20input%20perturbations%2C%20we%20propose%20a%20novel%20metric%0Acalled%20the%20%5Ctextbf%7BRetention%20Score%7D.%20Retention%20Score%20is%20a%20multi-modal%0Aevaluation%20metric%20that%20includes%20Retention-I%20and%20Retention-T%20scores%20for%0Aquantifying%20jailbreak%20risks%20in%20visual%20and%20textual%20components%20of%20VLMs.%20Our%0Aprocess%20involves%20generating%20synthetic%20image-text%20pairs%20using%20a%20conditional%0Adiffusion%20model.%20These%20pairs%20are%20then%20predicted%20for%20toxicity%20score%20by%20a%20VLM%0Aalongside%20a%20toxicity%20judgment%20classifier.%20By%20calculating%20the%20margin%20in%20toxicity%0Ascores%2C%20we%20can%20quantify%20the%20robustness%20of%20the%20VLM%20in%20an%20attack-agnostic%20manner.%0AOur%20work%20has%20four%20main%20contributions.%20First%2C%20we%20prove%20that%20Retention%20Score%20can%0Aserve%20as%20a%20certified%20robustness%20metric.%20Second%2C%20we%20demonstrate%20that%20most%20VLMs%0Awith%20visual%20components%20are%20less%20robust%20against%20jailbreak%20attacks%20than%20the%0Acorresponding%20plain%20VLMs.%20Additionally%2C%20we%20evaluate%20black-box%20VLM%20APIs%20and%20find%0Athat%20the%20security%20settings%20in%20Google%20Gemini%20significantly%20affect%20the%20score%20and%0Arobustness.%20Moreover%2C%20the%20robustness%20of%20GPT4V%20is%20similar%20to%20the%20medium%20settings%0Aof%20Gemini.%20Finally%2C%20our%20approach%20offers%20a%20time-efficient%20alternative%20to%0Aexisting%20adversarial%20attack%20methods%20and%20provides%20consistent%20model%20robustness%0Arankings%20when%20evaluated%20on%20VLMs%20including%20MiniGPT-4%2C%20InstructBLIP%2C%20and%20LLaVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetention%2520Score%253A%2520Quantifying%2520Jailbreak%2520Risks%2520for%2520Vision%2520Language%2520Models%26entry.906535625%3DZaitang%2520Li%2520and%2520Pin-Yu%2520Chen%2520and%2520Tsung-Yi%2520Ho%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520is%2520a%2520significant%2520advancement%250Ain%2520integrating%2520computer%2520vision%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520enhance%250Amulti-modal%2520machine%2520learning%2520capabilities.%2520However%252C%2520this%2520progress%2520has%2520also%2520made%250AVLMs%2520vulnerable%2520to%2520sophisticated%2520adversarial%2520attacks%252C%2520raising%2520concerns%2520about%250Atheir%2520reliability.%2520The%2520objective%2520of%2520this%2520paper%2520is%2520to%2520assess%2520the%2520resilience%2520of%250AVLMs%2520against%2520jailbreak%2520attacks%2520that%2520can%2520compromise%2520model%2520safety%2520compliance%2520and%250Aresult%2520in%2520harmful%2520outputs.%2520To%2520evaluate%2520a%2520VLM%2527s%2520ability%2520to%2520maintain%2520its%250Arobustness%2520against%2520adversarial%2520input%2520perturbations%252C%2520we%2520propose%2520a%2520novel%2520metric%250Acalled%2520the%2520%255Ctextbf%257BRetention%2520Score%257D.%2520Retention%2520Score%2520is%2520a%2520multi-modal%250Aevaluation%2520metric%2520that%2520includes%2520Retention-I%2520and%2520Retention-T%2520scores%2520for%250Aquantifying%2520jailbreak%2520risks%2520in%2520visual%2520and%2520textual%2520components%2520of%2520VLMs.%2520Our%250Aprocess%2520involves%2520generating%2520synthetic%2520image-text%2520pairs%2520using%2520a%2520conditional%250Adiffusion%2520model.%2520These%2520pairs%2520are%2520then%2520predicted%2520for%2520toxicity%2520score%2520by%2520a%2520VLM%250Aalongside%2520a%2520toxicity%2520judgment%2520classifier.%2520By%2520calculating%2520the%2520margin%2520in%2520toxicity%250Ascores%252C%2520we%2520can%2520quantify%2520the%2520robustness%2520of%2520the%2520VLM%2520in%2520an%2520attack-agnostic%2520manner.%250AOur%2520work%2520has%2520four%2520main%2520contributions.%2520First%252C%2520we%2520prove%2520that%2520Retention%2520Score%2520can%250Aserve%2520as%2520a%2520certified%2520robustness%2520metric.%2520Second%252C%2520we%2520demonstrate%2520that%2520most%2520VLMs%250Awith%2520visual%2520components%2520are%2520less%2520robust%2520against%2520jailbreak%2520attacks%2520than%2520the%250Acorresponding%2520plain%2520VLMs.%2520Additionally%252C%2520we%2520evaluate%2520black-box%2520VLM%2520APIs%2520and%2520find%250Athat%2520the%2520security%2520settings%2520in%2520Google%2520Gemini%2520significantly%2520affect%2520the%2520score%2520and%250Arobustness.%2520Moreover%252C%2520the%2520robustness%2520of%2520GPT4V%2520is%2520similar%2520to%2520the%2520medium%2520settings%250Aof%2520Gemini.%2520Finally%252C%2520our%2520approach%2520offers%2520a%2520time-efficient%2520alternative%2520to%250Aexisting%2520adversarial%2520attack%2520methods%2520and%2520provides%2520consistent%2520model%2520robustness%250Arankings%2520when%2520evaluated%2520on%2520VLMs%2520including%2520MiniGPT-4%252C%2520InstructBLIP%252C%2520and%2520LLaVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retention%20Score%3A%20Quantifying%20Jailbreak%20Risks%20for%20Vision%20Language%20Models&entry.906535625=Zaitang%20Li%20and%20Pin-Yu%20Chen%20and%20Tsung-Yi%20Ho&entry.1292438233=%20%20The%20emergence%20of%20Vision-Language%20Models%20%28VLMs%29%20is%20a%20significant%20advancement%0Ain%20integrating%20computer%20vision%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20enhance%0Amulti-modal%20machine%20learning%20capabilities.%20However%2C%20this%20progress%20has%20also%20made%0AVLMs%20vulnerable%20to%20sophisticated%20adversarial%20attacks%2C%20raising%20concerns%20about%0Atheir%20reliability.%20The%20objective%20of%20this%20paper%20is%20to%20assess%20the%20resilience%20of%0AVLMs%20against%20jailbreak%20attacks%20that%20can%20compromise%20model%20safety%20compliance%20and%0Aresult%20in%20harmful%20outputs.%20To%20evaluate%20a%20VLM%27s%20ability%20to%20maintain%20its%0Arobustness%20against%20adversarial%20input%20perturbations%2C%20we%20propose%20a%20novel%20metric%0Acalled%20the%20%5Ctextbf%7BRetention%20Score%7D.%20Retention%20Score%20is%20a%20multi-modal%0Aevaluation%20metric%20that%20includes%20Retention-I%20and%20Retention-T%20scores%20for%0Aquantifying%20jailbreak%20risks%20in%20visual%20and%20textual%20components%20of%20VLMs.%20Our%0Aprocess%20involves%20generating%20synthetic%20image-text%20pairs%20using%20a%20conditional%0Adiffusion%20model.%20These%20pairs%20are%20then%20predicted%20for%20toxicity%20score%20by%20a%20VLM%0Aalongside%20a%20toxicity%20judgment%20classifier.%20By%20calculating%20the%20margin%20in%20toxicity%0Ascores%2C%20we%20can%20quantify%20the%20robustness%20of%20the%20VLM%20in%20an%20attack-agnostic%20manner.%0AOur%20work%20has%20four%20main%20contributions.%20First%2C%20we%20prove%20that%20Retention%20Score%20can%0Aserve%20as%20a%20certified%20robustness%20metric.%20Second%2C%20we%20demonstrate%20that%20most%20VLMs%0Awith%20visual%20components%20are%20less%20robust%20against%20jailbreak%20attacks%20than%20the%0Acorresponding%20plain%20VLMs.%20Additionally%2C%20we%20evaluate%20black-box%20VLM%20APIs%20and%20find%0Athat%20the%20security%20settings%20in%20Google%20Gemini%20significantly%20affect%20the%20score%20and%0Arobustness.%20Moreover%2C%20the%20robustness%20of%20GPT4V%20is%20similar%20to%20the%20medium%20settings%0Aof%20Gemini.%20Finally%2C%20our%20approach%20offers%20a%20time-efficient%20alternative%20to%0Aexisting%20adversarial%20attack%20methods%20and%20provides%20consistent%20model%20robustness%0Arankings%20when%20evaluated%20on%20VLMs%20including%20MiniGPT-4%2C%20InstructBLIP%2C%20and%20LLaVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17544v1&entry.124074799=Read"},
{"title": "Global Optimization with A Power-Transformed Objective and Gaussian\n  Smoothing", "author": "Chen Xu", "abstract": "  We propose a novel method that solves global optimization problems in two\nsteps: (1) perform a (exponential) power-$N$ transformation to the\nnot-necessarily differentiable objective function $f$ and get $f_N$, and (2)\noptimize the Gaussian-smoothed $f_N$ with stochastic approximations. Under mild\nconditions on $f$, for any $\\delta>0$, we prove that with a sufficiently large\npower $N_\\delta$, this method converges to a solution in the\n$\\delta$-neighborhood of $f$'s global optimum point. The convergence rate is\n$O(d^2\\sigma^4\\varepsilon^{-2})$, which is faster than both the standard and\nsingle-loop homotopy methods if $\\sigma$ is pre-selected to be in $(0,1)$. In\nmost of the experiments performed, our method produces better solutions than\nother algorithms that also apply smoothing techniques.\n", "link": "http://arxiv.org/abs/2412.05204v2", "date": "2024-12-23", "relevancy": 2.3471, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4788}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4696}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Optimization%20with%20A%20Power-Transformed%20Objective%20and%20Gaussian%0A%20%20Smoothing&body=Title%3A%20Global%20Optimization%20with%20A%20Power-Transformed%20Objective%20and%20Gaussian%0A%20%20Smoothing%0AAuthor%3A%20Chen%20Xu%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%20that%20solves%20global%20optimization%20problems%20in%20two%0Asteps%3A%20%281%29%20perform%20a%20%28exponential%29%20power-%24N%24%20transformation%20to%20the%0Anot-necessarily%20differentiable%20objective%20function%20%24f%24%20and%20get%20%24f_N%24%2C%20and%20%282%29%0Aoptimize%20the%20Gaussian-smoothed%20%24f_N%24%20with%20stochastic%20approximations.%20Under%20mild%0Aconditions%20on%20%24f%24%2C%20for%20any%20%24%5Cdelta%3E0%24%2C%20we%20prove%20that%20with%20a%20sufficiently%20large%0Apower%20%24N_%5Cdelta%24%2C%20this%20method%20converges%20to%20a%20solution%20in%20the%0A%24%5Cdelta%24-neighborhood%20of%20%24f%24%27s%20global%20optimum%20point.%20The%20convergence%20rate%20is%0A%24O%28d%5E2%5Csigma%5E4%5Cvarepsilon%5E%7B-2%7D%29%24%2C%20which%20is%20faster%20than%20both%20the%20standard%20and%0Asingle-loop%20homotopy%20methods%20if%20%24%5Csigma%24%20is%20pre-selected%20to%20be%20in%20%24%280%2C1%29%24.%20In%0Amost%20of%20the%20experiments%20performed%2C%20our%20method%20produces%20better%20solutions%20than%0Aother%20algorithms%20that%20also%20apply%20smoothing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Optimization%2520with%2520A%2520Power-Transformed%2520Objective%2520and%2520Gaussian%250A%2520%2520Smoothing%26entry.906535625%3DChen%2520Xu%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520method%2520that%2520solves%2520global%2520optimization%2520problems%2520in%2520two%250Asteps%253A%2520%25281%2529%2520perform%2520a%2520%2528exponential%2529%2520power-%2524N%2524%2520transformation%2520to%2520the%250Anot-necessarily%2520differentiable%2520objective%2520function%2520%2524f%2524%2520and%2520get%2520%2524f_N%2524%252C%2520and%2520%25282%2529%250Aoptimize%2520the%2520Gaussian-smoothed%2520%2524f_N%2524%2520with%2520stochastic%2520approximations.%2520Under%2520mild%250Aconditions%2520on%2520%2524f%2524%252C%2520for%2520any%2520%2524%255Cdelta%253E0%2524%252C%2520we%2520prove%2520that%2520with%2520a%2520sufficiently%2520large%250Apower%2520%2524N_%255Cdelta%2524%252C%2520this%2520method%2520converges%2520to%2520a%2520solution%2520in%2520the%250A%2524%255Cdelta%2524-neighborhood%2520of%2520%2524f%2524%2527s%2520global%2520optimum%2520point.%2520The%2520convergence%2520rate%2520is%250A%2524O%2528d%255E2%255Csigma%255E4%255Cvarepsilon%255E%257B-2%257D%2529%2524%252C%2520which%2520is%2520faster%2520than%2520both%2520the%2520standard%2520and%250Asingle-loop%2520homotopy%2520methods%2520if%2520%2524%255Csigma%2524%2520is%2520pre-selected%2520to%2520be%2520in%2520%2524%25280%252C1%2529%2524.%2520In%250Amost%2520of%2520the%2520experiments%2520performed%252C%2520our%2520method%2520produces%2520better%2520solutions%2520than%250Aother%2520algorithms%2520that%2520also%2520apply%2520smoothing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Optimization%20with%20A%20Power-Transformed%20Objective%20and%20Gaussian%0A%20%20Smoothing&entry.906535625=Chen%20Xu&entry.1292438233=%20%20We%20propose%20a%20novel%20method%20that%20solves%20global%20optimization%20problems%20in%20two%0Asteps%3A%20%281%29%20perform%20a%20%28exponential%29%20power-%24N%24%20transformation%20to%20the%0Anot-necessarily%20differentiable%20objective%20function%20%24f%24%20and%20get%20%24f_N%24%2C%20and%20%282%29%0Aoptimize%20the%20Gaussian-smoothed%20%24f_N%24%20with%20stochastic%20approximations.%20Under%20mild%0Aconditions%20on%20%24f%24%2C%20for%20any%20%24%5Cdelta%3E0%24%2C%20we%20prove%20that%20with%20a%20sufficiently%20large%0Apower%20%24N_%5Cdelta%24%2C%20this%20method%20converges%20to%20a%20solution%20in%20the%0A%24%5Cdelta%24-neighborhood%20of%20%24f%24%27s%20global%20optimum%20point.%20The%20convergence%20rate%20is%0A%24O%28d%5E2%5Csigma%5E4%5Cvarepsilon%5E%7B-2%7D%29%24%2C%20which%20is%20faster%20than%20both%20the%20standard%20and%0Asingle-loop%20homotopy%20methods%20if%20%24%5Csigma%24%20is%20pre-selected%20to%20be%20in%20%24%280%2C1%29%24.%20In%0Amost%20of%20the%20experiments%20performed%2C%20our%20method%20produces%20better%20solutions%20than%0Aother%20algorithms%20that%20also%20apply%20smoothing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05204v2&entry.124074799=Read"},
{"title": "Randomized Approach to Matrix Completion: Applications in Recommendation\n  Systems and Image Inpainting", "author": "Antonina Krajewska and Ewa Niewiadomska-Szynkiewicz", "abstract": "  We present a novel method for matrix completion, specifically designed for\nmatrices where one dimension significantly exceeds the other. Our Columns\nSelected Matrix Completion (CSMC) method combines Column Subset Selection and\nLow-Rank Matrix Completion to efficiently reconstruct incomplete datasets. In\neach step, CSMC solves a convex optimization task. We introduce two algorithms\nthat implement CSMC, each tailored to different problem sizes. A formal\nanalysis outlines the necessary assumptions and the probability of a correct\nsolution. To assess the impact of matrix size, rank, and the proportion of\nmissing entries on solution quality and computation time, we conducted\nexperiments on synthetic data. The method was applied to two real-world\nproblems: recommendation systems and image inpainting. Our results show that\nCSMC delivers solutions comparable to state-of-the-art matrix completion\nalgorithms based on convex optimization, but with significant runtime savings.\nThis makes CSMC especially valuable for systems that require efficient\nprocessing of large, incomplete datasets while maintaining the integrity of the\nderived insights.\n", "link": "http://arxiv.org/abs/2403.01919v4", "date": "2024-12-23", "relevancy": 2.3308, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4703}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4651}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Randomized%20Approach%20to%20Matrix%20Completion%3A%20Applications%20in%20Recommendation%0A%20%20Systems%20and%20Image%20Inpainting&body=Title%3A%20Randomized%20Approach%20to%20Matrix%20Completion%3A%20Applications%20in%20Recommendation%0A%20%20Systems%20and%20Image%20Inpainting%0AAuthor%3A%20Antonina%20Krajewska%20and%20Ewa%20Niewiadomska-Szynkiewicz%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20matrix%20completion%2C%20specifically%20designed%20for%0Amatrices%20where%20one%20dimension%20significantly%20exceeds%20the%20other.%20Our%20Columns%0ASelected%20Matrix%20Completion%20%28CSMC%29%20method%20combines%20Column%20Subset%20Selection%20and%0ALow-Rank%20Matrix%20Completion%20to%20efficiently%20reconstruct%20incomplete%20datasets.%20In%0Aeach%20step%2C%20CSMC%20solves%20a%20convex%20optimization%20task.%20We%20introduce%20two%20algorithms%0Athat%20implement%20CSMC%2C%20each%20tailored%20to%20different%20problem%20sizes.%20A%20formal%0Aanalysis%20outlines%20the%20necessary%20assumptions%20and%20the%20probability%20of%20a%20correct%0Asolution.%20To%20assess%20the%20impact%20of%20matrix%20size%2C%20rank%2C%20and%20the%20proportion%20of%0Amissing%20entries%20on%20solution%20quality%20and%20computation%20time%2C%20we%20conducted%0Aexperiments%20on%20synthetic%20data.%20The%20method%20was%20applied%20to%20two%20real-world%0Aproblems%3A%20recommendation%20systems%20and%20image%20inpainting.%20Our%20results%20show%20that%0ACSMC%20delivers%20solutions%20comparable%20to%20state-of-the-art%20matrix%20completion%0Aalgorithms%20based%20on%20convex%20optimization%2C%20but%20with%20significant%20runtime%20savings.%0AThis%20makes%20CSMC%20especially%20valuable%20for%20systems%20that%20require%20efficient%0Aprocessing%20of%20large%2C%20incomplete%20datasets%20while%20maintaining%20the%20integrity%20of%20the%0Aderived%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01919v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandomized%2520Approach%2520to%2520Matrix%2520Completion%253A%2520Applications%2520in%2520Recommendation%250A%2520%2520Systems%2520and%2520Image%2520Inpainting%26entry.906535625%3DAntonina%2520Krajewska%2520and%2520Ewa%2520Niewiadomska-Szynkiewicz%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520method%2520for%2520matrix%2520completion%252C%2520specifically%2520designed%2520for%250Amatrices%2520where%2520one%2520dimension%2520significantly%2520exceeds%2520the%2520other.%2520Our%2520Columns%250ASelected%2520Matrix%2520Completion%2520%2528CSMC%2529%2520method%2520combines%2520Column%2520Subset%2520Selection%2520and%250ALow-Rank%2520Matrix%2520Completion%2520to%2520efficiently%2520reconstruct%2520incomplete%2520datasets.%2520In%250Aeach%2520step%252C%2520CSMC%2520solves%2520a%2520convex%2520optimization%2520task.%2520We%2520introduce%2520two%2520algorithms%250Athat%2520implement%2520CSMC%252C%2520each%2520tailored%2520to%2520different%2520problem%2520sizes.%2520A%2520formal%250Aanalysis%2520outlines%2520the%2520necessary%2520assumptions%2520and%2520the%2520probability%2520of%2520a%2520correct%250Asolution.%2520To%2520assess%2520the%2520impact%2520of%2520matrix%2520size%252C%2520rank%252C%2520and%2520the%2520proportion%2520of%250Amissing%2520entries%2520on%2520solution%2520quality%2520and%2520computation%2520time%252C%2520we%2520conducted%250Aexperiments%2520on%2520synthetic%2520data.%2520The%2520method%2520was%2520applied%2520to%2520two%2520real-world%250Aproblems%253A%2520recommendation%2520systems%2520and%2520image%2520inpainting.%2520Our%2520results%2520show%2520that%250ACSMC%2520delivers%2520solutions%2520comparable%2520to%2520state-of-the-art%2520matrix%2520completion%250Aalgorithms%2520based%2520on%2520convex%2520optimization%252C%2520but%2520with%2520significant%2520runtime%2520savings.%250AThis%2520makes%2520CSMC%2520especially%2520valuable%2520for%2520systems%2520that%2520require%2520efficient%250Aprocessing%2520of%2520large%252C%2520incomplete%2520datasets%2520while%2520maintaining%2520the%2520integrity%2520of%2520the%250Aderived%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01919v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomized%20Approach%20to%20Matrix%20Completion%3A%20Applications%20in%20Recommendation%0A%20%20Systems%20and%20Image%20Inpainting&entry.906535625=Antonina%20Krajewska%20and%20Ewa%20Niewiadomska-Szynkiewicz&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20matrix%20completion%2C%20specifically%20designed%20for%0Amatrices%20where%20one%20dimension%20significantly%20exceeds%20the%20other.%20Our%20Columns%0ASelected%20Matrix%20Completion%20%28CSMC%29%20method%20combines%20Column%20Subset%20Selection%20and%0ALow-Rank%20Matrix%20Completion%20to%20efficiently%20reconstruct%20incomplete%20datasets.%20In%0Aeach%20step%2C%20CSMC%20solves%20a%20convex%20optimization%20task.%20We%20introduce%20two%20algorithms%0Athat%20implement%20CSMC%2C%20each%20tailored%20to%20different%20problem%20sizes.%20A%20formal%0Aanalysis%20outlines%20the%20necessary%20assumptions%20and%20the%20probability%20of%20a%20correct%0Asolution.%20To%20assess%20the%20impact%20of%20matrix%20size%2C%20rank%2C%20and%20the%20proportion%20of%0Amissing%20entries%20on%20solution%20quality%20and%20computation%20time%2C%20we%20conducted%0Aexperiments%20on%20synthetic%20data.%20The%20method%20was%20applied%20to%20two%20real-world%0Aproblems%3A%20recommendation%20systems%20and%20image%20inpainting.%20Our%20results%20show%20that%0ACSMC%20delivers%20solutions%20comparable%20to%20state-of-the-art%20matrix%20completion%0Aalgorithms%20based%20on%20convex%20optimization%2C%20but%20with%20significant%20runtime%20savings.%0AThis%20makes%20CSMC%20especially%20valuable%20for%20systems%20that%20require%20efficient%0Aprocessing%20of%20large%2C%20incomplete%20datasets%20while%20maintaining%20the%20integrity%20of%20the%0Aderived%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01919v4&entry.124074799=Read"},
{"title": "Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene\n  Interaction Learning via Human Mimicking", "author": "Yun Liu and Bowen Yang and Licheng Zhong and He Wang and Li Yi", "abstract": "  Learning generic skills for humanoid robots interacting with 3D scenes by\nmimicking human data is a key research challenge with significant implications\nfor robotics and real-world applications. However, existing methodologies and\nbenchmarks are constrained by the use of small-scale, manually collected\ndemonstrations, lacking the general dataset and benchmark support necessary to\nexplore scene geometry generalization effectively. To address this gap, we\nintroduce Mimicking-Bench, the first comprehensive benchmark designed for\ngeneralizable humanoid-scene interaction learning through mimicking large-scale\nhuman animation references. Mimicking-Bench includes six household full-body\nhumanoid-scene interaction tasks, covering 11K diverse object shapes, along\nwith 20K synthetic and 3K real-world human interaction skill references. We\nconstruct a complete humanoid skill learning pipeline and benchmark approaches\nfor motion retargeting, motion tracking, imitation learning, and their various\ncombinations. Extensive experiments highlight the value of human mimicking for\nskill learning, revealing key challenges and research directions.\n", "link": "http://arxiv.org/abs/2412.17730v1", "date": "2024-12-23", "relevancy": 2.3158, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6114}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.584}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mimicking-Bench%3A%20A%20Benchmark%20for%20Generalizable%20Humanoid-Scene%0A%20%20Interaction%20Learning%20via%20Human%20Mimicking&body=Title%3A%20Mimicking-Bench%3A%20A%20Benchmark%20for%20Generalizable%20Humanoid-Scene%0A%20%20Interaction%20Learning%20via%20Human%20Mimicking%0AAuthor%3A%20Yun%20Liu%20and%20Bowen%20Yang%20and%20Licheng%20Zhong%20and%20He%20Wang%20and%20Li%20Yi%0AAbstract%3A%20%20%20Learning%20generic%20skills%20for%20humanoid%20robots%20interacting%20with%203D%20scenes%20by%0Amimicking%20human%20data%20is%20a%20key%20research%20challenge%20with%20significant%20implications%0Afor%20robotics%20and%20real-world%20applications.%20However%2C%20existing%20methodologies%20and%0Abenchmarks%20are%20constrained%20by%20the%20use%20of%20small-scale%2C%20manually%20collected%0Ademonstrations%2C%20lacking%20the%20general%20dataset%20and%20benchmark%20support%20necessary%20to%0Aexplore%20scene%20geometry%20generalization%20effectively.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20Mimicking-Bench%2C%20the%20first%20comprehensive%20benchmark%20designed%20for%0Ageneralizable%20humanoid-scene%20interaction%20learning%20through%20mimicking%20large-scale%0Ahuman%20animation%20references.%20Mimicking-Bench%20includes%20six%20household%20full-body%0Ahumanoid-scene%20interaction%20tasks%2C%20covering%2011K%20diverse%20object%20shapes%2C%20along%0Awith%2020K%20synthetic%20and%203K%20real-world%20human%20interaction%20skill%20references.%20We%0Aconstruct%20a%20complete%20humanoid%20skill%20learning%20pipeline%20and%20benchmark%20approaches%0Afor%20motion%20retargeting%2C%20motion%20tracking%2C%20imitation%20learning%2C%20and%20their%20various%0Acombinations.%20Extensive%20experiments%20highlight%20the%20value%20of%20human%20mimicking%20for%0Askill%20learning%2C%20revealing%20key%20challenges%20and%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimicking-Bench%253A%2520A%2520Benchmark%2520for%2520Generalizable%2520Humanoid-Scene%250A%2520%2520Interaction%2520Learning%2520via%2520Human%2520Mimicking%26entry.906535625%3DYun%2520Liu%2520and%2520Bowen%2520Yang%2520and%2520Licheng%2520Zhong%2520and%2520He%2520Wang%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%2520Learning%2520generic%2520skills%2520for%2520humanoid%2520robots%2520interacting%2520with%25203D%2520scenes%2520by%250Amimicking%2520human%2520data%2520is%2520a%2520key%2520research%2520challenge%2520with%2520significant%2520implications%250Afor%2520robotics%2520and%2520real-world%2520applications.%2520However%252C%2520existing%2520methodologies%2520and%250Abenchmarks%2520are%2520constrained%2520by%2520the%2520use%2520of%2520small-scale%252C%2520manually%2520collected%250Ademonstrations%252C%2520lacking%2520the%2520general%2520dataset%2520and%2520benchmark%2520support%2520necessary%2520to%250Aexplore%2520scene%2520geometry%2520generalization%2520effectively.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520Mimicking-Bench%252C%2520the%2520first%2520comprehensive%2520benchmark%2520designed%2520for%250Ageneralizable%2520humanoid-scene%2520interaction%2520learning%2520through%2520mimicking%2520large-scale%250Ahuman%2520animation%2520references.%2520Mimicking-Bench%2520includes%2520six%2520household%2520full-body%250Ahumanoid-scene%2520interaction%2520tasks%252C%2520covering%252011K%2520diverse%2520object%2520shapes%252C%2520along%250Awith%252020K%2520synthetic%2520and%25203K%2520real-world%2520human%2520interaction%2520skill%2520references.%2520We%250Aconstruct%2520a%2520complete%2520humanoid%2520skill%2520learning%2520pipeline%2520and%2520benchmark%2520approaches%250Afor%2520motion%2520retargeting%252C%2520motion%2520tracking%252C%2520imitation%2520learning%252C%2520and%2520their%2520various%250Acombinations.%2520Extensive%2520experiments%2520highlight%2520the%2520value%2520of%2520human%2520mimicking%2520for%250Askill%2520learning%252C%2520revealing%2520key%2520challenges%2520and%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mimicking-Bench%3A%20A%20Benchmark%20for%20Generalizable%20Humanoid-Scene%0A%20%20Interaction%20Learning%20via%20Human%20Mimicking&entry.906535625=Yun%20Liu%20and%20Bowen%20Yang%20and%20Licheng%20Zhong%20and%20He%20Wang%20and%20Li%20Yi&entry.1292438233=%20%20Learning%20generic%20skills%20for%20humanoid%20robots%20interacting%20with%203D%20scenes%20by%0Amimicking%20human%20data%20is%20a%20key%20research%20challenge%20with%20significant%20implications%0Afor%20robotics%20and%20real-world%20applications.%20However%2C%20existing%20methodologies%20and%0Abenchmarks%20are%20constrained%20by%20the%20use%20of%20small-scale%2C%20manually%20collected%0Ademonstrations%2C%20lacking%20the%20general%20dataset%20and%20benchmark%20support%20necessary%20to%0Aexplore%20scene%20geometry%20generalization%20effectively.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20Mimicking-Bench%2C%20the%20first%20comprehensive%20benchmark%20designed%20for%0Ageneralizable%20humanoid-scene%20interaction%20learning%20through%20mimicking%20large-scale%0Ahuman%20animation%20references.%20Mimicking-Bench%20includes%20six%20household%20full-body%0Ahumanoid-scene%20interaction%20tasks%2C%20covering%2011K%20diverse%20object%20shapes%2C%20along%0Awith%2020K%20synthetic%20and%203K%20real-world%20human%20interaction%20skill%20references.%20We%0Aconstruct%20a%20complete%20humanoid%20skill%20learning%20pipeline%20and%20benchmark%20approaches%0Afor%20motion%20retargeting%2C%20motion%20tracking%2C%20imitation%20learning%2C%20and%20their%20various%0Acombinations.%20Extensive%20experiments%20highlight%20the%20value%20of%20human%20mimicking%20for%0Askill%20learning%2C%20revealing%20key%20challenges%20and%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17730v1&entry.124074799=Read"},
{"title": "Learning on Large Graphs using Intersecting Communities", "author": "Ben Finkelshtein and \u0130smail \u0130lkan Ceylan and Michael Bronstein and Ron Levie", "abstract": "  Message Passing Neural Networks (MPNNs) are a staple of graph machine\nlearning. MPNNs iteratively update each node's representation in an input graph\nby aggregating messages from the node's neighbors, which necessitates a memory\ncomplexity of the order of the number of graph edges. This complexity might\nquickly become prohibitive for large graphs provided they are not very sparse.\nIn this paper, we propose a novel approach to alleviate this problem by\napproximating the input graph as an intersecting community graph (ICG) -- a\ncombination of intersecting cliques. The key insight is that the number of\ncommunities required to approximate a graph does not depend on the graph size.\nWe develop a new constructive version of the Weak Graph Regularity Lemma to\nefficiently construct an approximating ICG for any input graph. We then devise\nan efficient graph learning algorithm operating directly on ICG in linear\nmemory and time with respect to the number of nodes (rather than edges). This\noffers a new and fundamentally different pipeline for learning on very large\nnon-sparse graphs, whose applicability is demonstrated empirically on node\nclassification tasks and spatio-temporal data processing.\n", "link": "http://arxiv.org/abs/2405.20724v2", "date": "2024-12-23", "relevancy": 2.307, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4813}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4544}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20on%20Large%20Graphs%20using%20Intersecting%20Communities&body=Title%3A%20Learning%20on%20Large%20Graphs%20using%20Intersecting%20Communities%0AAuthor%3A%20Ben%20Finkelshtein%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Michael%20Bronstein%20and%20Ron%20Levie%0AAbstract%3A%20%20%20Message%20Passing%20Neural%20Networks%20%28MPNNs%29%20are%20a%20staple%20of%20graph%20machine%0Alearning.%20MPNNs%20iteratively%20update%20each%20node%27s%20representation%20in%20an%20input%20graph%0Aby%20aggregating%20messages%20from%20the%20node%27s%20neighbors%2C%20which%20necessitates%20a%20memory%0Acomplexity%20of%20the%20order%20of%20the%20number%20of%20graph%20edges.%20This%20complexity%20might%0Aquickly%20become%20prohibitive%20for%20large%20graphs%20provided%20they%20are%20not%20very%20sparse.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20alleviate%20this%20problem%20by%0Aapproximating%20the%20input%20graph%20as%20an%20intersecting%20community%20graph%20%28ICG%29%20--%20a%0Acombination%20of%20intersecting%20cliques.%20The%20key%20insight%20is%20that%20the%20number%20of%0Acommunities%20required%20to%20approximate%20a%20graph%20does%20not%20depend%20on%20the%20graph%20size.%0AWe%20develop%20a%20new%20constructive%20version%20of%20the%20Weak%20Graph%20Regularity%20Lemma%20to%0Aefficiently%20construct%20an%20approximating%20ICG%20for%20any%20input%20graph.%20We%20then%20devise%0Aan%20efficient%20graph%20learning%20algorithm%20operating%20directly%20on%20ICG%20in%20linear%0Amemory%20and%20time%20with%20respect%20to%20the%20number%20of%20nodes%20%28rather%20than%20edges%29.%20This%0Aoffers%20a%20new%20and%20fundamentally%20different%20pipeline%20for%20learning%20on%20very%20large%0Anon-sparse%20graphs%2C%20whose%20applicability%20is%20demonstrated%20empirically%20on%20node%0Aclassification%20tasks%20and%20spatio-temporal%20data%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520on%2520Large%2520Graphs%2520using%2520Intersecting%2520Communities%26entry.906535625%3DBen%2520Finkelshtein%2520and%2520%25C4%25B0smail%2520%25C4%25B0lkan%2520Ceylan%2520and%2520Michael%2520Bronstein%2520and%2520Ron%2520Levie%26entry.1292438233%3D%2520%2520Message%2520Passing%2520Neural%2520Networks%2520%2528MPNNs%2529%2520are%2520a%2520staple%2520of%2520graph%2520machine%250Alearning.%2520MPNNs%2520iteratively%2520update%2520each%2520node%2527s%2520representation%2520in%2520an%2520input%2520graph%250Aby%2520aggregating%2520messages%2520from%2520the%2520node%2527s%2520neighbors%252C%2520which%2520necessitates%2520a%2520memory%250Acomplexity%2520of%2520the%2520order%2520of%2520the%2520number%2520of%2520graph%2520edges.%2520This%2520complexity%2520might%250Aquickly%2520become%2520prohibitive%2520for%2520large%2520graphs%2520provided%2520they%2520are%2520not%2520very%2520sparse.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520alleviate%2520this%2520problem%2520by%250Aapproximating%2520the%2520input%2520graph%2520as%2520an%2520intersecting%2520community%2520graph%2520%2528ICG%2529%2520--%2520a%250Acombination%2520of%2520intersecting%2520cliques.%2520The%2520key%2520insight%2520is%2520that%2520the%2520number%2520of%250Acommunities%2520required%2520to%2520approximate%2520a%2520graph%2520does%2520not%2520depend%2520on%2520the%2520graph%2520size.%250AWe%2520develop%2520a%2520new%2520constructive%2520version%2520of%2520the%2520Weak%2520Graph%2520Regularity%2520Lemma%2520to%250Aefficiently%2520construct%2520an%2520approximating%2520ICG%2520for%2520any%2520input%2520graph.%2520We%2520then%2520devise%250Aan%2520efficient%2520graph%2520learning%2520algorithm%2520operating%2520directly%2520on%2520ICG%2520in%2520linear%250Amemory%2520and%2520time%2520with%2520respect%2520to%2520the%2520number%2520of%2520nodes%2520%2528rather%2520than%2520edges%2529.%2520This%250Aoffers%2520a%2520new%2520and%2520fundamentally%2520different%2520pipeline%2520for%2520learning%2520on%2520very%2520large%250Anon-sparse%2520graphs%252C%2520whose%2520applicability%2520is%2520demonstrated%2520empirically%2520on%2520node%250Aclassification%2520tasks%2520and%2520spatio-temporal%2520data%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20on%20Large%20Graphs%20using%20Intersecting%20Communities&entry.906535625=Ben%20Finkelshtein%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Michael%20Bronstein%20and%20Ron%20Levie&entry.1292438233=%20%20Message%20Passing%20Neural%20Networks%20%28MPNNs%29%20are%20a%20staple%20of%20graph%20machine%0Alearning.%20MPNNs%20iteratively%20update%20each%20node%27s%20representation%20in%20an%20input%20graph%0Aby%20aggregating%20messages%20from%20the%20node%27s%20neighbors%2C%20which%20necessitates%20a%20memory%0Acomplexity%20of%20the%20order%20of%20the%20number%20of%20graph%20edges.%20This%20complexity%20might%0Aquickly%20become%20prohibitive%20for%20large%20graphs%20provided%20they%20are%20not%20very%20sparse.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20alleviate%20this%20problem%20by%0Aapproximating%20the%20input%20graph%20as%20an%20intersecting%20community%20graph%20%28ICG%29%20--%20a%0Acombination%20of%20intersecting%20cliques.%20The%20key%20insight%20is%20that%20the%20number%20of%0Acommunities%20required%20to%20approximate%20a%20graph%20does%20not%20depend%20on%20the%20graph%20size.%0AWe%20develop%20a%20new%20constructive%20version%20of%20the%20Weak%20Graph%20Regularity%20Lemma%20to%0Aefficiently%20construct%20an%20approximating%20ICG%20for%20any%20input%20graph.%20We%20then%20devise%0Aan%20efficient%20graph%20learning%20algorithm%20operating%20directly%20on%20ICG%20in%20linear%0Amemory%20and%20time%20with%20respect%20to%20the%20number%20of%20nodes%20%28rather%20than%20edges%29.%20This%0Aoffers%20a%20new%20and%20fundamentally%20different%20pipeline%20for%20learning%20on%20very%20large%0Anon-sparse%20graphs%2C%20whose%20applicability%20is%20demonstrated%20empirically%20on%20node%0Aclassification%20tasks%20and%20spatio-temporal%20data%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20724v2&entry.124074799=Read"},
{"title": "Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of\n  Large Language Models such as ChatGPT in Educational Settings", "author": "J\u00e9r\u00e9mie Sublime and Ilaria Renna", "abstract": "  The rapid adoption of Generative AI (GenAI) based on Large Language Models\n(LLMs) such as ChatGPT has recently and profoundly impacted education, offering\ntransformative opportunities while raising significant concerns. In this study\nwe present the results of a survey that investigates how 395 students aged 13\nto 25 years old in France and Italy integrate LLMs into their educational\nroutines.\n  Key findings include the widespread use of these tools across all age groups\nand disciplines, with older students and male students demonstrating higher\nusage frequencies, particularly in scientific contexts. The results also show\ngender disparities, raising concerns about an emerging AI literacy and\ntechnological gender gap. Additionally, while most students utilise LLMs\nconstructively, the lack of systematic proofreading and critical evaluation\namong younger users suggests potential risks to cognitive skills development,\nincluding critical thinking and foundational knowledge. The survey results\nunderscore the need for educational institutions to adapt their curricula to\nintegrate AI tools effectively, promoting ethical use, critical thinking, and\nawareness of AI limitations and environmental costs. This paper provides\nactionable recommendations for fostering equitable and effective cohabitation\nof LLMs and education while addressing emerging challenges.\n", "link": "http://arxiv.org/abs/2412.17486v1", "date": "2024-12-23", "relevancy": 2.2872, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4837}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4669}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20ChatGPT%20Massively%20Used%20by%20Students%20Nowadays%3F%20A%20Survey%20on%20the%20Use%20of%0A%20%20Large%20Language%20Models%20such%20as%20ChatGPT%20in%20Educational%20Settings&body=Title%3A%20Is%20ChatGPT%20Massively%20Used%20by%20Students%20Nowadays%3F%20A%20Survey%20on%20the%20Use%20of%0A%20%20Large%20Language%20Models%20such%20as%20ChatGPT%20in%20Educational%20Settings%0AAuthor%3A%20J%C3%A9r%C3%A9mie%20Sublime%20and%20Ilaria%20Renna%0AAbstract%3A%20%20%20The%20rapid%20adoption%20of%20Generative%20AI%20%28GenAI%29%20based%20on%20Large%20Language%20Models%0A%28LLMs%29%20such%20as%20ChatGPT%20has%20recently%20and%20profoundly%20impacted%20education%2C%20offering%0Atransformative%20opportunities%20while%20raising%20significant%20concerns.%20In%20this%20study%0Awe%20present%20the%20results%20of%20a%20survey%20that%20investigates%20how%20395%20students%20aged%2013%0Ato%2025%20years%20old%20in%20France%20and%20Italy%20integrate%20LLMs%20into%20their%20educational%0Aroutines.%0A%20%20Key%20findings%20include%20the%20widespread%20use%20of%20these%20tools%20across%20all%20age%20groups%0Aand%20disciplines%2C%20with%20older%20students%20and%20male%20students%20demonstrating%20higher%0Ausage%20frequencies%2C%20particularly%20in%20scientific%20contexts.%20The%20results%20also%20show%0Agender%20disparities%2C%20raising%20concerns%20about%20an%20emerging%20AI%20literacy%20and%0Atechnological%20gender%20gap.%20Additionally%2C%20while%20most%20students%20utilise%20LLMs%0Aconstructively%2C%20the%20lack%20of%20systematic%20proofreading%20and%20critical%20evaluation%0Aamong%20younger%20users%20suggests%20potential%20risks%20to%20cognitive%20skills%20development%2C%0Aincluding%20critical%20thinking%20and%20foundational%20knowledge.%20The%20survey%20results%0Aunderscore%20the%20need%20for%20educational%20institutions%20to%20adapt%20their%20curricula%20to%0Aintegrate%20AI%20tools%20effectively%2C%20promoting%20ethical%20use%2C%20critical%20thinking%2C%20and%0Aawareness%20of%20AI%20limitations%20and%20environmental%20costs.%20This%20paper%20provides%0Aactionable%20recommendations%20for%20fostering%20equitable%20and%20effective%20cohabitation%0Aof%20LLMs%20and%20education%20while%20addressing%20emerging%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520ChatGPT%2520Massively%2520Used%2520by%2520Students%2520Nowadays%253F%2520A%2520Survey%2520on%2520the%2520Use%2520of%250A%2520%2520Large%2520Language%2520Models%2520such%2520as%2520ChatGPT%2520in%2520Educational%2520Settings%26entry.906535625%3DJ%25C3%25A9r%25C3%25A9mie%2520Sublime%2520and%2520Ilaria%2520Renna%26entry.1292438233%3D%2520%2520The%2520rapid%2520adoption%2520of%2520Generative%2520AI%2520%2528GenAI%2529%2520based%2520on%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520such%2520as%2520ChatGPT%2520has%2520recently%2520and%2520profoundly%2520impacted%2520education%252C%2520offering%250Atransformative%2520opportunities%2520while%2520raising%2520significant%2520concerns.%2520In%2520this%2520study%250Awe%2520present%2520the%2520results%2520of%2520a%2520survey%2520that%2520investigates%2520how%2520395%2520students%2520aged%252013%250Ato%252025%2520years%2520old%2520in%2520France%2520and%2520Italy%2520integrate%2520LLMs%2520into%2520their%2520educational%250Aroutines.%250A%2520%2520Key%2520findings%2520include%2520the%2520widespread%2520use%2520of%2520these%2520tools%2520across%2520all%2520age%2520groups%250Aand%2520disciplines%252C%2520with%2520older%2520students%2520and%2520male%2520students%2520demonstrating%2520higher%250Ausage%2520frequencies%252C%2520particularly%2520in%2520scientific%2520contexts.%2520The%2520results%2520also%2520show%250Agender%2520disparities%252C%2520raising%2520concerns%2520about%2520an%2520emerging%2520AI%2520literacy%2520and%250Atechnological%2520gender%2520gap.%2520Additionally%252C%2520while%2520most%2520students%2520utilise%2520LLMs%250Aconstructively%252C%2520the%2520lack%2520of%2520systematic%2520proofreading%2520and%2520critical%2520evaluation%250Aamong%2520younger%2520users%2520suggests%2520potential%2520risks%2520to%2520cognitive%2520skills%2520development%252C%250Aincluding%2520critical%2520thinking%2520and%2520foundational%2520knowledge.%2520The%2520survey%2520results%250Aunderscore%2520the%2520need%2520for%2520educational%2520institutions%2520to%2520adapt%2520their%2520curricula%2520to%250Aintegrate%2520AI%2520tools%2520effectively%252C%2520promoting%2520ethical%2520use%252C%2520critical%2520thinking%252C%2520and%250Aawareness%2520of%2520AI%2520limitations%2520and%2520environmental%2520costs.%2520This%2520paper%2520provides%250Aactionable%2520recommendations%2520for%2520fostering%2520equitable%2520and%2520effective%2520cohabitation%250Aof%2520LLMs%2520and%2520education%2520while%2520addressing%2520emerging%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20ChatGPT%20Massively%20Used%20by%20Students%20Nowadays%3F%20A%20Survey%20on%20the%20Use%20of%0A%20%20Large%20Language%20Models%20such%20as%20ChatGPT%20in%20Educational%20Settings&entry.906535625=J%C3%A9r%C3%A9mie%20Sublime%20and%20Ilaria%20Renna&entry.1292438233=%20%20The%20rapid%20adoption%20of%20Generative%20AI%20%28GenAI%29%20based%20on%20Large%20Language%20Models%0A%28LLMs%29%20such%20as%20ChatGPT%20has%20recently%20and%20profoundly%20impacted%20education%2C%20offering%0Atransformative%20opportunities%20while%20raising%20significant%20concerns.%20In%20this%20study%0Awe%20present%20the%20results%20of%20a%20survey%20that%20investigates%20how%20395%20students%20aged%2013%0Ato%2025%20years%20old%20in%20France%20and%20Italy%20integrate%20LLMs%20into%20their%20educational%0Aroutines.%0A%20%20Key%20findings%20include%20the%20widespread%20use%20of%20these%20tools%20across%20all%20age%20groups%0Aand%20disciplines%2C%20with%20older%20students%20and%20male%20students%20demonstrating%20higher%0Ausage%20frequencies%2C%20particularly%20in%20scientific%20contexts.%20The%20results%20also%20show%0Agender%20disparities%2C%20raising%20concerns%20about%20an%20emerging%20AI%20literacy%20and%0Atechnological%20gender%20gap.%20Additionally%2C%20while%20most%20students%20utilise%20LLMs%0Aconstructively%2C%20the%20lack%20of%20systematic%20proofreading%20and%20critical%20evaluation%0Aamong%20younger%20users%20suggests%20potential%20risks%20to%20cognitive%20skills%20development%2C%0Aincluding%20critical%20thinking%20and%20foundational%20knowledge.%20The%20survey%20results%0Aunderscore%20the%20need%20for%20educational%20institutions%20to%20adapt%20their%20curricula%20to%0Aintegrate%20AI%20tools%20effectively%2C%20promoting%20ethical%20use%2C%20critical%20thinking%2C%20and%0Aawareness%20of%20AI%20limitations%20and%20environmental%20costs.%20This%20paper%20provides%0Aactionable%20recommendations%20for%20fostering%20equitable%20and%20effective%20cohabitation%0Aof%20LLMs%20and%20education%20while%20addressing%20emerging%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17486v1&entry.124074799=Read"},
{"title": "Personalized Large Vision-Language Models", "author": "Chau Pham and Hoang Phan and David Doermann and Yunjie Tian", "abstract": "  The personalization model has gained significant attention in image\ngeneration yet remains underexplored for large vision-language models (LVLMs).\nBeyond generic ones, with personalization, LVLMs handle interactive dialogues\nusing referential concepts (e.g., ``Mike and Susan are talking.'') instead of\nthe generic form (e.g., ``a boy and a girl are talking.''), making the\nconversation more customizable and referentially friendly. In addition, PLVM is\nequipped to continuously add new concepts during a dialogue without incurring\nadditional costs, which significantly enhances the practicality. PLVM proposes\nAligner, a pre-trained visual encoder to align referential concepts with the\nqueried images. During the dialogues, it extracts features of reference images\nwith these corresponding concepts and recognizes them in the queried image,\nenabling personalization. We note that the computational cost and parameter\ncount of the Aligner are negligible within the entire framework. With\ncomprehensive qualitative and quantitative analyses, we reveal the\neffectiveness and superiority of PLVM.\n", "link": "http://arxiv.org/abs/2412.17610v1", "date": "2024-12-23", "relevancy": 2.2802, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5746}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Large%20Vision-Language%20Models&body=Title%3A%20Personalized%20Large%20Vision-Language%20Models%0AAuthor%3A%20Chau%20Pham%20and%20Hoang%20Phan%20and%20David%20Doermann%20and%20Yunjie%20Tian%0AAbstract%3A%20%20%20The%20personalization%20model%20has%20gained%20significant%20attention%20in%20image%0Ageneration%20yet%20remains%20underexplored%20for%20large%20vision-language%20models%20%28LVLMs%29.%0ABeyond%20generic%20ones%2C%20with%20personalization%2C%20LVLMs%20handle%20interactive%20dialogues%0Ausing%20referential%20concepts%20%28e.g.%2C%20%60%60Mike%20and%20Susan%20are%20talking.%27%27%29%20instead%20of%0Athe%20generic%20form%20%28e.g.%2C%20%60%60a%20boy%20and%20a%20girl%20are%20talking.%27%27%29%2C%20making%20the%0Aconversation%20more%20customizable%20and%20referentially%20friendly.%20In%20addition%2C%20PLVM%20is%0Aequipped%20to%20continuously%20add%20new%20concepts%20during%20a%20dialogue%20without%20incurring%0Aadditional%20costs%2C%20which%20significantly%20enhances%20the%20practicality.%20PLVM%20proposes%0AAligner%2C%20a%20pre-trained%20visual%20encoder%20to%20align%20referential%20concepts%20with%20the%0Aqueried%20images.%20During%20the%20dialogues%2C%20it%20extracts%20features%20of%20reference%20images%0Awith%20these%20corresponding%20concepts%20and%20recognizes%20them%20in%20the%20queried%20image%2C%0Aenabling%20personalization.%20We%20note%20that%20the%20computational%20cost%20and%20parameter%0Acount%20of%20the%20Aligner%20are%20negligible%20within%20the%20entire%20framework.%20With%0Acomprehensive%20qualitative%20and%20quantitative%20analyses%2C%20we%20reveal%20the%0Aeffectiveness%20and%20superiority%20of%20PLVM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DChau%2520Pham%2520and%2520Hoang%2520Phan%2520and%2520David%2520Doermann%2520and%2520Yunjie%2520Tian%26entry.1292438233%3D%2520%2520The%2520personalization%2520model%2520has%2520gained%2520significant%2520attention%2520in%2520image%250Ageneration%2520yet%2520remains%2520underexplored%2520for%2520large%2520vision-language%2520models%2520%2528LVLMs%2529.%250ABeyond%2520generic%2520ones%252C%2520with%2520personalization%252C%2520LVLMs%2520handle%2520interactive%2520dialogues%250Ausing%2520referential%2520concepts%2520%2528e.g.%252C%2520%2560%2560Mike%2520and%2520Susan%2520are%2520talking.%2527%2527%2529%2520instead%2520of%250Athe%2520generic%2520form%2520%2528e.g.%252C%2520%2560%2560a%2520boy%2520and%2520a%2520girl%2520are%2520talking.%2527%2527%2529%252C%2520making%2520the%250Aconversation%2520more%2520customizable%2520and%2520referentially%2520friendly.%2520In%2520addition%252C%2520PLVM%2520is%250Aequipped%2520to%2520continuously%2520add%2520new%2520concepts%2520during%2520a%2520dialogue%2520without%2520incurring%250Aadditional%2520costs%252C%2520which%2520significantly%2520enhances%2520the%2520practicality.%2520PLVM%2520proposes%250AAligner%252C%2520a%2520pre-trained%2520visual%2520encoder%2520to%2520align%2520referential%2520concepts%2520with%2520the%250Aqueried%2520images.%2520During%2520the%2520dialogues%252C%2520it%2520extracts%2520features%2520of%2520reference%2520images%250Awith%2520these%2520corresponding%2520concepts%2520and%2520recognizes%2520them%2520in%2520the%2520queried%2520image%252C%250Aenabling%2520personalization.%2520We%2520note%2520that%2520the%2520computational%2520cost%2520and%2520parameter%250Acount%2520of%2520the%2520Aligner%2520are%2520negligible%2520within%2520the%2520entire%2520framework.%2520With%250Acomprehensive%2520qualitative%2520and%2520quantitative%2520analyses%252C%2520we%2520reveal%2520the%250Aeffectiveness%2520and%2520superiority%2520of%2520PLVM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Large%20Vision-Language%20Models&entry.906535625=Chau%20Pham%20and%20Hoang%20Phan%20and%20David%20Doermann%20and%20Yunjie%20Tian&entry.1292438233=%20%20The%20personalization%20model%20has%20gained%20significant%20attention%20in%20image%0Ageneration%20yet%20remains%20underexplored%20for%20large%20vision-language%20models%20%28LVLMs%29.%0ABeyond%20generic%20ones%2C%20with%20personalization%2C%20LVLMs%20handle%20interactive%20dialogues%0Ausing%20referential%20concepts%20%28e.g.%2C%20%60%60Mike%20and%20Susan%20are%20talking.%27%27%29%20instead%20of%0Athe%20generic%20form%20%28e.g.%2C%20%60%60a%20boy%20and%20a%20girl%20are%20talking.%27%27%29%2C%20making%20the%0Aconversation%20more%20customizable%20and%20referentially%20friendly.%20In%20addition%2C%20PLVM%20is%0Aequipped%20to%20continuously%20add%20new%20concepts%20during%20a%20dialogue%20without%20incurring%0Aadditional%20costs%2C%20which%20significantly%20enhances%20the%20practicality.%20PLVM%20proposes%0AAligner%2C%20a%20pre-trained%20visual%20encoder%20to%20align%20referential%20concepts%20with%20the%0Aqueried%20images.%20During%20the%20dialogues%2C%20it%20extracts%20features%20of%20reference%20images%0Awith%20these%20corresponding%20concepts%20and%20recognizes%20them%20in%20the%20queried%20image%2C%0Aenabling%20personalization.%20We%20note%20that%20the%20computational%20cost%20and%20parameter%0Acount%20of%20the%20Aligner%20are%20negligible%20within%20the%20entire%20framework.%20With%0Acomprehensive%20qualitative%20and%20quantitative%20analyses%2C%20we%20reveal%20the%0Aeffectiveness%20and%20superiority%20of%20PLVM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17610v1&entry.124074799=Read"},
{"title": "A Tunable Despeckling Neural Network Stabilized via Diffusion Equation", "author": "Yi Ran and Zhichang Guo and Jia Li and Yao Li and Martin Burger and Boying Wu", "abstract": "  The removal of multiplicative Gamma noise is a critical research area in the\napplication of synthetic aperture radar (SAR) imaging, where neural networks\nserve as a potent tool. However, real-world data often diverges from\ntheoretical models, exhibiting various disturbances, which makes the neural\nnetwork less effective. Adversarial attacks can be used as a criterion for\njudging the adaptability of neural networks to real data, since adversarial\nattacks can find the most extreme perturbations that make neural networks\nineffective. In this work, the diffusion equation is designed as a\nregularization block to provide sufficient regularity to the whole neural\nnetwork, due to its spontaneous dissipative nature. We propose a tunable,\nregularized neural network framework that unrolls a shallow denoising neural\nnetwork block and a diffusion regularity block into a single network for\nend-to-end training. The linear heat equation, known for its inherent\nsmoothness and low-pass filtering properties, is adopted as the diffusion\nregularization block. In our model, a single time step hyperparameter governs\nthe smoothness of the outputs and can be adjusted dynamically, significantly\nenhancing flexibility. The stability and convergence of our model are\ntheoretically proven. Experimental results demonstrate that the proposed model\neffectively eliminates high-frequency oscillations induced by adversarial\nattacks. Finally, the proposed model is benchmarked against several\nstate-of-the-art denoising methods on simulated images, adversarial samples,\nand real SAR images, achieving superior performance in both quantitative and\nvisual evaluations.\n", "link": "http://arxiv.org/abs/2411.15921v2", "date": "2024-12-23", "relevancy": 2.2778, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6003}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5864}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tunable%20Despeckling%20Neural%20Network%20Stabilized%20via%20Diffusion%20Equation&body=Title%3A%20A%20Tunable%20Despeckling%20Neural%20Network%20Stabilized%20via%20Diffusion%20Equation%0AAuthor%3A%20Yi%20Ran%20and%20Zhichang%20Guo%20and%20Jia%20Li%20and%20Yao%20Li%20and%20Martin%20Burger%20and%20Boying%20Wu%0AAbstract%3A%20%20%20The%20removal%20of%20multiplicative%20Gamma%20noise%20is%20a%20critical%20research%20area%20in%20the%0Aapplication%20of%20synthetic%20aperture%20radar%20%28SAR%29%20imaging%2C%20where%20neural%20networks%0Aserve%20as%20a%20potent%20tool.%20However%2C%20real-world%20data%20often%20diverges%20from%0Atheoretical%20models%2C%20exhibiting%20various%20disturbances%2C%20which%20makes%20the%20neural%0Anetwork%20less%20effective.%20Adversarial%20attacks%20can%20be%20used%20as%20a%20criterion%20for%0Ajudging%20the%20adaptability%20of%20neural%20networks%20to%20real%20data%2C%20since%20adversarial%0Aattacks%20can%20find%20the%20most%20extreme%20perturbations%20that%20make%20neural%20networks%0Aineffective.%20In%20this%20work%2C%20the%20diffusion%20equation%20is%20designed%20as%20a%0Aregularization%20block%20to%20provide%20sufficient%20regularity%20to%20the%20whole%20neural%0Anetwork%2C%20due%20to%20its%20spontaneous%20dissipative%20nature.%20We%20propose%20a%20tunable%2C%0Aregularized%20neural%20network%20framework%20that%20unrolls%20a%20shallow%20denoising%20neural%0Anetwork%20block%20and%20a%20diffusion%20regularity%20block%20into%20a%20single%20network%20for%0Aend-to-end%20training.%20The%20linear%20heat%20equation%2C%20known%20for%20its%20inherent%0Asmoothness%20and%20low-pass%20filtering%20properties%2C%20is%20adopted%20as%20the%20diffusion%0Aregularization%20block.%20In%20our%20model%2C%20a%20single%20time%20step%20hyperparameter%20governs%0Athe%20smoothness%20of%20the%20outputs%20and%20can%20be%20adjusted%20dynamically%2C%20significantly%0Aenhancing%20flexibility.%20The%20stability%20and%20convergence%20of%20our%20model%20are%0Atheoretically%20proven.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20model%0Aeffectively%20eliminates%20high-frequency%20oscillations%20induced%20by%20adversarial%0Aattacks.%20Finally%2C%20the%20proposed%20model%20is%20benchmarked%20against%20several%0Astate-of-the-art%20denoising%20methods%20on%20simulated%20images%2C%20adversarial%20samples%2C%0Aand%20real%20SAR%20images%2C%20achieving%20superior%20performance%20in%20both%20quantitative%20and%0Avisual%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tunable%2520Despeckling%2520Neural%2520Network%2520Stabilized%2520via%2520Diffusion%2520Equation%26entry.906535625%3DYi%2520Ran%2520and%2520Zhichang%2520Guo%2520and%2520Jia%2520Li%2520and%2520Yao%2520Li%2520and%2520Martin%2520Burger%2520and%2520Boying%2520Wu%26entry.1292438233%3D%2520%2520The%2520removal%2520of%2520multiplicative%2520Gamma%2520noise%2520is%2520a%2520critical%2520research%2520area%2520in%2520the%250Aapplication%2520of%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520imaging%252C%2520where%2520neural%2520networks%250Aserve%2520as%2520a%2520potent%2520tool.%2520However%252C%2520real-world%2520data%2520often%2520diverges%2520from%250Atheoretical%2520models%252C%2520exhibiting%2520various%2520disturbances%252C%2520which%2520makes%2520the%2520neural%250Anetwork%2520less%2520effective.%2520Adversarial%2520attacks%2520can%2520be%2520used%2520as%2520a%2520criterion%2520for%250Ajudging%2520the%2520adaptability%2520of%2520neural%2520networks%2520to%2520real%2520data%252C%2520since%2520adversarial%250Aattacks%2520can%2520find%2520the%2520most%2520extreme%2520perturbations%2520that%2520make%2520neural%2520networks%250Aineffective.%2520In%2520this%2520work%252C%2520the%2520diffusion%2520equation%2520is%2520designed%2520as%2520a%250Aregularization%2520block%2520to%2520provide%2520sufficient%2520regularity%2520to%2520the%2520whole%2520neural%250Anetwork%252C%2520due%2520to%2520its%2520spontaneous%2520dissipative%2520nature.%2520We%2520propose%2520a%2520tunable%252C%250Aregularized%2520neural%2520network%2520framework%2520that%2520unrolls%2520a%2520shallow%2520denoising%2520neural%250Anetwork%2520block%2520and%2520a%2520diffusion%2520regularity%2520block%2520into%2520a%2520single%2520network%2520for%250Aend-to-end%2520training.%2520The%2520linear%2520heat%2520equation%252C%2520known%2520for%2520its%2520inherent%250Asmoothness%2520and%2520low-pass%2520filtering%2520properties%252C%2520is%2520adopted%2520as%2520the%2520diffusion%250Aregularization%2520block.%2520In%2520our%2520model%252C%2520a%2520single%2520time%2520step%2520hyperparameter%2520governs%250Athe%2520smoothness%2520of%2520the%2520outputs%2520and%2520can%2520be%2520adjusted%2520dynamically%252C%2520significantly%250Aenhancing%2520flexibility.%2520The%2520stability%2520and%2520convergence%2520of%2520our%2520model%2520are%250Atheoretically%2520proven.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520model%250Aeffectively%2520eliminates%2520high-frequency%2520oscillations%2520induced%2520by%2520adversarial%250Aattacks.%2520Finally%252C%2520the%2520proposed%2520model%2520is%2520benchmarked%2520against%2520several%250Astate-of-the-art%2520denoising%2520methods%2520on%2520simulated%2520images%252C%2520adversarial%2520samples%252C%250Aand%2520real%2520SAR%2520images%252C%2520achieving%2520superior%2520performance%2520in%2520both%2520quantitative%2520and%250Avisual%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tunable%20Despeckling%20Neural%20Network%20Stabilized%20via%20Diffusion%20Equation&entry.906535625=Yi%20Ran%20and%20Zhichang%20Guo%20and%20Jia%20Li%20and%20Yao%20Li%20and%20Martin%20Burger%20and%20Boying%20Wu&entry.1292438233=%20%20The%20removal%20of%20multiplicative%20Gamma%20noise%20is%20a%20critical%20research%20area%20in%20the%0Aapplication%20of%20synthetic%20aperture%20radar%20%28SAR%29%20imaging%2C%20where%20neural%20networks%0Aserve%20as%20a%20potent%20tool.%20However%2C%20real-world%20data%20often%20diverges%20from%0Atheoretical%20models%2C%20exhibiting%20various%20disturbances%2C%20which%20makes%20the%20neural%0Anetwork%20less%20effective.%20Adversarial%20attacks%20can%20be%20used%20as%20a%20criterion%20for%0Ajudging%20the%20adaptability%20of%20neural%20networks%20to%20real%20data%2C%20since%20adversarial%0Aattacks%20can%20find%20the%20most%20extreme%20perturbations%20that%20make%20neural%20networks%0Aineffective.%20In%20this%20work%2C%20the%20diffusion%20equation%20is%20designed%20as%20a%0Aregularization%20block%20to%20provide%20sufficient%20regularity%20to%20the%20whole%20neural%0Anetwork%2C%20due%20to%20its%20spontaneous%20dissipative%20nature.%20We%20propose%20a%20tunable%2C%0Aregularized%20neural%20network%20framework%20that%20unrolls%20a%20shallow%20denoising%20neural%0Anetwork%20block%20and%20a%20diffusion%20regularity%20block%20into%20a%20single%20network%20for%0Aend-to-end%20training.%20The%20linear%20heat%20equation%2C%20known%20for%20its%20inherent%0Asmoothness%20and%20low-pass%20filtering%20properties%2C%20is%20adopted%20as%20the%20diffusion%0Aregularization%20block.%20In%20our%20model%2C%20a%20single%20time%20step%20hyperparameter%20governs%0Athe%20smoothness%20of%20the%20outputs%20and%20can%20be%20adjusted%20dynamically%2C%20significantly%0Aenhancing%20flexibility.%20The%20stability%20and%20convergence%20of%20our%20model%20are%0Atheoretically%20proven.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20model%0Aeffectively%20eliminates%20high-frequency%20oscillations%20induced%20by%20adversarial%0Aattacks.%20Finally%2C%20the%20proposed%20model%20is%20benchmarked%20against%20several%0Astate-of-the-art%20denoising%20methods%20on%20simulated%20images%2C%20adversarial%20samples%2C%0Aand%20real%20SAR%20images%2C%20achieving%20superior%20performance%20in%20both%20quantitative%20and%0Avisual%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15921v2&entry.124074799=Read"},
{"title": "Align-DETR: Enhancing End-to-end Object Detection with Aligned Loss", "author": "Zhi Cai and Songtao Liu and Guodong Wang and Zheng Ge and Xiangyu Zhang and Di Huang", "abstract": "  DETR has set up a simple end-to-end pipeline for object detection by\nformulating this task as a set prediction problem, showing promising potential.\nDespite its notable advancements, this paper identifies two key forms of\nmisalignment within the model: classification-regression misalignment and\ncross-layer target misalignment. Both issues impede DETR's convergence and\ndegrade its overall performance. To tackle both issues simultaneously, we\nintroduce a novel loss function, termed as Align Loss, designed to resolve the\ndiscrepancy between the two tasks. Align Loss guides the optimization of DETR\nthrough a joint quality metric, strengthening the connection between\nclassification and regression. Furthermore, it incorporates an exponential\ndown-weighting term to facilitate a smooth transition from positive to negative\nsamples. Align-DETR also employs many-to-one matching for supervision of\nintermediate layers, akin to the design of H-DETR, which enhances robustness\nagainst instability. We conducted extensive experiments, yielding highly\ncompetitive results. Notably, our method achieves a 49.3% (+0.6) AP on the\nH-DETR baseline with the ResNet-50 backbone. It also sets a new\nstate-of-the-art performance, reaching 50.5% AP in the 1x setting and 51.7% AP\nin the 2x setting, surpassing several strong competitors. Our code is available\nat https://github.com/FelixCaae/AlignDETR.\n", "link": "http://arxiv.org/abs/2304.07527v2", "date": "2024-12-23", "relevancy": 2.2772, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5974}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Align-DETR%3A%20Enhancing%20End-to-end%20Object%20Detection%20with%20Aligned%20Loss&body=Title%3A%20Align-DETR%3A%20Enhancing%20End-to-end%20Object%20Detection%20with%20Aligned%20Loss%0AAuthor%3A%20Zhi%20Cai%20and%20Songtao%20Liu%20and%20Guodong%20Wang%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Di%20Huang%0AAbstract%3A%20%20%20DETR%20has%20set%20up%20a%20simple%20end-to-end%20pipeline%20for%20object%20detection%20by%0Aformulating%20this%20task%20as%20a%20set%20prediction%20problem%2C%20showing%20promising%20potential.%0ADespite%20its%20notable%20advancements%2C%20this%20paper%20identifies%20two%20key%20forms%20of%0Amisalignment%20within%20the%20model%3A%20classification-regression%20misalignment%20and%0Across-layer%20target%20misalignment.%20Both%20issues%20impede%20DETR%27s%20convergence%20and%0Adegrade%20its%20overall%20performance.%20To%20tackle%20both%20issues%20simultaneously%2C%20we%0Aintroduce%20a%20novel%20loss%20function%2C%20termed%20as%20Align%20Loss%2C%20designed%20to%20resolve%20the%0Adiscrepancy%20between%20the%20two%20tasks.%20Align%20Loss%20guides%20the%20optimization%20of%20DETR%0Athrough%20a%20joint%20quality%20metric%2C%20strengthening%20the%20connection%20between%0Aclassification%20and%20regression.%20Furthermore%2C%20it%20incorporates%20an%20exponential%0Adown-weighting%20term%20to%20facilitate%20a%20smooth%20transition%20from%20positive%20to%20negative%0Asamples.%20Align-DETR%20also%20employs%20many-to-one%20matching%20for%20supervision%20of%0Aintermediate%20layers%2C%20akin%20to%20the%20design%20of%20H-DETR%2C%20which%20enhances%20robustness%0Aagainst%20instability.%20We%20conducted%20extensive%20experiments%2C%20yielding%20highly%0Acompetitive%20results.%20Notably%2C%20our%20method%20achieves%20a%2049.3%25%20%28%2B0.6%29%20AP%20on%20the%0AH-DETR%20baseline%20with%20the%20ResNet-50%20backbone.%20It%20also%20sets%20a%20new%0Astate-of-the-art%20performance%2C%20reaching%2050.5%25%20AP%20in%20the%201x%20setting%20and%2051.7%25%20AP%0Ain%20the%202x%20setting%2C%20surpassing%20several%20strong%20competitors.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/FelixCaae/AlignDETR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.07527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlign-DETR%253A%2520Enhancing%2520End-to-end%2520Object%2520Detection%2520with%2520Aligned%2520Loss%26entry.906535625%3DZhi%2520Cai%2520and%2520Songtao%2520Liu%2520and%2520Guodong%2520Wang%2520and%2520Zheng%2520Ge%2520and%2520Xiangyu%2520Zhang%2520and%2520Di%2520Huang%26entry.1292438233%3D%2520%2520DETR%2520has%2520set%2520up%2520a%2520simple%2520end-to-end%2520pipeline%2520for%2520object%2520detection%2520by%250Aformulating%2520this%2520task%2520as%2520a%2520set%2520prediction%2520problem%252C%2520showing%2520promising%2520potential.%250ADespite%2520its%2520notable%2520advancements%252C%2520this%2520paper%2520identifies%2520two%2520key%2520forms%2520of%250Amisalignment%2520within%2520the%2520model%253A%2520classification-regression%2520misalignment%2520and%250Across-layer%2520target%2520misalignment.%2520Both%2520issues%2520impede%2520DETR%2527s%2520convergence%2520and%250Adegrade%2520its%2520overall%2520performance.%2520To%2520tackle%2520both%2520issues%2520simultaneously%252C%2520we%250Aintroduce%2520a%2520novel%2520loss%2520function%252C%2520termed%2520as%2520Align%2520Loss%252C%2520designed%2520to%2520resolve%2520the%250Adiscrepancy%2520between%2520the%2520two%2520tasks.%2520Align%2520Loss%2520guides%2520the%2520optimization%2520of%2520DETR%250Athrough%2520a%2520joint%2520quality%2520metric%252C%2520strengthening%2520the%2520connection%2520between%250Aclassification%2520and%2520regression.%2520Furthermore%252C%2520it%2520incorporates%2520an%2520exponential%250Adown-weighting%2520term%2520to%2520facilitate%2520a%2520smooth%2520transition%2520from%2520positive%2520to%2520negative%250Asamples.%2520Align-DETR%2520also%2520employs%2520many-to-one%2520matching%2520for%2520supervision%2520of%250Aintermediate%2520layers%252C%2520akin%2520to%2520the%2520design%2520of%2520H-DETR%252C%2520which%2520enhances%2520robustness%250Aagainst%2520instability.%2520We%2520conducted%2520extensive%2520experiments%252C%2520yielding%2520highly%250Acompetitive%2520results.%2520Notably%252C%2520our%2520method%2520achieves%2520a%252049.3%2525%2520%2528%252B0.6%2529%2520AP%2520on%2520the%250AH-DETR%2520baseline%2520with%2520the%2520ResNet-50%2520backbone.%2520It%2520also%2520sets%2520a%2520new%250Astate-of-the-art%2520performance%252C%2520reaching%252050.5%2525%2520AP%2520in%2520the%25201x%2520setting%2520and%252051.7%2525%2520AP%250Ain%2520the%25202x%2520setting%252C%2520surpassing%2520several%2520strong%2520competitors.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/FelixCaae/AlignDETR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.07527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Align-DETR%3A%20Enhancing%20End-to-end%20Object%20Detection%20with%20Aligned%20Loss&entry.906535625=Zhi%20Cai%20and%20Songtao%20Liu%20and%20Guodong%20Wang%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Di%20Huang&entry.1292438233=%20%20DETR%20has%20set%20up%20a%20simple%20end-to-end%20pipeline%20for%20object%20detection%20by%0Aformulating%20this%20task%20as%20a%20set%20prediction%20problem%2C%20showing%20promising%20potential.%0ADespite%20its%20notable%20advancements%2C%20this%20paper%20identifies%20two%20key%20forms%20of%0Amisalignment%20within%20the%20model%3A%20classification-regression%20misalignment%20and%0Across-layer%20target%20misalignment.%20Both%20issues%20impede%20DETR%27s%20convergence%20and%0Adegrade%20its%20overall%20performance.%20To%20tackle%20both%20issues%20simultaneously%2C%20we%0Aintroduce%20a%20novel%20loss%20function%2C%20termed%20as%20Align%20Loss%2C%20designed%20to%20resolve%20the%0Adiscrepancy%20between%20the%20two%20tasks.%20Align%20Loss%20guides%20the%20optimization%20of%20DETR%0Athrough%20a%20joint%20quality%20metric%2C%20strengthening%20the%20connection%20between%0Aclassification%20and%20regression.%20Furthermore%2C%20it%20incorporates%20an%20exponential%0Adown-weighting%20term%20to%20facilitate%20a%20smooth%20transition%20from%20positive%20to%20negative%0Asamples.%20Align-DETR%20also%20employs%20many-to-one%20matching%20for%20supervision%20of%0Aintermediate%20layers%2C%20akin%20to%20the%20design%20of%20H-DETR%2C%20which%20enhances%20robustness%0Aagainst%20instability.%20We%20conducted%20extensive%20experiments%2C%20yielding%20highly%0Acompetitive%20results.%20Notably%2C%20our%20method%20achieves%20a%2049.3%25%20%28%2B0.6%29%20AP%20on%20the%0AH-DETR%20baseline%20with%20the%20ResNet-50%20backbone.%20It%20also%20sets%20a%20new%0Astate-of-the-art%20performance%2C%20reaching%2050.5%25%20AP%20in%20the%201x%20setting%20and%2051.7%25%20AP%0Ain%20the%202x%20setting%2C%20surpassing%20several%20strong%20competitors.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/FelixCaae/AlignDETR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.07527v2&entry.124074799=Read"},
{"title": "Towards Generalist Robot Policies: What Matters in Building\n  Vision-Language-Action Models", "author": "Xinghang Li and Peiyan Li and Minghuan Liu and Dong Wang and Jirong Liu and Bingyi Kang and Xiao Ma and Tao Kong and Hanbo Zhang and Huaping Liu", "abstract": "  Foundation Vision Language Models (VLMs) exhibit strong capabilities in\nmulti-modal representation learning, comprehension, and reasoning. By injecting\naction components into the VLMs, Vision-Language-Action Models (VLAs) can be\nnaturally formed and also show promising performance. Existing work has\ndemonstrated the effectiveness and generalization of VLAs in multiple scenarios\nand tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since\nexisting VLAs differ in their backbones, action-prediction formulations, data\ndistributions, and training recipes. This leads to a missing piece for a\nsystematic understanding of the design choices of VLAs. In this work, we\ndisclose the key factors that significantly influence the performance of VLA\nand focus on answering three essential design choices: which backbone to\nselect, how to formulate the VLA architectures, and when to add\ncross-embodiment data. The obtained results convince us firmly to explain why\nwe need VLA and develop a new family of VLAs, RoboVLMs, which require very few\nmanual designs and achieve a new state-of-the-art performance in three\nsimulation tasks and real-world experiments. Through our extensive experiments,\nwhich include over 8 VLM backbones, 4 policy architectures, and over 600\ndistinct designed experiments, we provide a detailed guidebook for the future\ndesign of VLAs. In addition to the study, the highly flexible RoboVLMs\nframework, which supports easy integrations of new VLMs and free combinations\nof various design choices, is made public to facilitate future research. We\nopen-source all details, including codes, models, datasets, and toolkits, along\nwith detailed training and evaluation recipes at: robovlms.github.io.\n", "link": "http://arxiv.org/abs/2412.14058v2", "date": "2024-12-23", "relevancy": 2.2762, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalist%20Robot%20Policies%3A%20What%20Matters%20in%20Building%0A%20%20Vision-Language-Action%20Models&body=Title%3A%20Towards%20Generalist%20Robot%20Policies%3A%20What%20Matters%20in%20Building%0A%20%20Vision-Language-Action%20Models%0AAuthor%3A%20Xinghang%20Li%20and%20Peiyan%20Li%20and%20Minghuan%20Liu%20and%20Dong%20Wang%20and%20Jirong%20Liu%20and%20Bingyi%20Kang%20and%20Xiao%20Ma%20and%20Tao%20Kong%20and%20Hanbo%20Zhang%20and%20Huaping%20Liu%0AAbstract%3A%20%20%20Foundation%20Vision%20Language%20Models%20%28VLMs%29%20exhibit%20strong%20capabilities%20in%0Amulti-modal%20representation%20learning%2C%20comprehension%2C%20and%20reasoning.%20By%20injecting%0Aaction%20components%20into%20the%20VLMs%2C%20Vision-Language-Action%20Models%20%28VLAs%29%20can%20be%0Anaturally%20formed%20and%20also%20show%20promising%20performance.%20Existing%20work%20has%0Ademonstrated%20the%20effectiveness%20and%20generalization%20of%20VLAs%20in%20multiple%20scenarios%0Aand%20tasks.%20Nevertheless%2C%20the%20transfer%20from%20VLMs%20to%20VLAs%20is%20not%20trivial%20since%0Aexisting%20VLAs%20differ%20in%20their%20backbones%2C%20action-prediction%20formulations%2C%20data%0Adistributions%2C%20and%20training%20recipes.%20This%20leads%20to%20a%20missing%20piece%20for%20a%0Asystematic%20understanding%20of%20the%20design%20choices%20of%20VLAs.%20In%20this%20work%2C%20we%0Adisclose%20the%20key%20factors%20that%20significantly%20influence%20the%20performance%20of%20VLA%0Aand%20focus%20on%20answering%20three%20essential%20design%20choices%3A%20which%20backbone%20to%0Aselect%2C%20how%20to%20formulate%20the%20VLA%20architectures%2C%20and%20when%20to%20add%0Across-embodiment%20data.%20The%20obtained%20results%20convince%20us%20firmly%20to%20explain%20why%0Awe%20need%20VLA%20and%20develop%20a%20new%20family%20of%20VLAs%2C%20RoboVLMs%2C%20which%20require%20very%20few%0Amanual%20designs%20and%20achieve%20a%20new%20state-of-the-art%20performance%20in%20three%0Asimulation%20tasks%20and%20real-world%20experiments.%20Through%20our%20extensive%20experiments%2C%0Awhich%20include%20over%208%20VLM%20backbones%2C%204%20policy%20architectures%2C%20and%20over%20600%0Adistinct%20designed%20experiments%2C%20we%20provide%20a%20detailed%20guidebook%20for%20the%20future%0Adesign%20of%20VLAs.%20In%20addition%20to%20the%20study%2C%20the%20highly%20flexible%20RoboVLMs%0Aframework%2C%20which%20supports%20easy%20integrations%20of%20new%20VLMs%20and%20free%20combinations%0Aof%20various%20design%20choices%2C%20is%20made%20public%20to%20facilitate%20future%20research.%20We%0Aopen-source%20all%20details%2C%20including%20codes%2C%20models%2C%20datasets%2C%20and%20toolkits%2C%20along%0Awith%20detailed%20training%20and%20evaluation%20recipes%20at%3A%20robovlms.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalist%2520Robot%2520Policies%253A%2520What%2520Matters%2520in%2520Building%250A%2520%2520Vision-Language-Action%2520Models%26entry.906535625%3DXinghang%2520Li%2520and%2520Peiyan%2520Li%2520and%2520Minghuan%2520Liu%2520and%2520Dong%2520Wang%2520and%2520Jirong%2520Liu%2520and%2520Bingyi%2520Kang%2520and%2520Xiao%2520Ma%2520and%2520Tao%2520Kong%2520and%2520Hanbo%2520Zhang%2520and%2520Huaping%2520Liu%26entry.1292438233%3D%2520%2520Foundation%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520strong%2520capabilities%2520in%250Amulti-modal%2520representation%2520learning%252C%2520comprehension%252C%2520and%2520reasoning.%2520By%2520injecting%250Aaction%2520components%2520into%2520the%2520VLMs%252C%2520Vision-Language-Action%2520Models%2520%2528VLAs%2529%2520can%2520be%250Anaturally%2520formed%2520and%2520also%2520show%2520promising%2520performance.%2520Existing%2520work%2520has%250Ademonstrated%2520the%2520effectiveness%2520and%2520generalization%2520of%2520VLAs%2520in%2520multiple%2520scenarios%250Aand%2520tasks.%2520Nevertheless%252C%2520the%2520transfer%2520from%2520VLMs%2520to%2520VLAs%2520is%2520not%2520trivial%2520since%250Aexisting%2520VLAs%2520differ%2520in%2520their%2520backbones%252C%2520action-prediction%2520formulations%252C%2520data%250Adistributions%252C%2520and%2520training%2520recipes.%2520This%2520leads%2520to%2520a%2520missing%2520piece%2520for%2520a%250Asystematic%2520understanding%2520of%2520the%2520design%2520choices%2520of%2520VLAs.%2520In%2520this%2520work%252C%2520we%250Adisclose%2520the%2520key%2520factors%2520that%2520significantly%2520influence%2520the%2520performance%2520of%2520VLA%250Aand%2520focus%2520on%2520answering%2520three%2520essential%2520design%2520choices%253A%2520which%2520backbone%2520to%250Aselect%252C%2520how%2520to%2520formulate%2520the%2520VLA%2520architectures%252C%2520and%2520when%2520to%2520add%250Across-embodiment%2520data.%2520The%2520obtained%2520results%2520convince%2520us%2520firmly%2520to%2520explain%2520why%250Awe%2520need%2520VLA%2520and%2520develop%2520a%2520new%2520family%2520of%2520VLAs%252C%2520RoboVLMs%252C%2520which%2520require%2520very%2520few%250Amanual%2520designs%2520and%2520achieve%2520a%2520new%2520state-of-the-art%2520performance%2520in%2520three%250Asimulation%2520tasks%2520and%2520real-world%2520experiments.%2520Through%2520our%2520extensive%2520experiments%252C%250Awhich%2520include%2520over%25208%2520VLM%2520backbones%252C%25204%2520policy%2520architectures%252C%2520and%2520over%2520600%250Adistinct%2520designed%2520experiments%252C%2520we%2520provide%2520a%2520detailed%2520guidebook%2520for%2520the%2520future%250Adesign%2520of%2520VLAs.%2520In%2520addition%2520to%2520the%2520study%252C%2520the%2520highly%2520flexible%2520RoboVLMs%250Aframework%252C%2520which%2520supports%2520easy%2520integrations%2520of%2520new%2520VLMs%2520and%2520free%2520combinations%250Aof%2520various%2520design%2520choices%252C%2520is%2520made%2520public%2520to%2520facilitate%2520future%2520research.%2520We%250Aopen-source%2520all%2520details%252C%2520including%2520codes%252C%2520models%252C%2520datasets%252C%2520and%2520toolkits%252C%2520along%250Awith%2520detailed%2520training%2520and%2520evaluation%2520recipes%2520at%253A%2520robovlms.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalist%20Robot%20Policies%3A%20What%20Matters%20in%20Building%0A%20%20Vision-Language-Action%20Models&entry.906535625=Xinghang%20Li%20and%20Peiyan%20Li%20and%20Minghuan%20Liu%20and%20Dong%20Wang%20and%20Jirong%20Liu%20and%20Bingyi%20Kang%20and%20Xiao%20Ma%20and%20Tao%20Kong%20and%20Hanbo%20Zhang%20and%20Huaping%20Liu&entry.1292438233=%20%20Foundation%20Vision%20Language%20Models%20%28VLMs%29%20exhibit%20strong%20capabilities%20in%0Amulti-modal%20representation%20learning%2C%20comprehension%2C%20and%20reasoning.%20By%20injecting%0Aaction%20components%20into%20the%20VLMs%2C%20Vision-Language-Action%20Models%20%28VLAs%29%20can%20be%0Anaturally%20formed%20and%20also%20show%20promising%20performance.%20Existing%20work%20has%0Ademonstrated%20the%20effectiveness%20and%20generalization%20of%20VLAs%20in%20multiple%20scenarios%0Aand%20tasks.%20Nevertheless%2C%20the%20transfer%20from%20VLMs%20to%20VLAs%20is%20not%20trivial%20since%0Aexisting%20VLAs%20differ%20in%20their%20backbones%2C%20action-prediction%20formulations%2C%20data%0Adistributions%2C%20and%20training%20recipes.%20This%20leads%20to%20a%20missing%20piece%20for%20a%0Asystematic%20understanding%20of%20the%20design%20choices%20of%20VLAs.%20In%20this%20work%2C%20we%0Adisclose%20the%20key%20factors%20that%20significantly%20influence%20the%20performance%20of%20VLA%0Aand%20focus%20on%20answering%20three%20essential%20design%20choices%3A%20which%20backbone%20to%0Aselect%2C%20how%20to%20formulate%20the%20VLA%20architectures%2C%20and%20when%20to%20add%0Across-embodiment%20data.%20The%20obtained%20results%20convince%20us%20firmly%20to%20explain%20why%0Awe%20need%20VLA%20and%20develop%20a%20new%20family%20of%20VLAs%2C%20RoboVLMs%2C%20which%20require%20very%20few%0Amanual%20designs%20and%20achieve%20a%20new%20state-of-the-art%20performance%20in%20three%0Asimulation%20tasks%20and%20real-world%20experiments.%20Through%20our%20extensive%20experiments%2C%0Awhich%20include%20over%208%20VLM%20backbones%2C%204%20policy%20architectures%2C%20and%20over%20600%0Adistinct%20designed%20experiments%2C%20we%20provide%20a%20detailed%20guidebook%20for%20the%20future%0Adesign%20of%20VLAs.%20In%20addition%20to%20the%20study%2C%20the%20highly%20flexible%20RoboVLMs%0Aframework%2C%20which%20supports%20easy%20integrations%20of%20new%20VLMs%20and%20free%20combinations%0Aof%20various%20design%20choices%2C%20is%20made%20public%20to%20facilitate%20future%20research.%20We%0Aopen-source%20all%20details%2C%20including%20codes%2C%20models%2C%20datasets%2C%20and%20toolkits%2C%20along%0Awith%20detailed%20training%20and%20evaluation%20recipes%20at%3A%20robovlms.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14058v2&entry.124074799=Read"},
{"title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus", "author": "Terufumi Morishita and Gaku Morio and Atsuki Yamaguchi and Yasuhiro Sogawa", "abstract": "  Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$_{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH.\n", "link": "http://arxiv.org/abs/2411.12498v2", "date": "2024-12-23", "relevancy": 2.2751, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4675}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Reasoning%20Capabilities%20of%20LLMs%20via%20Principled%20Synthetic%20Logic%0A%20%20Corpus&body=Title%3A%20Enhancing%20Reasoning%20Capabilities%20of%20LLMs%20via%20Principled%20Synthetic%20Logic%0A%20%20Corpus%0AAuthor%3A%20Terufumi%20Morishita%20and%20Gaku%20Morio%20and%20Atsuki%20Yamaguchi%20and%20Yasuhiro%20Sogawa%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20capable%20of%20solving%20a%20wide%20range%20of%20tasks%2C%0Ayet%20they%20have%20struggled%20with%20reasoning.%20To%20address%20this%2C%20we%20propose%0A%24%5Ctextbf%7BAdditional%20Logic%20Training%20%28ALT%29%7D%24%2C%20which%20aims%20to%20enhance%20LLMs%27%0Areasoning%20capabilities%20by%20program-generated%20logical%20reasoning%20samples.%20We%20first%0Aestablish%20principles%20for%20designing%20high-quality%20samples%20by%20integrating%20symbolic%0Alogic%20theory%20and%20previous%20empirical%20insights.%20Then%2C%20based%20on%20these%20principles%2C%0Awe%20construct%20a%20synthetic%20corpus%20named%20%24%5Ctextbf%7BFormal%20Logic%20Deduction%20Diverse%7D%24%0A%28%24%5Ctextbf%7BFLD%7D%24%24_%7B%5Ctimes%202%7D%24%29%2C%20comprising%20numerous%20samples%20of%20multi-step%0Adeduction%20with%20unknown%20facts%2C%20diverse%20reasoning%20rules%2C%20diverse%20linguistic%0Aexpressions%2C%20and%20challenging%20distractors.%20Finally%2C%20we%20empirically%20show%20that%20ALT%0Aon%20FLD%24_%7B%5Ctimes2%7D%24%20substantially%20enhances%20the%20reasoning%20capabilities%20of%0Astate-of-the-art%20LLMs%2C%20including%20LLaMA-3.1-70B.%20Improvements%20include%20gains%20of%0Aup%20to%2030%20points%20on%20logical%20reasoning%20benchmarks%2C%20up%20to%2010%20points%20on%20math%20and%0Acoding%20benchmarks%2C%20and%205%20points%20on%20the%20benchmark%20suite%20BBH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12498v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Reasoning%2520Capabilities%2520of%2520LLMs%2520via%2520Principled%2520Synthetic%2520Logic%250A%2520%2520Corpus%26entry.906535625%3DTerufumi%2520Morishita%2520and%2520Gaku%2520Morio%2520and%2520Atsuki%2520Yamaguchi%2520and%2520Yasuhiro%2520Sogawa%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520capable%2520of%2520solving%2520a%2520wide%2520range%2520of%2520tasks%252C%250Ayet%2520they%2520have%2520struggled%2520with%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520propose%250A%2524%255Ctextbf%257BAdditional%2520Logic%2520Training%2520%2528ALT%2529%257D%2524%252C%2520which%2520aims%2520to%2520enhance%2520LLMs%2527%250Areasoning%2520capabilities%2520by%2520program-generated%2520logical%2520reasoning%2520samples.%2520We%2520first%250Aestablish%2520principles%2520for%2520designing%2520high-quality%2520samples%2520by%2520integrating%2520symbolic%250Alogic%2520theory%2520and%2520previous%2520empirical%2520insights.%2520Then%252C%2520based%2520on%2520these%2520principles%252C%250Awe%2520construct%2520a%2520synthetic%2520corpus%2520named%2520%2524%255Ctextbf%257BFormal%2520Logic%2520Deduction%2520Diverse%257D%2524%250A%2528%2524%255Ctextbf%257BFLD%257D%2524%2524_%257B%255Ctimes%25202%257D%2524%2529%252C%2520comprising%2520numerous%2520samples%2520of%2520multi-step%250Adeduction%2520with%2520unknown%2520facts%252C%2520diverse%2520reasoning%2520rules%252C%2520diverse%2520linguistic%250Aexpressions%252C%2520and%2520challenging%2520distractors.%2520Finally%252C%2520we%2520empirically%2520show%2520that%2520ALT%250Aon%2520FLD%2524_%257B%255Ctimes2%257D%2524%2520substantially%2520enhances%2520the%2520reasoning%2520capabilities%2520of%250Astate-of-the-art%2520LLMs%252C%2520including%2520LLaMA-3.1-70B.%2520Improvements%2520include%2520gains%2520of%250Aup%2520to%252030%2520points%2520on%2520logical%2520reasoning%2520benchmarks%252C%2520up%2520to%252010%2520points%2520on%2520math%2520and%250Acoding%2520benchmarks%252C%2520and%25205%2520points%2520on%2520the%2520benchmark%2520suite%2520BBH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12498v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Reasoning%20Capabilities%20of%20LLMs%20via%20Principled%20Synthetic%20Logic%0A%20%20Corpus&entry.906535625=Terufumi%20Morishita%20and%20Gaku%20Morio%20and%20Atsuki%20Yamaguchi%20and%20Yasuhiro%20Sogawa&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20capable%20of%20solving%20a%20wide%20range%20of%20tasks%2C%0Ayet%20they%20have%20struggled%20with%20reasoning.%20To%20address%20this%2C%20we%20propose%0A%24%5Ctextbf%7BAdditional%20Logic%20Training%20%28ALT%29%7D%24%2C%20which%20aims%20to%20enhance%20LLMs%27%0Areasoning%20capabilities%20by%20program-generated%20logical%20reasoning%20samples.%20We%20first%0Aestablish%20principles%20for%20designing%20high-quality%20samples%20by%20integrating%20symbolic%0Alogic%20theory%20and%20previous%20empirical%20insights.%20Then%2C%20based%20on%20these%20principles%2C%0Awe%20construct%20a%20synthetic%20corpus%20named%20%24%5Ctextbf%7BFormal%20Logic%20Deduction%20Diverse%7D%24%0A%28%24%5Ctextbf%7BFLD%7D%24%24_%7B%5Ctimes%202%7D%24%29%2C%20comprising%20numerous%20samples%20of%20multi-step%0Adeduction%20with%20unknown%20facts%2C%20diverse%20reasoning%20rules%2C%20diverse%20linguistic%0Aexpressions%2C%20and%20challenging%20distractors.%20Finally%2C%20we%20empirically%20show%20that%20ALT%0Aon%20FLD%24_%7B%5Ctimes2%7D%24%20substantially%20enhances%20the%20reasoning%20capabilities%20of%0Astate-of-the-art%20LLMs%2C%20including%20LLaMA-3.1-70B.%20Improvements%20include%20gains%20of%0Aup%20to%2030%20points%20on%20logical%20reasoning%20benchmarks%2C%20up%20to%2010%20points%20on%20math%20and%0Acoding%20benchmarks%2C%20and%205%20points%20on%20the%20benchmark%20suite%20BBH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12498v2&entry.124074799=Read"},
{"title": "Detail-Preserving Latent Diffusion for Stable Shadow Removal", "author": "Jiamin Xu and Yuxin Zheng and Zelong Li and Chi Wang and Renshu Gu and Weiwei Xu and Gang Xu", "abstract": "  Achieving high-quality shadow removal with strong generalizability is\nchallenging in scenes with complex global illumination. Due to the limited\ndiversity in shadow removal datasets, current methods are prone to overfitting\ntraining data, often leading to reduced performance on unseen cases. To address\nthis, we leverage the rich visual priors of a pre-trained Stable Diffusion (SD)\nmodel and propose a two-stage fine-tuning pipeline to adapt the SD model for\nstable and efficient shadow removal. In the first stage, we fix the VAE and\nfine-tune the denoiser in latent space, which yields substantial shadow removal\nbut may lose some high-frequency details. To resolve this, we introduce a\nsecond stage, called the detail injection stage. This stage selectively\nextracts features from the VAE encoder to modulate the decoder, injecting fine\ndetails into the final results. Experimental results show that our method\noutperforms state-of-the-art shadow removal techniques. The cross-dataset\nevaluation further demonstrates that our method generalizes effectively to\nunseen data, enhancing the applicability of shadow removal methods.\n", "link": "http://arxiv.org/abs/2412.17630v1", "date": "2024-12-23", "relevancy": 2.2639, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6112}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5575}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detail-Preserving%20Latent%20Diffusion%20for%20Stable%20Shadow%20Removal&body=Title%3A%20Detail-Preserving%20Latent%20Diffusion%20for%20Stable%20Shadow%20Removal%0AAuthor%3A%20Jiamin%20Xu%20and%20Yuxin%20Zheng%20and%20Zelong%20Li%20and%20Chi%20Wang%20and%20Renshu%20Gu%20and%20Weiwei%20Xu%20and%20Gang%20Xu%0AAbstract%3A%20%20%20Achieving%20high-quality%20shadow%20removal%20with%20strong%20generalizability%20is%0Achallenging%20in%20scenes%20with%20complex%20global%20illumination.%20Due%20to%20the%20limited%0Adiversity%20in%20shadow%20removal%20datasets%2C%20current%20methods%20are%20prone%20to%20overfitting%0Atraining%20data%2C%20often%20leading%20to%20reduced%20performance%20on%20unseen%20cases.%20To%20address%0Athis%2C%20we%20leverage%20the%20rich%20visual%20priors%20of%20a%20pre-trained%20Stable%20Diffusion%20%28SD%29%0Amodel%20and%20propose%20a%20two-stage%20fine-tuning%20pipeline%20to%20adapt%20the%20SD%20model%20for%0Astable%20and%20efficient%20shadow%20removal.%20In%20the%20first%20stage%2C%20we%20fix%20the%20VAE%20and%0Afine-tune%20the%20denoiser%20in%20latent%20space%2C%20which%20yields%20substantial%20shadow%20removal%0Abut%20may%20lose%20some%20high-frequency%20details.%20To%20resolve%20this%2C%20we%20introduce%20a%0Asecond%20stage%2C%20called%20the%20detail%20injection%20stage.%20This%20stage%20selectively%0Aextracts%20features%20from%20the%20VAE%20encoder%20to%20modulate%20the%20decoder%2C%20injecting%20fine%0Adetails%20into%20the%20final%20results.%20Experimental%20results%20show%20that%20our%20method%0Aoutperforms%20state-of-the-art%20shadow%20removal%20techniques.%20The%20cross-dataset%0Aevaluation%20further%20demonstrates%20that%20our%20method%20generalizes%20effectively%20to%0Aunseen%20data%2C%20enhancing%20the%20applicability%20of%20shadow%20removal%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetail-Preserving%2520Latent%2520Diffusion%2520for%2520Stable%2520Shadow%2520Removal%26entry.906535625%3DJiamin%2520Xu%2520and%2520Yuxin%2520Zheng%2520and%2520Zelong%2520Li%2520and%2520Chi%2520Wang%2520and%2520Renshu%2520Gu%2520and%2520Weiwei%2520Xu%2520and%2520Gang%2520Xu%26entry.1292438233%3D%2520%2520Achieving%2520high-quality%2520shadow%2520removal%2520with%2520strong%2520generalizability%2520is%250Achallenging%2520in%2520scenes%2520with%2520complex%2520global%2520illumination.%2520Due%2520to%2520the%2520limited%250Adiversity%2520in%2520shadow%2520removal%2520datasets%252C%2520current%2520methods%2520are%2520prone%2520to%2520overfitting%250Atraining%2520data%252C%2520often%2520leading%2520to%2520reduced%2520performance%2520on%2520unseen%2520cases.%2520To%2520address%250Athis%252C%2520we%2520leverage%2520the%2520rich%2520visual%2520priors%2520of%2520a%2520pre-trained%2520Stable%2520Diffusion%2520%2528SD%2529%250Amodel%2520and%2520propose%2520a%2520two-stage%2520fine-tuning%2520pipeline%2520to%2520adapt%2520the%2520SD%2520model%2520for%250Astable%2520and%2520efficient%2520shadow%2520removal.%2520In%2520the%2520first%2520stage%252C%2520we%2520fix%2520the%2520VAE%2520and%250Afine-tune%2520the%2520denoiser%2520in%2520latent%2520space%252C%2520which%2520yields%2520substantial%2520shadow%2520removal%250Abut%2520may%2520lose%2520some%2520high-frequency%2520details.%2520To%2520resolve%2520this%252C%2520we%2520introduce%2520a%250Asecond%2520stage%252C%2520called%2520the%2520detail%2520injection%2520stage.%2520This%2520stage%2520selectively%250Aextracts%2520features%2520from%2520the%2520VAE%2520encoder%2520to%2520modulate%2520the%2520decoder%252C%2520injecting%2520fine%250Adetails%2520into%2520the%2520final%2520results.%2520Experimental%2520results%2520show%2520that%2520our%2520method%250Aoutperforms%2520state-of-the-art%2520shadow%2520removal%2520techniques.%2520The%2520cross-dataset%250Aevaluation%2520further%2520demonstrates%2520that%2520our%2520method%2520generalizes%2520effectively%2520to%250Aunseen%2520data%252C%2520enhancing%2520the%2520applicability%2520of%2520shadow%2520removal%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detail-Preserving%20Latent%20Diffusion%20for%20Stable%20Shadow%20Removal&entry.906535625=Jiamin%20Xu%20and%20Yuxin%20Zheng%20and%20Zelong%20Li%20and%20Chi%20Wang%20and%20Renshu%20Gu%20and%20Weiwei%20Xu%20and%20Gang%20Xu&entry.1292438233=%20%20Achieving%20high-quality%20shadow%20removal%20with%20strong%20generalizability%20is%0Achallenging%20in%20scenes%20with%20complex%20global%20illumination.%20Due%20to%20the%20limited%0Adiversity%20in%20shadow%20removal%20datasets%2C%20current%20methods%20are%20prone%20to%20overfitting%0Atraining%20data%2C%20often%20leading%20to%20reduced%20performance%20on%20unseen%20cases.%20To%20address%0Athis%2C%20we%20leverage%20the%20rich%20visual%20priors%20of%20a%20pre-trained%20Stable%20Diffusion%20%28SD%29%0Amodel%20and%20propose%20a%20two-stage%20fine-tuning%20pipeline%20to%20adapt%20the%20SD%20model%20for%0Astable%20and%20efficient%20shadow%20removal.%20In%20the%20first%20stage%2C%20we%20fix%20the%20VAE%20and%0Afine-tune%20the%20denoiser%20in%20latent%20space%2C%20which%20yields%20substantial%20shadow%20removal%0Abut%20may%20lose%20some%20high-frequency%20details.%20To%20resolve%20this%2C%20we%20introduce%20a%0Asecond%20stage%2C%20called%20the%20detail%20injection%20stage.%20This%20stage%20selectively%0Aextracts%20features%20from%20the%20VAE%20encoder%20to%20modulate%20the%20decoder%2C%20injecting%20fine%0Adetails%20into%20the%20final%20results.%20Experimental%20results%20show%20that%20our%20method%0Aoutperforms%20state-of-the-art%20shadow%20removal%20techniques.%20The%20cross-dataset%0Aevaluation%20further%20demonstrates%20that%20our%20method%20generalizes%20effectively%20to%0Aunseen%20data%2C%20enhancing%20the%20applicability%20of%20shadow%20removal%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17630v1&entry.124074799=Read"},
{"title": "CityBench: Evaluating the Capabilities of Large Language Models for\n  Urban Tasks", "author": "Jie Feng and Jun Zhang and Tianhui Liu and Xin Zhang and Tianjian Ouyang and Junbo Yan and Yuwei Du and Siqi Guo and Yong Li", "abstract": "  Recently, large language models (LLMs) with extensive general knowledge and\npowerful reasoning abilities have seen rapid development and widespread\napplication. A systematic and reliable evaluation of LLMs or vision-language\nmodel (VLMs) is a crucial step in applying and developing them for various\nfields. There have been some early explorations about the usability of LLMs for\nlimited urban tasks, but a systematic and scalable evaluation benchmark is\nstill lacking. The challenge in constructing a systematic evaluation benchmark\nfor urban research lies in the diversity of urban data, the complexity of\napplication scenarios and the highly dynamic nature of the urban environment.\nIn this paper, we design CityBench, an interactive simulator based evaluation\nplatform, as the first systematic benchmark for evaluating the capabilities of\nLLMs for diverse tasks in urban research. First, we build CityData to integrate\nthe diverse urban data and CitySimu to simulate fine-grained urban dynamics.\nBased on CityData and CitySimu, we design 8 representative urban tasks in 2\ncategories of perception-understanding and decision-making as the CityBench.\nWith extensive results from 30 well-known LLMs and VLMs in 13 cities around the\nworld, we find that advanced LLMs and VLMs can achieve competitive performance\nin diverse urban tasks requiring commonsense and semantic understanding\nabilities, e.g., understanding the human dynamics and semantic inference of\nurban images. Meanwhile, they fail to solve the challenging urban tasks\nrequiring professional knowledge and high-level reasoning abilities, e.g.,\ngeospatial prediction and traffic control task. These observations provide\nvaluable perspectives for utilizing and developing LLMs in the future. Codes\nare openly accessible via https://github.com/tsinghua-fib-lab/CityBench.\n", "link": "http://arxiv.org/abs/2406.13945v2", "date": "2024-12-23", "relevancy": 2.2415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CityBench%3A%20Evaluating%20the%20Capabilities%20of%20Large%20Language%20Models%20for%0A%20%20Urban%20Tasks&body=Title%3A%20CityBench%3A%20Evaluating%20the%20Capabilities%20of%20Large%20Language%20Models%20for%0A%20%20Urban%20Tasks%0AAuthor%3A%20Jie%20Feng%20and%20Jun%20Zhang%20and%20Tianhui%20Liu%20and%20Xin%20Zhang%20and%20Tianjian%20Ouyang%20and%20Junbo%20Yan%20and%20Yuwei%20Du%20and%20Siqi%20Guo%20and%20Yong%20Li%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20with%20extensive%20general%20knowledge%20and%0Apowerful%20reasoning%20abilities%20have%20seen%20rapid%20development%20and%20widespread%0Aapplication.%20A%20systematic%20and%20reliable%20evaluation%20of%20LLMs%20or%20vision-language%0Amodel%20%28VLMs%29%20is%20a%20crucial%20step%20in%20applying%20and%20developing%20them%20for%20various%0Afields.%20There%20have%20been%20some%20early%20explorations%20about%20the%20usability%20of%20LLMs%20for%0Alimited%20urban%20tasks%2C%20but%20a%20systematic%20and%20scalable%20evaluation%20benchmark%20is%0Astill%20lacking.%20The%20challenge%20in%20constructing%20a%20systematic%20evaluation%20benchmark%0Afor%20urban%20research%20lies%20in%20the%20diversity%20of%20urban%20data%2C%20the%20complexity%20of%0Aapplication%20scenarios%20and%20the%20highly%20dynamic%20nature%20of%20the%20urban%20environment.%0AIn%20this%20paper%2C%20we%20design%20CityBench%2C%20an%20interactive%20simulator%20based%20evaluation%0Aplatform%2C%20as%20the%20first%20systematic%20benchmark%20for%20evaluating%20the%20capabilities%20of%0ALLMs%20for%20diverse%20tasks%20in%20urban%20research.%20First%2C%20we%20build%20CityData%20to%20integrate%0Athe%20diverse%20urban%20data%20and%20CitySimu%20to%20simulate%20fine-grained%20urban%20dynamics.%0ABased%20on%20CityData%20and%20CitySimu%2C%20we%20design%208%20representative%20urban%20tasks%20in%202%0Acategories%20of%20perception-understanding%20and%20decision-making%20as%20the%20CityBench.%0AWith%20extensive%20results%20from%2030%20well-known%20LLMs%20and%20VLMs%20in%2013%20cities%20around%20the%0Aworld%2C%20we%20find%20that%20advanced%20LLMs%20and%20VLMs%20can%20achieve%20competitive%20performance%0Ain%20diverse%20urban%20tasks%20requiring%20commonsense%20and%20semantic%20understanding%0Aabilities%2C%20e.g.%2C%20understanding%20the%20human%20dynamics%20and%20semantic%20inference%20of%0Aurban%20images.%20Meanwhile%2C%20they%20fail%20to%20solve%20the%20challenging%20urban%20tasks%0Arequiring%20professional%20knowledge%20and%20high-level%20reasoning%20abilities%2C%20e.g.%2C%0Ageospatial%20prediction%20and%20traffic%20control%20task.%20These%20observations%20provide%0Avaluable%20perspectives%20for%20utilizing%20and%20developing%20LLMs%20in%20the%20future.%20Codes%0Aare%20openly%20accessible%20via%20https%3A//github.com/tsinghua-fib-lab/CityBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCityBench%253A%2520Evaluating%2520the%2520Capabilities%2520of%2520Large%2520Language%2520Models%2520for%250A%2520%2520Urban%2520Tasks%26entry.906535625%3DJie%2520Feng%2520and%2520Jun%2520Zhang%2520and%2520Tianhui%2520Liu%2520and%2520Xin%2520Zhang%2520and%2520Tianjian%2520Ouyang%2520and%2520Junbo%2520Yan%2520and%2520Yuwei%2520Du%2520and%2520Siqi%2520Guo%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520extensive%2520general%2520knowledge%2520and%250Apowerful%2520reasoning%2520abilities%2520have%2520seen%2520rapid%2520development%2520and%2520widespread%250Aapplication.%2520A%2520systematic%2520and%2520reliable%2520evaluation%2520of%2520LLMs%2520or%2520vision-language%250Amodel%2520%2528VLMs%2529%2520is%2520a%2520crucial%2520step%2520in%2520applying%2520and%2520developing%2520them%2520for%2520various%250Afields.%2520There%2520have%2520been%2520some%2520early%2520explorations%2520about%2520the%2520usability%2520of%2520LLMs%2520for%250Alimited%2520urban%2520tasks%252C%2520but%2520a%2520systematic%2520and%2520scalable%2520evaluation%2520benchmark%2520is%250Astill%2520lacking.%2520The%2520challenge%2520in%2520constructing%2520a%2520systematic%2520evaluation%2520benchmark%250Afor%2520urban%2520research%2520lies%2520in%2520the%2520diversity%2520of%2520urban%2520data%252C%2520the%2520complexity%2520of%250Aapplication%2520scenarios%2520and%2520the%2520highly%2520dynamic%2520nature%2520of%2520the%2520urban%2520environment.%250AIn%2520this%2520paper%252C%2520we%2520design%2520CityBench%252C%2520an%2520interactive%2520simulator%2520based%2520evaluation%250Aplatform%252C%2520as%2520the%2520first%2520systematic%2520benchmark%2520for%2520evaluating%2520the%2520capabilities%2520of%250ALLMs%2520for%2520diverse%2520tasks%2520in%2520urban%2520research.%2520First%252C%2520we%2520build%2520CityData%2520to%2520integrate%250Athe%2520diverse%2520urban%2520data%2520and%2520CitySimu%2520to%2520simulate%2520fine-grained%2520urban%2520dynamics.%250ABased%2520on%2520CityData%2520and%2520CitySimu%252C%2520we%2520design%25208%2520representative%2520urban%2520tasks%2520in%25202%250Acategories%2520of%2520perception-understanding%2520and%2520decision-making%2520as%2520the%2520CityBench.%250AWith%2520extensive%2520results%2520from%252030%2520well-known%2520LLMs%2520and%2520VLMs%2520in%252013%2520cities%2520around%2520the%250Aworld%252C%2520we%2520find%2520that%2520advanced%2520LLMs%2520and%2520VLMs%2520can%2520achieve%2520competitive%2520performance%250Ain%2520diverse%2520urban%2520tasks%2520requiring%2520commonsense%2520and%2520semantic%2520understanding%250Aabilities%252C%2520e.g.%252C%2520understanding%2520the%2520human%2520dynamics%2520and%2520semantic%2520inference%2520of%250Aurban%2520images.%2520Meanwhile%252C%2520they%2520fail%2520to%2520solve%2520the%2520challenging%2520urban%2520tasks%250Arequiring%2520professional%2520knowledge%2520and%2520high-level%2520reasoning%2520abilities%252C%2520e.g.%252C%250Ageospatial%2520prediction%2520and%2520traffic%2520control%2520task.%2520These%2520observations%2520provide%250Avaluable%2520perspectives%2520for%2520utilizing%2520and%2520developing%2520LLMs%2520in%2520the%2520future.%2520Codes%250Aare%2520openly%2520accessible%2520via%2520https%253A//github.com/tsinghua-fib-lab/CityBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CityBench%3A%20Evaluating%20the%20Capabilities%20of%20Large%20Language%20Models%20for%0A%20%20Urban%20Tasks&entry.906535625=Jie%20Feng%20and%20Jun%20Zhang%20and%20Tianhui%20Liu%20and%20Xin%20Zhang%20and%20Tianjian%20Ouyang%20and%20Junbo%20Yan%20and%20Yuwei%20Du%20and%20Siqi%20Guo%20and%20Yong%20Li&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20with%20extensive%20general%20knowledge%20and%0Apowerful%20reasoning%20abilities%20have%20seen%20rapid%20development%20and%20widespread%0Aapplication.%20A%20systematic%20and%20reliable%20evaluation%20of%20LLMs%20or%20vision-language%0Amodel%20%28VLMs%29%20is%20a%20crucial%20step%20in%20applying%20and%20developing%20them%20for%20various%0Afields.%20There%20have%20been%20some%20early%20explorations%20about%20the%20usability%20of%20LLMs%20for%0Alimited%20urban%20tasks%2C%20but%20a%20systematic%20and%20scalable%20evaluation%20benchmark%20is%0Astill%20lacking.%20The%20challenge%20in%20constructing%20a%20systematic%20evaluation%20benchmark%0Afor%20urban%20research%20lies%20in%20the%20diversity%20of%20urban%20data%2C%20the%20complexity%20of%0Aapplication%20scenarios%20and%20the%20highly%20dynamic%20nature%20of%20the%20urban%20environment.%0AIn%20this%20paper%2C%20we%20design%20CityBench%2C%20an%20interactive%20simulator%20based%20evaluation%0Aplatform%2C%20as%20the%20first%20systematic%20benchmark%20for%20evaluating%20the%20capabilities%20of%0ALLMs%20for%20diverse%20tasks%20in%20urban%20research.%20First%2C%20we%20build%20CityData%20to%20integrate%0Athe%20diverse%20urban%20data%20and%20CitySimu%20to%20simulate%20fine-grained%20urban%20dynamics.%0ABased%20on%20CityData%20and%20CitySimu%2C%20we%20design%208%20representative%20urban%20tasks%20in%202%0Acategories%20of%20perception-understanding%20and%20decision-making%20as%20the%20CityBench.%0AWith%20extensive%20results%20from%2030%20well-known%20LLMs%20and%20VLMs%20in%2013%20cities%20around%20the%0Aworld%2C%20we%20find%20that%20advanced%20LLMs%20and%20VLMs%20can%20achieve%20competitive%20performance%0Ain%20diverse%20urban%20tasks%20requiring%20commonsense%20and%20semantic%20understanding%0Aabilities%2C%20e.g.%2C%20understanding%20the%20human%20dynamics%20and%20semantic%20inference%20of%0Aurban%20images.%20Meanwhile%2C%20they%20fail%20to%20solve%20the%20challenging%20urban%20tasks%0Arequiring%20professional%20knowledge%20and%20high-level%20reasoning%20abilities%2C%20e.g.%2C%0Ageospatial%20prediction%20and%20traffic%20control%20task.%20These%20observations%20provide%0Avaluable%20perspectives%20for%20utilizing%20and%20developing%20LLMs%20in%20the%20future.%20Codes%0Aare%20openly%20accessible%20via%20https%3A//github.com/tsinghua-fib-lab/CityBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13945v2&entry.124074799=Read"},
{"title": "Survey of Large Multimodal Model Datasets, Application Categories and\n  Taxonomy", "author": "Priyaranjan Pattnayak and Hitesh Laxmichand Patel and Bhargava Kumar and Amit Agarwal and Ishan Banerjee and Srikant Panda and Tejaswini Kumar", "abstract": "  Multimodal learning, a rapidly evolving field in artificial intelligence,\nseeks to construct more versatile and robust systems by integrating and\nanalyzing diverse types of data, including text, images, audio, and video.\nInspired by the human ability to assimilate information through many senses,\nthis method enables applications such as text-to-video conversion, visual\nquestion answering, and image captioning. Recent developments in datasets that\nsupport multimodal language models (MLLMs) are highlighted in this overview.\nLarge-scale multimodal datasets are essential because they allow for thorough\ntesting and training of these models. With an emphasis on their contributions\nto the discipline, the study examines a variety of datasets, including those\nfor training, domain-specific tasks, and real-world applications. It also\nemphasizes how crucial benchmark datasets are for assessing models' performance\nin a range of scenarios, scalability, and applicability. Since multimodal\nlearning is always changing, overcoming these obstacles will help AI research\nand applications reach new heights.\n", "link": "http://arxiv.org/abs/2412.17759v1", "date": "2024-12-23", "relevancy": 2.2406, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5756}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20of%20Large%20Multimodal%20Model%20Datasets%2C%20Application%20Categories%20and%0A%20%20Taxonomy&body=Title%3A%20Survey%20of%20Large%20Multimodal%20Model%20Datasets%2C%20Application%20Categories%20and%0A%20%20Taxonomy%0AAuthor%3A%20Priyaranjan%20Pattnayak%20and%20Hitesh%20Laxmichand%20Patel%20and%20Bhargava%20Kumar%20and%20Amit%20Agarwal%20and%20Ishan%20Banerjee%20and%20Srikant%20Panda%20and%20Tejaswini%20Kumar%0AAbstract%3A%20%20%20Multimodal%20learning%2C%20a%20rapidly%20evolving%20field%20in%20artificial%20intelligence%2C%0Aseeks%20to%20construct%20more%20versatile%20and%20robust%20systems%20by%20integrating%20and%0Aanalyzing%20diverse%20types%20of%20data%2C%20including%20text%2C%20images%2C%20audio%2C%20and%20video.%0AInspired%20by%20the%20human%20ability%20to%20assimilate%20information%20through%20many%20senses%2C%0Athis%20method%20enables%20applications%20such%20as%20text-to-video%20conversion%2C%20visual%0Aquestion%20answering%2C%20and%20image%20captioning.%20Recent%20developments%20in%20datasets%20that%0Asupport%20multimodal%20language%20models%20%28MLLMs%29%20are%20highlighted%20in%20this%20overview.%0ALarge-scale%20multimodal%20datasets%20are%20essential%20because%20they%20allow%20for%20thorough%0Atesting%20and%20training%20of%20these%20models.%20With%20an%20emphasis%20on%20their%20contributions%0Ato%20the%20discipline%2C%20the%20study%20examines%20a%20variety%20of%20datasets%2C%20including%20those%0Afor%20training%2C%20domain-specific%20tasks%2C%20and%20real-world%20applications.%20It%20also%0Aemphasizes%20how%20crucial%20benchmark%20datasets%20are%20for%20assessing%20models%27%20performance%0Ain%20a%20range%20of%20scenarios%2C%20scalability%2C%20and%20applicability.%20Since%20multimodal%0Alearning%20is%20always%20changing%2C%20overcoming%20these%20obstacles%20will%20help%20AI%20research%0Aand%20applications%20reach%20new%20heights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520of%2520Large%2520Multimodal%2520Model%2520Datasets%252C%2520Application%2520Categories%2520and%250A%2520%2520Taxonomy%26entry.906535625%3DPriyaranjan%2520Pattnayak%2520and%2520Hitesh%2520Laxmichand%2520Patel%2520and%2520Bhargava%2520Kumar%2520and%2520Amit%2520Agarwal%2520and%2520Ishan%2520Banerjee%2520and%2520Srikant%2520Panda%2520and%2520Tejaswini%2520Kumar%26entry.1292438233%3D%2520%2520Multimodal%2520learning%252C%2520a%2520rapidly%2520evolving%2520field%2520in%2520artificial%2520intelligence%252C%250Aseeks%2520to%2520construct%2520more%2520versatile%2520and%2520robust%2520systems%2520by%2520integrating%2520and%250Aanalyzing%2520diverse%2520types%2520of%2520data%252C%2520including%2520text%252C%2520images%252C%2520audio%252C%2520and%2520video.%250AInspired%2520by%2520the%2520human%2520ability%2520to%2520assimilate%2520information%2520through%2520many%2520senses%252C%250Athis%2520method%2520enables%2520applications%2520such%2520as%2520text-to-video%2520conversion%252C%2520visual%250Aquestion%2520answering%252C%2520and%2520image%2520captioning.%2520Recent%2520developments%2520in%2520datasets%2520that%250Asupport%2520multimodal%2520language%2520models%2520%2528MLLMs%2529%2520are%2520highlighted%2520in%2520this%2520overview.%250ALarge-scale%2520multimodal%2520datasets%2520are%2520essential%2520because%2520they%2520allow%2520for%2520thorough%250Atesting%2520and%2520training%2520of%2520these%2520models.%2520With%2520an%2520emphasis%2520on%2520their%2520contributions%250Ato%2520the%2520discipline%252C%2520the%2520study%2520examines%2520a%2520variety%2520of%2520datasets%252C%2520including%2520those%250Afor%2520training%252C%2520domain-specific%2520tasks%252C%2520and%2520real-world%2520applications.%2520It%2520also%250Aemphasizes%2520how%2520crucial%2520benchmark%2520datasets%2520are%2520for%2520assessing%2520models%2527%2520performance%250Ain%2520a%2520range%2520of%2520scenarios%252C%2520scalability%252C%2520and%2520applicability.%2520Since%2520multimodal%250Alearning%2520is%2520always%2520changing%252C%2520overcoming%2520these%2520obstacles%2520will%2520help%2520AI%2520research%250Aand%2520applications%2520reach%2520new%2520heights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20of%20Large%20Multimodal%20Model%20Datasets%2C%20Application%20Categories%20and%0A%20%20Taxonomy&entry.906535625=Priyaranjan%20Pattnayak%20and%20Hitesh%20Laxmichand%20Patel%20and%20Bhargava%20Kumar%20and%20Amit%20Agarwal%20and%20Ishan%20Banerjee%20and%20Srikant%20Panda%20and%20Tejaswini%20Kumar&entry.1292438233=%20%20Multimodal%20learning%2C%20a%20rapidly%20evolving%20field%20in%20artificial%20intelligence%2C%0Aseeks%20to%20construct%20more%20versatile%20and%20robust%20systems%20by%20integrating%20and%0Aanalyzing%20diverse%20types%20of%20data%2C%20including%20text%2C%20images%2C%20audio%2C%20and%20video.%0AInspired%20by%20the%20human%20ability%20to%20assimilate%20information%20through%20many%20senses%2C%0Athis%20method%20enables%20applications%20such%20as%20text-to-video%20conversion%2C%20visual%0Aquestion%20answering%2C%20and%20image%20captioning.%20Recent%20developments%20in%20datasets%20that%0Asupport%20multimodal%20language%20models%20%28MLLMs%29%20are%20highlighted%20in%20this%20overview.%0ALarge-scale%20multimodal%20datasets%20are%20essential%20because%20they%20allow%20for%20thorough%0Atesting%20and%20training%20of%20these%20models.%20With%20an%20emphasis%20on%20their%20contributions%0Ato%20the%20discipline%2C%20the%20study%20examines%20a%20variety%20of%20datasets%2C%20including%20those%0Afor%20training%2C%20domain-specific%20tasks%2C%20and%20real-world%20applications.%20It%20also%0Aemphasizes%20how%20crucial%20benchmark%20datasets%20are%20for%20assessing%20models%27%20performance%0Ain%20a%20range%20of%20scenarios%2C%20scalability%2C%20and%20applicability.%20Since%20multimodal%0Alearning%20is%20always%20changing%2C%20overcoming%20these%20obstacles%20will%20help%20AI%20research%0Aand%20applications%20reach%20new%20heights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17759v1&entry.124074799=Read"},
{"title": "A Bias-Free Training Paradigm for More General AI-generated Image\n  Detection", "author": "Fabrizio Guillaro and Giada Zingarini and Ben Usman and Avneesh Sud and Davide Cozzolino and Luisa Verdoliva", "abstract": "  Successful forensic detectors can produce excellent results in supervised\nlearning benchmarks but struggle to transfer to real-world applications. We\nbelieve this limitation is largely due to inadequate training data quality.\nWhile most research focuses on developing new algorithms, less attention is\ngiven to training data selection, despite evidence that performance can be\nstrongly impacted by spurious correlations such as content, format, or\nresolution. A well-designed forensic detector should detect generator specific\nartifacts rather than reflect data biases. To this end, we propose B-Free, a\nbias-free training paradigm, where fake images are generated from real ones\nusing the conditioning procedure of stable diffusion models. This ensures\nsemantic alignment between real and fake images, allowing any differences to\nstem solely from the subtle artifacts introduced by AI generation. Through\ncontent-based augmentation, we show significant improvements in both\ngeneralization and robustness over state-of-the-art detectors and more\ncalibrated results across 27 different generative models, including recent\nreleases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the\nimportance of a careful dataset curation, highlighting the need for further\nresearch in dataset design. Code and data will be publicly available at\nhttps://grip-unina.github.io/B-Free/\n", "link": "http://arxiv.org/abs/2412.17671v1", "date": "2024-12-23", "relevancy": 2.2403, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.575}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5714}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bias-Free%20Training%20Paradigm%20for%20More%20General%20AI-generated%20Image%0A%20%20Detection&body=Title%3A%20A%20Bias-Free%20Training%20Paradigm%20for%20More%20General%20AI-generated%20Image%0A%20%20Detection%0AAuthor%3A%20Fabrizio%20Guillaro%20and%20Giada%20Zingarini%20and%20Ben%20Usman%20and%20Avneesh%20Sud%20and%20Davide%20Cozzolino%20and%20Luisa%20Verdoliva%0AAbstract%3A%20%20%20Successful%20forensic%20detectors%20can%20produce%20excellent%20results%20in%20supervised%0Alearning%20benchmarks%20but%20struggle%20to%20transfer%20to%20real-world%20applications.%20We%0Abelieve%20this%20limitation%20is%20largely%20due%20to%20inadequate%20training%20data%20quality.%0AWhile%20most%20research%20focuses%20on%20developing%20new%20algorithms%2C%20less%20attention%20is%0Agiven%20to%20training%20data%20selection%2C%20despite%20evidence%20that%20performance%20can%20be%0Astrongly%20impacted%20by%20spurious%20correlations%20such%20as%20content%2C%20format%2C%20or%0Aresolution.%20A%20well-designed%20forensic%20detector%20should%20detect%20generator%20specific%0Aartifacts%20rather%20than%20reflect%20data%20biases.%20To%20this%20end%2C%20we%20propose%20B-Free%2C%20a%0Abias-free%20training%20paradigm%2C%20where%20fake%20images%20are%20generated%20from%20real%20ones%0Ausing%20the%20conditioning%20procedure%20of%20stable%20diffusion%20models.%20This%20ensures%0Asemantic%20alignment%20between%20real%20and%20fake%20images%2C%20allowing%20any%20differences%20to%0Astem%20solely%20from%20the%20subtle%20artifacts%20introduced%20by%20AI%20generation.%20Through%0Acontent-based%20augmentation%2C%20we%20show%20significant%20improvements%20in%20both%0Ageneralization%20and%20robustness%20over%20state-of-the-art%20detectors%20and%20more%0Acalibrated%20results%20across%2027%20different%20generative%20models%2C%20including%20recent%0Areleases%2C%20like%20FLUX%20and%20Stable%20Diffusion%203.5.%20Our%20findings%20emphasize%20the%0Aimportance%20of%20a%20careful%20dataset%20curation%2C%20highlighting%20the%20need%20for%20further%0Aresearch%20in%20dataset%20design.%20Code%20and%20data%20will%20be%20publicly%20available%20at%0Ahttps%3A//grip-unina.github.io/B-Free/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bias-Free%2520Training%2520Paradigm%2520for%2520More%2520General%2520AI-generated%2520Image%250A%2520%2520Detection%26entry.906535625%3DFabrizio%2520Guillaro%2520and%2520Giada%2520Zingarini%2520and%2520Ben%2520Usman%2520and%2520Avneesh%2520Sud%2520and%2520Davide%2520Cozzolino%2520and%2520Luisa%2520Verdoliva%26entry.1292438233%3D%2520%2520Successful%2520forensic%2520detectors%2520can%2520produce%2520excellent%2520results%2520in%2520supervised%250Alearning%2520benchmarks%2520but%2520struggle%2520to%2520transfer%2520to%2520real-world%2520applications.%2520We%250Abelieve%2520this%2520limitation%2520is%2520largely%2520due%2520to%2520inadequate%2520training%2520data%2520quality.%250AWhile%2520most%2520research%2520focuses%2520on%2520developing%2520new%2520algorithms%252C%2520less%2520attention%2520is%250Agiven%2520to%2520training%2520data%2520selection%252C%2520despite%2520evidence%2520that%2520performance%2520can%2520be%250Astrongly%2520impacted%2520by%2520spurious%2520correlations%2520such%2520as%2520content%252C%2520format%252C%2520or%250Aresolution.%2520A%2520well-designed%2520forensic%2520detector%2520should%2520detect%2520generator%2520specific%250Aartifacts%2520rather%2520than%2520reflect%2520data%2520biases.%2520To%2520this%2520end%252C%2520we%2520propose%2520B-Free%252C%2520a%250Abias-free%2520training%2520paradigm%252C%2520where%2520fake%2520images%2520are%2520generated%2520from%2520real%2520ones%250Ausing%2520the%2520conditioning%2520procedure%2520of%2520stable%2520diffusion%2520models.%2520This%2520ensures%250Asemantic%2520alignment%2520between%2520real%2520and%2520fake%2520images%252C%2520allowing%2520any%2520differences%2520to%250Astem%2520solely%2520from%2520the%2520subtle%2520artifacts%2520introduced%2520by%2520AI%2520generation.%2520Through%250Acontent-based%2520augmentation%252C%2520we%2520show%2520significant%2520improvements%2520in%2520both%250Ageneralization%2520and%2520robustness%2520over%2520state-of-the-art%2520detectors%2520and%2520more%250Acalibrated%2520results%2520across%252027%2520different%2520generative%2520models%252C%2520including%2520recent%250Areleases%252C%2520like%2520FLUX%2520and%2520Stable%2520Diffusion%25203.5.%2520Our%2520findings%2520emphasize%2520the%250Aimportance%2520of%2520a%2520careful%2520dataset%2520curation%252C%2520highlighting%2520the%2520need%2520for%2520further%250Aresearch%2520in%2520dataset%2520design.%2520Code%2520and%2520data%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//grip-unina.github.io/B-Free/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bias-Free%20Training%20Paradigm%20for%20More%20General%20AI-generated%20Image%0A%20%20Detection&entry.906535625=Fabrizio%20Guillaro%20and%20Giada%20Zingarini%20and%20Ben%20Usman%20and%20Avneesh%20Sud%20and%20Davide%20Cozzolino%20and%20Luisa%20Verdoliva&entry.1292438233=%20%20Successful%20forensic%20detectors%20can%20produce%20excellent%20results%20in%20supervised%0Alearning%20benchmarks%20but%20struggle%20to%20transfer%20to%20real-world%20applications.%20We%0Abelieve%20this%20limitation%20is%20largely%20due%20to%20inadequate%20training%20data%20quality.%0AWhile%20most%20research%20focuses%20on%20developing%20new%20algorithms%2C%20less%20attention%20is%0Agiven%20to%20training%20data%20selection%2C%20despite%20evidence%20that%20performance%20can%20be%0Astrongly%20impacted%20by%20spurious%20correlations%20such%20as%20content%2C%20format%2C%20or%0Aresolution.%20A%20well-designed%20forensic%20detector%20should%20detect%20generator%20specific%0Aartifacts%20rather%20than%20reflect%20data%20biases.%20To%20this%20end%2C%20we%20propose%20B-Free%2C%20a%0Abias-free%20training%20paradigm%2C%20where%20fake%20images%20are%20generated%20from%20real%20ones%0Ausing%20the%20conditioning%20procedure%20of%20stable%20diffusion%20models.%20This%20ensures%0Asemantic%20alignment%20between%20real%20and%20fake%20images%2C%20allowing%20any%20differences%20to%0Astem%20solely%20from%20the%20subtle%20artifacts%20introduced%20by%20AI%20generation.%20Through%0Acontent-based%20augmentation%2C%20we%20show%20significant%20improvements%20in%20both%0Ageneralization%20and%20robustness%20over%20state-of-the-art%20detectors%20and%20more%0Acalibrated%20results%20across%2027%20different%20generative%20models%2C%20including%20recent%0Areleases%2C%20like%20FLUX%20and%20Stable%20Diffusion%203.5.%20Our%20findings%20emphasize%20the%0Aimportance%20of%20a%20careful%20dataset%20curation%2C%20highlighting%20the%20need%20for%20further%0Aresearch%20in%20dataset%20design.%20Code%20and%20data%20will%20be%20publicly%20available%20at%0Ahttps%3A//grip-unina.github.io/B-Free/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17671v1&entry.124074799=Read"},
{"title": "C2F-TP: A Coarse-to-Fine Denoising Framework for Uncertainty-Aware\n  Trajectory Prediction", "author": "Zichen Wang and Hao Miao and Senzhang Wang and Renzhi Wang and Jianxin Wang and Jian Zhang", "abstract": "  Accurately predicting the trajectory of vehicles is critically important for\nensuring safety and reliability in autonomous driving. Although considerable\nresearch efforts have been made recently, the inherent trajectory uncertainty\ncaused by various factors including the dynamic driving intends and the diverse\ndriving scenarios still poses significant challenges to accurate trajectory\nprediction. To address this issue, we propose C2F-TP, a coarse-to-fine\ndenoising framework for uncertainty-aware vehicle trajectory prediction. C2F-TP\nfeatures an innovative two-stage coarse-to-fine prediction process.\nSpecifically, in the spatial-temporal interaction stage, we propose a\nspatial-temporal interaction module to capture the inter-vehicle interactions\nand learn a multimodal trajectory distribution, from which a certain number of\nnoisy trajectories are sampled. Next, in the trajectory refinement stage, we\ndesign a conditional denoising model to reduce the uncertainty of the sampled\ntrajectories through a step-wise denoising operation. Extensive experiments are\nconducted on two real datasets NGSIM and highD that are widely adopted in\ntrajectory prediction. The result demonstrates the effectiveness of our\nproposal.\n", "link": "http://arxiv.org/abs/2412.13231v2", "date": "2024-12-23", "relevancy": 2.2394, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5641}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5641}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C2F-TP%3A%20A%20Coarse-to-Fine%20Denoising%20Framework%20for%20Uncertainty-Aware%0A%20%20Trajectory%20Prediction&body=Title%3A%20C2F-TP%3A%20A%20Coarse-to-Fine%20Denoising%20Framework%20for%20Uncertainty-Aware%0A%20%20Trajectory%20Prediction%0AAuthor%3A%20Zichen%20Wang%20and%20Hao%20Miao%20and%20Senzhang%20Wang%20and%20Renzhi%20Wang%20and%20Jianxin%20Wang%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20trajectory%20of%20vehicles%20is%20critically%20important%20for%0Aensuring%20safety%20and%20reliability%20in%20autonomous%20driving.%20Although%20considerable%0Aresearch%20efforts%20have%20been%20made%20recently%2C%20the%20inherent%20trajectory%20uncertainty%0Acaused%20by%20various%20factors%20including%20the%20dynamic%20driving%20intends%20and%20the%20diverse%0Adriving%20scenarios%20still%20poses%20significant%20challenges%20to%20accurate%20trajectory%0Aprediction.%20To%20address%20this%20issue%2C%20we%20propose%20C2F-TP%2C%20a%20coarse-to-fine%0Adenoising%20framework%20for%20uncertainty-aware%20vehicle%20trajectory%20prediction.%20C2F-TP%0Afeatures%20an%20innovative%20two-stage%20coarse-to-fine%20prediction%20process.%0ASpecifically%2C%20in%20the%20spatial-temporal%20interaction%20stage%2C%20we%20propose%20a%0Aspatial-temporal%20interaction%20module%20to%20capture%20the%20inter-vehicle%20interactions%0Aand%20learn%20a%20multimodal%20trajectory%20distribution%2C%20from%20which%20a%20certain%20number%20of%0Anoisy%20trajectories%20are%20sampled.%20Next%2C%20in%20the%20trajectory%20refinement%20stage%2C%20we%0Adesign%20a%20conditional%20denoising%20model%20to%20reduce%20the%20uncertainty%20of%20the%20sampled%0Atrajectories%20through%20a%20step-wise%20denoising%20operation.%20Extensive%20experiments%20are%0Aconducted%20on%20two%20real%20datasets%20NGSIM%20and%20highD%20that%20are%20widely%20adopted%20in%0Atrajectory%20prediction.%20The%20result%20demonstrates%20the%20effectiveness%20of%20our%0Aproposal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC2F-TP%253A%2520A%2520Coarse-to-Fine%2520Denoising%2520Framework%2520for%2520Uncertainty-Aware%250A%2520%2520Trajectory%2520Prediction%26entry.906535625%3DZichen%2520Wang%2520and%2520Hao%2520Miao%2520and%2520Senzhang%2520Wang%2520and%2520Renzhi%2520Wang%2520and%2520Jianxin%2520Wang%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520trajectory%2520of%2520vehicles%2520is%2520critically%2520important%2520for%250Aensuring%2520safety%2520and%2520reliability%2520in%2520autonomous%2520driving.%2520Although%2520considerable%250Aresearch%2520efforts%2520have%2520been%2520made%2520recently%252C%2520the%2520inherent%2520trajectory%2520uncertainty%250Acaused%2520by%2520various%2520factors%2520including%2520the%2520dynamic%2520driving%2520intends%2520and%2520the%2520diverse%250Adriving%2520scenarios%2520still%2520poses%2520significant%2520challenges%2520to%2520accurate%2520trajectory%250Aprediction.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520C2F-TP%252C%2520a%2520coarse-to-fine%250Adenoising%2520framework%2520for%2520uncertainty-aware%2520vehicle%2520trajectory%2520prediction.%2520C2F-TP%250Afeatures%2520an%2520innovative%2520two-stage%2520coarse-to-fine%2520prediction%2520process.%250ASpecifically%252C%2520in%2520the%2520spatial-temporal%2520interaction%2520stage%252C%2520we%2520propose%2520a%250Aspatial-temporal%2520interaction%2520module%2520to%2520capture%2520the%2520inter-vehicle%2520interactions%250Aand%2520learn%2520a%2520multimodal%2520trajectory%2520distribution%252C%2520from%2520which%2520a%2520certain%2520number%2520of%250Anoisy%2520trajectories%2520are%2520sampled.%2520Next%252C%2520in%2520the%2520trajectory%2520refinement%2520stage%252C%2520we%250Adesign%2520a%2520conditional%2520denoising%2520model%2520to%2520reduce%2520the%2520uncertainty%2520of%2520the%2520sampled%250Atrajectories%2520through%2520a%2520step-wise%2520denoising%2520operation.%2520Extensive%2520experiments%2520are%250Aconducted%2520on%2520two%2520real%2520datasets%2520NGSIM%2520and%2520highD%2520that%2520are%2520widely%2520adopted%2520in%250Atrajectory%2520prediction.%2520The%2520result%2520demonstrates%2520the%2520effectiveness%2520of%2520our%250Aproposal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C2F-TP%3A%20A%20Coarse-to-Fine%20Denoising%20Framework%20for%20Uncertainty-Aware%0A%20%20Trajectory%20Prediction&entry.906535625=Zichen%20Wang%20and%20Hao%20Miao%20and%20Senzhang%20Wang%20and%20Renzhi%20Wang%20and%20Jianxin%20Wang%20and%20Jian%20Zhang&entry.1292438233=%20%20Accurately%20predicting%20the%20trajectory%20of%20vehicles%20is%20critically%20important%20for%0Aensuring%20safety%20and%20reliability%20in%20autonomous%20driving.%20Although%20considerable%0Aresearch%20efforts%20have%20been%20made%20recently%2C%20the%20inherent%20trajectory%20uncertainty%0Acaused%20by%20various%20factors%20including%20the%20dynamic%20driving%20intends%20and%20the%20diverse%0Adriving%20scenarios%20still%20poses%20significant%20challenges%20to%20accurate%20trajectory%0Aprediction.%20To%20address%20this%20issue%2C%20we%20propose%20C2F-TP%2C%20a%20coarse-to-fine%0Adenoising%20framework%20for%20uncertainty-aware%20vehicle%20trajectory%20prediction.%20C2F-TP%0Afeatures%20an%20innovative%20two-stage%20coarse-to-fine%20prediction%20process.%0ASpecifically%2C%20in%20the%20spatial-temporal%20interaction%20stage%2C%20we%20propose%20a%0Aspatial-temporal%20interaction%20module%20to%20capture%20the%20inter-vehicle%20interactions%0Aand%20learn%20a%20multimodal%20trajectory%20distribution%2C%20from%20which%20a%20certain%20number%20of%0Anoisy%20trajectories%20are%20sampled.%20Next%2C%20in%20the%20trajectory%20refinement%20stage%2C%20we%0Adesign%20a%20conditional%20denoising%20model%20to%20reduce%20the%20uncertainty%20of%20the%20sampled%0Atrajectories%20through%20a%20step-wise%20denoising%20operation.%20Extensive%20experiments%20are%0Aconducted%20on%20two%20real%20datasets%20NGSIM%20and%20highD%20that%20are%20widely%20adopted%20in%0Atrajectory%20prediction.%20The%20result%20demonstrates%20the%20effectiveness%20of%20our%0Aproposal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13231v2&entry.124074799=Read"},
{"title": "Benchmarking Generative AI Models for Deep Learning Test Input\n  Generation", "author": " Maryam and Matteo Biagiola and Andrea Stocco and Vincenzo Riccio", "abstract": "  Test Input Generators (TIGs) are crucial to assess the ability of Deep\nLearning (DL) image classifiers to provide correct predictions for inputs\nbeyond their training and test sets. Recent advancements in Generative AI\n(GenAI) models have made them a powerful tool for creating and manipulating\nsynthetic images, although these advancements also imply increased complexity\nand resource demands for training.\n  In this work, we benchmark and combine different GenAI models with TIGs,\nassessing their effectiveness, efficiency, and quality of the generated test\nimages, in terms of domain validity and label preservation. We conduct an\nempirical study involving three different GenAI architectures (VAEs, GANs,\nDiffusion Models), five classification tasks of increasing complexity, and 364\nhuman evaluations. Our results show that simpler architectures, such as VAEs,\nare sufficient for less complex datasets like MNIST. However, when dealing with\nfeature-rich datasets, such as ImageNet, more sophisticated architectures like\nDiffusion Models achieve superior performance by generating a higher number of\nvalid, misclassification-inducing inputs.\n", "link": "http://arxiv.org/abs/2412.17652v1", "date": "2024-12-23", "relevancy": 2.2388, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5744}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5599}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Generative%20AI%20Models%20for%20Deep%20Learning%20Test%20Input%0A%20%20Generation&body=Title%3A%20Benchmarking%20Generative%20AI%20Models%20for%20Deep%20Learning%20Test%20Input%0A%20%20Generation%0AAuthor%3A%20%20Maryam%20and%20Matteo%20Biagiola%20and%20Andrea%20Stocco%20and%20Vincenzo%20Riccio%0AAbstract%3A%20%20%20Test%20Input%20Generators%20%28TIGs%29%20are%20crucial%20to%20assess%20the%20ability%20of%20Deep%0ALearning%20%28DL%29%20image%20classifiers%20to%20provide%20correct%20predictions%20for%20inputs%0Abeyond%20their%20training%20and%20test%20sets.%20Recent%20advancements%20in%20Generative%20AI%0A%28GenAI%29%20models%20have%20made%20them%20a%20powerful%20tool%20for%20creating%20and%20manipulating%0Asynthetic%20images%2C%20although%20these%20advancements%20also%20imply%20increased%20complexity%0Aand%20resource%20demands%20for%20training.%0A%20%20In%20this%20work%2C%20we%20benchmark%20and%20combine%20different%20GenAI%20models%20with%20TIGs%2C%0Aassessing%20their%20effectiveness%2C%20efficiency%2C%20and%20quality%20of%20the%20generated%20test%0Aimages%2C%20in%20terms%20of%20domain%20validity%20and%20label%20preservation.%20We%20conduct%20an%0Aempirical%20study%20involving%20three%20different%20GenAI%20architectures%20%28VAEs%2C%20GANs%2C%0ADiffusion%20Models%29%2C%20five%20classification%20tasks%20of%20increasing%20complexity%2C%20and%20364%0Ahuman%20evaluations.%20Our%20results%20show%20that%20simpler%20architectures%2C%20such%20as%20VAEs%2C%0Aare%20sufficient%20for%20less%20complex%20datasets%20like%20MNIST.%20However%2C%20when%20dealing%20with%0Afeature-rich%20datasets%2C%20such%20as%20ImageNet%2C%20more%20sophisticated%20architectures%20like%0ADiffusion%20Models%20achieve%20superior%20performance%20by%20generating%20a%20higher%20number%20of%0Avalid%2C%20misclassification-inducing%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Generative%2520AI%2520Models%2520for%2520Deep%2520Learning%2520Test%2520Input%250A%2520%2520Generation%26entry.906535625%3D%2520Maryam%2520and%2520Matteo%2520Biagiola%2520and%2520Andrea%2520Stocco%2520and%2520Vincenzo%2520Riccio%26entry.1292438233%3D%2520%2520Test%2520Input%2520Generators%2520%2528TIGs%2529%2520are%2520crucial%2520to%2520assess%2520the%2520ability%2520of%2520Deep%250ALearning%2520%2528DL%2529%2520image%2520classifiers%2520to%2520provide%2520correct%2520predictions%2520for%2520inputs%250Abeyond%2520their%2520training%2520and%2520test%2520sets.%2520Recent%2520advancements%2520in%2520Generative%2520AI%250A%2528GenAI%2529%2520models%2520have%2520made%2520them%2520a%2520powerful%2520tool%2520for%2520creating%2520and%2520manipulating%250Asynthetic%2520images%252C%2520although%2520these%2520advancements%2520also%2520imply%2520increased%2520complexity%250Aand%2520resource%2520demands%2520for%2520training.%250A%2520%2520In%2520this%2520work%252C%2520we%2520benchmark%2520and%2520combine%2520different%2520GenAI%2520models%2520with%2520TIGs%252C%250Aassessing%2520their%2520effectiveness%252C%2520efficiency%252C%2520and%2520quality%2520of%2520the%2520generated%2520test%250Aimages%252C%2520in%2520terms%2520of%2520domain%2520validity%2520and%2520label%2520preservation.%2520We%2520conduct%2520an%250Aempirical%2520study%2520involving%2520three%2520different%2520GenAI%2520architectures%2520%2528VAEs%252C%2520GANs%252C%250ADiffusion%2520Models%2529%252C%2520five%2520classification%2520tasks%2520of%2520increasing%2520complexity%252C%2520and%2520364%250Ahuman%2520evaluations.%2520Our%2520results%2520show%2520that%2520simpler%2520architectures%252C%2520such%2520as%2520VAEs%252C%250Aare%2520sufficient%2520for%2520less%2520complex%2520datasets%2520like%2520MNIST.%2520However%252C%2520when%2520dealing%2520with%250Afeature-rich%2520datasets%252C%2520such%2520as%2520ImageNet%252C%2520more%2520sophisticated%2520architectures%2520like%250ADiffusion%2520Models%2520achieve%2520superior%2520performance%2520by%2520generating%2520a%2520higher%2520number%2520of%250Avalid%252C%2520misclassification-inducing%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Generative%20AI%20Models%20for%20Deep%20Learning%20Test%20Input%0A%20%20Generation&entry.906535625=%20Maryam%20and%20Matteo%20Biagiola%20and%20Andrea%20Stocco%20and%20Vincenzo%20Riccio&entry.1292438233=%20%20Test%20Input%20Generators%20%28TIGs%29%20are%20crucial%20to%20assess%20the%20ability%20of%20Deep%0ALearning%20%28DL%29%20image%20classifiers%20to%20provide%20correct%20predictions%20for%20inputs%0Abeyond%20their%20training%20and%20test%20sets.%20Recent%20advancements%20in%20Generative%20AI%0A%28GenAI%29%20models%20have%20made%20them%20a%20powerful%20tool%20for%20creating%20and%20manipulating%0Asynthetic%20images%2C%20although%20these%20advancements%20also%20imply%20increased%20complexity%0Aand%20resource%20demands%20for%20training.%0A%20%20In%20this%20work%2C%20we%20benchmark%20and%20combine%20different%20GenAI%20models%20with%20TIGs%2C%0Aassessing%20their%20effectiveness%2C%20efficiency%2C%20and%20quality%20of%20the%20generated%20test%0Aimages%2C%20in%20terms%20of%20domain%20validity%20and%20label%20preservation.%20We%20conduct%20an%0Aempirical%20study%20involving%20three%20different%20GenAI%20architectures%20%28VAEs%2C%20GANs%2C%0ADiffusion%20Models%29%2C%20five%20classification%20tasks%20of%20increasing%20complexity%2C%20and%20364%0Ahuman%20evaluations.%20Our%20results%20show%20that%20simpler%20architectures%2C%20such%20as%20VAEs%2C%0Aare%20sufficient%20for%20less%20complex%20datasets%20like%20MNIST.%20However%2C%20when%20dealing%20with%0Afeature-rich%20datasets%2C%20such%20as%20ImageNet%2C%20more%20sophisticated%20architectures%20like%0ADiffusion%20Models%20achieve%20superior%20performance%20by%20generating%20a%20higher%20number%20of%0Avalid%2C%20misclassification-inducing%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17652v1&entry.124074799=Read"},
{"title": "Be More Diverse than the Most Diverse: Online Selection of Diverse\n  Mixtures of Generative Models", "author": "Parham Rezaei and Farzan Farnia and Cheuk Ting Li", "abstract": "  The availability of multiple training algorithms and architectures for\ngenerative models requires a selection mechanism to form a single model over a\ngroup of well-trained generation models. The selection task is commonly\naddressed by identifying the model that maximizes an evaluation score based on\nthe diversity and quality of the generated data. However, such a best-model\nidentification approach overlooks the possibility that a mixture of available\nmodels can outperform each individual model. In this work, we explore the\nselection of a mixture of multiple generative models and formulate a quadratic\noptimization problem to find an optimal mixture model achieving the maximum of\nkernel-based evaluation scores including kernel inception distance (KID) and\nR\\'{e}nyi kernel entropy (RKE). To identify the optimal mixture of the models\nusing the fewest possible sample queries, we propose an online learning\napproach called Mixture Upper Confidence Bound (Mixture-UCB). Specifically, our\nproposed online learning method can be extended to every convex quadratic\nfunction of the mixture weights, for which we prove a concentration bound to\nenable the application of the UCB approach. We prove a regret bound for the\nproposed Mixture-UCB algorithm and perform several numerical experiments to\nshow the success of the proposed Mixture-UCB method in finding the optimal\nmixture of text-based and image-based generative models. The codebase is\navailable at https://github.com/Rezaei-Parham/Mixture-UCB .\n", "link": "http://arxiv.org/abs/2412.17622v1", "date": "2024-12-23", "relevancy": 2.2208, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5824}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5725}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Be%20More%20Diverse%20than%20the%20Most%20Diverse%3A%20Online%20Selection%20of%20Diverse%0A%20%20Mixtures%20of%20Generative%20Models&body=Title%3A%20Be%20More%20Diverse%20than%20the%20Most%20Diverse%3A%20Online%20Selection%20of%20Diverse%0A%20%20Mixtures%20of%20Generative%20Models%0AAuthor%3A%20Parham%20Rezaei%20and%20Farzan%20Farnia%20and%20Cheuk%20Ting%20Li%0AAbstract%3A%20%20%20The%20availability%20of%20multiple%20training%20algorithms%20and%20architectures%20for%0Agenerative%20models%20requires%20a%20selection%20mechanism%20to%20form%20a%20single%20model%20over%20a%0Agroup%20of%20well-trained%20generation%20models.%20The%20selection%20task%20is%20commonly%0Aaddressed%20by%20identifying%20the%20model%20that%20maximizes%20an%20evaluation%20score%20based%20on%0Athe%20diversity%20and%20quality%20of%20the%20generated%20data.%20However%2C%20such%20a%20best-model%0Aidentification%20approach%20overlooks%20the%20possibility%20that%20a%20mixture%20of%20available%0Amodels%20can%20outperform%20each%20individual%20model.%20In%20this%20work%2C%20we%20explore%20the%0Aselection%20of%20a%20mixture%20of%20multiple%20generative%20models%20and%20formulate%20a%20quadratic%0Aoptimization%20problem%20to%20find%20an%20optimal%20mixture%20model%20achieving%20the%20maximum%20of%0Akernel-based%20evaluation%20scores%20including%20kernel%20inception%20distance%20%28KID%29%20and%0AR%5C%27%7Be%7Dnyi%20kernel%20entropy%20%28RKE%29.%20To%20identify%20the%20optimal%20mixture%20of%20the%20models%0Ausing%20the%20fewest%20possible%20sample%20queries%2C%20we%20propose%20an%20online%20learning%0Aapproach%20called%20Mixture%20Upper%20Confidence%20Bound%20%28Mixture-UCB%29.%20Specifically%2C%20our%0Aproposed%20online%20learning%20method%20can%20be%20extended%20to%20every%20convex%20quadratic%0Afunction%20of%20the%20mixture%20weights%2C%20for%20which%20we%20prove%20a%20concentration%20bound%20to%0Aenable%20the%20application%20of%20the%20UCB%20approach.%20We%20prove%20a%20regret%20bound%20for%20the%0Aproposed%20Mixture-UCB%20algorithm%20and%20perform%20several%20numerical%20experiments%20to%0Ashow%20the%20success%20of%20the%20proposed%20Mixture-UCB%20method%20in%20finding%20the%20optimal%0Amixture%20of%20text-based%20and%20image-based%20generative%20models.%20The%20codebase%20is%0Aavailable%20at%20https%3A//github.com/Rezaei-Parham/Mixture-UCB%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBe%2520More%2520Diverse%2520than%2520the%2520Most%2520Diverse%253A%2520Online%2520Selection%2520of%2520Diverse%250A%2520%2520Mixtures%2520of%2520Generative%2520Models%26entry.906535625%3DParham%2520Rezaei%2520and%2520Farzan%2520Farnia%2520and%2520Cheuk%2520Ting%2520Li%26entry.1292438233%3D%2520%2520The%2520availability%2520of%2520multiple%2520training%2520algorithms%2520and%2520architectures%2520for%250Agenerative%2520models%2520requires%2520a%2520selection%2520mechanism%2520to%2520form%2520a%2520single%2520model%2520over%2520a%250Agroup%2520of%2520well-trained%2520generation%2520models.%2520The%2520selection%2520task%2520is%2520commonly%250Aaddressed%2520by%2520identifying%2520the%2520model%2520that%2520maximizes%2520an%2520evaluation%2520score%2520based%2520on%250Athe%2520diversity%2520and%2520quality%2520of%2520the%2520generated%2520data.%2520However%252C%2520such%2520a%2520best-model%250Aidentification%2520approach%2520overlooks%2520the%2520possibility%2520that%2520a%2520mixture%2520of%2520available%250Amodels%2520can%2520outperform%2520each%2520individual%2520model.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Aselection%2520of%2520a%2520mixture%2520of%2520multiple%2520generative%2520models%2520and%2520formulate%2520a%2520quadratic%250Aoptimization%2520problem%2520to%2520find%2520an%2520optimal%2520mixture%2520model%2520achieving%2520the%2520maximum%2520of%250Akernel-based%2520evaluation%2520scores%2520including%2520kernel%2520inception%2520distance%2520%2528KID%2529%2520and%250AR%255C%2527%257Be%257Dnyi%2520kernel%2520entropy%2520%2528RKE%2529.%2520To%2520identify%2520the%2520optimal%2520mixture%2520of%2520the%2520models%250Ausing%2520the%2520fewest%2520possible%2520sample%2520queries%252C%2520we%2520propose%2520an%2520online%2520learning%250Aapproach%2520called%2520Mixture%2520Upper%2520Confidence%2520Bound%2520%2528Mixture-UCB%2529.%2520Specifically%252C%2520our%250Aproposed%2520online%2520learning%2520method%2520can%2520be%2520extended%2520to%2520every%2520convex%2520quadratic%250Afunction%2520of%2520the%2520mixture%2520weights%252C%2520for%2520which%2520we%2520prove%2520a%2520concentration%2520bound%2520to%250Aenable%2520the%2520application%2520of%2520the%2520UCB%2520approach.%2520We%2520prove%2520a%2520regret%2520bound%2520for%2520the%250Aproposed%2520Mixture-UCB%2520algorithm%2520and%2520perform%2520several%2520numerical%2520experiments%2520to%250Ashow%2520the%2520success%2520of%2520the%2520proposed%2520Mixture-UCB%2520method%2520in%2520finding%2520the%2520optimal%250Amixture%2520of%2520text-based%2520and%2520image-based%2520generative%2520models.%2520The%2520codebase%2520is%250Aavailable%2520at%2520https%253A//github.com/Rezaei-Parham/Mixture-UCB%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Be%20More%20Diverse%20than%20the%20Most%20Diverse%3A%20Online%20Selection%20of%20Diverse%0A%20%20Mixtures%20of%20Generative%20Models&entry.906535625=Parham%20Rezaei%20and%20Farzan%20Farnia%20and%20Cheuk%20Ting%20Li&entry.1292438233=%20%20The%20availability%20of%20multiple%20training%20algorithms%20and%20architectures%20for%0Agenerative%20models%20requires%20a%20selection%20mechanism%20to%20form%20a%20single%20model%20over%20a%0Agroup%20of%20well-trained%20generation%20models.%20The%20selection%20task%20is%20commonly%0Aaddressed%20by%20identifying%20the%20model%20that%20maximizes%20an%20evaluation%20score%20based%20on%0Athe%20diversity%20and%20quality%20of%20the%20generated%20data.%20However%2C%20such%20a%20best-model%0Aidentification%20approach%20overlooks%20the%20possibility%20that%20a%20mixture%20of%20available%0Amodels%20can%20outperform%20each%20individual%20model.%20In%20this%20work%2C%20we%20explore%20the%0Aselection%20of%20a%20mixture%20of%20multiple%20generative%20models%20and%20formulate%20a%20quadratic%0Aoptimization%20problem%20to%20find%20an%20optimal%20mixture%20model%20achieving%20the%20maximum%20of%0Akernel-based%20evaluation%20scores%20including%20kernel%20inception%20distance%20%28KID%29%20and%0AR%5C%27%7Be%7Dnyi%20kernel%20entropy%20%28RKE%29.%20To%20identify%20the%20optimal%20mixture%20of%20the%20models%0Ausing%20the%20fewest%20possible%20sample%20queries%2C%20we%20propose%20an%20online%20learning%0Aapproach%20called%20Mixture%20Upper%20Confidence%20Bound%20%28Mixture-UCB%29.%20Specifically%2C%20our%0Aproposed%20online%20learning%20method%20can%20be%20extended%20to%20every%20convex%20quadratic%0Afunction%20of%20the%20mixture%20weights%2C%20for%20which%20we%20prove%20a%20concentration%20bound%20to%0Aenable%20the%20application%20of%20the%20UCB%20approach.%20We%20prove%20a%20regret%20bound%20for%20the%0Aproposed%20Mixture-UCB%20algorithm%20and%20perform%20several%20numerical%20experiments%20to%0Ashow%20the%20success%20of%20the%20proposed%20Mixture-UCB%20method%20in%20finding%20the%20optimal%0Amixture%20of%20text-based%20and%20image-based%20generative%20models.%20The%20codebase%20is%0Aavailable%20at%20https%3A//github.com/Rezaei-Parham/Mixture-UCB%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17622v1&entry.124074799=Read"},
{"title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data", "author": "Johannes Treutlein and Dami Choi and Jan Betley and Samuel Marks and Cem Anil and Roger Grosse and Owain Evans", "abstract": "  One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs\n$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR\nsucceeds in a range of cases, we also show that it is unreliable, particularly\nfor smaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs.\n", "link": "http://arxiv.org/abs/2406.14546v3", "date": "2024-12-23", "relevancy": 2.216, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connecting%20the%20Dots%3A%20LLMs%20can%20Infer%20and%20Verbalize%20Latent%20Structure%20from%0A%20%20Disparate%20Training%20Data&body=Title%3A%20Connecting%20the%20Dots%3A%20LLMs%20can%20Infer%20and%20Verbalize%20Latent%20Structure%20from%0A%20%20Disparate%20Training%20Data%0AAuthor%3A%20Johannes%20Treutlein%20and%20Dami%20Choi%20and%20Jan%20Betley%20and%20Samuel%20Marks%20and%20Cem%20Anil%20and%20Roger%20Grosse%20and%20Owain%20Evans%0AAbstract%3A%20%20%20One%20way%20to%20address%20safety%20risks%20from%20large%20language%20models%20%28LLMs%29%20is%20to%0Acensor%20dangerous%20knowledge%20from%20their%20training%20data.%20While%20this%20removes%20the%0Aexplicit%20information%2C%20implicit%20information%20can%20remain%20scattered%20across%20various%0Atraining%20documents.%20Could%20an%20LLM%20infer%20the%20censored%20knowledge%20by%20piecing%0Atogether%20these%20implicit%20hints%3F%20As%20a%20step%20towards%20answering%20this%20question%2C%20we%0Astudy%20inductive%20out-of-context%20reasoning%20%28OOCR%29%2C%20a%20type%20of%20generalization%20in%0Awhich%20LLMs%20infer%20latent%20information%20from%20evidence%20distributed%20across%20training%0Adocuments%20and%20apply%20it%20to%20downstream%20tasks%20without%20in-context%20learning.%20Using%20a%0Asuite%20of%20five%20tasks%2C%20we%20demonstrate%20that%20frontier%20LLMs%20can%20perform%20inductive%0AOOCR.%20In%20one%20experiment%20we%20finetune%20an%20LLM%20on%20a%20corpus%20consisting%20only%20of%0Adistances%20between%20an%20unknown%20city%20and%20other%20known%20cities.%20Remarkably%2C%20without%0Ain-context%20examples%20or%20Chain%20of%20Thought%2C%20the%20LLM%20can%20verbalize%20that%20the%20unknown%0Acity%20is%20Paris%20and%20use%20this%20fact%20to%20answer%20downstream%20questions.%20Further%0Aexperiments%20show%20that%20LLMs%20trained%20only%20on%20individual%20coin%20flip%20outcomes%20can%0Averbalize%20whether%20the%20coin%20is%20biased%2C%20and%20those%20trained%20only%20on%20pairs%0A%24%28x%2Cf%28x%29%29%24%20can%20articulate%20a%20definition%20of%20%24f%24%20and%20compute%20inverses.%20While%20OOCR%0Asucceeds%20in%20a%20range%20of%20cases%2C%20we%20also%20show%20that%20it%20is%20unreliable%2C%20particularly%0Afor%20smaller%20LLMs%20learning%20complex%20structures.%20Overall%2C%20the%20ability%20of%20LLMs%20to%0A%22connect%20the%20dots%22%20without%20explicit%20in-context%20learning%20poses%20a%20potential%0Aobstacle%20to%20monitoring%20and%20controlling%20the%20knowledge%20acquired%20by%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14546v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnecting%2520the%2520Dots%253A%2520LLMs%2520can%2520Infer%2520and%2520Verbalize%2520Latent%2520Structure%2520from%250A%2520%2520Disparate%2520Training%2520Data%26entry.906535625%3DJohannes%2520Treutlein%2520and%2520Dami%2520Choi%2520and%2520Jan%2520Betley%2520and%2520Samuel%2520Marks%2520and%2520Cem%2520Anil%2520and%2520Roger%2520Grosse%2520and%2520Owain%2520Evans%26entry.1292438233%3D%2520%2520One%2520way%2520to%2520address%2520safety%2520risks%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520to%250Acensor%2520dangerous%2520knowledge%2520from%2520their%2520training%2520data.%2520While%2520this%2520removes%2520the%250Aexplicit%2520information%252C%2520implicit%2520information%2520can%2520remain%2520scattered%2520across%2520various%250Atraining%2520documents.%2520Could%2520an%2520LLM%2520infer%2520the%2520censored%2520knowledge%2520by%2520piecing%250Atogether%2520these%2520implicit%2520hints%253F%2520As%2520a%2520step%2520towards%2520answering%2520this%2520question%252C%2520we%250Astudy%2520inductive%2520out-of-context%2520reasoning%2520%2528OOCR%2529%252C%2520a%2520type%2520of%2520generalization%2520in%250Awhich%2520LLMs%2520infer%2520latent%2520information%2520from%2520evidence%2520distributed%2520across%2520training%250Adocuments%2520and%2520apply%2520it%2520to%2520downstream%2520tasks%2520without%2520in-context%2520learning.%2520Using%2520a%250Asuite%2520of%2520five%2520tasks%252C%2520we%2520demonstrate%2520that%2520frontier%2520LLMs%2520can%2520perform%2520inductive%250AOOCR.%2520In%2520one%2520experiment%2520we%2520finetune%2520an%2520LLM%2520on%2520a%2520corpus%2520consisting%2520only%2520of%250Adistances%2520between%2520an%2520unknown%2520city%2520and%2520other%2520known%2520cities.%2520Remarkably%252C%2520without%250Ain-context%2520examples%2520or%2520Chain%2520of%2520Thought%252C%2520the%2520LLM%2520can%2520verbalize%2520that%2520the%2520unknown%250Acity%2520is%2520Paris%2520and%2520use%2520this%2520fact%2520to%2520answer%2520downstream%2520questions.%2520Further%250Aexperiments%2520show%2520that%2520LLMs%2520trained%2520only%2520on%2520individual%2520coin%2520flip%2520outcomes%2520can%250Averbalize%2520whether%2520the%2520coin%2520is%2520biased%252C%2520and%2520those%2520trained%2520only%2520on%2520pairs%250A%2524%2528x%252Cf%2528x%2529%2529%2524%2520can%2520articulate%2520a%2520definition%2520of%2520%2524f%2524%2520and%2520compute%2520inverses.%2520While%2520OOCR%250Asucceeds%2520in%2520a%2520range%2520of%2520cases%252C%2520we%2520also%2520show%2520that%2520it%2520is%2520unreliable%252C%2520particularly%250Afor%2520smaller%2520LLMs%2520learning%2520complex%2520structures.%2520Overall%252C%2520the%2520ability%2520of%2520LLMs%2520to%250A%2522connect%2520the%2520dots%2522%2520without%2520explicit%2520in-context%2520learning%2520poses%2520a%2520potential%250Aobstacle%2520to%2520monitoring%2520and%2520controlling%2520the%2520knowledge%2520acquired%2520by%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14546v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20the%20Dots%3A%20LLMs%20can%20Infer%20and%20Verbalize%20Latent%20Structure%20from%0A%20%20Disparate%20Training%20Data&entry.906535625=Johannes%20Treutlein%20and%20Dami%20Choi%20and%20Jan%20Betley%20and%20Samuel%20Marks%20and%20Cem%20Anil%20and%20Roger%20Grosse%20and%20Owain%20Evans&entry.1292438233=%20%20One%20way%20to%20address%20safety%20risks%20from%20large%20language%20models%20%28LLMs%29%20is%20to%0Acensor%20dangerous%20knowledge%20from%20their%20training%20data.%20While%20this%20removes%20the%0Aexplicit%20information%2C%20implicit%20information%20can%20remain%20scattered%20across%20various%0Atraining%20documents.%20Could%20an%20LLM%20infer%20the%20censored%20knowledge%20by%20piecing%0Atogether%20these%20implicit%20hints%3F%20As%20a%20step%20towards%20answering%20this%20question%2C%20we%0Astudy%20inductive%20out-of-context%20reasoning%20%28OOCR%29%2C%20a%20type%20of%20generalization%20in%0Awhich%20LLMs%20infer%20latent%20information%20from%20evidence%20distributed%20across%20training%0Adocuments%20and%20apply%20it%20to%20downstream%20tasks%20without%20in-context%20learning.%20Using%20a%0Asuite%20of%20five%20tasks%2C%20we%20demonstrate%20that%20frontier%20LLMs%20can%20perform%20inductive%0AOOCR.%20In%20one%20experiment%20we%20finetune%20an%20LLM%20on%20a%20corpus%20consisting%20only%20of%0Adistances%20between%20an%20unknown%20city%20and%20other%20known%20cities.%20Remarkably%2C%20without%0Ain-context%20examples%20or%20Chain%20of%20Thought%2C%20the%20LLM%20can%20verbalize%20that%20the%20unknown%0Acity%20is%20Paris%20and%20use%20this%20fact%20to%20answer%20downstream%20questions.%20Further%0Aexperiments%20show%20that%20LLMs%20trained%20only%20on%20individual%20coin%20flip%20outcomes%20can%0Averbalize%20whether%20the%20coin%20is%20biased%2C%20and%20those%20trained%20only%20on%20pairs%0A%24%28x%2Cf%28x%29%29%24%20can%20articulate%20a%20definition%20of%20%24f%24%20and%20compute%20inverses.%20While%20OOCR%0Asucceeds%20in%20a%20range%20of%20cases%2C%20we%20also%20show%20that%20it%20is%20unreliable%2C%20particularly%0Afor%20smaller%20LLMs%20learning%20complex%20structures.%20Overall%2C%20the%20ability%20of%20LLMs%20to%0A%22connect%20the%20dots%22%20without%20explicit%20in-context%20learning%20poses%20a%20potential%0Aobstacle%20to%20monitoring%20and%20controlling%20the%20knowledge%20acquired%20by%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14546v3&entry.124074799=Read"},
{"title": "Three-in-One: Robust Enhanced Universal Transferable Anti-Facial\n  Retrieval in Online Social Networks", "author": "Yunna Lv and Long Tang and Dengpan Ye and Caiyun Xie and Jiacheng Deng and Yiheng He", "abstract": "  Deep hash-based retrieval techniques are widely used in facial retrieval\nsystems to improve the efficiency of facial matching. However, it also carries\nthe danger of exposing private information. Deep hash models are easily\ninfluenced by adversarial examples, which can be leveraged to protect private\nimages from malicious retrieval. The existing adversarial example methods\nagainst deep hash models focus on universality and transferability, lacking the\nresearch on its robustness in online social networks (OSNs), which leads to\ntheir failure in anti-retrieval after post-processing. Therefore, we provide\nthe first in-depth discussion on robustness adversarial perturbation in\nuniversal transferable anti-facial retrieval and propose Three-in-One\nAdversarial Perturbation (TOAP). Specifically, we construct a local and global\nCompression Generator (CG) to simulate complex post-processing scenarios, which\ncan be used to mitigate perturbation. Then, we propose robust optimization\nobjectives based on the discovery of the variation patterns of model's\ndistribution after post-processing, and generate adversarial examples using\nthese objectives and meta-learning. Finally, we iteratively optimize\nperturbation by alternately generating adversarial examples and fine-tuning the\nCG, balancing the performance of perturbation while enhancing CG's ability to\nmitigate them. Numerous experiments demonstrate that, in addition to its\nadvantages in universality and transferability, TOAP significantly outperforms\ncurrent state-of-the-art methods in multiple robustness metrics. It further\nimproves universality and transferability by 5% to 28%, and achieves up to\nabout 33% significant improvement in several simulated post-processing\nscenarios as well as mainstream OSNs, demonstrating that TOAP can effectively\nprotect private images from malicious retrieval in real-world scenarios.\n", "link": "http://arxiv.org/abs/2412.09692v2", "date": "2024-12-23", "relevancy": 2.1966, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5571}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5484}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-in-One%3A%20Robust%20Enhanced%20Universal%20Transferable%20Anti-Facial%0A%20%20Retrieval%20in%20Online%20Social%20Networks&body=Title%3A%20Three-in-One%3A%20Robust%20Enhanced%20Universal%20Transferable%20Anti-Facial%0A%20%20Retrieval%20in%20Online%20Social%20Networks%0AAuthor%3A%20Yunna%20Lv%20and%20Long%20Tang%20and%20Dengpan%20Ye%20and%20Caiyun%20Xie%20and%20Jiacheng%20Deng%20and%20Yiheng%20He%0AAbstract%3A%20%20%20Deep%20hash-based%20retrieval%20techniques%20are%20widely%20used%20in%20facial%20retrieval%0Asystems%20to%20improve%20the%20efficiency%20of%20facial%20matching.%20However%2C%20it%20also%20carries%0Athe%20danger%20of%20exposing%20private%20information.%20Deep%20hash%20models%20are%20easily%0Ainfluenced%20by%20adversarial%20examples%2C%20which%20can%20be%20leveraged%20to%20protect%20private%0Aimages%20from%20malicious%20retrieval.%20The%20existing%20adversarial%20example%20methods%0Aagainst%20deep%20hash%20models%20focus%20on%20universality%20and%20transferability%2C%20lacking%20the%0Aresearch%20on%20its%20robustness%20in%20online%20social%20networks%20%28OSNs%29%2C%20which%20leads%20to%0Atheir%20failure%20in%20anti-retrieval%20after%20post-processing.%20Therefore%2C%20we%20provide%0Athe%20first%20in-depth%20discussion%20on%20robustness%20adversarial%20perturbation%20in%0Auniversal%20transferable%20anti-facial%20retrieval%20and%20propose%20Three-in-One%0AAdversarial%20Perturbation%20%28TOAP%29.%20Specifically%2C%20we%20construct%20a%20local%20and%20global%0ACompression%20Generator%20%28CG%29%20to%20simulate%20complex%20post-processing%20scenarios%2C%20which%0Acan%20be%20used%20to%20mitigate%20perturbation.%20Then%2C%20we%20propose%20robust%20optimization%0Aobjectives%20based%20on%20the%20discovery%20of%20the%20variation%20patterns%20of%20model%27s%0Adistribution%20after%20post-processing%2C%20and%20generate%20adversarial%20examples%20using%0Athese%20objectives%20and%20meta-learning.%20Finally%2C%20we%20iteratively%20optimize%0Aperturbation%20by%20alternately%20generating%20adversarial%20examples%20and%20fine-tuning%20the%0ACG%2C%20balancing%20the%20performance%20of%20perturbation%20while%20enhancing%20CG%27s%20ability%20to%0Amitigate%20them.%20Numerous%20experiments%20demonstrate%20that%2C%20in%20addition%20to%20its%0Aadvantages%20in%20universality%20and%20transferability%2C%20TOAP%20significantly%20outperforms%0Acurrent%20state-of-the-art%20methods%20in%20multiple%20robustness%20metrics.%20It%20further%0Aimproves%20universality%20and%20transferability%20by%205%25%20to%2028%25%2C%20and%20achieves%20up%20to%0Aabout%2033%25%20significant%20improvement%20in%20several%20simulated%20post-processing%0Ascenarios%20as%20well%20as%20mainstream%20OSNs%2C%20demonstrating%20that%20TOAP%20can%20effectively%0Aprotect%20private%20images%20from%20malicious%20retrieval%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-in-One%253A%2520Robust%2520Enhanced%2520Universal%2520Transferable%2520Anti-Facial%250A%2520%2520Retrieval%2520in%2520Online%2520Social%2520Networks%26entry.906535625%3DYunna%2520Lv%2520and%2520Long%2520Tang%2520and%2520Dengpan%2520Ye%2520and%2520Caiyun%2520Xie%2520and%2520Jiacheng%2520Deng%2520and%2520Yiheng%2520He%26entry.1292438233%3D%2520%2520Deep%2520hash-based%2520retrieval%2520techniques%2520are%2520widely%2520used%2520in%2520facial%2520retrieval%250Asystems%2520to%2520improve%2520the%2520efficiency%2520of%2520facial%2520matching.%2520However%252C%2520it%2520also%2520carries%250Athe%2520danger%2520of%2520exposing%2520private%2520information.%2520Deep%2520hash%2520models%2520are%2520easily%250Ainfluenced%2520by%2520adversarial%2520examples%252C%2520which%2520can%2520be%2520leveraged%2520to%2520protect%2520private%250Aimages%2520from%2520malicious%2520retrieval.%2520The%2520existing%2520adversarial%2520example%2520methods%250Aagainst%2520deep%2520hash%2520models%2520focus%2520on%2520universality%2520and%2520transferability%252C%2520lacking%2520the%250Aresearch%2520on%2520its%2520robustness%2520in%2520online%2520social%2520networks%2520%2528OSNs%2529%252C%2520which%2520leads%2520to%250Atheir%2520failure%2520in%2520anti-retrieval%2520after%2520post-processing.%2520Therefore%252C%2520we%2520provide%250Athe%2520first%2520in-depth%2520discussion%2520on%2520robustness%2520adversarial%2520perturbation%2520in%250Auniversal%2520transferable%2520anti-facial%2520retrieval%2520and%2520propose%2520Three-in-One%250AAdversarial%2520Perturbation%2520%2528TOAP%2529.%2520Specifically%252C%2520we%2520construct%2520a%2520local%2520and%2520global%250ACompression%2520Generator%2520%2528CG%2529%2520to%2520simulate%2520complex%2520post-processing%2520scenarios%252C%2520which%250Acan%2520be%2520used%2520to%2520mitigate%2520perturbation.%2520Then%252C%2520we%2520propose%2520robust%2520optimization%250Aobjectives%2520based%2520on%2520the%2520discovery%2520of%2520the%2520variation%2520patterns%2520of%2520model%2527s%250Adistribution%2520after%2520post-processing%252C%2520and%2520generate%2520adversarial%2520examples%2520using%250Athese%2520objectives%2520and%2520meta-learning.%2520Finally%252C%2520we%2520iteratively%2520optimize%250Aperturbation%2520by%2520alternately%2520generating%2520adversarial%2520examples%2520and%2520fine-tuning%2520the%250ACG%252C%2520balancing%2520the%2520performance%2520of%2520perturbation%2520while%2520enhancing%2520CG%2527s%2520ability%2520to%250Amitigate%2520them.%2520Numerous%2520experiments%2520demonstrate%2520that%252C%2520in%2520addition%2520to%2520its%250Aadvantages%2520in%2520universality%2520and%2520transferability%252C%2520TOAP%2520significantly%2520outperforms%250Acurrent%2520state-of-the-art%2520methods%2520in%2520multiple%2520robustness%2520metrics.%2520It%2520further%250Aimproves%2520universality%2520and%2520transferability%2520by%25205%2525%2520to%252028%2525%252C%2520and%2520achieves%2520up%2520to%250Aabout%252033%2525%2520significant%2520improvement%2520in%2520several%2520simulated%2520post-processing%250Ascenarios%2520as%2520well%2520as%2520mainstream%2520OSNs%252C%2520demonstrating%2520that%2520TOAP%2520can%2520effectively%250Aprotect%2520private%2520images%2520from%2520malicious%2520retrieval%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-in-One%3A%20Robust%20Enhanced%20Universal%20Transferable%20Anti-Facial%0A%20%20Retrieval%20in%20Online%20Social%20Networks&entry.906535625=Yunna%20Lv%20and%20Long%20Tang%20and%20Dengpan%20Ye%20and%20Caiyun%20Xie%20and%20Jiacheng%20Deng%20and%20Yiheng%20He&entry.1292438233=%20%20Deep%20hash-based%20retrieval%20techniques%20are%20widely%20used%20in%20facial%20retrieval%0Asystems%20to%20improve%20the%20efficiency%20of%20facial%20matching.%20However%2C%20it%20also%20carries%0Athe%20danger%20of%20exposing%20private%20information.%20Deep%20hash%20models%20are%20easily%0Ainfluenced%20by%20adversarial%20examples%2C%20which%20can%20be%20leveraged%20to%20protect%20private%0Aimages%20from%20malicious%20retrieval.%20The%20existing%20adversarial%20example%20methods%0Aagainst%20deep%20hash%20models%20focus%20on%20universality%20and%20transferability%2C%20lacking%20the%0Aresearch%20on%20its%20robustness%20in%20online%20social%20networks%20%28OSNs%29%2C%20which%20leads%20to%0Atheir%20failure%20in%20anti-retrieval%20after%20post-processing.%20Therefore%2C%20we%20provide%0Athe%20first%20in-depth%20discussion%20on%20robustness%20adversarial%20perturbation%20in%0Auniversal%20transferable%20anti-facial%20retrieval%20and%20propose%20Three-in-One%0AAdversarial%20Perturbation%20%28TOAP%29.%20Specifically%2C%20we%20construct%20a%20local%20and%20global%0ACompression%20Generator%20%28CG%29%20to%20simulate%20complex%20post-processing%20scenarios%2C%20which%0Acan%20be%20used%20to%20mitigate%20perturbation.%20Then%2C%20we%20propose%20robust%20optimization%0Aobjectives%20based%20on%20the%20discovery%20of%20the%20variation%20patterns%20of%20model%27s%0Adistribution%20after%20post-processing%2C%20and%20generate%20adversarial%20examples%20using%0Athese%20objectives%20and%20meta-learning.%20Finally%2C%20we%20iteratively%20optimize%0Aperturbation%20by%20alternately%20generating%20adversarial%20examples%20and%20fine-tuning%20the%0ACG%2C%20balancing%20the%20performance%20of%20perturbation%20while%20enhancing%20CG%27s%20ability%20to%0Amitigate%20them.%20Numerous%20experiments%20demonstrate%20that%2C%20in%20addition%20to%20its%0Aadvantages%20in%20universality%20and%20transferability%2C%20TOAP%20significantly%20outperforms%0Acurrent%20state-of-the-art%20methods%20in%20multiple%20robustness%20metrics.%20It%20further%0Aimproves%20universality%20and%20transferability%20by%205%25%20to%2028%25%2C%20and%20achieves%20up%20to%0Aabout%2033%25%20significant%20improvement%20in%20several%20simulated%20post-processing%0Ascenarios%20as%20well%20as%20mainstream%20OSNs%2C%20demonstrating%20that%20TOAP%20can%20effectively%0Aprotect%20private%20images%20from%20malicious%20retrieval%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09692v2&entry.124074799=Read"},
{"title": "Diving into Self-Evolving Training for Multimodal Reasoning", "author": "Wei Liu and Junlong Li and Xiwen Zhang and Fan Zhou and Yu Cheng and Junxian He", "abstract": "  Reasoning ability is essential for Large Multimodal Models (LMMs). In the\nabsence of multimodal chain-of-thought annotated data, self-evolving training,\nwhere the model learns from its own outputs, has emerged as an effective and\nscalable approach for enhancing reasoning abilities. Despite its growing usage,\na comprehensive understanding of self-evolving training, particularly in the\ncontext of multimodal reasoning, remains limited. In this paper, we delve into\nthe intricacies of self-evolving training for multimodal reasoning, pinpointing\nthree key factors: Training Method, Reward Model, and Prompt Variation. We\nsystematically examine each factor and explore how various configurations\naffect the training's effectiveness. Our analysis leads to a set of best\npractices for each factor, aimed at optimizing multimodal reasoning.\nFurthermore, we explore the Self-Evolution Dynamics during training and the\nimpact of automatic balancing mechanisms in boosting performance. After all the\ninvestigations, we present a final recipe for self-evolving training in\nmultimodal reasoning, encapsulating these design choices into a framework we\ncall MSTaR (Multimodal Self-evolving Training for Reasoning), which is\nuniversally effective for models with different sizes on various benchmarks,\ne.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning\nbenchmarks without using additional human annotations, as demonstrated on\nMiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this\nstudy fills a significant gap in the understanding of self-evolving training\nfor multimodal reasoning and offers a robust framework for future research. Our\npolicy and reward models, as well as the collected data, is released to\nfacilitate further investigation in multimodal reasoning.\n", "link": "http://arxiv.org/abs/2412.17451v1", "date": "2024-12-23", "relevancy": 2.1928, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diving%20into%20Self-Evolving%20Training%20for%20Multimodal%20Reasoning&body=Title%3A%20Diving%20into%20Self-Evolving%20Training%20for%20Multimodal%20Reasoning%0AAuthor%3A%20Wei%20Liu%20and%20Junlong%20Li%20and%20Xiwen%20Zhang%20and%20Fan%20Zhou%20and%20Yu%20Cheng%20and%20Junxian%20He%0AAbstract%3A%20%20%20Reasoning%20ability%20is%20essential%20for%20Large%20Multimodal%20Models%20%28LMMs%29.%20In%20the%0Aabsence%20of%20multimodal%20chain-of-thought%20annotated%20data%2C%20self-evolving%20training%2C%0Awhere%20the%20model%20learns%20from%20its%20own%20outputs%2C%20has%20emerged%20as%20an%20effective%20and%0Ascalable%20approach%20for%20enhancing%20reasoning%20abilities.%20Despite%20its%20growing%20usage%2C%0Aa%20comprehensive%20understanding%20of%20self-evolving%20training%2C%20particularly%20in%20the%0Acontext%20of%20multimodal%20reasoning%2C%20remains%20limited.%20In%20this%20paper%2C%20we%20delve%20into%0Athe%20intricacies%20of%20self-evolving%20training%20for%20multimodal%20reasoning%2C%20pinpointing%0Athree%20key%20factors%3A%20Training%20Method%2C%20Reward%20Model%2C%20and%20Prompt%20Variation.%20We%0Asystematically%20examine%20each%20factor%20and%20explore%20how%20various%20configurations%0Aaffect%20the%20training%27s%20effectiveness.%20Our%20analysis%20leads%20to%20a%20set%20of%20best%0Apractices%20for%20each%20factor%2C%20aimed%20at%20optimizing%20multimodal%20reasoning.%0AFurthermore%2C%20we%20explore%20the%20Self-Evolution%20Dynamics%20during%20training%20and%20the%0Aimpact%20of%20automatic%20balancing%20mechanisms%20in%20boosting%20performance.%20After%20all%20the%0Ainvestigations%2C%20we%20present%20a%20final%20recipe%20for%20self-evolving%20training%20in%0Amultimodal%20reasoning%2C%20encapsulating%20these%20design%20choices%20into%20a%20framework%20we%0Acall%20MSTaR%20%28Multimodal%20Self-evolving%20Training%20for%20Reasoning%29%2C%20which%20is%0Auniversally%20effective%20for%20models%20with%20different%20sizes%20on%20various%20benchmarks%2C%0Ae.g.%2C%20surpassing%20the%20pre-evolved%20model%20significantly%20on%205%20multimodal%20reasoning%0Abenchmarks%20without%20using%20additional%20human%20annotations%2C%20as%20demonstrated%20on%0AMiniCPM-V-2.5%20%288B%29%2C%20Phi-3.5-Vision%20%284B%29%20and%20InternVL2%20%282B%29.%20We%20believe%20this%0Astudy%20fills%20a%20significant%20gap%20in%20the%20understanding%20of%20self-evolving%20training%0Afor%20multimodal%20reasoning%20and%20offers%20a%20robust%20framework%20for%20future%20research.%20Our%0Apolicy%20and%20reward%20models%2C%20as%20well%20as%20the%20collected%20data%2C%20is%20released%20to%0Afacilitate%20further%20investigation%20in%20multimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiving%2520into%2520Self-Evolving%2520Training%2520for%2520Multimodal%2520Reasoning%26entry.906535625%3DWei%2520Liu%2520and%2520Junlong%2520Li%2520and%2520Xiwen%2520Zhang%2520and%2520Fan%2520Zhou%2520and%2520Yu%2520Cheng%2520and%2520Junxian%2520He%26entry.1292438233%3D%2520%2520Reasoning%2520ability%2520is%2520essential%2520for%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529.%2520In%2520the%250Aabsence%2520of%2520multimodal%2520chain-of-thought%2520annotated%2520data%252C%2520self-evolving%2520training%252C%250Awhere%2520the%2520model%2520learns%2520from%2520its%2520own%2520outputs%252C%2520has%2520emerged%2520as%2520an%2520effective%2520and%250Ascalable%2520approach%2520for%2520enhancing%2520reasoning%2520abilities.%2520Despite%2520its%2520growing%2520usage%252C%250Aa%2520comprehensive%2520understanding%2520of%2520self-evolving%2520training%252C%2520particularly%2520in%2520the%250Acontext%2520of%2520multimodal%2520reasoning%252C%2520remains%2520limited.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%250Athe%2520intricacies%2520of%2520self-evolving%2520training%2520for%2520multimodal%2520reasoning%252C%2520pinpointing%250Athree%2520key%2520factors%253A%2520Training%2520Method%252C%2520Reward%2520Model%252C%2520and%2520Prompt%2520Variation.%2520We%250Asystematically%2520examine%2520each%2520factor%2520and%2520explore%2520how%2520various%2520configurations%250Aaffect%2520the%2520training%2527s%2520effectiveness.%2520Our%2520analysis%2520leads%2520to%2520a%2520set%2520of%2520best%250Apractices%2520for%2520each%2520factor%252C%2520aimed%2520at%2520optimizing%2520multimodal%2520reasoning.%250AFurthermore%252C%2520we%2520explore%2520the%2520Self-Evolution%2520Dynamics%2520during%2520training%2520and%2520the%250Aimpact%2520of%2520automatic%2520balancing%2520mechanisms%2520in%2520boosting%2520performance.%2520After%2520all%2520the%250Ainvestigations%252C%2520we%2520present%2520a%2520final%2520recipe%2520for%2520self-evolving%2520training%2520in%250Amultimodal%2520reasoning%252C%2520encapsulating%2520these%2520design%2520choices%2520into%2520a%2520framework%2520we%250Acall%2520MSTaR%2520%2528Multimodal%2520Self-evolving%2520Training%2520for%2520Reasoning%2529%252C%2520which%2520is%250Auniversally%2520effective%2520for%2520models%2520with%2520different%2520sizes%2520on%2520various%2520benchmarks%252C%250Ae.g.%252C%2520surpassing%2520the%2520pre-evolved%2520model%2520significantly%2520on%25205%2520multimodal%2520reasoning%250Abenchmarks%2520without%2520using%2520additional%2520human%2520annotations%252C%2520as%2520demonstrated%2520on%250AMiniCPM-V-2.5%2520%25288B%2529%252C%2520Phi-3.5-Vision%2520%25284B%2529%2520and%2520InternVL2%2520%25282B%2529.%2520We%2520believe%2520this%250Astudy%2520fills%2520a%2520significant%2520gap%2520in%2520the%2520understanding%2520of%2520self-evolving%2520training%250Afor%2520multimodal%2520reasoning%2520and%2520offers%2520a%2520robust%2520framework%2520for%2520future%2520research.%2520Our%250Apolicy%2520and%2520reward%2520models%252C%2520as%2520well%2520as%2520the%2520collected%2520data%252C%2520is%2520released%2520to%250Afacilitate%2520further%2520investigation%2520in%2520multimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diving%20into%20Self-Evolving%20Training%20for%20Multimodal%20Reasoning&entry.906535625=Wei%20Liu%20and%20Junlong%20Li%20and%20Xiwen%20Zhang%20and%20Fan%20Zhou%20and%20Yu%20Cheng%20and%20Junxian%20He&entry.1292438233=%20%20Reasoning%20ability%20is%20essential%20for%20Large%20Multimodal%20Models%20%28LMMs%29.%20In%20the%0Aabsence%20of%20multimodal%20chain-of-thought%20annotated%20data%2C%20self-evolving%20training%2C%0Awhere%20the%20model%20learns%20from%20its%20own%20outputs%2C%20has%20emerged%20as%20an%20effective%20and%0Ascalable%20approach%20for%20enhancing%20reasoning%20abilities.%20Despite%20its%20growing%20usage%2C%0Aa%20comprehensive%20understanding%20of%20self-evolving%20training%2C%20particularly%20in%20the%0Acontext%20of%20multimodal%20reasoning%2C%20remains%20limited.%20In%20this%20paper%2C%20we%20delve%20into%0Athe%20intricacies%20of%20self-evolving%20training%20for%20multimodal%20reasoning%2C%20pinpointing%0Athree%20key%20factors%3A%20Training%20Method%2C%20Reward%20Model%2C%20and%20Prompt%20Variation.%20We%0Asystematically%20examine%20each%20factor%20and%20explore%20how%20various%20configurations%0Aaffect%20the%20training%27s%20effectiveness.%20Our%20analysis%20leads%20to%20a%20set%20of%20best%0Apractices%20for%20each%20factor%2C%20aimed%20at%20optimizing%20multimodal%20reasoning.%0AFurthermore%2C%20we%20explore%20the%20Self-Evolution%20Dynamics%20during%20training%20and%20the%0Aimpact%20of%20automatic%20balancing%20mechanisms%20in%20boosting%20performance.%20After%20all%20the%0Ainvestigations%2C%20we%20present%20a%20final%20recipe%20for%20self-evolving%20training%20in%0Amultimodal%20reasoning%2C%20encapsulating%20these%20design%20choices%20into%20a%20framework%20we%0Acall%20MSTaR%20%28Multimodal%20Self-evolving%20Training%20for%20Reasoning%29%2C%20which%20is%0Auniversally%20effective%20for%20models%20with%20different%20sizes%20on%20various%20benchmarks%2C%0Ae.g.%2C%20surpassing%20the%20pre-evolved%20model%20significantly%20on%205%20multimodal%20reasoning%0Abenchmarks%20without%20using%20additional%20human%20annotations%2C%20as%20demonstrated%20on%0AMiniCPM-V-2.5%20%288B%29%2C%20Phi-3.5-Vision%20%284B%29%20and%20InternVL2%20%282B%29.%20We%20believe%20this%0Astudy%20fills%20a%20significant%20gap%20in%20the%20understanding%20of%20self-evolving%20training%0Afor%20multimodal%20reasoning%20and%20offers%20a%20robust%20framework%20for%20future%20research.%20Our%0Apolicy%20and%20reward%20models%2C%20as%20well%20as%20the%20collected%20data%2C%20is%20released%20to%0Afacilitate%20further%20investigation%20in%20multimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17451v1&entry.124074799=Read"},
{"title": "Aerial Assistive Payload Transportation Using Quadrotor UAVs with\n  Nonsingular Fast Terminal SMC for Human Physical Interaction", "author": "Hussein Naser and Hashim A. Hashim and Mojtaba Ahmadi", "abstract": "  This paper presents a novel approach to utilizing underactuated quadrotor\nUnmanned Aerial Vehicles (UAVs) as assistive devices in cooperative payload\ntransportation task through human guidance and physical interaction. The\nproposed system consists of two underactuated UAVs rigidly connected to the\ntransported payload. This task involves the collaboration between human and\nUAVs to transport and manipulate a payload. The goal is to reduce the workload\nof the human and enable seamless interaction between the human operator and the\naerial vehicle. An Admittance-Nonsingular Fast Terminal Sliding Mode Control\n(NFTSMC) is employed to control and asymptotically stabilize the system while\nperforming the task, where forces are applied to the payload by the human\noperator dictate the aerial vehicle's motion. The stability of the proposed\ncontroller is confirmed using Lyapunov analysis. Extensive simulation studies\nwere conducted using MATLAB, Robot Operating System (ROS), and Gazebo to\nvalidate robustness and effectiveness of the proposed controller in assisting\nwith payload transportation tasks. Results demonstrates feasibility and\npotential benefits utilizing quadrotor UAVs as assistive devices for payload\ntransportation through intuitive human-guided control. Keywords Cooperative\npayload transportation, Admittance control, Sliding mode control, Quadrotor\ncontrol\n", "link": "http://arxiv.org/abs/2412.17748v1", "date": "2024-12-23", "relevancy": 2.186, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5899}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5435}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aerial%20Assistive%20Payload%20Transportation%20Using%20Quadrotor%20UAVs%20with%0A%20%20Nonsingular%20Fast%20Terminal%20SMC%20for%20Human%20Physical%20Interaction&body=Title%3A%20Aerial%20Assistive%20Payload%20Transportation%20Using%20Quadrotor%20UAVs%20with%0A%20%20Nonsingular%20Fast%20Terminal%20SMC%20for%20Human%20Physical%20Interaction%0AAuthor%3A%20Hussein%20Naser%20and%20Hashim%20A.%20Hashim%20and%20Mojtaba%20Ahmadi%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20utilizing%20underactuated%20quadrotor%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20as%20assistive%20devices%20in%20cooperative%20payload%0Atransportation%20task%20through%20human%20guidance%20and%20physical%20interaction.%20The%0Aproposed%20system%20consists%20of%20two%20underactuated%20UAVs%20rigidly%20connected%20to%20the%0Atransported%20payload.%20This%20task%20involves%20the%20collaboration%20between%20human%20and%0AUAVs%20to%20transport%20and%20manipulate%20a%20payload.%20The%20goal%20is%20to%20reduce%20the%20workload%0Aof%20the%20human%20and%20enable%20seamless%20interaction%20between%20the%20human%20operator%20and%20the%0Aaerial%20vehicle.%20An%20Admittance-Nonsingular%20Fast%20Terminal%20Sliding%20Mode%20Control%0A%28NFTSMC%29%20is%20employed%20to%20control%20and%20asymptotically%20stabilize%20the%20system%20while%0Aperforming%20the%20task%2C%20where%20forces%20are%20applied%20to%20the%20payload%20by%20the%20human%0Aoperator%20dictate%20the%20aerial%20vehicle%27s%20motion.%20The%20stability%20of%20the%20proposed%0Acontroller%20is%20confirmed%20using%20Lyapunov%20analysis.%20Extensive%20simulation%20studies%0Awere%20conducted%20using%20MATLAB%2C%20Robot%20Operating%20System%20%28ROS%29%2C%20and%20Gazebo%20to%0Avalidate%20robustness%20and%20effectiveness%20of%20the%20proposed%20controller%20in%20assisting%0Awith%20payload%20transportation%20tasks.%20Results%20demonstrates%20feasibility%20and%0Apotential%20benefits%20utilizing%20quadrotor%20UAVs%20as%20assistive%20devices%20for%20payload%0Atransportation%20through%20intuitive%20human-guided%20control.%20Keywords%20Cooperative%0Apayload%20transportation%2C%20Admittance%20control%2C%20Sliding%20mode%20control%2C%20Quadrotor%0Acontrol%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerial%2520Assistive%2520Payload%2520Transportation%2520Using%2520Quadrotor%2520UAVs%2520with%250A%2520%2520Nonsingular%2520Fast%2520Terminal%2520SMC%2520for%2520Human%2520Physical%2520Interaction%26entry.906535625%3DHussein%2520Naser%2520and%2520Hashim%2520A.%2520Hashim%2520and%2520Mojtaba%2520Ahmadi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520utilizing%2520underactuated%2520quadrotor%250AUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520as%2520assistive%2520devices%2520in%2520cooperative%2520payload%250Atransportation%2520task%2520through%2520human%2520guidance%2520and%2520physical%2520interaction.%2520The%250Aproposed%2520system%2520consists%2520of%2520two%2520underactuated%2520UAVs%2520rigidly%2520connected%2520to%2520the%250Atransported%2520payload.%2520This%2520task%2520involves%2520the%2520collaboration%2520between%2520human%2520and%250AUAVs%2520to%2520transport%2520and%2520manipulate%2520a%2520payload.%2520The%2520goal%2520is%2520to%2520reduce%2520the%2520workload%250Aof%2520the%2520human%2520and%2520enable%2520seamless%2520interaction%2520between%2520the%2520human%2520operator%2520and%2520the%250Aaerial%2520vehicle.%2520An%2520Admittance-Nonsingular%2520Fast%2520Terminal%2520Sliding%2520Mode%2520Control%250A%2528NFTSMC%2529%2520is%2520employed%2520to%2520control%2520and%2520asymptotically%2520stabilize%2520the%2520system%2520while%250Aperforming%2520the%2520task%252C%2520where%2520forces%2520are%2520applied%2520to%2520the%2520payload%2520by%2520the%2520human%250Aoperator%2520dictate%2520the%2520aerial%2520vehicle%2527s%2520motion.%2520The%2520stability%2520of%2520the%2520proposed%250Acontroller%2520is%2520confirmed%2520using%2520Lyapunov%2520analysis.%2520Extensive%2520simulation%2520studies%250Awere%2520conducted%2520using%2520MATLAB%252C%2520Robot%2520Operating%2520System%2520%2528ROS%2529%252C%2520and%2520Gazebo%2520to%250Avalidate%2520robustness%2520and%2520effectiveness%2520of%2520the%2520proposed%2520controller%2520in%2520assisting%250Awith%2520payload%2520transportation%2520tasks.%2520Results%2520demonstrates%2520feasibility%2520and%250Apotential%2520benefits%2520utilizing%2520quadrotor%2520UAVs%2520as%2520assistive%2520devices%2520for%2520payload%250Atransportation%2520through%2520intuitive%2520human-guided%2520control.%2520Keywords%2520Cooperative%250Apayload%2520transportation%252C%2520Admittance%2520control%252C%2520Sliding%2520mode%2520control%252C%2520Quadrotor%250Acontrol%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aerial%20Assistive%20Payload%20Transportation%20Using%20Quadrotor%20UAVs%20with%0A%20%20Nonsingular%20Fast%20Terminal%20SMC%20for%20Human%20Physical%20Interaction&entry.906535625=Hussein%20Naser%20and%20Hashim%20A.%20Hashim%20and%20Mojtaba%20Ahmadi&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20utilizing%20underactuated%20quadrotor%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20as%20assistive%20devices%20in%20cooperative%20payload%0Atransportation%20task%20through%20human%20guidance%20and%20physical%20interaction.%20The%0Aproposed%20system%20consists%20of%20two%20underactuated%20UAVs%20rigidly%20connected%20to%20the%0Atransported%20payload.%20This%20task%20involves%20the%20collaboration%20between%20human%20and%0AUAVs%20to%20transport%20and%20manipulate%20a%20payload.%20The%20goal%20is%20to%20reduce%20the%20workload%0Aof%20the%20human%20and%20enable%20seamless%20interaction%20between%20the%20human%20operator%20and%20the%0Aaerial%20vehicle.%20An%20Admittance-Nonsingular%20Fast%20Terminal%20Sliding%20Mode%20Control%0A%28NFTSMC%29%20is%20employed%20to%20control%20and%20asymptotically%20stabilize%20the%20system%20while%0Aperforming%20the%20task%2C%20where%20forces%20are%20applied%20to%20the%20payload%20by%20the%20human%0Aoperator%20dictate%20the%20aerial%20vehicle%27s%20motion.%20The%20stability%20of%20the%20proposed%0Acontroller%20is%20confirmed%20using%20Lyapunov%20analysis.%20Extensive%20simulation%20studies%0Awere%20conducted%20using%20MATLAB%2C%20Robot%20Operating%20System%20%28ROS%29%2C%20and%20Gazebo%20to%0Avalidate%20robustness%20and%20effectiveness%20of%20the%20proposed%20controller%20in%20assisting%0Awith%20payload%20transportation%20tasks.%20Results%20demonstrates%20feasibility%20and%0Apotential%20benefits%20utilizing%20quadrotor%20UAVs%20as%20assistive%20devices%20for%20payload%0Atransportation%20through%20intuitive%20human-guided%20control.%20Keywords%20Cooperative%0Apayload%20transportation%2C%20Admittance%20control%2C%20Sliding%20mode%20control%2C%20Quadrotor%0Acontrol%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17748v1&entry.124074799=Read"},
{"title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging", "author": "\u0141ukasz Borchmann", "abstract": "  ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily\ndue to an evaluation setup that prevents direct comparison of answer choices\nrather than inherent complexity. Although some researchers have quietly shifted\nto a more appropriate scheme over the last year, the implications of this\nchange have yet to be widely acknowledged. We highlight this overlooked shift,\nshow how similar evaluation practices falsely imply reasoning deficits in other\nbenchmarks, and demonstrate that fairer methods dramatically reduce performance\ngaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing\nso, we reveal how evaluation shapes perceived difficulty and offer guidelines\nto ensure that multiple-choice evaluations accurately reflect actual model\ncapabilities.\n", "link": "http://arxiv.org/abs/2412.17758v1", "date": "2024-12-23", "relevancy": 2.179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4489}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20Case%20You%20Missed%20It%3A%20ARC%20%27Challenge%27%20Is%20Not%20That%20Challenging&body=Title%3A%20In%20Case%20You%20Missed%20It%3A%20ARC%20%27Challenge%27%20Is%20Not%20That%20Challenging%0AAuthor%3A%20%C5%81ukasz%20Borchmann%0AAbstract%3A%20%20%20ARC%20Challenge%20appears%20more%20difficult%20than%20ARC%20Easy%20for%20modern%20LLMs%20primarily%0Adue%20to%20an%20evaluation%20setup%20that%20prevents%20direct%20comparison%20of%20answer%20choices%0Arather%20than%20inherent%20complexity.%20Although%20some%20researchers%20have%20quietly%20shifted%0Ato%20a%20more%20appropriate%20scheme%20over%20the%20last%20year%2C%20the%20implications%20of%20this%0Achange%20have%20yet%20to%20be%20widely%20acknowledged.%20We%20highlight%20this%20overlooked%20shift%2C%0Ashow%20how%20similar%20evaluation%20practices%20falsely%20imply%20reasoning%20deficits%20in%20other%0Abenchmarks%2C%20and%20demonstrate%20that%20fairer%20methods%20dramatically%20reduce%20performance%0Agaps%20%28e.g.%20on%20SIQA%29%20and%20even%20yield%20superhuman%20results%20%28OpenBookQA%29.%20In%20doing%0Aso%2C%20we%20reveal%20how%20evaluation%20shapes%20perceived%20difficulty%20and%20offer%20guidelines%0Ato%20ensure%20that%20multiple-choice%20evaluations%20accurately%20reflect%20actual%20model%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520Case%2520You%2520Missed%2520It%253A%2520ARC%2520%2527Challenge%2527%2520Is%2520Not%2520That%2520Challenging%26entry.906535625%3D%25C5%2581ukasz%2520Borchmann%26entry.1292438233%3D%2520%2520ARC%2520Challenge%2520appears%2520more%2520difficult%2520than%2520ARC%2520Easy%2520for%2520modern%2520LLMs%2520primarily%250Adue%2520to%2520an%2520evaluation%2520setup%2520that%2520prevents%2520direct%2520comparison%2520of%2520answer%2520choices%250Arather%2520than%2520inherent%2520complexity.%2520Although%2520some%2520researchers%2520have%2520quietly%2520shifted%250Ato%2520a%2520more%2520appropriate%2520scheme%2520over%2520the%2520last%2520year%252C%2520the%2520implications%2520of%2520this%250Achange%2520have%2520yet%2520to%2520be%2520widely%2520acknowledged.%2520We%2520highlight%2520this%2520overlooked%2520shift%252C%250Ashow%2520how%2520similar%2520evaluation%2520practices%2520falsely%2520imply%2520reasoning%2520deficits%2520in%2520other%250Abenchmarks%252C%2520and%2520demonstrate%2520that%2520fairer%2520methods%2520dramatically%2520reduce%2520performance%250Agaps%2520%2528e.g.%2520on%2520SIQA%2529%2520and%2520even%2520yield%2520superhuman%2520results%2520%2528OpenBookQA%2529.%2520In%2520doing%250Aso%252C%2520we%2520reveal%2520how%2520evaluation%2520shapes%2520perceived%2520difficulty%2520and%2520offer%2520guidelines%250Ato%2520ensure%2520that%2520multiple-choice%2520evaluations%2520accurately%2520reflect%2520actual%2520model%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20Case%20You%20Missed%20It%3A%20ARC%20%27Challenge%27%20Is%20Not%20That%20Challenging&entry.906535625=%C5%81ukasz%20Borchmann&entry.1292438233=%20%20ARC%20Challenge%20appears%20more%20difficult%20than%20ARC%20Easy%20for%20modern%20LLMs%20primarily%0Adue%20to%20an%20evaluation%20setup%20that%20prevents%20direct%20comparison%20of%20answer%20choices%0Arather%20than%20inherent%20complexity.%20Although%20some%20researchers%20have%20quietly%20shifted%0Ato%20a%20more%20appropriate%20scheme%20over%20the%20last%20year%2C%20the%20implications%20of%20this%0Achange%20have%20yet%20to%20be%20widely%20acknowledged.%20We%20highlight%20this%20overlooked%20shift%2C%0Ashow%20how%20similar%20evaluation%20practices%20falsely%20imply%20reasoning%20deficits%20in%20other%0Abenchmarks%2C%20and%20demonstrate%20that%20fairer%20methods%20dramatically%20reduce%20performance%0Agaps%20%28e.g.%20on%20SIQA%29%20and%20even%20yield%20superhuman%20results%20%28OpenBookQA%29.%20In%20doing%0Aso%2C%20we%20reveal%20how%20evaluation%20shapes%20perceived%20difficulty%20and%20offer%20guidelines%0Ato%20ensure%20that%20multiple-choice%20evaluations%20accurately%20reflect%20actual%20model%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17758v1&entry.124074799=Read"},
{"title": "Large Language Model-Brained GUI Agents: A Survey", "author": "Chaoyun Zhang and Shilin He and Jiaxu Qian and Bowen Li and Liqun Li and Si Qin and Yu Kang and Minghua Ma and Guyue Liu and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang and Qi Zhang", "abstract": "  GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.\n", "link": "http://arxiv.org/abs/2411.18279v5", "date": "2024-12-23", "relevancy": 2.1753, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5735}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey&body=Title%3A%20Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey%0AAuthor%3A%20Chaoyun%20Zhang%20and%20Shilin%20He%20and%20Jiaxu%20Qian%20and%20Bowen%20Li%20and%20Liqun%20Li%20and%20Si%20Qin%20and%20Yu%20Kang%20and%20Minghua%20Ma%20and%20Guyue%20Liu%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20GUIs%20have%20long%20been%20central%20to%20human-computer%20interaction%2C%20providing%20an%0Aintuitive%20and%20visually-driven%20way%20to%20access%20and%20interact%20with%20digital%20systems.%0AThe%20advent%20of%20LLMs%2C%20particularly%20multimodal%20models%2C%20has%20ushered%20in%20a%20new%20era%20of%0AGUI%20automation.%20They%20have%20demonstrated%20exceptional%20capabilities%20in%20natural%0Alanguage%20understanding%2C%20code%20generation%2C%20and%20visual%20processing.%20This%20has%20paved%0Athe%20way%20for%20a%20new%20generation%20of%20LLM-brained%20GUI%20agents%20capable%20of%20interpreting%0Acomplex%20GUI%20elements%20and%20autonomously%20executing%20actions%20based%20on%20natural%0Alanguage%20instructions.%20These%20agents%20represent%20a%20paradigm%20shift%2C%20enabling%20users%0Ato%20perform%20intricate%2C%20multi-step%20tasks%20through%20simple%20conversational%20commands.%0ATheir%20applications%20span%20across%20web%20navigation%2C%20mobile%20app%20interactions%2C%20and%0Adesktop%20automation%2C%20offering%20a%20transformative%20user%20experience%20that%0Arevolutionizes%20how%20individuals%20interact%20with%20software.%20This%20emerging%20field%20is%0Arapidly%20advancing%2C%20with%20significant%20progress%20in%20both%20research%20and%20industry.%0A%20%20To%20provide%20a%20structured%20understanding%20of%20this%20trend%2C%20this%20paper%20presents%20a%0Acomprehensive%20survey%20of%20LLM-brained%20GUI%20agents%2C%20exploring%20their%20historical%0Aevolution%2C%20core%20components%2C%20and%20advanced%20techniques.%20We%20address%20research%0Aquestions%20such%20as%20existing%20GUI%20agent%20frameworks%2C%20the%20collection%20and%20utilization%0Aof%20data%20for%20training%20specialized%20GUI%20agents%2C%20the%20development%20of%20large%20action%0Amodels%20tailored%20for%20GUI%20tasks%2C%20and%20the%20evaluation%20metrics%20and%20benchmarks%0Anecessary%20to%20assess%20their%20effectiveness.%20Additionally%2C%20we%20examine%20emerging%0Aapplications%20powered%20by%20these%20agents.%20Through%20a%20detailed%20analysis%2C%20this%20survey%0Aidentifies%20key%20research%20gaps%20and%20outlines%20a%20roadmap%20for%20future%20advancements%20in%0Athe%20field.%20By%20consolidating%20foundational%20knowledge%20and%20state-of-the-art%0Adevelopments%2C%20this%20work%20aims%20to%20guide%20both%20researchers%20and%20practitioners%20in%0Aovercoming%20challenges%20and%20unlocking%20the%20full%20potential%20of%20LLM-brained%20GUI%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18279v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-Brained%2520GUI%2520Agents%253A%2520A%2520Survey%26entry.906535625%3DChaoyun%2520Zhang%2520and%2520Shilin%2520He%2520and%2520Jiaxu%2520Qian%2520and%2520Bowen%2520Li%2520and%2520Liqun%2520Li%2520and%2520Si%2520Qin%2520and%2520Yu%2520Kang%2520and%2520Minghua%2520Ma%2520and%2520Guyue%2520Liu%2520and%2520Qingwei%2520Lin%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520GUIs%2520have%2520long%2520been%2520central%2520to%2520human-computer%2520interaction%252C%2520providing%2520an%250Aintuitive%2520and%2520visually-driven%2520way%2520to%2520access%2520and%2520interact%2520with%2520digital%2520systems.%250AThe%2520advent%2520of%2520LLMs%252C%2520particularly%2520multimodal%2520models%252C%2520has%2520ushered%2520in%2520a%2520new%2520era%2520of%250AGUI%2520automation.%2520They%2520have%2520demonstrated%2520exceptional%2520capabilities%2520in%2520natural%250Alanguage%2520understanding%252C%2520code%2520generation%252C%2520and%2520visual%2520processing.%2520This%2520has%2520paved%250Athe%2520way%2520for%2520a%2520new%2520generation%2520of%2520LLM-brained%2520GUI%2520agents%2520capable%2520of%2520interpreting%250Acomplex%2520GUI%2520elements%2520and%2520autonomously%2520executing%2520actions%2520based%2520on%2520natural%250Alanguage%2520instructions.%2520These%2520agents%2520represent%2520a%2520paradigm%2520shift%252C%2520enabling%2520users%250Ato%2520perform%2520intricate%252C%2520multi-step%2520tasks%2520through%2520simple%2520conversational%2520commands.%250ATheir%2520applications%2520span%2520across%2520web%2520navigation%252C%2520mobile%2520app%2520interactions%252C%2520and%250Adesktop%2520automation%252C%2520offering%2520a%2520transformative%2520user%2520experience%2520that%250Arevolutionizes%2520how%2520individuals%2520interact%2520with%2520software.%2520This%2520emerging%2520field%2520is%250Arapidly%2520advancing%252C%2520with%2520significant%2520progress%2520in%2520both%2520research%2520and%2520industry.%250A%2520%2520To%2520provide%2520a%2520structured%2520understanding%2520of%2520this%2520trend%252C%2520this%2520paper%2520presents%2520a%250Acomprehensive%2520survey%2520of%2520LLM-brained%2520GUI%2520agents%252C%2520exploring%2520their%2520historical%250Aevolution%252C%2520core%2520components%252C%2520and%2520advanced%2520techniques.%2520We%2520address%2520research%250Aquestions%2520such%2520as%2520existing%2520GUI%2520agent%2520frameworks%252C%2520the%2520collection%2520and%2520utilization%250Aof%2520data%2520for%2520training%2520specialized%2520GUI%2520agents%252C%2520the%2520development%2520of%2520large%2520action%250Amodels%2520tailored%2520for%2520GUI%2520tasks%252C%2520and%2520the%2520evaluation%2520metrics%2520and%2520benchmarks%250Anecessary%2520to%2520assess%2520their%2520effectiveness.%2520Additionally%252C%2520we%2520examine%2520emerging%250Aapplications%2520powered%2520by%2520these%2520agents.%2520Through%2520a%2520detailed%2520analysis%252C%2520this%2520survey%250Aidentifies%2520key%2520research%2520gaps%2520and%2520outlines%2520a%2520roadmap%2520for%2520future%2520advancements%2520in%250Athe%2520field.%2520By%2520consolidating%2520foundational%2520knowledge%2520and%2520state-of-the-art%250Adevelopments%252C%2520this%2520work%2520aims%2520to%2520guide%2520both%2520researchers%2520and%2520practitioners%2520in%250Aovercoming%2520challenges%2520and%2520unlocking%2520the%2520full%2520potential%2520of%2520LLM-brained%2520GUI%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18279v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey&entry.906535625=Chaoyun%20Zhang%20and%20Shilin%20He%20and%20Jiaxu%20Qian%20and%20Bowen%20Li%20and%20Liqun%20Li%20and%20Si%20Qin%20and%20Yu%20Kang%20and%20Minghua%20Ma%20and%20Guyue%20Liu%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20GUIs%20have%20long%20been%20central%20to%20human-computer%20interaction%2C%20providing%20an%0Aintuitive%20and%20visually-driven%20way%20to%20access%20and%20interact%20with%20digital%20systems.%0AThe%20advent%20of%20LLMs%2C%20particularly%20multimodal%20models%2C%20has%20ushered%20in%20a%20new%20era%20of%0AGUI%20automation.%20They%20have%20demonstrated%20exceptional%20capabilities%20in%20natural%0Alanguage%20understanding%2C%20code%20generation%2C%20and%20visual%20processing.%20This%20has%20paved%0Athe%20way%20for%20a%20new%20generation%20of%20LLM-brained%20GUI%20agents%20capable%20of%20interpreting%0Acomplex%20GUI%20elements%20and%20autonomously%20executing%20actions%20based%20on%20natural%0Alanguage%20instructions.%20These%20agents%20represent%20a%20paradigm%20shift%2C%20enabling%20users%0Ato%20perform%20intricate%2C%20multi-step%20tasks%20through%20simple%20conversational%20commands.%0ATheir%20applications%20span%20across%20web%20navigation%2C%20mobile%20app%20interactions%2C%20and%0Adesktop%20automation%2C%20offering%20a%20transformative%20user%20experience%20that%0Arevolutionizes%20how%20individuals%20interact%20with%20software.%20This%20emerging%20field%20is%0Arapidly%20advancing%2C%20with%20significant%20progress%20in%20both%20research%20and%20industry.%0A%20%20To%20provide%20a%20structured%20understanding%20of%20this%20trend%2C%20this%20paper%20presents%20a%0Acomprehensive%20survey%20of%20LLM-brained%20GUI%20agents%2C%20exploring%20their%20historical%0Aevolution%2C%20core%20components%2C%20and%20advanced%20techniques.%20We%20address%20research%0Aquestions%20such%20as%20existing%20GUI%20agent%20frameworks%2C%20the%20collection%20and%20utilization%0Aof%20data%20for%20training%20specialized%20GUI%20agents%2C%20the%20development%20of%20large%20action%0Amodels%20tailored%20for%20GUI%20tasks%2C%20and%20the%20evaluation%20metrics%20and%20benchmarks%0Anecessary%20to%20assess%20their%20effectiveness.%20Additionally%2C%20we%20examine%20emerging%0Aapplications%20powered%20by%20these%20agents.%20Through%20a%20detailed%20analysis%2C%20this%20survey%0Aidentifies%20key%20research%20gaps%20and%20outlines%20a%20roadmap%20for%20future%20advancements%20in%0Athe%20field.%20By%20consolidating%20foundational%20knowledge%20and%20state-of-the-art%0Adevelopments%2C%20this%20work%20aims%20to%20guide%20both%20researchers%20and%20practitioners%20in%0Aovercoming%20challenges%20and%20unlocking%20the%20full%20potential%20of%20LLM-brained%20GUI%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18279v5&entry.124074799=Read"},
{"title": "Enhanced Temporal Processing in Spiking Neural Networks for Static\n  Object Detection Using 3D Convolutions", "author": "Huaxu He", "abstract": "  Spiking Neural Networks (SNNs) are a class of network models capable of\nprocessing spatiotemporal information, with event-driven characteristics and\nenergy efficiency advantages. Recently, directly trained SNNs have shown\npotential to match or surpass the performance of traditional Artificial Neural\nNetworks (ANNs) in classification tasks. However, in object detection tasks,\ndirectly trained SNNs still exhibit a significant performance gap compared to\nANNs when tested on frame-based static object datasets (such as COCO2017).\nTherefore, bridging this performance gap and enabling directly trained SNNs to\nachieve performance comparable to ANNs on these static datasets has become one\nof the key challenges in the development of SNNs.To address this challenge,\nthis paper focuses on enhancing the SNN's unique ability to process\nspatiotemporal information. Spiking neurons, as the core components of SNNs,\nfacilitate the exchange of information between different temporal channels\nduring the process of converting input floating-point data into binary spike\nsignals. However, existing neuron models still have certain limitations in the\ncommunication of temporal information. Some studies have even suggested that\ndisabling the backpropagation in the time dimension during SNN training can\nstill yield good training results. To improve the SNN handling of temporal\ninformation, this paper proposes replacing traditional 2D convolutions with 3D\nconvolutions, thus directly incorporating temporal information into the\nconvolutional process. Additionally, temporal information recurrence mechanism\nis introduced within the neurons to further enhance the neurons' efficiency in\nutilizing temporal information.Experimental results show that the proposed\nmethod enables directly trained SNNs to achieve performance levels comparable\nto ANNs on the COCO2017 and VOC datasets.\n", "link": "http://arxiv.org/abs/2412.17654v1", "date": "2024-12-23", "relevancy": 2.174, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5509}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.547}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Temporal%20Processing%20in%20Spiking%20Neural%20Networks%20for%20Static%0A%20%20Object%20Detection%20Using%203D%20Convolutions&body=Title%3A%20Enhanced%20Temporal%20Processing%20in%20Spiking%20Neural%20Networks%20for%20Static%0A%20%20Object%20Detection%20Using%203D%20Convolutions%0AAuthor%3A%20Huaxu%20He%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20a%20class%20of%20network%20models%20capable%20of%0Aprocessing%20spatiotemporal%20information%2C%20with%20event-driven%20characteristics%20and%0Aenergy%20efficiency%20advantages.%20Recently%2C%20directly%20trained%20SNNs%20have%20shown%0Apotential%20to%20match%20or%20surpass%20the%20performance%20of%20traditional%20Artificial%20Neural%0ANetworks%20%28ANNs%29%20in%20classification%20tasks.%20However%2C%20in%20object%20detection%20tasks%2C%0Adirectly%20trained%20SNNs%20still%20exhibit%20a%20significant%20performance%20gap%20compared%20to%0AANNs%20when%20tested%20on%20frame-based%20static%20object%20datasets%20%28such%20as%20COCO2017%29.%0ATherefore%2C%20bridging%20this%20performance%20gap%20and%20enabling%20directly%20trained%20SNNs%20to%0Aachieve%20performance%20comparable%20to%20ANNs%20on%20these%20static%20datasets%20has%20become%20one%0Aof%20the%20key%20challenges%20in%20the%20development%20of%20SNNs.To%20address%20this%20challenge%2C%0Athis%20paper%20focuses%20on%20enhancing%20the%20SNN%27s%20unique%20ability%20to%20process%0Aspatiotemporal%20information.%20Spiking%20neurons%2C%20as%20the%20core%20components%20of%20SNNs%2C%0Afacilitate%20the%20exchange%20of%20information%20between%20different%20temporal%20channels%0Aduring%20the%20process%20of%20converting%20input%20floating-point%20data%20into%20binary%20spike%0Asignals.%20However%2C%20existing%20neuron%20models%20still%20have%20certain%20limitations%20in%20the%0Acommunication%20of%20temporal%20information.%20Some%20studies%20have%20even%20suggested%20that%0Adisabling%20the%20backpropagation%20in%20the%20time%20dimension%20during%20SNN%20training%20can%0Astill%20yield%20good%20training%20results.%20To%20improve%20the%20SNN%20handling%20of%20temporal%0Ainformation%2C%20this%20paper%20proposes%20replacing%20traditional%202D%20convolutions%20with%203D%0Aconvolutions%2C%20thus%20directly%20incorporating%20temporal%20information%20into%20the%0Aconvolutional%20process.%20Additionally%2C%20temporal%20information%20recurrence%20mechanism%0Ais%20introduced%20within%20the%20neurons%20to%20further%20enhance%20the%20neurons%27%20efficiency%20in%0Autilizing%20temporal%20information.Experimental%20results%20show%20that%20the%20proposed%0Amethod%20enables%20directly%20trained%20SNNs%20to%20achieve%20performance%20levels%20comparable%0Ato%20ANNs%20on%20the%20COCO2017%20and%20VOC%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Temporal%2520Processing%2520in%2520Spiking%2520Neural%2520Networks%2520for%2520Static%250A%2520%2520Object%2520Detection%2520Using%25203D%2520Convolutions%26entry.906535625%3DHuaxu%2520He%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520a%2520class%2520of%2520network%2520models%2520capable%2520of%250Aprocessing%2520spatiotemporal%2520information%252C%2520with%2520event-driven%2520characteristics%2520and%250Aenergy%2520efficiency%2520advantages.%2520Recently%252C%2520directly%2520trained%2520SNNs%2520have%2520shown%250Apotential%2520to%2520match%2520or%2520surpass%2520the%2520performance%2520of%2520traditional%2520Artificial%2520Neural%250ANetworks%2520%2528ANNs%2529%2520in%2520classification%2520tasks.%2520However%252C%2520in%2520object%2520detection%2520tasks%252C%250Adirectly%2520trained%2520SNNs%2520still%2520exhibit%2520a%2520significant%2520performance%2520gap%2520compared%2520to%250AANNs%2520when%2520tested%2520on%2520frame-based%2520static%2520object%2520datasets%2520%2528such%2520as%2520COCO2017%2529.%250ATherefore%252C%2520bridging%2520this%2520performance%2520gap%2520and%2520enabling%2520directly%2520trained%2520SNNs%2520to%250Aachieve%2520performance%2520comparable%2520to%2520ANNs%2520on%2520these%2520static%2520datasets%2520has%2520become%2520one%250Aof%2520the%2520key%2520challenges%2520in%2520the%2520development%2520of%2520SNNs.To%2520address%2520this%2520challenge%252C%250Athis%2520paper%2520focuses%2520on%2520enhancing%2520the%2520SNN%2527s%2520unique%2520ability%2520to%2520process%250Aspatiotemporal%2520information.%2520Spiking%2520neurons%252C%2520as%2520the%2520core%2520components%2520of%2520SNNs%252C%250Afacilitate%2520the%2520exchange%2520of%2520information%2520between%2520different%2520temporal%2520channels%250Aduring%2520the%2520process%2520of%2520converting%2520input%2520floating-point%2520data%2520into%2520binary%2520spike%250Asignals.%2520However%252C%2520existing%2520neuron%2520models%2520still%2520have%2520certain%2520limitations%2520in%2520the%250Acommunication%2520of%2520temporal%2520information.%2520Some%2520studies%2520have%2520even%2520suggested%2520that%250Adisabling%2520the%2520backpropagation%2520in%2520the%2520time%2520dimension%2520during%2520SNN%2520training%2520can%250Astill%2520yield%2520good%2520training%2520results.%2520To%2520improve%2520the%2520SNN%2520handling%2520of%2520temporal%250Ainformation%252C%2520this%2520paper%2520proposes%2520replacing%2520traditional%25202D%2520convolutions%2520with%25203D%250Aconvolutions%252C%2520thus%2520directly%2520incorporating%2520temporal%2520information%2520into%2520the%250Aconvolutional%2520process.%2520Additionally%252C%2520temporal%2520information%2520recurrence%2520mechanism%250Ais%2520introduced%2520within%2520the%2520neurons%2520to%2520further%2520enhance%2520the%2520neurons%2527%2520efficiency%2520in%250Autilizing%2520temporal%2520information.Experimental%2520results%2520show%2520that%2520the%2520proposed%250Amethod%2520enables%2520directly%2520trained%2520SNNs%2520to%2520achieve%2520performance%2520levels%2520comparable%250Ato%2520ANNs%2520on%2520the%2520COCO2017%2520and%2520VOC%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Temporal%20Processing%20in%20Spiking%20Neural%20Networks%20for%20Static%0A%20%20Object%20Detection%20Using%203D%20Convolutions&entry.906535625=Huaxu%20He&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20a%20class%20of%20network%20models%20capable%20of%0Aprocessing%20spatiotemporal%20information%2C%20with%20event-driven%20characteristics%20and%0Aenergy%20efficiency%20advantages.%20Recently%2C%20directly%20trained%20SNNs%20have%20shown%0Apotential%20to%20match%20or%20surpass%20the%20performance%20of%20traditional%20Artificial%20Neural%0ANetworks%20%28ANNs%29%20in%20classification%20tasks.%20However%2C%20in%20object%20detection%20tasks%2C%0Adirectly%20trained%20SNNs%20still%20exhibit%20a%20significant%20performance%20gap%20compared%20to%0AANNs%20when%20tested%20on%20frame-based%20static%20object%20datasets%20%28such%20as%20COCO2017%29.%0ATherefore%2C%20bridging%20this%20performance%20gap%20and%20enabling%20directly%20trained%20SNNs%20to%0Aachieve%20performance%20comparable%20to%20ANNs%20on%20these%20static%20datasets%20has%20become%20one%0Aof%20the%20key%20challenges%20in%20the%20development%20of%20SNNs.To%20address%20this%20challenge%2C%0Athis%20paper%20focuses%20on%20enhancing%20the%20SNN%27s%20unique%20ability%20to%20process%0Aspatiotemporal%20information.%20Spiking%20neurons%2C%20as%20the%20core%20components%20of%20SNNs%2C%0Afacilitate%20the%20exchange%20of%20information%20between%20different%20temporal%20channels%0Aduring%20the%20process%20of%20converting%20input%20floating-point%20data%20into%20binary%20spike%0Asignals.%20However%2C%20existing%20neuron%20models%20still%20have%20certain%20limitations%20in%20the%0Acommunication%20of%20temporal%20information.%20Some%20studies%20have%20even%20suggested%20that%0Adisabling%20the%20backpropagation%20in%20the%20time%20dimension%20during%20SNN%20training%20can%0Astill%20yield%20good%20training%20results.%20To%20improve%20the%20SNN%20handling%20of%20temporal%0Ainformation%2C%20this%20paper%20proposes%20replacing%20traditional%202D%20convolutions%20with%203D%0Aconvolutions%2C%20thus%20directly%20incorporating%20temporal%20information%20into%20the%0Aconvolutional%20process.%20Additionally%2C%20temporal%20information%20recurrence%20mechanism%0Ais%20introduced%20within%20the%20neurons%20to%20further%20enhance%20the%20neurons%27%20efficiency%20in%0Autilizing%20temporal%20information.Experimental%20results%20show%20that%20the%20proposed%0Amethod%20enables%20directly%20trained%20SNNs%20to%20achieve%20performance%20levels%20comparable%0Ato%20ANNs%20on%20the%20COCO2017%20and%20VOC%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17654v1&entry.124074799=Read"},
{"title": "Guided Real Image Dehazing using YCbCr Color Space", "author": "Wenxuan Fang and Jankai Fan and Yu Zheng and Jiangwei Weng and Ying Tai and Jun Li", "abstract": "  Image dehazing, particularly with learning-based methods, has gained\nsignificant attention due to its importance in real-world applications.\nHowever, relying solely on the RGB color space often fall short, frequently\nleaving residual haze. This arises from two main issues: the difficulty in\nobtaining clear textural features from hazy RGB images and the complexity of\nacquiring real haze/clean image pairs outside controlled environments like\nsmoke-filled scenes. To address these issues, we first propose a novel\nStructure Guided Dehazing Network (SGDN) that leverages the superior structural\nproperties of YCbCr features over RGB. It comprises two key modules: Bi-Color\nGuidance Bridge (BGB) and Color Enhancement Module (CEM). BGB integrates a\nphase integration module and an interactive attention module, utilizing the\nrich texture features of the YCbCr space to guide the RGB space, thereby\nrecovering clearer features in both frequency and spatial domains. To maintain\ntonal consistency, CEM further enhances the color perception of RGB features by\naggregating YCbCr channel information. Furthermore, for effective supervised\nlearning, we introduce a Real-World Well-Aligned Haze (RW$^2$AH) dataset, which\nincludes a diverse range of scenes from various geographical regions and\nclimate conditions. Experimental results demonstrate that our method surpasses\nexisting state-of-the-art methods across multiple real-world smoke/haze\ndatasets. Code and Dataset:\n\\textcolor{blue}{\\url{https://github.com/fiwy0527/AAAI25_SGDN.}}\n", "link": "http://arxiv.org/abs/2412.17496v1", "date": "2024-12-23", "relevancy": 2.1728, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5581}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5496}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Real%20Image%20Dehazing%20using%20YCbCr%20Color%20Space&body=Title%3A%20Guided%20Real%20Image%20Dehazing%20using%20YCbCr%20Color%20Space%0AAuthor%3A%20Wenxuan%20Fang%20and%20Jankai%20Fan%20and%20Yu%20Zheng%20and%20Jiangwei%20Weng%20and%20Ying%20Tai%20and%20Jun%20Li%0AAbstract%3A%20%20%20Image%20dehazing%2C%20particularly%20with%20learning-based%20methods%2C%20has%20gained%0Asignificant%20attention%20due%20to%20its%20importance%20in%20real-world%20applications.%0AHowever%2C%20relying%20solely%20on%20the%20RGB%20color%20space%20often%20fall%20short%2C%20frequently%0Aleaving%20residual%20haze.%20This%20arises%20from%20two%20main%20issues%3A%20the%20difficulty%20in%0Aobtaining%20clear%20textural%20features%20from%20hazy%20RGB%20images%20and%20the%20complexity%20of%0Aacquiring%20real%20haze/clean%20image%20pairs%20outside%20controlled%20environments%20like%0Asmoke-filled%20scenes.%20To%20address%20these%20issues%2C%20we%20first%20propose%20a%20novel%0AStructure%20Guided%20Dehazing%20Network%20%28SGDN%29%20that%20leverages%20the%20superior%20structural%0Aproperties%20of%20YCbCr%20features%20over%20RGB.%20It%20comprises%20two%20key%20modules%3A%20Bi-Color%0AGuidance%20Bridge%20%28BGB%29%20and%20Color%20Enhancement%20Module%20%28CEM%29.%20BGB%20integrates%20a%0Aphase%20integration%20module%20and%20an%20interactive%20attention%20module%2C%20utilizing%20the%0Arich%20texture%20features%20of%20the%20YCbCr%20space%20to%20guide%20the%20RGB%20space%2C%20thereby%0Arecovering%20clearer%20features%20in%20both%20frequency%20and%20spatial%20domains.%20To%20maintain%0Atonal%20consistency%2C%20CEM%20further%20enhances%20the%20color%20perception%20of%20RGB%20features%20by%0Aaggregating%20YCbCr%20channel%20information.%20Furthermore%2C%20for%20effective%20supervised%0Alearning%2C%20we%20introduce%20a%20Real-World%20Well-Aligned%20Haze%20%28RW%24%5E2%24AH%29%20dataset%2C%20which%0Aincludes%20a%20diverse%20range%20of%20scenes%20from%20various%20geographical%20regions%20and%0Aclimate%20conditions.%20Experimental%20results%20demonstrate%20that%20our%20method%20surpasses%0Aexisting%20state-of-the-art%20methods%20across%20multiple%20real-world%20smoke/haze%0Adatasets.%20Code%20and%20Dataset%3A%0A%5Ctextcolor%7Bblue%7D%7B%5Curl%7Bhttps%3A//github.com/fiwy0527/AAAI25_SGDN.%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Real%2520Image%2520Dehazing%2520using%2520YCbCr%2520Color%2520Space%26entry.906535625%3DWenxuan%2520Fang%2520and%2520Jankai%2520Fan%2520and%2520Yu%2520Zheng%2520and%2520Jiangwei%2520Weng%2520and%2520Ying%2520Tai%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520Image%2520dehazing%252C%2520particularly%2520with%2520learning-based%2520methods%252C%2520has%2520gained%250Asignificant%2520attention%2520due%2520to%2520its%2520importance%2520in%2520real-world%2520applications.%250AHowever%252C%2520relying%2520solely%2520on%2520the%2520RGB%2520color%2520space%2520often%2520fall%2520short%252C%2520frequently%250Aleaving%2520residual%2520haze.%2520This%2520arises%2520from%2520two%2520main%2520issues%253A%2520the%2520difficulty%2520in%250Aobtaining%2520clear%2520textural%2520features%2520from%2520hazy%2520RGB%2520images%2520and%2520the%2520complexity%2520of%250Aacquiring%2520real%2520haze/clean%2520image%2520pairs%2520outside%2520controlled%2520environments%2520like%250Asmoke-filled%2520scenes.%2520To%2520address%2520these%2520issues%252C%2520we%2520first%2520propose%2520a%2520novel%250AStructure%2520Guided%2520Dehazing%2520Network%2520%2528SGDN%2529%2520that%2520leverages%2520the%2520superior%2520structural%250Aproperties%2520of%2520YCbCr%2520features%2520over%2520RGB.%2520It%2520comprises%2520two%2520key%2520modules%253A%2520Bi-Color%250AGuidance%2520Bridge%2520%2528BGB%2529%2520and%2520Color%2520Enhancement%2520Module%2520%2528CEM%2529.%2520BGB%2520integrates%2520a%250Aphase%2520integration%2520module%2520and%2520an%2520interactive%2520attention%2520module%252C%2520utilizing%2520the%250Arich%2520texture%2520features%2520of%2520the%2520YCbCr%2520space%2520to%2520guide%2520the%2520RGB%2520space%252C%2520thereby%250Arecovering%2520clearer%2520features%2520in%2520both%2520frequency%2520and%2520spatial%2520domains.%2520To%2520maintain%250Atonal%2520consistency%252C%2520CEM%2520further%2520enhances%2520the%2520color%2520perception%2520of%2520RGB%2520features%2520by%250Aaggregating%2520YCbCr%2520channel%2520information.%2520Furthermore%252C%2520for%2520effective%2520supervised%250Alearning%252C%2520we%2520introduce%2520a%2520Real-World%2520Well-Aligned%2520Haze%2520%2528RW%2524%255E2%2524AH%2529%2520dataset%252C%2520which%250Aincludes%2520a%2520diverse%2520range%2520of%2520scenes%2520from%2520various%2520geographical%2520regions%2520and%250Aclimate%2520conditions.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520surpasses%250Aexisting%2520state-of-the-art%2520methods%2520across%2520multiple%2520real-world%2520smoke/haze%250Adatasets.%2520Code%2520and%2520Dataset%253A%250A%255Ctextcolor%257Bblue%257D%257B%255Curl%257Bhttps%253A//github.com/fiwy0527/AAAI25_SGDN.%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Real%20Image%20Dehazing%20using%20YCbCr%20Color%20Space&entry.906535625=Wenxuan%20Fang%20and%20Jankai%20Fan%20and%20Yu%20Zheng%20and%20Jiangwei%20Weng%20and%20Ying%20Tai%20and%20Jun%20Li&entry.1292438233=%20%20Image%20dehazing%2C%20particularly%20with%20learning-based%20methods%2C%20has%20gained%0Asignificant%20attention%20due%20to%20its%20importance%20in%20real-world%20applications.%0AHowever%2C%20relying%20solely%20on%20the%20RGB%20color%20space%20often%20fall%20short%2C%20frequently%0Aleaving%20residual%20haze.%20This%20arises%20from%20two%20main%20issues%3A%20the%20difficulty%20in%0Aobtaining%20clear%20textural%20features%20from%20hazy%20RGB%20images%20and%20the%20complexity%20of%0Aacquiring%20real%20haze/clean%20image%20pairs%20outside%20controlled%20environments%20like%0Asmoke-filled%20scenes.%20To%20address%20these%20issues%2C%20we%20first%20propose%20a%20novel%0AStructure%20Guided%20Dehazing%20Network%20%28SGDN%29%20that%20leverages%20the%20superior%20structural%0Aproperties%20of%20YCbCr%20features%20over%20RGB.%20It%20comprises%20two%20key%20modules%3A%20Bi-Color%0AGuidance%20Bridge%20%28BGB%29%20and%20Color%20Enhancement%20Module%20%28CEM%29.%20BGB%20integrates%20a%0Aphase%20integration%20module%20and%20an%20interactive%20attention%20module%2C%20utilizing%20the%0Arich%20texture%20features%20of%20the%20YCbCr%20space%20to%20guide%20the%20RGB%20space%2C%20thereby%0Arecovering%20clearer%20features%20in%20both%20frequency%20and%20spatial%20domains.%20To%20maintain%0Atonal%20consistency%2C%20CEM%20further%20enhances%20the%20color%20perception%20of%20RGB%20features%20by%0Aaggregating%20YCbCr%20channel%20information.%20Furthermore%2C%20for%20effective%20supervised%0Alearning%2C%20we%20introduce%20a%20Real-World%20Well-Aligned%20Haze%20%28RW%24%5E2%24AH%29%20dataset%2C%20which%0Aincludes%20a%20diverse%20range%20of%20scenes%20from%20various%20geographical%20regions%20and%0Aclimate%20conditions.%20Experimental%20results%20demonstrate%20that%20our%20method%20surpasses%0Aexisting%20state-of-the-art%20methods%20across%20multiple%20real-world%20smoke/haze%0Adatasets.%20Code%20and%20Dataset%3A%0A%5Ctextcolor%7Bblue%7D%7B%5Curl%7Bhttps%3A//github.com/fiwy0527/AAAI25_SGDN.%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17496v1&entry.124074799=Read"},
{"title": "SCBench: A Sports Commentary Benchmark for Video LLMs", "author": "Kuangzhi Ge and Lingjun Chen and Kevin Zhang and Yulin Luo and Tianyu Shi and Liaoyuan Fan and Xiang Li and Guanqun Wang and Shanghang Zhang", "abstract": "  Recently, significant advances have been made in Video Large Language Models\n(Video LLMs) in both academia and industry. However, methods to evaluate and\nbenchmark the performance of different Video LLMs, especially their\nfine-grained, temporal visual capabilities, remain very limited. On one hand,\ncurrent benchmarks use relatively simple videos (e.g., subtitled movie clips)\nwhere the model can understand the entire video by processing just a few\nframes. On the other hand, their datasets lack diversity in task format,\ncomprising only QA or multi-choice QA, which overlooks the models' capacity for\ngenerating in-depth and precise texts. Sports videos, which feature intricate\nvisual information, sequential events, and emotionally charged commentary,\npresent a critical challenge for Video LLMs, making sports commentary an ideal\nbenchmarking task. Inspired by these challenges, we propose a novel task:\nsports video commentary generation, developed $\\textbf{SCBench}$ for Video\nLLMs. To construct such a benchmark, we introduce (1) $\\textbf{SCORES}$, a\nsix-dimensional metric specifically designed for our task, upon which we\npropose a GPT-based evaluation method, and (2) $\\textbf{CommentarySet}$, a\ndataset consisting of 5,775 annotated video clips and ground-truth labels\ntailored to our metric. Based on SCBench, we conduct comprehensive evaluations\non multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought\nbaseline methods. Our results found that InternVL-Chat-2 achieves the best\nperformance with 5.44, surpassing the second-best by 1.04. Our work provides a\nfresh perspective for future research, aiming to enhance models' overall\ncapabilities in complex visual understanding tasks. Our dataset will be\nreleased soon.\n", "link": "http://arxiv.org/abs/2412.17637v1", "date": "2024-12-23", "relevancy": 2.1592, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCBench%3A%20A%20Sports%20Commentary%20Benchmark%20for%20Video%20LLMs&body=Title%3A%20SCBench%3A%20A%20Sports%20Commentary%20Benchmark%20for%20Video%20LLMs%0AAuthor%3A%20Kuangzhi%20Ge%20and%20Lingjun%20Chen%20and%20Kevin%20Zhang%20and%20Yulin%20Luo%20and%20Tianyu%20Shi%20and%20Liaoyuan%20Fan%20and%20Xiang%20Li%20and%20Guanqun%20Wang%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20significant%20advances%20have%20been%20made%20in%20Video%20Large%20Language%20Models%0A%28Video%20LLMs%29%20in%20both%20academia%20and%20industry.%20However%2C%20methods%20to%20evaluate%20and%0Abenchmark%20the%20performance%20of%20different%20Video%20LLMs%2C%20especially%20their%0Afine-grained%2C%20temporal%20visual%20capabilities%2C%20remain%20very%20limited.%20On%20one%20hand%2C%0Acurrent%20benchmarks%20use%20relatively%20simple%20videos%20%28e.g.%2C%20subtitled%20movie%20clips%29%0Awhere%20the%20model%20can%20understand%20the%20entire%20video%20by%20processing%20just%20a%20few%0Aframes.%20On%20the%20other%20hand%2C%20their%20datasets%20lack%20diversity%20in%20task%20format%2C%0Acomprising%20only%20QA%20or%20multi-choice%20QA%2C%20which%20overlooks%20the%20models%27%20capacity%20for%0Agenerating%20in-depth%20and%20precise%20texts.%20Sports%20videos%2C%20which%20feature%20intricate%0Avisual%20information%2C%20sequential%20events%2C%20and%20emotionally%20charged%20commentary%2C%0Apresent%20a%20critical%20challenge%20for%20Video%20LLMs%2C%20making%20sports%20commentary%20an%20ideal%0Abenchmarking%20task.%20Inspired%20by%20these%20challenges%2C%20we%20propose%20a%20novel%20task%3A%0Asports%20video%20commentary%20generation%2C%20developed%20%24%5Ctextbf%7BSCBench%7D%24%20for%20Video%0ALLMs.%20To%20construct%20such%20a%20benchmark%2C%20we%20introduce%20%281%29%20%24%5Ctextbf%7BSCORES%7D%24%2C%20a%0Asix-dimensional%20metric%20specifically%20designed%20for%20our%20task%2C%20upon%20which%20we%0Apropose%20a%20GPT-based%20evaluation%20method%2C%20and%20%282%29%20%24%5Ctextbf%7BCommentarySet%7D%24%2C%20a%0Adataset%20consisting%20of%205%2C775%20annotated%20video%20clips%20and%20ground-truth%20labels%0Atailored%20to%20our%20metric.%20Based%20on%20SCBench%2C%20we%20conduct%20comprehensive%20evaluations%0Aon%20multiple%20Video%20LLMs%20%28e.g.%20VILA%2C%20Video-LLaVA%2C%20etc.%29%20and%20chain-of-thought%0Abaseline%20methods.%20Our%20results%20found%20that%20InternVL-Chat-2%20achieves%20the%20best%0Aperformance%20with%205.44%2C%20surpassing%20the%20second-best%20by%201.04.%20Our%20work%20provides%20a%0Afresh%20perspective%20for%20future%20research%2C%20aiming%20to%20enhance%20models%27%20overall%0Acapabilities%20in%20complex%20visual%20understanding%20tasks.%20Our%20dataset%20will%20be%0Areleased%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCBench%253A%2520A%2520Sports%2520Commentary%2520Benchmark%2520for%2520Video%2520LLMs%26entry.906535625%3DKuangzhi%2520Ge%2520and%2520Lingjun%2520Chen%2520and%2520Kevin%2520Zhang%2520and%2520Yulin%2520Luo%2520and%2520Tianyu%2520Shi%2520and%2520Liaoyuan%2520Fan%2520and%2520Xiang%2520Li%2520and%2520Guanqun%2520Wang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520significant%2520advances%2520have%2520been%2520made%2520in%2520Video%2520Large%2520Language%2520Models%250A%2528Video%2520LLMs%2529%2520in%2520both%2520academia%2520and%2520industry.%2520However%252C%2520methods%2520to%2520evaluate%2520and%250Abenchmark%2520the%2520performance%2520of%2520different%2520Video%2520LLMs%252C%2520especially%2520their%250Afine-grained%252C%2520temporal%2520visual%2520capabilities%252C%2520remain%2520very%2520limited.%2520On%2520one%2520hand%252C%250Acurrent%2520benchmarks%2520use%2520relatively%2520simple%2520videos%2520%2528e.g.%252C%2520subtitled%2520movie%2520clips%2529%250Awhere%2520the%2520model%2520can%2520understand%2520the%2520entire%2520video%2520by%2520processing%2520just%2520a%2520few%250Aframes.%2520On%2520the%2520other%2520hand%252C%2520their%2520datasets%2520lack%2520diversity%2520in%2520task%2520format%252C%250Acomprising%2520only%2520QA%2520or%2520multi-choice%2520QA%252C%2520which%2520overlooks%2520the%2520models%2527%2520capacity%2520for%250Agenerating%2520in-depth%2520and%2520precise%2520texts.%2520Sports%2520videos%252C%2520which%2520feature%2520intricate%250Avisual%2520information%252C%2520sequential%2520events%252C%2520and%2520emotionally%2520charged%2520commentary%252C%250Apresent%2520a%2520critical%2520challenge%2520for%2520Video%2520LLMs%252C%2520making%2520sports%2520commentary%2520an%2520ideal%250Abenchmarking%2520task.%2520Inspired%2520by%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520task%253A%250Asports%2520video%2520commentary%2520generation%252C%2520developed%2520%2524%255Ctextbf%257BSCBench%257D%2524%2520for%2520Video%250ALLMs.%2520To%2520construct%2520such%2520a%2520benchmark%252C%2520we%2520introduce%2520%25281%2529%2520%2524%255Ctextbf%257BSCORES%257D%2524%252C%2520a%250Asix-dimensional%2520metric%2520specifically%2520designed%2520for%2520our%2520task%252C%2520upon%2520which%2520we%250Apropose%2520a%2520GPT-based%2520evaluation%2520method%252C%2520and%2520%25282%2529%2520%2524%255Ctextbf%257BCommentarySet%257D%2524%252C%2520a%250Adataset%2520consisting%2520of%25205%252C775%2520annotated%2520video%2520clips%2520and%2520ground-truth%2520labels%250Atailored%2520to%2520our%2520metric.%2520Based%2520on%2520SCBench%252C%2520we%2520conduct%2520comprehensive%2520evaluations%250Aon%2520multiple%2520Video%2520LLMs%2520%2528e.g.%2520VILA%252C%2520Video-LLaVA%252C%2520etc.%2529%2520and%2520chain-of-thought%250Abaseline%2520methods.%2520Our%2520results%2520found%2520that%2520InternVL-Chat-2%2520achieves%2520the%2520best%250Aperformance%2520with%25205.44%252C%2520surpassing%2520the%2520second-best%2520by%25201.04.%2520Our%2520work%2520provides%2520a%250Afresh%2520perspective%2520for%2520future%2520research%252C%2520aiming%2520to%2520enhance%2520models%2527%2520overall%250Acapabilities%2520in%2520complex%2520visual%2520understanding%2520tasks.%2520Our%2520dataset%2520will%2520be%250Areleased%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCBench%3A%20A%20Sports%20Commentary%20Benchmark%20for%20Video%20LLMs&entry.906535625=Kuangzhi%20Ge%20and%20Lingjun%20Chen%20and%20Kevin%20Zhang%20and%20Yulin%20Luo%20and%20Tianyu%20Shi%20and%20Liaoyuan%20Fan%20and%20Xiang%20Li%20and%20Guanqun%20Wang%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Recently%2C%20significant%20advances%20have%20been%20made%20in%20Video%20Large%20Language%20Models%0A%28Video%20LLMs%29%20in%20both%20academia%20and%20industry.%20However%2C%20methods%20to%20evaluate%20and%0Abenchmark%20the%20performance%20of%20different%20Video%20LLMs%2C%20especially%20their%0Afine-grained%2C%20temporal%20visual%20capabilities%2C%20remain%20very%20limited.%20On%20one%20hand%2C%0Acurrent%20benchmarks%20use%20relatively%20simple%20videos%20%28e.g.%2C%20subtitled%20movie%20clips%29%0Awhere%20the%20model%20can%20understand%20the%20entire%20video%20by%20processing%20just%20a%20few%0Aframes.%20On%20the%20other%20hand%2C%20their%20datasets%20lack%20diversity%20in%20task%20format%2C%0Acomprising%20only%20QA%20or%20multi-choice%20QA%2C%20which%20overlooks%20the%20models%27%20capacity%20for%0Agenerating%20in-depth%20and%20precise%20texts.%20Sports%20videos%2C%20which%20feature%20intricate%0Avisual%20information%2C%20sequential%20events%2C%20and%20emotionally%20charged%20commentary%2C%0Apresent%20a%20critical%20challenge%20for%20Video%20LLMs%2C%20making%20sports%20commentary%20an%20ideal%0Abenchmarking%20task.%20Inspired%20by%20these%20challenges%2C%20we%20propose%20a%20novel%20task%3A%0Asports%20video%20commentary%20generation%2C%20developed%20%24%5Ctextbf%7BSCBench%7D%24%20for%20Video%0ALLMs.%20To%20construct%20such%20a%20benchmark%2C%20we%20introduce%20%281%29%20%24%5Ctextbf%7BSCORES%7D%24%2C%20a%0Asix-dimensional%20metric%20specifically%20designed%20for%20our%20task%2C%20upon%20which%20we%0Apropose%20a%20GPT-based%20evaluation%20method%2C%20and%20%282%29%20%24%5Ctextbf%7BCommentarySet%7D%24%2C%20a%0Adataset%20consisting%20of%205%2C775%20annotated%20video%20clips%20and%20ground-truth%20labels%0Atailored%20to%20our%20metric.%20Based%20on%20SCBench%2C%20we%20conduct%20comprehensive%20evaluations%0Aon%20multiple%20Video%20LLMs%20%28e.g.%20VILA%2C%20Video-LLaVA%2C%20etc.%29%20and%20chain-of-thought%0Abaseline%20methods.%20Our%20results%20found%20that%20InternVL-Chat-2%20achieves%20the%20best%0Aperformance%20with%205.44%2C%20surpassing%20the%20second-best%20by%201.04.%20Our%20work%20provides%20a%0Afresh%20perspective%20for%20future%20research%2C%20aiming%20to%20enhance%20models%27%20overall%0Acapabilities%20in%20complex%20visual%20understanding%20tasks.%20Our%20dataset%20will%20be%0Areleased%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17637v1&entry.124074799=Read"},
{"title": "FocusLLM: Precise Understanding of Long Context by Dynamic Condensing", "author": "Zhenyu Li and Yike Zhang and Tengyu Pan and Yutao Sun and Zhichao Duan and Junjie Fang and Rong Han and Zixuan Wang and Jianyong Wang", "abstract": "  Empowering LLMs with the ability to precisely understand long contexts is\ncrucial for many downstream applications. However, handling long contexts with\nconventional transformer architecture requires substantial training and\ninference resources. Existing context condensing methods cannot accurately\nunderstand the full context, as there is a considerable amount of information\nloss in the condensing process. To address these issues, we present FocusLLM, a\nframework designed to extend the fixed context length of any decoder-only LLM,\nallowing the model to focus on relevant information from very long sequences.\nFocusLLM first divides long text input into chunks based on the model's\noriginal context length. It then employs the dynamic condensing process to\ndistill crucial information from each chunk. Ultimately, through the novel\nparallel decoding mechanism, FocusLLM can integrate the extracted information\ninto its local context. FocusLLM stands out for great training efficiency and\nversatility: trained with an 8K input length and with much less training cost\nthan previous methods, FocusLLM exhibits superior performance across downstream\ntasks and maintains strong language modeling ability when handling extensive\nlong texts, even up to 400K tokens. Our code is available at\nhttps://github.com/leezythu/FocusLLM.\n", "link": "http://arxiv.org/abs/2408.11745v2", "date": "2024-12-23", "relevancy": 2.1569, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.55}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.55}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocusLLM%3A%20Precise%20Understanding%20of%20Long%20Context%20by%20Dynamic%20Condensing&body=Title%3A%20FocusLLM%3A%20Precise%20Understanding%20of%20Long%20Context%20by%20Dynamic%20Condensing%0AAuthor%3A%20Zhenyu%20Li%20and%20Yike%20Zhang%20and%20Tengyu%20Pan%20and%20Yutao%20Sun%20and%20Zhichao%20Duan%20and%20Junjie%20Fang%20and%20Rong%20Han%20and%20Zixuan%20Wang%20and%20Jianyong%20Wang%0AAbstract%3A%20%20%20Empowering%20LLMs%20with%20the%20ability%20to%20precisely%20understand%20long%20contexts%20is%0Acrucial%20for%20many%20downstream%20applications.%20However%2C%20handling%20long%20contexts%20with%0Aconventional%20transformer%20architecture%20requires%20substantial%20training%20and%0Ainference%20resources.%20Existing%20context%20condensing%20methods%20cannot%20accurately%0Aunderstand%20the%20full%20context%2C%20as%20there%20is%20a%20considerable%20amount%20of%20information%0Aloss%20in%20the%20condensing%20process.%20To%20address%20these%20issues%2C%20we%20present%20FocusLLM%2C%20a%0Aframework%20designed%20to%20extend%20the%20fixed%20context%20length%20of%20any%20decoder-only%20LLM%2C%0Aallowing%20the%20model%20to%20focus%20on%20relevant%20information%20from%20very%20long%20sequences.%0AFocusLLM%20first%20divides%20long%20text%20input%20into%20chunks%20based%20on%20the%20model%27s%0Aoriginal%20context%20length.%20It%20then%20employs%20the%20dynamic%20condensing%20process%20to%0Adistill%20crucial%20information%20from%20each%20chunk.%20Ultimately%2C%20through%20the%20novel%0Aparallel%20decoding%20mechanism%2C%20FocusLLM%20can%20integrate%20the%20extracted%20information%0Ainto%20its%20local%20context.%20FocusLLM%20stands%20out%20for%20great%20training%20efficiency%20and%0Aversatility%3A%20trained%20with%20an%208K%20input%20length%20and%20with%20much%20less%20training%20cost%0Athan%20previous%20methods%2C%20FocusLLM%20exhibits%20superior%20performance%20across%20downstream%0Atasks%20and%20maintains%20strong%20language%20modeling%20ability%20when%20handling%20extensive%0Along%20texts%2C%20even%20up%20to%20400K%20tokens.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/leezythu/FocusLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocusLLM%253A%2520Precise%2520Understanding%2520of%2520Long%2520Context%2520by%2520Dynamic%2520Condensing%26entry.906535625%3DZhenyu%2520Li%2520and%2520Yike%2520Zhang%2520and%2520Tengyu%2520Pan%2520and%2520Yutao%2520Sun%2520and%2520Zhichao%2520Duan%2520and%2520Junjie%2520Fang%2520and%2520Rong%2520Han%2520and%2520Zixuan%2520Wang%2520and%2520Jianyong%2520Wang%26entry.1292438233%3D%2520%2520Empowering%2520LLMs%2520with%2520the%2520ability%2520to%2520precisely%2520understand%2520long%2520contexts%2520is%250Acrucial%2520for%2520many%2520downstream%2520applications.%2520However%252C%2520handling%2520long%2520contexts%2520with%250Aconventional%2520transformer%2520architecture%2520requires%2520substantial%2520training%2520and%250Ainference%2520resources.%2520Existing%2520context%2520condensing%2520methods%2520cannot%2520accurately%250Aunderstand%2520the%2520full%2520context%252C%2520as%2520there%2520is%2520a%2520considerable%2520amount%2520of%2520information%250Aloss%2520in%2520the%2520condensing%2520process.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520FocusLLM%252C%2520a%250Aframework%2520designed%2520to%2520extend%2520the%2520fixed%2520context%2520length%2520of%2520any%2520decoder-only%2520LLM%252C%250Aallowing%2520the%2520model%2520to%2520focus%2520on%2520relevant%2520information%2520from%2520very%2520long%2520sequences.%250AFocusLLM%2520first%2520divides%2520long%2520text%2520input%2520into%2520chunks%2520based%2520on%2520the%2520model%2527s%250Aoriginal%2520context%2520length.%2520It%2520then%2520employs%2520the%2520dynamic%2520condensing%2520process%2520to%250Adistill%2520crucial%2520information%2520from%2520each%2520chunk.%2520Ultimately%252C%2520through%2520the%2520novel%250Aparallel%2520decoding%2520mechanism%252C%2520FocusLLM%2520can%2520integrate%2520the%2520extracted%2520information%250Ainto%2520its%2520local%2520context.%2520FocusLLM%2520stands%2520out%2520for%2520great%2520training%2520efficiency%2520and%250Aversatility%253A%2520trained%2520with%2520an%25208K%2520input%2520length%2520and%2520with%2520much%2520less%2520training%2520cost%250Athan%2520previous%2520methods%252C%2520FocusLLM%2520exhibits%2520superior%2520performance%2520across%2520downstream%250Atasks%2520and%2520maintains%2520strong%2520language%2520modeling%2520ability%2520when%2520handling%2520extensive%250Along%2520texts%252C%2520even%2520up%2520to%2520400K%2520tokens.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/leezythu/FocusLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocusLLM%3A%20Precise%20Understanding%20of%20Long%20Context%20by%20Dynamic%20Condensing&entry.906535625=Zhenyu%20Li%20and%20Yike%20Zhang%20and%20Tengyu%20Pan%20and%20Yutao%20Sun%20and%20Zhichao%20Duan%20and%20Junjie%20Fang%20and%20Rong%20Han%20and%20Zixuan%20Wang%20and%20Jianyong%20Wang&entry.1292438233=%20%20Empowering%20LLMs%20with%20the%20ability%20to%20precisely%20understand%20long%20contexts%20is%0Acrucial%20for%20many%20downstream%20applications.%20However%2C%20handling%20long%20contexts%20with%0Aconventional%20transformer%20architecture%20requires%20substantial%20training%20and%0Ainference%20resources.%20Existing%20context%20condensing%20methods%20cannot%20accurately%0Aunderstand%20the%20full%20context%2C%20as%20there%20is%20a%20considerable%20amount%20of%20information%0Aloss%20in%20the%20condensing%20process.%20To%20address%20these%20issues%2C%20we%20present%20FocusLLM%2C%20a%0Aframework%20designed%20to%20extend%20the%20fixed%20context%20length%20of%20any%20decoder-only%20LLM%2C%0Aallowing%20the%20model%20to%20focus%20on%20relevant%20information%20from%20very%20long%20sequences.%0AFocusLLM%20first%20divides%20long%20text%20input%20into%20chunks%20based%20on%20the%20model%27s%0Aoriginal%20context%20length.%20It%20then%20employs%20the%20dynamic%20condensing%20process%20to%0Adistill%20crucial%20information%20from%20each%20chunk.%20Ultimately%2C%20through%20the%20novel%0Aparallel%20decoding%20mechanism%2C%20FocusLLM%20can%20integrate%20the%20extracted%20information%0Ainto%20its%20local%20context.%20FocusLLM%20stands%20out%20for%20great%20training%20efficiency%20and%0Aversatility%3A%20trained%20with%20an%208K%20input%20length%20and%20with%20much%20less%20training%20cost%0Athan%20previous%20methods%2C%20FocusLLM%20exhibits%20superior%20performance%20across%20downstream%0Atasks%20and%20maintains%20strong%20language%20modeling%20ability%20when%20handling%20extensive%0Along%20texts%2C%20even%20up%20to%20400K%20tokens.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/leezythu/FocusLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11745v2&entry.124074799=Read"},
{"title": "What to Say and When to Say it: Live Fitness Coaching as a Testbed for\n  Situated Interaction", "author": "Sunny Panchal and Apratim Bhattacharyya and Guillaume Berger and Antoine Mercier and Cornelius Bohm and Florian Dietrichkeit and Reza Pourreza and Xuanlin Li and Pulkit Madan and Mingu Lee and Mark Todorovich and Ingo Bax and Roland Memisevic", "abstract": "  Vision-language models have shown impressive progress in recent years.\nHowever, existing models are largely limited to turn-based interactions, where\neach turn must be stepped (i.e., prompted) by the user. Open-ended,\nasynchronous interactions, where an AI model may proactively deliver timely\nresponses or feedback based on the unfolding situation in real-time, are an\nopen challenge. In this work, we present the QEVD benchmark and dataset, which\nexplores human-AI interaction in the challenging, yet controlled, real-world\ndomain of fitness coaching -- a task which intrinsically requires monitoring\nlive user activity and providing immediate feedback. The benchmark requires\nvision-language models to recognize complex human actions, identify possible\nmistakes, and provide appropriate feedback in real-time. Our experiments reveal\nthe limitations of existing state-of-the-art vision-language models for such\nasynchronous situated interactions. Motivated by this, we propose a simple\nend-to-end streaming baseline that can respond asynchronously to human actions\nwith appropriate feedback at the appropriate time.\n", "link": "http://arxiv.org/abs/2407.08101v3", "date": "2024-12-23", "relevancy": 2.1528, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5377}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20to%20Say%20and%20When%20to%20Say%20it%3A%20Live%20Fitness%20Coaching%20as%20a%20Testbed%20for%0A%20%20Situated%20Interaction&body=Title%3A%20What%20to%20Say%20and%20When%20to%20Say%20it%3A%20Live%20Fitness%20Coaching%20as%20a%20Testbed%20for%0A%20%20Situated%20Interaction%0AAuthor%3A%20Sunny%20Panchal%20and%20Apratim%20Bhattacharyya%20and%20Guillaume%20Berger%20and%20Antoine%20Mercier%20and%20Cornelius%20Bohm%20and%20Florian%20Dietrichkeit%20and%20Reza%20Pourreza%20and%20Xuanlin%20Li%20and%20Pulkit%20Madan%20and%20Mingu%20Lee%20and%20Mark%20Todorovich%20and%20Ingo%20Bax%20and%20Roland%20Memisevic%0AAbstract%3A%20%20%20Vision-language%20models%20have%20shown%20impressive%20progress%20in%20recent%20years.%0AHowever%2C%20existing%20models%20are%20largely%20limited%20to%20turn-based%20interactions%2C%20where%0Aeach%20turn%20must%20be%20stepped%20%28i.e.%2C%20prompted%29%20by%20the%20user.%20Open-ended%2C%0Aasynchronous%20interactions%2C%20where%20an%20AI%20model%20may%20proactively%20deliver%20timely%0Aresponses%20or%20feedback%20based%20on%20the%20unfolding%20situation%20in%20real-time%2C%20are%20an%0Aopen%20challenge.%20In%20this%20work%2C%20we%20present%20the%20QEVD%20benchmark%20and%20dataset%2C%20which%0Aexplores%20human-AI%20interaction%20in%20the%20challenging%2C%20yet%20controlled%2C%20real-world%0Adomain%20of%20fitness%20coaching%20--%20a%20task%20which%20intrinsically%20requires%20monitoring%0Alive%20user%20activity%20and%20providing%20immediate%20feedback.%20The%20benchmark%20requires%0Avision-language%20models%20to%20recognize%20complex%20human%20actions%2C%20identify%20possible%0Amistakes%2C%20and%20provide%20appropriate%20feedback%20in%20real-time.%20Our%20experiments%20reveal%0Athe%20limitations%20of%20existing%20state-of-the-art%20vision-language%20models%20for%20such%0Aasynchronous%20situated%20interactions.%20Motivated%20by%20this%2C%20we%20propose%20a%20simple%0Aend-to-end%20streaming%20baseline%20that%20can%20respond%20asynchronously%20to%20human%20actions%0Awith%20appropriate%20feedback%20at%20the%20appropriate%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08101v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520to%2520Say%2520and%2520When%2520to%2520Say%2520it%253A%2520Live%2520Fitness%2520Coaching%2520as%2520a%2520Testbed%2520for%250A%2520%2520Situated%2520Interaction%26entry.906535625%3DSunny%2520Panchal%2520and%2520Apratim%2520Bhattacharyya%2520and%2520Guillaume%2520Berger%2520and%2520Antoine%2520Mercier%2520and%2520Cornelius%2520Bohm%2520and%2520Florian%2520Dietrichkeit%2520and%2520Reza%2520Pourreza%2520and%2520Xuanlin%2520Li%2520and%2520Pulkit%2520Madan%2520and%2520Mingu%2520Lee%2520and%2520Mark%2520Todorovich%2520and%2520Ingo%2520Bax%2520and%2520Roland%2520Memisevic%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520have%2520shown%2520impressive%2520progress%2520in%2520recent%2520years.%250AHowever%252C%2520existing%2520models%2520are%2520largely%2520limited%2520to%2520turn-based%2520interactions%252C%2520where%250Aeach%2520turn%2520must%2520be%2520stepped%2520%2528i.e.%252C%2520prompted%2529%2520by%2520the%2520user.%2520Open-ended%252C%250Aasynchronous%2520interactions%252C%2520where%2520an%2520AI%2520model%2520may%2520proactively%2520deliver%2520timely%250Aresponses%2520or%2520feedback%2520based%2520on%2520the%2520unfolding%2520situation%2520in%2520real-time%252C%2520are%2520an%250Aopen%2520challenge.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520QEVD%2520benchmark%2520and%2520dataset%252C%2520which%250Aexplores%2520human-AI%2520interaction%2520in%2520the%2520challenging%252C%2520yet%2520controlled%252C%2520real-world%250Adomain%2520of%2520fitness%2520coaching%2520--%2520a%2520task%2520which%2520intrinsically%2520requires%2520monitoring%250Alive%2520user%2520activity%2520and%2520providing%2520immediate%2520feedback.%2520The%2520benchmark%2520requires%250Avision-language%2520models%2520to%2520recognize%2520complex%2520human%2520actions%252C%2520identify%2520possible%250Amistakes%252C%2520and%2520provide%2520appropriate%2520feedback%2520in%2520real-time.%2520Our%2520experiments%2520reveal%250Athe%2520limitations%2520of%2520existing%2520state-of-the-art%2520vision-language%2520models%2520for%2520such%250Aasynchronous%2520situated%2520interactions.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520simple%250Aend-to-end%2520streaming%2520baseline%2520that%2520can%2520respond%2520asynchronously%2520to%2520human%2520actions%250Awith%2520appropriate%2520feedback%2520at%2520the%2520appropriate%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08101v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20to%20Say%20and%20When%20to%20Say%20it%3A%20Live%20Fitness%20Coaching%20as%20a%20Testbed%20for%0A%20%20Situated%20Interaction&entry.906535625=Sunny%20Panchal%20and%20Apratim%20Bhattacharyya%20and%20Guillaume%20Berger%20and%20Antoine%20Mercier%20and%20Cornelius%20Bohm%20and%20Florian%20Dietrichkeit%20and%20Reza%20Pourreza%20and%20Xuanlin%20Li%20and%20Pulkit%20Madan%20and%20Mingu%20Lee%20and%20Mark%20Todorovich%20and%20Ingo%20Bax%20and%20Roland%20Memisevic&entry.1292438233=%20%20Vision-language%20models%20have%20shown%20impressive%20progress%20in%20recent%20years.%0AHowever%2C%20existing%20models%20are%20largely%20limited%20to%20turn-based%20interactions%2C%20where%0Aeach%20turn%20must%20be%20stepped%20%28i.e.%2C%20prompted%29%20by%20the%20user.%20Open-ended%2C%0Aasynchronous%20interactions%2C%20where%20an%20AI%20model%20may%20proactively%20deliver%20timely%0Aresponses%20or%20feedback%20based%20on%20the%20unfolding%20situation%20in%20real-time%2C%20are%20an%0Aopen%20challenge.%20In%20this%20work%2C%20we%20present%20the%20QEVD%20benchmark%20and%20dataset%2C%20which%0Aexplores%20human-AI%20interaction%20in%20the%20challenging%2C%20yet%20controlled%2C%20real-world%0Adomain%20of%20fitness%20coaching%20--%20a%20task%20which%20intrinsically%20requires%20monitoring%0Alive%20user%20activity%20and%20providing%20immediate%20feedback.%20The%20benchmark%20requires%0Avision-language%20models%20to%20recognize%20complex%20human%20actions%2C%20identify%20possible%0Amistakes%2C%20and%20provide%20appropriate%20feedback%20in%20real-time.%20Our%20experiments%20reveal%0Athe%20limitations%20of%20existing%20state-of-the-art%20vision-language%20models%20for%20such%0Aasynchronous%20situated%20interactions.%20Motivated%20by%20this%2C%20we%20propose%20a%20simple%0Aend-to-end%20streaming%20baseline%20that%20can%20respond%20asynchronously%20to%20human%20actions%0Awith%20appropriate%20feedback%20at%20the%20appropriate%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08101v3&entry.124074799=Read"},
{"title": "WavePlanes: Compact Hex Planes for Dynamic Novel View Synthesis", "author": "Adrian Azzarelli and Nantheera Anantrasirichai and David R Bull", "abstract": "  Dynamic Novel View Synthesis (Dynamic NVS) enhances NVS technologies to model\nmoving 3-D scenes. However, current methods are resource intensive and\nchallenging to compress. To address this, we present WavePlanes, a fast and\nmore compact hex plane representation, applicable to both Neural Radiance\nFields and Gaussian Splatting methods. Rather than modeling many feature scales\nseparately (as done previously), we use the inverse discrete wavelet transform\nto reconstruct features at varying scales. This leads to a more compact\nrepresentation and allows us to explore wavelet-based compression schemes for\nfurther gains. The proposed compression scheme exploits the sparsity of wavelet\ncoefficients, by applying hard thresholding to the wavelet planes and storing\nnonzero coefficients and their locations on each plane in a Hash Map. Compared\nto the state-of-the-art (SotA), WavePlanes is significantly smaller, less\nresource demanding and competitive in reconstruction quality. Compared to small\nSotA models, WavePlanes outperforms methods in both model size and quality of\nnovel views.\n", "link": "http://arxiv.org/abs/2312.02218v4", "date": "2024-12-23", "relevancy": 2.1501, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5491}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5325}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WavePlanes%3A%20Compact%20Hex%20Planes%20for%20Dynamic%20Novel%20View%20Synthesis&body=Title%3A%20WavePlanes%3A%20Compact%20Hex%20Planes%20for%20Dynamic%20Novel%20View%20Synthesis%0AAuthor%3A%20Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull%0AAbstract%3A%20%20%20Dynamic%20Novel%20View%20Synthesis%20%28Dynamic%20NVS%29%20enhances%20NVS%20technologies%20to%20model%0Amoving%203-D%20scenes.%20However%2C%20current%20methods%20are%20resource%20intensive%20and%0Achallenging%20to%20compress.%20To%20address%20this%2C%20we%20present%20WavePlanes%2C%20a%20fast%20and%0Amore%20compact%20hex%20plane%20representation%2C%20applicable%20to%20both%20Neural%20Radiance%0AFields%20and%20Gaussian%20Splatting%20methods.%20Rather%20than%20modeling%20many%20feature%20scales%0Aseparately%20%28as%20done%20previously%29%2C%20we%20use%20the%20inverse%20discrete%20wavelet%20transform%0Ato%20reconstruct%20features%20at%20varying%20scales.%20This%20leads%20to%20a%20more%20compact%0Arepresentation%20and%20allows%20us%20to%20explore%20wavelet-based%20compression%20schemes%20for%0Afurther%20gains.%20The%20proposed%20compression%20scheme%20exploits%20the%20sparsity%20of%20wavelet%0Acoefficients%2C%20by%20applying%20hard%20thresholding%20to%20the%20wavelet%20planes%20and%20storing%0Anonzero%20coefficients%20and%20their%20locations%20on%20each%20plane%20in%20a%20Hash%20Map.%20Compared%0Ato%20the%20state-of-the-art%20%28SotA%29%2C%20WavePlanes%20is%20significantly%20smaller%2C%20less%0Aresource%20demanding%20and%20competitive%20in%20reconstruction%20quality.%20Compared%20to%20small%0ASotA%20models%2C%20WavePlanes%20outperforms%20methods%20in%20both%20model%20size%20and%20quality%20of%0Anovel%20views.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02218v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavePlanes%253A%2520Compact%2520Hex%2520Planes%2520for%2520Dynamic%2520Novel%2520View%2520Synthesis%26entry.906535625%3DAdrian%2520Azzarelli%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520David%2520R%2520Bull%26entry.1292438233%3D%2520%2520Dynamic%2520Novel%2520View%2520Synthesis%2520%2528Dynamic%2520NVS%2529%2520enhances%2520NVS%2520technologies%2520to%2520model%250Amoving%25203-D%2520scenes.%2520However%252C%2520current%2520methods%2520are%2520resource%2520intensive%2520and%250Achallenging%2520to%2520compress.%2520To%2520address%2520this%252C%2520we%2520present%2520WavePlanes%252C%2520a%2520fast%2520and%250Amore%2520compact%2520hex%2520plane%2520representation%252C%2520applicable%2520to%2520both%2520Neural%2520Radiance%250AFields%2520and%2520Gaussian%2520Splatting%2520methods.%2520Rather%2520than%2520modeling%2520many%2520feature%2520scales%250Aseparately%2520%2528as%2520done%2520previously%2529%252C%2520we%2520use%2520the%2520inverse%2520discrete%2520wavelet%2520transform%250Ato%2520reconstruct%2520features%2520at%2520varying%2520scales.%2520This%2520leads%2520to%2520a%2520more%2520compact%250Arepresentation%2520and%2520allows%2520us%2520to%2520explore%2520wavelet-based%2520compression%2520schemes%2520for%250Afurther%2520gains.%2520The%2520proposed%2520compression%2520scheme%2520exploits%2520the%2520sparsity%2520of%2520wavelet%250Acoefficients%252C%2520by%2520applying%2520hard%2520thresholding%2520to%2520the%2520wavelet%2520planes%2520and%2520storing%250Anonzero%2520coefficients%2520and%2520their%2520locations%2520on%2520each%2520plane%2520in%2520a%2520Hash%2520Map.%2520Compared%250Ato%2520the%2520state-of-the-art%2520%2528SotA%2529%252C%2520WavePlanes%2520is%2520significantly%2520smaller%252C%2520less%250Aresource%2520demanding%2520and%2520competitive%2520in%2520reconstruction%2520quality.%2520Compared%2520to%2520small%250ASotA%2520models%252C%2520WavePlanes%2520outperforms%2520methods%2520in%2520both%2520model%2520size%2520and%2520quality%2520of%250Anovel%2520views.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02218v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WavePlanes%3A%20Compact%20Hex%20Planes%20for%20Dynamic%20Novel%20View%20Synthesis&entry.906535625=Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull&entry.1292438233=%20%20Dynamic%20Novel%20View%20Synthesis%20%28Dynamic%20NVS%29%20enhances%20NVS%20technologies%20to%20model%0Amoving%203-D%20scenes.%20However%2C%20current%20methods%20are%20resource%20intensive%20and%0Achallenging%20to%20compress.%20To%20address%20this%2C%20we%20present%20WavePlanes%2C%20a%20fast%20and%0Amore%20compact%20hex%20plane%20representation%2C%20applicable%20to%20both%20Neural%20Radiance%0AFields%20and%20Gaussian%20Splatting%20methods.%20Rather%20than%20modeling%20many%20feature%20scales%0Aseparately%20%28as%20done%20previously%29%2C%20we%20use%20the%20inverse%20discrete%20wavelet%20transform%0Ato%20reconstruct%20features%20at%20varying%20scales.%20This%20leads%20to%20a%20more%20compact%0Arepresentation%20and%20allows%20us%20to%20explore%20wavelet-based%20compression%20schemes%20for%0Afurther%20gains.%20The%20proposed%20compression%20scheme%20exploits%20the%20sparsity%20of%20wavelet%0Acoefficients%2C%20by%20applying%20hard%20thresholding%20to%20the%20wavelet%20planes%20and%20storing%0Anonzero%20coefficients%20and%20their%20locations%20on%20each%20plane%20in%20a%20Hash%20Map.%20Compared%0Ato%20the%20state-of-the-art%20%28SotA%29%2C%20WavePlanes%20is%20significantly%20smaller%2C%20less%0Aresource%20demanding%20and%20competitive%20in%20reconstruction%20quality.%20Compared%20to%20small%0ASotA%20models%2C%20WavePlanes%20outperforms%20methods%20in%20both%20model%20size%20and%20quality%20of%0Anovel%20views.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02218v4&entry.124074799=Read"},
{"title": "Editing Implicit and Explicit Representations of Radiance Fields: A\n  Survey", "author": "Arthur Hubert and Gamal Elghazaly and Raphael Frank", "abstract": "  Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent\nyears by offering a new volumetric representation, which is compact and\nprovides high-quality image rendering. However, the methods to edit those\nradiance fields developed slower than the many improvements to other aspects of\nNeRF. With the recent development of alternative radiance field-based\nrepresentations inspired by NeRF as well as the worldwide rise in popularity of\ntext-to-image models, many new opportunities and strategies have emerged to\nprovide radiance field editing. In this paper, we deliver a comprehensive\nsurvey of the different editing methods present in the literature for NeRF and\nother similar radiance field representations. We propose a new taxonomy for\nclassifying existing works based on their editing methodologies, review\npioneering models, reflect on current and potential new applications of\nradiance field editing, and compare state-of-the-art approaches in terms of\nediting options and performance.\n", "link": "http://arxiv.org/abs/2412.17628v1", "date": "2024-12-23", "relevancy": 2.1456, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5736}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Editing%20Implicit%20and%20Explicit%20Representations%20of%20Radiance%20Fields%3A%20A%0A%20%20Survey&body=Title%3A%20Editing%20Implicit%20and%20Explicit%20Representations%20of%20Radiance%20Fields%3A%20A%0A%20%20Survey%0AAuthor%3A%20Arthur%20Hubert%20and%20Gamal%20Elghazaly%20and%20Raphael%20Frank%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20revolutionized%20novel%20view%20synthesis%20in%20recent%0Ayears%20by%20offering%20a%20new%20volumetric%20representation%2C%20which%20is%20compact%20and%0Aprovides%20high-quality%20image%20rendering.%20However%2C%20the%20methods%20to%20edit%20those%0Aradiance%20fields%20developed%20slower%20than%20the%20many%20improvements%20to%20other%20aspects%20of%0ANeRF.%20With%20the%20recent%20development%20of%20alternative%20radiance%20field-based%0Arepresentations%20inspired%20by%20NeRF%20as%20well%20as%20the%20worldwide%20rise%20in%20popularity%20of%0Atext-to-image%20models%2C%20many%20new%20opportunities%20and%20strategies%20have%20emerged%20to%0Aprovide%20radiance%20field%20editing.%20In%20this%20paper%2C%20we%20deliver%20a%20comprehensive%0Asurvey%20of%20the%20different%20editing%20methods%20present%20in%20the%20literature%20for%20NeRF%20and%0Aother%20similar%20radiance%20field%20representations.%20We%20propose%20a%20new%20taxonomy%20for%0Aclassifying%20existing%20works%20based%20on%20their%20editing%20methodologies%2C%20review%0Apioneering%20models%2C%20reflect%20on%20current%20and%20potential%20new%20applications%20of%0Aradiance%20field%20editing%2C%20and%20compare%20state-of-the-art%20approaches%20in%20terms%20of%0Aediting%20options%20and%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEditing%2520Implicit%2520and%2520Explicit%2520Representations%2520of%2520Radiance%2520Fields%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DArthur%2520Hubert%2520and%2520Gamal%2520Elghazaly%2520and%2520Raphael%2520Frank%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520revolutionized%2520novel%2520view%2520synthesis%2520in%2520recent%250Ayears%2520by%2520offering%2520a%2520new%2520volumetric%2520representation%252C%2520which%2520is%2520compact%2520and%250Aprovides%2520high-quality%2520image%2520rendering.%2520However%252C%2520the%2520methods%2520to%2520edit%2520those%250Aradiance%2520fields%2520developed%2520slower%2520than%2520the%2520many%2520improvements%2520to%2520other%2520aspects%2520of%250ANeRF.%2520With%2520the%2520recent%2520development%2520of%2520alternative%2520radiance%2520field-based%250Arepresentations%2520inspired%2520by%2520NeRF%2520as%2520well%2520as%2520the%2520worldwide%2520rise%2520in%2520popularity%2520of%250Atext-to-image%2520models%252C%2520many%2520new%2520opportunities%2520and%2520strategies%2520have%2520emerged%2520to%250Aprovide%2520radiance%2520field%2520editing.%2520In%2520this%2520paper%252C%2520we%2520deliver%2520a%2520comprehensive%250Asurvey%2520of%2520the%2520different%2520editing%2520methods%2520present%2520in%2520the%2520literature%2520for%2520NeRF%2520and%250Aother%2520similar%2520radiance%2520field%2520representations.%2520We%2520propose%2520a%2520new%2520taxonomy%2520for%250Aclassifying%2520existing%2520works%2520based%2520on%2520their%2520editing%2520methodologies%252C%2520review%250Apioneering%2520models%252C%2520reflect%2520on%2520current%2520and%2520potential%2520new%2520applications%2520of%250Aradiance%2520field%2520editing%252C%2520and%2520compare%2520state-of-the-art%2520approaches%2520in%2520terms%2520of%250Aediting%2520options%2520and%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Editing%20Implicit%20and%20Explicit%20Representations%20of%20Radiance%20Fields%3A%20A%0A%20%20Survey&entry.906535625=Arthur%20Hubert%20and%20Gamal%20Elghazaly%20and%20Raphael%20Frank&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20revolutionized%20novel%20view%20synthesis%20in%20recent%0Ayears%20by%20offering%20a%20new%20volumetric%20representation%2C%20which%20is%20compact%20and%0Aprovides%20high-quality%20image%20rendering.%20However%2C%20the%20methods%20to%20edit%20those%0Aradiance%20fields%20developed%20slower%20than%20the%20many%20improvements%20to%20other%20aspects%20of%0ANeRF.%20With%20the%20recent%20development%20of%20alternative%20radiance%20field-based%0Arepresentations%20inspired%20by%20NeRF%20as%20well%20as%20the%20worldwide%20rise%20in%20popularity%20of%0Atext-to-image%20models%2C%20many%20new%20opportunities%20and%20strategies%20have%20emerged%20to%0Aprovide%20radiance%20field%20editing.%20In%20this%20paper%2C%20we%20deliver%20a%20comprehensive%0Asurvey%20of%20the%20different%20editing%20methods%20present%20in%20the%20literature%20for%20NeRF%20and%0Aother%20similar%20radiance%20field%20representations.%20We%20propose%20a%20new%20taxonomy%20for%0Aclassifying%20existing%20works%20based%20on%20their%20editing%20methodologies%2C%20review%0Apioneering%20models%2C%20reflect%20on%20current%20and%20potential%20new%20applications%20of%0Aradiance%20field%20editing%2C%20and%20compare%20state-of-the-art%20approaches%20in%20terms%20of%0Aediting%20options%20and%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17628v1&entry.124074799=Read"},
{"title": "Developmental Predictive Coding Model for Early Infancy Mono and\n  Bilingual Vocal Continual Learning", "author": "Xiaodan Chen and Alexandre Pitti and Mathias Quoy and Nancy F Chen", "abstract": "  Understanding how infants perceive speech sounds and language structures is\nstill an open problem. Previous research in artificial neural networks has\nmainly focused on large dataset-dependent generative models, aiming to\nreplicate language-related phenomena such as ''perceptual narrowing''. In this\npaper, we propose a novel approach using a small-sized generative neural\nnetwork equipped with a continual learning mechanism based on predictive coding\nfor mono-and bilingual speech sound learning (referred to as language sound\nacquisition during ''critical period'') and a compositional optimization\nmechanism for generation where no learning is involved (later infancy sound\nimitation). Our model prioritizes interpretability and demonstrates the\nadvantages of online learning: Unlike deep networks requiring substantial\noffline training, our model continuously updates with new data, making it\nadaptable and responsive to changing inputs. Through experiments, we\ndemonstrate that if second language acquisition occurs during later infancy,\nthe challenges associated with learning a foreign language after the critical\nperiod amplify, replicating the perceptual narrowing effect.\n", "link": "http://arxiv.org/abs/2412.17456v1", "date": "2024-12-23", "relevancy": 2.1415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Developmental%20Predictive%20Coding%20Model%20for%20Early%20Infancy%20Mono%20and%0A%20%20Bilingual%20Vocal%20Continual%20Learning&body=Title%3A%20Developmental%20Predictive%20Coding%20Model%20for%20Early%20Infancy%20Mono%20and%0A%20%20Bilingual%20Vocal%20Continual%20Learning%0AAuthor%3A%20Xiaodan%20Chen%20and%20Alexandre%20Pitti%20and%20Mathias%20Quoy%20and%20Nancy%20F%20Chen%0AAbstract%3A%20%20%20Understanding%20how%20infants%20perceive%20speech%20sounds%20and%20language%20structures%20is%0Astill%20an%20open%20problem.%20Previous%20research%20in%20artificial%20neural%20networks%20has%0Amainly%20focused%20on%20large%20dataset-dependent%20generative%20models%2C%20aiming%20to%0Areplicate%20language-related%20phenomena%20such%20as%20%27%27perceptual%20narrowing%27%27.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20approach%20using%20a%20small-sized%20generative%20neural%0Anetwork%20equipped%20with%20a%20continual%20learning%20mechanism%20based%20on%20predictive%20coding%0Afor%20mono-and%20bilingual%20speech%20sound%20learning%20%28referred%20to%20as%20language%20sound%0Aacquisition%20during%20%27%27critical%20period%27%27%29%20and%20a%20compositional%20optimization%0Amechanism%20for%20generation%20where%20no%20learning%20is%20involved%20%28later%20infancy%20sound%0Aimitation%29.%20Our%20model%20prioritizes%20interpretability%20and%20demonstrates%20the%0Aadvantages%20of%20online%20learning%3A%20Unlike%20deep%20networks%20requiring%20substantial%0Aoffline%20training%2C%20our%20model%20continuously%20updates%20with%20new%20data%2C%20making%20it%0Aadaptable%20and%20responsive%20to%20changing%20inputs.%20Through%20experiments%2C%20we%0Ademonstrate%20that%20if%20second%20language%20acquisition%20occurs%20during%20later%20infancy%2C%0Athe%20challenges%20associated%20with%20learning%20a%20foreign%20language%20after%20the%20critical%0Aperiod%20amplify%2C%20replicating%20the%20perceptual%20narrowing%20effect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevelopmental%2520Predictive%2520Coding%2520Model%2520for%2520Early%2520Infancy%2520Mono%2520and%250A%2520%2520Bilingual%2520Vocal%2520Continual%2520Learning%26entry.906535625%3DXiaodan%2520Chen%2520and%2520Alexandre%2520Pitti%2520and%2520Mathias%2520Quoy%2520and%2520Nancy%2520F%2520Chen%26entry.1292438233%3D%2520%2520Understanding%2520how%2520infants%2520perceive%2520speech%2520sounds%2520and%2520language%2520structures%2520is%250Astill%2520an%2520open%2520problem.%2520Previous%2520research%2520in%2520artificial%2520neural%2520networks%2520has%250Amainly%2520focused%2520on%2520large%2520dataset-dependent%2520generative%2520models%252C%2520aiming%2520to%250Areplicate%2520language-related%2520phenomena%2520such%2520as%2520%2527%2527perceptual%2520narrowing%2527%2527.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520using%2520a%2520small-sized%2520generative%2520neural%250Anetwork%2520equipped%2520with%2520a%2520continual%2520learning%2520mechanism%2520based%2520on%2520predictive%2520coding%250Afor%2520mono-and%2520bilingual%2520speech%2520sound%2520learning%2520%2528referred%2520to%2520as%2520language%2520sound%250Aacquisition%2520during%2520%2527%2527critical%2520period%2527%2527%2529%2520and%2520a%2520compositional%2520optimization%250Amechanism%2520for%2520generation%2520where%2520no%2520learning%2520is%2520involved%2520%2528later%2520infancy%2520sound%250Aimitation%2529.%2520Our%2520model%2520prioritizes%2520interpretability%2520and%2520demonstrates%2520the%250Aadvantages%2520of%2520online%2520learning%253A%2520Unlike%2520deep%2520networks%2520requiring%2520substantial%250Aoffline%2520training%252C%2520our%2520model%2520continuously%2520updates%2520with%2520new%2520data%252C%2520making%2520it%250Aadaptable%2520and%2520responsive%2520to%2520changing%2520inputs.%2520Through%2520experiments%252C%2520we%250Ademonstrate%2520that%2520if%2520second%2520language%2520acquisition%2520occurs%2520during%2520later%2520infancy%252C%250Athe%2520challenges%2520associated%2520with%2520learning%2520a%2520foreign%2520language%2520after%2520the%2520critical%250Aperiod%2520amplify%252C%2520replicating%2520the%2520perceptual%2520narrowing%2520effect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Developmental%20Predictive%20Coding%20Model%20for%20Early%20Infancy%20Mono%20and%0A%20%20Bilingual%20Vocal%20Continual%20Learning&entry.906535625=Xiaodan%20Chen%20and%20Alexandre%20Pitti%20and%20Mathias%20Quoy%20and%20Nancy%20F%20Chen&entry.1292438233=%20%20Understanding%20how%20infants%20perceive%20speech%20sounds%20and%20language%20structures%20is%0Astill%20an%20open%20problem.%20Previous%20research%20in%20artificial%20neural%20networks%20has%0Amainly%20focused%20on%20large%20dataset-dependent%20generative%20models%2C%20aiming%20to%0Areplicate%20language-related%20phenomena%20such%20as%20%27%27perceptual%20narrowing%27%27.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20approach%20using%20a%20small-sized%20generative%20neural%0Anetwork%20equipped%20with%20a%20continual%20learning%20mechanism%20based%20on%20predictive%20coding%0Afor%20mono-and%20bilingual%20speech%20sound%20learning%20%28referred%20to%20as%20language%20sound%0Aacquisition%20during%20%27%27critical%20period%27%27%29%20and%20a%20compositional%20optimization%0Amechanism%20for%20generation%20where%20no%20learning%20is%20involved%20%28later%20infancy%20sound%0Aimitation%29.%20Our%20model%20prioritizes%20interpretability%20and%20demonstrates%20the%0Aadvantages%20of%20online%20learning%3A%20Unlike%20deep%20networks%20requiring%20substantial%0Aoffline%20training%2C%20our%20model%20continuously%20updates%20with%20new%20data%2C%20making%20it%0Aadaptable%20and%20responsive%20to%20changing%20inputs.%20Through%20experiments%2C%20we%0Ademonstrate%20that%20if%20second%20language%20acquisition%20occurs%20during%20later%20infancy%2C%0Athe%20challenges%20associated%20with%20learning%20a%20foreign%20language%20after%20the%20critical%0Aperiod%20amplify%2C%20replicating%20the%20perceptual%20narrowing%20effect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17456v1&entry.124074799=Read"},
{"title": "CALLIC: Content Adaptive Learning for Lossless Image Compression", "author": "Daxin Li and Yuanchao Bai and Kai Wang and Junjun Jiang and Xianming Liu and Wen Gao", "abstract": "  Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.\n", "link": "http://arxiv.org/abs/2412.17464v1", "date": "2024-12-23", "relevancy": 2.1383, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5603}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5165}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CALLIC%3A%20Content%20Adaptive%20Learning%20for%20Lossless%20Image%20Compression&body=Title%3A%20CALLIC%3A%20Content%20Adaptive%20Learning%20for%20Lossless%20Image%20Compression%0AAuthor%3A%20Daxin%20Li%20and%20Yuanchao%20Bai%20and%20Kai%20Wang%20and%20Junjun%20Jiang%20and%20Xianming%20Liu%20and%20Wen%20Gao%0AAbstract%3A%20%20%20Learned%20lossless%20image%20compression%20has%20achieved%20significant%20advancements%20in%0Arecent%20years.%20However%2C%20existing%20methods%20often%20rely%20on%20training%20amortized%0Agenerative%20models%20on%20massive%20datasets%2C%20resulting%20in%20sub-optimal%20probability%0Adistribution%20estimation%20for%20specific%20testing%20images%20during%20encoding%20process.%20To%0Aaddress%20this%20challenge%2C%20we%20explore%20the%20connection%20between%20the%20Minimum%0ADescription%20Length%20%28MDL%29%20principle%20and%20Parameter-Efficient%20Transfer%20Learning%0A%28PETL%29%2C%20leading%20to%20the%20development%20of%20a%20novel%20content-adaptive%20approach%20for%0Alearned%20lossless%20image%20compression%2C%20dubbed%20CALLIC.%20Specifically%2C%20we%20first%0Apropose%20a%20content-aware%20autoregressive%20self-attention%20mechanism%20by%20leveraging%0Aconvolutional%20gating%20operations%2C%20termed%20Masked%20Gated%20ConvFormer%20%28MGCF%29%2C%20and%0Apretrain%20MGCF%20on%20training%20dataset.%20Cache%20then%20Crop%20Inference%20%28CCI%29%20is%20proposed%0Ato%20accelerate%20the%20coding%20process.%20During%20encoding%2C%20we%20decompose%20pre-trained%0Alayers%2C%20including%20depth-wise%20convolutions%2C%20using%20low-rank%20matrices%20and%20then%0Aadapt%20the%20incremental%20weights%20on%20testing%20image%20by%20Rate-guided%20Progressive%0AFine-Tuning%20%28RPFT%29.%20RPFT%20fine-tunes%20with%20gradually%20increasing%20patches%20that%20are%0Asorted%20in%20descending%20order%20by%20estimated%20entropy%2C%20optimizing%20learning%20process%0Aand%20reducing%20adaptation%20time.%20Extensive%20experiments%20across%20diverse%20datasets%0Ademonstrate%20that%20CALLIC%20sets%20a%20new%20state-of-the-art%20%28SOTA%29%20for%20learned%20lossless%0Aimage%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCALLIC%253A%2520Content%2520Adaptive%2520Learning%2520for%2520Lossless%2520Image%2520Compression%26entry.906535625%3DDaxin%2520Li%2520and%2520Yuanchao%2520Bai%2520and%2520Kai%2520Wang%2520and%2520Junjun%2520Jiang%2520and%2520Xianming%2520Liu%2520and%2520Wen%2520Gao%26entry.1292438233%3D%2520%2520Learned%2520lossless%2520image%2520compression%2520has%2520achieved%2520significant%2520advancements%2520in%250Arecent%2520years.%2520However%252C%2520existing%2520methods%2520often%2520rely%2520on%2520training%2520amortized%250Agenerative%2520models%2520on%2520massive%2520datasets%252C%2520resulting%2520in%2520sub-optimal%2520probability%250Adistribution%2520estimation%2520for%2520specific%2520testing%2520images%2520during%2520encoding%2520process.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520explore%2520the%2520connection%2520between%2520the%2520Minimum%250ADescription%2520Length%2520%2528MDL%2529%2520principle%2520and%2520Parameter-Efficient%2520Transfer%2520Learning%250A%2528PETL%2529%252C%2520leading%2520to%2520the%2520development%2520of%2520a%2520novel%2520content-adaptive%2520approach%2520for%250Alearned%2520lossless%2520image%2520compression%252C%2520dubbed%2520CALLIC.%2520Specifically%252C%2520we%2520first%250Apropose%2520a%2520content-aware%2520autoregressive%2520self-attention%2520mechanism%2520by%2520leveraging%250Aconvolutional%2520gating%2520operations%252C%2520termed%2520Masked%2520Gated%2520ConvFormer%2520%2528MGCF%2529%252C%2520and%250Apretrain%2520MGCF%2520on%2520training%2520dataset.%2520Cache%2520then%2520Crop%2520Inference%2520%2528CCI%2529%2520is%2520proposed%250Ato%2520accelerate%2520the%2520coding%2520process.%2520During%2520encoding%252C%2520we%2520decompose%2520pre-trained%250Alayers%252C%2520including%2520depth-wise%2520convolutions%252C%2520using%2520low-rank%2520matrices%2520and%2520then%250Aadapt%2520the%2520incremental%2520weights%2520on%2520testing%2520image%2520by%2520Rate-guided%2520Progressive%250AFine-Tuning%2520%2528RPFT%2529.%2520RPFT%2520fine-tunes%2520with%2520gradually%2520increasing%2520patches%2520that%2520are%250Asorted%2520in%2520descending%2520order%2520by%2520estimated%2520entropy%252C%2520optimizing%2520learning%2520process%250Aand%2520reducing%2520adaptation%2520time.%2520Extensive%2520experiments%2520across%2520diverse%2520datasets%250Ademonstrate%2520that%2520CALLIC%2520sets%2520a%2520new%2520state-of-the-art%2520%2528SOTA%2529%2520for%2520learned%2520lossless%250Aimage%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CALLIC%3A%20Content%20Adaptive%20Learning%20for%20Lossless%20Image%20Compression&entry.906535625=Daxin%20Li%20and%20Yuanchao%20Bai%20and%20Kai%20Wang%20and%20Junjun%20Jiang%20and%20Xianming%20Liu%20and%20Wen%20Gao&entry.1292438233=%20%20Learned%20lossless%20image%20compression%20has%20achieved%20significant%20advancements%20in%0Arecent%20years.%20However%2C%20existing%20methods%20often%20rely%20on%20training%20amortized%0Agenerative%20models%20on%20massive%20datasets%2C%20resulting%20in%20sub-optimal%20probability%0Adistribution%20estimation%20for%20specific%20testing%20images%20during%20encoding%20process.%20To%0Aaddress%20this%20challenge%2C%20we%20explore%20the%20connection%20between%20the%20Minimum%0ADescription%20Length%20%28MDL%29%20principle%20and%20Parameter-Efficient%20Transfer%20Learning%0A%28PETL%29%2C%20leading%20to%20the%20development%20of%20a%20novel%20content-adaptive%20approach%20for%0Alearned%20lossless%20image%20compression%2C%20dubbed%20CALLIC.%20Specifically%2C%20we%20first%0Apropose%20a%20content-aware%20autoregressive%20self-attention%20mechanism%20by%20leveraging%0Aconvolutional%20gating%20operations%2C%20termed%20Masked%20Gated%20ConvFormer%20%28MGCF%29%2C%20and%0Apretrain%20MGCF%20on%20training%20dataset.%20Cache%20then%20Crop%20Inference%20%28CCI%29%20is%20proposed%0Ato%20accelerate%20the%20coding%20process.%20During%20encoding%2C%20we%20decompose%20pre-trained%0Alayers%2C%20including%20depth-wise%20convolutions%2C%20using%20low-rank%20matrices%20and%20then%0Aadapt%20the%20incremental%20weights%20on%20testing%20image%20by%20Rate-guided%20Progressive%0AFine-Tuning%20%28RPFT%29.%20RPFT%20fine-tunes%20with%20gradually%20increasing%20patches%20that%20are%0Asorted%20in%20descending%20order%20by%20estimated%20entropy%2C%20optimizing%20learning%20process%0Aand%20reducing%20adaptation%20time.%20Extensive%20experiments%20across%20diverse%20datasets%0Ademonstrate%20that%20CALLIC%20sets%20a%20new%20state-of-the-art%20%28SOTA%29%20for%20learned%20lossless%0Aimage%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17464v1&entry.124074799=Read"},
{"title": "NightHaze: Nighttime Image Dehazing via Self-Prior Learning", "author": "Beibei Lin and Yeying Jin and Wending Yan and Wei Ye and Yuan Yuan and Robby T. Tan", "abstract": "  Masked autoencoder (MAE) shows that severe augmentation during training\nproduces robust representations for high-level tasks. This paper brings the\nMAE-like framework to nighttime image enhancement, demonstrating that severe\naugmentation during training produces strong network priors that are resilient\nto real-world night haze degradations. We propose a novel nighttime image\ndehazing method with self-prior learning. Our main novelty lies in the design\nof severe augmentation, which allows our model to learn robust priors. Unlike\nMAE that uses masking, we leverage two key challenging factors of nighttime\nimages as augmentation: light effects and noise. During training, we\nintentionally degrade clear images by blending them with light effects as well\nas by adding noise, and subsequently restore the clear images. This enables our\nmodel to learn clear background priors. By increasing the noise values to\napproach as high as the pixel intensity values of the glow and light effect\nblended images, our augmentation becomes severe, resulting in stronger priors.\nWhile our self-prior learning is considerably effective in suppressing glow and\nrevealing details of background scenes, in some cases, there are still some\nundesired artifacts that remain, particularly in the forms of over-suppression.\nTo address these artifacts, we propose a self-refinement module based on the\nsemi-supervised teacher-student framework. Our NightHaze, especially our\nMAE-like self-prior learning, shows that models trained with severe\naugmentation effectively improve the visibility of input haze images,\napproaching the clarity of clear nighttime images. Extensive experiments\ndemonstrate that our NightHaze achieves state-of-the-art performance,\noutperforming existing nighttime image dehazing methods by a substantial margin\nof 15.5% for MUSIQ and 23.5% for ClipIQA.\n", "link": "http://arxiv.org/abs/2403.07408v2", "date": "2024-12-23", "relevancy": 2.1322, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5415}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5283}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NightHaze%3A%20Nighttime%20Image%20Dehazing%20via%20Self-Prior%20Learning&body=Title%3A%20NightHaze%3A%20Nighttime%20Image%20Dehazing%20via%20Self-Prior%20Learning%0AAuthor%3A%20Beibei%20Lin%20and%20Yeying%20Jin%20and%20Wending%20Yan%20and%20Wei%20Ye%20and%20Yuan%20Yuan%20and%20Robby%20T.%20Tan%0AAbstract%3A%20%20%20Masked%20autoencoder%20%28MAE%29%20shows%20that%20severe%20augmentation%20during%20training%0Aproduces%20robust%20representations%20for%20high-level%20tasks.%20This%20paper%20brings%20the%0AMAE-like%20framework%20to%20nighttime%20image%20enhancement%2C%20demonstrating%20that%20severe%0Aaugmentation%20during%20training%20produces%20strong%20network%20priors%20that%20are%20resilient%0Ato%20real-world%20night%20haze%20degradations.%20We%20propose%20a%20novel%20nighttime%20image%0Adehazing%20method%20with%20self-prior%20learning.%20Our%20main%20novelty%20lies%20in%20the%20design%0Aof%20severe%20augmentation%2C%20which%20allows%20our%20model%20to%20learn%20robust%20priors.%20Unlike%0AMAE%20that%20uses%20masking%2C%20we%20leverage%20two%20key%20challenging%20factors%20of%20nighttime%0Aimages%20as%20augmentation%3A%20light%20effects%20and%20noise.%20During%20training%2C%20we%0Aintentionally%20degrade%20clear%20images%20by%20blending%20them%20with%20light%20effects%20as%20well%0Aas%20by%20adding%20noise%2C%20and%20subsequently%20restore%20the%20clear%20images.%20This%20enables%20our%0Amodel%20to%20learn%20clear%20background%20priors.%20By%20increasing%20the%20noise%20values%20to%0Aapproach%20as%20high%20as%20the%20pixel%20intensity%20values%20of%20the%20glow%20and%20light%20effect%0Ablended%20images%2C%20our%20augmentation%20becomes%20severe%2C%20resulting%20in%20stronger%20priors.%0AWhile%20our%20self-prior%20learning%20is%20considerably%20effective%20in%20suppressing%20glow%20and%0Arevealing%20details%20of%20background%20scenes%2C%20in%20some%20cases%2C%20there%20are%20still%20some%0Aundesired%20artifacts%20that%20remain%2C%20particularly%20in%20the%20forms%20of%20over-suppression.%0ATo%20address%20these%20artifacts%2C%20we%20propose%20a%20self-refinement%20module%20based%20on%20the%0Asemi-supervised%20teacher-student%20framework.%20Our%20NightHaze%2C%20especially%20our%0AMAE-like%20self-prior%20learning%2C%20shows%20that%20models%20trained%20with%20severe%0Aaugmentation%20effectively%20improve%20the%20visibility%20of%20input%20haze%20images%2C%0Aapproaching%20the%20clarity%20of%20clear%20nighttime%20images.%20Extensive%20experiments%0Ademonstrate%20that%20our%20NightHaze%20achieves%20state-of-the-art%20performance%2C%0Aoutperforming%20existing%20nighttime%20image%20dehazing%20methods%20by%20a%20substantial%20margin%0Aof%2015.5%25%20for%20MUSIQ%20and%2023.5%25%20for%20ClipIQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNightHaze%253A%2520Nighttime%2520Image%2520Dehazing%2520via%2520Self-Prior%2520Learning%26entry.906535625%3DBeibei%2520Lin%2520and%2520Yeying%2520Jin%2520and%2520Wending%2520Yan%2520and%2520Wei%2520Ye%2520and%2520Yuan%2520Yuan%2520and%2520Robby%2520T.%2520Tan%26entry.1292438233%3D%2520%2520Masked%2520autoencoder%2520%2528MAE%2529%2520shows%2520that%2520severe%2520augmentation%2520during%2520training%250Aproduces%2520robust%2520representations%2520for%2520high-level%2520tasks.%2520This%2520paper%2520brings%2520the%250AMAE-like%2520framework%2520to%2520nighttime%2520image%2520enhancement%252C%2520demonstrating%2520that%2520severe%250Aaugmentation%2520during%2520training%2520produces%2520strong%2520network%2520priors%2520that%2520are%2520resilient%250Ato%2520real-world%2520night%2520haze%2520degradations.%2520We%2520propose%2520a%2520novel%2520nighttime%2520image%250Adehazing%2520method%2520with%2520self-prior%2520learning.%2520Our%2520main%2520novelty%2520lies%2520in%2520the%2520design%250Aof%2520severe%2520augmentation%252C%2520which%2520allows%2520our%2520model%2520to%2520learn%2520robust%2520priors.%2520Unlike%250AMAE%2520that%2520uses%2520masking%252C%2520we%2520leverage%2520two%2520key%2520challenging%2520factors%2520of%2520nighttime%250Aimages%2520as%2520augmentation%253A%2520light%2520effects%2520and%2520noise.%2520During%2520training%252C%2520we%250Aintentionally%2520degrade%2520clear%2520images%2520by%2520blending%2520them%2520with%2520light%2520effects%2520as%2520well%250Aas%2520by%2520adding%2520noise%252C%2520and%2520subsequently%2520restore%2520the%2520clear%2520images.%2520This%2520enables%2520our%250Amodel%2520to%2520learn%2520clear%2520background%2520priors.%2520By%2520increasing%2520the%2520noise%2520values%2520to%250Aapproach%2520as%2520high%2520as%2520the%2520pixel%2520intensity%2520values%2520of%2520the%2520glow%2520and%2520light%2520effect%250Ablended%2520images%252C%2520our%2520augmentation%2520becomes%2520severe%252C%2520resulting%2520in%2520stronger%2520priors.%250AWhile%2520our%2520self-prior%2520learning%2520is%2520considerably%2520effective%2520in%2520suppressing%2520glow%2520and%250Arevealing%2520details%2520of%2520background%2520scenes%252C%2520in%2520some%2520cases%252C%2520there%2520are%2520still%2520some%250Aundesired%2520artifacts%2520that%2520remain%252C%2520particularly%2520in%2520the%2520forms%2520of%2520over-suppression.%250ATo%2520address%2520these%2520artifacts%252C%2520we%2520propose%2520a%2520self-refinement%2520module%2520based%2520on%2520the%250Asemi-supervised%2520teacher-student%2520framework.%2520Our%2520NightHaze%252C%2520especially%2520our%250AMAE-like%2520self-prior%2520learning%252C%2520shows%2520that%2520models%2520trained%2520with%2520severe%250Aaugmentation%2520effectively%2520improve%2520the%2520visibility%2520of%2520input%2520haze%2520images%252C%250Aapproaching%2520the%2520clarity%2520of%2520clear%2520nighttime%2520images.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520NightHaze%2520achieves%2520state-of-the-art%2520performance%252C%250Aoutperforming%2520existing%2520nighttime%2520image%2520dehazing%2520methods%2520by%2520a%2520substantial%2520margin%250Aof%252015.5%2525%2520for%2520MUSIQ%2520and%252023.5%2525%2520for%2520ClipIQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NightHaze%3A%20Nighttime%20Image%20Dehazing%20via%20Self-Prior%20Learning&entry.906535625=Beibei%20Lin%20and%20Yeying%20Jin%20and%20Wending%20Yan%20and%20Wei%20Ye%20and%20Yuan%20Yuan%20and%20Robby%20T.%20Tan&entry.1292438233=%20%20Masked%20autoencoder%20%28MAE%29%20shows%20that%20severe%20augmentation%20during%20training%0Aproduces%20robust%20representations%20for%20high-level%20tasks.%20This%20paper%20brings%20the%0AMAE-like%20framework%20to%20nighttime%20image%20enhancement%2C%20demonstrating%20that%20severe%0Aaugmentation%20during%20training%20produces%20strong%20network%20priors%20that%20are%20resilient%0Ato%20real-world%20night%20haze%20degradations.%20We%20propose%20a%20novel%20nighttime%20image%0Adehazing%20method%20with%20self-prior%20learning.%20Our%20main%20novelty%20lies%20in%20the%20design%0Aof%20severe%20augmentation%2C%20which%20allows%20our%20model%20to%20learn%20robust%20priors.%20Unlike%0AMAE%20that%20uses%20masking%2C%20we%20leverage%20two%20key%20challenging%20factors%20of%20nighttime%0Aimages%20as%20augmentation%3A%20light%20effects%20and%20noise.%20During%20training%2C%20we%0Aintentionally%20degrade%20clear%20images%20by%20blending%20them%20with%20light%20effects%20as%20well%0Aas%20by%20adding%20noise%2C%20and%20subsequently%20restore%20the%20clear%20images.%20This%20enables%20our%0Amodel%20to%20learn%20clear%20background%20priors.%20By%20increasing%20the%20noise%20values%20to%0Aapproach%20as%20high%20as%20the%20pixel%20intensity%20values%20of%20the%20glow%20and%20light%20effect%0Ablended%20images%2C%20our%20augmentation%20becomes%20severe%2C%20resulting%20in%20stronger%20priors.%0AWhile%20our%20self-prior%20learning%20is%20considerably%20effective%20in%20suppressing%20glow%20and%0Arevealing%20details%20of%20background%20scenes%2C%20in%20some%20cases%2C%20there%20are%20still%20some%0Aundesired%20artifacts%20that%20remain%2C%20particularly%20in%20the%20forms%20of%20over-suppression.%0ATo%20address%20these%20artifacts%2C%20we%20propose%20a%20self-refinement%20module%20based%20on%20the%0Asemi-supervised%20teacher-student%20framework.%20Our%20NightHaze%2C%20especially%20our%0AMAE-like%20self-prior%20learning%2C%20shows%20that%20models%20trained%20with%20severe%0Aaugmentation%20effectively%20improve%20the%20visibility%20of%20input%20haze%20images%2C%0Aapproaching%20the%20clarity%20of%20clear%20nighttime%20images.%20Extensive%20experiments%0Ademonstrate%20that%20our%20NightHaze%20achieves%20state-of-the-art%20performance%2C%0Aoutperforming%20existing%20nighttime%20image%20dehazing%20methods%20by%20a%20substantial%20margin%0Aof%2015.5%25%20for%20MUSIQ%20and%2023.5%25%20for%20ClipIQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07408v2&entry.124074799=Read"},
{"title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference", "author": "Chao Zeng and Songwei Liu and Shu Yang and Fangmin Chen and Xing Mei and Lean Fu", "abstract": "  With the rapid growth in the scale and complexity of large language models\n(LLMs), the costs of training and inference have risen substantially. Model\ncompression has emerged as a mainstream solution to reduce memory usage and\ncomputational overhead. This paper presents Group Quantization and Sparse\nAcceleration (\\textbf{GQSA}), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. The proposed method consists of three key steps. First, GQSA\napplies group structured pruning to adhere to GPU-friendly sparse pattern\nconstraints. Second, a two-stage sparsity-aware training process is employed to\nmaximize performance retention after compression. Finally, the framework adopts\nthe Block Sparse Row (BSR) format to enable practical deployment and efficient\nexecution. Experimental results on the LLaMA model family show that GQSA\nachieves an excellent balance between model speed and accuracy. Furthermore, on\nthe latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM\ncompression techniques significantly.\n", "link": "http://arxiv.org/abs/2412.17560v1", "date": "2024-12-23", "relevancy": 2.1222, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.54}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.531}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GQSA%3A%20Group%20Quantization%20and%20Sparsity%20for%20Accelerating%20Large%20Language%0A%20%20Model%20Inference&body=Title%3A%20GQSA%3A%20Group%20Quantization%20and%20Sparsity%20for%20Accelerating%20Large%20Language%0A%20%20Model%20Inference%0AAuthor%3A%20Chao%20Zeng%20and%20Songwei%20Liu%20and%20Shu%20Yang%20and%20Fangmin%20Chen%20and%20Xing%20Mei%20and%20Lean%20Fu%0AAbstract%3A%20%20%20With%20the%20rapid%20growth%20in%20the%20scale%20and%20complexity%20of%20large%20language%20models%0A%28LLMs%29%2C%20the%20costs%20of%20training%20and%20inference%20have%20risen%20substantially.%20Model%0Acompression%20has%20emerged%20as%20a%20mainstream%20solution%20to%20reduce%20memory%20usage%20and%0Acomputational%20overhead.%20This%20paper%20presents%20Group%20Quantization%20and%20Sparse%0AAcceleration%20%28%5Ctextbf%7BGQSA%7D%29%2C%20a%20novel%20compression%20technique%20tailored%20for%20LLMs.%0ATraditional%20methods%20typically%20focus%20exclusively%20on%20either%20quantization%20or%0Asparsification%2C%20but%20relying%20on%20a%20single%20strategy%20often%20results%20in%20significant%0Aperformance%20loss%20at%20high%20compression%20rates.%20In%20contrast%2C%20GQSA%20integrates%0Aquantization%20and%20sparsification%20in%20a%20tightly%20coupled%20manner%2C%20leveraging%0AGPU-friendly%20structured%20group%20sparsity%20and%20quantization%20for%20efficient%0Aacceleration.%20The%20proposed%20method%20consists%20of%20three%20key%20steps.%20First%2C%20GQSA%0Aapplies%20group%20structured%20pruning%20to%20adhere%20to%20GPU-friendly%20sparse%20pattern%0Aconstraints.%20Second%2C%20a%20two-stage%20sparsity-aware%20training%20process%20is%20employed%20to%0Amaximize%20performance%20retention%20after%20compression.%20Finally%2C%20the%20framework%20adopts%0Athe%20Block%20Sparse%20Row%20%28BSR%29%20format%20to%20enable%20practical%20deployment%20and%20efficient%0Aexecution.%20Experimental%20results%20on%20the%20LLaMA%20model%20family%20show%20that%20GQSA%0Aachieves%20an%20excellent%20balance%20between%20model%20speed%20and%20accuracy.%20Furthermore%2C%20on%0Athe%20latest%20LLaMA-3%20and%20LLaMA-3.1%20models%2C%20GQSA%20outperforms%20existing%20LLM%0Acompression%20techniques%20significantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGQSA%253A%2520Group%2520Quantization%2520and%2520Sparsity%2520for%2520Accelerating%2520Large%2520Language%250A%2520%2520Model%2520Inference%26entry.906535625%3DChao%2520Zeng%2520and%2520Songwei%2520Liu%2520and%2520Shu%2520Yang%2520and%2520Fangmin%2520Chen%2520and%2520Xing%2520Mei%2520and%2520Lean%2520Fu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520growth%2520in%2520the%2520scale%2520and%2520complexity%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520the%2520costs%2520of%2520training%2520and%2520inference%2520have%2520risen%2520substantially.%2520Model%250Acompression%2520has%2520emerged%2520as%2520a%2520mainstream%2520solution%2520to%2520reduce%2520memory%2520usage%2520and%250Acomputational%2520overhead.%2520This%2520paper%2520presents%2520Group%2520Quantization%2520and%2520Sparse%250AAcceleration%2520%2528%255Ctextbf%257BGQSA%257D%2529%252C%2520a%2520novel%2520compression%2520technique%2520tailored%2520for%2520LLMs.%250ATraditional%2520methods%2520typically%2520focus%2520exclusively%2520on%2520either%2520quantization%2520or%250Asparsification%252C%2520but%2520relying%2520on%2520a%2520single%2520strategy%2520often%2520results%2520in%2520significant%250Aperformance%2520loss%2520at%2520high%2520compression%2520rates.%2520In%2520contrast%252C%2520GQSA%2520integrates%250Aquantization%2520and%2520sparsification%2520in%2520a%2520tightly%2520coupled%2520manner%252C%2520leveraging%250AGPU-friendly%2520structured%2520group%2520sparsity%2520and%2520quantization%2520for%2520efficient%250Aacceleration.%2520The%2520proposed%2520method%2520consists%2520of%2520three%2520key%2520steps.%2520First%252C%2520GQSA%250Aapplies%2520group%2520structured%2520pruning%2520to%2520adhere%2520to%2520GPU-friendly%2520sparse%2520pattern%250Aconstraints.%2520Second%252C%2520a%2520two-stage%2520sparsity-aware%2520training%2520process%2520is%2520employed%2520to%250Amaximize%2520performance%2520retention%2520after%2520compression.%2520Finally%252C%2520the%2520framework%2520adopts%250Athe%2520Block%2520Sparse%2520Row%2520%2528BSR%2529%2520format%2520to%2520enable%2520practical%2520deployment%2520and%2520efficient%250Aexecution.%2520Experimental%2520results%2520on%2520the%2520LLaMA%2520model%2520family%2520show%2520that%2520GQSA%250Aachieves%2520an%2520excellent%2520balance%2520between%2520model%2520speed%2520and%2520accuracy.%2520Furthermore%252C%2520on%250Athe%2520latest%2520LLaMA-3%2520and%2520LLaMA-3.1%2520models%252C%2520GQSA%2520outperforms%2520existing%2520LLM%250Acompression%2520techniques%2520significantly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GQSA%3A%20Group%20Quantization%20and%20Sparsity%20for%20Accelerating%20Large%20Language%0A%20%20Model%20Inference&entry.906535625=Chao%20Zeng%20and%20Songwei%20Liu%20and%20Shu%20Yang%20and%20Fangmin%20Chen%20and%20Xing%20Mei%20and%20Lean%20Fu&entry.1292438233=%20%20With%20the%20rapid%20growth%20in%20the%20scale%20and%20complexity%20of%20large%20language%20models%0A%28LLMs%29%2C%20the%20costs%20of%20training%20and%20inference%20have%20risen%20substantially.%20Model%0Acompression%20has%20emerged%20as%20a%20mainstream%20solution%20to%20reduce%20memory%20usage%20and%0Acomputational%20overhead.%20This%20paper%20presents%20Group%20Quantization%20and%20Sparse%0AAcceleration%20%28%5Ctextbf%7BGQSA%7D%29%2C%20a%20novel%20compression%20technique%20tailored%20for%20LLMs.%0ATraditional%20methods%20typically%20focus%20exclusively%20on%20either%20quantization%20or%0Asparsification%2C%20but%20relying%20on%20a%20single%20strategy%20often%20results%20in%20significant%0Aperformance%20loss%20at%20high%20compression%20rates.%20In%20contrast%2C%20GQSA%20integrates%0Aquantization%20and%20sparsification%20in%20a%20tightly%20coupled%20manner%2C%20leveraging%0AGPU-friendly%20structured%20group%20sparsity%20and%20quantization%20for%20efficient%0Aacceleration.%20The%20proposed%20method%20consists%20of%20three%20key%20steps.%20First%2C%20GQSA%0Aapplies%20group%20structured%20pruning%20to%20adhere%20to%20GPU-friendly%20sparse%20pattern%0Aconstraints.%20Second%2C%20a%20two-stage%20sparsity-aware%20training%20process%20is%20employed%20to%0Amaximize%20performance%20retention%20after%20compression.%20Finally%2C%20the%20framework%20adopts%0Athe%20Block%20Sparse%20Row%20%28BSR%29%20format%20to%20enable%20practical%20deployment%20and%20efficient%0Aexecution.%20Experimental%20results%20on%20the%20LLaMA%20model%20family%20show%20that%20GQSA%0Aachieves%20an%20excellent%20balance%20between%20model%20speed%20and%20accuracy.%20Furthermore%2C%20on%0Athe%20latest%20LLaMA-3%20and%20LLaMA-3.1%20models%2C%20GQSA%20outperforms%20existing%20LLM%0Acompression%20techniques%20significantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17560v1&entry.124074799=Read"},
{"title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation", "author": "Federico Spurio and Emad Bahrami and Gianpiero Francesca and Juergen Gall", "abstract": "  In this work, we address unsupervised temporal action segmentation, which\nsegments a set of long, untrimmed videos into semantically meaningful segments\nthat are consistent across videos. While recent approaches combine\nrepresentation learning and clustering in a single step for this task, they do\nnot cope with large variations within temporal segments of the same class. To\naddress this limitation, we propose a novel method, termed Hierarchical Vector\nQuantization (\\ours), that consists of two subsequent vector quantization\nmodules. This results in a hierarchical clustering where the additional\nsubclusters cover the variations within a cluster. We demonstrate that our\napproach captures the distribution of segment lengths much better than the\nstate of the art. To this end, we introduce a new metric based on the\nJensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We\nevaluate our approach on three public datasets, namely Breakfast, YouTube\nInstructional and IKEA ASM. Our approach outperforms the state of the art in\nterms of F1 score, recall and JSD.\n", "link": "http://arxiv.org/abs/2412.17640v1", "date": "2024-12-23", "relevancy": 2.1178, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5412}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5231}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Vector%20Quantization%20for%20Unsupervised%20Action%20Segmentation&body=Title%3A%20Hierarchical%20Vector%20Quantization%20for%20Unsupervised%20Action%20Segmentation%0AAuthor%3A%20Federico%20Spurio%20and%20Emad%20Bahrami%20and%20Gianpiero%20Francesca%20and%20Juergen%20Gall%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20unsupervised%20temporal%20action%20segmentation%2C%20which%0Asegments%20a%20set%20of%20long%2C%20untrimmed%20videos%20into%20semantically%20meaningful%20segments%0Athat%20are%20consistent%20across%20videos.%20While%20recent%20approaches%20combine%0Arepresentation%20learning%20and%20clustering%20in%20a%20single%20step%20for%20this%20task%2C%20they%20do%0Anot%20cope%20with%20large%20variations%20within%20temporal%20segments%20of%20the%20same%20class.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20method%2C%20termed%20Hierarchical%20Vector%0AQuantization%20%28%5Cours%29%2C%20that%20consists%20of%20two%20subsequent%20vector%20quantization%0Amodules.%20This%20results%20in%20a%20hierarchical%20clustering%20where%20the%20additional%0Asubclusters%20cover%20the%20variations%20within%20a%20cluster.%20We%20demonstrate%20that%20our%0Aapproach%20captures%20the%20distribution%20of%20segment%20lengths%20much%20better%20than%20the%0Astate%20of%20the%20art.%20To%20this%20end%2C%20we%20introduce%20a%20new%20metric%20based%20on%20the%0AJensen-Shannon%20Distance%20%28JSD%29%20for%20unsupervised%20temporal%20action%20segmentation.%20We%0Aevaluate%20our%20approach%20on%20three%20public%20datasets%2C%20namely%20Breakfast%2C%20YouTube%0AInstructional%20and%20IKEA%20ASM.%20Our%20approach%20outperforms%20the%20state%20of%20the%20art%20in%0Aterms%20of%20F1%20score%2C%20recall%20and%20JSD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Vector%2520Quantization%2520for%2520Unsupervised%2520Action%2520Segmentation%26entry.906535625%3DFederico%2520Spurio%2520and%2520Emad%2520Bahrami%2520and%2520Gianpiero%2520Francesca%2520and%2520Juergen%2520Gall%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520unsupervised%2520temporal%2520action%2520segmentation%252C%2520which%250Asegments%2520a%2520set%2520of%2520long%252C%2520untrimmed%2520videos%2520into%2520semantically%2520meaningful%2520segments%250Athat%2520are%2520consistent%2520across%2520videos.%2520While%2520recent%2520approaches%2520combine%250Arepresentation%2520learning%2520and%2520clustering%2520in%2520a%2520single%2520step%2520for%2520this%2520task%252C%2520they%2520do%250Anot%2520cope%2520with%2520large%2520variations%2520within%2520temporal%2520segments%2520of%2520the%2520same%2520class.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520termed%2520Hierarchical%2520Vector%250AQuantization%2520%2528%255Cours%2529%252C%2520that%2520consists%2520of%2520two%2520subsequent%2520vector%2520quantization%250Amodules.%2520This%2520results%2520in%2520a%2520hierarchical%2520clustering%2520where%2520the%2520additional%250Asubclusters%2520cover%2520the%2520variations%2520within%2520a%2520cluster.%2520We%2520demonstrate%2520that%2520our%250Aapproach%2520captures%2520the%2520distribution%2520of%2520segment%2520lengths%2520much%2520better%2520than%2520the%250Astate%2520of%2520the%2520art.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520new%2520metric%2520based%2520on%2520the%250AJensen-Shannon%2520Distance%2520%2528JSD%2529%2520for%2520unsupervised%2520temporal%2520action%2520segmentation.%2520We%250Aevaluate%2520our%2520approach%2520on%2520three%2520public%2520datasets%252C%2520namely%2520Breakfast%252C%2520YouTube%250AInstructional%2520and%2520IKEA%2520ASM.%2520Our%2520approach%2520outperforms%2520the%2520state%2520of%2520the%2520art%2520in%250Aterms%2520of%2520F1%2520score%252C%2520recall%2520and%2520JSD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Vector%20Quantization%20for%20Unsupervised%20Action%20Segmentation&entry.906535625=Federico%20Spurio%20and%20Emad%20Bahrami%20and%20Gianpiero%20Francesca%20and%20Juergen%20Gall&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20unsupervised%20temporal%20action%20segmentation%2C%20which%0Asegments%20a%20set%20of%20long%2C%20untrimmed%20videos%20into%20semantically%20meaningful%20segments%0Athat%20are%20consistent%20across%20videos.%20While%20recent%20approaches%20combine%0Arepresentation%20learning%20and%20clustering%20in%20a%20single%20step%20for%20this%20task%2C%20they%20do%0Anot%20cope%20with%20large%20variations%20within%20temporal%20segments%20of%20the%20same%20class.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20method%2C%20termed%20Hierarchical%20Vector%0AQuantization%20%28%5Cours%29%2C%20that%20consists%20of%20two%20subsequent%20vector%20quantization%0Amodules.%20This%20results%20in%20a%20hierarchical%20clustering%20where%20the%20additional%0Asubclusters%20cover%20the%20variations%20within%20a%20cluster.%20We%20demonstrate%20that%20our%0Aapproach%20captures%20the%20distribution%20of%20segment%20lengths%20much%20better%20than%20the%0Astate%20of%20the%20art.%20To%20this%20end%2C%20we%20introduce%20a%20new%20metric%20based%20on%20the%0AJensen-Shannon%20Distance%20%28JSD%29%20for%20unsupervised%20temporal%20action%20segmentation.%20We%0Aevaluate%20our%20approach%20on%20three%20public%20datasets%2C%20namely%20Breakfast%2C%20YouTube%0AInstructional%20and%20IKEA%20ASM.%20Our%20approach%20outperforms%20the%20state%20of%20the%20art%20in%0Aterms%20of%20F1%20score%2C%20recall%20and%20JSD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17640v1&entry.124074799=Read"},
{"title": "EDGE: Unknown-aware Multi-label Learning by Energy Distribution Gap\n  Expansion", "author": "Yuchen Sun and Qianqian Xu and Zitai Wang and Zhiyong Yang and Junwei He", "abstract": "  Multi-label Out-Of-Distribution (OOD) detection aims to discriminate the OOD\nsamples from the multi-label In-Distribution (ID) ones. Compared with its\nmulticlass counterpart, it is crucial to model the joint information among\nclasses. To this end, JointEnergy, which is a representative multi-label OOD\ninference criterion, summarizes the logits of all the classes. However, we find\nthat JointEnergy can produce an imbalance problem in OOD detection, especially\nwhen the model lacks enough discrimination ability. Specifically, we find that\nthe samples only related to minority classes tend to be classified as OOD\nsamples due to the ambiguous energy decision boundary. Besides, imbalanced\nmulti-label learning methods, originally designed for ID ones, would not be\nsuitable for OOD detection scenarios, even producing a serious negative\ntransfer effect. In this paper, we resort to auxiliary outlier exposure (OE)\nand propose an unknown-aware multi-label learning framework to reshape the\nuncertainty energy space layout. In this framework, the energy score is\nseparately optimized for tail ID samples and unknown samples, and the energy\ndistribution gap between them is expanded, such that the tail ID samples can\nhave a significantly larger energy score than the OOD ones. What's more, a\nsimple yet effective measure is designed to select more informative OE\ndatasets. Finally, comprehensive experimental results on multiple multi-label\nand OOD datasets reveal the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2412.07499v2", "date": "2024-12-23", "relevancy": 2.1093, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.579}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5203}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EDGE%3A%20Unknown-aware%20Multi-label%20Learning%20by%20Energy%20Distribution%20Gap%0A%20%20Expansion&body=Title%3A%20EDGE%3A%20Unknown-aware%20Multi-label%20Learning%20by%20Energy%20Distribution%20Gap%0A%20%20Expansion%0AAuthor%3A%20Yuchen%20Sun%20and%20Qianqian%20Xu%20and%20Zitai%20Wang%20and%20Zhiyong%20Yang%20and%20Junwei%20He%0AAbstract%3A%20%20%20Multi-label%20Out-Of-Distribution%20%28OOD%29%20detection%20aims%20to%20discriminate%20the%20OOD%0Asamples%20from%20the%20multi-label%20In-Distribution%20%28ID%29%20ones.%20Compared%20with%20its%0Amulticlass%20counterpart%2C%20it%20is%20crucial%20to%20model%20the%20joint%20information%20among%0Aclasses.%20To%20this%20end%2C%20JointEnergy%2C%20which%20is%20a%20representative%20multi-label%20OOD%0Ainference%20criterion%2C%20summarizes%20the%20logits%20of%20all%20the%20classes.%20However%2C%20we%20find%0Athat%20JointEnergy%20can%20produce%20an%20imbalance%20problem%20in%20OOD%20detection%2C%20especially%0Awhen%20the%20model%20lacks%20enough%20discrimination%20ability.%20Specifically%2C%20we%20find%20that%0Athe%20samples%20only%20related%20to%20minority%20classes%20tend%20to%20be%20classified%20as%20OOD%0Asamples%20due%20to%20the%20ambiguous%20energy%20decision%20boundary.%20Besides%2C%20imbalanced%0Amulti-label%20learning%20methods%2C%20originally%20designed%20for%20ID%20ones%2C%20would%20not%20be%0Asuitable%20for%20OOD%20detection%20scenarios%2C%20even%20producing%20a%20serious%20negative%0Atransfer%20effect.%20In%20this%20paper%2C%20we%20resort%20to%20auxiliary%20outlier%20exposure%20%28OE%29%0Aand%20propose%20an%20unknown-aware%20multi-label%20learning%20framework%20to%20reshape%20the%0Auncertainty%20energy%20space%20layout.%20In%20this%20framework%2C%20the%20energy%20score%20is%0Aseparately%20optimized%20for%20tail%20ID%20samples%20and%20unknown%20samples%2C%20and%20the%20energy%0Adistribution%20gap%20between%20them%20is%20expanded%2C%20such%20that%20the%20tail%20ID%20samples%20can%0Ahave%20a%20significantly%20larger%20energy%20score%20than%20the%20OOD%20ones.%20What%27s%20more%2C%20a%0Asimple%20yet%20effective%20measure%20is%20designed%20to%20select%20more%20informative%20OE%0Adatasets.%20Finally%2C%20comprehensive%20experimental%20results%20on%20multiple%20multi-label%0Aand%20OOD%20datasets%20reveal%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07499v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEDGE%253A%2520Unknown-aware%2520Multi-label%2520Learning%2520by%2520Energy%2520Distribution%2520Gap%250A%2520%2520Expansion%26entry.906535625%3DYuchen%2520Sun%2520and%2520Qianqian%2520Xu%2520and%2520Zitai%2520Wang%2520and%2520Zhiyong%2520Yang%2520and%2520Junwei%2520He%26entry.1292438233%3D%2520%2520Multi-label%2520Out-Of-Distribution%2520%2528OOD%2529%2520detection%2520aims%2520to%2520discriminate%2520the%2520OOD%250Asamples%2520from%2520the%2520multi-label%2520In-Distribution%2520%2528ID%2529%2520ones.%2520Compared%2520with%2520its%250Amulticlass%2520counterpart%252C%2520it%2520is%2520crucial%2520to%2520model%2520the%2520joint%2520information%2520among%250Aclasses.%2520To%2520this%2520end%252C%2520JointEnergy%252C%2520which%2520is%2520a%2520representative%2520multi-label%2520OOD%250Ainference%2520criterion%252C%2520summarizes%2520the%2520logits%2520of%2520all%2520the%2520classes.%2520However%252C%2520we%2520find%250Athat%2520JointEnergy%2520can%2520produce%2520an%2520imbalance%2520problem%2520in%2520OOD%2520detection%252C%2520especially%250Awhen%2520the%2520model%2520lacks%2520enough%2520discrimination%2520ability.%2520Specifically%252C%2520we%2520find%2520that%250Athe%2520samples%2520only%2520related%2520to%2520minority%2520classes%2520tend%2520to%2520be%2520classified%2520as%2520OOD%250Asamples%2520due%2520to%2520the%2520ambiguous%2520energy%2520decision%2520boundary.%2520Besides%252C%2520imbalanced%250Amulti-label%2520learning%2520methods%252C%2520originally%2520designed%2520for%2520ID%2520ones%252C%2520would%2520not%2520be%250Asuitable%2520for%2520OOD%2520detection%2520scenarios%252C%2520even%2520producing%2520a%2520serious%2520negative%250Atransfer%2520effect.%2520In%2520this%2520paper%252C%2520we%2520resort%2520to%2520auxiliary%2520outlier%2520exposure%2520%2528OE%2529%250Aand%2520propose%2520an%2520unknown-aware%2520multi-label%2520learning%2520framework%2520to%2520reshape%2520the%250Auncertainty%2520energy%2520space%2520layout.%2520In%2520this%2520framework%252C%2520the%2520energy%2520score%2520is%250Aseparately%2520optimized%2520for%2520tail%2520ID%2520samples%2520and%2520unknown%2520samples%252C%2520and%2520the%2520energy%250Adistribution%2520gap%2520between%2520them%2520is%2520expanded%252C%2520such%2520that%2520the%2520tail%2520ID%2520samples%2520can%250Ahave%2520a%2520significantly%2520larger%2520energy%2520score%2520than%2520the%2520OOD%2520ones.%2520What%2527s%2520more%252C%2520a%250Asimple%2520yet%2520effective%2520measure%2520is%2520designed%2520to%2520select%2520more%2520informative%2520OE%250Adatasets.%2520Finally%252C%2520comprehensive%2520experimental%2520results%2520on%2520multiple%2520multi-label%250Aand%2520OOD%2520datasets%2520reveal%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07499v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDGE%3A%20Unknown-aware%20Multi-label%20Learning%20by%20Energy%20Distribution%20Gap%0A%20%20Expansion&entry.906535625=Yuchen%20Sun%20and%20Qianqian%20Xu%20and%20Zitai%20Wang%20and%20Zhiyong%20Yang%20and%20Junwei%20He&entry.1292438233=%20%20Multi-label%20Out-Of-Distribution%20%28OOD%29%20detection%20aims%20to%20discriminate%20the%20OOD%0Asamples%20from%20the%20multi-label%20In-Distribution%20%28ID%29%20ones.%20Compared%20with%20its%0Amulticlass%20counterpart%2C%20it%20is%20crucial%20to%20model%20the%20joint%20information%20among%0Aclasses.%20To%20this%20end%2C%20JointEnergy%2C%20which%20is%20a%20representative%20multi-label%20OOD%0Ainference%20criterion%2C%20summarizes%20the%20logits%20of%20all%20the%20classes.%20However%2C%20we%20find%0Athat%20JointEnergy%20can%20produce%20an%20imbalance%20problem%20in%20OOD%20detection%2C%20especially%0Awhen%20the%20model%20lacks%20enough%20discrimination%20ability.%20Specifically%2C%20we%20find%20that%0Athe%20samples%20only%20related%20to%20minority%20classes%20tend%20to%20be%20classified%20as%20OOD%0Asamples%20due%20to%20the%20ambiguous%20energy%20decision%20boundary.%20Besides%2C%20imbalanced%0Amulti-label%20learning%20methods%2C%20originally%20designed%20for%20ID%20ones%2C%20would%20not%20be%0Asuitable%20for%20OOD%20detection%20scenarios%2C%20even%20producing%20a%20serious%20negative%0Atransfer%20effect.%20In%20this%20paper%2C%20we%20resort%20to%20auxiliary%20outlier%20exposure%20%28OE%29%0Aand%20propose%20an%20unknown-aware%20multi-label%20learning%20framework%20to%20reshape%20the%0Auncertainty%20energy%20space%20layout.%20In%20this%20framework%2C%20the%20energy%20score%20is%0Aseparately%20optimized%20for%20tail%20ID%20samples%20and%20unknown%20samples%2C%20and%20the%20energy%0Adistribution%20gap%20between%20them%20is%20expanded%2C%20such%20that%20the%20tail%20ID%20samples%20can%0Ahave%20a%20significantly%20larger%20energy%20score%20than%20the%20OOD%20ones.%20What%27s%20more%2C%20a%0Asimple%20yet%20effective%20measure%20is%20designed%20to%20select%20more%20informative%20OE%0Adatasets.%20Finally%2C%20comprehensive%20experimental%20results%20on%20multiple%20multi-label%0Aand%20OOD%20datasets%20reveal%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07499v2&entry.124074799=Read"},
{"title": "Deliberation in Latent Space via Differentiable Cache Augmentation", "author": "Luyang Liu and Jonas Pfeiffer and Jiaxing Wu and Jun Xie and Arthur Szlam", "abstract": "  Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.\n", "link": "http://arxiv.org/abs/2412.17747v1", "date": "2024-12-23", "relevancy": 2.1038, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deliberation%20in%20Latent%20Space%20via%20Differentiable%20Cache%20Augmentation&body=Title%3A%20Deliberation%20in%20Latent%20Space%20via%20Differentiable%20Cache%20Augmentation%0AAuthor%3A%20Luyang%20Liu%20and%20Jonas%20Pfeiffer%20and%20Jiaxing%20Wu%20and%20Jun%20Xie%20and%20Arthur%20Szlam%0AAbstract%3A%20%20%20Techniques%20enabling%20large%20language%20models%20%28LLMs%29%20to%20%22think%20more%22%20by%0Agenerating%20and%20attending%20to%20intermediate%20reasoning%20steps%20have%20shown%20promise%20in%0Asolving%20complex%20problems.%20However%2C%20the%20standard%20approaches%20generate%20sequences%0Aof%20discrete%20tokens%20immediately%20before%20responding%2C%20and%20so%20they%20can%20incur%0Asignificant%20latency%20costs%20and%20be%20challenging%20to%20optimize.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20a%20frozen%20LLM%20can%20be%20augmented%20with%20an%20offline%20coprocessor%20that%0Aoperates%20on%20the%20model%27s%20key-value%20%28kv%29%20cache.%20This%20coprocessor%20augments%20the%0Acache%20with%20a%20set%20of%20latent%20embeddings%20designed%20to%20improve%20the%20fidelity%20of%0Asubsequent%20decoding.%20We%20train%20this%20coprocessor%20using%20the%20language%20modeling%20loss%0Afrom%20the%20decoder%20on%20standard%20pretraining%20data%2C%20while%20keeping%20the%20decoder%20itself%0Afrozen.%20This%20approach%20enables%20the%20model%20to%20learn%2C%20in%20an%20end-to-end%0Adifferentiable%20fashion%2C%20how%20to%20distill%20additional%20computation%20into%20its%0Akv-cache.%20Because%20the%20decoder%20remains%20unchanged%2C%20the%20coprocessor%20can%20operate%0Aoffline%20and%20asynchronously%2C%20and%20the%20language%20model%20can%20function%20normally%20if%20the%0Acoprocessor%20is%20unavailable%20or%20if%20a%20given%20cache%20is%20deemed%20not%20to%20require%20extra%0Acomputation.%20We%20show%20experimentally%20that%20when%20a%20cache%20is%20augmented%2C%20the%20decoder%0Aachieves%20lower%20perplexity%20on%20numerous%20subsequent%20tokens.%20Furthermore%2C%20even%0Awithout%20any%20task-specific%20training%2C%20our%20experiments%20demonstrate%20that%20cache%0Aaugmentation%20consistently%20reduces%20perplexity%20and%20improves%20performance%20across%20a%0Arange%20of%20reasoning-intensive%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeliberation%2520in%2520Latent%2520Space%2520via%2520Differentiable%2520Cache%2520Augmentation%26entry.906535625%3DLuyang%2520Liu%2520and%2520Jonas%2520Pfeiffer%2520and%2520Jiaxing%2520Wu%2520and%2520Jun%2520Xie%2520and%2520Arthur%2520Szlam%26entry.1292438233%3D%2520%2520Techniques%2520enabling%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520%2522think%2520more%2522%2520by%250Agenerating%2520and%2520attending%2520to%2520intermediate%2520reasoning%2520steps%2520have%2520shown%2520promise%2520in%250Asolving%2520complex%2520problems.%2520However%252C%2520the%2520standard%2520approaches%2520generate%2520sequences%250Aof%2520discrete%2520tokens%2520immediately%2520before%2520responding%252C%2520and%2520so%2520they%2520can%2520incur%250Asignificant%2520latency%2520costs%2520and%2520be%2520challenging%2520to%2520optimize.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520a%2520frozen%2520LLM%2520can%2520be%2520augmented%2520with%2520an%2520offline%2520coprocessor%2520that%250Aoperates%2520on%2520the%2520model%2527s%2520key-value%2520%2528kv%2529%2520cache.%2520This%2520coprocessor%2520augments%2520the%250Acache%2520with%2520a%2520set%2520of%2520latent%2520embeddings%2520designed%2520to%2520improve%2520the%2520fidelity%2520of%250Asubsequent%2520decoding.%2520We%2520train%2520this%2520coprocessor%2520using%2520the%2520language%2520modeling%2520loss%250Afrom%2520the%2520decoder%2520on%2520standard%2520pretraining%2520data%252C%2520while%2520keeping%2520the%2520decoder%2520itself%250Afrozen.%2520This%2520approach%2520enables%2520the%2520model%2520to%2520learn%252C%2520in%2520an%2520end-to-end%250Adifferentiable%2520fashion%252C%2520how%2520to%2520distill%2520additional%2520computation%2520into%2520its%250Akv-cache.%2520Because%2520the%2520decoder%2520remains%2520unchanged%252C%2520the%2520coprocessor%2520can%2520operate%250Aoffline%2520and%2520asynchronously%252C%2520and%2520the%2520language%2520model%2520can%2520function%2520normally%2520if%2520the%250Acoprocessor%2520is%2520unavailable%2520or%2520if%2520a%2520given%2520cache%2520is%2520deemed%2520not%2520to%2520require%2520extra%250Acomputation.%2520We%2520show%2520experimentally%2520that%2520when%2520a%2520cache%2520is%2520augmented%252C%2520the%2520decoder%250Aachieves%2520lower%2520perplexity%2520on%2520numerous%2520subsequent%2520tokens.%2520Furthermore%252C%2520even%250Awithout%2520any%2520task-specific%2520training%252C%2520our%2520experiments%2520demonstrate%2520that%2520cache%250Aaugmentation%2520consistently%2520reduces%2520perplexity%2520and%2520improves%2520performance%2520across%2520a%250Arange%2520of%2520reasoning-intensive%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deliberation%20in%20Latent%20Space%20via%20Differentiable%20Cache%20Augmentation&entry.906535625=Luyang%20Liu%20and%20Jonas%20Pfeiffer%20and%20Jiaxing%20Wu%20and%20Jun%20Xie%20and%20Arthur%20Szlam&entry.1292438233=%20%20Techniques%20enabling%20large%20language%20models%20%28LLMs%29%20to%20%22think%20more%22%20by%0Agenerating%20and%20attending%20to%20intermediate%20reasoning%20steps%20have%20shown%20promise%20in%0Asolving%20complex%20problems.%20However%2C%20the%20standard%20approaches%20generate%20sequences%0Aof%20discrete%20tokens%20immediately%20before%20responding%2C%20and%20so%20they%20can%20incur%0Asignificant%20latency%20costs%20and%20be%20challenging%20to%20optimize.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20a%20frozen%20LLM%20can%20be%20augmented%20with%20an%20offline%20coprocessor%20that%0Aoperates%20on%20the%20model%27s%20key-value%20%28kv%29%20cache.%20This%20coprocessor%20augments%20the%0Acache%20with%20a%20set%20of%20latent%20embeddings%20designed%20to%20improve%20the%20fidelity%20of%0Asubsequent%20decoding.%20We%20train%20this%20coprocessor%20using%20the%20language%20modeling%20loss%0Afrom%20the%20decoder%20on%20standard%20pretraining%20data%2C%20while%20keeping%20the%20decoder%20itself%0Afrozen.%20This%20approach%20enables%20the%20model%20to%20learn%2C%20in%20an%20end-to-end%0Adifferentiable%20fashion%2C%20how%20to%20distill%20additional%20computation%20into%20its%0Akv-cache.%20Because%20the%20decoder%20remains%20unchanged%2C%20the%20coprocessor%20can%20operate%0Aoffline%20and%20asynchronously%2C%20and%20the%20language%20model%20can%20function%20normally%20if%20the%0Acoprocessor%20is%20unavailable%20or%20if%20a%20given%20cache%20is%20deemed%20not%20to%20require%20extra%0Acomputation.%20We%20show%20experimentally%20that%20when%20a%20cache%20is%20augmented%2C%20the%20decoder%0Aachieves%20lower%20perplexity%20on%20numerous%20subsequent%20tokens.%20Furthermore%2C%20even%0Awithout%20any%20task-specific%20training%2C%20our%20experiments%20demonstrate%20that%20cache%0Aaugmentation%20consistently%20reduces%20perplexity%20and%20improves%20performance%20across%20a%0Arange%20of%20reasoning-intensive%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17747v1&entry.124074799=Read"},
{"title": "Neural Continuous-Time Supermartingale Certificates", "author": "Grigory Neustroev and Mirco Giacobbe and Anna Lukina", "abstract": "  We introduce for the first time a neural-certificate framework for\ncontinuous-time stochastic dynamical systems. Autonomous learning systems in\nthe physical world demand continuous-time reasoning, yet existing learnable\ncertificates for probabilistic verification assume discretization of the time\ncontinuum. Inspired by the success of training neural Lyapunov certificates for\ndeterministic continuous-time systems and neural supermartingale certificates\nfor stochastic discrete-time systems, we propose a framework that bridges the\ngap between continuous-time and probabilistic neural certification for\ndynamical systems under complex requirements. Our method combines machine\nlearning and symbolic reasoning to produce formally certified bounds on the\nprobabilities that a nonlinear system satisfies specifications of reachability,\navoidance, and persistence. We present both the theoretical justification and\nthe algorithmic implementation of our framework and showcase its efficacy on\npopular benchmarks.\n", "link": "http://arxiv.org/abs/2412.17432v1", "date": "2024-12-23", "relevancy": 1.9176, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4721}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Continuous-Time%20Supermartingale%20Certificates&body=Title%3A%20Neural%20Continuous-Time%20Supermartingale%20Certificates%0AAuthor%3A%20Grigory%20Neustroev%20and%20Mirco%20Giacobbe%20and%20Anna%20Lukina%0AAbstract%3A%20%20%20We%20introduce%20for%20the%20first%20time%20a%20neural-certificate%20framework%20for%0Acontinuous-time%20stochastic%20dynamical%20systems.%20Autonomous%20learning%20systems%20in%0Athe%20physical%20world%20demand%20continuous-time%20reasoning%2C%20yet%20existing%20learnable%0Acertificates%20for%20probabilistic%20verification%20assume%20discretization%20of%20the%20time%0Acontinuum.%20Inspired%20by%20the%20success%20of%20training%20neural%20Lyapunov%20certificates%20for%0Adeterministic%20continuous-time%20systems%20and%20neural%20supermartingale%20certificates%0Afor%20stochastic%20discrete-time%20systems%2C%20we%20propose%20a%20framework%20that%20bridges%20the%0Agap%20between%20continuous-time%20and%20probabilistic%20neural%20certification%20for%0Adynamical%20systems%20under%20complex%20requirements.%20Our%20method%20combines%20machine%0Alearning%20and%20symbolic%20reasoning%20to%20produce%20formally%20certified%20bounds%20on%20the%0Aprobabilities%20that%20a%20nonlinear%20system%20satisfies%20specifications%20of%20reachability%2C%0Aavoidance%2C%20and%20persistence.%20We%20present%20both%20the%20theoretical%20justification%20and%0Athe%20algorithmic%20implementation%20of%20our%20framework%20and%20showcase%20its%20efficacy%20on%0Apopular%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Continuous-Time%2520Supermartingale%2520Certificates%26entry.906535625%3DGrigory%2520Neustroev%2520and%2520Mirco%2520Giacobbe%2520and%2520Anna%2520Lukina%26entry.1292438233%3D%2520%2520We%2520introduce%2520for%2520the%2520first%2520time%2520a%2520neural-certificate%2520framework%2520for%250Acontinuous-time%2520stochastic%2520dynamical%2520systems.%2520Autonomous%2520learning%2520systems%2520in%250Athe%2520physical%2520world%2520demand%2520continuous-time%2520reasoning%252C%2520yet%2520existing%2520learnable%250Acertificates%2520for%2520probabilistic%2520verification%2520assume%2520discretization%2520of%2520the%2520time%250Acontinuum.%2520Inspired%2520by%2520the%2520success%2520of%2520training%2520neural%2520Lyapunov%2520certificates%2520for%250Adeterministic%2520continuous-time%2520systems%2520and%2520neural%2520supermartingale%2520certificates%250Afor%2520stochastic%2520discrete-time%2520systems%252C%2520we%2520propose%2520a%2520framework%2520that%2520bridges%2520the%250Agap%2520between%2520continuous-time%2520and%2520probabilistic%2520neural%2520certification%2520for%250Adynamical%2520systems%2520under%2520complex%2520requirements.%2520Our%2520method%2520combines%2520machine%250Alearning%2520and%2520symbolic%2520reasoning%2520to%2520produce%2520formally%2520certified%2520bounds%2520on%2520the%250Aprobabilities%2520that%2520a%2520nonlinear%2520system%2520satisfies%2520specifications%2520of%2520reachability%252C%250Aavoidance%252C%2520and%2520persistence.%2520We%2520present%2520both%2520the%2520theoretical%2520justification%2520and%250Athe%2520algorithmic%2520implementation%2520of%2520our%2520framework%2520and%2520showcase%2520its%2520efficacy%2520on%250Apopular%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Continuous-Time%20Supermartingale%20Certificates&entry.906535625=Grigory%20Neustroev%20and%20Mirco%20Giacobbe%20and%20Anna%20Lukina&entry.1292438233=%20%20We%20introduce%20for%20the%20first%20time%20a%20neural-certificate%20framework%20for%0Acontinuous-time%20stochastic%20dynamical%20systems.%20Autonomous%20learning%20systems%20in%0Athe%20physical%20world%20demand%20continuous-time%20reasoning%2C%20yet%20existing%20learnable%0Acertificates%20for%20probabilistic%20verification%20assume%20discretization%20of%20the%20time%0Acontinuum.%20Inspired%20by%20the%20success%20of%20training%20neural%20Lyapunov%20certificates%20for%0Adeterministic%20continuous-time%20systems%20and%20neural%20supermartingale%20certificates%0Afor%20stochastic%20discrete-time%20systems%2C%20we%20propose%20a%20framework%20that%20bridges%20the%0Agap%20between%20continuous-time%20and%20probabilistic%20neural%20certification%20for%0Adynamical%20systems%20under%20complex%20requirements.%20Our%20method%20combines%20machine%0Alearning%20and%20symbolic%20reasoning%20to%20produce%20formally%20certified%20bounds%20on%20the%0Aprobabilities%20that%20a%20nonlinear%20system%20satisfies%20specifications%20of%20reachability%2C%0Aavoidance%2C%20and%20persistence.%20We%20present%20both%20the%20theoretical%20justification%20and%0Athe%20algorithmic%20implementation%20of%20our%20framework%20and%20showcase%20its%20efficacy%20on%0Apopular%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17432v1&entry.124074799=Read"},
{"title": "Minimax Optimal Simple Regret in Two-Armed Best-Arm Identification", "author": "Masahiro Kato", "abstract": "  This study investigates an asymptotically minimax optimal algorithm in the\ntwo-armed fixed-budget best-arm identification (BAI) problem. Given two\ntreatment arms, the objective is to identify the arm with the highest expected\noutcome through an adaptive experiment. We focus on the Neyman allocation,\nwhere treatment arms are allocated following the ratio of their outcome\nstandard deviations. Our primary contribution is to prove the minimax\noptimality of the Neyman allocation for the simple regret, defined as the\ndifference between the expected outcomes of the true best arm and the estimated\nbest arm. Specifically, we first derive a minimax lower bound for the expected\nsimple regret, which characterizes the worst-case performance achievable under\nthe location-shift distributions, including Gaussian distributions. We then\nshow that the simple regret of the Neyman allocation asymptotically matches\nthis lower bound, including the constant term, not just the rate in terms of\nthe sample size, under the worst-case distribution. Notably, our optimality\nresult holds without imposing locality restrictions on the distribution, such\nas the local asymptotic normality. Furthermore, we demonstrate that the Neyman\nallocation reduces to the uniform allocation, i.e., the standard randomized\ncontrolled trial, under Bernoulli distributions.\n", "link": "http://arxiv.org/abs/2412.17753v1", "date": "2024-12-23", "relevancy": 1.6026, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4082}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4057}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimax%20Optimal%20Simple%20Regret%20in%20Two-Armed%20Best-Arm%20Identification&body=Title%3A%20Minimax%20Optimal%20Simple%20Regret%20in%20Two-Armed%20Best-Arm%20Identification%0AAuthor%3A%20Masahiro%20Kato%0AAbstract%3A%20%20%20This%20study%20investigates%20an%20asymptotically%20minimax%20optimal%20algorithm%20in%20the%0Atwo-armed%20fixed-budget%20best-arm%20identification%20%28BAI%29%20problem.%20Given%20two%0Atreatment%20arms%2C%20the%20objective%20is%20to%20identify%20the%20arm%20with%20the%20highest%20expected%0Aoutcome%20through%20an%20adaptive%20experiment.%20We%20focus%20on%20the%20Neyman%20allocation%2C%0Awhere%20treatment%20arms%20are%20allocated%20following%20the%20ratio%20of%20their%20outcome%0Astandard%20deviations.%20Our%20primary%20contribution%20is%20to%20prove%20the%20minimax%0Aoptimality%20of%20the%20Neyman%20allocation%20for%20the%20simple%20regret%2C%20defined%20as%20the%0Adifference%20between%20the%20expected%20outcomes%20of%20the%20true%20best%20arm%20and%20the%20estimated%0Abest%20arm.%20Specifically%2C%20we%20first%20derive%20a%20minimax%20lower%20bound%20for%20the%20expected%0Asimple%20regret%2C%20which%20characterizes%20the%20worst-case%20performance%20achievable%20under%0Athe%20location-shift%20distributions%2C%20including%20Gaussian%20distributions.%20We%20then%0Ashow%20that%20the%20simple%20regret%20of%20the%20Neyman%20allocation%20asymptotically%20matches%0Athis%20lower%20bound%2C%20including%20the%20constant%20term%2C%20not%20just%20the%20rate%20in%20terms%20of%0Athe%20sample%20size%2C%20under%20the%20worst-case%20distribution.%20Notably%2C%20our%20optimality%0Aresult%20holds%20without%20imposing%20locality%20restrictions%20on%20the%20distribution%2C%20such%0Aas%20the%20local%20asymptotic%20normality.%20Furthermore%2C%20we%20demonstrate%20that%20the%20Neyman%0Aallocation%20reduces%20to%20the%20uniform%20allocation%2C%20i.e.%2C%20the%20standard%20randomized%0Acontrolled%20trial%2C%20under%20Bernoulli%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimax%2520Optimal%2520Simple%2520Regret%2520in%2520Two-Armed%2520Best-Arm%2520Identification%26entry.906535625%3DMasahiro%2520Kato%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520an%2520asymptotically%2520minimax%2520optimal%2520algorithm%2520in%2520the%250Atwo-armed%2520fixed-budget%2520best-arm%2520identification%2520%2528BAI%2529%2520problem.%2520Given%2520two%250Atreatment%2520arms%252C%2520the%2520objective%2520is%2520to%2520identify%2520the%2520arm%2520with%2520the%2520highest%2520expected%250Aoutcome%2520through%2520an%2520adaptive%2520experiment.%2520We%2520focus%2520on%2520the%2520Neyman%2520allocation%252C%250Awhere%2520treatment%2520arms%2520are%2520allocated%2520following%2520the%2520ratio%2520of%2520their%2520outcome%250Astandard%2520deviations.%2520Our%2520primary%2520contribution%2520is%2520to%2520prove%2520the%2520minimax%250Aoptimality%2520of%2520the%2520Neyman%2520allocation%2520for%2520the%2520simple%2520regret%252C%2520defined%2520as%2520the%250Adifference%2520between%2520the%2520expected%2520outcomes%2520of%2520the%2520true%2520best%2520arm%2520and%2520the%2520estimated%250Abest%2520arm.%2520Specifically%252C%2520we%2520first%2520derive%2520a%2520minimax%2520lower%2520bound%2520for%2520the%2520expected%250Asimple%2520regret%252C%2520which%2520characterizes%2520the%2520worst-case%2520performance%2520achievable%2520under%250Athe%2520location-shift%2520distributions%252C%2520including%2520Gaussian%2520distributions.%2520We%2520then%250Ashow%2520that%2520the%2520simple%2520regret%2520of%2520the%2520Neyman%2520allocation%2520asymptotically%2520matches%250Athis%2520lower%2520bound%252C%2520including%2520the%2520constant%2520term%252C%2520not%2520just%2520the%2520rate%2520in%2520terms%2520of%250Athe%2520sample%2520size%252C%2520under%2520the%2520worst-case%2520distribution.%2520Notably%252C%2520our%2520optimality%250Aresult%2520holds%2520without%2520imposing%2520locality%2520restrictions%2520on%2520the%2520distribution%252C%2520such%250Aas%2520the%2520local%2520asymptotic%2520normality.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520the%2520Neyman%250Aallocation%2520reduces%2520to%2520the%2520uniform%2520allocation%252C%2520i.e.%252C%2520the%2520standard%2520randomized%250Acontrolled%2520trial%252C%2520under%2520Bernoulli%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimax%20Optimal%20Simple%20Regret%20in%20Two-Armed%20Best-Arm%20Identification&entry.906535625=Masahiro%20Kato&entry.1292438233=%20%20This%20study%20investigates%20an%20asymptotically%20minimax%20optimal%20algorithm%20in%20the%0Atwo-armed%20fixed-budget%20best-arm%20identification%20%28BAI%29%20problem.%20Given%20two%0Atreatment%20arms%2C%20the%20objective%20is%20to%20identify%20the%20arm%20with%20the%20highest%20expected%0Aoutcome%20through%20an%20adaptive%20experiment.%20We%20focus%20on%20the%20Neyman%20allocation%2C%0Awhere%20treatment%20arms%20are%20allocated%20following%20the%20ratio%20of%20their%20outcome%0Astandard%20deviations.%20Our%20primary%20contribution%20is%20to%20prove%20the%20minimax%0Aoptimality%20of%20the%20Neyman%20allocation%20for%20the%20simple%20regret%2C%20defined%20as%20the%0Adifference%20between%20the%20expected%20outcomes%20of%20the%20true%20best%20arm%20and%20the%20estimated%0Abest%20arm.%20Specifically%2C%20we%20first%20derive%20a%20minimax%20lower%20bound%20for%20the%20expected%0Asimple%20regret%2C%20which%20characterizes%20the%20worst-case%20performance%20achievable%20under%0Athe%20location-shift%20distributions%2C%20including%20Gaussian%20distributions.%20We%20then%0Ashow%20that%20the%20simple%20regret%20of%20the%20Neyman%20allocation%20asymptotically%20matches%0Athis%20lower%20bound%2C%20including%20the%20constant%20term%2C%20not%20just%20the%20rate%20in%20terms%20of%0Athe%20sample%20size%2C%20under%20the%20worst-case%20distribution.%20Notably%2C%20our%20optimality%0Aresult%20holds%20without%20imposing%20locality%20restrictions%20on%20the%20distribution%2C%20such%0Aas%20the%20local%20asymptotic%20normality.%20Furthermore%2C%20we%20demonstrate%20that%20the%20Neyman%0Aallocation%20reduces%20to%20the%20uniform%20allocation%2C%20i.e.%2C%20the%20standard%20randomized%0Acontrolled%20trial%2C%20under%20Bernoulli%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17753v1&entry.124074799=Read"},
{"title": "Fast Causal Discovery by Approximate Kernel-based Generalized Score\n  Functions with Linear Computational Complexity", "author": "Yixin Ren and Haocheng Zhang and Yewei Xia and Hao Zhang and Jihong Guan and Shuigeng Zhou", "abstract": "  Score-based causal discovery methods can effectively identify causal\nrelationships by evaluating candidate graphs and selecting the one with the\nhighest score. One popular class of scores is kernel-based generalized score\nfunctions, which can adapt to a wide range of scenarios and work well in\npractice because they circumvent assumptions about causal mechanisms and data\ndistributions. Despite these advantages, kernel-based generalized score\nfunctions pose serious computational challenges in time and space, with a time\ncomplexity of $\\mathcal{O}(n^3)$ and a memory complexity of $\\mathcal{O}(n^2)$,\nwhere $n$ is the sample size. In this paper, we propose an approximate\nkernel-based generalized score function with $\\mathcal{O}(n)$ time and space\ncomplexities by using low-rank technique and designing a set of rules to handle\nthe complex composite matrix operations required to calculate the score, as\nwell as developing sampling algorithms for different data types to benefit the\nhandling of diverse data types efficiently. Our extensive causal discovery\nexperiments on both synthetic and real-world data demonstrate that compared to\nthe state-of-the-art method, our method can not only significantly reduce\ncomputational costs, but also achieve comparable accuracy, especially for large\ndatasets.\n", "link": "http://arxiv.org/abs/2412.17717v1", "date": "2024-12-23", "relevancy": 1.1626, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4021}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3906}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Causal%20Discovery%20by%20Approximate%20Kernel-based%20Generalized%20Score%0A%20%20Functions%20with%20Linear%20Computational%20Complexity&body=Title%3A%20Fast%20Causal%20Discovery%20by%20Approximate%20Kernel-based%20Generalized%20Score%0A%20%20Functions%20with%20Linear%20Computational%20Complexity%0AAuthor%3A%20Yixin%20Ren%20and%20Haocheng%20Zhang%20and%20Yewei%20Xia%20and%20Hao%20Zhang%20and%20Jihong%20Guan%20and%20Shuigeng%20Zhou%0AAbstract%3A%20%20%20Score-based%20causal%20discovery%20methods%20can%20effectively%20identify%20causal%0Arelationships%20by%20evaluating%20candidate%20graphs%20and%20selecting%20the%20one%20with%20the%0Ahighest%20score.%20One%20popular%20class%20of%20scores%20is%20kernel-based%20generalized%20score%0Afunctions%2C%20which%20can%20adapt%20to%20a%20wide%20range%20of%20scenarios%20and%20work%20well%20in%0Apractice%20because%20they%20circumvent%20assumptions%20about%20causal%20mechanisms%20and%20data%0Adistributions.%20Despite%20these%20advantages%2C%20kernel-based%20generalized%20score%0Afunctions%20pose%20serious%20computational%20challenges%20in%20time%20and%20space%2C%20with%20a%20time%0Acomplexity%20of%20%24%5Cmathcal%7BO%7D%28n%5E3%29%24%20and%20a%20memory%20complexity%20of%20%24%5Cmathcal%7BO%7D%28n%5E2%29%24%2C%0Awhere%20%24n%24%20is%20the%20sample%20size.%20In%20this%20paper%2C%20we%20propose%20an%20approximate%0Akernel-based%20generalized%20score%20function%20with%20%24%5Cmathcal%7BO%7D%28n%29%24%20time%20and%20space%0Acomplexities%20by%20using%20low-rank%20technique%20and%20designing%20a%20set%20of%20rules%20to%20handle%0Athe%20complex%20composite%20matrix%20operations%20required%20to%20calculate%20the%20score%2C%20as%0Awell%20as%20developing%20sampling%20algorithms%20for%20different%20data%20types%20to%20benefit%20the%0Ahandling%20of%20diverse%20data%20types%20efficiently.%20Our%20extensive%20causal%20discovery%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20data%20demonstrate%20that%20compared%20to%0Athe%20state-of-the-art%20method%2C%20our%20method%20can%20not%20only%20significantly%20reduce%0Acomputational%20costs%2C%20but%20also%20achieve%20comparable%20accuracy%2C%20especially%20for%20large%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Causal%2520Discovery%2520by%2520Approximate%2520Kernel-based%2520Generalized%2520Score%250A%2520%2520Functions%2520with%2520Linear%2520Computational%2520Complexity%26entry.906535625%3DYixin%2520Ren%2520and%2520Haocheng%2520Zhang%2520and%2520Yewei%2520Xia%2520and%2520Hao%2520Zhang%2520and%2520Jihong%2520Guan%2520and%2520Shuigeng%2520Zhou%26entry.1292438233%3D%2520%2520Score-based%2520causal%2520discovery%2520methods%2520can%2520effectively%2520identify%2520causal%250Arelationships%2520by%2520evaluating%2520candidate%2520graphs%2520and%2520selecting%2520the%2520one%2520with%2520the%250Ahighest%2520score.%2520One%2520popular%2520class%2520of%2520scores%2520is%2520kernel-based%2520generalized%2520score%250Afunctions%252C%2520which%2520can%2520adapt%2520to%2520a%2520wide%2520range%2520of%2520scenarios%2520and%2520work%2520well%2520in%250Apractice%2520because%2520they%2520circumvent%2520assumptions%2520about%2520causal%2520mechanisms%2520and%2520data%250Adistributions.%2520Despite%2520these%2520advantages%252C%2520kernel-based%2520generalized%2520score%250Afunctions%2520pose%2520serious%2520computational%2520challenges%2520in%2520time%2520and%2520space%252C%2520with%2520a%2520time%250Acomplexity%2520of%2520%2524%255Cmathcal%257BO%257D%2528n%255E3%2529%2524%2520and%2520a%2520memory%2520complexity%2520of%2520%2524%255Cmathcal%257BO%257D%2528n%255E2%2529%2524%252C%250Awhere%2520%2524n%2524%2520is%2520the%2520sample%2520size.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520approximate%250Akernel-based%2520generalized%2520score%2520function%2520with%2520%2524%255Cmathcal%257BO%257D%2528n%2529%2524%2520time%2520and%2520space%250Acomplexities%2520by%2520using%2520low-rank%2520technique%2520and%2520designing%2520a%2520set%2520of%2520rules%2520to%2520handle%250Athe%2520complex%2520composite%2520matrix%2520operations%2520required%2520to%2520calculate%2520the%2520score%252C%2520as%250Awell%2520as%2520developing%2520sampling%2520algorithms%2520for%2520different%2520data%2520types%2520to%2520benefit%2520the%250Ahandling%2520of%2520diverse%2520data%2520types%2520efficiently.%2520Our%2520extensive%2520causal%2520discovery%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520data%2520demonstrate%2520that%2520compared%2520to%250Athe%2520state-of-the-art%2520method%252C%2520our%2520method%2520can%2520not%2520only%2520significantly%2520reduce%250Acomputational%2520costs%252C%2520but%2520also%2520achieve%2520comparable%2520accuracy%252C%2520especially%2520for%2520large%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Causal%20Discovery%20by%20Approximate%20Kernel-based%20Generalized%20Score%0A%20%20Functions%20with%20Linear%20Computational%20Complexity&entry.906535625=Yixin%20Ren%20and%20Haocheng%20Zhang%20and%20Yewei%20Xia%20and%20Hao%20Zhang%20and%20Jihong%20Guan%20and%20Shuigeng%20Zhou&entry.1292438233=%20%20Score-based%20causal%20discovery%20methods%20can%20effectively%20identify%20causal%0Arelationships%20by%20evaluating%20candidate%20graphs%20and%20selecting%20the%20one%20with%20the%0Ahighest%20score.%20One%20popular%20class%20of%20scores%20is%20kernel-based%20generalized%20score%0Afunctions%2C%20which%20can%20adapt%20to%20a%20wide%20range%20of%20scenarios%20and%20work%20well%20in%0Apractice%20because%20they%20circumvent%20assumptions%20about%20causal%20mechanisms%20and%20data%0Adistributions.%20Despite%20these%20advantages%2C%20kernel-based%20generalized%20score%0Afunctions%20pose%20serious%20computational%20challenges%20in%20time%20and%20space%2C%20with%20a%20time%0Acomplexity%20of%20%24%5Cmathcal%7BO%7D%28n%5E3%29%24%20and%20a%20memory%20complexity%20of%20%24%5Cmathcal%7BO%7D%28n%5E2%29%24%2C%0Awhere%20%24n%24%20is%20the%20sample%20size.%20In%20this%20paper%2C%20we%20propose%20an%20approximate%0Akernel-based%20generalized%20score%20function%20with%20%24%5Cmathcal%7BO%7D%28n%29%24%20time%20and%20space%0Acomplexities%20by%20using%20low-rank%20technique%20and%20designing%20a%20set%20of%20rules%20to%20handle%0Athe%20complex%20composite%20matrix%20operations%20required%20to%20calculate%20the%20score%2C%20as%0Awell%20as%20developing%20sampling%20algorithms%20for%20different%20data%20types%20to%20benefit%20the%0Ahandling%20of%20diverse%20data%20types%20efficiently.%20Our%20extensive%20causal%20discovery%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20data%20demonstrate%20that%20compared%20to%0Athe%20state-of-the-art%20method%2C%20our%20method%20can%20not%20only%20significantly%20reduce%0Acomputational%20costs%2C%20but%20also%20achieve%20comparable%20accuracy%2C%20especially%20for%20large%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17717v1&entry.124074799=Read"},
{"title": "Sharp bounds on aggregate expert error", "author": "Aryeh Kontorovich and Ariel Avital", "abstract": "  We revisit the classic problem of aggregating binary advice from\nconditionally independent experts, also known as the Naive Bayes setting. Our\nquantity of interest is the error probability of the optimal decision rule. In\nthe case of symmetric errors (sensitivity = specificity), reasonably tight\nbounds on the optimal error probability are known. In the general asymmetric\ncase, we are not aware of any nontrivial estimates on this quantity. Our\ncontribution consists of sharp upper and lower bounds on the optimal error\nprobability in the general case, which recover and sharpen the best known\nresults in the symmetric special case. Since this turns out to be equivalent to\nestimating the total variation distance between two product distributions, our\nresults also have bearing on this important and challenging problem.\n", "link": "http://arxiv.org/abs/2407.16642v4", "date": "2024-12-23", "relevancy": 1.7219, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4164}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharp%20bounds%20on%20aggregate%20expert%20error&body=Title%3A%20Sharp%20bounds%20on%20aggregate%20expert%20error%0AAuthor%3A%20Aryeh%20Kontorovich%20and%20Ariel%20Avital%0AAbstract%3A%20%20%20We%20revisit%20the%20classic%20problem%20of%20aggregating%20binary%20advice%20from%0Aconditionally%20independent%20experts%2C%20also%20known%20as%20the%20Naive%20Bayes%20setting.%20Our%0Aquantity%20of%20interest%20is%20the%20error%20probability%20of%20the%20optimal%20decision%20rule.%20In%0Athe%20case%20of%20symmetric%20errors%20%28sensitivity%20%3D%20specificity%29%2C%20reasonably%20tight%0Abounds%20on%20the%20optimal%20error%20probability%20are%20known.%20In%20the%20general%20asymmetric%0Acase%2C%20we%20are%20not%20aware%20of%20any%20nontrivial%20estimates%20on%20this%20quantity.%20Our%0Acontribution%20consists%20of%20sharp%20upper%20and%20lower%20bounds%20on%20the%20optimal%20error%0Aprobability%20in%20the%20general%20case%2C%20which%20recover%20and%20sharpen%20the%20best%20known%0Aresults%20in%20the%20symmetric%20special%20case.%20Since%20this%20turns%20out%20to%20be%20equivalent%20to%0Aestimating%20the%20total%20variation%20distance%20between%20two%20product%20distributions%2C%20our%0Aresults%20also%20have%20bearing%20on%20this%20important%20and%20challenging%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16642v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharp%2520bounds%2520on%2520aggregate%2520expert%2520error%26entry.906535625%3DAryeh%2520Kontorovich%2520and%2520Ariel%2520Avital%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520classic%2520problem%2520of%2520aggregating%2520binary%2520advice%2520from%250Aconditionally%2520independent%2520experts%252C%2520also%2520known%2520as%2520the%2520Naive%2520Bayes%2520setting.%2520Our%250Aquantity%2520of%2520interest%2520is%2520the%2520error%2520probability%2520of%2520the%2520optimal%2520decision%2520rule.%2520In%250Athe%2520case%2520of%2520symmetric%2520errors%2520%2528sensitivity%2520%253D%2520specificity%2529%252C%2520reasonably%2520tight%250Abounds%2520on%2520the%2520optimal%2520error%2520probability%2520are%2520known.%2520In%2520the%2520general%2520asymmetric%250Acase%252C%2520we%2520are%2520not%2520aware%2520of%2520any%2520nontrivial%2520estimates%2520on%2520this%2520quantity.%2520Our%250Acontribution%2520consists%2520of%2520sharp%2520upper%2520and%2520lower%2520bounds%2520on%2520the%2520optimal%2520error%250Aprobability%2520in%2520the%2520general%2520case%252C%2520which%2520recover%2520and%2520sharpen%2520the%2520best%2520known%250Aresults%2520in%2520the%2520symmetric%2520special%2520case.%2520Since%2520this%2520turns%2520out%2520to%2520be%2520equivalent%2520to%250Aestimating%2520the%2520total%2520variation%2520distance%2520between%2520two%2520product%2520distributions%252C%2520our%250Aresults%2520also%2520have%2520bearing%2520on%2520this%2520important%2520and%2520challenging%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16642v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20bounds%20on%20aggregate%20expert%20error&entry.906535625=Aryeh%20Kontorovich%20and%20Ariel%20Avital&entry.1292438233=%20%20We%20revisit%20the%20classic%20problem%20of%20aggregating%20binary%20advice%20from%0Aconditionally%20independent%20experts%2C%20also%20known%20as%20the%20Naive%20Bayes%20setting.%20Our%0Aquantity%20of%20interest%20is%20the%20error%20probability%20of%20the%20optimal%20decision%20rule.%20In%0Athe%20case%20of%20symmetric%20errors%20%28sensitivity%20%3D%20specificity%29%2C%20reasonably%20tight%0Abounds%20on%20the%20optimal%20error%20probability%20are%20known.%20In%20the%20general%20asymmetric%0Acase%2C%20we%20are%20not%20aware%20of%20any%20nontrivial%20estimates%20on%20this%20quantity.%20Our%0Acontribution%20consists%20of%20sharp%20upper%20and%20lower%20bounds%20on%20the%20optimal%20error%0Aprobability%20in%20the%20general%20case%2C%20which%20recover%20and%20sharpen%20the%20best%20known%0Aresults%20in%20the%20symmetric%20special%20case.%20Since%20this%20turns%20out%20to%20be%20equivalent%20to%0Aestimating%20the%20total%20variation%20distance%20between%20two%20product%20distributions%2C%20our%0Aresults%20also%20have%20bearing%20on%20this%20important%20and%20challenging%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16642v4&entry.124074799=Read"},
{"title": "The Superposition of Diffusion Models Using the It\u00f4 Density Estimator", "author": "Marta Skreta and Lazar Atanackovic and Avishek Joey Bose and Alexander Tong and Kirill Neklyudov", "abstract": "  The Cambrian explosion of easily accessible pre-trained diffusion models\nsuggests a demand for methods that combine multiple different pre-trained\ndiffusion models without incurring the significant computational burden of\nre-training a larger combined model. In this paper, we cast the problem of\ncombining multiple pre-trained diffusion models at the generation stage under a\nnovel proposed framework termed superposition. Theoretically, we derive\nsuperposition from rigorous first principles stemming from the celebrated\ncontinuity equation and design two novel algorithms tailor-made for combining\ndiffusion models in SuperDiff. SuperDiff leverages a new scalable It\\^o density\nestimator for the log likelihood of the diffusion SDE which incurs no\nadditional overhead compared to the well-known Hutchinson's estimator needed\nfor divergence calculations. We demonstrate that SuperDiff is scalable to large\npre-trained diffusion models as superposition is performed solely through\ncomposition during inference, and also enjoys painless implementation as it\ncombines different pre-trained vector fields through an automated re-weighting\nscheme. Notably, we show that SuperDiff is efficient during inference time, and\nmimics traditional composition operators such as the logical OR and the logical\nAND. We empirically demonstrate the utility of using SuperDiff for generating\nmore diverse images on CIFAR-10, more faithful prompt conditioned image editing\nusing Stable Diffusion, and improved unconditional de novo structure design of\nproteins. https://github.com/necludov/super-diffusion\n", "link": "http://arxiv.org/abs/2412.17762v1", "date": "2024-12-23", "relevancy": 1.2388, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6636}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.599}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Superposition%20of%20Diffusion%20Models%20Using%20the%20It%C3%B4%20Density%20Estimator&body=Title%3A%20The%20Superposition%20of%20Diffusion%20Models%20Using%20the%20It%C3%B4%20Density%20Estimator%0AAuthor%3A%20Marta%20Skreta%20and%20Lazar%20Atanackovic%20and%20Avishek%20Joey%20Bose%20and%20Alexander%20Tong%20and%20Kirill%20Neklyudov%0AAbstract%3A%20%20%20The%20Cambrian%20explosion%20of%20easily%20accessible%20pre-trained%20diffusion%20models%0Asuggests%20a%20demand%20for%20methods%20that%20combine%20multiple%20different%20pre-trained%0Adiffusion%20models%20without%20incurring%20the%20significant%20computational%20burden%20of%0Are-training%20a%20larger%20combined%20model.%20In%20this%20paper%2C%20we%20cast%20the%20problem%20of%0Acombining%20multiple%20pre-trained%20diffusion%20models%20at%20the%20generation%20stage%20under%20a%0Anovel%20proposed%20framework%20termed%20superposition.%20Theoretically%2C%20we%20derive%0Asuperposition%20from%20rigorous%20first%20principles%20stemming%20from%20the%20celebrated%0Acontinuity%20equation%20and%20design%20two%20novel%20algorithms%20tailor-made%20for%20combining%0Adiffusion%20models%20in%20SuperDiff.%20SuperDiff%20leverages%20a%20new%20scalable%20It%5C%5Eo%20density%0Aestimator%20for%20the%20log%20likelihood%20of%20the%20diffusion%20SDE%20which%20incurs%20no%0Aadditional%20overhead%20compared%20to%20the%20well-known%20Hutchinson%27s%20estimator%20needed%0Afor%20divergence%20calculations.%20We%20demonstrate%20that%20SuperDiff%20is%20scalable%20to%20large%0Apre-trained%20diffusion%20models%20as%20superposition%20is%20performed%20solely%20through%0Acomposition%20during%20inference%2C%20and%20also%20enjoys%20painless%20implementation%20as%20it%0Acombines%20different%20pre-trained%20vector%20fields%20through%20an%20automated%20re-weighting%0Ascheme.%20Notably%2C%20we%20show%20that%20SuperDiff%20is%20efficient%20during%20inference%20time%2C%20and%0Amimics%20traditional%20composition%20operators%20such%20as%20the%20logical%20OR%20and%20the%20logical%0AAND.%20We%20empirically%20demonstrate%20the%20utility%20of%20using%20SuperDiff%20for%20generating%0Amore%20diverse%20images%20on%20CIFAR-10%2C%20more%20faithful%20prompt%20conditioned%20image%20editing%0Ausing%20Stable%20Diffusion%2C%20and%20improved%20unconditional%20de%20novo%20structure%20design%20of%0Aproteins.%20https%3A//github.com/necludov/super-diffusion%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Superposition%2520of%2520Diffusion%2520Models%2520Using%2520the%2520It%25C3%25B4%2520Density%2520Estimator%26entry.906535625%3DMarta%2520Skreta%2520and%2520Lazar%2520Atanackovic%2520and%2520Avishek%2520Joey%2520Bose%2520and%2520Alexander%2520Tong%2520and%2520Kirill%2520Neklyudov%26entry.1292438233%3D%2520%2520The%2520Cambrian%2520explosion%2520of%2520easily%2520accessible%2520pre-trained%2520diffusion%2520models%250Asuggests%2520a%2520demand%2520for%2520methods%2520that%2520combine%2520multiple%2520different%2520pre-trained%250Adiffusion%2520models%2520without%2520incurring%2520the%2520significant%2520computational%2520burden%2520of%250Are-training%2520a%2520larger%2520combined%2520model.%2520In%2520this%2520paper%252C%2520we%2520cast%2520the%2520problem%2520of%250Acombining%2520multiple%2520pre-trained%2520diffusion%2520models%2520at%2520the%2520generation%2520stage%2520under%2520a%250Anovel%2520proposed%2520framework%2520termed%2520superposition.%2520Theoretically%252C%2520we%2520derive%250Asuperposition%2520from%2520rigorous%2520first%2520principles%2520stemming%2520from%2520the%2520celebrated%250Acontinuity%2520equation%2520and%2520design%2520two%2520novel%2520algorithms%2520tailor-made%2520for%2520combining%250Adiffusion%2520models%2520in%2520SuperDiff.%2520SuperDiff%2520leverages%2520a%2520new%2520scalable%2520It%255C%255Eo%2520density%250Aestimator%2520for%2520the%2520log%2520likelihood%2520of%2520the%2520diffusion%2520SDE%2520which%2520incurs%2520no%250Aadditional%2520overhead%2520compared%2520to%2520the%2520well-known%2520Hutchinson%2527s%2520estimator%2520needed%250Afor%2520divergence%2520calculations.%2520We%2520demonstrate%2520that%2520SuperDiff%2520is%2520scalable%2520to%2520large%250Apre-trained%2520diffusion%2520models%2520as%2520superposition%2520is%2520performed%2520solely%2520through%250Acomposition%2520during%2520inference%252C%2520and%2520also%2520enjoys%2520painless%2520implementation%2520as%2520it%250Acombines%2520different%2520pre-trained%2520vector%2520fields%2520through%2520an%2520automated%2520re-weighting%250Ascheme.%2520Notably%252C%2520we%2520show%2520that%2520SuperDiff%2520is%2520efficient%2520during%2520inference%2520time%252C%2520and%250Amimics%2520traditional%2520composition%2520operators%2520such%2520as%2520the%2520logical%2520OR%2520and%2520the%2520logical%250AAND.%2520We%2520empirically%2520demonstrate%2520the%2520utility%2520of%2520using%2520SuperDiff%2520for%2520generating%250Amore%2520diverse%2520images%2520on%2520CIFAR-10%252C%2520more%2520faithful%2520prompt%2520conditioned%2520image%2520editing%250Ausing%2520Stable%2520Diffusion%252C%2520and%2520improved%2520unconditional%2520de%2520novo%2520structure%2520design%2520of%250Aproteins.%2520https%253A//github.com/necludov/super-diffusion%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Superposition%20of%20Diffusion%20Models%20Using%20the%20It%C3%B4%20Density%20Estimator&entry.906535625=Marta%20Skreta%20and%20Lazar%20Atanackovic%20and%20Avishek%20Joey%20Bose%20and%20Alexander%20Tong%20and%20Kirill%20Neklyudov&entry.1292438233=%20%20The%20Cambrian%20explosion%20of%20easily%20accessible%20pre-trained%20diffusion%20models%0Asuggests%20a%20demand%20for%20methods%20that%20combine%20multiple%20different%20pre-trained%0Adiffusion%20models%20without%20incurring%20the%20significant%20computational%20burden%20of%0Are-training%20a%20larger%20combined%20model.%20In%20this%20paper%2C%20we%20cast%20the%20problem%20of%0Acombining%20multiple%20pre-trained%20diffusion%20models%20at%20the%20generation%20stage%20under%20a%0Anovel%20proposed%20framework%20termed%20superposition.%20Theoretically%2C%20we%20derive%0Asuperposition%20from%20rigorous%20first%20principles%20stemming%20from%20the%20celebrated%0Acontinuity%20equation%20and%20design%20two%20novel%20algorithms%20tailor-made%20for%20combining%0Adiffusion%20models%20in%20SuperDiff.%20SuperDiff%20leverages%20a%20new%20scalable%20It%5C%5Eo%20density%0Aestimator%20for%20the%20log%20likelihood%20of%20the%20diffusion%20SDE%20which%20incurs%20no%0Aadditional%20overhead%20compared%20to%20the%20well-known%20Hutchinson%27s%20estimator%20needed%0Afor%20divergence%20calculations.%20We%20demonstrate%20that%20SuperDiff%20is%20scalable%20to%20large%0Apre-trained%20diffusion%20models%20as%20superposition%20is%20performed%20solely%20through%0Acomposition%20during%20inference%2C%20and%20also%20enjoys%20painless%20implementation%20as%20it%0Acombines%20different%20pre-trained%20vector%20fields%20through%20an%20automated%20re-weighting%0Ascheme.%20Notably%2C%20we%20show%20that%20SuperDiff%20is%20efficient%20during%20inference%20time%2C%20and%0Amimics%20traditional%20composition%20operators%20such%20as%20the%20logical%20OR%20and%20the%20logical%0AAND.%20We%20empirically%20demonstrate%20the%20utility%20of%20using%20SuperDiff%20for%20generating%0Amore%20diverse%20images%20on%20CIFAR-10%2C%20more%20faithful%20prompt%20conditioned%20image%20editing%0Ausing%20Stable%20Diffusion%2C%20and%20improved%20unconditional%20de%20novo%20structure%20design%20of%0Aproteins.%20https%3A//github.com/necludov/super-diffusion%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17762v1&entry.124074799=Read"},
{"title": "Are Self-Attentions Effective for Time Series Forecasting?", "author": "Dongbin Kim and Jinseong Park and Jaewook Lee and Hoki Kim", "abstract": "  Time series forecasting is crucial for applications across multiple domains\nand various scenarios. Although Transformer models have dramatically advanced\nthe landscape of forecasting, their effectiveness remains debated. Recent\nfindings have indicated that simpler linear models might outperform complex\nTransformer-based approaches, highlighting the potential for more streamlined\narchitectures. In this paper, we shift the focus from evaluating the overall\nTransformer architecture to specifically examining the effectiveness of\nself-attention for time series forecasting. To this end, we introduce a new\narchitecture, Cross-Attention-only Time Series transformer (CATS), that\nrethinks the traditional Transformer framework by eliminating self-attention\nand leveraging cross-attention mechanisms instead. By establishing future\nhorizon-dependent parameters as queries and enhanced parameter sharing, our\nmodel not only improves long-term forecasting accuracy but also reduces the\nnumber of parameters and memory usage. Extensive experiment across various\ndatasets demonstrates that our model achieves superior performance with the\nlowest mean squared error and uses fewer parameters compared to existing\nmodels. The implementation of our model is available at:\nhttps://github.com/dongbeank/CATS.\n", "link": "http://arxiv.org/abs/2405.16877v3", "date": "2024-12-23", "relevancy": 1.9679, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5035}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4958}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Self-Attentions%20Effective%20for%20Time%20Series%20Forecasting%3F&body=Title%3A%20Are%20Self-Attentions%20Effective%20for%20Time%20Series%20Forecasting%3F%0AAuthor%3A%20Dongbin%20Kim%20and%20Jinseong%20Park%20and%20Jaewook%20Lee%20and%20Hoki%20Kim%0AAbstract%3A%20%20%20Time%20series%20forecasting%20is%20crucial%20for%20applications%20across%20multiple%20domains%0Aand%20various%20scenarios.%20Although%20Transformer%20models%20have%20dramatically%20advanced%0Athe%20landscape%20of%20forecasting%2C%20their%20effectiveness%20remains%20debated.%20Recent%0Afindings%20have%20indicated%20that%20simpler%20linear%20models%20might%20outperform%20complex%0ATransformer-based%20approaches%2C%20highlighting%20the%20potential%20for%20more%20streamlined%0Aarchitectures.%20In%20this%20paper%2C%20we%20shift%20the%20focus%20from%20evaluating%20the%20overall%0ATransformer%20architecture%20to%20specifically%20examining%20the%20effectiveness%20of%0Aself-attention%20for%20time%20series%20forecasting.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Aarchitecture%2C%20Cross-Attention-only%20Time%20Series%20transformer%20%28CATS%29%2C%20that%0Arethinks%20the%20traditional%20Transformer%20framework%20by%20eliminating%20self-attention%0Aand%20leveraging%20cross-attention%20mechanisms%20instead.%20By%20establishing%20future%0Ahorizon-dependent%20parameters%20as%20queries%20and%20enhanced%20parameter%20sharing%2C%20our%0Amodel%20not%20only%20improves%20long-term%20forecasting%20accuracy%20but%20also%20reduces%20the%0Anumber%20of%20parameters%20and%20memory%20usage.%20Extensive%20experiment%20across%20various%0Adatasets%20demonstrates%20that%20our%20model%20achieves%20superior%20performance%20with%20the%0Alowest%20mean%20squared%20error%20and%20uses%20fewer%20parameters%20compared%20to%20existing%0Amodels.%20The%20implementation%20of%20our%20model%20is%20available%20at%3A%0Ahttps%3A//github.com/dongbeank/CATS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16877v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Self-Attentions%2520Effective%2520for%2520Time%2520Series%2520Forecasting%253F%26entry.906535625%3DDongbin%2520Kim%2520and%2520Jinseong%2520Park%2520and%2520Jaewook%2520Lee%2520and%2520Hoki%2520Kim%26entry.1292438233%3D%2520%2520Time%2520series%2520forecasting%2520is%2520crucial%2520for%2520applications%2520across%2520multiple%2520domains%250Aand%2520various%2520scenarios.%2520Although%2520Transformer%2520models%2520have%2520dramatically%2520advanced%250Athe%2520landscape%2520of%2520forecasting%252C%2520their%2520effectiveness%2520remains%2520debated.%2520Recent%250Afindings%2520have%2520indicated%2520that%2520simpler%2520linear%2520models%2520might%2520outperform%2520complex%250ATransformer-based%2520approaches%252C%2520highlighting%2520the%2520potential%2520for%2520more%2520streamlined%250Aarchitectures.%2520In%2520this%2520paper%252C%2520we%2520shift%2520the%2520focus%2520from%2520evaluating%2520the%2520overall%250ATransformer%2520architecture%2520to%2520specifically%2520examining%2520the%2520effectiveness%2520of%250Aself-attention%2520for%2520time%2520series%2520forecasting.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520new%250Aarchitecture%252C%2520Cross-Attention-only%2520Time%2520Series%2520transformer%2520%2528CATS%2529%252C%2520that%250Arethinks%2520the%2520traditional%2520Transformer%2520framework%2520by%2520eliminating%2520self-attention%250Aand%2520leveraging%2520cross-attention%2520mechanisms%2520instead.%2520By%2520establishing%2520future%250Ahorizon-dependent%2520parameters%2520as%2520queries%2520and%2520enhanced%2520parameter%2520sharing%252C%2520our%250Amodel%2520not%2520only%2520improves%2520long-term%2520forecasting%2520accuracy%2520but%2520also%2520reduces%2520the%250Anumber%2520of%2520parameters%2520and%2520memory%2520usage.%2520Extensive%2520experiment%2520across%2520various%250Adatasets%2520demonstrates%2520that%2520our%2520model%2520achieves%2520superior%2520performance%2520with%2520the%250Alowest%2520mean%2520squared%2520error%2520and%2520uses%2520fewer%2520parameters%2520compared%2520to%2520existing%250Amodels.%2520The%2520implementation%2520of%2520our%2520model%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/dongbeank/CATS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16877v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Self-Attentions%20Effective%20for%20Time%20Series%20Forecasting%3F&entry.906535625=Dongbin%20Kim%20and%20Jinseong%20Park%20and%20Jaewook%20Lee%20and%20Hoki%20Kim&entry.1292438233=%20%20Time%20series%20forecasting%20is%20crucial%20for%20applications%20across%20multiple%20domains%0Aand%20various%20scenarios.%20Although%20Transformer%20models%20have%20dramatically%20advanced%0Athe%20landscape%20of%20forecasting%2C%20their%20effectiveness%20remains%20debated.%20Recent%0Afindings%20have%20indicated%20that%20simpler%20linear%20models%20might%20outperform%20complex%0ATransformer-based%20approaches%2C%20highlighting%20the%20potential%20for%20more%20streamlined%0Aarchitectures.%20In%20this%20paper%2C%20we%20shift%20the%20focus%20from%20evaluating%20the%20overall%0ATransformer%20architecture%20to%20specifically%20examining%20the%20effectiveness%20of%0Aself-attention%20for%20time%20series%20forecasting.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Aarchitecture%2C%20Cross-Attention-only%20Time%20Series%20transformer%20%28CATS%29%2C%20that%0Arethinks%20the%20traditional%20Transformer%20framework%20by%20eliminating%20self-attention%0Aand%20leveraging%20cross-attention%20mechanisms%20instead.%20By%20establishing%20future%0Ahorizon-dependent%20parameters%20as%20queries%20and%20enhanced%20parameter%20sharing%2C%20our%0Amodel%20not%20only%20improves%20long-term%20forecasting%20accuracy%20but%20also%20reduces%20the%0Anumber%20of%20parameters%20and%20memory%20usage.%20Extensive%20experiment%20across%20various%0Adatasets%20demonstrates%20that%20our%20model%20achieves%20superior%20performance%20with%20the%0Alowest%20mean%20squared%20error%20and%20uses%20fewer%20parameters%20compared%20to%20existing%0Amodels.%20The%20implementation%20of%20our%20model%20is%20available%20at%3A%0Ahttps%3A//github.com/dongbeank/CATS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16877v3&entry.124074799=Read"},
{"title": "Chumor 2.0: Towards Benchmarking Chinese Humor Understanding", "author": "Ruiqi He and Yushu He and Longju Bai and Jiarui Liu and Zhenjie Sun and Zenghao Tang and He Wang and Hanchen Xia and Rada Mihalcea and Naihao Deng", "abstract": "  Existing humor datasets and evaluations predominantly focus on English,\nleaving limited resources for culturally nuanced humor in non-English languages\nlike Chinese. To address this gap, we construct Chumor, the first Chinese humor\nexplanation dataset that exceeds the size of existing humor datasets. Chumor is\nsourced from Ruo Zhi Ba, a Chinese Reddit-like platform known for sharing\nintellectually challenging and culturally specific jokes. We test ten LLMs\nthrough direct and chain-of-thought prompting, revealing that Chumor poses\nsignificant challenges to existing LLMs, with their accuracy slightly above\nrandom and far below human. In addition, our analysis highlights that\nhuman-annotated humor explanations are significantly better than those\ngenerated by GPT-4o and ERNIE-4-turbo. We release Chumor at\nhttps://huggingface.co/datasets/dnaihao/Chumor, our project page is at\nhttps://dnaihao.github.io/Chumor-dataset/, our leaderboard is at\nhttps://huggingface.co/spaces/dnaihao/Chumor, and our codebase is at\nhttps://github.com/dnaihao/Chumor-dataset.\n", "link": "http://arxiv.org/abs/2412.17729v1", "date": "2024-12-23", "relevancy": 2.0657, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chumor%202.0%3A%20Towards%20Benchmarking%20Chinese%20Humor%20Understanding&body=Title%3A%20Chumor%202.0%3A%20Towards%20Benchmarking%20Chinese%20Humor%20Understanding%0AAuthor%3A%20Ruiqi%20He%20and%20Yushu%20He%20and%20Longju%20Bai%20and%20Jiarui%20Liu%20and%20Zhenjie%20Sun%20and%20Zenghao%20Tang%20and%20He%20Wang%20and%20Hanchen%20Xia%20and%20Rada%20Mihalcea%20and%20Naihao%20Deng%0AAbstract%3A%20%20%20Existing%20humor%20datasets%20and%20evaluations%20predominantly%20focus%20on%20English%2C%0Aleaving%20limited%20resources%20for%20culturally%20nuanced%20humor%20in%20non-English%20languages%0Alike%20Chinese.%20To%20address%20this%20gap%2C%20we%20construct%20Chumor%2C%20the%20first%20Chinese%20humor%0Aexplanation%20dataset%20that%20exceeds%20the%20size%20of%20existing%20humor%20datasets.%20Chumor%20is%0Asourced%20from%20Ruo%20Zhi%20Ba%2C%20a%20Chinese%20Reddit-like%20platform%20known%20for%20sharing%0Aintellectually%20challenging%20and%20culturally%20specific%20jokes.%20We%20test%20ten%20LLMs%0Athrough%20direct%20and%20chain-of-thought%20prompting%2C%20revealing%20that%20Chumor%20poses%0Asignificant%20challenges%20to%20existing%20LLMs%2C%20with%20their%20accuracy%20slightly%20above%0Arandom%20and%20far%20below%20human.%20In%20addition%2C%20our%20analysis%20highlights%20that%0Ahuman-annotated%20humor%20explanations%20are%20significantly%20better%20than%20those%0Agenerated%20by%20GPT-4o%20and%20ERNIE-4-turbo.%20We%20release%20Chumor%20at%0Ahttps%3A//huggingface.co/datasets/dnaihao/Chumor%2C%20our%20project%20page%20is%20at%0Ahttps%3A//dnaihao.github.io/Chumor-dataset/%2C%20our%20leaderboard%20is%20at%0Ahttps%3A//huggingface.co/spaces/dnaihao/Chumor%2C%20and%20our%20codebase%20is%20at%0Ahttps%3A//github.com/dnaihao/Chumor-dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChumor%25202.0%253A%2520Towards%2520Benchmarking%2520Chinese%2520Humor%2520Understanding%26entry.906535625%3DRuiqi%2520He%2520and%2520Yushu%2520He%2520and%2520Longju%2520Bai%2520and%2520Jiarui%2520Liu%2520and%2520Zhenjie%2520Sun%2520and%2520Zenghao%2520Tang%2520and%2520He%2520Wang%2520and%2520Hanchen%2520Xia%2520and%2520Rada%2520Mihalcea%2520and%2520Naihao%2520Deng%26entry.1292438233%3D%2520%2520Existing%2520humor%2520datasets%2520and%2520evaluations%2520predominantly%2520focus%2520on%2520English%252C%250Aleaving%2520limited%2520resources%2520for%2520culturally%2520nuanced%2520humor%2520in%2520non-English%2520languages%250Alike%2520Chinese.%2520To%2520address%2520this%2520gap%252C%2520we%2520construct%2520Chumor%252C%2520the%2520first%2520Chinese%2520humor%250Aexplanation%2520dataset%2520that%2520exceeds%2520the%2520size%2520of%2520existing%2520humor%2520datasets.%2520Chumor%2520is%250Asourced%2520from%2520Ruo%2520Zhi%2520Ba%252C%2520a%2520Chinese%2520Reddit-like%2520platform%2520known%2520for%2520sharing%250Aintellectually%2520challenging%2520and%2520culturally%2520specific%2520jokes.%2520We%2520test%2520ten%2520LLMs%250Athrough%2520direct%2520and%2520chain-of-thought%2520prompting%252C%2520revealing%2520that%2520Chumor%2520poses%250Asignificant%2520challenges%2520to%2520existing%2520LLMs%252C%2520with%2520their%2520accuracy%2520slightly%2520above%250Arandom%2520and%2520far%2520below%2520human.%2520In%2520addition%252C%2520our%2520analysis%2520highlights%2520that%250Ahuman-annotated%2520humor%2520explanations%2520are%2520significantly%2520better%2520than%2520those%250Agenerated%2520by%2520GPT-4o%2520and%2520ERNIE-4-turbo.%2520We%2520release%2520Chumor%2520at%250Ahttps%253A//huggingface.co/datasets/dnaihao/Chumor%252C%2520our%2520project%2520page%2520is%2520at%250Ahttps%253A//dnaihao.github.io/Chumor-dataset/%252C%2520our%2520leaderboard%2520is%2520at%250Ahttps%253A//huggingface.co/spaces/dnaihao/Chumor%252C%2520and%2520our%2520codebase%2520is%2520at%250Ahttps%253A//github.com/dnaihao/Chumor-dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chumor%202.0%3A%20Towards%20Benchmarking%20Chinese%20Humor%20Understanding&entry.906535625=Ruiqi%20He%20and%20Yushu%20He%20and%20Longju%20Bai%20and%20Jiarui%20Liu%20and%20Zhenjie%20Sun%20and%20Zenghao%20Tang%20and%20He%20Wang%20and%20Hanchen%20Xia%20and%20Rada%20Mihalcea%20and%20Naihao%20Deng&entry.1292438233=%20%20Existing%20humor%20datasets%20and%20evaluations%20predominantly%20focus%20on%20English%2C%0Aleaving%20limited%20resources%20for%20culturally%20nuanced%20humor%20in%20non-English%20languages%0Alike%20Chinese.%20To%20address%20this%20gap%2C%20we%20construct%20Chumor%2C%20the%20first%20Chinese%20humor%0Aexplanation%20dataset%20that%20exceeds%20the%20size%20of%20existing%20humor%20datasets.%20Chumor%20is%0Asourced%20from%20Ruo%20Zhi%20Ba%2C%20a%20Chinese%20Reddit-like%20platform%20known%20for%20sharing%0Aintellectually%20challenging%20and%20culturally%20specific%20jokes.%20We%20test%20ten%20LLMs%0Athrough%20direct%20and%20chain-of-thought%20prompting%2C%20revealing%20that%20Chumor%20poses%0Asignificant%20challenges%20to%20existing%20LLMs%2C%20with%20their%20accuracy%20slightly%20above%0Arandom%20and%20far%20below%20human.%20In%20addition%2C%20our%20analysis%20highlights%20that%0Ahuman-annotated%20humor%20explanations%20are%20significantly%20better%20than%20those%0Agenerated%20by%20GPT-4o%20and%20ERNIE-4-turbo.%20We%20release%20Chumor%20at%0Ahttps%3A//huggingface.co/datasets/dnaihao/Chumor%2C%20our%20project%20page%20is%20at%0Ahttps%3A//dnaihao.github.io/Chumor-dataset/%2C%20our%20leaderboard%20is%20at%0Ahttps%3A//huggingface.co/spaces/dnaihao/Chumor%2C%20and%20our%20codebase%20is%20at%0Ahttps%3A//github.com/dnaihao/Chumor-dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17729v1&entry.124074799=Read"},
{"title": "Detecting anxiety and depression in dialogues: a multi-label and\n  explainable approach", "author": "Francisco de Arriba-P\u00e9rez and Silvia Garc\u00eda-M\u00e9ndez", "abstract": "  Anxiety and depression are the most common mental health issues worldwide,\naffecting a non-negligible part of the population. Accordingly, stakeholders,\nincluding governments' health systems, are developing new strategies to promote\nearly detection and prevention from a holistic perspective (i.e., addressing\nseveral disorders simultaneously). In this work, an entirely novel system for\nthe multi-label classification of anxiety and depression is proposed. The input\ndata consists of dialogues from user interactions with an assistant chatbot.\nAnother relevant contribution lies in using Large Language Models (LLMs) for\nfeature extraction, provided the complexity and variability of language. The\ncombination of LLMs, given their high capability for language understanding,\nand Machine Learning (ML) models, provided their contextual knowledge about the\nclassification problem thanks to the labeled data, constitute a promising\napproach towards mental health assessment. To promote the solution's\ntrustworthiness, reliability, and accountability, explainability descriptions\nof the model's decision are provided in a graphical dashboard. Experimental\nresults on a real dataset attain 90 % accuracy, improving those in the prior\nliterature. The ultimate objective is to contribute in an accessible and\nscalable way before formal treatment occurs in the healthcare systems.\n", "link": "http://arxiv.org/abs/2412.17651v1", "date": "2024-12-23", "relevancy": 1.3596, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5119}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20anxiety%20and%20depression%20in%20dialogues%3A%20a%20multi-label%20and%0A%20%20explainable%20approach&body=Title%3A%20Detecting%20anxiety%20and%20depression%20in%20dialogues%3A%20a%20multi-label%20and%0A%20%20explainable%20approach%0AAuthor%3A%20Francisco%20de%20Arriba-P%C3%A9rez%20and%20Silvia%20Garc%C3%ADa-M%C3%A9ndez%0AAbstract%3A%20%20%20Anxiety%20and%20depression%20are%20the%20most%20common%20mental%20health%20issues%20worldwide%2C%0Aaffecting%20a%20non-negligible%20part%20of%20the%20population.%20Accordingly%2C%20stakeholders%2C%0Aincluding%20governments%27%20health%20systems%2C%20are%20developing%20new%20strategies%20to%20promote%0Aearly%20detection%20and%20prevention%20from%20a%20holistic%20perspective%20%28i.e.%2C%20addressing%0Aseveral%20disorders%20simultaneously%29.%20In%20this%20work%2C%20an%20entirely%20novel%20system%20for%0Athe%20multi-label%20classification%20of%20anxiety%20and%20depression%20is%20proposed.%20The%20input%0Adata%20consists%20of%20dialogues%20from%20user%20interactions%20with%20an%20assistant%20chatbot.%0AAnother%20relevant%20contribution%20lies%20in%20using%20Large%20Language%20Models%20%28LLMs%29%20for%0Afeature%20extraction%2C%20provided%20the%20complexity%20and%20variability%20of%20language.%20The%0Acombination%20of%20LLMs%2C%20given%20their%20high%20capability%20for%20language%20understanding%2C%0Aand%20Machine%20Learning%20%28ML%29%20models%2C%20provided%20their%20contextual%20knowledge%20about%20the%0Aclassification%20problem%20thanks%20to%20the%20labeled%20data%2C%20constitute%20a%20promising%0Aapproach%20towards%20mental%20health%20assessment.%20To%20promote%20the%20solution%27s%0Atrustworthiness%2C%20reliability%2C%20and%20accountability%2C%20explainability%20descriptions%0Aof%20the%20model%27s%20decision%20are%20provided%20in%20a%20graphical%20dashboard.%20Experimental%0Aresults%20on%20a%20real%20dataset%20attain%2090%20%25%20accuracy%2C%20improving%20those%20in%20the%20prior%0Aliterature.%20The%20ultimate%20objective%20is%20to%20contribute%20in%20an%20accessible%20and%0Ascalable%20way%20before%20formal%20treatment%20occurs%20in%20the%20healthcare%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520anxiety%2520and%2520depression%2520in%2520dialogues%253A%2520a%2520multi-label%2520and%250A%2520%2520explainable%2520approach%26entry.906535625%3DFrancisco%2520de%2520Arriba-P%25C3%25A9rez%2520and%2520Silvia%2520Garc%25C3%25ADa-M%25C3%25A9ndez%26entry.1292438233%3D%2520%2520Anxiety%2520and%2520depression%2520are%2520the%2520most%2520common%2520mental%2520health%2520issues%2520worldwide%252C%250Aaffecting%2520a%2520non-negligible%2520part%2520of%2520the%2520population.%2520Accordingly%252C%2520stakeholders%252C%250Aincluding%2520governments%2527%2520health%2520systems%252C%2520are%2520developing%2520new%2520strategies%2520to%2520promote%250Aearly%2520detection%2520and%2520prevention%2520from%2520a%2520holistic%2520perspective%2520%2528i.e.%252C%2520addressing%250Aseveral%2520disorders%2520simultaneously%2529.%2520In%2520this%2520work%252C%2520an%2520entirely%2520novel%2520system%2520for%250Athe%2520multi-label%2520classification%2520of%2520anxiety%2520and%2520depression%2520is%2520proposed.%2520The%2520input%250Adata%2520consists%2520of%2520dialogues%2520from%2520user%2520interactions%2520with%2520an%2520assistant%2520chatbot.%250AAnother%2520relevant%2520contribution%2520lies%2520in%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%250Afeature%2520extraction%252C%2520provided%2520the%2520complexity%2520and%2520variability%2520of%2520language.%2520The%250Acombination%2520of%2520LLMs%252C%2520given%2520their%2520high%2520capability%2520for%2520language%2520understanding%252C%250Aand%2520Machine%2520Learning%2520%2528ML%2529%2520models%252C%2520provided%2520their%2520contextual%2520knowledge%2520about%2520the%250Aclassification%2520problem%2520thanks%2520to%2520the%2520labeled%2520data%252C%2520constitute%2520a%2520promising%250Aapproach%2520towards%2520mental%2520health%2520assessment.%2520To%2520promote%2520the%2520solution%2527s%250Atrustworthiness%252C%2520reliability%252C%2520and%2520accountability%252C%2520explainability%2520descriptions%250Aof%2520the%2520model%2527s%2520decision%2520are%2520provided%2520in%2520a%2520graphical%2520dashboard.%2520Experimental%250Aresults%2520on%2520a%2520real%2520dataset%2520attain%252090%2520%2525%2520accuracy%252C%2520improving%2520those%2520in%2520the%2520prior%250Aliterature.%2520The%2520ultimate%2520objective%2520is%2520to%2520contribute%2520in%2520an%2520accessible%2520and%250Ascalable%2520way%2520before%2520formal%2520treatment%2520occurs%2520in%2520the%2520healthcare%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20anxiety%20and%20depression%20in%20dialogues%3A%20a%20multi-label%20and%0A%20%20explainable%20approach&entry.906535625=Francisco%20de%20Arriba-P%C3%A9rez%20and%20Silvia%20Garc%C3%ADa-M%C3%A9ndez&entry.1292438233=%20%20Anxiety%20and%20depression%20are%20the%20most%20common%20mental%20health%20issues%20worldwide%2C%0Aaffecting%20a%20non-negligible%20part%20of%20the%20population.%20Accordingly%2C%20stakeholders%2C%0Aincluding%20governments%27%20health%20systems%2C%20are%20developing%20new%20strategies%20to%20promote%0Aearly%20detection%20and%20prevention%20from%20a%20holistic%20perspective%20%28i.e.%2C%20addressing%0Aseveral%20disorders%20simultaneously%29.%20In%20this%20work%2C%20an%20entirely%20novel%20system%20for%0Athe%20multi-label%20classification%20of%20anxiety%20and%20depression%20is%20proposed.%20The%20input%0Adata%20consists%20of%20dialogues%20from%20user%20interactions%20with%20an%20assistant%20chatbot.%0AAnother%20relevant%20contribution%20lies%20in%20using%20Large%20Language%20Models%20%28LLMs%29%20for%0Afeature%20extraction%2C%20provided%20the%20complexity%20and%20variability%20of%20language.%20The%0Acombination%20of%20LLMs%2C%20given%20their%20high%20capability%20for%20language%20understanding%2C%0Aand%20Machine%20Learning%20%28ML%29%20models%2C%20provided%20their%20contextual%20knowledge%20about%20the%0Aclassification%20problem%20thanks%20to%20the%20labeled%20data%2C%20constitute%20a%20promising%0Aapproach%20towards%20mental%20health%20assessment.%20To%20promote%20the%20solution%27s%0Atrustworthiness%2C%20reliability%2C%20and%20accountability%2C%20explainability%20descriptions%0Aof%20the%20model%27s%20decision%20are%20provided%20in%20a%20graphical%20dashboard.%20Experimental%0Aresults%20on%20a%20real%20dataset%20attain%2090%20%25%20accuracy%2C%20improving%20those%20in%20the%20prior%0Aliterature.%20The%20ultimate%20objective%20is%20to%20contribute%20in%20an%20accessible%20and%0Ascalable%20way%20before%20formal%20treatment%20occurs%20in%20the%20healthcare%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17651v1&entry.124074799=Read"},
{"title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought", "author": "Jiaan Wang and Fandong Meng and Yunlong Liang and Jie Zhou", "abstract": "  Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to judge whether the translation in the current round is better\nthan the previous one or not. In this manner, we collect tens of thousands of\nlong-thought MT data, which is used to train our DRT-o1. The experimental\nresults on literature translation demonstrate the effectiveness of the DRT-o1.\nUsing Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by\nDRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can\noutperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its\neffectiveness. The project is available at https://github.com/krystalan/DRT-o1\n", "link": "http://arxiv.org/abs/2412.17498v1", "date": "2024-12-23", "relevancy": 2.0033, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRT-o1%3A%20Optimized%20Deep%20Reasoning%20Translation%20via%20Long%20Chain-of-Thought&body=Title%3A%20DRT-o1%3A%20Optimized%20Deep%20Reasoning%20Translation%20via%20Long%20Chain-of-Thought%0AAuthor%3A%20Jiaan%20Wang%20and%20Fandong%20Meng%20and%20Yunlong%20Liang%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Recently%2C%20O1-like%20models%20have%20emerged%20as%20representative%20examples%2C%0Aillustrating%20the%20effectiveness%20of%20long%20chain-of-thought%20%28CoT%29%20in%20reasoning%0Atasks%20such%20as%20math%20and%20coding%20tasks.%20In%20this%20paper%2C%20we%20introduce%20DRT-o1%2C%20an%0Aattempt%20to%20bring%20the%20success%20of%20long%20CoT%20to%20neural%20machine%20translation%20%28MT%29.%0ASpecifically%2C%20in%20view%20of%20the%20literature%20books%20that%20might%20involve%20similes%20and%0Ametaphors%2C%20translating%20these%20texts%20to%20a%20target%20language%20is%20very%20difficult%20in%0Apractice%20due%20to%20cultural%20differences.%20In%20such%20cases%2C%20literal%20translation%20often%0Afails%20to%20convey%20the%20intended%20meaning%20effectively.%20Even%20for%20professional%20human%0Atranslators%2C%20considerable%20thought%20must%20be%20given%20to%20preserving%20semantics%0Athroughout%20the%20translation%20process.%20To%20simulate%20LLMs%27%20long%20thought%20ability%20in%0AMT%2C%20we%20first%20mine%20sentences%20containing%20similes%20or%20metaphors%20from%20existing%0Aliterature%20books%2C%20and%20then%20develop%20a%20multi-agent%20framework%20to%20translate%20these%0Asentences%20via%20long%20thought.%20In%20the%20multi-agent%20framework%2C%20a%20translator%20is%20used%0Ato%20iteratively%20translate%20the%20source%20sentence%20under%20the%20suggestions%20provided%20by%0Aan%20advisor.%20To%20ensure%20the%20effectiveness%20of%20the%20long%20thoughts%2C%20an%20evaluator%20is%0Aalso%20employed%20to%20judge%20whether%20the%20translation%20in%20the%20current%20round%20is%20better%0Athan%20the%20previous%20one%20or%20not.%20In%20this%20manner%2C%20we%20collect%20tens%20of%20thousands%20of%0Along-thought%20MT%20data%2C%20which%20is%20used%20to%20train%20our%20DRT-o1.%20The%20experimental%0Aresults%20on%20literature%20translation%20demonstrate%20the%20effectiveness%20of%20the%20DRT-o1.%0AUsing%20Qwen2.5-7B%20and%20Qwen2.5-14B%20as%20the%20backbones%2C%20the%20improvement%20brought%20by%0ADRT-o1%20achieves%207.33~8.26%20BLEU%20and%201.66~3.36%20CometScore.%20Besides%2C%20DRT-o1-7B%20can%0Aoutperform%20QwQ-32B-Preview%20by%207.82%20BLEU%20and%201.46%20CometScore%2C%20showing%20its%0Aeffectiveness.%20The%20project%20is%20available%20at%20https%3A//github.com/krystalan/DRT-o1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRT-o1%253A%2520Optimized%2520Deep%2520Reasoning%2520Translation%2520via%2520Long%2520Chain-of-Thought%26entry.906535625%3DJiaan%2520Wang%2520and%2520Fandong%2520Meng%2520and%2520Yunlong%2520Liang%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Recently%252C%2520O1-like%2520models%2520have%2520emerged%2520as%2520representative%2520examples%252C%250Aillustrating%2520the%2520effectiveness%2520of%2520long%2520chain-of-thought%2520%2528CoT%2529%2520in%2520reasoning%250Atasks%2520such%2520as%2520math%2520and%2520coding%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DRT-o1%252C%2520an%250Aattempt%2520to%2520bring%2520the%2520success%2520of%2520long%2520CoT%2520to%2520neural%2520machine%2520translation%2520%2528MT%2529.%250ASpecifically%252C%2520in%2520view%2520of%2520the%2520literature%2520books%2520that%2520might%2520involve%2520similes%2520and%250Ametaphors%252C%2520translating%2520these%2520texts%2520to%2520a%2520target%2520language%2520is%2520very%2520difficult%2520in%250Apractice%2520due%2520to%2520cultural%2520differences.%2520In%2520such%2520cases%252C%2520literal%2520translation%2520often%250Afails%2520to%2520convey%2520the%2520intended%2520meaning%2520effectively.%2520Even%2520for%2520professional%2520human%250Atranslators%252C%2520considerable%2520thought%2520must%2520be%2520given%2520to%2520preserving%2520semantics%250Athroughout%2520the%2520translation%2520process.%2520To%2520simulate%2520LLMs%2527%2520long%2520thought%2520ability%2520in%250AMT%252C%2520we%2520first%2520mine%2520sentences%2520containing%2520similes%2520or%2520metaphors%2520from%2520existing%250Aliterature%2520books%252C%2520and%2520then%2520develop%2520a%2520multi-agent%2520framework%2520to%2520translate%2520these%250Asentences%2520via%2520long%2520thought.%2520In%2520the%2520multi-agent%2520framework%252C%2520a%2520translator%2520is%2520used%250Ato%2520iteratively%2520translate%2520the%2520source%2520sentence%2520under%2520the%2520suggestions%2520provided%2520by%250Aan%2520advisor.%2520To%2520ensure%2520the%2520effectiveness%2520of%2520the%2520long%2520thoughts%252C%2520an%2520evaluator%2520is%250Aalso%2520employed%2520to%2520judge%2520whether%2520the%2520translation%2520in%2520the%2520current%2520round%2520is%2520better%250Athan%2520the%2520previous%2520one%2520or%2520not.%2520In%2520this%2520manner%252C%2520we%2520collect%2520tens%2520of%2520thousands%2520of%250Along-thought%2520MT%2520data%252C%2520which%2520is%2520used%2520to%2520train%2520our%2520DRT-o1.%2520The%2520experimental%250Aresults%2520on%2520literature%2520translation%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520DRT-o1.%250AUsing%2520Qwen2.5-7B%2520and%2520Qwen2.5-14B%2520as%2520the%2520backbones%252C%2520the%2520improvement%2520brought%2520by%250ADRT-o1%2520achieves%25207.33~8.26%2520BLEU%2520and%25201.66~3.36%2520CometScore.%2520Besides%252C%2520DRT-o1-7B%2520can%250Aoutperform%2520QwQ-32B-Preview%2520by%25207.82%2520BLEU%2520and%25201.46%2520CometScore%252C%2520showing%2520its%250Aeffectiveness.%2520The%2520project%2520is%2520available%2520at%2520https%253A//github.com/krystalan/DRT-o1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRT-o1%3A%20Optimized%20Deep%20Reasoning%20Translation%20via%20Long%20Chain-of-Thought&entry.906535625=Jiaan%20Wang%20and%20Fandong%20Meng%20and%20Yunlong%20Liang%20and%20Jie%20Zhou&entry.1292438233=%20%20Recently%2C%20O1-like%20models%20have%20emerged%20as%20representative%20examples%2C%0Aillustrating%20the%20effectiveness%20of%20long%20chain-of-thought%20%28CoT%29%20in%20reasoning%0Atasks%20such%20as%20math%20and%20coding%20tasks.%20In%20this%20paper%2C%20we%20introduce%20DRT-o1%2C%20an%0Aattempt%20to%20bring%20the%20success%20of%20long%20CoT%20to%20neural%20machine%20translation%20%28MT%29.%0ASpecifically%2C%20in%20view%20of%20the%20literature%20books%20that%20might%20involve%20similes%20and%0Ametaphors%2C%20translating%20these%20texts%20to%20a%20target%20language%20is%20very%20difficult%20in%0Apractice%20due%20to%20cultural%20differences.%20In%20such%20cases%2C%20literal%20translation%20often%0Afails%20to%20convey%20the%20intended%20meaning%20effectively.%20Even%20for%20professional%20human%0Atranslators%2C%20considerable%20thought%20must%20be%20given%20to%20preserving%20semantics%0Athroughout%20the%20translation%20process.%20To%20simulate%20LLMs%27%20long%20thought%20ability%20in%0AMT%2C%20we%20first%20mine%20sentences%20containing%20similes%20or%20metaphors%20from%20existing%0Aliterature%20books%2C%20and%20then%20develop%20a%20multi-agent%20framework%20to%20translate%20these%0Asentences%20via%20long%20thought.%20In%20the%20multi-agent%20framework%2C%20a%20translator%20is%20used%0Ato%20iteratively%20translate%20the%20source%20sentence%20under%20the%20suggestions%20provided%20by%0Aan%20advisor.%20To%20ensure%20the%20effectiveness%20of%20the%20long%20thoughts%2C%20an%20evaluator%20is%0Aalso%20employed%20to%20judge%20whether%20the%20translation%20in%20the%20current%20round%20is%20better%0Athan%20the%20previous%20one%20or%20not.%20In%20this%20manner%2C%20we%20collect%20tens%20of%20thousands%20of%0Along-thought%20MT%20data%2C%20which%20is%20used%20to%20train%20our%20DRT-o1.%20The%20experimental%0Aresults%20on%20literature%20translation%20demonstrate%20the%20effectiveness%20of%20the%20DRT-o1.%0AUsing%20Qwen2.5-7B%20and%20Qwen2.5-14B%20as%20the%20backbones%2C%20the%20improvement%20brought%20by%0ADRT-o1%20achieves%207.33~8.26%20BLEU%20and%201.66~3.36%20CometScore.%20Besides%2C%20DRT-o1-7B%20can%0Aoutperform%20QwQ-32B-Preview%20by%207.82%20BLEU%20and%201.46%20CometScore%2C%20showing%20its%0Aeffectiveness.%20The%20project%20is%20available%20at%20https%3A//github.com/krystalan/DRT-o1%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17498v1&entry.124074799=Read"},
{"title": "Line Graph Vietoris-Rips Persistence Diagram for Topological Graph\n  Representation Learning", "author": "Jaesun Shin and Eunjoo Jeon and Taewon Cho and Namkyeong Cho and Youngjune Gwon", "abstract": "  While message passing graph neural networks result in informative node\nembeddings, they may suffer from describing the topological properties of\ngraphs. To this end, node filtration has been widely used as an attempt to\nobtain the topological information of a graph using persistence diagrams.\nHowever, these attempts have faced the problem of losing node embedding\ninformation, which in turn prevents them from providing a more expressive graph\nrepresentation. To tackle this issue, we shift our focus to edge filtration and\nintroduce a novel edge filtration-based persistence diagram, named Topological\nEdge Diagram (TED), which is mathematically proven to preserve node embedding\ninformation as well as contain additional topological information. To implement\nTED, we propose a neural network based algorithm, named Line Graph\nVietoris-Rips (LGVR) Persistence Diagram, that extracts edge information by\ntransforming a graph into its line graph. Through LGVR, we propose two model\nframeworks that can be applied to any message passing GNNs, and prove that they\nare strictly more powerful than Weisfeiler-Lehman type colorings. Finally we\nempirically validate superior performance of our models on several graph\nclassification and regression benchmarks.\n", "link": "http://arxiv.org/abs/2412.17468v1", "date": "2024-12-23", "relevancy": 1.8814, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5017}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4755}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Line%20Graph%20Vietoris-Rips%20Persistence%20Diagram%20for%20Topological%20Graph%0A%20%20Representation%20Learning&body=Title%3A%20Line%20Graph%20Vietoris-Rips%20Persistence%20Diagram%20for%20Topological%20Graph%0A%20%20Representation%20Learning%0AAuthor%3A%20Jaesun%20Shin%20and%20Eunjoo%20Jeon%20and%20Taewon%20Cho%20and%20Namkyeong%20Cho%20and%20Youngjune%20Gwon%0AAbstract%3A%20%20%20While%20message%20passing%20graph%20neural%20networks%20result%20in%20informative%20node%0Aembeddings%2C%20they%20may%20suffer%20from%20describing%20the%20topological%20properties%20of%0Agraphs.%20To%20this%20end%2C%20node%20filtration%20has%20been%20widely%20used%20as%20an%20attempt%20to%0Aobtain%20the%20topological%20information%20of%20a%20graph%20using%20persistence%20diagrams.%0AHowever%2C%20these%20attempts%20have%20faced%20the%20problem%20of%20losing%20node%20embedding%0Ainformation%2C%20which%20in%20turn%20prevents%20them%20from%20providing%20a%20more%20expressive%20graph%0Arepresentation.%20To%20tackle%20this%20issue%2C%20we%20shift%20our%20focus%20to%20edge%20filtration%20and%0Aintroduce%20a%20novel%20edge%20filtration-based%20persistence%20diagram%2C%20named%20Topological%0AEdge%20Diagram%20%28TED%29%2C%20which%20is%20mathematically%20proven%20to%20preserve%20node%20embedding%0Ainformation%20as%20well%20as%20contain%20additional%20topological%20information.%20To%20implement%0ATED%2C%20we%20propose%20a%20neural%20network%20based%20algorithm%2C%20named%20Line%20Graph%0AVietoris-Rips%20%28LGVR%29%20Persistence%20Diagram%2C%20that%20extracts%20edge%20information%20by%0Atransforming%20a%20graph%20into%20its%20line%20graph.%20Through%20LGVR%2C%20we%20propose%20two%20model%0Aframeworks%20that%20can%20be%20applied%20to%20any%20message%20passing%20GNNs%2C%20and%20prove%20that%20they%0Aare%20strictly%20more%20powerful%20than%20Weisfeiler-Lehman%20type%20colorings.%20Finally%20we%0Aempirically%20validate%20superior%20performance%20of%20our%20models%20on%20several%20graph%0Aclassification%20and%20regression%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLine%2520Graph%2520Vietoris-Rips%2520Persistence%2520Diagram%2520for%2520Topological%2520Graph%250A%2520%2520Representation%2520Learning%26entry.906535625%3DJaesun%2520Shin%2520and%2520Eunjoo%2520Jeon%2520and%2520Taewon%2520Cho%2520and%2520Namkyeong%2520Cho%2520and%2520Youngjune%2520Gwon%26entry.1292438233%3D%2520%2520While%2520message%2520passing%2520graph%2520neural%2520networks%2520result%2520in%2520informative%2520node%250Aembeddings%252C%2520they%2520may%2520suffer%2520from%2520describing%2520the%2520topological%2520properties%2520of%250Agraphs.%2520To%2520this%2520end%252C%2520node%2520filtration%2520has%2520been%2520widely%2520used%2520as%2520an%2520attempt%2520to%250Aobtain%2520the%2520topological%2520information%2520of%2520a%2520graph%2520using%2520persistence%2520diagrams.%250AHowever%252C%2520these%2520attempts%2520have%2520faced%2520the%2520problem%2520of%2520losing%2520node%2520embedding%250Ainformation%252C%2520which%2520in%2520turn%2520prevents%2520them%2520from%2520providing%2520a%2520more%2520expressive%2520graph%250Arepresentation.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520shift%2520our%2520focus%2520to%2520edge%2520filtration%2520and%250Aintroduce%2520a%2520novel%2520edge%2520filtration-based%2520persistence%2520diagram%252C%2520named%2520Topological%250AEdge%2520Diagram%2520%2528TED%2529%252C%2520which%2520is%2520mathematically%2520proven%2520to%2520preserve%2520node%2520embedding%250Ainformation%2520as%2520well%2520as%2520contain%2520additional%2520topological%2520information.%2520To%2520implement%250ATED%252C%2520we%2520propose%2520a%2520neural%2520network%2520based%2520algorithm%252C%2520named%2520Line%2520Graph%250AVietoris-Rips%2520%2528LGVR%2529%2520Persistence%2520Diagram%252C%2520that%2520extracts%2520edge%2520information%2520by%250Atransforming%2520a%2520graph%2520into%2520its%2520line%2520graph.%2520Through%2520LGVR%252C%2520we%2520propose%2520two%2520model%250Aframeworks%2520that%2520can%2520be%2520applied%2520to%2520any%2520message%2520passing%2520GNNs%252C%2520and%2520prove%2520that%2520they%250Aare%2520strictly%2520more%2520powerful%2520than%2520Weisfeiler-Lehman%2520type%2520colorings.%2520Finally%2520we%250Aempirically%2520validate%2520superior%2520performance%2520of%2520our%2520models%2520on%2520several%2520graph%250Aclassification%2520and%2520regression%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Line%20Graph%20Vietoris-Rips%20Persistence%20Diagram%20for%20Topological%20Graph%0A%20%20Representation%20Learning&entry.906535625=Jaesun%20Shin%20and%20Eunjoo%20Jeon%20and%20Taewon%20Cho%20and%20Namkyeong%20Cho%20and%20Youngjune%20Gwon&entry.1292438233=%20%20While%20message%20passing%20graph%20neural%20networks%20result%20in%20informative%20node%0Aembeddings%2C%20they%20may%20suffer%20from%20describing%20the%20topological%20properties%20of%0Agraphs.%20To%20this%20end%2C%20node%20filtration%20has%20been%20widely%20used%20as%20an%20attempt%20to%0Aobtain%20the%20topological%20information%20of%20a%20graph%20using%20persistence%20diagrams.%0AHowever%2C%20these%20attempts%20have%20faced%20the%20problem%20of%20losing%20node%20embedding%0Ainformation%2C%20which%20in%20turn%20prevents%20them%20from%20providing%20a%20more%20expressive%20graph%0Arepresentation.%20To%20tackle%20this%20issue%2C%20we%20shift%20our%20focus%20to%20edge%20filtration%20and%0Aintroduce%20a%20novel%20edge%20filtration-based%20persistence%20diagram%2C%20named%20Topological%0AEdge%20Diagram%20%28TED%29%2C%20which%20is%20mathematically%20proven%20to%20preserve%20node%20embedding%0Ainformation%20as%20well%20as%20contain%20additional%20topological%20information.%20To%20implement%0ATED%2C%20we%20propose%20a%20neural%20network%20based%20algorithm%2C%20named%20Line%20Graph%0AVietoris-Rips%20%28LGVR%29%20Persistence%20Diagram%2C%20that%20extracts%20edge%20information%20by%0Atransforming%20a%20graph%20into%20its%20line%20graph.%20Through%20LGVR%2C%20we%20propose%20two%20model%0Aframeworks%20that%20can%20be%20applied%20to%20any%20message%20passing%20GNNs%2C%20and%20prove%20that%20they%0Aare%20strictly%20more%20powerful%20than%20Weisfeiler-Lehman%20type%20colorings.%20Finally%20we%0Aempirically%20validate%20superior%20performance%20of%20our%20models%20on%20several%20graph%0Aclassification%20and%20regression%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17468v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


