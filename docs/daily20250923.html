<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250922.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting\n  from Monocular Videos", "author": "Shi Chen and Erik Sandstr\u00f6m and Sandro Lombardi and Siyuan Li and Martin R. Oswald", "abstract": "  Achieving truly practical dynamic 3D reconstruction requires online\noperation, global pose and map consistency, detailed appearance modeling, and\nthe flexibility to handle both RGB and RGB-D inputs. However, existing SLAM\nmethods typically merely remove the dynamic parts or require RGB-D input, while\noffline methods are not scalable to long video sequences, and current\ntransformer-based feedforward methods lack global consistency and appearance\ndetails. To this end, we achieve online dynamic scene reconstruction by\ndisentangling the static and dynamic parts within a SLAM system. The poses are\ntracked robustly with a novel motion masking strategy, and dynamic parts are\nreconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.\nOur method yields novel view renderings competitive to offline methods and\nachieves on-par tracking with state-of-the-art dynamic SLAM methods.\n", "link": "http://arxiv.org/abs/2509.17864v1", "date": "2025-09-22", "relevancy": 3.5954, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7542}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.732}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProDyG%3A%20Progressive%20Dynamic%20Scene%20Reconstruction%20via%20Gaussian%20Splatting%0A%20%20from%20Monocular%20Videos&body=Title%3A%20ProDyG%3A%20Progressive%20Dynamic%20Scene%20Reconstruction%20via%20Gaussian%20Splatting%0A%20%20from%20Monocular%20Videos%0AAuthor%3A%20Shi%20Chen%20and%20Erik%20Sandstr%C3%B6m%20and%20Sandro%20Lombardi%20and%20Siyuan%20Li%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Achieving%20truly%20practical%20dynamic%203D%20reconstruction%20requires%20online%0Aoperation%2C%20global%20pose%20and%20map%20consistency%2C%20detailed%20appearance%20modeling%2C%20and%0Athe%20flexibility%20to%20handle%20both%20RGB%20and%20RGB-D%20inputs.%20However%2C%20existing%20SLAM%0Amethods%20typically%20merely%20remove%20the%20dynamic%20parts%20or%20require%20RGB-D%20input%2C%20while%0Aoffline%20methods%20are%20not%20scalable%20to%20long%20video%20sequences%2C%20and%20current%0Atransformer-based%20feedforward%20methods%20lack%20global%20consistency%20and%20appearance%0Adetails.%20To%20this%20end%2C%20we%20achieve%20online%20dynamic%20scene%20reconstruction%20by%0Adisentangling%20the%20static%20and%20dynamic%20parts%20within%20a%20SLAM%20system.%20The%20poses%20are%0Atracked%20robustly%20with%20a%20novel%20motion%20masking%20strategy%2C%20and%20dynamic%20parts%20are%0Areconstructed%20leveraging%20a%20progressive%20adaptation%20of%20a%20Motion%20Scaffolds%20graph.%0AOur%20method%20yields%20novel%20view%20renderings%20competitive%20to%20offline%20methods%20and%0Aachieves%20on-par%20tracking%20with%20state-of-the-art%20dynamic%20SLAM%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProDyG%253A%2520Progressive%2520Dynamic%2520Scene%2520Reconstruction%2520via%2520Gaussian%2520Splatting%250A%2520%2520from%2520Monocular%2520Videos%26entry.906535625%3DShi%2520Chen%2520and%2520Erik%2520Sandstr%25C3%25B6m%2520and%2520Sandro%2520Lombardi%2520and%2520Siyuan%2520Li%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Achieving%2520truly%2520practical%2520dynamic%25203D%2520reconstruction%2520requires%2520online%250Aoperation%252C%2520global%2520pose%2520and%2520map%2520consistency%252C%2520detailed%2520appearance%2520modeling%252C%2520and%250Athe%2520flexibility%2520to%2520handle%2520both%2520RGB%2520and%2520RGB-D%2520inputs.%2520However%252C%2520existing%2520SLAM%250Amethods%2520typically%2520merely%2520remove%2520the%2520dynamic%2520parts%2520or%2520require%2520RGB-D%2520input%252C%2520while%250Aoffline%2520methods%2520are%2520not%2520scalable%2520to%2520long%2520video%2520sequences%252C%2520and%2520current%250Atransformer-based%2520feedforward%2520methods%2520lack%2520global%2520consistency%2520and%2520appearance%250Adetails.%2520To%2520this%2520end%252C%2520we%2520achieve%2520online%2520dynamic%2520scene%2520reconstruction%2520by%250Adisentangling%2520the%2520static%2520and%2520dynamic%2520parts%2520within%2520a%2520SLAM%2520system.%2520The%2520poses%2520are%250Atracked%2520robustly%2520with%2520a%2520novel%2520motion%2520masking%2520strategy%252C%2520and%2520dynamic%2520parts%2520are%250Areconstructed%2520leveraging%2520a%2520progressive%2520adaptation%2520of%2520a%2520Motion%2520Scaffolds%2520graph.%250AOur%2520method%2520yields%2520novel%2520view%2520renderings%2520competitive%2520to%2520offline%2520methods%2520and%250Aachieves%2520on-par%2520tracking%2520with%2520state-of-the-art%2520dynamic%2520SLAM%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProDyG%3A%20Progressive%20Dynamic%20Scene%20Reconstruction%20via%20Gaussian%20Splatting%0A%20%20from%20Monocular%20Videos&entry.906535625=Shi%20Chen%20and%20Erik%20Sandstr%C3%B6m%20and%20Sandro%20Lombardi%20and%20Siyuan%20Li%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Achieving%20truly%20practical%20dynamic%203D%20reconstruction%20requires%20online%0Aoperation%2C%20global%20pose%20and%20map%20consistency%2C%20detailed%20appearance%20modeling%2C%20and%0Athe%20flexibility%20to%20handle%20both%20RGB%20and%20RGB-D%20inputs.%20However%2C%20existing%20SLAM%0Amethods%20typically%20merely%20remove%20the%20dynamic%20parts%20or%20require%20RGB-D%20input%2C%20while%0Aoffline%20methods%20are%20not%20scalable%20to%20long%20video%20sequences%2C%20and%20current%0Atransformer-based%20feedforward%20methods%20lack%20global%20consistency%20and%20appearance%0Adetails.%20To%20this%20end%2C%20we%20achieve%20online%20dynamic%20scene%20reconstruction%20by%0Adisentangling%20the%20static%20and%20dynamic%20parts%20within%20a%20SLAM%20system.%20The%20poses%20are%0Atracked%20robustly%20with%20a%20novel%20motion%20masking%20strategy%2C%20and%20dynamic%20parts%20are%0Areconstructed%20leveraging%20a%20progressive%20adaptation%20of%20a%20Motion%20Scaffolds%20graph.%0AOur%20method%20yields%20novel%20view%20renderings%20competitive%20to%20offline%20methods%20and%0Aachieves%20on-par%20tracking%20with%20state-of-the-art%20dynamic%20SLAM%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17864v1&entry.124074799=Read"},
{"title": "MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild", "author": "Deming Li and Kaiwen Jiang and Yutao Tang and Ravi Ramamoorthi and Rama Chellappa and Cheng Peng", "abstract": "  In-the-wild photo collections often contain limited volumes of imagery and\nexhibit multiple appearances, e.g., taken at different times of day or seasons,\nposing significant challenges to scene reconstruction and novel view synthesis.\nAlthough recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian\nSplatting (3DGS) have improved in these areas, they tend to oversmooth and are\nprone to overfitting. In this paper, we present MS-GS, a novel framework\ndesigned with Multi-appearance capabilities in Sparse-view scenarios using\n3DGS. To address the lack of support due to sparse initializations, our\napproach is built on the geometric priors elicited from monocular depth\nestimations. The key lies in extracting and utilizing local semantic regions\nwith a Structure-from-Motion (SfM) points anchored algorithm for reliable\nalignment and geometry cues. Then, to introduce multi-view constraints, we\npropose a series of geometry-guided supervision at virtual views in a\nfine-grained and coarse scheme to encourage 3D consistency and reduce\noverfitting. We also introduce a dataset and an in-the-wild experiment setting\nto set up more realistic benchmarks. We demonstrate that MS-GS achieves\nphotorealistic renderings under various challenging sparse-view and\nmulti-appearance conditions and outperforms existing approaches significantly\nacross different datasets.\n", "link": "http://arxiv.org/abs/2509.15548v2", "date": "2025-09-22", "relevancy": 3.5118, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7209}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7133}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-GS%3A%20Multi-Appearance%20Sparse-View%203D%20Gaussian%20Splatting%20in%20the%20Wild&body=Title%3A%20MS-GS%3A%20Multi-Appearance%20Sparse-View%203D%20Gaussian%20Splatting%20in%20the%20Wild%0AAuthor%3A%20Deming%20Li%20and%20Kaiwen%20Jiang%20and%20Yutao%20Tang%20and%20Ravi%20Ramamoorthi%20and%20Rama%20Chellappa%20and%20Cheng%20Peng%0AAbstract%3A%20%20%20In-the-wild%20photo%20collections%20often%20contain%20limited%20volumes%20of%20imagery%20and%0Aexhibit%20multiple%20appearances%2C%20e.g.%2C%20taken%20at%20different%20times%20of%20day%20or%20seasons%2C%0Aposing%20significant%20challenges%20to%20scene%20reconstruction%20and%20novel%20view%20synthesis.%0AAlthough%20recent%20adaptations%20of%20Neural%20Radiance%20Field%20%28NeRF%29%20and%203D%20Gaussian%0ASplatting%20%283DGS%29%20have%20improved%20in%20these%20areas%2C%20they%20tend%20to%20oversmooth%20and%20are%0Aprone%20to%20overfitting.%20In%20this%20paper%2C%20we%20present%20MS-GS%2C%20a%20novel%20framework%0Adesigned%20with%20Multi-appearance%20capabilities%20in%20Sparse-view%20scenarios%20using%0A3DGS.%20To%20address%20the%20lack%20of%20support%20due%20to%20sparse%20initializations%2C%20our%0Aapproach%20is%20built%20on%20the%20geometric%20priors%20elicited%20from%20monocular%20depth%0Aestimations.%20The%20key%20lies%20in%20extracting%20and%20utilizing%20local%20semantic%20regions%0Awith%20a%20Structure-from-Motion%20%28SfM%29%20points%20anchored%20algorithm%20for%20reliable%0Aalignment%20and%20geometry%20cues.%20Then%2C%20to%20introduce%20multi-view%20constraints%2C%20we%0Apropose%20a%20series%20of%20geometry-guided%20supervision%20at%20virtual%20views%20in%20a%0Afine-grained%20and%20coarse%20scheme%20to%20encourage%203D%20consistency%20and%20reduce%0Aoverfitting.%20We%20also%20introduce%20a%20dataset%20and%20an%20in-the-wild%20experiment%20setting%0Ato%20set%20up%20more%20realistic%20benchmarks.%20We%20demonstrate%20that%20MS-GS%20achieves%0Aphotorealistic%20renderings%20under%20various%20challenging%20sparse-view%20and%0Amulti-appearance%20conditions%20and%20outperforms%20existing%20approaches%20significantly%0Aacross%20different%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15548v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-GS%253A%2520Multi-Appearance%2520Sparse-View%25203D%2520Gaussian%2520Splatting%2520in%2520the%2520Wild%26entry.906535625%3DDeming%2520Li%2520and%2520Kaiwen%2520Jiang%2520and%2520Yutao%2520Tang%2520and%2520Ravi%2520Ramamoorthi%2520and%2520Rama%2520Chellappa%2520and%2520Cheng%2520Peng%26entry.1292438233%3D%2520%2520In-the-wild%2520photo%2520collections%2520often%2520contain%2520limited%2520volumes%2520of%2520imagery%2520and%250Aexhibit%2520multiple%2520appearances%252C%2520e.g.%252C%2520taken%2520at%2520different%2520times%2520of%2520day%2520or%2520seasons%252C%250Aposing%2520significant%2520challenges%2520to%2520scene%2520reconstruction%2520and%2520novel%2520view%2520synthesis.%250AAlthough%2520recent%2520adaptations%2520of%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520have%2520improved%2520in%2520these%2520areas%252C%2520they%2520tend%2520to%2520oversmooth%2520and%2520are%250Aprone%2520to%2520overfitting.%2520In%2520this%2520paper%252C%2520we%2520present%2520MS-GS%252C%2520a%2520novel%2520framework%250Adesigned%2520with%2520Multi-appearance%2520capabilities%2520in%2520Sparse-view%2520scenarios%2520using%250A3DGS.%2520To%2520address%2520the%2520lack%2520of%2520support%2520due%2520to%2520sparse%2520initializations%252C%2520our%250Aapproach%2520is%2520built%2520on%2520the%2520geometric%2520priors%2520elicited%2520from%2520monocular%2520depth%250Aestimations.%2520The%2520key%2520lies%2520in%2520extracting%2520and%2520utilizing%2520local%2520semantic%2520regions%250Awith%2520a%2520Structure-from-Motion%2520%2528SfM%2529%2520points%2520anchored%2520algorithm%2520for%2520reliable%250Aalignment%2520and%2520geometry%2520cues.%2520Then%252C%2520to%2520introduce%2520multi-view%2520constraints%252C%2520we%250Apropose%2520a%2520series%2520of%2520geometry-guided%2520supervision%2520at%2520virtual%2520views%2520in%2520a%250Afine-grained%2520and%2520coarse%2520scheme%2520to%2520encourage%25203D%2520consistency%2520and%2520reduce%250Aoverfitting.%2520We%2520also%2520introduce%2520a%2520dataset%2520and%2520an%2520in-the-wild%2520experiment%2520setting%250Ato%2520set%2520up%2520more%2520realistic%2520benchmarks.%2520We%2520demonstrate%2520that%2520MS-GS%2520achieves%250Aphotorealistic%2520renderings%2520under%2520various%2520challenging%2520sparse-view%2520and%250Amulti-appearance%2520conditions%2520and%2520outperforms%2520existing%2520approaches%2520significantly%250Aacross%2520different%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15548v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-GS%3A%20Multi-Appearance%20Sparse-View%203D%20Gaussian%20Splatting%20in%20the%20Wild&entry.906535625=Deming%20Li%20and%20Kaiwen%20Jiang%20and%20Yutao%20Tang%20and%20Ravi%20Ramamoorthi%20and%20Rama%20Chellappa%20and%20Cheng%20Peng&entry.1292438233=%20%20In-the-wild%20photo%20collections%20often%20contain%20limited%20volumes%20of%20imagery%20and%0Aexhibit%20multiple%20appearances%2C%20e.g.%2C%20taken%20at%20different%20times%20of%20day%20or%20seasons%2C%0Aposing%20significant%20challenges%20to%20scene%20reconstruction%20and%20novel%20view%20synthesis.%0AAlthough%20recent%20adaptations%20of%20Neural%20Radiance%20Field%20%28NeRF%29%20and%203D%20Gaussian%0ASplatting%20%283DGS%29%20have%20improved%20in%20these%20areas%2C%20they%20tend%20to%20oversmooth%20and%20are%0Aprone%20to%20overfitting.%20In%20this%20paper%2C%20we%20present%20MS-GS%2C%20a%20novel%20framework%0Adesigned%20with%20Multi-appearance%20capabilities%20in%20Sparse-view%20scenarios%20using%0A3DGS.%20To%20address%20the%20lack%20of%20support%20due%20to%20sparse%20initializations%2C%20our%0Aapproach%20is%20built%20on%20the%20geometric%20priors%20elicited%20from%20monocular%20depth%0Aestimations.%20The%20key%20lies%20in%20extracting%20and%20utilizing%20local%20semantic%20regions%0Awith%20a%20Structure-from-Motion%20%28SfM%29%20points%20anchored%20algorithm%20for%20reliable%0Aalignment%20and%20geometry%20cues.%20Then%2C%20to%20introduce%20multi-view%20constraints%2C%20we%0Apropose%20a%20series%20of%20geometry-guided%20supervision%20at%20virtual%20views%20in%20a%0Afine-grained%20and%20coarse%20scheme%20to%20encourage%203D%20consistency%20and%20reduce%0Aoverfitting.%20We%20also%20introduce%20a%20dataset%20and%20an%20in-the-wild%20experiment%20setting%0Ato%20set%20up%20more%20realistic%20benchmarks.%20We%20demonstrate%20that%20MS-GS%20achieves%0Aphotorealistic%20renderings%20under%20various%20challenging%20sparse-view%20and%0Amulti-appearance%20conditions%20and%20outperforms%20existing%20approaches%20significantly%0Aacross%20different%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15548v2&entry.124074799=Read"},
{"title": "AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting", "author": "Gurutva Patle and Nilay Girgaonkar and Nagabhushan Somraj and Rajiv Soundararajan", "abstract": "  3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel\nview synthesis. However, it often struggles under sparse-view settings,\nproducing undesirable artifacts such as floaters, inaccurate geometry, and\noverfitting due to limited observations. We find that a key contributing factor\nis uncontrolled densification, where adding Gaussian primitives rapidly without\nguidance can harm geometry and cause artifacts. We propose AD-GS, a novel\nalternating densification framework that interleaves high and low densification\nphases. During high densification, the model densifies aggressively, followed\nby photometric loss based training to capture fine-grained scene details. Low\ndensification then primarily involves aggressive opacity pruning of Gaussians\nfollowed by regularizing their geometry through pseudo-view consistency and\nedge-aware depth smoothness. This alternating approach helps reduce overfitting\nby carefully controlling model capacity growth while progressively refining the\nscene representation. Extensive experiments on challenging datasets demonstrate\nthat AD-GS significantly improves rendering quality and geometric consistency\ncompared to existing methods. The source code for our model can be found on our\nproject page: https://gurutvapatle.github.io/publications/2025/ADGS.html .\n", "link": "http://arxiv.org/abs/2509.11003v2", "date": "2025-09-22", "relevancy": 3.5108, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7121}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7096}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AD-GS%3A%20Alternating%20Densification%20for%20Sparse-Input%203D%20Gaussian%20Splatting&body=Title%3A%20AD-GS%3A%20Alternating%20Densification%20for%20Sparse-Input%203D%20Gaussian%20Splatting%0AAuthor%3A%20Gurutva%20Patle%20and%20Nilay%20Girgaonkar%20and%20Nagabhushan%20Somraj%20and%20Rajiv%20Soundararajan%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20impressive%20results%20in%20real-time%20novel%0Aview%20synthesis.%20However%2C%20it%20often%20struggles%20under%20sparse-view%20settings%2C%0Aproducing%20undesirable%20artifacts%20such%20as%20floaters%2C%20inaccurate%20geometry%2C%20and%0Aoverfitting%20due%20to%20limited%20observations.%20We%20find%20that%20a%20key%20contributing%20factor%0Ais%20uncontrolled%20densification%2C%20where%20adding%20Gaussian%20primitives%20rapidly%20without%0Aguidance%20can%20harm%20geometry%20and%20cause%20artifacts.%20We%20propose%20AD-GS%2C%20a%20novel%0Aalternating%20densification%20framework%20that%20interleaves%20high%20and%20low%20densification%0Aphases.%20During%20high%20densification%2C%20the%20model%20densifies%20aggressively%2C%20followed%0Aby%20photometric%20loss%20based%20training%20to%20capture%20fine-grained%20scene%20details.%20Low%0Adensification%20then%20primarily%20involves%20aggressive%20opacity%20pruning%20of%20Gaussians%0Afollowed%20by%20regularizing%20their%20geometry%20through%20pseudo-view%20consistency%20and%0Aedge-aware%20depth%20smoothness.%20This%20alternating%20approach%20helps%20reduce%20overfitting%0Aby%20carefully%20controlling%20model%20capacity%20growth%20while%20progressively%20refining%20the%0Ascene%20representation.%20Extensive%20experiments%20on%20challenging%20datasets%20demonstrate%0Athat%20AD-GS%20significantly%20improves%20rendering%20quality%20and%20geometric%20consistency%0Acompared%20to%20existing%20methods.%20The%20source%20code%20for%20our%20model%20can%20be%20found%20on%20our%0Aproject%20page%3A%20https%3A//gurutvapatle.github.io/publications/2025/ADGS.html%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAD-GS%253A%2520Alternating%2520Densification%2520for%2520Sparse-Input%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DGurutva%2520Patle%2520and%2520Nilay%2520Girgaonkar%2520and%2520Nagabhushan%2520Somraj%2520and%2520Rajiv%2520Soundararajan%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520shown%2520impressive%2520results%2520in%2520real-time%2520novel%250Aview%2520synthesis.%2520However%252C%2520it%2520often%2520struggles%2520under%2520sparse-view%2520settings%252C%250Aproducing%2520undesirable%2520artifacts%2520such%2520as%2520floaters%252C%2520inaccurate%2520geometry%252C%2520and%250Aoverfitting%2520due%2520to%2520limited%2520observations.%2520We%2520find%2520that%2520a%2520key%2520contributing%2520factor%250Ais%2520uncontrolled%2520densification%252C%2520where%2520adding%2520Gaussian%2520primitives%2520rapidly%2520without%250Aguidance%2520can%2520harm%2520geometry%2520and%2520cause%2520artifacts.%2520We%2520propose%2520AD-GS%252C%2520a%2520novel%250Aalternating%2520densification%2520framework%2520that%2520interleaves%2520high%2520and%2520low%2520densification%250Aphases.%2520During%2520high%2520densification%252C%2520the%2520model%2520densifies%2520aggressively%252C%2520followed%250Aby%2520photometric%2520loss%2520based%2520training%2520to%2520capture%2520fine-grained%2520scene%2520details.%2520Low%250Adensification%2520then%2520primarily%2520involves%2520aggressive%2520opacity%2520pruning%2520of%2520Gaussians%250Afollowed%2520by%2520regularizing%2520their%2520geometry%2520through%2520pseudo-view%2520consistency%2520and%250Aedge-aware%2520depth%2520smoothness.%2520This%2520alternating%2520approach%2520helps%2520reduce%2520overfitting%250Aby%2520carefully%2520controlling%2520model%2520capacity%2520growth%2520while%2520progressively%2520refining%2520the%250Ascene%2520representation.%2520Extensive%2520experiments%2520on%2520challenging%2520datasets%2520demonstrate%250Athat%2520AD-GS%2520significantly%2520improves%2520rendering%2520quality%2520and%2520geometric%2520consistency%250Acompared%2520to%2520existing%2520methods.%2520The%2520source%2520code%2520for%2520our%2520model%2520can%2520be%2520found%2520on%2520our%250Aproject%2520page%253A%2520https%253A//gurutvapatle.github.io/publications/2025/ADGS.html%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AD-GS%3A%20Alternating%20Densification%20for%20Sparse-Input%203D%20Gaussian%20Splatting&entry.906535625=Gurutva%20Patle%20and%20Nilay%20Girgaonkar%20and%20Nagabhushan%20Somraj%20and%20Rajiv%20Soundararajan&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20impressive%20results%20in%20real-time%20novel%0Aview%20synthesis.%20However%2C%20it%20often%20struggles%20under%20sparse-view%20settings%2C%0Aproducing%20undesirable%20artifacts%20such%20as%20floaters%2C%20inaccurate%20geometry%2C%20and%0Aoverfitting%20due%20to%20limited%20observations.%20We%20find%20that%20a%20key%20contributing%20factor%0Ais%20uncontrolled%20densification%2C%20where%20adding%20Gaussian%20primitives%20rapidly%20without%0Aguidance%20can%20harm%20geometry%20and%20cause%20artifacts.%20We%20propose%20AD-GS%2C%20a%20novel%0Aalternating%20densification%20framework%20that%20interleaves%20high%20and%20low%20densification%0Aphases.%20During%20high%20densification%2C%20the%20model%20densifies%20aggressively%2C%20followed%0Aby%20photometric%20loss%20based%20training%20to%20capture%20fine-grained%20scene%20details.%20Low%0Adensification%20then%20primarily%20involves%20aggressive%20opacity%20pruning%20of%20Gaussians%0Afollowed%20by%20regularizing%20their%20geometry%20through%20pseudo-view%20consistency%20and%0Aedge-aware%20depth%20smoothness.%20This%20alternating%20approach%20helps%20reduce%20overfitting%0Aby%20carefully%20controlling%20model%20capacity%20growth%20while%20progressively%20refining%20the%0Ascene%20representation.%20Extensive%20experiments%20on%20challenging%20datasets%20demonstrate%0Athat%20AD-GS%20significantly%20improves%20rendering%20quality%20and%20geometric%20consistency%0Acompared%20to%20existing%20methods.%20The%20source%20code%20for%20our%20model%20can%20be%20found%20on%20our%0Aproject%20page%3A%20https%3A//gurutvapatle.github.io/publications/2025/ADGS.html%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11003v2&entry.124074799=Read"},
{"title": "Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene\n  Reconstruction", "author": "Sitian Shen and Georgi Pramatarov and Yifu Tao and Daniele De Martini", "abstract": "  This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal\nlarge-scale scene reconstruction that fuses multiple sensing modalities in a\nper-gaussian compact, learnable embedding. While recent works focusing on\nlarge-scale scene reconstruction have incorporated LiDAR data to provide more\naccurate geometric constraints, we argue that LiDAR's rich physical properties\nremain underexplored. Similarly, semantic information has been used for object\nretrieval, but could provide valuable high-level context for scene\nreconstruction. Traditional approaches append these properties to Gaussians as\nseparate parameters, increasing memory usage and limiting information exchange\nacross modalities. Instead, our approach fuses all modalities -- image, LiDAR,\nand semantics -- into a compact, learnable embedding that implicitly encodes\noptical, physical, and semantic features in each Gaussian. We then train\nlightweight neural decoders to map these embeddings to Gaussian parameters,\nenabling the reconstruction of each sensing modality with lower memory overhead\nand improved scalability. We evaluate Neural-MMGS on the Oxford Spires and\nKITTI-360 datasets. On Oxford Spires, we achieve higher-quality\nreconstructions, while on KITTI-360, our method reaches competitive results\nwith less storage consumption compared with current approaches in LiDAR-based\nnovel-view synthesis.\n", "link": "http://arxiv.org/abs/2509.17762v1", "date": "2025-09-22", "relevancy": 3.4526, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7146}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6887}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural-MMGS%3A%20Multi-modal%20Neural%20Gaussian%20Splats%20for%20Large-Scale%20Scene%0A%20%20Reconstruction&body=Title%3A%20Neural-MMGS%3A%20Multi-modal%20Neural%20Gaussian%20Splats%20for%20Large-Scale%20Scene%0A%20%20Reconstruction%0AAuthor%3A%20Sitian%20Shen%20and%20Georgi%20Pramatarov%20and%20Yifu%20Tao%20and%20Daniele%20De%20Martini%0AAbstract%3A%20%20%20This%20paper%20proposes%20Neural-MMGS%2C%20a%20novel%20neural%203DGS%20framework%20for%20multimodal%0Alarge-scale%20scene%20reconstruction%20that%20fuses%20multiple%20sensing%20modalities%20in%20a%0Aper-gaussian%20compact%2C%20learnable%20embedding.%20While%20recent%20works%20focusing%20on%0Alarge-scale%20scene%20reconstruction%20have%20incorporated%20LiDAR%20data%20to%20provide%20more%0Aaccurate%20geometric%20constraints%2C%20we%20argue%20that%20LiDAR%27s%20rich%20physical%20properties%0Aremain%20underexplored.%20Similarly%2C%20semantic%20information%20has%20been%20used%20for%20object%0Aretrieval%2C%20but%20could%20provide%20valuable%20high-level%20context%20for%20scene%0Areconstruction.%20Traditional%20approaches%20append%20these%20properties%20to%20Gaussians%20as%0Aseparate%20parameters%2C%20increasing%20memory%20usage%20and%20limiting%20information%20exchange%0Aacross%20modalities.%20Instead%2C%20our%20approach%20fuses%20all%20modalities%20--%20image%2C%20LiDAR%2C%0Aand%20semantics%20--%20into%20a%20compact%2C%20learnable%20embedding%20that%20implicitly%20encodes%0Aoptical%2C%20physical%2C%20and%20semantic%20features%20in%20each%20Gaussian.%20We%20then%20train%0Alightweight%20neural%20decoders%20to%20map%20these%20embeddings%20to%20Gaussian%20parameters%2C%0Aenabling%20the%20reconstruction%20of%20each%20sensing%20modality%20with%20lower%20memory%20overhead%0Aand%20improved%20scalability.%20We%20evaluate%20Neural-MMGS%20on%20the%20Oxford%20Spires%20and%0AKITTI-360%20datasets.%20On%20Oxford%20Spires%2C%20we%20achieve%20higher-quality%0Areconstructions%2C%20while%20on%20KITTI-360%2C%20our%20method%20reaches%20competitive%20results%0Awith%20less%20storage%20consumption%20compared%20with%20current%20approaches%20in%20LiDAR-based%0Anovel-view%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural-MMGS%253A%2520Multi-modal%2520Neural%2520Gaussian%2520Splats%2520for%2520Large-Scale%2520Scene%250A%2520%2520Reconstruction%26entry.906535625%3DSitian%2520Shen%2520and%2520Georgi%2520Pramatarov%2520and%2520Yifu%2520Tao%2520and%2520Daniele%2520De%2520Martini%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520Neural-MMGS%252C%2520a%2520novel%2520neural%25203DGS%2520framework%2520for%2520multimodal%250Alarge-scale%2520scene%2520reconstruction%2520that%2520fuses%2520multiple%2520sensing%2520modalities%2520in%2520a%250Aper-gaussian%2520compact%252C%2520learnable%2520embedding.%2520While%2520recent%2520works%2520focusing%2520on%250Alarge-scale%2520scene%2520reconstruction%2520have%2520incorporated%2520LiDAR%2520data%2520to%2520provide%2520more%250Aaccurate%2520geometric%2520constraints%252C%2520we%2520argue%2520that%2520LiDAR%2527s%2520rich%2520physical%2520properties%250Aremain%2520underexplored.%2520Similarly%252C%2520semantic%2520information%2520has%2520been%2520used%2520for%2520object%250Aretrieval%252C%2520but%2520could%2520provide%2520valuable%2520high-level%2520context%2520for%2520scene%250Areconstruction.%2520Traditional%2520approaches%2520append%2520these%2520properties%2520to%2520Gaussians%2520as%250Aseparate%2520parameters%252C%2520increasing%2520memory%2520usage%2520and%2520limiting%2520information%2520exchange%250Aacross%2520modalities.%2520Instead%252C%2520our%2520approach%2520fuses%2520all%2520modalities%2520--%2520image%252C%2520LiDAR%252C%250Aand%2520semantics%2520--%2520into%2520a%2520compact%252C%2520learnable%2520embedding%2520that%2520implicitly%2520encodes%250Aoptical%252C%2520physical%252C%2520and%2520semantic%2520features%2520in%2520each%2520Gaussian.%2520We%2520then%2520train%250Alightweight%2520neural%2520decoders%2520to%2520map%2520these%2520embeddings%2520to%2520Gaussian%2520parameters%252C%250Aenabling%2520the%2520reconstruction%2520of%2520each%2520sensing%2520modality%2520with%2520lower%2520memory%2520overhead%250Aand%2520improved%2520scalability.%2520We%2520evaluate%2520Neural-MMGS%2520on%2520the%2520Oxford%2520Spires%2520and%250AKITTI-360%2520datasets.%2520On%2520Oxford%2520Spires%252C%2520we%2520achieve%2520higher-quality%250Areconstructions%252C%2520while%2520on%2520KITTI-360%252C%2520our%2520method%2520reaches%2520competitive%2520results%250Awith%2520less%2520storage%2520consumption%2520compared%2520with%2520current%2520approaches%2520in%2520LiDAR-based%250Anovel-view%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural-MMGS%3A%20Multi-modal%20Neural%20Gaussian%20Splats%20for%20Large-Scale%20Scene%0A%20%20Reconstruction&entry.906535625=Sitian%20Shen%20and%20Georgi%20Pramatarov%20and%20Yifu%20Tao%20and%20Daniele%20De%20Martini&entry.1292438233=%20%20This%20paper%20proposes%20Neural-MMGS%2C%20a%20novel%20neural%203DGS%20framework%20for%20multimodal%0Alarge-scale%20scene%20reconstruction%20that%20fuses%20multiple%20sensing%20modalities%20in%20a%0Aper-gaussian%20compact%2C%20learnable%20embedding.%20While%20recent%20works%20focusing%20on%0Alarge-scale%20scene%20reconstruction%20have%20incorporated%20LiDAR%20data%20to%20provide%20more%0Aaccurate%20geometric%20constraints%2C%20we%20argue%20that%20LiDAR%27s%20rich%20physical%20properties%0Aremain%20underexplored.%20Similarly%2C%20semantic%20information%20has%20been%20used%20for%20object%0Aretrieval%2C%20but%20could%20provide%20valuable%20high-level%20context%20for%20scene%0Areconstruction.%20Traditional%20approaches%20append%20these%20properties%20to%20Gaussians%20as%0Aseparate%20parameters%2C%20increasing%20memory%20usage%20and%20limiting%20information%20exchange%0Aacross%20modalities.%20Instead%2C%20our%20approach%20fuses%20all%20modalities%20--%20image%2C%20LiDAR%2C%0Aand%20semantics%20--%20into%20a%20compact%2C%20learnable%20embedding%20that%20implicitly%20encodes%0Aoptical%2C%20physical%2C%20and%20semantic%20features%20in%20each%20Gaussian.%20We%20then%20train%0Alightweight%20neural%20decoders%20to%20map%20these%20embeddings%20to%20Gaussian%20parameters%2C%0Aenabling%20the%20reconstruction%20of%20each%20sensing%20modality%20with%20lower%20memory%20overhead%0Aand%20improved%20scalability.%20We%20evaluate%20Neural-MMGS%20on%20the%20Oxford%20Spires%20and%0AKITTI-360%20datasets.%20On%20Oxford%20Spires%2C%20we%20achieve%20higher-quality%0Areconstructions%2C%20while%20on%20KITTI-360%2C%20our%20method%20reaches%20competitive%20results%0Awith%20less%20storage%20consumption%20compared%20with%20current%20approaches%20in%20LiDAR-based%0Anovel-view%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17762v1&entry.124074799=Read"},
{"title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for\n  Underwater Scenes", "author": "Guoxi Huang and Haoran Wang and Zipeng Qi and Wenjun Lu and David Bull and Nantheera Anantrasirichai", "abstract": "  Underwater image degradation poses significant challenges for 3D\nreconstruction, where simplified physical models often fail in complex scenes.\nWe propose \\textbf{R-Splatting}, a unified framework that bridges underwater\nimage restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both\nrendering quality and geometric fidelity. Our method integrates multiple\nenhanced views produced by diverse UIR models into a single reconstruction\npipeline. During inference, a lightweight illumination generator samples latent\ncodes to support diverse yet coherent renderings, while a contrastive loss\nensures disentangled and stable illumination representations. Furthermore, we\npropose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models\nopacity as a stochastic function to regularize training. This suppresses abrupt\ngradient responses triggered by illumination variation and mitigates\noverfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF\nand our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong\nbaselines in both rendering quality and geometric accuracy.\n", "link": "http://arxiv.org/abs/2509.17789v1", "date": "2025-09-22", "relevancy": 3.2647, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7005}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6431}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Restoration%20to%20Reconstruction%3A%20Rethinking%203D%20Gaussian%20Splatting%20for%0A%20%20Underwater%20Scenes&body=Title%3A%20From%20Restoration%20to%20Reconstruction%3A%20Rethinking%203D%20Gaussian%20Splatting%20for%0A%20%20Underwater%20Scenes%0AAuthor%3A%20Guoxi%20Huang%20and%20Haoran%20Wang%20and%20Zipeng%20Qi%20and%20Wenjun%20Lu%20and%20David%20Bull%20and%20Nantheera%20Anantrasirichai%0AAbstract%3A%20%20%20Underwater%20image%20degradation%20poses%20significant%20challenges%20for%203D%0Areconstruction%2C%20where%20simplified%20physical%20models%20often%20fail%20in%20complex%20scenes.%0AWe%20propose%20%5Ctextbf%7BR-Splatting%7D%2C%20a%20unified%20framework%20that%20bridges%20underwater%0Aimage%20restoration%20%28UIR%29%20with%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20improve%20both%0Arendering%20quality%20and%20geometric%20fidelity.%20Our%20method%20integrates%20multiple%0Aenhanced%20views%20produced%20by%20diverse%20UIR%20models%20into%20a%20single%20reconstruction%0Apipeline.%20During%20inference%2C%20a%20lightweight%20illumination%20generator%20samples%20latent%0Acodes%20to%20support%20diverse%20yet%20coherent%20renderings%2C%20while%20a%20contrastive%20loss%0Aensures%20disentangled%20and%20stable%20illumination%20representations.%20Furthermore%2C%20we%0Apropose%20%5Ctextit%7BUncertainty-Aware%20Opacity%20Optimization%20%28UAOO%29%7D%2C%20which%20models%0Aopacity%20as%20a%20stochastic%20function%20to%20regularize%20training.%20This%20suppresses%20abrupt%0Agradient%20responses%20triggered%20by%20illumination%20variation%20and%20mitigates%0Aoverfitting%20to%20noisy%20or%20view-specific%20artifacts.%20Experiments%20on%20Seathru-NeRF%0Aand%20our%20new%20BlueCoral3D%20dataset%20demonstrate%20that%20R-Splatting%20outperforms%20strong%0Abaselines%20in%20both%20rendering%20quality%20and%20geometric%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Restoration%2520to%2520Reconstruction%253A%2520Rethinking%25203D%2520Gaussian%2520Splatting%2520for%250A%2520%2520Underwater%2520Scenes%26entry.906535625%3DGuoxi%2520Huang%2520and%2520Haoran%2520Wang%2520and%2520Zipeng%2520Qi%2520and%2520Wenjun%2520Lu%2520and%2520David%2520Bull%2520and%2520Nantheera%2520Anantrasirichai%26entry.1292438233%3D%2520%2520Underwater%2520image%2520degradation%2520poses%2520significant%2520challenges%2520for%25203D%250Areconstruction%252C%2520where%2520simplified%2520physical%2520models%2520often%2520fail%2520in%2520complex%2520scenes.%250AWe%2520propose%2520%255Ctextbf%257BR-Splatting%257D%252C%2520a%2520unified%2520framework%2520that%2520bridges%2520underwater%250Aimage%2520restoration%2520%2528UIR%2529%2520with%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520to%2520improve%2520both%250Arendering%2520quality%2520and%2520geometric%2520fidelity.%2520Our%2520method%2520integrates%2520multiple%250Aenhanced%2520views%2520produced%2520by%2520diverse%2520UIR%2520models%2520into%2520a%2520single%2520reconstruction%250Apipeline.%2520During%2520inference%252C%2520a%2520lightweight%2520illumination%2520generator%2520samples%2520latent%250Acodes%2520to%2520support%2520diverse%2520yet%2520coherent%2520renderings%252C%2520while%2520a%2520contrastive%2520loss%250Aensures%2520disentangled%2520and%2520stable%2520illumination%2520representations.%2520Furthermore%252C%2520we%250Apropose%2520%255Ctextit%257BUncertainty-Aware%2520Opacity%2520Optimization%2520%2528UAOO%2529%257D%252C%2520which%2520models%250Aopacity%2520as%2520a%2520stochastic%2520function%2520to%2520regularize%2520training.%2520This%2520suppresses%2520abrupt%250Agradient%2520responses%2520triggered%2520by%2520illumination%2520variation%2520and%2520mitigates%250Aoverfitting%2520to%2520noisy%2520or%2520view-specific%2520artifacts.%2520Experiments%2520on%2520Seathru-NeRF%250Aand%2520our%2520new%2520BlueCoral3D%2520dataset%2520demonstrate%2520that%2520R-Splatting%2520outperforms%2520strong%250Abaselines%2520in%2520both%2520rendering%2520quality%2520and%2520geometric%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Restoration%20to%20Reconstruction%3A%20Rethinking%203D%20Gaussian%20Splatting%20for%0A%20%20Underwater%20Scenes&entry.906535625=Guoxi%20Huang%20and%20Haoran%20Wang%20and%20Zipeng%20Qi%20and%20Wenjun%20Lu%20and%20David%20Bull%20and%20Nantheera%20Anantrasirichai&entry.1292438233=%20%20Underwater%20image%20degradation%20poses%20significant%20challenges%20for%203D%0Areconstruction%2C%20where%20simplified%20physical%20models%20often%20fail%20in%20complex%20scenes.%0AWe%20propose%20%5Ctextbf%7BR-Splatting%7D%2C%20a%20unified%20framework%20that%20bridges%20underwater%0Aimage%20restoration%20%28UIR%29%20with%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20improve%20both%0Arendering%20quality%20and%20geometric%20fidelity.%20Our%20method%20integrates%20multiple%0Aenhanced%20views%20produced%20by%20diverse%20UIR%20models%20into%20a%20single%20reconstruction%0Apipeline.%20During%20inference%2C%20a%20lightweight%20illumination%20generator%20samples%20latent%0Acodes%20to%20support%20diverse%20yet%20coherent%20renderings%2C%20while%20a%20contrastive%20loss%0Aensures%20disentangled%20and%20stable%20illumination%20representations.%20Furthermore%2C%20we%0Apropose%20%5Ctextit%7BUncertainty-Aware%20Opacity%20Optimization%20%28UAOO%29%7D%2C%20which%20models%0Aopacity%20as%20a%20stochastic%20function%20to%20regularize%20training.%20This%20suppresses%20abrupt%0Agradient%20responses%20triggered%20by%20illumination%20variation%20and%20mitigates%0Aoverfitting%20to%20noisy%20or%20view-specific%20artifacts.%20Experiments%20on%20Seathru-NeRF%0Aand%20our%20new%20BlueCoral3D%20dataset%20demonstrate%20that%20R-Splatting%20outperforms%20strong%0Abaselines%20in%20both%20rendering%20quality%20and%20geometric%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17789v1&entry.124074799=Read"},
{"title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded\n  Vision-Language Models", "author": "Pingyi Chen and Yujing Lou and Shen Cao and Jinhui Guo and Lubin Fan and Yue Wu and Lin Yang and Lizhuang Ma and Jieping Ye", "abstract": "  While vision language models (VLMs) excel in 2D semantic visual\nunderstanding, their ability to quantitatively reason about 3D spatial\nrelationships remains under-explored, due to the deficiency of 2D images'\nspatial representation ability. In this paper, we analyze the problem hindering\nVLMs' spatial understanding abilities and propose SD-VLM, a novel framework\nthat significantly enhances fundamental spatial perception abilities of VLMs\nthrough two key contributions: (1) propose Massive Spatial Measuring and\nUnderstanding (MSMU) dataset with precise spatial annotations, and (2)\nintroduce a simple depth positional encoding method strengthening VLMs' spatial\nawareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA\npairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented\nsamples. We have trained SD-VLM, a strong generalist VLM which shows superior\nquantitative spatial measuring and understanding capability. SD-VLM not only\nachieves state-of-the-art performance on our proposed MSMU-Bench, but also\nshows spatial generalization abilities on other spatial understanding\nbenchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments\ndemonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and\n25.56% respectively on MSMU-Bench. Code and models are released at\nhttps://github.com/cpystan/SD-VLM.\n", "link": "http://arxiv.org/abs/2509.17664v1", "date": "2025-09-22", "relevancy": 3.2583, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6779}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SD-VLM%3A%20Spatial%20Measuring%20and%20Understanding%20with%20Depth-Encoded%0A%20%20Vision-Language%20Models&body=Title%3A%20SD-VLM%3A%20Spatial%20Measuring%20and%20Understanding%20with%20Depth-Encoded%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Pingyi%20Chen%20and%20Yujing%20Lou%20and%20Shen%20Cao%20and%20Jinhui%20Guo%20and%20Lubin%20Fan%20and%20Yue%20Wu%20and%20Lin%20Yang%20and%20Lizhuang%20Ma%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20While%20vision%20language%20models%20%28VLMs%29%20excel%20in%202D%20semantic%20visual%0Aunderstanding%2C%20their%20ability%20to%20quantitatively%20reason%20about%203D%20spatial%0Arelationships%20remains%20under-explored%2C%20due%20to%20the%20deficiency%20of%202D%20images%27%0Aspatial%20representation%20ability.%20In%20this%20paper%2C%20we%20analyze%20the%20problem%20hindering%0AVLMs%27%20spatial%20understanding%20abilities%20and%20propose%20SD-VLM%2C%20a%20novel%20framework%0Athat%20significantly%20enhances%20fundamental%20spatial%20perception%20abilities%20of%20VLMs%0Athrough%20two%20key%20contributions%3A%20%281%29%20propose%20Massive%20Spatial%20Measuring%20and%0AUnderstanding%20%28MSMU%29%20dataset%20with%20precise%20spatial%20annotations%2C%20and%20%282%29%0Aintroduce%20a%20simple%20depth%20positional%20encoding%20method%20strengthening%20VLMs%27%20spatial%0Aawareness.%20MSMU%20dataset%20covers%20massive%20quantitative%20spatial%20tasks%20with%20700K%20QA%0Apairs%2C%202.5M%20physical%20numerical%20annotations%2C%20and%2010K%20chain-of-thought%20augmented%0Asamples.%20We%20have%20trained%20SD-VLM%2C%20a%20strong%20generalist%20VLM%20which%20shows%20superior%0Aquantitative%20spatial%20measuring%20and%20understanding%20capability.%20SD-VLM%20not%20only%0Aachieves%20state-of-the-art%20performance%20on%20our%20proposed%20MSMU-Bench%2C%20but%20also%0Ashows%20spatial%20generalization%20abilities%20on%20other%20spatial%20understanding%0Abenchmarks%20including%20Q-Spatial%20and%20SpatialRGPT-Bench.%20Extensive%20experiments%0Ademonstrate%20that%20SD-VLM%20outperforms%20GPT-4o%20and%20Intern-VL3-78B%20by%2026.91%25%20and%0A25.56%25%20respectively%20on%20MSMU-Bench.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/cpystan/SD-VLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSD-VLM%253A%2520Spatial%2520Measuring%2520and%2520Understanding%2520with%2520Depth-Encoded%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DPingyi%2520Chen%2520and%2520Yujing%2520Lou%2520and%2520Shen%2520Cao%2520and%2520Jinhui%2520Guo%2520and%2520Lubin%2520Fan%2520and%2520Yue%2520Wu%2520and%2520Lin%2520Yang%2520and%2520Lizhuang%2520Ma%2520and%2520Jieping%2520Ye%26entry.1292438233%3D%2520%2520While%2520vision%2520language%2520models%2520%2528VLMs%2529%2520excel%2520in%25202D%2520semantic%2520visual%250Aunderstanding%252C%2520their%2520ability%2520to%2520quantitatively%2520reason%2520about%25203D%2520spatial%250Arelationships%2520remains%2520under-explored%252C%2520due%2520to%2520the%2520deficiency%2520of%25202D%2520images%2527%250Aspatial%2520representation%2520ability.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520problem%2520hindering%250AVLMs%2527%2520spatial%2520understanding%2520abilities%2520and%2520propose%2520SD-VLM%252C%2520a%2520novel%2520framework%250Athat%2520significantly%2520enhances%2520fundamental%2520spatial%2520perception%2520abilities%2520of%2520VLMs%250Athrough%2520two%2520key%2520contributions%253A%2520%25281%2529%2520propose%2520Massive%2520Spatial%2520Measuring%2520and%250AUnderstanding%2520%2528MSMU%2529%2520dataset%2520with%2520precise%2520spatial%2520annotations%252C%2520and%2520%25282%2529%250Aintroduce%2520a%2520simple%2520depth%2520positional%2520encoding%2520method%2520strengthening%2520VLMs%2527%2520spatial%250Aawareness.%2520MSMU%2520dataset%2520covers%2520massive%2520quantitative%2520spatial%2520tasks%2520with%2520700K%2520QA%250Apairs%252C%25202.5M%2520physical%2520numerical%2520annotations%252C%2520and%252010K%2520chain-of-thought%2520augmented%250Asamples.%2520We%2520have%2520trained%2520SD-VLM%252C%2520a%2520strong%2520generalist%2520VLM%2520which%2520shows%2520superior%250Aquantitative%2520spatial%2520measuring%2520and%2520understanding%2520capability.%2520SD-VLM%2520not%2520only%250Aachieves%2520state-of-the-art%2520performance%2520on%2520our%2520proposed%2520MSMU-Bench%252C%2520but%2520also%250Ashows%2520spatial%2520generalization%2520abilities%2520on%2520other%2520spatial%2520understanding%250Abenchmarks%2520including%2520Q-Spatial%2520and%2520SpatialRGPT-Bench.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520SD-VLM%2520outperforms%2520GPT-4o%2520and%2520Intern-VL3-78B%2520by%252026.91%2525%2520and%250A25.56%2525%2520respectively%2520on%2520MSMU-Bench.%2520Code%2520and%2520models%2520are%2520released%2520at%250Ahttps%253A//github.com/cpystan/SD-VLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SD-VLM%3A%20Spatial%20Measuring%20and%20Understanding%20with%20Depth-Encoded%0A%20%20Vision-Language%20Models&entry.906535625=Pingyi%20Chen%20and%20Yujing%20Lou%20and%20Shen%20Cao%20and%20Jinhui%20Guo%20and%20Lubin%20Fan%20and%20Yue%20Wu%20and%20Lin%20Yang%20and%20Lizhuang%20Ma%20and%20Jieping%20Ye&entry.1292438233=%20%20While%20vision%20language%20models%20%28VLMs%29%20excel%20in%202D%20semantic%20visual%0Aunderstanding%2C%20their%20ability%20to%20quantitatively%20reason%20about%203D%20spatial%0Arelationships%20remains%20under-explored%2C%20due%20to%20the%20deficiency%20of%202D%20images%27%0Aspatial%20representation%20ability.%20In%20this%20paper%2C%20we%20analyze%20the%20problem%20hindering%0AVLMs%27%20spatial%20understanding%20abilities%20and%20propose%20SD-VLM%2C%20a%20novel%20framework%0Athat%20significantly%20enhances%20fundamental%20spatial%20perception%20abilities%20of%20VLMs%0Athrough%20two%20key%20contributions%3A%20%281%29%20propose%20Massive%20Spatial%20Measuring%20and%0AUnderstanding%20%28MSMU%29%20dataset%20with%20precise%20spatial%20annotations%2C%20and%20%282%29%0Aintroduce%20a%20simple%20depth%20positional%20encoding%20method%20strengthening%20VLMs%27%20spatial%0Aawareness.%20MSMU%20dataset%20covers%20massive%20quantitative%20spatial%20tasks%20with%20700K%20QA%0Apairs%2C%202.5M%20physical%20numerical%20annotations%2C%20and%2010K%20chain-of-thought%20augmented%0Asamples.%20We%20have%20trained%20SD-VLM%2C%20a%20strong%20generalist%20VLM%20which%20shows%20superior%0Aquantitative%20spatial%20measuring%20and%20understanding%20capability.%20SD-VLM%20not%20only%0Aachieves%20state-of-the-art%20performance%20on%20our%20proposed%20MSMU-Bench%2C%20but%20also%0Ashows%20spatial%20generalization%20abilities%20on%20other%20spatial%20understanding%0Abenchmarks%20including%20Q-Spatial%20and%20SpatialRGPT-Bench.%20Extensive%20experiments%0Ademonstrate%20that%20SD-VLM%20outperforms%20GPT-4o%20and%20Intern-VL3-78B%20by%2026.91%25%20and%0A25.56%25%20respectively%20on%20MSMU-Bench.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/cpystan/SD-VLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17664v1&entry.124074799=Read"},
{"title": "VideoArtGS: Building Digital Twins of Articulated Objects from Monocular\n  Video", "author": "Yu Liu and Baoxiong Jia and Ruijie Lu and Chuyue Gan and Huayu Chen and Junfeng Ni and Song-Chun Zhu and Siyuan Huang", "abstract": "  Building digital twins of articulated objects from monocular video presents\nan essential challenge in computer vision, which requires simultaneous\nreconstruction of object geometry, part segmentation, and articulation\nparameters from limited viewpoint inputs. Monocular video offers an attractive\ninput format due to its simplicity and scalability; however, it's challenging\nto disentangle the object geometry and part dynamics with visual supervision\nalone, as the joint movement of the camera and parts leads to ill-posed\nestimation. While motion priors from pre-trained tracking models can alleviate\nthe issue, how to effectively integrate them for articulation learning remains\nlargely unexplored. To address this problem, we introduce VideoArtGS, a novel\napproach that reconstructs high-fidelity digital twins of articulated objects\nfrom monocular video. We propose a motion prior guidance pipeline that analyzes\n3D tracks, filters noise, and provides reliable initialization of articulation\nparameters. We also design a hybrid center-grid part assignment module for\narticulation-based deformation fields that captures accurate part motion.\nVideoArtGS demonstrates state-of-the-art performance in articulation and mesh\nreconstruction, reducing the reconstruction error by about two orders of\nmagnitude compared to existing methods. VideoArtGS enables practical digital\ntwin creation from monocular video, establishing a new benchmark for\nvideo-based articulated object reconstruction. Our work is made publicly\navailable at: https://videoartgs.github.io.\n", "link": "http://arxiv.org/abs/2509.17647v1", "date": "2025-09-22", "relevancy": 3.1971, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7005}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6104}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoArtGS%3A%20Building%20Digital%20Twins%20of%20Articulated%20Objects%20from%20Monocular%0A%20%20Video&body=Title%3A%20VideoArtGS%3A%20Building%20Digital%20Twins%20of%20Articulated%20Objects%20from%20Monocular%0A%20%20Video%0AAuthor%3A%20Yu%20Liu%20and%20Baoxiong%20Jia%20and%20Ruijie%20Lu%20and%20Chuyue%20Gan%20and%20Huayu%20Chen%20and%20Junfeng%20Ni%20and%20Song-Chun%20Zhu%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Building%20digital%20twins%20of%20articulated%20objects%20from%20monocular%20video%20presents%0Aan%20essential%20challenge%20in%20computer%20vision%2C%20which%20requires%20simultaneous%0Areconstruction%20of%20object%20geometry%2C%20part%20segmentation%2C%20and%20articulation%0Aparameters%20from%20limited%20viewpoint%20inputs.%20Monocular%20video%20offers%20an%20attractive%0Ainput%20format%20due%20to%20its%20simplicity%20and%20scalability%3B%20however%2C%20it%27s%20challenging%0Ato%20disentangle%20the%20object%20geometry%20and%20part%20dynamics%20with%20visual%20supervision%0Aalone%2C%20as%20the%20joint%20movement%20of%20the%20camera%20and%20parts%20leads%20to%20ill-posed%0Aestimation.%20While%20motion%20priors%20from%20pre-trained%20tracking%20models%20can%20alleviate%0Athe%20issue%2C%20how%20to%20effectively%20integrate%20them%20for%20articulation%20learning%20remains%0Alargely%20unexplored.%20To%20address%20this%20problem%2C%20we%20introduce%20VideoArtGS%2C%20a%20novel%0Aapproach%20that%20reconstructs%20high-fidelity%20digital%20twins%20of%20articulated%20objects%0Afrom%20monocular%20video.%20We%20propose%20a%20motion%20prior%20guidance%20pipeline%20that%20analyzes%0A3D%20tracks%2C%20filters%20noise%2C%20and%20provides%20reliable%20initialization%20of%20articulation%0Aparameters.%20We%20also%20design%20a%20hybrid%20center-grid%20part%20assignment%20module%20for%0Aarticulation-based%20deformation%20fields%20that%20captures%20accurate%20part%20motion.%0AVideoArtGS%20demonstrates%20state-of-the-art%20performance%20in%20articulation%20and%20mesh%0Areconstruction%2C%20reducing%20the%20reconstruction%20error%20by%20about%20two%20orders%20of%0Amagnitude%20compared%20to%20existing%20methods.%20VideoArtGS%20enables%20practical%20digital%0Atwin%20creation%20from%20monocular%20video%2C%20establishing%20a%20new%20benchmark%20for%0Avideo-based%20articulated%20object%20reconstruction.%20Our%20work%20is%20made%20publicly%0Aavailable%20at%3A%20https%3A//videoartgs.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoArtGS%253A%2520Building%2520Digital%2520Twins%2520of%2520Articulated%2520Objects%2520from%2520Monocular%250A%2520%2520Video%26entry.906535625%3DYu%2520Liu%2520and%2520Baoxiong%2520Jia%2520and%2520Ruijie%2520Lu%2520and%2520Chuyue%2520Gan%2520and%2520Huayu%2520Chen%2520and%2520Junfeng%2520Ni%2520and%2520Song-Chun%2520Zhu%2520and%2520Siyuan%2520Huang%26entry.1292438233%3D%2520%2520Building%2520digital%2520twins%2520of%2520articulated%2520objects%2520from%2520monocular%2520video%2520presents%250Aan%2520essential%2520challenge%2520in%2520computer%2520vision%252C%2520which%2520requires%2520simultaneous%250Areconstruction%2520of%2520object%2520geometry%252C%2520part%2520segmentation%252C%2520and%2520articulation%250Aparameters%2520from%2520limited%2520viewpoint%2520inputs.%2520Monocular%2520video%2520offers%2520an%2520attractive%250Ainput%2520format%2520due%2520to%2520its%2520simplicity%2520and%2520scalability%253B%2520however%252C%2520it%2527s%2520challenging%250Ato%2520disentangle%2520the%2520object%2520geometry%2520and%2520part%2520dynamics%2520with%2520visual%2520supervision%250Aalone%252C%2520as%2520the%2520joint%2520movement%2520of%2520the%2520camera%2520and%2520parts%2520leads%2520to%2520ill-posed%250Aestimation.%2520While%2520motion%2520priors%2520from%2520pre-trained%2520tracking%2520models%2520can%2520alleviate%250Athe%2520issue%252C%2520how%2520to%2520effectively%2520integrate%2520them%2520for%2520articulation%2520learning%2520remains%250Alargely%2520unexplored.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520VideoArtGS%252C%2520a%2520novel%250Aapproach%2520that%2520reconstructs%2520high-fidelity%2520digital%2520twins%2520of%2520articulated%2520objects%250Afrom%2520monocular%2520video.%2520We%2520propose%2520a%2520motion%2520prior%2520guidance%2520pipeline%2520that%2520analyzes%250A3D%2520tracks%252C%2520filters%2520noise%252C%2520and%2520provides%2520reliable%2520initialization%2520of%2520articulation%250Aparameters.%2520We%2520also%2520design%2520a%2520hybrid%2520center-grid%2520part%2520assignment%2520module%2520for%250Aarticulation-based%2520deformation%2520fields%2520that%2520captures%2520accurate%2520part%2520motion.%250AVideoArtGS%2520demonstrates%2520state-of-the-art%2520performance%2520in%2520articulation%2520and%2520mesh%250Areconstruction%252C%2520reducing%2520the%2520reconstruction%2520error%2520by%2520about%2520two%2520orders%2520of%250Amagnitude%2520compared%2520to%2520existing%2520methods.%2520VideoArtGS%2520enables%2520practical%2520digital%250Atwin%2520creation%2520from%2520monocular%2520video%252C%2520establishing%2520a%2520new%2520benchmark%2520for%250Avideo-based%2520articulated%2520object%2520reconstruction.%2520Our%2520work%2520is%2520made%2520publicly%250Aavailable%2520at%253A%2520https%253A//videoartgs.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoArtGS%3A%20Building%20Digital%20Twins%20of%20Articulated%20Objects%20from%20Monocular%0A%20%20Video&entry.906535625=Yu%20Liu%20and%20Baoxiong%20Jia%20and%20Ruijie%20Lu%20and%20Chuyue%20Gan%20and%20Huayu%20Chen%20and%20Junfeng%20Ni%20and%20Song-Chun%20Zhu%20and%20Siyuan%20Huang&entry.1292438233=%20%20Building%20digital%20twins%20of%20articulated%20objects%20from%20monocular%20video%20presents%0Aan%20essential%20challenge%20in%20computer%20vision%2C%20which%20requires%20simultaneous%0Areconstruction%20of%20object%20geometry%2C%20part%20segmentation%2C%20and%20articulation%0Aparameters%20from%20limited%20viewpoint%20inputs.%20Monocular%20video%20offers%20an%20attractive%0Ainput%20format%20due%20to%20its%20simplicity%20and%20scalability%3B%20however%2C%20it%27s%20challenging%0Ato%20disentangle%20the%20object%20geometry%20and%20part%20dynamics%20with%20visual%20supervision%0Aalone%2C%20as%20the%20joint%20movement%20of%20the%20camera%20and%20parts%20leads%20to%20ill-posed%0Aestimation.%20While%20motion%20priors%20from%20pre-trained%20tracking%20models%20can%20alleviate%0Athe%20issue%2C%20how%20to%20effectively%20integrate%20them%20for%20articulation%20learning%20remains%0Alargely%20unexplored.%20To%20address%20this%20problem%2C%20we%20introduce%20VideoArtGS%2C%20a%20novel%0Aapproach%20that%20reconstructs%20high-fidelity%20digital%20twins%20of%20articulated%20objects%0Afrom%20monocular%20video.%20We%20propose%20a%20motion%20prior%20guidance%20pipeline%20that%20analyzes%0A3D%20tracks%2C%20filters%20noise%2C%20and%20provides%20reliable%20initialization%20of%20articulation%0Aparameters.%20We%20also%20design%20a%20hybrid%20center-grid%20part%20assignment%20module%20for%0Aarticulation-based%20deformation%20fields%20that%20captures%20accurate%20part%20motion.%0AVideoArtGS%20demonstrates%20state-of-the-art%20performance%20in%20articulation%20and%20mesh%0Areconstruction%2C%20reducing%20the%20reconstruction%20error%20by%20about%20two%20orders%20of%0Amagnitude%20compared%20to%20existing%20methods.%20VideoArtGS%20enables%20practical%20digital%0Atwin%20creation%20from%20monocular%20video%2C%20establishing%20a%20new%20benchmark%20for%0Avideo-based%20articulated%20object%20reconstruction.%20Our%20work%20is%20made%20publicly%0Aavailable%20at%3A%20https%3A//videoartgs.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17647v1&entry.124074799=Read"},
{"title": "Visual Instruction Pretraining for Domain-Specific Foundation Models", "author": "Yuxuan Li and Yicheng Zhang and Wenhao Tang and Yimian Dai and Ming-Ming Cheng and Xiang Li and Jian Yang", "abstract": "  Modern computer vision is converging on a closed loop in which perception,\nreasoning and generation mutually reinforce each other. However, this loop\nremains incomplete: the top-down influence of high-level reasoning on the\nfoundational learning of low-level perceptual features is not yet\nunderexplored. This paper addresses this gap by proposing a new paradigm for\npretraining foundation models in downstream domains. We introduce Visual\ninsTruction Pretraining (ViTP), a novel approach that directly leverages\nreasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)\nbackbone within a Vision-Language Model and pretrains it end-to-end using a\nrich corpus of visual instruction data curated from target downstream domains.\nViTP is powered by our proposed Visual Robustness Learning (VRL), which compels\nthe ViT to learn robust and domain-relevant features from a sparse set of\nvisual tokens. Extensive experiments on 16 challenging remote sensing and\nmedical imaging benchmarks demonstrate that ViTP establishes new\nstate-of-the-art performance across a diverse range of downstream tasks. The\ncode is available at github.com/zcablii/ViTP.\n", "link": "http://arxiv.org/abs/2509.17562v1", "date": "2025-09-22", "relevancy": 3.0639, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6299}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Instruction%20Pretraining%20for%20Domain-Specific%20Foundation%20Models&body=Title%3A%20Visual%20Instruction%20Pretraining%20for%20Domain-Specific%20Foundation%20Models%0AAuthor%3A%20Yuxuan%20Li%20and%20Yicheng%20Zhang%20and%20Wenhao%20Tang%20and%20Yimian%20Dai%20and%20Ming-Ming%20Cheng%20and%20Xiang%20Li%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Modern%20computer%20vision%20is%20converging%20on%20a%20closed%20loop%20in%20which%20perception%2C%0Areasoning%20and%20generation%20mutually%20reinforce%20each%20other.%20However%2C%20this%20loop%0Aremains%20incomplete%3A%20the%20top-down%20influence%20of%20high-level%20reasoning%20on%20the%0Afoundational%20learning%20of%20low-level%20perceptual%20features%20is%20not%20yet%0Aunderexplored.%20This%20paper%20addresses%20this%20gap%20by%20proposing%20a%20new%20paradigm%20for%0Apretraining%20foundation%20models%20in%20downstream%20domains.%20We%20introduce%20Visual%0AinsTruction%20Pretraining%20%28ViTP%29%2C%20a%20novel%20approach%20that%20directly%20leverages%0Areasoning%20to%20enhance%20perception.%20ViTP%20embeds%20a%20Vision%20Transformer%20%28ViT%29%0Abackbone%20within%20a%20Vision-Language%20Model%20and%20pretrains%20it%20end-to-end%20using%20a%0Arich%20corpus%20of%20visual%20instruction%20data%20curated%20from%20target%20downstream%20domains.%0AViTP%20is%20powered%20by%20our%20proposed%20Visual%20Robustness%20Learning%20%28VRL%29%2C%20which%20compels%0Athe%20ViT%20to%20learn%20robust%20and%20domain-relevant%20features%20from%20a%20sparse%20set%20of%0Avisual%20tokens.%20Extensive%20experiments%20on%2016%20challenging%20remote%20sensing%20and%0Amedical%20imaging%20benchmarks%20demonstrate%20that%20ViTP%20establishes%20new%0Astate-of-the-art%20performance%20across%20a%20diverse%20range%20of%20downstream%20tasks.%20The%0Acode%20is%20available%20at%20github.com/zcablii/ViTP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Instruction%2520Pretraining%2520for%2520Domain-Specific%2520Foundation%2520Models%26entry.906535625%3DYuxuan%2520Li%2520and%2520Yicheng%2520Zhang%2520and%2520Wenhao%2520Tang%2520and%2520Yimian%2520Dai%2520and%2520Ming-Ming%2520Cheng%2520and%2520Xiang%2520Li%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Modern%2520computer%2520vision%2520is%2520converging%2520on%2520a%2520closed%2520loop%2520in%2520which%2520perception%252C%250Areasoning%2520and%2520generation%2520mutually%2520reinforce%2520each%2520other.%2520However%252C%2520this%2520loop%250Aremains%2520incomplete%253A%2520the%2520top-down%2520influence%2520of%2520high-level%2520reasoning%2520on%2520the%250Afoundational%2520learning%2520of%2520low-level%2520perceptual%2520features%2520is%2520not%2520yet%250Aunderexplored.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%2520proposing%2520a%2520new%2520paradigm%2520for%250Apretraining%2520foundation%2520models%2520in%2520downstream%2520domains.%2520We%2520introduce%2520Visual%250AinsTruction%2520Pretraining%2520%2528ViTP%2529%252C%2520a%2520novel%2520approach%2520that%2520directly%2520leverages%250Areasoning%2520to%2520enhance%2520perception.%2520ViTP%2520embeds%2520a%2520Vision%2520Transformer%2520%2528ViT%2529%250Abackbone%2520within%2520a%2520Vision-Language%2520Model%2520and%2520pretrains%2520it%2520end-to-end%2520using%2520a%250Arich%2520corpus%2520of%2520visual%2520instruction%2520data%2520curated%2520from%2520target%2520downstream%2520domains.%250AViTP%2520is%2520powered%2520by%2520our%2520proposed%2520Visual%2520Robustness%2520Learning%2520%2528VRL%2529%252C%2520which%2520compels%250Athe%2520ViT%2520to%2520learn%2520robust%2520and%2520domain-relevant%2520features%2520from%2520a%2520sparse%2520set%2520of%250Avisual%2520tokens.%2520Extensive%2520experiments%2520on%252016%2520challenging%2520remote%2520sensing%2520and%250Amedical%2520imaging%2520benchmarks%2520demonstrate%2520that%2520ViTP%2520establishes%2520new%250Astate-of-the-art%2520performance%2520across%2520a%2520diverse%2520range%2520of%2520downstream%2520tasks.%2520The%250Acode%2520is%2520available%2520at%2520github.com/zcablii/ViTP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Instruction%20Pretraining%20for%20Domain-Specific%20Foundation%20Models&entry.906535625=Yuxuan%20Li%20and%20Yicheng%20Zhang%20and%20Wenhao%20Tang%20and%20Yimian%20Dai%20and%20Ming-Ming%20Cheng%20and%20Xiang%20Li%20and%20Jian%20Yang&entry.1292438233=%20%20Modern%20computer%20vision%20is%20converging%20on%20a%20closed%20loop%20in%20which%20perception%2C%0Areasoning%20and%20generation%20mutually%20reinforce%20each%20other.%20However%2C%20this%20loop%0Aremains%20incomplete%3A%20the%20top-down%20influence%20of%20high-level%20reasoning%20on%20the%0Afoundational%20learning%20of%20low-level%20perceptual%20features%20is%20not%20yet%0Aunderexplored.%20This%20paper%20addresses%20this%20gap%20by%20proposing%20a%20new%20paradigm%20for%0Apretraining%20foundation%20models%20in%20downstream%20domains.%20We%20introduce%20Visual%0AinsTruction%20Pretraining%20%28ViTP%29%2C%20a%20novel%20approach%20that%20directly%20leverages%0Areasoning%20to%20enhance%20perception.%20ViTP%20embeds%20a%20Vision%20Transformer%20%28ViT%29%0Abackbone%20within%20a%20Vision-Language%20Model%20and%20pretrains%20it%20end-to-end%20using%20a%0Arich%20corpus%20of%20visual%20instruction%20data%20curated%20from%20target%20downstream%20domains.%0AViTP%20is%20powered%20by%20our%20proposed%20Visual%20Robustness%20Learning%20%28VRL%29%2C%20which%20compels%0Athe%20ViT%20to%20learn%20robust%20and%20domain-relevant%20features%20from%20a%20sparse%20set%20of%0Avisual%20tokens.%20Extensive%20experiments%20on%2016%20challenging%20remote%20sensing%20and%0Amedical%20imaging%20benchmarks%20demonstrate%20that%20ViTP%20establishes%20new%0Astate-of-the-art%20performance%20across%20a%20diverse%20range%20of%20downstream%20tasks.%20The%0Acode%20is%20available%20at%20github.com/zcablii/ViTP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17562v1&entry.124074799=Read"},
{"title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning", "author": "Ye Liu and Zongyang Ma and Junfu Pu and Zhongang Qi and Yang Wu and Ying Shan and Chang Wen Chen", "abstract": "  Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.\n", "link": "http://arxiv.org/abs/2509.18094v1", "date": "2025-09-22", "relevancy": 3.0268, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6195}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniPixel%3A%20Unified%20Object%20Referring%20and%20Segmentation%20for%20Pixel-Level%0A%20%20Visual%20Reasoning&body=Title%3A%20UniPixel%3A%20Unified%20Object%20Referring%20and%20Segmentation%20for%20Pixel-Level%0A%20%20Visual%20Reasoning%0AAuthor%3A%20Ye%20Liu%20and%20Zongyang%20Ma%20and%20Junfu%20Pu%20and%20Zhongang%20Qi%20and%20Yang%20Wu%20and%20Ying%20Shan%20and%20Chang%20Wen%20Chen%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Multi-modal%20Models%20%28LMMs%29%20have%20demonstrated%20their%0Aremarkable%20success%20as%20general-purpose%20multi-modal%20assistants%2C%20with%20particular%0Afocuses%20on%20holistic%20image-%20and%20video-language%20understanding.%20Conversely%2C%20less%0Aattention%20has%20been%20given%20to%20scaling%20fine-grained%20pixel-level%20understanding%0Acapabilities%2C%20where%20the%20models%20are%20expected%20to%20realize%20pixel-level%20alignment%0Abetween%20visual%20signals%20and%20language%20semantics.%20Some%20previous%20studies%20have%0Aapplied%20LMMs%20to%20related%20tasks%20such%20as%20region-level%20captioning%20and%20referring%0Aexpression%20segmentation.%20However%2C%20these%20models%20are%20limited%20to%20performing%20either%0Areferring%20or%20segmentation%20tasks%20independently%20and%20fail%20to%20integrate%20these%0Afine-grained%20perception%20capabilities%20into%20visual%20reasoning.%20To%20bridge%20this%20gap%2C%0Awe%20propose%20UniPixel%2C%20a%20large%20multi-modal%20model%20capable%20of%20flexibly%0Acomprehending%20visual%20prompt%20inputs%20and%20generating%20mask-grounded%20responses.%20Our%0Amodel%20distinguishes%20itself%20by%20seamlessly%20integrating%20pixel-level%20perception%0Awith%20general%20visual%20understanding%20capabilities.%20Specifically%2C%20UniPixel%0Aprocesses%20visual%20prompts%20and%20generates%20relevant%20masks%20on%20demand%2C%20and%20performs%0Asubsequent%20reasoning%20conditioning%20on%20these%20intermediate%20pointers%20during%0Ainference%2C%20thereby%20enabling%20fine-grained%20pixel-level%20reasoning.%20The%0Aeffectiveness%20of%20our%20approach%20has%20been%20verified%20on%2010%20benchmarks%20across%20a%0Adiverse%20set%20of%20tasks%2C%20including%20pixel-level%20referring/segmentation%20and%0Aobject-centric%20understanding%20in%20images/videos.%20A%20novel%20PixelQA%20task%20that%0Ajointly%20requires%20referring%2C%20segmentation%2C%20and%20question%20answering%20is%20also%0Adesigned%20to%20verify%20the%20flexibility%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniPixel%253A%2520Unified%2520Object%2520Referring%2520and%2520Segmentation%2520for%2520Pixel-Level%250A%2520%2520Visual%2520Reasoning%26entry.906535625%3DYe%2520Liu%2520and%2520Zongyang%2520Ma%2520and%2520Junfu%2520Pu%2520and%2520Zhongang%2520Qi%2520and%2520Yang%2520Wu%2520and%2520Ying%2520Shan%2520and%2520Chang%2520Wen%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Multi-modal%2520Models%2520%2528LMMs%2529%2520have%2520demonstrated%2520their%250Aremarkable%2520success%2520as%2520general-purpose%2520multi-modal%2520assistants%252C%2520with%2520particular%250Afocuses%2520on%2520holistic%2520image-%2520and%2520video-language%2520understanding.%2520Conversely%252C%2520less%250Aattention%2520has%2520been%2520given%2520to%2520scaling%2520fine-grained%2520pixel-level%2520understanding%250Acapabilities%252C%2520where%2520the%2520models%2520are%2520expected%2520to%2520realize%2520pixel-level%2520alignment%250Abetween%2520visual%2520signals%2520and%2520language%2520semantics.%2520Some%2520previous%2520studies%2520have%250Aapplied%2520LMMs%2520to%2520related%2520tasks%2520such%2520as%2520region-level%2520captioning%2520and%2520referring%250Aexpression%2520segmentation.%2520However%252C%2520these%2520models%2520are%2520limited%2520to%2520performing%2520either%250Areferring%2520or%2520segmentation%2520tasks%2520independently%2520and%2520fail%2520to%2520integrate%2520these%250Afine-grained%2520perception%2520capabilities%2520into%2520visual%2520reasoning.%2520To%2520bridge%2520this%2520gap%252C%250Awe%2520propose%2520UniPixel%252C%2520a%2520large%2520multi-modal%2520model%2520capable%2520of%2520flexibly%250Acomprehending%2520visual%2520prompt%2520inputs%2520and%2520generating%2520mask-grounded%2520responses.%2520Our%250Amodel%2520distinguishes%2520itself%2520by%2520seamlessly%2520integrating%2520pixel-level%2520perception%250Awith%2520general%2520visual%2520understanding%2520capabilities.%2520Specifically%252C%2520UniPixel%250Aprocesses%2520visual%2520prompts%2520and%2520generates%2520relevant%2520masks%2520on%2520demand%252C%2520and%2520performs%250Asubsequent%2520reasoning%2520conditioning%2520on%2520these%2520intermediate%2520pointers%2520during%250Ainference%252C%2520thereby%2520enabling%2520fine-grained%2520pixel-level%2520reasoning.%2520The%250Aeffectiveness%2520of%2520our%2520approach%2520has%2520been%2520verified%2520on%252010%2520benchmarks%2520across%2520a%250Adiverse%2520set%2520of%2520tasks%252C%2520including%2520pixel-level%2520referring/segmentation%2520and%250Aobject-centric%2520understanding%2520in%2520images/videos.%2520A%2520novel%2520PixelQA%2520task%2520that%250Ajointly%2520requires%2520referring%252C%2520segmentation%252C%2520and%2520question%2520answering%2520is%2520also%250Adesigned%2520to%2520verify%2520the%2520flexibility%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPixel%3A%20Unified%20Object%20Referring%20and%20Segmentation%20for%20Pixel-Level%0A%20%20Visual%20Reasoning&entry.906535625=Ye%20Liu%20and%20Zongyang%20Ma%20and%20Junfu%20Pu%20and%20Zhongang%20Qi%20and%20Yang%20Wu%20and%20Ying%20Shan%20and%20Chang%20Wen%20Chen&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Multi-modal%20Models%20%28LMMs%29%20have%20demonstrated%20their%0Aremarkable%20success%20as%20general-purpose%20multi-modal%20assistants%2C%20with%20particular%0Afocuses%20on%20holistic%20image-%20and%20video-language%20understanding.%20Conversely%2C%20less%0Aattention%20has%20been%20given%20to%20scaling%20fine-grained%20pixel-level%20understanding%0Acapabilities%2C%20where%20the%20models%20are%20expected%20to%20realize%20pixel-level%20alignment%0Abetween%20visual%20signals%20and%20language%20semantics.%20Some%20previous%20studies%20have%0Aapplied%20LMMs%20to%20related%20tasks%20such%20as%20region-level%20captioning%20and%20referring%0Aexpression%20segmentation.%20However%2C%20these%20models%20are%20limited%20to%20performing%20either%0Areferring%20or%20segmentation%20tasks%20independently%20and%20fail%20to%20integrate%20these%0Afine-grained%20perception%20capabilities%20into%20visual%20reasoning.%20To%20bridge%20this%20gap%2C%0Awe%20propose%20UniPixel%2C%20a%20large%20multi-modal%20model%20capable%20of%20flexibly%0Acomprehending%20visual%20prompt%20inputs%20and%20generating%20mask-grounded%20responses.%20Our%0Amodel%20distinguishes%20itself%20by%20seamlessly%20integrating%20pixel-level%20perception%0Awith%20general%20visual%20understanding%20capabilities.%20Specifically%2C%20UniPixel%0Aprocesses%20visual%20prompts%20and%20generates%20relevant%20masks%20on%20demand%2C%20and%20performs%0Asubsequent%20reasoning%20conditioning%20on%20these%20intermediate%20pointers%20during%0Ainference%2C%20thereby%20enabling%20fine-grained%20pixel-level%20reasoning.%20The%0Aeffectiveness%20of%20our%20approach%20has%20been%20verified%20on%2010%20benchmarks%20across%20a%0Adiverse%20set%20of%20tasks%2C%20including%20pixel-level%20referring/segmentation%20and%0Aobject-centric%20understanding%20in%20images/videos.%20A%20novel%20PixelQA%20task%20that%0Ajointly%20requires%20referring%2C%20segmentation%2C%20and%20question%20answering%20is%20also%0Adesigned%20to%20verify%20the%20flexibility%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18094v1&entry.124074799=Read"},
{"title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\n  Reconstruction", "author": "Jiahe Li and Jiawei Zhang and Youmin Zhang and Xiao Bai and Jin Zheng and Xiaohan Yu and Lin Gu", "abstract": "  Reconstructing accurate surfaces with radiance fields has achieved remarkable\nprogress in recent years. However, prevailing approaches, primarily based on\nGaussian Splatting, are increasingly constrained by representational\nbottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based\nframework that explores and extends the under-investigated potential of sparse\nvoxels for achieving accurate, detailed, and complete surface reconstruction.\nAs strengths, sparse voxels support preserving the coverage completeness and\ngeometric clarity, while corresponding challenges also arise from absent scene\nconstraints and locality in surface refinement. To ensure correct scene\nconvergence, we first propose a Voxel-Uncertainty Depth Constraint that\nmaximizes the effect of monocular depth cues while presenting a voxel-oriented\nuncertainty to avoid quality degradation, enabling effective and robust scene\nconstraints yet preserving highly accurate geometries. Subsequently, Sparse\nVoxel Surface Regularization is designed to enhance geometric consistency for\ntiny voxels and facilitate the voxel-based formation of sharp and accurate\nsurfaces. Extensive experiments demonstrate our superior performance compared\nto existing methods across diverse challenging scenarios, excelling in\ngeometric accuracy, detail preservation, and reconstruction completeness while\nmaintaining high efficiency. Code is available at\nhttps://github.com/Fictionarry/GeoSVR.\n", "link": "http://arxiv.org/abs/2509.18090v1", "date": "2025-09-22", "relevancy": 3.0038, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6361}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5972}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSVR%3A%20Taming%20Sparse%20Voxels%20for%20Geometrically%20Accurate%20Surface%0A%20%20Reconstruction&body=Title%3A%20GeoSVR%3A%20Taming%20Sparse%20Voxels%20for%20Geometrically%20Accurate%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Jiahe%20Li%20and%20Jiawei%20Zhang%20and%20Youmin%20Zhang%20and%20Xiao%20Bai%20and%20Jin%20Zheng%20and%20Xiaohan%20Yu%20and%20Lin%20Gu%0AAbstract%3A%20%20%20Reconstructing%20accurate%20surfaces%20with%20radiance%20fields%20has%20achieved%20remarkable%0Aprogress%20in%20recent%20years.%20However%2C%20prevailing%20approaches%2C%20primarily%20based%20on%0AGaussian%20Splatting%2C%20are%20increasingly%20constrained%20by%20representational%0Abottlenecks.%20In%20this%20paper%2C%20we%20introduce%20GeoSVR%2C%20an%20explicit%20voxel-based%0Aframework%20that%20explores%20and%20extends%20the%20under-investigated%20potential%20of%20sparse%0Avoxels%20for%20achieving%20accurate%2C%20detailed%2C%20and%20complete%20surface%20reconstruction.%0AAs%20strengths%2C%20sparse%20voxels%20support%20preserving%20the%20coverage%20completeness%20and%0Ageometric%20clarity%2C%20while%20corresponding%20challenges%20also%20arise%20from%20absent%20scene%0Aconstraints%20and%20locality%20in%20surface%20refinement.%20To%20ensure%20correct%20scene%0Aconvergence%2C%20we%20first%20propose%20a%20Voxel-Uncertainty%20Depth%20Constraint%20that%0Amaximizes%20the%20effect%20of%20monocular%20depth%20cues%20while%20presenting%20a%20voxel-oriented%0Auncertainty%20to%20avoid%20quality%20degradation%2C%20enabling%20effective%20and%20robust%20scene%0Aconstraints%20yet%20preserving%20highly%20accurate%20geometries.%20Subsequently%2C%20Sparse%0AVoxel%20Surface%20Regularization%20is%20designed%20to%20enhance%20geometric%20consistency%20for%0Atiny%20voxels%20and%20facilitate%20the%20voxel-based%20formation%20of%20sharp%20and%20accurate%0Asurfaces.%20Extensive%20experiments%20demonstrate%20our%20superior%20performance%20compared%0Ato%20existing%20methods%20across%20diverse%20challenging%20scenarios%2C%20excelling%20in%0Ageometric%20accuracy%2C%20detail%20preservation%2C%20and%20reconstruction%20completeness%20while%0Amaintaining%20high%20efficiency.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Fictionarry/GeoSVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSVR%253A%2520Taming%2520Sparse%2520Voxels%2520for%2520Geometrically%2520Accurate%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DJiahe%2520Li%2520and%2520Jiawei%2520Zhang%2520and%2520Youmin%2520Zhang%2520and%2520Xiao%2520Bai%2520and%2520Jin%2520Zheng%2520and%2520Xiaohan%2520Yu%2520and%2520Lin%2520Gu%26entry.1292438233%3D%2520%2520Reconstructing%2520accurate%2520surfaces%2520with%2520radiance%2520fields%2520has%2520achieved%2520remarkable%250Aprogress%2520in%2520recent%2520years.%2520However%252C%2520prevailing%2520approaches%252C%2520primarily%2520based%2520on%250AGaussian%2520Splatting%252C%2520are%2520increasingly%2520constrained%2520by%2520representational%250Abottlenecks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GeoSVR%252C%2520an%2520explicit%2520voxel-based%250Aframework%2520that%2520explores%2520and%2520extends%2520the%2520under-investigated%2520potential%2520of%2520sparse%250Avoxels%2520for%2520achieving%2520accurate%252C%2520detailed%252C%2520and%2520complete%2520surface%2520reconstruction.%250AAs%2520strengths%252C%2520sparse%2520voxels%2520support%2520preserving%2520the%2520coverage%2520completeness%2520and%250Ageometric%2520clarity%252C%2520while%2520corresponding%2520challenges%2520also%2520arise%2520from%2520absent%2520scene%250Aconstraints%2520and%2520locality%2520in%2520surface%2520refinement.%2520To%2520ensure%2520correct%2520scene%250Aconvergence%252C%2520we%2520first%2520propose%2520a%2520Voxel-Uncertainty%2520Depth%2520Constraint%2520that%250Amaximizes%2520the%2520effect%2520of%2520monocular%2520depth%2520cues%2520while%2520presenting%2520a%2520voxel-oriented%250Auncertainty%2520to%2520avoid%2520quality%2520degradation%252C%2520enabling%2520effective%2520and%2520robust%2520scene%250Aconstraints%2520yet%2520preserving%2520highly%2520accurate%2520geometries.%2520Subsequently%252C%2520Sparse%250AVoxel%2520Surface%2520Regularization%2520is%2520designed%2520to%2520enhance%2520geometric%2520consistency%2520for%250Atiny%2520voxels%2520and%2520facilitate%2520the%2520voxel-based%2520formation%2520of%2520sharp%2520and%2520accurate%250Asurfaces.%2520Extensive%2520experiments%2520demonstrate%2520our%2520superior%2520performance%2520compared%250Ato%2520existing%2520methods%2520across%2520diverse%2520challenging%2520scenarios%252C%2520excelling%2520in%250Ageometric%2520accuracy%252C%2520detail%2520preservation%252C%2520and%2520reconstruction%2520completeness%2520while%250Amaintaining%2520high%2520efficiency.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Fictionarry/GeoSVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSVR%3A%20Taming%20Sparse%20Voxels%20for%20Geometrically%20Accurate%20Surface%0A%20%20Reconstruction&entry.906535625=Jiahe%20Li%20and%20Jiawei%20Zhang%20and%20Youmin%20Zhang%20and%20Xiao%20Bai%20and%20Jin%20Zheng%20and%20Xiaohan%20Yu%20and%20Lin%20Gu&entry.1292438233=%20%20Reconstructing%20accurate%20surfaces%20with%20radiance%20fields%20has%20achieved%20remarkable%0Aprogress%20in%20recent%20years.%20However%2C%20prevailing%20approaches%2C%20primarily%20based%20on%0AGaussian%20Splatting%2C%20are%20increasingly%20constrained%20by%20representational%0Abottlenecks.%20In%20this%20paper%2C%20we%20introduce%20GeoSVR%2C%20an%20explicit%20voxel-based%0Aframework%20that%20explores%20and%20extends%20the%20under-investigated%20potential%20of%20sparse%0Avoxels%20for%20achieving%20accurate%2C%20detailed%2C%20and%20complete%20surface%20reconstruction.%0AAs%20strengths%2C%20sparse%20voxels%20support%20preserving%20the%20coverage%20completeness%20and%0Ageometric%20clarity%2C%20while%20corresponding%20challenges%20also%20arise%20from%20absent%20scene%0Aconstraints%20and%20locality%20in%20surface%20refinement.%20To%20ensure%20correct%20scene%0Aconvergence%2C%20we%20first%20propose%20a%20Voxel-Uncertainty%20Depth%20Constraint%20that%0Amaximizes%20the%20effect%20of%20monocular%20depth%20cues%20while%20presenting%20a%20voxel-oriented%0Auncertainty%20to%20avoid%20quality%20degradation%2C%20enabling%20effective%20and%20robust%20scene%0Aconstraints%20yet%20preserving%20highly%20accurate%20geometries.%20Subsequently%2C%20Sparse%0AVoxel%20Surface%20Regularization%20is%20designed%20to%20enhance%20geometric%20consistency%20for%0Atiny%20voxels%20and%20facilitate%20the%20voxel-based%20formation%20of%20sharp%20and%20accurate%0Asurfaces.%20Extensive%20experiments%20demonstrate%20our%20superior%20performance%20compared%0Ato%20existing%20methods%20across%20diverse%20challenging%20scenarios%2C%20excelling%20in%0Ageometric%20accuracy%2C%20detail%20preservation%2C%20and%20reconstruction%20completeness%20while%0Amaintaining%20high%20efficiency.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Fictionarry/GeoSVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18090v1&entry.124074799=Read"},
{"title": "DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for\n  Visuomotor Diffusion Policy Learning", "author": "ThankGod Egbe and Peng Wang and Zhihao Guo and Zidong Chen", "abstract": "  This paper evaluates DINOv3, a recent large-scale self-supervised vision\nbackbone, for visuomotor diffusion policy learning in robotic manipulation. We\ninvestigate whether a purely self-supervised encoder can match or surpass\nconventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under\nthree regimes: training from scratch, frozen, and finetuned. Across four\nbenchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned\ndiffusion policy, we find that (i) finetuned DINOv3 matches or exceeds\nResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating\nstrong transferable priors, and (iii) self-supervised features improve sample\nefficiency and robustness. These results support self-supervised large visual\nmodels as effective, generalizable perceptual front-ends for action diffusion\npolicies, motivating further exploration of scalable label-free pretraining in\nrobotic manipulation. Compared to using ResNet18 as a backbone, our approach\nwith DINOv3 achieves up to a 10% absolute increase in test-time success rates\non challenging tasks such as Can, and on-the-par performance in tasks like\nLift, PushT, and Square.\n", "link": "http://arxiv.org/abs/2509.17684v1", "date": "2025-09-22", "relevancy": 2.9424, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5923}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINOv3-Diffusion%20Policy%3A%20Self-Supervised%20Large%20Visual%20Model%20for%0A%20%20Visuomotor%20Diffusion%20Policy%20Learning&body=Title%3A%20DINOv3-Diffusion%20Policy%3A%20Self-Supervised%20Large%20Visual%20Model%20for%0A%20%20Visuomotor%20Diffusion%20Policy%20Learning%0AAuthor%3A%20ThankGod%20Egbe%20and%20Peng%20Wang%20and%20Zhihao%20Guo%20and%20Zidong%20Chen%0AAbstract%3A%20%20%20This%20paper%20evaluates%20DINOv3%2C%20a%20recent%20large-scale%20self-supervised%20vision%0Abackbone%2C%20for%20visuomotor%20diffusion%20policy%20learning%20in%20robotic%20manipulation.%20We%0Ainvestigate%20whether%20a%20purely%20self-supervised%20encoder%20can%20match%20or%20surpass%0Aconventional%20supervised%20ImageNet-pretrained%20backbones%20%28e.g.%2C%20ResNet-18%29%20under%0Athree%20regimes%3A%20training%20from%20scratch%2C%20frozen%2C%20and%20finetuned.%20Across%20four%0Abenchmark%20tasks%20%28Push-T%2C%20Lift%2C%20Can%2C%20Square%29%20using%20a%20unified%20FiLM-conditioned%0Adiffusion%20policy%2C%20we%20find%20that%20%28i%29%20finetuned%20DINOv3%20matches%20or%20exceeds%0AResNet-18%20on%20several%20tasks%2C%20%28ii%29%20frozen%20DINOv3%20remains%20competitive%2C%20indicating%0Astrong%20transferable%20priors%2C%20and%20%28iii%29%20self-supervised%20features%20improve%20sample%0Aefficiency%20and%20robustness.%20These%20results%20support%20self-supervised%20large%20visual%0Amodels%20as%20effective%2C%20generalizable%20perceptual%20front-ends%20for%20action%20diffusion%0Apolicies%2C%20motivating%20further%20exploration%20of%20scalable%20label-free%20pretraining%20in%0Arobotic%20manipulation.%20Compared%20to%20using%20ResNet18%20as%20a%20backbone%2C%20our%20approach%0Awith%20DINOv3%20achieves%20up%20to%20a%2010%25%20absolute%20increase%20in%20test-time%20success%20rates%0Aon%20challenging%20tasks%20such%20as%20Can%2C%20and%20on-the-par%20performance%20in%20tasks%20like%0ALift%2C%20PushT%2C%20and%20Square.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINOv3-Diffusion%2520Policy%253A%2520Self-Supervised%2520Large%2520Visual%2520Model%2520for%250A%2520%2520Visuomotor%2520Diffusion%2520Policy%2520Learning%26entry.906535625%3DThankGod%2520Egbe%2520and%2520Peng%2520Wang%2520and%2520Zhihao%2520Guo%2520and%2520Zidong%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520evaluates%2520DINOv3%252C%2520a%2520recent%2520large-scale%2520self-supervised%2520vision%250Abackbone%252C%2520for%2520visuomotor%2520diffusion%2520policy%2520learning%2520in%2520robotic%2520manipulation.%2520We%250Ainvestigate%2520whether%2520a%2520purely%2520self-supervised%2520encoder%2520can%2520match%2520or%2520surpass%250Aconventional%2520supervised%2520ImageNet-pretrained%2520backbones%2520%2528e.g.%252C%2520ResNet-18%2529%2520under%250Athree%2520regimes%253A%2520training%2520from%2520scratch%252C%2520frozen%252C%2520and%2520finetuned.%2520Across%2520four%250Abenchmark%2520tasks%2520%2528Push-T%252C%2520Lift%252C%2520Can%252C%2520Square%2529%2520using%2520a%2520unified%2520FiLM-conditioned%250Adiffusion%2520policy%252C%2520we%2520find%2520that%2520%2528i%2529%2520finetuned%2520DINOv3%2520matches%2520or%2520exceeds%250AResNet-18%2520on%2520several%2520tasks%252C%2520%2528ii%2529%2520frozen%2520DINOv3%2520remains%2520competitive%252C%2520indicating%250Astrong%2520transferable%2520priors%252C%2520and%2520%2528iii%2529%2520self-supervised%2520features%2520improve%2520sample%250Aefficiency%2520and%2520robustness.%2520These%2520results%2520support%2520self-supervised%2520large%2520visual%250Amodels%2520as%2520effective%252C%2520generalizable%2520perceptual%2520front-ends%2520for%2520action%2520diffusion%250Apolicies%252C%2520motivating%2520further%2520exploration%2520of%2520scalable%2520label-free%2520pretraining%2520in%250Arobotic%2520manipulation.%2520Compared%2520to%2520using%2520ResNet18%2520as%2520a%2520backbone%252C%2520our%2520approach%250Awith%2520DINOv3%2520achieves%2520up%2520to%2520a%252010%2525%2520absolute%2520increase%2520in%2520test-time%2520success%2520rates%250Aon%2520challenging%2520tasks%2520such%2520as%2520Can%252C%2520and%2520on-the-par%2520performance%2520in%2520tasks%2520like%250ALift%252C%2520PushT%252C%2520and%2520Square.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINOv3-Diffusion%20Policy%3A%20Self-Supervised%20Large%20Visual%20Model%20for%0A%20%20Visuomotor%20Diffusion%20Policy%20Learning&entry.906535625=ThankGod%20Egbe%20and%20Peng%20Wang%20and%20Zhihao%20Guo%20and%20Zidong%20Chen&entry.1292438233=%20%20This%20paper%20evaluates%20DINOv3%2C%20a%20recent%20large-scale%20self-supervised%20vision%0Abackbone%2C%20for%20visuomotor%20diffusion%20policy%20learning%20in%20robotic%20manipulation.%20We%0Ainvestigate%20whether%20a%20purely%20self-supervised%20encoder%20can%20match%20or%20surpass%0Aconventional%20supervised%20ImageNet-pretrained%20backbones%20%28e.g.%2C%20ResNet-18%29%20under%0Athree%20regimes%3A%20training%20from%20scratch%2C%20frozen%2C%20and%20finetuned.%20Across%20four%0Abenchmark%20tasks%20%28Push-T%2C%20Lift%2C%20Can%2C%20Square%29%20using%20a%20unified%20FiLM-conditioned%0Adiffusion%20policy%2C%20we%20find%20that%20%28i%29%20finetuned%20DINOv3%20matches%20or%20exceeds%0AResNet-18%20on%20several%20tasks%2C%20%28ii%29%20frozen%20DINOv3%20remains%20competitive%2C%20indicating%0Astrong%20transferable%20priors%2C%20and%20%28iii%29%20self-supervised%20features%20improve%20sample%0Aefficiency%20and%20robustness.%20These%20results%20support%20self-supervised%20large%20visual%0Amodels%20as%20effective%2C%20generalizable%20perceptual%20front-ends%20for%20action%20diffusion%0Apolicies%2C%20motivating%20further%20exploration%20of%20scalable%20label-free%20pretraining%20in%0Arobotic%20manipulation.%20Compared%20to%20using%20ResNet18%20as%20a%20backbone%2C%20our%20approach%0Awith%20DINOv3%20achieves%20up%20to%20a%2010%25%20absolute%20increase%20in%20test-time%20success%20rates%0Aon%20challenging%20tasks%20such%20as%20Can%2C%20and%20on-the-par%20performance%20in%20tasks%20like%0ALift%2C%20PushT%2C%20and%20Square.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17684v1&entry.124074799=Read"},
{"title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model", "author": "Yihao Wang and Pengxiang Ding and Lingxiao Li and Can Cui and Zirui Ge and Xinyang Tong and Wenxuan Song and Han Zhao and Wei Zhao and Pengxu Hou and Siteng Huang and Yifan Tang and Wenhui Wang and Ru Zhang and Jianyi Liu and Donglin Wang", "abstract": "  Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.\n", "link": "http://arxiv.org/abs/2509.09372v2", "date": "2025-09-22", "relevancy": 2.8611, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA-Adapter%3A%20An%20Effective%20Paradigm%20for%20Tiny-Scale%20Vision-Language-Action%0A%20%20Model&body=Title%3A%20VLA-Adapter%3A%20An%20Effective%20Paradigm%20for%20Tiny-Scale%20Vision-Language-Action%0A%20%20Model%0AAuthor%3A%20Yihao%20Wang%20and%20Pengxiang%20Ding%20and%20Lingxiao%20Li%20and%20Can%20Cui%20and%20Zirui%20Ge%20and%20Xinyang%20Tong%20and%20Wenxuan%20Song%20and%20Han%20Zhao%20and%20Wei%20Zhao%20and%20Pengxu%20Hou%20and%20Siteng%20Huang%20and%20Yifan%20Tang%20and%20Wenhui%20Wang%20and%20Ru%20Zhang%20and%20Jianyi%20Liu%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20typically%20bridge%20the%20gap%20between%0Aperceptual%20and%20action%20spaces%20by%20pre-training%20a%20large-scale%20Vision-Language%0AModel%20%28VLM%29%20on%20robotic%20data.%20While%20this%20approach%20greatly%20enhances%20performance%2C%0Ait%20also%20incurs%20significant%20training%20costs.%20In%20this%20paper%2C%20we%20investigate%20how%20to%0Aeffectively%20bridge%20vision-language%20%28VL%29%20representations%20to%20action%20%28A%29.%20We%0Aintroduce%20VLA-Adapter%2C%20a%20novel%20paradigm%20designed%20to%20reduce%20the%20reliance%20of%20VLA%0Amodels%20on%20large-scale%20VLMs%20and%20extensive%20pre-training.%20To%20this%20end%2C%20we%20first%0Asystematically%20analyze%20the%20effectiveness%20of%20various%20VL%20conditions%20and%20present%0Akey%20findings%20on%20which%20conditions%20are%20essential%20for%20bridging%20perception%20and%0Aaction%20spaces.%20Based%20on%20these%20insights%2C%20we%20propose%20a%20lightweight%20Policy%20module%0Awith%20Bridge%20Attention%2C%20which%20autonomously%20injects%20the%20optimal%20condition%20into%0Athe%20action%20space.%20In%20this%20way%2C%20our%20method%20achieves%20high%20performance%20using%20only%0Aa%200.5B-parameter%20backbone%2C%20without%20any%20robotic%20data%20pre-training.%20Extensive%0Aexperiments%20on%20both%20simulated%20and%20real-world%20robotic%20benchmarks%20demonstrate%0Athat%20VLA-Adapter%20not%20only%20achieves%20state-of-the-art%20level%20performance%2C%20but%20also%0Aoffers%20the%20fast%20inference%20speed%20reported%20to%20date.%20Furthermore%2C%20thanks%20to%20the%0Aproposed%20advanced%20bridging%20paradigm%2C%20VLA-Adapter%20enables%20the%20training%20of%20a%0Apowerful%20VLA%20model%20in%20just%208%20hours%20on%20a%20single%20consumer-grade%20GPU%2C%20greatly%0Alowering%20the%20barrier%20to%20deploying%20the%20VLA%20model.%20Project%20page%3A%0Ahttps%3A//vla-adapter.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA-Adapter%253A%2520An%2520Effective%2520Paradigm%2520for%2520Tiny-Scale%2520Vision-Language-Action%250A%2520%2520Model%26entry.906535625%3DYihao%2520Wang%2520and%2520Pengxiang%2520Ding%2520and%2520Lingxiao%2520Li%2520and%2520Can%2520Cui%2520and%2520Zirui%2520Ge%2520and%2520Xinyang%2520Tong%2520and%2520Wenxuan%2520Song%2520and%2520Han%2520Zhao%2520and%2520Wei%2520Zhao%2520and%2520Pengxu%2520Hou%2520and%2520Siteng%2520Huang%2520and%2520Yifan%2520Tang%2520and%2520Wenhui%2520Wang%2520and%2520Ru%2520Zhang%2520and%2520Jianyi%2520Liu%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520typically%2520bridge%2520the%2520gap%2520between%250Aperceptual%2520and%2520action%2520spaces%2520by%2520pre-training%2520a%2520large-scale%2520Vision-Language%250AModel%2520%2528VLM%2529%2520on%2520robotic%2520data.%2520While%2520this%2520approach%2520greatly%2520enhances%2520performance%252C%250Ait%2520also%2520incurs%2520significant%2520training%2520costs.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520to%250Aeffectively%2520bridge%2520vision-language%2520%2528VL%2529%2520representations%2520to%2520action%2520%2528A%2529.%2520We%250Aintroduce%2520VLA-Adapter%252C%2520a%2520novel%2520paradigm%2520designed%2520to%2520reduce%2520the%2520reliance%2520of%2520VLA%250Amodels%2520on%2520large-scale%2520VLMs%2520and%2520extensive%2520pre-training.%2520To%2520this%2520end%252C%2520we%2520first%250Asystematically%2520analyze%2520the%2520effectiveness%2520of%2520various%2520VL%2520conditions%2520and%2520present%250Akey%2520findings%2520on%2520which%2520conditions%2520are%2520essential%2520for%2520bridging%2520perception%2520and%250Aaction%2520spaces.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520a%2520lightweight%2520Policy%2520module%250Awith%2520Bridge%2520Attention%252C%2520which%2520autonomously%2520injects%2520the%2520optimal%2520condition%2520into%250Athe%2520action%2520space.%2520In%2520this%2520way%252C%2520our%2520method%2520achieves%2520high%2520performance%2520using%2520only%250Aa%25200.5B-parameter%2520backbone%252C%2520without%2520any%2520robotic%2520data%2520pre-training.%2520Extensive%250Aexperiments%2520on%2520both%2520simulated%2520and%2520real-world%2520robotic%2520benchmarks%2520demonstrate%250Athat%2520VLA-Adapter%2520not%2520only%2520achieves%2520state-of-the-art%2520level%2520performance%252C%2520but%2520also%250Aoffers%2520the%2520fast%2520inference%2520speed%2520reported%2520to%2520date.%2520Furthermore%252C%2520thanks%2520to%2520the%250Aproposed%2520advanced%2520bridging%2520paradigm%252C%2520VLA-Adapter%2520enables%2520the%2520training%2520of%2520a%250Apowerful%2520VLA%2520model%2520in%2520just%25208%2520hours%2520on%2520a%2520single%2520consumer-grade%2520GPU%252C%2520greatly%250Alowering%2520the%2520barrier%2520to%2520deploying%2520the%2520VLA%2520model.%2520Project%2520page%253A%250Ahttps%253A//vla-adapter.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA-Adapter%3A%20An%20Effective%20Paradigm%20for%20Tiny-Scale%20Vision-Language-Action%0A%20%20Model&entry.906535625=Yihao%20Wang%20and%20Pengxiang%20Ding%20and%20Lingxiao%20Li%20and%20Can%20Cui%20and%20Zirui%20Ge%20and%20Xinyang%20Tong%20and%20Wenxuan%20Song%20and%20Han%20Zhao%20and%20Wei%20Zhao%20and%20Pengxu%20Hou%20and%20Siteng%20Huang%20and%20Yifan%20Tang%20and%20Wenhui%20Wang%20and%20Ru%20Zhang%20and%20Jianyi%20Liu%20and%20Donglin%20Wang&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20typically%20bridge%20the%20gap%20between%0Aperceptual%20and%20action%20spaces%20by%20pre-training%20a%20large-scale%20Vision-Language%0AModel%20%28VLM%29%20on%20robotic%20data.%20While%20this%20approach%20greatly%20enhances%20performance%2C%0Ait%20also%20incurs%20significant%20training%20costs.%20In%20this%20paper%2C%20we%20investigate%20how%20to%0Aeffectively%20bridge%20vision-language%20%28VL%29%20representations%20to%20action%20%28A%29.%20We%0Aintroduce%20VLA-Adapter%2C%20a%20novel%20paradigm%20designed%20to%20reduce%20the%20reliance%20of%20VLA%0Amodels%20on%20large-scale%20VLMs%20and%20extensive%20pre-training.%20To%20this%20end%2C%20we%20first%0Asystematically%20analyze%20the%20effectiveness%20of%20various%20VL%20conditions%20and%20present%0Akey%20findings%20on%20which%20conditions%20are%20essential%20for%20bridging%20perception%20and%0Aaction%20spaces.%20Based%20on%20these%20insights%2C%20we%20propose%20a%20lightweight%20Policy%20module%0Awith%20Bridge%20Attention%2C%20which%20autonomously%20injects%20the%20optimal%20condition%20into%0Athe%20action%20space.%20In%20this%20way%2C%20our%20method%20achieves%20high%20performance%20using%20only%0Aa%200.5B-parameter%20backbone%2C%20without%20any%20robotic%20data%20pre-training.%20Extensive%0Aexperiments%20on%20both%20simulated%20and%20real-world%20robotic%20benchmarks%20demonstrate%0Athat%20VLA-Adapter%20not%20only%20achieves%20state-of-the-art%20level%20performance%2C%20but%20also%0Aoffers%20the%20fast%20inference%20speed%20reported%20to%20date.%20Furthermore%2C%20thanks%20to%20the%0Aproposed%20advanced%20bridging%20paradigm%2C%20VLA-Adapter%20enables%20the%20training%20of%20a%0Apowerful%20VLA%20model%20in%20just%208%20hours%20on%20a%20single%20consumer-grade%20GPU%2C%20greatly%0Alowering%20the%20barrier%20to%20deploying%20the%20VLA%20model.%20Project%20page%3A%0Ahttps%3A//vla-adapter.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09372v2&entry.124074799=Read"},
{"title": "Contextual Gesture: Co-Speech Gesture Video Generation through\n  Context-aware Gesture Representation", "author": "Pinxin Liu and Pengfei Zhang and Hyeongwoo Kim and Pablo Garrido and Ari Shapiro and Kyle Olszewski", "abstract": "  Co-speech gesture generation is crucial for creating lifelike avatars and\nenhancing human-computer interactions by synchronizing gestures with speech.\nDespite recent advancements, existing methods struggle with accurately\nidentifying the rhythmic or semantic triggers from audio for generating\ncontextualized gesture patterns and achieving pixel-level realism. To address\nthese challenges, we introduce Contextual Gesture, a framework that improves\nco-speech gesture video generation through three innovative components: (1) a\nchronological speech-gesture alignment that temporally connects two modalities,\n(2) a contextualized gesture tokenization that incorporate speech context into\nmotion pattern representation through distillation, and (3) a structure-aware\nrefinement module that employs edge connection to link gesture keypoints to\nimprove video generation. Our extensive experiments demonstrate that Contextual\nGesture not only produces realistic and speech-aligned gesture videos but also\nsupports long-sequence generation and video gesture editing applications, shown\nin Fig.1.\n", "link": "http://arxiv.org/abs/2502.07239v3", "date": "2025-09-22", "relevancy": 2.854, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5738}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5737}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Gesture%3A%20Co-Speech%20Gesture%20Video%20Generation%20through%0A%20%20Context-aware%20Gesture%20Representation&body=Title%3A%20Contextual%20Gesture%3A%20Co-Speech%20Gesture%20Video%20Generation%20through%0A%20%20Context-aware%20Gesture%20Representation%0AAuthor%3A%20Pinxin%20Liu%20and%20Pengfei%20Zhang%20and%20Hyeongwoo%20Kim%20and%20Pablo%20Garrido%20and%20Ari%20Shapiro%20and%20Kyle%20Olszewski%0AAbstract%3A%20%20%20Co-speech%20gesture%20generation%20is%20crucial%20for%20creating%20lifelike%20avatars%20and%0Aenhancing%20human-computer%20interactions%20by%20synchronizing%20gestures%20with%20speech.%0ADespite%20recent%20advancements%2C%20existing%20methods%20struggle%20with%20accurately%0Aidentifying%20the%20rhythmic%20or%20semantic%20triggers%20from%20audio%20for%20generating%0Acontextualized%20gesture%20patterns%20and%20achieving%20pixel-level%20realism.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20Contextual%20Gesture%2C%20a%20framework%20that%20improves%0Aco-speech%20gesture%20video%20generation%20through%20three%20innovative%20components%3A%20%281%29%20a%0Achronological%20speech-gesture%20alignment%20that%20temporally%20connects%20two%20modalities%2C%0A%282%29%20a%20contextualized%20gesture%20tokenization%20that%20incorporate%20speech%20context%20into%0Amotion%20pattern%20representation%20through%20distillation%2C%20and%20%283%29%20a%20structure-aware%0Arefinement%20module%20that%20employs%20edge%20connection%20to%20link%20gesture%20keypoints%20to%0Aimprove%20video%20generation.%20Our%20extensive%20experiments%20demonstrate%20that%20Contextual%0AGesture%20not%20only%20produces%20realistic%20and%20speech-aligned%20gesture%20videos%20but%20also%0Asupports%20long-sequence%20generation%20and%20video%20gesture%20editing%20applications%2C%20shown%0Ain%20Fig.1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07239v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Gesture%253A%2520Co-Speech%2520Gesture%2520Video%2520Generation%2520through%250A%2520%2520Context-aware%2520Gesture%2520Representation%26entry.906535625%3DPinxin%2520Liu%2520and%2520Pengfei%2520Zhang%2520and%2520Hyeongwoo%2520Kim%2520and%2520Pablo%2520Garrido%2520and%2520Ari%2520Shapiro%2520and%2520Kyle%2520Olszewski%26entry.1292438233%3D%2520%2520Co-speech%2520gesture%2520generation%2520is%2520crucial%2520for%2520creating%2520lifelike%2520avatars%2520and%250Aenhancing%2520human-computer%2520interactions%2520by%2520synchronizing%2520gestures%2520with%2520speech.%250ADespite%2520recent%2520advancements%252C%2520existing%2520methods%2520struggle%2520with%2520accurately%250Aidentifying%2520the%2520rhythmic%2520or%2520semantic%2520triggers%2520from%2520audio%2520for%2520generating%250Acontextualized%2520gesture%2520patterns%2520and%2520achieving%2520pixel-level%2520realism.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520Contextual%2520Gesture%252C%2520a%2520framework%2520that%2520improves%250Aco-speech%2520gesture%2520video%2520generation%2520through%2520three%2520innovative%2520components%253A%2520%25281%2529%2520a%250Achronological%2520speech-gesture%2520alignment%2520that%2520temporally%2520connects%2520two%2520modalities%252C%250A%25282%2529%2520a%2520contextualized%2520gesture%2520tokenization%2520that%2520incorporate%2520speech%2520context%2520into%250Amotion%2520pattern%2520representation%2520through%2520distillation%252C%2520and%2520%25283%2529%2520a%2520structure-aware%250Arefinement%2520module%2520that%2520employs%2520edge%2520connection%2520to%2520link%2520gesture%2520keypoints%2520to%250Aimprove%2520video%2520generation.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%2520Contextual%250AGesture%2520not%2520only%2520produces%2520realistic%2520and%2520speech-aligned%2520gesture%2520videos%2520but%2520also%250Asupports%2520long-sequence%2520generation%2520and%2520video%2520gesture%2520editing%2520applications%252C%2520shown%250Ain%2520Fig.1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07239v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Gesture%3A%20Co-Speech%20Gesture%20Video%20Generation%20through%0A%20%20Context-aware%20Gesture%20Representation&entry.906535625=Pinxin%20Liu%20and%20Pengfei%20Zhang%20and%20Hyeongwoo%20Kim%20and%20Pablo%20Garrido%20and%20Ari%20Shapiro%20and%20Kyle%20Olszewski&entry.1292438233=%20%20Co-speech%20gesture%20generation%20is%20crucial%20for%20creating%20lifelike%20avatars%20and%0Aenhancing%20human-computer%20interactions%20by%20synchronizing%20gestures%20with%20speech.%0ADespite%20recent%20advancements%2C%20existing%20methods%20struggle%20with%20accurately%0Aidentifying%20the%20rhythmic%20or%20semantic%20triggers%20from%20audio%20for%20generating%0Acontextualized%20gesture%20patterns%20and%20achieving%20pixel-level%20realism.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20Contextual%20Gesture%2C%20a%20framework%20that%20improves%0Aco-speech%20gesture%20video%20generation%20through%20three%20innovative%20components%3A%20%281%29%20a%0Achronological%20speech-gesture%20alignment%20that%20temporally%20connects%20two%20modalities%2C%0A%282%29%20a%20contextualized%20gesture%20tokenization%20that%20incorporate%20speech%20context%20into%0Amotion%20pattern%20representation%20through%20distillation%2C%20and%20%283%29%20a%20structure-aware%0Arefinement%20module%20that%20employs%20edge%20connection%20to%20link%20gesture%20keypoints%20to%0Aimprove%20video%20generation.%20Our%20extensive%20experiments%20demonstrate%20that%20Contextual%0AGesture%20not%20only%20produces%20realistic%20and%20speech-aligned%20gesture%20videos%20but%20also%0Asupports%20long-sequence%20generation%20and%20video%20gesture%20editing%20applications%2C%20shown%0Ain%20Fig.1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07239v3&entry.124074799=Read"},
{"title": "Unsupervised Interpretable Basis Extraction for Concept-Based Visual\n  Explanations", "author": "Alexandros Doumanoglou and Stylianos Asteriadis and Dimitrios Zarpalas", "abstract": "  An important line of research attempts to explain CNN image classifier\npredictions and intermediate layer representations in terms of\nhuman-understandable concepts. Previous work supports that deep representations\nare linearly separable with respect to their concept label, implying that the\nfeature space has directions where intermediate representations may be\nprojected onto, to become more understandable. These directions are called\ninterpretable, and when considered as a set, they may form an interpretable\nfeature space basis. Compared to previous top-down probing approaches which use\nconcept annotations to identify the interpretable directions one at a time, in\nthis work, we take a bottom-up approach, identifying the directions from the\nstructure of the feature space, collectively, without relying on supervision\nfrom concept labels. Instead, we learn the directions by optimizing for a\nsparsity property that holds for any interpretable basis. We experiment with\nexisting popular CNNs and demonstrate the effectiveness of our method in\nextracting an interpretable basis across network architectures and training\ndatasets. We make extensions to existing basis interpretability metrics and\nshow that intermediate layer representations become more interpretable when\ntransformed with the extracted bases. Finally, we compare the bases extracted\nwith our method with the bases derived with supervision and find that, in one\naspect, unsupervised basis extraction has a strength that constitutes a\nlimitation of learning the basis with supervision, and we provide potential\ndirections for future research.\n", "link": "http://arxiv.org/abs/2303.10523v3", "date": "2025-09-22", "relevancy": 2.8161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Interpretable%20Basis%20Extraction%20for%20Concept-Based%20Visual%0A%20%20Explanations&body=Title%3A%20Unsupervised%20Interpretable%20Basis%20Extraction%20for%20Concept-Based%20Visual%0A%20%20Explanations%0AAuthor%3A%20Alexandros%20Doumanoglou%20and%20Stylianos%20Asteriadis%20and%20Dimitrios%20Zarpalas%0AAbstract%3A%20%20%20An%20important%20line%20of%20research%20attempts%20to%20explain%20CNN%20image%20classifier%0Apredictions%20and%20intermediate%20layer%20representations%20in%20terms%20of%0Ahuman-understandable%20concepts.%20Previous%20work%20supports%20that%20deep%20representations%0Aare%20linearly%20separable%20with%20respect%20to%20their%20concept%20label%2C%20implying%20that%20the%0Afeature%20space%20has%20directions%20where%20intermediate%20representations%20may%20be%0Aprojected%20onto%2C%20to%20become%20more%20understandable.%20These%20directions%20are%20called%0Ainterpretable%2C%20and%20when%20considered%20as%20a%20set%2C%20they%20may%20form%20an%20interpretable%0Afeature%20space%20basis.%20Compared%20to%20previous%20top-down%20probing%20approaches%20which%20use%0Aconcept%20annotations%20to%20identify%20the%20interpretable%20directions%20one%20at%20a%20time%2C%20in%0Athis%20work%2C%20we%20take%20a%20bottom-up%20approach%2C%20identifying%20the%20directions%20from%20the%0Astructure%20of%20the%20feature%20space%2C%20collectively%2C%20without%20relying%20on%20supervision%0Afrom%20concept%20labels.%20Instead%2C%20we%20learn%20the%20directions%20by%20optimizing%20for%20a%0Asparsity%20property%20that%20holds%20for%20any%20interpretable%20basis.%20We%20experiment%20with%0Aexisting%20popular%20CNNs%20and%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%0Aextracting%20an%20interpretable%20basis%20across%20network%20architectures%20and%20training%0Adatasets.%20We%20make%20extensions%20to%20existing%20basis%20interpretability%20metrics%20and%0Ashow%20that%20intermediate%20layer%20representations%20become%20more%20interpretable%20when%0Atransformed%20with%20the%20extracted%20bases.%20Finally%2C%20we%20compare%20the%20bases%20extracted%0Awith%20our%20method%20with%20the%20bases%20derived%20with%20supervision%20and%20find%20that%2C%20in%20one%0Aaspect%2C%20unsupervised%20basis%20extraction%20has%20a%20strength%20that%20constitutes%20a%0Alimitation%20of%20learning%20the%20basis%20with%20supervision%2C%20and%20we%20provide%20potential%0Adirections%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.10523v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Interpretable%2520Basis%2520Extraction%2520for%2520Concept-Based%2520Visual%250A%2520%2520Explanations%26entry.906535625%3DAlexandros%2520Doumanoglou%2520and%2520Stylianos%2520Asteriadis%2520and%2520Dimitrios%2520Zarpalas%26entry.1292438233%3D%2520%2520An%2520important%2520line%2520of%2520research%2520attempts%2520to%2520explain%2520CNN%2520image%2520classifier%250Apredictions%2520and%2520intermediate%2520layer%2520representations%2520in%2520terms%2520of%250Ahuman-understandable%2520concepts.%2520Previous%2520work%2520supports%2520that%2520deep%2520representations%250Aare%2520linearly%2520separable%2520with%2520respect%2520to%2520their%2520concept%2520label%252C%2520implying%2520that%2520the%250Afeature%2520space%2520has%2520directions%2520where%2520intermediate%2520representations%2520may%2520be%250Aprojected%2520onto%252C%2520to%2520become%2520more%2520understandable.%2520These%2520directions%2520are%2520called%250Ainterpretable%252C%2520and%2520when%2520considered%2520as%2520a%2520set%252C%2520they%2520may%2520form%2520an%2520interpretable%250Afeature%2520space%2520basis.%2520Compared%2520to%2520previous%2520top-down%2520probing%2520approaches%2520which%2520use%250Aconcept%2520annotations%2520to%2520identify%2520the%2520interpretable%2520directions%2520one%2520at%2520a%2520time%252C%2520in%250Athis%2520work%252C%2520we%2520take%2520a%2520bottom-up%2520approach%252C%2520identifying%2520the%2520directions%2520from%2520the%250Astructure%2520of%2520the%2520feature%2520space%252C%2520collectively%252C%2520without%2520relying%2520on%2520supervision%250Afrom%2520concept%2520labels.%2520Instead%252C%2520we%2520learn%2520the%2520directions%2520by%2520optimizing%2520for%2520a%250Asparsity%2520property%2520that%2520holds%2520for%2520any%2520interpretable%2520basis.%2520We%2520experiment%2520with%250Aexisting%2520popular%2520CNNs%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%250Aextracting%2520an%2520interpretable%2520basis%2520across%2520network%2520architectures%2520and%2520training%250Adatasets.%2520We%2520make%2520extensions%2520to%2520existing%2520basis%2520interpretability%2520metrics%2520and%250Ashow%2520that%2520intermediate%2520layer%2520representations%2520become%2520more%2520interpretable%2520when%250Atransformed%2520with%2520the%2520extracted%2520bases.%2520Finally%252C%2520we%2520compare%2520the%2520bases%2520extracted%250Awith%2520our%2520method%2520with%2520the%2520bases%2520derived%2520with%2520supervision%2520and%2520find%2520that%252C%2520in%2520one%250Aaspect%252C%2520unsupervised%2520basis%2520extraction%2520has%2520a%2520strength%2520that%2520constitutes%2520a%250Alimitation%2520of%2520learning%2520the%2520basis%2520with%2520supervision%252C%2520and%2520we%2520provide%2520potential%250Adirections%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.10523v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Interpretable%20Basis%20Extraction%20for%20Concept-Based%20Visual%0A%20%20Explanations&entry.906535625=Alexandros%20Doumanoglou%20and%20Stylianos%20Asteriadis%20and%20Dimitrios%20Zarpalas&entry.1292438233=%20%20An%20important%20line%20of%20research%20attempts%20to%20explain%20CNN%20image%20classifier%0Apredictions%20and%20intermediate%20layer%20representations%20in%20terms%20of%0Ahuman-understandable%20concepts.%20Previous%20work%20supports%20that%20deep%20representations%0Aare%20linearly%20separable%20with%20respect%20to%20their%20concept%20label%2C%20implying%20that%20the%0Afeature%20space%20has%20directions%20where%20intermediate%20representations%20may%20be%0Aprojected%20onto%2C%20to%20become%20more%20understandable.%20These%20directions%20are%20called%0Ainterpretable%2C%20and%20when%20considered%20as%20a%20set%2C%20they%20may%20form%20an%20interpretable%0Afeature%20space%20basis.%20Compared%20to%20previous%20top-down%20probing%20approaches%20which%20use%0Aconcept%20annotations%20to%20identify%20the%20interpretable%20directions%20one%20at%20a%20time%2C%20in%0Athis%20work%2C%20we%20take%20a%20bottom-up%20approach%2C%20identifying%20the%20directions%20from%20the%0Astructure%20of%20the%20feature%20space%2C%20collectively%2C%20without%20relying%20on%20supervision%0Afrom%20concept%20labels.%20Instead%2C%20we%20learn%20the%20directions%20by%20optimizing%20for%20a%0Asparsity%20property%20that%20holds%20for%20any%20interpretable%20basis.%20We%20experiment%20with%0Aexisting%20popular%20CNNs%20and%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%0Aextracting%20an%20interpretable%20basis%20across%20network%20architectures%20and%20training%0Adatasets.%20We%20make%20extensions%20to%20existing%20basis%20interpretability%20metrics%20and%0Ashow%20that%20intermediate%20layer%20representations%20become%20more%20interpretable%20when%0Atransformed%20with%20the%20extracted%20bases.%20Finally%2C%20we%20compare%20the%20bases%20extracted%0Awith%20our%20method%20with%20the%20bases%20derived%20with%20supervision%20and%20find%20that%2C%20in%20one%0Aaspect%2C%20unsupervised%20basis%20extraction%20has%20a%20strength%20that%20constitutes%20a%0Alimitation%20of%20learning%20the%20basis%20with%20supervision%2C%20and%20we%20provide%20potential%0Adirections%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10523v3&entry.124074799=Read"},
{"title": "MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's\n  Disease with Limited 3D MR Data", "author": "Ding Shaodong and Liu Ziyang and Zhou Yijun and Liu Tao", "abstract": "  The automatic diagnosis of Parkinson's disease is in high clinical demand due\nto its prevalence and the importance of targeted treatment. Current clinical\npractice often relies on diagnostic biomarkers in QSM and NM-MRI images.\nHowever, the lack of large, high-quality datasets makes training diagnostic\nmodels from scratch prone to overfitting. Adapting pre-trained 3D medical\nmodels is also challenging, as the diversity of medical imaging leads to\nmismatches in voxel spacing and modality between pre-training and fine-tuning\ndata. In this paper, we address these challenges by leveraging 2D vision\nfoundation models (VFMs). Specifically, we crop multiple key ROIs from NM and\nQSM images, process each ROI through separate branches to compress the ROI into\na token, and then combine these tokens into a unified patient representation\nfor classification. Within each branch, we use 2D VFMs to encode axial slices\nof the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary\nsegmentation head that steers the feature extraction toward specific brain\nnuclei. Additionally, we introduce multi-ROI supervised contrastive learning,\nwhich improves diagnostic performance by pulling together representations of\npatients from the same class while pushing away those from different classes.\nOur approach achieved first place in the MICCAI 2025 PDCADxFoundation\nchallenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled\nQSM and NM-MRI scans, outperforming the second-place method by 5.5%.These\nresults highlight the potential of 2D VFMs for clinical analysis of 3D MR\nimages.\n", "link": "http://arxiv.org/abs/2509.17566v1", "date": "2025-09-22", "relevancy": 2.801, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRN%3A%20Harnessing%202D%20Vision%20Foundation%20Models%20for%20Diagnosing%20Parkinson%27s%0A%20%20Disease%20with%20Limited%203D%20MR%20Data&body=Title%3A%20MRN%3A%20Harnessing%202D%20Vision%20Foundation%20Models%20for%20Diagnosing%20Parkinson%27s%0A%20%20Disease%20with%20Limited%203D%20MR%20Data%0AAuthor%3A%20Ding%20Shaodong%20and%20Liu%20Ziyang%20and%20Zhou%20Yijun%20and%20Liu%20Tao%0AAbstract%3A%20%20%20The%20automatic%20diagnosis%20of%20Parkinson%27s%20disease%20is%20in%20high%20clinical%20demand%20due%0Ato%20its%20prevalence%20and%20the%20importance%20of%20targeted%20treatment.%20Current%20clinical%0Apractice%20often%20relies%20on%20diagnostic%20biomarkers%20in%20QSM%20and%20NM-MRI%20images.%0AHowever%2C%20the%20lack%20of%20large%2C%20high-quality%20datasets%20makes%20training%20diagnostic%0Amodels%20from%20scratch%20prone%20to%20overfitting.%20Adapting%20pre-trained%203D%20medical%0Amodels%20is%20also%20challenging%2C%20as%20the%20diversity%20of%20medical%20imaging%20leads%20to%0Amismatches%20in%20voxel%20spacing%20and%20modality%20between%20pre-training%20and%20fine-tuning%0Adata.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20leveraging%202D%20vision%0Afoundation%20models%20%28VFMs%29.%20Specifically%2C%20we%20crop%20multiple%20key%20ROIs%20from%20NM%20and%0AQSM%20images%2C%20process%20each%20ROI%20through%20separate%20branches%20to%20compress%20the%20ROI%20into%0Aa%20token%2C%20and%20then%20combine%20these%20tokens%20into%20a%20unified%20patient%20representation%0Afor%20classification.%20Within%20each%20branch%2C%20we%20use%202D%20VFMs%20to%20encode%20axial%20slices%0Aof%20the%203D%20ROI%20volume%20and%20fuse%20them%20into%20the%20ROI%20token%2C%20guided%20by%20an%20auxiliary%0Asegmentation%20head%20that%20steers%20the%20feature%20extraction%20toward%20specific%20brain%0Anuclei.%20Additionally%2C%20we%20introduce%20multi-ROI%20supervised%20contrastive%20learning%2C%0Awhich%20improves%20diagnostic%20performance%20by%20pulling%20together%20representations%20of%0Apatients%20from%20the%20same%20class%20while%20pushing%20away%20those%20from%20different%20classes.%0AOur%20approach%20achieved%20first%20place%20in%20the%20MICCAI%202025%20PDCADxFoundation%0Achallenge%2C%20with%20an%20accuracy%20of%2086.0%25%20trained%20on%20a%20dataset%20of%20only%20300%20labeled%0AQSM%20and%20NM-MRI%20scans%2C%20outperforming%20the%20second-place%20method%20by%205.5%25.These%0Aresults%20highlight%20the%20potential%20of%202D%20VFMs%20for%20clinical%20analysis%20of%203D%20MR%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRN%253A%2520Harnessing%25202D%2520Vision%2520Foundation%2520Models%2520for%2520Diagnosing%2520Parkinson%2527s%250A%2520%2520Disease%2520with%2520Limited%25203D%2520MR%2520Data%26entry.906535625%3DDing%2520Shaodong%2520and%2520Liu%2520Ziyang%2520and%2520Zhou%2520Yijun%2520and%2520Liu%2520Tao%26entry.1292438233%3D%2520%2520The%2520automatic%2520diagnosis%2520of%2520Parkinson%2527s%2520disease%2520is%2520in%2520high%2520clinical%2520demand%2520due%250Ato%2520its%2520prevalence%2520and%2520the%2520importance%2520of%2520targeted%2520treatment.%2520Current%2520clinical%250Apractice%2520often%2520relies%2520on%2520diagnostic%2520biomarkers%2520in%2520QSM%2520and%2520NM-MRI%2520images.%250AHowever%252C%2520the%2520lack%2520of%2520large%252C%2520high-quality%2520datasets%2520makes%2520training%2520diagnostic%250Amodels%2520from%2520scratch%2520prone%2520to%2520overfitting.%2520Adapting%2520pre-trained%25203D%2520medical%250Amodels%2520is%2520also%2520challenging%252C%2520as%2520the%2520diversity%2520of%2520medical%2520imaging%2520leads%2520to%250Amismatches%2520in%2520voxel%2520spacing%2520and%2520modality%2520between%2520pre-training%2520and%2520fine-tuning%250Adata.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520challenges%2520by%2520leveraging%25202D%2520vision%250Afoundation%2520models%2520%2528VFMs%2529.%2520Specifically%252C%2520we%2520crop%2520multiple%2520key%2520ROIs%2520from%2520NM%2520and%250AQSM%2520images%252C%2520process%2520each%2520ROI%2520through%2520separate%2520branches%2520to%2520compress%2520the%2520ROI%2520into%250Aa%2520token%252C%2520and%2520then%2520combine%2520these%2520tokens%2520into%2520a%2520unified%2520patient%2520representation%250Afor%2520classification.%2520Within%2520each%2520branch%252C%2520we%2520use%25202D%2520VFMs%2520to%2520encode%2520axial%2520slices%250Aof%2520the%25203D%2520ROI%2520volume%2520and%2520fuse%2520them%2520into%2520the%2520ROI%2520token%252C%2520guided%2520by%2520an%2520auxiliary%250Asegmentation%2520head%2520that%2520steers%2520the%2520feature%2520extraction%2520toward%2520specific%2520brain%250Anuclei.%2520Additionally%252C%2520we%2520introduce%2520multi-ROI%2520supervised%2520contrastive%2520learning%252C%250Awhich%2520improves%2520diagnostic%2520performance%2520by%2520pulling%2520together%2520representations%2520of%250Apatients%2520from%2520the%2520same%2520class%2520while%2520pushing%2520away%2520those%2520from%2520different%2520classes.%250AOur%2520approach%2520achieved%2520first%2520place%2520in%2520the%2520MICCAI%25202025%2520PDCADxFoundation%250Achallenge%252C%2520with%2520an%2520accuracy%2520of%252086.0%2525%2520trained%2520on%2520a%2520dataset%2520of%2520only%2520300%2520labeled%250AQSM%2520and%2520NM-MRI%2520scans%252C%2520outperforming%2520the%2520second-place%2520method%2520by%25205.5%2525.These%250Aresults%2520highlight%2520the%2520potential%2520of%25202D%2520VFMs%2520for%2520clinical%2520analysis%2520of%25203D%2520MR%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRN%3A%20Harnessing%202D%20Vision%20Foundation%20Models%20for%20Diagnosing%20Parkinson%27s%0A%20%20Disease%20with%20Limited%203D%20MR%20Data&entry.906535625=Ding%20Shaodong%20and%20Liu%20Ziyang%20and%20Zhou%20Yijun%20and%20Liu%20Tao&entry.1292438233=%20%20The%20automatic%20diagnosis%20of%20Parkinson%27s%20disease%20is%20in%20high%20clinical%20demand%20due%0Ato%20its%20prevalence%20and%20the%20importance%20of%20targeted%20treatment.%20Current%20clinical%0Apractice%20often%20relies%20on%20diagnostic%20biomarkers%20in%20QSM%20and%20NM-MRI%20images.%0AHowever%2C%20the%20lack%20of%20large%2C%20high-quality%20datasets%20makes%20training%20diagnostic%0Amodels%20from%20scratch%20prone%20to%20overfitting.%20Adapting%20pre-trained%203D%20medical%0Amodels%20is%20also%20challenging%2C%20as%20the%20diversity%20of%20medical%20imaging%20leads%20to%0Amismatches%20in%20voxel%20spacing%20and%20modality%20between%20pre-training%20and%20fine-tuning%0Adata.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20leveraging%202D%20vision%0Afoundation%20models%20%28VFMs%29.%20Specifically%2C%20we%20crop%20multiple%20key%20ROIs%20from%20NM%20and%0AQSM%20images%2C%20process%20each%20ROI%20through%20separate%20branches%20to%20compress%20the%20ROI%20into%0Aa%20token%2C%20and%20then%20combine%20these%20tokens%20into%20a%20unified%20patient%20representation%0Afor%20classification.%20Within%20each%20branch%2C%20we%20use%202D%20VFMs%20to%20encode%20axial%20slices%0Aof%20the%203D%20ROI%20volume%20and%20fuse%20them%20into%20the%20ROI%20token%2C%20guided%20by%20an%20auxiliary%0Asegmentation%20head%20that%20steers%20the%20feature%20extraction%20toward%20specific%20brain%0Anuclei.%20Additionally%2C%20we%20introduce%20multi-ROI%20supervised%20contrastive%20learning%2C%0Awhich%20improves%20diagnostic%20performance%20by%20pulling%20together%20representations%20of%0Apatients%20from%20the%20same%20class%20while%20pushing%20away%20those%20from%20different%20classes.%0AOur%20approach%20achieved%20first%20place%20in%20the%20MICCAI%202025%20PDCADxFoundation%0Achallenge%2C%20with%20an%20accuracy%20of%2086.0%25%20trained%20on%20a%20dataset%20of%20only%20300%20labeled%0AQSM%20and%20NM-MRI%20scans%2C%20outperforming%20the%20second-place%20method%20by%205.5%25.These%0Aresults%20highlight%20the%20potential%20of%202D%20VFMs%20for%20clinical%20analysis%20of%203D%20MR%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17566v1&entry.124074799=Read"},
{"title": "GaussianPSL: A novel framework based on Gaussian Splatting for exploring\n  the Pareto frontier in multi-criteria optimization", "author": "Phuong Mai Dinh and Van-Nam Huynh", "abstract": "  Multi-objective optimization (MOO) is essential for solving complex\nreal-world problems involving multiple conflicting objectives. However, many\npractical applications - including engineering design, autonomous systems, and\nmachine learning - often yield non-convex, degenerate, or discontinuous Pareto\nfrontiers, which involve traditional scalarization and Pareto Set Learning\n(PSL) methods that struggle to approximate accurately. Existing PSL approaches\nperform well on convex fronts but tend to fail in capturing the diversity and\nstructure of irregular Pareto sets commonly observed in real-world scenarios.\nIn this paper, we propose Gaussian-PSL, a novel framework that integrates\nGaussian Splatting into PSL to address the challenges posed by non-convex\nPareto frontiers. Our method dynamically partitions the preference vector\nspace, enabling simple MLP networks to learn localized features within each\nregion, which are then integrated by an additional MLP aggregator. This\npartition-aware strategy enhances both exploration and convergence, reduces\nsensi- tivity to initialization, and improves robustness against local optima.\nWe first provide the mathematical formulation for controllable Pareto set\nlearning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL\narchitecture and evaluate its performance on synthetic and real-world\nmulti-objective benchmarks. Experimental results demonstrate that our approach\noutperforms standard PSL models in learning irregular Pareto fronts while\nmaintaining computational efficiency and model simplicity. This work offers a\nnew direction for effective and scalable MOO under challenging frontier\ngeometries.\n", "link": "http://arxiv.org/abs/2509.17889v1", "date": "2025-09-22", "relevancy": 2.7958, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5676}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5563}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianPSL%3A%20A%20novel%20framework%20based%20on%20Gaussian%20Splatting%20for%20exploring%0A%20%20the%20Pareto%20frontier%20in%20multi-criteria%20optimization&body=Title%3A%20GaussianPSL%3A%20A%20novel%20framework%20based%20on%20Gaussian%20Splatting%20for%20exploring%0A%20%20the%20Pareto%20frontier%20in%20multi-criteria%20optimization%0AAuthor%3A%20Phuong%20Mai%20Dinh%20and%20Van-Nam%20Huynh%0AAbstract%3A%20%20%20Multi-objective%20optimization%20%28MOO%29%20is%20essential%20for%20solving%20complex%0Areal-world%20problems%20involving%20multiple%20conflicting%20objectives.%20However%2C%20many%0Apractical%20applications%20-%20including%20engineering%20design%2C%20autonomous%20systems%2C%20and%0Amachine%20learning%20-%20often%20yield%20non-convex%2C%20degenerate%2C%20or%20discontinuous%20Pareto%0Afrontiers%2C%20which%20involve%20traditional%20scalarization%20and%20Pareto%20Set%20Learning%0A%28PSL%29%20methods%20that%20struggle%20to%20approximate%20accurately.%20Existing%20PSL%20approaches%0Aperform%20well%20on%20convex%20fronts%20but%20tend%20to%20fail%20in%20capturing%20the%20diversity%20and%0Astructure%20of%20irregular%20Pareto%20sets%20commonly%20observed%20in%20real-world%20scenarios.%0AIn%20this%20paper%2C%20we%20propose%20Gaussian-PSL%2C%20a%20novel%20framework%20that%20integrates%0AGaussian%20Splatting%20into%20PSL%20to%20address%20the%20challenges%20posed%20by%20non-convex%0APareto%20frontiers.%20Our%20method%20dynamically%20partitions%20the%20preference%20vector%0Aspace%2C%20enabling%20simple%20MLP%20networks%20to%20learn%20localized%20features%20within%20each%0Aregion%2C%20which%20are%20then%20integrated%20by%20an%20additional%20MLP%20aggregator.%20This%0Apartition-aware%20strategy%20enhances%20both%20exploration%20and%20convergence%2C%20reduces%0Asensi-%20tivity%20to%20initialization%2C%20and%20improves%20robustness%20against%20local%20optima.%0AWe%20first%20provide%20the%20mathematical%20formulation%20for%20controllable%20Pareto%20set%0Alearning%20using%20Gaussian%20Splat-%20ting.%20Then%2C%20we%20introduce%20the%20Gaussian-PSL%0Aarchitecture%20and%20evaluate%20its%20performance%20on%20synthetic%20and%20real-world%0Amulti-objective%20benchmarks.%20Experimental%20results%20demonstrate%20that%20our%20approach%0Aoutperforms%20standard%20PSL%20models%20in%20learning%20irregular%20Pareto%20fronts%20while%0Amaintaining%20computational%20efficiency%20and%20model%20simplicity.%20This%20work%20offers%20a%0Anew%20direction%20for%20effective%20and%20scalable%20MOO%20under%20challenging%20frontier%0Ageometries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianPSL%253A%2520A%2520novel%2520framework%2520based%2520on%2520Gaussian%2520Splatting%2520for%2520exploring%250A%2520%2520the%2520Pareto%2520frontier%2520in%2520multi-criteria%2520optimization%26entry.906535625%3DPhuong%2520Mai%2520Dinh%2520and%2520Van-Nam%2520Huynh%26entry.1292438233%3D%2520%2520Multi-objective%2520optimization%2520%2528MOO%2529%2520is%2520essential%2520for%2520solving%2520complex%250Areal-world%2520problems%2520involving%2520multiple%2520conflicting%2520objectives.%2520However%252C%2520many%250Apractical%2520applications%2520-%2520including%2520engineering%2520design%252C%2520autonomous%2520systems%252C%2520and%250Amachine%2520learning%2520-%2520often%2520yield%2520non-convex%252C%2520degenerate%252C%2520or%2520discontinuous%2520Pareto%250Afrontiers%252C%2520which%2520involve%2520traditional%2520scalarization%2520and%2520Pareto%2520Set%2520Learning%250A%2528PSL%2529%2520methods%2520that%2520struggle%2520to%2520approximate%2520accurately.%2520Existing%2520PSL%2520approaches%250Aperform%2520well%2520on%2520convex%2520fronts%2520but%2520tend%2520to%2520fail%2520in%2520capturing%2520the%2520diversity%2520and%250Astructure%2520of%2520irregular%2520Pareto%2520sets%2520commonly%2520observed%2520in%2520real-world%2520scenarios.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520Gaussian-PSL%252C%2520a%2520novel%2520framework%2520that%2520integrates%250AGaussian%2520Splatting%2520into%2520PSL%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520non-convex%250APareto%2520frontiers.%2520Our%2520method%2520dynamically%2520partitions%2520the%2520preference%2520vector%250Aspace%252C%2520enabling%2520simple%2520MLP%2520networks%2520to%2520learn%2520localized%2520features%2520within%2520each%250Aregion%252C%2520which%2520are%2520then%2520integrated%2520by%2520an%2520additional%2520MLP%2520aggregator.%2520This%250Apartition-aware%2520strategy%2520enhances%2520both%2520exploration%2520and%2520convergence%252C%2520reduces%250Asensi-%2520tivity%2520to%2520initialization%252C%2520and%2520improves%2520robustness%2520against%2520local%2520optima.%250AWe%2520first%2520provide%2520the%2520mathematical%2520formulation%2520for%2520controllable%2520Pareto%2520set%250Alearning%2520using%2520Gaussian%2520Splat-%2520ting.%2520Then%252C%2520we%2520introduce%2520the%2520Gaussian-PSL%250Aarchitecture%2520and%2520evaluate%2520its%2520performance%2520on%2520synthetic%2520and%2520real-world%250Amulti-objective%2520benchmarks.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520standard%2520PSL%2520models%2520in%2520learning%2520irregular%2520Pareto%2520fronts%2520while%250Amaintaining%2520computational%2520efficiency%2520and%2520model%2520simplicity.%2520This%2520work%2520offers%2520a%250Anew%2520direction%2520for%2520effective%2520and%2520scalable%2520MOO%2520under%2520challenging%2520frontier%250Ageometries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianPSL%3A%20A%20novel%20framework%20based%20on%20Gaussian%20Splatting%20for%20exploring%0A%20%20the%20Pareto%20frontier%20in%20multi-criteria%20optimization&entry.906535625=Phuong%20Mai%20Dinh%20and%20Van-Nam%20Huynh&entry.1292438233=%20%20Multi-objective%20optimization%20%28MOO%29%20is%20essential%20for%20solving%20complex%0Areal-world%20problems%20involving%20multiple%20conflicting%20objectives.%20However%2C%20many%0Apractical%20applications%20-%20including%20engineering%20design%2C%20autonomous%20systems%2C%20and%0Amachine%20learning%20-%20often%20yield%20non-convex%2C%20degenerate%2C%20or%20discontinuous%20Pareto%0Afrontiers%2C%20which%20involve%20traditional%20scalarization%20and%20Pareto%20Set%20Learning%0A%28PSL%29%20methods%20that%20struggle%20to%20approximate%20accurately.%20Existing%20PSL%20approaches%0Aperform%20well%20on%20convex%20fronts%20but%20tend%20to%20fail%20in%20capturing%20the%20diversity%20and%0Astructure%20of%20irregular%20Pareto%20sets%20commonly%20observed%20in%20real-world%20scenarios.%0AIn%20this%20paper%2C%20we%20propose%20Gaussian-PSL%2C%20a%20novel%20framework%20that%20integrates%0AGaussian%20Splatting%20into%20PSL%20to%20address%20the%20challenges%20posed%20by%20non-convex%0APareto%20frontiers.%20Our%20method%20dynamically%20partitions%20the%20preference%20vector%0Aspace%2C%20enabling%20simple%20MLP%20networks%20to%20learn%20localized%20features%20within%20each%0Aregion%2C%20which%20are%20then%20integrated%20by%20an%20additional%20MLP%20aggregator.%20This%0Apartition-aware%20strategy%20enhances%20both%20exploration%20and%20convergence%2C%20reduces%0Asensi-%20tivity%20to%20initialization%2C%20and%20improves%20robustness%20against%20local%20optima.%0AWe%20first%20provide%20the%20mathematical%20formulation%20for%20controllable%20Pareto%20set%0Alearning%20using%20Gaussian%20Splat-%20ting.%20Then%2C%20we%20introduce%20the%20Gaussian-PSL%0Aarchitecture%20and%20evaluate%20its%20performance%20on%20synthetic%20and%20real-world%0Amulti-objective%20benchmarks.%20Experimental%20results%20demonstrate%20that%20our%20approach%0Aoutperforms%20standard%20PSL%20models%20in%20learning%20irregular%20Pareto%20fronts%20while%0Amaintaining%20computational%20efficiency%20and%20model%20simplicity.%20This%20work%20offers%20a%0Anew%20direction%20for%20effective%20and%20scalable%20MOO%20under%20challenging%20frontier%0Ageometries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17889v1&entry.124074799=Read"},
{"title": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?", "author": "Geewook Kim and Minjoon Seo", "abstract": "  Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.\n", "link": "http://arxiv.org/abs/2509.17901v1", "date": "2025-09-22", "relevancy": 2.7866, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5723}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Audio%20Matter%20for%20Modern%20Video-LLMs%20and%20Their%20Benchmarks%3F&body=Title%3A%20Does%20Audio%20Matter%20for%20Modern%20Video-LLMs%20and%20Their%20Benchmarks%3F%0AAuthor%3A%20Geewook%20Kim%20and%20Minjoon%20Seo%0AAbstract%3A%20%20%20Modern%20multimodal%20large%20language%20models%20often%20claim%20%22video%20understanding%2C%22%0Ayet%20most%20evaluations%20use%20muted%20videos%20or%20simply%20discard%20audio.%20We%20ask%20a%20direct%0Aquestion%3A%20how%20much%20does%20audio%20actually%20matter%20for%20contemporary%20Video-LLMs%20and%0Athe%20benchmarks%20that%20certify%20them%3F%20We%20audit%20widely%20used%20suites%20and%20observe%20that%0Amany%20items%20are%20even%20solvable%20from%20a%20single%20frame%2C%20rendering%20audio%20largely%0Aredundant.%20Building%20on%20LLaVA-OneVision%20architecture%2C%20we%20attach%20a%20speech/audio%0Aencoder%20%28e.g.%2C%20Whisper%29%20and%20analyze%20when%20audio%20helps%2C%20while%20addressing%20audio%0Atoken%20explosion%20with%20a%20lightweight%20Mamba-based%20state-space%20token%20compressor.%20We%0Afind%20that%20audio%20yields%20minimal%20gains%20on%20recent%20video%20benchmarks%20but%20is%20decisive%0Aon%20curated%2C%20audio-sensitive%20subsets.%20To%20enable%20faithful%20evaluation%2C%20we%20release%0AAVQA-Hard%20and%20Music-AVQA-Hard%2C%20our%20model%2C%20and%20code.%20Our%20findings%20surface%20a%0Agrowing%20gap%20between%20current%20academic%20practice%20and%20real-world%20expectations%2C%20and%0Aprovide%20practical%20tools%20for%20scalable%20audio-visual%20Video-LLMs.%20We%20will%20fully%0Aopen-source%20our%20work%20at%20https%3A//github.com/naver-ai/LLaVA-AV-SSM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Audio%2520Matter%2520for%2520Modern%2520Video-LLMs%2520and%2520Their%2520Benchmarks%253F%26entry.906535625%3DGeewook%2520Kim%2520and%2520Minjoon%2520Seo%26entry.1292438233%3D%2520%2520Modern%2520multimodal%2520large%2520language%2520models%2520often%2520claim%2520%2522video%2520understanding%252C%2522%250Ayet%2520most%2520evaluations%2520use%2520muted%2520videos%2520or%2520simply%2520discard%2520audio.%2520We%2520ask%2520a%2520direct%250Aquestion%253A%2520how%2520much%2520does%2520audio%2520actually%2520matter%2520for%2520contemporary%2520Video-LLMs%2520and%250Athe%2520benchmarks%2520that%2520certify%2520them%253F%2520We%2520audit%2520widely%2520used%2520suites%2520and%2520observe%2520that%250Amany%2520items%2520are%2520even%2520solvable%2520from%2520a%2520single%2520frame%252C%2520rendering%2520audio%2520largely%250Aredundant.%2520Building%2520on%2520LLaVA-OneVision%2520architecture%252C%2520we%2520attach%2520a%2520speech/audio%250Aencoder%2520%2528e.g.%252C%2520Whisper%2529%2520and%2520analyze%2520when%2520audio%2520helps%252C%2520while%2520addressing%2520audio%250Atoken%2520explosion%2520with%2520a%2520lightweight%2520Mamba-based%2520state-space%2520token%2520compressor.%2520We%250Afind%2520that%2520audio%2520yields%2520minimal%2520gains%2520on%2520recent%2520video%2520benchmarks%2520but%2520is%2520decisive%250Aon%2520curated%252C%2520audio-sensitive%2520subsets.%2520To%2520enable%2520faithful%2520evaluation%252C%2520we%2520release%250AAVQA-Hard%2520and%2520Music-AVQA-Hard%252C%2520our%2520model%252C%2520and%2520code.%2520Our%2520findings%2520surface%2520a%250Agrowing%2520gap%2520between%2520current%2520academic%2520practice%2520and%2520real-world%2520expectations%252C%2520and%250Aprovide%2520practical%2520tools%2520for%2520scalable%2520audio-visual%2520Video-LLMs.%2520We%2520will%2520fully%250Aopen-source%2520our%2520work%2520at%2520https%253A//github.com/naver-ai/LLaVA-AV-SSM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Audio%20Matter%20for%20Modern%20Video-LLMs%20and%20Their%20Benchmarks%3F&entry.906535625=Geewook%20Kim%20and%20Minjoon%20Seo&entry.1292438233=%20%20Modern%20multimodal%20large%20language%20models%20often%20claim%20%22video%20understanding%2C%22%0Ayet%20most%20evaluations%20use%20muted%20videos%20or%20simply%20discard%20audio.%20We%20ask%20a%20direct%0Aquestion%3A%20how%20much%20does%20audio%20actually%20matter%20for%20contemporary%20Video-LLMs%20and%0Athe%20benchmarks%20that%20certify%20them%3F%20We%20audit%20widely%20used%20suites%20and%20observe%20that%0Amany%20items%20are%20even%20solvable%20from%20a%20single%20frame%2C%20rendering%20audio%20largely%0Aredundant.%20Building%20on%20LLaVA-OneVision%20architecture%2C%20we%20attach%20a%20speech/audio%0Aencoder%20%28e.g.%2C%20Whisper%29%20and%20analyze%20when%20audio%20helps%2C%20while%20addressing%20audio%0Atoken%20explosion%20with%20a%20lightweight%20Mamba-based%20state-space%20token%20compressor.%20We%0Afind%20that%20audio%20yields%20minimal%20gains%20on%20recent%20video%20benchmarks%20but%20is%20decisive%0Aon%20curated%2C%20audio-sensitive%20subsets.%20To%20enable%20faithful%20evaluation%2C%20we%20release%0AAVQA-Hard%20and%20Music-AVQA-Hard%2C%20our%20model%2C%20and%20code.%20Our%20findings%20surface%20a%0Agrowing%20gap%20between%20current%20academic%20practice%20and%20real-world%20expectations%2C%20and%0Aprovide%20practical%20tools%20for%20scalable%20audio-visual%20Video-LLMs.%20We%20will%20fully%0Aopen-source%20our%20work%20at%20https%3A//github.com/naver-ai/LLaVA-AV-SSM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17901v1&entry.124074799=Read"},
{"title": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA", "author": "Chenglin Li and Feng Han and  FengTao and Ruilin Li and Qianglong Chen and Jingqi Tong and Yin Zhang and Jiaqi Wang", "abstract": "  Large language models (LLMs) have shown promise in generating program\nworkflows for visual tasks. However, previous approaches often rely on\nclosed-source models, lack systematic reasoning, and struggle with long-form\nvideo question answering (videoQA). To address these challenges, we introduce\nthe FS-VisPR framework, an adaptive visual program reasoning approach that\nbalances fast reasoning for simple queries with slow reasoning for difficult\nones. First, we design efficient visual modules (e.g., key clip retrieval and\nsubtitle retrieval) to support long-form video tasks. Then, we construct a\ndiverse and high-quality fast-slow reasoning dataset with a strong LLM to align\nopen-source language models' ability to generate visual program workflows as\nFS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple\nqueries are directly solved by VideoLLMs, while difficult ones invoke visual\nprogram reasoning, motivated by human-like reasoning processes. During this\nprocess, low-confidence fast-thinking answers will trigger a second-stage\nslow-reasoning process, and a fallback mechanism to fast reasoning is activated\nif the program execution fails. Moreover, we improve visual programs through\nparameter search during both training and inference. By adjusting the\nparameters of the visual modules within the program, multiple variants are\ngenerated: during training, programs that yield correct answers are selected,\nwhile during inference, the program with the highest confidence result is\napplied. Experiments show that FS-VisPR improves both efficiency and\nreliability in visual program workflows. It achieves 50.4% accuracy on LVBench,\nsurpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.\n", "link": "http://arxiv.org/abs/2509.17743v1", "date": "2025-09-22", "relevancy": 2.7791, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Fast-and-Slow%20Visual%20Program%20Reasoning%20for%20Long-Form%20VideoQA&body=Title%3A%20Adaptive%20Fast-and-Slow%20Visual%20Program%20Reasoning%20for%20Long-Form%20VideoQA%0AAuthor%3A%20Chenglin%20Li%20and%20Feng%20Han%20and%20%20FengTao%20and%20Ruilin%20Li%20and%20Qianglong%20Chen%20and%20Jingqi%20Tong%20and%20Yin%20Zhang%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20in%20generating%20program%0Aworkflows%20for%20visual%20tasks.%20However%2C%20previous%20approaches%20often%20rely%20on%0Aclosed-source%20models%2C%20lack%20systematic%20reasoning%2C%20and%20struggle%20with%20long-form%0Avideo%20question%20answering%20%28videoQA%29.%20To%20address%20these%20challenges%2C%20we%20introduce%0Athe%20FS-VisPR%20framework%2C%20an%20adaptive%20visual%20program%20reasoning%20approach%20that%0Abalances%20fast%20reasoning%20for%20simple%20queries%20with%20slow%20reasoning%20for%20difficult%0Aones.%20First%2C%20we%20design%20efficient%20visual%20modules%20%28e.g.%2C%20key%20clip%20retrieval%20and%0Asubtitle%20retrieval%29%20to%20support%20long-form%20video%20tasks.%20Then%2C%20we%20construct%20a%0Adiverse%20and%20high-quality%20fast-slow%20reasoning%20dataset%20with%20a%20strong%20LLM%20to%20align%0Aopen-source%20language%20models%27%20ability%20to%20generate%20visual%20program%20workflows%20as%0AFS-LLM.%20Next%2C%20we%20design%20a%20fast-slow%20reasoning%20framework%20with%20FS-LLM%3A%20Simple%0Aqueries%20are%20directly%20solved%20by%20VideoLLMs%2C%20while%20difficult%20ones%20invoke%20visual%0Aprogram%20reasoning%2C%20motivated%20by%20human-like%20reasoning%20processes.%20During%20this%0Aprocess%2C%20low-confidence%20fast-thinking%20answers%20will%20trigger%20a%20second-stage%0Aslow-reasoning%20process%2C%20and%20a%20fallback%20mechanism%20to%20fast%20reasoning%20is%20activated%0Aif%20the%20program%20execution%20fails.%20Moreover%2C%20we%20improve%20visual%20programs%20through%0Aparameter%20search%20during%20both%20training%20and%20inference.%20By%20adjusting%20the%0Aparameters%20of%20the%20visual%20modules%20within%20the%20program%2C%20multiple%20variants%20are%0Agenerated%3A%20during%20training%2C%20programs%20that%20yield%20correct%20answers%20are%20selected%2C%0Awhile%20during%20inference%2C%20the%20program%20with%20the%20highest%20confidence%20result%20is%0Aapplied.%20Experiments%20show%20that%20FS-VisPR%20improves%20both%20efficiency%20and%0Areliability%20in%20visual%20program%20workflows.%20It%20achieves%2050.4%25%20accuracy%20on%20LVBench%2C%0Asurpassing%20GPT-4o%2C%20matching%20the%20performance%20of%20Qwen2.5VL-72B%20on%20VideoMME.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Fast-and-Slow%2520Visual%2520Program%2520Reasoning%2520for%2520Long-Form%2520VideoQA%26entry.906535625%3DChenglin%2520Li%2520and%2520Feng%2520Han%2520and%2520%2520FengTao%2520and%2520Ruilin%2520Li%2520and%2520Qianglong%2520Chen%2520and%2520Jingqi%2520Tong%2520and%2520Yin%2520Zhang%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520in%2520generating%2520program%250Aworkflows%2520for%2520visual%2520tasks.%2520However%252C%2520previous%2520approaches%2520often%2520rely%2520on%250Aclosed-source%2520models%252C%2520lack%2520systematic%2520reasoning%252C%2520and%2520struggle%2520with%2520long-form%250Avideo%2520question%2520answering%2520%2528videoQA%2529.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250Athe%2520FS-VisPR%2520framework%252C%2520an%2520adaptive%2520visual%2520program%2520reasoning%2520approach%2520that%250Abalances%2520fast%2520reasoning%2520for%2520simple%2520queries%2520with%2520slow%2520reasoning%2520for%2520difficult%250Aones.%2520First%252C%2520we%2520design%2520efficient%2520visual%2520modules%2520%2528e.g.%252C%2520key%2520clip%2520retrieval%2520and%250Asubtitle%2520retrieval%2529%2520to%2520support%2520long-form%2520video%2520tasks.%2520Then%252C%2520we%2520construct%2520a%250Adiverse%2520and%2520high-quality%2520fast-slow%2520reasoning%2520dataset%2520with%2520a%2520strong%2520LLM%2520to%2520align%250Aopen-source%2520language%2520models%2527%2520ability%2520to%2520generate%2520visual%2520program%2520workflows%2520as%250AFS-LLM.%2520Next%252C%2520we%2520design%2520a%2520fast-slow%2520reasoning%2520framework%2520with%2520FS-LLM%253A%2520Simple%250Aqueries%2520are%2520directly%2520solved%2520by%2520VideoLLMs%252C%2520while%2520difficult%2520ones%2520invoke%2520visual%250Aprogram%2520reasoning%252C%2520motivated%2520by%2520human-like%2520reasoning%2520processes.%2520During%2520this%250Aprocess%252C%2520low-confidence%2520fast-thinking%2520answers%2520will%2520trigger%2520a%2520second-stage%250Aslow-reasoning%2520process%252C%2520and%2520a%2520fallback%2520mechanism%2520to%2520fast%2520reasoning%2520is%2520activated%250Aif%2520the%2520program%2520execution%2520fails.%2520Moreover%252C%2520we%2520improve%2520visual%2520programs%2520through%250Aparameter%2520search%2520during%2520both%2520training%2520and%2520inference.%2520By%2520adjusting%2520the%250Aparameters%2520of%2520the%2520visual%2520modules%2520within%2520the%2520program%252C%2520multiple%2520variants%2520are%250Agenerated%253A%2520during%2520training%252C%2520programs%2520that%2520yield%2520correct%2520answers%2520are%2520selected%252C%250Awhile%2520during%2520inference%252C%2520the%2520program%2520with%2520the%2520highest%2520confidence%2520result%2520is%250Aapplied.%2520Experiments%2520show%2520that%2520FS-VisPR%2520improves%2520both%2520efficiency%2520and%250Areliability%2520in%2520visual%2520program%2520workflows.%2520It%2520achieves%252050.4%2525%2520accuracy%2520on%2520LVBench%252C%250Asurpassing%2520GPT-4o%252C%2520matching%2520the%2520performance%2520of%2520Qwen2.5VL-72B%2520on%2520VideoMME.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Fast-and-Slow%20Visual%20Program%20Reasoning%20for%20Long-Form%20VideoQA&entry.906535625=Chenglin%20Li%20and%20Feng%20Han%20and%20%20FengTao%20and%20Ruilin%20Li%20and%20Qianglong%20Chen%20and%20Jingqi%20Tong%20and%20Yin%20Zhang%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20in%20generating%20program%0Aworkflows%20for%20visual%20tasks.%20However%2C%20previous%20approaches%20often%20rely%20on%0Aclosed-source%20models%2C%20lack%20systematic%20reasoning%2C%20and%20struggle%20with%20long-form%0Avideo%20question%20answering%20%28videoQA%29.%20To%20address%20these%20challenges%2C%20we%20introduce%0Athe%20FS-VisPR%20framework%2C%20an%20adaptive%20visual%20program%20reasoning%20approach%20that%0Abalances%20fast%20reasoning%20for%20simple%20queries%20with%20slow%20reasoning%20for%20difficult%0Aones.%20First%2C%20we%20design%20efficient%20visual%20modules%20%28e.g.%2C%20key%20clip%20retrieval%20and%0Asubtitle%20retrieval%29%20to%20support%20long-form%20video%20tasks.%20Then%2C%20we%20construct%20a%0Adiverse%20and%20high-quality%20fast-slow%20reasoning%20dataset%20with%20a%20strong%20LLM%20to%20align%0Aopen-source%20language%20models%27%20ability%20to%20generate%20visual%20program%20workflows%20as%0AFS-LLM.%20Next%2C%20we%20design%20a%20fast-slow%20reasoning%20framework%20with%20FS-LLM%3A%20Simple%0Aqueries%20are%20directly%20solved%20by%20VideoLLMs%2C%20while%20difficult%20ones%20invoke%20visual%0Aprogram%20reasoning%2C%20motivated%20by%20human-like%20reasoning%20processes.%20During%20this%0Aprocess%2C%20low-confidence%20fast-thinking%20answers%20will%20trigger%20a%20second-stage%0Aslow-reasoning%20process%2C%20and%20a%20fallback%20mechanism%20to%20fast%20reasoning%20is%20activated%0Aif%20the%20program%20execution%20fails.%20Moreover%2C%20we%20improve%20visual%20programs%20through%0Aparameter%20search%20during%20both%20training%20and%20inference.%20By%20adjusting%20the%0Aparameters%20of%20the%20visual%20modules%20within%20the%20program%2C%20multiple%20variants%20are%0Agenerated%3A%20during%20training%2C%20programs%20that%20yield%20correct%20answers%20are%20selected%2C%0Awhile%20during%20inference%2C%20the%20program%20with%20the%20highest%20confidence%20result%20is%0Aapplied.%20Experiments%20show%20that%20FS-VisPR%20improves%20both%20efficiency%20and%0Areliability%20in%20visual%20program%20workflows.%20It%20achieves%2050.4%25%20accuracy%20on%20LVBench%2C%0Asurpassing%20GPT-4o%2C%20matching%20the%20performance%20of%20Qwen2.5VL-72B%20on%20VideoMME.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17743v1&entry.124074799=Read"},
{"title": "NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and\n  Neuro-Symbolic Reasoning", "author": "Sahil Shah and S P Sharan and Harsh Goel and Minkyu Choi and Mustafa Munir and Manvik Pasula and Radu Marculescu and Sandeep Chinchali", "abstract": "  Long-Form Video Question Answering (LVQA) poses challenges beyond traditional\nvisual question answering (VQA), which is often limited to static images or\nshort video clips. While current vision-language models (VLMs) perform well in\nthose settings, they struggle with complex queries in LVQA over long videos\ninvolving multi-step temporal reasoning and causality. Vanilla approaches,\nwhich sample frames uniformly and feed them to a VLM with the question, incur\nsignificant token overhead, forcing severe downsampling. As a result, the model\noften misses fine-grained visual structure, subtle event transitions, or key\ntemporal cues, ultimately leading to incorrect answers. To address these\nlimitations, recent works have explored query-adaptive frame sampling,\nhierarchical keyframe selection, and agent-based iterative querying. However,\nthese methods remain fundamentally heuristic: they lack explicit temporal\nrepresentations and cannot enforce or verify logical event relationships. As a\nresult, there are no formal guarantees that the sampled context actually\nencodes the compositional or causal logic demanded by the question. To address\nthese foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play\nneuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language\nquestion into a formal temporal logic expression, constructs a video automaton\nfrom frame-level semantic propositions, and applies model checking to\nrigorously identify video segments satisfying the question's logical\nrequirements. Only these logic-verified segments are submitted to the VLM, thus\nimproving interpretability, reducing hallucinations, and enabling compositional\nreasoning without modifying or fine-tuning the model. Experiments on\nLongVideoBench and CinePile show NeuS-QA improves performance by over 10%,\nespecially on questions involving event ordering, causality, and multi-step\ncompositional reasoning.\n", "link": "http://arxiv.org/abs/2509.18041v1", "date": "2025-09-22", "relevancy": 2.7766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5724}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuS-QA%3A%20Grounding%20Long-Form%20Video%20Understanding%20in%20Temporal%20Logic%20and%0A%20%20Neuro-Symbolic%20Reasoning&body=Title%3A%20NeuS-QA%3A%20Grounding%20Long-Form%20Video%20Understanding%20in%20Temporal%20Logic%20and%0A%20%20Neuro-Symbolic%20Reasoning%0AAuthor%3A%20Sahil%20Shah%20and%20S%20P%20Sharan%20and%20Harsh%20Goel%20and%20Minkyu%20Choi%20and%20Mustafa%20Munir%20and%20Manvik%20Pasula%20and%20Radu%20Marculescu%20and%20Sandeep%20Chinchali%0AAbstract%3A%20%20%20Long-Form%20Video%20Question%20Answering%20%28LVQA%29%20poses%20challenges%20beyond%20traditional%0Avisual%20question%20answering%20%28VQA%29%2C%20which%20is%20often%20limited%20to%20static%20images%20or%0Ashort%20video%20clips.%20While%20current%20vision-language%20models%20%28VLMs%29%20perform%20well%20in%0Athose%20settings%2C%20they%20struggle%20with%20complex%20queries%20in%20LVQA%20over%20long%20videos%0Ainvolving%20multi-step%20temporal%20reasoning%20and%20causality.%20Vanilla%20approaches%2C%0Awhich%20sample%20frames%20uniformly%20and%20feed%20them%20to%20a%20VLM%20with%20the%20question%2C%20incur%0Asignificant%20token%20overhead%2C%20forcing%20severe%20downsampling.%20As%20a%20result%2C%20the%20model%0Aoften%20misses%20fine-grained%20visual%20structure%2C%20subtle%20event%20transitions%2C%20or%20key%0Atemporal%20cues%2C%20ultimately%20leading%20to%20incorrect%20answers.%20To%20address%20these%0Alimitations%2C%20recent%20works%20have%20explored%20query-adaptive%20frame%20sampling%2C%0Ahierarchical%20keyframe%20selection%2C%20and%20agent-based%20iterative%20querying.%20However%2C%0Athese%20methods%20remain%20fundamentally%20heuristic%3A%20they%20lack%20explicit%20temporal%0Arepresentations%20and%20cannot%20enforce%20or%20verify%20logical%20event%20relationships.%20As%20a%0Aresult%2C%20there%20are%20no%20formal%20guarantees%20that%20the%20sampled%20context%20actually%0Aencodes%20the%20compositional%20or%20causal%20logic%20demanded%20by%20the%20question.%20To%20address%0Athese%20foundational%20gaps%2C%20we%20introduce%20NeuS-QA%2C%20a%20training-free%2C%20plug-and-play%0Aneuro-symbolic%20pipeline%20for%20LVQA.%20NeuS-QA%20translates%20a%20natural%20language%0Aquestion%20into%20a%20formal%20temporal%20logic%20expression%2C%20constructs%20a%20video%20automaton%0Afrom%20frame-level%20semantic%20propositions%2C%20and%20applies%20model%20checking%20to%0Arigorously%20identify%20video%20segments%20satisfying%20the%20question%27s%20logical%0Arequirements.%20Only%20these%20logic-verified%20segments%20are%20submitted%20to%20the%20VLM%2C%20thus%0Aimproving%20interpretability%2C%20reducing%20hallucinations%2C%20and%20enabling%20compositional%0Areasoning%20without%20modifying%20or%20fine-tuning%20the%20model.%20Experiments%20on%0ALongVideoBench%20and%20CinePile%20show%20NeuS-QA%20improves%20performance%20by%20over%2010%25%2C%0Aespecially%20on%20questions%20involving%20event%20ordering%2C%20causality%2C%20and%20multi-step%0Acompositional%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuS-QA%253A%2520Grounding%2520Long-Form%2520Video%2520Understanding%2520in%2520Temporal%2520Logic%2520and%250A%2520%2520Neuro-Symbolic%2520Reasoning%26entry.906535625%3DSahil%2520Shah%2520and%2520S%2520P%2520Sharan%2520and%2520Harsh%2520Goel%2520and%2520Minkyu%2520Choi%2520and%2520Mustafa%2520Munir%2520and%2520Manvik%2520Pasula%2520and%2520Radu%2520Marculescu%2520and%2520Sandeep%2520Chinchali%26entry.1292438233%3D%2520%2520Long-Form%2520Video%2520Question%2520Answering%2520%2528LVQA%2529%2520poses%2520challenges%2520beyond%2520traditional%250Avisual%2520question%2520answering%2520%2528VQA%2529%252C%2520which%2520is%2520often%2520limited%2520to%2520static%2520images%2520or%250Ashort%2520video%2520clips.%2520While%2520current%2520vision-language%2520models%2520%2528VLMs%2529%2520perform%2520well%2520in%250Athose%2520settings%252C%2520they%2520struggle%2520with%2520complex%2520queries%2520in%2520LVQA%2520over%2520long%2520videos%250Ainvolving%2520multi-step%2520temporal%2520reasoning%2520and%2520causality.%2520Vanilla%2520approaches%252C%250Awhich%2520sample%2520frames%2520uniformly%2520and%2520feed%2520them%2520to%2520a%2520VLM%2520with%2520the%2520question%252C%2520incur%250Asignificant%2520token%2520overhead%252C%2520forcing%2520severe%2520downsampling.%2520As%2520a%2520result%252C%2520the%2520model%250Aoften%2520misses%2520fine-grained%2520visual%2520structure%252C%2520subtle%2520event%2520transitions%252C%2520or%2520key%250Atemporal%2520cues%252C%2520ultimately%2520leading%2520to%2520incorrect%2520answers.%2520To%2520address%2520these%250Alimitations%252C%2520recent%2520works%2520have%2520explored%2520query-adaptive%2520frame%2520sampling%252C%250Ahierarchical%2520keyframe%2520selection%252C%2520and%2520agent-based%2520iterative%2520querying.%2520However%252C%250Athese%2520methods%2520remain%2520fundamentally%2520heuristic%253A%2520they%2520lack%2520explicit%2520temporal%250Arepresentations%2520and%2520cannot%2520enforce%2520or%2520verify%2520logical%2520event%2520relationships.%2520As%2520a%250Aresult%252C%2520there%2520are%2520no%2520formal%2520guarantees%2520that%2520the%2520sampled%2520context%2520actually%250Aencodes%2520the%2520compositional%2520or%2520causal%2520logic%2520demanded%2520by%2520the%2520question.%2520To%2520address%250Athese%2520foundational%2520gaps%252C%2520we%2520introduce%2520NeuS-QA%252C%2520a%2520training-free%252C%2520plug-and-play%250Aneuro-symbolic%2520pipeline%2520for%2520LVQA.%2520NeuS-QA%2520translates%2520a%2520natural%2520language%250Aquestion%2520into%2520a%2520formal%2520temporal%2520logic%2520expression%252C%2520constructs%2520a%2520video%2520automaton%250Afrom%2520frame-level%2520semantic%2520propositions%252C%2520and%2520applies%2520model%2520checking%2520to%250Arigorously%2520identify%2520video%2520segments%2520satisfying%2520the%2520question%2527s%2520logical%250Arequirements.%2520Only%2520these%2520logic-verified%2520segments%2520are%2520submitted%2520to%2520the%2520VLM%252C%2520thus%250Aimproving%2520interpretability%252C%2520reducing%2520hallucinations%252C%2520and%2520enabling%2520compositional%250Areasoning%2520without%2520modifying%2520or%2520fine-tuning%2520the%2520model.%2520Experiments%2520on%250ALongVideoBench%2520and%2520CinePile%2520show%2520NeuS-QA%2520improves%2520performance%2520by%2520over%252010%2525%252C%250Aespecially%2520on%2520questions%2520involving%2520event%2520ordering%252C%2520causality%252C%2520and%2520multi-step%250Acompositional%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuS-QA%3A%20Grounding%20Long-Form%20Video%20Understanding%20in%20Temporal%20Logic%20and%0A%20%20Neuro-Symbolic%20Reasoning&entry.906535625=Sahil%20Shah%20and%20S%20P%20Sharan%20and%20Harsh%20Goel%20and%20Minkyu%20Choi%20and%20Mustafa%20Munir%20and%20Manvik%20Pasula%20and%20Radu%20Marculescu%20and%20Sandeep%20Chinchali&entry.1292438233=%20%20Long-Form%20Video%20Question%20Answering%20%28LVQA%29%20poses%20challenges%20beyond%20traditional%0Avisual%20question%20answering%20%28VQA%29%2C%20which%20is%20often%20limited%20to%20static%20images%20or%0Ashort%20video%20clips.%20While%20current%20vision-language%20models%20%28VLMs%29%20perform%20well%20in%0Athose%20settings%2C%20they%20struggle%20with%20complex%20queries%20in%20LVQA%20over%20long%20videos%0Ainvolving%20multi-step%20temporal%20reasoning%20and%20causality.%20Vanilla%20approaches%2C%0Awhich%20sample%20frames%20uniformly%20and%20feed%20them%20to%20a%20VLM%20with%20the%20question%2C%20incur%0Asignificant%20token%20overhead%2C%20forcing%20severe%20downsampling.%20As%20a%20result%2C%20the%20model%0Aoften%20misses%20fine-grained%20visual%20structure%2C%20subtle%20event%20transitions%2C%20or%20key%0Atemporal%20cues%2C%20ultimately%20leading%20to%20incorrect%20answers.%20To%20address%20these%0Alimitations%2C%20recent%20works%20have%20explored%20query-adaptive%20frame%20sampling%2C%0Ahierarchical%20keyframe%20selection%2C%20and%20agent-based%20iterative%20querying.%20However%2C%0Athese%20methods%20remain%20fundamentally%20heuristic%3A%20they%20lack%20explicit%20temporal%0Arepresentations%20and%20cannot%20enforce%20or%20verify%20logical%20event%20relationships.%20As%20a%0Aresult%2C%20there%20are%20no%20formal%20guarantees%20that%20the%20sampled%20context%20actually%0Aencodes%20the%20compositional%20or%20causal%20logic%20demanded%20by%20the%20question.%20To%20address%0Athese%20foundational%20gaps%2C%20we%20introduce%20NeuS-QA%2C%20a%20training-free%2C%20plug-and-play%0Aneuro-symbolic%20pipeline%20for%20LVQA.%20NeuS-QA%20translates%20a%20natural%20language%0Aquestion%20into%20a%20formal%20temporal%20logic%20expression%2C%20constructs%20a%20video%20automaton%0Afrom%20frame-level%20semantic%20propositions%2C%20and%20applies%20model%20checking%20to%0Arigorously%20identify%20video%20segments%20satisfying%20the%20question%27s%20logical%0Arequirements.%20Only%20these%20logic-verified%20segments%20are%20submitted%20to%20the%20VLM%2C%20thus%0Aimproving%20interpretability%2C%20reducing%20hallucinations%2C%20and%20enabling%20compositional%0Areasoning%20without%20modifying%20or%20fine-tuning%20the%20model.%20Experiments%20on%0ALongVideoBench%20and%20CinePile%20show%20NeuS-QA%20improves%20performance%20by%20over%2010%25%2C%0Aespecially%20on%20questions%20involving%20event%20ordering%2C%20causality%2C%20and%20multi-step%0Acompositional%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18041v1&entry.124074799=Read"},
{"title": "Enhancing Semantic Segmentation with Continual Self-Supervised\n  Pre-training", "author": "Brown Ebouky and Ajad Chhatkuli and Cristiano Malossi and Christoph Studer and Roy Assaf and Andrea Bartezzaghi", "abstract": "  Self-supervised learning (SSL) has emerged as a central paradigm for training\nfoundation models by leveraging large-scale unlabeled datasets, often producing\nrepresentations with strong generalization capabilities. These models are\ntypically pre-trained on general-purpose datasets such as ImageNet and\nsubsequently adapted to various downstream tasks through finetuning. While\nrecent advances have explored parameter-efficient strategies for adapting\npre-trained models, extending SSL pre-training itself to new domains -\nparticularly under limited data regimes and for dense prediction tasks -\nremains underexplored. In this work, we address the problem of adapting vision\nfoundation models to new domains in an unsupervised and data-efficient manner,\nspecifically targeting downstream semantic segmentation. We propose GLARE\n(Global Local and Regional Enforcement), a novel continual self-supervised\npre-training task designed to enhance downstream segmentation performance.\nGLARE introduces patch-level augmentations to encourage local consistency and\nincorporates a regional consistency constraint that leverages spatial semantics\nin the data. For efficient continual pre-training, we initialize Vision\nTransformers (ViTs) with weights from existing SSL models and update only\nlightweight adapter modules - specifically UniAdapter - while keeping the rest\nof the backbone frozen. Experiments across multiple semantic segmentation\nbenchmarks on different domains demonstrate that GLARE consistently improves\ndownstream performance with minimal computational and parameter overhead.\n", "link": "http://arxiv.org/abs/2509.17816v1", "date": "2025-09-22", "relevancy": 2.77, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Semantic%20Segmentation%20with%20Continual%20Self-Supervised%0A%20%20Pre-training&body=Title%3A%20Enhancing%20Semantic%20Segmentation%20with%20Continual%20Self-Supervised%0A%20%20Pre-training%0AAuthor%3A%20Brown%20Ebouky%20and%20Ajad%20Chhatkuli%20and%20Cristiano%20Malossi%20and%20Christoph%20Studer%20and%20Roy%20Assaf%20and%20Andrea%20Bartezzaghi%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20central%20paradigm%20for%20training%0Afoundation%20models%20by%20leveraging%20large-scale%20unlabeled%20datasets%2C%20often%20producing%0Arepresentations%20with%20strong%20generalization%20capabilities.%20These%20models%20are%0Atypically%20pre-trained%20on%20general-purpose%20datasets%20such%20as%20ImageNet%20and%0Asubsequently%20adapted%20to%20various%20downstream%20tasks%20through%20finetuning.%20While%0Arecent%20advances%20have%20explored%20parameter-efficient%20strategies%20for%20adapting%0Apre-trained%20models%2C%20extending%20SSL%20pre-training%20itself%20to%20new%20domains%20-%0Aparticularly%20under%20limited%20data%20regimes%20and%20for%20dense%20prediction%20tasks%20-%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20adapting%20vision%0Afoundation%20models%20to%20new%20domains%20in%20an%20unsupervised%20and%20data-efficient%20manner%2C%0Aspecifically%20targeting%20downstream%20semantic%20segmentation.%20We%20propose%20GLARE%0A%28Global%20Local%20and%20Regional%20Enforcement%29%2C%20a%20novel%20continual%20self-supervised%0Apre-training%20task%20designed%20to%20enhance%20downstream%20segmentation%20performance.%0AGLARE%20introduces%20patch-level%20augmentations%20to%20encourage%20local%20consistency%20and%0Aincorporates%20a%20regional%20consistency%20constraint%20that%20leverages%20spatial%20semantics%0Ain%20the%20data.%20For%20efficient%20continual%20pre-training%2C%20we%20initialize%20Vision%0ATransformers%20%28ViTs%29%20with%20weights%20from%20existing%20SSL%20models%20and%20update%20only%0Alightweight%20adapter%20modules%20-%20specifically%20UniAdapter%20-%20while%20keeping%20the%20rest%0Aof%20the%20backbone%20frozen.%20Experiments%20across%20multiple%20semantic%20segmentation%0Abenchmarks%20on%20different%20domains%20demonstrate%20that%20GLARE%20consistently%20improves%0Adownstream%20performance%20with%20minimal%20computational%20and%20parameter%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Semantic%2520Segmentation%2520with%2520Continual%2520Self-Supervised%250A%2520%2520Pre-training%26entry.906535625%3DBrown%2520Ebouky%2520and%2520Ajad%2520Chhatkuli%2520and%2520Cristiano%2520Malossi%2520and%2520Christoph%2520Studer%2520and%2520Roy%2520Assaf%2520and%2520Andrea%2520Bartezzaghi%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520central%2520paradigm%2520for%2520training%250Afoundation%2520models%2520by%2520leveraging%2520large-scale%2520unlabeled%2520datasets%252C%2520often%2520producing%250Arepresentations%2520with%2520strong%2520generalization%2520capabilities.%2520These%2520models%2520are%250Atypically%2520pre-trained%2520on%2520general-purpose%2520datasets%2520such%2520as%2520ImageNet%2520and%250Asubsequently%2520adapted%2520to%2520various%2520downstream%2520tasks%2520through%2520finetuning.%2520While%250Arecent%2520advances%2520have%2520explored%2520parameter-efficient%2520strategies%2520for%2520adapting%250Apre-trained%2520models%252C%2520extending%2520SSL%2520pre-training%2520itself%2520to%2520new%2520domains%2520-%250Aparticularly%2520under%2520limited%2520data%2520regimes%2520and%2520for%2520dense%2520prediction%2520tasks%2520-%250Aremains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520problem%2520of%2520adapting%2520vision%250Afoundation%2520models%2520to%2520new%2520domains%2520in%2520an%2520unsupervised%2520and%2520data-efficient%2520manner%252C%250Aspecifically%2520targeting%2520downstream%2520semantic%2520segmentation.%2520We%2520propose%2520GLARE%250A%2528Global%2520Local%2520and%2520Regional%2520Enforcement%2529%252C%2520a%2520novel%2520continual%2520self-supervised%250Apre-training%2520task%2520designed%2520to%2520enhance%2520downstream%2520segmentation%2520performance.%250AGLARE%2520introduces%2520patch-level%2520augmentations%2520to%2520encourage%2520local%2520consistency%2520and%250Aincorporates%2520a%2520regional%2520consistency%2520constraint%2520that%2520leverages%2520spatial%2520semantics%250Ain%2520the%2520data.%2520For%2520efficient%2520continual%2520pre-training%252C%2520we%2520initialize%2520Vision%250ATransformers%2520%2528ViTs%2529%2520with%2520weights%2520from%2520existing%2520SSL%2520models%2520and%2520update%2520only%250Alightweight%2520adapter%2520modules%2520-%2520specifically%2520UniAdapter%2520-%2520while%2520keeping%2520the%2520rest%250Aof%2520the%2520backbone%2520frozen.%2520Experiments%2520across%2520multiple%2520semantic%2520segmentation%250Abenchmarks%2520on%2520different%2520domains%2520demonstrate%2520that%2520GLARE%2520consistently%2520improves%250Adownstream%2520performance%2520with%2520minimal%2520computational%2520and%2520parameter%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Semantic%20Segmentation%20with%20Continual%20Self-Supervised%0A%20%20Pre-training&entry.906535625=Brown%20Ebouky%20and%20Ajad%20Chhatkuli%20and%20Cristiano%20Malossi%20and%20Christoph%20Studer%20and%20Roy%20Assaf%20and%20Andrea%20Bartezzaghi&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20central%20paradigm%20for%20training%0Afoundation%20models%20by%20leveraging%20large-scale%20unlabeled%20datasets%2C%20often%20producing%0Arepresentations%20with%20strong%20generalization%20capabilities.%20These%20models%20are%0Atypically%20pre-trained%20on%20general-purpose%20datasets%20such%20as%20ImageNet%20and%0Asubsequently%20adapted%20to%20various%20downstream%20tasks%20through%20finetuning.%20While%0Arecent%20advances%20have%20explored%20parameter-efficient%20strategies%20for%20adapting%0Apre-trained%20models%2C%20extending%20SSL%20pre-training%20itself%20to%20new%20domains%20-%0Aparticularly%20under%20limited%20data%20regimes%20and%20for%20dense%20prediction%20tasks%20-%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20adapting%20vision%0Afoundation%20models%20to%20new%20domains%20in%20an%20unsupervised%20and%20data-efficient%20manner%2C%0Aspecifically%20targeting%20downstream%20semantic%20segmentation.%20We%20propose%20GLARE%0A%28Global%20Local%20and%20Regional%20Enforcement%29%2C%20a%20novel%20continual%20self-supervised%0Apre-training%20task%20designed%20to%20enhance%20downstream%20segmentation%20performance.%0AGLARE%20introduces%20patch-level%20augmentations%20to%20encourage%20local%20consistency%20and%0Aincorporates%20a%20regional%20consistency%20constraint%20that%20leverages%20spatial%20semantics%0Ain%20the%20data.%20For%20efficient%20continual%20pre-training%2C%20we%20initialize%20Vision%0ATransformers%20%28ViTs%29%20with%20weights%20from%20existing%20SSL%20models%20and%20update%20only%0Alightweight%20adapter%20modules%20-%20specifically%20UniAdapter%20-%20while%20keeping%20the%20rest%0Aof%20the%20backbone%20frozen.%20Experiments%20across%20multiple%20semantic%20segmentation%0Abenchmarks%20on%20different%20domains%20demonstrate%20that%20GLARE%20consistently%20improves%0Adownstream%20performance%20with%20minimal%20computational%20and%20parameter%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17816v1&entry.124074799=Read"},
{"title": "Qwen3-Omni Technical Report", "author": "Jin Xu and Zhifang Guo and Hangrui Hu and Yunfei Chu and Xiong Wang and Jinzheng He and Yuxuan Wang and Xian Shi and Ting He and Xinfa Zhu and Yuanjun Lv and Yongqi Wang and Dake Guo and He Wang and Linhan Ma and Pei Zhang and Xinyu Zhang and Hongkun Hao and Zishan Guo and Baosong Yang and Bin Zhang and Ziyang Ma and Xipin Wei and Shuai Bai and Keqin Chen and Xuejing Liu and Peng Wang and Mingkun Yang and Dayiheng Liu and Xingzhang Ren and Bo Zheng and Rui Men and Fan Zhou and Bowen Yu and Jianxin Yang and Le Yu and Jingren Zhou and Junyang Lin", "abstract": "  We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.\n", "link": "http://arxiv.org/abs/2509.17765v1", "date": "2025-09-22", "relevancy": 2.749, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5659}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5659}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qwen3-Omni%20Technical%20Report&body=Title%3A%20Qwen3-Omni%20Technical%20Report%0AAuthor%3A%20Jin%20Xu%20and%20Zhifang%20Guo%20and%20Hangrui%20Hu%20and%20Yunfei%20Chu%20and%20Xiong%20Wang%20and%20Jinzheng%20He%20and%20Yuxuan%20Wang%20and%20Xian%20Shi%20and%20Ting%20He%20and%20Xinfa%20Zhu%20and%20Yuanjun%20Lv%20and%20Yongqi%20Wang%20and%20Dake%20Guo%20and%20He%20Wang%20and%20Linhan%20Ma%20and%20Pei%20Zhang%20and%20Xinyu%20Zhang%20and%20Hongkun%20Hao%20and%20Zishan%20Guo%20and%20Baosong%20Yang%20and%20Bin%20Zhang%20and%20Ziyang%20Ma%20and%20Xipin%20Wei%20and%20Shuai%20Bai%20and%20Keqin%20Chen%20and%20Xuejing%20Liu%20and%20Peng%20Wang%20and%20Mingkun%20Yang%20and%20Dayiheng%20Liu%20and%20Xingzhang%20Ren%20and%20Bo%20Zheng%20and%20Rui%20Men%20and%20Fan%20Zhou%20and%20Bowen%20Yu%20and%20Jianxin%20Yang%20and%20Le%20Yu%20and%20Jingren%20Zhou%20and%20Junyang%20Lin%0AAbstract%3A%20%20%20We%20present%20Qwen3-Omni%2C%20a%20single%20multimodal%20model%20that%2C%20for%20the%20first%20time%2C%0Amaintains%20state-of-the-art%20performance%20across%20text%2C%20image%2C%20audio%2C%20and%20video%0Awithout%20any%20degradation%20relative%20to%20single-modal%20counterparts.%20Qwen3-Omni%0Amatches%20the%20performance%20of%20same-sized%20single-modal%20models%20within%20the%20Qwen%0Aseries%20and%20excels%20particularly%20on%20audio%20tasks.%20Across%2036%20audio%20and%20audio-visual%0Abenchmarks%2C%20Qwen3-Omni%20achieves%20open-source%20SOTA%20on%2032%20benchmarks%20and%20overall%0ASOTA%20on%2022%2C%20outperforming%20strong%20closed-source%20models%20such%20as%20Gemini-2.5-Pro%2C%0ASeed-ASR%2C%20and%20GPT-4o-Transcribe.%20Qwen3-Omni%20adopts%20a%20Thinker-Talker%20MoE%0Aarchitecture%20that%20unifies%20perception%20and%20generation%20across%20text%2C%20images%2C%20audio%2C%0Aand%20video%2C%20yielding%20fluent%20text%20and%20natural%20real-time%20speech.%20It%20supports%20text%0Ainteraction%20in%20119%20languages%2C%20speech%20understanding%20in%2019%20languages%2C%20and%20speech%0Ageneration%20in%2010%20languages.%20To%20reduce%20first-packet%20latency%20in%20streaming%0Asynthesis%2C%20Talker%20autoregressively%20predicts%20discrete%20speech%20codecs%20using%20a%0Amulti-codebook%20scheme.%20Leveraging%20the%20representational%20capacity%20of%20these%0Acodebooks%2C%20we%20replace%20computationally%20intensive%20block-wise%20diffusion%20with%20a%0Alightweight%20causal%20ConvNet%2C%20enabling%20streaming%20from%20the%20first%20codec%20frame.%20In%0Acold-start%20settings%2C%20Qwen3-Omni%20achieves%20a%20theoretical%20end-to-end%20first-packet%0Alatency%20of%20234%20ms.%20To%20further%20strengthen%20multimodal%20reasoning%2C%20we%20introduce%20a%0AThinking%20model%20that%20explicitly%20reasons%20over%20inputs%20from%20any%20modality.%20Since%20the%0Aresearch%20community%20currently%20lacks%20a%20general-purpose%20audio%20captioning%20model%2C%20we%0Afine-tuned%20Qwen3-Omni-30B-A3B%20to%20obtain%20Qwen3-Omni-30B-A3B-Captioner%2C%20which%0Aproduces%20detailed%2C%20low-hallucination%20captions%20for%20arbitrary%20audio%20inputs.%0AQwen3-Omni-30B-A3B%2C%20Qwen3-Omni-30B-A3B-Thinking%2C%20and%0AQwen3-Omni-30B-A3B-Captioner%20are%20publicly%20released%20under%20the%20Apache%202.0%0Alicense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQwen3-Omni%2520Technical%2520Report%26entry.906535625%3DJin%2520Xu%2520and%2520Zhifang%2520Guo%2520and%2520Hangrui%2520Hu%2520and%2520Yunfei%2520Chu%2520and%2520Xiong%2520Wang%2520and%2520Jinzheng%2520He%2520and%2520Yuxuan%2520Wang%2520and%2520Xian%2520Shi%2520and%2520Ting%2520He%2520and%2520Xinfa%2520Zhu%2520and%2520Yuanjun%2520Lv%2520and%2520Yongqi%2520Wang%2520and%2520Dake%2520Guo%2520and%2520He%2520Wang%2520and%2520Linhan%2520Ma%2520and%2520Pei%2520Zhang%2520and%2520Xinyu%2520Zhang%2520and%2520Hongkun%2520Hao%2520and%2520Zishan%2520Guo%2520and%2520Baosong%2520Yang%2520and%2520Bin%2520Zhang%2520and%2520Ziyang%2520Ma%2520and%2520Xipin%2520Wei%2520and%2520Shuai%2520Bai%2520and%2520Keqin%2520Chen%2520and%2520Xuejing%2520Liu%2520and%2520Peng%2520Wang%2520and%2520Mingkun%2520Yang%2520and%2520Dayiheng%2520Liu%2520and%2520Xingzhang%2520Ren%2520and%2520Bo%2520Zheng%2520and%2520Rui%2520Men%2520and%2520Fan%2520Zhou%2520and%2520Bowen%2520Yu%2520and%2520Jianxin%2520Yang%2520and%2520Le%2520Yu%2520and%2520Jingren%2520Zhou%2520and%2520Junyang%2520Lin%26entry.1292438233%3D%2520%2520We%2520present%2520Qwen3-Omni%252C%2520a%2520single%2520multimodal%2520model%2520that%252C%2520for%2520the%2520first%2520time%252C%250Amaintains%2520state-of-the-art%2520performance%2520across%2520text%252C%2520image%252C%2520audio%252C%2520and%2520video%250Awithout%2520any%2520degradation%2520relative%2520to%2520single-modal%2520counterparts.%2520Qwen3-Omni%250Amatches%2520the%2520performance%2520of%2520same-sized%2520single-modal%2520models%2520within%2520the%2520Qwen%250Aseries%2520and%2520excels%2520particularly%2520on%2520audio%2520tasks.%2520Across%252036%2520audio%2520and%2520audio-visual%250Abenchmarks%252C%2520Qwen3-Omni%2520achieves%2520open-source%2520SOTA%2520on%252032%2520benchmarks%2520and%2520overall%250ASOTA%2520on%252022%252C%2520outperforming%2520strong%2520closed-source%2520models%2520such%2520as%2520Gemini-2.5-Pro%252C%250ASeed-ASR%252C%2520and%2520GPT-4o-Transcribe.%2520Qwen3-Omni%2520adopts%2520a%2520Thinker-Talker%2520MoE%250Aarchitecture%2520that%2520unifies%2520perception%2520and%2520generation%2520across%2520text%252C%2520images%252C%2520audio%252C%250Aand%2520video%252C%2520yielding%2520fluent%2520text%2520and%2520natural%2520real-time%2520speech.%2520It%2520supports%2520text%250Ainteraction%2520in%2520119%2520languages%252C%2520speech%2520understanding%2520in%252019%2520languages%252C%2520and%2520speech%250Ageneration%2520in%252010%2520languages.%2520To%2520reduce%2520first-packet%2520latency%2520in%2520streaming%250Asynthesis%252C%2520Talker%2520autoregressively%2520predicts%2520discrete%2520speech%2520codecs%2520using%2520a%250Amulti-codebook%2520scheme.%2520Leveraging%2520the%2520representational%2520capacity%2520of%2520these%250Acodebooks%252C%2520we%2520replace%2520computationally%2520intensive%2520block-wise%2520diffusion%2520with%2520a%250Alightweight%2520causal%2520ConvNet%252C%2520enabling%2520streaming%2520from%2520the%2520first%2520codec%2520frame.%2520In%250Acold-start%2520settings%252C%2520Qwen3-Omni%2520achieves%2520a%2520theoretical%2520end-to-end%2520first-packet%250Alatency%2520of%2520234%2520ms.%2520To%2520further%2520strengthen%2520multimodal%2520reasoning%252C%2520we%2520introduce%2520a%250AThinking%2520model%2520that%2520explicitly%2520reasons%2520over%2520inputs%2520from%2520any%2520modality.%2520Since%2520the%250Aresearch%2520community%2520currently%2520lacks%2520a%2520general-purpose%2520audio%2520captioning%2520model%252C%2520we%250Afine-tuned%2520Qwen3-Omni-30B-A3B%2520to%2520obtain%2520Qwen3-Omni-30B-A3B-Captioner%252C%2520which%250Aproduces%2520detailed%252C%2520low-hallucination%2520captions%2520for%2520arbitrary%2520audio%2520inputs.%250AQwen3-Omni-30B-A3B%252C%2520Qwen3-Omni-30B-A3B-Thinking%252C%2520and%250AQwen3-Omni-30B-A3B-Captioner%2520are%2520publicly%2520released%2520under%2520the%2520Apache%25202.0%250Alicense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qwen3-Omni%20Technical%20Report&entry.906535625=Jin%20Xu%20and%20Zhifang%20Guo%20and%20Hangrui%20Hu%20and%20Yunfei%20Chu%20and%20Xiong%20Wang%20and%20Jinzheng%20He%20and%20Yuxuan%20Wang%20and%20Xian%20Shi%20and%20Ting%20He%20and%20Xinfa%20Zhu%20and%20Yuanjun%20Lv%20and%20Yongqi%20Wang%20and%20Dake%20Guo%20and%20He%20Wang%20and%20Linhan%20Ma%20and%20Pei%20Zhang%20and%20Xinyu%20Zhang%20and%20Hongkun%20Hao%20and%20Zishan%20Guo%20and%20Baosong%20Yang%20and%20Bin%20Zhang%20and%20Ziyang%20Ma%20and%20Xipin%20Wei%20and%20Shuai%20Bai%20and%20Keqin%20Chen%20and%20Xuejing%20Liu%20and%20Peng%20Wang%20and%20Mingkun%20Yang%20and%20Dayiheng%20Liu%20and%20Xingzhang%20Ren%20and%20Bo%20Zheng%20and%20Rui%20Men%20and%20Fan%20Zhou%20and%20Bowen%20Yu%20and%20Jianxin%20Yang%20and%20Le%20Yu%20and%20Jingren%20Zhou%20and%20Junyang%20Lin&entry.1292438233=%20%20We%20present%20Qwen3-Omni%2C%20a%20single%20multimodal%20model%20that%2C%20for%20the%20first%20time%2C%0Amaintains%20state-of-the-art%20performance%20across%20text%2C%20image%2C%20audio%2C%20and%20video%0Awithout%20any%20degradation%20relative%20to%20single-modal%20counterparts.%20Qwen3-Omni%0Amatches%20the%20performance%20of%20same-sized%20single-modal%20models%20within%20the%20Qwen%0Aseries%20and%20excels%20particularly%20on%20audio%20tasks.%20Across%2036%20audio%20and%20audio-visual%0Abenchmarks%2C%20Qwen3-Omni%20achieves%20open-source%20SOTA%20on%2032%20benchmarks%20and%20overall%0ASOTA%20on%2022%2C%20outperforming%20strong%20closed-source%20models%20such%20as%20Gemini-2.5-Pro%2C%0ASeed-ASR%2C%20and%20GPT-4o-Transcribe.%20Qwen3-Omni%20adopts%20a%20Thinker-Talker%20MoE%0Aarchitecture%20that%20unifies%20perception%20and%20generation%20across%20text%2C%20images%2C%20audio%2C%0Aand%20video%2C%20yielding%20fluent%20text%20and%20natural%20real-time%20speech.%20It%20supports%20text%0Ainteraction%20in%20119%20languages%2C%20speech%20understanding%20in%2019%20languages%2C%20and%20speech%0Ageneration%20in%2010%20languages.%20To%20reduce%20first-packet%20latency%20in%20streaming%0Asynthesis%2C%20Talker%20autoregressively%20predicts%20discrete%20speech%20codecs%20using%20a%0Amulti-codebook%20scheme.%20Leveraging%20the%20representational%20capacity%20of%20these%0Acodebooks%2C%20we%20replace%20computationally%20intensive%20block-wise%20diffusion%20with%20a%0Alightweight%20causal%20ConvNet%2C%20enabling%20streaming%20from%20the%20first%20codec%20frame.%20In%0Acold-start%20settings%2C%20Qwen3-Omni%20achieves%20a%20theoretical%20end-to-end%20first-packet%0Alatency%20of%20234%20ms.%20To%20further%20strengthen%20multimodal%20reasoning%2C%20we%20introduce%20a%0AThinking%20model%20that%20explicitly%20reasons%20over%20inputs%20from%20any%20modality.%20Since%20the%0Aresearch%20community%20currently%20lacks%20a%20general-purpose%20audio%20captioning%20model%2C%20we%0Afine-tuned%20Qwen3-Omni-30B-A3B%20to%20obtain%20Qwen3-Omni-30B-A3B-Captioner%2C%20which%0Aproduces%20detailed%2C%20low-hallucination%20captions%20for%20arbitrary%20audio%20inputs.%0AQwen3-Omni-30B-A3B%2C%20Qwen3-Omni-30B-A3B-Thinking%2C%20and%0AQwen3-Omni-30B-A3B-Captioner%20are%20publicly%20released%20under%20the%20Apache%202.0%0Alicense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17765v1&entry.124074799=Read"},
{"title": "MOSEv2: A More Challenging Dataset for Video Object Segmentation in\n  Complex Scenes", "author": "Henghui Ding and Kaining Ying and Chang Liu and Shuting He and Xudong Jiang and Yu-Gang Jiang and Philip H. S. Torr and Song Bai", "abstract": "  Video object segmentation (VOS) aims to segment specified target objects\nthroughout a video. Although state-of-the-art methods have achieved impressive\nperformance (e.g., 90+% J&F) on benchmarks such as DAVIS and YouTube-VOS, these\ndatasets primarily contain salient, dominant, and isolated objects, limiting\ntheir generalization to real-world scenarios. To bridge this gap, the coMplex\nvideo Object SEgmentation (MOSEv1) dataset was introduced to facilitate VOS\nresearch in complex scenes. Building on the foundations and insights of MOSEv1,\nwe present MOSEv2, a significantly more challenging dataset designed to further\nadvance VOS methods under real-world conditions. MOSEv2 consists of 5,024\nvideos and 701,976 high-quality masks for 10,074 objects across 200 categories.\nCompared to its predecessor, MOSEv2 introduces much greater scene complexity,\nincluding {more frequent object disappearance and reappearance, severe\nocclusions and crowding, smaller objects, as well as a range of new challenges\nsuch as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g.,\nnighttime, underwater), multi-shot sequences, camouflaged objects, non-physical\ntargets (e.g., shadows, reflections), and scenarios requiring external\nknowledge.} We benchmark 20 representative VOS methods under 5 different\nsettings and observe consistent performance drops on MOSEv2. For example, SAM2\ndrops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video\nobject tracking methods and observe similar declines, demonstrating that MOSEv2\nposes challenges across tasks. These results highlight that despite strong\nperformance on existing datasets, current VOS methods still fall short under\nreal-world complexities. Based on our analysis of the observed challenges, we\nfurther propose several practical tricks that enhance model performance. MOSEv2\nis publicly available at https://MOSE.video.\n", "link": "http://arxiv.org/abs/2508.05630v2", "date": "2025-09-22", "relevancy": 2.7447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOSEv2%3A%20A%20More%20Challenging%20Dataset%20for%20Video%20Object%20Segmentation%20in%0A%20%20Complex%20Scenes&body=Title%3A%20MOSEv2%3A%20A%20More%20Challenging%20Dataset%20for%20Video%20Object%20Segmentation%20in%0A%20%20Complex%20Scenes%0AAuthor%3A%20Henghui%20Ding%20and%20Kaining%20Ying%20and%20Chang%20Liu%20and%20Shuting%20He%20and%20Xudong%20Jiang%20and%20Yu-Gang%20Jiang%20and%20Philip%20H.%20S.%20Torr%20and%20Song%20Bai%0AAbstract%3A%20%20%20Video%20object%20segmentation%20%28VOS%29%20aims%20to%20segment%20specified%20target%20objects%0Athroughout%20a%20video.%20Although%20state-of-the-art%20methods%20have%20achieved%20impressive%0Aperformance%20%28e.g.%2C%2090%2B%25%20J%26F%29%20on%20benchmarks%20such%20as%20DAVIS%20and%20YouTube-VOS%2C%20these%0Adatasets%20primarily%20contain%20salient%2C%20dominant%2C%20and%20isolated%20objects%2C%20limiting%0Atheir%20generalization%20to%20real-world%20scenarios.%20To%20bridge%20this%20gap%2C%20the%20coMplex%0Avideo%20Object%20SEgmentation%20%28MOSEv1%29%20dataset%20was%20introduced%20to%20facilitate%20VOS%0Aresearch%20in%20complex%20scenes.%20Building%20on%20the%20foundations%20and%20insights%20of%20MOSEv1%2C%0Awe%20present%20MOSEv2%2C%20a%20significantly%20more%20challenging%20dataset%20designed%20to%20further%0Aadvance%20VOS%20methods%20under%20real-world%20conditions.%20MOSEv2%20consists%20of%205%2C024%0Avideos%20and%20701%2C976%20high-quality%20masks%20for%2010%2C074%20objects%20across%20200%20categories.%0ACompared%20to%20its%20predecessor%2C%20MOSEv2%20introduces%20much%20greater%20scene%20complexity%2C%0Aincluding%20%7Bmore%20frequent%20object%20disappearance%20and%20reappearance%2C%20severe%0Aocclusions%20and%20crowding%2C%20smaller%20objects%2C%20as%20well%20as%20a%20range%20of%20new%20challenges%0Asuch%20as%20adverse%20weather%20%28e.g.%2C%20rain%2C%20snow%2C%20fog%29%2C%20low-light%20scenes%20%28e.g.%2C%0Anighttime%2C%20underwater%29%2C%20multi-shot%20sequences%2C%20camouflaged%20objects%2C%20non-physical%0Atargets%20%28e.g.%2C%20shadows%2C%20reflections%29%2C%20and%20scenarios%20requiring%20external%0Aknowledge.%7D%20We%20benchmark%2020%20representative%20VOS%20methods%20under%205%20different%0Asettings%20and%20observe%20consistent%20performance%20drops%20on%20MOSEv2.%20For%20example%2C%20SAM2%0Adrops%20from%2076.4%25%20on%20MOSEv1%20to%20only%2050.9%25%20on%20MOSEv2.%20We%20further%20evaluate%209%20video%0Aobject%20tracking%20methods%20and%20observe%20similar%20declines%2C%20demonstrating%20that%20MOSEv2%0Aposes%20challenges%20across%20tasks.%20These%20results%20highlight%20that%20despite%20strong%0Aperformance%20on%20existing%20datasets%2C%20current%20VOS%20methods%20still%20fall%20short%20under%0Areal-world%20complexities.%20Based%20on%20our%20analysis%20of%20the%20observed%20challenges%2C%20we%0Afurther%20propose%20several%20practical%20tricks%20that%20enhance%20model%20performance.%20MOSEv2%0Ais%20publicly%20available%20at%20https%3A//MOSE.video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOSEv2%253A%2520A%2520More%2520Challenging%2520Dataset%2520for%2520Video%2520Object%2520Segmentation%2520in%250A%2520%2520Complex%2520Scenes%26entry.906535625%3DHenghui%2520Ding%2520and%2520Kaining%2520Ying%2520and%2520Chang%2520Liu%2520and%2520Shuting%2520He%2520and%2520Xudong%2520Jiang%2520and%2520Yu-Gang%2520Jiang%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Song%2520Bai%26entry.1292438233%3D%2520%2520Video%2520object%2520segmentation%2520%2528VOS%2529%2520aims%2520to%2520segment%2520specified%2520target%2520objects%250Athroughout%2520a%2520video.%2520Although%2520state-of-the-art%2520methods%2520have%2520achieved%2520impressive%250Aperformance%2520%2528e.g.%252C%252090%252B%2525%2520J%2526F%2529%2520on%2520benchmarks%2520such%2520as%2520DAVIS%2520and%2520YouTube-VOS%252C%2520these%250Adatasets%2520primarily%2520contain%2520salient%252C%2520dominant%252C%2520and%2520isolated%2520objects%252C%2520limiting%250Atheir%2520generalization%2520to%2520real-world%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520the%2520coMplex%250Avideo%2520Object%2520SEgmentation%2520%2528MOSEv1%2529%2520dataset%2520was%2520introduced%2520to%2520facilitate%2520VOS%250Aresearch%2520in%2520complex%2520scenes.%2520Building%2520on%2520the%2520foundations%2520and%2520insights%2520of%2520MOSEv1%252C%250Awe%2520present%2520MOSEv2%252C%2520a%2520significantly%2520more%2520challenging%2520dataset%2520designed%2520to%2520further%250Aadvance%2520VOS%2520methods%2520under%2520real-world%2520conditions.%2520MOSEv2%2520consists%2520of%25205%252C024%250Avideos%2520and%2520701%252C976%2520high-quality%2520masks%2520for%252010%252C074%2520objects%2520across%2520200%2520categories.%250ACompared%2520to%2520its%2520predecessor%252C%2520MOSEv2%2520introduces%2520much%2520greater%2520scene%2520complexity%252C%250Aincluding%2520%257Bmore%2520frequent%2520object%2520disappearance%2520and%2520reappearance%252C%2520severe%250Aocclusions%2520and%2520crowding%252C%2520smaller%2520objects%252C%2520as%2520well%2520as%2520a%2520range%2520of%2520new%2520challenges%250Asuch%2520as%2520adverse%2520weather%2520%2528e.g.%252C%2520rain%252C%2520snow%252C%2520fog%2529%252C%2520low-light%2520scenes%2520%2528e.g.%252C%250Anighttime%252C%2520underwater%2529%252C%2520multi-shot%2520sequences%252C%2520camouflaged%2520objects%252C%2520non-physical%250Atargets%2520%2528e.g.%252C%2520shadows%252C%2520reflections%2529%252C%2520and%2520scenarios%2520requiring%2520external%250Aknowledge.%257D%2520We%2520benchmark%252020%2520representative%2520VOS%2520methods%2520under%25205%2520different%250Asettings%2520and%2520observe%2520consistent%2520performance%2520drops%2520on%2520MOSEv2.%2520For%2520example%252C%2520SAM2%250Adrops%2520from%252076.4%2525%2520on%2520MOSEv1%2520to%2520only%252050.9%2525%2520on%2520MOSEv2.%2520We%2520further%2520evaluate%25209%2520video%250Aobject%2520tracking%2520methods%2520and%2520observe%2520similar%2520declines%252C%2520demonstrating%2520that%2520MOSEv2%250Aposes%2520challenges%2520across%2520tasks.%2520These%2520results%2520highlight%2520that%2520despite%2520strong%250Aperformance%2520on%2520existing%2520datasets%252C%2520current%2520VOS%2520methods%2520still%2520fall%2520short%2520under%250Areal-world%2520complexities.%2520Based%2520on%2520our%2520analysis%2520of%2520the%2520observed%2520challenges%252C%2520we%250Afurther%2520propose%2520several%2520practical%2520tricks%2520that%2520enhance%2520model%2520performance.%2520MOSEv2%250Ais%2520publicly%2520available%2520at%2520https%253A//MOSE.video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOSEv2%3A%20A%20More%20Challenging%20Dataset%20for%20Video%20Object%20Segmentation%20in%0A%20%20Complex%20Scenes&entry.906535625=Henghui%20Ding%20and%20Kaining%20Ying%20and%20Chang%20Liu%20and%20Shuting%20He%20and%20Xudong%20Jiang%20and%20Yu-Gang%20Jiang%20and%20Philip%20H.%20S.%20Torr%20and%20Song%20Bai&entry.1292438233=%20%20Video%20object%20segmentation%20%28VOS%29%20aims%20to%20segment%20specified%20target%20objects%0Athroughout%20a%20video.%20Although%20state-of-the-art%20methods%20have%20achieved%20impressive%0Aperformance%20%28e.g.%2C%2090%2B%25%20J%26F%29%20on%20benchmarks%20such%20as%20DAVIS%20and%20YouTube-VOS%2C%20these%0Adatasets%20primarily%20contain%20salient%2C%20dominant%2C%20and%20isolated%20objects%2C%20limiting%0Atheir%20generalization%20to%20real-world%20scenarios.%20To%20bridge%20this%20gap%2C%20the%20coMplex%0Avideo%20Object%20SEgmentation%20%28MOSEv1%29%20dataset%20was%20introduced%20to%20facilitate%20VOS%0Aresearch%20in%20complex%20scenes.%20Building%20on%20the%20foundations%20and%20insights%20of%20MOSEv1%2C%0Awe%20present%20MOSEv2%2C%20a%20significantly%20more%20challenging%20dataset%20designed%20to%20further%0Aadvance%20VOS%20methods%20under%20real-world%20conditions.%20MOSEv2%20consists%20of%205%2C024%0Avideos%20and%20701%2C976%20high-quality%20masks%20for%2010%2C074%20objects%20across%20200%20categories.%0ACompared%20to%20its%20predecessor%2C%20MOSEv2%20introduces%20much%20greater%20scene%20complexity%2C%0Aincluding%20%7Bmore%20frequent%20object%20disappearance%20and%20reappearance%2C%20severe%0Aocclusions%20and%20crowding%2C%20smaller%20objects%2C%20as%20well%20as%20a%20range%20of%20new%20challenges%0Asuch%20as%20adverse%20weather%20%28e.g.%2C%20rain%2C%20snow%2C%20fog%29%2C%20low-light%20scenes%20%28e.g.%2C%0Anighttime%2C%20underwater%29%2C%20multi-shot%20sequences%2C%20camouflaged%20objects%2C%20non-physical%0Atargets%20%28e.g.%2C%20shadows%2C%20reflections%29%2C%20and%20scenarios%20requiring%20external%0Aknowledge.%7D%20We%20benchmark%2020%20representative%20VOS%20methods%20under%205%20different%0Asettings%20and%20observe%20consistent%20performance%20drops%20on%20MOSEv2.%20For%20example%2C%20SAM2%0Adrops%20from%2076.4%25%20on%20MOSEv1%20to%20only%2050.9%25%20on%20MOSEv2.%20We%20further%20evaluate%209%20video%0Aobject%20tracking%20methods%20and%20observe%20similar%20declines%2C%20demonstrating%20that%20MOSEv2%0Aposes%20challenges%20across%20tasks.%20These%20results%20highlight%20that%20despite%20strong%0Aperformance%20on%20existing%20datasets%2C%20current%20VOS%20methods%20still%20fall%20short%20under%0Areal-world%20complexities.%20Based%20on%20our%20analysis%20of%20the%20observed%20challenges%2C%20we%0Afurther%20propose%20several%20practical%20tricks%20that%20enhance%20model%20performance.%20MOSEv2%0Ais%20publicly%20available%20at%20https%3A//MOSE.video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05630v2&entry.124074799=Read"},
{"title": "Robust, Online, and Adaptive Decentralized Gaussian Processes", "author": "Fernando Llorente and Daniel Waxman and Sanket Jantre and Nathan M. Urban and Susan E. Minkoff", "abstract": "  Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for\nmodeling complex signals, but scale cubically with data, assume static targets,\nand are brittle to outliers, limiting their applicability in large-scale\nproblems with dynamic and noisy environments. Recent work introduced\ndecentralized random Fourier feature Gaussian processes (DRFGP), an online and\ndistributed algorithm that casts GPs in an information-filter form, enabling\nexact sequential inference and fully distributed computation without reliance\non a fusion center. In this paper, we extend DRFGP along two key directions:\nfirst, by introducing a robust-filtering update that downweights the impact of\natypical observations; and second, by incorporating a dynamic adaptation\nmechanism that adapts to time-varying functions. The resulting algorithm\nretains the recursive information-filter structure while enhancing stability\nand accuracy. We demonstrate its effectiveness on a large-scale Earth system\napplication, underscoring its potential for in-situ modeling.\n", "link": "http://arxiv.org/abs/2509.18011v1", "date": "2025-09-22", "relevancy": 2.7362, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.569}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5592}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%2C%20Online%2C%20and%20Adaptive%20Decentralized%20Gaussian%20Processes&body=Title%3A%20Robust%2C%20Online%2C%20and%20Adaptive%20Decentralized%20Gaussian%20Processes%0AAuthor%3A%20Fernando%20Llorente%20and%20Daniel%20Waxman%20and%20Sanket%20Jantre%20and%20Nathan%20M.%20Urban%20and%20Susan%20E.%20Minkoff%0AAbstract%3A%20%20%20Gaussian%20processes%20%28GPs%29%20offer%20a%20flexible%2C%20uncertainty-aware%20framework%20for%0Amodeling%20complex%20signals%2C%20but%20scale%20cubically%20with%20data%2C%20assume%20static%20targets%2C%0Aand%20are%20brittle%20to%20outliers%2C%20limiting%20their%20applicability%20in%20large-scale%0Aproblems%20with%20dynamic%20and%20noisy%20environments.%20Recent%20work%20introduced%0Adecentralized%20random%20Fourier%20feature%20Gaussian%20processes%20%28DRFGP%29%2C%20an%20online%20and%0Adistributed%20algorithm%20that%20casts%20GPs%20in%20an%20information-filter%20form%2C%20enabling%0Aexact%20sequential%20inference%20and%20fully%20distributed%20computation%20without%20reliance%0Aon%20a%20fusion%20center.%20In%20this%20paper%2C%20we%20extend%20DRFGP%20along%20two%20key%20directions%3A%0Afirst%2C%20by%20introducing%20a%20robust-filtering%20update%20that%20downweights%20the%20impact%20of%0Aatypical%20observations%3B%20and%20second%2C%20by%20incorporating%20a%20dynamic%20adaptation%0Amechanism%20that%20adapts%20to%20time-varying%20functions.%20The%20resulting%20algorithm%0Aretains%20the%20recursive%20information-filter%20structure%20while%20enhancing%20stability%0Aand%20accuracy.%20We%20demonstrate%20its%20effectiveness%20on%20a%20large-scale%20Earth%20system%0Aapplication%2C%20underscoring%20its%20potential%20for%20in-situ%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%252C%2520Online%252C%2520and%2520Adaptive%2520Decentralized%2520Gaussian%2520Processes%26entry.906535625%3DFernando%2520Llorente%2520and%2520Daniel%2520Waxman%2520and%2520Sanket%2520Jantre%2520and%2520Nathan%2520M.%2520Urban%2520and%2520Susan%2520E.%2520Minkoff%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520%2528GPs%2529%2520offer%2520a%2520flexible%252C%2520uncertainty-aware%2520framework%2520for%250Amodeling%2520complex%2520signals%252C%2520but%2520scale%2520cubically%2520with%2520data%252C%2520assume%2520static%2520targets%252C%250Aand%2520are%2520brittle%2520to%2520outliers%252C%2520limiting%2520their%2520applicability%2520in%2520large-scale%250Aproblems%2520with%2520dynamic%2520and%2520noisy%2520environments.%2520Recent%2520work%2520introduced%250Adecentralized%2520random%2520Fourier%2520feature%2520Gaussian%2520processes%2520%2528DRFGP%2529%252C%2520an%2520online%2520and%250Adistributed%2520algorithm%2520that%2520casts%2520GPs%2520in%2520an%2520information-filter%2520form%252C%2520enabling%250Aexact%2520sequential%2520inference%2520and%2520fully%2520distributed%2520computation%2520without%2520reliance%250Aon%2520a%2520fusion%2520center.%2520In%2520this%2520paper%252C%2520we%2520extend%2520DRFGP%2520along%2520two%2520key%2520directions%253A%250Afirst%252C%2520by%2520introducing%2520a%2520robust-filtering%2520update%2520that%2520downweights%2520the%2520impact%2520of%250Aatypical%2520observations%253B%2520and%2520second%252C%2520by%2520incorporating%2520a%2520dynamic%2520adaptation%250Amechanism%2520that%2520adapts%2520to%2520time-varying%2520functions.%2520The%2520resulting%2520algorithm%250Aretains%2520the%2520recursive%2520information-filter%2520structure%2520while%2520enhancing%2520stability%250Aand%2520accuracy.%2520We%2520demonstrate%2520its%2520effectiveness%2520on%2520a%2520large-scale%2520Earth%2520system%250Aapplication%252C%2520underscoring%2520its%2520potential%2520for%2520in-situ%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%2C%20Online%2C%20and%20Adaptive%20Decentralized%20Gaussian%20Processes&entry.906535625=Fernando%20Llorente%20and%20Daniel%20Waxman%20and%20Sanket%20Jantre%20and%20Nathan%20M.%20Urban%20and%20Susan%20E.%20Minkoff&entry.1292438233=%20%20Gaussian%20processes%20%28GPs%29%20offer%20a%20flexible%2C%20uncertainty-aware%20framework%20for%0Amodeling%20complex%20signals%2C%20but%20scale%20cubically%20with%20data%2C%20assume%20static%20targets%2C%0Aand%20are%20brittle%20to%20outliers%2C%20limiting%20their%20applicability%20in%20large-scale%0Aproblems%20with%20dynamic%20and%20noisy%20environments.%20Recent%20work%20introduced%0Adecentralized%20random%20Fourier%20feature%20Gaussian%20processes%20%28DRFGP%29%2C%20an%20online%20and%0Adistributed%20algorithm%20that%20casts%20GPs%20in%20an%20information-filter%20form%2C%20enabling%0Aexact%20sequential%20inference%20and%20fully%20distributed%20computation%20without%20reliance%0Aon%20a%20fusion%20center.%20In%20this%20paper%2C%20we%20extend%20DRFGP%20along%20two%20key%20directions%3A%0Afirst%2C%20by%20introducing%20a%20robust-filtering%20update%20that%20downweights%20the%20impact%20of%0Aatypical%20observations%3B%20and%20second%2C%20by%20incorporating%20a%20dynamic%20adaptation%0Amechanism%20that%20adapts%20to%20time-varying%20functions.%20The%20resulting%20algorithm%0Aretains%20the%20recursive%20information-filter%20structure%20while%20enhancing%20stability%0Aand%20accuracy.%20We%20demonstrate%20its%20effectiveness%20on%20a%20large-scale%20Earth%20system%0Aapplication%2C%20underscoring%20its%20potential%20for%20in-situ%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18011v1&entry.124074799=Read"},
{"title": "Automatic Intermodal Loading Unit Identification using Computer Vision:\n  A Scoping Review", "author": "Emre G\u00fclsoylu and Alhassan Abdelhalim and Derya Kara Boztas and Ole Grasse and Carlos Jahn and Simone Frintrop and Janick Edinger", "abstract": "  The standardisation of Intermodal Loading Units (ILUs), such as containers,\nsemi-trailers and swap bodies, has revolutionised global trade yet their\nefficient and robust identification remains a critical bottleneck in\nhigh-throughput ports and terminals. This paper reviews 63 empirical studies\nthat propose computer vision (CV) based solutions. It covers the last 35 years\n(1990-2025), tracing the field's evolution from early digital image processing\n(DIP) and traditional machine learning (ML) to the current dominance of deep\nlearning (DL) techniques. While CV offers cost-effective alternatives for other\ntypes of identification techniques, its development is hindered by the lack of\npublicly available benchmarking datasets. This results in high variance for the\nreported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond\ndataset limitations, this review highlights the emerging challenges especially\nintroduced by the shift from character-based text recognition to scene-text\nspotting and the integration of mobile cameras (e.g. drones, sensor equipped\nground vehicles) for dynamic terminal monitoring. To advance the field, the\npaper calls for standardised terminology, open-access datasets, shared source\ncode, while outlining future research directions such as contextless text\nrecognition optimised for ISO6346 codes.\n", "link": "http://arxiv.org/abs/2509.17707v1", "date": "2025-09-22", "relevancy": 2.731, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Intermodal%20Loading%20Unit%20Identification%20using%20Computer%20Vision%3A%0A%20%20A%20Scoping%20Review&body=Title%3A%20Automatic%20Intermodal%20Loading%20Unit%20Identification%20using%20Computer%20Vision%3A%0A%20%20A%20Scoping%20Review%0AAuthor%3A%20Emre%20G%C3%BClsoylu%20and%20Alhassan%20Abdelhalim%20and%20Derya%20Kara%20Boztas%20and%20Ole%20Grasse%20and%20Carlos%20Jahn%20and%20Simone%20Frintrop%20and%20Janick%20Edinger%0AAbstract%3A%20%20%20The%20standardisation%20of%20Intermodal%20Loading%20Units%20%28ILUs%29%2C%20such%20as%20containers%2C%0Asemi-trailers%20and%20swap%20bodies%2C%20has%20revolutionised%20global%20trade%20yet%20their%0Aefficient%20and%20robust%20identification%20remains%20a%20critical%20bottleneck%20in%0Ahigh-throughput%20ports%20and%20terminals.%20This%20paper%20reviews%2063%20empirical%20studies%0Athat%20propose%20computer%20vision%20%28CV%29%20based%20solutions.%20It%20covers%20the%20last%2035%20years%0A%281990-2025%29%2C%20tracing%20the%20field%27s%20evolution%20from%20early%20digital%20image%20processing%0A%28DIP%29%20and%20traditional%20machine%20learning%20%28ML%29%20to%20the%20current%20dominance%20of%20deep%0Alearning%20%28DL%29%20techniques.%20While%20CV%20offers%20cost-effective%20alternatives%20for%20other%0Atypes%20of%20identification%20techniques%2C%20its%20development%20is%20hindered%20by%20the%20lack%20of%0Apublicly%20available%20benchmarking%20datasets.%20This%20results%20in%20high%20variance%20for%20the%0Areported%20results%20such%20as%20end-to-end%20accuracy%20ranging%20from%205%20%25%20to%2096%20%25.%20Beyond%0Adataset%20limitations%2C%20this%20review%20highlights%20the%20emerging%20challenges%20especially%0Aintroduced%20by%20the%20shift%20from%20character-based%20text%20recognition%20to%20scene-text%0Aspotting%20and%20the%20integration%20of%20mobile%20cameras%20%28e.g.%20drones%2C%20sensor%20equipped%0Aground%20vehicles%29%20for%20dynamic%20terminal%20monitoring.%20To%20advance%20the%20field%2C%20the%0Apaper%20calls%20for%20standardised%20terminology%2C%20open-access%20datasets%2C%20shared%20source%0Acode%2C%20while%20outlining%20future%20research%20directions%20such%20as%20contextless%20text%0Arecognition%20optimised%20for%20ISO6346%20codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Intermodal%2520Loading%2520Unit%2520Identification%2520using%2520Computer%2520Vision%253A%250A%2520%2520A%2520Scoping%2520Review%26entry.906535625%3DEmre%2520G%25C3%25BClsoylu%2520and%2520Alhassan%2520Abdelhalim%2520and%2520Derya%2520Kara%2520Boztas%2520and%2520Ole%2520Grasse%2520and%2520Carlos%2520Jahn%2520and%2520Simone%2520Frintrop%2520and%2520Janick%2520Edinger%26entry.1292438233%3D%2520%2520The%2520standardisation%2520of%2520Intermodal%2520Loading%2520Units%2520%2528ILUs%2529%252C%2520such%2520as%2520containers%252C%250Asemi-trailers%2520and%2520swap%2520bodies%252C%2520has%2520revolutionised%2520global%2520trade%2520yet%2520their%250Aefficient%2520and%2520robust%2520identification%2520remains%2520a%2520critical%2520bottleneck%2520in%250Ahigh-throughput%2520ports%2520and%2520terminals.%2520This%2520paper%2520reviews%252063%2520empirical%2520studies%250Athat%2520propose%2520computer%2520vision%2520%2528CV%2529%2520based%2520solutions.%2520It%2520covers%2520the%2520last%252035%2520years%250A%25281990-2025%2529%252C%2520tracing%2520the%2520field%2527s%2520evolution%2520from%2520early%2520digital%2520image%2520processing%250A%2528DIP%2529%2520and%2520traditional%2520machine%2520learning%2520%2528ML%2529%2520to%2520the%2520current%2520dominance%2520of%2520deep%250Alearning%2520%2528DL%2529%2520techniques.%2520While%2520CV%2520offers%2520cost-effective%2520alternatives%2520for%2520other%250Atypes%2520of%2520identification%2520techniques%252C%2520its%2520development%2520is%2520hindered%2520by%2520the%2520lack%2520of%250Apublicly%2520available%2520benchmarking%2520datasets.%2520This%2520results%2520in%2520high%2520variance%2520for%2520the%250Areported%2520results%2520such%2520as%2520end-to-end%2520accuracy%2520ranging%2520from%25205%2520%2525%2520to%252096%2520%2525.%2520Beyond%250Adataset%2520limitations%252C%2520this%2520review%2520highlights%2520the%2520emerging%2520challenges%2520especially%250Aintroduced%2520by%2520the%2520shift%2520from%2520character-based%2520text%2520recognition%2520to%2520scene-text%250Aspotting%2520and%2520the%2520integration%2520of%2520mobile%2520cameras%2520%2528e.g.%2520drones%252C%2520sensor%2520equipped%250Aground%2520vehicles%2529%2520for%2520dynamic%2520terminal%2520monitoring.%2520To%2520advance%2520the%2520field%252C%2520the%250Apaper%2520calls%2520for%2520standardised%2520terminology%252C%2520open-access%2520datasets%252C%2520shared%2520source%250Acode%252C%2520while%2520outlining%2520future%2520research%2520directions%2520such%2520as%2520contextless%2520text%250Arecognition%2520optimised%2520for%2520ISO6346%2520codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Intermodal%20Loading%20Unit%20Identification%20using%20Computer%20Vision%3A%0A%20%20A%20Scoping%20Review&entry.906535625=Emre%20G%C3%BClsoylu%20and%20Alhassan%20Abdelhalim%20and%20Derya%20Kara%20Boztas%20and%20Ole%20Grasse%20and%20Carlos%20Jahn%20and%20Simone%20Frintrop%20and%20Janick%20Edinger&entry.1292438233=%20%20The%20standardisation%20of%20Intermodal%20Loading%20Units%20%28ILUs%29%2C%20such%20as%20containers%2C%0Asemi-trailers%20and%20swap%20bodies%2C%20has%20revolutionised%20global%20trade%20yet%20their%0Aefficient%20and%20robust%20identification%20remains%20a%20critical%20bottleneck%20in%0Ahigh-throughput%20ports%20and%20terminals.%20This%20paper%20reviews%2063%20empirical%20studies%0Athat%20propose%20computer%20vision%20%28CV%29%20based%20solutions.%20It%20covers%20the%20last%2035%20years%0A%281990-2025%29%2C%20tracing%20the%20field%27s%20evolution%20from%20early%20digital%20image%20processing%0A%28DIP%29%20and%20traditional%20machine%20learning%20%28ML%29%20to%20the%20current%20dominance%20of%20deep%0Alearning%20%28DL%29%20techniques.%20While%20CV%20offers%20cost-effective%20alternatives%20for%20other%0Atypes%20of%20identification%20techniques%2C%20its%20development%20is%20hindered%20by%20the%20lack%20of%0Apublicly%20available%20benchmarking%20datasets.%20This%20results%20in%20high%20variance%20for%20the%0Areported%20results%20such%20as%20end-to-end%20accuracy%20ranging%20from%205%20%25%20to%2096%20%25.%20Beyond%0Adataset%20limitations%2C%20this%20review%20highlights%20the%20emerging%20challenges%20especially%0Aintroduced%20by%20the%20shift%20from%20character-based%20text%20recognition%20to%20scene-text%0Aspotting%20and%20the%20integration%20of%20mobile%20cameras%20%28e.g.%20drones%2C%20sensor%20equipped%0Aground%20vehicles%29%20for%20dynamic%20terminal%20monitoring.%20To%20advance%20the%20field%2C%20the%0Apaper%20calls%20for%20standardised%20terminology%2C%20open-access%20datasets%2C%20shared%20source%0Acode%2C%20while%20outlining%20future%20research%20directions%20such%20as%20contextless%20text%0Arecognition%20optimised%20for%20ISO6346%20codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17707v1&entry.124074799=Read"},
{"title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal\n  LLMs in Image Classification", "author": "Yiwen Jiang and Deval Mehta and Siyuan Yan and Yaling Shen and Zimu Wang and Zongyuan Ge", "abstract": "  Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding.\n", "link": "http://arxiv.org/abs/2509.17740v1", "date": "2025-09-22", "relevancy": 2.7211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5378}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WISE%3A%20Weak-Supervision-Guided%20Step-by-Step%20Explanations%20for%20Multimodal%0A%20%20LLMs%20in%20Image%20Classification&body=Title%3A%20WISE%3A%20Weak-Supervision-Guided%20Step-by-Step%20Explanations%20for%20Multimodal%0A%20%20LLMs%20in%20Image%20Classification%0AAuthor%3A%20Yiwen%20Jiang%20and%20Deval%20Mehta%20and%20Siyuan%20Yan%20and%20Yaling%20Shen%20and%20Zimu%20Wang%20and%20Zongyuan%20Ge%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20promise%20in%20visual-textual%0Areasoning%2C%20with%20Multimodal%20Chain-of-Thought%20%28MCoT%29%20prompting%20significantly%0Aenhancing%20interpretability.%20However%2C%20existing%20MCoT%20methods%20rely%20on%0Arationale-rich%20datasets%20and%20largely%20focus%20on%20inter-object%20reasoning%2C%0Aoverlooking%20the%20intra-object%20understanding%20crucial%20for%20image%20classification.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20WISE%2C%20a%20Weak-supervision-guided%20Step-by-step%0AExplanation%20method%20that%20augments%20any%20image%20classification%20dataset%20with%20MCoTs%20by%0Areformulating%20the%20concept-based%20representations%20from%20Concept%20Bottleneck%20Models%0A%28CBMs%29%20into%20concise%2C%20interpretable%20reasoning%20chains%20under%20weak%20supervision.%0AExperiments%20across%20ten%20datasets%20show%20that%20our%20generated%20MCoTs%20not%20only%20improve%0Ainterpretability%20by%2037%25%20but%20also%20lead%20to%20gains%20in%20classification%20accuracy%20when%0Aused%20to%20fine-tune%20MLLMs.%20Our%20work%20bridges%20concept-based%20interpretability%20and%0Agenerative%20MCoT%20reasoning%2C%20providing%20a%20generalizable%20framework%20for%20enhancing%0AMLLMs%20in%20fine-grained%20visual%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWISE%253A%2520Weak-Supervision-Guided%2520Step-by-Step%2520Explanations%2520for%2520Multimodal%250A%2520%2520LLMs%2520in%2520Image%2520Classification%26entry.906535625%3DYiwen%2520Jiang%2520and%2520Deval%2520Mehta%2520and%2520Siyuan%2520Yan%2520and%2520Yaling%2520Shen%2520and%2520Zimu%2520Wang%2520and%2520Zongyuan%2520Ge%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520promise%2520in%2520visual-textual%250Areasoning%252C%2520with%2520Multimodal%2520Chain-of-Thought%2520%2528MCoT%2529%2520prompting%2520significantly%250Aenhancing%2520interpretability.%2520However%252C%2520existing%2520MCoT%2520methods%2520rely%2520on%250Arationale-rich%2520datasets%2520and%2520largely%2520focus%2520on%2520inter-object%2520reasoning%252C%250Aoverlooking%2520the%2520intra-object%2520understanding%2520crucial%2520for%2520image%2520classification.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520propose%2520WISE%252C%2520a%2520Weak-supervision-guided%2520Step-by-step%250AExplanation%2520method%2520that%2520augments%2520any%2520image%2520classification%2520dataset%2520with%2520MCoTs%2520by%250Areformulating%2520the%2520concept-based%2520representations%2520from%2520Concept%2520Bottleneck%2520Models%250A%2528CBMs%2529%2520into%2520concise%252C%2520interpretable%2520reasoning%2520chains%2520under%2520weak%2520supervision.%250AExperiments%2520across%2520ten%2520datasets%2520show%2520that%2520our%2520generated%2520MCoTs%2520not%2520only%2520improve%250Ainterpretability%2520by%252037%2525%2520but%2520also%2520lead%2520to%2520gains%2520in%2520classification%2520accuracy%2520when%250Aused%2520to%2520fine-tune%2520MLLMs.%2520Our%2520work%2520bridges%2520concept-based%2520interpretability%2520and%250Agenerative%2520MCoT%2520reasoning%252C%2520providing%2520a%2520generalizable%2520framework%2520for%2520enhancing%250AMLLMs%2520in%2520fine-grained%2520visual%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WISE%3A%20Weak-Supervision-Guided%20Step-by-Step%20Explanations%20for%20Multimodal%0A%20%20LLMs%20in%20Image%20Classification&entry.906535625=Yiwen%20Jiang%20and%20Deval%20Mehta%20and%20Siyuan%20Yan%20and%20Yaling%20Shen%20and%20Zimu%20Wang%20and%20Zongyuan%20Ge&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20promise%20in%20visual-textual%0Areasoning%2C%20with%20Multimodal%20Chain-of-Thought%20%28MCoT%29%20prompting%20significantly%0Aenhancing%20interpretability.%20However%2C%20existing%20MCoT%20methods%20rely%20on%0Arationale-rich%20datasets%20and%20largely%20focus%20on%20inter-object%20reasoning%2C%0Aoverlooking%20the%20intra-object%20understanding%20crucial%20for%20image%20classification.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20WISE%2C%20a%20Weak-supervision-guided%20Step-by-step%0AExplanation%20method%20that%20augments%20any%20image%20classification%20dataset%20with%20MCoTs%20by%0Areformulating%20the%20concept-based%20representations%20from%20Concept%20Bottleneck%20Models%0A%28CBMs%29%20into%20concise%2C%20interpretable%20reasoning%20chains%20under%20weak%20supervision.%0AExperiments%20across%20ten%20datasets%20show%20that%20our%20generated%20MCoTs%20not%20only%20improve%0Ainterpretability%20by%2037%25%20but%20also%20lead%20to%20gains%20in%20classification%20accuracy%20when%0Aused%20to%20fine-tune%20MLLMs.%20Our%20work%20bridges%20concept-based%20interpretability%20and%0Agenerative%20MCoT%20reasoning%2C%20providing%20a%20generalizable%20framework%20for%20enhancing%0AMLLMs%20in%20fine-grained%20visual%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17740v1&entry.124074799=Read"},
{"title": "Clothing agnostic Pre-inpainting Virtual Try-ON", "author": "Sehyun Kim and Hye Jun Lee and Jiwoo Lee and Taemin Lee", "abstract": "  With the development of deep learning technology, virtual try-on technology\nhas become an important application value in the fields of e-commerce, fashion,\nand entertainment. The recently proposed Leffa has improved the texture\ndistortion problem of diffu-sion-based models, but there are limitations in\nthat the bottom detection inaccuracy and the existing clothing silhouette\nremain in the synthesis results. To solve this problem, this study proposes\nCaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has\nimproved the naturalness and consistency of whole-body clothing syn-thesis by\nintegrating multi-category masking based on Dress Code and skin inpainting\nbased on Stable Diffusion. In particular, a generate skin module was introduced\nto solve the skin restoration problem that occurs when long-sleeved images are\nconverted into short-sleeved or sleeveless ones, and high-quality restoration\nwas implemented consider-ing the human body posture and color. As a result,\nCaP-VTON recorded 92.5\\%, which is 15.4\\% better than Leffa in short-sleeved\nsynthesis accuracy, and showed the performance of consistently reproducing the\nstyle and shape of reference clothing in visual evaluation. These structures\nmaintain model-agnostic properties and are applicable to various\ndiffu-sion-based virtual inspection systems, and can contribute to applications\nthat require high-precision virtual wearing, such as e-commerce, custom\nstyling, and avatar creation.\n", "link": "http://arxiv.org/abs/2509.17654v1", "date": "2025-09-22", "relevancy": 2.708, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.705}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6905}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clothing%20agnostic%20Pre-inpainting%20Virtual%20Try-ON&body=Title%3A%20Clothing%20agnostic%20Pre-inpainting%20Virtual%20Try-ON%0AAuthor%3A%20Sehyun%20Kim%20and%20Hye%20Jun%20Lee%20and%20Jiwoo%20Lee%20and%20Taemin%20Lee%0AAbstract%3A%20%20%20With%20the%20development%20of%20deep%20learning%20technology%2C%20virtual%20try-on%20technology%0Ahas%20become%20an%20important%20application%20value%20in%20the%20fields%20of%20e-commerce%2C%20fashion%2C%0Aand%20entertainment.%20The%20recently%20proposed%20Leffa%20has%20improved%20the%20texture%0Adistortion%20problem%20of%20diffu-sion-based%20models%2C%20but%20there%20are%20limitations%20in%0Athat%20the%20bottom%20detection%20inaccuracy%20and%20the%20existing%20clothing%20silhouette%0Aremain%20in%20the%20synthesis%20results.%20To%20solve%20this%20problem%2C%20this%20study%20proposes%0ACaP-VTON%20%28Clothing%20agnostic%20Pre-inpainting%20Virtual%20Try-ON%29.%20CaP-VTON%20has%0Aimproved%20the%20naturalness%20and%20consistency%20of%20whole-body%20clothing%20syn-thesis%20by%0Aintegrating%20multi-category%20masking%20based%20on%20Dress%20Code%20and%20skin%20inpainting%0Abased%20on%20Stable%20Diffusion.%20In%20particular%2C%20a%20generate%20skin%20module%20was%20introduced%0Ato%20solve%20the%20skin%20restoration%20problem%20that%20occurs%20when%20long-sleeved%20images%20are%0Aconverted%20into%20short-sleeved%20or%20sleeveless%20ones%2C%20and%20high-quality%20restoration%0Awas%20implemented%20consider-ing%20the%20human%20body%20posture%20and%20color.%20As%20a%20result%2C%0ACaP-VTON%20recorded%2092.5%5C%25%2C%20which%20is%2015.4%5C%25%20better%20than%20Leffa%20in%20short-sleeved%0Asynthesis%20accuracy%2C%20and%20showed%20the%20performance%20of%20consistently%20reproducing%20the%0Astyle%20and%20shape%20of%20reference%20clothing%20in%20visual%20evaluation.%20These%20structures%0Amaintain%20model-agnostic%20properties%20and%20are%20applicable%20to%20various%0Adiffu-sion-based%20virtual%20inspection%20systems%2C%20and%20can%20contribute%20to%20applications%0Athat%20require%20high-precision%20virtual%20wearing%2C%20such%20as%20e-commerce%2C%20custom%0Astyling%2C%20and%20avatar%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClothing%2520agnostic%2520Pre-inpainting%2520Virtual%2520Try-ON%26entry.906535625%3DSehyun%2520Kim%2520and%2520Hye%2520Jun%2520Lee%2520and%2520Jiwoo%2520Lee%2520and%2520Taemin%2520Lee%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520deep%2520learning%2520technology%252C%2520virtual%2520try-on%2520technology%250Ahas%2520become%2520an%2520important%2520application%2520value%2520in%2520the%2520fields%2520of%2520e-commerce%252C%2520fashion%252C%250Aand%2520entertainment.%2520The%2520recently%2520proposed%2520Leffa%2520has%2520improved%2520the%2520texture%250Adistortion%2520problem%2520of%2520diffu-sion-based%2520models%252C%2520but%2520there%2520are%2520limitations%2520in%250Athat%2520the%2520bottom%2520detection%2520inaccuracy%2520and%2520the%2520existing%2520clothing%2520silhouette%250Aremain%2520in%2520the%2520synthesis%2520results.%2520To%2520solve%2520this%2520problem%252C%2520this%2520study%2520proposes%250ACaP-VTON%2520%2528Clothing%2520agnostic%2520Pre-inpainting%2520Virtual%2520Try-ON%2529.%2520CaP-VTON%2520has%250Aimproved%2520the%2520naturalness%2520and%2520consistency%2520of%2520whole-body%2520clothing%2520syn-thesis%2520by%250Aintegrating%2520multi-category%2520masking%2520based%2520on%2520Dress%2520Code%2520and%2520skin%2520inpainting%250Abased%2520on%2520Stable%2520Diffusion.%2520In%2520particular%252C%2520a%2520generate%2520skin%2520module%2520was%2520introduced%250Ato%2520solve%2520the%2520skin%2520restoration%2520problem%2520that%2520occurs%2520when%2520long-sleeved%2520images%2520are%250Aconverted%2520into%2520short-sleeved%2520or%2520sleeveless%2520ones%252C%2520and%2520high-quality%2520restoration%250Awas%2520implemented%2520consider-ing%2520the%2520human%2520body%2520posture%2520and%2520color.%2520As%2520a%2520result%252C%250ACaP-VTON%2520recorded%252092.5%255C%2525%252C%2520which%2520is%252015.4%255C%2525%2520better%2520than%2520Leffa%2520in%2520short-sleeved%250Asynthesis%2520accuracy%252C%2520and%2520showed%2520the%2520performance%2520of%2520consistently%2520reproducing%2520the%250Astyle%2520and%2520shape%2520of%2520reference%2520clothing%2520in%2520visual%2520evaluation.%2520These%2520structures%250Amaintain%2520model-agnostic%2520properties%2520and%2520are%2520applicable%2520to%2520various%250Adiffu-sion-based%2520virtual%2520inspection%2520systems%252C%2520and%2520can%2520contribute%2520to%2520applications%250Athat%2520require%2520high-precision%2520virtual%2520wearing%252C%2520such%2520as%2520e-commerce%252C%2520custom%250Astyling%252C%2520and%2520avatar%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clothing%20agnostic%20Pre-inpainting%20Virtual%20Try-ON&entry.906535625=Sehyun%20Kim%20and%20Hye%20Jun%20Lee%20and%20Jiwoo%20Lee%20and%20Taemin%20Lee&entry.1292438233=%20%20With%20the%20development%20of%20deep%20learning%20technology%2C%20virtual%20try-on%20technology%0Ahas%20become%20an%20important%20application%20value%20in%20the%20fields%20of%20e-commerce%2C%20fashion%2C%0Aand%20entertainment.%20The%20recently%20proposed%20Leffa%20has%20improved%20the%20texture%0Adistortion%20problem%20of%20diffu-sion-based%20models%2C%20but%20there%20are%20limitations%20in%0Athat%20the%20bottom%20detection%20inaccuracy%20and%20the%20existing%20clothing%20silhouette%0Aremain%20in%20the%20synthesis%20results.%20To%20solve%20this%20problem%2C%20this%20study%20proposes%0ACaP-VTON%20%28Clothing%20agnostic%20Pre-inpainting%20Virtual%20Try-ON%29.%20CaP-VTON%20has%0Aimproved%20the%20naturalness%20and%20consistency%20of%20whole-body%20clothing%20syn-thesis%20by%0Aintegrating%20multi-category%20masking%20based%20on%20Dress%20Code%20and%20skin%20inpainting%0Abased%20on%20Stable%20Diffusion.%20In%20particular%2C%20a%20generate%20skin%20module%20was%20introduced%0Ato%20solve%20the%20skin%20restoration%20problem%20that%20occurs%20when%20long-sleeved%20images%20are%0Aconverted%20into%20short-sleeved%20or%20sleeveless%20ones%2C%20and%20high-quality%20restoration%0Awas%20implemented%20consider-ing%20the%20human%20body%20posture%20and%20color.%20As%20a%20result%2C%0ACaP-VTON%20recorded%2092.5%5C%25%2C%20which%20is%2015.4%5C%25%20better%20than%20Leffa%20in%20short-sleeved%0Asynthesis%20accuracy%2C%20and%20showed%20the%20performance%20of%20consistently%20reproducing%20the%0Astyle%20and%20shape%20of%20reference%20clothing%20in%20visual%20evaluation.%20These%20structures%0Amaintain%20model-agnostic%20properties%20and%20are%20applicable%20to%20various%0Adiffu-sion-based%20virtual%20inspection%20systems%2C%20and%20can%20contribute%20to%20applications%0Athat%20require%20high-precision%20virtual%20wearing%2C%20such%20as%20e-commerce%2C%20custom%0Astyling%2C%20and%20avatar%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17654v1&entry.124074799=Read"},
{"title": "DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph\n  Knowledge Distillation for Gait Disorders Recognition", "author": "Zakariae Zrimek and Youssef Mourchid and Mohammed El Hassouni", "abstract": "  Gait disorder recognition plays a crucial role in the early diagnosis and\nmonitoring of movement disorders. Existing approaches, including\nspatio-temporal graph convolutional networks (ST-GCNs), often face high memory\ndemands and struggle to capture complex spatio-temporal dependencies, limiting\ntheir efficiency in clinical applications. To address these challenges, we\nintroduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework\nthat combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The\nDF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts\nspatial connections between skeletal joints and temporal interactions across\ndifferent movement phases. This approach ensures better feature propagation\nthrough dynamic graph structures by considering the hierarchical nature and\ndynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba\nadapted for skeletal motion data, ensures a continuous propagation of states,\nfacilitating the capture of long-term dependencies while reducing computational\ncomplexity. To reduce the number of model parameters and computational costs\nwhile maintaining consistency, we propose Cross-Graph Relational Knowledge\nDistillation, a novel knowledge transfer mechanism that aligns relational\ninformation between teacher (large architecture) and student models (small\narchitecture) while using shared memory. This ensures that the interactions and\nmovement patterns of the joints are accurately preserved in the motion\nsequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA\ndatasets, where it outperforms state-of-the-art approaches by achieving in\nterms of Accuracy, F1-score, and Recall. Our results highlight the efficiency\nand robustness of our approach, offering a lightweight yet highly accurate\nsolution for automated gait analysis and movement disorder assessment.\n", "link": "http://arxiv.org/abs/2503.13156v2", "date": "2025-09-22", "relevancy": 2.6997, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5416}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5403}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynSTG-Mamba%3A%20Dynamic%20Spatio-Temporal%20Graph%20Mamba%20with%20Cross-Graph%0A%20%20Knowledge%20Distillation%20for%20Gait%20Disorders%20Recognition&body=Title%3A%20DynSTG-Mamba%3A%20Dynamic%20Spatio-Temporal%20Graph%20Mamba%20with%20Cross-Graph%0A%20%20Knowledge%20Distillation%20for%20Gait%20Disorders%20Recognition%0AAuthor%3A%20Zakariae%20Zrimek%20and%20Youssef%20Mourchid%20and%20Mohammed%20El%20Hassouni%0AAbstract%3A%20%20%20Gait%20disorder%20recognition%20plays%20a%20crucial%20role%20in%20the%20early%20diagnosis%20and%0Amonitoring%20of%20movement%20disorders.%20Existing%20approaches%2C%20including%0Aspatio-temporal%20graph%20convolutional%20networks%20%28ST-GCNs%29%2C%20often%20face%20high%20memory%0Ademands%20and%20struggle%20to%20capture%20complex%20spatio-temporal%20dependencies%2C%20limiting%0Atheir%20efficiency%20in%20clinical%20applications.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20DynSTG-Mamba%20%28Dynamic%20Spatio-Temporal%20Graph%20Mamba%29%2C%20a%20novel%20framework%0Athat%20combines%20DF-STGNN%20and%20STG-Mamba%20to%20enhance%20motion%20sequence%20modeling.%20The%0ADF-STGNN%20incorporates%20a%20dynamic%20spatio-temporal%20filter%20that%20adaptively%20adjusts%0Aspatial%20connections%20between%20skeletal%20joints%20and%20temporal%20interactions%20across%0Adifferent%20movement%20phases.%20This%20approach%20ensures%20better%20feature%20propagation%0Athrough%20dynamic%20graph%20structures%20by%20considering%20the%20hierarchical%20nature%20and%0Adynamics%20of%20skeletal%20gait%20data.%20Meanwhile%2C%20STG-Mamba%2C%20an%20extension%20of%20Mamba%0Aadapted%20for%20skeletal%20motion%20data%2C%20ensures%20a%20continuous%20propagation%20of%20states%2C%0Afacilitating%20the%20capture%20of%20long-term%20dependencies%20while%20reducing%20computational%0Acomplexity.%20To%20reduce%20the%20number%20of%20model%20parameters%20and%20computational%20costs%0Awhile%20maintaining%20consistency%2C%20we%20propose%20Cross-Graph%20Relational%20Knowledge%0ADistillation%2C%20a%20novel%20knowledge%20transfer%20mechanism%20that%20aligns%20relational%0Ainformation%20between%20teacher%20%28large%20architecture%29%20and%20student%20models%20%28small%0Aarchitecture%29%20while%20using%20shared%20memory.%20This%20ensures%20that%20the%20interactions%20and%0Amovement%20patterns%20of%20the%20joints%20are%20accurately%20preserved%20in%20the%20motion%0Asequences.%20We%20validate%20our%20DynSTG-Mamba%20on%20KOA-NM%2C%20PD-WALK%2C%20and%20ATAXIA%0Adatasets%2C%20where%20it%20outperforms%20state-of-the-art%20approaches%20by%20achieving%20in%0Aterms%20of%20Accuracy%2C%20F1-score%2C%20and%20Recall.%20Our%20results%20highlight%20the%20efficiency%0Aand%20robustness%20of%20our%20approach%2C%20offering%20a%20lightweight%20yet%20highly%20accurate%0Asolution%20for%20automated%20gait%20analysis%20and%20movement%20disorder%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13156v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynSTG-Mamba%253A%2520Dynamic%2520Spatio-Temporal%2520Graph%2520Mamba%2520with%2520Cross-Graph%250A%2520%2520Knowledge%2520Distillation%2520for%2520Gait%2520Disorders%2520Recognition%26entry.906535625%3DZakariae%2520Zrimek%2520and%2520Youssef%2520Mourchid%2520and%2520Mohammed%2520El%2520Hassouni%26entry.1292438233%3D%2520%2520Gait%2520disorder%2520recognition%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520early%2520diagnosis%2520and%250Amonitoring%2520of%2520movement%2520disorders.%2520Existing%2520approaches%252C%2520including%250Aspatio-temporal%2520graph%2520convolutional%2520networks%2520%2528ST-GCNs%2529%252C%2520often%2520face%2520high%2520memory%250Ademands%2520and%2520struggle%2520to%2520capture%2520complex%2520spatio-temporal%2520dependencies%252C%2520limiting%250Atheir%2520efficiency%2520in%2520clinical%2520applications.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520DynSTG-Mamba%2520%2528Dynamic%2520Spatio-Temporal%2520Graph%2520Mamba%2529%252C%2520a%2520novel%2520framework%250Athat%2520combines%2520DF-STGNN%2520and%2520STG-Mamba%2520to%2520enhance%2520motion%2520sequence%2520modeling.%2520The%250ADF-STGNN%2520incorporates%2520a%2520dynamic%2520spatio-temporal%2520filter%2520that%2520adaptively%2520adjusts%250Aspatial%2520connections%2520between%2520skeletal%2520joints%2520and%2520temporal%2520interactions%2520across%250Adifferent%2520movement%2520phases.%2520This%2520approach%2520ensures%2520better%2520feature%2520propagation%250Athrough%2520dynamic%2520graph%2520structures%2520by%2520considering%2520the%2520hierarchical%2520nature%2520and%250Adynamics%2520of%2520skeletal%2520gait%2520data.%2520Meanwhile%252C%2520STG-Mamba%252C%2520an%2520extension%2520of%2520Mamba%250Aadapted%2520for%2520skeletal%2520motion%2520data%252C%2520ensures%2520a%2520continuous%2520propagation%2520of%2520states%252C%250Afacilitating%2520the%2520capture%2520of%2520long-term%2520dependencies%2520while%2520reducing%2520computational%250Acomplexity.%2520To%2520reduce%2520the%2520number%2520of%2520model%2520parameters%2520and%2520computational%2520costs%250Awhile%2520maintaining%2520consistency%252C%2520we%2520propose%2520Cross-Graph%2520Relational%2520Knowledge%250ADistillation%252C%2520a%2520novel%2520knowledge%2520transfer%2520mechanism%2520that%2520aligns%2520relational%250Ainformation%2520between%2520teacher%2520%2528large%2520architecture%2529%2520and%2520student%2520models%2520%2528small%250Aarchitecture%2529%2520while%2520using%2520shared%2520memory.%2520This%2520ensures%2520that%2520the%2520interactions%2520and%250Amovement%2520patterns%2520of%2520the%2520joints%2520are%2520accurately%2520preserved%2520in%2520the%2520motion%250Asequences.%2520We%2520validate%2520our%2520DynSTG-Mamba%2520on%2520KOA-NM%252C%2520PD-WALK%252C%2520and%2520ATAXIA%250Adatasets%252C%2520where%2520it%2520outperforms%2520state-of-the-art%2520approaches%2520by%2520achieving%2520in%250Aterms%2520of%2520Accuracy%252C%2520F1-score%252C%2520and%2520Recall.%2520Our%2520results%2520highlight%2520the%2520efficiency%250Aand%2520robustness%2520of%2520our%2520approach%252C%2520offering%2520a%2520lightweight%2520yet%2520highly%2520accurate%250Asolution%2520for%2520automated%2520gait%2520analysis%2520and%2520movement%2520disorder%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13156v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynSTG-Mamba%3A%20Dynamic%20Spatio-Temporal%20Graph%20Mamba%20with%20Cross-Graph%0A%20%20Knowledge%20Distillation%20for%20Gait%20Disorders%20Recognition&entry.906535625=Zakariae%20Zrimek%20and%20Youssef%20Mourchid%20and%20Mohammed%20El%20Hassouni&entry.1292438233=%20%20Gait%20disorder%20recognition%20plays%20a%20crucial%20role%20in%20the%20early%20diagnosis%20and%0Amonitoring%20of%20movement%20disorders.%20Existing%20approaches%2C%20including%0Aspatio-temporal%20graph%20convolutional%20networks%20%28ST-GCNs%29%2C%20often%20face%20high%20memory%0Ademands%20and%20struggle%20to%20capture%20complex%20spatio-temporal%20dependencies%2C%20limiting%0Atheir%20efficiency%20in%20clinical%20applications.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20DynSTG-Mamba%20%28Dynamic%20Spatio-Temporal%20Graph%20Mamba%29%2C%20a%20novel%20framework%0Athat%20combines%20DF-STGNN%20and%20STG-Mamba%20to%20enhance%20motion%20sequence%20modeling.%20The%0ADF-STGNN%20incorporates%20a%20dynamic%20spatio-temporal%20filter%20that%20adaptively%20adjusts%0Aspatial%20connections%20between%20skeletal%20joints%20and%20temporal%20interactions%20across%0Adifferent%20movement%20phases.%20This%20approach%20ensures%20better%20feature%20propagation%0Athrough%20dynamic%20graph%20structures%20by%20considering%20the%20hierarchical%20nature%20and%0Adynamics%20of%20skeletal%20gait%20data.%20Meanwhile%2C%20STG-Mamba%2C%20an%20extension%20of%20Mamba%0Aadapted%20for%20skeletal%20motion%20data%2C%20ensures%20a%20continuous%20propagation%20of%20states%2C%0Afacilitating%20the%20capture%20of%20long-term%20dependencies%20while%20reducing%20computational%0Acomplexity.%20To%20reduce%20the%20number%20of%20model%20parameters%20and%20computational%20costs%0Awhile%20maintaining%20consistency%2C%20we%20propose%20Cross-Graph%20Relational%20Knowledge%0ADistillation%2C%20a%20novel%20knowledge%20transfer%20mechanism%20that%20aligns%20relational%0Ainformation%20between%20teacher%20%28large%20architecture%29%20and%20student%20models%20%28small%0Aarchitecture%29%20while%20using%20shared%20memory.%20This%20ensures%20that%20the%20interactions%20and%0Amovement%20patterns%20of%20the%20joints%20are%20accurately%20preserved%20in%20the%20motion%0Asequences.%20We%20validate%20our%20DynSTG-Mamba%20on%20KOA-NM%2C%20PD-WALK%2C%20and%20ATAXIA%0Adatasets%2C%20where%20it%20outperforms%20state-of-the-art%20approaches%20by%20achieving%20in%0Aterms%20of%20Accuracy%2C%20F1-score%2C%20and%20Recall.%20Our%20results%20highlight%20the%20efficiency%0Aand%20robustness%20of%20our%20approach%2C%20offering%20a%20lightweight%20yet%20highly%20accurate%0Asolution%20for%20automated%20gait%20analysis%20and%20movement%20disorder%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13156v2&entry.124074799=Read"},
{"title": "Improving Large Language Models Function Calling and Interpretability\n  via Guided-Structured Templates", "author": "Hy Dang and Tianyi Liu and Zhuofeng Wu and Jingfeng Yang and Haoming Jiang and Tao Yang and Pei Chen and Zhengyang Wang and Helen Wang and Huasheng Li and Bing Yin and Meng Jiang", "abstract": "  Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications.\n", "link": "http://arxiv.org/abs/2509.18076v1", "date": "2025-09-22", "relevancy": 2.6984, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5408}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Large%20Language%20Models%20Function%20Calling%20and%20Interpretability%0A%20%20via%20Guided-Structured%20Templates&body=Title%3A%20Improving%20Large%20Language%20Models%20Function%20Calling%20and%20Interpretability%0A%20%20via%20Guided-Structured%20Templates%0AAuthor%3A%20Hy%20Dang%20and%20Tianyi%20Liu%20and%20Zhuofeng%20Wu%20and%20Jingfeng%20Yang%20and%20Haoming%20Jiang%20and%20Tao%20Yang%20and%20Pei%20Chen%20and%20Zhengyang%20Wang%20and%20Helen%20Wang%20and%20Huasheng%20Li%20and%20Bing%20Yin%20and%20Meng%20Jiang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20reasoning%20and%20tool-use%0Acapabilities%2C%20yet%20they%20often%20fail%20in%20real-world%20tool-interactions%20due%20to%0Aincorrect%20parameterization%2C%20poor%20tool%20selection%2C%20or%20misinterpretation%20of%20user%0Aintent.%20These%20issues%20often%20stem%20from%20an%20incomplete%20understanding%20of%20user%20goals%0Aand%20inadequate%20comprehension%20of%20tool%20documentation.%20While%20Chain-of-Thought%0A%28CoT%29%20prompting%20has%20proven%20effective%20for%20enhancing%20reasoning%20in%20general%0Acontexts%2C%20our%20analysis%20reveals%20that%20free-form%20CoT%20is%20insufficient%20and%20sometimes%0Acounterproductive%20for%20structured%20function-calling%20tasks.%20To%20address%20this%2C%20we%0Aintroduce%20a%20curriculum-inspired%20framework%20that%20leverages%20structured%20reasoning%0Atemplates%20to%20guide%20LLMs%20through%20more%20deliberate%20step-by-step%20instructions%20for%0Agenerating%20function%20callings.%20Experimental%20results%20show%20that%20our%20method%20reduces%0Atool-use%20errors%2C%20achieving%203-12%25%20relative%20improvements%20over%20strong%20baselines%0Aacross%20diverse%20model%20series%20and%20approaches.%20Moreover%2C%20our%20framework%20enhances%0Athe%20robustness%2C%20interpretability%2C%20and%20transparency%20of%20tool-using%20agents%2C%0Aadvancing%20the%20development%20of%20more%20reliable%20AI%20assistants%20for%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Large%2520Language%2520Models%2520Function%2520Calling%2520and%2520Interpretability%250A%2520%2520via%2520Guided-Structured%2520Templates%26entry.906535625%3DHy%2520Dang%2520and%2520Tianyi%2520Liu%2520and%2520Zhuofeng%2520Wu%2520and%2520Jingfeng%2520Yang%2520and%2520Haoming%2520Jiang%2520and%2520Tao%2520Yang%2520and%2520Pei%2520Chen%2520and%2520Zhengyang%2520Wang%2520and%2520Helen%2520Wang%2520and%2520Huasheng%2520Li%2520and%2520Bing%2520Yin%2520and%2520Meng%2520Jiang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520reasoning%2520and%2520tool-use%250Acapabilities%252C%2520yet%2520they%2520often%2520fail%2520in%2520real-world%2520tool-interactions%2520due%2520to%250Aincorrect%2520parameterization%252C%2520poor%2520tool%2520selection%252C%2520or%2520misinterpretation%2520of%2520user%250Aintent.%2520These%2520issues%2520often%2520stem%2520from%2520an%2520incomplete%2520understanding%2520of%2520user%2520goals%250Aand%2520inadequate%2520comprehension%2520of%2520tool%2520documentation.%2520While%2520Chain-of-Thought%250A%2528CoT%2529%2520prompting%2520has%2520proven%2520effective%2520for%2520enhancing%2520reasoning%2520in%2520general%250Acontexts%252C%2520our%2520analysis%2520reveals%2520that%2520free-form%2520CoT%2520is%2520insufficient%2520and%2520sometimes%250Acounterproductive%2520for%2520structured%2520function-calling%2520tasks.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520curriculum-inspired%2520framework%2520that%2520leverages%2520structured%2520reasoning%250Atemplates%2520to%2520guide%2520LLMs%2520through%2520more%2520deliberate%2520step-by-step%2520instructions%2520for%250Agenerating%2520function%2520callings.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520reduces%250Atool-use%2520errors%252C%2520achieving%25203-12%2525%2520relative%2520improvements%2520over%2520strong%2520baselines%250Aacross%2520diverse%2520model%2520series%2520and%2520approaches.%2520Moreover%252C%2520our%2520framework%2520enhances%250Athe%2520robustness%252C%2520interpretability%252C%2520and%2520transparency%2520of%2520tool-using%2520agents%252C%250Aadvancing%2520the%2520development%2520of%2520more%2520reliable%2520AI%2520assistants%2520for%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Large%20Language%20Models%20Function%20Calling%20and%20Interpretability%0A%20%20via%20Guided-Structured%20Templates&entry.906535625=Hy%20Dang%20and%20Tianyi%20Liu%20and%20Zhuofeng%20Wu%20and%20Jingfeng%20Yang%20and%20Haoming%20Jiang%20and%20Tao%20Yang%20and%20Pei%20Chen%20and%20Zhengyang%20Wang%20and%20Helen%20Wang%20and%20Huasheng%20Li%20and%20Bing%20Yin%20and%20Meng%20Jiang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20reasoning%20and%20tool-use%0Acapabilities%2C%20yet%20they%20often%20fail%20in%20real-world%20tool-interactions%20due%20to%0Aincorrect%20parameterization%2C%20poor%20tool%20selection%2C%20or%20misinterpretation%20of%20user%0Aintent.%20These%20issues%20often%20stem%20from%20an%20incomplete%20understanding%20of%20user%20goals%0Aand%20inadequate%20comprehension%20of%20tool%20documentation.%20While%20Chain-of-Thought%0A%28CoT%29%20prompting%20has%20proven%20effective%20for%20enhancing%20reasoning%20in%20general%0Acontexts%2C%20our%20analysis%20reveals%20that%20free-form%20CoT%20is%20insufficient%20and%20sometimes%0Acounterproductive%20for%20structured%20function-calling%20tasks.%20To%20address%20this%2C%20we%0Aintroduce%20a%20curriculum-inspired%20framework%20that%20leverages%20structured%20reasoning%0Atemplates%20to%20guide%20LLMs%20through%20more%20deliberate%20step-by-step%20instructions%20for%0Agenerating%20function%20callings.%20Experimental%20results%20show%20that%20our%20method%20reduces%0Atool-use%20errors%2C%20achieving%203-12%25%20relative%20improvements%20over%20strong%20baselines%0Aacross%20diverse%20model%20series%20and%20approaches.%20Moreover%2C%20our%20framework%20enhances%0Athe%20robustness%2C%20interpretability%2C%20and%20transparency%20of%20tool-using%20agents%2C%0Aadvancing%20the%20development%20of%20more%20reliable%20AI%20assistants%20for%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18076v1&entry.124074799=Read"},
{"title": "Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware\n  Concept Bottleneck Models", "author": "Itay Benou and Tammy Riklin-Raviv", "abstract": "  Modern deep neural networks have now reached human-level performance across a\nvariety of tasks. However, unlike humans they lack the ability to explain their\ndecisions by showing where and telling what concepts guided them. In this work,\nwe present a unified framework for transforming any vision neural network into\na spatially and conceptually interpretable model. We introduce a\nspatially-aware concept bottleneck layer that projects \"black-box\" features of\npre-trained backbone models into interpretable concept maps, without requiring\nhuman labels. By training a classification layer over this bottleneck, we\nobtain a self-explaining model that articulates which concepts most influenced\nits prediction, along with heatmaps that ground them in the input image.\nAccordingly, we name this method \"Spatially-Aware and Label-Free Concept\nBottleneck Model\" (SALF-CBM). Our results show that the proposed SALF-CBM: (1)\nOutperforms non-spatial CBM methods, as well as the original backbone, on a\nvariety of classification tasks; (2) Produces high-quality spatial\nexplanations, outperforming widely used heatmap-based methods on a zero-shot\nsegmentation task; (3) Facilitates model exploration and debugging, enabling\nusers to query specific image regions and refine the model's decisions by\nlocally editing its concept maps.\n", "link": "http://arxiv.org/abs/2502.20134v4", "date": "2025-09-22", "relevancy": 2.6825, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Show%20and%20Tell%3A%20Visually%20Explainable%20Deep%20Neural%20Nets%20via%20Spatially-Aware%0A%20%20Concept%20Bottleneck%20Models&body=Title%3A%20Show%20and%20Tell%3A%20Visually%20Explainable%20Deep%20Neural%20Nets%20via%20Spatially-Aware%0A%20%20Concept%20Bottleneck%20Models%0AAuthor%3A%20Itay%20Benou%20and%20Tammy%20Riklin-Raviv%0AAbstract%3A%20%20%20Modern%20deep%20neural%20networks%20have%20now%20reached%20human-level%20performance%20across%20a%0Avariety%20of%20tasks.%20However%2C%20unlike%20humans%20they%20lack%20the%20ability%20to%20explain%20their%0Adecisions%20by%20showing%20where%20and%20telling%20what%20concepts%20guided%20them.%20In%20this%20work%2C%0Awe%20present%20a%20unified%20framework%20for%20transforming%20any%20vision%20neural%20network%20into%0Aa%20spatially%20and%20conceptually%20interpretable%20model.%20We%20introduce%20a%0Aspatially-aware%20concept%20bottleneck%20layer%20that%20projects%20%22black-box%22%20features%20of%0Apre-trained%20backbone%20models%20into%20interpretable%20concept%20maps%2C%20without%20requiring%0Ahuman%20labels.%20By%20training%20a%20classification%20layer%20over%20this%20bottleneck%2C%20we%0Aobtain%20a%20self-explaining%20model%20that%20articulates%20which%20concepts%20most%20influenced%0Aits%20prediction%2C%20along%20with%20heatmaps%20that%20ground%20them%20in%20the%20input%20image.%0AAccordingly%2C%20we%20name%20this%20method%20%22Spatially-Aware%20and%20Label-Free%20Concept%0ABottleneck%20Model%22%20%28SALF-CBM%29.%20Our%20results%20show%20that%20the%20proposed%20SALF-CBM%3A%20%281%29%0AOutperforms%20non-spatial%20CBM%20methods%2C%20as%20well%20as%20the%20original%20backbone%2C%20on%20a%0Avariety%20of%20classification%20tasks%3B%20%282%29%20Produces%20high-quality%20spatial%0Aexplanations%2C%20outperforming%20widely%20used%20heatmap-based%20methods%20on%20a%20zero-shot%0Asegmentation%20task%3B%20%283%29%20Facilitates%20model%20exploration%20and%20debugging%2C%20enabling%0Ausers%20to%20query%20specific%20image%20regions%20and%20refine%20the%20model%27s%20decisions%20by%0Alocally%20editing%20its%20concept%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20134v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShow%2520and%2520Tell%253A%2520Visually%2520Explainable%2520Deep%2520Neural%2520Nets%2520via%2520Spatially-Aware%250A%2520%2520Concept%2520Bottleneck%2520Models%26entry.906535625%3DItay%2520Benou%2520and%2520Tammy%2520Riklin-Raviv%26entry.1292438233%3D%2520%2520Modern%2520deep%2520neural%2520networks%2520have%2520now%2520reached%2520human-level%2520performance%2520across%2520a%250Avariety%2520of%2520tasks.%2520However%252C%2520unlike%2520humans%2520they%2520lack%2520the%2520ability%2520to%2520explain%2520their%250Adecisions%2520by%2520showing%2520where%2520and%2520telling%2520what%2520concepts%2520guided%2520them.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520unified%2520framework%2520for%2520transforming%2520any%2520vision%2520neural%2520network%2520into%250Aa%2520spatially%2520and%2520conceptually%2520interpretable%2520model.%2520We%2520introduce%2520a%250Aspatially-aware%2520concept%2520bottleneck%2520layer%2520that%2520projects%2520%2522black-box%2522%2520features%2520of%250Apre-trained%2520backbone%2520models%2520into%2520interpretable%2520concept%2520maps%252C%2520without%2520requiring%250Ahuman%2520labels.%2520By%2520training%2520a%2520classification%2520layer%2520over%2520this%2520bottleneck%252C%2520we%250Aobtain%2520a%2520self-explaining%2520model%2520that%2520articulates%2520which%2520concepts%2520most%2520influenced%250Aits%2520prediction%252C%2520along%2520with%2520heatmaps%2520that%2520ground%2520them%2520in%2520the%2520input%2520image.%250AAccordingly%252C%2520we%2520name%2520this%2520method%2520%2522Spatially-Aware%2520and%2520Label-Free%2520Concept%250ABottleneck%2520Model%2522%2520%2528SALF-CBM%2529.%2520Our%2520results%2520show%2520that%2520the%2520proposed%2520SALF-CBM%253A%2520%25281%2529%250AOutperforms%2520non-spatial%2520CBM%2520methods%252C%2520as%2520well%2520as%2520the%2520original%2520backbone%252C%2520on%2520a%250Avariety%2520of%2520classification%2520tasks%253B%2520%25282%2529%2520Produces%2520high-quality%2520spatial%250Aexplanations%252C%2520outperforming%2520widely%2520used%2520heatmap-based%2520methods%2520on%2520a%2520zero-shot%250Asegmentation%2520task%253B%2520%25283%2529%2520Facilitates%2520model%2520exploration%2520and%2520debugging%252C%2520enabling%250Ausers%2520to%2520query%2520specific%2520image%2520regions%2520and%2520refine%2520the%2520model%2527s%2520decisions%2520by%250Alocally%2520editing%2520its%2520concept%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20134v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Show%20and%20Tell%3A%20Visually%20Explainable%20Deep%20Neural%20Nets%20via%20Spatially-Aware%0A%20%20Concept%20Bottleneck%20Models&entry.906535625=Itay%20Benou%20and%20Tammy%20Riklin-Raviv&entry.1292438233=%20%20Modern%20deep%20neural%20networks%20have%20now%20reached%20human-level%20performance%20across%20a%0Avariety%20of%20tasks.%20However%2C%20unlike%20humans%20they%20lack%20the%20ability%20to%20explain%20their%0Adecisions%20by%20showing%20where%20and%20telling%20what%20concepts%20guided%20them.%20In%20this%20work%2C%0Awe%20present%20a%20unified%20framework%20for%20transforming%20any%20vision%20neural%20network%20into%0Aa%20spatially%20and%20conceptually%20interpretable%20model.%20We%20introduce%20a%0Aspatially-aware%20concept%20bottleneck%20layer%20that%20projects%20%22black-box%22%20features%20of%0Apre-trained%20backbone%20models%20into%20interpretable%20concept%20maps%2C%20without%20requiring%0Ahuman%20labels.%20By%20training%20a%20classification%20layer%20over%20this%20bottleneck%2C%20we%0Aobtain%20a%20self-explaining%20model%20that%20articulates%20which%20concepts%20most%20influenced%0Aits%20prediction%2C%20along%20with%20heatmaps%20that%20ground%20them%20in%20the%20input%20image.%0AAccordingly%2C%20we%20name%20this%20method%20%22Spatially-Aware%20and%20Label-Free%20Concept%0ABottleneck%20Model%22%20%28SALF-CBM%29.%20Our%20results%20show%20that%20the%20proposed%20SALF-CBM%3A%20%281%29%0AOutperforms%20non-spatial%20CBM%20methods%2C%20as%20well%20as%20the%20original%20backbone%2C%20on%20a%0Avariety%20of%20classification%20tasks%3B%20%282%29%20Produces%20high-quality%20spatial%0Aexplanations%2C%20outperforming%20widely%20used%20heatmap-based%20methods%20on%20a%20zero-shot%0Asegmentation%20task%3B%20%283%29%20Facilitates%20model%20exploration%20and%20debugging%2C%20enabling%0Ausers%20to%20query%20specific%20image%20regions%20and%20refine%20the%20model%27s%20decisions%20by%0Alocally%20editing%20its%20concept%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20134v4&entry.124074799=Read"},
{"title": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization\n  in Chest Radiographs", "author": "Advait Gosai and Arun Kavishwar and Stephanie L. McNamara and Soujanya Samineni and Renato Umeton and Alexander Chowdhury and William Lotter", "abstract": "  Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use.\n", "link": "http://arxiv.org/abs/2509.18015v1", "date": "2025-09-22", "relevancy": 2.6595, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5377}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Diagnosis%3A%20Evaluating%20Multimodal%20LLMs%20for%20Pathology%20Localization%0A%20%20in%20Chest%20Radiographs&body=Title%3A%20Beyond%20Diagnosis%3A%20Evaluating%20Multimodal%20LLMs%20for%20Pathology%20Localization%0A%20%20in%20Chest%20Radiographs%0AAuthor%3A%20Advait%20Gosai%20and%20Arun%20Kavishwar%20and%20Stephanie%20L.%20McNamara%20and%20Soujanya%20Samineni%20and%20Renato%20Umeton%20and%20Alexander%20Chowdhury%20and%20William%20Lotter%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20promising%20performance%20of%20frontier%20large%20language%20models%0A%28LLMs%29%20and%20their%20multimodal%20counterparts%20in%20medical%20quizzes%20and%20diagnostic%0Atasks%2C%20highlighting%20their%20potential%20for%20broad%20clinical%20utility%20given%20their%0Aaccessible%2C%20general-purpose%20nature.%20However%2C%20beyond%20diagnosis%2C%20a%20fundamental%0Aaspect%20of%20medical%20image%20interpretation%20is%20the%20ability%20to%20localize%20pathological%0Afindings.%20Evaluating%20localization%20not%20only%20has%20clinical%20and%20educational%0Arelevance%20but%20also%20provides%20insight%20into%20a%20model%27s%20spatial%20understanding%20of%0Aanatomy%20and%20disease.%20Here%2C%20we%20systematically%20assess%20two%20general-purpose%20MLLMs%0A%28GPT-4%20and%20GPT-5%29%20and%20a%20domain-specific%20model%20%28MedGemma%29%20in%20their%20ability%20to%0Alocalize%20pathologies%20on%20chest%20radiographs%2C%20using%20a%20prompting%20pipeline%20that%0Aoverlays%20a%20spatial%20grid%20and%20elicits%20coordinate-based%20predictions.%20Averaged%0Aacross%20nine%20pathologies%20in%20the%20CheXlocalize%20dataset%2C%20GPT-5%20exhibited%20a%0Alocalization%20accuracy%20of%2049.7%25%2C%20followed%20by%20GPT-4%20%2839.1%25%29%20and%20MedGemma%20%2817.7%25%29%2C%0Aall%20lower%20than%20a%20task-specific%20CNN%20baseline%20%2859.9%25%29%20and%20a%20radiologist%20benchmark%0A%2880.1%25%29.%20Despite%20modest%20performance%2C%20error%20analysis%20revealed%20that%20GPT-5%27s%0Apredictions%20were%20largely%20in%20anatomically%20plausible%20regions%2C%20just%20not%20always%0Aprecisely%20localized.%20GPT-4%20performed%20well%20on%20pathologies%20with%20fixed%20anatomical%0Alocations%2C%20but%20struggled%20with%20spatially%20variable%20findings%20and%20exhibited%0Aanatomically%20implausible%20predictions%20more%20frequently.%20MedGemma%20demonstrated%20the%0Alowest%20performance%20on%20all%20pathologies%2C%20showing%20limited%20capacity%20to%20generalize%0Ato%20this%20novel%20task.%20Our%20findings%20highlight%20both%20the%20promise%20and%20limitations%20of%0Acurrent%20MLLMs%20in%20medical%20imaging%20and%20underscore%20the%20importance%20of%20integrating%0Athem%20with%20task-specific%20tools%20for%20reliable%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Diagnosis%253A%2520Evaluating%2520Multimodal%2520LLMs%2520for%2520Pathology%2520Localization%250A%2520%2520in%2520Chest%2520Radiographs%26entry.906535625%3DAdvait%2520Gosai%2520and%2520Arun%2520Kavishwar%2520and%2520Stephanie%2520L.%2520McNamara%2520and%2520Soujanya%2520Samineni%2520and%2520Renato%2520Umeton%2520and%2520Alexander%2520Chowdhury%2520and%2520William%2520Lotter%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520promising%2520performance%2520of%2520frontier%2520large%2520language%2520models%250A%2528LLMs%2529%2520and%2520their%2520multimodal%2520counterparts%2520in%2520medical%2520quizzes%2520and%2520diagnostic%250Atasks%252C%2520highlighting%2520their%2520potential%2520for%2520broad%2520clinical%2520utility%2520given%2520their%250Aaccessible%252C%2520general-purpose%2520nature.%2520However%252C%2520beyond%2520diagnosis%252C%2520a%2520fundamental%250Aaspect%2520of%2520medical%2520image%2520interpretation%2520is%2520the%2520ability%2520to%2520localize%2520pathological%250Afindings.%2520Evaluating%2520localization%2520not%2520only%2520has%2520clinical%2520and%2520educational%250Arelevance%2520but%2520also%2520provides%2520insight%2520into%2520a%2520model%2527s%2520spatial%2520understanding%2520of%250Aanatomy%2520and%2520disease.%2520Here%252C%2520we%2520systematically%2520assess%2520two%2520general-purpose%2520MLLMs%250A%2528GPT-4%2520and%2520GPT-5%2529%2520and%2520a%2520domain-specific%2520model%2520%2528MedGemma%2529%2520in%2520their%2520ability%2520to%250Alocalize%2520pathologies%2520on%2520chest%2520radiographs%252C%2520using%2520a%2520prompting%2520pipeline%2520that%250Aoverlays%2520a%2520spatial%2520grid%2520and%2520elicits%2520coordinate-based%2520predictions.%2520Averaged%250Aacross%2520nine%2520pathologies%2520in%2520the%2520CheXlocalize%2520dataset%252C%2520GPT-5%2520exhibited%2520a%250Alocalization%2520accuracy%2520of%252049.7%2525%252C%2520followed%2520by%2520GPT-4%2520%252839.1%2525%2529%2520and%2520MedGemma%2520%252817.7%2525%2529%252C%250Aall%2520lower%2520than%2520a%2520task-specific%2520CNN%2520baseline%2520%252859.9%2525%2529%2520and%2520a%2520radiologist%2520benchmark%250A%252880.1%2525%2529.%2520Despite%2520modest%2520performance%252C%2520error%2520analysis%2520revealed%2520that%2520GPT-5%2527s%250Apredictions%2520were%2520largely%2520in%2520anatomically%2520plausible%2520regions%252C%2520just%2520not%2520always%250Aprecisely%2520localized.%2520GPT-4%2520performed%2520well%2520on%2520pathologies%2520with%2520fixed%2520anatomical%250Alocations%252C%2520but%2520struggled%2520with%2520spatially%2520variable%2520findings%2520and%2520exhibited%250Aanatomically%2520implausible%2520predictions%2520more%2520frequently.%2520MedGemma%2520demonstrated%2520the%250Alowest%2520performance%2520on%2520all%2520pathologies%252C%2520showing%2520limited%2520capacity%2520to%2520generalize%250Ato%2520this%2520novel%2520task.%2520Our%2520findings%2520highlight%2520both%2520the%2520promise%2520and%2520limitations%2520of%250Acurrent%2520MLLMs%2520in%2520medical%2520imaging%2520and%2520underscore%2520the%2520importance%2520of%2520integrating%250Athem%2520with%2520task-specific%2520tools%2520for%2520reliable%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Diagnosis%3A%20Evaluating%20Multimodal%20LLMs%20for%20Pathology%20Localization%0A%20%20in%20Chest%20Radiographs&entry.906535625=Advait%20Gosai%20and%20Arun%20Kavishwar%20and%20Stephanie%20L.%20McNamara%20and%20Soujanya%20Samineni%20and%20Renato%20Umeton%20and%20Alexander%20Chowdhury%20and%20William%20Lotter&entry.1292438233=%20%20Recent%20work%20has%20shown%20promising%20performance%20of%20frontier%20large%20language%20models%0A%28LLMs%29%20and%20their%20multimodal%20counterparts%20in%20medical%20quizzes%20and%20diagnostic%0Atasks%2C%20highlighting%20their%20potential%20for%20broad%20clinical%20utility%20given%20their%0Aaccessible%2C%20general-purpose%20nature.%20However%2C%20beyond%20diagnosis%2C%20a%20fundamental%0Aaspect%20of%20medical%20image%20interpretation%20is%20the%20ability%20to%20localize%20pathological%0Afindings.%20Evaluating%20localization%20not%20only%20has%20clinical%20and%20educational%0Arelevance%20but%20also%20provides%20insight%20into%20a%20model%27s%20spatial%20understanding%20of%0Aanatomy%20and%20disease.%20Here%2C%20we%20systematically%20assess%20two%20general-purpose%20MLLMs%0A%28GPT-4%20and%20GPT-5%29%20and%20a%20domain-specific%20model%20%28MedGemma%29%20in%20their%20ability%20to%0Alocalize%20pathologies%20on%20chest%20radiographs%2C%20using%20a%20prompting%20pipeline%20that%0Aoverlays%20a%20spatial%20grid%20and%20elicits%20coordinate-based%20predictions.%20Averaged%0Aacross%20nine%20pathologies%20in%20the%20CheXlocalize%20dataset%2C%20GPT-5%20exhibited%20a%0Alocalization%20accuracy%20of%2049.7%25%2C%20followed%20by%20GPT-4%20%2839.1%25%29%20and%20MedGemma%20%2817.7%25%29%2C%0Aall%20lower%20than%20a%20task-specific%20CNN%20baseline%20%2859.9%25%29%20and%20a%20radiologist%20benchmark%0A%2880.1%25%29.%20Despite%20modest%20performance%2C%20error%20analysis%20revealed%20that%20GPT-5%27s%0Apredictions%20were%20largely%20in%20anatomically%20plausible%20regions%2C%20just%20not%20always%0Aprecisely%20localized.%20GPT-4%20performed%20well%20on%20pathologies%20with%20fixed%20anatomical%0Alocations%2C%20but%20struggled%20with%20spatially%20variable%20findings%20and%20exhibited%0Aanatomically%20implausible%20predictions%20more%20frequently.%20MedGemma%20demonstrated%20the%0Alowest%20performance%20on%20all%20pathologies%2C%20showing%20limited%20capacity%20to%20generalize%0Ato%20this%20novel%20task.%20Our%20findings%20highlight%20both%20the%20promise%20and%20limitations%20of%0Acurrent%20MLLMs%20in%20medical%20imaging%20and%20underscore%20the%20importance%20of%20integrating%0Athem%20with%20task-specific%20tools%20for%20reliable%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18015v1&entry.124074799=Read"},
{"title": "Cover Learning for Large-Scale Topology Representation", "author": "Luis Scoccola and Uzu Lim and Heather A. Harrington", "abstract": "  Classical unsupervised learning methods like clustering and linear\ndimensionality reduction parametrize large-scale geometry when it is discrete\nor linear, while more modern methods from manifold learning find low\ndimensional representation or infer local geometry by constructing a graph on\nthe input data. More recently, topological data analysis popularized the use of\nsimplicial complexes to represent data topology with two main methodologies:\ntopological inference with geometric complexes and large-scale topology\nvisualization with Mapper graphs -- central to these is the nerve construction\nfrom topology, which builds a simplicial complex given a cover of a space by\nsubsets. While successful, these have limitations: geometric complexes scale\npoorly with data size, and Mapper graphs can be hard to tune and only contain\nlow dimensional information. In this paper, we propose to study the problem of\nlearning covers in its own right, and from the perspective of optimization. We\ndescribe a method for learning topologically-faithful covers of geometric\ndatasets, and show that the simplicial complexes thus obtained can outperform\nstandard topological inference approaches in terms of size, and Mapper-type\nalgorithms in terms of representation of large-scale topology.\n", "link": "http://arxiv.org/abs/2503.09767v2", "date": "2025-09-22", "relevancy": 2.6509, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6063}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cover%20Learning%20for%20Large-Scale%20Topology%20Representation&body=Title%3A%20Cover%20Learning%20for%20Large-Scale%20Topology%20Representation%0AAuthor%3A%20Luis%20Scoccola%20and%20Uzu%20Lim%20and%20Heather%20A.%20Harrington%0AAbstract%3A%20%20%20Classical%20unsupervised%20learning%20methods%20like%20clustering%20and%20linear%0Adimensionality%20reduction%20parametrize%20large-scale%20geometry%20when%20it%20is%20discrete%0Aor%20linear%2C%20while%20more%20modern%20methods%20from%20manifold%20learning%20find%20low%0Adimensional%20representation%20or%20infer%20local%20geometry%20by%20constructing%20a%20graph%20on%0Athe%20input%20data.%20More%20recently%2C%20topological%20data%20analysis%20popularized%20the%20use%20of%0Asimplicial%20complexes%20to%20represent%20data%20topology%20with%20two%20main%20methodologies%3A%0Atopological%20inference%20with%20geometric%20complexes%20and%20large-scale%20topology%0Avisualization%20with%20Mapper%20graphs%20--%20central%20to%20these%20is%20the%20nerve%20construction%0Afrom%20topology%2C%20which%20builds%20a%20simplicial%20complex%20given%20a%20cover%20of%20a%20space%20by%0Asubsets.%20While%20successful%2C%20these%20have%20limitations%3A%20geometric%20complexes%20scale%0Apoorly%20with%20data%20size%2C%20and%20Mapper%20graphs%20can%20be%20hard%20to%20tune%20and%20only%20contain%0Alow%20dimensional%20information.%20In%20this%20paper%2C%20we%20propose%20to%20study%20the%20problem%20of%0Alearning%20covers%20in%20its%20own%20right%2C%20and%20from%20the%20perspective%20of%20optimization.%20We%0Adescribe%20a%20method%20for%20learning%20topologically-faithful%20covers%20of%20geometric%0Adatasets%2C%20and%20show%20that%20the%20simplicial%20complexes%20thus%20obtained%20can%20outperform%0Astandard%20topological%20inference%20approaches%20in%20terms%20of%20size%2C%20and%20Mapper-type%0Aalgorithms%20in%20terms%20of%20representation%20of%20large-scale%20topology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCover%2520Learning%2520for%2520Large-Scale%2520Topology%2520Representation%26entry.906535625%3DLuis%2520Scoccola%2520and%2520Uzu%2520Lim%2520and%2520Heather%2520A.%2520Harrington%26entry.1292438233%3D%2520%2520Classical%2520unsupervised%2520learning%2520methods%2520like%2520clustering%2520and%2520linear%250Adimensionality%2520reduction%2520parametrize%2520large-scale%2520geometry%2520when%2520it%2520is%2520discrete%250Aor%2520linear%252C%2520while%2520more%2520modern%2520methods%2520from%2520manifold%2520learning%2520find%2520low%250Adimensional%2520representation%2520or%2520infer%2520local%2520geometry%2520by%2520constructing%2520a%2520graph%2520on%250Athe%2520input%2520data.%2520More%2520recently%252C%2520topological%2520data%2520analysis%2520popularized%2520the%2520use%2520of%250Asimplicial%2520complexes%2520to%2520represent%2520data%2520topology%2520with%2520two%2520main%2520methodologies%253A%250Atopological%2520inference%2520with%2520geometric%2520complexes%2520and%2520large-scale%2520topology%250Avisualization%2520with%2520Mapper%2520graphs%2520--%2520central%2520to%2520these%2520is%2520the%2520nerve%2520construction%250Afrom%2520topology%252C%2520which%2520builds%2520a%2520simplicial%2520complex%2520given%2520a%2520cover%2520of%2520a%2520space%2520by%250Asubsets.%2520While%2520successful%252C%2520these%2520have%2520limitations%253A%2520geometric%2520complexes%2520scale%250Apoorly%2520with%2520data%2520size%252C%2520and%2520Mapper%2520graphs%2520can%2520be%2520hard%2520to%2520tune%2520and%2520only%2520contain%250Alow%2520dimensional%2520information.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520study%2520the%2520problem%2520of%250Alearning%2520covers%2520in%2520its%2520own%2520right%252C%2520and%2520from%2520the%2520perspective%2520of%2520optimization.%2520We%250Adescribe%2520a%2520method%2520for%2520learning%2520topologically-faithful%2520covers%2520of%2520geometric%250Adatasets%252C%2520and%2520show%2520that%2520the%2520simplicial%2520complexes%2520thus%2520obtained%2520can%2520outperform%250Astandard%2520topological%2520inference%2520approaches%2520in%2520terms%2520of%2520size%252C%2520and%2520Mapper-type%250Aalgorithms%2520in%2520terms%2520of%2520representation%2520of%2520large-scale%2520topology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cover%20Learning%20for%20Large-Scale%20Topology%20Representation&entry.906535625=Luis%20Scoccola%20and%20Uzu%20Lim%20and%20Heather%20A.%20Harrington&entry.1292438233=%20%20Classical%20unsupervised%20learning%20methods%20like%20clustering%20and%20linear%0Adimensionality%20reduction%20parametrize%20large-scale%20geometry%20when%20it%20is%20discrete%0Aor%20linear%2C%20while%20more%20modern%20methods%20from%20manifold%20learning%20find%20low%0Adimensional%20representation%20or%20infer%20local%20geometry%20by%20constructing%20a%20graph%20on%0Athe%20input%20data.%20More%20recently%2C%20topological%20data%20analysis%20popularized%20the%20use%20of%0Asimplicial%20complexes%20to%20represent%20data%20topology%20with%20two%20main%20methodologies%3A%0Atopological%20inference%20with%20geometric%20complexes%20and%20large-scale%20topology%0Avisualization%20with%20Mapper%20graphs%20--%20central%20to%20these%20is%20the%20nerve%20construction%0Afrom%20topology%2C%20which%20builds%20a%20simplicial%20complex%20given%20a%20cover%20of%20a%20space%20by%0Asubsets.%20While%20successful%2C%20these%20have%20limitations%3A%20geometric%20complexes%20scale%0Apoorly%20with%20data%20size%2C%20and%20Mapper%20graphs%20can%20be%20hard%20to%20tune%20and%20only%20contain%0Alow%20dimensional%20information.%20In%20this%20paper%2C%20we%20propose%20to%20study%20the%20problem%20of%0Alearning%20covers%20in%20its%20own%20right%2C%20and%20from%20the%20perspective%20of%20optimization.%20We%0Adescribe%20a%20method%20for%20learning%20topologically-faithful%20covers%20of%20geometric%0Adatasets%2C%20and%20show%20that%20the%20simplicial%20complexes%20thus%20obtained%20can%20outperform%0Astandard%20topological%20inference%20approaches%20in%20terms%20of%20size%2C%20and%20Mapper-type%0Aalgorithms%20in%20terms%20of%20representation%20of%20large-scale%20topology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09767v2&entry.124074799=Read"},
{"title": "Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on\n  Tip-handle Detection and Matching", "author": "Zhuo Xiao and Fugen Zhou and Jingjing Wang and Chongyu He and Bo Liu and Haitao Sun and Zhe Ji and Yuliang Jiang and Junjie Wang and Qiuwen Wu", "abstract": "  Accurate multi-needle localization in intraoperative CT images is crucial for\noptimizing seed placement in pelvic seed implant brachytherapy. However, this\ntask is challenging due to poor image contrast and needle adhesion. This paper\npresents a novel approach that reframes needle localization as a tip-handle\ndetection and matching problem to overcome these difficulties. An anchor-free\nnetwork, based on HRNet, is proposed to extract multi-scale features and\naccurately detect needle tips and handles by predicting their centers and\norientations using decoupled branches for heatmap regression and polar angle\nprediction. To associate detected tips and handles into individual needles, a\ngreedy matching and merging (GMM) method designed to solve the unbalanced\nassignment problem with constraints (UAP-C) is presented. The GMM method\niteratively selects the most probable tip-handle pairs and merges them based on\na distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100\npatients, the proposed method demonstrates superior performance, achieving\nhigher precision and F1 score compared to a segmentation-based method utilizing\nthe nnUNet model,thereby offering a more robust and accurate solution for\nneedle localization in complex clinical scenarios.\n", "link": "http://arxiv.org/abs/2509.17931v1", "date": "2025-09-22", "relevancy": 2.6469, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5812}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5099}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-needle%20Localization%20for%20Pelvic%20Seed%20Implant%20Brachytherapy%20based%20on%0A%20%20Tip-handle%20Detection%20and%20Matching&body=Title%3A%20Multi-needle%20Localization%20for%20Pelvic%20Seed%20Implant%20Brachytherapy%20based%20on%0A%20%20Tip-handle%20Detection%20and%20Matching%0AAuthor%3A%20Zhuo%20Xiao%20and%20Fugen%20Zhou%20and%20Jingjing%20Wang%20and%20Chongyu%20He%20and%20Bo%20Liu%20and%20Haitao%20Sun%20and%20Zhe%20Ji%20and%20Yuliang%20Jiang%20and%20Junjie%20Wang%20and%20Qiuwen%20Wu%0AAbstract%3A%20%20%20Accurate%20multi-needle%20localization%20in%20intraoperative%20CT%20images%20is%20crucial%20for%0Aoptimizing%20seed%20placement%20in%20pelvic%20seed%20implant%20brachytherapy.%20However%2C%20this%0Atask%20is%20challenging%20due%20to%20poor%20image%20contrast%20and%20needle%20adhesion.%20This%20paper%0Apresents%20a%20novel%20approach%20that%20reframes%20needle%20localization%20as%20a%20tip-handle%0Adetection%20and%20matching%20problem%20to%20overcome%20these%20difficulties.%20An%20anchor-free%0Anetwork%2C%20based%20on%20HRNet%2C%20is%20proposed%20to%20extract%20multi-scale%20features%20and%0Aaccurately%20detect%20needle%20tips%20and%20handles%20by%20predicting%20their%20centers%20and%0Aorientations%20using%20decoupled%20branches%20for%20heatmap%20regression%20and%20polar%20angle%0Aprediction.%20To%20associate%20detected%20tips%20and%20handles%20into%20individual%20needles%2C%20a%0Agreedy%20matching%20and%20merging%20%28GMM%29%20method%20designed%20to%20solve%20the%20unbalanced%0Aassignment%20problem%20with%20constraints%20%28UAP-C%29%20is%20presented.%20The%20GMM%20method%0Aiteratively%20selects%20the%20most%20probable%20tip-handle%20pairs%20and%20merges%20them%20based%20on%0Aa%20distance%20metric%20to%20reconstruct%203D%20needle%20paths.%20Evaluated%20on%20a%20dataset%20of%20100%0Apatients%2C%20the%20proposed%20method%20demonstrates%20superior%20performance%2C%20achieving%0Ahigher%20precision%20and%20F1%20score%20compared%20to%20a%20segmentation-based%20method%20utilizing%0Athe%20nnUNet%20model%2Cthereby%20offering%20a%20more%20robust%20and%20accurate%20solution%20for%0Aneedle%20localization%20in%20complex%20clinical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-needle%2520Localization%2520for%2520Pelvic%2520Seed%2520Implant%2520Brachytherapy%2520based%2520on%250A%2520%2520Tip-handle%2520Detection%2520and%2520Matching%26entry.906535625%3DZhuo%2520Xiao%2520and%2520Fugen%2520Zhou%2520and%2520Jingjing%2520Wang%2520and%2520Chongyu%2520He%2520and%2520Bo%2520Liu%2520and%2520Haitao%2520Sun%2520and%2520Zhe%2520Ji%2520and%2520Yuliang%2520Jiang%2520and%2520Junjie%2520Wang%2520and%2520Qiuwen%2520Wu%26entry.1292438233%3D%2520%2520Accurate%2520multi-needle%2520localization%2520in%2520intraoperative%2520CT%2520images%2520is%2520crucial%2520for%250Aoptimizing%2520seed%2520placement%2520in%2520pelvic%2520seed%2520implant%2520brachytherapy.%2520However%252C%2520this%250Atask%2520is%2520challenging%2520due%2520to%2520poor%2520image%2520contrast%2520and%2520needle%2520adhesion.%2520This%2520paper%250Apresents%2520a%2520novel%2520approach%2520that%2520reframes%2520needle%2520localization%2520as%2520a%2520tip-handle%250Adetection%2520and%2520matching%2520problem%2520to%2520overcome%2520these%2520difficulties.%2520An%2520anchor-free%250Anetwork%252C%2520based%2520on%2520HRNet%252C%2520is%2520proposed%2520to%2520extract%2520multi-scale%2520features%2520and%250Aaccurately%2520detect%2520needle%2520tips%2520and%2520handles%2520by%2520predicting%2520their%2520centers%2520and%250Aorientations%2520using%2520decoupled%2520branches%2520for%2520heatmap%2520regression%2520and%2520polar%2520angle%250Aprediction.%2520To%2520associate%2520detected%2520tips%2520and%2520handles%2520into%2520individual%2520needles%252C%2520a%250Agreedy%2520matching%2520and%2520merging%2520%2528GMM%2529%2520method%2520designed%2520to%2520solve%2520the%2520unbalanced%250Aassignment%2520problem%2520with%2520constraints%2520%2528UAP-C%2529%2520is%2520presented.%2520The%2520GMM%2520method%250Aiteratively%2520selects%2520the%2520most%2520probable%2520tip-handle%2520pairs%2520and%2520merges%2520them%2520based%2520on%250Aa%2520distance%2520metric%2520to%2520reconstruct%25203D%2520needle%2520paths.%2520Evaluated%2520on%2520a%2520dataset%2520of%2520100%250Apatients%252C%2520the%2520proposed%2520method%2520demonstrates%2520superior%2520performance%252C%2520achieving%250Ahigher%2520precision%2520and%2520F1%2520score%2520compared%2520to%2520a%2520segmentation-based%2520method%2520utilizing%250Athe%2520nnUNet%2520model%252Cthereby%2520offering%2520a%2520more%2520robust%2520and%2520accurate%2520solution%2520for%250Aneedle%2520localization%2520in%2520complex%2520clinical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-needle%20Localization%20for%20Pelvic%20Seed%20Implant%20Brachytherapy%20based%20on%0A%20%20Tip-handle%20Detection%20and%20Matching&entry.906535625=Zhuo%20Xiao%20and%20Fugen%20Zhou%20and%20Jingjing%20Wang%20and%20Chongyu%20He%20and%20Bo%20Liu%20and%20Haitao%20Sun%20and%20Zhe%20Ji%20and%20Yuliang%20Jiang%20and%20Junjie%20Wang%20and%20Qiuwen%20Wu&entry.1292438233=%20%20Accurate%20multi-needle%20localization%20in%20intraoperative%20CT%20images%20is%20crucial%20for%0Aoptimizing%20seed%20placement%20in%20pelvic%20seed%20implant%20brachytherapy.%20However%2C%20this%0Atask%20is%20challenging%20due%20to%20poor%20image%20contrast%20and%20needle%20adhesion.%20This%20paper%0Apresents%20a%20novel%20approach%20that%20reframes%20needle%20localization%20as%20a%20tip-handle%0Adetection%20and%20matching%20problem%20to%20overcome%20these%20difficulties.%20An%20anchor-free%0Anetwork%2C%20based%20on%20HRNet%2C%20is%20proposed%20to%20extract%20multi-scale%20features%20and%0Aaccurately%20detect%20needle%20tips%20and%20handles%20by%20predicting%20their%20centers%20and%0Aorientations%20using%20decoupled%20branches%20for%20heatmap%20regression%20and%20polar%20angle%0Aprediction.%20To%20associate%20detected%20tips%20and%20handles%20into%20individual%20needles%2C%20a%0Agreedy%20matching%20and%20merging%20%28GMM%29%20method%20designed%20to%20solve%20the%20unbalanced%0Aassignment%20problem%20with%20constraints%20%28UAP-C%29%20is%20presented.%20The%20GMM%20method%0Aiteratively%20selects%20the%20most%20probable%20tip-handle%20pairs%20and%20merges%20them%20based%20on%0Aa%20distance%20metric%20to%20reconstruct%203D%20needle%20paths.%20Evaluated%20on%20a%20dataset%20of%20100%0Apatients%2C%20the%20proposed%20method%20demonstrates%20superior%20performance%2C%20achieving%0Ahigher%20precision%20and%20F1%20score%20compared%20to%20a%20segmentation-based%20method%20utilizing%0Athe%20nnUNet%20model%2Cthereby%20offering%20a%20more%20robust%20and%20accurate%20solution%20for%0Aneedle%20localization%20in%20complex%20clinical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17931v1&entry.124074799=Read"},
{"title": "SD-VSum: A Method and Dataset for Script-Driven Video Summarization", "author": "Manolis Mylonas and Evlampios Apostolidis and Vasileios Mezaris", "abstract": "  In this work, we introduce the task of script-driven video summarization,\nwhich aims to produce a summary of the full-length video by selecting the parts\nthat are most relevant to a user-provided script outlining the visual content\nof the desired summary. Following, we extend a recently-introduced large-scale\ndataset for generic video summarization (VideoXum) by producing natural\nlanguage descriptions of the different human-annotated summaries that are\navailable per video. In this way we make it compatible with the introduced\ntask, since the available triplets of ``video, summary and summary\ndescription'' can be used for training a method that is able to produce\ndifferent summaries for a given video, driven by the provided script about the\ncontent of each summary. Finally, we develop a new network architecture for\nscript-driven video summarization (SD-VSum), that employs a cross-modal\nattention mechanism for aligning and fusing information from the visual and\ntext modalities. Our experimental evaluations demonstrate the advanced\nperformance of SD-VSum against SOTA approaches for query-driven and generic\n(unimodal and multimodal) summarization from the literature, and document its\ncapacity to produce video summaries that are adapted to each user's needs about\ntheir content.\n", "link": "http://arxiv.org/abs/2505.03319v2", "date": "2025-09-22", "relevancy": 2.6412, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SD-VSum%3A%20A%20Method%20and%20Dataset%20for%20Script-Driven%20Video%20Summarization&body=Title%3A%20SD-VSum%3A%20A%20Method%20and%20Dataset%20for%20Script-Driven%20Video%20Summarization%0AAuthor%3A%20Manolis%20Mylonas%20and%20Evlampios%20Apostolidis%20and%20Vasileios%20Mezaris%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20the%20task%20of%20script-driven%20video%20summarization%2C%0Awhich%20aims%20to%20produce%20a%20summary%20of%20the%20full-length%20video%20by%20selecting%20the%20parts%0Athat%20are%20most%20relevant%20to%20a%20user-provided%20script%20outlining%20the%20visual%20content%0Aof%20the%20desired%20summary.%20Following%2C%20we%20extend%20a%20recently-introduced%20large-scale%0Adataset%20for%20generic%20video%20summarization%20%28VideoXum%29%20by%20producing%20natural%0Alanguage%20descriptions%20of%20the%20different%20human-annotated%20summaries%20that%20are%0Aavailable%20per%20video.%20In%20this%20way%20we%20make%20it%20compatible%20with%20the%20introduced%0Atask%2C%20since%20the%20available%20triplets%20of%20%60%60video%2C%20summary%20and%20summary%0Adescription%27%27%20can%20be%20used%20for%20training%20a%20method%20that%20is%20able%20to%20produce%0Adifferent%20summaries%20for%20a%20given%20video%2C%20driven%20by%20the%20provided%20script%20about%20the%0Acontent%20of%20each%20summary.%20Finally%2C%20we%20develop%20a%20new%20network%20architecture%20for%0Ascript-driven%20video%20summarization%20%28SD-VSum%29%2C%20that%20employs%20a%20cross-modal%0Aattention%20mechanism%20for%20aligning%20and%20fusing%20information%20from%20the%20visual%20and%0Atext%20modalities.%20Our%20experimental%20evaluations%20demonstrate%20the%20advanced%0Aperformance%20of%20SD-VSum%20against%20SOTA%20approaches%20for%20query-driven%20and%20generic%0A%28unimodal%20and%20multimodal%29%20summarization%20from%20the%20literature%2C%20and%20document%20its%0Acapacity%20to%20produce%20video%20summaries%20that%20are%20adapted%20to%20each%20user%27s%20needs%20about%0Atheir%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSD-VSum%253A%2520A%2520Method%2520and%2520Dataset%2520for%2520Script-Driven%2520Video%2520Summarization%26entry.906535625%3DManolis%2520Mylonas%2520and%2520Evlampios%2520Apostolidis%2520and%2520Vasileios%2520Mezaris%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520task%2520of%2520script-driven%2520video%2520summarization%252C%250Awhich%2520aims%2520to%2520produce%2520a%2520summary%2520of%2520the%2520full-length%2520video%2520by%2520selecting%2520the%2520parts%250Athat%2520are%2520most%2520relevant%2520to%2520a%2520user-provided%2520script%2520outlining%2520the%2520visual%2520content%250Aof%2520the%2520desired%2520summary.%2520Following%252C%2520we%2520extend%2520a%2520recently-introduced%2520large-scale%250Adataset%2520for%2520generic%2520video%2520summarization%2520%2528VideoXum%2529%2520by%2520producing%2520natural%250Alanguage%2520descriptions%2520of%2520the%2520different%2520human-annotated%2520summaries%2520that%2520are%250Aavailable%2520per%2520video.%2520In%2520this%2520way%2520we%2520make%2520it%2520compatible%2520with%2520the%2520introduced%250Atask%252C%2520since%2520the%2520available%2520triplets%2520of%2520%2560%2560video%252C%2520summary%2520and%2520summary%250Adescription%2527%2527%2520can%2520be%2520used%2520for%2520training%2520a%2520method%2520that%2520is%2520able%2520to%2520produce%250Adifferent%2520summaries%2520for%2520a%2520given%2520video%252C%2520driven%2520by%2520the%2520provided%2520script%2520about%2520the%250Acontent%2520of%2520each%2520summary.%2520Finally%252C%2520we%2520develop%2520a%2520new%2520network%2520architecture%2520for%250Ascript-driven%2520video%2520summarization%2520%2528SD-VSum%2529%252C%2520that%2520employs%2520a%2520cross-modal%250Aattention%2520mechanism%2520for%2520aligning%2520and%2520fusing%2520information%2520from%2520the%2520visual%2520and%250Atext%2520modalities.%2520Our%2520experimental%2520evaluations%2520demonstrate%2520the%2520advanced%250Aperformance%2520of%2520SD-VSum%2520against%2520SOTA%2520approaches%2520for%2520query-driven%2520and%2520generic%250A%2528unimodal%2520and%2520multimodal%2529%2520summarization%2520from%2520the%2520literature%252C%2520and%2520document%2520its%250Acapacity%2520to%2520produce%2520video%2520summaries%2520that%2520are%2520adapted%2520to%2520each%2520user%2527s%2520needs%2520about%250Atheir%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SD-VSum%3A%20A%20Method%20and%20Dataset%20for%20Script-Driven%20Video%20Summarization&entry.906535625=Manolis%20Mylonas%20and%20Evlampios%20Apostolidis%20and%20Vasileios%20Mezaris&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20the%20task%20of%20script-driven%20video%20summarization%2C%0Awhich%20aims%20to%20produce%20a%20summary%20of%20the%20full-length%20video%20by%20selecting%20the%20parts%0Athat%20are%20most%20relevant%20to%20a%20user-provided%20script%20outlining%20the%20visual%20content%0Aof%20the%20desired%20summary.%20Following%2C%20we%20extend%20a%20recently-introduced%20large-scale%0Adataset%20for%20generic%20video%20summarization%20%28VideoXum%29%20by%20producing%20natural%0Alanguage%20descriptions%20of%20the%20different%20human-annotated%20summaries%20that%20are%0Aavailable%20per%20video.%20In%20this%20way%20we%20make%20it%20compatible%20with%20the%20introduced%0Atask%2C%20since%20the%20available%20triplets%20of%20%60%60video%2C%20summary%20and%20summary%0Adescription%27%27%20can%20be%20used%20for%20training%20a%20method%20that%20is%20able%20to%20produce%0Adifferent%20summaries%20for%20a%20given%20video%2C%20driven%20by%20the%20provided%20script%20about%20the%0Acontent%20of%20each%20summary.%20Finally%2C%20we%20develop%20a%20new%20network%20architecture%20for%0Ascript-driven%20video%20summarization%20%28SD-VSum%29%2C%20that%20employs%20a%20cross-modal%0Aattention%20mechanism%20for%20aligning%20and%20fusing%20information%20from%20the%20visual%20and%0Atext%20modalities.%20Our%20experimental%20evaluations%20demonstrate%20the%20advanced%0Aperformance%20of%20SD-VSum%20against%20SOTA%20approaches%20for%20query-driven%20and%20generic%0A%28unimodal%20and%20multimodal%29%20summarization%20from%20the%20literature%2C%20and%20document%20its%0Acapacity%20to%20produce%20video%20summaries%20that%20are%20adapted%20to%20each%20user%27s%20needs%20about%0Atheir%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03319v2&entry.124074799=Read"},
{"title": "Fr\u00e9chet Geodesic Boosting", "author": "Yidong Zhou and Su I Iao and Hans-Georg M\u00fcller", "abstract": "  Gradient boosting has become a cornerstone of machine learning, enabling base\nlearners such as decision trees to achieve exceptional predictive performance.\nWhile existing algorithms primarily handle scalar or Euclidean outputs,\nincreasingly prevalent complex-structured data, such as distributions,\nnetworks, and manifold-valued outputs, present challenges for traditional\nmethods. Such non-Euclidean data lack algebraic structures such as addition,\nsubtraction, or scalar multiplication required by standard gradient boosting\nframeworks. To address these challenges, we introduce Fr\\'echet geodesic\nboosting (FGBoost), a novel approach tailored for outputs residing in geodesic\nmetric spaces. FGBoost leverages geodesics as proxies for residuals and\nconstructs ensembles in a way that respects the intrinsic geometry of the\noutput space. Through theoretical analysis, extensive simulations, and\nreal-world applications, we demonstrate the strong performance and adaptability\nof FGBoost, showcasing its potential for modeling complex data.\n", "link": "http://arxiv.org/abs/2509.18013v1", "date": "2025-09-22", "relevancy": 2.619, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5441}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5289}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fr%C3%A9chet%20Geodesic%20Boosting&body=Title%3A%20Fr%C3%A9chet%20Geodesic%20Boosting%0AAuthor%3A%20Yidong%20Zhou%20and%20Su%20I%20Iao%20and%20Hans-Georg%20M%C3%BCller%0AAbstract%3A%20%20%20Gradient%20boosting%20has%20become%20a%20cornerstone%20of%20machine%20learning%2C%20enabling%20base%0Alearners%20such%20as%20decision%20trees%20to%20achieve%20exceptional%20predictive%20performance.%0AWhile%20existing%20algorithms%20primarily%20handle%20scalar%20or%20Euclidean%20outputs%2C%0Aincreasingly%20prevalent%20complex-structured%20data%2C%20such%20as%20distributions%2C%0Anetworks%2C%20and%20manifold-valued%20outputs%2C%20present%20challenges%20for%20traditional%0Amethods.%20Such%20non-Euclidean%20data%20lack%20algebraic%20structures%20such%20as%20addition%2C%0Asubtraction%2C%20or%20scalar%20multiplication%20required%20by%20standard%20gradient%20boosting%0Aframeworks.%20To%20address%20these%20challenges%2C%20we%20introduce%20Fr%5C%27echet%20geodesic%0Aboosting%20%28FGBoost%29%2C%20a%20novel%20approach%20tailored%20for%20outputs%20residing%20in%20geodesic%0Ametric%20spaces.%20FGBoost%20leverages%20geodesics%20as%20proxies%20for%20residuals%20and%0Aconstructs%20ensembles%20in%20a%20way%20that%20respects%20the%20intrinsic%20geometry%20of%20the%0Aoutput%20space.%20Through%20theoretical%20analysis%2C%20extensive%20simulations%2C%20and%0Areal-world%20applications%2C%20we%20demonstrate%20the%20strong%20performance%20and%20adaptability%0Aof%20FGBoost%2C%20showcasing%20its%20potential%20for%20modeling%20complex%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFr%25C3%25A9chet%2520Geodesic%2520Boosting%26entry.906535625%3DYidong%2520Zhou%2520and%2520Su%2520I%2520Iao%2520and%2520Hans-Georg%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520Gradient%2520boosting%2520has%2520become%2520a%2520cornerstone%2520of%2520machine%2520learning%252C%2520enabling%2520base%250Alearners%2520such%2520as%2520decision%2520trees%2520to%2520achieve%2520exceptional%2520predictive%2520performance.%250AWhile%2520existing%2520algorithms%2520primarily%2520handle%2520scalar%2520or%2520Euclidean%2520outputs%252C%250Aincreasingly%2520prevalent%2520complex-structured%2520data%252C%2520such%2520as%2520distributions%252C%250Anetworks%252C%2520and%2520manifold-valued%2520outputs%252C%2520present%2520challenges%2520for%2520traditional%250Amethods.%2520Such%2520non-Euclidean%2520data%2520lack%2520algebraic%2520structures%2520such%2520as%2520addition%252C%250Asubtraction%252C%2520or%2520scalar%2520multiplication%2520required%2520by%2520standard%2520gradient%2520boosting%250Aframeworks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Fr%255C%2527echet%2520geodesic%250Aboosting%2520%2528FGBoost%2529%252C%2520a%2520novel%2520approach%2520tailored%2520for%2520outputs%2520residing%2520in%2520geodesic%250Ametric%2520spaces.%2520FGBoost%2520leverages%2520geodesics%2520as%2520proxies%2520for%2520residuals%2520and%250Aconstructs%2520ensembles%2520in%2520a%2520way%2520that%2520respects%2520the%2520intrinsic%2520geometry%2520of%2520the%250Aoutput%2520space.%2520Through%2520theoretical%2520analysis%252C%2520extensive%2520simulations%252C%2520and%250Areal-world%2520applications%252C%2520we%2520demonstrate%2520the%2520strong%2520performance%2520and%2520adaptability%250Aof%2520FGBoost%252C%2520showcasing%2520its%2520potential%2520for%2520modeling%2520complex%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fr%C3%A9chet%20Geodesic%20Boosting&entry.906535625=Yidong%20Zhou%20and%20Su%20I%20Iao%20and%20Hans-Georg%20M%C3%BCller&entry.1292438233=%20%20Gradient%20boosting%20has%20become%20a%20cornerstone%20of%20machine%20learning%2C%20enabling%20base%0Alearners%20such%20as%20decision%20trees%20to%20achieve%20exceptional%20predictive%20performance.%0AWhile%20existing%20algorithms%20primarily%20handle%20scalar%20or%20Euclidean%20outputs%2C%0Aincreasingly%20prevalent%20complex-structured%20data%2C%20such%20as%20distributions%2C%0Anetworks%2C%20and%20manifold-valued%20outputs%2C%20present%20challenges%20for%20traditional%0Amethods.%20Such%20non-Euclidean%20data%20lack%20algebraic%20structures%20such%20as%20addition%2C%0Asubtraction%2C%20or%20scalar%20multiplication%20required%20by%20standard%20gradient%20boosting%0Aframeworks.%20To%20address%20these%20challenges%2C%20we%20introduce%20Fr%5C%27echet%20geodesic%0Aboosting%20%28FGBoost%29%2C%20a%20novel%20approach%20tailored%20for%20outputs%20residing%20in%20geodesic%0Ametric%20spaces.%20FGBoost%20leverages%20geodesics%20as%20proxies%20for%20residuals%20and%0Aconstructs%20ensembles%20in%20a%20way%20that%20respects%20the%20intrinsic%20geometry%20of%20the%0Aoutput%20space.%20Through%20theoretical%20analysis%2C%20extensive%20simulations%2C%20and%0Areal-world%20applications%2C%20we%20demonstrate%20the%20strong%20performance%20and%20adaptability%0Aof%20FGBoost%2C%20showcasing%20its%20potential%20for%20modeling%20complex%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18013v1&entry.124074799=Read"},
{"title": "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain\n  Brain Tumor Segmentation in MRI", "author": "Yuanhan Wang and Yifei Chen and Shuo Jiang and Wenjing Yu and Mingxuan Liu and Beining Wu and Jinying Zong and Feiwei Qin and Changmiao Wang and Qiyuan Tian", "abstract": "  Reliable brain tumor segmentation in MRI is indispensable for treatment\nplanning and outcome monitoring, yet models trained on curated benchmarks often\nfail under domain shifts arising from scanner and protocol variability as well\nas population heterogeneity. Such gaps are especially severe in low-resource\nand pediatric cohorts, where conventional test-time or source-free adaptation\nstrategies often suffer from instability and structural inconsistency. We\npropose SmaRT, a style-modulated robust test-time adaptation framework that\nenables source-free cross-domain generalization. SmaRT integrates style-aware\naugmentation to mitigate appearance discrepancies, a dual-branch momentum\nstrategy for stable pseudo-label refinement, and structural priors enforcing\nconsistency, integrity, and connectivity. This synergy ensures both adaptation\nstability and anatomical fidelity under extreme domain shifts. Extensive\nevaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT\nconsistently outperforms state-of-the-art methods, with notable gains in Dice\naccuracy and boundary precision. Overall, SmaRT bridges the gap between\nalgorithmic advances and equitable clinical applicability, supporting robust\ndeployment of MRI-based neuro-oncology tools in diverse clinical environments.\nOur source code is available at https://github.com/baiyou1234/SmaRT.\n", "link": "http://arxiv.org/abs/2509.17925v1", "date": "2025-09-22", "relevancy": 2.6182, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5576}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5102}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SmaRT%3A%20Style-Modulated%20Robust%20Test-Time%20Adaptation%20for%20Cross-Domain%0A%20%20Brain%20Tumor%20Segmentation%20in%20MRI&body=Title%3A%20SmaRT%3A%20Style-Modulated%20Robust%20Test-Time%20Adaptation%20for%20Cross-Domain%0A%20%20Brain%20Tumor%20Segmentation%20in%20MRI%0AAuthor%3A%20Yuanhan%20Wang%20and%20Yifei%20Chen%20and%20Shuo%20Jiang%20and%20Wenjing%20Yu%20and%20Mingxuan%20Liu%20and%20Beining%20Wu%20and%20Jinying%20Zong%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%20and%20Qiyuan%20Tian%0AAbstract%3A%20%20%20Reliable%20brain%20tumor%20segmentation%20in%20MRI%20is%20indispensable%20for%20treatment%0Aplanning%20and%20outcome%20monitoring%2C%20yet%20models%20trained%20on%20curated%20benchmarks%20often%0Afail%20under%20domain%20shifts%20arising%20from%20scanner%20and%20protocol%20variability%20as%20well%0Aas%20population%20heterogeneity.%20Such%20gaps%20are%20especially%20severe%20in%20low-resource%0Aand%20pediatric%20cohorts%2C%20where%20conventional%20test-time%20or%20source-free%20adaptation%0Astrategies%20often%20suffer%20from%20instability%20and%20structural%20inconsistency.%20We%0Apropose%20SmaRT%2C%20a%20style-modulated%20robust%20test-time%20adaptation%20framework%20that%0Aenables%20source-free%20cross-domain%20generalization.%20SmaRT%20integrates%20style-aware%0Aaugmentation%20to%20mitigate%20appearance%20discrepancies%2C%20a%20dual-branch%20momentum%0Astrategy%20for%20stable%20pseudo-label%20refinement%2C%20and%20structural%20priors%20enforcing%0Aconsistency%2C%20integrity%2C%20and%20connectivity.%20This%20synergy%20ensures%20both%20adaptation%0Astability%20and%20anatomical%20fidelity%20under%20extreme%20domain%20shifts.%20Extensive%0Aevaluations%20on%20sub-Saharan%20Africa%20and%20pediatric%20glioma%20datasets%20show%20that%20SmaRT%0Aconsistently%20outperforms%20state-of-the-art%20methods%2C%20with%20notable%20gains%20in%20Dice%0Aaccuracy%20and%20boundary%20precision.%20Overall%2C%20SmaRT%20bridges%20the%20gap%20between%0Aalgorithmic%20advances%20and%20equitable%20clinical%20applicability%2C%20supporting%20robust%0Adeployment%20of%20MRI-based%20neuro-oncology%20tools%20in%20diverse%20clinical%20environments.%0AOur%20source%20code%20is%20available%20at%20https%3A//github.com/baiyou1234/SmaRT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmaRT%253A%2520Style-Modulated%2520Robust%2520Test-Time%2520Adaptation%2520for%2520Cross-Domain%250A%2520%2520Brain%2520Tumor%2520Segmentation%2520in%2520MRI%26entry.906535625%3DYuanhan%2520Wang%2520and%2520Yifei%2520Chen%2520and%2520Shuo%2520Jiang%2520and%2520Wenjing%2520Yu%2520and%2520Mingxuan%2520Liu%2520and%2520Beining%2520Wu%2520and%2520Jinying%2520Zong%2520and%2520Feiwei%2520Qin%2520and%2520Changmiao%2520Wang%2520and%2520Qiyuan%2520Tian%26entry.1292438233%3D%2520%2520Reliable%2520brain%2520tumor%2520segmentation%2520in%2520MRI%2520is%2520indispensable%2520for%2520treatment%250Aplanning%2520and%2520outcome%2520monitoring%252C%2520yet%2520models%2520trained%2520on%2520curated%2520benchmarks%2520often%250Afail%2520under%2520domain%2520shifts%2520arising%2520from%2520scanner%2520and%2520protocol%2520variability%2520as%2520well%250Aas%2520population%2520heterogeneity.%2520Such%2520gaps%2520are%2520especially%2520severe%2520in%2520low-resource%250Aand%2520pediatric%2520cohorts%252C%2520where%2520conventional%2520test-time%2520or%2520source-free%2520adaptation%250Astrategies%2520often%2520suffer%2520from%2520instability%2520and%2520structural%2520inconsistency.%2520We%250Apropose%2520SmaRT%252C%2520a%2520style-modulated%2520robust%2520test-time%2520adaptation%2520framework%2520that%250Aenables%2520source-free%2520cross-domain%2520generalization.%2520SmaRT%2520integrates%2520style-aware%250Aaugmentation%2520to%2520mitigate%2520appearance%2520discrepancies%252C%2520a%2520dual-branch%2520momentum%250Astrategy%2520for%2520stable%2520pseudo-label%2520refinement%252C%2520and%2520structural%2520priors%2520enforcing%250Aconsistency%252C%2520integrity%252C%2520and%2520connectivity.%2520This%2520synergy%2520ensures%2520both%2520adaptation%250Astability%2520and%2520anatomical%2520fidelity%2520under%2520extreme%2520domain%2520shifts.%2520Extensive%250Aevaluations%2520on%2520sub-Saharan%2520Africa%2520and%2520pediatric%2520glioma%2520datasets%2520show%2520that%2520SmaRT%250Aconsistently%2520outperforms%2520state-of-the-art%2520methods%252C%2520with%2520notable%2520gains%2520in%2520Dice%250Aaccuracy%2520and%2520boundary%2520precision.%2520Overall%252C%2520SmaRT%2520bridges%2520the%2520gap%2520between%250Aalgorithmic%2520advances%2520and%2520equitable%2520clinical%2520applicability%252C%2520supporting%2520robust%250Adeployment%2520of%2520MRI-based%2520neuro-oncology%2520tools%2520in%2520diverse%2520clinical%2520environments.%250AOur%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/baiyou1234/SmaRT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SmaRT%3A%20Style-Modulated%20Robust%20Test-Time%20Adaptation%20for%20Cross-Domain%0A%20%20Brain%20Tumor%20Segmentation%20in%20MRI&entry.906535625=Yuanhan%20Wang%20and%20Yifei%20Chen%20and%20Shuo%20Jiang%20and%20Wenjing%20Yu%20and%20Mingxuan%20Liu%20and%20Beining%20Wu%20and%20Jinying%20Zong%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%20and%20Qiyuan%20Tian&entry.1292438233=%20%20Reliable%20brain%20tumor%20segmentation%20in%20MRI%20is%20indispensable%20for%20treatment%0Aplanning%20and%20outcome%20monitoring%2C%20yet%20models%20trained%20on%20curated%20benchmarks%20often%0Afail%20under%20domain%20shifts%20arising%20from%20scanner%20and%20protocol%20variability%20as%20well%0Aas%20population%20heterogeneity.%20Such%20gaps%20are%20especially%20severe%20in%20low-resource%0Aand%20pediatric%20cohorts%2C%20where%20conventional%20test-time%20or%20source-free%20adaptation%0Astrategies%20often%20suffer%20from%20instability%20and%20structural%20inconsistency.%20We%0Apropose%20SmaRT%2C%20a%20style-modulated%20robust%20test-time%20adaptation%20framework%20that%0Aenables%20source-free%20cross-domain%20generalization.%20SmaRT%20integrates%20style-aware%0Aaugmentation%20to%20mitigate%20appearance%20discrepancies%2C%20a%20dual-branch%20momentum%0Astrategy%20for%20stable%20pseudo-label%20refinement%2C%20and%20structural%20priors%20enforcing%0Aconsistency%2C%20integrity%2C%20and%20connectivity.%20This%20synergy%20ensures%20both%20adaptation%0Astability%20and%20anatomical%20fidelity%20under%20extreme%20domain%20shifts.%20Extensive%0Aevaluations%20on%20sub-Saharan%20Africa%20and%20pediatric%20glioma%20datasets%20show%20that%20SmaRT%0Aconsistently%20outperforms%20state-of-the-art%20methods%2C%20with%20notable%20gains%20in%20Dice%0Aaccuracy%20and%20boundary%20precision.%20Overall%2C%20SmaRT%20bridges%20the%20gap%20between%0Aalgorithmic%20advances%20and%20equitable%20clinical%20applicability%2C%20supporting%20robust%0Adeployment%20of%20MRI-based%20neuro-oncology%20tools%20in%20diverse%20clinical%20environments.%0AOur%20source%20code%20is%20available%20at%20https%3A//github.com/baiyou1234/SmaRT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17925v1&entry.124074799=Read"},
{"title": "I2VWM: Robust Watermarking for Image to Video Generation", "author": "Guanjie Wang and Zehua Ma and Han Fang and Weiming Zhang", "abstract": "  The rapid progress of image-guided video generation (I2V) has raised concerns\nabout its potential misuse in misinformation and fraud, underscoring the urgent\nneed for effective digital watermarking. While existing watermarking methods\ndemonstrate robustness within a single modality, they fail to trace source\nimages in I2V settings. To address this gap, we introduce the concept of Robust\nDiffusion Distance, which measures the temporal persistence of watermark\nsignals in generated videos. Building on this, we propose I2VWM, a cross-modal\nwatermarking framework designed to enhance watermark robustness across time.\nI2VWM leverages a video-simulation noise layer during training and employs an\noptical-flow-based alignment module during inference. Experiments on both\nopen-source and commercial I2V models demonstrate that I2VWM significantly\nimproves robustness while maintaining imperceptibility, establishing a new\nparadigm for cross-modal watermarking in the era of generative video.\n\\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code\nReleased.}\n", "link": "http://arxiv.org/abs/2509.17773v1", "date": "2025-09-22", "relevancy": 2.6148, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6679}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6636}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I2VWM%3A%20Robust%20Watermarking%20for%20Image%20to%20Video%20Generation&body=Title%3A%20I2VWM%3A%20Robust%20Watermarking%20for%20Image%20to%20Video%20Generation%0AAuthor%3A%20Guanjie%20Wang%20and%20Zehua%20Ma%20and%20Han%20Fang%20and%20Weiming%20Zhang%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20image-guided%20video%20generation%20%28I2V%29%20has%20raised%20concerns%0Aabout%20its%20potential%20misuse%20in%20misinformation%20and%20fraud%2C%20underscoring%20the%20urgent%0Aneed%20for%20effective%20digital%20watermarking.%20While%20existing%20watermarking%20methods%0Ademonstrate%20robustness%20within%20a%20single%20modality%2C%20they%20fail%20to%20trace%20source%0Aimages%20in%20I2V%20settings.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20concept%20of%20Robust%0ADiffusion%20Distance%2C%20which%20measures%20the%20temporal%20persistence%20of%20watermark%0Asignals%20in%20generated%20videos.%20Building%20on%20this%2C%20we%20propose%20I2VWM%2C%20a%20cross-modal%0Awatermarking%20framework%20designed%20to%20enhance%20watermark%20robustness%20across%20time.%0AI2VWM%20leverages%20a%20video-simulation%20noise%20layer%20during%20training%20and%20employs%20an%0Aoptical-flow-based%20alignment%20module%20during%20inference.%20Experiments%20on%20both%0Aopen-source%20and%20commercial%20I2V%20models%20demonstrate%20that%20I2VWM%20significantly%0Aimproves%20robustness%20while%20maintaining%20imperceptibility%2C%20establishing%20a%20new%0Aparadigm%20for%20cross-modal%20watermarking%20in%20the%20era%20of%20generative%20video.%0A%5Chref%7Bhttps%3A//github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation%7D%7BCode%0AReleased.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI2VWM%253A%2520Robust%2520Watermarking%2520for%2520Image%2520to%2520Video%2520Generation%26entry.906535625%3DGuanjie%2520Wang%2520and%2520Zehua%2520Ma%2520and%2520Han%2520Fang%2520and%2520Weiming%2520Zhang%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520image-guided%2520video%2520generation%2520%2528I2V%2529%2520has%2520raised%2520concerns%250Aabout%2520its%2520potential%2520misuse%2520in%2520misinformation%2520and%2520fraud%252C%2520underscoring%2520the%2520urgent%250Aneed%2520for%2520effective%2520digital%2520watermarking.%2520While%2520existing%2520watermarking%2520methods%250Ademonstrate%2520robustness%2520within%2520a%2520single%2520modality%252C%2520they%2520fail%2520to%2520trace%2520source%250Aimages%2520in%2520I2V%2520settings.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520concept%2520of%2520Robust%250ADiffusion%2520Distance%252C%2520which%2520measures%2520the%2520temporal%2520persistence%2520of%2520watermark%250Asignals%2520in%2520generated%2520videos.%2520Building%2520on%2520this%252C%2520we%2520propose%2520I2VWM%252C%2520a%2520cross-modal%250Awatermarking%2520framework%2520designed%2520to%2520enhance%2520watermark%2520robustness%2520across%2520time.%250AI2VWM%2520leverages%2520a%2520video-simulation%2520noise%2520layer%2520during%2520training%2520and%2520employs%2520an%250Aoptical-flow-based%2520alignment%2520module%2520during%2520inference.%2520Experiments%2520on%2520both%250Aopen-source%2520and%2520commercial%2520I2V%2520models%2520demonstrate%2520that%2520I2VWM%2520significantly%250Aimproves%2520robustness%2520while%2520maintaining%2520imperceptibility%252C%2520establishing%2520a%2520new%250Aparadigm%2520for%2520cross-modal%2520watermarking%2520in%2520the%2520era%2520of%2520generative%2520video.%250A%255Chref%257Bhttps%253A//github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation%257D%257BCode%250AReleased.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2VWM%3A%20Robust%20Watermarking%20for%20Image%20to%20Video%20Generation&entry.906535625=Guanjie%20Wang%20and%20Zehua%20Ma%20and%20Han%20Fang%20and%20Weiming%20Zhang&entry.1292438233=%20%20The%20rapid%20progress%20of%20image-guided%20video%20generation%20%28I2V%29%20has%20raised%20concerns%0Aabout%20its%20potential%20misuse%20in%20misinformation%20and%20fraud%2C%20underscoring%20the%20urgent%0Aneed%20for%20effective%20digital%20watermarking.%20While%20existing%20watermarking%20methods%0Ademonstrate%20robustness%20within%20a%20single%20modality%2C%20they%20fail%20to%20trace%20source%0Aimages%20in%20I2V%20settings.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20concept%20of%20Robust%0ADiffusion%20Distance%2C%20which%20measures%20the%20temporal%20persistence%20of%20watermark%0Asignals%20in%20generated%20videos.%20Building%20on%20this%2C%20we%20propose%20I2VWM%2C%20a%20cross-modal%0Awatermarking%20framework%20designed%20to%20enhance%20watermark%20robustness%20across%20time.%0AI2VWM%20leverages%20a%20video-simulation%20noise%20layer%20during%20training%20and%20employs%20an%0Aoptical-flow-based%20alignment%20module%20during%20inference.%20Experiments%20on%20both%0Aopen-source%20and%20commercial%20I2V%20models%20demonstrate%20that%20I2VWM%20significantly%0Aimproves%20robustness%20while%20maintaining%20imperceptibility%2C%20establishing%20a%20new%0Aparadigm%20for%20cross-modal%20watermarking%20in%20the%20era%20of%20generative%20video.%0A%5Chref%7Bhttps%3A//github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation%7D%7BCode%0AReleased.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17773v1&entry.124074799=Read"},
{"title": "Fresh in memory: Training-order recency is linearly encoded in language\n  model activations", "author": "Dmitrii Krasheninnikov and Richard E. Turner and David Krueger", "abstract": "  We show that language models' activations linearly encode when information\nwas learned during training. Our setup involves creating a model with a known\ntraining order by sequentially fine-tuning Llama-3.2-1B on six disjoint but\notherwise similar datasets about named entities. We find that the average\nactivations of test samples corresponding to the six training datasets encode\nthe training order: when projected into a 2D subspace, these centroids are\narranged exactly in the order of training and lie on a straight line. Further,\nwe show that linear probes can accurately (~90%) distinguish \"early\" vs. \"late\"\nentities, generalizing to entities unseen during the probes' own training. The\nmodel can also be fine-tuned to explicitly report an unseen entity's training\nstage (~80% accuracy). Interestingly, the training-order encoding does not seem\nattributable to simple differences in activation magnitudes, losses, or model\nconfidence. Our paper demonstrates that models are capable of differentiating\ninformation by its acquisition time, and carries significant implications for\nhow they might manage conflicting data and respond to knowledge modifications.\n", "link": "http://arxiv.org/abs/2509.14223v2", "date": "2025-09-22", "relevancy": 2.588, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fresh%20in%20memory%3A%20Training-order%20recency%20is%20linearly%20encoded%20in%20language%0A%20%20model%20activations&body=Title%3A%20Fresh%20in%20memory%3A%20Training-order%20recency%20is%20linearly%20encoded%20in%20language%0A%20%20model%20activations%0AAuthor%3A%20Dmitrii%20Krasheninnikov%20and%20Richard%20E.%20Turner%20and%20David%20Krueger%0AAbstract%3A%20%20%20We%20show%20that%20language%20models%27%20activations%20linearly%20encode%20when%20information%0Awas%20learned%20during%20training.%20Our%20setup%20involves%20creating%20a%20model%20with%20a%20known%0Atraining%20order%20by%20sequentially%20fine-tuning%20Llama-3.2-1B%20on%20six%20disjoint%20but%0Aotherwise%20similar%20datasets%20about%20named%20entities.%20We%20find%20that%20the%20average%0Aactivations%20of%20test%20samples%20corresponding%20to%20the%20six%20training%20datasets%20encode%0Athe%20training%20order%3A%20when%20projected%20into%20a%202D%20subspace%2C%20these%20centroids%20are%0Aarranged%20exactly%20in%20the%20order%20of%20training%20and%20lie%20on%20a%20straight%20line.%20Further%2C%0Awe%20show%20that%20linear%20probes%20can%20accurately%20%28~90%25%29%20distinguish%20%22early%22%20vs.%20%22late%22%0Aentities%2C%20generalizing%20to%20entities%20unseen%20during%20the%20probes%27%20own%20training.%20The%0Amodel%20can%20also%20be%20fine-tuned%20to%20explicitly%20report%20an%20unseen%20entity%27s%20training%0Astage%20%28~80%25%20accuracy%29.%20Interestingly%2C%20the%20training-order%20encoding%20does%20not%20seem%0Aattributable%20to%20simple%20differences%20in%20activation%20magnitudes%2C%20losses%2C%20or%20model%0Aconfidence.%20Our%20paper%20demonstrates%20that%20models%20are%20capable%20of%20differentiating%0Ainformation%20by%20its%20acquisition%20time%2C%20and%20carries%20significant%20implications%20for%0Ahow%20they%20might%20manage%20conflicting%20data%20and%20respond%20to%20knowledge%20modifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14223v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFresh%2520in%2520memory%253A%2520Training-order%2520recency%2520is%2520linearly%2520encoded%2520in%2520language%250A%2520%2520model%2520activations%26entry.906535625%3DDmitrii%2520Krasheninnikov%2520and%2520Richard%2520E.%2520Turner%2520and%2520David%2520Krueger%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520language%2520models%2527%2520activations%2520linearly%2520encode%2520when%2520information%250Awas%2520learned%2520during%2520training.%2520Our%2520setup%2520involves%2520creating%2520a%2520model%2520with%2520a%2520known%250Atraining%2520order%2520by%2520sequentially%2520fine-tuning%2520Llama-3.2-1B%2520on%2520six%2520disjoint%2520but%250Aotherwise%2520similar%2520datasets%2520about%2520named%2520entities.%2520We%2520find%2520that%2520the%2520average%250Aactivations%2520of%2520test%2520samples%2520corresponding%2520to%2520the%2520six%2520training%2520datasets%2520encode%250Athe%2520training%2520order%253A%2520when%2520projected%2520into%2520a%25202D%2520subspace%252C%2520these%2520centroids%2520are%250Aarranged%2520exactly%2520in%2520the%2520order%2520of%2520training%2520and%2520lie%2520on%2520a%2520straight%2520line.%2520Further%252C%250Awe%2520show%2520that%2520linear%2520probes%2520can%2520accurately%2520%2528~90%2525%2529%2520distinguish%2520%2522early%2522%2520vs.%2520%2522late%2522%250Aentities%252C%2520generalizing%2520to%2520entities%2520unseen%2520during%2520the%2520probes%2527%2520own%2520training.%2520The%250Amodel%2520can%2520also%2520be%2520fine-tuned%2520to%2520explicitly%2520report%2520an%2520unseen%2520entity%2527s%2520training%250Astage%2520%2528~80%2525%2520accuracy%2529.%2520Interestingly%252C%2520the%2520training-order%2520encoding%2520does%2520not%2520seem%250Aattributable%2520to%2520simple%2520differences%2520in%2520activation%2520magnitudes%252C%2520losses%252C%2520or%2520model%250Aconfidence.%2520Our%2520paper%2520demonstrates%2520that%2520models%2520are%2520capable%2520of%2520differentiating%250Ainformation%2520by%2520its%2520acquisition%2520time%252C%2520and%2520carries%2520significant%2520implications%2520for%250Ahow%2520they%2520might%2520manage%2520conflicting%2520data%2520and%2520respond%2520to%2520knowledge%2520modifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14223v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fresh%20in%20memory%3A%20Training-order%20recency%20is%20linearly%20encoded%20in%20language%0A%20%20model%20activations&entry.906535625=Dmitrii%20Krasheninnikov%20and%20Richard%20E.%20Turner%20and%20David%20Krueger&entry.1292438233=%20%20We%20show%20that%20language%20models%27%20activations%20linearly%20encode%20when%20information%0Awas%20learned%20during%20training.%20Our%20setup%20involves%20creating%20a%20model%20with%20a%20known%0Atraining%20order%20by%20sequentially%20fine-tuning%20Llama-3.2-1B%20on%20six%20disjoint%20but%0Aotherwise%20similar%20datasets%20about%20named%20entities.%20We%20find%20that%20the%20average%0Aactivations%20of%20test%20samples%20corresponding%20to%20the%20six%20training%20datasets%20encode%0Athe%20training%20order%3A%20when%20projected%20into%20a%202D%20subspace%2C%20these%20centroids%20are%0Aarranged%20exactly%20in%20the%20order%20of%20training%20and%20lie%20on%20a%20straight%20line.%20Further%2C%0Awe%20show%20that%20linear%20probes%20can%20accurately%20%28~90%25%29%20distinguish%20%22early%22%20vs.%20%22late%22%0Aentities%2C%20generalizing%20to%20entities%20unseen%20during%20the%20probes%27%20own%20training.%20The%0Amodel%20can%20also%20be%20fine-tuned%20to%20explicitly%20report%20an%20unseen%20entity%27s%20training%0Astage%20%28~80%25%20accuracy%29.%20Interestingly%2C%20the%20training-order%20encoding%20does%20not%20seem%0Aattributable%20to%20simple%20differences%20in%20activation%20magnitudes%2C%20losses%2C%20or%20model%0Aconfidence.%20Our%20paper%20demonstrates%20that%20models%20are%20capable%20of%20differentiating%0Ainformation%20by%20its%20acquisition%20time%2C%20and%20carries%20significant%20implications%20for%0Ahow%20they%20might%20manage%20conflicting%20data%20and%20respond%20to%20knowledge%20modifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14223v2&entry.124074799=Read"},
{"title": "Single-step Diffusion for Image Compression at Ultra-Low Bitrates", "author": "Chanung Park and Joo Chan Lee and Jong Hwan Ko", "abstract": "  Although there have been significant advancements in image compression\ntechniques, such as standard and learned codecs, these methods still suffer\nfrom severe quality degradation at extremely low bits per pixel. While recent\ndiffusion-based models provided enhanced generative performance at low\nbitrates, they often yields limited perceptual quality and prohibitive decoding\nlatency due to multiple denoising steps. In this paper, we propose the\nsingle-step diffusion model for image compression that delivers high perceptual\nquality and fast decoding at ultra-low bitrates. Our approach incorporates two\nkey innovations: (i) Vector-Quantized Residual (VQ-Residual) training, which\nfactorizes a structural base code and a learned residual in latent space,\ncapturing both global geometry and high-frequency details; and (ii) rate-aware\nnoise modulation, which tunes denoising strength to match the desired bitrate.\nExtensive experiments show that ours achieves comparable compression\nperformance to state-of-the-art methods while improving decoding speed by about\n50x compared to prior diffusion-based methods, greatly enhancing the\npracticality of generative codecs.\n", "link": "http://arxiv.org/abs/2506.16572v2", "date": "2025-09-22", "relevancy": 2.5788, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7132}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6317}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-step%20Diffusion%20for%20Image%20Compression%20at%20Ultra-Low%20Bitrates&body=Title%3A%20Single-step%20Diffusion%20for%20Image%20Compression%20at%20Ultra-Low%20Bitrates%0AAuthor%3A%20Chanung%20Park%20and%20Joo%20Chan%20Lee%20and%20Jong%20Hwan%20Ko%0AAbstract%3A%20%20%20Although%20there%20have%20been%20significant%20advancements%20in%20image%20compression%0Atechniques%2C%20such%20as%20standard%20and%20learned%20codecs%2C%20these%20methods%20still%20suffer%0Afrom%20severe%20quality%20degradation%20at%20extremely%20low%20bits%20per%20pixel.%20While%20recent%0Adiffusion-based%20models%20provided%20enhanced%20generative%20performance%20at%20low%0Abitrates%2C%20they%20often%20yields%20limited%20perceptual%20quality%20and%20prohibitive%20decoding%0Alatency%20due%20to%20multiple%20denoising%20steps.%20In%20this%20paper%2C%20we%20propose%20the%0Asingle-step%20diffusion%20model%20for%20image%20compression%20that%20delivers%20high%20perceptual%0Aquality%20and%20fast%20decoding%20at%20ultra-low%20bitrates.%20Our%20approach%20incorporates%20two%0Akey%20innovations%3A%20%28i%29%20Vector-Quantized%20Residual%20%28VQ-Residual%29%20training%2C%20which%0Afactorizes%20a%20structural%20base%20code%20and%20a%20learned%20residual%20in%20latent%20space%2C%0Acapturing%20both%20global%20geometry%20and%20high-frequency%20details%3B%20and%20%28ii%29%20rate-aware%0Anoise%20modulation%2C%20which%20tunes%20denoising%20strength%20to%20match%20the%20desired%20bitrate.%0AExtensive%20experiments%20show%20that%20ours%20achieves%20comparable%20compression%0Aperformance%20to%20state-of-the-art%20methods%20while%20improving%20decoding%20speed%20by%20about%0A50x%20compared%20to%20prior%20diffusion-based%20methods%2C%20greatly%20enhancing%20the%0Apracticality%20of%20generative%20codecs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-step%2520Diffusion%2520for%2520Image%2520Compression%2520at%2520Ultra-Low%2520Bitrates%26entry.906535625%3DChanung%2520Park%2520and%2520Joo%2520Chan%2520Lee%2520and%2520Jong%2520Hwan%2520Ko%26entry.1292438233%3D%2520%2520Although%2520there%2520have%2520been%2520significant%2520advancements%2520in%2520image%2520compression%250Atechniques%252C%2520such%2520as%2520standard%2520and%2520learned%2520codecs%252C%2520these%2520methods%2520still%2520suffer%250Afrom%2520severe%2520quality%2520degradation%2520at%2520extremely%2520low%2520bits%2520per%2520pixel.%2520While%2520recent%250Adiffusion-based%2520models%2520provided%2520enhanced%2520generative%2520performance%2520at%2520low%250Abitrates%252C%2520they%2520often%2520yields%2520limited%2520perceptual%2520quality%2520and%2520prohibitive%2520decoding%250Alatency%2520due%2520to%2520multiple%2520denoising%2520steps.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250Asingle-step%2520diffusion%2520model%2520for%2520image%2520compression%2520that%2520delivers%2520high%2520perceptual%250Aquality%2520and%2520fast%2520decoding%2520at%2520ultra-low%2520bitrates.%2520Our%2520approach%2520incorporates%2520two%250Akey%2520innovations%253A%2520%2528i%2529%2520Vector-Quantized%2520Residual%2520%2528VQ-Residual%2529%2520training%252C%2520which%250Afactorizes%2520a%2520structural%2520base%2520code%2520and%2520a%2520learned%2520residual%2520in%2520latent%2520space%252C%250Acapturing%2520both%2520global%2520geometry%2520and%2520high-frequency%2520details%253B%2520and%2520%2528ii%2529%2520rate-aware%250Anoise%2520modulation%252C%2520which%2520tunes%2520denoising%2520strength%2520to%2520match%2520the%2520desired%2520bitrate.%250AExtensive%2520experiments%2520show%2520that%2520ours%2520achieves%2520comparable%2520compression%250Aperformance%2520to%2520state-of-the-art%2520methods%2520while%2520improving%2520decoding%2520speed%2520by%2520about%250A50x%2520compared%2520to%2520prior%2520diffusion-based%2520methods%252C%2520greatly%2520enhancing%2520the%250Apracticality%2520of%2520generative%2520codecs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-step%20Diffusion%20for%20Image%20Compression%20at%20Ultra-Low%20Bitrates&entry.906535625=Chanung%20Park%20and%20Joo%20Chan%20Lee%20and%20Jong%20Hwan%20Ko&entry.1292438233=%20%20Although%20there%20have%20been%20significant%20advancements%20in%20image%20compression%0Atechniques%2C%20such%20as%20standard%20and%20learned%20codecs%2C%20these%20methods%20still%20suffer%0Afrom%20severe%20quality%20degradation%20at%20extremely%20low%20bits%20per%20pixel.%20While%20recent%0Adiffusion-based%20models%20provided%20enhanced%20generative%20performance%20at%20low%0Abitrates%2C%20they%20often%20yields%20limited%20perceptual%20quality%20and%20prohibitive%20decoding%0Alatency%20due%20to%20multiple%20denoising%20steps.%20In%20this%20paper%2C%20we%20propose%20the%0Asingle-step%20diffusion%20model%20for%20image%20compression%20that%20delivers%20high%20perceptual%0Aquality%20and%20fast%20decoding%20at%20ultra-low%20bitrates.%20Our%20approach%20incorporates%20two%0Akey%20innovations%3A%20%28i%29%20Vector-Quantized%20Residual%20%28VQ-Residual%29%20training%2C%20which%0Afactorizes%20a%20structural%20base%20code%20and%20a%20learned%20residual%20in%20latent%20space%2C%0Acapturing%20both%20global%20geometry%20and%20high-frequency%20details%3B%20and%20%28ii%29%20rate-aware%0Anoise%20modulation%2C%20which%20tunes%20denoising%20strength%20to%20match%20the%20desired%20bitrate.%0AExtensive%20experiments%20show%20that%20ours%20achieves%20comparable%20compression%0Aperformance%20to%20state-of-the-art%20methods%20while%20improving%20decoding%20speed%20by%20about%0A50x%20compared%20to%20prior%20diffusion-based%20methods%2C%20greatly%20enhancing%20the%0Apracticality%20of%20generative%20codecs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16572v2&entry.124074799=Read"},
{"title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings", "author": "Stephen Zhang and Mustafa Khan and Vardan Papyan", "abstract": "  Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.\n", "link": "http://arxiv.org/abs/2502.00919v2", "date": "2025-09-22", "relevancy": 2.5756, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Sinks%3A%20A%20%27Catch%2C%20Tag%2C%20Release%27%20Mechanism%20for%20Embeddings&body=Title%3A%20Attention%20Sinks%3A%20A%20%27Catch%2C%20Tag%2C%20Release%27%20Mechanism%20for%20Embeddings%0AAuthor%3A%20Stephen%20Zhang%20and%20Mustafa%20Khan%20and%20Vardan%20Papyan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20concentrate%20their%20attention%20on%20a%20few%0Aspecific%20tokens%20referred%20to%20as%20attention%20sinks.%20Common%20examples%20include%20the%0Afirst%20token%2C%20a%20prompt-independent%20sink%2C%20and%20punctuation%20tokens%2C%20which%20are%0Aprompt-dependent.%20While%20the%20tokens%20causing%20the%20sinks%20often%20lack%20direct%20semantic%0Ameaning%2C%20the%20presence%20of%20the%20sinks%20is%20critical%20for%20model%20performance%2C%0Aparticularly%20under%20model%20compression%20and%20KV-caching.%20Despite%20their%20ubiquity%2C%0Athe%20function%2C%20semantic%20role%2C%20and%20origin%20of%20attention%20sinks%20--%20especially%20those%0Abeyond%20the%20first%20token%20--%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%20a%0Acomprehensive%20investigation%20demonstrating%20that%20attention%20sinks%3A%20catch%20a%0Asequence%20of%20tokens%2C%20tag%20them%20using%20a%20common%20direction%20in%20embedding%20space%2C%20and%0Arelease%20them%20back%20into%20the%20residual%20stream%2C%20where%20tokens%20are%20later%20retrieved%0Abased%20on%20the%20tags%20they%20have%20acquired.%20Probing%20experiments%20reveal%20these%20tags%0Acarry%20semantically%20meaningful%20information%2C%20such%20as%20the%20truth%20of%20a%20statement.%0AThese%20findings%20extend%20to%20reasoning%20models%2C%20where%20the%20mechanism%20spans%20more%20heads%0Aand%20explains%20greater%20variance%20in%20embeddings%2C%20or%20recent%20models%20with%20query-key%0Anormalization%2C%20where%20sinks%20remain%20just%20as%20prevalent.%20To%20encourage%20future%0Atheoretical%20analysis%2C%20we%20introduce%20a%20minimal%20problem%20which%20can%20be%20solved%0Athrough%20the%20%27catch%2C%20tag%2C%20release%27%20mechanism%2C%20and%20where%20it%20emerges%20through%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Sinks%253A%2520A%2520%2527Catch%252C%2520Tag%252C%2520Release%2527%2520Mechanism%2520for%2520Embeddings%26entry.906535625%3DStephen%2520Zhang%2520and%2520Mustafa%2520Khan%2520and%2520Vardan%2520Papyan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520concentrate%2520their%2520attention%2520on%2520a%2520few%250Aspecific%2520tokens%2520referred%2520to%2520as%2520attention%2520sinks.%2520Common%2520examples%2520include%2520the%250Afirst%2520token%252C%2520a%2520prompt-independent%2520sink%252C%2520and%2520punctuation%2520tokens%252C%2520which%2520are%250Aprompt-dependent.%2520While%2520the%2520tokens%2520causing%2520the%2520sinks%2520often%2520lack%2520direct%2520semantic%250Ameaning%252C%2520the%2520presence%2520of%2520the%2520sinks%2520is%2520critical%2520for%2520model%2520performance%252C%250Aparticularly%2520under%2520model%2520compression%2520and%2520KV-caching.%2520Despite%2520their%2520ubiquity%252C%250Athe%2520function%252C%2520semantic%2520role%252C%2520and%2520origin%2520of%2520attention%2520sinks%2520--%2520especially%2520those%250Abeyond%2520the%2520first%2520token%2520--%2520remain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%250Acomprehensive%2520investigation%2520demonstrating%2520that%2520attention%2520sinks%253A%2520catch%2520a%250Asequence%2520of%2520tokens%252C%2520tag%2520them%2520using%2520a%2520common%2520direction%2520in%2520embedding%2520space%252C%2520and%250Arelease%2520them%2520back%2520into%2520the%2520residual%2520stream%252C%2520where%2520tokens%2520are%2520later%2520retrieved%250Abased%2520on%2520the%2520tags%2520they%2520have%2520acquired.%2520Probing%2520experiments%2520reveal%2520these%2520tags%250Acarry%2520semantically%2520meaningful%2520information%252C%2520such%2520as%2520the%2520truth%2520of%2520a%2520statement.%250AThese%2520findings%2520extend%2520to%2520reasoning%2520models%252C%2520where%2520the%2520mechanism%2520spans%2520more%2520heads%250Aand%2520explains%2520greater%2520variance%2520in%2520embeddings%252C%2520or%2520recent%2520models%2520with%2520query-key%250Anormalization%252C%2520where%2520sinks%2520remain%2520just%2520as%2520prevalent.%2520To%2520encourage%2520future%250Atheoretical%2520analysis%252C%2520we%2520introduce%2520a%2520minimal%2520problem%2520which%2520can%2520be%2520solved%250Athrough%2520the%2520%2527catch%252C%2520tag%252C%2520release%2527%2520mechanism%252C%2520and%2520where%2520it%2520emerges%2520through%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Sinks%3A%20A%20%27Catch%2C%20Tag%2C%20Release%27%20Mechanism%20for%20Embeddings&entry.906535625=Stephen%20Zhang%20and%20Mustafa%20Khan%20and%20Vardan%20Papyan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20concentrate%20their%20attention%20on%20a%20few%0Aspecific%20tokens%20referred%20to%20as%20attention%20sinks.%20Common%20examples%20include%20the%0Afirst%20token%2C%20a%20prompt-independent%20sink%2C%20and%20punctuation%20tokens%2C%20which%20are%0Aprompt-dependent.%20While%20the%20tokens%20causing%20the%20sinks%20often%20lack%20direct%20semantic%0Ameaning%2C%20the%20presence%20of%20the%20sinks%20is%20critical%20for%20model%20performance%2C%0Aparticularly%20under%20model%20compression%20and%20KV-caching.%20Despite%20their%20ubiquity%2C%0Athe%20function%2C%20semantic%20role%2C%20and%20origin%20of%20attention%20sinks%20--%20especially%20those%0Abeyond%20the%20first%20token%20--%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%20a%0Acomprehensive%20investigation%20demonstrating%20that%20attention%20sinks%3A%20catch%20a%0Asequence%20of%20tokens%2C%20tag%20them%20using%20a%20common%20direction%20in%20embedding%20space%2C%20and%0Arelease%20them%20back%20into%20the%20residual%20stream%2C%20where%20tokens%20are%20later%20retrieved%0Abased%20on%20the%20tags%20they%20have%20acquired.%20Probing%20experiments%20reveal%20these%20tags%0Acarry%20semantically%20meaningful%20information%2C%20such%20as%20the%20truth%20of%20a%20statement.%0AThese%20findings%20extend%20to%20reasoning%20models%2C%20where%20the%20mechanism%20spans%20more%20heads%0Aand%20explains%20greater%20variance%20in%20embeddings%2C%20or%20recent%20models%20with%20query-key%0Anormalization%2C%20where%20sinks%20remain%20just%20as%20prevalent.%20To%20encourage%20future%0Atheoretical%20analysis%2C%20we%20introduce%20a%20minimal%20problem%20which%20can%20be%20solved%0Athrough%20the%20%27catch%2C%20tag%2C%20release%27%20mechanism%2C%20and%20where%20it%20emerges%20through%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00919v2&entry.124074799=Read"},
{"title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding", "author": "Ziyin Zhang and Hang Yu and Shijie Li and Peng Di and Jianguo Li and Rui Wang", "abstract": "  Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.\n", "link": "http://arxiv.org/abs/2409.04183v3", "date": "2025-09-22", "relevancy": 2.5563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding&body=Title%3A%20GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding%0AAuthor%3A%20Ziyin%20Zhang%20and%20Hang%20Yu%20and%20Shijie%20Li%20and%20Peng%20Di%20and%20Jianguo%20Li%20and%20Rui%20Wang%0AAbstract%3A%20%20%20Programming%20languages%20possess%20rich%20semantic%20information%20-%20such%20as%20data%20flow%20-%0Athat%20is%20represented%20by%20graphs%20and%20not%20available%20from%20the%20surface%20form%20of%20source%0Acode.%20Recent%20code%20language%20models%20have%20scaled%20to%20billions%20of%20parameters%2C%20but%0Amodel%20source%20code%20solely%20as%20text%20tokens%20while%20ignoring%20any%20other%20structural%0Ainformation.%20Conversely%2C%20models%20that%20do%20encode%20structural%20information%20of%20code%0Amake%20modifications%20to%20the%20Transformer%20architecture%2C%20limiting%20their%20scale%20and%0Acompatibility%20with%20pretrained%20LLMs.%20In%20this%20work%2C%20we%20take%20the%20best%20of%20both%0Aworlds%20with%20GALLa%20-%20Graph%20Aligned%20Large%20Language%20Models.%20GALLa%20utilizes%20graph%0Aneural%20networks%20and%20cross-modal%20alignment%20technologies%20to%20inject%20the%20structural%0Ainformation%20of%20code%20into%20LLMs%20as%20an%20auxiliary%20task%20during%20finetuning.%20This%0Aframework%20is%20both%20model-agnostic%20and%20task-agnostic%2C%20as%20it%20can%20be%20applied%20to%20any%0Acode%20LLM%20for%20any%20code%20downstream%20task%2C%20and%20requires%20the%20structural%20graph%20data%0Aonly%20at%20training%20time%20from%20a%20corpus%20unrelated%20to%20the%20finetuning%20data%2C%20while%0Aincurring%20no%20cost%20at%20inference%20time%20over%20the%20baseline%20LLM.%20Experiments%20on%20five%0Acode%20tasks%20with%20seven%20different%20baseline%20LLMs%20ranging%20in%20size%20from%20350M%20to%2014B%0Avalidate%20the%20effectiveness%20of%20GALLa%2C%20demonstrating%20consistent%20improvement%20over%0Athe%20baseline%2C%20even%20for%20powerful%20models%20such%20as%20LLaMA3%20and%20Qwen2.5-Coder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04183v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGALLa%253A%2520Graph%2520Aligned%2520Large%2520Language%2520Models%2520for%2520Improved%2520Source%2520Code%250A%2520%2520Understanding%26entry.906535625%3DZiyin%2520Zhang%2520and%2520Hang%2520Yu%2520and%2520Shijie%2520Li%2520and%2520Peng%2520Di%2520and%2520Jianguo%2520Li%2520and%2520Rui%2520Wang%26entry.1292438233%3D%2520%2520Programming%2520languages%2520possess%2520rich%2520semantic%2520information%2520-%2520such%2520as%2520data%2520flow%2520-%250Athat%2520is%2520represented%2520by%2520graphs%2520and%2520not%2520available%2520from%2520the%2520surface%2520form%2520of%2520source%250Acode.%2520Recent%2520code%2520language%2520models%2520have%2520scaled%2520to%2520billions%2520of%2520parameters%252C%2520but%250Amodel%2520source%2520code%2520solely%2520as%2520text%2520tokens%2520while%2520ignoring%2520any%2520other%2520structural%250Ainformation.%2520Conversely%252C%2520models%2520that%2520do%2520encode%2520structural%2520information%2520of%2520code%250Amake%2520modifications%2520to%2520the%2520Transformer%2520architecture%252C%2520limiting%2520their%2520scale%2520and%250Acompatibility%2520with%2520pretrained%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520take%2520the%2520best%2520of%2520both%250Aworlds%2520with%2520GALLa%2520-%2520Graph%2520Aligned%2520Large%2520Language%2520Models.%2520GALLa%2520utilizes%2520graph%250Aneural%2520networks%2520and%2520cross-modal%2520alignment%2520technologies%2520to%2520inject%2520the%2520structural%250Ainformation%2520of%2520code%2520into%2520LLMs%2520as%2520an%2520auxiliary%2520task%2520during%2520finetuning.%2520This%250Aframework%2520is%2520both%2520model-agnostic%2520and%2520task-agnostic%252C%2520as%2520it%2520can%2520be%2520applied%2520to%2520any%250Acode%2520LLM%2520for%2520any%2520code%2520downstream%2520task%252C%2520and%2520requires%2520the%2520structural%2520graph%2520data%250Aonly%2520at%2520training%2520time%2520from%2520a%2520corpus%2520unrelated%2520to%2520the%2520finetuning%2520data%252C%2520while%250Aincurring%2520no%2520cost%2520at%2520inference%2520time%2520over%2520the%2520baseline%2520LLM.%2520Experiments%2520on%2520five%250Acode%2520tasks%2520with%2520seven%2520different%2520baseline%2520LLMs%2520ranging%2520in%2520size%2520from%2520350M%2520to%252014B%250Avalidate%2520the%2520effectiveness%2520of%2520GALLa%252C%2520demonstrating%2520consistent%2520improvement%2520over%250Athe%2520baseline%252C%2520even%2520for%2520powerful%2520models%2520such%2520as%2520LLaMA3%2520and%2520Qwen2.5-Coder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04183v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding&entry.906535625=Ziyin%20Zhang%20and%20Hang%20Yu%20and%20Shijie%20Li%20and%20Peng%20Di%20and%20Jianguo%20Li%20and%20Rui%20Wang&entry.1292438233=%20%20Programming%20languages%20possess%20rich%20semantic%20information%20-%20such%20as%20data%20flow%20-%0Athat%20is%20represented%20by%20graphs%20and%20not%20available%20from%20the%20surface%20form%20of%20source%0Acode.%20Recent%20code%20language%20models%20have%20scaled%20to%20billions%20of%20parameters%2C%20but%0Amodel%20source%20code%20solely%20as%20text%20tokens%20while%20ignoring%20any%20other%20structural%0Ainformation.%20Conversely%2C%20models%20that%20do%20encode%20structural%20information%20of%20code%0Amake%20modifications%20to%20the%20Transformer%20architecture%2C%20limiting%20their%20scale%20and%0Acompatibility%20with%20pretrained%20LLMs.%20In%20this%20work%2C%20we%20take%20the%20best%20of%20both%0Aworlds%20with%20GALLa%20-%20Graph%20Aligned%20Large%20Language%20Models.%20GALLa%20utilizes%20graph%0Aneural%20networks%20and%20cross-modal%20alignment%20technologies%20to%20inject%20the%20structural%0Ainformation%20of%20code%20into%20LLMs%20as%20an%20auxiliary%20task%20during%20finetuning.%20This%0Aframework%20is%20both%20model-agnostic%20and%20task-agnostic%2C%20as%20it%20can%20be%20applied%20to%20any%0Acode%20LLM%20for%20any%20code%20downstream%20task%2C%20and%20requires%20the%20structural%20graph%20data%0Aonly%20at%20training%20time%20from%20a%20corpus%20unrelated%20to%20the%20finetuning%20data%2C%20while%0Aincurring%20no%20cost%20at%20inference%20time%20over%20the%20baseline%20LLM.%20Experiments%20on%20five%0Acode%20tasks%20with%20seven%20different%20baseline%20LLMs%20ranging%20in%20size%20from%20350M%20to%2014B%0Avalidate%20the%20effectiveness%20of%20GALLa%2C%20demonstrating%20consistent%20improvement%20over%0Athe%20baseline%2C%20even%20for%20powerful%20models%20such%20as%20LLaMA3%20and%20Qwen2.5-Coder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04183v3&entry.124074799=Read"},
{"title": "Quantifying Student Success with Generative AI: A Monte Carlo Simulation\n  Informed by Systematic Review", "author": "Seyma Yaman Kayadibi", "abstract": "  The exponential development of generative artificial intelligence (GenAI)\ntechnologies like ChatGPT has raised increasing curiosity about their use in\nhigher education, specifically with respect to how students view them, make use\nof them, and the implications for learning outcomes. This paper employs a\nhybrid methodological approach involving a systematic literature review and\nsimulation-based modeling to explore student perceptions of GenAI use in the\ncontext of higher education. A total of nineteen empirical articles from 2023\nthrough 2025 were selected from the PRISMA-based search targeting the Scopus\ndatabase. Synthesis of emerging patterns from the literature was achieved by\nthematic categorization. Six of these had enough quantitative information,\ni.e., item-level means and standard deviations, to permit probabilistic\nmodeling. One dataset, from the resulting subset, was itself selected as a\nrepresentative case with which to illustrate inverse-variance weighting by\nMonte Carlo simulation, by virtue of its well-designed Likert scale format and\nthematic alignment with the use of computing systems by the researcher.\n  The simulation provided a composite \"Success Score\" forecasting the strength\nof the relationship between student perceptions and learning achievements.\nFindings reveal that attitude factors concerned with usability and real-world\nusefulness are significantly better predictors of positive learning achievement\nthan affective or trust-based factors. Such an interdisciplinary perspective\nprovides a unique means of linking thematic results with predictive modelling,\nresonating with longstanding controversies about the proper use of GenAI tools\nwithin the university.\n", "link": "http://arxiv.org/abs/2507.01062v2", "date": "2025-09-22", "relevancy": 2.5487, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5243}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5185}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Student%20Success%20with%20Generative%20AI%3A%20A%20Monte%20Carlo%20Simulation%0A%20%20Informed%20by%20Systematic%20Review&body=Title%3A%20Quantifying%20Student%20Success%20with%20Generative%20AI%3A%20A%20Monte%20Carlo%20Simulation%0A%20%20Informed%20by%20Systematic%20Review%0AAuthor%3A%20Seyma%20Yaman%20Kayadibi%0AAbstract%3A%20%20%20The%20exponential%20development%20of%20generative%20artificial%20intelligence%20%28GenAI%29%0Atechnologies%20like%20ChatGPT%20has%20raised%20increasing%20curiosity%20about%20their%20use%20in%0Ahigher%20education%2C%20specifically%20with%20respect%20to%20how%20students%20view%20them%2C%20make%20use%0Aof%20them%2C%20and%20the%20implications%20for%20learning%20outcomes.%20This%20paper%20employs%20a%0Ahybrid%20methodological%20approach%20involving%20a%20systematic%20literature%20review%20and%0Asimulation-based%20modeling%20to%20explore%20student%20perceptions%20of%20GenAI%20use%20in%20the%0Acontext%20of%20higher%20education.%20A%20total%20of%20nineteen%20empirical%20articles%20from%202023%0Athrough%202025%20were%20selected%20from%20the%20PRISMA-based%20search%20targeting%20the%20Scopus%0Adatabase.%20Synthesis%20of%20emerging%20patterns%20from%20the%20literature%20was%20achieved%20by%0Athematic%20categorization.%20Six%20of%20these%20had%20enough%20quantitative%20information%2C%0Ai.e.%2C%20item-level%20means%20and%20standard%20deviations%2C%20to%20permit%20probabilistic%0Amodeling.%20One%20dataset%2C%20from%20the%20resulting%20subset%2C%20was%20itself%20selected%20as%20a%0Arepresentative%20case%20with%20which%20to%20illustrate%20inverse-variance%20weighting%20by%0AMonte%20Carlo%20simulation%2C%20by%20virtue%20of%20its%20well-designed%20Likert%20scale%20format%20and%0Athematic%20alignment%20with%20the%20use%20of%20computing%20systems%20by%20the%20researcher.%0A%20%20The%20simulation%20provided%20a%20composite%20%22Success%20Score%22%20forecasting%20the%20strength%0Aof%20the%20relationship%20between%20student%20perceptions%20and%20learning%20achievements.%0AFindings%20reveal%20that%20attitude%20factors%20concerned%20with%20usability%20and%20real-world%0Ausefulness%20are%20significantly%20better%20predictors%20of%20positive%20learning%20achievement%0Athan%20affective%20or%20trust-based%20factors.%20Such%20an%20interdisciplinary%20perspective%0Aprovides%20a%20unique%20means%20of%20linking%20thematic%20results%20with%20predictive%20modelling%2C%0Aresonating%20with%20longstanding%20controversies%20about%20the%20proper%20use%20of%20GenAI%20tools%0Awithin%20the%20university.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01062v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Student%2520Success%2520with%2520Generative%2520AI%253A%2520A%2520Monte%2520Carlo%2520Simulation%250A%2520%2520Informed%2520by%2520Systematic%2520Review%26entry.906535625%3DSeyma%2520Yaman%2520Kayadibi%26entry.1292438233%3D%2520%2520The%2520exponential%2520development%2520of%2520generative%2520artificial%2520intelligence%2520%2528GenAI%2529%250Atechnologies%2520like%2520ChatGPT%2520has%2520raised%2520increasing%2520curiosity%2520about%2520their%2520use%2520in%250Ahigher%2520education%252C%2520specifically%2520with%2520respect%2520to%2520how%2520students%2520view%2520them%252C%2520make%2520use%250Aof%2520them%252C%2520and%2520the%2520implications%2520for%2520learning%2520outcomes.%2520This%2520paper%2520employs%2520a%250Ahybrid%2520methodological%2520approach%2520involving%2520a%2520systematic%2520literature%2520review%2520and%250Asimulation-based%2520modeling%2520to%2520explore%2520student%2520perceptions%2520of%2520GenAI%2520use%2520in%2520the%250Acontext%2520of%2520higher%2520education.%2520A%2520total%2520of%2520nineteen%2520empirical%2520articles%2520from%25202023%250Athrough%25202025%2520were%2520selected%2520from%2520the%2520PRISMA-based%2520search%2520targeting%2520the%2520Scopus%250Adatabase.%2520Synthesis%2520of%2520emerging%2520patterns%2520from%2520the%2520literature%2520was%2520achieved%2520by%250Athematic%2520categorization.%2520Six%2520of%2520these%2520had%2520enough%2520quantitative%2520information%252C%250Ai.e.%252C%2520item-level%2520means%2520and%2520standard%2520deviations%252C%2520to%2520permit%2520probabilistic%250Amodeling.%2520One%2520dataset%252C%2520from%2520the%2520resulting%2520subset%252C%2520was%2520itself%2520selected%2520as%2520a%250Arepresentative%2520case%2520with%2520which%2520to%2520illustrate%2520inverse-variance%2520weighting%2520by%250AMonte%2520Carlo%2520simulation%252C%2520by%2520virtue%2520of%2520its%2520well-designed%2520Likert%2520scale%2520format%2520and%250Athematic%2520alignment%2520with%2520the%2520use%2520of%2520computing%2520systems%2520by%2520the%2520researcher.%250A%2520%2520The%2520simulation%2520provided%2520a%2520composite%2520%2522Success%2520Score%2522%2520forecasting%2520the%2520strength%250Aof%2520the%2520relationship%2520between%2520student%2520perceptions%2520and%2520learning%2520achievements.%250AFindings%2520reveal%2520that%2520attitude%2520factors%2520concerned%2520with%2520usability%2520and%2520real-world%250Ausefulness%2520are%2520significantly%2520better%2520predictors%2520of%2520positive%2520learning%2520achievement%250Athan%2520affective%2520or%2520trust-based%2520factors.%2520Such%2520an%2520interdisciplinary%2520perspective%250Aprovides%2520a%2520unique%2520means%2520of%2520linking%2520thematic%2520results%2520with%2520predictive%2520modelling%252C%250Aresonating%2520with%2520longstanding%2520controversies%2520about%2520the%2520proper%2520use%2520of%2520GenAI%2520tools%250Awithin%2520the%2520university.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01062v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Student%20Success%20with%20Generative%20AI%3A%20A%20Monte%20Carlo%20Simulation%0A%20%20Informed%20by%20Systematic%20Review&entry.906535625=Seyma%20Yaman%20Kayadibi&entry.1292438233=%20%20The%20exponential%20development%20of%20generative%20artificial%20intelligence%20%28GenAI%29%0Atechnologies%20like%20ChatGPT%20has%20raised%20increasing%20curiosity%20about%20their%20use%20in%0Ahigher%20education%2C%20specifically%20with%20respect%20to%20how%20students%20view%20them%2C%20make%20use%0Aof%20them%2C%20and%20the%20implications%20for%20learning%20outcomes.%20This%20paper%20employs%20a%0Ahybrid%20methodological%20approach%20involving%20a%20systematic%20literature%20review%20and%0Asimulation-based%20modeling%20to%20explore%20student%20perceptions%20of%20GenAI%20use%20in%20the%0Acontext%20of%20higher%20education.%20A%20total%20of%20nineteen%20empirical%20articles%20from%202023%0Athrough%202025%20were%20selected%20from%20the%20PRISMA-based%20search%20targeting%20the%20Scopus%0Adatabase.%20Synthesis%20of%20emerging%20patterns%20from%20the%20literature%20was%20achieved%20by%0Athematic%20categorization.%20Six%20of%20these%20had%20enough%20quantitative%20information%2C%0Ai.e.%2C%20item-level%20means%20and%20standard%20deviations%2C%20to%20permit%20probabilistic%0Amodeling.%20One%20dataset%2C%20from%20the%20resulting%20subset%2C%20was%20itself%20selected%20as%20a%0Arepresentative%20case%20with%20which%20to%20illustrate%20inverse-variance%20weighting%20by%0AMonte%20Carlo%20simulation%2C%20by%20virtue%20of%20its%20well-designed%20Likert%20scale%20format%20and%0Athematic%20alignment%20with%20the%20use%20of%20computing%20systems%20by%20the%20researcher.%0A%20%20The%20simulation%20provided%20a%20composite%20%22Success%20Score%22%20forecasting%20the%20strength%0Aof%20the%20relationship%20between%20student%20perceptions%20and%20learning%20achievements.%0AFindings%20reveal%20that%20attitude%20factors%20concerned%20with%20usability%20and%20real-world%0Ausefulness%20are%20significantly%20better%20predictors%20of%20positive%20learning%20achievement%0Athan%20affective%20or%20trust-based%20factors.%20Such%20an%20interdisciplinary%20perspective%0Aprovides%20a%20unique%20means%20of%20linking%20thematic%20results%20with%20predictive%20modelling%2C%0Aresonating%20with%20longstanding%20controversies%20about%20the%20proper%20use%20of%20GenAI%20tools%0Awithin%20the%20university.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01062v2&entry.124074799=Read"},
{"title": "Control Disturbance Rejection in Neural ODEs", "author": "Erkan Bayram and Mohamed-Ali Belabbas and Tamer Ba\u015far", "abstract": "  In this paper, we propose an iterative training algorithm for Neural ODEs\nthat provides models resilient to control (parameter) disturbances. The method\nbuilds on our earlier work Tuning without Forgetting-and similarly introduces\ntraining points sequentially, and updates the parameters on new data within the\nspace of parameters that do not decrease performance on the previously learned\ntraining points-with the key difference that, inspired by the concept of flat\nminima, we solve a minimax problem for a non-convex non-concave functional over\nan infinite-dimensional control space. We develop a projected gradient descent\nalgorithm on the space of parameters that admits the structure of an\ninfinite-dimensional Banach subspace. We show through simulations that this\nformulation enables the model to effectively learn new data points and gain\nrobustness against control disturbance.\n", "link": "http://arxiv.org/abs/2509.18034v1", "date": "2025-09-22", "relevancy": 2.5323, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5157}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5071}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Control%20Disturbance%20Rejection%20in%20Neural%20ODEs&body=Title%3A%20Control%20Disturbance%20Rejection%20in%20Neural%20ODEs%0AAuthor%3A%20Erkan%20Bayram%20and%20Mohamed-Ali%20Belabbas%20and%20Tamer%20Ba%C5%9Far%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20iterative%20training%20algorithm%20for%20Neural%20ODEs%0Athat%20provides%20models%20resilient%20to%20control%20%28parameter%29%20disturbances.%20The%20method%0Abuilds%20on%20our%20earlier%20work%20Tuning%20without%20Forgetting-and%20similarly%20introduces%0Atraining%20points%20sequentially%2C%20and%20updates%20the%20parameters%20on%20new%20data%20within%20the%0Aspace%20of%20parameters%20that%20do%20not%20decrease%20performance%20on%20the%20previously%20learned%0Atraining%20points-with%20the%20key%20difference%20that%2C%20inspired%20by%20the%20concept%20of%20flat%0Aminima%2C%20we%20solve%20a%20minimax%20problem%20for%20a%20non-convex%20non-concave%20functional%20over%0Aan%20infinite-dimensional%20control%20space.%20We%20develop%20a%20projected%20gradient%20descent%0Aalgorithm%20on%20the%20space%20of%20parameters%20that%20admits%20the%20structure%20of%20an%0Ainfinite-dimensional%20Banach%20subspace.%20We%20show%20through%20simulations%20that%20this%0Aformulation%20enables%20the%20model%20to%20effectively%20learn%20new%20data%20points%20and%20gain%0Arobustness%20against%20control%20disturbance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControl%2520Disturbance%2520Rejection%2520in%2520Neural%2520ODEs%26entry.906535625%3DErkan%2520Bayram%2520and%2520Mohamed-Ali%2520Belabbas%2520and%2520Tamer%2520Ba%25C5%259Far%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520iterative%2520training%2520algorithm%2520for%2520Neural%2520ODEs%250Athat%2520provides%2520models%2520resilient%2520to%2520control%2520%2528parameter%2529%2520disturbances.%2520The%2520method%250Abuilds%2520on%2520our%2520earlier%2520work%2520Tuning%2520without%2520Forgetting-and%2520similarly%2520introduces%250Atraining%2520points%2520sequentially%252C%2520and%2520updates%2520the%2520parameters%2520on%2520new%2520data%2520within%2520the%250Aspace%2520of%2520parameters%2520that%2520do%2520not%2520decrease%2520performance%2520on%2520the%2520previously%2520learned%250Atraining%2520points-with%2520the%2520key%2520difference%2520that%252C%2520inspired%2520by%2520the%2520concept%2520of%2520flat%250Aminima%252C%2520we%2520solve%2520a%2520minimax%2520problem%2520for%2520a%2520non-convex%2520non-concave%2520functional%2520over%250Aan%2520infinite-dimensional%2520control%2520space.%2520We%2520develop%2520a%2520projected%2520gradient%2520descent%250Aalgorithm%2520on%2520the%2520space%2520of%2520parameters%2520that%2520admits%2520the%2520structure%2520of%2520an%250Ainfinite-dimensional%2520Banach%2520subspace.%2520We%2520show%2520through%2520simulations%2520that%2520this%250Aformulation%2520enables%2520the%2520model%2520to%2520effectively%2520learn%2520new%2520data%2520points%2520and%2520gain%250Arobustness%2520against%2520control%2520disturbance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control%20Disturbance%20Rejection%20in%20Neural%20ODEs&entry.906535625=Erkan%20Bayram%20and%20Mohamed-Ali%20Belabbas%20and%20Tamer%20Ba%C5%9Far&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20iterative%20training%20algorithm%20for%20Neural%20ODEs%0Athat%20provides%20models%20resilient%20to%20control%20%28parameter%29%20disturbances.%20The%20method%0Abuilds%20on%20our%20earlier%20work%20Tuning%20without%20Forgetting-and%20similarly%20introduces%0Atraining%20points%20sequentially%2C%20and%20updates%20the%20parameters%20on%20new%20data%20within%20the%0Aspace%20of%20parameters%20that%20do%20not%20decrease%20performance%20on%20the%20previously%20learned%0Atraining%20points-with%20the%20key%20difference%20that%2C%20inspired%20by%20the%20concept%20of%20flat%0Aminima%2C%20we%20solve%20a%20minimax%20problem%20for%20a%20non-convex%20non-concave%20functional%20over%0Aan%20infinite-dimensional%20control%20space.%20We%20develop%20a%20projected%20gradient%20descent%0Aalgorithm%20on%20the%20space%20of%20parameters%20that%20admits%20the%20structure%20of%20an%0Ainfinite-dimensional%20Banach%20subspace.%20We%20show%20through%20simulations%20that%20this%0Aformulation%20enables%20the%20model%20to%20effectively%20learn%20new%20data%20points%20and%20gain%0Arobustness%20against%20control%20disturbance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18034v1&entry.124074799=Read"},
{"title": "Variation in Verification: Understanding Verification Dynamics in Large\n  Language Models", "author": "Yefan Zhou and Austin Xu and Yilun Zhou and Janvijay Singh and Jiang Gui and Shafiq Joty", "abstract": "  Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges.\n", "link": "http://arxiv.org/abs/2509.17995v1", "date": "2025-09-22", "relevancy": 2.5261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variation%20in%20Verification%3A%20Understanding%20Verification%20Dynamics%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Variation%20in%20Verification%3A%20Understanding%20Verification%20Dynamics%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yefan%20Zhou%20and%20Austin%20Xu%20and%20Yilun%20Zhou%20and%20Janvijay%20Singh%20and%20Jiang%20Gui%20and%20Shafiq%20Joty%0AAbstract%3A%20%20%20Recent%20advances%20have%20shown%20that%20scaling%20test-time%20computation%20enables%20large%0Alanguage%20models%20%28LLMs%29%20to%20solve%20increasingly%20complex%20problems%20across%20diverse%0Adomains.%20One%20effective%20paradigm%20for%20test-time%20scaling%20%28TTS%29%20involves%20LLM%0Agenerators%20producing%20multiple%20solution%20candidates%2C%20with%20LLM%20verifiers%20assessing%0Athe%20correctness%20of%20these%20candidates%20without%20reference%20answers.%20In%20this%20paper%2C%0Awe%20study%20generative%20verifiers%2C%20which%20perform%20verification%20by%20generating%0Achain-of-thought%20%28CoT%29%20reasoning%20followed%20by%20a%20binary%20verdict.%20We%0Asystematically%20analyze%20verification%20dynamics%20across%20three%20dimensions%20-%20problem%0Adifficulty%2C%20generator%20capability%2C%20and%20verifier%20generation%20capability%20-%20with%0Aempirical%20studies%20on%2012%20benchmarks%20across%20mathematical%20reasoning%2C%20knowledge%2C%0Aand%20natural%20language%20reasoning%20tasks%20using%2014%20open-source%20models%20%282B%20to%2072B%0Aparameter%20range%29%20and%20GPT-4o.%20Our%20experiments%20reveal%20three%20key%20findings%20about%0Averification%20effectiveness%3A%20%281%29%20Easy%20problems%20allow%20verifiers%20to%20more%20reliably%0Acertify%20correct%20responses%3B%20%282%29%20Weak%20generators%20produce%20errors%20that%20are%20easier%0Ato%20detect%20than%20strong%20generators%3B%20%283%29%20Verification%20ability%20is%20generally%0Acorrelated%20with%20the%20verifier%27s%20own%20problem-solving%20capability%2C%20but%20this%0Arelationship%20varies%20with%20problem%20difficulty.%20These%20findings%20reveal%0Aopportunities%20to%20optimize%20basic%20verification%20strategies%20in%20TTS%20applications.%0AFirst%2C%20given%20the%20same%20verifier%2C%20some%20weak%20generators%20can%20nearly%20match%20stronger%0Aones%20in%20post-verification%20TTS%20performance%20%28e.g.%2C%20the%20Gemma2-9B%20to%20Gemma2-27B%0Aperformance%20gap%20shrinks%20by%2075.5%25%29.%20Second%2C%20we%20identify%20cases%20where%20strong%0Averifiers%20offer%20limited%20advantage%20over%20weak%20ones%2C%20as%20both%20fail%20to%20provide%0Ameaningful%20verification%20gains%2C%20suggesting%20that%20verifier%20scaling%20alone%20cannot%0Aovercome%20fundamental%20verification%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariation%2520in%2520Verification%253A%2520Understanding%2520Verification%2520Dynamics%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYefan%2520Zhou%2520and%2520Austin%2520Xu%2520and%2520Yilun%2520Zhou%2520and%2520Janvijay%2520Singh%2520and%2520Jiang%2520Gui%2520and%2520Shafiq%2520Joty%26entry.1292438233%3D%2520%2520Recent%2520advances%2520have%2520shown%2520that%2520scaling%2520test-time%2520computation%2520enables%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520solve%2520increasingly%2520complex%2520problems%2520across%2520diverse%250Adomains.%2520One%2520effective%2520paradigm%2520for%2520test-time%2520scaling%2520%2528TTS%2529%2520involves%2520LLM%250Agenerators%2520producing%2520multiple%2520solution%2520candidates%252C%2520with%2520LLM%2520verifiers%2520assessing%250Athe%2520correctness%2520of%2520these%2520candidates%2520without%2520reference%2520answers.%2520In%2520this%2520paper%252C%250Awe%2520study%2520generative%2520verifiers%252C%2520which%2520perform%2520verification%2520by%2520generating%250Achain-of-thought%2520%2528CoT%2529%2520reasoning%2520followed%2520by%2520a%2520binary%2520verdict.%2520We%250Asystematically%2520analyze%2520verification%2520dynamics%2520across%2520three%2520dimensions%2520-%2520problem%250Adifficulty%252C%2520generator%2520capability%252C%2520and%2520verifier%2520generation%2520capability%2520-%2520with%250Aempirical%2520studies%2520on%252012%2520benchmarks%2520across%2520mathematical%2520reasoning%252C%2520knowledge%252C%250Aand%2520natural%2520language%2520reasoning%2520tasks%2520using%252014%2520open-source%2520models%2520%25282B%2520to%252072B%250Aparameter%2520range%2529%2520and%2520GPT-4o.%2520Our%2520experiments%2520reveal%2520three%2520key%2520findings%2520about%250Averification%2520effectiveness%253A%2520%25281%2529%2520Easy%2520problems%2520allow%2520verifiers%2520to%2520more%2520reliably%250Acertify%2520correct%2520responses%253B%2520%25282%2529%2520Weak%2520generators%2520produce%2520errors%2520that%2520are%2520easier%250Ato%2520detect%2520than%2520strong%2520generators%253B%2520%25283%2529%2520Verification%2520ability%2520is%2520generally%250Acorrelated%2520with%2520the%2520verifier%2527s%2520own%2520problem-solving%2520capability%252C%2520but%2520this%250Arelationship%2520varies%2520with%2520problem%2520difficulty.%2520These%2520findings%2520reveal%250Aopportunities%2520to%2520optimize%2520basic%2520verification%2520strategies%2520in%2520TTS%2520applications.%250AFirst%252C%2520given%2520the%2520same%2520verifier%252C%2520some%2520weak%2520generators%2520can%2520nearly%2520match%2520stronger%250Aones%2520in%2520post-verification%2520TTS%2520performance%2520%2528e.g.%252C%2520the%2520Gemma2-9B%2520to%2520Gemma2-27B%250Aperformance%2520gap%2520shrinks%2520by%252075.5%2525%2529.%2520Second%252C%2520we%2520identify%2520cases%2520where%2520strong%250Averifiers%2520offer%2520limited%2520advantage%2520over%2520weak%2520ones%252C%2520as%2520both%2520fail%2520to%2520provide%250Ameaningful%2520verification%2520gains%252C%2520suggesting%2520that%2520verifier%2520scaling%2520alone%2520cannot%250Aovercome%2520fundamental%2520verification%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variation%20in%20Verification%3A%20Understanding%20Verification%20Dynamics%20in%20Large%0A%20%20Language%20Models&entry.906535625=Yefan%20Zhou%20and%20Austin%20Xu%20and%20Yilun%20Zhou%20and%20Janvijay%20Singh%20and%20Jiang%20Gui%20and%20Shafiq%20Joty&entry.1292438233=%20%20Recent%20advances%20have%20shown%20that%20scaling%20test-time%20computation%20enables%20large%0Alanguage%20models%20%28LLMs%29%20to%20solve%20increasingly%20complex%20problems%20across%20diverse%0Adomains.%20One%20effective%20paradigm%20for%20test-time%20scaling%20%28TTS%29%20involves%20LLM%0Agenerators%20producing%20multiple%20solution%20candidates%2C%20with%20LLM%20verifiers%20assessing%0Athe%20correctness%20of%20these%20candidates%20without%20reference%20answers.%20In%20this%20paper%2C%0Awe%20study%20generative%20verifiers%2C%20which%20perform%20verification%20by%20generating%0Achain-of-thought%20%28CoT%29%20reasoning%20followed%20by%20a%20binary%20verdict.%20We%0Asystematically%20analyze%20verification%20dynamics%20across%20three%20dimensions%20-%20problem%0Adifficulty%2C%20generator%20capability%2C%20and%20verifier%20generation%20capability%20-%20with%0Aempirical%20studies%20on%2012%20benchmarks%20across%20mathematical%20reasoning%2C%20knowledge%2C%0Aand%20natural%20language%20reasoning%20tasks%20using%2014%20open-source%20models%20%282B%20to%2072B%0Aparameter%20range%29%20and%20GPT-4o.%20Our%20experiments%20reveal%20three%20key%20findings%20about%0Averification%20effectiveness%3A%20%281%29%20Easy%20problems%20allow%20verifiers%20to%20more%20reliably%0Acertify%20correct%20responses%3B%20%282%29%20Weak%20generators%20produce%20errors%20that%20are%20easier%0Ato%20detect%20than%20strong%20generators%3B%20%283%29%20Verification%20ability%20is%20generally%0Acorrelated%20with%20the%20verifier%27s%20own%20problem-solving%20capability%2C%20but%20this%0Arelationship%20varies%20with%20problem%20difficulty.%20These%20findings%20reveal%0Aopportunities%20to%20optimize%20basic%20verification%20strategies%20in%20TTS%20applications.%0AFirst%2C%20given%20the%20same%20verifier%2C%20some%20weak%20generators%20can%20nearly%20match%20stronger%0Aones%20in%20post-verification%20TTS%20performance%20%28e.g.%2C%20the%20Gemma2-9B%20to%20Gemma2-27B%0Aperformance%20gap%20shrinks%20by%2075.5%25%29.%20Second%2C%20we%20identify%20cases%20where%20strong%0Averifiers%20offer%20limited%20advantage%20over%20weak%20ones%2C%20as%20both%20fail%20to%20provide%0Ameaningful%20verification%20gains%2C%20suggesting%20that%20verifier%20scaling%20alone%20cannot%0Aovercome%20fundamental%20verification%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17995v1&entry.124074799=Read"},
{"title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?", "author": "Zhongyuan Bao and Lejun Zhang", "abstract": "  Multimodal large language models (MLLMs) excel at general video understanding\nbut struggle with fast, high-frequency sports like tennis, where rally clips\nare short yet information-dense. To systematically evaluate MLLMs in this\nchallenging domain, we present TennisTV, the first and most comprehensive\nbenchmark for tennis video understanding. TennisTV models each rally as a\ntemporal-ordered sequence of consecutive stroke events, using automated\npipelines for filtering and question generation. It covers 9 tasks from the\nstroke level to the rally level and includes 2943 human-verified questions.\nEvaluating 17 representative MLLMs, we provide the first systematic assessment\nof tennis video understanding. Results reveal substantial shortcomings and\nyield two key insights: (i) frame-sampling density should be tailored and\nbalanced across tasks, and (ii) improving temporal grounding is essential for\nstronger reasoning.\n", "link": "http://arxiv.org/abs/2509.15602v2", "date": "2025-09-22", "relevancy": 2.5195, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TennisTV%3A%20Do%20Multimodal%20Large%20Language%20Models%20Understand%20Tennis%20Rallies%3F&body=Title%3A%20TennisTV%3A%20Do%20Multimodal%20Large%20Language%20Models%20Understand%20Tennis%20Rallies%3F%0AAuthor%3A%20Zhongyuan%20Bao%20and%20Lejun%20Zhang%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20excel%20at%20general%20video%20understanding%0Abut%20struggle%20with%20fast%2C%20high-frequency%20sports%20like%20tennis%2C%20where%20rally%20clips%0Aare%20short%20yet%20information-dense.%20To%20systematically%20evaluate%20MLLMs%20in%20this%0Achallenging%20domain%2C%20we%20present%20TennisTV%2C%20the%20first%20and%20most%20comprehensive%0Abenchmark%20for%20tennis%20video%20understanding.%20TennisTV%20models%20each%20rally%20as%20a%0Atemporal-ordered%20sequence%20of%20consecutive%20stroke%20events%2C%20using%20automated%0Apipelines%20for%20filtering%20and%20question%20generation.%20It%20covers%209%20tasks%20from%20the%0Astroke%20level%20to%20the%20rally%20level%20and%20includes%202943%20human-verified%20questions.%0AEvaluating%2017%20representative%20MLLMs%2C%20we%20provide%20the%20first%20systematic%20assessment%0Aof%20tennis%20video%20understanding.%20Results%20reveal%20substantial%20shortcomings%20and%0Ayield%20two%20key%20insights%3A%20%28i%29%20frame-sampling%20density%20should%20be%20tailored%20and%0Abalanced%20across%20tasks%2C%20and%20%28ii%29%20improving%20temporal%20grounding%20is%20essential%20for%0Astronger%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTennisTV%253A%2520Do%2520Multimodal%2520Large%2520Language%2520Models%2520Understand%2520Tennis%2520Rallies%253F%26entry.906535625%3DZhongyuan%2520Bao%2520and%2520Lejun%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520excel%2520at%2520general%2520video%2520understanding%250Abut%2520struggle%2520with%2520fast%252C%2520high-frequency%2520sports%2520like%2520tennis%252C%2520where%2520rally%2520clips%250Aare%2520short%2520yet%2520information-dense.%2520To%2520systematically%2520evaluate%2520MLLMs%2520in%2520this%250Achallenging%2520domain%252C%2520we%2520present%2520TennisTV%252C%2520the%2520first%2520and%2520most%2520comprehensive%250Abenchmark%2520for%2520tennis%2520video%2520understanding.%2520TennisTV%2520models%2520each%2520rally%2520as%2520a%250Atemporal-ordered%2520sequence%2520of%2520consecutive%2520stroke%2520events%252C%2520using%2520automated%250Apipelines%2520for%2520filtering%2520and%2520question%2520generation.%2520It%2520covers%25209%2520tasks%2520from%2520the%250Astroke%2520level%2520to%2520the%2520rally%2520level%2520and%2520includes%25202943%2520human-verified%2520questions.%250AEvaluating%252017%2520representative%2520MLLMs%252C%2520we%2520provide%2520the%2520first%2520systematic%2520assessment%250Aof%2520tennis%2520video%2520understanding.%2520Results%2520reveal%2520substantial%2520shortcomings%2520and%250Ayield%2520two%2520key%2520insights%253A%2520%2528i%2529%2520frame-sampling%2520density%2520should%2520be%2520tailored%2520and%250Abalanced%2520across%2520tasks%252C%2520and%2520%2528ii%2529%2520improving%2520temporal%2520grounding%2520is%2520essential%2520for%250Astronger%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TennisTV%3A%20Do%20Multimodal%20Large%20Language%20Models%20Understand%20Tennis%20Rallies%3F&entry.906535625=Zhongyuan%20Bao%20and%20Lejun%20Zhang&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20excel%20at%20general%20video%20understanding%0Abut%20struggle%20with%20fast%2C%20high-frequency%20sports%20like%20tennis%2C%20where%20rally%20clips%0Aare%20short%20yet%20information-dense.%20To%20systematically%20evaluate%20MLLMs%20in%20this%0Achallenging%20domain%2C%20we%20present%20TennisTV%2C%20the%20first%20and%20most%20comprehensive%0Abenchmark%20for%20tennis%20video%20understanding.%20TennisTV%20models%20each%20rally%20as%20a%0Atemporal-ordered%20sequence%20of%20consecutive%20stroke%20events%2C%20using%20automated%0Apipelines%20for%20filtering%20and%20question%20generation.%20It%20covers%209%20tasks%20from%20the%0Astroke%20level%20to%20the%20rally%20level%20and%20includes%202943%20human-verified%20questions.%0AEvaluating%2017%20representative%20MLLMs%2C%20we%20provide%20the%20first%20systematic%20assessment%0Aof%20tennis%20video%20understanding.%20Results%20reveal%20substantial%20shortcomings%20and%0Ayield%20two%20key%20insights%3A%20%28i%29%20frame-sampling%20density%20should%20be%20tailored%20and%0Abalanced%20across%20tasks%2C%20and%20%28ii%29%20improving%20temporal%20grounding%20is%20essential%20for%0Astronger%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15602v2&entry.124074799=Read"},
{"title": "DragOSM: Extract Building Roofs and Footprints from Aerial Images by\n  Aligning Historical Labels", "author": "Kai Li and Xingxing Weng and Yupeng Deng and Yu Meng and Chao Pang and Gui-Song Xia and Xiangyu Zhao", "abstract": "  Extracting polygonal roofs and footprints from remote sensing images is\ncritical for large-scale urban analysis. Most existing methods rely on\nsegmentation-based models that assume clear semantic boundaries of roofs, but\nthese approaches struggle in off- nadir images, where the roof and footprint\nare significantly displaced, and facade pixels are fused with the roof\nboundary. With the increasing availability of open vector map annotations,\ne.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation\nhas become viable because remote sensing images are georeferenced once\ncaptured. However, these historical labels commonly suffer from significant\npositional discrepancies with new images and only have one annotation (roof or\nfootprint), which fails to describe the correct structures of a building. To\naddress these discrepancies, we first introduce a concept of an alignment\ntoken, which encodes the correction vector to guide the label correction. Based\non this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel\nmodel designed to align dislocated historical labels with roofs and footprints.\nSpecifically, DragOSM formulates the label alignment as an interactive\ndenoising process, modeling the positional discrepancy as a Gaussian\ndistribution. During training, it learns to correct these errors by simulating\nmisalignment with random Gaussian perturbations; during inference, it\niteratively refines the positions of input labels. To validate our method, we\nfurther present a new dataset, Repairing Buildings in OSM (ReBO), comprising\n179,265 buildings with both OpenStreetMap and manually corrected annotations\nacross 5,473 images from 41 cities. Experimental results on ReBO demonstrate\nthe effectiveness of DragOSM. Code, dataset, and trained models are publicly\navailable at https://github.com/likaiucas/DragOSM.git.\n", "link": "http://arxiv.org/abs/2509.17951v1", "date": "2025-09-22", "relevancy": 2.505, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4974}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DragOSM%3A%20Extract%20Building%20Roofs%20and%20Footprints%20from%20Aerial%20Images%20by%0A%20%20Aligning%20Historical%20Labels&body=Title%3A%20DragOSM%3A%20Extract%20Building%20Roofs%20and%20Footprints%20from%20Aerial%20Images%20by%0A%20%20Aligning%20Historical%20Labels%0AAuthor%3A%20Kai%20Li%20and%20Xingxing%20Weng%20and%20Yupeng%20Deng%20and%20Yu%20Meng%20and%20Chao%20Pang%20and%20Gui-Song%20Xia%20and%20Xiangyu%20Zhao%0AAbstract%3A%20%20%20Extracting%20polygonal%20roofs%20and%20footprints%20from%20remote%20sensing%20images%20is%0Acritical%20for%20large-scale%20urban%20analysis.%20Most%20existing%20methods%20rely%20on%0Asegmentation-based%20models%20that%20assume%20clear%20semantic%20boundaries%20of%20roofs%2C%20but%0Athese%20approaches%20struggle%20in%20off-%20nadir%20images%2C%20where%20the%20roof%20and%20footprint%0Aare%20significantly%20displaced%2C%20and%20facade%20pixels%20are%20fused%20with%20the%20roof%0Aboundary.%20With%20the%20increasing%20availability%20of%20open%20vector%20map%20annotations%2C%0Ae.g.%2C%20OpenStreetMap%2C%20utilizing%20historical%20labels%20for%20off-nadir%20image%20annotation%0Ahas%20become%20viable%20because%20remote%20sensing%20images%20are%20georeferenced%20once%0Acaptured.%20However%2C%20these%20historical%20labels%20commonly%20suffer%20from%20significant%0Apositional%20discrepancies%20with%20new%20images%20and%20only%20have%20one%20annotation%20%28roof%20or%0Afootprint%29%2C%20which%20fails%20to%20describe%20the%20correct%20structures%20of%20a%20building.%20To%0Aaddress%20these%20discrepancies%2C%20we%20first%20introduce%20a%20concept%20of%20an%20alignment%0Atoken%2C%20which%20encodes%20the%20correction%20vector%20to%20guide%20the%20label%20correction.%20Based%0Aon%20this%20concept%2C%20we%20then%20propose%20Drag%20OpenStreetMap%20Labels%20%28DragOSM%29%2C%20a%20novel%0Amodel%20designed%20to%20align%20dislocated%20historical%20labels%20with%20roofs%20and%20footprints.%0ASpecifically%2C%20DragOSM%20formulates%20the%20label%20alignment%20as%20an%20interactive%0Adenoising%20process%2C%20modeling%20the%20positional%20discrepancy%20as%20a%20Gaussian%0Adistribution.%20During%20training%2C%20it%20learns%20to%20correct%20these%20errors%20by%20simulating%0Amisalignment%20with%20random%20Gaussian%20perturbations%3B%20during%20inference%2C%20it%0Aiteratively%20refines%20the%20positions%20of%20input%20labels.%20To%20validate%20our%20method%2C%20we%0Afurther%20present%20a%20new%20dataset%2C%20Repairing%20Buildings%20in%20OSM%20%28ReBO%29%2C%20comprising%0A179%2C265%20buildings%20with%20both%20OpenStreetMap%20and%20manually%20corrected%20annotations%0Aacross%205%2C473%20images%20from%2041%20cities.%20Experimental%20results%20on%20ReBO%20demonstrate%0Athe%20effectiveness%20of%20DragOSM.%20Code%2C%20dataset%2C%20and%20trained%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/likaiucas/DragOSM.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDragOSM%253A%2520Extract%2520Building%2520Roofs%2520and%2520Footprints%2520from%2520Aerial%2520Images%2520by%250A%2520%2520Aligning%2520Historical%2520Labels%26entry.906535625%3DKai%2520Li%2520and%2520Xingxing%2520Weng%2520and%2520Yupeng%2520Deng%2520and%2520Yu%2520Meng%2520and%2520Chao%2520Pang%2520and%2520Gui-Song%2520Xia%2520and%2520Xiangyu%2520Zhao%26entry.1292438233%3D%2520%2520Extracting%2520polygonal%2520roofs%2520and%2520footprints%2520from%2520remote%2520sensing%2520images%2520is%250Acritical%2520for%2520large-scale%2520urban%2520analysis.%2520Most%2520existing%2520methods%2520rely%2520on%250Asegmentation-based%2520models%2520that%2520assume%2520clear%2520semantic%2520boundaries%2520of%2520roofs%252C%2520but%250Athese%2520approaches%2520struggle%2520in%2520off-%2520nadir%2520images%252C%2520where%2520the%2520roof%2520and%2520footprint%250Aare%2520significantly%2520displaced%252C%2520and%2520facade%2520pixels%2520are%2520fused%2520with%2520the%2520roof%250Aboundary.%2520With%2520the%2520increasing%2520availability%2520of%2520open%2520vector%2520map%2520annotations%252C%250Ae.g.%252C%2520OpenStreetMap%252C%2520utilizing%2520historical%2520labels%2520for%2520off-nadir%2520image%2520annotation%250Ahas%2520become%2520viable%2520because%2520remote%2520sensing%2520images%2520are%2520georeferenced%2520once%250Acaptured.%2520However%252C%2520these%2520historical%2520labels%2520commonly%2520suffer%2520from%2520significant%250Apositional%2520discrepancies%2520with%2520new%2520images%2520and%2520only%2520have%2520one%2520annotation%2520%2528roof%2520or%250Afootprint%2529%252C%2520which%2520fails%2520to%2520describe%2520the%2520correct%2520structures%2520of%2520a%2520building.%2520To%250Aaddress%2520these%2520discrepancies%252C%2520we%2520first%2520introduce%2520a%2520concept%2520of%2520an%2520alignment%250Atoken%252C%2520which%2520encodes%2520the%2520correction%2520vector%2520to%2520guide%2520the%2520label%2520correction.%2520Based%250Aon%2520this%2520concept%252C%2520we%2520then%2520propose%2520Drag%2520OpenStreetMap%2520Labels%2520%2528DragOSM%2529%252C%2520a%2520novel%250Amodel%2520designed%2520to%2520align%2520dislocated%2520historical%2520labels%2520with%2520roofs%2520and%2520footprints.%250ASpecifically%252C%2520DragOSM%2520formulates%2520the%2520label%2520alignment%2520as%2520an%2520interactive%250Adenoising%2520process%252C%2520modeling%2520the%2520positional%2520discrepancy%2520as%2520a%2520Gaussian%250Adistribution.%2520During%2520training%252C%2520it%2520learns%2520to%2520correct%2520these%2520errors%2520by%2520simulating%250Amisalignment%2520with%2520random%2520Gaussian%2520perturbations%253B%2520during%2520inference%252C%2520it%250Aiteratively%2520refines%2520the%2520positions%2520of%2520input%2520labels.%2520To%2520validate%2520our%2520method%252C%2520we%250Afurther%2520present%2520a%2520new%2520dataset%252C%2520Repairing%2520Buildings%2520in%2520OSM%2520%2528ReBO%2529%252C%2520comprising%250A179%252C265%2520buildings%2520with%2520both%2520OpenStreetMap%2520and%2520manually%2520corrected%2520annotations%250Aacross%25205%252C473%2520images%2520from%252041%2520cities.%2520Experimental%2520results%2520on%2520ReBO%2520demonstrate%250Athe%2520effectiveness%2520of%2520DragOSM.%2520Code%252C%2520dataset%252C%2520and%2520trained%2520models%2520are%2520publicly%250Aavailable%2520at%2520https%253A//github.com/likaiucas/DragOSM.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DragOSM%3A%20Extract%20Building%20Roofs%20and%20Footprints%20from%20Aerial%20Images%20by%0A%20%20Aligning%20Historical%20Labels&entry.906535625=Kai%20Li%20and%20Xingxing%20Weng%20and%20Yupeng%20Deng%20and%20Yu%20Meng%20and%20Chao%20Pang%20and%20Gui-Song%20Xia%20and%20Xiangyu%20Zhao&entry.1292438233=%20%20Extracting%20polygonal%20roofs%20and%20footprints%20from%20remote%20sensing%20images%20is%0Acritical%20for%20large-scale%20urban%20analysis.%20Most%20existing%20methods%20rely%20on%0Asegmentation-based%20models%20that%20assume%20clear%20semantic%20boundaries%20of%20roofs%2C%20but%0Athese%20approaches%20struggle%20in%20off-%20nadir%20images%2C%20where%20the%20roof%20and%20footprint%0Aare%20significantly%20displaced%2C%20and%20facade%20pixels%20are%20fused%20with%20the%20roof%0Aboundary.%20With%20the%20increasing%20availability%20of%20open%20vector%20map%20annotations%2C%0Ae.g.%2C%20OpenStreetMap%2C%20utilizing%20historical%20labels%20for%20off-nadir%20image%20annotation%0Ahas%20become%20viable%20because%20remote%20sensing%20images%20are%20georeferenced%20once%0Acaptured.%20However%2C%20these%20historical%20labels%20commonly%20suffer%20from%20significant%0Apositional%20discrepancies%20with%20new%20images%20and%20only%20have%20one%20annotation%20%28roof%20or%0Afootprint%29%2C%20which%20fails%20to%20describe%20the%20correct%20structures%20of%20a%20building.%20To%0Aaddress%20these%20discrepancies%2C%20we%20first%20introduce%20a%20concept%20of%20an%20alignment%0Atoken%2C%20which%20encodes%20the%20correction%20vector%20to%20guide%20the%20label%20correction.%20Based%0Aon%20this%20concept%2C%20we%20then%20propose%20Drag%20OpenStreetMap%20Labels%20%28DragOSM%29%2C%20a%20novel%0Amodel%20designed%20to%20align%20dislocated%20historical%20labels%20with%20roofs%20and%20footprints.%0ASpecifically%2C%20DragOSM%20formulates%20the%20label%20alignment%20as%20an%20interactive%0Adenoising%20process%2C%20modeling%20the%20positional%20discrepancy%20as%20a%20Gaussian%0Adistribution.%20During%20training%2C%20it%20learns%20to%20correct%20these%20errors%20by%20simulating%0Amisalignment%20with%20random%20Gaussian%20perturbations%3B%20during%20inference%2C%20it%0Aiteratively%20refines%20the%20positions%20of%20input%20labels.%20To%20validate%20our%20method%2C%20we%0Afurther%20present%20a%20new%20dataset%2C%20Repairing%20Buildings%20in%20OSM%20%28ReBO%29%2C%20comprising%0A179%2C265%20buildings%20with%20both%20OpenStreetMap%20and%20manually%20corrected%20annotations%0Aacross%205%2C473%20images%20from%2041%20cities.%20Experimental%20results%20on%20ReBO%20demonstrate%0Athe%20effectiveness%20of%20DragOSM.%20Code%2C%20dataset%2C%20and%20trained%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/likaiucas/DragOSM.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17951v1&entry.124074799=Read"},
{"title": "Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of\n  Large Language Models on CFA Level III", "author": "Pranam Shetty and Abhisek Upadhayaya and Parth Mitesh Shah and Srikanth Jagabathula and Shilpi Nayak and Anna Joo Fee", "abstract": "  As financial institutions increasingly adopt Large Language Models (LLMs),\nrigorous domain-specific evaluation becomes critical for responsible\ndeployment. This paper presents a comprehensive benchmark evaluating 23\nstate-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam -\nthe gold standard for advanced financial reasoning. We assess both\nmultiple-choice questions (MCQs) and essay-style responses using multiple\nprompting strategies including Chain-of-Thought and Self-Discover. Our\nevaluation reveals that leading models demonstrate strong capabilities, with\ncomposite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA\nLevel III. These results, achieved under a revised, stricter essay grading\nmethodology, indicate significant progress in LLM capabilities for high-stakes\nfinancial applications. Our findings provide crucial guidance for practitioners\non model selection and highlight remaining challenges in cost-effective\ndeployment and the need for nuanced interpretation of performance against\nprofessional benchmarks.\n", "link": "http://arxiv.org/abs/2507.02954v2", "date": "2025-09-22", "relevancy": 2.5045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5318}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Financial%20Reasoning%20at%20Scale%3A%20A%20Comprehensive%20Evaluation%20of%0A%20%20Large%20Language%20Models%20on%20CFA%20Level%20III&body=Title%3A%20Advanced%20Financial%20Reasoning%20at%20Scale%3A%20A%20Comprehensive%20Evaluation%20of%0A%20%20Large%20Language%20Models%20on%20CFA%20Level%20III%0AAuthor%3A%20Pranam%20Shetty%20and%20Abhisek%20Upadhayaya%20and%20Parth%20Mitesh%20Shah%20and%20Srikanth%20Jagabathula%20and%20Shilpi%20Nayak%20and%20Anna%20Joo%20Fee%0AAbstract%3A%20%20%20As%20financial%20institutions%20increasingly%20adopt%20Large%20Language%20Models%20%28LLMs%29%2C%0Arigorous%20domain-specific%20evaluation%20becomes%20critical%20for%20responsible%0Adeployment.%20This%20paper%20presents%20a%20comprehensive%20benchmark%20evaluating%2023%0Astate-of-the-art%20LLMs%20on%20the%20Chartered%20Financial%20Analyst%20%28CFA%29%20Level%20III%20exam%20-%0Athe%20gold%20standard%20for%20advanced%20financial%20reasoning.%20We%20assess%20both%0Amultiple-choice%20questions%20%28MCQs%29%20and%20essay-style%20responses%20using%20multiple%0Aprompting%20strategies%20including%20Chain-of-Thought%20and%20Self-Discover.%20Our%0Aevaluation%20reveals%20that%20leading%20models%20demonstrate%20strong%20capabilities%2C%20with%0Acomposite%20scores%20such%20as%2079.1%25%20%28o4-mini%29%20and%2077.3%25%20%28Gemini%202.5%20Flash%29%20on%20CFA%0ALevel%20III.%20These%20results%2C%20achieved%20under%20a%20revised%2C%20stricter%20essay%20grading%0Amethodology%2C%20indicate%20significant%20progress%20in%20LLM%20capabilities%20for%20high-stakes%0Afinancial%20applications.%20Our%20findings%20provide%20crucial%20guidance%20for%20practitioners%0Aon%20model%20selection%20and%20highlight%20remaining%20challenges%20in%20cost-effective%0Adeployment%20and%20the%20need%20for%20nuanced%20interpretation%20of%20performance%20against%0Aprofessional%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Financial%2520Reasoning%2520at%2520Scale%253A%2520A%2520Comprehensive%2520Evaluation%2520of%250A%2520%2520Large%2520Language%2520Models%2520on%2520CFA%2520Level%2520III%26entry.906535625%3DPranam%2520Shetty%2520and%2520Abhisek%2520Upadhayaya%2520and%2520Parth%2520Mitesh%2520Shah%2520and%2520Srikanth%2520Jagabathula%2520and%2520Shilpi%2520Nayak%2520and%2520Anna%2520Joo%2520Fee%26entry.1292438233%3D%2520%2520As%2520financial%2520institutions%2520increasingly%2520adopt%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Arigorous%2520domain-specific%2520evaluation%2520becomes%2520critical%2520for%2520responsible%250Adeployment.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520benchmark%2520evaluating%252023%250Astate-of-the-art%2520LLMs%2520on%2520the%2520Chartered%2520Financial%2520Analyst%2520%2528CFA%2529%2520Level%2520III%2520exam%2520-%250Athe%2520gold%2520standard%2520for%2520advanced%2520financial%2520reasoning.%2520We%2520assess%2520both%250Amultiple-choice%2520questions%2520%2528MCQs%2529%2520and%2520essay-style%2520responses%2520using%2520multiple%250Aprompting%2520strategies%2520including%2520Chain-of-Thought%2520and%2520Self-Discover.%2520Our%250Aevaluation%2520reveals%2520that%2520leading%2520models%2520demonstrate%2520strong%2520capabilities%252C%2520with%250Acomposite%2520scores%2520such%2520as%252079.1%2525%2520%2528o4-mini%2529%2520and%252077.3%2525%2520%2528Gemini%25202.5%2520Flash%2529%2520on%2520CFA%250ALevel%2520III.%2520These%2520results%252C%2520achieved%2520under%2520a%2520revised%252C%2520stricter%2520essay%2520grading%250Amethodology%252C%2520indicate%2520significant%2520progress%2520in%2520LLM%2520capabilities%2520for%2520high-stakes%250Afinancial%2520applications.%2520Our%2520findings%2520provide%2520crucial%2520guidance%2520for%2520practitioners%250Aon%2520model%2520selection%2520and%2520highlight%2520remaining%2520challenges%2520in%2520cost-effective%250Adeployment%2520and%2520the%2520need%2520for%2520nuanced%2520interpretation%2520of%2520performance%2520against%250Aprofessional%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Financial%20Reasoning%20at%20Scale%3A%20A%20Comprehensive%20Evaluation%20of%0A%20%20Large%20Language%20Models%20on%20CFA%20Level%20III&entry.906535625=Pranam%20Shetty%20and%20Abhisek%20Upadhayaya%20and%20Parth%20Mitesh%20Shah%20and%20Srikanth%20Jagabathula%20and%20Shilpi%20Nayak%20and%20Anna%20Joo%20Fee&entry.1292438233=%20%20As%20financial%20institutions%20increasingly%20adopt%20Large%20Language%20Models%20%28LLMs%29%2C%0Arigorous%20domain-specific%20evaluation%20becomes%20critical%20for%20responsible%0Adeployment.%20This%20paper%20presents%20a%20comprehensive%20benchmark%20evaluating%2023%0Astate-of-the-art%20LLMs%20on%20the%20Chartered%20Financial%20Analyst%20%28CFA%29%20Level%20III%20exam%20-%0Athe%20gold%20standard%20for%20advanced%20financial%20reasoning.%20We%20assess%20both%0Amultiple-choice%20questions%20%28MCQs%29%20and%20essay-style%20responses%20using%20multiple%0Aprompting%20strategies%20including%20Chain-of-Thought%20and%20Self-Discover.%20Our%0Aevaluation%20reveals%20that%20leading%20models%20demonstrate%20strong%20capabilities%2C%20with%0Acomposite%20scores%20such%20as%2079.1%25%20%28o4-mini%29%20and%2077.3%25%20%28Gemini%202.5%20Flash%29%20on%20CFA%0ALevel%20III.%20These%20results%2C%20achieved%20under%20a%20revised%2C%20stricter%20essay%20grading%0Amethodology%2C%20indicate%20significant%20progress%20in%20LLM%20capabilities%20for%20high-stakes%0Afinancial%20applications.%20Our%20findings%20provide%20crucial%20guidance%20for%20practitioners%0Aon%20model%20selection%20and%20highlight%20remaining%20challenges%20in%20cost-effective%0Adeployment%20and%20the%20need%20for%20nuanced%20interpretation%20of%20performance%20against%0Aprofessional%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02954v2&entry.124074799=Read"},
{"title": "Improving Instruct Models for Free: A Study on Partial Adaptation", "author": "Ozan \u0130rsoy and Pengxiang Cheng and Jennifer L. Chen and Daniel Preo\u0163iuc-Pietro and Shiyue Zhang and Duccio Pappadopulo", "abstract": "  Instruct models, obtained from various instruction tuning or post-training\nsteps, are commonly deemed superior and more usable than their base\ncounterpart. While the model gains instruction following ability, instruction\ntuning may lead to forgetting the knowledge from pre-training or it may\nencourage the model to become overly conversational or verbose. This, in turn,\ncan lead to degradation of in-context few-shot learning performance. In this\nwork, we study the performance trajectory between base and instruct models by\nscaling down the strength of instruction-tuning via the partial adaption\nmethod. We show that, across several model families and model sizes, reducing\nthe strength of instruction-tuning results in material improvement on a\nfew-shot in-context learning benchmark covering a variety of classic natural\nlanguage tasks. This comes at the cost of losing some degree of instruction\nfollowing ability as measured by AlpacaEval. Our study shines light on the\npotential trade-off between in-context learning and instruction following\nabilities that is worth considering in practice.\n", "link": "http://arxiv.org/abs/2504.11626v2", "date": "2025-09-22", "relevancy": 2.4969, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Instruct%20Models%20for%20Free%3A%20A%20Study%20on%20Partial%20Adaptation&body=Title%3A%20Improving%20Instruct%20Models%20for%20Free%3A%20A%20Study%20on%20Partial%20Adaptation%0AAuthor%3A%20Ozan%20%C4%B0rsoy%20and%20Pengxiang%20Cheng%20and%20Jennifer%20L.%20Chen%20and%20Daniel%20Preo%C5%A3iuc-Pietro%20and%20Shiyue%20Zhang%20and%20Duccio%20Pappadopulo%0AAbstract%3A%20%20%20Instruct%20models%2C%20obtained%20from%20various%20instruction%20tuning%20or%20post-training%0Asteps%2C%20are%20commonly%20deemed%20superior%20and%20more%20usable%20than%20their%20base%0Acounterpart.%20While%20the%20model%20gains%20instruction%20following%20ability%2C%20instruction%0Atuning%20may%20lead%20to%20forgetting%20the%20knowledge%20from%20pre-training%20or%20it%20may%0Aencourage%20the%20model%20to%20become%20overly%20conversational%20or%20verbose.%20This%2C%20in%20turn%2C%0Acan%20lead%20to%20degradation%20of%20in-context%20few-shot%20learning%20performance.%20In%20this%0Awork%2C%20we%20study%20the%20performance%20trajectory%20between%20base%20and%20instruct%20models%20by%0Ascaling%20down%20the%20strength%20of%20instruction-tuning%20via%20the%20partial%20adaption%0Amethod.%20We%20show%20that%2C%20across%20several%20model%20families%20and%20model%20sizes%2C%20reducing%0Athe%20strength%20of%20instruction-tuning%20results%20in%20material%20improvement%20on%20a%0Afew-shot%20in-context%20learning%20benchmark%20covering%20a%20variety%20of%20classic%20natural%0Alanguage%20tasks.%20This%20comes%20at%20the%20cost%20of%20losing%20some%20degree%20of%20instruction%0Afollowing%20ability%20as%20measured%20by%20AlpacaEval.%20Our%20study%20shines%20light%20on%20the%0Apotential%20trade-off%20between%20in-context%20learning%20and%20instruction%20following%0Aabilities%20that%20is%20worth%20considering%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Instruct%2520Models%2520for%2520Free%253A%2520A%2520Study%2520on%2520Partial%2520Adaptation%26entry.906535625%3DOzan%2520%25C4%25B0rsoy%2520and%2520Pengxiang%2520Cheng%2520and%2520Jennifer%2520L.%2520Chen%2520and%2520Daniel%2520Preo%25C5%25A3iuc-Pietro%2520and%2520Shiyue%2520Zhang%2520and%2520Duccio%2520Pappadopulo%26entry.1292438233%3D%2520%2520Instruct%2520models%252C%2520obtained%2520from%2520various%2520instruction%2520tuning%2520or%2520post-training%250Asteps%252C%2520are%2520commonly%2520deemed%2520superior%2520and%2520more%2520usable%2520than%2520their%2520base%250Acounterpart.%2520While%2520the%2520model%2520gains%2520instruction%2520following%2520ability%252C%2520instruction%250Atuning%2520may%2520lead%2520to%2520forgetting%2520the%2520knowledge%2520from%2520pre-training%2520or%2520it%2520may%250Aencourage%2520the%2520model%2520to%2520become%2520overly%2520conversational%2520or%2520verbose.%2520This%252C%2520in%2520turn%252C%250Acan%2520lead%2520to%2520degradation%2520of%2520in-context%2520few-shot%2520learning%2520performance.%2520In%2520this%250Awork%252C%2520we%2520study%2520the%2520performance%2520trajectory%2520between%2520base%2520and%2520instruct%2520models%2520by%250Ascaling%2520down%2520the%2520strength%2520of%2520instruction-tuning%2520via%2520the%2520partial%2520adaption%250Amethod.%2520We%2520show%2520that%252C%2520across%2520several%2520model%2520families%2520and%2520model%2520sizes%252C%2520reducing%250Athe%2520strength%2520of%2520instruction-tuning%2520results%2520in%2520material%2520improvement%2520on%2520a%250Afew-shot%2520in-context%2520learning%2520benchmark%2520covering%2520a%2520variety%2520of%2520classic%2520natural%250Alanguage%2520tasks.%2520This%2520comes%2520at%2520the%2520cost%2520of%2520losing%2520some%2520degree%2520of%2520instruction%250Afollowing%2520ability%2520as%2520measured%2520by%2520AlpacaEval.%2520Our%2520study%2520shines%2520light%2520on%2520the%250Apotential%2520trade-off%2520between%2520in-context%2520learning%2520and%2520instruction%2520following%250Aabilities%2520that%2520is%2520worth%2520considering%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Instruct%20Models%20for%20Free%3A%20A%20Study%20on%20Partial%20Adaptation&entry.906535625=Ozan%20%C4%B0rsoy%20and%20Pengxiang%20Cheng%20and%20Jennifer%20L.%20Chen%20and%20Daniel%20Preo%C5%A3iuc-Pietro%20and%20Shiyue%20Zhang%20and%20Duccio%20Pappadopulo&entry.1292438233=%20%20Instruct%20models%2C%20obtained%20from%20various%20instruction%20tuning%20or%20post-training%0Asteps%2C%20are%20commonly%20deemed%20superior%20and%20more%20usable%20than%20their%20base%0Acounterpart.%20While%20the%20model%20gains%20instruction%20following%20ability%2C%20instruction%0Atuning%20may%20lead%20to%20forgetting%20the%20knowledge%20from%20pre-training%20or%20it%20may%0Aencourage%20the%20model%20to%20become%20overly%20conversational%20or%20verbose.%20This%2C%20in%20turn%2C%0Acan%20lead%20to%20degradation%20of%20in-context%20few-shot%20learning%20performance.%20In%20this%0Awork%2C%20we%20study%20the%20performance%20trajectory%20between%20base%20and%20instruct%20models%20by%0Ascaling%20down%20the%20strength%20of%20instruction-tuning%20via%20the%20partial%20adaption%0Amethod.%20We%20show%20that%2C%20across%20several%20model%20families%20and%20model%20sizes%2C%20reducing%0Athe%20strength%20of%20instruction-tuning%20results%20in%20material%20improvement%20on%20a%0Afew-shot%20in-context%20learning%20benchmark%20covering%20a%20variety%20of%20classic%20natural%0Alanguage%20tasks.%20This%20comes%20at%20the%20cost%20of%20losing%20some%20degree%20of%20instruction%0Afollowing%20ability%20as%20measured%20by%20AlpacaEval.%20Our%20study%20shines%20light%20on%20the%0Apotential%20trade-off%20between%20in-context%20learning%20and%20instruction%20following%0Aabilities%20that%20is%20worth%20considering%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11626v2&entry.124074799=Read"},
{"title": "A non-smooth regularization framework for learning over multitask graphs", "author": "Yara Zgheib and Luca Calatroni and Marc Antonini and Roula Nassif", "abstract": "  In this work, we consider learning over multitask graphs, where each agent\naims to estimate its own parameter vector. Although agents seek distinct\nobjectives, collaboration among them can be beneficial in scenarios where\nrelationships between tasks exist. Among the various approaches to promoting\nrelationships between tasks and, consequently, enhancing collaboration between\nagents, one notable method is regularization. While previous multitask learning\nstudies have focused on smooth regularization to enforce graph smoothness, this\nwork explores non-smooth regularization techniques that promote sparsity,\nmaking them particularly effective in encouraging piecewise constant\ntransitions on the graph. We begin by formulating a global regularized\noptimization problem, which involves minimizing the aggregate sum of individual\ncosts, regularized by a general non-smooth term designed to promote\npiecewise-constant relationships between the tasks of neighboring agents. Based\non the forward-backward splitting strategy, we propose a decentralized learning\napproach that enables efficient solutions to the regularized optimization\nproblem. Then, under convexity assumptions on the cost functions and\nco-regularization, we establish that the proposed approach converges in the\nmean-square-error sense within $O(\\mu)$ of the optimal solution of the globally\nregularized cost. For broader applicability and improved computational\nefficiency, we also derive closed-form expressions for commonly used non-smooth\n(and, possibly, non-convex) regularizers, such as the weighted sum of the\n$\\ell_0$-norm, $\\ell_1$-norm, and elastic net regularization. Finally, we\nillustrate both the theoretical findings and the effectiveness of the approach\nthrough simulations.\n", "link": "http://arxiv.org/abs/2509.17728v1", "date": "2025-09-22", "relevancy": 2.4968, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5095}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20non-smooth%20regularization%20framework%20for%20learning%20over%20multitask%20graphs&body=Title%3A%20A%20non-smooth%20regularization%20framework%20for%20learning%20over%20multitask%20graphs%0AAuthor%3A%20Yara%20Zgheib%20and%20Luca%20Calatroni%20and%20Marc%20Antonini%20and%20Roula%20Nassif%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20consider%20learning%20over%20multitask%20graphs%2C%20where%20each%20agent%0Aaims%20to%20estimate%20its%20own%20parameter%20vector.%20Although%20agents%20seek%20distinct%0Aobjectives%2C%20collaboration%20among%20them%20can%20be%20beneficial%20in%20scenarios%20where%0Arelationships%20between%20tasks%20exist.%20Among%20the%20various%20approaches%20to%20promoting%0Arelationships%20between%20tasks%20and%2C%20consequently%2C%20enhancing%20collaboration%20between%0Aagents%2C%20one%20notable%20method%20is%20regularization.%20While%20previous%20multitask%20learning%0Astudies%20have%20focused%20on%20smooth%20regularization%20to%20enforce%20graph%20smoothness%2C%20this%0Awork%20explores%20non-smooth%20regularization%20techniques%20that%20promote%20sparsity%2C%0Amaking%20them%20particularly%20effective%20in%20encouraging%20piecewise%20constant%0Atransitions%20on%20the%20graph.%20We%20begin%20by%20formulating%20a%20global%20regularized%0Aoptimization%20problem%2C%20which%20involves%20minimizing%20the%20aggregate%20sum%20of%20individual%0Acosts%2C%20regularized%20by%20a%20general%20non-smooth%20term%20designed%20to%20promote%0Apiecewise-constant%20relationships%20between%20the%20tasks%20of%20neighboring%20agents.%20Based%0Aon%20the%20forward-backward%20splitting%20strategy%2C%20we%20propose%20a%20decentralized%20learning%0Aapproach%20that%20enables%20efficient%20solutions%20to%20the%20regularized%20optimization%0Aproblem.%20Then%2C%20under%20convexity%20assumptions%20on%20the%20cost%20functions%20and%0Aco-regularization%2C%20we%20establish%20that%20the%20proposed%20approach%20converges%20in%20the%0Amean-square-error%20sense%20within%20%24O%28%5Cmu%29%24%20of%20the%20optimal%20solution%20of%20the%20globally%0Aregularized%20cost.%20For%20broader%20applicability%20and%20improved%20computational%0Aefficiency%2C%20we%20also%20derive%20closed-form%20expressions%20for%20commonly%20used%20non-smooth%0A%28and%2C%20possibly%2C%20non-convex%29%20regularizers%2C%20such%20as%20the%20weighted%20sum%20of%20the%0A%24%5Cell_0%24-norm%2C%20%24%5Cell_1%24-norm%2C%20and%20elastic%20net%20regularization.%20Finally%2C%20we%0Aillustrate%20both%20the%20theoretical%20findings%20and%20the%20effectiveness%20of%20the%20approach%0Athrough%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520non-smooth%2520regularization%2520framework%2520for%2520learning%2520over%2520multitask%2520graphs%26entry.906535625%3DYara%2520Zgheib%2520and%2520Luca%2520Calatroni%2520and%2520Marc%2520Antonini%2520and%2520Roula%2520Nassif%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520consider%2520learning%2520over%2520multitask%2520graphs%252C%2520where%2520each%2520agent%250Aaims%2520to%2520estimate%2520its%2520own%2520parameter%2520vector.%2520Although%2520agents%2520seek%2520distinct%250Aobjectives%252C%2520collaboration%2520among%2520them%2520can%2520be%2520beneficial%2520in%2520scenarios%2520where%250Arelationships%2520between%2520tasks%2520exist.%2520Among%2520the%2520various%2520approaches%2520to%2520promoting%250Arelationships%2520between%2520tasks%2520and%252C%2520consequently%252C%2520enhancing%2520collaboration%2520between%250Aagents%252C%2520one%2520notable%2520method%2520is%2520regularization.%2520While%2520previous%2520multitask%2520learning%250Astudies%2520have%2520focused%2520on%2520smooth%2520regularization%2520to%2520enforce%2520graph%2520smoothness%252C%2520this%250Awork%2520explores%2520non-smooth%2520regularization%2520techniques%2520that%2520promote%2520sparsity%252C%250Amaking%2520them%2520particularly%2520effective%2520in%2520encouraging%2520piecewise%2520constant%250Atransitions%2520on%2520the%2520graph.%2520We%2520begin%2520by%2520formulating%2520a%2520global%2520regularized%250Aoptimization%2520problem%252C%2520which%2520involves%2520minimizing%2520the%2520aggregate%2520sum%2520of%2520individual%250Acosts%252C%2520regularized%2520by%2520a%2520general%2520non-smooth%2520term%2520designed%2520to%2520promote%250Apiecewise-constant%2520relationships%2520between%2520the%2520tasks%2520of%2520neighboring%2520agents.%2520Based%250Aon%2520the%2520forward-backward%2520splitting%2520strategy%252C%2520we%2520propose%2520a%2520decentralized%2520learning%250Aapproach%2520that%2520enables%2520efficient%2520solutions%2520to%2520the%2520regularized%2520optimization%250Aproblem.%2520Then%252C%2520under%2520convexity%2520assumptions%2520on%2520the%2520cost%2520functions%2520and%250Aco-regularization%252C%2520we%2520establish%2520that%2520the%2520proposed%2520approach%2520converges%2520in%2520the%250Amean-square-error%2520sense%2520within%2520%2524O%2528%255Cmu%2529%2524%2520of%2520the%2520optimal%2520solution%2520of%2520the%2520globally%250Aregularized%2520cost.%2520For%2520broader%2520applicability%2520and%2520improved%2520computational%250Aefficiency%252C%2520we%2520also%2520derive%2520closed-form%2520expressions%2520for%2520commonly%2520used%2520non-smooth%250A%2528and%252C%2520possibly%252C%2520non-convex%2529%2520regularizers%252C%2520such%2520as%2520the%2520weighted%2520sum%2520of%2520the%250A%2524%255Cell_0%2524-norm%252C%2520%2524%255Cell_1%2524-norm%252C%2520and%2520elastic%2520net%2520regularization.%2520Finally%252C%2520we%250Aillustrate%2520both%2520the%2520theoretical%2520findings%2520and%2520the%2520effectiveness%2520of%2520the%2520approach%250Athrough%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20non-smooth%20regularization%20framework%20for%20learning%20over%20multitask%20graphs&entry.906535625=Yara%20Zgheib%20and%20Luca%20Calatroni%20and%20Marc%20Antonini%20and%20Roula%20Nassif&entry.1292438233=%20%20In%20this%20work%2C%20we%20consider%20learning%20over%20multitask%20graphs%2C%20where%20each%20agent%0Aaims%20to%20estimate%20its%20own%20parameter%20vector.%20Although%20agents%20seek%20distinct%0Aobjectives%2C%20collaboration%20among%20them%20can%20be%20beneficial%20in%20scenarios%20where%0Arelationships%20between%20tasks%20exist.%20Among%20the%20various%20approaches%20to%20promoting%0Arelationships%20between%20tasks%20and%2C%20consequently%2C%20enhancing%20collaboration%20between%0Aagents%2C%20one%20notable%20method%20is%20regularization.%20While%20previous%20multitask%20learning%0Astudies%20have%20focused%20on%20smooth%20regularization%20to%20enforce%20graph%20smoothness%2C%20this%0Awork%20explores%20non-smooth%20regularization%20techniques%20that%20promote%20sparsity%2C%0Amaking%20them%20particularly%20effective%20in%20encouraging%20piecewise%20constant%0Atransitions%20on%20the%20graph.%20We%20begin%20by%20formulating%20a%20global%20regularized%0Aoptimization%20problem%2C%20which%20involves%20minimizing%20the%20aggregate%20sum%20of%20individual%0Acosts%2C%20regularized%20by%20a%20general%20non-smooth%20term%20designed%20to%20promote%0Apiecewise-constant%20relationships%20between%20the%20tasks%20of%20neighboring%20agents.%20Based%0Aon%20the%20forward-backward%20splitting%20strategy%2C%20we%20propose%20a%20decentralized%20learning%0Aapproach%20that%20enables%20efficient%20solutions%20to%20the%20regularized%20optimization%0Aproblem.%20Then%2C%20under%20convexity%20assumptions%20on%20the%20cost%20functions%20and%0Aco-regularization%2C%20we%20establish%20that%20the%20proposed%20approach%20converges%20in%20the%0Amean-square-error%20sense%20within%20%24O%28%5Cmu%29%24%20of%20the%20optimal%20solution%20of%20the%20globally%0Aregularized%20cost.%20For%20broader%20applicability%20and%20improved%20computational%0Aefficiency%2C%20we%20also%20derive%20closed-form%20expressions%20for%20commonly%20used%20non-smooth%0A%28and%2C%20possibly%2C%20non-convex%29%20regularizers%2C%20such%20as%20the%20weighted%20sum%20of%20the%0A%24%5Cell_0%24-norm%2C%20%24%5Cell_1%24-norm%2C%20and%20elastic%20net%20regularization.%20Finally%2C%20we%0Aillustrate%20both%20the%20theoretical%20findings%20and%20the%20effectiveness%20of%20the%20approach%0Athrough%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17728v1&entry.124074799=Read"},
{"title": "SingLEM: Single-Channel Large EEG Model", "author": "Jamiyan Sukhbaatar and Satoshi Imamura and Ibuki Inoue and Shoya Murakami and Kazi Mahmudul Hassan and Seungwoo Han and Ingon Chanpornpakdi and Toshihisa Tanaka", "abstract": "  Current deep learning models for electroencephalography (EEG) are often\ntask-specific and depend on large labeled datasets, limiting their\nadaptability. Although emerging foundation models aim for broader\napplicability, their rigid dependence on fixed, high-density multi-channel\nmontages restricts their use across heterogeneous datasets and in\nmissing-channel or practical low-channel settings. To address these\nlimitations, we introduce SingLEM, a self-supervised foundation model that\nlearns robust, general-purpose representations from single-channel EEG, making\nit inherently hardware agnostic. The model employs a hybrid encoder\narchitecture that combines convolutional layers to extract local features with\na hierarchical transformer to model both short- and long-range temporal\ndependencies. SingLEM is pretrained on 71 public datasets comprising over 9,200\nsubjects and 357,000 single-channel hours of EEG. When evaluated as a fixed\nfeature extractor across six motor imagery and cognitive tasks, aggregated\nsingle-channel representations consistently outperformed leading multi-channel\nfoundation models and handcrafted baselines. These results demonstrate that a\nsingle-channel approach can achieve state-of-the-art generalization while\nenabling fine-grained neurophysiological analysis and enhancing\ninterpretability. The source code and pretrained models are available at\nhttps://github.com/ttlabtuat/SingLEM.\n", "link": "http://arxiv.org/abs/2509.17920v1", "date": "2025-09-22", "relevancy": 2.4927, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SingLEM%3A%20Single-Channel%20Large%20EEG%20Model&body=Title%3A%20SingLEM%3A%20Single-Channel%20Large%20EEG%20Model%0AAuthor%3A%20Jamiyan%20Sukhbaatar%20and%20Satoshi%20Imamura%20and%20Ibuki%20Inoue%20and%20Shoya%20Murakami%20and%20Kazi%20Mahmudul%20Hassan%20and%20Seungwoo%20Han%20and%20Ingon%20Chanpornpakdi%20and%20Toshihisa%20Tanaka%0AAbstract%3A%20%20%20Current%20deep%20learning%20models%20for%20electroencephalography%20%28EEG%29%20are%20often%0Atask-specific%20and%20depend%20on%20large%20labeled%20datasets%2C%20limiting%20their%0Aadaptability.%20Although%20emerging%20foundation%20models%20aim%20for%20broader%0Aapplicability%2C%20their%20rigid%20dependence%20on%20fixed%2C%20high-density%20multi-channel%0Amontages%20restricts%20their%20use%20across%20heterogeneous%20datasets%20and%20in%0Amissing-channel%20or%20practical%20low-channel%20settings.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20SingLEM%2C%20a%20self-supervised%20foundation%20model%20that%0Alearns%20robust%2C%20general-purpose%20representations%20from%20single-channel%20EEG%2C%20making%0Ait%20inherently%20hardware%20agnostic.%20The%20model%20employs%20a%20hybrid%20encoder%0Aarchitecture%20that%20combines%20convolutional%20layers%20to%20extract%20local%20features%20with%0Aa%20hierarchical%20transformer%20to%20model%20both%20short-%20and%20long-range%20temporal%0Adependencies.%20SingLEM%20is%20pretrained%20on%2071%20public%20datasets%20comprising%20over%209%2C200%0Asubjects%20and%20357%2C000%20single-channel%20hours%20of%20EEG.%20When%20evaluated%20as%20a%20fixed%0Afeature%20extractor%20across%20six%20motor%20imagery%20and%20cognitive%20tasks%2C%20aggregated%0Asingle-channel%20representations%20consistently%20outperformed%20leading%20multi-channel%0Afoundation%20models%20and%20handcrafted%20baselines.%20These%20results%20demonstrate%20that%20a%0Asingle-channel%20approach%20can%20achieve%20state-of-the-art%20generalization%20while%0Aenabling%20fine-grained%20neurophysiological%20analysis%20and%20enhancing%0Ainterpretability.%20The%20source%20code%20and%20pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/ttlabtuat/SingLEM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingLEM%253A%2520Single-Channel%2520Large%2520EEG%2520Model%26entry.906535625%3DJamiyan%2520Sukhbaatar%2520and%2520Satoshi%2520Imamura%2520and%2520Ibuki%2520Inoue%2520and%2520Shoya%2520Murakami%2520and%2520Kazi%2520Mahmudul%2520Hassan%2520and%2520Seungwoo%2520Han%2520and%2520Ingon%2520Chanpornpakdi%2520and%2520Toshihisa%2520Tanaka%26entry.1292438233%3D%2520%2520Current%2520deep%2520learning%2520models%2520for%2520electroencephalography%2520%2528EEG%2529%2520are%2520often%250Atask-specific%2520and%2520depend%2520on%2520large%2520labeled%2520datasets%252C%2520limiting%2520their%250Aadaptability.%2520Although%2520emerging%2520foundation%2520models%2520aim%2520for%2520broader%250Aapplicability%252C%2520their%2520rigid%2520dependence%2520on%2520fixed%252C%2520high-density%2520multi-channel%250Amontages%2520restricts%2520their%2520use%2520across%2520heterogeneous%2520datasets%2520and%2520in%250Amissing-channel%2520or%2520practical%2520low-channel%2520settings.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520SingLEM%252C%2520a%2520self-supervised%2520foundation%2520model%2520that%250Alearns%2520robust%252C%2520general-purpose%2520representations%2520from%2520single-channel%2520EEG%252C%2520making%250Ait%2520inherently%2520hardware%2520agnostic.%2520The%2520model%2520employs%2520a%2520hybrid%2520encoder%250Aarchitecture%2520that%2520combines%2520convolutional%2520layers%2520to%2520extract%2520local%2520features%2520with%250Aa%2520hierarchical%2520transformer%2520to%2520model%2520both%2520short-%2520and%2520long-range%2520temporal%250Adependencies.%2520SingLEM%2520is%2520pretrained%2520on%252071%2520public%2520datasets%2520comprising%2520over%25209%252C200%250Asubjects%2520and%2520357%252C000%2520single-channel%2520hours%2520of%2520EEG.%2520When%2520evaluated%2520as%2520a%2520fixed%250Afeature%2520extractor%2520across%2520six%2520motor%2520imagery%2520and%2520cognitive%2520tasks%252C%2520aggregated%250Asingle-channel%2520representations%2520consistently%2520outperformed%2520leading%2520multi-channel%250Afoundation%2520models%2520and%2520handcrafted%2520baselines.%2520These%2520results%2520demonstrate%2520that%2520a%250Asingle-channel%2520approach%2520can%2520achieve%2520state-of-the-art%2520generalization%2520while%250Aenabling%2520fine-grained%2520neurophysiological%2520analysis%2520and%2520enhancing%250Ainterpretability.%2520The%2520source%2520code%2520and%2520pretrained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/ttlabtuat/SingLEM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SingLEM%3A%20Single-Channel%20Large%20EEG%20Model&entry.906535625=Jamiyan%20Sukhbaatar%20and%20Satoshi%20Imamura%20and%20Ibuki%20Inoue%20and%20Shoya%20Murakami%20and%20Kazi%20Mahmudul%20Hassan%20and%20Seungwoo%20Han%20and%20Ingon%20Chanpornpakdi%20and%20Toshihisa%20Tanaka&entry.1292438233=%20%20Current%20deep%20learning%20models%20for%20electroencephalography%20%28EEG%29%20are%20often%0Atask-specific%20and%20depend%20on%20large%20labeled%20datasets%2C%20limiting%20their%0Aadaptability.%20Although%20emerging%20foundation%20models%20aim%20for%20broader%0Aapplicability%2C%20their%20rigid%20dependence%20on%20fixed%2C%20high-density%20multi-channel%0Amontages%20restricts%20their%20use%20across%20heterogeneous%20datasets%20and%20in%0Amissing-channel%20or%20practical%20low-channel%20settings.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20SingLEM%2C%20a%20self-supervised%20foundation%20model%20that%0Alearns%20robust%2C%20general-purpose%20representations%20from%20single-channel%20EEG%2C%20making%0Ait%20inherently%20hardware%20agnostic.%20The%20model%20employs%20a%20hybrid%20encoder%0Aarchitecture%20that%20combines%20convolutional%20layers%20to%20extract%20local%20features%20with%0Aa%20hierarchical%20transformer%20to%20model%20both%20short-%20and%20long-range%20temporal%0Adependencies.%20SingLEM%20is%20pretrained%20on%2071%20public%20datasets%20comprising%20over%209%2C200%0Asubjects%20and%20357%2C000%20single-channel%20hours%20of%20EEG.%20When%20evaluated%20as%20a%20fixed%0Afeature%20extractor%20across%20six%20motor%20imagery%20and%20cognitive%20tasks%2C%20aggregated%0Asingle-channel%20representations%20consistently%20outperformed%20leading%20multi-channel%0Afoundation%20models%20and%20handcrafted%20baselines.%20These%20results%20demonstrate%20that%20a%0Asingle-channel%20approach%20can%20achieve%20state-of-the-art%20generalization%20while%0Aenabling%20fine-grained%20neurophysiological%20analysis%20and%20enhancing%0Ainterpretability.%20The%20source%20code%20and%20pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/ttlabtuat/SingLEM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17920v1&entry.124074799=Read"},
{"title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning", "author": "Huanyu Liu and Jia Li and Chang Yu and Taozhi Chen and Yihong Dong and Lecheng Wang and XiaoLong Hu and Ge Li", "abstract": "  Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on teacher models for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens CoT steps to expand the space in a controlled way. The\nframework enables LLMs to stably learn from initially unsolved hard problems\nunder sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen,\nDeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve\npreviously unsolved problems, improves reasoning capability without external\nCoT supervision, and is compatible with various RL fine-tuning methods. We\nrelease the source code to support future research.\n", "link": "http://arxiv.org/abs/2508.07809v3", "date": "2025-09-22", "relevancy": 2.4867, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoCoT%3A%20Overcoming%20the%20Exploration%20Bottleneck%20in%20Reinforcement%20Learning&body=Title%3A%20EvoCoT%3A%20Overcoming%20the%20Exploration%20Bottleneck%20in%20Reinforcement%20Learning%0AAuthor%3A%20Huanyu%20Liu%20and%20Jia%20Li%20and%20Chang%20Yu%20and%20Taozhi%20Chen%20and%20Yihong%20Dong%20and%20Lecheng%20Wang%20and%20XiaoLong%20Hu%20and%20Ge%20Li%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20reward%20%28RLVR%29%20has%20become%20a%20promising%0Aparadigm%20for%20post-training%20large%20language%20models%20%28LLMs%29%20to%20improve%20their%0Areasoning%20capability.%20However%2C%20when%20the%20rollout%20accuracy%20is%20low%20on%20hard%0Aproblems%2C%20the%20reward%20becomes%20sparse%2C%20limiting%20learning%20efficiency%20and%20causing%0Aexploration%20bottlenecks.%20Existing%20approaches%20either%20rely%20on%20teacher%20models%20for%0Adistillation%20or%20filter%20out%20difficult%20problems%2C%20which%20limits%20scalability%20or%0Arestricts%20reasoning%20improvement%20through%20exploration.%0A%20%20We%20propose%20EvoCoT%2C%20a%20self-evolving%20curriculum%20learning%20framework%20based%20on%0Atwo-stage%20chain-of-thought%20%28CoT%29%20reasoning%20optimization.%20EvoCoT%20constrains%20the%0Aexploration%20space%20by%20self-generating%20and%20verifying%20CoT%20trajectories%2C%20then%0Agradually%20shortens%20CoT%20steps%20to%20expand%20the%20space%20in%20a%20controlled%20way.%20The%0Aframework%20enables%20LLMs%20to%20stably%20learn%20from%20initially%20unsolved%20hard%20problems%0Aunder%20sparse%20rewards.%20We%20apply%20EvoCoT%20to%20multiple%20LLM%20families%2C%20including%20Qwen%2C%0ADeepSeek%2C%20and%20Llama.%20Experiments%20show%20that%20EvoCoT%20enables%20LLMs%20to%20solve%0Apreviously%20unsolved%20problems%2C%20improves%20reasoning%20capability%20without%20external%0ACoT%20supervision%2C%20and%20is%20compatible%20with%20various%20RL%20fine-tuning%20methods.%20We%0Arelease%20the%20source%20code%20to%20support%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07809v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoCoT%253A%2520Overcoming%2520the%2520Exploration%2520Bottleneck%2520in%2520Reinforcement%2520Learning%26entry.906535625%3DHuanyu%2520Liu%2520and%2520Jia%2520Li%2520and%2520Chang%2520Yu%2520and%2520Taozhi%2520Chen%2520and%2520Yihong%2520Dong%2520and%2520Lecheng%2520Wang%2520and%2520XiaoLong%2520Hu%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520reward%2520%2528RLVR%2529%2520has%2520become%2520a%2520promising%250Aparadigm%2520for%2520post-training%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520improve%2520their%250Areasoning%2520capability.%2520However%252C%2520when%2520the%2520rollout%2520accuracy%2520is%2520low%2520on%2520hard%250Aproblems%252C%2520the%2520reward%2520becomes%2520sparse%252C%2520limiting%2520learning%2520efficiency%2520and%2520causing%250Aexploration%2520bottlenecks.%2520Existing%2520approaches%2520either%2520rely%2520on%2520teacher%2520models%2520for%250Adistillation%2520or%2520filter%2520out%2520difficult%2520problems%252C%2520which%2520limits%2520scalability%2520or%250Arestricts%2520reasoning%2520improvement%2520through%2520exploration.%250A%2520%2520We%2520propose%2520EvoCoT%252C%2520a%2520self-evolving%2520curriculum%2520learning%2520framework%2520based%2520on%250Atwo-stage%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520optimization.%2520EvoCoT%2520constrains%2520the%250Aexploration%2520space%2520by%2520self-generating%2520and%2520verifying%2520CoT%2520trajectories%252C%2520then%250Agradually%2520shortens%2520CoT%2520steps%2520to%2520expand%2520the%2520space%2520in%2520a%2520controlled%2520way.%2520The%250Aframework%2520enables%2520LLMs%2520to%2520stably%2520learn%2520from%2520initially%2520unsolved%2520hard%2520problems%250Aunder%2520sparse%2520rewards.%2520We%2520apply%2520EvoCoT%2520to%2520multiple%2520LLM%2520families%252C%2520including%2520Qwen%252C%250ADeepSeek%252C%2520and%2520Llama.%2520Experiments%2520show%2520that%2520EvoCoT%2520enables%2520LLMs%2520to%2520solve%250Apreviously%2520unsolved%2520problems%252C%2520improves%2520reasoning%2520capability%2520without%2520external%250ACoT%2520supervision%252C%2520and%2520is%2520compatible%2520with%2520various%2520RL%2520fine-tuning%2520methods.%2520We%250Arelease%2520the%2520source%2520code%2520to%2520support%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07809v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoCoT%3A%20Overcoming%20the%20Exploration%20Bottleneck%20in%20Reinforcement%20Learning&entry.906535625=Huanyu%20Liu%20and%20Jia%20Li%20and%20Chang%20Yu%20and%20Taozhi%20Chen%20and%20Yihong%20Dong%20and%20Lecheng%20Wang%20and%20XiaoLong%20Hu%20and%20Ge%20Li&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20reward%20%28RLVR%29%20has%20become%20a%20promising%0Aparadigm%20for%20post-training%20large%20language%20models%20%28LLMs%29%20to%20improve%20their%0Areasoning%20capability.%20However%2C%20when%20the%20rollout%20accuracy%20is%20low%20on%20hard%0Aproblems%2C%20the%20reward%20becomes%20sparse%2C%20limiting%20learning%20efficiency%20and%20causing%0Aexploration%20bottlenecks.%20Existing%20approaches%20either%20rely%20on%20teacher%20models%20for%0Adistillation%20or%20filter%20out%20difficult%20problems%2C%20which%20limits%20scalability%20or%0Arestricts%20reasoning%20improvement%20through%20exploration.%0A%20%20We%20propose%20EvoCoT%2C%20a%20self-evolving%20curriculum%20learning%20framework%20based%20on%0Atwo-stage%20chain-of-thought%20%28CoT%29%20reasoning%20optimization.%20EvoCoT%20constrains%20the%0Aexploration%20space%20by%20self-generating%20and%20verifying%20CoT%20trajectories%2C%20then%0Agradually%20shortens%20CoT%20steps%20to%20expand%20the%20space%20in%20a%20controlled%20way.%20The%0Aframework%20enables%20LLMs%20to%20stably%20learn%20from%20initially%20unsolved%20hard%20problems%0Aunder%20sparse%20rewards.%20We%20apply%20EvoCoT%20to%20multiple%20LLM%20families%2C%20including%20Qwen%2C%0ADeepSeek%2C%20and%20Llama.%20Experiments%20show%20that%20EvoCoT%20enables%20LLMs%20to%20solve%0Apreviously%20unsolved%20problems%2C%20improves%20reasoning%20capability%20without%20external%0ACoT%20supervision%2C%20and%20is%20compatible%20with%20various%20RL%20fine-tuning%20methods.%20We%0Arelease%20the%20source%20code%20to%20support%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07809v3&entry.124074799=Read"},
{"title": "Tailored Transformation Invariance for Industrial Anomaly Detection", "author": "Mariette Sch\u00f6nfeld and Wannes Meert and Hendrik Blockeel", "abstract": "  Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision\nAnomaly Detection that has been receiving increasing amounts of attention due\nto its applicability to real-life scenarios. Recent research has focused on how\nto extract the most informative features, contrasting older kNN-based methods\nthat use only pretrained features. These recent methods are much more expensive\nto train however and could complicate real-life application. Careful study of\nrelated work with regards to transformation invariance leads to the idea that\npopular benchmarks require robustness to only minor translations. With this\nidea we then formulate LWinNN, a local window based approach that creates a\nmiddle ground between kNN based methods that have either complete or no\ntranslation invariance. Our experiments demonstrate that this small change\nincreases accuracy considerably, while simultaneously decreasing both train and\ntest time. This teaches us two things: first, the gap between kNN-based\napproaches and more complex state-of-the-art methodology can still be narrowed\nby effective usage of the limited data available. Second, our assumption of\nrequiring only limited translation invariance highlights potential areas of\ninterest for future work and the need for more spatially diverse benchmarks,\nfor which our method can hopefully serve as a new baseline. Our code can be\nfound at https://github.com/marietteschonfeld/LWinNN .\n", "link": "http://arxiv.org/abs/2509.17670v1", "date": "2025-09-22", "relevancy": 2.4859, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4992}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4965}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailored%20Transformation%20Invariance%20for%20Industrial%20Anomaly%20Detection&body=Title%3A%20Tailored%20Transformation%20Invariance%20for%20Industrial%20Anomaly%20Detection%0AAuthor%3A%20Mariette%20Sch%C3%B6nfeld%20and%20Wannes%20Meert%20and%20Hendrik%20Blockeel%0AAbstract%3A%20%20%20Industrial%20Anomaly%20Detection%20%28IAD%29%20is%20a%20subproblem%20within%20Computer%20Vision%0AAnomaly%20Detection%20that%20has%20been%20receiving%20increasing%20amounts%20of%20attention%20due%0Ato%20its%20applicability%20to%20real-life%20scenarios.%20Recent%20research%20has%20focused%20on%20how%0Ato%20extract%20the%20most%20informative%20features%2C%20contrasting%20older%20kNN-based%20methods%0Athat%20use%20only%20pretrained%20features.%20These%20recent%20methods%20are%20much%20more%20expensive%0Ato%20train%20however%20and%20could%20complicate%20real-life%20application.%20Careful%20study%20of%0Arelated%20work%20with%20regards%20to%20transformation%20invariance%20leads%20to%20the%20idea%20that%0Apopular%20benchmarks%20require%20robustness%20to%20only%20minor%20translations.%20With%20this%0Aidea%20we%20then%20formulate%20LWinNN%2C%20a%20local%20window%20based%20approach%20that%20creates%20a%0Amiddle%20ground%20between%20kNN%20based%20methods%20that%20have%20either%20complete%20or%20no%0Atranslation%20invariance.%20Our%20experiments%20demonstrate%20that%20this%20small%20change%0Aincreases%20accuracy%20considerably%2C%20while%20simultaneously%20decreasing%20both%20train%20and%0Atest%20time.%20This%20teaches%20us%20two%20things%3A%20first%2C%20the%20gap%20between%20kNN-based%0Aapproaches%20and%20more%20complex%20state-of-the-art%20methodology%20can%20still%20be%20narrowed%0Aby%20effective%20usage%20of%20the%20limited%20data%20available.%20Second%2C%20our%20assumption%20of%0Arequiring%20only%20limited%20translation%20invariance%20highlights%20potential%20areas%20of%0Ainterest%20for%20future%20work%20and%20the%20need%20for%20more%20spatially%20diverse%20benchmarks%2C%0Afor%20which%20our%20method%20can%20hopefully%20serve%20as%20a%20new%20baseline.%20Our%20code%20can%20be%0Afound%20at%20https%3A//github.com/marietteschonfeld/LWinNN%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailored%2520Transformation%2520Invariance%2520for%2520Industrial%2520Anomaly%2520Detection%26entry.906535625%3DMariette%2520Sch%25C3%25B6nfeld%2520and%2520Wannes%2520Meert%2520and%2520Hendrik%2520Blockeel%26entry.1292438233%3D%2520%2520Industrial%2520Anomaly%2520Detection%2520%2528IAD%2529%2520is%2520a%2520subproblem%2520within%2520Computer%2520Vision%250AAnomaly%2520Detection%2520that%2520has%2520been%2520receiving%2520increasing%2520amounts%2520of%2520attention%2520due%250Ato%2520its%2520applicability%2520to%2520real-life%2520scenarios.%2520Recent%2520research%2520has%2520focused%2520on%2520how%250Ato%2520extract%2520the%2520most%2520informative%2520features%252C%2520contrasting%2520older%2520kNN-based%2520methods%250Athat%2520use%2520only%2520pretrained%2520features.%2520These%2520recent%2520methods%2520are%2520much%2520more%2520expensive%250Ato%2520train%2520however%2520and%2520could%2520complicate%2520real-life%2520application.%2520Careful%2520study%2520of%250Arelated%2520work%2520with%2520regards%2520to%2520transformation%2520invariance%2520leads%2520to%2520the%2520idea%2520that%250Apopular%2520benchmarks%2520require%2520robustness%2520to%2520only%2520minor%2520translations.%2520With%2520this%250Aidea%2520we%2520then%2520formulate%2520LWinNN%252C%2520a%2520local%2520window%2520based%2520approach%2520that%2520creates%2520a%250Amiddle%2520ground%2520between%2520kNN%2520based%2520methods%2520that%2520have%2520either%2520complete%2520or%2520no%250Atranslation%2520invariance.%2520Our%2520experiments%2520demonstrate%2520that%2520this%2520small%2520change%250Aincreases%2520accuracy%2520considerably%252C%2520while%2520simultaneously%2520decreasing%2520both%2520train%2520and%250Atest%2520time.%2520This%2520teaches%2520us%2520two%2520things%253A%2520first%252C%2520the%2520gap%2520between%2520kNN-based%250Aapproaches%2520and%2520more%2520complex%2520state-of-the-art%2520methodology%2520can%2520still%2520be%2520narrowed%250Aby%2520effective%2520usage%2520of%2520the%2520limited%2520data%2520available.%2520Second%252C%2520our%2520assumption%2520of%250Arequiring%2520only%2520limited%2520translation%2520invariance%2520highlights%2520potential%2520areas%2520of%250Ainterest%2520for%2520future%2520work%2520and%2520the%2520need%2520for%2520more%2520spatially%2520diverse%2520benchmarks%252C%250Afor%2520which%2520our%2520method%2520can%2520hopefully%2520serve%2520as%2520a%2520new%2520baseline.%2520Our%2520code%2520can%2520be%250Afound%2520at%2520https%253A//github.com/marietteschonfeld/LWinNN%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailored%20Transformation%20Invariance%20for%20Industrial%20Anomaly%20Detection&entry.906535625=Mariette%20Sch%C3%B6nfeld%20and%20Wannes%20Meert%20and%20Hendrik%20Blockeel&entry.1292438233=%20%20Industrial%20Anomaly%20Detection%20%28IAD%29%20is%20a%20subproblem%20within%20Computer%20Vision%0AAnomaly%20Detection%20that%20has%20been%20receiving%20increasing%20amounts%20of%20attention%20due%0Ato%20its%20applicability%20to%20real-life%20scenarios.%20Recent%20research%20has%20focused%20on%20how%0Ato%20extract%20the%20most%20informative%20features%2C%20contrasting%20older%20kNN-based%20methods%0Athat%20use%20only%20pretrained%20features.%20These%20recent%20methods%20are%20much%20more%20expensive%0Ato%20train%20however%20and%20could%20complicate%20real-life%20application.%20Careful%20study%20of%0Arelated%20work%20with%20regards%20to%20transformation%20invariance%20leads%20to%20the%20idea%20that%0Apopular%20benchmarks%20require%20robustness%20to%20only%20minor%20translations.%20With%20this%0Aidea%20we%20then%20formulate%20LWinNN%2C%20a%20local%20window%20based%20approach%20that%20creates%20a%0Amiddle%20ground%20between%20kNN%20based%20methods%20that%20have%20either%20complete%20or%20no%0Atranslation%20invariance.%20Our%20experiments%20demonstrate%20that%20this%20small%20change%0Aincreases%20accuracy%20considerably%2C%20while%20simultaneously%20decreasing%20both%20train%20and%0Atest%20time.%20This%20teaches%20us%20two%20things%3A%20first%2C%20the%20gap%20between%20kNN-based%0Aapproaches%20and%20more%20complex%20state-of-the-art%20methodology%20can%20still%20be%20narrowed%0Aby%20effective%20usage%20of%20the%20limited%20data%20available.%20Second%2C%20our%20assumption%20of%0Arequiring%20only%20limited%20translation%20invariance%20highlights%20potential%20areas%20of%0Ainterest%20for%20future%20work%20and%20the%20need%20for%20more%20spatially%20diverse%20benchmarks%2C%0Afor%20which%20our%20method%20can%20hopefully%20serve%20as%20a%20new%20baseline.%20Our%20code%20can%20be%0Afound%20at%20https%3A//github.com/marietteschonfeld/LWinNN%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17670v1&entry.124074799=Read"},
{"title": "ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image\n  Generation", "author": "Guocheng Gordon Qian and Daniil Ostashev and Egor Nemchinov and Avihay Assouline and Sergey Tulyakov and Kuan-Chieh Jackson Wang and Kfir Aberman", "abstract": "  Generating high-fidelity images of humans with fine-grained control over\nattributes such as hairstyle and clothing remains a core challenge in\npersonalized text-to-image synthesis. While prior methods emphasize identity\npreservation from a reference image, they lack modularity and fail to provide\ndisentangled control over specific visual attributes. We introduce a new\nparadigm for attribute-specific image prompting, in which distinct sets of\nreference images are used to guide the generation of individual aspects of\nhuman appearance, such as hair, clothing, and identity. Our method encodes\nthese inputs into attribute-specific tokens, which are injected into a\npre-trained text-to-image diffusion model. This enables compositional and\ndisentangled control over multiple visual factors, even across multiple people\nwithin a single image. To promote natural composition and robust\ndisentanglement, we curate a cross-reference training dataset featuring\nsubjects in diverse poses and expressions, and propose a multi-attribute\ncross-reference training strategy that encourages the model to generate\nfaithful outputs from misaligned attribute inputs while adhering to both\nidentity and textual conditioning. Extensive experiments show that our method\nachieves state-of-the-art performance in accurately following both visual and\ntextual prompts. Our framework paves the way for more configurable human image\nsynthesis by combining visual prompting with text-driven generation. Webpage is\navailable at: https://snap-research.github.io/composeme/.\n", "link": "http://arxiv.org/abs/2509.18092v1", "date": "2025-09-22", "relevancy": 2.4768, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6317}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6233}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComposeMe%3A%20Attribute-Specific%20Image%20Prompts%20for%20Controllable%20Human%20Image%0A%20%20Generation&body=Title%3A%20ComposeMe%3A%20Attribute-Specific%20Image%20Prompts%20for%20Controllable%20Human%20Image%0A%20%20Generation%0AAuthor%3A%20Guocheng%20Gordon%20Qian%20and%20Daniil%20Ostashev%20and%20Egor%20Nemchinov%20and%20Avihay%20Assouline%20and%20Sergey%20Tulyakov%20and%20Kuan-Chieh%20Jackson%20Wang%20and%20Kfir%20Aberman%0AAbstract%3A%20%20%20Generating%20high-fidelity%20images%20of%20humans%20with%20fine-grained%20control%20over%0Aattributes%20such%20as%20hairstyle%20and%20clothing%20remains%20a%20core%20challenge%20in%0Apersonalized%20text-to-image%20synthesis.%20While%20prior%20methods%20emphasize%20identity%0Apreservation%20from%20a%20reference%20image%2C%20they%20lack%20modularity%20and%20fail%20to%20provide%0Adisentangled%20control%20over%20specific%20visual%20attributes.%20We%20introduce%20a%20new%0Aparadigm%20for%20attribute-specific%20image%20prompting%2C%20in%20which%20distinct%20sets%20of%0Areference%20images%20are%20used%20to%20guide%20the%20generation%20of%20individual%20aspects%20of%0Ahuman%20appearance%2C%20such%20as%20hair%2C%20clothing%2C%20and%20identity.%20Our%20method%20encodes%0Athese%20inputs%20into%20attribute-specific%20tokens%2C%20which%20are%20injected%20into%20a%0Apre-trained%20text-to-image%20diffusion%20model.%20This%20enables%20compositional%20and%0Adisentangled%20control%20over%20multiple%20visual%20factors%2C%20even%20across%20multiple%20people%0Awithin%20a%20single%20image.%20To%20promote%20natural%20composition%20and%20robust%0Adisentanglement%2C%20we%20curate%20a%20cross-reference%20training%20dataset%20featuring%0Asubjects%20in%20diverse%20poses%20and%20expressions%2C%20and%20propose%20a%20multi-attribute%0Across-reference%20training%20strategy%20that%20encourages%20the%20model%20to%20generate%0Afaithful%20outputs%20from%20misaligned%20attribute%20inputs%20while%20adhering%20to%20both%0Aidentity%20and%20textual%20conditioning.%20Extensive%20experiments%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20performance%20in%20accurately%20following%20both%20visual%20and%0Atextual%20prompts.%20Our%20framework%20paves%20the%20way%20for%20more%20configurable%20human%20image%0Asynthesis%20by%20combining%20visual%20prompting%20with%20text-driven%20generation.%20Webpage%20is%0Aavailable%20at%3A%20https%3A//snap-research.github.io/composeme/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComposeMe%253A%2520Attribute-Specific%2520Image%2520Prompts%2520for%2520Controllable%2520Human%2520Image%250A%2520%2520Generation%26entry.906535625%3DGuocheng%2520Gordon%2520Qian%2520and%2520Daniil%2520Ostashev%2520and%2520Egor%2520Nemchinov%2520and%2520Avihay%2520Assouline%2520and%2520Sergey%2520Tulyakov%2520and%2520Kuan-Chieh%2520Jackson%2520Wang%2520and%2520Kfir%2520Aberman%26entry.1292438233%3D%2520%2520Generating%2520high-fidelity%2520images%2520of%2520humans%2520with%2520fine-grained%2520control%2520over%250Aattributes%2520such%2520as%2520hairstyle%2520and%2520clothing%2520remains%2520a%2520core%2520challenge%2520in%250Apersonalized%2520text-to-image%2520synthesis.%2520While%2520prior%2520methods%2520emphasize%2520identity%250Apreservation%2520from%2520a%2520reference%2520image%252C%2520they%2520lack%2520modularity%2520and%2520fail%2520to%2520provide%250Adisentangled%2520control%2520over%2520specific%2520visual%2520attributes.%2520We%2520introduce%2520a%2520new%250Aparadigm%2520for%2520attribute-specific%2520image%2520prompting%252C%2520in%2520which%2520distinct%2520sets%2520of%250Areference%2520images%2520are%2520used%2520to%2520guide%2520the%2520generation%2520of%2520individual%2520aspects%2520of%250Ahuman%2520appearance%252C%2520such%2520as%2520hair%252C%2520clothing%252C%2520and%2520identity.%2520Our%2520method%2520encodes%250Athese%2520inputs%2520into%2520attribute-specific%2520tokens%252C%2520which%2520are%2520injected%2520into%2520a%250Apre-trained%2520text-to-image%2520diffusion%2520model.%2520This%2520enables%2520compositional%2520and%250Adisentangled%2520control%2520over%2520multiple%2520visual%2520factors%252C%2520even%2520across%2520multiple%2520people%250Awithin%2520a%2520single%2520image.%2520To%2520promote%2520natural%2520composition%2520and%2520robust%250Adisentanglement%252C%2520we%2520curate%2520a%2520cross-reference%2520training%2520dataset%2520featuring%250Asubjects%2520in%2520diverse%2520poses%2520and%2520expressions%252C%2520and%2520propose%2520a%2520multi-attribute%250Across-reference%2520training%2520strategy%2520that%2520encourages%2520the%2520model%2520to%2520generate%250Afaithful%2520outputs%2520from%2520misaligned%2520attribute%2520inputs%2520while%2520adhering%2520to%2520both%250Aidentity%2520and%2520textual%2520conditioning.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520in%2520accurately%2520following%2520both%2520visual%2520and%250Atextual%2520prompts.%2520Our%2520framework%2520paves%2520the%2520way%2520for%2520more%2520configurable%2520human%2520image%250Asynthesis%2520by%2520combining%2520visual%2520prompting%2520with%2520text-driven%2520generation.%2520Webpage%2520is%250Aavailable%2520at%253A%2520https%253A//snap-research.github.io/composeme/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComposeMe%3A%20Attribute-Specific%20Image%20Prompts%20for%20Controllable%20Human%20Image%0A%20%20Generation&entry.906535625=Guocheng%20Gordon%20Qian%20and%20Daniil%20Ostashev%20and%20Egor%20Nemchinov%20and%20Avihay%20Assouline%20and%20Sergey%20Tulyakov%20and%20Kuan-Chieh%20Jackson%20Wang%20and%20Kfir%20Aberman&entry.1292438233=%20%20Generating%20high-fidelity%20images%20of%20humans%20with%20fine-grained%20control%20over%0Aattributes%20such%20as%20hairstyle%20and%20clothing%20remains%20a%20core%20challenge%20in%0Apersonalized%20text-to-image%20synthesis.%20While%20prior%20methods%20emphasize%20identity%0Apreservation%20from%20a%20reference%20image%2C%20they%20lack%20modularity%20and%20fail%20to%20provide%0Adisentangled%20control%20over%20specific%20visual%20attributes.%20We%20introduce%20a%20new%0Aparadigm%20for%20attribute-specific%20image%20prompting%2C%20in%20which%20distinct%20sets%20of%0Areference%20images%20are%20used%20to%20guide%20the%20generation%20of%20individual%20aspects%20of%0Ahuman%20appearance%2C%20such%20as%20hair%2C%20clothing%2C%20and%20identity.%20Our%20method%20encodes%0Athese%20inputs%20into%20attribute-specific%20tokens%2C%20which%20are%20injected%20into%20a%0Apre-trained%20text-to-image%20diffusion%20model.%20This%20enables%20compositional%20and%0Adisentangled%20control%20over%20multiple%20visual%20factors%2C%20even%20across%20multiple%20people%0Awithin%20a%20single%20image.%20To%20promote%20natural%20composition%20and%20robust%0Adisentanglement%2C%20we%20curate%20a%20cross-reference%20training%20dataset%20featuring%0Asubjects%20in%20diverse%20poses%20and%20expressions%2C%20and%20propose%20a%20multi-attribute%0Across-reference%20training%20strategy%20that%20encourages%20the%20model%20to%20generate%0Afaithful%20outputs%20from%20misaligned%20attribute%20inputs%20while%20adhering%20to%20both%0Aidentity%20and%20textual%20conditioning.%20Extensive%20experiments%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20performance%20in%20accurately%20following%20both%20visual%20and%0Atextual%20prompts.%20Our%20framework%20paves%20the%20way%20for%20more%20configurable%20human%20image%0Asynthesis%20by%20combining%20visual%20prompting%20with%20text-driven%20generation.%20Webpage%20is%0Aavailable%20at%3A%20https%3A//snap-research.github.io/composeme/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18092v1&entry.124074799=Read"},
{"title": "Datasets for Fairness in Language Models: An In-Depth Survey", "author": "Jiale Zhang and Zichong Wang and Avash Palikhe and Zhipeng Yin and Wenbin Zhang", "abstract": "  Despite the growing reliance on fairness benchmarks to evaluate language\nmodels, the datasets that underpin these benchmarks remain critically\nunderexamined. This survey addresses that overlooked foundation by offering a\ncomprehensive analysis of the most widely used fairness datasets in language\nmodel research. To ground this analysis, we characterize each dataset across\nkey dimensions, including provenance, demographic scope, annotation design, and\nintended use, revealing the assumptions and limitations baked into current\nevaluation practices. Building on this foundation, we propose a unified\nevaluation framework that surfaces consistent patterns of demographic\ndisparities across benchmarks and scoring metrics. Applying this framework to\nsixteen popular datasets, we uncover overlooked biases that may distort\nconclusions about model fairness and offer guidance on selecting, combining,\nand interpreting these resources more effectively and responsibly. Our findings\nhighlight an urgent need for new benchmarks that capture a broader range of\nsocial contexts and fairness notions. To support future research, we release\nall data, code, and results at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets,\nfostering transparency and reproducibility in the evaluation of language model\nfairness.\n", "link": "http://arxiv.org/abs/2506.23411v2", "date": "2025-09-22", "relevancy": 2.4645, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Datasets%20for%20Fairness%20in%20Language%20Models%3A%20An%20In-Depth%20Survey&body=Title%3A%20Datasets%20for%20Fairness%20in%20Language%20Models%3A%20An%20In-Depth%20Survey%0AAuthor%3A%20Jiale%20Zhang%20and%20Zichong%20Wang%20and%20Avash%20Palikhe%20and%20Zhipeng%20Yin%20and%20Wenbin%20Zhang%0AAbstract%3A%20%20%20Despite%20the%20growing%20reliance%20on%20fairness%20benchmarks%20to%20evaluate%20language%0Amodels%2C%20the%20datasets%20that%20underpin%20these%20benchmarks%20remain%20critically%0Aunderexamined.%20This%20survey%20addresses%20that%20overlooked%20foundation%20by%20offering%20a%0Acomprehensive%20analysis%20of%20the%20most%20widely%20used%20fairness%20datasets%20in%20language%0Amodel%20research.%20To%20ground%20this%20analysis%2C%20we%20characterize%20each%20dataset%20across%0Akey%20dimensions%2C%20including%20provenance%2C%20demographic%20scope%2C%20annotation%20design%2C%20and%0Aintended%20use%2C%20revealing%20the%20assumptions%20and%20limitations%20baked%20into%20current%0Aevaluation%20practices.%20Building%20on%20this%20foundation%2C%20we%20propose%20a%20unified%0Aevaluation%20framework%20that%20surfaces%20consistent%20patterns%20of%20demographic%0Adisparities%20across%20benchmarks%20and%20scoring%20metrics.%20Applying%20this%20framework%20to%0Asixteen%20popular%20datasets%2C%20we%20uncover%20overlooked%20biases%20that%20may%20distort%0Aconclusions%20about%20model%20fairness%20and%20offer%20guidance%20on%20selecting%2C%20combining%2C%0Aand%20interpreting%20these%20resources%20more%20effectively%20and%20responsibly.%20Our%20findings%0Ahighlight%20an%20urgent%20need%20for%20new%20benchmarks%20that%20capture%20a%20broader%20range%20of%0Asocial%20contexts%20and%20fairness%20notions.%20To%20support%20future%20research%2C%20we%20release%0Aall%20data%2C%20code%2C%20and%20results%20at%0Ahttps%3A//github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets%2C%0Afostering%20transparency%20and%20reproducibility%20in%20the%20evaluation%20of%20language%20model%0Afairness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDatasets%2520for%2520Fairness%2520in%2520Language%2520Models%253A%2520An%2520In-Depth%2520Survey%26entry.906535625%3DJiale%2520Zhang%2520and%2520Zichong%2520Wang%2520and%2520Avash%2520Palikhe%2520and%2520Zhipeng%2520Yin%2520and%2520Wenbin%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520the%2520growing%2520reliance%2520on%2520fairness%2520benchmarks%2520to%2520evaluate%2520language%250Amodels%252C%2520the%2520datasets%2520that%2520underpin%2520these%2520benchmarks%2520remain%2520critically%250Aunderexamined.%2520This%2520survey%2520addresses%2520that%2520overlooked%2520foundation%2520by%2520offering%2520a%250Acomprehensive%2520analysis%2520of%2520the%2520most%2520widely%2520used%2520fairness%2520datasets%2520in%2520language%250Amodel%2520research.%2520To%2520ground%2520this%2520analysis%252C%2520we%2520characterize%2520each%2520dataset%2520across%250Akey%2520dimensions%252C%2520including%2520provenance%252C%2520demographic%2520scope%252C%2520annotation%2520design%252C%2520and%250Aintended%2520use%252C%2520revealing%2520the%2520assumptions%2520and%2520limitations%2520baked%2520into%2520current%250Aevaluation%2520practices.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520propose%2520a%2520unified%250Aevaluation%2520framework%2520that%2520surfaces%2520consistent%2520patterns%2520of%2520demographic%250Adisparities%2520across%2520benchmarks%2520and%2520scoring%2520metrics.%2520Applying%2520this%2520framework%2520to%250Asixteen%2520popular%2520datasets%252C%2520we%2520uncover%2520overlooked%2520biases%2520that%2520may%2520distort%250Aconclusions%2520about%2520model%2520fairness%2520and%2520offer%2520guidance%2520on%2520selecting%252C%2520combining%252C%250Aand%2520interpreting%2520these%2520resources%2520more%2520effectively%2520and%2520responsibly.%2520Our%2520findings%250Ahighlight%2520an%2520urgent%2520need%2520for%2520new%2520benchmarks%2520that%2520capture%2520a%2520broader%2520range%2520of%250Asocial%2520contexts%2520and%2520fairness%2520notions.%2520To%2520support%2520future%2520research%252C%2520we%2520release%250Aall%2520data%252C%2520code%252C%2520and%2520results%2520at%250Ahttps%253A//github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets%252C%250Afostering%2520transparency%2520and%2520reproducibility%2520in%2520the%2520evaluation%2520of%2520language%2520model%250Afairness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Datasets%20for%20Fairness%20in%20Language%20Models%3A%20An%20In-Depth%20Survey&entry.906535625=Jiale%20Zhang%20and%20Zichong%20Wang%20and%20Avash%20Palikhe%20and%20Zhipeng%20Yin%20and%20Wenbin%20Zhang&entry.1292438233=%20%20Despite%20the%20growing%20reliance%20on%20fairness%20benchmarks%20to%20evaluate%20language%0Amodels%2C%20the%20datasets%20that%20underpin%20these%20benchmarks%20remain%20critically%0Aunderexamined.%20This%20survey%20addresses%20that%20overlooked%20foundation%20by%20offering%20a%0Acomprehensive%20analysis%20of%20the%20most%20widely%20used%20fairness%20datasets%20in%20language%0Amodel%20research.%20To%20ground%20this%20analysis%2C%20we%20characterize%20each%20dataset%20across%0Akey%20dimensions%2C%20including%20provenance%2C%20demographic%20scope%2C%20annotation%20design%2C%20and%0Aintended%20use%2C%20revealing%20the%20assumptions%20and%20limitations%20baked%20into%20current%0Aevaluation%20practices.%20Building%20on%20this%20foundation%2C%20we%20propose%20a%20unified%0Aevaluation%20framework%20that%20surfaces%20consistent%20patterns%20of%20demographic%0Adisparities%20across%20benchmarks%20and%20scoring%20metrics.%20Applying%20this%20framework%20to%0Asixteen%20popular%20datasets%2C%20we%20uncover%20overlooked%20biases%20that%20may%20distort%0Aconclusions%20about%20model%20fairness%20and%20offer%20guidance%20on%20selecting%2C%20combining%2C%0Aand%20interpreting%20these%20resources%20more%20effectively%20and%20responsibly.%20Our%20findings%0Ahighlight%20an%20urgent%20need%20for%20new%20benchmarks%20that%20capture%20a%20broader%20range%20of%0Asocial%20contexts%20and%20fairness%20notions.%20To%20support%20future%20research%2C%20we%20release%0Aall%20data%2C%20code%2C%20and%20results%20at%0Ahttps%3A//github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets%2C%0Afostering%20transparency%20and%20reproducibility%20in%20the%20evaluation%20of%20language%20model%0Afairness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23411v2&entry.124074799=Read"},
{"title": "Brainprint-Modulated Target Speaker Extraction", "author": "Qiushi Han and Yuan Liao and Youhao Si and Liya Huang", "abstract": "  Achieving robust and personalized performance in neuro-steered Target Speaker\nExtraction (TSE) remains a significant challenge for next-generation hearing\naids. This is primarily due to two factors: the inherent non-stationarity of\nEEG signals across sessions, and the high inter-subject variability that limits\nthe efficacy of generalized models. To address these issues, we propose\nBrainprint-Modulated Target Speaker Extraction (BM-TSE), a novel framework for\npersonalized and high-fidelity extraction. BM-TSE first employs a\nspatio-temporal EEG encoder with an Adaptive Spectral Gain (ASG) module to\nextract stable features resilient to non-stationarity. The core of our\nframework is a personalized modulation mechanism, where a unified brainmap\nembedding is learned under the joint supervision of subject identification\n(SID) and auditory attention decoding (AAD) tasks. This learned brainmap,\nencoding both static user traits and dynamic attentional states, actively\nrefines the audio separation process, dynamically tailoring the output to each\nuser. Evaluations on the public KUL and Cocktail Party datasets demonstrate\nthat BM-TSE achieves state-of-the-art performance, significantly outperforming\nexisting methods. Our code is publicly accessible at:\nhttps://github.com/rosshan-orz/BM-TSE.\n", "link": "http://arxiv.org/abs/2509.17883v1", "date": "2025-09-22", "relevancy": 2.438, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4989}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brainprint-Modulated%20Target%20Speaker%20Extraction&body=Title%3A%20Brainprint-Modulated%20Target%20Speaker%20Extraction%0AAuthor%3A%20Qiushi%20Han%20and%20Yuan%20Liao%20and%20Youhao%20Si%20and%20Liya%20Huang%0AAbstract%3A%20%20%20Achieving%20robust%20and%20personalized%20performance%20in%20neuro-steered%20Target%20Speaker%0AExtraction%20%28TSE%29%20remains%20a%20significant%20challenge%20for%20next-generation%20hearing%0Aaids.%20This%20is%20primarily%20due%20to%20two%20factors%3A%20the%20inherent%20non-stationarity%20of%0AEEG%20signals%20across%20sessions%2C%20and%20the%20high%20inter-subject%20variability%20that%20limits%0Athe%20efficacy%20of%20generalized%20models.%20To%20address%20these%20issues%2C%20we%20propose%0ABrainprint-Modulated%20Target%20Speaker%20Extraction%20%28BM-TSE%29%2C%20a%20novel%20framework%20for%0Apersonalized%20and%20high-fidelity%20extraction.%20BM-TSE%20first%20employs%20a%0Aspatio-temporal%20EEG%20encoder%20with%20an%20Adaptive%20Spectral%20Gain%20%28ASG%29%20module%20to%0Aextract%20stable%20features%20resilient%20to%20non-stationarity.%20The%20core%20of%20our%0Aframework%20is%20a%20personalized%20modulation%20mechanism%2C%20where%20a%20unified%20brainmap%0Aembedding%20is%20learned%20under%20the%20joint%20supervision%20of%20subject%20identification%0A%28SID%29%20and%20auditory%20attention%20decoding%20%28AAD%29%20tasks.%20This%20learned%20brainmap%2C%0Aencoding%20both%20static%20user%20traits%20and%20dynamic%20attentional%20states%2C%20actively%0Arefines%20the%20audio%20separation%20process%2C%20dynamically%20tailoring%20the%20output%20to%20each%0Auser.%20Evaluations%20on%20the%20public%20KUL%20and%20Cocktail%20Party%20datasets%20demonstrate%0Athat%20BM-TSE%20achieves%20state-of-the-art%20performance%2C%20significantly%20outperforming%0Aexisting%20methods.%20Our%20code%20is%20publicly%20accessible%20at%3A%0Ahttps%3A//github.com/rosshan-orz/BM-TSE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainprint-Modulated%2520Target%2520Speaker%2520Extraction%26entry.906535625%3DQiushi%2520Han%2520and%2520Yuan%2520Liao%2520and%2520Youhao%2520Si%2520and%2520Liya%2520Huang%26entry.1292438233%3D%2520%2520Achieving%2520robust%2520and%2520personalized%2520performance%2520in%2520neuro-steered%2520Target%2520Speaker%250AExtraction%2520%2528TSE%2529%2520remains%2520a%2520significant%2520challenge%2520for%2520next-generation%2520hearing%250Aaids.%2520This%2520is%2520primarily%2520due%2520to%2520two%2520factors%253A%2520the%2520inherent%2520non-stationarity%2520of%250AEEG%2520signals%2520across%2520sessions%252C%2520and%2520the%2520high%2520inter-subject%2520variability%2520that%2520limits%250Athe%2520efficacy%2520of%2520generalized%2520models.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ABrainprint-Modulated%2520Target%2520Speaker%2520Extraction%2520%2528BM-TSE%2529%252C%2520a%2520novel%2520framework%2520for%250Apersonalized%2520and%2520high-fidelity%2520extraction.%2520BM-TSE%2520first%2520employs%2520a%250Aspatio-temporal%2520EEG%2520encoder%2520with%2520an%2520Adaptive%2520Spectral%2520Gain%2520%2528ASG%2529%2520module%2520to%250Aextract%2520stable%2520features%2520resilient%2520to%2520non-stationarity.%2520The%2520core%2520of%2520our%250Aframework%2520is%2520a%2520personalized%2520modulation%2520mechanism%252C%2520where%2520a%2520unified%2520brainmap%250Aembedding%2520is%2520learned%2520under%2520the%2520joint%2520supervision%2520of%2520subject%2520identification%250A%2528SID%2529%2520and%2520auditory%2520attention%2520decoding%2520%2528AAD%2529%2520tasks.%2520This%2520learned%2520brainmap%252C%250Aencoding%2520both%2520static%2520user%2520traits%2520and%2520dynamic%2520attentional%2520states%252C%2520actively%250Arefines%2520the%2520audio%2520separation%2520process%252C%2520dynamically%2520tailoring%2520the%2520output%2520to%2520each%250Auser.%2520Evaluations%2520on%2520the%2520public%2520KUL%2520and%2520Cocktail%2520Party%2520datasets%2520demonstrate%250Athat%2520BM-TSE%2520achieves%2520state-of-the-art%2520performance%252C%2520significantly%2520outperforming%250Aexisting%2520methods.%2520Our%2520code%2520is%2520publicly%2520accessible%2520at%253A%250Ahttps%253A//github.com/rosshan-orz/BM-TSE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brainprint-Modulated%20Target%20Speaker%20Extraction&entry.906535625=Qiushi%20Han%20and%20Yuan%20Liao%20and%20Youhao%20Si%20and%20Liya%20Huang&entry.1292438233=%20%20Achieving%20robust%20and%20personalized%20performance%20in%20neuro-steered%20Target%20Speaker%0AExtraction%20%28TSE%29%20remains%20a%20significant%20challenge%20for%20next-generation%20hearing%0Aaids.%20This%20is%20primarily%20due%20to%20two%20factors%3A%20the%20inherent%20non-stationarity%20of%0AEEG%20signals%20across%20sessions%2C%20and%20the%20high%20inter-subject%20variability%20that%20limits%0Athe%20efficacy%20of%20generalized%20models.%20To%20address%20these%20issues%2C%20we%20propose%0ABrainprint-Modulated%20Target%20Speaker%20Extraction%20%28BM-TSE%29%2C%20a%20novel%20framework%20for%0Apersonalized%20and%20high-fidelity%20extraction.%20BM-TSE%20first%20employs%20a%0Aspatio-temporal%20EEG%20encoder%20with%20an%20Adaptive%20Spectral%20Gain%20%28ASG%29%20module%20to%0Aextract%20stable%20features%20resilient%20to%20non-stationarity.%20The%20core%20of%20our%0Aframework%20is%20a%20personalized%20modulation%20mechanism%2C%20where%20a%20unified%20brainmap%0Aembedding%20is%20learned%20under%20the%20joint%20supervision%20of%20subject%20identification%0A%28SID%29%20and%20auditory%20attention%20decoding%20%28AAD%29%20tasks.%20This%20learned%20brainmap%2C%0Aencoding%20both%20static%20user%20traits%20and%20dynamic%20attentional%20states%2C%20actively%0Arefines%20the%20audio%20separation%20process%2C%20dynamically%20tailoring%20the%20output%20to%20each%0Auser.%20Evaluations%20on%20the%20public%20KUL%20and%20Cocktail%20Party%20datasets%20demonstrate%0Athat%20BM-TSE%20achieves%20state-of-the-art%20performance%2C%20significantly%20outperforming%0Aexisting%20methods.%20Our%20code%20is%20publicly%20accessible%20at%3A%0Ahttps%3A//github.com/rosshan-orz/BM-TSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17883v1&entry.124074799=Read"},
{"title": "Accurate and Efficient Low-Rank Model Merging in Core Space", "author": "Aniello Panariello and Daniel Marczak and Simone Magistri and Angelo Porrello and Bart\u0142omiej Twardowski and Andrew D. Bagdanov and Simone Calderara and Joost van de Weijer", "abstract": "  In this paper, we address the challenges associated with merging low-rank\nadaptations of large neural networks. With the rise of parameter-efficient\nadaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning\nhas become more accessible. While fine-tuning models with LoRA is highly\nefficient, existing merging methods often sacrifice this efficiency by merging\nfully-sized weight matrices. We propose the Core Space merging framework, which\nenables the merging of LoRA-adapted models within a common alignment basis,\nthereby preserving the efficiency of low-rank adaptation while substantially\nimproving accuracy across tasks. We further provide a formal proof that\nprojection into Core Space ensures no loss of information and provide a\ncomplexity analysis showing the efficiency gains. Extensive empirical results\ndemonstrate that Core Space significantly improves existing merging techniques\nand achieves state-of-the-art results on both vision and language tasks while\nutilizing a fraction of the computational resources. Codebase is available at\nhttps://github.com/apanariello4/core-space-merging.\n", "link": "http://arxiv.org/abs/2509.17786v1", "date": "2025-09-22", "relevancy": 2.4108, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.495}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4765}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accurate%20and%20Efficient%20Low-Rank%20Model%20Merging%20in%20Core%20Space&body=Title%3A%20Accurate%20and%20Efficient%20Low-Rank%20Model%20Merging%20in%20Core%20Space%0AAuthor%3A%20Aniello%20Panariello%20and%20Daniel%20Marczak%20and%20Simone%20Magistri%20and%20Angelo%20Porrello%20and%20Bart%C5%82omiej%20Twardowski%20and%20Andrew%20D.%20Bagdanov%20and%20Simone%20Calderara%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenges%20associated%20with%20merging%20low-rank%0Aadaptations%20of%20large%20neural%20networks.%20With%20the%20rise%20of%20parameter-efficient%0Aadaptation%20techniques%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20model%20fine-tuning%0Ahas%20become%20more%20accessible.%20While%20fine-tuning%20models%20with%20LoRA%20is%20highly%0Aefficient%2C%20existing%20merging%20methods%20often%20sacrifice%20this%20efficiency%20by%20merging%0Afully-sized%20weight%20matrices.%20We%20propose%20the%20Core%20Space%20merging%20framework%2C%20which%0Aenables%20the%20merging%20of%20LoRA-adapted%20models%20within%20a%20common%20alignment%20basis%2C%0Athereby%20preserving%20the%20efficiency%20of%20low-rank%20adaptation%20while%20substantially%0Aimproving%20accuracy%20across%20tasks.%20We%20further%20provide%20a%20formal%20proof%20that%0Aprojection%20into%20Core%20Space%20ensures%20no%20loss%20of%20information%20and%20provide%20a%0Acomplexity%20analysis%20showing%20the%20efficiency%20gains.%20Extensive%20empirical%20results%0Ademonstrate%20that%20Core%20Space%20significantly%20improves%20existing%20merging%20techniques%0Aand%20achieves%20state-of-the-art%20results%20on%20both%20vision%20and%20language%20tasks%20while%0Autilizing%20a%20fraction%20of%20the%20computational%20resources.%20Codebase%20is%20available%20at%0Ahttps%3A//github.com/apanariello4/core-space-merging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccurate%2520and%2520Efficient%2520Low-Rank%2520Model%2520Merging%2520in%2520Core%2520Space%26entry.906535625%3DAniello%2520Panariello%2520and%2520Daniel%2520Marczak%2520and%2520Simone%2520Magistri%2520and%2520Angelo%2520Porrello%2520and%2520Bart%25C5%2582omiej%2520Twardowski%2520and%2520Andrew%2520D.%2520Bagdanov%2520and%2520Simone%2520Calderara%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenges%2520associated%2520with%2520merging%2520low-rank%250Aadaptations%2520of%2520large%2520neural%2520networks.%2520With%2520the%2520rise%2520of%2520parameter-efficient%250Aadaptation%2520techniques%252C%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520model%2520fine-tuning%250Ahas%2520become%2520more%2520accessible.%2520While%2520fine-tuning%2520models%2520with%2520LoRA%2520is%2520highly%250Aefficient%252C%2520existing%2520merging%2520methods%2520often%2520sacrifice%2520this%2520efficiency%2520by%2520merging%250Afully-sized%2520weight%2520matrices.%2520We%2520propose%2520the%2520Core%2520Space%2520merging%2520framework%252C%2520which%250Aenables%2520the%2520merging%2520of%2520LoRA-adapted%2520models%2520within%2520a%2520common%2520alignment%2520basis%252C%250Athereby%2520preserving%2520the%2520efficiency%2520of%2520low-rank%2520adaptation%2520while%2520substantially%250Aimproving%2520accuracy%2520across%2520tasks.%2520We%2520further%2520provide%2520a%2520formal%2520proof%2520that%250Aprojection%2520into%2520Core%2520Space%2520ensures%2520no%2520loss%2520of%2520information%2520and%2520provide%2520a%250Acomplexity%2520analysis%2520showing%2520the%2520efficiency%2520gains.%2520Extensive%2520empirical%2520results%250Ademonstrate%2520that%2520Core%2520Space%2520significantly%2520improves%2520existing%2520merging%2520techniques%250Aand%2520achieves%2520state-of-the-art%2520results%2520on%2520both%2520vision%2520and%2520language%2520tasks%2520while%250Autilizing%2520a%2520fraction%2520of%2520the%2520computational%2520resources.%2520Codebase%2520is%2520available%2520at%250Ahttps%253A//github.com/apanariello4/core-space-merging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20and%20Efficient%20Low-Rank%20Model%20Merging%20in%20Core%20Space&entry.906535625=Aniello%20Panariello%20and%20Daniel%20Marczak%20and%20Simone%20Magistri%20and%20Angelo%20Porrello%20and%20Bart%C5%82omiej%20Twardowski%20and%20Andrew%20D.%20Bagdanov%20and%20Simone%20Calderara%20and%20Joost%20van%20de%20Weijer&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenges%20associated%20with%20merging%20low-rank%0Aadaptations%20of%20large%20neural%20networks.%20With%20the%20rise%20of%20parameter-efficient%0Aadaptation%20techniques%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20model%20fine-tuning%0Ahas%20become%20more%20accessible.%20While%20fine-tuning%20models%20with%20LoRA%20is%20highly%0Aefficient%2C%20existing%20merging%20methods%20often%20sacrifice%20this%20efficiency%20by%20merging%0Afully-sized%20weight%20matrices.%20We%20propose%20the%20Core%20Space%20merging%20framework%2C%20which%0Aenables%20the%20merging%20of%20LoRA-adapted%20models%20within%20a%20common%20alignment%20basis%2C%0Athereby%20preserving%20the%20efficiency%20of%20low-rank%20adaptation%20while%20substantially%0Aimproving%20accuracy%20across%20tasks.%20We%20further%20provide%20a%20formal%20proof%20that%0Aprojection%20into%20Core%20Space%20ensures%20no%20loss%20of%20information%20and%20provide%20a%0Acomplexity%20analysis%20showing%20the%20efficiency%20gains.%20Extensive%20empirical%20results%0Ademonstrate%20that%20Core%20Space%20significantly%20improves%20existing%20merging%20techniques%0Aand%20achieves%20state-of-the-art%20results%20on%20both%20vision%20and%20language%20tasks%20while%0Autilizing%20a%20fraction%20of%20the%20computational%20resources.%20Codebase%20is%20available%20at%0Ahttps%3A//github.com/apanariello4/core-space-merging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17786v1&entry.124074799=Read"},
{"title": "How Good are Foundation Models in Step-by-Step Embodied Reasoning?", "author": "Dinura Dissanayake and Ahmed Heakl and Omkar Thawakar and Noor Ahsan and Ritesh Thawkar and Ketan More and Jean Lahoud and Rao Anwer and Hisham Cholakkal and Ivan Laptev and Fahad Shahbaz Khan and Salman Khan", "abstract": "  Embodied agents operating in the physical world must make decisions that are\nnot only effective but also safe, spatially coherent, and grounded in context.\nWhile recent advances in large multimodal models (LMMs) have shown promising\ncapabilities in visual understanding and language generation, their ability to\nperform structured reasoning for real-world embodied tasks remains\nunderexplored. In this work, we aim to understand how well foundation models\ncan perform step-by-step reasoning in embodied environments. To this end, we\npropose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to\nevaluate the reasoning capabilities of LMMs in complex embodied decision-making\nscenarios. Our benchmark spans a diverse set of tasks that require agents to\ninterpret multimodal observations, reason about physical constraints and\nsafety, and generate valid next actions in natural language. We present (i) a\nlarge-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation\nframework that disentangles perceptual grounding from action reasoning, and\n(iii) empirical analysis of several leading LMMs under this setting. Our\nbenchmark includes over 1.1k samples with detailed step-by-step reasoning\nacross 10 tasks and 8 embodiments, covering three different robot types. Our\nresults highlight both the potential and current limitations of LMMs in\nembodied reasoning, pointing towards key challenges and opportunities for\nfuture research in robot intelligence. Our data and code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2509.15293v2", "date": "2025-09-22", "relevancy": 2.4056, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Good%20are%20Foundation%20Models%20in%20Step-by-Step%20Embodied%20Reasoning%3F&body=Title%3A%20How%20Good%20are%20Foundation%20Models%20in%20Step-by-Step%20Embodied%20Reasoning%3F%0AAuthor%3A%20Dinura%20Dissanayake%20and%20Ahmed%20Heakl%20and%20Omkar%20Thawakar%20and%20Noor%20Ahsan%20and%20Ritesh%20Thawkar%20and%20Ketan%20More%20and%20Jean%20Lahoud%20and%20Rao%20Anwer%20and%20Hisham%20Cholakkal%20and%20Ivan%20Laptev%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Embodied%20agents%20operating%20in%20the%20physical%20world%20must%20make%20decisions%20that%20are%0Anot%20only%20effective%20but%20also%20safe%2C%20spatially%20coherent%2C%20and%20grounded%20in%20context.%0AWhile%20recent%20advances%20in%20large%20multimodal%20models%20%28LMMs%29%20have%20shown%20promising%0Acapabilities%20in%20visual%20understanding%20and%20language%20generation%2C%20their%20ability%20to%0Aperform%20structured%20reasoning%20for%20real-world%20embodied%20tasks%20remains%0Aunderexplored.%20In%20this%20work%2C%20we%20aim%20to%20understand%20how%20well%20foundation%20models%0Acan%20perform%20step-by-step%20reasoning%20in%20embodied%20environments.%20To%20this%20end%2C%20we%0Apropose%20the%20Foundation%20Model%20Embodied%20Reasoning%20%28FoMER%29%20benchmark%2C%20designed%20to%0Aevaluate%20the%20reasoning%20capabilities%20of%20LMMs%20in%20complex%20embodied%20decision-making%0Ascenarios.%20Our%20benchmark%20spans%20a%20diverse%20set%20of%20tasks%20that%20require%20agents%20to%0Ainterpret%20multimodal%20observations%2C%20reason%20about%20physical%20constraints%20and%0Asafety%2C%20and%20generate%20valid%20next%20actions%20in%20natural%20language.%20We%20present%20%28i%29%20a%0Alarge-scale%2C%20curated%20suite%20of%20embodied%20reasoning%20tasks%2C%20%28ii%29%20a%20novel%20evaluation%0Aframework%20that%20disentangles%20perceptual%20grounding%20from%20action%20reasoning%2C%20and%0A%28iii%29%20empirical%20analysis%20of%20several%20leading%20LMMs%20under%20this%20setting.%20Our%0Abenchmark%20includes%20over%201.1k%20samples%20with%20detailed%20step-by-step%20reasoning%0Aacross%2010%20tasks%20and%208%20embodiments%2C%20covering%20three%20different%20robot%20types.%20Our%0Aresults%20highlight%20both%20the%20potential%20and%20current%20limitations%20of%20LMMs%20in%0Aembodied%20reasoning%2C%20pointing%20towards%20key%20challenges%20and%20opportunities%20for%0Afuture%20research%20in%20robot%20intelligence.%20Our%20data%20and%20code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Good%2520are%2520Foundation%2520Models%2520in%2520Step-by-Step%2520Embodied%2520Reasoning%253F%26entry.906535625%3DDinura%2520Dissanayake%2520and%2520Ahmed%2520Heakl%2520and%2520Omkar%2520Thawakar%2520and%2520Noor%2520Ahsan%2520and%2520Ritesh%2520Thawkar%2520and%2520Ketan%2520More%2520and%2520Jean%2520Lahoud%2520and%2520Rao%2520Anwer%2520and%2520Hisham%2520Cholakkal%2520and%2520Ivan%2520Laptev%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Embodied%2520agents%2520operating%2520in%2520the%2520physical%2520world%2520must%2520make%2520decisions%2520that%2520are%250Anot%2520only%2520effective%2520but%2520also%2520safe%252C%2520spatially%2520coherent%252C%2520and%2520grounded%2520in%2520context.%250AWhile%2520recent%2520advances%2520in%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520shown%2520promising%250Acapabilities%2520in%2520visual%2520understanding%2520and%2520language%2520generation%252C%2520their%2520ability%2520to%250Aperform%2520structured%2520reasoning%2520for%2520real-world%2520embodied%2520tasks%2520remains%250Aunderexplored.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520understand%2520how%2520well%2520foundation%2520models%250Acan%2520perform%2520step-by-step%2520reasoning%2520in%2520embodied%2520environments.%2520To%2520this%2520end%252C%2520we%250Apropose%2520the%2520Foundation%2520Model%2520Embodied%2520Reasoning%2520%2528FoMER%2529%2520benchmark%252C%2520designed%2520to%250Aevaluate%2520the%2520reasoning%2520capabilities%2520of%2520LMMs%2520in%2520complex%2520embodied%2520decision-making%250Ascenarios.%2520Our%2520benchmark%2520spans%2520a%2520diverse%2520set%2520of%2520tasks%2520that%2520require%2520agents%2520to%250Ainterpret%2520multimodal%2520observations%252C%2520reason%2520about%2520physical%2520constraints%2520and%250Asafety%252C%2520and%2520generate%2520valid%2520next%2520actions%2520in%2520natural%2520language.%2520We%2520present%2520%2528i%2529%2520a%250Alarge-scale%252C%2520curated%2520suite%2520of%2520embodied%2520reasoning%2520tasks%252C%2520%2528ii%2529%2520a%2520novel%2520evaluation%250Aframework%2520that%2520disentangles%2520perceptual%2520grounding%2520from%2520action%2520reasoning%252C%2520and%250A%2528iii%2529%2520empirical%2520analysis%2520of%2520several%2520leading%2520LMMs%2520under%2520this%2520setting.%2520Our%250Abenchmark%2520includes%2520over%25201.1k%2520samples%2520with%2520detailed%2520step-by-step%2520reasoning%250Aacross%252010%2520tasks%2520and%25208%2520embodiments%252C%2520covering%2520three%2520different%2520robot%2520types.%2520Our%250Aresults%2520highlight%2520both%2520the%2520potential%2520and%2520current%2520limitations%2520of%2520LMMs%2520in%250Aembodied%2520reasoning%252C%2520pointing%2520towards%2520key%2520challenges%2520and%2520opportunities%2520for%250Afuture%2520research%2520in%2520robot%2520intelligence.%2520Our%2520data%2520and%2520code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Good%20are%20Foundation%20Models%20in%20Step-by-Step%20Embodied%20Reasoning%3F&entry.906535625=Dinura%20Dissanayake%20and%20Ahmed%20Heakl%20and%20Omkar%20Thawakar%20and%20Noor%20Ahsan%20and%20Ritesh%20Thawkar%20and%20Ketan%20More%20and%20Jean%20Lahoud%20and%20Rao%20Anwer%20and%20Hisham%20Cholakkal%20and%20Ivan%20Laptev%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Embodied%20agents%20operating%20in%20the%20physical%20world%20must%20make%20decisions%20that%20are%0Anot%20only%20effective%20but%20also%20safe%2C%20spatially%20coherent%2C%20and%20grounded%20in%20context.%0AWhile%20recent%20advances%20in%20large%20multimodal%20models%20%28LMMs%29%20have%20shown%20promising%0Acapabilities%20in%20visual%20understanding%20and%20language%20generation%2C%20their%20ability%20to%0Aperform%20structured%20reasoning%20for%20real-world%20embodied%20tasks%20remains%0Aunderexplored.%20In%20this%20work%2C%20we%20aim%20to%20understand%20how%20well%20foundation%20models%0Acan%20perform%20step-by-step%20reasoning%20in%20embodied%20environments.%20To%20this%20end%2C%20we%0Apropose%20the%20Foundation%20Model%20Embodied%20Reasoning%20%28FoMER%29%20benchmark%2C%20designed%20to%0Aevaluate%20the%20reasoning%20capabilities%20of%20LMMs%20in%20complex%20embodied%20decision-making%0Ascenarios.%20Our%20benchmark%20spans%20a%20diverse%20set%20of%20tasks%20that%20require%20agents%20to%0Ainterpret%20multimodal%20observations%2C%20reason%20about%20physical%20constraints%20and%0Asafety%2C%20and%20generate%20valid%20next%20actions%20in%20natural%20language.%20We%20present%20%28i%29%20a%0Alarge-scale%2C%20curated%20suite%20of%20embodied%20reasoning%20tasks%2C%20%28ii%29%20a%20novel%20evaluation%0Aframework%20that%20disentangles%20perceptual%20grounding%20from%20action%20reasoning%2C%20and%0A%28iii%29%20empirical%20analysis%20of%20several%20leading%20LMMs%20under%20this%20setting.%20Our%0Abenchmark%20includes%20over%201.1k%20samples%20with%20detailed%20step-by-step%20reasoning%0Aacross%2010%20tasks%20and%208%20embodiments%2C%20covering%20three%20different%20robot%20types.%20Our%0Aresults%20highlight%20both%20the%20potential%20and%20current%20limitations%20of%20LMMs%20in%0Aembodied%20reasoning%2C%20pointing%20towards%20key%20challenges%20and%20opportunities%20for%0Afuture%20research%20in%20robot%20intelligence.%20Our%20data%20and%20code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15293v2&entry.124074799=Read"},
{"title": "Whitening Spherical Gaussian Mixtures in the Large-Dimensional Regime", "author": "Mohammed Racim Moussa Boudjemaa and Alper Kalle and Xiaoyi Mai and Jos\u00e9 Henrique de Morais Goulart and C\u00e9dric F\u00e9votte", "abstract": "  Whitening is a classical technique in unsupervised learning that can\nfacilitate estimation tasks by standardizing data. An important application is\nthe estimation of latent variable models via the decomposition of tensors built\nfrom high-order moments. In particular, whitening orthogonalizes the means of a\nspherical Gaussian mixture model (GMM), thereby making the corresponding moment\ntensor orthogonally decomposable, hence easier to decompose. However, in the\nlarge-dimensional regime (LDR) where data are high-dimensional and scarce, the\nstandard whitening matrix built from the sample covariance becomes ineffective\nbecause the latter is spectrally distorted. Consequently, whitened means of a\nspherical GMM are no longer orthogonal. Using random matrix theory, we derive\nexact limits for their dot products, which are generally nonzero in the LDR. As\nour main contribution, we then construct a corrected whitening matrix that\nrestores asymptotic orthogonality, allowing for performance gains in spherical\nGMM estimation.\n", "link": "http://arxiv.org/abs/2509.17636v1", "date": "2025-09-22", "relevancy": 2.3959, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4858}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4806}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whitening%20Spherical%20Gaussian%20Mixtures%20in%20the%20Large-Dimensional%20Regime&body=Title%3A%20Whitening%20Spherical%20Gaussian%20Mixtures%20in%20the%20Large-Dimensional%20Regime%0AAuthor%3A%20Mohammed%20Racim%20Moussa%20Boudjemaa%20and%20Alper%20Kalle%20and%20Xiaoyi%20Mai%20and%20Jos%C3%A9%20Henrique%20de%20Morais%20Goulart%20and%20C%C3%A9dric%20F%C3%A9votte%0AAbstract%3A%20%20%20Whitening%20is%20a%20classical%20technique%20in%20unsupervised%20learning%20that%20can%0Afacilitate%20estimation%20tasks%20by%20standardizing%20data.%20An%20important%20application%20is%0Athe%20estimation%20of%20latent%20variable%20models%20via%20the%20decomposition%20of%20tensors%20built%0Afrom%20high-order%20moments.%20In%20particular%2C%20whitening%20orthogonalizes%20the%20means%20of%20a%0Aspherical%20Gaussian%20mixture%20model%20%28GMM%29%2C%20thereby%20making%20the%20corresponding%20moment%0Atensor%20orthogonally%20decomposable%2C%20hence%20easier%20to%20decompose.%20However%2C%20in%20the%0Alarge-dimensional%20regime%20%28LDR%29%20where%20data%20are%20high-dimensional%20and%20scarce%2C%20the%0Astandard%20whitening%20matrix%20built%20from%20the%20sample%20covariance%20becomes%20ineffective%0Abecause%20the%20latter%20is%20spectrally%20distorted.%20Consequently%2C%20whitened%20means%20of%20a%0Aspherical%20GMM%20are%20no%20longer%20orthogonal.%20Using%20random%20matrix%20theory%2C%20we%20derive%0Aexact%20limits%20for%20their%20dot%20products%2C%20which%20are%20generally%20nonzero%20in%20the%20LDR.%20As%0Aour%20main%20contribution%2C%20we%20then%20construct%20a%20corrected%20whitening%20matrix%20that%0Arestores%20asymptotic%20orthogonality%2C%20allowing%20for%20performance%20gains%20in%20spherical%0AGMM%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhitening%2520Spherical%2520Gaussian%2520Mixtures%2520in%2520the%2520Large-Dimensional%2520Regime%26entry.906535625%3DMohammed%2520Racim%2520Moussa%2520Boudjemaa%2520and%2520Alper%2520Kalle%2520and%2520Xiaoyi%2520Mai%2520and%2520Jos%25C3%25A9%2520Henrique%2520de%2520Morais%2520Goulart%2520and%2520C%25C3%25A9dric%2520F%25C3%25A9votte%26entry.1292438233%3D%2520%2520Whitening%2520is%2520a%2520classical%2520technique%2520in%2520unsupervised%2520learning%2520that%2520can%250Afacilitate%2520estimation%2520tasks%2520by%2520standardizing%2520data.%2520An%2520important%2520application%2520is%250Athe%2520estimation%2520of%2520latent%2520variable%2520models%2520via%2520the%2520decomposition%2520of%2520tensors%2520built%250Afrom%2520high-order%2520moments.%2520In%2520particular%252C%2520whitening%2520orthogonalizes%2520the%2520means%2520of%2520a%250Aspherical%2520Gaussian%2520mixture%2520model%2520%2528GMM%2529%252C%2520thereby%2520making%2520the%2520corresponding%2520moment%250Atensor%2520orthogonally%2520decomposable%252C%2520hence%2520easier%2520to%2520decompose.%2520However%252C%2520in%2520the%250Alarge-dimensional%2520regime%2520%2528LDR%2529%2520where%2520data%2520are%2520high-dimensional%2520and%2520scarce%252C%2520the%250Astandard%2520whitening%2520matrix%2520built%2520from%2520the%2520sample%2520covariance%2520becomes%2520ineffective%250Abecause%2520the%2520latter%2520is%2520spectrally%2520distorted.%2520Consequently%252C%2520whitened%2520means%2520of%2520a%250Aspherical%2520GMM%2520are%2520no%2520longer%2520orthogonal.%2520Using%2520random%2520matrix%2520theory%252C%2520we%2520derive%250Aexact%2520limits%2520for%2520their%2520dot%2520products%252C%2520which%2520are%2520generally%2520nonzero%2520in%2520the%2520LDR.%2520As%250Aour%2520main%2520contribution%252C%2520we%2520then%2520construct%2520a%2520corrected%2520whitening%2520matrix%2520that%250Arestores%2520asymptotic%2520orthogonality%252C%2520allowing%2520for%2520performance%2520gains%2520in%2520spherical%250AGMM%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whitening%20Spherical%20Gaussian%20Mixtures%20in%20the%20Large-Dimensional%20Regime&entry.906535625=Mohammed%20Racim%20Moussa%20Boudjemaa%20and%20Alper%20Kalle%20and%20Xiaoyi%20Mai%20and%20Jos%C3%A9%20Henrique%20de%20Morais%20Goulart%20and%20C%C3%A9dric%20F%C3%A9votte&entry.1292438233=%20%20Whitening%20is%20a%20classical%20technique%20in%20unsupervised%20learning%20that%20can%0Afacilitate%20estimation%20tasks%20by%20standardizing%20data.%20An%20important%20application%20is%0Athe%20estimation%20of%20latent%20variable%20models%20via%20the%20decomposition%20of%20tensors%20built%0Afrom%20high-order%20moments.%20In%20particular%2C%20whitening%20orthogonalizes%20the%20means%20of%20a%0Aspherical%20Gaussian%20mixture%20model%20%28GMM%29%2C%20thereby%20making%20the%20corresponding%20moment%0Atensor%20orthogonally%20decomposable%2C%20hence%20easier%20to%20decompose.%20However%2C%20in%20the%0Alarge-dimensional%20regime%20%28LDR%29%20where%20data%20are%20high-dimensional%20and%20scarce%2C%20the%0Astandard%20whitening%20matrix%20built%20from%20the%20sample%20covariance%20becomes%20ineffective%0Abecause%20the%20latter%20is%20spectrally%20distorted.%20Consequently%2C%20whitened%20means%20of%20a%0Aspherical%20GMM%20are%20no%20longer%20orthogonal.%20Using%20random%20matrix%20theory%2C%20we%20derive%0Aexact%20limits%20for%20their%20dot%20products%2C%20which%20are%20generally%20nonzero%20in%20the%20LDR.%20As%0Aour%20main%20contribution%2C%20we%20then%20construct%20a%20corrected%20whitening%20matrix%20that%0Arestores%20asymptotic%20orthogonality%2C%20allowing%20for%20performance%20gains%20in%20spherical%0AGMM%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17636v1&entry.124074799=Read"},
{"title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large\n  Language Models", "author": "Zhentao He and Can Zhang and Ziheng Wu and Zhenghao Chen and Yufei Zhan and Yifan Li and Zhao Zhang and Xian Wang and Minghui Qiu", "abstract": "  Recent advancements in multimodal large language models have enhanced\ndocument understanding by integrating textual and visual information. However,\nexisting models exhibit incompleteness within their paradigm in real-world\nscenarios, particularly under visual degradation. In such conditions, the\ncurrent response paradigm often fails to adequately perceive visual degradation\nand ambiguity, leading to overreliance on linguistic priors or misaligned\nvisual-textual reasoning. This difficulty in recognizing uncertainty frequently\nresults in the generation of hallucinatory content, especially when a precise\nanswer is not feasible. To better demonstrate and analyze this phenomenon and\nproblem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR\nhallucination in degraded document understanding. This dataset includes test\nsamples spanning identity cards and invoices, with simulated real-world\ndegradations for OCR reliability. This setup allows for evaluating models'\ncapacity, under degraded input, to distinguish reliable visual information and\nanswer accordingly, thereby highlighting the challenge of avoiding\nhallucination on uncertain data. To achieve vision-faithful reasoning and\nthereby avoid the aforementioned issues, we further introduce a GRPO-based\nframework featuring a novel reward mechanism. By incorporating a self-awareness\nof visual uncertainty and an analysis method that initiates refusal to answer\nto increase task difficulty within our supervised fine-tuning and reinforcement\nlearning framework, we successfully mitigated hallucinations in ambiguous\nregions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model\nachieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o\non KIE-HVQA and there is no significant performance drop in standard tasks,\nhighlighting both effectiveness and robustness.\n", "link": "http://arxiv.org/abs/2506.20168v2", "date": "2025-09-22", "relevancy": 2.3932, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20is%20Believing%3F%20Mitigating%20OCR%20Hallucinations%20in%20Multimodal%20Large%0A%20%20Language%20Models&body=Title%3A%20Seeing%20is%20Believing%3F%20Mitigating%20OCR%20Hallucinations%20in%20Multimodal%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Zhentao%20He%20and%20Can%20Zhang%20and%20Ziheng%20Wu%20and%20Zhenghao%20Chen%20and%20Yufei%20Zhan%20and%20Yifan%20Li%20and%20Zhao%20Zhang%20and%20Xian%20Wang%20and%20Minghui%20Qiu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20have%20enhanced%0Adocument%20understanding%20by%20integrating%20textual%20and%20visual%20information.%20However%2C%0Aexisting%20models%20exhibit%20incompleteness%20within%20their%20paradigm%20in%20real-world%0Ascenarios%2C%20particularly%20under%20visual%20degradation.%20In%20such%20conditions%2C%20the%0Acurrent%20response%20paradigm%20often%20fails%20to%20adequately%20perceive%20visual%20degradation%0Aand%20ambiguity%2C%20leading%20to%20overreliance%20on%20linguistic%20priors%20or%20misaligned%0Avisual-textual%20reasoning.%20This%20difficulty%20in%20recognizing%20uncertainty%20frequently%0Aresults%20in%20the%20generation%20of%20hallucinatory%20content%2C%20especially%20when%20a%20precise%0Aanswer%20is%20not%20feasible.%20To%20better%20demonstrate%20and%20analyze%20this%20phenomenon%20and%0Aproblem%2C%20we%20propose%20KIE-HVQA%2C%20the%20first%20benchmark%20dedicated%20to%20evaluating%20OCR%0Ahallucination%20in%20degraded%20document%20understanding.%20This%20dataset%20includes%20test%0Asamples%20spanning%20identity%20cards%20and%20invoices%2C%20with%20simulated%20real-world%0Adegradations%20for%20OCR%20reliability.%20This%20setup%20allows%20for%20evaluating%20models%27%0Acapacity%2C%20under%20degraded%20input%2C%20to%20distinguish%20reliable%20visual%20information%20and%0Aanswer%20accordingly%2C%20thereby%20highlighting%20the%20challenge%20of%20avoiding%0Ahallucination%20on%20uncertain%20data.%20To%20achieve%20vision-faithful%20reasoning%20and%0Athereby%20avoid%20the%20aforementioned%20issues%2C%20we%20further%20introduce%20a%20GRPO-based%0Aframework%20featuring%20a%20novel%20reward%20mechanism.%20By%20incorporating%20a%20self-awareness%0Aof%20visual%20uncertainty%20and%20an%20analysis%20method%20that%20initiates%20refusal%20to%20answer%0Ato%20increase%20task%20difficulty%20within%20our%20supervised%20fine-tuning%20and%20reinforcement%0Alearning%20framework%2C%20we%20successfully%20mitigated%20hallucinations%20in%20ambiguous%0Aregions.%20Experiments%20on%20Qwen2.5-VL%20demonstrate%20that%20our%207B-parameter%20model%0Aachieves%20a%2022%5C%25%20absolute%20improvement%20in%20hallucination-free%20accuracy%20over%20GPT-4o%0Aon%20KIE-HVQA%20and%20there%20is%20no%20significant%20performance%20drop%20in%20standard%20tasks%2C%0Ahighlighting%20both%20effectiveness%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20168v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520is%2520Believing%253F%2520Mitigating%2520OCR%2520Hallucinations%2520in%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DZhentao%2520He%2520and%2520Can%2520Zhang%2520and%2520Ziheng%2520Wu%2520and%2520Zhenghao%2520Chen%2520and%2520Yufei%2520Zhan%2520and%2520Yifan%2520Li%2520and%2520Zhao%2520Zhang%2520and%2520Xian%2520Wang%2520and%2520Minghui%2520Qiu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520large%2520language%2520models%2520have%2520enhanced%250Adocument%2520understanding%2520by%2520integrating%2520textual%2520and%2520visual%2520information.%2520However%252C%250Aexisting%2520models%2520exhibit%2520incompleteness%2520within%2520their%2520paradigm%2520in%2520real-world%250Ascenarios%252C%2520particularly%2520under%2520visual%2520degradation.%2520In%2520such%2520conditions%252C%2520the%250Acurrent%2520response%2520paradigm%2520often%2520fails%2520to%2520adequately%2520perceive%2520visual%2520degradation%250Aand%2520ambiguity%252C%2520leading%2520to%2520overreliance%2520on%2520linguistic%2520priors%2520or%2520misaligned%250Avisual-textual%2520reasoning.%2520This%2520difficulty%2520in%2520recognizing%2520uncertainty%2520frequently%250Aresults%2520in%2520the%2520generation%2520of%2520hallucinatory%2520content%252C%2520especially%2520when%2520a%2520precise%250Aanswer%2520is%2520not%2520feasible.%2520To%2520better%2520demonstrate%2520and%2520analyze%2520this%2520phenomenon%2520and%250Aproblem%252C%2520we%2520propose%2520KIE-HVQA%252C%2520the%2520first%2520benchmark%2520dedicated%2520to%2520evaluating%2520OCR%250Ahallucination%2520in%2520degraded%2520document%2520understanding.%2520This%2520dataset%2520includes%2520test%250Asamples%2520spanning%2520identity%2520cards%2520and%2520invoices%252C%2520with%2520simulated%2520real-world%250Adegradations%2520for%2520OCR%2520reliability.%2520This%2520setup%2520allows%2520for%2520evaluating%2520models%2527%250Acapacity%252C%2520under%2520degraded%2520input%252C%2520to%2520distinguish%2520reliable%2520visual%2520information%2520and%250Aanswer%2520accordingly%252C%2520thereby%2520highlighting%2520the%2520challenge%2520of%2520avoiding%250Ahallucination%2520on%2520uncertain%2520data.%2520To%2520achieve%2520vision-faithful%2520reasoning%2520and%250Athereby%2520avoid%2520the%2520aforementioned%2520issues%252C%2520we%2520further%2520introduce%2520a%2520GRPO-based%250Aframework%2520featuring%2520a%2520novel%2520reward%2520mechanism.%2520By%2520incorporating%2520a%2520self-awareness%250Aof%2520visual%2520uncertainty%2520and%2520an%2520analysis%2520method%2520that%2520initiates%2520refusal%2520to%2520answer%250Ato%2520increase%2520task%2520difficulty%2520within%2520our%2520supervised%2520fine-tuning%2520and%2520reinforcement%250Alearning%2520framework%252C%2520we%2520successfully%2520mitigated%2520hallucinations%2520in%2520ambiguous%250Aregions.%2520Experiments%2520on%2520Qwen2.5-VL%2520demonstrate%2520that%2520our%25207B-parameter%2520model%250Aachieves%2520a%252022%255C%2525%2520absolute%2520improvement%2520in%2520hallucination-free%2520accuracy%2520over%2520GPT-4o%250Aon%2520KIE-HVQA%2520and%2520there%2520is%2520no%2520significant%2520performance%2520drop%2520in%2520standard%2520tasks%252C%250Ahighlighting%2520both%2520effectiveness%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20168v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20is%20Believing%3F%20Mitigating%20OCR%20Hallucinations%20in%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Zhentao%20He%20and%20Can%20Zhang%20and%20Ziheng%20Wu%20and%20Zhenghao%20Chen%20and%20Yufei%20Zhan%20and%20Yifan%20Li%20and%20Zhao%20Zhang%20and%20Xian%20Wang%20and%20Minghui%20Qiu&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20have%20enhanced%0Adocument%20understanding%20by%20integrating%20textual%20and%20visual%20information.%20However%2C%0Aexisting%20models%20exhibit%20incompleteness%20within%20their%20paradigm%20in%20real-world%0Ascenarios%2C%20particularly%20under%20visual%20degradation.%20In%20such%20conditions%2C%20the%0Acurrent%20response%20paradigm%20often%20fails%20to%20adequately%20perceive%20visual%20degradation%0Aand%20ambiguity%2C%20leading%20to%20overreliance%20on%20linguistic%20priors%20or%20misaligned%0Avisual-textual%20reasoning.%20This%20difficulty%20in%20recognizing%20uncertainty%20frequently%0Aresults%20in%20the%20generation%20of%20hallucinatory%20content%2C%20especially%20when%20a%20precise%0Aanswer%20is%20not%20feasible.%20To%20better%20demonstrate%20and%20analyze%20this%20phenomenon%20and%0Aproblem%2C%20we%20propose%20KIE-HVQA%2C%20the%20first%20benchmark%20dedicated%20to%20evaluating%20OCR%0Ahallucination%20in%20degraded%20document%20understanding.%20This%20dataset%20includes%20test%0Asamples%20spanning%20identity%20cards%20and%20invoices%2C%20with%20simulated%20real-world%0Adegradations%20for%20OCR%20reliability.%20This%20setup%20allows%20for%20evaluating%20models%27%0Acapacity%2C%20under%20degraded%20input%2C%20to%20distinguish%20reliable%20visual%20information%20and%0Aanswer%20accordingly%2C%20thereby%20highlighting%20the%20challenge%20of%20avoiding%0Ahallucination%20on%20uncertain%20data.%20To%20achieve%20vision-faithful%20reasoning%20and%0Athereby%20avoid%20the%20aforementioned%20issues%2C%20we%20further%20introduce%20a%20GRPO-based%0Aframework%20featuring%20a%20novel%20reward%20mechanism.%20By%20incorporating%20a%20self-awareness%0Aof%20visual%20uncertainty%20and%20an%20analysis%20method%20that%20initiates%20refusal%20to%20answer%0Ato%20increase%20task%20difficulty%20within%20our%20supervised%20fine-tuning%20and%20reinforcement%0Alearning%20framework%2C%20we%20successfully%20mitigated%20hallucinations%20in%20ambiguous%0Aregions.%20Experiments%20on%20Qwen2.5-VL%20demonstrate%20that%20our%207B-parameter%20model%0Aachieves%20a%2022%5C%25%20absolute%20improvement%20in%20hallucination-free%20accuracy%20over%20GPT-4o%0Aon%20KIE-HVQA%20and%20there%20is%20no%20significant%20performance%20drop%20in%20standard%20tasks%2C%0Ahighlighting%20both%20effectiveness%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20168v2&entry.124074799=Read"},
{"title": "Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous\n  Tissue Synthesis in Histopathology", "author": "Saghir Alfasly and Wataru Uegami and MD Enamul Hoq and Ghazal Alabtah and H. R. Tizhoosh", "abstract": "  Synthetic data generation in histopathology faces unique challenges:\npreserving tissue heterogeneity, capturing subtle morphological features, and\nscaling to unannotated datasets. We present a latent diffusion model that\ngenerates realistic heterogeneous histopathology images through a novel\ndual-conditioning approach combining semantic segmentation maps with\ntissue-specific visual crops. Unlike existing methods that rely on text prompts\nor abstract visual embeddings, our approach preserves critical morphological\ndetails by directly incorporating raw tissue crops from corresponding semantic\nregions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches\nensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we\nintroduce a self-supervised extension that clusters whole-slide images into 100\ntissue types using foundation model embeddings, automatically generating\npseudo-semantic maps for training. Our method synthesizes high-fidelity images\nwith precise region-wise annotations, achieving superior performance on\ndownstream segmentation tasks. When evaluated on annotated datasets, models\ntrained on our synthetic data show competitive performance to those trained on\nreal data, demonstrating the utility of controlled heterogeneous tissue\ngeneration. In quantitative evaluation, prompt-guided synthesis reduces Frechet\nDistance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower\nFD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on\nsynthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within\n1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA\nwhole-slide images without manual annotations, our framework offers a practical\nsolution for an urgent need for generating diverse, annotated histopathology\ndata, addressing a critical bottleneck in computational pathology.\n", "link": "http://arxiv.org/abs/2509.17847v1", "date": "2025-09-22", "relevancy": 2.3903, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6125}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5946}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20and%20Visual%20Crop-Guided%20Diffusion%20Models%20for%20Heterogeneous%0A%20%20Tissue%20Synthesis%20in%20Histopathology&body=Title%3A%20Semantic%20and%20Visual%20Crop-Guided%20Diffusion%20Models%20for%20Heterogeneous%0A%20%20Tissue%20Synthesis%20in%20Histopathology%0AAuthor%3A%20Saghir%20Alfasly%20and%20Wataru%20Uegami%20and%20MD%20Enamul%20Hoq%20and%20Ghazal%20Alabtah%20and%20H.%20R.%20Tizhoosh%0AAbstract%3A%20%20%20Synthetic%20data%20generation%20in%20histopathology%20faces%20unique%20challenges%3A%0Apreserving%20tissue%20heterogeneity%2C%20capturing%20subtle%20morphological%20features%2C%20and%0Ascaling%20to%20unannotated%20datasets.%20We%20present%20a%20latent%20diffusion%20model%20that%0Agenerates%20realistic%20heterogeneous%20histopathology%20images%20through%20a%20novel%0Adual-conditioning%20approach%20combining%20semantic%20segmentation%20maps%20with%0Atissue-specific%20visual%20crops.%20Unlike%20existing%20methods%20that%20rely%20on%20text%20prompts%0Aor%20abstract%20visual%20embeddings%2C%20our%20approach%20preserves%20critical%20morphological%0Adetails%20by%20directly%20incorporating%20raw%20tissue%20crops%20from%20corresponding%20semantic%0Aregions.%20For%20annotated%20datasets%20%28i.e.%2C%20Camelyon16%2C%20Panda%29%2C%20we%20extract%20patches%0Aensuring%2020-80%25%20tissue%20heterogeneity.%20For%20unannotated%20data%20%28i.e.%2C%20TCGA%29%2C%20we%0Aintroduce%20a%20self-supervised%20extension%20that%20clusters%20whole-slide%20images%20into%20100%0Atissue%20types%20using%20foundation%20model%20embeddings%2C%20automatically%20generating%0Apseudo-semantic%20maps%20for%20training.%20Our%20method%20synthesizes%20high-fidelity%20images%0Awith%20precise%20region-wise%20annotations%2C%20achieving%20superior%20performance%20on%0Adownstream%20segmentation%20tasks.%20When%20evaluated%20on%20annotated%20datasets%2C%20models%0Atrained%20on%20our%20synthetic%20data%20show%20competitive%20performance%20to%20those%20trained%20on%0Areal%20data%2C%20demonstrating%20the%20utility%20of%20controlled%20heterogeneous%20tissue%0Ageneration.%20In%20quantitative%20evaluation%2C%20prompt-guided%20synthesis%20reduces%20Frechet%0ADistance%20by%20up%20to%206X%20on%20Camelyon16%20%28from%20430.1%20to%2072.0%29%20and%20yields%202-3x%20lower%0AFD%20across%20Panda%20and%20TCGA.%20Downstream%20DeepLabv3%2B%20models%20trained%20solely%20on%0Asynthetic%20data%20attain%20test%20IoU%20of%200.71%20and%200.95%20on%20Camelyon16%20and%20Panda%2C%20within%0A1-2%25%20of%20real-data%20baselines%20%280.72%20and%200.96%29.%20By%20scaling%20to%2011%2C765%20TCGA%0Awhole-slide%20images%20without%20manual%20annotations%2C%20our%20framework%20offers%20a%20practical%0Asolution%20for%20an%20urgent%20need%20for%20generating%20diverse%2C%20annotated%20histopathology%0Adata%2C%20addressing%20a%20critical%20bottleneck%20in%20computational%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520and%2520Visual%2520Crop-Guided%2520Diffusion%2520Models%2520for%2520Heterogeneous%250A%2520%2520Tissue%2520Synthesis%2520in%2520Histopathology%26entry.906535625%3DSaghir%2520Alfasly%2520and%2520Wataru%2520Uegami%2520and%2520MD%2520Enamul%2520Hoq%2520and%2520Ghazal%2520Alabtah%2520and%2520H.%2520R.%2520Tizhoosh%26entry.1292438233%3D%2520%2520Synthetic%2520data%2520generation%2520in%2520histopathology%2520faces%2520unique%2520challenges%253A%250Apreserving%2520tissue%2520heterogeneity%252C%2520capturing%2520subtle%2520morphological%2520features%252C%2520and%250Ascaling%2520to%2520unannotated%2520datasets.%2520We%2520present%2520a%2520latent%2520diffusion%2520model%2520that%250Agenerates%2520realistic%2520heterogeneous%2520histopathology%2520images%2520through%2520a%2520novel%250Adual-conditioning%2520approach%2520combining%2520semantic%2520segmentation%2520maps%2520with%250Atissue-specific%2520visual%2520crops.%2520Unlike%2520existing%2520methods%2520that%2520rely%2520on%2520text%2520prompts%250Aor%2520abstract%2520visual%2520embeddings%252C%2520our%2520approach%2520preserves%2520critical%2520morphological%250Adetails%2520by%2520directly%2520incorporating%2520raw%2520tissue%2520crops%2520from%2520corresponding%2520semantic%250Aregions.%2520For%2520annotated%2520datasets%2520%2528i.e.%252C%2520Camelyon16%252C%2520Panda%2529%252C%2520we%2520extract%2520patches%250Aensuring%252020-80%2525%2520tissue%2520heterogeneity.%2520For%2520unannotated%2520data%2520%2528i.e.%252C%2520TCGA%2529%252C%2520we%250Aintroduce%2520a%2520self-supervised%2520extension%2520that%2520clusters%2520whole-slide%2520images%2520into%2520100%250Atissue%2520types%2520using%2520foundation%2520model%2520embeddings%252C%2520automatically%2520generating%250Apseudo-semantic%2520maps%2520for%2520training.%2520Our%2520method%2520synthesizes%2520high-fidelity%2520images%250Awith%2520precise%2520region-wise%2520annotations%252C%2520achieving%2520superior%2520performance%2520on%250Adownstream%2520segmentation%2520tasks.%2520When%2520evaluated%2520on%2520annotated%2520datasets%252C%2520models%250Atrained%2520on%2520our%2520synthetic%2520data%2520show%2520competitive%2520performance%2520to%2520those%2520trained%2520on%250Areal%2520data%252C%2520demonstrating%2520the%2520utility%2520of%2520controlled%2520heterogeneous%2520tissue%250Ageneration.%2520In%2520quantitative%2520evaluation%252C%2520prompt-guided%2520synthesis%2520reduces%2520Frechet%250ADistance%2520by%2520up%2520to%25206X%2520on%2520Camelyon16%2520%2528from%2520430.1%2520to%252072.0%2529%2520and%2520yields%25202-3x%2520lower%250AFD%2520across%2520Panda%2520and%2520TCGA.%2520Downstream%2520DeepLabv3%252B%2520models%2520trained%2520solely%2520on%250Asynthetic%2520data%2520attain%2520test%2520IoU%2520of%25200.71%2520and%25200.95%2520on%2520Camelyon16%2520and%2520Panda%252C%2520within%250A1-2%2525%2520of%2520real-data%2520baselines%2520%25280.72%2520and%25200.96%2529.%2520By%2520scaling%2520to%252011%252C765%2520TCGA%250Awhole-slide%2520images%2520without%2520manual%2520annotations%252C%2520our%2520framework%2520offers%2520a%2520practical%250Asolution%2520for%2520an%2520urgent%2520need%2520for%2520generating%2520diverse%252C%2520annotated%2520histopathology%250Adata%252C%2520addressing%2520a%2520critical%2520bottleneck%2520in%2520computational%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20and%20Visual%20Crop-Guided%20Diffusion%20Models%20for%20Heterogeneous%0A%20%20Tissue%20Synthesis%20in%20Histopathology&entry.906535625=Saghir%20Alfasly%20and%20Wataru%20Uegami%20and%20MD%20Enamul%20Hoq%20and%20Ghazal%20Alabtah%20and%20H.%20R.%20Tizhoosh&entry.1292438233=%20%20Synthetic%20data%20generation%20in%20histopathology%20faces%20unique%20challenges%3A%0Apreserving%20tissue%20heterogeneity%2C%20capturing%20subtle%20morphological%20features%2C%20and%0Ascaling%20to%20unannotated%20datasets.%20We%20present%20a%20latent%20diffusion%20model%20that%0Agenerates%20realistic%20heterogeneous%20histopathology%20images%20through%20a%20novel%0Adual-conditioning%20approach%20combining%20semantic%20segmentation%20maps%20with%0Atissue-specific%20visual%20crops.%20Unlike%20existing%20methods%20that%20rely%20on%20text%20prompts%0Aor%20abstract%20visual%20embeddings%2C%20our%20approach%20preserves%20critical%20morphological%0Adetails%20by%20directly%20incorporating%20raw%20tissue%20crops%20from%20corresponding%20semantic%0Aregions.%20For%20annotated%20datasets%20%28i.e.%2C%20Camelyon16%2C%20Panda%29%2C%20we%20extract%20patches%0Aensuring%2020-80%25%20tissue%20heterogeneity.%20For%20unannotated%20data%20%28i.e.%2C%20TCGA%29%2C%20we%0Aintroduce%20a%20self-supervised%20extension%20that%20clusters%20whole-slide%20images%20into%20100%0Atissue%20types%20using%20foundation%20model%20embeddings%2C%20automatically%20generating%0Apseudo-semantic%20maps%20for%20training.%20Our%20method%20synthesizes%20high-fidelity%20images%0Awith%20precise%20region-wise%20annotations%2C%20achieving%20superior%20performance%20on%0Adownstream%20segmentation%20tasks.%20When%20evaluated%20on%20annotated%20datasets%2C%20models%0Atrained%20on%20our%20synthetic%20data%20show%20competitive%20performance%20to%20those%20trained%20on%0Areal%20data%2C%20demonstrating%20the%20utility%20of%20controlled%20heterogeneous%20tissue%0Ageneration.%20In%20quantitative%20evaluation%2C%20prompt-guided%20synthesis%20reduces%20Frechet%0ADistance%20by%20up%20to%206X%20on%20Camelyon16%20%28from%20430.1%20to%2072.0%29%20and%20yields%202-3x%20lower%0AFD%20across%20Panda%20and%20TCGA.%20Downstream%20DeepLabv3%2B%20models%20trained%20solely%20on%0Asynthetic%20data%20attain%20test%20IoU%20of%200.71%20and%200.95%20on%20Camelyon16%20and%20Panda%2C%20within%0A1-2%25%20of%20real-data%20baselines%20%280.72%20and%200.96%29.%20By%20scaling%20to%2011%2C765%20TCGA%0Awhole-slide%20images%20without%20manual%20annotations%2C%20our%20framework%20offers%20a%20practical%0Asolution%20for%20an%20urgent%20need%20for%20generating%20diverse%2C%20annotated%20histopathology%0Adata%2C%20addressing%20a%20critical%20bottleneck%20in%20computational%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17847v1&entry.124074799=Read"},
{"title": "Preconditioned Deformation Grids", "author": "Julian Kaltheuner and Alexander Oebel and Hannah Droege and Patrick Stotko and Reinhard Klein", "abstract": "  Dynamic surface reconstruction of objects from point cloud sequences is a\nchallenging field in computer graphics. Existing approaches either require\nmultiple regularization terms or extensive training data which, however, lead\nto compromises in reconstruction accuracy as well as over-smoothing or poor\ngeneralization to unseen objects and motions. To address these lim- itations,\nwe introduce Preconditioned Deformation Grids, a novel technique for estimating\ncoherent deformation fields directly from unstructured point cloud sequences\nwithout requiring or forming explicit correspondences. Key to our approach is\nthe use of multi-resolution voxel grids that capture the overall motion at\nvarying spatial scales, enabling a more flexible deformation representation. In\nconjunction with incorporating grid-based Sobolev preconditioning into\ngradient-based optimization, we show that applying a Chamfer loss between the\ninput point clouds as well as to an evolving template mesh is sufficient to\nobtain accurate deformations. To ensure temporal consistency along the object\nsurface, we include a weak isometry loss on mesh edges which complements the\nmain objective without constraining deformation fidelity. Extensive evaluations\ndemonstrate that our method achieves superior results, particularly for long\nsequences, compared to state-of-the-art techniques.\n", "link": "http://arxiv.org/abs/2509.18097v1", "date": "2025-09-22", "relevancy": 2.3897, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6587}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5581}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preconditioned%20Deformation%20Grids&body=Title%3A%20Preconditioned%20Deformation%20Grids%0AAuthor%3A%20Julian%20Kaltheuner%20and%20Alexander%20Oebel%20and%20Hannah%20Droege%20and%20Patrick%20Stotko%20and%20Reinhard%20Klein%0AAbstract%3A%20%20%20Dynamic%20surface%20reconstruction%20of%20objects%20from%20point%20cloud%20sequences%20is%20a%0Achallenging%20field%20in%20computer%20graphics.%20Existing%20approaches%20either%20require%0Amultiple%20regularization%20terms%20or%20extensive%20training%20data%20which%2C%20however%2C%20lead%0Ato%20compromises%20in%20reconstruction%20accuracy%20as%20well%20as%20over-smoothing%20or%20poor%0Ageneralization%20to%20unseen%20objects%20and%20motions.%20To%20address%20these%20lim-%20itations%2C%0Awe%20introduce%20Preconditioned%20Deformation%20Grids%2C%20a%20novel%20technique%20for%20estimating%0Acoherent%20deformation%20fields%20directly%20from%20unstructured%20point%20cloud%20sequences%0Awithout%20requiring%20or%20forming%20explicit%20correspondences.%20Key%20to%20our%20approach%20is%0Athe%20use%20of%20multi-resolution%20voxel%20grids%20that%20capture%20the%20overall%20motion%20at%0Avarying%20spatial%20scales%2C%20enabling%20a%20more%20flexible%20deformation%20representation.%20In%0Aconjunction%20with%20incorporating%20grid-based%20Sobolev%20preconditioning%20into%0Agradient-based%20optimization%2C%20we%20show%20that%20applying%20a%20Chamfer%20loss%20between%20the%0Ainput%20point%20clouds%20as%20well%20as%20to%20an%20evolving%20template%20mesh%20is%20sufficient%20to%0Aobtain%20accurate%20deformations.%20To%20ensure%20temporal%20consistency%20along%20the%20object%0Asurface%2C%20we%20include%20a%20weak%20isometry%20loss%20on%20mesh%20edges%20which%20complements%20the%0Amain%20objective%20without%20constraining%20deformation%20fidelity.%20Extensive%20evaluations%0Ademonstrate%20that%20our%20method%20achieves%20superior%20results%2C%20particularly%20for%20long%0Asequences%2C%20compared%20to%20state-of-the-art%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreconditioned%2520Deformation%2520Grids%26entry.906535625%3DJulian%2520Kaltheuner%2520and%2520Alexander%2520Oebel%2520and%2520Hannah%2520Droege%2520and%2520Patrick%2520Stotko%2520and%2520Reinhard%2520Klein%26entry.1292438233%3D%2520%2520Dynamic%2520surface%2520reconstruction%2520of%2520objects%2520from%2520point%2520cloud%2520sequences%2520is%2520a%250Achallenging%2520field%2520in%2520computer%2520graphics.%2520Existing%2520approaches%2520either%2520require%250Amultiple%2520regularization%2520terms%2520or%2520extensive%2520training%2520data%2520which%252C%2520however%252C%2520lead%250Ato%2520compromises%2520in%2520reconstruction%2520accuracy%2520as%2520well%2520as%2520over-smoothing%2520or%2520poor%250Ageneralization%2520to%2520unseen%2520objects%2520and%2520motions.%2520To%2520address%2520these%2520lim-%2520itations%252C%250Awe%2520introduce%2520Preconditioned%2520Deformation%2520Grids%252C%2520a%2520novel%2520technique%2520for%2520estimating%250Acoherent%2520deformation%2520fields%2520directly%2520from%2520unstructured%2520point%2520cloud%2520sequences%250Awithout%2520requiring%2520or%2520forming%2520explicit%2520correspondences.%2520Key%2520to%2520our%2520approach%2520is%250Athe%2520use%2520of%2520multi-resolution%2520voxel%2520grids%2520that%2520capture%2520the%2520overall%2520motion%2520at%250Avarying%2520spatial%2520scales%252C%2520enabling%2520a%2520more%2520flexible%2520deformation%2520representation.%2520In%250Aconjunction%2520with%2520incorporating%2520grid-based%2520Sobolev%2520preconditioning%2520into%250Agradient-based%2520optimization%252C%2520we%2520show%2520that%2520applying%2520a%2520Chamfer%2520loss%2520between%2520the%250Ainput%2520point%2520clouds%2520as%2520well%2520as%2520to%2520an%2520evolving%2520template%2520mesh%2520is%2520sufficient%2520to%250Aobtain%2520accurate%2520deformations.%2520To%2520ensure%2520temporal%2520consistency%2520along%2520the%2520object%250Asurface%252C%2520we%2520include%2520a%2520weak%2520isometry%2520loss%2520on%2520mesh%2520edges%2520which%2520complements%2520the%250Amain%2520objective%2520without%2520constraining%2520deformation%2520fidelity.%2520Extensive%2520evaluations%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520results%252C%2520particularly%2520for%2520long%250Asequences%252C%2520compared%2520to%2520state-of-the-art%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preconditioned%20Deformation%20Grids&entry.906535625=Julian%20Kaltheuner%20and%20Alexander%20Oebel%20and%20Hannah%20Droege%20and%20Patrick%20Stotko%20and%20Reinhard%20Klein&entry.1292438233=%20%20Dynamic%20surface%20reconstruction%20of%20objects%20from%20point%20cloud%20sequences%20is%20a%0Achallenging%20field%20in%20computer%20graphics.%20Existing%20approaches%20either%20require%0Amultiple%20regularization%20terms%20or%20extensive%20training%20data%20which%2C%20however%2C%20lead%0Ato%20compromises%20in%20reconstruction%20accuracy%20as%20well%20as%20over-smoothing%20or%20poor%0Ageneralization%20to%20unseen%20objects%20and%20motions.%20To%20address%20these%20lim-%20itations%2C%0Awe%20introduce%20Preconditioned%20Deformation%20Grids%2C%20a%20novel%20technique%20for%20estimating%0Acoherent%20deformation%20fields%20directly%20from%20unstructured%20point%20cloud%20sequences%0Awithout%20requiring%20or%20forming%20explicit%20correspondences.%20Key%20to%20our%20approach%20is%0Athe%20use%20of%20multi-resolution%20voxel%20grids%20that%20capture%20the%20overall%20motion%20at%0Avarying%20spatial%20scales%2C%20enabling%20a%20more%20flexible%20deformation%20representation.%20In%0Aconjunction%20with%20incorporating%20grid-based%20Sobolev%20preconditioning%20into%0Agradient-based%20optimization%2C%20we%20show%20that%20applying%20a%20Chamfer%20loss%20between%20the%0Ainput%20point%20clouds%20as%20well%20as%20to%20an%20evolving%20template%20mesh%20is%20sufficient%20to%0Aobtain%20accurate%20deformations.%20To%20ensure%20temporal%20consistency%20along%20the%20object%0Asurface%2C%20we%20include%20a%20weak%20isometry%20loss%20on%20mesh%20edges%20which%20complements%20the%0Amain%20objective%20without%20constraining%20deformation%20fidelity.%20Extensive%20evaluations%0Ademonstrate%20that%20our%20method%20achieves%20superior%20results%2C%20particularly%20for%20long%0Asequences%2C%20compared%20to%20state-of-the-art%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18097v1&entry.124074799=Read"},
{"title": "Fast, Accurate and Interpretable Graph Classification with Topological\n  Kernels", "author": "Adam Weso\u0142owski and Ronin Wu and Karim Essafi", "abstract": "  We introduce a novel class of explicit feature maps based on topological\nindices that represent each graph by a compact feature vector, enabling fast\nand interpretable graph classification. Using radial basis function kernels on\nthese compact vectors, we define a measure of similarity between graphs. We\nperform evaluation on standard molecular datasets and observe that\nclassification accuracies based on single topological-index feature vectors\nunderperform compared to state-of-the-art substructure-based kernels. However,\nwe achieve significantly faster Gram matrix evaluation -- up to $20\\times$\nfaster -- compared to the Weisfeiler--Lehman subtree kernel. To enhance\nperformance, we propose two extensions: 1) concatenating multiple topological\nindices into an \\emph{Extended Feature Vector} (EFV), and 2) \\emph{Linear\nCombination of Topological Kernels} (LCTK) by linearly combining Radial Basis\nFunction kernels computed on feature vectors of individual topological graph\nindices. These extensions deliver up to $12\\%$ percent accuracy gains across\nall the molecular datasets. A complexity analysis highlights the potential for\nexponential quantum speedup for some of the vector components. Our results\nindicate that LCTK and EFV offer a favourable trade-off between accuracy and\nefficiency, making them strong candidates for practical graph learning\napplications.\n", "link": "http://arxiv.org/abs/2509.17693v1", "date": "2025-09-22", "relevancy": 2.3553, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4739}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4701}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%2C%20Accurate%20and%20Interpretable%20Graph%20Classification%20with%20Topological%0A%20%20Kernels&body=Title%3A%20Fast%2C%20Accurate%20and%20Interpretable%20Graph%20Classification%20with%20Topological%0A%20%20Kernels%0AAuthor%3A%20Adam%20Weso%C5%82owski%20and%20Ronin%20Wu%20and%20Karim%20Essafi%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20class%20of%20explicit%20feature%20maps%20based%20on%20topological%0Aindices%20that%20represent%20each%20graph%20by%20a%20compact%20feature%20vector%2C%20enabling%20fast%0Aand%20interpretable%20graph%20classification.%20Using%20radial%20basis%20function%20kernels%20on%0Athese%20compact%20vectors%2C%20we%20define%20a%20measure%20of%20similarity%20between%20graphs.%20We%0Aperform%20evaluation%20on%20standard%20molecular%20datasets%20and%20observe%20that%0Aclassification%20accuracies%20based%20on%20single%20topological-index%20feature%20vectors%0Aunderperform%20compared%20to%20state-of-the-art%20substructure-based%20kernels.%20However%2C%0Awe%20achieve%20significantly%20faster%20Gram%20matrix%20evaluation%20--%20up%20to%20%2420%5Ctimes%24%0Afaster%20--%20compared%20to%20the%20Weisfeiler--Lehman%20subtree%20kernel.%20To%20enhance%0Aperformance%2C%20we%20propose%20two%20extensions%3A%201%29%20concatenating%20multiple%20topological%0Aindices%20into%20an%20%5Cemph%7BExtended%20Feature%20Vector%7D%20%28EFV%29%2C%20and%202%29%20%5Cemph%7BLinear%0ACombination%20of%20Topological%20Kernels%7D%20%28LCTK%29%20by%20linearly%20combining%20Radial%20Basis%0AFunction%20kernels%20computed%20on%20feature%20vectors%20of%20individual%20topological%20graph%0Aindices.%20These%20extensions%20deliver%20up%20to%20%2412%5C%25%24%20percent%20accuracy%20gains%20across%0Aall%20the%20molecular%20datasets.%20A%20complexity%20analysis%20highlights%20the%20potential%20for%0Aexponential%20quantum%20speedup%20for%20some%20of%20the%20vector%20components.%20Our%20results%0Aindicate%20that%20LCTK%20and%20EFV%20offer%20a%20favourable%20trade-off%20between%20accuracy%20and%0Aefficiency%2C%20making%20them%20strong%20candidates%20for%20practical%20graph%20learning%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%252C%2520Accurate%2520and%2520Interpretable%2520Graph%2520Classification%2520with%2520Topological%250A%2520%2520Kernels%26entry.906535625%3DAdam%2520Weso%25C5%2582owski%2520and%2520Ronin%2520Wu%2520and%2520Karim%2520Essafi%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520class%2520of%2520explicit%2520feature%2520maps%2520based%2520on%2520topological%250Aindices%2520that%2520represent%2520each%2520graph%2520by%2520a%2520compact%2520feature%2520vector%252C%2520enabling%2520fast%250Aand%2520interpretable%2520graph%2520classification.%2520Using%2520radial%2520basis%2520function%2520kernels%2520on%250Athese%2520compact%2520vectors%252C%2520we%2520define%2520a%2520measure%2520of%2520similarity%2520between%2520graphs.%2520We%250Aperform%2520evaluation%2520on%2520standard%2520molecular%2520datasets%2520and%2520observe%2520that%250Aclassification%2520accuracies%2520based%2520on%2520single%2520topological-index%2520feature%2520vectors%250Aunderperform%2520compared%2520to%2520state-of-the-art%2520substructure-based%2520kernels.%2520However%252C%250Awe%2520achieve%2520significantly%2520faster%2520Gram%2520matrix%2520evaluation%2520--%2520up%2520to%2520%252420%255Ctimes%2524%250Afaster%2520--%2520compared%2520to%2520the%2520Weisfeiler--Lehman%2520subtree%2520kernel.%2520To%2520enhance%250Aperformance%252C%2520we%2520propose%2520two%2520extensions%253A%25201%2529%2520concatenating%2520multiple%2520topological%250Aindices%2520into%2520an%2520%255Cemph%257BExtended%2520Feature%2520Vector%257D%2520%2528EFV%2529%252C%2520and%25202%2529%2520%255Cemph%257BLinear%250ACombination%2520of%2520Topological%2520Kernels%257D%2520%2528LCTK%2529%2520by%2520linearly%2520combining%2520Radial%2520Basis%250AFunction%2520kernels%2520computed%2520on%2520feature%2520vectors%2520of%2520individual%2520topological%2520graph%250Aindices.%2520These%2520extensions%2520deliver%2520up%2520to%2520%252412%255C%2525%2524%2520percent%2520accuracy%2520gains%2520across%250Aall%2520the%2520molecular%2520datasets.%2520A%2520complexity%2520analysis%2520highlights%2520the%2520potential%2520for%250Aexponential%2520quantum%2520speedup%2520for%2520some%2520of%2520the%2520vector%2520components.%2520Our%2520results%250Aindicate%2520that%2520LCTK%2520and%2520EFV%2520offer%2520a%2520favourable%2520trade-off%2520between%2520accuracy%2520and%250Aefficiency%252C%2520making%2520them%2520strong%2520candidates%2520for%2520practical%2520graph%2520learning%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%2C%20Accurate%20and%20Interpretable%20Graph%20Classification%20with%20Topological%0A%20%20Kernels&entry.906535625=Adam%20Weso%C5%82owski%20and%20Ronin%20Wu%20and%20Karim%20Essafi&entry.1292438233=%20%20We%20introduce%20a%20novel%20class%20of%20explicit%20feature%20maps%20based%20on%20topological%0Aindices%20that%20represent%20each%20graph%20by%20a%20compact%20feature%20vector%2C%20enabling%20fast%0Aand%20interpretable%20graph%20classification.%20Using%20radial%20basis%20function%20kernels%20on%0Athese%20compact%20vectors%2C%20we%20define%20a%20measure%20of%20similarity%20between%20graphs.%20We%0Aperform%20evaluation%20on%20standard%20molecular%20datasets%20and%20observe%20that%0Aclassification%20accuracies%20based%20on%20single%20topological-index%20feature%20vectors%0Aunderperform%20compared%20to%20state-of-the-art%20substructure-based%20kernels.%20However%2C%0Awe%20achieve%20significantly%20faster%20Gram%20matrix%20evaluation%20--%20up%20to%20%2420%5Ctimes%24%0Afaster%20--%20compared%20to%20the%20Weisfeiler--Lehman%20subtree%20kernel.%20To%20enhance%0Aperformance%2C%20we%20propose%20two%20extensions%3A%201%29%20concatenating%20multiple%20topological%0Aindices%20into%20an%20%5Cemph%7BExtended%20Feature%20Vector%7D%20%28EFV%29%2C%20and%202%29%20%5Cemph%7BLinear%0ACombination%20of%20Topological%20Kernels%7D%20%28LCTK%29%20by%20linearly%20combining%20Radial%20Basis%0AFunction%20kernels%20computed%20on%20feature%20vectors%20of%20individual%20topological%20graph%0Aindices.%20These%20extensions%20deliver%20up%20to%20%2412%5C%25%24%20percent%20accuracy%20gains%20across%0Aall%20the%20molecular%20datasets.%20A%20complexity%20analysis%20highlights%20the%20potential%20for%0Aexponential%20quantum%20speedup%20for%20some%20of%20the%20vector%20components.%20Our%20results%0Aindicate%20that%20LCTK%20and%20EFV%20offer%20a%20favourable%20trade-off%20between%20accuracy%20and%0Aefficiency%2C%20making%20them%20strong%20candidates%20for%20practical%20graph%20learning%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17693v1&entry.124074799=Read"},
{"title": "Cross-Attention is Half Explanation in Speech-to-Text Models", "author": "Sara Papi and Dennis Fucci and Marco Gaido and Matteo Negri and Luisa Bentivogli", "abstract": "  Cross-attention is a core mechanism in encoder-decoder architectures,\nwidespread in many fields, including speech-to-text (S2T) processing. Its\nscores have been repurposed for various downstream applications--such as\ntimestamp estimation and audio-text alignment--under the assumption that they\nreflect the dependencies between input speech representation and the generated\ntext. While the explanatory nature of attention mechanisms has been widely\ndebated in the broader NLP literature, this assumption remains largely\nunexplored within the speech domain. To address this gap, we assess the\nexplanatory power of cross-attention in S2T models by comparing its scores to\ninput saliency maps derived from feature attribution. Our analysis spans\nmonolingual and multilingual, single-task and multi-task models at multiple\nscales, and shows that attention scores moderately to strongly align with\nsaliency-based explanations, particularly when aggregated across heads and\nlayers. However, it also shows that cross-attention captures only about 50% of\nthe input relevance and, in the best case, only partially reflects how the\ndecoder attends to the encoder's representations--accounting for just 52-75% of\nthe saliency. These findings uncover fundamental limitations in interpreting\ncross-attention as an explanatory proxy, suggesting that it offers an\ninformative yet incomplete view of the factors driving predictions in S2T\nmodels.\n", "link": "http://arxiv.org/abs/2509.18010v1", "date": "2025-09-22", "relevancy": 2.3547, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Attention%20is%20Half%20Explanation%20in%20Speech-to-Text%20Models&body=Title%3A%20Cross-Attention%20is%20Half%20Explanation%20in%20Speech-to-Text%20Models%0AAuthor%3A%20Sara%20Papi%20and%20Dennis%20Fucci%20and%20Marco%20Gaido%20and%20Matteo%20Negri%20and%20Luisa%20Bentivogli%0AAbstract%3A%20%20%20Cross-attention%20is%20a%20core%20mechanism%20in%20encoder-decoder%20architectures%2C%0Awidespread%20in%20many%20fields%2C%20including%20speech-to-text%20%28S2T%29%20processing.%20Its%0Ascores%20have%20been%20repurposed%20for%20various%20downstream%20applications--such%20as%0Atimestamp%20estimation%20and%20audio-text%20alignment--under%20the%20assumption%20that%20they%0Areflect%20the%20dependencies%20between%20input%20speech%20representation%20and%20the%20generated%0Atext.%20While%20the%20explanatory%20nature%20of%20attention%20mechanisms%20has%20been%20widely%0Adebated%20in%20the%20broader%20NLP%20literature%2C%20this%20assumption%20remains%20largely%0Aunexplored%20within%20the%20speech%20domain.%20To%20address%20this%20gap%2C%20we%20assess%20the%0Aexplanatory%20power%20of%20cross-attention%20in%20S2T%20models%20by%20comparing%20its%20scores%20to%0Ainput%20saliency%20maps%20derived%20from%20feature%20attribution.%20Our%20analysis%20spans%0Amonolingual%20and%20multilingual%2C%20single-task%20and%20multi-task%20models%20at%20multiple%0Ascales%2C%20and%20shows%20that%20attention%20scores%20moderately%20to%20strongly%20align%20with%0Asaliency-based%20explanations%2C%20particularly%20when%20aggregated%20across%20heads%20and%0Alayers.%20However%2C%20it%20also%20shows%20that%20cross-attention%20captures%20only%20about%2050%25%20of%0Athe%20input%20relevance%20and%2C%20in%20the%20best%20case%2C%20only%20partially%20reflects%20how%20the%0Adecoder%20attends%20to%20the%20encoder%27s%20representations--accounting%20for%20just%2052-75%25%20of%0Athe%20saliency.%20These%20findings%20uncover%20fundamental%20limitations%20in%20interpreting%0Across-attention%20as%20an%20explanatory%20proxy%2C%20suggesting%20that%20it%20offers%20an%0Ainformative%20yet%20incomplete%20view%20of%20the%20factors%20driving%20predictions%20in%20S2T%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Attention%2520is%2520Half%2520Explanation%2520in%2520Speech-to-Text%2520Models%26entry.906535625%3DSara%2520Papi%2520and%2520Dennis%2520Fucci%2520and%2520Marco%2520Gaido%2520and%2520Matteo%2520Negri%2520and%2520Luisa%2520Bentivogli%26entry.1292438233%3D%2520%2520Cross-attention%2520is%2520a%2520core%2520mechanism%2520in%2520encoder-decoder%2520architectures%252C%250Awidespread%2520in%2520many%2520fields%252C%2520including%2520speech-to-text%2520%2528S2T%2529%2520processing.%2520Its%250Ascores%2520have%2520been%2520repurposed%2520for%2520various%2520downstream%2520applications--such%2520as%250Atimestamp%2520estimation%2520and%2520audio-text%2520alignment--under%2520the%2520assumption%2520that%2520they%250Areflect%2520the%2520dependencies%2520between%2520input%2520speech%2520representation%2520and%2520the%2520generated%250Atext.%2520While%2520the%2520explanatory%2520nature%2520of%2520attention%2520mechanisms%2520has%2520been%2520widely%250Adebated%2520in%2520the%2520broader%2520NLP%2520literature%252C%2520this%2520assumption%2520remains%2520largely%250Aunexplored%2520within%2520the%2520speech%2520domain.%2520To%2520address%2520this%2520gap%252C%2520we%2520assess%2520the%250Aexplanatory%2520power%2520of%2520cross-attention%2520in%2520S2T%2520models%2520by%2520comparing%2520its%2520scores%2520to%250Ainput%2520saliency%2520maps%2520derived%2520from%2520feature%2520attribution.%2520Our%2520analysis%2520spans%250Amonolingual%2520and%2520multilingual%252C%2520single-task%2520and%2520multi-task%2520models%2520at%2520multiple%250Ascales%252C%2520and%2520shows%2520that%2520attention%2520scores%2520moderately%2520to%2520strongly%2520align%2520with%250Asaliency-based%2520explanations%252C%2520particularly%2520when%2520aggregated%2520across%2520heads%2520and%250Alayers.%2520However%252C%2520it%2520also%2520shows%2520that%2520cross-attention%2520captures%2520only%2520about%252050%2525%2520of%250Athe%2520input%2520relevance%2520and%252C%2520in%2520the%2520best%2520case%252C%2520only%2520partially%2520reflects%2520how%2520the%250Adecoder%2520attends%2520to%2520the%2520encoder%2527s%2520representations--accounting%2520for%2520just%252052-75%2525%2520of%250Athe%2520saliency.%2520These%2520findings%2520uncover%2520fundamental%2520limitations%2520in%2520interpreting%250Across-attention%2520as%2520an%2520explanatory%2520proxy%252C%2520suggesting%2520that%2520it%2520offers%2520an%250Ainformative%2520yet%2520incomplete%2520view%2520of%2520the%2520factors%2520driving%2520predictions%2520in%2520S2T%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Attention%20is%20Half%20Explanation%20in%20Speech-to-Text%20Models&entry.906535625=Sara%20Papi%20and%20Dennis%20Fucci%20and%20Marco%20Gaido%20and%20Matteo%20Negri%20and%20Luisa%20Bentivogli&entry.1292438233=%20%20Cross-attention%20is%20a%20core%20mechanism%20in%20encoder-decoder%20architectures%2C%0Awidespread%20in%20many%20fields%2C%20including%20speech-to-text%20%28S2T%29%20processing.%20Its%0Ascores%20have%20been%20repurposed%20for%20various%20downstream%20applications--such%20as%0Atimestamp%20estimation%20and%20audio-text%20alignment--under%20the%20assumption%20that%20they%0Areflect%20the%20dependencies%20between%20input%20speech%20representation%20and%20the%20generated%0Atext.%20While%20the%20explanatory%20nature%20of%20attention%20mechanisms%20has%20been%20widely%0Adebated%20in%20the%20broader%20NLP%20literature%2C%20this%20assumption%20remains%20largely%0Aunexplored%20within%20the%20speech%20domain.%20To%20address%20this%20gap%2C%20we%20assess%20the%0Aexplanatory%20power%20of%20cross-attention%20in%20S2T%20models%20by%20comparing%20its%20scores%20to%0Ainput%20saliency%20maps%20derived%20from%20feature%20attribution.%20Our%20analysis%20spans%0Amonolingual%20and%20multilingual%2C%20single-task%20and%20multi-task%20models%20at%20multiple%0Ascales%2C%20and%20shows%20that%20attention%20scores%20moderately%20to%20strongly%20align%20with%0Asaliency-based%20explanations%2C%20particularly%20when%20aggregated%20across%20heads%20and%0Alayers.%20However%2C%20it%20also%20shows%20that%20cross-attention%20captures%20only%20about%2050%25%20of%0Athe%20input%20relevance%20and%2C%20in%20the%20best%20case%2C%20only%20partially%20reflects%20how%20the%0Adecoder%20attends%20to%20the%20encoder%27s%20representations--accounting%20for%20just%2052-75%25%20of%0Athe%20saliency.%20These%20findings%20uncover%20fundamental%20limitations%20in%20interpreting%0Across-attention%20as%20an%20explanatory%20proxy%2C%20suggesting%20that%20it%20offers%20an%0Ainformative%20yet%20incomplete%20view%20of%20the%20factors%20driving%20predictions%20in%20S2T%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18010v1&entry.124074799=Read"},
{"title": "Interpreting Graph Inference with Skyline Explanations", "author": "Dazhuo Qiu and Haolai Che and Arijit Khan and Yinghui Wu", "abstract": "  Inference queries have been routinely issued to graph machine learning models\nsuch as graph neural networks (GNNs) for various network analytical tasks.\nNevertheless, GNN outputs are often hard to interpret comprehensively. Existing\nmethods typically conform to individual pre-defined explainability measures\n(such as fidelity), which often leads to biased, ``one-side'' interpretations.\nThis paper introduces skyline explanation, a new paradigm that interprets GNN\noutputs by simultaneously optimizing multiple explainability measures of users'\ninterests. (1) We propose skyline explanations as a Pareto set of explanatory\nsubgraphs that dominate others over multiple explanatory measures. We formulate\nskyline explanation as a multi-criteria optimization problem, and establish its\nhardness results. (2) We design efficient algorithms with an onion-peeling\napproach, which strategically prioritizes nodes and removes unpromising edges\nto incrementally assemble skyline explanations. (3) We also develop an\nalgorithm to diversify the skyline explanations to enrich the comprehensive\ninterpretation. (4) We introduce efficient parallel algorithms with\nload-balancing strategies to scale skyline explanation for large-scale\nGNN-based inference. Using real-world and synthetic graphs, we experimentally\nverify our algorithms' effectiveness and scalability.\n", "link": "http://arxiv.org/abs/2505.07635v3", "date": "2025-09-22", "relevancy": 2.3324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4694}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations&body=Title%3A%20Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations%0AAuthor%3A%20Dazhuo%20Qiu%20and%20Haolai%20Che%20and%20Arijit%20Khan%20and%20Yinghui%20Wu%0AAbstract%3A%20%20%20Inference%20queries%20have%20been%20routinely%20issued%20to%20graph%20machine%20learning%20models%0Asuch%20as%20graph%20neural%20networks%20%28GNNs%29%20for%20various%20network%20analytical%20tasks.%0ANevertheless%2C%20GNN%20outputs%20are%20often%20hard%20to%20interpret%20comprehensively.%20Existing%0Amethods%20typically%20conform%20to%20individual%20pre-defined%20explainability%20measures%0A%28such%20as%20fidelity%29%2C%20which%20often%20leads%20to%20biased%2C%20%60%60one-side%27%27%20interpretations.%0AThis%20paper%20introduces%20skyline%20explanation%2C%20a%20new%20paradigm%20that%20interprets%20GNN%0Aoutputs%20by%20simultaneously%20optimizing%20multiple%20explainability%20measures%20of%20users%27%0Ainterests.%20%281%29%20We%20propose%20skyline%20explanations%20as%20a%20Pareto%20set%20of%20explanatory%0Asubgraphs%20that%20dominate%20others%20over%20multiple%20explanatory%20measures.%20We%20formulate%0Askyline%20explanation%20as%20a%20multi-criteria%20optimization%20problem%2C%20and%20establish%20its%0Ahardness%20results.%20%282%29%20We%20design%20efficient%20algorithms%20with%20an%20onion-peeling%0Aapproach%2C%20which%20strategically%20prioritizes%20nodes%20and%20removes%20unpromising%20edges%0Ato%20incrementally%20assemble%20skyline%20explanations.%20%283%29%20We%20also%20develop%20an%0Aalgorithm%20to%20diversify%20the%20skyline%20explanations%20to%20enrich%20the%20comprehensive%0Ainterpretation.%20%284%29%20We%20introduce%20efficient%20parallel%20algorithms%20with%0Aload-balancing%20strategies%20to%20scale%20skyline%20explanation%20for%20large-scale%0AGNN-based%20inference.%20Using%20real-world%20and%20synthetic%20graphs%2C%20we%20experimentally%0Averify%20our%20algorithms%27%20effectiveness%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07635v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Graph%2520Inference%2520with%2520Skyline%2520Explanations%26entry.906535625%3DDazhuo%2520Qiu%2520and%2520Haolai%2520Che%2520and%2520Arijit%2520Khan%2520and%2520Yinghui%2520Wu%26entry.1292438233%3D%2520%2520Inference%2520queries%2520have%2520been%2520routinely%2520issued%2520to%2520graph%2520machine%2520learning%2520models%250Asuch%2520as%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520for%2520various%2520network%2520analytical%2520tasks.%250ANevertheless%252C%2520GNN%2520outputs%2520are%2520often%2520hard%2520to%2520interpret%2520comprehensively.%2520Existing%250Amethods%2520typically%2520conform%2520to%2520individual%2520pre-defined%2520explainability%2520measures%250A%2528such%2520as%2520fidelity%2529%252C%2520which%2520often%2520leads%2520to%2520biased%252C%2520%2560%2560one-side%2527%2527%2520interpretations.%250AThis%2520paper%2520introduces%2520skyline%2520explanation%252C%2520a%2520new%2520paradigm%2520that%2520interprets%2520GNN%250Aoutputs%2520by%2520simultaneously%2520optimizing%2520multiple%2520explainability%2520measures%2520of%2520users%2527%250Ainterests.%2520%25281%2529%2520We%2520propose%2520skyline%2520explanations%2520as%2520a%2520Pareto%2520set%2520of%2520explanatory%250Asubgraphs%2520that%2520dominate%2520others%2520over%2520multiple%2520explanatory%2520measures.%2520We%2520formulate%250Askyline%2520explanation%2520as%2520a%2520multi-criteria%2520optimization%2520problem%252C%2520and%2520establish%2520its%250Ahardness%2520results.%2520%25282%2529%2520We%2520design%2520efficient%2520algorithms%2520with%2520an%2520onion-peeling%250Aapproach%252C%2520which%2520strategically%2520prioritizes%2520nodes%2520and%2520removes%2520unpromising%2520edges%250Ato%2520incrementally%2520assemble%2520skyline%2520explanations.%2520%25283%2529%2520We%2520also%2520develop%2520an%250Aalgorithm%2520to%2520diversify%2520the%2520skyline%2520explanations%2520to%2520enrich%2520the%2520comprehensive%250Ainterpretation.%2520%25284%2529%2520We%2520introduce%2520efficient%2520parallel%2520algorithms%2520with%250Aload-balancing%2520strategies%2520to%2520scale%2520skyline%2520explanation%2520for%2520large-scale%250AGNN-based%2520inference.%2520Using%2520real-world%2520and%2520synthetic%2520graphs%252C%2520we%2520experimentally%250Averify%2520our%2520algorithms%2527%2520effectiveness%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07635v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations&entry.906535625=Dazhuo%20Qiu%20and%20Haolai%20Che%20and%20Arijit%20Khan%20and%20Yinghui%20Wu&entry.1292438233=%20%20Inference%20queries%20have%20been%20routinely%20issued%20to%20graph%20machine%20learning%20models%0Asuch%20as%20graph%20neural%20networks%20%28GNNs%29%20for%20various%20network%20analytical%20tasks.%0ANevertheless%2C%20GNN%20outputs%20are%20often%20hard%20to%20interpret%20comprehensively.%20Existing%0Amethods%20typically%20conform%20to%20individual%20pre-defined%20explainability%20measures%0A%28such%20as%20fidelity%29%2C%20which%20often%20leads%20to%20biased%2C%20%60%60one-side%27%27%20interpretations.%0AThis%20paper%20introduces%20skyline%20explanation%2C%20a%20new%20paradigm%20that%20interprets%20GNN%0Aoutputs%20by%20simultaneously%20optimizing%20multiple%20explainability%20measures%20of%20users%27%0Ainterests.%20%281%29%20We%20propose%20skyline%20explanations%20as%20a%20Pareto%20set%20of%20explanatory%0Asubgraphs%20that%20dominate%20others%20over%20multiple%20explanatory%20measures.%20We%20formulate%0Askyline%20explanation%20as%20a%20multi-criteria%20optimization%20problem%2C%20and%20establish%20its%0Ahardness%20results.%20%282%29%20We%20design%20efficient%20algorithms%20with%20an%20onion-peeling%0Aapproach%2C%20which%20strategically%20prioritizes%20nodes%20and%20removes%20unpromising%20edges%0Ato%20incrementally%20assemble%20skyline%20explanations.%20%283%29%20We%20also%20develop%20an%0Aalgorithm%20to%20diversify%20the%20skyline%20explanations%20to%20enrich%20the%20comprehensive%0Ainterpretation.%20%284%29%20We%20introduce%20efficient%20parallel%20algorithms%20with%0Aload-balancing%20strategies%20to%20scale%20skyline%20explanation%20for%20large-scale%0AGNN-based%20inference.%20Using%20real-world%20and%20synthetic%20graphs%2C%20we%20experimentally%0Averify%20our%20algorithms%27%20effectiveness%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07635v3&entry.124074799=Read"},
{"title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs", "author": "Yunheng Li and Jing Cheng and Shaoyong Jia and Hangyi Kuang and Shaohui Jiao and Qibin Hou and Ming-Ming Cheng", "abstract": "  This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1\n", "link": "http://arxiv.org/abs/2509.18056v1", "date": "2025-09-22", "relevancy": 2.3207, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5996}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5781}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempSamp-R1%3A%20Effective%20Temporal%20Sampling%20with%20Reinforcement%20Fine-Tuning%0A%20%20for%20Video%20LLMs&body=Title%3A%20TempSamp-R1%3A%20Effective%20Temporal%20Sampling%20with%20Reinforcement%20Fine-Tuning%0A%20%20for%20Video%20LLMs%0AAuthor%3A%20Yunheng%20Li%20and%20Jing%20Cheng%20and%20Shaoyong%20Jia%20and%20Hangyi%20Kuang%20and%20Shaohui%20Jiao%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%20This%20paper%20introduces%20TempSamp-R1%2C%20a%20new%20reinforcement%20fine-tuning%20framework%0Adesigned%20to%20improve%20the%20effectiveness%20of%20adapting%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20to%20video%20temporal%20grounding%20tasks.%20We%20reveal%20that%20existing%0Areinforcement%20learning%20methods%2C%20such%20as%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20rely%20on%20on-policy%20sampling%20for%20policy%20updates.%20However%2C%20in%20tasks%20with%0Alarge%20temporal%20search%20spaces%2C%20this%20strategy%20becomes%20both%20inefficient%20and%0Alimited%20in%20performance%2C%20as%20it%20often%20fails%20to%20identify%20temporally%20accurate%0Asolutions.%20To%20address%20this%20limitation%2C%20TempSamp-R1%20leverages%20ground-truth%0Aannotations%20as%20off-policy%20supervision%20to%20provide%20temporally%20precise%20guidance%2C%0Aeffectively%20compensating%20for%20the%20sparsity%20and%20misalignment%20in%20on-policy%0Asolutions.%20To%20further%20stabilize%20training%20and%20reduce%20variance%20in%20reward-based%0Aupdates%2C%20TempSamp-R1%20provides%20a%20non-linear%20soft%20advantage%20computation%20method%0Athat%20dynamically%20reshapes%20the%20reward%20feedback%20via%20an%20asymmetric%20transformation.%0ABy%20employing%20a%20hybrid%20Chain-of-Thought%20%28CoT%29%20training%20paradigm%2C%20TempSamp-R1%0Aoptimizes%20a%20single%20unified%20model%20to%20support%20both%20CoT%20and%20non-CoT%20inference%0Amodes%2C%20enabling%20efficient%20handling%20of%20queries%20with%20varying%20reasoning%0Acomplexity.%20Experimental%20results%20demonstrate%20that%20TempSamp-R1%20outperforms%0AGRPO-based%20baselines%2C%20establishing%20new%20state-of-the-art%20performance%20on%0Abenchmark%20datasets%3A%20Charades-STA%20%28R1%400.7%3A%2052.9%25%2C%20%2B2.7%25%29%2C%20ActivityNet%20Captions%0A%28R1%400.5%3A%2056.0%25%2C%20%2B5.3%25%29%2C%20and%20QVHighlights%20%28mAP%3A%2030.0%25%2C%20%2B3.0%25%29.%20Moreover%2C%0ATempSamp-R1%20shows%20robust%20few-shot%20generalization%20capabilities%20under%20limited%0Adata.%20Code%3A%20https%3A//github.com/HVision-NKU/TempSamp-R1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempSamp-R1%253A%2520Effective%2520Temporal%2520Sampling%2520with%2520Reinforcement%2520Fine-Tuning%250A%2520%2520for%2520Video%2520LLMs%26entry.906535625%3DYunheng%2520Li%2520and%2520Jing%2520Cheng%2520and%2520Shaoyong%2520Jia%2520and%2520Hangyi%2520Kuang%2520and%2520Shaohui%2520Jiao%2520and%2520Qibin%2520Hou%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520TempSamp-R1%252C%2520a%2520new%2520reinforcement%2520fine-tuning%2520framework%250Adesigned%2520to%2520improve%2520the%2520effectiveness%2520of%2520adapting%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%2520to%2520video%2520temporal%2520grounding%2520tasks.%2520We%2520reveal%2520that%2520existing%250Areinforcement%2520learning%2520methods%252C%2520such%2520as%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%252C%2520rely%2520on%2520on-policy%2520sampling%2520for%2520policy%2520updates.%2520However%252C%2520in%2520tasks%2520with%250Alarge%2520temporal%2520search%2520spaces%252C%2520this%2520strategy%2520becomes%2520both%2520inefficient%2520and%250Alimited%2520in%2520performance%252C%2520as%2520it%2520often%2520fails%2520to%2520identify%2520temporally%2520accurate%250Asolutions.%2520To%2520address%2520this%2520limitation%252C%2520TempSamp-R1%2520leverages%2520ground-truth%250Aannotations%2520as%2520off-policy%2520supervision%2520to%2520provide%2520temporally%2520precise%2520guidance%252C%250Aeffectively%2520compensating%2520for%2520the%2520sparsity%2520and%2520misalignment%2520in%2520on-policy%250Asolutions.%2520To%2520further%2520stabilize%2520training%2520and%2520reduce%2520variance%2520in%2520reward-based%250Aupdates%252C%2520TempSamp-R1%2520provides%2520a%2520non-linear%2520soft%2520advantage%2520computation%2520method%250Athat%2520dynamically%2520reshapes%2520the%2520reward%2520feedback%2520via%2520an%2520asymmetric%2520transformation.%250ABy%2520employing%2520a%2520hybrid%2520Chain-of-Thought%2520%2528CoT%2529%2520training%2520paradigm%252C%2520TempSamp-R1%250Aoptimizes%2520a%2520single%2520unified%2520model%2520to%2520support%2520both%2520CoT%2520and%2520non-CoT%2520inference%250Amodes%252C%2520enabling%2520efficient%2520handling%2520of%2520queries%2520with%2520varying%2520reasoning%250Acomplexity.%2520Experimental%2520results%2520demonstrate%2520that%2520TempSamp-R1%2520outperforms%250AGRPO-based%2520baselines%252C%2520establishing%2520new%2520state-of-the-art%2520performance%2520on%250Abenchmark%2520datasets%253A%2520Charades-STA%2520%2528R1%25400.7%253A%252052.9%2525%252C%2520%252B2.7%2525%2529%252C%2520ActivityNet%2520Captions%250A%2528R1%25400.5%253A%252056.0%2525%252C%2520%252B5.3%2525%2529%252C%2520and%2520QVHighlights%2520%2528mAP%253A%252030.0%2525%252C%2520%252B3.0%2525%2529.%2520Moreover%252C%250ATempSamp-R1%2520shows%2520robust%2520few-shot%2520generalization%2520capabilities%2520under%2520limited%250Adata.%2520Code%253A%2520https%253A//github.com/HVision-NKU/TempSamp-R1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempSamp-R1%3A%20Effective%20Temporal%20Sampling%20with%20Reinforcement%20Fine-Tuning%0A%20%20for%20Video%20LLMs&entry.906535625=Yunheng%20Li%20and%20Jing%20Cheng%20and%20Shaoyong%20Jia%20and%20Hangyi%20Kuang%20and%20Shaohui%20Jiao%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20This%20paper%20introduces%20TempSamp-R1%2C%20a%20new%20reinforcement%20fine-tuning%20framework%0Adesigned%20to%20improve%20the%20effectiveness%20of%20adapting%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20to%20video%20temporal%20grounding%20tasks.%20We%20reveal%20that%20existing%0Areinforcement%20learning%20methods%2C%20such%20as%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20rely%20on%20on-policy%20sampling%20for%20policy%20updates.%20However%2C%20in%20tasks%20with%0Alarge%20temporal%20search%20spaces%2C%20this%20strategy%20becomes%20both%20inefficient%20and%0Alimited%20in%20performance%2C%20as%20it%20often%20fails%20to%20identify%20temporally%20accurate%0Asolutions.%20To%20address%20this%20limitation%2C%20TempSamp-R1%20leverages%20ground-truth%0Aannotations%20as%20off-policy%20supervision%20to%20provide%20temporally%20precise%20guidance%2C%0Aeffectively%20compensating%20for%20the%20sparsity%20and%20misalignment%20in%20on-policy%0Asolutions.%20To%20further%20stabilize%20training%20and%20reduce%20variance%20in%20reward-based%0Aupdates%2C%20TempSamp-R1%20provides%20a%20non-linear%20soft%20advantage%20computation%20method%0Athat%20dynamically%20reshapes%20the%20reward%20feedback%20via%20an%20asymmetric%20transformation.%0ABy%20employing%20a%20hybrid%20Chain-of-Thought%20%28CoT%29%20training%20paradigm%2C%20TempSamp-R1%0Aoptimizes%20a%20single%20unified%20model%20to%20support%20both%20CoT%20and%20non-CoT%20inference%0Amodes%2C%20enabling%20efficient%20handling%20of%20queries%20with%20varying%20reasoning%0Acomplexity.%20Experimental%20results%20demonstrate%20that%20TempSamp-R1%20outperforms%0AGRPO-based%20baselines%2C%20establishing%20new%20state-of-the-art%20performance%20on%0Abenchmark%20datasets%3A%20Charades-STA%20%28R1%400.7%3A%2052.9%25%2C%20%2B2.7%25%29%2C%20ActivityNet%20Captions%0A%28R1%400.5%3A%2056.0%25%2C%20%2B5.3%25%29%2C%20and%20QVHighlights%20%28mAP%3A%2030.0%25%2C%20%2B3.0%25%29.%20Moreover%2C%0ATempSamp-R1%20shows%20robust%20few-shot%20generalization%20capabilities%20under%20limited%0Adata.%20Code%3A%20https%3A//github.com/HVision-NKU/TempSamp-R1%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18056v1&entry.124074799=Read"},
{"title": "TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model\n  Adaptation", "author": "Daiye Miao and Yufang Liu and Jie Wang and Changzhi Sun and Yunke Zhang and Demei Yan and Shaokang Dong and Qi Zhang and Yuanbin Wu", "abstract": "  LoRA has become one of the most widely used parameter-efficient fine-tuning\nmethods due to its simplicity and effectiveness. However, numerous studies have\nshown that LoRA often introduces substantial parameter redundancy, which not\nonly increases the number of trainable parameters but also hinders the\neffectiveness of fine-tuning. Since identifying redundant parameters in LoRA is\ninherently difficult, how to eliminate them efficiently and accurately remains\na challenging problem. In this paper, we propose TASO, a redundancy reduction\nmethod that leverages importance information from the pretrained model's\nweights to mitigate LoRA redundancy. Specifically, we estimate parameter\nimportance on downstream tasks and identify task-specific core regions based on\nthe distribution of importance scores. The location information of these core\nregions is then used to determine the sparse structure of LoRA modules,\nenabling redundancy removal before fine-tuning. Our approach significantly\nreduces the number of trainable parameters required for task adaptation, while\nproviding a novel task-aligned perspective for LoRA redundancy reduction.\nExperimental results demonstrate that, with a parameter budget comparable to\nLoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across\nmultiple tasks, achieving strong fine-tuning performance while effectively\neliminating redundant parameters.\n", "link": "http://arxiv.org/abs/2509.17688v1", "date": "2025-09-22", "relevancy": 2.3112, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.49}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4511}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TASO%3A%20Task-Aligned%20Sparse%20Optimization%20for%20Parameter-Efficient%20Model%0A%20%20Adaptation&body=Title%3A%20TASO%3A%20Task-Aligned%20Sparse%20Optimization%20for%20Parameter-Efficient%20Model%0A%20%20Adaptation%0AAuthor%3A%20Daiye%20Miao%20and%20Yufang%20Liu%20and%20Jie%20Wang%20and%20Changzhi%20Sun%20and%20Yunke%20Zhang%20and%20Demei%20Yan%20and%20Shaokang%20Dong%20and%20Qi%20Zhang%20and%20Yuanbin%20Wu%0AAbstract%3A%20%20%20LoRA%20has%20become%20one%20of%20the%20most%20widely%20used%20parameter-efficient%20fine-tuning%0Amethods%20due%20to%20its%20simplicity%20and%20effectiveness.%20However%2C%20numerous%20studies%20have%0Ashown%20that%20LoRA%20often%20introduces%20substantial%20parameter%20redundancy%2C%20which%20not%0Aonly%20increases%20the%20number%20of%20trainable%20parameters%20but%20also%20hinders%20the%0Aeffectiveness%20of%20fine-tuning.%20Since%20identifying%20redundant%20parameters%20in%20LoRA%20is%0Ainherently%20difficult%2C%20how%20to%20eliminate%20them%20efficiently%20and%20accurately%20remains%0Aa%20challenging%20problem.%20In%20this%20paper%2C%20we%20propose%20TASO%2C%20a%20redundancy%20reduction%0Amethod%20that%20leverages%20importance%20information%20from%20the%20pretrained%20model%27s%0Aweights%20to%20mitigate%20LoRA%20redundancy.%20Specifically%2C%20we%20estimate%20parameter%0Aimportance%20on%20downstream%20tasks%20and%20identify%20task-specific%20core%20regions%20based%20on%0Athe%20distribution%20of%20importance%20scores.%20The%20location%20information%20of%20these%20core%0Aregions%20is%20then%20used%20to%20determine%20the%20sparse%20structure%20of%20LoRA%20modules%2C%0Aenabling%20redundancy%20removal%20before%20fine-tuning.%20Our%20approach%20significantly%0Areduces%20the%20number%20of%20trainable%20parameters%20required%20for%20task%20adaptation%2C%20while%0Aproviding%20a%20novel%20task-aligned%20perspective%20for%20LoRA%20redundancy%20reduction.%0AExperimental%20results%20demonstrate%20that%2C%20with%20a%20parameter%20budget%20comparable%20to%0ALoRA%20with%20rank%20%24r%20%3D%201%24%2C%20TASO%20consistently%20outperforms%20standard%20LoRA%20across%0Amultiple%20tasks%2C%20achieving%20strong%20fine-tuning%20performance%20while%20effectively%0Aeliminating%20redundant%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTASO%253A%2520Task-Aligned%2520Sparse%2520Optimization%2520for%2520Parameter-Efficient%2520Model%250A%2520%2520Adaptation%26entry.906535625%3DDaiye%2520Miao%2520and%2520Yufang%2520Liu%2520and%2520Jie%2520Wang%2520and%2520Changzhi%2520Sun%2520and%2520Yunke%2520Zhang%2520and%2520Demei%2520Yan%2520and%2520Shaokang%2520Dong%2520and%2520Qi%2520Zhang%2520and%2520Yuanbin%2520Wu%26entry.1292438233%3D%2520%2520LoRA%2520has%2520become%2520one%2520of%2520the%2520most%2520widely%2520used%2520parameter-efficient%2520fine-tuning%250Amethods%2520due%2520to%2520its%2520simplicity%2520and%2520effectiveness.%2520However%252C%2520numerous%2520studies%2520have%250Ashown%2520that%2520LoRA%2520often%2520introduces%2520substantial%2520parameter%2520redundancy%252C%2520which%2520not%250Aonly%2520increases%2520the%2520number%2520of%2520trainable%2520parameters%2520but%2520also%2520hinders%2520the%250Aeffectiveness%2520of%2520fine-tuning.%2520Since%2520identifying%2520redundant%2520parameters%2520in%2520LoRA%2520is%250Ainherently%2520difficult%252C%2520how%2520to%2520eliminate%2520them%2520efficiently%2520and%2520accurately%2520remains%250Aa%2520challenging%2520problem.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TASO%252C%2520a%2520redundancy%2520reduction%250Amethod%2520that%2520leverages%2520importance%2520information%2520from%2520the%2520pretrained%2520model%2527s%250Aweights%2520to%2520mitigate%2520LoRA%2520redundancy.%2520Specifically%252C%2520we%2520estimate%2520parameter%250Aimportance%2520on%2520downstream%2520tasks%2520and%2520identify%2520task-specific%2520core%2520regions%2520based%2520on%250Athe%2520distribution%2520of%2520importance%2520scores.%2520The%2520location%2520information%2520of%2520these%2520core%250Aregions%2520is%2520then%2520used%2520to%2520determine%2520the%2520sparse%2520structure%2520of%2520LoRA%2520modules%252C%250Aenabling%2520redundancy%2520removal%2520before%2520fine-tuning.%2520Our%2520approach%2520significantly%250Areduces%2520the%2520number%2520of%2520trainable%2520parameters%2520required%2520for%2520task%2520adaptation%252C%2520while%250Aproviding%2520a%2520novel%2520task-aligned%2520perspective%2520for%2520LoRA%2520redundancy%2520reduction.%250AExperimental%2520results%2520demonstrate%2520that%252C%2520with%2520a%2520parameter%2520budget%2520comparable%2520to%250ALoRA%2520with%2520rank%2520%2524r%2520%253D%25201%2524%252C%2520TASO%2520consistently%2520outperforms%2520standard%2520LoRA%2520across%250Amultiple%2520tasks%252C%2520achieving%2520strong%2520fine-tuning%2520performance%2520while%2520effectively%250Aeliminating%2520redundant%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TASO%3A%20Task-Aligned%20Sparse%20Optimization%20for%20Parameter-Efficient%20Model%0A%20%20Adaptation&entry.906535625=Daiye%20Miao%20and%20Yufang%20Liu%20and%20Jie%20Wang%20and%20Changzhi%20Sun%20and%20Yunke%20Zhang%20and%20Demei%20Yan%20and%20Shaokang%20Dong%20and%20Qi%20Zhang%20and%20Yuanbin%20Wu&entry.1292438233=%20%20LoRA%20has%20become%20one%20of%20the%20most%20widely%20used%20parameter-efficient%20fine-tuning%0Amethods%20due%20to%20its%20simplicity%20and%20effectiveness.%20However%2C%20numerous%20studies%20have%0Ashown%20that%20LoRA%20often%20introduces%20substantial%20parameter%20redundancy%2C%20which%20not%0Aonly%20increases%20the%20number%20of%20trainable%20parameters%20but%20also%20hinders%20the%0Aeffectiveness%20of%20fine-tuning.%20Since%20identifying%20redundant%20parameters%20in%20LoRA%20is%0Ainherently%20difficult%2C%20how%20to%20eliminate%20them%20efficiently%20and%20accurately%20remains%0Aa%20challenging%20problem.%20In%20this%20paper%2C%20we%20propose%20TASO%2C%20a%20redundancy%20reduction%0Amethod%20that%20leverages%20importance%20information%20from%20the%20pretrained%20model%27s%0Aweights%20to%20mitigate%20LoRA%20redundancy.%20Specifically%2C%20we%20estimate%20parameter%0Aimportance%20on%20downstream%20tasks%20and%20identify%20task-specific%20core%20regions%20based%20on%0Athe%20distribution%20of%20importance%20scores.%20The%20location%20information%20of%20these%20core%0Aregions%20is%20then%20used%20to%20determine%20the%20sparse%20structure%20of%20LoRA%20modules%2C%0Aenabling%20redundancy%20removal%20before%20fine-tuning.%20Our%20approach%20significantly%0Areduces%20the%20number%20of%20trainable%20parameters%20required%20for%20task%20adaptation%2C%20while%0Aproviding%20a%20novel%20task-aligned%20perspective%20for%20LoRA%20redundancy%20reduction.%0AExperimental%20results%20demonstrate%20that%2C%20with%20a%20parameter%20budget%20comparable%20to%0ALoRA%20with%20rank%20%24r%20%3D%201%24%2C%20TASO%20consistently%20outperforms%20standard%20LoRA%20across%0Amultiple%20tasks%2C%20achieving%20strong%20fine-tuning%20performance%20while%20effectively%0Aeliminating%20redundant%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17688v1&entry.124074799=Read"},
{"title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers", "author": "Soroush Mahdi and Fardin Ayar and Ehsan Javanmardi and Manabu Tsukada and Mahdi Javanmardi", "abstract": "  Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.\n", "link": "http://arxiv.org/abs/2509.17650v1", "date": "2025-09-22", "relevancy": 2.3025, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5835}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5762}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evict3R%3A%20Training-Free%20Token%20Eviction%20for%20Memory-Bounded%20Streaming%0A%20%20Visual%20Geometry%20Transformers&body=Title%3A%20Evict3R%3A%20Training-Free%20Token%20Eviction%20for%20Memory-Bounded%20Streaming%0A%20%20Visual%20Geometry%20Transformers%0AAuthor%3A%20Soroush%20Mahdi%20and%20Fardin%20Ayar%20and%20Ehsan%20Javanmardi%20and%20Manabu%20Tsukada%20and%20Mahdi%20Javanmardi%0AAbstract%3A%20%20%20Streaming%20visual%20transformers%20like%20StreamVGGT%20achieve%20strong%203D%20perception%0Abut%20suffer%20from%20unbounded%20growth%20of%20key%20value%20%28KV%29%20memory%2C%20which%20limits%0Ascalability.%20We%20propose%20a%20training-free%2C%20inference-time%20token%20eviction%20policy%0Athat%20bounds%20memory%20by%20discarding%20redundant%20tokens%20while%20keeping%20the%20most%0Ainformative%20ones.%20Our%20method%20uses%20significantly%20less%20memory%20with%20little%20to%20no%0Adrop%20in%20accuracy%3A%20on%207-Scenes%20with%20long%20sequences%20it%20reduces%20peak%20memory%20from%0A18.63%20GB%20to%209.39%20GB%20while%20accuracy%20and%20completeness%20drop%20by%20only%200.003.%20Under%0Astrict%20memory%20budgets%2C%20eviction%20enables%20denser%20frame%20sampling%2C%20which%20improves%0Areconstruction%20accuracy%20compared%20to%20the%20baseline.%20Experiments%20across%20video%0Adepth%20estimation%20%28Sintel%2C%20KITTI%29%2C%203D%20reconstruction%20%287-Scenes%2C%20NRGBD%29%2C%20and%0Acamera%20pose%20estimation%20%28Sintel%2C%20TUM-dynamics%29%20show%20that%20our%20approach%20closely%0Amatches%20StreamVGGT%20at%20a%20fraction%20of%20the%20memory%20and%20makes%20long-horizon%20streaming%0Ainference%20more%20practical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvict3R%253A%2520Training-Free%2520Token%2520Eviction%2520for%2520Memory-Bounded%2520Streaming%250A%2520%2520Visual%2520Geometry%2520Transformers%26entry.906535625%3DSoroush%2520Mahdi%2520and%2520Fardin%2520Ayar%2520and%2520Ehsan%2520Javanmardi%2520and%2520Manabu%2520Tsukada%2520and%2520Mahdi%2520Javanmardi%26entry.1292438233%3D%2520%2520Streaming%2520visual%2520transformers%2520like%2520StreamVGGT%2520achieve%2520strong%25203D%2520perception%250Abut%2520suffer%2520from%2520unbounded%2520growth%2520of%2520key%2520value%2520%2528KV%2529%2520memory%252C%2520which%2520limits%250Ascalability.%2520We%2520propose%2520a%2520training-free%252C%2520inference-time%2520token%2520eviction%2520policy%250Athat%2520bounds%2520memory%2520by%2520discarding%2520redundant%2520tokens%2520while%2520keeping%2520the%2520most%250Ainformative%2520ones.%2520Our%2520method%2520uses%2520significantly%2520less%2520memory%2520with%2520little%2520to%2520no%250Adrop%2520in%2520accuracy%253A%2520on%25207-Scenes%2520with%2520long%2520sequences%2520it%2520reduces%2520peak%2520memory%2520from%250A18.63%2520GB%2520to%25209.39%2520GB%2520while%2520accuracy%2520and%2520completeness%2520drop%2520by%2520only%25200.003.%2520Under%250Astrict%2520memory%2520budgets%252C%2520eviction%2520enables%2520denser%2520frame%2520sampling%252C%2520which%2520improves%250Areconstruction%2520accuracy%2520compared%2520to%2520the%2520baseline.%2520Experiments%2520across%2520video%250Adepth%2520estimation%2520%2528Sintel%252C%2520KITTI%2529%252C%25203D%2520reconstruction%2520%25287-Scenes%252C%2520NRGBD%2529%252C%2520and%250Acamera%2520pose%2520estimation%2520%2528Sintel%252C%2520TUM-dynamics%2529%2520show%2520that%2520our%2520approach%2520closely%250Amatches%2520StreamVGGT%2520at%2520a%2520fraction%2520of%2520the%2520memory%2520and%2520makes%2520long-horizon%2520streaming%250Ainference%2520more%2520practical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evict3R%3A%20Training-Free%20Token%20Eviction%20for%20Memory-Bounded%20Streaming%0A%20%20Visual%20Geometry%20Transformers&entry.906535625=Soroush%20Mahdi%20and%20Fardin%20Ayar%20and%20Ehsan%20Javanmardi%20and%20Manabu%20Tsukada%20and%20Mahdi%20Javanmardi&entry.1292438233=%20%20Streaming%20visual%20transformers%20like%20StreamVGGT%20achieve%20strong%203D%20perception%0Abut%20suffer%20from%20unbounded%20growth%20of%20key%20value%20%28KV%29%20memory%2C%20which%20limits%0Ascalability.%20We%20propose%20a%20training-free%2C%20inference-time%20token%20eviction%20policy%0Athat%20bounds%20memory%20by%20discarding%20redundant%20tokens%20while%20keeping%20the%20most%0Ainformative%20ones.%20Our%20method%20uses%20significantly%20less%20memory%20with%20little%20to%20no%0Adrop%20in%20accuracy%3A%20on%207-Scenes%20with%20long%20sequences%20it%20reduces%20peak%20memory%20from%0A18.63%20GB%20to%209.39%20GB%20while%20accuracy%20and%20completeness%20drop%20by%20only%200.003.%20Under%0Astrict%20memory%20budgets%2C%20eviction%20enables%20denser%20frame%20sampling%2C%20which%20improves%0Areconstruction%20accuracy%20compared%20to%20the%20baseline.%20Experiments%20across%20video%0Adepth%20estimation%20%28Sintel%2C%20KITTI%29%2C%203D%20reconstruction%20%287-Scenes%2C%20NRGBD%29%2C%20and%0Acamera%20pose%20estimation%20%28Sintel%2C%20TUM-dynamics%29%20show%20that%20our%20approach%20closely%0Amatches%20StreamVGGT%20at%20a%20fraction%20of%20the%20memory%20and%20makes%20long-horizon%20streaming%0Ainference%20more%20practical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17650v1&entry.124074799=Read"},
{"title": "Predicting Depth Maps from Single RGB Images and Addressing Missing\n  Information in Depth Estimation", "author": "Mohamad Mofeed Chaar and Jamal Raiyn and Galia Weidl", "abstract": "  Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it\nplays a key role in detecting and measuring objects in the vehicle's\nsurroundings. However, a significant challenge in this domain arises from\nmissing information in Depth images, where certain points are not measurable\ndue to gaps or inconsistencies in pixel data. Our research addresses two key\ntasks to overcome this challenge. First, we developed an algorithm using a\nmulti-layered training approach to generate Depth images from a single RGB\nimage. Second, we addressed the issue of missing information in Depth images by\napplying our algorithm to rectify these gaps, resulting in Depth images with\ncomplete and accurate data. We further tested our algorithm on the Cityscapes\ndataset and successfully resolved the missing information in its Depth images,\ndemonstrating the effectiveness of our approach in real-world urban\nenvironments.\n", "link": "http://arxiv.org/abs/2509.17686v1", "date": "2025-09-22", "relevancy": 2.3023, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.595}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5738}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Depth%20Maps%20from%20Single%20RGB%20Images%20and%20Addressing%20Missing%0A%20%20Information%20in%20Depth%20Estimation&body=Title%3A%20Predicting%20Depth%20Maps%20from%20Single%20RGB%20Images%20and%20Addressing%20Missing%0A%20%20Information%20in%20Depth%20Estimation%0AAuthor%3A%20Mohamad%20Mofeed%20Chaar%20and%20Jamal%20Raiyn%20and%20Galia%20Weidl%0AAbstract%3A%20%20%20Depth%20imaging%20is%20a%20crucial%20area%20in%20Autonomous%20Driving%20Systems%20%28ADS%29%2C%20as%20it%0Aplays%20a%20key%20role%20in%20detecting%20and%20measuring%20objects%20in%20the%20vehicle%27s%0Asurroundings.%20However%2C%20a%20significant%20challenge%20in%20this%20domain%20arises%20from%0Amissing%20information%20in%20Depth%20images%2C%20where%20certain%20points%20are%20not%20measurable%0Adue%20to%20gaps%20or%20inconsistencies%20in%20pixel%20data.%20Our%20research%20addresses%20two%20key%0Atasks%20to%20overcome%20this%20challenge.%20First%2C%20we%20developed%20an%20algorithm%20using%20a%0Amulti-layered%20training%20approach%20to%20generate%20Depth%20images%20from%20a%20single%20RGB%0Aimage.%20Second%2C%20we%20addressed%20the%20issue%20of%20missing%20information%20in%20Depth%20images%20by%0Aapplying%20our%20algorithm%20to%20rectify%20these%20gaps%2C%20resulting%20in%20Depth%20images%20with%0Acomplete%20and%20accurate%20data.%20We%20further%20tested%20our%20algorithm%20on%20the%20Cityscapes%0Adataset%20and%20successfully%20resolved%20the%20missing%20information%20in%20its%20Depth%20images%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20approach%20in%20real-world%20urban%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Depth%2520Maps%2520from%2520Single%2520RGB%2520Images%2520and%2520Addressing%2520Missing%250A%2520%2520Information%2520in%2520Depth%2520Estimation%26entry.906535625%3DMohamad%2520Mofeed%2520Chaar%2520and%2520Jamal%2520Raiyn%2520and%2520Galia%2520Weidl%26entry.1292438233%3D%2520%2520Depth%2520imaging%2520is%2520a%2520crucial%2520area%2520in%2520Autonomous%2520Driving%2520Systems%2520%2528ADS%2529%252C%2520as%2520it%250Aplays%2520a%2520key%2520role%2520in%2520detecting%2520and%2520measuring%2520objects%2520in%2520the%2520vehicle%2527s%250Asurroundings.%2520However%252C%2520a%2520significant%2520challenge%2520in%2520this%2520domain%2520arises%2520from%250Amissing%2520information%2520in%2520Depth%2520images%252C%2520where%2520certain%2520points%2520are%2520not%2520measurable%250Adue%2520to%2520gaps%2520or%2520inconsistencies%2520in%2520pixel%2520data.%2520Our%2520research%2520addresses%2520two%2520key%250Atasks%2520to%2520overcome%2520this%2520challenge.%2520First%252C%2520we%2520developed%2520an%2520algorithm%2520using%2520a%250Amulti-layered%2520training%2520approach%2520to%2520generate%2520Depth%2520images%2520from%2520a%2520single%2520RGB%250Aimage.%2520Second%252C%2520we%2520addressed%2520the%2520issue%2520of%2520missing%2520information%2520in%2520Depth%2520images%2520by%250Aapplying%2520our%2520algorithm%2520to%2520rectify%2520these%2520gaps%252C%2520resulting%2520in%2520Depth%2520images%2520with%250Acomplete%2520and%2520accurate%2520data.%2520We%2520further%2520tested%2520our%2520algorithm%2520on%2520the%2520Cityscapes%250Adataset%2520and%2520successfully%2520resolved%2520the%2520missing%2520information%2520in%2520its%2520Depth%2520images%252C%250Ademonstrating%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520real-world%2520urban%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Depth%20Maps%20from%20Single%20RGB%20Images%20and%20Addressing%20Missing%0A%20%20Information%20in%20Depth%20Estimation&entry.906535625=Mohamad%20Mofeed%20Chaar%20and%20Jamal%20Raiyn%20and%20Galia%20Weidl&entry.1292438233=%20%20Depth%20imaging%20is%20a%20crucial%20area%20in%20Autonomous%20Driving%20Systems%20%28ADS%29%2C%20as%20it%0Aplays%20a%20key%20role%20in%20detecting%20and%20measuring%20objects%20in%20the%20vehicle%27s%0Asurroundings.%20However%2C%20a%20significant%20challenge%20in%20this%20domain%20arises%20from%0Amissing%20information%20in%20Depth%20images%2C%20where%20certain%20points%20are%20not%20measurable%0Adue%20to%20gaps%20or%20inconsistencies%20in%20pixel%20data.%20Our%20research%20addresses%20two%20key%0Atasks%20to%20overcome%20this%20challenge.%20First%2C%20we%20developed%20an%20algorithm%20using%20a%0Amulti-layered%20training%20approach%20to%20generate%20Depth%20images%20from%20a%20single%20RGB%0Aimage.%20Second%2C%20we%20addressed%20the%20issue%20of%20missing%20information%20in%20Depth%20images%20by%0Aapplying%20our%20algorithm%20to%20rectify%20these%20gaps%2C%20resulting%20in%20Depth%20images%20with%0Acomplete%20and%20accurate%20data.%20We%20further%20tested%20our%20algorithm%20on%20the%20Cityscapes%0Adataset%20and%20successfully%20resolved%20the%20missing%20information%20in%20its%20Depth%20images%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20approach%20in%20real-world%20urban%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17686v1&entry.124074799=Read"},
{"title": "Towards Seeing Bones at Radio Frequency", "author": "Yiwen Song and Hongyang Li and Kuang Yuan and Ran Bi and Swarun Kumar", "abstract": "  Wireless sensing literature has long aspired to achieve X-ray-like vision at\nradio frequencies. Yet, state-of-the-art wireless sensing literature has yet to\ngenerate the archetypal X-ray image: one of the bones beneath flesh. In this\npaper, we explore MCT, a penetration-based RF-imaging system for imaging bones\nat mm-resolution, one that significantly exceeds prior penetration-based RF\nimaging literature. Indeed the long wavelength, significant attenuation and\ncomplex diffraction that occur as RF propagates through flesh, have long\nlimited imaging resolution (to several centimeters at best). We address these\nconcerns through a novel penetration-based synthetic aperture algorithm,\ncoupled with a learning-based pipeline to correct for diffraction-induced\nartifacts. A detailed evaluation of meat models demonstrates a resolution\nimprovement from sub-decimeter to sub-centimeter over prior art in RF\npenetrative imaging.\n", "link": "http://arxiv.org/abs/2509.17979v1", "date": "2025-09-22", "relevancy": 2.3014, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4705}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Seeing%20Bones%20at%20Radio%20Frequency&body=Title%3A%20Towards%20Seeing%20Bones%20at%20Radio%20Frequency%0AAuthor%3A%20Yiwen%20Song%20and%20Hongyang%20Li%20and%20Kuang%20Yuan%20and%20Ran%20Bi%20and%20Swarun%20Kumar%0AAbstract%3A%20%20%20Wireless%20sensing%20literature%20has%20long%20aspired%20to%20achieve%20X-ray-like%20vision%20at%0Aradio%20frequencies.%20Yet%2C%20state-of-the-art%20wireless%20sensing%20literature%20has%20yet%20to%0Agenerate%20the%20archetypal%20X-ray%20image%3A%20one%20of%20the%20bones%20beneath%20flesh.%20In%20this%0Apaper%2C%20we%20explore%20MCT%2C%20a%20penetration-based%20RF-imaging%20system%20for%20imaging%20bones%0Aat%20mm-resolution%2C%20one%20that%20significantly%20exceeds%20prior%20penetration-based%20RF%0Aimaging%20literature.%20Indeed%20the%20long%20wavelength%2C%20significant%20attenuation%20and%0Acomplex%20diffraction%20that%20occur%20as%20RF%20propagates%20through%20flesh%2C%20have%20long%0Alimited%20imaging%20resolution%20%28to%20several%20centimeters%20at%20best%29.%20We%20address%20these%0Aconcerns%20through%20a%20novel%20penetration-based%20synthetic%20aperture%20algorithm%2C%0Acoupled%20with%20a%20learning-based%20pipeline%20to%20correct%20for%20diffraction-induced%0Aartifacts.%20A%20detailed%20evaluation%20of%20meat%20models%20demonstrates%20a%20resolution%0Aimprovement%20from%20sub-decimeter%20to%20sub-centimeter%20over%20prior%20art%20in%20RF%0Apenetrative%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Seeing%2520Bones%2520at%2520Radio%2520Frequency%26entry.906535625%3DYiwen%2520Song%2520and%2520Hongyang%2520Li%2520and%2520Kuang%2520Yuan%2520and%2520Ran%2520Bi%2520and%2520Swarun%2520Kumar%26entry.1292438233%3D%2520%2520Wireless%2520sensing%2520literature%2520has%2520long%2520aspired%2520to%2520achieve%2520X-ray-like%2520vision%2520at%250Aradio%2520frequencies.%2520Yet%252C%2520state-of-the-art%2520wireless%2520sensing%2520literature%2520has%2520yet%2520to%250Agenerate%2520the%2520archetypal%2520X-ray%2520image%253A%2520one%2520of%2520the%2520bones%2520beneath%2520flesh.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520MCT%252C%2520a%2520penetration-based%2520RF-imaging%2520system%2520for%2520imaging%2520bones%250Aat%2520mm-resolution%252C%2520one%2520that%2520significantly%2520exceeds%2520prior%2520penetration-based%2520RF%250Aimaging%2520literature.%2520Indeed%2520the%2520long%2520wavelength%252C%2520significant%2520attenuation%2520and%250Acomplex%2520diffraction%2520that%2520occur%2520as%2520RF%2520propagates%2520through%2520flesh%252C%2520have%2520long%250Alimited%2520imaging%2520resolution%2520%2528to%2520several%2520centimeters%2520at%2520best%2529.%2520We%2520address%2520these%250Aconcerns%2520through%2520a%2520novel%2520penetration-based%2520synthetic%2520aperture%2520algorithm%252C%250Acoupled%2520with%2520a%2520learning-based%2520pipeline%2520to%2520correct%2520for%2520diffraction-induced%250Aartifacts.%2520A%2520detailed%2520evaluation%2520of%2520meat%2520models%2520demonstrates%2520a%2520resolution%250Aimprovement%2520from%2520sub-decimeter%2520to%2520sub-centimeter%2520over%2520prior%2520art%2520in%2520RF%250Apenetrative%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Seeing%20Bones%20at%20Radio%20Frequency&entry.906535625=Yiwen%20Song%20and%20Hongyang%20Li%20and%20Kuang%20Yuan%20and%20Ran%20Bi%20and%20Swarun%20Kumar&entry.1292438233=%20%20Wireless%20sensing%20literature%20has%20long%20aspired%20to%20achieve%20X-ray-like%20vision%20at%0Aradio%20frequencies.%20Yet%2C%20state-of-the-art%20wireless%20sensing%20literature%20has%20yet%20to%0Agenerate%20the%20archetypal%20X-ray%20image%3A%20one%20of%20the%20bones%20beneath%20flesh.%20In%20this%0Apaper%2C%20we%20explore%20MCT%2C%20a%20penetration-based%20RF-imaging%20system%20for%20imaging%20bones%0Aat%20mm-resolution%2C%20one%20that%20significantly%20exceeds%20prior%20penetration-based%20RF%0Aimaging%20literature.%20Indeed%20the%20long%20wavelength%2C%20significant%20attenuation%20and%0Acomplex%20diffraction%20that%20occur%20as%20RF%20propagates%20through%20flesh%2C%20have%20long%0Alimited%20imaging%20resolution%20%28to%20several%20centimeters%20at%20best%29.%20We%20address%20these%0Aconcerns%20through%20a%20novel%20penetration-based%20synthetic%20aperture%20algorithm%2C%0Acoupled%20with%20a%20learning-based%20pipeline%20to%20correct%20for%20diffraction-induced%0Aartifacts.%20A%20detailed%20evaluation%20of%20meat%20models%20demonstrates%20a%20resolution%0Aimprovement%20from%20sub-decimeter%20to%20sub-centimeter%20over%20prior%20art%20in%20RF%0Apenetrative%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17979v1&entry.124074799=Read"},
{"title": "Vector Field-Guided Learning Predictive Control for Motion Planning of\n  Mobile Robots with Uncertain Dynamics", "author": "Yang Lu and Weijia Yao and Yongqian Xiao and Xinglong Zhang and Xin Xu and Yaonan Wang and Dingbang Xiao", "abstract": "  In obstacle-dense scenarios, providing safe guidance for mobile robots is\ncritical to improve the safe maneuvering capability. However, the guidance\nprovided by standard guiding vector fields (GVFs) may limit the motion\ncapability due to the improper curvature of the integral curve when traversing\nobstacles. On the other hand, robotic system dynamics are often time-varying,\nuncertain, and even unknown during the motion planning process. Therefore, many\nexisting kinodynamic motion planning methods could not achieve satisfactory\nreliability in guaranteeing safety. To address these challenges, we propose a\ntwo-level Vector Field-guided Learning Predictive Control (VF-LPC) approach\nthat improves safe maneuverability. The first level, the guiding level,\ngenerates safe desired trajectories using the designed kinodynamic GVF,\nenabling safe motion in obstacle-dense environments. The second level, the\nIntegrated Motion Planning and Control (IMPC) level, first uses a deep Koopman\noperator to learn a nominal dynamics model offline and then updates the model\nuncertainties online using sparse Gaussian processes (GPs). The learned\ndynamics and a game-based safe barrier function are then incorporated into the\nLPC framework to generate near-optimal planning solutions. Extensive\nsimulations and real-world experiments were conducted on quadrotor unmanned\naerial vehicles and unmanned ground vehicles, demonstrating that VF-LPC enables\nrobots to maneuver safely.\n", "link": "http://arxiv.org/abs/2405.08283v4", "date": "2025-09-22", "relevancy": 2.2934, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5856}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5687}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vector%20Field-Guided%20Learning%20Predictive%20Control%20for%20Motion%20Planning%20of%0A%20%20Mobile%20Robots%20with%20Uncertain%20Dynamics&body=Title%3A%20Vector%20Field-Guided%20Learning%20Predictive%20Control%20for%20Motion%20Planning%20of%0A%20%20Mobile%20Robots%20with%20Uncertain%20Dynamics%0AAuthor%3A%20Yang%20Lu%20and%20Weijia%20Yao%20and%20Yongqian%20Xiao%20and%20Xinglong%20Zhang%20and%20Xin%20Xu%20and%20Yaonan%20Wang%20and%20Dingbang%20Xiao%0AAbstract%3A%20%20%20In%20obstacle-dense%20scenarios%2C%20providing%20safe%20guidance%20for%20mobile%20robots%20is%0Acritical%20to%20improve%20the%20safe%20maneuvering%20capability.%20However%2C%20the%20guidance%0Aprovided%20by%20standard%20guiding%20vector%20fields%20%28GVFs%29%20may%20limit%20the%20motion%0Acapability%20due%20to%20the%20improper%20curvature%20of%20the%20integral%20curve%20when%20traversing%0Aobstacles.%20On%20the%20other%20hand%2C%20robotic%20system%20dynamics%20are%20often%20time-varying%2C%0Auncertain%2C%20and%20even%20unknown%20during%20the%20motion%20planning%20process.%20Therefore%2C%20many%0Aexisting%20kinodynamic%20motion%20planning%20methods%20could%20not%20achieve%20satisfactory%0Areliability%20in%20guaranteeing%20safety.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Atwo-level%20Vector%20Field-guided%20Learning%20Predictive%20Control%20%28VF-LPC%29%20approach%0Athat%20improves%20safe%20maneuverability.%20The%20first%20level%2C%20the%20guiding%20level%2C%0Agenerates%20safe%20desired%20trajectories%20using%20the%20designed%20kinodynamic%20GVF%2C%0Aenabling%20safe%20motion%20in%20obstacle-dense%20environments.%20The%20second%20level%2C%20the%0AIntegrated%20Motion%20Planning%20and%20Control%20%28IMPC%29%20level%2C%20first%20uses%20a%20deep%20Koopman%0Aoperator%20to%20learn%20a%20nominal%20dynamics%20model%20offline%20and%20then%20updates%20the%20model%0Auncertainties%20online%20using%20sparse%20Gaussian%20processes%20%28GPs%29.%20The%20learned%0Adynamics%20and%20a%20game-based%20safe%20barrier%20function%20are%20then%20incorporated%20into%20the%0ALPC%20framework%20to%20generate%20near-optimal%20planning%20solutions.%20Extensive%0Asimulations%20and%20real-world%20experiments%20were%20conducted%20on%20quadrotor%20unmanned%0Aaerial%20vehicles%20and%20unmanned%20ground%20vehicles%2C%20demonstrating%20that%20VF-LPC%20enables%0Arobots%20to%20maneuver%20safely.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08283v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVector%2520Field-Guided%2520Learning%2520Predictive%2520Control%2520for%2520Motion%2520Planning%2520of%250A%2520%2520Mobile%2520Robots%2520with%2520Uncertain%2520Dynamics%26entry.906535625%3DYang%2520Lu%2520and%2520Weijia%2520Yao%2520and%2520Yongqian%2520Xiao%2520and%2520Xinglong%2520Zhang%2520and%2520Xin%2520Xu%2520and%2520Yaonan%2520Wang%2520and%2520Dingbang%2520Xiao%26entry.1292438233%3D%2520%2520In%2520obstacle-dense%2520scenarios%252C%2520providing%2520safe%2520guidance%2520for%2520mobile%2520robots%2520is%250Acritical%2520to%2520improve%2520the%2520safe%2520maneuvering%2520capability.%2520However%252C%2520the%2520guidance%250Aprovided%2520by%2520standard%2520guiding%2520vector%2520fields%2520%2528GVFs%2529%2520may%2520limit%2520the%2520motion%250Acapability%2520due%2520to%2520the%2520improper%2520curvature%2520of%2520the%2520integral%2520curve%2520when%2520traversing%250Aobstacles.%2520On%2520the%2520other%2520hand%252C%2520robotic%2520system%2520dynamics%2520are%2520often%2520time-varying%252C%250Auncertain%252C%2520and%2520even%2520unknown%2520during%2520the%2520motion%2520planning%2520process.%2520Therefore%252C%2520many%250Aexisting%2520kinodynamic%2520motion%2520planning%2520methods%2520could%2520not%2520achieve%2520satisfactory%250Areliability%2520in%2520guaranteeing%2520safety.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Atwo-level%2520Vector%2520Field-guided%2520Learning%2520Predictive%2520Control%2520%2528VF-LPC%2529%2520approach%250Athat%2520improves%2520safe%2520maneuverability.%2520The%2520first%2520level%252C%2520the%2520guiding%2520level%252C%250Agenerates%2520safe%2520desired%2520trajectories%2520using%2520the%2520designed%2520kinodynamic%2520GVF%252C%250Aenabling%2520safe%2520motion%2520in%2520obstacle-dense%2520environments.%2520The%2520second%2520level%252C%2520the%250AIntegrated%2520Motion%2520Planning%2520and%2520Control%2520%2528IMPC%2529%2520level%252C%2520first%2520uses%2520a%2520deep%2520Koopman%250Aoperator%2520to%2520learn%2520a%2520nominal%2520dynamics%2520model%2520offline%2520and%2520then%2520updates%2520the%2520model%250Auncertainties%2520online%2520using%2520sparse%2520Gaussian%2520processes%2520%2528GPs%2529.%2520The%2520learned%250Adynamics%2520and%2520a%2520game-based%2520safe%2520barrier%2520function%2520are%2520then%2520incorporated%2520into%2520the%250ALPC%2520framework%2520to%2520generate%2520near-optimal%2520planning%2520solutions.%2520Extensive%250Asimulations%2520and%2520real-world%2520experiments%2520were%2520conducted%2520on%2520quadrotor%2520unmanned%250Aaerial%2520vehicles%2520and%2520unmanned%2520ground%2520vehicles%252C%2520demonstrating%2520that%2520VF-LPC%2520enables%250Arobots%2520to%2520maneuver%2520safely.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08283v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vector%20Field-Guided%20Learning%20Predictive%20Control%20for%20Motion%20Planning%20of%0A%20%20Mobile%20Robots%20with%20Uncertain%20Dynamics&entry.906535625=Yang%20Lu%20and%20Weijia%20Yao%20and%20Yongqian%20Xiao%20and%20Xinglong%20Zhang%20and%20Xin%20Xu%20and%20Yaonan%20Wang%20and%20Dingbang%20Xiao&entry.1292438233=%20%20In%20obstacle-dense%20scenarios%2C%20providing%20safe%20guidance%20for%20mobile%20robots%20is%0Acritical%20to%20improve%20the%20safe%20maneuvering%20capability.%20However%2C%20the%20guidance%0Aprovided%20by%20standard%20guiding%20vector%20fields%20%28GVFs%29%20may%20limit%20the%20motion%0Acapability%20due%20to%20the%20improper%20curvature%20of%20the%20integral%20curve%20when%20traversing%0Aobstacles.%20On%20the%20other%20hand%2C%20robotic%20system%20dynamics%20are%20often%20time-varying%2C%0Auncertain%2C%20and%20even%20unknown%20during%20the%20motion%20planning%20process.%20Therefore%2C%20many%0Aexisting%20kinodynamic%20motion%20planning%20methods%20could%20not%20achieve%20satisfactory%0Areliability%20in%20guaranteeing%20safety.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Atwo-level%20Vector%20Field-guided%20Learning%20Predictive%20Control%20%28VF-LPC%29%20approach%0Athat%20improves%20safe%20maneuverability.%20The%20first%20level%2C%20the%20guiding%20level%2C%0Agenerates%20safe%20desired%20trajectories%20using%20the%20designed%20kinodynamic%20GVF%2C%0Aenabling%20safe%20motion%20in%20obstacle-dense%20environments.%20The%20second%20level%2C%20the%0AIntegrated%20Motion%20Planning%20and%20Control%20%28IMPC%29%20level%2C%20first%20uses%20a%20deep%20Koopman%0Aoperator%20to%20learn%20a%20nominal%20dynamics%20model%20offline%20and%20then%20updates%20the%20model%0Auncertainties%20online%20using%20sparse%20Gaussian%20processes%20%28GPs%29.%20The%20learned%0Adynamics%20and%20a%20game-based%20safe%20barrier%20function%20are%20then%20incorporated%20into%20the%0ALPC%20framework%20to%20generate%20near-optimal%20planning%20solutions.%20Extensive%0Asimulations%20and%20real-world%20experiments%20were%20conducted%20on%20quadrotor%20unmanned%0Aaerial%20vehicles%20and%20unmanned%20ground%20vehicles%2C%20demonstrating%20that%20VF-LPC%20enables%0Arobots%20to%20maneuver%20safely.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08283v4&entry.124074799=Read"},
{"title": "LieDetect: Detection of representation orbits of compact Lie groups from\n  point clouds", "author": "Henrique Ennes and Rapha\u00ebl Tinarrage", "abstract": "  We suggest a new algorithm to estimate representations of compact Lie groups\nfrom finite samples of their orbits. Different from other reported techniques,\nour method allows the retrieval of the precise representation type as a direct\nsum of irreducible representations. Moreover, the knowledge of the\nrepresentation type permits the reconstruction of its orbit, which is useful\nfor identifying the Lie group that generates the action, from a finite list of\ncandidates. Our algorithm is general for any compact Lie group, but only\ninstantiations for SO(2), T^d, SU(2), and SO(3) are considered. Theoretical\nguarantees of robustness in terms of Hausdorff and Wasserstein distances are\nderived. Our tools are drawn from geometric measure theory, computational\ngeometry, and optimization on matrix manifolds. The algorithm is tested for\nsynthetic data up to dimension 32, as well as real-life applications in image\nanalysis, harmonic analysis, density estimation, equivariant neural networks,\nchemical conformational spaces, and classical mechanics systems, achieving very\naccurate results.\n", "link": "http://arxiv.org/abs/2309.03086v3", "date": "2025-09-22", "relevancy": 2.2932, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4658}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4596}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LieDetect%3A%20Detection%20of%20representation%20orbits%20of%20compact%20Lie%20groups%20from%0A%20%20point%20clouds&body=Title%3A%20LieDetect%3A%20Detection%20of%20representation%20orbits%20of%20compact%20Lie%20groups%20from%0A%20%20point%20clouds%0AAuthor%3A%20Henrique%20Ennes%20and%20Rapha%C3%ABl%20Tinarrage%0AAbstract%3A%20%20%20We%20suggest%20a%20new%20algorithm%20to%20estimate%20representations%20of%20compact%20Lie%20groups%0Afrom%20finite%20samples%20of%20their%20orbits.%20Different%20from%20other%20reported%20techniques%2C%0Aour%20method%20allows%20the%20retrieval%20of%20the%20precise%20representation%20type%20as%20a%20direct%0Asum%20of%20irreducible%20representations.%20Moreover%2C%20the%20knowledge%20of%20the%0Arepresentation%20type%20permits%20the%20reconstruction%20of%20its%20orbit%2C%20which%20is%20useful%0Afor%20identifying%20the%20Lie%20group%20that%20generates%20the%20action%2C%20from%20a%20finite%20list%20of%0Acandidates.%20Our%20algorithm%20is%20general%20for%20any%20compact%20Lie%20group%2C%20but%20only%0Ainstantiations%20for%20SO%282%29%2C%20T%5Ed%2C%20SU%282%29%2C%20and%20SO%283%29%20are%20considered.%20Theoretical%0Aguarantees%20of%20robustness%20in%20terms%20of%20Hausdorff%20and%20Wasserstein%20distances%20are%0Aderived.%20Our%20tools%20are%20drawn%20from%20geometric%20measure%20theory%2C%20computational%0Ageometry%2C%20and%20optimization%20on%20matrix%20manifolds.%20The%20algorithm%20is%20tested%20for%0Asynthetic%20data%20up%20to%20dimension%2032%2C%20as%20well%20as%20real-life%20applications%20in%20image%0Aanalysis%2C%20harmonic%20analysis%2C%20density%20estimation%2C%20equivariant%20neural%20networks%2C%0Achemical%20conformational%20spaces%2C%20and%20classical%20mechanics%20systems%2C%20achieving%20very%0Aaccurate%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03086v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLieDetect%253A%2520Detection%2520of%2520representation%2520orbits%2520of%2520compact%2520Lie%2520groups%2520from%250A%2520%2520point%2520clouds%26entry.906535625%3DHenrique%2520Ennes%2520and%2520Rapha%25C3%25ABl%2520Tinarrage%26entry.1292438233%3D%2520%2520We%2520suggest%2520a%2520new%2520algorithm%2520to%2520estimate%2520representations%2520of%2520compact%2520Lie%2520groups%250Afrom%2520finite%2520samples%2520of%2520their%2520orbits.%2520Different%2520from%2520other%2520reported%2520techniques%252C%250Aour%2520method%2520allows%2520the%2520retrieval%2520of%2520the%2520precise%2520representation%2520type%2520as%2520a%2520direct%250Asum%2520of%2520irreducible%2520representations.%2520Moreover%252C%2520the%2520knowledge%2520of%2520the%250Arepresentation%2520type%2520permits%2520the%2520reconstruction%2520of%2520its%2520orbit%252C%2520which%2520is%2520useful%250Afor%2520identifying%2520the%2520Lie%2520group%2520that%2520generates%2520the%2520action%252C%2520from%2520a%2520finite%2520list%2520of%250Acandidates.%2520Our%2520algorithm%2520is%2520general%2520for%2520any%2520compact%2520Lie%2520group%252C%2520but%2520only%250Ainstantiations%2520for%2520SO%25282%2529%252C%2520T%255Ed%252C%2520SU%25282%2529%252C%2520and%2520SO%25283%2529%2520are%2520considered.%2520Theoretical%250Aguarantees%2520of%2520robustness%2520in%2520terms%2520of%2520Hausdorff%2520and%2520Wasserstein%2520distances%2520are%250Aderived.%2520Our%2520tools%2520are%2520drawn%2520from%2520geometric%2520measure%2520theory%252C%2520computational%250Ageometry%252C%2520and%2520optimization%2520on%2520matrix%2520manifolds.%2520The%2520algorithm%2520is%2520tested%2520for%250Asynthetic%2520data%2520up%2520to%2520dimension%252032%252C%2520as%2520well%2520as%2520real-life%2520applications%2520in%2520image%250Aanalysis%252C%2520harmonic%2520analysis%252C%2520density%2520estimation%252C%2520equivariant%2520neural%2520networks%252C%250Achemical%2520conformational%2520spaces%252C%2520and%2520classical%2520mechanics%2520systems%252C%2520achieving%2520very%250Aaccurate%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.03086v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LieDetect%3A%20Detection%20of%20representation%20orbits%20of%20compact%20Lie%20groups%20from%0A%20%20point%20clouds&entry.906535625=Henrique%20Ennes%20and%20Rapha%C3%ABl%20Tinarrage&entry.1292438233=%20%20We%20suggest%20a%20new%20algorithm%20to%20estimate%20representations%20of%20compact%20Lie%20groups%0Afrom%20finite%20samples%20of%20their%20orbits.%20Different%20from%20other%20reported%20techniques%2C%0Aour%20method%20allows%20the%20retrieval%20of%20the%20precise%20representation%20type%20as%20a%20direct%0Asum%20of%20irreducible%20representations.%20Moreover%2C%20the%20knowledge%20of%20the%0Arepresentation%20type%20permits%20the%20reconstruction%20of%20its%20orbit%2C%20which%20is%20useful%0Afor%20identifying%20the%20Lie%20group%20that%20generates%20the%20action%2C%20from%20a%20finite%20list%20of%0Acandidates.%20Our%20algorithm%20is%20general%20for%20any%20compact%20Lie%20group%2C%20but%20only%0Ainstantiations%20for%20SO%282%29%2C%20T%5Ed%2C%20SU%282%29%2C%20and%20SO%283%29%20are%20considered.%20Theoretical%0Aguarantees%20of%20robustness%20in%20terms%20of%20Hausdorff%20and%20Wasserstein%20distances%20are%0Aderived.%20Our%20tools%20are%20drawn%20from%20geometric%20measure%20theory%2C%20computational%0Ageometry%2C%20and%20optimization%20on%20matrix%20manifolds.%20The%20algorithm%20is%20tested%20for%0Asynthetic%20data%20up%20to%20dimension%2032%2C%20as%20well%20as%20real-life%20applications%20in%20image%0Aanalysis%2C%20harmonic%20analysis%2C%20density%20estimation%2C%20equivariant%20neural%20networks%2C%0Achemical%20conformational%20spaces%2C%20and%20classical%20mechanics%20systems%2C%20achieving%20very%0Aaccurate%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03086v3&entry.124074799=Read"},
{"title": "SISMA: Semantic Face Image Synthesis with Mamba", "author": "Filippo Botti and Alex Ergasti and Tomaso Fontanini and Claudio Ferrari and Massimo Bertozzi and Andrea Prati", "abstract": "  Diffusion Models have become very popular for Semantic Image Synthesis (SIS)\nof human faces. Nevertheless, their training and inference is computationally\nexpensive and their computational requirements are high due to the quadratic\ncomplexity of attention layers. In this paper, we propose a novel architecture\ncalled SISMA, based on the recently proposed Mamba. SISMA generates high\nquality samples by controlling their shape using a semantic mask at a reduced\ncomputational demand. We validated our approach through comprehensive\nexperiments with CelebAMask-HQ, revealing that our architecture not only\nachieves a better FID score yet also operates at three times the speed of\nstate-of-the-art architectures. This indicates that the proposed design is a\nviable, lightweight substitute to transformer-based models.\n", "link": "http://arxiv.org/abs/2509.17651v1", "date": "2025-09-22", "relevancy": 2.2871, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5811}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5729}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SISMA%3A%20Semantic%20Face%20Image%20Synthesis%20with%20Mamba&body=Title%3A%20SISMA%3A%20Semantic%20Face%20Image%20Synthesis%20with%20Mamba%0AAuthor%3A%20Filippo%20Botti%20and%20Alex%20Ergasti%20and%20Tomaso%20Fontanini%20and%20Claudio%20Ferrari%20and%20Massimo%20Bertozzi%20and%20Andrea%20Prati%0AAbstract%3A%20%20%20Diffusion%20Models%20have%20become%20very%20popular%20for%20Semantic%20Image%20Synthesis%20%28SIS%29%0Aof%20human%20faces.%20Nevertheless%2C%20their%20training%20and%20inference%20is%20computationally%0Aexpensive%20and%20their%20computational%20requirements%20are%20high%20due%20to%20the%20quadratic%0Acomplexity%20of%20attention%20layers.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20architecture%0Acalled%20SISMA%2C%20based%20on%20the%20recently%20proposed%20Mamba.%20SISMA%20generates%20high%0Aquality%20samples%20by%20controlling%20their%20shape%20using%20a%20semantic%20mask%20at%20a%20reduced%0Acomputational%20demand.%20We%20validated%20our%20approach%20through%20comprehensive%0Aexperiments%20with%20CelebAMask-HQ%2C%20revealing%20that%20our%20architecture%20not%20only%0Aachieves%20a%20better%20FID%20score%20yet%20also%20operates%20at%20three%20times%20the%20speed%20of%0Astate-of-the-art%20architectures.%20This%20indicates%20that%20the%20proposed%20design%20is%20a%0Aviable%2C%20lightweight%20substitute%20to%20transformer-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSISMA%253A%2520Semantic%2520Face%2520Image%2520Synthesis%2520with%2520Mamba%26entry.906535625%3DFilippo%2520Botti%2520and%2520Alex%2520Ergasti%2520and%2520Tomaso%2520Fontanini%2520and%2520Claudio%2520Ferrari%2520and%2520Massimo%2520Bertozzi%2520and%2520Andrea%2520Prati%26entry.1292438233%3D%2520%2520Diffusion%2520Models%2520have%2520become%2520very%2520popular%2520for%2520Semantic%2520Image%2520Synthesis%2520%2528SIS%2529%250Aof%2520human%2520faces.%2520Nevertheless%252C%2520their%2520training%2520and%2520inference%2520is%2520computationally%250Aexpensive%2520and%2520their%2520computational%2520requirements%2520are%2520high%2520due%2520to%2520the%2520quadratic%250Acomplexity%2520of%2520attention%2520layers.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520architecture%250Acalled%2520SISMA%252C%2520based%2520on%2520the%2520recently%2520proposed%2520Mamba.%2520SISMA%2520generates%2520high%250Aquality%2520samples%2520by%2520controlling%2520their%2520shape%2520using%2520a%2520semantic%2520mask%2520at%2520a%2520reduced%250Acomputational%2520demand.%2520We%2520validated%2520our%2520approach%2520through%2520comprehensive%250Aexperiments%2520with%2520CelebAMask-HQ%252C%2520revealing%2520that%2520our%2520architecture%2520not%2520only%250Aachieves%2520a%2520better%2520FID%2520score%2520yet%2520also%2520operates%2520at%2520three%2520times%2520the%2520speed%2520of%250Astate-of-the-art%2520architectures.%2520This%2520indicates%2520that%2520the%2520proposed%2520design%2520is%2520a%250Aviable%252C%2520lightweight%2520substitute%2520to%2520transformer-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SISMA%3A%20Semantic%20Face%20Image%20Synthesis%20with%20Mamba&entry.906535625=Filippo%20Botti%20and%20Alex%20Ergasti%20and%20Tomaso%20Fontanini%20and%20Claudio%20Ferrari%20and%20Massimo%20Bertozzi%20and%20Andrea%20Prati&entry.1292438233=%20%20Diffusion%20Models%20have%20become%20very%20popular%20for%20Semantic%20Image%20Synthesis%20%28SIS%29%0Aof%20human%20faces.%20Nevertheless%2C%20their%20training%20and%20inference%20is%20computationally%0Aexpensive%20and%20their%20computational%20requirements%20are%20high%20due%20to%20the%20quadratic%0Acomplexity%20of%20attention%20layers.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20architecture%0Acalled%20SISMA%2C%20based%20on%20the%20recently%20proposed%20Mamba.%20SISMA%20generates%20high%0Aquality%20samples%20by%20controlling%20their%20shape%20using%20a%20semantic%20mask%20at%20a%20reduced%0Acomputational%20demand.%20We%20validated%20our%20approach%20through%20comprehensive%0Aexperiments%20with%20CelebAMask-HQ%2C%20revealing%20that%20our%20architecture%20not%20only%0Aachieves%20a%20better%20FID%20score%20yet%20also%20operates%20at%20three%20times%20the%20speed%20of%0Astate-of-the-art%20architectures.%20This%20indicates%20that%20the%20proposed%20design%20is%20a%0Aviable%2C%20lightweight%20substitute%20to%20transformer-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17651v1&entry.124074799=Read"},
{"title": "Interpreting Attention Heads for Image-to-Text Information Flow in Large\n  Vision-Language Models", "author": "Jinyeong Kim and Seil Kang and Jiwoo Park and Junhyeok Kim and Seong Jae Hwang", "abstract": "  Large Vision-Language Models (LVLMs) answer visual questions by transferring\ninformation from images to text through a series of attention heads. While this\nimage-to-text information flow is central to visual question answering, its\nunderlying mechanism remains difficult to interpret due to the simultaneous\noperation of numerous attention heads. To address this challenge, we propose\nhead attribution, a technique inspired by component attribution methods, to\nidentify consistent patterns among attention heads that play a key role in\ninformation transfer. Using head attribution, we investigate how LVLMs rely on\nspecific attention heads to identify and answer questions about the main object\nin an image. Our analysis reveals that a distinct subset of attention heads\nfacilitates the image-to-text information flow. Remarkably, we find that the\nselection of these heads is governed by the semantic content of the input image\nrather than its visual appearance. We further examine the flow of information\nat the token level and discover that (1) text information first propagates to\nrole-related tokens and the final token before receiving image information, and\n(2) image information is embedded in both object-related and background tokens.\nOur work provides evidence that image-to-text information flow follows a\nstructured process, and that analysis at the attention-head level offers a\npromising direction toward understanding the mechanisms of LVLMs.\n", "link": "http://arxiv.org/abs/2509.17588v1", "date": "2025-09-22", "relevancy": 2.2866, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Attention%20Heads%20for%20Image-to-Text%20Information%20Flow%20in%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20Interpreting%20Attention%20Heads%20for%20Image-to-Text%20Information%20Flow%20in%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Jinyeong%20Kim%20and%20Seil%20Kang%20and%20Jiwoo%20Park%20and%20Junhyeok%20Kim%20and%20Seong%20Jae%20Hwang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20answer%20visual%20questions%20by%20transferring%0Ainformation%20from%20images%20to%20text%20through%20a%20series%20of%20attention%20heads.%20While%20this%0Aimage-to-text%20information%20flow%20is%20central%20to%20visual%20question%20answering%2C%20its%0Aunderlying%20mechanism%20remains%20difficult%20to%20interpret%20due%20to%20the%20simultaneous%0Aoperation%20of%20numerous%20attention%20heads.%20To%20address%20this%20challenge%2C%20we%20propose%0Ahead%20attribution%2C%20a%20technique%20inspired%20by%20component%20attribution%20methods%2C%20to%0Aidentify%20consistent%20patterns%20among%20attention%20heads%20that%20play%20a%20key%20role%20in%0Ainformation%20transfer.%20Using%20head%20attribution%2C%20we%20investigate%20how%20LVLMs%20rely%20on%0Aspecific%20attention%20heads%20to%20identify%20and%20answer%20questions%20about%20the%20main%20object%0Ain%20an%20image.%20Our%20analysis%20reveals%20that%20a%20distinct%20subset%20of%20attention%20heads%0Afacilitates%20the%20image-to-text%20information%20flow.%20Remarkably%2C%20we%20find%20that%20the%0Aselection%20of%20these%20heads%20is%20governed%20by%20the%20semantic%20content%20of%20the%20input%20image%0Arather%20than%20its%20visual%20appearance.%20We%20further%20examine%20the%20flow%20of%20information%0Aat%20the%20token%20level%20and%20discover%20that%20%281%29%20text%20information%20first%20propagates%20to%0Arole-related%20tokens%20and%20the%20final%20token%20before%20receiving%20image%20information%2C%20and%0A%282%29%20image%20information%20is%20embedded%20in%20both%20object-related%20and%20background%20tokens.%0AOur%20work%20provides%20evidence%20that%20image-to-text%20information%20flow%20follows%20a%0Astructured%20process%2C%20and%20that%20analysis%20at%20the%20attention-head%20level%20offers%20a%0Apromising%20direction%20toward%20understanding%20the%20mechanisms%20of%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Attention%2520Heads%2520for%2520Image-to-Text%2520Information%2520Flow%2520in%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DJinyeong%2520Kim%2520and%2520Seil%2520Kang%2520and%2520Jiwoo%2520Park%2520and%2520Junhyeok%2520Kim%2520and%2520Seong%2520Jae%2520Hwang%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520answer%2520visual%2520questions%2520by%2520transferring%250Ainformation%2520from%2520images%2520to%2520text%2520through%2520a%2520series%2520of%2520attention%2520heads.%2520While%2520this%250Aimage-to-text%2520information%2520flow%2520is%2520central%2520to%2520visual%2520question%2520answering%252C%2520its%250Aunderlying%2520mechanism%2520remains%2520difficult%2520to%2520interpret%2520due%2520to%2520the%2520simultaneous%250Aoperation%2520of%2520numerous%2520attention%2520heads.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250Ahead%2520attribution%252C%2520a%2520technique%2520inspired%2520by%2520component%2520attribution%2520methods%252C%2520to%250Aidentify%2520consistent%2520patterns%2520among%2520attention%2520heads%2520that%2520play%2520a%2520key%2520role%2520in%250Ainformation%2520transfer.%2520Using%2520head%2520attribution%252C%2520we%2520investigate%2520how%2520LVLMs%2520rely%2520on%250Aspecific%2520attention%2520heads%2520to%2520identify%2520and%2520answer%2520questions%2520about%2520the%2520main%2520object%250Ain%2520an%2520image.%2520Our%2520analysis%2520reveals%2520that%2520a%2520distinct%2520subset%2520of%2520attention%2520heads%250Afacilitates%2520the%2520image-to-text%2520information%2520flow.%2520Remarkably%252C%2520we%2520find%2520that%2520the%250Aselection%2520of%2520these%2520heads%2520is%2520governed%2520by%2520the%2520semantic%2520content%2520of%2520the%2520input%2520image%250Arather%2520than%2520its%2520visual%2520appearance.%2520We%2520further%2520examine%2520the%2520flow%2520of%2520information%250Aat%2520the%2520token%2520level%2520and%2520discover%2520that%2520%25281%2529%2520text%2520information%2520first%2520propagates%2520to%250Arole-related%2520tokens%2520and%2520the%2520final%2520token%2520before%2520receiving%2520image%2520information%252C%2520and%250A%25282%2529%2520image%2520information%2520is%2520embedded%2520in%2520both%2520object-related%2520and%2520background%2520tokens.%250AOur%2520work%2520provides%2520evidence%2520that%2520image-to-text%2520information%2520flow%2520follows%2520a%250Astructured%2520process%252C%2520and%2520that%2520analysis%2520at%2520the%2520attention-head%2520level%2520offers%2520a%250Apromising%2520direction%2520toward%2520understanding%2520the%2520mechanisms%2520of%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Attention%20Heads%20for%20Image-to-Text%20Information%20Flow%20in%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Jinyeong%20Kim%20and%20Seil%20Kang%20and%20Jiwoo%20Park%20and%20Junhyeok%20Kim%20and%20Seong%20Jae%20Hwang&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20answer%20visual%20questions%20by%20transferring%0Ainformation%20from%20images%20to%20text%20through%20a%20series%20of%20attention%20heads.%20While%20this%0Aimage-to-text%20information%20flow%20is%20central%20to%20visual%20question%20answering%2C%20its%0Aunderlying%20mechanism%20remains%20difficult%20to%20interpret%20due%20to%20the%20simultaneous%0Aoperation%20of%20numerous%20attention%20heads.%20To%20address%20this%20challenge%2C%20we%20propose%0Ahead%20attribution%2C%20a%20technique%20inspired%20by%20component%20attribution%20methods%2C%20to%0Aidentify%20consistent%20patterns%20among%20attention%20heads%20that%20play%20a%20key%20role%20in%0Ainformation%20transfer.%20Using%20head%20attribution%2C%20we%20investigate%20how%20LVLMs%20rely%20on%0Aspecific%20attention%20heads%20to%20identify%20and%20answer%20questions%20about%20the%20main%20object%0Ain%20an%20image.%20Our%20analysis%20reveals%20that%20a%20distinct%20subset%20of%20attention%20heads%0Afacilitates%20the%20image-to-text%20information%20flow.%20Remarkably%2C%20we%20find%20that%20the%0Aselection%20of%20these%20heads%20is%20governed%20by%20the%20semantic%20content%20of%20the%20input%20image%0Arather%20than%20its%20visual%20appearance.%20We%20further%20examine%20the%20flow%20of%20information%0Aat%20the%20token%20level%20and%20discover%20that%20%281%29%20text%20information%20first%20propagates%20to%0Arole-related%20tokens%20and%20the%20final%20token%20before%20receiving%20image%20information%2C%20and%0A%282%29%20image%20information%20is%20embedded%20in%20both%20object-related%20and%20background%20tokens.%0AOur%20work%20provides%20evidence%20that%20image-to-text%20information%20flow%20follows%20a%0Astructured%20process%2C%20and%20that%20analysis%20at%20the%20attention-head%20level%20offers%20a%0Apromising%20direction%20toward%20understanding%20the%20mechanisms%20of%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17588v1&entry.124074799=Read"},
{"title": "Remote Sensing-Oriented World Model", "author": "Yuxi Lu and Biao Wu and Zhidong Li and Kunqi Li and Chenya Huang and Huacan Wang and Qizhen Lan and Ronghao Chen and Ling Chen and Bin Liang", "abstract": "  World models have shown potential in artificial intelligence by predicting\nand reasoning about world states beyond direct observations. However, existing\napproaches are predominantly evaluated in synthetic environments or constrained\nscene settings, limiting their validation in real-world contexts with broad\nspatial coverage and complex semantics. Meanwhile, remote sensing applications\nurgently require spatial reasoning capabilities for disaster response and urban\nplanning. This paper bridges these gaps by introducing the first framework for\nworld modeling in remote sensing. We formulate remote sensing world modeling as\ndirection-conditioned spatial extrapolation, where models generate semantically\nconsistent adjacent image tiles given a central observation and directional\ninstruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing\nWorld-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks\nacross four scenarios: general, flood, urban, and rural. RSWISE combines visual\nfidelity assessment with instruction compliance evaluation using GPT-4o as a\nsemantic judge, ensuring models genuinely perform spatial reasoning rather than\nsimple replication. Afterwards, we present RemoteBAGEL, a unified multimodal\nmodel fine-tuned on remote sensing data for spatial extrapolation tasks.\nExtensive experiments demonstrate that RemoteBAGEL consistently outperforms\nstate-of-the-art baselines on RSWISE.\n", "link": "http://arxiv.org/abs/2509.17808v1", "date": "2025-09-22", "relevancy": 2.2852, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Remote%20Sensing-Oriented%20World%20Model&body=Title%3A%20Remote%20Sensing-Oriented%20World%20Model%0AAuthor%3A%20Yuxi%20Lu%20and%20Biao%20Wu%20and%20Zhidong%20Li%20and%20Kunqi%20Li%20and%20Chenya%20Huang%20and%20Huacan%20Wang%20and%20Qizhen%20Lan%20and%20Ronghao%20Chen%20and%20Ling%20Chen%20and%20Bin%20Liang%0AAbstract%3A%20%20%20World%20models%20have%20shown%20potential%20in%20artificial%20intelligence%20by%20predicting%0Aand%20reasoning%20about%20world%20states%20beyond%20direct%20observations.%20However%2C%20existing%0Aapproaches%20are%20predominantly%20evaluated%20in%20synthetic%20environments%20or%20constrained%0Ascene%20settings%2C%20limiting%20their%20validation%20in%20real-world%20contexts%20with%20broad%0Aspatial%20coverage%20and%20complex%20semantics.%20Meanwhile%2C%20remote%20sensing%20applications%0Aurgently%20require%20spatial%20reasoning%20capabilities%20for%20disaster%20response%20and%20urban%0Aplanning.%20This%20paper%20bridges%20these%20gaps%20by%20introducing%20the%20first%20framework%20for%0Aworld%20modeling%20in%20remote%20sensing.%20We%20formulate%20remote%20sensing%20world%20modeling%20as%0Adirection-conditioned%20spatial%20extrapolation%2C%20where%20models%20generate%20semantically%0Aconsistent%20adjacent%20image%20tiles%20given%20a%20central%20observation%20and%20directional%0Ainstruction.%20To%20enable%20rigorous%20evaluation%2C%20we%20develop%20RSWISE%20%28Remote%20Sensing%0AWorld-Image%20Spatial%20Evaluation%29%2C%20a%20benchmark%20containing%201%2C600%20evaluation%20tasks%0Aacross%20four%20scenarios%3A%20general%2C%20flood%2C%20urban%2C%20and%20rural.%20RSWISE%20combines%20visual%0Afidelity%20assessment%20with%20instruction%20compliance%20evaluation%20using%20GPT-4o%20as%20a%0Asemantic%20judge%2C%20ensuring%20models%20genuinely%20perform%20spatial%20reasoning%20rather%20than%0Asimple%20replication.%20Afterwards%2C%20we%20present%20RemoteBAGEL%2C%20a%20unified%20multimodal%0Amodel%20fine-tuned%20on%20remote%20sensing%20data%20for%20spatial%20extrapolation%20tasks.%0AExtensive%20experiments%20demonstrate%20that%20RemoteBAGEL%20consistently%20outperforms%0Astate-of-the-art%20baselines%20on%20RSWISE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemote%2520Sensing-Oriented%2520World%2520Model%26entry.906535625%3DYuxi%2520Lu%2520and%2520Biao%2520Wu%2520and%2520Zhidong%2520Li%2520and%2520Kunqi%2520Li%2520and%2520Chenya%2520Huang%2520and%2520Huacan%2520Wang%2520and%2520Qizhen%2520Lan%2520and%2520Ronghao%2520Chen%2520and%2520Ling%2520Chen%2520and%2520Bin%2520Liang%26entry.1292438233%3D%2520%2520World%2520models%2520have%2520shown%2520potential%2520in%2520artificial%2520intelligence%2520by%2520predicting%250Aand%2520reasoning%2520about%2520world%2520states%2520beyond%2520direct%2520observations.%2520However%252C%2520existing%250Aapproaches%2520are%2520predominantly%2520evaluated%2520in%2520synthetic%2520environments%2520or%2520constrained%250Ascene%2520settings%252C%2520limiting%2520their%2520validation%2520in%2520real-world%2520contexts%2520with%2520broad%250Aspatial%2520coverage%2520and%2520complex%2520semantics.%2520Meanwhile%252C%2520remote%2520sensing%2520applications%250Aurgently%2520require%2520spatial%2520reasoning%2520capabilities%2520for%2520disaster%2520response%2520and%2520urban%250Aplanning.%2520This%2520paper%2520bridges%2520these%2520gaps%2520by%2520introducing%2520the%2520first%2520framework%2520for%250Aworld%2520modeling%2520in%2520remote%2520sensing.%2520We%2520formulate%2520remote%2520sensing%2520world%2520modeling%2520as%250Adirection-conditioned%2520spatial%2520extrapolation%252C%2520where%2520models%2520generate%2520semantically%250Aconsistent%2520adjacent%2520image%2520tiles%2520given%2520a%2520central%2520observation%2520and%2520directional%250Ainstruction.%2520To%2520enable%2520rigorous%2520evaluation%252C%2520we%2520develop%2520RSWISE%2520%2528Remote%2520Sensing%250AWorld-Image%2520Spatial%2520Evaluation%2529%252C%2520a%2520benchmark%2520containing%25201%252C600%2520evaluation%2520tasks%250Aacross%2520four%2520scenarios%253A%2520general%252C%2520flood%252C%2520urban%252C%2520and%2520rural.%2520RSWISE%2520combines%2520visual%250Afidelity%2520assessment%2520with%2520instruction%2520compliance%2520evaluation%2520using%2520GPT-4o%2520as%2520a%250Asemantic%2520judge%252C%2520ensuring%2520models%2520genuinely%2520perform%2520spatial%2520reasoning%2520rather%2520than%250Asimple%2520replication.%2520Afterwards%252C%2520we%2520present%2520RemoteBAGEL%252C%2520a%2520unified%2520multimodal%250Amodel%2520fine-tuned%2520on%2520remote%2520sensing%2520data%2520for%2520spatial%2520extrapolation%2520tasks.%250AExtensive%2520experiments%2520demonstrate%2520that%2520RemoteBAGEL%2520consistently%2520outperforms%250Astate-of-the-art%2520baselines%2520on%2520RSWISE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Remote%20Sensing-Oriented%20World%20Model&entry.906535625=Yuxi%20Lu%20and%20Biao%20Wu%20and%20Zhidong%20Li%20and%20Kunqi%20Li%20and%20Chenya%20Huang%20and%20Huacan%20Wang%20and%20Qizhen%20Lan%20and%20Ronghao%20Chen%20and%20Ling%20Chen%20and%20Bin%20Liang&entry.1292438233=%20%20World%20models%20have%20shown%20potential%20in%20artificial%20intelligence%20by%20predicting%0Aand%20reasoning%20about%20world%20states%20beyond%20direct%20observations.%20However%2C%20existing%0Aapproaches%20are%20predominantly%20evaluated%20in%20synthetic%20environments%20or%20constrained%0Ascene%20settings%2C%20limiting%20their%20validation%20in%20real-world%20contexts%20with%20broad%0Aspatial%20coverage%20and%20complex%20semantics.%20Meanwhile%2C%20remote%20sensing%20applications%0Aurgently%20require%20spatial%20reasoning%20capabilities%20for%20disaster%20response%20and%20urban%0Aplanning.%20This%20paper%20bridges%20these%20gaps%20by%20introducing%20the%20first%20framework%20for%0Aworld%20modeling%20in%20remote%20sensing.%20We%20formulate%20remote%20sensing%20world%20modeling%20as%0Adirection-conditioned%20spatial%20extrapolation%2C%20where%20models%20generate%20semantically%0Aconsistent%20adjacent%20image%20tiles%20given%20a%20central%20observation%20and%20directional%0Ainstruction.%20To%20enable%20rigorous%20evaluation%2C%20we%20develop%20RSWISE%20%28Remote%20Sensing%0AWorld-Image%20Spatial%20Evaluation%29%2C%20a%20benchmark%20containing%201%2C600%20evaluation%20tasks%0Aacross%20four%20scenarios%3A%20general%2C%20flood%2C%20urban%2C%20and%20rural.%20RSWISE%20combines%20visual%0Afidelity%20assessment%20with%20instruction%20compliance%20evaluation%20using%20GPT-4o%20as%20a%0Asemantic%20judge%2C%20ensuring%20models%20genuinely%20perform%20spatial%20reasoning%20rather%20than%0Asimple%20replication.%20Afterwards%2C%20we%20present%20RemoteBAGEL%2C%20a%20unified%20multimodal%0Amodel%20fine-tuned%20on%20remote%20sensing%20data%20for%20spatial%20extrapolation%20tasks.%0AExtensive%20experiments%20demonstrate%20that%20RemoteBAGEL%20consistently%20outperforms%0Astate-of-the-art%20baselines%20on%20RSWISE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17808v1&entry.124074799=Read"},
{"title": "How Persuasive is Your Context?", "author": "Tu Nguyen and Kevin Du and Alexander Miserlis Hoyle and Ryan Cotterell", "abstract": "  Two central capabilities of language models (LMs) are: (i) drawing on prior\nknowledge about entities, which allows them to answer queries such as \"What's\nthe official language of Austria?\", and (ii) adapting to new information\nprovided in context, e.g., \"Pretend the official language of Austria is\nTagalog.\", that is pre-pended to the question. In this article, we introduce\ntargeted persuasion score (TPS), designed to quantify how persuasive a given\ncontext is to an LM where persuasion is operationalized as the ability of the\ncontext to alter the LM's answer to the question. In contrast to evaluating\npersuasiveness only by inspecting the greedily decoded answer under the model,\nTPS provides a more fine-grained view of model behavior. Based on the\nWasserstein distance, TPS measures how much a context shifts a model's original\nanswer distribution toward a target distribution. Empirically, through a series\nof experiments, we show that TPS captures a more nuanced notion of\npersuasiveness than previously proposed metrics.\n", "link": "http://arxiv.org/abs/2509.17879v1", "date": "2025-09-22", "relevancy": 2.2794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Persuasive%20is%20Your%20Context%3F&body=Title%3A%20How%20Persuasive%20is%20Your%20Context%3F%0AAuthor%3A%20Tu%20Nguyen%20and%20Kevin%20Du%20and%20Alexander%20Miserlis%20Hoyle%20and%20Ryan%20Cotterell%0AAbstract%3A%20%20%20Two%20central%20capabilities%20of%20language%20models%20%28LMs%29%20are%3A%20%28i%29%20drawing%20on%20prior%0Aknowledge%20about%20entities%2C%20which%20allows%20them%20to%20answer%20queries%20such%20as%20%22What%27s%0Athe%20official%20language%20of%20Austria%3F%22%2C%20and%20%28ii%29%20adapting%20to%20new%20information%0Aprovided%20in%20context%2C%20e.g.%2C%20%22Pretend%20the%20official%20language%20of%20Austria%20is%0ATagalog.%22%2C%20that%20is%20pre-pended%20to%20the%20question.%20In%20this%20article%2C%20we%20introduce%0Atargeted%20persuasion%20score%20%28TPS%29%2C%20designed%20to%20quantify%20how%20persuasive%20a%20given%0Acontext%20is%20to%20an%20LM%20where%20persuasion%20is%20operationalized%20as%20the%20ability%20of%20the%0Acontext%20to%20alter%20the%20LM%27s%20answer%20to%20the%20question.%20In%20contrast%20to%20evaluating%0Apersuasiveness%20only%20by%20inspecting%20the%20greedily%20decoded%20answer%20under%20the%20model%2C%0ATPS%20provides%20a%20more%20fine-grained%20view%20of%20model%20behavior.%20Based%20on%20the%0AWasserstein%20distance%2C%20TPS%20measures%20how%20much%20a%20context%20shifts%20a%20model%27s%20original%0Aanswer%20distribution%20toward%20a%20target%20distribution.%20Empirically%2C%20through%20a%20series%0Aof%20experiments%2C%20we%20show%20that%20TPS%20captures%20a%20more%20nuanced%20notion%20of%0Apersuasiveness%20than%20previously%20proposed%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Persuasive%2520is%2520Your%2520Context%253F%26entry.906535625%3DTu%2520Nguyen%2520and%2520Kevin%2520Du%2520and%2520Alexander%2520Miserlis%2520Hoyle%2520and%2520Ryan%2520Cotterell%26entry.1292438233%3D%2520%2520Two%2520central%2520capabilities%2520of%2520language%2520models%2520%2528LMs%2529%2520are%253A%2520%2528i%2529%2520drawing%2520on%2520prior%250Aknowledge%2520about%2520entities%252C%2520which%2520allows%2520them%2520to%2520answer%2520queries%2520such%2520as%2520%2522What%2527s%250Athe%2520official%2520language%2520of%2520Austria%253F%2522%252C%2520and%2520%2528ii%2529%2520adapting%2520to%2520new%2520information%250Aprovided%2520in%2520context%252C%2520e.g.%252C%2520%2522Pretend%2520the%2520official%2520language%2520of%2520Austria%2520is%250ATagalog.%2522%252C%2520that%2520is%2520pre-pended%2520to%2520the%2520question.%2520In%2520this%2520article%252C%2520we%2520introduce%250Atargeted%2520persuasion%2520score%2520%2528TPS%2529%252C%2520designed%2520to%2520quantify%2520how%2520persuasive%2520a%2520given%250Acontext%2520is%2520to%2520an%2520LM%2520where%2520persuasion%2520is%2520operationalized%2520as%2520the%2520ability%2520of%2520the%250Acontext%2520to%2520alter%2520the%2520LM%2527s%2520answer%2520to%2520the%2520question.%2520In%2520contrast%2520to%2520evaluating%250Apersuasiveness%2520only%2520by%2520inspecting%2520the%2520greedily%2520decoded%2520answer%2520under%2520the%2520model%252C%250ATPS%2520provides%2520a%2520more%2520fine-grained%2520view%2520of%2520model%2520behavior.%2520Based%2520on%2520the%250AWasserstein%2520distance%252C%2520TPS%2520measures%2520how%2520much%2520a%2520context%2520shifts%2520a%2520model%2527s%2520original%250Aanswer%2520distribution%2520toward%2520a%2520target%2520distribution.%2520Empirically%252C%2520through%2520a%2520series%250Aof%2520experiments%252C%2520we%2520show%2520that%2520TPS%2520captures%2520a%2520more%2520nuanced%2520notion%2520of%250Apersuasiveness%2520than%2520previously%2520proposed%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Persuasive%20is%20Your%20Context%3F&entry.906535625=Tu%20Nguyen%20and%20Kevin%20Du%20and%20Alexander%20Miserlis%20Hoyle%20and%20Ryan%20Cotterell&entry.1292438233=%20%20Two%20central%20capabilities%20of%20language%20models%20%28LMs%29%20are%3A%20%28i%29%20drawing%20on%20prior%0Aknowledge%20about%20entities%2C%20which%20allows%20them%20to%20answer%20queries%20such%20as%20%22What%27s%0Athe%20official%20language%20of%20Austria%3F%22%2C%20and%20%28ii%29%20adapting%20to%20new%20information%0Aprovided%20in%20context%2C%20e.g.%2C%20%22Pretend%20the%20official%20language%20of%20Austria%20is%0ATagalog.%22%2C%20that%20is%20pre-pended%20to%20the%20question.%20In%20this%20article%2C%20we%20introduce%0Atargeted%20persuasion%20score%20%28TPS%29%2C%20designed%20to%20quantify%20how%20persuasive%20a%20given%0Acontext%20is%20to%20an%20LM%20where%20persuasion%20is%20operationalized%20as%20the%20ability%20of%20the%0Acontext%20to%20alter%20the%20LM%27s%20answer%20to%20the%20question.%20In%20contrast%20to%20evaluating%0Apersuasiveness%20only%20by%20inspecting%20the%20greedily%20decoded%20answer%20under%20the%20model%2C%0ATPS%20provides%20a%20more%20fine-grained%20view%20of%20model%20behavior.%20Based%20on%20the%0AWasserstein%20distance%2C%20TPS%20measures%20how%20much%20a%20context%20shifts%20a%20model%27s%20original%0Aanswer%20distribution%20toward%20a%20target%20distribution.%20Empirically%2C%20through%20a%20series%0Aof%20experiments%2C%20we%20show%20that%20TPS%20captures%20a%20more%20nuanced%20notion%20of%0Apersuasiveness%20than%20previously%20proposed%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17879v1&entry.124074799=Read"},
{"title": "Dual-View Alignment Learning with Hierarchical-Prompt for\n  Class-Imbalance Multi-Label Classification", "author": "Sheng Huang and Jiexuan Yan and Beiyan Liu and Bo Liu and Richang Hong", "abstract": "  Real-world datasets often exhibit class imbalance across multiple categories,\nmanifesting as long-tailed distributions and few-shot scenarios. This is\nespecially challenging in Class-Imbalanced Multi-Label Image Classification\n(CI-MLIC) tasks, where data imbalance and multi-object recognition present\nsignificant obstacles. To address these challenges, we propose a novel method\ntermed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which\nleverages multi-modal knowledge from vision-language pretrained (VLP) models to\nmitigate the class-imbalance problem in multi-label settings. Specifically,\nHP-DVAL employs dual-view alignment learning to transfer the powerful feature\nrepresentation capabilities from VLP models by extracting complementary\nfeatures for accurate image-text alignment. To better adapt VLP models for\nCI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes\nglobal and local prompts to learn task-specific and context-related prior\nknowledge. Additionally, we design a semantic consistency loss during prompt\ntuning to prevent learned prompts from deviating from general knowledge\nembedded in VLP models. The effectiveness of our approach is validated on two\nCI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results\ndemonstrate the superiority of our method over SOTA approaches, achieving mAP\nimprovements of 10.0\\% and 5.2\\% on the long-tailed multi-label image\nclassification task, and 6.8\\% and 2.9\\% on the multi-label few-shot image\nclassification task.\n", "link": "http://arxiv.org/abs/2509.17747v1", "date": "2025-09-22", "relevancy": 2.2722, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6027}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5435}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-View%20Alignment%20Learning%20with%20Hierarchical-Prompt%20for%0A%20%20Class-Imbalance%20Multi-Label%20Classification&body=Title%3A%20Dual-View%20Alignment%20Learning%20with%20Hierarchical-Prompt%20for%0A%20%20Class-Imbalance%20Multi-Label%20Classification%0AAuthor%3A%20Sheng%20Huang%20and%20Jiexuan%20Yan%20and%20Beiyan%20Liu%20and%20Bo%20Liu%20and%20Richang%20Hong%0AAbstract%3A%20%20%20Real-world%20datasets%20often%20exhibit%20class%20imbalance%20across%20multiple%20categories%2C%0Amanifesting%20as%20long-tailed%20distributions%20and%20few-shot%20scenarios.%20This%20is%0Aespecially%20challenging%20in%20Class-Imbalanced%20Multi-Label%20Image%20Classification%0A%28CI-MLIC%29%20tasks%2C%20where%20data%20imbalance%20and%20multi-object%20recognition%20present%0Asignificant%20obstacles.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20method%0Atermed%20Dual-View%20Alignment%20Learning%20with%20Hierarchical%20Prompt%20%28HP-DVAL%29%2C%20which%0Aleverages%20multi-modal%20knowledge%20from%20vision-language%20pretrained%20%28VLP%29%20models%20to%0Amitigate%20the%20class-imbalance%20problem%20in%20multi-label%20settings.%20Specifically%2C%0AHP-DVAL%20employs%20dual-view%20alignment%20learning%20to%20transfer%20the%20powerful%20feature%0Arepresentation%20capabilities%20from%20VLP%20models%20by%20extracting%20complementary%0Afeatures%20for%20accurate%20image-text%20alignment.%20To%20better%20adapt%20VLP%20models%20for%0ACI-MLIC%20tasks%2C%20we%20introduce%20a%20hierarchical%20prompt-tuning%20strategy%20that%20utilizes%0Aglobal%20and%20local%20prompts%20to%20learn%20task-specific%20and%20context-related%20prior%0Aknowledge.%20Additionally%2C%20we%20design%20a%20semantic%20consistency%20loss%20during%20prompt%0Atuning%20to%20prevent%20learned%20prompts%20from%20deviating%20from%20general%20knowledge%0Aembedded%20in%20VLP%20models.%20The%20effectiveness%20of%20our%20approach%20is%20validated%20on%20two%0ACI-MLIC%20benchmarks%3A%20MS-COCO%20and%20VOC2007.%20Extensive%20experimental%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20SOTA%20approaches%2C%20achieving%20mAP%0Aimprovements%20of%2010.0%5C%25%20and%205.2%5C%25%20on%20the%20long-tailed%20multi-label%20image%0Aclassification%20task%2C%20and%206.8%5C%25%20and%202.9%5C%25%20on%20the%20multi-label%20few-shot%20image%0Aclassification%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-View%2520Alignment%2520Learning%2520with%2520Hierarchical-Prompt%2520for%250A%2520%2520Class-Imbalance%2520Multi-Label%2520Classification%26entry.906535625%3DSheng%2520Huang%2520and%2520Jiexuan%2520Yan%2520and%2520Beiyan%2520Liu%2520and%2520Bo%2520Liu%2520and%2520Richang%2520Hong%26entry.1292438233%3D%2520%2520Real-world%2520datasets%2520often%2520exhibit%2520class%2520imbalance%2520across%2520multiple%2520categories%252C%250Amanifesting%2520as%2520long-tailed%2520distributions%2520and%2520few-shot%2520scenarios.%2520This%2520is%250Aespecially%2520challenging%2520in%2520Class-Imbalanced%2520Multi-Label%2520Image%2520Classification%250A%2528CI-MLIC%2529%2520tasks%252C%2520where%2520data%2520imbalance%2520and%2520multi-object%2520recognition%2520present%250Asignificant%2520obstacles.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520method%250Atermed%2520Dual-View%2520Alignment%2520Learning%2520with%2520Hierarchical%2520Prompt%2520%2528HP-DVAL%2529%252C%2520which%250Aleverages%2520multi-modal%2520knowledge%2520from%2520vision-language%2520pretrained%2520%2528VLP%2529%2520models%2520to%250Amitigate%2520the%2520class-imbalance%2520problem%2520in%2520multi-label%2520settings.%2520Specifically%252C%250AHP-DVAL%2520employs%2520dual-view%2520alignment%2520learning%2520to%2520transfer%2520the%2520powerful%2520feature%250Arepresentation%2520capabilities%2520from%2520VLP%2520models%2520by%2520extracting%2520complementary%250Afeatures%2520for%2520accurate%2520image-text%2520alignment.%2520To%2520better%2520adapt%2520VLP%2520models%2520for%250ACI-MLIC%2520tasks%252C%2520we%2520introduce%2520a%2520hierarchical%2520prompt-tuning%2520strategy%2520that%2520utilizes%250Aglobal%2520and%2520local%2520prompts%2520to%2520learn%2520task-specific%2520and%2520context-related%2520prior%250Aknowledge.%2520Additionally%252C%2520we%2520design%2520a%2520semantic%2520consistency%2520loss%2520during%2520prompt%250Atuning%2520to%2520prevent%2520learned%2520prompts%2520from%2520deviating%2520from%2520general%2520knowledge%250Aembedded%2520in%2520VLP%2520models.%2520The%2520effectiveness%2520of%2520our%2520approach%2520is%2520validated%2520on%2520two%250ACI-MLIC%2520benchmarks%253A%2520MS-COCO%2520and%2520VOC2007.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520SOTA%2520approaches%252C%2520achieving%2520mAP%250Aimprovements%2520of%252010.0%255C%2525%2520and%25205.2%255C%2525%2520on%2520the%2520long-tailed%2520multi-label%2520image%250Aclassification%2520task%252C%2520and%25206.8%255C%2525%2520and%25202.9%255C%2525%2520on%2520the%2520multi-label%2520few-shot%2520image%250Aclassification%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-View%20Alignment%20Learning%20with%20Hierarchical-Prompt%20for%0A%20%20Class-Imbalance%20Multi-Label%20Classification&entry.906535625=Sheng%20Huang%20and%20Jiexuan%20Yan%20and%20Beiyan%20Liu%20and%20Bo%20Liu%20and%20Richang%20Hong&entry.1292438233=%20%20Real-world%20datasets%20often%20exhibit%20class%20imbalance%20across%20multiple%20categories%2C%0Amanifesting%20as%20long-tailed%20distributions%20and%20few-shot%20scenarios.%20This%20is%0Aespecially%20challenging%20in%20Class-Imbalanced%20Multi-Label%20Image%20Classification%0A%28CI-MLIC%29%20tasks%2C%20where%20data%20imbalance%20and%20multi-object%20recognition%20present%0Asignificant%20obstacles.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20method%0Atermed%20Dual-View%20Alignment%20Learning%20with%20Hierarchical%20Prompt%20%28HP-DVAL%29%2C%20which%0Aleverages%20multi-modal%20knowledge%20from%20vision-language%20pretrained%20%28VLP%29%20models%20to%0Amitigate%20the%20class-imbalance%20problem%20in%20multi-label%20settings.%20Specifically%2C%0AHP-DVAL%20employs%20dual-view%20alignment%20learning%20to%20transfer%20the%20powerful%20feature%0Arepresentation%20capabilities%20from%20VLP%20models%20by%20extracting%20complementary%0Afeatures%20for%20accurate%20image-text%20alignment.%20To%20better%20adapt%20VLP%20models%20for%0ACI-MLIC%20tasks%2C%20we%20introduce%20a%20hierarchical%20prompt-tuning%20strategy%20that%20utilizes%0Aglobal%20and%20local%20prompts%20to%20learn%20task-specific%20and%20context-related%20prior%0Aknowledge.%20Additionally%2C%20we%20design%20a%20semantic%20consistency%20loss%20during%20prompt%0Atuning%20to%20prevent%20learned%20prompts%20from%20deviating%20from%20general%20knowledge%0Aembedded%20in%20VLP%20models.%20The%20effectiveness%20of%20our%20approach%20is%20validated%20on%20two%0ACI-MLIC%20benchmarks%3A%20MS-COCO%20and%20VOC2007.%20Extensive%20experimental%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20SOTA%20approaches%2C%20achieving%20mAP%0Aimprovements%20of%2010.0%5C%25%20and%205.2%5C%25%20on%20the%20long-tailed%20multi-label%20image%0Aclassification%20task%2C%20and%206.8%5C%25%20and%202.9%5C%25%20on%20the%20multi-label%20few-shot%20image%0Aclassification%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17747v1&entry.124074799=Read"},
{"title": "Confidence-gated training for efficient early-exit neural networks", "author": "Saad Mokssit and Ouassim Karrakchou and Alejandro Mousist and Mounir Ghogho", "abstract": "  Early-exit neural networks reduce inference cost by enabling confident\npredictions at intermediate layers. However, joint training often leads to\ngradient interference, with deeper classifiers dominating optimization. We\npropose Confidence-Gated Training (CGT), a paradigm that conditionally\npropagates gradients from deeper exits only when preceding exits fail. This\nencourages shallow classifiers to act as primary decision points while\nreserving deeper layers for harder inputs. By aligning training with the\ninference-time policy, CGT mitigates overthinking, improves early-exit\naccuracy, and preserves efficiency. Experiments on the Indian Pines and\nFashion-MNIST benchmarks show that CGT lowers average inference cost while\nimproving overall accuracy, offering a practical solution for deploying deep\nmodels in resource-constrained environments.\n", "link": "http://arxiv.org/abs/2509.17885v1", "date": "2025-09-22", "relevancy": 2.271, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6203}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5388}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence-gated%20training%20for%20efficient%20early-exit%20neural%20networks&body=Title%3A%20Confidence-gated%20training%20for%20efficient%20early-exit%20neural%20networks%0AAuthor%3A%20Saad%20Mokssit%20and%20Ouassim%20Karrakchou%20and%20Alejandro%20Mousist%20and%20Mounir%20Ghogho%0AAbstract%3A%20%20%20Early-exit%20neural%20networks%20reduce%20inference%20cost%20by%20enabling%20confident%0Apredictions%20at%20intermediate%20layers.%20However%2C%20joint%20training%20often%20leads%20to%0Agradient%20interference%2C%20with%20deeper%20classifiers%20dominating%20optimization.%20We%0Apropose%20Confidence-Gated%20Training%20%28CGT%29%2C%20a%20paradigm%20that%20conditionally%0Apropagates%20gradients%20from%20deeper%20exits%20only%20when%20preceding%20exits%20fail.%20This%0Aencourages%20shallow%20classifiers%20to%20act%20as%20primary%20decision%20points%20while%0Areserving%20deeper%20layers%20for%20harder%20inputs.%20By%20aligning%20training%20with%20the%0Ainference-time%20policy%2C%20CGT%20mitigates%20overthinking%2C%20improves%20early-exit%0Aaccuracy%2C%20and%20preserves%20efficiency.%20Experiments%20on%20the%20Indian%20Pines%20and%0AFashion-MNIST%20benchmarks%20show%20that%20CGT%20lowers%20average%20inference%20cost%20while%0Aimproving%20overall%20accuracy%2C%20offering%20a%20practical%20solution%20for%20deploying%20deep%0Amodels%20in%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence-gated%2520training%2520for%2520efficient%2520early-exit%2520neural%2520networks%26entry.906535625%3DSaad%2520Mokssit%2520and%2520Ouassim%2520Karrakchou%2520and%2520Alejandro%2520Mousist%2520and%2520Mounir%2520Ghogho%26entry.1292438233%3D%2520%2520Early-exit%2520neural%2520networks%2520reduce%2520inference%2520cost%2520by%2520enabling%2520confident%250Apredictions%2520at%2520intermediate%2520layers.%2520However%252C%2520joint%2520training%2520often%2520leads%2520to%250Agradient%2520interference%252C%2520with%2520deeper%2520classifiers%2520dominating%2520optimization.%2520We%250Apropose%2520Confidence-Gated%2520Training%2520%2528CGT%2529%252C%2520a%2520paradigm%2520that%2520conditionally%250Apropagates%2520gradients%2520from%2520deeper%2520exits%2520only%2520when%2520preceding%2520exits%2520fail.%2520This%250Aencourages%2520shallow%2520classifiers%2520to%2520act%2520as%2520primary%2520decision%2520points%2520while%250Areserving%2520deeper%2520layers%2520for%2520harder%2520inputs.%2520By%2520aligning%2520training%2520with%2520the%250Ainference-time%2520policy%252C%2520CGT%2520mitigates%2520overthinking%252C%2520improves%2520early-exit%250Aaccuracy%252C%2520and%2520preserves%2520efficiency.%2520Experiments%2520on%2520the%2520Indian%2520Pines%2520and%250AFashion-MNIST%2520benchmarks%2520show%2520that%2520CGT%2520lowers%2520average%2520inference%2520cost%2520while%250Aimproving%2520overall%2520accuracy%252C%2520offering%2520a%2520practical%2520solution%2520for%2520deploying%2520deep%250Amodels%2520in%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence-gated%20training%20for%20efficient%20early-exit%20neural%20networks&entry.906535625=Saad%20Mokssit%20and%20Ouassim%20Karrakchou%20and%20Alejandro%20Mousist%20and%20Mounir%20Ghogho&entry.1292438233=%20%20Early-exit%20neural%20networks%20reduce%20inference%20cost%20by%20enabling%20confident%0Apredictions%20at%20intermediate%20layers.%20However%2C%20joint%20training%20often%20leads%20to%0Agradient%20interference%2C%20with%20deeper%20classifiers%20dominating%20optimization.%20We%0Apropose%20Confidence-Gated%20Training%20%28CGT%29%2C%20a%20paradigm%20that%20conditionally%0Apropagates%20gradients%20from%20deeper%20exits%20only%20when%20preceding%20exits%20fail.%20This%0Aencourages%20shallow%20classifiers%20to%20act%20as%20primary%20decision%20points%20while%0Areserving%20deeper%20layers%20for%20harder%20inputs.%20By%20aligning%20training%20with%20the%0Ainference-time%20policy%2C%20CGT%20mitigates%20overthinking%2C%20improves%20early-exit%0Aaccuracy%2C%20and%20preserves%20efficiency.%20Experiments%20on%20the%20Indian%20Pines%20and%0AFashion-MNIST%20benchmarks%20show%20that%20CGT%20lowers%20average%20inference%20cost%20while%0Aimproving%20overall%20accuracy%2C%20offering%20a%20practical%20solution%20for%20deploying%20deep%0Amodels%20in%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17885v1&entry.124074799=Read"},
{"title": "Core-elements Subsampling for Alternating Least Squares", "author": "Dunyao Xue and Mengyu Li and Cheng Meng and Jingyi Zhang", "abstract": "  In this paper, we propose a novel element-wise subset selection method for\nthe alternating least squares (ALS) algorithm, focusing on low-rank matrix\nfactorization involving matrices with missing values, as commonly encountered\nin recommender systems. While ALS is widely used for providing personalized\nrecommendations based on user-item interaction data, its high computational\ncost, stemming from repeated regression operations, poses significant\nchallenges for large-scale datasets. To enhance the efficiency of ALS, we\npropose a core-elements subsampling method that selects a representative subset\nof data and leverages sparse matrix operations to approximate ALS estimations\nefficiently. We establish theoretical guarantees for the approximation and\nconvergence of the proposed approach, showing that it achieves similar accuracy\nwith significantly reduced computational time compared to full-data ALS.\nExtensive simulations and real-world applications demonstrate the effectiveness\nof our method in various scenarios, emphasizing its potential in large-scale\nrecommendation systems.\n", "link": "http://arxiv.org/abs/2509.18024v1", "date": "2025-09-22", "relevancy": 2.2607, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.488}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4368}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Core-elements%20Subsampling%20for%20Alternating%20Least%20Squares&body=Title%3A%20Core-elements%20Subsampling%20for%20Alternating%20Least%20Squares%0AAuthor%3A%20Dunyao%20Xue%20and%20Mengyu%20Li%20and%20Cheng%20Meng%20and%20Jingyi%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20element-wise%20subset%20selection%20method%20for%0Athe%20alternating%20least%20squares%20%28ALS%29%20algorithm%2C%20focusing%20on%20low-rank%20matrix%0Afactorization%20involving%20matrices%20with%20missing%20values%2C%20as%20commonly%20encountered%0Ain%20recommender%20systems.%20While%20ALS%20is%20widely%20used%20for%20providing%20personalized%0Arecommendations%20based%20on%20user-item%20interaction%20data%2C%20its%20high%20computational%0Acost%2C%20stemming%20from%20repeated%20regression%20operations%2C%20poses%20significant%0Achallenges%20for%20large-scale%20datasets.%20To%20enhance%20the%20efficiency%20of%20ALS%2C%20we%0Apropose%20a%20core-elements%20subsampling%20method%20that%20selects%20a%20representative%20subset%0Aof%20data%20and%20leverages%20sparse%20matrix%20operations%20to%20approximate%20ALS%20estimations%0Aefficiently.%20We%20establish%20theoretical%20guarantees%20for%20the%20approximation%20and%0Aconvergence%20of%20the%20proposed%20approach%2C%20showing%20that%20it%20achieves%20similar%20accuracy%0Awith%20significantly%20reduced%20computational%20time%20compared%20to%20full-data%20ALS.%0AExtensive%20simulations%20and%20real-world%20applications%20demonstrate%20the%20effectiveness%0Aof%20our%20method%20in%20various%20scenarios%2C%20emphasizing%20its%20potential%20in%20large-scale%0Arecommendation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCore-elements%2520Subsampling%2520for%2520Alternating%2520Least%2520Squares%26entry.906535625%3DDunyao%2520Xue%2520and%2520Mengyu%2520Li%2520and%2520Cheng%2520Meng%2520and%2520Jingyi%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520element-wise%2520subset%2520selection%2520method%2520for%250Athe%2520alternating%2520least%2520squares%2520%2528ALS%2529%2520algorithm%252C%2520focusing%2520on%2520low-rank%2520matrix%250Afactorization%2520involving%2520matrices%2520with%2520missing%2520values%252C%2520as%2520commonly%2520encountered%250Ain%2520recommender%2520systems.%2520While%2520ALS%2520is%2520widely%2520used%2520for%2520providing%2520personalized%250Arecommendations%2520based%2520on%2520user-item%2520interaction%2520data%252C%2520its%2520high%2520computational%250Acost%252C%2520stemming%2520from%2520repeated%2520regression%2520operations%252C%2520poses%2520significant%250Achallenges%2520for%2520large-scale%2520datasets.%2520To%2520enhance%2520the%2520efficiency%2520of%2520ALS%252C%2520we%250Apropose%2520a%2520core-elements%2520subsampling%2520method%2520that%2520selects%2520a%2520representative%2520subset%250Aof%2520data%2520and%2520leverages%2520sparse%2520matrix%2520operations%2520to%2520approximate%2520ALS%2520estimations%250Aefficiently.%2520We%2520establish%2520theoretical%2520guarantees%2520for%2520the%2520approximation%2520and%250Aconvergence%2520of%2520the%2520proposed%2520approach%252C%2520showing%2520that%2520it%2520achieves%2520similar%2520accuracy%250Awith%2520significantly%2520reduced%2520computational%2520time%2520compared%2520to%2520full-data%2520ALS.%250AExtensive%2520simulations%2520and%2520real-world%2520applications%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520method%2520in%2520various%2520scenarios%252C%2520emphasizing%2520its%2520potential%2520in%2520large-scale%250Arecommendation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Core-elements%20Subsampling%20for%20Alternating%20Least%20Squares&entry.906535625=Dunyao%20Xue%20and%20Mengyu%20Li%20and%20Cheng%20Meng%20and%20Jingyi%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20element-wise%20subset%20selection%20method%20for%0Athe%20alternating%20least%20squares%20%28ALS%29%20algorithm%2C%20focusing%20on%20low-rank%20matrix%0Afactorization%20involving%20matrices%20with%20missing%20values%2C%20as%20commonly%20encountered%0Ain%20recommender%20systems.%20While%20ALS%20is%20widely%20used%20for%20providing%20personalized%0Arecommendations%20based%20on%20user-item%20interaction%20data%2C%20its%20high%20computational%0Acost%2C%20stemming%20from%20repeated%20regression%20operations%2C%20poses%20significant%0Achallenges%20for%20large-scale%20datasets.%20To%20enhance%20the%20efficiency%20of%20ALS%2C%20we%0Apropose%20a%20core-elements%20subsampling%20method%20that%20selects%20a%20representative%20subset%0Aof%20data%20and%20leverages%20sparse%20matrix%20operations%20to%20approximate%20ALS%20estimations%0Aefficiently.%20We%20establish%20theoretical%20guarantees%20for%20the%20approximation%20and%0Aconvergence%20of%20the%20proposed%20approach%2C%20showing%20that%20it%20achieves%20similar%20accuracy%0Awith%20significantly%20reduced%20computational%20time%20compared%20to%20full-data%20ALS.%0AExtensive%20simulations%20and%20real-world%20applications%20demonstrate%20the%20effectiveness%0Aof%20our%20method%20in%20various%20scenarios%2C%20emphasizing%20its%20potential%20in%20large-scale%0Arecommendation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18024v1&entry.124074799=Read"},
{"title": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding", "author": "S M A Sharif and Abdur Rehman and Fayaz Ali Dharejo and Radu Timofte and Rizwan Ali Naqvi", "abstract": "  Real-world images often suffer from spatially diverse degradations such as\nhaze, rain, snow, and low-light, significantly impacting visual quality and\ndownstream vision tasks. Existing all-in-one restoration (AIR) approaches\neither depend on external text prompts or embed hand-crafted architectural\npriors (e.g., frequency heuristics); both impose discrete, brittle assumptions\nthat weaken generalization to unseen or mixed degradations. To address this\nlimitation, we propose to reframe AIR as learned latent prior inference, where\ndegradation-aware representations are automatically inferred from the input\nwithout explicit task cues. Based on latent priors, we formulate AIR as a\nstructured reasoning paradigm: (1) which features to route (adaptive feature\nselection), (2) where to restore (spatial localization), and (3) what to\nrestore (degradation semantics). We design a lightweight decoding module that\nefficiently leverages these latent encoded cues for spatially-adaptive\nrestoration. Extensive experiments across six common degradation tasks, five\ncompound settings, and previously unseen degradations demonstrate that our\nmethod outperforms state-of-the-art (SOTA) approaches, achieving an average\nPSNR improvement of 1.68 dB while being three times more efficient.\n", "link": "http://arxiv.org/abs/2509.17792v1", "date": "2025-09-22", "relevancy": 2.26, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5849}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5678}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding&body=Title%3A%20Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding%0AAuthor%3A%20S%20M%20A%20Sharif%20and%20Abdur%20Rehman%20and%20Fayaz%20Ali%20Dharejo%20and%20Radu%20Timofte%20and%20Rizwan%20Ali%20Naqvi%0AAbstract%3A%20%20%20Real-world%20images%20often%20suffer%20from%20spatially%20diverse%20degradations%20such%20as%0Ahaze%2C%20rain%2C%20snow%2C%20and%20low-light%2C%20significantly%20impacting%20visual%20quality%20and%0Adownstream%20vision%20tasks.%20Existing%20all-in-one%20restoration%20%28AIR%29%20approaches%0Aeither%20depend%20on%20external%20text%20prompts%20or%20embed%20hand-crafted%20architectural%0Apriors%20%28e.g.%2C%20frequency%20heuristics%29%3B%20both%20impose%20discrete%2C%20brittle%20assumptions%0Athat%20weaken%20generalization%20to%20unseen%20or%20mixed%20degradations.%20To%20address%20this%0Alimitation%2C%20we%20propose%20to%20reframe%20AIR%20as%20learned%20latent%20prior%20inference%2C%20where%0Adegradation-aware%20representations%20are%20automatically%20inferred%20from%20the%20input%0Awithout%20explicit%20task%20cues.%20Based%20on%20latent%20priors%2C%20we%20formulate%20AIR%20as%20a%0Astructured%20reasoning%20paradigm%3A%20%281%29%20which%20features%20to%20route%20%28adaptive%20feature%0Aselection%29%2C%20%282%29%20where%20to%20restore%20%28spatial%20localization%29%2C%20and%20%283%29%20what%20to%0Arestore%20%28degradation%20semantics%29.%20We%20design%20a%20lightweight%20decoding%20module%20that%0Aefficiently%20leverages%20these%20latent%20encoded%20cues%20for%20spatially-adaptive%0Arestoration.%20Extensive%20experiments%20across%20six%20common%20degradation%20tasks%2C%20five%0Acompound%20settings%2C%20and%20previously%20unseen%20degradations%20demonstrate%20that%20our%0Amethod%20outperforms%20state-of-the-art%20%28SOTA%29%20approaches%2C%20achieving%20an%20average%0APSNR%20improvement%20of%201.68%20dB%20while%20being%20three%20times%20more%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegradation-Aware%2520All-in-One%2520Image%2520Restoration%2520via%2520Latent%2520Prior%2520Encoding%26entry.906535625%3DS%2520M%2520A%2520Sharif%2520and%2520Abdur%2520Rehman%2520and%2520Fayaz%2520Ali%2520Dharejo%2520and%2520Radu%2520Timofte%2520and%2520Rizwan%2520Ali%2520Naqvi%26entry.1292438233%3D%2520%2520Real-world%2520images%2520often%2520suffer%2520from%2520spatially%2520diverse%2520degradations%2520such%2520as%250Ahaze%252C%2520rain%252C%2520snow%252C%2520and%2520low-light%252C%2520significantly%2520impacting%2520visual%2520quality%2520and%250Adownstream%2520vision%2520tasks.%2520Existing%2520all-in-one%2520restoration%2520%2528AIR%2529%2520approaches%250Aeither%2520depend%2520on%2520external%2520text%2520prompts%2520or%2520embed%2520hand-crafted%2520architectural%250Apriors%2520%2528e.g.%252C%2520frequency%2520heuristics%2529%253B%2520both%2520impose%2520discrete%252C%2520brittle%2520assumptions%250Athat%2520weaken%2520generalization%2520to%2520unseen%2520or%2520mixed%2520degradations.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520to%2520reframe%2520AIR%2520as%2520learned%2520latent%2520prior%2520inference%252C%2520where%250Adegradation-aware%2520representations%2520are%2520automatically%2520inferred%2520from%2520the%2520input%250Awithout%2520explicit%2520task%2520cues.%2520Based%2520on%2520latent%2520priors%252C%2520we%2520formulate%2520AIR%2520as%2520a%250Astructured%2520reasoning%2520paradigm%253A%2520%25281%2529%2520which%2520features%2520to%2520route%2520%2528adaptive%2520feature%250Aselection%2529%252C%2520%25282%2529%2520where%2520to%2520restore%2520%2528spatial%2520localization%2529%252C%2520and%2520%25283%2529%2520what%2520to%250Arestore%2520%2528degradation%2520semantics%2529.%2520We%2520design%2520a%2520lightweight%2520decoding%2520module%2520that%250Aefficiently%2520leverages%2520these%2520latent%2520encoded%2520cues%2520for%2520spatially-adaptive%250Arestoration.%2520Extensive%2520experiments%2520across%2520six%2520common%2520degradation%2520tasks%252C%2520five%250Acompound%2520settings%252C%2520and%2520previously%2520unseen%2520degradations%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520approaches%252C%2520achieving%2520an%2520average%250APSNR%2520improvement%2520of%25201.68%2520dB%2520while%2520being%2520three%2520times%2520more%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding&entry.906535625=S%20M%20A%20Sharif%20and%20Abdur%20Rehman%20and%20Fayaz%20Ali%20Dharejo%20and%20Radu%20Timofte%20and%20Rizwan%20Ali%20Naqvi&entry.1292438233=%20%20Real-world%20images%20often%20suffer%20from%20spatially%20diverse%20degradations%20such%20as%0Ahaze%2C%20rain%2C%20snow%2C%20and%20low-light%2C%20significantly%20impacting%20visual%20quality%20and%0Adownstream%20vision%20tasks.%20Existing%20all-in-one%20restoration%20%28AIR%29%20approaches%0Aeither%20depend%20on%20external%20text%20prompts%20or%20embed%20hand-crafted%20architectural%0Apriors%20%28e.g.%2C%20frequency%20heuristics%29%3B%20both%20impose%20discrete%2C%20brittle%20assumptions%0Athat%20weaken%20generalization%20to%20unseen%20or%20mixed%20degradations.%20To%20address%20this%0Alimitation%2C%20we%20propose%20to%20reframe%20AIR%20as%20learned%20latent%20prior%20inference%2C%20where%0Adegradation-aware%20representations%20are%20automatically%20inferred%20from%20the%20input%0Awithout%20explicit%20task%20cues.%20Based%20on%20latent%20priors%2C%20we%20formulate%20AIR%20as%20a%0Astructured%20reasoning%20paradigm%3A%20%281%29%20which%20features%20to%20route%20%28adaptive%20feature%0Aselection%29%2C%20%282%29%20where%20to%20restore%20%28spatial%20localization%29%2C%20and%20%283%29%20what%20to%0Arestore%20%28degradation%20semantics%29.%20We%20design%20a%20lightweight%20decoding%20module%20that%0Aefficiently%20leverages%20these%20latent%20encoded%20cues%20for%20spatially-adaptive%0Arestoration.%20Extensive%20experiments%20across%20six%20common%20degradation%20tasks%2C%20five%0Acompound%20settings%2C%20and%20previously%20unseen%20degradations%20demonstrate%20that%20our%0Amethod%20outperforms%20state-of-the-art%20%28SOTA%29%20approaches%2C%20achieving%20an%20average%0APSNR%20improvement%20of%201.68%20dB%20while%20being%20three%20times%20more%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17792v1&entry.124074799=Read"},
{"title": "ClusterRCA: An End-to-End Approach for Network Fault Localization and\n  Classification for HPC System", "author": "Yongqian Sun and Xijie Pan and Xiao Xiong and Lei Tao and Jiaju Wang and Shenglin Zhang and Yuan Yuan and Yuqi Li and Kunlin Jian", "abstract": "  Network failure diagnosis is challenging yet critical for high-performance\ncomputing (HPC) systems. Existing methods cannot be directly applied to HPC\nscenarios due to data heterogeneity and lack of accuracy. This paper proposes a\nnovel framework, called ClusterRCA, to localize culprit nodes and determine\nfailure types by leveraging multimodal data. ClusterRCA extracts features from\ntopologically connected network interface controller (NIC) pairs to analyze the\ndiverse, multimodal data in HPC systems. To accurately localize culprit nodes\nand determine failure types, ClusterRCA combines classifier-based and\ngraph-based approaches. A failure graph is constructed based on the output of\nthe state classifier, and then it performs a customized random walk on the\ngraph to localize the root cause. Experiments on datasets collected by a\ntop-tier global HPC device vendor show ClusterRCA achieves high accuracy in\ndiagnosing network failure for HPC systems. ClusterRCA also maintains robust\nperformance across different application scenarios.\n", "link": "http://arxiv.org/abs/2506.20673v2", "date": "2025-09-22", "relevancy": 2.2549, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4621}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4578}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClusterRCA%3A%20An%20End-to-End%20Approach%20for%20Network%20Fault%20Localization%20and%0A%20%20Classification%20for%20HPC%20System&body=Title%3A%20ClusterRCA%3A%20An%20End-to-End%20Approach%20for%20Network%20Fault%20Localization%20and%0A%20%20Classification%20for%20HPC%20System%0AAuthor%3A%20Yongqian%20Sun%20and%20Xijie%20Pan%20and%20Xiao%20Xiong%20and%20Lei%20Tao%20and%20Jiaju%20Wang%20and%20Shenglin%20Zhang%20and%20Yuan%20Yuan%20and%20Yuqi%20Li%20and%20Kunlin%20Jian%0AAbstract%3A%20%20%20Network%20failure%20diagnosis%20is%20challenging%20yet%20critical%20for%20high-performance%0Acomputing%20%28HPC%29%20systems.%20Existing%20methods%20cannot%20be%20directly%20applied%20to%20HPC%0Ascenarios%20due%20to%20data%20heterogeneity%20and%20lack%20of%20accuracy.%20This%20paper%20proposes%20a%0Anovel%20framework%2C%20called%20ClusterRCA%2C%20to%20localize%20culprit%20nodes%20and%20determine%0Afailure%20types%20by%20leveraging%20multimodal%20data.%20ClusterRCA%20extracts%20features%20from%0Atopologically%20connected%20network%20interface%20controller%20%28NIC%29%20pairs%20to%20analyze%20the%0Adiverse%2C%20multimodal%20data%20in%20HPC%20systems.%20To%20accurately%20localize%20culprit%20nodes%0Aand%20determine%20failure%20types%2C%20ClusterRCA%20combines%20classifier-based%20and%0Agraph-based%20approaches.%20A%20failure%20graph%20is%20constructed%20based%20on%20the%20output%20of%0Athe%20state%20classifier%2C%20and%20then%20it%20performs%20a%20customized%20random%20walk%20on%20the%0Agraph%20to%20localize%20the%20root%20cause.%20Experiments%20on%20datasets%20collected%20by%20a%0Atop-tier%20global%20HPC%20device%20vendor%20show%20ClusterRCA%20achieves%20high%20accuracy%20in%0Adiagnosing%20network%20failure%20for%20HPC%20systems.%20ClusterRCA%20also%20maintains%20robust%0Aperformance%20across%20different%20application%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClusterRCA%253A%2520An%2520End-to-End%2520Approach%2520for%2520Network%2520Fault%2520Localization%2520and%250A%2520%2520Classification%2520for%2520HPC%2520System%26entry.906535625%3DYongqian%2520Sun%2520and%2520Xijie%2520Pan%2520and%2520Xiao%2520Xiong%2520and%2520Lei%2520Tao%2520and%2520Jiaju%2520Wang%2520and%2520Shenglin%2520Zhang%2520and%2520Yuan%2520Yuan%2520and%2520Yuqi%2520Li%2520and%2520Kunlin%2520Jian%26entry.1292438233%3D%2520%2520Network%2520failure%2520diagnosis%2520is%2520challenging%2520yet%2520critical%2520for%2520high-performance%250Acomputing%2520%2528HPC%2529%2520systems.%2520Existing%2520methods%2520cannot%2520be%2520directly%2520applied%2520to%2520HPC%250Ascenarios%2520due%2520to%2520data%2520heterogeneity%2520and%2520lack%2520of%2520accuracy.%2520This%2520paper%2520proposes%2520a%250Anovel%2520framework%252C%2520called%2520ClusterRCA%252C%2520to%2520localize%2520culprit%2520nodes%2520and%2520determine%250Afailure%2520types%2520by%2520leveraging%2520multimodal%2520data.%2520ClusterRCA%2520extracts%2520features%2520from%250Atopologically%2520connected%2520network%2520interface%2520controller%2520%2528NIC%2529%2520pairs%2520to%2520analyze%2520the%250Adiverse%252C%2520multimodal%2520data%2520in%2520HPC%2520systems.%2520To%2520accurately%2520localize%2520culprit%2520nodes%250Aand%2520determine%2520failure%2520types%252C%2520ClusterRCA%2520combines%2520classifier-based%2520and%250Agraph-based%2520approaches.%2520A%2520failure%2520graph%2520is%2520constructed%2520based%2520on%2520the%2520output%2520of%250Athe%2520state%2520classifier%252C%2520and%2520then%2520it%2520performs%2520a%2520customized%2520random%2520walk%2520on%2520the%250Agraph%2520to%2520localize%2520the%2520root%2520cause.%2520Experiments%2520on%2520datasets%2520collected%2520by%2520a%250Atop-tier%2520global%2520HPC%2520device%2520vendor%2520show%2520ClusterRCA%2520achieves%2520high%2520accuracy%2520in%250Adiagnosing%2520network%2520failure%2520for%2520HPC%2520systems.%2520ClusterRCA%2520also%2520maintains%2520robust%250Aperformance%2520across%2520different%2520application%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClusterRCA%3A%20An%20End-to-End%20Approach%20for%20Network%20Fault%20Localization%20and%0A%20%20Classification%20for%20HPC%20System&entry.906535625=Yongqian%20Sun%20and%20Xijie%20Pan%20and%20Xiao%20Xiong%20and%20Lei%20Tao%20and%20Jiaju%20Wang%20and%20Shenglin%20Zhang%20and%20Yuan%20Yuan%20and%20Yuqi%20Li%20and%20Kunlin%20Jian&entry.1292438233=%20%20Network%20failure%20diagnosis%20is%20challenging%20yet%20critical%20for%20high-performance%0Acomputing%20%28HPC%29%20systems.%20Existing%20methods%20cannot%20be%20directly%20applied%20to%20HPC%0Ascenarios%20due%20to%20data%20heterogeneity%20and%20lack%20of%20accuracy.%20This%20paper%20proposes%20a%0Anovel%20framework%2C%20called%20ClusterRCA%2C%20to%20localize%20culprit%20nodes%20and%20determine%0Afailure%20types%20by%20leveraging%20multimodal%20data.%20ClusterRCA%20extracts%20features%20from%0Atopologically%20connected%20network%20interface%20controller%20%28NIC%29%20pairs%20to%20analyze%20the%0Adiverse%2C%20multimodal%20data%20in%20HPC%20systems.%20To%20accurately%20localize%20culprit%20nodes%0Aand%20determine%20failure%20types%2C%20ClusterRCA%20combines%20classifier-based%20and%0Agraph-based%20approaches.%20A%20failure%20graph%20is%20constructed%20based%20on%20the%20output%20of%0Athe%20state%20classifier%2C%20and%20then%20it%20performs%20a%20customized%20random%20walk%20on%20the%0Agraph%20to%20localize%20the%20root%20cause.%20Experiments%20on%20datasets%20collected%20by%20a%0Atop-tier%20global%20HPC%20device%20vendor%20show%20ClusterRCA%20achieves%20high%20accuracy%20in%0Adiagnosing%20network%20failure%20for%20HPC%20systems.%20ClusterRCA%20also%20maintains%20robust%0Aperformance%20across%20different%20application%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20673v2&entry.124074799=Read"},
{"title": "RCTDistill: Cross-Modal Knowledge Distillation Framework for\n  Radar-Camera 3D Object Detection with Temporal Fusion", "author": "Geonho Bang and Minjae Seong and Jisong Kim and Geunju Baek and Daye Oh and Junhyung Kim and Junho Koh and Jun Won Choi", "abstract": "  Radar-camera fusion methods have emerged as a cost-effective approach for 3D\nobject detection but still lag behind LiDAR-based methods in performance.\nRecent works have focused on employing temporal fusion and Knowledge\nDistillation (KD) strategies to overcome these limitations. However, existing\napproaches have not sufficiently accounted for uncertainties arising from\nobject motion or sensor-specific errors inherent in radar and camera\nmodalities. In this work, we propose RCTDistill, a novel cross-modal KD method\nbased on temporal fusion, comprising three key modules: Range-Azimuth Knowledge\nDistillation (RAKD), Temporal Knowledge Distillation (TKD), and\nRegion-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider\nthe inherent errors in the range and azimuth directions, enabling effective\nknowledge transfer from LiDAR features to refine inaccurate BEV\nrepresentations. TKD mitigates temporal misalignment caused by dynamic objects\nby aligning historical radar-camera BEV features with current LiDAR\nrepresentations. RDKD enhances feature discrimination by distilling relational\nknowledge from the teacher model, allowing the student to differentiate\nforeground and background features. RCTDistill achieves state-of-the-art\nradar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)\ndatasets, with the fastest inference speed of 26.2 FPS.\n", "link": "http://arxiv.org/abs/2509.17712v1", "date": "2025-09-22", "relevancy": 2.2529, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5674}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5624}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RCTDistill%3A%20Cross-Modal%20Knowledge%20Distillation%20Framework%20for%0A%20%20Radar-Camera%203D%20Object%20Detection%20with%20Temporal%20Fusion&body=Title%3A%20RCTDistill%3A%20Cross-Modal%20Knowledge%20Distillation%20Framework%20for%0A%20%20Radar-Camera%203D%20Object%20Detection%20with%20Temporal%20Fusion%0AAuthor%3A%20Geonho%20Bang%20and%20Minjae%20Seong%20and%20Jisong%20Kim%20and%20Geunju%20Baek%20and%20Daye%20Oh%20and%20Junhyung%20Kim%20and%20Junho%20Koh%20and%20Jun%20Won%20Choi%0AAbstract%3A%20%20%20Radar-camera%20fusion%20methods%20have%20emerged%20as%20a%20cost-effective%20approach%20for%203D%0Aobject%20detection%20but%20still%20lag%20behind%20LiDAR-based%20methods%20in%20performance.%0ARecent%20works%20have%20focused%20on%20employing%20temporal%20fusion%20and%20Knowledge%0ADistillation%20%28KD%29%20strategies%20to%20overcome%20these%20limitations.%20However%2C%20existing%0Aapproaches%20have%20not%20sufficiently%20accounted%20for%20uncertainties%20arising%20from%0Aobject%20motion%20or%20sensor-specific%20errors%20inherent%20in%20radar%20and%20camera%0Amodalities.%20In%20this%20work%2C%20we%20propose%20RCTDistill%2C%20a%20novel%20cross-modal%20KD%20method%0Abased%20on%20temporal%20fusion%2C%20comprising%20three%20key%20modules%3A%20Range-Azimuth%20Knowledge%0ADistillation%20%28RAKD%29%2C%20Temporal%20Knowledge%20Distillation%20%28TKD%29%2C%20and%0ARegion-Decoupled%20Knowledge%20Distillation%20%28RDKD%29.%20RAKD%20is%20designed%20to%20consider%0Athe%20inherent%20errors%20in%20the%20range%20and%20azimuth%20directions%2C%20enabling%20effective%0Aknowledge%20transfer%20from%20LiDAR%20features%20to%20refine%20inaccurate%20BEV%0Arepresentations.%20TKD%20mitigates%20temporal%20misalignment%20caused%20by%20dynamic%20objects%0Aby%20aligning%20historical%20radar-camera%20BEV%20features%20with%20current%20LiDAR%0Arepresentations.%20RDKD%20enhances%20feature%20discrimination%20by%20distilling%20relational%0Aknowledge%20from%20the%20teacher%20model%2C%20allowing%20the%20student%20to%20differentiate%0Aforeground%20and%20background%20features.%20RCTDistill%20achieves%20state-of-the-art%0Aradar-camera%20fusion%20performance%20on%20both%20the%20nuScenes%20and%20View-of-Delft%20%28VoD%29%0Adatasets%2C%20with%20the%20fastest%20inference%20speed%20of%2026.2%20FPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRCTDistill%253A%2520Cross-Modal%2520Knowledge%2520Distillation%2520Framework%2520for%250A%2520%2520Radar-Camera%25203D%2520Object%2520Detection%2520with%2520Temporal%2520Fusion%26entry.906535625%3DGeonho%2520Bang%2520and%2520Minjae%2520Seong%2520and%2520Jisong%2520Kim%2520and%2520Geunju%2520Baek%2520and%2520Daye%2520Oh%2520and%2520Junhyung%2520Kim%2520and%2520Junho%2520Koh%2520and%2520Jun%2520Won%2520Choi%26entry.1292438233%3D%2520%2520Radar-camera%2520fusion%2520methods%2520have%2520emerged%2520as%2520a%2520cost-effective%2520approach%2520for%25203D%250Aobject%2520detection%2520but%2520still%2520lag%2520behind%2520LiDAR-based%2520methods%2520in%2520performance.%250ARecent%2520works%2520have%2520focused%2520on%2520employing%2520temporal%2520fusion%2520and%2520Knowledge%250ADistillation%2520%2528KD%2529%2520strategies%2520to%2520overcome%2520these%2520limitations.%2520However%252C%2520existing%250Aapproaches%2520have%2520not%2520sufficiently%2520accounted%2520for%2520uncertainties%2520arising%2520from%250Aobject%2520motion%2520or%2520sensor-specific%2520errors%2520inherent%2520in%2520radar%2520and%2520camera%250Amodalities.%2520In%2520this%2520work%252C%2520we%2520propose%2520RCTDistill%252C%2520a%2520novel%2520cross-modal%2520KD%2520method%250Abased%2520on%2520temporal%2520fusion%252C%2520comprising%2520three%2520key%2520modules%253A%2520Range-Azimuth%2520Knowledge%250ADistillation%2520%2528RAKD%2529%252C%2520Temporal%2520Knowledge%2520Distillation%2520%2528TKD%2529%252C%2520and%250ARegion-Decoupled%2520Knowledge%2520Distillation%2520%2528RDKD%2529.%2520RAKD%2520is%2520designed%2520to%2520consider%250Athe%2520inherent%2520errors%2520in%2520the%2520range%2520and%2520azimuth%2520directions%252C%2520enabling%2520effective%250Aknowledge%2520transfer%2520from%2520LiDAR%2520features%2520to%2520refine%2520inaccurate%2520BEV%250Arepresentations.%2520TKD%2520mitigates%2520temporal%2520misalignment%2520caused%2520by%2520dynamic%2520objects%250Aby%2520aligning%2520historical%2520radar-camera%2520BEV%2520features%2520with%2520current%2520LiDAR%250Arepresentations.%2520RDKD%2520enhances%2520feature%2520discrimination%2520by%2520distilling%2520relational%250Aknowledge%2520from%2520the%2520teacher%2520model%252C%2520allowing%2520the%2520student%2520to%2520differentiate%250Aforeground%2520and%2520background%2520features.%2520RCTDistill%2520achieves%2520state-of-the-art%250Aradar-camera%2520fusion%2520performance%2520on%2520both%2520the%2520nuScenes%2520and%2520View-of-Delft%2520%2528VoD%2529%250Adatasets%252C%2520with%2520the%2520fastest%2520inference%2520speed%2520of%252026.2%2520FPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RCTDistill%3A%20Cross-Modal%20Knowledge%20Distillation%20Framework%20for%0A%20%20Radar-Camera%203D%20Object%20Detection%20with%20Temporal%20Fusion&entry.906535625=Geonho%20Bang%20and%20Minjae%20Seong%20and%20Jisong%20Kim%20and%20Geunju%20Baek%20and%20Daye%20Oh%20and%20Junhyung%20Kim%20and%20Junho%20Koh%20and%20Jun%20Won%20Choi&entry.1292438233=%20%20Radar-camera%20fusion%20methods%20have%20emerged%20as%20a%20cost-effective%20approach%20for%203D%0Aobject%20detection%20but%20still%20lag%20behind%20LiDAR-based%20methods%20in%20performance.%0ARecent%20works%20have%20focused%20on%20employing%20temporal%20fusion%20and%20Knowledge%0ADistillation%20%28KD%29%20strategies%20to%20overcome%20these%20limitations.%20However%2C%20existing%0Aapproaches%20have%20not%20sufficiently%20accounted%20for%20uncertainties%20arising%20from%0Aobject%20motion%20or%20sensor-specific%20errors%20inherent%20in%20radar%20and%20camera%0Amodalities.%20In%20this%20work%2C%20we%20propose%20RCTDistill%2C%20a%20novel%20cross-modal%20KD%20method%0Abased%20on%20temporal%20fusion%2C%20comprising%20three%20key%20modules%3A%20Range-Azimuth%20Knowledge%0ADistillation%20%28RAKD%29%2C%20Temporal%20Knowledge%20Distillation%20%28TKD%29%2C%20and%0ARegion-Decoupled%20Knowledge%20Distillation%20%28RDKD%29.%20RAKD%20is%20designed%20to%20consider%0Athe%20inherent%20errors%20in%20the%20range%20and%20azimuth%20directions%2C%20enabling%20effective%0Aknowledge%20transfer%20from%20LiDAR%20features%20to%20refine%20inaccurate%20BEV%0Arepresentations.%20TKD%20mitigates%20temporal%20misalignment%20caused%20by%20dynamic%20objects%0Aby%20aligning%20historical%20radar-camera%20BEV%20features%20with%20current%20LiDAR%0Arepresentations.%20RDKD%20enhances%20feature%20discrimination%20by%20distilling%20relational%0Aknowledge%20from%20the%20teacher%20model%2C%20allowing%20the%20student%20to%20differentiate%0Aforeground%20and%20background%20features.%20RCTDistill%20achieves%20state-of-the-art%0Aradar-camera%20fusion%20performance%20on%20both%20the%20nuScenes%20and%20View-of-Delft%20%28VoD%29%0Adatasets%2C%20with%20the%20fastest%20inference%20speed%20of%2026.2%20FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17712v1&entry.124074799=Read"},
{"title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and\n  Foveated Vision Transformers", "author": "Ian Chuang and Jinyu Zou and Andrew Lee and Dechen Gao and Iman Soltani", "abstract": "  Human vision is a highly active process driven by gaze, which directs\nattention to task-relevant regions through foveation, dramatically reducing\nvisual processing. In contrast, robot learning systems typically rely on\npassive, uniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance\nefficiency and robustness. We develop GIAVA (Gaze Integrated Active-Vision\nALOHA), a robot vision system that emulates human head and neck movement, and\ngaze adjustment for foveated processing. Extending the AV-ALOHA robot platform,\nwe introduce a framework for simultaneously collecting eye-tracking,\nperspective control, and robot manipulation demonstration data from a human\noperator. We also open-source a simulation benchmark and dataset for training\nrobot policies that incorporate human gaze. Inspired by recent work in foveated\nimage segmentation and given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme. Compared to uniform patch tokenization, this\nsignificantly reduces the number of tokens, and thus computation. Our results\nshow that our method for foveated robot vision drastically reduces\ncomputational overhead, and enhances robustness to background distractors.\nNotably, on certain high-precision tasks, foveated vision also improves\nperformance, as reflected in higher success rates. Together, these findings\nsuggest that human-inspired foveated visual processing offers untapped\npotential and should be further considered as a useful inductive bias in\nrobotic vision systems. https://ian-chuang.github.io/gaze-av-aloha/\n", "link": "http://arxiv.org/abs/2507.15833v2", "date": "2025-09-22", "relevancy": 2.2468, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5706}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5558}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%2C%20Focus%2C%20Act%3A%20Efficient%20and%20Robust%20Robot%20Learning%20via%20Human%20Gaze%20and%0A%20%20Foveated%20Vision%20Transformers&body=Title%3A%20Look%2C%20Focus%2C%20Act%3A%20Efficient%20and%20Robust%20Robot%20Learning%20via%20Human%20Gaze%20and%0A%20%20Foveated%20Vision%20Transformers%0AAuthor%3A%20Ian%20Chuang%20and%20Jinyu%20Zou%20and%20Andrew%20Lee%20and%20Dechen%20Gao%20and%20Iman%20Soltani%0AAbstract%3A%20%20%20Human%20vision%20is%20a%20highly%20active%20process%20driven%20by%20gaze%2C%20which%20directs%0Aattention%20to%20task-relevant%20regions%20through%20foveation%2C%20dramatically%20reducing%0Avisual%20processing.%20In%20contrast%2C%20robot%20learning%20systems%20typically%20rely%20on%0Apassive%2C%20uniform%20processing%20of%20raw%20camera%20images.%20In%20this%20work%2C%20we%20explore%20how%0Aincorporating%20human-like%20active%20gaze%20into%20robotic%20policies%20can%20enhance%0Aefficiency%20and%20robustness.%20We%20develop%20GIAVA%20%28Gaze%20Integrated%20Active-Vision%0AALOHA%29%2C%20a%20robot%20vision%20system%20that%20emulates%20human%20head%20and%20neck%20movement%2C%20and%0Agaze%20adjustment%20for%20foveated%20processing.%20Extending%20the%20AV-ALOHA%20robot%20platform%2C%0Awe%20introduce%20a%20framework%20for%20simultaneously%20collecting%20eye-tracking%2C%0Aperspective%20control%2C%20and%20robot%20manipulation%20demonstration%20data%20from%20a%20human%0Aoperator.%20We%20also%20open-source%20a%20simulation%20benchmark%20and%20dataset%20for%20training%0Arobot%20policies%20that%20incorporate%20human%20gaze.%20Inspired%20by%20recent%20work%20in%20foveated%0Aimage%20segmentation%20and%20given%20the%20widespread%20use%20of%20Vision%20Transformers%20%28ViTs%29%0Ain%20robot%20learning%2C%20we%20integrate%20gaze%20information%20into%20ViTs%20using%20a%20foveated%0Apatch%20tokenization%20scheme.%20Compared%20to%20uniform%20patch%20tokenization%2C%20this%0Asignificantly%20reduces%20the%20number%20of%20tokens%2C%20and%20thus%20computation.%20Our%20results%0Ashow%20that%20our%20method%20for%20foveated%20robot%20vision%20drastically%20reduces%0Acomputational%20overhead%2C%20and%20enhances%20robustness%20to%20background%20distractors.%0ANotably%2C%20on%20certain%20high-precision%20tasks%2C%20foveated%20vision%20also%20improves%0Aperformance%2C%20as%20reflected%20in%20higher%20success%20rates.%20Together%2C%20these%20findings%0Asuggest%20that%20human-inspired%20foveated%20visual%20processing%20offers%20untapped%0Apotential%20and%20should%20be%20further%20considered%20as%20a%20useful%20inductive%20bias%20in%0Arobotic%20vision%20systems.%20https%3A//ian-chuang.github.io/gaze-av-aloha/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%252C%2520Focus%252C%2520Act%253A%2520Efficient%2520and%2520Robust%2520Robot%2520Learning%2520via%2520Human%2520Gaze%2520and%250A%2520%2520Foveated%2520Vision%2520Transformers%26entry.906535625%3DIan%2520Chuang%2520and%2520Jinyu%2520Zou%2520and%2520Andrew%2520Lee%2520and%2520Dechen%2520Gao%2520and%2520Iman%2520Soltani%26entry.1292438233%3D%2520%2520Human%2520vision%2520is%2520a%2520highly%2520active%2520process%2520driven%2520by%2520gaze%252C%2520which%2520directs%250Aattention%2520to%2520task-relevant%2520regions%2520through%2520foveation%252C%2520dramatically%2520reducing%250Avisual%2520processing.%2520In%2520contrast%252C%2520robot%2520learning%2520systems%2520typically%2520rely%2520on%250Apassive%252C%2520uniform%2520processing%2520of%2520raw%2520camera%2520images.%2520In%2520this%2520work%252C%2520we%2520explore%2520how%250Aincorporating%2520human-like%2520active%2520gaze%2520into%2520robotic%2520policies%2520can%2520enhance%250Aefficiency%2520and%2520robustness.%2520We%2520develop%2520GIAVA%2520%2528Gaze%2520Integrated%2520Active-Vision%250AALOHA%2529%252C%2520a%2520robot%2520vision%2520system%2520that%2520emulates%2520human%2520head%2520and%2520neck%2520movement%252C%2520and%250Agaze%2520adjustment%2520for%2520foveated%2520processing.%2520Extending%2520the%2520AV-ALOHA%2520robot%2520platform%252C%250Awe%2520introduce%2520a%2520framework%2520for%2520simultaneously%2520collecting%2520eye-tracking%252C%250Aperspective%2520control%252C%2520and%2520robot%2520manipulation%2520demonstration%2520data%2520from%2520a%2520human%250Aoperator.%2520We%2520also%2520open-source%2520a%2520simulation%2520benchmark%2520and%2520dataset%2520for%2520training%250Arobot%2520policies%2520that%2520incorporate%2520human%2520gaze.%2520Inspired%2520by%2520recent%2520work%2520in%2520foveated%250Aimage%2520segmentation%2520and%2520given%2520the%2520widespread%2520use%2520of%2520Vision%2520Transformers%2520%2528ViTs%2529%250Ain%2520robot%2520learning%252C%2520we%2520integrate%2520gaze%2520information%2520into%2520ViTs%2520using%2520a%2520foveated%250Apatch%2520tokenization%2520scheme.%2520Compared%2520to%2520uniform%2520patch%2520tokenization%252C%2520this%250Asignificantly%2520reduces%2520the%2520number%2520of%2520tokens%252C%2520and%2520thus%2520computation.%2520Our%2520results%250Ashow%2520that%2520our%2520method%2520for%2520foveated%2520robot%2520vision%2520drastically%2520reduces%250Acomputational%2520overhead%252C%2520and%2520enhances%2520robustness%2520to%2520background%2520distractors.%250ANotably%252C%2520on%2520certain%2520high-precision%2520tasks%252C%2520foveated%2520vision%2520also%2520improves%250Aperformance%252C%2520as%2520reflected%2520in%2520higher%2520success%2520rates.%2520Together%252C%2520these%2520findings%250Asuggest%2520that%2520human-inspired%2520foveated%2520visual%2520processing%2520offers%2520untapped%250Apotential%2520and%2520should%2520be%2520further%2520considered%2520as%2520a%2520useful%2520inductive%2520bias%2520in%250Arobotic%2520vision%2520systems.%2520https%253A//ian-chuang.github.io/gaze-av-aloha/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%2C%20Focus%2C%20Act%3A%20Efficient%20and%20Robust%20Robot%20Learning%20via%20Human%20Gaze%20and%0A%20%20Foveated%20Vision%20Transformers&entry.906535625=Ian%20Chuang%20and%20Jinyu%20Zou%20and%20Andrew%20Lee%20and%20Dechen%20Gao%20and%20Iman%20Soltani&entry.1292438233=%20%20Human%20vision%20is%20a%20highly%20active%20process%20driven%20by%20gaze%2C%20which%20directs%0Aattention%20to%20task-relevant%20regions%20through%20foveation%2C%20dramatically%20reducing%0Avisual%20processing.%20In%20contrast%2C%20robot%20learning%20systems%20typically%20rely%20on%0Apassive%2C%20uniform%20processing%20of%20raw%20camera%20images.%20In%20this%20work%2C%20we%20explore%20how%0Aincorporating%20human-like%20active%20gaze%20into%20robotic%20policies%20can%20enhance%0Aefficiency%20and%20robustness.%20We%20develop%20GIAVA%20%28Gaze%20Integrated%20Active-Vision%0AALOHA%29%2C%20a%20robot%20vision%20system%20that%20emulates%20human%20head%20and%20neck%20movement%2C%20and%0Agaze%20adjustment%20for%20foveated%20processing.%20Extending%20the%20AV-ALOHA%20robot%20platform%2C%0Awe%20introduce%20a%20framework%20for%20simultaneously%20collecting%20eye-tracking%2C%0Aperspective%20control%2C%20and%20robot%20manipulation%20demonstration%20data%20from%20a%20human%0Aoperator.%20We%20also%20open-source%20a%20simulation%20benchmark%20and%20dataset%20for%20training%0Arobot%20policies%20that%20incorporate%20human%20gaze.%20Inspired%20by%20recent%20work%20in%20foveated%0Aimage%20segmentation%20and%20given%20the%20widespread%20use%20of%20Vision%20Transformers%20%28ViTs%29%0Ain%20robot%20learning%2C%20we%20integrate%20gaze%20information%20into%20ViTs%20using%20a%20foveated%0Apatch%20tokenization%20scheme.%20Compared%20to%20uniform%20patch%20tokenization%2C%20this%0Asignificantly%20reduces%20the%20number%20of%20tokens%2C%20and%20thus%20computation.%20Our%20results%0Ashow%20that%20our%20method%20for%20foveated%20robot%20vision%20drastically%20reduces%0Acomputational%20overhead%2C%20and%20enhances%20robustness%20to%20background%20distractors.%0ANotably%2C%20on%20certain%20high-precision%20tasks%2C%20foveated%20vision%20also%20improves%0Aperformance%2C%20as%20reflected%20in%20higher%20success%20rates.%20Together%2C%20these%20findings%0Asuggest%20that%20human-inspired%20foveated%20visual%20processing%20offers%20untapped%0Apotential%20and%20should%20be%20further%20considered%20as%20a%20useful%20inductive%20bias%20in%0Arobotic%20vision%20systems.%20https%3A//ian-chuang.github.io/gaze-av-aloha/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15833v2&entry.124074799=Read"},
{"title": "Domain Adaptive Object Detection for Space Applications with Real-Time\n  Constraints", "author": "Samet Hicsonmez and Abd El Rahman Shabayek and Arunkumar Rathinam and Djamila Aouada", "abstract": "  Object detection is essential in space applications targeting Space Domain\nAwareness and also applications involving relative navigation scenarios.\nCurrent deep learning models for Object Detection in space applications are\noften trained on synthetic data from simulators, however, the model performance\ndrops significantly on real-world data due to the domain gap. However, domain\nadaptive object detection is an overlooked problem in the community. In this\nwork, we first show the importance of domain adaptation and then explore\nSupervised Domain Adaptation (SDA) to reduce this gap using minimal labeled\nreal data. We build on a recent semi-supervised adaptation method and tailor it\nfor object detection. Our approach combines domain-invariant feature learning\nwith a CNN-based domain discriminator and invariant risk minimization using a\ndomain-independent regression head. To meet real-time deployment needs, we test\nour method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet\nbackbone and on the more advanced Fully Convolutional One-Stage object detector\n(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and\nSPARK. The results show up to 20-point improvements in average precision (AP)\nwith just 250 labeled real images.\n", "link": "http://arxiv.org/abs/2509.17593v1", "date": "2025-09-22", "relevancy": 2.2382, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5774}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptive%20Object%20Detection%20for%20Space%20Applications%20with%20Real-Time%0A%20%20Constraints&body=Title%3A%20Domain%20Adaptive%20Object%20Detection%20for%20Space%20Applications%20with%20Real-Time%0A%20%20Constraints%0AAuthor%3A%20Samet%20Hicsonmez%20and%20Abd%20El%20Rahman%20Shabayek%20and%20Arunkumar%20Rathinam%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Object%20detection%20is%20essential%20in%20space%20applications%20targeting%20Space%20Domain%0AAwareness%20and%20also%20applications%20involving%20relative%20navigation%20scenarios.%0ACurrent%20deep%20learning%20models%20for%20Object%20Detection%20in%20space%20applications%20are%0Aoften%20trained%20on%20synthetic%20data%20from%20simulators%2C%20however%2C%20the%20model%20performance%0Adrops%20significantly%20on%20real-world%20data%20due%20to%20the%20domain%20gap.%20However%2C%20domain%0Aadaptive%20object%20detection%20is%20an%20overlooked%20problem%20in%20the%20community.%20In%20this%0Awork%2C%20we%20first%20show%20the%20importance%20of%20domain%20adaptation%20and%20then%20explore%0ASupervised%20Domain%20Adaptation%20%28SDA%29%20to%20reduce%20this%20gap%20using%20minimal%20labeled%0Areal%20data.%20We%20build%20on%20a%20recent%20semi-supervised%20adaptation%20method%20and%20tailor%20it%0Afor%20object%20detection.%20Our%20approach%20combines%20domain-invariant%20feature%20learning%0Awith%20a%20CNN-based%20domain%20discriminator%20and%20invariant%20risk%20minimization%20using%20a%0Adomain-independent%20regression%20head.%20To%20meet%20real-time%20deployment%20needs%2C%20we%20test%0Aour%20method%20on%20a%20lightweight%20Single%20Shot%20Multibox%20Detector%20%28SSD%29%20with%20MobileNet%0Abackbone%20and%20on%20the%20more%20advanced%20Fully%20Convolutional%20One-Stage%20object%20detector%0A%28FCOS%29%20with%20ResNet-50%20backbone.%20We%20evaluated%20on%20two%20space%20datasets%2C%20SPEED%2B%20and%0ASPARK.%20The%20results%20show%20up%20to%2020-point%20improvements%20in%20average%20precision%20%28AP%29%0Awith%20just%20250%20labeled%20real%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Adaptive%2520Object%2520Detection%2520for%2520Space%2520Applications%2520with%2520Real-Time%250A%2520%2520Constraints%26entry.906535625%3DSamet%2520Hicsonmez%2520and%2520Abd%2520El%2520Rahman%2520Shabayek%2520and%2520Arunkumar%2520Rathinam%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Object%2520detection%2520is%2520essential%2520in%2520space%2520applications%2520targeting%2520Space%2520Domain%250AAwareness%2520and%2520also%2520applications%2520involving%2520relative%2520navigation%2520scenarios.%250ACurrent%2520deep%2520learning%2520models%2520for%2520Object%2520Detection%2520in%2520space%2520applications%2520are%250Aoften%2520trained%2520on%2520synthetic%2520data%2520from%2520simulators%252C%2520however%252C%2520the%2520model%2520performance%250Adrops%2520significantly%2520on%2520real-world%2520data%2520due%2520to%2520the%2520domain%2520gap.%2520However%252C%2520domain%250Aadaptive%2520object%2520detection%2520is%2520an%2520overlooked%2520problem%2520in%2520the%2520community.%2520In%2520this%250Awork%252C%2520we%2520first%2520show%2520the%2520importance%2520of%2520domain%2520adaptation%2520and%2520then%2520explore%250ASupervised%2520Domain%2520Adaptation%2520%2528SDA%2529%2520to%2520reduce%2520this%2520gap%2520using%2520minimal%2520labeled%250Areal%2520data.%2520We%2520build%2520on%2520a%2520recent%2520semi-supervised%2520adaptation%2520method%2520and%2520tailor%2520it%250Afor%2520object%2520detection.%2520Our%2520approach%2520combines%2520domain-invariant%2520feature%2520learning%250Awith%2520a%2520CNN-based%2520domain%2520discriminator%2520and%2520invariant%2520risk%2520minimization%2520using%2520a%250Adomain-independent%2520regression%2520head.%2520To%2520meet%2520real-time%2520deployment%2520needs%252C%2520we%2520test%250Aour%2520method%2520on%2520a%2520lightweight%2520Single%2520Shot%2520Multibox%2520Detector%2520%2528SSD%2529%2520with%2520MobileNet%250Abackbone%2520and%2520on%2520the%2520more%2520advanced%2520Fully%2520Convolutional%2520One-Stage%2520object%2520detector%250A%2528FCOS%2529%2520with%2520ResNet-50%2520backbone.%2520We%2520evaluated%2520on%2520two%2520space%2520datasets%252C%2520SPEED%252B%2520and%250ASPARK.%2520The%2520results%2520show%2520up%2520to%252020-point%2520improvements%2520in%2520average%2520precision%2520%2528AP%2529%250Awith%2520just%2520250%2520labeled%2520real%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptive%20Object%20Detection%20for%20Space%20Applications%20with%20Real-Time%0A%20%20Constraints&entry.906535625=Samet%20Hicsonmez%20and%20Abd%20El%20Rahman%20Shabayek%20and%20Arunkumar%20Rathinam%20and%20Djamila%20Aouada&entry.1292438233=%20%20Object%20detection%20is%20essential%20in%20space%20applications%20targeting%20Space%20Domain%0AAwareness%20and%20also%20applications%20involving%20relative%20navigation%20scenarios.%0ACurrent%20deep%20learning%20models%20for%20Object%20Detection%20in%20space%20applications%20are%0Aoften%20trained%20on%20synthetic%20data%20from%20simulators%2C%20however%2C%20the%20model%20performance%0Adrops%20significantly%20on%20real-world%20data%20due%20to%20the%20domain%20gap.%20However%2C%20domain%0Aadaptive%20object%20detection%20is%20an%20overlooked%20problem%20in%20the%20community.%20In%20this%0Awork%2C%20we%20first%20show%20the%20importance%20of%20domain%20adaptation%20and%20then%20explore%0ASupervised%20Domain%20Adaptation%20%28SDA%29%20to%20reduce%20this%20gap%20using%20minimal%20labeled%0Areal%20data.%20We%20build%20on%20a%20recent%20semi-supervised%20adaptation%20method%20and%20tailor%20it%0Afor%20object%20detection.%20Our%20approach%20combines%20domain-invariant%20feature%20learning%0Awith%20a%20CNN-based%20domain%20discriminator%20and%20invariant%20risk%20minimization%20using%20a%0Adomain-independent%20regression%20head.%20To%20meet%20real-time%20deployment%20needs%2C%20we%20test%0Aour%20method%20on%20a%20lightweight%20Single%20Shot%20Multibox%20Detector%20%28SSD%29%20with%20MobileNet%0Abackbone%20and%20on%20the%20more%20advanced%20Fully%20Convolutional%20One-Stage%20object%20detector%0A%28FCOS%29%20with%20ResNet-50%20backbone.%20We%20evaluated%20on%20two%20space%20datasets%2C%20SPEED%2B%20and%0ASPARK.%20The%20results%20show%20up%20to%2020-point%20improvements%20in%20average%20precision%20%28AP%29%0Awith%20just%20250%20labeled%20real%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17593v1&entry.124074799=Read"},
{"title": "A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot\n  Action Recognition", "author": "Zilin Gao and Qilong Wang and Bingbing Zhang and Qinghua Hu and Peihua Li", "abstract": "  Thanks to capability to alleviate the cost of large-scale annotation,\nfew-shot action recognition (FSAR) has attracted increased attention of\nresearchers in recent years. Existing FSAR approaches typically neglect the\nrole of individual motion pattern in comparison, and under-explore the feature\nstatistics for video dynamics. Thereby, they struggle to handle the challenging\ntemporal misalignment in video dynamics, particularly by using 2D backbones. To\novercome these limitations, this work proposes an adaptively aligned\nmulti-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the\nlatent video dynamics with a collection of powerful representation candidates\nand adaptively align them in an instance-guided manner. To this end, our\nA$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$\nmodule) for matching, and multi-scale second-order moment (M$^2$ block) for\nstrong representation. Specifically, M$^2$ block develops a collection of\nsemantic second-order descriptors at multiple spatio-temporal scales.\nFurthermore, A$^2$ module aims to adaptively select informative candidate\ndescriptors while considering the individual motion pattern. By such means, our\nA$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem\nby establishing an adaptive alignment protocol for strong representation.\nNotably, our proposed method generalizes well to various few-shot settings and\ndiverse metrics. The experiments are conducted on five widely used FSAR\nbenchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive\nperformance compared to state-of-the-arts, demonstrating its effectiveness and\ngeneralization.\n", "link": "http://arxiv.org/abs/2509.17638v1", "date": "2025-09-22", "relevancy": 2.2131, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5558}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%24%5E2%24M%24%5E2%24-Net%3A%20Adaptively%20Aligned%20Multi-Scale%20Moment%20for%20Few-Shot%0A%20%20Action%20Recognition&body=Title%3A%20A%24%5E2%24M%24%5E2%24-Net%3A%20Adaptively%20Aligned%20Multi-Scale%20Moment%20for%20Few-Shot%0A%20%20Action%20Recognition%0AAuthor%3A%20Zilin%20Gao%20and%20Qilong%20Wang%20and%20Bingbing%20Zhang%20and%20Qinghua%20Hu%20and%20Peihua%20Li%0AAbstract%3A%20%20%20Thanks%20to%20capability%20to%20alleviate%20the%20cost%20of%20large-scale%20annotation%2C%0Afew-shot%20action%20recognition%20%28FSAR%29%20has%20attracted%20increased%20attention%20of%0Aresearchers%20in%20recent%20years.%20Existing%20FSAR%20approaches%20typically%20neglect%20the%0Arole%20of%20individual%20motion%20pattern%20in%20comparison%2C%20and%20under-explore%20the%20feature%0Astatistics%20for%20video%20dynamics.%20Thereby%2C%20they%20struggle%20to%20handle%20the%20challenging%0Atemporal%20misalignment%20in%20video%20dynamics%2C%20particularly%20by%20using%202D%20backbones.%20To%0Aovercome%20these%20limitations%2C%20this%20work%20proposes%20an%20adaptively%20aligned%0Amulti-scale%20second-order%20moment%20network%2C%20namely%20A%24%5E2%24M%24%5E2%24-Net%2C%20to%20describe%20the%0Alatent%20video%20dynamics%20with%20a%20collection%20of%20powerful%20representation%20candidates%0Aand%20adaptively%20align%20them%20in%20an%20instance-guided%20manner.%20To%20this%20end%2C%20our%0AA%24%5E2%24M%24%5E2%24-Net%20involves%20two%20core%20components%2C%20namely%2C%20adaptive%20alignment%20%28A%24%5E2%24%0Amodule%29%20for%20matching%2C%20and%20multi-scale%20second-order%20moment%20%28M%24%5E2%24%20block%29%20for%0Astrong%20representation.%20Specifically%2C%20M%24%5E2%24%20block%20develops%20a%20collection%20of%0Asemantic%20second-order%20descriptors%20at%20multiple%20spatio-temporal%20scales.%0AFurthermore%2C%20A%24%5E2%24%20module%20aims%20to%20adaptively%20select%20informative%20candidate%0Adescriptors%20while%20considering%20the%20individual%20motion%20pattern.%20By%20such%20means%2C%20our%0AA%24%5E2%24M%24%5E2%24-Net%20is%20able%20to%20handle%20the%20challenging%20temporal%20misalignment%20problem%0Aby%20establishing%20an%20adaptive%20alignment%20protocol%20for%20strong%20representation.%0ANotably%2C%20our%20proposed%20method%20generalizes%20well%20to%20various%20few-shot%20settings%20and%0Adiverse%20metrics.%20The%20experiments%20are%20conducted%20on%20five%20widely%20used%20FSAR%0Abenchmarks%2C%20and%20the%20results%20show%20our%20A%24%5E2%24M%24%5E2%24-Net%20achieves%20very%20competitive%0Aperformance%20compared%20to%20state-of-the-arts%2C%20demonstrating%20its%20effectiveness%20and%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2524%255E2%2524M%2524%255E2%2524-Net%253A%2520Adaptively%2520Aligned%2520Multi-Scale%2520Moment%2520for%2520Few-Shot%250A%2520%2520Action%2520Recognition%26entry.906535625%3DZilin%2520Gao%2520and%2520Qilong%2520Wang%2520and%2520Bingbing%2520Zhang%2520and%2520Qinghua%2520Hu%2520and%2520Peihua%2520Li%26entry.1292438233%3D%2520%2520Thanks%2520to%2520capability%2520to%2520alleviate%2520the%2520cost%2520of%2520large-scale%2520annotation%252C%250Afew-shot%2520action%2520recognition%2520%2528FSAR%2529%2520has%2520attracted%2520increased%2520attention%2520of%250Aresearchers%2520in%2520recent%2520years.%2520Existing%2520FSAR%2520approaches%2520typically%2520neglect%2520the%250Arole%2520of%2520individual%2520motion%2520pattern%2520in%2520comparison%252C%2520and%2520under-explore%2520the%2520feature%250Astatistics%2520for%2520video%2520dynamics.%2520Thereby%252C%2520they%2520struggle%2520to%2520handle%2520the%2520challenging%250Atemporal%2520misalignment%2520in%2520video%2520dynamics%252C%2520particularly%2520by%2520using%25202D%2520backbones.%2520To%250Aovercome%2520these%2520limitations%252C%2520this%2520work%2520proposes%2520an%2520adaptively%2520aligned%250Amulti-scale%2520second-order%2520moment%2520network%252C%2520namely%2520A%2524%255E2%2524M%2524%255E2%2524-Net%252C%2520to%2520describe%2520the%250Alatent%2520video%2520dynamics%2520with%2520a%2520collection%2520of%2520powerful%2520representation%2520candidates%250Aand%2520adaptively%2520align%2520them%2520in%2520an%2520instance-guided%2520manner.%2520To%2520this%2520end%252C%2520our%250AA%2524%255E2%2524M%2524%255E2%2524-Net%2520involves%2520two%2520core%2520components%252C%2520namely%252C%2520adaptive%2520alignment%2520%2528A%2524%255E2%2524%250Amodule%2529%2520for%2520matching%252C%2520and%2520multi-scale%2520second-order%2520moment%2520%2528M%2524%255E2%2524%2520block%2529%2520for%250Astrong%2520representation.%2520Specifically%252C%2520M%2524%255E2%2524%2520block%2520develops%2520a%2520collection%2520of%250Asemantic%2520second-order%2520descriptors%2520at%2520multiple%2520spatio-temporal%2520scales.%250AFurthermore%252C%2520A%2524%255E2%2524%2520module%2520aims%2520to%2520adaptively%2520select%2520informative%2520candidate%250Adescriptors%2520while%2520considering%2520the%2520individual%2520motion%2520pattern.%2520By%2520such%2520means%252C%2520our%250AA%2524%255E2%2524M%2524%255E2%2524-Net%2520is%2520able%2520to%2520handle%2520the%2520challenging%2520temporal%2520misalignment%2520problem%250Aby%2520establishing%2520an%2520adaptive%2520alignment%2520protocol%2520for%2520strong%2520representation.%250ANotably%252C%2520our%2520proposed%2520method%2520generalizes%2520well%2520to%2520various%2520few-shot%2520settings%2520and%250Adiverse%2520metrics.%2520The%2520experiments%2520are%2520conducted%2520on%2520five%2520widely%2520used%2520FSAR%250Abenchmarks%252C%2520and%2520the%2520results%2520show%2520our%2520A%2524%255E2%2524M%2524%255E2%2524-Net%2520achieves%2520very%2520competitive%250Aperformance%2520compared%2520to%2520state-of-the-arts%252C%2520demonstrating%2520its%2520effectiveness%2520and%250Ageneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%24%5E2%24M%24%5E2%24-Net%3A%20Adaptively%20Aligned%20Multi-Scale%20Moment%20for%20Few-Shot%0A%20%20Action%20Recognition&entry.906535625=Zilin%20Gao%20and%20Qilong%20Wang%20and%20Bingbing%20Zhang%20and%20Qinghua%20Hu%20and%20Peihua%20Li&entry.1292438233=%20%20Thanks%20to%20capability%20to%20alleviate%20the%20cost%20of%20large-scale%20annotation%2C%0Afew-shot%20action%20recognition%20%28FSAR%29%20has%20attracted%20increased%20attention%20of%0Aresearchers%20in%20recent%20years.%20Existing%20FSAR%20approaches%20typically%20neglect%20the%0Arole%20of%20individual%20motion%20pattern%20in%20comparison%2C%20and%20under-explore%20the%20feature%0Astatistics%20for%20video%20dynamics.%20Thereby%2C%20they%20struggle%20to%20handle%20the%20challenging%0Atemporal%20misalignment%20in%20video%20dynamics%2C%20particularly%20by%20using%202D%20backbones.%20To%0Aovercome%20these%20limitations%2C%20this%20work%20proposes%20an%20adaptively%20aligned%0Amulti-scale%20second-order%20moment%20network%2C%20namely%20A%24%5E2%24M%24%5E2%24-Net%2C%20to%20describe%20the%0Alatent%20video%20dynamics%20with%20a%20collection%20of%20powerful%20representation%20candidates%0Aand%20adaptively%20align%20them%20in%20an%20instance-guided%20manner.%20To%20this%20end%2C%20our%0AA%24%5E2%24M%24%5E2%24-Net%20involves%20two%20core%20components%2C%20namely%2C%20adaptive%20alignment%20%28A%24%5E2%24%0Amodule%29%20for%20matching%2C%20and%20multi-scale%20second-order%20moment%20%28M%24%5E2%24%20block%29%20for%0Astrong%20representation.%20Specifically%2C%20M%24%5E2%24%20block%20develops%20a%20collection%20of%0Asemantic%20second-order%20descriptors%20at%20multiple%20spatio-temporal%20scales.%0AFurthermore%2C%20A%24%5E2%24%20module%20aims%20to%20adaptively%20select%20informative%20candidate%0Adescriptors%20while%20considering%20the%20individual%20motion%20pattern.%20By%20such%20means%2C%20our%0AA%24%5E2%24M%24%5E2%24-Net%20is%20able%20to%20handle%20the%20challenging%20temporal%20misalignment%20problem%0Aby%20establishing%20an%20adaptive%20alignment%20protocol%20for%20strong%20representation.%0ANotably%2C%20our%20proposed%20method%20generalizes%20well%20to%20various%20few-shot%20settings%20and%0Adiverse%20metrics.%20The%20experiments%20are%20conducted%20on%20five%20widely%20used%20FSAR%0Abenchmarks%2C%20and%20the%20results%20show%20our%20A%24%5E2%24M%24%5E2%24-Net%20achieves%20very%20competitive%0Aperformance%20compared%20to%20state-of-the-arts%2C%20demonstrating%20its%20effectiveness%20and%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17638v1&entry.124074799=Read"},
{"title": "I-FailSense: Towards General Robotic Failure Detection with\n  Vision-Language Models", "author": "Clemence Grislain and Hamed Rahimi and Olivier Sigaud and Mohamed Chetouani", "abstract": "  Language-conditioned robotic manipulation in open-world settings requires not\nonly accurate task execution but also the ability to detect failures for robust\ndeployment in real-world environments. Although recent advances in\nvision-language models (VLMs) have significantly improved the spatial reasoning\nand task-planning capabilities of robots, they remain limited in their ability\nto recognize their own failures. In particular, a critical yet underexplored\nchallenge lies in detecting semantic misalignment errors, where the robot\nexecutes a task that is semantically meaningful but inconsistent with the given\ninstruction. To address this, we propose a method for building datasets\ntargeting Semantic Misalignment Failures detection, from existing\nlanguage-conditioned manipulation datasets. We also present I-FailSense, an\nopen-source VLM framework with grounded arbitration designed specifically for\nfailure detection. Our approach relies on post-training a base VLM, followed by\ntraining lightweight classification heads, called FS blocks, attached to\ndifferent internal layers of the VLM and whose predictions are aggregated using\nan ensembling mechanism. Experiments show that I-FailSense outperforms\nstate-of-the-art VLMs, both comparable in size and larger, in detecting\nsemantic misalignment errors. Notably, despite being trained only on semantic\nmisalignment detection, I-FailSense generalizes to broader robotic failure\ncategories and effectively transfers to other simulation environments and\nreal-world with zero-shot or minimal post-training. The datasets and models are\npublicly released on HuggingFace (Webpage:\nhttps://clemgris.github.io/I-FailSense/).\n", "link": "http://arxiv.org/abs/2509.16072v2", "date": "2025-09-22", "relevancy": 2.202, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5982}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I-FailSense%3A%20Towards%20General%20Robotic%20Failure%20Detection%20with%0A%20%20Vision-Language%20Models&body=Title%3A%20I-FailSense%3A%20Towards%20General%20Robotic%20Failure%20Detection%20with%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Clemence%20Grislain%20and%20Hamed%20Rahimi%20and%20Olivier%20Sigaud%20and%20Mohamed%20Chetouani%0AAbstract%3A%20%20%20Language-conditioned%20robotic%20manipulation%20in%20open-world%20settings%20requires%20not%0Aonly%20accurate%20task%20execution%20but%20also%20the%20ability%20to%20detect%20failures%20for%20robust%0Adeployment%20in%20real-world%20environments.%20Although%20recent%20advances%20in%0Avision-language%20models%20%28VLMs%29%20have%20significantly%20improved%20the%20spatial%20reasoning%0Aand%20task-planning%20capabilities%20of%20robots%2C%20they%20remain%20limited%20in%20their%20ability%0Ato%20recognize%20their%20own%20failures.%20In%20particular%2C%20a%20critical%20yet%20underexplored%0Achallenge%20lies%20in%20detecting%20semantic%20misalignment%20errors%2C%20where%20the%20robot%0Aexecutes%20a%20task%20that%20is%20semantically%20meaningful%20but%20inconsistent%20with%20the%20given%0Ainstruction.%20To%20address%20this%2C%20we%20propose%20a%20method%20for%20building%20datasets%0Atargeting%20Semantic%20Misalignment%20Failures%20detection%2C%20from%20existing%0Alanguage-conditioned%20manipulation%20datasets.%20We%20also%20present%20I-FailSense%2C%20an%0Aopen-source%20VLM%20framework%20with%20grounded%20arbitration%20designed%20specifically%20for%0Afailure%20detection.%20Our%20approach%20relies%20on%20post-training%20a%20base%20VLM%2C%20followed%20by%0Atraining%20lightweight%20classification%20heads%2C%20called%20FS%20blocks%2C%20attached%20to%0Adifferent%20internal%20layers%20of%20the%20VLM%20and%20whose%20predictions%20are%20aggregated%20using%0Aan%20ensembling%20mechanism.%20Experiments%20show%20that%20I-FailSense%20outperforms%0Astate-of-the-art%20VLMs%2C%20both%20comparable%20in%20size%20and%20larger%2C%20in%20detecting%0Asemantic%20misalignment%20errors.%20Notably%2C%20despite%20being%20trained%20only%20on%20semantic%0Amisalignment%20detection%2C%20I-FailSense%20generalizes%20to%20broader%20robotic%20failure%0Acategories%20and%20effectively%20transfers%20to%20other%20simulation%20environments%20and%0Areal-world%20with%20zero-shot%20or%20minimal%20post-training.%20The%20datasets%20and%20models%20are%0Apublicly%20released%20on%20HuggingFace%20%28Webpage%3A%0Ahttps%3A//clemgris.github.io/I-FailSense/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI-FailSense%253A%2520Towards%2520General%2520Robotic%2520Failure%2520Detection%2520with%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DClemence%2520Grislain%2520and%2520Hamed%2520Rahimi%2520and%2520Olivier%2520Sigaud%2520and%2520Mohamed%2520Chetouani%26entry.1292438233%3D%2520%2520Language-conditioned%2520robotic%2520manipulation%2520in%2520open-world%2520settings%2520requires%2520not%250Aonly%2520accurate%2520task%2520execution%2520but%2520also%2520the%2520ability%2520to%2520detect%2520failures%2520for%2520robust%250Adeployment%2520in%2520real-world%2520environments.%2520Although%2520recent%2520advances%2520in%250Avision-language%2520models%2520%2528VLMs%2529%2520have%2520significantly%2520improved%2520the%2520spatial%2520reasoning%250Aand%2520task-planning%2520capabilities%2520of%2520robots%252C%2520they%2520remain%2520limited%2520in%2520their%2520ability%250Ato%2520recognize%2520their%2520own%2520failures.%2520In%2520particular%252C%2520a%2520critical%2520yet%2520underexplored%250Achallenge%2520lies%2520in%2520detecting%2520semantic%2520misalignment%2520errors%252C%2520where%2520the%2520robot%250Aexecutes%2520a%2520task%2520that%2520is%2520semantically%2520meaningful%2520but%2520inconsistent%2520with%2520the%2520given%250Ainstruction.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520method%2520for%2520building%2520datasets%250Atargeting%2520Semantic%2520Misalignment%2520Failures%2520detection%252C%2520from%2520existing%250Alanguage-conditioned%2520manipulation%2520datasets.%2520We%2520also%2520present%2520I-FailSense%252C%2520an%250Aopen-source%2520VLM%2520framework%2520with%2520grounded%2520arbitration%2520designed%2520specifically%2520for%250Afailure%2520detection.%2520Our%2520approach%2520relies%2520on%2520post-training%2520a%2520base%2520VLM%252C%2520followed%2520by%250Atraining%2520lightweight%2520classification%2520heads%252C%2520called%2520FS%2520blocks%252C%2520attached%2520to%250Adifferent%2520internal%2520layers%2520of%2520the%2520VLM%2520and%2520whose%2520predictions%2520are%2520aggregated%2520using%250Aan%2520ensembling%2520mechanism.%2520Experiments%2520show%2520that%2520I-FailSense%2520outperforms%250Astate-of-the-art%2520VLMs%252C%2520both%2520comparable%2520in%2520size%2520and%2520larger%252C%2520in%2520detecting%250Asemantic%2520misalignment%2520errors.%2520Notably%252C%2520despite%2520being%2520trained%2520only%2520on%2520semantic%250Amisalignment%2520detection%252C%2520I-FailSense%2520generalizes%2520to%2520broader%2520robotic%2520failure%250Acategories%2520and%2520effectively%2520transfers%2520to%2520other%2520simulation%2520environments%2520and%250Areal-world%2520with%2520zero-shot%2520or%2520minimal%2520post-training.%2520The%2520datasets%2520and%2520models%2520are%250Apublicly%2520released%2520on%2520HuggingFace%2520%2528Webpage%253A%250Ahttps%253A//clemgris.github.io/I-FailSense/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I-FailSense%3A%20Towards%20General%20Robotic%20Failure%20Detection%20with%0A%20%20Vision-Language%20Models&entry.906535625=Clemence%20Grislain%20and%20Hamed%20Rahimi%20and%20Olivier%20Sigaud%20and%20Mohamed%20Chetouani&entry.1292438233=%20%20Language-conditioned%20robotic%20manipulation%20in%20open-world%20settings%20requires%20not%0Aonly%20accurate%20task%20execution%20but%20also%20the%20ability%20to%20detect%20failures%20for%20robust%0Adeployment%20in%20real-world%20environments.%20Although%20recent%20advances%20in%0Avision-language%20models%20%28VLMs%29%20have%20significantly%20improved%20the%20spatial%20reasoning%0Aand%20task-planning%20capabilities%20of%20robots%2C%20they%20remain%20limited%20in%20their%20ability%0Ato%20recognize%20their%20own%20failures.%20In%20particular%2C%20a%20critical%20yet%20underexplored%0Achallenge%20lies%20in%20detecting%20semantic%20misalignment%20errors%2C%20where%20the%20robot%0Aexecutes%20a%20task%20that%20is%20semantically%20meaningful%20but%20inconsistent%20with%20the%20given%0Ainstruction.%20To%20address%20this%2C%20we%20propose%20a%20method%20for%20building%20datasets%0Atargeting%20Semantic%20Misalignment%20Failures%20detection%2C%20from%20existing%0Alanguage-conditioned%20manipulation%20datasets.%20We%20also%20present%20I-FailSense%2C%20an%0Aopen-source%20VLM%20framework%20with%20grounded%20arbitration%20designed%20specifically%20for%0Afailure%20detection.%20Our%20approach%20relies%20on%20post-training%20a%20base%20VLM%2C%20followed%20by%0Atraining%20lightweight%20classification%20heads%2C%20called%20FS%20blocks%2C%20attached%20to%0Adifferent%20internal%20layers%20of%20the%20VLM%20and%20whose%20predictions%20are%20aggregated%20using%0Aan%20ensembling%20mechanism.%20Experiments%20show%20that%20I-FailSense%20outperforms%0Astate-of-the-art%20VLMs%2C%20both%20comparable%20in%20size%20and%20larger%2C%20in%20detecting%0Asemantic%20misalignment%20errors.%20Notably%2C%20despite%20being%20trained%20only%20on%20semantic%0Amisalignment%20detection%2C%20I-FailSense%20generalizes%20to%20broader%20robotic%20failure%0Acategories%20and%20effectively%20transfers%20to%20other%20simulation%20environments%20and%0Areal-world%20with%20zero-shot%20or%20minimal%20post-training.%20The%20datasets%20and%20models%20are%0Apublicly%20released%20on%20HuggingFace%20%28Webpage%3A%0Ahttps%3A//clemgris.github.io/I-FailSense/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16072v2&entry.124074799=Read"},
{"title": "The Surprising Effectiveness of Linear Models for Whole-Body\n  Model-Predictive Control", "author": "Arun L. Bishop and Juan Alvarez-Padilla and Sam Schoedel and Ibrahima Sory Sow and Juee Chandrachud and Sheitej Sharma and Will Kraus and Beomyeong Park and Robert J. Griffin and John M. Dolan and Zachary Manchester", "abstract": "  When do locomotion controllers require reasoning about nonlinearities? In\nthis work, we show that a whole-body model-predictive controller using a simple\nlinear time-invariant approximation of the whole-body dynamics is able to\nexecute basic locomotion tasks on complex legged robots. The formulation\nrequires no online nonlinear dynamics evaluations or matrix inversions. We\ndemonstrate walking, disturbance rejection, and even navigation to a goal\nposition without a separate footstep planner on a quadrupedal robot. In\naddition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with\nsignificant limb inertia, complex actuator dynamics, and large sim-to-real gap.\n", "link": "http://arxiv.org/abs/2509.17884v1", "date": "2025-09-22", "relevancy": 2.198, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6326}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5438}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Surprising%20Effectiveness%20of%20Linear%20Models%20for%20Whole-Body%0A%20%20Model-Predictive%20Control&body=Title%3A%20The%20Surprising%20Effectiveness%20of%20Linear%20Models%20for%20Whole-Body%0A%20%20Model-Predictive%20Control%0AAuthor%3A%20Arun%20L.%20Bishop%20and%20Juan%20Alvarez-Padilla%20and%20Sam%20Schoedel%20and%20Ibrahima%20Sory%20Sow%20and%20Juee%20Chandrachud%20and%20Sheitej%20Sharma%20and%20Will%20Kraus%20and%20Beomyeong%20Park%20and%20Robert%20J.%20Griffin%20and%20John%20M.%20Dolan%20and%20Zachary%20Manchester%0AAbstract%3A%20%20%20When%20do%20locomotion%20controllers%20require%20reasoning%20about%20nonlinearities%3F%20In%0Athis%20work%2C%20we%20show%20that%20a%20whole-body%20model-predictive%20controller%20using%20a%20simple%0Alinear%20time-invariant%20approximation%20of%20the%20whole-body%20dynamics%20is%20able%20to%0Aexecute%20basic%20locomotion%20tasks%20on%20complex%20legged%20robots.%20The%20formulation%0Arequires%20no%20online%20nonlinear%20dynamics%20evaluations%20or%20matrix%20inversions.%20We%0Ademonstrate%20walking%2C%20disturbance%20rejection%2C%20and%20even%20navigation%20to%20a%20goal%0Aposition%20without%20a%20separate%20footstep%20planner%20on%20a%20quadrupedal%20robot.%20In%0Aaddition%2C%20we%20demonstrate%20dynamic%20walking%20on%20a%20hydraulic%20humanoid%2C%20a%20robot%20with%0Asignificant%20limb%20inertia%2C%20complex%20actuator%20dynamics%2C%20and%20large%20sim-to-real%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Surprising%2520Effectiveness%2520of%2520Linear%2520Models%2520for%2520Whole-Body%250A%2520%2520Model-Predictive%2520Control%26entry.906535625%3DArun%2520L.%2520Bishop%2520and%2520Juan%2520Alvarez-Padilla%2520and%2520Sam%2520Schoedel%2520and%2520Ibrahima%2520Sory%2520Sow%2520and%2520Juee%2520Chandrachud%2520and%2520Sheitej%2520Sharma%2520and%2520Will%2520Kraus%2520and%2520Beomyeong%2520Park%2520and%2520Robert%2520J.%2520Griffin%2520and%2520John%2520M.%2520Dolan%2520and%2520Zachary%2520Manchester%26entry.1292438233%3D%2520%2520When%2520do%2520locomotion%2520controllers%2520require%2520reasoning%2520about%2520nonlinearities%253F%2520In%250Athis%2520work%252C%2520we%2520show%2520that%2520a%2520whole-body%2520model-predictive%2520controller%2520using%2520a%2520simple%250Alinear%2520time-invariant%2520approximation%2520of%2520the%2520whole-body%2520dynamics%2520is%2520able%2520to%250Aexecute%2520basic%2520locomotion%2520tasks%2520on%2520complex%2520legged%2520robots.%2520The%2520formulation%250Arequires%2520no%2520online%2520nonlinear%2520dynamics%2520evaluations%2520or%2520matrix%2520inversions.%2520We%250Ademonstrate%2520walking%252C%2520disturbance%2520rejection%252C%2520and%2520even%2520navigation%2520to%2520a%2520goal%250Aposition%2520without%2520a%2520separate%2520footstep%2520planner%2520on%2520a%2520quadrupedal%2520robot.%2520In%250Aaddition%252C%2520we%2520demonstrate%2520dynamic%2520walking%2520on%2520a%2520hydraulic%2520humanoid%252C%2520a%2520robot%2520with%250Asignificant%2520limb%2520inertia%252C%2520complex%2520actuator%2520dynamics%252C%2520and%2520large%2520sim-to-real%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Surprising%20Effectiveness%20of%20Linear%20Models%20for%20Whole-Body%0A%20%20Model-Predictive%20Control&entry.906535625=Arun%20L.%20Bishop%20and%20Juan%20Alvarez-Padilla%20and%20Sam%20Schoedel%20and%20Ibrahima%20Sory%20Sow%20and%20Juee%20Chandrachud%20and%20Sheitej%20Sharma%20and%20Will%20Kraus%20and%20Beomyeong%20Park%20and%20Robert%20J.%20Griffin%20and%20John%20M.%20Dolan%20and%20Zachary%20Manchester&entry.1292438233=%20%20When%20do%20locomotion%20controllers%20require%20reasoning%20about%20nonlinearities%3F%20In%0Athis%20work%2C%20we%20show%20that%20a%20whole-body%20model-predictive%20controller%20using%20a%20simple%0Alinear%20time-invariant%20approximation%20of%20the%20whole-body%20dynamics%20is%20able%20to%0Aexecute%20basic%20locomotion%20tasks%20on%20complex%20legged%20robots.%20The%20formulation%0Arequires%20no%20online%20nonlinear%20dynamics%20evaluations%20or%20matrix%20inversions.%20We%0Ademonstrate%20walking%2C%20disturbance%20rejection%2C%20and%20even%20navigation%20to%20a%20goal%0Aposition%20without%20a%20separate%20footstep%20planner%20on%20a%20quadrupedal%20robot.%20In%0Aaddition%2C%20we%20demonstrate%20dynamic%20walking%20on%20a%20hydraulic%20humanoid%2C%20a%20robot%20with%0Asignificant%20limb%20inertia%2C%20complex%20actuator%20dynamics%2C%20and%20large%20sim-to-real%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17884v1&entry.124074799=Read"},
{"title": "SEQR: Secure and Efficient QR-based LoRA Routing", "author": "William Fleshman and Benjamin Van Durme", "abstract": "  Low-Rank Adaptation (LoRA) has become a standard technique for\nparameter-efficient fine-tuning of large language models, enabling large\nlibraries of LoRAs, each for a specific task or domain. Efficiently selecting\nthe correct LoRA adapter for a given input remains a challenge, particularly in\nsecure environments where supervised training of routers may raise privacy\nconcerns. Motivated by previous approaches, we formalize the goal of\nunsupervised LoRA routing in terms of activation norm maximization, providing a\ntheoretical framework for analysis. We demonstrate the discriminative power of\nactivation norms and introduce SEQR, an unsupervised LoRA routing algorithm\ndesigned to maximize efficiency while providing strict routing guarantees. SEQR\nprovably identifies the norm-maximizing adapter with significantly greater\nefficiency, making it a highly scalable and effective solution for dynamic LoRA\ncomposition. We validate our results through experiments that demonstrate\nimproved multi-task performance and efficiency.\n", "link": "http://arxiv.org/abs/2509.18093v1", "date": "2025-09-22", "relevancy": 2.1942, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4338}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEQR%3A%20Secure%20and%20Efficient%20QR-based%20LoRA%20Routing&body=Title%3A%20SEQR%3A%20Secure%20and%20Efficient%20QR-based%20LoRA%20Routing%0AAuthor%3A%20William%20Fleshman%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20become%20a%20standard%20technique%20for%0Aparameter-efficient%20fine-tuning%20of%20large%20language%20models%2C%20enabling%20large%0Alibraries%20of%20LoRAs%2C%20each%20for%20a%20specific%20task%20or%20domain.%20Efficiently%20selecting%0Athe%20correct%20LoRA%20adapter%20for%20a%20given%20input%20remains%20a%20challenge%2C%20particularly%20in%0Asecure%20environments%20where%20supervised%20training%20of%20routers%20may%20raise%20privacy%0Aconcerns.%20Motivated%20by%20previous%20approaches%2C%20we%20formalize%20the%20goal%20of%0Aunsupervised%20LoRA%20routing%20in%20terms%20of%20activation%20norm%20maximization%2C%20providing%20a%0Atheoretical%20framework%20for%20analysis.%20We%20demonstrate%20the%20discriminative%20power%20of%0Aactivation%20norms%20and%20introduce%20SEQR%2C%20an%20unsupervised%20LoRA%20routing%20algorithm%0Adesigned%20to%20maximize%20efficiency%20while%20providing%20strict%20routing%20guarantees.%20SEQR%0Aprovably%20identifies%20the%20norm-maximizing%20adapter%20with%20significantly%20greater%0Aefficiency%2C%20making%20it%20a%20highly%20scalable%20and%20effective%20solution%20for%20dynamic%20LoRA%0Acomposition.%20We%20validate%20our%20results%20through%20experiments%20that%20demonstrate%0Aimproved%20multi-task%20performance%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEQR%253A%2520Secure%2520and%2520Efficient%2520QR-based%2520LoRA%2520Routing%26entry.906535625%3DWilliam%2520Fleshman%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520become%2520a%2520standard%2520technique%2520for%250Aparameter-efficient%2520fine-tuning%2520of%2520large%2520language%2520models%252C%2520enabling%2520large%250Alibraries%2520of%2520LoRAs%252C%2520each%2520for%2520a%2520specific%2520task%2520or%2520domain.%2520Efficiently%2520selecting%250Athe%2520correct%2520LoRA%2520adapter%2520for%2520a%2520given%2520input%2520remains%2520a%2520challenge%252C%2520particularly%2520in%250Asecure%2520environments%2520where%2520supervised%2520training%2520of%2520routers%2520may%2520raise%2520privacy%250Aconcerns.%2520Motivated%2520by%2520previous%2520approaches%252C%2520we%2520formalize%2520the%2520goal%2520of%250Aunsupervised%2520LoRA%2520routing%2520in%2520terms%2520of%2520activation%2520norm%2520maximization%252C%2520providing%2520a%250Atheoretical%2520framework%2520for%2520analysis.%2520We%2520demonstrate%2520the%2520discriminative%2520power%2520of%250Aactivation%2520norms%2520and%2520introduce%2520SEQR%252C%2520an%2520unsupervised%2520LoRA%2520routing%2520algorithm%250Adesigned%2520to%2520maximize%2520efficiency%2520while%2520providing%2520strict%2520routing%2520guarantees.%2520SEQR%250Aprovably%2520identifies%2520the%2520norm-maximizing%2520adapter%2520with%2520significantly%2520greater%250Aefficiency%252C%2520making%2520it%2520a%2520highly%2520scalable%2520and%2520effective%2520solution%2520for%2520dynamic%2520LoRA%250Acomposition.%2520We%2520validate%2520our%2520results%2520through%2520experiments%2520that%2520demonstrate%250Aimproved%2520multi-task%2520performance%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEQR%3A%20Secure%20and%20Efficient%20QR-based%20LoRA%20Routing&entry.906535625=William%20Fleshman%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20become%20a%20standard%20technique%20for%0Aparameter-efficient%20fine-tuning%20of%20large%20language%20models%2C%20enabling%20large%0Alibraries%20of%20LoRAs%2C%20each%20for%20a%20specific%20task%20or%20domain.%20Efficiently%20selecting%0Athe%20correct%20LoRA%20adapter%20for%20a%20given%20input%20remains%20a%20challenge%2C%20particularly%20in%0Asecure%20environments%20where%20supervised%20training%20of%20routers%20may%20raise%20privacy%0Aconcerns.%20Motivated%20by%20previous%20approaches%2C%20we%20formalize%20the%20goal%20of%0Aunsupervised%20LoRA%20routing%20in%20terms%20of%20activation%20norm%20maximization%2C%20providing%20a%0Atheoretical%20framework%20for%20analysis.%20We%20demonstrate%20the%20discriminative%20power%20of%0Aactivation%20norms%20and%20introduce%20SEQR%2C%20an%20unsupervised%20LoRA%20routing%20algorithm%0Adesigned%20to%20maximize%20efficiency%20while%20providing%20strict%20routing%20guarantees.%20SEQR%0Aprovably%20identifies%20the%20norm-maximizing%20adapter%20with%20significantly%20greater%0Aefficiency%2C%20making%20it%20a%20highly%20scalable%20and%20effective%20solution%20for%20dynamic%20LoRA%0Acomposition.%20We%20validate%20our%20results%20through%20experiments%20that%20demonstrate%0Aimproved%20multi-task%20performance%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18093v1&entry.124074799=Read"},
{"title": "Can multimodal representation learning by alignment preserve\n  modality-specific information?", "author": "Romain Thoreau and Jessie Levillain and Dawa Derksen", "abstract": "  Combining multimodal data is a key issue in a wide range of machine learning\ntasks, including many remote sensing problems. In Earth observation, early\nmultimodal data fusion methods were based on specific neural network\narchitectures and supervised learning. Ever since, the scarcity of labeled data\nhas motivated self-supervised learning techniques. State-of-the-art multimodal\nrepresentation learning techniques leverage the spatial alignment between\nsatellite data from different modalities acquired over the same geographic area\nin order to foster a semantic alignment in the latent space. In this paper, we\ninvestigate how this methods can preserve task-relevant information that is not\nshared across modalities. First, we show, under simplifying assumptions, when\nalignment strategies fundamentally lead to an information loss. Then, we\nsupport our theoretical insight through numerical experiments in more realistic\nsettings. With those theoretical and empirical evidences, we hope to support\nnew developments in contrastive learning for the combination of multimodal\nsatellite data. Our code and data is publicly available at\nhttps://github.com/Romain3Ch216/alg_maclean_25.\n", "link": "http://arxiv.org/abs/2509.17943v1", "date": "2025-09-22", "relevancy": 2.1937, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5798}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5546}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20multimodal%20representation%20learning%20by%20alignment%20preserve%0A%20%20modality-specific%20information%3F&body=Title%3A%20Can%20multimodal%20representation%20learning%20by%20alignment%20preserve%0A%20%20modality-specific%20information%3F%0AAuthor%3A%20Romain%20Thoreau%20and%20Jessie%20Levillain%20and%20Dawa%20Derksen%0AAbstract%3A%20%20%20Combining%20multimodal%20data%20is%20a%20key%20issue%20in%20a%20wide%20range%20of%20machine%20learning%0Atasks%2C%20including%20many%20remote%20sensing%20problems.%20In%20Earth%20observation%2C%20early%0Amultimodal%20data%20fusion%20methods%20were%20based%20on%20specific%20neural%20network%0Aarchitectures%20and%20supervised%20learning.%20Ever%20since%2C%20the%20scarcity%20of%20labeled%20data%0Ahas%20motivated%20self-supervised%20learning%20techniques.%20State-of-the-art%20multimodal%0Arepresentation%20learning%20techniques%20leverage%20the%20spatial%20alignment%20between%0Asatellite%20data%20from%20different%20modalities%20acquired%20over%20the%20same%20geographic%20area%0Ain%20order%20to%20foster%20a%20semantic%20alignment%20in%20the%20latent%20space.%20In%20this%20paper%2C%20we%0Ainvestigate%20how%20this%20methods%20can%20preserve%20task-relevant%20information%20that%20is%20not%0Ashared%20across%20modalities.%20First%2C%20we%20show%2C%20under%20simplifying%20assumptions%2C%20when%0Aalignment%20strategies%20fundamentally%20lead%20to%20an%20information%20loss.%20Then%2C%20we%0Asupport%20our%20theoretical%20insight%20through%20numerical%20experiments%20in%20more%20realistic%0Asettings.%20With%20those%20theoretical%20and%20empirical%20evidences%2C%20we%20hope%20to%20support%0Anew%20developments%20in%20contrastive%20learning%20for%20the%20combination%20of%20multimodal%0Asatellite%20data.%20Our%20code%20and%20data%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Romain3Ch216/alg_maclean_25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520multimodal%2520representation%2520learning%2520by%2520alignment%2520preserve%250A%2520%2520modality-specific%2520information%253F%26entry.906535625%3DRomain%2520Thoreau%2520and%2520Jessie%2520Levillain%2520and%2520Dawa%2520Derksen%26entry.1292438233%3D%2520%2520Combining%2520multimodal%2520data%2520is%2520a%2520key%2520issue%2520in%2520a%2520wide%2520range%2520of%2520machine%2520learning%250Atasks%252C%2520including%2520many%2520remote%2520sensing%2520problems.%2520In%2520Earth%2520observation%252C%2520early%250Amultimodal%2520data%2520fusion%2520methods%2520were%2520based%2520on%2520specific%2520neural%2520network%250Aarchitectures%2520and%2520supervised%2520learning.%2520Ever%2520since%252C%2520the%2520scarcity%2520of%2520labeled%2520data%250Ahas%2520motivated%2520self-supervised%2520learning%2520techniques.%2520State-of-the-art%2520multimodal%250Arepresentation%2520learning%2520techniques%2520leverage%2520the%2520spatial%2520alignment%2520between%250Asatellite%2520data%2520from%2520different%2520modalities%2520acquired%2520over%2520the%2520same%2520geographic%2520area%250Ain%2520order%2520to%2520foster%2520a%2520semantic%2520alignment%2520in%2520the%2520latent%2520space.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520how%2520this%2520methods%2520can%2520preserve%2520task-relevant%2520information%2520that%2520is%2520not%250Ashared%2520across%2520modalities.%2520First%252C%2520we%2520show%252C%2520under%2520simplifying%2520assumptions%252C%2520when%250Aalignment%2520strategies%2520fundamentally%2520lead%2520to%2520an%2520information%2520loss.%2520Then%252C%2520we%250Asupport%2520our%2520theoretical%2520insight%2520through%2520numerical%2520experiments%2520in%2520more%2520realistic%250Asettings.%2520With%2520those%2520theoretical%2520and%2520empirical%2520evidences%252C%2520we%2520hope%2520to%2520support%250Anew%2520developments%2520in%2520contrastive%2520learning%2520for%2520the%2520combination%2520of%2520multimodal%250Asatellite%2520data.%2520Our%2520code%2520and%2520data%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Romain3Ch216/alg_maclean_25.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20multimodal%20representation%20learning%20by%20alignment%20preserve%0A%20%20modality-specific%20information%3F&entry.906535625=Romain%20Thoreau%20and%20Jessie%20Levillain%20and%20Dawa%20Derksen&entry.1292438233=%20%20Combining%20multimodal%20data%20is%20a%20key%20issue%20in%20a%20wide%20range%20of%20machine%20learning%0Atasks%2C%20including%20many%20remote%20sensing%20problems.%20In%20Earth%20observation%2C%20early%0Amultimodal%20data%20fusion%20methods%20were%20based%20on%20specific%20neural%20network%0Aarchitectures%20and%20supervised%20learning.%20Ever%20since%2C%20the%20scarcity%20of%20labeled%20data%0Ahas%20motivated%20self-supervised%20learning%20techniques.%20State-of-the-art%20multimodal%0Arepresentation%20learning%20techniques%20leverage%20the%20spatial%20alignment%20between%0Asatellite%20data%20from%20different%20modalities%20acquired%20over%20the%20same%20geographic%20area%0Ain%20order%20to%20foster%20a%20semantic%20alignment%20in%20the%20latent%20space.%20In%20this%20paper%2C%20we%0Ainvestigate%20how%20this%20methods%20can%20preserve%20task-relevant%20information%20that%20is%20not%0Ashared%20across%20modalities.%20First%2C%20we%20show%2C%20under%20simplifying%20assumptions%2C%20when%0Aalignment%20strategies%20fundamentally%20lead%20to%20an%20information%20loss.%20Then%2C%20we%0Asupport%20our%20theoretical%20insight%20through%20numerical%20experiments%20in%20more%20realistic%0Asettings.%20With%20those%20theoretical%20and%20empirical%20evidences%2C%20we%20hope%20to%20support%0Anew%20developments%20in%20contrastive%20learning%20for%20the%20combination%20of%20multimodal%0Asatellite%20data.%20Our%20code%20and%20data%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Romain3Ch216/alg_maclean_25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17943v1&entry.124074799=Read"},
{"title": "Visual Detector Compression via Location-Aware Discriminant Analysis", "author": "Qizhen Lan and Jung Im Choi and Qing Tian", "abstract": "  Deep neural networks are powerful, yet their high complexity greatly limits\ntheir potential to be deployed on billions of resource-constrained edge\ndevices. Pruning is a crucial network compression technique, yet most existing\nmethods focus on classification models, with limited attention to detection.\nEven among those addressing detection, there is a lack of utilization of\nessential localization information. Also, many pruning methods passively rely\non pre-trained models, in which useful and useless components are intertwined,\nmaking it difficult to remove the latter without harming the former at the\nneuron/filter level. To address the above issues, in this paper, we propose a\nproactive detection-discriminants-based network compression approach for deep\nvisual detectors, which alternates between two steps: (1) maximizing and\ncompressing detection-related discriminants and aligning them with a subset of\nneurons/filters immediately before the detection head, and (2) tracing the\ndetection-related discriminating power across the layers and discarding\nfeatures of lower importance. Object location information is exploited in both\nsteps. Extensive experiments, employing four advanced detection models and four\nstate-of-the-art competing methods on the KITTI and COCO datasets, highlight\nthe superiority of our approach. Remarkably, our compressed models can even\nbeat the original base models with a substantial reduction in complexity.\n", "link": "http://arxiv.org/abs/2509.17968v1", "date": "2025-09-22", "relevancy": 2.1892, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5616}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5444}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Detector%20Compression%20via%20Location-Aware%20Discriminant%20Analysis&body=Title%3A%20Visual%20Detector%20Compression%20via%20Location-Aware%20Discriminant%20Analysis%0AAuthor%3A%20Qizhen%20Lan%20and%20Jung%20Im%20Choi%20and%20Qing%20Tian%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20powerful%2C%20yet%20their%20high%20complexity%20greatly%20limits%0Atheir%20potential%20to%20be%20deployed%20on%20billions%20of%20resource-constrained%20edge%0Adevices.%20Pruning%20is%20a%20crucial%20network%20compression%20technique%2C%20yet%20most%20existing%0Amethods%20focus%20on%20classification%20models%2C%20with%20limited%20attention%20to%20detection.%0AEven%20among%20those%20addressing%20detection%2C%20there%20is%20a%20lack%20of%20utilization%20of%0Aessential%20localization%20information.%20Also%2C%20many%20pruning%20methods%20passively%20rely%0Aon%20pre-trained%20models%2C%20in%20which%20useful%20and%20useless%20components%20are%20intertwined%2C%0Amaking%20it%20difficult%20to%20remove%20the%20latter%20without%20harming%20the%20former%20at%20the%0Aneuron/filter%20level.%20To%20address%20the%20above%20issues%2C%20in%20this%20paper%2C%20we%20propose%20a%0Aproactive%20detection-discriminants-based%20network%20compression%20approach%20for%20deep%0Avisual%20detectors%2C%20which%20alternates%20between%20two%20steps%3A%20%281%29%20maximizing%20and%0Acompressing%20detection-related%20discriminants%20and%20aligning%20them%20with%20a%20subset%20of%0Aneurons/filters%20immediately%20before%20the%20detection%20head%2C%20and%20%282%29%20tracing%20the%0Adetection-related%20discriminating%20power%20across%20the%20layers%20and%20discarding%0Afeatures%20of%20lower%20importance.%20Object%20location%20information%20is%20exploited%20in%20both%0Asteps.%20Extensive%20experiments%2C%20employing%20four%20advanced%20detection%20models%20and%20four%0Astate-of-the-art%20competing%20methods%20on%20the%20KITTI%20and%20COCO%20datasets%2C%20highlight%0Athe%20superiority%20of%20our%20approach.%20Remarkably%2C%20our%20compressed%20models%20can%20even%0Abeat%20the%20original%20base%20models%20with%20a%20substantial%20reduction%20in%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Detector%2520Compression%2520via%2520Location-Aware%2520Discriminant%2520Analysis%26entry.906535625%3DQizhen%2520Lan%2520and%2520Jung%2520Im%2520Choi%2520and%2520Qing%2520Tian%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520powerful%252C%2520yet%2520their%2520high%2520complexity%2520greatly%2520limits%250Atheir%2520potential%2520to%2520be%2520deployed%2520on%2520billions%2520of%2520resource-constrained%2520edge%250Adevices.%2520Pruning%2520is%2520a%2520crucial%2520network%2520compression%2520technique%252C%2520yet%2520most%2520existing%250Amethods%2520focus%2520on%2520classification%2520models%252C%2520with%2520limited%2520attention%2520to%2520detection.%250AEven%2520among%2520those%2520addressing%2520detection%252C%2520there%2520is%2520a%2520lack%2520of%2520utilization%2520of%250Aessential%2520localization%2520information.%2520Also%252C%2520many%2520pruning%2520methods%2520passively%2520rely%250Aon%2520pre-trained%2520models%252C%2520in%2520which%2520useful%2520and%2520useless%2520components%2520are%2520intertwined%252C%250Amaking%2520it%2520difficult%2520to%2520remove%2520the%2520latter%2520without%2520harming%2520the%2520former%2520at%2520the%250Aneuron/filter%2520level.%2520To%2520address%2520the%2520above%2520issues%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%250Aproactive%2520detection-discriminants-based%2520network%2520compression%2520approach%2520for%2520deep%250Avisual%2520detectors%252C%2520which%2520alternates%2520between%2520two%2520steps%253A%2520%25281%2529%2520maximizing%2520and%250Acompressing%2520detection-related%2520discriminants%2520and%2520aligning%2520them%2520with%2520a%2520subset%2520of%250Aneurons/filters%2520immediately%2520before%2520the%2520detection%2520head%252C%2520and%2520%25282%2529%2520tracing%2520the%250Adetection-related%2520discriminating%2520power%2520across%2520the%2520layers%2520and%2520discarding%250Afeatures%2520of%2520lower%2520importance.%2520Object%2520location%2520information%2520is%2520exploited%2520in%2520both%250Asteps.%2520Extensive%2520experiments%252C%2520employing%2520four%2520advanced%2520detection%2520models%2520and%2520four%250Astate-of-the-art%2520competing%2520methods%2520on%2520the%2520KITTI%2520and%2520COCO%2520datasets%252C%2520highlight%250Athe%2520superiority%2520of%2520our%2520approach.%2520Remarkably%252C%2520our%2520compressed%2520models%2520can%2520even%250Abeat%2520the%2520original%2520base%2520models%2520with%2520a%2520substantial%2520reduction%2520in%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Detector%20Compression%20via%20Location-Aware%20Discriminant%20Analysis&entry.906535625=Qizhen%20Lan%20and%20Jung%20Im%20Choi%20and%20Qing%20Tian&entry.1292438233=%20%20Deep%20neural%20networks%20are%20powerful%2C%20yet%20their%20high%20complexity%20greatly%20limits%0Atheir%20potential%20to%20be%20deployed%20on%20billions%20of%20resource-constrained%20edge%0Adevices.%20Pruning%20is%20a%20crucial%20network%20compression%20technique%2C%20yet%20most%20existing%0Amethods%20focus%20on%20classification%20models%2C%20with%20limited%20attention%20to%20detection.%0AEven%20among%20those%20addressing%20detection%2C%20there%20is%20a%20lack%20of%20utilization%20of%0Aessential%20localization%20information.%20Also%2C%20many%20pruning%20methods%20passively%20rely%0Aon%20pre-trained%20models%2C%20in%20which%20useful%20and%20useless%20components%20are%20intertwined%2C%0Amaking%20it%20difficult%20to%20remove%20the%20latter%20without%20harming%20the%20former%20at%20the%0Aneuron/filter%20level.%20To%20address%20the%20above%20issues%2C%20in%20this%20paper%2C%20we%20propose%20a%0Aproactive%20detection-discriminants-based%20network%20compression%20approach%20for%20deep%0Avisual%20detectors%2C%20which%20alternates%20between%20two%20steps%3A%20%281%29%20maximizing%20and%0Acompressing%20detection-related%20discriminants%20and%20aligning%20them%20with%20a%20subset%20of%0Aneurons/filters%20immediately%20before%20the%20detection%20head%2C%20and%20%282%29%20tracing%20the%0Adetection-related%20discriminating%20power%20across%20the%20layers%20and%20discarding%0Afeatures%20of%20lower%20importance.%20Object%20location%20information%20is%20exploited%20in%20both%0Asteps.%20Extensive%20experiments%2C%20employing%20four%20advanced%20detection%20models%20and%20four%0Astate-of-the-art%20competing%20methods%20on%20the%20KITTI%20and%20COCO%20datasets%2C%20highlight%0Athe%20superiority%20of%20our%20approach.%20Remarkably%2C%20our%20compressed%20models%20can%20even%0Abeat%20the%20original%20base%20models%20with%20a%20substantial%20reduction%20in%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17968v1&entry.124074799=Read"},
{"title": "COLA: Context-aware Language-driven Test-time Adaptation", "author": "Aiming Zhang and Tianyuan Yu and Liang Bai and Jun Tang and Yanming Guo and Yirun Ruan and Yun Zhou and Zhihe Lu", "abstract": "  Test-time adaptation (TTA) has gained increasing popularity due to its\nefficacy in addressing ``distribution shift'' issue while simultaneously\nprotecting data privacy.\n  However, most prior methods assume that a paired source domain model and\ntarget domain sharing the same label space coexist, heavily limiting their\napplicability.\n  In this paper, we investigate a more general source model capable of\nadaptation to multiple target domains without needing shared labels.\n  This is achieved by using a pre-trained vision-language model (VLM), \\egno,\nCLIP, that can recognize images through matching with class descriptions.\n  While the zero-shot performance of VLMs is impressive, they struggle to\neffectively capture the distinctive attributes of a target domain.\n  To that end, we propose a novel method -- Context-aware Language-driven TTA\n(COLA).\n  The proposed method incorporates a lightweight context-aware module that\nconsists of three key components: a task-aware adapter, a context-aware unit,\nand a residual connection unit for exploring task-specific knowledge,\ndomain-specific knowledge from the VLM and prior knowledge of the VLM,\nrespectively.\n  It is worth noting that the context-aware module can be seamlessly integrated\ninto a frozen VLM, ensuring both minimal effort and parameter efficiency.\n  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy\nto mitigate the adverse effects caused by class imbalance.\n  We demonstrate the effectiveness of our method not only in TTA scenarios but\nalso in class generalisation tasks.\n  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.\n", "link": "http://arxiv.org/abs/2509.17598v1", "date": "2025-09-22", "relevancy": 2.1881, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6028}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COLA%3A%20Context-aware%20Language-driven%20Test-time%20Adaptation&body=Title%3A%20COLA%3A%20Context-aware%20Language-driven%20Test-time%20Adaptation%0AAuthor%3A%20Aiming%20Zhang%20and%20Tianyuan%20Yu%20and%20Liang%20Bai%20and%20Jun%20Tang%20and%20Yanming%20Guo%20and%20Yirun%20Ruan%20and%20Yun%20Zhou%20and%20Zhihe%20Lu%0AAbstract%3A%20%20%20Test-time%20adaptation%20%28TTA%29%20has%20gained%20increasing%20popularity%20due%20to%20its%0Aefficacy%20in%20addressing%20%60%60distribution%20shift%27%27%20issue%20while%20simultaneously%0Aprotecting%20data%20privacy.%0A%20%20However%2C%20most%20prior%20methods%20assume%20that%20a%20paired%20source%20domain%20model%20and%0Atarget%20domain%20sharing%20the%20same%20label%20space%20coexist%2C%20heavily%20limiting%20their%0Aapplicability.%0A%20%20In%20this%20paper%2C%20we%20investigate%20a%20more%20general%20source%20model%20capable%20of%0Aadaptation%20to%20multiple%20target%20domains%20without%20needing%20shared%20labels.%0A%20%20This%20is%20achieved%20by%20using%20a%20pre-trained%20vision-language%20model%20%28VLM%29%2C%20%5Cegno%2C%0ACLIP%2C%20that%20can%20recognize%20images%20through%20matching%20with%20class%20descriptions.%0A%20%20While%20the%20zero-shot%20performance%20of%20VLMs%20is%20impressive%2C%20they%20struggle%20to%0Aeffectively%20capture%20the%20distinctive%20attributes%20of%20a%20target%20domain.%0A%20%20To%20that%20end%2C%20we%20propose%20a%20novel%20method%20--%20Context-aware%20Language-driven%20TTA%0A%28COLA%29.%0A%20%20The%20proposed%20method%20incorporates%20a%20lightweight%20context-aware%20module%20that%0Aconsists%20of%20three%20key%20components%3A%20a%20task-aware%20adapter%2C%20a%20context-aware%20unit%2C%0Aand%20a%20residual%20connection%20unit%20for%20exploring%20task-specific%20knowledge%2C%0Adomain-specific%20knowledge%20from%20the%20VLM%20and%20prior%20knowledge%20of%20the%20VLM%2C%0Arespectively.%0A%20%20It%20is%20worth%20noting%20that%20the%20context-aware%20module%20can%20be%20seamlessly%20integrated%0Ainto%20a%20frozen%20VLM%2C%20ensuring%20both%20minimal%20effort%20and%20parameter%20efficiency.%0A%20%20Additionally%2C%20we%20introduce%20a%20Class-Balanced%20Pseudo-labeling%20%28CBPL%29%20strategy%0Ato%20mitigate%20the%20adverse%20effects%20caused%20by%20class%20imbalance.%0A%20%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20not%20only%20in%20TTA%20scenarios%20but%0Aalso%20in%20class%20generalisation%20tasks.%0A%20%20The%20source%20code%20is%20available%20at%20https%3A//github.com/NUDT-Bai-Group/COLA-TTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOLA%253A%2520Context-aware%2520Language-driven%2520Test-time%2520Adaptation%26entry.906535625%3DAiming%2520Zhang%2520and%2520Tianyuan%2520Yu%2520and%2520Liang%2520Bai%2520and%2520Jun%2520Tang%2520and%2520Yanming%2520Guo%2520and%2520Yirun%2520Ruan%2520and%2520Yun%2520Zhou%2520and%2520Zhihe%2520Lu%26entry.1292438233%3D%2520%2520Test-time%2520adaptation%2520%2528TTA%2529%2520has%2520gained%2520increasing%2520popularity%2520due%2520to%2520its%250Aefficacy%2520in%2520addressing%2520%2560%2560distribution%2520shift%2527%2527%2520issue%2520while%2520simultaneously%250Aprotecting%2520data%2520privacy.%250A%2520%2520However%252C%2520most%2520prior%2520methods%2520assume%2520that%2520a%2520paired%2520source%2520domain%2520model%2520and%250Atarget%2520domain%2520sharing%2520the%2520same%2520label%2520space%2520coexist%252C%2520heavily%2520limiting%2520their%250Aapplicability.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520a%2520more%2520general%2520source%2520model%2520capable%2520of%250Aadaptation%2520to%2520multiple%2520target%2520domains%2520without%2520needing%2520shared%2520labels.%250A%2520%2520This%2520is%2520achieved%2520by%2520using%2520a%2520pre-trained%2520vision-language%2520model%2520%2528VLM%2529%252C%2520%255Cegno%252C%250ACLIP%252C%2520that%2520can%2520recognize%2520images%2520through%2520matching%2520with%2520class%2520descriptions.%250A%2520%2520While%2520the%2520zero-shot%2520performance%2520of%2520VLMs%2520is%2520impressive%252C%2520they%2520struggle%2520to%250Aeffectively%2520capture%2520the%2520distinctive%2520attributes%2520of%2520a%2520target%2520domain.%250A%2520%2520To%2520that%2520end%252C%2520we%2520propose%2520a%2520novel%2520method%2520--%2520Context-aware%2520Language-driven%2520TTA%250A%2528COLA%2529.%250A%2520%2520The%2520proposed%2520method%2520incorporates%2520a%2520lightweight%2520context-aware%2520module%2520that%250Aconsists%2520of%2520three%2520key%2520components%253A%2520a%2520task-aware%2520adapter%252C%2520a%2520context-aware%2520unit%252C%250Aand%2520a%2520residual%2520connection%2520unit%2520for%2520exploring%2520task-specific%2520knowledge%252C%250Adomain-specific%2520knowledge%2520from%2520the%2520VLM%2520and%2520prior%2520knowledge%2520of%2520the%2520VLM%252C%250Arespectively.%250A%2520%2520It%2520is%2520worth%2520noting%2520that%2520the%2520context-aware%2520module%2520can%2520be%2520seamlessly%2520integrated%250Ainto%2520a%2520frozen%2520VLM%252C%2520ensuring%2520both%2520minimal%2520effort%2520and%2520parameter%2520efficiency.%250A%2520%2520Additionally%252C%2520we%2520introduce%2520a%2520Class-Balanced%2520Pseudo-labeling%2520%2528CBPL%2529%2520strategy%250Ato%2520mitigate%2520the%2520adverse%2520effects%2520caused%2520by%2520class%2520imbalance.%250A%2520%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520not%2520only%2520in%2520TTA%2520scenarios%2520but%250Aalso%2520in%2520class%2520generalisation%2520tasks.%250A%2520%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/NUDT-Bai-Group/COLA-TTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COLA%3A%20Context-aware%20Language-driven%20Test-time%20Adaptation&entry.906535625=Aiming%20Zhang%20and%20Tianyuan%20Yu%20and%20Liang%20Bai%20and%20Jun%20Tang%20and%20Yanming%20Guo%20and%20Yirun%20Ruan%20and%20Yun%20Zhou%20and%20Zhihe%20Lu&entry.1292438233=%20%20Test-time%20adaptation%20%28TTA%29%20has%20gained%20increasing%20popularity%20due%20to%20its%0Aefficacy%20in%20addressing%20%60%60distribution%20shift%27%27%20issue%20while%20simultaneously%0Aprotecting%20data%20privacy.%0A%20%20However%2C%20most%20prior%20methods%20assume%20that%20a%20paired%20source%20domain%20model%20and%0Atarget%20domain%20sharing%20the%20same%20label%20space%20coexist%2C%20heavily%20limiting%20their%0Aapplicability.%0A%20%20In%20this%20paper%2C%20we%20investigate%20a%20more%20general%20source%20model%20capable%20of%0Aadaptation%20to%20multiple%20target%20domains%20without%20needing%20shared%20labels.%0A%20%20This%20is%20achieved%20by%20using%20a%20pre-trained%20vision-language%20model%20%28VLM%29%2C%20%5Cegno%2C%0ACLIP%2C%20that%20can%20recognize%20images%20through%20matching%20with%20class%20descriptions.%0A%20%20While%20the%20zero-shot%20performance%20of%20VLMs%20is%20impressive%2C%20they%20struggle%20to%0Aeffectively%20capture%20the%20distinctive%20attributes%20of%20a%20target%20domain.%0A%20%20To%20that%20end%2C%20we%20propose%20a%20novel%20method%20--%20Context-aware%20Language-driven%20TTA%0A%28COLA%29.%0A%20%20The%20proposed%20method%20incorporates%20a%20lightweight%20context-aware%20module%20that%0Aconsists%20of%20three%20key%20components%3A%20a%20task-aware%20adapter%2C%20a%20context-aware%20unit%2C%0Aand%20a%20residual%20connection%20unit%20for%20exploring%20task-specific%20knowledge%2C%0Adomain-specific%20knowledge%20from%20the%20VLM%20and%20prior%20knowledge%20of%20the%20VLM%2C%0Arespectively.%0A%20%20It%20is%20worth%20noting%20that%20the%20context-aware%20module%20can%20be%20seamlessly%20integrated%0Ainto%20a%20frozen%20VLM%2C%20ensuring%20both%20minimal%20effort%20and%20parameter%20efficiency.%0A%20%20Additionally%2C%20we%20introduce%20a%20Class-Balanced%20Pseudo-labeling%20%28CBPL%29%20strategy%0Ato%20mitigate%20the%20adverse%20effects%20caused%20by%20class%20imbalance.%0A%20%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20not%20only%20in%20TTA%20scenarios%20but%0Aalso%20in%20class%20generalisation%20tasks.%0A%20%20The%20source%20code%20is%20available%20at%20https%3A//github.com/NUDT-Bai-Group/COLA-TTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17598v1&entry.124074799=Read"},
{"title": "Trainee Action Recognition through Interaction Analysis in CCATT\n  Mixed-Reality Training", "author": "Divya Mereddy and Marcos Quinones-Grueiro and Ashwin T S and Eduardo Davalos and Gautam Biswas and Kent Etherton and Tyler Davis and Katelyn Kay and Jill Lear and Benjamin Goldberg", "abstract": "  This study examines how Critical Care Air Transport Team (CCATT) members are\ntrained using mixed-reality simulations that replicate the high-pressure\nconditions of aeromedical evacuation. Each team - a physician, nurse, and\nrespiratory therapist - must stabilize severely injured soldiers by managing\nventilators, IV pumps, and suction devices during flight. Proficient\nperformance requires clinical expertise and cognitive skills, such as\nsituational awareness, rapid decision-making, effective communication, and\ncoordinated task management, all of which must be maintained under stress.\nRecent advances in simulation and multimodal data analytics enable more\nobjective and comprehensive performance evaluation. In contrast, traditional\ninstructor-led assessments are subjective and may overlook critical events,\nthereby limiting generalizability and consistency. However, AI-based automated\nand more objective evaluation metrics still demand human input to train the AI\nalgorithms to assess complex team dynamics in the presence of environmental\nnoise and the need for accurate re-identification in multi-person tracking. To\naddress these challenges, we introduce a systematic, data-driven assessment\nframework that combines Cognitive Task Analysis (CTA) with Multimodal Learning\nAnalytics (MMLA). We have developed a domain-specific CTA model for CCATT\ntraining and a vision-based action recognition pipeline using a fine-tuned\nHuman-Object Interaction model, the Cascade Disentangling Network (CDN), to\ndetect and track trainee-equipment interactions over time. These interactions\nautomatically yield performance indicators (e.g., reaction time, task\nduration), which are mapped onto a hierarchical CTA model tailored to CCATT\noperations, enabling interpretable, domain-relevant performance evaluations.\n", "link": "http://arxiv.org/abs/2509.17888v1", "date": "2025-09-22", "relevancy": 2.1872, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5451}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trainee%20Action%20Recognition%20through%20Interaction%20Analysis%20in%20CCATT%0A%20%20Mixed-Reality%20Training&body=Title%3A%20Trainee%20Action%20Recognition%20through%20Interaction%20Analysis%20in%20CCATT%0A%20%20Mixed-Reality%20Training%0AAuthor%3A%20Divya%20Mereddy%20and%20Marcos%20Quinones-Grueiro%20and%20Ashwin%20T%20S%20and%20Eduardo%20Davalos%20and%20Gautam%20Biswas%20and%20Kent%20Etherton%20and%20Tyler%20Davis%20and%20Katelyn%20Kay%20and%20Jill%20Lear%20and%20Benjamin%20Goldberg%0AAbstract%3A%20%20%20This%20study%20examines%20how%20Critical%20Care%20Air%20Transport%20Team%20%28CCATT%29%20members%20are%0Atrained%20using%20mixed-reality%20simulations%20that%20replicate%20the%20high-pressure%0Aconditions%20of%20aeromedical%20evacuation.%20Each%20team%20-%20a%20physician%2C%20nurse%2C%20and%0Arespiratory%20therapist%20-%20must%20stabilize%20severely%20injured%20soldiers%20by%20managing%0Aventilators%2C%20IV%20pumps%2C%20and%20suction%20devices%20during%20flight.%20Proficient%0Aperformance%20requires%20clinical%20expertise%20and%20cognitive%20skills%2C%20such%20as%0Asituational%20awareness%2C%20rapid%20decision-making%2C%20effective%20communication%2C%20and%0Acoordinated%20task%20management%2C%20all%20of%20which%20must%20be%20maintained%20under%20stress.%0ARecent%20advances%20in%20simulation%20and%20multimodal%20data%20analytics%20enable%20more%0Aobjective%20and%20comprehensive%20performance%20evaluation.%20In%20contrast%2C%20traditional%0Ainstructor-led%20assessments%20are%20subjective%20and%20may%20overlook%20critical%20events%2C%0Athereby%20limiting%20generalizability%20and%20consistency.%20However%2C%20AI-based%20automated%0Aand%20more%20objective%20evaluation%20metrics%20still%20demand%20human%20input%20to%20train%20the%20AI%0Aalgorithms%20to%20assess%20complex%20team%20dynamics%20in%20the%20presence%20of%20environmental%0Anoise%20and%20the%20need%20for%20accurate%20re-identification%20in%20multi-person%20tracking.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20systematic%2C%20data-driven%20assessment%0Aframework%20that%20combines%20Cognitive%20Task%20Analysis%20%28CTA%29%20with%20Multimodal%20Learning%0AAnalytics%20%28MMLA%29.%20We%20have%20developed%20a%20domain-specific%20CTA%20model%20for%20CCATT%0Atraining%20and%20a%20vision-based%20action%20recognition%20pipeline%20using%20a%20fine-tuned%0AHuman-Object%20Interaction%20model%2C%20the%20Cascade%20Disentangling%20Network%20%28CDN%29%2C%20to%0Adetect%20and%20track%20trainee-equipment%20interactions%20over%20time.%20These%20interactions%0Aautomatically%20yield%20performance%20indicators%20%28e.g.%2C%20reaction%20time%2C%20task%0Aduration%29%2C%20which%20are%20mapped%20onto%20a%20hierarchical%20CTA%20model%20tailored%20to%20CCATT%0Aoperations%2C%20enabling%20interpretable%2C%20domain-relevant%20performance%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrainee%2520Action%2520Recognition%2520through%2520Interaction%2520Analysis%2520in%2520CCATT%250A%2520%2520Mixed-Reality%2520Training%26entry.906535625%3DDivya%2520Mereddy%2520and%2520Marcos%2520Quinones-Grueiro%2520and%2520Ashwin%2520T%2520S%2520and%2520Eduardo%2520Davalos%2520and%2520Gautam%2520Biswas%2520and%2520Kent%2520Etherton%2520and%2520Tyler%2520Davis%2520and%2520Katelyn%2520Kay%2520and%2520Jill%2520Lear%2520and%2520Benjamin%2520Goldberg%26entry.1292438233%3D%2520%2520This%2520study%2520examines%2520how%2520Critical%2520Care%2520Air%2520Transport%2520Team%2520%2528CCATT%2529%2520members%2520are%250Atrained%2520using%2520mixed-reality%2520simulations%2520that%2520replicate%2520the%2520high-pressure%250Aconditions%2520of%2520aeromedical%2520evacuation.%2520Each%2520team%2520-%2520a%2520physician%252C%2520nurse%252C%2520and%250Arespiratory%2520therapist%2520-%2520must%2520stabilize%2520severely%2520injured%2520soldiers%2520by%2520managing%250Aventilators%252C%2520IV%2520pumps%252C%2520and%2520suction%2520devices%2520during%2520flight.%2520Proficient%250Aperformance%2520requires%2520clinical%2520expertise%2520and%2520cognitive%2520skills%252C%2520such%2520as%250Asituational%2520awareness%252C%2520rapid%2520decision-making%252C%2520effective%2520communication%252C%2520and%250Acoordinated%2520task%2520management%252C%2520all%2520of%2520which%2520must%2520be%2520maintained%2520under%2520stress.%250ARecent%2520advances%2520in%2520simulation%2520and%2520multimodal%2520data%2520analytics%2520enable%2520more%250Aobjective%2520and%2520comprehensive%2520performance%2520evaluation.%2520In%2520contrast%252C%2520traditional%250Ainstructor-led%2520assessments%2520are%2520subjective%2520and%2520may%2520overlook%2520critical%2520events%252C%250Athereby%2520limiting%2520generalizability%2520and%2520consistency.%2520However%252C%2520AI-based%2520automated%250Aand%2520more%2520objective%2520evaluation%2520metrics%2520still%2520demand%2520human%2520input%2520to%2520train%2520the%2520AI%250Aalgorithms%2520to%2520assess%2520complex%2520team%2520dynamics%2520in%2520the%2520presence%2520of%2520environmental%250Anoise%2520and%2520the%2520need%2520for%2520accurate%2520re-identification%2520in%2520multi-person%2520tracking.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520systematic%252C%2520data-driven%2520assessment%250Aframework%2520that%2520combines%2520Cognitive%2520Task%2520Analysis%2520%2528CTA%2529%2520with%2520Multimodal%2520Learning%250AAnalytics%2520%2528MMLA%2529.%2520We%2520have%2520developed%2520a%2520domain-specific%2520CTA%2520model%2520for%2520CCATT%250Atraining%2520and%2520a%2520vision-based%2520action%2520recognition%2520pipeline%2520using%2520a%2520fine-tuned%250AHuman-Object%2520Interaction%2520model%252C%2520the%2520Cascade%2520Disentangling%2520Network%2520%2528CDN%2529%252C%2520to%250Adetect%2520and%2520track%2520trainee-equipment%2520interactions%2520over%2520time.%2520These%2520interactions%250Aautomatically%2520yield%2520performance%2520indicators%2520%2528e.g.%252C%2520reaction%2520time%252C%2520task%250Aduration%2529%252C%2520which%2520are%2520mapped%2520onto%2520a%2520hierarchical%2520CTA%2520model%2520tailored%2520to%2520CCATT%250Aoperations%252C%2520enabling%2520interpretable%252C%2520domain-relevant%2520performance%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trainee%20Action%20Recognition%20through%20Interaction%20Analysis%20in%20CCATT%0A%20%20Mixed-Reality%20Training&entry.906535625=Divya%20Mereddy%20and%20Marcos%20Quinones-Grueiro%20and%20Ashwin%20T%20S%20and%20Eduardo%20Davalos%20and%20Gautam%20Biswas%20and%20Kent%20Etherton%20and%20Tyler%20Davis%20and%20Katelyn%20Kay%20and%20Jill%20Lear%20and%20Benjamin%20Goldberg&entry.1292438233=%20%20This%20study%20examines%20how%20Critical%20Care%20Air%20Transport%20Team%20%28CCATT%29%20members%20are%0Atrained%20using%20mixed-reality%20simulations%20that%20replicate%20the%20high-pressure%0Aconditions%20of%20aeromedical%20evacuation.%20Each%20team%20-%20a%20physician%2C%20nurse%2C%20and%0Arespiratory%20therapist%20-%20must%20stabilize%20severely%20injured%20soldiers%20by%20managing%0Aventilators%2C%20IV%20pumps%2C%20and%20suction%20devices%20during%20flight.%20Proficient%0Aperformance%20requires%20clinical%20expertise%20and%20cognitive%20skills%2C%20such%20as%0Asituational%20awareness%2C%20rapid%20decision-making%2C%20effective%20communication%2C%20and%0Acoordinated%20task%20management%2C%20all%20of%20which%20must%20be%20maintained%20under%20stress.%0ARecent%20advances%20in%20simulation%20and%20multimodal%20data%20analytics%20enable%20more%0Aobjective%20and%20comprehensive%20performance%20evaluation.%20In%20contrast%2C%20traditional%0Ainstructor-led%20assessments%20are%20subjective%20and%20may%20overlook%20critical%20events%2C%0Athereby%20limiting%20generalizability%20and%20consistency.%20However%2C%20AI-based%20automated%0Aand%20more%20objective%20evaluation%20metrics%20still%20demand%20human%20input%20to%20train%20the%20AI%0Aalgorithms%20to%20assess%20complex%20team%20dynamics%20in%20the%20presence%20of%20environmental%0Anoise%20and%20the%20need%20for%20accurate%20re-identification%20in%20multi-person%20tracking.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20systematic%2C%20data-driven%20assessment%0Aframework%20that%20combines%20Cognitive%20Task%20Analysis%20%28CTA%29%20with%20Multimodal%20Learning%0AAnalytics%20%28MMLA%29.%20We%20have%20developed%20a%20domain-specific%20CTA%20model%20for%20CCATT%0Atraining%20and%20a%20vision-based%20action%20recognition%20pipeline%20using%20a%20fine-tuned%0AHuman-Object%20Interaction%20model%2C%20the%20Cascade%20Disentangling%20Network%20%28CDN%29%2C%20to%0Adetect%20and%20track%20trainee-equipment%20interactions%20over%20time.%20These%20interactions%0Aautomatically%20yield%20performance%20indicators%20%28e.g.%2C%20reaction%20time%2C%20task%0Aduration%29%2C%20which%20are%20mapped%20onto%20a%20hierarchical%20CTA%20model%20tailored%20to%20CCATT%0Aoperations%2C%20enabling%20interpretable%2C%20domain-relevant%20performance%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17888v1&entry.124074799=Read"},
{"title": "Kernel K-means clustering of distributional data", "author": "Amparo Ba\u00edllo and Jose R. Berrendero and Mart\u00edn S\u00e1nchez-Signorini", "abstract": "  We consider the problem of clustering a sample of probability distributions\nfrom a random distribution on $\\mathbb R^p$. Our proposed partitioning method\nmakes use of a symmetric, positive-definite kernel $k$ and its associated\nreproducing kernel Hilbert space (RKHS) $\\mathcal H$. By mapping each\ndistribution to its corresponding kernel mean embedding in $\\mathcal H$, we\nobtain a sample in this RKHS where we carry out the $K$-means clustering\nprocedure, which provides an unsupervised classification of the original\nsample. The procedure is simple and computationally feasible even for dimension\n$p>1$. The simulation studies provide insight into the choice of the kernel and\nits tuning parameter. The performance of the proposed clustering procedure is\nillustrated on a collection of Synthetic Aperture Radar (SAR) images.\n", "link": "http://arxiv.org/abs/2509.18037v1", "date": "2025-09-22", "relevancy": 1.636, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4177}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4099}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20K-means%20clustering%20of%20distributional%20data&body=Title%3A%20Kernel%20K-means%20clustering%20of%20distributional%20data%0AAuthor%3A%20Amparo%20Ba%C3%ADllo%20and%20Jose%20R.%20Berrendero%20and%20Mart%C3%ADn%20S%C3%A1nchez-Signorini%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20clustering%20a%20sample%20of%20probability%20distributions%0Afrom%20a%20random%20distribution%20on%20%24%5Cmathbb%20R%5Ep%24.%20Our%20proposed%20partitioning%20method%0Amakes%20use%20of%20a%20symmetric%2C%20positive-definite%20kernel%20%24k%24%20and%20its%20associated%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20%24%5Cmathcal%20H%24.%20By%20mapping%20each%0Adistribution%20to%20its%20corresponding%20kernel%20mean%20embedding%20in%20%24%5Cmathcal%20H%24%2C%20we%0Aobtain%20a%20sample%20in%20this%20RKHS%20where%20we%20carry%20out%20the%20%24K%24-means%20clustering%0Aprocedure%2C%20which%20provides%20an%20unsupervised%20classification%20of%20the%20original%0Asample.%20The%20procedure%20is%20simple%20and%20computationally%20feasible%20even%20for%20dimension%0A%24p%3E1%24.%20The%20simulation%20studies%20provide%20insight%20into%20the%20choice%20of%20the%20kernel%20and%0Aits%20tuning%20parameter.%20The%20performance%20of%20the%20proposed%20clustering%20procedure%20is%0Aillustrated%20on%20a%20collection%20of%20Synthetic%20Aperture%20Radar%20%28SAR%29%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520K-means%2520clustering%2520of%2520distributional%2520data%26entry.906535625%3DAmparo%2520Ba%25C3%25ADllo%2520and%2520Jose%2520R.%2520Berrendero%2520and%2520Mart%25C3%25ADn%2520S%25C3%25A1nchez-Signorini%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520clustering%2520a%2520sample%2520of%2520probability%2520distributions%250Afrom%2520a%2520random%2520distribution%2520on%2520%2524%255Cmathbb%2520R%255Ep%2524.%2520Our%2520proposed%2520partitioning%2520method%250Amakes%2520use%2520of%2520a%2520symmetric%252C%2520positive-definite%2520kernel%2520%2524k%2524%2520and%2520its%2520associated%250Areproducing%2520kernel%2520Hilbert%2520space%2520%2528RKHS%2529%2520%2524%255Cmathcal%2520H%2524.%2520By%2520mapping%2520each%250Adistribution%2520to%2520its%2520corresponding%2520kernel%2520mean%2520embedding%2520in%2520%2524%255Cmathcal%2520H%2524%252C%2520we%250Aobtain%2520a%2520sample%2520in%2520this%2520RKHS%2520where%2520we%2520carry%2520out%2520the%2520%2524K%2524-means%2520clustering%250Aprocedure%252C%2520which%2520provides%2520an%2520unsupervised%2520classification%2520of%2520the%2520original%250Asample.%2520The%2520procedure%2520is%2520simple%2520and%2520computationally%2520feasible%2520even%2520for%2520dimension%250A%2524p%253E1%2524.%2520The%2520simulation%2520studies%2520provide%2520insight%2520into%2520the%2520choice%2520of%2520the%2520kernel%2520and%250Aits%2520tuning%2520parameter.%2520The%2520performance%2520of%2520the%2520proposed%2520clustering%2520procedure%2520is%250Aillustrated%2520on%2520a%2520collection%2520of%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20K-means%20clustering%20of%20distributional%20data&entry.906535625=Amparo%20Ba%C3%ADllo%20and%20Jose%20R.%20Berrendero%20and%20Mart%C3%ADn%20S%C3%A1nchez-Signorini&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20clustering%20a%20sample%20of%20probability%20distributions%0Afrom%20a%20random%20distribution%20on%20%24%5Cmathbb%20R%5Ep%24.%20Our%20proposed%20partitioning%20method%0Amakes%20use%20of%20a%20symmetric%2C%20positive-definite%20kernel%20%24k%24%20and%20its%20associated%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20%24%5Cmathcal%20H%24.%20By%20mapping%20each%0Adistribution%20to%20its%20corresponding%20kernel%20mean%20embedding%20in%20%24%5Cmathcal%20H%24%2C%20we%0Aobtain%20a%20sample%20in%20this%20RKHS%20where%20we%20carry%20out%20the%20%24K%24-means%20clustering%0Aprocedure%2C%20which%20provides%20an%20unsupervised%20classification%20of%20the%20original%0Asample.%20The%20procedure%20is%20simple%20and%20computationally%20feasible%20even%20for%20dimension%0A%24p%3E1%24.%20The%20simulation%20studies%20provide%20insight%20into%20the%20choice%20of%20the%20kernel%20and%0Aits%20tuning%20parameter.%20The%20performance%20of%20the%20proposed%20clustering%20procedure%20is%0Aillustrated%20on%20a%20collection%20of%20Synthetic%20Aperture%20Radar%20%28SAR%29%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18037v1&entry.124074799=Read"},
{"title": "Medical priority fusion: achieving dual optimization of sensitivity and\n  interpretability in nipt anomaly detection", "author": "Xiuqi Ge and Zhibo Yao and Yaosong Du", "abstract": "  Clinical machine learning faces a critical dilemma in high-stakes medical\napplications: algorithms achieving optimal diagnostic performance typically\nsacrifice the interpretability essential for physician decision-making, while\ninterpretable methods compromise sensitivity in complex scenarios. This paradox\nbecomes particularly acute in non-invasive prenatal testing (NIPT), where\nmissed chromosomal abnormalities carry profound clinical consequences yet\nregulatory frameworks mandate explainable AI systems. We introduce Medical\nPriority Fusion (MPF), a constrained multi-objective optimization framework\nthat resolves this fundamental trade-off by systematically integrating Naive\nBayes probabilistic reasoning with Decision Tree rule-based logic through\nmathematically-principled weighted fusion under explicit medical constraints.\nRigorous validation on 1,687 real-world NIPT samples characterized by extreme\nclass imbalance (43.4:1 normal-to-abnormal ratio) employed stratified 5-fold\ncross-validation with comprehensive ablation studies and statistical hypothesis\ntesting using McNemar's paired comparisons. MPF achieved simultaneous\noptimization of dual objectives: 89.3% sensitivity (95% CI: 83.9-94.7%) with\n80% interpretability score, significantly outperforming individual algorithms\n(McNemar's test, p < 0.001). The optimal fusion configuration achieved Grade A\nclinical deployment criteria with large effect size (d = 1.24), establishing\nthe first clinically-deployable solution that maintains both diagnostic\naccuracy and decision transparency essential for prenatal care. This work\ndemonstrates that medical-constrained algorithm fusion can resolve the\ninterpretability-performance trade-off, providing a mathematical framework for\ndeveloping high-stakes medical decision support systems that meet both clinical\nefficacy and explainability requirements.\n", "link": "http://arxiv.org/abs/2509.17924v1", "date": "2025-09-22", "relevancy": 0.9468, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4891}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4657}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Medical%20priority%20fusion%3A%20achieving%20dual%20optimization%20of%20sensitivity%20and%0A%20%20interpretability%20in%20nipt%20anomaly%20detection&body=Title%3A%20Medical%20priority%20fusion%3A%20achieving%20dual%20optimization%20of%20sensitivity%20and%0A%20%20interpretability%20in%20nipt%20anomaly%20detection%0AAuthor%3A%20Xiuqi%20Ge%20and%20Zhibo%20Yao%20and%20Yaosong%20Du%0AAbstract%3A%20%20%20Clinical%20machine%20learning%20faces%20a%20critical%20dilemma%20in%20high-stakes%20medical%0Aapplications%3A%20algorithms%20achieving%20optimal%20diagnostic%20performance%20typically%0Asacrifice%20the%20interpretability%20essential%20for%20physician%20decision-making%2C%20while%0Ainterpretable%20methods%20compromise%20sensitivity%20in%20complex%20scenarios.%20This%20paradox%0Abecomes%20particularly%20acute%20in%20non-invasive%20prenatal%20testing%20%28NIPT%29%2C%20where%0Amissed%20chromosomal%20abnormalities%20carry%20profound%20clinical%20consequences%20yet%0Aregulatory%20frameworks%20mandate%20explainable%20AI%20systems.%20We%20introduce%20Medical%0APriority%20Fusion%20%28MPF%29%2C%20a%20constrained%20multi-objective%20optimization%20framework%0Athat%20resolves%20this%20fundamental%20trade-off%20by%20systematically%20integrating%20Naive%0ABayes%20probabilistic%20reasoning%20with%20Decision%20Tree%20rule-based%20logic%20through%0Amathematically-principled%20weighted%20fusion%20under%20explicit%20medical%20constraints.%0ARigorous%20validation%20on%201%2C687%20real-world%20NIPT%20samples%20characterized%20by%20extreme%0Aclass%20imbalance%20%2843.4%3A1%20normal-to-abnormal%20ratio%29%20employed%20stratified%205-fold%0Across-validation%20with%20comprehensive%20ablation%20studies%20and%20statistical%20hypothesis%0Atesting%20using%20McNemar%27s%20paired%20comparisons.%20MPF%20achieved%20simultaneous%0Aoptimization%20of%20dual%20objectives%3A%2089.3%25%20sensitivity%20%2895%25%20CI%3A%2083.9-94.7%25%29%20with%0A80%25%20interpretability%20score%2C%20significantly%20outperforming%20individual%20algorithms%0A%28McNemar%27s%20test%2C%20p%20%3C%200.001%29.%20The%20optimal%20fusion%20configuration%20achieved%20Grade%20A%0Aclinical%20deployment%20criteria%20with%20large%20effect%20size%20%28d%20%3D%201.24%29%2C%20establishing%0Athe%20first%20clinically-deployable%20solution%20that%20maintains%20both%20diagnostic%0Aaccuracy%20and%20decision%20transparency%20essential%20for%20prenatal%20care.%20This%20work%0Ademonstrates%20that%20medical-constrained%20algorithm%20fusion%20can%20resolve%20the%0Ainterpretability-performance%20trade-off%2C%20providing%20a%20mathematical%20framework%20for%0Adeveloping%20high-stakes%20medical%20decision%20support%20systems%20that%20meet%20both%20clinical%0Aefficacy%20and%20explainability%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedical%2520priority%2520fusion%253A%2520achieving%2520dual%2520optimization%2520of%2520sensitivity%2520and%250A%2520%2520interpretability%2520in%2520nipt%2520anomaly%2520detection%26entry.906535625%3DXiuqi%2520Ge%2520and%2520Zhibo%2520Yao%2520and%2520Yaosong%2520Du%26entry.1292438233%3D%2520%2520Clinical%2520machine%2520learning%2520faces%2520a%2520critical%2520dilemma%2520in%2520high-stakes%2520medical%250Aapplications%253A%2520algorithms%2520achieving%2520optimal%2520diagnostic%2520performance%2520typically%250Asacrifice%2520the%2520interpretability%2520essential%2520for%2520physician%2520decision-making%252C%2520while%250Ainterpretable%2520methods%2520compromise%2520sensitivity%2520in%2520complex%2520scenarios.%2520This%2520paradox%250Abecomes%2520particularly%2520acute%2520in%2520non-invasive%2520prenatal%2520testing%2520%2528NIPT%2529%252C%2520where%250Amissed%2520chromosomal%2520abnormalities%2520carry%2520profound%2520clinical%2520consequences%2520yet%250Aregulatory%2520frameworks%2520mandate%2520explainable%2520AI%2520systems.%2520We%2520introduce%2520Medical%250APriority%2520Fusion%2520%2528MPF%2529%252C%2520a%2520constrained%2520multi-objective%2520optimization%2520framework%250Athat%2520resolves%2520this%2520fundamental%2520trade-off%2520by%2520systematically%2520integrating%2520Naive%250ABayes%2520probabilistic%2520reasoning%2520with%2520Decision%2520Tree%2520rule-based%2520logic%2520through%250Amathematically-principled%2520weighted%2520fusion%2520under%2520explicit%2520medical%2520constraints.%250ARigorous%2520validation%2520on%25201%252C687%2520real-world%2520NIPT%2520samples%2520characterized%2520by%2520extreme%250Aclass%2520imbalance%2520%252843.4%253A1%2520normal-to-abnormal%2520ratio%2529%2520employed%2520stratified%25205-fold%250Across-validation%2520with%2520comprehensive%2520ablation%2520studies%2520and%2520statistical%2520hypothesis%250Atesting%2520using%2520McNemar%2527s%2520paired%2520comparisons.%2520MPF%2520achieved%2520simultaneous%250Aoptimization%2520of%2520dual%2520objectives%253A%252089.3%2525%2520sensitivity%2520%252895%2525%2520CI%253A%252083.9-94.7%2525%2529%2520with%250A80%2525%2520interpretability%2520score%252C%2520significantly%2520outperforming%2520individual%2520algorithms%250A%2528McNemar%2527s%2520test%252C%2520p%2520%253C%25200.001%2529.%2520The%2520optimal%2520fusion%2520configuration%2520achieved%2520Grade%2520A%250Aclinical%2520deployment%2520criteria%2520with%2520large%2520effect%2520size%2520%2528d%2520%253D%25201.24%2529%252C%2520establishing%250Athe%2520first%2520clinically-deployable%2520solution%2520that%2520maintains%2520both%2520diagnostic%250Aaccuracy%2520and%2520decision%2520transparency%2520essential%2520for%2520prenatal%2520care.%2520This%2520work%250Ademonstrates%2520that%2520medical-constrained%2520algorithm%2520fusion%2520can%2520resolve%2520the%250Ainterpretability-performance%2520trade-off%252C%2520providing%2520a%2520mathematical%2520framework%2520for%250Adeveloping%2520high-stakes%2520medical%2520decision%2520support%2520systems%2520that%2520meet%2520both%2520clinical%250Aefficacy%2520and%2520explainability%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medical%20priority%20fusion%3A%20achieving%20dual%20optimization%20of%20sensitivity%20and%0A%20%20interpretability%20in%20nipt%20anomaly%20detection&entry.906535625=Xiuqi%20Ge%20and%20Zhibo%20Yao%20and%20Yaosong%20Du&entry.1292438233=%20%20Clinical%20machine%20learning%20faces%20a%20critical%20dilemma%20in%20high-stakes%20medical%0Aapplications%3A%20algorithms%20achieving%20optimal%20diagnostic%20performance%20typically%0Asacrifice%20the%20interpretability%20essential%20for%20physician%20decision-making%2C%20while%0Ainterpretable%20methods%20compromise%20sensitivity%20in%20complex%20scenarios.%20This%20paradox%0Abecomes%20particularly%20acute%20in%20non-invasive%20prenatal%20testing%20%28NIPT%29%2C%20where%0Amissed%20chromosomal%20abnormalities%20carry%20profound%20clinical%20consequences%20yet%0Aregulatory%20frameworks%20mandate%20explainable%20AI%20systems.%20We%20introduce%20Medical%0APriority%20Fusion%20%28MPF%29%2C%20a%20constrained%20multi-objective%20optimization%20framework%0Athat%20resolves%20this%20fundamental%20trade-off%20by%20systematically%20integrating%20Naive%0ABayes%20probabilistic%20reasoning%20with%20Decision%20Tree%20rule-based%20logic%20through%0Amathematically-principled%20weighted%20fusion%20under%20explicit%20medical%20constraints.%0ARigorous%20validation%20on%201%2C687%20real-world%20NIPT%20samples%20characterized%20by%20extreme%0Aclass%20imbalance%20%2843.4%3A1%20normal-to-abnormal%20ratio%29%20employed%20stratified%205-fold%0Across-validation%20with%20comprehensive%20ablation%20studies%20and%20statistical%20hypothesis%0Atesting%20using%20McNemar%27s%20paired%20comparisons.%20MPF%20achieved%20simultaneous%0Aoptimization%20of%20dual%20objectives%3A%2089.3%25%20sensitivity%20%2895%25%20CI%3A%2083.9-94.7%25%29%20with%0A80%25%20interpretability%20score%2C%20significantly%20outperforming%20individual%20algorithms%0A%28McNemar%27s%20test%2C%20p%20%3C%200.001%29.%20The%20optimal%20fusion%20configuration%20achieved%20Grade%20A%0Aclinical%20deployment%20criteria%20with%20large%20effect%20size%20%28d%20%3D%201.24%29%2C%20establishing%0Athe%20first%20clinically-deployable%20solution%20that%20maintains%20both%20diagnostic%0Aaccuracy%20and%20decision%20transparency%20essential%20for%20prenatal%20care.%20This%20work%0Ademonstrates%20that%20medical-constrained%20algorithm%20fusion%20can%20resolve%20the%0Ainterpretability-performance%20trade-off%2C%20providing%20a%20mathematical%20framework%20for%0Adeveloping%20high-stakes%20medical%20decision%20support%20systems%20that%20meet%20both%20clinical%0Aefficacy%20and%20explainability%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17924v1&entry.124074799=Read"},
{"title": "M3ET: Efficient Vision-Language Learning for Robotics based on\n  Multimodal Mamba-Enhanced Transformer", "author": "Yanxin Zhang and Liang He and Zeyi Kang and Zuheng Ming and Kaixing Zhao", "abstract": "  In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms.\n", "link": "http://arxiv.org/abs/2509.18005v1", "date": "2025-09-22", "relevancy": 1.7652, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5968}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5941}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3ET%3A%20Efficient%20Vision-Language%20Learning%20for%20Robotics%20based%20on%0A%20%20Multimodal%20Mamba-Enhanced%20Transformer&body=Title%3A%20M3ET%3A%20Efficient%20Vision-Language%20Learning%20for%20Robotics%20based%20on%0A%20%20Multimodal%20Mamba-Enhanced%20Transformer%0AAuthor%3A%20Yanxin%20Zhang%20and%20Liang%20He%20and%20Zeyi%20Kang%20and%20Zuheng%20Ming%20and%20Kaixing%20Zhao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20multimodal%20learning%20has%20become%20essential%20in%20robotic%20vision%0Aand%20information%20fusion%2C%20especially%20for%20understanding%20human%20behavior%20in%20complex%0Aenvironments.%20However%2C%20current%20methods%20struggle%20to%20fully%20leverage%20the%20textual%0Amodality%2C%20relying%20on%20supervised%20pretrained%20models%2C%20which%20limits%20semantic%0Aextraction%20in%20unsupervised%20robotic%20environments%2C%20particularly%20with%20significant%0Amodality%20loss.%20These%20methods%20also%20tend%20to%20be%20computationally%20intensive%2C%20leading%0Ato%20high%20resource%20consumption%20in%20real-world%20applications.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Multi%20Modal%20Mamba%20Enhanced%20Transformer%20%28M3ET%29%2C%20a%0Alightweight%20model%20designed%20for%20efficient%20multimodal%20learning%2C%20particularly%20on%0Amobile%20platforms.%20By%20incorporating%20the%20Mamba%20module%20and%20a%20semantic-based%0Aadaptive%20attention%20mechanism%2C%20M3ET%20optimizes%20feature%20fusion%2C%20alignment%2C%20and%0Amodality%20reconstruction.%20Our%20experiments%20show%20that%20M3ET%20improves%20cross-task%0Aperformance%2C%20with%20a%202.3%20times%20increase%20in%20pretraining%20inference%20speed.%20In%0Aparticular%2C%20the%20core%20VQA%20task%20accuracy%20of%20M3ET%20remains%20at%200.74%2C%20while%20the%0Amodel%27s%20parameter%20count%20is%20reduced%20by%200.67.%20Although%20performance%20on%20the%20EQA%0Atask%20is%20limited%2C%20M3ET%27s%20lightweight%20design%20makes%20it%20well%20suited%20for%20deployment%0Aon%20resource-constrained%20robotic%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3ET%253A%2520Efficient%2520Vision-Language%2520Learning%2520for%2520Robotics%2520based%2520on%250A%2520%2520Multimodal%2520Mamba-Enhanced%2520Transformer%26entry.906535625%3DYanxin%2520Zhang%2520and%2520Liang%2520He%2520and%2520Zeyi%2520Kang%2520and%2520Zuheng%2520Ming%2520and%2520Kaixing%2520Zhao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520multimodal%2520learning%2520has%2520become%2520essential%2520in%2520robotic%2520vision%250Aand%2520information%2520fusion%252C%2520especially%2520for%2520understanding%2520human%2520behavior%2520in%2520complex%250Aenvironments.%2520However%252C%2520current%2520methods%2520struggle%2520to%2520fully%2520leverage%2520the%2520textual%250Amodality%252C%2520relying%2520on%2520supervised%2520pretrained%2520models%252C%2520which%2520limits%2520semantic%250Aextraction%2520in%2520unsupervised%2520robotic%2520environments%252C%2520particularly%2520with%2520significant%250Amodality%2520loss.%2520These%2520methods%2520also%2520tend%2520to%2520be%2520computationally%2520intensive%252C%2520leading%250Ato%2520high%2520resource%2520consumption%2520in%2520real-world%2520applications.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520the%2520Multi%2520Modal%2520Mamba%2520Enhanced%2520Transformer%2520%2528M3ET%2529%252C%2520a%250Alightweight%2520model%2520designed%2520for%2520efficient%2520multimodal%2520learning%252C%2520particularly%2520on%250Amobile%2520platforms.%2520By%2520incorporating%2520the%2520Mamba%2520module%2520and%2520a%2520semantic-based%250Aadaptive%2520attention%2520mechanism%252C%2520M3ET%2520optimizes%2520feature%2520fusion%252C%2520alignment%252C%2520and%250Amodality%2520reconstruction.%2520Our%2520experiments%2520show%2520that%2520M3ET%2520improves%2520cross-task%250Aperformance%252C%2520with%2520a%25202.3%2520times%2520increase%2520in%2520pretraining%2520inference%2520speed.%2520In%250Aparticular%252C%2520the%2520core%2520VQA%2520task%2520accuracy%2520of%2520M3ET%2520remains%2520at%25200.74%252C%2520while%2520the%250Amodel%2527s%2520parameter%2520count%2520is%2520reduced%2520by%25200.67.%2520Although%2520performance%2520on%2520the%2520EQA%250Atask%2520is%2520limited%252C%2520M3ET%2527s%2520lightweight%2520design%2520makes%2520it%2520well%2520suited%2520for%2520deployment%250Aon%2520resource-constrained%2520robotic%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3ET%3A%20Efficient%20Vision-Language%20Learning%20for%20Robotics%20based%20on%0A%20%20Multimodal%20Mamba-Enhanced%20Transformer&entry.906535625=Yanxin%20Zhang%20and%20Liang%20He%20and%20Zeyi%20Kang%20and%20Zuheng%20Ming%20and%20Kaixing%20Zhao&entry.1292438233=%20%20In%20recent%20years%2C%20multimodal%20learning%20has%20become%20essential%20in%20robotic%20vision%0Aand%20information%20fusion%2C%20especially%20for%20understanding%20human%20behavior%20in%20complex%0Aenvironments.%20However%2C%20current%20methods%20struggle%20to%20fully%20leverage%20the%20textual%0Amodality%2C%20relying%20on%20supervised%20pretrained%20models%2C%20which%20limits%20semantic%0Aextraction%20in%20unsupervised%20robotic%20environments%2C%20particularly%20with%20significant%0Amodality%20loss.%20These%20methods%20also%20tend%20to%20be%20computationally%20intensive%2C%20leading%0Ato%20high%20resource%20consumption%20in%20real-world%20applications.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Multi%20Modal%20Mamba%20Enhanced%20Transformer%20%28M3ET%29%2C%20a%0Alightweight%20model%20designed%20for%20efficient%20multimodal%20learning%2C%20particularly%20on%0Amobile%20platforms.%20By%20incorporating%20the%20Mamba%20module%20and%20a%20semantic-based%0Aadaptive%20attention%20mechanism%2C%20M3ET%20optimizes%20feature%20fusion%2C%20alignment%2C%20and%0Amodality%20reconstruction.%20Our%20experiments%20show%20that%20M3ET%20improves%20cross-task%0Aperformance%2C%20with%20a%202.3%20times%20increase%20in%20pretraining%20inference%20speed.%20In%0Aparticular%2C%20the%20core%20VQA%20task%20accuracy%20of%20M3ET%20remains%20at%200.74%2C%20while%20the%0Amodel%27s%20parameter%20count%20is%20reduced%20by%200.67.%20Although%20performance%20on%20the%20EQA%0Atask%20is%20limited%2C%20M3ET%27s%20lightweight%20design%20makes%20it%20well%20suited%20for%20deployment%0Aon%20resource-constrained%20robotic%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18005v1&entry.124074799=Read"},
{"title": "Revealing Multimodal Causality with Large Language Models", "author": "Jin Li and Shoujin Wang and Qi Zhang and Feng Liu and Tongliang Liu and Longbing Cao and Shui Yu and Fang Chen", "abstract": "  Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data.\n", "link": "http://arxiv.org/abs/2509.17784v1", "date": "2025-09-22", "relevancy": 2.0804, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20Multimodal%20Causality%20with%20Large%20Language%20Models&body=Title%3A%20Revealing%20Multimodal%20Causality%20with%20Large%20Language%20Models%0AAuthor%3A%20Jin%20Li%20and%20Shoujin%20Wang%20and%20Qi%20Zhang%20and%20Feng%20Liu%20and%20Tongliang%20Liu%20and%20Longbing%20Cao%20and%20Shui%20Yu%20and%20Fang%20Chen%0AAbstract%3A%20%20%20Uncovering%20cause-and-effect%20mechanisms%20from%20data%20is%20fundamental%20to%20scientific%0Aprogress.%20While%20large%20language%20models%20%28LLMs%29%20show%20promise%20for%20enhancing%20causal%0Adiscovery%20%28CD%29%20from%20unstructured%20data%2C%20their%20application%20to%20the%20increasingly%0Aprevalent%20multimodal%20setting%20remains%20a%20critical%20challenge.%20Even%20with%20the%20advent%0Aof%20multimodal%20LLMs%20%28MLLMs%29%2C%20their%20efficacy%20in%20multimodal%20CD%20is%20hindered%20by%20two%0Aprimary%20limitations%3A%20%281%29%20difficulty%20in%20exploring%20intra-%20and%20inter-modal%0Ainteractions%20for%20comprehensive%20causal%20variable%20identification%3B%20and%20%282%29%0Ainsufficiency%20to%20handle%20structural%20ambiguities%20with%20purely%20observational%20data.%0ATo%20address%20these%20challenges%2C%20we%20propose%20MLLM-CD%2C%20a%20novel%20framework%20for%0Amultimodal%20causal%20discovery%20from%20unstructured%20data.%20It%20consists%20of%20three%20key%0Acomponents%3A%20%281%29%20a%20novel%20contrastive%20factor%20discovery%20module%20to%20identify%20genuine%0Amultimodal%20factors%20based%20on%20the%20interactions%20explored%20from%20contrastive%20sample%0Apairs%3B%20%282%29%20a%20statistical%20causal%20structure%20discovery%20module%20to%20infer%20causal%0Arelationships%20among%20discovered%20factors%3B%20and%20%283%29%20an%20iterative%20multimodal%0Acounterfactual%20reasoning%20module%20to%20refine%20the%20discovery%20outcomes%20iteratively%20by%0Aincorporating%20the%20world%20knowledge%20and%20reasoning%20capabilities%20of%20MLLMs.%0AExtensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20MLLM-CD%20in%20revealing%20genuine%20factors%20and%20causal%20relationships%0Aamong%20them%20from%20multimodal%20unstructured%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520Multimodal%2520Causality%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DJin%2520Li%2520and%2520Shoujin%2520Wang%2520and%2520Qi%2520Zhang%2520and%2520Feng%2520Liu%2520and%2520Tongliang%2520Liu%2520and%2520Longbing%2520Cao%2520and%2520Shui%2520Yu%2520and%2520Fang%2520Chen%26entry.1292438233%3D%2520%2520Uncovering%2520cause-and-effect%2520mechanisms%2520from%2520data%2520is%2520fundamental%2520to%2520scientific%250Aprogress.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%2520promise%2520for%2520enhancing%2520causal%250Adiscovery%2520%2528CD%2529%2520from%2520unstructured%2520data%252C%2520their%2520application%2520to%2520the%2520increasingly%250Aprevalent%2520multimodal%2520setting%2520remains%2520a%2520critical%2520challenge.%2520Even%2520with%2520the%2520advent%250Aof%2520multimodal%2520LLMs%2520%2528MLLMs%2529%252C%2520their%2520efficacy%2520in%2520multimodal%2520CD%2520is%2520hindered%2520by%2520two%250Aprimary%2520limitations%253A%2520%25281%2529%2520difficulty%2520in%2520exploring%2520intra-%2520and%2520inter-modal%250Ainteractions%2520for%2520comprehensive%2520causal%2520variable%2520identification%253B%2520and%2520%25282%2529%250Ainsufficiency%2520to%2520handle%2520structural%2520ambiguities%2520with%2520purely%2520observational%2520data.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MLLM-CD%252C%2520a%2520novel%2520framework%2520for%250Amultimodal%2520causal%2520discovery%2520from%2520unstructured%2520data.%2520It%2520consists%2520of%2520three%2520key%250Acomponents%253A%2520%25281%2529%2520a%2520novel%2520contrastive%2520factor%2520discovery%2520module%2520to%2520identify%2520genuine%250Amultimodal%2520factors%2520based%2520on%2520the%2520interactions%2520explored%2520from%2520contrastive%2520sample%250Apairs%253B%2520%25282%2529%2520a%2520statistical%2520causal%2520structure%2520discovery%2520module%2520to%2520infer%2520causal%250Arelationships%2520among%2520discovered%2520factors%253B%2520and%2520%25283%2529%2520an%2520iterative%2520multimodal%250Acounterfactual%2520reasoning%2520module%2520to%2520refine%2520the%2520discovery%2520outcomes%2520iteratively%2520by%250Aincorporating%2520the%2520world%2520knowledge%2520and%2520reasoning%2520capabilities%2520of%2520MLLMs.%250AExtensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520MLLM-CD%2520in%2520revealing%2520genuine%2520factors%2520and%2520causal%2520relationships%250Aamong%2520them%2520from%2520multimodal%2520unstructured%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20Multimodal%20Causality%20with%20Large%20Language%20Models&entry.906535625=Jin%20Li%20and%20Shoujin%20Wang%20and%20Qi%20Zhang%20and%20Feng%20Liu%20and%20Tongliang%20Liu%20and%20Longbing%20Cao%20and%20Shui%20Yu%20and%20Fang%20Chen&entry.1292438233=%20%20Uncovering%20cause-and-effect%20mechanisms%20from%20data%20is%20fundamental%20to%20scientific%0Aprogress.%20While%20large%20language%20models%20%28LLMs%29%20show%20promise%20for%20enhancing%20causal%0Adiscovery%20%28CD%29%20from%20unstructured%20data%2C%20their%20application%20to%20the%20increasingly%0Aprevalent%20multimodal%20setting%20remains%20a%20critical%20challenge.%20Even%20with%20the%20advent%0Aof%20multimodal%20LLMs%20%28MLLMs%29%2C%20their%20efficacy%20in%20multimodal%20CD%20is%20hindered%20by%20two%0Aprimary%20limitations%3A%20%281%29%20difficulty%20in%20exploring%20intra-%20and%20inter-modal%0Ainteractions%20for%20comprehensive%20causal%20variable%20identification%3B%20and%20%282%29%0Ainsufficiency%20to%20handle%20structural%20ambiguities%20with%20purely%20observational%20data.%0ATo%20address%20these%20challenges%2C%20we%20propose%20MLLM-CD%2C%20a%20novel%20framework%20for%0Amultimodal%20causal%20discovery%20from%20unstructured%20data.%20It%20consists%20of%20three%20key%0Acomponents%3A%20%281%29%20a%20novel%20contrastive%20factor%20discovery%20module%20to%20identify%20genuine%0Amultimodal%20factors%20based%20on%20the%20interactions%20explored%20from%20contrastive%20sample%0Apairs%3B%20%282%29%20a%20statistical%20causal%20structure%20discovery%20module%20to%20infer%20causal%0Arelationships%20among%20discovered%20factors%3B%20and%20%283%29%20an%20iterative%20multimodal%0Acounterfactual%20reasoning%20module%20to%20refine%20the%20discovery%20outcomes%20iteratively%20by%0Aincorporating%20the%20world%20knowledge%20and%20reasoning%20capabilities%20of%20MLLMs.%0AExtensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20MLLM-CD%20in%20revealing%20genuine%20factors%20and%20causal%20relationships%0Aamong%20them%20from%20multimodal%20unstructured%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17784v1&entry.124074799=Read"},
{"title": "Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale\n  Representation for Variable-Length Long Time Series", "author": "Kai Zhang and Siming Sun and Zhengyu Fan and Qinmin Yang and Xuejun Jiang", "abstract": "  Time series analysis faces significant challenges in handling variable-length\ndata and achieving robust generalization. While Transformer-based models have\nadvanced time series tasks, they often struggle with feature redundancy and\nlimited generalization capabilities. Drawing inspiration from classical CNN\narchitectures' pyramidal structure, we propose a Multi-Scale Representation\nLearning Framework based on a Conv-like ScaleFusion Transformer. Our approach\nintroduces a temporal convolution-like structure that combines patching\noperations with multi-head attention, enabling progressive temporal dimension\ncompression and feature channel expansion. We further develop a novel\ncross-scale attention mechanism for effective feature fusion across different\ntemporal scales, along with a log-space normalization method for\nvariable-length sequences. Extensive experiments demonstrate that our framework\nachieves superior feature independence, reduced redundancy, and better\nperformance in forecasting and classification tasks compared to\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2509.17845v1", "date": "2025-09-22", "relevancy": 1.5714, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6117}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conv-like%20Scale-Fusion%20Time%20Series%20Transformer%3A%20A%20Multi-Scale%0A%20%20Representation%20for%20Variable-Length%20Long%20Time%20Series&body=Title%3A%20Conv-like%20Scale-Fusion%20Time%20Series%20Transformer%3A%20A%20Multi-Scale%0A%20%20Representation%20for%20Variable-Length%20Long%20Time%20Series%0AAuthor%3A%20Kai%20Zhang%20and%20Siming%20Sun%20and%20Zhengyu%20Fan%20and%20Qinmin%20Yang%20and%20Xuejun%20Jiang%0AAbstract%3A%20%20%20Time%20series%20analysis%20faces%20significant%20challenges%20in%20handling%20variable-length%0Adata%20and%20achieving%20robust%20generalization.%20While%20Transformer-based%20models%20have%0Aadvanced%20time%20series%20tasks%2C%20they%20often%20struggle%20with%20feature%20redundancy%20and%0Alimited%20generalization%20capabilities.%20Drawing%20inspiration%20from%20classical%20CNN%0Aarchitectures%27%20pyramidal%20structure%2C%20we%20propose%20a%20Multi-Scale%20Representation%0ALearning%20Framework%20based%20on%20a%20Conv-like%20ScaleFusion%20Transformer.%20Our%20approach%0Aintroduces%20a%20temporal%20convolution-like%20structure%20that%20combines%20patching%0Aoperations%20with%20multi-head%20attention%2C%20enabling%20progressive%20temporal%20dimension%0Acompression%20and%20feature%20channel%20expansion.%20We%20further%20develop%20a%20novel%0Across-scale%20attention%20mechanism%20for%20effective%20feature%20fusion%20across%20different%0Atemporal%20scales%2C%20along%20with%20a%20log-space%20normalization%20method%20for%0Avariable-length%20sequences.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%0Aachieves%20superior%20feature%20independence%2C%20reduced%20redundancy%2C%20and%20better%0Aperformance%20in%20forecasting%20and%20classification%20tasks%20compared%20to%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConv-like%2520Scale-Fusion%2520Time%2520Series%2520Transformer%253A%2520A%2520Multi-Scale%250A%2520%2520Representation%2520for%2520Variable-Length%2520Long%2520Time%2520Series%26entry.906535625%3DKai%2520Zhang%2520and%2520Siming%2520Sun%2520and%2520Zhengyu%2520Fan%2520and%2520Qinmin%2520Yang%2520and%2520Xuejun%2520Jiang%26entry.1292438233%3D%2520%2520Time%2520series%2520analysis%2520faces%2520significant%2520challenges%2520in%2520handling%2520variable-length%250Adata%2520and%2520achieving%2520robust%2520generalization.%2520While%2520Transformer-based%2520models%2520have%250Aadvanced%2520time%2520series%2520tasks%252C%2520they%2520often%2520struggle%2520with%2520feature%2520redundancy%2520and%250Alimited%2520generalization%2520capabilities.%2520Drawing%2520inspiration%2520from%2520classical%2520CNN%250Aarchitectures%2527%2520pyramidal%2520structure%252C%2520we%2520propose%2520a%2520Multi-Scale%2520Representation%250ALearning%2520Framework%2520based%2520on%2520a%2520Conv-like%2520ScaleFusion%2520Transformer.%2520Our%2520approach%250Aintroduces%2520a%2520temporal%2520convolution-like%2520structure%2520that%2520combines%2520patching%250Aoperations%2520with%2520multi-head%2520attention%252C%2520enabling%2520progressive%2520temporal%2520dimension%250Acompression%2520and%2520feature%2520channel%2520expansion.%2520We%2520further%2520develop%2520a%2520novel%250Across-scale%2520attention%2520mechanism%2520for%2520effective%2520feature%2520fusion%2520across%2520different%250Atemporal%2520scales%252C%2520along%2520with%2520a%2520log-space%2520normalization%2520method%2520for%250Avariable-length%2520sequences.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%250Aachieves%2520superior%2520feature%2520independence%252C%2520reduced%2520redundancy%252C%2520and%2520better%250Aperformance%2520in%2520forecasting%2520and%2520classification%2520tasks%2520compared%2520to%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conv-like%20Scale-Fusion%20Time%20Series%20Transformer%3A%20A%20Multi-Scale%0A%20%20Representation%20for%20Variable-Length%20Long%20Time%20Series&entry.906535625=Kai%20Zhang%20and%20Siming%20Sun%20and%20Zhengyu%20Fan%20and%20Qinmin%20Yang%20and%20Xuejun%20Jiang&entry.1292438233=%20%20Time%20series%20analysis%20faces%20significant%20challenges%20in%20handling%20variable-length%0Adata%20and%20achieving%20robust%20generalization.%20While%20Transformer-based%20models%20have%0Aadvanced%20time%20series%20tasks%2C%20they%20often%20struggle%20with%20feature%20redundancy%20and%0Alimited%20generalization%20capabilities.%20Drawing%20inspiration%20from%20classical%20CNN%0Aarchitectures%27%20pyramidal%20structure%2C%20we%20propose%20a%20Multi-Scale%20Representation%0ALearning%20Framework%20based%20on%20a%20Conv-like%20ScaleFusion%20Transformer.%20Our%20approach%0Aintroduces%20a%20temporal%20convolution-like%20structure%20that%20combines%20patching%0Aoperations%20with%20multi-head%20attention%2C%20enabling%20progressive%20temporal%20dimension%0Acompression%20and%20feature%20channel%20expansion.%20We%20further%20develop%20a%20novel%0Across-scale%20attention%20mechanism%20for%20effective%20feature%20fusion%20across%20different%0Atemporal%20scales%2C%20along%20with%20a%20log-space%20normalization%20method%20for%0Avariable-length%20sequences.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%0Aachieves%20superior%20feature%20independence%2C%20reduced%20redundancy%2C%20and%20better%0Aperformance%20in%20forecasting%20and%20classification%20tasks%20compared%20to%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17845v1&entry.124074799=Read"},
{"title": "TextOCVP: Object-Centric Video Prediction with Language Guidance", "author": "Angel Villar-Corrales and Gjergj Plepi and Sven Behnke", "abstract": "  Understanding and forecasting future scene states is critical for autonomous\nagents to plan and act effectively in complex environments. Object-centric\nmodels, with structured latent spaces, have shown promise in modeling object\ndynamics and predicting future scene states, but often struggle to scale beyond\nsimple synthetic datasets and to integrate external guidance, limiting their\napplicability in robotics. To address these limitations, we propose TextOCVP,\nan object-centric model for video prediction guided by textual descriptions.\nTextOCVP parses an observed scene into object representations, called slots,\nand utilizes a text-conditioned transformer predictor to forecast future object\nstates and video frames. Our approach jointly models object dynamics and\ninteractions while incorporating textual guidance, enabling accurate and\ncontrollable predictions. TextOCVP's structured latent space offers a more\nprecise control of the forecasting process, outperforming several video\nprediction baselines on two datasets. Additionally, we show that structured\nobject-centric representations provide superior robustness to novel scene\nconfigurations, as well as improved controllability and interpretability,\nenabling more precise and understandable predictions. Videos and code are\navailable at https://play-slot.github.io/TextOCVP.\n", "link": "http://arxiv.org/abs/2502.11655v2", "date": "2025-09-22", "relevancy": 1.7637, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6005}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextOCVP%3A%20Object-Centric%20Video%20Prediction%20with%20Language%20Guidance&body=Title%3A%20TextOCVP%3A%20Object-Centric%20Video%20Prediction%20with%20Language%20Guidance%0AAuthor%3A%20Angel%20Villar-Corrales%20and%20Gjergj%20Plepi%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20Understanding%20and%20forecasting%20future%20scene%20states%20is%20critical%20for%20autonomous%0Aagents%20to%20plan%20and%20act%20effectively%20in%20complex%20environments.%20Object-centric%0Amodels%2C%20with%20structured%20latent%20spaces%2C%20have%20shown%20promise%20in%20modeling%20object%0Adynamics%20and%20predicting%20future%20scene%20states%2C%20but%20often%20struggle%20to%20scale%20beyond%0Asimple%20synthetic%20datasets%20and%20to%20integrate%20external%20guidance%2C%20limiting%20their%0Aapplicability%20in%20robotics.%20To%20address%20these%20limitations%2C%20we%20propose%20TextOCVP%2C%0Aan%20object-centric%20model%20for%20video%20prediction%20guided%20by%20textual%20descriptions.%0ATextOCVP%20parses%20an%20observed%20scene%20into%20object%20representations%2C%20called%20slots%2C%0Aand%20utilizes%20a%20text-conditioned%20transformer%20predictor%20to%20forecast%20future%20object%0Astates%20and%20video%20frames.%20Our%20approach%20jointly%20models%20object%20dynamics%20and%0Ainteractions%20while%20incorporating%20textual%20guidance%2C%20enabling%20accurate%20and%0Acontrollable%20predictions.%20TextOCVP%27s%20structured%20latent%20space%20offers%20a%20more%0Aprecise%20control%20of%20the%20forecasting%20process%2C%20outperforming%20several%20video%0Aprediction%20baselines%20on%20two%20datasets.%20Additionally%2C%20we%20show%20that%20structured%0Aobject-centric%20representations%20provide%20superior%20robustness%20to%20novel%20scene%0Aconfigurations%2C%20as%20well%20as%20improved%20controllability%20and%20interpretability%2C%0Aenabling%20more%20precise%20and%20understandable%20predictions.%20Videos%20and%20code%20are%0Aavailable%20at%20https%3A//play-slot.github.io/TextOCVP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextOCVP%253A%2520Object-Centric%2520Video%2520Prediction%2520with%2520Language%2520Guidance%26entry.906535625%3DAngel%2520Villar-Corrales%2520and%2520Gjergj%2520Plepi%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520Understanding%2520and%2520forecasting%2520future%2520scene%2520states%2520is%2520critical%2520for%2520autonomous%250Aagents%2520to%2520plan%2520and%2520act%2520effectively%2520in%2520complex%2520environments.%2520Object-centric%250Amodels%252C%2520with%2520structured%2520latent%2520spaces%252C%2520have%2520shown%2520promise%2520in%2520modeling%2520object%250Adynamics%2520and%2520predicting%2520future%2520scene%2520states%252C%2520but%2520often%2520struggle%2520to%2520scale%2520beyond%250Asimple%2520synthetic%2520datasets%2520and%2520to%2520integrate%2520external%2520guidance%252C%2520limiting%2520their%250Aapplicability%2520in%2520robotics.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520TextOCVP%252C%250Aan%2520object-centric%2520model%2520for%2520video%2520prediction%2520guided%2520by%2520textual%2520descriptions.%250ATextOCVP%2520parses%2520an%2520observed%2520scene%2520into%2520object%2520representations%252C%2520called%2520slots%252C%250Aand%2520utilizes%2520a%2520text-conditioned%2520transformer%2520predictor%2520to%2520forecast%2520future%2520object%250Astates%2520and%2520video%2520frames.%2520Our%2520approach%2520jointly%2520models%2520object%2520dynamics%2520and%250Ainteractions%2520while%2520incorporating%2520textual%2520guidance%252C%2520enabling%2520accurate%2520and%250Acontrollable%2520predictions.%2520TextOCVP%2527s%2520structured%2520latent%2520space%2520offers%2520a%2520more%250Aprecise%2520control%2520of%2520the%2520forecasting%2520process%252C%2520outperforming%2520several%2520video%250Aprediction%2520baselines%2520on%2520two%2520datasets.%2520Additionally%252C%2520we%2520show%2520that%2520structured%250Aobject-centric%2520representations%2520provide%2520superior%2520robustness%2520to%2520novel%2520scene%250Aconfigurations%252C%2520as%2520well%2520as%2520improved%2520controllability%2520and%2520interpretability%252C%250Aenabling%2520more%2520precise%2520and%2520understandable%2520predictions.%2520Videos%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//play-slot.github.io/TextOCVP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextOCVP%3A%20Object-Centric%20Video%20Prediction%20with%20Language%20Guidance&entry.906535625=Angel%20Villar-Corrales%20and%20Gjergj%20Plepi%20and%20Sven%20Behnke&entry.1292438233=%20%20Understanding%20and%20forecasting%20future%20scene%20states%20is%20critical%20for%20autonomous%0Aagents%20to%20plan%20and%20act%20effectively%20in%20complex%20environments.%20Object-centric%0Amodels%2C%20with%20structured%20latent%20spaces%2C%20have%20shown%20promise%20in%20modeling%20object%0Adynamics%20and%20predicting%20future%20scene%20states%2C%20but%20often%20struggle%20to%20scale%20beyond%0Asimple%20synthetic%20datasets%20and%20to%20integrate%20external%20guidance%2C%20limiting%20their%0Aapplicability%20in%20robotics.%20To%20address%20these%20limitations%2C%20we%20propose%20TextOCVP%2C%0Aan%20object-centric%20model%20for%20video%20prediction%20guided%20by%20textual%20descriptions.%0ATextOCVP%20parses%20an%20observed%20scene%20into%20object%20representations%2C%20called%20slots%2C%0Aand%20utilizes%20a%20text-conditioned%20transformer%20predictor%20to%20forecast%20future%20object%0Astates%20and%20video%20frames.%20Our%20approach%20jointly%20models%20object%20dynamics%20and%0Ainteractions%20while%20incorporating%20textual%20guidance%2C%20enabling%20accurate%20and%0Acontrollable%20predictions.%20TextOCVP%27s%20structured%20latent%20space%20offers%20a%20more%0Aprecise%20control%20of%20the%20forecasting%20process%2C%20outperforming%20several%20video%0Aprediction%20baselines%20on%20two%20datasets.%20Additionally%2C%20we%20show%20that%20structured%0Aobject-centric%20representations%20provide%20superior%20robustness%20to%20novel%20scene%0Aconfigurations%2C%20as%20well%20as%20improved%20controllability%20and%20interpretability%2C%0Aenabling%20more%20precise%20and%20understandable%20predictions.%20Videos%20and%20code%20are%0Aavailable%20at%20https%3A//play-slot.github.io/TextOCVP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11655v2&entry.124074799=Read"},
{"title": "Towards Sample-Efficiency and Generalization of Transfer and Inverse\n  Reinforcement Learning: A Comprehensive Literature Review", "author": "Hossein Hassani and Ehsan Hallaji and Roozbeh Razavi-Far and Mehrdad Saif and Liang Lin", "abstract": "  Reinforcement learning (RL) is a sub-domain of machine learning, mainly\nconcerned with solving sequential decision-making problems by a learning agent\nthat interacts with the decision environment to improve its behavior through\nthe reward it receives from the environment. This learning paradigm is,\nhowever, well-known for being time-consuming due to the necessity of collecting\na large amount of data, making RL suffer from sample inefficiency and difficult\ngeneralization. Furthermore, the construction of an explicit reward function\nthat accounts for the trade-off between multiple desiderata of a decision\nproblem is often a laborious task. These challenges have been recently\naddressed utilizing transfer and inverse reinforcement learning (T-IRL). In\nthis regard, this paper is devoted to a comprehensive review of realizing the\nsample efficiency and generalization of RL algorithms through T-IRL. Following\na brief introduction to RL, the fundamental T-IRL methods are presented and the\nmost recent advancements in each research field have been extensively reviewed.\nOur findings denote that a majority of recent research works have dealt with\nthe aforementioned challenges by utilizing human-in-the-loop and sim-to-real\nstrategies for the efficient transfer of knowledge from source domains to the\ntarget domain under the transfer learning scheme. Under the IRL structure,\ntraining schemes that require a low number of experience transitions and\nextension of such frameworks to multi-agent and multi-intention problems have\nbeen the priority of researchers in recent years.\n", "link": "http://arxiv.org/abs/2411.10268v2", "date": "2025-09-22", "relevancy": 1.4127, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.477}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4708}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Sample-Efficiency%20and%20Generalization%20of%20Transfer%20and%20Inverse%0A%20%20Reinforcement%20Learning%3A%20A%20Comprehensive%20Literature%20Review&body=Title%3A%20Towards%20Sample-Efficiency%20and%20Generalization%20of%20Transfer%20and%20Inverse%0A%20%20Reinforcement%20Learning%3A%20A%20Comprehensive%20Literature%20Review%0AAuthor%3A%20Hossein%20Hassani%20and%20Ehsan%20Hallaji%20and%20Roozbeh%20Razavi-Far%20and%20Mehrdad%20Saif%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20is%20a%20sub-domain%20of%20machine%20learning%2C%20mainly%0Aconcerned%20with%20solving%20sequential%20decision-making%20problems%20by%20a%20learning%20agent%0Athat%20interacts%20with%20the%20decision%20environment%20to%20improve%20its%20behavior%20through%0Athe%20reward%20it%20receives%20from%20the%20environment.%20This%20learning%20paradigm%20is%2C%0Ahowever%2C%20well-known%20for%20being%20time-consuming%20due%20to%20the%20necessity%20of%20collecting%0Aa%20large%20amount%20of%20data%2C%20making%20RL%20suffer%20from%20sample%20inefficiency%20and%20difficult%0Ageneralization.%20Furthermore%2C%20the%20construction%20of%20an%20explicit%20reward%20function%0Athat%20accounts%20for%20the%20trade-off%20between%20multiple%20desiderata%20of%20a%20decision%0Aproblem%20is%20often%20a%20laborious%20task.%20These%20challenges%20have%20been%20recently%0Aaddressed%20utilizing%20transfer%20and%20inverse%20reinforcement%20learning%20%28T-IRL%29.%20In%0Athis%20regard%2C%20this%20paper%20is%20devoted%20to%20a%20comprehensive%20review%20of%20realizing%20the%0Asample%20efficiency%20and%20generalization%20of%20RL%20algorithms%20through%20T-IRL.%20Following%0Aa%20brief%20introduction%20to%20RL%2C%20the%20fundamental%20T-IRL%20methods%20are%20presented%20and%20the%0Amost%20recent%20advancements%20in%20each%20research%20field%20have%20been%20extensively%20reviewed.%0AOur%20findings%20denote%20that%20a%20majority%20of%20recent%20research%20works%20have%20dealt%20with%0Athe%20aforementioned%20challenges%20by%20utilizing%20human-in-the-loop%20and%20sim-to-real%0Astrategies%20for%20the%20efficient%20transfer%20of%20knowledge%20from%20source%20domains%20to%20the%0Atarget%20domain%20under%20the%20transfer%20learning%20scheme.%20Under%20the%20IRL%20structure%2C%0Atraining%20schemes%20that%20require%20a%20low%20number%20of%20experience%20transitions%20and%0Aextension%20of%20such%20frameworks%20to%20multi-agent%20and%20multi-intention%20problems%20have%0Abeen%20the%20priority%20of%20researchers%20in%20recent%20years.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Sample-Efficiency%2520and%2520Generalization%2520of%2520Transfer%2520and%2520Inverse%250A%2520%2520Reinforcement%2520Learning%253A%2520A%2520Comprehensive%2520Literature%2520Review%26entry.906535625%3DHossein%2520Hassani%2520and%2520Ehsan%2520Hallaji%2520and%2520Roozbeh%2520Razavi-Far%2520and%2520Mehrdad%2520Saif%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520is%2520a%2520sub-domain%2520of%2520machine%2520learning%252C%2520mainly%250Aconcerned%2520with%2520solving%2520sequential%2520decision-making%2520problems%2520by%2520a%2520learning%2520agent%250Athat%2520interacts%2520with%2520the%2520decision%2520environment%2520to%2520improve%2520its%2520behavior%2520through%250Athe%2520reward%2520it%2520receives%2520from%2520the%2520environment.%2520This%2520learning%2520paradigm%2520is%252C%250Ahowever%252C%2520well-known%2520for%2520being%2520time-consuming%2520due%2520to%2520the%2520necessity%2520of%2520collecting%250Aa%2520large%2520amount%2520of%2520data%252C%2520making%2520RL%2520suffer%2520from%2520sample%2520inefficiency%2520and%2520difficult%250Ageneralization.%2520Furthermore%252C%2520the%2520construction%2520of%2520an%2520explicit%2520reward%2520function%250Athat%2520accounts%2520for%2520the%2520trade-off%2520between%2520multiple%2520desiderata%2520of%2520a%2520decision%250Aproblem%2520is%2520often%2520a%2520laborious%2520task.%2520These%2520challenges%2520have%2520been%2520recently%250Aaddressed%2520utilizing%2520transfer%2520and%2520inverse%2520reinforcement%2520learning%2520%2528T-IRL%2529.%2520In%250Athis%2520regard%252C%2520this%2520paper%2520is%2520devoted%2520to%2520a%2520comprehensive%2520review%2520of%2520realizing%2520the%250Asample%2520efficiency%2520and%2520generalization%2520of%2520RL%2520algorithms%2520through%2520T-IRL.%2520Following%250Aa%2520brief%2520introduction%2520to%2520RL%252C%2520the%2520fundamental%2520T-IRL%2520methods%2520are%2520presented%2520and%2520the%250Amost%2520recent%2520advancements%2520in%2520each%2520research%2520field%2520have%2520been%2520extensively%2520reviewed.%250AOur%2520findings%2520denote%2520that%2520a%2520majority%2520of%2520recent%2520research%2520works%2520have%2520dealt%2520with%250Athe%2520aforementioned%2520challenges%2520by%2520utilizing%2520human-in-the-loop%2520and%2520sim-to-real%250Astrategies%2520for%2520the%2520efficient%2520transfer%2520of%2520knowledge%2520from%2520source%2520domains%2520to%2520the%250Atarget%2520domain%2520under%2520the%2520transfer%2520learning%2520scheme.%2520Under%2520the%2520IRL%2520structure%252C%250Atraining%2520schemes%2520that%2520require%2520a%2520low%2520number%2520of%2520experience%2520transitions%2520and%250Aextension%2520of%2520such%2520frameworks%2520to%2520multi-agent%2520and%2520multi-intention%2520problems%2520have%250Abeen%2520the%2520priority%2520of%2520researchers%2520in%2520recent%2520years.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Sample-Efficiency%20and%20Generalization%20of%20Transfer%20and%20Inverse%0A%20%20Reinforcement%20Learning%3A%20A%20Comprehensive%20Literature%20Review&entry.906535625=Hossein%20Hassani%20and%20Ehsan%20Hallaji%20and%20Roozbeh%20Razavi-Far%20and%20Mehrdad%20Saif%20and%20Liang%20Lin&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20is%20a%20sub-domain%20of%20machine%20learning%2C%20mainly%0Aconcerned%20with%20solving%20sequential%20decision-making%20problems%20by%20a%20learning%20agent%0Athat%20interacts%20with%20the%20decision%20environment%20to%20improve%20its%20behavior%20through%0Athe%20reward%20it%20receives%20from%20the%20environment.%20This%20learning%20paradigm%20is%2C%0Ahowever%2C%20well-known%20for%20being%20time-consuming%20due%20to%20the%20necessity%20of%20collecting%0Aa%20large%20amount%20of%20data%2C%20making%20RL%20suffer%20from%20sample%20inefficiency%20and%20difficult%0Ageneralization.%20Furthermore%2C%20the%20construction%20of%20an%20explicit%20reward%20function%0Athat%20accounts%20for%20the%20trade-off%20between%20multiple%20desiderata%20of%20a%20decision%0Aproblem%20is%20often%20a%20laborious%20task.%20These%20challenges%20have%20been%20recently%0Aaddressed%20utilizing%20transfer%20and%20inverse%20reinforcement%20learning%20%28T-IRL%29.%20In%0Athis%20regard%2C%20this%20paper%20is%20devoted%20to%20a%20comprehensive%20review%20of%20realizing%20the%0Asample%20efficiency%20and%20generalization%20of%20RL%20algorithms%20through%20T-IRL.%20Following%0Aa%20brief%20introduction%20to%20RL%2C%20the%20fundamental%20T-IRL%20methods%20are%20presented%20and%20the%0Amost%20recent%20advancements%20in%20each%20research%20field%20have%20been%20extensively%20reviewed.%0AOur%20findings%20denote%20that%20a%20majority%20of%20recent%20research%20works%20have%20dealt%20with%0Athe%20aforementioned%20challenges%20by%20utilizing%20human-in-the-loop%20and%20sim-to-real%0Astrategies%20for%20the%20efficient%20transfer%20of%20knowledge%20from%20source%20domains%20to%20the%0Atarget%20domain%20under%20the%20transfer%20learning%20scheme.%20Under%20the%20IRL%20structure%2C%0Atraining%20schemes%20that%20require%20a%20low%20number%20of%20experience%20transitions%20and%0Aextension%20of%20such%20frameworks%20to%20multi-agent%20and%20multi-intention%20problems%20have%0Abeen%20the%20priority%20of%20researchers%20in%20recent%20years.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10268v2&entry.124074799=Read"},
{"title": "TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for\n  \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "author": "Yutong Liu and Ziyue Zhang and Ban Ma-bao and Renzeng Duojie and Yuqing Cai and Yongbin Yu and Xiangxiang Wang and Fan Gao and Cheng Huang and Nyima Tashi", "abstract": "  Tibetan is a low-resource language with limited parallel speech corpora\nspanning its three major dialects (\\\"U-Tsang, Amdo, and Kham), limiting\nprogress in speech modeling. To address this issue, we propose TMD-TTS, a\nunified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes\nparallel dialectal speech from explicit dialect labels. Our method features a\ndialect fusion module and a Dialect-Specialized Dynamic Routing Network\n(DSDR-Net) to capture fine-grained acoustic and linguistic variations across\ndialects. Extensive objective and subjective evaluations demonstrate that\nTMD-TTS significantly outperforms baselines in dialectal expressiveness. We\nfurther validate the quality and utility of the synthesized speech through a\nchallenging Speech-to-Speech Dialect Conversion (S2SDC) task.\n", "link": "http://arxiv.org/abs/2509.18060v1", "date": "2025-09-22", "relevancy": 1.8959, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4903}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4646}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TMD-TTS%3A%20A%20Unified%20Tibetan%20Multi-Dialect%20Text-to-Speech%20Synthesis%20for%0A%20%20%C3%9C-Tsang%2C%20Amdo%20and%20Kham%20Speech%20Dataset%20Generation&body=Title%3A%20TMD-TTS%3A%20A%20Unified%20Tibetan%20Multi-Dialect%20Text-to-Speech%20Synthesis%20for%0A%20%20%C3%9C-Tsang%2C%20Amdo%20and%20Kham%20Speech%20Dataset%20Generation%0AAuthor%3A%20Yutong%20Liu%20and%20Ziyue%20Zhang%20and%20Ban%20Ma-bao%20and%20Renzeng%20Duojie%20and%20Yuqing%20Cai%20and%20Yongbin%20Yu%20and%20Xiangxiang%20Wang%20and%20Fan%20Gao%20and%20Cheng%20Huang%20and%20Nyima%20Tashi%0AAbstract%3A%20%20%20Tibetan%20is%20a%20low-resource%20language%20with%20limited%20parallel%20speech%20corpora%0Aspanning%20its%20three%20major%20dialects%20%28%5C%22U-Tsang%2C%20Amdo%2C%20and%20Kham%29%2C%20limiting%0Aprogress%20in%20speech%20modeling.%20To%20address%20this%20issue%2C%20we%20propose%20TMD-TTS%2C%20a%0Aunified%20Tibetan%20multi-dialect%20text-to-speech%20%28TTS%29%20framework%20that%20synthesizes%0Aparallel%20dialectal%20speech%20from%20explicit%20dialect%20labels.%20Our%20method%20features%20a%0Adialect%20fusion%20module%20and%20a%20Dialect-Specialized%20Dynamic%20Routing%20Network%0A%28DSDR-Net%29%20to%20capture%20fine-grained%20acoustic%20and%20linguistic%20variations%20across%0Adialects.%20Extensive%20objective%20and%20subjective%20evaluations%20demonstrate%20that%0ATMD-TTS%20significantly%20outperforms%20baselines%20in%20dialectal%20expressiveness.%20We%0Afurther%20validate%20the%20quality%20and%20utility%20of%20the%20synthesized%20speech%20through%20a%0Achallenging%20Speech-to-Speech%20Dialect%20Conversion%20%28S2SDC%29%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTMD-TTS%253A%2520A%2520Unified%2520Tibetan%2520Multi-Dialect%2520Text-to-Speech%2520Synthesis%2520for%250A%2520%2520%25C3%259C-Tsang%252C%2520Amdo%2520and%2520Kham%2520Speech%2520Dataset%2520Generation%26entry.906535625%3DYutong%2520Liu%2520and%2520Ziyue%2520Zhang%2520and%2520Ban%2520Ma-bao%2520and%2520Renzeng%2520Duojie%2520and%2520Yuqing%2520Cai%2520and%2520Yongbin%2520Yu%2520and%2520Xiangxiang%2520Wang%2520and%2520Fan%2520Gao%2520and%2520Cheng%2520Huang%2520and%2520Nyima%2520Tashi%26entry.1292438233%3D%2520%2520Tibetan%2520is%2520a%2520low-resource%2520language%2520with%2520limited%2520parallel%2520speech%2520corpora%250Aspanning%2520its%2520three%2520major%2520dialects%2520%2528%255C%2522U-Tsang%252C%2520Amdo%252C%2520and%2520Kham%2529%252C%2520limiting%250Aprogress%2520in%2520speech%2520modeling.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520TMD-TTS%252C%2520a%250Aunified%2520Tibetan%2520multi-dialect%2520text-to-speech%2520%2528TTS%2529%2520framework%2520that%2520synthesizes%250Aparallel%2520dialectal%2520speech%2520from%2520explicit%2520dialect%2520labels.%2520Our%2520method%2520features%2520a%250Adialect%2520fusion%2520module%2520and%2520a%2520Dialect-Specialized%2520Dynamic%2520Routing%2520Network%250A%2528DSDR-Net%2529%2520to%2520capture%2520fine-grained%2520acoustic%2520and%2520linguistic%2520variations%2520across%250Adialects.%2520Extensive%2520objective%2520and%2520subjective%2520evaluations%2520demonstrate%2520that%250ATMD-TTS%2520significantly%2520outperforms%2520baselines%2520in%2520dialectal%2520expressiveness.%2520We%250Afurther%2520validate%2520the%2520quality%2520and%2520utility%2520of%2520the%2520synthesized%2520speech%2520through%2520a%250Achallenging%2520Speech-to-Speech%2520Dialect%2520Conversion%2520%2528S2SDC%2529%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TMD-TTS%3A%20A%20Unified%20Tibetan%20Multi-Dialect%20Text-to-Speech%20Synthesis%20for%0A%20%20%C3%9C-Tsang%2C%20Amdo%20and%20Kham%20Speech%20Dataset%20Generation&entry.906535625=Yutong%20Liu%20and%20Ziyue%20Zhang%20and%20Ban%20Ma-bao%20and%20Renzeng%20Duojie%20and%20Yuqing%20Cai%20and%20Yongbin%20Yu%20and%20Xiangxiang%20Wang%20and%20Fan%20Gao%20and%20Cheng%20Huang%20and%20Nyima%20Tashi&entry.1292438233=%20%20Tibetan%20is%20a%20low-resource%20language%20with%20limited%20parallel%20speech%20corpora%0Aspanning%20its%20three%20major%20dialects%20%28%5C%22U-Tsang%2C%20Amdo%2C%20and%20Kham%29%2C%20limiting%0Aprogress%20in%20speech%20modeling.%20To%20address%20this%20issue%2C%20we%20propose%20TMD-TTS%2C%20a%0Aunified%20Tibetan%20multi-dialect%20text-to-speech%20%28TTS%29%20framework%20that%20synthesizes%0Aparallel%20dialectal%20speech%20from%20explicit%20dialect%20labels.%20Our%20method%20features%20a%0Adialect%20fusion%20module%20and%20a%20Dialect-Specialized%20Dynamic%20Routing%20Network%0A%28DSDR-Net%29%20to%20capture%20fine-grained%20acoustic%20and%20linguistic%20variations%20across%0Adialects.%20Extensive%20objective%20and%20subjective%20evaluations%20demonstrate%20that%0ATMD-TTS%20significantly%20outperforms%20baselines%20in%20dialectal%20expressiveness.%20We%0Afurther%20validate%20the%20quality%20and%20utility%20of%20the%20synthesized%20speech%20through%20a%0Achallenging%20Speech-to-Speech%20Dialect%20Conversion%20%28S2SDC%29%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18060v1&entry.124074799=Read"},
{"title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous\n  Driving", "author": "Shuyao Shang and Yuntao Chen and Yuqi Wang and Yingyan Li and Zhaoxiang Zhang", "abstract": "  End-to-end autonomous driving has substantially progressed by directly\npredicting future trajectories from raw perception inputs, which bypasses\ntraditional modular pipelines. However, mainstream methods trained via\nimitation learning suffer from critical safety limitations, as they fail to\ndistinguish between trajectories that appear human-like but are potentially\nunsafe. Some recent approaches attempt to address this by regressing multiple\nrule-driven scores but decoupling supervision from policy optimization,\nresulting in suboptimal performance. To tackle these challenges, we propose\nDriveDPO, a Safety Direct Preference Optimization Policy Learning framework.\nFirst, we distill a unified policy distribution from human imitation similarity\nand rule-based safety scores for direct policy optimization. Further, we\nintroduce an iterative Direct Preference Optimization stage formulated as\ntrajectory-level preference alignment. Extensive experiments on the NAVSIM\nbenchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of\n90.0. Furthermore, qualitative results across diverse challenging scenarios\nhighlight DriveDPO's ability to produce safer and more reliable driving\nbehaviors.\n", "link": "http://arxiv.org/abs/2509.17940v1", "date": "2025-09-22", "relevancy": 2.0007, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4967}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveDPO%3A%20Policy%20Learning%20via%20Safety%20DPO%20For%20End-to-End%20Autonomous%0A%20%20Driving&body=Title%3A%20DriveDPO%3A%20Policy%20Learning%20via%20Safety%20DPO%20For%20End-to-End%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Shuyao%20Shang%20and%20Yuntao%20Chen%20and%20Yuqi%20Wang%20and%20Yingyan%20Li%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20has%20substantially%20progressed%20by%20directly%0Apredicting%20future%20trajectories%20from%20raw%20perception%20inputs%2C%20which%20bypasses%0Atraditional%20modular%20pipelines.%20However%2C%20mainstream%20methods%20trained%20via%0Aimitation%20learning%20suffer%20from%20critical%20safety%20limitations%2C%20as%20they%20fail%20to%0Adistinguish%20between%20trajectories%20that%20appear%20human-like%20but%20are%20potentially%0Aunsafe.%20Some%20recent%20approaches%20attempt%20to%20address%20this%20by%20regressing%20multiple%0Arule-driven%20scores%20but%20decoupling%20supervision%20from%20policy%20optimization%2C%0Aresulting%20in%20suboptimal%20performance.%20To%20tackle%20these%20challenges%2C%20we%20propose%0ADriveDPO%2C%20a%20Safety%20Direct%20Preference%20Optimization%20Policy%20Learning%20framework.%0AFirst%2C%20we%20distill%20a%20unified%20policy%20distribution%20from%20human%20imitation%20similarity%0Aand%20rule-based%20safety%20scores%20for%20direct%20policy%20optimization.%20Further%2C%20we%0Aintroduce%20an%20iterative%20Direct%20Preference%20Optimization%20stage%20formulated%20as%0Atrajectory-level%20preference%20alignment.%20Extensive%20experiments%20on%20the%20NAVSIM%0Abenchmark%20demonstrate%20that%20DriveDPO%20achieves%20a%20new%20state-of-the-art%20PDMS%20of%0A90.0.%20Furthermore%2C%20qualitative%20results%20across%20diverse%20challenging%20scenarios%0Ahighlight%20DriveDPO%27s%20ability%20to%20produce%20safer%20and%20more%20reliable%20driving%0Abehaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveDPO%253A%2520Policy%2520Learning%2520via%2520Safety%2520DPO%2520For%2520End-to-End%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DShuyao%2520Shang%2520and%2520Yuntao%2520Chen%2520and%2520Yuqi%2520Wang%2520and%2520Yingyan%2520Li%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520has%2520substantially%2520progressed%2520by%2520directly%250Apredicting%2520future%2520trajectories%2520from%2520raw%2520perception%2520inputs%252C%2520which%2520bypasses%250Atraditional%2520modular%2520pipelines.%2520However%252C%2520mainstream%2520methods%2520trained%2520via%250Aimitation%2520learning%2520suffer%2520from%2520critical%2520safety%2520limitations%252C%2520as%2520they%2520fail%2520to%250Adistinguish%2520between%2520trajectories%2520that%2520appear%2520human-like%2520but%2520are%2520potentially%250Aunsafe.%2520Some%2520recent%2520approaches%2520attempt%2520to%2520address%2520this%2520by%2520regressing%2520multiple%250Arule-driven%2520scores%2520but%2520decoupling%2520supervision%2520from%2520policy%2520optimization%252C%250Aresulting%2520in%2520suboptimal%2520performance.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%250ADriveDPO%252C%2520a%2520Safety%2520Direct%2520Preference%2520Optimization%2520Policy%2520Learning%2520framework.%250AFirst%252C%2520we%2520distill%2520a%2520unified%2520policy%2520distribution%2520from%2520human%2520imitation%2520similarity%250Aand%2520rule-based%2520safety%2520scores%2520for%2520direct%2520policy%2520optimization.%2520Further%252C%2520we%250Aintroduce%2520an%2520iterative%2520Direct%2520Preference%2520Optimization%2520stage%2520formulated%2520as%250Atrajectory-level%2520preference%2520alignment.%2520Extensive%2520experiments%2520on%2520the%2520NAVSIM%250Abenchmark%2520demonstrate%2520that%2520DriveDPO%2520achieves%2520a%2520new%2520state-of-the-art%2520PDMS%2520of%250A90.0.%2520Furthermore%252C%2520qualitative%2520results%2520across%2520diverse%2520challenging%2520scenarios%250Ahighlight%2520DriveDPO%2527s%2520ability%2520to%2520produce%2520safer%2520and%2520more%2520reliable%2520driving%250Abehaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveDPO%3A%20Policy%20Learning%20via%20Safety%20DPO%20For%20End-to-End%20Autonomous%0A%20%20Driving&entry.906535625=Shuyao%20Shang%20and%20Yuntao%20Chen%20and%20Yuqi%20Wang%20and%20Yingyan%20Li%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20has%20substantially%20progressed%20by%20directly%0Apredicting%20future%20trajectories%20from%20raw%20perception%20inputs%2C%20which%20bypasses%0Atraditional%20modular%20pipelines.%20However%2C%20mainstream%20methods%20trained%20via%0Aimitation%20learning%20suffer%20from%20critical%20safety%20limitations%2C%20as%20they%20fail%20to%0Adistinguish%20between%20trajectories%20that%20appear%20human-like%20but%20are%20potentially%0Aunsafe.%20Some%20recent%20approaches%20attempt%20to%20address%20this%20by%20regressing%20multiple%0Arule-driven%20scores%20but%20decoupling%20supervision%20from%20policy%20optimization%2C%0Aresulting%20in%20suboptimal%20performance.%20To%20tackle%20these%20challenges%2C%20we%20propose%0ADriveDPO%2C%20a%20Safety%20Direct%20Preference%20Optimization%20Policy%20Learning%20framework.%0AFirst%2C%20we%20distill%20a%20unified%20policy%20distribution%20from%20human%20imitation%20similarity%0Aand%20rule-based%20safety%20scores%20for%20direct%20policy%20optimization.%20Further%2C%20we%0Aintroduce%20an%20iterative%20Direct%20Preference%20Optimization%20stage%20formulated%20as%0Atrajectory-level%20preference%20alignment.%20Extensive%20experiments%20on%20the%20NAVSIM%0Abenchmark%20demonstrate%20that%20DriveDPO%20achieves%20a%20new%20state-of-the-art%20PDMS%20of%0A90.0.%20Furthermore%2C%20qualitative%20results%20across%20diverse%20challenging%20scenarios%0Ahighlight%20DriveDPO%27s%20ability%20to%20produce%20safer%20and%20more%20reliable%20driving%0Abehaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17940v1&entry.124074799=Read"},
{"title": "Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method", "author": "Gregory Schroeder and Mohamed Sabry and Cristina Olaverri-Monreal", "abstract": "  Estimating camera intrinsic parameters without prior scene knowledge is a\nfundamental challenge in computer vision. This capability is particularly\nimportant for applications such as autonomous driving and vehicle platooning,\nwhere precalibrated setups are impractical and real-time adaptability is\nnecessary. To advance the state-of-the-art, we present a set of equations based\non the calibrated trifocal tensor, enabling projective camera self-calibration\nfrom minimal image data. Our method, termed TrifocalCalib, significantly\nimproves accuracy and robustness compared to both recent learning-based and\nclassical approaches. Unlike many existing techniques, our approach requires no\ncalibration target, imposes no constraints on camera motion, and simultaneously\nestimates both focal length and principal point. Evaluations in both\nprocedurally generated synthetic environments and structured dataset-based\nscenarios demonstrate the effectiveness of our approach. To support\nreproducibility, we make the code publicly available.\n", "link": "http://arxiv.org/abs/2509.17620v1", "date": "2025-09-22", "relevancy": 1.5941, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6042}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5157}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensor-Based%20Self-Calibration%20of%20Cameras%20via%20the%20TrifocalCalib%20Method&body=Title%3A%20Tensor-Based%20Self-Calibration%20of%20Cameras%20via%20the%20TrifocalCalib%20Method%0AAuthor%3A%20Gregory%20Schroeder%20and%20Mohamed%20Sabry%20and%20Cristina%20Olaverri-Monreal%0AAbstract%3A%20%20%20Estimating%20camera%20intrinsic%20parameters%20without%20prior%20scene%20knowledge%20is%20a%0Afundamental%20challenge%20in%20computer%20vision.%20This%20capability%20is%20particularly%0Aimportant%20for%20applications%20such%20as%20autonomous%20driving%20and%20vehicle%20platooning%2C%0Awhere%20precalibrated%20setups%20are%20impractical%20and%20real-time%20adaptability%20is%0Anecessary.%20To%20advance%20the%20state-of-the-art%2C%20we%20present%20a%20set%20of%20equations%20based%0Aon%20the%20calibrated%20trifocal%20tensor%2C%20enabling%20projective%20camera%20self-calibration%0Afrom%20minimal%20image%20data.%20Our%20method%2C%20termed%20TrifocalCalib%2C%20significantly%0Aimproves%20accuracy%20and%20robustness%20compared%20to%20both%20recent%20learning-based%20and%0Aclassical%20approaches.%20Unlike%20many%20existing%20techniques%2C%20our%20approach%20requires%20no%0Acalibration%20target%2C%20imposes%20no%20constraints%20on%20camera%20motion%2C%20and%20simultaneously%0Aestimates%20both%20focal%20length%20and%20principal%20point.%20Evaluations%20in%20both%0Aprocedurally%20generated%20synthetic%20environments%20and%20structured%20dataset-based%0Ascenarios%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20To%20support%0Areproducibility%2C%20we%20make%20the%20code%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensor-Based%2520Self-Calibration%2520of%2520Cameras%2520via%2520the%2520TrifocalCalib%2520Method%26entry.906535625%3DGregory%2520Schroeder%2520and%2520Mohamed%2520Sabry%2520and%2520Cristina%2520Olaverri-Monreal%26entry.1292438233%3D%2520%2520Estimating%2520camera%2520intrinsic%2520parameters%2520without%2520prior%2520scene%2520knowledge%2520is%2520a%250Afundamental%2520challenge%2520in%2520computer%2520vision.%2520This%2520capability%2520is%2520particularly%250Aimportant%2520for%2520applications%2520such%2520as%2520autonomous%2520driving%2520and%2520vehicle%2520platooning%252C%250Awhere%2520precalibrated%2520setups%2520are%2520impractical%2520and%2520real-time%2520adaptability%2520is%250Anecessary.%2520To%2520advance%2520the%2520state-of-the-art%252C%2520we%2520present%2520a%2520set%2520of%2520equations%2520based%250Aon%2520the%2520calibrated%2520trifocal%2520tensor%252C%2520enabling%2520projective%2520camera%2520self-calibration%250Afrom%2520minimal%2520image%2520data.%2520Our%2520method%252C%2520termed%2520TrifocalCalib%252C%2520significantly%250Aimproves%2520accuracy%2520and%2520robustness%2520compared%2520to%2520both%2520recent%2520learning-based%2520and%250Aclassical%2520approaches.%2520Unlike%2520many%2520existing%2520techniques%252C%2520our%2520approach%2520requires%2520no%250Acalibration%2520target%252C%2520imposes%2520no%2520constraints%2520on%2520camera%2520motion%252C%2520and%2520simultaneously%250Aestimates%2520both%2520focal%2520length%2520and%2520principal%2520point.%2520Evaluations%2520in%2520both%250Aprocedurally%2520generated%2520synthetic%2520environments%2520and%2520structured%2520dataset-based%250Ascenarios%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520To%2520support%250Areproducibility%252C%2520we%2520make%2520the%2520code%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensor-Based%20Self-Calibration%20of%20Cameras%20via%20the%20TrifocalCalib%20Method&entry.906535625=Gregory%20Schroeder%20and%20Mohamed%20Sabry%20and%20Cristina%20Olaverri-Monreal&entry.1292438233=%20%20Estimating%20camera%20intrinsic%20parameters%20without%20prior%20scene%20knowledge%20is%20a%0Afundamental%20challenge%20in%20computer%20vision.%20This%20capability%20is%20particularly%0Aimportant%20for%20applications%20such%20as%20autonomous%20driving%20and%20vehicle%20platooning%2C%0Awhere%20precalibrated%20setups%20are%20impractical%20and%20real-time%20adaptability%20is%0Anecessary.%20To%20advance%20the%20state-of-the-art%2C%20we%20present%20a%20set%20of%20equations%20based%0Aon%20the%20calibrated%20trifocal%20tensor%2C%20enabling%20projective%20camera%20self-calibration%0Afrom%20minimal%20image%20data.%20Our%20method%2C%20termed%20TrifocalCalib%2C%20significantly%0Aimproves%20accuracy%20and%20robustness%20compared%20to%20both%20recent%20learning-based%20and%0Aclassical%20approaches.%20Unlike%20many%20existing%20techniques%2C%20our%20approach%20requires%20no%0Acalibration%20target%2C%20imposes%20no%20constraints%20on%20camera%20motion%2C%20and%20simultaneously%0Aestimates%20both%20focal%20length%20and%20principal%20point.%20Evaluations%20in%20both%0Aprocedurally%20generated%20synthetic%20environments%20and%20structured%20dataset-based%0Ascenarios%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20To%20support%0Areproducibility%2C%20we%20make%20the%20code%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17620v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


