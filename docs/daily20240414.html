<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240411.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "EgoGen: An Egocentric Synthetic Data Generator", "author": "Gen Li and Kaifeng Zhao and Siwei Zhang and Xiaozhong Lyu and Mihai Dusmanu and Yan Zhang and Marc Pollefeys and Siyu Tang", "abstract": "  Understanding the world in first-person view is fundamental in Augmented\nReality (AR). This immersive perspective brings dramatic visual changes and\nunique challenges compared to third-person views. Synthetic data has empowered\nthird-person-view vision models, but its application to embodied egocentric\nperception tasks remains largely unexplored. A critical challenge lies in\nsimulating natural human movements and behaviors that effectively steer the\nembodied cameras to capture a faithful egocentric representation of the 3D\nworld. To address this challenge, we introduce EgoGen, a new synthetic data\ngenerator that can produce accurate and rich ground-truth training data for\negocentric perception tasks. At the heart of EgoGen is a novel human motion\nsynthesis model that directly leverages egocentric visual inputs of a virtual\nhuman to sense the 3D environment. Combined with collision-avoiding motion\nprimitives and a two-stage reinforcement learning approach, our motion\nsynthesis model offers a closed-loop solution where the embodied perception and\nmovement of the virtual human are seamlessly coupled. Compared to previous\nworks, our model eliminates the need for a pre-defined global path, and is\ndirectly applicable to dynamic environments. Combined with our easy-to-use and\nscalable data generation pipeline, we demonstrate EgoGen's efficacy in three\ntasks: mapping and localization for head-mounted cameras, egocentric camera\ntracking, and human mesh recovery from egocentric views. EgoGen will be fully\nopen-sourced, offering a practical solution for creating realistic egocentric\ntraining data and aiming to serve as a useful tool for egocentric computer\nvision research. Refer to our project page: https://ego-gen.github.io/.\n", "link": "http://arxiv.org/abs/2401.08739v2", "date": "2024-04-11", "relevancy": 3.0238, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.652}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5884}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5739}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EgoGen%3A%20An%20Egocentric%20Synthetic%20Data%20Generator&body=Title%3A%20EgoGen%3A%20An%20Egocentric%20Synthetic%20Data%20Generator%0AAuthor%3A%20Gen%20Li%20and%20Kaifeng%20Zhao%20and%20Siwei%20Zhang%20and%20Xiaozhong%20Lyu%20and%20Mihai%20Dusmanu%20and%20Yan%20Zhang%20and%20Marc%20Pollefeys%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20Understanding%20the%20world%20in%20first-person%20view%20is%20fundamental%20in%20Augmented%0AReality%20%28AR%29.%20This%20immersive%20perspective%20brings%20dramatic%20visual%20changes%20and%0Aunique%20challenges%20compared%20to%20third-person%20views.%20Synthetic%20data%20has%20empowered%0Athird-person-view%20vision%20models%2C%20but%20its%20application%20to%20embodied%20egocentric%0Aperception%20tasks%20remains%20largely%20unexplored.%20A%20critical%20challenge%20lies%20in%0Asimulating%20natural%20human%20movements%20and%20behaviors%20that%20effectively%20steer%20the%0Aembodied%20cameras%20to%20capture%20a%20faithful%20egocentric%20representation%20of%20the%203D%0Aworld.%20To%20address%20this%20challenge%2C%20we%20introduce%20EgoGen%2C%20a%20new%20synthetic%20data%0Agenerator%20that%20can%20produce%20accurate%20and%20rich%20ground-truth%20training%20data%20for%0Aegocentric%20perception%20tasks.%20At%20the%20heart%20of%20EgoGen%20is%20a%20novel%20human%20motion%0Asynthesis%20model%20that%20directly%20leverages%20egocentric%20visual%20inputs%20of%20a%20virtual%0Ahuman%20to%20sense%20the%203D%20environment.%20Combined%20with%20collision-avoiding%20motion%0Aprimitives%20and%20a%20two-stage%20reinforcement%20learning%20approach%2C%20our%20motion%0Asynthesis%20model%20offers%20a%20closed-loop%20solution%20where%20the%20embodied%20perception%20and%0Amovement%20of%20the%20virtual%20human%20are%20seamlessly%20coupled.%20Compared%20to%20previous%0Aworks%2C%20our%20model%20eliminates%20the%20need%20for%20a%20pre-defined%20global%20path%2C%20and%20is%0Adirectly%20applicable%20to%20dynamic%20environments.%20Combined%20with%20our%20easy-to-use%20and%0Ascalable%20data%20generation%20pipeline%2C%20we%20demonstrate%20EgoGen%27s%20efficacy%20in%20three%0Atasks%3A%20mapping%20and%20localization%20for%20head-mounted%20cameras%2C%20egocentric%20camera%0Atracking%2C%20and%20human%20mesh%20recovery%20from%20egocentric%20views.%20EgoGen%20will%20be%20fully%0Aopen-sourced%2C%20offering%20a%20practical%20solution%20for%20creating%20realistic%20egocentric%0Atraining%20data%20and%20aiming%20to%20serve%20as%20a%20useful%20tool%20for%20egocentric%20computer%0Avision%20research.%20Refer%20to%20our%20project%20page%3A%20https%3A//ego-gen.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08739v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoGen%3A%20An%20Egocentric%20Synthetic%20Data%20Generator&entry.906535625=Gen%20Li%20and%20Kaifeng%20Zhao%20and%20Siwei%20Zhang%20and%20Xiaozhong%20Lyu%20and%20Mihai%20Dusmanu%20and%20Yan%20Zhang%20and%20Marc%20Pollefeys%20and%20Siyu%20Tang&entry.1292438233=%20%20Understanding%20the%20world%20in%20first-person%20view%20is%20fundamental%20in%20Augmented%0AReality%20%28AR%29.%20This%20immersive%20perspective%20brings%20dramatic%20visual%20changes%20and%0Aunique%20challenges%20compared%20to%20third-person%20views.%20Synthetic%20data%20has%20empowered%0Athird-person-view%20vision%20models%2C%20but%20its%20application%20to%20embodied%20egocentric%0Aperception%20tasks%20remains%20largely%20unexplored.%20A%20critical%20challenge%20lies%20in%0Asimulating%20natural%20human%20movements%20and%20behaviors%20that%20effectively%20steer%20the%0Aembodied%20cameras%20to%20capture%20a%20faithful%20egocentric%20representation%20of%20the%203D%0Aworld.%20To%20address%20this%20challenge%2C%20we%20introduce%20EgoGen%2C%20a%20new%20synthetic%20data%0Agenerator%20that%20can%20produce%20accurate%20and%20rich%20ground-truth%20training%20data%20for%0Aegocentric%20perception%20tasks.%20At%20the%20heart%20of%20EgoGen%20is%20a%20novel%20human%20motion%0Asynthesis%20model%20that%20directly%20leverages%20egocentric%20visual%20inputs%20of%20a%20virtual%0Ahuman%20to%20sense%20the%203D%20environment.%20Combined%20with%20collision-avoiding%20motion%0Aprimitives%20and%20a%20two-stage%20reinforcement%20learning%20approach%2C%20our%20motion%0Asynthesis%20model%20offers%20a%20closed-loop%20solution%20where%20the%20embodied%20perception%20and%0Amovement%20of%20the%20virtual%20human%20are%20seamlessly%20coupled.%20Compared%20to%20previous%0Aworks%2C%20our%20model%20eliminates%20the%20need%20for%20a%20pre-defined%20global%20path%2C%20and%20is%0Adirectly%20applicable%20to%20dynamic%20environments.%20Combined%20with%20our%20easy-to-use%20and%0Ascalable%20data%20generation%20pipeline%2C%20we%20demonstrate%20EgoGen%27s%20efficacy%20in%20three%0Atasks%3A%20mapping%20and%20localization%20for%20head-mounted%20cameras%2C%20egocentric%20camera%0Atracking%2C%20and%20human%20mesh%20recovery%20from%20egocentric%20views.%20EgoGen%20will%20be%20fully%0Aopen-sourced%2C%20offering%20a%20practical%20solution%20for%20creating%20realistic%20egocentric%0Atraining%20data%20and%20aiming%20to%20serve%20as%20a%20useful%20tool%20for%20egocentric%20computer%0Avision%20research.%20Refer%20to%20our%20project%20page%3A%20https%3A//ego-gen.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08739v2&entry.124074799=Read"},
{"title": "PRAM: Place Recognition Anywhere Model for Efficient Visual Localization", "author": "Fei Xue and Ignas Budvytis and Roberto Cipolla", "abstract": "  Humans localize themselves efficiently in known environments by first\nrecognizing landmarks defined on certain objects and their spatial\nrelationships, and then verifying the location by aligning detailed structures\nof recognized objects with those in the memory. Inspired by this, we propose\nthe place recognition anywhere model (PRAM) to perform visual localization as\nefficiently as humans do. PRAM consists of two main components - recognition\nand registration. In detail, first of all, a self-supervised map-centric\nlandmark definition strategy is adopted, making places in either indoor or\noutdoor scenes act as unique landmarks. Then, sparse keypoints extracted from\nimages, are utilized as the input to a transformer-based deep neural network\nfor landmark recognition; these keypoints enable PRAM to recognize hundreds of\nlandmarks with high time and memory efficiency. Keypoints along with recognized\nlandmark labels are further used for registration between query images and the\n3D landmark map. Different from previous hierarchical methods, PRAM discards\nglobal and local descriptors, and reduces over 90% storage. Since PRAM utilizes\nrecognition and landmark-wise verification to replace global reference search\nand exhaustive matching respectively, it runs 2.4 times faster than prior\nstate-of-the-art approaches. Moreover, PRAM opens new directions for visual\nlocalization including multi-modality localization, map-centric feature\nlearning, and hierarchical scene coordinate regression.\n", "link": "http://arxiv.org/abs/2404.07785v1", "date": "2024-04-11", "relevancy": 3.0026, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6396}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5948}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5672}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PRAM%3A%20Place%20Recognition%20Anywhere%20Model%20for%20Efficient%20Visual%20Localization&body=Title%3A%20PRAM%3A%20Place%20Recognition%20Anywhere%20Model%20for%20Efficient%20Visual%20Localization%0AAuthor%3A%20Fei%20Xue%20and%20Ignas%20Budvytis%20and%20Roberto%20Cipolla%0AAbstract%3A%20%20%20Humans%20localize%20themselves%20efficiently%20in%20known%20environments%20by%20first%0Arecognizing%20landmarks%20defined%20on%20certain%20objects%20and%20their%20spatial%0Arelationships%2C%20and%20then%20verifying%20the%20location%20by%20aligning%20detailed%20structures%0Aof%20recognized%20objects%20with%20those%20in%20the%20memory.%20Inspired%20by%20this%2C%20we%20propose%0Athe%20place%20recognition%20anywhere%20model%20%28PRAM%29%20to%20perform%20visual%20localization%20as%0Aefficiently%20as%20humans%20do.%20PRAM%20consists%20of%20two%20main%20components%20-%20recognition%0Aand%20registration.%20In%20detail%2C%20first%20of%20all%2C%20a%20self-supervised%20map-centric%0Alandmark%20definition%20strategy%20is%20adopted%2C%20making%20places%20in%20either%20indoor%20or%0Aoutdoor%20scenes%20act%20as%20unique%20landmarks.%20Then%2C%20sparse%20keypoints%20extracted%20from%0Aimages%2C%20are%20utilized%20as%20the%20input%20to%20a%20transformer-based%20deep%20neural%20network%0Afor%20landmark%20recognition%3B%20these%20keypoints%20enable%20PRAM%20to%20recognize%20hundreds%20of%0Alandmarks%20with%20high%20time%20and%20memory%20efficiency.%20Keypoints%20along%20with%20recognized%0Alandmark%20labels%20are%20further%20used%20for%20registration%20between%20query%20images%20and%20the%0A3D%20landmark%20map.%20Different%20from%20previous%20hierarchical%20methods%2C%20PRAM%20discards%0Aglobal%20and%20local%20descriptors%2C%20and%20reduces%20over%2090%25%20storage.%20Since%20PRAM%20utilizes%0Arecognition%20and%20landmark-wise%20verification%20to%20replace%20global%20reference%20search%0Aand%20exhaustive%20matching%20respectively%2C%20it%20runs%202.4%20times%20faster%20than%20prior%0Astate-of-the-art%20approaches.%20Moreover%2C%20PRAM%20opens%20new%20directions%20for%20visual%0Alocalization%20including%20multi-modality%20localization%2C%20map-centric%20feature%0Alearning%2C%20and%20hierarchical%20scene%20coordinate%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07785v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRAM%3A%20Place%20Recognition%20Anywhere%20Model%20for%20Efficient%20Visual%20Localization&entry.906535625=Fei%20Xue%20and%20Ignas%20Budvytis%20and%20Roberto%20Cipolla&entry.1292438233=%20%20Humans%20localize%20themselves%20efficiently%20in%20known%20environments%20by%20first%0Arecognizing%20landmarks%20defined%20on%20certain%20objects%20and%20their%20spatial%0Arelationships%2C%20and%20then%20verifying%20the%20location%20by%20aligning%20detailed%20structures%0Aof%20recognized%20objects%20with%20those%20in%20the%20memory.%20Inspired%20by%20this%2C%20we%20propose%0Athe%20place%20recognition%20anywhere%20model%20%28PRAM%29%20to%20perform%20visual%20localization%20as%0Aefficiently%20as%20humans%20do.%20PRAM%20consists%20of%20two%20main%20components%20-%20recognition%0Aand%20registration.%20In%20detail%2C%20first%20of%20all%2C%20a%20self-supervised%20map-centric%0Alandmark%20definition%20strategy%20is%20adopted%2C%20making%20places%20in%20either%20indoor%20or%0Aoutdoor%20scenes%20act%20as%20unique%20landmarks.%20Then%2C%20sparse%20keypoints%20extracted%20from%0Aimages%2C%20are%20utilized%20as%20the%20input%20to%20a%20transformer-based%20deep%20neural%20network%0Afor%20landmark%20recognition%3B%20these%20keypoints%20enable%20PRAM%20to%20recognize%20hundreds%20of%0Alandmarks%20with%20high%20time%20and%20memory%20efficiency.%20Keypoints%20along%20with%20recognized%0Alandmark%20labels%20are%20further%20used%20for%20registration%20between%20query%20images%20and%20the%0A3D%20landmark%20map.%20Different%20from%20previous%20hierarchical%20methods%2C%20PRAM%20discards%0Aglobal%20and%20local%20descriptors%2C%20and%20reduces%20over%2090%25%20storage.%20Since%20PRAM%20utilizes%0Arecognition%20and%20landmark-wise%20verification%20to%20replace%20global%20reference%20search%0Aand%20exhaustive%20matching%20respectively%2C%20it%20runs%202.4%20times%20faster%20than%20prior%0Astate-of-the-art%20approaches.%20Moreover%2C%20PRAM%20opens%20new%20directions%20for%20visual%0Alocalization%20including%20multi-modality%20localization%2C%20map-centric%20feature%0Alearning%2C%20and%20hierarchical%20scene%20coordinate%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07785v1&entry.124074799=Read"},
{"title": "Gaga: Group Any Gaussians via 3D-aware Memory Bank", "author": "Weijie Lyu and Xueting Li and Abhijit Kundu and Yi-Hsuan Tsai and Ming-Hsuan Yang", "abstract": "  We introduce Gaga, a framework that reconstructs and segments open-world 3D\nscenes by leveraging inconsistent 2D masks predicted by zero-shot segmentation\nmodels. Contrasted to prior 3D scene segmentation approaches that heavily rely\non video object tracking, Gaga utilizes spatial information and effectively\nassociates object masks across diverse camera poses. By eliminating the\nassumption of continuous view changes in training images, Gaga demonstrates\nrobustness to variations in camera poses, particularly beneficial for sparsely\nsampled images, ensuring precise mask label consistency. Furthermore, Gaga\naccommodates 2D segmentation masks from diverse sources and demonstrates robust\nperformance with different open-world zero-shot segmentation models, enhancing\nits versatility. Extensive qualitative and quantitative evaluations demonstrate\nthat Gaga performs favorably against state-of-the-art methods, emphasizing its\npotential for real-world applications such as scene understanding and\nmanipulation.\n", "link": "http://arxiv.org/abs/2404.07977v1", "date": "2024-04-11", "relevancy": 2.7128, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5517}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5411}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5349}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gaga%3A%20Group%20Any%20Gaussians%20via%203D-aware%20Memory%20Bank&body=Title%3A%20Gaga%3A%20Group%20Any%20Gaussians%20via%203D-aware%20Memory%20Bank%0AAuthor%3A%20Weijie%20Lyu%20and%20Xueting%20Li%20and%20Abhijit%20Kundu%20and%20Yi-Hsuan%20Tsai%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20We%20introduce%20Gaga%2C%20a%20framework%20that%20reconstructs%20and%20segments%20open-world%203D%0Ascenes%20by%20leveraging%20inconsistent%202D%20masks%20predicted%20by%20zero-shot%20segmentation%0Amodels.%20Contrasted%20to%20prior%203D%20scene%20segmentation%20approaches%20that%20heavily%20rely%0Aon%20video%20object%20tracking%2C%20Gaga%20utilizes%20spatial%20information%20and%20effectively%0Aassociates%20object%20masks%20across%20diverse%20camera%20poses.%20By%20eliminating%20the%0Aassumption%20of%20continuous%20view%20changes%20in%20training%20images%2C%20Gaga%20demonstrates%0Arobustness%20to%20variations%20in%20camera%20poses%2C%20particularly%20beneficial%20for%20sparsely%0Asampled%20images%2C%20ensuring%20precise%20mask%20label%20consistency.%20Furthermore%2C%20Gaga%0Aaccommodates%202D%20segmentation%20masks%20from%20diverse%20sources%20and%20demonstrates%20robust%0Aperformance%20with%20different%20open-world%20zero-shot%20segmentation%20models%2C%20enhancing%0Aits%20versatility.%20Extensive%20qualitative%20and%20quantitative%20evaluations%20demonstrate%0Athat%20Gaga%20performs%20favorably%20against%20state-of-the-art%20methods%2C%20emphasizing%20its%0Apotential%20for%20real-world%20applications%20such%20as%20scene%20understanding%20and%0Amanipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07977v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaga%3A%20Group%20Any%20Gaussians%20via%203D-aware%20Memory%20Bank&entry.906535625=Weijie%20Lyu%20and%20Xueting%20Li%20and%20Abhijit%20Kundu%20and%20Yi-Hsuan%20Tsai%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20We%20introduce%20Gaga%2C%20a%20framework%20that%20reconstructs%20and%20segments%20open-world%203D%0Ascenes%20by%20leveraging%20inconsistent%202D%20masks%20predicted%20by%20zero-shot%20segmentation%0Amodels.%20Contrasted%20to%20prior%203D%20scene%20segmentation%20approaches%20that%20heavily%20rely%0Aon%20video%20object%20tracking%2C%20Gaga%20utilizes%20spatial%20information%20and%20effectively%0Aassociates%20object%20masks%20across%20diverse%20camera%20poses.%20By%20eliminating%20the%0Aassumption%20of%20continuous%20view%20changes%20in%20training%20images%2C%20Gaga%20demonstrates%0Arobustness%20to%20variations%20in%20camera%20poses%2C%20particularly%20beneficial%20for%20sparsely%0Asampled%20images%2C%20ensuring%20precise%20mask%20label%20consistency.%20Furthermore%2C%20Gaga%0Aaccommodates%202D%20segmentation%20masks%20from%20diverse%20sources%20and%20demonstrates%20robust%0Aperformance%20with%20different%20open-world%20zero-shot%20segmentation%20models%2C%20enhancing%0Aits%20versatility.%20Extensive%20qualitative%20and%20quantitative%20evaluations%20demonstrate%0Athat%20Gaga%20performs%20favorably%20against%20state-of-the-art%20methods%2C%20emphasizing%20its%0Apotential%20for%20real-world%20applications%20such%20as%20scene%20understanding%20and%0Amanipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07977v1&entry.124074799=Read"},
{"title": "RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric\n  Stereo Network", "author": "Kai Luo and Yakun Ju and Lin Qi and Kaixuan Wang and Junyu Dong", "abstract": "  Predicting accurate normal maps of objects from two-dimensional images in\nregions of complex structure and spatial material variations is challenging\nusing photometric stereo methods due to the influence of surface reflection\nproperties caused by variations in object geometry and surface materials. To\naddress this issue, we propose a photometric stereo network called a RMAFF-PSN\nthat uses residual multiscale attentional feature fusion to handle the\n``difficult'' regions of the object. Unlike previous approaches that only use\nstacked convolutional layers to extract deep features from the input image, our\nmethod integrates feature information from different resolution stages and\nscales of the image. This approach preserves more physical information, such as\ntexture and geometry of the object in complex regions, through shallow-deep\nstage feature extraction, double branching enhancement, and attention\noptimization. To test the network structure under real-world conditions, we\npropose a new real dataset called Simple PS data, which contains multiple\nobjects with varying structures and materials. Experimental results on a\npublicly available benchmark dataset demonstrate that our method outperforms\nmost existing calibrated photometric stereo methods for the same number of\ninput images, especially in the case of highly non-convex object structures.\nOur method also obtains good results under sparse lighting conditions.\n", "link": "http://arxiv.org/abs/2404.07766v1", "date": "2024-04-11", "relevancy": 2.687, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5432}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5391}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5299}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RMAFF-PSN%3A%20A%20Residual%20Multi-Scale%20Attention%20Feature%20Fusion%20Photometric%0A%20%20Stereo%20Network&body=Title%3A%20RMAFF-PSN%3A%20A%20Residual%20Multi-Scale%20Attention%20Feature%20Fusion%20Photometric%0A%20%20Stereo%20Network%0AAuthor%3A%20Kai%20Luo%20and%20Yakun%20Ju%20and%20Lin%20Qi%20and%20Kaixuan%20Wang%20and%20Junyu%20Dong%0AAbstract%3A%20%20%20Predicting%20accurate%20normal%20maps%20of%20objects%20from%20two-dimensional%20images%20in%0Aregions%20of%20complex%20structure%20and%20spatial%20material%20variations%20is%20challenging%0Ausing%20photometric%20stereo%20methods%20due%20to%20the%20influence%20of%20surface%20reflection%0Aproperties%20caused%20by%20variations%20in%20object%20geometry%20and%20surface%20materials.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20photometric%20stereo%20network%20called%20a%20RMAFF-PSN%0Athat%20uses%20residual%20multiscale%20attentional%20feature%20fusion%20to%20handle%20the%0A%60%60difficult%27%27%20regions%20of%20the%20object.%20Unlike%20previous%20approaches%20that%20only%20use%0Astacked%20convolutional%20layers%20to%20extract%20deep%20features%20from%20the%20input%20image%2C%20our%0Amethod%20integrates%20feature%20information%20from%20different%20resolution%20stages%20and%0Ascales%20of%20the%20image.%20This%20approach%20preserves%20more%20physical%20information%2C%20such%20as%0Atexture%20and%20geometry%20of%20the%20object%20in%20complex%20regions%2C%20through%20shallow-deep%0Astage%20feature%20extraction%2C%20double%20branching%20enhancement%2C%20and%20attention%0Aoptimization.%20To%20test%20the%20network%20structure%20under%20real-world%20conditions%2C%20we%0Apropose%20a%20new%20real%20dataset%20called%20Simple%20PS%20data%2C%20which%20contains%20multiple%0Aobjects%20with%20varying%20structures%20and%20materials.%20Experimental%20results%20on%20a%0Apublicly%20available%20benchmark%20dataset%20demonstrate%20that%20our%20method%20outperforms%0Amost%20existing%20calibrated%20photometric%20stereo%20methods%20for%20the%20same%20number%20of%0Ainput%20images%2C%20especially%20in%20the%20case%20of%20highly%20non-convex%20object%20structures.%0AOur%20method%20also%20obtains%20good%20results%20under%20sparse%20lighting%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07766v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RMAFF-PSN%3A%20A%20Residual%20Multi-Scale%20Attention%20Feature%20Fusion%20Photometric%0A%20%20Stereo%20Network&entry.906535625=Kai%20Luo%20and%20Yakun%20Ju%20and%20Lin%20Qi%20and%20Kaixuan%20Wang%20and%20Junyu%20Dong&entry.1292438233=%20%20Predicting%20accurate%20normal%20maps%20of%20objects%20from%20two-dimensional%20images%20in%0Aregions%20of%20complex%20structure%20and%20spatial%20material%20variations%20is%20challenging%0Ausing%20photometric%20stereo%20methods%20due%20to%20the%20influence%20of%20surface%20reflection%0Aproperties%20caused%20by%20variations%20in%20object%20geometry%20and%20surface%20materials.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20photometric%20stereo%20network%20called%20a%20RMAFF-PSN%0Athat%20uses%20residual%20multiscale%20attentional%20feature%20fusion%20to%20handle%20the%0A%60%60difficult%27%27%20regions%20of%20the%20object.%20Unlike%20previous%20approaches%20that%20only%20use%0Astacked%20convolutional%20layers%20to%20extract%20deep%20features%20from%20the%20input%20image%2C%20our%0Amethod%20integrates%20feature%20information%20from%20different%20resolution%20stages%20and%0Ascales%20of%20the%20image.%20This%20approach%20preserves%20more%20physical%20information%2C%20such%20as%0Atexture%20and%20geometry%20of%20the%20object%20in%20complex%20regions%2C%20through%20shallow-deep%0Astage%20feature%20extraction%2C%20double%20branching%20enhancement%2C%20and%20attention%0Aoptimization.%20To%20test%20the%20network%20structure%20under%20real-world%20conditions%2C%20we%0Apropose%20a%20new%20real%20dataset%20called%20Simple%20PS%20data%2C%20which%20contains%20multiple%0Aobjects%20with%20varying%20structures%20and%20materials.%20Experimental%20results%20on%20a%0Apublicly%20available%20benchmark%20dataset%20demonstrate%20that%20our%20method%20outperforms%0Amost%20existing%20calibrated%20photometric%20stereo%20methods%20for%20the%20same%20number%20of%0Ainput%20images%2C%20especially%20in%20the%20case%20of%20highly%20non-convex%20object%20structures.%0AOur%20method%20also%20obtains%20good%20results%20under%20sparse%20lighting%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07766v1&entry.124074799=Read"},
{"title": "Homography Guided Temporal Fusion for Road Line and Marking Segmentation", "author": "Shan Wang and Chuong Nguyen and Jiawei Liu and Kaihao Zhang and Wenhan Luo and Yanhao Zhang and Sundaram Muthu and Fahira Afzal Maken and Hongdong Li", "abstract": "  Reliable segmentation of road lines and markings is critical to autonomous\ndriving. Our work is motivated by the observations that road lines and markings\nare (1) frequently occluded in the presence of moving vehicles, shadow, and\nglare and (2) highly structured with low intra-class shape variance and overall\nhigh appearance consistency. To solve these issues, we propose a Homography\nGuided Fusion (HomoFusion) module to exploit temporally-adjacent video frames\nfor complementary cues facilitating the correct classification of the partially\noccluded road lines or markings. To reduce computational complexity, a novel\nsurface normal estimator is proposed to establish spatial correspondences\nbetween the sampled frames, allowing the HomoFusion module to perform a\npixel-to-pixel attention mechanism in updating the representation of the\noccluded road lines or markings. Experiments on ApolloScape, a large-scale lane\nmark segmentation dataset, and ApolloScape Night with artificial simulated\nnight-time road conditions, demonstrate that our method outperforms other\nexisting SOTA lane mark segmentation models with less than 9\\% of their\nparameters and computational complexity. We show that exploiting available\ncamera intrinsic data and ground plane assumption for cross-frame\ncorrespondence can lead to a light-weight network with significantly improved\nperformances in speed and accuracy. We also prove the versatility of our\nHomoFusion approach by applying it to the problem of water puddle segmentation\nand achieving SOTA performance.\n", "link": "http://arxiv.org/abs/2404.07626v1", "date": "2024-04-11", "relevancy": 2.682, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5229}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Homography%20Guided%20Temporal%20Fusion%20for%20Road%20Line%20and%20Marking%20Segmentation&body=Title%3A%20Homography%20Guided%20Temporal%20Fusion%20for%20Road%20Line%20and%20Marking%20Segmentation%0AAuthor%3A%20Shan%20Wang%20and%20Chuong%20Nguyen%20and%20Jiawei%20Liu%20and%20Kaihao%20Zhang%20and%20Wenhan%20Luo%20and%20Yanhao%20Zhang%20and%20Sundaram%20Muthu%20and%20Fahira%20Afzal%20Maken%20and%20Hongdong%20Li%0AAbstract%3A%20%20%20Reliable%20segmentation%20of%20road%20lines%20and%20markings%20is%20critical%20to%20autonomous%0Adriving.%20Our%20work%20is%20motivated%20by%20the%20observations%20that%20road%20lines%20and%20markings%0Aare%20%281%29%20frequently%20occluded%20in%20the%20presence%20of%20moving%20vehicles%2C%20shadow%2C%20and%0Aglare%20and%20%282%29%20highly%20structured%20with%20low%20intra-class%20shape%20variance%20and%20overall%0Ahigh%20appearance%20consistency.%20To%20solve%20these%20issues%2C%20we%20propose%20a%20Homography%0AGuided%20Fusion%20%28HomoFusion%29%20module%20to%20exploit%20temporally-adjacent%20video%20frames%0Afor%20complementary%20cues%20facilitating%20the%20correct%20classification%20of%20the%20partially%0Aoccluded%20road%20lines%20or%20markings.%20To%20reduce%20computational%20complexity%2C%20a%20novel%0Asurface%20normal%20estimator%20is%20proposed%20to%20establish%20spatial%20correspondences%0Abetween%20the%20sampled%20frames%2C%20allowing%20the%20HomoFusion%20module%20to%20perform%20a%0Apixel-to-pixel%20attention%20mechanism%20in%20updating%20the%20representation%20of%20the%0Aoccluded%20road%20lines%20or%20markings.%20Experiments%20on%20ApolloScape%2C%20a%20large-scale%20lane%0Amark%20segmentation%20dataset%2C%20and%20ApolloScape%20Night%20with%20artificial%20simulated%0Anight-time%20road%20conditions%2C%20demonstrate%20that%20our%20method%20outperforms%20other%0Aexisting%20SOTA%20lane%20mark%20segmentation%20models%20with%20less%20than%209%5C%25%20of%20their%0Aparameters%20and%20computational%20complexity.%20We%20show%20that%20exploiting%20available%0Acamera%20intrinsic%20data%20and%20ground%20plane%20assumption%20for%20cross-frame%0Acorrespondence%20can%20lead%20to%20a%20light-weight%20network%20with%20significantly%20improved%0Aperformances%20in%20speed%20and%20accuracy.%20We%20also%20prove%20the%20versatility%20of%20our%0AHomoFusion%20approach%20by%20applying%20it%20to%20the%20problem%20of%20water%20puddle%20segmentation%0Aand%20achieving%20SOTA%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07626v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Homography%20Guided%20Temporal%20Fusion%20for%20Road%20Line%20and%20Marking%20Segmentation&entry.906535625=Shan%20Wang%20and%20Chuong%20Nguyen%20and%20Jiawei%20Liu%20and%20Kaihao%20Zhang%20and%20Wenhan%20Luo%20and%20Yanhao%20Zhang%20and%20Sundaram%20Muthu%20and%20Fahira%20Afzal%20Maken%20and%20Hongdong%20Li&entry.1292438233=%20%20Reliable%20segmentation%20of%20road%20lines%20and%20markings%20is%20critical%20to%20autonomous%0Adriving.%20Our%20work%20is%20motivated%20by%20the%20observations%20that%20road%20lines%20and%20markings%0Aare%20%281%29%20frequently%20occluded%20in%20the%20presence%20of%20moving%20vehicles%2C%20shadow%2C%20and%0Aglare%20and%20%282%29%20highly%20structured%20with%20low%20intra-class%20shape%20variance%20and%20overall%0Ahigh%20appearance%20consistency.%20To%20solve%20these%20issues%2C%20we%20propose%20a%20Homography%0AGuided%20Fusion%20%28HomoFusion%29%20module%20to%20exploit%20temporally-adjacent%20video%20frames%0Afor%20complementary%20cues%20facilitating%20the%20correct%20classification%20of%20the%20partially%0Aoccluded%20road%20lines%20or%20markings.%20To%20reduce%20computational%20complexity%2C%20a%20novel%0Asurface%20normal%20estimator%20is%20proposed%20to%20establish%20spatial%20correspondences%0Abetween%20the%20sampled%20frames%2C%20allowing%20the%20HomoFusion%20module%20to%20perform%20a%0Apixel-to-pixel%20attention%20mechanism%20in%20updating%20the%20representation%20of%20the%0Aoccluded%20road%20lines%20or%20markings.%20Experiments%20on%20ApolloScape%2C%20a%20large-scale%20lane%0Amark%20segmentation%20dataset%2C%20and%20ApolloScape%20Night%20with%20artificial%20simulated%0Anight-time%20road%20conditions%2C%20demonstrate%20that%20our%20method%20outperforms%20other%0Aexisting%20SOTA%20lane%20mark%20segmentation%20models%20with%20less%20than%209%5C%25%20of%20their%0Aparameters%20and%20computational%20complexity.%20We%20show%20that%20exploiting%20available%0Acamera%20intrinsic%20data%20and%20ground%20plane%20assumption%20for%20cross-frame%0Acorrespondence%20can%20lead%20to%20a%20light-weight%20network%20with%20significantly%20improved%0Aperformances%20in%20speed%20and%20accuracy.%20We%20also%20prove%20the%20versatility%20of%20our%0AHomoFusion%20approach%20by%20applying%20it%20to%20the%20problem%20of%20water%20puddle%20segmentation%0Aand%20achieving%20SOTA%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07626v1&entry.124074799=Read"},
{"title": "Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning", "author": "Shiming Chen and Wenjin Hou and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Zero-shot learning (ZSL) recognizes the unseen classes by conducting\nvisual-semantic interactions to transfer semantic knowledge from seen classes\nto unseen ones, supported by semantic information (e.g., attributes). However,\nexisting ZSL methods simply extract visual features using a pre-trained network\nbackbone (i.e., CNN or ViT), which fail to learn matched visual-semantic\ncorrespondences for representing semantic-related visual features as lacking of\nthe guidance of semantic information, resulting in undesirable visual-semantic\ninteractions. To tackle this issue, we propose a progressive semantic-guided\nvision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly\nconsiders two properties in the whole network: i) discover the semantic-related\nvisual representations explicitly, and ii) discard the semantic-unrelated\nvisual information. Specifically, we first introduce semantic-embedded token\nlearning to improve the visual-semantic correspondences via semantic\nenhancement and discover the semantic-related visual tokens explicitly with\nsemantic-guided token attention. Then, we fuse low semantic-visual\ncorrespondence visual tokens to discard the semantic-unrelated visual\ninformation for visual enhancement. These two operations are integrated into\nvarious encoders to progressively learn semantic-related visual representations\nfor accurate visual-semantic interactions in ZSL. The extensive experiments\nshow that our ZSLViT achieves significant performance gains on three popular\nbenchmark datasets, i.e., CUB, SUN, and AWA2.\n", "link": "http://arxiv.org/abs/2404.07713v1", "date": "2024-04-11", "relevancy": 2.6654, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5702}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5249}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5042}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Progressive%20Semantic-Guided%20Vision%20Transformer%20for%20Zero-Shot%20Learning&body=Title%3A%20Progressive%20Semantic-Guided%20Vision%20Transformer%20for%20Zero-Shot%20Learning%0AAuthor%3A%20Shiming%20Chen%20and%20Wenjin%20Hou%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Zero-shot%20learning%20%28ZSL%29%20recognizes%20the%20unseen%20classes%20by%20conducting%0Avisual-semantic%20interactions%20to%20transfer%20semantic%20knowledge%20from%20seen%20classes%0Ato%20unseen%20ones%2C%20supported%20by%20semantic%20information%20%28e.g.%2C%20attributes%29.%20However%2C%0Aexisting%20ZSL%20methods%20simply%20extract%20visual%20features%20using%20a%20pre-trained%20network%0Abackbone%20%28i.e.%2C%20CNN%20or%20ViT%29%2C%20which%20fail%20to%20learn%20matched%20visual-semantic%0Acorrespondences%20for%20representing%20semantic-related%20visual%20features%20as%20lacking%20of%0Athe%20guidance%20of%20semantic%20information%2C%20resulting%20in%20undesirable%20visual-semantic%0Ainteractions.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20progressive%20semantic-guided%0Avision%20transformer%20for%20zero-shot%20learning%20%28dubbed%20ZSLViT%29.%20ZSLViT%20mainly%0Aconsiders%20two%20properties%20in%20the%20whole%20network%3A%20i%29%20discover%20the%20semantic-related%0Avisual%20representations%20explicitly%2C%20and%20ii%29%20discard%20the%20semantic-unrelated%0Avisual%20information.%20Specifically%2C%20we%20first%20introduce%20semantic-embedded%20token%0Alearning%20to%20improve%20the%20visual-semantic%20correspondences%20via%20semantic%0Aenhancement%20and%20discover%20the%20semantic-related%20visual%20tokens%20explicitly%20with%0Asemantic-guided%20token%20attention.%20Then%2C%20we%20fuse%20low%20semantic-visual%0Acorrespondence%20visual%20tokens%20to%20discard%20the%20semantic-unrelated%20visual%0Ainformation%20for%20visual%20enhancement.%20These%20two%20operations%20are%20integrated%20into%0Avarious%20encoders%20to%20progressively%20learn%20semantic-related%20visual%20representations%0Afor%20accurate%20visual-semantic%20interactions%20in%20ZSL.%20The%20extensive%20experiments%0Ashow%20that%20our%20ZSLViT%20achieves%20significant%20performance%20gains%20on%20three%20popular%0Abenchmark%20datasets%2C%20i.e.%2C%20CUB%2C%20SUN%2C%20and%20AWA2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07713v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Semantic-Guided%20Vision%20Transformer%20for%20Zero-Shot%20Learning&entry.906535625=Shiming%20Chen%20and%20Wenjin%20Hou%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Zero-shot%20learning%20%28ZSL%29%20recognizes%20the%20unseen%20classes%20by%20conducting%0Avisual-semantic%20interactions%20to%20transfer%20semantic%20knowledge%20from%20seen%20classes%0Ato%20unseen%20ones%2C%20supported%20by%20semantic%20information%20%28e.g.%2C%20attributes%29.%20However%2C%0Aexisting%20ZSL%20methods%20simply%20extract%20visual%20features%20using%20a%20pre-trained%20network%0Abackbone%20%28i.e.%2C%20CNN%20or%20ViT%29%2C%20which%20fail%20to%20learn%20matched%20visual-semantic%0Acorrespondences%20for%20representing%20semantic-related%20visual%20features%20as%20lacking%20of%0Athe%20guidance%20of%20semantic%20information%2C%20resulting%20in%20undesirable%20visual-semantic%0Ainteractions.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20progressive%20semantic-guided%0Avision%20transformer%20for%20zero-shot%20learning%20%28dubbed%20ZSLViT%29.%20ZSLViT%20mainly%0Aconsiders%20two%20properties%20in%20the%20whole%20network%3A%20i%29%20discover%20the%20semantic-related%0Avisual%20representations%20explicitly%2C%20and%20ii%29%20discard%20the%20semantic-unrelated%0Avisual%20information.%20Specifically%2C%20we%20first%20introduce%20semantic-embedded%20token%0Alearning%20to%20improve%20the%20visual-semantic%20correspondences%20via%20semantic%0Aenhancement%20and%20discover%20the%20semantic-related%20visual%20tokens%20explicitly%20with%0Asemantic-guided%20token%20attention.%20Then%2C%20we%20fuse%20low%20semantic-visual%0Acorrespondence%20visual%20tokens%20to%20discard%20the%20semantic-unrelated%20visual%0Ainformation%20for%20visual%20enhancement.%20These%20two%20operations%20are%20integrated%20into%0Avarious%20encoders%20to%20progressively%20learn%20semantic-related%20visual%20representations%0Afor%20accurate%20visual-semantic%20interactions%20in%20ZSL.%20The%20extensive%20experiments%0Ashow%20that%20our%20ZSLViT%20achieves%20significant%20performance%20gains%20on%20three%20popular%0Abenchmark%20datasets%2C%20i.e.%2C%20CUB%2C%20SUN%2C%20and%20AWA2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07713v1&entry.124074799=Read"},
{"title": "Sparse Laneformer", "author": "Ji Liu and Zifeng Zhang and Mingjie Lu and Hongyang Wei and Dong Li and Yile Xie and Jinzhang Peng and Lu Tian and Ashish Sirasao and Emad Barsoum", "abstract": "  Lane detection is a fundamental task in autonomous driving, and has achieved\ngreat progress as deep learning emerges. Previous anchor-based methods often\ndesign dense anchors, which highly depend on the training dataset and remain\nfixed during inference. We analyze that dense anchors are not necessary for\nlane detection, and propose a transformer-based lane detection framework based\non a sparse anchor mechanism. To this end, we generate sparse anchors with\nposition-aware lane queries and angle queries instead of traditional explicit\nanchors. We adopt Horizontal Perceptual Attention (HPA) to aggregate the lane\nfeatures along the horizontal direction, and adopt Lane-Angle Cross Attention\n(LACA) to perform interactions between lane queries and angle queries. We also\npropose Lane Perceptual Attention (LPA) based on deformable cross attention to\nfurther refine the lane predictions. Our method, named Sparse Laneformer, is\neasy-to-implement and end-to-end trainable. Extensive experiments demonstrate\nthat Sparse Laneformer performs favorably against the state-of-the-art methods,\ne.g., surpassing Laneformer by 3.0% F1 score and O2SFormer by 0.7% F1 score\nwith fewer MACs on CULane with the same ResNet-34 backbone.\n", "link": "http://arxiv.org/abs/2404.07821v1", "date": "2024-04-11", "relevancy": 2.6225, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5105}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5063}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sparse%20Laneformer&body=Title%3A%20Sparse%20Laneformer%0AAuthor%3A%20Ji%20Liu%20and%20Zifeng%20Zhang%20and%20Mingjie%20Lu%20and%20Hongyang%20Wei%20and%20Dong%20Li%20and%20Yile%20Xie%20and%20Jinzhang%20Peng%20and%20Lu%20Tian%20and%20Ashish%20Sirasao%20and%20Emad%20Barsoum%0AAbstract%3A%20%20%20Lane%20detection%20is%20a%20fundamental%20task%20in%20autonomous%20driving%2C%20and%20has%20achieved%0Agreat%20progress%20as%20deep%20learning%20emerges.%20Previous%20anchor-based%20methods%20often%0Adesign%20dense%20anchors%2C%20which%20highly%20depend%20on%20the%20training%20dataset%20and%20remain%0Afixed%20during%20inference.%20We%20analyze%20that%20dense%20anchors%20are%20not%20necessary%20for%0Alane%20detection%2C%20and%20propose%20a%20transformer-based%20lane%20detection%20framework%20based%0Aon%20a%20sparse%20anchor%20mechanism.%20To%20this%20end%2C%20we%20generate%20sparse%20anchors%20with%0Aposition-aware%20lane%20queries%20and%20angle%20queries%20instead%20of%20traditional%20explicit%0Aanchors.%20We%20adopt%20Horizontal%20Perceptual%20Attention%20%28HPA%29%20to%20aggregate%20the%20lane%0Afeatures%20along%20the%20horizontal%20direction%2C%20and%20adopt%20Lane-Angle%20Cross%20Attention%0A%28LACA%29%20to%20perform%20interactions%20between%20lane%20queries%20and%20angle%20queries.%20We%20also%0Apropose%20Lane%20Perceptual%20Attention%20%28LPA%29%20based%20on%20deformable%20cross%20attention%20to%0Afurther%20refine%20the%20lane%20predictions.%20Our%20method%2C%20named%20Sparse%20Laneformer%2C%20is%0Aeasy-to-implement%20and%20end-to-end%20trainable.%20Extensive%20experiments%20demonstrate%0Athat%20Sparse%20Laneformer%20performs%20favorably%20against%20the%20state-of-the-art%20methods%2C%0Ae.g.%2C%20surpassing%20Laneformer%20by%203.0%25%20F1%20score%20and%20O2SFormer%20by%200.7%25%20F1%20score%0Awith%20fewer%20MACs%20on%20CULane%20with%20the%20same%20ResNet-34%20backbone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07821v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Laneformer&entry.906535625=Ji%20Liu%20and%20Zifeng%20Zhang%20and%20Mingjie%20Lu%20and%20Hongyang%20Wei%20and%20Dong%20Li%20and%20Yile%20Xie%20and%20Jinzhang%20Peng%20and%20Lu%20Tian%20and%20Ashish%20Sirasao%20and%20Emad%20Barsoum&entry.1292438233=%20%20Lane%20detection%20is%20a%20fundamental%20task%20in%20autonomous%20driving%2C%20and%20has%20achieved%0Agreat%20progress%20as%20deep%20learning%20emerges.%20Previous%20anchor-based%20methods%20often%0Adesign%20dense%20anchors%2C%20which%20highly%20depend%20on%20the%20training%20dataset%20and%20remain%0Afixed%20during%20inference.%20We%20analyze%20that%20dense%20anchors%20are%20not%20necessary%20for%0Alane%20detection%2C%20and%20propose%20a%20transformer-based%20lane%20detection%20framework%20based%0Aon%20a%20sparse%20anchor%20mechanism.%20To%20this%20end%2C%20we%20generate%20sparse%20anchors%20with%0Aposition-aware%20lane%20queries%20and%20angle%20queries%20instead%20of%20traditional%20explicit%0Aanchors.%20We%20adopt%20Horizontal%20Perceptual%20Attention%20%28HPA%29%20to%20aggregate%20the%20lane%0Afeatures%20along%20the%20horizontal%20direction%2C%20and%20adopt%20Lane-Angle%20Cross%20Attention%0A%28LACA%29%20to%20perform%20interactions%20between%20lane%20queries%20and%20angle%20queries.%20We%20also%0Apropose%20Lane%20Perceptual%20Attention%20%28LPA%29%20based%20on%20deformable%20cross%20attention%20to%0Afurther%20refine%20the%20lane%20predictions.%20Our%20method%2C%20named%20Sparse%20Laneformer%2C%20is%0Aeasy-to-implement%20and%20end-to-end%20trainable.%20Extensive%20experiments%20demonstrate%0Athat%20Sparse%20Laneformer%20performs%20favorably%20against%20the%20state-of-the-art%20methods%2C%0Ae.g.%2C%20surpassing%20Laneformer%20by%203.0%25%20F1%20score%20and%20O2SFormer%20by%200.7%25%20F1%20score%0Awith%20fewer%20MACs%20on%20CULane%20with%20the%20same%20ResNet-34%20backbone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07821v1&entry.124074799=Read"},
{"title": "Parameter Hierarchical Optimization for Visible-Infrared Person\n  Re-Identification", "author": "Zeng YU and Yunxiao Shi", "abstract": "  Visible-infrared person re-identification (VI-reID) aims at matching\ncross-modality pedestrian images captured by disjoint visible or infrared\ncameras. Existing methods alleviate the cross-modality discrepancies via\ndesigning different kinds of network architectures. Different from available\nmethods, in this paper, we propose a novel parameter optimizing paradigm,\nparameter hierarchical optimization (PHO) method, for the task of VI-ReID. It\nallows part of parameters to be directly optimized without any training, which\nnarrows the search space of parameters and makes the whole network more easier\nto be trained. Specifically, we first divide the parameters into different\ntypes, and then introduce a self-adaptive alignment strategy (SAS) to\nautomatically align the visible and infrared images through transformation.\nConsidering that features in different dimension have varying importance, we\ndevelop an auto-weighted alignment learning (AAL) module that can automatically\nweight features according to their importance. Importantly, in the alignment\nprocess of SAS and AAL, all the parameters are immediately optimized with\noptimization principles rather than training the whole network, which yields a\nbetter parameter training manner. Furthermore, we establish the cross-modality\nconsistent learning (CCL) loss to extract discriminative person representations\nwith translation consistency. We provide both theoretical justification and\nempirical evidence that our proposed PHO method outperform existing VI-reID\napproaches.\n", "link": "http://arxiv.org/abs/2404.07930v1", "date": "2024-04-11", "relevancy": 2.5747, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5196}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5156}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5096}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Parameter%20Hierarchical%20Optimization%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification&body=Title%3A%20Parameter%20Hierarchical%20Optimization%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Zeng%20YU%20and%20Yunxiao%20Shi%0AAbstract%3A%20%20%20Visible-infrared%20person%20re-identification%20%28VI-reID%29%20aims%20at%20matching%0Across-modality%20pedestrian%20images%20captured%20by%20disjoint%20visible%20or%20infrared%0Acameras.%20Existing%20methods%20alleviate%20the%20cross-modality%20discrepancies%20via%0Adesigning%20different%20kinds%20of%20network%20architectures.%20Different%20from%20available%0Amethods%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20parameter%20optimizing%20paradigm%2C%0Aparameter%20hierarchical%20optimization%20%28PHO%29%20method%2C%20for%20the%20task%20of%20VI-ReID.%20It%0Aallows%20part%20of%20parameters%20to%20be%20directly%20optimized%20without%20any%20training%2C%20which%0Anarrows%20the%20search%20space%20of%20parameters%20and%20makes%20the%20whole%20network%20more%20easier%0Ato%20be%20trained.%20Specifically%2C%20we%20first%20divide%20the%20parameters%20into%20different%0Atypes%2C%20and%20then%20introduce%20a%20self-adaptive%20alignment%20strategy%20%28SAS%29%20to%0Aautomatically%20align%20the%20visible%20and%20infrared%20images%20through%20transformation.%0AConsidering%20that%20features%20in%20different%20dimension%20have%20varying%20importance%2C%20we%0Adevelop%20an%20auto-weighted%20alignment%20learning%20%28AAL%29%20module%20that%20can%20automatically%0Aweight%20features%20according%20to%20their%20importance.%20Importantly%2C%20in%20the%20alignment%0Aprocess%20of%20SAS%20and%20AAL%2C%20all%20the%20parameters%20are%20immediately%20optimized%20with%0Aoptimization%20principles%20rather%20than%20training%20the%20whole%20network%2C%20which%20yields%20a%0Abetter%20parameter%20training%20manner.%20Furthermore%2C%20we%20establish%20the%20cross-modality%0Aconsistent%20learning%20%28CCL%29%20loss%20to%20extract%20discriminative%20person%20representations%0Awith%20translation%20consistency.%20We%20provide%20both%20theoretical%20justification%20and%0Aempirical%20evidence%20that%20our%20proposed%20PHO%20method%20outperform%20existing%20VI-reID%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07930v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter%20Hierarchical%20Optimization%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification&entry.906535625=Zeng%20YU%20and%20Yunxiao%20Shi&entry.1292438233=%20%20Visible-infrared%20person%20re-identification%20%28VI-reID%29%20aims%20at%20matching%0Across-modality%20pedestrian%20images%20captured%20by%20disjoint%20visible%20or%20infrared%0Acameras.%20Existing%20methods%20alleviate%20the%20cross-modality%20discrepancies%20via%0Adesigning%20different%20kinds%20of%20network%20architectures.%20Different%20from%20available%0Amethods%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20parameter%20optimizing%20paradigm%2C%0Aparameter%20hierarchical%20optimization%20%28PHO%29%20method%2C%20for%20the%20task%20of%20VI-ReID.%20It%0Aallows%20part%20of%20parameters%20to%20be%20directly%20optimized%20without%20any%20training%2C%20which%0Anarrows%20the%20search%20space%20of%20parameters%20and%20makes%20the%20whole%20network%20more%20easier%0Ato%20be%20trained.%20Specifically%2C%20we%20first%20divide%20the%20parameters%20into%20different%0Atypes%2C%20and%20then%20introduce%20a%20self-adaptive%20alignment%20strategy%20%28SAS%29%20to%0Aautomatically%20align%20the%20visible%20and%20infrared%20images%20through%20transformation.%0AConsidering%20that%20features%20in%20different%20dimension%20have%20varying%20importance%2C%20we%0Adevelop%20an%20auto-weighted%20alignment%20learning%20%28AAL%29%20module%20that%20can%20automatically%0Aweight%20features%20according%20to%20their%20importance.%20Importantly%2C%20in%20the%20alignment%0Aprocess%20of%20SAS%20and%20AAL%2C%20all%20the%20parameters%20are%20immediately%20optimized%20with%0Aoptimization%20principles%20rather%20than%20training%20the%20whole%20network%2C%20which%20yields%20a%0Abetter%20parameter%20training%20manner.%20Furthermore%2C%20we%20establish%20the%20cross-modality%0Aconsistent%20learning%20%28CCL%29%20loss%20to%20extract%20discriminative%20person%20representations%0Awith%20translation%20consistency.%20We%20provide%20both%20theoretical%20justification%20and%0Aempirical%20evidence%20that%20our%20proposed%20PHO%20method%20outperform%20existing%20VI-reID%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07930v1&entry.124074799=Read"},
{"title": "PINNACLE: PINN Adaptive ColLocation and Experimental points selection", "author": "Gregory Kang Ruey Lau and Apivich Hemachandra and See-Kiong Ng and Bryan Kian Hsiang Low", "abstract": "  Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft\nconstraints, train with a composite loss function that contains multiple\ntraining point types: different types of collocation points chosen during\ntraining to enforce each PDE and initial/boundary conditions, and experimental\npoints which are usually costly to obtain via experiments or simulations.\nTraining PINNs using this loss function is challenging as it typically requires\nselecting large numbers of points of different types, each with different\ntraining dynamics. Unlike past works that focused on the selection of either\ncollocation or experimental points, this work introduces PINN Adaptive\nColLocation and Experimental points selection (PINNACLE), the first algorithm\nthat jointly optimizes the selection of all training point types, while\nautomatically adjusting the proportion of collocation point types as training\nprogresses. PINNACLE uses information on the interaction among training point\ntypes, which had not been considered before, based on an analysis of PINN\ntraining dynamics via the Neural Tangent Kernel (NTK). We theoretically show\nthat the criterion used by PINNACLE is related to the PINN generalization\nerror, and empirically demonstrate that PINNACLE is able to outperform existing\npoint selection methods for forward, inverse, and transfer learning problems.\n", "link": "http://arxiv.org/abs/2404.07662v1", "date": "2024-04-11", "relevancy": 2.5484, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5221}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4994}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PINNACLE%3A%20PINN%20Adaptive%20ColLocation%20and%20Experimental%20points%20selection&body=Title%3A%20PINNACLE%3A%20PINN%20Adaptive%20ColLocation%20and%20Experimental%20points%20selection%0AAuthor%3A%20Gregory%20Kang%20Ruey%20Lau%20and%20Apivich%20Hemachandra%20and%20See-Kiong%20Ng%20and%20Bryan%20Kian%20Hsiang%20Low%0AAbstract%3A%20%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20which%20incorporate%20PDEs%20as%20soft%0Aconstraints%2C%20train%20with%20a%20composite%20loss%20function%20that%20contains%20multiple%0Atraining%20point%20types%3A%20different%20types%20of%20collocation%20points%20chosen%20during%0Atraining%20to%20enforce%20each%20PDE%20and%20initial/boundary%20conditions%2C%20and%20experimental%0Apoints%20which%20are%20usually%20costly%20to%20obtain%20via%20experiments%20or%20simulations.%0ATraining%20PINNs%20using%20this%20loss%20function%20is%20challenging%20as%20it%20typically%20requires%0Aselecting%20large%20numbers%20of%20points%20of%20different%20types%2C%20each%20with%20different%0Atraining%20dynamics.%20Unlike%20past%20works%20that%20focused%20on%20the%20selection%20of%20either%0Acollocation%20or%20experimental%20points%2C%20this%20work%20introduces%20PINN%20Adaptive%0AColLocation%20and%20Experimental%20points%20selection%20%28PINNACLE%29%2C%20the%20first%20algorithm%0Athat%20jointly%20optimizes%20the%20selection%20of%20all%20training%20point%20types%2C%20while%0Aautomatically%20adjusting%20the%20proportion%20of%20collocation%20point%20types%20as%20training%0Aprogresses.%20PINNACLE%20uses%20information%20on%20the%20interaction%20among%20training%20point%0Atypes%2C%20which%20had%20not%20been%20considered%20before%2C%20based%20on%20an%20analysis%20of%20PINN%0Atraining%20dynamics%20via%20the%20Neural%20Tangent%20Kernel%20%28NTK%29.%20We%20theoretically%20show%0Athat%20the%20criterion%20used%20by%20PINNACLE%20is%20related%20to%20the%20PINN%20generalization%0Aerror%2C%20and%20empirically%20demonstrate%20that%20PINNACLE%20is%20able%20to%20outperform%20existing%0Apoint%20selection%20methods%20for%20forward%2C%20inverse%2C%20and%20transfer%20learning%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07662v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PINNACLE%3A%20PINN%20Adaptive%20ColLocation%20and%20Experimental%20points%20selection&entry.906535625=Gregory%20Kang%20Ruey%20Lau%20and%20Apivich%20Hemachandra%20and%20See-Kiong%20Ng%20and%20Bryan%20Kian%20Hsiang%20Low&entry.1292438233=%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20which%20incorporate%20PDEs%20as%20soft%0Aconstraints%2C%20train%20with%20a%20composite%20loss%20function%20that%20contains%20multiple%0Atraining%20point%20types%3A%20different%20types%20of%20collocation%20points%20chosen%20during%0Atraining%20to%20enforce%20each%20PDE%20and%20initial/boundary%20conditions%2C%20and%20experimental%0Apoints%20which%20are%20usually%20costly%20to%20obtain%20via%20experiments%20or%20simulations.%0ATraining%20PINNs%20using%20this%20loss%20function%20is%20challenging%20as%20it%20typically%20requires%0Aselecting%20large%20numbers%20of%20points%20of%20different%20types%2C%20each%20with%20different%0Atraining%20dynamics.%20Unlike%20past%20works%20that%20focused%20on%20the%20selection%20of%20either%0Acollocation%20or%20experimental%20points%2C%20this%20work%20introduces%20PINN%20Adaptive%0AColLocation%20and%20Experimental%20points%20selection%20%28PINNACLE%29%2C%20the%20first%20algorithm%0Athat%20jointly%20optimizes%20the%20selection%20of%20all%20training%20point%20types%2C%20while%0Aautomatically%20adjusting%20the%20proportion%20of%20collocation%20point%20types%20as%20training%0Aprogresses.%20PINNACLE%20uses%20information%20on%20the%20interaction%20among%20training%20point%0Atypes%2C%20which%20had%20not%20been%20considered%20before%2C%20based%20on%20an%20analysis%20of%20PINN%0Atraining%20dynamics%20via%20the%20Neural%20Tangent%20Kernel%20%28NTK%29.%20We%20theoretically%20show%0Athat%20the%20criterion%20used%20by%20PINNACLE%20is%20related%20to%20the%20PINN%20generalization%0Aerror%2C%20and%20empirically%20demonstrate%20that%20PINNACLE%20is%20able%20to%20outperform%20existing%0Apoint%20selection%20methods%20for%20forward%2C%20inverse%2C%20and%20transfer%20learning%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07662v1&entry.124074799=Read"},
{"title": "Attention Calibration for Disentangled Text-to-Image Personalization", "author": "Yanbing Zhang and Mengping Yang and Qin Zhou and Zhe Wang", "abstract": "  Recent thrilling progress in large-scale text-to-image (T2I) models has\nunlocked unprecedented synthesis quality of AI-generated content (AIGC)\nincluding image generation, 3D and video composition. Further, personalized\ntechniques enable appealing customized production of a novel concept given only\nseveral images as reference. However, an intriguing problem persists: Is it\npossible to capture multiple, novel concepts from one single reference image?\nIn this paper, we identify that existing approaches fail to preserve visual\nconsistency with the reference image and eliminate cross-influence from\nconcepts. To alleviate this, we propose an attention calibration mechanism to\nimprove the concept-level understanding of the T2I model. Specifically, we\nfirst introduce new learnable modifiers bound with classes to capture\nattributes of multiple concepts. Then, the classes are separated and\nstrengthened following the activation of the cross-attention operation,\nensuring comprehensive and self-contained concepts. Additionally, we suppress\nthe attention activation of different classes to mitigate mutual influence\namong concepts. Together, our proposed method, dubbed DisenDiff, can learn\ndisentangled multiple concepts from one single image and produce novel\ncustomized images with learned concepts. We demonstrate that our method\noutperforms the current state of the art in both qualitative and quantitative\nevaluations. More importantly, our proposed techniques are compatible with LoRA\nand inpainting pipelines, enabling more interactive experiences.\n", "link": "http://arxiv.org/abs/2403.18551v2", "date": "2024-04-11", "relevancy": 2.547, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6482}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6432}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6257}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attention%20Calibration%20for%20Disentangled%20Text-to-Image%20Personalization&body=Title%3A%20Attention%20Calibration%20for%20Disentangled%20Text-to-Image%20Personalization%0AAuthor%3A%20Yanbing%20Zhang%20and%20Mengping%20Yang%20and%20Qin%20Zhou%20and%20Zhe%20Wang%0AAbstract%3A%20%20%20Recent%20thrilling%20progress%20in%20large-scale%20text-to-image%20%28T2I%29%20models%20has%0Aunlocked%20unprecedented%20synthesis%20quality%20of%20AI-generated%20content%20%28AIGC%29%0Aincluding%20image%20generation%2C%203D%20and%20video%20composition.%20Further%2C%20personalized%0Atechniques%20enable%20appealing%20customized%20production%20of%20a%20novel%20concept%20given%20only%0Aseveral%20images%20as%20reference.%20However%2C%20an%20intriguing%20problem%20persists%3A%20Is%20it%0Apossible%20to%20capture%20multiple%2C%20novel%20concepts%20from%20one%20single%20reference%20image%3F%0AIn%20this%20paper%2C%20we%20identify%20that%20existing%20approaches%20fail%20to%20preserve%20visual%0Aconsistency%20with%20the%20reference%20image%20and%20eliminate%20cross-influence%20from%0Aconcepts.%20To%20alleviate%20this%2C%20we%20propose%20an%20attention%20calibration%20mechanism%20to%0Aimprove%20the%20concept-level%20understanding%20of%20the%20T2I%20model.%20Specifically%2C%20we%0Afirst%20introduce%20new%20learnable%20modifiers%20bound%20with%20classes%20to%20capture%0Aattributes%20of%20multiple%20concepts.%20Then%2C%20the%20classes%20are%20separated%20and%0Astrengthened%20following%20the%20activation%20of%20the%20cross-attention%20operation%2C%0Aensuring%20comprehensive%20and%20self-contained%20concepts.%20Additionally%2C%20we%20suppress%0Athe%20attention%20activation%20of%20different%20classes%20to%20mitigate%20mutual%20influence%0Aamong%20concepts.%20Together%2C%20our%20proposed%20method%2C%20dubbed%20DisenDiff%2C%20can%20learn%0Adisentangled%20multiple%20concepts%20from%20one%20single%20image%20and%20produce%20novel%0Acustomized%20images%20with%20learned%20concepts.%20We%20demonstrate%20that%20our%20method%0Aoutperforms%20the%20current%20state%20of%20the%20art%20in%20both%20qualitative%20and%20quantitative%0Aevaluations.%20More%20importantly%2C%20our%20proposed%20techniques%20are%20compatible%20with%20LoRA%0Aand%20inpainting%20pipelines%2C%20enabling%20more%20interactive%20experiences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18551v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Calibration%20for%20Disentangled%20Text-to-Image%20Personalization&entry.906535625=Yanbing%20Zhang%20and%20Mengping%20Yang%20and%20Qin%20Zhou%20and%20Zhe%20Wang&entry.1292438233=%20%20Recent%20thrilling%20progress%20in%20large-scale%20text-to-image%20%28T2I%29%20models%20has%0Aunlocked%20unprecedented%20synthesis%20quality%20of%20AI-generated%20content%20%28AIGC%29%0Aincluding%20image%20generation%2C%203D%20and%20video%20composition.%20Further%2C%20personalized%0Atechniques%20enable%20appealing%20customized%20production%20of%20a%20novel%20concept%20given%20only%0Aseveral%20images%20as%20reference.%20However%2C%20an%20intriguing%20problem%20persists%3A%20Is%20it%0Apossible%20to%20capture%20multiple%2C%20novel%20concepts%20from%20one%20single%20reference%20image%3F%0AIn%20this%20paper%2C%20we%20identify%20that%20existing%20approaches%20fail%20to%20preserve%20visual%0Aconsistency%20with%20the%20reference%20image%20and%20eliminate%20cross-influence%20from%0Aconcepts.%20To%20alleviate%20this%2C%20we%20propose%20an%20attention%20calibration%20mechanism%20to%0Aimprove%20the%20concept-level%20understanding%20of%20the%20T2I%20model.%20Specifically%2C%20we%0Afirst%20introduce%20new%20learnable%20modifiers%20bound%20with%20classes%20to%20capture%0Aattributes%20of%20multiple%20concepts.%20Then%2C%20the%20classes%20are%20separated%20and%0Astrengthened%20following%20the%20activation%20of%20the%20cross-attention%20operation%2C%0Aensuring%20comprehensive%20and%20self-contained%20concepts.%20Additionally%2C%20we%20suppress%0Athe%20attention%20activation%20of%20different%20classes%20to%20mitigate%20mutual%20influence%0Aamong%20concepts.%20Together%2C%20our%20proposed%20method%2C%20dubbed%20DisenDiff%2C%20can%20learn%0Adisentangled%20multiple%20concepts%20from%20one%20single%20image%20and%20produce%20novel%0Acustomized%20images%20with%20learned%20concepts.%20We%20demonstrate%20that%20our%20method%0Aoutperforms%20the%20current%20state%20of%20the%20art%20in%20both%20qualitative%20and%20quantitative%0Aevaluations.%20More%20importantly%2C%20our%20proposed%20techniques%20are%20compatible%20with%20LoRA%0Aand%20inpainting%20pipelines%2C%20enabling%20more%20interactive%20experiences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18551v2&entry.124074799=Read"},
{"title": "Realistic Continual Learning Approach using Pre-trained Models", "author": "Nadia Nasri and Carlos Guti\u00e9rrez-\u00c1lvarez and Sergio Lafuente-Arroyo and Saturnino Maldonado-Basc\u00f3n and Roberto J. L\u00f3pez-Sastre", "abstract": "  Continual learning (CL) is crucial for evaluating adaptability in learning\nsolutions to retain knowledge. Our research addresses the challenge of\ncatastrophic forgetting, where models lose proficiency in previously learned\ntasks as they acquire new ones. While numerous solutions have been proposed,\nexisting experimental setups often rely on idealized class-incremental learning\nscenarios. We introduce Realistic Continual Learning (RealCL), a novel CL\nparadigm where class distributions across tasks are random, departing from\nstructured setups.\n  We also present CLARE (Continual Learning Approach with pRE-trained models\nfor RealCL scenarios), a pre-trained model-based solution designed to integrate\nnew knowledge while preserving past learning. Our contributions include\npioneering RealCL as a generalization of traditional CL setups, proposing CLARE\nas an adaptable approach for RealCL tasks, and conducting extensive experiments\ndemonstrating its effectiveness across various RealCL scenarios. Notably, CLARE\noutperforms existing models on RealCL benchmarks, highlighting its versatility\nand robustness in unpredictable learning environments.\n", "link": "http://arxiv.org/abs/2404.07729v1", "date": "2024-04-11", "relevancy": 2.5191, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4908}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4882}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Realistic%20Continual%20Learning%20Approach%20using%20Pre-trained%20Models&body=Title%3A%20Realistic%20Continual%20Learning%20Approach%20using%20Pre-trained%20Models%0AAuthor%3A%20Nadia%20Nasri%20and%20Carlos%20Guti%C3%A9rrez-%C3%81lvarez%20and%20Sergio%20Lafuente-Arroyo%20and%20Saturnino%20Maldonado-Basc%C3%B3n%20and%20Roberto%20J.%20L%C3%B3pez-Sastre%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20is%20crucial%20for%20evaluating%20adaptability%20in%20learning%0Asolutions%20to%20retain%20knowledge.%20Our%20research%20addresses%20the%20challenge%20of%0Acatastrophic%20forgetting%2C%20where%20models%20lose%20proficiency%20in%20previously%20learned%0Atasks%20as%20they%20acquire%20new%20ones.%20While%20numerous%20solutions%20have%20been%20proposed%2C%0Aexisting%20experimental%20setups%20often%20rely%20on%20idealized%20class-incremental%20learning%0Ascenarios.%20We%20introduce%20Realistic%20Continual%20Learning%20%28RealCL%29%2C%20a%20novel%20CL%0Aparadigm%20where%20class%20distributions%20across%20tasks%20are%20random%2C%20departing%20from%0Astructured%20setups.%0A%20%20We%20also%20present%20CLARE%20%28Continual%20Learning%20Approach%20with%20pRE-trained%20models%0Afor%20RealCL%20scenarios%29%2C%20a%20pre-trained%20model-based%20solution%20designed%20to%20integrate%0Anew%20knowledge%20while%20preserving%20past%20learning.%20Our%20contributions%20include%0Apioneering%20RealCL%20as%20a%20generalization%20of%20traditional%20CL%20setups%2C%20proposing%20CLARE%0Aas%20an%20adaptable%20approach%20for%20RealCL%20tasks%2C%20and%20conducting%20extensive%20experiments%0Ademonstrating%20its%20effectiveness%20across%20various%20RealCL%20scenarios.%20Notably%2C%20CLARE%0Aoutperforms%20existing%20models%20on%20RealCL%20benchmarks%2C%20highlighting%20its%20versatility%0Aand%20robustness%20in%20unpredictable%20learning%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07729v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Continual%20Learning%20Approach%20using%20Pre-trained%20Models&entry.906535625=Nadia%20Nasri%20and%20Carlos%20Guti%C3%A9rrez-%C3%81lvarez%20and%20Sergio%20Lafuente-Arroyo%20and%20Saturnino%20Maldonado-Basc%C3%B3n%20and%20Roberto%20J.%20L%C3%B3pez-Sastre&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20is%20crucial%20for%20evaluating%20adaptability%20in%20learning%0Asolutions%20to%20retain%20knowledge.%20Our%20research%20addresses%20the%20challenge%20of%0Acatastrophic%20forgetting%2C%20where%20models%20lose%20proficiency%20in%20previously%20learned%0Atasks%20as%20they%20acquire%20new%20ones.%20While%20numerous%20solutions%20have%20been%20proposed%2C%0Aexisting%20experimental%20setups%20often%20rely%20on%20idealized%20class-incremental%20learning%0Ascenarios.%20We%20introduce%20Realistic%20Continual%20Learning%20%28RealCL%29%2C%20a%20novel%20CL%0Aparadigm%20where%20class%20distributions%20across%20tasks%20are%20random%2C%20departing%20from%0Astructured%20setups.%0A%20%20We%20also%20present%20CLARE%20%28Continual%20Learning%20Approach%20with%20pRE-trained%20models%0Afor%20RealCL%20scenarios%29%2C%20a%20pre-trained%20model-based%20solution%20designed%20to%20integrate%0Anew%20knowledge%20while%20preserving%20past%20learning.%20Our%20contributions%20include%0Apioneering%20RealCL%20as%20a%20generalization%20of%20traditional%20CL%20setups%2C%20proposing%20CLARE%0Aas%20an%20adaptable%20approach%20for%20RealCL%20tasks%2C%20and%20conducting%20extensive%20experiments%0Ademonstrating%20its%20effectiveness%20across%20various%20RealCL%20scenarios.%20Notably%2C%20CLARE%0Aoutperforms%20existing%20models%20on%20RealCL%20benchmarks%2C%20highlighting%20its%20versatility%0Aand%20robustness%20in%20unpredictable%20learning%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07729v1&entry.124074799=Read"},
{"title": "Unsupervised Concept Drift Detection based on Parallel Activations of\n  Neural Network", "author": "Joanna Komorniczak and Pawe\u0142 Ksieniewicz", "abstract": "  Practical applications of artificial intelligence increasingly often have to\ndeal with the streaming properties of real data, which, considering the time\nfactor, are subject to phenomena such as periodicity and more or less chaotic\ndegeneration - resulting directly in the concept drifts. The modern concept\ndrift detectors almost always assume immediate access to labels, which due to\ntheir cost, limited availability and possible delay has been shown to be\nunrealistic. This work proposes an unsupervised Parallel Activations Drift\nDetector, utilizing the outputs of an untrained neural network, presenting its\nkey design elements, intuitions about processing properties, and a pool of\ncomputer experiments demonstrating its competitiveness with state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2404.07776v1", "date": "2024-04-11", "relevancy": 2.5051, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5146}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4989}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4896}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Concept%20Drift%20Detection%20based%20on%20Parallel%20Activations%20of%0A%20%20Neural%20Network&body=Title%3A%20Unsupervised%20Concept%20Drift%20Detection%20based%20on%20Parallel%20Activations%20of%0A%20%20Neural%20Network%0AAuthor%3A%20Joanna%20Komorniczak%20and%20Pawe%C5%82%20Ksieniewicz%0AAbstract%3A%20%20%20Practical%20applications%20of%20artificial%20intelligence%20increasingly%20often%20have%20to%0Adeal%20with%20the%20streaming%20properties%20of%20real%20data%2C%20which%2C%20considering%20the%20time%0Afactor%2C%20are%20subject%20to%20phenomena%20such%20as%20periodicity%20and%20more%20or%20less%20chaotic%0Adegeneration%20-%20resulting%20directly%20in%20the%20concept%20drifts.%20The%20modern%20concept%0Adrift%20detectors%20almost%20always%20assume%20immediate%20access%20to%20labels%2C%20which%20due%20to%0Atheir%20cost%2C%20limited%20availability%20and%20possible%20delay%20has%20been%20shown%20to%20be%0Aunrealistic.%20This%20work%20proposes%20an%20unsupervised%20Parallel%20Activations%20Drift%0ADetector%2C%20utilizing%20the%20outputs%20of%20an%20untrained%20neural%20network%2C%20presenting%20its%0Akey%20design%20elements%2C%20intuitions%20about%20processing%20properties%2C%20and%20a%20pool%20of%0Acomputer%20experiments%20demonstrating%20its%20competitiveness%20with%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07776v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Concept%20Drift%20Detection%20based%20on%20Parallel%20Activations%20of%0A%20%20Neural%20Network&entry.906535625=Joanna%20Komorniczak%20and%20Pawe%C5%82%20Ksieniewicz&entry.1292438233=%20%20Practical%20applications%20of%20artificial%20intelligence%20increasingly%20often%20have%20to%0Adeal%20with%20the%20streaming%20properties%20of%20real%20data%2C%20which%2C%20considering%20the%20time%0Afactor%2C%20are%20subject%20to%20phenomena%20such%20as%20periodicity%20and%20more%20or%20less%20chaotic%0Adegeneration%20-%20resulting%20directly%20in%20the%20concept%20drifts.%20The%20modern%20concept%0Adrift%20detectors%20almost%20always%20assume%20immediate%20access%20to%20labels%2C%20which%20due%20to%0Atheir%20cost%2C%20limited%20availability%20and%20possible%20delay%20has%20been%20shown%20to%20be%0Aunrealistic.%20This%20work%20proposes%20an%20unsupervised%20Parallel%20Activations%20Drift%0ADetector%2C%20utilizing%20the%20outputs%20of%20an%20untrained%20neural%20network%2C%20presenting%20its%0Akey%20design%20elements%2C%20intuitions%20about%20processing%20properties%2C%20and%20a%20pool%20of%0Acomputer%20experiments%20demonstrating%20its%20competitiveness%20with%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07776v1&entry.124074799=Read"},
{"title": "Fuss-Free Network: A Simplified and Efficient Neural Network for Crowd\n  Counting", "author": "Lei Chen and Xingen Gao", "abstract": "  In the field of crowd-counting research, many recent deep learning based\nmethods have demonstrated robust capabilities for accurately estimating crowd\nsizes. However, the enhancement in their performance often arises from an\nincrease in the complexity of the model structure. This paper introduces the\nFuss-Free Network (FFNet), a crowd counting deep learning model that is\ncharacterized by its simplicity and efficiency in terms of its structure. The\nmodel comprises only a backbone of a neural network and a multi-scale feature\nfusion structure.The multi-scale feature fusion structure is a simple\narchitecture consisting of three branches, each only equipped with a focus\ntransition module, and combines the features from these branches through the\nconcatenation operation.Our proposed crowd counting model is trained and\nevaluated on four widely used public datasets, and it achieves accuracy that is\ncomparable to that of existing complex models.The experimental results further\nindicate that excellent performance in crowd counting tasks can also be\nachieved by utilizing a simple, low-parameter, and computationally efficient\nneural network structure.\n", "link": "http://arxiv.org/abs/2404.07847v1", "date": "2024-04-11", "relevancy": 2.4898, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5163}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5093}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4683}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fuss-Free%20Network%3A%20A%20Simplified%20and%20Efficient%20Neural%20Network%20for%20Crowd%0A%20%20Counting&body=Title%3A%20Fuss-Free%20Network%3A%20A%20Simplified%20and%20Efficient%20Neural%20Network%20for%20Crowd%0A%20%20Counting%0AAuthor%3A%20Lei%20Chen%20and%20Xingen%20Gao%0AAbstract%3A%20%20%20In%20the%20field%20of%20crowd-counting%20research%2C%20many%20recent%20deep%20learning%20based%0Amethods%20have%20demonstrated%20robust%20capabilities%20for%20accurately%20estimating%20crowd%0Asizes.%20However%2C%20the%20enhancement%20in%20their%20performance%20often%20arises%20from%20an%0Aincrease%20in%20the%20complexity%20of%20the%20model%20structure.%20This%20paper%20introduces%20the%0AFuss-Free%20Network%20%28FFNet%29%2C%20a%20crowd%20counting%20deep%20learning%20model%20that%20is%0Acharacterized%20by%20its%20simplicity%20and%20efficiency%20in%20terms%20of%20its%20structure.%20The%0Amodel%20comprises%20only%20a%20backbone%20of%20a%20neural%20network%20and%20a%20multi-scale%20feature%0Afusion%20structure.The%20multi-scale%20feature%20fusion%20structure%20is%20a%20simple%0Aarchitecture%20consisting%20of%20three%20branches%2C%20each%20only%20equipped%20with%20a%20focus%0Atransition%20module%2C%20and%20combines%20the%20features%20from%20these%20branches%20through%20the%0Aconcatenation%20operation.Our%20proposed%20crowd%20counting%20model%20is%20trained%20and%0Aevaluated%20on%20four%20widely%20used%20public%20datasets%2C%20and%20it%20achieves%20accuracy%20that%20is%0Acomparable%20to%20that%20of%20existing%20complex%20models.The%20experimental%20results%20further%0Aindicate%20that%20excellent%20performance%20in%20crowd%20counting%20tasks%20can%20also%20be%0Aachieved%20by%20utilizing%20a%20simple%2C%20low-parameter%2C%20and%20computationally%20efficient%0Aneural%20network%20structure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07847v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fuss-Free%20Network%3A%20A%20Simplified%20and%20Efficient%20Neural%20Network%20for%20Crowd%0A%20%20Counting&entry.906535625=Lei%20Chen%20and%20Xingen%20Gao&entry.1292438233=%20%20In%20the%20field%20of%20crowd-counting%20research%2C%20many%20recent%20deep%20learning%20based%0Amethods%20have%20demonstrated%20robust%20capabilities%20for%20accurately%20estimating%20crowd%0Asizes.%20However%2C%20the%20enhancement%20in%20their%20performance%20often%20arises%20from%20an%0Aincrease%20in%20the%20complexity%20of%20the%20model%20structure.%20This%20paper%20introduces%20the%0AFuss-Free%20Network%20%28FFNet%29%2C%20a%20crowd%20counting%20deep%20learning%20model%20that%20is%0Acharacterized%20by%20its%20simplicity%20and%20efficiency%20in%20terms%20of%20its%20structure.%20The%0Amodel%20comprises%20only%20a%20backbone%20of%20a%20neural%20network%20and%20a%20multi-scale%20feature%0Afusion%20structure.The%20multi-scale%20feature%20fusion%20structure%20is%20a%20simple%0Aarchitecture%20consisting%20of%20three%20branches%2C%20each%20only%20equipped%20with%20a%20focus%0Atransition%20module%2C%20and%20combines%20the%20features%20from%20these%20branches%20through%20the%0Aconcatenation%20operation.Our%20proposed%20crowd%20counting%20model%20is%20trained%20and%0Aevaluated%20on%20four%20widely%20used%20public%20datasets%2C%20and%20it%20achieves%20accuracy%20that%20is%0Acomparable%20to%20that%20of%20existing%20complex%20models.The%20experimental%20results%20further%0Aindicate%20that%20excellent%20performance%20in%20crowd%20counting%20tasks%20can%20also%20be%0Aachieved%20by%20utilizing%20a%20simple%2C%20low-parameter%2C%20and%20computationally%20efficient%0Aneural%20network%20structure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07847v1&entry.124074799=Read"},
{"title": "Grokking as the Transition from Lazy to Rich Training Dynamics", "author": "Tanishq Kumar and Blake Bordelon and Samuel J. Gershman and Cengiz Pehlevan", "abstract": "  We propose that the grokking phenomenon, where the train loss of a neural\nnetwork decreases much earlier than its test loss, can arise due to a neural\nnetwork transitioning from lazy training dynamics to a rich, feature learning\nregime. To illustrate this mechanism, we study the simple setting of vanilla\ngradient descent on a polynomial regression problem with a two layer neural\nnetwork which exhibits grokking without regularization in a way that cannot be\nexplained by existing theories. We identify sufficient statistics for the test\nloss of such a network, and tracking these over training reveals that grokking\narises in this setting when the network first attempts to fit a kernel\nregression solution with its initial features, followed by late-time feature\nlearning where a generalizing solution is identified after train loss is\nalready low. We find that the key determinants of grokking are the rate of\nfeature learning -- which can be controlled precisely by parameters that scale\nthe network output -- and the alignment of the initial features with the target\nfunction $y(x)$. We argue this delayed generalization arises when (1) the top\neigenvectors of the initial neural tangent kernel and the task labels $y(x)$\nare misaligned, but (2) the dataset size is large enough so that it is possible\nfor the network to generalize eventually, but not so large that train loss\nperfectly tracks test loss at all epochs, and (3) the network begins training\nin the lazy regime so does not learn features immediately. We conclude with\nevidence that this transition from lazy (linear model) to rich training\n(feature learning) can control grokking in more general settings, like on\nMNIST, one-layer Transformers, and student-teacher networks.\n", "link": "http://arxiv.org/abs/2310.06110v3", "date": "2024-04-11", "relevancy": 2.4566, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5125}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5012}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4603}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Grokking%20as%20the%20Transition%20from%20Lazy%20to%20Rich%20Training%20Dynamics&body=Title%3A%20Grokking%20as%20the%20Transition%20from%20Lazy%20to%20Rich%20Training%20Dynamics%0AAuthor%3A%20Tanishq%20Kumar%20and%20Blake%20Bordelon%20and%20Samuel%20J.%20Gershman%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20We%20propose%20that%20the%20grokking%20phenomenon%2C%20where%20the%20train%20loss%20of%20a%20neural%0Anetwork%20decreases%20much%20earlier%20than%20its%20test%20loss%2C%20can%20arise%20due%20to%20a%20neural%0Anetwork%20transitioning%20from%20lazy%20training%20dynamics%20to%20a%20rich%2C%20feature%20learning%0Aregime.%20To%20illustrate%20this%20mechanism%2C%20we%20study%20the%20simple%20setting%20of%20vanilla%0Agradient%20descent%20on%20a%20polynomial%20regression%20problem%20with%20a%20two%20layer%20neural%0Anetwork%20which%20exhibits%20grokking%20without%20regularization%20in%20a%20way%20that%20cannot%20be%0Aexplained%20by%20existing%20theories.%20We%20identify%20sufficient%20statistics%20for%20the%20test%0Aloss%20of%20such%20a%20network%2C%20and%20tracking%20these%20over%20training%20reveals%20that%20grokking%0Aarises%20in%20this%20setting%20when%20the%20network%20first%20attempts%20to%20fit%20a%20kernel%0Aregression%20solution%20with%20its%20initial%20features%2C%20followed%20by%20late-time%20feature%0Alearning%20where%20a%20generalizing%20solution%20is%20identified%20after%20train%20loss%20is%0Aalready%20low.%20We%20find%20that%20the%20key%20determinants%20of%20grokking%20are%20the%20rate%20of%0Afeature%20learning%20--%20which%20can%20be%20controlled%20precisely%20by%20parameters%20that%20scale%0Athe%20network%20output%20--%20and%20the%20alignment%20of%20the%20initial%20features%20with%20the%20target%0Afunction%20%24y%28x%29%24.%20We%20argue%20this%20delayed%20generalization%20arises%20when%20%281%29%20the%20top%0Aeigenvectors%20of%20the%20initial%20neural%20tangent%20kernel%20and%20the%20task%20labels%20%24y%28x%29%24%0Aare%20misaligned%2C%20but%20%282%29%20the%20dataset%20size%20is%20large%20enough%20so%20that%20it%20is%20possible%0Afor%20the%20network%20to%20generalize%20eventually%2C%20but%20not%20so%20large%20that%20train%20loss%0Aperfectly%20tracks%20test%20loss%20at%20all%20epochs%2C%20and%20%283%29%20the%20network%20begins%20training%0Ain%20the%20lazy%20regime%20so%20does%20not%20learn%20features%20immediately.%20We%20conclude%20with%0Aevidence%20that%20this%20transition%20from%20lazy%20%28linear%20model%29%20to%20rich%20training%0A%28feature%20learning%29%20can%20control%20grokking%20in%20more%20general%20settings%2C%20like%20on%0AMNIST%2C%20one-layer%20Transformers%2C%20and%20student-teacher%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06110v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grokking%20as%20the%20Transition%20from%20Lazy%20to%20Rich%20Training%20Dynamics&entry.906535625=Tanishq%20Kumar%20and%20Blake%20Bordelon%20and%20Samuel%20J.%20Gershman%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20We%20propose%20that%20the%20grokking%20phenomenon%2C%20where%20the%20train%20loss%20of%20a%20neural%0Anetwork%20decreases%20much%20earlier%20than%20its%20test%20loss%2C%20can%20arise%20due%20to%20a%20neural%0Anetwork%20transitioning%20from%20lazy%20training%20dynamics%20to%20a%20rich%2C%20feature%20learning%0Aregime.%20To%20illustrate%20this%20mechanism%2C%20we%20study%20the%20simple%20setting%20of%20vanilla%0Agradient%20descent%20on%20a%20polynomial%20regression%20problem%20with%20a%20two%20layer%20neural%0Anetwork%20which%20exhibits%20grokking%20without%20regularization%20in%20a%20way%20that%20cannot%20be%0Aexplained%20by%20existing%20theories.%20We%20identify%20sufficient%20statistics%20for%20the%20test%0Aloss%20of%20such%20a%20network%2C%20and%20tracking%20these%20over%20training%20reveals%20that%20grokking%0Aarises%20in%20this%20setting%20when%20the%20network%20first%20attempts%20to%20fit%20a%20kernel%0Aregression%20solution%20with%20its%20initial%20features%2C%20followed%20by%20late-time%20feature%0Alearning%20where%20a%20generalizing%20solution%20is%20identified%20after%20train%20loss%20is%0Aalready%20low.%20We%20find%20that%20the%20key%20determinants%20of%20grokking%20are%20the%20rate%20of%0Afeature%20learning%20--%20which%20can%20be%20controlled%20precisely%20by%20parameters%20that%20scale%0Athe%20network%20output%20--%20and%20the%20alignment%20of%20the%20initial%20features%20with%20the%20target%0Afunction%20%24y%28x%29%24.%20We%20argue%20this%20delayed%20generalization%20arises%20when%20%281%29%20the%20top%0Aeigenvectors%20of%20the%20initial%20neural%20tangent%20kernel%20and%20the%20task%20labels%20%24y%28x%29%24%0Aare%20misaligned%2C%20but%20%282%29%20the%20dataset%20size%20is%20large%20enough%20so%20that%20it%20is%20possible%0Afor%20the%20network%20to%20generalize%20eventually%2C%20but%20not%20so%20large%20that%20train%20loss%0Aperfectly%20tracks%20test%20loss%20at%20all%20epochs%2C%20and%20%283%29%20the%20network%20begins%20training%0Ain%20the%20lazy%20regime%20so%20does%20not%20learn%20features%20immediately.%20We%20conclude%20with%0Aevidence%20that%20this%20transition%20from%20lazy%20%28linear%20model%29%20to%20rich%20training%0A%28feature%20learning%29%20can%20control%20grokking%20in%20more%20general%20settings%2C%20like%20on%0AMNIST%2C%20one-layer%20Transformers%2C%20and%20student-teacher%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06110v3&entry.124074799=Read"},
{"title": "Flatness Improves Backbone Generalisation in Few-shot Classification", "author": "Rui Li and Martin Trapp and Marcus Klasson and Arno Solin", "abstract": "  Deployment of deep neural networks in real-world settings typically requires\nadaptation to new tasks with few examples. Few-shot classification (FSC)\nprovides a solution to this problem by leveraging pre-trained backbones for\nfast adaptation to new classes. Surprisingly, most efforts have only focused on\ndeveloping architectures for easing the adaptation to the target domain without\nconsidering the importance of backbone training for good generalisation. We\nshow that flatness-aware backbone training with vanilla fine-tuning results in\na simpler yet competitive baseline compared to the state-of-the-art. Our\nresults indicate that for in- and cross-domain FSC, backbone training is\ncrucial to achieving good generalisation across different adaptation methods.\nWe advocate more care should be taken when training these models.\n", "link": "http://arxiv.org/abs/2404.07696v1", "date": "2024-04-11", "relevancy": 2.418, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4882}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4768}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Flatness%20Improves%20Backbone%20Generalisation%20in%20Few-shot%20Classification&body=Title%3A%20Flatness%20Improves%20Backbone%20Generalisation%20in%20Few-shot%20Classification%0AAuthor%3A%20Rui%20Li%20and%20Martin%20Trapp%20and%20Marcus%20Klasson%20and%20Arno%20Solin%0AAbstract%3A%20%20%20Deployment%20of%20deep%20neural%20networks%20in%20real-world%20settings%20typically%20requires%0Aadaptation%20to%20new%20tasks%20with%20few%20examples.%20Few-shot%20classification%20%28FSC%29%0Aprovides%20a%20solution%20to%20this%20problem%20by%20leveraging%20pre-trained%20backbones%20for%0Afast%20adaptation%20to%20new%20classes.%20Surprisingly%2C%20most%20efforts%20have%20only%20focused%20on%0Adeveloping%20architectures%20for%20easing%20the%20adaptation%20to%20the%20target%20domain%20without%0Aconsidering%20the%20importance%20of%20backbone%20training%20for%20good%20generalisation.%20We%0Ashow%20that%20flatness-aware%20backbone%20training%20with%20vanilla%20fine-tuning%20results%20in%0Aa%20simpler%20yet%20competitive%20baseline%20compared%20to%20the%20state-of-the-art.%20Our%0Aresults%20indicate%20that%20for%20in-%20and%20cross-domain%20FSC%2C%20backbone%20training%20is%0Acrucial%20to%20achieving%20good%20generalisation%20across%20different%20adaptation%20methods.%0AWe%20advocate%20more%20care%20should%20be%20taken%20when%20training%20these%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07696v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flatness%20Improves%20Backbone%20Generalisation%20in%20Few-shot%20Classification&entry.906535625=Rui%20Li%20and%20Martin%20Trapp%20and%20Marcus%20Klasson%20and%20Arno%20Solin&entry.1292438233=%20%20Deployment%20of%20deep%20neural%20networks%20in%20real-world%20settings%20typically%20requires%0Aadaptation%20to%20new%20tasks%20with%20few%20examples.%20Few-shot%20classification%20%28FSC%29%0Aprovides%20a%20solution%20to%20this%20problem%20by%20leveraging%20pre-trained%20backbones%20for%0Afast%20adaptation%20to%20new%20classes.%20Surprisingly%2C%20most%20efforts%20have%20only%20focused%20on%0Adeveloping%20architectures%20for%20easing%20the%20adaptation%20to%20the%20target%20domain%20without%0Aconsidering%20the%20importance%20of%20backbone%20training%20for%20good%20generalisation.%20We%0Ashow%20that%20flatness-aware%20backbone%20training%20with%20vanilla%20fine-tuning%20results%20in%0Aa%20simpler%20yet%20competitive%20baseline%20compared%20to%20the%20state-of-the-art.%20Our%0Aresults%20indicate%20that%20for%20in-%20and%20cross-domain%20FSC%2C%20backbone%20training%20is%0Acrucial%20to%20achieving%20good%20generalisation%20across%20different%20adaptation%20methods.%0AWe%20advocate%20more%20care%20should%20be%20taken%20when%20training%20these%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07696v1&entry.124074799=Read"},
{"title": "Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised\n  Medical Image Segmentation", "author": "Yuanpeng He and Lijian Li", "abstract": "  Although the existing uncertainty-based semi-supervised medical segmentation\nmethods have achieved excellent performance, they usually only consider a\nsingle uncertainty evaluation, which often fails to solve the problem related\nto credibility completely. Therefore, based on the framework of evidential deep\nlearning, this paper integrates the evidential predictive results in the\ncross-region of mixed and original samples to reallocate the confidence degree\nand uncertainty measure of each voxel, which is realized by emphasizing\nuncertain information of probability assignments fusion rule of traditional\nevidence theory. Furthermore, we design a voxel-level asymptotic learning\nstrategy by introducing information entropy to combine with the fused\nuncertainty measure to estimate voxel prediction more precisely. The model will\ngradually pay attention to the prediction results with high uncertainty in the\nlearning process, to learn the features that are difficult to master. The\nexperimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate the\nsuperior performance of our proposed method in comparison with the existing\nstate of the arts.\n", "link": "http://arxiv.org/abs/2404.06177v2", "date": "2024-04-11", "relevancy": 2.4027, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6942}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.597}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5669}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Uncertainty-aware%20Evidential%20Fusion-based%20Learning%20for%20Semi-supervised%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20Uncertainty-aware%20Evidential%20Fusion-based%20Learning%20for%20Semi-supervised%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yuanpeng%20He%20and%20Lijian%20Li%0AAbstract%3A%20%20%20Although%20the%20existing%20uncertainty-based%20semi-supervised%20medical%20segmentation%0Amethods%20have%20achieved%20excellent%20performance%2C%20they%20usually%20only%20consider%20a%0Asingle%20uncertainty%20evaluation%2C%20which%20often%20fails%20to%20solve%20the%20problem%20related%0Ato%20credibility%20completely.%20Therefore%2C%20based%20on%20the%20framework%20of%20evidential%20deep%0Alearning%2C%20this%20paper%20integrates%20the%20evidential%20predictive%20results%20in%20the%0Across-region%20of%20mixed%20and%20original%20samples%20to%20reallocate%20the%20confidence%20degree%0Aand%20uncertainty%20measure%20of%20each%20voxel%2C%20which%20is%20realized%20by%20emphasizing%0Auncertain%20information%20of%20probability%20assignments%20fusion%20rule%20of%20traditional%0Aevidence%20theory.%20Furthermore%2C%20we%20design%20a%20voxel-level%20asymptotic%20learning%0Astrategy%20by%20introducing%20information%20entropy%20to%20combine%20with%20the%20fused%0Auncertainty%20measure%20to%20estimate%20voxel%20prediction%20more%20precisely.%20The%20model%20will%0Agradually%20pay%20attention%20to%20the%20prediction%20results%20with%20high%20uncertainty%20in%20the%0Alearning%20process%2C%20to%20learn%20the%20features%20that%20are%20difficult%20to%20master.%20The%0Aexperimental%20results%20on%20LA%2C%20Pancreas-CT%2C%20ACDC%20and%20TBAD%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20our%20proposed%20method%20in%20comparison%20with%20the%20existing%0Astate%20of%20the%20arts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06177v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-aware%20Evidential%20Fusion-based%20Learning%20for%20Semi-supervised%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Yuanpeng%20He%20and%20Lijian%20Li&entry.1292438233=%20%20Although%20the%20existing%20uncertainty-based%20semi-supervised%20medical%20segmentation%0Amethods%20have%20achieved%20excellent%20performance%2C%20they%20usually%20only%20consider%20a%0Asingle%20uncertainty%20evaluation%2C%20which%20often%20fails%20to%20solve%20the%20problem%20related%0Ato%20credibility%20completely.%20Therefore%2C%20based%20on%20the%20framework%20of%20evidential%20deep%0Alearning%2C%20this%20paper%20integrates%20the%20evidential%20predictive%20results%20in%20the%0Across-region%20of%20mixed%20and%20original%20samples%20to%20reallocate%20the%20confidence%20degree%0Aand%20uncertainty%20measure%20of%20each%20voxel%2C%20which%20is%20realized%20by%20emphasizing%0Auncertain%20information%20of%20probability%20assignments%20fusion%20rule%20of%20traditional%0Aevidence%20theory.%20Furthermore%2C%20we%20design%20a%20voxel-level%20asymptotic%20learning%0Astrategy%20by%20introducing%20information%20entropy%20to%20combine%20with%20the%20fused%0Auncertainty%20measure%20to%20estimate%20voxel%20prediction%20more%20precisely.%20The%20model%20will%0Agradually%20pay%20attention%20to%20the%20prediction%20results%20with%20high%20uncertainty%20in%20the%0Alearning%20process%2C%20to%20learn%20the%20features%20that%20are%20difficult%20to%20master.%20The%0Aexperimental%20results%20on%20LA%2C%20Pancreas-CT%2C%20ACDC%20and%20TBAD%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20our%20proposed%20method%20in%20comparison%20with%20the%20existing%0Astate%20of%20the%20arts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06177v2&entry.124074799=Read"},
{"title": "3D Human Reconstruction in the Wild with Synthetic Data Using Generative\n  Models", "author": "Yongtao Ge and Wenjia Wang and Yongfan Chen and Hao Chen and Chunhua Shen", "abstract": "  In this work, we show that synthetic data created by generative models is\ncomplementary to computer graphics (CG) rendered data for achieving remarkable\ngeneralization performance on diverse real-world scenes for 3D human pose and\nshape estimation (HPS). Specifically, we propose an effective approach based on\nrecent diffusion models, termed HumanWild, which can effortlessly generate\nhuman images and corresponding 3D mesh annotations. We first collect a\nlarge-scale human-centric dataset with comprehensive annotations, e.g., text\ncaptions and surface normal images. Then, we train a customized ControlNet\nmodel upon this dataset to generate diverse human images and initial\nground-truth labels. At the core of this step is that we can easily obtain\nnumerous surface normal images from a 3D human parametric model, e.g., SMPL-X,\nby rendering the 3D mesh onto the image plane. As there exists inevitable noise\nin the initial labels, we then apply an off-the-shelf foundation segmentation\nmodel, i.e., SAM, to filter negative data samples. Our data generation pipeline\nis flexible and customizable to facilitate different real-world tasks, e.g.,\nego-centric scenes and perspective-distortion scenes. The generated dataset\ncomprises 0.79M images with corresponding 3D annotations, covering versatile\nviewpoints, scenes, and human identities. We train various HPS regressors on\ntop of the generated data and evaluate them on a wide range of benchmarks\n(3DPW, RICH, EgoBody, AGORA, SSP-3D) to verify the effectiveness of the\ngenerated data. By exclusively employing generative models, we generate\nlarge-scale in-the-wild human images and high-quality annotations, eliminating\nthe need for real-world data collection.\n", "link": "http://arxiv.org/abs/2403.11111v2", "date": "2024-04-11", "relevancy": 2.3895, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6032}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5952}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5883}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Human%20Reconstruction%20in%20the%20Wild%20with%20Synthetic%20Data%20Using%20Generative%0A%20%20Models&body=Title%3A%203D%20Human%20Reconstruction%20in%20the%20Wild%20with%20Synthetic%20Data%20Using%20Generative%0A%20%20Models%0AAuthor%3A%20Yongtao%20Ge%20and%20Wenjia%20Wang%20and%20Yongfan%20Chen%20and%20Hao%20Chen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20show%20that%20synthetic%20data%20created%20by%20generative%20models%20is%0Acomplementary%20to%20computer%20graphics%20%28CG%29%20rendered%20data%20for%20achieving%20remarkable%0Ageneralization%20performance%20on%20diverse%20real-world%20scenes%20for%203D%20human%20pose%20and%0Ashape%20estimation%20%28HPS%29.%20Specifically%2C%20we%20propose%20an%20effective%20approach%20based%20on%0Arecent%20diffusion%20models%2C%20termed%20HumanWild%2C%20which%20can%20effortlessly%20generate%0Ahuman%20images%20and%20corresponding%203D%20mesh%20annotations.%20We%20first%20collect%20a%0Alarge-scale%20human-centric%20dataset%20with%20comprehensive%20annotations%2C%20e.g.%2C%20text%0Acaptions%20and%20surface%20normal%20images.%20Then%2C%20we%20train%20a%20customized%20ControlNet%0Amodel%20upon%20this%20dataset%20to%20generate%20diverse%20human%20images%20and%20initial%0Aground-truth%20labels.%20At%20the%20core%20of%20this%20step%20is%20that%20we%20can%20easily%20obtain%0Anumerous%20surface%20normal%20images%20from%20a%203D%20human%20parametric%20model%2C%20e.g.%2C%20SMPL-X%2C%0Aby%20rendering%20the%203D%20mesh%20onto%20the%20image%20plane.%20As%20there%20exists%20inevitable%20noise%0Ain%20the%20initial%20labels%2C%20we%20then%20apply%20an%20off-the-shelf%20foundation%20segmentation%0Amodel%2C%20i.e.%2C%20SAM%2C%20to%20filter%20negative%20data%20samples.%20Our%20data%20generation%20pipeline%0Ais%20flexible%20and%20customizable%20to%20facilitate%20different%20real-world%20tasks%2C%20e.g.%2C%0Aego-centric%20scenes%20and%20perspective-distortion%20scenes.%20The%20generated%20dataset%0Acomprises%200.79M%20images%20with%20corresponding%203D%20annotations%2C%20covering%20versatile%0Aviewpoints%2C%20scenes%2C%20and%20human%20identities.%20We%20train%20various%20HPS%20regressors%20on%0Atop%20of%20the%20generated%20data%20and%20evaluate%20them%20on%20a%20wide%20range%20of%20benchmarks%0A%283DPW%2C%20RICH%2C%20EgoBody%2C%20AGORA%2C%20SSP-3D%29%20to%20verify%20the%20effectiveness%20of%20the%0Agenerated%20data.%20By%20exclusively%20employing%20generative%20models%2C%20we%20generate%0Alarge-scale%20in-the-wild%20human%20images%20and%20high-quality%20annotations%2C%20eliminating%0Athe%20need%20for%20real-world%20data%20collection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11111v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Reconstruction%20in%20the%20Wild%20with%20Synthetic%20Data%20Using%20Generative%0A%20%20Models&entry.906535625=Yongtao%20Ge%20and%20Wenjia%20Wang%20and%20Yongfan%20Chen%20and%20Hao%20Chen%20and%20Chunhua%20Shen&entry.1292438233=%20%20In%20this%20work%2C%20we%20show%20that%20synthetic%20data%20created%20by%20generative%20models%20is%0Acomplementary%20to%20computer%20graphics%20%28CG%29%20rendered%20data%20for%20achieving%20remarkable%0Ageneralization%20performance%20on%20diverse%20real-world%20scenes%20for%203D%20human%20pose%20and%0Ashape%20estimation%20%28HPS%29.%20Specifically%2C%20we%20propose%20an%20effective%20approach%20based%20on%0Arecent%20diffusion%20models%2C%20termed%20HumanWild%2C%20which%20can%20effortlessly%20generate%0Ahuman%20images%20and%20corresponding%203D%20mesh%20annotations.%20We%20first%20collect%20a%0Alarge-scale%20human-centric%20dataset%20with%20comprehensive%20annotations%2C%20e.g.%2C%20text%0Acaptions%20and%20surface%20normal%20images.%20Then%2C%20we%20train%20a%20customized%20ControlNet%0Amodel%20upon%20this%20dataset%20to%20generate%20diverse%20human%20images%20and%20initial%0Aground-truth%20labels.%20At%20the%20core%20of%20this%20step%20is%20that%20we%20can%20easily%20obtain%0Anumerous%20surface%20normal%20images%20from%20a%203D%20human%20parametric%20model%2C%20e.g.%2C%20SMPL-X%2C%0Aby%20rendering%20the%203D%20mesh%20onto%20the%20image%20plane.%20As%20there%20exists%20inevitable%20noise%0Ain%20the%20initial%20labels%2C%20we%20then%20apply%20an%20off-the-shelf%20foundation%20segmentation%0Amodel%2C%20i.e.%2C%20SAM%2C%20to%20filter%20negative%20data%20samples.%20Our%20data%20generation%20pipeline%0Ais%20flexible%20and%20customizable%20to%20facilitate%20different%20real-world%20tasks%2C%20e.g.%2C%0Aego-centric%20scenes%20and%20perspective-distortion%20scenes.%20The%20generated%20dataset%0Acomprises%200.79M%20images%20with%20corresponding%203D%20annotations%2C%20covering%20versatile%0Aviewpoints%2C%20scenes%2C%20and%20human%20identities.%20We%20train%20various%20HPS%20regressors%20on%0Atop%20of%20the%20generated%20data%20and%20evaluate%20them%20on%20a%20wide%20range%20of%20benchmarks%0A%283DPW%2C%20RICH%2C%20EgoBody%2C%20AGORA%2C%20SSP-3D%29%20to%20verify%20the%20effectiveness%20of%20the%0Agenerated%20data.%20By%20exclusively%20employing%20generative%20models%2C%20we%20generate%0Alarge-scale%20in-the-wild%20human%20images%20and%20high-quality%20annotations%2C%20eliminating%0Athe%20need%20for%20real-world%20data%20collection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11111v2&entry.124074799=Read"},
{"title": "Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding", "author": "Yiwen Tang and Jiaming Liu and Dong Wang and Zhigang Wang and Shanghang Zhang and Bin Zhao and Xuelong Li", "abstract": "  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n", "link": "http://arxiv.org/abs/2404.07989v1", "date": "2024-04-11", "relevancy": 2.3878, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6245}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5862}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.555}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Any2Point%3A%20Empowering%20Any-modality%20Large%20Models%20for%20Efficient%203D%0A%20%20Understanding&body=Title%3A%20Any2Point%3A%20Empowering%20Any-modality%20Large%20Models%20for%20Efficient%203D%0A%20%20Understanding%0AAuthor%3A%20Yiwen%20Tang%20and%20Jiaming%20Liu%20and%20Dong%20Wang%20and%20Zhigang%20Wang%20and%20Shanghang%20Zhang%20and%20Bin%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Large%20foundation%20models%20have%20recently%20emerged%20as%20a%20prominent%20focus%20of%0Ainterest%2C%20attaining%20superior%20performance%20in%20widespread%20scenarios.%20Due%20to%20the%0Ascarcity%20of%203D%20data%2C%20many%20efforts%20have%20been%20made%20to%20adapt%20pre-trained%0Atransformers%20from%20vision%20to%203D%20domains.%20However%2C%20such%202D-to-3D%20approaches%20are%0Astill%20limited%2C%20due%20to%20the%20potential%20loss%20of%20spatial%20geometries%20and%20high%0Acomputation%20cost.%20More%20importantly%2C%20their%20frameworks%20are%20mainly%20designed%20for%202D%0Amodels%2C%20lacking%20a%20general%20any-to-3D%20paradigm.%20In%20this%20paper%2C%20we%20introduce%0AAny2Point%2C%20a%20parameter-efficient%20method%20to%20empower%20any-modality%20large%20models%0A%28vision%2C%20language%2C%20audio%29%20for%203D%20understanding.%20Given%20a%20frozen%20transformer%20from%0Aany%20source%20modality%2C%20we%20propose%20a%203D-to-any%20%281D%20or%202D%29%20virtual%20projection%0Astrategy%20that%20correlates%20the%20input%203D%20points%20to%20the%20original%201D%20or%202D%20positions%0Awithin%20the%20source%20modality.%20This%20mechanism%20enables%20us%20to%20assign%20each%203D%20token%0Awith%20a%20positional%20encoding%20paired%20with%20the%20pre-trained%20model%2C%20which%20avoids%203D%0Ageometry%20loss%20caused%20by%20the%20true%20projection%20and%20better%20motivates%20the%0Atransformer%20for%203D%20learning%20with%201D/2D%20positional%20priors.%20Then%2C%20within%20each%0Atransformer%20block%2C%20we%20insert%20an%20any-to-3D%20guided%20adapter%20module%20for%0Aparameter-efficient%20fine-tuning.%20The%20adapter%20incorporates%20prior%20spatial%0Aknowledge%20from%20the%20source%20modality%20to%20guide%20the%20local%20feature%20aggregation%20of%203D%0Atokens%2C%20compelling%20the%20semantic%20adaption%20of%20any-modality%20transformers.%20We%0Aconduct%20extensive%20experiments%20to%20showcase%20the%20effectiveness%20and%20efficiency%20of%0Aour%20method.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/Ivan-Tang-3D/Any2Point.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07989v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any2Point%3A%20Empowering%20Any-modality%20Large%20Models%20for%20Efficient%203D%0A%20%20Understanding&entry.906535625=Yiwen%20Tang%20and%20Jiaming%20Liu%20and%20Dong%20Wang%20and%20Zhigang%20Wang%20and%20Shanghang%20Zhang%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=%20%20Large%20foundation%20models%20have%20recently%20emerged%20as%20a%20prominent%20focus%20of%0Ainterest%2C%20attaining%20superior%20performance%20in%20widespread%20scenarios.%20Due%20to%20the%0Ascarcity%20of%203D%20data%2C%20many%20efforts%20have%20been%20made%20to%20adapt%20pre-trained%0Atransformers%20from%20vision%20to%203D%20domains.%20However%2C%20such%202D-to-3D%20approaches%20are%0Astill%20limited%2C%20due%20to%20the%20potential%20loss%20of%20spatial%20geometries%20and%20high%0Acomputation%20cost.%20More%20importantly%2C%20their%20frameworks%20are%20mainly%20designed%20for%202D%0Amodels%2C%20lacking%20a%20general%20any-to-3D%20paradigm.%20In%20this%20paper%2C%20we%20introduce%0AAny2Point%2C%20a%20parameter-efficient%20method%20to%20empower%20any-modality%20large%20models%0A%28vision%2C%20language%2C%20audio%29%20for%203D%20understanding.%20Given%20a%20frozen%20transformer%20from%0Aany%20source%20modality%2C%20we%20propose%20a%203D-to-any%20%281D%20or%202D%29%20virtual%20projection%0Astrategy%20that%20correlates%20the%20input%203D%20points%20to%20the%20original%201D%20or%202D%20positions%0Awithin%20the%20source%20modality.%20This%20mechanism%20enables%20us%20to%20assign%20each%203D%20token%0Awith%20a%20positional%20encoding%20paired%20with%20the%20pre-trained%20model%2C%20which%20avoids%203D%0Ageometry%20loss%20caused%20by%20the%20true%20projection%20and%20better%20motivates%20the%0Atransformer%20for%203D%20learning%20with%201D/2D%20positional%20priors.%20Then%2C%20within%20each%0Atransformer%20block%2C%20we%20insert%20an%20any-to-3D%20guided%20adapter%20module%20for%0Aparameter-efficient%20fine-tuning.%20The%20adapter%20incorporates%20prior%20spatial%0Aknowledge%20from%20the%20source%20modality%20to%20guide%20the%20local%20feature%20aggregation%20of%203D%0Atokens%2C%20compelling%20the%20semantic%20adaption%20of%20any-modality%20transformers.%20We%0Aconduct%20extensive%20experiments%20to%20showcase%20the%20effectiveness%20and%20efficiency%20of%0Aour%20method.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/Ivan-Tang-3D/Any2Point.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07989v1&entry.124074799=Read"},
{"title": "2DLIW-SLAM:2D LiDAR-Inertial-Wheel Odometry with Real-Time Loop Closure", "author": "Bin Zhang and Zexin Peng and Bi Zeng and Junjie Lu", "abstract": "  Due to budgetary constraints, indoor navigation typically employs 2D LiDAR\nrather than 3D LiDAR. However, the utilization of 2D LiDAR in Simultaneous\nLocalization And Mapping (SLAM) frequently encounters challenges related to\nmotion degeneracy, particularly in geometrically similar environments. To\naddress this problem, this paper proposes a robust, accurate, and\nmulti-sensor-fused 2D LiDAR SLAM system specifically designed for indoor mobile\nrobots. To commence, the original LiDAR data undergoes meticulous processing\nthrough point and line extraction. Leveraging the distinctive characteristics\nof indoor environments, line-line constraints are established to complement\nother sensor data effectively, thereby augmenting the overall robustness and\nprecision of the system. Concurrently, a tightly-coupled front-end is created,\nintegrating data from the 2D LiDAR, IMU, and wheel odometry, thus enabling\nreal-time state estimation. Building upon this solid foundation, a novel global\nfeature point matching-based loop closure detection algorithm is proposed. This\nalgorithm proves highly effective in mitigating front-end accumulated errors\nand ultimately constructs a globally consistent map. The experimental results\nindicate that our system fully meets real-time requirements. When compared to\nCartographer, our system not only exhibits lower trajectory errors but also\ndemonstrates stronger robustness, particularly in degeneracy problem.\n", "link": "http://arxiv.org/abs/2404.07644v1", "date": "2024-04-11", "relevancy": 2.3776, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6206}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5878}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5708}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%202DLIW-SLAM%3A2D%20LiDAR-Inertial-Wheel%20Odometry%20with%20Real-Time%20Loop%20Closure&body=Title%3A%202DLIW-SLAM%3A2D%20LiDAR-Inertial-Wheel%20Odometry%20with%20Real-Time%20Loop%20Closure%0AAuthor%3A%20Bin%20Zhang%20and%20Zexin%20Peng%20and%20Bi%20Zeng%20and%20Junjie%20Lu%0AAbstract%3A%20%20%20Due%20to%20budgetary%20constraints%2C%20indoor%20navigation%20typically%20employs%202D%20LiDAR%0Arather%20than%203D%20LiDAR.%20However%2C%20the%20utilization%20of%202D%20LiDAR%20in%20Simultaneous%0ALocalization%20And%20Mapping%20%28SLAM%29%20frequently%20encounters%20challenges%20related%20to%0Amotion%20degeneracy%2C%20particularly%20in%20geometrically%20similar%20environments.%20To%0Aaddress%20this%20problem%2C%20this%20paper%20proposes%20a%20robust%2C%20accurate%2C%20and%0Amulti-sensor-fused%202D%20LiDAR%20SLAM%20system%20specifically%20designed%20for%20indoor%20mobile%0Arobots.%20To%20commence%2C%20the%20original%20LiDAR%20data%20undergoes%20meticulous%20processing%0Athrough%20point%20and%20line%20extraction.%20Leveraging%20the%20distinctive%20characteristics%0Aof%20indoor%20environments%2C%20line-line%20constraints%20are%20established%20to%20complement%0Aother%20sensor%20data%20effectively%2C%20thereby%20augmenting%20the%20overall%20robustness%20and%0Aprecision%20of%20the%20system.%20Concurrently%2C%20a%20tightly-coupled%20front-end%20is%20created%2C%0Aintegrating%20data%20from%20the%202D%20LiDAR%2C%20IMU%2C%20and%20wheel%20odometry%2C%20thus%20enabling%0Areal-time%20state%20estimation.%20Building%20upon%20this%20solid%20foundation%2C%20a%20novel%20global%0Afeature%20point%20matching-based%20loop%20closure%20detection%20algorithm%20is%20proposed.%20This%0Aalgorithm%20proves%20highly%20effective%20in%20mitigating%20front-end%20accumulated%20errors%0Aand%20ultimately%20constructs%20a%20globally%20consistent%20map.%20The%20experimental%20results%0Aindicate%20that%20our%20system%20fully%20meets%20real-time%20requirements.%20When%20compared%20to%0ACartographer%2C%20our%20system%20not%20only%20exhibits%20lower%20trajectory%20errors%20but%20also%0Ademonstrates%20stronger%20robustness%2C%20particularly%20in%20degeneracy%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07644v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2DLIW-SLAM%3A2D%20LiDAR-Inertial-Wheel%20Odometry%20with%20Real-Time%20Loop%20Closure&entry.906535625=Bin%20Zhang%20and%20Zexin%20Peng%20and%20Bi%20Zeng%20and%20Junjie%20Lu&entry.1292438233=%20%20Due%20to%20budgetary%20constraints%2C%20indoor%20navigation%20typically%20employs%202D%20LiDAR%0Arather%20than%203D%20LiDAR.%20However%2C%20the%20utilization%20of%202D%20LiDAR%20in%20Simultaneous%0ALocalization%20And%20Mapping%20%28SLAM%29%20frequently%20encounters%20challenges%20related%20to%0Amotion%20degeneracy%2C%20particularly%20in%20geometrically%20similar%20environments.%20To%0Aaddress%20this%20problem%2C%20this%20paper%20proposes%20a%20robust%2C%20accurate%2C%20and%0Amulti-sensor-fused%202D%20LiDAR%20SLAM%20system%20specifically%20designed%20for%20indoor%20mobile%0Arobots.%20To%20commence%2C%20the%20original%20LiDAR%20data%20undergoes%20meticulous%20processing%0Athrough%20point%20and%20line%20extraction.%20Leveraging%20the%20distinctive%20characteristics%0Aof%20indoor%20environments%2C%20line-line%20constraints%20are%20established%20to%20complement%0Aother%20sensor%20data%20effectively%2C%20thereby%20augmenting%20the%20overall%20robustness%20and%0Aprecision%20of%20the%20system.%20Concurrently%2C%20a%20tightly-coupled%20front-end%20is%20created%2C%0Aintegrating%20data%20from%20the%202D%20LiDAR%2C%20IMU%2C%20and%20wheel%20odometry%2C%20thus%20enabling%0Areal-time%20state%20estimation.%20Building%20upon%20this%20solid%20foundation%2C%20a%20novel%20global%0Afeature%20point%20matching-based%20loop%20closure%20detection%20algorithm%20is%20proposed.%20This%0Aalgorithm%20proves%20highly%20effective%20in%20mitigating%20front-end%20accumulated%20errors%0Aand%20ultimately%20constructs%20a%20globally%20consistent%20map.%20The%20experimental%20results%0Aindicate%20that%20our%20system%20fully%20meets%20real-time%20requirements.%20When%20compared%20to%0ACartographer%2C%20our%20system%20not%20only%20exhibits%20lower%20trajectory%20errors%20but%20also%0Ademonstrates%20stronger%20robustness%2C%20particularly%20in%20degeneracy%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07644v1&entry.124074799=Read"},
{"title": "LLoCO: Learning Long Contexts Offline", "author": "Sijun Tan and Xiuyu Li and Shishir Patil and Ziyang Wu and Tianjun Zhang and Kurt Keutzer and Joseph E. Gonzalez and Raluca Ada Popa", "abstract": "  Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose a\nnovel approach to address this problem by learning contexts offline through\ncontext compression and in-domain parameter-efficient finetuning. Our method\nenables an LLM to create a concise representation of the original context and\nefficiently retrieve relevant information to answer questions accurately. We\nintroduce LLoCO, a technique that combines context compression, retrieval, and\nparameter-efficient finetuning using LoRA. Our approach extends the effective\ncontext window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We\nevaluate our approach on several long-context question-answering datasets,\ndemonstrating that LLoCO significantly outperforms in-context learning while\nusing $30\\times$ fewer tokens during inference. LLoCO achieves up to\n$7.62\\times$ speed-up and substantially reduces the cost of long document\nquestion answering, making it a promising solution for efficient long context\nprocessing. Our code is publicly available at\nhttps://github.com/jeffreysijuntan/lloco.\n", "link": "http://arxiv.org/abs/2404.07979v1", "date": "2024-04-11", "relevancy": 2.3696, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4908}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4742}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4568}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLoCO%3A%20Learning%20Long%20Contexts%20Offline&body=Title%3A%20LLoCO%3A%20Learning%20Long%20Contexts%20Offline%0AAuthor%3A%20Sijun%20Tan%20and%20Xiuyu%20Li%20and%20Shishir%20Patil%20and%20Ziyang%20Wu%20and%20Tianjun%20Zhang%20and%20Kurt%20Keutzer%20and%20Joseph%20E.%20Gonzalez%20and%20Raluca%20Ada%20Popa%0AAbstract%3A%20%20%20Processing%20long%20contexts%20remains%20a%20challenge%20for%20large%20language%20models%20%28LLMs%29%0Adue%20to%20the%20quadratic%20computational%20and%20memory%20overhead%20of%20the%20self-attention%0Amechanism%20and%20the%20substantial%20KV%20cache%20sizes%20during%20generation.%20We%20propose%20a%0Anovel%20approach%20to%20address%20this%20problem%20by%20learning%20contexts%20offline%20through%0Acontext%20compression%20and%20in-domain%20parameter-efficient%20finetuning.%20Our%20method%0Aenables%20an%20LLM%20to%20create%20a%20concise%20representation%20of%20the%20original%20context%20and%0Aefficiently%20retrieve%20relevant%20information%20to%20answer%20questions%20accurately.%20We%0Aintroduce%20LLoCO%2C%20a%20technique%20that%20combines%20context%20compression%2C%20retrieval%2C%20and%0Aparameter-efficient%20finetuning%20using%20LoRA.%20Our%20approach%20extends%20the%20effective%0Acontext%20window%20of%20a%204k%20token%20LLaMA2-7B%20model%20to%20handle%20up%20to%20128k%20tokens.%20We%0Aevaluate%20our%20approach%20on%20several%20long-context%20question-answering%20datasets%2C%0Ademonstrating%20that%20LLoCO%20significantly%20outperforms%20in-context%20learning%20while%0Ausing%20%2430%5Ctimes%24%20fewer%20tokens%20during%20inference.%20LLoCO%20achieves%20up%20to%0A%247.62%5Ctimes%24%20speed-up%20and%20substantially%20reduces%20the%20cost%20of%20long%20document%0Aquestion%20answering%2C%20making%20it%20a%20promising%20solution%20for%20efficient%20long%20context%0Aprocessing.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/jeffreysijuntan/lloco.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07979v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLoCO%3A%20Learning%20Long%20Contexts%20Offline&entry.906535625=Sijun%20Tan%20and%20Xiuyu%20Li%20and%20Shishir%20Patil%20and%20Ziyang%20Wu%20and%20Tianjun%20Zhang%20and%20Kurt%20Keutzer%20and%20Joseph%20E.%20Gonzalez%20and%20Raluca%20Ada%20Popa&entry.1292438233=%20%20Processing%20long%20contexts%20remains%20a%20challenge%20for%20large%20language%20models%20%28LLMs%29%0Adue%20to%20the%20quadratic%20computational%20and%20memory%20overhead%20of%20the%20self-attention%0Amechanism%20and%20the%20substantial%20KV%20cache%20sizes%20during%20generation.%20We%20propose%20a%0Anovel%20approach%20to%20address%20this%20problem%20by%20learning%20contexts%20offline%20through%0Acontext%20compression%20and%20in-domain%20parameter-efficient%20finetuning.%20Our%20method%0Aenables%20an%20LLM%20to%20create%20a%20concise%20representation%20of%20the%20original%20context%20and%0Aefficiently%20retrieve%20relevant%20information%20to%20answer%20questions%20accurately.%20We%0Aintroduce%20LLoCO%2C%20a%20technique%20that%20combines%20context%20compression%2C%20retrieval%2C%20and%0Aparameter-efficient%20finetuning%20using%20LoRA.%20Our%20approach%20extends%20the%20effective%0Acontext%20window%20of%20a%204k%20token%20LLaMA2-7B%20model%20to%20handle%20up%20to%20128k%20tokens.%20We%0Aevaluate%20our%20approach%20on%20several%20long-context%20question-answering%20datasets%2C%0Ademonstrating%20that%20LLoCO%20significantly%20outperforms%20in-context%20learning%20while%0Ausing%20%2430%5Ctimes%24%20fewer%20tokens%20during%20inference.%20LLoCO%20achieves%20up%20to%0A%247.62%5Ctimes%24%20speed-up%20and%20substantially%20reduces%20the%20cost%20of%20long%20document%0Aquestion%20answering%2C%20making%20it%20a%20promising%20solution%20for%20efficient%20long%20context%0Aprocessing.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/jeffreysijuntan/lloco.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07979v1&entry.124074799=Read"},
{"title": "FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning", "author": "Yongcun Song and Ziqi Wang and Enrique Zuazua", "abstract": "  Federated learning (FL) is a promising framework for learning from\ndistributed data while maintaining privacy. The development of efficient FL\nalgorithms encounters various challenges, including heterogeneous data and\nsystems, limited communication capacities, and constrained local computational\nresources. Recently developed FedADMM methods show great resilience to both\ndata and system heterogeneity. However, they still suffer from performance\ndeterioration if the hyperparameters are not carefully tuned. To address this\nissue, we propose an inexact and self-adaptive FedADMM algorithm, termed\nFedADMM-InSa. First, we design an inexactness criterion for the clients' local\nupdates to eliminate the need for empirically setting the local training\naccuracy. This inexactness criterion can be assessed by each client\nindependently based on its unique condition, thereby reducing the local\ncomputational cost and mitigating the undesirable straggle effect. The\nconvergence of the resulting inexact ADMM is proved under the assumption of\nstrongly convex loss functions. Additionally, we present a self-adaptive scheme\nthat dynamically adjusts each client's penalty parameter, enhancing algorithm\nrobustness by mitigating the need for empirical penalty parameter choices for\neach client. Extensive numerical experiments on both synthetic and real-world\ndatasets are conducted. As validated by some numerical tests, our proposed\nalgorithm can reduce the clients' local computational load significantly and\nalso accelerate the learning process compared to the vanilla FedADMM.\n", "link": "http://arxiv.org/abs/2402.13989v2", "date": "2024-04-11", "relevancy": 2.3568, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4927}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4622}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4592}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FedADMM-InSa%3A%20An%20Inexact%20and%20Self-Adaptive%20ADMM%20for%20Federated%20Learning&body=Title%3A%20FedADMM-InSa%3A%20An%20Inexact%20and%20Self-Adaptive%20ADMM%20for%20Federated%20Learning%0AAuthor%3A%20Yongcun%20Song%20and%20Ziqi%20Wang%20and%20Enrique%20Zuazua%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20promising%20framework%20for%20learning%20from%0Adistributed%20data%20while%20maintaining%20privacy.%20The%20development%20of%20efficient%20FL%0Aalgorithms%20encounters%20various%20challenges%2C%20including%20heterogeneous%20data%20and%0Asystems%2C%20limited%20communication%20capacities%2C%20and%20constrained%20local%20computational%0Aresources.%20Recently%20developed%20FedADMM%20methods%20show%20great%20resilience%20to%20both%0Adata%20and%20system%20heterogeneity.%20However%2C%20they%20still%20suffer%20from%20performance%0Adeterioration%20if%20the%20hyperparameters%20are%20not%20carefully%20tuned.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20inexact%20and%20self-adaptive%20FedADMM%20algorithm%2C%20termed%0AFedADMM-InSa.%20First%2C%20we%20design%20an%20inexactness%20criterion%20for%20the%20clients%27%20local%0Aupdates%20to%20eliminate%20the%20need%20for%20empirically%20setting%20the%20local%20training%0Aaccuracy.%20This%20inexactness%20criterion%20can%20be%20assessed%20by%20each%20client%0Aindependently%20based%20on%20its%20unique%20condition%2C%20thereby%20reducing%20the%20local%0Acomputational%20cost%20and%20mitigating%20the%20undesirable%20straggle%20effect.%20The%0Aconvergence%20of%20the%20resulting%20inexact%20ADMM%20is%20proved%20under%20the%20assumption%20of%0Astrongly%20convex%20loss%20functions.%20Additionally%2C%20we%20present%20a%20self-adaptive%20scheme%0Athat%20dynamically%20adjusts%20each%20client%27s%20penalty%20parameter%2C%20enhancing%20algorithm%0Arobustness%20by%20mitigating%20the%20need%20for%20empirical%20penalty%20parameter%20choices%20for%0Aeach%20client.%20Extensive%20numerical%20experiments%20on%20both%20synthetic%20and%20real-world%0Adatasets%20are%20conducted.%20As%20validated%20by%20some%20numerical%20tests%2C%20our%20proposed%0Aalgorithm%20can%20reduce%20the%20clients%27%20local%20computational%20load%20significantly%20and%0Aalso%20accelerate%20the%20learning%20process%20compared%20to%20the%20vanilla%20FedADMM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13989v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedADMM-InSa%3A%20An%20Inexact%20and%20Self-Adaptive%20ADMM%20for%20Federated%20Learning&entry.906535625=Yongcun%20Song%20and%20Ziqi%20Wang%20and%20Enrique%20Zuazua&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20promising%20framework%20for%20learning%20from%0Adistributed%20data%20while%20maintaining%20privacy.%20The%20development%20of%20efficient%20FL%0Aalgorithms%20encounters%20various%20challenges%2C%20including%20heterogeneous%20data%20and%0Asystems%2C%20limited%20communication%20capacities%2C%20and%20constrained%20local%20computational%0Aresources.%20Recently%20developed%20FedADMM%20methods%20show%20great%20resilience%20to%20both%0Adata%20and%20system%20heterogeneity.%20However%2C%20they%20still%20suffer%20from%20performance%0Adeterioration%20if%20the%20hyperparameters%20are%20not%20carefully%20tuned.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20inexact%20and%20self-adaptive%20FedADMM%20algorithm%2C%20termed%0AFedADMM-InSa.%20First%2C%20we%20design%20an%20inexactness%20criterion%20for%20the%20clients%27%20local%0Aupdates%20to%20eliminate%20the%20need%20for%20empirically%20setting%20the%20local%20training%0Aaccuracy.%20This%20inexactness%20criterion%20can%20be%20assessed%20by%20each%20client%0Aindependently%20based%20on%20its%20unique%20condition%2C%20thereby%20reducing%20the%20local%0Acomputational%20cost%20and%20mitigating%20the%20undesirable%20straggle%20effect.%20The%0Aconvergence%20of%20the%20resulting%20inexact%20ADMM%20is%20proved%20under%20the%20assumption%20of%0Astrongly%20convex%20loss%20functions.%20Additionally%2C%20we%20present%20a%20self-adaptive%20scheme%0Athat%20dynamically%20adjusts%20each%20client%27s%20penalty%20parameter%2C%20enhancing%20algorithm%0Arobustness%20by%20mitigating%20the%20need%20for%20empirical%20penalty%20parameter%20choices%20for%0Aeach%20client.%20Extensive%20numerical%20experiments%20on%20both%20synthetic%20and%20real-world%0Adatasets%20are%20conducted.%20As%20validated%20by%20some%20numerical%20tests%2C%20our%20proposed%0Aalgorithm%20can%20reduce%20the%20clients%27%20local%20computational%20load%20significantly%20and%0Aalso%20accelerate%20the%20learning%20process%20compared%20to%20the%20vanilla%20FedADMM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13989v2&entry.124074799=Read"},
{"title": "AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene\n  Graph Generation", "author": "Yansheng Li and Kun Li and Yongjun Zhang and Linlin Wang and Dingwen Zhang", "abstract": "  Scene graph generation (SGG) aims to understand the visual objects and their\nsemantic relationships from one given image. Until now, lots of SGG datasets\nwith the eyelevel view are released but the SGG dataset with the overhead view\nis scarcely studied. By contrast to the object occlusion problem in the\neyelevel view, which impedes the SGG, the overhead view provides a new\nperspective that helps to promote the SGG by providing a clear perception of\nthe spatial relationships of objects in the ground scene. To fill in the gap of\nthe overhead view dataset, this paper constructs and releases an aerial image\nurban scene graph generation (AUG) dataset. Images from the AUG dataset are\ncaptured with the low-attitude overhead view. In the AUG dataset, 25,594\nobjects, 16,970 relationships, and 27,175 attributes are manually annotated. To\navoid the local context being overwhelmed in the complex aerial urban scene,\nthis paper proposes one new locality-preserving graph convolutional network\n(LPG). Different from the traditional graph convolutional network, which has\nthe natural advantage of capturing the global context for SGG, the\nconvolutional layer in the LPG integrates the non-destructive initial features\nof the objects with dynamically updated neighborhood information to preserve\nthe local context under the premise of mining the global context. To address\nthe problem that there exists an extra-large number of potential object\nrelationship pairs but only a small part of them is meaningful in AUG, we\npropose the adaptive bounding box scaling factor for potential relationship\ndetection (ABS-PRD) to intelligently prune the meaningless relationship pairs.\nExtensive experiments on the AUG dataset show that our LPG can significantly\noutperform the state-of-the-art methods and the effectiveness of the proposed\nlocality-preserving strategy.\n", "link": "http://arxiv.org/abs/2404.07788v1", "date": "2024-04-11", "relevancy": 2.3137, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5901}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5725}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5691}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AUG%3A%20A%20New%20Dataset%20and%20An%20Efficient%20Model%20for%20Aerial%20Image%20Urban%20Scene%0A%20%20Graph%20Generation&body=Title%3A%20AUG%3A%20A%20New%20Dataset%20and%20An%20Efficient%20Model%20for%20Aerial%20Image%20Urban%20Scene%0A%20%20Graph%20Generation%0AAuthor%3A%20Yansheng%20Li%20and%20Kun%20Li%20and%20Yongjun%20Zhang%20and%20Linlin%20Wang%20and%20Dingwen%20Zhang%0AAbstract%3A%20%20%20Scene%20graph%20generation%20%28SGG%29%20aims%20to%20understand%20the%20visual%20objects%20and%20their%0Asemantic%20relationships%20from%20one%20given%20image.%20Until%20now%2C%20lots%20of%20SGG%20datasets%0Awith%20the%20eyelevel%20view%20are%20released%20but%20the%20SGG%20dataset%20with%20the%20overhead%20view%0Ais%20scarcely%20studied.%20By%20contrast%20to%20the%20object%20occlusion%20problem%20in%20the%0Aeyelevel%20view%2C%20which%20impedes%20the%20SGG%2C%20the%20overhead%20view%20provides%20a%20new%0Aperspective%20that%20helps%20to%20promote%20the%20SGG%20by%20providing%20a%20clear%20perception%20of%0Athe%20spatial%20relationships%20of%20objects%20in%20the%20ground%20scene.%20To%20fill%20in%20the%20gap%20of%0Athe%20overhead%20view%20dataset%2C%20this%20paper%20constructs%20and%20releases%20an%20aerial%20image%0Aurban%20scene%20graph%20generation%20%28AUG%29%20dataset.%20Images%20from%20the%20AUG%20dataset%20are%0Acaptured%20with%20the%20low-attitude%20overhead%20view.%20In%20the%20AUG%20dataset%2C%2025%2C594%0Aobjects%2C%2016%2C970%20relationships%2C%20and%2027%2C175%20attributes%20are%20manually%20annotated.%20To%0Aavoid%20the%20local%20context%20being%20overwhelmed%20in%20the%20complex%20aerial%20urban%20scene%2C%0Athis%20paper%20proposes%20one%20new%20locality-preserving%20graph%20convolutional%20network%0A%28LPG%29.%20Different%20from%20the%20traditional%20graph%20convolutional%20network%2C%20which%20has%0Athe%20natural%20advantage%20of%20capturing%20the%20global%20context%20for%20SGG%2C%20the%0Aconvolutional%20layer%20in%20the%20LPG%20integrates%20the%20non-destructive%20initial%20features%0Aof%20the%20objects%20with%20dynamically%20updated%20neighborhood%20information%20to%20preserve%0Athe%20local%20context%20under%20the%20premise%20of%20mining%20the%20global%20context.%20To%20address%0Athe%20problem%20that%20there%20exists%20an%20extra-large%20number%20of%20potential%20object%0Arelationship%20pairs%20but%20only%20a%20small%20part%20of%20them%20is%20meaningful%20in%20AUG%2C%20we%0Apropose%20the%20adaptive%20bounding%20box%20scaling%20factor%20for%20potential%20relationship%0Adetection%20%28ABS-PRD%29%20to%20intelligently%20prune%20the%20meaningless%20relationship%20pairs.%0AExtensive%20experiments%20on%20the%20AUG%20dataset%20show%20that%20our%20LPG%20can%20significantly%0Aoutperform%20the%20state-of-the-art%20methods%20and%20the%20effectiveness%20of%20the%20proposed%0Alocality-preserving%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07788v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AUG%3A%20A%20New%20Dataset%20and%20An%20Efficient%20Model%20for%20Aerial%20Image%20Urban%20Scene%0A%20%20Graph%20Generation&entry.906535625=Yansheng%20Li%20and%20Kun%20Li%20and%20Yongjun%20Zhang%20and%20Linlin%20Wang%20and%20Dingwen%20Zhang&entry.1292438233=%20%20Scene%20graph%20generation%20%28SGG%29%20aims%20to%20understand%20the%20visual%20objects%20and%20their%0Asemantic%20relationships%20from%20one%20given%20image.%20Until%20now%2C%20lots%20of%20SGG%20datasets%0Awith%20the%20eyelevel%20view%20are%20released%20but%20the%20SGG%20dataset%20with%20the%20overhead%20view%0Ais%20scarcely%20studied.%20By%20contrast%20to%20the%20object%20occlusion%20problem%20in%20the%0Aeyelevel%20view%2C%20which%20impedes%20the%20SGG%2C%20the%20overhead%20view%20provides%20a%20new%0Aperspective%20that%20helps%20to%20promote%20the%20SGG%20by%20providing%20a%20clear%20perception%20of%0Athe%20spatial%20relationships%20of%20objects%20in%20the%20ground%20scene.%20To%20fill%20in%20the%20gap%20of%0Athe%20overhead%20view%20dataset%2C%20this%20paper%20constructs%20and%20releases%20an%20aerial%20image%0Aurban%20scene%20graph%20generation%20%28AUG%29%20dataset.%20Images%20from%20the%20AUG%20dataset%20are%0Acaptured%20with%20the%20low-attitude%20overhead%20view.%20In%20the%20AUG%20dataset%2C%2025%2C594%0Aobjects%2C%2016%2C970%20relationships%2C%20and%2027%2C175%20attributes%20are%20manually%20annotated.%20To%0Aavoid%20the%20local%20context%20being%20overwhelmed%20in%20the%20complex%20aerial%20urban%20scene%2C%0Athis%20paper%20proposes%20one%20new%20locality-preserving%20graph%20convolutional%20network%0A%28LPG%29.%20Different%20from%20the%20traditional%20graph%20convolutional%20network%2C%20which%20has%0Athe%20natural%20advantage%20of%20capturing%20the%20global%20context%20for%20SGG%2C%20the%0Aconvolutional%20layer%20in%20the%20LPG%20integrates%20the%20non-destructive%20initial%20features%0Aof%20the%20objects%20with%20dynamically%20updated%20neighborhood%20information%20to%20preserve%0Athe%20local%20context%20under%20the%20premise%20of%20mining%20the%20global%20context.%20To%20address%0Athe%20problem%20that%20there%20exists%20an%20extra-large%20number%20of%20potential%20object%0Arelationship%20pairs%20but%20only%20a%20small%20part%20of%20them%20is%20meaningful%20in%20AUG%2C%20we%0Apropose%20the%20adaptive%20bounding%20box%20scaling%20factor%20for%20potential%20relationship%0Adetection%20%28ABS-PRD%29%20to%20intelligently%20prune%20the%20meaningless%20relationship%20pairs.%0AExtensive%20experiments%20on%20the%20AUG%20dataset%20show%20that%20our%20LPG%20can%20significantly%0Aoutperform%20the%20state-of-the-art%20methods%20and%20the%20effectiveness%20of%20the%20proposed%0Alocality-preserving%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07788v1&entry.124074799=Read"},
{"title": "Two Effects, One Trigger: On the Modality Gap, Object Bias, and\n  Information Imbalance in Contrastive Vision-Language Representation Learning", "author": "Simon Schrodi and David T. Hoffmann and Max Argus and Volker Fischer and Thomas Brox", "abstract": "  Contrastive vision-language models like CLIP have gained popularity for their\nversatile applicable learned representations in various downstream tasks.\nDespite their successes in some tasks, like zero-shot image recognition, they\nalso perform surprisingly poor on other tasks, like attribute detection.\nPrevious work has attributed these challenges to the modality gap, a separation\nof image and text in the shared representation space, and a bias towards\nobjects over other factors, such as attributes. In this work we investigate\nboth phenomena. We find that only a few embedding dimensions drive the modality\ngap. Further, we propose a measure for object bias and find that object bias\ndoes not lead to worse performance on other concepts, such as attributes. But\nwhat leads to the emergence of the modality gap and object bias? To answer this\nquestion we carefully designed an experimental setting which allows us to\ncontrol the amount of shared information between the modalities. This revealed\nthat the driving factor behind both, the modality gap and the object bias, is\nthe information imbalance between images and captions.\n", "link": "http://arxiv.org/abs/2404.07983v1", "date": "2024-04-11", "relevancy": 2.3094, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6214}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5482}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.545}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Two%20Effects%2C%20One%20Trigger%3A%20On%20the%20Modality%20Gap%2C%20Object%20Bias%2C%20and%0A%20%20Information%20Imbalance%20in%20Contrastive%20Vision-Language%20Representation%20Learning&body=Title%3A%20Two%20Effects%2C%20One%20Trigger%3A%20On%20the%20Modality%20Gap%2C%20Object%20Bias%2C%20and%0A%20%20Information%20Imbalance%20in%20Contrastive%20Vision-Language%20Representation%20Learning%0AAuthor%3A%20Simon%20Schrodi%20and%20David%20T.%20Hoffmann%20and%20Max%20Argus%20and%20Volker%20Fischer%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20Contrastive%20vision-language%20models%20like%20CLIP%20have%20gained%20popularity%20for%20their%0Aversatile%20applicable%20learned%20representations%20in%20various%20downstream%20tasks.%0ADespite%20their%20successes%20in%20some%20tasks%2C%20like%20zero-shot%20image%20recognition%2C%20they%0Aalso%20perform%20surprisingly%20poor%20on%20other%20tasks%2C%20like%20attribute%20detection.%0APrevious%20work%20has%20attributed%20these%20challenges%20to%20the%20modality%20gap%2C%20a%20separation%0Aof%20image%20and%20text%20in%20the%20shared%20representation%20space%2C%20and%20a%20bias%20towards%0Aobjects%20over%20other%20factors%2C%20such%20as%20attributes.%20In%20this%20work%20we%20investigate%0Aboth%20phenomena.%20We%20find%20that%20only%20a%20few%20embedding%20dimensions%20drive%20the%20modality%0Agap.%20Further%2C%20we%20propose%20a%20measure%20for%20object%20bias%20and%20find%20that%20object%20bias%0Adoes%20not%20lead%20to%20worse%20performance%20on%20other%20concepts%2C%20such%20as%20attributes.%20But%0Awhat%20leads%20to%20the%20emergence%20of%20the%20modality%20gap%20and%20object%20bias%3F%20To%20answer%20this%0Aquestion%20we%20carefully%20designed%20an%20experimental%20setting%20which%20allows%20us%20to%0Acontrol%20the%20amount%20of%20shared%20information%20between%20the%20modalities.%20This%20revealed%0Athat%20the%20driving%20factor%20behind%20both%2C%20the%20modality%20gap%20and%20the%20object%20bias%2C%20is%0Athe%20information%20imbalance%20between%20images%20and%20captions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07983v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Effects%2C%20One%20Trigger%3A%20On%20the%20Modality%20Gap%2C%20Object%20Bias%2C%20and%0A%20%20Information%20Imbalance%20in%20Contrastive%20Vision-Language%20Representation%20Learning&entry.906535625=Simon%20Schrodi%20and%20David%20T.%20Hoffmann%20and%20Max%20Argus%20and%20Volker%20Fischer%20and%20Thomas%20Brox&entry.1292438233=%20%20Contrastive%20vision-language%20models%20like%20CLIP%20have%20gained%20popularity%20for%20their%0Aversatile%20applicable%20learned%20representations%20in%20various%20downstream%20tasks.%0ADespite%20their%20successes%20in%20some%20tasks%2C%20like%20zero-shot%20image%20recognition%2C%20they%0Aalso%20perform%20surprisingly%20poor%20on%20other%20tasks%2C%20like%20attribute%20detection.%0APrevious%20work%20has%20attributed%20these%20challenges%20to%20the%20modality%20gap%2C%20a%20separation%0Aof%20image%20and%20text%20in%20the%20shared%20representation%20space%2C%20and%20a%20bias%20towards%0Aobjects%20over%20other%20factors%2C%20such%20as%20attributes.%20In%20this%20work%20we%20investigate%0Aboth%20phenomena.%20We%20find%20that%20only%20a%20few%20embedding%20dimensions%20drive%20the%20modality%0Agap.%20Further%2C%20we%20propose%20a%20measure%20for%20object%20bias%20and%20find%20that%20object%20bias%0Adoes%20not%20lead%20to%20worse%20performance%20on%20other%20concepts%2C%20such%20as%20attributes.%20But%0Awhat%20leads%20to%20the%20emergence%20of%20the%20modality%20gap%20and%20object%20bias%3F%20To%20answer%20this%0Aquestion%20we%20carefully%20designed%20an%20experimental%20setting%20which%20allows%20us%20to%0Acontrol%20the%20amount%20of%20shared%20information%20between%20the%20modalities.%20This%20revealed%0Athat%20the%20driving%20factor%20behind%20both%2C%20the%20modality%20gap%20and%20the%20object%20bias%2C%20is%0Athe%20information%20imbalance%20between%20images%20and%20captions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07983v1&entry.124074799=Read"},
{"title": "Gemma: Open Models Based on Gemini Research and Technology", "author": " Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivi\u00e8re and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and L\u00e9onard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Am\u00e9lie H\u00e9liou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Cl\u00e9ment Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Miku\u0142a and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Cl\u00e9ment Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy", "abstract": "  This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.\n", "link": "http://arxiv.org/abs/2403.08295v3", "date": "2024-04-11", "relevancy": 2.3049, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.481}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology&body=Title%3A%20Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology%0AAuthor%3A%20%20Gemma%20Team%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20L%C3%A9onard%20Hussenot%20and%20Pier%20Giuseppe%20Sessa%20and%20Aakanksha%20Chowdhery%20and%20Adam%20Roberts%20and%20Aditya%20Barua%20and%20Alex%20Botev%20and%20Alex%20Castro-Ros%20and%20Ambrose%20Slone%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Andrea%20Tacchetti%20and%20Anna%20Bulanova%20and%20Antonia%20Paterson%20and%20Beth%20Tsai%20and%20Bobak%20Shahriari%20and%20Charline%20Le%20Lan%20and%20Christopher%20A.%20Choquette-Choo%20and%20Cl%C3%A9ment%20Crepy%20and%20Daniel%20Cer%20and%20Daphne%20Ippolito%20and%20David%20Reid%20and%20Elena%20Buchatskaya%20and%20Eric%20Ni%20and%20Eric%20Noland%20and%20Geng%20Yan%20and%20George%20Tucker%20and%20George-Christian%20Muraru%20and%20Grigory%20Rozhdestvenskiy%20and%20Henryk%20Michalewski%20and%20Ian%20Tenney%20and%20Ivan%20Grishchenko%20and%20Jacob%20Austin%20and%20James%20Keeling%20and%20Jane%20Labanowski%20and%20Jean-Baptiste%20Lespiau%20and%20Jeff%20Stanway%20and%20Jenny%20Brennan%20and%20Jeremy%20Chen%20and%20Johan%20Ferret%20and%20Justin%20Chiu%20and%20Justin%20Mao-Jones%20and%20Katherine%20Lee%20and%20Kathy%20Yu%20and%20Katie%20Millican%20and%20Lars%20Lowe%20Sjoesund%20and%20Lisa%20Lee%20and%20Lucas%20Dixon%20and%20Machel%20Reid%20and%20Maciej%20Miku%C5%82a%20and%20Mateo%20Wirth%20and%20Michael%20Sharman%20and%20Nikolai%20Chinaev%20and%20Nithum%20Thain%20and%20Olivier%20Bachem%20and%20Oscar%20Chang%20and%20Oscar%20Wahltinez%20and%20Paige%20Bailey%20and%20Paul%20Michel%20and%20Petko%20Yotov%20and%20Rahma%20Chaabouni%20and%20Ramona%20Comanescu%20and%20Reena%20Jana%20and%20Rohan%20Anil%20and%20Ross%20McIlroy%20and%20Ruibo%20Liu%20and%20Ryan%20Mullins%20and%20Samuel%20L%20Smith%20and%20Sebastian%20Borgeaud%20and%20Sertan%20Girgin%20and%20Sholto%20Douglas%20and%20Shree%20Pandya%20and%20Siamak%20Shakeri%20and%20Soham%20De%20and%20Ted%20Klimenko%20and%20Tom%20Hennigan%20and%20Vlad%20Feinberg%20and%20Wojciech%20Stokowiec%20and%20Yu-hui%20Chen%20and%20Zafarali%20Ahmed%20and%20Zhitao%20Gong%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Cl%C3%A9ment%20Farabet%20and%20Oriol%20Vinyals%20and%20Jeff%20Dean%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Zoubin%20Ghahramani%20and%20Douglas%20Eck%20and%20Joelle%20Barral%20and%20Fernando%20Pereira%20and%20Eli%20Collins%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy%0AAbstract%3A%20%20%20This%20work%20introduces%20Gemma%2C%20a%20family%20of%20lightweight%2C%20state-of-the%20art%20open%0Amodels%20built%20from%20the%20research%20and%20technology%20used%20to%20create%20Gemini%20models.%0AGemma%20models%20demonstrate%20strong%20performance%20across%20academic%20benchmarks%20for%0Alanguage%20understanding%2C%20reasoning%2C%20and%20safety.%20We%20release%20two%20sizes%20of%20models%0A%282%20billion%20and%207%20billion%20parameters%29%2C%20and%20provide%20both%20pretrained%20and%0Afine-tuned%20checkpoints.%20Gemma%20outperforms%20similarly%20sized%20open%20models%20on%2011%20out%0Aof%2018%20text-based%20tasks%2C%20and%20we%20present%20comprehensive%20evaluations%20of%20safety%20and%0Aresponsibility%20aspects%20of%20the%20models%2C%20alongside%20a%20detailed%20description%20of%20model%0Adevelopment.%20We%20believe%20the%20responsible%20release%20of%20LLMs%20is%20critical%20for%0Aimproving%20the%20safety%20of%20frontier%20models%2C%20and%20for%20enabling%20the%20next%20wave%20of%20LLM%0Ainnovations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08295v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology&entry.906535625=%20Gemma%20Team%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20L%C3%A9onard%20Hussenot%20and%20Pier%20Giuseppe%20Sessa%20and%20Aakanksha%20Chowdhery%20and%20Adam%20Roberts%20and%20Aditya%20Barua%20and%20Alex%20Botev%20and%20Alex%20Castro-Ros%20and%20Ambrose%20Slone%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Andrea%20Tacchetti%20and%20Anna%20Bulanova%20and%20Antonia%20Paterson%20and%20Beth%20Tsai%20and%20Bobak%20Shahriari%20and%20Charline%20Le%20Lan%20and%20Christopher%20A.%20Choquette-Choo%20and%20Cl%C3%A9ment%20Crepy%20and%20Daniel%20Cer%20and%20Daphne%20Ippolito%20and%20David%20Reid%20and%20Elena%20Buchatskaya%20and%20Eric%20Ni%20and%20Eric%20Noland%20and%20Geng%20Yan%20and%20George%20Tucker%20and%20George-Christian%20Muraru%20and%20Grigory%20Rozhdestvenskiy%20and%20Henryk%20Michalewski%20and%20Ian%20Tenney%20and%20Ivan%20Grishchenko%20and%20Jacob%20Austin%20and%20James%20Keeling%20and%20Jane%20Labanowski%20and%20Jean-Baptiste%20Lespiau%20and%20Jeff%20Stanway%20and%20Jenny%20Brennan%20and%20Jeremy%20Chen%20and%20Johan%20Ferret%20and%20Justin%20Chiu%20and%20Justin%20Mao-Jones%20and%20Katherine%20Lee%20and%20Kathy%20Yu%20and%20Katie%20Millican%20and%20Lars%20Lowe%20Sjoesund%20and%20Lisa%20Lee%20and%20Lucas%20Dixon%20and%20Machel%20Reid%20and%20Maciej%20Miku%C5%82a%20and%20Mateo%20Wirth%20and%20Michael%20Sharman%20and%20Nikolai%20Chinaev%20and%20Nithum%20Thain%20and%20Olivier%20Bachem%20and%20Oscar%20Chang%20and%20Oscar%20Wahltinez%20and%20Paige%20Bailey%20and%20Paul%20Michel%20and%20Petko%20Yotov%20and%20Rahma%20Chaabouni%20and%20Ramona%20Comanescu%20and%20Reena%20Jana%20and%20Rohan%20Anil%20and%20Ross%20McIlroy%20and%20Ruibo%20Liu%20and%20Ryan%20Mullins%20and%20Samuel%20L%20Smith%20and%20Sebastian%20Borgeaud%20and%20Sertan%20Girgin%20and%20Sholto%20Douglas%20and%20Shree%20Pandya%20and%20Siamak%20Shakeri%20and%20Soham%20De%20and%20Ted%20Klimenko%20and%20Tom%20Hennigan%20and%20Vlad%20Feinberg%20and%20Wojciech%20Stokowiec%20and%20Yu-hui%20Chen%20and%20Zafarali%20Ahmed%20and%20Zhitao%20Gong%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Cl%C3%A9ment%20Farabet%20and%20Oriol%20Vinyals%20and%20Jeff%20Dean%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Zoubin%20Ghahramani%20and%20Douglas%20Eck%20and%20Joelle%20Barral%20and%20Fernando%20Pereira%20and%20Eli%20Collins%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy&entry.1292438233=%20%20This%20work%20introduces%20Gemma%2C%20a%20family%20of%20lightweight%2C%20state-of-the%20art%20open%0Amodels%20built%20from%20the%20research%20and%20technology%20used%20to%20create%20Gemini%20models.%0AGemma%20models%20demonstrate%20strong%20performance%20across%20academic%20benchmarks%20for%0Alanguage%20understanding%2C%20reasoning%2C%20and%20safety.%20We%20release%20two%20sizes%20of%20models%0A%282%20billion%20and%207%20billion%20parameters%29%2C%20and%20provide%20both%20pretrained%20and%0Afine-tuned%20checkpoints.%20Gemma%20outperforms%20similarly%20sized%20open%20models%20on%2011%20out%0Aof%2018%20text-based%20tasks%2C%20and%20we%20present%20comprehensive%20evaluations%20of%20safety%20and%0Aresponsibility%20aspects%20of%20the%20models%2C%20alongside%20a%20detailed%20description%20of%20model%0Adevelopment.%20We%20believe%20the%20responsible%20release%20of%20LLMs%20is%20critical%20for%0Aimproving%20the%20safety%20of%20frontier%20models%2C%20and%20for%20enabling%20the%20next%20wave%20of%20LLM%0Ainnovations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08295v3&entry.124074799=Read"},
{"title": "Demystifying Why Local Aggregation Helps: Convergence Analysis of\n  Hierarchical SGD", "author": "Jiayi Wang and Shiqiang Wang and Rong-Rong Chen and Mingyue Ji", "abstract": "  Hierarchical SGD (H-SGD) has emerged as a new distributed SGD algorithm for\nmulti-level communication networks. In H-SGD, before each global aggregation,\nworkers send their updated local models to local servers for aggregations.\nDespite recent research efforts, the effect of local aggregation on global\nconvergence still lacks theoretical understanding. In this work, we first\nintroduce a new notion of \"upward\" and \"downward\" divergences. We then use it\nto conduct a novel analysis to obtain a worst-case convergence upper bound for\ntwo-level H-SGD with non-IID data, non-convex objective function, and\nstochastic gradient. By extending this result to the case with random grouping,\nwe observe that this convergence upper bound of H-SGD is between the upper\nbounds of two single-level local SGD settings, with the number of local\niterations equal to the local and global update periods in H-SGD, respectively.\nWe refer to this as the \"sandwich behavior\". Furthermore, we extend our\nanalytical approach based on \"upward\" and \"downward\" divergences to study the\nconvergence for the general case of H-SGD with more than two levels, where the\n\"sandwich behavior\" still holds. Our theoretical results provide key insights\nof why local aggregation can be beneficial in improving the convergence of\nH-SGD.\n", "link": "http://arxiv.org/abs/2010.12998v4", "date": "2024-04-11", "relevancy": 2.2953, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4666}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4575}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4531}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Demystifying%20Why%20Local%20Aggregation%20Helps%3A%20Convergence%20Analysis%20of%0A%20%20Hierarchical%20SGD&body=Title%3A%20Demystifying%20Why%20Local%20Aggregation%20Helps%3A%20Convergence%20Analysis%20of%0A%20%20Hierarchical%20SGD%0AAuthor%3A%20Jiayi%20Wang%20and%20Shiqiang%20Wang%20and%20Rong-Rong%20Chen%20and%20Mingyue%20Ji%0AAbstract%3A%20%20%20Hierarchical%20SGD%20%28H-SGD%29%20has%20emerged%20as%20a%20new%20distributed%20SGD%20algorithm%20for%0Amulti-level%20communication%20networks.%20In%20H-SGD%2C%20before%20each%20global%20aggregation%2C%0Aworkers%20send%20their%20updated%20local%20models%20to%20local%20servers%20for%20aggregations.%0ADespite%20recent%20research%20efforts%2C%20the%20effect%20of%20local%20aggregation%20on%20global%0Aconvergence%20still%20lacks%20theoretical%20understanding.%20In%20this%20work%2C%20we%20first%0Aintroduce%20a%20new%20notion%20of%20%22upward%22%20and%20%22downward%22%20divergences.%20We%20then%20use%20it%0Ato%20conduct%20a%20novel%20analysis%20to%20obtain%20a%20worst-case%20convergence%20upper%20bound%20for%0Atwo-level%20H-SGD%20with%20non-IID%20data%2C%20non-convex%20objective%20function%2C%20and%0Astochastic%20gradient.%20By%20extending%20this%20result%20to%20the%20case%20with%20random%20grouping%2C%0Awe%20observe%20that%20this%20convergence%20upper%20bound%20of%20H-SGD%20is%20between%20the%20upper%0Abounds%20of%20two%20single-level%20local%20SGD%20settings%2C%20with%20the%20number%20of%20local%0Aiterations%20equal%20to%20the%20local%20and%20global%20update%20periods%20in%20H-SGD%2C%20respectively.%0AWe%20refer%20to%20this%20as%20the%20%22sandwich%20behavior%22.%20Furthermore%2C%20we%20extend%20our%0Aanalytical%20approach%20based%20on%20%22upward%22%20and%20%22downward%22%20divergences%20to%20study%20the%0Aconvergence%20for%20the%20general%20case%20of%20H-SGD%20with%20more%20than%20two%20levels%2C%20where%20the%0A%22sandwich%20behavior%22%20still%20holds.%20Our%20theoretical%20results%20provide%20key%20insights%0Aof%20why%20local%20aggregation%20can%20be%20beneficial%20in%20improving%20the%20convergence%20of%0AH-SGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2010.12998v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20Why%20Local%20Aggregation%20Helps%3A%20Convergence%20Analysis%20of%0A%20%20Hierarchical%20SGD&entry.906535625=Jiayi%20Wang%20and%20Shiqiang%20Wang%20and%20Rong-Rong%20Chen%20and%20Mingyue%20Ji&entry.1292438233=%20%20Hierarchical%20SGD%20%28H-SGD%29%20has%20emerged%20as%20a%20new%20distributed%20SGD%20algorithm%20for%0Amulti-level%20communication%20networks.%20In%20H-SGD%2C%20before%20each%20global%20aggregation%2C%0Aworkers%20send%20their%20updated%20local%20models%20to%20local%20servers%20for%20aggregations.%0ADespite%20recent%20research%20efforts%2C%20the%20effect%20of%20local%20aggregation%20on%20global%0Aconvergence%20still%20lacks%20theoretical%20understanding.%20In%20this%20work%2C%20we%20first%0Aintroduce%20a%20new%20notion%20of%20%22upward%22%20and%20%22downward%22%20divergences.%20We%20then%20use%20it%0Ato%20conduct%20a%20novel%20analysis%20to%20obtain%20a%20worst-case%20convergence%20upper%20bound%20for%0Atwo-level%20H-SGD%20with%20non-IID%20data%2C%20non-convex%20objective%20function%2C%20and%0Astochastic%20gradient.%20By%20extending%20this%20result%20to%20the%20case%20with%20random%20grouping%2C%0Awe%20observe%20that%20this%20convergence%20upper%20bound%20of%20H-SGD%20is%20between%20the%20upper%0Abounds%20of%20two%20single-level%20local%20SGD%20settings%2C%20with%20the%20number%20of%20local%0Aiterations%20equal%20to%20the%20local%20and%20global%20update%20periods%20in%20H-SGD%2C%20respectively.%0AWe%20refer%20to%20this%20as%20the%20%22sandwich%20behavior%22.%20Furthermore%2C%20we%20extend%20our%0Aanalytical%20approach%20based%20on%20%22upward%22%20and%20%22downward%22%20divergences%20to%20study%20the%0Aconvergence%20for%20the%20general%20case%20of%20H-SGD%20with%20more%20than%20two%20levels%2C%20where%20the%0A%22sandwich%20behavior%22%20still%20holds.%20Our%20theoretical%20results%20provide%20key%20insights%0Aof%20why%20local%20aggregation%20can%20be%20beneficial%20in%20improving%20the%20convergence%20of%0AH-SGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2010.12998v4&entry.124074799=Read"},
{"title": "Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively\n  Generalizable Spatial Concepts for Language-Guided Robot Manipulation", "author": "Namasivayam Kalithasan and Sachit Sachdeva and Himanshu Gaurav Singh and Divyanshu Aggarwal and Gurarmaan Singh Panjeta and Vishal Bindal and Arnav Tuli and Rohan Paul and Parag Singla", "abstract": "  Our goal is to build embodied agents that can learn inductively generalizable\nspatial concepts in a continual manner, e.g, constructing a tower of a given\nheight. Existing work suffers from certain limitations (a) (Liang et al., 2023)\nand their multi-modal extensions, rely heavily on prior knowledge and are not\ngrounded in the demonstrations (b) (Liu et al., 2023) lack the ability to\ngeneralize due to their purely neural approach. A key challenge is to achieve a\nfine balance between symbolic representations which have the capability to\ngeneralize, and neural representations that are physically grounded. In\nresponse, we propose a neuro-symbolic approach by expressing inductive concepts\nas symbolic compositions over grounded neural concepts. Our key insight is to\ndecompose the concept learning problem into the following steps 1) Sketch:\nGetting a programmatic representation for the given instruction 2) Plan:\nPerform Model-Based RL over the sequence of grounded neural action concepts to\nlearn a grounded plan 3) Generalize: Abstract out a generic (lifted) Python\nprogram to facilitate generalizability. Continual learning is achieved by\ninterspersing learning of grounded neural concepts with higher level symbolic\nconstructs. Our experiments demonstrate that our approach significantly\noutperforms existing baselines in terms of its ability to learn novel concepts\nand generalize inductively.\n", "link": "http://arxiv.org/abs/2404.07774v1", "date": "2024-04-11", "relevancy": 2.2675, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5869}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5636}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5621}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sketch-Plan-Generalize%3A%20Continual%20Few-Shot%20Learning%20of%20Inductively%0A%20%20Generalizable%20Spatial%20Concepts%20for%20Language-Guided%20Robot%20Manipulation&body=Title%3A%20Sketch-Plan-Generalize%3A%20Continual%20Few-Shot%20Learning%20of%20Inductively%0A%20%20Generalizable%20Spatial%20Concepts%20for%20Language-Guided%20Robot%20Manipulation%0AAuthor%3A%20Namasivayam%20Kalithasan%20and%20Sachit%20Sachdeva%20and%20Himanshu%20Gaurav%20Singh%20and%20Divyanshu%20Aggarwal%20and%20Gurarmaan%20Singh%20Panjeta%20and%20Vishal%20Bindal%20and%20Arnav%20Tuli%20and%20Rohan%20Paul%20and%20Parag%20Singla%0AAbstract%3A%20%20%20Our%20goal%20is%20to%20build%20embodied%20agents%20that%20can%20learn%20inductively%20generalizable%0Aspatial%20concepts%20in%20a%20continual%20manner%2C%20e.g%2C%20constructing%20a%20tower%20of%20a%20given%0Aheight.%20Existing%20work%20suffers%20from%20certain%20limitations%20%28a%29%20%28Liang%20et%20al.%2C%202023%29%0Aand%20their%20multi-modal%20extensions%2C%20rely%20heavily%20on%20prior%20knowledge%20and%20are%20not%0Agrounded%20in%20the%20demonstrations%20%28b%29%20%28Liu%20et%20al.%2C%202023%29%20lack%20the%20ability%20to%0Ageneralize%20due%20to%20their%20purely%20neural%20approach.%20A%20key%20challenge%20is%20to%20achieve%20a%0Afine%20balance%20between%20symbolic%20representations%20which%20have%20the%20capability%20to%0Ageneralize%2C%20and%20neural%20representations%20that%20are%20physically%20grounded.%20In%0Aresponse%2C%20we%20propose%20a%20neuro-symbolic%20approach%20by%20expressing%20inductive%20concepts%0Aas%20symbolic%20compositions%20over%20grounded%20neural%20concepts.%20Our%20key%20insight%20is%20to%0Adecompose%20the%20concept%20learning%20problem%20into%20the%20following%20steps%201%29%20Sketch%3A%0AGetting%20a%20programmatic%20representation%20for%20the%20given%20instruction%202%29%20Plan%3A%0APerform%20Model-Based%20RL%20over%20the%20sequence%20of%20grounded%20neural%20action%20concepts%20to%0Alearn%20a%20grounded%20plan%203%29%20Generalize%3A%20Abstract%20out%20a%20generic%20%28lifted%29%20Python%0Aprogram%20to%20facilitate%20generalizability.%20Continual%20learning%20is%20achieved%20by%0Ainterspersing%20learning%20of%20grounded%20neural%20concepts%20with%20higher%20level%20symbolic%0Aconstructs.%20Our%20experiments%20demonstrate%20that%20our%20approach%20significantly%0Aoutperforms%20existing%20baselines%20in%20terms%20of%20its%20ability%20to%20learn%20novel%20concepts%0Aand%20generalize%20inductively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07774v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-Plan-Generalize%3A%20Continual%20Few-Shot%20Learning%20of%20Inductively%0A%20%20Generalizable%20Spatial%20Concepts%20for%20Language-Guided%20Robot%20Manipulation&entry.906535625=Namasivayam%20Kalithasan%20and%20Sachit%20Sachdeva%20and%20Himanshu%20Gaurav%20Singh%20and%20Divyanshu%20Aggarwal%20and%20Gurarmaan%20Singh%20Panjeta%20and%20Vishal%20Bindal%20and%20Arnav%20Tuli%20and%20Rohan%20Paul%20and%20Parag%20Singla&entry.1292438233=%20%20Our%20goal%20is%20to%20build%20embodied%20agents%20that%20can%20learn%20inductively%20generalizable%0Aspatial%20concepts%20in%20a%20continual%20manner%2C%20e.g%2C%20constructing%20a%20tower%20of%20a%20given%0Aheight.%20Existing%20work%20suffers%20from%20certain%20limitations%20%28a%29%20%28Liang%20et%20al.%2C%202023%29%0Aand%20their%20multi-modal%20extensions%2C%20rely%20heavily%20on%20prior%20knowledge%20and%20are%20not%0Agrounded%20in%20the%20demonstrations%20%28b%29%20%28Liu%20et%20al.%2C%202023%29%20lack%20the%20ability%20to%0Ageneralize%20due%20to%20their%20purely%20neural%20approach.%20A%20key%20challenge%20is%20to%20achieve%20a%0Afine%20balance%20between%20symbolic%20representations%20which%20have%20the%20capability%20to%0Ageneralize%2C%20and%20neural%20representations%20that%20are%20physically%20grounded.%20In%0Aresponse%2C%20we%20propose%20a%20neuro-symbolic%20approach%20by%20expressing%20inductive%20concepts%0Aas%20symbolic%20compositions%20over%20grounded%20neural%20concepts.%20Our%20key%20insight%20is%20to%0Adecompose%20the%20concept%20learning%20problem%20into%20the%20following%20steps%201%29%20Sketch%3A%0AGetting%20a%20programmatic%20representation%20for%20the%20given%20instruction%202%29%20Plan%3A%0APerform%20Model-Based%20RL%20over%20the%20sequence%20of%20grounded%20neural%20action%20concepts%20to%0Alearn%20a%20grounded%20plan%203%29%20Generalize%3A%20Abstract%20out%20a%20generic%20%28lifted%29%20Python%0Aprogram%20to%20facilitate%20generalizability.%20Continual%20learning%20is%20achieved%20by%0Ainterspersing%20learning%20of%20grounded%20neural%20concepts%20with%20higher%20level%20symbolic%0Aconstructs.%20Our%20experiments%20demonstrate%20that%20our%20approach%20significantly%0Aoutperforms%20existing%20baselines%20in%20terms%20of%20its%20ability%20to%20learn%20novel%20concepts%0Aand%20generalize%20inductively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07774v1&entry.124074799=Read"},
{"title": "An Autonomous Vision-Based Algorithm for Interplanetary Navigation", "author": "Eleonora Andreis and Paolo Panicucci and Francesco Topputo", "abstract": "  The surge of deep-space probes makes it unsustainable to navigate them with\nstandard radiometric tracking. Self-driving interplanetary satellites represent\na solution to this problem. In this work, a full vision-based navigation\nalgorithm is built by combining an orbit determination method with an image\nprocessing pipeline suitable for interplanetary transfers of autonomous\nplatforms. To increase the computational efficiency of the algorithm, a\nnon-dimensional extended Kalman filter is selected as state estimator, fed by\nthe positions of the planets extracted from deep-space images. An enhancement\nof the estimation accuracy is performed by applying an optimal strategy to\nselect the best pair of planets to track. Moreover, a novel analytical\nmeasurement model for deep-space navigation is developed providing a\nfirst-order approximation of the light-aberration and light-time effects.\nAlgorithm performance is tested on a high-fidelity, Earth--Mars interplanetary\ntransfer, showing the algorithm applicability for deep-space navigation.\n", "link": "http://arxiv.org/abs/2309.09590v2", "date": "2024-04-11", "relevancy": 2.2602, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5919}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5246}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Autonomous%20Vision-Based%20Algorithm%20for%20Interplanetary%20Navigation&body=Title%3A%20An%20Autonomous%20Vision-Based%20Algorithm%20for%20Interplanetary%20Navigation%0AAuthor%3A%20Eleonora%20Andreis%20and%20Paolo%20Panicucci%20and%20Francesco%20Topputo%0AAbstract%3A%20%20%20The%20surge%20of%20deep-space%20probes%20makes%20it%20unsustainable%20to%20navigate%20them%20with%0Astandard%20radiometric%20tracking.%20Self-driving%20interplanetary%20satellites%20represent%0Aa%20solution%20to%20this%20problem.%20In%20this%20work%2C%20a%20full%20vision-based%20navigation%0Aalgorithm%20is%20built%20by%20combining%20an%20orbit%20determination%20method%20with%20an%20image%0Aprocessing%20pipeline%20suitable%20for%20interplanetary%20transfers%20of%20autonomous%0Aplatforms.%20To%20increase%20the%20computational%20efficiency%20of%20the%20algorithm%2C%20a%0Anon-dimensional%20extended%20Kalman%20filter%20is%20selected%20as%20state%20estimator%2C%20fed%20by%0Athe%20positions%20of%20the%20planets%20extracted%20from%20deep-space%20images.%20An%20enhancement%0Aof%20the%20estimation%20accuracy%20is%20performed%20by%20applying%20an%20optimal%20strategy%20to%0Aselect%20the%20best%20pair%20of%20planets%20to%20track.%20Moreover%2C%20a%20novel%20analytical%0Ameasurement%20model%20for%20deep-space%20navigation%20is%20developed%20providing%20a%0Afirst-order%20approximation%20of%20the%20light-aberration%20and%20light-time%20effects.%0AAlgorithm%20performance%20is%20tested%20on%20a%20high-fidelity%2C%20Earth--Mars%20interplanetary%0Atransfer%2C%20showing%20the%20algorithm%20applicability%20for%20deep-space%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09590v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Autonomous%20Vision-Based%20Algorithm%20for%20Interplanetary%20Navigation&entry.906535625=Eleonora%20Andreis%20and%20Paolo%20Panicucci%20and%20Francesco%20Topputo&entry.1292438233=%20%20The%20surge%20of%20deep-space%20probes%20makes%20it%20unsustainable%20to%20navigate%20them%20with%0Astandard%20radiometric%20tracking.%20Self-driving%20interplanetary%20satellites%20represent%0Aa%20solution%20to%20this%20problem.%20In%20this%20work%2C%20a%20full%20vision-based%20navigation%0Aalgorithm%20is%20built%20by%20combining%20an%20orbit%20determination%20method%20with%20an%20image%0Aprocessing%20pipeline%20suitable%20for%20interplanetary%20transfers%20of%20autonomous%0Aplatforms.%20To%20increase%20the%20computational%20efficiency%20of%20the%20algorithm%2C%20a%0Anon-dimensional%20extended%20Kalman%20filter%20is%20selected%20as%20state%20estimator%2C%20fed%20by%0Athe%20positions%20of%20the%20planets%20extracted%20from%20deep-space%20images.%20An%20enhancement%0Aof%20the%20estimation%20accuracy%20is%20performed%20by%20applying%20an%20optimal%20strategy%20to%0Aselect%20the%20best%20pair%20of%20planets%20to%20track.%20Moreover%2C%20a%20novel%20analytical%0Ameasurement%20model%20for%20deep-space%20navigation%20is%20developed%20providing%20a%0Afirst-order%20approximation%20of%20the%20light-aberration%20and%20light-time%20effects.%0AAlgorithm%20performance%20is%20tested%20on%20a%20high-fidelity%2C%20Earth--Mars%20interplanetary%0Atransfer%2C%20showing%20the%20algorithm%20applicability%20for%20deep-space%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09590v2&entry.124074799=Read"},
{"title": "Depth Estimation using Weighted-loss and Transfer Learning", "author": "Muhammad Adeel Hafeez and Michael G. Madden and Ganesh Sistu and Ihsan Ullah", "abstract": "  Depth estimation from 2D images is a common computer vision task that has\napplications in many fields including autonomous vehicles, scene understanding\nand robotics. The accuracy of a supervised depth estimation method mainly\nrelies on the chosen loss function, the model architecture, quality of data and\nperformance metrics. In this study, we propose a simplified and adaptable\napproach to improve depth estimation accuracy using transfer learning and an\noptimized loss function. The optimized loss function is a combination of\nweighted losses to which enhance robustness and generalization: Mean Absolute\nError (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid\nsearch and a random search method to find optimized weights for the losses,\nwhich leads to an improved model. We explore multiple encoder-decoder-based\nmodels including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for\nthe supervised depth estimation model on NYU Depth Dataset v2. We observe that\nthe EfficientNet model, pre-trained on ImageNet for classification when used as\nan encoder, with a simple upsampling decoder, gives the best results in terms\nof RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a\nqualitative analysis which illustrates that our model produces depth maps that\nclosely resemble ground truth, even in cases where the ground truth is flawed.\nThe results indicate significant improvements in accuracy and robustness, with\nEfficientNet being the most successful architecture.\n", "link": "http://arxiv.org/abs/2404.07686v1", "date": "2024-04-11", "relevancy": 2.2527, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5839}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5651}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5529}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Depth%20Estimation%20using%20Weighted-loss%20and%20Transfer%20Learning&body=Title%3A%20Depth%20Estimation%20using%20Weighted-loss%20and%20Transfer%20Learning%0AAuthor%3A%20Muhammad%20Adeel%20Hafeez%20and%20Michael%20G.%20Madden%20and%20Ganesh%20Sistu%20and%20Ihsan%20Ullah%0AAbstract%3A%20%20%20Depth%20estimation%20from%202D%20images%20is%20a%20common%20computer%20vision%20task%20that%20has%0Aapplications%20in%20many%20fields%20including%20autonomous%20vehicles%2C%20scene%20understanding%0Aand%20robotics.%20The%20accuracy%20of%20a%20supervised%20depth%20estimation%20method%20mainly%0Arelies%20on%20the%20chosen%20loss%20function%2C%20the%20model%20architecture%2C%20quality%20of%20data%20and%0Aperformance%20metrics.%20In%20this%20study%2C%20we%20propose%20a%20simplified%20and%20adaptable%0Aapproach%20to%20improve%20depth%20estimation%20accuracy%20using%20transfer%20learning%20and%20an%0Aoptimized%20loss%20function.%20The%20optimized%20loss%20function%20is%20a%20combination%20of%0Aweighted%20losses%20to%20which%20enhance%20robustness%20and%20generalization%3A%20Mean%20Absolute%0AError%20%28MAE%29%2C%20Edge%20Loss%20and%20Structural%20Similarity%20Index%20%28SSIM%29.%20We%20use%20a%20grid%0Asearch%20and%20a%20random%20search%20method%20to%20find%20optimized%20weights%20for%20the%20losses%2C%0Awhich%20leads%20to%20an%20improved%20model.%20We%20explore%20multiple%20encoder-decoder-based%0Amodels%20including%20DenseNet121%2C%20DenseNet169%2C%20DenseNet201%2C%20and%20EfficientNet%20for%0Athe%20supervised%20depth%20estimation%20model%20on%20NYU%20Depth%20Dataset%20v2.%20We%20observe%20that%0Athe%20EfficientNet%20model%2C%20pre-trained%20on%20ImageNet%20for%20classification%20when%20used%20as%0Aan%20encoder%2C%20with%20a%20simple%20upsampling%20decoder%2C%20gives%20the%20best%20results%20in%20terms%0Aof%20RSME%2C%20REL%20and%20log10%3A%200.386%2C%200.113%20and%200.049%2C%20respectively.%20We%20also%20perform%20a%0Aqualitative%20analysis%20which%20illustrates%20that%20our%20model%20produces%20depth%20maps%20that%0Aclosely%20resemble%20ground%20truth%2C%20even%20in%20cases%20where%20the%20ground%20truth%20is%20flawed.%0AThe%20results%20indicate%20significant%20improvements%20in%20accuracy%20and%20robustness%2C%20with%0AEfficientNet%20being%20the%20most%20successful%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07686v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Estimation%20using%20Weighted-loss%20and%20Transfer%20Learning&entry.906535625=Muhammad%20Adeel%20Hafeez%20and%20Michael%20G.%20Madden%20and%20Ganesh%20Sistu%20and%20Ihsan%20Ullah&entry.1292438233=%20%20Depth%20estimation%20from%202D%20images%20is%20a%20common%20computer%20vision%20task%20that%20has%0Aapplications%20in%20many%20fields%20including%20autonomous%20vehicles%2C%20scene%20understanding%0Aand%20robotics.%20The%20accuracy%20of%20a%20supervised%20depth%20estimation%20method%20mainly%0Arelies%20on%20the%20chosen%20loss%20function%2C%20the%20model%20architecture%2C%20quality%20of%20data%20and%0Aperformance%20metrics.%20In%20this%20study%2C%20we%20propose%20a%20simplified%20and%20adaptable%0Aapproach%20to%20improve%20depth%20estimation%20accuracy%20using%20transfer%20learning%20and%20an%0Aoptimized%20loss%20function.%20The%20optimized%20loss%20function%20is%20a%20combination%20of%0Aweighted%20losses%20to%20which%20enhance%20robustness%20and%20generalization%3A%20Mean%20Absolute%0AError%20%28MAE%29%2C%20Edge%20Loss%20and%20Structural%20Similarity%20Index%20%28SSIM%29.%20We%20use%20a%20grid%0Asearch%20and%20a%20random%20search%20method%20to%20find%20optimized%20weights%20for%20the%20losses%2C%0Awhich%20leads%20to%20an%20improved%20model.%20We%20explore%20multiple%20encoder-decoder-based%0Amodels%20including%20DenseNet121%2C%20DenseNet169%2C%20DenseNet201%2C%20and%20EfficientNet%20for%0Athe%20supervised%20depth%20estimation%20model%20on%20NYU%20Depth%20Dataset%20v2.%20We%20observe%20that%0Athe%20EfficientNet%20model%2C%20pre-trained%20on%20ImageNet%20for%20classification%20when%20used%20as%0Aan%20encoder%2C%20with%20a%20simple%20upsampling%20decoder%2C%20gives%20the%20best%20results%20in%20terms%0Aof%20RSME%2C%20REL%20and%20log10%3A%200.386%2C%200.113%20and%200.049%2C%20respectively.%20We%20also%20perform%20a%0Aqualitative%20analysis%20which%20illustrates%20that%20our%20model%20produces%20depth%20maps%20that%0Aclosely%20resemble%20ground%20truth%2C%20even%20in%20cases%20where%20the%20ground%20truth%20is%20flawed.%0AThe%20results%20indicate%20significant%20improvements%20in%20accuracy%20and%20robustness%2C%20with%0AEfficientNet%20being%20the%20most%20successful%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07686v1&entry.124074799=Read"},
{"title": "Exploring Efficient Asymmetric Blind-Spots for Self-Supervised Denoising\n  in Real-World Scenarios", "author": "Shiyan Chen and Jiyuan Zhang and Zhaofei Yu and Tiejun Huang", "abstract": "  Self-supervised denoising has attracted widespread attention due to its\nability to train without clean images. However, noise in real-world scenarios\nis often spatially correlated, which causes many self-supervised algorithms\nthat assume pixel-wise independent noise to perform poorly. Recent works have\nattempted to break noise correlation with downsampling or neighborhood masking.\nHowever, denoising on downsampled subgraphs can lead to aliasing effects and\nloss of details due to a lower sampling rate. Furthermore, the neighborhood\nmasking methods either come with high computational complexity or do not\nconsider local spatial preservation during inference. Through the analysis of\nexisting methods, we point out that the key to obtaining high-quality and\ntexture-rich results in real-world self-supervised denoising tasks is to train\nat the original input resolution structure and use asymmetric operations during\ntraining and inference. Based on this, we propose Asymmetric Tunable Blind-Spot\nNetwork (AT-BSN), where the blind-spot size can be freely adjusted, thus better\nbalancing noise correlation suppression and image local spatial destruction\nduring training and inference. In addition, we regard the pre-trained AT-BSN as\na meta-teacher network capable of generating various teacher networks by\nsampling different blind-spots. We propose a blind-spot based multi-teacher\ndistillation strategy to distill a lightweight network, significantly improving\nperformance. Experimental results on multiple datasets prove that our method\nachieves state-of-the-art, and is superior to other self-supervised algorithms\nin terms of computational overhead and visual effects.\n", "link": "http://arxiv.org/abs/2303.16783v2", "date": "2024-04-11", "relevancy": 2.2454, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5805}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5502}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5466}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Efficient%20Asymmetric%20Blind-Spots%20for%20Self-Supervised%20Denoising%0A%20%20in%20Real-World%20Scenarios&body=Title%3A%20Exploring%20Efficient%20Asymmetric%20Blind-Spots%20for%20Self-Supervised%20Denoising%0A%20%20in%20Real-World%20Scenarios%0AAuthor%3A%20Shiyan%20Chen%20and%20Jiyuan%20Zhang%20and%20Zhaofei%20Yu%20and%20Tiejun%20Huang%0AAbstract%3A%20%20%20Self-supervised%20denoising%20has%20attracted%20widespread%20attention%20due%20to%20its%0Aability%20to%20train%20without%20clean%20images.%20However%2C%20noise%20in%20real-world%20scenarios%0Ais%20often%20spatially%20correlated%2C%20which%20causes%20many%20self-supervised%20algorithms%0Athat%20assume%20pixel-wise%20independent%20noise%20to%20perform%20poorly.%20Recent%20works%20have%0Aattempted%20to%20break%20noise%20correlation%20with%20downsampling%20or%20neighborhood%20masking.%0AHowever%2C%20denoising%20on%20downsampled%20subgraphs%20can%20lead%20to%20aliasing%20effects%20and%0Aloss%20of%20details%20due%20to%20a%20lower%20sampling%20rate.%20Furthermore%2C%20the%20neighborhood%0Amasking%20methods%20either%20come%20with%20high%20computational%20complexity%20or%20do%20not%0Aconsider%20local%20spatial%20preservation%20during%20inference.%20Through%20the%20analysis%20of%0Aexisting%20methods%2C%20we%20point%20out%20that%20the%20key%20to%20obtaining%20high-quality%20and%0Atexture-rich%20results%20in%20real-world%20self-supervised%20denoising%20tasks%20is%20to%20train%0Aat%20the%20original%20input%20resolution%20structure%20and%20use%20asymmetric%20operations%20during%0Atraining%20and%20inference.%20Based%20on%20this%2C%20we%20propose%20Asymmetric%20Tunable%20Blind-Spot%0ANetwork%20%28AT-BSN%29%2C%20where%20the%20blind-spot%20size%20can%20be%20freely%20adjusted%2C%20thus%20better%0Abalancing%20noise%20correlation%20suppression%20and%20image%20local%20spatial%20destruction%0Aduring%20training%20and%20inference.%20In%20addition%2C%20we%20regard%20the%20pre-trained%20AT-BSN%20as%0Aa%20meta-teacher%20network%20capable%20of%20generating%20various%20teacher%20networks%20by%0Asampling%20different%20blind-spots.%20We%20propose%20a%20blind-spot%20based%20multi-teacher%0Adistillation%20strategy%20to%20distill%20a%20lightweight%20network%2C%20significantly%20improving%0Aperformance.%20Experimental%20results%20on%20multiple%20datasets%20prove%20that%20our%20method%0Aachieves%20state-of-the-art%2C%20and%20is%20superior%20to%20other%20self-supervised%20algorithms%0Ain%20terms%20of%20computational%20overhead%20and%20visual%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.16783v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Efficient%20Asymmetric%20Blind-Spots%20for%20Self-Supervised%20Denoising%0A%20%20in%20Real-World%20Scenarios&entry.906535625=Shiyan%20Chen%20and%20Jiyuan%20Zhang%20and%20Zhaofei%20Yu%20and%20Tiejun%20Huang&entry.1292438233=%20%20Self-supervised%20denoising%20has%20attracted%20widespread%20attention%20due%20to%20its%0Aability%20to%20train%20without%20clean%20images.%20However%2C%20noise%20in%20real-world%20scenarios%0Ais%20often%20spatially%20correlated%2C%20which%20causes%20many%20self-supervised%20algorithms%0Athat%20assume%20pixel-wise%20independent%20noise%20to%20perform%20poorly.%20Recent%20works%20have%0Aattempted%20to%20break%20noise%20correlation%20with%20downsampling%20or%20neighborhood%20masking.%0AHowever%2C%20denoising%20on%20downsampled%20subgraphs%20can%20lead%20to%20aliasing%20effects%20and%0Aloss%20of%20details%20due%20to%20a%20lower%20sampling%20rate.%20Furthermore%2C%20the%20neighborhood%0Amasking%20methods%20either%20come%20with%20high%20computational%20complexity%20or%20do%20not%0Aconsider%20local%20spatial%20preservation%20during%20inference.%20Through%20the%20analysis%20of%0Aexisting%20methods%2C%20we%20point%20out%20that%20the%20key%20to%20obtaining%20high-quality%20and%0Atexture-rich%20results%20in%20real-world%20self-supervised%20denoising%20tasks%20is%20to%20train%0Aat%20the%20original%20input%20resolution%20structure%20and%20use%20asymmetric%20operations%20during%0Atraining%20and%20inference.%20Based%20on%20this%2C%20we%20propose%20Asymmetric%20Tunable%20Blind-Spot%0ANetwork%20%28AT-BSN%29%2C%20where%20the%20blind-spot%20size%20can%20be%20freely%20adjusted%2C%20thus%20better%0Abalancing%20noise%20correlation%20suppression%20and%20image%20local%20spatial%20destruction%0Aduring%20training%20and%20inference.%20In%20addition%2C%20we%20regard%20the%20pre-trained%20AT-BSN%20as%0Aa%20meta-teacher%20network%20capable%20of%20generating%20various%20teacher%20networks%20by%0Asampling%20different%20blind-spots.%20We%20propose%20a%20blind-spot%20based%20multi-teacher%0Adistillation%20strategy%20to%20distill%20a%20lightweight%20network%2C%20significantly%20improving%0Aperformance.%20Experimental%20results%20on%20multiple%20datasets%20prove%20that%20our%20method%0Aachieves%20state-of-the-art%2C%20and%20is%20superior%20to%20other%20self-supervised%20algorithms%0Ain%20terms%20of%20computational%20overhead%20and%20visual%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.16783v2&entry.124074799=Read"},
{"title": "Supervised Fine-tuning in turn Improves Visual Foundation Models", "author": "Xiaohu Jiang and Yixiao Ge and Yuying Ge and Dachuan Shi and Chun Yuan and Ying Shan", "abstract": "  Image-text training like CLIP has dominated the pretraining of vision\nfoundation models in recent years. Subsequent efforts have been made to\nintroduce region-level visual learning into CLIP's pretraining but face\nscalability challenges due to the lack of large-scale region-level datasets.\nDrawing inspiration from supervised fine-tuning (SFT) in natural language\nprocessing such as instruction tuning, we explore the potential of fine-grained\nSFT in enhancing the generation of vision foundation models after their\npretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash\nthe fine-grained knowledge of vision foundation models. In ViSFT, the vision\nfoundation model is enhanced by performing visual joint learning on some\nin-domain tasks and then tested on out-of-domain benchmarks. With updating\nusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over\n4.4B parameters shows improvements across various out-of-domain benchmarks\nincluding vision and vision-linguistic scenarios.\n", "link": "http://arxiv.org/abs/2401.10222v2", "date": "2024-04-11", "relevancy": 2.2399, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5871}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Supervised%20Fine-tuning%20in%20turn%20Improves%20Visual%20Foundation%20Models&body=Title%3A%20Supervised%20Fine-tuning%20in%20turn%20Improves%20Visual%20Foundation%20Models%0AAuthor%3A%20Xiaohu%20Jiang%20and%20Yixiao%20Ge%20and%20Yuying%20Ge%20and%20Dachuan%20Shi%20and%20Chun%20Yuan%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Image-text%20training%20like%20CLIP%20has%20dominated%20the%20pretraining%20of%20vision%0Afoundation%20models%20in%20recent%20years.%20Subsequent%20efforts%20have%20been%20made%20to%0Aintroduce%20region-level%20visual%20learning%20into%20CLIP%27s%20pretraining%20but%20face%0Ascalability%20challenges%20due%20to%20the%20lack%20of%20large-scale%20region-level%20datasets.%0ADrawing%20inspiration%20from%20supervised%20fine-tuning%20%28SFT%29%20in%20natural%20language%0Aprocessing%20such%20as%20instruction%20tuning%2C%20we%20explore%20the%20potential%20of%20fine-grained%0ASFT%20in%20enhancing%20the%20generation%20of%20vision%20foundation%20models%20after%20their%0Apretraining.%20Thus%20a%20two-stage%20method%20ViSFT%20%28Vision%20SFT%29%20is%20proposed%20to%20unleash%0Athe%20fine-grained%20knowledge%20of%20vision%20foundation%20models.%20In%20ViSFT%2C%20the%20vision%0Afoundation%20model%20is%20enhanced%20by%20performing%20visual%20joint%20learning%20on%20some%0Ain-domain%20tasks%20and%20then%20tested%20on%20out-of-domain%20benchmarks.%20With%20updating%0Ausing%20ViSFT%20on%208%20V100%20GPUs%20in%20less%20than%202%20days%2C%20a%20vision%20transformer%20with%20over%0A4.4B%20parameters%20shows%20improvements%20across%20various%20out-of-domain%20benchmarks%0Aincluding%20vision%20and%20vision-linguistic%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10222v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Fine-tuning%20in%20turn%20Improves%20Visual%20Foundation%20Models&entry.906535625=Xiaohu%20Jiang%20and%20Yixiao%20Ge%20and%20Yuying%20Ge%20and%20Dachuan%20Shi%20and%20Chun%20Yuan%20and%20Ying%20Shan&entry.1292438233=%20%20Image-text%20training%20like%20CLIP%20has%20dominated%20the%20pretraining%20of%20vision%0Afoundation%20models%20in%20recent%20years.%20Subsequent%20efforts%20have%20been%20made%20to%0Aintroduce%20region-level%20visual%20learning%20into%20CLIP%27s%20pretraining%20but%20face%0Ascalability%20challenges%20due%20to%20the%20lack%20of%20large-scale%20region-level%20datasets.%0ADrawing%20inspiration%20from%20supervised%20fine-tuning%20%28SFT%29%20in%20natural%20language%0Aprocessing%20such%20as%20instruction%20tuning%2C%20we%20explore%20the%20potential%20of%20fine-grained%0ASFT%20in%20enhancing%20the%20generation%20of%20vision%20foundation%20models%20after%20their%0Apretraining.%20Thus%20a%20two-stage%20method%20ViSFT%20%28Vision%20SFT%29%20is%20proposed%20to%20unleash%0Athe%20fine-grained%20knowledge%20of%20vision%20foundation%20models.%20In%20ViSFT%2C%20the%20vision%0Afoundation%20model%20is%20enhanced%20by%20performing%20visual%20joint%20learning%20on%20some%0Ain-domain%20tasks%20and%20then%20tested%20on%20out-of-domain%20benchmarks.%20With%20updating%0Ausing%20ViSFT%20on%208%20V100%20GPUs%20in%20less%20than%202%20days%2C%20a%20vision%20transformer%20with%20over%0A4.4B%20parameters%20shows%20improvements%20across%20various%20out-of-domain%20benchmarks%0Aincluding%20vision%20and%20vision-linguistic%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10222v2&entry.124074799=Read"},
{"title": "How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey", "author": "Fabio Tosi and Youmin Zhang and Ziren Gong and Erik Sandstr\u00f6m and Stefano Mattoccia and Martin R. Oswald and Matteo Poggi", "abstract": "  Over the past two decades, research in the field of Simultaneous Localization\nand Mapping (SLAM) has undergone a significant evolution, highlighting its\ncritical role in enabling autonomous exploration of unknown environments. This\nevolution ranges from hand-crafted methods, through the era of deep learning,\nto more recent developments focused on Neural Radiance Fields (NeRFs) and 3D\nGaussian Splatting (3DGS) representations. Recognizing the growing body of\nresearch and the absence of a comprehensive survey on the topic, this paper\naims to provide the first comprehensive overview of SLAM progress through the\nlens of the latest advancements in radiance fields. It sheds light on the\nbackground, evolutionary path, inherent strengths and limitations, and serves\nas a fundamental reference to highlight the dynamic progress and specific\nchallenges.\n", "link": "http://arxiv.org/abs/2402.13255v2", "date": "2024-04-11", "relevancy": 2.2385, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6095}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5293}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5108}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20NeRFs%20and%203D%20Gaussian%20Splatting%20are%20Reshaping%20SLAM%3A%20a%20Survey&body=Title%3A%20How%20NeRFs%20and%203D%20Gaussian%20Splatting%20are%20Reshaping%20SLAM%3A%20a%20Survey%0AAuthor%3A%20Fabio%20Tosi%20and%20Youmin%20Zhang%20and%20Ziren%20Gong%20and%20Erik%20Sandstr%C3%B6m%20and%20Stefano%20Mattoccia%20and%20Martin%20R.%20Oswald%20and%20Matteo%20Poggi%0AAbstract%3A%20%20%20Over%20the%20past%20two%20decades%2C%20research%20in%20the%20field%20of%20Simultaneous%20Localization%0Aand%20Mapping%20%28SLAM%29%20has%20undergone%20a%20significant%20evolution%2C%20highlighting%20its%0Acritical%20role%20in%20enabling%20autonomous%20exploration%20of%20unknown%20environments.%20This%0Aevolution%20ranges%20from%20hand-crafted%20methods%2C%20through%20the%20era%20of%20deep%20learning%2C%0Ato%20more%20recent%20developments%20focused%20on%20Neural%20Radiance%20Fields%20%28NeRFs%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20representations.%20Recognizing%20the%20growing%20body%20of%0Aresearch%20and%20the%20absence%20of%20a%20comprehensive%20survey%20on%20the%20topic%2C%20this%20paper%0Aaims%20to%20provide%20the%20first%20comprehensive%20overview%20of%20SLAM%20progress%20through%20the%0Alens%20of%20the%20latest%20advancements%20in%20radiance%20fields.%20It%20sheds%20light%20on%20the%0Abackground%2C%20evolutionary%20path%2C%20inherent%20strengths%20and%20limitations%2C%20and%20serves%0Aas%20a%20fundamental%20reference%20to%20highlight%20the%20dynamic%20progress%20and%20specific%0Achallenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13255v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20NeRFs%20and%203D%20Gaussian%20Splatting%20are%20Reshaping%20SLAM%3A%20a%20Survey&entry.906535625=Fabio%20Tosi%20and%20Youmin%20Zhang%20and%20Ziren%20Gong%20and%20Erik%20Sandstr%C3%B6m%20and%20Stefano%20Mattoccia%20and%20Martin%20R.%20Oswald%20and%20Matteo%20Poggi&entry.1292438233=%20%20Over%20the%20past%20two%20decades%2C%20research%20in%20the%20field%20of%20Simultaneous%20Localization%0Aand%20Mapping%20%28SLAM%29%20has%20undergone%20a%20significant%20evolution%2C%20highlighting%20its%0Acritical%20role%20in%20enabling%20autonomous%20exploration%20of%20unknown%20environments.%20This%0Aevolution%20ranges%20from%20hand-crafted%20methods%2C%20through%20the%20era%20of%20deep%20learning%2C%0Ato%20more%20recent%20developments%20focused%20on%20Neural%20Radiance%20Fields%20%28NeRFs%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20representations.%20Recognizing%20the%20growing%20body%20of%0Aresearch%20and%20the%20absence%20of%20a%20comprehensive%20survey%20on%20the%20topic%2C%20this%20paper%0Aaims%20to%20provide%20the%20first%20comprehensive%20overview%20of%20SLAM%20progress%20through%20the%0Alens%20of%20the%20latest%20advancements%20in%20radiance%20fields.%20It%20sheds%20light%20on%20the%0Abackground%2C%20evolutionary%20path%2C%20inherent%20strengths%20and%20limitations%2C%20and%20serves%0Aas%20a%20fundamental%20reference%20to%20highlight%20the%20dynamic%20progress%20and%20specific%0Achallenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13255v2&entry.124074799=Read"},
{"title": "Reflectance Estimation for Proximity Sensing by Vision-Language Models:\n  Utilizing Distributional Semantics for Low-Level Cognition in Robotics", "author": "Masashi Osada and Gustavo A. Garcia Ricardez and Yosuke Suzuki and Tadahiro Taniguchi", "abstract": "  Large language models (LLMs) and vision-language models (VLMs) have been\nincreasingly used in robotics for high-level cognition, but their use for\nlow-level cognition, such as interpreting sensor information, remains\nunderexplored. In robotic grasping, estimating the reflectance of objects is\ncrucial for successful grasping, as it significantly impacts the distance\nmeasured by proximity sensors. We investigate whether LLMs can estimate\nreflectance from object names alone, leveraging the embedded human knowledge in\ndistributional semantics, and if the latent structure of language in VLMs\npositively affects image-based reflectance estimation. In this paper, we verify\nthat 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance\nusing only text as input; and 2) VLMs such as CLIP can increase their\ngeneralization capabilities in reflectance estimation from images. Our\nexperiments show that GPT-4 can estimate an object's reflectance using only\ntext input with a mean error of 14.7%, lower than the image-only ResNet.\nMoreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained\na competitive 19.9% compared to ResNet's 17.8%. These results suggest that the\ndistributional semantics in LLMs and VLMs increases their generalization\ncapabilities, and the knowledge acquired by VLMs benefits from the latent\nstructure of language.\n", "link": "http://arxiv.org/abs/2404.07717v1", "date": "2024-04-11", "relevancy": 2.2378, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6208}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5513}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5431}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reflectance%20Estimation%20for%20Proximity%20Sensing%20by%20Vision-Language%20Models%3A%0A%20%20Utilizing%20Distributional%20Semantics%20for%20Low-Level%20Cognition%20in%20Robotics&body=Title%3A%20Reflectance%20Estimation%20for%20Proximity%20Sensing%20by%20Vision-Language%20Models%3A%0A%20%20Utilizing%20Distributional%20Semantics%20for%20Low-Level%20Cognition%20in%20Robotics%0AAuthor%3A%20Masashi%20Osada%20and%20Gustavo%20A.%20Garcia%20Ricardez%20and%20Yosuke%20Suzuki%20and%20Tadahiro%20Taniguchi%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20and%20vision-language%20models%20%28VLMs%29%20have%20been%0Aincreasingly%20used%20in%20robotics%20for%20high-level%20cognition%2C%20but%20their%20use%20for%0Alow-level%20cognition%2C%20such%20as%20interpreting%20sensor%20information%2C%20remains%0Aunderexplored.%20In%20robotic%20grasping%2C%20estimating%20the%20reflectance%20of%20objects%20is%0Acrucial%20for%20successful%20grasping%2C%20as%20it%20significantly%20impacts%20the%20distance%0Ameasured%20by%20proximity%20sensors.%20We%20investigate%20whether%20LLMs%20can%20estimate%0Areflectance%20from%20object%20names%20alone%2C%20leveraging%20the%20embedded%20human%20knowledge%20in%0Adistributional%20semantics%2C%20and%20if%20the%20latent%20structure%20of%20language%20in%20VLMs%0Apositively%20affects%20image-based%20reflectance%20estimation.%20In%20this%20paper%2C%20we%20verify%0Athat%201%29%20LLMs%20such%20as%20GPT-3.5%20and%20GPT-4%20can%20estimate%20an%20object%27s%20reflectance%0Ausing%20only%20text%20as%20input%3B%20and%202%29%20VLMs%20such%20as%20CLIP%20can%20increase%20their%0Ageneralization%20capabilities%20in%20reflectance%20estimation%20from%20images.%20Our%0Aexperiments%20show%20that%20GPT-4%20can%20estimate%20an%20object%27s%20reflectance%20using%20only%0Atext%20input%20with%20a%20mean%20error%20of%2014.7%25%2C%20lower%20than%20the%20image-only%20ResNet.%0AMoreover%2C%20CLIP%20achieved%20the%20lowest%20mean%20error%20of%2011.8%25%2C%20while%20GPT-3.5%20obtained%0Aa%20competitive%2019.9%25%20compared%20to%20ResNet%27s%2017.8%25.%20These%20results%20suggest%20that%20the%0Adistributional%20semantics%20in%20LLMs%20and%20VLMs%20increases%20their%20generalization%0Acapabilities%2C%20and%20the%20knowledge%20acquired%20by%20VLMs%20benefits%20from%20the%20latent%0Astructure%20of%20language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07717v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflectance%20Estimation%20for%20Proximity%20Sensing%20by%20Vision-Language%20Models%3A%0A%20%20Utilizing%20Distributional%20Semantics%20for%20Low-Level%20Cognition%20in%20Robotics&entry.906535625=Masashi%20Osada%20and%20Gustavo%20A.%20Garcia%20Ricardez%20and%20Yosuke%20Suzuki%20and%20Tadahiro%20Taniguchi&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20and%20vision-language%20models%20%28VLMs%29%20have%20been%0Aincreasingly%20used%20in%20robotics%20for%20high-level%20cognition%2C%20but%20their%20use%20for%0Alow-level%20cognition%2C%20such%20as%20interpreting%20sensor%20information%2C%20remains%0Aunderexplored.%20In%20robotic%20grasping%2C%20estimating%20the%20reflectance%20of%20objects%20is%0Acrucial%20for%20successful%20grasping%2C%20as%20it%20significantly%20impacts%20the%20distance%0Ameasured%20by%20proximity%20sensors.%20We%20investigate%20whether%20LLMs%20can%20estimate%0Areflectance%20from%20object%20names%20alone%2C%20leveraging%20the%20embedded%20human%20knowledge%20in%0Adistributional%20semantics%2C%20and%20if%20the%20latent%20structure%20of%20language%20in%20VLMs%0Apositively%20affects%20image-based%20reflectance%20estimation.%20In%20this%20paper%2C%20we%20verify%0Athat%201%29%20LLMs%20such%20as%20GPT-3.5%20and%20GPT-4%20can%20estimate%20an%20object%27s%20reflectance%0Ausing%20only%20text%20as%20input%3B%20and%202%29%20VLMs%20such%20as%20CLIP%20can%20increase%20their%0Ageneralization%20capabilities%20in%20reflectance%20estimation%20from%20images.%20Our%0Aexperiments%20show%20that%20GPT-4%20can%20estimate%20an%20object%27s%20reflectance%20using%20only%0Atext%20input%20with%20a%20mean%20error%20of%2014.7%25%2C%20lower%20than%20the%20image-only%20ResNet.%0AMoreover%2C%20CLIP%20achieved%20the%20lowest%20mean%20error%20of%2011.8%25%2C%20while%20GPT-3.5%20obtained%0Aa%20competitive%2019.9%25%20compared%20to%20ResNet%27s%2017.8%25.%20These%20results%20suggest%20that%20the%0Adistributional%20semantics%20in%20LLMs%20and%20VLMs%20increases%20their%20generalization%0Acapabilities%2C%20and%20the%20knowledge%20acquired%20by%20VLMs%20benefits%20from%20the%20latent%0Astructure%20of%20language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07717v1&entry.124074799=Read"},
{"title": "Mitigating Vulnerable Road Users Occlusion Risk Via Collective\n  Perception: An Empirical Analysis", "author": "Vincent Albert Wolff and Edmir Xhoxhi", "abstract": "  Recent reports from the World Health Organization highlight that Vulnerable\nRoad Users (VRUs) have been involved in over half of the road fatalities in\nrecent years, with occlusion risk - a scenario where VRUs are hidden from\ndrivers' view by obstacles like parked vehicles - being a critical contributing\nfactor. To address this, we present a novel algorithm that quantifies occlusion\nrisk based on the dynamics of both vehicles and VRUs. This algorithm has\nundergone testing and evaluation using a real-world dataset from German\nintersections. Additionally, we introduce the concept of Maximum Tracking Loss\n(MTL), which measures the longest consecutive duration a VRU remains untracked\nby any vehicle in a given scenario. Our study extends to examining the role of\nthe Collective Perception Service (CPS) in VRU safety. CPS enhances safety by\nenabling vehicles to share sensor information, thereby potentially reducing\nocclusion risks. Our analysis reveals that a 25% market penetration of\nCPS-equipped vehicles can substantially diminish occlusion risks and\nsignificantly curtail MTL. These findings demonstrate how various scenarios\npose different levels of risk to VRUs and how the deployment of Collective\nPerception can markedly improve their safety. Furthermore, they underline the\nefficacy of our proposed metrics to capture occlusion risk as a safety factor.\n", "link": "http://arxiv.org/abs/2404.07753v1", "date": "2024-04-11", "relevancy": 2.2149, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5449}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5279}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Vulnerable%20Road%20Users%20Occlusion%20Risk%20Via%20Collective%0A%20%20Perception%3A%20An%20Empirical%20Analysis&body=Title%3A%20Mitigating%20Vulnerable%20Road%20Users%20Occlusion%20Risk%20Via%20Collective%0A%20%20Perception%3A%20An%20Empirical%20Analysis%0AAuthor%3A%20Vincent%20Albert%20Wolff%20and%20Edmir%20Xhoxhi%0AAbstract%3A%20%20%20Recent%20reports%20from%20the%20World%20Health%20Organization%20highlight%20that%20Vulnerable%0ARoad%20Users%20%28VRUs%29%20have%20been%20involved%20in%20over%20half%20of%20the%20road%20fatalities%20in%0Arecent%20years%2C%20with%20occlusion%20risk%20-%20a%20scenario%20where%20VRUs%20are%20hidden%20from%0Adrivers%27%20view%20by%20obstacles%20like%20parked%20vehicles%20-%20being%20a%20critical%20contributing%0Afactor.%20To%20address%20this%2C%20we%20present%20a%20novel%20algorithm%20that%20quantifies%20occlusion%0Arisk%20based%20on%20the%20dynamics%20of%20both%20vehicles%20and%20VRUs.%20This%20algorithm%20has%0Aundergone%20testing%20and%20evaluation%20using%20a%20real-world%20dataset%20from%20German%0Aintersections.%20Additionally%2C%20we%20introduce%20the%20concept%20of%20Maximum%20Tracking%20Loss%0A%28MTL%29%2C%20which%20measures%20the%20longest%20consecutive%20duration%20a%20VRU%20remains%20untracked%0Aby%20any%20vehicle%20in%20a%20given%20scenario.%20Our%20study%20extends%20to%20examining%20the%20role%20of%0Athe%20Collective%20Perception%20Service%20%28CPS%29%20in%20VRU%20safety.%20CPS%20enhances%20safety%20by%0Aenabling%20vehicles%20to%20share%20sensor%20information%2C%20thereby%20potentially%20reducing%0Aocclusion%20risks.%20Our%20analysis%20reveals%20that%20a%2025%25%20market%20penetration%20of%0ACPS-equipped%20vehicles%20can%20substantially%20diminish%20occlusion%20risks%20and%0Asignificantly%20curtail%20MTL.%20These%20findings%20demonstrate%20how%20various%20scenarios%0Apose%20different%20levels%20of%20risk%20to%20VRUs%20and%20how%20the%20deployment%20of%20Collective%0APerception%20can%20markedly%20improve%20their%20safety.%20Furthermore%2C%20they%20underline%20the%0Aefficacy%20of%20our%20proposed%20metrics%20to%20capture%20occlusion%20risk%20as%20a%20safety%20factor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07753v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Vulnerable%20Road%20Users%20Occlusion%20Risk%20Via%20Collective%0A%20%20Perception%3A%20An%20Empirical%20Analysis&entry.906535625=Vincent%20Albert%20Wolff%20and%20Edmir%20Xhoxhi&entry.1292438233=%20%20Recent%20reports%20from%20the%20World%20Health%20Organization%20highlight%20that%20Vulnerable%0ARoad%20Users%20%28VRUs%29%20have%20been%20involved%20in%20over%20half%20of%20the%20road%20fatalities%20in%0Arecent%20years%2C%20with%20occlusion%20risk%20-%20a%20scenario%20where%20VRUs%20are%20hidden%20from%0Adrivers%27%20view%20by%20obstacles%20like%20parked%20vehicles%20-%20being%20a%20critical%20contributing%0Afactor.%20To%20address%20this%2C%20we%20present%20a%20novel%20algorithm%20that%20quantifies%20occlusion%0Arisk%20based%20on%20the%20dynamics%20of%20both%20vehicles%20and%20VRUs.%20This%20algorithm%20has%0Aundergone%20testing%20and%20evaluation%20using%20a%20real-world%20dataset%20from%20German%0Aintersections.%20Additionally%2C%20we%20introduce%20the%20concept%20of%20Maximum%20Tracking%20Loss%0A%28MTL%29%2C%20which%20measures%20the%20longest%20consecutive%20duration%20a%20VRU%20remains%20untracked%0Aby%20any%20vehicle%20in%20a%20given%20scenario.%20Our%20study%20extends%20to%20examining%20the%20role%20of%0Athe%20Collective%20Perception%20Service%20%28CPS%29%20in%20VRU%20safety.%20CPS%20enhances%20safety%20by%0Aenabling%20vehicles%20to%20share%20sensor%20information%2C%20thereby%20potentially%20reducing%0Aocclusion%20risks.%20Our%20analysis%20reveals%20that%20a%2025%25%20market%20penetration%20of%0ACPS-equipped%20vehicles%20can%20substantially%20diminish%20occlusion%20risks%20and%0Asignificantly%20curtail%20MTL.%20These%20findings%20demonstrate%20how%20various%20scenarios%0Apose%20different%20levels%20of%20risk%20to%20VRUs%20and%20how%20the%20deployment%20of%20Collective%0APerception%20can%20markedly%20improve%20their%20safety.%20Furthermore%2C%20they%20underline%20the%0Aefficacy%20of%20our%20proposed%20metrics%20to%20capture%20occlusion%20risk%20as%20a%20safety%20factor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07753v1&entry.124074799=Read"},
{"title": "Estimating Visibility from Alternate Perspectives for Motion Planning\n  with Occlusions", "author": "Barry Gilhuly and Armin Sadeghi and Stephen L. Smith", "abstract": "  Visibility is a crucial aspect of planning and control of autonomous vehicles\n(AV), particularly when navigating environments with occlusions. However, when\nan AV follows a trajectory with multiple occlusions, existing methods evaluate\neach occlusion individually, calculate a visibility cost for each, and rely on\nthe planner to minimize the overall cost. This can result in conflicting\npriorities for the planner, as individual occlusion costs may appear to be in\nopposition. We solve this problem by creating an alternate perspective cost map\nthat allows for an aggregate view of the occlusions in the environment. The\nvalue of each cell on the cost map is a measure of the amount of visual\ninformation that the vehicle can gain about the environment by visiting that\nlocation. Our proposed method identifies observation locations and occlusion\ntargets drawn from both map data and sensor data. We show how to estimate an\nalternate perspective for each observation location and then combine all\nestimates into a single alternate perspective cost map for motion planning.\n", "link": "http://arxiv.org/abs/2404.07781v1", "date": "2024-04-11", "relevancy": 2.2113, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5646}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5344}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Estimating%20Visibility%20from%20Alternate%20Perspectives%20for%20Motion%20Planning%0A%20%20with%20Occlusions&body=Title%3A%20Estimating%20Visibility%20from%20Alternate%20Perspectives%20for%20Motion%20Planning%0A%20%20with%20Occlusions%0AAuthor%3A%20Barry%20Gilhuly%20and%20Armin%20Sadeghi%20and%20Stephen%20L.%20Smith%0AAbstract%3A%20%20%20Visibility%20is%20a%20crucial%20aspect%20of%20planning%20and%20control%20of%20autonomous%20vehicles%0A%28AV%29%2C%20particularly%20when%20navigating%20environments%20with%20occlusions.%20However%2C%20when%0Aan%20AV%20follows%20a%20trajectory%20with%20multiple%20occlusions%2C%20existing%20methods%20evaluate%0Aeach%20occlusion%20individually%2C%20calculate%20a%20visibility%20cost%20for%20each%2C%20and%20rely%20on%0Athe%20planner%20to%20minimize%20the%20overall%20cost.%20This%20can%20result%20in%20conflicting%0Apriorities%20for%20the%20planner%2C%20as%20individual%20occlusion%20costs%20may%20appear%20to%20be%20in%0Aopposition.%20We%20solve%20this%20problem%20by%20creating%20an%20alternate%20perspective%20cost%20map%0Athat%20allows%20for%20an%20aggregate%20view%20of%20the%20occlusions%20in%20the%20environment.%20The%0Avalue%20of%20each%20cell%20on%20the%20cost%20map%20is%20a%20measure%20of%20the%20amount%20of%20visual%0Ainformation%20that%20the%20vehicle%20can%20gain%20about%20the%20environment%20by%20visiting%20that%0Alocation.%20Our%20proposed%20method%20identifies%20observation%20locations%20and%20occlusion%0Atargets%20drawn%20from%20both%20map%20data%20and%20sensor%20data.%20We%20show%20how%20to%20estimate%20an%0Aalternate%20perspective%20for%20each%20observation%20location%20and%20then%20combine%20all%0Aestimates%20into%20a%20single%20alternate%20perspective%20cost%20map%20for%20motion%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07781v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Visibility%20from%20Alternate%20Perspectives%20for%20Motion%20Planning%0A%20%20with%20Occlusions&entry.906535625=Barry%20Gilhuly%20and%20Armin%20Sadeghi%20and%20Stephen%20L.%20Smith&entry.1292438233=%20%20Visibility%20is%20a%20crucial%20aspect%20of%20planning%20and%20control%20of%20autonomous%20vehicles%0A%28AV%29%2C%20particularly%20when%20navigating%20environments%20with%20occlusions.%20However%2C%20when%0Aan%20AV%20follows%20a%20trajectory%20with%20multiple%20occlusions%2C%20existing%20methods%20evaluate%0Aeach%20occlusion%20individually%2C%20calculate%20a%20visibility%20cost%20for%20each%2C%20and%20rely%20on%0Athe%20planner%20to%20minimize%20the%20overall%20cost.%20This%20can%20result%20in%20conflicting%0Apriorities%20for%20the%20planner%2C%20as%20individual%20occlusion%20costs%20may%20appear%20to%20be%20in%0Aopposition.%20We%20solve%20this%20problem%20by%20creating%20an%20alternate%20perspective%20cost%20map%0Athat%20allows%20for%20an%20aggregate%20view%20of%20the%20occlusions%20in%20the%20environment.%20The%0Avalue%20of%20each%20cell%20on%20the%20cost%20map%20is%20a%20measure%20of%20the%20amount%20of%20visual%0Ainformation%20that%20the%20vehicle%20can%20gain%20about%20the%20environment%20by%20visiting%20that%0Alocation.%20Our%20proposed%20method%20identifies%20observation%20locations%20and%20occlusion%0Atargets%20drawn%20from%20both%20map%20data%20and%20sensor%20data.%20We%20show%20how%20to%20estimate%20an%0Aalternate%20perspective%20for%20each%20observation%20location%20and%20then%20combine%20all%0Aestimates%20into%20a%20single%20alternate%20perspective%20cost%20map%20for%20motion%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07781v1&entry.124074799=Read"},
{"title": "T-DEED: Temporal-Discriminability Enhancer Encoder-Decoder for Precise\n  Event Spotting in Sports Videos", "author": "Artur Xarles and Sergio Escalera and Thomas B. Moeslund and Albert Clap\u00e9s", "abstract": "  In this paper, we introduce T-DEED, a Temporal-Discriminability Enhancer\nEncoder-Decoder for Precise Event Spotting in sports videos. T-DEED addresses\nmultiple challenges in the task, including the need for discriminability among\nframe representations, high output temporal resolution to maintain prediction\nprecision, and the necessity to capture information at different temporal\nscales to handle events with varying dynamics. It tackles these challenges\nthrough its specifically designed architecture, featuring an encoder-decoder\nfor leveraging multiple temporal scales and achieving high output temporal\nresolution, along with temporal modules designed to increase token\ndiscriminability. Leveraging these characteristics, T-DEED achieves SOTA\nperformance on the FigureSkating and FineDiving datasets. Code is available at\nhttps://github.com/arturxe2/T-DEED.\n", "link": "http://arxiv.org/abs/2404.05392v2", "date": "2024-04-11", "relevancy": 2.1997, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5537}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5485}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20T-DEED%3A%20Temporal-Discriminability%20Enhancer%20Encoder-Decoder%20for%20Precise%0A%20%20Event%20Spotting%20in%20Sports%20Videos&body=Title%3A%20T-DEED%3A%20Temporal-Discriminability%20Enhancer%20Encoder-Decoder%20for%20Precise%0A%20%20Event%20Spotting%20in%20Sports%20Videos%0AAuthor%3A%20Artur%20Xarles%20and%20Sergio%20Escalera%20and%20Thomas%20B.%20Moeslund%20and%20Albert%20Clap%C3%A9s%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20T-DEED%2C%20a%20Temporal-Discriminability%20Enhancer%0AEncoder-Decoder%20for%20Precise%20Event%20Spotting%20in%20sports%20videos.%20T-DEED%20addresses%0Amultiple%20challenges%20in%20the%20task%2C%20including%20the%20need%20for%20discriminability%20among%0Aframe%20representations%2C%20high%20output%20temporal%20resolution%20to%20maintain%20prediction%0Aprecision%2C%20and%20the%20necessity%20to%20capture%20information%20at%20different%20temporal%0Ascales%20to%20handle%20events%20with%20varying%20dynamics.%20It%20tackles%20these%20challenges%0Athrough%20its%20specifically%20designed%20architecture%2C%20featuring%20an%20encoder-decoder%0Afor%20leveraging%20multiple%20temporal%20scales%20and%20achieving%20high%20output%20temporal%0Aresolution%2C%20along%20with%20temporal%20modules%20designed%20to%20increase%20token%0Adiscriminability.%20Leveraging%20these%20characteristics%2C%20T-DEED%20achieves%20SOTA%0Aperformance%20on%20the%20FigureSkating%20and%20FineDiving%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/arturxe2/T-DEED.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05392v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-DEED%3A%20Temporal-Discriminability%20Enhancer%20Encoder-Decoder%20for%20Precise%0A%20%20Event%20Spotting%20in%20Sports%20Videos&entry.906535625=Artur%20Xarles%20and%20Sergio%20Escalera%20and%20Thomas%20B.%20Moeslund%20and%20Albert%20Clap%C3%A9s&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20T-DEED%2C%20a%20Temporal-Discriminability%20Enhancer%0AEncoder-Decoder%20for%20Precise%20Event%20Spotting%20in%20sports%20videos.%20T-DEED%20addresses%0Amultiple%20challenges%20in%20the%20task%2C%20including%20the%20need%20for%20discriminability%20among%0Aframe%20representations%2C%20high%20output%20temporal%20resolution%20to%20maintain%20prediction%0Aprecision%2C%20and%20the%20necessity%20to%20capture%20information%20at%20different%20temporal%0Ascales%20to%20handle%20events%20with%20varying%20dynamics.%20It%20tackles%20these%20challenges%0Athrough%20its%20specifically%20designed%20architecture%2C%20featuring%20an%20encoder-decoder%0Afor%20leveraging%20multiple%20temporal%20scales%20and%20achieving%20high%20output%20temporal%0Aresolution%2C%20along%20with%20temporal%20modules%20designed%20to%20increase%20token%0Adiscriminability.%20Leveraging%20these%20characteristics%2C%20T-DEED%20achieves%20SOTA%0Aperformance%20on%20the%20FigureSkating%20and%20FineDiving%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/arturxe2/T-DEED.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05392v2&entry.124074799=Read"},
{"title": "COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy\n  Prediction", "author": "Qihang Ma and Xin Tan and Yanyun Qu and Lizhuang Ma and Zhizhong Zhang and Yuan Xie", "abstract": "  The autonomous driving community has shown significant interest in 3D\noccupancy prediction, driven by its exceptional geometric perception and\ngeneral object recognition capabilities. To achieve this, current works try to\nconstruct a Tri-Perspective View (TPV) or Occupancy (OCC) representation\nextending from the Bird-Eye-View perception. However, compressed views like TPV\nrepresentation lose 3D geometry information while raw and sparse OCC\nrepresentation requires heavy but redundant computational costs. To address the\nabove limitations, we propose Compact Occupancy TRansformer (COTR), with a\ngeometry-aware occupancy encoder and a semantic-aware group decoder to\nreconstruct a compact 3D OCC representation. The occupancy encoder first\ngenerates a compact geometrical OCC feature through efficient explicit-implicit\nview transformation. Then, the occupancy decoder further enhances the semantic\ndiscriminability of the compact OCC representation by a coarse-to-fine semantic\ngrouping strategy. Empirical experiments show that there are evident\nperformance gains across multiple baselines, e.g., COTR outperforms baselines\nwith a relative improvement of 8%-15%, demonstrating the superiority of our\nmethod.\n", "link": "http://arxiv.org/abs/2312.01919v2", "date": "2024-04-11", "relevancy": 2.1846, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5586}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5531}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5309}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20COTR%3A%20Compact%20Occupancy%20TRansformer%20for%20Vision-based%203D%20Occupancy%0A%20%20Prediction&body=Title%3A%20COTR%3A%20Compact%20Occupancy%20TRansformer%20for%20Vision-based%203D%20Occupancy%0A%20%20Prediction%0AAuthor%3A%20Qihang%20Ma%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Lizhuang%20Ma%20and%20Zhizhong%20Zhang%20and%20Yuan%20Xie%0AAbstract%3A%20%20%20The%20autonomous%20driving%20community%20has%20shown%20significant%20interest%20in%203D%0Aoccupancy%20prediction%2C%20driven%20by%20its%20exceptional%20geometric%20perception%20and%0Ageneral%20object%20recognition%20capabilities.%20To%20achieve%20this%2C%20current%20works%20try%20to%0Aconstruct%20a%20Tri-Perspective%20View%20%28TPV%29%20or%20Occupancy%20%28OCC%29%20representation%0Aextending%20from%20the%20Bird-Eye-View%20perception.%20However%2C%20compressed%20views%20like%20TPV%0Arepresentation%20lose%203D%20geometry%20information%20while%20raw%20and%20sparse%20OCC%0Arepresentation%20requires%20heavy%20but%20redundant%20computational%20costs.%20To%20address%20the%0Aabove%20limitations%2C%20we%20propose%20Compact%20Occupancy%20TRansformer%20%28COTR%29%2C%20with%20a%0Ageometry-aware%20occupancy%20encoder%20and%20a%20semantic-aware%20group%20decoder%20to%0Areconstruct%20a%20compact%203D%20OCC%20representation.%20The%20occupancy%20encoder%20first%0Agenerates%20a%20compact%20geometrical%20OCC%20feature%20through%20efficient%20explicit-implicit%0Aview%20transformation.%20Then%2C%20the%20occupancy%20decoder%20further%20enhances%20the%20semantic%0Adiscriminability%20of%20the%20compact%20OCC%20representation%20by%20a%20coarse-to-fine%20semantic%0Agrouping%20strategy.%20Empirical%20experiments%20show%20that%20there%20are%20evident%0Aperformance%20gains%20across%20multiple%20baselines%2C%20e.g.%2C%20COTR%20outperforms%20baselines%0Awith%20a%20relative%20improvement%20of%208%25-15%25%2C%20demonstrating%20the%20superiority%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01919v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COTR%3A%20Compact%20Occupancy%20TRansformer%20for%20Vision-based%203D%20Occupancy%0A%20%20Prediction&entry.906535625=Qihang%20Ma%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Lizhuang%20Ma%20and%20Zhizhong%20Zhang%20and%20Yuan%20Xie&entry.1292438233=%20%20The%20autonomous%20driving%20community%20has%20shown%20significant%20interest%20in%203D%0Aoccupancy%20prediction%2C%20driven%20by%20its%20exceptional%20geometric%20perception%20and%0Ageneral%20object%20recognition%20capabilities.%20To%20achieve%20this%2C%20current%20works%20try%20to%0Aconstruct%20a%20Tri-Perspective%20View%20%28TPV%29%20or%20Occupancy%20%28OCC%29%20representation%0Aextending%20from%20the%20Bird-Eye-View%20perception.%20However%2C%20compressed%20views%20like%20TPV%0Arepresentation%20lose%203D%20geometry%20information%20while%20raw%20and%20sparse%20OCC%0Arepresentation%20requires%20heavy%20but%20redundant%20computational%20costs.%20To%20address%20the%0Aabove%20limitations%2C%20we%20propose%20Compact%20Occupancy%20TRansformer%20%28COTR%29%2C%20with%20a%0Ageometry-aware%20occupancy%20encoder%20and%20a%20semantic-aware%20group%20decoder%20to%0Areconstruct%20a%20compact%203D%20OCC%20representation.%20The%20occupancy%20encoder%20first%0Agenerates%20a%20compact%20geometrical%20OCC%20feature%20through%20efficient%20explicit-implicit%0Aview%20transformation.%20Then%2C%20the%20occupancy%20decoder%20further%20enhances%20the%20semantic%0Adiscriminability%20of%20the%20compact%20OCC%20representation%20by%20a%20coarse-to-fine%20semantic%0Agrouping%20strategy.%20Empirical%20experiments%20show%20that%20there%20are%20evident%0Aperformance%20gains%20across%20multiple%20baselines%2C%20e.g.%2C%20COTR%20outperforms%20baselines%0Awith%20a%20relative%20improvement%20of%208%25-15%25%2C%20demonstrating%20the%20superiority%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01919v2&entry.124074799=Read"},
{"title": "Connecting NeRFs, Images, and Text", "author": "Francesco Ballerini and Pierluigi Zama Ramirez and Roberto Mirabella and Samuele Salti and Luigi Di Stefano", "abstract": "  Neural Radiance Fields (NeRFs) have emerged as a standard framework for\nrepresenting 3D scenes and objects, introducing a novel data type for\ninformation exchange and storage. Concurrently, significant progress has been\nmade in multimodal representation learning for text and image data. This paper\nexplores a novel research direction that aims to connect the NeRF modality with\nother modalities, similar to established methodologies for images and text. To\nthis end, we propose a simple framework that exploits pre-trained models for\nNeRF representations alongside multimodal models for text and image processing.\nOur framework learns a bidirectional mapping between NeRF embeddings and those\nobtained from corresponding images and text. This mapping unlocks several novel\nand useful applications, including NeRF zero-shot classification and NeRF\nretrieval from images or text.\n", "link": "http://arxiv.org/abs/2404.07993v1", "date": "2024-04-11", "relevancy": 2.1801, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5435}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5342}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Connecting%20NeRFs%2C%20Images%2C%20and%20Text&body=Title%3A%20Connecting%20NeRFs%2C%20Images%2C%20and%20Text%0AAuthor%3A%20Francesco%20Ballerini%20and%20Pierluigi%20Zama%20Ramirez%20and%20Roberto%20Mirabella%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20a%20standard%20framework%20for%0Arepresenting%203D%20scenes%20and%20objects%2C%20introducing%20a%20novel%20data%20type%20for%0Ainformation%20exchange%20and%20storage.%20Concurrently%2C%20significant%20progress%20has%20been%0Amade%20in%20multimodal%20representation%20learning%20for%20text%20and%20image%20data.%20This%20paper%0Aexplores%20a%20novel%20research%20direction%20that%20aims%20to%20connect%20the%20NeRF%20modality%20with%0Aother%20modalities%2C%20similar%20to%20established%20methodologies%20for%20images%20and%20text.%20To%0Athis%20end%2C%20we%20propose%20a%20simple%20framework%20that%20exploits%20pre-trained%20models%20for%0ANeRF%20representations%20alongside%20multimodal%20models%20for%20text%20and%20image%20processing.%0AOur%20framework%20learns%20a%20bidirectional%20mapping%20between%20NeRF%20embeddings%20and%20those%0Aobtained%20from%20corresponding%20images%20and%20text.%20This%20mapping%20unlocks%20several%20novel%0Aand%20useful%20applications%2C%20including%20NeRF%20zero-shot%20classification%20and%20NeRF%0Aretrieval%20from%20images%20or%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07993v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20NeRFs%2C%20Images%2C%20and%20Text&entry.906535625=Francesco%20Ballerini%20and%20Pierluigi%20Zama%20Ramirez%20and%20Roberto%20Mirabella%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20a%20standard%20framework%20for%0Arepresenting%203D%20scenes%20and%20objects%2C%20introducing%20a%20novel%20data%20type%20for%0Ainformation%20exchange%20and%20storage.%20Concurrently%2C%20significant%20progress%20has%20been%0Amade%20in%20multimodal%20representation%20learning%20for%20text%20and%20image%20data.%20This%20paper%0Aexplores%20a%20novel%20research%20direction%20that%20aims%20to%20connect%20the%20NeRF%20modality%20with%0Aother%20modalities%2C%20similar%20to%20established%20methodologies%20for%20images%20and%20text.%20To%0Athis%20end%2C%20we%20propose%20a%20simple%20framework%20that%20exploits%20pre-trained%20models%20for%0ANeRF%20representations%20alongside%20multimodal%20models%20for%20text%20and%20image%20processing.%0AOur%20framework%20learns%20a%20bidirectional%20mapping%20between%20NeRF%20embeddings%20and%20those%0Aobtained%20from%20corresponding%20images%20and%20text.%20This%20mapping%20unlocks%20several%20novel%0Aand%20useful%20applications%2C%20including%20NeRF%20zero-shot%20classification%20and%20NeRF%0Aretrieval%20from%20images%20or%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07993v1&entry.124074799=Read"},
{"title": "DGMamba: Domain Generalization via Generalized State Space Model", "author": "Shaocong Long and Qianyu Zhou and Xiangtai Li and Xuequan Lu and Chenhao Ying and Yuan Luo and Lizhuang Ma and Shuicheng Yan", "abstract": "  Domain generalization~(DG) aims at solving distribution shift problems in\nvarious scenes. Existing approaches are based on Convolution Neural Networks\n(CNNs) or Vision Transformers (ViTs), which suffer from limited receptive\nfields or quadratic complexities issues. Mamba, as an emerging state space\nmodel (SSM), possesses superior linear complexity and global receptive fields.\nDespite this, it can hardly be applied to DG to address distribution shifts,\ndue to the hidden state issues and inappropriate scan mechanisms. In this\npaper, we propose a novel framework for DG, named DGMamba, that excels in\nstrong generalizability toward unseen domains and meanwhile has the advantages\nof global receptive fields, and efficient linear complexity. Our DGMamba\ncompromises two core components: Hidden State Suppressing~(HSS) and\nSemantic-aware Patch refining~(SPR). In particular, HSS is introduced to\nmitigate the influence of hidden states associated with domain-specific\nfeatures during output prediction. SPR strives to encourage the model to\nconcentrate more on objects rather than context, consisting of two designs:\nPrior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely,\nPFS aims to shuffle the non-semantic patches within images, creating more\nflexible and effective sequences from images, and DCI is designed to regularize\nMamba with the combination of mismatched non-semantic and semantic information\nby fusing patches among domains. Extensive experiments on four commonly used DG\nbenchmarks demonstrate that the proposed DGMamba achieves remarkably superior\nresults to state-of-the-art models. The code will be made publicly available.\n", "link": "http://arxiv.org/abs/2404.07794v1", "date": "2024-04-11", "relevancy": 2.1753, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5719}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.536}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5189}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DGMamba%3A%20Domain%20Generalization%20via%20Generalized%20State%20Space%20Model&body=Title%3A%20DGMamba%3A%20Domain%20Generalization%20via%20Generalized%20State%20Space%20Model%0AAuthor%3A%20Shaocong%20Long%20and%20Qianyu%20Zhou%20and%20Xiangtai%20Li%20and%20Xuequan%20Lu%20and%20Chenhao%20Ying%20and%20Yuan%20Luo%20and%20Lizhuang%20Ma%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Domain%20generalization~%28DG%29%20aims%20at%20solving%20distribution%20shift%20problems%20in%0Avarious%20scenes.%20Existing%20approaches%20are%20based%20on%20Convolution%20Neural%20Networks%0A%28CNNs%29%20or%20Vision%20Transformers%20%28ViTs%29%2C%20which%20suffer%20from%20limited%20receptive%0Afields%20or%20quadratic%20complexities%20issues.%20Mamba%2C%20as%20an%20emerging%20state%20space%0Amodel%20%28SSM%29%2C%20possesses%20superior%20linear%20complexity%20and%20global%20receptive%20fields.%0ADespite%20this%2C%20it%20can%20hardly%20be%20applied%20to%20DG%20to%20address%20distribution%20shifts%2C%0Adue%20to%20the%20hidden%20state%20issues%20and%20inappropriate%20scan%20mechanisms.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%20for%20DG%2C%20named%20DGMamba%2C%20that%20excels%20in%0Astrong%20generalizability%20toward%20unseen%20domains%20and%20meanwhile%20has%20the%20advantages%0Aof%20global%20receptive%20fields%2C%20and%20efficient%20linear%20complexity.%20Our%20DGMamba%0Acompromises%20two%20core%20components%3A%20Hidden%20State%20Suppressing~%28HSS%29%20and%0ASemantic-aware%20Patch%20refining~%28SPR%29.%20In%20particular%2C%20HSS%20is%20introduced%20to%0Amitigate%20the%20influence%20of%20hidden%20states%20associated%20with%20domain-specific%0Afeatures%20during%20output%20prediction.%20SPR%20strives%20to%20encourage%20the%20model%20to%0Aconcentrate%20more%20on%20objects%20rather%20than%20context%2C%20consisting%20of%20two%20designs%3A%0APrior-Free%20Scanning~%28PFS%29%2C%20and%20Domain%20Context%20Interchange~%28DCI%29.%20Concretely%2C%0APFS%20aims%20to%20shuffle%20the%20non-semantic%20patches%20within%20images%2C%20creating%20more%0Aflexible%20and%20effective%20sequences%20from%20images%2C%20and%20DCI%20is%20designed%20to%20regularize%0AMamba%20with%20the%20combination%20of%20mismatched%20non-semantic%20and%20semantic%20information%0Aby%20fusing%20patches%20among%20domains.%20Extensive%20experiments%20on%20four%20commonly%20used%20DG%0Abenchmarks%20demonstrate%20that%20the%20proposed%20DGMamba%20achieves%20remarkably%20superior%0Aresults%20to%20state-of-the-art%20models.%20The%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07794v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGMamba%3A%20Domain%20Generalization%20via%20Generalized%20State%20Space%20Model&entry.906535625=Shaocong%20Long%20and%20Qianyu%20Zhou%20and%20Xiangtai%20Li%20and%20Xuequan%20Lu%20and%20Chenhao%20Ying%20and%20Yuan%20Luo%20and%20Lizhuang%20Ma%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Domain%20generalization~%28DG%29%20aims%20at%20solving%20distribution%20shift%20problems%20in%0Avarious%20scenes.%20Existing%20approaches%20are%20based%20on%20Convolution%20Neural%20Networks%0A%28CNNs%29%20or%20Vision%20Transformers%20%28ViTs%29%2C%20which%20suffer%20from%20limited%20receptive%0Afields%20or%20quadratic%20complexities%20issues.%20Mamba%2C%20as%20an%20emerging%20state%20space%0Amodel%20%28SSM%29%2C%20possesses%20superior%20linear%20complexity%20and%20global%20receptive%20fields.%0ADespite%20this%2C%20it%20can%20hardly%20be%20applied%20to%20DG%20to%20address%20distribution%20shifts%2C%0Adue%20to%20the%20hidden%20state%20issues%20and%20inappropriate%20scan%20mechanisms.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%20for%20DG%2C%20named%20DGMamba%2C%20that%20excels%20in%0Astrong%20generalizability%20toward%20unseen%20domains%20and%20meanwhile%20has%20the%20advantages%0Aof%20global%20receptive%20fields%2C%20and%20efficient%20linear%20complexity.%20Our%20DGMamba%0Acompromises%20two%20core%20components%3A%20Hidden%20State%20Suppressing~%28HSS%29%20and%0ASemantic-aware%20Patch%20refining~%28SPR%29.%20In%20particular%2C%20HSS%20is%20introduced%20to%0Amitigate%20the%20influence%20of%20hidden%20states%20associated%20with%20domain-specific%0Afeatures%20during%20output%20prediction.%20SPR%20strives%20to%20encourage%20the%20model%20to%0Aconcentrate%20more%20on%20objects%20rather%20than%20context%2C%20consisting%20of%20two%20designs%3A%0APrior-Free%20Scanning~%28PFS%29%2C%20and%20Domain%20Context%20Interchange~%28DCI%29.%20Concretely%2C%0APFS%20aims%20to%20shuffle%20the%20non-semantic%20patches%20within%20images%2C%20creating%20more%0Aflexible%20and%20effective%20sequences%20from%20images%2C%20and%20DCI%20is%20designed%20to%20regularize%0AMamba%20with%20the%20combination%20of%20mismatched%20non-semantic%20and%20semantic%20information%0Aby%20fusing%20patches%20among%20domains.%20Extensive%20experiments%20on%20four%20commonly%20used%20DG%0Abenchmarks%20demonstrate%20that%20the%20proposed%20DGMamba%20achieves%20remarkably%20superior%0Aresults%20to%20state-of-the-art%20models.%20The%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07794v1&entry.124074799=Read"},
{"title": "Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in\n  Remote Sensing", "author": "Jakob Hackstein and Gencer Sumbul and Kai Norman Clasen and Beg\u00fcm Demir", "abstract": "  Self-supervised learning through masked autoencoders (MAEs) has recently\nattracted great attention for remote sensing (RS) image representation\nlearning, and thus embodies a significant potential for content-based image\nretrieval (CBIR) from ever-growing RS image archives. However, the existing\nstudies on MAEs in RS assume that the considered RS images are acquired by a\nsingle image sensor, and thus are only suitable for uni-modal CBIR problems.\nThe effectiveness of MAEs for cross-sensor CBIR, which aims to search\nsemantically similar images across different image modalities, has not been\nexplored yet. In this paper, we take the first step to explore the\neffectiveness of MAEs for sensor-agnostic CBIR in RS. To this end, we present a\nsystematic overview on the possible adaptations of the vanilla MAE to exploit\nmasked image modeling on multi-sensor RS image archives (denoted as\ncross-sensor masked autoencoders [CSMAEs]). Based on different adjustments\napplied to the vanilla MAE, we introduce different CSMAE models. We also\nprovide an extensive experimental analysis of these CSMAE models. We finally\nderive a guideline to exploit masked image modeling for uni-modal and\ncross-modal CBIR problems in RS. The code of this work is publicly available at\nhttps://github.com/jakhac/CSMAE.\n", "link": "http://arxiv.org/abs/2401.07782v2", "date": "2024-04-11", "relevancy": 2.1695, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5742}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5248}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5176}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Masked%20Autoencoders%20for%20Sensor-Agnostic%20Image%20Retrieval%20in%0A%20%20Remote%20Sensing&body=Title%3A%20Exploring%20Masked%20Autoencoders%20for%20Sensor-Agnostic%20Image%20Retrieval%20in%0A%20%20Remote%20Sensing%0AAuthor%3A%20Jakob%20Hackstein%20and%20Gencer%20Sumbul%20and%20Kai%20Norman%20Clasen%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20Self-supervised%20learning%20through%20masked%20autoencoders%20%28MAEs%29%20has%20recently%0Aattracted%20great%20attention%20for%20remote%20sensing%20%28RS%29%20image%20representation%0Alearning%2C%20and%20thus%20embodies%20a%20significant%20potential%20for%20content-based%20image%0Aretrieval%20%28CBIR%29%20from%20ever-growing%20RS%20image%20archives.%20However%2C%20the%20existing%0Astudies%20on%20MAEs%20in%20RS%20assume%20that%20the%20considered%20RS%20images%20are%20acquired%20by%20a%0Asingle%20image%20sensor%2C%20and%20thus%20are%20only%20suitable%20for%20uni-modal%20CBIR%20problems.%0AThe%20effectiveness%20of%20MAEs%20for%20cross-sensor%20CBIR%2C%20which%20aims%20to%20search%0Asemantically%20similar%20images%20across%20different%20image%20modalities%2C%20has%20not%20been%0Aexplored%20yet.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20to%20explore%20the%0Aeffectiveness%20of%20MAEs%20for%20sensor-agnostic%20CBIR%20in%20RS.%20To%20this%20end%2C%20we%20present%20a%0Asystematic%20overview%20on%20the%20possible%20adaptations%20of%20the%20vanilla%20MAE%20to%20exploit%0Amasked%20image%20modeling%20on%20multi-sensor%20RS%20image%20archives%20%28denoted%20as%0Across-sensor%20masked%20autoencoders%20%5BCSMAEs%5D%29.%20Based%20on%20different%20adjustments%0Aapplied%20to%20the%20vanilla%20MAE%2C%20we%20introduce%20different%20CSMAE%20models.%20We%20also%0Aprovide%20an%20extensive%20experimental%20analysis%20of%20these%20CSMAE%20models.%20We%20finally%0Aderive%20a%20guideline%20to%20exploit%20masked%20image%20modeling%20for%20uni-modal%20and%0Across-modal%20CBIR%20problems%20in%20RS.%20The%20code%20of%20this%20work%20is%20publicly%20available%20at%0Ahttps%3A//github.com/jakhac/CSMAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07782v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Masked%20Autoencoders%20for%20Sensor-Agnostic%20Image%20Retrieval%20in%0A%20%20Remote%20Sensing&entry.906535625=Jakob%20Hackstein%20and%20Gencer%20Sumbul%20and%20Kai%20Norman%20Clasen%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20Self-supervised%20learning%20through%20masked%20autoencoders%20%28MAEs%29%20has%20recently%0Aattracted%20great%20attention%20for%20remote%20sensing%20%28RS%29%20image%20representation%0Alearning%2C%20and%20thus%20embodies%20a%20significant%20potential%20for%20content-based%20image%0Aretrieval%20%28CBIR%29%20from%20ever-growing%20RS%20image%20archives.%20However%2C%20the%20existing%0Astudies%20on%20MAEs%20in%20RS%20assume%20that%20the%20considered%20RS%20images%20are%20acquired%20by%20a%0Asingle%20image%20sensor%2C%20and%20thus%20are%20only%20suitable%20for%20uni-modal%20CBIR%20problems.%0AThe%20effectiveness%20of%20MAEs%20for%20cross-sensor%20CBIR%2C%20which%20aims%20to%20search%0Asemantically%20similar%20images%20across%20different%20image%20modalities%2C%20has%20not%20been%0Aexplored%20yet.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20to%20explore%20the%0Aeffectiveness%20of%20MAEs%20for%20sensor-agnostic%20CBIR%20in%20RS.%20To%20this%20end%2C%20we%20present%20a%0Asystematic%20overview%20on%20the%20possible%20adaptations%20of%20the%20vanilla%20MAE%20to%20exploit%0Amasked%20image%20modeling%20on%20multi-sensor%20RS%20image%20archives%20%28denoted%20as%0Across-sensor%20masked%20autoencoders%20%5BCSMAEs%5D%29.%20Based%20on%20different%20adjustments%0Aapplied%20to%20the%20vanilla%20MAE%2C%20we%20introduce%20different%20CSMAE%20models.%20We%20also%0Aprovide%20an%20extensive%20experimental%20analysis%20of%20these%20CSMAE%20models.%20We%20finally%0Aderive%20a%20guideline%20to%20exploit%20masked%20image%20modeling%20for%20uni-modal%20and%0Across-modal%20CBIR%20problems%20in%20RS.%20The%20code%20of%20this%20work%20is%20publicly%20available%20at%0Ahttps%3A//github.com/jakhac/CSMAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07782v2&entry.124074799=Read"},
{"title": "MoCha-Stereo: Motif Channel Attention Network for Stereo Matching", "author": "Ziyang Chen and Wei Long and He Yao and Yongjun Zhang and Bingshu Wang and Yongbin Qin and Jia Wu", "abstract": "  Learning-based stereo matching techniques have made significant progress.\nHowever, existing methods inevitably lose geometrical structure information\nduring the feature channel generation process, resulting in edge detail\nmismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network\n(MoCha-Stereo) is designed to address this problem. We provide the Motif\nChannel Correlation Volume (MCCV) to determine more accurate edge matching\ncosts. MCCV is achieved by projecting motif channels, which capture common\ngeometric structures in feature channels, onto feature maps and cost volumes.\nIn addition, edge variations in %potential feature channels of the\nreconstruction error map also affect details matching, we propose the\nReconstruction Error Motif Penalty (REMP) module to further refine the\nfull-resolution disparity estimation. REMP integrates the frequency information\nof typical channel features from the reconstruction error. MoCha-Stereo ranks\n1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure\nalso shows excellent performance in Multi-View Stereo. Code is avaliable at\nhttps://github.com/ZYangChen/MoCha-Stereo.\n", "link": "http://arxiv.org/abs/2404.06842v2", "date": "2024-04-11", "relevancy": 2.1607, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5633}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5328}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MoCha-Stereo%3A%20Motif%20Channel%20Attention%20Network%20for%20Stereo%20Matching&body=Title%3A%20MoCha-Stereo%3A%20Motif%20Channel%20Attention%20Network%20for%20Stereo%20Matching%0AAuthor%3A%20Ziyang%20Chen%20and%20Wei%20Long%20and%20He%20Yao%20and%20Yongjun%20Zhang%20and%20Bingshu%20Wang%20and%20Yongbin%20Qin%20and%20Jia%20Wu%0AAbstract%3A%20%20%20Learning-based%20stereo%20matching%20techniques%20have%20made%20significant%20progress.%0AHowever%2C%20existing%20methods%20inevitably%20lose%20geometrical%20structure%20information%0Aduring%20the%20feature%20channel%20generation%20process%2C%20resulting%20in%20edge%20detail%0Amismatches.%20In%20this%20paper%2C%20the%20Motif%20Cha%7Dnnel%20Attention%20Stereo%20Matching%20Network%0A%28MoCha-Stereo%29%20is%20designed%20to%20address%20this%20problem.%20We%20provide%20the%20Motif%0AChannel%20Correlation%20Volume%20%28MCCV%29%20to%20determine%20more%20accurate%20edge%20matching%0Acosts.%20MCCV%20is%20achieved%20by%20projecting%20motif%20channels%2C%20which%20capture%20common%0Ageometric%20structures%20in%20feature%20channels%2C%20onto%20feature%20maps%20and%20cost%20volumes.%0AIn%20addition%2C%20edge%20variations%20in%20%25potential%20feature%20channels%20of%20the%0Areconstruction%20error%20map%20also%20affect%20details%20matching%2C%20we%20propose%20the%0AReconstruction%20Error%20Motif%20Penalty%20%28REMP%29%20module%20to%20further%20refine%20the%0Afull-resolution%20disparity%20estimation.%20REMP%20integrates%20the%20frequency%20information%0Aof%20typical%20channel%20features%20from%20the%20reconstruction%20error.%20MoCha-Stereo%20ranks%0A1st%20on%20the%20KITTI-2015%20and%20KITTI-2012%20Reflective%20leaderboards.%20Our%20structure%0Aalso%20shows%20excellent%20performance%20in%20Multi-View%20Stereo.%20Code%20is%20avaliable%20at%0Ahttps%3A//github.com/ZYangChen/MoCha-Stereo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06842v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoCha-Stereo%3A%20Motif%20Channel%20Attention%20Network%20for%20Stereo%20Matching&entry.906535625=Ziyang%20Chen%20and%20Wei%20Long%20and%20He%20Yao%20and%20Yongjun%20Zhang%20and%20Bingshu%20Wang%20and%20Yongbin%20Qin%20and%20Jia%20Wu&entry.1292438233=%20%20Learning-based%20stereo%20matching%20techniques%20have%20made%20significant%20progress.%0AHowever%2C%20existing%20methods%20inevitably%20lose%20geometrical%20structure%20information%0Aduring%20the%20feature%20channel%20generation%20process%2C%20resulting%20in%20edge%20detail%0Amismatches.%20In%20this%20paper%2C%20the%20Motif%20Cha%7Dnnel%20Attention%20Stereo%20Matching%20Network%0A%28MoCha-Stereo%29%20is%20designed%20to%20address%20this%20problem.%20We%20provide%20the%20Motif%0AChannel%20Correlation%20Volume%20%28MCCV%29%20to%20determine%20more%20accurate%20edge%20matching%0Acosts.%20MCCV%20is%20achieved%20by%20projecting%20motif%20channels%2C%20which%20capture%20common%0Ageometric%20structures%20in%20feature%20channels%2C%20onto%20feature%20maps%20and%20cost%20volumes.%0AIn%20addition%2C%20edge%20variations%20in%20%25potential%20feature%20channels%20of%20the%0Areconstruction%20error%20map%20also%20affect%20details%20matching%2C%20we%20propose%20the%0AReconstruction%20Error%20Motif%20Penalty%20%28REMP%29%20module%20to%20further%20refine%20the%0Afull-resolution%20disparity%20estimation.%20REMP%20integrates%20the%20frequency%20information%0Aof%20typical%20channel%20features%20from%20the%20reconstruction%20error.%20MoCha-Stereo%20ranks%0A1st%20on%20the%20KITTI-2015%20and%20KITTI-2012%20Reflective%20leaderboards.%20Our%20structure%0Aalso%20shows%20excellent%20performance%20in%20Multi-View%20Stereo.%20Code%20is%20avaliable%20at%0Ahttps%3A//github.com/ZYangChen/MoCha-Stereo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06842v2&entry.124074799=Read"},
{"title": "MambaAD: Exploring State Space Models for Multi-class Unsupervised\n  Anomaly Detection", "author": "Haoyang He and Yuhu Bai and Jiangning Zhang and Qingdong He and Hongxu Chen and Zhenye Gan and Chengjie Wang and Xiangtai Li and Guanzhong Tian and Lei Xie", "abstract": "  Recent advancements in anomaly detection have seen the efficacy of CNN- and\ntransformer-based approaches. However, CNNs struggle with long-range\ndependencies, while transformers are burdened by quadratic computational\ncomplexity. Mamba-based models, with their superior long-range modeling and\nlinear efficiency, have garnered substantial attention. This study pioneers the\napplication of Mamba to multi-class unsupervised anomaly detection, presenting\nMambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring\n(Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSS\nmodule, integrating parallel cascaded (Hybrid State Space) HSS blocks and\nmulti-kernel convolutions operations, effectively captures both long-range and\nlocal information. The HSS block, utilizing (Hybrid Scanning) HS encoders,\nencodes feature maps into five scanning methods and eight directions, thereby\nstrengthening global connections through the (State Space Model) SSM. The use\nof Hilbert scanning and eight directions significantly improves feature\nsequence modeling. Comprehensive experiments on six diverse anomaly detection\ndatasets and seven metrics demonstrate state-of-the-art performance,\nsubstantiating the method's effectiveness.\n", "link": "http://arxiv.org/abs/2404.06564v2", "date": "2024-04-11", "relevancy": 2.1602, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5647}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.519}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MambaAD%3A%20Exploring%20State%20Space%20Models%20for%20Multi-class%20Unsupervised%0A%20%20Anomaly%20Detection&body=Title%3A%20MambaAD%3A%20Exploring%20State%20Space%20Models%20for%20Multi-class%20Unsupervised%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Haoyang%20He%20and%20Yuhu%20Bai%20and%20Jiangning%20Zhang%20and%20Qingdong%20He%20and%20Hongxu%20Chen%20and%20Zhenye%20Gan%20and%20Chengjie%20Wang%20and%20Xiangtai%20Li%20and%20Guanzhong%20Tian%20and%20Lei%20Xie%0AAbstract%3A%20%20%20Recent%20advancements%20in%20anomaly%20detection%20have%20seen%20the%20efficacy%20of%20CNN-%20and%0Atransformer-based%20approaches.%20However%2C%20CNNs%20struggle%20with%20long-range%0Adependencies%2C%20while%20transformers%20are%20burdened%20by%20quadratic%20computational%0Acomplexity.%20Mamba-based%20models%2C%20with%20their%20superior%20long-range%20modeling%20and%0Alinear%20efficiency%2C%20have%20garnered%20substantial%20attention.%20This%20study%20pioneers%20the%0Aapplication%20of%20Mamba%20to%20multi-class%20unsupervised%20anomaly%20detection%2C%20presenting%0AMambaAD%2C%20which%20consists%20of%20a%20pre-trained%20encoder%20and%20a%20Mamba%20decoder%20featuring%0A%28Locality-Enhanced%20State%20Space%29%20LSS%20modules%20at%20multi-scales.%20The%20proposed%20LSS%0Amodule%2C%20integrating%20parallel%20cascaded%20%28Hybrid%20State%20Space%29%20HSS%20blocks%20and%0Amulti-kernel%20convolutions%20operations%2C%20effectively%20captures%20both%20long-range%20and%0Alocal%20information.%20The%20HSS%20block%2C%20utilizing%20%28Hybrid%20Scanning%29%20HS%20encoders%2C%0Aencodes%20feature%20maps%20into%20five%20scanning%20methods%20and%20eight%20directions%2C%20thereby%0Astrengthening%20global%20connections%20through%20the%20%28State%20Space%20Model%29%20SSM.%20The%20use%0Aof%20Hilbert%20scanning%20and%20eight%20directions%20significantly%20improves%20feature%0Asequence%20modeling.%20Comprehensive%20experiments%20on%20six%20diverse%20anomaly%20detection%0Adatasets%20and%20seven%20metrics%20demonstrate%20state-of-the-art%20performance%2C%0Asubstantiating%20the%20method%27s%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06564v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaAD%3A%20Exploring%20State%20Space%20Models%20for%20Multi-class%20Unsupervised%0A%20%20Anomaly%20Detection&entry.906535625=Haoyang%20He%20and%20Yuhu%20Bai%20and%20Jiangning%20Zhang%20and%20Qingdong%20He%20and%20Hongxu%20Chen%20and%20Zhenye%20Gan%20and%20Chengjie%20Wang%20and%20Xiangtai%20Li%20and%20Guanzhong%20Tian%20and%20Lei%20Xie&entry.1292438233=%20%20Recent%20advancements%20in%20anomaly%20detection%20have%20seen%20the%20efficacy%20of%20CNN-%20and%0Atransformer-based%20approaches.%20However%2C%20CNNs%20struggle%20with%20long-range%0Adependencies%2C%20while%20transformers%20are%20burdened%20by%20quadratic%20computational%0Acomplexity.%20Mamba-based%20models%2C%20with%20their%20superior%20long-range%20modeling%20and%0Alinear%20efficiency%2C%20have%20garnered%20substantial%20attention.%20This%20study%20pioneers%20the%0Aapplication%20of%20Mamba%20to%20multi-class%20unsupervised%20anomaly%20detection%2C%20presenting%0AMambaAD%2C%20which%20consists%20of%20a%20pre-trained%20encoder%20and%20a%20Mamba%20decoder%20featuring%0A%28Locality-Enhanced%20State%20Space%29%20LSS%20modules%20at%20multi-scales.%20The%20proposed%20LSS%0Amodule%2C%20integrating%20parallel%20cascaded%20%28Hybrid%20State%20Space%29%20HSS%20blocks%20and%0Amulti-kernel%20convolutions%20operations%2C%20effectively%20captures%20both%20long-range%20and%0Alocal%20information.%20The%20HSS%20block%2C%20utilizing%20%28Hybrid%20Scanning%29%20HS%20encoders%2C%0Aencodes%20feature%20maps%20into%20five%20scanning%20methods%20and%20eight%20directions%2C%20thereby%0Astrengthening%20global%20connections%20through%20the%20%28State%20Space%20Model%29%20SSM.%20The%20use%0Aof%20Hilbert%20scanning%20and%20eight%20directions%20significantly%20improves%20feature%0Asequence%20modeling.%20Comprehensive%20experiments%20on%20six%20diverse%20anomaly%20detection%0Adatasets%20and%20seven%20metrics%20demonstrate%20state-of-the-art%20performance%2C%0Asubstantiating%20the%20method%27s%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06564v2&entry.124074799=Read"},
{"title": "Run-time Monitoring of 3D Object Detection in Automated Driving Systems\n  Using Early Layer Neural Activation Patterns", "author": "Hakan Yekta Yatbaz and Mehrdad Dianati and Konstantinos Koufos and Roger Woodman", "abstract": "  Monitoring the integrity of object detection for errors within the perception\nmodule of automated driving systems (ADS) is paramount for ensuring safety.\nDespite recent advancements in deep neural network (DNN)-based object\ndetectors, their susceptibility to detection errors, particularly in the\nless-explored realm of 3D object detection, remains a significant concern.\nState-of-the-art integrity monitoring (also known as introspection) mechanisms\nin 2D object detection mainly utilise the activation patterns in the final\nlayer of the DNN-based detector's backbone. However, that may not sufficiently\naddress the complexities and sparsity of data in 3D object detection. To this\nend, we conduct, in this article, an extensive investigation into the effects\nof activation patterns extracted from various layers of the backbone network\nfor introspecting the operation of 3D object detectors. Through a comparative\nanalysis using Kitti and NuScenes datasets with PointPillars and CenterPoint\ndetectors, we demonstrate that using earlier layers' activation patterns\nenhances the error detection performance of the integrity monitoring system,\nyet increases computational complexity. To address the real-time operation\nrequirements in ADS, we also introduce a novel introspection method that\ncombines activation patterns from multiple layers of the detector's backbone\nand report its performance.\n", "link": "http://arxiv.org/abs/2404.07685v1", "date": "2024-04-11", "relevancy": 2.1599, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5437}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5416}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5356}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Run-time%20Monitoring%20of%203D%20Object%20Detection%20in%20Automated%20Driving%20Systems%0A%20%20Using%20Early%20Layer%20Neural%20Activation%20Patterns&body=Title%3A%20Run-time%20Monitoring%20of%203D%20Object%20Detection%20in%20Automated%20Driving%20Systems%0A%20%20Using%20Early%20Layer%20Neural%20Activation%20Patterns%0AAuthor%3A%20Hakan%20Yekta%20Yatbaz%20and%20Mehrdad%20Dianati%20and%20Konstantinos%20Koufos%20and%20Roger%20Woodman%0AAbstract%3A%20%20%20Monitoring%20the%20integrity%20of%20object%20detection%20for%20errors%20within%20the%20perception%0Amodule%20of%20automated%20driving%20systems%20%28ADS%29%20is%20paramount%20for%20ensuring%20safety.%0ADespite%20recent%20advancements%20in%20deep%20neural%20network%20%28DNN%29-based%20object%0Adetectors%2C%20their%20susceptibility%20to%20detection%20errors%2C%20particularly%20in%20the%0Aless-explored%20realm%20of%203D%20object%20detection%2C%20remains%20a%20significant%20concern.%0AState-of-the-art%20integrity%20monitoring%20%28also%20known%20as%20introspection%29%20mechanisms%0Ain%202D%20object%20detection%20mainly%20utilise%20the%20activation%20patterns%20in%20the%20final%0Alayer%20of%20the%20DNN-based%20detector%27s%20backbone.%20However%2C%20that%20may%20not%20sufficiently%0Aaddress%20the%20complexities%20and%20sparsity%20of%20data%20in%203D%20object%20detection.%20To%20this%0Aend%2C%20we%20conduct%2C%20in%20this%20article%2C%20an%20extensive%20investigation%20into%20the%20effects%0Aof%20activation%20patterns%20extracted%20from%20various%20layers%20of%20the%20backbone%20network%0Afor%20introspecting%20the%20operation%20of%203D%20object%20detectors.%20Through%20a%20comparative%0Aanalysis%20using%20Kitti%20and%20NuScenes%20datasets%20with%20PointPillars%20and%20CenterPoint%0Adetectors%2C%20we%20demonstrate%20that%20using%20earlier%20layers%27%20activation%20patterns%0Aenhances%20the%20error%20detection%20performance%20of%20the%20integrity%20monitoring%20system%2C%0Ayet%20increases%20computational%20complexity.%20To%20address%20the%20real-time%20operation%0Arequirements%20in%20ADS%2C%20we%20also%20introduce%20a%20novel%20introspection%20method%20that%0Acombines%20activation%20patterns%20from%20multiple%20layers%20of%20the%20detector%27s%20backbone%0Aand%20report%20its%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07685v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Run-time%20Monitoring%20of%203D%20Object%20Detection%20in%20Automated%20Driving%20Systems%0A%20%20Using%20Early%20Layer%20Neural%20Activation%20Patterns&entry.906535625=Hakan%20Yekta%20Yatbaz%20and%20Mehrdad%20Dianati%20and%20Konstantinos%20Koufos%20and%20Roger%20Woodman&entry.1292438233=%20%20Monitoring%20the%20integrity%20of%20object%20detection%20for%20errors%20within%20the%20perception%0Amodule%20of%20automated%20driving%20systems%20%28ADS%29%20is%20paramount%20for%20ensuring%20safety.%0ADespite%20recent%20advancements%20in%20deep%20neural%20network%20%28DNN%29-based%20object%0Adetectors%2C%20their%20susceptibility%20to%20detection%20errors%2C%20particularly%20in%20the%0Aless-explored%20realm%20of%203D%20object%20detection%2C%20remains%20a%20significant%20concern.%0AState-of-the-art%20integrity%20monitoring%20%28also%20known%20as%20introspection%29%20mechanisms%0Ain%202D%20object%20detection%20mainly%20utilise%20the%20activation%20patterns%20in%20the%20final%0Alayer%20of%20the%20DNN-based%20detector%27s%20backbone.%20However%2C%20that%20may%20not%20sufficiently%0Aaddress%20the%20complexities%20and%20sparsity%20of%20data%20in%203D%20object%20detection.%20To%20this%0Aend%2C%20we%20conduct%2C%20in%20this%20article%2C%20an%20extensive%20investigation%20into%20the%20effects%0Aof%20activation%20patterns%20extracted%20from%20various%20layers%20of%20the%20backbone%20network%0Afor%20introspecting%20the%20operation%20of%203D%20object%20detectors.%20Through%20a%20comparative%0Aanalysis%20using%20Kitti%20and%20NuScenes%20datasets%20with%20PointPillars%20and%20CenterPoint%0Adetectors%2C%20we%20demonstrate%20that%20using%20earlier%20layers%27%20activation%20patterns%0Aenhances%20the%20error%20detection%20performance%20of%20the%20integrity%20monitoring%20system%2C%0Ayet%20increases%20computational%20complexity.%20To%20address%20the%20real-time%20operation%0Arequirements%20in%20ADS%2C%20we%20also%20introduce%20a%20novel%20introspection%20method%20that%0Acombines%20activation%20patterns%20from%20multiple%20layers%20of%20the%20detector%27s%20backbone%0Aand%20report%20its%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07685v1&entry.124074799=Read"},
{"title": "Boosting Self-Supervision for Single-View Scene Completion via Knowledge\n  Distillation", "author": "Keonhee Han and Dominik Muhle and Felix Wimbauer and Daniel Cremers", "abstract": "  Inferring scene geometry from images via Structure from Motion is a\nlong-standing and fundamental problem in computer vision. While classical\napproaches and, more recently, depth map predictions only focus on the visible\nparts of a scene, the task of scene completion aims to reason about geometry\neven in occluded regions. With the popularity of neural radiance fields\n(NeRFs), implicit representations also became popular for scene completion by\npredicting so-called density fields. Unlike explicit approaches. e.g.\nvoxel-based methods, density fields also allow for accurate depth prediction\nand novel-view synthesis via image-based rendering. In this work, we propose to\nfuse the scene reconstruction from multiple images and distill this knowledge\ninto a more accurate single-view scene reconstruction. To this end, we propose\nMulti-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed\nimages, trained fully self-supervised only from image data. Using knowledge\ndistillation, we use MVBTS to train a single-view scene completion network via\ndirect supervision called KDBTS. It achieves state-of-the-art performance on\noccupancy prediction, especially in occluded regions.\n", "link": "http://arxiv.org/abs/2404.07933v1", "date": "2024-04-11", "relevancy": 2.1504, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5531}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Boosting%20Self-Supervision%20for%20Single-View%20Scene%20Completion%20via%20Knowledge%0A%20%20Distillation&body=Title%3A%20Boosting%20Self-Supervision%20for%20Single-View%20Scene%20Completion%20via%20Knowledge%0A%20%20Distillation%0AAuthor%3A%20Keonhee%20Han%20and%20Dominik%20Muhle%20and%20Felix%20Wimbauer%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Inferring%20scene%20geometry%20from%20images%20via%20Structure%20from%20Motion%20is%20a%0Along-standing%20and%20fundamental%20problem%20in%20computer%20vision.%20While%20classical%0Aapproaches%20and%2C%20more%20recently%2C%20depth%20map%20predictions%20only%20focus%20on%20the%20visible%0Aparts%20of%20a%20scene%2C%20the%20task%20of%20scene%20completion%20aims%20to%20reason%20about%20geometry%0Aeven%20in%20occluded%20regions.%20With%20the%20popularity%20of%20neural%20radiance%20fields%0A%28NeRFs%29%2C%20implicit%20representations%20also%20became%20popular%20for%20scene%20completion%20by%0Apredicting%20so-called%20density%20fields.%20Unlike%20explicit%20approaches.%20e.g.%0Avoxel-based%20methods%2C%20density%20fields%20also%20allow%20for%20accurate%20depth%20prediction%0Aand%20novel-view%20synthesis%20via%20image-based%20rendering.%20In%20this%20work%2C%20we%20propose%20to%0Afuse%20the%20scene%20reconstruction%20from%20multiple%20images%20and%20distill%20this%20knowledge%0Ainto%20a%20more%20accurate%20single-view%20scene%20reconstruction.%20To%20this%20end%2C%20we%20propose%0AMulti-View%20Behind%20the%20Scenes%20%28MVBTS%29%20to%20fuse%20density%20fields%20from%20multiple%20posed%0Aimages%2C%20trained%20fully%20self-supervised%20only%20from%20image%20data.%20Using%20knowledge%0Adistillation%2C%20we%20use%20MVBTS%20to%20train%20a%20single-view%20scene%20completion%20network%20via%0Adirect%20supervision%20called%20KDBTS.%20It%20achieves%20state-of-the-art%20performance%20on%0Aoccupancy%20prediction%2C%20especially%20in%20occluded%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07933v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Self-Supervision%20for%20Single-View%20Scene%20Completion%20via%20Knowledge%0A%20%20Distillation&entry.906535625=Keonhee%20Han%20and%20Dominik%20Muhle%20and%20Felix%20Wimbauer%20and%20Daniel%20Cremers&entry.1292438233=%20%20Inferring%20scene%20geometry%20from%20images%20via%20Structure%20from%20Motion%20is%20a%0Along-standing%20and%20fundamental%20problem%20in%20computer%20vision.%20While%20classical%0Aapproaches%20and%2C%20more%20recently%2C%20depth%20map%20predictions%20only%20focus%20on%20the%20visible%0Aparts%20of%20a%20scene%2C%20the%20task%20of%20scene%20completion%20aims%20to%20reason%20about%20geometry%0Aeven%20in%20occluded%20regions.%20With%20the%20popularity%20of%20neural%20radiance%20fields%0A%28NeRFs%29%2C%20implicit%20representations%20also%20became%20popular%20for%20scene%20completion%20by%0Apredicting%20so-called%20density%20fields.%20Unlike%20explicit%20approaches.%20e.g.%0Avoxel-based%20methods%2C%20density%20fields%20also%20allow%20for%20accurate%20depth%20prediction%0Aand%20novel-view%20synthesis%20via%20image-based%20rendering.%20In%20this%20work%2C%20we%20propose%20to%0Afuse%20the%20scene%20reconstruction%20from%20multiple%20images%20and%20distill%20this%20knowledge%0Ainto%20a%20more%20accurate%20single-view%20scene%20reconstruction.%20To%20this%20end%2C%20we%20propose%0AMulti-View%20Behind%20the%20Scenes%20%28MVBTS%29%20to%20fuse%20density%20fields%20from%20multiple%20posed%0Aimages%2C%20trained%20fully%20self-supervised%20only%20from%20image%20data.%20Using%20knowledge%0Adistillation%2C%20we%20use%20MVBTS%20to%20train%20a%20single-view%20scene%20completion%20network%20via%0Adirect%20supervision%20called%20KDBTS.%20It%20achieves%20state-of-the-art%20performance%20on%0Aoccupancy%20prediction%2C%20especially%20in%20occluded%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07933v1&entry.124074799=Read"},
{"title": "Safe haptic teleoperations of admittance controlled robots with\n  virtualization of the force feedback", "author": "Lorenzo Pagliara and Enrico Ferrentino and Andrea Chiacchio and Giovanni Russo", "abstract": "  Haptic teleoperations play a key role in extending human capabilities to\nperform complex tasks remotely, employing a robotic system. The impact of\nhaptics is far-reaching and can improve the sensory awareness and motor\naccuracy of the operator. In this context, a key challenge is attaining a\nnatural, stable and safe haptic human-robot interaction. Achieving these\nconflicting requirements is particularly crucial for complex procedures, e.g.\nmedical ones. To address this challenge, in this work we develop a novel haptic\nbilateral teleoperation system (HBTS), featuring a virtualized force feedback,\nbased on the motion error generated by an admittance controlled robot. This\napproach allows decoupling the force rendering system from the control of the\ninteraction: the rendered force is assigned with the desired dynamics, while\nthe admittance control parameters are separately tuned to maximize interaction\nperformance. Furthermore, recognizing the necessity to limit the forces exerted\nby the robot on the environment, to ensure a safe interaction, we embed a\nsaturation strategy of the motion references provided by the haptic device to\nadmittance control. We validate the different aspects of the proposed HBTS,\nthrough a teleoperated blackboard writing experiment, against two other\narchitectures. The results indicate that the proposed HBTS improves the\nnaturalness of teleoperation, as well as safety and accuracy of the\ninteraction.\n", "link": "http://arxiv.org/abs/2404.07672v1", "date": "2024-04-11", "relevancy": 2.1486, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.57}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5544}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4975}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Safe%20haptic%20teleoperations%20of%20admittance%20controlled%20robots%20with%0A%20%20virtualization%20of%20the%20force%20feedback&body=Title%3A%20Safe%20haptic%20teleoperations%20of%20admittance%20controlled%20robots%20with%0A%20%20virtualization%20of%20the%20force%20feedback%0AAuthor%3A%20Lorenzo%20Pagliara%20and%20Enrico%20Ferrentino%20and%20Andrea%20Chiacchio%20and%20Giovanni%20Russo%0AAbstract%3A%20%20%20Haptic%20teleoperations%20play%20a%20key%20role%20in%20extending%20human%20capabilities%20to%0Aperform%20complex%20tasks%20remotely%2C%20employing%20a%20robotic%20system.%20The%20impact%20of%0Ahaptics%20is%20far-reaching%20and%20can%20improve%20the%20sensory%20awareness%20and%20motor%0Aaccuracy%20of%20the%20operator.%20In%20this%20context%2C%20a%20key%20challenge%20is%20attaining%20a%0Anatural%2C%20stable%20and%20safe%20haptic%20human-robot%20interaction.%20Achieving%20these%0Aconflicting%20requirements%20is%20particularly%20crucial%20for%20complex%20procedures%2C%20e.g.%0Amedical%20ones.%20To%20address%20this%20challenge%2C%20in%20this%20work%20we%20develop%20a%20novel%20haptic%0Abilateral%20teleoperation%20system%20%28HBTS%29%2C%20featuring%20a%20virtualized%20force%20feedback%2C%0Abased%20on%20the%20motion%20error%20generated%20by%20an%20admittance%20controlled%20robot.%20This%0Aapproach%20allows%20decoupling%20the%20force%20rendering%20system%20from%20the%20control%20of%20the%0Ainteraction%3A%20the%20rendered%20force%20is%20assigned%20with%20the%20desired%20dynamics%2C%20while%0Athe%20admittance%20control%20parameters%20are%20separately%20tuned%20to%20maximize%20interaction%0Aperformance.%20Furthermore%2C%20recognizing%20the%20necessity%20to%20limit%20the%20forces%20exerted%0Aby%20the%20robot%20on%20the%20environment%2C%20to%20ensure%20a%20safe%20interaction%2C%20we%20embed%20a%0Asaturation%20strategy%20of%20the%20motion%20references%20provided%20by%20the%20haptic%20device%20to%0Aadmittance%20control.%20We%20validate%20the%20different%20aspects%20of%20the%20proposed%20HBTS%2C%0Athrough%20a%20teleoperated%20blackboard%20writing%20experiment%2C%20against%20two%20other%0Aarchitectures.%20The%20results%20indicate%20that%20the%20proposed%20HBTS%20improves%20the%0Anaturalness%20of%20teleoperation%2C%20as%20well%20as%20safety%20and%20accuracy%20of%20the%0Ainteraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07672v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20haptic%20teleoperations%20of%20admittance%20controlled%20robots%20with%0A%20%20virtualization%20of%20the%20force%20feedback&entry.906535625=Lorenzo%20Pagliara%20and%20Enrico%20Ferrentino%20and%20Andrea%20Chiacchio%20and%20Giovanni%20Russo&entry.1292438233=%20%20Haptic%20teleoperations%20play%20a%20key%20role%20in%20extending%20human%20capabilities%20to%0Aperform%20complex%20tasks%20remotely%2C%20employing%20a%20robotic%20system.%20The%20impact%20of%0Ahaptics%20is%20far-reaching%20and%20can%20improve%20the%20sensory%20awareness%20and%20motor%0Aaccuracy%20of%20the%20operator.%20In%20this%20context%2C%20a%20key%20challenge%20is%20attaining%20a%0Anatural%2C%20stable%20and%20safe%20haptic%20human-robot%20interaction.%20Achieving%20these%0Aconflicting%20requirements%20is%20particularly%20crucial%20for%20complex%20procedures%2C%20e.g.%0Amedical%20ones.%20To%20address%20this%20challenge%2C%20in%20this%20work%20we%20develop%20a%20novel%20haptic%0Abilateral%20teleoperation%20system%20%28HBTS%29%2C%20featuring%20a%20virtualized%20force%20feedback%2C%0Abased%20on%20the%20motion%20error%20generated%20by%20an%20admittance%20controlled%20robot.%20This%0Aapproach%20allows%20decoupling%20the%20force%20rendering%20system%20from%20the%20control%20of%20the%0Ainteraction%3A%20the%20rendered%20force%20is%20assigned%20with%20the%20desired%20dynamics%2C%20while%0Athe%20admittance%20control%20parameters%20are%20separately%20tuned%20to%20maximize%20interaction%0Aperformance.%20Furthermore%2C%20recognizing%20the%20necessity%20to%20limit%20the%20forces%20exerted%0Aby%20the%20robot%20on%20the%20environment%2C%20to%20ensure%20a%20safe%20interaction%2C%20we%20embed%20a%0Asaturation%20strategy%20of%20the%20motion%20references%20provided%20by%20the%20haptic%20device%20to%0Aadmittance%20control.%20We%20validate%20the%20different%20aspects%20of%20the%20proposed%20HBTS%2C%0Athrough%20a%20teleoperated%20blackboard%20writing%20experiment%2C%20against%20two%20other%0Aarchitectures.%20The%20results%20indicate%20that%20the%20proposed%20HBTS%20improves%20the%0Anaturalness%20of%20teleoperation%2C%20as%20well%20as%20safety%20and%20accuracy%20of%20the%0Ainteraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07672v1&entry.124074799=Read"},
{"title": "Finding Dino: A plug-and-play framework for unsupervised detection of\n  out-of-distribution objects using prototypes", "author": "Poulami Sinhamahapatra and Franziska Schwaiger and Shirsha Bose and Huiyu Wang and Karsten Roscher and Stephan Guennemann", "abstract": "  Detecting and localising unknown or Out-of-distribution (OOD) objects in any\nscene can be a challenging task in vision. Particularly, in safety-critical\ncases involving autonomous systems like automated vehicles or trains.\nSupervised anomaly segmentation or open-world object detection models depend on\ntraining on exhaustively annotated datasets for every domain and still struggle\nin distinguishing between background and OOD objects. In this work, we present\na plug-and-play generalised framework - PRototype-based zero-shot OOD detection\nWithout Labels (PROWL). It is an inference-based method that does not require\ntraining on the domain dataset and relies on extracting relevant features from\nself-supervised pre-trained models. PROWL can be easily adapted to detect OOD\nobjects in any operational design domain by specifying a list of known classes\nfrom this domain. PROWL, as an unsupervised method, outperforms other\nsupervised methods trained without auxiliary OOD data on the RoadAnomaly and\nRoadObstacle datasets provided in SegmentMeIfYouCan (SMIYC) benchmark. We also\ndemonstrate its suitability for other domains such as rail and maritime scenes.\n", "link": "http://arxiv.org/abs/2404.07664v1", "date": "2024-04-11", "relevancy": 2.1486, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5459}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5353}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5291}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Finding%20Dino%3A%20A%20plug-and-play%20framework%20for%20unsupervised%20detection%20of%0A%20%20out-of-distribution%20objects%20using%20prototypes&body=Title%3A%20Finding%20Dino%3A%20A%20plug-and-play%20framework%20for%20unsupervised%20detection%20of%0A%20%20out-of-distribution%20objects%20using%20prototypes%0AAuthor%3A%20Poulami%20Sinhamahapatra%20and%20Franziska%20Schwaiger%20and%20Shirsha%20Bose%20and%20Huiyu%20Wang%20and%20Karsten%20Roscher%20and%20Stephan%20Guennemann%0AAbstract%3A%20%20%20Detecting%20and%20localising%20unknown%20or%20Out-of-distribution%20%28OOD%29%20objects%20in%20any%0Ascene%20can%20be%20a%20challenging%20task%20in%20vision.%20Particularly%2C%20in%20safety-critical%0Acases%20involving%20autonomous%20systems%20like%20automated%20vehicles%20or%20trains.%0ASupervised%20anomaly%20segmentation%20or%20open-world%20object%20detection%20models%20depend%20on%0Atraining%20on%20exhaustively%20annotated%20datasets%20for%20every%20domain%20and%20still%20struggle%0Ain%20distinguishing%20between%20background%20and%20OOD%20objects.%20In%20this%20work%2C%20we%20present%0Aa%20plug-and-play%20generalised%20framework%20-%20PRototype-based%20zero-shot%20OOD%20detection%0AWithout%20Labels%20%28PROWL%29.%20It%20is%20an%20inference-based%20method%20that%20does%20not%20require%0Atraining%20on%20the%20domain%20dataset%20and%20relies%20on%20extracting%20relevant%20features%20from%0Aself-supervised%20pre-trained%20models.%20PROWL%20can%20be%20easily%20adapted%20to%20detect%20OOD%0Aobjects%20in%20any%20operational%20design%20domain%20by%20specifying%20a%20list%20of%20known%20classes%0Afrom%20this%20domain.%20PROWL%2C%20as%20an%20unsupervised%20method%2C%20outperforms%20other%0Asupervised%20methods%20trained%20without%20auxiliary%20OOD%20data%20on%20the%20RoadAnomaly%20and%0ARoadObstacle%20datasets%20provided%20in%20SegmentMeIfYouCan%20%28SMIYC%29%20benchmark.%20We%20also%0Ademonstrate%20its%20suitability%20for%20other%20domains%20such%20as%20rail%20and%20maritime%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07664v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20Dino%3A%20A%20plug-and-play%20framework%20for%20unsupervised%20detection%20of%0A%20%20out-of-distribution%20objects%20using%20prototypes&entry.906535625=Poulami%20Sinhamahapatra%20and%20Franziska%20Schwaiger%20and%20Shirsha%20Bose%20and%20Huiyu%20Wang%20and%20Karsten%20Roscher%20and%20Stephan%20Guennemann&entry.1292438233=%20%20Detecting%20and%20localising%20unknown%20or%20Out-of-distribution%20%28OOD%29%20objects%20in%20any%0Ascene%20can%20be%20a%20challenging%20task%20in%20vision.%20Particularly%2C%20in%20safety-critical%0Acases%20involving%20autonomous%20systems%20like%20automated%20vehicles%20or%20trains.%0ASupervised%20anomaly%20segmentation%20or%20open-world%20object%20detection%20models%20depend%20on%0Atraining%20on%20exhaustively%20annotated%20datasets%20for%20every%20domain%20and%20still%20struggle%0Ain%20distinguishing%20between%20background%20and%20OOD%20objects.%20In%20this%20work%2C%20we%20present%0Aa%20plug-and-play%20generalised%20framework%20-%20PRototype-based%20zero-shot%20OOD%20detection%0AWithout%20Labels%20%28PROWL%29.%20It%20is%20an%20inference-based%20method%20that%20does%20not%20require%0Atraining%20on%20the%20domain%20dataset%20and%20relies%20on%20extracting%20relevant%20features%20from%0Aself-supervised%20pre-trained%20models.%20PROWL%20can%20be%20easily%20adapted%20to%20detect%20OOD%0Aobjects%20in%20any%20operational%20design%20domain%20by%20specifying%20a%20list%20of%20known%20classes%0Afrom%20this%20domain.%20PROWL%2C%20as%20an%20unsupervised%20method%2C%20outperforms%20other%0Asupervised%20methods%20trained%20without%20auxiliary%20OOD%20data%20on%20the%20RoadAnomaly%20and%0ARoadObstacle%20datasets%20provided%20in%20SegmentMeIfYouCan%20%28SMIYC%29%20benchmark.%20We%20also%0Ademonstrate%20its%20suitability%20for%20other%20domains%20such%20as%20rail%20and%20maritime%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07664v1&entry.124074799=Read"},
{"title": "Bi-level Trajectory Optimization on Uneven Terrains with Differentiable\n  Wheel-Terrain Interaction Model", "author": "Amith Manoharan and Aditya Sharma and Himani Belsare and Kaustab Pal and K. Madhava Krishna and Arun Kumar Singh", "abstract": "  Navigation of wheeled vehicles on uneven terrain necessitates going beyond\nthe 2D approaches for trajectory planning. Specifically, it is essential to\nincorporate the full 6dof variation of vehicle pose and its associated\nstability cost in the planning process. To this end, most recent works aim to\nlearn a neural network model to predict the vehicle evolution. However, such\napproaches are data-intensive and fraught with generalization issues. In this\npaper, we present a purely model-based approach that just requires the digital\nelevation information of the terrain. Specifically, we express the\nwheel-terrain interaction and 6dof pose prediction as a non-linear least\nsquares (NLS) problem. As a result, trajectory planning can be viewed as a\nbi-level optimization. The inner optimization layer predicts the pose on the\nterrain along a given trajectory, while the outer layer deforms the trajectory\nitself to reduce the stability and kinematic costs of the pose. We improve the\nstate-of-the-art in the following respects. First, we show that our NLS based\npose prediction closely matches the output from a high-fidelity physics engine.\nThis result coupled with the fact that we can query gradients of the NLS\nsolver, makes our pose predictor, a differentiable wheel-terrain interaction\nmodel. We further leverage this differentiability to efficiently solve the\nproposed bi-level trajectory optimization problem. Finally, we perform\nextensive experiments, and comparison with a baseline to showcase the\neffectiveness of our approach in obtaining smooth, stable trajectories.\n", "link": "http://arxiv.org/abs/2404.03307v2", "date": "2024-04-11", "relevancy": 2.1404, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5798}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4911}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bi-level%20Trajectory%20Optimization%20on%20Uneven%20Terrains%20with%20Differentiable%0A%20%20Wheel-Terrain%20Interaction%20Model&body=Title%3A%20Bi-level%20Trajectory%20Optimization%20on%20Uneven%20Terrains%20with%20Differentiable%0A%20%20Wheel-Terrain%20Interaction%20Model%0AAuthor%3A%20Amith%20Manoharan%20and%20Aditya%20Sharma%20and%20Himani%20Belsare%20and%20Kaustab%20Pal%20and%20K.%20Madhava%20Krishna%20and%20Arun%20Kumar%20Singh%0AAbstract%3A%20%20%20Navigation%20of%20wheeled%20vehicles%20on%20uneven%20terrain%20necessitates%20going%20beyond%0Athe%202D%20approaches%20for%20trajectory%20planning.%20Specifically%2C%20it%20is%20essential%20to%0Aincorporate%20the%20full%206dof%20variation%20of%20vehicle%20pose%20and%20its%20associated%0Astability%20cost%20in%20the%20planning%20process.%20To%20this%20end%2C%20most%20recent%20works%20aim%20to%0Alearn%20a%20neural%20network%20model%20to%20predict%20the%20vehicle%20evolution.%20However%2C%20such%0Aapproaches%20are%20data-intensive%20and%20fraught%20with%20generalization%20issues.%20In%20this%0Apaper%2C%20we%20present%20a%20purely%20model-based%20approach%20that%20just%20requires%20the%20digital%0Aelevation%20information%20of%20the%20terrain.%20Specifically%2C%20we%20express%20the%0Awheel-terrain%20interaction%20and%206dof%20pose%20prediction%20as%20a%20non-linear%20least%0Asquares%20%28NLS%29%20problem.%20As%20a%20result%2C%20trajectory%20planning%20can%20be%20viewed%20as%20a%0Abi-level%20optimization.%20The%20inner%20optimization%20layer%20predicts%20the%20pose%20on%20the%0Aterrain%20along%20a%20given%20trajectory%2C%20while%20the%20outer%20layer%20deforms%20the%20trajectory%0Aitself%20to%20reduce%20the%20stability%20and%20kinematic%20costs%20of%20the%20pose.%20We%20improve%20the%0Astate-of-the-art%20in%20the%20following%20respects.%20First%2C%20we%20show%20that%20our%20NLS%20based%0Apose%20prediction%20closely%20matches%20the%20output%20from%20a%20high-fidelity%20physics%20engine.%0AThis%20result%20coupled%20with%20the%20fact%20that%20we%20can%20query%20gradients%20of%20the%20NLS%0Asolver%2C%20makes%20our%20pose%20predictor%2C%20a%20differentiable%20wheel-terrain%20interaction%0Amodel.%20We%20further%20leverage%20this%20differentiability%20to%20efficiently%20solve%20the%0Aproposed%20bi-level%20trajectory%20optimization%20problem.%20Finally%2C%20we%20perform%0Aextensive%20experiments%2C%20and%20comparison%20with%20a%20baseline%20to%20showcase%20the%0Aeffectiveness%20of%20our%20approach%20in%20obtaining%20smooth%2C%20stable%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03307v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-level%20Trajectory%20Optimization%20on%20Uneven%20Terrains%20with%20Differentiable%0A%20%20Wheel-Terrain%20Interaction%20Model&entry.906535625=Amith%20Manoharan%20and%20Aditya%20Sharma%20and%20Himani%20Belsare%20and%20Kaustab%20Pal%20and%20K.%20Madhava%20Krishna%20and%20Arun%20Kumar%20Singh&entry.1292438233=%20%20Navigation%20of%20wheeled%20vehicles%20on%20uneven%20terrain%20necessitates%20going%20beyond%0Athe%202D%20approaches%20for%20trajectory%20planning.%20Specifically%2C%20it%20is%20essential%20to%0Aincorporate%20the%20full%206dof%20variation%20of%20vehicle%20pose%20and%20its%20associated%0Astability%20cost%20in%20the%20planning%20process.%20To%20this%20end%2C%20most%20recent%20works%20aim%20to%0Alearn%20a%20neural%20network%20model%20to%20predict%20the%20vehicle%20evolution.%20However%2C%20such%0Aapproaches%20are%20data-intensive%20and%20fraught%20with%20generalization%20issues.%20In%20this%0Apaper%2C%20we%20present%20a%20purely%20model-based%20approach%20that%20just%20requires%20the%20digital%0Aelevation%20information%20of%20the%20terrain.%20Specifically%2C%20we%20express%20the%0Awheel-terrain%20interaction%20and%206dof%20pose%20prediction%20as%20a%20non-linear%20least%0Asquares%20%28NLS%29%20problem.%20As%20a%20result%2C%20trajectory%20planning%20can%20be%20viewed%20as%20a%0Abi-level%20optimization.%20The%20inner%20optimization%20layer%20predicts%20the%20pose%20on%20the%0Aterrain%20along%20a%20given%20trajectory%2C%20while%20the%20outer%20layer%20deforms%20the%20trajectory%0Aitself%20to%20reduce%20the%20stability%20and%20kinematic%20costs%20of%20the%20pose.%20We%20improve%20the%0Astate-of-the-art%20in%20the%20following%20respects.%20First%2C%20we%20show%20that%20our%20NLS%20based%0Apose%20prediction%20closely%20matches%20the%20output%20from%20a%20high-fidelity%20physics%20engine.%0AThis%20result%20coupled%20with%20the%20fact%20that%20we%20can%20query%20gradients%20of%20the%20NLS%0Asolver%2C%20makes%20our%20pose%20predictor%2C%20a%20differentiable%20wheel-terrain%20interaction%0Amodel.%20We%20further%20leverage%20this%20differentiability%20to%20efficiently%20solve%20the%0Aproposed%20bi-level%20trajectory%20optimization%20problem.%20Finally%2C%20we%20perform%0Aextensive%20experiments%2C%20and%20comparison%20with%20a%20baseline%20to%20showcase%20the%0Aeffectiveness%20of%20our%20approach%20in%20obtaining%20smooth%2C%20stable%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03307v2&entry.124074799=Read"},
{"title": "Context-aware Video Anomaly Detection in Long-Term Datasets", "author": "Zhengye Yang and Richard Radke", "abstract": "  Video anomaly detection research is generally evaluated on short, isolated\nbenchmark videos only a few minutes long. However, in real-world environments,\nsecurity cameras observe the same scene for months or years at a time, and the\nnotion of anomalous behavior critically depends on context, such as the time of\nday, day of week, or schedule of events. Here, we propose a context-aware video\nanomaly detection algorithm, Trinity, specifically targeted to these scenarios.\nTrinity is especially well-suited to crowded scenes in which individuals cannot\nbe easily tracked, and anomalies are due to speed, direction, or absence of\ngroup motion. Trinity is a contrastive learning framework that aims to learn\nalignments between context, appearance, and motion, and uses alignment quality\nto classify videos as normal or anomalous. We evaluate our algorithm on both\nconventional benchmarks and a public webcam-based dataset we collected that\nspans more than three months of activity.\n", "link": "http://arxiv.org/abs/2404.07887v1", "date": "2024-04-11", "relevancy": 2.1377, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5499}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5431}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Context-aware%20Video%20Anomaly%20Detection%20in%20Long-Term%20Datasets&body=Title%3A%20Context-aware%20Video%20Anomaly%20Detection%20in%20Long-Term%20Datasets%0AAuthor%3A%20Zhengye%20Yang%20and%20Richard%20Radke%0AAbstract%3A%20%20%20Video%20anomaly%20detection%20research%20is%20generally%20evaluated%20on%20short%2C%20isolated%0Abenchmark%20videos%20only%20a%20few%20minutes%20long.%20However%2C%20in%20real-world%20environments%2C%0Asecurity%20cameras%20observe%20the%20same%20scene%20for%20months%20or%20years%20at%20a%20time%2C%20and%20the%0Anotion%20of%20anomalous%20behavior%20critically%20depends%20on%20context%2C%20such%20as%20the%20time%20of%0Aday%2C%20day%20of%20week%2C%20or%20schedule%20of%20events.%20Here%2C%20we%20propose%20a%20context-aware%20video%0Aanomaly%20detection%20algorithm%2C%20Trinity%2C%20specifically%20targeted%20to%20these%20scenarios.%0ATrinity%20is%20especially%20well-suited%20to%20crowded%20scenes%20in%20which%20individuals%20cannot%0Abe%20easily%20tracked%2C%20and%20anomalies%20are%20due%20to%20speed%2C%20direction%2C%20or%20absence%20of%0Agroup%20motion.%20Trinity%20is%20a%20contrastive%20learning%20framework%20that%20aims%20to%20learn%0Aalignments%20between%20context%2C%20appearance%2C%20and%20motion%2C%20and%20uses%20alignment%20quality%0Ato%20classify%20videos%20as%20normal%20or%20anomalous.%20We%20evaluate%20our%20algorithm%20on%20both%0Aconventional%20benchmarks%20and%20a%20public%20webcam-based%20dataset%20we%20collected%20that%0Aspans%20more%20than%20three%20months%20of%20activity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07887v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-aware%20Video%20Anomaly%20Detection%20in%20Long-Term%20Datasets&entry.906535625=Zhengye%20Yang%20and%20Richard%20Radke&entry.1292438233=%20%20Video%20anomaly%20detection%20research%20is%20generally%20evaluated%20on%20short%2C%20isolated%0Abenchmark%20videos%20only%20a%20few%20minutes%20long.%20However%2C%20in%20real-world%20environments%2C%0Asecurity%20cameras%20observe%20the%20same%20scene%20for%20months%20or%20years%20at%20a%20time%2C%20and%20the%0Anotion%20of%20anomalous%20behavior%20critically%20depends%20on%20context%2C%20such%20as%20the%20time%20of%0Aday%2C%20day%20of%20week%2C%20or%20schedule%20of%20events.%20Here%2C%20we%20propose%20a%20context-aware%20video%0Aanomaly%20detection%20algorithm%2C%20Trinity%2C%20specifically%20targeted%20to%20these%20scenarios.%0ATrinity%20is%20especially%20well-suited%20to%20crowded%20scenes%20in%20which%20individuals%20cannot%0Abe%20easily%20tracked%2C%20and%20anomalies%20are%20due%20to%20speed%2C%20direction%2C%20or%20absence%20of%0Agroup%20motion.%20Trinity%20is%20a%20contrastive%20learning%20framework%20that%20aims%20to%20learn%0Aalignments%20between%20context%2C%20appearance%2C%20and%20motion%2C%20and%20uses%20alignment%20quality%0Ato%20classify%20videos%20as%20normal%20or%20anomalous.%20We%20evaluate%20our%20algorithm%20on%20both%0Aconventional%20benchmarks%20and%20a%20public%20webcam-based%20dataset%20we%20collected%20that%0Aspans%20more%20than%20three%20months%20of%20activity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07887v1&entry.124074799=Read"},
{"title": "Do You Remember? Dense Video Captioning with Cross-Modal Memory\n  Retrieval", "author": "Minkuk Kim and Hyeon Bae Kim and Jinyoung Moon and Jinwoo Choi and Seong Tae Kim", "abstract": "  There has been significant attention to the research on dense video\ncaptioning, which aims to automatically localize and caption all events within\nuntrimmed video. Several studies introduce methods by designing dense video\ncaptioning as a multitasking problem of event localization and event captioning\nto consider inter-task relations. However, addressing both tasks using only\nvisual input is challenging due to the lack of semantic content. In this study,\nwe address this by proposing a novel framework inspired by the cognitive\ninformation processing of humans. Our model utilizes external memory to\nincorporate prior knowledge. The memory retrieval method is proposed with\ncross-modal video-to-text matching. To effectively incorporate retrieved text\nfeatures, the versatile encoder and the decoder with visual and textual\ncross-attention modules are designed. Comparative experiments have been\nconducted to show the effectiveness of the proposed method on ActivityNet\nCaptions and YouCook2 datasets. Experimental results show promising performance\nof our model without extensive pretraining from a large video dataset.\n", "link": "http://arxiv.org/abs/2404.07610v1", "date": "2024-04-11", "relevancy": 2.1329, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.539}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5337}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Do%20You%20Remember%3F%20Dense%20Video%20Captioning%20with%20Cross-Modal%20Memory%0A%20%20Retrieval&body=Title%3A%20Do%20You%20Remember%3F%20Dense%20Video%20Captioning%20with%20Cross-Modal%20Memory%0A%20%20Retrieval%0AAuthor%3A%20Minkuk%20Kim%20and%20Hyeon%20Bae%20Kim%20and%20Jinyoung%20Moon%20and%20Jinwoo%20Choi%20and%20Seong%20Tae%20Kim%0AAbstract%3A%20%20%20There%20has%20been%20significant%20attention%20to%20the%20research%20on%20dense%20video%0Acaptioning%2C%20which%20aims%20to%20automatically%20localize%20and%20caption%20all%20events%20within%0Auntrimmed%20video.%20Several%20studies%20introduce%20methods%20by%20designing%20dense%20video%0Acaptioning%20as%20a%20multitasking%20problem%20of%20event%20localization%20and%20event%20captioning%0Ato%20consider%20inter-task%20relations.%20However%2C%20addressing%20both%20tasks%20using%20only%0Avisual%20input%20is%20challenging%20due%20to%20the%20lack%20of%20semantic%20content.%20In%20this%20study%2C%0Awe%20address%20this%20by%20proposing%20a%20novel%20framework%20inspired%20by%20the%20cognitive%0Ainformation%20processing%20of%20humans.%20Our%20model%20utilizes%20external%20memory%20to%0Aincorporate%20prior%20knowledge.%20The%20memory%20retrieval%20method%20is%20proposed%20with%0Across-modal%20video-to-text%20matching.%20To%20effectively%20incorporate%20retrieved%20text%0Afeatures%2C%20the%20versatile%20encoder%20and%20the%20decoder%20with%20visual%20and%20textual%0Across-attention%20modules%20are%20designed.%20Comparative%20experiments%20have%20been%0Aconducted%20to%20show%20the%20effectiveness%20of%20the%20proposed%20method%20on%20ActivityNet%0ACaptions%20and%20YouCook2%20datasets.%20Experimental%20results%20show%20promising%20performance%0Aof%20our%20model%20without%20extensive%20pretraining%20from%20a%20large%20video%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07610v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20You%20Remember%3F%20Dense%20Video%20Captioning%20with%20Cross-Modal%20Memory%0A%20%20Retrieval&entry.906535625=Minkuk%20Kim%20and%20Hyeon%20Bae%20Kim%20and%20Jinyoung%20Moon%20and%20Jinwoo%20Choi%20and%20Seong%20Tae%20Kim&entry.1292438233=%20%20There%20has%20been%20significant%20attention%20to%20the%20research%20on%20dense%20video%0Acaptioning%2C%20which%20aims%20to%20automatically%20localize%20and%20caption%20all%20events%20within%0Auntrimmed%20video.%20Several%20studies%20introduce%20methods%20by%20designing%20dense%20video%0Acaptioning%20as%20a%20multitasking%20problem%20of%20event%20localization%20and%20event%20captioning%0Ato%20consider%20inter-task%20relations.%20However%2C%20addressing%20both%20tasks%20using%20only%0Avisual%20input%20is%20challenging%20due%20to%20the%20lack%20of%20semantic%20content.%20In%20this%20study%2C%0Awe%20address%20this%20by%20proposing%20a%20novel%20framework%20inspired%20by%20the%20cognitive%0Ainformation%20processing%20of%20humans.%20Our%20model%20utilizes%20external%20memory%20to%0Aincorporate%20prior%20knowledge.%20The%20memory%20retrieval%20method%20is%20proposed%20with%0Across-modal%20video-to-text%20matching.%20To%20effectively%20incorporate%20retrieved%20text%0Afeatures%2C%20the%20versatile%20encoder%20and%20the%20decoder%20with%20visual%20and%20textual%0Across-attention%20modules%20are%20designed.%20Comparative%20experiments%20have%20been%0Aconducted%20to%20show%20the%20effectiveness%20of%20the%20proposed%20method%20on%20ActivityNet%0ACaptions%20and%20YouCook2%20datasets.%20Experimental%20results%20show%20promising%20performance%0Aof%20our%20model%20without%20extensive%20pretraining%20from%20a%20large%20video%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07610v1&entry.124074799=Read"},
{"title": "NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous\n  Driving", "author": "William Ljungbergh and Adam Tonderski and Joakim Johnander and Holger Caesar and Kalle \u00c5str\u00f6m and Michael Felsberg and Christoffer Petersson", "abstract": "  We present a versatile NeRF-based simulator for testing autonomous driving\n(AD) software systems, designed with a focus on sensor-realistic closed-loop\nevaluation and the creation of safety-critical scenarios. The simulator learns\nfrom sequences of real-world driving sensor data and enables reconfigurations\nand renderings of new, unseen scenarios. In this work, we use our simulator to\ntest the responses of AD models to safety-critical scenarios inspired by the\nEuropean New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,\nwhile state-of-the-art end-to-end planners excel in nominal driving scenarios\nin an open-loop setting, they exhibit critical flaws when navigating our\nsafety-critical scenarios in a closed-loop setting. This highlights the need\nfor advancements in the safety and real-world usability of end-to-end planners.\nBy publicly releasing our simulator and scenarios as an easy-to-run evaluation\nsuite, we invite the research community to explore, refine, and validate their\nAD models in controlled, yet highly configurable and challenging\nsensor-realistic environments. Code and instructions can be found at\nhttps://github.com/wljungbergh/NeuroNCAP\n", "link": "http://arxiv.org/abs/2404.07762v1", "date": "2024-04-11", "relevancy": 2.1247, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5367}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5304}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5298}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NeuroNCAP%3A%20Photorealistic%20Closed-loop%20Safety%20Testing%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20NeuroNCAP%3A%20Photorealistic%20Closed-loop%20Safety%20Testing%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20William%20Ljungbergh%20and%20Adam%20Tonderski%20and%20Joakim%20Johnander%20and%20Holger%20Caesar%20and%20Kalle%20%C3%85str%C3%B6m%20and%20Michael%20Felsberg%20and%20Christoffer%20Petersson%0AAbstract%3A%20%20%20We%20present%20a%20versatile%20NeRF-based%20simulator%20for%20testing%20autonomous%20driving%0A%28AD%29%20software%20systems%2C%20designed%20with%20a%20focus%20on%20sensor-realistic%20closed-loop%0Aevaluation%20and%20the%20creation%20of%20safety-critical%20scenarios.%20The%20simulator%20learns%0Afrom%20sequences%20of%20real-world%20driving%20sensor%20data%20and%20enables%20reconfigurations%0Aand%20renderings%20of%20new%2C%20unseen%20scenarios.%20In%20this%20work%2C%20we%20use%20our%20simulator%20to%0Atest%20the%20responses%20of%20AD%20models%20to%20safety-critical%20scenarios%20inspired%20by%20the%0AEuropean%20New%20Car%20Assessment%20Programme%20%28Euro%20NCAP%29.%20Our%20evaluation%20reveals%20that%2C%0Awhile%20state-of-the-art%20end-to-end%20planners%20excel%20in%20nominal%20driving%20scenarios%0Ain%20an%20open-loop%20setting%2C%20they%20exhibit%20critical%20flaws%20when%20navigating%20our%0Asafety-critical%20scenarios%20in%20a%20closed-loop%20setting.%20This%20highlights%20the%20need%0Afor%20advancements%20in%20the%20safety%20and%20real-world%20usability%20of%20end-to-end%20planners.%0ABy%20publicly%20releasing%20our%20simulator%20and%20scenarios%20as%20an%20easy-to-run%20evaluation%0Asuite%2C%20we%20invite%20the%20research%20community%20to%20explore%2C%20refine%2C%20and%20validate%20their%0AAD%20models%20in%20controlled%2C%20yet%20highly%20configurable%20and%20challenging%0Asensor-realistic%20environments.%20Code%20and%20instructions%20can%20be%20found%20at%0Ahttps%3A//github.com/wljungbergh/NeuroNCAP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07762v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroNCAP%3A%20Photorealistic%20Closed-loop%20Safety%20Testing%20for%20Autonomous%0A%20%20Driving&entry.906535625=William%20Ljungbergh%20and%20Adam%20Tonderski%20and%20Joakim%20Johnander%20and%20Holger%20Caesar%20and%20Kalle%20%C3%85str%C3%B6m%20and%20Michael%20Felsberg%20and%20Christoffer%20Petersson&entry.1292438233=%20%20We%20present%20a%20versatile%20NeRF-based%20simulator%20for%20testing%20autonomous%20driving%0A%28AD%29%20software%20systems%2C%20designed%20with%20a%20focus%20on%20sensor-realistic%20closed-loop%0Aevaluation%20and%20the%20creation%20of%20safety-critical%20scenarios.%20The%20simulator%20learns%0Afrom%20sequences%20of%20real-world%20driving%20sensor%20data%20and%20enables%20reconfigurations%0Aand%20renderings%20of%20new%2C%20unseen%20scenarios.%20In%20this%20work%2C%20we%20use%20our%20simulator%20to%0Atest%20the%20responses%20of%20AD%20models%20to%20safety-critical%20scenarios%20inspired%20by%20the%0AEuropean%20New%20Car%20Assessment%20Programme%20%28Euro%20NCAP%29.%20Our%20evaluation%20reveals%20that%2C%0Awhile%20state-of-the-art%20end-to-end%20planners%20excel%20in%20nominal%20driving%20scenarios%0Ain%20an%20open-loop%20setting%2C%20they%20exhibit%20critical%20flaws%20when%20navigating%20our%0Asafety-critical%20scenarios%20in%20a%20closed-loop%20setting.%20This%20highlights%20the%20need%0Afor%20advancements%20in%20the%20safety%20and%20real-world%20usability%20of%20end-to-end%20planners.%0ABy%20publicly%20releasing%20our%20simulator%20and%20scenarios%20as%20an%20easy-to-run%20evaluation%0Asuite%2C%20we%20invite%20the%20research%20community%20to%20explore%2C%20refine%2C%20and%20validate%20their%0AAD%20models%20in%20controlled%2C%20yet%20highly%20configurable%20and%20challenging%0Asensor-realistic%20environments.%20Code%20and%20instructions%20can%20be%20found%20at%0Ahttps%3A//github.com/wljungbergh/NeuroNCAP%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07762v1&entry.124074799=Read"},
{"title": "Exploiting Object-based and Segmentation-based Semantic Features for\n  Deep Learning-based Indoor Scene Classification", "author": "Ricardo Pereira and Lu\u00eds Garrote and Tiago Barros and Ana Lopes and Urbano J. Nunes", "abstract": "  Indoor scenes are usually characterized by scattered objects and their\nrelationships, which turns the indoor scene classification task into a\nchallenging computer vision task. Despite the significant performance boost in\nclassification tasks achieved in recent years, provided by the use of\ndeep-learning-based methods, limitations such as inter-category ambiguity and\nintra-category variation have been holding back their performance. To overcome\nsuch issues, gathering semantic information has been shown to be a promising\nsource of information towards a more complete and discriminative feature\nrepresentation of indoor scenes. Therefore, the work described in this paper\nuses both semantic information, obtained from object detection, and semantic\nsegmentation techniques. While object detection techniques provide the 2D\nlocation of objects allowing to obtain spatial distributions between objects,\nsemantic segmentation techniques provide pixel-level information that allows to\nobtain, at a pixel-level, a spatial distribution and shape-related features of\nthe segmentation categories. Hence, a novel approach that uses a semantic\nsegmentation mask to provide Hu-moments-based segmentation categories' shape\ncharacterization, designated by Segmentation-based Hu-Moments Features (SHMFs),\nis proposed. Moreover, a three-main-branch network, designated by\nGOS$^2$F$^2$App, that exploits deep-learning-based global features,\nobject-based features, and semantic segmentation-based features is also\nproposed. GOS$^2$F$^2$App was evaluated in two indoor scene benchmark datasets:\nSUN RGB-D and NYU Depth V2, where, to the best of our knowledge,\nstate-of-the-art results were achieved on both datasets, which present\nevidences of the effectiveness of the proposed approach.\n", "link": "http://arxiv.org/abs/2404.07739v1", "date": "2024-04-11", "relevancy": 2.1221, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5577}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Object-based%20and%20Segmentation-based%20Semantic%20Features%20for%0A%20%20Deep%20Learning-based%20Indoor%20Scene%20Classification&body=Title%3A%20Exploiting%20Object-based%20and%20Segmentation-based%20Semantic%20Features%20for%0A%20%20Deep%20Learning-based%20Indoor%20Scene%20Classification%0AAuthor%3A%20Ricardo%20Pereira%20and%20Lu%C3%ADs%20Garrote%20and%20Tiago%20Barros%20and%20Ana%20Lopes%20and%20Urbano%20J.%20Nunes%0AAbstract%3A%20%20%20Indoor%20scenes%20are%20usually%20characterized%20by%20scattered%20objects%20and%20their%0Arelationships%2C%20which%20turns%20the%20indoor%20scene%20classification%20task%20into%20a%0Achallenging%20computer%20vision%20task.%20Despite%20the%20significant%20performance%20boost%20in%0Aclassification%20tasks%20achieved%20in%20recent%20years%2C%20provided%20by%20the%20use%20of%0Adeep-learning-based%20methods%2C%20limitations%20such%20as%20inter-category%20ambiguity%20and%0Aintra-category%20variation%20have%20been%20holding%20back%20their%20performance.%20To%20overcome%0Asuch%20issues%2C%20gathering%20semantic%20information%20has%20been%20shown%20to%20be%20a%20promising%0Asource%20of%20information%20towards%20a%20more%20complete%20and%20discriminative%20feature%0Arepresentation%20of%20indoor%20scenes.%20Therefore%2C%20the%20work%20described%20in%20this%20paper%0Auses%20both%20semantic%20information%2C%20obtained%20from%20object%20detection%2C%20and%20semantic%0Asegmentation%20techniques.%20While%20object%20detection%20techniques%20provide%20the%202D%0Alocation%20of%20objects%20allowing%20to%20obtain%20spatial%20distributions%20between%20objects%2C%0Asemantic%20segmentation%20techniques%20provide%20pixel-level%20information%20that%20allows%20to%0Aobtain%2C%20at%20a%20pixel-level%2C%20a%20spatial%20distribution%20and%20shape-related%20features%20of%0Athe%20segmentation%20categories.%20Hence%2C%20a%20novel%20approach%20that%20uses%20a%20semantic%0Asegmentation%20mask%20to%20provide%20Hu-moments-based%20segmentation%20categories%27%20shape%0Acharacterization%2C%20designated%20by%20Segmentation-based%20Hu-Moments%20Features%20%28SHMFs%29%2C%0Ais%20proposed.%20Moreover%2C%20a%20three-main-branch%20network%2C%20designated%20by%0AGOS%24%5E2%24F%24%5E2%24App%2C%20that%20exploits%20deep-learning-based%20global%20features%2C%0Aobject-based%20features%2C%20and%20semantic%20segmentation-based%20features%20is%20also%0Aproposed.%20GOS%24%5E2%24F%24%5E2%24App%20was%20evaluated%20in%20two%20indoor%20scene%20benchmark%20datasets%3A%0ASUN%20RGB-D%20and%20NYU%20Depth%20V2%2C%20where%2C%20to%20the%20best%20of%20our%20knowledge%2C%0Astate-of-the-art%20results%20were%20achieved%20on%20both%20datasets%2C%20which%20present%0Aevidences%20of%20the%20effectiveness%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07739v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Object-based%20and%20Segmentation-based%20Semantic%20Features%20for%0A%20%20Deep%20Learning-based%20Indoor%20Scene%20Classification&entry.906535625=Ricardo%20Pereira%20and%20Lu%C3%ADs%20Garrote%20and%20Tiago%20Barros%20and%20Ana%20Lopes%20and%20Urbano%20J.%20Nunes&entry.1292438233=%20%20Indoor%20scenes%20are%20usually%20characterized%20by%20scattered%20objects%20and%20their%0Arelationships%2C%20which%20turns%20the%20indoor%20scene%20classification%20task%20into%20a%0Achallenging%20computer%20vision%20task.%20Despite%20the%20significant%20performance%20boost%20in%0Aclassification%20tasks%20achieved%20in%20recent%20years%2C%20provided%20by%20the%20use%20of%0Adeep-learning-based%20methods%2C%20limitations%20such%20as%20inter-category%20ambiguity%20and%0Aintra-category%20variation%20have%20been%20holding%20back%20their%20performance.%20To%20overcome%0Asuch%20issues%2C%20gathering%20semantic%20information%20has%20been%20shown%20to%20be%20a%20promising%0Asource%20of%20information%20towards%20a%20more%20complete%20and%20discriminative%20feature%0Arepresentation%20of%20indoor%20scenes.%20Therefore%2C%20the%20work%20described%20in%20this%20paper%0Auses%20both%20semantic%20information%2C%20obtained%20from%20object%20detection%2C%20and%20semantic%0Asegmentation%20techniques.%20While%20object%20detection%20techniques%20provide%20the%202D%0Alocation%20of%20objects%20allowing%20to%20obtain%20spatial%20distributions%20between%20objects%2C%0Asemantic%20segmentation%20techniques%20provide%20pixel-level%20information%20that%20allows%20to%0Aobtain%2C%20at%20a%20pixel-level%2C%20a%20spatial%20distribution%20and%20shape-related%20features%20of%0Athe%20segmentation%20categories.%20Hence%2C%20a%20novel%20approach%20that%20uses%20a%20semantic%0Asegmentation%20mask%20to%20provide%20Hu-moments-based%20segmentation%20categories%27%20shape%0Acharacterization%2C%20designated%20by%20Segmentation-based%20Hu-Moments%20Features%20%28SHMFs%29%2C%0Ais%20proposed.%20Moreover%2C%20a%20three-main-branch%20network%2C%20designated%20by%0AGOS%24%5E2%24F%24%5E2%24App%2C%20that%20exploits%20deep-learning-based%20global%20features%2C%0Aobject-based%20features%2C%20and%20semantic%20segmentation-based%20features%20is%20also%0Aproposed.%20GOS%24%5E2%24F%24%5E2%24App%20was%20evaluated%20in%20two%20indoor%20scene%20benchmark%20datasets%3A%0ASUN%20RGB-D%20and%20NYU%20Depth%20V2%2C%20where%2C%20to%20the%20best%20of%20our%20knowledge%2C%0Astate-of-the-art%20results%20were%20achieved%20on%20both%20datasets%2C%20which%20present%0Aevidences%20of%20the%20effectiveness%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07739v1&entry.124074799=Read"},
{"title": "GoMAvatar: Efficient Animatable Human Modeling from Monocular Video\n  Using Gaussians-on-Mesh", "author": "Jing Wen and Xiaoming Zhao and Zhongzheng Ren and Alexander G. Schwing and Shenlong Wang", "abstract": "  We introduce GoMAvatar, a novel approach for real-time, memory-efficient,\nhigh-quality animatable human modeling. GoMAvatar takes as input a single\nmonocular video to create a digital avatar capable of re-articulation in new\nposes and real-time rendering from novel viewpoints, while seamlessly\nintegrating with rasterization-based graphics pipelines. Central to our method\nis the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering\nquality and speed of Gaussian splatting with geometry modeling and\ncompatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and\nvarious YouTube videos. GoMAvatar matches or surpasses current monocular human\nmodeling algorithms in rendering quality and significantly outperforms them in\ncomputational efficiency (43 FPS) while being memory-efficient (3.63 MB per\nsubject).\n", "link": "http://arxiv.org/abs/2404.07991v1", "date": "2024-04-11", "relevancy": 2.1129, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5381}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5281}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5244}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GoMAvatar%3A%20Efficient%20Animatable%20Human%20Modeling%20from%20Monocular%20Video%0A%20%20Using%20Gaussians-on-Mesh&body=Title%3A%20GoMAvatar%3A%20Efficient%20Animatable%20Human%20Modeling%20from%20Monocular%20Video%0A%20%20Using%20Gaussians-on-Mesh%0AAuthor%3A%20Jing%20Wen%20and%20Xiaoming%20Zhao%20and%20Zhongzheng%20Ren%20and%20Alexander%20G.%20Schwing%20and%20Shenlong%20Wang%0AAbstract%3A%20%20%20We%20introduce%20GoMAvatar%2C%20a%20novel%20approach%20for%20real-time%2C%20memory-efficient%2C%0Ahigh-quality%20animatable%20human%20modeling.%20GoMAvatar%20takes%20as%20input%20a%20single%0Amonocular%20video%20to%20create%20a%20digital%20avatar%20capable%20of%20re-articulation%20in%20new%0Aposes%20and%20real-time%20rendering%20from%20novel%20viewpoints%2C%20while%20seamlessly%0Aintegrating%20with%20rasterization-based%20graphics%20pipelines.%20Central%20to%20our%20method%0Ais%20the%20Gaussians-on-Mesh%20representation%2C%20a%20hybrid%203D%20model%20combining%20rendering%0Aquality%20and%20speed%20of%20Gaussian%20splatting%20with%20geometry%20modeling%20and%0Acompatibility%20of%20deformable%20meshes.%20We%20assess%20GoMAvatar%20on%20ZJU-MoCap%20data%20and%0Avarious%20YouTube%20videos.%20GoMAvatar%20matches%20or%20surpasses%20current%20monocular%20human%0Amodeling%20algorithms%20in%20rendering%20quality%20and%20significantly%20outperforms%20them%20in%0Acomputational%20efficiency%20%2843%20FPS%29%20while%20being%20memory-efficient%20%283.63%20MB%20per%0Asubject%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07991v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GoMAvatar%3A%20Efficient%20Animatable%20Human%20Modeling%20from%20Monocular%20Video%0A%20%20Using%20Gaussians-on-Mesh&entry.906535625=Jing%20Wen%20and%20Xiaoming%20Zhao%20and%20Zhongzheng%20Ren%20and%20Alexander%20G.%20Schwing%20and%20Shenlong%20Wang&entry.1292438233=%20%20We%20introduce%20GoMAvatar%2C%20a%20novel%20approach%20for%20real-time%2C%20memory-efficient%2C%0Ahigh-quality%20animatable%20human%20modeling.%20GoMAvatar%20takes%20as%20input%20a%20single%0Amonocular%20video%20to%20create%20a%20digital%20avatar%20capable%20of%20re-articulation%20in%20new%0Aposes%20and%20real-time%20rendering%20from%20novel%20viewpoints%2C%20while%20seamlessly%0Aintegrating%20with%20rasterization-based%20graphics%20pipelines.%20Central%20to%20our%20method%0Ais%20the%20Gaussians-on-Mesh%20representation%2C%20a%20hybrid%203D%20model%20combining%20rendering%0Aquality%20and%20speed%20of%20Gaussian%20splatting%20with%20geometry%20modeling%20and%0Acompatibility%20of%20deformable%20meshes.%20We%20assess%20GoMAvatar%20on%20ZJU-MoCap%20data%20and%0Avarious%20YouTube%20videos.%20GoMAvatar%20matches%20or%20surpasses%20current%20monocular%20human%0Amodeling%20algorithms%20in%20rendering%20quality%20and%20significantly%20outperforms%20them%20in%0Acomputational%20efficiency%20%2843%20FPS%29%20while%20being%20memory-efficient%20%283.63%20MB%20per%0Asubject%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07991v1&entry.124074799=Read"},
{"title": "ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State\n  Space Model", "author": "Hongruixuan Chen and Jian Song and Chengxi Han and Junshi Xia and Naoto Yokoya", "abstract": "  Convolutional neural networks (CNN) and Transformers have made impressive\nprogress in the field of remote sensing change detection (CD). However, both\narchitectures have inherent shortcomings. Recently, the Mamba architecture,\nbased on state space models, has shown remarkable performance in a series of\nnatural language processing tasks, which can effectively compensate for the\nshortcomings of the above two architectures. In this paper, we explore for the\nfirst time the potential of the Mamba architecture for remote sensing CD tasks.\nWe tailor the corresponding frameworks, called MambaBCD, MambaSCD, and\nMambaBDA, for binary change detection (BCD), semantic change detection (SCD),\nand building damage assessment (BDA), respectively. All three frameworks adopt\nthe cutting-edge Visual Mamba architecture as the encoder, which allows full\nlearning of global spatial contextual information from the input images. For\nthe change decoder, which is available in all three architectures, we propose\nthree spatio-temporal relationship modeling mechanisms, which can be naturally\ncombined with the Mamba architecture and fully utilize its attribute to achieve\nspatio-temporal interaction of multi-temporal features, thereby obtaining\naccurate change information. On five benchmark datasets, our proposed\nframeworks outperform current CNN- and Transformer-based approaches without\nusing any complex training strategies or tricks, fully demonstrating the\npotential of the Mamba architecture in CD tasks. Specifically, we obtained\n83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU, LEVIR-CD+,\nand WHU-CD; on the SCD dataset SECOND, we obtained 24.11% SeK; and on the BDA\ndataset xBD, we obtained 81.41% overall F1 score. Further experiments show that\nour architecture is quite robust to degraded data. The source code will be\navailable in https://github.com/ChenHongruixuan/MambaCD\n", "link": "http://arxiv.org/abs/2404.03425v2", "date": "2024-04-11", "relevancy": 2.1123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5407}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5273}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5238}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model&body=Title%3A%20ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model%0AAuthor%3A%20Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Chengxi%20Han%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNN%29%20and%20Transformers%20have%20made%20impressive%0Aprogress%20in%20the%20field%20of%20remote%20sensing%20change%20detection%20%28CD%29.%20However%2C%20both%0Aarchitectures%20have%20inherent%20shortcomings.%20Recently%2C%20the%20Mamba%20architecture%2C%0Abased%20on%20state%20space%20models%2C%20has%20shown%20remarkable%20performance%20in%20a%20series%20of%0Anatural%20language%20processing%20tasks%2C%20which%20can%20effectively%20compensate%20for%20the%0Ashortcomings%20of%20the%20above%20two%20architectures.%20In%20this%20paper%2C%20we%20explore%20for%20the%0Afirst%20time%20the%20potential%20of%20the%20Mamba%20architecture%20for%20remote%20sensing%20CD%20tasks.%0AWe%20tailor%20the%20corresponding%20frameworks%2C%20called%20MambaBCD%2C%20MambaSCD%2C%20and%0AMambaBDA%2C%20for%20binary%20change%20detection%20%28BCD%29%2C%20semantic%20change%20detection%20%28SCD%29%2C%0Aand%20building%20damage%20assessment%20%28BDA%29%2C%20respectively.%20All%20three%20frameworks%20adopt%0Athe%20cutting-edge%20Visual%20Mamba%20architecture%20as%20the%20encoder%2C%20which%20allows%20full%0Alearning%20of%20global%20spatial%20contextual%20information%20from%20the%20input%20images.%20For%0Athe%20change%20decoder%2C%20which%20is%20available%20in%20all%20three%20architectures%2C%20we%20propose%0Athree%20spatio-temporal%20relationship%20modeling%20mechanisms%2C%20which%20can%20be%20naturally%0Acombined%20with%20the%20Mamba%20architecture%20and%20fully%20utilize%20its%20attribute%20to%20achieve%0Aspatio-temporal%20interaction%20of%20multi-temporal%20features%2C%20thereby%20obtaining%0Aaccurate%20change%20information.%20On%20five%20benchmark%20datasets%2C%20our%20proposed%0Aframeworks%20outperform%20current%20CNN-%20and%20Transformer-based%20approaches%20without%0Ausing%20any%20complex%20training%20strategies%20or%20tricks%2C%20fully%20demonstrating%20the%0Apotential%20of%20the%20Mamba%20architecture%20in%20CD%20tasks.%20Specifically%2C%20we%20obtained%0A83.11%25%2C%2088.39%25%20and%2094.19%25%20F1%20scores%20on%20the%20three%20BCD%20datasets%20SYSU%2C%20LEVIR-CD%2B%2C%0Aand%20WHU-CD%3B%20on%20the%20SCD%20dataset%20SECOND%2C%20we%20obtained%2024.11%25%20SeK%3B%20and%20on%20the%20BDA%0Adataset%20xBD%2C%20we%20obtained%2081.41%25%20overall%20F1%20score.%20Further%20experiments%20show%20that%0Aour%20architecture%20is%20quite%20robust%20to%20degraded%20data.%20The%20source%20code%20will%20be%0Aavailable%20in%20https%3A//github.com/ChenHongruixuan/MambaCD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03425v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model&entry.906535625=Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Chengxi%20Han%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNN%29%20and%20Transformers%20have%20made%20impressive%0Aprogress%20in%20the%20field%20of%20remote%20sensing%20change%20detection%20%28CD%29.%20However%2C%20both%0Aarchitectures%20have%20inherent%20shortcomings.%20Recently%2C%20the%20Mamba%20architecture%2C%0Abased%20on%20state%20space%20models%2C%20has%20shown%20remarkable%20performance%20in%20a%20series%20of%0Anatural%20language%20processing%20tasks%2C%20which%20can%20effectively%20compensate%20for%20the%0Ashortcomings%20of%20the%20above%20two%20architectures.%20In%20this%20paper%2C%20we%20explore%20for%20the%0Afirst%20time%20the%20potential%20of%20the%20Mamba%20architecture%20for%20remote%20sensing%20CD%20tasks.%0AWe%20tailor%20the%20corresponding%20frameworks%2C%20called%20MambaBCD%2C%20MambaSCD%2C%20and%0AMambaBDA%2C%20for%20binary%20change%20detection%20%28BCD%29%2C%20semantic%20change%20detection%20%28SCD%29%2C%0Aand%20building%20damage%20assessment%20%28BDA%29%2C%20respectively.%20All%20three%20frameworks%20adopt%0Athe%20cutting-edge%20Visual%20Mamba%20architecture%20as%20the%20encoder%2C%20which%20allows%20full%0Alearning%20of%20global%20spatial%20contextual%20information%20from%20the%20input%20images.%20For%0Athe%20change%20decoder%2C%20which%20is%20available%20in%20all%20three%20architectures%2C%20we%20propose%0Athree%20spatio-temporal%20relationship%20modeling%20mechanisms%2C%20which%20can%20be%20naturally%0Acombined%20with%20the%20Mamba%20architecture%20and%20fully%20utilize%20its%20attribute%20to%20achieve%0Aspatio-temporal%20interaction%20of%20multi-temporal%20features%2C%20thereby%20obtaining%0Aaccurate%20change%20information.%20On%20five%20benchmark%20datasets%2C%20our%20proposed%0Aframeworks%20outperform%20current%20CNN-%20and%20Transformer-based%20approaches%20without%0Ausing%20any%20complex%20training%20strategies%20or%20tricks%2C%20fully%20demonstrating%20the%0Apotential%20of%20the%20Mamba%20architecture%20in%20CD%20tasks.%20Specifically%2C%20we%20obtained%0A83.11%25%2C%2088.39%25%20and%2094.19%25%20F1%20scores%20on%20the%20three%20BCD%20datasets%20SYSU%2C%20LEVIR-CD%2B%2C%0Aand%20WHU-CD%3B%20on%20the%20SCD%20dataset%20SECOND%2C%20we%20obtained%2024.11%25%20SeK%3B%20and%20on%20the%20BDA%0Adataset%20xBD%2C%20we%20obtained%2081.41%25%20overall%20F1%20score.%20Further%20experiments%20show%20that%0Aour%20architecture%20is%20quite%20robust%20to%20degraded%20data.%20The%20source%20code%20will%20be%0Aavailable%20in%20https%3A//github.com/ChenHongruixuan/MambaCD%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03425v2&entry.124074799=Read"},
{"title": "Low-Resource Vision Challenges for Foundation Models", "author": "Yunhua Zhang and Hazel Doughty and Cees G. M. Snoek", "abstract": "  Low-resource settings are well-established in natural language processing,\nwhere many languages lack sufficient data for deep learning at scale. However,\nlow-resource problems are under-explored in computer vision. In this paper, we\naddress this gap and explore the challenges of low-resource image tasks with\nvision foundation models. We first collect a benchmark of genuinely\nlow-resource image data, covering historic maps, circuit diagrams, and\nmechanical drawings. These low-resource settings all share three challenges:\ndata scarcity, fine-grained differences, and the distribution shift from\nnatural images to the specialized domain of interest. While existing foundation\nmodels have shown impressive generalizability, we find they cannot transfer\nwell to our low-resource tasks. To begin to tackle the challenges of\nlow-resource vision, we introduce one simple baseline per challenge.\nSpecifically, we i) enlarge the data space by generative models, ii) adopt the\nbest sub-kernels to encode local regions for fine-grained difference discovery\nand iii) learn attention for specialized domains. Experiments on our three\nlow-resource tasks demonstrate our proposals already provide a better baseline\nthan transfer learning, data augmentation, and fine-grained methods. This\nhighlights the unique characteristics and challenges of low-resource vision for\nfoundation models that warrant further investigation. Project page:\nhttps://xiaobai1217.github.io/Low-Resource-Vision/.\n", "link": "http://arxiv.org/abs/2401.04716v3", "date": "2024-04-11", "relevancy": 2.1111, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5384}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5269}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5175}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Low-Resource%20Vision%20Challenges%20for%20Foundation%20Models&body=Title%3A%20Low-Resource%20Vision%20Challenges%20for%20Foundation%20Models%0AAuthor%3A%20Yunhua%20Zhang%20and%20Hazel%20Doughty%20and%20Cees%20G.%20M.%20Snoek%0AAbstract%3A%20%20%20Low-resource%20settings%20are%20well-established%20in%20natural%20language%20processing%2C%0Awhere%20many%20languages%20lack%20sufficient%20data%20for%20deep%20learning%20at%20scale.%20However%2C%0Alow-resource%20problems%20are%20under-explored%20in%20computer%20vision.%20In%20this%20paper%2C%20we%0Aaddress%20this%20gap%20and%20explore%20the%20challenges%20of%20low-resource%20image%20tasks%20with%0Avision%20foundation%20models.%20We%20first%20collect%20a%20benchmark%20of%20genuinely%0Alow-resource%20image%20data%2C%20covering%20historic%20maps%2C%20circuit%20diagrams%2C%20and%0Amechanical%20drawings.%20These%20low-resource%20settings%20all%20share%20three%20challenges%3A%0Adata%20scarcity%2C%20fine-grained%20differences%2C%20and%20the%20distribution%20shift%20from%0Anatural%20images%20to%20the%20specialized%20domain%20of%20interest.%20While%20existing%20foundation%0Amodels%20have%20shown%20impressive%20generalizability%2C%20we%20find%20they%20cannot%20transfer%0Awell%20to%20our%20low-resource%20tasks.%20To%20begin%20to%20tackle%20the%20challenges%20of%0Alow-resource%20vision%2C%20we%20introduce%20one%20simple%20baseline%20per%20challenge.%0ASpecifically%2C%20we%20i%29%20enlarge%20the%20data%20space%20by%20generative%20models%2C%20ii%29%20adopt%20the%0Abest%20sub-kernels%20to%20encode%20local%20regions%20for%20fine-grained%20difference%20discovery%0Aand%20iii%29%20learn%20attention%20for%20specialized%20domains.%20Experiments%20on%20our%20three%0Alow-resource%20tasks%20demonstrate%20our%20proposals%20already%20provide%20a%20better%20baseline%0Athan%20transfer%20learning%2C%20data%20augmentation%2C%20and%20fine-grained%20methods.%20This%0Ahighlights%20the%20unique%20characteristics%20and%20challenges%20of%20low-resource%20vision%20for%0Afoundation%20models%20that%20warrant%20further%20investigation.%20Project%20page%3A%0Ahttps%3A//xiaobai1217.github.io/Low-Resource-Vision/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04716v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Resource%20Vision%20Challenges%20for%20Foundation%20Models&entry.906535625=Yunhua%20Zhang%20and%20Hazel%20Doughty%20and%20Cees%20G.%20M.%20Snoek&entry.1292438233=%20%20Low-resource%20settings%20are%20well-established%20in%20natural%20language%20processing%2C%0Awhere%20many%20languages%20lack%20sufficient%20data%20for%20deep%20learning%20at%20scale.%20However%2C%0Alow-resource%20problems%20are%20under-explored%20in%20computer%20vision.%20In%20this%20paper%2C%20we%0Aaddress%20this%20gap%20and%20explore%20the%20challenges%20of%20low-resource%20image%20tasks%20with%0Avision%20foundation%20models.%20We%20first%20collect%20a%20benchmark%20of%20genuinely%0Alow-resource%20image%20data%2C%20covering%20historic%20maps%2C%20circuit%20diagrams%2C%20and%0Amechanical%20drawings.%20These%20low-resource%20settings%20all%20share%20three%20challenges%3A%0Adata%20scarcity%2C%20fine-grained%20differences%2C%20and%20the%20distribution%20shift%20from%0Anatural%20images%20to%20the%20specialized%20domain%20of%20interest.%20While%20existing%20foundation%0Amodels%20have%20shown%20impressive%20generalizability%2C%20we%20find%20they%20cannot%20transfer%0Awell%20to%20our%20low-resource%20tasks.%20To%20begin%20to%20tackle%20the%20challenges%20of%0Alow-resource%20vision%2C%20we%20introduce%20one%20simple%20baseline%20per%20challenge.%0ASpecifically%2C%20we%20i%29%20enlarge%20the%20data%20space%20by%20generative%20models%2C%20ii%29%20adopt%20the%0Abest%20sub-kernels%20to%20encode%20local%20regions%20for%20fine-grained%20difference%20discovery%0Aand%20iii%29%20learn%20attention%20for%20specialized%20domains.%20Experiments%20on%20our%20three%0Alow-resource%20tasks%20demonstrate%20our%20proposals%20already%20provide%20a%20better%20baseline%0Athan%20transfer%20learning%2C%20data%20augmentation%2C%20and%20fine-grained%20methods.%20This%0Ahighlights%20the%20unique%20characteristics%20and%20challenges%20of%20low-resource%20vision%20for%0Afoundation%20models%20that%20warrant%20further%20investigation.%20Project%20page%3A%0Ahttps%3A//xiaobai1217.github.io/Low-Resource-Vision/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04716v3&entry.124074799=Read"},
{"title": "Separated Attention: An Improved Cycle GAN Based Under Water Image\n  Enhancement Method", "author": "Tashmoy Ghosh", "abstract": "  In this paper we have present an improved Cycle GAN based model for under\nwater image enhancement. We have utilized the cycle consistent learning\ntechnique of the state-of-the-art Cycle GAN model with modification in the loss\nfunction in terms of depth-oriented attention which enhance the contrast of the\noverall image, keeping global content, color, local texture, and style\ninformation intact. We trained the Cycle GAN model with the modified loss\nfunctions on the benchmarked Enhancing Underwater Visual Perception (EUPV)\ndataset a large dataset including paired and unpaired sets of underwater images\n(poor and good quality) taken with seven distinct cameras in a range of\nvisibility situation during research on ocean exploration and human-robot\ncooperation. In addition, we perform qualitative and quantitative evaluation\nwhich supports the given technique applied and provided a better contrast\nenhancement model of underwater imagery. More significantly, the upgraded\nimages provide better results from conventional models and further for under\nwater navigation, pose estimation, saliency prediction, object detection and\ntracking. The results validate the appropriateness of the model for autonomous\nunderwater vehicles (AUV) in visual navigation.\n", "link": "http://arxiv.org/abs/2404.07649v1", "date": "2024-04-11", "relevancy": 2.0782, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5221}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5217}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5079}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Separated%20Attention%3A%20An%20Improved%20Cycle%20GAN%20Based%20Under%20Water%20Image%0A%20%20Enhancement%20Method&body=Title%3A%20Separated%20Attention%3A%20An%20Improved%20Cycle%20GAN%20Based%20Under%20Water%20Image%0A%20%20Enhancement%20Method%0AAuthor%3A%20Tashmoy%20Ghosh%0AAbstract%3A%20%20%20In%20this%20paper%20we%20have%20present%20an%20improved%20Cycle%20GAN%20based%20model%20for%20under%0Awater%20image%20enhancement.%20We%20have%20utilized%20the%20cycle%20consistent%20learning%0Atechnique%20of%20the%20state-of-the-art%20Cycle%20GAN%20model%20with%20modification%20in%20the%20loss%0Afunction%20in%20terms%20of%20depth-oriented%20attention%20which%20enhance%20the%20contrast%20of%20the%0Aoverall%20image%2C%20keeping%20global%20content%2C%20color%2C%20local%20texture%2C%20and%20style%0Ainformation%20intact.%20We%20trained%20the%20Cycle%20GAN%20model%20with%20the%20modified%20loss%0Afunctions%20on%20the%20benchmarked%20Enhancing%20Underwater%20Visual%20Perception%20%28EUPV%29%0Adataset%20a%20large%20dataset%20including%20paired%20and%20unpaired%20sets%20of%20underwater%20images%0A%28poor%20and%20good%20quality%29%20taken%20with%20seven%20distinct%20cameras%20in%20a%20range%20of%0Avisibility%20situation%20during%20research%20on%20ocean%20exploration%20and%20human-robot%0Acooperation.%20In%20addition%2C%20we%20perform%20qualitative%20and%20quantitative%20evaluation%0Awhich%20supports%20the%20given%20technique%20applied%20and%20provided%20a%20better%20contrast%0Aenhancement%20model%20of%20underwater%20imagery.%20More%20significantly%2C%20the%20upgraded%0Aimages%20provide%20better%20results%20from%20conventional%20models%20and%20further%20for%20under%0Awater%20navigation%2C%20pose%20estimation%2C%20saliency%20prediction%2C%20object%20detection%20and%0Atracking.%20The%20results%20validate%20the%20appropriateness%20of%20the%20model%20for%20autonomous%0Aunderwater%20vehicles%20%28AUV%29%20in%20visual%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07649v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separated%20Attention%3A%20An%20Improved%20Cycle%20GAN%20Based%20Under%20Water%20Image%0A%20%20Enhancement%20Method&entry.906535625=Tashmoy%20Ghosh&entry.1292438233=%20%20In%20this%20paper%20we%20have%20present%20an%20improved%20Cycle%20GAN%20based%20model%20for%20under%0Awater%20image%20enhancement.%20We%20have%20utilized%20the%20cycle%20consistent%20learning%0Atechnique%20of%20the%20state-of-the-art%20Cycle%20GAN%20model%20with%20modification%20in%20the%20loss%0Afunction%20in%20terms%20of%20depth-oriented%20attention%20which%20enhance%20the%20contrast%20of%20the%0Aoverall%20image%2C%20keeping%20global%20content%2C%20color%2C%20local%20texture%2C%20and%20style%0Ainformation%20intact.%20We%20trained%20the%20Cycle%20GAN%20model%20with%20the%20modified%20loss%0Afunctions%20on%20the%20benchmarked%20Enhancing%20Underwater%20Visual%20Perception%20%28EUPV%29%0Adataset%20a%20large%20dataset%20including%20paired%20and%20unpaired%20sets%20of%20underwater%20images%0A%28poor%20and%20good%20quality%29%20taken%20with%20seven%20distinct%20cameras%20in%20a%20range%20of%0Avisibility%20situation%20during%20research%20on%20ocean%20exploration%20and%20human-robot%0Acooperation.%20In%20addition%2C%20we%20perform%20qualitative%20and%20quantitative%20evaluation%0Awhich%20supports%20the%20given%20technique%20applied%20and%20provided%20a%20better%20contrast%0Aenhancement%20model%20of%20underwater%20imagery.%20More%20significantly%2C%20the%20upgraded%0Aimages%20provide%20better%20results%20from%20conventional%20models%20and%20further%20for%20under%0Awater%20navigation%2C%20pose%20estimation%2C%20saliency%20prediction%2C%20object%20detection%20and%0Atracking.%20The%20results%20validate%20the%20appropriateness%20of%20the%20model%20for%20autonomous%0Aunderwater%20vehicles%20%28AUV%29%20in%20visual%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07649v1&entry.124074799=Read"},
{"title": "Shape Completion in the Dark: Completing Vertebrae Morphology from 3D\n  Ultrasound", "author": "Miruna-Alexandra Gafencu and Yordanka Velikova and Mahdi Saleh and Tamas Ungi and Nassir Navab and Thomas Wendler and Mohammad Farid Azampour", "abstract": "  Purpose: Ultrasound (US) imaging, while advantageous for its radiation-free\nnature, is challenging to interpret due to only partially visible organs and a\nlack of complete 3D information. While performing US-based diagnosis or\ninvestigation, medical professionals therefore create a mental map of the 3D\nanatomy. In this work, we aim to replicate this process and enhance the visual\nrepresentation of anatomical structures.\n  Methods: We introduce a point-cloud-based probabilistic DL method to complete\noccluded anatomical structures through 3D shape completion and choose US-based\nspine examinations as our application. To enable training, we generate\nsynthetic 3D representations of partially occluded spinal views by mimicking US\nphysics and accounting for inherent artifacts.\n  Results: The proposed model performs consistently on synthetic and patient\ndata, with mean and median differences of 2.02 and 0.03 in CD, respectively.\nOur ablation study demonstrates the importance of US physics-based data\ngeneration, reflected in the large mean and median difference of 11.8 CD and\n9.55 CD, respectively. Additionally, we demonstrate that anatomic landmarks,\nsuch as the spinous process (with reconstruction CD of 4.73) and the facet\njoints (mean distance to GT of 4.96mm) are preserved in the 3D completion.\n  Conclusion: Our work establishes the feasibility of 3D shape completion for\nlumbar vertebrae, ensuring the preservation of level-wise characteristics and\nsuccessful generalization from synthetic to real data. The incorporation of US\nphysics contributes to more accurate patient data completions. Notably, our\nmethod preserves essential anatomic landmarks and reconstructs crucial\ninjections sites at their correct locations. The generated data and source code\nwill be made publicly available\n(https://github.com/miruna20/Shape-Completion-in-the-Dark).\n", "link": "http://arxiv.org/abs/2404.07668v1", "date": "2024-04-11", "relevancy": 2.0738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5175}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4902}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Shape%20Completion%20in%20the%20Dark%3A%20Completing%20Vertebrae%20Morphology%20from%203D%0A%20%20Ultrasound&body=Title%3A%20Shape%20Completion%20in%20the%20Dark%3A%20Completing%20Vertebrae%20Morphology%20from%203D%0A%20%20Ultrasound%0AAuthor%3A%20Miruna-Alexandra%20Gafencu%20and%20Yordanka%20Velikova%20and%20Mahdi%20Saleh%20and%20Tamas%20Ungi%20and%20Nassir%20Navab%20and%20Thomas%20Wendler%20and%20Mohammad%20Farid%20Azampour%0AAbstract%3A%20%20%20Purpose%3A%20Ultrasound%20%28US%29%20imaging%2C%20while%20advantageous%20for%20its%20radiation-free%0Anature%2C%20is%20challenging%20to%20interpret%20due%20to%20only%20partially%20visible%20organs%20and%20a%0Alack%20of%20complete%203D%20information.%20While%20performing%20US-based%20diagnosis%20or%0Ainvestigation%2C%20medical%20professionals%20therefore%20create%20a%20mental%20map%20of%20the%203D%0Aanatomy.%20In%20this%20work%2C%20we%20aim%20to%20replicate%20this%20process%20and%20enhance%20the%20visual%0Arepresentation%20of%20anatomical%20structures.%0A%20%20Methods%3A%20We%20introduce%20a%20point-cloud-based%20probabilistic%20DL%20method%20to%20complete%0Aoccluded%20anatomical%20structures%20through%203D%20shape%20completion%20and%20choose%20US-based%0Aspine%20examinations%20as%20our%20application.%20To%20enable%20training%2C%20we%20generate%0Asynthetic%203D%20representations%20of%20partially%20occluded%20spinal%20views%20by%20mimicking%20US%0Aphysics%20and%20accounting%20for%20inherent%20artifacts.%0A%20%20Results%3A%20The%20proposed%20model%20performs%20consistently%20on%20synthetic%20and%20patient%0Adata%2C%20with%20mean%20and%20median%20differences%20of%202.02%20and%200.03%20in%20CD%2C%20respectively.%0AOur%20ablation%20study%20demonstrates%20the%20importance%20of%20US%20physics-based%20data%0Ageneration%2C%20reflected%20in%20the%20large%20mean%20and%20median%20difference%20of%2011.8%20CD%20and%0A9.55%20CD%2C%20respectively.%20Additionally%2C%20we%20demonstrate%20that%20anatomic%20landmarks%2C%0Asuch%20as%20the%20spinous%20process%20%28with%20reconstruction%20CD%20of%204.73%29%20and%20the%20facet%0Ajoints%20%28mean%20distance%20to%20GT%20of%204.96mm%29%20are%20preserved%20in%20the%203D%20completion.%0A%20%20Conclusion%3A%20Our%20work%20establishes%20the%20feasibility%20of%203D%20shape%20completion%20for%0Alumbar%20vertebrae%2C%20ensuring%20the%20preservation%20of%20level-wise%20characteristics%20and%0Asuccessful%20generalization%20from%20synthetic%20to%20real%20data.%20The%20incorporation%20of%20US%0Aphysics%20contributes%20to%20more%20accurate%20patient%20data%20completions.%20Notably%2C%20our%0Amethod%20preserves%20essential%20anatomic%20landmarks%20and%20reconstructs%20crucial%0Ainjections%20sites%20at%20their%20correct%20locations.%20The%20generated%20data%20and%20source%20code%0Awill%20be%20made%20publicly%20available%0A%28https%3A//github.com/miruna20/Shape-Completion-in-the-Dark%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07668v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shape%20Completion%20in%20the%20Dark%3A%20Completing%20Vertebrae%20Morphology%20from%203D%0A%20%20Ultrasound&entry.906535625=Miruna-Alexandra%20Gafencu%20and%20Yordanka%20Velikova%20and%20Mahdi%20Saleh%20and%20Tamas%20Ungi%20and%20Nassir%20Navab%20and%20Thomas%20Wendler%20and%20Mohammad%20Farid%20Azampour&entry.1292438233=%20%20Purpose%3A%20Ultrasound%20%28US%29%20imaging%2C%20while%20advantageous%20for%20its%20radiation-free%0Anature%2C%20is%20challenging%20to%20interpret%20due%20to%20only%20partially%20visible%20organs%20and%20a%0Alack%20of%20complete%203D%20information.%20While%20performing%20US-based%20diagnosis%20or%0Ainvestigation%2C%20medical%20professionals%20therefore%20create%20a%20mental%20map%20of%20the%203D%0Aanatomy.%20In%20this%20work%2C%20we%20aim%20to%20replicate%20this%20process%20and%20enhance%20the%20visual%0Arepresentation%20of%20anatomical%20structures.%0A%20%20Methods%3A%20We%20introduce%20a%20point-cloud-based%20probabilistic%20DL%20method%20to%20complete%0Aoccluded%20anatomical%20structures%20through%203D%20shape%20completion%20and%20choose%20US-based%0Aspine%20examinations%20as%20our%20application.%20To%20enable%20training%2C%20we%20generate%0Asynthetic%203D%20representations%20of%20partially%20occluded%20spinal%20views%20by%20mimicking%20US%0Aphysics%20and%20accounting%20for%20inherent%20artifacts.%0A%20%20Results%3A%20The%20proposed%20model%20performs%20consistently%20on%20synthetic%20and%20patient%0Adata%2C%20with%20mean%20and%20median%20differences%20of%202.02%20and%200.03%20in%20CD%2C%20respectively.%0AOur%20ablation%20study%20demonstrates%20the%20importance%20of%20US%20physics-based%20data%0Ageneration%2C%20reflected%20in%20the%20large%20mean%20and%20median%20difference%20of%2011.8%20CD%20and%0A9.55%20CD%2C%20respectively.%20Additionally%2C%20we%20demonstrate%20that%20anatomic%20landmarks%2C%0Asuch%20as%20the%20spinous%20process%20%28with%20reconstruction%20CD%20of%204.73%29%20and%20the%20facet%0Ajoints%20%28mean%20distance%20to%20GT%20of%204.96mm%29%20are%20preserved%20in%20the%203D%20completion.%0A%20%20Conclusion%3A%20Our%20work%20establishes%20the%20feasibility%20of%203D%20shape%20completion%20for%0Alumbar%20vertebrae%2C%20ensuring%20the%20preservation%20of%20level-wise%20characteristics%20and%0Asuccessful%20generalization%20from%20synthetic%20to%20real%20data.%20The%20incorporation%20of%20US%0Aphysics%20contributes%20to%20more%20accurate%20patient%20data%20completions.%20Notably%2C%20our%0Amethod%20preserves%20essential%20anatomic%20landmarks%20and%20reconstructs%20crucial%0Ainjections%20sites%20at%20their%20correct%20locations.%20The%20generated%20data%20and%20source%20code%0Awill%20be%20made%20publicly%20available%0A%28https%3A//github.com/miruna20/Shape-Completion-in-the-Dark%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07668v1&entry.124074799=Read"},
{"title": "WaveMo: Learning Wavefront Modulations to See Through Scattering", "author": "Mingyang Xie and Haiyun Guo and Brandon Y. Feng and Lingbo Jin and Ashok Veeraraghavan and Christopher A. Metzler", "abstract": "  Imaging through scattering media is a fundamental and pervasive challenge in\nfields ranging from medical diagnostics to astronomy. A promising strategy to\novercome this challenge is wavefront modulation, which induces measurement\ndiversity during image acquisition. Despite its importance, designing optimal\nwavefront modulations to image through scattering remains under-explored. This\npaper introduces a novel learning-based framework to address the gap. Our\napproach jointly optimizes wavefront modulations and a computationally\nlightweight feedforward \"proxy\" reconstruction network. This network is trained\nto recover scenes obscured by scattering, using measurements that are modified\nby these modulations. The learned modulations produced by our framework\ngeneralize effectively to unseen scattering scenarios and exhibit remarkable\nversatility. During deployment, the learned modulations can be decoupled from\nthe proxy network to augment other more computationally expensive restoration\nalgorithms. Through extensive experiments, we demonstrate our approach\nsignificantly advances the state of the art in imaging through scattering\nmedia. Our project webpage is at https://wavemo-2024.github.io/.\n", "link": "http://arxiv.org/abs/2404.07985v1", "date": "2024-04-11", "relevancy": 2.072, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WaveMo%3A%20Learning%20Wavefront%20Modulations%20to%20See%20Through%20Scattering&body=Title%3A%20WaveMo%3A%20Learning%20Wavefront%20Modulations%20to%20See%20Through%20Scattering%0AAuthor%3A%20Mingyang%20Xie%20and%20Haiyun%20Guo%20and%20Brandon%20Y.%20Feng%20and%20Lingbo%20Jin%20and%20Ashok%20Veeraraghavan%20and%20Christopher%20A.%20Metzler%0AAbstract%3A%20%20%20Imaging%20through%20scattering%20media%20is%20a%20fundamental%20and%20pervasive%20challenge%20in%0Afields%20ranging%20from%20medical%20diagnostics%20to%20astronomy.%20A%20promising%20strategy%20to%0Aovercome%20this%20challenge%20is%20wavefront%20modulation%2C%20which%20induces%20measurement%0Adiversity%20during%20image%20acquisition.%20Despite%20its%20importance%2C%20designing%20optimal%0Awavefront%20modulations%20to%20image%20through%20scattering%20remains%20under-explored.%20This%0Apaper%20introduces%20a%20novel%20learning-based%20framework%20to%20address%20the%20gap.%20Our%0Aapproach%20jointly%20optimizes%20wavefront%20modulations%20and%20a%20computationally%0Alightweight%20feedforward%20%22proxy%22%20reconstruction%20network.%20This%20network%20is%20trained%0Ato%20recover%20scenes%20obscured%20by%20scattering%2C%20using%20measurements%20that%20are%20modified%0Aby%20these%20modulations.%20The%20learned%20modulations%20produced%20by%20our%20framework%0Ageneralize%20effectively%20to%20unseen%20scattering%20scenarios%20and%20exhibit%20remarkable%0Aversatility.%20During%20deployment%2C%20the%20learned%20modulations%20can%20be%20decoupled%20from%0Athe%20proxy%20network%20to%20augment%20other%20more%20computationally%20expensive%20restoration%0Aalgorithms.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20our%20approach%0Asignificantly%20advances%20the%20state%20of%20the%20art%20in%20imaging%20through%20scattering%0Amedia.%20Our%20project%20webpage%20is%20at%20https%3A//wavemo-2024.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07985v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaveMo%3A%20Learning%20Wavefront%20Modulations%20to%20See%20Through%20Scattering&entry.906535625=Mingyang%20Xie%20and%20Haiyun%20Guo%20and%20Brandon%20Y.%20Feng%20and%20Lingbo%20Jin%20and%20Ashok%20Veeraraghavan%20and%20Christopher%20A.%20Metzler&entry.1292438233=%20%20Imaging%20through%20scattering%20media%20is%20a%20fundamental%20and%20pervasive%20challenge%20in%0Afields%20ranging%20from%20medical%20diagnostics%20to%20astronomy.%20A%20promising%20strategy%20to%0Aovercome%20this%20challenge%20is%20wavefront%20modulation%2C%20which%20induces%20measurement%0Adiversity%20during%20image%20acquisition.%20Despite%20its%20importance%2C%20designing%20optimal%0Awavefront%20modulations%20to%20image%20through%20scattering%20remains%20under-explored.%20This%0Apaper%20introduces%20a%20novel%20learning-based%20framework%20to%20address%20the%20gap.%20Our%0Aapproach%20jointly%20optimizes%20wavefront%20modulations%20and%20a%20computationally%0Alightweight%20feedforward%20%22proxy%22%20reconstruction%20network.%20This%20network%20is%20trained%0Ato%20recover%20scenes%20obscured%20by%20scattering%2C%20using%20measurements%20that%20are%20modified%0Aby%20these%20modulations.%20The%20learned%20modulations%20produced%20by%20our%20framework%0Ageneralize%20effectively%20to%20unseen%20scattering%20scenarios%20and%20exhibit%20remarkable%0Aversatility.%20During%20deployment%2C%20the%20learned%20modulations%20can%20be%20decoupled%20from%0Athe%20proxy%20network%20to%20augment%20other%20more%20computationally%20expensive%20restoration%0Aalgorithms.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20our%20approach%0Asignificantly%20advances%20the%20state%20of%20the%20art%20in%20imaging%20through%20scattering%0Amedia.%20Our%20project%20webpage%20is%20at%20https%3A//wavemo-2024.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07985v1&entry.124074799=Read"},
{"title": "Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents\n  Probability Estimator", "author": "Daniele Mari and Andr\u00e9 F. R. Guarda and Nuno M. M. Rodrigues and Simone Milani and Fernando Pereira", "abstract": "  The widespread usage of point clouds (PC) for immersive visual applications\nhas resulted in the use of very heterogeneous receiving conditions and devices,\nnotably in terms of network, hardware, and display capabilities. In this\nscenario, quality scalability, i.e., the ability to reconstruct a signal at\ndifferent qualities by progressively decoding a single bitstream, is a major\nrequirement that has yet to be conveniently addressed, notably in most\nlearning-based PC coding solutions. This paper proposes a quality scalability\nscheme, named Scalable Quality Hyperprior (SQH), adaptable to learning-based\nstatic point cloud geometry codecs, which uses a Quality-conditioned Latents\nProbability Estimator (QuLPE) to decode a high-quality version of a PC\nlearning-based representation, based on an available lower quality base layer.\nSQH is integrated in the future JPEG PC coding standard, allowing to create a\nlayered bitstream that can be used to progressively decode the PC geometry with\nincreasing quality and fidelity. Experimental results show that SQH offers the\nquality scalability feature with very limited or no compression performance\npenalty at all when compared with the corresponding non-scalable solution, thus\npreserving the significant compression gains over other state-of-the-art PC\ncodecs.\n", "link": "http://arxiv.org/abs/2404.07698v1", "date": "2024-04-11", "relevancy": 2.0601, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5293}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5166}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5002}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Point%20Cloud%20Geometry%20Scalable%20Coding%20with%20a%20Quality-Conditioned%20Latents%0A%20%20Probability%20Estimator&body=Title%3A%20Point%20Cloud%20Geometry%20Scalable%20Coding%20with%20a%20Quality-Conditioned%20Latents%0A%20%20Probability%20Estimator%0AAuthor%3A%20Daniele%20Mari%20and%20Andr%C3%A9%20F.%20R.%20Guarda%20and%20Nuno%20M.%20M.%20Rodrigues%20and%20Simone%20Milani%20and%20Fernando%20Pereira%0AAbstract%3A%20%20%20The%20widespread%20usage%20of%20point%20clouds%20%28PC%29%20for%20immersive%20visual%20applications%0Ahas%20resulted%20in%20the%20use%20of%20very%20heterogeneous%20receiving%20conditions%20and%20devices%2C%0Anotably%20in%20terms%20of%20network%2C%20hardware%2C%20and%20display%20capabilities.%20In%20this%0Ascenario%2C%20quality%20scalability%2C%20i.e.%2C%20the%20ability%20to%20reconstruct%20a%20signal%20at%0Adifferent%20qualities%20by%20progressively%20decoding%20a%20single%20bitstream%2C%20is%20a%20major%0Arequirement%20that%20has%20yet%20to%20be%20conveniently%20addressed%2C%20notably%20in%20most%0Alearning-based%20PC%20coding%20solutions.%20This%20paper%20proposes%20a%20quality%20scalability%0Ascheme%2C%20named%20Scalable%20Quality%20Hyperprior%20%28SQH%29%2C%20adaptable%20to%20learning-based%0Astatic%20point%20cloud%20geometry%20codecs%2C%20which%20uses%20a%20Quality-conditioned%20Latents%0AProbability%20Estimator%20%28QuLPE%29%20to%20decode%20a%20high-quality%20version%20of%20a%20PC%0Alearning-based%20representation%2C%20based%20on%20an%20available%20lower%20quality%20base%20layer.%0ASQH%20is%20integrated%20in%20the%20future%20JPEG%20PC%20coding%20standard%2C%20allowing%20to%20create%20a%0Alayered%20bitstream%20that%20can%20be%20used%20to%20progressively%20decode%20the%20PC%20geometry%20with%0Aincreasing%20quality%20and%20fidelity.%20Experimental%20results%20show%20that%20SQH%20offers%20the%0Aquality%20scalability%20feature%20with%20very%20limited%20or%20no%20compression%20performance%0Apenalty%20at%20all%20when%20compared%20with%20the%20corresponding%20non-scalable%20solution%2C%20thus%0Apreserving%20the%20significant%20compression%20gains%20over%20other%20state-of-the-art%20PC%0Acodecs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07698v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Cloud%20Geometry%20Scalable%20Coding%20with%20a%20Quality-Conditioned%20Latents%0A%20%20Probability%20Estimator&entry.906535625=Daniele%20Mari%20and%20Andr%C3%A9%20F.%20R.%20Guarda%20and%20Nuno%20M.%20M.%20Rodrigues%20and%20Simone%20Milani%20and%20Fernando%20Pereira&entry.1292438233=%20%20The%20widespread%20usage%20of%20point%20clouds%20%28PC%29%20for%20immersive%20visual%20applications%0Ahas%20resulted%20in%20the%20use%20of%20very%20heterogeneous%20receiving%20conditions%20and%20devices%2C%0Anotably%20in%20terms%20of%20network%2C%20hardware%2C%20and%20display%20capabilities.%20In%20this%0Ascenario%2C%20quality%20scalability%2C%20i.e.%2C%20the%20ability%20to%20reconstruct%20a%20signal%20at%0Adifferent%20qualities%20by%20progressively%20decoding%20a%20single%20bitstream%2C%20is%20a%20major%0Arequirement%20that%20has%20yet%20to%20be%20conveniently%20addressed%2C%20notably%20in%20most%0Alearning-based%20PC%20coding%20solutions.%20This%20paper%20proposes%20a%20quality%20scalability%0Ascheme%2C%20named%20Scalable%20Quality%20Hyperprior%20%28SQH%29%2C%20adaptable%20to%20learning-based%0Astatic%20point%20cloud%20geometry%20codecs%2C%20which%20uses%20a%20Quality-conditioned%20Latents%0AProbability%20Estimator%20%28QuLPE%29%20to%20decode%20a%20high-quality%20version%20of%20a%20PC%0Alearning-based%20representation%2C%20based%20on%20an%20available%20lower%20quality%20base%20layer.%0ASQH%20is%20integrated%20in%20the%20future%20JPEG%20PC%20coding%20standard%2C%20allowing%20to%20create%20a%0Alayered%20bitstream%20that%20can%20be%20used%20to%20progressively%20decode%20the%20PC%20geometry%20with%0Aincreasing%20quality%20and%20fidelity.%20Experimental%20results%20show%20that%20SQH%20offers%20the%0Aquality%20scalability%20feature%20with%20very%20limited%20or%20no%20compression%20performance%0Apenalty%20at%20all%20when%20compared%20with%20the%20corresponding%20non-scalable%20solution%2C%20thus%0Apreserving%20the%20significant%20compression%20gains%20over%20other%20state-of-the-art%20PC%0Acodecs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07698v1&entry.124074799=Read"},
{"title": "Dealing with Subject Similarity in Differential Morphing Attack\n  Detection", "author": "Nicol\u00f2 Di Domenico and Guido Borghi and Annalisa Franco and Davide Maltoni", "abstract": "  The advent of morphing attacks has posed significant security concerns for\nautomated Face Recognition systems, raising the pressing need for robust and\neffective Morphing Attack Detection (MAD) methods able to effectively address\nthis issue. In this paper, we focus on Differential MAD (D-MAD), where a\ntrusted live capture, usually representing the criminal, is compared with the\ndocument image to classify it as morphed or bona fide. We show these approaches\nbased on identity features are effective when the morphed image and the live\none are sufficiently diverse; unfortunately, the effectiveness is significantly\nreduced when the same approaches are applied to look-alike subjects or in all\nthose cases when the similarity between the two compared images is high (e.g.\ncomparison between the morphed image and the accomplice). Therefore, in this\npaper, we propose ACIdA, a modular D-MAD system, consisting of a module for the\nattempt type classification, and two modules for the identity and artifacts\nanalysis on input images. Successfully addressing this task would allow\nbroadening the D-MAD applications including, for instance, the document\nenrollment stage, which currently relies entirely on human evaluation, thus\nlimiting the possibility of releasing ID documents with manipulated images, as\nwell as the automated gates to detect both accomplices and criminals. An\nextensive cross-dataset experimental evaluation conducted on the introduced\nscenario shows that ACIdA achieves state-of-the-art results, outperforming\nliterature competitors, while maintaining good performance in traditional D-MAD\nbenchmarks.\n", "link": "http://arxiv.org/abs/2404.07667v1", "date": "2024-04-11", "relevancy": 2.0583, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5064}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4899}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dealing%20with%20Subject%20Similarity%20in%20Differential%20Morphing%20Attack%0A%20%20Detection&body=Title%3A%20Dealing%20with%20Subject%20Similarity%20in%20Differential%20Morphing%20Attack%0A%20%20Detection%0AAuthor%3A%20Nicol%C3%B2%20Di%20Domenico%20and%20Guido%20Borghi%20and%20Annalisa%20Franco%20and%20Davide%20Maltoni%0AAbstract%3A%20%20%20The%20advent%20of%20morphing%20attacks%20has%20posed%20significant%20security%20concerns%20for%0Aautomated%20Face%20Recognition%20systems%2C%20raising%20the%20pressing%20need%20for%20robust%20and%0Aeffective%20Morphing%20Attack%20Detection%20%28MAD%29%20methods%20able%20to%20effectively%20address%0Athis%20issue.%20In%20this%20paper%2C%20we%20focus%20on%20Differential%20MAD%20%28D-MAD%29%2C%20where%20a%0Atrusted%20live%20capture%2C%20usually%20representing%20the%20criminal%2C%20is%20compared%20with%20the%0Adocument%20image%20to%20classify%20it%20as%20morphed%20or%20bona%20fide.%20We%20show%20these%20approaches%0Abased%20on%20identity%20features%20are%20effective%20when%20the%20morphed%20image%20and%20the%20live%0Aone%20are%20sufficiently%20diverse%3B%20unfortunately%2C%20the%20effectiveness%20is%20significantly%0Areduced%20when%20the%20same%20approaches%20are%20applied%20to%20look-alike%20subjects%20or%20in%20all%0Athose%20cases%20when%20the%20similarity%20between%20the%20two%20compared%20images%20is%20high%20%28e.g.%0Acomparison%20between%20the%20morphed%20image%20and%20the%20accomplice%29.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20propose%20ACIdA%2C%20a%20modular%20D-MAD%20system%2C%20consisting%20of%20a%20module%20for%20the%0Aattempt%20type%20classification%2C%20and%20two%20modules%20for%20the%20identity%20and%20artifacts%0Aanalysis%20on%20input%20images.%20Successfully%20addressing%20this%20task%20would%20allow%0Abroadening%20the%20D-MAD%20applications%20including%2C%20for%20instance%2C%20the%20document%0Aenrollment%20stage%2C%20which%20currently%20relies%20entirely%20on%20human%20evaluation%2C%20thus%0Alimiting%20the%20possibility%20of%20releasing%20ID%20documents%20with%20manipulated%20images%2C%20as%0Awell%20as%20the%20automated%20gates%20to%20detect%20both%20accomplices%20and%20criminals.%20An%0Aextensive%20cross-dataset%20experimental%20evaluation%20conducted%20on%20the%20introduced%0Ascenario%20shows%20that%20ACIdA%20achieves%20state-of-the-art%20results%2C%20outperforming%0Aliterature%20competitors%2C%20while%20maintaining%20good%20performance%20in%20traditional%20D-MAD%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07667v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dealing%20with%20Subject%20Similarity%20in%20Differential%20Morphing%20Attack%0A%20%20Detection&entry.906535625=Nicol%C3%B2%20Di%20Domenico%20and%20Guido%20Borghi%20and%20Annalisa%20Franco%20and%20Davide%20Maltoni&entry.1292438233=%20%20The%20advent%20of%20morphing%20attacks%20has%20posed%20significant%20security%20concerns%20for%0Aautomated%20Face%20Recognition%20systems%2C%20raising%20the%20pressing%20need%20for%20robust%20and%0Aeffective%20Morphing%20Attack%20Detection%20%28MAD%29%20methods%20able%20to%20effectively%20address%0Athis%20issue.%20In%20this%20paper%2C%20we%20focus%20on%20Differential%20MAD%20%28D-MAD%29%2C%20where%20a%0Atrusted%20live%20capture%2C%20usually%20representing%20the%20criminal%2C%20is%20compared%20with%20the%0Adocument%20image%20to%20classify%20it%20as%20morphed%20or%20bona%20fide.%20We%20show%20these%20approaches%0Abased%20on%20identity%20features%20are%20effective%20when%20the%20morphed%20image%20and%20the%20live%0Aone%20are%20sufficiently%20diverse%3B%20unfortunately%2C%20the%20effectiveness%20is%20significantly%0Areduced%20when%20the%20same%20approaches%20are%20applied%20to%20look-alike%20subjects%20or%20in%20all%0Athose%20cases%20when%20the%20similarity%20between%20the%20two%20compared%20images%20is%20high%20%28e.g.%0Acomparison%20between%20the%20morphed%20image%20and%20the%20accomplice%29.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20propose%20ACIdA%2C%20a%20modular%20D-MAD%20system%2C%20consisting%20of%20a%20module%20for%20the%0Aattempt%20type%20classification%2C%20and%20two%20modules%20for%20the%20identity%20and%20artifacts%0Aanalysis%20on%20input%20images.%20Successfully%20addressing%20this%20task%20would%20allow%0Abroadening%20the%20D-MAD%20applications%20including%2C%20for%20instance%2C%20the%20document%0Aenrollment%20stage%2C%20which%20currently%20relies%20entirely%20on%20human%20evaluation%2C%20thus%0Alimiting%20the%20possibility%20of%20releasing%20ID%20documents%20with%20manipulated%20images%2C%20as%0Awell%20as%20the%20automated%20gates%20to%20detect%20both%20accomplices%20and%20criminals.%20An%0Aextensive%20cross-dataset%20experimental%20evaluation%20conducted%20on%20the%20introduced%0Ascenario%20shows%20that%20ACIdA%20achieves%20state-of-the-art%20results%2C%20outperforming%0Aliterature%20competitors%2C%20while%20maintaining%20good%20performance%20in%20traditional%20D-MAD%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07667v1&entry.124074799=Read"},
{"title": "3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing\n  Surfaces", "author": "Xuanming Cao and Chengyu Tao and Juan Du", "abstract": "  The surface quality inspection of manufacturing parts based on 3D point cloud\ndata has attracted increasing attention in recent years. The reason is that the\n3D point cloud can capture the entire surface of manufacturing parts, unlike\nthe previous practices that focus on some key product characteristics. However,\nachieving accurate 3D anomaly detection is challenging, due to the complex\nsurfaces of manufacturing parts and the difficulty of collecting sufficient\nanomaly samples. To address these challenges, we propose a novel untrained\nanomaly detection method based on 3D point cloud data for complex manufacturing\nparts, which can achieve accurate anomaly detection in a single sample without\ntraining data. In the proposed framework, we transform an input sample into two\nsets of profiles along different directions. Based on one set of the profiles,\na novel segmentation module is devised to segment the complex surface into\nmultiple basic and simple components. In each component, another set of\nprofiles, which have the nature of similar shapes, can be modeled as a low-rank\nmatrix. Thus, accurate 3D anomaly detection can be achieved by using Robust\nPrincipal Component Analysis (RPCA) on these low-rank matrices. Extensive\nnumerical experiments on different types of parts show that our method achieves\npromising results compared with the benchmark methods.\n", "link": "http://arxiv.org/abs/2404.07748v1", "date": "2024-04-11", "relevancy": 2.0524, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5321}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4931}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D-CSAD%3A%20Untrained%203D%20Anomaly%20Detection%20for%20Complex%20Manufacturing%0A%20%20Surfaces&body=Title%3A%203D-CSAD%3A%20Untrained%203D%20Anomaly%20Detection%20for%20Complex%20Manufacturing%0A%20%20Surfaces%0AAuthor%3A%20Xuanming%20Cao%20and%20Chengyu%20Tao%20and%20Juan%20Du%0AAbstract%3A%20%20%20The%20surface%20quality%20inspection%20of%20manufacturing%20parts%20based%20on%203D%20point%20cloud%0Adata%20has%20attracted%20increasing%20attention%20in%20recent%20years.%20The%20reason%20is%20that%20the%0A3D%20point%20cloud%20can%20capture%20the%20entire%20surface%20of%20manufacturing%20parts%2C%20unlike%0Athe%20previous%20practices%20that%20focus%20on%20some%20key%20product%20characteristics.%20However%2C%0Aachieving%20accurate%203D%20anomaly%20detection%20is%20challenging%2C%20due%20to%20the%20complex%0Asurfaces%20of%20manufacturing%20parts%20and%20the%20difficulty%20of%20collecting%20sufficient%0Aanomaly%20samples.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20untrained%0Aanomaly%20detection%20method%20based%20on%203D%20point%20cloud%20data%20for%20complex%20manufacturing%0Aparts%2C%20which%20can%20achieve%20accurate%20anomaly%20detection%20in%20a%20single%20sample%20without%0Atraining%20data.%20In%20the%20proposed%20framework%2C%20we%20transform%20an%20input%20sample%20into%20two%0Asets%20of%20profiles%20along%20different%20directions.%20Based%20on%20one%20set%20of%20the%20profiles%2C%0Aa%20novel%20segmentation%20module%20is%20devised%20to%20segment%20the%20complex%20surface%20into%0Amultiple%20basic%20and%20simple%20components.%20In%20each%20component%2C%20another%20set%20of%0Aprofiles%2C%20which%20have%20the%20nature%20of%20similar%20shapes%2C%20can%20be%20modeled%20as%20a%20low-rank%0Amatrix.%20Thus%2C%20accurate%203D%20anomaly%20detection%20can%20be%20achieved%20by%20using%20Robust%0APrincipal%20Component%20Analysis%20%28RPCA%29%20on%20these%20low-rank%20matrices.%20Extensive%0Anumerical%20experiments%20on%20different%20types%20of%20parts%20show%20that%20our%20method%20achieves%0Apromising%20results%20compared%20with%20the%20benchmark%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07748v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-CSAD%3A%20Untrained%203D%20Anomaly%20Detection%20for%20Complex%20Manufacturing%0A%20%20Surfaces&entry.906535625=Xuanming%20Cao%20and%20Chengyu%20Tao%20and%20Juan%20Du&entry.1292438233=%20%20The%20surface%20quality%20inspection%20of%20manufacturing%20parts%20based%20on%203D%20point%20cloud%0Adata%20has%20attracted%20increasing%20attention%20in%20recent%20years.%20The%20reason%20is%20that%20the%0A3D%20point%20cloud%20can%20capture%20the%20entire%20surface%20of%20manufacturing%20parts%2C%20unlike%0Athe%20previous%20practices%20that%20focus%20on%20some%20key%20product%20characteristics.%20However%2C%0Aachieving%20accurate%203D%20anomaly%20detection%20is%20challenging%2C%20due%20to%20the%20complex%0Asurfaces%20of%20manufacturing%20parts%20and%20the%20difficulty%20of%20collecting%20sufficient%0Aanomaly%20samples.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20untrained%0Aanomaly%20detection%20method%20based%20on%203D%20point%20cloud%20data%20for%20complex%20manufacturing%0Aparts%2C%20which%20can%20achieve%20accurate%20anomaly%20detection%20in%20a%20single%20sample%20without%0Atraining%20data.%20In%20the%20proposed%20framework%2C%20we%20transform%20an%20input%20sample%20into%20two%0Asets%20of%20profiles%20along%20different%20directions.%20Based%20on%20one%20set%20of%20the%20profiles%2C%0Aa%20novel%20segmentation%20module%20is%20devised%20to%20segment%20the%20complex%20surface%20into%0Amultiple%20basic%20and%20simple%20components.%20In%20each%20component%2C%20another%20set%20of%0Aprofiles%2C%20which%20have%20the%20nature%20of%20similar%20shapes%2C%20can%20be%20modeled%20as%20a%20low-rank%0Amatrix.%20Thus%2C%20accurate%203D%20anomaly%20detection%20can%20be%20achieved%20by%20using%20Robust%0APrincipal%20Component%20Analysis%20%28RPCA%29%20on%20these%20low-rank%20matrices.%20Extensive%0Anumerical%20experiments%20on%20different%20types%20of%20parts%20show%20that%20our%20method%20achieves%0Apromising%20results%20compared%20with%20the%20benchmark%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07748v1&entry.124074799=Read"},
{"title": "ViM-UNet: Vision Mamba for Biomedical Segmentation", "author": "Anwai Archit and Constantin Pape", "abstract": "  CNNs, most notably the UNet, are the default architecture for biomedical\nsegmentation. Transformer-based approaches, such as UNETR, have been proposed\nto replace them, benefiting from a global field of view, but suffering from\nlarger runtimes and higher parameter counts. The recent Vision Mamba\narchitecture offers a compelling alternative to transformers, also providing a\nglobal field of view, but at higher efficiency. Here, we introduce ViM-UNet, a\nnovel segmentation architecture based on it and compare it to UNet and UNETR\nfor two challenging microscopy instance segmentation tasks. We find that it\nperforms similarly or better than UNet, depending on the task, and outperforms\nUNETR while being more efficient. Our code is open source and documented at\nhttps://github.com/constantinpape/torch-em/blob/main/vimunet.md.\n", "link": "http://arxiv.org/abs/2404.07705v1", "date": "2024-04-11", "relevancy": 2.0523, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.521}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.482}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ViM-UNet%3A%20Vision%20Mamba%20for%20Biomedical%20Segmentation&body=Title%3A%20ViM-UNet%3A%20Vision%20Mamba%20for%20Biomedical%20Segmentation%0AAuthor%3A%20Anwai%20Archit%20and%20Constantin%20Pape%0AAbstract%3A%20%20%20CNNs%2C%20most%20notably%20the%20UNet%2C%20are%20the%20default%20architecture%20for%20biomedical%0Asegmentation.%20Transformer-based%20approaches%2C%20such%20as%20UNETR%2C%20have%20been%20proposed%0Ato%20replace%20them%2C%20benefiting%20from%20a%20global%20field%20of%20view%2C%20but%20suffering%20from%0Alarger%20runtimes%20and%20higher%20parameter%20counts.%20The%20recent%20Vision%20Mamba%0Aarchitecture%20offers%20a%20compelling%20alternative%20to%20transformers%2C%20also%20providing%20a%0Aglobal%20field%20of%20view%2C%20but%20at%20higher%20efficiency.%20Here%2C%20we%20introduce%20ViM-UNet%2C%20a%0Anovel%20segmentation%20architecture%20based%20on%20it%20and%20compare%20it%20to%20UNet%20and%20UNETR%0Afor%20two%20challenging%20microscopy%20instance%20segmentation%20tasks.%20We%20find%20that%20it%0Aperforms%20similarly%20or%20better%20than%20UNet%2C%20depending%20on%20the%20task%2C%20and%20outperforms%0AUNETR%20while%20being%20more%20efficient.%20Our%20code%20is%20open%20source%20and%20documented%20at%0Ahttps%3A//github.com/constantinpape/torch-em/blob/main/vimunet.md.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07705v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViM-UNet%3A%20Vision%20Mamba%20for%20Biomedical%20Segmentation&entry.906535625=Anwai%20Archit%20and%20Constantin%20Pape&entry.1292438233=%20%20CNNs%2C%20most%20notably%20the%20UNet%2C%20are%20the%20default%20architecture%20for%20biomedical%0Asegmentation.%20Transformer-based%20approaches%2C%20such%20as%20UNETR%2C%20have%20been%20proposed%0Ato%20replace%20them%2C%20benefiting%20from%20a%20global%20field%20of%20view%2C%20but%20suffering%20from%0Alarger%20runtimes%20and%20higher%20parameter%20counts.%20The%20recent%20Vision%20Mamba%0Aarchitecture%20offers%20a%20compelling%20alternative%20to%20transformers%2C%20also%20providing%20a%0Aglobal%20field%20of%20view%2C%20but%20at%20higher%20efficiency.%20Here%2C%20we%20introduce%20ViM-UNet%2C%20a%0Anovel%20segmentation%20architecture%20based%20on%20it%20and%20compare%20it%20to%20UNet%20and%20UNETR%0Afor%20two%20challenging%20microscopy%20instance%20segmentation%20tasks.%20We%20find%20that%20it%0Aperforms%20similarly%20or%20better%20than%20UNet%2C%20depending%20on%20the%20task%2C%20and%20outperforms%0AUNETR%20while%20being%20more%20efficient.%20Our%20code%20is%20open%20source%20and%20documented%20at%0Ahttps%3A//github.com/constantinpape/torch-em/blob/main/vimunet.md.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07705v1&entry.124074799=Read"},
{"title": "VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving\n  Zero-Shot Voice Editing", "author": "Philip Anastassiou and Zhenyu Tang and Kainan Peng and Dongya Jia and Jiaxin Li and Ming Tu and Yuping Wang and Yuxuan Wang and Mingbo Ma", "abstract": "  We present VoiceShop, a novel speech-to-speech framework that can modify\nmultiple attributes of speech, such as age, gender, accent, and speech style,\nin a single forward pass while preserving the input speaker's timbre. Previous\nworks have been constrained to specialized models that can only edit these\nattributes individually and suffer from the following pitfalls: the magnitude\nof the conversion effect is weak, there is no zero-shot capability for\nout-of-distribution speakers, or the synthesized outputs exhibit undesirable\ntimbre leakage. Our work proposes solutions for each of these issues in a\nsimple modular framework based on a conditional diffusion backbone model with\noptional normalizing flow-based and sequence-to-sequence speaker\nattribute-editing modules, whose components can be combined or removed during\ninference to meet a wide array of tasks without additional model finetuning.\nAudio samples are available at \\url{https://voiceshopai.github.io}.\n", "link": "http://arxiv.org/abs/2404.06674v2", "date": "2024-04-11", "relevancy": 2.0475, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5483}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5314}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4777}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VoiceShop%3A%20A%20Unified%20Speech-to-Speech%20Framework%20for%20Identity-Preserving%0A%20%20Zero-Shot%20Voice%20Editing&body=Title%3A%20VoiceShop%3A%20A%20Unified%20Speech-to-Speech%20Framework%20for%20Identity-Preserving%0A%20%20Zero-Shot%20Voice%20Editing%0AAuthor%3A%20Philip%20Anastassiou%20and%20Zhenyu%20Tang%20and%20Kainan%20Peng%20and%20Dongya%20Jia%20and%20Jiaxin%20Li%20and%20Ming%20Tu%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Mingbo%20Ma%0AAbstract%3A%20%20%20We%20present%20VoiceShop%2C%20a%20novel%20speech-to-speech%20framework%20that%20can%20modify%0Amultiple%20attributes%20of%20speech%2C%20such%20as%20age%2C%20gender%2C%20accent%2C%20and%20speech%20style%2C%0Ain%20a%20single%20forward%20pass%20while%20preserving%20the%20input%20speaker%27s%20timbre.%20Previous%0Aworks%20have%20been%20constrained%20to%20specialized%20models%20that%20can%20only%20edit%20these%0Aattributes%20individually%20and%20suffer%20from%20the%20following%20pitfalls%3A%20the%20magnitude%0Aof%20the%20conversion%20effect%20is%20weak%2C%20there%20is%20no%20zero-shot%20capability%20for%0Aout-of-distribution%20speakers%2C%20or%20the%20synthesized%20outputs%20exhibit%20undesirable%0Atimbre%20leakage.%20Our%20work%20proposes%20solutions%20for%20each%20of%20these%20issues%20in%20a%0Asimple%20modular%20framework%20based%20on%20a%20conditional%20diffusion%20backbone%20model%20with%0Aoptional%20normalizing%20flow-based%20and%20sequence-to-sequence%20speaker%0Aattribute-editing%20modules%2C%20whose%20components%20can%20be%20combined%20or%20removed%20during%0Ainference%20to%20meet%20a%20wide%20array%20of%20tasks%20without%20additional%20model%20finetuning.%0AAudio%20samples%20are%20available%20at%20%5Curl%7Bhttps%3A//voiceshopai.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06674v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoiceShop%3A%20A%20Unified%20Speech-to-Speech%20Framework%20for%20Identity-Preserving%0A%20%20Zero-Shot%20Voice%20Editing&entry.906535625=Philip%20Anastassiou%20and%20Zhenyu%20Tang%20and%20Kainan%20Peng%20and%20Dongya%20Jia%20and%20Jiaxin%20Li%20and%20Ming%20Tu%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Mingbo%20Ma&entry.1292438233=%20%20We%20present%20VoiceShop%2C%20a%20novel%20speech-to-speech%20framework%20that%20can%20modify%0Amultiple%20attributes%20of%20speech%2C%20such%20as%20age%2C%20gender%2C%20accent%2C%20and%20speech%20style%2C%0Ain%20a%20single%20forward%20pass%20while%20preserving%20the%20input%20speaker%27s%20timbre.%20Previous%0Aworks%20have%20been%20constrained%20to%20specialized%20models%20that%20can%20only%20edit%20these%0Aattributes%20individually%20and%20suffer%20from%20the%20following%20pitfalls%3A%20the%20magnitude%0Aof%20the%20conversion%20effect%20is%20weak%2C%20there%20is%20no%20zero-shot%20capability%20for%0Aout-of-distribution%20speakers%2C%20or%20the%20synthesized%20outputs%20exhibit%20undesirable%0Atimbre%20leakage.%20Our%20work%20proposes%20solutions%20for%20each%20of%20these%20issues%20in%20a%0Asimple%20modular%20framework%20based%20on%20a%20conditional%20diffusion%20backbone%20model%20with%0Aoptional%20normalizing%20flow-based%20and%20sequence-to-sequence%20speaker%0Aattribute-editing%20modules%2C%20whose%20components%20can%20be%20combined%20or%20removed%20during%0Ainference%20to%20meet%20a%20wide%20array%20of%20tasks%20without%20additional%20model%20finetuning.%0AAudio%20samples%20are%20available%20at%20%5Curl%7Bhttps%3A//voiceshopai.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06674v2&entry.124074799=Read"},
{"title": "A Deep Learning Method for Simultaneous Denoising and Missing Wedge\n  Reconstruction in Cryogenic Electron Tomography", "author": "Simon Wiedemann and Reinhard Heckel", "abstract": "  Cryogenic electron tomography is a technique for imaging biological samples\nin 3D. A microscope collects a series of 2D projections of the sample, and the\ngoal is to reconstruct the 3D density of the sample called the tomogram.\nReconstruction is difficult as the 2D projections are noisy and can not be\nrecorded from all directions, resulting in a missing wedge of information.\nTomograms conventionally reconstructed with filtered back-projection suffer\nfrom noise and strong artifacts due to the missing wedge. Here, we propose a\ndeep-learning approach for simultaneous denoising and missing wedge\nreconstruction called DeepDeWedge. The algorithm requires no ground truth data\nand is based on fitting a neural network to the 2D projections using a\nself-supervised loss. DeepDeWedge performs better than CryoCARE and IsoNet,\nwhich are state-of-the-art methods for denoising and missing wedge\nreconstruction, and similarly and, in some cases, better than the combination\nof the two methods. At the same time, DeepDeWedge is simpler than this two-step\napproach, as it does denoising and missing wedge reconstruction simultaneously\nrather than sequentially.\n", "link": "http://arxiv.org/abs/2311.05539v2", "date": "2024-04-11", "relevancy": 2.0336, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5204}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4969}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Learning%20Method%20for%20Simultaneous%20Denoising%20and%20Missing%20Wedge%0A%20%20Reconstruction%20in%20Cryogenic%20Electron%20Tomography&body=Title%3A%20A%20Deep%20Learning%20Method%20for%20Simultaneous%20Denoising%20and%20Missing%20Wedge%0A%20%20Reconstruction%20in%20Cryogenic%20Electron%20Tomography%0AAuthor%3A%20Simon%20Wiedemann%20and%20Reinhard%20Heckel%0AAbstract%3A%20%20%20Cryogenic%20electron%20tomography%20is%20a%20technique%20for%20imaging%20biological%20samples%0Ain%203D.%20A%20microscope%20collects%20a%20series%20of%202D%20projections%20of%20the%20sample%2C%20and%20the%0Agoal%20is%20to%20reconstruct%20the%203D%20density%20of%20the%20sample%20called%20the%20tomogram.%0AReconstruction%20is%20difficult%20as%20the%202D%20projections%20are%20noisy%20and%20can%20not%20be%0Arecorded%20from%20all%20directions%2C%20resulting%20in%20a%20missing%20wedge%20of%20information.%0ATomograms%20conventionally%20reconstructed%20with%20filtered%20back-projection%20suffer%0Afrom%20noise%20and%20strong%20artifacts%20due%20to%20the%20missing%20wedge.%20Here%2C%20we%20propose%20a%0Adeep-learning%20approach%20for%20simultaneous%20denoising%20and%20missing%20wedge%0Areconstruction%20called%20DeepDeWedge.%20The%20algorithm%20requires%20no%20ground%20truth%20data%0Aand%20is%20based%20on%20fitting%20a%20neural%20network%20to%20the%202D%20projections%20using%20a%0Aself-supervised%20loss.%20DeepDeWedge%20performs%20better%20than%20CryoCARE%20and%20IsoNet%2C%0Awhich%20are%20state-of-the-art%20methods%20for%20denoising%20and%20missing%20wedge%0Areconstruction%2C%20and%20similarly%20and%2C%20in%20some%20cases%2C%20better%20than%20the%20combination%0Aof%20the%20two%20methods.%20At%20the%20same%20time%2C%20DeepDeWedge%20is%20simpler%20than%20this%20two-step%0Aapproach%2C%20as%20it%20does%20denoising%20and%20missing%20wedge%20reconstruction%20simultaneously%0Arather%20than%20sequentially.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05539v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Learning%20Method%20for%20Simultaneous%20Denoising%20and%20Missing%20Wedge%0A%20%20Reconstruction%20in%20Cryogenic%20Electron%20Tomography&entry.906535625=Simon%20Wiedemann%20and%20Reinhard%20Heckel&entry.1292438233=%20%20Cryogenic%20electron%20tomography%20is%20a%20technique%20for%20imaging%20biological%20samples%0Ain%203D.%20A%20microscope%20collects%20a%20series%20of%202D%20projections%20of%20the%20sample%2C%20and%20the%0Agoal%20is%20to%20reconstruct%20the%203D%20density%20of%20the%20sample%20called%20the%20tomogram.%0AReconstruction%20is%20difficult%20as%20the%202D%20projections%20are%20noisy%20and%20can%20not%20be%0Arecorded%20from%20all%20directions%2C%20resulting%20in%20a%20missing%20wedge%20of%20information.%0ATomograms%20conventionally%20reconstructed%20with%20filtered%20back-projection%20suffer%0Afrom%20noise%20and%20strong%20artifacts%20due%20to%20the%20missing%20wedge.%20Here%2C%20we%20propose%20a%0Adeep-learning%20approach%20for%20simultaneous%20denoising%20and%20missing%20wedge%0Areconstruction%20called%20DeepDeWedge.%20The%20algorithm%20requires%20no%20ground%20truth%20data%0Aand%20is%20based%20on%20fitting%20a%20neural%20network%20to%20the%202D%20projections%20using%20a%0Aself-supervised%20loss.%20DeepDeWedge%20performs%20better%20than%20CryoCARE%20and%20IsoNet%2C%0Awhich%20are%20state-of-the-art%20methods%20for%20denoising%20and%20missing%20wedge%0Areconstruction%2C%20and%20similarly%20and%2C%20in%20some%20cases%2C%20better%20than%20the%20combination%0Aof%20the%20two%20methods.%20At%20the%20same%20time%2C%20DeepDeWedge%20is%20simpler%20than%20this%20two-step%0Aapproach%2C%20as%20it%20does%20denoising%20and%20missing%20wedge%20reconstruction%20simultaneously%0Arather%20than%20sequentially.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05539v2&entry.124074799=Read"},
{"title": "OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic\n  Segmentation of Underground Utilities", "author": "Lasse H. Hansen and Simon B. Jensen and Mark P. Philipsen and Andreas M\u00f8gelmose and Lars Bodum and Thomas B. Moeslund", "abstract": "  Identifying and classifying underground utilities is an important task for\nefficient and effective urban planning and infrastructure maintenance. We\npresent OpenTrench3D, a novel and comprehensive 3D Semantic Segmentation point\ncloud dataset, designed to advance research and development in underground\nutility surveying and mapping. OpenTrench3D covers a completely novel domain\nfor public 3D point cloud datasets and is unique in its focus, scope, and\ncost-effective capturing method. The dataset consists of 310 point clouds\ncollected across 7 distinct areas. These include 5 water utility areas and 2\ndistrict heating utility areas. The inclusion of different geographical areas\nand main utilities (water and district heating utilities) makes OpenTrench3D\nparticularly valuable for inter-domain transfer learning experiments. We\nprovide benchmark results for the dataset using three state-of-the-art semantic\nsegmentation models, PointNeXt, PointVector and PointMetaBase. Benchmarks are\nconducted by training on data from water areas, fine-tuning on district heating\narea 1 and evaluating on district heating area 2. The dataset is publicly\navailable. With OpenTrench3D, we seek to foster innovation and progress in the\nfield of 3D semantic segmentation in applications related to detection and\ndocumentation of underground utilities as well as in transfer learning methods\nin general.\n", "link": "http://arxiv.org/abs/2404.07711v1", "date": "2024-04-11", "relevancy": 2.0263, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5163}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5125}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OpenTrench3D%3A%20A%20Photogrammetric%203D%20Point%20Cloud%20Dataset%20for%20Semantic%0A%20%20Segmentation%20of%20Underground%20Utilities&body=Title%3A%20OpenTrench3D%3A%20A%20Photogrammetric%203D%20Point%20Cloud%20Dataset%20for%20Semantic%0A%20%20Segmentation%20of%20Underground%20Utilities%0AAuthor%3A%20Lasse%20H.%20Hansen%20and%20Simon%20B.%20Jensen%20and%20Mark%20P.%20Philipsen%20and%20Andreas%20M%C3%B8gelmose%20and%20Lars%20Bodum%20and%20Thomas%20B.%20Moeslund%0AAbstract%3A%20%20%20Identifying%20and%20classifying%20underground%20utilities%20is%20an%20important%20task%20for%0Aefficient%20and%20effective%20urban%20planning%20and%20infrastructure%20maintenance.%20We%0Apresent%20OpenTrench3D%2C%20a%20novel%20and%20comprehensive%203D%20Semantic%20Segmentation%20point%0Acloud%20dataset%2C%20designed%20to%20advance%20research%20and%20development%20in%20underground%0Autility%20surveying%20and%20mapping.%20OpenTrench3D%20covers%20a%20completely%20novel%20domain%0Afor%20public%203D%20point%20cloud%20datasets%20and%20is%20unique%20in%20its%20focus%2C%20scope%2C%20and%0Acost-effective%20capturing%20method.%20The%20dataset%20consists%20of%20310%20point%20clouds%0Acollected%20across%207%20distinct%20areas.%20These%20include%205%20water%20utility%20areas%20and%202%0Adistrict%20heating%20utility%20areas.%20The%20inclusion%20of%20different%20geographical%20areas%0Aand%20main%20utilities%20%28water%20and%20district%20heating%20utilities%29%20makes%20OpenTrench3D%0Aparticularly%20valuable%20for%20inter-domain%20transfer%20learning%20experiments.%20We%0Aprovide%20benchmark%20results%20for%20the%20dataset%20using%20three%20state-of-the-art%20semantic%0Asegmentation%20models%2C%20PointNeXt%2C%20PointVector%20and%20PointMetaBase.%20Benchmarks%20are%0Aconducted%20by%20training%20on%20data%20from%20water%20areas%2C%20fine-tuning%20on%20district%20heating%0Aarea%201%20and%20evaluating%20on%20district%20heating%20area%202.%20The%20dataset%20is%20publicly%0Aavailable.%20With%20OpenTrench3D%2C%20we%20seek%20to%20foster%20innovation%20and%20progress%20in%20the%0Afield%20of%203D%20semantic%20segmentation%20in%20applications%20related%20to%20detection%20and%0Adocumentation%20of%20underground%20utilities%20as%20well%20as%20in%20transfer%20learning%20methods%0Ain%20general.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07711v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenTrench3D%3A%20A%20Photogrammetric%203D%20Point%20Cloud%20Dataset%20for%20Semantic%0A%20%20Segmentation%20of%20Underground%20Utilities&entry.906535625=Lasse%20H.%20Hansen%20and%20Simon%20B.%20Jensen%20and%20Mark%20P.%20Philipsen%20and%20Andreas%20M%C3%B8gelmose%20and%20Lars%20Bodum%20and%20Thomas%20B.%20Moeslund&entry.1292438233=%20%20Identifying%20and%20classifying%20underground%20utilities%20is%20an%20important%20task%20for%0Aefficient%20and%20effective%20urban%20planning%20and%20infrastructure%20maintenance.%20We%0Apresent%20OpenTrench3D%2C%20a%20novel%20and%20comprehensive%203D%20Semantic%20Segmentation%20point%0Acloud%20dataset%2C%20designed%20to%20advance%20research%20and%20development%20in%20underground%0Autility%20surveying%20and%20mapping.%20OpenTrench3D%20covers%20a%20completely%20novel%20domain%0Afor%20public%203D%20point%20cloud%20datasets%20and%20is%20unique%20in%20its%20focus%2C%20scope%2C%20and%0Acost-effective%20capturing%20method.%20The%20dataset%20consists%20of%20310%20point%20clouds%0Acollected%20across%207%20distinct%20areas.%20These%20include%205%20water%20utility%20areas%20and%202%0Adistrict%20heating%20utility%20areas.%20The%20inclusion%20of%20different%20geographical%20areas%0Aand%20main%20utilities%20%28water%20and%20district%20heating%20utilities%29%20makes%20OpenTrench3D%0Aparticularly%20valuable%20for%20inter-domain%20transfer%20learning%20experiments.%20We%0Aprovide%20benchmark%20results%20for%20the%20dataset%20using%20three%20state-of-the-art%20semantic%0Asegmentation%20models%2C%20PointNeXt%2C%20PointVector%20and%20PointMetaBase.%20Benchmarks%20are%0Aconducted%20by%20training%20on%20data%20from%20water%20areas%2C%20fine-tuning%20on%20district%20heating%0Aarea%201%20and%20evaluating%20on%20district%20heating%20area%202.%20The%20dataset%20is%20publicly%0Aavailable.%20With%20OpenTrench3D%2C%20we%20seek%20to%20foster%20innovation%20and%20progress%20in%20the%0Afield%20of%203D%20semantic%20segmentation%20in%20applications%20related%20to%20detection%20and%0Adocumentation%20of%20underground%20utilities%20as%20well%20as%20in%20transfer%20learning%20methods%0Ain%20general.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07711v1&entry.124074799=Read"},
{"title": "Neural population geometry and optimal coding of tasks with shared\n  latent structure", "author": "Albert J. Wakhloo and Will Slatton and SueYeon Chung", "abstract": "  Humans and animals can recognize latent structures in their environment and\napply this information to efficiently navigate the world. However, it remains\nunclear what aspects of neural activity contribute to these computational\ncapabilities. Here, we develop an analytical theory linking the geometry of a\nneural population's activity to the generalization performance of a linear\nreadout on a set of tasks that depend on a common latent structure. We show\nthat four geometric measures of the activity determine performance across\ntasks. Using this theory, we find that experimentally observed disentangled\nrepresentations naturally emerge as an optimal solution to the multi-task\nlearning problem. When data is scarce, these optimal neural codes compress less\ninformative latent variables, and when data is abundant, they expand these\nvariables in the state space. We validate our theory using macaque ventral\nstream recordings. Our results therefore tie population geometry to multi-task\nlearning.\n", "link": "http://arxiv.org/abs/2402.16770v2", "date": "2024-04-11", "relevancy": 2.0214, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5109}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4989}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20population%20geometry%20and%20optimal%20coding%20of%20tasks%20with%20shared%0A%20%20latent%20structure&body=Title%3A%20Neural%20population%20geometry%20and%20optimal%20coding%20of%20tasks%20with%20shared%0A%20%20latent%20structure%0AAuthor%3A%20Albert%20J.%20Wakhloo%20and%20Will%20Slatton%20and%20SueYeon%20Chung%0AAbstract%3A%20%20%20Humans%20and%20animals%20can%20recognize%20latent%20structures%20in%20their%20environment%20and%0Aapply%20this%20information%20to%20efficiently%20navigate%20the%20world.%20However%2C%20it%20remains%0Aunclear%20what%20aspects%20of%20neural%20activity%20contribute%20to%20these%20computational%0Acapabilities.%20Here%2C%20we%20develop%20an%20analytical%20theory%20linking%20the%20geometry%20of%20a%0Aneural%20population%27s%20activity%20to%20the%20generalization%20performance%20of%20a%20linear%0Areadout%20on%20a%20set%20of%20tasks%20that%20depend%20on%20a%20common%20latent%20structure.%20We%20show%0Athat%20four%20geometric%20measures%20of%20the%20activity%20determine%20performance%20across%0Atasks.%20Using%20this%20theory%2C%20we%20find%20that%20experimentally%20observed%20disentangled%0Arepresentations%20naturally%20emerge%20as%20an%20optimal%20solution%20to%20the%20multi-task%0Alearning%20problem.%20When%20data%20is%20scarce%2C%20these%20optimal%20neural%20codes%20compress%20less%0Ainformative%20latent%20variables%2C%20and%20when%20data%20is%20abundant%2C%20they%20expand%20these%0Avariables%20in%20the%20state%20space.%20We%20validate%20our%20theory%20using%20macaque%20ventral%0Astream%20recordings.%20Our%20results%20therefore%20tie%20population%20geometry%20to%20multi-task%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16770v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20population%20geometry%20and%20optimal%20coding%20of%20tasks%20with%20shared%0A%20%20latent%20structure&entry.906535625=Albert%20J.%20Wakhloo%20and%20Will%20Slatton%20and%20SueYeon%20Chung&entry.1292438233=%20%20Humans%20and%20animals%20can%20recognize%20latent%20structures%20in%20their%20environment%20and%0Aapply%20this%20information%20to%20efficiently%20navigate%20the%20world.%20However%2C%20it%20remains%0Aunclear%20what%20aspects%20of%20neural%20activity%20contribute%20to%20these%20computational%0Acapabilities.%20Here%2C%20we%20develop%20an%20analytical%20theory%20linking%20the%20geometry%20of%20a%0Aneural%20population%27s%20activity%20to%20the%20generalization%20performance%20of%20a%20linear%0Areadout%20on%20a%20set%20of%20tasks%20that%20depend%20on%20a%20common%20latent%20structure.%20We%20show%0Athat%20four%20geometric%20measures%20of%20the%20activity%20determine%20performance%20across%0Atasks.%20Using%20this%20theory%2C%20we%20find%20that%20experimentally%20observed%20disentangled%0Arepresentations%20naturally%20emerge%20as%20an%20optimal%20solution%20to%20the%20multi-task%0Alearning%20problem.%20When%20data%20is%20scarce%2C%20these%20optimal%20neural%20codes%20compress%20less%0Ainformative%20latent%20variables%2C%20and%20when%20data%20is%20abundant%2C%20they%20expand%20these%0Avariables%20in%20the%20state%20space.%20We%20validate%20our%20theory%20using%20macaque%20ventral%0Astream%20recordings.%20Our%20results%20therefore%20tie%20population%20geometry%20to%20multi-task%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16770v2&entry.124074799=Read"},
{"title": "Post-Hoc Reversal: Are We Selecting Models Prematurely?", "author": "Rishabh Ranjan and Saurabh Garg and Mrigank Raman and Carlos Guestrin and Zachary Chase Lipton", "abstract": "  Trained models are often composed with post-hoc transforms such as\ntemperature scaling (TS), ensembling and stochastic weight averaging (SWA) to\nimprove performance, robustness, uncertainty estimation, etc. However, such\ntransforms are typically applied only after the base models have already been\nfinalized by standard means. In this paper, we challenge this practice with an\nextensive empirical study. In particular, we demonstrate a phenomenon that we\ncall post-hoc reversal, where performance trends are reversed after applying\nthese post-hoc transforms. This phenomenon is especially prominent in\nhigh-noise settings. For example, while base models overfit badly early in\ntraining, both conventional ensembling and SWA favor base models trained for\nmore epochs. Post-hoc reversal can also suppress the appearance of double\ndescent and mitigate mismatches between test loss and test error seen in base\nmodels. Based on our findings, we propose post-hoc selection, a simple\ntechnique whereby post-hoc metrics inform model development decisions such as\nearly stopping, checkpointing, and broader hyperparameter choices. Our\nexperimental analyses span real-world vision, language, tabular and graph\ndatasets from domains like satellite imaging, language modeling, census\nprediction and social network analysis. On an LLM instruction tuning dataset,\npost-hoc selection results in > 1.5x MMLU improvement compared to naive\nselection. Code is available at\nhttps://github.com/rishabh-ranjan/post-hoc-reversal.\n", "link": "http://arxiv.org/abs/2404.07815v1", "date": "2024-04-11", "relevancy": 2.0207, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5386}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5003}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4967}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Post-Hoc%20Reversal%3A%20Are%20We%20Selecting%20Models%20Prematurely%3F&body=Title%3A%20Post-Hoc%20Reversal%3A%20Are%20We%20Selecting%20Models%20Prematurely%3F%0AAuthor%3A%20Rishabh%20Ranjan%20and%20Saurabh%20Garg%20and%20Mrigank%20Raman%20and%20Carlos%20Guestrin%20and%20Zachary%20Chase%20Lipton%0AAbstract%3A%20%20%20Trained%20models%20are%20often%20composed%20with%20post-hoc%20transforms%20such%20as%0Atemperature%20scaling%20%28TS%29%2C%20ensembling%20and%20stochastic%20weight%20averaging%20%28SWA%29%20to%0Aimprove%20performance%2C%20robustness%2C%20uncertainty%20estimation%2C%20etc.%20However%2C%20such%0Atransforms%20are%20typically%20applied%20only%20after%20the%20base%20models%20have%20already%20been%0Afinalized%20by%20standard%20means.%20In%20this%20paper%2C%20we%20challenge%20this%20practice%20with%20an%0Aextensive%20empirical%20study.%20In%20particular%2C%20we%20demonstrate%20a%20phenomenon%20that%20we%0Acall%20post-hoc%20reversal%2C%20where%20performance%20trends%20are%20reversed%20after%20applying%0Athese%20post-hoc%20transforms.%20This%20phenomenon%20is%20especially%20prominent%20in%0Ahigh-noise%20settings.%20For%20example%2C%20while%20base%20models%20overfit%20badly%20early%20in%0Atraining%2C%20both%20conventional%20ensembling%20and%20SWA%20favor%20base%20models%20trained%20for%0Amore%20epochs.%20Post-hoc%20reversal%20can%20also%20suppress%20the%20appearance%20of%20double%0Adescent%20and%20mitigate%20mismatches%20between%20test%20loss%20and%20test%20error%20seen%20in%20base%0Amodels.%20Based%20on%20our%20findings%2C%20we%20propose%20post-hoc%20selection%2C%20a%20simple%0Atechnique%20whereby%20post-hoc%20metrics%20inform%20model%20development%20decisions%20such%20as%0Aearly%20stopping%2C%20checkpointing%2C%20and%20broader%20hyperparameter%20choices.%20Our%0Aexperimental%20analyses%20span%20real-world%20vision%2C%20language%2C%20tabular%20and%20graph%0Adatasets%20from%20domains%20like%20satellite%20imaging%2C%20language%20modeling%2C%20census%0Aprediction%20and%20social%20network%20analysis.%20On%20an%20LLM%20instruction%20tuning%20dataset%2C%0Apost-hoc%20selection%20results%20in%20%3E%201.5x%20MMLU%20improvement%20compared%20to%20naive%0Aselection.%20Code%20is%20available%20at%0Ahttps%3A//github.com/rishabh-ranjan/post-hoc-reversal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07815v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-Hoc%20Reversal%3A%20Are%20We%20Selecting%20Models%20Prematurely%3F&entry.906535625=Rishabh%20Ranjan%20and%20Saurabh%20Garg%20and%20Mrigank%20Raman%20and%20Carlos%20Guestrin%20and%20Zachary%20Chase%20Lipton&entry.1292438233=%20%20Trained%20models%20are%20often%20composed%20with%20post-hoc%20transforms%20such%20as%0Atemperature%20scaling%20%28TS%29%2C%20ensembling%20and%20stochastic%20weight%20averaging%20%28SWA%29%20to%0Aimprove%20performance%2C%20robustness%2C%20uncertainty%20estimation%2C%20etc.%20However%2C%20such%0Atransforms%20are%20typically%20applied%20only%20after%20the%20base%20models%20have%20already%20been%0Afinalized%20by%20standard%20means.%20In%20this%20paper%2C%20we%20challenge%20this%20practice%20with%20an%0Aextensive%20empirical%20study.%20In%20particular%2C%20we%20demonstrate%20a%20phenomenon%20that%20we%0Acall%20post-hoc%20reversal%2C%20where%20performance%20trends%20are%20reversed%20after%20applying%0Athese%20post-hoc%20transforms.%20This%20phenomenon%20is%20especially%20prominent%20in%0Ahigh-noise%20settings.%20For%20example%2C%20while%20base%20models%20overfit%20badly%20early%20in%0Atraining%2C%20both%20conventional%20ensembling%20and%20SWA%20favor%20base%20models%20trained%20for%0Amore%20epochs.%20Post-hoc%20reversal%20can%20also%20suppress%20the%20appearance%20of%20double%0Adescent%20and%20mitigate%20mismatches%20between%20test%20loss%20and%20test%20error%20seen%20in%20base%0Amodels.%20Based%20on%20our%20findings%2C%20we%20propose%20post-hoc%20selection%2C%20a%20simple%0Atechnique%20whereby%20post-hoc%20metrics%20inform%20model%20development%20decisions%20such%20as%0Aearly%20stopping%2C%20checkpointing%2C%20and%20broader%20hyperparameter%20choices.%20Our%0Aexperimental%20analyses%20span%20real-world%20vision%2C%20language%2C%20tabular%20and%20graph%0Adatasets%20from%20domains%20like%20satellite%20imaging%2C%20language%20modeling%2C%20census%0Aprediction%20and%20social%20network%20analysis.%20On%20an%20LLM%20instruction%20tuning%20dataset%2C%0Apost-hoc%20selection%20results%20in%20%3E%201.5x%20MMLU%20improvement%20compared%20to%20naive%0Aselection.%20Code%20is%20available%20at%0Ahttps%3A//github.com/rishabh-ranjan/post-hoc-reversal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07815v1&entry.124074799=Read"},
{"title": "RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models", "author": "Aleksandar Botev and Soham De and Samuel L Smith and Anushan Fernando and George-Cristian Muraru and Ruba Haroun and Leonard Berrada and Razvan Pascanu and Pier Giuseppe Sessa and Robert Dadashi and L\u00e9onard Hussenot and Johan Ferret and Sertan Girgin and Olivier Bachem and Alek Andreev and Kathleen Kenealy and Thomas Mesnard and Cassidy Hardin and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivi\u00e8re and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Armand Joulin and Noah Fiedel and Evan Senter and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and David Budden and Arnaud Doucet and Sharad Vikram and Adam Paszke and Trevor Gale and Sebastian Borgeaud and Charlie Chen and Andy Brock and Antonia Paterson and Jenny Brennan and Meg Risdal and Raj Gundluru and Nesh Devanathan and Paul Mooney and Nilay Chauhan and Phil Culliton and Luiz GUStavo Martins and Elisa Bandy and David Huntsperger and Glenn Cameron and Arthur Zucker and Tris Warkentin and Ludovic Peran and Minh Giang and Zoubin Ghahramani and Cl\u00e9ment Farabet and Koray Kavukcuoglu and Demis Hassabis and Raia Hadsell and Yee Whye Teh and Nando de Frietas", "abstract": "  We introduce RecurrentGemma, an open language model which uses Google's novel\nGriffin architecture. Griffin combines linear recurrences with local attention\nto achieve excellent performance on language. It has a fixed-sized state, which\nreduces memory use and enables efficient inference on long sequences. We\nprovide a pre-trained model with 2B non-embedding parameters, and an\ninstruction tuned variant. Both models achieve comparable performance to\nGemma-2B despite being trained on fewer tokens.\n", "link": "http://arxiv.org/abs/2404.07839v1", "date": "2024-04-11", "relevancy": 2.0121, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5123}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5011}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4845}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RecurrentGemma%3A%20Moving%20Past%20Transformers%20for%20Efficient%20Open%20Language%0A%20%20Models&body=Title%3A%20RecurrentGemma%3A%20Moving%20Past%20Transformers%20for%20Efficient%20Open%20Language%0A%20%20Models%0AAuthor%3A%20Aleksandar%20Botev%20and%20Soham%20De%20and%20Samuel%20L%20Smith%20and%20Anushan%20Fernando%20and%20George-Cristian%20Muraru%20and%20Ruba%20Haroun%20and%20Leonard%20Berrada%20and%20Razvan%20Pascanu%20and%20Pier%20Giuseppe%20Sessa%20and%20Robert%20Dadashi%20and%20L%C3%A9onard%20Hussenot%20and%20Johan%20Ferret%20and%20Sertan%20Girgin%20and%20Olivier%20Bachem%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Yutian%20Chen%20and%20Srivatsan%20Srinivasan%20and%20Guillaume%20Desjardins%20and%20David%20Budden%20and%20Arnaud%20Doucet%20and%20Sharad%20Vikram%20and%20Adam%20Paszke%20and%20Trevor%20Gale%20and%20Sebastian%20Borgeaud%20and%20Charlie%20Chen%20and%20Andy%20Brock%20and%20Antonia%20Paterson%20and%20Jenny%20Brennan%20and%20Meg%20Risdal%20and%20Raj%20Gundluru%20and%20Nesh%20Devanathan%20and%20Paul%20Mooney%20and%20Nilay%20Chauhan%20and%20Phil%20Culliton%20and%20Luiz%20GUStavo%20Martins%20and%20Elisa%20Bandy%20and%20David%20Huntsperger%20and%20Glenn%20Cameron%20and%20Arthur%20Zucker%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Zoubin%20Ghahramani%20and%20Cl%C3%A9ment%20Farabet%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Raia%20Hadsell%20and%20Yee%20Whye%20Teh%20and%20Nando%20de%20Frietas%0AAbstract%3A%20%20%20We%20introduce%20RecurrentGemma%2C%20an%20open%20language%20model%20which%20uses%20Google%27s%20novel%0AGriffin%20architecture.%20Griffin%20combines%20linear%20recurrences%20with%20local%20attention%0Ato%20achieve%20excellent%20performance%20on%20language.%20It%20has%20a%20fixed-sized%20state%2C%20which%0Areduces%20memory%20use%20and%20enables%20efficient%20inference%20on%20long%20sequences.%20We%0Aprovide%20a%20pre-trained%20model%20with%202B%20non-embedding%20parameters%2C%20and%20an%0Ainstruction%20tuned%20variant.%20Both%20models%20achieve%20comparable%20performance%20to%0AGemma-2B%20despite%20being%20trained%20on%20fewer%20tokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07839v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RecurrentGemma%3A%20Moving%20Past%20Transformers%20for%20Efficient%20Open%20Language%0A%20%20Models&entry.906535625=Aleksandar%20Botev%20and%20Soham%20De%20and%20Samuel%20L%20Smith%20and%20Anushan%20Fernando%20and%20George-Cristian%20Muraru%20and%20Ruba%20Haroun%20and%20Leonard%20Berrada%20and%20Razvan%20Pascanu%20and%20Pier%20Giuseppe%20Sessa%20and%20Robert%20Dadashi%20and%20L%C3%A9onard%20Hussenot%20and%20Johan%20Ferret%20and%20Sertan%20Girgin%20and%20Olivier%20Bachem%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Yutian%20Chen%20and%20Srivatsan%20Srinivasan%20and%20Guillaume%20Desjardins%20and%20David%20Budden%20and%20Arnaud%20Doucet%20and%20Sharad%20Vikram%20and%20Adam%20Paszke%20and%20Trevor%20Gale%20and%20Sebastian%20Borgeaud%20and%20Charlie%20Chen%20and%20Andy%20Brock%20and%20Antonia%20Paterson%20and%20Jenny%20Brennan%20and%20Meg%20Risdal%20and%20Raj%20Gundluru%20and%20Nesh%20Devanathan%20and%20Paul%20Mooney%20and%20Nilay%20Chauhan%20and%20Phil%20Culliton%20and%20Luiz%20GUStavo%20Martins%20and%20Elisa%20Bandy%20and%20David%20Huntsperger%20and%20Glenn%20Cameron%20and%20Arthur%20Zucker%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Zoubin%20Ghahramani%20and%20Cl%C3%A9ment%20Farabet%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Raia%20Hadsell%20and%20Yee%20Whye%20Teh%20and%20Nando%20de%20Frietas&entry.1292438233=%20%20We%20introduce%20RecurrentGemma%2C%20an%20open%20language%20model%20which%20uses%20Google%27s%20novel%0AGriffin%20architecture.%20Griffin%20combines%20linear%20recurrences%20with%20local%20attention%0Ato%20achieve%20excellent%20performance%20on%20language.%20It%20has%20a%20fixed-sized%20state%2C%20which%0Areduces%20memory%20use%20and%20enables%20efficient%20inference%20on%20long%20sequences.%20We%0Aprovide%20a%20pre-trained%20model%20with%202B%20non-embedding%20parameters%2C%20and%20an%0Ainstruction%20tuned%20variant.%20Both%20models%20achieve%20comparable%20performance%20to%0AGemma-2B%20despite%20being%20trained%20on%20fewer%20tokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07839v1&entry.124074799=Read"},
{"title": "Lyapunov-stable Neural Control for State and Output Feedback: A Novel\n  Formulation for Efficient Synthesis and Verification", "author": "Lujie Yang and Hongkai Dai and Zhouxing Shi and Cho-Jui Hsieh and Russ Tedrake and Huan Zhang", "abstract": "  Learning-based neural network (NN) control policies have shown impressive\nempirical performance in a wide range of tasks in robotics and control.\nHowever, formal (Lyapunov) stability guarantees over the region-of-attraction\n(ROA) for NN controllers with nonlinear dynamical systems are challenging to\nobtain, and most existing approaches rely on expensive solvers such as\nsums-of-squares (SOS), mixed-integer programming (MIP), or satisfiability\nmodulo theories (SMT). In this paper, we demonstrate a new framework for\nlearning NN controllers together with Lyapunov certificates using fast\nempirical falsification and strategic regularizations. We propose a novel\nformulation that defines a larger verifiable region-of-attraction (ROA) than\nshown in the literature, and refines the conventional restrictive constraints\non Lyapunov derivatives to focus only on certifiable ROAs. The Lyapunov\ncondition is rigorously verified post-hoc using branch-and-bound with scalable\nlinear bound propagation-based NN verification techniques. The approach is\nefficient and flexible, and the full training and verification procedure is\naccelerated on GPUs without relying on expensive solvers for SOS, MIP, nor SMT.\nThe flexibility and efficiency of our framework allow us to demonstrate\nLyapunov-stable output feedback control with synthesized NN-based controllers\nand NN-based observers with formal stability guarantees, for the first time in\nliterature. Source code at\nhttps://github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers.\n", "link": "http://arxiv.org/abs/2404.07956v1", "date": "2024-04-11", "relevancy": 2.0051, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5209}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4785}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lyapunov-stable%20Neural%20Control%20for%20State%20and%20Output%20Feedback%3A%20A%20Novel%0A%20%20Formulation%20for%20Efficient%20Synthesis%20and%20Verification&body=Title%3A%20Lyapunov-stable%20Neural%20Control%20for%20State%20and%20Output%20Feedback%3A%20A%20Novel%0A%20%20Formulation%20for%20Efficient%20Synthesis%20and%20Verification%0AAuthor%3A%20Lujie%20Yang%20and%20Hongkai%20Dai%20and%20Zhouxing%20Shi%20and%20Cho-Jui%20Hsieh%20and%20Russ%20Tedrake%20and%20Huan%20Zhang%0AAbstract%3A%20%20%20Learning-based%20neural%20network%20%28NN%29%20control%20policies%20have%20shown%20impressive%0Aempirical%20performance%20in%20a%20wide%20range%20of%20tasks%20in%20robotics%20and%20control.%0AHowever%2C%20formal%20%28Lyapunov%29%20stability%20guarantees%20over%20the%20region-of-attraction%0A%28ROA%29%20for%20NN%20controllers%20with%20nonlinear%20dynamical%20systems%20are%20challenging%20to%0Aobtain%2C%20and%20most%20existing%20approaches%20rely%20on%20expensive%20solvers%20such%20as%0Asums-of-squares%20%28SOS%29%2C%20mixed-integer%20programming%20%28MIP%29%2C%20or%20satisfiability%0Amodulo%20theories%20%28SMT%29.%20In%20this%20paper%2C%20we%20demonstrate%20a%20new%20framework%20for%0Alearning%20NN%20controllers%20together%20with%20Lyapunov%20certificates%20using%20fast%0Aempirical%20falsification%20and%20strategic%20regularizations.%20We%20propose%20a%20novel%0Aformulation%20that%20defines%20a%20larger%20verifiable%20region-of-attraction%20%28ROA%29%20than%0Ashown%20in%20the%20literature%2C%20and%20refines%20the%20conventional%20restrictive%20constraints%0Aon%20Lyapunov%20derivatives%20to%20focus%20only%20on%20certifiable%20ROAs.%20The%20Lyapunov%0Acondition%20is%20rigorously%20verified%20post-hoc%20using%20branch-and-bound%20with%20scalable%0Alinear%20bound%20propagation-based%20NN%20verification%20techniques.%20The%20approach%20is%0Aefficient%20and%20flexible%2C%20and%20the%20full%20training%20and%20verification%20procedure%20is%0Aaccelerated%20on%20GPUs%20without%20relying%20on%20expensive%20solvers%20for%20SOS%2C%20MIP%2C%20nor%20SMT.%0AThe%20flexibility%20and%20efficiency%20of%20our%20framework%20allow%20us%20to%20demonstrate%0ALyapunov-stable%20output%20feedback%20control%20with%20synthesized%20NN-based%20controllers%0Aand%20NN-based%20observers%20with%20formal%20stability%20guarantees%2C%20for%20the%20first%20time%20in%0Aliterature.%20Source%20code%20at%0Ahttps%3A//github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07956v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lyapunov-stable%20Neural%20Control%20for%20State%20and%20Output%20Feedback%3A%20A%20Novel%0A%20%20Formulation%20for%20Efficient%20Synthesis%20and%20Verification&entry.906535625=Lujie%20Yang%20and%20Hongkai%20Dai%20and%20Zhouxing%20Shi%20and%20Cho-Jui%20Hsieh%20and%20Russ%20Tedrake%20and%20Huan%20Zhang&entry.1292438233=%20%20Learning-based%20neural%20network%20%28NN%29%20control%20policies%20have%20shown%20impressive%0Aempirical%20performance%20in%20a%20wide%20range%20of%20tasks%20in%20robotics%20and%20control.%0AHowever%2C%20formal%20%28Lyapunov%29%20stability%20guarantees%20over%20the%20region-of-attraction%0A%28ROA%29%20for%20NN%20controllers%20with%20nonlinear%20dynamical%20systems%20are%20challenging%20to%0Aobtain%2C%20and%20most%20existing%20approaches%20rely%20on%20expensive%20solvers%20such%20as%0Asums-of-squares%20%28SOS%29%2C%20mixed-integer%20programming%20%28MIP%29%2C%20or%20satisfiability%0Amodulo%20theories%20%28SMT%29.%20In%20this%20paper%2C%20we%20demonstrate%20a%20new%20framework%20for%0Alearning%20NN%20controllers%20together%20with%20Lyapunov%20certificates%20using%20fast%0Aempirical%20falsification%20and%20strategic%20regularizations.%20We%20propose%20a%20novel%0Aformulation%20that%20defines%20a%20larger%20verifiable%20region-of-attraction%20%28ROA%29%20than%0Ashown%20in%20the%20literature%2C%20and%20refines%20the%20conventional%20restrictive%20constraints%0Aon%20Lyapunov%20derivatives%20to%20focus%20only%20on%20certifiable%20ROAs.%20The%20Lyapunov%0Acondition%20is%20rigorously%20verified%20post-hoc%20using%20branch-and-bound%20with%20scalable%0Alinear%20bound%20propagation-based%20NN%20verification%20techniques.%20The%20approach%20is%0Aefficient%20and%20flexible%2C%20and%20the%20full%20training%20and%20verification%20procedure%20is%0Aaccelerated%20on%20GPUs%20without%20relying%20on%20expensive%20solvers%20for%20SOS%2C%20MIP%2C%20nor%20SMT.%0AThe%20flexibility%20and%20efficiency%20of%20our%20framework%20allow%20us%20to%20demonstrate%0ALyapunov-stable%20output%20feedback%20control%20with%20synthesized%20NN-based%20controllers%0Aand%20NN-based%20observers%20with%20formal%20stability%20guarantees%2C%20for%20the%20first%20time%20in%0Aliterature.%20Source%20code%20at%0Ahttps%3A//github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07956v1&entry.124074799=Read"},
{"title": "Leveraging Large Language Models (LLMs) to Support Collaborative\n  Human-AI Online Risk Data Annotation", "author": "Jinkyung Park and Pamela Wisniewski and Vivek Singh", "abstract": "  In this position paper, we discuss the potential for leveraging LLMs as\ninteractive research tools to facilitate collaboration between human coders and\nAI to effectively annotate online risk data at scale. Collaborative human-AI\nlabeling is a promising approach to annotating large-scale and complex data for\nvarious tasks. Yet, tools and methods to support effective human-AI\ncollaboration for data annotation are under-studied. This gap is pertinent\nbecause co-labeling tasks need to support a two-way interactive discussion that\ncan add nuance and context, particularly in the context of online risk, which\nis highly subjective and contextualized. Therefore, we provide some of the\nearly benefits and challenges of using LLMs-based tools for risk annotation and\nsuggest future directions for the HCI research community to leverage LLMs as\nresearch tools to facilitate human-AI collaboration in contextualized online\ndata annotation. Our research interests align very well with the purposes of\nthe LLMs as Research Tools workshop to identify ongoing applications and\nchallenges of using LLMs to work with data in HCI research. We anticipate\nlearning valuable insights from organizers and participants into how LLMs can\nhelp reshape the HCI community's methods for working with data.\n", "link": "http://arxiv.org/abs/2404.07926v1", "date": "2024-04-11", "relevancy": 2.0042, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5541}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4835}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20%28LLMs%29%20to%20Support%20Collaborative%0A%20%20Human-AI%20Online%20Risk%20Data%20Annotation&body=Title%3A%20Leveraging%20Large%20Language%20Models%20%28LLMs%29%20to%20Support%20Collaborative%0A%20%20Human-AI%20Online%20Risk%20Data%20Annotation%0AAuthor%3A%20Jinkyung%20Park%20and%20Pamela%20Wisniewski%20and%20Vivek%20Singh%0AAbstract%3A%20%20%20In%20this%20position%20paper%2C%20we%20discuss%20the%20potential%20for%20leveraging%20LLMs%20as%0Ainteractive%20research%20tools%20to%20facilitate%20collaboration%20between%20human%20coders%20and%0AAI%20to%20effectively%20annotate%20online%20risk%20data%20at%20scale.%20Collaborative%20human-AI%0Alabeling%20is%20a%20promising%20approach%20to%20annotating%20large-scale%20and%20complex%20data%20for%0Avarious%20tasks.%20Yet%2C%20tools%20and%20methods%20to%20support%20effective%20human-AI%0Acollaboration%20for%20data%20annotation%20are%20under-studied.%20This%20gap%20is%20pertinent%0Abecause%20co-labeling%20tasks%20need%20to%20support%20a%20two-way%20interactive%20discussion%20that%0Acan%20add%20nuance%20and%20context%2C%20particularly%20in%20the%20context%20of%20online%20risk%2C%20which%0Ais%20highly%20subjective%20and%20contextualized.%20Therefore%2C%20we%20provide%20some%20of%20the%0Aearly%20benefits%20and%20challenges%20of%20using%20LLMs-based%20tools%20for%20risk%20annotation%20and%0Asuggest%20future%20directions%20for%20the%20HCI%20research%20community%20to%20leverage%20LLMs%20as%0Aresearch%20tools%20to%20facilitate%20human-AI%20collaboration%20in%20contextualized%20online%0Adata%20annotation.%20Our%20research%20interests%20align%20very%20well%20with%20the%20purposes%20of%0Athe%20LLMs%20as%20Research%20Tools%20workshop%20to%20identify%20ongoing%20applications%20and%0Achallenges%20of%20using%20LLMs%20to%20work%20with%20data%20in%20HCI%20research.%20We%20anticipate%0Alearning%20valuable%20insights%20from%20organizers%20and%20participants%20into%20how%20LLMs%20can%0Ahelp%20reshape%20the%20HCI%20community%27s%20methods%20for%20working%20with%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07926v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20%28LLMs%29%20to%20Support%20Collaborative%0A%20%20Human-AI%20Online%20Risk%20Data%20Annotation&entry.906535625=Jinkyung%20Park%20and%20Pamela%20Wisniewski%20and%20Vivek%20Singh&entry.1292438233=%20%20In%20this%20position%20paper%2C%20we%20discuss%20the%20potential%20for%20leveraging%20LLMs%20as%0Ainteractive%20research%20tools%20to%20facilitate%20collaboration%20between%20human%20coders%20and%0AAI%20to%20effectively%20annotate%20online%20risk%20data%20at%20scale.%20Collaborative%20human-AI%0Alabeling%20is%20a%20promising%20approach%20to%20annotating%20large-scale%20and%20complex%20data%20for%0Avarious%20tasks.%20Yet%2C%20tools%20and%20methods%20to%20support%20effective%20human-AI%0Acollaboration%20for%20data%20annotation%20are%20under-studied.%20This%20gap%20is%20pertinent%0Abecause%20co-labeling%20tasks%20need%20to%20support%20a%20two-way%20interactive%20discussion%20that%0Acan%20add%20nuance%20and%20context%2C%20particularly%20in%20the%20context%20of%20online%20risk%2C%20which%0Ais%20highly%20subjective%20and%20contextualized.%20Therefore%2C%20we%20provide%20some%20of%20the%0Aearly%20benefits%20and%20challenges%20of%20using%20LLMs-based%20tools%20for%20risk%20annotation%20and%0Asuggest%20future%20directions%20for%20the%20HCI%20research%20community%20to%20leverage%20LLMs%20as%0Aresearch%20tools%20to%20facilitate%20human-AI%20collaboration%20in%20contextualized%20online%0Adata%20annotation.%20Our%20research%20interests%20align%20very%20well%20with%20the%20purposes%20of%0Athe%20LLMs%20as%20Research%20Tools%20workshop%20to%20identify%20ongoing%20applications%20and%0Achallenges%20of%20using%20LLMs%20to%20work%20with%20data%20in%20HCI%20research.%20We%20anticipate%0Alearning%20valuable%20insights%20from%20organizers%20and%20participants%20into%20how%20LLMs%20can%0Ahelp%20reshape%20the%20HCI%20community%27s%20methods%20for%20working%20with%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07926v1&entry.124074799=Read"},
{"title": "Discourse-Aware In-Context Learning for Temporal Expression\n  Normalization", "author": "Akash Kumar Gautam and Lukas Lange and Jannik Str\u00f6tgen", "abstract": "  Temporal expression (TE) normalization is a well-studied problem. However,\nthe predominately used rule-based systems are highly restricted to specific\nsettings, and upcoming machine learning approaches suffer from a lack of\nlabeled data. In this work, we explore the feasibility of proprietary and\nopen-source large language models (LLMs) for TE normalization using in-context\nlearning to inject task, document, and example information into the model. We\nexplore various sample selection strategies to retrieve the most relevant set\nof examples. By using a window-based prompt design approach, we can perform TE\nnormalization across sentences, while leveraging the LLM knowledge without\ntraining the model. Our experiments show competitive results to models designed\nfor this task. In particular, our method achieves large performance\nimprovements for non-standard settings by dynamically including relevant\nexamples during inference.\n", "link": "http://arxiv.org/abs/2404.07775v1", "date": "2024-04-11", "relevancy": 1.996, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5269}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4799}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.477}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Discourse-Aware%20In-Context%20Learning%20for%20Temporal%20Expression%0A%20%20Normalization&body=Title%3A%20Discourse-Aware%20In-Context%20Learning%20for%20Temporal%20Expression%0A%20%20Normalization%0AAuthor%3A%20Akash%20Kumar%20Gautam%20and%20Lukas%20Lange%20and%20Jannik%20Str%C3%B6tgen%0AAbstract%3A%20%20%20Temporal%20expression%20%28TE%29%20normalization%20is%20a%20well-studied%20problem.%20However%2C%0Athe%20predominately%20used%20rule-based%20systems%20are%20highly%20restricted%20to%20specific%0Asettings%2C%20and%20upcoming%20machine%20learning%20approaches%20suffer%20from%20a%20lack%20of%0Alabeled%20data.%20In%20this%20work%2C%20we%20explore%20the%20feasibility%20of%20proprietary%20and%0Aopen-source%20large%20language%20models%20%28LLMs%29%20for%20TE%20normalization%20using%20in-context%0Alearning%20to%20inject%20task%2C%20document%2C%20and%20example%20information%20into%20the%20model.%20We%0Aexplore%20various%20sample%20selection%20strategies%20to%20retrieve%20the%20most%20relevant%20set%0Aof%20examples.%20By%20using%20a%20window-based%20prompt%20design%20approach%2C%20we%20can%20perform%20TE%0Anormalization%20across%20sentences%2C%20while%20leveraging%20the%20LLM%20knowledge%20without%0Atraining%20the%20model.%20Our%20experiments%20show%20competitive%20results%20to%20models%20designed%0Afor%20this%20task.%20In%20particular%2C%20our%20method%20achieves%20large%20performance%0Aimprovements%20for%20non-standard%20settings%20by%20dynamically%20including%20relevant%0Aexamples%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07775v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discourse-Aware%20In-Context%20Learning%20for%20Temporal%20Expression%0A%20%20Normalization&entry.906535625=Akash%20Kumar%20Gautam%20and%20Lukas%20Lange%20and%20Jannik%20Str%C3%B6tgen&entry.1292438233=%20%20Temporal%20expression%20%28TE%29%20normalization%20is%20a%20well-studied%20problem.%20However%2C%0Athe%20predominately%20used%20rule-based%20systems%20are%20highly%20restricted%20to%20specific%0Asettings%2C%20and%20upcoming%20machine%20learning%20approaches%20suffer%20from%20a%20lack%20of%0Alabeled%20data.%20In%20this%20work%2C%20we%20explore%20the%20feasibility%20of%20proprietary%20and%0Aopen-source%20large%20language%20models%20%28LLMs%29%20for%20TE%20normalization%20using%20in-context%0Alearning%20to%20inject%20task%2C%20document%2C%20and%20example%20information%20into%20the%20model.%20We%0Aexplore%20various%20sample%20selection%20strategies%20to%20retrieve%20the%20most%20relevant%20set%0Aof%20examples.%20By%20using%20a%20window-based%20prompt%20design%20approach%2C%20we%20can%20perform%20TE%0Anormalization%20across%20sentences%2C%20while%20leveraging%20the%20LLM%20knowledge%20without%0Atraining%20the%20model.%20Our%20experiments%20show%20competitive%20results%20to%20models%20designed%0Afor%20this%20task.%20In%20particular%2C%20our%20method%20achieves%20large%20performance%0Aimprovements%20for%20non-standard%20settings%20by%20dynamically%20including%20relevant%0Aexamples%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07775v1&entry.124074799=Read"},
{"title": "A Novel Optimization-Based Collision Avoidance For Autonomous On-Orbit\n  Assembly", "author": "Siavash Tavana and Sepideh Faghihi and Anton de Ruiter and Krishna Dev Kumar", "abstract": "  The collision avoidance constraints are prominent as non-convex,\nnon-differentiable, and challenging when defined in optimization-based motion\nplanning problems. To overcome these issues, this paper presents a novel\nnon-conservative collision avoidance technique using the notion of convex\noptimization to establish the distance between robotic spacecraft and space\nstructures for autonomous on-orbit assembly operations. The proposed technique\ndefines each ellipsoidal- and polyhedral-shaped object as the union of convex\ncompact sets, each represented non-conservatively by a real-valued convex\nfunction. Then, the functions are introduced as a set of constraints to a\nconvex optimization problem to produce a new set of differentiable constraints\nresulting from the optimality conditions. These new constraints are later fed\ninto an optimal control problem to enforce collision avoidance where the motion\nplanning for the autonomous on-orbit assembly takes place. Numerical\nexperiments for two assembly scenarios in tight environments are presented to\ndemonstrate the capability and effectiveness of the proposed technique. The\nresults show that this framework leads to optimal non-conservative trajectories\nfor robotic spacecraft in tight environments. Although developed for autonomous\non-orbit assembly, this technique could be used for any generic motion planning\nproblem where collision avoidance is crucial.\n", "link": "http://arxiv.org/abs/2404.07916v1", "date": "2024-04-11", "relevancy": 1.9829, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.503}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4918}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4875}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Optimization-Based%20Collision%20Avoidance%20For%20Autonomous%20On-Orbit%0A%20%20Assembly&body=Title%3A%20A%20Novel%20Optimization-Based%20Collision%20Avoidance%20For%20Autonomous%20On-Orbit%0A%20%20Assembly%0AAuthor%3A%20Siavash%20Tavana%20and%20Sepideh%20Faghihi%20and%20Anton%20de%20Ruiter%20and%20Krishna%20Dev%20Kumar%0AAbstract%3A%20%20%20The%20collision%20avoidance%20constraints%20are%20prominent%20as%20non-convex%2C%0Anon-differentiable%2C%20and%20challenging%20when%20defined%20in%20optimization-based%20motion%0Aplanning%20problems.%20To%20overcome%20these%20issues%2C%20this%20paper%20presents%20a%20novel%0Anon-conservative%20collision%20avoidance%20technique%20using%20the%20notion%20of%20convex%0Aoptimization%20to%20establish%20the%20distance%20between%20robotic%20spacecraft%20and%20space%0Astructures%20for%20autonomous%20on-orbit%20assembly%20operations.%20The%20proposed%20technique%0Adefines%20each%20ellipsoidal-%20and%20polyhedral-shaped%20object%20as%20the%20union%20of%20convex%0Acompact%20sets%2C%20each%20represented%20non-conservatively%20by%20a%20real-valued%20convex%0Afunction.%20Then%2C%20the%20functions%20are%20introduced%20as%20a%20set%20of%20constraints%20to%20a%0Aconvex%20optimization%20problem%20to%20produce%20a%20new%20set%20of%20differentiable%20constraints%0Aresulting%20from%20the%20optimality%20conditions.%20These%20new%20constraints%20are%20later%20fed%0Ainto%20an%20optimal%20control%20problem%20to%20enforce%20collision%20avoidance%20where%20the%20motion%0Aplanning%20for%20the%20autonomous%20on-orbit%20assembly%20takes%20place.%20Numerical%0Aexperiments%20for%20two%20assembly%20scenarios%20in%20tight%20environments%20are%20presented%20to%0Ademonstrate%20the%20capability%20and%20effectiveness%20of%20the%20proposed%20technique.%20The%0Aresults%20show%20that%20this%20framework%20leads%20to%20optimal%20non-conservative%20trajectories%0Afor%20robotic%20spacecraft%20in%20tight%20environments.%20Although%20developed%20for%20autonomous%0Aon-orbit%20assembly%2C%20this%20technique%20could%20be%20used%20for%20any%20generic%20motion%20planning%0Aproblem%20where%20collision%20avoidance%20is%20crucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07916v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Optimization-Based%20Collision%20Avoidance%20For%20Autonomous%20On-Orbit%0A%20%20Assembly&entry.906535625=Siavash%20Tavana%20and%20Sepideh%20Faghihi%20and%20Anton%20de%20Ruiter%20and%20Krishna%20Dev%20Kumar&entry.1292438233=%20%20The%20collision%20avoidance%20constraints%20are%20prominent%20as%20non-convex%2C%0Anon-differentiable%2C%20and%20challenging%20when%20defined%20in%20optimization-based%20motion%0Aplanning%20problems.%20To%20overcome%20these%20issues%2C%20this%20paper%20presents%20a%20novel%0Anon-conservative%20collision%20avoidance%20technique%20using%20the%20notion%20of%20convex%0Aoptimization%20to%20establish%20the%20distance%20between%20robotic%20spacecraft%20and%20space%0Astructures%20for%20autonomous%20on-orbit%20assembly%20operations.%20The%20proposed%20technique%0Adefines%20each%20ellipsoidal-%20and%20polyhedral-shaped%20object%20as%20the%20union%20of%20convex%0Acompact%20sets%2C%20each%20represented%20non-conservatively%20by%20a%20real-valued%20convex%0Afunction.%20Then%2C%20the%20functions%20are%20introduced%20as%20a%20set%20of%20constraints%20to%20a%0Aconvex%20optimization%20problem%20to%20produce%20a%20new%20set%20of%20differentiable%20constraints%0Aresulting%20from%20the%20optimality%20conditions.%20These%20new%20constraints%20are%20later%20fed%0Ainto%20an%20optimal%20control%20problem%20to%20enforce%20collision%20avoidance%20where%20the%20motion%0Aplanning%20for%20the%20autonomous%20on-orbit%20assembly%20takes%20place.%20Numerical%0Aexperiments%20for%20two%20assembly%20scenarios%20in%20tight%20environments%20are%20presented%20to%0Ademonstrate%20the%20capability%20and%20effectiveness%20of%20the%20proposed%20technique.%20The%0Aresults%20show%20that%20this%20framework%20leads%20to%20optimal%20non-conservative%20trajectories%0Afor%20robotic%20spacecraft%20in%20tight%20environments.%20Although%20developed%20for%20autonomous%0Aon-orbit%20assembly%2C%20this%20technique%20could%20be%20used%20for%20any%20generic%20motion%20planning%0Aproblem%20where%20collision%20avoidance%20is%20crucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07916v1&entry.124074799=Read"},
{"title": "CLUE: A Clinical Language Understanding Evaluation for LLMs", "author": "Amin Dada and Marie Bauer and Amanda Butler Contreras and Osman Alperen Kora\u015f and Constantin Marc Seibold and Kaleb E Smith and Jens Kleesiek", "abstract": "  Large Language Models (LLMs) have shown the potential to significantly\ncontribute to patient care, diagnostics, and administrative processes. Emerging\nbiomedical LLMs address healthcare-specific challenges, including privacy\ndemands and computational constraints. However, evaluation of these models has\nprimarily been limited to non-clinical tasks, which do not reflect the\ncomplexity of practical clinical applications. Additionally, there has been no\nthorough comparison between biomedical and general-domain LLMs for clinical\ntasks. To fill this gap, we present the Clinical Language Understanding\nEvaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical\ntasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters\nand four existing tasks designed to test the practical applicability of LLMs in\nhealthcare settings. Our evaluation covers several biomedical and general\ndomain LLMs, providing insights into their clinical performance and\napplicability. CLUE represents a step towards a standardized approach to\nevaluating and developing LLMs in healthcare to align future model development\nwith the real-world needs of clinical application. We publish our evaluation\nand data generation scripts: https://github.com/TIO-IKIM/CLUE.\n", "link": "http://arxiv.org/abs/2404.04067v2", "date": "2024-04-11", "relevancy": 1.9769, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs&body=Title%3A%20CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs%0AAuthor%3A%20Amin%20Dada%20and%20Marie%20Bauer%20and%20Amanda%20Butler%20Contreras%20and%20Osman%20Alperen%20Kora%C5%9F%20and%20Constantin%20Marc%20Seibold%20and%20Kaleb%20E%20Smith%20and%20Jens%20Kleesiek%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20the%20potential%20to%20significantly%0Acontribute%20to%20patient%20care%2C%20diagnostics%2C%20and%20administrative%20processes.%20Emerging%0Abiomedical%20LLMs%20address%20healthcare-specific%20challenges%2C%20including%20privacy%0Ademands%20and%20computational%20constraints.%20However%2C%20evaluation%20of%20these%20models%20has%0Aprimarily%20been%20limited%20to%20non-clinical%20tasks%2C%20which%20do%20not%20reflect%20the%0Acomplexity%20of%20practical%20clinical%20applications.%20Additionally%2C%20there%20has%20been%20no%0Athorough%20comparison%20between%20biomedical%20and%20general-domain%20LLMs%20for%20clinical%0Atasks.%20To%20fill%20this%20gap%2C%20we%20present%20the%20Clinical%20Language%20Understanding%0AEvaluation%20%28CLUE%29%2C%20a%20benchmark%20tailored%20to%20evaluate%20LLMs%20on%20real-world%20clinical%0Atasks.%20CLUE%20includes%20two%20novel%20datasets%20derived%20from%20MIMIC%20IV%20discharge%20letters%0Aand%20four%20existing%20tasks%20designed%20to%20test%20the%20practical%20applicability%20of%20LLMs%20in%0Ahealthcare%20settings.%20Our%20evaluation%20covers%20several%20biomedical%20and%20general%0Adomain%20LLMs%2C%20providing%20insights%20into%20their%20clinical%20performance%20and%0Aapplicability.%20CLUE%20represents%20a%20step%20towards%20a%20standardized%20approach%20to%0Aevaluating%20and%20developing%20LLMs%20in%20healthcare%20to%20align%20future%20model%20development%0Awith%20the%20real-world%20needs%20of%20clinical%20application.%20We%20publish%20our%20evaluation%0Aand%20data%20generation%20scripts%3A%20https%3A//github.com/TIO-IKIM/CLUE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04067v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs&entry.906535625=Amin%20Dada%20and%20Marie%20Bauer%20and%20Amanda%20Butler%20Contreras%20and%20Osman%20Alperen%20Kora%C5%9F%20and%20Constantin%20Marc%20Seibold%20and%20Kaleb%20E%20Smith%20and%20Jens%20Kleesiek&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20the%20potential%20to%20significantly%0Acontribute%20to%20patient%20care%2C%20diagnostics%2C%20and%20administrative%20processes.%20Emerging%0Abiomedical%20LLMs%20address%20healthcare-specific%20challenges%2C%20including%20privacy%0Ademands%20and%20computational%20constraints.%20However%2C%20evaluation%20of%20these%20models%20has%0Aprimarily%20been%20limited%20to%20non-clinical%20tasks%2C%20which%20do%20not%20reflect%20the%0Acomplexity%20of%20practical%20clinical%20applications.%20Additionally%2C%20there%20has%20been%20no%0Athorough%20comparison%20between%20biomedical%20and%20general-domain%20LLMs%20for%20clinical%0Atasks.%20To%20fill%20this%20gap%2C%20we%20present%20the%20Clinical%20Language%20Understanding%0AEvaluation%20%28CLUE%29%2C%20a%20benchmark%20tailored%20to%20evaluate%20LLMs%20on%20real-world%20clinical%0Atasks.%20CLUE%20includes%20two%20novel%20datasets%20derived%20from%20MIMIC%20IV%20discharge%20letters%0Aand%20four%20existing%20tasks%20designed%20to%20test%20the%20practical%20applicability%20of%20LLMs%20in%0Ahealthcare%20settings.%20Our%20evaluation%20covers%20several%20biomedical%20and%20general%0Adomain%20LLMs%2C%20providing%20insights%20into%20their%20clinical%20performance%20and%0Aapplicability.%20CLUE%20represents%20a%20step%20towards%20a%20standardized%20approach%20to%0Aevaluating%20and%20developing%20LLMs%20in%20healthcare%20to%20align%20future%20model%20development%0Awith%20the%20real-world%20needs%20of%20clinical%20application.%20We%20publish%20our%20evaluation%0Aand%20data%20generation%20scripts%3A%20https%3A//github.com/TIO-IKIM/CLUE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04067v2&entry.124074799=Read"},
{"title": "WWW: A Unified Framework for Explaining What, Where and Why of Neural\n  Networks by Interpretation of Neuron Concepts", "author": "Yong Hyun Ahn and Hyeon Bae Kim and Seong Tae Kim", "abstract": "  Recent advancements in neural networks have showcased their remarkable\ncapabilities across various domains. Despite these successes, the \"black box\"\nproblem still remains. Addressing this, we propose a novel framework, WWW, that\noffers the 'what', 'where', and 'why' of the neural network decisions in\nhuman-understandable terms. Specifically, WWW utilizes adaptive selection for\nconcept discovery, employing adaptive cosine similarity and thresholding\ntechniques to effectively explain 'what'. To address the 'where' and 'why', we\nproposed a novel combination of neuron activation maps (NAMs) with Shapley\nvalues, generating localized concept maps and heatmaps for individual inputs.\nFurthermore, WWW introduces a method for predicting uncertainty, leveraging\nheatmap similarities to estimate 'how' reliable the prediction is. Experimental\nevaluations of WWW demonstrate superior performance in both quantitative and\nqualitative metrics, outperforming existing methods in interpretability. WWW\nprovides a unified solution for explaining 'what', 'where', and 'why',\nintroducing a method for localized explanations from global interpretations and\noffering a plug-and-play solution adaptable to various architectures.\n", "link": "http://arxiv.org/abs/2402.18956v2", "date": "2024-04-11", "relevancy": 1.9732, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5104}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4765}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WWW%3A%20A%20Unified%20Framework%20for%20Explaining%20What%2C%20Where%20and%20Why%20of%20Neural%0A%20%20Networks%20by%20Interpretation%20of%20Neuron%20Concepts&body=Title%3A%20WWW%3A%20A%20Unified%20Framework%20for%20Explaining%20What%2C%20Where%20and%20Why%20of%20Neural%0A%20%20Networks%20by%20Interpretation%20of%20Neuron%20Concepts%0AAuthor%3A%20Yong%20Hyun%20Ahn%20and%20Hyeon%20Bae%20Kim%20and%20Seong%20Tae%20Kim%0AAbstract%3A%20%20%20Recent%20advancements%20in%20neural%20networks%20have%20showcased%20their%20remarkable%0Acapabilities%20across%20various%20domains.%20Despite%20these%20successes%2C%20the%20%22black%20box%22%0Aproblem%20still%20remains.%20Addressing%20this%2C%20we%20propose%20a%20novel%20framework%2C%20WWW%2C%20that%0Aoffers%20the%20%27what%27%2C%20%27where%27%2C%20and%20%27why%27%20of%20the%20neural%20network%20decisions%20in%0Ahuman-understandable%20terms.%20Specifically%2C%20WWW%20utilizes%20adaptive%20selection%20for%0Aconcept%20discovery%2C%20employing%20adaptive%20cosine%20similarity%20and%20thresholding%0Atechniques%20to%20effectively%20explain%20%27what%27.%20To%20address%20the%20%27where%27%20and%20%27why%27%2C%20we%0Aproposed%20a%20novel%20combination%20of%20neuron%20activation%20maps%20%28NAMs%29%20with%20Shapley%0Avalues%2C%20generating%20localized%20concept%20maps%20and%20heatmaps%20for%20individual%20inputs.%0AFurthermore%2C%20WWW%20introduces%20a%20method%20for%20predicting%20uncertainty%2C%20leveraging%0Aheatmap%20similarities%20to%20estimate%20%27how%27%20reliable%20the%20prediction%20is.%20Experimental%0Aevaluations%20of%20WWW%20demonstrate%20superior%20performance%20in%20both%20quantitative%20and%0Aqualitative%20metrics%2C%20outperforming%20existing%20methods%20in%20interpretability.%20WWW%0Aprovides%20a%20unified%20solution%20for%20explaining%20%27what%27%2C%20%27where%27%2C%20and%20%27why%27%2C%0Aintroducing%20a%20method%20for%20localized%20explanations%20from%20global%20interpretations%20and%0Aoffering%20a%20plug-and-play%20solution%20adaptable%20to%20various%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18956v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WWW%3A%20A%20Unified%20Framework%20for%20Explaining%20What%2C%20Where%20and%20Why%20of%20Neural%0A%20%20Networks%20by%20Interpretation%20of%20Neuron%20Concepts&entry.906535625=Yong%20Hyun%20Ahn%20and%20Hyeon%20Bae%20Kim%20and%20Seong%20Tae%20Kim&entry.1292438233=%20%20Recent%20advancements%20in%20neural%20networks%20have%20showcased%20their%20remarkable%0Acapabilities%20across%20various%20domains.%20Despite%20these%20successes%2C%20the%20%22black%20box%22%0Aproblem%20still%20remains.%20Addressing%20this%2C%20we%20propose%20a%20novel%20framework%2C%20WWW%2C%20that%0Aoffers%20the%20%27what%27%2C%20%27where%27%2C%20and%20%27why%27%20of%20the%20neural%20network%20decisions%20in%0Ahuman-understandable%20terms.%20Specifically%2C%20WWW%20utilizes%20adaptive%20selection%20for%0Aconcept%20discovery%2C%20employing%20adaptive%20cosine%20similarity%20and%20thresholding%0Atechniques%20to%20effectively%20explain%20%27what%27.%20To%20address%20the%20%27where%27%20and%20%27why%27%2C%20we%0Aproposed%20a%20novel%20combination%20of%20neuron%20activation%20maps%20%28NAMs%29%20with%20Shapley%0Avalues%2C%20generating%20localized%20concept%20maps%20and%20heatmaps%20for%20individual%20inputs.%0AFurthermore%2C%20WWW%20introduces%20a%20method%20for%20predicting%20uncertainty%2C%20leveraging%0Aheatmap%20similarities%20to%20estimate%20%27how%27%20reliable%20the%20prediction%20is.%20Experimental%0Aevaluations%20of%20WWW%20demonstrate%20superior%20performance%20in%20both%20quantitative%20and%0Aqualitative%20metrics%2C%20outperforming%20existing%20methods%20in%20interpretability.%20WWW%0Aprovides%20a%20unified%20solution%20for%20explaining%20%27what%27%2C%20%27where%27%2C%20and%20%27why%27%2C%0Aintroducing%20a%20method%20for%20localized%20explanations%20from%20global%20interpretations%20and%0Aoffering%20a%20plug-and-play%20solution%20adaptable%20to%20various%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18956v2&entry.124074799=Read"},
{"title": "Efficient Online Unlearning via Hessian-Free Recollection of Individual\n  Data Statistics", "author": "Xinbao Qiao and Meng Zhang and Ming Tang and Ermin Wei", "abstract": "  Machine unlearning strives to uphold the data owners' right to be forgotten\nby enabling models to selectively forget specific data. Recent methods suggest\nthat one approach of data forgetting is by precomputing and storing statistics\ncarrying second-order information to improve computational and memory\nefficiency. However, they rely on restrictive assumptions and the\ncomputation/storage suffer from the curse of model parameter dimensionality,\nmaking it challenging to apply to most deep neural networks. In this work, we\npropose a Hessian-free online unlearning method. We propose to maintain a\nstatistical vector for each data point, computed through affine stochastic\nrecursion approximation of the difference between retrained and learned models.\nOur proposed algorithm achieves near-instantaneous online unlearning as it only\nrequires a vector addition operation. Based on the strategy that recollecting\nstatistics for forgetting data, the proposed method significantly reduces the\nunlearning runtime. Experimental studies demonstrate that the proposed scheme\nsurpasses existing results by orders of magnitude in terms of time and memory\ncosts, while also enhancing accuracy.\n", "link": "http://arxiv.org/abs/2404.01712v2", "date": "2024-04-11", "relevancy": 1.9695, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5095}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4833}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4723}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Online%20Unlearning%20via%20Hessian-Free%20Recollection%20of%20Individual%0A%20%20Data%20Statistics&body=Title%3A%20Efficient%20Online%20Unlearning%20via%20Hessian-Free%20Recollection%20of%20Individual%0A%20%20Data%20Statistics%0AAuthor%3A%20Xinbao%20Qiao%20and%20Meng%20Zhang%20and%20Ming%20Tang%20and%20Ermin%20Wei%0AAbstract%3A%20%20%20Machine%20unlearning%20strives%20to%20uphold%20the%20data%20owners%27%20right%20to%20be%20forgotten%0Aby%20enabling%20models%20to%20selectively%20forget%20specific%20data.%20Recent%20methods%20suggest%0Athat%20one%20approach%20of%20data%20forgetting%20is%20by%20precomputing%20and%20storing%20statistics%0Acarrying%20second-order%20information%20to%20improve%20computational%20and%20memory%0Aefficiency.%20However%2C%20they%20rely%20on%20restrictive%20assumptions%20and%20the%0Acomputation/storage%20suffer%20from%20the%20curse%20of%20model%20parameter%20dimensionality%2C%0Amaking%20it%20challenging%20to%20apply%20to%20most%20deep%20neural%20networks.%20In%20this%20work%2C%20we%0Apropose%20a%20Hessian-free%20online%20unlearning%20method.%20We%20propose%20to%20maintain%20a%0Astatistical%20vector%20for%20each%20data%20point%2C%20computed%20through%20affine%20stochastic%0Arecursion%20approximation%20of%20the%20difference%20between%20retrained%20and%20learned%20models.%0AOur%20proposed%20algorithm%20achieves%20near-instantaneous%20online%20unlearning%20as%20it%20only%0Arequires%20a%20vector%20addition%20operation.%20Based%20on%20the%20strategy%20that%20recollecting%0Astatistics%20for%20forgetting%20data%2C%20the%20proposed%20method%20significantly%20reduces%20the%0Aunlearning%20runtime.%20Experimental%20studies%20demonstrate%20that%20the%20proposed%20scheme%0Asurpasses%20existing%20results%20by%20orders%20of%20magnitude%20in%20terms%20of%20time%20and%20memory%0Acosts%2C%20while%20also%20enhancing%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01712v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Online%20Unlearning%20via%20Hessian-Free%20Recollection%20of%20Individual%0A%20%20Data%20Statistics&entry.906535625=Xinbao%20Qiao%20and%20Meng%20Zhang%20and%20Ming%20Tang%20and%20Ermin%20Wei&entry.1292438233=%20%20Machine%20unlearning%20strives%20to%20uphold%20the%20data%20owners%27%20right%20to%20be%20forgotten%0Aby%20enabling%20models%20to%20selectively%20forget%20specific%20data.%20Recent%20methods%20suggest%0Athat%20one%20approach%20of%20data%20forgetting%20is%20by%20precomputing%20and%20storing%20statistics%0Acarrying%20second-order%20information%20to%20improve%20computational%20and%20memory%0Aefficiency.%20However%2C%20they%20rely%20on%20restrictive%20assumptions%20and%20the%0Acomputation/storage%20suffer%20from%20the%20curse%20of%20model%20parameter%20dimensionality%2C%0Amaking%20it%20challenging%20to%20apply%20to%20most%20deep%20neural%20networks.%20In%20this%20work%2C%20we%0Apropose%20a%20Hessian-free%20online%20unlearning%20method.%20We%20propose%20to%20maintain%20a%0Astatistical%20vector%20for%20each%20data%20point%2C%20computed%20through%20affine%20stochastic%0Arecursion%20approximation%20of%20the%20difference%20between%20retrained%20and%20learned%20models.%0AOur%20proposed%20algorithm%20achieves%20near-instantaneous%20online%20unlearning%20as%20it%20only%0Arequires%20a%20vector%20addition%20operation.%20Based%20on%20the%20strategy%20that%20recollecting%0Astatistics%20for%20forgetting%20data%2C%20the%20proposed%20method%20significantly%20reduces%20the%0Aunlearning%20runtime.%20Experimental%20studies%20demonstrate%20that%20the%20proposed%20scheme%0Asurpasses%20existing%20results%20by%20orders%20of%20magnitude%20in%20terms%20of%20time%20and%20memory%0Acosts%2C%20while%20also%20enhancing%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01712v2&entry.124074799=Read"},
{"title": "On Training Data Influence of GPT Models", "author": "Qingyi Liu and Yekun Chai and Shuohuan Wang and Yu Sun and Keze Wang and Hua Wu", "abstract": "  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.\n", "link": "http://arxiv.org/abs/2404.07840v1", "date": "2024-04-11", "relevancy": 1.9675, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5175}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5103}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4632}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Training%20Data%20Influence%20of%20GPT%20Models&body=Title%3A%20On%20Training%20Data%20Influence%20of%20GPT%20Models%0AAuthor%3A%20Qingyi%20Liu%20and%20Yekun%20Chai%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Keze%20Wang%20and%20Hua%20Wu%0AAbstract%3A%20%20%20Amidst%20the%20rapid%20advancements%20in%20generative%20language%20models%2C%20the%0Ainvestigation%20of%20how%20training%20data%20shapes%20the%20performance%20of%20GPT%20models%20is%0Astill%20emerging.%20This%20paper%20presents%20GPTfluence%2C%20a%20novel%20approach%20that%20leverages%0Aa%20featurized%20simulation%20to%20assess%20the%20impact%20of%20training%20examples%20on%20the%0Atraining%20dynamics%20of%20GPT%20models.%20Our%20approach%20not%20only%20traces%20the%20influence%20of%0Aindividual%20training%20instances%20on%20performance%20trajectories%2C%20such%20as%20loss%20and%0Aother%20key%20metrics%2C%20on%20targeted%20test%20points%20but%20also%20enables%20a%20comprehensive%0Acomparison%20with%20existing%20methods%20across%20various%20training%20scenarios%20in%20GPT%0Amodels%2C%20ranging%20from%2014%20million%20to%202.8%20billion%20parameters%2C%20across%20a%20range%20of%0Adownstream%20tasks.%20Contrary%20to%20earlier%20methods%20that%20struggle%20with%20generalization%0Ato%20new%20data%2C%20GPTfluence%20introduces%20a%20parameterized%20simulation%20of%20training%0Adynamics%2C%20demonstrating%20robust%20generalization%20capabilities%20to%20unseen%20training%0Adata.%20This%20adaptability%20is%20evident%20across%20both%20fine-tuning%20and%0Ainstruction-tuning%20scenarios%2C%20spanning%20tasks%20in%20natural%20language%20understanding%0Aand%20generation.%20We%20will%20make%20our%20code%20and%20data%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07840v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Training%20Data%20Influence%20of%20GPT%20Models&entry.906535625=Qingyi%20Liu%20and%20Yekun%20Chai%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Keze%20Wang%20and%20Hua%20Wu&entry.1292438233=%20%20Amidst%20the%20rapid%20advancements%20in%20generative%20language%20models%2C%20the%0Ainvestigation%20of%20how%20training%20data%20shapes%20the%20performance%20of%20GPT%20models%20is%0Astill%20emerging.%20This%20paper%20presents%20GPTfluence%2C%20a%20novel%20approach%20that%20leverages%0Aa%20featurized%20simulation%20to%20assess%20the%20impact%20of%20training%20examples%20on%20the%0Atraining%20dynamics%20of%20GPT%20models.%20Our%20approach%20not%20only%20traces%20the%20influence%20of%0Aindividual%20training%20instances%20on%20performance%20trajectories%2C%20such%20as%20loss%20and%0Aother%20key%20metrics%2C%20on%20targeted%20test%20points%20but%20also%20enables%20a%20comprehensive%0Acomparison%20with%20existing%20methods%20across%20various%20training%20scenarios%20in%20GPT%0Amodels%2C%20ranging%20from%2014%20million%20to%202.8%20billion%20parameters%2C%20across%20a%20range%20of%0Adownstream%20tasks.%20Contrary%20to%20earlier%20methods%20that%20struggle%20with%20generalization%0Ato%20new%20data%2C%20GPTfluence%20introduces%20a%20parameterized%20simulation%20of%20training%0Adynamics%2C%20demonstrating%20robust%20generalization%20capabilities%20to%20unseen%20training%0Adata.%20This%20adaptability%20is%20evident%20across%20both%20fine-tuning%20and%0Ainstruction-tuning%20scenarios%2C%20spanning%20tasks%20in%20natural%20language%20understanding%0Aand%20generation.%20We%20will%20make%20our%20code%20and%20data%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07840v1&entry.124074799=Read"},
{"title": "Data-Driven System Identification of Quadrotors Subject to Motor Delays", "author": "Jonas Eschmann and Dario Albani and Giuseppe Loianno", "abstract": "  Recently non-linear control methods like Model Predictive Control (MPC) and\nReinforcement Learning (RL) have attracted increased interest in the quadrotor\ncontrol community. In contrast to classic control methods like cascaded PID\ncontrollers, MPC and RL heavily rely on an accurate model of the system\ndynamics. The process of quadrotor system identification is notoriously tedious\nand is often pursued with additional equipment like a thrust stand.\nFurthermore, low-level details like motor delays which are crucial for accurate\nend-to-end control are often neglected. In this work, we introduce a\ndata-driven method to identify a quadrotor's inertia parameters, thrust curves,\ntorque coefficients, and first-order motor delay purely based on proprioceptive\ndata. The estimation of the motor delay is particularly challenging as usually,\nthe RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based\nmethod to estimate the latent time constant. Our approach only requires about a\nminute of flying data that can be collected without any additional equipment\nand usually consists of three simple maneuvers. Experimental results\ndemonstrate the ability of our method to accurately recover the parameters of\nmultiple quadrotors. It also facilitates the deployment of RL-based, end-to-end\nquadrotor control of a large quadrotor under harsh, outdoor conditions.\n", "link": "http://arxiv.org/abs/2404.07837v1", "date": "2024-04-11", "relevancy": 1.9675, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.533}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4891}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4782}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20System%20Identification%20of%20Quadrotors%20Subject%20to%20Motor%20Delays&body=Title%3A%20Data-Driven%20System%20Identification%20of%20Quadrotors%20Subject%20to%20Motor%20Delays%0AAuthor%3A%20Jonas%20Eschmann%20and%20Dario%20Albani%20and%20Giuseppe%20Loianno%0AAbstract%3A%20%20%20Recently%20non-linear%20control%20methods%20like%20Model%20Predictive%20Control%20%28MPC%29%20and%0AReinforcement%20Learning%20%28RL%29%20have%20attracted%20increased%20interest%20in%20the%20quadrotor%0Acontrol%20community.%20In%20contrast%20to%20classic%20control%20methods%20like%20cascaded%20PID%0Acontrollers%2C%20MPC%20and%20RL%20heavily%20rely%20on%20an%20accurate%20model%20of%20the%20system%0Adynamics.%20The%20process%20of%20quadrotor%20system%20identification%20is%20notoriously%20tedious%0Aand%20is%20often%20pursued%20with%20additional%20equipment%20like%20a%20thrust%20stand.%0AFurthermore%2C%20low-level%20details%20like%20motor%20delays%20which%20are%20crucial%20for%20accurate%0Aend-to-end%20control%20are%20often%20neglected.%20In%20this%20work%2C%20we%20introduce%20a%0Adata-driven%20method%20to%20identify%20a%20quadrotor%27s%20inertia%20parameters%2C%20thrust%20curves%2C%0Atorque%20coefficients%2C%20and%20first-order%20motor%20delay%20purely%20based%20on%20proprioceptive%0Adata.%20The%20estimation%20of%20the%20motor%20delay%20is%20particularly%20challenging%20as%20usually%2C%0Athe%20RPMs%20can%20not%20be%20measured.%20We%20derive%20a%20Maximum%20A%20Posteriori%20%28MAP%29-based%0Amethod%20to%20estimate%20the%20latent%20time%20constant.%20Our%20approach%20only%20requires%20about%20a%0Aminute%20of%20flying%20data%20that%20can%20be%20collected%20without%20any%20additional%20equipment%0Aand%20usually%20consists%20of%20three%20simple%20maneuvers.%20Experimental%20results%0Ademonstrate%20the%20ability%20of%20our%20method%20to%20accurately%20recover%20the%20parameters%20of%0Amultiple%20quadrotors.%20It%20also%20facilitates%20the%20deployment%20of%20RL-based%2C%20end-to-end%0Aquadrotor%20control%20of%20a%20large%20quadrotor%20under%20harsh%2C%20outdoor%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07837v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20System%20Identification%20of%20Quadrotors%20Subject%20to%20Motor%20Delays&entry.906535625=Jonas%20Eschmann%20and%20Dario%20Albani%20and%20Giuseppe%20Loianno&entry.1292438233=%20%20Recently%20non-linear%20control%20methods%20like%20Model%20Predictive%20Control%20%28MPC%29%20and%0AReinforcement%20Learning%20%28RL%29%20have%20attracted%20increased%20interest%20in%20the%20quadrotor%0Acontrol%20community.%20In%20contrast%20to%20classic%20control%20methods%20like%20cascaded%20PID%0Acontrollers%2C%20MPC%20and%20RL%20heavily%20rely%20on%20an%20accurate%20model%20of%20the%20system%0Adynamics.%20The%20process%20of%20quadrotor%20system%20identification%20is%20notoriously%20tedious%0Aand%20is%20often%20pursued%20with%20additional%20equipment%20like%20a%20thrust%20stand.%0AFurthermore%2C%20low-level%20details%20like%20motor%20delays%20which%20are%20crucial%20for%20accurate%0Aend-to-end%20control%20are%20often%20neglected.%20In%20this%20work%2C%20we%20introduce%20a%0Adata-driven%20method%20to%20identify%20a%20quadrotor%27s%20inertia%20parameters%2C%20thrust%20curves%2C%0Atorque%20coefficients%2C%20and%20first-order%20motor%20delay%20purely%20based%20on%20proprioceptive%0Adata.%20The%20estimation%20of%20the%20motor%20delay%20is%20particularly%20challenging%20as%20usually%2C%0Athe%20RPMs%20can%20not%20be%20measured.%20We%20derive%20a%20Maximum%20A%20Posteriori%20%28MAP%29-based%0Amethod%20to%20estimate%20the%20latent%20time%20constant.%20Our%20approach%20only%20requires%20about%20a%0Aminute%20of%20flying%20data%20that%20can%20be%20collected%20without%20any%20additional%20equipment%0Aand%20usually%20consists%20of%20three%20simple%20maneuvers.%20Experimental%20results%0Ademonstrate%20the%20ability%20of%20our%20method%20to%20accurately%20recover%20the%20parameters%20of%0Amultiple%20quadrotors.%20It%20also%20facilitates%20the%20deployment%20of%20RL-based%2C%20end-to-end%0Aquadrotor%20control%20of%20a%20large%20quadrotor%20under%20harsh%2C%20outdoor%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07837v1&entry.124074799=Read"},
{"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "author": "Muhammad Khalifa and David Wadden and Emma Strubell and Honglak Lee and Lu Wang and Iz Beltagy and Hao Peng", "abstract": "  Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\npost pretraining recipe that involves (i) training the LLM to associate unique\nsource document identifiers with the knowledge in each document, followed by\n(ii) an instruction-tuning to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training can easily be applied to pretrained\nLLMs off the shelf, and diverges minimally from existing\npretraining/fine-tuning frameworks. Through experiments on carefully curated\ndata, we demonstrate that our training recipe can enable faithful attribution\nto the pretraining data without a substantial impact on the model's quality\ncompared to standard pretraining. Our results also highlight the importance of\ndata augmentation in achieving attribution. Code and data available here:\n\\url{https://github.com/mukhal/intrinsic-source-citation}\n", "link": "http://arxiv.org/abs/2404.01019v2", "date": "2024-04-11", "relevancy": 1.9593, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4977}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4888}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4823}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Source-Aware%20Training%20Enables%20Knowledge%20Attribution%20in%20Language%20Models&body=Title%3A%20Source-Aware%20Training%20Enables%20Knowledge%20Attribution%20in%20Language%20Models%0AAuthor%3A%20Muhammad%20Khalifa%20and%20David%20Wadden%20and%20Emma%20Strubell%20and%20Honglak%20Lee%20and%20Lu%20Wang%20and%20Iz%20Beltagy%20and%20Hao%20Peng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20learn%20a%20vast%20amount%20of%20knowledge%20during%0Apretraining%2C%20but%20they%20are%20often%20oblivious%20to%20the%20source%28s%29%20of%20such%20knowledge.%0AWe%20investigate%20the%20problem%20of%20intrinsic%20source%20citation%2C%20where%20LLMs%20are%0Arequired%20to%20cite%20the%20pretraining%20source%20supporting%20a%20generated%20response.%0AIntrinsic%20source%20citation%20can%20enhance%20LLM%20transparency%2C%20interpretability%2C%20and%0Averifiability.%20To%20give%20LLMs%20such%20ability%2C%20we%20explore%20source-aware%20training%20--%20a%0Apost%20pretraining%20recipe%20that%20involves%20%28i%29%20training%20the%20LLM%20to%20associate%20unique%0Asource%20document%20identifiers%20with%20the%20knowledge%20in%20each%20document%2C%20followed%20by%0A%28ii%29%20an%20instruction-tuning%20to%20teach%20the%20LLM%20to%20cite%20a%20supporting%20pretraining%0Asource%20when%20prompted.%20Source-aware%20training%20can%20easily%20be%20applied%20to%20pretrained%0ALLMs%20off%20the%20shelf%2C%20and%20diverges%20minimally%20from%20existing%0Apretraining/fine-tuning%20frameworks.%20Through%20experiments%20on%20carefully%20curated%0Adata%2C%20we%20demonstrate%20that%20our%20training%20recipe%20can%20enable%20faithful%20attribution%0Ato%20the%20pretraining%20data%20without%20a%20substantial%20impact%20on%20the%20model%27s%20quality%0Acompared%20to%20standard%20pretraining.%20Our%20results%20also%20highlight%20the%20importance%20of%0Adata%20augmentation%20in%20achieving%20attribution.%20Code%20and%20data%20available%20here%3A%0A%5Curl%7Bhttps%3A//github.com/mukhal/intrinsic-source-citation%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01019v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source-Aware%20Training%20Enables%20Knowledge%20Attribution%20in%20Language%20Models&entry.906535625=Muhammad%20Khalifa%20and%20David%20Wadden%20and%20Emma%20Strubell%20and%20Honglak%20Lee%20and%20Lu%20Wang%20and%20Iz%20Beltagy%20and%20Hao%20Peng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20learn%20a%20vast%20amount%20of%20knowledge%20during%0Apretraining%2C%20but%20they%20are%20often%20oblivious%20to%20the%20source%28s%29%20of%20such%20knowledge.%0AWe%20investigate%20the%20problem%20of%20intrinsic%20source%20citation%2C%20where%20LLMs%20are%0Arequired%20to%20cite%20the%20pretraining%20source%20supporting%20a%20generated%20response.%0AIntrinsic%20source%20citation%20can%20enhance%20LLM%20transparency%2C%20interpretability%2C%20and%0Averifiability.%20To%20give%20LLMs%20such%20ability%2C%20we%20explore%20source-aware%20training%20--%20a%0Apost%20pretraining%20recipe%20that%20involves%20%28i%29%20training%20the%20LLM%20to%20associate%20unique%0Asource%20document%20identifiers%20with%20the%20knowledge%20in%20each%20document%2C%20followed%20by%0A%28ii%29%20an%20instruction-tuning%20to%20teach%20the%20LLM%20to%20cite%20a%20supporting%20pretraining%0Asource%20when%20prompted.%20Source-aware%20training%20can%20easily%20be%20applied%20to%20pretrained%0ALLMs%20off%20the%20shelf%2C%20and%20diverges%20minimally%20from%20existing%0Apretraining/fine-tuning%20frameworks.%20Through%20experiments%20on%20carefully%20curated%0Adata%2C%20we%20demonstrate%20that%20our%20training%20recipe%20can%20enable%20faithful%20attribution%0Ato%20the%20pretraining%20data%20without%20a%20substantial%20impact%20on%20the%20model%27s%20quality%0Acompared%20to%20standard%20pretraining.%20Our%20results%20also%20highlight%20the%20importance%20of%0Adata%20augmentation%20in%20achieving%20attribution.%20Code%20and%20data%20available%20here%3A%0A%5Curl%7Bhttps%3A//github.com/mukhal/intrinsic-source-citation%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01019v2&entry.124074799=Read"},
{"title": "Low-rank Adaptation for Spatio-Temporal Forecasting", "author": "Weilin Ruan and Wei Chen and Xilin Dang and Jianxiang Zhou and Weichuang Li and Xu Liu and Yuxuan Liang", "abstract": "  Spatio-temporal forecasting is crucial in real-world dynamic systems,\npredicting future changes using historical data from diverse locations.\nExisting methods often prioritize the development of intricate neural networks\nto capture the complex dependencies of the data, yet their accuracy fails to\nshow sustained improvement. Besides, these methods also overlook node\nheterogeneity, hindering customized prediction modules from handling diverse\nregional nodes effectively. In this paper, our goal is not to propose a new\nmodel but to present a novel low-rank adaptation framework as an off-the-shelf\nplugin for existing spatial-temporal prediction models, termed ST-LoRA, which\nalleviates the aforementioned problems through node-level adjustments.\nSpecifically, we first tailor a node adaptive low-rank layer comprising\nmultiple trainable low-rank matrices. Additionally, we devise a multi-layer\nresidual fusion stacking module, injecting the low-rank adapters into predictor\nmodules of various models. Across six real-world traffic datasets and six\ndifferent types of spatio-temporal prediction models, our approach minimally\nincreases the parameters and training time of the original models by less than\n4%, still achieving consistent and sustained performance enhancement.\n", "link": "http://arxiv.org/abs/2404.07919v1", "date": "2024-04-11", "relevancy": 1.9545, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4942}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.494}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.481}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Low-rank%20Adaptation%20for%20Spatio-Temporal%20Forecasting&body=Title%3A%20Low-rank%20Adaptation%20for%20Spatio-Temporal%20Forecasting%0AAuthor%3A%20Weilin%20Ruan%20and%20Wei%20Chen%20and%20Xilin%20Dang%20and%20Jianxiang%20Zhou%20and%20Weichuang%20Li%20and%20Xu%20Liu%20and%20Yuxuan%20Liang%0AAbstract%3A%20%20%20Spatio-temporal%20forecasting%20is%20crucial%20in%20real-world%20dynamic%20systems%2C%0Apredicting%20future%20changes%20using%20historical%20data%20from%20diverse%20locations.%0AExisting%20methods%20often%20prioritize%20the%20development%20of%20intricate%20neural%20networks%0Ato%20capture%20the%20complex%20dependencies%20of%20the%20data%2C%20yet%20their%20accuracy%20fails%20to%0Ashow%20sustained%20improvement.%20Besides%2C%20these%20methods%20also%20overlook%20node%0Aheterogeneity%2C%20hindering%20customized%20prediction%20modules%20from%20handling%20diverse%0Aregional%20nodes%20effectively.%20In%20this%20paper%2C%20our%20goal%20is%20not%20to%20propose%20a%20new%0Amodel%20but%20to%20present%20a%20novel%20low-rank%20adaptation%20framework%20as%20an%20off-the-shelf%0Aplugin%20for%20existing%20spatial-temporal%20prediction%20models%2C%20termed%20ST-LoRA%2C%20which%0Aalleviates%20the%20aforementioned%20problems%20through%20node-level%20adjustments.%0ASpecifically%2C%20we%20first%20tailor%20a%20node%20adaptive%20low-rank%20layer%20comprising%0Amultiple%20trainable%20low-rank%20matrices.%20Additionally%2C%20we%20devise%20a%20multi-layer%0Aresidual%20fusion%20stacking%20module%2C%20injecting%20the%20low-rank%20adapters%20into%20predictor%0Amodules%20of%20various%20models.%20Across%20six%20real-world%20traffic%20datasets%20and%20six%0Adifferent%20types%20of%20spatio-temporal%20prediction%20models%2C%20our%20approach%20minimally%0Aincreases%20the%20parameters%20and%20training%20time%20of%20the%20original%20models%20by%20less%20than%0A4%25%2C%20still%20achieving%20consistent%20and%20sustained%20performance%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07919v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-rank%20Adaptation%20for%20Spatio-Temporal%20Forecasting&entry.906535625=Weilin%20Ruan%20and%20Wei%20Chen%20and%20Xilin%20Dang%20and%20Jianxiang%20Zhou%20and%20Weichuang%20Li%20and%20Xu%20Liu%20and%20Yuxuan%20Liang&entry.1292438233=%20%20Spatio-temporal%20forecasting%20is%20crucial%20in%20real-world%20dynamic%20systems%2C%0Apredicting%20future%20changes%20using%20historical%20data%20from%20diverse%20locations.%0AExisting%20methods%20often%20prioritize%20the%20development%20of%20intricate%20neural%20networks%0Ato%20capture%20the%20complex%20dependencies%20of%20the%20data%2C%20yet%20their%20accuracy%20fails%20to%0Ashow%20sustained%20improvement.%20Besides%2C%20these%20methods%20also%20overlook%20node%0Aheterogeneity%2C%20hindering%20customized%20prediction%20modules%20from%20handling%20diverse%0Aregional%20nodes%20effectively.%20In%20this%20paper%2C%20our%20goal%20is%20not%20to%20propose%20a%20new%0Amodel%20but%20to%20present%20a%20novel%20low-rank%20adaptation%20framework%20as%20an%20off-the-shelf%0Aplugin%20for%20existing%20spatial-temporal%20prediction%20models%2C%20termed%20ST-LoRA%2C%20which%0Aalleviates%20the%20aforementioned%20problems%20through%20node-level%20adjustments.%0ASpecifically%2C%20we%20first%20tailor%20a%20node%20adaptive%20low-rank%20layer%20comprising%0Amultiple%20trainable%20low-rank%20matrices.%20Additionally%2C%20we%20devise%20a%20multi-layer%0Aresidual%20fusion%20stacking%20module%2C%20injecting%20the%20low-rank%20adapters%20into%20predictor%0Amodules%20of%20various%20models.%20Across%20six%20real-world%20traffic%20datasets%20and%20six%0Adifferent%20types%20of%20spatio-temporal%20prediction%20models%2C%20our%20approach%20minimally%0Aincreases%20the%20parameters%20and%20training%20time%20of%20the%20original%20models%20by%20less%20than%0A4%25%2C%20still%20achieving%20consistent%20and%20sustained%20performance%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07919v1&entry.124074799=Read"},
{"title": "Resolve Domain Conflicts for Generalizable Remote Physiological\n  Measurement", "author": "Weiyu Sun and Xinyu Zhang and Hao Lu and Ying Chen and Yun Ge and Xiaolin Huang and Jie Yuan and Yingcong Chen", "abstract": "  Remote photoplethysmography (rPPG) technology has become increasingly popular\ndue to its non-invasive monitoring of various physiological indicators, making\nit widely applicable in multimedia interaction, healthcare, and emotion\nanalysis. Existing rPPG methods utilize multiple datasets for training to\nenhance the generalizability of models. However, they often overlook the\nunderlying conflict issues across different datasets, such as (1) label\nconflict resulting from different phase delays between physiological signal\nlabels and face videos at the instance level, and (2) attribute conflict\nstemming from distribution shifts caused by head movements, illumination\nchanges, skin types, etc. To address this, we introduce the DOmain-HArmonious\nframework (DOHA). Specifically, we first propose a harmonious phase strategy to\neliminate uncertain phase delays and preserve the temporal variation of\nphysiological signals. Next, we design a harmonious hyperplane optimization\nthat reduces irrelevant attribute shifts and encourages the model's\noptimization towards a global solution that fits more valid scenarios. Our\nexperiments demonstrate that DOHA significantly improves the performance of\nexisting methods under multiple protocols. Our code is available at\nhttps://github.com/SWY666/rPPG-DOHA.\n", "link": "http://arxiv.org/abs/2404.07855v1", "date": "2024-04-11", "relevancy": 1.9515, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4899}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4889}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Resolve%20Domain%20Conflicts%20for%20Generalizable%20Remote%20Physiological%0A%20%20Measurement&body=Title%3A%20Resolve%20Domain%20Conflicts%20for%20Generalizable%20Remote%20Physiological%0A%20%20Measurement%0AAuthor%3A%20Weiyu%20Sun%20and%20Xinyu%20Zhang%20and%20Hao%20Lu%20and%20Ying%20Chen%20and%20Yun%20Ge%20and%20Xiaolin%20Huang%20and%20Jie%20Yuan%20and%20Yingcong%20Chen%0AAbstract%3A%20%20%20Remote%20photoplethysmography%20%28rPPG%29%20technology%20has%20become%20increasingly%20popular%0Adue%20to%20its%20non-invasive%20monitoring%20of%20various%20physiological%20indicators%2C%20making%0Ait%20widely%20applicable%20in%20multimedia%20interaction%2C%20healthcare%2C%20and%20emotion%0Aanalysis.%20Existing%20rPPG%20methods%20utilize%20multiple%20datasets%20for%20training%20to%0Aenhance%20the%20generalizability%20of%20models.%20However%2C%20they%20often%20overlook%20the%0Aunderlying%20conflict%20issues%20across%20different%20datasets%2C%20such%20as%20%281%29%20label%0Aconflict%20resulting%20from%20different%20phase%20delays%20between%20physiological%20signal%0Alabels%20and%20face%20videos%20at%20the%20instance%20level%2C%20and%20%282%29%20attribute%20conflict%0Astemming%20from%20distribution%20shifts%20caused%20by%20head%20movements%2C%20illumination%0Achanges%2C%20skin%20types%2C%20etc.%20To%20address%20this%2C%20we%20introduce%20the%20DOmain-HArmonious%0Aframework%20%28DOHA%29.%20Specifically%2C%20we%20first%20propose%20a%20harmonious%20phase%20strategy%20to%0Aeliminate%20uncertain%20phase%20delays%20and%20preserve%20the%20temporal%20variation%20of%0Aphysiological%20signals.%20Next%2C%20we%20design%20a%20harmonious%20hyperplane%20optimization%0Athat%20reduces%20irrelevant%20attribute%20shifts%20and%20encourages%20the%20model%27s%0Aoptimization%20towards%20a%20global%20solution%20that%20fits%20more%20valid%20scenarios.%20Our%0Aexperiments%20demonstrate%20that%20DOHA%20significantly%20improves%20the%20performance%20of%0Aexisting%20methods%20under%20multiple%20protocols.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/SWY666/rPPG-DOHA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07855v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resolve%20Domain%20Conflicts%20for%20Generalizable%20Remote%20Physiological%0A%20%20Measurement&entry.906535625=Weiyu%20Sun%20and%20Xinyu%20Zhang%20and%20Hao%20Lu%20and%20Ying%20Chen%20and%20Yun%20Ge%20and%20Xiaolin%20Huang%20and%20Jie%20Yuan%20and%20Yingcong%20Chen&entry.1292438233=%20%20Remote%20photoplethysmography%20%28rPPG%29%20technology%20has%20become%20increasingly%20popular%0Adue%20to%20its%20non-invasive%20monitoring%20of%20various%20physiological%20indicators%2C%20making%0Ait%20widely%20applicable%20in%20multimedia%20interaction%2C%20healthcare%2C%20and%20emotion%0Aanalysis.%20Existing%20rPPG%20methods%20utilize%20multiple%20datasets%20for%20training%20to%0Aenhance%20the%20generalizability%20of%20models.%20However%2C%20they%20often%20overlook%20the%0Aunderlying%20conflict%20issues%20across%20different%20datasets%2C%20such%20as%20%281%29%20label%0Aconflict%20resulting%20from%20different%20phase%20delays%20between%20physiological%20signal%0Alabels%20and%20face%20videos%20at%20the%20instance%20level%2C%20and%20%282%29%20attribute%20conflict%0Astemming%20from%20distribution%20shifts%20caused%20by%20head%20movements%2C%20illumination%0Achanges%2C%20skin%20types%2C%20etc.%20To%20address%20this%2C%20we%20introduce%20the%20DOmain-HArmonious%0Aframework%20%28DOHA%29.%20Specifically%2C%20we%20first%20propose%20a%20harmonious%20phase%20strategy%20to%0Aeliminate%20uncertain%20phase%20delays%20and%20preserve%20the%20temporal%20variation%20of%0Aphysiological%20signals.%20Next%2C%20we%20design%20a%20harmonious%20hyperplane%20optimization%0Athat%20reduces%20irrelevant%20attribute%20shifts%20and%20encourages%20the%20model%27s%0Aoptimization%20towards%20a%20global%20solution%20that%20fits%20more%20valid%20scenarios.%20Our%0Aexperiments%20demonstrate%20that%20DOHA%20significantly%20improves%20the%20performance%20of%0Aexisting%20methods%20under%20multiple%20protocols.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/SWY666/rPPG-DOHA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07855v1&entry.124074799=Read"},
{"title": "Rho-1: Not All Tokens Are What You Need", "author": "Zhenghao Lin and Zhibin Gou and Yeyun Gong and Xiao Liu and Yelong Shen and Ruochen Xu and Chen Lin and Yujiu Yang and Jian Jiao and Nan Duan and Weizhu Chen", "abstract": "  Previous language model pre-training methods have uniformly applied a\nnext-token prediction loss to all training tokens. Challenging this norm, we\nposit that \"Not all tokens in a corpus are equally important for language model\ntraining\". Our initial analysis delves into token-level training dynamics of\nlanguage model, revealing distinct loss patterns for different tokens.\nLeveraging these insights, we introduce a new language model called Rho-1.\nUnlike traditional LMs that learn to predict every next token in a corpus,\nRho-1 employs Selective Language Modeling (SLM), which selectively trains on\nuseful tokens that aligned with the desired distribution. This approach\ninvolves scoring pretraining tokens using a reference model, and then training\nthe language model with a focused loss on tokens with higher excess loss. When\ncontinual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute\nimprovement in few-shot accuracy of up to 30% in 9 math tasks. After\nfine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and\n51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the\npretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1\nachieves 6.8% average enhancement across 15 diverse tasks, increasing both\nefficiency and performance of the language model pre-training.\n", "link": "http://arxiv.org/abs/2404.07965v1", "date": "2024-04-11", "relevancy": 1.9511, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4925}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4906}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4819}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rho-1%3A%20Not%20All%20Tokens%20Are%20What%20You%20Need&body=Title%3A%20Rho-1%3A%20Not%20All%20Tokens%20Are%20What%20You%20Need%0AAuthor%3A%20Zhenghao%20Lin%20and%20Zhibin%20Gou%20and%20Yeyun%20Gong%20and%20Xiao%20Liu%20and%20Yelong%20Shen%20and%20Ruochen%20Xu%20and%20Chen%20Lin%20and%20Yujiu%20Yang%20and%20Jian%20Jiao%20and%20Nan%20Duan%20and%20Weizhu%20Chen%0AAbstract%3A%20%20%20Previous%20language%20model%20pre-training%20methods%20have%20uniformly%20applied%20a%0Anext-token%20prediction%20loss%20to%20all%20training%20tokens.%20Challenging%20this%20norm%2C%20we%0Aposit%20that%20%22Not%20all%20tokens%20in%20a%20corpus%20are%20equally%20important%20for%20language%20model%0Atraining%22.%20Our%20initial%20analysis%20delves%20into%20token-level%20training%20dynamics%20of%0Alanguage%20model%2C%20revealing%20distinct%20loss%20patterns%20for%20different%20tokens.%0ALeveraging%20these%20insights%2C%20we%20introduce%20a%20new%20language%20model%20called%20Rho-1.%0AUnlike%20traditional%20LMs%20that%20learn%20to%20predict%20every%20next%20token%20in%20a%20corpus%2C%0ARho-1%20employs%20Selective%20Language%20Modeling%20%28SLM%29%2C%20which%20selectively%20trains%20on%0Auseful%20tokens%20that%20aligned%20with%20the%20desired%20distribution.%20This%20approach%0Ainvolves%20scoring%20pretraining%20tokens%20using%20a%20reference%20model%2C%20and%20then%20training%0Athe%20language%20model%20with%20a%20focused%20loss%20on%20tokens%20with%20higher%20excess%20loss.%20When%0Acontinual%20pretraining%20on%2015B%20OpenWebMath%20corpus%2C%20Rho-1%20yields%20an%20absolute%0Aimprovement%20in%20few-shot%20accuracy%20of%20up%20to%2030%25%20in%209%20math%20tasks.%20After%0Afine-tuning%2C%20Rho-1-1B%20and%207B%20achieved%20state-of-the-art%20results%20of%2040.6%25%20and%0A51.8%25%20on%20MATH%20dataset%2C%20respectively%20-%20matching%20DeepSeekMath%20with%20only%203%25%20of%20the%0Apretraining%20tokens.%20Furthermore%2C%20when%20pretraining%20on%2080B%20general%20tokens%2C%20Rho-1%0Aachieves%206.8%25%20average%20enhancement%20across%2015%20diverse%20tasks%2C%20increasing%20both%0Aefficiency%20and%20performance%20of%20the%20language%20model%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07965v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rho-1%3A%20Not%20All%20Tokens%20Are%20What%20You%20Need&entry.906535625=Zhenghao%20Lin%20and%20Zhibin%20Gou%20and%20Yeyun%20Gong%20and%20Xiao%20Liu%20and%20Yelong%20Shen%20and%20Ruochen%20Xu%20and%20Chen%20Lin%20and%20Yujiu%20Yang%20and%20Jian%20Jiao%20and%20Nan%20Duan%20and%20Weizhu%20Chen&entry.1292438233=%20%20Previous%20language%20model%20pre-training%20methods%20have%20uniformly%20applied%20a%0Anext-token%20prediction%20loss%20to%20all%20training%20tokens.%20Challenging%20this%20norm%2C%20we%0Aposit%20that%20%22Not%20all%20tokens%20in%20a%20corpus%20are%20equally%20important%20for%20language%20model%0Atraining%22.%20Our%20initial%20analysis%20delves%20into%20token-level%20training%20dynamics%20of%0Alanguage%20model%2C%20revealing%20distinct%20loss%20patterns%20for%20different%20tokens.%0ALeveraging%20these%20insights%2C%20we%20introduce%20a%20new%20language%20model%20called%20Rho-1.%0AUnlike%20traditional%20LMs%20that%20learn%20to%20predict%20every%20next%20token%20in%20a%20corpus%2C%0ARho-1%20employs%20Selective%20Language%20Modeling%20%28SLM%29%2C%20which%20selectively%20trains%20on%0Auseful%20tokens%20that%20aligned%20with%20the%20desired%20distribution.%20This%20approach%0Ainvolves%20scoring%20pretraining%20tokens%20using%20a%20reference%20model%2C%20and%20then%20training%0Athe%20language%20model%20with%20a%20focused%20loss%20on%20tokens%20with%20higher%20excess%20loss.%20When%0Acontinual%20pretraining%20on%2015B%20OpenWebMath%20corpus%2C%20Rho-1%20yields%20an%20absolute%0Aimprovement%20in%20few-shot%20accuracy%20of%20up%20to%2030%25%20in%209%20math%20tasks.%20After%0Afine-tuning%2C%20Rho-1-1B%20and%207B%20achieved%20state-of-the-art%20results%20of%2040.6%25%20and%0A51.8%25%20on%20MATH%20dataset%2C%20respectively%20-%20matching%20DeepSeekMath%20with%20only%203%25%20of%20the%0Apretraining%20tokens.%20Furthermore%2C%20when%20pretraining%20on%2080B%20general%20tokens%2C%20Rho-1%0Aachieves%206.8%25%20average%20enhancement%20across%2015%20diverse%20tasks%2C%20increasing%20both%0Aefficiency%20and%20performance%20of%20the%20language%20model%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07965v1&entry.124074799=Read"},
{"title": "Me LLaMA: Foundation Large Language Models for Medical Applications", "author": "Qianqian Xie and Qingyu Chen and Aokun Chen and Cheng Peng and Yan Hu and Fongci Lin and Xueqing Peng and Jimin Huang and Jeffrey Zhang and Vipina Keloth and Xinyu Zhou and Huan He and Lucila Ohno-Machado and Yonghui Wu and Hua Xu and Jiang Bian", "abstract": "  Recent advancements in large language models (LLMs) such as ChatGPT and LLaMA\nhave hinted at their potential to revolutionize medical applications, yet their\napplication in clinical settings often reveals limitations due to a lack of\nspecialized training on medical-specific data. In response to this challenge,\nthis study introduces Me-LLaMA, a novel medical LLM family that includes\nfoundation models - Me-LLaMA 13/70B, along with their chat-enhanced versions -\nMe-LLaMA 13/70B-chat, developed through continual pre-training and instruction\ntuning of LLaMA2 using large medical datasets. Our methodology leverages a\ncomprehensive domain-specific data suite, including a large-scale, continual\npre-training dataset with 129B tokens, an instruction tuning dataset with 214k\nsamples, and a new medical evaluation benchmark (MIBE) across six critical\nmedical tasks with 12 datasets. Our extensive evaluation using the MIBE shows\nthat Me-LLaMA models achieve overall better performance than existing\nopen-source medical LLMs in zero-shot, few-shot and supervised learning\nabilities. With task-specific instruction tuning, Me-LLaMA models outperform\nChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets. In addition,\nwe investigated the catastrophic forgetting problem, and our results show that\nMe-LLaMA models outperform other open-source medical LLMs in mitigating this\nissue. Me-LLaMA is one of the largest open-source medical foundation LLMs that\nuse both biomedical and clinical data. It exhibits superior performance across\nboth general and medical tasks compared to other open-source medical LLMs,\nrendering it an attractive choice for medical AI applications. We release our\nmodels, datasets, and evaluation scripts at:\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.\n", "link": "http://arxiv.org/abs/2402.12749v4", "date": "2024-04-11", "relevancy": 1.9507, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5089}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Me%20LLaMA%3A%20Foundation%20Large%20Language%20Models%20for%20Medical%20Applications&body=Title%3A%20Me%20LLaMA%3A%20Foundation%20Large%20Language%20Models%20for%20Medical%20Applications%0AAuthor%3A%20Qianqian%20Xie%20and%20Qingyu%20Chen%20and%20Aokun%20Chen%20and%20Cheng%20Peng%20and%20Yan%20Hu%20and%20Fongci%20Lin%20and%20Xueqing%20Peng%20and%20Jimin%20Huang%20and%20Jeffrey%20Zhang%20and%20Vipina%20Keloth%20and%20Xinyu%20Zhou%20and%20Huan%20He%20and%20Lucila%20Ohno-Machado%20and%20Yonghui%20Wu%20and%20Hua%20Xu%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20such%20as%20ChatGPT%20and%20LLaMA%0Ahave%20hinted%20at%20their%20potential%20to%20revolutionize%20medical%20applications%2C%20yet%20their%0Aapplication%20in%20clinical%20settings%20often%20reveals%20limitations%20due%20to%20a%20lack%20of%0Aspecialized%20training%20on%20medical-specific%20data.%20In%20response%20to%20this%20challenge%2C%0Athis%20study%20introduces%20Me-LLaMA%2C%20a%20novel%20medical%20LLM%20family%20that%20includes%0Afoundation%20models%20-%20Me-LLaMA%2013/70B%2C%20along%20with%20their%20chat-enhanced%20versions%20-%0AMe-LLaMA%2013/70B-chat%2C%20developed%20through%20continual%20pre-training%20and%20instruction%0Atuning%20of%20LLaMA2%20using%20large%20medical%20datasets.%20Our%20methodology%20leverages%20a%0Acomprehensive%20domain-specific%20data%20suite%2C%20including%20a%20large-scale%2C%20continual%0Apre-training%20dataset%20with%20129B%20tokens%2C%20an%20instruction%20tuning%20dataset%20with%20214k%0Asamples%2C%20and%20a%20new%20medical%20evaluation%20benchmark%20%28MIBE%29%20across%20six%20critical%0Amedical%20tasks%20with%2012%20datasets.%20Our%20extensive%20evaluation%20using%20the%20MIBE%20shows%0Athat%20Me-LLaMA%20models%20achieve%20overall%20better%20performance%20than%20existing%0Aopen-source%20medical%20LLMs%20in%20zero-shot%2C%20few-shot%20and%20supervised%20learning%0Aabilities.%20With%20task-specific%20instruction%20tuning%2C%20Me-LLaMA%20models%20outperform%0AChatGPT%20on%207%20out%20of%208%20datasets%20and%20GPT-4%20on%205%20out%20of%208%20datasets.%20In%20addition%2C%0Awe%20investigated%20the%20catastrophic%20forgetting%20problem%2C%20and%20our%20results%20show%20that%0AMe-LLaMA%20models%20outperform%20other%20open-source%20medical%20LLMs%20in%20mitigating%20this%0Aissue.%20Me-LLaMA%20is%20one%20of%20the%20largest%20open-source%20medical%20foundation%20LLMs%20that%0Ause%20both%20biomedical%20and%20clinical%20data.%20It%20exhibits%20superior%20performance%20across%0Aboth%20general%20and%20medical%20tasks%20compared%20to%20other%20open-source%20medical%20LLMs%2C%0Arendering%20it%20an%20attractive%20choice%20for%20medical%20AI%20applications.%20We%20release%20our%0Amodels%2C%20datasets%2C%20and%20evaluation%20scripts%20at%3A%0Ahttps%3A//github.com/BIDS-Xu-Lab/Me-LLaMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12749v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Me%20LLaMA%3A%20Foundation%20Large%20Language%20Models%20for%20Medical%20Applications&entry.906535625=Qianqian%20Xie%20and%20Qingyu%20Chen%20and%20Aokun%20Chen%20and%20Cheng%20Peng%20and%20Yan%20Hu%20and%20Fongci%20Lin%20and%20Xueqing%20Peng%20and%20Jimin%20Huang%20and%20Jeffrey%20Zhang%20and%20Vipina%20Keloth%20and%20Xinyu%20Zhou%20and%20Huan%20He%20and%20Lucila%20Ohno-Machado%20and%20Yonghui%20Wu%20and%20Hua%20Xu%20and%20Jiang%20Bian&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20such%20as%20ChatGPT%20and%20LLaMA%0Ahave%20hinted%20at%20their%20potential%20to%20revolutionize%20medical%20applications%2C%20yet%20their%0Aapplication%20in%20clinical%20settings%20often%20reveals%20limitations%20due%20to%20a%20lack%20of%0Aspecialized%20training%20on%20medical-specific%20data.%20In%20response%20to%20this%20challenge%2C%0Athis%20study%20introduces%20Me-LLaMA%2C%20a%20novel%20medical%20LLM%20family%20that%20includes%0Afoundation%20models%20-%20Me-LLaMA%2013/70B%2C%20along%20with%20their%20chat-enhanced%20versions%20-%0AMe-LLaMA%2013/70B-chat%2C%20developed%20through%20continual%20pre-training%20and%20instruction%0Atuning%20of%20LLaMA2%20using%20large%20medical%20datasets.%20Our%20methodology%20leverages%20a%0Acomprehensive%20domain-specific%20data%20suite%2C%20including%20a%20large-scale%2C%20continual%0Apre-training%20dataset%20with%20129B%20tokens%2C%20an%20instruction%20tuning%20dataset%20with%20214k%0Asamples%2C%20and%20a%20new%20medical%20evaluation%20benchmark%20%28MIBE%29%20across%20six%20critical%0Amedical%20tasks%20with%2012%20datasets.%20Our%20extensive%20evaluation%20using%20the%20MIBE%20shows%0Athat%20Me-LLaMA%20models%20achieve%20overall%20better%20performance%20than%20existing%0Aopen-source%20medical%20LLMs%20in%20zero-shot%2C%20few-shot%20and%20supervised%20learning%0Aabilities.%20With%20task-specific%20instruction%20tuning%2C%20Me-LLaMA%20models%20outperform%0AChatGPT%20on%207%20out%20of%208%20datasets%20and%20GPT-4%20on%205%20out%20of%208%20datasets.%20In%20addition%2C%0Awe%20investigated%20the%20catastrophic%20forgetting%20problem%2C%20and%20our%20results%20show%20that%0AMe-LLaMA%20models%20outperform%20other%20open-source%20medical%20LLMs%20in%20mitigating%20this%0Aissue.%20Me-LLaMA%20is%20one%20of%20the%20largest%20open-source%20medical%20foundation%20LLMs%20that%0Ause%20both%20biomedical%20and%20clinical%20data.%20It%20exhibits%20superior%20performance%20across%0Aboth%20general%20and%20medical%20tasks%20compared%20to%20other%20open-source%20medical%20LLMs%2C%0Arendering%20it%20an%20attractive%20choice%20for%20medical%20AI%20applications.%20We%20release%20our%0Amodels%2C%20datasets%2C%20and%20evaluation%20scripts%20at%3A%0Ahttps%3A//github.com/BIDS-Xu-Lab/Me-LLaMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12749v4&entry.124074799=Read"},
{"title": "Minimizing Chebyshev Prototype Risk Magically Mitigates the Perils of\n  Overfitting", "author": "Nathaniel Dean and Dilip Sarkar", "abstract": "  Overparameterized deep neural networks (DNNs), if not sufficiently\nregularized, are susceptible to overfitting their training examples and not\ngeneralizing well to test data. To discourage overfitting, researchers have\ndeveloped multicomponent loss functions that reduce intra-class feature\ncorrelation and maximize inter-class feature distance in one or more layers of\nthe network. By analyzing the penultimate feature layer activations output by a\nDNN's feature extraction section prior to the linear classifier, we find that\nmodified forms of the intra-class feature covariance and inter-class prototype\nseparation are key components of a fundamental Chebyshev upper bound on the\nprobability of misclassification, which we designate the Chebyshev Prototype\nRisk (CPR). While previous approaches' covariance loss terms scale\nquadratically with the number of network features, our CPR bound indicates that\nan approximate covariance loss in log-linear time is sufficient to reduce the\nbound and is scalable to large architectures. We implement the terms of the CPR\nbound into our Explicit CPR (exCPR) loss function and observe from empirical\nresults on multiple datasets and network architectures that our training\nalgorithm reduces overfitting and improves upon previous approaches in many\nsettings. Our code is available at\nhttps://github.com/Deano1718/Regularization_exCPR .\n", "link": "http://arxiv.org/abs/2404.07083v2", "date": "2024-04-11", "relevancy": 1.9498, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5441}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4507}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Minimizing%20Chebyshev%20Prototype%20Risk%20Magically%20Mitigates%20the%20Perils%20of%0A%20%20Overfitting&body=Title%3A%20Minimizing%20Chebyshev%20Prototype%20Risk%20Magically%20Mitigates%20the%20Perils%20of%0A%20%20Overfitting%0AAuthor%3A%20Nathaniel%20Dean%20and%20Dilip%20Sarkar%0AAbstract%3A%20%20%20Overparameterized%20deep%20neural%20networks%20%28DNNs%29%2C%20if%20not%20sufficiently%0Aregularized%2C%20are%20susceptible%20to%20overfitting%20their%20training%20examples%20and%20not%0Ageneralizing%20well%20to%20test%20data.%20To%20discourage%20overfitting%2C%20researchers%20have%0Adeveloped%20multicomponent%20loss%20functions%20that%20reduce%20intra-class%20feature%0Acorrelation%20and%20maximize%20inter-class%20feature%20distance%20in%20one%20or%20more%20layers%20of%0Athe%20network.%20By%20analyzing%20the%20penultimate%20feature%20layer%20activations%20output%20by%20a%0ADNN%27s%20feature%20extraction%20section%20prior%20to%20the%20linear%20classifier%2C%20we%20find%20that%0Amodified%20forms%20of%20the%20intra-class%20feature%20covariance%20and%20inter-class%20prototype%0Aseparation%20are%20key%20components%20of%20a%20fundamental%20Chebyshev%20upper%20bound%20on%20the%0Aprobability%20of%20misclassification%2C%20which%20we%20designate%20the%20Chebyshev%20Prototype%0ARisk%20%28CPR%29.%20While%20previous%20approaches%27%20covariance%20loss%20terms%20scale%0Aquadratically%20with%20the%20number%20of%20network%20features%2C%20our%20CPR%20bound%20indicates%20that%0Aan%20approximate%20covariance%20loss%20in%20log-linear%20time%20is%20sufficient%20to%20reduce%20the%0Abound%20and%20is%20scalable%20to%20large%20architectures.%20We%20implement%20the%20terms%20of%20the%20CPR%0Abound%20into%20our%20Explicit%20CPR%20%28exCPR%29%20loss%20function%20and%20observe%20from%20empirical%0Aresults%20on%20multiple%20datasets%20and%20network%20architectures%20that%20our%20training%0Aalgorithm%20reduces%20overfitting%20and%20improves%20upon%20previous%20approaches%20in%20many%0Asettings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Deano1718/Regularization_exCPR%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07083v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimizing%20Chebyshev%20Prototype%20Risk%20Magically%20Mitigates%20the%20Perils%20of%0A%20%20Overfitting&entry.906535625=Nathaniel%20Dean%20and%20Dilip%20Sarkar&entry.1292438233=%20%20Overparameterized%20deep%20neural%20networks%20%28DNNs%29%2C%20if%20not%20sufficiently%0Aregularized%2C%20are%20susceptible%20to%20overfitting%20their%20training%20examples%20and%20not%0Ageneralizing%20well%20to%20test%20data.%20To%20discourage%20overfitting%2C%20researchers%20have%0Adeveloped%20multicomponent%20loss%20functions%20that%20reduce%20intra-class%20feature%0Acorrelation%20and%20maximize%20inter-class%20feature%20distance%20in%20one%20or%20more%20layers%20of%0Athe%20network.%20By%20analyzing%20the%20penultimate%20feature%20layer%20activations%20output%20by%20a%0ADNN%27s%20feature%20extraction%20section%20prior%20to%20the%20linear%20classifier%2C%20we%20find%20that%0Amodified%20forms%20of%20the%20intra-class%20feature%20covariance%20and%20inter-class%20prototype%0Aseparation%20are%20key%20components%20of%20a%20fundamental%20Chebyshev%20upper%20bound%20on%20the%0Aprobability%20of%20misclassification%2C%20which%20we%20designate%20the%20Chebyshev%20Prototype%0ARisk%20%28CPR%29.%20While%20previous%20approaches%27%20covariance%20loss%20terms%20scale%0Aquadratically%20with%20the%20number%20of%20network%20features%2C%20our%20CPR%20bound%20indicates%20that%0Aan%20approximate%20covariance%20loss%20in%20log-linear%20time%20is%20sufficient%20to%20reduce%20the%0Abound%20and%20is%20scalable%20to%20large%20architectures.%20We%20implement%20the%20terms%20of%20the%20CPR%0Abound%20into%20our%20Explicit%20CPR%20%28exCPR%29%20loss%20function%20and%20observe%20from%20empirical%0Aresults%20on%20multiple%20datasets%20and%20network%20architectures%20that%20our%20training%0Aalgorithm%20reduces%20overfitting%20and%20improves%20upon%20previous%20approaches%20in%20many%0Asettings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Deano1718/Regularization_exCPR%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07083v2&entry.124074799=Read"},
{"title": "Flattening the Parent Bias: Hierarchical Semantic Segmentation in the\n  Poincar{\u00e9} Ball", "author": "Simon Weber and Bar\u0131\u015f Z\u00f6ng\u00fcr and Nikita Araslanov and Daniel Cremers", "abstract": "  Hierarchy is a natural representation of semantic taxonomies, including the\nones routinely used in image segmentation. Indeed, recent work on semantic\nsegmentation reports improved accuracy from supervised training leveraging\nhierarchical label structures. Encouraged by these results, we revisit the\nfundamental assumptions behind that work. We postulate and then empirically\nverify that the reasons for the observed improvement in segmentation accuracy\nmay be entirely unrelated to the use of the semantic hierarchy. To demonstrate\nthis, we design a range of cross-domain experiments with a representative\nhierarchical approach. We find that on the new testing domains, a flat\n(non-hierarchical) segmentation network, in which the parents are inferred from\nthe children, has superior segmentation accuracy to the hierarchical approach\nacross the board. Complementing these findings and inspired by the intrinsic\nproperties of hyperbolic spaces, we study a more principled approach to\nhierarchical segmentation using the Poincar\\'e ball model. The hyperbolic\nrepresentation largely outperforms the previous (Euclidean) hierarchical\napproach as well and is on par with our flat Euclidean baseline in terms of\nsegmentation accuracy. However, it additionally exhibits surprisingly strong\ncalibration quality of the parent nodes in the semantic hierarchy, especially\non the more challenging domains. Our combined analysis suggests that the\nestablished practice of hierarchical segmentation may be limited to in-domain\nsettings, whereas flat classifiers generalize substantially better, especially\nif they are modeled in the hyperbolic space.\n", "link": "http://arxiv.org/abs/2404.03778v2", "date": "2024-04-11", "relevancy": 1.9394, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5171}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4803}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4765}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Flattening%20the%20Parent%20Bias%3A%20Hierarchical%20Semantic%20Segmentation%20in%20the%0A%20%20Poincar%7B%C3%A9%7D%20Ball&body=Title%3A%20Flattening%20the%20Parent%20Bias%3A%20Hierarchical%20Semantic%20Segmentation%20in%20the%0A%20%20Poincar%7B%C3%A9%7D%20Ball%0AAuthor%3A%20Simon%20Weber%20and%20Bar%C4%B1%C5%9F%20Z%C3%B6ng%C3%BCr%20and%20Nikita%20Araslanov%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Hierarchy%20is%20a%20natural%20representation%20of%20semantic%20taxonomies%2C%20including%20the%0Aones%20routinely%20used%20in%20image%20segmentation.%20Indeed%2C%20recent%20work%20on%20semantic%0Asegmentation%20reports%20improved%20accuracy%20from%20supervised%20training%20leveraging%0Ahierarchical%20label%20structures.%20Encouraged%20by%20these%20results%2C%20we%20revisit%20the%0Afundamental%20assumptions%20behind%20that%20work.%20We%20postulate%20and%20then%20empirically%0Averify%20that%20the%20reasons%20for%20the%20observed%20improvement%20in%20segmentation%20accuracy%0Amay%20be%20entirely%20unrelated%20to%20the%20use%20of%20the%20semantic%20hierarchy.%20To%20demonstrate%0Athis%2C%20we%20design%20a%20range%20of%20cross-domain%20experiments%20with%20a%20representative%0Ahierarchical%20approach.%20We%20find%20that%20on%20the%20new%20testing%20domains%2C%20a%20flat%0A%28non-hierarchical%29%20segmentation%20network%2C%20in%20which%20the%20parents%20are%20inferred%20from%0Athe%20children%2C%20has%20superior%20segmentation%20accuracy%20to%20the%20hierarchical%20approach%0Aacross%20the%20board.%20Complementing%20these%20findings%20and%20inspired%20by%20the%20intrinsic%0Aproperties%20of%20hyperbolic%20spaces%2C%20we%20study%20a%20more%20principled%20approach%20to%0Ahierarchical%20segmentation%20using%20the%20Poincar%5C%27e%20ball%20model.%20The%20hyperbolic%0Arepresentation%20largely%20outperforms%20the%20previous%20%28Euclidean%29%20hierarchical%0Aapproach%20as%20well%20and%20is%20on%20par%20with%20our%20flat%20Euclidean%20baseline%20in%20terms%20of%0Asegmentation%20accuracy.%20However%2C%20it%20additionally%20exhibits%20surprisingly%20strong%0Acalibration%20quality%20of%20the%20parent%20nodes%20in%20the%20semantic%20hierarchy%2C%20especially%0Aon%20the%20more%20challenging%20domains.%20Our%20combined%20analysis%20suggests%20that%20the%0Aestablished%20practice%20of%20hierarchical%20segmentation%20may%20be%20limited%20to%20in-domain%0Asettings%2C%20whereas%20flat%20classifiers%20generalize%20substantially%20better%2C%20especially%0Aif%20they%20are%20modeled%20in%20the%20hyperbolic%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03778v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flattening%20the%20Parent%20Bias%3A%20Hierarchical%20Semantic%20Segmentation%20in%20the%0A%20%20Poincar%7B%C3%A9%7D%20Ball&entry.906535625=Simon%20Weber%20and%20Bar%C4%B1%C5%9F%20Z%C3%B6ng%C3%BCr%20and%20Nikita%20Araslanov%20and%20Daniel%20Cremers&entry.1292438233=%20%20Hierarchy%20is%20a%20natural%20representation%20of%20semantic%20taxonomies%2C%20including%20the%0Aones%20routinely%20used%20in%20image%20segmentation.%20Indeed%2C%20recent%20work%20on%20semantic%0Asegmentation%20reports%20improved%20accuracy%20from%20supervised%20training%20leveraging%0Ahierarchical%20label%20structures.%20Encouraged%20by%20these%20results%2C%20we%20revisit%20the%0Afundamental%20assumptions%20behind%20that%20work.%20We%20postulate%20and%20then%20empirically%0Averify%20that%20the%20reasons%20for%20the%20observed%20improvement%20in%20segmentation%20accuracy%0Amay%20be%20entirely%20unrelated%20to%20the%20use%20of%20the%20semantic%20hierarchy.%20To%20demonstrate%0Athis%2C%20we%20design%20a%20range%20of%20cross-domain%20experiments%20with%20a%20representative%0Ahierarchical%20approach.%20We%20find%20that%20on%20the%20new%20testing%20domains%2C%20a%20flat%0A%28non-hierarchical%29%20segmentation%20network%2C%20in%20which%20the%20parents%20are%20inferred%20from%0Athe%20children%2C%20has%20superior%20segmentation%20accuracy%20to%20the%20hierarchical%20approach%0Aacross%20the%20board.%20Complementing%20these%20findings%20and%20inspired%20by%20the%20intrinsic%0Aproperties%20of%20hyperbolic%20spaces%2C%20we%20study%20a%20more%20principled%20approach%20to%0Ahierarchical%20segmentation%20using%20the%20Poincar%5C%27e%20ball%20model.%20The%20hyperbolic%0Arepresentation%20largely%20outperforms%20the%20previous%20%28Euclidean%29%20hierarchical%0Aapproach%20as%20well%20and%20is%20on%20par%20with%20our%20flat%20Euclidean%20baseline%20in%20terms%20of%0Asegmentation%20accuracy.%20However%2C%20it%20additionally%20exhibits%20surprisingly%20strong%0Acalibration%20quality%20of%20the%20parent%20nodes%20in%20the%20semantic%20hierarchy%2C%20especially%0Aon%20the%20more%20challenging%20domains.%20Our%20combined%20analysis%20suggests%20that%20the%0Aestablished%20practice%20of%20hierarchical%20segmentation%20may%20be%20limited%20to%20in-domain%0Asettings%2C%20whereas%20flat%20classifiers%20generalize%20substantially%20better%2C%20especially%0Aif%20they%20are%20modeled%20in%20the%20hyperbolic%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03778v2&entry.124074799=Read"},
{"title": "GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo", "author": "Jiang Wu and Rui Li and Haofei Xu and Wenxun Zhao and Yu Zhu and Jinqiu Sun and Yanning Zhang", "abstract": "  Matching cost aggregation plays a fundamental role in learning-based\nmulti-view stereo networks. However, directly aggregating adjacent costs can\nlead to suboptimal results due to local geometric inconsistency. Related\nmethods either seek selective aggregation or improve aggregated depth in the 2D\nspace, both are unable to handle geometric inconsistency in the cost volume\neffectively. In this paper, we propose GoMVS to aggregate geometrically\nconsistent costs, yielding better utilization of adjacent geometries. More\nspecifically, we correspond and propagate adjacent costs to the reference pixel\nby leveraging the local geometric smoothness in conjunction with surface\nnormals. We achieve this by the geometric consistent propagation (GCP) module.\nIt computes the correspondence from the adjacent depth hypothesis space to the\nreference depth space using surface normals, then uses the correspondence to\npropagate adjacent costs to the reference geometry, followed by a convolution\nfor aggregation. Our method achieves new state-of-the-art performance on DTU,\nTanks & Temple, and ETH3D datasets. Notably, our method ranks 1st on the Tanks\n& Temple Advanced benchmark.\n", "link": "http://arxiv.org/abs/2404.07992v1", "date": "2024-04-11", "relevancy": 1.9333, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4935}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4748}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GoMVS%3A%20Geometrically%20Consistent%20Cost%20Aggregation%20for%20Multi-View%20Stereo&body=Title%3A%20GoMVS%3A%20Geometrically%20Consistent%20Cost%20Aggregation%20for%20Multi-View%20Stereo%0AAuthor%3A%20Jiang%20Wu%20and%20Rui%20Li%20and%20Haofei%20Xu%20and%20Wenxun%20Zhao%20and%20Yu%20Zhu%20and%20Jinqiu%20Sun%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20Matching%20cost%20aggregation%20plays%20a%20fundamental%20role%20in%20learning-based%0Amulti-view%20stereo%20networks.%20However%2C%20directly%20aggregating%20adjacent%20costs%20can%0Alead%20to%20suboptimal%20results%20due%20to%20local%20geometric%20inconsistency.%20Related%0Amethods%20either%20seek%20selective%20aggregation%20or%20improve%20aggregated%20depth%20in%20the%202D%0Aspace%2C%20both%20are%20unable%20to%20handle%20geometric%20inconsistency%20in%20the%20cost%20volume%0Aeffectively.%20In%20this%20paper%2C%20we%20propose%20GoMVS%20to%20aggregate%20geometrically%0Aconsistent%20costs%2C%20yielding%20better%20utilization%20of%20adjacent%20geometries.%20More%0Aspecifically%2C%20we%20correspond%20and%20propagate%20adjacent%20costs%20to%20the%20reference%20pixel%0Aby%20leveraging%20the%20local%20geometric%20smoothness%20in%20conjunction%20with%20surface%0Anormals.%20We%20achieve%20this%20by%20the%20geometric%20consistent%20propagation%20%28GCP%29%20module.%0AIt%20computes%20the%20correspondence%20from%20the%20adjacent%20depth%20hypothesis%20space%20to%20the%0Areference%20depth%20space%20using%20surface%20normals%2C%20then%20uses%20the%20correspondence%20to%0Apropagate%20adjacent%20costs%20to%20the%20reference%20geometry%2C%20followed%20by%20a%20convolution%0Afor%20aggregation.%20Our%20method%20achieves%20new%20state-of-the-art%20performance%20on%20DTU%2C%0ATanks%20%26%20Temple%2C%20and%20ETH3D%20datasets.%20Notably%2C%20our%20method%20ranks%201st%20on%20the%20Tanks%0A%26%20Temple%20Advanced%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07992v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GoMVS%3A%20Geometrically%20Consistent%20Cost%20Aggregation%20for%20Multi-View%20Stereo&entry.906535625=Jiang%20Wu%20and%20Rui%20Li%20and%20Haofei%20Xu%20and%20Wenxun%20Zhao%20and%20Yu%20Zhu%20and%20Jinqiu%20Sun%20and%20Yanning%20Zhang&entry.1292438233=%20%20Matching%20cost%20aggregation%20plays%20a%20fundamental%20role%20in%20learning-based%0Amulti-view%20stereo%20networks.%20However%2C%20directly%20aggregating%20adjacent%20costs%20can%0Alead%20to%20suboptimal%20results%20due%20to%20local%20geometric%20inconsistency.%20Related%0Amethods%20either%20seek%20selective%20aggregation%20or%20improve%20aggregated%20depth%20in%20the%202D%0Aspace%2C%20both%20are%20unable%20to%20handle%20geometric%20inconsistency%20in%20the%20cost%20volume%0Aeffectively.%20In%20this%20paper%2C%20we%20propose%20GoMVS%20to%20aggregate%20geometrically%0Aconsistent%20costs%2C%20yielding%20better%20utilization%20of%20adjacent%20geometries.%20More%0Aspecifically%2C%20we%20correspond%20and%20propagate%20adjacent%20costs%20to%20the%20reference%20pixel%0Aby%20leveraging%20the%20local%20geometric%20smoothness%20in%20conjunction%20with%20surface%0Anormals.%20We%20achieve%20this%20by%20the%20geometric%20consistent%20propagation%20%28GCP%29%20module.%0AIt%20computes%20the%20correspondence%20from%20the%20adjacent%20depth%20hypothesis%20space%20to%20the%0Areference%20depth%20space%20using%20surface%20normals%2C%20then%20uses%20the%20correspondence%20to%0Apropagate%20adjacent%20costs%20to%20the%20reference%20geometry%2C%20followed%20by%20a%20convolution%0Afor%20aggregation.%20Our%20method%20achieves%20new%20state-of-the-art%20performance%20on%20DTU%2C%0ATanks%20%26%20Temple%2C%20and%20ETH3D%20datasets.%20Notably%2C%20our%20method%20ranks%201st%20on%20the%20Tanks%0A%26%20Temple%20Advanced%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07992v1&entry.124074799=Read"},
{"title": "Goal Recognition via Linear Programming", "author": "Felipe Meneguzzi and Lu\u00edsa R. de A. Santos and Ramon Fraga Pereira and Andr\u00e9 G. Pereira", "abstract": "  Goal Recognition is the task by which an observer aims to discern the goals\nthat correspond to plans that comply with the perceived behavior of subject\nagents given as a sequence of observations. Research on Goal Recognition as\nPlanning encompasses reasoning about the model of a planning task, the\nobservations, and the goals using planning techniques, resulting in very\nefficient recognition approaches. In this article, we design novel recognition\napproaches that rely on the Operator-Counting framework, proposing new\nconstraints, and analyze their constraints' properties both theoretically and\nempirically. The Operator-Counting framework is a technique that efficiently\ncomputes heuristic estimates of cost-to-goal using Integer/Linear Programming\n(IP/LP). In the realm of theory, we prove that the new constraints provide\nlower bounds on the cost of plans that comply with observations. We also\nprovide an extensive empirical evaluation to assess how the new constraints\nimprove the quality of the solution, and we found that they are especially\ninformed in deciding which goals are unlikely to be part of the solution. Our\nnovel recognition approaches have two pivotal advantages: first, they employ\nnew IP/LP constraints for efficiently recognizing goals; second, we show how\nthe new IP/LP constraints can improve the recognition of goals under both\npartial and noisy observability.\n", "link": "http://arxiv.org/abs/2404.07934v1", "date": "2024-04-11", "relevancy": 1.9309, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5017}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4693}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4688}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Goal%20Recognition%20via%20Linear%20Programming&body=Title%3A%20Goal%20Recognition%20via%20Linear%20Programming%0AAuthor%3A%20Felipe%20Meneguzzi%20and%20Lu%C3%ADsa%20R.%20de%20A.%20Santos%20and%20Ramon%20Fraga%20Pereira%20and%20Andr%C3%A9%20G.%20Pereira%0AAbstract%3A%20%20%20Goal%20Recognition%20is%20the%20task%20by%20which%20an%20observer%20aims%20to%20discern%20the%20goals%0Athat%20correspond%20to%20plans%20that%20comply%20with%20the%20perceived%20behavior%20of%20subject%0Aagents%20given%20as%20a%20sequence%20of%20observations.%20Research%20on%20Goal%20Recognition%20as%0APlanning%20encompasses%20reasoning%20about%20the%20model%20of%20a%20planning%20task%2C%20the%0Aobservations%2C%20and%20the%20goals%20using%20planning%20techniques%2C%20resulting%20in%20very%0Aefficient%20recognition%20approaches.%20In%20this%20article%2C%20we%20design%20novel%20recognition%0Aapproaches%20that%20rely%20on%20the%20Operator-Counting%20framework%2C%20proposing%20new%0Aconstraints%2C%20and%20analyze%20their%20constraints%27%20properties%20both%20theoretically%20and%0Aempirically.%20The%20Operator-Counting%20framework%20is%20a%20technique%20that%20efficiently%0Acomputes%20heuristic%20estimates%20of%20cost-to-goal%20using%20Integer/Linear%20Programming%0A%28IP/LP%29.%20In%20the%20realm%20of%20theory%2C%20we%20prove%20that%20the%20new%20constraints%20provide%0Alower%20bounds%20on%20the%20cost%20of%20plans%20that%20comply%20with%20observations.%20We%20also%0Aprovide%20an%20extensive%20empirical%20evaluation%20to%20assess%20how%20the%20new%20constraints%0Aimprove%20the%20quality%20of%20the%20solution%2C%20and%20we%20found%20that%20they%20are%20especially%0Ainformed%20in%20deciding%20which%20goals%20are%20unlikely%20to%20be%20part%20of%20the%20solution.%20Our%0Anovel%20recognition%20approaches%20have%20two%20pivotal%20advantages%3A%20first%2C%20they%20employ%0Anew%20IP/LP%20constraints%20for%20efficiently%20recognizing%20goals%3B%20second%2C%20we%20show%20how%0Athe%20new%20IP/LP%20constraints%20can%20improve%20the%20recognition%20of%20goals%20under%20both%0Apartial%20and%20noisy%20observability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07934v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal%20Recognition%20via%20Linear%20Programming&entry.906535625=Felipe%20Meneguzzi%20and%20Lu%C3%ADsa%20R.%20de%20A.%20Santos%20and%20Ramon%20Fraga%20Pereira%20and%20Andr%C3%A9%20G.%20Pereira&entry.1292438233=%20%20Goal%20Recognition%20is%20the%20task%20by%20which%20an%20observer%20aims%20to%20discern%20the%20goals%0Athat%20correspond%20to%20plans%20that%20comply%20with%20the%20perceived%20behavior%20of%20subject%0Aagents%20given%20as%20a%20sequence%20of%20observations.%20Research%20on%20Goal%20Recognition%20as%0APlanning%20encompasses%20reasoning%20about%20the%20model%20of%20a%20planning%20task%2C%20the%0Aobservations%2C%20and%20the%20goals%20using%20planning%20techniques%2C%20resulting%20in%20very%0Aefficient%20recognition%20approaches.%20In%20this%20article%2C%20we%20design%20novel%20recognition%0Aapproaches%20that%20rely%20on%20the%20Operator-Counting%20framework%2C%20proposing%20new%0Aconstraints%2C%20and%20analyze%20their%20constraints%27%20properties%20both%20theoretically%20and%0Aempirically.%20The%20Operator-Counting%20framework%20is%20a%20technique%20that%20efficiently%0Acomputes%20heuristic%20estimates%20of%20cost-to-goal%20using%20Integer/Linear%20Programming%0A%28IP/LP%29.%20In%20the%20realm%20of%20theory%2C%20we%20prove%20that%20the%20new%20constraints%20provide%0Alower%20bounds%20on%20the%20cost%20of%20plans%20that%20comply%20with%20observations.%20We%20also%0Aprovide%20an%20extensive%20empirical%20evaluation%20to%20assess%20how%20the%20new%20constraints%0Aimprove%20the%20quality%20of%20the%20solution%2C%20and%20we%20found%20that%20they%20are%20especially%0Ainformed%20in%20deciding%20which%20goals%20are%20unlikely%20to%20be%20part%20of%20the%20solution.%20Our%0Anovel%20recognition%20approaches%20have%20two%20pivotal%20advantages%3A%20first%2C%20they%20employ%0Anew%20IP/LP%20constraints%20for%20efficiently%20recognizing%20goals%3B%20second%2C%20we%20show%20how%0Athe%20new%20IP/LP%20constraints%20can%20improve%20the%20recognition%20of%20goals%20under%20both%0Apartial%20and%20noisy%20observability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07934v1&entry.124074799=Read"},
{"title": "Automatic nonlinear MPC approximation with closed-loop guarantees", "author": "Abdullah Tokmak and Christian Fiedler and Melanie N. Zeilinger and Sebastian Trimpe and Johannes K\u00f6hler", "abstract": "  Safety guarantees are vital in many control applications, such as robotics.\nModel predictive control (MPC) provides a constructive framework for\ncontrolling safety-critical systems, but is limited by its computational\ncomplexity. We address this problem by presenting a novel algorithm that\nautomatically computes an explicit approximation to nonlinear MPC schemes while\nretaining closed-loop guarantees. Specifically, the problem can be reduced to a\nfunction approximation problem, which we then tackle by proposing ALKIA-X, the\nAdaptive and Localized Kernel Interpolation Algorithm with eXtrapolated\nreproducing kernel Hilbert space norm. ALKIA-X is a non-iterative algorithm\nthat ensures numerically well-conditioned computations, a fast-to-evaluate\napproximating function, and the guaranteed satisfaction of any desired bound on\nthe approximation error. Hence, ALKIA-X automatically computes an explicit\nfunction that approximates the MPC, yielding a controller suitable for\nsafety-critical systems and high sampling rates. We apply ALKIA-X to\napproximate two nonlinear MPC schemes, demonstrating reduced computational\ndemand and applicability to realistic problems.\n", "link": "http://arxiv.org/abs/2312.10199v2", "date": "2024-04-11", "relevancy": 1.9215, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.502}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4768}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4753}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20nonlinear%20MPC%20approximation%20with%20closed-loop%20guarantees&body=Title%3A%20Automatic%20nonlinear%20MPC%20approximation%20with%20closed-loop%20guarantees%0AAuthor%3A%20Abdullah%20Tokmak%20and%20Christian%20Fiedler%20and%20Melanie%20N.%20Zeilinger%20and%20Sebastian%20Trimpe%20and%20Johannes%20K%C3%B6hler%0AAbstract%3A%20%20%20Safety%20guarantees%20are%20vital%20in%20many%20control%20applications%2C%20such%20as%20robotics.%0AModel%20predictive%20control%20%28MPC%29%20provides%20a%20constructive%20framework%20for%0Acontrolling%20safety-critical%20systems%2C%20but%20is%20limited%20by%20its%20computational%0Acomplexity.%20We%20address%20this%20problem%20by%20presenting%20a%20novel%20algorithm%20that%0Aautomatically%20computes%20an%20explicit%20approximation%20to%20nonlinear%20MPC%20schemes%20while%0Aretaining%20closed-loop%20guarantees.%20Specifically%2C%20the%20problem%20can%20be%20reduced%20to%20a%0Afunction%20approximation%20problem%2C%20which%20we%20then%20tackle%20by%20proposing%20ALKIA-X%2C%20the%0AAdaptive%20and%20Localized%20Kernel%20Interpolation%20Algorithm%20with%20eXtrapolated%0Areproducing%20kernel%20Hilbert%20space%20norm.%20ALKIA-X%20is%20a%20non-iterative%20algorithm%0Athat%20ensures%20numerically%20well-conditioned%20computations%2C%20a%20fast-to-evaluate%0Aapproximating%20function%2C%20and%20the%20guaranteed%20satisfaction%20of%20any%20desired%20bound%20on%0Athe%20approximation%20error.%20Hence%2C%20ALKIA-X%20automatically%20computes%20an%20explicit%0Afunction%20that%20approximates%20the%20MPC%2C%20yielding%20a%20controller%20suitable%20for%0Asafety-critical%20systems%20and%20high%20sampling%20rates.%20We%20apply%20ALKIA-X%20to%0Aapproximate%20two%20nonlinear%20MPC%20schemes%2C%20demonstrating%20reduced%20computational%0Ademand%20and%20applicability%20to%20realistic%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10199v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20nonlinear%20MPC%20approximation%20with%20closed-loop%20guarantees&entry.906535625=Abdullah%20Tokmak%20and%20Christian%20Fiedler%20and%20Melanie%20N.%20Zeilinger%20and%20Sebastian%20Trimpe%20and%20Johannes%20K%C3%B6hler&entry.1292438233=%20%20Safety%20guarantees%20are%20vital%20in%20many%20control%20applications%2C%20such%20as%20robotics.%0AModel%20predictive%20control%20%28MPC%29%20provides%20a%20constructive%20framework%20for%0Acontrolling%20safety-critical%20systems%2C%20but%20is%20limited%20by%20its%20computational%0Acomplexity.%20We%20address%20this%20problem%20by%20presenting%20a%20novel%20algorithm%20that%0Aautomatically%20computes%20an%20explicit%20approximation%20to%20nonlinear%20MPC%20schemes%20while%0Aretaining%20closed-loop%20guarantees.%20Specifically%2C%20the%20problem%20can%20be%20reduced%20to%20a%0Afunction%20approximation%20problem%2C%20which%20we%20then%20tackle%20by%20proposing%20ALKIA-X%2C%20the%0AAdaptive%20and%20Localized%20Kernel%20Interpolation%20Algorithm%20with%20eXtrapolated%0Areproducing%20kernel%20Hilbert%20space%20norm.%20ALKIA-X%20is%20a%20non-iterative%20algorithm%0Athat%20ensures%20numerically%20well-conditioned%20computations%2C%20a%20fast-to-evaluate%0Aapproximating%20function%2C%20and%20the%20guaranteed%20satisfaction%20of%20any%20desired%20bound%20on%0Athe%20approximation%20error.%20Hence%2C%20ALKIA-X%20automatically%20computes%20an%20explicit%0Afunction%20that%20approximates%20the%20MPC%2C%20yielding%20a%20controller%20suitable%20for%0Asafety-critical%20systems%20and%20high%20sampling%20rates.%20We%20apply%20ALKIA-X%20to%0Aapproximate%20two%20nonlinear%20MPC%20schemes%2C%20demonstrating%20reduced%20computational%0Ademand%20and%20applicability%20to%20realistic%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10199v2&entry.124074799=Read"},
{"title": "Calibration of Continual Learning Models", "author": "Lanpei Li and Elia Piccoli and Andrea Cossu and Davide Bacciu and Vincenzo Lomonaco", "abstract": "  Continual Learning (CL) focuses on maximizing the predictive performance of a\nmodel across a non-stationary stream of data. Unfortunately, CL models tend to\nforget previous knowledge, thus often underperforming when compared with an\noffline model trained jointly on the entire data stream. Given that any CL\nmodel will eventually make mistakes, it is of crucial importance to build\ncalibrated CL models: models that can reliably tell their confidence when\nmaking a prediction. Model calibration is an active research topic in machine\nlearning, yet to be properly investigated in CL. We provide the first empirical\nstudy of the behavior of calibration approaches in CL, showing that CL\nstrategies do not inherently learn calibrated models. To mitigate this issue,\nwe design a continual calibration approach that improves the performance of\npost-processing calibration methods over a wide range of different benchmarks\nand CL strategies. CL does not necessarily need perfect predictive models, but\nrather it can benefit from reliable predictive models. We believe our study on\ncontinual calibration represents a first step towards this direction.\n", "link": "http://arxiv.org/abs/2404.07817v1", "date": "2024-04-11", "relevancy": 1.9101, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4754}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4687}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Calibration%20of%20Continual%20Learning%20Models&body=Title%3A%20Calibration%20of%20Continual%20Learning%20Models%0AAuthor%3A%20Lanpei%20Li%20and%20Elia%20Piccoli%20and%20Andrea%20Cossu%20and%20Davide%20Bacciu%20and%20Vincenzo%20Lomonaco%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20focuses%20on%20maximizing%20the%20predictive%20performance%20of%20a%0Amodel%20across%20a%20non-stationary%20stream%20of%20data.%20Unfortunately%2C%20CL%20models%20tend%20to%0Aforget%20previous%20knowledge%2C%20thus%20often%20underperforming%20when%20compared%20with%20an%0Aoffline%20model%20trained%20jointly%20on%20the%20entire%20data%20stream.%20Given%20that%20any%20CL%0Amodel%20will%20eventually%20make%20mistakes%2C%20it%20is%20of%20crucial%20importance%20to%20build%0Acalibrated%20CL%20models%3A%20models%20that%20can%20reliably%20tell%20their%20confidence%20when%0Amaking%20a%20prediction.%20Model%20calibration%20is%20an%20active%20research%20topic%20in%20machine%0Alearning%2C%20yet%20to%20be%20properly%20investigated%20in%20CL.%20We%20provide%20the%20first%20empirical%0Astudy%20of%20the%20behavior%20of%20calibration%20approaches%20in%20CL%2C%20showing%20that%20CL%0Astrategies%20do%20not%20inherently%20learn%20calibrated%20models.%20To%20mitigate%20this%20issue%2C%0Awe%20design%20a%20continual%20calibration%20approach%20that%20improves%20the%20performance%20of%0Apost-processing%20calibration%20methods%20over%20a%20wide%20range%20of%20different%20benchmarks%0Aand%20CL%20strategies.%20CL%20does%20not%20necessarily%20need%20perfect%20predictive%20models%2C%20but%0Arather%20it%20can%20benefit%20from%20reliable%20predictive%20models.%20We%20believe%20our%20study%20on%0Acontinual%20calibration%20represents%20a%20first%20step%20towards%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07817v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibration%20of%20Continual%20Learning%20Models&entry.906535625=Lanpei%20Li%20and%20Elia%20Piccoli%20and%20Andrea%20Cossu%20and%20Davide%20Bacciu%20and%20Vincenzo%20Lomonaco&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20focuses%20on%20maximizing%20the%20predictive%20performance%20of%20a%0Amodel%20across%20a%20non-stationary%20stream%20of%20data.%20Unfortunately%2C%20CL%20models%20tend%20to%0Aforget%20previous%20knowledge%2C%20thus%20often%20underperforming%20when%20compared%20with%20an%0Aoffline%20model%20trained%20jointly%20on%20the%20entire%20data%20stream.%20Given%20that%20any%20CL%0Amodel%20will%20eventually%20make%20mistakes%2C%20it%20is%20of%20crucial%20importance%20to%20build%0Acalibrated%20CL%20models%3A%20models%20that%20can%20reliably%20tell%20their%20confidence%20when%0Amaking%20a%20prediction.%20Model%20calibration%20is%20an%20active%20research%20topic%20in%20machine%0Alearning%2C%20yet%20to%20be%20properly%20investigated%20in%20CL.%20We%20provide%20the%20first%20empirical%0Astudy%20of%20the%20behavior%20of%20calibration%20approaches%20in%20CL%2C%20showing%20that%20CL%0Astrategies%20do%20not%20inherently%20learn%20calibrated%20models.%20To%20mitigate%20this%20issue%2C%0Awe%20design%20a%20continual%20calibration%20approach%20that%20improves%20the%20performance%20of%0Apost-processing%20calibration%20methods%20over%20a%20wide%20range%20of%20different%20benchmarks%0Aand%20CL%20strategies.%20CL%20does%20not%20necessarily%20need%20perfect%20predictive%20models%2C%20but%0Arather%20it%20can%20benefit%20from%20reliable%20predictive%20models.%20We%20believe%20our%20study%20on%0Acontinual%20calibration%20represents%20a%20first%20step%20towards%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07817v1&entry.124074799=Read"},
{"title": "Inferring Change Points in High-Dimensional Linear Regression via\n  Approximate Message Passing", "author": "Gabriel Arpino and Xiaoqi Liu and Ramji Venkataramanan", "abstract": "  We consider the problem of localizing change points in high-dimensional\nlinear regression. We propose an Approximate Message Passing (AMP) algorithm\nfor estimating both the signals and the change point locations. Assuming\nGaussian covariates, we give an exact asymptotic characterization of its\nestimation performance in the limit where the number of samples grows\nproportionally to the signal dimension. Our algorithm can be tailored to\nexploit any prior information on the signal, noise, and change points. It also\nenables uncertainty quantification in the form of an efficiently computable\napproximate posterior distribution, whose asymptotic form we characterize\nexactly. We validate our theory via numerical experiments, and demonstrate the\nfavorable performance of our estimators on both synthetic data and images.\n", "link": "http://arxiv.org/abs/2404.07864v1", "date": "2024-04-11", "relevancy": 1.8958, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.482}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.474}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4658}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Inferring%20Change%20Points%20in%20High-Dimensional%20Linear%20Regression%20via%0A%20%20Approximate%20Message%20Passing&body=Title%3A%20Inferring%20Change%20Points%20in%20High-Dimensional%20Linear%20Regression%20via%0A%20%20Approximate%20Message%20Passing%0AAuthor%3A%20Gabriel%20Arpino%20and%20Xiaoqi%20Liu%20and%20Ramji%20Venkataramanan%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20localizing%20change%20points%20in%20high-dimensional%0Alinear%20regression.%20We%20propose%20an%20Approximate%20Message%20Passing%20%28AMP%29%20algorithm%0Afor%20estimating%20both%20the%20signals%20and%20the%20change%20point%20locations.%20Assuming%0AGaussian%20covariates%2C%20we%20give%20an%20exact%20asymptotic%20characterization%20of%20its%0Aestimation%20performance%20in%20the%20limit%20where%20the%20number%20of%20samples%20grows%0Aproportionally%20to%20the%20signal%20dimension.%20Our%20algorithm%20can%20be%20tailored%20to%0Aexploit%20any%20prior%20information%20on%20the%20signal%2C%20noise%2C%20and%20change%20points.%20It%20also%0Aenables%20uncertainty%20quantification%20in%20the%20form%20of%20an%20efficiently%20computable%0Aapproximate%20posterior%20distribution%2C%20whose%20asymptotic%20form%20we%20characterize%0Aexactly.%20We%20validate%20our%20theory%20via%20numerical%20experiments%2C%20and%20demonstrate%20the%0Afavorable%20performance%20of%20our%20estimators%20on%20both%20synthetic%20data%20and%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07864v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inferring%20Change%20Points%20in%20High-Dimensional%20Linear%20Regression%20via%0A%20%20Approximate%20Message%20Passing&entry.906535625=Gabriel%20Arpino%20and%20Xiaoqi%20Liu%20and%20Ramji%20Venkataramanan&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20localizing%20change%20points%20in%20high-dimensional%0Alinear%20regression.%20We%20propose%20an%20Approximate%20Message%20Passing%20%28AMP%29%20algorithm%0Afor%20estimating%20both%20the%20signals%20and%20the%20change%20point%20locations.%20Assuming%0AGaussian%20covariates%2C%20we%20give%20an%20exact%20asymptotic%20characterization%20of%20its%0Aestimation%20performance%20in%20the%20limit%20where%20the%20number%20of%20samples%20grows%0Aproportionally%20to%20the%20signal%20dimension.%20Our%20algorithm%20can%20be%20tailored%20to%0Aexploit%20any%20prior%20information%20on%20the%20signal%2C%20noise%2C%20and%20change%20points.%20It%20also%0Aenables%20uncertainty%20quantification%20in%20the%20form%20of%20an%20efficiently%20computable%0Aapproximate%20posterior%20distribution%2C%20whose%20asymptotic%20form%20we%20characterize%0Aexactly.%20We%20validate%20our%20theory%20via%20numerical%20experiments%2C%20and%20demonstrate%20the%0Afavorable%20performance%20of%20our%20estimators%20on%20both%20synthetic%20data%20and%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07864v1&entry.124074799=Read"},
{"title": "Deep Learning for Satellite Image Time Series Analysis: A Review", "author": "Lynn Miller and Charlotte Pelletier and Geoffrey I. Webb", "abstract": "  Earth observation (EO) satellite missions have been providing detailed images\nabout the state of the Earth and its land cover for over 50 years. Long term\nmissions, such as NASA's Landsat, Terra, and Aqua satellites, and more\nrecently, the ESA's Sentinel missions, record images of the entire world every\nfew days. Although single images provide point-in-time data, repeated images of\nthe same area, or satellite image time series (SITS) provide information about\nthe changing state of vegetation and land use. These SITS are useful for\nmodeling dynamic processes and seasonal changes such as plant phenology. They\nhave potential benefits for many aspects of land and natural resource\nmanagement, including applications in agricultural, forest, water, and disaster\nmanagement, urban planning, and mining. However, the resulting satellite image\ntime series (SITS) are complex, incorporating information from the temporal,\nspatial, and spectral dimensions. Therefore, deep learning methods are often\ndeployed as they can analyze these complex relationships. This review presents\na summary of the state-of-the-art methods of modelling environmental,\nagricultural, and other Earth observation variables from SITS data using deep\nlearning methods. We aim to provide a resource for remote sensing experts\ninterested in using deep learning techniques to enhance Earth observation\nmodels with temporal information.\n", "link": "http://arxiv.org/abs/2404.03936v2", "date": "2024-04-11", "relevancy": 1.8918, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4936}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4868}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4467}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Satellite%20Image%20Time%20Series%20Analysis%3A%20A%20Review&body=Title%3A%20Deep%20Learning%20for%20Satellite%20Image%20Time%20Series%20Analysis%3A%20A%20Review%0AAuthor%3A%20Lynn%20Miller%20and%20Charlotte%20Pelletier%20and%20Geoffrey%20I.%20Webb%0AAbstract%3A%20%20%20Earth%20observation%20%28EO%29%20satellite%20missions%20have%20been%20providing%20detailed%20images%0Aabout%20the%20state%20of%20the%20Earth%20and%20its%20land%20cover%20for%20over%2050%20years.%20Long%20term%0Amissions%2C%20such%20as%20NASA%27s%20Landsat%2C%20Terra%2C%20and%20Aqua%20satellites%2C%20and%20more%0Arecently%2C%20the%20ESA%27s%20Sentinel%20missions%2C%20record%20images%20of%20the%20entire%20world%20every%0Afew%20days.%20Although%20single%20images%20provide%20point-in-time%20data%2C%20repeated%20images%20of%0Athe%20same%20area%2C%20or%20satellite%20image%20time%20series%20%28SITS%29%20provide%20information%20about%0Athe%20changing%20state%20of%20vegetation%20and%20land%20use.%20These%20SITS%20are%20useful%20for%0Amodeling%20dynamic%20processes%20and%20seasonal%20changes%20such%20as%20plant%20phenology.%20They%0Ahave%20potential%20benefits%20for%20many%20aspects%20of%20land%20and%20natural%20resource%0Amanagement%2C%20including%20applications%20in%20agricultural%2C%20forest%2C%20water%2C%20and%20disaster%0Amanagement%2C%20urban%20planning%2C%20and%20mining.%20However%2C%20the%20resulting%20satellite%20image%0Atime%20series%20%28SITS%29%20are%20complex%2C%20incorporating%20information%20from%20the%20temporal%2C%0Aspatial%2C%20and%20spectral%20dimensions.%20Therefore%2C%20deep%20learning%20methods%20are%20often%0Adeployed%20as%20they%20can%20analyze%20these%20complex%20relationships.%20This%20review%20presents%0Aa%20summary%20of%20the%20state-of-the-art%20methods%20of%20modelling%20environmental%2C%0Aagricultural%2C%20and%20other%20Earth%20observation%20variables%20from%20SITS%20data%20using%20deep%0Alearning%20methods.%20We%20aim%20to%20provide%20a%20resource%20for%20remote%20sensing%20experts%0Ainterested%20in%20using%20deep%20learning%20techniques%20to%20enhance%20Earth%20observation%0Amodels%20with%20temporal%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03936v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Satellite%20Image%20Time%20Series%20Analysis%3A%20A%20Review&entry.906535625=Lynn%20Miller%20and%20Charlotte%20Pelletier%20and%20Geoffrey%20I.%20Webb&entry.1292438233=%20%20Earth%20observation%20%28EO%29%20satellite%20missions%20have%20been%20providing%20detailed%20images%0Aabout%20the%20state%20of%20the%20Earth%20and%20its%20land%20cover%20for%20over%2050%20years.%20Long%20term%0Amissions%2C%20such%20as%20NASA%27s%20Landsat%2C%20Terra%2C%20and%20Aqua%20satellites%2C%20and%20more%0Arecently%2C%20the%20ESA%27s%20Sentinel%20missions%2C%20record%20images%20of%20the%20entire%20world%20every%0Afew%20days.%20Although%20single%20images%20provide%20point-in-time%20data%2C%20repeated%20images%20of%0Athe%20same%20area%2C%20or%20satellite%20image%20time%20series%20%28SITS%29%20provide%20information%20about%0Athe%20changing%20state%20of%20vegetation%20and%20land%20use.%20These%20SITS%20are%20useful%20for%0Amodeling%20dynamic%20processes%20and%20seasonal%20changes%20such%20as%20plant%20phenology.%20They%0Ahave%20potential%20benefits%20for%20many%20aspects%20of%20land%20and%20natural%20resource%0Amanagement%2C%20including%20applications%20in%20agricultural%2C%20forest%2C%20water%2C%20and%20disaster%0Amanagement%2C%20urban%20planning%2C%20and%20mining.%20However%2C%20the%20resulting%20satellite%20image%0Atime%20series%20%28SITS%29%20are%20complex%2C%20incorporating%20information%20from%20the%20temporal%2C%0Aspatial%2C%20and%20spectral%20dimensions.%20Therefore%2C%20deep%20learning%20methods%20are%20often%0Adeployed%20as%20they%20can%20analyze%20these%20complex%20relationships.%20This%20review%20presents%0Aa%20summary%20of%20the%20state-of-the-art%20methods%20of%20modelling%20environmental%2C%0Aagricultural%2C%20and%20other%20Earth%20observation%20variables%20from%20SITS%20data%20using%20deep%0Alearning%20methods.%20We%20aim%20to%20provide%20a%20resource%20for%20remote%20sensing%20experts%0Ainterested%20in%20using%20deep%20learning%20techniques%20to%20enhance%20Earth%20observation%0Amodels%20with%20temporal%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03936v2&entry.124074799=Read"},
{"title": "Dual Quaternion Control of UAVs with Cable-suspended Load", "author": "Yuxia Yuan and Markus Ryll", "abstract": "  Modeling the kinematics and dynamics of robotics systems with suspended loads\nusing dual quaternions has not been explored so far. This paper introduces a\nnew innovative control strategy using dual quaternions for UAVs with\ncable-suspended loads, focusing on the sling load lifting and tracking\nproblems. By utilizing the mathematical efficiency and compactness of dual\nquaternions, a unified representation of the UAV and its suspended load's\ndynamics and kinematics is achieved, facilitating the realization of load\nlifting and trajectory tracking. The simulation results have tested the\nproposed strategy's accuracy, efficiency, and robustness. This study makes a\nsubstantial contribution to present this novel control strategy that harnesses\nthe benefits of dual quaternions for cargo UAVs. Our work also holds promise\nfor inspiring future innovations in under-actuated systems control using dual\nquaternions.\n", "link": "http://arxiv.org/abs/2404.07635v1", "date": "2024-04-11", "relevancy": 1.8907, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4905}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.463}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4587}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual%20Quaternion%20Control%20of%20UAVs%20with%20Cable-suspended%20Load&body=Title%3A%20Dual%20Quaternion%20Control%20of%20UAVs%20with%20Cable-suspended%20Load%0AAuthor%3A%20Yuxia%20Yuan%20and%20Markus%20Ryll%0AAbstract%3A%20%20%20Modeling%20the%20kinematics%20and%20dynamics%20of%20robotics%20systems%20with%20suspended%20loads%0Ausing%20dual%20quaternions%20has%20not%20been%20explored%20so%20far.%20This%20paper%20introduces%20a%0Anew%20innovative%20control%20strategy%20using%20dual%20quaternions%20for%20UAVs%20with%0Acable-suspended%20loads%2C%20focusing%20on%20the%20sling%20load%20lifting%20and%20tracking%0Aproblems.%20By%20utilizing%20the%20mathematical%20efficiency%20and%20compactness%20of%20dual%0Aquaternions%2C%20a%20unified%20representation%20of%20the%20UAV%20and%20its%20suspended%20load%27s%0Adynamics%20and%20kinematics%20is%20achieved%2C%20facilitating%20the%20realization%20of%20load%0Alifting%20and%20trajectory%20tracking.%20The%20simulation%20results%20have%20tested%20the%0Aproposed%20strategy%27s%20accuracy%2C%20efficiency%2C%20and%20robustness.%20This%20study%20makes%20a%0Asubstantial%20contribution%20to%20present%20this%20novel%20control%20strategy%20that%20harnesses%0Athe%20benefits%20of%20dual%20quaternions%20for%20cargo%20UAVs.%20Our%20work%20also%20holds%20promise%0Afor%20inspiring%20future%20innovations%20in%20under-actuated%20systems%20control%20using%20dual%0Aquaternions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07635v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Quaternion%20Control%20of%20UAVs%20with%20Cable-suspended%20Load&entry.906535625=Yuxia%20Yuan%20and%20Markus%20Ryll&entry.1292438233=%20%20Modeling%20the%20kinematics%20and%20dynamics%20of%20robotics%20systems%20with%20suspended%20loads%0Ausing%20dual%20quaternions%20has%20not%20been%20explored%20so%20far.%20This%20paper%20introduces%20a%0Anew%20innovative%20control%20strategy%20using%20dual%20quaternions%20for%20UAVs%20with%0Acable-suspended%20loads%2C%20focusing%20on%20the%20sling%20load%20lifting%20and%20tracking%0Aproblems.%20By%20utilizing%20the%20mathematical%20efficiency%20and%20compactness%20of%20dual%0Aquaternions%2C%20a%20unified%20representation%20of%20the%20UAV%20and%20its%20suspended%20load%27s%0Adynamics%20and%20kinematics%20is%20achieved%2C%20facilitating%20the%20realization%20of%20load%0Alifting%20and%20trajectory%20tracking.%20The%20simulation%20results%20have%20tested%20the%0Aproposed%20strategy%27s%20accuracy%2C%20efficiency%2C%20and%20robustness.%20This%20study%20makes%20a%0Asubstantial%20contribution%20to%20present%20this%20novel%20control%20strategy%20that%20harnesses%0Athe%20benefits%20of%20dual%20quaternions%20for%20cargo%20UAVs.%20Our%20work%20also%20holds%20promise%0Afor%20inspiring%20future%20innovations%20in%20under-actuated%20systems%20control%20using%20dual%0Aquaternions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07635v1&entry.124074799=Read"},
{"title": "Diffusion Time-step Curriculum for One Image to 3D Generation", "author": "Xuanyu Yi and Zike Wu and Qingshan Xu and Pan Zhou and Joo-Hwee Lim and Hanwang Zhang", "abstract": "  Score distillation sampling~(SDS) has been widely adopted to overcome the\nabsence of unseen views in reconstructing 3D objects from a \\textbf{single}\nimage. It leverages pre-trained 2D diffusion models as teacher to guide the\nreconstruction of student 3D models. Despite their remarkable success,\nSDS-based methods often encounter geometric artifacts and texture saturation.\nWe find out the crux is the overlooked indiscriminate treatment of diffusion\ntime-steps during optimization: it unreasonably treats the student-teacher\nknowledge distillation to be equal at all time-steps and thus entangles\ncoarse-grained and fine-grained modeling. Therefore, we propose the Diffusion\nTime-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the\nteacher and student models collaborating with the time-step curriculum in a\ncoarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO and\nLevel50 benchmark demonstrate that DTC123 can produce multi-view consistent,\nhigh-quality, and diverse 3D assets. Codes and more generation demos will be\nreleased in https://github.com/yxymessi/DTC123.\n", "link": "http://arxiv.org/abs/2404.04562v2", "date": "2024-04-11", "relevancy": 1.88, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6843}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6104}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6097}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Time-step%20Curriculum%20for%20One%20Image%20to%203D%20Generation&body=Title%3A%20Diffusion%20Time-step%20Curriculum%20for%20One%20Image%20to%203D%20Generation%0AAuthor%3A%20Xuanyu%20Yi%20and%20Zike%20Wu%20and%20Qingshan%20Xu%20and%20Pan%20Zhou%20and%20Joo-Hwee%20Lim%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20Score%20distillation%20sampling~%28SDS%29%20has%20been%20widely%20adopted%20to%20overcome%20the%0Aabsence%20of%20unseen%20views%20in%20reconstructing%203D%20objects%20from%20a%20%5Ctextbf%7Bsingle%7D%0Aimage.%20It%20leverages%20pre-trained%202D%20diffusion%20models%20as%20teacher%20to%20guide%20the%0Areconstruction%20of%20student%203D%20models.%20Despite%20their%20remarkable%20success%2C%0ASDS-based%20methods%20often%20encounter%20geometric%20artifacts%20and%20texture%20saturation.%0AWe%20find%20out%20the%20crux%20is%20the%20overlooked%20indiscriminate%20treatment%20of%20diffusion%0Atime-steps%20during%20optimization%3A%20it%20unreasonably%20treats%20the%20student-teacher%0Aknowledge%20distillation%20to%20be%20equal%20at%20all%20time-steps%20and%20thus%20entangles%0Acoarse-grained%20and%20fine-grained%20modeling.%20Therefore%2C%20we%20propose%20the%20Diffusion%0ATime-step%20Curriculum%20one-image-to-3D%20pipeline%20%28DTC123%29%2C%20which%20involves%20both%20the%0Ateacher%20and%20student%20models%20collaborating%20with%20the%20time-step%20curriculum%20in%20a%0Acoarse-to-fine%20manner.%20Extensive%20experiments%20on%20NeRF4%2C%20RealFusion15%2C%20GSO%20and%0ALevel50%20benchmark%20demonstrate%20that%20DTC123%20can%20produce%20multi-view%20consistent%2C%0Ahigh-quality%2C%20and%20diverse%203D%20assets.%20Codes%20and%20more%20generation%20demos%20will%20be%0Areleased%20in%20https%3A//github.com/yxymessi/DTC123.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04562v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Time-step%20Curriculum%20for%20One%20Image%20to%203D%20Generation&entry.906535625=Xuanyu%20Yi%20and%20Zike%20Wu%20and%20Qingshan%20Xu%20and%20Pan%20Zhou%20and%20Joo-Hwee%20Lim%20and%20Hanwang%20Zhang&entry.1292438233=%20%20Score%20distillation%20sampling~%28SDS%29%20has%20been%20widely%20adopted%20to%20overcome%20the%0Aabsence%20of%20unseen%20views%20in%20reconstructing%203D%20objects%20from%20a%20%5Ctextbf%7Bsingle%7D%0Aimage.%20It%20leverages%20pre-trained%202D%20diffusion%20models%20as%20teacher%20to%20guide%20the%0Areconstruction%20of%20student%203D%20models.%20Despite%20their%20remarkable%20success%2C%0ASDS-based%20methods%20often%20encounter%20geometric%20artifacts%20and%20texture%20saturation.%0AWe%20find%20out%20the%20crux%20is%20the%20overlooked%20indiscriminate%20treatment%20of%20diffusion%0Atime-steps%20during%20optimization%3A%20it%20unreasonably%20treats%20the%20student-teacher%0Aknowledge%20distillation%20to%20be%20equal%20at%20all%20time-steps%20and%20thus%20entangles%0Acoarse-grained%20and%20fine-grained%20modeling.%20Therefore%2C%20we%20propose%20the%20Diffusion%0ATime-step%20Curriculum%20one-image-to-3D%20pipeline%20%28DTC123%29%2C%20which%20involves%20both%20the%0Ateacher%20and%20student%20models%20collaborating%20with%20the%20time-step%20curriculum%20in%20a%0Acoarse-to-fine%20manner.%20Extensive%20experiments%20on%20NeRF4%2C%20RealFusion15%2C%20GSO%20and%0ALevel50%20benchmark%20demonstrate%20that%20DTC123%20can%20produce%20multi-view%20consistent%2C%0Ahigh-quality%2C%20and%20diverse%203D%20assets.%20Codes%20and%20more%20generation%20demos%20will%20be%0Areleased%20in%20https%3A//github.com/yxymessi/DTC123.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04562v2&entry.124074799=Read"},
{"title": "A Multi-Label Dataset of French Fake News: Human and Machine Insights", "author": "Benjamin Icard and Fran\u00e7ois Maine and Morgane Casanova and G\u00e9raud Faye and Julien Chanson and Guillaume Gadek and Ghislain Atemezing and Fran\u00e7ois Bancilhon and Paul \u00c9gr\u00e9", "abstract": "  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of\nFrench press considered unreliable by expert agencies, annotated using 11\nlabels by 8 annotators. By collecting more labels than usual, by more\nannotators than is typically done, we can identify features that humans\nconsider as characteristic of fake news, and compare them to the predictions of\nautomated classifiers. We present a topic and genre analysis using Gate Cloud,\nindicative of the prevalence of satire-like text in the corpus. We then use the\nsubjectivity analyzer VAGO, and a neural version of it, to clarify the link\nbetween ascriptions of the label Subjective and ascriptions of the label Fake\nNews. The annotated dataset is available online at the following url:\nhttps://github.com/obs-info/obsinfox\n  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,\nExaggeration, French Press\n", "link": "http://arxiv.org/abs/2403.16099v2", "date": "2024-04-11", "relevancy": 1.3638, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4869}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4588}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.44}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Label%20Dataset%20of%20French%20Fake%20News%3A%20Human%20and%20Machine%20Insights&body=Title%3A%20A%20Multi-Label%20Dataset%20of%20French%20Fake%20News%3A%20Human%20and%20Machine%20Insights%0AAuthor%3A%20Benjamin%20Icard%20and%20Fran%C3%A7ois%20Maine%20and%20Morgane%20Casanova%20and%20G%C3%A9raud%20Faye%20and%20Julien%20Chanson%20and%20Guillaume%20Gadek%20and%20Ghislain%20Atemezing%20and%20Fran%C3%A7ois%20Bancilhon%20and%20Paul%20%C3%89gr%C3%A9%0AAbstract%3A%20%20%20We%20present%20a%20corpus%20of%20100%20documents%2C%20OBSINFOX%2C%20selected%20from%2017%20sources%20of%0AFrench%20press%20considered%20unreliable%20by%20expert%20agencies%2C%20annotated%20using%2011%0Alabels%20by%208%20annotators.%20By%20collecting%20more%20labels%20than%20usual%2C%20by%20more%0Aannotators%20than%20is%20typically%20done%2C%20we%20can%20identify%20features%20that%20humans%0Aconsider%20as%20characteristic%20of%20fake%20news%2C%20and%20compare%20them%20to%20the%20predictions%20of%0Aautomated%20classifiers.%20We%20present%20a%20topic%20and%20genre%20analysis%20using%20Gate%20Cloud%2C%0Aindicative%20of%20the%20prevalence%20of%20satire-like%20text%20in%20the%20corpus.%20We%20then%20use%20the%0Asubjectivity%20analyzer%20VAGO%2C%20and%20a%20neural%20version%20of%20it%2C%20to%20clarify%20the%20link%0Abetween%20ascriptions%20of%20the%20label%20Subjective%20and%20ascriptions%20of%20the%20label%20Fake%0ANews.%20The%20annotated%20dataset%20is%20available%20online%20at%20the%20following%20url%3A%0Ahttps%3A//github.com/obs-info/obsinfox%0A%20%20Keywords%3A%20Fake%20News%2C%20Multi-Labels%2C%20Subjectivity%2C%20Vagueness%2C%20Detail%2C%20Opinion%2C%0AExaggeration%2C%20French%20Press%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16099v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Label%20Dataset%20of%20French%20Fake%20News%3A%20Human%20and%20Machine%20Insights&entry.906535625=Benjamin%20Icard%20and%20Fran%C3%A7ois%20Maine%20and%20Morgane%20Casanova%20and%20G%C3%A9raud%20Faye%20and%20Julien%20Chanson%20and%20Guillaume%20Gadek%20and%20Ghislain%20Atemezing%20and%20Fran%C3%A7ois%20Bancilhon%20and%20Paul%20%C3%89gr%C3%A9&entry.1292438233=%20%20We%20present%20a%20corpus%20of%20100%20documents%2C%20OBSINFOX%2C%20selected%20from%2017%20sources%20of%0AFrench%20press%20considered%20unreliable%20by%20expert%20agencies%2C%20annotated%20using%2011%0Alabels%20by%208%20annotators.%20By%20collecting%20more%20labels%20than%20usual%2C%20by%20more%0Aannotators%20than%20is%20typically%20done%2C%20we%20can%20identify%20features%20that%20humans%0Aconsider%20as%20characteristic%20of%20fake%20news%2C%20and%20compare%20them%20to%20the%20predictions%20of%0Aautomated%20classifiers.%20We%20present%20a%20topic%20and%20genre%20analysis%20using%20Gate%20Cloud%2C%0Aindicative%20of%20the%20prevalence%20of%20satire-like%20text%20in%20the%20corpus.%20We%20then%20use%20the%0Asubjectivity%20analyzer%20VAGO%2C%20and%20a%20neural%20version%20of%20it%2C%20to%20clarify%20the%20link%0Abetween%20ascriptions%20of%20the%20label%20Subjective%20and%20ascriptions%20of%20the%20label%20Fake%0ANews.%20The%20annotated%20dataset%20is%20available%20online%20at%20the%20following%20url%3A%0Ahttps%3A//github.com/obs-info/obsinfox%0A%20%20Keywords%3A%20Fake%20News%2C%20Multi-Labels%2C%20Subjectivity%2C%20Vagueness%2C%20Detail%2C%20Opinion%2C%0AExaggeration%2C%20French%20Press%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16099v2&entry.124074799=Read"},
{"title": "Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in\n  Videos", "author": "Soumyabrata Chaudhuri and Saumik Bhattacharya", "abstract": "  Skeleton Action Recognition (SAR) involves identifying human actions using\nskeletal joint coordinates and their interconnections. While plain Transformers\nhave been attempted for this task, they still fall short compared to the\ncurrent leading methods, which are rooted in Graph Convolutional Networks\n(GCNs) due to the absence of structural priors. Recently, a novel selective\nstate space model, Mamba, has surfaced as a compelling alternative to the\nattention mechanism in Transformers, offering efficient modeling of long\nsequences. In this work, to the utmost extent of our awareness, we present the\nfirst SAR framework incorporating Mamba. Each fundamental block of our model\nadopts a novel U-ShiftGCN architecture with Mamba as its core component. The\nencoder segment of the U-ShiftGCN is devised to extract spatial features from\nthe skeletal data using downsampling vanilla Shift S-GCN blocks. These spatial\nfeatures then undergo intermediate temporal modeling facilitated by the Mamba\nblock before progressing to the encoder section, which comprises vanilla\nupsampling Shift S-GCN blocks. Additionally, a Shift T-GCN (ShiftTCN) temporal\nmodeling unit is employed before the exit of each fundamental block to refine\ntemporal representations. This particular integration of downsampling spatial,\nintermediate temporal, upsampling spatial, and ultimate temporal subunits\nyields promising results for skeleton action recognition. We dub the resulting\nmodel \\textbf{Simba}, which attains state-of-the-art performance across three\nwell-known benchmark skeleton action recognition datasets: NTU RGB+D, NTU RGB+D\n120, and Northwestern-UCLA. Interestingly, U-ShiftGCN (Simba without\nIntermediate Mamba Block) by itself is capable of performing reasonably well\nand surpasses our baseline.\n", "link": "http://arxiv.org/abs/2404.07645v1", "date": "2024-04-11", "relevancy": 1.639, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5531}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.538}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5376}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simba%3A%20Mamba%20augmented%20U-ShiftGCN%20for%20Skeletal%20Action%20Recognition%20in%0A%20%20Videos&body=Title%3A%20Simba%3A%20Mamba%20augmented%20U-ShiftGCN%20for%20Skeletal%20Action%20Recognition%20in%0A%20%20Videos%0AAuthor%3A%20Soumyabrata%20Chaudhuri%20and%20Saumik%20Bhattacharya%0AAbstract%3A%20%20%20Skeleton%20Action%20Recognition%20%28SAR%29%20involves%20identifying%20human%20actions%20using%0Askeletal%20joint%20coordinates%20and%20their%20interconnections.%20While%20plain%20Transformers%0Ahave%20been%20attempted%20for%20this%20task%2C%20they%20still%20fall%20short%20compared%20to%20the%0Acurrent%20leading%20methods%2C%20which%20are%20rooted%20in%20Graph%20Convolutional%20Networks%0A%28GCNs%29%20due%20to%20the%20absence%20of%20structural%20priors.%20Recently%2C%20a%20novel%20selective%0Astate%20space%20model%2C%20Mamba%2C%20has%20surfaced%20as%20a%20compelling%20alternative%20to%20the%0Aattention%20mechanism%20in%20Transformers%2C%20offering%20efficient%20modeling%20of%20long%0Asequences.%20In%20this%20work%2C%20to%20the%20utmost%20extent%20of%20our%20awareness%2C%20we%20present%20the%0Afirst%20SAR%20framework%20incorporating%20Mamba.%20Each%20fundamental%20block%20of%20our%20model%0Aadopts%20a%20novel%20U-ShiftGCN%20architecture%20with%20Mamba%20as%20its%20core%20component.%20The%0Aencoder%20segment%20of%20the%20U-ShiftGCN%20is%20devised%20to%20extract%20spatial%20features%20from%0Athe%20skeletal%20data%20using%20downsampling%20vanilla%20Shift%20S-GCN%20blocks.%20These%20spatial%0Afeatures%20then%20undergo%20intermediate%20temporal%20modeling%20facilitated%20by%20the%20Mamba%0Ablock%20before%20progressing%20to%20the%20encoder%20section%2C%20which%20comprises%20vanilla%0Aupsampling%20Shift%20S-GCN%20blocks.%20Additionally%2C%20a%20Shift%20T-GCN%20%28ShiftTCN%29%20temporal%0Amodeling%20unit%20is%20employed%20before%20the%20exit%20of%20each%20fundamental%20block%20to%20refine%0Atemporal%20representations.%20This%20particular%20integration%20of%20downsampling%20spatial%2C%0Aintermediate%20temporal%2C%20upsampling%20spatial%2C%20and%20ultimate%20temporal%20subunits%0Ayields%20promising%20results%20for%20skeleton%20action%20recognition.%20We%20dub%20the%20resulting%0Amodel%20%5Ctextbf%7BSimba%7D%2C%20which%20attains%20state-of-the-art%20performance%20across%20three%0Awell-known%20benchmark%20skeleton%20action%20recognition%20datasets%3A%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%0A120%2C%20and%20Northwestern-UCLA.%20Interestingly%2C%20U-ShiftGCN%20%28Simba%20without%0AIntermediate%20Mamba%20Block%29%20by%20itself%20is%20capable%20of%20performing%20reasonably%20well%0Aand%20surpasses%20our%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07645v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simba%3A%20Mamba%20augmented%20U-ShiftGCN%20for%20Skeletal%20Action%20Recognition%20in%0A%20%20Videos&entry.906535625=Soumyabrata%20Chaudhuri%20and%20Saumik%20Bhattacharya&entry.1292438233=%20%20Skeleton%20Action%20Recognition%20%28SAR%29%20involves%20identifying%20human%20actions%20using%0Askeletal%20joint%20coordinates%20and%20their%20interconnections.%20While%20plain%20Transformers%0Ahave%20been%20attempted%20for%20this%20task%2C%20they%20still%20fall%20short%20compared%20to%20the%0Acurrent%20leading%20methods%2C%20which%20are%20rooted%20in%20Graph%20Convolutional%20Networks%0A%28GCNs%29%20due%20to%20the%20absence%20of%20structural%20priors.%20Recently%2C%20a%20novel%20selective%0Astate%20space%20model%2C%20Mamba%2C%20has%20surfaced%20as%20a%20compelling%20alternative%20to%20the%0Aattention%20mechanism%20in%20Transformers%2C%20offering%20efficient%20modeling%20of%20long%0Asequences.%20In%20this%20work%2C%20to%20the%20utmost%20extent%20of%20our%20awareness%2C%20we%20present%20the%0Afirst%20SAR%20framework%20incorporating%20Mamba.%20Each%20fundamental%20block%20of%20our%20model%0Aadopts%20a%20novel%20U-ShiftGCN%20architecture%20with%20Mamba%20as%20its%20core%20component.%20The%0Aencoder%20segment%20of%20the%20U-ShiftGCN%20is%20devised%20to%20extract%20spatial%20features%20from%0Athe%20skeletal%20data%20using%20downsampling%20vanilla%20Shift%20S-GCN%20blocks.%20These%20spatial%0Afeatures%20then%20undergo%20intermediate%20temporal%20modeling%20facilitated%20by%20the%20Mamba%0Ablock%20before%20progressing%20to%20the%20encoder%20section%2C%20which%20comprises%20vanilla%0Aupsampling%20Shift%20S-GCN%20blocks.%20Additionally%2C%20a%20Shift%20T-GCN%20%28ShiftTCN%29%20temporal%0Amodeling%20unit%20is%20employed%20before%20the%20exit%20of%20each%20fundamental%20block%20to%20refine%0Atemporal%20representations.%20This%20particular%20integration%20of%20downsampling%20spatial%2C%0Aintermediate%20temporal%2C%20upsampling%20spatial%2C%20and%20ultimate%20temporal%20subunits%0Ayields%20promising%20results%20for%20skeleton%20action%20recognition.%20We%20dub%20the%20resulting%0Amodel%20%5Ctextbf%7BSimba%7D%2C%20which%20attains%20state-of-the-art%20performance%20across%20three%0Awell-known%20benchmark%20skeleton%20action%20recognition%20datasets%3A%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%0A120%2C%20and%20Northwestern-UCLA.%20Interestingly%2C%20U-ShiftGCN%20%28Simba%20without%0AIntermediate%20Mamba%20Block%29%20by%20itself%20is%20capable%20of%20performing%20reasonably%20well%0Aand%20surpasses%20our%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07645v1&entry.124074799=Read"},
{"title": "From the Lab to the Theater: An Unconventional Field Robotics Journey", "author": "Ali Imran and Vivek Shankar Varadharajan and Rafael Gomes Braga and Yann Bouteiller and Abdalwhab Bakheet Mohamed Abdalwhab and Matthis Di-Giacomo and Alexandra Mercader and Giovanni Beltrame and David St-Onge", "abstract": "  Artistic performances involving robotic systems present unique technical\nchallenges akin to those encountered in other field deployments. In this paper,\nwe delve into the orchestration of robotic artistic performances, focusing on\nthe complexities inherent in communication protocols and localization methods.\nThrough our case studies and experimental insights, we demonstrate the breadth\nof technical requirements for this type of deployment, and, most importantly,\nthe significant contributions of working closely with non-experts.\n", "link": "http://arxiv.org/abs/2404.07795v1", "date": "2024-04-11", "relevancy": 1.5455, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5883}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5091}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4884}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20the%20Lab%20to%20the%20Theater%3A%20An%20Unconventional%20Field%20Robotics%20Journey&body=Title%3A%20From%20the%20Lab%20to%20the%20Theater%3A%20An%20Unconventional%20Field%20Robotics%20Journey%0AAuthor%3A%20Ali%20Imran%20and%20Vivek%20Shankar%20Varadharajan%20and%20Rafael%20Gomes%20Braga%20and%20Yann%20Bouteiller%20and%20Abdalwhab%20Bakheet%20Mohamed%20Abdalwhab%20and%20Matthis%20Di-Giacomo%20and%20Alexandra%20Mercader%20and%20Giovanni%20Beltrame%20and%20David%20St-Onge%0AAbstract%3A%20%20%20Artistic%20performances%20involving%20robotic%20systems%20present%20unique%20technical%0Achallenges%20akin%20to%20those%20encountered%20in%20other%20field%20deployments.%20In%20this%20paper%2C%0Awe%20delve%20into%20the%20orchestration%20of%20robotic%20artistic%20performances%2C%20focusing%20on%0Athe%20complexities%20inherent%20in%20communication%20protocols%20and%20localization%20methods.%0AThrough%20our%20case%20studies%20and%20experimental%20insights%2C%20we%20demonstrate%20the%20breadth%0Aof%20technical%20requirements%20for%20this%20type%20of%20deployment%2C%20and%2C%20most%20importantly%2C%0Athe%20significant%20contributions%20of%20working%20closely%20with%20non-experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07795v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20the%20Lab%20to%20the%20Theater%3A%20An%20Unconventional%20Field%20Robotics%20Journey&entry.906535625=Ali%20Imran%20and%20Vivek%20Shankar%20Varadharajan%20and%20Rafael%20Gomes%20Braga%20and%20Yann%20Bouteiller%20and%20Abdalwhab%20Bakheet%20Mohamed%20Abdalwhab%20and%20Matthis%20Di-Giacomo%20and%20Alexandra%20Mercader%20and%20Giovanni%20Beltrame%20and%20David%20St-Onge&entry.1292438233=%20%20Artistic%20performances%20involving%20robotic%20systems%20present%20unique%20technical%0Achallenges%20akin%20to%20those%20encountered%20in%20other%20field%20deployments.%20In%20this%20paper%2C%0Awe%20delve%20into%20the%20orchestration%20of%20robotic%20artistic%20performances%2C%20focusing%20on%0Athe%20complexities%20inherent%20in%20communication%20protocols%20and%20localization%20methods.%0AThrough%20our%20case%20studies%20and%20experimental%20insights%2C%20we%20demonstrate%20the%20breadth%0Aof%20technical%20requirements%20for%20this%20type%20of%20deployment%2C%20and%2C%20most%20importantly%2C%0Athe%20significant%20contributions%20of%20working%20closely%20with%20non-experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07795v1&entry.124074799=Read"},
{"title": "BAMBOO: a predictive and transferable machine learning force field\n  framework for liquid electrolyte development", "author": "Sheng Gong and Yumin Zhang and Zhenliang Mu and Zhichen Pu and Hongyi Wang and Zhiao Yu and Mengyi Chen and Tianze Zheng and Zhi Wang and Lifei Chen and Xiaojie Wu and Shaochen Shi and Weihao Gao and Wen Yan and Liang Xiang", "abstract": "  Despite the widespread applications of machine learning force field (MLFF) on\nsolids and small molecules, there is a notable gap in applying MLFF to complex\nliquid electrolytes. In this work, we introduce BAMBOO (ByteDance AI Molecular\nSimulation Booster), a novel framework for molecular dynamics (MD) simulations,\nwith a demonstration of its capabilities in the context of liquid electrolytes\nfor lithium batteries. We design a physics-inspired graph equivariant\ntransformer architecture as the backbone of BAMBOO to learn from quantum\nmechanical simulations. Additionally, we pioneer an ensemble knowledge\ndistillation approach and apply it on MLFFs to improve the stability of MD\nsimulations. Finally, we propose the density alignment algorithm to align\nBAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-art\naccuracy in predicting key electrolyte properties such as density, viscosity,\nand ionic conductivity across various solvents and salt combinations. Our\ncurrent model, trained on more than 15 chemical species, achieves the average\ndensity error of 0.01 g/cm$^3$ on various compositions compared with\nexperimental data. Moreover, our model demonstrates transferability to\nmolecules not included in the quantum mechanical dataset. We envision this work\nas paving the way to a \"universal MLFF\" capable of simulating properties of\ncommon organic liquids.\n", "link": "http://arxiv.org/abs/2404.07181v2", "date": "2024-04-11", "relevancy": 1.367, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.426}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BAMBOO%3A%20a%20predictive%20and%20transferable%20machine%20learning%20force%20field%0A%20%20framework%20for%20liquid%20electrolyte%20development&body=Title%3A%20BAMBOO%3A%20a%20predictive%20and%20transferable%20machine%20learning%20force%20field%0A%20%20framework%20for%20liquid%20electrolyte%20development%0AAuthor%3A%20Sheng%20Gong%20and%20Yumin%20Zhang%20and%20Zhenliang%20Mu%20and%20Zhichen%20Pu%20and%20Hongyi%20Wang%20and%20Zhiao%20Yu%20and%20Mengyi%20Chen%20and%20Tianze%20Zheng%20and%20Zhi%20Wang%20and%20Lifei%20Chen%20and%20Xiaojie%20Wu%20and%20Shaochen%20Shi%20and%20Weihao%20Gao%20and%20Wen%20Yan%20and%20Liang%20Xiang%0AAbstract%3A%20%20%20Despite%20the%20widespread%20applications%20of%20machine%20learning%20force%20field%20%28MLFF%29%20on%0Asolids%20and%20small%20molecules%2C%20there%20is%20a%20notable%20gap%20in%20applying%20MLFF%20to%20complex%0Aliquid%20electrolytes.%20In%20this%20work%2C%20we%20introduce%20BAMBOO%20%28ByteDance%20AI%20Molecular%0ASimulation%20Booster%29%2C%20a%20novel%20framework%20for%20molecular%20dynamics%20%28MD%29%20simulations%2C%0Awith%20a%20demonstration%20of%20its%20capabilities%20in%20the%20context%20of%20liquid%20electrolytes%0Afor%20lithium%20batteries.%20We%20design%20a%20physics-inspired%20graph%20equivariant%0Atransformer%20architecture%20as%20the%20backbone%20of%20BAMBOO%20to%20learn%20from%20quantum%0Amechanical%20simulations.%20Additionally%2C%20we%20pioneer%20an%20ensemble%20knowledge%0Adistillation%20approach%20and%20apply%20it%20on%20MLFFs%20to%20improve%20the%20stability%20of%20MD%0Asimulations.%20Finally%2C%20we%20propose%20the%20density%20alignment%20algorithm%20to%20align%0ABAMBOO%20with%20experimental%20measurements.%20BAMBOO%20demonstrates%20state-of-the-art%0Aaccuracy%20in%20predicting%20key%20electrolyte%20properties%20such%20as%20density%2C%20viscosity%2C%0Aand%20ionic%20conductivity%20across%20various%20solvents%20and%20salt%20combinations.%20Our%0Acurrent%20model%2C%20trained%20on%20more%20than%2015%20chemical%20species%2C%20achieves%20the%20average%0Adensity%20error%20of%200.01%20g/cm%24%5E3%24%20on%20various%20compositions%20compared%20with%0Aexperimental%20data.%20Moreover%2C%20our%20model%20demonstrates%20transferability%20to%0Amolecules%20not%20included%20in%20the%20quantum%20mechanical%20dataset.%20We%20envision%20this%20work%0Aas%20paving%20the%20way%20to%20a%20%22universal%20MLFF%22%20capable%20of%20simulating%20properties%20of%0Acommon%20organic%20liquids.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07181v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAMBOO%3A%20a%20predictive%20and%20transferable%20machine%20learning%20force%20field%0A%20%20framework%20for%20liquid%20electrolyte%20development&entry.906535625=Sheng%20Gong%20and%20Yumin%20Zhang%20and%20Zhenliang%20Mu%20and%20Zhichen%20Pu%20and%20Hongyi%20Wang%20and%20Zhiao%20Yu%20and%20Mengyi%20Chen%20and%20Tianze%20Zheng%20and%20Zhi%20Wang%20and%20Lifei%20Chen%20and%20Xiaojie%20Wu%20and%20Shaochen%20Shi%20and%20Weihao%20Gao%20and%20Wen%20Yan%20and%20Liang%20Xiang&entry.1292438233=%20%20Despite%20the%20widespread%20applications%20of%20machine%20learning%20force%20field%20%28MLFF%29%20on%0Asolids%20and%20small%20molecules%2C%20there%20is%20a%20notable%20gap%20in%20applying%20MLFF%20to%20complex%0Aliquid%20electrolytes.%20In%20this%20work%2C%20we%20introduce%20BAMBOO%20%28ByteDance%20AI%20Molecular%0ASimulation%20Booster%29%2C%20a%20novel%20framework%20for%20molecular%20dynamics%20%28MD%29%20simulations%2C%0Awith%20a%20demonstration%20of%20its%20capabilities%20in%20the%20context%20of%20liquid%20electrolytes%0Afor%20lithium%20batteries.%20We%20design%20a%20physics-inspired%20graph%20equivariant%0Atransformer%20architecture%20as%20the%20backbone%20of%20BAMBOO%20to%20learn%20from%20quantum%0Amechanical%20simulations.%20Additionally%2C%20we%20pioneer%20an%20ensemble%20knowledge%0Adistillation%20approach%20and%20apply%20it%20on%20MLFFs%20to%20improve%20the%20stability%20of%20MD%0Asimulations.%20Finally%2C%20we%20propose%20the%20density%20alignment%20algorithm%20to%20align%0ABAMBOO%20with%20experimental%20measurements.%20BAMBOO%20demonstrates%20state-of-the-art%0Aaccuracy%20in%20predicting%20key%20electrolyte%20properties%20such%20as%20density%2C%20viscosity%2C%0Aand%20ionic%20conductivity%20across%20various%20solvents%20and%20salt%20combinations.%20Our%0Acurrent%20model%2C%20trained%20on%20more%20than%2015%20chemical%20species%2C%20achieves%20the%20average%0Adensity%20error%20of%200.01%20g/cm%24%5E3%24%20on%20various%20compositions%20compared%20with%0Aexperimental%20data.%20Moreover%2C%20our%20model%20demonstrates%20transferability%20to%0Amolecules%20not%20included%20in%20the%20quantum%20mechanical%20dataset.%20We%20envision%20this%20work%0Aas%20paving%20the%20way%20to%20a%20%22universal%20MLFF%22%20capable%20of%20simulating%20properties%20of%0Acommon%20organic%20liquids.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07181v2&entry.124074799=Read"},
{"title": "Automatic Detection of Dark Ship-to-Ship Transfers using Deep Learning\n  and Satellite Imagery", "author": "Ollie Ballinger", "abstract": "  Despite extensive research into ship detection via remote sensing, no studies\nidentify ship-to-ship transfers in satellite imagery. Given the importance of\ntransshipment in illicit shipping practices, this is a significant gap. In what\nfollows, I train a convolutional neural network to accurately detect 4\ndifferent types of cargo vessel and two different types of Ship-to-Ship\ntransfer in PlanetScope satellite imagery. I then elaborate a pipeline for the\nautomatic detection of suspected illicit ship-to-ship transfers by\ncross-referencing satellite detections with vessel borne GPS data. Finally, I\napply this method to the Kerch Strait between Ukraine and Russia to identify\nover 400 dark transshipment events since 2022.\n", "link": "http://arxiv.org/abs/2404.07607v1", "date": "2024-04-11", "relevancy": 1.401, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4793}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4517}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Detection%20of%20Dark%20Ship-to-Ship%20Transfers%20using%20Deep%20Learning%0A%20%20and%20Satellite%20Imagery&body=Title%3A%20Automatic%20Detection%20of%20Dark%20Ship-to-Ship%20Transfers%20using%20Deep%20Learning%0A%20%20and%20Satellite%20Imagery%0AAuthor%3A%20Ollie%20Ballinger%0AAbstract%3A%20%20%20Despite%20extensive%20research%20into%20ship%20detection%20via%20remote%20sensing%2C%20no%20studies%0Aidentify%20ship-to-ship%20transfers%20in%20satellite%20imagery.%20Given%20the%20importance%20of%0Atransshipment%20in%20illicit%20shipping%20practices%2C%20this%20is%20a%20significant%20gap.%20In%20what%0Afollows%2C%20I%20train%20a%20convolutional%20neural%20network%20to%20accurately%20detect%204%0Adifferent%20types%20of%20cargo%20vessel%20and%20two%20different%20types%20of%20Ship-to-Ship%0Atransfer%20in%20PlanetScope%20satellite%20imagery.%20I%20then%20elaborate%20a%20pipeline%20for%20the%0Aautomatic%20detection%20of%20suspected%20illicit%20ship-to-ship%20transfers%20by%0Across-referencing%20satellite%20detections%20with%20vessel%20borne%20GPS%20data.%20Finally%2C%20I%0Aapply%20this%20method%20to%20the%20Kerch%20Strait%20between%20Ukraine%20and%20Russia%20to%20identify%0Aover%20400%20dark%20transshipment%20events%20since%202022.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07607v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Detection%20of%20Dark%20Ship-to-Ship%20Transfers%20using%20Deep%20Learning%0A%20%20and%20Satellite%20Imagery&entry.906535625=Ollie%20Ballinger&entry.1292438233=%20%20Despite%20extensive%20research%20into%20ship%20detection%20via%20remote%20sensing%2C%20no%20studies%0Aidentify%20ship-to-ship%20transfers%20in%20satellite%20imagery.%20Given%20the%20importance%20of%0Atransshipment%20in%20illicit%20shipping%20practices%2C%20this%20is%20a%20significant%20gap.%20In%20what%0Afollows%2C%20I%20train%20a%20convolutional%20neural%20network%20to%20accurately%20detect%204%0Adifferent%20types%20of%20cargo%20vessel%20and%20two%20different%20types%20of%20Ship-to-Ship%0Atransfer%20in%20PlanetScope%20satellite%20imagery.%20I%20then%20elaborate%20a%20pipeline%20for%20the%0Aautomatic%20detection%20of%20suspected%20illicit%20ship-to-ship%20transfers%20by%0Across-referencing%20satellite%20detections%20with%20vessel%20borne%20GPS%20data.%20Finally%2C%20I%0Aapply%20this%20method%20to%20the%20Kerch%20Strait%20between%20Ukraine%20and%20Russia%20to%20identify%0Aover%20400%20dark%20transshipment%20events%20since%202022.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07607v1&entry.124074799=Read"},
{"title": "ResearchAgent: Iterative Research Idea Generation over Scientific\n  Literature with Large Language Models", "author": "Jinheon Baek and Sujay Kumar Jauhar and Silviu Cucerzan and Sung Ju Hwang", "abstract": "  Scientific Research, vital for improving human life, is hindered by its\ninherent complexity, slow pace, and the need for specialized experts. To\nenhance its productivity, we propose a ResearchAgent, a large language\nmodel-powered research idea writing agent, which automatically generates\nproblems, methods, and experiment designs while iteratively refining them based\non scientific literature. Specifically, starting with a core paper as the\nprimary focus to generate ideas, our ResearchAgent is augmented not only with\nrelevant publications through connecting information over an academic graph but\nalso entities retrieved from an entity-centric knowledge store based on their\nunderlying concepts, mined and shared across numerous papers. In addition,\nmirroring the human approach to iteratively improving ideas with peer\ndiscussions, we leverage multiple ReviewingAgents that provide reviews and\nfeedback iteratively. Further, they are instantiated with human\npreference-aligned large language models whose criteria for evaluation are\nderived from actual human judgments. We experimentally validate our\nResearchAgent on scientific publications across multiple disciplines,\nshowcasing its effectiveness in generating novel, clear, and valid research\nideas based on human and model-based evaluation results.\n", "link": "http://arxiv.org/abs/2404.07738v1", "date": "2024-04-11", "relevancy": 1.4899, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5118}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4654}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ResearchAgent%3A%20Iterative%20Research%20Idea%20Generation%20over%20Scientific%0A%20%20Literature%20with%20Large%20Language%20Models&body=Title%3A%20ResearchAgent%3A%20Iterative%20Research%20Idea%20Generation%20over%20Scientific%0A%20%20Literature%20with%20Large%20Language%20Models%0AAuthor%3A%20Jinheon%20Baek%20and%20Sujay%20Kumar%20Jauhar%20and%20Silviu%20Cucerzan%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Scientific%20Research%2C%20vital%20for%20improving%20human%20life%2C%20is%20hindered%20by%20its%0Ainherent%20complexity%2C%20slow%20pace%2C%20and%20the%20need%20for%20specialized%20experts.%20To%0Aenhance%20its%20productivity%2C%20we%20propose%20a%20ResearchAgent%2C%20a%20large%20language%0Amodel-powered%20research%20idea%20writing%20agent%2C%20which%20automatically%20generates%0Aproblems%2C%20methods%2C%20and%20experiment%20designs%20while%20iteratively%20refining%20them%20based%0Aon%20scientific%20literature.%20Specifically%2C%20starting%20with%20a%20core%20paper%20as%20the%0Aprimary%20focus%20to%20generate%20ideas%2C%20our%20ResearchAgent%20is%20augmented%20not%20only%20with%0Arelevant%20publications%20through%20connecting%20information%20over%20an%20academic%20graph%20but%0Aalso%20entities%20retrieved%20from%20an%20entity-centric%20knowledge%20store%20based%20on%20their%0Aunderlying%20concepts%2C%20mined%20and%20shared%20across%20numerous%20papers.%20In%20addition%2C%0Amirroring%20the%20human%20approach%20to%20iteratively%20improving%20ideas%20with%20peer%0Adiscussions%2C%20we%20leverage%20multiple%20ReviewingAgents%20that%20provide%20reviews%20and%0Afeedback%20iteratively.%20Further%2C%20they%20are%20instantiated%20with%20human%0Apreference-aligned%20large%20language%20models%20whose%20criteria%20for%20evaluation%20are%0Aderived%20from%20actual%20human%20judgments.%20We%20experimentally%20validate%20our%0AResearchAgent%20on%20scientific%20publications%20across%20multiple%20disciplines%2C%0Ashowcasing%20its%20effectiveness%20in%20generating%20novel%2C%20clear%2C%20and%20valid%20research%0Aideas%20based%20on%20human%20and%20model-based%20evaluation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07738v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResearchAgent%3A%20Iterative%20Research%20Idea%20Generation%20over%20Scientific%0A%20%20Literature%20with%20Large%20Language%20Models&entry.906535625=Jinheon%20Baek%20and%20Sujay%20Kumar%20Jauhar%20and%20Silviu%20Cucerzan%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Scientific%20Research%2C%20vital%20for%20improving%20human%20life%2C%20is%20hindered%20by%20its%0Ainherent%20complexity%2C%20slow%20pace%2C%20and%20the%20need%20for%20specialized%20experts.%20To%0Aenhance%20its%20productivity%2C%20we%20propose%20a%20ResearchAgent%2C%20a%20large%20language%0Amodel-powered%20research%20idea%20writing%20agent%2C%20which%20automatically%20generates%0Aproblems%2C%20methods%2C%20and%20experiment%20designs%20while%20iteratively%20refining%20them%20based%0Aon%20scientific%20literature.%20Specifically%2C%20starting%20with%20a%20core%20paper%20as%20the%0Aprimary%20focus%20to%20generate%20ideas%2C%20our%20ResearchAgent%20is%20augmented%20not%20only%20with%0Arelevant%20publications%20through%20connecting%20information%20over%20an%20academic%20graph%20but%0Aalso%20entities%20retrieved%20from%20an%20entity-centric%20knowledge%20store%20based%20on%20their%0Aunderlying%20concepts%2C%20mined%20and%20shared%20across%20numerous%20papers.%20In%20addition%2C%0Amirroring%20the%20human%20approach%20to%20iteratively%20improving%20ideas%20with%20peer%0Adiscussions%2C%20we%20leverage%20multiple%20ReviewingAgents%20that%20provide%20reviews%20and%0Afeedback%20iteratively.%20Further%2C%20they%20are%20instantiated%20with%20human%0Apreference-aligned%20large%20language%20models%20whose%20criteria%20for%20evaluation%20are%0Aderived%20from%20actual%20human%20judgments.%20We%20experimentally%20validate%20our%0AResearchAgent%20on%20scientific%20publications%20across%20multiple%20disciplines%2C%0Ashowcasing%20its%20effectiveness%20in%20generating%20novel%2C%20clear%2C%20and%20valid%20research%0Aideas%20based%20on%20human%20and%20model-based%20evaluation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07738v1&entry.124074799=Read"},
{"title": "Risk Estimation in a Markov Cost Process: Lower and Upper Bounds", "author": "Gugan Thoppe and L. A. Prashanth and Sanjay Bhat", "abstract": "  We tackle the problem of estimating risk measures of the infinite-horizon\ndiscounted cost within a Markov cost process. The risk measures we study\ninclude variance, Value-at-Risk (VaR), and Conditional Value-at-Risk (CVaR).\nFirst, we show that estimating any of these risk measures with\n$\\epsilon$-accuracy, either in expected or high-probability sense, requires at\nleast $\\Omega(1/\\epsilon^2)$ samples. Then, using a truncation scheme, we\nderive an upper bound for the CVaR and variance estimation. This bound matches\nour lower bound up to logarithmic factors. Finally, we discuss an extension of\nour estimation scheme that covers more general risk measures satisfying a\ncertain continuity criterion, e.g., spectral risk measures, utility-based\nshortfall risk. To the best of our knowledge, our work is the first to provide\nlower and upper bounds for estimating any risk measure beyond the mean within a\nMarkovian setting. Our lower bounds also extend to the infinite-horizon\ndiscounted costs' mean. Even in that case, our lower bound of\n$\\Omega(1/\\epsilon^2) $ improves upon the existing $\\Omega(1/\\epsilon)$ bound\n[13].\n", "link": "http://arxiv.org/abs/2310.11389v2", "date": "2024-04-11", "relevancy": 1.1811, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4279}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3854}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3803}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Risk%20Estimation%20in%20a%20Markov%20Cost%20Process%3A%20Lower%20and%20Upper%20Bounds&body=Title%3A%20Risk%20Estimation%20in%20a%20Markov%20Cost%20Process%3A%20Lower%20and%20Upper%20Bounds%0AAuthor%3A%20Gugan%20Thoppe%20and%20L.%20A.%20Prashanth%20and%20Sanjay%20Bhat%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20estimating%20risk%20measures%20of%20the%20infinite-horizon%0Adiscounted%20cost%20within%20a%20Markov%20cost%20process.%20The%20risk%20measures%20we%20study%0Ainclude%20variance%2C%20Value-at-Risk%20%28VaR%29%2C%20and%20Conditional%20Value-at-Risk%20%28CVaR%29.%0AFirst%2C%20we%20show%20that%20estimating%20any%20of%20these%20risk%20measures%20with%0A%24%5Cepsilon%24-accuracy%2C%20either%20in%20expected%20or%20high-probability%20sense%2C%20requires%20at%0Aleast%20%24%5COmega%281/%5Cepsilon%5E2%29%24%20samples.%20Then%2C%20using%20a%20truncation%20scheme%2C%20we%0Aderive%20an%20upper%20bound%20for%20the%20CVaR%20and%20variance%20estimation.%20This%20bound%20matches%0Aour%20lower%20bound%20up%20to%20logarithmic%20factors.%20Finally%2C%20we%20discuss%20an%20extension%20of%0Aour%20estimation%20scheme%20that%20covers%20more%20general%20risk%20measures%20satisfying%20a%0Acertain%20continuity%20criterion%2C%20e.g.%2C%20spectral%20risk%20measures%2C%20utility-based%0Ashortfall%20risk.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20provide%0Alower%20and%20upper%20bounds%20for%20estimating%20any%20risk%20measure%20beyond%20the%20mean%20within%20a%0AMarkovian%20setting.%20Our%20lower%20bounds%20also%20extend%20to%20the%20infinite-horizon%0Adiscounted%20costs%27%20mean.%20Even%20in%20that%20case%2C%20our%20lower%20bound%20of%0A%24%5COmega%281/%5Cepsilon%5E2%29%20%24%20improves%20upon%20the%20existing%20%24%5COmega%281/%5Cepsilon%29%24%20bound%0A%5B13%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11389v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk%20Estimation%20in%20a%20Markov%20Cost%20Process%3A%20Lower%20and%20Upper%20Bounds&entry.906535625=Gugan%20Thoppe%20and%20L.%20A.%20Prashanth%20and%20Sanjay%20Bhat&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20estimating%20risk%20measures%20of%20the%20infinite-horizon%0Adiscounted%20cost%20within%20a%20Markov%20cost%20process.%20The%20risk%20measures%20we%20study%0Ainclude%20variance%2C%20Value-at-Risk%20%28VaR%29%2C%20and%20Conditional%20Value-at-Risk%20%28CVaR%29.%0AFirst%2C%20we%20show%20that%20estimating%20any%20of%20these%20risk%20measures%20with%0A%24%5Cepsilon%24-accuracy%2C%20either%20in%20expected%20or%20high-probability%20sense%2C%20requires%20at%0Aleast%20%24%5COmega%281/%5Cepsilon%5E2%29%24%20samples.%20Then%2C%20using%20a%20truncation%20scheme%2C%20we%0Aderive%20an%20upper%20bound%20for%20the%20CVaR%20and%20variance%20estimation.%20This%20bound%20matches%0Aour%20lower%20bound%20up%20to%20logarithmic%20factors.%20Finally%2C%20we%20discuss%20an%20extension%20of%0Aour%20estimation%20scheme%20that%20covers%20more%20general%20risk%20measures%20satisfying%20a%0Acertain%20continuity%20criterion%2C%20e.g.%2C%20spectral%20risk%20measures%2C%20utility-based%0Ashortfall%20risk.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20provide%0Alower%20and%20upper%20bounds%20for%20estimating%20any%20risk%20measure%20beyond%20the%20mean%20within%20a%0AMarkovian%20setting.%20Our%20lower%20bounds%20also%20extend%20to%20the%20infinite-horizon%0Adiscounted%20costs%27%20mean.%20Even%20in%20that%20case%2C%20our%20lower%20bound%20of%0A%24%5COmega%281/%5Cepsilon%5E2%29%20%24%20improves%20upon%20the%20existing%20%24%5COmega%281/%5Cepsilon%29%24%20bound%0A%5B13%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11389v2&entry.124074799=Read"},
{"title": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian\n  Perspective", "author": "Victor Gallego", "abstract": "  This paper proposes an interpretation of RLAIF as Bayesian inference by\nintroducing distilled Self-Critique (dSC), which refines the outputs of a LLM\nthrough a Gibbs sampler that is later distilled into a fine-tuned model. Only\nrequiring synthetic data, dSC is exercised in experiments regarding safety,\nsentiment, and privacy control, showing it can be a viable and cheap\nalternative to align LLMs. Code released at\n\\url{https://github.com/vicgalle/distilled-self-critique}.\n", "link": "http://arxiv.org/abs/2312.01957v3", "date": "2024-04-11", "relevancy": 1.8741, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5001}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4634}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.461}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Distilled%20Self-Critique%20of%20LLMs%20with%20Synthetic%20Data%3A%20a%20Bayesian%0A%20%20Perspective&body=Title%3A%20Distilled%20Self-Critique%20of%20LLMs%20with%20Synthetic%20Data%3A%20a%20Bayesian%0A%20%20Perspective%0AAuthor%3A%20Victor%20Gallego%0AAbstract%3A%20%20%20This%20paper%20proposes%20an%20interpretation%20of%20RLAIF%20as%20Bayesian%20inference%20by%0Aintroducing%20distilled%20Self-Critique%20%28dSC%29%2C%20which%20refines%20the%20outputs%20of%20a%20LLM%0Athrough%20a%20Gibbs%20sampler%20that%20is%20later%20distilled%20into%20a%20fine-tuned%20model.%20Only%0Arequiring%20synthetic%20data%2C%20dSC%20is%20exercised%20in%20experiments%20regarding%20safety%2C%0Asentiment%2C%20and%20privacy%20control%2C%20showing%20it%20can%20be%20a%20viable%20and%20cheap%0Aalternative%20to%20align%20LLMs.%20Code%20released%20at%0A%5Curl%7Bhttps%3A//github.com/vicgalle/distilled-self-critique%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01957v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilled%20Self-Critique%20of%20LLMs%20with%20Synthetic%20Data%3A%20a%20Bayesian%0A%20%20Perspective&entry.906535625=Victor%20Gallego&entry.1292438233=%20%20This%20paper%20proposes%20an%20interpretation%20of%20RLAIF%20as%20Bayesian%20inference%20by%0Aintroducing%20distilled%20Self-Critique%20%28dSC%29%2C%20which%20refines%20the%20outputs%20of%20a%20LLM%0Athrough%20a%20Gibbs%20sampler%20that%20is%20later%20distilled%20into%20a%20fine-tuned%20model.%20Only%0Arequiring%20synthetic%20data%2C%20dSC%20is%20exercised%20in%20experiments%20regarding%20safety%2C%0Asentiment%2C%20and%20privacy%20control%2C%20showing%20it%20can%20be%20a%20viable%20and%20cheap%0Aalternative%20to%20align%20LLMs.%20Code%20released%20at%0A%5Curl%7Bhttps%3A//github.com/vicgalle/distilled-self-critique%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01957v3&entry.124074799=Read"},
{"title": "FusionMamba: Efficient Image Fusion with State Space Model", "author": "Siran Peng and Xiangyu Zhu and Haoyu Deng and Zhen Lei and Liang-Jian Deng", "abstract": "  Image fusion aims to generate a high-resolution multi/hyper-spectral image by\ncombining a high-resolution image with limited spectral information and a\nlow-resolution image with abundant spectral data. Current deep learning\n(DL)-based methods for image fusion primarily rely on CNNs or Transformers to\nextract features and merge different types of data. While CNNs are efficient,\ntheir receptive fields are limited, restricting their capacity to capture\nglobal context. Conversely, Transformers excel at learning global information\nbut are hindered by their quadratic complexity. Fortunately, recent\nadvancements in the State Space Model (SSM), particularly Mamba, offer a\npromising solution to this issue by enabling global awareness with linear\ncomplexity. However, there have been few attempts to explore the potential of\nSSM in information fusion, which is a crucial ability in domains like image\nfusion. Therefore, we propose FusionMamba, an innovative method for efficient\nimage fusion. Our contributions mainly focus on two aspects. Firstly,\nrecognizing that images from different sources possess distinct properties, we\nincorporate Mamba blocks into two U-shaped networks, presenting a novel\narchitecture that extracts spatial and spectral features in an efficient,\nindependent, and hierarchical manner. Secondly, to effectively combine spatial\nand spectral information, we extend the Mamba block to accommodate dual inputs.\nThis expansion leads to the creation of a new module called the FusionMamba\nblock, which outperforms existing fusion techniques such as concatenation and\ncross-attention. To validate FusionMamba's effectiveness, we conduct a series\nof experiments on five datasets related to three image fusion tasks. The\nquantitative and qualitative evaluation results demonstrate that our method\nachieves state-of-the-art (SOTA) performance, underscoring the superiority of\nFusionMamba.\n", "link": "http://arxiv.org/abs/2404.07932v1", "date": "2024-04-11", "relevancy": 1.5311, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5327}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5069}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4968}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FusionMamba%3A%20Efficient%20Image%20Fusion%20with%20State%20Space%20Model&body=Title%3A%20FusionMamba%3A%20Efficient%20Image%20Fusion%20with%20State%20Space%20Model%0AAuthor%3A%20Siran%20Peng%20and%20Xiangyu%20Zhu%20and%20Haoyu%20Deng%20and%20Zhen%20Lei%20and%20Liang-Jian%20Deng%0AAbstract%3A%20%20%20Image%20fusion%20aims%20to%20generate%20a%20high-resolution%20multi/hyper-spectral%20image%20by%0Acombining%20a%20high-resolution%20image%20with%20limited%20spectral%20information%20and%20a%0Alow-resolution%20image%20with%20abundant%20spectral%20data.%20Current%20deep%20learning%0A%28DL%29-based%20methods%20for%20image%20fusion%20primarily%20rely%20on%20CNNs%20or%20Transformers%20to%0Aextract%20features%20and%20merge%20different%20types%20of%20data.%20While%20CNNs%20are%20efficient%2C%0Atheir%20receptive%20fields%20are%20limited%2C%20restricting%20their%20capacity%20to%20capture%0Aglobal%20context.%20Conversely%2C%20Transformers%20excel%20at%20learning%20global%20information%0Abut%20are%20hindered%20by%20their%20quadratic%20complexity.%20Fortunately%2C%20recent%0Aadvancements%20in%20the%20State%20Space%20Model%20%28SSM%29%2C%20particularly%20Mamba%2C%20offer%20a%0Apromising%20solution%20to%20this%20issue%20by%20enabling%20global%20awareness%20with%20linear%0Acomplexity.%20However%2C%20there%20have%20been%20few%20attempts%20to%20explore%20the%20potential%20of%0ASSM%20in%20information%20fusion%2C%20which%20is%20a%20crucial%20ability%20in%20domains%20like%20image%0Afusion.%20Therefore%2C%20we%20propose%20FusionMamba%2C%20an%20innovative%20method%20for%20efficient%0Aimage%20fusion.%20Our%20contributions%20mainly%20focus%20on%20two%20aspects.%20Firstly%2C%0Arecognizing%20that%20images%20from%20different%20sources%20possess%20distinct%20properties%2C%20we%0Aincorporate%20Mamba%20blocks%20into%20two%20U-shaped%20networks%2C%20presenting%20a%20novel%0Aarchitecture%20that%20extracts%20spatial%20and%20spectral%20features%20in%20an%20efficient%2C%0Aindependent%2C%20and%20hierarchical%20manner.%20Secondly%2C%20to%20effectively%20combine%20spatial%0Aand%20spectral%20information%2C%20we%20extend%20the%20Mamba%20block%20to%20accommodate%20dual%20inputs.%0AThis%20expansion%20leads%20to%20the%20creation%20of%20a%20new%20module%20called%20the%20FusionMamba%0Ablock%2C%20which%20outperforms%20existing%20fusion%20techniques%20such%20as%20concatenation%20and%0Across-attention.%20To%20validate%20FusionMamba%27s%20effectiveness%2C%20we%20conduct%20a%20series%0Aof%20experiments%20on%20five%20datasets%20related%20to%20three%20image%20fusion%20tasks.%20The%0Aquantitative%20and%20qualitative%20evaluation%20results%20demonstrate%20that%20our%20method%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%2C%20underscoring%20the%20superiority%20of%0AFusionMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07932v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionMamba%3A%20Efficient%20Image%20Fusion%20with%20State%20Space%20Model&entry.906535625=Siran%20Peng%20and%20Xiangyu%20Zhu%20and%20Haoyu%20Deng%20and%20Zhen%20Lei%20and%20Liang-Jian%20Deng&entry.1292438233=%20%20Image%20fusion%20aims%20to%20generate%20a%20high-resolution%20multi/hyper-spectral%20image%20by%0Acombining%20a%20high-resolution%20image%20with%20limited%20spectral%20information%20and%20a%0Alow-resolution%20image%20with%20abundant%20spectral%20data.%20Current%20deep%20learning%0A%28DL%29-based%20methods%20for%20image%20fusion%20primarily%20rely%20on%20CNNs%20or%20Transformers%20to%0Aextract%20features%20and%20merge%20different%20types%20of%20data.%20While%20CNNs%20are%20efficient%2C%0Atheir%20receptive%20fields%20are%20limited%2C%20restricting%20their%20capacity%20to%20capture%0Aglobal%20context.%20Conversely%2C%20Transformers%20excel%20at%20learning%20global%20information%0Abut%20are%20hindered%20by%20their%20quadratic%20complexity.%20Fortunately%2C%20recent%0Aadvancements%20in%20the%20State%20Space%20Model%20%28SSM%29%2C%20particularly%20Mamba%2C%20offer%20a%0Apromising%20solution%20to%20this%20issue%20by%20enabling%20global%20awareness%20with%20linear%0Acomplexity.%20However%2C%20there%20have%20been%20few%20attempts%20to%20explore%20the%20potential%20of%0ASSM%20in%20information%20fusion%2C%20which%20is%20a%20crucial%20ability%20in%20domains%20like%20image%0Afusion.%20Therefore%2C%20we%20propose%20FusionMamba%2C%20an%20innovative%20method%20for%20efficient%0Aimage%20fusion.%20Our%20contributions%20mainly%20focus%20on%20two%20aspects.%20Firstly%2C%0Arecognizing%20that%20images%20from%20different%20sources%20possess%20distinct%20properties%2C%20we%0Aincorporate%20Mamba%20blocks%20into%20two%20U-shaped%20networks%2C%20presenting%20a%20novel%0Aarchitecture%20that%20extracts%20spatial%20and%20spectral%20features%20in%20an%20efficient%2C%0Aindependent%2C%20and%20hierarchical%20manner.%20Secondly%2C%20to%20effectively%20combine%20spatial%0Aand%20spectral%20information%2C%20we%20extend%20the%20Mamba%20block%20to%20accommodate%20dual%20inputs.%0AThis%20expansion%20leads%20to%20the%20creation%20of%20a%20new%20module%20called%20the%20FusionMamba%0Ablock%2C%20which%20outperforms%20existing%20fusion%20techniques%20such%20as%20concatenation%20and%0Across-attention.%20To%20validate%20FusionMamba%27s%20effectiveness%2C%20we%20conduct%20a%20series%0Aof%20experiments%20on%20five%20datasets%20related%20to%20three%20image%20fusion%20tasks.%20The%0Aquantitative%20and%20qualitative%20evaluation%20results%20demonstrate%20that%20our%20method%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%2C%20underscoring%20the%20superiority%20of%0AFusionMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07932v1&entry.124074799=Read"},
{"title": "On Unified Prompt Tuning for Request Quality Assurance in Public Code\n  Review", "author": "Xinyu Chen and Lin Li and Rui Zhang and Peng Liang", "abstract": "  Public Code Review (PCR) can be implemented through a Software Question\nAnswering (SQA) community, which facilitates high knowledge dissemination.\nCurrent methods mainly focus on the reviewer's perspective, including finding a\ncapable reviewer, predicting comment quality, and recommending/generating\nreview comments. Our intuition is that satisfying review necessity requests can\nincrease their visibility, which in turn is a prerequisite for better review\nresponses. To this end, we propose a unified framework called UniPCR to\ncomplete developer-based request quality assurance (i.e., predicting request\nnecessity and recommending tags subtask) under a Masked Language Model (MLM).\nSpecifically, we reformulate both subtasks via 1) text prompt tuning, which\nconverts two subtasks into MLM by constructing prompt templates using hard\nprompt; 2) code prefix tuning, which optimizes a small segment of generated\ncontinuous vectors as the prefix of the code representation using soft prompt.\nExperimental results on the Public Code Review dataset for the time span\n2011-2022 demonstrate that our UniPCR framework adapts to the two subtasks and\noutperforms comparable accuracy-based results with state-of-the-art methods for\nrequest quality assurance. These conclusions highlight the effectiveness of our\nunified framework from the developer's perspective in public code review.\n", "link": "http://arxiv.org/abs/2404.07942v1", "date": "2024-04-11", "relevancy": 1.7382, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4408}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4344}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4322}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Unified%20Prompt%20Tuning%20for%20Request%20Quality%20Assurance%20in%20Public%20Code%0A%20%20Review&body=Title%3A%20On%20Unified%20Prompt%20Tuning%20for%20Request%20Quality%20Assurance%20in%20Public%20Code%0A%20%20Review%0AAuthor%3A%20Xinyu%20Chen%20and%20Lin%20Li%20and%20Rui%20Zhang%20and%20Peng%20Liang%0AAbstract%3A%20%20%20Public%20Code%20Review%20%28PCR%29%20can%20be%20implemented%20through%20a%20Software%20Question%0AAnswering%20%28SQA%29%20community%2C%20which%20facilitates%20high%20knowledge%20dissemination.%0ACurrent%20methods%20mainly%20focus%20on%20the%20reviewer%27s%20perspective%2C%20including%20finding%20a%0Acapable%20reviewer%2C%20predicting%20comment%20quality%2C%20and%20recommending/generating%0Areview%20comments.%20Our%20intuition%20is%20that%20satisfying%20review%20necessity%20requests%20can%0Aincrease%20their%20visibility%2C%20which%20in%20turn%20is%20a%20prerequisite%20for%20better%20review%0Aresponses.%20To%20this%20end%2C%20we%20propose%20a%20unified%20framework%20called%20UniPCR%20to%0Acomplete%20developer-based%20request%20quality%20assurance%20%28i.e.%2C%20predicting%20request%0Anecessity%20and%20recommending%20tags%20subtask%29%20under%20a%20Masked%20Language%20Model%20%28MLM%29.%0ASpecifically%2C%20we%20reformulate%20both%20subtasks%20via%201%29%20text%20prompt%20tuning%2C%20which%0Aconverts%20two%20subtasks%20into%20MLM%20by%20constructing%20prompt%20templates%20using%20hard%0Aprompt%3B%202%29%20code%20prefix%20tuning%2C%20which%20optimizes%20a%20small%20segment%20of%20generated%0Acontinuous%20vectors%20as%20the%20prefix%20of%20the%20code%20representation%20using%20soft%20prompt.%0AExperimental%20results%20on%20the%20Public%20Code%20Review%20dataset%20for%20the%20time%20span%0A2011-2022%20demonstrate%20that%20our%20UniPCR%20framework%20adapts%20to%20the%20two%20subtasks%20and%0Aoutperforms%20comparable%20accuracy-based%20results%20with%20state-of-the-art%20methods%20for%0Arequest%20quality%20assurance.%20These%20conclusions%20highlight%20the%20effectiveness%20of%20our%0Aunified%20framework%20from%20the%20developer%27s%20perspective%20in%20public%20code%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07942v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Unified%20Prompt%20Tuning%20for%20Request%20Quality%20Assurance%20in%20Public%20Code%0A%20%20Review&entry.906535625=Xinyu%20Chen%20and%20Lin%20Li%20and%20Rui%20Zhang%20and%20Peng%20Liang&entry.1292438233=%20%20Public%20Code%20Review%20%28PCR%29%20can%20be%20implemented%20through%20a%20Software%20Question%0AAnswering%20%28SQA%29%20community%2C%20which%20facilitates%20high%20knowledge%20dissemination.%0ACurrent%20methods%20mainly%20focus%20on%20the%20reviewer%27s%20perspective%2C%20including%20finding%20a%0Acapable%20reviewer%2C%20predicting%20comment%20quality%2C%20and%20recommending/generating%0Areview%20comments.%20Our%20intuition%20is%20that%20satisfying%20review%20necessity%20requests%20can%0Aincrease%20their%20visibility%2C%20which%20in%20turn%20is%20a%20prerequisite%20for%20better%20review%0Aresponses.%20To%20this%20end%2C%20we%20propose%20a%20unified%20framework%20called%20UniPCR%20to%0Acomplete%20developer-based%20request%20quality%20assurance%20%28i.e.%2C%20predicting%20request%0Anecessity%20and%20recommending%20tags%20subtask%29%20under%20a%20Masked%20Language%20Model%20%28MLM%29.%0ASpecifically%2C%20we%20reformulate%20both%20subtasks%20via%201%29%20text%20prompt%20tuning%2C%20which%0Aconverts%20two%20subtasks%20into%20MLM%20by%20constructing%20prompt%20templates%20using%20hard%0Aprompt%3B%202%29%20code%20prefix%20tuning%2C%20which%20optimizes%20a%20small%20segment%20of%20generated%0Acontinuous%20vectors%20as%20the%20prefix%20of%20the%20code%20representation%20using%20soft%20prompt.%0AExperimental%20results%20on%20the%20Public%20Code%20Review%20dataset%20for%20the%20time%20span%0A2011-2022%20demonstrate%20that%20our%20UniPCR%20framework%20adapts%20to%20the%20two%20subtasks%20and%0Aoutperforms%20comparable%20accuracy-based%20results%20with%20state-of-the-art%20methods%20for%0Arequest%20quality%20assurance.%20These%20conclusions%20highlight%20the%20effectiveness%20of%20our%0Aunified%20framework%20from%20the%20developer%27s%20perspective%20in%20public%20code%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07942v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


