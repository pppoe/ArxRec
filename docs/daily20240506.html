<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240505.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular\n  Videos", "author": "Wen-Hsuan Chu and Lei Ke and Katerina Fragkiadaki", "abstract": "  Existing VLMs can track in-the-wild 2D video objects while current generative\nmodels provide powerful visual priors for synthesizing novel views for the\nhighly under-constrained 2D-to-3D object lifting. Building upon this exciting\nprogress, we present DreamScene4D, the first approach that can generate\nthree-dimensional dynamic scenes of multiple objects from monocular in-the-wild\nvideos with large object motion across occlusions and novel viewpoints. Our key\ninsight is to design a \"decompose-then-recompose\" scheme to factorize both the\nwhole video scene and each object's 3D motion. We first decompose the video\nscene by using open-vocabulary mask trackers and an adapted image diffusion\nmodel to segment, track, and amodally complete the objects and background in\nthe video. Each object track is mapped to a set of 3D Gaussians that deform and\nmove in space and time. We also factorize the observed motion into multiple\ncomponents to handle fast motion. The camera motion can be inferred by\nre-rendering the background to match the video frames. For the object motion,\nwe first model the object-centric deformation of the objects by leveraging\nrendering losses and multi-view generative priors in an object-centric frame,\nthen optimize object-centric to world-frame transformations by comparing the\nrendered outputs against the perceived pixel and optical flow. Finally, we\nrecompose the background and objects and optimize for relative object scales\nusing monocular depth prediction guidance. We show extensive results on the\nchallenging DAVIS, Kubric, and self-captured videos, detail some limitations,\nand provide future directions. Besides 4D scene generation, our results show\nthat DreamScene4D enables accurate 2D point motion tracking by projecting the\ninferred 3D trajectories to 2D, while never explicitly trained to do so.\n", "link": "http://arxiv.org/abs/2405.02280v1", "date": "2024-05-03", "relevancy": 3.1011, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6624}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6056}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamScene4D%3A%20Dynamic%20Multi-Object%20Scene%20Generation%20from%20Monocular%0A%20%20Videos&body=Title%3A%20DreamScene4D%3A%20Dynamic%20Multi-Object%20Scene%20Generation%20from%20Monocular%0A%20%20Videos%0AAuthor%3A%20Wen-Hsuan%20Chu%20and%20Lei%20Ke%20and%20Katerina%20Fragkiadaki%0AAbstract%3A%20%20%20Existing%20VLMs%20can%20track%20in-the-wild%202D%20video%20objects%20while%20current%20generative%0Amodels%20provide%20powerful%20visual%20priors%20for%20synthesizing%20novel%20views%20for%20the%0Ahighly%20under-constrained%202D-to-3D%20object%20lifting.%20Building%20upon%20this%20exciting%0Aprogress%2C%20we%20present%20DreamScene4D%2C%20the%20first%20approach%20that%20can%20generate%0Athree-dimensional%20dynamic%20scenes%20of%20multiple%20objects%20from%20monocular%20in-the-wild%0Avideos%20with%20large%20object%20motion%20across%20occlusions%20and%20novel%20viewpoints.%20Our%20key%0Ainsight%20is%20to%20design%20a%20%22decompose-then-recompose%22%20scheme%20to%20factorize%20both%20the%0Awhole%20video%20scene%20and%20each%20object%27s%203D%20motion.%20We%20first%20decompose%20the%20video%0Ascene%20by%20using%20open-vocabulary%20mask%20trackers%20and%20an%20adapted%20image%20diffusion%0Amodel%20to%20segment%2C%20track%2C%20and%20amodally%20complete%20the%20objects%20and%20background%20in%0Athe%20video.%20Each%20object%20track%20is%20mapped%20to%20a%20set%20of%203D%20Gaussians%20that%20deform%20and%0Amove%20in%20space%20and%20time.%20We%20also%20factorize%20the%20observed%20motion%20into%20multiple%0Acomponents%20to%20handle%20fast%20motion.%20The%20camera%20motion%20can%20be%20inferred%20by%0Are-rendering%20the%20background%20to%20match%20the%20video%20frames.%20For%20the%20object%20motion%2C%0Awe%20first%20model%20the%20object-centric%20deformation%20of%20the%20objects%20by%20leveraging%0Arendering%20losses%20and%20multi-view%20generative%20priors%20in%20an%20object-centric%20frame%2C%0Athen%20optimize%20object-centric%20to%20world-frame%20transformations%20by%20comparing%20the%0Arendered%20outputs%20against%20the%20perceived%20pixel%20and%20optical%20flow.%20Finally%2C%20we%0Arecompose%20the%20background%20and%20objects%20and%20optimize%20for%20relative%20object%20scales%0Ausing%20monocular%20depth%20prediction%20guidance.%20We%20show%20extensive%20results%20on%20the%0Achallenging%20DAVIS%2C%20Kubric%2C%20and%20self-captured%20videos%2C%20detail%20some%20limitations%2C%0Aand%20provide%20future%20directions.%20Besides%204D%20scene%20generation%2C%20our%20results%20show%0Athat%20DreamScene4D%20enables%20accurate%202D%20point%20motion%20tracking%20by%20projecting%20the%0Ainferred%203D%20trajectories%20to%202D%2C%20while%20never%20explicitly%20trained%20to%20do%20so.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamScene4D%253A%2520Dynamic%2520Multi-Object%2520Scene%2520Generation%2520from%2520Monocular%250A%2520%2520Videos%26entry.906535625%3DWen-Hsuan%2520Chu%2520and%2520Lei%2520Ke%2520and%2520Katerina%2520Fragkiadaki%26entry.1292438233%3D%2520%2520Existing%2520VLMs%2520can%2520track%2520in-the-wild%25202D%2520video%2520objects%2520while%2520current%2520generative%250Amodels%2520provide%2520powerful%2520visual%2520priors%2520for%2520synthesizing%2520novel%2520views%2520for%2520the%250Ahighly%2520under-constrained%25202D-to-3D%2520object%2520lifting.%2520Building%2520upon%2520this%2520exciting%250Aprogress%252C%2520we%2520present%2520DreamScene4D%252C%2520the%2520first%2520approach%2520that%2520can%2520generate%250Athree-dimensional%2520dynamic%2520scenes%2520of%2520multiple%2520objects%2520from%2520monocular%2520in-the-wild%250Avideos%2520with%2520large%2520object%2520motion%2520across%2520occlusions%2520and%2520novel%2520viewpoints.%2520Our%2520key%250Ainsight%2520is%2520to%2520design%2520a%2520%2522decompose-then-recompose%2522%2520scheme%2520to%2520factorize%2520both%2520the%250Awhole%2520video%2520scene%2520and%2520each%2520object%2527s%25203D%2520motion.%2520We%2520first%2520decompose%2520the%2520video%250Ascene%2520by%2520using%2520open-vocabulary%2520mask%2520trackers%2520and%2520an%2520adapted%2520image%2520diffusion%250Amodel%2520to%2520segment%252C%2520track%252C%2520and%2520amodally%2520complete%2520the%2520objects%2520and%2520background%2520in%250Athe%2520video.%2520Each%2520object%2520track%2520is%2520mapped%2520to%2520a%2520set%2520of%25203D%2520Gaussians%2520that%2520deform%2520and%250Amove%2520in%2520space%2520and%2520time.%2520We%2520also%2520factorize%2520the%2520observed%2520motion%2520into%2520multiple%250Acomponents%2520to%2520handle%2520fast%2520motion.%2520The%2520camera%2520motion%2520can%2520be%2520inferred%2520by%250Are-rendering%2520the%2520background%2520to%2520match%2520the%2520video%2520frames.%2520For%2520the%2520object%2520motion%252C%250Awe%2520first%2520model%2520the%2520object-centric%2520deformation%2520of%2520the%2520objects%2520by%2520leveraging%250Arendering%2520losses%2520and%2520multi-view%2520generative%2520priors%2520in%2520an%2520object-centric%2520frame%252C%250Athen%2520optimize%2520object-centric%2520to%2520world-frame%2520transformations%2520by%2520comparing%2520the%250Arendered%2520outputs%2520against%2520the%2520perceived%2520pixel%2520and%2520optical%2520flow.%2520Finally%252C%2520we%250Arecompose%2520the%2520background%2520and%2520objects%2520and%2520optimize%2520for%2520relative%2520object%2520scales%250Ausing%2520monocular%2520depth%2520prediction%2520guidance.%2520We%2520show%2520extensive%2520results%2520on%2520the%250Achallenging%2520DAVIS%252C%2520Kubric%252C%2520and%2520self-captured%2520videos%252C%2520detail%2520some%2520limitations%252C%250Aand%2520provide%2520future%2520directions.%2520Besides%25204D%2520scene%2520generation%252C%2520our%2520results%2520show%250Athat%2520DreamScene4D%2520enables%2520accurate%25202D%2520point%2520motion%2520tracking%2520by%2520projecting%2520the%250Ainferred%25203D%2520trajectories%2520to%25202D%252C%2520while%2520never%2520explicitly%2520trained%2520to%2520do%2520so.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamScene4D%3A%20Dynamic%20Multi-Object%20Scene%20Generation%20from%20Monocular%0A%20%20Videos&entry.906535625=Wen-Hsuan%20Chu%20and%20Lei%20Ke%20and%20Katerina%20Fragkiadaki&entry.1292438233=%20%20Existing%20VLMs%20can%20track%20in-the-wild%202D%20video%20objects%20while%20current%20generative%0Amodels%20provide%20powerful%20visual%20priors%20for%20synthesizing%20novel%20views%20for%20the%0Ahighly%20under-constrained%202D-to-3D%20object%20lifting.%20Building%20upon%20this%20exciting%0Aprogress%2C%20we%20present%20DreamScene4D%2C%20the%20first%20approach%20that%20can%20generate%0Athree-dimensional%20dynamic%20scenes%20of%20multiple%20objects%20from%20monocular%20in-the-wild%0Avideos%20with%20large%20object%20motion%20across%20occlusions%20and%20novel%20viewpoints.%20Our%20key%0Ainsight%20is%20to%20design%20a%20%22decompose-then-recompose%22%20scheme%20to%20factorize%20both%20the%0Awhole%20video%20scene%20and%20each%20object%27s%203D%20motion.%20We%20first%20decompose%20the%20video%0Ascene%20by%20using%20open-vocabulary%20mask%20trackers%20and%20an%20adapted%20image%20diffusion%0Amodel%20to%20segment%2C%20track%2C%20and%20amodally%20complete%20the%20objects%20and%20background%20in%0Athe%20video.%20Each%20object%20track%20is%20mapped%20to%20a%20set%20of%203D%20Gaussians%20that%20deform%20and%0Amove%20in%20space%20and%20time.%20We%20also%20factorize%20the%20observed%20motion%20into%20multiple%0Acomponents%20to%20handle%20fast%20motion.%20The%20camera%20motion%20can%20be%20inferred%20by%0Are-rendering%20the%20background%20to%20match%20the%20video%20frames.%20For%20the%20object%20motion%2C%0Awe%20first%20model%20the%20object-centric%20deformation%20of%20the%20objects%20by%20leveraging%0Arendering%20losses%20and%20multi-view%20generative%20priors%20in%20an%20object-centric%20frame%2C%0Athen%20optimize%20object-centric%20to%20world-frame%20transformations%20by%20comparing%20the%0Arendered%20outputs%20against%20the%20perceived%20pixel%20and%20optical%20flow.%20Finally%2C%20we%0Arecompose%20the%20background%20and%20objects%20and%20optimize%20for%20relative%20object%20scales%0Ausing%20monocular%20depth%20prediction%20guidance.%20We%20show%20extensive%20results%20on%20the%0Achallenging%20DAVIS%2C%20Kubric%2C%20and%20self-captured%20videos%2C%20detail%20some%20limitations%2C%0Aand%20provide%20future%20directions.%20Besides%204D%20scene%20generation%2C%20our%20results%20show%0Athat%20DreamScene4D%20enables%20accurate%202D%20point%20motion%20tracking%20by%20projecting%20the%0Ainferred%203D%20trajectories%20to%202D%2C%20while%20never%20explicitly%20trained%20to%20do%20so.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02280v1&entry.124074799=Read"},
{"title": "HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft\n  HoloLens 2", "author": "Miriam J\u00e4ger and Theodor Kapler and Michael Fe\u00dfenbecker and Felix Birkelbach and Markus Hillemann and Boris Jutzi", "abstract": "  In the fields of photogrammetry, computer vision and computer graphics, the\ntask of neural 3D scene reconstruction has led to the exploration of various\ntechniques. Among these, 3D Gaussian Splatting stands out for its explicit\nrepresentation of scenes using 3D Gaussians, making it appealing for tasks like\n3D point cloud extraction and surface reconstruction. Motivated by its\npotential, we address the domain of 3D scene reconstruction, aiming to leverage\nthe capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting.\nWe present HoloGS, a novel workflow utilizing HoloLens sensor data, which\nbypasses the need for pre-processing steps like Structure from Motion by\ninstantly accessing the required input data i.e. the images, camera poses and\nthe point cloud from depth sensing. We provide comprehensive investigations,\nincluding the training process and the rendering quality, assessed through the\nPeak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified\npoint cloud from Gaussian centers, measured by Chamfer Distance. We evaluate\nour approach on two self-captured scenes: An outdoor scene of a cultural\nheritage statue and an indoor scene of a fine-structured plant. Our results\nshow that the HoloLens data, including RGB images, corresponding camera poses,\nand depth sensing based point clouds to initialize the Gaussians, are suitable\nas input for 3D Gaussian Splatting.\n", "link": "http://arxiv.org/abs/2405.02005v1", "date": "2024-05-03", "relevancy": 3.0441, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6789}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6312}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoloGS%3A%20Instant%20Depth-based%203D%20Gaussian%20Splatting%20with%20Microsoft%0A%20%20HoloLens%202&body=Title%3A%20HoloGS%3A%20Instant%20Depth-based%203D%20Gaussian%20Splatting%20with%20Microsoft%0A%20%20HoloLens%202%0AAuthor%3A%20Miriam%20J%C3%A4ger%20and%20Theodor%20Kapler%20and%20Michael%20Fe%C3%9Fenbecker%20and%20Felix%20Birkelbach%20and%20Markus%20Hillemann%20and%20Boris%20Jutzi%0AAbstract%3A%20%20%20In%20the%20fields%20of%20photogrammetry%2C%20computer%20vision%20and%20computer%20graphics%2C%20the%0Atask%20of%20neural%203D%20scene%20reconstruction%20has%20led%20to%20the%20exploration%20of%20various%0Atechniques.%20Among%20these%2C%203D%20Gaussian%20Splatting%20stands%20out%20for%20its%20explicit%0Arepresentation%20of%20scenes%20using%203D%20Gaussians%2C%20making%20it%20appealing%20for%20tasks%20like%0A3D%20point%20cloud%20extraction%20and%20surface%20reconstruction.%20Motivated%20by%20its%0Apotential%2C%20we%20address%20the%20domain%20of%203D%20scene%20reconstruction%2C%20aiming%20to%20leverage%0Athe%20capabilities%20of%20the%20Microsoft%20HoloLens%202%20for%20instant%203D%20Gaussian%20Splatting.%0AWe%20present%20HoloGS%2C%20a%20novel%20workflow%20utilizing%20HoloLens%20sensor%20data%2C%20which%0Abypasses%20the%20need%20for%20pre-processing%20steps%20like%20Structure%20from%20Motion%20by%0Ainstantly%20accessing%20the%20required%20input%20data%20i.e.%20the%20images%2C%20camera%20poses%20and%0Athe%20point%20cloud%20from%20depth%20sensing.%20We%20provide%20comprehensive%20investigations%2C%0Aincluding%20the%20training%20process%20and%20the%20rendering%20quality%2C%20assessed%20through%20the%0APeak%20Signal-to-Noise%20Ratio%2C%20and%20the%20geometric%203D%20accuracy%20of%20the%20densified%0Apoint%20cloud%20from%20Gaussian%20centers%2C%20measured%20by%20Chamfer%20Distance.%20We%20evaluate%0Aour%20approach%20on%20two%20self-captured%20scenes%3A%20An%20outdoor%20scene%20of%20a%20cultural%0Aheritage%20statue%20and%20an%20indoor%20scene%20of%20a%20fine-structured%20plant.%20Our%20results%0Ashow%20that%20the%20HoloLens%20data%2C%20including%20RGB%20images%2C%20corresponding%20camera%20poses%2C%0Aand%20depth%20sensing%20based%20point%20clouds%20to%20initialize%20the%20Gaussians%2C%20are%20suitable%0Aas%20input%20for%203D%20Gaussian%20Splatting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloGS%253A%2520Instant%2520Depth-based%25203D%2520Gaussian%2520Splatting%2520with%2520Microsoft%250A%2520%2520HoloLens%25202%26entry.906535625%3DMiriam%2520J%25C3%25A4ger%2520and%2520Theodor%2520Kapler%2520and%2520Michael%2520Fe%25C3%259Fenbecker%2520and%2520Felix%2520Birkelbach%2520and%2520Markus%2520Hillemann%2520and%2520Boris%2520Jutzi%26entry.1292438233%3D%2520%2520In%2520the%2520fields%2520of%2520photogrammetry%252C%2520computer%2520vision%2520and%2520computer%2520graphics%252C%2520the%250Atask%2520of%2520neural%25203D%2520scene%2520reconstruction%2520has%2520led%2520to%2520the%2520exploration%2520of%2520various%250Atechniques.%2520Among%2520these%252C%25203D%2520Gaussian%2520Splatting%2520stands%2520out%2520for%2520its%2520explicit%250Arepresentation%2520of%2520scenes%2520using%25203D%2520Gaussians%252C%2520making%2520it%2520appealing%2520for%2520tasks%2520like%250A3D%2520point%2520cloud%2520extraction%2520and%2520surface%2520reconstruction.%2520Motivated%2520by%2520its%250Apotential%252C%2520we%2520address%2520the%2520domain%2520of%25203D%2520scene%2520reconstruction%252C%2520aiming%2520to%2520leverage%250Athe%2520capabilities%2520of%2520the%2520Microsoft%2520HoloLens%25202%2520for%2520instant%25203D%2520Gaussian%2520Splatting.%250AWe%2520present%2520HoloGS%252C%2520a%2520novel%2520workflow%2520utilizing%2520HoloLens%2520sensor%2520data%252C%2520which%250Abypasses%2520the%2520need%2520for%2520pre-processing%2520steps%2520like%2520Structure%2520from%2520Motion%2520by%250Ainstantly%2520accessing%2520the%2520required%2520input%2520data%2520i.e.%2520the%2520images%252C%2520camera%2520poses%2520and%250Athe%2520point%2520cloud%2520from%2520depth%2520sensing.%2520We%2520provide%2520comprehensive%2520investigations%252C%250Aincluding%2520the%2520training%2520process%2520and%2520the%2520rendering%2520quality%252C%2520assessed%2520through%2520the%250APeak%2520Signal-to-Noise%2520Ratio%252C%2520and%2520the%2520geometric%25203D%2520accuracy%2520of%2520the%2520densified%250Apoint%2520cloud%2520from%2520Gaussian%2520centers%252C%2520measured%2520by%2520Chamfer%2520Distance.%2520We%2520evaluate%250Aour%2520approach%2520on%2520two%2520self-captured%2520scenes%253A%2520An%2520outdoor%2520scene%2520of%2520a%2520cultural%250Aheritage%2520statue%2520and%2520an%2520indoor%2520scene%2520of%2520a%2520fine-structured%2520plant.%2520Our%2520results%250Ashow%2520that%2520the%2520HoloLens%2520data%252C%2520including%2520RGB%2520images%252C%2520corresponding%2520camera%2520poses%252C%250Aand%2520depth%2520sensing%2520based%2520point%2520clouds%2520to%2520initialize%2520the%2520Gaussians%252C%2520are%2520suitable%250Aas%2520input%2520for%25203D%2520Gaussian%2520Splatting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloGS%3A%20Instant%20Depth-based%203D%20Gaussian%20Splatting%20with%20Microsoft%0A%20%20HoloLens%202&entry.906535625=Miriam%20J%C3%A4ger%20and%20Theodor%20Kapler%20and%20Michael%20Fe%C3%9Fenbecker%20and%20Felix%20Birkelbach%20and%20Markus%20Hillemann%20and%20Boris%20Jutzi&entry.1292438233=%20%20In%20the%20fields%20of%20photogrammetry%2C%20computer%20vision%20and%20computer%20graphics%2C%20the%0Atask%20of%20neural%203D%20scene%20reconstruction%20has%20led%20to%20the%20exploration%20of%20various%0Atechniques.%20Among%20these%2C%203D%20Gaussian%20Splatting%20stands%20out%20for%20its%20explicit%0Arepresentation%20of%20scenes%20using%203D%20Gaussians%2C%20making%20it%20appealing%20for%20tasks%20like%0A3D%20point%20cloud%20extraction%20and%20surface%20reconstruction.%20Motivated%20by%20its%0Apotential%2C%20we%20address%20the%20domain%20of%203D%20scene%20reconstruction%2C%20aiming%20to%20leverage%0Athe%20capabilities%20of%20the%20Microsoft%20HoloLens%202%20for%20instant%203D%20Gaussian%20Splatting.%0AWe%20present%20HoloGS%2C%20a%20novel%20workflow%20utilizing%20HoloLens%20sensor%20data%2C%20which%0Abypasses%20the%20need%20for%20pre-processing%20steps%20like%20Structure%20from%20Motion%20by%0Ainstantly%20accessing%20the%20required%20input%20data%20i.e.%20the%20images%2C%20camera%20poses%20and%0Athe%20point%20cloud%20from%20depth%20sensing.%20We%20provide%20comprehensive%20investigations%2C%0Aincluding%20the%20training%20process%20and%20the%20rendering%20quality%2C%20assessed%20through%20the%0APeak%20Signal-to-Noise%20Ratio%2C%20and%20the%20geometric%203D%20accuracy%20of%20the%20densified%0Apoint%20cloud%20from%20Gaussian%20centers%2C%20measured%20by%20Chamfer%20Distance.%20We%20evaluate%0Aour%20approach%20on%20two%20self-captured%20scenes%3A%20An%20outdoor%20scene%20of%20a%20cultural%0Aheritage%20statue%20and%20an%20indoor%20scene%20of%20a%20fine-structured%20plant.%20Our%20results%0Ashow%20that%20the%20HoloLens%20data%2C%20including%20RGB%20images%2C%20corresponding%20camera%20poses%2C%0Aand%20depth%20sensing%20based%20point%20clouds%20to%20initialize%20the%20Gaussians%2C%20are%20suitable%0Aas%20input%20for%203D%20Gaussian%20Splatting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02005v1&entry.124074799=Read"},
{"title": "TULIP: Transformer for Upsampling of LiDAR Point Clouds", "author": "Bin Yang and Patrick Pfreundschuh and Roland Siegwart and Marco Hutter and Peyman Moghadam and Vaishakh Patil", "abstract": "  LiDAR Upsampling is a challenging task for the perception systems of robots\nand autonomous vehicles, due to the sparse and irregular structure of\nlarge-scale scene contexts. Recent works propose to solve this problem by\nconverting LiDAR data from 3D Euclidean space into an image super-resolution\nproblem in 2D image space. Although their methods can generate high-resolution\nrange images with fine-grained details, the resulting 3D point clouds often\nblur out details and predict invalid points. In this paper, we propose TULIP, a\nnew method to reconstruct high-resolution LiDAR point clouds from\nlow-resolution LiDAR input. We also follow a range image-based approach but\nspecifically modify the patch and window geometries of a Swin-Transformer-based\nnetwork to better fit the characteristics of range images. We conducted several\nexperiments on three public real-world and simulated datasets. TULIP\noutperforms state-of-the-art methods in all relevant metrics and generates\nrobust and more realistic point clouds than prior works.\n", "link": "http://arxiv.org/abs/2312.06733v4", "date": "2024-05-03", "relevancy": 2.7644, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5712}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TULIP%3A%20Transformer%20for%20Upsampling%20of%20LiDAR%20Point%20Clouds&body=Title%3A%20TULIP%3A%20Transformer%20for%20Upsampling%20of%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Bin%20Yang%20and%20Patrick%20Pfreundschuh%20and%20Roland%20Siegwart%20and%20Marco%20Hutter%20and%20Peyman%20Moghadam%20and%20Vaishakh%20Patil%0AAbstract%3A%20%20%20LiDAR%20Upsampling%20is%20a%20challenging%20task%20for%20the%20perception%20systems%20of%20robots%0Aand%20autonomous%20vehicles%2C%20due%20to%20the%20sparse%20and%20irregular%20structure%20of%0Alarge-scale%20scene%20contexts.%20Recent%20works%20propose%20to%20solve%20this%20problem%20by%0Aconverting%20LiDAR%20data%20from%203D%20Euclidean%20space%20into%20an%20image%20super-resolution%0Aproblem%20in%202D%20image%20space.%20Although%20their%20methods%20can%20generate%20high-resolution%0Arange%20images%20with%20fine-grained%20details%2C%20the%20resulting%203D%20point%20clouds%20often%0Ablur%20out%20details%20and%20predict%20invalid%20points.%20In%20this%20paper%2C%20we%20propose%20TULIP%2C%20a%0Anew%20method%20to%20reconstruct%20high-resolution%20LiDAR%20point%20clouds%20from%0Alow-resolution%20LiDAR%20input.%20We%20also%20follow%20a%20range%20image-based%20approach%20but%0Aspecifically%20modify%20the%20patch%20and%20window%20geometries%20of%20a%20Swin-Transformer-based%0Anetwork%20to%20better%20fit%20the%20characteristics%20of%20range%20images.%20We%20conducted%20several%0Aexperiments%20on%20three%20public%20real-world%20and%20simulated%20datasets.%20TULIP%0Aoutperforms%20state-of-the-art%20methods%20in%20all%20relevant%20metrics%20and%20generates%0Arobust%20and%20more%20realistic%20point%20clouds%20than%20prior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06733v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTULIP%253A%2520Transformer%2520for%2520Upsampling%2520of%2520LiDAR%2520Point%2520Clouds%26entry.906535625%3DBin%2520Yang%2520and%2520Patrick%2520Pfreundschuh%2520and%2520Roland%2520Siegwart%2520and%2520Marco%2520Hutter%2520and%2520Peyman%2520Moghadam%2520and%2520Vaishakh%2520Patil%26entry.1292438233%3D%2520%2520LiDAR%2520Upsampling%2520is%2520a%2520challenging%2520task%2520for%2520the%2520perception%2520systems%2520of%2520robots%250Aand%2520autonomous%2520vehicles%252C%2520due%2520to%2520the%2520sparse%2520and%2520irregular%2520structure%2520of%250Alarge-scale%2520scene%2520contexts.%2520Recent%2520works%2520propose%2520to%2520solve%2520this%2520problem%2520by%250Aconverting%2520LiDAR%2520data%2520from%25203D%2520Euclidean%2520space%2520into%2520an%2520image%2520super-resolution%250Aproblem%2520in%25202D%2520image%2520space.%2520Although%2520their%2520methods%2520can%2520generate%2520high-resolution%250Arange%2520images%2520with%2520fine-grained%2520details%252C%2520the%2520resulting%25203D%2520point%2520clouds%2520often%250Ablur%2520out%2520details%2520and%2520predict%2520invalid%2520points.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TULIP%252C%2520a%250Anew%2520method%2520to%2520reconstruct%2520high-resolution%2520LiDAR%2520point%2520clouds%2520from%250Alow-resolution%2520LiDAR%2520input.%2520We%2520also%2520follow%2520a%2520range%2520image-based%2520approach%2520but%250Aspecifically%2520modify%2520the%2520patch%2520and%2520window%2520geometries%2520of%2520a%2520Swin-Transformer-based%250Anetwork%2520to%2520better%2520fit%2520the%2520characteristics%2520of%2520range%2520images.%2520We%2520conducted%2520several%250Aexperiments%2520on%2520three%2520public%2520real-world%2520and%2520simulated%2520datasets.%2520TULIP%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520all%2520relevant%2520metrics%2520and%2520generates%250Arobust%2520and%2520more%2520realistic%2520point%2520clouds%2520than%2520prior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06733v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TULIP%3A%20Transformer%20for%20Upsampling%20of%20LiDAR%20Point%20Clouds&entry.906535625=Bin%20Yang%20and%20Patrick%20Pfreundschuh%20and%20Roland%20Siegwart%20and%20Marco%20Hutter%20and%20Peyman%20Moghadam%20and%20Vaishakh%20Patil&entry.1292438233=%20%20LiDAR%20Upsampling%20is%20a%20challenging%20task%20for%20the%20perception%20systems%20of%20robots%0Aand%20autonomous%20vehicles%2C%20due%20to%20the%20sparse%20and%20irregular%20structure%20of%0Alarge-scale%20scene%20contexts.%20Recent%20works%20propose%20to%20solve%20this%20problem%20by%0Aconverting%20LiDAR%20data%20from%203D%20Euclidean%20space%20into%20an%20image%20super-resolution%0Aproblem%20in%202D%20image%20space.%20Although%20their%20methods%20can%20generate%20high-resolution%0Arange%20images%20with%20fine-grained%20details%2C%20the%20resulting%203D%20point%20clouds%20often%0Ablur%20out%20details%20and%20predict%20invalid%20points.%20In%20this%20paper%2C%20we%20propose%20TULIP%2C%20a%0Anew%20method%20to%20reconstruct%20high-resolution%20LiDAR%20point%20clouds%20from%0Alow-resolution%20LiDAR%20input.%20We%20also%20follow%20a%20range%20image-based%20approach%20but%0Aspecifically%20modify%20the%20patch%20and%20window%20geometries%20of%20a%20Swin-Transformer-based%0Anetwork%20to%20better%20fit%20the%20characteristics%20of%20range%20images.%20We%20conducted%20several%0Aexperiments%20on%20three%20public%20real-world%20and%20simulated%20datasets.%20TULIP%0Aoutperforms%20state-of-the-art%20methods%20in%20all%20relevant%20metrics%20and%20generates%0Arobust%20and%20more%20realistic%20point%20clouds%20than%20prior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06733v4&entry.124074799=Read"},
{"title": "Automated National Urban Map Extraction", "author": "Hasan Nasrallah and Abed Ellatif Samhat and Cristiano Nattero and Ali J. Ghandour", "abstract": "  Developing countries usually lack the proper governance means to generate and\nregularly update a national rooftop map. Using traditional photogrammetry and\nsurveying methods to produce a building map at the federal level is costly and\ntime consuming. Using earth observation and deep learning methods, we can\nbridge this gap and propose an automated pipeline to fetch such national urban\nmaps. This paper aims to exploit the power of fully convolutional neural\nnetworks for multi-class buildings' instance segmentation to leverage high\nobject-wise accuracy results. Buildings' instance segmentation from sub-meter\nhigh-resolution satellite images can be achieved with relatively high\npixel-wise metric scores. We detail all engineering steps to replicate this\nwork and ensure highly accurate results in dense and slum areas witnessed in\nregions that lack proper urban planning in the Global South. We applied a case\nstudy of the proposed pipeline to Lebanon and successfully produced the first\ncomprehensive national building footprint map with approximately 1 Million\nunits with an 84% accuracy. The proposed architecture relies on advanced\naugmentation techniques to overcome dataset scarcity, which is often the case\nin developing countries.\n", "link": "http://arxiv.org/abs/2404.06202v2", "date": "2024-05-03", "relevancy": 2.7304, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6047}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5243}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20National%20Urban%20Map%20Extraction&body=Title%3A%20Automated%20National%20Urban%20Map%20Extraction%0AAuthor%3A%20Hasan%20Nasrallah%20and%20Abed%20Ellatif%20Samhat%20and%20Cristiano%20Nattero%20and%20Ali%20J.%20Ghandour%0AAbstract%3A%20%20%20Developing%20countries%20usually%20lack%20the%20proper%20governance%20means%20to%20generate%20and%0Aregularly%20update%20a%20national%20rooftop%20map.%20Using%20traditional%20photogrammetry%20and%0Asurveying%20methods%20to%20produce%20a%20building%20map%20at%20the%20federal%20level%20is%20costly%20and%0Atime%20consuming.%20Using%20earth%20observation%20and%20deep%20learning%20methods%2C%20we%20can%0Abridge%20this%20gap%20and%20propose%20an%20automated%20pipeline%20to%20fetch%20such%20national%20urban%0Amaps.%20This%20paper%20aims%20to%20exploit%20the%20power%20of%20fully%20convolutional%20neural%0Anetworks%20for%20multi-class%20buildings%27%20instance%20segmentation%20to%20leverage%20high%0Aobject-wise%20accuracy%20results.%20Buildings%27%20instance%20segmentation%20from%20sub-meter%0Ahigh-resolution%20satellite%20images%20can%20be%20achieved%20with%20relatively%20high%0Apixel-wise%20metric%20scores.%20We%20detail%20all%20engineering%20steps%20to%20replicate%20this%0Awork%20and%20ensure%20highly%20accurate%20results%20in%20dense%20and%20slum%20areas%20witnessed%20in%0Aregions%20that%20lack%20proper%20urban%20planning%20in%20the%20Global%20South.%20We%20applied%20a%20case%0Astudy%20of%20the%20proposed%20pipeline%20to%20Lebanon%20and%20successfully%20produced%20the%20first%0Acomprehensive%20national%20building%20footprint%20map%20with%20approximately%201%20Million%0Aunits%20with%20an%2084%25%20accuracy.%20The%20proposed%20architecture%20relies%20on%20advanced%0Aaugmentation%20techniques%20to%20overcome%20dataset%20scarcity%2C%20which%20is%20often%20the%20case%0Ain%20developing%20countries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520National%2520Urban%2520Map%2520Extraction%26entry.906535625%3DHasan%2520Nasrallah%2520and%2520Abed%2520Ellatif%2520Samhat%2520and%2520Cristiano%2520Nattero%2520and%2520Ali%2520J.%2520Ghandour%26entry.1292438233%3D%2520%2520Developing%2520countries%2520usually%2520lack%2520the%2520proper%2520governance%2520means%2520to%2520generate%2520and%250Aregularly%2520update%2520a%2520national%2520rooftop%2520map.%2520Using%2520traditional%2520photogrammetry%2520and%250Asurveying%2520methods%2520to%2520produce%2520a%2520building%2520map%2520at%2520the%2520federal%2520level%2520is%2520costly%2520and%250Atime%2520consuming.%2520Using%2520earth%2520observation%2520and%2520deep%2520learning%2520methods%252C%2520we%2520can%250Abridge%2520this%2520gap%2520and%2520propose%2520an%2520automated%2520pipeline%2520to%2520fetch%2520such%2520national%2520urban%250Amaps.%2520This%2520paper%2520aims%2520to%2520exploit%2520the%2520power%2520of%2520fully%2520convolutional%2520neural%250Anetworks%2520for%2520multi-class%2520buildings%2527%2520instance%2520segmentation%2520to%2520leverage%2520high%250Aobject-wise%2520accuracy%2520results.%2520Buildings%2527%2520instance%2520segmentation%2520from%2520sub-meter%250Ahigh-resolution%2520satellite%2520images%2520can%2520be%2520achieved%2520with%2520relatively%2520high%250Apixel-wise%2520metric%2520scores.%2520We%2520detail%2520all%2520engineering%2520steps%2520to%2520replicate%2520this%250Awork%2520and%2520ensure%2520highly%2520accurate%2520results%2520in%2520dense%2520and%2520slum%2520areas%2520witnessed%2520in%250Aregions%2520that%2520lack%2520proper%2520urban%2520planning%2520in%2520the%2520Global%2520South.%2520We%2520applied%2520a%2520case%250Astudy%2520of%2520the%2520proposed%2520pipeline%2520to%2520Lebanon%2520and%2520successfully%2520produced%2520the%2520first%250Acomprehensive%2520national%2520building%2520footprint%2520map%2520with%2520approximately%25201%2520Million%250Aunits%2520with%2520an%252084%2525%2520accuracy.%2520The%2520proposed%2520architecture%2520relies%2520on%2520advanced%250Aaugmentation%2520techniques%2520to%2520overcome%2520dataset%2520scarcity%252C%2520which%2520is%2520often%2520the%2520case%250Ain%2520developing%2520countries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20National%20Urban%20Map%20Extraction&entry.906535625=Hasan%20Nasrallah%20and%20Abed%20Ellatif%20Samhat%20and%20Cristiano%20Nattero%20and%20Ali%20J.%20Ghandour&entry.1292438233=%20%20Developing%20countries%20usually%20lack%20the%20proper%20governance%20means%20to%20generate%20and%0Aregularly%20update%20a%20national%20rooftop%20map.%20Using%20traditional%20photogrammetry%20and%0Asurveying%20methods%20to%20produce%20a%20building%20map%20at%20the%20federal%20level%20is%20costly%20and%0Atime%20consuming.%20Using%20earth%20observation%20and%20deep%20learning%20methods%2C%20we%20can%0Abridge%20this%20gap%20and%20propose%20an%20automated%20pipeline%20to%20fetch%20such%20national%20urban%0Amaps.%20This%20paper%20aims%20to%20exploit%20the%20power%20of%20fully%20convolutional%20neural%0Anetworks%20for%20multi-class%20buildings%27%20instance%20segmentation%20to%20leverage%20high%0Aobject-wise%20accuracy%20results.%20Buildings%27%20instance%20segmentation%20from%20sub-meter%0Ahigh-resolution%20satellite%20images%20can%20be%20achieved%20with%20relatively%20high%0Apixel-wise%20metric%20scores.%20We%20detail%20all%20engineering%20steps%20to%20replicate%20this%0Awork%20and%20ensure%20highly%20accurate%20results%20in%20dense%20and%20slum%20areas%20witnessed%20in%0Aregions%20that%20lack%20proper%20urban%20planning%20in%20the%20Global%20South.%20We%20applied%20a%20case%0Astudy%20of%20the%20proposed%20pipeline%20to%20Lebanon%20and%20successfully%20produced%20the%20first%0Acomprehensive%20national%20building%20footprint%20map%20with%20approximately%201%20Million%0Aunits%20with%20an%2084%25%20accuracy.%20The%20proposed%20architecture%20relies%20on%20advanced%0Aaugmentation%20techniques%20to%20overcome%20dataset%20scarcity%2C%20which%20is%20often%20the%20case%0Ain%20developing%20countries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06202v2&entry.124074799=Read"},
{"title": "Convex Combination Consistency between Neighbors for Weakly-supervised\n  Action Localization", "author": "Qinying Liu and Zilei Wang and Ruoxi Chen and Zhilin Li", "abstract": "  Weakly-supervised temporal action localization (WTAL) intends to detect\naction instances with only weak supervision, e.g., video-level labels. The\ncurrent~\\textit{de facto} pipeline locates action instances by thresholding and\ngrouping continuous high-score regions on temporal class activation sequences.\nIn this route, the capacity of the model to recognize the relationships between\nadjacent snippets is of vital importance which determines the quality of the\naction boundaries. However, it is error-prone since the variations between\nadjacent snippets are typically subtle, and unfortunately this is overlooked in\nthe literature. To tackle the issue, we propose a novel WTAL approach named\nConvex Combination Consistency between Neighbors (C$^3$BN). C$^3$BN consists of\ntwo key ingredients: a micro data augmentation strategy that increases the\ndiversity in-between adjacent snippets by convex combination of adjacent\nsnippets, and a macro-micro consistency regularization that enforces the model\nto be invariant to the transformations~\\textit{w.r.t.} video semantics, snippet\npredictions, and snippet representations. Consequently, fine-grained patterns\nin-between adjacent snippets are enforced to be explored, thereby resulting in\na more robust action boundary localization. Experimental results demonstrate\nthe effectiveness of C$^3$BN on top of various baselines for WTAL with\nvideo-level and point-level supervisions. Code is at\nhttps://github.com/Qinying-Liu/C3BN.\n", "link": "http://arxiv.org/abs/2205.00400v3", "date": "2024-05-03", "relevancy": 2.7278, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.571}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5392}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convex%20Combination%20Consistency%20between%20Neighbors%20for%20Weakly-supervised%0A%20%20Action%20Localization&body=Title%3A%20Convex%20Combination%20Consistency%20between%20Neighbors%20for%20Weakly-supervised%0A%20%20Action%20Localization%0AAuthor%3A%20Qinying%20Liu%20and%20Zilei%20Wang%20and%20Ruoxi%20Chen%20and%20Zhilin%20Li%0AAbstract%3A%20%20%20Weakly-supervised%20temporal%20action%20localization%20%28WTAL%29%20intends%20to%20detect%0Aaction%20instances%20with%20only%20weak%20supervision%2C%20e.g.%2C%20video-level%20labels.%20The%0Acurrent~%5Ctextit%7Bde%20facto%7D%20pipeline%20locates%20action%20instances%20by%20thresholding%20and%0Agrouping%20continuous%20high-score%20regions%20on%20temporal%20class%20activation%20sequences.%0AIn%20this%20route%2C%20the%20capacity%20of%20the%20model%20to%20recognize%20the%20relationships%20between%0Aadjacent%20snippets%20is%20of%20vital%20importance%20which%20determines%20the%20quality%20of%20the%0Aaction%20boundaries.%20However%2C%20it%20is%20error-prone%20since%20the%20variations%20between%0Aadjacent%20snippets%20are%20typically%20subtle%2C%20and%20unfortunately%20this%20is%20overlooked%20in%0Athe%20literature.%20To%20tackle%20the%20issue%2C%20we%20propose%20a%20novel%20WTAL%20approach%20named%0AConvex%20Combination%20Consistency%20between%20Neighbors%20%28C%24%5E3%24BN%29.%20C%24%5E3%24BN%20consists%20of%0Atwo%20key%20ingredients%3A%20a%20micro%20data%20augmentation%20strategy%20that%20increases%20the%0Adiversity%20in-between%20adjacent%20snippets%20by%20convex%20combination%20of%20adjacent%0Asnippets%2C%20and%20a%20macro-micro%20consistency%20regularization%20that%20enforces%20the%20model%0Ato%20be%20invariant%20to%20the%20transformations~%5Ctextit%7Bw.r.t.%7D%20video%20semantics%2C%20snippet%0Apredictions%2C%20and%20snippet%20representations.%20Consequently%2C%20fine-grained%20patterns%0Ain-between%20adjacent%20snippets%20are%20enforced%20to%20be%20explored%2C%20thereby%20resulting%20in%0Aa%20more%20robust%20action%20boundary%20localization.%20Experimental%20results%20demonstrate%0Athe%20effectiveness%20of%20C%24%5E3%24BN%20on%20top%20of%20various%20baselines%20for%20WTAL%20with%0Avideo-level%20and%20point-level%20supervisions.%20Code%20is%20at%0Ahttps%3A//github.com/Qinying-Liu/C3BN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.00400v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvex%2520Combination%2520Consistency%2520between%2520Neighbors%2520for%2520Weakly-supervised%250A%2520%2520Action%2520Localization%26entry.906535625%3DQinying%2520Liu%2520and%2520Zilei%2520Wang%2520and%2520Ruoxi%2520Chen%2520and%2520Zhilin%2520Li%26entry.1292438233%3D%2520%2520Weakly-supervised%2520temporal%2520action%2520localization%2520%2528WTAL%2529%2520intends%2520to%2520detect%250Aaction%2520instances%2520with%2520only%2520weak%2520supervision%252C%2520e.g.%252C%2520video-level%2520labels.%2520The%250Acurrent~%255Ctextit%257Bde%2520facto%257D%2520pipeline%2520locates%2520action%2520instances%2520by%2520thresholding%2520and%250Agrouping%2520continuous%2520high-score%2520regions%2520on%2520temporal%2520class%2520activation%2520sequences.%250AIn%2520this%2520route%252C%2520the%2520capacity%2520of%2520the%2520model%2520to%2520recognize%2520the%2520relationships%2520between%250Aadjacent%2520snippets%2520is%2520of%2520vital%2520importance%2520which%2520determines%2520the%2520quality%2520of%2520the%250Aaction%2520boundaries.%2520However%252C%2520it%2520is%2520error-prone%2520since%2520the%2520variations%2520between%250Aadjacent%2520snippets%2520are%2520typically%2520subtle%252C%2520and%2520unfortunately%2520this%2520is%2520overlooked%2520in%250Athe%2520literature.%2520To%2520tackle%2520the%2520issue%252C%2520we%2520propose%2520a%2520novel%2520WTAL%2520approach%2520named%250AConvex%2520Combination%2520Consistency%2520between%2520Neighbors%2520%2528C%2524%255E3%2524BN%2529.%2520C%2524%255E3%2524BN%2520consists%2520of%250Atwo%2520key%2520ingredients%253A%2520a%2520micro%2520data%2520augmentation%2520strategy%2520that%2520increases%2520the%250Adiversity%2520in-between%2520adjacent%2520snippets%2520by%2520convex%2520combination%2520of%2520adjacent%250Asnippets%252C%2520and%2520a%2520macro-micro%2520consistency%2520regularization%2520that%2520enforces%2520the%2520model%250Ato%2520be%2520invariant%2520to%2520the%2520transformations~%255Ctextit%257Bw.r.t.%257D%2520video%2520semantics%252C%2520snippet%250Apredictions%252C%2520and%2520snippet%2520representations.%2520Consequently%252C%2520fine-grained%2520patterns%250Ain-between%2520adjacent%2520snippets%2520are%2520enforced%2520to%2520be%2520explored%252C%2520thereby%2520resulting%2520in%250Aa%2520more%2520robust%2520action%2520boundary%2520localization.%2520Experimental%2520results%2520demonstrate%250Athe%2520effectiveness%2520of%2520C%2524%255E3%2524BN%2520on%2520top%2520of%2520various%2520baselines%2520for%2520WTAL%2520with%250Avideo-level%2520and%2520point-level%2520supervisions.%2520Code%2520is%2520at%250Ahttps%253A//github.com/Qinying-Liu/C3BN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.00400v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convex%20Combination%20Consistency%20between%20Neighbors%20for%20Weakly-supervised%0A%20%20Action%20Localization&entry.906535625=Qinying%20Liu%20and%20Zilei%20Wang%20and%20Ruoxi%20Chen%20and%20Zhilin%20Li&entry.1292438233=%20%20Weakly-supervised%20temporal%20action%20localization%20%28WTAL%29%20intends%20to%20detect%0Aaction%20instances%20with%20only%20weak%20supervision%2C%20e.g.%2C%20video-level%20labels.%20The%0Acurrent~%5Ctextit%7Bde%20facto%7D%20pipeline%20locates%20action%20instances%20by%20thresholding%20and%0Agrouping%20continuous%20high-score%20regions%20on%20temporal%20class%20activation%20sequences.%0AIn%20this%20route%2C%20the%20capacity%20of%20the%20model%20to%20recognize%20the%20relationships%20between%0Aadjacent%20snippets%20is%20of%20vital%20importance%20which%20determines%20the%20quality%20of%20the%0Aaction%20boundaries.%20However%2C%20it%20is%20error-prone%20since%20the%20variations%20between%0Aadjacent%20snippets%20are%20typically%20subtle%2C%20and%20unfortunately%20this%20is%20overlooked%20in%0Athe%20literature.%20To%20tackle%20the%20issue%2C%20we%20propose%20a%20novel%20WTAL%20approach%20named%0AConvex%20Combination%20Consistency%20between%20Neighbors%20%28C%24%5E3%24BN%29.%20C%24%5E3%24BN%20consists%20of%0Atwo%20key%20ingredients%3A%20a%20micro%20data%20augmentation%20strategy%20that%20increases%20the%0Adiversity%20in-between%20adjacent%20snippets%20by%20convex%20combination%20of%20adjacent%0Asnippets%2C%20and%20a%20macro-micro%20consistency%20regularization%20that%20enforces%20the%20model%0Ato%20be%20invariant%20to%20the%20transformations~%5Ctextit%7Bw.r.t.%7D%20video%20semantics%2C%20snippet%0Apredictions%2C%20and%20snippet%20representations.%20Consequently%2C%20fine-grained%20patterns%0Ain-between%20adjacent%20snippets%20are%20enforced%20to%20be%20explored%2C%20thereby%20resulting%20in%0Aa%20more%20robust%20action%20boundary%20localization.%20Experimental%20results%20demonstrate%0Athe%20effectiveness%20of%20C%24%5E3%24BN%20on%20top%20of%20various%20baselines%20for%20WTAL%20with%0Avideo-level%20and%20point-level%20supervisions.%20Code%20is%20at%0Ahttps%3A//github.com/Qinying-Liu/C3BN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.00400v3&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and\n  Multiple Zoomed Observations", "author": "Zhilu Zhang and Ruohao Wang and Hongzhi Zhang and Wangmeng Zuo", "abstract": "  In this paper, we consider two challenging issues in reference-based\nsuper-resolution (RefSR) for smartphone, (i) how to choose a proper reference\nimage, and (ii) how to learn RefSR in a self-supervised manner. Particularly,\nwe propose a novel self-supervised learning approach for real-world RefSR from\nobservations at dual and multiple camera zooms. Firstly, considering the\npopularity of multiple cameras in modern smartphones, the more zoomed\n(telephoto) image can be naturally leveraged as the reference to guide the\nsuper-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a\nchance to learn a deep network that performs SR from the dual zoomed\nobservations (DZSR). Secondly, for self-supervised learning of DZSR, we take\nthe telephoto image instead of an additional high-resolution image as the\nsupervision information, and select a center patch from it as the reference to\nsuper-resolve the corresponding ultra-wide image patch. To mitigate the effect\nof the misalignment between ultra-wide low-resolution (LR) patch and telephoto\nground-truth (GT) image during training, we first adopt patch-based optical\nflow alignment and then design an auxiliary-LR to guide the deforming of the\nwarped LR features. To generate visually pleasing results, we present local\noverlapped sliced Wasserstein loss to better represent the perceptual\ndifference between GT and output in the feature space. During testing, DZSR can\nbe directly deployed to super-solve the whole ultra-wide image with the\nreference of the telephoto image. In addition, we further take multiple zoomed\nobservations to explore self-supervised RefSR, and present a progressive fusion\nscheme for the effective utilization of reference images. Experiments show that\nour methods achieve better quantitative and qualitative performance against\nstate-of-the-arts. Codes are available at\nhttps://github.com/cszhilu1998/SelfDZSR_PlusPlus.\n", "link": "http://arxiv.org/abs/2405.02171v1", "date": "2024-05-03", "relevancy": 2.6614, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5395}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Real-World%20Super-Resolution%20from%20Dual%20and%0A%20%20Multiple%20Zoomed%20Observations&body=Title%3A%20Self-Supervised%20Learning%20for%20Real-World%20Super-Resolution%20from%20Dual%20and%0A%20%20Multiple%20Zoomed%20Observations%0AAuthor%3A%20Zhilu%20Zhang%20and%20Ruohao%20Wang%20and%20Hongzhi%20Zhang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20two%20challenging%20issues%20in%20reference-based%0Asuper-resolution%20%28RefSR%29%20for%20smartphone%2C%20%28i%29%20how%20to%20choose%20a%20proper%20reference%0Aimage%2C%20and%20%28ii%29%20how%20to%20learn%20RefSR%20in%20a%20self-supervised%20manner.%20Particularly%2C%0Awe%20propose%20a%20novel%20self-supervised%20learning%20approach%20for%20real-world%20RefSR%20from%0Aobservations%20at%20dual%20and%20multiple%20camera%20zooms.%20Firstly%2C%20considering%20the%0Apopularity%20of%20multiple%20cameras%20in%20modern%20smartphones%2C%20the%20more%20zoomed%0A%28telephoto%29%20image%20can%20be%20naturally%20leveraged%20as%20the%20reference%20to%20guide%20the%0Asuper-resolution%20%28SR%29%20of%20the%20lesser%20zoomed%20%28ultra-wide%29%20image%2C%20which%20gives%20us%20a%0Achance%20to%20learn%20a%20deep%20network%20that%20performs%20SR%20from%20the%20dual%20zoomed%0Aobservations%20%28DZSR%29.%20Secondly%2C%20for%20self-supervised%20learning%20of%20DZSR%2C%20we%20take%0Athe%20telephoto%20image%20instead%20of%20an%20additional%20high-resolution%20image%20as%20the%0Asupervision%20information%2C%20and%20select%20a%20center%20patch%20from%20it%20as%20the%20reference%20to%0Asuper-resolve%20the%20corresponding%20ultra-wide%20image%20patch.%20To%20mitigate%20the%20effect%0Aof%20the%20misalignment%20between%20ultra-wide%20low-resolution%20%28LR%29%20patch%20and%20telephoto%0Aground-truth%20%28GT%29%20image%20during%20training%2C%20we%20first%20adopt%20patch-based%20optical%0Aflow%20alignment%20and%20then%20design%20an%20auxiliary-LR%20to%20guide%20the%20deforming%20of%20the%0Awarped%20LR%20features.%20To%20generate%20visually%20pleasing%20results%2C%20we%20present%20local%0Aoverlapped%20sliced%20Wasserstein%20loss%20to%20better%20represent%20the%20perceptual%0Adifference%20between%20GT%20and%20output%20in%20the%20feature%20space.%20During%20testing%2C%20DZSR%20can%0Abe%20directly%20deployed%20to%20super-solve%20the%20whole%20ultra-wide%20image%20with%20the%0Areference%20of%20the%20telephoto%20image.%20In%20addition%2C%20we%20further%20take%20multiple%20zoomed%0Aobservations%20to%20explore%20self-supervised%20RefSR%2C%20and%20present%20a%20progressive%20fusion%0Ascheme%20for%20the%20effective%20utilization%20of%20reference%20images.%20Experiments%20show%20that%0Aour%20methods%20achieve%20better%20quantitative%20and%20qualitative%20performance%20against%0Astate-of-the-arts.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/cszhilu1998/SelfDZSR_PlusPlus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520for%2520Real-World%2520Super-Resolution%2520from%2520Dual%2520and%250A%2520%2520Multiple%2520Zoomed%2520Observations%26entry.906535625%3DZhilu%2520Zhang%2520and%2520Ruohao%2520Wang%2520and%2520Hongzhi%2520Zhang%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520two%2520challenging%2520issues%2520in%2520reference-based%250Asuper-resolution%2520%2528RefSR%2529%2520for%2520smartphone%252C%2520%2528i%2529%2520how%2520to%2520choose%2520a%2520proper%2520reference%250Aimage%252C%2520and%2520%2528ii%2529%2520how%2520to%2520learn%2520RefSR%2520in%2520a%2520self-supervised%2520manner.%2520Particularly%252C%250Awe%2520propose%2520a%2520novel%2520self-supervised%2520learning%2520approach%2520for%2520real-world%2520RefSR%2520from%250Aobservations%2520at%2520dual%2520and%2520multiple%2520camera%2520zooms.%2520Firstly%252C%2520considering%2520the%250Apopularity%2520of%2520multiple%2520cameras%2520in%2520modern%2520smartphones%252C%2520the%2520more%2520zoomed%250A%2528telephoto%2529%2520image%2520can%2520be%2520naturally%2520leveraged%2520as%2520the%2520reference%2520to%2520guide%2520the%250Asuper-resolution%2520%2528SR%2529%2520of%2520the%2520lesser%2520zoomed%2520%2528ultra-wide%2529%2520image%252C%2520which%2520gives%2520us%2520a%250Achance%2520to%2520learn%2520a%2520deep%2520network%2520that%2520performs%2520SR%2520from%2520the%2520dual%2520zoomed%250Aobservations%2520%2528DZSR%2529.%2520Secondly%252C%2520for%2520self-supervised%2520learning%2520of%2520DZSR%252C%2520we%2520take%250Athe%2520telephoto%2520image%2520instead%2520of%2520an%2520additional%2520high-resolution%2520image%2520as%2520the%250Asupervision%2520information%252C%2520and%2520select%2520a%2520center%2520patch%2520from%2520it%2520as%2520the%2520reference%2520to%250Asuper-resolve%2520the%2520corresponding%2520ultra-wide%2520image%2520patch.%2520To%2520mitigate%2520the%2520effect%250Aof%2520the%2520misalignment%2520between%2520ultra-wide%2520low-resolution%2520%2528LR%2529%2520patch%2520and%2520telephoto%250Aground-truth%2520%2528GT%2529%2520image%2520during%2520training%252C%2520we%2520first%2520adopt%2520patch-based%2520optical%250Aflow%2520alignment%2520and%2520then%2520design%2520an%2520auxiliary-LR%2520to%2520guide%2520the%2520deforming%2520of%2520the%250Awarped%2520LR%2520features.%2520To%2520generate%2520visually%2520pleasing%2520results%252C%2520we%2520present%2520local%250Aoverlapped%2520sliced%2520Wasserstein%2520loss%2520to%2520better%2520represent%2520the%2520perceptual%250Adifference%2520between%2520GT%2520and%2520output%2520in%2520the%2520feature%2520space.%2520During%2520testing%252C%2520DZSR%2520can%250Abe%2520directly%2520deployed%2520to%2520super-solve%2520the%2520whole%2520ultra-wide%2520image%2520with%2520the%250Areference%2520of%2520the%2520telephoto%2520image.%2520In%2520addition%252C%2520we%2520further%2520take%2520multiple%2520zoomed%250Aobservations%2520to%2520explore%2520self-supervised%2520RefSR%252C%2520and%2520present%2520a%2520progressive%2520fusion%250Ascheme%2520for%2520the%2520effective%2520utilization%2520of%2520reference%2520images.%2520Experiments%2520show%2520that%250Aour%2520methods%2520achieve%2520better%2520quantitative%2520and%2520qualitative%2520performance%2520against%250Astate-of-the-arts.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/cszhilu1998/SelfDZSR_PlusPlus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Real-World%20Super-Resolution%20from%20Dual%20and%0A%20%20Multiple%20Zoomed%20Observations&entry.906535625=Zhilu%20Zhang%20and%20Ruohao%20Wang%20and%20Hongzhi%20Zhang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20two%20challenging%20issues%20in%20reference-based%0Asuper-resolution%20%28RefSR%29%20for%20smartphone%2C%20%28i%29%20how%20to%20choose%20a%20proper%20reference%0Aimage%2C%20and%20%28ii%29%20how%20to%20learn%20RefSR%20in%20a%20self-supervised%20manner.%20Particularly%2C%0Awe%20propose%20a%20novel%20self-supervised%20learning%20approach%20for%20real-world%20RefSR%20from%0Aobservations%20at%20dual%20and%20multiple%20camera%20zooms.%20Firstly%2C%20considering%20the%0Apopularity%20of%20multiple%20cameras%20in%20modern%20smartphones%2C%20the%20more%20zoomed%0A%28telephoto%29%20image%20can%20be%20naturally%20leveraged%20as%20the%20reference%20to%20guide%20the%0Asuper-resolution%20%28SR%29%20of%20the%20lesser%20zoomed%20%28ultra-wide%29%20image%2C%20which%20gives%20us%20a%0Achance%20to%20learn%20a%20deep%20network%20that%20performs%20SR%20from%20the%20dual%20zoomed%0Aobservations%20%28DZSR%29.%20Secondly%2C%20for%20self-supervised%20learning%20of%20DZSR%2C%20we%20take%0Athe%20telephoto%20image%20instead%20of%20an%20additional%20high-resolution%20image%20as%20the%0Asupervision%20information%2C%20and%20select%20a%20center%20patch%20from%20it%20as%20the%20reference%20to%0Asuper-resolve%20the%20corresponding%20ultra-wide%20image%20patch.%20To%20mitigate%20the%20effect%0Aof%20the%20misalignment%20between%20ultra-wide%20low-resolution%20%28LR%29%20patch%20and%20telephoto%0Aground-truth%20%28GT%29%20image%20during%20training%2C%20we%20first%20adopt%20patch-based%20optical%0Aflow%20alignment%20and%20then%20design%20an%20auxiliary-LR%20to%20guide%20the%20deforming%20of%20the%0Awarped%20LR%20features.%20To%20generate%20visually%20pleasing%20results%2C%20we%20present%20local%0Aoverlapped%20sliced%20Wasserstein%20loss%20to%20better%20represent%20the%20perceptual%0Adifference%20between%20GT%20and%20output%20in%20the%20feature%20space.%20During%20testing%2C%20DZSR%20can%0Abe%20directly%20deployed%20to%20super-solve%20the%20whole%20ultra-wide%20image%20with%20the%0Areference%20of%20the%20telephoto%20image.%20In%20addition%2C%20we%20further%20take%20multiple%20zoomed%0Aobservations%20to%20explore%20self-supervised%20RefSR%2C%20and%20present%20a%20progressive%20fusion%0Ascheme%20for%20the%20effective%20utilization%20of%20reference%20images.%20Experiments%20show%20that%0Aour%20methods%20achieve%20better%20quantitative%20and%20qualitative%20performance%20against%0Astate-of-the-arts.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/cszhilu1998/SelfDZSR_PlusPlus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02171v1&entry.124074799=Read"},
{"title": "GReAT: A Graph Regularized Adversarial Training Method", "author": "Samet Bayram and Kenneth Barner", "abstract": "  This paper presents GReAT (Graph Regularized Adversarial Training), a novel\nregularization method designed to enhance the robust classification performance\nof deep learning models. Adversarial examples, characterized by subtle\nperturbations that can mislead models, pose a significant challenge in machine\nlearning. Although adversarial training is effective in defending against such\nattacks, it often overlooks the underlying data structure. In response, GReAT\nintegrates graph based regularization into the adversarial training process,\nleveraging the data's inherent structure to enhance model robustness. By\nincorporating graph information during training, GReAT defends against\nadversarial attacks and improves generalization to unseen data. Extensive\nevaluations on benchmark datasets demonstrate that GReAT outperforms state of\nthe art methods in robustness, achieving notable improvements in classification\naccuracy. Specifically, compared to the second best methods, GReAT achieves a\nperformance increase of approximately 4.87% for CIFAR10 against FGSM attack and\n10.57% for SVHN against FGSM attack. Additionally, for CIFAR10, GReAT\ndemonstrates a performance increase of approximately 11.05% against PGD attack,\nand for SVHN, a 5.54% increase against PGD attack. This paper provides detailed\ninsights into the proposed methodology, including numerical results and\ncomparisons with existing approaches, highlighting the significant impact of\nGReAT in advancing the performance of deep learning models.\n", "link": "http://arxiv.org/abs/2310.05336v2", "date": "2024-05-03", "relevancy": 2.5932, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5245}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5182}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GReAT%3A%20A%20Graph%20Regularized%20Adversarial%20Training%20Method&body=Title%3A%20GReAT%3A%20A%20Graph%20Regularized%20Adversarial%20Training%20Method%0AAuthor%3A%20Samet%20Bayram%20and%20Kenneth%20Barner%0AAbstract%3A%20%20%20This%20paper%20presents%20GReAT%20%28Graph%20Regularized%20Adversarial%20Training%29%2C%20a%20novel%0Aregularization%20method%20designed%20to%20enhance%20the%20robust%20classification%20performance%0Aof%20deep%20learning%20models.%20Adversarial%20examples%2C%20characterized%20by%20subtle%0Aperturbations%20that%20can%20mislead%20models%2C%20pose%20a%20significant%20challenge%20in%20machine%0Alearning.%20Although%20adversarial%20training%20is%20effective%20in%20defending%20against%20such%0Aattacks%2C%20it%20often%20overlooks%20the%20underlying%20data%20structure.%20In%20response%2C%20GReAT%0Aintegrates%20graph%20based%20regularization%20into%20the%20adversarial%20training%20process%2C%0Aleveraging%20the%20data%27s%20inherent%20structure%20to%20enhance%20model%20robustness.%20By%0Aincorporating%20graph%20information%20during%20training%2C%20GReAT%20defends%20against%0Aadversarial%20attacks%20and%20improves%20generalization%20to%20unseen%20data.%20Extensive%0Aevaluations%20on%20benchmark%20datasets%20demonstrate%20that%20GReAT%20outperforms%20state%20of%0Athe%20art%20methods%20in%20robustness%2C%20achieving%20notable%20improvements%20in%20classification%0Aaccuracy.%20Specifically%2C%20compared%20to%20the%20second%20best%20methods%2C%20GReAT%20achieves%20a%0Aperformance%20increase%20of%20approximately%204.87%25%20for%20CIFAR10%20against%20FGSM%20attack%20and%0A10.57%25%20for%20SVHN%20against%20FGSM%20attack.%20Additionally%2C%20for%20CIFAR10%2C%20GReAT%0Ademonstrates%20a%20performance%20increase%20of%20approximately%2011.05%25%20against%20PGD%20attack%2C%0Aand%20for%20SVHN%2C%20a%205.54%25%20increase%20against%20PGD%20attack.%20This%20paper%20provides%20detailed%0Ainsights%20into%20the%20proposed%20methodology%2C%20including%20numerical%20results%20and%0Acomparisons%20with%20existing%20approaches%2C%20highlighting%20the%20significant%20impact%20of%0AGReAT%20in%20advancing%20the%20performance%20of%20deep%20learning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05336v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGReAT%253A%2520A%2520Graph%2520Regularized%2520Adversarial%2520Training%2520Method%26entry.906535625%3DSamet%2520Bayram%2520and%2520Kenneth%2520Barner%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520GReAT%2520%2528Graph%2520Regularized%2520Adversarial%2520Training%2529%252C%2520a%2520novel%250Aregularization%2520method%2520designed%2520to%2520enhance%2520the%2520robust%2520classification%2520performance%250Aof%2520deep%2520learning%2520models.%2520Adversarial%2520examples%252C%2520characterized%2520by%2520subtle%250Aperturbations%2520that%2520can%2520mislead%2520models%252C%2520pose%2520a%2520significant%2520challenge%2520in%2520machine%250Alearning.%2520Although%2520adversarial%2520training%2520is%2520effective%2520in%2520defending%2520against%2520such%250Aattacks%252C%2520it%2520often%2520overlooks%2520the%2520underlying%2520data%2520structure.%2520In%2520response%252C%2520GReAT%250Aintegrates%2520graph%2520based%2520regularization%2520into%2520the%2520adversarial%2520training%2520process%252C%250Aleveraging%2520the%2520data%2527s%2520inherent%2520structure%2520to%2520enhance%2520model%2520robustness.%2520By%250Aincorporating%2520graph%2520information%2520during%2520training%252C%2520GReAT%2520defends%2520against%250Aadversarial%2520attacks%2520and%2520improves%2520generalization%2520to%2520unseen%2520data.%2520Extensive%250Aevaluations%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520GReAT%2520outperforms%2520state%2520of%250Athe%2520art%2520methods%2520in%2520robustness%252C%2520achieving%2520notable%2520improvements%2520in%2520classification%250Aaccuracy.%2520Specifically%252C%2520compared%2520to%2520the%2520second%2520best%2520methods%252C%2520GReAT%2520achieves%2520a%250Aperformance%2520increase%2520of%2520approximately%25204.87%2525%2520for%2520CIFAR10%2520against%2520FGSM%2520attack%2520and%250A10.57%2525%2520for%2520SVHN%2520against%2520FGSM%2520attack.%2520Additionally%252C%2520for%2520CIFAR10%252C%2520GReAT%250Ademonstrates%2520a%2520performance%2520increase%2520of%2520approximately%252011.05%2525%2520against%2520PGD%2520attack%252C%250Aand%2520for%2520SVHN%252C%2520a%25205.54%2525%2520increase%2520against%2520PGD%2520attack.%2520This%2520paper%2520provides%2520detailed%250Ainsights%2520into%2520the%2520proposed%2520methodology%252C%2520including%2520numerical%2520results%2520and%250Acomparisons%2520with%2520existing%2520approaches%252C%2520highlighting%2520the%2520significant%2520impact%2520of%250AGReAT%2520in%2520advancing%2520the%2520performance%2520of%2520deep%2520learning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05336v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GReAT%3A%20A%20Graph%20Regularized%20Adversarial%20Training%20Method&entry.906535625=Samet%20Bayram%20and%20Kenneth%20Barner&entry.1292438233=%20%20This%20paper%20presents%20GReAT%20%28Graph%20Regularized%20Adversarial%20Training%29%2C%20a%20novel%0Aregularization%20method%20designed%20to%20enhance%20the%20robust%20classification%20performance%0Aof%20deep%20learning%20models.%20Adversarial%20examples%2C%20characterized%20by%20subtle%0Aperturbations%20that%20can%20mislead%20models%2C%20pose%20a%20significant%20challenge%20in%20machine%0Alearning.%20Although%20adversarial%20training%20is%20effective%20in%20defending%20against%20such%0Aattacks%2C%20it%20often%20overlooks%20the%20underlying%20data%20structure.%20In%20response%2C%20GReAT%0Aintegrates%20graph%20based%20regularization%20into%20the%20adversarial%20training%20process%2C%0Aleveraging%20the%20data%27s%20inherent%20structure%20to%20enhance%20model%20robustness.%20By%0Aincorporating%20graph%20information%20during%20training%2C%20GReAT%20defends%20against%0Aadversarial%20attacks%20and%20improves%20generalization%20to%20unseen%20data.%20Extensive%0Aevaluations%20on%20benchmark%20datasets%20demonstrate%20that%20GReAT%20outperforms%20state%20of%0Athe%20art%20methods%20in%20robustness%2C%20achieving%20notable%20improvements%20in%20classification%0Aaccuracy.%20Specifically%2C%20compared%20to%20the%20second%20best%20methods%2C%20GReAT%20achieves%20a%0Aperformance%20increase%20of%20approximately%204.87%25%20for%20CIFAR10%20against%20FGSM%20attack%20and%0A10.57%25%20for%20SVHN%20against%20FGSM%20attack.%20Additionally%2C%20for%20CIFAR10%2C%20GReAT%0Ademonstrates%20a%20performance%20increase%20of%20approximately%2011.05%25%20against%20PGD%20attack%2C%0Aand%20for%20SVHN%2C%20a%205.54%25%20increase%20against%20PGD%20attack.%20This%20paper%20provides%20detailed%0Ainsights%20into%20the%20proposed%20methodology%2C%20including%20numerical%20results%20and%0Acomparisons%20with%20existing%20approaches%2C%20highlighting%20the%20significant%20impact%20of%0AGReAT%20in%20advancing%20the%20performance%20of%20deep%20learning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05336v2&entry.124074799=Read"},
{"title": "Towards Diverse Binary Segmentation via A Simple yet General Gated\n  Network", "author": "Xiaoqi Zhao and Youwei Pang and Lihe Zhang and Huchuan Lu and Lei Zhang", "abstract": "  In many binary segmentation tasks, most CNNs-based methods use a U-shape\nencoder-decoder network as their basic structure. They ignore two key problems\nwhen the encoder exchanges information with the decoder: one is the lack of\ninterference control mechanism between them, the other is without considering\nthe disparity of the contributions from different encoder levels. In this work,\nwe propose a simple yet general gated network (GateNet) to tackle them all at\nonce. With the help of multi-level gate units, the valuable context information\nfrom the encoder can be selectively transmitted to the decoder. In addition, we\ndesign a gated dual branch structure to build the cooperation among the\nfeatures of different levels and improve the discrimination ability of the\nnetwork. Furthermore, we introduce a \"Fold\" operation to improve the atrous\nconvolution and form a novel folded atrous convolution, which can be flexibly\nembedded in ASPP or DenseASPP to accurately localize foreground objects of\nvarious scales. GateNet can be easily generalized to many binary segmentation\ntasks, including general and specific object segmentation and multi-modal\nsegmentation. Without bells and whistles, our network consistently performs\nfavorably against the state-of-the-art methods under 10 metrics on 33 datasets\nof 10 binary segmentation tasks.\n", "link": "http://arxiv.org/abs/2303.10396v2", "date": "2024-05-03", "relevancy": 2.5863, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5258}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5179}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Diverse%20Binary%20Segmentation%20via%20A%20Simple%20yet%20General%20Gated%0A%20%20Network&body=Title%3A%20Towards%20Diverse%20Binary%20Segmentation%20via%20A%20Simple%20yet%20General%20Gated%0A%20%20Network%0AAuthor%3A%20Xiaoqi%20Zhao%20and%20Youwei%20Pang%20and%20Lihe%20Zhang%20and%20Huchuan%20Lu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20In%20many%20binary%20segmentation%20tasks%2C%20most%20CNNs-based%20methods%20use%20a%20U-shape%0Aencoder-decoder%20network%20as%20their%20basic%20structure.%20They%20ignore%20two%20key%20problems%0Awhen%20the%20encoder%20exchanges%20information%20with%20the%20decoder%3A%20one%20is%20the%20lack%20of%0Ainterference%20control%20mechanism%20between%20them%2C%20the%20other%20is%20without%20considering%0Athe%20disparity%20of%20the%20contributions%20from%20different%20encoder%20levels.%20In%20this%20work%2C%0Awe%20propose%20a%20simple%20yet%20general%20gated%20network%20%28GateNet%29%20to%20tackle%20them%20all%20at%0Aonce.%20With%20the%20help%20of%20multi-level%20gate%20units%2C%20the%20valuable%20context%20information%0Afrom%20the%20encoder%20can%20be%20selectively%20transmitted%20to%20the%20decoder.%20In%20addition%2C%20we%0Adesign%20a%20gated%20dual%20branch%20structure%20to%20build%20the%20cooperation%20among%20the%0Afeatures%20of%20different%20levels%20and%20improve%20the%20discrimination%20ability%20of%20the%0Anetwork.%20Furthermore%2C%20we%20introduce%20a%20%22Fold%22%20operation%20to%20improve%20the%20atrous%0Aconvolution%20and%20form%20a%20novel%20folded%20atrous%20convolution%2C%20which%20can%20be%20flexibly%0Aembedded%20in%20ASPP%20or%20DenseASPP%20to%20accurately%20localize%20foreground%20objects%20of%0Avarious%20scales.%20GateNet%20can%20be%20easily%20generalized%20to%20many%20binary%20segmentation%0Atasks%2C%20including%20general%20and%20specific%20object%20segmentation%20and%20multi-modal%0Asegmentation.%20Without%20bells%20and%20whistles%2C%20our%20network%20consistently%20performs%0Afavorably%20against%20the%20state-of-the-art%20methods%20under%2010%20metrics%20on%2033%20datasets%0Aof%2010%20binary%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.10396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Diverse%2520Binary%2520Segmentation%2520via%2520A%2520Simple%2520yet%2520General%2520Gated%250A%2520%2520Network%26entry.906535625%3DXiaoqi%2520Zhao%2520and%2520Youwei%2520Pang%2520and%2520Lihe%2520Zhang%2520and%2520Huchuan%2520Lu%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520In%2520many%2520binary%2520segmentation%2520tasks%252C%2520most%2520CNNs-based%2520methods%2520use%2520a%2520U-shape%250Aencoder-decoder%2520network%2520as%2520their%2520basic%2520structure.%2520They%2520ignore%2520two%2520key%2520problems%250Awhen%2520the%2520encoder%2520exchanges%2520information%2520with%2520the%2520decoder%253A%2520one%2520is%2520the%2520lack%2520of%250Ainterference%2520control%2520mechanism%2520between%2520them%252C%2520the%2520other%2520is%2520without%2520considering%250Athe%2520disparity%2520of%2520the%2520contributions%2520from%2520different%2520encoder%2520levels.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520simple%2520yet%2520general%2520gated%2520network%2520%2528GateNet%2529%2520to%2520tackle%2520them%2520all%2520at%250Aonce.%2520With%2520the%2520help%2520of%2520multi-level%2520gate%2520units%252C%2520the%2520valuable%2520context%2520information%250Afrom%2520the%2520encoder%2520can%2520be%2520selectively%2520transmitted%2520to%2520the%2520decoder.%2520In%2520addition%252C%2520we%250Adesign%2520a%2520gated%2520dual%2520branch%2520structure%2520to%2520build%2520the%2520cooperation%2520among%2520the%250Afeatures%2520of%2520different%2520levels%2520and%2520improve%2520the%2520discrimination%2520ability%2520of%2520the%250Anetwork.%2520Furthermore%252C%2520we%2520introduce%2520a%2520%2522Fold%2522%2520operation%2520to%2520improve%2520the%2520atrous%250Aconvolution%2520and%2520form%2520a%2520novel%2520folded%2520atrous%2520convolution%252C%2520which%2520can%2520be%2520flexibly%250Aembedded%2520in%2520ASPP%2520or%2520DenseASPP%2520to%2520accurately%2520localize%2520foreground%2520objects%2520of%250Avarious%2520scales.%2520GateNet%2520can%2520be%2520easily%2520generalized%2520to%2520many%2520binary%2520segmentation%250Atasks%252C%2520including%2520general%2520and%2520specific%2520object%2520segmentation%2520and%2520multi-modal%250Asegmentation.%2520Without%2520bells%2520and%2520whistles%252C%2520our%2520network%2520consistently%2520performs%250Afavorably%2520against%2520the%2520state-of-the-art%2520methods%2520under%252010%2520metrics%2520on%252033%2520datasets%250Aof%252010%2520binary%2520segmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.10396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Diverse%20Binary%20Segmentation%20via%20A%20Simple%20yet%20General%20Gated%0A%20%20Network&entry.906535625=Xiaoqi%20Zhao%20and%20Youwei%20Pang%20and%20Lihe%20Zhang%20and%20Huchuan%20Lu%20and%20Lei%20Zhang&entry.1292438233=%20%20In%20many%20binary%20segmentation%20tasks%2C%20most%20CNNs-based%20methods%20use%20a%20U-shape%0Aencoder-decoder%20network%20as%20their%20basic%20structure.%20They%20ignore%20two%20key%20problems%0Awhen%20the%20encoder%20exchanges%20information%20with%20the%20decoder%3A%20one%20is%20the%20lack%20of%0Ainterference%20control%20mechanism%20between%20them%2C%20the%20other%20is%20without%20considering%0Athe%20disparity%20of%20the%20contributions%20from%20different%20encoder%20levels.%20In%20this%20work%2C%0Awe%20propose%20a%20simple%20yet%20general%20gated%20network%20%28GateNet%29%20to%20tackle%20them%20all%20at%0Aonce.%20With%20the%20help%20of%20multi-level%20gate%20units%2C%20the%20valuable%20context%20information%0Afrom%20the%20encoder%20can%20be%20selectively%20transmitted%20to%20the%20decoder.%20In%20addition%2C%20we%0Adesign%20a%20gated%20dual%20branch%20structure%20to%20build%20the%20cooperation%20among%20the%0Afeatures%20of%20different%20levels%20and%20improve%20the%20discrimination%20ability%20of%20the%0Anetwork.%20Furthermore%2C%20we%20introduce%20a%20%22Fold%22%20operation%20to%20improve%20the%20atrous%0Aconvolution%20and%20form%20a%20novel%20folded%20atrous%20convolution%2C%20which%20can%20be%20flexibly%0Aembedded%20in%20ASPP%20or%20DenseASPP%20to%20accurately%20localize%20foreground%20objects%20of%0Avarious%20scales.%20GateNet%20can%20be%20easily%20generalized%20to%20many%20binary%20segmentation%0Atasks%2C%20including%20general%20and%20specific%20object%20segmentation%20and%20multi-modal%0Asegmentation.%20Without%20bells%20and%20whistles%2C%20our%20network%20consistently%20performs%0Afavorably%20against%20the%20state-of-the-art%20methods%20under%2010%20metrics%20on%2033%20datasets%0Aof%2010%20binary%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10396v2&entry.124074799=Read"},
{"title": "Panoptic-SLAM: Visual SLAM in Dynamic Environments using Panoptic\n  Segmentation", "author": "Gabriel Fischer Abati and Jo\u00e3o Carlos Virgolino Soares and Vivian Suzano Medeiros and Marco Antonio Meggiolaro and Claudio Semini", "abstract": "  The majority of visual SLAM systems are not robust in dynamic scenarios. The\nones that deal with dynamic objects in the scenes usually rely on\ndeep-learning-based methods to detect and filter these objects. However, these\nmethods cannot deal with unknown moving objects. This work presents\nPanoptic-SLAM, an open-source visual SLAM system robust to dynamic\nenvironments, even in the presence of unknown objects. It uses panoptic\nsegmentation to filter dynamic objects from the scene during the state\nestimation process. Panoptic-SLAM is based on ORB-SLAM3, a state-of-the-art\nSLAM system for static environments. The implementation was tested using\nreal-world datasets and compared with several state-of-the-art systems from the\nliterature, including DynaSLAM, DS-SLAM, SaD-SLAM, PVO and FusingPanoptic. For\nexample, Panoptic-SLAM is on average four times more accurate than PVO, the\nmost recent panoptic-based approach for visual SLAM. Also, experiments were\nperformed using a quadruped robot with an RGB-D camera to test the\napplicability of our method in real-world scenarios. The tests were validated\nby a ground-truth created with a motion capture system.\n", "link": "http://arxiv.org/abs/2405.02177v1", "date": "2024-05-03", "relevancy": 2.5493, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6798}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6136}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panoptic-SLAM%3A%20Visual%20SLAM%20in%20Dynamic%20Environments%20using%20Panoptic%0A%20%20Segmentation&body=Title%3A%20Panoptic-SLAM%3A%20Visual%20SLAM%20in%20Dynamic%20Environments%20using%20Panoptic%0A%20%20Segmentation%0AAuthor%3A%20Gabriel%20Fischer%20Abati%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Vivian%20Suzano%20Medeiros%20and%20Marco%20Antonio%20Meggiolaro%20and%20Claudio%20Semini%0AAbstract%3A%20%20%20The%20majority%20of%20visual%20SLAM%20systems%20are%20not%20robust%20in%20dynamic%20scenarios.%20The%0Aones%20that%20deal%20with%20dynamic%20objects%20in%20the%20scenes%20usually%20rely%20on%0Adeep-learning-based%20methods%20to%20detect%20and%20filter%20these%20objects.%20However%2C%20these%0Amethods%20cannot%20deal%20with%20unknown%20moving%20objects.%20This%20work%20presents%0APanoptic-SLAM%2C%20an%20open-source%20visual%20SLAM%20system%20robust%20to%20dynamic%0Aenvironments%2C%20even%20in%20the%20presence%20of%20unknown%20objects.%20It%20uses%20panoptic%0Asegmentation%20to%20filter%20dynamic%20objects%20from%20the%20scene%20during%20the%20state%0Aestimation%20process.%20Panoptic-SLAM%20is%20based%20on%20ORB-SLAM3%2C%20a%20state-of-the-art%0ASLAM%20system%20for%20static%20environments.%20The%20implementation%20was%20tested%20using%0Areal-world%20datasets%20and%20compared%20with%20several%20state-of-the-art%20systems%20from%20the%0Aliterature%2C%20including%20DynaSLAM%2C%20DS-SLAM%2C%20SaD-SLAM%2C%20PVO%20and%20FusingPanoptic.%20For%0Aexample%2C%20Panoptic-SLAM%20is%20on%20average%20four%20times%20more%20accurate%20than%20PVO%2C%20the%0Amost%20recent%20panoptic-based%20approach%20for%20visual%20SLAM.%20Also%2C%20experiments%20were%0Aperformed%20using%20a%20quadruped%20robot%20with%20an%20RGB-D%20camera%20to%20test%20the%0Aapplicability%20of%20our%20method%20in%20real-world%20scenarios.%20The%20tests%20were%20validated%0Aby%20a%20ground-truth%20created%20with%20a%20motion%20capture%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoptic-SLAM%253A%2520Visual%2520SLAM%2520in%2520Dynamic%2520Environments%2520using%2520Panoptic%250A%2520%2520Segmentation%26entry.906535625%3DGabriel%2520Fischer%2520Abati%2520and%2520Jo%25C3%25A3o%2520Carlos%2520Virgolino%2520Soares%2520and%2520Vivian%2520Suzano%2520Medeiros%2520and%2520Marco%2520Antonio%2520Meggiolaro%2520and%2520Claudio%2520Semini%26entry.1292438233%3D%2520%2520The%2520majority%2520of%2520visual%2520SLAM%2520systems%2520are%2520not%2520robust%2520in%2520dynamic%2520scenarios.%2520The%250Aones%2520that%2520deal%2520with%2520dynamic%2520objects%2520in%2520the%2520scenes%2520usually%2520rely%2520on%250Adeep-learning-based%2520methods%2520to%2520detect%2520and%2520filter%2520these%2520objects.%2520However%252C%2520these%250Amethods%2520cannot%2520deal%2520with%2520unknown%2520moving%2520objects.%2520This%2520work%2520presents%250APanoptic-SLAM%252C%2520an%2520open-source%2520visual%2520SLAM%2520system%2520robust%2520to%2520dynamic%250Aenvironments%252C%2520even%2520in%2520the%2520presence%2520of%2520unknown%2520objects.%2520It%2520uses%2520panoptic%250Asegmentation%2520to%2520filter%2520dynamic%2520objects%2520from%2520the%2520scene%2520during%2520the%2520state%250Aestimation%2520process.%2520Panoptic-SLAM%2520is%2520based%2520on%2520ORB-SLAM3%252C%2520a%2520state-of-the-art%250ASLAM%2520system%2520for%2520static%2520environments.%2520The%2520implementation%2520was%2520tested%2520using%250Areal-world%2520datasets%2520and%2520compared%2520with%2520several%2520state-of-the-art%2520systems%2520from%2520the%250Aliterature%252C%2520including%2520DynaSLAM%252C%2520DS-SLAM%252C%2520SaD-SLAM%252C%2520PVO%2520and%2520FusingPanoptic.%2520For%250Aexample%252C%2520Panoptic-SLAM%2520is%2520on%2520average%2520four%2520times%2520more%2520accurate%2520than%2520PVO%252C%2520the%250Amost%2520recent%2520panoptic-based%2520approach%2520for%2520visual%2520SLAM.%2520Also%252C%2520experiments%2520were%250Aperformed%2520using%2520a%2520quadruped%2520robot%2520with%2520an%2520RGB-D%2520camera%2520to%2520test%2520the%250Aapplicability%2520of%2520our%2520method%2520in%2520real-world%2520scenarios.%2520The%2520tests%2520were%2520validated%250Aby%2520a%2520ground-truth%2520created%2520with%2520a%2520motion%2520capture%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panoptic-SLAM%3A%20Visual%20SLAM%20in%20Dynamic%20Environments%20using%20Panoptic%0A%20%20Segmentation&entry.906535625=Gabriel%20Fischer%20Abati%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Vivian%20Suzano%20Medeiros%20and%20Marco%20Antonio%20Meggiolaro%20and%20Claudio%20Semini&entry.1292438233=%20%20The%20majority%20of%20visual%20SLAM%20systems%20are%20not%20robust%20in%20dynamic%20scenarios.%20The%0Aones%20that%20deal%20with%20dynamic%20objects%20in%20the%20scenes%20usually%20rely%20on%0Adeep-learning-based%20methods%20to%20detect%20and%20filter%20these%20objects.%20However%2C%20these%0Amethods%20cannot%20deal%20with%20unknown%20moving%20objects.%20This%20work%20presents%0APanoptic-SLAM%2C%20an%20open-source%20visual%20SLAM%20system%20robust%20to%20dynamic%0Aenvironments%2C%20even%20in%20the%20presence%20of%20unknown%20objects.%20It%20uses%20panoptic%0Asegmentation%20to%20filter%20dynamic%20objects%20from%20the%20scene%20during%20the%20state%0Aestimation%20process.%20Panoptic-SLAM%20is%20based%20on%20ORB-SLAM3%2C%20a%20state-of-the-art%0ASLAM%20system%20for%20static%20environments.%20The%20implementation%20was%20tested%20using%0Areal-world%20datasets%20and%20compared%20with%20several%20state-of-the-art%20systems%20from%20the%0Aliterature%2C%20including%20DynaSLAM%2C%20DS-SLAM%2C%20SaD-SLAM%2C%20PVO%20and%20FusingPanoptic.%20For%0Aexample%2C%20Panoptic-SLAM%20is%20on%20average%20four%20times%20more%20accurate%20than%20PVO%2C%20the%0Amost%20recent%20panoptic-based%20approach%20for%20visual%20SLAM.%20Also%2C%20experiments%20were%0Aperformed%20using%20a%20quadruped%20robot%20with%20an%20RGB-D%20camera%20to%20test%20the%0Aapplicability%20of%20our%20method%20in%20real-world%20scenarios.%20The%20tests%20were%20validated%0Aby%20a%20ground-truth%20created%20with%20a%20motion%20capture%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02177v1&entry.124074799=Read"},
{"title": "Weisfeiler-Lehman goes Dynamic: An Analysis of the Expressive Power of\n  Graph Neural Networks for Attributed and Dynamic Graphs", "author": "Silvia Beddar-Wiesing and Giuseppe Alessio D'Inverno and Caterina Graziani and Veronica Lachi and Alice Moallemy-Oureh and Franco Scarselli and Josephine Maria Thomas", "abstract": "  Graph Neural Networks (GNNs) are a large class of relational models for graph\nprocessing. Recent theoretical studies on the expressive power of GNNs have\nfocused on two issues. On the one hand, it has been proven that GNNs are as\npowerful as the Weisfeiler-Lehman test (1-WL) in their ability to distinguish\ngraphs. Moreover, it has been shown that the equivalence enforced by 1-WL\nequals unfolding equivalence. On the other hand, GNNs turned out to be\nuniversal approximators on graphs modulo the constraints enforced by\n1-WL/unfolding equivalence. However, these results only apply to Static\nAttributed Undirected Homogeneous Graphs (SAUHG) with node attributes. In\ncontrast, real-life applications often involve a much larger variety of graph\ntypes. In this paper, we conduct a theoretical analysis of the expressive power\nof GNNs for two other graph domains that are particularly interesting in\npractical applications, namely dynamic graphs and SAUGHs with edge attributes.\nDynamic graphs are widely used in modern applications; hence, the study of the\nexpressive capability of GNNs in this domain is essential for practical reasons\nand, in addition, it requires a new analyzing approach due to the difference in\nthe architecture of dynamic GNNs compared to static ones. On the other hand,\nthe examination of SAUHGs is of particular relevance since they act as a\nstandard form for all graph types: it has been shown that all graph types can\nbe transformed without loss of information to SAUHGs with both attributes on\nnodes and edges. This paper considers generic GNN models and appropriate 1-WL\ntests for those domains. Then, the known results on the expressive power of\nGNNs are extended to the mentioned domains: it is proven that GNNs have the\nsame capability as the 1-WL test, the 1-WL equivalence equals unfolding\nequivalence and that GNNs are universal approximators modulo 1-WL/unfolding\nequivalence.\n", "link": "http://arxiv.org/abs/2210.03990v2", "date": "2024-05-03", "relevancy": 2.5208, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5345}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.512}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weisfeiler-Lehman%20goes%20Dynamic%3A%20An%20Analysis%20of%20the%20Expressive%20Power%20of%0A%20%20Graph%20Neural%20Networks%20for%20Attributed%20and%20Dynamic%20Graphs&body=Title%3A%20Weisfeiler-Lehman%20goes%20Dynamic%3A%20An%20Analysis%20of%20the%20Expressive%20Power%20of%0A%20%20Graph%20Neural%20Networks%20for%20Attributed%20and%20Dynamic%20Graphs%0AAuthor%3A%20Silvia%20Beddar-Wiesing%20and%20Giuseppe%20Alessio%20D%27Inverno%20and%20Caterina%20Graziani%20and%20Veronica%20Lachi%20and%20Alice%20Moallemy-Oureh%20and%20Franco%20Scarselli%20and%20Josephine%20Maria%20Thomas%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20large%20class%20of%20relational%20models%20for%20graph%0Aprocessing.%20Recent%20theoretical%20studies%20on%20the%20expressive%20power%20of%20GNNs%20have%0Afocused%20on%20two%20issues.%20On%20the%20one%20hand%2C%20it%20has%20been%20proven%20that%20GNNs%20are%20as%0Apowerful%20as%20the%20Weisfeiler-Lehman%20test%20%281-WL%29%20in%20their%20ability%20to%20distinguish%0Agraphs.%20Moreover%2C%20it%20has%20been%20shown%20that%20the%20equivalence%20enforced%20by%201-WL%0Aequals%20unfolding%20equivalence.%20On%20the%20other%20hand%2C%20GNNs%20turned%20out%20to%20be%0Auniversal%20approximators%20on%20graphs%20modulo%20the%20constraints%20enforced%20by%0A1-WL/unfolding%20equivalence.%20However%2C%20these%20results%20only%20apply%20to%20Static%0AAttributed%20Undirected%20Homogeneous%20Graphs%20%28SAUHG%29%20with%20node%20attributes.%20In%0Acontrast%2C%20real-life%20applications%20often%20involve%20a%20much%20larger%20variety%20of%20graph%0Atypes.%20In%20this%20paper%2C%20we%20conduct%20a%20theoretical%20analysis%20of%20the%20expressive%20power%0Aof%20GNNs%20for%20two%20other%20graph%20domains%20that%20are%20particularly%20interesting%20in%0Apractical%20applications%2C%20namely%20dynamic%20graphs%20and%20SAUGHs%20with%20edge%20attributes.%0ADynamic%20graphs%20are%20widely%20used%20in%20modern%20applications%3B%20hence%2C%20the%20study%20of%20the%0Aexpressive%20capability%20of%20GNNs%20in%20this%20domain%20is%20essential%20for%20practical%20reasons%0Aand%2C%20in%20addition%2C%20it%20requires%20a%20new%20analyzing%20approach%20due%20to%20the%20difference%20in%0Athe%20architecture%20of%20dynamic%20GNNs%20compared%20to%20static%20ones.%20On%20the%20other%20hand%2C%0Athe%20examination%20of%20SAUHGs%20is%20of%20particular%20relevance%20since%20they%20act%20as%20a%0Astandard%20form%20for%20all%20graph%20types%3A%20it%20has%20been%20shown%20that%20all%20graph%20types%20can%0Abe%20transformed%20without%20loss%20of%20information%20to%20SAUHGs%20with%20both%20attributes%20on%0Anodes%20and%20edges.%20This%20paper%20considers%20generic%20GNN%20models%20and%20appropriate%201-WL%0Atests%20for%20those%20domains.%20Then%2C%20the%20known%20results%20on%20the%20expressive%20power%20of%0AGNNs%20are%20extended%20to%20the%20mentioned%20domains%3A%20it%20is%20proven%20that%20GNNs%20have%20the%0Asame%20capability%20as%20the%201-WL%20test%2C%20the%201-WL%20equivalence%20equals%20unfolding%0Aequivalence%20and%20that%20GNNs%20are%20universal%20approximators%20modulo%201-WL/unfolding%0Aequivalence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.03990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeisfeiler-Lehman%2520goes%2520Dynamic%253A%2520An%2520Analysis%2520of%2520the%2520Expressive%2520Power%2520of%250A%2520%2520Graph%2520Neural%2520Networks%2520for%2520Attributed%2520and%2520Dynamic%2520Graphs%26entry.906535625%3DSilvia%2520Beddar-Wiesing%2520and%2520Giuseppe%2520Alessio%2520D%2527Inverno%2520and%2520Caterina%2520Graziani%2520and%2520Veronica%2520Lachi%2520and%2520Alice%2520Moallemy-Oureh%2520and%2520Franco%2520Scarselli%2520and%2520Josephine%2520Maria%2520Thomas%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520a%2520large%2520class%2520of%2520relational%2520models%2520for%2520graph%250Aprocessing.%2520Recent%2520theoretical%2520studies%2520on%2520the%2520expressive%2520power%2520of%2520GNNs%2520have%250Afocused%2520on%2520two%2520issues.%2520On%2520the%2520one%2520hand%252C%2520it%2520has%2520been%2520proven%2520that%2520GNNs%2520are%2520as%250Apowerful%2520as%2520the%2520Weisfeiler-Lehman%2520test%2520%25281-WL%2529%2520in%2520their%2520ability%2520to%2520distinguish%250Agraphs.%2520Moreover%252C%2520it%2520has%2520been%2520shown%2520that%2520the%2520equivalence%2520enforced%2520by%25201-WL%250Aequals%2520unfolding%2520equivalence.%2520On%2520the%2520other%2520hand%252C%2520GNNs%2520turned%2520out%2520to%2520be%250Auniversal%2520approximators%2520on%2520graphs%2520modulo%2520the%2520constraints%2520enforced%2520by%250A1-WL/unfolding%2520equivalence.%2520However%252C%2520these%2520results%2520only%2520apply%2520to%2520Static%250AAttributed%2520Undirected%2520Homogeneous%2520Graphs%2520%2528SAUHG%2529%2520with%2520node%2520attributes.%2520In%250Acontrast%252C%2520real-life%2520applications%2520often%2520involve%2520a%2520much%2520larger%2520variety%2520of%2520graph%250Atypes.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520theoretical%2520analysis%2520of%2520the%2520expressive%2520power%250Aof%2520GNNs%2520for%2520two%2520other%2520graph%2520domains%2520that%2520are%2520particularly%2520interesting%2520in%250Apractical%2520applications%252C%2520namely%2520dynamic%2520graphs%2520and%2520SAUGHs%2520with%2520edge%2520attributes.%250ADynamic%2520graphs%2520are%2520widely%2520used%2520in%2520modern%2520applications%253B%2520hence%252C%2520the%2520study%2520of%2520the%250Aexpressive%2520capability%2520of%2520GNNs%2520in%2520this%2520domain%2520is%2520essential%2520for%2520practical%2520reasons%250Aand%252C%2520in%2520addition%252C%2520it%2520requires%2520a%2520new%2520analyzing%2520approach%2520due%2520to%2520the%2520difference%2520in%250Athe%2520architecture%2520of%2520dynamic%2520GNNs%2520compared%2520to%2520static%2520ones.%2520On%2520the%2520other%2520hand%252C%250Athe%2520examination%2520of%2520SAUHGs%2520is%2520of%2520particular%2520relevance%2520since%2520they%2520act%2520as%2520a%250Astandard%2520form%2520for%2520all%2520graph%2520types%253A%2520it%2520has%2520been%2520shown%2520that%2520all%2520graph%2520types%2520can%250Abe%2520transformed%2520without%2520loss%2520of%2520information%2520to%2520SAUHGs%2520with%2520both%2520attributes%2520on%250Anodes%2520and%2520edges.%2520This%2520paper%2520considers%2520generic%2520GNN%2520models%2520and%2520appropriate%25201-WL%250Atests%2520for%2520those%2520domains.%2520Then%252C%2520the%2520known%2520results%2520on%2520the%2520expressive%2520power%2520of%250AGNNs%2520are%2520extended%2520to%2520the%2520mentioned%2520domains%253A%2520it%2520is%2520proven%2520that%2520GNNs%2520have%2520the%250Asame%2520capability%2520as%2520the%25201-WL%2520test%252C%2520the%25201-WL%2520equivalence%2520equals%2520unfolding%250Aequivalence%2520and%2520that%2520GNNs%2520are%2520universal%2520approximators%2520modulo%25201-WL/unfolding%250Aequivalence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.03990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weisfeiler-Lehman%20goes%20Dynamic%3A%20An%20Analysis%20of%20the%20Expressive%20Power%20of%0A%20%20Graph%20Neural%20Networks%20for%20Attributed%20and%20Dynamic%20Graphs&entry.906535625=Silvia%20Beddar-Wiesing%20and%20Giuseppe%20Alessio%20D%27Inverno%20and%20Caterina%20Graziani%20and%20Veronica%20Lachi%20and%20Alice%20Moallemy-Oureh%20and%20Franco%20Scarselli%20and%20Josephine%20Maria%20Thomas&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20large%20class%20of%20relational%20models%20for%20graph%0Aprocessing.%20Recent%20theoretical%20studies%20on%20the%20expressive%20power%20of%20GNNs%20have%0Afocused%20on%20two%20issues.%20On%20the%20one%20hand%2C%20it%20has%20been%20proven%20that%20GNNs%20are%20as%0Apowerful%20as%20the%20Weisfeiler-Lehman%20test%20%281-WL%29%20in%20their%20ability%20to%20distinguish%0Agraphs.%20Moreover%2C%20it%20has%20been%20shown%20that%20the%20equivalence%20enforced%20by%201-WL%0Aequals%20unfolding%20equivalence.%20On%20the%20other%20hand%2C%20GNNs%20turned%20out%20to%20be%0Auniversal%20approximators%20on%20graphs%20modulo%20the%20constraints%20enforced%20by%0A1-WL/unfolding%20equivalence.%20However%2C%20these%20results%20only%20apply%20to%20Static%0AAttributed%20Undirected%20Homogeneous%20Graphs%20%28SAUHG%29%20with%20node%20attributes.%20In%0Acontrast%2C%20real-life%20applications%20often%20involve%20a%20much%20larger%20variety%20of%20graph%0Atypes.%20In%20this%20paper%2C%20we%20conduct%20a%20theoretical%20analysis%20of%20the%20expressive%20power%0Aof%20GNNs%20for%20two%20other%20graph%20domains%20that%20are%20particularly%20interesting%20in%0Apractical%20applications%2C%20namely%20dynamic%20graphs%20and%20SAUGHs%20with%20edge%20attributes.%0ADynamic%20graphs%20are%20widely%20used%20in%20modern%20applications%3B%20hence%2C%20the%20study%20of%20the%0Aexpressive%20capability%20of%20GNNs%20in%20this%20domain%20is%20essential%20for%20practical%20reasons%0Aand%2C%20in%20addition%2C%20it%20requires%20a%20new%20analyzing%20approach%20due%20to%20the%20difference%20in%0Athe%20architecture%20of%20dynamic%20GNNs%20compared%20to%20static%20ones.%20On%20the%20other%20hand%2C%0Athe%20examination%20of%20SAUHGs%20is%20of%20particular%20relevance%20since%20they%20act%20as%20a%0Astandard%20form%20for%20all%20graph%20types%3A%20it%20has%20been%20shown%20that%20all%20graph%20types%20can%0Abe%20transformed%20without%20loss%20of%20information%20to%20SAUHGs%20with%20both%20attributes%20on%0Anodes%20and%20edges.%20This%20paper%20considers%20generic%20GNN%20models%20and%20appropriate%201-WL%0Atests%20for%20those%20domains.%20Then%2C%20the%20known%20results%20on%20the%20expressive%20power%20of%0AGNNs%20are%20extended%20to%20the%20mentioned%20domains%3A%20it%20is%20proven%20that%20GNNs%20have%20the%0Asame%20capability%20as%20the%201-WL%20test%2C%20the%201-WL%20equivalence%20equals%20unfolding%0Aequivalence%20and%20that%20GNNs%20are%20universal%20approximators%20modulo%201-WL/unfolding%0Aequivalence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.03990v2&entry.124074799=Read"},
{"title": "Visual Enumeration is Challenging for Large-scale Generative AI", "author": "Alberto Testolin and Kuinan Hou and Marco Zorzi", "abstract": "  Humans can readily judge the number of objects in a visual scene, even\nwithout counting, and such a skill has been documented in many animal species\nand babies prior to language development and formal schooling. Numerical\njudgments are error-free for small sets, while for larger collections responses\nbecome approximate, with variability increasing proportionally to the target\nnumber. This response pattern is observed for items of all kinds, despite\nvariation in object features (such as color or shape), suggesting that our\nvisual number sense relies on abstract representations of numerosity. Here, we\ninvestigate whether large-scale generative Artificial Intelligence (AI) systems\nhave a human-like number sense, which should allow them to reliably name the\nnumber of objects in simple visual stimuli or generate images containing a\ntarget number of items in the 1-10 range. Surprisingly, most of the foundation\nmodels considered have a poor number sense: They make striking errors even with\nsmall numbers, the response variability does not increase in a systematic way,\nand the pattern of errors depends on object category. Only the most recent\nproprietary systems exhibit signatures of a visual number sense. Our findings\ndemonstrate that having an intuitive visual understanding of number remains\nchallenging for foundation models, which in turn might be detrimental to the\nperceptual grounding of numeracy that in humans is crucial for mathematical\nlearning.\n", "link": "http://arxiv.org/abs/2402.03328v2", "date": "2024-05-03", "relevancy": 2.4629, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.511}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4956}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Enumeration%20is%20Challenging%20for%20Large-scale%20Generative%20AI&body=Title%3A%20Visual%20Enumeration%20is%20Challenging%20for%20Large-scale%20Generative%20AI%0AAuthor%3A%20Alberto%20Testolin%20and%20Kuinan%20Hou%20and%20Marco%20Zorzi%0AAbstract%3A%20%20%20Humans%20can%20readily%20judge%20the%20number%20of%20objects%20in%20a%20visual%20scene%2C%20even%0Awithout%20counting%2C%20and%20such%20a%20skill%20has%20been%20documented%20in%20many%20animal%20species%0Aand%20babies%20prior%20to%20language%20development%20and%20formal%20schooling.%20Numerical%0Ajudgments%20are%20error-free%20for%20small%20sets%2C%20while%20for%20larger%20collections%20responses%0Abecome%20approximate%2C%20with%20variability%20increasing%20proportionally%20to%20the%20target%0Anumber.%20This%20response%20pattern%20is%20observed%20for%20items%20of%20all%20kinds%2C%20despite%0Avariation%20in%20object%20features%20%28such%20as%20color%20or%20shape%29%2C%20suggesting%20that%20our%0Avisual%20number%20sense%20relies%20on%20abstract%20representations%20of%20numerosity.%20Here%2C%20we%0Ainvestigate%20whether%20large-scale%20generative%20Artificial%20Intelligence%20%28AI%29%20systems%0Ahave%20a%20human-like%20number%20sense%2C%20which%20should%20allow%20them%20to%20reliably%20name%20the%0Anumber%20of%20objects%20in%20simple%20visual%20stimuli%20or%20generate%20images%20containing%20a%0Atarget%20number%20of%20items%20in%20the%201-10%20range.%20Surprisingly%2C%20most%20of%20the%20foundation%0Amodels%20considered%20have%20a%20poor%20number%20sense%3A%20They%20make%20striking%20errors%20even%20with%0Asmall%20numbers%2C%20the%20response%20variability%20does%20not%20increase%20in%20a%20systematic%20way%2C%0Aand%20the%20pattern%20of%20errors%20depends%20on%20object%20category.%20Only%20the%20most%20recent%0Aproprietary%20systems%20exhibit%20signatures%20of%20a%20visual%20number%20sense.%20Our%20findings%0Ademonstrate%20that%20having%20an%20intuitive%20visual%20understanding%20of%20number%20remains%0Achallenging%20for%20foundation%20models%2C%20which%20in%20turn%20might%20be%20detrimental%20to%20the%0Aperceptual%20grounding%20of%20numeracy%20that%20in%20humans%20is%20crucial%20for%20mathematical%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Enumeration%2520is%2520Challenging%2520for%2520Large-scale%2520Generative%2520AI%26entry.906535625%3DAlberto%2520Testolin%2520and%2520Kuinan%2520Hou%2520and%2520Marco%2520Zorzi%26entry.1292438233%3D%2520%2520Humans%2520can%2520readily%2520judge%2520the%2520number%2520of%2520objects%2520in%2520a%2520visual%2520scene%252C%2520even%250Awithout%2520counting%252C%2520and%2520such%2520a%2520skill%2520has%2520been%2520documented%2520in%2520many%2520animal%2520species%250Aand%2520babies%2520prior%2520to%2520language%2520development%2520and%2520formal%2520schooling.%2520Numerical%250Ajudgments%2520are%2520error-free%2520for%2520small%2520sets%252C%2520while%2520for%2520larger%2520collections%2520responses%250Abecome%2520approximate%252C%2520with%2520variability%2520increasing%2520proportionally%2520to%2520the%2520target%250Anumber.%2520This%2520response%2520pattern%2520is%2520observed%2520for%2520items%2520of%2520all%2520kinds%252C%2520despite%250Avariation%2520in%2520object%2520features%2520%2528such%2520as%2520color%2520or%2520shape%2529%252C%2520suggesting%2520that%2520our%250Avisual%2520number%2520sense%2520relies%2520on%2520abstract%2520representations%2520of%2520numerosity.%2520Here%252C%2520we%250Ainvestigate%2520whether%2520large-scale%2520generative%2520Artificial%2520Intelligence%2520%2528AI%2529%2520systems%250Ahave%2520a%2520human-like%2520number%2520sense%252C%2520which%2520should%2520allow%2520them%2520to%2520reliably%2520name%2520the%250Anumber%2520of%2520objects%2520in%2520simple%2520visual%2520stimuli%2520or%2520generate%2520images%2520containing%2520a%250Atarget%2520number%2520of%2520items%2520in%2520the%25201-10%2520range.%2520Surprisingly%252C%2520most%2520of%2520the%2520foundation%250Amodels%2520considered%2520have%2520a%2520poor%2520number%2520sense%253A%2520They%2520make%2520striking%2520errors%2520even%2520with%250Asmall%2520numbers%252C%2520the%2520response%2520variability%2520does%2520not%2520increase%2520in%2520a%2520systematic%2520way%252C%250Aand%2520the%2520pattern%2520of%2520errors%2520depends%2520on%2520object%2520category.%2520Only%2520the%2520most%2520recent%250Aproprietary%2520systems%2520exhibit%2520signatures%2520of%2520a%2520visual%2520number%2520sense.%2520Our%2520findings%250Ademonstrate%2520that%2520having%2520an%2520intuitive%2520visual%2520understanding%2520of%2520number%2520remains%250Achallenging%2520for%2520foundation%2520models%252C%2520which%2520in%2520turn%2520might%2520be%2520detrimental%2520to%2520the%250Aperceptual%2520grounding%2520of%2520numeracy%2520that%2520in%2520humans%2520is%2520crucial%2520for%2520mathematical%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Enumeration%20is%20Challenging%20for%20Large-scale%20Generative%20AI&entry.906535625=Alberto%20Testolin%20and%20Kuinan%20Hou%20and%20Marco%20Zorzi&entry.1292438233=%20%20Humans%20can%20readily%20judge%20the%20number%20of%20objects%20in%20a%20visual%20scene%2C%20even%0Awithout%20counting%2C%20and%20such%20a%20skill%20has%20been%20documented%20in%20many%20animal%20species%0Aand%20babies%20prior%20to%20language%20development%20and%20formal%20schooling.%20Numerical%0Ajudgments%20are%20error-free%20for%20small%20sets%2C%20while%20for%20larger%20collections%20responses%0Abecome%20approximate%2C%20with%20variability%20increasing%20proportionally%20to%20the%20target%0Anumber.%20This%20response%20pattern%20is%20observed%20for%20items%20of%20all%20kinds%2C%20despite%0Avariation%20in%20object%20features%20%28such%20as%20color%20or%20shape%29%2C%20suggesting%20that%20our%0Avisual%20number%20sense%20relies%20on%20abstract%20representations%20of%20numerosity.%20Here%2C%20we%0Ainvestigate%20whether%20large-scale%20generative%20Artificial%20Intelligence%20%28AI%29%20systems%0Ahave%20a%20human-like%20number%20sense%2C%20which%20should%20allow%20them%20to%20reliably%20name%20the%0Anumber%20of%20objects%20in%20simple%20visual%20stimuli%20or%20generate%20images%20containing%20a%0Atarget%20number%20of%20items%20in%20the%201-10%20range.%20Surprisingly%2C%20most%20of%20the%20foundation%0Amodels%20considered%20have%20a%20poor%20number%20sense%3A%20They%20make%20striking%20errors%20even%20with%0Asmall%20numbers%2C%20the%20response%20variability%20does%20not%20increase%20in%20a%20systematic%20way%2C%0Aand%20the%20pattern%20of%20errors%20depends%20on%20object%20category.%20Only%20the%20most%20recent%0Aproprietary%20systems%20exhibit%20signatures%20of%20a%20visual%20number%20sense.%20Our%20findings%0Ademonstrate%20that%20having%20an%20intuitive%20visual%20understanding%20of%20number%20remains%0Achallenging%20for%20foundation%20models%2C%20which%20in%20turn%20might%20be%20detrimental%20to%20the%0Aperceptual%20grounding%20of%20numeracy%20that%20in%20humans%20is%20crucial%20for%20mathematical%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03328v2&entry.124074799=Read"},
{"title": "X-SLAM: Scalable Dense SLAM for Task-aware Optimization using CSFD", "author": "Zhexi Peng and Yin Yang and Tianjia Shao and Chenfanfu Jiang and Kun Zhou", "abstract": "  We present X-SLAM, a real-time dense differentiable SLAM system that\nleverages the complex-step finite difference (CSFD) method for efficient\ncalculation of numerical derivatives, bypassing the need for a large-scale\ncomputational graph. The key to our approach is treating the SLAM process as a\ndifferentiable function, enabling the calculation of the derivatives of\nimportant SLAM parameters through Taylor series expansion within the complex\ndomain. Our system allows for the real-time calculation of not just the\ngradient, but also higher-order differentiation. This facilitates the use of\nhigh-order optimizers to achieve better accuracy and faster convergence.\nBuilding on X-SLAM, we implemented end-to-end optimization frameworks for two\nimportant tasks: camera relocalization in wide outdoor scenes and active\nrobotic scanning in complex indoor environments. Comprehensive evaluations on\npublic benchmarks and intricate real scenes underscore the improvements in the\naccuracy of camera relocalization and the efficiency of robotic navigation\nachieved through our task-aware optimization. The code and data are available\nat https://gapszju.github.io/X-SLAM.\n", "link": "http://arxiv.org/abs/2405.02187v1", "date": "2024-05-03", "relevancy": 2.3849, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6543}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-SLAM%3A%20Scalable%20Dense%20SLAM%20for%20Task-aware%20Optimization%20using%20CSFD&body=Title%3A%20X-SLAM%3A%20Scalable%20Dense%20SLAM%20for%20Task-aware%20Optimization%20using%20CSFD%0AAuthor%3A%20Zhexi%20Peng%20and%20Yin%20Yang%20and%20Tianjia%20Shao%20and%20Chenfanfu%20Jiang%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20We%20present%20X-SLAM%2C%20a%20real-time%20dense%20differentiable%20SLAM%20system%20that%0Aleverages%20the%20complex-step%20finite%20difference%20%28CSFD%29%20method%20for%20efficient%0Acalculation%20of%20numerical%20derivatives%2C%20bypassing%20the%20need%20for%20a%20large-scale%0Acomputational%20graph.%20The%20key%20to%20our%20approach%20is%20treating%20the%20SLAM%20process%20as%20a%0Adifferentiable%20function%2C%20enabling%20the%20calculation%20of%20the%20derivatives%20of%0Aimportant%20SLAM%20parameters%20through%20Taylor%20series%20expansion%20within%20the%20complex%0Adomain.%20Our%20system%20allows%20for%20the%20real-time%20calculation%20of%20not%20just%20the%0Agradient%2C%20but%20also%20higher-order%20differentiation.%20This%20facilitates%20the%20use%20of%0Ahigh-order%20optimizers%20to%20achieve%20better%20accuracy%20and%20faster%20convergence.%0ABuilding%20on%20X-SLAM%2C%20we%20implemented%20end-to-end%20optimization%20frameworks%20for%20two%0Aimportant%20tasks%3A%20camera%20relocalization%20in%20wide%20outdoor%20scenes%20and%20active%0Arobotic%20scanning%20in%20complex%20indoor%20environments.%20Comprehensive%20evaluations%20on%0Apublic%20benchmarks%20and%20intricate%20real%20scenes%20underscore%20the%20improvements%20in%20the%0Aaccuracy%20of%20camera%20relocalization%20and%20the%20efficiency%20of%20robotic%20navigation%0Aachieved%20through%20our%20task-aware%20optimization.%20The%20code%20and%20data%20are%20available%0Aat%20https%3A//gapszju.github.io/X-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-SLAM%253A%2520Scalable%2520Dense%2520SLAM%2520for%2520Task-aware%2520Optimization%2520using%2520CSFD%26entry.906535625%3DZhexi%2520Peng%2520and%2520Yin%2520Yang%2520and%2520Tianjia%2520Shao%2520and%2520Chenfanfu%2520Jiang%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520X-SLAM%252C%2520a%2520real-time%2520dense%2520differentiable%2520SLAM%2520system%2520that%250Aleverages%2520the%2520complex-step%2520finite%2520difference%2520%2528CSFD%2529%2520method%2520for%2520efficient%250Acalculation%2520of%2520numerical%2520derivatives%252C%2520bypassing%2520the%2520need%2520for%2520a%2520large-scale%250Acomputational%2520graph.%2520The%2520key%2520to%2520our%2520approach%2520is%2520treating%2520the%2520SLAM%2520process%2520as%2520a%250Adifferentiable%2520function%252C%2520enabling%2520the%2520calculation%2520of%2520the%2520derivatives%2520of%250Aimportant%2520SLAM%2520parameters%2520through%2520Taylor%2520series%2520expansion%2520within%2520the%2520complex%250Adomain.%2520Our%2520system%2520allows%2520for%2520the%2520real-time%2520calculation%2520of%2520not%2520just%2520the%250Agradient%252C%2520but%2520also%2520higher-order%2520differentiation.%2520This%2520facilitates%2520the%2520use%2520of%250Ahigh-order%2520optimizers%2520to%2520achieve%2520better%2520accuracy%2520and%2520faster%2520convergence.%250ABuilding%2520on%2520X-SLAM%252C%2520we%2520implemented%2520end-to-end%2520optimization%2520frameworks%2520for%2520two%250Aimportant%2520tasks%253A%2520camera%2520relocalization%2520in%2520wide%2520outdoor%2520scenes%2520and%2520active%250Arobotic%2520scanning%2520in%2520complex%2520indoor%2520environments.%2520Comprehensive%2520evaluations%2520on%250Apublic%2520benchmarks%2520and%2520intricate%2520real%2520scenes%2520underscore%2520the%2520improvements%2520in%2520the%250Aaccuracy%2520of%2520camera%2520relocalization%2520and%2520the%2520efficiency%2520of%2520robotic%2520navigation%250Aachieved%2520through%2520our%2520task-aware%2520optimization.%2520The%2520code%2520and%2520data%2520are%2520available%250Aat%2520https%253A//gapszju.github.io/X-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-SLAM%3A%20Scalable%20Dense%20SLAM%20for%20Task-aware%20Optimization%20using%20CSFD&entry.906535625=Zhexi%20Peng%20and%20Yin%20Yang%20and%20Tianjia%20Shao%20and%20Chenfanfu%20Jiang%20and%20Kun%20Zhou&entry.1292438233=%20%20We%20present%20X-SLAM%2C%20a%20real-time%20dense%20differentiable%20SLAM%20system%20that%0Aleverages%20the%20complex-step%20finite%20difference%20%28CSFD%29%20method%20for%20efficient%0Acalculation%20of%20numerical%20derivatives%2C%20bypassing%20the%20need%20for%20a%20large-scale%0Acomputational%20graph.%20The%20key%20to%20our%20approach%20is%20treating%20the%20SLAM%20process%20as%20a%0Adifferentiable%20function%2C%20enabling%20the%20calculation%20of%20the%20derivatives%20of%0Aimportant%20SLAM%20parameters%20through%20Taylor%20series%20expansion%20within%20the%20complex%0Adomain.%20Our%20system%20allows%20for%20the%20real-time%20calculation%20of%20not%20just%20the%0Agradient%2C%20but%20also%20higher-order%20differentiation.%20This%20facilitates%20the%20use%20of%0Ahigh-order%20optimizers%20to%20achieve%20better%20accuracy%20and%20faster%20convergence.%0ABuilding%20on%20X-SLAM%2C%20we%20implemented%20end-to-end%20optimization%20frameworks%20for%20two%0Aimportant%20tasks%3A%20camera%20relocalization%20in%20wide%20outdoor%20scenes%20and%20active%0Arobotic%20scanning%20in%20complex%20indoor%20environments.%20Comprehensive%20evaluations%20on%0Apublic%20benchmarks%20and%20intricate%20real%20scenes%20underscore%20the%20improvements%20in%20the%0Aaccuracy%20of%20camera%20relocalization%20and%20the%20efficiency%20of%20robotic%20navigation%0Aachieved%20through%20our%20task-aware%20optimization.%20The%20code%20and%20data%20are%20available%0Aat%20https%3A//gapszju.github.io/X-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02187v1&entry.124074799=Read"},
{"title": "A Federated Learning Benchmark on Tabular Data: Comparing Tree-Based\n  Models and Neural Networks", "author": "William Lindskog and Christian Prehofer", "abstract": "  Federated Learning (FL) has lately gained traction as it addresses how\nmachine learning models train on distributed datasets. FL was designed for\nparametric models, namely Deep Neural Networks (DNNs).Thus, it has shown\npromise on image and text tasks. However, FL for tabular data has received\nlittle attention. Tree-Based Models (TBMs) have been considered to perform\nbetter on tabular data and they are starting to see FL integrations. In this\nstudy, we benchmark federated TBMs and DNNs for horizontal FL, with varying\ndata partitions, on 10 well-known tabular datasets. Our novel benchmark results\nindicates that current federated boosted TBMs perform better than federated\nDNNs in different data partitions. Furthermore, a federated XGBoost outperforms\nall other models. Lastly, we find that federated TBMs perform better than\nfederated parametric models, even when increasing the number of clients\nsignificantly.\n", "link": "http://arxiv.org/abs/2405.02074v1", "date": "2024-05-03", "relevancy": 2.3618, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4794}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4789}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Federated%20Learning%20Benchmark%20on%20Tabular%20Data%3A%20Comparing%20Tree-Based%0A%20%20Models%20and%20Neural%20Networks&body=Title%3A%20A%20Federated%20Learning%20Benchmark%20on%20Tabular%20Data%3A%20Comparing%20Tree-Based%0A%20%20Models%20and%20Neural%20Networks%0AAuthor%3A%20William%20Lindskog%20and%20Christian%20Prehofer%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20lately%20gained%20traction%20as%20it%20addresses%20how%0Amachine%20learning%20models%20train%20on%20distributed%20datasets.%20FL%20was%20designed%20for%0Aparametric%20models%2C%20namely%20Deep%20Neural%20Networks%20%28DNNs%29.Thus%2C%20it%20has%20shown%0Apromise%20on%20image%20and%20text%20tasks.%20However%2C%20FL%20for%20tabular%20data%20has%20received%0Alittle%20attention.%20Tree-Based%20Models%20%28TBMs%29%20have%20been%20considered%20to%20perform%0Abetter%20on%20tabular%20data%20and%20they%20are%20starting%20to%20see%20FL%20integrations.%20In%20this%0Astudy%2C%20we%20benchmark%20federated%20TBMs%20and%20DNNs%20for%20horizontal%20FL%2C%20with%20varying%0Adata%20partitions%2C%20on%2010%20well-known%20tabular%20datasets.%20Our%20novel%20benchmark%20results%0Aindicates%20that%20current%20federated%20boosted%20TBMs%20perform%20better%20than%20federated%0ADNNs%20in%20different%20data%20partitions.%20Furthermore%2C%20a%20federated%20XGBoost%20outperforms%0Aall%20other%20models.%20Lastly%2C%20we%20find%20that%20federated%20TBMs%20perform%20better%20than%0Afederated%20parametric%20models%2C%20even%20when%20increasing%20the%20number%20of%20clients%0Asignificantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Federated%2520Learning%2520Benchmark%2520on%2520Tabular%2520Data%253A%2520Comparing%2520Tree-Based%250A%2520%2520Models%2520and%2520Neural%2520Networks%26entry.906535625%3DWilliam%2520Lindskog%2520and%2520Christian%2520Prehofer%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520lately%2520gained%2520traction%2520as%2520it%2520addresses%2520how%250Amachine%2520learning%2520models%2520train%2520on%2520distributed%2520datasets.%2520FL%2520was%2520designed%2520for%250Aparametric%2520models%252C%2520namely%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529.Thus%252C%2520it%2520has%2520shown%250Apromise%2520on%2520image%2520and%2520text%2520tasks.%2520However%252C%2520FL%2520for%2520tabular%2520data%2520has%2520received%250Alittle%2520attention.%2520Tree-Based%2520Models%2520%2528TBMs%2529%2520have%2520been%2520considered%2520to%2520perform%250Abetter%2520on%2520tabular%2520data%2520and%2520they%2520are%2520starting%2520to%2520see%2520FL%2520integrations.%2520In%2520this%250Astudy%252C%2520we%2520benchmark%2520federated%2520TBMs%2520and%2520DNNs%2520for%2520horizontal%2520FL%252C%2520with%2520varying%250Adata%2520partitions%252C%2520on%252010%2520well-known%2520tabular%2520datasets.%2520Our%2520novel%2520benchmark%2520results%250Aindicates%2520that%2520current%2520federated%2520boosted%2520TBMs%2520perform%2520better%2520than%2520federated%250ADNNs%2520in%2520different%2520data%2520partitions.%2520Furthermore%252C%2520a%2520federated%2520XGBoost%2520outperforms%250Aall%2520other%2520models.%2520Lastly%252C%2520we%2520find%2520that%2520federated%2520TBMs%2520perform%2520better%2520than%250Afederated%2520parametric%2520models%252C%2520even%2520when%2520increasing%2520the%2520number%2520of%2520clients%250Asignificantly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Federated%20Learning%20Benchmark%20on%20Tabular%20Data%3A%20Comparing%20Tree-Based%0A%20%20Models%20and%20Neural%20Networks&entry.906535625=William%20Lindskog%20and%20Christian%20Prehofer&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20lately%20gained%20traction%20as%20it%20addresses%20how%0Amachine%20learning%20models%20train%20on%20distributed%20datasets.%20FL%20was%20designed%20for%0Aparametric%20models%2C%20namely%20Deep%20Neural%20Networks%20%28DNNs%29.Thus%2C%20it%20has%20shown%0Apromise%20on%20image%20and%20text%20tasks.%20However%2C%20FL%20for%20tabular%20data%20has%20received%0Alittle%20attention.%20Tree-Based%20Models%20%28TBMs%29%20have%20been%20considered%20to%20perform%0Abetter%20on%20tabular%20data%20and%20they%20are%20starting%20to%20see%20FL%20integrations.%20In%20this%0Astudy%2C%20we%20benchmark%20federated%20TBMs%20and%20DNNs%20for%20horizontal%20FL%2C%20with%20varying%0Adata%20partitions%2C%20on%2010%20well-known%20tabular%20datasets.%20Our%20novel%20benchmark%20results%0Aindicates%20that%20current%20federated%20boosted%20TBMs%20perform%20better%20than%20federated%0ADNNs%20in%20different%20data%20partitions.%20Furthermore%2C%20a%20federated%20XGBoost%20outperforms%0Aall%20other%20models.%20Lastly%2C%20we%20find%20that%20federated%20TBMs%20perform%20better%20than%0Afederated%20parametric%20models%2C%20even%20when%20increasing%20the%20number%20of%20clients%0Asignificantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02074v1&entry.124074799=Read"},
{"title": "Multi-level projection with exponential parallel speedup; Application to\n  sparse auto-encoders neural networks", "author": "Guillaume Perez and Michel Barlaud", "abstract": "  The $\\ell_{1,\\infty}$ norm is an efficient structured projection but the\ncomplexity of the best algorithm is unfortunately $\\mathcal{O}\\big(n m \\log(n\nm)\\big)$ for a matrix in $\\mathbb{R}^{n\\times m}$. In this paper, we propose a\nnew bi-level projection method for which we show that the time complexity for\nthe $\\ell_{1,\\infty}$ norm is only $\\mathcal{O}\\big(n m \\big)$ for a matrix in\n$\\mathbb{R}^{n\\times m}$, and $\\mathcal{O}\\big(n + m \\big)$ with full parallel\npower. We generalize our method to tensors and we propose a new multi-level\nprojection, having an induced decomposition that yields a linear parallel\nspeedup up to an exponential speedup factor, resulting in a time complexity\nlower-bounded by the sum of the dimensions. Experiments show that our bi-level\n$\\ell_{1,\\infty}$ projection is $2.5$ times faster than the actual fastest\nalgorithm provided by \\textit{Chu et. al.} while providing same accuracy and\nbetter sparsity in neural networks applications.\n", "link": "http://arxiv.org/abs/2405.02086v1", "date": "2024-05-03", "relevancy": 2.346, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4801}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4667}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-level%20projection%20with%20exponential%20parallel%20speedup%3B%20Application%20to%0A%20%20sparse%20auto-encoders%20neural%20networks&body=Title%3A%20Multi-level%20projection%20with%20exponential%20parallel%20speedup%3B%20Application%20to%0A%20%20sparse%20auto-encoders%20neural%20networks%0AAuthor%3A%20Guillaume%20Perez%20and%20Michel%20Barlaud%0AAbstract%3A%20%20%20The%20%24%5Cell_%7B1%2C%5Cinfty%7D%24%20norm%20is%20an%20efficient%20structured%20projection%20but%20the%0Acomplexity%20of%20the%20best%20algorithm%20is%20unfortunately%20%24%5Cmathcal%7BO%7D%5Cbig%28n%20m%20%5Clog%28n%0Am%29%5Cbig%29%24%20for%20a%20matrix%20in%20%24%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20m%7D%24.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20bi-level%20projection%20method%20for%20which%20we%20show%20that%20the%20time%20complexity%20for%0Athe%20%24%5Cell_%7B1%2C%5Cinfty%7D%24%20norm%20is%20only%20%24%5Cmathcal%7BO%7D%5Cbig%28n%20m%20%5Cbig%29%24%20for%20a%20matrix%20in%0A%24%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20m%7D%24%2C%20and%20%24%5Cmathcal%7BO%7D%5Cbig%28n%20%2B%20m%20%5Cbig%29%24%20with%20full%20parallel%0Apower.%20We%20generalize%20our%20method%20to%20tensors%20and%20we%20propose%20a%20new%20multi-level%0Aprojection%2C%20having%20an%20induced%20decomposition%20that%20yields%20a%20linear%20parallel%0Aspeedup%20up%20to%20an%20exponential%20speedup%20factor%2C%20resulting%20in%20a%20time%20complexity%0Alower-bounded%20by%20the%20sum%20of%20the%20dimensions.%20Experiments%20show%20that%20our%20bi-level%0A%24%5Cell_%7B1%2C%5Cinfty%7D%24%20projection%20is%20%242.5%24%20times%20faster%20than%20the%20actual%20fastest%0Aalgorithm%20provided%20by%20%5Ctextit%7BChu%20et.%20al.%7D%20while%20providing%20same%20accuracy%20and%0Abetter%20sparsity%20in%20neural%20networks%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-level%2520projection%2520with%2520exponential%2520parallel%2520speedup%253B%2520Application%2520to%250A%2520%2520sparse%2520auto-encoders%2520neural%2520networks%26entry.906535625%3DGuillaume%2520Perez%2520and%2520Michel%2520Barlaud%26entry.1292438233%3D%2520%2520The%2520%2524%255Cell_%257B1%252C%255Cinfty%257D%2524%2520norm%2520is%2520an%2520efficient%2520structured%2520projection%2520but%2520the%250Acomplexity%2520of%2520the%2520best%2520algorithm%2520is%2520unfortunately%2520%2524%255Cmathcal%257BO%257D%255Cbig%2528n%2520m%2520%255Clog%2528n%250Am%2529%255Cbig%2529%2524%2520for%2520a%2520matrix%2520in%2520%2524%255Cmathbb%257BR%257D%255E%257Bn%255Ctimes%2520m%257D%2524.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anew%2520bi-level%2520projection%2520method%2520for%2520which%2520we%2520show%2520that%2520the%2520time%2520complexity%2520for%250Athe%2520%2524%255Cell_%257B1%252C%255Cinfty%257D%2524%2520norm%2520is%2520only%2520%2524%255Cmathcal%257BO%257D%255Cbig%2528n%2520m%2520%255Cbig%2529%2524%2520for%2520a%2520matrix%2520in%250A%2524%255Cmathbb%257BR%257D%255E%257Bn%255Ctimes%2520m%257D%2524%252C%2520and%2520%2524%255Cmathcal%257BO%257D%255Cbig%2528n%2520%252B%2520m%2520%255Cbig%2529%2524%2520with%2520full%2520parallel%250Apower.%2520We%2520generalize%2520our%2520method%2520to%2520tensors%2520and%2520we%2520propose%2520a%2520new%2520multi-level%250Aprojection%252C%2520having%2520an%2520induced%2520decomposition%2520that%2520yields%2520a%2520linear%2520parallel%250Aspeedup%2520up%2520to%2520an%2520exponential%2520speedup%2520factor%252C%2520resulting%2520in%2520a%2520time%2520complexity%250Alower-bounded%2520by%2520the%2520sum%2520of%2520the%2520dimensions.%2520Experiments%2520show%2520that%2520our%2520bi-level%250A%2524%255Cell_%257B1%252C%255Cinfty%257D%2524%2520projection%2520is%2520%25242.5%2524%2520times%2520faster%2520than%2520the%2520actual%2520fastest%250Aalgorithm%2520provided%2520by%2520%255Ctextit%257BChu%2520et.%2520al.%257D%2520while%2520providing%2520same%2520accuracy%2520and%250Abetter%2520sparsity%2520in%2520neural%2520networks%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-level%20projection%20with%20exponential%20parallel%20speedup%3B%20Application%20to%0A%20%20sparse%20auto-encoders%20neural%20networks&entry.906535625=Guillaume%20Perez%20and%20Michel%20Barlaud&entry.1292438233=%20%20The%20%24%5Cell_%7B1%2C%5Cinfty%7D%24%20norm%20is%20an%20efficient%20structured%20projection%20but%20the%0Acomplexity%20of%20the%20best%20algorithm%20is%20unfortunately%20%24%5Cmathcal%7BO%7D%5Cbig%28n%20m%20%5Clog%28n%0Am%29%5Cbig%29%24%20for%20a%20matrix%20in%20%24%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20m%7D%24.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20bi-level%20projection%20method%20for%20which%20we%20show%20that%20the%20time%20complexity%20for%0Athe%20%24%5Cell_%7B1%2C%5Cinfty%7D%24%20norm%20is%20only%20%24%5Cmathcal%7BO%7D%5Cbig%28n%20m%20%5Cbig%29%24%20for%20a%20matrix%20in%0A%24%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20m%7D%24%2C%20and%20%24%5Cmathcal%7BO%7D%5Cbig%28n%20%2B%20m%20%5Cbig%29%24%20with%20full%20parallel%0Apower.%20We%20generalize%20our%20method%20to%20tensors%20and%20we%20propose%20a%20new%20multi-level%0Aprojection%2C%20having%20an%20induced%20decomposition%20that%20yields%20a%20linear%20parallel%0Aspeedup%20up%20to%20an%20exponential%20speedup%20factor%2C%20resulting%20in%20a%20time%20complexity%0Alower-bounded%20by%20the%20sum%20of%20the%20dimensions.%20Experiments%20show%20that%20our%20bi-level%0A%24%5Cell_%7B1%2C%5Cinfty%7D%24%20projection%20is%20%242.5%24%20times%20faster%20than%20the%20actual%20fastest%0Aalgorithm%20provided%20by%20%5Ctextit%7BChu%20et.%20al.%7D%20while%20providing%20same%20accuracy%20and%0Abetter%20sparsity%20in%20neural%20networks%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02086v1&entry.124074799=Read"},
{"title": "Single-Task Continual Offline Reinforcement Learning", "author": "Sibo Gai and Donglin Wang", "abstract": "  In this paper, we study the continual learning problem of single-task offline\nreinforcement learning. In the past, continual reinforcement learning usually\nonly dealt with multitasking, that is, learning multiple related or unrelated\ntasks in a row, but once each learned task was learned, it was not relearned,\nbut only used in subsequent processes. However, offline reinforcement learning\ntasks require the continuously learning of multiple different datasets for the\nsame task. Existing algorithms will try their best to achieve the best results\nin each offline dataset they have learned and the skills of the network will\noverwrite the high-quality datasets that have been learned after learning the\nsubsequent poor datasets. On the other hand, if too much emphasis is placed on\nstability, the network will learn the subsequent better dataset after learning\nthe poor offline dataset, and the problem of insufficient plasticity and\nnon-learning will occur. How to design a strategy that can always preserve the\nbest performance for each state in the data that has been learned is a new\nchallenge and the focus of this study. Therefore, this study proposes a new\nalgorithm, called Ensemble Offline Reinforcement Learning Based on Experience\nReplay, which introduces multiple value networks to learn the same dataset and\njudge whether the strategy has been learned by the discrete degree of the value\nnetwork, to improve the performance of the network in single-task offline\nreinforcement learning.\n", "link": "http://arxiv.org/abs/2404.12639v2", "date": "2024-05-03", "relevancy": 2.3204, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4901}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4557}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-Task%20Continual%20Offline%20Reinforcement%20Learning&body=Title%3A%20Single-Task%20Continual%20Offline%20Reinforcement%20Learning%0AAuthor%3A%20Sibo%20Gai%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20continual%20learning%20problem%20of%20single-task%20offline%0Areinforcement%20learning.%20In%20the%20past%2C%20continual%20reinforcement%20learning%20usually%0Aonly%20dealt%20with%20multitasking%2C%20that%20is%2C%20learning%20multiple%20related%20or%20unrelated%0Atasks%20in%20a%20row%2C%20but%20once%20each%20learned%20task%20was%20learned%2C%20it%20was%20not%20relearned%2C%0Abut%20only%20used%20in%20subsequent%20processes.%20However%2C%20offline%20reinforcement%20learning%0Atasks%20require%20the%20continuously%20learning%20of%20multiple%20different%20datasets%20for%20the%0Asame%20task.%20Existing%20algorithms%20will%20try%20their%20best%20to%20achieve%20the%20best%20results%0Ain%20each%20offline%20dataset%20they%20have%20learned%20and%20the%20skills%20of%20the%20network%20will%0Aoverwrite%20the%20high-quality%20datasets%20that%20have%20been%20learned%20after%20learning%20the%0Asubsequent%20poor%20datasets.%20On%20the%20other%20hand%2C%20if%20too%20much%20emphasis%20is%20placed%20on%0Astability%2C%20the%20network%20will%20learn%20the%20subsequent%20better%20dataset%20after%20learning%0Athe%20poor%20offline%20dataset%2C%20and%20the%20problem%20of%20insufficient%20plasticity%20and%0Anon-learning%20will%20occur.%20How%20to%20design%20a%20strategy%20that%20can%20always%20preserve%20the%0Abest%20performance%20for%20each%20state%20in%20the%20data%20that%20has%20been%20learned%20is%20a%20new%0Achallenge%20and%20the%20focus%20of%20this%20study.%20Therefore%2C%20this%20study%20proposes%20a%20new%0Aalgorithm%2C%20called%20Ensemble%20Offline%20Reinforcement%20Learning%20Based%20on%20Experience%0AReplay%2C%20which%20introduces%20multiple%20value%20networks%20to%20learn%20the%20same%20dataset%20and%0Ajudge%20whether%20the%20strategy%20has%20been%20learned%20by%20the%20discrete%20degree%20of%20the%20value%0Anetwork%2C%20to%20improve%20the%20performance%20of%20the%20network%20in%20single-task%20offline%0Areinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-Task%2520Continual%2520Offline%2520Reinforcement%2520Learning%26entry.906535625%3DSibo%2520Gai%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520continual%2520learning%2520problem%2520of%2520single-task%2520offline%250Areinforcement%2520learning.%2520In%2520the%2520past%252C%2520continual%2520reinforcement%2520learning%2520usually%250Aonly%2520dealt%2520with%2520multitasking%252C%2520that%2520is%252C%2520learning%2520multiple%2520related%2520or%2520unrelated%250Atasks%2520in%2520a%2520row%252C%2520but%2520once%2520each%2520learned%2520task%2520was%2520learned%252C%2520it%2520was%2520not%2520relearned%252C%250Abut%2520only%2520used%2520in%2520subsequent%2520processes.%2520However%252C%2520offline%2520reinforcement%2520learning%250Atasks%2520require%2520the%2520continuously%2520learning%2520of%2520multiple%2520different%2520datasets%2520for%2520the%250Asame%2520task.%2520Existing%2520algorithms%2520will%2520try%2520their%2520best%2520to%2520achieve%2520the%2520best%2520results%250Ain%2520each%2520offline%2520dataset%2520they%2520have%2520learned%2520and%2520the%2520skills%2520of%2520the%2520network%2520will%250Aoverwrite%2520the%2520high-quality%2520datasets%2520that%2520have%2520been%2520learned%2520after%2520learning%2520the%250Asubsequent%2520poor%2520datasets.%2520On%2520the%2520other%2520hand%252C%2520if%2520too%2520much%2520emphasis%2520is%2520placed%2520on%250Astability%252C%2520the%2520network%2520will%2520learn%2520the%2520subsequent%2520better%2520dataset%2520after%2520learning%250Athe%2520poor%2520offline%2520dataset%252C%2520and%2520the%2520problem%2520of%2520insufficient%2520plasticity%2520and%250Anon-learning%2520will%2520occur.%2520How%2520to%2520design%2520a%2520strategy%2520that%2520can%2520always%2520preserve%2520the%250Abest%2520performance%2520for%2520each%2520state%2520in%2520the%2520data%2520that%2520has%2520been%2520learned%2520is%2520a%2520new%250Achallenge%2520and%2520the%2520focus%2520of%2520this%2520study.%2520Therefore%252C%2520this%2520study%2520proposes%2520a%2520new%250Aalgorithm%252C%2520called%2520Ensemble%2520Offline%2520Reinforcement%2520Learning%2520Based%2520on%2520Experience%250AReplay%252C%2520which%2520introduces%2520multiple%2520value%2520networks%2520to%2520learn%2520the%2520same%2520dataset%2520and%250Ajudge%2520whether%2520the%2520strategy%2520has%2520been%2520learned%2520by%2520the%2520discrete%2520degree%2520of%2520the%2520value%250Anetwork%252C%2520to%2520improve%2520the%2520performance%2520of%2520the%2520network%2520in%2520single-task%2520offline%250Areinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Task%20Continual%20Offline%20Reinforcement%20Learning&entry.906535625=Sibo%20Gai%20and%20Donglin%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20continual%20learning%20problem%20of%20single-task%20offline%0Areinforcement%20learning.%20In%20the%20past%2C%20continual%20reinforcement%20learning%20usually%0Aonly%20dealt%20with%20multitasking%2C%20that%20is%2C%20learning%20multiple%20related%20or%20unrelated%0Atasks%20in%20a%20row%2C%20but%20once%20each%20learned%20task%20was%20learned%2C%20it%20was%20not%20relearned%2C%0Abut%20only%20used%20in%20subsequent%20processes.%20However%2C%20offline%20reinforcement%20learning%0Atasks%20require%20the%20continuously%20learning%20of%20multiple%20different%20datasets%20for%20the%0Asame%20task.%20Existing%20algorithms%20will%20try%20their%20best%20to%20achieve%20the%20best%20results%0Ain%20each%20offline%20dataset%20they%20have%20learned%20and%20the%20skills%20of%20the%20network%20will%0Aoverwrite%20the%20high-quality%20datasets%20that%20have%20been%20learned%20after%20learning%20the%0Asubsequent%20poor%20datasets.%20On%20the%20other%20hand%2C%20if%20too%20much%20emphasis%20is%20placed%20on%0Astability%2C%20the%20network%20will%20learn%20the%20subsequent%20better%20dataset%20after%20learning%0Athe%20poor%20offline%20dataset%2C%20and%20the%20problem%20of%20insufficient%20plasticity%20and%0Anon-learning%20will%20occur.%20How%20to%20design%20a%20strategy%20that%20can%20always%20preserve%20the%0Abest%20performance%20for%20each%20state%20in%20the%20data%20that%20has%20been%20learned%20is%20a%20new%0Achallenge%20and%20the%20focus%20of%20this%20study.%20Therefore%2C%20this%20study%20proposes%20a%20new%0Aalgorithm%2C%20called%20Ensemble%20Offline%20Reinforcement%20Learning%20Based%20on%20Experience%0AReplay%2C%20which%20introduces%20multiple%20value%20networks%20to%20learn%20the%20same%20dataset%20and%0Ajudge%20whether%20the%20strategy%20has%20been%20learned%20by%20the%20discrete%20degree%20of%20the%20value%0Anetwork%2C%20to%20improve%20the%20performance%20of%20the%20network%20in%20single-task%20offline%0Areinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12639v2&entry.124074799=Read"},
{"title": "Histogram-Based Federated XGBoost using Minimal Variance Sampling for\n  Federated Tabular Data", "author": "William Lindskog and Christian Prehofer and Sarandeep Singh", "abstract": "  Federated Learning (FL) has gained considerable traction, yet, for tabular\ndata, FL has received less attention. Most FL research has focused on Neural\nNetworks while Tree-Based Models (TBMs) such as XGBoost have historically\nperformed better on tabular data. It has been shown that subsampling of\ntraining data when building trees can improve performance but it is an open\nproblem whether such subsampling can improve performance in FL. In this paper,\nwe evaluate a histogram-based federated XGBoost that uses Minimal Variance\nSampling (MVS). We demonstrate the underlying algorithm and show that our model\nusing MVS can improve performance in terms of accuracy and regression error in\na federated setting. In our evaluation, our model using MVS performs better\nthan uniform (random) sampling and no sampling at all. It achieves both\noutstanding local and global performance on a new set of federated tabular\ndatasets. Federated XGBoost using MVS also outperforms centralized XGBoost in\nhalf of the studied cases.\n", "link": "http://arxiv.org/abs/2405.02067v1", "date": "2024-05-03", "relevancy": 2.3135, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5274}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4392}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Histogram-Based%20Federated%20XGBoost%20using%20Minimal%20Variance%20Sampling%20for%0A%20%20Federated%20Tabular%20Data&body=Title%3A%20Histogram-Based%20Federated%20XGBoost%20using%20Minimal%20Variance%20Sampling%20for%0A%20%20Federated%20Tabular%20Data%0AAuthor%3A%20William%20Lindskog%20and%20Christian%20Prehofer%20and%20Sarandeep%20Singh%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20gained%20considerable%20traction%2C%20yet%2C%20for%20tabular%0Adata%2C%20FL%20has%20received%20less%20attention.%20Most%20FL%20research%20has%20focused%20on%20Neural%0ANetworks%20while%20Tree-Based%20Models%20%28TBMs%29%20such%20as%20XGBoost%20have%20historically%0Aperformed%20better%20on%20tabular%20data.%20It%20has%20been%20shown%20that%20subsampling%20of%0Atraining%20data%20when%20building%20trees%20can%20improve%20performance%20but%20it%20is%20an%20open%0Aproblem%20whether%20such%20subsampling%20can%20improve%20performance%20in%20FL.%20In%20this%20paper%2C%0Awe%20evaluate%20a%20histogram-based%20federated%20XGBoost%20that%20uses%20Minimal%20Variance%0ASampling%20%28MVS%29.%20We%20demonstrate%20the%20underlying%20algorithm%20and%20show%20that%20our%20model%0Ausing%20MVS%20can%20improve%20performance%20in%20terms%20of%20accuracy%20and%20regression%20error%20in%0Aa%20federated%20setting.%20In%20our%20evaluation%2C%20our%20model%20using%20MVS%20performs%20better%0Athan%20uniform%20%28random%29%20sampling%20and%20no%20sampling%20at%20all.%20It%20achieves%20both%0Aoutstanding%20local%20and%20global%20performance%20on%20a%20new%20set%20of%20federated%20tabular%0Adatasets.%20Federated%20XGBoost%20using%20MVS%20also%20outperforms%20centralized%20XGBoost%20in%0Ahalf%20of%20the%20studied%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistogram-Based%2520Federated%2520XGBoost%2520using%2520Minimal%2520Variance%2520Sampling%2520for%250A%2520%2520Federated%2520Tabular%2520Data%26entry.906535625%3DWilliam%2520Lindskog%2520and%2520Christian%2520Prehofer%2520and%2520Sarandeep%2520Singh%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520gained%2520considerable%2520traction%252C%2520yet%252C%2520for%2520tabular%250Adata%252C%2520FL%2520has%2520received%2520less%2520attention.%2520Most%2520FL%2520research%2520has%2520focused%2520on%2520Neural%250ANetworks%2520while%2520Tree-Based%2520Models%2520%2528TBMs%2529%2520such%2520as%2520XGBoost%2520have%2520historically%250Aperformed%2520better%2520on%2520tabular%2520data.%2520It%2520has%2520been%2520shown%2520that%2520subsampling%2520of%250Atraining%2520data%2520when%2520building%2520trees%2520can%2520improve%2520performance%2520but%2520it%2520is%2520an%2520open%250Aproblem%2520whether%2520such%2520subsampling%2520can%2520improve%2520performance%2520in%2520FL.%2520In%2520this%2520paper%252C%250Awe%2520evaluate%2520a%2520histogram-based%2520federated%2520XGBoost%2520that%2520uses%2520Minimal%2520Variance%250ASampling%2520%2528MVS%2529.%2520We%2520demonstrate%2520the%2520underlying%2520algorithm%2520and%2520show%2520that%2520our%2520model%250Ausing%2520MVS%2520can%2520improve%2520performance%2520in%2520terms%2520of%2520accuracy%2520and%2520regression%2520error%2520in%250Aa%2520federated%2520setting.%2520In%2520our%2520evaluation%252C%2520our%2520model%2520using%2520MVS%2520performs%2520better%250Athan%2520uniform%2520%2528random%2529%2520sampling%2520and%2520no%2520sampling%2520at%2520all.%2520It%2520achieves%2520both%250Aoutstanding%2520local%2520and%2520global%2520performance%2520on%2520a%2520new%2520set%2520of%2520federated%2520tabular%250Adatasets.%2520Federated%2520XGBoost%2520using%2520MVS%2520also%2520outperforms%2520centralized%2520XGBoost%2520in%250Ahalf%2520of%2520the%2520studied%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histogram-Based%20Federated%20XGBoost%20using%20Minimal%20Variance%20Sampling%20for%0A%20%20Federated%20Tabular%20Data&entry.906535625=William%20Lindskog%20and%20Christian%20Prehofer%20and%20Sarandeep%20Singh&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20gained%20considerable%20traction%2C%20yet%2C%20for%20tabular%0Adata%2C%20FL%20has%20received%20less%20attention.%20Most%20FL%20research%20has%20focused%20on%20Neural%0ANetworks%20while%20Tree-Based%20Models%20%28TBMs%29%20such%20as%20XGBoost%20have%20historically%0Aperformed%20better%20on%20tabular%20data.%20It%20has%20been%20shown%20that%20subsampling%20of%0Atraining%20data%20when%20building%20trees%20can%20improve%20performance%20but%20it%20is%20an%20open%0Aproblem%20whether%20such%20subsampling%20can%20improve%20performance%20in%20FL.%20In%20this%20paper%2C%0Awe%20evaluate%20a%20histogram-based%20federated%20XGBoost%20that%20uses%20Minimal%20Variance%0ASampling%20%28MVS%29.%20We%20demonstrate%20the%20underlying%20algorithm%20and%20show%20that%20our%20model%0Ausing%20MVS%20can%20improve%20performance%20in%20terms%20of%20accuracy%20and%20regression%20error%20in%0Aa%20federated%20setting.%20In%20our%20evaluation%2C%20our%20model%20using%20MVS%20performs%20better%0Athan%20uniform%20%28random%29%20sampling%20and%20no%20sampling%20at%20all.%20It%20achieves%20both%0Aoutstanding%20local%20and%20global%20performance%20on%20a%20new%20set%20of%20federated%20tabular%0Adatasets.%20Federated%20XGBoost%20using%20MVS%20also%20outperforms%20centralized%20XGBoost%20in%0Ahalf%20of%20the%20studied%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02067v1&entry.124074799=Read"},
{"title": "Visual Environment Assessment for Safe Autonomous Quadrotor Landing", "author": "Mattia Secchiero and Nishanth Bobbili and Yang Zhou and Giuseppe Loianno", "abstract": "  Autonomous identification and evaluation of safe landing zones are of\nparamount importance for ensuring the safety and effectiveness of aerial robots\nin the event of system failures, low battery, or the successful completion of\nspecific tasks. In this paper, we present a novel approach for detection and\nassessment of potential landing sites for safe quadrotor landing. Our solution\nefficiently integrates 2D and 3D environmental information, eliminating the\nneed for external aids such as GPS and computationally intensive elevation\nmaps. The proposed pipeline combines semantic data derived from a Neural\nNetwork (NN), to extract environmental features, with geometric data obtained\nfrom a disparity map, to extract critical geometric attributes such as slope,\nflatness, and roughness. We define several cost metrics based on these\nattributes to evaluate safety, stability, and suitability of regions in the\nenvironments and identify the most suitable landing area. Our approach runs in\nreal-time on quadrotors equipped with limited computational capabilities.\nExperimental results conducted in diverse environments demonstrate that the\nproposed method can effectively assess and identify suitable landing areas,\nenabling the safe and autonomous landing of a quadrotor.\n", "link": "http://arxiv.org/abs/2311.10065v3", "date": "2024-05-03", "relevancy": 2.2886, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5786}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Environment%20Assessment%20for%20Safe%20Autonomous%20Quadrotor%20Landing&body=Title%3A%20Visual%20Environment%20Assessment%20for%20Safe%20Autonomous%20Quadrotor%20Landing%0AAuthor%3A%20Mattia%20Secchiero%20and%20Nishanth%20Bobbili%20and%20Yang%20Zhou%20and%20Giuseppe%20Loianno%0AAbstract%3A%20%20%20Autonomous%20identification%20and%20evaluation%20of%20safe%20landing%20zones%20are%20of%0Aparamount%20importance%20for%20ensuring%20the%20safety%20and%20effectiveness%20of%20aerial%20robots%0Ain%20the%20event%20of%20system%20failures%2C%20low%20battery%2C%20or%20the%20successful%20completion%20of%0Aspecific%20tasks.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20for%20detection%20and%0Aassessment%20of%20potential%20landing%20sites%20for%20safe%20quadrotor%20landing.%20Our%20solution%0Aefficiently%20integrates%202D%20and%203D%20environmental%20information%2C%20eliminating%20the%0Aneed%20for%20external%20aids%20such%20as%20GPS%20and%20computationally%20intensive%20elevation%0Amaps.%20The%20proposed%20pipeline%20combines%20semantic%20data%20derived%20from%20a%20Neural%0ANetwork%20%28NN%29%2C%20to%20extract%20environmental%20features%2C%20with%20geometric%20data%20obtained%0Afrom%20a%20disparity%20map%2C%20to%20extract%20critical%20geometric%20attributes%20such%20as%20slope%2C%0Aflatness%2C%20and%20roughness.%20We%20define%20several%20cost%20metrics%20based%20on%20these%0Aattributes%20to%20evaluate%20safety%2C%20stability%2C%20and%20suitability%20of%20regions%20in%20the%0Aenvironments%20and%20identify%20the%20most%20suitable%20landing%20area.%20Our%20approach%20runs%20in%0Areal-time%20on%20quadrotors%20equipped%20with%20limited%20computational%20capabilities.%0AExperimental%20results%20conducted%20in%20diverse%20environments%20demonstrate%20that%20the%0Aproposed%20method%20can%20effectively%20assess%20and%20identify%20suitable%20landing%20areas%2C%0Aenabling%20the%20safe%20and%20autonomous%20landing%20of%20a%20quadrotor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10065v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Environment%2520Assessment%2520for%2520Safe%2520Autonomous%2520Quadrotor%2520Landing%26entry.906535625%3DMattia%2520Secchiero%2520and%2520Nishanth%2520Bobbili%2520and%2520Yang%2520Zhou%2520and%2520Giuseppe%2520Loianno%26entry.1292438233%3D%2520%2520Autonomous%2520identification%2520and%2520evaluation%2520of%2520safe%2520landing%2520zones%2520are%2520of%250Aparamount%2520importance%2520for%2520ensuring%2520the%2520safety%2520and%2520effectiveness%2520of%2520aerial%2520robots%250Ain%2520the%2520event%2520of%2520system%2520failures%252C%2520low%2520battery%252C%2520or%2520the%2520successful%2520completion%2520of%250Aspecific%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520for%2520detection%2520and%250Aassessment%2520of%2520potential%2520landing%2520sites%2520for%2520safe%2520quadrotor%2520landing.%2520Our%2520solution%250Aefficiently%2520integrates%25202D%2520and%25203D%2520environmental%2520information%252C%2520eliminating%2520the%250Aneed%2520for%2520external%2520aids%2520such%2520as%2520GPS%2520and%2520computationally%2520intensive%2520elevation%250Amaps.%2520The%2520proposed%2520pipeline%2520combines%2520semantic%2520data%2520derived%2520from%2520a%2520Neural%250ANetwork%2520%2528NN%2529%252C%2520to%2520extract%2520environmental%2520features%252C%2520with%2520geometric%2520data%2520obtained%250Afrom%2520a%2520disparity%2520map%252C%2520to%2520extract%2520critical%2520geometric%2520attributes%2520such%2520as%2520slope%252C%250Aflatness%252C%2520and%2520roughness.%2520We%2520define%2520several%2520cost%2520metrics%2520based%2520on%2520these%250Aattributes%2520to%2520evaluate%2520safety%252C%2520stability%252C%2520and%2520suitability%2520of%2520regions%2520in%2520the%250Aenvironments%2520and%2520identify%2520the%2520most%2520suitable%2520landing%2520area.%2520Our%2520approach%2520runs%2520in%250Areal-time%2520on%2520quadrotors%2520equipped%2520with%2520limited%2520computational%2520capabilities.%250AExperimental%2520results%2520conducted%2520in%2520diverse%2520environments%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520can%2520effectively%2520assess%2520and%2520identify%2520suitable%2520landing%2520areas%252C%250Aenabling%2520the%2520safe%2520and%2520autonomous%2520landing%2520of%2520a%2520quadrotor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10065v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Environment%20Assessment%20for%20Safe%20Autonomous%20Quadrotor%20Landing&entry.906535625=Mattia%20Secchiero%20and%20Nishanth%20Bobbili%20and%20Yang%20Zhou%20and%20Giuseppe%20Loianno&entry.1292438233=%20%20Autonomous%20identification%20and%20evaluation%20of%20safe%20landing%20zones%20are%20of%0Aparamount%20importance%20for%20ensuring%20the%20safety%20and%20effectiveness%20of%20aerial%20robots%0Ain%20the%20event%20of%20system%20failures%2C%20low%20battery%2C%20or%20the%20successful%20completion%20of%0Aspecific%20tasks.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20for%20detection%20and%0Aassessment%20of%20potential%20landing%20sites%20for%20safe%20quadrotor%20landing.%20Our%20solution%0Aefficiently%20integrates%202D%20and%203D%20environmental%20information%2C%20eliminating%20the%0Aneed%20for%20external%20aids%20such%20as%20GPS%20and%20computationally%20intensive%20elevation%0Amaps.%20The%20proposed%20pipeline%20combines%20semantic%20data%20derived%20from%20a%20Neural%0ANetwork%20%28NN%29%2C%20to%20extract%20environmental%20features%2C%20with%20geometric%20data%20obtained%0Afrom%20a%20disparity%20map%2C%20to%20extract%20critical%20geometric%20attributes%20such%20as%20slope%2C%0Aflatness%2C%20and%20roughness.%20We%20define%20several%20cost%20metrics%20based%20on%20these%0Aattributes%20to%20evaluate%20safety%2C%20stability%2C%20and%20suitability%20of%20regions%20in%20the%0Aenvironments%20and%20identify%20the%20most%20suitable%20landing%20area.%20Our%20approach%20runs%20in%0Areal-time%20on%20quadrotors%20equipped%20with%20limited%20computational%20capabilities.%0AExperimental%20results%20conducted%20in%20diverse%20environments%20demonstrate%20that%20the%0Aproposed%20method%20can%20effectively%20assess%20and%20identify%20suitable%20landing%20areas%2C%0Aenabling%20the%20safe%20and%20autonomous%20landing%20of%20a%20quadrotor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10065v3&entry.124074799=Read"},
{"title": "Accurate Pose Prediction on Signed Distance Fields for Mobile Ground\n  Robots in Rough Terrain", "author": "Martin Oehler and Oskar von Stryk", "abstract": "  Autonomous locomotion for mobile ground robots in unstructured environments\nsuch as waypoint navigation or flipper control requires a sufficiently accurate\nprediction of the robot-terrain interaction. Heuristics like occupancy grids or\ntraversability maps are widely used but limit actions available to robots with\nactive flippers as joint positions are not taken into account. We present a\nnovel iterative geometric method to predict the 3D pose of mobile ground robots\nwith active flippers on uneven ground with high accuracy and online planning\ncapabilities. This is achieved by utilizing the ability of signed distance\nfields to represent surfaces with sub-voxel accuracy. The effectiveness of the\npresented approach is demonstrated on two different tracked robots in\nsimulation and on a real platform. Compared to a tracking system as ground\ntruth, our method predicts the robot position and orientation with an average\naccuracy of 3.11 cm and 3.91{\\deg}, outperforming a recent heightmap-based\napproach. The implementation is made available as an open-source ROS package.\n", "link": "http://arxiv.org/abs/2405.02121v1", "date": "2024-05-03", "relevancy": 2.2723, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5561}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accurate%20Pose%20Prediction%20on%20Signed%20Distance%20Fields%20for%20Mobile%20Ground%0A%20%20Robots%20in%20Rough%20Terrain&body=Title%3A%20Accurate%20Pose%20Prediction%20on%20Signed%20Distance%20Fields%20for%20Mobile%20Ground%0A%20%20Robots%20in%20Rough%20Terrain%0AAuthor%3A%20Martin%20Oehler%20and%20Oskar%20von%20Stryk%0AAbstract%3A%20%20%20Autonomous%20locomotion%20for%20mobile%20ground%20robots%20in%20unstructured%20environments%0Asuch%20as%20waypoint%20navigation%20or%20flipper%20control%20requires%20a%20sufficiently%20accurate%0Aprediction%20of%20the%20robot-terrain%20interaction.%20Heuristics%20like%20occupancy%20grids%20or%0Atraversability%20maps%20are%20widely%20used%20but%20limit%20actions%20available%20to%20robots%20with%0Aactive%20flippers%20as%20joint%20positions%20are%20not%20taken%20into%20account.%20We%20present%20a%0Anovel%20iterative%20geometric%20method%20to%20predict%20the%203D%20pose%20of%20mobile%20ground%20robots%0Awith%20active%20flippers%20on%20uneven%20ground%20with%20high%20accuracy%20and%20online%20planning%0Acapabilities.%20This%20is%20achieved%20by%20utilizing%20the%20ability%20of%20signed%20distance%0Afields%20to%20represent%20surfaces%20with%20sub-voxel%20accuracy.%20The%20effectiveness%20of%20the%0Apresented%20approach%20is%20demonstrated%20on%20two%20different%20tracked%20robots%20in%0Asimulation%20and%20on%20a%20real%20platform.%20Compared%20to%20a%20tracking%20system%20as%20ground%0Atruth%2C%20our%20method%20predicts%20the%20robot%20position%20and%20orientation%20with%20an%20average%0Aaccuracy%20of%203.11%20cm%20and%203.91%7B%5Cdeg%7D%2C%20outperforming%20a%20recent%20heightmap-based%0Aapproach.%20The%20implementation%20is%20made%20available%20as%20an%20open-source%20ROS%20package.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccurate%2520Pose%2520Prediction%2520on%2520Signed%2520Distance%2520Fields%2520for%2520Mobile%2520Ground%250A%2520%2520Robots%2520in%2520Rough%2520Terrain%26entry.906535625%3DMartin%2520Oehler%2520and%2520Oskar%2520von%2520Stryk%26entry.1292438233%3D%2520%2520Autonomous%2520locomotion%2520for%2520mobile%2520ground%2520robots%2520in%2520unstructured%2520environments%250Asuch%2520as%2520waypoint%2520navigation%2520or%2520flipper%2520control%2520requires%2520a%2520sufficiently%2520accurate%250Aprediction%2520of%2520the%2520robot-terrain%2520interaction.%2520Heuristics%2520like%2520occupancy%2520grids%2520or%250Atraversability%2520maps%2520are%2520widely%2520used%2520but%2520limit%2520actions%2520available%2520to%2520robots%2520with%250Aactive%2520flippers%2520as%2520joint%2520positions%2520are%2520not%2520taken%2520into%2520account.%2520We%2520present%2520a%250Anovel%2520iterative%2520geometric%2520method%2520to%2520predict%2520the%25203D%2520pose%2520of%2520mobile%2520ground%2520robots%250Awith%2520active%2520flippers%2520on%2520uneven%2520ground%2520with%2520high%2520accuracy%2520and%2520online%2520planning%250Acapabilities.%2520This%2520is%2520achieved%2520by%2520utilizing%2520the%2520ability%2520of%2520signed%2520distance%250Afields%2520to%2520represent%2520surfaces%2520with%2520sub-voxel%2520accuracy.%2520The%2520effectiveness%2520of%2520the%250Apresented%2520approach%2520is%2520demonstrated%2520on%2520two%2520different%2520tracked%2520robots%2520in%250Asimulation%2520and%2520on%2520a%2520real%2520platform.%2520Compared%2520to%2520a%2520tracking%2520system%2520as%2520ground%250Atruth%252C%2520our%2520method%2520predicts%2520the%2520robot%2520position%2520and%2520orientation%2520with%2520an%2520average%250Aaccuracy%2520of%25203.11%2520cm%2520and%25203.91%257B%255Cdeg%257D%252C%2520outperforming%2520a%2520recent%2520heightmap-based%250Aapproach.%2520The%2520implementation%2520is%2520made%2520available%2520as%2520an%2520open-source%2520ROS%2520package.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20Pose%20Prediction%20on%20Signed%20Distance%20Fields%20for%20Mobile%20Ground%0A%20%20Robots%20in%20Rough%20Terrain&entry.906535625=Martin%20Oehler%20and%20Oskar%20von%20Stryk&entry.1292438233=%20%20Autonomous%20locomotion%20for%20mobile%20ground%20robots%20in%20unstructured%20environments%0Asuch%20as%20waypoint%20navigation%20or%20flipper%20control%20requires%20a%20sufficiently%20accurate%0Aprediction%20of%20the%20robot-terrain%20interaction.%20Heuristics%20like%20occupancy%20grids%20or%0Atraversability%20maps%20are%20widely%20used%20but%20limit%20actions%20available%20to%20robots%20with%0Aactive%20flippers%20as%20joint%20positions%20are%20not%20taken%20into%20account.%20We%20present%20a%0Anovel%20iterative%20geometric%20method%20to%20predict%20the%203D%20pose%20of%20mobile%20ground%20robots%0Awith%20active%20flippers%20on%20uneven%20ground%20with%20high%20accuracy%20and%20online%20planning%0Acapabilities.%20This%20is%20achieved%20by%20utilizing%20the%20ability%20of%20signed%20distance%0Afields%20to%20represent%20surfaces%20with%20sub-voxel%20accuracy.%20The%20effectiveness%20of%20the%0Apresented%20approach%20is%20demonstrated%20on%20two%20different%20tracked%20robots%20in%0Asimulation%20and%20on%20a%20real%20platform.%20Compared%20to%20a%20tracking%20system%20as%20ground%0Atruth%2C%20our%20method%20predicts%20the%20robot%20position%20and%20orientation%20with%20an%20average%0Aaccuracy%20of%203.11%20cm%20and%203.91%7B%5Cdeg%7D%2C%20outperforming%20a%20recent%20heightmap-based%0Aapproach.%20The%20implementation%20is%20made%20available%20as%20an%20open-source%20ROS%20package.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02121v1&entry.124074799=Read"},
{"title": "Improving Interpretation Faithfulness for Vision Transformers", "author": "Lijie Hu and Yixin Liu and Ninghao Liu and Mengdi Huai and Lichao Sun and Di Wang", "abstract": "  Vision Transformers (ViTs) have achieved state-of-the-art performance for\nvarious vision tasks. One reason behind the success lies in their ability to\nprovide plausible innate explanations for the behavior of neural architectures.\nHowever, ViTs suffer from issues with explanation faithfulness, as their focal\npoints are fragile to adversarial attacks and can be easily changed with even\nslight perturbations on the input image. In this paper, we propose a rigorous\napproach to mitigate these issues by introducing Faithful ViTs (FViTs). Briefly\nspeaking, an FViT should have the following two properties: (1) The top-$k$\nindices of its self-attention vector should remain mostly unchanged under input\nperturbation, indicating stable explanations; (2) The prediction distribution\nshould be robust to perturbations. To achieve this, we propose a new method\ncalled Denoised Diffusion Smoothing (DDS), which adopts randomized smoothing\nand diffusion-based denoising. We theoretically prove that processing ViTs\ndirectly with DDS can turn them into FViTs. We also show that Gaussian noise is\nnearly optimal for both $\\ell_2$ and $\\ell_\\infty$-norm cases. Finally, we\ndemonstrate the effectiveness of our approach through comprehensive experiments\nand evaluations. Results show that FViTs are more robust against adversarial\nattacks while maintaining the explainability of attention, indicating higher\nfaithfulness.\n", "link": "http://arxiv.org/abs/2311.17983v2", "date": "2024-05-03", "relevancy": 2.2588, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5982}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5639}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Interpretation%20Faithfulness%20for%20Vision%20Transformers&body=Title%3A%20Improving%20Interpretation%20Faithfulness%20for%20Vision%20Transformers%0AAuthor%3A%20Lijie%20Hu%20and%20Yixin%20Liu%20and%20Ninghao%20Liu%20and%20Mengdi%20Huai%20and%20Lichao%20Sun%20and%20Di%20Wang%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20state-of-the-art%20performance%20for%0Avarious%20vision%20tasks.%20One%20reason%20behind%20the%20success%20lies%20in%20their%20ability%20to%0Aprovide%20plausible%20innate%20explanations%20for%20the%20behavior%20of%20neural%20architectures.%0AHowever%2C%20ViTs%20suffer%20from%20issues%20with%20explanation%20faithfulness%2C%20as%20their%20focal%0Apoints%20are%20fragile%20to%20adversarial%20attacks%20and%20can%20be%20easily%20changed%20with%20even%0Aslight%20perturbations%20on%20the%20input%20image.%20In%20this%20paper%2C%20we%20propose%20a%20rigorous%0Aapproach%20to%20mitigate%20these%20issues%20by%20introducing%20Faithful%20ViTs%20%28FViTs%29.%20Briefly%0Aspeaking%2C%20an%20FViT%20should%20have%20the%20following%20two%20properties%3A%20%281%29%20The%20top-%24k%24%0Aindices%20of%20its%20self-attention%20vector%20should%20remain%20mostly%20unchanged%20under%20input%0Aperturbation%2C%20indicating%20stable%20explanations%3B%20%282%29%20The%20prediction%20distribution%0Ashould%20be%20robust%20to%20perturbations.%20To%20achieve%20this%2C%20we%20propose%20a%20new%20method%0Acalled%20Denoised%20Diffusion%20Smoothing%20%28DDS%29%2C%20which%20adopts%20randomized%20smoothing%0Aand%20diffusion-based%20denoising.%20We%20theoretically%20prove%20that%20processing%20ViTs%0Adirectly%20with%20DDS%20can%20turn%20them%20into%20FViTs.%20We%20also%20show%20that%20Gaussian%20noise%20is%0Anearly%20optimal%20for%20both%20%24%5Cell_2%24%20and%20%24%5Cell_%5Cinfty%24-norm%20cases.%20Finally%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20through%20comprehensive%20experiments%0Aand%20evaluations.%20Results%20show%20that%20FViTs%20are%20more%20robust%20against%20adversarial%0Aattacks%20while%20maintaining%20the%20explainability%20of%20attention%2C%20indicating%20higher%0Afaithfulness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Interpretation%2520Faithfulness%2520for%2520Vision%2520Transformers%26entry.906535625%3DLijie%2520Hu%2520and%2520Yixin%2520Liu%2520and%2520Ninghao%2520Liu%2520and%2520Mengdi%2520Huai%2520and%2520Lichao%2520Sun%2520and%2520Di%2520Wang%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520achieved%2520state-of-the-art%2520performance%2520for%250Avarious%2520vision%2520tasks.%2520One%2520reason%2520behind%2520the%2520success%2520lies%2520in%2520their%2520ability%2520to%250Aprovide%2520plausible%2520innate%2520explanations%2520for%2520the%2520behavior%2520of%2520neural%2520architectures.%250AHowever%252C%2520ViTs%2520suffer%2520from%2520issues%2520with%2520explanation%2520faithfulness%252C%2520as%2520their%2520focal%250Apoints%2520are%2520fragile%2520to%2520adversarial%2520attacks%2520and%2520can%2520be%2520easily%2520changed%2520with%2520even%250Aslight%2520perturbations%2520on%2520the%2520input%2520image.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520rigorous%250Aapproach%2520to%2520mitigate%2520these%2520issues%2520by%2520introducing%2520Faithful%2520ViTs%2520%2528FViTs%2529.%2520Briefly%250Aspeaking%252C%2520an%2520FViT%2520should%2520have%2520the%2520following%2520two%2520properties%253A%2520%25281%2529%2520The%2520top-%2524k%2524%250Aindices%2520of%2520its%2520self-attention%2520vector%2520should%2520remain%2520mostly%2520unchanged%2520under%2520input%250Aperturbation%252C%2520indicating%2520stable%2520explanations%253B%2520%25282%2529%2520The%2520prediction%2520distribution%250Ashould%2520be%2520robust%2520to%2520perturbations.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%2520new%2520method%250Acalled%2520Denoised%2520Diffusion%2520Smoothing%2520%2528DDS%2529%252C%2520which%2520adopts%2520randomized%2520smoothing%250Aand%2520diffusion-based%2520denoising.%2520We%2520theoretically%2520prove%2520that%2520processing%2520ViTs%250Adirectly%2520with%2520DDS%2520can%2520turn%2520them%2520into%2520FViTs.%2520We%2520also%2520show%2520that%2520Gaussian%2520noise%2520is%250Anearly%2520optimal%2520for%2520both%2520%2524%255Cell_2%2524%2520and%2520%2524%255Cell_%255Cinfty%2524-norm%2520cases.%2520Finally%252C%2520we%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520through%2520comprehensive%2520experiments%250Aand%2520evaluations.%2520Results%2520show%2520that%2520FViTs%2520are%2520more%2520robust%2520against%2520adversarial%250Aattacks%2520while%2520maintaining%2520the%2520explainability%2520of%2520attention%252C%2520indicating%2520higher%250Afaithfulness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Interpretation%20Faithfulness%20for%20Vision%20Transformers&entry.906535625=Lijie%20Hu%20and%20Yixin%20Liu%20and%20Ninghao%20Liu%20and%20Mengdi%20Huai%20and%20Lichao%20Sun%20and%20Di%20Wang&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20state-of-the-art%20performance%20for%0Avarious%20vision%20tasks.%20One%20reason%20behind%20the%20success%20lies%20in%20their%20ability%20to%0Aprovide%20plausible%20innate%20explanations%20for%20the%20behavior%20of%20neural%20architectures.%0AHowever%2C%20ViTs%20suffer%20from%20issues%20with%20explanation%20faithfulness%2C%20as%20their%20focal%0Apoints%20are%20fragile%20to%20adversarial%20attacks%20and%20can%20be%20easily%20changed%20with%20even%0Aslight%20perturbations%20on%20the%20input%20image.%20In%20this%20paper%2C%20we%20propose%20a%20rigorous%0Aapproach%20to%20mitigate%20these%20issues%20by%20introducing%20Faithful%20ViTs%20%28FViTs%29.%20Briefly%0Aspeaking%2C%20an%20FViT%20should%20have%20the%20following%20two%20properties%3A%20%281%29%20The%20top-%24k%24%0Aindices%20of%20its%20self-attention%20vector%20should%20remain%20mostly%20unchanged%20under%20input%0Aperturbation%2C%20indicating%20stable%20explanations%3B%20%282%29%20The%20prediction%20distribution%0Ashould%20be%20robust%20to%20perturbations.%20To%20achieve%20this%2C%20we%20propose%20a%20new%20method%0Acalled%20Denoised%20Diffusion%20Smoothing%20%28DDS%29%2C%20which%20adopts%20randomized%20smoothing%0Aand%20diffusion-based%20denoising.%20We%20theoretically%20prove%20that%20processing%20ViTs%0Adirectly%20with%20DDS%20can%20turn%20them%20into%20FViTs.%20We%20also%20show%20that%20Gaussian%20noise%20is%0Anearly%20optimal%20for%20both%20%24%5Cell_2%24%20and%20%24%5Cell_%5Cinfty%24-norm%20cases.%20Finally%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20through%20comprehensive%20experiments%0Aand%20evaluations.%20Results%20show%20that%20FViTs%20are%20more%20robust%20against%20adversarial%0Aattacks%20while%20maintaining%20the%20explainability%20of%20attention%2C%20indicating%20higher%0Afaithfulness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17983v2&entry.124074799=Read"},
{"title": "DiffMap: Enhancing Map Segmentation with Map Prior Using Diffusion Model", "author": "Peijin Jia and Tuopu Wen and Ziang Luo and Mengmeng Yang and Kun Jiang and Zhiquan Lei and Xuewei Tang and Ziyuan Liu and Le Cui and Kehua Sheng and Bo Zhang and Diange Yang", "abstract": "  Constructing high-definition (HD) maps is a crucial requirement for enabling\nautonomous driving. In recent years, several map segmentation algorithms have\nbeen developed to address this need, leveraging advancements in Bird's-Eye View\n(BEV) perception. However, existing models still encounter challenges in\nproducing realistic and consistent semantic map layouts. One prominent issue is\nthe limited utilization of structured priors inherent in map segmentation\nmasks. In light of this, we propose DiffMap, a novel approach specifically\ndesigned to model the structured priors of map segmentation masks using latent\ndiffusion model. By incorporating this technique, the performance of existing\nsemantic segmentation methods can be significantly enhanced and certain\nstructural errors present in the segmentation outputs can be effectively\nrectified. Notably, the proposed module can be seamlessly integrated into any\nmap segmentation model, thereby augmenting its capability to accurately\ndelineate semantic information. Furthermore, through extensive visualization\nanalysis, our model demonstrates superior proficiency in generating results\nthat more accurately reflect real-world map layouts, further validating its\nefficacy in improving the quality of the generated maps.\n", "link": "http://arxiv.org/abs/2405.02008v1", "date": "2024-05-03", "relevancy": 2.2508, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5958}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5732}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffMap%3A%20Enhancing%20Map%20Segmentation%20with%20Map%20Prior%20Using%20Diffusion%20Model&body=Title%3A%20DiffMap%3A%20Enhancing%20Map%20Segmentation%20with%20Map%20Prior%20Using%20Diffusion%20Model%0AAuthor%3A%20Peijin%20Jia%20and%20Tuopu%20Wen%20and%20Ziang%20Luo%20and%20Mengmeng%20Yang%20and%20Kun%20Jiang%20and%20Zhiquan%20Lei%20and%20Xuewei%20Tang%20and%20Ziyuan%20Liu%20and%20Le%20Cui%20and%20Kehua%20Sheng%20and%20Bo%20Zhang%20and%20Diange%20Yang%0AAbstract%3A%20%20%20Constructing%20high-definition%20%28HD%29%20maps%20is%20a%20crucial%20requirement%20for%20enabling%0Aautonomous%20driving.%20In%20recent%20years%2C%20several%20map%20segmentation%20algorithms%20have%0Abeen%20developed%20to%20address%20this%20need%2C%20leveraging%20advancements%20in%20Bird%27s-Eye%20View%0A%28BEV%29%20perception.%20However%2C%20existing%20models%20still%20encounter%20challenges%20in%0Aproducing%20realistic%20and%20consistent%20semantic%20map%20layouts.%20One%20prominent%20issue%20is%0Athe%20limited%20utilization%20of%20structured%20priors%20inherent%20in%20map%20segmentation%0Amasks.%20In%20light%20of%20this%2C%20we%20propose%20DiffMap%2C%20a%20novel%20approach%20specifically%0Adesigned%20to%20model%20the%20structured%20priors%20of%20map%20segmentation%20masks%20using%20latent%0Adiffusion%20model.%20By%20incorporating%20this%20technique%2C%20the%20performance%20of%20existing%0Asemantic%20segmentation%20methods%20can%20be%20significantly%20enhanced%20and%20certain%0Astructural%20errors%20present%20in%20the%20segmentation%20outputs%20can%20be%20effectively%0Arectified.%20Notably%2C%20the%20proposed%20module%20can%20be%20seamlessly%20integrated%20into%20any%0Amap%20segmentation%20model%2C%20thereby%20augmenting%20its%20capability%20to%20accurately%0Adelineate%20semantic%20information.%20Furthermore%2C%20through%20extensive%20visualization%0Aanalysis%2C%20our%20model%20demonstrates%20superior%20proficiency%20in%20generating%20results%0Athat%20more%20accurately%20reflect%20real-world%20map%20layouts%2C%20further%20validating%20its%0Aefficacy%20in%20improving%20the%20quality%20of%20the%20generated%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffMap%253A%2520Enhancing%2520Map%2520Segmentation%2520with%2520Map%2520Prior%2520Using%2520Diffusion%2520Model%26entry.906535625%3DPeijin%2520Jia%2520and%2520Tuopu%2520Wen%2520and%2520Ziang%2520Luo%2520and%2520Mengmeng%2520Yang%2520and%2520Kun%2520Jiang%2520and%2520Zhiquan%2520Lei%2520and%2520Xuewei%2520Tang%2520and%2520Ziyuan%2520Liu%2520and%2520Le%2520Cui%2520and%2520Kehua%2520Sheng%2520and%2520Bo%2520Zhang%2520and%2520Diange%2520Yang%26entry.1292438233%3D%2520%2520Constructing%2520high-definition%2520%2528HD%2529%2520maps%2520is%2520a%2520crucial%2520requirement%2520for%2520enabling%250Aautonomous%2520driving.%2520In%2520recent%2520years%252C%2520several%2520map%2520segmentation%2520algorithms%2520have%250Abeen%2520developed%2520to%2520address%2520this%2520need%252C%2520leveraging%2520advancements%2520in%2520Bird%2527s-Eye%2520View%250A%2528BEV%2529%2520perception.%2520However%252C%2520existing%2520models%2520still%2520encounter%2520challenges%2520in%250Aproducing%2520realistic%2520and%2520consistent%2520semantic%2520map%2520layouts.%2520One%2520prominent%2520issue%2520is%250Athe%2520limited%2520utilization%2520of%2520structured%2520priors%2520inherent%2520in%2520map%2520segmentation%250Amasks.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520DiffMap%252C%2520a%2520novel%2520approach%2520specifically%250Adesigned%2520to%2520model%2520the%2520structured%2520priors%2520of%2520map%2520segmentation%2520masks%2520using%2520latent%250Adiffusion%2520model.%2520By%2520incorporating%2520this%2520technique%252C%2520the%2520performance%2520of%2520existing%250Asemantic%2520segmentation%2520methods%2520can%2520be%2520significantly%2520enhanced%2520and%2520certain%250Astructural%2520errors%2520present%2520in%2520the%2520segmentation%2520outputs%2520can%2520be%2520effectively%250Arectified.%2520Notably%252C%2520the%2520proposed%2520module%2520can%2520be%2520seamlessly%2520integrated%2520into%2520any%250Amap%2520segmentation%2520model%252C%2520thereby%2520augmenting%2520its%2520capability%2520to%2520accurately%250Adelineate%2520semantic%2520information.%2520Furthermore%252C%2520through%2520extensive%2520visualization%250Aanalysis%252C%2520our%2520model%2520demonstrates%2520superior%2520proficiency%2520in%2520generating%2520results%250Athat%2520more%2520accurately%2520reflect%2520real-world%2520map%2520layouts%252C%2520further%2520validating%2520its%250Aefficacy%2520in%2520improving%2520the%2520quality%2520of%2520the%2520generated%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffMap%3A%20Enhancing%20Map%20Segmentation%20with%20Map%20Prior%20Using%20Diffusion%20Model&entry.906535625=Peijin%20Jia%20and%20Tuopu%20Wen%20and%20Ziang%20Luo%20and%20Mengmeng%20Yang%20and%20Kun%20Jiang%20and%20Zhiquan%20Lei%20and%20Xuewei%20Tang%20and%20Ziyuan%20Liu%20and%20Le%20Cui%20and%20Kehua%20Sheng%20and%20Bo%20Zhang%20and%20Diange%20Yang&entry.1292438233=%20%20Constructing%20high-definition%20%28HD%29%20maps%20is%20a%20crucial%20requirement%20for%20enabling%0Aautonomous%20driving.%20In%20recent%20years%2C%20several%20map%20segmentation%20algorithms%20have%0Abeen%20developed%20to%20address%20this%20need%2C%20leveraging%20advancements%20in%20Bird%27s-Eye%20View%0A%28BEV%29%20perception.%20However%2C%20existing%20models%20still%20encounter%20challenges%20in%0Aproducing%20realistic%20and%20consistent%20semantic%20map%20layouts.%20One%20prominent%20issue%20is%0Athe%20limited%20utilization%20of%20structured%20priors%20inherent%20in%20map%20segmentation%0Amasks.%20In%20light%20of%20this%2C%20we%20propose%20DiffMap%2C%20a%20novel%20approach%20specifically%0Adesigned%20to%20model%20the%20structured%20priors%20of%20map%20segmentation%20masks%20using%20latent%0Adiffusion%20model.%20By%20incorporating%20this%20technique%2C%20the%20performance%20of%20existing%0Asemantic%20segmentation%20methods%20can%20be%20significantly%20enhanced%20and%20certain%0Astructural%20errors%20present%20in%20the%20segmentation%20outputs%20can%20be%20effectively%0Arectified.%20Notably%2C%20the%20proposed%20module%20can%20be%20seamlessly%20integrated%20into%20any%0Amap%20segmentation%20model%2C%20thereby%20augmenting%20its%20capability%20to%20accurately%0Adelineate%20semantic%20information.%20Furthermore%2C%20through%20extensive%20visualization%0Aanalysis%2C%20our%20model%20demonstrates%20superior%20proficiency%20in%20generating%20results%0Athat%20more%20accurately%20reflect%20real-world%20map%20layouts%2C%20further%20validating%20its%0Aefficacy%20in%20improving%20the%20quality%20of%20the%20generated%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02008v1&entry.124074799=Read"},
{"title": "MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot\n  Action Recognition", "author": "Hongyu Qu and Rui Yan and Xiangbo Shu and Haoliang Gao and Peng Huang and Guo-Sen Xie", "abstract": "  Recent few-shot action recognition (FSAR) methods achieve promising\nperformance by performing semantic matching on learned discriminative features.\nHowever, most FSAR methods focus on single-scale (e.g., frame-level,\nsegment-level, \\etc) feature alignment, which ignores that human actions with\nthe same semantic may appear at different velocities. To this end, we develop a\nnovel Multi-Velocity Progressive-alignment (MVP-Shot) framework to\nprogressively learn and align semantic-related action features at\nmulti-velocity levels. Concretely, a Multi-Velocity Feature Alignment (MVFA)\nmodule is designed to measure the similarity between features from support and\nquery videos with different velocity scales and then merge all similarity\nscores in a residual fashion. To avoid the multiple velocity features deviating\nfrom the underlying motion semantic, our proposed Progressive Semantic-Tailored\nInteraction (PSTI) module injects velocity-tailored text information into the\nvideo feature via feature interaction on channel and temporal domains at\ndifferent velocities. The above two modules compensate for each other to\npredict query categories more accurately under the few-shot settings.\nExperimental results show our method outperforms current state-of-the-art\nmethods on multiple standard few-shot benchmarks (i.e., HMDB51, UCF101,\nKinetics, and SSv2-small).\n", "link": "http://arxiv.org/abs/2405.02077v1", "date": "2024-05-03", "relevancy": 2.2025, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5541}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVP-Shot%3A%20Multi-Velocity%20Progressive-Alignment%20Framework%20for%20Few-Shot%0A%20%20Action%20Recognition&body=Title%3A%20MVP-Shot%3A%20Multi-Velocity%20Progressive-Alignment%20Framework%20for%20Few-Shot%0A%20%20Action%20Recognition%0AAuthor%3A%20Hongyu%20Qu%20and%20Rui%20Yan%20and%20Xiangbo%20Shu%20and%20Haoliang%20Gao%20and%20Peng%20Huang%20and%20Guo-Sen%20Xie%0AAbstract%3A%20%20%20Recent%20few-shot%20action%20recognition%20%28FSAR%29%20methods%20achieve%20promising%0Aperformance%20by%20performing%20semantic%20matching%20on%20learned%20discriminative%20features.%0AHowever%2C%20most%20FSAR%20methods%20focus%20on%20single-scale%20%28e.g.%2C%20frame-level%2C%0Asegment-level%2C%20%5Cetc%29%20feature%20alignment%2C%20which%20ignores%20that%20human%20actions%20with%0Athe%20same%20semantic%20may%20appear%20at%20different%20velocities.%20To%20this%20end%2C%20we%20develop%20a%0Anovel%20Multi-Velocity%20Progressive-alignment%20%28MVP-Shot%29%20framework%20to%0Aprogressively%20learn%20and%20align%20semantic-related%20action%20features%20at%0Amulti-velocity%20levels.%20Concretely%2C%20a%20Multi-Velocity%20Feature%20Alignment%20%28MVFA%29%0Amodule%20is%20designed%20to%20measure%20the%20similarity%20between%20features%20from%20support%20and%0Aquery%20videos%20with%20different%20velocity%20scales%20and%20then%20merge%20all%20similarity%0Ascores%20in%20a%20residual%20fashion.%20To%20avoid%20the%20multiple%20velocity%20features%20deviating%0Afrom%20the%20underlying%20motion%20semantic%2C%20our%20proposed%20Progressive%20Semantic-Tailored%0AInteraction%20%28PSTI%29%20module%20injects%20velocity-tailored%20text%20information%20into%20the%0Avideo%20feature%20via%20feature%20interaction%20on%20channel%20and%20temporal%20domains%20at%0Adifferent%20velocities.%20The%20above%20two%20modules%20compensate%20for%20each%20other%20to%0Apredict%20query%20categories%20more%20accurately%20under%20the%20few-shot%20settings.%0AExperimental%20results%20show%20our%20method%20outperforms%20current%20state-of-the-art%0Amethods%20on%20multiple%20standard%20few-shot%20benchmarks%20%28i.e.%2C%20HMDB51%2C%20UCF101%2C%0AKinetics%2C%20and%20SSv2-small%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVP-Shot%253A%2520Multi-Velocity%2520Progressive-Alignment%2520Framework%2520for%2520Few-Shot%250A%2520%2520Action%2520Recognition%26entry.906535625%3DHongyu%2520Qu%2520and%2520Rui%2520Yan%2520and%2520Xiangbo%2520Shu%2520and%2520Haoliang%2520Gao%2520and%2520Peng%2520Huang%2520and%2520Guo-Sen%2520Xie%26entry.1292438233%3D%2520%2520Recent%2520few-shot%2520action%2520recognition%2520%2528FSAR%2529%2520methods%2520achieve%2520promising%250Aperformance%2520by%2520performing%2520semantic%2520matching%2520on%2520learned%2520discriminative%2520features.%250AHowever%252C%2520most%2520FSAR%2520methods%2520focus%2520on%2520single-scale%2520%2528e.g.%252C%2520frame-level%252C%250Asegment-level%252C%2520%255Cetc%2529%2520feature%2520alignment%252C%2520which%2520ignores%2520that%2520human%2520actions%2520with%250Athe%2520same%2520semantic%2520may%2520appear%2520at%2520different%2520velocities.%2520To%2520this%2520end%252C%2520we%2520develop%2520a%250Anovel%2520Multi-Velocity%2520Progressive-alignment%2520%2528MVP-Shot%2529%2520framework%2520to%250Aprogressively%2520learn%2520and%2520align%2520semantic-related%2520action%2520features%2520at%250Amulti-velocity%2520levels.%2520Concretely%252C%2520a%2520Multi-Velocity%2520Feature%2520Alignment%2520%2528MVFA%2529%250Amodule%2520is%2520designed%2520to%2520measure%2520the%2520similarity%2520between%2520features%2520from%2520support%2520and%250Aquery%2520videos%2520with%2520different%2520velocity%2520scales%2520and%2520then%2520merge%2520all%2520similarity%250Ascores%2520in%2520a%2520residual%2520fashion.%2520To%2520avoid%2520the%2520multiple%2520velocity%2520features%2520deviating%250Afrom%2520the%2520underlying%2520motion%2520semantic%252C%2520our%2520proposed%2520Progressive%2520Semantic-Tailored%250AInteraction%2520%2528PSTI%2529%2520module%2520injects%2520velocity-tailored%2520text%2520information%2520into%2520the%250Avideo%2520feature%2520via%2520feature%2520interaction%2520on%2520channel%2520and%2520temporal%2520domains%2520at%250Adifferent%2520velocities.%2520The%2520above%2520two%2520modules%2520compensate%2520for%2520each%2520other%2520to%250Apredict%2520query%2520categories%2520more%2520accurately%2520under%2520the%2520few-shot%2520settings.%250AExperimental%2520results%2520show%2520our%2520method%2520outperforms%2520current%2520state-of-the-art%250Amethods%2520on%2520multiple%2520standard%2520few-shot%2520benchmarks%2520%2528i.e.%252C%2520HMDB51%252C%2520UCF101%252C%250AKinetics%252C%2520and%2520SSv2-small%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVP-Shot%3A%20Multi-Velocity%20Progressive-Alignment%20Framework%20for%20Few-Shot%0A%20%20Action%20Recognition&entry.906535625=Hongyu%20Qu%20and%20Rui%20Yan%20and%20Xiangbo%20Shu%20and%20Haoliang%20Gao%20and%20Peng%20Huang%20and%20Guo-Sen%20Xie&entry.1292438233=%20%20Recent%20few-shot%20action%20recognition%20%28FSAR%29%20methods%20achieve%20promising%0Aperformance%20by%20performing%20semantic%20matching%20on%20learned%20discriminative%20features.%0AHowever%2C%20most%20FSAR%20methods%20focus%20on%20single-scale%20%28e.g.%2C%20frame-level%2C%0Asegment-level%2C%20%5Cetc%29%20feature%20alignment%2C%20which%20ignores%20that%20human%20actions%20with%0Athe%20same%20semantic%20may%20appear%20at%20different%20velocities.%20To%20this%20end%2C%20we%20develop%20a%0Anovel%20Multi-Velocity%20Progressive-alignment%20%28MVP-Shot%29%20framework%20to%0Aprogressively%20learn%20and%20align%20semantic-related%20action%20features%20at%0Amulti-velocity%20levels.%20Concretely%2C%20a%20Multi-Velocity%20Feature%20Alignment%20%28MVFA%29%0Amodule%20is%20designed%20to%20measure%20the%20similarity%20between%20features%20from%20support%20and%0Aquery%20videos%20with%20different%20velocity%20scales%20and%20then%20merge%20all%20similarity%0Ascores%20in%20a%20residual%20fashion.%20To%20avoid%20the%20multiple%20velocity%20features%20deviating%0Afrom%20the%20underlying%20motion%20semantic%2C%20our%20proposed%20Progressive%20Semantic-Tailored%0AInteraction%20%28PSTI%29%20module%20injects%20velocity-tailored%20text%20information%20into%20the%0Avideo%20feature%20via%20feature%20interaction%20on%20channel%20and%20temporal%20domains%20at%0Adifferent%20velocities.%20The%20above%20two%20modules%20compensate%20for%20each%20other%20to%0Apredict%20query%20categories%20more%20accurately%20under%20the%20few-shot%20settings.%0AExperimental%20results%20show%20our%20method%20outperforms%20current%20state-of-the-art%0Amethods%20on%20multiple%20standard%20few-shot%20benchmarks%20%28i.e.%2C%20HMDB51%2C%20UCF101%2C%0AKinetics%2C%20and%20SSv2-small%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02077v1&entry.124074799=Read"},
{"title": "Multi-method Integration with Confidence-based Weighting for Zero-shot\n  Image Classification", "author": "Siqi Yin and Lifan Jiang", "abstract": "  This paper introduces a novel framework for zero-shot learning (ZSL), i.e.,\nto recognize new categories that are unseen during training, by using a\nmulti-model and multi-alignment integration method. Specifically, we propose\nthree strategies to enhance the model's performance to handle ZSL: 1) Utilizing\nthe extensive knowledge of ChatGPT and the powerful image generation\ncapabilities of DALL-E to create reference images that can precisely describe\nunseen categories and classification boundaries, thereby alleviating the\ninformation bottleneck issue; 2) Integrating the results of text-image\nalignment and image-image alignment from CLIP, along with the image-image\nalignment results from DINO, to achieve more accurate predictions; 3)\nIntroducing an adaptive weighting mechanism based on confidence levels to\naggregate the outcomes from different prediction methods. Experimental results\non multiple datasets, including CIFAR-10, CIFAR-100, and TinyImageNet,\ndemonstrate that our model can significantly improve classification accuracy\ncompared to single-model approaches, achieving AUROC scores above 96% across\nall test datasets, and notably surpassing 99% on the CIFAR-10 dataset.\n", "link": "http://arxiv.org/abs/2405.02155v1", "date": "2024-05-03", "relevancy": 2.1971, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5911}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5259}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-method%20Integration%20with%20Confidence-based%20Weighting%20for%20Zero-shot%0A%20%20Image%20Classification&body=Title%3A%20Multi-method%20Integration%20with%20Confidence-based%20Weighting%20for%20Zero-shot%0A%20%20Image%20Classification%0AAuthor%3A%20Siqi%20Yin%20and%20Lifan%20Jiang%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20framework%20for%20zero-shot%20learning%20%28ZSL%29%2C%20i.e.%2C%0Ato%20recognize%20new%20categories%20that%20are%20unseen%20during%20training%2C%20by%20using%20a%0Amulti-model%20and%20multi-alignment%20integration%20method.%20Specifically%2C%20we%20propose%0Athree%20strategies%20to%20enhance%20the%20model%27s%20performance%20to%20handle%20ZSL%3A%201%29%20Utilizing%0Athe%20extensive%20knowledge%20of%20ChatGPT%20and%20the%20powerful%20image%20generation%0Acapabilities%20of%20DALL-E%20to%20create%20reference%20images%20that%20can%20precisely%20describe%0Aunseen%20categories%20and%20classification%20boundaries%2C%20thereby%20alleviating%20the%0Ainformation%20bottleneck%20issue%3B%202%29%20Integrating%20the%20results%20of%20text-image%0Aalignment%20and%20image-image%20alignment%20from%20CLIP%2C%20along%20with%20the%20image-image%0Aalignment%20results%20from%20DINO%2C%20to%20achieve%20more%20accurate%20predictions%3B%203%29%0AIntroducing%20an%20adaptive%20weighting%20mechanism%20based%20on%20confidence%20levels%20to%0Aaggregate%20the%20outcomes%20from%20different%20prediction%20methods.%20Experimental%20results%0Aon%20multiple%20datasets%2C%20including%20CIFAR-10%2C%20CIFAR-100%2C%20and%20TinyImageNet%2C%0Ademonstrate%20that%20our%20model%20can%20significantly%20improve%20classification%20accuracy%0Acompared%20to%20single-model%20approaches%2C%20achieving%20AUROC%20scores%20above%2096%25%20across%0Aall%20test%20datasets%2C%20and%20notably%20surpassing%2099%25%20on%20the%20CIFAR-10%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-method%2520Integration%2520with%2520Confidence-based%2520Weighting%2520for%2520Zero-shot%250A%2520%2520Image%2520Classification%26entry.906535625%3DSiqi%2520Yin%2520and%2520Lifan%2520Jiang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520framework%2520for%2520zero-shot%2520learning%2520%2528ZSL%2529%252C%2520i.e.%252C%250Ato%2520recognize%2520new%2520categories%2520that%2520are%2520unseen%2520during%2520training%252C%2520by%2520using%2520a%250Amulti-model%2520and%2520multi-alignment%2520integration%2520method.%2520Specifically%252C%2520we%2520propose%250Athree%2520strategies%2520to%2520enhance%2520the%2520model%2527s%2520performance%2520to%2520handle%2520ZSL%253A%25201%2529%2520Utilizing%250Athe%2520extensive%2520knowledge%2520of%2520ChatGPT%2520and%2520the%2520powerful%2520image%2520generation%250Acapabilities%2520of%2520DALL-E%2520to%2520create%2520reference%2520images%2520that%2520can%2520precisely%2520describe%250Aunseen%2520categories%2520and%2520classification%2520boundaries%252C%2520thereby%2520alleviating%2520the%250Ainformation%2520bottleneck%2520issue%253B%25202%2529%2520Integrating%2520the%2520results%2520of%2520text-image%250Aalignment%2520and%2520image-image%2520alignment%2520from%2520CLIP%252C%2520along%2520with%2520the%2520image-image%250Aalignment%2520results%2520from%2520DINO%252C%2520to%2520achieve%2520more%2520accurate%2520predictions%253B%25203%2529%250AIntroducing%2520an%2520adaptive%2520weighting%2520mechanism%2520based%2520on%2520confidence%2520levels%2520to%250Aaggregate%2520the%2520outcomes%2520from%2520different%2520prediction%2520methods.%2520Experimental%2520results%250Aon%2520multiple%2520datasets%252C%2520including%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520TinyImageNet%252C%250Ademonstrate%2520that%2520our%2520model%2520can%2520significantly%2520improve%2520classification%2520accuracy%250Acompared%2520to%2520single-model%2520approaches%252C%2520achieving%2520AUROC%2520scores%2520above%252096%2525%2520across%250Aall%2520test%2520datasets%252C%2520and%2520notably%2520surpassing%252099%2525%2520on%2520the%2520CIFAR-10%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-method%20Integration%20with%20Confidence-based%20Weighting%20for%20Zero-shot%0A%20%20Image%20Classification&entry.906535625=Siqi%20Yin%20and%20Lifan%20Jiang&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20framework%20for%20zero-shot%20learning%20%28ZSL%29%2C%20i.e.%2C%0Ato%20recognize%20new%20categories%20that%20are%20unseen%20during%20training%2C%20by%20using%20a%0Amulti-model%20and%20multi-alignment%20integration%20method.%20Specifically%2C%20we%20propose%0Athree%20strategies%20to%20enhance%20the%20model%27s%20performance%20to%20handle%20ZSL%3A%201%29%20Utilizing%0Athe%20extensive%20knowledge%20of%20ChatGPT%20and%20the%20powerful%20image%20generation%0Acapabilities%20of%20DALL-E%20to%20create%20reference%20images%20that%20can%20precisely%20describe%0Aunseen%20categories%20and%20classification%20boundaries%2C%20thereby%20alleviating%20the%0Ainformation%20bottleneck%20issue%3B%202%29%20Integrating%20the%20results%20of%20text-image%0Aalignment%20and%20image-image%20alignment%20from%20CLIP%2C%20along%20with%20the%20image-image%0Aalignment%20results%20from%20DINO%2C%20to%20achieve%20more%20accurate%20predictions%3B%203%29%0AIntroducing%20an%20adaptive%20weighting%20mechanism%20based%20on%20confidence%20levels%20to%0Aaggregate%20the%20outcomes%20from%20different%20prediction%20methods.%20Experimental%20results%0Aon%20multiple%20datasets%2C%20including%20CIFAR-10%2C%20CIFAR-100%2C%20and%20TinyImageNet%2C%0Ademonstrate%20that%20our%20model%20can%20significantly%20improve%20classification%20accuracy%0Acompared%20to%20single-model%20approaches%2C%20achieving%20AUROC%20scores%20above%2096%25%20across%0Aall%20test%20datasets%2C%20and%20notably%20surpassing%2099%25%20on%20the%20CIFAR-10%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02155v1&entry.124074799=Read"},
{"title": "Automating Computational Design with Generative AI", "author": "Joern Ploennigs and Markus Berger", "abstract": "  AI image generators based on diffusion models have recently garnered\nattention for their capability to create images from simple text prompts.\nHowever, for practical use in civil engineering they need to be able to create\nspecific construction plans for given constraints. This paper investigates the\npotential of current AI generators in addressing such challenges, specifically\nfor the creation of simple floor plans. We explain how the underlying\ndiffusion-models work and propose novel refinement approaches to improve\nsemantic encoding and generation quality. In several experiments we show that\nwe can improve validity of generated floor plans from 6% to 90%. Based on these\nresults we derive future research challenges considering building information\nmodelling. With this we provide: (i) evaluation of current generative AIs; (ii)\npropose improved refinement approaches; (iii) evaluate them on various\nexamples; (iv) derive future directions for diffusion models in civil\nengineering.\n", "link": "http://arxiv.org/abs/2307.02511v2", "date": "2024-05-03", "relevancy": 2.1955, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5658}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5523}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20Computational%20Design%20with%20Generative%20AI&body=Title%3A%20Automating%20Computational%20Design%20with%20Generative%20AI%0AAuthor%3A%20Joern%20Ploennigs%20and%20Markus%20Berger%0AAbstract%3A%20%20%20AI%20image%20generators%20based%20on%20diffusion%20models%20have%20recently%20garnered%0Aattention%20for%20their%20capability%20to%20create%20images%20from%20simple%20text%20prompts.%0AHowever%2C%20for%20practical%20use%20in%20civil%20engineering%20they%20need%20to%20be%20able%20to%20create%0Aspecific%20construction%20plans%20for%20given%20constraints.%20This%20paper%20investigates%20the%0Apotential%20of%20current%20AI%20generators%20in%20addressing%20such%20challenges%2C%20specifically%0Afor%20the%20creation%20of%20simple%20floor%20plans.%20We%20explain%20how%20the%20underlying%0Adiffusion-models%20work%20and%20propose%20novel%20refinement%20approaches%20to%20improve%0Asemantic%20encoding%20and%20generation%20quality.%20In%20several%20experiments%20we%20show%20that%0Awe%20can%20improve%20validity%20of%20generated%20floor%20plans%20from%206%25%20to%2090%25.%20Based%20on%20these%0Aresults%20we%20derive%20future%20research%20challenges%20considering%20building%20information%0Amodelling.%20With%20this%20we%20provide%3A%20%28i%29%20evaluation%20of%20current%20generative%20AIs%3B%20%28ii%29%0Apropose%20improved%20refinement%20approaches%3B%20%28iii%29%20evaluate%20them%20on%20various%0Aexamples%3B%20%28iv%29%20derive%20future%20directions%20for%20diffusion%20models%20in%20civil%0Aengineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.02511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520Computational%2520Design%2520with%2520Generative%2520AI%26entry.906535625%3DJoern%2520Ploennigs%2520and%2520Markus%2520Berger%26entry.1292438233%3D%2520%2520AI%2520image%2520generators%2520based%2520on%2520diffusion%2520models%2520have%2520recently%2520garnered%250Aattention%2520for%2520their%2520capability%2520to%2520create%2520images%2520from%2520simple%2520text%2520prompts.%250AHowever%252C%2520for%2520practical%2520use%2520in%2520civil%2520engineering%2520they%2520need%2520to%2520be%2520able%2520to%2520create%250Aspecific%2520construction%2520plans%2520for%2520given%2520constraints.%2520This%2520paper%2520investigates%2520the%250Apotential%2520of%2520current%2520AI%2520generators%2520in%2520addressing%2520such%2520challenges%252C%2520specifically%250Afor%2520the%2520creation%2520of%2520simple%2520floor%2520plans.%2520We%2520explain%2520how%2520the%2520underlying%250Adiffusion-models%2520work%2520and%2520propose%2520novel%2520refinement%2520approaches%2520to%2520improve%250Asemantic%2520encoding%2520and%2520generation%2520quality.%2520In%2520several%2520experiments%2520we%2520show%2520that%250Awe%2520can%2520improve%2520validity%2520of%2520generated%2520floor%2520plans%2520from%25206%2525%2520to%252090%2525.%2520Based%2520on%2520these%250Aresults%2520we%2520derive%2520future%2520research%2520challenges%2520considering%2520building%2520information%250Amodelling.%2520With%2520this%2520we%2520provide%253A%2520%2528i%2529%2520evaluation%2520of%2520current%2520generative%2520AIs%253B%2520%2528ii%2529%250Apropose%2520improved%2520refinement%2520approaches%253B%2520%2528iii%2529%2520evaluate%2520them%2520on%2520various%250Aexamples%253B%2520%2528iv%2529%2520derive%2520future%2520directions%2520for%2520diffusion%2520models%2520in%2520civil%250Aengineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.02511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20Computational%20Design%20with%20Generative%20AI&entry.906535625=Joern%20Ploennigs%20and%20Markus%20Berger&entry.1292438233=%20%20AI%20image%20generators%20based%20on%20diffusion%20models%20have%20recently%20garnered%0Aattention%20for%20their%20capability%20to%20create%20images%20from%20simple%20text%20prompts.%0AHowever%2C%20for%20practical%20use%20in%20civil%20engineering%20they%20need%20to%20be%20able%20to%20create%0Aspecific%20construction%20plans%20for%20given%20constraints.%20This%20paper%20investigates%20the%0Apotential%20of%20current%20AI%20generators%20in%20addressing%20such%20challenges%2C%20specifically%0Afor%20the%20creation%20of%20simple%20floor%20plans.%20We%20explain%20how%20the%20underlying%0Adiffusion-models%20work%20and%20propose%20novel%20refinement%20approaches%20to%20improve%0Asemantic%20encoding%20and%20generation%20quality.%20In%20several%20experiments%20we%20show%20that%0Awe%20can%20improve%20validity%20of%20generated%20floor%20plans%20from%206%25%20to%2090%25.%20Based%20on%20these%0Aresults%20we%20derive%20future%20research%20challenges%20considering%20building%20information%0Amodelling.%20With%20this%20we%20provide%3A%20%28i%29%20evaluation%20of%20current%20generative%20AIs%3B%20%28ii%29%0Apropose%20improved%20refinement%20approaches%3B%20%28iii%29%20evaluate%20them%20on%20various%0Aexamples%3B%20%28iv%29%20derive%20future%20directions%20for%20diffusion%20models%20in%20civil%0Aengineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.02511v2&entry.124074799=Read"},
{"title": "A Simple Interpretable Transformer for Fine-Grained Image Classification\n  and Analysis", "author": "Dipanjyoti Paul and Arpita Chowdhury and Xinqi Xiong and Feng-Ju Chang and David Carlyn and Samuel Stevens and Kaiya L. Provost and Anuj Karpatne and Bryan Carstens and Daniel Rubenstein and Charles Stewart and Tanya Berger-Wolf and Yu Su and Wei-Lun Chao", "abstract": "  We present a novel usage of Transformers to make image classification\ninterpretable. Unlike mainstream classifiers that wait until the last fully\nconnected layer to incorporate class information to make predictions, we\ninvestigate a proactive approach, asking each class to search for itself in an\nimage. We realize this idea via a Transformer encoder-decoder inspired by\nDEtection TRansformer (DETR). We learn \"class-specific\" queries (one for each\nclass) as input to the decoder, enabling each class to localize its patterns in\nan image via cross-attention. We name our approach INterpretable TRansformer\n(INTR), which is fairly easy to implement and exhibits several compelling\nproperties. We show that INTR intrinsically encourages each class to attend\ndistinctively; the cross-attention weights thus provide a faithful\ninterpretation of the prediction. Interestingly, via \"multi-head\"\ncross-attention, INTR could identify different \"attributes\" of a class, making\nit particularly suitable for fine-grained classification and analysis, which we\ndemonstrate on eight datasets. Our code and pre-trained models are publicly\naccessible at the Imageomics Institute GitHub site:\nhttps://github.com/Imageomics/INTR.\n", "link": "http://arxiv.org/abs/2311.04157v2", "date": "2024-05-03", "relevancy": 2.179, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5819}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.56}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Interpretable%20Transformer%20for%20Fine-Grained%20Image%20Classification%0A%20%20and%20Analysis&body=Title%3A%20A%20Simple%20Interpretable%20Transformer%20for%20Fine-Grained%20Image%20Classification%0A%20%20and%20Analysis%0AAuthor%3A%20Dipanjyoti%20Paul%20and%20Arpita%20Chowdhury%20and%20Xinqi%20Xiong%20and%20Feng-Ju%20Chang%20and%20David%20Carlyn%20and%20Samuel%20Stevens%20and%20Kaiya%20L.%20Provost%20and%20Anuj%20Karpatne%20and%20Bryan%20Carstens%20and%20Daniel%20Rubenstein%20and%20Charles%20Stewart%20and%20Tanya%20Berger-Wolf%20and%20Yu%20Su%20and%20Wei-Lun%20Chao%0AAbstract%3A%20%20%20We%20present%20a%20novel%20usage%20of%20Transformers%20to%20make%20image%20classification%0Ainterpretable.%20Unlike%20mainstream%20classifiers%20that%20wait%20until%20the%20last%20fully%0Aconnected%20layer%20to%20incorporate%20class%20information%20to%20make%20predictions%2C%20we%0Ainvestigate%20a%20proactive%20approach%2C%20asking%20each%20class%20to%20search%20for%20itself%20in%20an%0Aimage.%20We%20realize%20this%20idea%20via%20a%20Transformer%20encoder-decoder%20inspired%20by%0ADEtection%20TRansformer%20%28DETR%29.%20We%20learn%20%22class-specific%22%20queries%20%28one%20for%20each%0Aclass%29%20as%20input%20to%20the%20decoder%2C%20enabling%20each%20class%20to%20localize%20its%20patterns%20in%0Aan%20image%20via%20cross-attention.%20We%20name%20our%20approach%20INterpretable%20TRansformer%0A%28INTR%29%2C%20which%20is%20fairly%20easy%20to%20implement%20and%20exhibits%20several%20compelling%0Aproperties.%20We%20show%20that%20INTR%20intrinsically%20encourages%20each%20class%20to%20attend%0Adistinctively%3B%20the%20cross-attention%20weights%20thus%20provide%20a%20faithful%0Ainterpretation%20of%20the%20prediction.%20Interestingly%2C%20via%20%22multi-head%22%0Across-attention%2C%20INTR%20could%20identify%20different%20%22attributes%22%20of%20a%20class%2C%20making%0Ait%20particularly%20suitable%20for%20fine-grained%20classification%20and%20analysis%2C%20which%20we%0Ademonstrate%20on%20eight%20datasets.%20Our%20code%20and%20pre-trained%20models%20are%20publicly%0Aaccessible%20at%20the%20Imageomics%20Institute%20GitHub%20site%3A%0Ahttps%3A//github.com/Imageomics/INTR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Interpretable%2520Transformer%2520for%2520Fine-Grained%2520Image%2520Classification%250A%2520%2520and%2520Analysis%26entry.906535625%3DDipanjyoti%2520Paul%2520and%2520Arpita%2520Chowdhury%2520and%2520Xinqi%2520Xiong%2520and%2520Feng-Ju%2520Chang%2520and%2520David%2520Carlyn%2520and%2520Samuel%2520Stevens%2520and%2520Kaiya%2520L.%2520Provost%2520and%2520Anuj%2520Karpatne%2520and%2520Bryan%2520Carstens%2520and%2520Daniel%2520Rubenstein%2520and%2520Charles%2520Stewart%2520and%2520Tanya%2520Berger-Wolf%2520and%2520Yu%2520Su%2520and%2520Wei-Lun%2520Chao%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520usage%2520of%2520Transformers%2520to%2520make%2520image%2520classification%250Ainterpretable.%2520Unlike%2520mainstream%2520classifiers%2520that%2520wait%2520until%2520the%2520last%2520fully%250Aconnected%2520layer%2520to%2520incorporate%2520class%2520information%2520to%2520make%2520predictions%252C%2520we%250Ainvestigate%2520a%2520proactive%2520approach%252C%2520asking%2520each%2520class%2520to%2520search%2520for%2520itself%2520in%2520an%250Aimage.%2520We%2520realize%2520this%2520idea%2520via%2520a%2520Transformer%2520encoder-decoder%2520inspired%2520by%250ADEtection%2520TRansformer%2520%2528DETR%2529.%2520We%2520learn%2520%2522class-specific%2522%2520queries%2520%2528one%2520for%2520each%250Aclass%2529%2520as%2520input%2520to%2520the%2520decoder%252C%2520enabling%2520each%2520class%2520to%2520localize%2520its%2520patterns%2520in%250Aan%2520image%2520via%2520cross-attention.%2520We%2520name%2520our%2520approach%2520INterpretable%2520TRansformer%250A%2528INTR%2529%252C%2520which%2520is%2520fairly%2520easy%2520to%2520implement%2520and%2520exhibits%2520several%2520compelling%250Aproperties.%2520We%2520show%2520that%2520INTR%2520intrinsically%2520encourages%2520each%2520class%2520to%2520attend%250Adistinctively%253B%2520the%2520cross-attention%2520weights%2520thus%2520provide%2520a%2520faithful%250Ainterpretation%2520of%2520the%2520prediction.%2520Interestingly%252C%2520via%2520%2522multi-head%2522%250Across-attention%252C%2520INTR%2520could%2520identify%2520different%2520%2522attributes%2522%2520of%2520a%2520class%252C%2520making%250Ait%2520particularly%2520suitable%2520for%2520fine-grained%2520classification%2520and%2520analysis%252C%2520which%2520we%250Ademonstrate%2520on%2520eight%2520datasets.%2520Our%2520code%2520and%2520pre-trained%2520models%2520are%2520publicly%250Aaccessible%2520at%2520the%2520Imageomics%2520Institute%2520GitHub%2520site%253A%250Ahttps%253A//github.com/Imageomics/INTR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Interpretable%20Transformer%20for%20Fine-Grained%20Image%20Classification%0A%20%20and%20Analysis&entry.906535625=Dipanjyoti%20Paul%20and%20Arpita%20Chowdhury%20and%20Xinqi%20Xiong%20and%20Feng-Ju%20Chang%20and%20David%20Carlyn%20and%20Samuel%20Stevens%20and%20Kaiya%20L.%20Provost%20and%20Anuj%20Karpatne%20and%20Bryan%20Carstens%20and%20Daniel%20Rubenstein%20and%20Charles%20Stewart%20and%20Tanya%20Berger-Wolf%20and%20Yu%20Su%20and%20Wei-Lun%20Chao&entry.1292438233=%20%20We%20present%20a%20novel%20usage%20of%20Transformers%20to%20make%20image%20classification%0Ainterpretable.%20Unlike%20mainstream%20classifiers%20that%20wait%20until%20the%20last%20fully%0Aconnected%20layer%20to%20incorporate%20class%20information%20to%20make%20predictions%2C%20we%0Ainvestigate%20a%20proactive%20approach%2C%20asking%20each%20class%20to%20search%20for%20itself%20in%20an%0Aimage.%20We%20realize%20this%20idea%20via%20a%20Transformer%20encoder-decoder%20inspired%20by%0ADEtection%20TRansformer%20%28DETR%29.%20We%20learn%20%22class-specific%22%20queries%20%28one%20for%20each%0Aclass%29%20as%20input%20to%20the%20decoder%2C%20enabling%20each%20class%20to%20localize%20its%20patterns%20in%0Aan%20image%20via%20cross-attention.%20We%20name%20our%20approach%20INterpretable%20TRansformer%0A%28INTR%29%2C%20which%20is%20fairly%20easy%20to%20implement%20and%20exhibits%20several%20compelling%0Aproperties.%20We%20show%20that%20INTR%20intrinsically%20encourages%20each%20class%20to%20attend%0Adistinctively%3B%20the%20cross-attention%20weights%20thus%20provide%20a%20faithful%0Ainterpretation%20of%20the%20prediction.%20Interestingly%2C%20via%20%22multi-head%22%0Across-attention%2C%20INTR%20could%20identify%20different%20%22attributes%22%20of%20a%20class%2C%20making%0Ait%20particularly%20suitable%20for%20fine-grained%20classification%20and%20analysis%2C%20which%20we%0Ademonstrate%20on%20eight%20datasets.%20Our%20code%20and%20pre-trained%20models%20are%20publicly%0Aaccessible%20at%20the%20Imageomics%20Institute%20GitHub%20site%3A%0Ahttps%3A//github.com/Imageomics/INTR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04157v2&entry.124074799=Read"},
{"title": "Introducing a microstructure-embedded autoencoder approach for\n  reconstructing high-resolution solution field from reduced parametric space", "author": "Rasoul Najafi Koopas and Shahed Rezaei and Natalie Rauter and Richard Ostwald and Rolf Lammering", "abstract": "  In this study, we develop a novel multi-fidelity deep learning approach that\ntransforms low-fidelity solution maps into high-fidelity ones by incorporating\nparametric space information into a standard autoencoder architecture. It is\nshown that, due to the integration of parametric space data, this method\nrequires significantly less training data to achieve effective performance in\npredicting high-fidelity solution from the low-fidelity one. In this study, our\nfocus is on a 2D steady-state heat transfer analysis in highly heterogeneous\nmaterials microstructure, where the spatial distribution of heat conductivity\ncoefficients for two distinct materials is condensed. Subsequently, the\nboundary value problem is solved on the coarsest grid using a pre-trained\nphysics-informed neural operator network. Afterward, the calculated\nlow-fidelity result is upscaled using the newly designed enhanced autoencoder.\nThe novelty of the developed enhanced autoencoder lies in the concatenation of\nheat conductivity maps of different resolutions to the decoder segment in\ndistinct steps. We then compare the outcomes of developed algorithm with the\ncorresponding finite element results, standard U-Net architecture as well as\nother upscaling approaches such as interpolation functions of varying orders\nand feedforward neural networks (FFNN). The analysis of the results based on\nthe new approach demonstrates superior performance compared to other approaches\nin terms of computational cost and error on the test cases. Therefore, as a\npotential supplement to neural operators networks, our architecture upscales\nlow-fidelity solutions to high-fidelity ones while preserving critical details\nthat are often lost in conventional upscaling methods, especially at sharp\ninterfaces, such as those encountered with interpolation methods.\n", "link": "http://arxiv.org/abs/2405.01975v1", "date": "2024-05-03", "relevancy": 2.1277, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5484}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.522}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20a%20microstructure-embedded%20autoencoder%20approach%20for%0A%20%20reconstructing%20high-resolution%20solution%20field%20from%20reduced%20parametric%20space&body=Title%3A%20Introducing%20a%20microstructure-embedded%20autoencoder%20approach%20for%0A%20%20reconstructing%20high-resolution%20solution%20field%20from%20reduced%20parametric%20space%0AAuthor%3A%20Rasoul%20Najafi%20Koopas%20and%20Shahed%20Rezaei%20and%20Natalie%20Rauter%20and%20Richard%20Ostwald%20and%20Rolf%20Lammering%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20develop%20a%20novel%20multi-fidelity%20deep%20learning%20approach%20that%0Atransforms%20low-fidelity%20solution%20maps%20into%20high-fidelity%20ones%20by%20incorporating%0Aparametric%20space%20information%20into%20a%20standard%20autoencoder%20architecture.%20It%20is%0Ashown%20that%2C%20due%20to%20the%20integration%20of%20parametric%20space%20data%2C%20this%20method%0Arequires%20significantly%20less%20training%20data%20to%20achieve%20effective%20performance%20in%0Apredicting%20high-fidelity%20solution%20from%20the%20low-fidelity%20one.%20In%20this%20study%2C%20our%0Afocus%20is%20on%20a%202D%20steady-state%20heat%20transfer%20analysis%20in%20highly%20heterogeneous%0Amaterials%20microstructure%2C%20where%20the%20spatial%20distribution%20of%20heat%20conductivity%0Acoefficients%20for%20two%20distinct%20materials%20is%20condensed.%20Subsequently%2C%20the%0Aboundary%20value%20problem%20is%20solved%20on%20the%20coarsest%20grid%20using%20a%20pre-trained%0Aphysics-informed%20neural%20operator%20network.%20Afterward%2C%20the%20calculated%0Alow-fidelity%20result%20is%20upscaled%20using%20the%20newly%20designed%20enhanced%20autoencoder.%0AThe%20novelty%20of%20the%20developed%20enhanced%20autoencoder%20lies%20in%20the%20concatenation%20of%0Aheat%20conductivity%20maps%20of%20different%20resolutions%20to%20the%20decoder%20segment%20in%0Adistinct%20steps.%20We%20then%20compare%20the%20outcomes%20of%20developed%20algorithm%20with%20the%0Acorresponding%20finite%20element%20results%2C%20standard%20U-Net%20architecture%20as%20well%20as%0Aother%20upscaling%20approaches%20such%20as%20interpolation%20functions%20of%20varying%20orders%0Aand%20feedforward%20neural%20networks%20%28FFNN%29.%20The%20analysis%20of%20the%20results%20based%20on%0Athe%20new%20approach%20demonstrates%20superior%20performance%20compared%20to%20other%20approaches%0Ain%20terms%20of%20computational%20cost%20and%20error%20on%20the%20test%20cases.%20Therefore%2C%20as%20a%0Apotential%20supplement%20to%20neural%20operators%20networks%2C%20our%20architecture%20upscales%0Alow-fidelity%20solutions%20to%20high-fidelity%20ones%20while%20preserving%20critical%20details%0Athat%20are%20often%20lost%20in%20conventional%20upscaling%20methods%2C%20especially%20at%20sharp%0Ainterfaces%2C%20such%20as%20those%20encountered%20with%20interpolation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520a%2520microstructure-embedded%2520autoencoder%2520approach%2520for%250A%2520%2520reconstructing%2520high-resolution%2520solution%2520field%2520from%2520reduced%2520parametric%2520space%26entry.906535625%3DRasoul%2520Najafi%2520Koopas%2520and%2520Shahed%2520Rezaei%2520and%2520Natalie%2520Rauter%2520and%2520Richard%2520Ostwald%2520and%2520Rolf%2520Lammering%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520develop%2520a%2520novel%2520multi-fidelity%2520deep%2520learning%2520approach%2520that%250Atransforms%2520low-fidelity%2520solution%2520maps%2520into%2520high-fidelity%2520ones%2520by%2520incorporating%250Aparametric%2520space%2520information%2520into%2520a%2520standard%2520autoencoder%2520architecture.%2520It%2520is%250Ashown%2520that%252C%2520due%2520to%2520the%2520integration%2520of%2520parametric%2520space%2520data%252C%2520this%2520method%250Arequires%2520significantly%2520less%2520training%2520data%2520to%2520achieve%2520effective%2520performance%2520in%250Apredicting%2520high-fidelity%2520solution%2520from%2520the%2520low-fidelity%2520one.%2520In%2520this%2520study%252C%2520our%250Afocus%2520is%2520on%2520a%25202D%2520steady-state%2520heat%2520transfer%2520analysis%2520in%2520highly%2520heterogeneous%250Amaterials%2520microstructure%252C%2520where%2520the%2520spatial%2520distribution%2520of%2520heat%2520conductivity%250Acoefficients%2520for%2520two%2520distinct%2520materials%2520is%2520condensed.%2520Subsequently%252C%2520the%250Aboundary%2520value%2520problem%2520is%2520solved%2520on%2520the%2520coarsest%2520grid%2520using%2520a%2520pre-trained%250Aphysics-informed%2520neural%2520operator%2520network.%2520Afterward%252C%2520the%2520calculated%250Alow-fidelity%2520result%2520is%2520upscaled%2520using%2520the%2520newly%2520designed%2520enhanced%2520autoencoder.%250AThe%2520novelty%2520of%2520the%2520developed%2520enhanced%2520autoencoder%2520lies%2520in%2520the%2520concatenation%2520of%250Aheat%2520conductivity%2520maps%2520of%2520different%2520resolutions%2520to%2520the%2520decoder%2520segment%2520in%250Adistinct%2520steps.%2520We%2520then%2520compare%2520the%2520outcomes%2520of%2520developed%2520algorithm%2520with%2520the%250Acorresponding%2520finite%2520element%2520results%252C%2520standard%2520U-Net%2520architecture%2520as%2520well%2520as%250Aother%2520upscaling%2520approaches%2520such%2520as%2520interpolation%2520functions%2520of%2520varying%2520orders%250Aand%2520feedforward%2520neural%2520networks%2520%2528FFNN%2529.%2520The%2520analysis%2520of%2520the%2520results%2520based%2520on%250Athe%2520new%2520approach%2520demonstrates%2520superior%2520performance%2520compared%2520to%2520other%2520approaches%250Ain%2520terms%2520of%2520computational%2520cost%2520and%2520error%2520on%2520the%2520test%2520cases.%2520Therefore%252C%2520as%2520a%250Apotential%2520supplement%2520to%2520neural%2520operators%2520networks%252C%2520our%2520architecture%2520upscales%250Alow-fidelity%2520solutions%2520to%2520high-fidelity%2520ones%2520while%2520preserving%2520critical%2520details%250Athat%2520are%2520often%2520lost%2520in%2520conventional%2520upscaling%2520methods%252C%2520especially%2520at%2520sharp%250Ainterfaces%252C%2520such%2520as%2520those%2520encountered%2520with%2520interpolation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20a%20microstructure-embedded%20autoencoder%20approach%20for%0A%20%20reconstructing%20high-resolution%20solution%20field%20from%20reduced%20parametric%20space&entry.906535625=Rasoul%20Najafi%20Koopas%20and%20Shahed%20Rezaei%20and%20Natalie%20Rauter%20and%20Richard%20Ostwald%20and%20Rolf%20Lammering&entry.1292438233=%20%20In%20this%20study%2C%20we%20develop%20a%20novel%20multi-fidelity%20deep%20learning%20approach%20that%0Atransforms%20low-fidelity%20solution%20maps%20into%20high-fidelity%20ones%20by%20incorporating%0Aparametric%20space%20information%20into%20a%20standard%20autoencoder%20architecture.%20It%20is%0Ashown%20that%2C%20due%20to%20the%20integration%20of%20parametric%20space%20data%2C%20this%20method%0Arequires%20significantly%20less%20training%20data%20to%20achieve%20effective%20performance%20in%0Apredicting%20high-fidelity%20solution%20from%20the%20low-fidelity%20one.%20In%20this%20study%2C%20our%0Afocus%20is%20on%20a%202D%20steady-state%20heat%20transfer%20analysis%20in%20highly%20heterogeneous%0Amaterials%20microstructure%2C%20where%20the%20spatial%20distribution%20of%20heat%20conductivity%0Acoefficients%20for%20two%20distinct%20materials%20is%20condensed.%20Subsequently%2C%20the%0Aboundary%20value%20problem%20is%20solved%20on%20the%20coarsest%20grid%20using%20a%20pre-trained%0Aphysics-informed%20neural%20operator%20network.%20Afterward%2C%20the%20calculated%0Alow-fidelity%20result%20is%20upscaled%20using%20the%20newly%20designed%20enhanced%20autoencoder.%0AThe%20novelty%20of%20the%20developed%20enhanced%20autoencoder%20lies%20in%20the%20concatenation%20of%0Aheat%20conductivity%20maps%20of%20different%20resolutions%20to%20the%20decoder%20segment%20in%0Adistinct%20steps.%20We%20then%20compare%20the%20outcomes%20of%20developed%20algorithm%20with%20the%0Acorresponding%20finite%20element%20results%2C%20standard%20U-Net%20architecture%20as%20well%20as%0Aother%20upscaling%20approaches%20such%20as%20interpolation%20functions%20of%20varying%20orders%0Aand%20feedforward%20neural%20networks%20%28FFNN%29.%20The%20analysis%20of%20the%20results%20based%20on%0Athe%20new%20approach%20demonstrates%20superior%20performance%20compared%20to%20other%20approaches%0Ain%20terms%20of%20computational%20cost%20and%20error%20on%20the%20test%20cases.%20Therefore%2C%20as%20a%0Apotential%20supplement%20to%20neural%20operators%20networks%2C%20our%20architecture%20upscales%0Alow-fidelity%20solutions%20to%20high-fidelity%20ones%20while%20preserving%20critical%20details%0Athat%20are%20often%20lost%20in%20conventional%20upscaling%20methods%2C%20especially%20at%20sharp%0Ainterfaces%2C%20such%20as%20those%20encountered%20with%20interpolation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01975v1&entry.124074799=Read"},
{"title": "A Sonar-based AUV Positioning System for Underwater Environments with\n  Low Infrastructure Density", "author": "Emilio Olivastri and Daniel Fusaro and Wanmeng Li and Simone Mosco and Alberto Pretto", "abstract": "  The increasing demand for underwater vehicles highlights the necessity for\nrobust localization solutions in inspection missions. In this work, we present\na novel real-time sonar-based underwater global positioning algorithm for AUVs\n(Autonomous Underwater Vehicles) designed for environments with a sparse\ndistribution of human-made assets. Our approach exploits two synergistic data\ninterpretation frontends applied to the same stream of sonar data acquired by a\nmultibeam Forward-Looking Sonar (FSD). These observations are fused within a\nParticle Filter (PF) either to weigh more particles that belong to\nhigh-likelihood regions or to solve symmetric ambiguities. Preliminary\nexperiments carried out on a simulated environment resembling a real underwater\nplant provided promising results. This work represents a starting point towards\nfuture developments of the method and consequent exhaustive evaluations also in\nreal-world scenarios.\n", "link": "http://arxiv.org/abs/2405.01971v1", "date": "2024-05-03", "relevancy": 2.1055, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5795}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sonar-based%20AUV%20Positioning%20System%20for%20Underwater%20Environments%20with%0A%20%20Low%20Infrastructure%20Density&body=Title%3A%20A%20Sonar-based%20AUV%20Positioning%20System%20for%20Underwater%20Environments%20with%0A%20%20Low%20Infrastructure%20Density%0AAuthor%3A%20Emilio%20Olivastri%20and%20Daniel%20Fusaro%20and%20Wanmeng%20Li%20and%20Simone%20Mosco%20and%20Alberto%20Pretto%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20underwater%20vehicles%20highlights%20the%20necessity%20for%0Arobust%20localization%20solutions%20in%20inspection%20missions.%20In%20this%20work%2C%20we%20present%0Aa%20novel%20real-time%20sonar-based%20underwater%20global%20positioning%20algorithm%20for%20AUVs%0A%28Autonomous%20Underwater%20Vehicles%29%20designed%20for%20environments%20with%20a%20sparse%0Adistribution%20of%20human-made%20assets.%20Our%20approach%20exploits%20two%20synergistic%20data%0Ainterpretation%20frontends%20applied%20to%20the%20same%20stream%20of%20sonar%20data%20acquired%20by%20a%0Amultibeam%20Forward-Looking%20Sonar%20%28FSD%29.%20These%20observations%20are%20fused%20within%20a%0AParticle%20Filter%20%28PF%29%20either%20to%20weigh%20more%20particles%20that%20belong%20to%0Ahigh-likelihood%20regions%20or%20to%20solve%20symmetric%20ambiguities.%20Preliminary%0Aexperiments%20carried%20out%20on%20a%20simulated%20environment%20resembling%20a%20real%20underwater%0Aplant%20provided%20promising%20results.%20This%20work%20represents%20a%20starting%20point%20towards%0Afuture%20developments%20of%20the%20method%20and%20consequent%20exhaustive%20evaluations%20also%20in%0Areal-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sonar-based%2520AUV%2520Positioning%2520System%2520for%2520Underwater%2520Environments%2520with%250A%2520%2520Low%2520Infrastructure%2520Density%26entry.906535625%3DEmilio%2520Olivastri%2520and%2520Daniel%2520Fusaro%2520and%2520Wanmeng%2520Li%2520and%2520Simone%2520Mosco%2520and%2520Alberto%2520Pretto%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520underwater%2520vehicles%2520highlights%2520the%2520necessity%2520for%250Arobust%2520localization%2520solutions%2520in%2520inspection%2520missions.%2520In%2520this%2520work%252C%2520we%2520present%250Aa%2520novel%2520real-time%2520sonar-based%2520underwater%2520global%2520positioning%2520algorithm%2520for%2520AUVs%250A%2528Autonomous%2520Underwater%2520Vehicles%2529%2520designed%2520for%2520environments%2520with%2520a%2520sparse%250Adistribution%2520of%2520human-made%2520assets.%2520Our%2520approach%2520exploits%2520two%2520synergistic%2520data%250Ainterpretation%2520frontends%2520applied%2520to%2520the%2520same%2520stream%2520of%2520sonar%2520data%2520acquired%2520by%2520a%250Amultibeam%2520Forward-Looking%2520Sonar%2520%2528FSD%2529.%2520These%2520observations%2520are%2520fused%2520within%2520a%250AParticle%2520Filter%2520%2528PF%2529%2520either%2520to%2520weigh%2520more%2520particles%2520that%2520belong%2520to%250Ahigh-likelihood%2520regions%2520or%2520to%2520solve%2520symmetric%2520ambiguities.%2520Preliminary%250Aexperiments%2520carried%2520out%2520on%2520a%2520simulated%2520environment%2520resembling%2520a%2520real%2520underwater%250Aplant%2520provided%2520promising%2520results.%2520This%2520work%2520represents%2520a%2520starting%2520point%2520towards%250Afuture%2520developments%2520of%2520the%2520method%2520and%2520consequent%2520exhaustive%2520evaluations%2520also%2520in%250Areal-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sonar-based%20AUV%20Positioning%20System%20for%20Underwater%20Environments%20with%0A%20%20Low%20Infrastructure%20Density&entry.906535625=Emilio%20Olivastri%20and%20Daniel%20Fusaro%20and%20Wanmeng%20Li%20and%20Simone%20Mosco%20and%20Alberto%20Pretto&entry.1292438233=%20%20The%20increasing%20demand%20for%20underwater%20vehicles%20highlights%20the%20necessity%20for%0Arobust%20localization%20solutions%20in%20inspection%20missions.%20In%20this%20work%2C%20we%20present%0Aa%20novel%20real-time%20sonar-based%20underwater%20global%20positioning%20algorithm%20for%20AUVs%0A%28Autonomous%20Underwater%20Vehicles%29%20designed%20for%20environments%20with%20a%20sparse%0Adistribution%20of%20human-made%20assets.%20Our%20approach%20exploits%20two%20synergistic%20data%0Ainterpretation%20frontends%20applied%20to%20the%20same%20stream%20of%20sonar%20data%20acquired%20by%20a%0Amultibeam%20Forward-Looking%20Sonar%20%28FSD%29.%20These%20observations%20are%20fused%20within%20a%0AParticle%20Filter%20%28PF%29%20either%20to%20weigh%20more%20particles%20that%20belong%20to%0Ahigh-likelihood%20regions%20or%20to%20solve%20symmetric%20ambiguities.%20Preliminary%0Aexperiments%20carried%20out%20on%20a%20simulated%20environment%20resembling%20a%20real%20underwater%0Aplant%20provided%20promising%20results.%20This%20work%20represents%20a%20starting%20point%20towards%0Afuture%20developments%20of%20the%20method%20and%20consequent%20exhaustive%20evaluations%20also%20in%0Areal-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01971v1&entry.124074799=Read"},
{"title": "Conformal Prediction for Natural Language Processing: A Survey", "author": "Margarida M. Campos and Ant\u00f3nio Farinhas and Chrysoula Zerva and M\u00e1rio A. T. Figueiredo and Andr\u00e9 F. T. Martins", "abstract": "  The rapid proliferation of large language models and natural language\nprocessing (NLP) applications creates a crucial need for uncertainty\nquantification to mitigate risks such as hallucinations and to enhance\ndecision-making reliability in critical applications. Conformal prediction is\nemerging as a theoretically sound and practically useful framework, combining\nflexibility with strong statistical guarantees. Its model-agnostic and\ndistribution-free nature makes it particularly promising to address the current\nshortcomings of NLP systems that stem from the absence of uncertainty\nquantification. This paper provides a comprehensive survey of conformal\nprediction techniques, their guarantees, and existing applications in NLP,\npointing to directions for future research and open challenges.\n", "link": "http://arxiv.org/abs/2405.01976v1", "date": "2024-05-03", "relevancy": 2.0955, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5445}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5239}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20for%20Natural%20Language%20Processing%3A%20A%20Survey&body=Title%3A%20Conformal%20Prediction%20for%20Natural%20Language%20Processing%3A%20A%20Survey%0AAuthor%3A%20Margarida%20M.%20Campos%20and%20Ant%C3%B3nio%20Farinhas%20and%20Chrysoula%20Zerva%20and%20M%C3%A1rio%20A.%20T.%20Figueiredo%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20The%20rapid%20proliferation%20of%20large%20language%20models%20and%20natural%20language%0Aprocessing%20%28NLP%29%20applications%20creates%20a%20crucial%20need%20for%20uncertainty%0Aquantification%20to%20mitigate%20risks%20such%20as%20hallucinations%20and%20to%20enhance%0Adecision-making%20reliability%20in%20critical%20applications.%20Conformal%20prediction%20is%0Aemerging%20as%20a%20theoretically%20sound%20and%20practically%20useful%20framework%2C%20combining%0Aflexibility%20with%20strong%20statistical%20guarantees.%20Its%20model-agnostic%20and%0Adistribution-free%20nature%20makes%20it%20particularly%20promising%20to%20address%20the%20current%0Ashortcomings%20of%20NLP%20systems%20that%20stem%20from%20the%20absence%20of%20uncertainty%0Aquantification.%20This%20paper%20provides%20a%20comprehensive%20survey%20of%20conformal%0Aprediction%20techniques%2C%20their%20guarantees%2C%20and%20existing%20applications%20in%20NLP%2C%0Apointing%20to%20directions%20for%20future%20research%20and%20open%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520for%2520Natural%2520Language%2520Processing%253A%2520A%2520Survey%26entry.906535625%3DMargarida%2520M.%2520Campos%2520and%2520Ant%25C3%25B3nio%2520Farinhas%2520and%2520Chrysoula%2520Zerva%2520and%2520M%25C3%25A1rio%2520A.%2520T.%2520Figueiredo%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520The%2520rapid%2520proliferation%2520of%2520large%2520language%2520models%2520and%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520applications%2520creates%2520a%2520crucial%2520need%2520for%2520uncertainty%250Aquantification%2520to%2520mitigate%2520risks%2520such%2520as%2520hallucinations%2520and%2520to%2520enhance%250Adecision-making%2520reliability%2520in%2520critical%2520applications.%2520Conformal%2520prediction%2520is%250Aemerging%2520as%2520a%2520theoretically%2520sound%2520and%2520practically%2520useful%2520framework%252C%2520combining%250Aflexibility%2520with%2520strong%2520statistical%2520guarantees.%2520Its%2520model-agnostic%2520and%250Adistribution-free%2520nature%2520makes%2520it%2520particularly%2520promising%2520to%2520address%2520the%2520current%250Ashortcomings%2520of%2520NLP%2520systems%2520that%2520stem%2520from%2520the%2520absence%2520of%2520uncertainty%250Aquantification.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520survey%2520of%2520conformal%250Aprediction%2520techniques%252C%2520their%2520guarantees%252C%2520and%2520existing%2520applications%2520in%2520NLP%252C%250Apointing%2520to%2520directions%2520for%2520future%2520research%2520and%2520open%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20for%20Natural%20Language%20Processing%3A%20A%20Survey&entry.906535625=Margarida%20M.%20Campos%20and%20Ant%C3%B3nio%20Farinhas%20and%20Chrysoula%20Zerva%20and%20M%C3%A1rio%20A.%20T.%20Figueiredo%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20The%20rapid%20proliferation%20of%20large%20language%20models%20and%20natural%20language%0Aprocessing%20%28NLP%29%20applications%20creates%20a%20crucial%20need%20for%20uncertainty%0Aquantification%20to%20mitigate%20risks%20such%20as%20hallucinations%20and%20to%20enhance%0Adecision-making%20reliability%20in%20critical%20applications.%20Conformal%20prediction%20is%0Aemerging%20as%20a%20theoretically%20sound%20and%20practically%20useful%20framework%2C%20combining%0Aflexibility%20with%20strong%20statistical%20guarantees.%20Its%20model-agnostic%20and%0Adistribution-free%20nature%20makes%20it%20particularly%20promising%20to%20address%20the%20current%0Ashortcomings%20of%20NLP%20systems%20that%20stem%20from%20the%20absence%20of%20uncertainty%0Aquantification.%20This%20paper%20provides%20a%20comprehensive%20survey%20of%20conformal%0Aprediction%20techniques%2C%20their%20guarantees%2C%20and%20existing%20applications%20in%20NLP%2C%0Apointing%20to%20directions%20for%20future%20research%20and%20open%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01976v1&entry.124074799=Read"},
{"title": "InceptionXML: A Lightweight Framework with Synchronized Negative\n  Sampling for Short Text Extreme Classification", "author": "Siddhant Kharbanda and Atmadeep Banerjee and Devaansh Gupta and Akash Palrecha and Rohit Babbar", "abstract": "  Automatic annotation of short-text data to a large number of target labels,\nreferred to as Short Text Extreme Classification, has found numerous\napplications including prediction of related searches and product\nrecommendation. In this paper, we propose a convolutional architecture\nInceptionXML which is light-weight, yet powerful, and robust to the inherent\nlack of word-order in short-text queries encountered in search and\nrecommendation. We demonstrate the efficacy of applying convolutions by\nrecasting the operation along the embedding dimension instead of the word\ndimension as applied in conventional CNNs for text classification. Towards\nscaling our model to datasets with millions of labels, we also propose SyncXML\npipeline which improves upon the shortcomings of the recently proposed dynamic\nhard-negative mining technique for label short-listing by synchronizing the\nlabel-shortlister and extreme classifier. SyncXML not only reduces the\ninference time to half but is also an order of magnitude smaller than\nstate-of-the-art Astec in terms of model size. Through a comprehensive\nempirical comparison, we show that not only can InceptionXML outperform\nexisting approaches on benchmark datasets but also the transformer baselines\nrequiring only 2% FLOPs. The code for InceptionXML is available at\nhttps://github.com/xmc-aalto/inceptionxml.\n", "link": "http://arxiv.org/abs/2109.07319v4", "date": "2024-05-03", "relevancy": 2.0923, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5572}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InceptionXML%3A%20A%20Lightweight%20Framework%20with%20Synchronized%20Negative%0A%20%20Sampling%20for%20Short%20Text%20Extreme%20Classification&body=Title%3A%20InceptionXML%3A%20A%20Lightweight%20Framework%20with%20Synchronized%20Negative%0A%20%20Sampling%20for%20Short%20Text%20Extreme%20Classification%0AAuthor%3A%20Siddhant%20Kharbanda%20and%20Atmadeep%20Banerjee%20and%20Devaansh%20Gupta%20and%20Akash%20Palrecha%20and%20Rohit%20Babbar%0AAbstract%3A%20%20%20Automatic%20annotation%20of%20short-text%20data%20to%20a%20large%20number%20of%20target%20labels%2C%0Areferred%20to%20as%20Short%20Text%20Extreme%20Classification%2C%20has%20found%20numerous%0Aapplications%20including%20prediction%20of%20related%20searches%20and%20product%0Arecommendation.%20In%20this%20paper%2C%20we%20propose%20a%20convolutional%20architecture%0AInceptionXML%20which%20is%20light-weight%2C%20yet%20powerful%2C%20and%20robust%20to%20the%20inherent%0Alack%20of%20word-order%20in%20short-text%20queries%20encountered%20in%20search%20and%0Arecommendation.%20We%20demonstrate%20the%20efficacy%20of%20applying%20convolutions%20by%0Arecasting%20the%20operation%20along%20the%20embedding%20dimension%20instead%20of%20the%20word%0Adimension%20as%20applied%20in%20conventional%20CNNs%20for%20text%20classification.%20Towards%0Ascaling%20our%20model%20to%20datasets%20with%20millions%20of%20labels%2C%20we%20also%20propose%20SyncXML%0Apipeline%20which%20improves%20upon%20the%20shortcomings%20of%20the%20recently%20proposed%20dynamic%0Ahard-negative%20mining%20technique%20for%20label%20short-listing%20by%20synchronizing%20the%0Alabel-shortlister%20and%20extreme%20classifier.%20SyncXML%20not%20only%20reduces%20the%0Ainference%20time%20to%20half%20but%20is%20also%20an%20order%20of%20magnitude%20smaller%20than%0Astate-of-the-art%20Astec%20in%20terms%20of%20model%20size.%20Through%20a%20comprehensive%0Aempirical%20comparison%2C%20we%20show%20that%20not%20only%20can%20InceptionXML%20outperform%0Aexisting%20approaches%20on%20benchmark%20datasets%20but%20also%20the%20transformer%20baselines%0Arequiring%20only%202%25%20FLOPs.%20The%20code%20for%20InceptionXML%20is%20available%20at%0Ahttps%3A//github.com/xmc-aalto/inceptionxml.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2109.07319v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInceptionXML%253A%2520A%2520Lightweight%2520Framework%2520with%2520Synchronized%2520Negative%250A%2520%2520Sampling%2520for%2520Short%2520Text%2520Extreme%2520Classification%26entry.906535625%3DSiddhant%2520Kharbanda%2520and%2520Atmadeep%2520Banerjee%2520and%2520Devaansh%2520Gupta%2520and%2520Akash%2520Palrecha%2520and%2520Rohit%2520Babbar%26entry.1292438233%3D%2520%2520Automatic%2520annotation%2520of%2520short-text%2520data%2520to%2520a%2520large%2520number%2520of%2520target%2520labels%252C%250Areferred%2520to%2520as%2520Short%2520Text%2520Extreme%2520Classification%252C%2520has%2520found%2520numerous%250Aapplications%2520including%2520prediction%2520of%2520related%2520searches%2520and%2520product%250Arecommendation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520convolutional%2520architecture%250AInceptionXML%2520which%2520is%2520light-weight%252C%2520yet%2520powerful%252C%2520and%2520robust%2520to%2520the%2520inherent%250Alack%2520of%2520word-order%2520in%2520short-text%2520queries%2520encountered%2520in%2520search%2520and%250Arecommendation.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520applying%2520convolutions%2520by%250Arecasting%2520the%2520operation%2520along%2520the%2520embedding%2520dimension%2520instead%2520of%2520the%2520word%250Adimension%2520as%2520applied%2520in%2520conventional%2520CNNs%2520for%2520text%2520classification.%2520Towards%250Ascaling%2520our%2520model%2520to%2520datasets%2520with%2520millions%2520of%2520labels%252C%2520we%2520also%2520propose%2520SyncXML%250Apipeline%2520which%2520improves%2520upon%2520the%2520shortcomings%2520of%2520the%2520recently%2520proposed%2520dynamic%250Ahard-negative%2520mining%2520technique%2520for%2520label%2520short-listing%2520by%2520synchronizing%2520the%250Alabel-shortlister%2520and%2520extreme%2520classifier.%2520SyncXML%2520not%2520only%2520reduces%2520the%250Ainference%2520time%2520to%2520half%2520but%2520is%2520also%2520an%2520order%2520of%2520magnitude%2520smaller%2520than%250Astate-of-the-art%2520Astec%2520in%2520terms%2520of%2520model%2520size.%2520Through%2520a%2520comprehensive%250Aempirical%2520comparison%252C%2520we%2520show%2520that%2520not%2520only%2520can%2520InceptionXML%2520outperform%250Aexisting%2520approaches%2520on%2520benchmark%2520datasets%2520but%2520also%2520the%2520transformer%2520baselines%250Arequiring%2520only%25202%2525%2520FLOPs.%2520The%2520code%2520for%2520InceptionXML%2520is%2520available%2520at%250Ahttps%253A//github.com/xmc-aalto/inceptionxml.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2109.07319v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InceptionXML%3A%20A%20Lightweight%20Framework%20with%20Synchronized%20Negative%0A%20%20Sampling%20for%20Short%20Text%20Extreme%20Classification&entry.906535625=Siddhant%20Kharbanda%20and%20Atmadeep%20Banerjee%20and%20Devaansh%20Gupta%20and%20Akash%20Palrecha%20and%20Rohit%20Babbar&entry.1292438233=%20%20Automatic%20annotation%20of%20short-text%20data%20to%20a%20large%20number%20of%20target%20labels%2C%0Areferred%20to%20as%20Short%20Text%20Extreme%20Classification%2C%20has%20found%20numerous%0Aapplications%20including%20prediction%20of%20related%20searches%20and%20product%0Arecommendation.%20In%20this%20paper%2C%20we%20propose%20a%20convolutional%20architecture%0AInceptionXML%20which%20is%20light-weight%2C%20yet%20powerful%2C%20and%20robust%20to%20the%20inherent%0Alack%20of%20word-order%20in%20short-text%20queries%20encountered%20in%20search%20and%0Arecommendation.%20We%20demonstrate%20the%20efficacy%20of%20applying%20convolutions%20by%0Arecasting%20the%20operation%20along%20the%20embedding%20dimension%20instead%20of%20the%20word%0Adimension%20as%20applied%20in%20conventional%20CNNs%20for%20text%20classification.%20Towards%0Ascaling%20our%20model%20to%20datasets%20with%20millions%20of%20labels%2C%20we%20also%20propose%20SyncXML%0Apipeline%20which%20improves%20upon%20the%20shortcomings%20of%20the%20recently%20proposed%20dynamic%0Ahard-negative%20mining%20technique%20for%20label%20short-listing%20by%20synchronizing%20the%0Alabel-shortlister%20and%20extreme%20classifier.%20SyncXML%20not%20only%20reduces%20the%0Ainference%20time%20to%20half%20but%20is%20also%20an%20order%20of%20magnitude%20smaller%20than%0Astate-of-the-art%20Astec%20in%20terms%20of%20model%20size.%20Through%20a%20comprehensive%0Aempirical%20comparison%2C%20we%20show%20that%20not%20only%20can%20InceptionXML%20outperform%0Aexisting%20approaches%20on%20benchmark%20datasets%20but%20also%20the%20transformer%20baselines%0Arequiring%20only%202%25%20FLOPs.%20The%20code%20for%20InceptionXML%20is%20available%20at%0Ahttps%3A//github.com/xmc-aalto/inceptionxml.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.07319v4&entry.124074799=Read"},
{"title": "Advancing Pre-trained Teacher: Towards Robust Feature Discrepancy for\n  Anomaly Detection", "author": "Canhui Tang and Sanping Zhou and Yizhe Li and Yonghao Dong and Le Wang", "abstract": "  With the wide application of knowledge distillation between an ImageNet\npre-trained teacher model and a learnable student model, industrial anomaly\ndetection has witnessed a significant achievement in the past few years. The\nsuccess of knowledge distillation mainly relies on how to keep the feature\ndiscrepancy between the teacher and student model, in which it assumes that:\n(1) the teacher model can jointly represent two different distributions for the\nnormal and abnormal patterns, while (2) the student model can only reconstruct\nthe normal distribution. However, it still remains a challenging issue to\nmaintain these ideal assumptions in practice. In this paper, we propose a\nsimple yet effective two-stage industrial anomaly detection framework, termed\nas AAND, which sequentially performs Anomaly Amplification and Normality\nDistillation to obtain robust feature discrepancy. In the first anomaly\namplification stage, we propose a novel Residual Anomaly Amplification (RAA)\nmodule to advance the pre-trained teacher encoder. With the exposure of\nsynthetic anomalies, it amplifies anomalies via residual generation while\nmaintaining the integrity of pre-trained model. It mainly comprises a\nMatching-guided Residual Gate and an Attribute-scaling Residual Generator,\nwhich can determine the residuals' proportion and characteristic, respectively.\nIn the second normality distillation stage, we further employ a reverse\ndistillation paradigm to train a student decoder, in which a novel Hard\nKnowledge Distillation (HKD) loss is built to better facilitate the\nreconstruction of normal patterns. Comprehensive experiments on the MvTecAD,\nVisA, and MvTec3D-RGB datasets show that our method achieves state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2405.02068v1", "date": "2024-05-03", "relevancy": 2.0523, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5105}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Pre-trained%20Teacher%3A%20Towards%20Robust%20Feature%20Discrepancy%20for%0A%20%20Anomaly%20Detection&body=Title%3A%20Advancing%20Pre-trained%20Teacher%3A%20Towards%20Robust%20Feature%20Discrepancy%20for%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Canhui%20Tang%20and%20Sanping%20Zhou%20and%20Yizhe%20Li%20and%20Yonghao%20Dong%20and%20Le%20Wang%0AAbstract%3A%20%20%20With%20the%20wide%20application%20of%20knowledge%20distillation%20between%20an%20ImageNet%0Apre-trained%20teacher%20model%20and%20a%20learnable%20student%20model%2C%20industrial%20anomaly%0Adetection%20has%20witnessed%20a%20significant%20achievement%20in%20the%20past%20few%20years.%20The%0Asuccess%20of%20knowledge%20distillation%20mainly%20relies%20on%20how%20to%20keep%20the%20feature%0Adiscrepancy%20between%20the%20teacher%20and%20student%20model%2C%20in%20which%20it%20assumes%20that%3A%0A%281%29%20the%20teacher%20model%20can%20jointly%20represent%20two%20different%20distributions%20for%20the%0Anormal%20and%20abnormal%20patterns%2C%20while%20%282%29%20the%20student%20model%20can%20only%20reconstruct%0Athe%20normal%20distribution.%20However%2C%20it%20still%20remains%20a%20challenging%20issue%20to%0Amaintain%20these%20ideal%20assumptions%20in%20practice.%20In%20this%20paper%2C%20we%20propose%20a%0Asimple%20yet%20effective%20two-stage%20industrial%20anomaly%20detection%20framework%2C%20termed%0Aas%20AAND%2C%20which%20sequentially%20performs%20Anomaly%20Amplification%20and%20Normality%0ADistillation%20to%20obtain%20robust%20feature%20discrepancy.%20In%20the%20first%20anomaly%0Aamplification%20stage%2C%20we%20propose%20a%20novel%20Residual%20Anomaly%20Amplification%20%28RAA%29%0Amodule%20to%20advance%20the%20pre-trained%20teacher%20encoder.%20With%20the%20exposure%20of%0Asynthetic%20anomalies%2C%20it%20amplifies%20anomalies%20via%20residual%20generation%20while%0Amaintaining%20the%20integrity%20of%20pre-trained%20model.%20It%20mainly%20comprises%20a%0AMatching-guided%20Residual%20Gate%20and%20an%20Attribute-scaling%20Residual%20Generator%2C%0Awhich%20can%20determine%20the%20residuals%27%20proportion%20and%20characteristic%2C%20respectively.%0AIn%20the%20second%20normality%20distillation%20stage%2C%20we%20further%20employ%20a%20reverse%0Adistillation%20paradigm%20to%20train%20a%20student%20decoder%2C%20in%20which%20a%20novel%20Hard%0AKnowledge%20Distillation%20%28HKD%29%20loss%20is%20built%20to%20better%20facilitate%20the%0Areconstruction%20of%20normal%20patterns.%20Comprehensive%20experiments%20on%20the%20MvTecAD%2C%0AVisA%2C%20and%20MvTec3D-RGB%20datasets%20show%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Pre-trained%2520Teacher%253A%2520Towards%2520Robust%2520Feature%2520Discrepancy%2520for%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DCanhui%2520Tang%2520and%2520Sanping%2520Zhou%2520and%2520Yizhe%2520Li%2520and%2520Yonghao%2520Dong%2520and%2520Le%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520wide%2520application%2520of%2520knowledge%2520distillation%2520between%2520an%2520ImageNet%250Apre-trained%2520teacher%2520model%2520and%2520a%2520learnable%2520student%2520model%252C%2520industrial%2520anomaly%250Adetection%2520has%2520witnessed%2520a%2520significant%2520achievement%2520in%2520the%2520past%2520few%2520years.%2520The%250Asuccess%2520of%2520knowledge%2520distillation%2520mainly%2520relies%2520on%2520how%2520to%2520keep%2520the%2520feature%250Adiscrepancy%2520between%2520the%2520teacher%2520and%2520student%2520model%252C%2520in%2520which%2520it%2520assumes%2520that%253A%250A%25281%2529%2520the%2520teacher%2520model%2520can%2520jointly%2520represent%2520two%2520different%2520distributions%2520for%2520the%250Anormal%2520and%2520abnormal%2520patterns%252C%2520while%2520%25282%2529%2520the%2520student%2520model%2520can%2520only%2520reconstruct%250Athe%2520normal%2520distribution.%2520However%252C%2520it%2520still%2520remains%2520a%2520challenging%2520issue%2520to%250Amaintain%2520these%2520ideal%2520assumptions%2520in%2520practice.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520two-stage%2520industrial%2520anomaly%2520detection%2520framework%252C%2520termed%250Aas%2520AAND%252C%2520which%2520sequentially%2520performs%2520Anomaly%2520Amplification%2520and%2520Normality%250ADistillation%2520to%2520obtain%2520robust%2520feature%2520discrepancy.%2520In%2520the%2520first%2520anomaly%250Aamplification%2520stage%252C%2520we%2520propose%2520a%2520novel%2520Residual%2520Anomaly%2520Amplification%2520%2528RAA%2529%250Amodule%2520to%2520advance%2520the%2520pre-trained%2520teacher%2520encoder.%2520With%2520the%2520exposure%2520of%250Asynthetic%2520anomalies%252C%2520it%2520amplifies%2520anomalies%2520via%2520residual%2520generation%2520while%250Amaintaining%2520the%2520integrity%2520of%2520pre-trained%2520model.%2520It%2520mainly%2520comprises%2520a%250AMatching-guided%2520Residual%2520Gate%2520and%2520an%2520Attribute-scaling%2520Residual%2520Generator%252C%250Awhich%2520can%2520determine%2520the%2520residuals%2527%2520proportion%2520and%2520characteristic%252C%2520respectively.%250AIn%2520the%2520second%2520normality%2520distillation%2520stage%252C%2520we%2520further%2520employ%2520a%2520reverse%250Adistillation%2520paradigm%2520to%2520train%2520a%2520student%2520decoder%252C%2520in%2520which%2520a%2520novel%2520Hard%250AKnowledge%2520Distillation%2520%2528HKD%2529%2520loss%2520is%2520built%2520to%2520better%2520facilitate%2520the%250Areconstruction%2520of%2520normal%2520patterns.%2520Comprehensive%2520experiments%2520on%2520the%2520MvTecAD%252C%250AVisA%252C%2520and%2520MvTec3D-RGB%2520datasets%2520show%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Pre-trained%20Teacher%3A%20Towards%20Robust%20Feature%20Discrepancy%20for%0A%20%20Anomaly%20Detection&entry.906535625=Canhui%20Tang%20and%20Sanping%20Zhou%20and%20Yizhe%20Li%20and%20Yonghao%20Dong%20and%20Le%20Wang&entry.1292438233=%20%20With%20the%20wide%20application%20of%20knowledge%20distillation%20between%20an%20ImageNet%0Apre-trained%20teacher%20model%20and%20a%20learnable%20student%20model%2C%20industrial%20anomaly%0Adetection%20has%20witnessed%20a%20significant%20achievement%20in%20the%20past%20few%20years.%20The%0Asuccess%20of%20knowledge%20distillation%20mainly%20relies%20on%20how%20to%20keep%20the%20feature%0Adiscrepancy%20between%20the%20teacher%20and%20student%20model%2C%20in%20which%20it%20assumes%20that%3A%0A%281%29%20the%20teacher%20model%20can%20jointly%20represent%20two%20different%20distributions%20for%20the%0Anormal%20and%20abnormal%20patterns%2C%20while%20%282%29%20the%20student%20model%20can%20only%20reconstruct%0Athe%20normal%20distribution.%20However%2C%20it%20still%20remains%20a%20challenging%20issue%20to%0Amaintain%20these%20ideal%20assumptions%20in%20practice.%20In%20this%20paper%2C%20we%20propose%20a%0Asimple%20yet%20effective%20two-stage%20industrial%20anomaly%20detection%20framework%2C%20termed%0Aas%20AAND%2C%20which%20sequentially%20performs%20Anomaly%20Amplification%20and%20Normality%0ADistillation%20to%20obtain%20robust%20feature%20discrepancy.%20In%20the%20first%20anomaly%0Aamplification%20stage%2C%20we%20propose%20a%20novel%20Residual%20Anomaly%20Amplification%20%28RAA%29%0Amodule%20to%20advance%20the%20pre-trained%20teacher%20encoder.%20With%20the%20exposure%20of%0Asynthetic%20anomalies%2C%20it%20amplifies%20anomalies%20via%20residual%20generation%20while%0Amaintaining%20the%20integrity%20of%20pre-trained%20model.%20It%20mainly%20comprises%20a%0AMatching-guided%20Residual%20Gate%20and%20an%20Attribute-scaling%20Residual%20Generator%2C%0Awhich%20can%20determine%20the%20residuals%27%20proportion%20and%20characteristic%2C%20respectively.%0AIn%20the%20second%20normality%20distillation%20stage%2C%20we%20further%20employ%20a%20reverse%0Adistillation%20paradigm%20to%20train%20a%20student%20decoder%2C%20in%20which%20a%20novel%20Hard%0AKnowledge%20Distillation%20%28HKD%29%20loss%20is%20built%20to%20better%20facilitate%20the%0Areconstruction%20of%20normal%20patterns.%20Comprehensive%20experiments%20on%20the%20MvTecAD%2C%0AVisA%2C%20and%20MvTec3D-RGB%20datasets%20show%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02068v1&entry.124074799=Read"},
{"title": "Forensic License Plate Recognition with Compression-Informed\n  Transformers", "author": "Denise Moussa and Anatol Maier and Andreas Spruck and J\u00fcrgen Seiler and Christian Riess", "abstract": "  Forensic license plate recognition (FLPR) remains an open challenge in legal\ncontexts such as criminal investigations, where unreadable license plates (LPs)\nneed to be deciphered from highly compressed and/or low resolution footage,\ne.g., from surveillance cameras. In this work, we propose a side-informed\nTransformer architecture that embeds knowledge on the input compression level\nto improve recognition under strong compression. We show the effectiveness of\nTransformers for license plate recognition (LPR) on a low-quality real-world\ndataset. We also provide a synthetic dataset that includes strongly degraded,\nillegible LP images and analyze the impact of knowledge embedding on it. The\nnetwork outperforms existing FLPR methods and standard state-of-the art image\nrecognition models while requiring less parameters. For the severest degraded\nimages, we can improve recognition by up to 8.9 percent points.\n", "link": "http://arxiv.org/abs/2207.14686v3", "date": "2024-05-03", "relevancy": 2.0498, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5249}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forensic%20License%20Plate%20Recognition%20with%20Compression-Informed%0A%20%20Transformers&body=Title%3A%20Forensic%20License%20Plate%20Recognition%20with%20Compression-Informed%0A%20%20Transformers%0AAuthor%3A%20Denise%20Moussa%20and%20Anatol%20Maier%20and%20Andreas%20Spruck%20and%20J%C3%BCrgen%20Seiler%20and%20Christian%20Riess%0AAbstract%3A%20%20%20Forensic%20license%20plate%20recognition%20%28FLPR%29%20remains%20an%20open%20challenge%20in%20legal%0Acontexts%20such%20as%20criminal%20investigations%2C%20where%20unreadable%20license%20plates%20%28LPs%29%0Aneed%20to%20be%20deciphered%20from%20highly%20compressed%20and/or%20low%20resolution%20footage%2C%0Ae.g.%2C%20from%20surveillance%20cameras.%20In%20this%20work%2C%20we%20propose%20a%20side-informed%0ATransformer%20architecture%20that%20embeds%20knowledge%20on%20the%20input%20compression%20level%0Ato%20improve%20recognition%20under%20strong%20compression.%20We%20show%20the%20effectiveness%20of%0ATransformers%20for%20license%20plate%20recognition%20%28LPR%29%20on%20a%20low-quality%20real-world%0Adataset.%20We%20also%20provide%20a%20synthetic%20dataset%20that%20includes%20strongly%20degraded%2C%0Aillegible%20LP%20images%20and%20analyze%20the%20impact%20of%20knowledge%20embedding%20on%20it.%20The%0Anetwork%20outperforms%20existing%20FLPR%20methods%20and%20standard%20state-of-the%20art%20image%0Arecognition%20models%20while%20requiring%20less%20parameters.%20For%20the%20severest%20degraded%0Aimages%2C%20we%20can%20improve%20recognition%20by%20up%20to%208.9%20percent%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.14686v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForensic%2520License%2520Plate%2520Recognition%2520with%2520Compression-Informed%250A%2520%2520Transformers%26entry.906535625%3DDenise%2520Moussa%2520and%2520Anatol%2520Maier%2520and%2520Andreas%2520Spruck%2520and%2520J%25C3%25BCrgen%2520Seiler%2520and%2520Christian%2520Riess%26entry.1292438233%3D%2520%2520Forensic%2520license%2520plate%2520recognition%2520%2528FLPR%2529%2520remains%2520an%2520open%2520challenge%2520in%2520legal%250Acontexts%2520such%2520as%2520criminal%2520investigations%252C%2520where%2520unreadable%2520license%2520plates%2520%2528LPs%2529%250Aneed%2520to%2520be%2520deciphered%2520from%2520highly%2520compressed%2520and/or%2520low%2520resolution%2520footage%252C%250Ae.g.%252C%2520from%2520surveillance%2520cameras.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520side-informed%250ATransformer%2520architecture%2520that%2520embeds%2520knowledge%2520on%2520the%2520input%2520compression%2520level%250Ato%2520improve%2520recognition%2520under%2520strong%2520compression.%2520We%2520show%2520the%2520effectiveness%2520of%250ATransformers%2520for%2520license%2520plate%2520recognition%2520%2528LPR%2529%2520on%2520a%2520low-quality%2520real-world%250Adataset.%2520We%2520also%2520provide%2520a%2520synthetic%2520dataset%2520that%2520includes%2520strongly%2520degraded%252C%250Aillegible%2520LP%2520images%2520and%2520analyze%2520the%2520impact%2520of%2520knowledge%2520embedding%2520on%2520it.%2520The%250Anetwork%2520outperforms%2520existing%2520FLPR%2520methods%2520and%2520standard%2520state-of-the%2520art%2520image%250Arecognition%2520models%2520while%2520requiring%2520less%2520parameters.%2520For%2520the%2520severest%2520degraded%250Aimages%252C%2520we%2520can%2520improve%2520recognition%2520by%2520up%2520to%25208.9%2520percent%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.14686v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forensic%20License%20Plate%20Recognition%20with%20Compression-Informed%0A%20%20Transformers&entry.906535625=Denise%20Moussa%20and%20Anatol%20Maier%20and%20Andreas%20Spruck%20and%20J%C3%BCrgen%20Seiler%20and%20Christian%20Riess&entry.1292438233=%20%20Forensic%20license%20plate%20recognition%20%28FLPR%29%20remains%20an%20open%20challenge%20in%20legal%0Acontexts%20such%20as%20criminal%20investigations%2C%20where%20unreadable%20license%20plates%20%28LPs%29%0Aneed%20to%20be%20deciphered%20from%20highly%20compressed%20and/or%20low%20resolution%20footage%2C%0Ae.g.%2C%20from%20surveillance%20cameras.%20In%20this%20work%2C%20we%20propose%20a%20side-informed%0ATransformer%20architecture%20that%20embeds%20knowledge%20on%20the%20input%20compression%20level%0Ato%20improve%20recognition%20under%20strong%20compression.%20We%20show%20the%20effectiveness%20of%0ATransformers%20for%20license%20plate%20recognition%20%28LPR%29%20on%20a%20low-quality%20real-world%0Adataset.%20We%20also%20provide%20a%20synthetic%20dataset%20that%20includes%20strongly%20degraded%2C%0Aillegible%20LP%20images%20and%20analyze%20the%20impact%20of%20knowledge%20embedding%20on%20it.%20The%0Anetwork%20outperforms%20existing%20FLPR%20methods%20and%20standard%20state-of-the%20art%20image%0Arecognition%20models%20while%20requiring%20less%20parameters.%20For%20the%20severest%20degraded%0Aimages%2C%20we%20can%20improve%20recognition%20by%20up%20to%208.9%20percent%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.14686v3&entry.124074799=Read"},
{"title": "Hysteresis Compensation of Flexible Continuum Manipulator using RGBD\n  Sensing and Temporal Convolutional Network", "author": "Junhyun Park and Seonghyeok Jang and Hyojae Park and Seongjun Bae and Minho Hwang", "abstract": "  Flexible continuum manipulators are valued for minimally invasive surgery,\noffering access to confined spaces through nonlinear paths. However,\ncable-driven manipulators face control difficulties due to hysteresis from\ncabling effects such as friction, elongation, and coupling. These effects are\ndifficult to model due to nonlinearity and the difficulties become even more\nevident when dealing with long and coupled, multi-segmented manipulator. This\npaper proposes a data-driven approach based on Deep Neural Networks (DNN) to\ncapture these nonlinear and previous states-dependent characteristics of cable\nactuation. We collect physical joint configurations according to command joint\nconfigurations using RGBD sensing and 7 fiducial markers to model the\nhysteresis of the proposed manipulator. Result on a study comparing the\nestimation performance of four DNN models show that the Temporal Convolution\nNetwork (TCN) demonstrates the highest predictive capability. Leveraging\ntrained TCNs, we build a control algorithm to compensate for hysteresis.\nTracking tests in task space using unseen trajectories show that the proposed\ncontrol algorithm reduces the average position and orientation error by 61.39%\n(from 13.7mm to 5.29 mm) and 64.04% (from 31.17{\\deg} to 11.21{\\deg}),\nrespectively. This result implies that the proposed calibrated controller\neffectively reaches the desired configurations by estimating the hysteresis of\nthe manipulator. Applying this method in real surgical scenarios has the\npotential to enhance control precision and improve surgical performance.\n", "link": "http://arxiv.org/abs/2402.11319v3", "date": "2024-05-03", "relevancy": 2.0486, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5144}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5129}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hysteresis%20Compensation%20of%20Flexible%20Continuum%20Manipulator%20using%20RGBD%0A%20%20Sensing%20and%20Temporal%20Convolutional%20Network&body=Title%3A%20Hysteresis%20Compensation%20of%20Flexible%20Continuum%20Manipulator%20using%20RGBD%0A%20%20Sensing%20and%20Temporal%20Convolutional%20Network%0AAuthor%3A%20Junhyun%20Park%20and%20Seonghyeok%20Jang%20and%20Hyojae%20Park%20and%20Seongjun%20Bae%20and%20Minho%20Hwang%0AAbstract%3A%20%20%20Flexible%20continuum%20manipulators%20are%20valued%20for%20minimally%20invasive%20surgery%2C%0Aoffering%20access%20to%20confined%20spaces%20through%20nonlinear%20paths.%20However%2C%0Acable-driven%20manipulators%20face%20control%20difficulties%20due%20to%20hysteresis%20from%0Acabling%20effects%20such%20as%20friction%2C%20elongation%2C%20and%20coupling.%20These%20effects%20are%0Adifficult%20to%20model%20due%20to%20nonlinearity%20and%20the%20difficulties%20become%20even%20more%0Aevident%20when%20dealing%20with%20long%20and%20coupled%2C%20multi-segmented%20manipulator.%20This%0Apaper%20proposes%20a%20data-driven%20approach%20based%20on%20Deep%20Neural%20Networks%20%28DNN%29%20to%0Acapture%20these%20nonlinear%20and%20previous%20states-dependent%20characteristics%20of%20cable%0Aactuation.%20We%20collect%20physical%20joint%20configurations%20according%20to%20command%20joint%0Aconfigurations%20using%20RGBD%20sensing%20and%207%20fiducial%20markers%20to%20model%20the%0Ahysteresis%20of%20the%20proposed%20manipulator.%20Result%20on%20a%20study%20comparing%20the%0Aestimation%20performance%20of%20four%20DNN%20models%20show%20that%20the%20Temporal%20Convolution%0ANetwork%20%28TCN%29%20demonstrates%20the%20highest%20predictive%20capability.%20Leveraging%0Atrained%20TCNs%2C%20we%20build%20a%20control%20algorithm%20to%20compensate%20for%20hysteresis.%0ATracking%20tests%20in%20task%20space%20using%20unseen%20trajectories%20show%20that%20the%20proposed%0Acontrol%20algorithm%20reduces%20the%20average%20position%20and%20orientation%20error%20by%2061.39%25%0A%28from%2013.7mm%20to%205.29%20mm%29%20and%2064.04%25%20%28from%2031.17%7B%5Cdeg%7D%20to%2011.21%7B%5Cdeg%7D%29%2C%0Arespectively.%20This%20result%20implies%20that%20the%20proposed%20calibrated%20controller%0Aeffectively%20reaches%20the%20desired%20configurations%20by%20estimating%20the%20hysteresis%20of%0Athe%20manipulator.%20Applying%20this%20method%20in%20real%20surgical%20scenarios%20has%20the%0Apotential%20to%20enhance%20control%20precision%20and%20improve%20surgical%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11319v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHysteresis%2520Compensation%2520of%2520Flexible%2520Continuum%2520Manipulator%2520using%2520RGBD%250A%2520%2520Sensing%2520and%2520Temporal%2520Convolutional%2520Network%26entry.906535625%3DJunhyun%2520Park%2520and%2520Seonghyeok%2520Jang%2520and%2520Hyojae%2520Park%2520and%2520Seongjun%2520Bae%2520and%2520Minho%2520Hwang%26entry.1292438233%3D%2520%2520Flexible%2520continuum%2520manipulators%2520are%2520valued%2520for%2520minimally%2520invasive%2520surgery%252C%250Aoffering%2520access%2520to%2520confined%2520spaces%2520through%2520nonlinear%2520paths.%2520However%252C%250Acable-driven%2520manipulators%2520face%2520control%2520difficulties%2520due%2520to%2520hysteresis%2520from%250Acabling%2520effects%2520such%2520as%2520friction%252C%2520elongation%252C%2520and%2520coupling.%2520These%2520effects%2520are%250Adifficult%2520to%2520model%2520due%2520to%2520nonlinearity%2520and%2520the%2520difficulties%2520become%2520even%2520more%250Aevident%2520when%2520dealing%2520with%2520long%2520and%2520coupled%252C%2520multi-segmented%2520manipulator.%2520This%250Apaper%2520proposes%2520a%2520data-driven%2520approach%2520based%2520on%2520Deep%2520Neural%2520Networks%2520%2528DNN%2529%2520to%250Acapture%2520these%2520nonlinear%2520and%2520previous%2520states-dependent%2520characteristics%2520of%2520cable%250Aactuation.%2520We%2520collect%2520physical%2520joint%2520configurations%2520according%2520to%2520command%2520joint%250Aconfigurations%2520using%2520RGBD%2520sensing%2520and%25207%2520fiducial%2520markers%2520to%2520model%2520the%250Ahysteresis%2520of%2520the%2520proposed%2520manipulator.%2520Result%2520on%2520a%2520study%2520comparing%2520the%250Aestimation%2520performance%2520of%2520four%2520DNN%2520models%2520show%2520that%2520the%2520Temporal%2520Convolution%250ANetwork%2520%2528TCN%2529%2520demonstrates%2520the%2520highest%2520predictive%2520capability.%2520Leveraging%250Atrained%2520TCNs%252C%2520we%2520build%2520a%2520control%2520algorithm%2520to%2520compensate%2520for%2520hysteresis.%250ATracking%2520tests%2520in%2520task%2520space%2520using%2520unseen%2520trajectories%2520show%2520that%2520the%2520proposed%250Acontrol%2520algorithm%2520reduces%2520the%2520average%2520position%2520and%2520orientation%2520error%2520by%252061.39%2525%250A%2528from%252013.7mm%2520to%25205.29%2520mm%2529%2520and%252064.04%2525%2520%2528from%252031.17%257B%255Cdeg%257D%2520to%252011.21%257B%255Cdeg%257D%2529%252C%250Arespectively.%2520This%2520result%2520implies%2520that%2520the%2520proposed%2520calibrated%2520controller%250Aeffectively%2520reaches%2520the%2520desired%2520configurations%2520by%2520estimating%2520the%2520hysteresis%2520of%250Athe%2520manipulator.%2520Applying%2520this%2520method%2520in%2520real%2520surgical%2520scenarios%2520has%2520the%250Apotential%2520to%2520enhance%2520control%2520precision%2520and%2520improve%2520surgical%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11319v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hysteresis%20Compensation%20of%20Flexible%20Continuum%20Manipulator%20using%20RGBD%0A%20%20Sensing%20and%20Temporal%20Convolutional%20Network&entry.906535625=Junhyun%20Park%20and%20Seonghyeok%20Jang%20and%20Hyojae%20Park%20and%20Seongjun%20Bae%20and%20Minho%20Hwang&entry.1292438233=%20%20Flexible%20continuum%20manipulators%20are%20valued%20for%20minimally%20invasive%20surgery%2C%0Aoffering%20access%20to%20confined%20spaces%20through%20nonlinear%20paths.%20However%2C%0Acable-driven%20manipulators%20face%20control%20difficulties%20due%20to%20hysteresis%20from%0Acabling%20effects%20such%20as%20friction%2C%20elongation%2C%20and%20coupling.%20These%20effects%20are%0Adifficult%20to%20model%20due%20to%20nonlinearity%20and%20the%20difficulties%20become%20even%20more%0Aevident%20when%20dealing%20with%20long%20and%20coupled%2C%20multi-segmented%20manipulator.%20This%0Apaper%20proposes%20a%20data-driven%20approach%20based%20on%20Deep%20Neural%20Networks%20%28DNN%29%20to%0Acapture%20these%20nonlinear%20and%20previous%20states-dependent%20characteristics%20of%20cable%0Aactuation.%20We%20collect%20physical%20joint%20configurations%20according%20to%20command%20joint%0Aconfigurations%20using%20RGBD%20sensing%20and%207%20fiducial%20markers%20to%20model%20the%0Ahysteresis%20of%20the%20proposed%20manipulator.%20Result%20on%20a%20study%20comparing%20the%0Aestimation%20performance%20of%20four%20DNN%20models%20show%20that%20the%20Temporal%20Convolution%0ANetwork%20%28TCN%29%20demonstrates%20the%20highest%20predictive%20capability.%20Leveraging%0Atrained%20TCNs%2C%20we%20build%20a%20control%20algorithm%20to%20compensate%20for%20hysteresis.%0ATracking%20tests%20in%20task%20space%20using%20unseen%20trajectories%20show%20that%20the%20proposed%0Acontrol%20algorithm%20reduces%20the%20average%20position%20and%20orientation%20error%20by%2061.39%25%0A%28from%2013.7mm%20to%205.29%20mm%29%20and%2064.04%25%20%28from%2031.17%7B%5Cdeg%7D%20to%2011.21%7B%5Cdeg%7D%29%2C%0Arespectively.%20This%20result%20implies%20that%20the%20proposed%20calibrated%20controller%0Aeffectively%20reaches%20the%20desired%20configurations%20by%20estimating%20the%20hysteresis%20of%0Athe%20manipulator.%20Applying%20this%20method%20in%20real%20surgical%20scenarios%20has%20the%0Apotential%20to%20enhance%20control%20precision%20and%20improve%20surgical%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11319v3&entry.124074799=Read"},
{"title": "SFFNet: A Wavelet-Based Spatial and Frequency Domain Fusion Network for\n  Remote Sensing Segmentation", "author": "Yunsong Yang and Genji Yuan and Jinjiang Li", "abstract": "  In order to fully utilize spatial information for segmentation and address\nthe challenge of handling areas with significant grayscale variations in remote\nsensing segmentation, we propose the SFFNet (Spatial and Frequency Domain\nFusion Network) framework. This framework employs a two-stage network design:\nthe first stage extracts features using spatial methods to obtain features with\nsufficient spatial details and semantic information; the second stage maps\nthese features in both spatial and frequency domains. In the frequency domain\nmapping, we introduce the Wavelet Transform Feature Decomposer (WTFD)\nstructure, which decomposes features into low-frequency and high-frequency\ncomponents using the Haar wavelet transform and integrates them with spatial\nfeatures. To bridge the semantic gap between frequency and spatial features,\nand facilitate significant feature selection to promote the combination of\nfeatures from different representation domains, we design the Multiscale\nDual-Representation Alignment Filter (MDAF). This structure utilizes multiscale\nconvolutions and dual-cross attentions. Comprehensive experimental results\ndemonstrate that, compared to existing methods, SFFNet achieves superior\nperformance in terms of mIoU, reaching 84.80% and 87.73% respectively.The code\nis located at https://github.com/yysdck/SFFNet.\n", "link": "http://arxiv.org/abs/2405.01992v1", "date": "2024-05-03", "relevancy": 2.0472, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5167}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5111}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFFNet%3A%20A%20Wavelet-Based%20Spatial%20and%20Frequency%20Domain%20Fusion%20Network%20for%0A%20%20Remote%20Sensing%20Segmentation&body=Title%3A%20SFFNet%3A%20A%20Wavelet-Based%20Spatial%20and%20Frequency%20Domain%20Fusion%20Network%20for%0A%20%20Remote%20Sensing%20Segmentation%0AAuthor%3A%20Yunsong%20Yang%20and%20Genji%20Yuan%20and%20Jinjiang%20Li%0AAbstract%3A%20%20%20In%20order%20to%20fully%20utilize%20spatial%20information%20for%20segmentation%20and%20address%0Athe%20challenge%20of%20handling%20areas%20with%20significant%20grayscale%20variations%20in%20remote%0Asensing%20segmentation%2C%20we%20propose%20the%20SFFNet%20%28Spatial%20and%20Frequency%20Domain%0AFusion%20Network%29%20framework.%20This%20framework%20employs%20a%20two-stage%20network%20design%3A%0Athe%20first%20stage%20extracts%20features%20using%20spatial%20methods%20to%20obtain%20features%20with%0Asufficient%20spatial%20details%20and%20semantic%20information%3B%20the%20second%20stage%20maps%0Athese%20features%20in%20both%20spatial%20and%20frequency%20domains.%20In%20the%20frequency%20domain%0Amapping%2C%20we%20introduce%20the%20Wavelet%20Transform%20Feature%20Decomposer%20%28WTFD%29%0Astructure%2C%20which%20decomposes%20features%20into%20low-frequency%20and%20high-frequency%0Acomponents%20using%20the%20Haar%20wavelet%20transform%20and%20integrates%20them%20with%20spatial%0Afeatures.%20To%20bridge%20the%20semantic%20gap%20between%20frequency%20and%20spatial%20features%2C%0Aand%20facilitate%20significant%20feature%20selection%20to%20promote%20the%20combination%20of%0Afeatures%20from%20different%20representation%20domains%2C%20we%20design%20the%20Multiscale%0ADual-Representation%20Alignment%20Filter%20%28MDAF%29.%20This%20structure%20utilizes%20multiscale%0Aconvolutions%20and%20dual-cross%20attentions.%20Comprehensive%20experimental%20results%0Ademonstrate%20that%2C%20compared%20to%20existing%20methods%2C%20SFFNet%20achieves%20superior%0Aperformance%20in%20terms%20of%20mIoU%2C%20reaching%2084.80%25%20and%2087.73%25%20respectively.The%20code%0Ais%20located%20at%20https%3A//github.com/yysdck/SFFNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFFNet%253A%2520A%2520Wavelet-Based%2520Spatial%2520and%2520Frequency%2520Domain%2520Fusion%2520Network%2520for%250A%2520%2520Remote%2520Sensing%2520Segmentation%26entry.906535625%3DYunsong%2520Yang%2520and%2520Genji%2520Yuan%2520and%2520Jinjiang%2520Li%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520fully%2520utilize%2520spatial%2520information%2520for%2520segmentation%2520and%2520address%250Athe%2520challenge%2520of%2520handling%2520areas%2520with%2520significant%2520grayscale%2520variations%2520in%2520remote%250Asensing%2520segmentation%252C%2520we%2520propose%2520the%2520SFFNet%2520%2528Spatial%2520and%2520Frequency%2520Domain%250AFusion%2520Network%2529%2520framework.%2520This%2520framework%2520employs%2520a%2520two-stage%2520network%2520design%253A%250Athe%2520first%2520stage%2520extracts%2520features%2520using%2520spatial%2520methods%2520to%2520obtain%2520features%2520with%250Asufficient%2520spatial%2520details%2520and%2520semantic%2520information%253B%2520the%2520second%2520stage%2520maps%250Athese%2520features%2520in%2520both%2520spatial%2520and%2520frequency%2520domains.%2520In%2520the%2520frequency%2520domain%250Amapping%252C%2520we%2520introduce%2520the%2520Wavelet%2520Transform%2520Feature%2520Decomposer%2520%2528WTFD%2529%250Astructure%252C%2520which%2520decomposes%2520features%2520into%2520low-frequency%2520and%2520high-frequency%250Acomponents%2520using%2520the%2520Haar%2520wavelet%2520transform%2520and%2520integrates%2520them%2520with%2520spatial%250Afeatures.%2520To%2520bridge%2520the%2520semantic%2520gap%2520between%2520frequency%2520and%2520spatial%2520features%252C%250Aand%2520facilitate%2520significant%2520feature%2520selection%2520to%2520promote%2520the%2520combination%2520of%250Afeatures%2520from%2520different%2520representation%2520domains%252C%2520we%2520design%2520the%2520Multiscale%250ADual-Representation%2520Alignment%2520Filter%2520%2528MDAF%2529.%2520This%2520structure%2520utilizes%2520multiscale%250Aconvolutions%2520and%2520dual-cross%2520attentions.%2520Comprehensive%2520experimental%2520results%250Ademonstrate%2520that%252C%2520compared%2520to%2520existing%2520methods%252C%2520SFFNet%2520achieves%2520superior%250Aperformance%2520in%2520terms%2520of%2520mIoU%252C%2520reaching%252084.80%2525%2520and%252087.73%2525%2520respectively.The%2520code%250Ais%2520located%2520at%2520https%253A//github.com/yysdck/SFFNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFFNet%3A%20A%20Wavelet-Based%20Spatial%20and%20Frequency%20Domain%20Fusion%20Network%20for%0A%20%20Remote%20Sensing%20Segmentation&entry.906535625=Yunsong%20Yang%20and%20Genji%20Yuan%20and%20Jinjiang%20Li&entry.1292438233=%20%20In%20order%20to%20fully%20utilize%20spatial%20information%20for%20segmentation%20and%20address%0Athe%20challenge%20of%20handling%20areas%20with%20significant%20grayscale%20variations%20in%20remote%0Asensing%20segmentation%2C%20we%20propose%20the%20SFFNet%20%28Spatial%20and%20Frequency%20Domain%0AFusion%20Network%29%20framework.%20This%20framework%20employs%20a%20two-stage%20network%20design%3A%0Athe%20first%20stage%20extracts%20features%20using%20spatial%20methods%20to%20obtain%20features%20with%0Asufficient%20spatial%20details%20and%20semantic%20information%3B%20the%20second%20stage%20maps%0Athese%20features%20in%20both%20spatial%20and%20frequency%20domains.%20In%20the%20frequency%20domain%0Amapping%2C%20we%20introduce%20the%20Wavelet%20Transform%20Feature%20Decomposer%20%28WTFD%29%0Astructure%2C%20which%20decomposes%20features%20into%20low-frequency%20and%20high-frequency%0Acomponents%20using%20the%20Haar%20wavelet%20transform%20and%20integrates%20them%20with%20spatial%0Afeatures.%20To%20bridge%20the%20semantic%20gap%20between%20frequency%20and%20spatial%20features%2C%0Aand%20facilitate%20significant%20feature%20selection%20to%20promote%20the%20combination%20of%0Afeatures%20from%20different%20representation%20domains%2C%20we%20design%20the%20Multiscale%0ADual-Representation%20Alignment%20Filter%20%28MDAF%29.%20This%20structure%20utilizes%20multiscale%0Aconvolutions%20and%20dual-cross%20attentions.%20Comprehensive%20experimental%20results%0Ademonstrate%20that%2C%20compared%20to%20existing%20methods%2C%20SFFNet%20achieves%20superior%0Aperformance%20in%20terms%20of%20mIoU%2C%20reaching%2084.80%25%20and%2087.73%25%20respectively.The%20code%0Ais%20located%20at%20https%3A//github.com/yysdck/SFFNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01992v1&entry.124074799=Read"},
{"title": "What matters when building vision-language models?", "author": "Hugo Lauren\u00e7on and L\u00e9o Tronchon and Matthieu Cord and Victor Sanh", "abstract": "  The growing interest in vision-language models (VLMs) has been driven by\nimprovements in large language models and vision transformers. Despite the\nabundance of literature on this subject, we observe that critical decisions\nregarding the design of VLMs are often not justified. We argue that these\nunsupported decisions impede progress in the field by making it difficult to\nidentify which choices improve model performance. To address this issue, we\nconduct extensive experiments around pre-trained models, architecture choice,\ndata, and training methods. Our consolidation of findings includes the\ndevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.\nIdefics2 achieves state-of-the-art performance within its size category across\nvarious multimodal benchmarks, and is often on par with models four times its\nsize. We release the model (base, instructed, and chat) along with the datasets\ncreated for its training.\n", "link": "http://arxiv.org/abs/2405.02246v1", "date": "2024-05-03", "relevancy": 2.0439, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5179}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20matters%20when%20building%20vision-language%20models%3F&body=Title%3A%20What%20matters%20when%20building%20vision-language%20models%3F%0AAuthor%3A%20Hugo%20Lauren%C3%A7on%20and%20L%C3%A9o%20Tronchon%20and%20Matthieu%20Cord%20and%20Victor%20Sanh%0AAbstract%3A%20%20%20The%20growing%20interest%20in%20vision-language%20models%20%28VLMs%29%20has%20been%20driven%20by%0Aimprovements%20in%20large%20language%20models%20and%20vision%20transformers.%20Despite%20the%0Aabundance%20of%20literature%20on%20this%20subject%2C%20we%20observe%20that%20critical%20decisions%0Aregarding%20the%20design%20of%20VLMs%20are%20often%20not%20justified.%20We%20argue%20that%20these%0Aunsupported%20decisions%20impede%20progress%20in%20the%20field%20by%20making%20it%20difficult%20to%0Aidentify%20which%20choices%20improve%20model%20performance.%20To%20address%20this%20issue%2C%20we%0Aconduct%20extensive%20experiments%20around%20pre-trained%20models%2C%20architecture%20choice%2C%0Adata%2C%20and%20training%20methods.%20Our%20consolidation%20of%20findings%20includes%20the%0Adevelopment%20of%20Idefics2%2C%20an%20efficient%20foundational%20VLM%20of%208%20billion%20parameters.%0AIdefics2%20achieves%20state-of-the-art%20performance%20within%20its%20size%20category%20across%0Avarious%20multimodal%20benchmarks%2C%20and%20is%20often%20on%20par%20with%20models%20four%20times%20its%0Asize.%20We%20release%20the%20model%20%28base%2C%20instructed%2C%20and%20chat%29%20along%20with%20the%20datasets%0Acreated%20for%20its%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520matters%2520when%2520building%2520vision-language%2520models%253F%26entry.906535625%3DHugo%2520Lauren%25C3%25A7on%2520and%2520L%25C3%25A9o%2520Tronchon%2520and%2520Matthieu%2520Cord%2520and%2520Victor%2520Sanh%26entry.1292438233%3D%2520%2520The%2520growing%2520interest%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520has%2520been%2520driven%2520by%250Aimprovements%2520in%2520large%2520language%2520models%2520and%2520vision%2520transformers.%2520Despite%2520the%250Aabundance%2520of%2520literature%2520on%2520this%2520subject%252C%2520we%2520observe%2520that%2520critical%2520decisions%250Aregarding%2520the%2520design%2520of%2520VLMs%2520are%2520often%2520not%2520justified.%2520We%2520argue%2520that%2520these%250Aunsupported%2520decisions%2520impede%2520progress%2520in%2520the%2520field%2520by%2520making%2520it%2520difficult%2520to%250Aidentify%2520which%2520choices%2520improve%2520model%2520performance.%2520To%2520address%2520this%2520issue%252C%2520we%250Aconduct%2520extensive%2520experiments%2520around%2520pre-trained%2520models%252C%2520architecture%2520choice%252C%250Adata%252C%2520and%2520training%2520methods.%2520Our%2520consolidation%2520of%2520findings%2520includes%2520the%250Adevelopment%2520of%2520Idefics2%252C%2520an%2520efficient%2520foundational%2520VLM%2520of%25208%2520billion%2520parameters.%250AIdefics2%2520achieves%2520state-of-the-art%2520performance%2520within%2520its%2520size%2520category%2520across%250Avarious%2520multimodal%2520benchmarks%252C%2520and%2520is%2520often%2520on%2520par%2520with%2520models%2520four%2520times%2520its%250Asize.%2520We%2520release%2520the%2520model%2520%2528base%252C%2520instructed%252C%2520and%2520chat%2529%2520along%2520with%2520the%2520datasets%250Acreated%2520for%2520its%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20matters%20when%20building%20vision-language%20models%3F&entry.906535625=Hugo%20Lauren%C3%A7on%20and%20L%C3%A9o%20Tronchon%20and%20Matthieu%20Cord%20and%20Victor%20Sanh&entry.1292438233=%20%20The%20growing%20interest%20in%20vision-language%20models%20%28VLMs%29%20has%20been%20driven%20by%0Aimprovements%20in%20large%20language%20models%20and%20vision%20transformers.%20Despite%20the%0Aabundance%20of%20literature%20on%20this%20subject%2C%20we%20observe%20that%20critical%20decisions%0Aregarding%20the%20design%20of%20VLMs%20are%20often%20not%20justified.%20We%20argue%20that%20these%0Aunsupported%20decisions%20impede%20progress%20in%20the%20field%20by%20making%20it%20difficult%20to%0Aidentify%20which%20choices%20improve%20model%20performance.%20To%20address%20this%20issue%2C%20we%0Aconduct%20extensive%20experiments%20around%20pre-trained%20models%2C%20architecture%20choice%2C%0Adata%2C%20and%20training%20methods.%20Our%20consolidation%20of%20findings%20includes%20the%0Adevelopment%20of%20Idefics2%2C%20an%20efficient%20foundational%20VLM%20of%208%20billion%20parameters.%0AIdefics2%20achieves%20state-of-the-art%20performance%20within%20its%20size%20category%20across%0Avarious%20multimodal%20benchmarks%2C%20and%20is%20often%20on%20par%20with%20models%20four%20times%20its%0Asize.%20We%20release%20the%20model%20%28base%2C%20instructed%2C%20and%20chat%29%20along%20with%20the%20datasets%0Acreated%20for%20its%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02246v1&entry.124074799=Read"},
{"title": "Designed Dithering Sign Activation for Binary Neural Networks", "author": "Brayan Monroy and Juan Estupi\u00f1an and Tatiana Gelvez-Barrera and Jorge Bacca and Henry Arguello", "abstract": "  Binary Neural Networks emerged as a cost-effective and energy-efficient\nsolution for computer vision tasks by binarizing either network weights or\nactivations. However, common binary activations, such as the Sign activation\nfunction, abruptly binarize the values with a single threshold, losing\nfine-grained details in the feature outputs. This work proposes an activation\nthat applies multiple thresholds following dithering principles, shifting the\nSign activation function for each pixel according to a spatially periodic\nthreshold kernel. Unlike literature methods, the shifting is defined jointly\nfor a set of adjacent pixels, taking advantage of spatial correlations.\nExperiments over the classification task demonstrate the effectiveness of the\ndesigned dithering Sign activation function as an alternative activation for\nbinary neural networks, without increasing the computational cost. Further,\nDeSign balances the preservation of details with the efficiency of binary\noperations.\n", "link": "http://arxiv.org/abs/2405.02220v1", "date": "2024-05-03", "relevancy": 2.0428, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5154}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5095}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designed%20Dithering%20Sign%20Activation%20for%20Binary%20Neural%20Networks&body=Title%3A%20Designed%20Dithering%20Sign%20Activation%20for%20Binary%20Neural%20Networks%0AAuthor%3A%20Brayan%20Monroy%20and%20Juan%20Estupi%C3%B1an%20and%20Tatiana%20Gelvez-Barrera%20and%20Jorge%20Bacca%20and%20Henry%20Arguello%0AAbstract%3A%20%20%20Binary%20Neural%20Networks%20emerged%20as%20a%20cost-effective%20and%20energy-efficient%0Asolution%20for%20computer%20vision%20tasks%20by%20binarizing%20either%20network%20weights%20or%0Aactivations.%20However%2C%20common%20binary%20activations%2C%20such%20as%20the%20Sign%20activation%0Afunction%2C%20abruptly%20binarize%20the%20values%20with%20a%20single%20threshold%2C%20losing%0Afine-grained%20details%20in%20the%20feature%20outputs.%20This%20work%20proposes%20an%20activation%0Athat%20applies%20multiple%20thresholds%20following%20dithering%20principles%2C%20shifting%20the%0ASign%20activation%20function%20for%20each%20pixel%20according%20to%20a%20spatially%20periodic%0Athreshold%20kernel.%20Unlike%20literature%20methods%2C%20the%20shifting%20is%20defined%20jointly%0Afor%20a%20set%20of%20adjacent%20pixels%2C%20taking%20advantage%20of%20spatial%20correlations.%0AExperiments%20over%20the%20classification%20task%20demonstrate%20the%20effectiveness%20of%20the%0Adesigned%20dithering%20Sign%20activation%20function%20as%20an%20alternative%20activation%20for%0Abinary%20neural%20networks%2C%20without%20increasing%20the%20computational%20cost.%20Further%2C%0ADeSign%20balances%20the%20preservation%20of%20details%20with%20the%20efficiency%20of%20binary%0Aoperations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigned%2520Dithering%2520Sign%2520Activation%2520for%2520Binary%2520Neural%2520Networks%26entry.906535625%3DBrayan%2520Monroy%2520and%2520Juan%2520Estupi%25C3%25B1an%2520and%2520Tatiana%2520Gelvez-Barrera%2520and%2520Jorge%2520Bacca%2520and%2520Henry%2520Arguello%26entry.1292438233%3D%2520%2520Binary%2520Neural%2520Networks%2520emerged%2520as%2520a%2520cost-effective%2520and%2520energy-efficient%250Asolution%2520for%2520computer%2520vision%2520tasks%2520by%2520binarizing%2520either%2520network%2520weights%2520or%250Aactivations.%2520However%252C%2520common%2520binary%2520activations%252C%2520such%2520as%2520the%2520Sign%2520activation%250Afunction%252C%2520abruptly%2520binarize%2520the%2520values%2520with%2520a%2520single%2520threshold%252C%2520losing%250Afine-grained%2520details%2520in%2520the%2520feature%2520outputs.%2520This%2520work%2520proposes%2520an%2520activation%250Athat%2520applies%2520multiple%2520thresholds%2520following%2520dithering%2520principles%252C%2520shifting%2520the%250ASign%2520activation%2520function%2520for%2520each%2520pixel%2520according%2520to%2520a%2520spatially%2520periodic%250Athreshold%2520kernel.%2520Unlike%2520literature%2520methods%252C%2520the%2520shifting%2520is%2520defined%2520jointly%250Afor%2520a%2520set%2520of%2520adjacent%2520pixels%252C%2520taking%2520advantage%2520of%2520spatial%2520correlations.%250AExperiments%2520over%2520the%2520classification%2520task%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Adesigned%2520dithering%2520Sign%2520activation%2520function%2520as%2520an%2520alternative%2520activation%2520for%250Abinary%2520neural%2520networks%252C%2520without%2520increasing%2520the%2520computational%2520cost.%2520Further%252C%250ADeSign%2520balances%2520the%2520preservation%2520of%2520details%2520with%2520the%2520efficiency%2520of%2520binary%250Aoperations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designed%20Dithering%20Sign%20Activation%20for%20Binary%20Neural%20Networks&entry.906535625=Brayan%20Monroy%20and%20Juan%20Estupi%C3%B1an%20and%20Tatiana%20Gelvez-Barrera%20and%20Jorge%20Bacca%20and%20Henry%20Arguello&entry.1292438233=%20%20Binary%20Neural%20Networks%20emerged%20as%20a%20cost-effective%20and%20energy-efficient%0Asolution%20for%20computer%20vision%20tasks%20by%20binarizing%20either%20network%20weights%20or%0Aactivations.%20However%2C%20common%20binary%20activations%2C%20such%20as%20the%20Sign%20activation%0Afunction%2C%20abruptly%20binarize%20the%20values%20with%20a%20single%20threshold%2C%20losing%0Afine-grained%20details%20in%20the%20feature%20outputs.%20This%20work%20proposes%20an%20activation%0Athat%20applies%20multiple%20thresholds%20following%20dithering%20principles%2C%20shifting%20the%0ASign%20activation%20function%20for%20each%20pixel%20according%20to%20a%20spatially%20periodic%0Athreshold%20kernel.%20Unlike%20literature%20methods%2C%20the%20shifting%20is%20defined%20jointly%0Afor%20a%20set%20of%20adjacent%20pixels%2C%20taking%20advantage%20of%20spatial%20correlations.%0AExperiments%20over%20the%20classification%20task%20demonstrate%20the%20effectiveness%20of%20the%0Adesigned%20dithering%20Sign%20activation%20function%20as%20an%20alternative%20activation%20for%0Abinary%20neural%20networks%2C%20without%20increasing%20the%20computational%20cost.%20Further%2C%0ADeSign%20balances%20the%20preservation%20of%20details%20with%20the%20efficiency%20of%20binary%0Aoperations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02220v1&entry.124074799=Read"},
{"title": "Cooperation and Federation in Distributed Radar Point Cloud Processing", "author": "S. Savazzi and V. Rampa and S. Kianoush and A. Minora and L. Costa", "abstract": "  The paper considers the problem of human-scale RF sensing utilizing a network\nof resource-constrained MIMO radars with low range-azimuth resolution. The\nradars operate in the mmWave band and obtain time-varying 3D point cloud (PC)\ninformation that is sensitive to body movements. They also observe the same\nscene from different views and cooperate while sensing the environment using a\nsidelink communication channel. Conventional cooperation setups allow the\nradars to mutually exchange raw PC information to improve ego sensing. The\npaper proposes a federation mechanism where the radars exchange the parameters\nof a Bayesian posterior measure of the observed PCs, rather than raw data. The\nradars act as distributed parameter servers to reconstruct a global posterior\n(i.e., federated posterior) using Bayesian tools. The paper quantifies and\ncompares the benefits of radar federation with respect to cooperation\nmechanisms. Both approaches are validated by experiments with a real-time\ndemonstration platform. Federation makes minimal use of the sidelink\ncommunication channel (20 {\\div} 25 times lower bandwidth use) and is less\nsensitive to unresolved targets. On the other hand, cooperation reduces the\nmean absolute target estimation error of about 20%.\n", "link": "http://arxiv.org/abs/2405.01995v1", "date": "2024-05-03", "relevancy": 2.0393, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5227}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperation%20and%20Federation%20in%20Distributed%20Radar%20Point%20Cloud%20Processing&body=Title%3A%20Cooperation%20and%20Federation%20in%20Distributed%20Radar%20Point%20Cloud%20Processing%0AAuthor%3A%20S.%20Savazzi%20and%20V.%20Rampa%20and%20S.%20Kianoush%20and%20A.%20Minora%20and%20L.%20Costa%0AAbstract%3A%20%20%20The%20paper%20considers%20the%20problem%20of%20human-scale%20RF%20sensing%20utilizing%20a%20network%0Aof%20resource-constrained%20MIMO%20radars%20with%20low%20range-azimuth%20resolution.%20The%0Aradars%20operate%20in%20the%20mmWave%20band%20and%20obtain%20time-varying%203D%20point%20cloud%20%28PC%29%0Ainformation%20that%20is%20sensitive%20to%20body%20movements.%20They%20also%20observe%20the%20same%0Ascene%20from%20different%20views%20and%20cooperate%20while%20sensing%20the%20environment%20using%20a%0Asidelink%20communication%20channel.%20Conventional%20cooperation%20setups%20allow%20the%0Aradars%20to%20mutually%20exchange%20raw%20PC%20information%20to%20improve%20ego%20sensing.%20The%0Apaper%20proposes%20a%20federation%20mechanism%20where%20the%20radars%20exchange%20the%20parameters%0Aof%20a%20Bayesian%20posterior%20measure%20of%20the%20observed%20PCs%2C%20rather%20than%20raw%20data.%20The%0Aradars%20act%20as%20distributed%20parameter%20servers%20to%20reconstruct%20a%20global%20posterior%0A%28i.e.%2C%20federated%20posterior%29%20using%20Bayesian%20tools.%20The%20paper%20quantifies%20and%0Acompares%20the%20benefits%20of%20radar%20federation%20with%20respect%20to%20cooperation%0Amechanisms.%20Both%20approaches%20are%20validated%20by%20experiments%20with%20a%20real-time%0Ademonstration%20platform.%20Federation%20makes%20minimal%20use%20of%20the%20sidelink%0Acommunication%20channel%20%2820%20%7B%5Cdiv%7D%2025%20times%20lower%20bandwidth%20use%29%20and%20is%20less%0Asensitive%20to%20unresolved%20targets.%20On%20the%20other%20hand%2C%20cooperation%20reduces%20the%0Amean%20absolute%20target%20estimation%20error%20of%20about%2020%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperation%2520and%2520Federation%2520in%2520Distributed%2520Radar%2520Point%2520Cloud%2520Processing%26entry.906535625%3DS.%2520Savazzi%2520and%2520V.%2520Rampa%2520and%2520S.%2520Kianoush%2520and%2520A.%2520Minora%2520and%2520L.%2520Costa%26entry.1292438233%3D%2520%2520The%2520paper%2520considers%2520the%2520problem%2520of%2520human-scale%2520RF%2520sensing%2520utilizing%2520a%2520network%250Aof%2520resource-constrained%2520MIMO%2520radars%2520with%2520low%2520range-azimuth%2520resolution.%2520The%250Aradars%2520operate%2520in%2520the%2520mmWave%2520band%2520and%2520obtain%2520time-varying%25203D%2520point%2520cloud%2520%2528PC%2529%250Ainformation%2520that%2520is%2520sensitive%2520to%2520body%2520movements.%2520They%2520also%2520observe%2520the%2520same%250Ascene%2520from%2520different%2520views%2520and%2520cooperate%2520while%2520sensing%2520the%2520environment%2520using%2520a%250Asidelink%2520communication%2520channel.%2520Conventional%2520cooperation%2520setups%2520allow%2520the%250Aradars%2520to%2520mutually%2520exchange%2520raw%2520PC%2520information%2520to%2520improve%2520ego%2520sensing.%2520The%250Apaper%2520proposes%2520a%2520federation%2520mechanism%2520where%2520the%2520radars%2520exchange%2520the%2520parameters%250Aof%2520a%2520Bayesian%2520posterior%2520measure%2520of%2520the%2520observed%2520PCs%252C%2520rather%2520than%2520raw%2520data.%2520The%250Aradars%2520act%2520as%2520distributed%2520parameter%2520servers%2520to%2520reconstruct%2520a%2520global%2520posterior%250A%2528i.e.%252C%2520federated%2520posterior%2529%2520using%2520Bayesian%2520tools.%2520The%2520paper%2520quantifies%2520and%250Acompares%2520the%2520benefits%2520of%2520radar%2520federation%2520with%2520respect%2520to%2520cooperation%250Amechanisms.%2520Both%2520approaches%2520are%2520validated%2520by%2520experiments%2520with%2520a%2520real-time%250Ademonstration%2520platform.%2520Federation%2520makes%2520minimal%2520use%2520of%2520the%2520sidelink%250Acommunication%2520channel%2520%252820%2520%257B%255Cdiv%257D%252025%2520times%2520lower%2520bandwidth%2520use%2529%2520and%2520is%2520less%250Asensitive%2520to%2520unresolved%2520targets.%2520On%2520the%2520other%2520hand%252C%2520cooperation%2520reduces%2520the%250Amean%2520absolute%2520target%2520estimation%2520error%2520of%2520about%252020%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperation%20and%20Federation%20in%20Distributed%20Radar%20Point%20Cloud%20Processing&entry.906535625=S.%20Savazzi%20and%20V.%20Rampa%20and%20S.%20Kianoush%20and%20A.%20Minora%20and%20L.%20Costa&entry.1292438233=%20%20The%20paper%20considers%20the%20problem%20of%20human-scale%20RF%20sensing%20utilizing%20a%20network%0Aof%20resource-constrained%20MIMO%20radars%20with%20low%20range-azimuth%20resolution.%20The%0Aradars%20operate%20in%20the%20mmWave%20band%20and%20obtain%20time-varying%203D%20point%20cloud%20%28PC%29%0Ainformation%20that%20is%20sensitive%20to%20body%20movements.%20They%20also%20observe%20the%20same%0Ascene%20from%20different%20views%20and%20cooperate%20while%20sensing%20the%20environment%20using%20a%0Asidelink%20communication%20channel.%20Conventional%20cooperation%20setups%20allow%20the%0Aradars%20to%20mutually%20exchange%20raw%20PC%20information%20to%20improve%20ego%20sensing.%20The%0Apaper%20proposes%20a%20federation%20mechanism%20where%20the%20radars%20exchange%20the%20parameters%0Aof%20a%20Bayesian%20posterior%20measure%20of%20the%20observed%20PCs%2C%20rather%20than%20raw%20data.%20The%0Aradars%20act%20as%20distributed%20parameter%20servers%20to%20reconstruct%20a%20global%20posterior%0A%28i.e.%2C%20federated%20posterior%29%20using%20Bayesian%20tools.%20The%20paper%20quantifies%20and%0Acompares%20the%20benefits%20of%20radar%20federation%20with%20respect%20to%20cooperation%0Amechanisms.%20Both%20approaches%20are%20validated%20by%20experiments%20with%20a%20real-time%0Ademonstration%20platform.%20Federation%20makes%20minimal%20use%20of%20the%20sidelink%0Acommunication%20channel%20%2820%20%7B%5Cdiv%7D%2025%20times%20lower%20bandwidth%20use%29%20and%20is%20less%0Asensitive%20to%20unresolved%20targets.%20On%20the%20other%20hand%2C%20cooperation%20reduces%20the%0Amean%20absolute%20target%20estimation%20error%20of%20about%2020%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01995v1&entry.124074799=Read"},
{"title": "SelfVC: Voice Conversion With Iterative Refinement using Self\n  Transformations", "author": "Paarth Neekhara and Shehzeen Hussain and Rafael Valle and Boris Ginsburg and Rishabh Ranjan and Shlomo Dubnov and Farinaz Koushanfar and Julian McAuley", "abstract": "  We propose SelfVC, a training strategy to iteratively improve a voice\nconversion model with self-synthesized examples. Previous efforts on voice\nconversion focus on factorizing speech into explicitly disentangled\nrepresentations that separately encode speaker characteristics and linguistic\ncontent. However, disentangling speech representations to capture such\nattributes using task-specific loss terms can lead to information loss. In this\nwork, instead of explicitly disentangling attributes with loss terms, we\npresent a framework to train a controllable voice conversion model on entangled\nspeech representations derived from self-supervised learning (SSL) and speaker\nverification models. First, we develop techniques to derive prosodic\ninformation from the audio signal and SSL representations to train predictive\nsubmodules in the synthesis model. Next, we propose a training strategy to\niteratively improve the synthesis model for voice conversion, by creating a\nchallenging training objective using self-synthesized examples. We demonstrate\nthat incorporating such self-synthesized examples during training improves the\nspeaker similarity of generated speech as compared to a baseline voice\nconversion model trained solely on heuristically perturbed inputs. Our\nframework is trained without any text and achieves state-of-the-art results in\nzero-shot voice conversion on metrics evaluating naturalness, speaker\nsimilarity, and intelligibility of synthesized audio.\n", "link": "http://arxiv.org/abs/2310.09653v2", "date": "2024-05-03", "relevancy": 2.0389, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5119}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5117}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfVC%3A%20Voice%20Conversion%20With%20Iterative%20Refinement%20using%20Self%0A%20%20Transformations&body=Title%3A%20SelfVC%3A%20Voice%20Conversion%20With%20Iterative%20Refinement%20using%20Self%0A%20%20Transformations%0AAuthor%3A%20Paarth%20Neekhara%20and%20Shehzeen%20Hussain%20and%20Rafael%20Valle%20and%20Boris%20Ginsburg%20and%20Rishabh%20Ranjan%20and%20Shlomo%20Dubnov%20and%20Farinaz%20Koushanfar%20and%20Julian%20McAuley%0AAbstract%3A%20%20%20We%20propose%20SelfVC%2C%20a%20training%20strategy%20to%20iteratively%20improve%20a%20voice%0Aconversion%20model%20with%20self-synthesized%20examples.%20Previous%20efforts%20on%20voice%0Aconversion%20focus%20on%20factorizing%20speech%20into%20explicitly%20disentangled%0Arepresentations%20that%20separately%20encode%20speaker%20characteristics%20and%20linguistic%0Acontent.%20However%2C%20disentangling%20speech%20representations%20to%20capture%20such%0Aattributes%20using%20task-specific%20loss%20terms%20can%20lead%20to%20information%20loss.%20In%20this%0Awork%2C%20instead%20of%20explicitly%20disentangling%20attributes%20with%20loss%20terms%2C%20we%0Apresent%20a%20framework%20to%20train%20a%20controllable%20voice%20conversion%20model%20on%20entangled%0Aspeech%20representations%20derived%20from%20self-supervised%20learning%20%28SSL%29%20and%20speaker%0Averification%20models.%20First%2C%20we%20develop%20techniques%20to%20derive%20prosodic%0Ainformation%20from%20the%20audio%20signal%20and%20SSL%20representations%20to%20train%20predictive%0Asubmodules%20in%20the%20synthesis%20model.%20Next%2C%20we%20propose%20a%20training%20strategy%20to%0Aiteratively%20improve%20the%20synthesis%20model%20for%20voice%20conversion%2C%20by%20creating%20a%0Achallenging%20training%20objective%20using%20self-synthesized%20examples.%20We%20demonstrate%0Athat%20incorporating%20such%20self-synthesized%20examples%20during%20training%20improves%20the%0Aspeaker%20similarity%20of%20generated%20speech%20as%20compared%20to%20a%20baseline%20voice%0Aconversion%20model%20trained%20solely%20on%20heuristically%20perturbed%20inputs.%20Our%0Aframework%20is%20trained%20without%20any%20text%20and%20achieves%20state-of-the-art%20results%20in%0Azero-shot%20voice%20conversion%20on%20metrics%20evaluating%20naturalness%2C%20speaker%0Asimilarity%2C%20and%20intelligibility%20of%20synthesized%20audio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfVC%253A%2520Voice%2520Conversion%2520With%2520Iterative%2520Refinement%2520using%2520Self%250A%2520%2520Transformations%26entry.906535625%3DPaarth%2520Neekhara%2520and%2520Shehzeen%2520Hussain%2520and%2520Rafael%2520Valle%2520and%2520Boris%2520Ginsburg%2520and%2520Rishabh%2520Ranjan%2520and%2520Shlomo%2520Dubnov%2520and%2520Farinaz%2520Koushanfar%2520and%2520Julian%2520McAuley%26entry.1292438233%3D%2520%2520We%2520propose%2520SelfVC%252C%2520a%2520training%2520strategy%2520to%2520iteratively%2520improve%2520a%2520voice%250Aconversion%2520model%2520with%2520self-synthesized%2520examples.%2520Previous%2520efforts%2520on%2520voice%250Aconversion%2520focus%2520on%2520factorizing%2520speech%2520into%2520explicitly%2520disentangled%250Arepresentations%2520that%2520separately%2520encode%2520speaker%2520characteristics%2520and%2520linguistic%250Acontent.%2520However%252C%2520disentangling%2520speech%2520representations%2520to%2520capture%2520such%250Aattributes%2520using%2520task-specific%2520loss%2520terms%2520can%2520lead%2520to%2520information%2520loss.%2520In%2520this%250Awork%252C%2520instead%2520of%2520explicitly%2520disentangling%2520attributes%2520with%2520loss%2520terms%252C%2520we%250Apresent%2520a%2520framework%2520to%2520train%2520a%2520controllable%2520voice%2520conversion%2520model%2520on%2520entangled%250Aspeech%2520representations%2520derived%2520from%2520self-supervised%2520learning%2520%2528SSL%2529%2520and%2520speaker%250Averification%2520models.%2520First%252C%2520we%2520develop%2520techniques%2520to%2520derive%2520prosodic%250Ainformation%2520from%2520the%2520audio%2520signal%2520and%2520SSL%2520representations%2520to%2520train%2520predictive%250Asubmodules%2520in%2520the%2520synthesis%2520model.%2520Next%252C%2520we%2520propose%2520a%2520training%2520strategy%2520to%250Aiteratively%2520improve%2520the%2520synthesis%2520model%2520for%2520voice%2520conversion%252C%2520by%2520creating%2520a%250Achallenging%2520training%2520objective%2520using%2520self-synthesized%2520examples.%2520We%2520demonstrate%250Athat%2520incorporating%2520such%2520self-synthesized%2520examples%2520during%2520training%2520improves%2520the%250Aspeaker%2520similarity%2520of%2520generated%2520speech%2520as%2520compared%2520to%2520a%2520baseline%2520voice%250Aconversion%2520model%2520trained%2520solely%2520on%2520heuristically%2520perturbed%2520inputs.%2520Our%250Aframework%2520is%2520trained%2520without%2520any%2520text%2520and%2520achieves%2520state-of-the-art%2520results%2520in%250Azero-shot%2520voice%2520conversion%2520on%2520metrics%2520evaluating%2520naturalness%252C%2520speaker%250Asimilarity%252C%2520and%2520intelligibility%2520of%2520synthesized%2520audio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfVC%3A%20Voice%20Conversion%20With%20Iterative%20Refinement%20using%20Self%0A%20%20Transformations&entry.906535625=Paarth%20Neekhara%20and%20Shehzeen%20Hussain%20and%20Rafael%20Valle%20and%20Boris%20Ginsburg%20and%20Rishabh%20Ranjan%20and%20Shlomo%20Dubnov%20and%20Farinaz%20Koushanfar%20and%20Julian%20McAuley&entry.1292438233=%20%20We%20propose%20SelfVC%2C%20a%20training%20strategy%20to%20iteratively%20improve%20a%20voice%0Aconversion%20model%20with%20self-synthesized%20examples.%20Previous%20efforts%20on%20voice%0Aconversion%20focus%20on%20factorizing%20speech%20into%20explicitly%20disentangled%0Arepresentations%20that%20separately%20encode%20speaker%20characteristics%20and%20linguistic%0Acontent.%20However%2C%20disentangling%20speech%20representations%20to%20capture%20such%0Aattributes%20using%20task-specific%20loss%20terms%20can%20lead%20to%20information%20loss.%20In%20this%0Awork%2C%20instead%20of%20explicitly%20disentangling%20attributes%20with%20loss%20terms%2C%20we%0Apresent%20a%20framework%20to%20train%20a%20controllable%20voice%20conversion%20model%20on%20entangled%0Aspeech%20representations%20derived%20from%20self-supervised%20learning%20%28SSL%29%20and%20speaker%0Averification%20models.%20First%2C%20we%20develop%20techniques%20to%20derive%20prosodic%0Ainformation%20from%20the%20audio%20signal%20and%20SSL%20representations%20to%20train%20predictive%0Asubmodules%20in%20the%20synthesis%20model.%20Next%2C%20we%20propose%20a%20training%20strategy%20to%0Aiteratively%20improve%20the%20synthesis%20model%20for%20voice%20conversion%2C%20by%20creating%20a%0Achallenging%20training%20objective%20using%20self-synthesized%20examples.%20We%20demonstrate%0Athat%20incorporating%20such%20self-synthesized%20examples%20during%20training%20improves%20the%0Aspeaker%20similarity%20of%20generated%20speech%20as%20compared%20to%20a%20baseline%20voice%0Aconversion%20model%20trained%20solely%20on%20heuristically%20perturbed%20inputs.%20Our%0Aframework%20is%20trained%20without%20any%20text%20and%20achieves%20state-of-the-art%20results%20in%0Azero-shot%20voice%20conversion%20on%20metrics%20evaluating%20naturalness%2C%20speaker%0Asimilarity%2C%20and%20intelligibility%20of%20synthesized%20audio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09653v2&entry.124074799=Read"},
{"title": "Autonomous Active Mapping in Steep Alpine Environments with Fixed-wing\n  Aerial Vehicles", "author": "Jaeyoung Lim and Florian Achermann and Nicholas Lawrance and Roland Siegwart", "abstract": "  Monitoring large scale environments is a crucial task for managing remote\nalpine environments, especially for hazardous events such as avalanches. One\nkey information for avalanche risk forecast is imagery of released avalanches.\nAs these happen in remote and potentially dangerous locations this data is\ndifficult to obtain. Fixed-wing vehicles, due to their long range and travel\nspeeds are a promising platform to gather aerial imagery to map avalanche\nactivities. However, operating such vehicles in mountainous terrain remains a\nchallenge due to the complex topography, regulations, and uncertain\nenvironment. In this work, we present a system that is capable of safely\nnavigating and mapping an avalanche using a fixed-wing aerial system and\ndiscuss the challenges arising when executing such a mission. We show in our\nfield experiments that we can effectively navigate in steep terrain\nenvironments while maximizing the map quality. We expect our work to enable\nmore autonomous operations of fixed-wing vehicles in alpine environments to\nmaximize the quality of the data gathered.\n", "link": "http://arxiv.org/abs/2405.02011v1", "date": "2024-05-03", "relevancy": 2.0362, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4901}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Active%20Mapping%20in%20Steep%20Alpine%20Environments%20with%20Fixed-wing%0A%20%20Aerial%20Vehicles&body=Title%3A%20Autonomous%20Active%20Mapping%20in%20Steep%20Alpine%20Environments%20with%20Fixed-wing%0A%20%20Aerial%20Vehicles%0AAuthor%3A%20Jaeyoung%20Lim%20and%20Florian%20Achermann%20and%20Nicholas%20Lawrance%20and%20Roland%20Siegwart%0AAbstract%3A%20%20%20Monitoring%20large%20scale%20environments%20is%20a%20crucial%20task%20for%20managing%20remote%0Aalpine%20environments%2C%20especially%20for%20hazardous%20events%20such%20as%20avalanches.%20One%0Akey%20information%20for%20avalanche%20risk%20forecast%20is%20imagery%20of%20released%20avalanches.%0AAs%20these%20happen%20in%20remote%20and%20potentially%20dangerous%20locations%20this%20data%20is%0Adifficult%20to%20obtain.%20Fixed-wing%20vehicles%2C%20due%20to%20their%20long%20range%20and%20travel%0Aspeeds%20are%20a%20promising%20platform%20to%20gather%20aerial%20imagery%20to%20map%20avalanche%0Aactivities.%20However%2C%20operating%20such%20vehicles%20in%20mountainous%20terrain%20remains%20a%0Achallenge%20due%20to%20the%20complex%20topography%2C%20regulations%2C%20and%20uncertain%0Aenvironment.%20In%20this%20work%2C%20we%20present%20a%20system%20that%20is%20capable%20of%20safely%0Anavigating%20and%20mapping%20an%20avalanche%20using%20a%20fixed-wing%20aerial%20system%20and%0Adiscuss%20the%20challenges%20arising%20when%20executing%20such%20a%20mission.%20We%20show%20in%20our%0Afield%20experiments%20that%20we%20can%20effectively%20navigate%20in%20steep%20terrain%0Aenvironments%20while%20maximizing%20the%20map%20quality.%20We%20expect%20our%20work%20to%20enable%0Amore%20autonomous%20operations%20of%20fixed-wing%20vehicles%20in%20alpine%20environments%20to%0Amaximize%20the%20quality%20of%20the%20data%20gathered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Active%2520Mapping%2520in%2520Steep%2520Alpine%2520Environments%2520with%2520Fixed-wing%250A%2520%2520Aerial%2520Vehicles%26entry.906535625%3DJaeyoung%2520Lim%2520and%2520Florian%2520Achermann%2520and%2520Nicholas%2520Lawrance%2520and%2520Roland%2520Siegwart%26entry.1292438233%3D%2520%2520Monitoring%2520large%2520scale%2520environments%2520is%2520a%2520crucial%2520task%2520for%2520managing%2520remote%250Aalpine%2520environments%252C%2520especially%2520for%2520hazardous%2520events%2520such%2520as%2520avalanches.%2520One%250Akey%2520information%2520for%2520avalanche%2520risk%2520forecast%2520is%2520imagery%2520of%2520released%2520avalanches.%250AAs%2520these%2520happen%2520in%2520remote%2520and%2520potentially%2520dangerous%2520locations%2520this%2520data%2520is%250Adifficult%2520to%2520obtain.%2520Fixed-wing%2520vehicles%252C%2520due%2520to%2520their%2520long%2520range%2520and%2520travel%250Aspeeds%2520are%2520a%2520promising%2520platform%2520to%2520gather%2520aerial%2520imagery%2520to%2520map%2520avalanche%250Aactivities.%2520However%252C%2520operating%2520such%2520vehicles%2520in%2520mountainous%2520terrain%2520remains%2520a%250Achallenge%2520due%2520to%2520the%2520complex%2520topography%252C%2520regulations%252C%2520and%2520uncertain%250Aenvironment.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520system%2520that%2520is%2520capable%2520of%2520safely%250Anavigating%2520and%2520mapping%2520an%2520avalanche%2520using%2520a%2520fixed-wing%2520aerial%2520system%2520and%250Adiscuss%2520the%2520challenges%2520arising%2520when%2520executing%2520such%2520a%2520mission.%2520We%2520show%2520in%2520our%250Afield%2520experiments%2520that%2520we%2520can%2520effectively%2520navigate%2520in%2520steep%2520terrain%250Aenvironments%2520while%2520maximizing%2520the%2520map%2520quality.%2520We%2520expect%2520our%2520work%2520to%2520enable%250Amore%2520autonomous%2520operations%2520of%2520fixed-wing%2520vehicles%2520in%2520alpine%2520environments%2520to%250Amaximize%2520the%2520quality%2520of%2520the%2520data%2520gathered.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Active%20Mapping%20in%20Steep%20Alpine%20Environments%20with%20Fixed-wing%0A%20%20Aerial%20Vehicles&entry.906535625=Jaeyoung%20Lim%20and%20Florian%20Achermann%20and%20Nicholas%20Lawrance%20and%20Roland%20Siegwart&entry.1292438233=%20%20Monitoring%20large%20scale%20environments%20is%20a%20crucial%20task%20for%20managing%20remote%0Aalpine%20environments%2C%20especially%20for%20hazardous%20events%20such%20as%20avalanches.%20One%0Akey%20information%20for%20avalanche%20risk%20forecast%20is%20imagery%20of%20released%20avalanches.%0AAs%20these%20happen%20in%20remote%20and%20potentially%20dangerous%20locations%20this%20data%20is%0Adifficult%20to%20obtain.%20Fixed-wing%20vehicles%2C%20due%20to%20their%20long%20range%20and%20travel%0Aspeeds%20are%20a%20promising%20platform%20to%20gather%20aerial%20imagery%20to%20map%20avalanche%0Aactivities.%20However%2C%20operating%20such%20vehicles%20in%20mountainous%20terrain%20remains%20a%0Achallenge%20due%20to%20the%20complex%20topography%2C%20regulations%2C%20and%20uncertain%0Aenvironment.%20In%20this%20work%2C%20we%20present%20a%20system%20that%20is%20capable%20of%20safely%0Anavigating%20and%20mapping%20an%20avalanche%20using%20a%20fixed-wing%20aerial%20system%20and%0Adiscuss%20the%20challenges%20arising%20when%20executing%20such%20a%20mission.%20We%20show%20in%20our%0Afield%20experiments%20that%20we%20can%20effectively%20navigate%20in%20steep%20terrain%0Aenvironments%20while%20maximizing%20the%20map%20quality.%20We%20expect%20our%20work%20to%20enable%0Amore%20autonomous%20operations%20of%20fixed-wing%20vehicles%20in%20alpine%20environments%20to%0Amaximize%20the%20quality%20of%20the%20data%20gathered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02011v1&entry.124074799=Read"},
{"title": "TIPAA-SSL: Text Independent Phone-to-Audio Alignment based on\n  Self-Supervised Learning and Knowledge Transfer", "author": "No\u00e9 Tits and Prernna Bhatnagar and Thierry Dutoit", "abstract": "  In this paper, we present a novel approach for text independent\nphone-to-audio alignment based on phoneme recognition, representation learning\nand knowledge transfer. Our method leverages a self-supervised model (wav2vec2)\nfine-tuned for phoneme recognition using a Connectionist Temporal\nClassification (CTC) loss, a dimension reduction model and a frame-level\nphoneme classifier trained thanks to forced-alignment labels (using Montreal\nForced Aligner) to produce multi-lingual phonetic representations, thus\nrequiring minimal additional training. We evaluate our model using synthetic\nnative data from the TIMIT dataset and the SCRIBE dataset for American and\nBritish English, respectively. Our proposed model outperforms the\nstate-of-the-art (charsiu) in statistical metrics and has applications in\nlanguage learning and speech processing systems. We leave experiments on other\nlanguages for future work but the design of the system makes it easily\nadaptable to other languages.\n", "link": "http://arxiv.org/abs/2405.02124v1", "date": "2024-05-03", "relevancy": 2.025, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.539}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4878}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIPAA-SSL%3A%20Text%20Independent%20Phone-to-Audio%20Alignment%20based%20on%0A%20%20Self-Supervised%20Learning%20and%20Knowledge%20Transfer&body=Title%3A%20TIPAA-SSL%3A%20Text%20Independent%20Phone-to-Audio%20Alignment%20based%20on%0A%20%20Self-Supervised%20Learning%20and%20Knowledge%20Transfer%0AAuthor%3A%20No%C3%A9%20Tits%20and%20Prernna%20Bhatnagar%20and%20Thierry%20Dutoit%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20for%20text%20independent%0Aphone-to-audio%20alignment%20based%20on%20phoneme%20recognition%2C%20representation%20learning%0Aand%20knowledge%20transfer.%20Our%20method%20leverages%20a%20self-supervised%20model%20%28wav2vec2%29%0Afine-tuned%20for%20phoneme%20recognition%20using%20a%20Connectionist%20Temporal%0AClassification%20%28CTC%29%20loss%2C%20a%20dimension%20reduction%20model%20and%20a%20frame-level%0Aphoneme%20classifier%20trained%20thanks%20to%20forced-alignment%20labels%20%28using%20Montreal%0AForced%20Aligner%29%20to%20produce%20multi-lingual%20phonetic%20representations%2C%20thus%0Arequiring%20minimal%20additional%20training.%20We%20evaluate%20our%20model%20using%20synthetic%0Anative%20data%20from%20the%20TIMIT%20dataset%20and%20the%20SCRIBE%20dataset%20for%20American%20and%0ABritish%20English%2C%20respectively.%20Our%20proposed%20model%20outperforms%20the%0Astate-of-the-art%20%28charsiu%29%20in%20statistical%20metrics%20and%20has%20applications%20in%0Alanguage%20learning%20and%20speech%20processing%20systems.%20We%20leave%20experiments%20on%20other%0Alanguages%20for%20future%20work%20but%20the%20design%20of%20the%20system%20makes%20it%20easily%0Aadaptable%20to%20other%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIPAA-SSL%253A%2520Text%2520Independent%2520Phone-to-Audio%2520Alignment%2520based%2520on%250A%2520%2520Self-Supervised%2520Learning%2520and%2520Knowledge%2520Transfer%26entry.906535625%3DNo%25C3%25A9%2520Tits%2520and%2520Prernna%2520Bhatnagar%2520and%2520Thierry%2520Dutoit%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520for%2520text%2520independent%250Aphone-to-audio%2520alignment%2520based%2520on%2520phoneme%2520recognition%252C%2520representation%2520learning%250Aand%2520knowledge%2520transfer.%2520Our%2520method%2520leverages%2520a%2520self-supervised%2520model%2520%2528wav2vec2%2529%250Afine-tuned%2520for%2520phoneme%2520recognition%2520using%2520a%2520Connectionist%2520Temporal%250AClassification%2520%2528CTC%2529%2520loss%252C%2520a%2520dimension%2520reduction%2520model%2520and%2520a%2520frame-level%250Aphoneme%2520classifier%2520trained%2520thanks%2520to%2520forced-alignment%2520labels%2520%2528using%2520Montreal%250AForced%2520Aligner%2529%2520to%2520produce%2520multi-lingual%2520phonetic%2520representations%252C%2520thus%250Arequiring%2520minimal%2520additional%2520training.%2520We%2520evaluate%2520our%2520model%2520using%2520synthetic%250Anative%2520data%2520from%2520the%2520TIMIT%2520dataset%2520and%2520the%2520SCRIBE%2520dataset%2520for%2520American%2520and%250ABritish%2520English%252C%2520respectively.%2520Our%2520proposed%2520model%2520outperforms%2520the%250Astate-of-the-art%2520%2528charsiu%2529%2520in%2520statistical%2520metrics%2520and%2520has%2520applications%2520in%250Alanguage%2520learning%2520and%2520speech%2520processing%2520systems.%2520We%2520leave%2520experiments%2520on%2520other%250Alanguages%2520for%2520future%2520work%2520but%2520the%2520design%2520of%2520the%2520system%2520makes%2520it%2520easily%250Aadaptable%2520to%2520other%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIPAA-SSL%3A%20Text%20Independent%20Phone-to-Audio%20Alignment%20based%20on%0A%20%20Self-Supervised%20Learning%20and%20Knowledge%20Transfer&entry.906535625=No%C3%A9%20Tits%20and%20Prernna%20Bhatnagar%20and%20Thierry%20Dutoit&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20for%20text%20independent%0Aphone-to-audio%20alignment%20based%20on%20phoneme%20recognition%2C%20representation%20learning%0Aand%20knowledge%20transfer.%20Our%20method%20leverages%20a%20self-supervised%20model%20%28wav2vec2%29%0Afine-tuned%20for%20phoneme%20recognition%20using%20a%20Connectionist%20Temporal%0AClassification%20%28CTC%29%20loss%2C%20a%20dimension%20reduction%20model%20and%20a%20frame-level%0Aphoneme%20classifier%20trained%20thanks%20to%20forced-alignment%20labels%20%28using%20Montreal%0AForced%20Aligner%29%20to%20produce%20multi-lingual%20phonetic%20representations%2C%20thus%0Arequiring%20minimal%20additional%20training.%20We%20evaluate%20our%20model%20using%20synthetic%0Anative%20data%20from%20the%20TIMIT%20dataset%20and%20the%20SCRIBE%20dataset%20for%20American%20and%0ABritish%20English%2C%20respectively.%20Our%20proposed%20model%20outperforms%20the%0Astate-of-the-art%20%28charsiu%29%20in%20statistical%20metrics%20and%20has%20applications%20in%0Alanguage%20learning%20and%20speech%20processing%20systems.%20We%20leave%20experiments%20on%20other%0Alanguages%20for%20future%20work%20but%20the%20design%20of%20the%20system%20makes%20it%20easily%0Aadaptable%20to%20other%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02124v1&entry.124074799=Read"},
{"title": "A Mutual Information Perspective on Federated Contrastive Learning", "author": "Christos Louizos and Matthias Reisser and Denis Korzhenkov", "abstract": "  We investigate contrastive learning in the federated setting through the lens\nof SimCLR and multi-view mutual information maximization. In doing so, we\nuncover a connection between contrastive representation learning and user\nverification; by adding a user verification loss to each client's local SimCLR\nloss we recover a lower bound to the global multi-view mutual information. To\naccommodate for the case of when some labelled data are available at the\nclients, we extend our SimCLR variant to the federated semi-supervised setting.\nWe see that a supervised SimCLR objective can be obtained with two changes: a)\nthe contrastive loss is computed between datapoints that share the same label\nand b) we require an additional auxiliary head that predicts the correct labels\nfrom either of the two views. Along with the proposed SimCLR extensions, we\nalso study how different sources of non-i.i.d.-ness can impact the performance\nof federated unsupervised learning through global mutual information\nmaximization; we find that a global objective is beneficial for some sources of\nnon-i.i.d.-ness but can be detrimental for others. We empirically evaluate our\nproposed extensions in various tasks to validate our claims and furthermore\ndemonstrate that our proposed modifications generalize to other pretraining\nmethods.\n", "link": "http://arxiv.org/abs/2405.02081v1", "date": "2024-05-03", "relevancy": 2.017, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mutual%20Information%20Perspective%20on%20Federated%20Contrastive%20Learning&body=Title%3A%20A%20Mutual%20Information%20Perspective%20on%20Federated%20Contrastive%20Learning%0AAuthor%3A%20Christos%20Louizos%20and%20Matthias%20Reisser%20and%20Denis%20Korzhenkov%0AAbstract%3A%20%20%20We%20investigate%20contrastive%20learning%20in%20the%20federated%20setting%20through%20the%20lens%0Aof%20SimCLR%20and%20multi-view%20mutual%20information%20maximization.%20In%20doing%20so%2C%20we%0Auncover%20a%20connection%20between%20contrastive%20representation%20learning%20and%20user%0Averification%3B%20by%20adding%20a%20user%20verification%20loss%20to%20each%20client%27s%20local%20SimCLR%0Aloss%20we%20recover%20a%20lower%20bound%20to%20the%20global%20multi-view%20mutual%20information.%20To%0Aaccommodate%20for%20the%20case%20of%20when%20some%20labelled%20data%20are%20available%20at%20the%0Aclients%2C%20we%20extend%20our%20SimCLR%20variant%20to%20the%20federated%20semi-supervised%20setting.%0AWe%20see%20that%20a%20supervised%20SimCLR%20objective%20can%20be%20obtained%20with%20two%20changes%3A%20a%29%0Athe%20contrastive%20loss%20is%20computed%20between%20datapoints%20that%20share%20the%20same%20label%0Aand%20b%29%20we%20require%20an%20additional%20auxiliary%20head%20that%20predicts%20the%20correct%20labels%0Afrom%20either%20of%20the%20two%20views.%20Along%20with%20the%20proposed%20SimCLR%20extensions%2C%20we%0Aalso%20study%20how%20different%20sources%20of%20non-i.i.d.-ness%20can%20impact%20the%20performance%0Aof%20federated%20unsupervised%20learning%20through%20global%20mutual%20information%0Amaximization%3B%20we%20find%20that%20a%20global%20objective%20is%20beneficial%20for%20some%20sources%20of%0Anon-i.i.d.-ness%20but%20can%20be%20detrimental%20for%20others.%20We%20empirically%20evaluate%20our%0Aproposed%20extensions%20in%20various%20tasks%20to%20validate%20our%20claims%20and%20furthermore%0Ademonstrate%20that%20our%20proposed%20modifications%20generalize%20to%20other%20pretraining%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mutual%2520Information%2520Perspective%2520on%2520Federated%2520Contrastive%2520Learning%26entry.906535625%3DChristos%2520Louizos%2520and%2520Matthias%2520Reisser%2520and%2520Denis%2520Korzhenkov%26entry.1292438233%3D%2520%2520We%2520investigate%2520contrastive%2520learning%2520in%2520the%2520federated%2520setting%2520through%2520the%2520lens%250Aof%2520SimCLR%2520and%2520multi-view%2520mutual%2520information%2520maximization.%2520In%2520doing%2520so%252C%2520we%250Auncover%2520a%2520connection%2520between%2520contrastive%2520representation%2520learning%2520and%2520user%250Averification%253B%2520by%2520adding%2520a%2520user%2520verification%2520loss%2520to%2520each%2520client%2527s%2520local%2520SimCLR%250Aloss%2520we%2520recover%2520a%2520lower%2520bound%2520to%2520the%2520global%2520multi-view%2520mutual%2520information.%2520To%250Aaccommodate%2520for%2520the%2520case%2520of%2520when%2520some%2520labelled%2520data%2520are%2520available%2520at%2520the%250Aclients%252C%2520we%2520extend%2520our%2520SimCLR%2520variant%2520to%2520the%2520federated%2520semi-supervised%2520setting.%250AWe%2520see%2520that%2520a%2520supervised%2520SimCLR%2520objective%2520can%2520be%2520obtained%2520with%2520two%2520changes%253A%2520a%2529%250Athe%2520contrastive%2520loss%2520is%2520computed%2520between%2520datapoints%2520that%2520share%2520the%2520same%2520label%250Aand%2520b%2529%2520we%2520require%2520an%2520additional%2520auxiliary%2520head%2520that%2520predicts%2520the%2520correct%2520labels%250Afrom%2520either%2520of%2520the%2520two%2520views.%2520Along%2520with%2520the%2520proposed%2520SimCLR%2520extensions%252C%2520we%250Aalso%2520study%2520how%2520different%2520sources%2520of%2520non-i.i.d.-ness%2520can%2520impact%2520the%2520performance%250Aof%2520federated%2520unsupervised%2520learning%2520through%2520global%2520mutual%2520information%250Amaximization%253B%2520we%2520find%2520that%2520a%2520global%2520objective%2520is%2520beneficial%2520for%2520some%2520sources%2520of%250Anon-i.i.d.-ness%2520but%2520can%2520be%2520detrimental%2520for%2520others.%2520We%2520empirically%2520evaluate%2520our%250Aproposed%2520extensions%2520in%2520various%2520tasks%2520to%2520validate%2520our%2520claims%2520and%2520furthermore%250Ademonstrate%2520that%2520our%2520proposed%2520modifications%2520generalize%2520to%2520other%2520pretraining%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mutual%20Information%20Perspective%20on%20Federated%20Contrastive%20Learning&entry.906535625=Christos%20Louizos%20and%20Matthias%20Reisser%20and%20Denis%20Korzhenkov&entry.1292438233=%20%20We%20investigate%20contrastive%20learning%20in%20the%20federated%20setting%20through%20the%20lens%0Aof%20SimCLR%20and%20multi-view%20mutual%20information%20maximization.%20In%20doing%20so%2C%20we%0Auncover%20a%20connection%20between%20contrastive%20representation%20learning%20and%20user%0Averification%3B%20by%20adding%20a%20user%20verification%20loss%20to%20each%20client%27s%20local%20SimCLR%0Aloss%20we%20recover%20a%20lower%20bound%20to%20the%20global%20multi-view%20mutual%20information.%20To%0Aaccommodate%20for%20the%20case%20of%20when%20some%20labelled%20data%20are%20available%20at%20the%0Aclients%2C%20we%20extend%20our%20SimCLR%20variant%20to%20the%20federated%20semi-supervised%20setting.%0AWe%20see%20that%20a%20supervised%20SimCLR%20objective%20can%20be%20obtained%20with%20two%20changes%3A%20a%29%0Athe%20contrastive%20loss%20is%20computed%20between%20datapoints%20that%20share%20the%20same%20label%0Aand%20b%29%20we%20require%20an%20additional%20auxiliary%20head%20that%20predicts%20the%20correct%20labels%0Afrom%20either%20of%20the%20two%20views.%20Along%20with%20the%20proposed%20SimCLR%20extensions%2C%20we%0Aalso%20study%20how%20different%20sources%20of%20non-i.i.d.-ness%20can%20impact%20the%20performance%0Aof%20federated%20unsupervised%20learning%20through%20global%20mutual%20information%0Amaximization%3B%20we%20find%20that%20a%20global%20objective%20is%20beneficial%20for%20some%20sources%20of%0Anon-i.i.d.-ness%20but%20can%20be%20detrimental%20for%20others.%20We%20empirically%20evaluate%20our%0Aproposed%20extensions%20in%20various%20tasks%20to%20validate%20our%20claims%20and%20furthermore%0Ademonstrate%20that%20our%20proposed%20modifications%20generalize%20to%20other%20pretraining%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02081v1&entry.124074799=Read"},
{"title": "LangProp: A code optimization framework using Large Language Models\n  applied to driving", "author": "Shu Ishida and Gianluca Corrado and George Fedoseev and Hudson Yeo and Lloyd Russell and Jamie Shotton and Jo\u00e3o F. Henriques and Anthony Hu", "abstract": "  We propose LangProp, a framework for iteratively optimizing code generated by\nlarge language models (LLMs), in both supervised and reinforcement learning\nsettings. While LLMs can generate sensible coding solutions zero-shot, they are\noften sub-optimal. Especially for code generation tasks, it is likely that the\ninitial code will fail on certain edge cases. LangProp automatically evaluates\nthe code performance on a dataset of input-output pairs, catches any\nexceptions, and feeds the results back to the LLM in the training loop, so that\nthe LLM can iteratively improve the code it generates. By adopting a metric-\nand data-driven training paradigm for this code optimization procedure, one\ncould easily adapt findings from traditional machine learning techniques such\nas imitation learning, DAgger, and reinforcement learning. We show LangProp's\napplicability to general domains such as Sudoku and CartPole, as well as\ndemonstrate the first proof of concept of automated code optimization for\nautonomous driving in CARLA. We show that LangProp can generate interpretable\nand transparent policies that can be verified and improved in a metric- and\ndata-driven way. Our code is available at\nhttps://github.com/shuishida/LangProp.\n", "link": "http://arxiv.org/abs/2401.10314v2", "date": "2024-05-03", "relevancy": 2.0156, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.506}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangProp%3A%20A%20code%20optimization%20framework%20using%20Large%20Language%20Models%0A%20%20applied%20to%20driving&body=Title%3A%20LangProp%3A%20A%20code%20optimization%20framework%20using%20Large%20Language%20Models%0A%20%20applied%20to%20driving%0AAuthor%3A%20Shu%20Ishida%20and%20Gianluca%20Corrado%20and%20George%20Fedoseev%20and%20Hudson%20Yeo%20and%20Lloyd%20Russell%20and%20Jamie%20Shotton%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Anthony%20Hu%0AAbstract%3A%20%20%20We%20propose%20LangProp%2C%20a%20framework%20for%20iteratively%20optimizing%20code%20generated%20by%0Alarge%20language%20models%20%28LLMs%29%2C%20in%20both%20supervised%20and%20reinforcement%20learning%0Asettings.%20While%20LLMs%20can%20generate%20sensible%20coding%20solutions%20zero-shot%2C%20they%20are%0Aoften%20sub-optimal.%20Especially%20for%20code%20generation%20tasks%2C%20it%20is%20likely%20that%20the%0Ainitial%20code%20will%20fail%20on%20certain%20edge%20cases.%20LangProp%20automatically%20evaluates%0Athe%20code%20performance%20on%20a%20dataset%20of%20input-output%20pairs%2C%20catches%20any%0Aexceptions%2C%20and%20feeds%20the%20results%20back%20to%20the%20LLM%20in%20the%20training%20loop%2C%20so%20that%0Athe%20LLM%20can%20iteratively%20improve%20the%20code%20it%20generates.%20By%20adopting%20a%20metric-%0Aand%20data-driven%20training%20paradigm%20for%20this%20code%20optimization%20procedure%2C%20one%0Acould%20easily%20adapt%20findings%20from%20traditional%20machine%20learning%20techniques%20such%0Aas%20imitation%20learning%2C%20DAgger%2C%20and%20reinforcement%20learning.%20We%20show%20LangProp%27s%0Aapplicability%20to%20general%20domains%20such%20as%20Sudoku%20and%20CartPole%2C%20as%20well%20as%0Ademonstrate%20the%20first%20proof%20of%20concept%20of%20automated%20code%20optimization%20for%0Aautonomous%20driving%20in%20CARLA.%20We%20show%20that%20LangProp%20can%20generate%20interpretable%0Aand%20transparent%20policies%20that%20can%20be%20verified%20and%20improved%20in%20a%20metric-%20and%0Adata-driven%20way.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/shuishida/LangProp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangProp%253A%2520A%2520code%2520optimization%2520framework%2520using%2520Large%2520Language%2520Models%250A%2520%2520applied%2520to%2520driving%26entry.906535625%3DShu%2520Ishida%2520and%2520Gianluca%2520Corrado%2520and%2520George%2520Fedoseev%2520and%2520Hudson%2520Yeo%2520and%2520Lloyd%2520Russell%2520and%2520Jamie%2520Shotton%2520and%2520Jo%25C3%25A3o%2520F.%2520Henriques%2520and%2520Anthony%2520Hu%26entry.1292438233%3D%2520%2520We%2520propose%2520LangProp%252C%2520a%2520framework%2520for%2520iteratively%2520optimizing%2520code%2520generated%2520by%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520in%2520both%2520supervised%2520and%2520reinforcement%2520learning%250Asettings.%2520While%2520LLMs%2520can%2520generate%2520sensible%2520coding%2520solutions%2520zero-shot%252C%2520they%2520are%250Aoften%2520sub-optimal.%2520Especially%2520for%2520code%2520generation%2520tasks%252C%2520it%2520is%2520likely%2520that%2520the%250Ainitial%2520code%2520will%2520fail%2520on%2520certain%2520edge%2520cases.%2520LangProp%2520automatically%2520evaluates%250Athe%2520code%2520performance%2520on%2520a%2520dataset%2520of%2520input-output%2520pairs%252C%2520catches%2520any%250Aexceptions%252C%2520and%2520feeds%2520the%2520results%2520back%2520to%2520the%2520LLM%2520in%2520the%2520training%2520loop%252C%2520so%2520that%250Athe%2520LLM%2520can%2520iteratively%2520improve%2520the%2520code%2520it%2520generates.%2520By%2520adopting%2520a%2520metric-%250Aand%2520data-driven%2520training%2520paradigm%2520for%2520this%2520code%2520optimization%2520procedure%252C%2520one%250Acould%2520easily%2520adapt%2520findings%2520from%2520traditional%2520machine%2520learning%2520techniques%2520such%250Aas%2520imitation%2520learning%252C%2520DAgger%252C%2520and%2520reinforcement%2520learning.%2520We%2520show%2520LangProp%2527s%250Aapplicability%2520to%2520general%2520domains%2520such%2520as%2520Sudoku%2520and%2520CartPole%252C%2520as%2520well%2520as%250Ademonstrate%2520the%2520first%2520proof%2520of%2520concept%2520of%2520automated%2520code%2520optimization%2520for%250Aautonomous%2520driving%2520in%2520CARLA.%2520We%2520show%2520that%2520LangProp%2520can%2520generate%2520interpretable%250Aand%2520transparent%2520policies%2520that%2520can%2520be%2520verified%2520and%2520improved%2520in%2520a%2520metric-%2520and%250Adata-driven%2520way.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shuishida/LangProp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangProp%3A%20A%20code%20optimization%20framework%20using%20Large%20Language%20Models%0A%20%20applied%20to%20driving&entry.906535625=Shu%20Ishida%20and%20Gianluca%20Corrado%20and%20George%20Fedoseev%20and%20Hudson%20Yeo%20and%20Lloyd%20Russell%20and%20Jamie%20Shotton%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Anthony%20Hu&entry.1292438233=%20%20We%20propose%20LangProp%2C%20a%20framework%20for%20iteratively%20optimizing%20code%20generated%20by%0Alarge%20language%20models%20%28LLMs%29%2C%20in%20both%20supervised%20and%20reinforcement%20learning%0Asettings.%20While%20LLMs%20can%20generate%20sensible%20coding%20solutions%20zero-shot%2C%20they%20are%0Aoften%20sub-optimal.%20Especially%20for%20code%20generation%20tasks%2C%20it%20is%20likely%20that%20the%0Ainitial%20code%20will%20fail%20on%20certain%20edge%20cases.%20LangProp%20automatically%20evaluates%0Athe%20code%20performance%20on%20a%20dataset%20of%20input-output%20pairs%2C%20catches%20any%0Aexceptions%2C%20and%20feeds%20the%20results%20back%20to%20the%20LLM%20in%20the%20training%20loop%2C%20so%20that%0Athe%20LLM%20can%20iteratively%20improve%20the%20code%20it%20generates.%20By%20adopting%20a%20metric-%0Aand%20data-driven%20training%20paradigm%20for%20this%20code%20optimization%20procedure%2C%20one%0Acould%20easily%20adapt%20findings%20from%20traditional%20machine%20learning%20techniques%20such%0Aas%20imitation%20learning%2C%20DAgger%2C%20and%20reinforcement%20learning.%20We%20show%20LangProp%27s%0Aapplicability%20to%20general%20domains%20such%20as%20Sudoku%20and%20CartPole%2C%20as%20well%20as%0Ademonstrate%20the%20first%20proof%20of%20concept%20of%20automated%20code%20optimization%20for%0Aautonomous%20driving%20in%20CARLA.%20We%20show%20that%20LangProp%20can%20generate%20interpretable%0Aand%20transparent%20policies%20that%20can%20be%20verified%20and%20improved%20in%20a%20metric-%20and%0Adata-driven%20way.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/shuishida/LangProp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10314v2&entry.124074799=Read"},
{"title": "Semi-Automatic Infrared Calibration for Augmented Reality Systems in\n  Surgery", "author": "Hisham Iqbal and Ferdinando Rodriguez y Baena", "abstract": "  Augmented reality (AR) has the potential to improve the immersion and\nefficiency of computer-assisted orthopaedic surgery (CAOS) by allowing surgeons\nto maintain focus on the operating site rather than external displays in the\noperating theatre. Successful deployment of AR to CAOS requires a calibration\nthat can accurately calculate the spatial relationship between real and\nholographic objects. Several studies attempt this calibration through manual\nalignment or with additional fiducial markers in the surgical scene. We propose\na calibration system that offers a direct method for the calibration of AR\nhead-mounted displays (HMDs) with CAOS systems, by using infrared-reflective\nmarker-arrays widely used in CAOS. In our fast, user-agnostic setup, a HoloLens\n2 detected the pose of marker arrays using infrared response and time-of-flight\ndepth obtained through sensors onboard the HMD. Registration with a\ncommercially available CAOS system was achieved when an IR marker-array was\nvisible to both devices. Study tests found relative-tracking mean errors of\n2.03 mm and 1.12{\\deg} when calculating the relative pose between two static\nmarker-arrays at short ranges. When using the calibration result to provide\nin-situ holographic guidance for a simulated wire-insertion task, a\npre-clinical test reported mean errors of 2.07 mm and 1.54{\\deg} when compared\nto a pre-planned trajectory.\n", "link": "http://arxiv.org/abs/2405.01999v1", "date": "2024-05-03", "relevancy": 1.9993, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5035}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5022}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Automatic%20Infrared%20Calibration%20for%20Augmented%20Reality%20Systems%20in%0A%20%20Surgery&body=Title%3A%20Semi-Automatic%20Infrared%20Calibration%20for%20Augmented%20Reality%20Systems%20in%0A%20%20Surgery%0AAuthor%3A%20Hisham%20Iqbal%20and%20Ferdinando%20Rodriguez%20y%20Baena%0AAbstract%3A%20%20%20Augmented%20reality%20%28AR%29%20has%20the%20potential%20to%20improve%20the%20immersion%20and%0Aefficiency%20of%20computer-assisted%20orthopaedic%20surgery%20%28CAOS%29%20by%20allowing%20surgeons%0Ato%20maintain%20focus%20on%20the%20operating%20site%20rather%20than%20external%20displays%20in%20the%0Aoperating%20theatre.%20Successful%20deployment%20of%20AR%20to%20CAOS%20requires%20a%20calibration%0Athat%20can%20accurately%20calculate%20the%20spatial%20relationship%20between%20real%20and%0Aholographic%20objects.%20Several%20studies%20attempt%20this%20calibration%20through%20manual%0Aalignment%20or%20with%20additional%20fiducial%20markers%20in%20the%20surgical%20scene.%20We%20propose%0Aa%20calibration%20system%20that%20offers%20a%20direct%20method%20for%20the%20calibration%20of%20AR%0Ahead-mounted%20displays%20%28HMDs%29%20with%20CAOS%20systems%2C%20by%20using%20infrared-reflective%0Amarker-arrays%20widely%20used%20in%20CAOS.%20In%20our%20fast%2C%20user-agnostic%20setup%2C%20a%20HoloLens%0A2%20detected%20the%20pose%20of%20marker%20arrays%20using%20infrared%20response%20and%20time-of-flight%0Adepth%20obtained%20through%20sensors%20onboard%20the%20HMD.%20Registration%20with%20a%0Acommercially%20available%20CAOS%20system%20was%20achieved%20when%20an%20IR%20marker-array%20was%0Avisible%20to%20both%20devices.%20Study%20tests%20found%20relative-tracking%20mean%20errors%20of%0A2.03%20mm%20and%201.12%7B%5Cdeg%7D%20when%20calculating%20the%20relative%20pose%20between%20two%20static%0Amarker-arrays%20at%20short%20ranges.%20When%20using%20the%20calibration%20result%20to%20provide%0Ain-situ%20holographic%20guidance%20for%20a%20simulated%20wire-insertion%20task%2C%20a%0Apre-clinical%20test%20reported%20mean%20errors%20of%202.07%20mm%20and%201.54%7B%5Cdeg%7D%20when%20compared%0Ato%20a%20pre-planned%20trajectory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Automatic%2520Infrared%2520Calibration%2520for%2520Augmented%2520Reality%2520Systems%2520in%250A%2520%2520Surgery%26entry.906535625%3DHisham%2520Iqbal%2520and%2520Ferdinando%2520Rodriguez%2520y%2520Baena%26entry.1292438233%3D%2520%2520Augmented%2520reality%2520%2528AR%2529%2520has%2520the%2520potential%2520to%2520improve%2520the%2520immersion%2520and%250Aefficiency%2520of%2520computer-assisted%2520orthopaedic%2520surgery%2520%2528CAOS%2529%2520by%2520allowing%2520surgeons%250Ato%2520maintain%2520focus%2520on%2520the%2520operating%2520site%2520rather%2520than%2520external%2520displays%2520in%2520the%250Aoperating%2520theatre.%2520Successful%2520deployment%2520of%2520AR%2520to%2520CAOS%2520requires%2520a%2520calibration%250Athat%2520can%2520accurately%2520calculate%2520the%2520spatial%2520relationship%2520between%2520real%2520and%250Aholographic%2520objects.%2520Several%2520studies%2520attempt%2520this%2520calibration%2520through%2520manual%250Aalignment%2520or%2520with%2520additional%2520fiducial%2520markers%2520in%2520the%2520surgical%2520scene.%2520We%2520propose%250Aa%2520calibration%2520system%2520that%2520offers%2520a%2520direct%2520method%2520for%2520the%2520calibration%2520of%2520AR%250Ahead-mounted%2520displays%2520%2528HMDs%2529%2520with%2520CAOS%2520systems%252C%2520by%2520using%2520infrared-reflective%250Amarker-arrays%2520widely%2520used%2520in%2520CAOS.%2520In%2520our%2520fast%252C%2520user-agnostic%2520setup%252C%2520a%2520HoloLens%250A2%2520detected%2520the%2520pose%2520of%2520marker%2520arrays%2520using%2520infrared%2520response%2520and%2520time-of-flight%250Adepth%2520obtained%2520through%2520sensors%2520onboard%2520the%2520HMD.%2520Registration%2520with%2520a%250Acommercially%2520available%2520CAOS%2520system%2520was%2520achieved%2520when%2520an%2520IR%2520marker-array%2520was%250Avisible%2520to%2520both%2520devices.%2520Study%2520tests%2520found%2520relative-tracking%2520mean%2520errors%2520of%250A2.03%2520mm%2520and%25201.12%257B%255Cdeg%257D%2520when%2520calculating%2520the%2520relative%2520pose%2520between%2520two%2520static%250Amarker-arrays%2520at%2520short%2520ranges.%2520When%2520using%2520the%2520calibration%2520result%2520to%2520provide%250Ain-situ%2520holographic%2520guidance%2520for%2520a%2520simulated%2520wire-insertion%2520task%252C%2520a%250Apre-clinical%2520test%2520reported%2520mean%2520errors%2520of%25202.07%2520mm%2520and%25201.54%257B%255Cdeg%257D%2520when%2520compared%250Ato%2520a%2520pre-planned%2520trajectory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Automatic%20Infrared%20Calibration%20for%20Augmented%20Reality%20Systems%20in%0A%20%20Surgery&entry.906535625=Hisham%20Iqbal%20and%20Ferdinando%20Rodriguez%20y%20Baena&entry.1292438233=%20%20Augmented%20reality%20%28AR%29%20has%20the%20potential%20to%20improve%20the%20immersion%20and%0Aefficiency%20of%20computer-assisted%20orthopaedic%20surgery%20%28CAOS%29%20by%20allowing%20surgeons%0Ato%20maintain%20focus%20on%20the%20operating%20site%20rather%20than%20external%20displays%20in%20the%0Aoperating%20theatre.%20Successful%20deployment%20of%20AR%20to%20CAOS%20requires%20a%20calibration%0Athat%20can%20accurately%20calculate%20the%20spatial%20relationship%20between%20real%20and%0Aholographic%20objects.%20Several%20studies%20attempt%20this%20calibration%20through%20manual%0Aalignment%20or%20with%20additional%20fiducial%20markers%20in%20the%20surgical%20scene.%20We%20propose%0Aa%20calibration%20system%20that%20offers%20a%20direct%20method%20for%20the%20calibration%20of%20AR%0Ahead-mounted%20displays%20%28HMDs%29%20with%20CAOS%20systems%2C%20by%20using%20infrared-reflective%0Amarker-arrays%20widely%20used%20in%20CAOS.%20In%20our%20fast%2C%20user-agnostic%20setup%2C%20a%20HoloLens%0A2%20detected%20the%20pose%20of%20marker%20arrays%20using%20infrared%20response%20and%20time-of-flight%0Adepth%20obtained%20through%20sensors%20onboard%20the%20HMD.%20Registration%20with%20a%0Acommercially%20available%20CAOS%20system%20was%20achieved%20when%20an%20IR%20marker-array%20was%0Avisible%20to%20both%20devices.%20Study%20tests%20found%20relative-tracking%20mean%20errors%20of%0A2.03%20mm%20and%201.12%7B%5Cdeg%7D%20when%20calculating%20the%20relative%20pose%20between%20two%20static%0Amarker-arrays%20at%20short%20ranges.%20When%20using%20the%20calibration%20result%20to%20provide%0Ain-situ%20holographic%20guidance%20for%20a%20simulated%20wire-insertion%20task%2C%20a%0Apre-clinical%20test%20reported%20mean%20errors%20of%202.07%20mm%20and%201.54%7B%5Cdeg%7D%20when%20compared%0Ato%20a%20pre-planned%20trajectory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01999v1&entry.124074799=Read"},
{"title": "An Information Theoretic Perspective on Conformal Prediction", "author": "Alvaro H. C. Correia and Fabio Valerio Massoli and Christos Louizos and Arash Behboodi", "abstract": "  Conformal Prediction (CP) is a distribution-free uncertainty estimation\nframework that constructs prediction sets guaranteed to contain the true answer\nwith a user-specified probability. Intuitively, the size of the prediction set\nencodes a general notion of uncertainty, with larger sets associated with\nhigher degrees of uncertainty. In this work, we leverage information theory to\nconnect conformal prediction to other notions of uncertainty. More precisely,\nwe prove three different ways to upper bound the intrinsic uncertainty, as\ndescribed by the conditional entropy of the target variable given the inputs,\nby combining CP with information theoretical inequalities. Moreover, we\ndemonstrate two direct and useful applications of such connection between\nconformal prediction and information theory: (i) more principled and effective\nconformal training objectives that generalize previous approaches and enable\nend-to-end training of machine learning models from scratch, and (ii) a natural\nmechanism to incorporate side information into conformal prediction. We\nempirically validate both applications in centralized and federated learning\nsettings, showing our theoretical results translate to lower inefficiency\n(average prediction set size) for popular CP methods.\n", "link": "http://arxiv.org/abs/2405.02140v1", "date": "2024-05-03", "relevancy": 1.995, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4948}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Information%20Theoretic%20Perspective%20on%20Conformal%20Prediction&body=Title%3A%20An%20Information%20Theoretic%20Perspective%20on%20Conformal%20Prediction%0AAuthor%3A%20Alvaro%20H.%20C.%20Correia%20and%20Fabio%20Valerio%20Massoli%20and%20Christos%20Louizos%20and%20Arash%20Behboodi%0AAbstract%3A%20%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20distribution-free%20uncertainty%20estimation%0Aframework%20that%20constructs%20prediction%20sets%20guaranteed%20to%20contain%20the%20true%20answer%0Awith%20a%20user-specified%20probability.%20Intuitively%2C%20the%20size%20of%20the%20prediction%20set%0Aencodes%20a%20general%20notion%20of%20uncertainty%2C%20with%20larger%20sets%20associated%20with%0Ahigher%20degrees%20of%20uncertainty.%20In%20this%20work%2C%20we%20leverage%20information%20theory%20to%0Aconnect%20conformal%20prediction%20to%20other%20notions%20of%20uncertainty.%20More%20precisely%2C%0Awe%20prove%20three%20different%20ways%20to%20upper%20bound%20the%20intrinsic%20uncertainty%2C%20as%0Adescribed%20by%20the%20conditional%20entropy%20of%20the%20target%20variable%20given%20the%20inputs%2C%0Aby%20combining%20CP%20with%20information%20theoretical%20inequalities.%20Moreover%2C%20we%0Ademonstrate%20two%20direct%20and%20useful%20applications%20of%20such%20connection%20between%0Aconformal%20prediction%20and%20information%20theory%3A%20%28i%29%20more%20principled%20and%20effective%0Aconformal%20training%20objectives%20that%20generalize%20previous%20approaches%20and%20enable%0Aend-to-end%20training%20of%20machine%20learning%20models%20from%20scratch%2C%20and%20%28ii%29%20a%20natural%0Amechanism%20to%20incorporate%20side%20information%20into%20conformal%20prediction.%20We%0Aempirically%20validate%20both%20applications%20in%20centralized%20and%20federated%20learning%0Asettings%2C%20showing%20our%20theoretical%20results%20translate%20to%20lower%20inefficiency%0A%28average%20prediction%20set%20size%29%20for%20popular%20CP%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Information%2520Theoretic%2520Perspective%2520on%2520Conformal%2520Prediction%26entry.906535625%3DAlvaro%2520H.%2520C.%2520Correia%2520and%2520Fabio%2520Valerio%2520Massoli%2520and%2520Christos%2520Louizos%2520and%2520Arash%2520Behboodi%26entry.1292438233%3D%2520%2520Conformal%2520Prediction%2520%2528CP%2529%2520is%2520a%2520distribution-free%2520uncertainty%2520estimation%250Aframework%2520that%2520constructs%2520prediction%2520sets%2520guaranteed%2520to%2520contain%2520the%2520true%2520answer%250Awith%2520a%2520user-specified%2520probability.%2520Intuitively%252C%2520the%2520size%2520of%2520the%2520prediction%2520set%250Aencodes%2520a%2520general%2520notion%2520of%2520uncertainty%252C%2520with%2520larger%2520sets%2520associated%2520with%250Ahigher%2520degrees%2520of%2520uncertainty.%2520In%2520this%2520work%252C%2520we%2520leverage%2520information%2520theory%2520to%250Aconnect%2520conformal%2520prediction%2520to%2520other%2520notions%2520of%2520uncertainty.%2520More%2520precisely%252C%250Awe%2520prove%2520three%2520different%2520ways%2520to%2520upper%2520bound%2520the%2520intrinsic%2520uncertainty%252C%2520as%250Adescribed%2520by%2520the%2520conditional%2520entropy%2520of%2520the%2520target%2520variable%2520given%2520the%2520inputs%252C%250Aby%2520combining%2520CP%2520with%2520information%2520theoretical%2520inequalities.%2520Moreover%252C%2520we%250Ademonstrate%2520two%2520direct%2520and%2520useful%2520applications%2520of%2520such%2520connection%2520between%250Aconformal%2520prediction%2520and%2520information%2520theory%253A%2520%2528i%2529%2520more%2520principled%2520and%2520effective%250Aconformal%2520training%2520objectives%2520that%2520generalize%2520previous%2520approaches%2520and%2520enable%250Aend-to-end%2520training%2520of%2520machine%2520learning%2520models%2520from%2520scratch%252C%2520and%2520%2528ii%2529%2520a%2520natural%250Amechanism%2520to%2520incorporate%2520side%2520information%2520into%2520conformal%2520prediction.%2520We%250Aempirically%2520validate%2520both%2520applications%2520in%2520centralized%2520and%2520federated%2520learning%250Asettings%252C%2520showing%2520our%2520theoretical%2520results%2520translate%2520to%2520lower%2520inefficiency%250A%2528average%2520prediction%2520set%2520size%2529%2520for%2520popular%2520CP%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Information%20Theoretic%20Perspective%20on%20Conformal%20Prediction&entry.906535625=Alvaro%20H.%20C.%20Correia%20and%20Fabio%20Valerio%20Massoli%20and%20Christos%20Louizos%20and%20Arash%20Behboodi&entry.1292438233=%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20distribution-free%20uncertainty%20estimation%0Aframework%20that%20constructs%20prediction%20sets%20guaranteed%20to%20contain%20the%20true%20answer%0Awith%20a%20user-specified%20probability.%20Intuitively%2C%20the%20size%20of%20the%20prediction%20set%0Aencodes%20a%20general%20notion%20of%20uncertainty%2C%20with%20larger%20sets%20associated%20with%0Ahigher%20degrees%20of%20uncertainty.%20In%20this%20work%2C%20we%20leverage%20information%20theory%20to%0Aconnect%20conformal%20prediction%20to%20other%20notions%20of%20uncertainty.%20More%20precisely%2C%0Awe%20prove%20three%20different%20ways%20to%20upper%20bound%20the%20intrinsic%20uncertainty%2C%20as%0Adescribed%20by%20the%20conditional%20entropy%20of%20the%20target%20variable%20given%20the%20inputs%2C%0Aby%20combining%20CP%20with%20information%20theoretical%20inequalities.%20Moreover%2C%20we%0Ademonstrate%20two%20direct%20and%20useful%20applications%20of%20such%20connection%20between%0Aconformal%20prediction%20and%20information%20theory%3A%20%28i%29%20more%20principled%20and%20effective%0Aconformal%20training%20objectives%20that%20generalize%20previous%20approaches%20and%20enable%0Aend-to-end%20training%20of%20machine%20learning%20models%20from%20scratch%2C%20and%20%28ii%29%20a%20natural%0Amechanism%20to%20incorporate%20side%20information%20into%20conformal%20prediction.%20We%0Aempirically%20validate%20both%20applications%20in%20centralized%20and%20federated%20learning%0Asettings%2C%20showing%20our%20theoretical%20results%20translate%20to%20lower%20inefficiency%0A%28average%20prediction%20set%20size%29%20for%20popular%20CP%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02140v1&entry.124074799=Read"},
{"title": "GMP-ATL: Gender-augmented Multi-scale Pseudo-label Enhanced Adaptive\n  Transfer Learning for Speech Emotion Recognition via HuBERT", "author": "Yu Pan and Yuguang Yang and Heng Lu and Lei Ma and Jianjun Zhao", "abstract": "  The continuous evolution of pre-trained speech models has greatly advanced\nSpeech Emotion Recognition (SER). However, there is still potential for\nenhancement in the performance of these methods. In this paper, we present\nGMP-ATL (Gender-augmented Multi-scale Pseudo-label Adaptive Transfer Learning),\na novel HuBERT-based adaptive transfer learning framework for SER.\nSpecifically, GMP-ATL initially employs the pre-trained HuBERT, implementing\nmulti-task learning and multi-scale k-means clustering to acquire frame-level\ngender-augmented multi-scale pseudo-labels. Then, to fully leverage both\nobtained frame-level and utterance-level emotion labels, we incorporate model\nretraining and fine-tuning methods to further optimize GMP-ATL. Experiments on\nIEMOCAP show that our GMP-ATL achieves superior recognition performance, with a\nWAR of 80.0\\% and a UAR of 82.0\\%, surpassing state-of-the-art unimodal SER\nmethods, while also yielding comparable results with multimodal SER approaches.\n", "link": "http://arxiv.org/abs/2405.02151v1", "date": "2024-05-03", "relevancy": 1.9876, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5028}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMP-ATL%3A%20Gender-augmented%20Multi-scale%20Pseudo-label%20Enhanced%20Adaptive%0A%20%20Transfer%20Learning%20for%20Speech%20Emotion%20Recognition%20via%20HuBERT&body=Title%3A%20GMP-ATL%3A%20Gender-augmented%20Multi-scale%20Pseudo-label%20Enhanced%20Adaptive%0A%20%20Transfer%20Learning%20for%20Speech%20Emotion%20Recognition%20via%20HuBERT%0AAuthor%3A%20Yu%20Pan%20and%20Yuguang%20Yang%20and%20Heng%20Lu%20and%20Lei%20Ma%20and%20Jianjun%20Zhao%0AAbstract%3A%20%20%20The%20continuous%20evolution%20of%20pre-trained%20speech%20models%20has%20greatly%20advanced%0ASpeech%20Emotion%20Recognition%20%28SER%29.%20However%2C%20there%20is%20still%20potential%20for%0Aenhancement%20in%20the%20performance%20of%20these%20methods.%20In%20this%20paper%2C%20we%20present%0AGMP-ATL%20%28Gender-augmented%20Multi-scale%20Pseudo-label%20Adaptive%20Transfer%20Learning%29%2C%0Aa%20novel%20HuBERT-based%20adaptive%20transfer%20learning%20framework%20for%20SER.%0ASpecifically%2C%20GMP-ATL%20initially%20employs%20the%20pre-trained%20HuBERT%2C%20implementing%0Amulti-task%20learning%20and%20multi-scale%20k-means%20clustering%20to%20acquire%20frame-level%0Agender-augmented%20multi-scale%20pseudo-labels.%20Then%2C%20to%20fully%20leverage%20both%0Aobtained%20frame-level%20and%20utterance-level%20emotion%20labels%2C%20we%20incorporate%20model%0Aretraining%20and%20fine-tuning%20methods%20to%20further%20optimize%20GMP-ATL.%20Experiments%20on%0AIEMOCAP%20show%20that%20our%20GMP-ATL%20achieves%20superior%20recognition%20performance%2C%20with%20a%0AWAR%20of%2080.0%5C%25%20and%20a%20UAR%20of%2082.0%5C%25%2C%20surpassing%20state-of-the-art%20unimodal%20SER%0Amethods%2C%20while%20also%20yielding%20comparable%20results%20with%20multimodal%20SER%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMP-ATL%253A%2520Gender-augmented%2520Multi-scale%2520Pseudo-label%2520Enhanced%2520Adaptive%250A%2520%2520Transfer%2520Learning%2520for%2520Speech%2520Emotion%2520Recognition%2520via%2520HuBERT%26entry.906535625%3DYu%2520Pan%2520and%2520Yuguang%2520Yang%2520and%2520Heng%2520Lu%2520and%2520Lei%2520Ma%2520and%2520Jianjun%2520Zhao%26entry.1292438233%3D%2520%2520The%2520continuous%2520evolution%2520of%2520pre-trained%2520speech%2520models%2520has%2520greatly%2520advanced%250ASpeech%2520Emotion%2520Recognition%2520%2528SER%2529.%2520However%252C%2520there%2520is%2520still%2520potential%2520for%250Aenhancement%2520in%2520the%2520performance%2520of%2520these%2520methods.%2520In%2520this%2520paper%252C%2520we%2520present%250AGMP-ATL%2520%2528Gender-augmented%2520Multi-scale%2520Pseudo-label%2520Adaptive%2520Transfer%2520Learning%2529%252C%250Aa%2520novel%2520HuBERT-based%2520adaptive%2520transfer%2520learning%2520framework%2520for%2520SER.%250ASpecifically%252C%2520GMP-ATL%2520initially%2520employs%2520the%2520pre-trained%2520HuBERT%252C%2520implementing%250Amulti-task%2520learning%2520and%2520multi-scale%2520k-means%2520clustering%2520to%2520acquire%2520frame-level%250Agender-augmented%2520multi-scale%2520pseudo-labels.%2520Then%252C%2520to%2520fully%2520leverage%2520both%250Aobtained%2520frame-level%2520and%2520utterance-level%2520emotion%2520labels%252C%2520we%2520incorporate%2520model%250Aretraining%2520and%2520fine-tuning%2520methods%2520to%2520further%2520optimize%2520GMP-ATL.%2520Experiments%2520on%250AIEMOCAP%2520show%2520that%2520our%2520GMP-ATL%2520achieves%2520superior%2520recognition%2520performance%252C%2520with%2520a%250AWAR%2520of%252080.0%255C%2525%2520and%2520a%2520UAR%2520of%252082.0%255C%2525%252C%2520surpassing%2520state-of-the-art%2520unimodal%2520SER%250Amethods%252C%2520while%2520also%2520yielding%2520comparable%2520results%2520with%2520multimodal%2520SER%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMP-ATL%3A%20Gender-augmented%20Multi-scale%20Pseudo-label%20Enhanced%20Adaptive%0A%20%20Transfer%20Learning%20for%20Speech%20Emotion%20Recognition%20via%20HuBERT&entry.906535625=Yu%20Pan%20and%20Yuguang%20Yang%20and%20Heng%20Lu%20and%20Lei%20Ma%20and%20Jianjun%20Zhao&entry.1292438233=%20%20The%20continuous%20evolution%20of%20pre-trained%20speech%20models%20has%20greatly%20advanced%0ASpeech%20Emotion%20Recognition%20%28SER%29.%20However%2C%20there%20is%20still%20potential%20for%0Aenhancement%20in%20the%20performance%20of%20these%20methods.%20In%20this%20paper%2C%20we%20present%0AGMP-ATL%20%28Gender-augmented%20Multi-scale%20Pseudo-label%20Adaptive%20Transfer%20Learning%29%2C%0Aa%20novel%20HuBERT-based%20adaptive%20transfer%20learning%20framework%20for%20SER.%0ASpecifically%2C%20GMP-ATL%20initially%20employs%20the%20pre-trained%20HuBERT%2C%20implementing%0Amulti-task%20learning%20and%20multi-scale%20k-means%20clustering%20to%20acquire%20frame-level%0Agender-augmented%20multi-scale%20pseudo-labels.%20Then%2C%20to%20fully%20leverage%20both%0Aobtained%20frame-level%20and%20utterance-level%20emotion%20labels%2C%20we%20incorporate%20model%0Aretraining%20and%20fine-tuning%20methods%20to%20further%20optimize%20GMP-ATL.%20Experiments%20on%0AIEMOCAP%20show%20that%20our%20GMP-ATL%20achieves%20superior%20recognition%20performance%2C%20with%20a%0AWAR%20of%2080.0%5C%25%20and%20a%20UAR%20of%2082.0%5C%25%2C%20surpassing%20state-of-the-art%20unimodal%20SER%0Amethods%2C%20while%20also%20yielding%20comparable%20results%20with%20multimodal%20SER%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02151v1&entry.124074799=Read"},
{"title": "Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic\n  Labeling using Foundation Models", "author": "Mohamad Al Mdfaa and Raghad Salameh and Sergey Zagoruyko and Gonzalo Ferrer", "abstract": "  In the field of robotics and computer vision, efficient and accurate semantic\nmapping remains a significant challenge due to the growing demand for\nintelligent machines that can comprehend and interact with complex\nenvironments. Conventional panoptic mapping methods, however, are limited by\npredefined semantic classes, thus making them ineffective for handling novel or\nunforeseen objects. In response to this limitation, we introduce the Unified\nPromptable Panoptic Mapping (UPPM) method. UPPM utilizes recent advances in\nfoundation models to enable real-time, on-demand label generation using natural\nlanguage prompts. By incorporating a dynamic labeling strategy into traditional\npanoptic mapping techniques, UPPM provides significant improvements in\nadaptability and versatility while maintaining high performance levels in map\nreconstruction. We demonstrate our approach on real-world and simulated\ndatasets. Results show that UPPM can accurately reconstruct scenes and segment\nobjects while generating rich semantic labels through natural language\ninteractions. A series of ablation experiments validated the advantages of\nfoundation model-based labeling over fixed label sets.\n", "link": "http://arxiv.org/abs/2405.02162v1", "date": "2024-05-03", "relevancy": 1.9728, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6417}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models&body=Title%3A%20Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models%0AAuthor%3A%20Mohamad%20Al%20Mdfaa%20and%20Raghad%20Salameh%20and%20Sergey%20Zagoruyko%20and%20Gonzalo%20Ferrer%0AAbstract%3A%20%20%20In%20the%20field%20of%20robotics%20and%20computer%20vision%2C%20efficient%20and%20accurate%20semantic%0Amapping%20remains%20a%20significant%20challenge%20due%20to%20the%20growing%20demand%20for%0Aintelligent%20machines%20that%20can%20comprehend%20and%20interact%20with%20complex%0Aenvironments.%20Conventional%20panoptic%20mapping%20methods%2C%20however%2C%20are%20limited%20by%0Apredefined%20semantic%20classes%2C%20thus%20making%20them%20ineffective%20for%20handling%20novel%20or%0Aunforeseen%20objects.%20In%20response%20to%20this%20limitation%2C%20we%20introduce%20the%20Unified%0APromptable%20Panoptic%20Mapping%20%28UPPM%29%20method.%20UPPM%20utilizes%20recent%20advances%20in%0Afoundation%20models%20to%20enable%20real-time%2C%20on-demand%20label%20generation%20using%20natural%0Alanguage%20prompts.%20By%20incorporating%20a%20dynamic%20labeling%20strategy%20into%20traditional%0Apanoptic%20mapping%20techniques%2C%20UPPM%20provides%20significant%20improvements%20in%0Aadaptability%20and%20versatility%20while%20maintaining%20high%20performance%20levels%20in%20map%0Areconstruction.%20We%20demonstrate%20our%20approach%20on%20real-world%20and%20simulated%0Adatasets.%20Results%20show%20that%20UPPM%20can%20accurately%20reconstruct%20scenes%20and%20segment%0Aobjects%20while%20generating%20rich%20semantic%20labels%20through%20natural%20language%0Ainteractions.%20A%20series%20of%20ablation%20experiments%20validated%20the%20advantages%20of%0Afoundation%20model-based%20labeling%20over%20fixed%20label%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520the%2520Unseen%253A%2520Unified%2520Promptable%2520Panoptic%2520Mapping%2520with%2520Dynamic%250A%2520%2520Labeling%2520using%2520Foundation%2520Models%26entry.906535625%3DMohamad%2520Al%2520Mdfaa%2520and%2520Raghad%2520Salameh%2520and%2520Sergey%2520Zagoruyko%2520and%2520Gonzalo%2520Ferrer%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520robotics%2520and%2520computer%2520vision%252C%2520efficient%2520and%2520accurate%2520semantic%250Amapping%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520growing%2520demand%2520for%250Aintelligent%2520machines%2520that%2520can%2520comprehend%2520and%2520interact%2520with%2520complex%250Aenvironments.%2520Conventional%2520panoptic%2520mapping%2520methods%252C%2520however%252C%2520are%2520limited%2520by%250Apredefined%2520semantic%2520classes%252C%2520thus%2520making%2520them%2520ineffective%2520for%2520handling%2520novel%2520or%250Aunforeseen%2520objects.%2520In%2520response%2520to%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520Unified%250APromptable%2520Panoptic%2520Mapping%2520%2528UPPM%2529%2520method.%2520UPPM%2520utilizes%2520recent%2520advances%2520in%250Afoundation%2520models%2520to%2520enable%2520real-time%252C%2520on-demand%2520label%2520generation%2520using%2520natural%250Alanguage%2520prompts.%2520By%2520incorporating%2520a%2520dynamic%2520labeling%2520strategy%2520into%2520traditional%250Apanoptic%2520mapping%2520techniques%252C%2520UPPM%2520provides%2520significant%2520improvements%2520in%250Aadaptability%2520and%2520versatility%2520while%2520maintaining%2520high%2520performance%2520levels%2520in%2520map%250Areconstruction.%2520We%2520demonstrate%2520our%2520approach%2520on%2520real-world%2520and%2520simulated%250Adatasets.%2520Results%2520show%2520that%2520UPPM%2520can%2520accurately%2520reconstruct%2520scenes%2520and%2520segment%250Aobjects%2520while%2520generating%2520rich%2520semantic%2520labels%2520through%2520natural%2520language%250Ainteractions.%2520A%2520series%2520of%2520ablation%2520experiments%2520validated%2520the%2520advantages%2520of%250Afoundation%2520model-based%2520labeling%2520over%2520fixed%2520label%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models&entry.906535625=Mohamad%20Al%20Mdfaa%20and%20Raghad%20Salameh%20and%20Sergey%20Zagoruyko%20and%20Gonzalo%20Ferrer&entry.1292438233=%20%20In%20the%20field%20of%20robotics%20and%20computer%20vision%2C%20efficient%20and%20accurate%20semantic%0Amapping%20remains%20a%20significant%20challenge%20due%20to%20the%20growing%20demand%20for%0Aintelligent%20machines%20that%20can%20comprehend%20and%20interact%20with%20complex%0Aenvironments.%20Conventional%20panoptic%20mapping%20methods%2C%20however%2C%20are%20limited%20by%0Apredefined%20semantic%20classes%2C%20thus%20making%20them%20ineffective%20for%20handling%20novel%20or%0Aunforeseen%20objects.%20In%20response%20to%20this%20limitation%2C%20we%20introduce%20the%20Unified%0APromptable%20Panoptic%20Mapping%20%28UPPM%29%20method.%20UPPM%20utilizes%20recent%20advances%20in%0Afoundation%20models%20to%20enable%20real-time%2C%20on-demand%20label%20generation%20using%20natural%0Alanguage%20prompts.%20By%20incorporating%20a%20dynamic%20labeling%20strategy%20into%20traditional%0Apanoptic%20mapping%20techniques%2C%20UPPM%20provides%20significant%20improvements%20in%0Aadaptability%20and%20versatility%20while%20maintaining%20high%20performance%20levels%20in%20map%0Areconstruction.%20We%20demonstrate%20our%20approach%20on%20real-world%20and%20simulated%0Adatasets.%20Results%20show%20that%20UPPM%20can%20accurately%20reconstruct%20scenes%20and%20segment%0Aobjects%20while%20generating%20rich%20semantic%20labels%20through%20natural%20language%0Ainteractions.%20A%20series%20of%20ablation%20experiments%20validated%20the%20advantages%20of%0Afoundation%20model-based%20labeling%20over%20fixed%20label%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02162v1&entry.124074799=Read"},
{"title": "Training-Free Deepfake Voice Recognition by Leveraging Large-Scale\n  Pre-Trained Models", "author": "Alessandro Pianese and Davide Cozzolino and Giovanni Poggi and Luisa Verdoliva", "abstract": "  Generalization is a main issue for current audio deepfake detectors, which\nstruggle to provide reliable results on out-of-distribution data. Given the\nspeed at which more and more accurate synthesis methods are developed, it is\nvery important to design techniques that work well also on data they were not\ntrained for.In this paper we study the potential of large-scale pre-trained\nmodels for audio deepfake detection, with special focus on generalization\nability. To this end, the detection problem is reformulated in a speaker\nverification framework and fake audios are exposed by the mismatch between the\nvoice sample under test and the voice of the claimed identity. With this\nparadigm, no fake speech sample is necessary in training, cutting off any link\nwith the generation method at the root, and ensuring full generalization\nability. Features are extracted by general-purpose large pre-trained models,\nwith no need for training or fine-tuning on specific fake detection or speaker\nverification datasets. At detection time only a limited set of voice fragments\nof the identity under test is required. Experiments on several datasets\nwidespread in the community show that detectors based on pre-trained models\nachieve excellent performance and show strong generalization ability, rivaling\nsupervised methods on in-distribution data and largely overcoming them on\nout-of-distribution data.\n", "link": "http://arxiv.org/abs/2405.02179v1", "date": "2024-05-03", "relevancy": 1.9686, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5029}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4884}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Deepfake%20Voice%20Recognition%20by%20Leveraging%20Large-Scale%0A%20%20Pre-Trained%20Models&body=Title%3A%20Training-Free%20Deepfake%20Voice%20Recognition%20by%20Leveraging%20Large-Scale%0A%20%20Pre-Trained%20Models%0AAuthor%3A%20Alessandro%20Pianese%20and%20Davide%20Cozzolino%20and%20Giovanni%20Poggi%20and%20Luisa%20Verdoliva%0AAbstract%3A%20%20%20Generalization%20is%20a%20main%20issue%20for%20current%20audio%20deepfake%20detectors%2C%20which%0Astruggle%20to%20provide%20reliable%20results%20on%20out-of-distribution%20data.%20Given%20the%0Aspeed%20at%20which%20more%20and%20more%20accurate%20synthesis%20methods%20are%20developed%2C%20it%20is%0Avery%20important%20to%20design%20techniques%20that%20work%20well%20also%20on%20data%20they%20were%20not%0Atrained%20for.In%20this%20paper%20we%20study%20the%20potential%20of%20large-scale%20pre-trained%0Amodels%20for%20audio%20deepfake%20detection%2C%20with%20special%20focus%20on%20generalization%0Aability.%20To%20this%20end%2C%20the%20detection%20problem%20is%20reformulated%20in%20a%20speaker%0Averification%20framework%20and%20fake%20audios%20are%20exposed%20by%20the%20mismatch%20between%20the%0Avoice%20sample%20under%20test%20and%20the%20voice%20of%20the%20claimed%20identity.%20With%20this%0Aparadigm%2C%20no%20fake%20speech%20sample%20is%20necessary%20in%20training%2C%20cutting%20off%20any%20link%0Awith%20the%20generation%20method%20at%20the%20root%2C%20and%20ensuring%20full%20generalization%0Aability.%20Features%20are%20extracted%20by%20general-purpose%20large%20pre-trained%20models%2C%0Awith%20no%20need%20for%20training%20or%20fine-tuning%20on%20specific%20fake%20detection%20or%20speaker%0Averification%20datasets.%20At%20detection%20time%20only%20a%20limited%20set%20of%20voice%20fragments%0Aof%20the%20identity%20under%20test%20is%20required.%20Experiments%20on%20several%20datasets%0Awidespread%20in%20the%20community%20show%20that%20detectors%20based%20on%20pre-trained%20models%0Aachieve%20excellent%20performance%20and%20show%20strong%20generalization%20ability%2C%20rivaling%0Asupervised%20methods%20on%20in-distribution%20data%20and%20largely%20overcoming%20them%20on%0Aout-of-distribution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Deepfake%2520Voice%2520Recognition%2520by%2520Leveraging%2520Large-Scale%250A%2520%2520Pre-Trained%2520Models%26entry.906535625%3DAlessandro%2520Pianese%2520and%2520Davide%2520Cozzolino%2520and%2520Giovanni%2520Poggi%2520and%2520Luisa%2520Verdoliva%26entry.1292438233%3D%2520%2520Generalization%2520is%2520a%2520main%2520issue%2520for%2520current%2520audio%2520deepfake%2520detectors%252C%2520which%250Astruggle%2520to%2520provide%2520reliable%2520results%2520on%2520out-of-distribution%2520data.%2520Given%2520the%250Aspeed%2520at%2520which%2520more%2520and%2520more%2520accurate%2520synthesis%2520methods%2520are%2520developed%252C%2520it%2520is%250Avery%2520important%2520to%2520design%2520techniques%2520that%2520work%2520well%2520also%2520on%2520data%2520they%2520were%2520not%250Atrained%2520for.In%2520this%2520paper%2520we%2520study%2520the%2520potential%2520of%2520large-scale%2520pre-trained%250Amodels%2520for%2520audio%2520deepfake%2520detection%252C%2520with%2520special%2520focus%2520on%2520generalization%250Aability.%2520To%2520this%2520end%252C%2520the%2520detection%2520problem%2520is%2520reformulated%2520in%2520a%2520speaker%250Averification%2520framework%2520and%2520fake%2520audios%2520are%2520exposed%2520by%2520the%2520mismatch%2520between%2520the%250Avoice%2520sample%2520under%2520test%2520and%2520the%2520voice%2520of%2520the%2520claimed%2520identity.%2520With%2520this%250Aparadigm%252C%2520no%2520fake%2520speech%2520sample%2520is%2520necessary%2520in%2520training%252C%2520cutting%2520off%2520any%2520link%250Awith%2520the%2520generation%2520method%2520at%2520the%2520root%252C%2520and%2520ensuring%2520full%2520generalization%250Aability.%2520Features%2520are%2520extracted%2520by%2520general-purpose%2520large%2520pre-trained%2520models%252C%250Awith%2520no%2520need%2520for%2520training%2520or%2520fine-tuning%2520on%2520specific%2520fake%2520detection%2520or%2520speaker%250Averification%2520datasets.%2520At%2520detection%2520time%2520only%2520a%2520limited%2520set%2520of%2520voice%2520fragments%250Aof%2520the%2520identity%2520under%2520test%2520is%2520required.%2520Experiments%2520on%2520several%2520datasets%250Awidespread%2520in%2520the%2520community%2520show%2520that%2520detectors%2520based%2520on%2520pre-trained%2520models%250Aachieve%2520excellent%2520performance%2520and%2520show%2520strong%2520generalization%2520ability%252C%2520rivaling%250Asupervised%2520methods%2520on%2520in-distribution%2520data%2520and%2520largely%2520overcoming%2520them%2520on%250Aout-of-distribution%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Deepfake%20Voice%20Recognition%20by%20Leveraging%20Large-Scale%0A%20%20Pre-Trained%20Models&entry.906535625=Alessandro%20Pianese%20and%20Davide%20Cozzolino%20and%20Giovanni%20Poggi%20and%20Luisa%20Verdoliva&entry.1292438233=%20%20Generalization%20is%20a%20main%20issue%20for%20current%20audio%20deepfake%20detectors%2C%20which%0Astruggle%20to%20provide%20reliable%20results%20on%20out-of-distribution%20data.%20Given%20the%0Aspeed%20at%20which%20more%20and%20more%20accurate%20synthesis%20methods%20are%20developed%2C%20it%20is%0Avery%20important%20to%20design%20techniques%20that%20work%20well%20also%20on%20data%20they%20were%20not%0Atrained%20for.In%20this%20paper%20we%20study%20the%20potential%20of%20large-scale%20pre-trained%0Amodels%20for%20audio%20deepfake%20detection%2C%20with%20special%20focus%20on%20generalization%0Aability.%20To%20this%20end%2C%20the%20detection%20problem%20is%20reformulated%20in%20a%20speaker%0Averification%20framework%20and%20fake%20audios%20are%20exposed%20by%20the%20mismatch%20between%20the%0Avoice%20sample%20under%20test%20and%20the%20voice%20of%20the%20claimed%20identity.%20With%20this%0Aparadigm%2C%20no%20fake%20speech%20sample%20is%20necessary%20in%20training%2C%20cutting%20off%20any%20link%0Awith%20the%20generation%20method%20at%20the%20root%2C%20and%20ensuring%20full%20generalization%0Aability.%20Features%20are%20extracted%20by%20general-purpose%20large%20pre-trained%20models%2C%0Awith%20no%20need%20for%20training%20or%20fine-tuning%20on%20specific%20fake%20detection%20or%20speaker%0Averification%20datasets.%20At%20detection%20time%20only%20a%20limited%20set%20of%20voice%20fragments%0Aof%20the%20identity%20under%20test%20is%20required.%20Experiments%20on%20several%20datasets%0Awidespread%20in%20the%20community%20show%20that%20detectors%20based%20on%20pre-trained%20models%0Aachieve%20excellent%20performance%20and%20show%20strong%20generalization%20ability%2C%20rivaling%0Asupervised%20methods%20on%20in-distribution%20data%20and%20largely%20overcoming%20them%20on%0Aout-of-distribution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02179v1&entry.124074799=Read"},
{"title": "IFNet: Deep Imaging and Focusing for Handheld SAR with Millimeter-wave\n  Signals", "author": "Li Yadong and Zhang Dongheng and Geng Ruixu and Wu Jincheng and Hu Yang and Sun Qibin and Chen Yan", "abstract": "  Recent advancements have showcased the potential of handheld millimeter-wave\n(mmWave) imaging, which applies synthetic aperture radar (SAR) principles in\nportable settings. However, existing studies addressing handheld motion errors\neither rely on costly tracking devices or employ simplified imaging models,\nleading to impractical deployment or limited performance. In this paper, we\npresent IFNet, a novel deep unfolding network that combines the strengths of\nsignal processing models and deep neural networks to achieve robust imaging and\nfocusing for handheld mmWave systems. We first formulate the handheld imaging\nmodel by integrating multiple priors about mmWave images and handheld phase\nerrors. Furthermore, we transform the optimization processes into an iterative\nnetwork structure for improved and efficient imaging performance. Extensive\nexperiments demonstrate that IFNet effectively compensates for handheld phase\nerrors and recovers high-fidelity images from severely distorted signals. In\ncomparison with existing methods, IFNet can achieve at least 11.89 dB\nimprovement in average peak signal-to-noise ratio (PSNR) and 64.91% improvement\nin average structural similarity index measure (SSIM) on a real-world dataset.\n", "link": "http://arxiv.org/abs/2405.02023v1", "date": "2024-05-03", "relevancy": 1.9658, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.497}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IFNet%3A%20Deep%20Imaging%20and%20Focusing%20for%20Handheld%20SAR%20with%20Millimeter-wave%0A%20%20Signals&body=Title%3A%20IFNet%3A%20Deep%20Imaging%20and%20Focusing%20for%20Handheld%20SAR%20with%20Millimeter-wave%0A%20%20Signals%0AAuthor%3A%20Li%20Yadong%20and%20Zhang%20Dongheng%20and%20Geng%20Ruixu%20and%20Wu%20Jincheng%20and%20Hu%20Yang%20and%20Sun%20Qibin%20and%20Chen%20Yan%0AAbstract%3A%20%20%20Recent%20advancements%20have%20showcased%20the%20potential%20of%20handheld%20millimeter-wave%0A%28mmWave%29%20imaging%2C%20which%20applies%20synthetic%20aperture%20radar%20%28SAR%29%20principles%20in%0Aportable%20settings.%20However%2C%20existing%20studies%20addressing%20handheld%20motion%20errors%0Aeither%20rely%20on%20costly%20tracking%20devices%20or%20employ%20simplified%20imaging%20models%2C%0Aleading%20to%20impractical%20deployment%20or%20limited%20performance.%20In%20this%20paper%2C%20we%0Apresent%20IFNet%2C%20a%20novel%20deep%20unfolding%20network%20that%20combines%20the%20strengths%20of%0Asignal%20processing%20models%20and%20deep%20neural%20networks%20to%20achieve%20robust%20imaging%20and%0Afocusing%20for%20handheld%20mmWave%20systems.%20We%20first%20formulate%20the%20handheld%20imaging%0Amodel%20by%20integrating%20multiple%20priors%20about%20mmWave%20images%20and%20handheld%20phase%0Aerrors.%20Furthermore%2C%20we%20transform%20the%20optimization%20processes%20into%20an%20iterative%0Anetwork%20structure%20for%20improved%20and%20efficient%20imaging%20performance.%20Extensive%0Aexperiments%20demonstrate%20that%20IFNet%20effectively%20compensates%20for%20handheld%20phase%0Aerrors%20and%20recovers%20high-fidelity%20images%20from%20severely%20distorted%20signals.%20In%0Acomparison%20with%20existing%20methods%2C%20IFNet%20can%20achieve%20at%20least%2011.89%20dB%0Aimprovement%20in%20average%20peak%20signal-to-noise%20ratio%20%28PSNR%29%20and%2064.91%25%20improvement%0Ain%20average%20structural%20similarity%20index%20measure%20%28SSIM%29%20on%20a%20real-world%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIFNet%253A%2520Deep%2520Imaging%2520and%2520Focusing%2520for%2520Handheld%2520SAR%2520with%2520Millimeter-wave%250A%2520%2520Signals%26entry.906535625%3DLi%2520Yadong%2520and%2520Zhang%2520Dongheng%2520and%2520Geng%2520Ruixu%2520and%2520Wu%2520Jincheng%2520and%2520Hu%2520Yang%2520and%2520Sun%2520Qibin%2520and%2520Chen%2520Yan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520showcased%2520the%2520potential%2520of%2520handheld%2520millimeter-wave%250A%2528mmWave%2529%2520imaging%252C%2520which%2520applies%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520principles%2520in%250Aportable%2520settings.%2520However%252C%2520existing%2520studies%2520addressing%2520handheld%2520motion%2520errors%250Aeither%2520rely%2520on%2520costly%2520tracking%2520devices%2520or%2520employ%2520simplified%2520imaging%2520models%252C%250Aleading%2520to%2520impractical%2520deployment%2520or%2520limited%2520performance.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520IFNet%252C%2520a%2520novel%2520deep%2520unfolding%2520network%2520that%2520combines%2520the%2520strengths%2520of%250Asignal%2520processing%2520models%2520and%2520deep%2520neural%2520networks%2520to%2520achieve%2520robust%2520imaging%2520and%250Afocusing%2520for%2520handheld%2520mmWave%2520systems.%2520We%2520first%2520formulate%2520the%2520handheld%2520imaging%250Amodel%2520by%2520integrating%2520multiple%2520priors%2520about%2520mmWave%2520images%2520and%2520handheld%2520phase%250Aerrors.%2520Furthermore%252C%2520we%2520transform%2520the%2520optimization%2520processes%2520into%2520an%2520iterative%250Anetwork%2520structure%2520for%2520improved%2520and%2520efficient%2520imaging%2520performance.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520IFNet%2520effectively%2520compensates%2520for%2520handheld%2520phase%250Aerrors%2520and%2520recovers%2520high-fidelity%2520images%2520from%2520severely%2520distorted%2520signals.%2520In%250Acomparison%2520with%2520existing%2520methods%252C%2520IFNet%2520can%2520achieve%2520at%2520least%252011.89%2520dB%250Aimprovement%2520in%2520average%2520peak%2520signal-to-noise%2520ratio%2520%2528PSNR%2529%2520and%252064.91%2525%2520improvement%250Ain%2520average%2520structural%2520similarity%2520index%2520measure%2520%2528SSIM%2529%2520on%2520a%2520real-world%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IFNet%3A%20Deep%20Imaging%20and%20Focusing%20for%20Handheld%20SAR%20with%20Millimeter-wave%0A%20%20Signals&entry.906535625=Li%20Yadong%20and%20Zhang%20Dongheng%20and%20Geng%20Ruixu%20and%20Wu%20Jincheng%20and%20Hu%20Yang%20and%20Sun%20Qibin%20and%20Chen%20Yan&entry.1292438233=%20%20Recent%20advancements%20have%20showcased%20the%20potential%20of%20handheld%20millimeter-wave%0A%28mmWave%29%20imaging%2C%20which%20applies%20synthetic%20aperture%20radar%20%28SAR%29%20principles%20in%0Aportable%20settings.%20However%2C%20existing%20studies%20addressing%20handheld%20motion%20errors%0Aeither%20rely%20on%20costly%20tracking%20devices%20or%20employ%20simplified%20imaging%20models%2C%0Aleading%20to%20impractical%20deployment%20or%20limited%20performance.%20In%20this%20paper%2C%20we%0Apresent%20IFNet%2C%20a%20novel%20deep%20unfolding%20network%20that%20combines%20the%20strengths%20of%0Asignal%20processing%20models%20and%20deep%20neural%20networks%20to%20achieve%20robust%20imaging%20and%0Afocusing%20for%20handheld%20mmWave%20systems.%20We%20first%20formulate%20the%20handheld%20imaging%0Amodel%20by%20integrating%20multiple%20priors%20about%20mmWave%20images%20and%20handheld%20phase%0Aerrors.%20Furthermore%2C%20we%20transform%20the%20optimization%20processes%20into%20an%20iterative%0Anetwork%20structure%20for%20improved%20and%20efficient%20imaging%20performance.%20Extensive%0Aexperiments%20demonstrate%20that%20IFNet%20effectively%20compensates%20for%20handheld%20phase%0Aerrors%20and%20recovers%20high-fidelity%20images%20from%20severely%20distorted%20signals.%20In%0Acomparison%20with%20existing%20methods%2C%20IFNet%20can%20achieve%20at%20least%2011.89%20dB%0Aimprovement%20in%20average%20peak%20signal-to-noise%20ratio%20%28PSNR%29%20and%2064.91%25%20improvement%0Ain%20average%20structural%20similarity%20index%20measure%20%28SSIM%29%20on%20a%20real-world%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02023v1&entry.124074799=Read"},
{"title": "Accelerating Convergence in Bayesian Few-Shot Classification", "author": "Tianjun Ke and Haoqun Cao and Feng Zhou", "abstract": "  Bayesian few-shot classification has been a focal point in the field of\nfew-shot learning. This paper seamlessly integrates mirror descent-based\nvariational inference into Gaussian process-based few-shot classification,\naddressing the challenge of non-conjugate inference. By leveraging\nnon-Euclidean geometry, mirror descent achieves accelerated convergence by\nproviding the steepest descent direction along the corresponding manifold. It\nalso exhibits the parameterization invariance property concerning the\nvariational distribution. Experimental results demonstrate competitive\nclassification accuracy, improved uncertainty quantification, and faster\nconvergence compared to baseline models. Additionally, we investigate the\nimpact of hyperparameters and components. Code is publicly available at\nhttps://github.com/keanson/MD-BSFC.\n", "link": "http://arxiv.org/abs/2405.01507v2", "date": "2024-05-03", "relevancy": 1.9618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5195}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification&body=Title%3A%20Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification%0AAuthor%3A%20Tianjun%20Ke%20and%20Haoqun%20Cao%20and%20Feng%20Zhou%0AAbstract%3A%20%20%20Bayesian%20few-shot%20classification%20has%20been%20a%20focal%20point%20in%20the%20field%20of%0Afew-shot%20learning.%20This%20paper%20seamlessly%20integrates%20mirror%20descent-based%0Avariational%20inference%20into%20Gaussian%20process-based%20few-shot%20classification%2C%0Aaddressing%20the%20challenge%20of%20non-conjugate%20inference.%20By%20leveraging%0Anon-Euclidean%20geometry%2C%20mirror%20descent%20achieves%20accelerated%20convergence%20by%0Aproviding%20the%20steepest%20descent%20direction%20along%20the%20corresponding%20manifold.%20It%0Aalso%20exhibits%20the%20parameterization%20invariance%20property%20concerning%20the%0Avariational%20distribution.%20Experimental%20results%20demonstrate%20competitive%0Aclassification%20accuracy%2C%20improved%20uncertainty%20quantification%2C%20and%20faster%0Aconvergence%20compared%20to%20baseline%20models.%20Additionally%2C%20we%20investigate%20the%0Aimpact%20of%20hyperparameters%20and%20components.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/keanson/MD-BSFC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Convergence%2520in%2520Bayesian%2520Few-Shot%2520Classification%26entry.906535625%3DTianjun%2520Ke%2520and%2520Haoqun%2520Cao%2520and%2520Feng%2520Zhou%26entry.1292438233%3D%2520%2520Bayesian%2520few-shot%2520classification%2520has%2520been%2520a%2520focal%2520point%2520in%2520the%2520field%2520of%250Afew-shot%2520learning.%2520This%2520paper%2520seamlessly%2520integrates%2520mirror%2520descent-based%250Avariational%2520inference%2520into%2520Gaussian%2520process-based%2520few-shot%2520classification%252C%250Aaddressing%2520the%2520challenge%2520of%2520non-conjugate%2520inference.%2520By%2520leveraging%250Anon-Euclidean%2520geometry%252C%2520mirror%2520descent%2520achieves%2520accelerated%2520convergence%2520by%250Aproviding%2520the%2520steepest%2520descent%2520direction%2520along%2520the%2520corresponding%2520manifold.%2520It%250Aalso%2520exhibits%2520the%2520parameterization%2520invariance%2520property%2520concerning%2520the%250Avariational%2520distribution.%2520Experimental%2520results%2520demonstrate%2520competitive%250Aclassification%2520accuracy%252C%2520improved%2520uncertainty%2520quantification%252C%2520and%2520faster%250Aconvergence%2520compared%2520to%2520baseline%2520models.%2520Additionally%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520hyperparameters%2520and%2520components.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/keanson/MD-BSFC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification&entry.906535625=Tianjun%20Ke%20and%20Haoqun%20Cao%20and%20Feng%20Zhou&entry.1292438233=%20%20Bayesian%20few-shot%20classification%20has%20been%20a%20focal%20point%20in%20the%20field%20of%0Afew-shot%20learning.%20This%20paper%20seamlessly%20integrates%20mirror%20descent-based%0Avariational%20inference%20into%20Gaussian%20process-based%20few-shot%20classification%2C%0Aaddressing%20the%20challenge%20of%20non-conjugate%20inference.%20By%20leveraging%0Anon-Euclidean%20geometry%2C%20mirror%20descent%20achieves%20accelerated%20convergence%20by%0Aproviding%20the%20steepest%20descent%20direction%20along%20the%20corresponding%20manifold.%20It%0Aalso%20exhibits%20the%20parameterization%20invariance%20property%20concerning%20the%0Avariational%20distribution.%20Experimental%20results%20demonstrate%20competitive%0Aclassification%20accuracy%2C%20improved%20uncertainty%20quantification%2C%20and%20faster%0Aconvergence%20compared%20to%20baseline%20models.%20Additionally%2C%20we%20investigate%20the%0Aimpact%20of%20hyperparameters%20and%20components.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/keanson/MD-BSFC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01507v2&entry.124074799=Read"},
{"title": "EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and\n  Multi-View Transformer", "author": "Hanwen Liu and Daniel Hajialigol and Benny Antony and Aiguo Han and Xuan Wang", "abstract": "  Deciphering the intricacies of the human brain has captivated curiosity for\ncenturies. Recent strides in Brain-Computer Interface (BCI) technology,\nparticularly using motor imagery, have restored motor functions such as\nreaching, grasping, and walking in paralyzed individuals. However, unraveling\nnatural language from brain signals remains a formidable challenge.\nElectroencephalography (EEG) is a non-invasive technique used to record\nelectrical activity in the brain by placing electrodes on the scalp. Previous\nstudies of EEG-to-text decoding have achieved high accuracy on small closed\nvocabularies, but still fall short of high accuracy when dealing with large\nopen vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy\nof open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG\npre-training to enhance the learning of semantics from EEG signals and proposes\na multi-view transformer to model the EEG signal processing by different\nspatial regions of the brain. Experiments show that EEG2TEXT has superior\nperformance, outperforming the state-of-the-art baseline methods by a large\nmargin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great\npotential for a high-performance open-vocabulary brain-to-text system to\nfacilitate communication.\n", "link": "http://arxiv.org/abs/2405.02165v1", "date": "2024-05-03", "relevancy": 1.9568, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5167}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5025}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG2TEXT%3A%20Open%20Vocabulary%20EEG-to-Text%20Decoding%20with%20EEG%20Pre-Training%20and%0A%20%20Multi-View%20Transformer&body=Title%3A%20EEG2TEXT%3A%20Open%20Vocabulary%20EEG-to-Text%20Decoding%20with%20EEG%20Pre-Training%20and%0A%20%20Multi-View%20Transformer%0AAuthor%3A%20Hanwen%20Liu%20and%20Daniel%20Hajialigol%20and%20Benny%20Antony%20and%20Aiguo%20Han%20and%20Xuan%20Wang%0AAbstract%3A%20%20%20Deciphering%20the%20intricacies%20of%20the%20human%20brain%20has%20captivated%20curiosity%20for%0Acenturies.%20Recent%20strides%20in%20Brain-Computer%20Interface%20%28BCI%29%20technology%2C%0Aparticularly%20using%20motor%20imagery%2C%20have%20restored%20motor%20functions%20such%20as%0Areaching%2C%20grasping%2C%20and%20walking%20in%20paralyzed%20individuals.%20However%2C%20unraveling%0Anatural%20language%20from%20brain%20signals%20remains%20a%20formidable%20challenge.%0AElectroencephalography%20%28EEG%29%20is%20a%20non-invasive%20technique%20used%20to%20record%0Aelectrical%20activity%20in%20the%20brain%20by%20placing%20electrodes%20on%20the%20scalp.%20Previous%0Astudies%20of%20EEG-to-text%20decoding%20have%20achieved%20high%20accuracy%20on%20small%20closed%0Avocabularies%2C%20but%20still%20fall%20short%20of%20high%20accuracy%20when%20dealing%20with%20large%0Aopen%20vocabularies.%20We%20propose%20a%20novel%20method%2C%20EEG2TEXT%2C%20to%20improve%20the%20accuracy%0Aof%20open%20vocabulary%20EEG-to-text%20decoding.%20Specifically%2C%20EEG2TEXT%20leverages%20EEG%0Apre-training%20to%20enhance%20the%20learning%20of%20semantics%20from%20EEG%20signals%20and%20proposes%0Aa%20multi-view%20transformer%20to%20model%20the%20EEG%20signal%20processing%20by%20different%0Aspatial%20regions%20of%20the%20brain.%20Experiments%20show%20that%20EEG2TEXT%20has%20superior%0Aperformance%2C%20outperforming%20the%20state-of-the-art%20baseline%20methods%20by%20a%20large%0Amargin%20of%20up%20to%205%25%20in%20absolute%20BLEU%20and%20ROUGE%20scores.%20EEG2TEXT%20shows%20great%0Apotential%20for%20a%20high-performance%20open-vocabulary%20brain-to-text%20system%20to%0Afacilitate%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG2TEXT%253A%2520Open%2520Vocabulary%2520EEG-to-Text%2520Decoding%2520with%2520EEG%2520Pre-Training%2520and%250A%2520%2520Multi-View%2520Transformer%26entry.906535625%3DHanwen%2520Liu%2520and%2520Daniel%2520Hajialigol%2520and%2520Benny%2520Antony%2520and%2520Aiguo%2520Han%2520and%2520Xuan%2520Wang%26entry.1292438233%3D%2520%2520Deciphering%2520the%2520intricacies%2520of%2520the%2520human%2520brain%2520has%2520captivated%2520curiosity%2520for%250Acenturies.%2520Recent%2520strides%2520in%2520Brain-Computer%2520Interface%2520%2528BCI%2529%2520technology%252C%250Aparticularly%2520using%2520motor%2520imagery%252C%2520have%2520restored%2520motor%2520functions%2520such%2520as%250Areaching%252C%2520grasping%252C%2520and%2520walking%2520in%2520paralyzed%2520individuals.%2520However%252C%2520unraveling%250Anatural%2520language%2520from%2520brain%2520signals%2520remains%2520a%2520formidable%2520challenge.%250AElectroencephalography%2520%2528EEG%2529%2520is%2520a%2520non-invasive%2520technique%2520used%2520to%2520record%250Aelectrical%2520activity%2520in%2520the%2520brain%2520by%2520placing%2520electrodes%2520on%2520the%2520scalp.%2520Previous%250Astudies%2520of%2520EEG-to-text%2520decoding%2520have%2520achieved%2520high%2520accuracy%2520on%2520small%2520closed%250Avocabularies%252C%2520but%2520still%2520fall%2520short%2520of%2520high%2520accuracy%2520when%2520dealing%2520with%2520large%250Aopen%2520vocabularies.%2520We%2520propose%2520a%2520novel%2520method%252C%2520EEG2TEXT%252C%2520to%2520improve%2520the%2520accuracy%250Aof%2520open%2520vocabulary%2520EEG-to-text%2520decoding.%2520Specifically%252C%2520EEG2TEXT%2520leverages%2520EEG%250Apre-training%2520to%2520enhance%2520the%2520learning%2520of%2520semantics%2520from%2520EEG%2520signals%2520and%2520proposes%250Aa%2520multi-view%2520transformer%2520to%2520model%2520the%2520EEG%2520signal%2520processing%2520by%2520different%250Aspatial%2520regions%2520of%2520the%2520brain.%2520Experiments%2520show%2520that%2520EEG2TEXT%2520has%2520superior%250Aperformance%252C%2520outperforming%2520the%2520state-of-the-art%2520baseline%2520methods%2520by%2520a%2520large%250Amargin%2520of%2520up%2520to%25205%2525%2520in%2520absolute%2520BLEU%2520and%2520ROUGE%2520scores.%2520EEG2TEXT%2520shows%2520great%250Apotential%2520for%2520a%2520high-performance%2520open-vocabulary%2520brain-to-text%2520system%2520to%250Afacilitate%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG2TEXT%3A%20Open%20Vocabulary%20EEG-to-Text%20Decoding%20with%20EEG%20Pre-Training%20and%0A%20%20Multi-View%20Transformer&entry.906535625=Hanwen%20Liu%20and%20Daniel%20Hajialigol%20and%20Benny%20Antony%20and%20Aiguo%20Han%20and%20Xuan%20Wang&entry.1292438233=%20%20Deciphering%20the%20intricacies%20of%20the%20human%20brain%20has%20captivated%20curiosity%20for%0Acenturies.%20Recent%20strides%20in%20Brain-Computer%20Interface%20%28BCI%29%20technology%2C%0Aparticularly%20using%20motor%20imagery%2C%20have%20restored%20motor%20functions%20such%20as%0Areaching%2C%20grasping%2C%20and%20walking%20in%20paralyzed%20individuals.%20However%2C%20unraveling%0Anatural%20language%20from%20brain%20signals%20remains%20a%20formidable%20challenge.%0AElectroencephalography%20%28EEG%29%20is%20a%20non-invasive%20technique%20used%20to%20record%0Aelectrical%20activity%20in%20the%20brain%20by%20placing%20electrodes%20on%20the%20scalp.%20Previous%0Astudies%20of%20EEG-to-text%20decoding%20have%20achieved%20high%20accuracy%20on%20small%20closed%0Avocabularies%2C%20but%20still%20fall%20short%20of%20high%20accuracy%20when%20dealing%20with%20large%0Aopen%20vocabularies.%20We%20propose%20a%20novel%20method%2C%20EEG2TEXT%2C%20to%20improve%20the%20accuracy%0Aof%20open%20vocabulary%20EEG-to-text%20decoding.%20Specifically%2C%20EEG2TEXT%20leverages%20EEG%0Apre-training%20to%20enhance%20the%20learning%20of%20semantics%20from%20EEG%20signals%20and%20proposes%0Aa%20multi-view%20transformer%20to%20model%20the%20EEG%20signal%20processing%20by%20different%0Aspatial%20regions%20of%20the%20brain.%20Experiments%20show%20that%20EEG2TEXT%20has%20superior%0Aperformance%2C%20outperforming%20the%20state-of-the-art%20baseline%20methods%20by%20a%20large%0Amargin%20of%20up%20to%205%25%20in%20absolute%20BLEU%20and%20ROUGE%20scores.%20EEG2TEXT%20shows%20great%0Apotential%20for%20a%20high-performance%20open-vocabulary%20brain-to-text%20system%20to%0Afacilitate%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02165v1&entry.124074799=Read"},
{"title": "Fairness Without Demographics in Human-Centered Federated Learning", "author": "Shaily Roy and Harshit Sharma and Asif Salekin", "abstract": "  Federated learning (FL) enables collaborative model training while preserving\ndata privacy, making it suitable for decentralized human-centered AI\napplications. However, a significant research gap remains in ensuring fairness\nin these systems. Current fairness strategies in FL require knowledge of\nbias-creating/sensitive attributes, clashing with FL's privacy principles.\nMoreover, in human-centered datasets, sensitive attributes may remain latent.\nTo tackle these challenges, we present a novel bias mitigation approach\ninspired by \"Fairness without Demographics\" in machine learning. The presented\napproach achieves fairness without needing knowledge of sensitive attributes by\nminimizing the top eigenvalue of the Hessian matrix during training, ensuring\nequitable loss landscapes across FL participants. Notably, we introduce a novel\nFL aggregation scheme that promotes participating models based on error rates\nand loss landscape curvature attributes, fostering fairness across the FL\nsystem. This work represents the first approach to attaining \"Fairness without\nDemographics\" in human-centered FL. Through comprehensive evaluation, our\napproach demonstrates effectiveness in balancing fairness and efficacy across\nvarious real-world applications, FL setups, and scenarios involving single and\nmultiple bias-inducing factors, representing a significant advancement in\nhuman-centered FL.\n", "link": "http://arxiv.org/abs/2404.19725v2", "date": "2024-05-03", "relevancy": 1.9538, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4883}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20Without%20Demographics%20in%20Human-Centered%20Federated%20Learning&body=Title%3A%20Fairness%20Without%20Demographics%20in%20Human-Centered%20Federated%20Learning%0AAuthor%3A%20Shaily%20Roy%20and%20Harshit%20Sharma%20and%20Asif%20Salekin%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20while%20preserving%0Adata%20privacy%2C%20making%20it%20suitable%20for%20decentralized%20human-centered%20AI%0Aapplications.%20However%2C%20a%20significant%20research%20gap%20remains%20in%20ensuring%20fairness%0Ain%20these%20systems.%20Current%20fairness%20strategies%20in%20FL%20require%20knowledge%20of%0Abias-creating/sensitive%20attributes%2C%20clashing%20with%20FL%27s%20privacy%20principles.%0AMoreover%2C%20in%20human-centered%20datasets%2C%20sensitive%20attributes%20may%20remain%20latent.%0ATo%20tackle%20these%20challenges%2C%20we%20present%20a%20novel%20bias%20mitigation%20approach%0Ainspired%20by%20%22Fairness%20without%20Demographics%22%20in%20machine%20learning.%20The%20presented%0Aapproach%20achieves%20fairness%20without%20needing%20knowledge%20of%20sensitive%20attributes%20by%0Aminimizing%20the%20top%20eigenvalue%20of%20the%20Hessian%20matrix%20during%20training%2C%20ensuring%0Aequitable%20loss%20landscapes%20across%20FL%20participants.%20Notably%2C%20we%20introduce%20a%20novel%0AFL%20aggregation%20scheme%20that%20promotes%20participating%20models%20based%20on%20error%20rates%0Aand%20loss%20landscape%20curvature%20attributes%2C%20fostering%20fairness%20across%20the%20FL%0Asystem.%20This%20work%20represents%20the%20first%20approach%20to%20attaining%20%22Fairness%20without%0ADemographics%22%20in%20human-centered%20FL.%20Through%20comprehensive%20evaluation%2C%20our%0Aapproach%20demonstrates%20effectiveness%20in%20balancing%20fairness%20and%20efficacy%20across%0Avarious%20real-world%20applications%2C%20FL%20setups%2C%20and%20scenarios%20involving%20single%20and%0Amultiple%20bias-inducing%20factors%2C%20representing%20a%20significant%20advancement%20in%0Ahuman-centered%20FL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19725v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520Without%2520Demographics%2520in%2520Human-Centered%2520Federated%2520Learning%26entry.906535625%3DShaily%2520Roy%2520and%2520Harshit%2520Sharma%2520and%2520Asif%2520Salekin%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520while%2520preserving%250Adata%2520privacy%252C%2520making%2520it%2520suitable%2520for%2520decentralized%2520human-centered%2520AI%250Aapplications.%2520However%252C%2520a%2520significant%2520research%2520gap%2520remains%2520in%2520ensuring%2520fairness%250Ain%2520these%2520systems.%2520Current%2520fairness%2520strategies%2520in%2520FL%2520require%2520knowledge%2520of%250Abias-creating/sensitive%2520attributes%252C%2520clashing%2520with%2520FL%2527s%2520privacy%2520principles.%250AMoreover%252C%2520in%2520human-centered%2520datasets%252C%2520sensitive%2520attributes%2520may%2520remain%2520latent.%250ATo%2520tackle%2520these%2520challenges%252C%2520we%2520present%2520a%2520novel%2520bias%2520mitigation%2520approach%250Ainspired%2520by%2520%2522Fairness%2520without%2520Demographics%2522%2520in%2520machine%2520learning.%2520The%2520presented%250Aapproach%2520achieves%2520fairness%2520without%2520needing%2520knowledge%2520of%2520sensitive%2520attributes%2520by%250Aminimizing%2520the%2520top%2520eigenvalue%2520of%2520the%2520Hessian%2520matrix%2520during%2520training%252C%2520ensuring%250Aequitable%2520loss%2520landscapes%2520across%2520FL%2520participants.%2520Notably%252C%2520we%2520introduce%2520a%2520novel%250AFL%2520aggregation%2520scheme%2520that%2520promotes%2520participating%2520models%2520based%2520on%2520error%2520rates%250Aand%2520loss%2520landscape%2520curvature%2520attributes%252C%2520fostering%2520fairness%2520across%2520the%2520FL%250Asystem.%2520This%2520work%2520represents%2520the%2520first%2520approach%2520to%2520attaining%2520%2522Fairness%2520without%250ADemographics%2522%2520in%2520human-centered%2520FL.%2520Through%2520comprehensive%2520evaluation%252C%2520our%250Aapproach%2520demonstrates%2520effectiveness%2520in%2520balancing%2520fairness%2520and%2520efficacy%2520across%250Avarious%2520real-world%2520applications%252C%2520FL%2520setups%252C%2520and%2520scenarios%2520involving%2520single%2520and%250Amultiple%2520bias-inducing%2520factors%252C%2520representing%2520a%2520significant%2520advancement%2520in%250Ahuman-centered%2520FL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19725v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Without%20Demographics%20in%20Human-Centered%20Federated%20Learning&entry.906535625=Shaily%20Roy%20and%20Harshit%20Sharma%20and%20Asif%20Salekin&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20while%20preserving%0Adata%20privacy%2C%20making%20it%20suitable%20for%20decentralized%20human-centered%20AI%0Aapplications.%20However%2C%20a%20significant%20research%20gap%20remains%20in%20ensuring%20fairness%0Ain%20these%20systems.%20Current%20fairness%20strategies%20in%20FL%20require%20knowledge%20of%0Abias-creating/sensitive%20attributes%2C%20clashing%20with%20FL%27s%20privacy%20principles.%0AMoreover%2C%20in%20human-centered%20datasets%2C%20sensitive%20attributes%20may%20remain%20latent.%0ATo%20tackle%20these%20challenges%2C%20we%20present%20a%20novel%20bias%20mitigation%20approach%0Ainspired%20by%20%22Fairness%20without%20Demographics%22%20in%20machine%20learning.%20The%20presented%0Aapproach%20achieves%20fairness%20without%20needing%20knowledge%20of%20sensitive%20attributes%20by%0Aminimizing%20the%20top%20eigenvalue%20of%20the%20Hessian%20matrix%20during%20training%2C%20ensuring%0Aequitable%20loss%20landscapes%20across%20FL%20participants.%20Notably%2C%20we%20introduce%20a%20novel%0AFL%20aggregation%20scheme%20that%20promotes%20participating%20models%20based%20on%20error%20rates%0Aand%20loss%20landscape%20curvature%20attributes%2C%20fostering%20fairness%20across%20the%20FL%0Asystem.%20This%20work%20represents%20the%20first%20approach%20to%20attaining%20%22Fairness%20without%0ADemographics%22%20in%20human-centered%20FL.%20Through%20comprehensive%20evaluation%2C%20our%0Aapproach%20demonstrates%20effectiveness%20in%20balancing%20fairness%20and%20efficacy%20across%0Avarious%20real-world%20applications%2C%20FL%20setups%2C%20and%20scenarios%20involving%20single%20and%0Amultiple%20bias-inducing%20factors%2C%20representing%20a%20significant%20advancement%20in%0Ahuman-centered%20FL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19725v2&entry.124074799=Read"},
{"title": "Federated Learning for Tabular Data using TabNet: A Vehicular Use-Case", "author": "William Lindskog and Christian Prehofer", "abstract": "  In this paper, we show how Federated Learning (FL) can be applied to\nvehicular use-cases in which we seek to classify obstacles, irregularities and\npavement types on roads. Our proposed framework utilizes FL and TabNet, a\nstate-of-the-art neural network for tabular data. We are the first to\ndemonstrate how TabNet can be integrated with FL. Moreover, we achieve a\nmaximum test accuracy of 93.6%. Finally, we reason why FL is a suitable concept\nfor this data set.\n", "link": "http://arxiv.org/abs/2405.02060v1", "date": "2024-05-03", "relevancy": 1.9536, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5209}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4863}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20for%20Tabular%20Data%20using%20TabNet%3A%20A%20Vehicular%20Use-Case&body=Title%3A%20Federated%20Learning%20for%20Tabular%20Data%20using%20TabNet%3A%20A%20Vehicular%20Use-Case%0AAuthor%3A%20William%20Lindskog%20and%20Christian%20Prehofer%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20show%20how%20Federated%20Learning%20%28FL%29%20can%20be%20applied%20to%0Avehicular%20use-cases%20in%20which%20we%20seek%20to%20classify%20obstacles%2C%20irregularities%20and%0Apavement%20types%20on%20roads.%20Our%20proposed%20framework%20utilizes%20FL%20and%20TabNet%2C%20a%0Astate-of-the-art%20neural%20network%20for%20tabular%20data.%20We%20are%20the%20first%20to%0Ademonstrate%20how%20TabNet%20can%20be%20integrated%20with%20FL.%20Moreover%2C%20we%20achieve%20a%0Amaximum%20test%20accuracy%20of%2093.6%25.%20Finally%2C%20we%20reason%20why%20FL%20is%20a%20suitable%20concept%0Afor%20this%20data%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520for%2520Tabular%2520Data%2520using%2520TabNet%253A%2520A%2520Vehicular%2520Use-Case%26entry.906535625%3DWilliam%2520Lindskog%2520and%2520Christian%2520Prehofer%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520show%2520how%2520Federated%2520Learning%2520%2528FL%2529%2520can%2520be%2520applied%2520to%250Avehicular%2520use-cases%2520in%2520which%2520we%2520seek%2520to%2520classify%2520obstacles%252C%2520irregularities%2520and%250Apavement%2520types%2520on%2520roads.%2520Our%2520proposed%2520framework%2520utilizes%2520FL%2520and%2520TabNet%252C%2520a%250Astate-of-the-art%2520neural%2520network%2520for%2520tabular%2520data.%2520We%2520are%2520the%2520first%2520to%250Ademonstrate%2520how%2520TabNet%2520can%2520be%2520integrated%2520with%2520FL.%2520Moreover%252C%2520we%2520achieve%2520a%250Amaximum%2520test%2520accuracy%2520of%252093.6%2525.%2520Finally%252C%2520we%2520reason%2520why%2520FL%2520is%2520a%2520suitable%2520concept%250Afor%2520this%2520data%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20for%20Tabular%20Data%20using%20TabNet%3A%20A%20Vehicular%20Use-Case&entry.906535625=William%20Lindskog%20and%20Christian%20Prehofer&entry.1292438233=%20%20In%20this%20paper%2C%20we%20show%20how%20Federated%20Learning%20%28FL%29%20can%20be%20applied%20to%0Avehicular%20use-cases%20in%20which%20we%20seek%20to%20classify%20obstacles%2C%20irregularities%20and%0Apavement%20types%20on%20roads.%20Our%20proposed%20framework%20utilizes%20FL%20and%20TabNet%2C%20a%0Astate-of-the-art%20neural%20network%20for%20tabular%20data.%20We%20are%20the%20first%20to%0Ademonstrate%20how%20TabNet%20can%20be%20integrated%20with%20FL.%20Moreover%2C%20we%20achieve%20a%0Amaximum%20test%20accuracy%20of%2093.6%25.%20Finally%2C%20we%20reason%20why%20FL%20is%20a%20suitable%20concept%0Afor%20this%20data%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02060v1&entry.124074799=Read"},
{"title": "Exploring Combinatorial Problem Solving with Large Language Models: A\n  Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo", "author": "Mahmoud Masoud and Ahmed Abdelhay and Mohammed Elhenawy", "abstract": "  Large Language Models (LLMs) are deep learning models designed to generate\ntext based on textual input. Although researchers have been developing these\nmodels for more complex tasks such as code generation and general reasoning,\nfew efforts have explored how LLMs can be applied to combinatorial problems. In\nthis research, we investigate the potential of LLMs to solve the Travelling\nSalesman Problem (TSP). Utilizing GPT-3.5 Turbo, we conducted experiments\nemploying various approaches, including zero-shot in-context learning, few-shot\nin-context learning, and chain-of-thoughts (CoT). Consequently, we fine-tuned\nGPT-3.5 Turbo to solve a specific problem size and tested it using a set of\nvarious instance sizes. The fine-tuned models demonstrated promising\nperformance on problems identical in size to the training instances and\ngeneralized well to larger problems. Furthermore, to improve the performance of\nthe fine-tuned model without incurring additional training costs, we adopted a\nself-ensemble approach to improve the quality of the solutions.\n", "link": "http://arxiv.org/abs/2405.01997v1", "date": "2024-05-03", "relevancy": 1.9372, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5086}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4688}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Combinatorial%20Problem%20Solving%20with%20Large%20Language%20Models%3A%20A%0A%20%20Case%20Study%20on%20the%20Travelling%20Salesman%20Problem%20Using%20GPT-3.5%20Turbo&body=Title%3A%20Exploring%20Combinatorial%20Problem%20Solving%20with%20Large%20Language%20Models%3A%20A%0A%20%20Case%20Study%20on%20the%20Travelling%20Salesman%20Problem%20Using%20GPT-3.5%20Turbo%0AAuthor%3A%20Mahmoud%20Masoud%20and%20Ahmed%20Abdelhay%20and%20Mohammed%20Elhenawy%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20deep%20learning%20models%20designed%20to%20generate%0Atext%20based%20on%20textual%20input.%20Although%20researchers%20have%20been%20developing%20these%0Amodels%20for%20more%20complex%20tasks%20such%20as%20code%20generation%20and%20general%20reasoning%2C%0Afew%20efforts%20have%20explored%20how%20LLMs%20can%20be%20applied%20to%20combinatorial%20problems.%20In%0Athis%20research%2C%20we%20investigate%20the%20potential%20of%20LLMs%20to%20solve%20the%20Travelling%0ASalesman%20Problem%20%28TSP%29.%20Utilizing%20GPT-3.5%20Turbo%2C%20we%20conducted%20experiments%0Aemploying%20various%20approaches%2C%20including%20zero-shot%20in-context%20learning%2C%20few-shot%0Ain-context%20learning%2C%20and%20chain-of-thoughts%20%28CoT%29.%20Consequently%2C%20we%20fine-tuned%0AGPT-3.5%20Turbo%20to%20solve%20a%20specific%20problem%20size%20and%20tested%20it%20using%20a%20set%20of%0Avarious%20instance%20sizes.%20The%20fine-tuned%20models%20demonstrated%20promising%0Aperformance%20on%20problems%20identical%20in%20size%20to%20the%20training%20instances%20and%0Ageneralized%20well%20to%20larger%20problems.%20Furthermore%2C%20to%20improve%20the%20performance%20of%0Athe%20fine-tuned%20model%20without%20incurring%20additional%20training%20costs%2C%20we%20adopted%20a%0Aself-ensemble%20approach%20to%20improve%20the%20quality%20of%20the%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Combinatorial%2520Problem%2520Solving%2520with%2520Large%2520Language%2520Models%253A%2520A%250A%2520%2520Case%2520Study%2520on%2520the%2520Travelling%2520Salesman%2520Problem%2520Using%2520GPT-3.5%2520Turbo%26entry.906535625%3DMahmoud%2520Masoud%2520and%2520Ahmed%2520Abdelhay%2520and%2520Mohammed%2520Elhenawy%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520deep%2520learning%2520models%2520designed%2520to%2520generate%250Atext%2520based%2520on%2520textual%2520input.%2520Although%2520researchers%2520have%2520been%2520developing%2520these%250Amodels%2520for%2520more%2520complex%2520tasks%2520such%2520as%2520code%2520generation%2520and%2520general%2520reasoning%252C%250Afew%2520efforts%2520have%2520explored%2520how%2520LLMs%2520can%2520be%2520applied%2520to%2520combinatorial%2520problems.%2520In%250Athis%2520research%252C%2520we%2520investigate%2520the%2520potential%2520of%2520LLMs%2520to%2520solve%2520the%2520Travelling%250ASalesman%2520Problem%2520%2528TSP%2529.%2520Utilizing%2520GPT-3.5%2520Turbo%252C%2520we%2520conducted%2520experiments%250Aemploying%2520various%2520approaches%252C%2520including%2520zero-shot%2520in-context%2520learning%252C%2520few-shot%250Ain-context%2520learning%252C%2520and%2520chain-of-thoughts%2520%2528CoT%2529.%2520Consequently%252C%2520we%2520fine-tuned%250AGPT-3.5%2520Turbo%2520to%2520solve%2520a%2520specific%2520problem%2520size%2520and%2520tested%2520it%2520using%2520a%2520set%2520of%250Avarious%2520instance%2520sizes.%2520The%2520fine-tuned%2520models%2520demonstrated%2520promising%250Aperformance%2520on%2520problems%2520identical%2520in%2520size%2520to%2520the%2520training%2520instances%2520and%250Ageneralized%2520well%2520to%2520larger%2520problems.%2520Furthermore%252C%2520to%2520improve%2520the%2520performance%2520of%250Athe%2520fine-tuned%2520model%2520without%2520incurring%2520additional%2520training%2520costs%252C%2520we%2520adopted%2520a%250Aself-ensemble%2520approach%2520to%2520improve%2520the%2520quality%2520of%2520the%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Combinatorial%20Problem%20Solving%20with%20Large%20Language%20Models%3A%20A%0A%20%20Case%20Study%20on%20the%20Travelling%20Salesman%20Problem%20Using%20GPT-3.5%20Turbo&entry.906535625=Mahmoud%20Masoud%20and%20Ahmed%20Abdelhay%20and%20Mohammed%20Elhenawy&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20deep%20learning%20models%20designed%20to%20generate%0Atext%20based%20on%20textual%20input.%20Although%20researchers%20have%20been%20developing%20these%0Amodels%20for%20more%20complex%20tasks%20such%20as%20code%20generation%20and%20general%20reasoning%2C%0Afew%20efforts%20have%20explored%20how%20LLMs%20can%20be%20applied%20to%20combinatorial%20problems.%20In%0Athis%20research%2C%20we%20investigate%20the%20potential%20of%20LLMs%20to%20solve%20the%20Travelling%0ASalesman%20Problem%20%28TSP%29.%20Utilizing%20GPT-3.5%20Turbo%2C%20we%20conducted%20experiments%0Aemploying%20various%20approaches%2C%20including%20zero-shot%20in-context%20learning%2C%20few-shot%0Ain-context%20learning%2C%20and%20chain-of-thoughts%20%28CoT%29.%20Consequently%2C%20we%20fine-tuned%0AGPT-3.5%20Turbo%20to%20solve%20a%20specific%20problem%20size%20and%20tested%20it%20using%20a%20set%20of%0Avarious%20instance%20sizes.%20The%20fine-tuned%20models%20demonstrated%20promising%0Aperformance%20on%20problems%20identical%20in%20size%20to%20the%20training%20instances%20and%0Ageneralized%20well%20to%20larger%20problems.%20Furthermore%2C%20to%20improve%20the%20performance%20of%0Athe%20fine-tuned%20model%20without%20incurring%20additional%20training%20costs%2C%20we%20adopted%20a%0Aself-ensemble%20approach%20to%20improve%20the%20quality%20of%20the%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01997v1&entry.124074799=Read"},
{"title": "Structural Pruning of Pre-trained Language Models via Neural\n  Architecture Search", "author": "Aaron Klein and Jacek Golebiowski and Xingchen Ma and Valerio Perrone and Cedric Archambeau", "abstract": "  Pre-trained language models (PLM), for example BERT or RoBERTa, mark the\nstate-of-the-art for natural language understanding task when fine-tuned on\nlabeled data. However, their large size poses challenges in deploying them for\ninference in real-world applications, due to significant GPU memory\nrequirements and high inference latency. This paper explores neural\narchitecture search (NAS) for structural pruning to find sub-parts of the\nfine-tuned network that optimally trade-off efficiency, for example in terms of\nmodel size or latency, and generalization performance. We also show how we can\nutilize more recently developed two-stage weight-sharing NAS approaches in this\nsetting to accelerate the search process. Unlike traditional pruning methods\nwith fixed thresholds, we propose to adopt a multi-objective approach that\nidentifies the Pareto optimal set of sub-networks, allowing for a more flexible\nand automated compression process.\n", "link": "http://arxiv.org/abs/2405.02267v1", "date": "2024-05-03", "relevancy": 1.935, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4934}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4852}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Pruning%20of%20Pre-trained%20Language%20Models%20via%20Neural%0A%20%20Architecture%20Search&body=Title%3A%20Structural%20Pruning%20of%20Pre-trained%20Language%20Models%20via%20Neural%0A%20%20Architecture%20Search%0AAuthor%3A%20Aaron%20Klein%20and%20Jacek%20Golebiowski%20and%20Xingchen%20Ma%20and%20Valerio%20Perrone%20and%20Cedric%20Archambeau%0AAbstract%3A%20%20%20Pre-trained%20language%20models%20%28PLM%29%2C%20for%20example%20BERT%20or%20RoBERTa%2C%20mark%20the%0Astate-of-the-art%20for%20natural%20language%20understanding%20task%20when%20fine-tuned%20on%0Alabeled%20data.%20However%2C%20their%20large%20size%20poses%20challenges%20in%20deploying%20them%20for%0Ainference%20in%20real-world%20applications%2C%20due%20to%20significant%20GPU%20memory%0Arequirements%20and%20high%20inference%20latency.%20This%20paper%20explores%20neural%0Aarchitecture%20search%20%28NAS%29%20for%20structural%20pruning%20to%20find%20sub-parts%20of%20the%0Afine-tuned%20network%20that%20optimally%20trade-off%20efficiency%2C%20for%20example%20in%20terms%20of%0Amodel%20size%20or%20latency%2C%20and%20generalization%20performance.%20We%20also%20show%20how%20we%20can%0Autilize%20more%20recently%20developed%20two-stage%20weight-sharing%20NAS%20approaches%20in%20this%0Asetting%20to%20accelerate%20the%20search%20process.%20Unlike%20traditional%20pruning%20methods%0Awith%20fixed%20thresholds%2C%20we%20propose%20to%20adopt%20a%20multi-objective%20approach%20that%0Aidentifies%20the%20Pareto%20optimal%20set%20of%20sub-networks%2C%20allowing%20for%20a%20more%20flexible%0Aand%20automated%20compression%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Pruning%2520of%2520Pre-trained%2520Language%2520Models%2520via%2520Neural%250A%2520%2520Architecture%2520Search%26entry.906535625%3DAaron%2520Klein%2520and%2520Jacek%2520Golebiowski%2520and%2520Xingchen%2520Ma%2520and%2520Valerio%2520Perrone%2520and%2520Cedric%2520Archambeau%26entry.1292438233%3D%2520%2520Pre-trained%2520language%2520models%2520%2528PLM%2529%252C%2520for%2520example%2520BERT%2520or%2520RoBERTa%252C%2520mark%2520the%250Astate-of-the-art%2520for%2520natural%2520language%2520understanding%2520task%2520when%2520fine-tuned%2520on%250Alabeled%2520data.%2520However%252C%2520their%2520large%2520size%2520poses%2520challenges%2520in%2520deploying%2520them%2520for%250Ainference%2520in%2520real-world%2520applications%252C%2520due%2520to%2520significant%2520GPU%2520memory%250Arequirements%2520and%2520high%2520inference%2520latency.%2520This%2520paper%2520explores%2520neural%250Aarchitecture%2520search%2520%2528NAS%2529%2520for%2520structural%2520pruning%2520to%2520find%2520sub-parts%2520of%2520the%250Afine-tuned%2520network%2520that%2520optimally%2520trade-off%2520efficiency%252C%2520for%2520example%2520in%2520terms%2520of%250Amodel%2520size%2520or%2520latency%252C%2520and%2520generalization%2520performance.%2520We%2520also%2520show%2520how%2520we%2520can%250Autilize%2520more%2520recently%2520developed%2520two-stage%2520weight-sharing%2520NAS%2520approaches%2520in%2520this%250Asetting%2520to%2520accelerate%2520the%2520search%2520process.%2520Unlike%2520traditional%2520pruning%2520methods%250Awith%2520fixed%2520thresholds%252C%2520we%2520propose%2520to%2520adopt%2520a%2520multi-objective%2520approach%2520that%250Aidentifies%2520the%2520Pareto%2520optimal%2520set%2520of%2520sub-networks%252C%2520allowing%2520for%2520a%2520more%2520flexible%250Aand%2520automated%2520compression%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Pruning%20of%20Pre-trained%20Language%20Models%20via%20Neural%0A%20%20Architecture%20Search&entry.906535625=Aaron%20Klein%20and%20Jacek%20Golebiowski%20and%20Xingchen%20Ma%20and%20Valerio%20Perrone%20and%20Cedric%20Archambeau&entry.1292438233=%20%20Pre-trained%20language%20models%20%28PLM%29%2C%20for%20example%20BERT%20or%20RoBERTa%2C%20mark%20the%0Astate-of-the-art%20for%20natural%20language%20understanding%20task%20when%20fine-tuned%20on%0Alabeled%20data.%20However%2C%20their%20large%20size%20poses%20challenges%20in%20deploying%20them%20for%0Ainference%20in%20real-world%20applications%2C%20due%20to%20significant%20GPU%20memory%0Arequirements%20and%20high%20inference%20latency.%20This%20paper%20explores%20neural%0Aarchitecture%20search%20%28NAS%29%20for%20structural%20pruning%20to%20find%20sub-parts%20of%20the%0Afine-tuned%20network%20that%20optimally%20trade-off%20efficiency%2C%20for%20example%20in%20terms%20of%0Amodel%20size%20or%20latency%2C%20and%20generalization%20performance.%20We%20also%20show%20how%20we%20can%0Autilize%20more%20recently%20developed%20two-stage%20weight-sharing%20NAS%20approaches%20in%20this%0Asetting%20to%20accelerate%20the%20search%20process.%20Unlike%20traditional%20pruning%20methods%0Awith%20fixed%20thresholds%2C%20we%20propose%20to%20adopt%20a%20multi-objective%20approach%20that%0Aidentifies%20the%20Pareto%20optimal%20set%20of%20sub-networks%2C%20allowing%20for%20a%20more%20flexible%0Aand%20automated%20compression%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02267v1&entry.124074799=Read"},
{"title": "Learning Optimal Deterministic Policies with Stochastic Policy Gradients", "author": "Alessandro Montenegro and Marco Mussi and Alberto Maria Metelli and Matteo Papini", "abstract": "  Policy gradient (PG) methods are successful approaches to deal with\ncontinuous reinforcement learning (RL) problems. They learn stochastic\nparametric (hyper)policies by either exploring in the space of actions or in\nthe space of parameters. Stochastic controllers, however, are often undesirable\nfrom a practical perspective because of their lack of robustness, safety, and\ntraceability. In common practice, stochastic (hyper)policies are learned only\nto deploy their deterministic version. In this paper, we make a step towards\nthe theoretical understanding of this practice. After introducing a novel\nframework for modeling this scenario, we study the global convergence to the\nbest deterministic policy, under (weak) gradient domination assumptions. Then,\nwe illustrate how to tune the exploration level used for learning to optimize\nthe trade-off between the sample complexity and the performance of the deployed\ndeterministic policy. Finally, we quantitatively compare action-based and\nparameter-based exploration, giving a formal guise to intuitive results.\n", "link": "http://arxiv.org/abs/2405.02235v1", "date": "2024-05-03", "relevancy": 1.929, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.496}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4872}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Optimal%20Deterministic%20Policies%20with%20Stochastic%20Policy%20Gradients&body=Title%3A%20Learning%20Optimal%20Deterministic%20Policies%20with%20Stochastic%20Policy%20Gradients%0AAuthor%3A%20Alessandro%20Montenegro%20and%20Marco%20Mussi%20and%20Alberto%20Maria%20Metelli%20and%20Matteo%20Papini%0AAbstract%3A%20%20%20Policy%20gradient%20%28PG%29%20methods%20are%20successful%20approaches%20to%20deal%20with%0Acontinuous%20reinforcement%20learning%20%28RL%29%20problems.%20They%20learn%20stochastic%0Aparametric%20%28hyper%29policies%20by%20either%20exploring%20in%20the%20space%20of%20actions%20or%20in%0Athe%20space%20of%20parameters.%20Stochastic%20controllers%2C%20however%2C%20are%20often%20undesirable%0Afrom%20a%20practical%20perspective%20because%20of%20their%20lack%20of%20robustness%2C%20safety%2C%20and%0Atraceability.%20In%20common%20practice%2C%20stochastic%20%28hyper%29policies%20are%20learned%20only%0Ato%20deploy%20their%20deterministic%20version.%20In%20this%20paper%2C%20we%20make%20a%20step%20towards%0Athe%20theoretical%20understanding%20of%20this%20practice.%20After%20introducing%20a%20novel%0Aframework%20for%20modeling%20this%20scenario%2C%20we%20study%20the%20global%20convergence%20to%20the%0Abest%20deterministic%20policy%2C%20under%20%28weak%29%20gradient%20domination%20assumptions.%20Then%2C%0Awe%20illustrate%20how%20to%20tune%20the%20exploration%20level%20used%20for%20learning%20to%20optimize%0Athe%20trade-off%20between%20the%20sample%20complexity%20and%20the%20performance%20of%20the%20deployed%0Adeterministic%20policy.%20Finally%2C%20we%20quantitatively%20compare%20action-based%20and%0Aparameter-based%20exploration%2C%20giving%20a%20formal%20guise%20to%20intuitive%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Optimal%2520Deterministic%2520Policies%2520with%2520Stochastic%2520Policy%2520Gradients%26entry.906535625%3DAlessandro%2520Montenegro%2520and%2520Marco%2520Mussi%2520and%2520Alberto%2520Maria%2520Metelli%2520and%2520Matteo%2520Papini%26entry.1292438233%3D%2520%2520Policy%2520gradient%2520%2528PG%2529%2520methods%2520are%2520successful%2520approaches%2520to%2520deal%2520with%250Acontinuous%2520reinforcement%2520learning%2520%2528RL%2529%2520problems.%2520They%2520learn%2520stochastic%250Aparametric%2520%2528hyper%2529policies%2520by%2520either%2520exploring%2520in%2520the%2520space%2520of%2520actions%2520or%2520in%250Athe%2520space%2520of%2520parameters.%2520Stochastic%2520controllers%252C%2520however%252C%2520are%2520often%2520undesirable%250Afrom%2520a%2520practical%2520perspective%2520because%2520of%2520their%2520lack%2520of%2520robustness%252C%2520safety%252C%2520and%250Atraceability.%2520In%2520common%2520practice%252C%2520stochastic%2520%2528hyper%2529policies%2520are%2520learned%2520only%250Ato%2520deploy%2520their%2520deterministic%2520version.%2520In%2520this%2520paper%252C%2520we%2520make%2520a%2520step%2520towards%250Athe%2520theoretical%2520understanding%2520of%2520this%2520practice.%2520After%2520introducing%2520a%2520novel%250Aframework%2520for%2520modeling%2520this%2520scenario%252C%2520we%2520study%2520the%2520global%2520convergence%2520to%2520the%250Abest%2520deterministic%2520policy%252C%2520under%2520%2528weak%2529%2520gradient%2520domination%2520assumptions.%2520Then%252C%250Awe%2520illustrate%2520how%2520to%2520tune%2520the%2520exploration%2520level%2520used%2520for%2520learning%2520to%2520optimize%250Athe%2520trade-off%2520between%2520the%2520sample%2520complexity%2520and%2520the%2520performance%2520of%2520the%2520deployed%250Adeterministic%2520policy.%2520Finally%252C%2520we%2520quantitatively%2520compare%2520action-based%2520and%250Aparameter-based%2520exploration%252C%2520giving%2520a%2520formal%2520guise%2520to%2520intuitive%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Optimal%20Deterministic%20Policies%20with%20Stochastic%20Policy%20Gradients&entry.906535625=Alessandro%20Montenegro%20and%20Marco%20Mussi%20and%20Alberto%20Maria%20Metelli%20and%20Matteo%20Papini&entry.1292438233=%20%20Policy%20gradient%20%28PG%29%20methods%20are%20successful%20approaches%20to%20deal%20with%0Acontinuous%20reinforcement%20learning%20%28RL%29%20problems.%20They%20learn%20stochastic%0Aparametric%20%28hyper%29policies%20by%20either%20exploring%20in%20the%20space%20of%20actions%20or%20in%0Athe%20space%20of%20parameters.%20Stochastic%20controllers%2C%20however%2C%20are%20often%20undesirable%0Afrom%20a%20practical%20perspective%20because%20of%20their%20lack%20of%20robustness%2C%20safety%2C%20and%0Atraceability.%20In%20common%20practice%2C%20stochastic%20%28hyper%29policies%20are%20learned%20only%0Ato%20deploy%20their%20deterministic%20version.%20In%20this%20paper%2C%20we%20make%20a%20step%20towards%0Athe%20theoretical%20understanding%20of%20this%20practice.%20After%20introducing%20a%20novel%0Aframework%20for%20modeling%20this%20scenario%2C%20we%20study%20the%20global%20convergence%20to%20the%0Abest%20deterministic%20policy%2C%20under%20%28weak%29%20gradient%20domination%20assumptions.%20Then%2C%0Awe%20illustrate%20how%20to%20tune%20the%20exploration%20level%20used%20for%20learning%20to%20optimize%0Athe%20trade-off%20between%20the%20sample%20complexity%20and%20the%20performance%20of%20the%20deployed%0Adeterministic%20policy.%20Finally%2C%20we%20quantitatively%20compare%20action-based%20and%0Aparameter-based%20exploration%2C%20giving%20a%20formal%20guise%20to%20intuitive%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02235v1&entry.124074799=Read"},
{"title": "Fisher Mask Nodes for Language Model Merging", "author": "Thennal D K and Ganesh Nathan and Suchithra M S", "abstract": "  Fine-tuning pre-trained models provides significant advantages in downstream\nperformance. The ubiquitous nature of pre-trained models such as BERT and its\nderivatives in natural language processing has also led to a proliferation of\ntask-specific fine-tuned models. As these models typically only perform one\ntask well, additional training or ensembling is required in multi-task\nscenarios. The growing field of model merging provides a solution, dealing with\nthe challenge of combining multiple task-specific models into a single\nmulti-task model. In this study, we introduce a novel model merging method for\nTransformers, combining insights from previous work in Fisher-weighted\naveraging and the use of Fisher information in model pruning. Utilizing the\nFisher information of mask nodes within the Transformer architecture, we devise\na computationally efficient weighted-averaging scheme. Our method exhibits a\nregular and significant performance increase across various models in the BERT\nfamily, outperforming full-scale Fisher-weighted averaging in a fraction of the\ncomputational cost, with baseline performance improvements of up to +6.5 and a\nspeedup between 57.4x and 321.7x across models. Our results prove the potential\nof our method in current multi-task learning environments and suggest its\nscalability and adaptability to new model architectures and learning scenarios.\n", "link": "http://arxiv.org/abs/2403.09891v3", "date": "2024-05-03", "relevancy": 1.9274, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fisher%20Mask%20Nodes%20for%20Language%20Model%20Merging&body=Title%3A%20Fisher%20Mask%20Nodes%20for%20Language%20Model%20Merging%0AAuthor%3A%20Thennal%20D%20K%20and%20Ganesh%20Nathan%20and%20Suchithra%20M%20S%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20models%20provides%20significant%20advantages%20in%20downstream%0Aperformance.%20The%20ubiquitous%20nature%20of%20pre-trained%20models%20such%20as%20BERT%20and%20its%0Aderivatives%20in%20natural%20language%20processing%20has%20also%20led%20to%20a%20proliferation%20of%0Atask-specific%20fine-tuned%20models.%20As%20these%20models%20typically%20only%20perform%20one%0Atask%20well%2C%20additional%20training%20or%20ensembling%20is%20required%20in%20multi-task%0Ascenarios.%20The%20growing%20field%20of%20model%20merging%20provides%20a%20solution%2C%20dealing%20with%0Athe%20challenge%20of%20combining%20multiple%20task-specific%20models%20into%20a%20single%0Amulti-task%20model.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20model%20merging%20method%20for%0ATransformers%2C%20combining%20insights%20from%20previous%20work%20in%20Fisher-weighted%0Aaveraging%20and%20the%20use%20of%20Fisher%20information%20in%20model%20pruning.%20Utilizing%20the%0AFisher%20information%20of%20mask%20nodes%20within%20the%20Transformer%20architecture%2C%20we%20devise%0Aa%20computationally%20efficient%20weighted-averaging%20scheme.%20Our%20method%20exhibits%20a%0Aregular%20and%20significant%20performance%20increase%20across%20various%20models%20in%20the%20BERT%0Afamily%2C%20outperforming%20full-scale%20Fisher-weighted%20averaging%20in%20a%20fraction%20of%20the%0Acomputational%20cost%2C%20with%20baseline%20performance%20improvements%20of%20up%20to%20%2B6.5%20and%20a%0Aspeedup%20between%2057.4x%20and%20321.7x%20across%20models.%20Our%20results%20prove%20the%20potential%0Aof%20our%20method%20in%20current%20multi-task%20learning%20environments%20and%20suggest%20its%0Ascalability%20and%20adaptability%20to%20new%20model%20architectures%20and%20learning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09891v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFisher%2520Mask%2520Nodes%2520for%2520Language%2520Model%2520Merging%26entry.906535625%3DThennal%2520D%2520K%2520and%2520Ganesh%2520Nathan%2520and%2520Suchithra%2520M%2520S%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520models%2520provides%2520significant%2520advantages%2520in%2520downstream%250Aperformance.%2520The%2520ubiquitous%2520nature%2520of%2520pre-trained%2520models%2520such%2520as%2520BERT%2520and%2520its%250Aderivatives%2520in%2520natural%2520language%2520processing%2520has%2520also%2520led%2520to%2520a%2520proliferation%2520of%250Atask-specific%2520fine-tuned%2520models.%2520As%2520these%2520models%2520typically%2520only%2520perform%2520one%250Atask%2520well%252C%2520additional%2520training%2520or%2520ensembling%2520is%2520required%2520in%2520multi-task%250Ascenarios.%2520The%2520growing%2520field%2520of%2520model%2520merging%2520provides%2520a%2520solution%252C%2520dealing%2520with%250Athe%2520challenge%2520of%2520combining%2520multiple%2520task-specific%2520models%2520into%2520a%2520single%250Amulti-task%2520model.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520model%2520merging%2520method%2520for%250ATransformers%252C%2520combining%2520insights%2520from%2520previous%2520work%2520in%2520Fisher-weighted%250Aaveraging%2520and%2520the%2520use%2520of%2520Fisher%2520information%2520in%2520model%2520pruning.%2520Utilizing%2520the%250AFisher%2520information%2520of%2520mask%2520nodes%2520within%2520the%2520Transformer%2520architecture%252C%2520we%2520devise%250Aa%2520computationally%2520efficient%2520weighted-averaging%2520scheme.%2520Our%2520method%2520exhibits%2520a%250Aregular%2520and%2520significant%2520performance%2520increase%2520across%2520various%2520models%2520in%2520the%2520BERT%250Afamily%252C%2520outperforming%2520full-scale%2520Fisher-weighted%2520averaging%2520in%2520a%2520fraction%2520of%2520the%250Acomputational%2520cost%252C%2520with%2520baseline%2520performance%2520improvements%2520of%2520up%2520to%2520%252B6.5%2520and%2520a%250Aspeedup%2520between%252057.4x%2520and%2520321.7x%2520across%2520models.%2520Our%2520results%2520prove%2520the%2520potential%250Aof%2520our%2520method%2520in%2520current%2520multi-task%2520learning%2520environments%2520and%2520suggest%2520its%250Ascalability%2520and%2520adaptability%2520to%2520new%2520model%2520architectures%2520and%2520learning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09891v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fisher%20Mask%20Nodes%20for%20Language%20Model%20Merging&entry.906535625=Thennal%20D%20K%20and%20Ganesh%20Nathan%20and%20Suchithra%20M%20S&entry.1292438233=%20%20Fine-tuning%20pre-trained%20models%20provides%20significant%20advantages%20in%20downstream%0Aperformance.%20The%20ubiquitous%20nature%20of%20pre-trained%20models%20such%20as%20BERT%20and%20its%0Aderivatives%20in%20natural%20language%20processing%20has%20also%20led%20to%20a%20proliferation%20of%0Atask-specific%20fine-tuned%20models.%20As%20these%20models%20typically%20only%20perform%20one%0Atask%20well%2C%20additional%20training%20or%20ensembling%20is%20required%20in%20multi-task%0Ascenarios.%20The%20growing%20field%20of%20model%20merging%20provides%20a%20solution%2C%20dealing%20with%0Athe%20challenge%20of%20combining%20multiple%20task-specific%20models%20into%20a%20single%0Amulti-task%20model.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20model%20merging%20method%20for%0ATransformers%2C%20combining%20insights%20from%20previous%20work%20in%20Fisher-weighted%0Aaveraging%20and%20the%20use%20of%20Fisher%20information%20in%20model%20pruning.%20Utilizing%20the%0AFisher%20information%20of%20mask%20nodes%20within%20the%20Transformer%20architecture%2C%20we%20devise%0Aa%20computationally%20efficient%20weighted-averaging%20scheme.%20Our%20method%20exhibits%20a%0Aregular%20and%20significant%20performance%20increase%20across%20various%20models%20in%20the%20BERT%0Afamily%2C%20outperforming%20full-scale%20Fisher-weighted%20averaging%20in%20a%20fraction%20of%20the%0Acomputational%20cost%2C%20with%20baseline%20performance%20improvements%20of%20up%20to%20%2B6.5%20and%20a%0Aspeedup%20between%2057.4x%20and%20321.7x%20across%20models.%20Our%20results%20prove%20the%20potential%0Aof%20our%20method%20in%20current%20multi-task%20learning%20environments%20and%20suggest%20its%0Ascalability%20and%20adaptability%20to%20new%20model%20architectures%20and%20learning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09891v3&entry.124074799=Read"},
{"title": "Zero-shot generalization across architectures for visual classification", "author": "Evan Gerritz and Luciano Dyballa and Steven W. Zucker", "abstract": "  Generalization to unseen data is a key desideratum for deep networks, but its\nrelation to classification accuracy is unclear. Using a minimalist vision\ndataset and a measure of generalizability, we show that popular networks, from\ndeep convolutional networks (CNNs) to transformers, vary in their power to\nextrapolate to unseen classes both across layers and across architectures.\nAccuracy is not a good predictor of generalizability, and generalization varies\nnon-monotonically with layer depth.\n", "link": "http://arxiv.org/abs/2402.14095v4", "date": "2024-05-03", "relevancy": 1.9181, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4835}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4828}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20generalization%20across%20architectures%20for%20visual%20classification&body=Title%3A%20Zero-shot%20generalization%20across%20architectures%20for%20visual%20classification%0AAuthor%3A%20Evan%20Gerritz%20and%20Luciano%20Dyballa%20and%20Steven%20W.%20Zucker%0AAbstract%3A%20%20%20Generalization%20to%20unseen%20data%20is%20a%20key%20desideratum%20for%20deep%20networks%2C%20but%20its%0Arelation%20to%20classification%20accuracy%20is%20unclear.%20Using%20a%20minimalist%20vision%0Adataset%20and%20a%20measure%20of%20generalizability%2C%20we%20show%20that%20popular%20networks%2C%20from%0Adeep%20convolutional%20networks%20%28CNNs%29%20to%20transformers%2C%20vary%20in%20their%20power%20to%0Aextrapolate%20to%20unseen%20classes%20both%20across%20layers%20and%20across%20architectures.%0AAccuracy%20is%20not%20a%20good%20predictor%20of%20generalizability%2C%20and%20generalization%20varies%0Anon-monotonically%20with%20layer%20depth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14095v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520generalization%2520across%2520architectures%2520for%2520visual%2520classification%26entry.906535625%3DEvan%2520Gerritz%2520and%2520Luciano%2520Dyballa%2520and%2520Steven%2520W.%2520Zucker%26entry.1292438233%3D%2520%2520Generalization%2520to%2520unseen%2520data%2520is%2520a%2520key%2520desideratum%2520for%2520deep%2520networks%252C%2520but%2520its%250Arelation%2520to%2520classification%2520accuracy%2520is%2520unclear.%2520Using%2520a%2520minimalist%2520vision%250Adataset%2520and%2520a%2520measure%2520of%2520generalizability%252C%2520we%2520show%2520that%2520popular%2520networks%252C%2520from%250Adeep%2520convolutional%2520networks%2520%2528CNNs%2529%2520to%2520transformers%252C%2520vary%2520in%2520their%2520power%2520to%250Aextrapolate%2520to%2520unseen%2520classes%2520both%2520across%2520layers%2520and%2520across%2520architectures.%250AAccuracy%2520is%2520not%2520a%2520good%2520predictor%2520of%2520generalizability%252C%2520and%2520generalization%2520varies%250Anon-monotonically%2520with%2520layer%2520depth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14095v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20generalization%20across%20architectures%20for%20visual%20classification&entry.906535625=Evan%20Gerritz%20and%20Luciano%20Dyballa%20and%20Steven%20W.%20Zucker&entry.1292438233=%20%20Generalization%20to%20unseen%20data%20is%20a%20key%20desideratum%20for%20deep%20networks%2C%20but%20its%0Arelation%20to%20classification%20accuracy%20is%20unclear.%20Using%20a%20minimalist%20vision%0Adataset%20and%20a%20measure%20of%20generalizability%2C%20we%20show%20that%20popular%20networks%2C%20from%0Adeep%20convolutional%20networks%20%28CNNs%29%20to%20transformers%2C%20vary%20in%20their%20power%20to%0Aextrapolate%20to%20unseen%20classes%20both%20across%20layers%20and%20across%20architectures.%0AAccuracy%20is%20not%20a%20good%20predictor%20of%20generalizability%2C%20and%20generalization%20varies%0Anon-monotonically%20with%20layer%20depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14095v4&entry.124074799=Read"},
{"title": "Multitask Extension of Geometrically Aligned Transfer Encoder", "author": "Sung Moon Ko and Sumin Lee and Dae-Woong Jeong and Hyunseung Kim and Chanhui Lee and Soorin Yim and Sehui Han", "abstract": "  Molecular datasets often suffer from a lack of data. It is well-known that\ngathering data is difficult due to the complexity of experimentation or\nsimulation involved. Here, we leverage mutual information across different\ntasks in molecular data to address this issue. We extend an algorithm that\nutilizes the geometric characteristics of the encoding space, known as the\nGeometrically Aligned Transfer Encoder (GATE), to a multi-task setup. Thus, we\nconnect multiple molecular tasks by aligning the curved coordinates onto\nlocally flat coordinates, ensuring the flow of information from source tasks to\nsupport performance on target data.\n", "link": "http://arxiv.org/abs/2405.01974v1", "date": "2024-05-03", "relevancy": 1.9096, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4694}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multitask%20Extension%20of%20Geometrically%20Aligned%20Transfer%20Encoder&body=Title%3A%20Multitask%20Extension%20of%20Geometrically%20Aligned%20Transfer%20Encoder%0AAuthor%3A%20Sung%20Moon%20Ko%20and%20Sumin%20Lee%20and%20Dae-Woong%20Jeong%20and%20Hyunseung%20Kim%20and%20Chanhui%20Lee%20and%20Soorin%20Yim%20and%20Sehui%20Han%0AAbstract%3A%20%20%20Molecular%20datasets%20often%20suffer%20from%20a%20lack%20of%20data.%20It%20is%20well-known%20that%0Agathering%20data%20is%20difficult%20due%20to%20the%20complexity%20of%20experimentation%20or%0Asimulation%20involved.%20Here%2C%20we%20leverage%20mutual%20information%20across%20different%0Atasks%20in%20molecular%20data%20to%20address%20this%20issue.%20We%20extend%20an%20algorithm%20that%0Autilizes%20the%20geometric%20characteristics%20of%20the%20encoding%20space%2C%20known%20as%20the%0AGeometrically%20Aligned%20Transfer%20Encoder%20%28GATE%29%2C%20to%20a%20multi-task%20setup.%20Thus%2C%20we%0Aconnect%20multiple%20molecular%20tasks%20by%20aligning%20the%20curved%20coordinates%20onto%0Alocally%20flat%20coordinates%2C%20ensuring%20the%20flow%20of%20information%20from%20source%20tasks%20to%0Asupport%20performance%20on%20target%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultitask%2520Extension%2520of%2520Geometrically%2520Aligned%2520Transfer%2520Encoder%26entry.906535625%3DSung%2520Moon%2520Ko%2520and%2520Sumin%2520Lee%2520and%2520Dae-Woong%2520Jeong%2520and%2520Hyunseung%2520Kim%2520and%2520Chanhui%2520Lee%2520and%2520Soorin%2520Yim%2520and%2520Sehui%2520Han%26entry.1292438233%3D%2520%2520Molecular%2520datasets%2520often%2520suffer%2520from%2520a%2520lack%2520of%2520data.%2520It%2520is%2520well-known%2520that%250Agathering%2520data%2520is%2520difficult%2520due%2520to%2520the%2520complexity%2520of%2520experimentation%2520or%250Asimulation%2520involved.%2520Here%252C%2520we%2520leverage%2520mutual%2520information%2520across%2520different%250Atasks%2520in%2520molecular%2520data%2520to%2520address%2520this%2520issue.%2520We%2520extend%2520an%2520algorithm%2520that%250Autilizes%2520the%2520geometric%2520characteristics%2520of%2520the%2520encoding%2520space%252C%2520known%2520as%2520the%250AGeometrically%2520Aligned%2520Transfer%2520Encoder%2520%2528GATE%2529%252C%2520to%2520a%2520multi-task%2520setup.%2520Thus%252C%2520we%250Aconnect%2520multiple%2520molecular%2520tasks%2520by%2520aligning%2520the%2520curved%2520coordinates%2520onto%250Alocally%2520flat%2520coordinates%252C%2520ensuring%2520the%2520flow%2520of%2520information%2520from%2520source%2520tasks%2520to%250Asupport%2520performance%2520on%2520target%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multitask%20Extension%20of%20Geometrically%20Aligned%20Transfer%20Encoder&entry.906535625=Sung%20Moon%20Ko%20and%20Sumin%20Lee%20and%20Dae-Woong%20Jeong%20and%20Hyunseung%20Kim%20and%20Chanhui%20Lee%20and%20Soorin%20Yim%20and%20Sehui%20Han&entry.1292438233=%20%20Molecular%20datasets%20often%20suffer%20from%20a%20lack%20of%20data.%20It%20is%20well-known%20that%0Agathering%20data%20is%20difficult%20due%20to%20the%20complexity%20of%20experimentation%20or%0Asimulation%20involved.%20Here%2C%20we%20leverage%20mutual%20information%20across%20different%0Atasks%20in%20molecular%20data%20to%20address%20this%20issue.%20We%20extend%20an%20algorithm%20that%0Autilizes%20the%20geometric%20characteristics%20of%20the%20encoding%20space%2C%20known%20as%20the%0AGeometrically%20Aligned%20Transfer%20Encoder%20%28GATE%29%2C%20to%20a%20multi-task%20setup.%20Thus%2C%20we%0Aconnect%20multiple%20molecular%20tasks%20by%20aligning%20the%20curved%20coordinates%20onto%0Alocally%20flat%20coordinates%2C%20ensuring%20the%20flow%20of%20information%20from%20source%20tasks%20to%0Asupport%20performance%20on%20target%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01974v1&entry.124074799=Read"},
{"title": "Can We Identify Unknown Audio Recording Environments in Forensic\n  Scenarios?", "author": "Denise Moussa and Germans Hirsch and Christian Riess", "abstract": "  Audio recordings may provide important evidence in criminal investigations.\nOne such case is the forensic association of the recorded audio to the\nrecording location. For example, a voice message may be the only investigative\ncue to narrow down the candidate sites for a crime. Up to now, several works\nprovide tools for closed-set recording environment classification under\nrelatively clean recording conditions. However, in forensic investigations, the\ncandidate locations are case-specific. Thus, closed-set tools are not\napplicable without retraining on a sufficient amount of training samples for\neach case and respective candidate set. In addition, a forensic tool has to\ndeal with audio material from uncontrolled sources with variable properties and\nquality.\n  In this work, we therefore attempt a major step towards practical forensic\napplication scenarios. We propose a representation learning framework called\nEnvId, short for environment identification. EnvId avoids case-specific\nretraining. Instead, it is the first tool for robust few-shot classification of\nunseen environment locations. We demonstrate that EnvId can handle forensically\nchallenging material. It provides good quality predictions even under unseen\nsignal degradations, environment characteristics or recording position\nmismatches.\n  Our code and datasets will be made publicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2405.02119v1", "date": "2024-05-03", "relevancy": 1.9071, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4812}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4792}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Identify%20Unknown%20Audio%20Recording%20Environments%20in%20Forensic%0A%20%20Scenarios%3F&body=Title%3A%20Can%20We%20Identify%20Unknown%20Audio%20Recording%20Environments%20in%20Forensic%0A%20%20Scenarios%3F%0AAuthor%3A%20Denise%20Moussa%20and%20Germans%20Hirsch%20and%20Christian%20Riess%0AAbstract%3A%20%20%20Audio%20recordings%20may%20provide%20important%20evidence%20in%20criminal%20investigations.%0AOne%20such%20case%20is%20the%20forensic%20association%20of%20the%20recorded%20audio%20to%20the%0Arecording%20location.%20For%20example%2C%20a%20voice%20message%20may%20be%20the%20only%20investigative%0Acue%20to%20narrow%20down%20the%20candidate%20sites%20for%20a%20crime.%20Up%20to%20now%2C%20several%20works%0Aprovide%20tools%20for%20closed-set%20recording%20environment%20classification%20under%0Arelatively%20clean%20recording%20conditions.%20However%2C%20in%20forensic%20investigations%2C%20the%0Acandidate%20locations%20are%20case-specific.%20Thus%2C%20closed-set%20tools%20are%20not%0Aapplicable%20without%20retraining%20on%20a%20sufficient%20amount%20of%20training%20samples%20for%0Aeach%20case%20and%20respective%20candidate%20set.%20In%20addition%2C%20a%20forensic%20tool%20has%20to%0Adeal%20with%20audio%20material%20from%20uncontrolled%20sources%20with%20variable%20properties%20and%0Aquality.%0A%20%20In%20this%20work%2C%20we%20therefore%20attempt%20a%20major%20step%20towards%20practical%20forensic%0Aapplication%20scenarios.%20We%20propose%20a%20representation%20learning%20framework%20called%0AEnvId%2C%20short%20for%20environment%20identification.%20EnvId%20avoids%20case-specific%0Aretraining.%20Instead%2C%20it%20is%20the%20first%20tool%20for%20robust%20few-shot%20classification%20of%0Aunseen%20environment%20locations.%20We%20demonstrate%20that%20EnvId%20can%20handle%20forensically%0Achallenging%20material.%20It%20provides%20good%20quality%20predictions%20even%20under%20unseen%0Asignal%20degradations%2C%20environment%20characteristics%20or%20recording%20position%0Amismatches.%0A%20%20Our%20code%20and%20datasets%20will%20be%20made%20publicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520We%2520Identify%2520Unknown%2520Audio%2520Recording%2520Environments%2520in%2520Forensic%250A%2520%2520Scenarios%253F%26entry.906535625%3DDenise%2520Moussa%2520and%2520Germans%2520Hirsch%2520and%2520Christian%2520Riess%26entry.1292438233%3D%2520%2520Audio%2520recordings%2520may%2520provide%2520important%2520evidence%2520in%2520criminal%2520investigations.%250AOne%2520such%2520case%2520is%2520the%2520forensic%2520association%2520of%2520the%2520recorded%2520audio%2520to%2520the%250Arecording%2520location.%2520For%2520example%252C%2520a%2520voice%2520message%2520may%2520be%2520the%2520only%2520investigative%250Acue%2520to%2520narrow%2520down%2520the%2520candidate%2520sites%2520for%2520a%2520crime.%2520Up%2520to%2520now%252C%2520several%2520works%250Aprovide%2520tools%2520for%2520closed-set%2520recording%2520environment%2520classification%2520under%250Arelatively%2520clean%2520recording%2520conditions.%2520However%252C%2520in%2520forensic%2520investigations%252C%2520the%250Acandidate%2520locations%2520are%2520case-specific.%2520Thus%252C%2520closed-set%2520tools%2520are%2520not%250Aapplicable%2520without%2520retraining%2520on%2520a%2520sufficient%2520amount%2520of%2520training%2520samples%2520for%250Aeach%2520case%2520and%2520respective%2520candidate%2520set.%2520In%2520addition%252C%2520a%2520forensic%2520tool%2520has%2520to%250Adeal%2520with%2520audio%2520material%2520from%2520uncontrolled%2520sources%2520with%2520variable%2520properties%2520and%250Aquality.%250A%2520%2520In%2520this%2520work%252C%2520we%2520therefore%2520attempt%2520a%2520major%2520step%2520towards%2520practical%2520forensic%250Aapplication%2520scenarios.%2520We%2520propose%2520a%2520representation%2520learning%2520framework%2520called%250AEnvId%252C%2520short%2520for%2520environment%2520identification.%2520EnvId%2520avoids%2520case-specific%250Aretraining.%2520Instead%252C%2520it%2520is%2520the%2520first%2520tool%2520for%2520robust%2520few-shot%2520classification%2520of%250Aunseen%2520environment%2520locations.%2520We%2520demonstrate%2520that%2520EnvId%2520can%2520handle%2520forensically%250Achallenging%2520material.%2520It%2520provides%2520good%2520quality%2520predictions%2520even%2520under%2520unseen%250Asignal%2520degradations%252C%2520environment%2520characteristics%2520or%2520recording%2520position%250Amismatches.%250A%2520%2520Our%2520code%2520and%2520datasets%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Identify%20Unknown%20Audio%20Recording%20Environments%20in%20Forensic%0A%20%20Scenarios%3F&entry.906535625=Denise%20Moussa%20and%20Germans%20Hirsch%20and%20Christian%20Riess&entry.1292438233=%20%20Audio%20recordings%20may%20provide%20important%20evidence%20in%20criminal%20investigations.%0AOne%20such%20case%20is%20the%20forensic%20association%20of%20the%20recorded%20audio%20to%20the%0Arecording%20location.%20For%20example%2C%20a%20voice%20message%20may%20be%20the%20only%20investigative%0Acue%20to%20narrow%20down%20the%20candidate%20sites%20for%20a%20crime.%20Up%20to%20now%2C%20several%20works%0Aprovide%20tools%20for%20closed-set%20recording%20environment%20classification%20under%0Arelatively%20clean%20recording%20conditions.%20However%2C%20in%20forensic%20investigations%2C%20the%0Acandidate%20locations%20are%20case-specific.%20Thus%2C%20closed-set%20tools%20are%20not%0Aapplicable%20without%20retraining%20on%20a%20sufficient%20amount%20of%20training%20samples%20for%0Aeach%20case%20and%20respective%20candidate%20set.%20In%20addition%2C%20a%20forensic%20tool%20has%20to%0Adeal%20with%20audio%20material%20from%20uncontrolled%20sources%20with%20variable%20properties%20and%0Aquality.%0A%20%20In%20this%20work%2C%20we%20therefore%20attempt%20a%20major%20step%20towards%20practical%20forensic%0Aapplication%20scenarios.%20We%20propose%20a%20representation%20learning%20framework%20called%0AEnvId%2C%20short%20for%20environment%20identification.%20EnvId%20avoids%20case-specific%0Aretraining.%20Instead%2C%20it%20is%20the%20first%20tool%20for%20robust%20few-shot%20classification%20of%0Aunseen%20environment%20locations.%20We%20demonstrate%20that%20EnvId%20can%20handle%20forensically%0Achallenging%20material.%20It%20provides%20good%20quality%20predictions%20even%20under%20unseen%0Asignal%20degradations%2C%20environment%20characteristics%20or%20recording%20position%0Amismatches.%0A%20%20Our%20code%20and%20datasets%20will%20be%20made%20publicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02119v1&entry.124074799=Read"},
{"title": "Privately Aligning Language Models with Reinforcement Learning", "author": "Fan Wu and Huseyin A. Inan and Arturs Backurs and Varun Chandrasekaran and Janardhan Kulkarni and Robert Sim", "abstract": "  Positioned between pre-training and user deployment, aligning large language\nmodels (LLMs) through reinforcement learning (RL) has emerged as a prevailing\nstrategy for training instruction following-models such as ChatGPT. In this\nwork, we initiate the study of privacy-preserving alignment of LLMs through\nDifferential Privacy (DP) in conjunction with RL. Following the influential\nwork of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment\nvia RL without human in the loop (e.g., positive review generation) and (ii)\nalignment via RL from human feedback (RLHF) (e.g., summarization in a\nhuman-preferred way). We give a new DP framework to achieve alignment via RL,\nand prove its correctness. Our experimental results validate the effectiveness\nof our approach, offering competitive utility while ensuring strong privacy\nprotections.\n", "link": "http://arxiv.org/abs/2310.16960v2", "date": "2024-05-03", "relevancy": 1.9068, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.491}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4886}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privately%20Aligning%20Language%20Models%20with%20Reinforcement%20Learning&body=Title%3A%20Privately%20Aligning%20Language%20Models%20with%20Reinforcement%20Learning%0AAuthor%3A%20Fan%20Wu%20and%20Huseyin%20A.%20Inan%20and%20Arturs%20Backurs%20and%20Varun%20Chandrasekaran%20and%20Janardhan%20Kulkarni%20and%20Robert%20Sim%0AAbstract%3A%20%20%20Positioned%20between%20pre-training%20and%20user%20deployment%2C%20aligning%20large%20language%0Amodels%20%28LLMs%29%20through%20reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20prevailing%0Astrategy%20for%20training%20instruction%20following-models%20such%20as%20ChatGPT.%20In%20this%0Awork%2C%20we%20initiate%20the%20study%20of%20privacy-preserving%20alignment%20of%20LLMs%20through%0ADifferential%20Privacy%20%28DP%29%20in%20conjunction%20with%20RL.%20Following%20the%20influential%0Awork%20of%20Ziegler%20et%20al.%20%282020%29%2C%20we%20study%20two%20dominant%20paradigms%3A%20%28i%29%20alignment%0Avia%20RL%20without%20human%20in%20the%20loop%20%28e.g.%2C%20positive%20review%20generation%29%20and%20%28ii%29%0Aalignment%20via%20RL%20from%20human%20feedback%20%28RLHF%29%20%28e.g.%2C%20summarization%20in%20a%0Ahuman-preferred%20way%29.%20We%20give%20a%20new%20DP%20framework%20to%20achieve%20alignment%20via%20RL%2C%0Aand%20prove%20its%20correctness.%20Our%20experimental%20results%20validate%20the%20effectiveness%0Aof%20our%20approach%2C%20offering%20competitive%20utility%20while%20ensuring%20strong%20privacy%0Aprotections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivately%2520Aligning%2520Language%2520Models%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DFan%2520Wu%2520and%2520Huseyin%2520A.%2520Inan%2520and%2520Arturs%2520Backurs%2520and%2520Varun%2520Chandrasekaran%2520and%2520Janardhan%2520Kulkarni%2520and%2520Robert%2520Sim%26entry.1292438233%3D%2520%2520Positioned%2520between%2520pre-training%2520and%2520user%2520deployment%252C%2520aligning%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520through%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520prevailing%250Astrategy%2520for%2520training%2520instruction%2520following-models%2520such%2520as%2520ChatGPT.%2520In%2520this%250Awork%252C%2520we%2520initiate%2520the%2520study%2520of%2520privacy-preserving%2520alignment%2520of%2520LLMs%2520through%250ADifferential%2520Privacy%2520%2528DP%2529%2520in%2520conjunction%2520with%2520RL.%2520Following%2520the%2520influential%250Awork%2520of%2520Ziegler%2520et%2520al.%2520%25282020%2529%252C%2520we%2520study%2520two%2520dominant%2520paradigms%253A%2520%2528i%2529%2520alignment%250Avia%2520RL%2520without%2520human%2520in%2520the%2520loop%2520%2528e.g.%252C%2520positive%2520review%2520generation%2529%2520and%2520%2528ii%2529%250Aalignment%2520via%2520RL%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520%2528e.g.%252C%2520summarization%2520in%2520a%250Ahuman-preferred%2520way%2529.%2520We%2520give%2520a%2520new%2520DP%2520framework%2520to%2520achieve%2520alignment%2520via%2520RL%252C%250Aand%2520prove%2520its%2520correctness.%2520Our%2520experimental%2520results%2520validate%2520the%2520effectiveness%250Aof%2520our%2520approach%252C%2520offering%2520competitive%2520utility%2520while%2520ensuring%2520strong%2520privacy%250Aprotections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privately%20Aligning%20Language%20Models%20with%20Reinforcement%20Learning&entry.906535625=Fan%20Wu%20and%20Huseyin%20A.%20Inan%20and%20Arturs%20Backurs%20and%20Varun%20Chandrasekaran%20and%20Janardhan%20Kulkarni%20and%20Robert%20Sim&entry.1292438233=%20%20Positioned%20between%20pre-training%20and%20user%20deployment%2C%20aligning%20large%20language%0Amodels%20%28LLMs%29%20through%20reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20prevailing%0Astrategy%20for%20training%20instruction%20following-models%20such%20as%20ChatGPT.%20In%20this%0Awork%2C%20we%20initiate%20the%20study%20of%20privacy-preserving%20alignment%20of%20LLMs%20through%0ADifferential%20Privacy%20%28DP%29%20in%20conjunction%20with%20RL.%20Following%20the%20influential%0Awork%20of%20Ziegler%20et%20al.%20%282020%29%2C%20we%20study%20two%20dominant%20paradigms%3A%20%28i%29%20alignment%0Avia%20RL%20without%20human%20in%20the%20loop%20%28e.g.%2C%20positive%20review%20generation%29%20and%20%28ii%29%0Aalignment%20via%20RL%20from%20human%20feedback%20%28RLHF%29%20%28e.g.%2C%20summarization%20in%20a%0Ahuman-preferred%20way%29.%20We%20give%20a%20new%20DP%20framework%20to%20achieve%20alignment%20via%20RL%2C%0Aand%20prove%20its%20correctness.%20Our%20experimental%20results%20validate%20the%20effectiveness%0Aof%20our%20approach%2C%20offering%20competitive%20utility%20while%20ensuring%20strong%20privacy%0Aprotections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16960v2&entry.124074799=Read"},
{"title": "Compressing neural network by tensor network with exponentially fewer\n  variational parameters", "author": "Yong Qing and Ke Li and Peng-Fei Zhou and Shi-Ju Ran", "abstract": "  Neural network (NN) designed for challenging machine learning tasks is in\ngeneral a highly nonlinear mapping that contains massive variational\nparameters. High complexity of NN, if unbounded or unconstrained, might\nunpredictably cause severe issues including over-fitting, loss of\ngeneralization power, and unbearable cost of hardware. In this work, we propose\na general compression scheme that significantly reduces the variational\nparameters of NN by encoding them to deep automatically-differentiable tensor\nnetwork (ADTN) that contains exponentially-fewer free parameters. Superior\ncompression performance of our scheme is demonstrated on several\nwidely-recognized NN's (FC-2, LeNet-5, AlextNet, ZFNet and VGG-16) and datasets\n(MNIST, CIFAR-10 and CIFAR-100). For instance, we compress two linear layers in\nVGG-16 with approximately $10^{7}$ parameters to two ADTN's with just 424\nparameters, where the testing accuracy on CIFAR-10 is improved from $90.17 \\%$\nto $91.74\\%$. Our work suggests TN as an exceptionally efficient mathematical\nstructure for representing the variational parameters of NN's, which exhibits\nsuperior compressibility over the commonly-used matrices and multi-way arrays.\n", "link": "http://arxiv.org/abs/2305.06058v2", "date": "2024-05-03", "relevancy": 1.9063, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4983}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4642}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressing%20neural%20network%20by%20tensor%20network%20with%20exponentially%20fewer%0A%20%20variational%20parameters&body=Title%3A%20Compressing%20neural%20network%20by%20tensor%20network%20with%20exponentially%20fewer%0A%20%20variational%20parameters%0AAuthor%3A%20Yong%20Qing%20and%20Ke%20Li%20and%20Peng-Fei%20Zhou%20and%20Shi-Ju%20Ran%0AAbstract%3A%20%20%20Neural%20network%20%28NN%29%20designed%20for%20challenging%20machine%20learning%20tasks%20is%20in%0Ageneral%20a%20highly%20nonlinear%20mapping%20that%20contains%20massive%20variational%0Aparameters.%20High%20complexity%20of%20NN%2C%20if%20unbounded%20or%20unconstrained%2C%20might%0Aunpredictably%20cause%20severe%20issues%20including%20over-fitting%2C%20loss%20of%0Ageneralization%20power%2C%20and%20unbearable%20cost%20of%20hardware.%20In%20this%20work%2C%20we%20propose%0Aa%20general%20compression%20scheme%20that%20significantly%20reduces%20the%20variational%0Aparameters%20of%20NN%20by%20encoding%20them%20to%20deep%20automatically-differentiable%20tensor%0Anetwork%20%28ADTN%29%20that%20contains%20exponentially-fewer%20free%20parameters.%20Superior%0Acompression%20performance%20of%20our%20scheme%20is%20demonstrated%20on%20several%0Awidely-recognized%20NN%27s%20%28FC-2%2C%20LeNet-5%2C%20AlextNet%2C%20ZFNet%20and%20VGG-16%29%20and%20datasets%0A%28MNIST%2C%20CIFAR-10%20and%20CIFAR-100%29.%20For%20instance%2C%20we%20compress%20two%20linear%20layers%20in%0AVGG-16%20with%20approximately%20%2410%5E%7B7%7D%24%20parameters%20to%20two%20ADTN%27s%20with%20just%20424%0Aparameters%2C%20where%20the%20testing%20accuracy%20on%20CIFAR-10%20is%20improved%20from%20%2490.17%20%5C%25%24%0Ato%20%2491.74%5C%25%24.%20Our%20work%20suggests%20TN%20as%20an%20exceptionally%20efficient%20mathematical%0Astructure%20for%20representing%20the%20variational%20parameters%20of%20NN%27s%2C%20which%20exhibits%0Asuperior%20compressibility%20over%20the%20commonly-used%20matrices%20and%20multi-way%20arrays.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.06058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressing%2520neural%2520network%2520by%2520tensor%2520network%2520with%2520exponentially%2520fewer%250A%2520%2520variational%2520parameters%26entry.906535625%3DYong%2520Qing%2520and%2520Ke%2520Li%2520and%2520Peng-Fei%2520Zhou%2520and%2520Shi-Ju%2520Ran%26entry.1292438233%3D%2520%2520Neural%2520network%2520%2528NN%2529%2520designed%2520for%2520challenging%2520machine%2520learning%2520tasks%2520is%2520in%250Ageneral%2520a%2520highly%2520nonlinear%2520mapping%2520that%2520contains%2520massive%2520variational%250Aparameters.%2520High%2520complexity%2520of%2520NN%252C%2520if%2520unbounded%2520or%2520unconstrained%252C%2520might%250Aunpredictably%2520cause%2520severe%2520issues%2520including%2520over-fitting%252C%2520loss%2520of%250Ageneralization%2520power%252C%2520and%2520unbearable%2520cost%2520of%2520hardware.%2520In%2520this%2520work%252C%2520we%2520propose%250Aa%2520general%2520compression%2520scheme%2520that%2520significantly%2520reduces%2520the%2520variational%250Aparameters%2520of%2520NN%2520by%2520encoding%2520them%2520to%2520deep%2520automatically-differentiable%2520tensor%250Anetwork%2520%2528ADTN%2529%2520that%2520contains%2520exponentially-fewer%2520free%2520parameters.%2520Superior%250Acompression%2520performance%2520of%2520our%2520scheme%2520is%2520demonstrated%2520on%2520several%250Awidely-recognized%2520NN%2527s%2520%2528FC-2%252C%2520LeNet-5%252C%2520AlextNet%252C%2520ZFNet%2520and%2520VGG-16%2529%2520and%2520datasets%250A%2528MNIST%252C%2520CIFAR-10%2520and%2520CIFAR-100%2529.%2520For%2520instance%252C%2520we%2520compress%2520two%2520linear%2520layers%2520in%250AVGG-16%2520with%2520approximately%2520%252410%255E%257B7%257D%2524%2520parameters%2520to%2520two%2520ADTN%2527s%2520with%2520just%2520424%250Aparameters%252C%2520where%2520the%2520testing%2520accuracy%2520on%2520CIFAR-10%2520is%2520improved%2520from%2520%252490.17%2520%255C%2525%2524%250Ato%2520%252491.74%255C%2525%2524.%2520Our%2520work%2520suggests%2520TN%2520as%2520an%2520exceptionally%2520efficient%2520mathematical%250Astructure%2520for%2520representing%2520the%2520variational%2520parameters%2520of%2520NN%2527s%252C%2520which%2520exhibits%250Asuperior%2520compressibility%2520over%2520the%2520commonly-used%2520matrices%2520and%2520multi-way%2520arrays.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.06058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressing%20neural%20network%20by%20tensor%20network%20with%20exponentially%20fewer%0A%20%20variational%20parameters&entry.906535625=Yong%20Qing%20and%20Ke%20Li%20and%20Peng-Fei%20Zhou%20and%20Shi-Ju%20Ran&entry.1292438233=%20%20Neural%20network%20%28NN%29%20designed%20for%20challenging%20machine%20learning%20tasks%20is%20in%0Ageneral%20a%20highly%20nonlinear%20mapping%20that%20contains%20massive%20variational%0Aparameters.%20High%20complexity%20of%20NN%2C%20if%20unbounded%20or%20unconstrained%2C%20might%0Aunpredictably%20cause%20severe%20issues%20including%20over-fitting%2C%20loss%20of%0Ageneralization%20power%2C%20and%20unbearable%20cost%20of%20hardware.%20In%20this%20work%2C%20we%20propose%0Aa%20general%20compression%20scheme%20that%20significantly%20reduces%20the%20variational%0Aparameters%20of%20NN%20by%20encoding%20them%20to%20deep%20automatically-differentiable%20tensor%0Anetwork%20%28ADTN%29%20that%20contains%20exponentially-fewer%20free%20parameters.%20Superior%0Acompression%20performance%20of%20our%20scheme%20is%20demonstrated%20on%20several%0Awidely-recognized%20NN%27s%20%28FC-2%2C%20LeNet-5%2C%20AlextNet%2C%20ZFNet%20and%20VGG-16%29%20and%20datasets%0A%28MNIST%2C%20CIFAR-10%20and%20CIFAR-100%29.%20For%20instance%2C%20we%20compress%20two%20linear%20layers%20in%0AVGG-16%20with%20approximately%20%2410%5E%7B7%7D%24%20parameters%20to%20two%20ADTN%27s%20with%20just%20424%0Aparameters%2C%20where%20the%20testing%20accuracy%20on%20CIFAR-10%20is%20improved%20from%20%2490.17%20%5C%25%24%0Ato%20%2491.74%5C%25%24.%20Our%20work%20suggests%20TN%20as%20an%20exceptionally%20efficient%20mathematical%0Astructure%20for%20representing%20the%20variational%20parameters%20of%20NN%27s%2C%20which%20exhibits%0Asuperior%20compressibility%20over%20the%20commonly-used%20matrices%20and%20multi-way%20arrays.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.06058v2&entry.124074799=Read"},
{"title": "Towards Unconstrained Audio Splicing Detection and Localization with\n  Neural Networks", "author": "Denise Moussa and Germans Hirsch and Christian Riess", "abstract": "  Freely available and easy-to-use audio editing tools make it straightforward\nto perform audio splicing. Convincing forgeries can be created by combining\nvarious speech samples from the same person. Detection of such splices is\nimportant both in the public sector when considering misinformation, and in a\nlegal context to verify the integrity of evidence. Unfortunately, most existing\ndetection algorithms for audio splicing use handcrafted features and make\nspecific assumptions. However, criminal investigators are often faced with\naudio samples from unconstrained sources with unknown characteristics, which\nraises the need for more generally applicable methods.\n  With this work, we aim to take a first step towards unconstrained audio\nsplicing detection to address this need. We simulate various attack scenarios\nin the form of post-processing operations that may disguise splicing. We\npropose a Transformer sequence-to-sequence (seq2seq) network for splicing\ndetection and localization. Our extensive evaluation shows that the proposed\nmethod outperforms existing dedicated approaches for splicing detection [3, 10]\nas well as the general-purpose networks EfficientNet [28] and RegNet [25].\n", "link": "http://arxiv.org/abs/2207.14682v4", "date": "2024-05-03", "relevancy": 1.9049, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4677}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unconstrained%20Audio%20Splicing%20Detection%20and%20Localization%20with%0A%20%20Neural%20Networks&body=Title%3A%20Towards%20Unconstrained%20Audio%20Splicing%20Detection%20and%20Localization%20with%0A%20%20Neural%20Networks%0AAuthor%3A%20Denise%20Moussa%20and%20Germans%20Hirsch%20and%20Christian%20Riess%0AAbstract%3A%20%20%20Freely%20available%20and%20easy-to-use%20audio%20editing%20tools%20make%20it%20straightforward%0Ato%20perform%20audio%20splicing.%20Convincing%20forgeries%20can%20be%20created%20by%20combining%0Avarious%20speech%20samples%20from%20the%20same%20person.%20Detection%20of%20such%20splices%20is%0Aimportant%20both%20in%20the%20public%20sector%20when%20considering%20misinformation%2C%20and%20in%20a%0Alegal%20context%20to%20verify%20the%20integrity%20of%20evidence.%20Unfortunately%2C%20most%20existing%0Adetection%20algorithms%20for%20audio%20splicing%20use%20handcrafted%20features%20and%20make%0Aspecific%20assumptions.%20However%2C%20criminal%20investigators%20are%20often%20faced%20with%0Aaudio%20samples%20from%20unconstrained%20sources%20with%20unknown%20characteristics%2C%20which%0Araises%20the%20need%20for%20more%20generally%20applicable%20methods.%0A%20%20With%20this%20work%2C%20we%20aim%20to%20take%20a%20first%20step%20towards%20unconstrained%20audio%0Asplicing%20detection%20to%20address%20this%20need.%20We%20simulate%20various%20attack%20scenarios%0Ain%20the%20form%20of%20post-processing%20operations%20that%20may%20disguise%20splicing.%20We%0Apropose%20a%20Transformer%20sequence-to-sequence%20%28seq2seq%29%20network%20for%20splicing%0Adetection%20and%20localization.%20Our%20extensive%20evaluation%20shows%20that%20the%20proposed%0Amethod%20outperforms%20existing%20dedicated%20approaches%20for%20splicing%20detection%20%5B3%2C%2010%5D%0Aas%20well%20as%20the%20general-purpose%20networks%20EfficientNet%20%5B28%5D%20and%20RegNet%20%5B25%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.14682v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unconstrained%2520Audio%2520Splicing%2520Detection%2520and%2520Localization%2520with%250A%2520%2520Neural%2520Networks%26entry.906535625%3DDenise%2520Moussa%2520and%2520Germans%2520Hirsch%2520and%2520Christian%2520Riess%26entry.1292438233%3D%2520%2520Freely%2520available%2520and%2520easy-to-use%2520audio%2520editing%2520tools%2520make%2520it%2520straightforward%250Ato%2520perform%2520audio%2520splicing.%2520Convincing%2520forgeries%2520can%2520be%2520created%2520by%2520combining%250Avarious%2520speech%2520samples%2520from%2520the%2520same%2520person.%2520Detection%2520of%2520such%2520splices%2520is%250Aimportant%2520both%2520in%2520the%2520public%2520sector%2520when%2520considering%2520misinformation%252C%2520and%2520in%2520a%250Alegal%2520context%2520to%2520verify%2520the%2520integrity%2520of%2520evidence.%2520Unfortunately%252C%2520most%2520existing%250Adetection%2520algorithms%2520for%2520audio%2520splicing%2520use%2520handcrafted%2520features%2520and%2520make%250Aspecific%2520assumptions.%2520However%252C%2520criminal%2520investigators%2520are%2520often%2520faced%2520with%250Aaudio%2520samples%2520from%2520unconstrained%2520sources%2520with%2520unknown%2520characteristics%252C%2520which%250Araises%2520the%2520need%2520for%2520more%2520generally%2520applicable%2520methods.%250A%2520%2520With%2520this%2520work%252C%2520we%2520aim%2520to%2520take%2520a%2520first%2520step%2520towards%2520unconstrained%2520audio%250Asplicing%2520detection%2520to%2520address%2520this%2520need.%2520We%2520simulate%2520various%2520attack%2520scenarios%250Ain%2520the%2520form%2520of%2520post-processing%2520operations%2520that%2520may%2520disguise%2520splicing.%2520We%250Apropose%2520a%2520Transformer%2520sequence-to-sequence%2520%2528seq2seq%2529%2520network%2520for%2520splicing%250Adetection%2520and%2520localization.%2520Our%2520extensive%2520evaluation%2520shows%2520that%2520the%2520proposed%250Amethod%2520outperforms%2520existing%2520dedicated%2520approaches%2520for%2520splicing%2520detection%2520%255B3%252C%252010%255D%250Aas%2520well%2520as%2520the%2520general-purpose%2520networks%2520EfficientNet%2520%255B28%255D%2520and%2520RegNet%2520%255B25%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.14682v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unconstrained%20Audio%20Splicing%20Detection%20and%20Localization%20with%0A%20%20Neural%20Networks&entry.906535625=Denise%20Moussa%20and%20Germans%20Hirsch%20and%20Christian%20Riess&entry.1292438233=%20%20Freely%20available%20and%20easy-to-use%20audio%20editing%20tools%20make%20it%20straightforward%0Ato%20perform%20audio%20splicing.%20Convincing%20forgeries%20can%20be%20created%20by%20combining%0Avarious%20speech%20samples%20from%20the%20same%20person.%20Detection%20of%20such%20splices%20is%0Aimportant%20both%20in%20the%20public%20sector%20when%20considering%20misinformation%2C%20and%20in%20a%0Alegal%20context%20to%20verify%20the%20integrity%20of%20evidence.%20Unfortunately%2C%20most%20existing%0Adetection%20algorithms%20for%20audio%20splicing%20use%20handcrafted%20features%20and%20make%0Aspecific%20assumptions.%20However%2C%20criminal%20investigators%20are%20often%20faced%20with%0Aaudio%20samples%20from%20unconstrained%20sources%20with%20unknown%20characteristics%2C%20which%0Araises%20the%20need%20for%20more%20generally%20applicable%20methods.%0A%20%20With%20this%20work%2C%20we%20aim%20to%20take%20a%20first%20step%20towards%20unconstrained%20audio%0Asplicing%20detection%20to%20address%20this%20need.%20We%20simulate%20various%20attack%20scenarios%0Ain%20the%20form%20of%20post-processing%20operations%20that%20may%20disguise%20splicing.%20We%0Apropose%20a%20Transformer%20sequence-to-sequence%20%28seq2seq%29%20network%20for%20splicing%0Adetection%20and%20localization.%20Our%20extensive%20evaluation%20shows%20that%20the%20proposed%0Amethod%20outperforms%20existing%20dedicated%20approaches%20for%20splicing%20detection%20%5B3%2C%2010%5D%0Aas%20well%20as%20the%20general-purpose%20networks%20EfficientNet%20%5B28%5D%20and%20RegNet%20%5B25%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.14682v4&entry.124074799=Read"},
{"title": "Few-sample Variational Inference of Bayesian Neural Networks with\n  Arbitrary Nonlinearities", "author": "David J. Schodt", "abstract": "  Bayesian Neural Networks (BNNs) extend traditional neural networks to provide\nuncertainties associated with their outputs. On the forward pass through a BNN,\npredictions (and their uncertainties) are made either by Monte Carlo sampling\nnetwork weights from the learned posterior or by analytically propagating\nstatistical moments through the network. Though flexible, Monte Carlo sampling\nis computationally expensive and can be infeasible or impractical under\nresource constraints or for large networks. While moment propagation can\nameliorate the computational costs of BNN inference, it can be difficult or\nimpossible for networks with arbitrary nonlinearities, thereby restricting the\npossible set of network layers permitted with such a scheme. In this work, we\ndemonstrate a simple yet effective approach for propagating statistical moments\nthrough arbitrary nonlinearities with only 3 deterministic samples, enabling\nfew-sample variational inference of BNNs without restricting the set of network\nlayers used. Furthermore, we leverage this approach to demonstrate a novel\nnonlinear activation function that we use to inject physics-informed prior\ninformation into output nodes of a BNN.\n", "link": "http://arxiv.org/abs/2405.02063v1", "date": "2024-05-03", "relevancy": 1.9041, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5398}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.474}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-sample%20Variational%20Inference%20of%20Bayesian%20Neural%20Networks%20with%0A%20%20Arbitrary%20Nonlinearities&body=Title%3A%20Few-sample%20Variational%20Inference%20of%20Bayesian%20Neural%20Networks%20with%0A%20%20Arbitrary%20Nonlinearities%0AAuthor%3A%20David%20J.%20Schodt%0AAbstract%3A%20%20%20Bayesian%20Neural%20Networks%20%28BNNs%29%20extend%20traditional%20neural%20networks%20to%20provide%0Auncertainties%20associated%20with%20their%20outputs.%20On%20the%20forward%20pass%20through%20a%20BNN%2C%0Apredictions%20%28and%20their%20uncertainties%29%20are%20made%20either%20by%20Monte%20Carlo%20sampling%0Anetwork%20weights%20from%20the%20learned%20posterior%20or%20by%20analytically%20propagating%0Astatistical%20moments%20through%20the%20network.%20Though%20flexible%2C%20Monte%20Carlo%20sampling%0Ais%20computationally%20expensive%20and%20can%20be%20infeasible%20or%20impractical%20under%0Aresource%20constraints%20or%20for%20large%20networks.%20While%20moment%20propagation%20can%0Aameliorate%20the%20computational%20costs%20of%20BNN%20inference%2C%20it%20can%20be%20difficult%20or%0Aimpossible%20for%20networks%20with%20arbitrary%20nonlinearities%2C%20thereby%20restricting%20the%0Apossible%20set%20of%20network%20layers%20permitted%20with%20such%20a%20scheme.%20In%20this%20work%2C%20we%0Ademonstrate%20a%20simple%20yet%20effective%20approach%20for%20propagating%20statistical%20moments%0Athrough%20arbitrary%20nonlinearities%20with%20only%203%20deterministic%20samples%2C%20enabling%0Afew-sample%20variational%20inference%20of%20BNNs%20without%20restricting%20the%20set%20of%20network%0Alayers%20used.%20Furthermore%2C%20we%20leverage%20this%20approach%20to%20demonstrate%20a%20novel%0Anonlinear%20activation%20function%20that%20we%20use%20to%20inject%20physics-informed%20prior%0Ainformation%20into%20output%20nodes%20of%20a%20BNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-sample%2520Variational%2520Inference%2520of%2520Bayesian%2520Neural%2520Networks%2520with%250A%2520%2520Arbitrary%2520Nonlinearities%26entry.906535625%3DDavid%2520J.%2520Schodt%26entry.1292438233%3D%2520%2520Bayesian%2520Neural%2520Networks%2520%2528BNNs%2529%2520extend%2520traditional%2520neural%2520networks%2520to%2520provide%250Auncertainties%2520associated%2520with%2520their%2520outputs.%2520On%2520the%2520forward%2520pass%2520through%2520a%2520BNN%252C%250Apredictions%2520%2528and%2520their%2520uncertainties%2529%2520are%2520made%2520either%2520by%2520Monte%2520Carlo%2520sampling%250Anetwork%2520weights%2520from%2520the%2520learned%2520posterior%2520or%2520by%2520analytically%2520propagating%250Astatistical%2520moments%2520through%2520the%2520network.%2520Though%2520flexible%252C%2520Monte%2520Carlo%2520sampling%250Ais%2520computationally%2520expensive%2520and%2520can%2520be%2520infeasible%2520or%2520impractical%2520under%250Aresource%2520constraints%2520or%2520for%2520large%2520networks.%2520While%2520moment%2520propagation%2520can%250Aameliorate%2520the%2520computational%2520costs%2520of%2520BNN%2520inference%252C%2520it%2520can%2520be%2520difficult%2520or%250Aimpossible%2520for%2520networks%2520with%2520arbitrary%2520nonlinearities%252C%2520thereby%2520restricting%2520the%250Apossible%2520set%2520of%2520network%2520layers%2520permitted%2520with%2520such%2520a%2520scheme.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520a%2520simple%2520yet%2520effective%2520approach%2520for%2520propagating%2520statistical%2520moments%250Athrough%2520arbitrary%2520nonlinearities%2520with%2520only%25203%2520deterministic%2520samples%252C%2520enabling%250Afew-sample%2520variational%2520inference%2520of%2520BNNs%2520without%2520restricting%2520the%2520set%2520of%2520network%250Alayers%2520used.%2520Furthermore%252C%2520we%2520leverage%2520this%2520approach%2520to%2520demonstrate%2520a%2520novel%250Anonlinear%2520activation%2520function%2520that%2520we%2520use%2520to%2520inject%2520physics-informed%2520prior%250Ainformation%2520into%2520output%2520nodes%2520of%2520a%2520BNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-sample%20Variational%20Inference%20of%20Bayesian%20Neural%20Networks%20with%0A%20%20Arbitrary%20Nonlinearities&entry.906535625=David%20J.%20Schodt&entry.1292438233=%20%20Bayesian%20Neural%20Networks%20%28BNNs%29%20extend%20traditional%20neural%20networks%20to%20provide%0Auncertainties%20associated%20with%20their%20outputs.%20On%20the%20forward%20pass%20through%20a%20BNN%2C%0Apredictions%20%28and%20their%20uncertainties%29%20are%20made%20either%20by%20Monte%20Carlo%20sampling%0Anetwork%20weights%20from%20the%20learned%20posterior%20or%20by%20analytically%20propagating%0Astatistical%20moments%20through%20the%20network.%20Though%20flexible%2C%20Monte%20Carlo%20sampling%0Ais%20computationally%20expensive%20and%20can%20be%20infeasible%20or%20impractical%20under%0Aresource%20constraints%20or%20for%20large%20networks.%20While%20moment%20propagation%20can%0Aameliorate%20the%20computational%20costs%20of%20BNN%20inference%2C%20it%20can%20be%20difficult%20or%0Aimpossible%20for%20networks%20with%20arbitrary%20nonlinearities%2C%20thereby%20restricting%20the%0Apossible%20set%20of%20network%20layers%20permitted%20with%20such%20a%20scheme.%20In%20this%20work%2C%20we%0Ademonstrate%20a%20simple%20yet%20effective%20approach%20for%20propagating%20statistical%20moments%0Athrough%20arbitrary%20nonlinearities%20with%20only%203%20deterministic%20samples%2C%20enabling%0Afew-sample%20variational%20inference%20of%20BNNs%20without%20restricting%20the%20set%20of%20network%0Alayers%20used.%20Furthermore%2C%20we%20leverage%20this%20approach%20to%20demonstrate%20a%20novel%0Anonlinear%20activation%20function%20that%20we%20use%20to%20inject%20physics-informed%20prior%0Ainformation%20into%20output%20nodes%20of%20a%20BNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02063v1&entry.124074799=Read"},
{"title": "Wasserstein Wormhole: Scalable Optimal Transport Distance with\n  Transformers", "author": "Doron Haviv and Russell Zhang Kunes and Thomas Dougherty and Cassandra Burdziak and Tal Nawy and Anna Gilbert and Dana Pe'er", "abstract": "  Optimal transport (OT) and the related Wasserstein metric (W) are powerful\nand ubiquitous tools for comparing distributions. However, computing pairwise\nWasserstein distances rapidly becomes intractable as cohort size grows. An\nattractive alternative would be to find an embedding space in which pairwise\nEuclidean distances map to OT distances, akin to standard multidimensional\nscaling (MDS). We present Wasserstein Wormhole, a transformer-based autoencoder\nthat embeds empirical distributions into a latent space wherein Euclidean\ndistances approximate OT distances. Extending MDS theory, we show that our\nobjective function implies a bound on the error incurred when embedding\nnon-Euclidean distances. Empirically, distances between Wormhole embeddings\nclosely match Wasserstein distances, enabling linear time computation of OT\ndistances. Along with an encoder that maps distributions to embeddings,\nWasserstein Wormhole includes a decoder that maps embeddings back to\ndistributions, allowing for operations in the embedding space to generalize to\nOT spaces, such as Wasserstein barycenter estimation and OT interpolation. By\nlending scalability and interpretability to OT approaches, Wasserstein Wormhole\nunlocks new avenues for data analysis in the fields of computational geometry\nand single-cell biology.\n", "link": "http://arxiv.org/abs/2404.09411v2", "date": "2024-05-03", "relevancy": 1.8909, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4589}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wasserstein%20Wormhole%3A%20Scalable%20Optimal%20Transport%20Distance%20with%0A%20%20Transformers&body=Title%3A%20Wasserstein%20Wormhole%3A%20Scalable%20Optimal%20Transport%20Distance%20with%0A%20%20Transformers%0AAuthor%3A%20Doron%20Haviv%20and%20Russell%20Zhang%20Kunes%20and%20Thomas%20Dougherty%20and%20Cassandra%20Burdziak%20and%20Tal%20Nawy%20and%20Anna%20Gilbert%20and%20Dana%20Pe%27er%0AAbstract%3A%20%20%20Optimal%20transport%20%28OT%29%20and%20the%20related%20Wasserstein%20metric%20%28W%29%20are%20powerful%0Aand%20ubiquitous%20tools%20for%20comparing%20distributions.%20However%2C%20computing%20pairwise%0AWasserstein%20distances%20rapidly%20becomes%20intractable%20as%20cohort%20size%20grows.%20An%0Aattractive%20alternative%20would%20be%20to%20find%20an%20embedding%20space%20in%20which%20pairwise%0AEuclidean%20distances%20map%20to%20OT%20distances%2C%20akin%20to%20standard%20multidimensional%0Ascaling%20%28MDS%29.%20We%20present%20Wasserstein%20Wormhole%2C%20a%20transformer-based%20autoencoder%0Athat%20embeds%20empirical%20distributions%20into%20a%20latent%20space%20wherein%20Euclidean%0Adistances%20approximate%20OT%20distances.%20Extending%20MDS%20theory%2C%20we%20show%20that%20our%0Aobjective%20function%20implies%20a%20bound%20on%20the%20error%20incurred%20when%20embedding%0Anon-Euclidean%20distances.%20Empirically%2C%20distances%20between%20Wormhole%20embeddings%0Aclosely%20match%20Wasserstein%20distances%2C%20enabling%20linear%20time%20computation%20of%20OT%0Adistances.%20Along%20with%20an%20encoder%20that%20maps%20distributions%20to%20embeddings%2C%0AWasserstein%20Wormhole%20includes%20a%20decoder%20that%20maps%20embeddings%20back%20to%0Adistributions%2C%20allowing%20for%20operations%20in%20the%20embedding%20space%20to%20generalize%20to%0AOT%20spaces%2C%20such%20as%20Wasserstein%20barycenter%20estimation%20and%20OT%20interpolation.%20By%0Alending%20scalability%20and%20interpretability%20to%20OT%20approaches%2C%20Wasserstein%20Wormhole%0Aunlocks%20new%20avenues%20for%20data%20analysis%20in%20the%20fields%20of%20computational%20geometry%0Aand%20single-cell%20biology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWasserstein%2520Wormhole%253A%2520Scalable%2520Optimal%2520Transport%2520Distance%2520with%250A%2520%2520Transformers%26entry.906535625%3DDoron%2520Haviv%2520and%2520Russell%2520Zhang%2520Kunes%2520and%2520Thomas%2520Dougherty%2520and%2520Cassandra%2520Burdziak%2520and%2520Tal%2520Nawy%2520and%2520Anna%2520Gilbert%2520and%2520Dana%2520Pe%2527er%26entry.1292438233%3D%2520%2520Optimal%2520transport%2520%2528OT%2529%2520and%2520the%2520related%2520Wasserstein%2520metric%2520%2528W%2529%2520are%2520powerful%250Aand%2520ubiquitous%2520tools%2520for%2520comparing%2520distributions.%2520However%252C%2520computing%2520pairwise%250AWasserstein%2520distances%2520rapidly%2520becomes%2520intractable%2520as%2520cohort%2520size%2520grows.%2520An%250Aattractive%2520alternative%2520would%2520be%2520to%2520find%2520an%2520embedding%2520space%2520in%2520which%2520pairwise%250AEuclidean%2520distances%2520map%2520to%2520OT%2520distances%252C%2520akin%2520to%2520standard%2520multidimensional%250Ascaling%2520%2528MDS%2529.%2520We%2520present%2520Wasserstein%2520Wormhole%252C%2520a%2520transformer-based%2520autoencoder%250Athat%2520embeds%2520empirical%2520distributions%2520into%2520a%2520latent%2520space%2520wherein%2520Euclidean%250Adistances%2520approximate%2520OT%2520distances.%2520Extending%2520MDS%2520theory%252C%2520we%2520show%2520that%2520our%250Aobjective%2520function%2520implies%2520a%2520bound%2520on%2520the%2520error%2520incurred%2520when%2520embedding%250Anon-Euclidean%2520distances.%2520Empirically%252C%2520distances%2520between%2520Wormhole%2520embeddings%250Aclosely%2520match%2520Wasserstein%2520distances%252C%2520enabling%2520linear%2520time%2520computation%2520of%2520OT%250Adistances.%2520Along%2520with%2520an%2520encoder%2520that%2520maps%2520distributions%2520to%2520embeddings%252C%250AWasserstein%2520Wormhole%2520includes%2520a%2520decoder%2520that%2520maps%2520embeddings%2520back%2520to%250Adistributions%252C%2520allowing%2520for%2520operations%2520in%2520the%2520embedding%2520space%2520to%2520generalize%2520to%250AOT%2520spaces%252C%2520such%2520as%2520Wasserstein%2520barycenter%2520estimation%2520and%2520OT%2520interpolation.%2520By%250Alending%2520scalability%2520and%2520interpretability%2520to%2520OT%2520approaches%252C%2520Wasserstein%2520Wormhole%250Aunlocks%2520new%2520avenues%2520for%2520data%2520analysis%2520in%2520the%2520fields%2520of%2520computational%2520geometry%250Aand%2520single-cell%2520biology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wasserstein%20Wormhole%3A%20Scalable%20Optimal%20Transport%20Distance%20with%0A%20%20Transformers&entry.906535625=Doron%20Haviv%20and%20Russell%20Zhang%20Kunes%20and%20Thomas%20Dougherty%20and%20Cassandra%20Burdziak%20and%20Tal%20Nawy%20and%20Anna%20Gilbert%20and%20Dana%20Pe%27er&entry.1292438233=%20%20Optimal%20transport%20%28OT%29%20and%20the%20related%20Wasserstein%20metric%20%28W%29%20are%20powerful%0Aand%20ubiquitous%20tools%20for%20comparing%20distributions.%20However%2C%20computing%20pairwise%0AWasserstein%20distances%20rapidly%20becomes%20intractable%20as%20cohort%20size%20grows.%20An%0Aattractive%20alternative%20would%20be%20to%20find%20an%20embedding%20space%20in%20which%20pairwise%0AEuclidean%20distances%20map%20to%20OT%20distances%2C%20akin%20to%20standard%20multidimensional%0Ascaling%20%28MDS%29.%20We%20present%20Wasserstein%20Wormhole%2C%20a%20transformer-based%20autoencoder%0Athat%20embeds%20empirical%20distributions%20into%20a%20latent%20space%20wherein%20Euclidean%0Adistances%20approximate%20OT%20distances.%20Extending%20MDS%20theory%2C%20we%20show%20that%20our%0Aobjective%20function%20implies%20a%20bound%20on%20the%20error%20incurred%20when%20embedding%0Anon-Euclidean%20distances.%20Empirically%2C%20distances%20between%20Wormhole%20embeddings%0Aclosely%20match%20Wasserstein%20distances%2C%20enabling%20linear%20time%20computation%20of%20OT%0Adistances.%20Along%20with%20an%20encoder%20that%20maps%20distributions%20to%20embeddings%2C%0AWasserstein%20Wormhole%20includes%20a%20decoder%20that%20maps%20embeddings%20back%20to%0Adistributions%2C%20allowing%20for%20operations%20in%20the%20embedding%20space%20to%20generalize%20to%0AOT%20spaces%2C%20such%20as%20Wasserstein%20barycenter%20estimation%20and%20OT%20interpolation.%20By%0Alending%20scalability%20and%20interpretability%20to%20OT%20approaches%2C%20Wasserstein%20Wormhole%0Aunlocks%20new%20avenues%20for%20data%20analysis%20in%20the%20fields%20of%20computational%20geometry%0Aand%20single-cell%20biology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09411v2&entry.124074799=Read"},
{"title": "On Gradient-like Explanation under a Black-box Setting: When Black-box\n  Explanations Become as Good as White-box", "author": "Yi Cai and Gerhard Wunder", "abstract": "  Attribution methods shed light on the explainability of data-driven\napproaches such as deep learning models by uncovering the most influential\nfeatures in a to-be-explained decision. While determining feature attributions\nvia gradients delivers promising results, the internal access required for\nacquiring gradients can be impractical under safety concerns, thus limiting the\napplicability of gradient-based approaches. In response to such limited\nflexibility, this paper presents \\methodAbr~(gradient-estimation-based\nexplanation), an approach that produces gradient-like explanations through only\nquery-level access. The proposed approach holds a set of fundamental properties\nfor attribution methods, which are mathematically rigorously proved, ensuring\nthe quality of its explanations. In addition to the theoretical analysis, with\na focus on image data, the experimental results empirically demonstrate the\nsuperiority of the proposed method over state-of-the-art black-box methods and\nits competitive performance compared to methods with full access.\n", "link": "http://arxiv.org/abs/2308.09381v2", "date": "2024-05-03", "relevancy": 1.8893, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4815}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4692}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Gradient-like%20Explanation%20under%20a%20Black-box%20Setting%3A%20When%20Black-box%0A%20%20Explanations%20Become%20as%20Good%20as%20White-box&body=Title%3A%20On%20Gradient-like%20Explanation%20under%20a%20Black-box%20Setting%3A%20When%20Black-box%0A%20%20Explanations%20Become%20as%20Good%20as%20White-box%0AAuthor%3A%20Yi%20Cai%20and%20Gerhard%20Wunder%0AAbstract%3A%20%20%20Attribution%20methods%20shed%20light%20on%20the%20explainability%20of%20data-driven%0Aapproaches%20such%20as%20deep%20learning%20models%20by%20uncovering%20the%20most%20influential%0Afeatures%20in%20a%20to-be-explained%20decision.%20While%20determining%20feature%20attributions%0Avia%20gradients%20delivers%20promising%20results%2C%20the%20internal%20access%20required%20for%0Aacquiring%20gradients%20can%20be%20impractical%20under%20safety%20concerns%2C%20thus%20limiting%20the%0Aapplicability%20of%20gradient-based%20approaches.%20In%20response%20to%20such%20limited%0Aflexibility%2C%20this%20paper%20presents%20%5CmethodAbr~%28gradient-estimation-based%0Aexplanation%29%2C%20an%20approach%20that%20produces%20gradient-like%20explanations%20through%20only%0Aquery-level%20access.%20The%20proposed%20approach%20holds%20a%20set%20of%20fundamental%20properties%0Afor%20attribution%20methods%2C%20which%20are%20mathematically%20rigorously%20proved%2C%20ensuring%0Athe%20quality%20of%20its%20explanations.%20In%20addition%20to%20the%20theoretical%20analysis%2C%20with%0Aa%20focus%20on%20image%20data%2C%20the%20experimental%20results%20empirically%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20over%20state-of-the-art%20black-box%20methods%20and%0Aits%20competitive%20performance%20compared%20to%20methods%20with%20full%20access.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09381v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Gradient-like%2520Explanation%2520under%2520a%2520Black-box%2520Setting%253A%2520When%2520Black-box%250A%2520%2520Explanations%2520Become%2520as%2520Good%2520as%2520White-box%26entry.906535625%3DYi%2520Cai%2520and%2520Gerhard%2520Wunder%26entry.1292438233%3D%2520%2520Attribution%2520methods%2520shed%2520light%2520on%2520the%2520explainability%2520of%2520data-driven%250Aapproaches%2520such%2520as%2520deep%2520learning%2520models%2520by%2520uncovering%2520the%2520most%2520influential%250Afeatures%2520in%2520a%2520to-be-explained%2520decision.%2520While%2520determining%2520feature%2520attributions%250Avia%2520gradients%2520delivers%2520promising%2520results%252C%2520the%2520internal%2520access%2520required%2520for%250Aacquiring%2520gradients%2520can%2520be%2520impractical%2520under%2520safety%2520concerns%252C%2520thus%2520limiting%2520the%250Aapplicability%2520of%2520gradient-based%2520approaches.%2520In%2520response%2520to%2520such%2520limited%250Aflexibility%252C%2520this%2520paper%2520presents%2520%255CmethodAbr~%2528gradient-estimation-based%250Aexplanation%2529%252C%2520an%2520approach%2520that%2520produces%2520gradient-like%2520explanations%2520through%2520only%250Aquery-level%2520access.%2520The%2520proposed%2520approach%2520holds%2520a%2520set%2520of%2520fundamental%2520properties%250Afor%2520attribution%2520methods%252C%2520which%2520are%2520mathematically%2520rigorously%2520proved%252C%2520ensuring%250Athe%2520quality%2520of%2520its%2520explanations.%2520In%2520addition%2520to%2520the%2520theoretical%2520analysis%252C%2520with%250Aa%2520focus%2520on%2520image%2520data%252C%2520the%2520experimental%2520results%2520empirically%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520method%2520over%2520state-of-the-art%2520black-box%2520methods%2520and%250Aits%2520competitive%2520performance%2520compared%2520to%2520methods%2520with%2520full%2520access.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09381v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Gradient-like%20Explanation%20under%20a%20Black-box%20Setting%3A%20When%20Black-box%0A%20%20Explanations%20Become%20as%20Good%20as%20White-box&entry.906535625=Yi%20Cai%20and%20Gerhard%20Wunder&entry.1292438233=%20%20Attribution%20methods%20shed%20light%20on%20the%20explainability%20of%20data-driven%0Aapproaches%20such%20as%20deep%20learning%20models%20by%20uncovering%20the%20most%20influential%0Afeatures%20in%20a%20to-be-explained%20decision.%20While%20determining%20feature%20attributions%0Avia%20gradients%20delivers%20promising%20results%2C%20the%20internal%20access%20required%20for%0Aacquiring%20gradients%20can%20be%20impractical%20under%20safety%20concerns%2C%20thus%20limiting%20the%0Aapplicability%20of%20gradient-based%20approaches.%20In%20response%20to%20such%20limited%0Aflexibility%2C%20this%20paper%20presents%20%5CmethodAbr~%28gradient-estimation-based%0Aexplanation%29%2C%20an%20approach%20that%20produces%20gradient-like%20explanations%20through%20only%0Aquery-level%20access.%20The%20proposed%20approach%20holds%20a%20set%20of%20fundamental%20properties%0Afor%20attribution%20methods%2C%20which%20are%20mathematically%20rigorously%20proved%2C%20ensuring%0Athe%20quality%20of%20its%20explanations.%20In%20addition%20to%20the%20theoretical%20analysis%2C%20with%0Aa%20focus%20on%20image%20data%2C%20the%20experimental%20results%20empirically%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20over%20state-of-the-art%20black-box%20methods%20and%0Aits%20competitive%20performance%20compared%20to%20methods%20with%20full%20access.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09381v2&entry.124074799=Read"},
{"title": "Advanced Detection of Source Code Clones via an Ensemble of Unsupervised\n  Similarity Measures", "author": "Jorge Martinez-Gil", "abstract": "  The capability of accurately determining code similarity is crucial in many\ntasks related to software development. For example, it might be essential to\nidentify code duplicates for performing software maintenance. This research\nintroduces a novel ensemble learning approach for code similarity assessment,\ncombining the strengths of multiple unsupervised similarity measures. The key\nidea is that the strengths of a diverse set of similarity measures can\ncomplement each other and mitigate individual weaknesses, leading to improved\nperformance. Preliminary results show that while Transformers-based CodeBERT\nand its variant GraphCodeBERT are undoubtedly the best option in the presence\nof abundant training data, in the case of specific small datasets (up to 500\nsamples), our ensemble achieves similar results, without prejudice to the\ninterpretability of the resulting solution, and with a much lower associated\ncarbon footprint due to training. The source code of this novel approach can be\ndownloaded from https://github.com/jorge-martinez-gil/ensemble-codesim.\n", "link": "http://arxiv.org/abs/2405.02095v1", "date": "2024-05-03", "relevancy": 1.8667, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4834}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.457}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Detection%20of%20Source%20Code%20Clones%20via%20an%20Ensemble%20of%20Unsupervised%0A%20%20Similarity%20Measures&body=Title%3A%20Advanced%20Detection%20of%20Source%20Code%20Clones%20via%20an%20Ensemble%20of%20Unsupervised%0A%20%20Similarity%20Measures%0AAuthor%3A%20Jorge%20Martinez-Gil%0AAbstract%3A%20%20%20The%20capability%20of%20accurately%20determining%20code%20similarity%20is%20crucial%20in%20many%0Atasks%20related%20to%20software%20development.%20For%20example%2C%20it%20might%20be%20essential%20to%0Aidentify%20code%20duplicates%20for%20performing%20software%20maintenance.%20This%20research%0Aintroduces%20a%20novel%20ensemble%20learning%20approach%20for%20code%20similarity%20assessment%2C%0Acombining%20the%20strengths%20of%20multiple%20unsupervised%20similarity%20measures.%20The%20key%0Aidea%20is%20that%20the%20strengths%20of%20a%20diverse%20set%20of%20similarity%20measures%20can%0Acomplement%20each%20other%20and%20mitigate%20individual%20weaknesses%2C%20leading%20to%20improved%0Aperformance.%20Preliminary%20results%20show%20that%20while%20Transformers-based%20CodeBERT%0Aand%20its%20variant%20GraphCodeBERT%20are%20undoubtedly%20the%20best%20option%20in%20the%20presence%0Aof%20abundant%20training%20data%2C%20in%20the%20case%20of%20specific%20small%20datasets%20%28up%20to%20500%0Asamples%29%2C%20our%20ensemble%20achieves%20similar%20results%2C%20without%20prejudice%20to%20the%0Ainterpretability%20of%20the%20resulting%20solution%2C%20and%20with%20a%20much%20lower%20associated%0Acarbon%20footprint%20due%20to%20training.%20The%20source%20code%20of%20this%20novel%20approach%20can%20be%0Adownloaded%20from%20https%3A//github.com/jorge-martinez-gil/ensemble-codesim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Detection%2520of%2520Source%2520Code%2520Clones%2520via%2520an%2520Ensemble%2520of%2520Unsupervised%250A%2520%2520Similarity%2520Measures%26entry.906535625%3DJorge%2520Martinez-Gil%26entry.1292438233%3D%2520%2520The%2520capability%2520of%2520accurately%2520determining%2520code%2520similarity%2520is%2520crucial%2520in%2520many%250Atasks%2520related%2520to%2520software%2520development.%2520For%2520example%252C%2520it%2520might%2520be%2520essential%2520to%250Aidentify%2520code%2520duplicates%2520for%2520performing%2520software%2520maintenance.%2520This%2520research%250Aintroduces%2520a%2520novel%2520ensemble%2520learning%2520approach%2520for%2520code%2520similarity%2520assessment%252C%250Acombining%2520the%2520strengths%2520of%2520multiple%2520unsupervised%2520similarity%2520measures.%2520The%2520key%250Aidea%2520is%2520that%2520the%2520strengths%2520of%2520a%2520diverse%2520set%2520of%2520similarity%2520measures%2520can%250Acomplement%2520each%2520other%2520and%2520mitigate%2520individual%2520weaknesses%252C%2520leading%2520to%2520improved%250Aperformance.%2520Preliminary%2520results%2520show%2520that%2520while%2520Transformers-based%2520CodeBERT%250Aand%2520its%2520variant%2520GraphCodeBERT%2520are%2520undoubtedly%2520the%2520best%2520option%2520in%2520the%2520presence%250Aof%2520abundant%2520training%2520data%252C%2520in%2520the%2520case%2520of%2520specific%2520small%2520datasets%2520%2528up%2520to%2520500%250Asamples%2529%252C%2520our%2520ensemble%2520achieves%2520similar%2520results%252C%2520without%2520prejudice%2520to%2520the%250Ainterpretability%2520of%2520the%2520resulting%2520solution%252C%2520and%2520with%2520a%2520much%2520lower%2520associated%250Acarbon%2520footprint%2520due%2520to%2520training.%2520The%2520source%2520code%2520of%2520this%2520novel%2520approach%2520can%2520be%250Adownloaded%2520from%2520https%253A//github.com/jorge-martinez-gil/ensemble-codesim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Detection%20of%20Source%20Code%20Clones%20via%20an%20Ensemble%20of%20Unsupervised%0A%20%20Similarity%20Measures&entry.906535625=Jorge%20Martinez-Gil&entry.1292438233=%20%20The%20capability%20of%20accurately%20determining%20code%20similarity%20is%20crucial%20in%20many%0Atasks%20related%20to%20software%20development.%20For%20example%2C%20it%20might%20be%20essential%20to%0Aidentify%20code%20duplicates%20for%20performing%20software%20maintenance.%20This%20research%0Aintroduces%20a%20novel%20ensemble%20learning%20approach%20for%20code%20similarity%20assessment%2C%0Acombining%20the%20strengths%20of%20multiple%20unsupervised%20similarity%20measures.%20The%20key%0Aidea%20is%20that%20the%20strengths%20of%20a%20diverse%20set%20of%20similarity%20measures%20can%0Acomplement%20each%20other%20and%20mitigate%20individual%20weaknesses%2C%20leading%20to%20improved%0Aperformance.%20Preliminary%20results%20show%20that%20while%20Transformers-based%20CodeBERT%0Aand%20its%20variant%20GraphCodeBERT%20are%20undoubtedly%20the%20best%20option%20in%20the%20presence%0Aof%20abundant%20training%20data%2C%20in%20the%20case%20of%20specific%20small%20datasets%20%28up%20to%20500%0Asamples%29%2C%20our%20ensemble%20achieves%20similar%20results%2C%20without%20prejudice%20to%20the%0Ainterpretability%20of%20the%20resulting%20solution%2C%20and%20with%20a%20much%20lower%20associated%0Acarbon%20footprint%20due%20to%20training.%20The%20source%20code%20of%20this%20novel%20approach%20can%20be%0Adownloaded%20from%20https%3A//github.com/jorge-martinez-gil/ensemble-codesim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02095v1&entry.124074799=Read"},
{"title": "Discovering Novel Actions from Open World Egocentric Videos with\n  Object-Grounded Visual Commonsense Reasoning", "author": "Sanjoy Kundu and Shubham Trehan and Sathyanarayanan N. Aakur", "abstract": "  Learning to infer labels in an open world, i.e., in an environment where the\ntarget ``labels'' are unknown, is an important characteristic for achieving\nautonomy. Foundation models, pre-trained on enormous amounts of data, have\nshown remarkable generalization skills through prompting, particularly in\nzero-shot inference. However, their performance is restricted to the\ncorrectness of the target label's search space, i.e., candidate labels provided\nin the prompt. This target search space can be unknown or exceptionally large\nin an open world, severely restricting their performance. To tackle this\nchallenging problem, we propose a two-step, neuro-symbolic framework called\nALGO - Action Learning with Grounded Object recognition that uses symbolic\nknowledge stored in large-scale knowledge bases to infer activities in\negocentric videos with limited supervision. First, we propose a neuro-symbolic\nprompting approach that uses object-centric vision-language models as a noisy\noracle to ground objects in the video through evidence-based reasoning. Second,\ndriven by prior commonsense knowledge, we discover plausible activities through\nan energy-based symbolic pattern theory framework and learn to ground\nknowledge-based action (verb) concepts in the video. Extensive experiments on\nfour publicly available datasets (EPIC-Kitchens, GTEA Gaze, GTEA Gaze Plus, and\nCharades-Ego) demonstrate its performance on open-world activity inference. We\nalso show that ALGO can be extended to zero-shot inference and demonstrate its\ncompetitive performance on the Charades-Ego dataset.\n", "link": "http://arxiv.org/abs/2305.16602v2", "date": "2024-05-03", "relevancy": 1.8665, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6564}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6305}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Novel%20Actions%20from%20Open%20World%20Egocentric%20Videos%20with%0A%20%20Object-Grounded%20Visual%20Commonsense%20Reasoning&body=Title%3A%20Discovering%20Novel%20Actions%20from%20Open%20World%20Egocentric%20Videos%20with%0A%20%20Object-Grounded%20Visual%20Commonsense%20Reasoning%0AAuthor%3A%20Sanjoy%20Kundu%20and%20Shubham%20Trehan%20and%20Sathyanarayanan%20N.%20Aakur%0AAbstract%3A%20%20%20Learning%20to%20infer%20labels%20in%20an%20open%20world%2C%20i.e.%2C%20in%20an%20environment%20where%20the%0Atarget%20%60%60labels%27%27%20are%20unknown%2C%20is%20an%20important%20characteristic%20for%20achieving%0Aautonomy.%20Foundation%20models%2C%20pre-trained%20on%20enormous%20amounts%20of%20data%2C%20have%0Ashown%20remarkable%20generalization%20skills%20through%20prompting%2C%20particularly%20in%0Azero-shot%20inference.%20However%2C%20their%20performance%20is%20restricted%20to%20the%0Acorrectness%20of%20the%20target%20label%27s%20search%20space%2C%20i.e.%2C%20candidate%20labels%20provided%0Ain%20the%20prompt.%20This%20target%20search%20space%20can%20be%20unknown%20or%20exceptionally%20large%0Ain%20an%20open%20world%2C%20severely%20restricting%20their%20performance.%20To%20tackle%20this%0Achallenging%20problem%2C%20we%20propose%20a%20two-step%2C%20neuro-symbolic%20framework%20called%0AALGO%20-%20Action%20Learning%20with%20Grounded%20Object%20recognition%20that%20uses%20symbolic%0Aknowledge%20stored%20in%20large-scale%20knowledge%20bases%20to%20infer%20activities%20in%0Aegocentric%20videos%20with%20limited%20supervision.%20First%2C%20we%20propose%20a%20neuro-symbolic%0Aprompting%20approach%20that%20uses%20object-centric%20vision-language%20models%20as%20a%20noisy%0Aoracle%20to%20ground%20objects%20in%20the%20video%20through%20evidence-based%20reasoning.%20Second%2C%0Adriven%20by%20prior%20commonsense%20knowledge%2C%20we%20discover%20plausible%20activities%20through%0Aan%20energy-based%20symbolic%20pattern%20theory%20framework%20and%20learn%20to%20ground%0Aknowledge-based%20action%20%28verb%29%20concepts%20in%20the%20video.%20Extensive%20experiments%20on%0Afour%20publicly%20available%20datasets%20%28EPIC-Kitchens%2C%20GTEA%20Gaze%2C%20GTEA%20Gaze%20Plus%2C%20and%0ACharades-Ego%29%20demonstrate%20its%20performance%20on%20open-world%20activity%20inference.%20We%0Aalso%20show%20that%20ALGO%20can%20be%20extended%20to%20zero-shot%20inference%20and%20demonstrate%20its%0Acompetitive%20performance%20on%20the%20Charades-Ego%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Novel%2520Actions%2520from%2520Open%2520World%2520Egocentric%2520Videos%2520with%250A%2520%2520Object-Grounded%2520Visual%2520Commonsense%2520Reasoning%26entry.906535625%3DSanjoy%2520Kundu%2520and%2520Shubham%2520Trehan%2520and%2520Sathyanarayanan%2520N.%2520Aakur%26entry.1292438233%3D%2520%2520Learning%2520to%2520infer%2520labels%2520in%2520an%2520open%2520world%252C%2520i.e.%252C%2520in%2520an%2520environment%2520where%2520the%250Atarget%2520%2560%2560labels%2527%2527%2520are%2520unknown%252C%2520is%2520an%2520important%2520characteristic%2520for%2520achieving%250Aautonomy.%2520Foundation%2520models%252C%2520pre-trained%2520on%2520enormous%2520amounts%2520of%2520data%252C%2520have%250Ashown%2520remarkable%2520generalization%2520skills%2520through%2520prompting%252C%2520particularly%2520in%250Azero-shot%2520inference.%2520However%252C%2520their%2520performance%2520is%2520restricted%2520to%2520the%250Acorrectness%2520of%2520the%2520target%2520label%2527s%2520search%2520space%252C%2520i.e.%252C%2520candidate%2520labels%2520provided%250Ain%2520the%2520prompt.%2520This%2520target%2520search%2520space%2520can%2520be%2520unknown%2520or%2520exceptionally%2520large%250Ain%2520an%2520open%2520world%252C%2520severely%2520restricting%2520their%2520performance.%2520To%2520tackle%2520this%250Achallenging%2520problem%252C%2520we%2520propose%2520a%2520two-step%252C%2520neuro-symbolic%2520framework%2520called%250AALGO%2520-%2520Action%2520Learning%2520with%2520Grounded%2520Object%2520recognition%2520that%2520uses%2520symbolic%250Aknowledge%2520stored%2520in%2520large-scale%2520knowledge%2520bases%2520to%2520infer%2520activities%2520in%250Aegocentric%2520videos%2520with%2520limited%2520supervision.%2520First%252C%2520we%2520propose%2520a%2520neuro-symbolic%250Aprompting%2520approach%2520that%2520uses%2520object-centric%2520vision-language%2520models%2520as%2520a%2520noisy%250Aoracle%2520to%2520ground%2520objects%2520in%2520the%2520video%2520through%2520evidence-based%2520reasoning.%2520Second%252C%250Adriven%2520by%2520prior%2520commonsense%2520knowledge%252C%2520we%2520discover%2520plausible%2520activities%2520through%250Aan%2520energy-based%2520symbolic%2520pattern%2520theory%2520framework%2520and%2520learn%2520to%2520ground%250Aknowledge-based%2520action%2520%2528verb%2529%2520concepts%2520in%2520the%2520video.%2520Extensive%2520experiments%2520on%250Afour%2520publicly%2520available%2520datasets%2520%2528EPIC-Kitchens%252C%2520GTEA%2520Gaze%252C%2520GTEA%2520Gaze%2520Plus%252C%2520and%250ACharades-Ego%2529%2520demonstrate%2520its%2520performance%2520on%2520open-world%2520activity%2520inference.%2520We%250Aalso%2520show%2520that%2520ALGO%2520can%2520be%2520extended%2520to%2520zero-shot%2520inference%2520and%2520demonstrate%2520its%250Acompetitive%2520performance%2520on%2520the%2520Charades-Ego%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.16602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Novel%20Actions%20from%20Open%20World%20Egocentric%20Videos%20with%0A%20%20Object-Grounded%20Visual%20Commonsense%20Reasoning&entry.906535625=Sanjoy%20Kundu%20and%20Shubham%20Trehan%20and%20Sathyanarayanan%20N.%20Aakur&entry.1292438233=%20%20Learning%20to%20infer%20labels%20in%20an%20open%20world%2C%20i.e.%2C%20in%20an%20environment%20where%20the%0Atarget%20%60%60labels%27%27%20are%20unknown%2C%20is%20an%20important%20characteristic%20for%20achieving%0Aautonomy.%20Foundation%20models%2C%20pre-trained%20on%20enormous%20amounts%20of%20data%2C%20have%0Ashown%20remarkable%20generalization%20skills%20through%20prompting%2C%20particularly%20in%0Azero-shot%20inference.%20However%2C%20their%20performance%20is%20restricted%20to%20the%0Acorrectness%20of%20the%20target%20label%27s%20search%20space%2C%20i.e.%2C%20candidate%20labels%20provided%0Ain%20the%20prompt.%20This%20target%20search%20space%20can%20be%20unknown%20or%20exceptionally%20large%0Ain%20an%20open%20world%2C%20severely%20restricting%20their%20performance.%20To%20tackle%20this%0Achallenging%20problem%2C%20we%20propose%20a%20two-step%2C%20neuro-symbolic%20framework%20called%0AALGO%20-%20Action%20Learning%20with%20Grounded%20Object%20recognition%20that%20uses%20symbolic%0Aknowledge%20stored%20in%20large-scale%20knowledge%20bases%20to%20infer%20activities%20in%0Aegocentric%20videos%20with%20limited%20supervision.%20First%2C%20we%20propose%20a%20neuro-symbolic%0Aprompting%20approach%20that%20uses%20object-centric%20vision-language%20models%20as%20a%20noisy%0Aoracle%20to%20ground%20objects%20in%20the%20video%20through%20evidence-based%20reasoning.%20Second%2C%0Adriven%20by%20prior%20commonsense%20knowledge%2C%20we%20discover%20plausible%20activities%20through%0Aan%20energy-based%20symbolic%20pattern%20theory%20framework%20and%20learn%20to%20ground%0Aknowledge-based%20action%20%28verb%29%20concepts%20in%20the%20video.%20Extensive%20experiments%20on%0Afour%20publicly%20available%20datasets%20%28EPIC-Kitchens%2C%20GTEA%20Gaze%2C%20GTEA%20Gaze%20Plus%2C%20and%0ACharades-Ego%29%20demonstrate%20its%20performance%20on%20open-world%20activity%20inference.%20We%0Aalso%20show%20that%20ALGO%20can%20be%20extended%20to%20zero-shot%20inference%20and%20demonstrate%20its%0Acompetitive%20performance%20on%20the%20Charades-Ego%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16602v2&entry.124074799=Read"},
{"title": "Multi-Objective Recommendation via Multivariate Policy Learning", "author": "Olivier Jeunen and Jatin Mandav and Ivan Potapov and Nakul Agarwal and Sourabh Vaid and Wenzhe Shi and Aleksei Ustimenko", "abstract": "  Real-world recommender systems often need to balance multiple objectives when\ndeciding which recommendations to present to users. These include behavioural\nsignals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g.\ndiversity, fairness). Scalarisation methods are commonly used to handle this\nbalancing task, where a weighted average of per-objective reward signals\ndetermines the final score used for ranking. Naturally, how these weights are\ncomputed exactly, is key to success for any online platform. We frame this as a\ndecision-making task, where the scalarisation weights are actions taken to\nmaximise an overall North Star reward (e.g. long-term user retention or\ngrowth). We extend existing policy learning methods to the continuous\nmultivariate action domain, proposing to maximise a pessimistic lower bound on\nthe North Star reward that the learnt policy will yield. Typical lower bounds\nbased on normal approximations suffer from insufficient coverage, and we\npropose an efficient and effective policy-dependent correction for this. We\nprovide guidance to design stochastic data collection policies, as well as\nhighly sensitive reward signals. Empirical observations from simulations,\noffline and online experiments highlight the efficacy of our deployed approach.\n", "link": "http://arxiv.org/abs/2405.02141v1", "date": "2024-05-03", "relevancy": 1.8582, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5543}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4624}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Objective%20Recommendation%20via%20Multivariate%20Policy%20Learning&body=Title%3A%20Multi-Objective%20Recommendation%20via%20Multivariate%20Policy%20Learning%0AAuthor%3A%20Olivier%20Jeunen%20and%20Jatin%20Mandav%20and%20Ivan%20Potapov%20and%20Nakul%20Agarwal%20and%20Sourabh%20Vaid%20and%20Wenzhe%20Shi%20and%20Aleksei%20Ustimenko%0AAbstract%3A%20%20%20Real-world%20recommender%20systems%20often%20need%20to%20balance%20multiple%20objectives%20when%0Adeciding%20which%20recommendations%20to%20present%20to%20users.%20These%20include%20behavioural%0Asignals%20%28e.g.%20clicks%2C%20shares%2C%20dwell%20time%29%2C%20as%20well%20as%20broader%20objectives%20%28e.g.%0Adiversity%2C%20fairness%29.%20Scalarisation%20methods%20are%20commonly%20used%20to%20handle%20this%0Abalancing%20task%2C%20where%20a%20weighted%20average%20of%20per-objective%20reward%20signals%0Adetermines%20the%20final%20score%20used%20for%20ranking.%20Naturally%2C%20how%20these%20weights%20are%0Acomputed%20exactly%2C%20is%20key%20to%20success%20for%20any%20online%20platform.%20We%20frame%20this%20as%20a%0Adecision-making%20task%2C%20where%20the%20scalarisation%20weights%20are%20actions%20taken%20to%0Amaximise%20an%20overall%20North%20Star%20reward%20%28e.g.%20long-term%20user%20retention%20or%0Agrowth%29.%20We%20extend%20existing%20policy%20learning%20methods%20to%20the%20continuous%0Amultivariate%20action%20domain%2C%20proposing%20to%20maximise%20a%20pessimistic%20lower%20bound%20on%0Athe%20North%20Star%20reward%20that%20the%20learnt%20policy%20will%20yield.%20Typical%20lower%20bounds%0Abased%20on%20normal%20approximations%20suffer%20from%20insufficient%20coverage%2C%20and%20we%0Apropose%20an%20efficient%20and%20effective%20policy-dependent%20correction%20for%20this.%20We%0Aprovide%20guidance%20to%20design%20stochastic%20data%20collection%20policies%2C%20as%20well%20as%0Ahighly%20sensitive%20reward%20signals.%20Empirical%20observations%20from%20simulations%2C%0Aoffline%20and%20online%20experiments%20highlight%20the%20efficacy%20of%20our%20deployed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Objective%2520Recommendation%2520via%2520Multivariate%2520Policy%2520Learning%26entry.906535625%3DOlivier%2520Jeunen%2520and%2520Jatin%2520Mandav%2520and%2520Ivan%2520Potapov%2520and%2520Nakul%2520Agarwal%2520and%2520Sourabh%2520Vaid%2520and%2520Wenzhe%2520Shi%2520and%2520Aleksei%2520Ustimenko%26entry.1292438233%3D%2520%2520Real-world%2520recommender%2520systems%2520often%2520need%2520to%2520balance%2520multiple%2520objectives%2520when%250Adeciding%2520which%2520recommendations%2520to%2520present%2520to%2520users.%2520These%2520include%2520behavioural%250Asignals%2520%2528e.g.%2520clicks%252C%2520shares%252C%2520dwell%2520time%2529%252C%2520as%2520well%2520as%2520broader%2520objectives%2520%2528e.g.%250Adiversity%252C%2520fairness%2529.%2520Scalarisation%2520methods%2520are%2520commonly%2520used%2520to%2520handle%2520this%250Abalancing%2520task%252C%2520where%2520a%2520weighted%2520average%2520of%2520per-objective%2520reward%2520signals%250Adetermines%2520the%2520final%2520score%2520used%2520for%2520ranking.%2520Naturally%252C%2520how%2520these%2520weights%2520are%250Acomputed%2520exactly%252C%2520is%2520key%2520to%2520success%2520for%2520any%2520online%2520platform.%2520We%2520frame%2520this%2520as%2520a%250Adecision-making%2520task%252C%2520where%2520the%2520scalarisation%2520weights%2520are%2520actions%2520taken%2520to%250Amaximise%2520an%2520overall%2520North%2520Star%2520reward%2520%2528e.g.%2520long-term%2520user%2520retention%2520or%250Agrowth%2529.%2520We%2520extend%2520existing%2520policy%2520learning%2520methods%2520to%2520the%2520continuous%250Amultivariate%2520action%2520domain%252C%2520proposing%2520to%2520maximise%2520a%2520pessimistic%2520lower%2520bound%2520on%250Athe%2520North%2520Star%2520reward%2520that%2520the%2520learnt%2520policy%2520will%2520yield.%2520Typical%2520lower%2520bounds%250Abased%2520on%2520normal%2520approximations%2520suffer%2520from%2520insufficient%2520coverage%252C%2520and%2520we%250Apropose%2520an%2520efficient%2520and%2520effective%2520policy-dependent%2520correction%2520for%2520this.%2520We%250Aprovide%2520guidance%2520to%2520design%2520stochastic%2520data%2520collection%2520policies%252C%2520as%2520well%2520as%250Ahighly%2520sensitive%2520reward%2520signals.%2520Empirical%2520observations%2520from%2520simulations%252C%250Aoffline%2520and%2520online%2520experiments%2520highlight%2520the%2520efficacy%2520of%2520our%2520deployed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Objective%20Recommendation%20via%20Multivariate%20Policy%20Learning&entry.906535625=Olivier%20Jeunen%20and%20Jatin%20Mandav%20and%20Ivan%20Potapov%20and%20Nakul%20Agarwal%20and%20Sourabh%20Vaid%20and%20Wenzhe%20Shi%20and%20Aleksei%20Ustimenko&entry.1292438233=%20%20Real-world%20recommender%20systems%20often%20need%20to%20balance%20multiple%20objectives%20when%0Adeciding%20which%20recommendations%20to%20present%20to%20users.%20These%20include%20behavioural%0Asignals%20%28e.g.%20clicks%2C%20shares%2C%20dwell%20time%29%2C%20as%20well%20as%20broader%20objectives%20%28e.g.%0Adiversity%2C%20fairness%29.%20Scalarisation%20methods%20are%20commonly%20used%20to%20handle%20this%0Abalancing%20task%2C%20where%20a%20weighted%20average%20of%20per-objective%20reward%20signals%0Adetermines%20the%20final%20score%20used%20for%20ranking.%20Naturally%2C%20how%20these%20weights%20are%0Acomputed%20exactly%2C%20is%20key%20to%20success%20for%20any%20online%20platform.%20We%20frame%20this%20as%20a%0Adecision-making%20task%2C%20where%20the%20scalarisation%20weights%20are%20actions%20taken%20to%0Amaximise%20an%20overall%20North%20Star%20reward%20%28e.g.%20long-term%20user%20retention%20or%0Agrowth%29.%20We%20extend%20existing%20policy%20learning%20methods%20to%20the%20continuous%0Amultivariate%20action%20domain%2C%20proposing%20to%20maximise%20a%20pessimistic%20lower%20bound%20on%0Athe%20North%20Star%20reward%20that%20the%20learnt%20policy%20will%20yield.%20Typical%20lower%20bounds%0Abased%20on%20normal%20approximations%20suffer%20from%20insufficient%20coverage%2C%20and%20we%0Apropose%20an%20efficient%20and%20effective%20policy-dependent%20correction%20for%20this.%20We%0Aprovide%20guidance%20to%20design%20stochastic%20data%20collection%20policies%2C%20as%20well%20as%0Ahighly%20sensitive%20reward%20signals.%20Empirical%20observations%20from%20simulations%2C%0Aoffline%20and%20online%20experiments%20highlight%20the%20efficacy%20of%20our%20deployed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02141v1&entry.124074799=Read"},
{"title": "Discrete Aware Matrix Completion via Convexized $\\ell_0$-Norm\n  Approximation", "author": "Niclas F\u00fchrling and Kengo Ando and Giuseppe Thadeu Freitas de Abreu and David Gonz\u00e1lez G. and Osvaldo Gonsa", "abstract": "  We consider a novel algorithm, for the completion of partially observed\nlow-rank matrices in a structured setting where each entry can be chosen from a\nfinite discrete alphabet set, such as in common recommender systems. The\nproposed low-rank matrix completion (MC) method is an improved variation of\nstate-of-the-art (SotA) discrete aware matrix completion method which we\npreviously proposed, in which discreteness is enforced by an $\\ell_0$-norm\nregularizer, not by replaced with the $\\ell_1$-norm, but instead approximated\nby a continuous and differentiable function normalized via fractional\nprogramming (FP) under a proximal gradient (PG) framework. Simulation results\ndemonstrate the superior performance of the new method compared to the SotA\ntechniques as well as the earlier $\\ell_1$-norm-based discrete-aware matrix\ncompletion approach.\n", "link": "http://arxiv.org/abs/2405.02101v1", "date": "2024-05-03", "relevancy": 1.8555, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4818}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Aware%20Matrix%20Completion%20via%20Convexized%20%24%5Cell_0%24-Norm%0A%20%20Approximation&body=Title%3A%20Discrete%20Aware%20Matrix%20Completion%20via%20Convexized%20%24%5Cell_0%24-Norm%0A%20%20Approximation%0AAuthor%3A%20Niclas%20F%C3%BChrling%20and%20Kengo%20Ando%20and%20Giuseppe%20Thadeu%20Freitas%20de%20Abreu%20and%20David%20Gonz%C3%A1lez%20G.%20and%20Osvaldo%20Gonsa%0AAbstract%3A%20%20%20We%20consider%20a%20novel%20algorithm%2C%20for%20the%20completion%20of%20partially%20observed%0Alow-rank%20matrices%20in%20a%20structured%20setting%20where%20each%20entry%20can%20be%20chosen%20from%20a%0Afinite%20discrete%20alphabet%20set%2C%20such%20as%20in%20common%20recommender%20systems.%20The%0Aproposed%20low-rank%20matrix%20completion%20%28MC%29%20method%20is%20an%20improved%20variation%20of%0Astate-of-the-art%20%28SotA%29%20discrete%20aware%20matrix%20completion%20method%20which%20we%0Apreviously%20proposed%2C%20in%20which%20discreteness%20is%20enforced%20by%20an%20%24%5Cell_0%24-norm%0Aregularizer%2C%20not%20by%20replaced%20with%20the%20%24%5Cell_1%24-norm%2C%20but%20instead%20approximated%0Aby%20a%20continuous%20and%20differentiable%20function%20normalized%20via%20fractional%0Aprogramming%20%28FP%29%20under%20a%20proximal%20gradient%20%28PG%29%20framework.%20Simulation%20results%0Ademonstrate%20the%20superior%20performance%20of%20the%20new%20method%20compared%20to%20the%20SotA%0Atechniques%20as%20well%20as%20the%20earlier%20%24%5Cell_1%24-norm-based%20discrete-aware%20matrix%0Acompletion%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Aware%2520Matrix%2520Completion%2520via%2520Convexized%2520%2524%255Cell_0%2524-Norm%250A%2520%2520Approximation%26entry.906535625%3DNiclas%2520F%25C3%25BChrling%2520and%2520Kengo%2520Ando%2520and%2520Giuseppe%2520Thadeu%2520Freitas%2520de%2520Abreu%2520and%2520David%2520Gonz%25C3%25A1lez%2520G.%2520and%2520Osvaldo%2520Gonsa%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520novel%2520algorithm%252C%2520for%2520the%2520completion%2520of%2520partially%2520observed%250Alow-rank%2520matrices%2520in%2520a%2520structured%2520setting%2520where%2520each%2520entry%2520can%2520be%2520chosen%2520from%2520a%250Afinite%2520discrete%2520alphabet%2520set%252C%2520such%2520as%2520in%2520common%2520recommender%2520systems.%2520The%250Aproposed%2520low-rank%2520matrix%2520completion%2520%2528MC%2529%2520method%2520is%2520an%2520improved%2520variation%2520of%250Astate-of-the-art%2520%2528SotA%2529%2520discrete%2520aware%2520matrix%2520completion%2520method%2520which%2520we%250Apreviously%2520proposed%252C%2520in%2520which%2520discreteness%2520is%2520enforced%2520by%2520an%2520%2524%255Cell_0%2524-norm%250Aregularizer%252C%2520not%2520by%2520replaced%2520with%2520the%2520%2524%255Cell_1%2524-norm%252C%2520but%2520instead%2520approximated%250Aby%2520a%2520continuous%2520and%2520differentiable%2520function%2520normalized%2520via%2520fractional%250Aprogramming%2520%2528FP%2529%2520under%2520a%2520proximal%2520gradient%2520%2528PG%2529%2520framework.%2520Simulation%2520results%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520the%2520new%2520method%2520compared%2520to%2520the%2520SotA%250Atechniques%2520as%2520well%2520as%2520the%2520earlier%2520%2524%255Cell_1%2524-norm-based%2520discrete-aware%2520matrix%250Acompletion%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Aware%20Matrix%20Completion%20via%20Convexized%20%24%5Cell_0%24-Norm%0A%20%20Approximation&entry.906535625=Niclas%20F%C3%BChrling%20and%20Kengo%20Ando%20and%20Giuseppe%20Thadeu%20Freitas%20de%20Abreu%20and%20David%20Gonz%C3%A1lez%20G.%20and%20Osvaldo%20Gonsa&entry.1292438233=%20%20We%20consider%20a%20novel%20algorithm%2C%20for%20the%20completion%20of%20partially%20observed%0Alow-rank%20matrices%20in%20a%20structured%20setting%20where%20each%20entry%20can%20be%20chosen%20from%20a%0Afinite%20discrete%20alphabet%20set%2C%20such%20as%20in%20common%20recommender%20systems.%20The%0Aproposed%20low-rank%20matrix%20completion%20%28MC%29%20method%20is%20an%20improved%20variation%20of%0Astate-of-the-art%20%28SotA%29%20discrete%20aware%20matrix%20completion%20method%20which%20we%0Apreviously%20proposed%2C%20in%20which%20discreteness%20is%20enforced%20by%20an%20%24%5Cell_0%24-norm%0Aregularizer%2C%20not%20by%20replaced%20with%20the%20%24%5Cell_1%24-norm%2C%20but%20instead%20approximated%0Aby%20a%20continuous%20and%20differentiable%20function%20normalized%20via%20fractional%0Aprogramming%20%28FP%29%20under%20a%20proximal%20gradient%20%28PG%29%20framework.%20Simulation%20results%0Ademonstrate%20the%20superior%20performance%20of%20the%20new%20method%20compared%20to%20the%20SotA%0Atechniques%20as%20well%20as%20the%20earlier%20%24%5Cell_1%24-norm-based%20discrete-aware%20matrix%0Acompletion%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02101v1&entry.124074799=Read"},
{"title": "Dyna-Style Learning with A Macroscopic Model for Vehicle Platooning in\n  Mixed-Autonomy Traffic", "author": "Yichuan Zou and Li Jin and Xi Xiong", "abstract": "  Platooning of connected and autonomous vehicles (CAVs) plays a vital role in\nmodernizing highways, ushering in enhanced efficiency and safety. This paper\nexplores the significance of platooning in smart highways, employing a coupled\npartial differential equation (PDE) and ordinary differential equation (ODE)\nmodel to elucidate the complex interaction between bulk traffic flow and CAV\nplatoons. Our study focuses on developing a Dyna-style planning and learning\nframework tailored for platoon control, with a specific goal of reducing fuel\nconsumption. By harnessing the coupled PDE-ODE model, we improve data\nefficiency in Dyna-style learning through virtual experiences. Simulation\nresults validate the effectiveness of our macroscopic model in modeling\nplatoons within mixed-autonomy settings, demonstrating a notable $10.11\\%$\nreduction in vehicular fuel consumption compared to conventional approaches.\n", "link": "http://arxiv.org/abs/2405.02062v1", "date": "2024-05-03", "relevancy": 1.7956, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4662}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4478}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dyna-Style%20Learning%20with%20A%20Macroscopic%20Model%20for%20Vehicle%20Platooning%20in%0A%20%20Mixed-Autonomy%20Traffic&body=Title%3A%20Dyna-Style%20Learning%20with%20A%20Macroscopic%20Model%20for%20Vehicle%20Platooning%20in%0A%20%20Mixed-Autonomy%20Traffic%0AAuthor%3A%20Yichuan%20Zou%20and%20Li%20Jin%20and%20Xi%20Xiong%0AAbstract%3A%20%20%20Platooning%20of%20connected%20and%20autonomous%20vehicles%20%28CAVs%29%20plays%20a%20vital%20role%20in%0Amodernizing%20highways%2C%20ushering%20in%20enhanced%20efficiency%20and%20safety.%20This%20paper%0Aexplores%20the%20significance%20of%20platooning%20in%20smart%20highways%2C%20employing%20a%20coupled%0Apartial%20differential%20equation%20%28PDE%29%20and%20ordinary%20differential%20equation%20%28ODE%29%0Amodel%20to%20elucidate%20the%20complex%20interaction%20between%20bulk%20traffic%20flow%20and%20CAV%0Aplatoons.%20Our%20study%20focuses%20on%20developing%20a%20Dyna-style%20planning%20and%20learning%0Aframework%20tailored%20for%20platoon%20control%2C%20with%20a%20specific%20goal%20of%20reducing%20fuel%0Aconsumption.%20By%20harnessing%20the%20coupled%20PDE-ODE%20model%2C%20we%20improve%20data%0Aefficiency%20in%20Dyna-style%20learning%20through%20virtual%20experiences.%20Simulation%0Aresults%20validate%20the%20effectiveness%20of%20our%20macroscopic%20model%20in%20modeling%0Aplatoons%20within%20mixed-autonomy%20settings%2C%20demonstrating%20a%20notable%20%2410.11%5C%25%24%0Areduction%20in%20vehicular%20fuel%20consumption%20compared%20to%20conventional%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyna-Style%2520Learning%2520with%2520A%2520Macroscopic%2520Model%2520for%2520Vehicle%2520Platooning%2520in%250A%2520%2520Mixed-Autonomy%2520Traffic%26entry.906535625%3DYichuan%2520Zou%2520and%2520Li%2520Jin%2520and%2520Xi%2520Xiong%26entry.1292438233%3D%2520%2520Platooning%2520of%2520connected%2520and%2520autonomous%2520vehicles%2520%2528CAVs%2529%2520plays%2520a%2520vital%2520role%2520in%250Amodernizing%2520highways%252C%2520ushering%2520in%2520enhanced%2520efficiency%2520and%2520safety.%2520This%2520paper%250Aexplores%2520the%2520significance%2520of%2520platooning%2520in%2520smart%2520highways%252C%2520employing%2520a%2520coupled%250Apartial%2520differential%2520equation%2520%2528PDE%2529%2520and%2520ordinary%2520differential%2520equation%2520%2528ODE%2529%250Amodel%2520to%2520elucidate%2520the%2520complex%2520interaction%2520between%2520bulk%2520traffic%2520flow%2520and%2520CAV%250Aplatoons.%2520Our%2520study%2520focuses%2520on%2520developing%2520a%2520Dyna-style%2520planning%2520and%2520learning%250Aframework%2520tailored%2520for%2520platoon%2520control%252C%2520with%2520a%2520specific%2520goal%2520of%2520reducing%2520fuel%250Aconsumption.%2520By%2520harnessing%2520the%2520coupled%2520PDE-ODE%2520model%252C%2520we%2520improve%2520data%250Aefficiency%2520in%2520Dyna-style%2520learning%2520through%2520virtual%2520experiences.%2520Simulation%250Aresults%2520validate%2520the%2520effectiveness%2520of%2520our%2520macroscopic%2520model%2520in%2520modeling%250Aplatoons%2520within%2520mixed-autonomy%2520settings%252C%2520demonstrating%2520a%2520notable%2520%252410.11%255C%2525%2524%250Areduction%2520in%2520vehicular%2520fuel%2520consumption%2520compared%2520to%2520conventional%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dyna-Style%20Learning%20with%20A%20Macroscopic%20Model%20for%20Vehicle%20Platooning%20in%0A%20%20Mixed-Autonomy%20Traffic&entry.906535625=Yichuan%20Zou%20and%20Li%20Jin%20and%20Xi%20Xiong&entry.1292438233=%20%20Platooning%20of%20connected%20and%20autonomous%20vehicles%20%28CAVs%29%20plays%20a%20vital%20role%20in%0Amodernizing%20highways%2C%20ushering%20in%20enhanced%20efficiency%20and%20safety.%20This%20paper%0Aexplores%20the%20significance%20of%20platooning%20in%20smart%20highways%2C%20employing%20a%20coupled%0Apartial%20differential%20equation%20%28PDE%29%20and%20ordinary%20differential%20equation%20%28ODE%29%0Amodel%20to%20elucidate%20the%20complex%20interaction%20between%20bulk%20traffic%20flow%20and%20CAV%0Aplatoons.%20Our%20study%20focuses%20on%20developing%20a%20Dyna-style%20planning%20and%20learning%0Aframework%20tailored%20for%20platoon%20control%2C%20with%20a%20specific%20goal%20of%20reducing%20fuel%0Aconsumption.%20By%20harnessing%20the%20coupled%20PDE-ODE%20model%2C%20we%20improve%20data%0Aefficiency%20in%20Dyna-style%20learning%20through%20virtual%20experiences.%20Simulation%0Aresults%20validate%20the%20effectiveness%20of%20our%20macroscopic%20model%20in%20modeling%0Aplatoons%20within%20mixed-autonomy%20settings%2C%20demonstrating%20a%20notable%20%2410.11%5C%25%24%0Areduction%20in%20vehicular%20fuel%20consumption%20compared%20to%20conventional%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02062v1&entry.124074799=Read"},
{"title": "Imitation Learning in Discounted Linear MDPs without exploration\n  assumptions", "author": "Luca Viano and Stratis Skoulakis and Volkan Cevher", "abstract": "  We present a new algorithm for imitation learning in infinite horizon linear\nMDPs dubbed ILARL which greatly improves the bound on the number of\ntrajectories that the learner needs to sample from the environment. In\nparticular, we remove exploration assumptions required in previous works and we\nimprove the dependence on the desired accuracy $\\epsilon$ from\n$\\mathcal{O}\\br{\\epsilon^{-5}}$ to $\\mathcal{O}\\br{\\epsilon^{-4}}$. Our result\nrelies on a connection between imitation learning and online learning in MDPs\nwith adversarial losses. For the latter setting, we present the first result\nfor infinite horizon linear MDP which may be of independent interest. Moreover,\nwe are able to provide a strengthen result for the finite horizon case where we\nachieve $\\mathcal{O}\\br{\\epsilon^{-2}}$. Numerical experiments with linear\nfunction approximation shows that ILARL outperforms other commonly used\nalgorithms.\n", "link": "http://arxiv.org/abs/2405.02181v1", "date": "2024-05-03", "relevancy": 1.7764, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4553}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.449}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20Learning%20in%20Discounted%20Linear%20MDPs%20without%20exploration%0A%20%20assumptions&body=Title%3A%20Imitation%20Learning%20in%20Discounted%20Linear%20MDPs%20without%20exploration%0A%20%20assumptions%0AAuthor%3A%20Luca%20Viano%20and%20Stratis%20Skoulakis%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20We%20present%20a%20new%20algorithm%20for%20imitation%20learning%20in%20infinite%20horizon%20linear%0AMDPs%20dubbed%20ILARL%20which%20greatly%20improves%20the%20bound%20on%20the%20number%20of%0Atrajectories%20that%20the%20learner%20needs%20to%20sample%20from%20the%20environment.%20In%0Aparticular%2C%20we%20remove%20exploration%20assumptions%20required%20in%20previous%20works%20and%20we%0Aimprove%20the%20dependence%20on%20the%20desired%20accuracy%20%24%5Cepsilon%24%20from%0A%24%5Cmathcal%7BO%7D%5Cbr%7B%5Cepsilon%5E%7B-5%7D%7D%24%20to%20%24%5Cmathcal%7BO%7D%5Cbr%7B%5Cepsilon%5E%7B-4%7D%7D%24.%20Our%20result%0Arelies%20on%20a%20connection%20between%20imitation%20learning%20and%20online%20learning%20in%20MDPs%0Awith%20adversarial%20losses.%20For%20the%20latter%20setting%2C%20we%20present%20the%20first%20result%0Afor%20infinite%20horizon%20linear%20MDP%20which%20may%20be%20of%20independent%20interest.%20Moreover%2C%0Awe%20are%20able%20to%20provide%20a%20strengthen%20result%20for%20the%20finite%20horizon%20case%20where%20we%0Aachieve%20%24%5Cmathcal%7BO%7D%5Cbr%7B%5Cepsilon%5E%7B-2%7D%7D%24.%20Numerical%20experiments%20with%20linear%0Afunction%20approximation%20shows%20that%20ILARL%20outperforms%20other%20commonly%20used%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520Learning%2520in%2520Discounted%2520Linear%2520MDPs%2520without%2520exploration%250A%2520%2520assumptions%26entry.906535625%3DLuca%2520Viano%2520and%2520Stratis%2520Skoulakis%2520and%2520Volkan%2520Cevher%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520algorithm%2520for%2520imitation%2520learning%2520in%2520infinite%2520horizon%2520linear%250AMDPs%2520dubbed%2520ILARL%2520which%2520greatly%2520improves%2520the%2520bound%2520on%2520the%2520number%2520of%250Atrajectories%2520that%2520the%2520learner%2520needs%2520to%2520sample%2520from%2520the%2520environment.%2520In%250Aparticular%252C%2520we%2520remove%2520exploration%2520assumptions%2520required%2520in%2520previous%2520works%2520and%2520we%250Aimprove%2520the%2520dependence%2520on%2520the%2520desired%2520accuracy%2520%2524%255Cepsilon%2524%2520from%250A%2524%255Cmathcal%257BO%257D%255Cbr%257B%255Cepsilon%255E%257B-5%257D%257D%2524%2520to%2520%2524%255Cmathcal%257BO%257D%255Cbr%257B%255Cepsilon%255E%257B-4%257D%257D%2524.%2520Our%2520result%250Arelies%2520on%2520a%2520connection%2520between%2520imitation%2520learning%2520and%2520online%2520learning%2520in%2520MDPs%250Awith%2520adversarial%2520losses.%2520For%2520the%2520latter%2520setting%252C%2520we%2520present%2520the%2520first%2520result%250Afor%2520infinite%2520horizon%2520linear%2520MDP%2520which%2520may%2520be%2520of%2520independent%2520interest.%2520Moreover%252C%250Awe%2520are%2520able%2520to%2520provide%2520a%2520strengthen%2520result%2520for%2520the%2520finite%2520horizon%2520case%2520where%2520we%250Aachieve%2520%2524%255Cmathcal%257BO%257D%255Cbr%257B%255Cepsilon%255E%257B-2%257D%257D%2524.%2520Numerical%2520experiments%2520with%2520linear%250Afunction%2520approximation%2520shows%2520that%2520ILARL%2520outperforms%2520other%2520commonly%2520used%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Learning%20in%20Discounted%20Linear%20MDPs%20without%20exploration%0A%20%20assumptions&entry.906535625=Luca%20Viano%20and%20Stratis%20Skoulakis%20and%20Volkan%20Cevher&entry.1292438233=%20%20We%20present%20a%20new%20algorithm%20for%20imitation%20learning%20in%20infinite%20horizon%20linear%0AMDPs%20dubbed%20ILARL%20which%20greatly%20improves%20the%20bound%20on%20the%20number%20of%0Atrajectories%20that%20the%20learner%20needs%20to%20sample%20from%20the%20environment.%20In%0Aparticular%2C%20we%20remove%20exploration%20assumptions%20required%20in%20previous%20works%20and%20we%0Aimprove%20the%20dependence%20on%20the%20desired%20accuracy%20%24%5Cepsilon%24%20from%0A%24%5Cmathcal%7BO%7D%5Cbr%7B%5Cepsilon%5E%7B-5%7D%7D%24%20to%20%24%5Cmathcal%7BO%7D%5Cbr%7B%5Cepsilon%5E%7B-4%7D%7D%24.%20Our%20result%0Arelies%20on%20a%20connection%20between%20imitation%20learning%20and%20online%20learning%20in%20MDPs%0Awith%20adversarial%20losses.%20For%20the%20latter%20setting%2C%20we%20present%20the%20first%20result%0Afor%20infinite%20horizon%20linear%20MDP%20which%20may%20be%20of%20independent%20interest.%20Moreover%2C%0Awe%20are%20able%20to%20provide%20a%20strengthen%20result%20for%20the%20finite%20horizon%20case%20where%20we%0Aachieve%20%24%5Cmathcal%7BO%7D%5Cbr%7B%5Cepsilon%5E%7B-2%7D%7D%24.%20Numerical%20experiments%20with%20linear%0Afunction%20approximation%20shows%20that%20ILARL%20outperforms%20other%20commonly%20used%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02181v1&entry.124074799=Read"},
{"title": "A Penalty-Based Guardrail Algorithm for Non-Decreasing Optimization with\n  Inequality Constraints", "author": "Ksenija Stepanovic and Wendelin B\u00f6hmer and Mathijs de Weerdt", "abstract": "  Traditional mathematical programming solvers require long computational times\nto solve constrained minimization problems of complex and large-scale physical\nsystems. Therefore, these problems are often transformed into unconstrained\nones, and solved with computationally efficient optimization approaches based\non first-order information, such as the gradient descent method. However, for\nunconstrained problems, balancing the minimization of the objective function\nwith the reduction of constraint violations is challenging. We consider the\nclass of time-dependent minimization problems with increasing (possibly)\nnonlinear and non-convex objective function and non-decreasing (possibly)\nnonlinear and non-convex inequality constraints. To efficiently solve them, we\npropose a penalty-based guardrail algorithm (PGA). This algorithm adapts a\nstandard penalty-based method by dynamically updating the right-hand side of\nthe constraints with a guardrail variable which adds a margin to prevent\nviolations. We evaluate PGA on two novel application domains: a simplified\nmodel of a district heating system and an optimization model derived from\nlearned deep neural networks. Our method significantly outperforms mathematical\nprogramming solvers and the standard penalty-based method, and achieves better\nperformance and faster convergence than a state-of-the-art algorithm (IPDD)\nwithin a specified time limit.\n", "link": "http://arxiv.org/abs/2405.01984v1", "date": "2024-05-03", "relevancy": 1.758, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4604}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4376}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Penalty-Based%20Guardrail%20Algorithm%20for%20Non-Decreasing%20Optimization%20with%0A%20%20Inequality%20Constraints&body=Title%3A%20A%20Penalty-Based%20Guardrail%20Algorithm%20for%20Non-Decreasing%20Optimization%20with%0A%20%20Inequality%20Constraints%0AAuthor%3A%20Ksenija%20Stepanovic%20and%20Wendelin%20B%C3%B6hmer%20and%20Mathijs%20de%20Weerdt%0AAbstract%3A%20%20%20Traditional%20mathematical%20programming%20solvers%20require%20long%20computational%20times%0Ato%20solve%20constrained%20minimization%20problems%20of%20complex%20and%20large-scale%20physical%0Asystems.%20Therefore%2C%20these%20problems%20are%20often%20transformed%20into%20unconstrained%0Aones%2C%20and%20solved%20with%20computationally%20efficient%20optimization%20approaches%20based%0Aon%20first-order%20information%2C%20such%20as%20the%20gradient%20descent%20method.%20However%2C%20for%0Aunconstrained%20problems%2C%20balancing%20the%20minimization%20of%20the%20objective%20function%0Awith%20the%20reduction%20of%20constraint%20violations%20is%20challenging.%20We%20consider%20the%0Aclass%20of%20time-dependent%20minimization%20problems%20with%20increasing%20%28possibly%29%0Anonlinear%20and%20non-convex%20objective%20function%20and%20non-decreasing%20%28possibly%29%0Anonlinear%20and%20non-convex%20inequality%20constraints.%20To%20efficiently%20solve%20them%2C%20we%0Apropose%20a%20penalty-based%20guardrail%20algorithm%20%28PGA%29.%20This%20algorithm%20adapts%20a%0Astandard%20penalty-based%20method%20by%20dynamically%20updating%20the%20right-hand%20side%20of%0Athe%20constraints%20with%20a%20guardrail%20variable%20which%20adds%20a%20margin%20to%20prevent%0Aviolations.%20We%20evaluate%20PGA%20on%20two%20novel%20application%20domains%3A%20a%20simplified%0Amodel%20of%20a%20district%20heating%20system%20and%20an%20optimization%20model%20derived%20from%0Alearned%20deep%20neural%20networks.%20Our%20method%20significantly%20outperforms%20mathematical%0Aprogramming%20solvers%20and%20the%20standard%20penalty-based%20method%2C%20and%20achieves%20better%0Aperformance%20and%20faster%20convergence%20than%20a%20state-of-the-art%20algorithm%20%28IPDD%29%0Awithin%20a%20specified%20time%20limit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Penalty-Based%2520Guardrail%2520Algorithm%2520for%2520Non-Decreasing%2520Optimization%2520with%250A%2520%2520Inequality%2520Constraints%26entry.906535625%3DKsenija%2520Stepanovic%2520and%2520Wendelin%2520B%25C3%25B6hmer%2520and%2520Mathijs%2520de%2520Weerdt%26entry.1292438233%3D%2520%2520Traditional%2520mathematical%2520programming%2520solvers%2520require%2520long%2520computational%2520times%250Ato%2520solve%2520constrained%2520minimization%2520problems%2520of%2520complex%2520and%2520large-scale%2520physical%250Asystems.%2520Therefore%252C%2520these%2520problems%2520are%2520often%2520transformed%2520into%2520unconstrained%250Aones%252C%2520and%2520solved%2520with%2520computationally%2520efficient%2520optimization%2520approaches%2520based%250Aon%2520first-order%2520information%252C%2520such%2520as%2520the%2520gradient%2520descent%2520method.%2520However%252C%2520for%250Aunconstrained%2520problems%252C%2520balancing%2520the%2520minimization%2520of%2520the%2520objective%2520function%250Awith%2520the%2520reduction%2520of%2520constraint%2520violations%2520is%2520challenging.%2520We%2520consider%2520the%250Aclass%2520of%2520time-dependent%2520minimization%2520problems%2520with%2520increasing%2520%2528possibly%2529%250Anonlinear%2520and%2520non-convex%2520objective%2520function%2520and%2520non-decreasing%2520%2528possibly%2529%250Anonlinear%2520and%2520non-convex%2520inequality%2520constraints.%2520To%2520efficiently%2520solve%2520them%252C%2520we%250Apropose%2520a%2520penalty-based%2520guardrail%2520algorithm%2520%2528PGA%2529.%2520This%2520algorithm%2520adapts%2520a%250Astandard%2520penalty-based%2520method%2520by%2520dynamically%2520updating%2520the%2520right-hand%2520side%2520of%250Athe%2520constraints%2520with%2520a%2520guardrail%2520variable%2520which%2520adds%2520a%2520margin%2520to%2520prevent%250Aviolations.%2520We%2520evaluate%2520PGA%2520on%2520two%2520novel%2520application%2520domains%253A%2520a%2520simplified%250Amodel%2520of%2520a%2520district%2520heating%2520system%2520and%2520an%2520optimization%2520model%2520derived%2520from%250Alearned%2520deep%2520neural%2520networks.%2520Our%2520method%2520significantly%2520outperforms%2520mathematical%250Aprogramming%2520solvers%2520and%2520the%2520standard%2520penalty-based%2520method%252C%2520and%2520achieves%2520better%250Aperformance%2520and%2520faster%2520convergence%2520than%2520a%2520state-of-the-art%2520algorithm%2520%2528IPDD%2529%250Awithin%2520a%2520specified%2520time%2520limit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Penalty-Based%20Guardrail%20Algorithm%20for%20Non-Decreasing%20Optimization%20with%0A%20%20Inequality%20Constraints&entry.906535625=Ksenija%20Stepanovic%20and%20Wendelin%20B%C3%B6hmer%20and%20Mathijs%20de%20Weerdt&entry.1292438233=%20%20Traditional%20mathematical%20programming%20solvers%20require%20long%20computational%20times%0Ato%20solve%20constrained%20minimization%20problems%20of%20complex%20and%20large-scale%20physical%0Asystems.%20Therefore%2C%20these%20problems%20are%20often%20transformed%20into%20unconstrained%0Aones%2C%20and%20solved%20with%20computationally%20efficient%20optimization%20approaches%20based%0Aon%20first-order%20information%2C%20such%20as%20the%20gradient%20descent%20method.%20However%2C%20for%0Aunconstrained%20problems%2C%20balancing%20the%20minimization%20of%20the%20objective%20function%0Awith%20the%20reduction%20of%20constraint%20violations%20is%20challenging.%20We%20consider%20the%0Aclass%20of%20time-dependent%20minimization%20problems%20with%20increasing%20%28possibly%29%0Anonlinear%20and%20non-convex%20objective%20function%20and%20non-decreasing%20%28possibly%29%0Anonlinear%20and%20non-convex%20inequality%20constraints.%20To%20efficiently%20solve%20them%2C%20we%0Apropose%20a%20penalty-based%20guardrail%20algorithm%20%28PGA%29.%20This%20algorithm%20adapts%20a%0Astandard%20penalty-based%20method%20by%20dynamically%20updating%20the%20right-hand%20side%20of%0Athe%20constraints%20with%20a%20guardrail%20variable%20which%20adds%20a%20margin%20to%20prevent%0Aviolations.%20We%20evaluate%20PGA%20on%20two%20novel%20application%20domains%3A%20a%20simplified%0Amodel%20of%20a%20district%20heating%20system%20and%20an%20optimization%20model%20derived%20from%0Alearned%20deep%20neural%20networks.%20Our%20method%20significantly%20outperforms%20mathematical%0Aprogramming%20solvers%20and%20the%20standard%20penalty-based%20method%2C%20and%20achieves%20better%0Aperformance%20and%20faster%20convergence%20than%20a%20state-of-the-art%20algorithm%20%28IPDD%29%0Awithin%20a%20specified%20time%20limit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01984v1&entry.124074799=Read"},
{"title": "Utilizing Deep Learning to Optimize Software Development Processes", "author": "Keqin Li and Armando Zhu and Peng Zhao and Jintong Song and Jiabei Liu", "abstract": "  This study explores the application of deep learning technologies in software\ndevelopment processes, particularly in automating code reviews, error\nprediction, and test generation to enhance code quality and development\nefficiency. Through a series of empirical studies, experimental groups using\ndeep learning tools and control groups using traditional methods were compared\nin terms of code error rates and project completion times. The results\ndemonstrated significant improvements in the experimental group, validating the\neffectiveness of deep learning technologies. The research also discusses\npotential optimization points, methodologies, and technical challenges of deep\nlearning in software development, as well as how to integrate these\ntechnologies into existing software development workflows.\n", "link": "http://arxiv.org/abs/2404.13630v2", "date": "2024-05-03", "relevancy": 1.7577, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4581}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4338}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Deep%20Learning%20to%20Optimize%20Software%20Development%20Processes&body=Title%3A%20Utilizing%20Deep%20Learning%20to%20Optimize%20Software%20Development%20Processes%0AAuthor%3A%20Keqin%20Li%20and%20Armando%20Zhu%20and%20Peng%20Zhao%20and%20Jintong%20Song%20and%20Jiabei%20Liu%0AAbstract%3A%20%20%20This%20study%20explores%20the%20application%20of%20deep%20learning%20technologies%20in%20software%0Adevelopment%20processes%2C%20particularly%20in%20automating%20code%20reviews%2C%20error%0Aprediction%2C%20and%20test%20generation%20to%20enhance%20code%20quality%20and%20development%0Aefficiency.%20Through%20a%20series%20of%20empirical%20studies%2C%20experimental%20groups%20using%0Adeep%20learning%20tools%20and%20control%20groups%20using%20traditional%20methods%20were%20compared%0Ain%20terms%20of%20code%20error%20rates%20and%20project%20completion%20times.%20The%20results%0Ademonstrated%20significant%20improvements%20in%20the%20experimental%20group%2C%20validating%20the%0Aeffectiveness%20of%20deep%20learning%20technologies.%20The%20research%20also%20discusses%0Apotential%20optimization%20points%2C%20methodologies%2C%20and%20technical%20challenges%20of%20deep%0Alearning%20in%20software%20development%2C%20as%20well%20as%20how%20to%20integrate%20these%0Atechnologies%20into%20existing%20software%20development%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520Deep%2520Learning%2520to%2520Optimize%2520Software%2520Development%2520Processes%26entry.906535625%3DKeqin%2520Li%2520and%2520Armando%2520Zhu%2520and%2520Peng%2520Zhao%2520and%2520Jintong%2520Song%2520and%2520Jiabei%2520Liu%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520application%2520of%2520deep%2520learning%2520technologies%2520in%2520software%250Adevelopment%2520processes%252C%2520particularly%2520in%2520automating%2520code%2520reviews%252C%2520error%250Aprediction%252C%2520and%2520test%2520generation%2520to%2520enhance%2520code%2520quality%2520and%2520development%250Aefficiency.%2520Through%2520a%2520series%2520of%2520empirical%2520studies%252C%2520experimental%2520groups%2520using%250Adeep%2520learning%2520tools%2520and%2520control%2520groups%2520using%2520traditional%2520methods%2520were%2520compared%250Ain%2520terms%2520of%2520code%2520error%2520rates%2520and%2520project%2520completion%2520times.%2520The%2520results%250Ademonstrated%2520significant%2520improvements%2520in%2520the%2520experimental%2520group%252C%2520validating%2520the%250Aeffectiveness%2520of%2520deep%2520learning%2520technologies.%2520The%2520research%2520also%2520discusses%250Apotential%2520optimization%2520points%252C%2520methodologies%252C%2520and%2520technical%2520challenges%2520of%2520deep%250Alearning%2520in%2520software%2520development%252C%2520as%2520well%2520as%2520how%2520to%2520integrate%2520these%250Atechnologies%2520into%2520existing%2520software%2520development%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Deep%20Learning%20to%20Optimize%20Software%20Development%20Processes&entry.906535625=Keqin%20Li%20and%20Armando%20Zhu%20and%20Peng%20Zhao%20and%20Jintong%20Song%20and%20Jiabei%20Liu&entry.1292438233=%20%20This%20study%20explores%20the%20application%20of%20deep%20learning%20technologies%20in%20software%0Adevelopment%20processes%2C%20particularly%20in%20automating%20code%20reviews%2C%20error%0Aprediction%2C%20and%20test%20generation%20to%20enhance%20code%20quality%20and%20development%0Aefficiency.%20Through%20a%20series%20of%20empirical%20studies%2C%20experimental%20groups%20using%0Adeep%20learning%20tools%20and%20control%20groups%20using%20traditional%20methods%20were%20compared%0Ain%20terms%20of%20code%20error%20rates%20and%20project%20completion%20times.%20The%20results%0Ademonstrated%20significant%20improvements%20in%20the%20experimental%20group%2C%20validating%20the%0Aeffectiveness%20of%20deep%20learning%20technologies.%20The%20research%20also%20discusses%0Apotential%20optimization%20points%2C%20methodologies%2C%20and%20technical%20challenges%20of%20deep%0Alearning%20in%20software%20development%2C%20as%20well%20as%20how%20to%20integrate%20these%0Atechnologies%20into%20existing%20software%20development%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13630v2&entry.124074799=Read"},
{"title": "Metalearners for Ranking Treatment Effects", "author": "Toon Vanderschueren and Wouter Verbeke and Felipe Moraes and Hugo Manuel Proen\u00e7a", "abstract": "  Efficiently allocating treatments with a budget constraint constitutes an\nimportant challenge across various domains. In marketing, for example, the use\nof promotions to target potential customers and boost conversions is limited by\nthe available budget. While much research focuses on estimating causal effects,\nthere is relatively limited work on learning to allocate treatments while\nconsidering the operational context. Existing methods for uplift modeling or\ncausal inference primarily estimate treatment effects, without considering how\nthis relates to a profit maximizing allocation policy that respects budget\nconstraints. The potential downside of using these methods is that the\nresulting predictive model is not aligned with the operational context.\nTherefore, prediction errors are propagated to the optimization of the budget\nallocation problem, subsequently leading to a suboptimal allocation policy. We\npropose an alternative approach based on learning to rank. Our proposed\nmethodology directly learns an allocation policy by prioritizing instances in\nterms of their incremental profit. We propose an efficient sampling procedure\nfor the optimization of the ranking model to scale our methodology to\nlarge-scale data sets. Theoretically, we show how learning to rank can maximize\nthe area under a policy's incremental profit curve. Empirically, we validate\nour methodology and show its effectiveness in practice through a series of\nexperiments on both synthetic and real-world data.\n", "link": "http://arxiv.org/abs/2405.02183v1", "date": "2024-05-03", "relevancy": 1.7505, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4467}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4342}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metalearners%20for%20Ranking%20Treatment%20Effects&body=Title%3A%20Metalearners%20for%20Ranking%20Treatment%20Effects%0AAuthor%3A%20Toon%20Vanderschueren%20and%20Wouter%20Verbeke%20and%20Felipe%20Moraes%20and%20Hugo%20Manuel%20Proen%C3%A7a%0AAbstract%3A%20%20%20Efficiently%20allocating%20treatments%20with%20a%20budget%20constraint%20constitutes%20an%0Aimportant%20challenge%20across%20various%20domains.%20In%20marketing%2C%20for%20example%2C%20the%20use%0Aof%20promotions%20to%20target%20potential%20customers%20and%20boost%20conversions%20is%20limited%20by%0Athe%20available%20budget.%20While%20much%20research%20focuses%20on%20estimating%20causal%20effects%2C%0Athere%20is%20relatively%20limited%20work%20on%20learning%20to%20allocate%20treatments%20while%0Aconsidering%20the%20operational%20context.%20Existing%20methods%20for%20uplift%20modeling%20or%0Acausal%20inference%20primarily%20estimate%20treatment%20effects%2C%20without%20considering%20how%0Athis%20relates%20to%20a%20profit%20maximizing%20allocation%20policy%20that%20respects%20budget%0Aconstraints.%20The%20potential%20downside%20of%20using%20these%20methods%20is%20that%20the%0Aresulting%20predictive%20model%20is%20not%20aligned%20with%20the%20operational%20context.%0ATherefore%2C%20prediction%20errors%20are%20propagated%20to%20the%20optimization%20of%20the%20budget%0Aallocation%20problem%2C%20subsequently%20leading%20to%20a%20suboptimal%20allocation%20policy.%20We%0Apropose%20an%20alternative%20approach%20based%20on%20learning%20to%20rank.%20Our%20proposed%0Amethodology%20directly%20learns%20an%20allocation%20policy%20by%20prioritizing%20instances%20in%0Aterms%20of%20their%20incremental%20profit.%20We%20propose%20an%20efficient%20sampling%20procedure%0Afor%20the%20optimization%20of%20the%20ranking%20model%20to%20scale%20our%20methodology%20to%0Alarge-scale%20data%20sets.%20Theoretically%2C%20we%20show%20how%20learning%20to%20rank%20can%20maximize%0Athe%20area%20under%20a%20policy%27s%20incremental%20profit%20curve.%20Empirically%2C%20we%20validate%0Aour%20methodology%20and%20show%20its%20effectiveness%20in%20practice%20through%20a%20series%20of%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetalearners%2520for%2520Ranking%2520Treatment%2520Effects%26entry.906535625%3DToon%2520Vanderschueren%2520and%2520Wouter%2520Verbeke%2520and%2520Felipe%2520Moraes%2520and%2520Hugo%2520Manuel%2520Proen%25C3%25A7a%26entry.1292438233%3D%2520%2520Efficiently%2520allocating%2520treatments%2520with%2520a%2520budget%2520constraint%2520constitutes%2520an%250Aimportant%2520challenge%2520across%2520various%2520domains.%2520In%2520marketing%252C%2520for%2520example%252C%2520the%2520use%250Aof%2520promotions%2520to%2520target%2520potential%2520customers%2520and%2520boost%2520conversions%2520is%2520limited%2520by%250Athe%2520available%2520budget.%2520While%2520much%2520research%2520focuses%2520on%2520estimating%2520causal%2520effects%252C%250Athere%2520is%2520relatively%2520limited%2520work%2520on%2520learning%2520to%2520allocate%2520treatments%2520while%250Aconsidering%2520the%2520operational%2520context.%2520Existing%2520methods%2520for%2520uplift%2520modeling%2520or%250Acausal%2520inference%2520primarily%2520estimate%2520treatment%2520effects%252C%2520without%2520considering%2520how%250Athis%2520relates%2520to%2520a%2520profit%2520maximizing%2520allocation%2520policy%2520that%2520respects%2520budget%250Aconstraints.%2520The%2520potential%2520downside%2520of%2520using%2520these%2520methods%2520is%2520that%2520the%250Aresulting%2520predictive%2520model%2520is%2520not%2520aligned%2520with%2520the%2520operational%2520context.%250ATherefore%252C%2520prediction%2520errors%2520are%2520propagated%2520to%2520the%2520optimization%2520of%2520the%2520budget%250Aallocation%2520problem%252C%2520subsequently%2520leading%2520to%2520a%2520suboptimal%2520allocation%2520policy.%2520We%250Apropose%2520an%2520alternative%2520approach%2520based%2520on%2520learning%2520to%2520rank.%2520Our%2520proposed%250Amethodology%2520directly%2520learns%2520an%2520allocation%2520policy%2520by%2520prioritizing%2520instances%2520in%250Aterms%2520of%2520their%2520incremental%2520profit.%2520We%2520propose%2520an%2520efficient%2520sampling%2520procedure%250Afor%2520the%2520optimization%2520of%2520the%2520ranking%2520model%2520to%2520scale%2520our%2520methodology%2520to%250Alarge-scale%2520data%2520sets.%2520Theoretically%252C%2520we%2520show%2520how%2520learning%2520to%2520rank%2520can%2520maximize%250Athe%2520area%2520under%2520a%2520policy%2527s%2520incremental%2520profit%2520curve.%2520Empirically%252C%2520we%2520validate%250Aour%2520methodology%2520and%2520show%2520its%2520effectiveness%2520in%2520practice%2520through%2520a%2520series%2520of%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metalearners%20for%20Ranking%20Treatment%20Effects&entry.906535625=Toon%20Vanderschueren%20and%20Wouter%20Verbeke%20and%20Felipe%20Moraes%20and%20Hugo%20Manuel%20Proen%C3%A7a&entry.1292438233=%20%20Efficiently%20allocating%20treatments%20with%20a%20budget%20constraint%20constitutes%20an%0Aimportant%20challenge%20across%20various%20domains.%20In%20marketing%2C%20for%20example%2C%20the%20use%0Aof%20promotions%20to%20target%20potential%20customers%20and%20boost%20conversions%20is%20limited%20by%0Athe%20available%20budget.%20While%20much%20research%20focuses%20on%20estimating%20causal%20effects%2C%0Athere%20is%20relatively%20limited%20work%20on%20learning%20to%20allocate%20treatments%20while%0Aconsidering%20the%20operational%20context.%20Existing%20methods%20for%20uplift%20modeling%20or%0Acausal%20inference%20primarily%20estimate%20treatment%20effects%2C%20without%20considering%20how%0Athis%20relates%20to%20a%20profit%20maximizing%20allocation%20policy%20that%20respects%20budget%0Aconstraints.%20The%20potential%20downside%20of%20using%20these%20methods%20is%20that%20the%0Aresulting%20predictive%20model%20is%20not%20aligned%20with%20the%20operational%20context.%0ATherefore%2C%20prediction%20errors%20are%20propagated%20to%20the%20optimization%20of%20the%20budget%0Aallocation%20problem%2C%20subsequently%20leading%20to%20a%20suboptimal%20allocation%20policy.%20We%0Apropose%20an%20alternative%20approach%20based%20on%20learning%20to%20rank.%20Our%20proposed%0Amethodology%20directly%20learns%20an%20allocation%20policy%20by%20prioritizing%20instances%20in%0Aterms%20of%20their%20incremental%20profit.%20We%20propose%20an%20efficient%20sampling%20procedure%0Afor%20the%20optimization%20of%20the%20ranking%20model%20to%20scale%20our%20methodology%20to%0Alarge-scale%20data%20sets.%20Theoretically%2C%20we%20show%20how%20learning%20to%20rank%20can%20maximize%0Athe%20area%20under%20a%20policy%27s%20incremental%20profit%20curve.%20Empirically%2C%20we%20validate%0Aour%20methodology%20and%20show%20its%20effectiveness%20in%20practice%20through%20a%20series%20of%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02183v1&entry.124074799=Read"},
{"title": "Mathematics of statistical sequential decision-making: concentration,\n  risk-awareness and modelling in stochastic bandits, with applications to\n  bariatric surgery", "author": "Patrick Saux", "abstract": "  This thesis aims to study some of the mathematical challenges that arise in\nthe analysis of statistical sequential decision-making algorithms for\npostoperative patients follow-up. Stochastic bandits (multiarmed, contextual)\nmodel the learning of a sequence of actions (policy) by an agent in an\nuncertain environment in order to maximise observed rewards. To learn optimal\npolicies, bandit algorithms have to balance the exploitation of current\nknowledge and the exploration of uncertain actions. Such algorithms have\nlargely been studied and deployed in industrial applications with large\ndatasets, low-risk decisions and clear modelling assumptions, such as\nclickthrough rate maximisation in online advertising. By contrast, digital\nhealth recommendations call for a whole new paradigm of small samples,\nrisk-averse agents and complex, nonparametric modelling. To this end, we\ndeveloped new safe, anytime-valid concentration bounds, (Bregman, empirical\nChernoff), introduced a new framework for risk-aware contextual bandits (with\nelicitable risk measures) and analysed a novel class of nonparametric bandit\nalgorithms under weak assumptions (Dirichlet sampling). In addition to the\ntheoretical guarantees, these results are supported by in-depth empirical\nevidence. Finally, as a first step towards personalised postoperative follow-up\nrecommendations, we developed with medical doctors and surgeons an\ninterpretable machine learning model to predict the long-term weight\ntrajectories of patients after bariatric surgery.\n", "link": "http://arxiv.org/abs/2405.01994v1", "date": "2024-05-03", "relevancy": 1.7436, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4257}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mathematics%20of%20statistical%20sequential%20decision-making%3A%20concentration%2C%0A%20%20risk-awareness%20and%20modelling%20in%20stochastic%20bandits%2C%20with%20applications%20to%0A%20%20bariatric%20surgery&body=Title%3A%20Mathematics%20of%20statistical%20sequential%20decision-making%3A%20concentration%2C%0A%20%20risk-awareness%20and%20modelling%20in%20stochastic%20bandits%2C%20with%20applications%20to%0A%20%20bariatric%20surgery%0AAuthor%3A%20Patrick%20Saux%0AAbstract%3A%20%20%20This%20thesis%20aims%20to%20study%20some%20of%20the%20mathematical%20challenges%20that%20arise%20in%0Athe%20analysis%20of%20statistical%20sequential%20decision-making%20algorithms%20for%0Apostoperative%20patients%20follow-up.%20Stochastic%20bandits%20%28multiarmed%2C%20contextual%29%0Amodel%20the%20learning%20of%20a%20sequence%20of%20actions%20%28policy%29%20by%20an%20agent%20in%20an%0Auncertain%20environment%20in%20order%20to%20maximise%20observed%20rewards.%20To%20learn%20optimal%0Apolicies%2C%20bandit%20algorithms%20have%20to%20balance%20the%20exploitation%20of%20current%0Aknowledge%20and%20the%20exploration%20of%20uncertain%20actions.%20Such%20algorithms%20have%0Alargely%20been%20studied%20and%20deployed%20in%20industrial%20applications%20with%20large%0Adatasets%2C%20low-risk%20decisions%20and%20clear%20modelling%20assumptions%2C%20such%20as%0Aclickthrough%20rate%20maximisation%20in%20online%20advertising.%20By%20contrast%2C%20digital%0Ahealth%20recommendations%20call%20for%20a%20whole%20new%20paradigm%20of%20small%20samples%2C%0Arisk-averse%20agents%20and%20complex%2C%20nonparametric%20modelling.%20To%20this%20end%2C%20we%0Adeveloped%20new%20safe%2C%20anytime-valid%20concentration%20bounds%2C%20%28Bregman%2C%20empirical%0AChernoff%29%2C%20introduced%20a%20new%20framework%20for%20risk-aware%20contextual%20bandits%20%28with%0Aelicitable%20risk%20measures%29%20and%20analysed%20a%20novel%20class%20of%20nonparametric%20bandit%0Aalgorithms%20under%20weak%20assumptions%20%28Dirichlet%20sampling%29.%20In%20addition%20to%20the%0Atheoretical%20guarantees%2C%20these%20results%20are%20supported%20by%20in-depth%20empirical%0Aevidence.%20Finally%2C%20as%20a%20first%20step%20towards%20personalised%20postoperative%20follow-up%0Arecommendations%2C%20we%20developed%20with%20medical%20doctors%20and%20surgeons%20an%0Ainterpretable%20machine%20learning%20model%20to%20predict%20the%20long-term%20weight%0Atrajectories%20of%20patients%20after%20bariatric%20surgery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathematics%2520of%2520statistical%2520sequential%2520decision-making%253A%2520concentration%252C%250A%2520%2520risk-awareness%2520and%2520modelling%2520in%2520stochastic%2520bandits%252C%2520with%2520applications%2520to%250A%2520%2520bariatric%2520surgery%26entry.906535625%3DPatrick%2520Saux%26entry.1292438233%3D%2520%2520This%2520thesis%2520aims%2520to%2520study%2520some%2520of%2520the%2520mathematical%2520challenges%2520that%2520arise%2520in%250Athe%2520analysis%2520of%2520statistical%2520sequential%2520decision-making%2520algorithms%2520for%250Apostoperative%2520patients%2520follow-up.%2520Stochastic%2520bandits%2520%2528multiarmed%252C%2520contextual%2529%250Amodel%2520the%2520learning%2520of%2520a%2520sequence%2520of%2520actions%2520%2528policy%2529%2520by%2520an%2520agent%2520in%2520an%250Auncertain%2520environment%2520in%2520order%2520to%2520maximise%2520observed%2520rewards.%2520To%2520learn%2520optimal%250Apolicies%252C%2520bandit%2520algorithms%2520have%2520to%2520balance%2520the%2520exploitation%2520of%2520current%250Aknowledge%2520and%2520the%2520exploration%2520of%2520uncertain%2520actions.%2520Such%2520algorithms%2520have%250Alargely%2520been%2520studied%2520and%2520deployed%2520in%2520industrial%2520applications%2520with%2520large%250Adatasets%252C%2520low-risk%2520decisions%2520and%2520clear%2520modelling%2520assumptions%252C%2520such%2520as%250Aclickthrough%2520rate%2520maximisation%2520in%2520online%2520advertising.%2520By%2520contrast%252C%2520digital%250Ahealth%2520recommendations%2520call%2520for%2520a%2520whole%2520new%2520paradigm%2520of%2520small%2520samples%252C%250Arisk-averse%2520agents%2520and%2520complex%252C%2520nonparametric%2520modelling.%2520To%2520this%2520end%252C%2520we%250Adeveloped%2520new%2520safe%252C%2520anytime-valid%2520concentration%2520bounds%252C%2520%2528Bregman%252C%2520empirical%250AChernoff%2529%252C%2520introduced%2520a%2520new%2520framework%2520for%2520risk-aware%2520contextual%2520bandits%2520%2528with%250Aelicitable%2520risk%2520measures%2529%2520and%2520analysed%2520a%2520novel%2520class%2520of%2520nonparametric%2520bandit%250Aalgorithms%2520under%2520weak%2520assumptions%2520%2528Dirichlet%2520sampling%2529.%2520In%2520addition%2520to%2520the%250Atheoretical%2520guarantees%252C%2520these%2520results%2520are%2520supported%2520by%2520in-depth%2520empirical%250Aevidence.%2520Finally%252C%2520as%2520a%2520first%2520step%2520towards%2520personalised%2520postoperative%2520follow-up%250Arecommendations%252C%2520we%2520developed%2520with%2520medical%2520doctors%2520and%2520surgeons%2520an%250Ainterpretable%2520machine%2520learning%2520model%2520to%2520predict%2520the%2520long-term%2520weight%250Atrajectories%2520of%2520patients%2520after%2520bariatric%2520surgery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mathematics%20of%20statistical%20sequential%20decision-making%3A%20concentration%2C%0A%20%20risk-awareness%20and%20modelling%20in%20stochastic%20bandits%2C%20with%20applications%20to%0A%20%20bariatric%20surgery&entry.906535625=Patrick%20Saux&entry.1292438233=%20%20This%20thesis%20aims%20to%20study%20some%20of%20the%20mathematical%20challenges%20that%20arise%20in%0Athe%20analysis%20of%20statistical%20sequential%20decision-making%20algorithms%20for%0Apostoperative%20patients%20follow-up.%20Stochastic%20bandits%20%28multiarmed%2C%20contextual%29%0Amodel%20the%20learning%20of%20a%20sequence%20of%20actions%20%28policy%29%20by%20an%20agent%20in%20an%0Auncertain%20environment%20in%20order%20to%20maximise%20observed%20rewards.%20To%20learn%20optimal%0Apolicies%2C%20bandit%20algorithms%20have%20to%20balance%20the%20exploitation%20of%20current%0Aknowledge%20and%20the%20exploration%20of%20uncertain%20actions.%20Such%20algorithms%20have%0Alargely%20been%20studied%20and%20deployed%20in%20industrial%20applications%20with%20large%0Adatasets%2C%20low-risk%20decisions%20and%20clear%20modelling%20assumptions%2C%20such%20as%0Aclickthrough%20rate%20maximisation%20in%20online%20advertising.%20By%20contrast%2C%20digital%0Ahealth%20recommendations%20call%20for%20a%20whole%20new%20paradigm%20of%20small%20samples%2C%0Arisk-averse%20agents%20and%20complex%2C%20nonparametric%20modelling.%20To%20this%20end%2C%20we%0Adeveloped%20new%20safe%2C%20anytime-valid%20concentration%20bounds%2C%20%28Bregman%2C%20empirical%0AChernoff%29%2C%20introduced%20a%20new%20framework%20for%20risk-aware%20contextual%20bandits%20%28with%0Aelicitable%20risk%20measures%29%20and%20analysed%20a%20novel%20class%20of%20nonparametric%20bandit%0Aalgorithms%20under%20weak%20assumptions%20%28Dirichlet%20sampling%29.%20In%20addition%20to%20the%0Atheoretical%20guarantees%2C%20these%20results%20are%20supported%20by%20in-depth%20empirical%0Aevidence.%20Finally%2C%20as%20a%20first%20step%20towards%20personalised%20postoperative%20follow-up%0Arecommendations%2C%20we%20developed%20with%20medical%20doctors%20and%20surgeons%20an%0Ainterpretable%20machine%20learning%20model%20to%20predict%20the%20long-term%20weight%0Atrajectories%20of%20patients%20after%20bariatric%20surgery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01994v1&entry.124074799=Read"},
{"title": "M${^2}$Depth: Self-supervised Two-Frame Multi-camera Metric Depth\n  Estimation", "author": "Yingshuang Zou and Yikang Ding and Xi Qiu and Haoqian Wang and Haotian Zhang", "abstract": "  This paper presents a novel self-supervised two-frame multi-camera metric\ndepth estimation network, termed M${^2}$Depth, which is designed to predict\nreliable scale-aware surrounding depth in autonomous driving. Unlike the\nprevious works that use multi-view images from a single time-step or multiple\ntime-step images from a single camera, M${^2}$Depth takes temporally adjacent\ntwo-frame images from multiple cameras as inputs and produces high-quality\nsurrounding depth. We first construct cost volumes in spatial and temporal\ndomains individually and propose a spatial-temporal fusion module that\nintegrates the spatial-temporal information to yield a strong volume\npresentation. We additionally combine the neural prior from SAM features with\ninternal features to reduce the ambiguity between foreground and background and\nstrengthen the depth edges. Extensive experimental results on nuScenes and DDAD\nbenchmarks show M${^2}$Depth achieves state-of-the-art performance. More\nresults can be found in https://heiheishuang.xyz/M2Depth .\n", "link": "http://arxiv.org/abs/2405.02004v1", "date": "2024-05-03", "relevancy": 1.7171, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5763}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%24%7B%5E2%7D%24Depth%3A%20Self-supervised%20Two-Frame%20Multi-camera%20Metric%20Depth%0A%20%20Estimation&body=Title%3A%20M%24%7B%5E2%7D%24Depth%3A%20Self-supervised%20Two-Frame%20Multi-camera%20Metric%20Depth%0A%20%20Estimation%0AAuthor%3A%20Yingshuang%20Zou%20and%20Yikang%20Ding%20and%20Xi%20Qiu%20and%20Haoqian%20Wang%20and%20Haotian%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20self-supervised%20two-frame%20multi-camera%20metric%0Adepth%20estimation%20network%2C%20termed%20M%24%7B%5E2%7D%24Depth%2C%20which%20is%20designed%20to%20predict%0Areliable%20scale-aware%20surrounding%20depth%20in%20autonomous%20driving.%20Unlike%20the%0Aprevious%20works%20that%20use%20multi-view%20images%20from%20a%20single%20time-step%20or%20multiple%0Atime-step%20images%20from%20a%20single%20camera%2C%20M%24%7B%5E2%7D%24Depth%20takes%20temporally%20adjacent%0Atwo-frame%20images%20from%20multiple%20cameras%20as%20inputs%20and%20produces%20high-quality%0Asurrounding%20depth.%20We%20first%20construct%20cost%20volumes%20in%20spatial%20and%20temporal%0Adomains%20individually%20and%20propose%20a%20spatial-temporal%20fusion%20module%20that%0Aintegrates%20the%20spatial-temporal%20information%20to%20yield%20a%20strong%20volume%0Apresentation.%20We%20additionally%20combine%20the%20neural%20prior%20from%20SAM%20features%20with%0Ainternal%20features%20to%20reduce%20the%20ambiguity%20between%20foreground%20and%20background%20and%0Astrengthen%20the%20depth%20edges.%20Extensive%20experimental%20results%20on%20nuScenes%20and%20DDAD%0Abenchmarks%20show%20M%24%7B%5E2%7D%24Depth%20achieves%20state-of-the-art%20performance.%20More%0Aresults%20can%20be%20found%20in%20https%3A//heiheishuang.xyz/M2Depth%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%2524%257B%255E2%257D%2524Depth%253A%2520Self-supervised%2520Two-Frame%2520Multi-camera%2520Metric%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DYingshuang%2520Zou%2520and%2520Yikang%2520Ding%2520and%2520Xi%2520Qiu%2520and%2520Haoqian%2520Wang%2520and%2520Haotian%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520self-supervised%2520two-frame%2520multi-camera%2520metric%250Adepth%2520estimation%2520network%252C%2520termed%2520M%2524%257B%255E2%257D%2524Depth%252C%2520which%2520is%2520designed%2520to%2520predict%250Areliable%2520scale-aware%2520surrounding%2520depth%2520in%2520autonomous%2520driving.%2520Unlike%2520the%250Aprevious%2520works%2520that%2520use%2520multi-view%2520images%2520from%2520a%2520single%2520time-step%2520or%2520multiple%250Atime-step%2520images%2520from%2520a%2520single%2520camera%252C%2520M%2524%257B%255E2%257D%2524Depth%2520takes%2520temporally%2520adjacent%250Atwo-frame%2520images%2520from%2520multiple%2520cameras%2520as%2520inputs%2520and%2520produces%2520high-quality%250Asurrounding%2520depth.%2520We%2520first%2520construct%2520cost%2520volumes%2520in%2520spatial%2520and%2520temporal%250Adomains%2520individually%2520and%2520propose%2520a%2520spatial-temporal%2520fusion%2520module%2520that%250Aintegrates%2520the%2520spatial-temporal%2520information%2520to%2520yield%2520a%2520strong%2520volume%250Apresentation.%2520We%2520additionally%2520combine%2520the%2520neural%2520prior%2520from%2520SAM%2520features%2520with%250Ainternal%2520features%2520to%2520reduce%2520the%2520ambiguity%2520between%2520foreground%2520and%2520background%2520and%250Astrengthen%2520the%2520depth%2520edges.%2520Extensive%2520experimental%2520results%2520on%2520nuScenes%2520and%2520DDAD%250Abenchmarks%2520show%2520M%2524%257B%255E2%257D%2524Depth%2520achieves%2520state-of-the-art%2520performance.%2520More%250Aresults%2520can%2520be%2520found%2520in%2520https%253A//heiheishuang.xyz/M2Depth%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%24%7B%5E2%7D%24Depth%3A%20Self-supervised%20Two-Frame%20Multi-camera%20Metric%20Depth%0A%20%20Estimation&entry.906535625=Yingshuang%20Zou%20and%20Yikang%20Ding%20and%20Xi%20Qiu%20and%20Haoqian%20Wang%20and%20Haotian%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20self-supervised%20two-frame%20multi-camera%20metric%0Adepth%20estimation%20network%2C%20termed%20M%24%7B%5E2%7D%24Depth%2C%20which%20is%20designed%20to%20predict%0Areliable%20scale-aware%20surrounding%20depth%20in%20autonomous%20driving.%20Unlike%20the%0Aprevious%20works%20that%20use%20multi-view%20images%20from%20a%20single%20time-step%20or%20multiple%0Atime-step%20images%20from%20a%20single%20camera%2C%20M%24%7B%5E2%7D%24Depth%20takes%20temporally%20adjacent%0Atwo-frame%20images%20from%20multiple%20cameras%20as%20inputs%20and%20produces%20high-quality%0Asurrounding%20depth.%20We%20first%20construct%20cost%20volumes%20in%20spatial%20and%20temporal%0Adomains%20individually%20and%20propose%20a%20spatial-temporal%20fusion%20module%20that%0Aintegrates%20the%20spatial-temporal%20information%20to%20yield%20a%20strong%20volume%0Apresentation.%20We%20additionally%20combine%20the%20neural%20prior%20from%20SAM%20features%20with%0Ainternal%20features%20to%20reduce%20the%20ambiguity%20between%20foreground%20and%20background%20and%0Astrengthen%20the%20depth%20edges.%20Extensive%20experimental%20results%20on%20nuScenes%20and%20DDAD%0Abenchmarks%20show%20M%24%7B%5E2%7D%24Depth%20achieves%20state-of-the-art%20performance.%20More%0Aresults%20can%20be%20found%20in%20https%3A//heiheishuang.xyz/M2Depth%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02004v1&entry.124074799=Read"},
{"title": "Towards a Formal Creativity Theory: Preliminary results in Novelty and\n  Transformativeness", "author": "Lu\u00eds Esp\u00edrito Santo and Geraint Wiggins and Am\u00edlcar Cardoso", "abstract": "  Formalizing creativity-related concepts has been a long-term goal of\nComputational Creativity. To the same end, we explore Formal Learning Theory in\nthe context of creativity. We provide an introduction to the main concepts of\nthis framework and a re-interpretation of terms commonly found in creativity\ndiscussions, proposing formal definitions for novelty and transformational\ncreativity. This formalisation marks the beginning of a research branch we call\nFormal Creativity Theory, exploring how learning can be included as preparation\nfor exploratory behaviour and how learning is a key part of transformational\ncreative behaviour. By employing these definitions, we argue that, while\nnovelty is neither necessary nor sufficient for transformational creativity in\ngeneral, when using an inspiring set, rather than a sequence of experiences, an\nagent actually requires novelty for transformational creativity to occur.\n", "link": "http://arxiv.org/abs/2405.02148v1", "date": "2024-05-03", "relevancy": 1.7143, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4307}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4284}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Formal%20Creativity%20Theory%3A%20Preliminary%20results%20in%20Novelty%20and%0A%20%20Transformativeness&body=Title%3A%20Towards%20a%20Formal%20Creativity%20Theory%3A%20Preliminary%20results%20in%20Novelty%20and%0A%20%20Transformativeness%0AAuthor%3A%20Lu%C3%ADs%20Esp%C3%ADrito%20Santo%20and%20Geraint%20Wiggins%20and%20Am%C3%ADlcar%20Cardoso%0AAbstract%3A%20%20%20Formalizing%20creativity-related%20concepts%20has%20been%20a%20long-term%20goal%20of%0AComputational%20Creativity.%20To%20the%20same%20end%2C%20we%20explore%20Formal%20Learning%20Theory%20in%0Athe%20context%20of%20creativity.%20We%20provide%20an%20introduction%20to%20the%20main%20concepts%20of%0Athis%20framework%20and%20a%20re-interpretation%20of%20terms%20commonly%20found%20in%20creativity%0Adiscussions%2C%20proposing%20formal%20definitions%20for%20novelty%20and%20transformational%0Acreativity.%20This%20formalisation%20marks%20the%20beginning%20of%20a%20research%20branch%20we%20call%0AFormal%20Creativity%20Theory%2C%20exploring%20how%20learning%20can%20be%20included%20as%20preparation%0Afor%20exploratory%20behaviour%20and%20how%20learning%20is%20a%20key%20part%20of%20transformational%0Acreative%20behaviour.%20By%20employing%20these%20definitions%2C%20we%20argue%20that%2C%20while%0Anovelty%20is%20neither%20necessary%20nor%20sufficient%20for%20transformational%20creativity%20in%0Ageneral%2C%20when%20using%20an%20inspiring%20set%2C%20rather%20than%20a%20sequence%20of%20experiences%2C%20an%0Aagent%20actually%20requires%20novelty%20for%20transformational%20creativity%20to%20occur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Formal%2520Creativity%2520Theory%253A%2520Preliminary%2520results%2520in%2520Novelty%2520and%250A%2520%2520Transformativeness%26entry.906535625%3DLu%25C3%25ADs%2520Esp%25C3%25ADrito%2520Santo%2520and%2520Geraint%2520Wiggins%2520and%2520Am%25C3%25ADlcar%2520Cardoso%26entry.1292438233%3D%2520%2520Formalizing%2520creativity-related%2520concepts%2520has%2520been%2520a%2520long-term%2520goal%2520of%250AComputational%2520Creativity.%2520To%2520the%2520same%2520end%252C%2520we%2520explore%2520Formal%2520Learning%2520Theory%2520in%250Athe%2520context%2520of%2520creativity.%2520We%2520provide%2520an%2520introduction%2520to%2520the%2520main%2520concepts%2520of%250Athis%2520framework%2520and%2520a%2520re-interpretation%2520of%2520terms%2520commonly%2520found%2520in%2520creativity%250Adiscussions%252C%2520proposing%2520formal%2520definitions%2520for%2520novelty%2520and%2520transformational%250Acreativity.%2520This%2520formalisation%2520marks%2520the%2520beginning%2520of%2520a%2520research%2520branch%2520we%2520call%250AFormal%2520Creativity%2520Theory%252C%2520exploring%2520how%2520learning%2520can%2520be%2520included%2520as%2520preparation%250Afor%2520exploratory%2520behaviour%2520and%2520how%2520learning%2520is%2520a%2520key%2520part%2520of%2520transformational%250Acreative%2520behaviour.%2520By%2520employing%2520these%2520definitions%252C%2520we%2520argue%2520that%252C%2520while%250Anovelty%2520is%2520neither%2520necessary%2520nor%2520sufficient%2520for%2520transformational%2520creativity%2520in%250Ageneral%252C%2520when%2520using%2520an%2520inspiring%2520set%252C%2520rather%2520than%2520a%2520sequence%2520of%2520experiences%252C%2520an%250Aagent%2520actually%2520requires%2520novelty%2520for%2520transformational%2520creativity%2520to%2520occur.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Formal%20Creativity%20Theory%3A%20Preliminary%20results%20in%20Novelty%20and%0A%20%20Transformativeness&entry.906535625=Lu%C3%ADs%20Esp%C3%ADrito%20Santo%20and%20Geraint%20Wiggins%20and%20Am%C3%ADlcar%20Cardoso&entry.1292438233=%20%20Formalizing%20creativity-related%20concepts%20has%20been%20a%20long-term%20goal%20of%0AComputational%20Creativity.%20To%20the%20same%20end%2C%20we%20explore%20Formal%20Learning%20Theory%20in%0Athe%20context%20of%20creativity.%20We%20provide%20an%20introduction%20to%20the%20main%20concepts%20of%0Athis%20framework%20and%20a%20re-interpretation%20of%20terms%20commonly%20found%20in%20creativity%0Adiscussions%2C%20proposing%20formal%20definitions%20for%20novelty%20and%20transformational%0Acreativity.%20This%20formalisation%20marks%20the%20beginning%20of%20a%20research%20branch%20we%20call%0AFormal%20Creativity%20Theory%2C%20exploring%20how%20learning%20can%20be%20included%20as%20preparation%0Afor%20exploratory%20behaviour%20and%20how%20learning%20is%20a%20key%20part%20of%20transformational%0Acreative%20behaviour.%20By%20employing%20these%20definitions%2C%20we%20argue%20that%2C%20while%0Anovelty%20is%20neither%20necessary%20nor%20sufficient%20for%20transformational%20creativity%20in%0Ageneral%2C%20when%20using%20an%20inspiring%20set%2C%20rather%20than%20a%20sequence%20of%20experiences%2C%20an%0Aagent%20actually%20requires%20novelty%20for%20transformational%20creativity%20to%20occur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02148v1&entry.124074799=Read"},
{"title": "Regularized Q-learning through Robust Averaging", "author": "Peter Schmitt-F\u00f6rster and Tobias Sutter", "abstract": "  We propose a new Q-learning variant, called 2RA Q-learning, that addresses\nsome weaknesses of existing Q-learning methods in a principled manner. One such\nweakness is an underlying estimation bias which cannot be controlled and often\nresults in poor performance. We propose a distributionally robust estimator for\nthe maximum expected value term, which allows us to precisely control the level\nof estimation bias introduced. The distributionally robust estimator admits a\nclosed-form solution such that the proposed algorithm has a computational cost\nper iteration comparable to Watkins' Q-learning. For the tabular case, we show\nthat 2RA Q-learning converges to the optimal policy and analyze its asymptotic\nmean-squared error. Lastly, we conduct numerical experiments for various\nsettings, which corroborate our theoretical findings and indicate that 2RA\nQ-learning often performs better than existing methods.\n", "link": "http://arxiv.org/abs/2405.02201v1", "date": "2024-05-03", "relevancy": 1.6985, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4194}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularized%20Q-learning%20through%20Robust%20Averaging&body=Title%3A%20Regularized%20Q-learning%20through%20Robust%20Averaging%0AAuthor%3A%20Peter%20Schmitt-F%C3%B6rster%20and%20Tobias%20Sutter%0AAbstract%3A%20%20%20We%20propose%20a%20new%20Q-learning%20variant%2C%20called%202RA%20Q-learning%2C%20that%20addresses%0Asome%20weaknesses%20of%20existing%20Q-learning%20methods%20in%20a%20principled%20manner.%20One%20such%0Aweakness%20is%20an%20underlying%20estimation%20bias%20which%20cannot%20be%20controlled%20and%20often%0Aresults%20in%20poor%20performance.%20We%20propose%20a%20distributionally%20robust%20estimator%20for%0Athe%20maximum%20expected%20value%20term%2C%20which%20allows%20us%20to%20precisely%20control%20the%20level%0Aof%20estimation%20bias%20introduced.%20The%20distributionally%20robust%20estimator%20admits%20a%0Aclosed-form%20solution%20such%20that%20the%20proposed%20algorithm%20has%20a%20computational%20cost%0Aper%20iteration%20comparable%20to%20Watkins%27%20Q-learning.%20For%20the%20tabular%20case%2C%20we%20show%0Athat%202RA%20Q-learning%20converges%20to%20the%20optimal%20policy%20and%20analyze%20its%20asymptotic%0Amean-squared%20error.%20Lastly%2C%20we%20conduct%20numerical%20experiments%20for%20various%0Asettings%2C%20which%20corroborate%20our%20theoretical%20findings%20and%20indicate%20that%202RA%0AQ-learning%20often%20performs%20better%20than%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularized%2520Q-learning%2520through%2520Robust%2520Averaging%26entry.906535625%3DPeter%2520Schmitt-F%25C3%25B6rster%2520and%2520Tobias%2520Sutter%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520Q-learning%2520variant%252C%2520called%25202RA%2520Q-learning%252C%2520that%2520addresses%250Asome%2520weaknesses%2520of%2520existing%2520Q-learning%2520methods%2520in%2520a%2520principled%2520manner.%2520One%2520such%250Aweakness%2520is%2520an%2520underlying%2520estimation%2520bias%2520which%2520cannot%2520be%2520controlled%2520and%2520often%250Aresults%2520in%2520poor%2520performance.%2520We%2520propose%2520a%2520distributionally%2520robust%2520estimator%2520for%250Athe%2520maximum%2520expected%2520value%2520term%252C%2520which%2520allows%2520us%2520to%2520precisely%2520control%2520the%2520level%250Aof%2520estimation%2520bias%2520introduced.%2520The%2520distributionally%2520robust%2520estimator%2520admits%2520a%250Aclosed-form%2520solution%2520such%2520that%2520the%2520proposed%2520algorithm%2520has%2520a%2520computational%2520cost%250Aper%2520iteration%2520comparable%2520to%2520Watkins%2527%2520Q-learning.%2520For%2520the%2520tabular%2520case%252C%2520we%2520show%250Athat%25202RA%2520Q-learning%2520converges%2520to%2520the%2520optimal%2520policy%2520and%2520analyze%2520its%2520asymptotic%250Amean-squared%2520error.%2520Lastly%252C%2520we%2520conduct%2520numerical%2520experiments%2520for%2520various%250Asettings%252C%2520which%2520corroborate%2520our%2520theoretical%2520findings%2520and%2520indicate%2520that%25202RA%250AQ-learning%2520often%2520performs%2520better%2520than%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Q-learning%20through%20Robust%20Averaging&entry.906535625=Peter%20Schmitt-F%C3%B6rster%20and%20Tobias%20Sutter&entry.1292438233=%20%20We%20propose%20a%20new%20Q-learning%20variant%2C%20called%202RA%20Q-learning%2C%20that%20addresses%0Asome%20weaknesses%20of%20existing%20Q-learning%20methods%20in%20a%20principled%20manner.%20One%20such%0Aweakness%20is%20an%20underlying%20estimation%20bias%20which%20cannot%20be%20controlled%20and%20often%0Aresults%20in%20poor%20performance.%20We%20propose%20a%20distributionally%20robust%20estimator%20for%0Athe%20maximum%20expected%20value%20term%2C%20which%20allows%20us%20to%20precisely%20control%20the%20level%0Aof%20estimation%20bias%20introduced.%20The%20distributionally%20robust%20estimator%20admits%20a%0Aclosed-form%20solution%20such%20that%20the%20proposed%20algorithm%20has%20a%20computational%20cost%0Aper%20iteration%20comparable%20to%20Watkins%27%20Q-learning.%20For%20the%20tabular%20case%2C%20we%20show%0Athat%202RA%20Q-learning%20converges%20to%20the%20optimal%20policy%20and%20analyze%20its%20asymptotic%0Amean-squared%20error.%20Lastly%2C%20we%20conduct%20numerical%20experiments%20for%20various%0Asettings%2C%20which%20corroborate%20our%20theoretical%20findings%20and%20indicate%20that%202RA%0AQ-learning%20often%20performs%20better%20than%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02201v1&entry.124074799=Read"},
{"title": "WeightedPose: Generalizable Cross-Pose Estimation via Weighted SVD", "author": "Xuxin Cheng and Heng Yu and Harry Zhang and Wenxing Deng", "abstract": "  We present a novel method for robotic manipulation tasks in human\nenvironments that require reasoning about the 3D geometric relationship between\na pair of objects. Traditional end-to-end trained policies, which map from\npixel observations to low-level robot actions, struggle to reason about complex\npose relationships and have difficulty generalizing to unseen object\nconfigurations. To address these challenges, we propose a method that learns to\nreason about the 3D geometric relationship between objects, focusing on the\nrelationship between key parts on one object with respect to key parts on\nanother object. Our standalone model utilizes Weighted SVD to reason about both\npose relationships between articulated parts and between free-floating objects.\nThis approach allows the robot to understand the relationship between the oven\ndoor and the oven body, as well as the relationship between the lasagna plate\nand the oven, for example. By considering the 3D geometric relationship between\nobjects, our method enables robots to perform complex manipulation tasks that\nreason about object-centric representations. We open source the code and\ndemonstrate the results here\n", "link": "http://arxiv.org/abs/2405.02241v1", "date": "2024-05-03", "relevancy": 1.6934, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5995}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5559}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeightedPose%3A%20Generalizable%20Cross-Pose%20Estimation%20via%20Weighted%20SVD&body=Title%3A%20WeightedPose%3A%20Generalizable%20Cross-Pose%20Estimation%20via%20Weighted%20SVD%0AAuthor%3A%20Xuxin%20Cheng%20and%20Heng%20Yu%20and%20Harry%20Zhang%20and%20Wenxing%20Deng%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20robotic%20manipulation%20tasks%20in%20human%0Aenvironments%20that%20require%20reasoning%20about%20the%203D%20geometric%20relationship%20between%0Aa%20pair%20of%20objects.%20Traditional%20end-to-end%20trained%20policies%2C%20which%20map%20from%0Apixel%20observations%20to%20low-level%20robot%20actions%2C%20struggle%20to%20reason%20about%20complex%0Apose%20relationships%20and%20have%20difficulty%20generalizing%20to%20unseen%20object%0Aconfigurations.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20method%20that%20learns%20to%0Areason%20about%20the%203D%20geometric%20relationship%20between%20objects%2C%20focusing%20on%20the%0Arelationship%20between%20key%20parts%20on%20one%20object%20with%20respect%20to%20key%20parts%20on%0Aanother%20object.%20Our%20standalone%20model%20utilizes%20Weighted%20SVD%20to%20reason%20about%20both%0Apose%20relationships%20between%20articulated%20parts%20and%20between%20free-floating%20objects.%0AThis%20approach%20allows%20the%20robot%20to%20understand%20the%20relationship%20between%20the%20oven%0Adoor%20and%20the%20oven%20body%2C%20as%20well%20as%20the%20relationship%20between%20the%20lasagna%20plate%0Aand%20the%20oven%2C%20for%20example.%20By%20considering%20the%203D%20geometric%20relationship%20between%0Aobjects%2C%20our%20method%20enables%20robots%20to%20perform%20complex%20manipulation%20tasks%20that%0Areason%20about%20object-centric%20representations.%20We%20open%20source%20the%20code%20and%0Ademonstrate%20the%20results%20here%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeightedPose%253A%2520Generalizable%2520Cross-Pose%2520Estimation%2520via%2520Weighted%2520SVD%26entry.906535625%3DXuxin%2520Cheng%2520and%2520Heng%2520Yu%2520and%2520Harry%2520Zhang%2520and%2520Wenxing%2520Deng%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520method%2520for%2520robotic%2520manipulation%2520tasks%2520in%2520human%250Aenvironments%2520that%2520require%2520reasoning%2520about%2520the%25203D%2520geometric%2520relationship%2520between%250Aa%2520pair%2520of%2520objects.%2520Traditional%2520end-to-end%2520trained%2520policies%252C%2520which%2520map%2520from%250Apixel%2520observations%2520to%2520low-level%2520robot%2520actions%252C%2520struggle%2520to%2520reason%2520about%2520complex%250Apose%2520relationships%2520and%2520have%2520difficulty%2520generalizing%2520to%2520unseen%2520object%250Aconfigurations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520method%2520that%2520learns%2520to%250Areason%2520about%2520the%25203D%2520geometric%2520relationship%2520between%2520objects%252C%2520focusing%2520on%2520the%250Arelationship%2520between%2520key%2520parts%2520on%2520one%2520object%2520with%2520respect%2520to%2520key%2520parts%2520on%250Aanother%2520object.%2520Our%2520standalone%2520model%2520utilizes%2520Weighted%2520SVD%2520to%2520reason%2520about%2520both%250Apose%2520relationships%2520between%2520articulated%2520parts%2520and%2520between%2520free-floating%2520objects.%250AThis%2520approach%2520allows%2520the%2520robot%2520to%2520understand%2520the%2520relationship%2520between%2520the%2520oven%250Adoor%2520and%2520the%2520oven%2520body%252C%2520as%2520well%2520as%2520the%2520relationship%2520between%2520the%2520lasagna%2520plate%250Aand%2520the%2520oven%252C%2520for%2520example.%2520By%2520considering%2520the%25203D%2520geometric%2520relationship%2520between%250Aobjects%252C%2520our%2520method%2520enables%2520robots%2520to%2520perform%2520complex%2520manipulation%2520tasks%2520that%250Areason%2520about%2520object-centric%2520representations.%2520We%2520open%2520source%2520the%2520code%2520and%250Ademonstrate%2520the%2520results%2520here%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeightedPose%3A%20Generalizable%20Cross-Pose%20Estimation%20via%20Weighted%20SVD&entry.906535625=Xuxin%20Cheng%20and%20Heng%20Yu%20and%20Harry%20Zhang%20and%20Wenxing%20Deng&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20robotic%20manipulation%20tasks%20in%20human%0Aenvironments%20that%20require%20reasoning%20about%20the%203D%20geometric%20relationship%20between%0Aa%20pair%20of%20objects.%20Traditional%20end-to-end%20trained%20policies%2C%20which%20map%20from%0Apixel%20observations%20to%20low-level%20robot%20actions%2C%20struggle%20to%20reason%20about%20complex%0Apose%20relationships%20and%20have%20difficulty%20generalizing%20to%20unseen%20object%0Aconfigurations.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20method%20that%20learns%20to%0Areason%20about%20the%203D%20geometric%20relationship%20between%20objects%2C%20focusing%20on%20the%0Arelationship%20between%20key%20parts%20on%20one%20object%20with%20respect%20to%20key%20parts%20on%0Aanother%20object.%20Our%20standalone%20model%20utilizes%20Weighted%20SVD%20to%20reason%20about%20both%0Apose%20relationships%20between%20articulated%20parts%20and%20between%20free-floating%20objects.%0AThis%20approach%20allows%20the%20robot%20to%20understand%20the%20relationship%20between%20the%20oven%0Adoor%20and%20the%20oven%20body%2C%20as%20well%20as%20the%20relationship%20between%20the%20lasagna%20plate%0Aand%20the%20oven%2C%20for%20example.%20By%20considering%20the%203D%20geometric%20relationship%20between%0Aobjects%2C%20our%20method%20enables%20robots%20to%20perform%20complex%20manipulation%20tasks%20that%0Areason%20about%20object-centric%20representations.%20We%20open%20source%20the%20code%20and%0Ademonstrate%20the%20results%20here%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02241v1&entry.124074799=Read"},
{"title": "Discretization Error of Fourier Neural Operators", "author": "Samuel Lanthaler and Andrew M. Stuart and Margaret Trautner", "abstract": "  Operator learning is a variant of machine learning that is designed to\napproximate maps between function spaces from data. The Fourier Neural Operator\n(FNO) is a common model architecture used for operator learning. The FNO\ncombines pointwise linear and nonlinear operations in physical space with\npointwise linear operations in Fourier space, leading to a parameterized map\nacting between function spaces. Although FNOs formally involve convolutions of\nfunctions on a continuum, in practice the computations are performed on a\ndiscretized grid, allowing efficient implementation via the FFT. In this paper,\nthe aliasing error that results from such a discretization is quantified and\nalgebraic rates of convergence in terms of the grid resolution are obtained as\na function of the regularity of the input. Numerical experiments that validate\nthe theory and describe model stability are performed.\n", "link": "http://arxiv.org/abs/2405.02221v1", "date": "2024-05-03", "relevancy": 1.682, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4388}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4171}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discretization%20Error%20of%20Fourier%20Neural%20Operators&body=Title%3A%20Discretization%20Error%20of%20Fourier%20Neural%20Operators%0AAuthor%3A%20Samuel%20Lanthaler%20and%20Andrew%20M.%20Stuart%20and%20Margaret%20Trautner%0AAbstract%3A%20%20%20Operator%20learning%20is%20a%20variant%20of%20machine%20learning%20that%20is%20designed%20to%0Aapproximate%20maps%20between%20function%20spaces%20from%20data.%20The%20Fourier%20Neural%20Operator%0A%28FNO%29%20is%20a%20common%20model%20architecture%20used%20for%20operator%20learning.%20The%20FNO%0Acombines%20pointwise%20linear%20and%20nonlinear%20operations%20in%20physical%20space%20with%0Apointwise%20linear%20operations%20in%20Fourier%20space%2C%20leading%20to%20a%20parameterized%20map%0Aacting%20between%20function%20spaces.%20Although%20FNOs%20formally%20involve%20convolutions%20of%0Afunctions%20on%20a%20continuum%2C%20in%20practice%20the%20computations%20are%20performed%20on%20a%0Adiscretized%20grid%2C%20allowing%20efficient%20implementation%20via%20the%20FFT.%20In%20this%20paper%2C%0Athe%20aliasing%20error%20that%20results%20from%20such%20a%20discretization%20is%20quantified%20and%0Aalgebraic%20rates%20of%20convergence%20in%20terms%20of%20the%20grid%20resolution%20are%20obtained%20as%0Aa%20function%20of%20the%20regularity%20of%20the%20input.%20Numerical%20experiments%20that%20validate%0Athe%20theory%20and%20describe%20model%20stability%20are%20performed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscretization%2520Error%2520of%2520Fourier%2520Neural%2520Operators%26entry.906535625%3DSamuel%2520Lanthaler%2520and%2520Andrew%2520M.%2520Stuart%2520and%2520Margaret%2520Trautner%26entry.1292438233%3D%2520%2520Operator%2520learning%2520is%2520a%2520variant%2520of%2520machine%2520learning%2520that%2520is%2520designed%2520to%250Aapproximate%2520maps%2520between%2520function%2520spaces%2520from%2520data.%2520The%2520Fourier%2520Neural%2520Operator%250A%2528FNO%2529%2520is%2520a%2520common%2520model%2520architecture%2520used%2520for%2520operator%2520learning.%2520The%2520FNO%250Acombines%2520pointwise%2520linear%2520and%2520nonlinear%2520operations%2520in%2520physical%2520space%2520with%250Apointwise%2520linear%2520operations%2520in%2520Fourier%2520space%252C%2520leading%2520to%2520a%2520parameterized%2520map%250Aacting%2520between%2520function%2520spaces.%2520Although%2520FNOs%2520formally%2520involve%2520convolutions%2520of%250Afunctions%2520on%2520a%2520continuum%252C%2520in%2520practice%2520the%2520computations%2520are%2520performed%2520on%2520a%250Adiscretized%2520grid%252C%2520allowing%2520efficient%2520implementation%2520via%2520the%2520FFT.%2520In%2520this%2520paper%252C%250Athe%2520aliasing%2520error%2520that%2520results%2520from%2520such%2520a%2520discretization%2520is%2520quantified%2520and%250Aalgebraic%2520rates%2520of%2520convergence%2520in%2520terms%2520of%2520the%2520grid%2520resolution%2520are%2520obtained%2520as%250Aa%2520function%2520of%2520the%2520regularity%2520of%2520the%2520input.%2520Numerical%2520experiments%2520that%2520validate%250Athe%2520theory%2520and%2520describe%2520model%2520stability%2520are%2520performed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discretization%20Error%20of%20Fourier%20Neural%20Operators&entry.906535625=Samuel%20Lanthaler%20and%20Andrew%20M.%20Stuart%20and%20Margaret%20Trautner&entry.1292438233=%20%20Operator%20learning%20is%20a%20variant%20of%20machine%20learning%20that%20is%20designed%20to%0Aapproximate%20maps%20between%20function%20spaces%20from%20data.%20The%20Fourier%20Neural%20Operator%0A%28FNO%29%20is%20a%20common%20model%20architecture%20used%20for%20operator%20learning.%20The%20FNO%0Acombines%20pointwise%20linear%20and%20nonlinear%20operations%20in%20physical%20space%20with%0Apointwise%20linear%20operations%20in%20Fourier%20space%2C%20leading%20to%20a%20parameterized%20map%0Aacting%20between%20function%20spaces.%20Although%20FNOs%20formally%20involve%20convolutions%20of%0Afunctions%20on%20a%20continuum%2C%20in%20practice%20the%20computations%20are%20performed%20on%20a%0Adiscretized%20grid%2C%20allowing%20efficient%20implementation%20via%20the%20FFT.%20In%20this%20paper%2C%0Athe%20aliasing%20error%20that%20results%20from%20such%20a%20discretization%20is%20quantified%20and%0Aalgebraic%20rates%20of%20convergence%20in%20terms%20of%20the%20grid%20resolution%20are%20obtained%20as%0Aa%20function%20of%20the%20regularity%20of%20the%20input.%20Numerical%20experiments%20that%20validate%0Athe%20theory%20and%20describe%20model%20stability%20are%20performed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02221v1&entry.124074799=Read"},
{"title": "SATO: Stable Text-to-Motion Framework", "author": "Wenshuo Chen and Hongru Xiao and Erhang Zhang and Lijie Hu and Lei Wang and Mengyuan Liu and Chen Chen", "abstract": "  Is the Text to Motion model robust? Recent advancements in Text to Motion\nmodels primarily stem from more accurate predictions of specific actions.\nHowever, the text modality typically relies solely on pre-trained Contrastive\nLanguage-Image Pretraining (CLIP) models. Our research has uncovered a\nsignificant issue with the text-to-motion model: its predictions often exhibit\ninconsistent outputs, resulting in vastly different or even incorrect poses\nwhen presented with semantically similar or identical text inputs. In this\npaper, we undertake an analysis to elucidate the underlying causes of this\ninstability, establishing a clear link between the unpredictability of model\noutputs and the erratic attention patterns of the text encoder module.\nConsequently, we introduce a formal framework aimed at addressing this issue,\nwhich we term the Stable Text-to-Motion Framework (SATO). SATO consists of\nthree modules, each dedicated to stable attention, stable prediction, and\nmaintaining a balance between accuracy and robustness trade-off. We present a\nmethodology for constructing an SATO that satisfies the stability of attention\nand prediction. To verify the stability of the model, we introduced a new\ntextual synonym perturbation dataset based on HumanML3D and KIT-ML. Results\nshow that SATO is significantly more stable against synonyms and other slight\nperturbations while keeping its high accuracy performance.\n", "link": "http://arxiv.org/abs/2405.01461v2", "date": "2024-05-03", "relevancy": 1.6774, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5916}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SATO%3A%20Stable%20Text-to-Motion%20Framework&body=Title%3A%20SATO%3A%20Stable%20Text-to-Motion%20Framework%0AAuthor%3A%20Wenshuo%20Chen%20and%20Hongru%20Xiao%20and%20Erhang%20Zhang%20and%20Lijie%20Hu%20and%20Lei%20Wang%20and%20Mengyuan%20Liu%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Is%20the%20Text%20to%20Motion%20model%20robust%3F%20Recent%20advancements%20in%20Text%20to%20Motion%0Amodels%20primarily%20stem%20from%20more%20accurate%20predictions%20of%20specific%20actions.%0AHowever%2C%20the%20text%20modality%20typically%20relies%20solely%20on%20pre-trained%20Contrastive%0ALanguage-Image%20Pretraining%20%28CLIP%29%20models.%20Our%20research%20has%20uncovered%20a%0Asignificant%20issue%20with%20the%20text-to-motion%20model%3A%20its%20predictions%20often%20exhibit%0Ainconsistent%20outputs%2C%20resulting%20in%20vastly%20different%20or%20even%20incorrect%20poses%0Awhen%20presented%20with%20semantically%20similar%20or%20identical%20text%20inputs.%20In%20this%0Apaper%2C%20we%20undertake%20an%20analysis%20to%20elucidate%20the%20underlying%20causes%20of%20this%0Ainstability%2C%20establishing%20a%20clear%20link%20between%20the%20unpredictability%20of%20model%0Aoutputs%20and%20the%20erratic%20attention%20patterns%20of%20the%20text%20encoder%20module.%0AConsequently%2C%20we%20introduce%20a%20formal%20framework%20aimed%20at%20addressing%20this%20issue%2C%0Awhich%20we%20term%20the%20Stable%20Text-to-Motion%20Framework%20%28SATO%29.%20SATO%20consists%20of%0Athree%20modules%2C%20each%20dedicated%20to%20stable%20attention%2C%20stable%20prediction%2C%20and%0Amaintaining%20a%20balance%20between%20accuracy%20and%20robustness%20trade-off.%20We%20present%20a%0Amethodology%20for%20constructing%20an%20SATO%20that%20satisfies%20the%20stability%20of%20attention%0Aand%20prediction.%20To%20verify%20the%20stability%20of%20the%20model%2C%20we%20introduced%20a%20new%0Atextual%20synonym%20perturbation%20dataset%20based%20on%20HumanML3D%20and%20KIT-ML.%20Results%0Ashow%20that%20SATO%20is%20significantly%20more%20stable%20against%20synonyms%20and%20other%20slight%0Aperturbations%20while%20keeping%20its%20high%20accuracy%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSATO%253A%2520Stable%2520Text-to-Motion%2520Framework%26entry.906535625%3DWenshuo%2520Chen%2520and%2520Hongru%2520Xiao%2520and%2520Erhang%2520Zhang%2520and%2520Lijie%2520Hu%2520and%2520Lei%2520Wang%2520and%2520Mengyuan%2520Liu%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Is%2520the%2520Text%2520to%2520Motion%2520model%2520robust%253F%2520Recent%2520advancements%2520in%2520Text%2520to%2520Motion%250Amodels%2520primarily%2520stem%2520from%2520more%2520accurate%2520predictions%2520of%2520specific%2520actions.%250AHowever%252C%2520the%2520text%2520modality%2520typically%2520relies%2520solely%2520on%2520pre-trained%2520Contrastive%250ALanguage-Image%2520Pretraining%2520%2528CLIP%2529%2520models.%2520Our%2520research%2520has%2520uncovered%2520a%250Asignificant%2520issue%2520with%2520the%2520text-to-motion%2520model%253A%2520its%2520predictions%2520often%2520exhibit%250Ainconsistent%2520outputs%252C%2520resulting%2520in%2520vastly%2520different%2520or%2520even%2520incorrect%2520poses%250Awhen%2520presented%2520with%2520semantically%2520similar%2520or%2520identical%2520text%2520inputs.%2520In%2520this%250Apaper%252C%2520we%2520undertake%2520an%2520analysis%2520to%2520elucidate%2520the%2520underlying%2520causes%2520of%2520this%250Ainstability%252C%2520establishing%2520a%2520clear%2520link%2520between%2520the%2520unpredictability%2520of%2520model%250Aoutputs%2520and%2520the%2520erratic%2520attention%2520patterns%2520of%2520the%2520text%2520encoder%2520module.%250AConsequently%252C%2520we%2520introduce%2520a%2520formal%2520framework%2520aimed%2520at%2520addressing%2520this%2520issue%252C%250Awhich%2520we%2520term%2520the%2520Stable%2520Text-to-Motion%2520Framework%2520%2528SATO%2529.%2520SATO%2520consists%2520of%250Athree%2520modules%252C%2520each%2520dedicated%2520to%2520stable%2520attention%252C%2520stable%2520prediction%252C%2520and%250Amaintaining%2520a%2520balance%2520between%2520accuracy%2520and%2520robustness%2520trade-off.%2520We%2520present%2520a%250Amethodology%2520for%2520constructing%2520an%2520SATO%2520that%2520satisfies%2520the%2520stability%2520of%2520attention%250Aand%2520prediction.%2520To%2520verify%2520the%2520stability%2520of%2520the%2520model%252C%2520we%2520introduced%2520a%2520new%250Atextual%2520synonym%2520perturbation%2520dataset%2520based%2520on%2520HumanML3D%2520and%2520KIT-ML.%2520Results%250Ashow%2520that%2520SATO%2520is%2520significantly%2520more%2520stable%2520against%2520synonyms%2520and%2520other%2520slight%250Aperturbations%2520while%2520keeping%2520its%2520high%2520accuracy%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SATO%3A%20Stable%20Text-to-Motion%20Framework&entry.906535625=Wenshuo%20Chen%20and%20Hongru%20Xiao%20and%20Erhang%20Zhang%20and%20Lijie%20Hu%20and%20Lei%20Wang%20and%20Mengyuan%20Liu%20and%20Chen%20Chen&entry.1292438233=%20%20Is%20the%20Text%20to%20Motion%20model%20robust%3F%20Recent%20advancements%20in%20Text%20to%20Motion%0Amodels%20primarily%20stem%20from%20more%20accurate%20predictions%20of%20specific%20actions.%0AHowever%2C%20the%20text%20modality%20typically%20relies%20solely%20on%20pre-trained%20Contrastive%0ALanguage-Image%20Pretraining%20%28CLIP%29%20models.%20Our%20research%20has%20uncovered%20a%0Asignificant%20issue%20with%20the%20text-to-motion%20model%3A%20its%20predictions%20often%20exhibit%0Ainconsistent%20outputs%2C%20resulting%20in%20vastly%20different%20or%20even%20incorrect%20poses%0Awhen%20presented%20with%20semantically%20similar%20or%20identical%20text%20inputs.%20In%20this%0Apaper%2C%20we%20undertake%20an%20analysis%20to%20elucidate%20the%20underlying%20causes%20of%20this%0Ainstability%2C%20establishing%20a%20clear%20link%20between%20the%20unpredictability%20of%20model%0Aoutputs%20and%20the%20erratic%20attention%20patterns%20of%20the%20text%20encoder%20module.%0AConsequently%2C%20we%20introduce%20a%20formal%20framework%20aimed%20at%20addressing%20this%20issue%2C%0Awhich%20we%20term%20the%20Stable%20Text-to-Motion%20Framework%20%28SATO%29.%20SATO%20consists%20of%0Athree%20modules%2C%20each%20dedicated%20to%20stable%20attention%2C%20stable%20prediction%2C%20and%0Amaintaining%20a%20balance%20between%20accuracy%20and%20robustness%20trade-off.%20We%20present%20a%0Amethodology%20for%20constructing%20an%20SATO%20that%20satisfies%20the%20stability%20of%20attention%0Aand%20prediction.%20To%20verify%20the%20stability%20of%20the%20model%2C%20we%20introduced%20a%20new%0Atextual%20synonym%20perturbation%20dataset%20based%20on%20HumanML3D%20and%20KIT-ML.%20Results%0Ashow%20that%20SATO%20is%20significantly%20more%20stable%20against%20synonyms%20and%20other%20slight%0Aperturbations%20while%20keeping%20its%20high%20accuracy%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01461v2&entry.124074799=Read"},
{"title": "On the test-time zero-shot generalization of vision-language models: Do\n  we really need prompt learning?", "author": "Maxime Zanella and Ismail Ben Ayed", "abstract": "  The development of large vision-language models, notably CLIP, has catalyzed\nresearch into effective adaptation techniques, with a particular focus on soft\nprompt tuning. Conjointly, test-time augmentation, which utilizes multiple\naugmented views of a single image to enhance zero-shot generalization, is\nemerging as a significant area of interest. This has predominantly directed\nresearch efforts toward test-time prompt tuning. In contrast, we introduce a\nrobust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-based\nmethods without requiring this intensive training procedure. This positions MTA\nas an ideal solution for both standalone and API-based applications.\nAdditionally, our method does not rely on ad hoc rules (e.g., confidence\nthreshold) used in some previous test-time augmentation techniques to filter\nthe augmented views. Instead, MTA incorporates a quality assessment variable\nfor each view directly into its optimization process, termed as the inlierness\nscore. This score is jointly optimized with a density mode seeking process,\nleading to an efficient training- and hyperparameter-free approach. We\nextensively benchmark our method on 15 datasets and demonstrate MTA's\nsuperiority and computational efficiency. Deployed easily as plug-and-play\nmodule on top of zero-shot models and state-of-the-art few-shot methods, MTA\nshows systematic and consistent improvements.\n", "link": "http://arxiv.org/abs/2405.02266v1", "date": "2024-05-03", "relevancy": 1.6615, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.574}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5363}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20test-time%20zero-shot%20generalization%20of%20vision-language%20models%3A%20Do%0A%20%20we%20really%20need%20prompt%20learning%3F&body=Title%3A%20On%20the%20test-time%20zero-shot%20generalization%20of%20vision-language%20models%3A%20Do%0A%20%20we%20really%20need%20prompt%20learning%3F%0AAuthor%3A%20Maxime%20Zanella%20and%20Ismail%20Ben%20Ayed%0AAbstract%3A%20%20%20The%20development%20of%20large%20vision-language%20models%2C%20notably%20CLIP%2C%20has%20catalyzed%0Aresearch%20into%20effective%20adaptation%20techniques%2C%20with%20a%20particular%20focus%20on%20soft%0Aprompt%20tuning.%20Conjointly%2C%20test-time%20augmentation%2C%20which%20utilizes%20multiple%0Aaugmented%20views%20of%20a%20single%20image%20to%20enhance%20zero-shot%20generalization%2C%20is%0Aemerging%20as%20a%20significant%20area%20of%20interest.%20This%20has%20predominantly%20directed%0Aresearch%20efforts%20toward%20test-time%20prompt%20tuning.%20In%20contrast%2C%20we%20introduce%20a%0Arobust%20MeanShift%20for%20Test-time%20Augmentation%20%28MTA%29%2C%20which%20surpasses%20prompt-based%0Amethods%20without%20requiring%20this%20intensive%20training%20procedure.%20This%20positions%20MTA%0Aas%20an%20ideal%20solution%20for%20both%20standalone%20and%20API-based%20applications.%0AAdditionally%2C%20our%20method%20does%20not%20rely%20on%20ad%20hoc%20rules%20%28e.g.%2C%20confidence%0Athreshold%29%20used%20in%20some%20previous%20test-time%20augmentation%20techniques%20to%20filter%0Athe%20augmented%20views.%20Instead%2C%20MTA%20incorporates%20a%20quality%20assessment%20variable%0Afor%20each%20view%20directly%20into%20its%20optimization%20process%2C%20termed%20as%20the%20inlierness%0Ascore.%20This%20score%20is%20jointly%20optimized%20with%20a%20density%20mode%20seeking%20process%2C%0Aleading%20to%20an%20efficient%20training-%20and%20hyperparameter-free%20approach.%20We%0Aextensively%20benchmark%20our%20method%20on%2015%20datasets%20and%20demonstrate%20MTA%27s%0Asuperiority%20and%20computational%20efficiency.%20Deployed%20easily%20as%20plug-and-play%0Amodule%20on%20top%20of%20zero-shot%20models%20and%20state-of-the-art%20few-shot%20methods%2C%20MTA%0Ashows%20systematic%20and%20consistent%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520test-time%2520zero-shot%2520generalization%2520of%2520vision-language%2520models%253A%2520Do%250A%2520%2520we%2520really%2520need%2520prompt%2520learning%253F%26entry.906535625%3DMaxime%2520Zanella%2520and%2520Ismail%2520Ben%2520Ayed%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520large%2520vision-language%2520models%252C%2520notably%2520CLIP%252C%2520has%2520catalyzed%250Aresearch%2520into%2520effective%2520adaptation%2520techniques%252C%2520with%2520a%2520particular%2520focus%2520on%2520soft%250Aprompt%2520tuning.%2520Conjointly%252C%2520test-time%2520augmentation%252C%2520which%2520utilizes%2520multiple%250Aaugmented%2520views%2520of%2520a%2520single%2520image%2520to%2520enhance%2520zero-shot%2520generalization%252C%2520is%250Aemerging%2520as%2520a%2520significant%2520area%2520of%2520interest.%2520This%2520has%2520predominantly%2520directed%250Aresearch%2520efforts%2520toward%2520test-time%2520prompt%2520tuning.%2520In%2520contrast%252C%2520we%2520introduce%2520a%250Arobust%2520MeanShift%2520for%2520Test-time%2520Augmentation%2520%2528MTA%2529%252C%2520which%2520surpasses%2520prompt-based%250Amethods%2520without%2520requiring%2520this%2520intensive%2520training%2520procedure.%2520This%2520positions%2520MTA%250Aas%2520an%2520ideal%2520solution%2520for%2520both%2520standalone%2520and%2520API-based%2520applications.%250AAdditionally%252C%2520our%2520method%2520does%2520not%2520rely%2520on%2520ad%2520hoc%2520rules%2520%2528e.g.%252C%2520confidence%250Athreshold%2529%2520used%2520in%2520some%2520previous%2520test-time%2520augmentation%2520techniques%2520to%2520filter%250Athe%2520augmented%2520views.%2520Instead%252C%2520MTA%2520incorporates%2520a%2520quality%2520assessment%2520variable%250Afor%2520each%2520view%2520directly%2520into%2520its%2520optimization%2520process%252C%2520termed%2520as%2520the%2520inlierness%250Ascore.%2520This%2520score%2520is%2520jointly%2520optimized%2520with%2520a%2520density%2520mode%2520seeking%2520process%252C%250Aleading%2520to%2520an%2520efficient%2520training-%2520and%2520hyperparameter-free%2520approach.%2520We%250Aextensively%2520benchmark%2520our%2520method%2520on%252015%2520datasets%2520and%2520demonstrate%2520MTA%2527s%250Asuperiority%2520and%2520computational%2520efficiency.%2520Deployed%2520easily%2520as%2520plug-and-play%250Amodule%2520on%2520top%2520of%2520zero-shot%2520models%2520and%2520state-of-the-art%2520few-shot%2520methods%252C%2520MTA%250Ashows%2520systematic%2520and%2520consistent%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20test-time%20zero-shot%20generalization%20of%20vision-language%20models%3A%20Do%0A%20%20we%20really%20need%20prompt%20learning%3F&entry.906535625=Maxime%20Zanella%20and%20Ismail%20Ben%20Ayed&entry.1292438233=%20%20The%20development%20of%20large%20vision-language%20models%2C%20notably%20CLIP%2C%20has%20catalyzed%0Aresearch%20into%20effective%20adaptation%20techniques%2C%20with%20a%20particular%20focus%20on%20soft%0Aprompt%20tuning.%20Conjointly%2C%20test-time%20augmentation%2C%20which%20utilizes%20multiple%0Aaugmented%20views%20of%20a%20single%20image%20to%20enhance%20zero-shot%20generalization%2C%20is%0Aemerging%20as%20a%20significant%20area%20of%20interest.%20This%20has%20predominantly%20directed%0Aresearch%20efforts%20toward%20test-time%20prompt%20tuning.%20In%20contrast%2C%20we%20introduce%20a%0Arobust%20MeanShift%20for%20Test-time%20Augmentation%20%28MTA%29%2C%20which%20surpasses%20prompt-based%0Amethods%20without%20requiring%20this%20intensive%20training%20procedure.%20This%20positions%20MTA%0Aas%20an%20ideal%20solution%20for%20both%20standalone%20and%20API-based%20applications.%0AAdditionally%2C%20our%20method%20does%20not%20rely%20on%20ad%20hoc%20rules%20%28e.g.%2C%20confidence%0Athreshold%29%20used%20in%20some%20previous%20test-time%20augmentation%20techniques%20to%20filter%0Athe%20augmented%20views.%20Instead%2C%20MTA%20incorporates%20a%20quality%20assessment%20variable%0Afor%20each%20view%20directly%20into%20its%20optimization%20process%2C%20termed%20as%20the%20inlierness%0Ascore.%20This%20score%20is%20jointly%20optimized%20with%20a%20density%20mode%20seeking%20process%2C%0Aleading%20to%20an%20efficient%20training-%20and%20hyperparameter-free%20approach.%20We%0Aextensively%20benchmark%20our%20method%20on%2015%20datasets%20and%20demonstrate%20MTA%27s%0Asuperiority%20and%20computational%20efficiency.%20Deployed%20easily%20as%20plug-and-play%0Amodule%20on%20top%20of%20zero-shot%20models%20and%20state-of-the-art%20few-shot%20methods%2C%20MTA%0Ashows%20systematic%20and%20consistent%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02266v1&entry.124074799=Read"},
{"title": "The Cambridge RoboMaster: An Agile Multi-Robot Research Platform", "author": "Jan Blumenkamp and Ajay Shankar and Matteo Bettini and Joshua Bird and Amanda Prorok", "abstract": "  Compact robotic platforms with powerful compute and actuation capabilities\nare key enablers for practical, real-world deployments of multi-agent research.\nThis article introduces a tightly integrated hardware, control, and simulation\nsoftware stack on a fleet of holonomic ground robot platforms designed with\nthis motivation. Our robots, a fleet of customised DJI Robomaster S1 vehicles,\noffer a balance between small robots that do not possess sufficient compute or\nactuation capabilities and larger robots that are unsuitable for indoor\nmulti-robot tests. They run a modular ROS2-based optimal estimation and control\nstack for full onboard autonomy, contain ad-hoc peer-to-peer communication\ninfrastructure, and can zero-shot run multi-agent reinforcement learning (MARL)\npolicies trained in our vectorized multi-agent simulation framework. We present\nan in-depth review of other platforms currently available, showcase new\nexperimental validation of our system's capabilities, and introduce case\nstudies that highlight the versatility and reliabilty of our system as a\ntestbed for a wide range of research demonstrations. Our system as well as\nsupplementary material is available online:\nhttps://proroklab.github.io/cambridge-robomaster\n", "link": "http://arxiv.org/abs/2405.02198v1", "date": "2024-05-03", "relevancy": 1.6372, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5951}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5943}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Cambridge%20RoboMaster%3A%20An%20Agile%20Multi-Robot%20Research%20Platform&body=Title%3A%20The%20Cambridge%20RoboMaster%3A%20An%20Agile%20Multi-Robot%20Research%20Platform%0AAuthor%3A%20Jan%20Blumenkamp%20and%20Ajay%20Shankar%20and%20Matteo%20Bettini%20and%20Joshua%20Bird%20and%20Amanda%20Prorok%0AAbstract%3A%20%20%20Compact%20robotic%20platforms%20with%20powerful%20compute%20and%20actuation%20capabilities%0Aare%20key%20enablers%20for%20practical%2C%20real-world%20deployments%20of%20multi-agent%20research.%0AThis%20article%20introduces%20a%20tightly%20integrated%20hardware%2C%20control%2C%20and%20simulation%0Asoftware%20stack%20on%20a%20fleet%20of%20holonomic%20ground%20robot%20platforms%20designed%20with%0Athis%20motivation.%20Our%20robots%2C%20a%20fleet%20of%20customised%20DJI%20Robomaster%20S1%20vehicles%2C%0Aoffer%20a%20balance%20between%20small%20robots%20that%20do%20not%20possess%20sufficient%20compute%20or%0Aactuation%20capabilities%20and%20larger%20robots%20that%20are%20unsuitable%20for%20indoor%0Amulti-robot%20tests.%20They%20run%20a%20modular%20ROS2-based%20optimal%20estimation%20and%20control%0Astack%20for%20full%20onboard%20autonomy%2C%20contain%20ad-hoc%20peer-to-peer%20communication%0Ainfrastructure%2C%20and%20can%20zero-shot%20run%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Apolicies%20trained%20in%20our%20vectorized%20multi-agent%20simulation%20framework.%20We%20present%0Aan%20in-depth%20review%20of%20other%20platforms%20currently%20available%2C%20showcase%20new%0Aexperimental%20validation%20of%20our%20system%27s%20capabilities%2C%20and%20introduce%20case%0Astudies%20that%20highlight%20the%20versatility%20and%20reliabilty%20of%20our%20system%20as%20a%0Atestbed%20for%20a%20wide%20range%20of%20research%20demonstrations.%20Our%20system%20as%20well%20as%0Asupplementary%20material%20is%20available%20online%3A%0Ahttps%3A//proroklab.github.io/cambridge-robomaster%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Cambridge%2520RoboMaster%253A%2520An%2520Agile%2520Multi-Robot%2520Research%2520Platform%26entry.906535625%3DJan%2520Blumenkamp%2520and%2520Ajay%2520Shankar%2520and%2520Matteo%2520Bettini%2520and%2520Joshua%2520Bird%2520and%2520Amanda%2520Prorok%26entry.1292438233%3D%2520%2520Compact%2520robotic%2520platforms%2520with%2520powerful%2520compute%2520and%2520actuation%2520capabilities%250Aare%2520key%2520enablers%2520for%2520practical%252C%2520real-world%2520deployments%2520of%2520multi-agent%2520research.%250AThis%2520article%2520introduces%2520a%2520tightly%2520integrated%2520hardware%252C%2520control%252C%2520and%2520simulation%250Asoftware%2520stack%2520on%2520a%2520fleet%2520of%2520holonomic%2520ground%2520robot%2520platforms%2520designed%2520with%250Athis%2520motivation.%2520Our%2520robots%252C%2520a%2520fleet%2520of%2520customised%2520DJI%2520Robomaster%2520S1%2520vehicles%252C%250Aoffer%2520a%2520balance%2520between%2520small%2520robots%2520that%2520do%2520not%2520possess%2520sufficient%2520compute%2520or%250Aactuation%2520capabilities%2520and%2520larger%2520robots%2520that%2520are%2520unsuitable%2520for%2520indoor%250Amulti-robot%2520tests.%2520They%2520run%2520a%2520modular%2520ROS2-based%2520optimal%2520estimation%2520and%2520control%250Astack%2520for%2520full%2520onboard%2520autonomy%252C%2520contain%2520ad-hoc%2520peer-to-peer%2520communication%250Ainfrastructure%252C%2520and%2520can%2520zero-shot%2520run%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%250Apolicies%2520trained%2520in%2520our%2520vectorized%2520multi-agent%2520simulation%2520framework.%2520We%2520present%250Aan%2520in-depth%2520review%2520of%2520other%2520platforms%2520currently%2520available%252C%2520showcase%2520new%250Aexperimental%2520validation%2520of%2520our%2520system%2527s%2520capabilities%252C%2520and%2520introduce%2520case%250Astudies%2520that%2520highlight%2520the%2520versatility%2520and%2520reliabilty%2520of%2520our%2520system%2520as%2520a%250Atestbed%2520for%2520a%2520wide%2520range%2520of%2520research%2520demonstrations.%2520Our%2520system%2520as%2520well%2520as%250Asupplementary%2520material%2520is%2520available%2520online%253A%250Ahttps%253A//proroklab.github.io/cambridge-robomaster%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Cambridge%20RoboMaster%3A%20An%20Agile%20Multi-Robot%20Research%20Platform&entry.906535625=Jan%20Blumenkamp%20and%20Ajay%20Shankar%20and%20Matteo%20Bettini%20and%20Joshua%20Bird%20and%20Amanda%20Prorok&entry.1292438233=%20%20Compact%20robotic%20platforms%20with%20powerful%20compute%20and%20actuation%20capabilities%0Aare%20key%20enablers%20for%20practical%2C%20real-world%20deployments%20of%20multi-agent%20research.%0AThis%20article%20introduces%20a%20tightly%20integrated%20hardware%2C%20control%2C%20and%20simulation%0Asoftware%20stack%20on%20a%20fleet%20of%20holonomic%20ground%20robot%20platforms%20designed%20with%0Athis%20motivation.%20Our%20robots%2C%20a%20fleet%20of%20customised%20DJI%20Robomaster%20S1%20vehicles%2C%0Aoffer%20a%20balance%20between%20small%20robots%20that%20do%20not%20possess%20sufficient%20compute%20or%0Aactuation%20capabilities%20and%20larger%20robots%20that%20are%20unsuitable%20for%20indoor%0Amulti-robot%20tests.%20They%20run%20a%20modular%20ROS2-based%20optimal%20estimation%20and%20control%0Astack%20for%20full%20onboard%20autonomy%2C%20contain%20ad-hoc%20peer-to-peer%20communication%0Ainfrastructure%2C%20and%20can%20zero-shot%20run%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Apolicies%20trained%20in%20our%20vectorized%20multi-agent%20simulation%20framework.%20We%20present%0Aan%20in-depth%20review%20of%20other%20platforms%20currently%20available%2C%20showcase%20new%0Aexperimental%20validation%20of%20our%20system%27s%20capabilities%2C%20and%20introduce%20case%0Astudies%20that%20highlight%20the%20versatility%20and%20reliabilty%20of%20our%20system%20as%20a%0Atestbed%20for%20a%20wide%20range%20of%20research%20demonstrations.%20Our%20system%20as%20well%20as%0Asupplementary%20material%20is%20available%20online%3A%0Ahttps%3A//proroklab.github.io/cambridge-robomaster%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02198v1&entry.124074799=Read"},
{"title": "Causal Discovery Under Local Privacy", "author": "R\u016bta Binkyt\u0117 and Carlos Pinz\u00f3n and Szilvia Lesty\u00e1n and Kangsoo Jung and H\u00e9ber H. Arcolezi and Catuscia Palamidessi", "abstract": "  Differential privacy is a widely adopted framework designed to safeguard the\nsensitive information of data providers within a data set. It is based on the\napplication of controlled noise at the interface between the server that stores\nand processes the data, and the data consumers. Local differential privacy is a\nvariant that allows data providers to apply the privatization mechanism\nthemselves on their data individually. Therefore it provides protection also in\ncontexts in which the server, or even the data collector, cannot be trusted.\nThe introduction of noise, however, inevitably affects the utility of the data,\nparticularly by distorting the correlations between individual data components.\nThis distortion can prove detrimental to tasks such as causal discovery. In\nthis paper, we consider various well-known locally differentially private\nmechanisms and compare the trade-off between the privacy they provide, and the\naccuracy of the causal structure produced by algorithms for causal learning\nwhen applied to data obfuscated by these mechanisms. Our analysis yields\nvaluable insights for selecting appropriate local differentially private\nprotocols for causal discovery tasks. We foresee that our findings will aid\nresearchers and practitioners in conducting locally private causal discovery.\n", "link": "http://arxiv.org/abs/2311.04037v3", "date": "2024-05-03", "relevancy": 1.6349, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4083}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Discovery%20Under%20Local%20Privacy&body=Title%3A%20Causal%20Discovery%20Under%20Local%20Privacy%0AAuthor%3A%20R%C5%ABta%20Binkyt%C4%97%20and%20Carlos%20Pinz%C3%B3n%20and%20Szilvia%20Lesty%C3%A1n%20and%20Kangsoo%20Jung%20and%20H%C3%A9ber%20H.%20Arcolezi%20and%20Catuscia%20Palamidessi%0AAbstract%3A%20%20%20Differential%20privacy%20is%20a%20widely%20adopted%20framework%20designed%20to%20safeguard%20the%0Asensitive%20information%20of%20data%20providers%20within%20a%20data%20set.%20It%20is%20based%20on%20the%0Aapplication%20of%20controlled%20noise%20at%20the%20interface%20between%20the%20server%20that%20stores%0Aand%20processes%20the%20data%2C%20and%20the%20data%20consumers.%20Local%20differential%20privacy%20is%20a%0Avariant%20that%20allows%20data%20providers%20to%20apply%20the%20privatization%20mechanism%0Athemselves%20on%20their%20data%20individually.%20Therefore%20it%20provides%20protection%20also%20in%0Acontexts%20in%20which%20the%20server%2C%20or%20even%20the%20data%20collector%2C%20cannot%20be%20trusted.%0AThe%20introduction%20of%20noise%2C%20however%2C%20inevitably%20affects%20the%20utility%20of%20the%20data%2C%0Aparticularly%20by%20distorting%20the%20correlations%20between%20individual%20data%20components.%0AThis%20distortion%20can%20prove%20detrimental%20to%20tasks%20such%20as%20causal%20discovery.%20In%0Athis%20paper%2C%20we%20consider%20various%20well-known%20locally%20differentially%20private%0Amechanisms%20and%20compare%20the%20trade-off%20between%20the%20privacy%20they%20provide%2C%20and%20the%0Aaccuracy%20of%20the%20causal%20structure%20produced%20by%20algorithms%20for%20causal%20learning%0Awhen%20applied%20to%20data%20obfuscated%20by%20these%20mechanisms.%20Our%20analysis%20yields%0Avaluable%20insights%20for%20selecting%20appropriate%20local%20differentially%20private%0Aprotocols%20for%20causal%20discovery%20tasks.%20We%20foresee%20that%20our%20findings%20will%20aid%0Aresearchers%20and%20practitioners%20in%20conducting%20locally%20private%20causal%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04037v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Discovery%2520Under%2520Local%2520Privacy%26entry.906535625%3DR%25C5%25ABta%2520Binkyt%25C4%2597%2520and%2520Carlos%2520Pinz%25C3%25B3n%2520and%2520Szilvia%2520Lesty%25C3%25A1n%2520and%2520Kangsoo%2520Jung%2520and%2520H%25C3%25A9ber%2520H.%2520Arcolezi%2520and%2520Catuscia%2520Palamidessi%26entry.1292438233%3D%2520%2520Differential%2520privacy%2520is%2520a%2520widely%2520adopted%2520framework%2520designed%2520to%2520safeguard%2520the%250Asensitive%2520information%2520of%2520data%2520providers%2520within%2520a%2520data%2520set.%2520It%2520is%2520based%2520on%2520the%250Aapplication%2520of%2520controlled%2520noise%2520at%2520the%2520interface%2520between%2520the%2520server%2520that%2520stores%250Aand%2520processes%2520the%2520data%252C%2520and%2520the%2520data%2520consumers.%2520Local%2520differential%2520privacy%2520is%2520a%250Avariant%2520that%2520allows%2520data%2520providers%2520to%2520apply%2520the%2520privatization%2520mechanism%250Athemselves%2520on%2520their%2520data%2520individually.%2520Therefore%2520it%2520provides%2520protection%2520also%2520in%250Acontexts%2520in%2520which%2520the%2520server%252C%2520or%2520even%2520the%2520data%2520collector%252C%2520cannot%2520be%2520trusted.%250AThe%2520introduction%2520of%2520noise%252C%2520however%252C%2520inevitably%2520affects%2520the%2520utility%2520of%2520the%2520data%252C%250Aparticularly%2520by%2520distorting%2520the%2520correlations%2520between%2520individual%2520data%2520components.%250AThis%2520distortion%2520can%2520prove%2520detrimental%2520to%2520tasks%2520such%2520as%2520causal%2520discovery.%2520In%250Athis%2520paper%252C%2520we%2520consider%2520various%2520well-known%2520locally%2520differentially%2520private%250Amechanisms%2520and%2520compare%2520the%2520trade-off%2520between%2520the%2520privacy%2520they%2520provide%252C%2520and%2520the%250Aaccuracy%2520of%2520the%2520causal%2520structure%2520produced%2520by%2520algorithms%2520for%2520causal%2520learning%250Awhen%2520applied%2520to%2520data%2520obfuscated%2520by%2520these%2520mechanisms.%2520Our%2520analysis%2520yields%250Avaluable%2520insights%2520for%2520selecting%2520appropriate%2520local%2520differentially%2520private%250Aprotocols%2520for%2520causal%2520discovery%2520tasks.%2520We%2520foresee%2520that%2520our%2520findings%2520will%2520aid%250Aresearchers%2520and%2520practitioners%2520in%2520conducting%2520locally%2520private%2520causal%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04037v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Discovery%20Under%20Local%20Privacy&entry.906535625=R%C5%ABta%20Binkyt%C4%97%20and%20Carlos%20Pinz%C3%B3n%20and%20Szilvia%20Lesty%C3%A1n%20and%20Kangsoo%20Jung%20and%20H%C3%A9ber%20H.%20Arcolezi%20and%20Catuscia%20Palamidessi&entry.1292438233=%20%20Differential%20privacy%20is%20a%20widely%20adopted%20framework%20designed%20to%20safeguard%20the%0Asensitive%20information%20of%20data%20providers%20within%20a%20data%20set.%20It%20is%20based%20on%20the%0Aapplication%20of%20controlled%20noise%20at%20the%20interface%20between%20the%20server%20that%20stores%0Aand%20processes%20the%20data%2C%20and%20the%20data%20consumers.%20Local%20differential%20privacy%20is%20a%0Avariant%20that%20allows%20data%20providers%20to%20apply%20the%20privatization%20mechanism%0Athemselves%20on%20their%20data%20individually.%20Therefore%20it%20provides%20protection%20also%20in%0Acontexts%20in%20which%20the%20server%2C%20or%20even%20the%20data%20collector%2C%20cannot%20be%20trusted.%0AThe%20introduction%20of%20noise%2C%20however%2C%20inevitably%20affects%20the%20utility%20of%20the%20data%2C%0Aparticularly%20by%20distorting%20the%20correlations%20between%20individual%20data%20components.%0AThis%20distortion%20can%20prove%20detrimental%20to%20tasks%20such%20as%20causal%20discovery.%20In%0Athis%20paper%2C%20we%20consider%20various%20well-known%20locally%20differentially%20private%0Amechanisms%20and%20compare%20the%20trade-off%20between%20the%20privacy%20they%20provide%2C%20and%20the%0Aaccuracy%20of%20the%20causal%20structure%20produced%20by%20algorithms%20for%20causal%20learning%0Awhen%20applied%20to%20data%20obfuscated%20by%20these%20mechanisms.%20Our%20analysis%20yields%0Avaluable%20insights%20for%20selecting%20appropriate%20local%20differentially%20private%0Aprotocols%20for%20causal%20discovery%20tasks.%20We%20foresee%20that%20our%20findings%20will%20aid%0Aresearchers%20and%20practitioners%20in%20conducting%20locally%20private%20causal%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04037v3&entry.124074799=Read"},
{"title": "Deep Learning Forecasts Caldera Collapse Events at Kilauea Volcano", "author": "Ian W. McBrearty and Paul Segall", "abstract": "  During the three month long eruption of Kilauea volcano, Hawaii in 2018, the\npre-existing summit caldera collapsed in over 60 quasi-periodic failure events.\nThe last 40 of these events, which generated Mw >5 very long period (VLP)\nearthquakes, had inter-event times between 0.8 - 2.2 days. These failure events\noffer a unique dataset for testing methods for predicting earthquake recurrence\nbased on locally recorded GPS, tilt, and seismicity data. In this work, we\ntrain a deep learning graph neural network (GNN) to predict the time-to-failure\nof the caldera collapse events using only a fraction of the data recorded at\nthe start of each cycle. We find that the GNN generalizes to unseen data and\ncan predict the time-to-failure to within a few hours using only 0.5 days of\ndata, substantially improving upon a null model based only on inter-event\nstatistics. Predictions improve with increasing input data length, and are most\naccurate when using high-SNR tilt-meter data. Applying the trained GNN to\nsynthetic data with different magma pressure decay times predicts failure at a\nnearly constant stress threshold, revealing that the GNN is sensing the\nunderling physics of caldera collapse. These findings demonstrate the\npredictability of caldera collapse sequences under well monitored conditions,\nand highlight the potential of machine learning methods for forecasting real\nworld catastrophic events with limited training data.\n", "link": "http://arxiv.org/abs/2404.19351v2", "date": "2024-05-03", "relevancy": 1.6344, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4131}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.407}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Forecasts%20Caldera%20Collapse%20Events%20at%20Kilauea%20Volcano&body=Title%3A%20Deep%20Learning%20Forecasts%20Caldera%20Collapse%20Events%20at%20Kilauea%20Volcano%0AAuthor%3A%20Ian%20W.%20McBrearty%20and%20Paul%20Segall%0AAbstract%3A%20%20%20During%20the%20three%20month%20long%20eruption%20of%20Kilauea%20volcano%2C%20Hawaii%20in%202018%2C%20the%0Apre-existing%20summit%20caldera%20collapsed%20in%20over%2060%20quasi-periodic%20failure%20events.%0AThe%20last%2040%20of%20these%20events%2C%20which%20generated%20Mw%20%3E5%20very%20long%20period%20%28VLP%29%0Aearthquakes%2C%20had%20inter-event%20times%20between%200.8%20-%202.2%20days.%20These%20failure%20events%0Aoffer%20a%20unique%20dataset%20for%20testing%20methods%20for%20predicting%20earthquake%20recurrence%0Abased%20on%20locally%20recorded%20GPS%2C%20tilt%2C%20and%20seismicity%20data.%20In%20this%20work%2C%20we%0Atrain%20a%20deep%20learning%20graph%20neural%20network%20%28GNN%29%20to%20predict%20the%20time-to-failure%0Aof%20the%20caldera%20collapse%20events%20using%20only%20a%20fraction%20of%20the%20data%20recorded%20at%0Athe%20start%20of%20each%20cycle.%20We%20find%20that%20the%20GNN%20generalizes%20to%20unseen%20data%20and%0Acan%20predict%20the%20time-to-failure%20to%20within%20a%20few%20hours%20using%20only%200.5%20days%20of%0Adata%2C%20substantially%20improving%20upon%20a%20null%20model%20based%20only%20on%20inter-event%0Astatistics.%20Predictions%20improve%20with%20increasing%20input%20data%20length%2C%20and%20are%20most%0Aaccurate%20when%20using%20high-SNR%20tilt-meter%20data.%20Applying%20the%20trained%20GNN%20to%0Asynthetic%20data%20with%20different%20magma%20pressure%20decay%20times%20predicts%20failure%20at%20a%0Anearly%20constant%20stress%20threshold%2C%20revealing%20that%20the%20GNN%20is%20sensing%20the%0Aunderling%20physics%20of%20caldera%20collapse.%20These%20findings%20demonstrate%20the%0Apredictability%20of%20caldera%20collapse%20sequences%20under%20well%20monitored%20conditions%2C%0Aand%20highlight%20the%20potential%20of%20machine%20learning%20methods%20for%20forecasting%20real%0Aworld%20catastrophic%20events%20with%20limited%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Forecasts%2520Caldera%2520Collapse%2520Events%2520at%2520Kilauea%2520Volcano%26entry.906535625%3DIan%2520W.%2520McBrearty%2520and%2520Paul%2520Segall%26entry.1292438233%3D%2520%2520During%2520the%2520three%2520month%2520long%2520eruption%2520of%2520Kilauea%2520volcano%252C%2520Hawaii%2520in%25202018%252C%2520the%250Apre-existing%2520summit%2520caldera%2520collapsed%2520in%2520over%252060%2520quasi-periodic%2520failure%2520events.%250AThe%2520last%252040%2520of%2520these%2520events%252C%2520which%2520generated%2520Mw%2520%253E5%2520very%2520long%2520period%2520%2528VLP%2529%250Aearthquakes%252C%2520had%2520inter-event%2520times%2520between%25200.8%2520-%25202.2%2520days.%2520These%2520failure%2520events%250Aoffer%2520a%2520unique%2520dataset%2520for%2520testing%2520methods%2520for%2520predicting%2520earthquake%2520recurrence%250Abased%2520on%2520locally%2520recorded%2520GPS%252C%2520tilt%252C%2520and%2520seismicity%2520data.%2520In%2520this%2520work%252C%2520we%250Atrain%2520a%2520deep%2520learning%2520graph%2520neural%2520network%2520%2528GNN%2529%2520to%2520predict%2520the%2520time-to-failure%250Aof%2520the%2520caldera%2520collapse%2520events%2520using%2520only%2520a%2520fraction%2520of%2520the%2520data%2520recorded%2520at%250Athe%2520start%2520of%2520each%2520cycle.%2520We%2520find%2520that%2520the%2520GNN%2520generalizes%2520to%2520unseen%2520data%2520and%250Acan%2520predict%2520the%2520time-to-failure%2520to%2520within%2520a%2520few%2520hours%2520using%2520only%25200.5%2520days%2520of%250Adata%252C%2520substantially%2520improving%2520upon%2520a%2520null%2520model%2520based%2520only%2520on%2520inter-event%250Astatistics.%2520Predictions%2520improve%2520with%2520increasing%2520input%2520data%2520length%252C%2520and%2520are%2520most%250Aaccurate%2520when%2520using%2520high-SNR%2520tilt-meter%2520data.%2520Applying%2520the%2520trained%2520GNN%2520to%250Asynthetic%2520data%2520with%2520different%2520magma%2520pressure%2520decay%2520times%2520predicts%2520failure%2520at%2520a%250Anearly%2520constant%2520stress%2520threshold%252C%2520revealing%2520that%2520the%2520GNN%2520is%2520sensing%2520the%250Aunderling%2520physics%2520of%2520caldera%2520collapse.%2520These%2520findings%2520demonstrate%2520the%250Apredictability%2520of%2520caldera%2520collapse%2520sequences%2520under%2520well%2520monitored%2520conditions%252C%250Aand%2520highlight%2520the%2520potential%2520of%2520machine%2520learning%2520methods%2520for%2520forecasting%2520real%250Aworld%2520catastrophic%2520events%2520with%2520limited%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Forecasts%20Caldera%20Collapse%20Events%20at%20Kilauea%20Volcano&entry.906535625=Ian%20W.%20McBrearty%20and%20Paul%20Segall&entry.1292438233=%20%20During%20the%20three%20month%20long%20eruption%20of%20Kilauea%20volcano%2C%20Hawaii%20in%202018%2C%20the%0Apre-existing%20summit%20caldera%20collapsed%20in%20over%2060%20quasi-periodic%20failure%20events.%0AThe%20last%2040%20of%20these%20events%2C%20which%20generated%20Mw%20%3E5%20very%20long%20period%20%28VLP%29%0Aearthquakes%2C%20had%20inter-event%20times%20between%200.8%20-%202.2%20days.%20These%20failure%20events%0Aoffer%20a%20unique%20dataset%20for%20testing%20methods%20for%20predicting%20earthquake%20recurrence%0Abased%20on%20locally%20recorded%20GPS%2C%20tilt%2C%20and%20seismicity%20data.%20In%20this%20work%2C%20we%0Atrain%20a%20deep%20learning%20graph%20neural%20network%20%28GNN%29%20to%20predict%20the%20time-to-failure%0Aof%20the%20caldera%20collapse%20events%20using%20only%20a%20fraction%20of%20the%20data%20recorded%20at%0Athe%20start%20of%20each%20cycle.%20We%20find%20that%20the%20GNN%20generalizes%20to%20unseen%20data%20and%0Acan%20predict%20the%20time-to-failure%20to%20within%20a%20few%20hours%20using%20only%200.5%20days%20of%0Adata%2C%20substantially%20improving%20upon%20a%20null%20model%20based%20only%20on%20inter-event%0Astatistics.%20Predictions%20improve%20with%20increasing%20input%20data%20length%2C%20and%20are%20most%0Aaccurate%20when%20using%20high-SNR%20tilt-meter%20data.%20Applying%20the%20trained%20GNN%20to%0Asynthetic%20data%20with%20different%20magma%20pressure%20decay%20times%20predicts%20failure%20at%20a%0Anearly%20constant%20stress%20threshold%2C%20revealing%20that%20the%20GNN%20is%20sensing%20the%0Aunderling%20physics%20of%20caldera%20collapse.%20These%20findings%20demonstrate%20the%0Apredictability%20of%20caldera%20collapse%20sequences%20under%20well%20monitored%20conditions%2C%0Aand%20highlight%20the%20potential%20of%20machine%20learning%20methods%20for%20forecasting%20real%0Aworld%20catastrophic%20events%20with%20limited%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19351v2&entry.124074799=Read"},
{"title": "Probablistic Restoration with Adaptive Noise Sampling for 3D Human Pose\n  Estimation", "author": "Xianzhou Zeng and Hao Qin and Ming Kong and Luyuan Chen and Qiang Zhu", "abstract": "  The accuracy and robustness of 3D human pose estimation (HPE) are limited by\n2D pose detection errors and 2D to 3D ill-posed challenges, which have drawn\ngreat attention to Multi-Hypothesis HPE research. Most existing MH-HPE methods\nare based on generative models, which are computationally expensive and\ndifficult to train. In this study, we propose a Probabilistic Restoration 3D\nHuman Pose Estimation framework (PRPose) that can be integrated with any\nlightweight single-hypothesis model. Specifically, PRPose employs a weakly\nsupervised approach to fit the hidden probability distribution of the 2D-to-3D\nlifting process in the Single-Hypothesis HPE model and then reverse-map the\ndistribution to the 2D pose input through an adaptive noise sampling strategy\nto generate reasonable multi-hypothesis samples effectively. Extensive\nexperiments on 3D HPE benchmarks (Human3.6M and MPI-INF-3DHP) highlight the\neffectiveness and efficiency of PRPose. Code is available at:\nhttps://github.com/xzhouzeng/PRPose.\n", "link": "http://arxiv.org/abs/2405.02114v1", "date": "2024-05-03", "relevancy": 1.6221, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5497}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5381}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probablistic%20Restoration%20with%20Adaptive%20Noise%20Sampling%20for%203D%20Human%20Pose%0A%20%20Estimation&body=Title%3A%20Probablistic%20Restoration%20with%20Adaptive%20Noise%20Sampling%20for%203D%20Human%20Pose%0A%20%20Estimation%0AAuthor%3A%20Xianzhou%20Zeng%20and%20Hao%20Qin%20and%20Ming%20Kong%20and%20Luyuan%20Chen%20and%20Qiang%20Zhu%0AAbstract%3A%20%20%20The%20accuracy%20and%20robustness%20of%203D%20human%20pose%20estimation%20%28HPE%29%20are%20limited%20by%0A2D%20pose%20detection%20errors%20and%202D%20to%203D%20ill-posed%20challenges%2C%20which%20have%20drawn%0Agreat%20attention%20to%20Multi-Hypothesis%20HPE%20research.%20Most%20existing%20MH-HPE%20methods%0Aare%20based%20on%20generative%20models%2C%20which%20are%20computationally%20expensive%20and%0Adifficult%20to%20train.%20In%20this%20study%2C%20we%20propose%20a%20Probabilistic%20Restoration%203D%0AHuman%20Pose%20Estimation%20framework%20%28PRPose%29%20that%20can%20be%20integrated%20with%20any%0Alightweight%20single-hypothesis%20model.%20Specifically%2C%20PRPose%20employs%20a%20weakly%0Asupervised%20approach%20to%20fit%20the%20hidden%20probability%20distribution%20of%20the%202D-to-3D%0Alifting%20process%20in%20the%20Single-Hypothesis%20HPE%20model%20and%20then%20reverse-map%20the%0Adistribution%20to%20the%202D%20pose%20input%20through%20an%20adaptive%20noise%20sampling%20strategy%0Ato%20generate%20reasonable%20multi-hypothesis%20samples%20effectively.%20Extensive%0Aexperiments%20on%203D%20HPE%20benchmarks%20%28Human3.6M%20and%20MPI-INF-3DHP%29%20highlight%20the%0Aeffectiveness%20and%20efficiency%20of%20PRPose.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/xzhouzeng/PRPose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbablistic%2520Restoration%2520with%2520Adaptive%2520Noise%2520Sampling%2520for%25203D%2520Human%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DXianzhou%2520Zeng%2520and%2520Hao%2520Qin%2520and%2520Ming%2520Kong%2520and%2520Luyuan%2520Chen%2520and%2520Qiang%2520Zhu%26entry.1292438233%3D%2520%2520The%2520accuracy%2520and%2520robustness%2520of%25203D%2520human%2520pose%2520estimation%2520%2528HPE%2529%2520are%2520limited%2520by%250A2D%2520pose%2520detection%2520errors%2520and%25202D%2520to%25203D%2520ill-posed%2520challenges%252C%2520which%2520have%2520drawn%250Agreat%2520attention%2520to%2520Multi-Hypothesis%2520HPE%2520research.%2520Most%2520existing%2520MH-HPE%2520methods%250Aare%2520based%2520on%2520generative%2520models%252C%2520which%2520are%2520computationally%2520expensive%2520and%250Adifficult%2520to%2520train.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520Probabilistic%2520Restoration%25203D%250AHuman%2520Pose%2520Estimation%2520framework%2520%2528PRPose%2529%2520that%2520can%2520be%2520integrated%2520with%2520any%250Alightweight%2520single-hypothesis%2520model.%2520Specifically%252C%2520PRPose%2520employs%2520a%2520weakly%250Asupervised%2520approach%2520to%2520fit%2520the%2520hidden%2520probability%2520distribution%2520of%2520the%25202D-to-3D%250Alifting%2520process%2520in%2520the%2520Single-Hypothesis%2520HPE%2520model%2520and%2520then%2520reverse-map%2520the%250Adistribution%2520to%2520the%25202D%2520pose%2520input%2520through%2520an%2520adaptive%2520noise%2520sampling%2520strategy%250Ato%2520generate%2520reasonable%2520multi-hypothesis%2520samples%2520effectively.%2520Extensive%250Aexperiments%2520on%25203D%2520HPE%2520benchmarks%2520%2528Human3.6M%2520and%2520MPI-INF-3DHP%2529%2520highlight%2520the%250Aeffectiveness%2520and%2520efficiency%2520of%2520PRPose.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/xzhouzeng/PRPose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probablistic%20Restoration%20with%20Adaptive%20Noise%20Sampling%20for%203D%20Human%20Pose%0A%20%20Estimation&entry.906535625=Xianzhou%20Zeng%20and%20Hao%20Qin%20and%20Ming%20Kong%20and%20Luyuan%20Chen%20and%20Qiang%20Zhu&entry.1292438233=%20%20The%20accuracy%20and%20robustness%20of%203D%20human%20pose%20estimation%20%28HPE%29%20are%20limited%20by%0A2D%20pose%20detection%20errors%20and%202D%20to%203D%20ill-posed%20challenges%2C%20which%20have%20drawn%0Agreat%20attention%20to%20Multi-Hypothesis%20HPE%20research.%20Most%20existing%20MH-HPE%20methods%0Aare%20based%20on%20generative%20models%2C%20which%20are%20computationally%20expensive%20and%0Adifficult%20to%20train.%20In%20this%20study%2C%20we%20propose%20a%20Probabilistic%20Restoration%203D%0AHuman%20Pose%20Estimation%20framework%20%28PRPose%29%20that%20can%20be%20integrated%20with%20any%0Alightweight%20single-hypothesis%20model.%20Specifically%2C%20PRPose%20employs%20a%20weakly%0Asupervised%20approach%20to%20fit%20the%20hidden%20probability%20distribution%20of%20the%202D-to-3D%0Alifting%20process%20in%20the%20Single-Hypothesis%20HPE%20model%20and%20then%20reverse-map%20the%0Adistribution%20to%20the%202D%20pose%20input%20through%20an%20adaptive%20noise%20sampling%20strategy%0Ato%20generate%20reasonable%20multi-hypothesis%20samples%20effectively.%20Extensive%0Aexperiments%20on%203D%20HPE%20benchmarks%20%28Human3.6M%20and%20MPI-INF-3DHP%29%20highlight%20the%0Aeffectiveness%20and%20efficiency%20of%20PRPose.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/xzhouzeng/PRPose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02114v1&entry.124074799=Read"},
{"title": "Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation", "author": "Hao Tang and Lianglun Cheng and Guoheng Huang and Zhengguang Tan and Junhao Lu and Kaihong Wu", "abstract": "  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n", "link": "http://arxiv.org/abs/2403.17701v4", "date": "2024-05-03", "relevancy": 1.6085, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5202}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rotate%20to%20Scan%3A%20UNet-like%20Mamba%20with%20Triplet%20SSM%20Module%20for%20Medical%0A%20%20Image%20Segmentation&body=Title%3A%20Rotate%20to%20Scan%3A%20UNet-like%20Mamba%20with%20Triplet%20SSM%20Module%20for%20Medical%0A%20%20Image%20Segmentation%0AAuthor%3A%20Hao%20Tang%20and%20Lianglun%20Cheng%20and%20Guoheng%20Huang%20and%20Zhengguang%20Tan%20and%20Junhao%20Lu%20and%20Kaihong%20Wu%0AAbstract%3A%20%20%20Image%20segmentation%20holds%20a%20vital%20position%20in%20the%20realms%20of%20diagnosis%20and%0Atreatment%20within%20the%20medical%20domain.%20Traditional%20convolutional%20neural%20networks%0A%28CNNs%29%20and%20Transformer%20models%20have%20made%20significant%20advancements%20in%20this%20realm%2C%0Abut%20they%20still%20encounter%20challenges%20because%20of%20limited%20receptive%20field%20or%20high%0Acomputing%20complexity.%20Recently%2C%20State%20Space%20Models%20%28SSMs%29%2C%20particularly%20Mamba%0Aand%20its%20variants%2C%20have%20demonstrated%20notable%20performance%20in%20the%20field%20of%20vision.%0AHowever%2C%20their%20feature%20extraction%20methods%20may%20not%20be%20sufficiently%20effective%20and%0Aretain%20some%20redundant%20structures%2C%20leaving%20room%20for%20parameter%20reduction.%0AMotivated%20by%20previous%20spatial%20and%20channel%20attention%20methods%2C%20we%20propose%20Triplet%0AMamba-UNet.%20The%20method%20leverages%20residual%20VSS%20Blocks%20to%20extract%20intensive%0Acontextual%20features%2C%20while%20Triplet%20SSM%20is%20employed%20to%20fuse%20features%20across%0Aspatial%20and%20channel%20dimensions.%20We%20conducted%20experiments%20on%20ISIC17%2C%20ISIC18%2C%0ACVC-300%2C%20CVC-ClinicDB%2C%20Kvasir-SEG%2C%20CVC-ColonDB%2C%20and%20Kvasir-Instrument%20datasets%2C%0Ademonstrating%20the%20superior%20segmentation%20performance%20of%20our%20proposed%20TM-UNet.%0AAdditionally%2C%20compared%20to%20the%20previous%20VM-UNet%2C%20our%20model%20achieves%20a%20one-third%0Areduction%20in%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17701v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRotate%2520to%2520Scan%253A%2520UNet-like%2520Mamba%2520with%2520Triplet%2520SSM%2520Module%2520for%2520Medical%250A%2520%2520Image%2520Segmentation%26entry.906535625%3DHao%2520Tang%2520and%2520Lianglun%2520Cheng%2520and%2520Guoheng%2520Huang%2520and%2520Zhengguang%2520Tan%2520and%2520Junhao%2520Lu%2520and%2520Kaihong%2520Wu%26entry.1292438233%3D%2520%2520Image%2520segmentation%2520holds%2520a%2520vital%2520position%2520in%2520the%2520realms%2520of%2520diagnosis%2520and%250Atreatment%2520within%2520the%2520medical%2520domain.%2520Traditional%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520and%2520Transformer%2520models%2520have%2520made%2520significant%2520advancements%2520in%2520this%2520realm%252C%250Abut%2520they%2520still%2520encounter%2520challenges%2520because%2520of%2520limited%2520receptive%2520field%2520or%2520high%250Acomputing%2520complexity.%2520Recently%252C%2520State%2520Space%2520Models%2520%2528SSMs%2529%252C%2520particularly%2520Mamba%250Aand%2520its%2520variants%252C%2520have%2520demonstrated%2520notable%2520performance%2520in%2520the%2520field%2520of%2520vision.%250AHowever%252C%2520their%2520feature%2520extraction%2520methods%2520may%2520not%2520be%2520sufficiently%2520effective%2520and%250Aretain%2520some%2520redundant%2520structures%252C%2520leaving%2520room%2520for%2520parameter%2520reduction.%250AMotivated%2520by%2520previous%2520spatial%2520and%2520channel%2520attention%2520methods%252C%2520we%2520propose%2520Triplet%250AMamba-UNet.%2520The%2520method%2520leverages%2520residual%2520VSS%2520Blocks%2520to%2520extract%2520intensive%250Acontextual%2520features%252C%2520while%2520Triplet%2520SSM%2520is%2520employed%2520to%2520fuse%2520features%2520across%250Aspatial%2520and%2520channel%2520dimensions.%2520We%2520conducted%2520experiments%2520on%2520ISIC17%252C%2520ISIC18%252C%250ACVC-300%252C%2520CVC-ClinicDB%252C%2520Kvasir-SEG%252C%2520CVC-ColonDB%252C%2520and%2520Kvasir-Instrument%2520datasets%252C%250Ademonstrating%2520the%2520superior%2520segmentation%2520performance%2520of%2520our%2520proposed%2520TM-UNet.%250AAdditionally%252C%2520compared%2520to%2520the%2520previous%2520VM-UNet%252C%2520our%2520model%2520achieves%2520a%2520one-third%250Areduction%2520in%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17701v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rotate%20to%20Scan%3A%20UNet-like%20Mamba%20with%20Triplet%20SSM%20Module%20for%20Medical%0A%20%20Image%20Segmentation&entry.906535625=Hao%20Tang%20and%20Lianglun%20Cheng%20and%20Guoheng%20Huang%20and%20Zhengguang%20Tan%20and%20Junhao%20Lu%20and%20Kaihong%20Wu&entry.1292438233=%20%20Image%20segmentation%20holds%20a%20vital%20position%20in%20the%20realms%20of%20diagnosis%20and%0Atreatment%20within%20the%20medical%20domain.%20Traditional%20convolutional%20neural%20networks%0A%28CNNs%29%20and%20Transformer%20models%20have%20made%20significant%20advancements%20in%20this%20realm%2C%0Abut%20they%20still%20encounter%20challenges%20because%20of%20limited%20receptive%20field%20or%20high%0Acomputing%20complexity.%20Recently%2C%20State%20Space%20Models%20%28SSMs%29%2C%20particularly%20Mamba%0Aand%20its%20variants%2C%20have%20demonstrated%20notable%20performance%20in%20the%20field%20of%20vision.%0AHowever%2C%20their%20feature%20extraction%20methods%20may%20not%20be%20sufficiently%20effective%20and%0Aretain%20some%20redundant%20structures%2C%20leaving%20room%20for%20parameter%20reduction.%0AMotivated%20by%20previous%20spatial%20and%20channel%20attention%20methods%2C%20we%20propose%20Triplet%0AMamba-UNet.%20The%20method%20leverages%20residual%20VSS%20Blocks%20to%20extract%20intensive%0Acontextual%20features%2C%20while%20Triplet%20SSM%20is%20employed%20to%20fuse%20features%20across%0Aspatial%20and%20channel%20dimensions.%20We%20conducted%20experiments%20on%20ISIC17%2C%20ISIC18%2C%0ACVC-300%2C%20CVC-ClinicDB%2C%20Kvasir-SEG%2C%20CVC-ColonDB%2C%20and%20Kvasir-Instrument%20datasets%2C%0Ademonstrating%20the%20superior%20segmentation%20performance%20of%20our%20proposed%20TM-UNet.%0AAdditionally%2C%20compared%20to%20the%20previous%20VM-UNet%2C%20our%20model%20achieves%20a%20one-third%0Areduction%20in%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17701v4&entry.124074799=Read"},
{"title": "Three-Dimensional Amyloid-Beta PET Synthesis from Structural MRI with\n  Conditional Generative Adversarial Networks", "author": "Fernando Vega and Abdoljalil Addeh and M. Ethan MacDonald", "abstract": "  Motivation: Alzheimer's Disease hallmarks include amyloid-beta deposits and\nbrain atrophy, detectable via PET and MRI scans, respectively. PET is\nexpensive, invasive and exposes patients to ionizing radiation. MRI is cheaper,\nnon-invasive, and free from ionizing radiation but limited to measuring brain\natrophy.\n  Goal: To develop an 3D image translation model that synthesizes amyloid-beta\nPET images from T1-weighted MRI, exploiting the known relationship between\namyloid-beta and brain atrophy.\n  Approach: The model was trained on 616 PET/MRI pairs and validated with 264\npairs.\n  Results: The model synthesized amyloid-beta PET images from T1-weighted MRI\nwith high-degree of similarity showing high SSIM and PSNR metrics\n(SSIM>0.95&PSNR=28).\n  Impact: Our model proves the feasibility of synthesizing amyloid-beta PET\nimages from structural MRI ones, significantly enhancing accessibility for\nlarge-cohort studies and early dementia detection, while also reducing cost,\ninvasiveness, and radiation exposure.\n", "link": "http://arxiv.org/abs/2405.02109v1", "date": "2024-05-03", "relevancy": 1.6066, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5452}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5303}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-Dimensional%20Amyloid-Beta%20PET%20Synthesis%20from%20Structural%20MRI%20with%0A%20%20Conditional%20Generative%20Adversarial%20Networks&body=Title%3A%20Three-Dimensional%20Amyloid-Beta%20PET%20Synthesis%20from%20Structural%20MRI%20with%0A%20%20Conditional%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Fernando%20Vega%20and%20Abdoljalil%20Addeh%20and%20M.%20Ethan%20MacDonald%0AAbstract%3A%20%20%20Motivation%3A%20Alzheimer%27s%20Disease%20hallmarks%20include%20amyloid-beta%20deposits%20and%0Abrain%20atrophy%2C%20detectable%20via%20PET%20and%20MRI%20scans%2C%20respectively.%20PET%20is%0Aexpensive%2C%20invasive%20and%20exposes%20patients%20to%20ionizing%20radiation.%20MRI%20is%20cheaper%2C%0Anon-invasive%2C%20and%20free%20from%20ionizing%20radiation%20but%20limited%20to%20measuring%20brain%0Aatrophy.%0A%20%20Goal%3A%20To%20develop%20an%203D%20image%20translation%20model%20that%20synthesizes%20amyloid-beta%0APET%20images%20from%20T1-weighted%20MRI%2C%20exploiting%20the%20known%20relationship%20between%0Aamyloid-beta%20and%20brain%20atrophy.%0A%20%20Approach%3A%20The%20model%20was%20trained%20on%20616%20PET/MRI%20pairs%20and%20validated%20with%20264%0Apairs.%0A%20%20Results%3A%20The%20model%20synthesized%20amyloid-beta%20PET%20images%20from%20T1-weighted%20MRI%0Awith%20high-degree%20of%20similarity%20showing%20high%20SSIM%20and%20PSNR%20metrics%0A%28SSIM%3E0.95%26PSNR%3D28%29.%0A%20%20Impact%3A%20Our%20model%20proves%20the%20feasibility%20of%20synthesizing%20amyloid-beta%20PET%0Aimages%20from%20structural%20MRI%20ones%2C%20significantly%20enhancing%20accessibility%20for%0Alarge-cohort%20studies%20and%20early%20dementia%20detection%2C%20while%20also%20reducing%20cost%2C%0Ainvasiveness%2C%20and%20radiation%20exposure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-Dimensional%2520Amyloid-Beta%2520PET%2520Synthesis%2520from%2520Structural%2520MRI%2520with%250A%2520%2520Conditional%2520Generative%2520Adversarial%2520Networks%26entry.906535625%3DFernando%2520Vega%2520and%2520Abdoljalil%2520Addeh%2520and%2520M.%2520Ethan%2520MacDonald%26entry.1292438233%3D%2520%2520Motivation%253A%2520Alzheimer%2527s%2520Disease%2520hallmarks%2520include%2520amyloid-beta%2520deposits%2520and%250Abrain%2520atrophy%252C%2520detectable%2520via%2520PET%2520and%2520MRI%2520scans%252C%2520respectively.%2520PET%2520is%250Aexpensive%252C%2520invasive%2520and%2520exposes%2520patients%2520to%2520ionizing%2520radiation.%2520MRI%2520is%2520cheaper%252C%250Anon-invasive%252C%2520and%2520free%2520from%2520ionizing%2520radiation%2520but%2520limited%2520to%2520measuring%2520brain%250Aatrophy.%250A%2520%2520Goal%253A%2520To%2520develop%2520an%25203D%2520image%2520translation%2520model%2520that%2520synthesizes%2520amyloid-beta%250APET%2520images%2520from%2520T1-weighted%2520MRI%252C%2520exploiting%2520the%2520known%2520relationship%2520between%250Aamyloid-beta%2520and%2520brain%2520atrophy.%250A%2520%2520Approach%253A%2520The%2520model%2520was%2520trained%2520on%2520616%2520PET/MRI%2520pairs%2520and%2520validated%2520with%2520264%250Apairs.%250A%2520%2520Results%253A%2520The%2520model%2520synthesized%2520amyloid-beta%2520PET%2520images%2520from%2520T1-weighted%2520MRI%250Awith%2520high-degree%2520of%2520similarity%2520showing%2520high%2520SSIM%2520and%2520PSNR%2520metrics%250A%2528SSIM%253E0.95%2526PSNR%253D28%2529.%250A%2520%2520Impact%253A%2520Our%2520model%2520proves%2520the%2520feasibility%2520of%2520synthesizing%2520amyloid-beta%2520PET%250Aimages%2520from%2520structural%2520MRI%2520ones%252C%2520significantly%2520enhancing%2520accessibility%2520for%250Alarge-cohort%2520studies%2520and%2520early%2520dementia%2520detection%252C%2520while%2520also%2520reducing%2520cost%252C%250Ainvasiveness%252C%2520and%2520radiation%2520exposure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-Dimensional%20Amyloid-Beta%20PET%20Synthesis%20from%20Structural%20MRI%20with%0A%20%20Conditional%20Generative%20Adversarial%20Networks&entry.906535625=Fernando%20Vega%20and%20Abdoljalil%20Addeh%20and%20M.%20Ethan%20MacDonald&entry.1292438233=%20%20Motivation%3A%20Alzheimer%27s%20Disease%20hallmarks%20include%20amyloid-beta%20deposits%20and%0Abrain%20atrophy%2C%20detectable%20via%20PET%20and%20MRI%20scans%2C%20respectively.%20PET%20is%0Aexpensive%2C%20invasive%20and%20exposes%20patients%20to%20ionizing%20radiation.%20MRI%20is%20cheaper%2C%0Anon-invasive%2C%20and%20free%20from%20ionizing%20radiation%20but%20limited%20to%20measuring%20brain%0Aatrophy.%0A%20%20Goal%3A%20To%20develop%20an%203D%20image%20translation%20model%20that%20synthesizes%20amyloid-beta%0APET%20images%20from%20T1-weighted%20MRI%2C%20exploiting%20the%20known%20relationship%20between%0Aamyloid-beta%20and%20brain%20atrophy.%0A%20%20Approach%3A%20The%20model%20was%20trained%20on%20616%20PET/MRI%20pairs%20and%20validated%20with%20264%0Apairs.%0A%20%20Results%3A%20The%20model%20synthesized%20amyloid-beta%20PET%20images%20from%20T1-weighted%20MRI%0Awith%20high-degree%20of%20similarity%20showing%20high%20SSIM%20and%20PSNR%20metrics%0A%28SSIM%3E0.95%26PSNR%3D28%29.%0A%20%20Impact%3A%20Our%20model%20proves%20the%20feasibility%20of%20synthesizing%20amyloid-beta%20PET%0Aimages%20from%20structural%20MRI%20ones%2C%20significantly%20enhancing%20accessibility%20for%0Alarge-cohort%20studies%20and%20early%20dementia%20detection%2C%20while%20also%20reducing%20cost%2C%0Ainvasiveness%2C%20and%20radiation%20exposure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02109v1&entry.124074799=Read"},
{"title": "A comparative study of conformal prediction methods for valid\n  uncertainty quantification in machine learning", "author": "Nicolas Dewolf", "abstract": "  In the past decades, most work in the area of data analysis and machine\nlearning was focused on optimizing predictive models and getting better results\nthan what was possible with existing models. To what extent the metrics with\nwhich such improvements were measured were accurately capturing the intended\ngoal, whether the numerical differences in the resulting values were\nsignificant, or whether uncertainty played a role in this study and if it\nshould have been taken into account, was of secondary importance. Whereas\nprobability theory, be it frequentist or Bayesian, used to be the gold standard\nin science before the advent of the supercomputer, it was quickly replaced in\nfavor of black box models and sheer computing power because of their ability to\nhandle large data sets. This evolution sadly happened at the expense of\ninterpretability and trustworthiness. However, while people are still trying to\nimprove the predictive power of their models, the community is starting to\nrealize that for many applications it is not so much the exact prediction that\nis of importance, but rather the variability or uncertainty.\n  The work in this dissertation tries to further the quest for a world where\neveryone is aware of uncertainty, of how important it is and how to embrace it\ninstead of fearing it. A specific, though general, framework that allows anyone\nto obtain accurate uncertainty estimates is singled out and analysed. Certain\naspects and applications of the framework -- dubbed `conformal prediction' --\nare studied in detail. Whereas many approaches to uncertainty quantification\nmake strong assumptions about the data, conformal prediction is, at the time of\nwriting, the only framework that deserves the title `distribution-free'. No\nparametric assumptions have to be made and the nonparametric results also hold\nwithout having to resort to the law of large numbers in the asymptotic regime.\n", "link": "http://arxiv.org/abs/2405.02082v1", "date": "2024-05-03", "relevancy": 1.6025, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5399}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5293}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comparative%20study%20of%20conformal%20prediction%20methods%20for%20valid%0A%20%20uncertainty%20quantification%20in%20machine%20learning&body=Title%3A%20A%20comparative%20study%20of%20conformal%20prediction%20methods%20for%20valid%0A%20%20uncertainty%20quantification%20in%20machine%20learning%0AAuthor%3A%20Nicolas%20Dewolf%0AAbstract%3A%20%20%20In%20the%20past%20decades%2C%20most%20work%20in%20the%20area%20of%20data%20analysis%20and%20machine%0Alearning%20was%20focused%20on%20optimizing%20predictive%20models%20and%20getting%20better%20results%0Athan%20what%20was%20possible%20with%20existing%20models.%20To%20what%20extent%20the%20metrics%20with%0Awhich%20such%20improvements%20were%20measured%20were%20accurately%20capturing%20the%20intended%0Agoal%2C%20whether%20the%20numerical%20differences%20in%20the%20resulting%20values%20were%0Asignificant%2C%20or%20whether%20uncertainty%20played%20a%20role%20in%20this%20study%20and%20if%20it%0Ashould%20have%20been%20taken%20into%20account%2C%20was%20of%20secondary%20importance.%20Whereas%0Aprobability%20theory%2C%20be%20it%20frequentist%20or%20Bayesian%2C%20used%20to%20be%20the%20gold%20standard%0Ain%20science%20before%20the%20advent%20of%20the%20supercomputer%2C%20it%20was%20quickly%20replaced%20in%0Afavor%20of%20black%20box%20models%20and%20sheer%20computing%20power%20because%20of%20their%20ability%20to%0Ahandle%20large%20data%20sets.%20This%20evolution%20sadly%20happened%20at%20the%20expense%20of%0Ainterpretability%20and%20trustworthiness.%20However%2C%20while%20people%20are%20still%20trying%20to%0Aimprove%20the%20predictive%20power%20of%20their%20models%2C%20the%20community%20is%20starting%20to%0Arealize%20that%20for%20many%20applications%20it%20is%20not%20so%20much%20the%20exact%20prediction%20that%0Ais%20of%20importance%2C%20but%20rather%20the%20variability%20or%20uncertainty.%0A%20%20The%20work%20in%20this%20dissertation%20tries%20to%20further%20the%20quest%20for%20a%20world%20where%0Aeveryone%20is%20aware%20of%20uncertainty%2C%20of%20how%20important%20it%20is%20and%20how%20to%20embrace%20it%0Ainstead%20of%20fearing%20it.%20A%20specific%2C%20though%20general%2C%20framework%20that%20allows%20anyone%0Ato%20obtain%20accurate%20uncertainty%20estimates%20is%20singled%20out%20and%20analysed.%20Certain%0Aaspects%20and%20applications%20of%20the%20framework%20--%20dubbed%20%60conformal%20prediction%27%20--%0Aare%20studied%20in%20detail.%20Whereas%20many%20approaches%20to%20uncertainty%20quantification%0Amake%20strong%20assumptions%20about%20the%20data%2C%20conformal%20prediction%20is%2C%20at%20the%20time%20of%0Awriting%2C%20the%20only%20framework%20that%20deserves%20the%20title%20%60distribution-free%27.%20No%0Aparametric%20assumptions%20have%20to%20be%20made%20and%20the%20nonparametric%20results%20also%20hold%0Awithout%20having%20to%20resort%20to%20the%20law%20of%20large%20numbers%20in%20the%20asymptotic%20regime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comparative%2520study%2520of%2520conformal%2520prediction%2520methods%2520for%2520valid%250A%2520%2520uncertainty%2520quantification%2520in%2520machine%2520learning%26entry.906535625%3DNicolas%2520Dewolf%26entry.1292438233%3D%2520%2520In%2520the%2520past%2520decades%252C%2520most%2520work%2520in%2520the%2520area%2520of%2520data%2520analysis%2520and%2520machine%250Alearning%2520was%2520focused%2520on%2520optimizing%2520predictive%2520models%2520and%2520getting%2520better%2520results%250Athan%2520what%2520was%2520possible%2520with%2520existing%2520models.%2520To%2520what%2520extent%2520the%2520metrics%2520with%250Awhich%2520such%2520improvements%2520were%2520measured%2520were%2520accurately%2520capturing%2520the%2520intended%250Agoal%252C%2520whether%2520the%2520numerical%2520differences%2520in%2520the%2520resulting%2520values%2520were%250Asignificant%252C%2520or%2520whether%2520uncertainty%2520played%2520a%2520role%2520in%2520this%2520study%2520and%2520if%2520it%250Ashould%2520have%2520been%2520taken%2520into%2520account%252C%2520was%2520of%2520secondary%2520importance.%2520Whereas%250Aprobability%2520theory%252C%2520be%2520it%2520frequentist%2520or%2520Bayesian%252C%2520used%2520to%2520be%2520the%2520gold%2520standard%250Ain%2520science%2520before%2520the%2520advent%2520of%2520the%2520supercomputer%252C%2520it%2520was%2520quickly%2520replaced%2520in%250Afavor%2520of%2520black%2520box%2520models%2520and%2520sheer%2520computing%2520power%2520because%2520of%2520their%2520ability%2520to%250Ahandle%2520large%2520data%2520sets.%2520This%2520evolution%2520sadly%2520happened%2520at%2520the%2520expense%2520of%250Ainterpretability%2520and%2520trustworthiness.%2520However%252C%2520while%2520people%2520are%2520still%2520trying%2520to%250Aimprove%2520the%2520predictive%2520power%2520of%2520their%2520models%252C%2520the%2520community%2520is%2520starting%2520to%250Arealize%2520that%2520for%2520many%2520applications%2520it%2520is%2520not%2520so%2520much%2520the%2520exact%2520prediction%2520that%250Ais%2520of%2520importance%252C%2520but%2520rather%2520the%2520variability%2520or%2520uncertainty.%250A%2520%2520The%2520work%2520in%2520this%2520dissertation%2520tries%2520to%2520further%2520the%2520quest%2520for%2520a%2520world%2520where%250Aeveryone%2520is%2520aware%2520of%2520uncertainty%252C%2520of%2520how%2520important%2520it%2520is%2520and%2520how%2520to%2520embrace%2520it%250Ainstead%2520of%2520fearing%2520it.%2520A%2520specific%252C%2520though%2520general%252C%2520framework%2520that%2520allows%2520anyone%250Ato%2520obtain%2520accurate%2520uncertainty%2520estimates%2520is%2520singled%2520out%2520and%2520analysed.%2520Certain%250Aaspects%2520and%2520applications%2520of%2520the%2520framework%2520--%2520dubbed%2520%2560conformal%2520prediction%2527%2520--%250Aare%2520studied%2520in%2520detail.%2520Whereas%2520many%2520approaches%2520to%2520uncertainty%2520quantification%250Amake%2520strong%2520assumptions%2520about%2520the%2520data%252C%2520conformal%2520prediction%2520is%252C%2520at%2520the%2520time%2520of%250Awriting%252C%2520the%2520only%2520framework%2520that%2520deserves%2520the%2520title%2520%2560distribution-free%2527.%2520No%250Aparametric%2520assumptions%2520have%2520to%2520be%2520made%2520and%2520the%2520nonparametric%2520results%2520also%2520hold%250Awithout%2520having%2520to%2520resort%2520to%2520the%2520law%2520of%2520large%2520numbers%2520in%2520the%2520asymptotic%2520regime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comparative%20study%20of%20conformal%20prediction%20methods%20for%20valid%0A%20%20uncertainty%20quantification%20in%20machine%20learning&entry.906535625=Nicolas%20Dewolf&entry.1292438233=%20%20In%20the%20past%20decades%2C%20most%20work%20in%20the%20area%20of%20data%20analysis%20and%20machine%0Alearning%20was%20focused%20on%20optimizing%20predictive%20models%20and%20getting%20better%20results%0Athan%20what%20was%20possible%20with%20existing%20models.%20To%20what%20extent%20the%20metrics%20with%0Awhich%20such%20improvements%20were%20measured%20were%20accurately%20capturing%20the%20intended%0Agoal%2C%20whether%20the%20numerical%20differences%20in%20the%20resulting%20values%20were%0Asignificant%2C%20or%20whether%20uncertainty%20played%20a%20role%20in%20this%20study%20and%20if%20it%0Ashould%20have%20been%20taken%20into%20account%2C%20was%20of%20secondary%20importance.%20Whereas%0Aprobability%20theory%2C%20be%20it%20frequentist%20or%20Bayesian%2C%20used%20to%20be%20the%20gold%20standard%0Ain%20science%20before%20the%20advent%20of%20the%20supercomputer%2C%20it%20was%20quickly%20replaced%20in%0Afavor%20of%20black%20box%20models%20and%20sheer%20computing%20power%20because%20of%20their%20ability%20to%0Ahandle%20large%20data%20sets.%20This%20evolution%20sadly%20happened%20at%20the%20expense%20of%0Ainterpretability%20and%20trustworthiness.%20However%2C%20while%20people%20are%20still%20trying%20to%0Aimprove%20the%20predictive%20power%20of%20their%20models%2C%20the%20community%20is%20starting%20to%0Arealize%20that%20for%20many%20applications%20it%20is%20not%20so%20much%20the%20exact%20prediction%20that%0Ais%20of%20importance%2C%20but%20rather%20the%20variability%20or%20uncertainty.%0A%20%20The%20work%20in%20this%20dissertation%20tries%20to%20further%20the%20quest%20for%20a%20world%20where%0Aeveryone%20is%20aware%20of%20uncertainty%2C%20of%20how%20important%20it%20is%20and%20how%20to%20embrace%20it%0Ainstead%20of%20fearing%20it.%20A%20specific%2C%20though%20general%2C%20framework%20that%20allows%20anyone%0Ato%20obtain%20accurate%20uncertainty%20estimates%20is%20singled%20out%20and%20analysed.%20Certain%0Aaspects%20and%20applications%20of%20the%20framework%20--%20dubbed%20%60conformal%20prediction%27%20--%0Aare%20studied%20in%20detail.%20Whereas%20many%20approaches%20to%20uncertainty%20quantification%0Amake%20strong%20assumptions%20about%20the%20data%2C%20conformal%20prediction%20is%2C%20at%20the%20time%20of%0Awriting%2C%20the%20only%20framework%20that%20deserves%20the%20title%20%60distribution-free%27.%20No%0Aparametric%20assumptions%20have%20to%20be%20made%20and%20the%20nonparametric%20results%20also%20hold%0Awithout%20having%20to%20resort%20to%20the%20law%20of%20large%20numbers%20in%20the%20asymptotic%20regime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02082v1&entry.124074799=Read"},
{"title": "Reconstructions of Jupiter's magnetic field using physics informed\n  neural networks", "author": "Philip W. Livermore and Leyuan Wu and Longwei Chen and Sjoerd A. L. de Ridder", "abstract": "  Magnetic sounding using data collected from the Juno mission can be used to\nprovide constraints on Jupiter's interior. However, inwards continuation of\nreconstructions assuming zero electrical conductivity and a representation in\nspherical harmonics are limited by the enhancement of noise at small scales.\nHere we describe new reconstructions of Jupiter's internal magnetic field based\non physics-informed neural networks and either the first 33 (PINN33) or the\nfirst 50 (PINN50) of Juno's orbits. The method can resolve local structures,\nand allows for weak ambient electrical currents. Our models are not hampered by\nnoise amplification at depth, and offer a much clearer picture of the interior\nstructure. We estimate that the dynamo boundary is at a fractional radius of\n0.8. At this depth, the magnetic field is arranged into longitudinal bands, and\nstrong local features such as the great blue spot appear to be rooted in\nneighbouring structures of oppositely signed flux.\n", "link": "http://arxiv.org/abs/2403.07507v2", "date": "2024-05-03", "relevancy": 1.5939, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.41}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3929}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructions%20of%20Jupiter%27s%20magnetic%20field%20using%20physics%20informed%0A%20%20neural%20networks&body=Title%3A%20Reconstructions%20of%20Jupiter%27s%20magnetic%20field%20using%20physics%20informed%0A%20%20neural%20networks%0AAuthor%3A%20Philip%20W.%20Livermore%20and%20Leyuan%20Wu%20and%20Longwei%20Chen%20and%20Sjoerd%20A.%20L.%20de%20Ridder%0AAbstract%3A%20%20%20Magnetic%20sounding%20using%20data%20collected%20from%20the%20Juno%20mission%20can%20be%20used%20to%0Aprovide%20constraints%20on%20Jupiter%27s%20interior.%20However%2C%20inwards%20continuation%20of%0Areconstructions%20assuming%20zero%20electrical%20conductivity%20and%20a%20representation%20in%0Aspherical%20harmonics%20are%20limited%20by%20the%20enhancement%20of%20noise%20at%20small%20scales.%0AHere%20we%20describe%20new%20reconstructions%20of%20Jupiter%27s%20internal%20magnetic%20field%20based%0Aon%20physics-informed%20neural%20networks%20and%20either%20the%20first%2033%20%28PINN33%29%20or%20the%0Afirst%2050%20%28PINN50%29%20of%20Juno%27s%20orbits.%20The%20method%20can%20resolve%20local%20structures%2C%0Aand%20allows%20for%20weak%20ambient%20electrical%20currents.%20Our%20models%20are%20not%20hampered%20by%0Anoise%20amplification%20at%20depth%2C%20and%20offer%20a%20much%20clearer%20picture%20of%20the%20interior%0Astructure.%20We%20estimate%20that%20the%20dynamo%20boundary%20is%20at%20a%20fractional%20radius%20of%0A0.8.%20At%20this%20depth%2C%20the%20magnetic%20field%20is%20arranged%20into%20longitudinal%20bands%2C%20and%0Astrong%20local%20features%20such%20as%20the%20great%20blue%20spot%20appear%20to%20be%20rooted%20in%0Aneighbouring%20structures%20of%20oppositely%20signed%20flux.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructions%2520of%2520Jupiter%2527s%2520magnetic%2520field%2520using%2520physics%2520informed%250A%2520%2520neural%2520networks%26entry.906535625%3DPhilip%2520W.%2520Livermore%2520and%2520Leyuan%2520Wu%2520and%2520Longwei%2520Chen%2520and%2520Sjoerd%2520A.%2520L.%2520de%2520Ridder%26entry.1292438233%3D%2520%2520Magnetic%2520sounding%2520using%2520data%2520collected%2520from%2520the%2520Juno%2520mission%2520can%2520be%2520used%2520to%250Aprovide%2520constraints%2520on%2520Jupiter%2527s%2520interior.%2520However%252C%2520inwards%2520continuation%2520of%250Areconstructions%2520assuming%2520zero%2520electrical%2520conductivity%2520and%2520a%2520representation%2520in%250Aspherical%2520harmonics%2520are%2520limited%2520by%2520the%2520enhancement%2520of%2520noise%2520at%2520small%2520scales.%250AHere%2520we%2520describe%2520new%2520reconstructions%2520of%2520Jupiter%2527s%2520internal%2520magnetic%2520field%2520based%250Aon%2520physics-informed%2520neural%2520networks%2520and%2520either%2520the%2520first%252033%2520%2528PINN33%2529%2520or%2520the%250Afirst%252050%2520%2528PINN50%2529%2520of%2520Juno%2527s%2520orbits.%2520The%2520method%2520can%2520resolve%2520local%2520structures%252C%250Aand%2520allows%2520for%2520weak%2520ambient%2520electrical%2520currents.%2520Our%2520models%2520are%2520not%2520hampered%2520by%250Anoise%2520amplification%2520at%2520depth%252C%2520and%2520offer%2520a%2520much%2520clearer%2520picture%2520of%2520the%2520interior%250Astructure.%2520We%2520estimate%2520that%2520the%2520dynamo%2520boundary%2520is%2520at%2520a%2520fractional%2520radius%2520of%250A0.8.%2520At%2520this%2520depth%252C%2520the%2520magnetic%2520field%2520is%2520arranged%2520into%2520longitudinal%2520bands%252C%2520and%250Astrong%2520local%2520features%2520such%2520as%2520the%2520great%2520blue%2520spot%2520appear%2520to%2520be%2520rooted%2520in%250Aneighbouring%2520structures%2520of%2520oppositely%2520signed%2520flux.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructions%20of%20Jupiter%27s%20magnetic%20field%20using%20physics%20informed%0A%20%20neural%20networks&entry.906535625=Philip%20W.%20Livermore%20and%20Leyuan%20Wu%20and%20Longwei%20Chen%20and%20Sjoerd%20A.%20L.%20de%20Ridder&entry.1292438233=%20%20Magnetic%20sounding%20using%20data%20collected%20from%20the%20Juno%20mission%20can%20be%20used%20to%0Aprovide%20constraints%20on%20Jupiter%27s%20interior.%20However%2C%20inwards%20continuation%20of%0Areconstructions%20assuming%20zero%20electrical%20conductivity%20and%20a%20representation%20in%0Aspherical%20harmonics%20are%20limited%20by%20the%20enhancement%20of%20noise%20at%20small%20scales.%0AHere%20we%20describe%20new%20reconstructions%20of%20Jupiter%27s%20internal%20magnetic%20field%20based%0Aon%20physics-informed%20neural%20networks%20and%20either%20the%20first%2033%20%28PINN33%29%20or%20the%0Afirst%2050%20%28PINN50%29%20of%20Juno%27s%20orbits.%20The%20method%20can%20resolve%20local%20structures%2C%0Aand%20allows%20for%20weak%20ambient%20electrical%20currents.%20Our%20models%20are%20not%20hampered%20by%0Anoise%20amplification%20at%20depth%2C%20and%20offer%20a%20much%20clearer%20picture%20of%20the%20interior%0Astructure.%20We%20estimate%20that%20the%20dynamo%20boundary%20is%20at%20a%20fractional%20radius%20of%0A0.8.%20At%20this%20depth%2C%20the%20magnetic%20field%20is%20arranged%20into%20longitudinal%20bands%2C%20and%0Astrong%20local%20features%20such%20as%20the%20great%20blue%20spot%20appear%20to%20be%20rooted%20in%0Aneighbouring%20structures%20of%20oppositely%20signed%20flux.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07507v2&entry.124074799=Read"},
{"title": "Geometric Fabrics: a Safe Guiding Medium for Policy Learning", "author": "Karl Van Wyk and Ankur Handa and Viktor Makoviychuk and Yijie Guo and Arthur Allshire and Nathan D. Ratliff", "abstract": "  Robotics policies are always subjected to complex, second order dynamics that\nentangle their actions with resulting states. In reinforcement learning (RL)\ncontexts, policies have the burden of deciphering these complicated\ninteractions over massive amounts of experience and complex reward functions to\nlearn how to accomplish tasks. Moreover, policies typically issue actions\ndirectly to controllers like Operational Space Control (OSC) or joint PD\ncontrol, which induces straightline motion towards these action targets in task\nor joint space. However, straightline motion in these spaces for the most part\ndo not capture the rich, nonlinear behavior our robots need to exhibit,\nshifting the burden of discovering these behaviors more completely to the\nagent. Unlike these simpler controllers, geometric fabrics capture a much\nricher and desirable set of behaviors via artificial, second order dynamics\ngrounded in nonlinear geometry. These artificial dynamics shift the\nuncontrolled dynamics of a robot via an appropriate control law to form\nbehavioral dynamics. Behavioral dynamics unlock a new action space and safe,\nguiding behavior over which RL policies are trained. Behavioral dynamics enable\nbang-bang-like RL policy actions that are still safe for real robots, simplify\nreward engineering, and help sequence real-world, high-performance policies. We\ndescribe the framework more generally and create a specific instantiation for\nthe problem of dexterous, in-hand reorientation of a cube by a highly actuated\nrobot hand.\n", "link": "http://arxiv.org/abs/2405.02250v1", "date": "2024-05-03", "relevancy": 1.5925, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5907}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5619}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Fabrics%3A%20a%20Safe%20Guiding%20Medium%20for%20Policy%20Learning&body=Title%3A%20Geometric%20Fabrics%3A%20a%20Safe%20Guiding%20Medium%20for%20Policy%20Learning%0AAuthor%3A%20Karl%20Van%20Wyk%20and%20Ankur%20Handa%20and%20Viktor%20Makoviychuk%20and%20Yijie%20Guo%20and%20Arthur%20Allshire%20and%20Nathan%20D.%20Ratliff%0AAbstract%3A%20%20%20Robotics%20policies%20are%20always%20subjected%20to%20complex%2C%20second%20order%20dynamics%20that%0Aentangle%20their%20actions%20with%20resulting%20states.%20In%20reinforcement%20learning%20%28RL%29%0Acontexts%2C%20policies%20have%20the%20burden%20of%20deciphering%20these%20complicated%0Ainteractions%20over%20massive%20amounts%20of%20experience%20and%20complex%20reward%20functions%20to%0Alearn%20how%20to%20accomplish%20tasks.%20Moreover%2C%20policies%20typically%20issue%20actions%0Adirectly%20to%20controllers%20like%20Operational%20Space%20Control%20%28OSC%29%20or%20joint%20PD%0Acontrol%2C%20which%20induces%20straightline%20motion%20towards%20these%20action%20targets%20in%20task%0Aor%20joint%20space.%20However%2C%20straightline%20motion%20in%20these%20spaces%20for%20the%20most%20part%0Ado%20not%20capture%20the%20rich%2C%20nonlinear%20behavior%20our%20robots%20need%20to%20exhibit%2C%0Ashifting%20the%20burden%20of%20discovering%20these%20behaviors%20more%20completely%20to%20the%0Aagent.%20Unlike%20these%20simpler%20controllers%2C%20geometric%20fabrics%20capture%20a%20much%0Aricher%20and%20desirable%20set%20of%20behaviors%20via%20artificial%2C%20second%20order%20dynamics%0Agrounded%20in%20nonlinear%20geometry.%20These%20artificial%20dynamics%20shift%20the%0Auncontrolled%20dynamics%20of%20a%20robot%20via%20an%20appropriate%20control%20law%20to%20form%0Abehavioral%20dynamics.%20Behavioral%20dynamics%20unlock%20a%20new%20action%20space%20and%20safe%2C%0Aguiding%20behavior%20over%20which%20RL%20policies%20are%20trained.%20Behavioral%20dynamics%20enable%0Abang-bang-like%20RL%20policy%20actions%20that%20are%20still%20safe%20for%20real%20robots%2C%20simplify%0Areward%20engineering%2C%20and%20help%20sequence%20real-world%2C%20high-performance%20policies.%20We%0Adescribe%20the%20framework%20more%20generally%20and%20create%20a%20specific%20instantiation%20for%0Athe%20problem%20of%20dexterous%2C%20in-hand%20reorientation%20of%20a%20cube%20by%20a%20highly%20actuated%0Arobot%20hand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Fabrics%253A%2520a%2520Safe%2520Guiding%2520Medium%2520for%2520Policy%2520Learning%26entry.906535625%3DKarl%2520Van%2520Wyk%2520and%2520Ankur%2520Handa%2520and%2520Viktor%2520Makoviychuk%2520and%2520Yijie%2520Guo%2520and%2520Arthur%2520Allshire%2520and%2520Nathan%2520D.%2520Ratliff%26entry.1292438233%3D%2520%2520Robotics%2520policies%2520are%2520always%2520subjected%2520to%2520complex%252C%2520second%2520order%2520dynamics%2520that%250Aentangle%2520their%2520actions%2520with%2520resulting%2520states.%2520In%2520reinforcement%2520learning%2520%2528RL%2529%250Acontexts%252C%2520policies%2520have%2520the%2520burden%2520of%2520deciphering%2520these%2520complicated%250Ainteractions%2520over%2520massive%2520amounts%2520of%2520experience%2520and%2520complex%2520reward%2520functions%2520to%250Alearn%2520how%2520to%2520accomplish%2520tasks.%2520Moreover%252C%2520policies%2520typically%2520issue%2520actions%250Adirectly%2520to%2520controllers%2520like%2520Operational%2520Space%2520Control%2520%2528OSC%2529%2520or%2520joint%2520PD%250Acontrol%252C%2520which%2520induces%2520straightline%2520motion%2520towards%2520these%2520action%2520targets%2520in%2520task%250Aor%2520joint%2520space.%2520However%252C%2520straightline%2520motion%2520in%2520these%2520spaces%2520for%2520the%2520most%2520part%250Ado%2520not%2520capture%2520the%2520rich%252C%2520nonlinear%2520behavior%2520our%2520robots%2520need%2520to%2520exhibit%252C%250Ashifting%2520the%2520burden%2520of%2520discovering%2520these%2520behaviors%2520more%2520completely%2520to%2520the%250Aagent.%2520Unlike%2520these%2520simpler%2520controllers%252C%2520geometric%2520fabrics%2520capture%2520a%2520much%250Aricher%2520and%2520desirable%2520set%2520of%2520behaviors%2520via%2520artificial%252C%2520second%2520order%2520dynamics%250Agrounded%2520in%2520nonlinear%2520geometry.%2520These%2520artificial%2520dynamics%2520shift%2520the%250Auncontrolled%2520dynamics%2520of%2520a%2520robot%2520via%2520an%2520appropriate%2520control%2520law%2520to%2520form%250Abehavioral%2520dynamics.%2520Behavioral%2520dynamics%2520unlock%2520a%2520new%2520action%2520space%2520and%2520safe%252C%250Aguiding%2520behavior%2520over%2520which%2520RL%2520policies%2520are%2520trained.%2520Behavioral%2520dynamics%2520enable%250Abang-bang-like%2520RL%2520policy%2520actions%2520that%2520are%2520still%2520safe%2520for%2520real%2520robots%252C%2520simplify%250Areward%2520engineering%252C%2520and%2520help%2520sequence%2520real-world%252C%2520high-performance%2520policies.%2520We%250Adescribe%2520the%2520framework%2520more%2520generally%2520and%2520create%2520a%2520specific%2520instantiation%2520for%250Athe%2520problem%2520of%2520dexterous%252C%2520in-hand%2520reorientation%2520of%2520a%2520cube%2520by%2520a%2520highly%2520actuated%250Arobot%2520hand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Fabrics%3A%20a%20Safe%20Guiding%20Medium%20for%20Policy%20Learning&entry.906535625=Karl%20Van%20Wyk%20and%20Ankur%20Handa%20and%20Viktor%20Makoviychuk%20and%20Yijie%20Guo%20and%20Arthur%20Allshire%20and%20Nathan%20D.%20Ratliff&entry.1292438233=%20%20Robotics%20policies%20are%20always%20subjected%20to%20complex%2C%20second%20order%20dynamics%20that%0Aentangle%20their%20actions%20with%20resulting%20states.%20In%20reinforcement%20learning%20%28RL%29%0Acontexts%2C%20policies%20have%20the%20burden%20of%20deciphering%20these%20complicated%0Ainteractions%20over%20massive%20amounts%20of%20experience%20and%20complex%20reward%20functions%20to%0Alearn%20how%20to%20accomplish%20tasks.%20Moreover%2C%20policies%20typically%20issue%20actions%0Adirectly%20to%20controllers%20like%20Operational%20Space%20Control%20%28OSC%29%20or%20joint%20PD%0Acontrol%2C%20which%20induces%20straightline%20motion%20towards%20these%20action%20targets%20in%20task%0Aor%20joint%20space.%20However%2C%20straightline%20motion%20in%20these%20spaces%20for%20the%20most%20part%0Ado%20not%20capture%20the%20rich%2C%20nonlinear%20behavior%20our%20robots%20need%20to%20exhibit%2C%0Ashifting%20the%20burden%20of%20discovering%20these%20behaviors%20more%20completely%20to%20the%0Aagent.%20Unlike%20these%20simpler%20controllers%2C%20geometric%20fabrics%20capture%20a%20much%0Aricher%20and%20desirable%20set%20of%20behaviors%20via%20artificial%2C%20second%20order%20dynamics%0Agrounded%20in%20nonlinear%20geometry.%20These%20artificial%20dynamics%20shift%20the%0Auncontrolled%20dynamics%20of%20a%20robot%20via%20an%20appropriate%20control%20law%20to%20form%0Abehavioral%20dynamics.%20Behavioral%20dynamics%20unlock%20a%20new%20action%20space%20and%20safe%2C%0Aguiding%20behavior%20over%20which%20RL%20policies%20are%20trained.%20Behavioral%20dynamics%20enable%0Abang-bang-like%20RL%20policy%20actions%20that%20are%20still%20safe%20for%20real%20robots%2C%20simplify%0Areward%20engineering%2C%20and%20help%20sequence%20real-world%2C%20high-performance%20policies.%20We%0Adescribe%20the%20framework%20more%20generally%20and%20create%20a%20specific%20instantiation%20for%0Athe%20problem%20of%20dexterous%2C%20in-hand%20reorientation%20of%20a%20cube%20by%20a%20highly%20actuated%0Arobot%20hand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02250v1&entry.124074799=Read"},
{"title": "Neural Context Flows for Learning Generalizable Dynamical Systems", "author": "Roussel Desmond Nzoyem and David A. W. Barton and Tom Deakin", "abstract": "  Neural Ordinary Differential Equations typically struggle to generalize to\nnew dynamical behaviors created by parameter changes in the underlying system,\neven when the dynamics are close to previously seen behaviors. The issue gets\nworse when the changing parameters are unobserved, i.e., their value or\ninfluence is not directly measurable when collecting data. We introduce Neural\nContext Flow (NCF), a framework that encodes said unobserved parameters in a\nlatent context vector as input to a vector field. NCFs leverage\ndifferentiability of the vector field with respect to the parameters, along\nwith first-order Taylor expansion to allow any context vector to influence\ntrajectories from other parameters. We validate our method and compare it to\nestablished Multi-Task and Meta-Learning alternatives, showing competitive\nperformance in mean squared error for in-domain and out-of-distribution\nevaluation on the Lotka-Volterra, Glycolytic Oscillator, and Gray-Scott\nproblems. This study holds practical implications for foundational models in\nscience and related areas that benefit from conditional neural ODEs. Our code\nis openly available at https://github.com/ddrous/ncflow.\n", "link": "http://arxiv.org/abs/2405.02154v1", "date": "2024-05-03", "relevancy": 1.586, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5366}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5334}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Context%20Flows%20for%20Learning%20Generalizable%20Dynamical%20Systems&body=Title%3A%20Neural%20Context%20Flows%20for%20Learning%20Generalizable%20Dynamical%20Systems%0AAuthor%3A%20Roussel%20Desmond%20Nzoyem%20and%20David%20A.%20W.%20Barton%20and%20Tom%20Deakin%0AAbstract%3A%20%20%20Neural%20Ordinary%20Differential%20Equations%20typically%20struggle%20to%20generalize%20to%0Anew%20dynamical%20behaviors%20created%20by%20parameter%20changes%20in%20the%20underlying%20system%2C%0Aeven%20when%20the%20dynamics%20are%20close%20to%20previously%20seen%20behaviors.%20The%20issue%20gets%0Aworse%20when%20the%20changing%20parameters%20are%20unobserved%2C%20i.e.%2C%20their%20value%20or%0Ainfluence%20is%20not%20directly%20measurable%20when%20collecting%20data.%20We%20introduce%20Neural%0AContext%20Flow%20%28NCF%29%2C%20a%20framework%20that%20encodes%20said%20unobserved%20parameters%20in%20a%0Alatent%20context%20vector%20as%20input%20to%20a%20vector%20field.%20NCFs%20leverage%0Adifferentiability%20of%20the%20vector%20field%20with%20respect%20to%20the%20parameters%2C%20along%0Awith%20first-order%20Taylor%20expansion%20to%20allow%20any%20context%20vector%20to%20influence%0Atrajectories%20from%20other%20parameters.%20We%20validate%20our%20method%20and%20compare%20it%20to%0Aestablished%20Multi-Task%20and%20Meta-Learning%20alternatives%2C%20showing%20competitive%0Aperformance%20in%20mean%20squared%20error%20for%20in-domain%20and%20out-of-distribution%0Aevaluation%20on%20the%20Lotka-Volterra%2C%20Glycolytic%20Oscillator%2C%20and%20Gray-Scott%0Aproblems.%20This%20study%20holds%20practical%20implications%20for%20foundational%20models%20in%0Ascience%20and%20related%20areas%20that%20benefit%20from%20conditional%20neural%20ODEs.%20Our%20code%0Ais%20openly%20available%20at%20https%3A//github.com/ddrous/ncflow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Context%2520Flows%2520for%2520Learning%2520Generalizable%2520Dynamical%2520Systems%26entry.906535625%3DRoussel%2520Desmond%2520Nzoyem%2520and%2520David%2520A.%2520W.%2520Barton%2520and%2520Tom%2520Deakin%26entry.1292438233%3D%2520%2520Neural%2520Ordinary%2520Differential%2520Equations%2520typically%2520struggle%2520to%2520generalize%2520to%250Anew%2520dynamical%2520behaviors%2520created%2520by%2520parameter%2520changes%2520in%2520the%2520underlying%2520system%252C%250Aeven%2520when%2520the%2520dynamics%2520are%2520close%2520to%2520previously%2520seen%2520behaviors.%2520The%2520issue%2520gets%250Aworse%2520when%2520the%2520changing%2520parameters%2520are%2520unobserved%252C%2520i.e.%252C%2520their%2520value%2520or%250Ainfluence%2520is%2520not%2520directly%2520measurable%2520when%2520collecting%2520data.%2520We%2520introduce%2520Neural%250AContext%2520Flow%2520%2528NCF%2529%252C%2520a%2520framework%2520that%2520encodes%2520said%2520unobserved%2520parameters%2520in%2520a%250Alatent%2520context%2520vector%2520as%2520input%2520to%2520a%2520vector%2520field.%2520NCFs%2520leverage%250Adifferentiability%2520of%2520the%2520vector%2520field%2520with%2520respect%2520to%2520the%2520parameters%252C%2520along%250Awith%2520first-order%2520Taylor%2520expansion%2520to%2520allow%2520any%2520context%2520vector%2520to%2520influence%250Atrajectories%2520from%2520other%2520parameters.%2520We%2520validate%2520our%2520method%2520and%2520compare%2520it%2520to%250Aestablished%2520Multi-Task%2520and%2520Meta-Learning%2520alternatives%252C%2520showing%2520competitive%250Aperformance%2520in%2520mean%2520squared%2520error%2520for%2520in-domain%2520and%2520out-of-distribution%250Aevaluation%2520on%2520the%2520Lotka-Volterra%252C%2520Glycolytic%2520Oscillator%252C%2520and%2520Gray-Scott%250Aproblems.%2520This%2520study%2520holds%2520practical%2520implications%2520for%2520foundational%2520models%2520in%250Ascience%2520and%2520related%2520areas%2520that%2520benefit%2520from%2520conditional%2520neural%2520ODEs.%2520Our%2520code%250Ais%2520openly%2520available%2520at%2520https%253A//github.com/ddrous/ncflow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Context%20Flows%20for%20Learning%20Generalizable%20Dynamical%20Systems&entry.906535625=Roussel%20Desmond%20Nzoyem%20and%20David%20A.%20W.%20Barton%20and%20Tom%20Deakin&entry.1292438233=%20%20Neural%20Ordinary%20Differential%20Equations%20typically%20struggle%20to%20generalize%20to%0Anew%20dynamical%20behaviors%20created%20by%20parameter%20changes%20in%20the%20underlying%20system%2C%0Aeven%20when%20the%20dynamics%20are%20close%20to%20previously%20seen%20behaviors.%20The%20issue%20gets%0Aworse%20when%20the%20changing%20parameters%20are%20unobserved%2C%20i.e.%2C%20their%20value%20or%0Ainfluence%20is%20not%20directly%20measurable%20when%20collecting%20data.%20We%20introduce%20Neural%0AContext%20Flow%20%28NCF%29%2C%20a%20framework%20that%20encodes%20said%20unobserved%20parameters%20in%20a%0Alatent%20context%20vector%20as%20input%20to%20a%20vector%20field.%20NCFs%20leverage%0Adifferentiability%20of%20the%20vector%20field%20with%20respect%20to%20the%20parameters%2C%20along%0Awith%20first-order%20Taylor%20expansion%20to%20allow%20any%20context%20vector%20to%20influence%0Atrajectories%20from%20other%20parameters.%20We%20validate%20our%20method%20and%20compare%20it%20to%0Aestablished%20Multi-Task%20and%20Meta-Learning%20alternatives%2C%20showing%20competitive%0Aperformance%20in%20mean%20squared%20error%20for%20in-domain%20and%20out-of-distribution%0Aevaluation%20on%20the%20Lotka-Volterra%2C%20Glycolytic%20Oscillator%2C%20and%20Gray-Scott%0Aproblems.%20This%20study%20holds%20practical%20implications%20for%20foundational%20models%20in%0Ascience%20and%20related%20areas%20that%20benefit%20from%20conditional%20neural%20ODEs.%20Our%20code%0Ais%20openly%20available%20at%20https%3A//github.com/ddrous/ncflow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02154v1&entry.124074799=Read"},
{"title": "Quantifying Distribution Shifts and Uncertainties for Enhanced Model\n  Robustness in Machine Learning Applications", "author": "Vegard Flovik", "abstract": "  Distribution shifts, where statistical properties differ between training and\ntest datasets, present a significant challenge in real-world machine learning\napplications where they directly impact model generalization and robustness. In\nthis study, we explore model adaptation and generalization by utilizing\nsynthetic data to systematically address distributional disparities. Our\ninvestigation aims to identify the prerequisites for successful model\nadaptation across diverse data distributions, while quantifying the associated\nuncertainties. Specifically, we generate synthetic data using the Van der Waals\nequation for gases and employ quantitative measures such as Kullback-Leibler\ndivergence, Jensen-Shannon distance, and Mahalanobis distance to assess data\nsimilarity. These metrics en able us to evaluate both model accuracy and\nquantify the associated uncertainty in predictions arising from data\ndistribution shifts. Our findings suggest that utilizing statistical measures,\nsuch as the Mahalanobis distance, to determine whether model predictions fall\nwithin the low-error \"interpolation regime\" or the high-error \"extrapolation\nregime\" provides a complementary method for assessing distribution shift and\nmodel uncertainty. These insights hold significant value for enhancing model\nrobustness and generalization, essential for the successful deployment of\nmachine learning applications in real-world scenarios.\n", "link": "http://arxiv.org/abs/2405.01978v1", "date": "2024-05-03", "relevancy": 1.5415, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5146}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Distribution%20Shifts%20and%20Uncertainties%20for%20Enhanced%20Model%0A%20%20Robustness%20in%20Machine%20Learning%20Applications&body=Title%3A%20Quantifying%20Distribution%20Shifts%20and%20Uncertainties%20for%20Enhanced%20Model%0A%20%20Robustness%20in%20Machine%20Learning%20Applications%0AAuthor%3A%20Vegard%20Flovik%0AAbstract%3A%20%20%20Distribution%20shifts%2C%20where%20statistical%20properties%20differ%20between%20training%20and%0Atest%20datasets%2C%20present%20a%20significant%20challenge%20in%20real-world%20machine%20learning%0Aapplications%20where%20they%20directly%20impact%20model%20generalization%20and%20robustness.%20In%0Athis%20study%2C%20we%20explore%20model%20adaptation%20and%20generalization%20by%20utilizing%0Asynthetic%20data%20to%20systematically%20address%20distributional%20disparities.%20Our%0Ainvestigation%20aims%20to%20identify%20the%20prerequisites%20for%20successful%20model%0Aadaptation%20across%20diverse%20data%20distributions%2C%20while%20quantifying%20the%20associated%0Auncertainties.%20Specifically%2C%20we%20generate%20synthetic%20data%20using%20the%20Van%20der%20Waals%0Aequation%20for%20gases%20and%20employ%20quantitative%20measures%20such%20as%20Kullback-Leibler%0Adivergence%2C%20Jensen-Shannon%20distance%2C%20and%20Mahalanobis%20distance%20to%20assess%20data%0Asimilarity.%20These%20metrics%20en%20able%20us%20to%20evaluate%20both%20model%20accuracy%20and%0Aquantify%20the%20associated%20uncertainty%20in%20predictions%20arising%20from%20data%0Adistribution%20shifts.%20Our%20findings%20suggest%20that%20utilizing%20statistical%20measures%2C%0Asuch%20as%20the%20Mahalanobis%20distance%2C%20to%20determine%20whether%20model%20predictions%20fall%0Awithin%20the%20low-error%20%22interpolation%20regime%22%20or%20the%20high-error%20%22extrapolation%0Aregime%22%20provides%20a%20complementary%20method%20for%20assessing%20distribution%20shift%20and%0Amodel%20uncertainty.%20These%20insights%20hold%20significant%20value%20for%20enhancing%20model%0Arobustness%20and%20generalization%2C%20essential%20for%20the%20successful%20deployment%20of%0Amachine%20learning%20applications%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Distribution%2520Shifts%2520and%2520Uncertainties%2520for%2520Enhanced%2520Model%250A%2520%2520Robustness%2520in%2520Machine%2520Learning%2520Applications%26entry.906535625%3DVegard%2520Flovik%26entry.1292438233%3D%2520%2520Distribution%2520shifts%252C%2520where%2520statistical%2520properties%2520differ%2520between%2520training%2520and%250Atest%2520datasets%252C%2520present%2520a%2520significant%2520challenge%2520in%2520real-world%2520machine%2520learning%250Aapplications%2520where%2520they%2520directly%2520impact%2520model%2520generalization%2520and%2520robustness.%2520In%250Athis%2520study%252C%2520we%2520explore%2520model%2520adaptation%2520and%2520generalization%2520by%2520utilizing%250Asynthetic%2520data%2520to%2520systematically%2520address%2520distributional%2520disparities.%2520Our%250Ainvestigation%2520aims%2520to%2520identify%2520the%2520prerequisites%2520for%2520successful%2520model%250Aadaptation%2520across%2520diverse%2520data%2520distributions%252C%2520while%2520quantifying%2520the%2520associated%250Auncertainties.%2520Specifically%252C%2520we%2520generate%2520synthetic%2520data%2520using%2520the%2520Van%2520der%2520Waals%250Aequation%2520for%2520gases%2520and%2520employ%2520quantitative%2520measures%2520such%2520as%2520Kullback-Leibler%250Adivergence%252C%2520Jensen-Shannon%2520distance%252C%2520and%2520Mahalanobis%2520distance%2520to%2520assess%2520data%250Asimilarity.%2520These%2520metrics%2520en%2520able%2520us%2520to%2520evaluate%2520both%2520model%2520accuracy%2520and%250Aquantify%2520the%2520associated%2520uncertainty%2520in%2520predictions%2520arising%2520from%2520data%250Adistribution%2520shifts.%2520Our%2520findings%2520suggest%2520that%2520utilizing%2520statistical%2520measures%252C%250Asuch%2520as%2520the%2520Mahalanobis%2520distance%252C%2520to%2520determine%2520whether%2520model%2520predictions%2520fall%250Awithin%2520the%2520low-error%2520%2522interpolation%2520regime%2522%2520or%2520the%2520high-error%2520%2522extrapolation%250Aregime%2522%2520provides%2520a%2520complementary%2520method%2520for%2520assessing%2520distribution%2520shift%2520and%250Amodel%2520uncertainty.%2520These%2520insights%2520hold%2520significant%2520value%2520for%2520enhancing%2520model%250Arobustness%2520and%2520generalization%252C%2520essential%2520for%2520the%2520successful%2520deployment%2520of%250Amachine%2520learning%2520applications%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Distribution%20Shifts%20and%20Uncertainties%20for%20Enhanced%20Model%0A%20%20Robustness%20in%20Machine%20Learning%20Applications&entry.906535625=Vegard%20Flovik&entry.1292438233=%20%20Distribution%20shifts%2C%20where%20statistical%20properties%20differ%20between%20training%20and%0Atest%20datasets%2C%20present%20a%20significant%20challenge%20in%20real-world%20machine%20learning%0Aapplications%20where%20they%20directly%20impact%20model%20generalization%20and%20robustness.%20In%0Athis%20study%2C%20we%20explore%20model%20adaptation%20and%20generalization%20by%20utilizing%0Asynthetic%20data%20to%20systematically%20address%20distributional%20disparities.%20Our%0Ainvestigation%20aims%20to%20identify%20the%20prerequisites%20for%20successful%20model%0Aadaptation%20across%20diverse%20data%20distributions%2C%20while%20quantifying%20the%20associated%0Auncertainties.%20Specifically%2C%20we%20generate%20synthetic%20data%20using%20the%20Van%20der%20Waals%0Aequation%20for%20gases%20and%20employ%20quantitative%20measures%20such%20as%20Kullback-Leibler%0Adivergence%2C%20Jensen-Shannon%20distance%2C%20and%20Mahalanobis%20distance%20to%20assess%20data%0Asimilarity.%20These%20metrics%20en%20able%20us%20to%20evaluate%20both%20model%20accuracy%20and%0Aquantify%20the%20associated%20uncertainty%20in%20predictions%20arising%20from%20data%0Adistribution%20shifts.%20Our%20findings%20suggest%20that%20utilizing%20statistical%20measures%2C%0Asuch%20as%20the%20Mahalanobis%20distance%2C%20to%20determine%20whether%20model%20predictions%20fall%0Awithin%20the%20low-error%20%22interpolation%20regime%22%20or%20the%20high-error%20%22extrapolation%0Aregime%22%20provides%20a%20complementary%20method%20for%20assessing%20distribution%20shift%20and%0Amodel%20uncertainty.%20These%20insights%20hold%20significant%20value%20for%20enhancing%20model%0Arobustness%20and%20generalization%2C%20essential%20for%20the%20successful%20deployment%20of%0Amachine%20learning%20applications%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01978v1&entry.124074799=Read"},
{"title": "From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?", "author": "Guangming Huang and Yingya Li and Shoaib Jameel and Yunfei Long and Giorgos Papanastasiou", "abstract": "  Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.\n", "link": "http://arxiv.org/abs/2403.11894v2", "date": "2024-05-03", "relevancy": 0.9037, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4915}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.44}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Explainable%20to%20Interpretable%20Deep%20Learning%20for%20Natural%20Language%0A%20%20Processing%20in%20Healthcare%3A%20How%20Far%20from%20Reality%3F&body=Title%3A%20From%20Explainable%20to%20Interpretable%20Deep%20Learning%20for%20Natural%20Language%0A%20%20Processing%20in%20Healthcare%3A%20How%20Far%20from%20Reality%3F%0AAuthor%3A%20Guangming%20Huang%20and%20Yingya%20Li%20and%20Shoaib%20Jameel%20and%20Yunfei%20Long%20and%20Giorgos%20Papanastasiou%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20has%20substantially%20enhanced%20natural%20language%20processing%0A%28NLP%29%20in%20healthcare%20research.%20However%2C%20the%20increasing%20complexity%20of%20DL-based%0ANLP%20necessitates%20transparent%20model%20interpretability%2C%20or%20at%20least%0Aexplainability%2C%20for%20reliable%20decision-making.%20This%20work%20presents%20a%20thorough%0Ascoping%20review%20of%20explainable%20and%20interpretable%20DL%20in%20healthcare%20NLP.%20The%20term%0A%22eXplainable%20and%20Interpretable%20Artificial%20Intelligence%22%20%28XIAI%29%20is%20introduced%20to%0Adistinguish%20XAI%20from%20IAI.%20Different%20models%20are%20further%20categorized%20based%20on%0Atheir%20functionality%20%28model-%2C%20input-%2C%20output-based%29%20and%20scope%20%28local%2C%20global%29.%0AOur%20analysis%20shows%20that%20attention%20mechanisms%20are%20the%20most%20prevalent%20emerging%0AIAI%20technique.%20The%20use%20of%20IAI%20is%20growing%2C%20distinguishing%20it%20from%20XAI.%20The%20major%0Achallenges%20identified%20are%20that%20most%20XIAI%20does%20not%20explore%20%22global%22%20modelling%0Aprocesses%2C%20the%20lack%20of%20best%20practices%2C%20and%20the%20lack%20of%20systematic%20evaluation%0Aand%20benchmarks.%20One%20important%20opportunity%20is%20to%20use%20attention%20mechanisms%20to%0Aenhance%20multi-modal%20XIAI%20for%20personalized%20medicine.%20Additionally%2C%20combining%20DL%0Awith%20causal%20logic%20holds%20promise.%20Our%20discussion%20encourages%20the%20integration%20of%0AXIAI%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20domain-specific%20smaller%20models.%20In%0Aconclusion%2C%20XIAI%20adoption%20in%20healthcare%20requires%20dedicated%20in-house%20expertise.%0ACollaboration%20with%20domain%20experts%2C%20end-users%2C%20and%20policymakers%20can%20lead%20to%0Aready-to-use%20XIAI%20methods%20across%20NLP%20and%20medical%20tasks.%20While%20challenges%20exist%2C%0AXIAI%20techniques%20offer%20a%20valuable%20foundation%20for%20interpretable%20NLP%20algorithms%20in%0Ahealthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Explainable%2520to%2520Interpretable%2520Deep%2520Learning%2520for%2520Natural%2520Language%250A%2520%2520Processing%2520in%2520Healthcare%253A%2520How%2520Far%2520from%2520Reality%253F%26entry.906535625%3DGuangming%2520Huang%2520and%2520Yingya%2520Li%2520and%2520Shoaib%2520Jameel%2520and%2520Yunfei%2520Long%2520and%2520Giorgos%2520Papanastasiou%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520has%2520substantially%2520enhanced%2520natural%2520language%2520processing%250A%2528NLP%2529%2520in%2520healthcare%2520research.%2520However%252C%2520the%2520increasing%2520complexity%2520of%2520DL-based%250ANLP%2520necessitates%2520transparent%2520model%2520interpretability%252C%2520or%2520at%2520least%250Aexplainability%252C%2520for%2520reliable%2520decision-making.%2520This%2520work%2520presents%2520a%2520thorough%250Ascoping%2520review%2520of%2520explainable%2520and%2520interpretable%2520DL%2520in%2520healthcare%2520NLP.%2520The%2520term%250A%2522eXplainable%2520and%2520Interpretable%2520Artificial%2520Intelligence%2522%2520%2528XIAI%2529%2520is%2520introduced%2520to%250Adistinguish%2520XAI%2520from%2520IAI.%2520Different%2520models%2520are%2520further%2520categorized%2520based%2520on%250Atheir%2520functionality%2520%2528model-%252C%2520input-%252C%2520output-based%2529%2520and%2520scope%2520%2528local%252C%2520global%2529.%250AOur%2520analysis%2520shows%2520that%2520attention%2520mechanisms%2520are%2520the%2520most%2520prevalent%2520emerging%250AIAI%2520technique.%2520The%2520use%2520of%2520IAI%2520is%2520growing%252C%2520distinguishing%2520it%2520from%2520XAI.%2520The%2520major%250Achallenges%2520identified%2520are%2520that%2520most%2520XIAI%2520does%2520not%2520explore%2520%2522global%2522%2520modelling%250Aprocesses%252C%2520the%2520lack%2520of%2520best%2520practices%252C%2520and%2520the%2520lack%2520of%2520systematic%2520evaluation%250Aand%2520benchmarks.%2520One%2520important%2520opportunity%2520is%2520to%2520use%2520attention%2520mechanisms%2520to%250Aenhance%2520multi-modal%2520XIAI%2520for%2520personalized%2520medicine.%2520Additionally%252C%2520combining%2520DL%250Awith%2520causal%2520logic%2520holds%2520promise.%2520Our%2520discussion%2520encourages%2520the%2520integration%2520of%250AXIAI%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520domain-specific%2520smaller%2520models.%2520In%250Aconclusion%252C%2520XIAI%2520adoption%2520in%2520healthcare%2520requires%2520dedicated%2520in-house%2520expertise.%250ACollaboration%2520with%2520domain%2520experts%252C%2520end-users%252C%2520and%2520policymakers%2520can%2520lead%2520to%250Aready-to-use%2520XIAI%2520methods%2520across%2520NLP%2520and%2520medical%2520tasks.%2520While%2520challenges%2520exist%252C%250AXIAI%2520techniques%2520offer%2520a%2520valuable%2520foundation%2520for%2520interpretable%2520NLP%2520algorithms%2520in%250Ahealthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Explainable%20to%20Interpretable%20Deep%20Learning%20for%20Natural%20Language%0A%20%20Processing%20in%20Healthcare%3A%20How%20Far%20from%20Reality%3F&entry.906535625=Guangming%20Huang%20and%20Yingya%20Li%20and%20Shoaib%20Jameel%20and%20Yunfei%20Long%20and%20Giorgos%20Papanastasiou&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20has%20substantially%20enhanced%20natural%20language%20processing%0A%28NLP%29%20in%20healthcare%20research.%20However%2C%20the%20increasing%20complexity%20of%20DL-based%0ANLP%20necessitates%20transparent%20model%20interpretability%2C%20or%20at%20least%0Aexplainability%2C%20for%20reliable%20decision-making.%20This%20work%20presents%20a%20thorough%0Ascoping%20review%20of%20explainable%20and%20interpretable%20DL%20in%20healthcare%20NLP.%20The%20term%0A%22eXplainable%20and%20Interpretable%20Artificial%20Intelligence%22%20%28XIAI%29%20is%20introduced%20to%0Adistinguish%20XAI%20from%20IAI.%20Different%20models%20are%20further%20categorized%20based%20on%0Atheir%20functionality%20%28model-%2C%20input-%2C%20output-based%29%20and%20scope%20%28local%2C%20global%29.%0AOur%20analysis%20shows%20that%20attention%20mechanisms%20are%20the%20most%20prevalent%20emerging%0AIAI%20technique.%20The%20use%20of%20IAI%20is%20growing%2C%20distinguishing%20it%20from%20XAI.%20The%20major%0Achallenges%20identified%20are%20that%20most%20XIAI%20does%20not%20explore%20%22global%22%20modelling%0Aprocesses%2C%20the%20lack%20of%20best%20practices%2C%20and%20the%20lack%20of%20systematic%20evaluation%0Aand%20benchmarks.%20One%20important%20opportunity%20is%20to%20use%20attention%20mechanisms%20to%0Aenhance%20multi-modal%20XIAI%20for%20personalized%20medicine.%20Additionally%2C%20combining%20DL%0Awith%20causal%20logic%20holds%20promise.%20Our%20discussion%20encourages%20the%20integration%20of%0AXIAI%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20domain-specific%20smaller%20models.%20In%0Aconclusion%2C%20XIAI%20adoption%20in%20healthcare%20requires%20dedicated%20in-house%20expertise.%0ACollaboration%20with%20domain%20experts%2C%20end-users%2C%20and%20policymakers%20can%20lead%20to%0Aready-to-use%20XIAI%20methods%20across%20NLP%20and%20medical%20tasks.%20While%20challenges%20exist%2C%0AXIAI%20techniques%20offer%20a%20valuable%20foundation%20for%20interpretable%20NLP%20algorithms%20in%0Ahealthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11894v2&entry.124074799=Read"},
{"title": "Process Mining Embeddings: Learning Vector Representations for Petri\n  Nets", "author": "Juan G. Colonna and Ahmed A. Fares and M\u00e1rcio Duarte and Ricardo Sousa", "abstract": "  Process mining offers powerful techniques for discovering, analyzing, and\nenhancing real-world business processes. In this context, Petri nets provide an\nexpressive means of modeling process behavior. However, directly analyzing and\ncomparing intricate Petri net presents challenges. This study introduces\nPetriNet2Vec, a novel unsupervised methodology based on Natural Language\nProcessing concepts inspired by Doc2Vec and designed to facilitate the\neffective comparison, clustering, and classification of process models\nrepresented as embedding vectors. These embedding vectors allow us to quantify\nsimilarities and relationships between different process models. Our\nmethodology was experimentally validated using the PDC Dataset, featuring 96\ndiverse Petri net models. We performed cluster analysis, created UMAP\nvisualizations, and trained a decision tree to provide compelling evidence for\nthe capability of PetriNet2Vec to discern meaningful patterns and relationships\namong process models and their constituent tasks. Through a series of\nexperiments, we demonstrated that PetriNet2Vec was capable of learning the\nstructure of Petri nets, as well as the main properties used to simulate the\nprocess models of our dataset. Furthermore, our results showcase the utility of\nthe learned embeddings in two crucial downstream tasks within process mining\nenhancement: process classification and process retrieval.\n", "link": "http://arxiv.org/abs/2404.17129v2", "date": "2024-05-03", "relevancy": 1.3906, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4721}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4664}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Process%20Mining%20Embeddings%3A%20Learning%20Vector%20Representations%20for%20Petri%0A%20%20Nets&body=Title%3A%20Process%20Mining%20Embeddings%3A%20Learning%20Vector%20Representations%20for%20Petri%0A%20%20Nets%0AAuthor%3A%20Juan%20G.%20Colonna%20and%20Ahmed%20A.%20Fares%20and%20M%C3%A1rcio%20Duarte%20and%20Ricardo%20Sousa%0AAbstract%3A%20%20%20Process%20mining%20offers%20powerful%20techniques%20for%20discovering%2C%20analyzing%2C%20and%0Aenhancing%20real-world%20business%20processes.%20In%20this%20context%2C%20Petri%20nets%20provide%20an%0Aexpressive%20means%20of%20modeling%20process%20behavior.%20However%2C%20directly%20analyzing%20and%0Acomparing%20intricate%20Petri%20net%20presents%20challenges.%20This%20study%20introduces%0APetriNet2Vec%2C%20a%20novel%20unsupervised%20methodology%20based%20on%20Natural%20Language%0AProcessing%20concepts%20inspired%20by%20Doc2Vec%20and%20designed%20to%20facilitate%20the%0Aeffective%20comparison%2C%20clustering%2C%20and%20classification%20of%20process%20models%0Arepresented%20as%20embedding%20vectors.%20These%20embedding%20vectors%20allow%20us%20to%20quantify%0Asimilarities%20and%20relationships%20between%20different%20process%20models.%20Our%0Amethodology%20was%20experimentally%20validated%20using%20the%20PDC%20Dataset%2C%20featuring%2096%0Adiverse%20Petri%20net%20models.%20We%20performed%20cluster%20analysis%2C%20created%20UMAP%0Avisualizations%2C%20and%20trained%20a%20decision%20tree%20to%20provide%20compelling%20evidence%20for%0Athe%20capability%20of%20PetriNet2Vec%20to%20discern%20meaningful%20patterns%20and%20relationships%0Aamong%20process%20models%20and%20their%20constituent%20tasks.%20Through%20a%20series%20of%0Aexperiments%2C%20we%20demonstrated%20that%20PetriNet2Vec%20was%20capable%20of%20learning%20the%0Astructure%20of%20Petri%20nets%2C%20as%20well%20as%20the%20main%20properties%20used%20to%20simulate%20the%0Aprocess%20models%20of%20our%20dataset.%20Furthermore%2C%20our%20results%20showcase%20the%20utility%20of%0Athe%20learned%20embeddings%20in%20two%20crucial%20downstream%20tasks%20within%20process%20mining%0Aenhancement%3A%20process%20classification%20and%20process%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcess%2520Mining%2520Embeddings%253A%2520Learning%2520Vector%2520Representations%2520for%2520Petri%250A%2520%2520Nets%26entry.906535625%3DJuan%2520G.%2520Colonna%2520and%2520Ahmed%2520A.%2520Fares%2520and%2520M%25C3%25A1rcio%2520Duarte%2520and%2520Ricardo%2520Sousa%26entry.1292438233%3D%2520%2520Process%2520mining%2520offers%2520powerful%2520techniques%2520for%2520discovering%252C%2520analyzing%252C%2520and%250Aenhancing%2520real-world%2520business%2520processes.%2520In%2520this%2520context%252C%2520Petri%2520nets%2520provide%2520an%250Aexpressive%2520means%2520of%2520modeling%2520process%2520behavior.%2520However%252C%2520directly%2520analyzing%2520and%250Acomparing%2520intricate%2520Petri%2520net%2520presents%2520challenges.%2520This%2520study%2520introduces%250APetriNet2Vec%252C%2520a%2520novel%2520unsupervised%2520methodology%2520based%2520on%2520Natural%2520Language%250AProcessing%2520concepts%2520inspired%2520by%2520Doc2Vec%2520and%2520designed%2520to%2520facilitate%2520the%250Aeffective%2520comparison%252C%2520clustering%252C%2520and%2520classification%2520of%2520process%2520models%250Arepresented%2520as%2520embedding%2520vectors.%2520These%2520embedding%2520vectors%2520allow%2520us%2520to%2520quantify%250Asimilarities%2520and%2520relationships%2520between%2520different%2520process%2520models.%2520Our%250Amethodology%2520was%2520experimentally%2520validated%2520using%2520the%2520PDC%2520Dataset%252C%2520featuring%252096%250Adiverse%2520Petri%2520net%2520models.%2520We%2520performed%2520cluster%2520analysis%252C%2520created%2520UMAP%250Avisualizations%252C%2520and%2520trained%2520a%2520decision%2520tree%2520to%2520provide%2520compelling%2520evidence%2520for%250Athe%2520capability%2520of%2520PetriNet2Vec%2520to%2520discern%2520meaningful%2520patterns%2520and%2520relationships%250Aamong%2520process%2520models%2520and%2520their%2520constituent%2520tasks.%2520Through%2520a%2520series%2520of%250Aexperiments%252C%2520we%2520demonstrated%2520that%2520PetriNet2Vec%2520was%2520capable%2520of%2520learning%2520the%250Astructure%2520of%2520Petri%2520nets%252C%2520as%2520well%2520as%2520the%2520main%2520properties%2520used%2520to%2520simulate%2520the%250Aprocess%2520models%2520of%2520our%2520dataset.%2520Furthermore%252C%2520our%2520results%2520showcase%2520the%2520utility%2520of%250Athe%2520learned%2520embeddings%2520in%2520two%2520crucial%2520downstream%2520tasks%2520within%2520process%2520mining%250Aenhancement%253A%2520process%2520classification%2520and%2520process%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Process%20Mining%20Embeddings%3A%20Learning%20Vector%20Representations%20for%20Petri%0A%20%20Nets&entry.906535625=Juan%20G.%20Colonna%20and%20Ahmed%20A.%20Fares%20and%20M%C3%A1rcio%20Duarte%20and%20Ricardo%20Sousa&entry.1292438233=%20%20Process%20mining%20offers%20powerful%20techniques%20for%20discovering%2C%20analyzing%2C%20and%0Aenhancing%20real-world%20business%20processes.%20In%20this%20context%2C%20Petri%20nets%20provide%20an%0Aexpressive%20means%20of%20modeling%20process%20behavior.%20However%2C%20directly%20analyzing%20and%0Acomparing%20intricate%20Petri%20net%20presents%20challenges.%20This%20study%20introduces%0APetriNet2Vec%2C%20a%20novel%20unsupervised%20methodology%20based%20on%20Natural%20Language%0AProcessing%20concepts%20inspired%20by%20Doc2Vec%20and%20designed%20to%20facilitate%20the%0Aeffective%20comparison%2C%20clustering%2C%20and%20classification%20of%20process%20models%0Arepresented%20as%20embedding%20vectors.%20These%20embedding%20vectors%20allow%20us%20to%20quantify%0Asimilarities%20and%20relationships%20between%20different%20process%20models.%20Our%0Amethodology%20was%20experimentally%20validated%20using%20the%20PDC%20Dataset%2C%20featuring%2096%0Adiverse%20Petri%20net%20models.%20We%20performed%20cluster%20analysis%2C%20created%20UMAP%0Avisualizations%2C%20and%20trained%20a%20decision%20tree%20to%20provide%20compelling%20evidence%20for%0Athe%20capability%20of%20PetriNet2Vec%20to%20discern%20meaningful%20patterns%20and%20relationships%0Aamong%20process%20models%20and%20their%20constituent%20tasks.%20Through%20a%20series%20of%0Aexperiments%2C%20we%20demonstrated%20that%20PetriNet2Vec%20was%20capable%20of%20learning%20the%0Astructure%20of%20Petri%20nets%2C%20as%20well%20as%20the%20main%20properties%20used%20to%20simulate%20the%0Aprocess%20models%20of%20our%20dataset.%20Furthermore%2C%20our%20results%20showcase%20the%20utility%20of%0Athe%20learned%20embeddings%20in%20two%20crucial%20downstream%20tasks%20within%20process%20mining%0Aenhancement%3A%20process%20classification%20and%20process%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17129v2&entry.124074799=Read"},
{"title": "Simulating the economic impact of rationality through reinforcement\n  learning and agent-based modelling", "author": "Simone Brusatin and Tommaso Padoan and Andrea Coletta and Domenico Delli Gatti and Aldo Glielmo", "abstract": "  Agent-based models (ABMs) are simulation models used in economics to overcome\nsome of the limitations of traditional frameworks based on general equilibrium\nassumptions. However, agents within an ABM follow predetermined, not fully\nrational, behavioural rules which can be cumbersome to design and difficult to\njustify. Here we leverage multi-agent reinforcement learning (RL) to expand the\ncapabilities of ABMs with the introduction of fully rational agents that learn\ntheir policy by interacting with the environment and maximising a reward\nfunction. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by\nextending a paradigmatic macro ABM from the economic literature. We show that\ngradually substituting ABM firms in the model with RL agents, trained to\nmaximise profits, allows for a thorough study of the impact of rationality on\nthe economy. We find that RL agents spontaneously learn three distinct\nstrategies for maximising profits, with the optimal strategy depending on the\nlevel of market competition and rationality. We also find that RL agents with\nindependent policies, and without the ability to communicate with each other,\nspontaneously learn to segregate into different strategic groups, thus\nincreasing market power and overall profits. Finally, we find that a higher\ndegree of rationality in the economy always improves the macroeconomic\nenvironment as measured by total output, depending on the specific rational\npolicy, this can come at the cost of higher instability. Our R-MABM framework\nis general, it allows for stable multi-agent learning, and represents a\nprincipled and robust direction to extend existing economic simulators.\n", "link": "http://arxiv.org/abs/2405.02161v1", "date": "2024-05-03", "relevancy": 1.4126, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4815}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulating%20the%20economic%20impact%20of%20rationality%20through%20reinforcement%0A%20%20learning%20and%20agent-based%20modelling&body=Title%3A%20Simulating%20the%20economic%20impact%20of%20rationality%20through%20reinforcement%0A%20%20learning%20and%20agent-based%20modelling%0AAuthor%3A%20Simone%20Brusatin%20and%20Tommaso%20Padoan%20and%20Andrea%20Coletta%20and%20Domenico%20Delli%20Gatti%20and%20Aldo%20Glielmo%0AAbstract%3A%20%20%20Agent-based%20models%20%28ABMs%29%20are%20simulation%20models%20used%20in%20economics%20to%20overcome%0Asome%20of%20the%20limitations%20of%20traditional%20frameworks%20based%20on%20general%20equilibrium%0Aassumptions.%20However%2C%20agents%20within%20an%20ABM%20follow%20predetermined%2C%20not%20fully%0Arational%2C%20behavioural%20rules%20which%20can%20be%20cumbersome%20to%20design%20and%20difficult%20to%0Ajustify.%20Here%20we%20leverage%20multi-agent%20reinforcement%20learning%20%28RL%29%20to%20expand%20the%0Acapabilities%20of%20ABMs%20with%20the%20introduction%20of%20fully%20rational%20agents%20that%20learn%0Atheir%20policy%20by%20interacting%20with%20the%20environment%20and%20maximising%20a%20reward%0Afunction.%20Specifically%2C%20we%20propose%20a%20%27Rational%20macro%20ABM%27%20%28R-MABM%29%20framework%20by%0Aextending%20a%20paradigmatic%20macro%20ABM%20from%20the%20economic%20literature.%20We%20show%20that%0Agradually%20substituting%20ABM%20firms%20in%20the%20model%20with%20RL%20agents%2C%20trained%20to%0Amaximise%20profits%2C%20allows%20for%20a%20thorough%20study%20of%20the%20impact%20of%20rationality%20on%0Athe%20economy.%20We%20find%20that%20RL%20agents%20spontaneously%20learn%20three%20distinct%0Astrategies%20for%20maximising%20profits%2C%20with%20the%20optimal%20strategy%20depending%20on%20the%0Alevel%20of%20market%20competition%20and%20rationality.%20We%20also%20find%20that%20RL%20agents%20with%0Aindependent%20policies%2C%20and%20without%20the%20ability%20to%20communicate%20with%20each%20other%2C%0Aspontaneously%20learn%20to%20segregate%20into%20different%20strategic%20groups%2C%20thus%0Aincreasing%20market%20power%20and%20overall%20profits.%20Finally%2C%20we%20find%20that%20a%20higher%0Adegree%20of%20rationality%20in%20the%20economy%20always%20improves%20the%20macroeconomic%0Aenvironment%20as%20measured%20by%20total%20output%2C%20depending%20on%20the%20specific%20rational%0Apolicy%2C%20this%20can%20come%20at%20the%20cost%20of%20higher%20instability.%20Our%20R-MABM%20framework%0Ais%20general%2C%20it%20allows%20for%20stable%20multi-agent%20learning%2C%20and%20represents%20a%0Aprincipled%20and%20robust%20direction%20to%20extend%20existing%20economic%20simulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulating%2520the%2520economic%2520impact%2520of%2520rationality%2520through%2520reinforcement%250A%2520%2520learning%2520and%2520agent-based%2520modelling%26entry.906535625%3DSimone%2520Brusatin%2520and%2520Tommaso%2520Padoan%2520and%2520Andrea%2520Coletta%2520and%2520Domenico%2520Delli%2520Gatti%2520and%2520Aldo%2520Glielmo%26entry.1292438233%3D%2520%2520Agent-based%2520models%2520%2528ABMs%2529%2520are%2520simulation%2520models%2520used%2520in%2520economics%2520to%2520overcome%250Asome%2520of%2520the%2520limitations%2520of%2520traditional%2520frameworks%2520based%2520on%2520general%2520equilibrium%250Aassumptions.%2520However%252C%2520agents%2520within%2520an%2520ABM%2520follow%2520predetermined%252C%2520not%2520fully%250Arational%252C%2520behavioural%2520rules%2520which%2520can%2520be%2520cumbersome%2520to%2520design%2520and%2520difficult%2520to%250Ajustify.%2520Here%2520we%2520leverage%2520multi-agent%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520expand%2520the%250Acapabilities%2520of%2520ABMs%2520with%2520the%2520introduction%2520of%2520fully%2520rational%2520agents%2520that%2520learn%250Atheir%2520policy%2520by%2520interacting%2520with%2520the%2520environment%2520and%2520maximising%2520a%2520reward%250Afunction.%2520Specifically%252C%2520we%2520propose%2520a%2520%2527Rational%2520macro%2520ABM%2527%2520%2528R-MABM%2529%2520framework%2520by%250Aextending%2520a%2520paradigmatic%2520macro%2520ABM%2520from%2520the%2520economic%2520literature.%2520We%2520show%2520that%250Agradually%2520substituting%2520ABM%2520firms%2520in%2520the%2520model%2520with%2520RL%2520agents%252C%2520trained%2520to%250Amaximise%2520profits%252C%2520allows%2520for%2520a%2520thorough%2520study%2520of%2520the%2520impact%2520of%2520rationality%2520on%250Athe%2520economy.%2520We%2520find%2520that%2520RL%2520agents%2520spontaneously%2520learn%2520three%2520distinct%250Astrategies%2520for%2520maximising%2520profits%252C%2520with%2520the%2520optimal%2520strategy%2520depending%2520on%2520the%250Alevel%2520of%2520market%2520competition%2520and%2520rationality.%2520We%2520also%2520find%2520that%2520RL%2520agents%2520with%250Aindependent%2520policies%252C%2520and%2520without%2520the%2520ability%2520to%2520communicate%2520with%2520each%2520other%252C%250Aspontaneously%2520learn%2520to%2520segregate%2520into%2520different%2520strategic%2520groups%252C%2520thus%250Aincreasing%2520market%2520power%2520and%2520overall%2520profits.%2520Finally%252C%2520we%2520find%2520that%2520a%2520higher%250Adegree%2520of%2520rationality%2520in%2520the%2520economy%2520always%2520improves%2520the%2520macroeconomic%250Aenvironment%2520as%2520measured%2520by%2520total%2520output%252C%2520depending%2520on%2520the%2520specific%2520rational%250Apolicy%252C%2520this%2520can%2520come%2520at%2520the%2520cost%2520of%2520higher%2520instability.%2520Our%2520R-MABM%2520framework%250Ais%2520general%252C%2520it%2520allows%2520for%2520stable%2520multi-agent%2520learning%252C%2520and%2520represents%2520a%250Aprincipled%2520and%2520robust%2520direction%2520to%2520extend%2520existing%2520economic%2520simulators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulating%20the%20economic%20impact%20of%20rationality%20through%20reinforcement%0A%20%20learning%20and%20agent-based%20modelling&entry.906535625=Simone%20Brusatin%20and%20Tommaso%20Padoan%20and%20Andrea%20Coletta%20and%20Domenico%20Delli%20Gatti%20and%20Aldo%20Glielmo&entry.1292438233=%20%20Agent-based%20models%20%28ABMs%29%20are%20simulation%20models%20used%20in%20economics%20to%20overcome%0Asome%20of%20the%20limitations%20of%20traditional%20frameworks%20based%20on%20general%20equilibrium%0Aassumptions.%20However%2C%20agents%20within%20an%20ABM%20follow%20predetermined%2C%20not%20fully%0Arational%2C%20behavioural%20rules%20which%20can%20be%20cumbersome%20to%20design%20and%20difficult%20to%0Ajustify.%20Here%20we%20leverage%20multi-agent%20reinforcement%20learning%20%28RL%29%20to%20expand%20the%0Acapabilities%20of%20ABMs%20with%20the%20introduction%20of%20fully%20rational%20agents%20that%20learn%0Atheir%20policy%20by%20interacting%20with%20the%20environment%20and%20maximising%20a%20reward%0Afunction.%20Specifically%2C%20we%20propose%20a%20%27Rational%20macro%20ABM%27%20%28R-MABM%29%20framework%20by%0Aextending%20a%20paradigmatic%20macro%20ABM%20from%20the%20economic%20literature.%20We%20show%20that%0Agradually%20substituting%20ABM%20firms%20in%20the%20model%20with%20RL%20agents%2C%20trained%20to%0Amaximise%20profits%2C%20allows%20for%20a%20thorough%20study%20of%20the%20impact%20of%20rationality%20on%0Athe%20economy.%20We%20find%20that%20RL%20agents%20spontaneously%20learn%20three%20distinct%0Astrategies%20for%20maximising%20profits%2C%20with%20the%20optimal%20strategy%20depending%20on%20the%0Alevel%20of%20market%20competition%20and%20rationality.%20We%20also%20find%20that%20RL%20agents%20with%0Aindependent%20policies%2C%20and%20without%20the%20ability%20to%20communicate%20with%20each%20other%2C%0Aspontaneously%20learn%20to%20segregate%20into%20different%20strategic%20groups%2C%20thus%0Aincreasing%20market%20power%20and%20overall%20profits.%20Finally%2C%20we%20find%20that%20a%20higher%0Adegree%20of%20rationality%20in%20the%20economy%20always%20improves%20the%20macroeconomic%0Aenvironment%20as%20measured%20by%20total%20output%2C%20depending%20on%20the%20specific%20rational%0Apolicy%2C%20this%20can%20come%20at%20the%20cost%20of%20higher%20instability.%20Our%20R-MABM%20framework%0Ais%20general%2C%20it%20allows%20for%20stable%20multi-agent%20learning%2C%20and%20represents%20a%0Aprincipled%20and%20robust%20direction%20to%20extend%20existing%20economic%20simulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02161v1&entry.124074799=Read"},
{"title": "Joint sentiment analysis of lyrics and audio in music", "author": "Lea Schaab and Anna Kruspe", "abstract": "  Sentiment or mood can express themselves on various levels in music. In\nautomatic analysis, the actual audio data is usually analyzed, but the lyrics\ncan also play a crucial role in the perception of moods. We first evaluate\nvarious models for sentiment analysis based on lyrics and audio separately. The\ncorresponding approaches already show satisfactory results, but they also\nexhibit weaknesses, the causes of which we examine in more detail. Furthermore,\ndifferent approaches to combining the audio and lyrics results are proposed and\nevaluated. Considering both modalities generally leads to improved performance.\nWe investigate misclassifications and (also intentional) contradictions between\naudio and lyrics sentiment more closely, and identify possible causes. Finally,\nwe address fundamental problems in this research area, such as high\nsubjectivity, lack of data, and inconsistency in emotion taxonomies.\n", "link": "http://arxiv.org/abs/2405.01988v1", "date": "2024-05-03", "relevancy": 0.8099, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4244}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3982}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.3922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20sentiment%20analysis%20of%20lyrics%20and%20audio%20in%20music&body=Title%3A%20Joint%20sentiment%20analysis%20of%20lyrics%20and%20audio%20in%20music%0AAuthor%3A%20Lea%20Schaab%20and%20Anna%20Kruspe%0AAbstract%3A%20%20%20Sentiment%20or%20mood%20can%20express%20themselves%20on%20various%20levels%20in%20music.%20In%0Aautomatic%20analysis%2C%20the%20actual%20audio%20data%20is%20usually%20analyzed%2C%20but%20the%20lyrics%0Acan%20also%20play%20a%20crucial%20role%20in%20the%20perception%20of%20moods.%20We%20first%20evaluate%0Avarious%20models%20for%20sentiment%20analysis%20based%20on%20lyrics%20and%20audio%20separately.%20The%0Acorresponding%20approaches%20already%20show%20satisfactory%20results%2C%20but%20they%20also%0Aexhibit%20weaknesses%2C%20the%20causes%20of%20which%20we%20examine%20in%20more%20detail.%20Furthermore%2C%0Adifferent%20approaches%20to%20combining%20the%20audio%20and%20lyrics%20results%20are%20proposed%20and%0Aevaluated.%20Considering%20both%20modalities%20generally%20leads%20to%20improved%20performance.%0AWe%20investigate%20misclassifications%20and%20%28also%20intentional%29%20contradictions%20between%0Aaudio%20and%20lyrics%20sentiment%20more%20closely%2C%20and%20identify%20possible%20causes.%20Finally%2C%0Awe%20address%20fundamental%20problems%20in%20this%20research%20area%2C%20such%20as%20high%0Asubjectivity%2C%20lack%20of%20data%2C%20and%20inconsistency%20in%20emotion%20taxonomies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520sentiment%2520analysis%2520of%2520lyrics%2520and%2520audio%2520in%2520music%26entry.906535625%3DLea%2520Schaab%2520and%2520Anna%2520Kruspe%26entry.1292438233%3D%2520%2520Sentiment%2520or%2520mood%2520can%2520express%2520themselves%2520on%2520various%2520levels%2520in%2520music.%2520In%250Aautomatic%2520analysis%252C%2520the%2520actual%2520audio%2520data%2520is%2520usually%2520analyzed%252C%2520but%2520the%2520lyrics%250Acan%2520also%2520play%2520a%2520crucial%2520role%2520in%2520the%2520perception%2520of%2520moods.%2520We%2520first%2520evaluate%250Avarious%2520models%2520for%2520sentiment%2520analysis%2520based%2520on%2520lyrics%2520and%2520audio%2520separately.%2520The%250Acorresponding%2520approaches%2520already%2520show%2520satisfactory%2520results%252C%2520but%2520they%2520also%250Aexhibit%2520weaknesses%252C%2520the%2520causes%2520of%2520which%2520we%2520examine%2520in%2520more%2520detail.%2520Furthermore%252C%250Adifferent%2520approaches%2520to%2520combining%2520the%2520audio%2520and%2520lyrics%2520results%2520are%2520proposed%2520and%250Aevaluated.%2520Considering%2520both%2520modalities%2520generally%2520leads%2520to%2520improved%2520performance.%250AWe%2520investigate%2520misclassifications%2520and%2520%2528also%2520intentional%2529%2520contradictions%2520between%250Aaudio%2520and%2520lyrics%2520sentiment%2520more%2520closely%252C%2520and%2520identify%2520possible%2520causes.%2520Finally%252C%250Awe%2520address%2520fundamental%2520problems%2520in%2520this%2520research%2520area%252C%2520such%2520as%2520high%250Asubjectivity%252C%2520lack%2520of%2520data%252C%2520and%2520inconsistency%2520in%2520emotion%2520taxonomies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20sentiment%20analysis%20of%20lyrics%20and%20audio%20in%20music&entry.906535625=Lea%20Schaab%20and%20Anna%20Kruspe&entry.1292438233=%20%20Sentiment%20or%20mood%20can%20express%20themselves%20on%20various%20levels%20in%20music.%20In%0Aautomatic%20analysis%2C%20the%20actual%20audio%20data%20is%20usually%20analyzed%2C%20but%20the%20lyrics%0Acan%20also%20play%20a%20crucial%20role%20in%20the%20perception%20of%20moods.%20We%20first%20evaluate%0Avarious%20models%20for%20sentiment%20analysis%20based%20on%20lyrics%20and%20audio%20separately.%20The%0Acorresponding%20approaches%20already%20show%20satisfactory%20results%2C%20but%20they%20also%0Aexhibit%20weaknesses%2C%20the%20causes%20of%20which%20we%20examine%20in%20more%20detail.%20Furthermore%2C%0Adifferent%20approaches%20to%20combining%20the%20audio%20and%20lyrics%20results%20are%20proposed%20and%0Aevaluated.%20Considering%20both%20modalities%20generally%20leads%20to%20improved%20performance.%0AWe%20investigate%20misclassifications%20and%20%28also%20intentional%29%20contradictions%20between%0Aaudio%20and%20lyrics%20sentiment%20more%20closely%2C%20and%20identify%20possible%20causes.%20Finally%2C%0Awe%20address%20fundamental%20problems%20in%20this%20research%20area%2C%20such%20as%20high%0Asubjectivity%2C%20lack%20of%20data%2C%20and%20inconsistency%20in%20emotion%20taxonomies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01988v1&entry.124074799=Read"},
{"title": "Characterized Diffusion and Spatial-Temporal Interaction Network for\n  Trajectory Prediction in Autonomous Driving", "author": "Haicheng Liao and Xuelin Li and Yongkang Li and Hanlin Kong and Chengyue Wang and Bonan Wang and Yanchen Guan and KaHou Tam and Zhenning Li and Chengzhong Xu", "abstract": "  Trajectory prediction is a cornerstone in autonomous driving (AD), playing a\ncritical role in enabling vehicles to navigate safely and efficiently in\ndynamic environments. To address this task, this paper presents a novel\ntrajectory prediction model tailored for accuracy in the face of heterogeneous\nand uncertain traffic scenarios. At the heart of this model lies the\nCharacterized Diffusion Module, an innovative module designed to simulate\ntraffic scenarios with inherent uncertainty. This module enriches the\npredictive process by infusing it with detailed semantic information, thereby\nenhancing trajectory prediction accuracy. Complementing this, our\nSpatio-Temporal (ST) Interaction Module captures the nuanced effects of traffic\nscenarios on vehicle dynamics across both spatial and temporal dimensions with\nremarkable effectiveness. Demonstrated through exhaustive evaluations, our\nmodel sets a new standard in trajectory prediction, achieving state-of-the-art\n(SOTA) results on the Next Generation Simulation (NGSIM), Highway Drone\n(HighD), and Macao Connected Autonomous Driving (MoCAD) datasets across both\nshort and extended temporal spans. This performance underscores the model's\nunparalleled adaptability and efficacy in navigating complex traffic scenarios,\nincluding highways, urban streets, and intersections.\n", "link": "http://arxiv.org/abs/2405.02145v1", "date": "2024-05-03", "relevancy": 1.0614, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5697}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5174}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterized%20Diffusion%20and%20Spatial-Temporal%20Interaction%20Network%20for%0A%20%20Trajectory%20Prediction%20in%20Autonomous%20Driving&body=Title%3A%20Characterized%20Diffusion%20and%20Spatial-Temporal%20Interaction%20Network%20for%0A%20%20Trajectory%20Prediction%20in%20Autonomous%20Driving%0AAuthor%3A%20Haicheng%20Liao%20and%20Xuelin%20Li%20and%20Yongkang%20Li%20and%20Hanlin%20Kong%20and%20Chengyue%20Wang%20and%20Bonan%20Wang%20and%20Yanchen%20Guan%20and%20KaHou%20Tam%20and%20Zhenning%20Li%20and%20Chengzhong%20Xu%0AAbstract%3A%20%20%20Trajectory%20prediction%20is%20a%20cornerstone%20in%20autonomous%20driving%20%28AD%29%2C%20playing%20a%0Acritical%20role%20in%20enabling%20vehicles%20to%20navigate%20safely%20and%20efficiently%20in%0Adynamic%20environments.%20To%20address%20this%20task%2C%20this%20paper%20presents%20a%20novel%0Atrajectory%20prediction%20model%20tailored%20for%20accuracy%20in%20the%20face%20of%20heterogeneous%0Aand%20uncertain%20traffic%20scenarios.%20At%20the%20heart%20of%20this%20model%20lies%20the%0ACharacterized%20Diffusion%20Module%2C%20an%20innovative%20module%20designed%20to%20simulate%0Atraffic%20scenarios%20with%20inherent%20uncertainty.%20This%20module%20enriches%20the%0Apredictive%20process%20by%20infusing%20it%20with%20detailed%20semantic%20information%2C%20thereby%0Aenhancing%20trajectory%20prediction%20accuracy.%20Complementing%20this%2C%20our%0ASpatio-Temporal%20%28ST%29%20Interaction%20Module%20captures%20the%20nuanced%20effects%20of%20traffic%0Ascenarios%20on%20vehicle%20dynamics%20across%20both%20spatial%20and%20temporal%20dimensions%20with%0Aremarkable%20effectiveness.%20Demonstrated%20through%20exhaustive%20evaluations%2C%20our%0Amodel%20sets%20a%20new%20standard%20in%20trajectory%20prediction%2C%20achieving%20state-of-the-art%0A%28SOTA%29%20results%20on%20the%20Next%20Generation%20Simulation%20%28NGSIM%29%2C%20Highway%20Drone%0A%28HighD%29%2C%20and%20Macao%20Connected%20Autonomous%20Driving%20%28MoCAD%29%20datasets%20across%20both%0Ashort%20and%20extended%20temporal%20spans.%20This%20performance%20underscores%20the%20model%27s%0Aunparalleled%20adaptability%20and%20efficacy%20in%20navigating%20complex%20traffic%20scenarios%2C%0Aincluding%20highways%2C%20urban%20streets%2C%20and%20intersections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterized%2520Diffusion%2520and%2520Spatial-Temporal%2520Interaction%2520Network%2520for%250A%2520%2520Trajectory%2520Prediction%2520in%2520Autonomous%2520Driving%26entry.906535625%3DHaicheng%2520Liao%2520and%2520Xuelin%2520Li%2520and%2520Yongkang%2520Li%2520and%2520Hanlin%2520Kong%2520and%2520Chengyue%2520Wang%2520and%2520Bonan%2520Wang%2520and%2520Yanchen%2520Guan%2520and%2520KaHou%2520Tam%2520and%2520Zhenning%2520Li%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520is%2520a%2520cornerstone%2520in%2520autonomous%2520driving%2520%2528AD%2529%252C%2520playing%2520a%250Acritical%2520role%2520in%2520enabling%2520vehicles%2520to%2520navigate%2520safely%2520and%2520efficiently%2520in%250Adynamic%2520environments.%2520To%2520address%2520this%2520task%252C%2520this%2520paper%2520presents%2520a%2520novel%250Atrajectory%2520prediction%2520model%2520tailored%2520for%2520accuracy%2520in%2520the%2520face%2520of%2520heterogeneous%250Aand%2520uncertain%2520traffic%2520scenarios.%2520At%2520the%2520heart%2520of%2520this%2520model%2520lies%2520the%250ACharacterized%2520Diffusion%2520Module%252C%2520an%2520innovative%2520module%2520designed%2520to%2520simulate%250Atraffic%2520scenarios%2520with%2520inherent%2520uncertainty.%2520This%2520module%2520enriches%2520the%250Apredictive%2520process%2520by%2520infusing%2520it%2520with%2520detailed%2520semantic%2520information%252C%2520thereby%250Aenhancing%2520trajectory%2520prediction%2520accuracy.%2520Complementing%2520this%252C%2520our%250ASpatio-Temporal%2520%2528ST%2529%2520Interaction%2520Module%2520captures%2520the%2520nuanced%2520effects%2520of%2520traffic%250Ascenarios%2520on%2520vehicle%2520dynamics%2520across%2520both%2520spatial%2520and%2520temporal%2520dimensions%2520with%250Aremarkable%2520effectiveness.%2520Demonstrated%2520through%2520exhaustive%2520evaluations%252C%2520our%250Amodel%2520sets%2520a%2520new%2520standard%2520in%2520trajectory%2520prediction%252C%2520achieving%2520state-of-the-art%250A%2528SOTA%2529%2520results%2520on%2520the%2520Next%2520Generation%2520Simulation%2520%2528NGSIM%2529%252C%2520Highway%2520Drone%250A%2528HighD%2529%252C%2520and%2520Macao%2520Connected%2520Autonomous%2520Driving%2520%2528MoCAD%2529%2520datasets%2520across%2520both%250Ashort%2520and%2520extended%2520temporal%2520spans.%2520This%2520performance%2520underscores%2520the%2520model%2527s%250Aunparalleled%2520adaptability%2520and%2520efficacy%2520in%2520navigating%2520complex%2520traffic%2520scenarios%252C%250Aincluding%2520highways%252C%2520urban%2520streets%252C%2520and%2520intersections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterized%20Diffusion%20and%20Spatial-Temporal%20Interaction%20Network%20for%0A%20%20Trajectory%20Prediction%20in%20Autonomous%20Driving&entry.906535625=Haicheng%20Liao%20and%20Xuelin%20Li%20and%20Yongkang%20Li%20and%20Hanlin%20Kong%20and%20Chengyue%20Wang%20and%20Bonan%20Wang%20and%20Yanchen%20Guan%20and%20KaHou%20Tam%20and%20Zhenning%20Li%20and%20Chengzhong%20Xu&entry.1292438233=%20%20Trajectory%20prediction%20is%20a%20cornerstone%20in%20autonomous%20driving%20%28AD%29%2C%20playing%20a%0Acritical%20role%20in%20enabling%20vehicles%20to%20navigate%20safely%20and%20efficiently%20in%0Adynamic%20environments.%20To%20address%20this%20task%2C%20this%20paper%20presents%20a%20novel%0Atrajectory%20prediction%20model%20tailored%20for%20accuracy%20in%20the%20face%20of%20heterogeneous%0Aand%20uncertain%20traffic%20scenarios.%20At%20the%20heart%20of%20this%20model%20lies%20the%0ACharacterized%20Diffusion%20Module%2C%20an%20innovative%20module%20designed%20to%20simulate%0Atraffic%20scenarios%20with%20inherent%20uncertainty.%20This%20module%20enriches%20the%0Apredictive%20process%20by%20infusing%20it%20with%20detailed%20semantic%20information%2C%20thereby%0Aenhancing%20trajectory%20prediction%20accuracy.%20Complementing%20this%2C%20our%0ASpatio-Temporal%20%28ST%29%20Interaction%20Module%20captures%20the%20nuanced%20effects%20of%20traffic%0Ascenarios%20on%20vehicle%20dynamics%20across%20both%20spatial%20and%20temporal%20dimensions%20with%0Aremarkable%20effectiveness.%20Demonstrated%20through%20exhaustive%20evaluations%2C%20our%0Amodel%20sets%20a%20new%20standard%20in%20trajectory%20prediction%2C%20achieving%20state-of-the-art%0A%28SOTA%29%20results%20on%20the%20Next%20Generation%20Simulation%20%28NGSIM%29%2C%20Highway%20Drone%0A%28HighD%29%2C%20and%20Macao%20Connected%20Autonomous%20Driving%20%28MoCAD%29%20datasets%20across%20both%0Ashort%20and%20extended%20temporal%20spans.%20This%20performance%20underscores%20the%20model%27s%0Aunparalleled%20adaptability%20and%20efficacy%20in%20navigating%20complex%20traffic%20scenarios%2C%0Aincluding%20highways%2C%20urban%20streets%2C%20and%20intersections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02145v1&entry.124074799=Read"},
{"title": "Impact of emoji exclusion on the performance of Arabic sarcasm detection\n  models", "author": "Ghalyah H. Aleryani and Wael Deabes and Khaled Albishre and Alaa E. Abdel-Hakim", "abstract": "  The complex challenge of detecting sarcasm in Arabic speech on social media\nis increased by the language diversity and the nature of sarcastic expressions.\nThere is a significant gap in the capability of existing models to effectively\ninterpret sarcasm in Arabic, which mandates the necessity for more\nsophisticated and precise detection methods. In this paper, we investigate the\nimpact of a fundamental preprocessing component on sarcasm speech detection.\nWhile emojis play a crucial role in mitigating the absence effect of body\nlanguage and facial expressions in modern communication, their impact on\nautomated text analysis, particularly in sarcasm detection, remains\nunderexplored. We investigate the impact of emoji exclusion from datasets on\nthe performance of sarcasm detection models in social media content for Arabic\nas a vocabulary-super rich language. This investigation includes the adaptation\nand enhancement of AraBERT pre-training models, specifically by excluding\nemojis, to improve sarcasm detection capabilities. We use AraBERT pre-training\nto refine the specified models, demonstrating that the removal of emojis can\nsignificantly boost the accuracy of sarcasm detection. This approach\nfacilitates a more refined interpretation of language, eliminating the\npotential confusion introduced by non-textual elements. The evaluated AraBERT\nmodels, through the focused strategy of emoji removal, adeptly navigate the\ncomplexities of Arabic sarcasm. This study establishes new benchmarks in Arabic\nnatural language processing and presents valuable insights for social media\nplatforms.\n", "link": "http://arxiv.org/abs/2405.02195v1", "date": "2024-05-03", "relevancy": 1.1665, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3893}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3888}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20emoji%20exclusion%20on%20the%20performance%20of%20Arabic%20sarcasm%20detection%0A%20%20models&body=Title%3A%20Impact%20of%20emoji%20exclusion%20on%20the%20performance%20of%20Arabic%20sarcasm%20detection%0A%20%20models%0AAuthor%3A%20Ghalyah%20H.%20Aleryani%20and%20Wael%20Deabes%20and%20Khaled%20Albishre%20and%20Alaa%20E.%20Abdel-Hakim%0AAbstract%3A%20%20%20The%20complex%20challenge%20of%20detecting%20sarcasm%20in%20Arabic%20speech%20on%20social%20media%0Ais%20increased%20by%20the%20language%20diversity%20and%20the%20nature%20of%20sarcastic%20expressions.%0AThere%20is%20a%20significant%20gap%20in%20the%20capability%20of%20existing%20models%20to%20effectively%0Ainterpret%20sarcasm%20in%20Arabic%2C%20which%20mandates%20the%20necessity%20for%20more%0Asophisticated%20and%20precise%20detection%20methods.%20In%20this%20paper%2C%20we%20investigate%20the%0Aimpact%20of%20a%20fundamental%20preprocessing%20component%20on%20sarcasm%20speech%20detection.%0AWhile%20emojis%20play%20a%20crucial%20role%20in%20mitigating%20the%20absence%20effect%20of%20body%0Alanguage%20and%20facial%20expressions%20in%20modern%20communication%2C%20their%20impact%20on%0Aautomated%20text%20analysis%2C%20particularly%20in%20sarcasm%20detection%2C%20remains%0Aunderexplored.%20We%20investigate%20the%20impact%20of%20emoji%20exclusion%20from%20datasets%20on%0Athe%20performance%20of%20sarcasm%20detection%20models%20in%20social%20media%20content%20for%20Arabic%0Aas%20a%20vocabulary-super%20rich%20language.%20This%20investigation%20includes%20the%20adaptation%0Aand%20enhancement%20of%20AraBERT%20pre-training%20models%2C%20specifically%20by%20excluding%0Aemojis%2C%20to%20improve%20sarcasm%20detection%20capabilities.%20We%20use%20AraBERT%20pre-training%0Ato%20refine%20the%20specified%20models%2C%20demonstrating%20that%20the%20removal%20of%20emojis%20can%0Asignificantly%20boost%20the%20accuracy%20of%20sarcasm%20detection.%20This%20approach%0Afacilitates%20a%20more%20refined%20interpretation%20of%20language%2C%20eliminating%20the%0Apotential%20confusion%20introduced%20by%20non-textual%20elements.%20The%20evaluated%20AraBERT%0Amodels%2C%20through%20the%20focused%20strategy%20of%20emoji%20removal%2C%20adeptly%20navigate%20the%0Acomplexities%20of%20Arabic%20sarcasm.%20This%20study%20establishes%20new%20benchmarks%20in%20Arabic%0Anatural%20language%20processing%20and%20presents%20valuable%20insights%20for%20social%20media%0Aplatforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520emoji%2520exclusion%2520on%2520the%2520performance%2520of%2520Arabic%2520sarcasm%2520detection%250A%2520%2520models%26entry.906535625%3DGhalyah%2520H.%2520Aleryani%2520and%2520Wael%2520Deabes%2520and%2520Khaled%2520Albishre%2520and%2520Alaa%2520E.%2520Abdel-Hakim%26entry.1292438233%3D%2520%2520The%2520complex%2520challenge%2520of%2520detecting%2520sarcasm%2520in%2520Arabic%2520speech%2520on%2520social%2520media%250Ais%2520increased%2520by%2520the%2520language%2520diversity%2520and%2520the%2520nature%2520of%2520sarcastic%2520expressions.%250AThere%2520is%2520a%2520significant%2520gap%2520in%2520the%2520capability%2520of%2520existing%2520models%2520to%2520effectively%250Ainterpret%2520sarcasm%2520in%2520Arabic%252C%2520which%2520mandates%2520the%2520necessity%2520for%2520more%250Asophisticated%2520and%2520precise%2520detection%2520methods.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520a%2520fundamental%2520preprocessing%2520component%2520on%2520sarcasm%2520speech%2520detection.%250AWhile%2520emojis%2520play%2520a%2520crucial%2520role%2520in%2520mitigating%2520the%2520absence%2520effect%2520of%2520body%250Alanguage%2520and%2520facial%2520expressions%2520in%2520modern%2520communication%252C%2520their%2520impact%2520on%250Aautomated%2520text%2520analysis%252C%2520particularly%2520in%2520sarcasm%2520detection%252C%2520remains%250Aunderexplored.%2520We%2520investigate%2520the%2520impact%2520of%2520emoji%2520exclusion%2520from%2520datasets%2520on%250Athe%2520performance%2520of%2520sarcasm%2520detection%2520models%2520in%2520social%2520media%2520content%2520for%2520Arabic%250Aas%2520a%2520vocabulary-super%2520rich%2520language.%2520This%2520investigation%2520includes%2520the%2520adaptation%250Aand%2520enhancement%2520of%2520AraBERT%2520pre-training%2520models%252C%2520specifically%2520by%2520excluding%250Aemojis%252C%2520to%2520improve%2520sarcasm%2520detection%2520capabilities.%2520We%2520use%2520AraBERT%2520pre-training%250Ato%2520refine%2520the%2520specified%2520models%252C%2520demonstrating%2520that%2520the%2520removal%2520of%2520emojis%2520can%250Asignificantly%2520boost%2520the%2520accuracy%2520of%2520sarcasm%2520detection.%2520This%2520approach%250Afacilitates%2520a%2520more%2520refined%2520interpretation%2520of%2520language%252C%2520eliminating%2520the%250Apotential%2520confusion%2520introduced%2520by%2520non-textual%2520elements.%2520The%2520evaluated%2520AraBERT%250Amodels%252C%2520through%2520the%2520focused%2520strategy%2520of%2520emoji%2520removal%252C%2520adeptly%2520navigate%2520the%250Acomplexities%2520of%2520Arabic%2520sarcasm.%2520This%2520study%2520establishes%2520new%2520benchmarks%2520in%2520Arabic%250Anatural%2520language%2520processing%2520and%2520presents%2520valuable%2520insights%2520for%2520social%2520media%250Aplatforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20emoji%20exclusion%20on%20the%20performance%20of%20Arabic%20sarcasm%20detection%0A%20%20models&entry.906535625=Ghalyah%20H.%20Aleryani%20and%20Wael%20Deabes%20and%20Khaled%20Albishre%20and%20Alaa%20E.%20Abdel-Hakim&entry.1292438233=%20%20The%20complex%20challenge%20of%20detecting%20sarcasm%20in%20Arabic%20speech%20on%20social%20media%0Ais%20increased%20by%20the%20language%20diversity%20and%20the%20nature%20of%20sarcastic%20expressions.%0AThere%20is%20a%20significant%20gap%20in%20the%20capability%20of%20existing%20models%20to%20effectively%0Ainterpret%20sarcasm%20in%20Arabic%2C%20which%20mandates%20the%20necessity%20for%20more%0Asophisticated%20and%20precise%20detection%20methods.%20In%20this%20paper%2C%20we%20investigate%20the%0Aimpact%20of%20a%20fundamental%20preprocessing%20component%20on%20sarcasm%20speech%20detection.%0AWhile%20emojis%20play%20a%20crucial%20role%20in%20mitigating%20the%20absence%20effect%20of%20body%0Alanguage%20and%20facial%20expressions%20in%20modern%20communication%2C%20their%20impact%20on%0Aautomated%20text%20analysis%2C%20particularly%20in%20sarcasm%20detection%2C%20remains%0Aunderexplored.%20We%20investigate%20the%20impact%20of%20emoji%20exclusion%20from%20datasets%20on%0Athe%20performance%20of%20sarcasm%20detection%20models%20in%20social%20media%20content%20for%20Arabic%0Aas%20a%20vocabulary-super%20rich%20language.%20This%20investigation%20includes%20the%20adaptation%0Aand%20enhancement%20of%20AraBERT%20pre-training%20models%2C%20specifically%20by%20excluding%0Aemojis%2C%20to%20improve%20sarcasm%20detection%20capabilities.%20We%20use%20AraBERT%20pre-training%0Ato%20refine%20the%20specified%20models%2C%20demonstrating%20that%20the%20removal%20of%20emojis%20can%0Asignificantly%20boost%20the%20accuracy%20of%20sarcasm%20detection.%20This%20approach%0Afacilitates%20a%20more%20refined%20interpretation%20of%20language%2C%20eliminating%20the%0Apotential%20confusion%20introduced%20by%20non-textual%20elements.%20The%20evaluated%20AraBERT%0Amodels%2C%20through%20the%20focused%20strategy%20of%20emoji%20removal%2C%20adeptly%20navigate%20the%0Acomplexities%20of%20Arabic%20sarcasm.%20This%20study%20establishes%20new%20benchmarks%20in%20Arabic%0Anatural%20language%20processing%20and%20presents%20valuable%20insights%20for%20social%20media%0Aplatforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02195v1&entry.124074799=Read"},
{"title": "A Careful Examination of Large Language Model Performance on Grade\n  School Arithmetic", "author": "Hugh Zhang and Jeff Da and Dean Lee and Vaughn Robinson and Catherine Wu and Will Song and Tiffany Zhao and Pranav Raja and Dylan Slack and Qin Lyu and Sean Hendryx and Russell Kaplan and Michele Lunati and Summer Yue", "abstract": "  Large language models (LLMs) have achieved impressive success on many\nbenchmarks for mathematical reasoning. However, there is growing concern that\nsome of this performance actually reflects dataset contamination, where data\nclosely resembling benchmark questions leaks into the training data, instead of\ntrue reasoning ability. To investigate this claim rigorously, we commission\nGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and\ncomplexity of the established GSM8k benchmark, the gold standard for measuring\nelementary mathematical reasoning. We ensure that the two benchmarks are\ncomparable across important metrics such as human solve rates, number of steps\nin solution, answer magnitude, and more. When evaluating leading open- and\nclosed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with\nseveral families of models (e.g., Phi and Mistral) showing evidence of\nsystematic overfitting across almost all model sizes. At the same time, many\nmodels, especially those on the frontier, (e.g., Gemini/GPT/Claude) show\nminimal signs of overfitting. Further analysis suggests a positive relationship\n(Spearman's r^2=0.32) between a model's probability of generating an example\nfrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting that\nmany models may have partially memorized GSM8k.\n", "link": "http://arxiv.org/abs/2405.00332v3", "date": "2024-05-03", "relevancy": 0.8652, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4447}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4316}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Careful%20Examination%20of%20Large%20Language%20Model%20Performance%20on%20Grade%0A%20%20School%20Arithmetic&body=Title%3A%20A%20Careful%20Examination%20of%20Large%20Language%20Model%20Performance%20on%20Grade%0A%20%20School%20Arithmetic%0AAuthor%3A%20Hugh%20Zhang%20and%20Jeff%20Da%20and%20Dean%20Lee%20and%20Vaughn%20Robinson%20and%20Catherine%20Wu%20and%20Will%20Song%20and%20Tiffany%20Zhao%20and%20Pranav%20Raja%20and%20Dylan%20Slack%20and%20Qin%20Lyu%20and%20Sean%20Hendryx%20and%20Russell%20Kaplan%20and%20Michele%20Lunati%20and%20Summer%20Yue%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20success%20on%20many%0Abenchmarks%20for%20mathematical%20reasoning.%20However%2C%20there%20is%20growing%20concern%20that%0Asome%20of%20this%20performance%20actually%20reflects%20dataset%20contamination%2C%20where%20data%0Aclosely%20resembling%20benchmark%20questions%20leaks%20into%20the%20training%20data%2C%20instead%20of%0Atrue%20reasoning%20ability.%20To%20investigate%20this%20claim%20rigorously%2C%20we%20commission%0AGrade%20School%20Math%201000%20%28GSM1k%29.%20GSM1k%20is%20designed%20to%20mirror%20the%20style%20and%0Acomplexity%20of%20the%20established%20GSM8k%20benchmark%2C%20the%20gold%20standard%20for%20measuring%0Aelementary%20mathematical%20reasoning.%20We%20ensure%20that%20the%20two%20benchmarks%20are%0Acomparable%20across%20important%20metrics%20such%20as%20human%20solve%20rates%2C%20number%20of%20steps%0Ain%20solution%2C%20answer%20magnitude%2C%20and%20more.%20When%20evaluating%20leading%20open-%20and%0Aclosed-source%20LLMs%20on%20GSM1k%2C%20we%20observe%20accuracy%20drops%20of%20up%20to%2013%25%2C%20with%0Aseveral%20families%20of%20models%20%28e.g.%2C%20Phi%20and%20Mistral%29%20showing%20evidence%20of%0Asystematic%20overfitting%20across%20almost%20all%20model%20sizes.%20At%20the%20same%20time%2C%20many%0Amodels%2C%20especially%20those%20on%20the%20frontier%2C%20%28e.g.%2C%20Gemini/GPT/Claude%29%20show%0Aminimal%20signs%20of%20overfitting.%20Further%20analysis%20suggests%20a%20positive%20relationship%0A%28Spearman%27s%20r%5E2%3D0.32%29%20between%20a%20model%27s%20probability%20of%20generating%20an%20example%0Afrom%20GSM8k%20and%20its%20performance%20gap%20between%20GSM8k%20and%20GSM1k%2C%20suggesting%20that%0Amany%20models%20may%20have%20partially%20memorized%20GSM8k.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00332v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Careful%2520Examination%2520of%2520Large%2520Language%2520Model%2520Performance%2520on%2520Grade%250A%2520%2520School%2520Arithmetic%26entry.906535625%3DHugh%2520Zhang%2520and%2520Jeff%2520Da%2520and%2520Dean%2520Lee%2520and%2520Vaughn%2520Robinson%2520and%2520Catherine%2520Wu%2520and%2520Will%2520Song%2520and%2520Tiffany%2520Zhao%2520and%2520Pranav%2520Raja%2520and%2520Dylan%2520Slack%2520and%2520Qin%2520Lyu%2520and%2520Sean%2520Hendryx%2520and%2520Russell%2520Kaplan%2520and%2520Michele%2520Lunati%2520and%2520Summer%2520Yue%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520success%2520on%2520many%250Abenchmarks%2520for%2520mathematical%2520reasoning.%2520However%252C%2520there%2520is%2520growing%2520concern%2520that%250Asome%2520of%2520this%2520performance%2520actually%2520reflects%2520dataset%2520contamination%252C%2520where%2520data%250Aclosely%2520resembling%2520benchmark%2520questions%2520leaks%2520into%2520the%2520training%2520data%252C%2520instead%2520of%250Atrue%2520reasoning%2520ability.%2520To%2520investigate%2520this%2520claim%2520rigorously%252C%2520we%2520commission%250AGrade%2520School%2520Math%25201000%2520%2528GSM1k%2529.%2520GSM1k%2520is%2520designed%2520to%2520mirror%2520the%2520style%2520and%250Acomplexity%2520of%2520the%2520established%2520GSM8k%2520benchmark%252C%2520the%2520gold%2520standard%2520for%2520measuring%250Aelementary%2520mathematical%2520reasoning.%2520We%2520ensure%2520that%2520the%2520two%2520benchmarks%2520are%250Acomparable%2520across%2520important%2520metrics%2520such%2520as%2520human%2520solve%2520rates%252C%2520number%2520of%2520steps%250Ain%2520solution%252C%2520answer%2520magnitude%252C%2520and%2520more.%2520When%2520evaluating%2520leading%2520open-%2520and%250Aclosed-source%2520LLMs%2520on%2520GSM1k%252C%2520we%2520observe%2520accuracy%2520drops%2520of%2520up%2520to%252013%2525%252C%2520with%250Aseveral%2520families%2520of%2520models%2520%2528e.g.%252C%2520Phi%2520and%2520Mistral%2529%2520showing%2520evidence%2520of%250Asystematic%2520overfitting%2520across%2520almost%2520all%2520model%2520sizes.%2520At%2520the%2520same%2520time%252C%2520many%250Amodels%252C%2520especially%2520those%2520on%2520the%2520frontier%252C%2520%2528e.g.%252C%2520Gemini/GPT/Claude%2529%2520show%250Aminimal%2520signs%2520of%2520overfitting.%2520Further%2520analysis%2520suggests%2520a%2520positive%2520relationship%250A%2528Spearman%2527s%2520r%255E2%253D0.32%2529%2520between%2520a%2520model%2527s%2520probability%2520of%2520generating%2520an%2520example%250Afrom%2520GSM8k%2520and%2520its%2520performance%2520gap%2520between%2520GSM8k%2520and%2520GSM1k%252C%2520suggesting%2520that%250Amany%2520models%2520may%2520have%2520partially%2520memorized%2520GSM8k.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00332v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Careful%20Examination%20of%20Large%20Language%20Model%20Performance%20on%20Grade%0A%20%20School%20Arithmetic&entry.906535625=Hugh%20Zhang%20and%20Jeff%20Da%20and%20Dean%20Lee%20and%20Vaughn%20Robinson%20and%20Catherine%20Wu%20and%20Will%20Song%20and%20Tiffany%20Zhao%20and%20Pranav%20Raja%20and%20Dylan%20Slack%20and%20Qin%20Lyu%20and%20Sean%20Hendryx%20and%20Russell%20Kaplan%20and%20Michele%20Lunati%20and%20Summer%20Yue&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20success%20on%20many%0Abenchmarks%20for%20mathematical%20reasoning.%20However%2C%20there%20is%20growing%20concern%20that%0Asome%20of%20this%20performance%20actually%20reflects%20dataset%20contamination%2C%20where%20data%0Aclosely%20resembling%20benchmark%20questions%20leaks%20into%20the%20training%20data%2C%20instead%20of%0Atrue%20reasoning%20ability.%20To%20investigate%20this%20claim%20rigorously%2C%20we%20commission%0AGrade%20School%20Math%201000%20%28GSM1k%29.%20GSM1k%20is%20designed%20to%20mirror%20the%20style%20and%0Acomplexity%20of%20the%20established%20GSM8k%20benchmark%2C%20the%20gold%20standard%20for%20measuring%0Aelementary%20mathematical%20reasoning.%20We%20ensure%20that%20the%20two%20benchmarks%20are%0Acomparable%20across%20important%20metrics%20such%20as%20human%20solve%20rates%2C%20number%20of%20steps%0Ain%20solution%2C%20answer%20magnitude%2C%20and%20more.%20When%20evaluating%20leading%20open-%20and%0Aclosed-source%20LLMs%20on%20GSM1k%2C%20we%20observe%20accuracy%20drops%20of%20up%20to%2013%25%2C%20with%0Aseveral%20families%20of%20models%20%28e.g.%2C%20Phi%20and%20Mistral%29%20showing%20evidence%20of%0Asystematic%20overfitting%20across%20almost%20all%20model%20sizes.%20At%20the%20same%20time%2C%20many%0Amodels%2C%20especially%20those%20on%20the%20frontier%2C%20%28e.g.%2C%20Gemini/GPT/Claude%29%20show%0Aminimal%20signs%20of%20overfitting.%20Further%20analysis%20suggests%20a%20positive%20relationship%0A%28Spearman%27s%20r%5E2%3D0.32%29%20between%20a%20model%27s%20probability%20of%20generating%20an%20example%0Afrom%20GSM8k%20and%20its%20performance%20gap%20between%20GSM8k%20and%20GSM1k%2C%20suggesting%20that%0Amany%20models%20may%20have%20partially%20memorized%20GSM8k.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00332v3&entry.124074799=Read"},
{"title": "FocusLearn: Fully-Interpretable, High-Performance Modular Neural\n  Networks for Time Series", "author": "Qiqi Su and Christos Kloukinas and Artur d'Avila Garcez", "abstract": "  Multivariate time series have many applications, from healthcare and\nmeteorology to life science. Although deep learning models have shown excellent\npredictive performance for time series, they have been criticised for being\n\"black-boxes\" or non-interpretable. This paper proposes a novel modular neural\nnetwork model for multivariate time series prediction that is interpretable by\nconstruction. A recurrent neural network learns the temporal dependencies in\nthe data while an attention-based feature selection component selects the most\nrelevant features and suppresses redundant features used in the learning of the\ntemporal dependencies. A modular deep network is trained from the selected\nfeatures independently to show the users how features influence outcomes,\nmaking the model interpretable. Experimental results show that this approach\ncan outperform state-of-the-art interpretable Neural Additive Models (NAM) and\nvariations thereof in both regression and classification of time series tasks,\nachieving a predictive performance that is comparable to the top\nnon-interpretable methods for time series, LSTM and XGBoost.\n", "link": "http://arxiv.org/abs/2311.16834v4", "date": "2024-05-03", "relevancy": 1.5004, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5019}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocusLearn%3A%20Fully-Interpretable%2C%20High-Performance%20Modular%20Neural%0A%20%20Networks%20for%20Time%20Series&body=Title%3A%20FocusLearn%3A%20Fully-Interpretable%2C%20High-Performance%20Modular%20Neural%0A%20%20Networks%20for%20Time%20Series%0AAuthor%3A%20Qiqi%20Su%20and%20Christos%20Kloukinas%20and%20Artur%20d%27Avila%20Garcez%0AAbstract%3A%20%20%20Multivariate%20time%20series%20have%20many%20applications%2C%20from%20healthcare%20and%0Ameteorology%20to%20life%20science.%20Although%20deep%20learning%20models%20have%20shown%20excellent%0Apredictive%20performance%20for%20time%20series%2C%20they%20have%20been%20criticised%20for%20being%0A%22black-boxes%22%20or%20non-interpretable.%20This%20paper%20proposes%20a%20novel%20modular%20neural%0Anetwork%20model%20for%20multivariate%20time%20series%20prediction%20that%20is%20interpretable%20by%0Aconstruction.%20A%20recurrent%20neural%20network%20learns%20the%20temporal%20dependencies%20in%0Athe%20data%20while%20an%20attention-based%20feature%20selection%20component%20selects%20the%20most%0Arelevant%20features%20and%20suppresses%20redundant%20features%20used%20in%20the%20learning%20of%20the%0Atemporal%20dependencies.%20A%20modular%20deep%20network%20is%20trained%20from%20the%20selected%0Afeatures%20independently%20to%20show%20the%20users%20how%20features%20influence%20outcomes%2C%0Amaking%20the%20model%20interpretable.%20Experimental%20results%20show%20that%20this%20approach%0Acan%20outperform%20state-of-the-art%20interpretable%20Neural%20Additive%20Models%20%28NAM%29%20and%0Avariations%20thereof%20in%20both%20regression%20and%20classification%20of%20time%20series%20tasks%2C%0Aachieving%20a%20predictive%20performance%20that%20is%20comparable%20to%20the%20top%0Anon-interpretable%20methods%20for%20time%20series%2C%20LSTM%20and%20XGBoost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16834v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocusLearn%253A%2520Fully-Interpretable%252C%2520High-Performance%2520Modular%2520Neural%250A%2520%2520Networks%2520for%2520Time%2520Series%26entry.906535625%3DQiqi%2520Su%2520and%2520Christos%2520Kloukinas%2520and%2520Artur%2520d%2527Avila%2520Garcez%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520have%2520many%2520applications%252C%2520from%2520healthcare%2520and%250Ameteorology%2520to%2520life%2520science.%2520Although%2520deep%2520learning%2520models%2520have%2520shown%2520excellent%250Apredictive%2520performance%2520for%2520time%2520series%252C%2520they%2520have%2520been%2520criticised%2520for%2520being%250A%2522black-boxes%2522%2520or%2520non-interpretable.%2520This%2520paper%2520proposes%2520a%2520novel%2520modular%2520neural%250Anetwork%2520model%2520for%2520multivariate%2520time%2520series%2520prediction%2520that%2520is%2520interpretable%2520by%250Aconstruction.%2520A%2520recurrent%2520neural%2520network%2520learns%2520the%2520temporal%2520dependencies%2520in%250Athe%2520data%2520while%2520an%2520attention-based%2520feature%2520selection%2520component%2520selects%2520the%2520most%250Arelevant%2520features%2520and%2520suppresses%2520redundant%2520features%2520used%2520in%2520the%2520learning%2520of%2520the%250Atemporal%2520dependencies.%2520A%2520modular%2520deep%2520network%2520is%2520trained%2520from%2520the%2520selected%250Afeatures%2520independently%2520to%2520show%2520the%2520users%2520how%2520features%2520influence%2520outcomes%252C%250Amaking%2520the%2520model%2520interpretable.%2520Experimental%2520results%2520show%2520that%2520this%2520approach%250Acan%2520outperform%2520state-of-the-art%2520interpretable%2520Neural%2520Additive%2520Models%2520%2528NAM%2529%2520and%250Avariations%2520thereof%2520in%2520both%2520regression%2520and%2520classification%2520of%2520time%2520series%2520tasks%252C%250Aachieving%2520a%2520predictive%2520performance%2520that%2520is%2520comparable%2520to%2520the%2520top%250Anon-interpretable%2520methods%2520for%2520time%2520series%252C%2520LSTM%2520and%2520XGBoost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16834v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocusLearn%3A%20Fully-Interpretable%2C%20High-Performance%20Modular%20Neural%0A%20%20Networks%20for%20Time%20Series&entry.906535625=Qiqi%20Su%20and%20Christos%20Kloukinas%20and%20Artur%20d%27Avila%20Garcez&entry.1292438233=%20%20Multivariate%20time%20series%20have%20many%20applications%2C%20from%20healthcare%20and%0Ameteorology%20to%20life%20science.%20Although%20deep%20learning%20models%20have%20shown%20excellent%0Apredictive%20performance%20for%20time%20series%2C%20they%20have%20been%20criticised%20for%20being%0A%22black-boxes%22%20or%20non-interpretable.%20This%20paper%20proposes%20a%20novel%20modular%20neural%0Anetwork%20model%20for%20multivariate%20time%20series%20prediction%20that%20is%20interpretable%20by%0Aconstruction.%20A%20recurrent%20neural%20network%20learns%20the%20temporal%20dependencies%20in%0Athe%20data%20while%20an%20attention-based%20feature%20selection%20component%20selects%20the%20most%0Arelevant%20features%20and%20suppresses%20redundant%20features%20used%20in%20the%20learning%20of%20the%0Atemporal%20dependencies.%20A%20modular%20deep%20network%20is%20trained%20from%20the%20selected%0Afeatures%20independently%20to%20show%20the%20users%20how%20features%20influence%20outcomes%2C%0Amaking%20the%20model%20interpretable.%20Experimental%20results%20show%20that%20this%20approach%0Acan%20outperform%20state-of-the-art%20interpretable%20Neural%20Additive%20Models%20%28NAM%29%20and%0Avariations%20thereof%20in%20both%20regression%20and%20classification%20of%20time%20series%20tasks%2C%0Aachieving%20a%20predictive%20performance%20that%20is%20comparable%20to%20the%20top%0Anon-interpretable%20methods%20for%20time%20series%2C%20LSTM%20and%20XGBoost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16834v4&entry.124074799=Read"},
{"title": "WateRF: Robust Watermarks in Radiance Fields for Protection of\n  Copyrights", "author": "Youngdong Jang and Dong In Lee and MinHyuk Jang and Jong Wook Kim and Feng Yang and Sangpil Kim", "abstract": "  The advances in the Neural Radiance Fields (NeRF) research offer extensive\napplications in diverse domains, but protecting their copyrights has not yet\nbeen researched in depth. Recently, NeRF watermarking has been considered one\nof the pivotal solutions for safely deploying NeRF-based 3D representations.\nHowever, existing methods are designed to apply only to implicit or explicit\nNeRF representations. In this work, we introduce an innovative watermarking\nmethod that can be employed in both representations of NeRF. This is achieved\nby fine-tuning NeRF to embed binary messages in the rendering process. In\ndetail, we propose utilizing the discrete wavelet transform in the NeRF space\nfor watermarking. Furthermore, we adopt a deferred back-propagation technique\nand introduce a combination with the patch-wise loss to improve rendering\nquality and bit accuracy with minimum trade-offs. We evaluate our method in\nthree different aspects: capacity, invisibility, and robustness of the embedded\nwatermarks in the 2D-rendered images. Our method achieves state-of-the-art\nperformance with faster training speed over the compared state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2405.02066v1", "date": "2024-05-03", "relevancy": 1.5715, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5431}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.51}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WateRF%3A%20Robust%20Watermarks%20in%20Radiance%20Fields%20for%20Protection%20of%0A%20%20Copyrights&body=Title%3A%20WateRF%3A%20Robust%20Watermarks%20in%20Radiance%20Fields%20for%20Protection%20of%0A%20%20Copyrights%0AAuthor%3A%20Youngdong%20Jang%20and%20Dong%20In%20Lee%20and%20MinHyuk%20Jang%20and%20Jong%20Wook%20Kim%20and%20Feng%20Yang%20and%20Sangpil%20Kim%0AAbstract%3A%20%20%20The%20advances%20in%20the%20Neural%20Radiance%20Fields%20%28NeRF%29%20research%20offer%20extensive%0Aapplications%20in%20diverse%20domains%2C%20but%20protecting%20their%20copyrights%20has%20not%20yet%0Abeen%20researched%20in%20depth.%20Recently%2C%20NeRF%20watermarking%20has%20been%20considered%20one%0Aof%20the%20pivotal%20solutions%20for%20safely%20deploying%20NeRF-based%203D%20representations.%0AHowever%2C%20existing%20methods%20are%20designed%20to%20apply%20only%20to%20implicit%20or%20explicit%0ANeRF%20representations.%20In%20this%20work%2C%20we%20introduce%20an%20innovative%20watermarking%0Amethod%20that%20can%20be%20employed%20in%20both%20representations%20of%20NeRF.%20This%20is%20achieved%0Aby%20fine-tuning%20NeRF%20to%20embed%20binary%20messages%20in%20the%20rendering%20process.%20In%0Adetail%2C%20we%20propose%20utilizing%20the%20discrete%20wavelet%20transform%20in%20the%20NeRF%20space%0Afor%20watermarking.%20Furthermore%2C%20we%20adopt%20a%20deferred%20back-propagation%20technique%0Aand%20introduce%20a%20combination%20with%20the%20patch-wise%20loss%20to%20improve%20rendering%0Aquality%20and%20bit%20accuracy%20with%20minimum%20trade-offs.%20We%20evaluate%20our%20method%20in%0Athree%20different%20aspects%3A%20capacity%2C%20invisibility%2C%20and%20robustness%20of%20the%20embedded%0Awatermarks%20in%20the%202D-rendered%20images.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%20with%20faster%20training%20speed%20over%20the%20compared%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWateRF%253A%2520Robust%2520Watermarks%2520in%2520Radiance%2520Fields%2520for%2520Protection%2520of%250A%2520%2520Copyrights%26entry.906535625%3DYoungdong%2520Jang%2520and%2520Dong%2520In%2520Lee%2520and%2520MinHyuk%2520Jang%2520and%2520Jong%2520Wook%2520Kim%2520and%2520Feng%2520Yang%2520and%2520Sangpil%2520Kim%26entry.1292438233%3D%2520%2520The%2520advances%2520in%2520the%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520research%2520offer%2520extensive%250Aapplications%2520in%2520diverse%2520domains%252C%2520but%2520protecting%2520their%2520copyrights%2520has%2520not%2520yet%250Abeen%2520researched%2520in%2520depth.%2520Recently%252C%2520NeRF%2520watermarking%2520has%2520been%2520considered%2520one%250Aof%2520the%2520pivotal%2520solutions%2520for%2520safely%2520deploying%2520NeRF-based%25203D%2520representations.%250AHowever%252C%2520existing%2520methods%2520are%2520designed%2520to%2520apply%2520only%2520to%2520implicit%2520or%2520explicit%250ANeRF%2520representations.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520innovative%2520watermarking%250Amethod%2520that%2520can%2520be%2520employed%2520in%2520both%2520representations%2520of%2520NeRF.%2520This%2520is%2520achieved%250Aby%2520fine-tuning%2520NeRF%2520to%2520embed%2520binary%2520messages%2520in%2520the%2520rendering%2520process.%2520In%250Adetail%252C%2520we%2520propose%2520utilizing%2520the%2520discrete%2520wavelet%2520transform%2520in%2520the%2520NeRF%2520space%250Afor%2520watermarking.%2520Furthermore%252C%2520we%2520adopt%2520a%2520deferred%2520back-propagation%2520technique%250Aand%2520introduce%2520a%2520combination%2520with%2520the%2520patch-wise%2520loss%2520to%2520improve%2520rendering%250Aquality%2520and%2520bit%2520accuracy%2520with%2520minimum%2520trade-offs.%2520We%2520evaluate%2520our%2520method%2520in%250Athree%2520different%2520aspects%253A%2520capacity%252C%2520invisibility%252C%2520and%2520robustness%2520of%2520the%2520embedded%250Awatermarks%2520in%2520the%25202D-rendered%2520images.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520with%2520faster%2520training%2520speed%2520over%2520the%2520compared%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WateRF%3A%20Robust%20Watermarks%20in%20Radiance%20Fields%20for%20Protection%20of%0A%20%20Copyrights&entry.906535625=Youngdong%20Jang%20and%20Dong%20In%20Lee%20and%20MinHyuk%20Jang%20and%20Jong%20Wook%20Kim%20and%20Feng%20Yang%20and%20Sangpil%20Kim&entry.1292438233=%20%20The%20advances%20in%20the%20Neural%20Radiance%20Fields%20%28NeRF%29%20research%20offer%20extensive%0Aapplications%20in%20diverse%20domains%2C%20but%20protecting%20their%20copyrights%20has%20not%20yet%0Abeen%20researched%20in%20depth.%20Recently%2C%20NeRF%20watermarking%20has%20been%20considered%20one%0Aof%20the%20pivotal%20solutions%20for%20safely%20deploying%20NeRF-based%203D%20representations.%0AHowever%2C%20existing%20methods%20are%20designed%20to%20apply%20only%20to%20implicit%20or%20explicit%0ANeRF%20representations.%20In%20this%20work%2C%20we%20introduce%20an%20innovative%20watermarking%0Amethod%20that%20can%20be%20employed%20in%20both%20representations%20of%20NeRF.%20This%20is%20achieved%0Aby%20fine-tuning%20NeRF%20to%20embed%20binary%20messages%20in%20the%20rendering%20process.%20In%0Adetail%2C%20we%20propose%20utilizing%20the%20discrete%20wavelet%20transform%20in%20the%20NeRF%20space%0Afor%20watermarking.%20Furthermore%2C%20we%20adopt%20a%20deferred%20back-propagation%20technique%0Aand%20introduce%20a%20combination%20with%20the%20patch-wise%20loss%20to%20improve%20rendering%0Aquality%20and%20bit%20accuracy%20with%20minimum%20trade-offs.%20We%20evaluate%20our%20method%20in%0Athree%20different%20aspects%3A%20capacity%2C%20invisibility%2C%20and%20robustness%20of%20the%20embedded%0Awatermarks%20in%20the%202D-rendered%20images.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%20with%20faster%20training%20speed%20over%20the%20compared%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02066v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


